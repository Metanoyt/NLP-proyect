__future__ annotations textwrap dataclasses dataclass typing TYPE_CHECKING torchgen api translate translate torchgen api types DispatcherSignature torchgen context method_with_native_function torchgen model Argument BaseTy BaseType FunctionSchema ListType NativeFunction OptionalType Return SchemaKind Type torchgen utils mapMaybe TYPE_CHECKING collections abc Sequence is_tensor typ Type - bool isinstance typ BaseType typ name == BaseTy Tensor is_optional_tensor typ Type - bool isinstance typ OptionalType is_tensor typ elem is_tensor_list typ Type - bool isinstance typ ListType is_tensor typ elem unwrap_tensor name str cur_level_var str - list str result = f \ auto name _value name _bdim = unwrapTensorAtLevel name cur_level_var textwrap dedent result split \n unwrap_optional_tensor name str cur_level_var str - list str result = f \ std optional Tensor name _value std optional int _t name _bdim name std tie name _value name _bdim = unwrapTensorAtLevel name value cur_level_var textwrap dedent result split \n gen_unwraps flat_arguments Sequence Argument cur_level_var str - tuple str list str arg_names = name flat_arguments arg_types = type flat_arguments tensors = name typ name zip arg_types arg_names is_tensor typ optional_tensors = name typ name zip arg_types arg_names is_optional_tensor typ unwraps = tensor tensors unwraps += unwrap_tensor tensor cur_level_var opt_tensor optional_tensors unwraps += unwrap_optional_tensor opt_tensor cur_level_var unwrap_code = \n join unwraps unwrapped_arg_list = arg arg_names arg tensors arg optional_tensors unwrapped_arg_list += f arg _value f arg _bdim unwrapped_arg_list append arg unwrap_code unwrapped_arg_list gen_case_where_all_bdims_are_none outer_sig DispatcherSignature schema FunctionSchema cur_level_var str - str conditions = flat_args = schema arguments flat_all arg flat_args arg type is_tensor_like continue conditions append f isBatchedAtLevel arg name cur_level_var sig = DispatcherSignature from_schema schema translated_args = join e expr e translate outer_sig arguments sig arguments f \ join conditions _ops sig func name unambiguous_name call translated_args gen_returns returns tuple Return cur_level_var str results_var str - str idx = wrapped_returns = ret returns is_tensor ret type wrapped_returns append f makeBatched std get idx results_var std get idx + results_var cur_level_var idx += is_tensor_list ret type wrapped_returns append f makeBatchedVector std get idx results_var std get idx + results_var cur_level_var idx += wrapped_returns append f std get idx results_var idx += len wrapped_returns == result = f wrapped_returns result = f std make_tuple join wrapped_returns result accepts_at_least_one_tensor_input schema FunctionSchema - bool any type is_tensor_like schema arguments flat_all is_mutated_arg argument Argument - bool argument annotation None argument annotation is_write gen_vmap_inplace_plumbing native_function NativeFunction - str &#124; None Assumptions - only one argument being modified in-place - argument being modified in-place first argument - all returns either Tensor tuple Tensor TensorList schema = native_function func sig = DispatcherSignature from_schema schema returns = schema returns Check assumptions If these invalid we None punt work handle them future assert schema kind == SchemaKind inplace is_mutated_arg schema arguments flat_all None len arg arg schema arguments flat_all is_mutated_arg arg = None Only support cases where all returns Tensors vector Tensor len returns == None all is_tensor ret type is_tensor_list ret type ret returns None accepts_at_least_one_tensor_input schema None cur_level_var = cur_level unwraps unwrapped_arg_list = gen_unwraps schema arguments flat_all cur_level_var bdims_all_none_case = gen_case_where_all_bdims_are_none sig schema cur_level_var f \ template typename batch_rule_t batch_rule_t batch_rule sig decl name=schema name unambiguous_name + _generated_plumbing c impl ExcludeDispatchKeyGuard guard DispatchKey FuncTorchBatched auto maybe_layer = maybeCurrentDynamicLayer vmap_check_escaped maybe_layer gen_vmap_inplace_plumbing int _t cur_level_var = maybe_layer- layerId textwrap indent bdims_all_none_case textwrap indent unwraps batch_rule join unwrapped_arg_list schema arguments flat_all name gen_vmap_plumbing_no_returns native_function NativeFunction - str schema = native_function func sig = DispatcherSignature from_schema schema cur_level_var = cur_level unwraps unwrapped_arg_list = gen_unwraps schema arguments flat_all cur_level_var bdims_all_none_case = gen_case_where_all_bdims_are_none sig schema cur_level_var f \ template typename batch_rule_t batch_rule_t batch_rule sig decl name=schema name unambiguous_name + _generated_plumbing c impl ExcludeDispatchKeyGuard guard DispatchKey FuncTorchBatched auto maybe_layer = maybeCurrentDynamicLayer vmap_check_escaped maybe_layer gen_vmap_plumbing_no_returns int _t cur_level_var = maybe_layer- layerId textwrap indent bdims_all_none_case textwrap indent unwraps batch_rule join unwrapped_arg_list gen_vmap_plumbing native_function NativeFunction - str &#124; None schema = native_function func sig = DispatcherSignature from_schema schema returns = schema returns Only support cases where all returns Tensors vector Tensor accepts_at_least_one_tensor_input schema None len returns == gen_vmap_plumbing_no_returns native_function return_symint_overrides = _scaled_dot_product_flash_attention _scaled_dot_product_cudnn_attention all ret type is_tensor_like ret returns schema name unambiguous_name return_symint_overrides None in-place views need special handling inplace_view native_function tags None schema kind == SchemaKind inplace gen_vmap_inplace_plumbing native_function Don t support these mutable out scratch schema kind = SchemaKind functional None results_var = results cur_level_var = cur_level unwraps unwrapped_arg_list = gen_unwraps schema arguments flat_all cur_level_var bdims_all_none_case = gen_case_where_all_bdims_are_none sig schema cur_level_var wrapped_returns = gen_returns returns cur_level_var results_var f \ template typename batch_rule_t batch_rule_t batch_rule sig decl name=schema name unambiguous_name + _generated_plumbing c impl ExcludeDispatchKeyGuard guard DispatchKey FuncTorchBatched auto maybe_layer = maybeCurrentDynamicLayer vmap_check_escaped maybe_layer gen_vmap_plumbing int _t cur_level_var = maybe_layer- layerId textwrap indent bdims_all_none_case textwrap indent unwraps auto results_var = batch_rule join unwrapped_arg_list wrapped_returns dataclass frozen=True ComputeBatchRulePlumbing method_with_native_function __call__ f NativeFunction - str &#124; None result = gen_vmap_plumbing f result gen_all_vmap_plumbing native_functions Sequence NativeFunction - str body = \n join list mapMaybe ComputeBatchRulePlumbing native_functions f #pragma once #include ATen Operators h #include ATen functorch PlumbingHelper h namespace namespace functorch body namespace functorch