mypy allow-untyped-defs collections abc Sequence typing Any Optional Union sympy torch torch _prims_common make_channels_last_strides_for StrideType torch utils _ordered_set OrderedSet ir ExternKernelAlloc FixedLayout FlexibleLayout get_device_type ir_node_to_tensor IRNode is_contiguous_storage_and_layout Layout may_convert_to_optional MultiOutput MultiOutputLayout MutationOutput NoneLayout ShapeAsConstantBuffer TensorBox utils convert_shape_to_inductor pad_listlike SUPPORTED_MKLDNN_DEVICES virtualized V _prepare_convolution_fusion_create cls x TensorBox weight TensorBox bias TensorBox padding Sequence int stride Sequence int dilation Sequence int groups int transposed bool = False output_padding Optional Sequence int = None quantize_args Optional list TensorBox = None other Optional TensorBox = None This function helper function prepare inputs layout constant args convolution post-op fusion s create function including deciding output layout channels first channels last realizing inputs make them etc The function only supports CPU XPU device since conv post-op fusion kernel only supported CPU XPU right now Port aten src ATen native ConvUtils h _conv_input_size _conv_input_size output_size weight_size padding output_padding stride dilation groups assert len output_size == len weight_size Expect input dim == weight dim dim = len output_size assert dim Expect input dim BATCH_DIM = WEIGHT_INPUT_CHANNELS_DIM = input_size = input_size append output_size BATCH_DIM input_size append weight_size WEIGHT_INPUT_CHANNELS_DIM groups d range dim kernel = weight_size d - dilation d - + input_size_d = output_size d - stride d - - padding d - + kernel + output_padding d - input_size append input_size_d list map int input_size Port aten src ATen native ConvUtils h _conv_output_size _conv_output_size input_size weight_size padding stride dilation=None has_dilation = dilation None dim = len input_size output_size = output_size append input_size output_size append weight_size d range dim pyrefly ignore unsupported-operation dilation_ = dilation d - has_dilation kernel = dilation_ weight_size d - + output_size_d = input_size d + padding d - - kernel stride d - + output_size append output_size_d output_size The size prepacked_weight prepacked weight size deconv Groups g o i g Groups == o i Returns original weight size i o _original_deconv_weight_size prepacked_weight groups prepacked_weight_size = prepacked_weight size dim = len prepacked_weight_size assert dim Expect weight dim groups weight_size = weight_size append prepacked_weight_size groups weight_size append prepacked_weight_size groups weight_size extend prepacked_weight_size d d range dim weight_size = prepacked_weight transpose size weight_size x realize weight realize bias None bias realize V graph fake_mode TODO Leslie cleaned up fake_tensor trace Linear implementation x_fake = ir_node_to_tensor x guard_shape=True weight_fake = ir_node_to_tensor weight guard_shape=True dims = len x_fake size - assert len padding = dims assert len dilation = dims assert len stride = dims padding = pad_listlike padding dims dilation = pad_listlike dilation dims stride = pad_listlike stride dims output_padding None output_padding = pad_listlike dims assert len output_padding = dims output_padding = pad_listlike output_padding dims assert isinstance groups int sympy core numbers Integer transposed When transposed size prepacked oneDNN weight different PyTorch weight We re able run aten conv such size We infer output size input params here weight_size = _original_deconv_weight_size weight_fake groups input_size = x_fake size output_size = _conv_input_size input_size weight_size padding output_padding stride dilation groups x_shape = list x_fake shape weight_shape = list weight_fake shape len x_shape = len weight_shape assert len x_shape == len weight_shape == weight_shape pop output_size = _conv_output_size x_shape weight_shape padding stride dilation req_stride_order = + list reversed range len stride + req_stride_order = len req_stride_order + req_stride_order x = cls require_stride_order x req_stride_order We won t do weight prepack Conv dynamic_shapes xpu In static shape cases since weight prepacked we ll always force output channels last Conv kernel In dynamic shape cases input channels = like tensor size s stride x = cls require_stride_order x req_stride_order where req_stride_order channels last order won t change stride tensor since stride dimensions size ignored While Conv kernel tensor considered channels first output will contiguous format To align behavior Conv kernel we set output_stride such case contiguous instead channels last dynamic_shapes = all isinstance i int i output_size dynamic_shapes get_device_type x == xpu is_contiguous_storage_and_layout x output_stride StrideType = FlexibleLayout contiguous_strides output_size Currently we don t support channel last situation stride input s batch dim eg input_size = input_stride= So we use NCHW hear instead Different cpu cpu conv always use channels_last convolution when weight prepacked xpu does do prepack so problem exposed here only xpu TODO support channels_last such zero stride input get_device_type x == xpu x get_stride == output_stride = FlexibleLayout contiguous_strides output_size output_stride = make_channels_last_strides_for output_size assert get_device_type x == get_device_type weight assert get_device_type x SUPPORTED_MKLDNN_DEVICES inputs = x quantize_args None x_scale x_zero_point w_scale w_zero_point = quantize_args x_scale realize x_zero_point realize w_scale realize w_zero_point realize inputs = inputs + x_scale x_zero_point + weight + w_scale w_zero_point inputs += weight other None other = cls require_stride_order other req_stride_order assert isinstance other TensorBox inputs += other kernel_layout = FixedLayout x get_device_or_error x get_dtype convert_shape_to_inductor output_size convert_shape_to_inductor output_stride constant_args = padding stride dilation groups transposed constant_args insert output_padding bias None inputs append bias constant_args insert bias inputs constant_args kernel_layout req_stride_order other _prepare_linear_fusion_create cls x TensorBox weight TensorBox bias TensorBox quantize_args Optional list TensorBox = None other Optional TensorBox = None binary_sum bool = False This function helper function prepare inputs layout constant args linear post-op fusion s create function The function only supports CPU device since linear post-op fusion kernel only supported CPU right now x realize weight realize bias None bias realize m _ = x get_size The weight has been transposed during qlinear weight prepack process https github com pytorch pytorch blob f c d e bb d f d f b aten src ATen native quantized cpu qlinear_prepack cpp#L _ oc = weight get_size output_size = list m + oc req_stride_order = list reversed range len x get_size x = cls require_stride_order x req_stride_order assert get_device_type x == get_device_type weight assert get_device_type x SUPPORTED_MKLDNN_DEVICES inputs = x quantize_args None x_scale x_zero_point w_scale w_zero_point = quantize_args x_scale realize x_zero_point realize w_scale realize w_zero_point realize inputs = inputs + x_scale x_zero_point + weight + w_scale w_zero_point inputs += weight other None binary_sum other = cls require_stride_order other req_stride_order inputs = inputs + other output_stride = FlexibleLayout contiguous_strides output_size kernel_layout = FixedLayout x get_device x get_dtype output_size output_stride constant_args list Any = bias None inputs append bias constant_args insert bias inputs constant_args kernel_layout req_stride_order other _create_output_node packed output_ir = MultiOutput packed get_layout packed packed layout = MultiOutputLayout device=packed get_device packed outputs = output_ir output_ir ConvolutionUnary ExternKernelAlloc __init__ layout inputs constant_args= - None device_type = get_device_type inputs super __init__ layout inputs constant_args None op_overload=torch ops mkldnn _convolution_pointwise default cpp_kernel_name=f aoti_torch_ device_type _mkldnn__convolution_pointwise codegen wrapper wrapper include_extra_header f torch csrc inductor aoti_torch c shim_ device_type h super codegen wrapper classmethod create cls x TensorBox weight TensorBox bias TensorBox padding_ list int stride_ list int dilation_ list int groups int attr scalars Optional list Any algorithm inputs constant_args kernel_layout _ _ = _prepare_convolution_fusion_create cls x weight bias padding_ stride_ dilation_ groups constant_args = constant_args + attr may_convert_to_optional scalars algorithm packed = ConvolutionUnary layout=kernel_layout inputs=inputs constant_args=constant_args _create_output_node packed ConvolutionBinary ExternKernelAlloc __init__ layout inputs constant_args= cpp_constant_args= - None device_type = get_device_type inputs super __init__ layout inputs constant_args None op_overload=torch ops mkldnn _convolution_pointwise binary cpp_kernel_name=f aoti_torch_ device_type _mkldnn__convolution_pointwise_binary cpp_constant_args = cpp_constant_args codegen wrapper wrapper include_extra_header f torch csrc inductor aoti_torch c shim_ device_type h super codegen wrapper classmethod create cls x TensorBox other TensorBox weight TensorBox bias TensorBox padding_ list int stride_ list int dilation_ list int groups int binary_attr str binary_alpha Optional float unary_attr Optional str unary_scalars Optional list Any unary_algorithm Optional str inputs constant_args kernel_layout req_stride_order _ = _prepare_convolution_fusion_create cls x weight bias padding_ stride_ dilation_ groups pyrefly ignore bad-assignment other = cls require_stride_order other req_stride_order inputs insert other constant_args = constant_args + binary_attr binary_alpha unary_attr may_convert_to_optional unary_scalars unary_algorithm packed = ConvolutionBinary layout=kernel_layout inputs=inputs constant_args=constant_args _create_output_node packed ConvolutionBinaryInplace ExternKernelAlloc __init__ kernel_layout inputs constant_args= - None Due constrain op call other Tensor should input device_type = get_device_type inputs reordered_inputs = inputs inputs + inputs super __init__ kernel_layout reordered_inputs constant_args None op_overload=torch ops mkldnn _convolution_pointwise_ binary cpp_kernel_name=f aoti_torch_ device_type _mkldnn__convolution_pointwise_binary_ mutation_outputs = MutationOutput NoneLayout device=inputs get_device inputs MutationOutput NoneLayout device=inputs get_device inputs codegen wrapper wrapper include_extra_header f torch csrc inductor aoti_torch c shim_ device_type h super codegen wrapper get_unbacked_symbol_defs - OrderedSet sympy Symbol OrderedSet classmethod create cls x TensorBox other TensorBox weight TensorBox bias TensorBox padding_ list int stride_ list int dilation_ list int groups int binary_attr str binary_alpha Optional float unary_attr Optional str unary_scalars Optional list Any unary_algorithm Optional str inputs constant_args _ req_stride_order _ = _prepare_convolution_fusion_create cls x weight bias padding_ stride_ dilation_ groups pyrefly ignore bad-assignment other = cls require_stride_order other req_stride_order inputs insert other constant_args = constant_args + binary_attr binary_alpha unary_attr may_convert_to_optional unary_scalars unary_algorithm packed = ConvolutionBinaryInplace kernel_layout=NoneLayout device=inputs get_device type ignore arg-type inputs=inputs constant_args=constant_args This op mutates place which means result target rather input being mutated init reorders inputs so inputs becomes packed inputs packed inputs ConvolutionTransposeUnary ExternKernelAlloc __init__ layout inputs constant_args= - None device_type = get_device_type inputs super __init__ layout inputs constant_args None op_overload=torch ops mkldnn _convolution_transpose_pointwise default cpp_kernel_name=f aoti_torch_ device_type _mkldnn__convolution_transpose_pointwise codegen wrapper wrapper include_extra_header f torch csrc inductor aoti_torch c shim_ device_type h super codegen wrapper classmethod create cls x TensorBox weight TensorBox bias TensorBox padding_ list int output_padding_ list int stride_ list int dilation_ list int groups_ int attr scalars Optional list Any algorithm transposed = True inputs constant_args kernel_layout _ _ = _prepare_convolution_fusion_create cls x weight bias padding_ stride_ dilation_ groups_ transposed output_padding_ constant_args = constant_args + attr may_convert_to_optional scalars algorithm packed = ConvolutionTransposeUnary layout=kernel_layout inputs=inputs constant_args=constant_args _create_output_node packed QConvPointWisePT E ExternKernelAlloc __init__ layout inputs constant_args= - None bias None - inputs = x w b weight_scale weight_zp - const_args stride padding dilation groups x_scale x_zp o_scale o_zp fp _output unary_attr unary_scalars unary_algorithm - inputs = x w weight_scale weight_zp - const_args bias stride padding dilation groups x_scale x_zp o_scale o_zp fp _output unary_attr unary_scalars unary_algorithm device_type = get_device_type inputs has_bias = len inputs == super __init__ layout inputs constant_args None op_overload=torch ops onednn qconv_pointwise default cpp_kernel_name=f aoti_torch_ device_type __qconv_pointwise_tensor codegen wrapper wrapper include_extra_header f torch csrc inductor aoti_torch c shim_ device_type h super codegen wrapper isinstance layout Layout codegen_size_asserts wrapper classmethod create cls qx TensorBox x_scale Union ShapeAsConstantBuffer TensorBox x_zero_point Union ShapeAsConstantBuffer TensorBox qw TensorBox qw w_scale TensorBox w_zero_point TensorBox bias TensorBox stride list int padding list int dilation list int groups int output_scale float output_zero_point int output_dtype attr scalars algorithm transposed = False output_padding = None inputs constant_args kernel_layout _ _ = _prepare_convolution_fusion_create cls qx qw bias padding stride dilation groups transposed output_padding x_scale x_zero_point w_scale w_zero_point type ignore list-item swap padding stride align functional conv arg order bias None constant_args constant_args = constant_args constant_args constant_args constant_args = constant_args constant_args constant_args = constant_args + output_scale output_zero_point output_dtype attr may_convert_to_optional scalars algorithm assert output_dtype None output_dtype torch float torch bfloat _prepare_convolution_fusion_create we use x dtype uint create kernel_layout we set output_dtype None output buf should output_dtype instead uint kernel_layout dtype = output_dtype QConvPointWisePT E layout=kernel_layout inputs=inputs constant_args=constant_args QConvPointWiseBinaryPT E ExternKernelAlloc __init__ layout inputs constant_args= - None Needs input weight output qparams bias None - inputs = x x_scale x_zp w w_scale w_zp accum b - const_args = stride padding dilation groups o_scale o_zp output_dtype accum_scale accum_zp binary_attr alpha unary_attr unary_scalars unary_algorithm - inputs = x x_scale x_zp w w_scale w_zp accum - const_args b stride padding dilation groups o_scale o_zp output_dtype accum_scale accum_zp binary_attr alpha unary_attr unary_scalars unary_algorithm device_type = get_device_type inputs has_bias = len inputs == idx_for_inplace_sum = super __init__ layout inputs constant_args None op_overload=torch ops onednn qconv d_pointwise binary cpp_kernel_name= f aoti_torch_ device_type __qconv d_pointwise_binary_tensor codegen wrapper wrapper include_extra_header f torch csrc inductor aoti_torch c shim_ device_type h super codegen wrapper isinstance layout Layout codegen_size_asserts wrapper get_mutation_names - Sequence str input_name idx_for_inplace_sum get_unbacked_symbol_defs - OrderedSet sympy Symbol OrderedSet classmethod create cls qx TensorBox x_scale TensorBox x_zero_point TensorBox qw TensorBox packed_weight w_scale w_zero_point qaccum TensorBox bias TensorBox stride list int padding list int dilation list int groups int output_scale TensorBox output_zero_point TensorBox output_dtype accum_scale accum_zero_point binary_attr alpha unary_attr unary_scalars unary_algorithm transposed = False output_padding = None inputs constant_args _kernel_layout req_stride_order qaccum = _prepare_convolution_fusion_create cls qx qw bias padding stride dilation groups transposed output_padding x_scale x_zero_point w_scale w_zero_point qaccum swap padding stride align functional conv arg order bias None constant_args constant_args = constant_args constant_args constant_args constant_args = constant_args constant_args constant_args = constant_args + output_scale output_zero_point output_dtype accum_scale accum_zero_point binary_attr alpha unary_attr may_convert_to_optional unary_scalars unary_algorithm assert binary_attr == sum For now only post op sum supported QConvPointWiseBinaryPT E V graph mark_buffer_mutated qaccum get_name packed = QConvPointWiseBinaryPT E layout=NoneLayout device=qaccum get_device inputs=inputs constant_args=constant_args Return accum since has been inplace changed packed inputs packed idx_for_inplace_sum MKLPackedLinear ExternKernelAlloc __init__ layout inputs constant_args= - None super __init__ layout inputs constant_args None op_overload=torch ops mkl _mkl_linear default codegen wrapper wrapper include_extra_header torch csrc inductor aoti_torch c shim_cpu h super codegen wrapper classmethod create cls x packed_w orig_w B batch_size x = cls require_stride cls realize_input x orig_w = cls require_stride cls realize_input orig_w m _ = x get_size oc _ = orig_w get_size output_size = list m + oc output_stride = FlexibleLayout contiguous_strides output_size inputs = x packed_w orig_w constant_args = batch_size B None inputs += B constant_args insert None device = x get_device assert device None MKLPackedLinear layout=FixedLayout device x get_dtype output_size output_stride inputs=inputs constant_args=constant_args LinearUnary ExternKernelAlloc __init__ layout inputs constant_args= - None device_type = get_device_type inputs super __init__ layout inputs constant_args None op_overload=torch ops mkldnn _linear_pointwise default cpp_kernel_name=f aoti_torch_ device_type __linear_pointwise codegen wrapper wrapper include_extra_header f torch csrc inductor aoti_torch c shim_ device_type h super codegen wrapper classmethod create cls x w B attr scalars algorithm x = cls require_contiguous cls realize_input x w = cls require_contiguous cls realize_input w m _ic = x get_size oc _ic = w get_size output_size = list m + oc inputs = x w constant_args = attr scalars scalars - algorithm B None B = cls require_contiguous cls realize_input B inputs append B constant_args insert None device = x get_device assert device None packed = LinearUnary layout=FixedLayout device=device dtype=x get_dtype size=output_size inputs=inputs constant_args=constant_args _create_output_node packed apply_constraint pass LinearBinary ExternKernelAlloc kernel = torch ops mkldnn _linear_pointwise binary __init__ layout inputs constant_args= - None device_type = get_device_type inputs super __init__ layout inputs constant_args None op_overload=torch ops mkldnn _linear_pointwise binary cpp_kernel_name=f aoti_torch_ device_type __linear_pointwise_binary codegen wrapper wrapper include_extra_header f torch csrc inductor aoti_torch c shim_ device_type h super codegen wrapper classmethod create cls x y w B attr x = cls require_contiguous cls realize_input x y = cls require_contiguous cls realize_input y w = cls require_contiguous cls realize_input w m _ic = x get_size oc _ic = w get_size output_size = list m + oc inputs = x y w constant_args = attr B None B = cls require_contiguous cls realize_input B inputs append B constant_args insert B device = x get_device assert device None packed = LinearBinary layout=FixedLayout device=device dtype=x get_dtype size=output_size inputs=inputs constant_args=constant_args _create_output_node packed apply_constraint pass QLinearPointwisePT E ExternKernelAlloc __init__ layout inputs constant_args= has_bias=True - None bias None - inputs = x w b weight_scale weight_zp - const_args x_scale x_zp o_scale o_zp fp _output unary_attr unary_scalars unary_algorithm - inputs = x w weight_scale weight_zp - const_args bias x_scale x_zp o_scale o_zp fp _output unary_attr unary_scalars unary_algorithm device_type = get_device_type inputs has_bias = has_bias super __init__ layout inputs constant_args None op_overload= torch ops onednn qlinear_pointwise tensor cpp_kernel_name= f aoti_torch_ device_type __qlinear_pointwise_tensor codegen wrapper wrapper include_extra_header f torch csrc inductor aoti_torch c shim_ device_type h super codegen wrapper isinstance layout Layout codegen_size_asserts wrapper classmethod create cls qx TensorBox x_scale TensorBox x_zero_point TensorBox qw TensorBox packed_weight w_scale TensorBox w_zero_point TensorBox bias TensorBox output_scale float output_zero_point int output_dtype post_op_name post_op_args post_op_algorithm inputs constant_args kernel_layout _ _ = _prepare_linear_fusion_create cls qx qw bias x_scale x_zero_point w_scale w_zero_point constant_args = constant_args + output_scale output_zero_point output_dtype post_op_name may_convert_to_optional post_op_args post_op_algorithm assert output_dtype None output_dtype torch float torch bfloat _prepare_linear_fusion_create we use x dtype uint create kernel_layout we set fp _output output buf should dtype float instead uint kernel_layout dtype = output_dtype QLinearPointwisePT E layout=kernel_layout inputs=inputs constant_args=constant_args has_bias= bias None QLinearPointwiseBinaryPT E ExternKernelAlloc __init__ layout inputs constant_args= has_bias=True - None bias None - inputs = x w x_scale x_zp weight_scale weight_zp x bias - const_args o_scale o_zp fp _output binary_attr alpha unary_attr unary_scalars unary_algorithm - inputs = x w x_scale x_zp weight_scale weight_zp x - const_args bias o_scale o_zp fp _output binary_attr alpha unary_attr unary_scalars unary_algorithm device_type = get_device_type inputs has_bias = has_bias idx_for_inplace_sum = super __init__ layout inputs constant_args None op_overload= torch ops onednn qlinear_pointwise binary_tensor cpp_kernel_name=f aoti_torch_ device_type __qlinear_pointwise_binary_tensor codegen wrapper wrapper include_extra_header f torch csrc inductor aoti_torch c shim_ device_type h super codegen wrapper isinstance layout Layout codegen_size_asserts wrapper get_mutation_names - Sequence str binary_post_op = constant_args - binary_post_op == sum input = inputs idx_for_inplace_sum assert isinstance input IRNode input get_name classmethod create cls qx TensorBox x_scale TensorBox x_zero_point TensorBox qw TensorBox packed_weight w_scale TensorBox w_zero_point TensorBox other TensorBox bias TensorBox output_scale float output_zero_point int output_dtype other_scale other_zp binary_post_op binary_alpha unary_post_op unary_post_op_args unary_post_op_algorithm inputs constant_args kernel_layout req_stride_order other = _prepare_linear_fusion_create cls qx qw bias x_scale x_zero_point w_scale w_zero_point other binary_post_op == sum constant_args = constant_args + output_scale output_zero_point output_dtype other_scale other_zp binary_post_op binary_alpha unary_post_op may_convert_to_optional unary_post_op_args unary_post_op_algorithm binary_post_op == sum V graph mark_buffer_mutated other get_name packed = QLinearPointwiseBinaryPT E layout=NoneLayout device=other get_device inputs=inputs constant_args=constant_args has_bias= bias None Return other since has been inplace changed packed inputs packed idx_for_inplace_sum assert output_dtype None output_dtype torch float torch bfloat _prepare_linear_fusion_create we use x dtype uint create kernel_layout we set fp _output output buf should dtype float instead uint kernel_layout dtype = output_dtype QLinearPointwiseBinaryPT E layout=kernel_layout inputs=inputs constant_args=constant_args has_bias= bias None MkldnnRnnLayer ExternKernelAlloc __init__ layout inputs constant_args= - None super __init__ layout inputs constant_args None op_overload=torch ops aten mkldnn_rnn_layer default classmethod create cls x TensorBox w TensorBox w TensorBox w TensorBox w TensorBox hx TensorBox cx TensorBox reverse bool batch_sizes list int mode int hidden_size int num_layers int has_biases bool bidirectional bool batch_first bool train bool pyrefly ignore bad-assignment x = cls require_stride cls realize_input x If batch_first x has been permuted lstm before entering mkldnn_rnn_layer Make sure x contiguous batch_first case x freeze_layout pyrefly ignore bad-assignment w = cls require_stride cls realize_input w pyrefly ignore bad-assignment w = cls require_stride cls realize_input w pyrefly ignore bad-assignment w = cls require_stride cls realize_input w pyrefly ignore bad-assignment w = cls require_stride cls realize_input w pyrefly ignore bad-assignment hx = cls require_stride cls realize_input hx hx freeze_layout pyrefly ignore bad-assignment cx = cls require_stride cls realize_input cx cx freeze_layout input_size = x get_size assert len input_size == Expect lstm input D batch_first handled lstm OP When entering rnn_layer here we ll always have batch_first = False seq_length mini_batch input_size = input_size output_shape = seq_length mini_batch hidden_size hy_shape = hx get_size cy_shape = cx get_size inputs = x w w w w hx cx constant_args = reverse batch_sizes mode hidden_size num_layers has_biases bidirectional batch_first train device = x get_device assert device None packed = MkldnnRnnLayer MultiOutputLayout device=device inputs=inputs constant_args=constant_args get_strides_of_lstm_output output_shape batch_first assert len output_shape == Expect output_shape D FlexibleLayout contiguous_strides output_shape C shim call requires all outputs passed thus last dummy value added output_sizes = output_shape hy_shape cy_shape output_strides = get_strides_of_lstm_output output_shape batch_first FlexibleLayout contiguous_strides hy_shape FlexibleLayout contiguous_strides cy_shape output_ir = MultiOutput FixedLayout x get_device type ignore arg-type x get_dtype output_size output_stride packed tuple i i output_size output_stride enumerate zip output_sizes output_strides packed outputs = output_ir output_ir codegen wrapper wrapper include_extra_header torch csrc inductor aoti_torch c shim_cpu h super codegen wrapper Add IR so we can include shim_cpu h cpp_wrapper WeightInt PackMatmul ExternKernelAlloc __init__ layout inputs constant_args= - None inputs = x w qGroupSize qScalesAndZeros constant_args = assert len inputs == assert len constant_args == super __init__ layout inputs constant_args None op_overload= torch ops quantized int mm_packed_weight_cpu default cpp_kernel_name= aoti_torch_cpu__weight_int pack_mm_cpu_tensor codegen wrapper wrapper include_extra_header torch csrc inductor aoti_torch c shim_cpu h super codegen wrapper isinstance layout Layout codegen_size_asserts wrapper classmethod create cls x TensorBox w TensorBox qGroupSize TensorBox qScalesAndZeros TensorBox inputs = x w qGroupSize qScalesAndZeros m _ = x get_size n _ = w get_size output_size = list m + n output_stride = FlexibleLayout contiguous_strides output_size kernel_layout = FixedLayout x get_device type ignore arg-type x get_dtype output_size output_stride WeightInt PackMatmul layout=kernel_layout inputs=inputs