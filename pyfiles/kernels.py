typing Any cutlass cutlass torch cutlass_torch utils BenchmarkKernel torch torch nn functional F more important shapes used internal models extra_shapes_for_norm = CrossEntropyForward BenchmarkKernel __init__ script_args super __init__ script_args available_backends = eager compiled quack liger get_shapes - tuple tuple int get_memory_bytes args kwargs - int Read x M N elements + read target M elements + write loss M elements x target = args M N = x shape dtype = x dtype M N + M + M dtype itemsize eager args kwargs=None - Any assert kwargs None x target = args lambda F cross_entropy x target reduction= none compiled args kwargs=None - Any assert kwargs None x target = args Mark batch size dynamic realistic workload torch _dynamo mark_dynamic x torch _dynamo mark_dynamic target Need ` lambda ` otherwise torch compile will trace function More discussion https github com pytorch pytorch issues compiled_cross_entropy = torch compile lambda x target F cross_entropy x target reduction= none mode=self compile_mode fullgraph=True lambda compiled_cross_entropy x target quack args kwargs=None - Any assert kwargs None x target = args quack cross_entropy _cross_entropy lambda _cross_entropy x target liger args kwargs=None - Any assert kwargs None liger_kernel transformers cross_entropy LigerCrossEntropyLoss x target = args cross_entropy = LigerCrossEntropyLoss reduction= none lambda cross_entropy x target benchmark M N get_shapes print f \n Tensor dimensions M N quack requires cutlass dtype torch_dtype = cutlass_torch dtype cutlass BFloat x = torch randn M N device= cuda dtype=torch_dtype target = torch randint N M device= cuda dtype=torch int benchmark_single_shape x target setting=f shape M N check_accuracy args kwargs - None res = backend available_backends args_ref kwargs_ref = clone_inputs args kwargs res backend = getattr backend args_ref kwargs_ref gold = res eager backend available_backends backend == eager continue backend == quack quack s cross_entropy only returns float loss output Need convert same dtype gold comparison res backend = res backend gold dtype try torch testing assert_close res backend gold print f Accuracy check \ m✓ succeed\ m backend backend name kernel except Exception e print f Accuracy check \ m✗ failed\ m backend backend name kernel Error e CrossEntropyBackward BenchmarkKernel __init__ script_args super __init__ script_args available_backends = eager compiled quack liger get_shapes - tuple tuple int get_memory_bytes args kwargs - int Read x M N elements + read target M elements + read dloss M elements + write grad M N elements x target dloss = args Memory ba M N = x shape M N x dtype itemsize + M target dtype itemsize + M dloss dtype itemsize eager args kwargs=None - Any assert kwargs None x target dloss = args loss = F cross_entropy x target reduction= none lambda torch autograd grad loss x grad_outputs=dloss retain_graph=True compiled args kwargs=None - Any assert kwargs None x target dloss = args compiled_cross_entropy = torch compile lambda x target F cross_entropy x target reduction= none mode=self compile_mode fullgraph=True loss = compiled_cross_entropy x target lambda torch autograd grad loss x grad_outputs=dloss retain_graph=True quack args kwargs=None - Any quack cross_entropy cross_entropy assert kwargs None x target dloss = args loss = cross_entropy x target lambda torch autograd grad loss x grad_outputs=dloss retain_graph=True liger args kwargs=None - Any assert kwargs None liger_kernel transformers cross_entropy LigerCrossEntropyLoss x target dloss = args cross_entropy = LigerCrossEntropyLoss reduction= none loss = cross_entropy x target lambda torch autograd grad loss x grad_outputs=dloss retain_graph=True benchmark M N get_shapes print f Tensor dimensions M N torch_dtype = cutlass_torch dtype cutlass BFloat x = torch randn M N device= cuda dtype=torch_dtype requires_grad=True target = torch randint N M device= cuda dtype=torch int dloss = torch randn M device= cuda dtype=torch float benchmark_single_shape x target dloss setting=f shape M N SoftmaxForward BenchmarkKernel __init__ script_args super __init__ script_args available_backends = eager compiled quack liger get_shapes - tuple tuple int get_memory_bytes args kwargs - int x = args M N = x shape M N x dtype itemsize eager args kwargs=None - Any assert kwargs None x = args lambda F softmax x dim=- compiled args kwargs=None - Any assert kwargs None x = args Mark batch size dynamic realistic workload torch _dynamo mark_dynamic x compiled_softmax = torch compile lambda x F softmax x dim=- mode=self compile_mode fullgraph=True lambda compiled_softmax x quack args kwargs=None - Any quack softmax softmax assert kwargs None x = args lambda softmax x liger args kwargs=None - Any liger_kernel transformers softmax LigerSoftmax assert kwargs None x = args softmax = LigerSoftmax cuda lambda softmax x benchmark M N get_shapes print f Tensor dimensions M N torch_dtype = cutlass_torch dtype cutlass BFloat x = torch randn M N device= cuda dtype=torch_dtype benchmark_single_shape x setting=f shape M N SoftmaxBackward BenchmarkKernel __init__ script_args super __init__ script_args available_backends = eager compiled quack liger get_shapes - tuple tuple int get_memory_bytes args kwargs - int Memory read dy y write ax backward x dy = args M N = x shape M N x dtype itemsize eager args kwargs=None - Any assert kwargs None x dy = args y = F softmax x dim=- lambda torch autograd grad y x grad_outputs=dy retain_graph=True compiled args kwargs=None - Any assert kwargs None x dy = args compiled_softmax = torch compile lambda x F softmax x dim=- mode=self compile_mode fullgraph=True y = compiled_softmax x lambda torch autograd grad y x grad_outputs=dy retain_graph=True quack args kwargs=None - Any quack softmax softmax assert kwargs None x dy = args y = softmax x lambda torch autograd grad y x grad_outputs=dy retain_graph=True liger args kwargs=None - Any liger_kernel transformers softmax LigerSoftmax assert kwargs None x dy = args softmax = LigerSoftmax cuda y = softmax x lambda torch autograd grad y x grad_outputs=dy retain_graph=True benchmark M N get_shapes print f Tensor dimensions M N torch_dtype = cutlass_torch dtype cutlass BFloat x = torch randn M N device= cuda dtype=torch_dtype requires_grad=True dy = torch randn M N device= cuda dtype=torch_dtype benchmark_single_shape x dy setting=f shape M N RMSNormForward BenchmarkKernel __init__ script_args super __init__ script_args available_backends = eager compiled quack liger get_shapes - tuple tuple int + extra_shapes_for_norm get_memory_bytes args kwargs - int x w = args M N = x shape M N x dtype itemsize + N w dtype itemsize rms_norm_ref x w x_f = x float x_f torch rsqrt torch mean x_f square dim=- keepdim=True + e- w x dtype eager args kwargs=None - Any assert kwargs None x w = args lambda rms_norm_ref x w compiled args kwargs=None - Any assert kwargs None x w = args Mark batch size dynamic realistic workload torch _dynamo mark_dynamic x compiled_rms_norm = torch compile rms_norm_ref mode=self compile_mode fullgraph=True lambda compiled_rms_norm x w quack args kwargs=None - Any Note only supper weight float dtype quack rmsnorm _rmsnorm_fwd x w = args y = torch empty_like x quack_fwd _rmsnorm_fwd x w out=y bias=None rstd=None residual=None residual_out=None eps= e- y quack_fwd liger args kwargs - Any liger_kernel transformers rms_norm LigerRMSNorm x w = args M N = x shape liger_rmsnorm = LigerRMSNorm hidden_size=N eps= e- cuda liger_rmsnorm weight data copy_ w lambda liger_rmsnorm x benchmark M N get_shapes print f Tensor dimensions M N torch_dtype = cutlass_torch dtype cutlass BFloat x = torch randn M N device= cuda dtype=torch_dtype w = torch randn N device= cuda dtype=torch float benchmark_single_shape x w setting=f shape M N RMSNormBackward BenchmarkKernel __init__ script_args super __init__ script_args available_backends = eager compiled quack liger get_shapes - tuple tuple int TODO OOM h + extra_shapes_for_norm get_memory_bytes args kwargs - int x w dy = args x dy M N w N M N = x shape Read x w dy write dx dw M N x dtype itemsize + N w dtype itemsize rms_norm_ref x w x_f = x float x_f torch rsqrt torch mean x_f square dim=- keepdim=True + e- w x dtype eager args kwargs=None - Any assert kwargs None x w dy = args y = rms_norm_ref x w lambda torch autograd grad y x w grad_outputs=dy retain_graph=True compiled args kwargs=None - Any assert kwargs None x w dy = args y = torch compile rms_norm_ref mode=self compile_mode fullgraph=True x w lambda torch autograd grad y x w grad_outputs=dy retain_graph=True compute_rstd x eps torch rsqrt torch mean x float square dim=- keepdim=True + eps quack args kwargs=None - Any quack rmsnorm _get_sm_count _rmsnorm_bwd x w dy = args M N = x shape rstd = compute_rstd x eps= e- dx = torch empty_like x sm_count = _get_sm_count x size x device dw_partial = torch empty sm_count x size device=x device dtype=torch float quack_bwd _rmsnorm_bwd x w dy rstd dx dw_partial db_partial=None dresidual_out=None dresidual=None sm_count=sm_count dw = dw_partial sum dim= w dtype dx dw quack_bwd liger args kwargs=None - Any liger_kernel transformers rms_norm LigerRMSNorm x w dy = args M N = x shape liger_rmsnorm = LigerRMSNorm hidden_size=N eps= e- casting_mode= gemma cuda liger_rmsnorm weight data copy_ w y = liger_rmsnorm x lambda torch autograd grad y x liger_rmsnorm weight grad_outputs=dy retain_graph=True benchmark M N get_shapes print f Tensor dimensions M N torch_dtype = cutlass_torch dtype cutlass BFloat x = torch randn M N device= cuda dtype=torch_dtype requires_grad=True w = torch randn N device= cuda dtype=torch float requires_grad=True dy = torch randn M N device= cuda dtype=torch_dtype benchmark_single_shape x w dy setting=f shape M N LayerNormForward BenchmarkKernel __init__ script_args super __init__ script_args available_backends = eager compiled quack liger get_shapes - tuple tuple int OOM h + extra_shapes_for_norm get_memory_bytes args kwargs - int x w = args M N = x shape Read x M N w N write y M N M N x dtype itemsize + N w dtype itemsize layernorm_ref x torch Tensor w torch Tensor eps float = e- x_f = x float F layer_norm x_f w shape w None eps x dtype eager args kwargs=None - Any assert kwargs None x w = args lambda layernorm_ref x w compiled args kwargs=None - Any assert kwargs None x w = args Mark batch size dynamic realistic workload torch _dynamo mark_dynamic x compiled_layernorm = torch compile layernorm_ref mode=self compile_mode fullgraph=True lambda compiled_layernorm x w eps= e- quack args kwargs - Any Note quack layernorm does support bias quack layernorm layernorm x w = args lambda layernorm x w eps= e- liger args kwargs - Any liger_kernel transformers layer_norm LigerLayerNorm x w = args M N = x shape liger_layernorm = LigerLayerNorm hidden_size=N eps= e- cuda liger_layernorm weight data copy_ w liger_layernorm bias data copy_ torch zeros N device= cuda dtype=torch float lambda liger_layernorm x benchmark M N get_shapes print f Tensor dimensions M N torch_dtype = cutlass_torch dtype cutlass BFloat x = torch randn M N device= cuda dtype=torch_dtype w = torch randn N device= cuda dtype=torch float benchmark_single_shape x w setting=f shape M N LayerNormBackward BenchmarkKernel __init__ script_args super __init__ script_args available_backends = eager compiled liger get_shapes - tuple tuple int OOM + extra_shapes_for_norm get_memory_bytes args kwargs - int x w dy = args M N = x shape Read x M N w N dy M N write dx M N dw N M N x dtype itemsize + N w dtype itemsize + M N dy dtype itemsize layernorm_ref x torch Tensor w torch Tensor eps float = e- x_f = x float F layer_norm x_f w shape w None eps x dtype eager args kwargs=None - Any assert kwargs None x w dy = args y = layernorm_ref x w lambda torch autograd grad y x w grad_outputs=dy retain_graph=True compiled args kwargs=None - Any assert kwargs None x w dy = args compiled_layernorm = torch compile layernorm_ref mode=self compile_mode fullgraph=True y = compiled_layernorm x w lambda torch autograd grad y x w grad_outputs=dy retain_graph=True compute_mean_rstd x eps x = x float var mean = torch var_mean x dim=- keepdim=True correction= rstd = torch rsqrt var + eps mean rstd liger args kwargs - Any Call layer_norm_backward directly rather than calling liger_kernel transformers layer_norm LigerLayerNorm torch autograd grad The latter fashion saves mean rstd x dtype which can fail accuracy test We call layer_norm_backward fp mean rstd liger_kernel ops layer_norm layer_norm_backward x w dy = args eps = e- mean rstd = compute_mean_rstd x eps M N = x shape lambda layer_norm_backward dy x w None mean rstd benchmark M N get_shapes print f Tensor dimensions M N torch_dtype = cutlass_torch dtype cutlass BFloat x = torch randn M N device= cuda dtype=torch_dtype requires_grad=True w = torch randn N device= cuda dtype=torch float requires_grad=True dy = torch randn M N device= cuda dtype=torch_dtype benchmark_single_shape x w dy setting=f shape M N