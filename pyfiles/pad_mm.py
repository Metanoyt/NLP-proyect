functools itertools operator typing collections abc Sequence typing Any Callable torch torch _inductor runtime runtime_utils torch Tensor torch _dynamo utils counters dynamo_timed torch _inductor utils torch _inductor autoheuristic autoheuristic AHContext AutoHeuristic LocalFeedback torch _inductor autoheuristic autoheuristic_utils context_add_strides context_add_using_tf pad_mm_operations pad_mm_precondition torch _subclasses fake_tensor FakeTensor torch utils _mode_utils no_dispatch utils _triton has_triton pattern_matcher fwd_only gen_register_replacement joint_fwd_bwd Match ReplaceFn SearchFn aten = torch ops aten This flag only used testing purpose Changing True will ignore comparing do_bench times between original pattern padded one _skip_do_bench_times = False fetch_fake_tensors match Match kwarg_names Sequence str - list Tensor kwargs = match kwargs kwargs name meta val name kwarg_names unwrap_fake_args arg_names str - Callable Callable Any Callable Match Any decorator func Callable Any - Callable Match Any wrapper match Match - Any fake_tensors = fetch_fake_tensors match arg_names func fake_tensors wrapper decorator get_alignment_size x Tensor - int get_alignment_size_dtype x dtype get_alignment_size_dtype dtype torch dtype - int dtype == torch float dtype == torch half dtype == torch bfloat dtype == torch float dtype == torch float check_device Tensor b Tensor - bool is_cuda b is_cuda check_dtype Tensor b Tensor - bool is_floating_point b is_floating_point should_pad_common mat Tensor mat Tensor input Tensor &#124; None = None - bool It s fine we have symbolic shapes strides long they have hints Later we will make sure we only pad non-symbolic dimensions valid_shape_and_stride t Tensor &#124; None - bool t None True symbolic_cnt = x t size isinstance x int continue utils is_symbolic x pyrefly ignore missing-attribute x node has_hint False symbolic_cnt += False filter out cases where all dimensions symbolic symbolic_cnt == len t size False all pyrefly ignore missing-attribute isinstance x int utils is_symbolic x x node has_hint x t stride torch _inductor config shape_padding check_device mat mat check_dtype mat mat all valid_shape_and_stride t t mat mat input get_padded_length x int &#124; torch SymInt alignment_size int - int we don t pad x symbolic isinstance x torch SymInt alignment_size == x alignment_size == ignore dim can squeezed away x == int x alignment_size + alignment_size - x pad_dim x Tensor padded_length int dim int - Tensor padded_length == x pad = x new_zeros x shape dim padded_length x shape dim + torch cat x pad dim=dim addmm_pattern input Tensor mat Tensor mat Tensor beta float alpha float - Tensor aten addmm input mat mat beta=beta alpha=alpha should_pad_addmm match Match - bool mat mat input = fetch_fake_tensors match mat mat input should_pad_common mat mat input should_pad_bench match mat mat torch ops aten addmm input=input pad_addmm input Tensor &#124; None mat Tensor mat Tensor m_padded_length int k_padded_length int n_padded_length int beta float = alpha float = mat _pre_padded bool = False mat _pre_padded bool = False - Tensor paddings dim order reversed some reasons every dim we need specify left right padding mat _pre_padded mat = pad_mat mat m_padded_length=m_padded_length k_padded_length=k_padded_length mat _pre_padded mat = pad_mat mat k_padded_length=k_padded_length n_padded_length=n_padded_length add broadcasts so we only pad dimension = input None n_padded_length = input dim == input shape = input = pad_dim input n_padded_length input dim == input shape = input = pad_dim input n_padded_length m_padded_length = input dim == input shape = input = pad_dim input m_padded_length res = aten addmm input mat mat beta=beta alpha=alpha m_padded_length = res = res -m_padded_length n_padded_length = res = res -n_padded_length res addmm_replace input Tensor &#124; None mat Tensor mat Tensor beta float = alpha float = - Tensor k_padded_length = get_padded_length mat shape get_alignment_size mat n_padded_length = get_padded_length mat shape get_alignment_size mat m_padded_length = get_padded_length mat shape get_alignment_size mat pad_addmm input mat mat m_padded_length k_padded_length n_padded_length beta alpha is_mm_compute_bound M int K int N int dtype torch dtype - bool denominator = M K + N K + M N denominator == False arithmetic_intensity = M N K denominator we have experienced some large perf hits case even bandwidth bound regimes dtype torch bfloat K M K N torch cuda get_device_capability doesn t repro h s True Fails AMD try machine_balance = utils get_device_tflops dtype utils get_gpu_dram_gbps except Exception True dram_gbps might underestimating bandwidth because cache we estimate machine balance too low we might miss some speedups we estimate too high there will unnecessary compilation time increase TODO - finetune coefficient here As reference point Triton mm model assumes reads cache cache x faster than dram_gbps machine_balance = machine_balance arithmetic_intensity machine_balance functools cache get_pad_cache - torch _inductor codecache LocalCache torch _inductor codecache LocalCache get_cached_should_pad key str - bool get_pad_cache lookup key type ignore return-value set_cached_should_pad key str value bool - None get_pad_cache set_value key value=value get_cached_base_mm_benchmark_time key str - float get_pad_cache lookup key type ignore return-value set_cached_base_mm_benchmark_time key str value float - None get_pad_cache set_value key value=value should_pad_bench_key match Match mat Tensor mat Tensor op torch _ops OpOverloadPacket input Tensor &#124; None = None is_base_time_key bool = False - str tensor_key t Tensor - tuple torch Size tuple int torch dtype t shape t stride t dtype tf _key = None mat dtype = torch float torch backends cuda matmul allow_tf fmt_pad name str - str &#124; None is_base_time_key None f exclude_pad should_exclude_padding_time match name key = tensor_key mat tensor_key mat fmt_pad mat fmt_pad mat op input input None tensor_key input tf _key key = str key is_base_time_key key = f base mm time key key get_non_view_def node torch fx Node - torch fx Node node op operator getitem get_non_view_def node args type ignore arg-type node op == call_function isinstance node target torch _ops OpOverload utils is_view node target get_non_view_def node all_input_nodes node should_exclude_padding_time match Match arg_name str - bool node_def = get_non_view_def match kwargs arg_name constant padding converts tensors contiguous so even input tensor can planned layout transform free TODO - way pad preserve layout fetch_fake_tensors match arg_name is_contiguous False TODO - see issue https github com pytorch pytorch issues We would only able completely plan these out we only doing first dimension padding non-first we would still need copy because these outputs fixed dense cannot_plan_output = aten mm default aten convolution default aten convolution_backward default aten bmm default aten addmm default aten _scaled_dot_product_flash_attention default aten _scaled_dot_product_efficient_attention default node_def target cannot_plan_output False node_def target aten cat default len node_def all_input_nodes torch _inductor config max_pointwise_cat_inputs False optimistically assume we should able memory plan away all non inputs node_def op = placeholder should_pad key str ori_time float pad_time float - bool multiplier = Shape padding introduces additional memory ops Based microbenchmarks x represents reasonable tradeoff between performance improvement shape padding overhead additional memory ops TODO Build learned model which would better than heuristic shape_padding_multiplier torch _inductor config post_grad_fusion_options multiplier = torch _inductor config post_grad_fusion_options shape_padding_multiplier get value counters inductor shape_padding_multiplier += should_pad = _skip_do_bench_times ori_time pad_time multiplier set_cached_should_pad key should_pad should_pad should_pad_mm_bf dtype torch dtype M int N int K int - bool always force pad mm bf when following satisfied avoid perf regression large_k_threshold_to_pad = torch _inductor config post_grad_fusion_options pad_aten_mm_pass get k_threshold_to_pad dtype torch bfloat K M K N N == K = large_k_threshold_to_pad torch cuda get_device_capability doesn t repro h s True False should_pad_bench args Any kwargs Any - bool dynamo_timed pad_mm_benchmark log_pt _compile_event=False dynamo_compile_column_us= compile_time_autotune_time_us _should_pad_bench args kwargs get_do_bench - Callable Callable Any float dynamo_timed pad_mm_benchmark_get_do_bench functools partial pyrefly ignore bad-argument-type torch _inductor runtime benchmarking benchmarker benchmark_gpu warmup= _should_pad_bench match Match mat Tensor mat Tensor op torch _ops OpOverloadPacket input Tensor &#124; None = None - bool do_bench = get_do_bench m_padded_length = n_padded_length = no_dispatch op torch ops aten mm op torch ops aten addmm m = mat shape k = mat shape n = mat shape k_padded_length = get_padded_length k get_alignment_size mat n_padded_length = get_padded_length n get_alignment_size mat m_padded_length = get_padded_length m get_alignment_size mat op torch ops aten bmm m = mat shape k = mat shape n = mat shape k_padded_length = get_padded_length k get_alignment_size mat m_padded_length = get_padded_length m get_alignment_size mat n_padded_length = get_padded_length n get_alignment_size mat False m_padded_length == k_padded_length == n_padded_length == False realize_symbols ds torch Size &#124; tuple torch SymInt - list int d isinstance d int d node hint d ds any dim == dim itertools chain realize_symbols mat shape realize_symbols mat shape False torch _inductor config force_shape_pad True torch _inductor config deterministic In deterministic mode don t benchmark pad-mm assumes no padding Check deterministic mode after force_shape_pad so unit test relying force_shape_pad should still pass False pad_aten_mm_pass torch _inductor config post_grad_fusion_options should_pad_mm_bf mat dtype m n k True has_triton False is_mm_compute_bound m k n mat dtype False We don t want look up cache cases trivially false since does file io key = should_pad_bench_key match mat mat op input cached_pad = get_cached_should_pad key cached_pad None cached_pad realize_tensor t isinstance t FakeTensor size_hints = realize_symbols t size pyrefly ignore bad-argument-type stride_hint = realize_symbols t stride real_size = sum d - s d s zip size_hints stride_hint + real_t = torch randn real_size dtype=t dtype device=t device torch as_strided real_t size_hints stride_hint torch randn_like t mat = realize_tensor mat mat = realize_tensor mat since we key whether inputs can memory planned set cache original time which unaffected whether input can planned ori_time_key = should_pad_bench_key match mat mat op input is_base_time_key=True ori_time = get_cached_base_mm_benchmark_time ori_time_key ori_time None op torch ops aten addmm input None realize bias addmm input = realize_tensor input mat _pad = mat mat _pad = mat is_bmm = op torch ops aten bmm mat _pre_padded = should_exclude_padding_time match mat fns = mat _pre_padded m_padded_length k_padded_length mat _pad = pad_mat mat _pad m_padded_length=m_padded_length k_padded_length=k_padded_length is_bmm=is_bmm write_pad is_bmm mat _pad -m_padded_length -k_padded_length fill_ mat _pad -m_padded_length -k_padded_length fill_ fns append write_pad mat _pre_padded = should_exclude_padding_time match mat mat _pre_padded k_padded_length n_padded_length mat _pad = pad_mat mat _pad k_padded_length=k_padded_length n_padded_length=n_padded_length is_bmm=is_bmm write_pad is_bmm mat _pad -k_padded_length -n_padded_length fill_ mat _pad -k_padded_length -n_padded_length fill_ fns append write_pad op torch ops aten addmm input_pad = None input None input is_cuda input_pad = torch randn_like input fns append lambda pad_addmm input_pad mat _pad mat _pad m_padded_length k_padded_length n_padded_length mat _pre_padded=mat _pre_padded mat _pre_padded=mat _pre_padded op torch ops aten mm fns append lambda pad_mm mat _pad mat _pad m_padded_length k_padded_length n_padded_length mat _pre_padded=mat _pre_padded mat _pre_padded=mat _pre_padded fns append lambda pad_bmm mat _pad mat _pad m_padded_length k_padded_length n_padded_length mat _pre_padded=mat _pre_padded mat _pre_padded=mat _pre_padded orig_bench_fn op torch ops aten bmm op torch ops aten mm op mat mat op input mat mat pad_bench_fn fn fns fn torch _inductor config run_autoheuristic pad_mm op torch ops aten mm ah_should_pad = run_autoheuristic mat mat orig_bench_fn pad_bench_fn m_padded_length k_padded_length n_padded_length do_bench mat _pre_padded mat _pre_padded ori_time ori_time_key key ah_should_pad None ah_should_pad ori_time None ori_time = do_bench orig_bench_fn set_cached_base_mm_benchmark_time ori_time_key ori_time pad_time = do_bench pad_bench_fn counters inductor pad_mm_bench += should_pad key ori_time pad_time get_context mat Tensor mat Tensor mat _pre_padded bool mat _pre_padded bool m_padded_length int k_padded_length int n_padded_length int - AHContext context = AHContext context add_feature m mat shape context add_feature k mat shape context add_feature n mat shape context_add_strides context mat mat stride context_add_strides context mat mat stride context add_feature m_padded_length m_padded_length context add_feature k_padded_length k_padded_length context add_feature n_padded_length n_padded_length context add_feature mat _align_size get_alignment_size mat context add_feature mat _align_size get_alignment_size mat context add_feature mat _dtype mat dtype is_categorical=True context add_feature mat _dtype mat dtype is_categorical=True context add_feature prepadded_mat mat _pre_padded is_categorical=True context add_feature prepadded_mat mat _pre_padded is_categorical=True context_add_using_tf context mat dtype context run_autoheuristic mat Tensor mat Tensor orig_bench_fn Callable None pad_bench_fn Callable None m_padded_length int k_padded_length int n_padded_length int do_bench Callable Callable Any float mat _pre_padded bool mat _pre_padded bool ori_time float ori_time_key str key str - bool &#124; None feedback_fn choice str - float &#124; None choice == orig_choice do_bench orig_bench_fn choice == pad_choice do_bench pad_bench_fn None fallback - str autotune orig_choice = orig pad_choice = pad choices = orig_choice pad_choice feedback = LocalFeedback feedback_fn type ignore arg-type context = get_context mat mat mat _pre_padded mat _pre_padded m_padded_length k_padded_length n_padded_length name = pad_mm autoheuristic = AutoHeuristic fallback=fallback choices=choices feedback=feedback context=context name=name augment_context=pad_mm_operations precondition=pad_mm_precondition choice = autoheuristic get_choice choice should_pad = orig_choice False pad_choice True autotune None ah_should_pad = choice should_pad get choice torch _inductor config collect_autoheuristic name ah_ori_time = autoheuristic get_collected_feedback orig_choice ah_pad_time = autoheuristic get_collected_feedback pad_choice precondition satisfied autoheuristic does collect data ah_ori_time None ah_pad_time None ori_time None set_cached_base_mm_benchmark_time ori_time_key ah_ori_time should_pad key ah_ori_time ah_pad_time ah_should_pad None set_cached_should_pad key ah_should_pad ah_should_pad mm_pattern mat Tensor mat Tensor - Tensor aten mm mat mat should_pad_mm match Match - bool mat mat = fetch_fake_tensors match mat mat should_pad_common mat mat should_pad_bench match mat mat torch ops aten mm pad_mat mat Tensor m_padded_length int k_padded_length int is_bmm bool = False - Tensor k_padded_length = m_padded_length = dim order reversed constant_pad_nd every dim we specify right left padding pad_arg = k_padded_length m_padded_length is_bmm pad_arg extend aten constant_pad_nd mat pad_arg mat pad_mat mat Tensor k_padded_length int n_padded_length int is_bmm bool = False - Tensor k_padded_length = n_padded_length = dim order reversed constant_pad_nd every dim we specify right left padding pad_arg = n_padded_length k_padded_length is_bmm pad_arg extend aten constant_pad_nd mat pad_arg mat pad_mm mat Tensor mat Tensor m_padded_length int k_padded_length int n_padded_length int mat _pre_padded bool = False mat _pre_padded bool = False - Tensor mat _pre_padded mat = pad_mat mat m_padded_length=m_padded_length k_padded_length=k_padded_length mat _pre_padded mat = pad_mat mat k_padded_length=k_padded_length n_padded_length=n_padded_length res = aten mm mat mat m_padded_length = res = res -m_padded_length n_padded_length = res = res -n_padded_length res mm_replace mat Tensor mat Tensor - Tensor k_padded_length = get_padded_length mat shape get_alignment_size mat m_padded_length = get_padded_length mat shape get_alignment_size mat n_padded_length = get_padded_length mat shape get_alignment_size mat pad_mm mat mat m_padded_length k_padded_length n_padded_length bmm_pattern mat Tensor mat Tensor - Tensor aten bmm mat mat should_pad_bmm match Match - bool mat mat = fetch_fake_tensors match mat mat should_pad_common mat mat should_pad_bench match mat mat torch ops aten bmm pad_bmm mat Tensor mat Tensor m_padded_length int k_padded_length int n_padded_length int mat _pre_padded bool = False mat _pre_padded bool = False - Tensor mat _pre_padded mat = pad_mat mat m_padded_length=m_padded_length k_padded_length=k_padded_length is_bmm=True mat _pre_padded mat = pad_mat mat k_padded_length=k_padded_length n_padded_length=n_padded_length is_bmm=True res = aten bmm mat mat m_padded_length = res = res -m_padded_length n_padded_length = res = res -n_padded_length res bmm_replace mat Tensor mat Tensor - Tensor k_padded_length = get_padded_length mat shape get_alignment_size mat n_padded_length = get_padded_length mat shape get_alignment_size mat m_padded_length = get_padded_length mat shape get_alignment_size mat pad_bmm mat mat m_padded_length k_padded_length n_padded_length functools cache _pad_mm_init - None joint_graph patterns torch cuda is_available workaround https github com pytorch pytorch issues device = cuda device = cpu sizes values dont actually matter initial trace once we get possible match we re-trace actual values verify match still holds dim = functools partial torch empty device=device requires_grad=True dim b = functools partial torch empty device=device requires_grad=True dim = functools partial torch empty device=device requires_grad=True dim b = functools partial torch empty device=device requires_grad=True dim = functools partial torch empty device=device requires_grad=True workaround https github com pytorch pytorch issues magic value lets us recover lost input arg relationship rep = beta alpha pattern replacement args workaround extra_check typing cast SearchFn mm_pattern typing cast ReplaceFn mm_replace dim dim b should_pad_mm typing cast SearchFn bmm_pattern typing cast ReplaceFn bmm_replace dim dim b should_pad_bmm typing cast SearchFn addmm_pattern typing cast ReplaceFn addmm_replace dim dim dim b rep should_pad_addmm assert isinstance workaround dict mypy unable infer type properly name = pattern __name__ gen_register_replacement f name _training pattern replacement args pyrefly ignore bad-argument-type joint_fwd_bwd pyrefly ignore bad-argument-type patterns extra_check=extra_check scalar_workaround=workaround gen_register_replacement f name _inference pattern replacement args pyrefly ignore bad-argument-type fwd_only pyrefly ignore bad-argument-type patterns extra_check=extra_check scalar_workaround=workaround