mypy allow-untyped-defs functools logging operator sys typing Any Optional TYPE_CHECKING Import sympy ShapeEnv during TYPE_CHECKING since importing sympy slow TYPE_CHECKING sympy torch fx experimental symbolic_shapes ShapeEnv ShapeEnv = Any torch torch utils _pytree pytree torch fx torch _subclasses meta_utils is_sparse_any torch fx _compatibility compatibility torch fx _utils lazy_format_graph_code torch fx experimental proxy_tensor py_sym_types torch fx experimental sym_node SymNode torch fx graph_module GraphModule __all__ = insert_deferred_runtime_asserts log = logging getLogger __name__ graph_code_log = torch _logging getArtifactLogger __name__ graph_code_verbose _get_example_value node fx Node - Optional str Get example value key node since dynamo uses example_value while non-strict export uses val example_value node meta node meta example_value val node meta node meta val None _get_sym_val node fx Node - Optional sympy Expr val = _get_example_value node isinstance val py_sym_types val node expr None compatibility is_backward_compatible=True insert_deferred_runtime_asserts gm GraphModule shape_env ShapeEnv name str export bool = False - None During tracing we may have discovered some data-dependent values had runtime assert them e g torch empty x item induces runtime x item = These asserts can happen unpredictably during fake tensor propagation so we cannot conveniently insert them into FX graph when they occur Instead we accumulate them ShapeEnv pass insert them into graph proper tests This pass also deduplicates size-related computation CSE-ing ops produce symbolic values involved runtime asserts Additionally shape calls size stride storage_offset turned into compute input sizes possible allowing intermediate tensors freed earlier For example here dynamo will DCE cat repeat calls z = torch cat x x dim= s w = z repeat y shape s s _w = w shape something _w w turns into - _w = s _w = _w s where s s either SymInt graph inputs result added size calls Redundant torch _check torch ops aten _assert_scalar default calls assert same expression redundant constrain_range calls also deduplicated Additionally because single-symbol bound checks e g u = u = accumulate information ShapeEnv ShapeEnv contains min max bounds each symbol we delete all previous calls adding bound checks end pass Import sympy locally sympy torch _export passes _node_metadata_hook _set_node_metadata_hook torch fx experimental symbolic_shapes _get_placeholder_expr _has_uninterpretable_sympy_function CallMethodKey cast_symbool_to_symint_guardless ConvertIntKey DivideByKey free_symbols InnerTensorKey resolve_unbacked_bindings torch utils _sympy numbers int_oo torch utils _sympy reference OptimizedPythonReferenceAnalysis PythonReferenceAnalysis torch utils _sympy value_ranges ValueRanges TODO Request simplification runtime asserts before emitting them ras_by_symbol = shape_env deferred_runtime_asserts copy graph = gm graph tracer = fx proxy GraphAppendingTracer graph graph_code_log debug s lazy_format_graph_code f pre insert_deferred_runtime_asserts name gm colored=True We going mutate dict expr_to_proxy dict sympy Expr fx Proxy = placeholders = set first_non_placeholder = None node graph nodes node op = placeholder first_non_placeholder = node break placeholders add node _is_intermediate_tensor_sym_call node fx Node - bool If size stride storage offset call intermediate tensor we can try compute value input shapes instead val = _get_sym_val node None isinstance val sympy Number holds back reifying anything torch utils _sympy functions py s unsupported _has_uninterpretable_sympy_function val any isinstance arg fx Node isinstance _get_example_value arg torch Tensor torch Size arg op = placeholder arg node args Figure out what key use val example_value val_key = val node graph nodes example_value node meta val_key = example_value break val node meta break _node_metadata_hook node torch fx Node stack_trace Optional str = None nn_module_stack Optional dict str Any = None - None fake_args = pytree tree_map lambda arg _get_example_value arg isinstance arg torch fx Node arg node args try target = node target node op == call_method assert isinstance node target str target = getattr fake_args node target fake_args = fake_args node meta val_key = target fake_args type ignore operator except NotImplementedError This can happen when attempting reify symbol unsupported call_function node e g NestedTensors + sym_size int via match_symbol This seems fine node gets CSE d deleted later favor SymInt graph input pass stack_trace None node meta stack_trace = stack_trace nn_module_stack None node meta nn_module_stack = nn_module_stack Track asserts checks we ve added added_asserts set sympy Expr = set constrained_unbacked_symbols set sympy Symbol = set Analysis = PythonReferenceAnalysis export OptimizedPythonReferenceAnalysis _sympy_interp expr_to_proxy expr sympy_interp hash consing sympy Integer Number Symbol sympy logic boolalg BooleanAtom torch utils _sympy interp _run_sympy_handler sympy_interp hash cons expr expr_to_proxy expr_to_proxy expr base cases don t cache isinstance expr Integer Number Symbol BooleanAtom sympy_interp Analysis expr_to_proxy expr hash cons arguments run expr handler expr_to_proxy expr = _run_sympy_handler Analysis _sympy_interp expr_to_proxy arg arg expr args expr expr_to_proxy expr _is_bound_expr_for_symbol expr sympy Expr - bool This probably unnecessary since torch _check calls single-symbol bounds like u = = u accumulate range info ShapeEnv we designate these calls redundant instead add runtime asserts end pass min max bounds non-trivial len expr args = expr func sympy LessThan sympy GreaterThan False lhs rhs = expr args isinstance lhs sympy Symbol isinstance rhs sympy Number isinstance rhs sympy Symbol isinstance lhs sympy Number add_runtime_asserts ras ra ras redundant ra expr added_asserts we ve already added constrain_range call symbol then single-symbol bound asserts like u = u = redundant len ra expr free_symbols == next iter ra expr free_symbols constrained_unbacked_symbols _is_bound_expr_for_symbol ra expr don t try reify sympy functions we can t turn into FX nodes _has_uninterpretable_sympy_function ra expr continue log debug inserting runtime assert s ra expr Need process ALL free symbols just unbacked ones fvs = free_symbols ra expr missing = fvs - expr_to_proxy keys missing i = min missing key=str TODO Remove relaxing assert unbacked_symint https github com pytorch pytorch issues assert shape_env is_unbacked_symint i i ras_by_symbol setdefault i append ra Convert sympy expression into sequence FX nodes _set_node_metadata_hook gm _node_metadata_hook res = _sympy_interp expr_to_proxy ra expr node graph call_function torch ops aten _assert_scalar default TODO use ra msg here s pretty useless right now res f Runtime assertion failed expression ra expr node res added_asserts add ra expr nodes = list graph nodes i node enumerate nodes - Placeholders can match symbols when we destructure them size we have make sure we insert nodes after all placeholders graph inserting_before nodes i + node placeholders first_non_placeholder Unfortunately logic still must remain because manual make_fx calls may explicitly bind all symbolic ints arguments function so we must infer other arguments node placeholders example_value = _get_example_value node None match_symbol symint cb isinstance symint torch SymInt isinstance symint node SymNode isinstance s = _get_placeholder_expr symint node sympy Symbol s expr_to_proxy _set_node_metadata_hook gm _node_metadata_hook expr_to_proxy s = fx Proxy cb tracer=tracer log debug expr_to_proxy s = s s expr_to_proxy s match_symbol example_value lambda node isinstance t = example_value torch Tensor i s enumerate t size match_symbol s lambda graph call_function torch ops aten sym_size int node i is_sparse_any t i s enumerate t stride match_symbol s lambda graph call_function torch ops aten sym_stride int node i match_symbol t storage_offset lambda graph call_function torch ops aten sym_storage_offset default node Handle asserts aren t associated any symbol This doesn t really have loop will only run once just needs happen right after placeholders insert after placeholders added sym nodes before non-placeholders node == first_non_placeholder add_runtime_asserts ras_by_symbol pop None type ignore call-overload deduplicate asserts already present graph remove trivial asserts node target torch _check torch ops aten _assert_scalar default cond = node args node args node kwargs get cond cond == True noqa E assert_expr = _get_sym_val cond expr_to_proxy assert_expr added_asserts arg = cond gm graph erase_node node isinstance arg fx Node arg users gm graph erase_node arg added_asserts add assert_expr type ignore arg-type hash cons replace function calls torch SymInts direct references FX nodes built up reify sympy expression node op = placeholder sym_expr = _get_sym_val node None guards against deleting calls like item produce new untracked symbols has_new_untracked_symbols pyrefly ignore missing-attribute symbol sym_expr free_symbols symbol expr_to_proxy True False guards against deleting calls produce unbacked bindings we haven t yet seen case looking sym_expr free_symbols might enough example value has hint backed produces unbacked symbol In case keep node alive resolved_unbacked_bindings = resolve_unbacked_bindings shape_env node meta get unbacked_bindings assert resolved_unbacked_bindings None has_new_unbacked_bindings pyrefly ignore missing-attribute key resolved_unbacked_bindings keys key expr_to_proxy True False maybe re-reify expression replace current node sym_expr expr_to_proxy example value redundant _is_intermediate_tensor_sym_call node shape call intermediate tensor turn into computation input shapes has_new_untracked_symbols has_new_unbacked_bindings _is_intermediate_tensor_sym_call node reify input shapes _set_node_metadata_hook gm functools partial _node_metadata_hook stack_trace=node meta get stack_trace nn_module_stack=node meta get nn_module_stack expr_to_proxy sym_expr = _sympy_interp expr_to_proxy sym_expr type ignore arg-type won t try DCE-ing tensor compute here hash_node = expr_to_proxy sym_expr node type ignore arg-type node replace_all_uses_with hash_node gm graph erase_node node log debug CSE node s - s expr s node hash_node sym_expr store node hash cons don t delete replace sym_expr expr_to_proxy isinstance sym_expr sympy Number sympy logic boolalg BooleanAtom don t hash cons primitives expr_to_proxy sym_expr = fx Proxy node tracer=tracer type ignore arg-type We add sym_constrain_range calls symbols later any case they re size-like range-constrained so calls before redundant node target torch ops aten sym_constrain_range default torch ops aten sym_constrain_range_for_size default gm graph erase_node node defs = AOTAutograd will create new symbols unbacked_bindings keys which PropagateSymInts will set equivalent refinement calls we perform pass may struggle associating two More concretely when re-exporting tracing constraining only new symbol may communicate enough information about old symbol when we re-export raising errors data-dependent guards Call resolve_unbacked_bindings get original symbol present otherwise we take unbacked_bindings = resolve_unbacked_bindings shape_env node meta get unbacked_bindings s keypath unbacked_bindings items defs append s TODO some CSE when generating these nodes can probably help reduce graph size improve compile time go node keypath keypath == node len keypath = isinstance keypath CallMethodKey isinstance keypath pytree SequenceKey keypath name == size go graph call_function torch ops aten sym_size int node keypath idx keypath keypath name == stride go graph call_function torch ops aten sym_stride int node keypath idx keypath go graph call_method keypath name node keypath idx keypath isinstance keypath CallMethodKey keypath name == storage_offset go graph call_function torch ops aten sym_storage_offset default node keypath go graph call_method keypath name node keypath isinstance keypath pytree SequenceKey go graph call_function operator getitem node keypath idx keypath isinstance keypath ConvertIntKey go graph call_function cast_symbool_to_symint_guardless node keypath isinstance keypath DivideByKey TODO need assert divisibility go graph call_function operator floordiv node keypath divisor keypath isinstance keypath InnerTensorKey go graph call_function getattr node keypath inner_name keypath raise AssertionError f unrecognized keypath keypath s expr_to_proxy _set_node_metadata_hook gm _node_metadata_hook expr_to_proxy s = fx Proxy go node keypath tracer=tracer log debug expr_to_proxy s = s s expr_to_proxy s i defs ras = ras_by_symbol pop i Before we perform any asserts first apply range refinement This important because we going retrace graph we typically we send graph AOTAutograd we need make sure we apply range refinement ala _check_is_size first BEFORE we run any asserts Otherwise we may decide perform substitutions based asserts which we then can t back out because value ranges can only applied asserts A perhaps better long term plan avoid order dependence making possible refine ranges arbitrary expressions just symbols But so easy make use information see https twitter com ezyang status We actually made attempt https github com pytorch pytorch pull which didn t work Another ideas how do - Have bound_sympy source truth ranges any expression - Cache intermediate results every subexpression bound_sympy - This cache should possible edit refine ranges One issue proposal we have bound x we going able apply x Similarly we may have bounds equivalent expression we applying because s perfect match e g x y vs y x The first issue we already have s impossible solve general so any implementation best effort basis should do The second issue preexisting one It can mitigated normalization algorithm In general may also best effort basis since our grammar terribly difficult chances we could even fully normalize SymPy expressions who knows i constrained_unbacked_symbols continue constrain symbol just once i shape_env size_like export graph call_function torch ops aten sym_constrain_range_for_size default expr_to_proxy i node graph call_function torch _check_is_size expr_to_proxy i node vr = shape_env var_to_range i vr is_int vr upper == sys maxsize - treat upper bound == sys maxsize - int symbols +oo avoid redundant runtime assert vr = ValueRanges vr lower int_oo shape_env _default_unspecified_value_range issubset vr The runtime range constrained so add runtime assert also explicitly refine range refinement should necessary once runtime asserts cause refinement s NYI convert s s int_oo -int_oo None try int s except TypeError None expr_to_proxy i node target cast_symbool_to_symint_guardless TODO pianpwk calling sym_constrain_range_for_size adding bound asserts raises AOTAutograd errors cast_symbool_to_symint_guardless _set_node_metadata_hook gm functools partial _node_metadata_hook stack_trace=node meta get stack_trace nn_module_stack=node meta get nn_module_stack min_val = convert vr lower None ge = _sympy_interp expr_to_proxy i = min_val node graph call_function torch ops aten _assert_scalar default ge f Runtime assertion failed expression i = min_val node ge added_asserts add i = min_val max_val = convert vr upper None le = _sympy_interp expr_to_proxy i = max_val node graph call_function torch ops aten _assert_scalar default le f Runtime assertion failed expression i = max_val node le added_asserts add i = max_val constrained_unbacked_symbols add i add_runtime_asserts ras delete unused reified symbols expr proxy expr_to_proxy items isinstance expr sympy Symbol proxy node op = placeholder keep placeholders intact proxy node users log debug deleting unused reified symbol s expr gm graph erase_node proxy node