mypy allow-untyped-defs typing Optional torch torch optim _functional F torch Tensor torch distributed optim _deprecation_warning _scripted_functional_optimizer_deprecation_warning __all__ list str = Define TorchScript compatible Functional Adagrad Optimizer where we use these optimizer functional way Instead using ` param grad ` when updating parameters we explicitly let user pass gradients ` step ` function so we could separate gradients parameters allow multithreaded trainer update parameters without data traces accumulating same grad NOTE This should only used distributed optimizer internals meant expose user torch jit script _FunctionalAdagrad __init__ params list Tensor lr float = e- lr_decay float = weight_decay float = initial_accumulator_value float = warmup_lr_multiplier float = warmup_num_iters float = eps float = e- coalesce_grad bool = True foreach bool = False fused bool = False maximize bool = False _allow_empty_param_list bool = False _scripted_functional_optimizer_deprecation_warning stacklevel= defaults = lr lr lr_decay lr_decay eps eps weight_decay weight_decay initial_accumulator_value initial_accumulator_value warmup_lr_multiplier warmup_lr_multiplier warmup_num_iters warmup_num_iters coalesce_grad = coalesce_grad foreach = foreach fused = fused maximize = maximize state = torch jit annotate dict torch Tensor dict str torch Tensor len params == _allow_empty_param_list raise ValueError optimizer got empty parameter list NOTE we only have one param_group don t allow user add additional param group s common use case param_group = params params TODO no union any types TorchScript make step scalar tensor instead This also needed we want share_memory step across processes p param_group params state p = sum torch full_like p data initial_accumulator_value step torch tensor step gradients list Optional Tensor params = param_group params params_with_grad = grads = state_sums = state_steps list Tensor = len params = len gradients raise ValueError gradients passed does equal size parameters + f Params length len params + f Gradients length len gradients has_sparse_grad has_complex = False False param gradient zip param_group params gradients gradient None has_sparse_grad &#124; = gradient is_sparse has_complex &#124; = torch is_complex param params_with_grad append param grads append gradient state = state param state_sums append state sum state_steps append state step torch no_grad F adagrad params grads state_sums state_steps lr=self defaults lr weight_decay=self defaults weight_decay lr_decay=self defaults lr_decay eps=self defaults eps has_sparse_grad=has_sparse_grad foreach=self foreach maximize=self maximize has_complex=has_complex fused=self fused grad_scale=None found_inf=None