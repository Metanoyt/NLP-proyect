Owner s module onnx Test support onnxscript PyTorch-ONNX converter io onnx onnxscript onnxscript onnx_types FLOAT torch torch onnx _internal torchscript_exporter jit_utils torch testing _internal common_utils TestONNXScriptExport common_utils TestCase opset version local function supported after opset onnx-script requires users determine opset local function opset_version = test_onnxscript_registration_with_multiple_models onnxscript onnx_opset opset op Register Selu onnxscript function custom Op custom_opset = onnxscript values Opset domain= onnx-script version= onnxscript script custom_opset Selu X default value supported onnxscript alpha = auto wrapped Constants gamma = alphaX = op CastLike alpha X gammaX = op CastLike gamma X neg = gammaX alphaX op Exp X - alphaX pos = gammaX X zero = op CastLike X op Where X = zero neg pos custom_selu g jit_utils GraphContext X g onnxscript_op Selu X setType X type torch onnx register_custom_op_symbolic symbolic_name= aten selu symbolic_fn=custom_selu opset_version=self opset_version Register layer_norm onnxscript function custom Op onnxscript script custom_opset layer_norm X axes list int weight FLOAT bias FLOAT eps float mean = op ReduceMean X axes=axes D = X - mean op Sub X mean DD = D D op Mul D D var = op ReduceMean DD axes=axes vareps = var + eps op Add var eps stddev = op Sqrt vareps invstddev = op Reciprocal stddev normalized = D invstddev op Mul D invstddev normalizedw = op CastLike normalized weight Type issue missing Op normalizedscaled = normalizedw weight op Mul normalized weight normalizedscaled + bias torch onnx symbolic_helper parse_args v v v f none custom_layer_norm g input normalized_shape weight bias eps cudnn_enable comprehension supported onnxscript axes = -i i range len normalized_shape - g onnxscript_op layer_norm input weight bias axes_i=axes eps_f=eps setType input type torch onnx register_custom_op_symbolic symbolic_name= aten layer_norm symbolic_fn=custom_layer_norm opset_version=self opset_version export two models x = torch randn requires_grad=True model_selu = torch nn SELU selu_onnx = io BytesIO torch onnx export model_selu x selu_onnx opset_version=self opset_version dynamo=False N C = y = torch randn N C model_layer_norm = torch nn LayerNorm C layer_norm_onnx = io BytesIO torch onnx export model_layer_norm y layer_norm_onnx opset_version=self opset_version dynamo=False test models selu_proto = onnx load io BytesIO selu_onnx getvalue layer_norm_proto = onnx load io BytesIO layer_norm_onnx getvalue assertEqual len selu_proto functions assertEqual len layer_norm_proto functions assertEqual selu_proto functions name Selu assertEqual layer_norm_proto functions name layer_norm test_loop_registration Control flow tested _find_onnxscript_op function torch onnx utils py which has recursive logic go through every nodes subgraph model proto NestedLoopsModel torch jit ScriptModule __init__ - None super __init__ selu = torch nn SELU torch jit script_method forward x y = x i range x size i == y = selu x y += i y model = NestedLoopsModel inputs = torch zeros onnxscript onnx_opset opset op custom_opset = onnxscript values Opset domain= onnx-script version= onnxscript script custom_opset Selu X alpha = gamma = alphaX = op CastLike alpha X gammaX = op CastLike gamma X neg = gammaX alphaX op Exp X - alphaX pos = gammaX X zero = op CastLike X op Where X = zero neg pos custom_selu g X domain Op should aligned onnx-script setType API required custom Op support torchscript shape type inference print custom_selu used g onnxscript_op Selu X setType X type torch onnx register_custom_op_symbolic symbolic_name= aten selu symbolic_fn=custom_selu opset_version= saved_model = io BytesIO torch onnx export torch jit script model inputs f=saved_model opset_version= dynamo=False loop_selu_proto = onnx load io BytesIO saved_model getvalue assertEqual len loop_selu_proto functions __name__ == __main__ raise RuntimeError This test currently used should enabled discover_tests py required