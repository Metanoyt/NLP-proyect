mypy allow-untyped-defs torch _calculate_meta_reordering_scatter_offsets m meta_ncols meta_dtype device This PyTorch implementation main part reorder_meta function tools util include cutlass util host_reorder h file CUTLASS source tree Furthermore CUTLASS template sparse GEMM decides upon layout matrix moment sparse GEMM executed tensor cores layout described ColumnMajorInterleaved data structure include cutlass layout matrix h CUTLASS source tree The reordering meta matrix into meta_reordered matrix calculated according these segments CUTLASS code re-implemented here Note calculation produces offsets scattering metadata matrix elements into reordered metadata matrix elements equivalently gathering reordered metadata matrix element back into metadata matrix elements dst_rows = torch arange m device=device None repeat meta_ncols dst_cols = torch arange meta_ncols device=device repeat m Reorder rows then swizzle x blocks group = meta_dtype itemsize == interweave = meta_dtype itemsize == dst_rows = dst_rows group group + dst_rows interweave + dst_rows group topright = dst_rows == dst_cols == torch int bottomleft = dst_rows == dst_cols == torch int dst_rows += topright - bottomleft dst_cols -= topright - bottomleft Assumed meta tensor stored CUTLASS InterleavedColumnMajor layout reverse engineered corresponding code store values into tensor interleave = cols_maj = dst_cols interleave cols_min = dst_cols interleave cols_maj m interleave + dst_rows interleave + cols_min view - sparse_semi_structured_from_dense_cutlass dense This function converts dense matrix into sparse semi-structured representation producing compressed matrix layout used CUTLASS backend corresponding metadata matrix dense dim = raise RuntimeError f Expected -dimensional dense tensor got dense dim -dimensional tensor m k = dense shape device = dense device meta_dtype = torch int dense dtype == torch int meta_dtype = torch int dense dtype torch half torch bfloat torch float meta_dtype = torch int raise RuntimeError f Invalid datatype dense dtype dense matrix quadbits_per_meta_elem = meta_dtype itemsize quadbits_per_meta_elem raise RuntimeError Invalid number elements per meta element calculated meta_dtype == torch int m = raise RuntimeError f Number rows dense matrix m must divisible m = raise RuntimeError f Number rows dense matrix m must divisible k quadbits_per_meta_elem = raise RuntimeError f Number columns dense matrix k must divisible quadbits_per_meta_elem dense dtype = torch float ksparse = dense_ = dense view - k ksparse ksparse m m _m m = dense_ = unbind - ksparse = dense_ = dense view - k ksparse ksparse m _m = m m = dense_ = unbind - meta_ncols = k ksparse quadbits_per_meta_elem Encoding quadruples True False values follows True True False False - b True False True False - b False True True False - b True False False True - b False True False True - b False False True True - b Thus lower two bits encoding index True value lowest index quadruple higher two bits encoding index other True value quadruple In case there less than two True values than False value values some index indices considered True encoding In case there more than two True values then excess True value s some indices considered False encoding The exact encodings used these cases follows False False False False - b False False False True - b False False True False - b False True False False - b False True True True - b True False False False - b True False True True - b True True False True - b True True True False - b True True True True - b These particular encodings chosen help Espresso logic minimizer software purpose minimization corresponding Boolean functions translate non-zero flags into encoding bits Note also possible choices first last these encodings limited only b b order produce valid encodings sparsity case expr = m m expr = ~m m expr = ~m ~m bit = expr bit = expr bit = expr &#124; expr &#124; m bit = expr &#124; ~m idxs = bit &#124; bit torch int idxs = bit &#124; bit torch int dense dtype = torch float sparse = dense_ gather - idxs unsqueeze - type ignore possibly-undefined pyrefly ignore unbound-name sparse = dense_ gather - idxs unsqueeze - sparse = torch stack sparse sparse dim=- view m k sparse = dense_ gather - idxs unsqueeze - view m k type ignore possibly-undefined meta_ = idxs &#124; idxs meta_n = meta_ view - meta_ncols quadbits_per_meta_elem meta_dtype quadbits_per_meta_elem == meta = meta_n &#124; meta_n &#124; meta_n &#124; meta_n quadbits_per_meta_elem == meta = meta_n &#124; meta_n &#124; meta_n &#124; meta_n &#124; meta_n &#124; meta_n &#124; meta_n &#124; meta_n Reorder meta tensor elements meta_reordered = meta new_empty m meta_ncols type ignore possibly-undefined meta_offsets = _calculate_meta_reordering_scatter_offsets m meta_ncols meta_dtype device pyrefly ignore unbound-name meta_reordered scatter_ meta_offsets meta view - sparse meta_reordered view m meta_ncols sparse_semi_structured_to_dense_cutlass sparse meta_reordered This function performs reverse function above - reconstructs dense matrix pair compressed matrix given layout used CUTLASS backend accompanying metadata matrix sparse dim = raise RuntimeError f Expected -dimensional sparse tensor got sparse dim -dimensional tensor m k = sparse shape device = sparse device meta_reordered dim = raise RuntimeError f Expected -dimensional meta tensor got meta_reordered dim -dimensional tensor meta_reordered device = device raise RuntimeError f Expected meta matrix device device got matrix meta_reordered device device meta_dtype = meta_reordered dtype meta_dtype torch int torch int raise RuntimeError f Invalid datatype meta_dtype meta matrix quadbits_per_meta_elem = meta_dtype itemsize sparse dtype = torch float ksparse = ksparse = meta_nrows meta_ncols = meta_reordered shape meta_nrows = m raise RuntimeError f Number rows meta matrix meta_nrows must equal number columns spase matrix m meta_ncols ksparse quadbits_per_meta_elem = k raise RuntimeError f Number columns sparse matrix k different meta_ncols ksparse quadbits_per_meta_elem expected according number columns meta matrix Undo meta tensor elements reordering meta_offsets = _calculate_meta_reordering_scatter_offsets m meta_ncols meta_dtype device meta = torch gather meta_reordered view - meta_offsets view m meta_ncols Unpack sparse tensor back original dense tensor using information provided meta tensor Note torch float datatype handled pretty much same torch half torch bfloat metadata pair torch float value encoded underlying bytes contain four torch half torch bfloat values where either first two last two zeros meta_ = torch empty m meta_ncols quadbits_per_meta_elem dtype=meta_dtype device=device quadbits_per_meta_elem == meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b quadbits_per_meta_elem == meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b meta_ = meta b dense_offsets = meta_ view - + torch arange m k ksparse device=device view - repeat view - dense = torch zeros m k dtype=sparse dtype device=device sparse dtype = torch float dense scatter_ dense_offsets sparse view - dense view torch half scatter_ dense_offsets sparse view torch half view - dense view m k _sparse_semi_structured_tile dense This function computes sparse tile greedily taking largest values Since we take largest values greedily how sorting algorithm handles duplicates affects ultimate sparsity pattern Note function does have same sorting semantics our CUDA backend which exposed via ` torch _sparse_semi_structured_tile ` thus returns different pattern greedy_prune_tile tile num_kept_row = num_kept_col = x tile flatten sort descending=True stable=True indices r c = x x num_kept_row r num_kept_col c num_kept_row r += num_kept_col c += tile r c = batch dense unfold unfold tile batch greedy_prune_tile tile dense _compute_compressed_swizzled_bitmask dense Calculates compressed swizzled bitmask dense tensor first we need convert dense tensor bitmask int_bitmask = dense bool torch uint Each thread responsible x tile which contains x tiles A B C D displayed following schema + --- + --- + &#124; A &#124; B &#124; + --- + --- + &#124; C &#124; D &#124; + --- + --- + we first need split into x tiles bitmask_ x _chunks = int_bitmask unfold unfold then we unfold again get our individual x tiles bitmask_ x _chunks = bitmask_ x _chunks unfold unfold Each x bitmask defines two -bit integers which encode sparsity pattern tile Note least significant bit stored first - - reshape tensor expand tiles into -bit vectors bitmask_binary_representation = bitmask_ x _chunks reshape bitmask_ x _chunks shape convert binary representation we can do matmul powers two powers_of_two = torch arange dtype=torch float device= cuda To run GPU cast float do matmul then cast back compressed_swizzled_bitmask = bitmask_binary_representation torch float powers_of_two torch uint compressed_swizzled_bitmask