mypy allow-untyped-defs contextlib torch __all__ = fallback_dispatcher semi_sparse_values semi_sparse_indices semi_sparse_t semi_sparse_view semi_sparse_detach semi_sparse_mm semi_sparse_addmm semi_sparse_linear semi_sparse_scaled_mm contextlib contextmanager no_dispatch guard = torch _C _DisableTorchDispatch try yield finally del guard fallback_dispatcher func types args kwargs no_dispatch func args semi_sparse_values func types args= kwargs=None - torch Tensor assert len args == A = args assert isinstance A torch sparse SparseSemiStructuredTensor assert A packed None A meta None m k = A shape num_kept_elements = m k A packed ravel num_kept_elements view m - A packed detach semi_sparse_indices func types args= kwargs=None - torch Tensor assert len args == A = args assert isinstance A torch sparse SparseSemiStructuredTensor assert A packed None A meta None m k = A shape num_kept_elements = m k metadata = A packed ravel num_kept_elements view m - metadata view torch int A dtype == torch int torch int A meta semi_sparse_t func types args= kwargs=None - torch Tensor assert len args == = args assert isinstance torch sparse SparseSemiStructuredTensor assert len shape == Because we cannot go compressed representation back dense representation currently we just keep track how many times we have been transposed Depending whether sparse matrix first second argument we expect even odd number calls transpose respectively pyrefly ignore no-matching-overload __class__ torch Size shape - shape packed=self packed_t meta=self meta_t packed_t=self packed meta_t=self meta compressed_swizzled_bitmask= compressed_swizzled_bitmask transpose compressed_swizzled_bitmask None None fuse_transpose_cusparselt=args fuse_transpose_cusparselt alg_id_cusparselt=args alg_id_cusparselt semi_sparse_view func types args= kwargs=None - torch Tensor assert len args == shape = args tuple shape = shape raise NotImplementedError f ` view ` implemented SparseSemiStructuredTensor except dummy case shape= shape semi_sparse_detach func types args kwargs - torch Tensor assert len args == = args __class__ shape=self shape packed=self packed meta=self meta packed_t=self packed_t meta_t=self meta_t compressed_swizzled_bitmask=self compressed_swizzled_bitmask fuse_transpose_cusparselt=self fuse_transpose_cusparselt alg_id_cusparselt=self alg_id_cusparselt requires_grad=False semi_sparse_mm func types args= kwargs=None - torch Tensor assert len args == A B = args A ndim = B ndim = raise NotImplementedError ` SparseSemiStructuredTensor ` matmul Broadcasting implemented isinstance A torch sparse SparseSemiStructuredTensor row col = B shape B_padded = A _pad_dense_input B res = A _mm B_padded res col B_t = B t assert isinstance B_t torch sparse SparseSemiStructuredTensor row col = A shape A_padded = B _pad_dense_input A res = B_t _mm A_padded t t res row semi_sparse_addmm func types args= kwargs=None - torch Tensor assert len args == bias A B = args A ndim = B ndim = raise NotImplementedError ` SparseSemiStructuredTensor ` matmul Broadcasting implemented bias ndim = raise NotImplementedError f ` SparseSemiStructuredTensor ` matmul only bias dim= supported Shape= bias shape isinstance A torch sparse SparseSemiStructuredTensor raise NotImplementedError ` SparseSemiStructuredTensor ` matmul only operand B ` addmm ` can sparse B_t = B t assert isinstance B_t torch sparse SparseSemiStructuredTensor row _col = A shape A_padded = B_t _pad_dense_input A result = B_t _mm A_padded t bias=bias t result row semi_sparse_linear func types args= kwargs=None - torch Tensor assert len args A B = args bias = args len args == None shape = A shape A_ d = A view - shape - bias None res = A_ d B t res = semi_sparse_addmm func=None types=None args= bias A_ d B t res view shape - - semi_sparse_scaled_mm func types args= kwargs=None - torch Tensor pull all args excluding use_fast_accum flag set A B A_scale B_scale bias scale_result out_dtype = args assert A dtype == torch float _e m fn assert B dtype == torch float _e m fn only cuSPARSELt supports float _e m fn currently assert isinstance A torch sparse SparseSemiStructuredTensorCUSPARSELT assert A packed None Currently we only support per-tensor scaling float scales assert A_scale numel == B_scale numel == assert A_scale dtype == torch float B_scale dtype == torch float cuSPARSELt lacks A B operand scaling support so instead we use alpha scale result Note limits us per-tensor scalig only sparse_result = torch _cslt_sparse_mm A packed B alpha=A_scale B_scale out_dtype=out_dtype sparse_result