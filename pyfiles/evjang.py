Eric Jang originally wrote implementation MAML JAX https github com ericjang maml-jax We translated his implementation JAX PyTorch math matplotlib mpl matplotlib pyplot plt numpy np torch torch nn functional F mpl use Agg net x params x = F linear x params params x = F relu x x = F linear x params params x = F relu x x = F linear x params params x params = torch Tensor uniform_ - requires_grad_ torch Tensor zero_ requires_grad_ torch Tensor uniform_ - math sqrt math sqrt requires_grad_ torch Tensor zero_ requires_grad_ torch Tensor uniform_ - math sqrt math sqrt requires_grad_ torch Tensor zero_ requires_grad_ opt = torch optim Adam params lr= e- alpha = K = losses = num_tasks = sample_tasks outer_batch_size inner_batch_size Select amplitude phase task As = phases = _ range outer_batch_size As append np random uniform low= high= phases append np random uniform low= high=np pi get_batch xs ys = A phase zip As phases x = np random uniform low=- high= size= inner_batch_size y = A np sin x + phase xs append x ys append y torch tensor xs dtype=torch float torch tensor ys dtype=torch float x y = get_batch x y = get_batch x y x y range loss = opt zero_grad get_loss_for_task x y x y f = net x params loss = F mse_loss f y create_graph=True because computing grads here part forward pass We want differentiate through SGD update steps get higher order derivatives backward pass grads = torch autograd grad loss params create_graph=True new_params = params i - alpha grads i i range len params v_f = net x new_params F mse_loss v_f y task = sample_tasks num_tasks K inner_losses = get_loss_for_task task i task i task i task i i range num_tasks loss = sum inner_losses len inner_losses loss backward opt step == print f Iteration d -- Outer Loss loss f losses append loss detach t_A = torch tensor uniform_ t_b = torch tensor uniform_ math pi t_x = torch empty uniform_ - t_y = t_A torch sin t_x + t_b opt zero_grad t_params = params k range t_f = net t_x t_params t_loss = F l _loss t_f t_y grads = torch autograd grad t_loss t_params create_graph=True t_params = t_params i - alpha grads i i range len params test_x = torch arange - math pi math pi step= unsqueeze test_y = t_A torch sin test_x + t_b test_f = net test_x t_params plt plot test_x data numpy test_y data numpy label= sin x plt plot test_x data numpy test_f data numpy label= net x plt plot t_x data numpy t_y data numpy o label= Examples plt legend plt savefig maml-sine png plt figure plt plot np convolve losses plt savefig losses png