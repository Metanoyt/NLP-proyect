hashlib os pathlib Path typing NamedTuple file_io_utils copy_file download_s _objects_with_prefix load_json_file sanitize_for_s unzip_folder upload_file_to_s write_json_file zip_folder PYTEST_CACHE_KEY_PREFIX = pytest_cache PYTEST_CACHE_DIR_NAME = pytest_cache BUCKET = gha-artifacts LASTFAILED_FILE_PATH = Path v cache lastfailed TD_HEURISTIC_PREVIOUSLY_FAILED_ADDITIONAL = previous_failures_additional json Temp folders ZIP_UPLOAD = zip-upload CACHE_ZIP_DOWNLOADS = cache-zip-downloads UNZIPPED_CACHES = unzipped-caches Since pr identifier can based include user defined text like branch name we hash sanitize input avoid corner cases PRIdentifier str __slots__ = __new__ cls value str - PRIdentifier md = hashlib md value encode utf- usedforsecurity=False hexdigest super __new__ cls md GithubRepo NamedTuple owner str name str Create Repo string like owner repo classmethod from_string cls repo_string str - GithubRepo repo_string raise ValueError f repo_string must form owner repo repo_string owner name = repo_string split cls owner name __str__ - str f owner name upload_pytest_cache pr_identifier PRIdentifier repo GithubRepo job_identifier str sha str test_config str shard str cache_dir Path temp_dir Path bucket str = BUCKET - None Uploads pytest cache S merging any previous caches previous runs same job In particular keeps all failed tests across all runs job cache so future jobs download cache will prioritize running tests have failed past Args pr_identifier A unique human readable identifier PR job The name job uploading cache isinstance pr_identifier PRIdentifier raise ValueError f pr_identifier must type PRIdentifier type pr_identifier bucket bucket = BUCKET Upload cache obj_key_prefix = _get_s _key_prefix pr_identifier repo job_identifier sha test_config shard zip_file_path = zip_folder cache_dir temp_dir ZIP_UPLOAD obj_key_prefix obj_key = f obj_key_prefix os path splitext zip_file_path Keep new file extension upload_file_to_s zip_file_path bucket obj_key download_pytest_cache pr_identifier PRIdentifier repo GithubRepo job_identifier str dest_cache_dir Path temp_dir Path bucket str = BUCKET - None Downloads pytest cache S The goal detect any tests have failed past run them first so dev can get faster feedback them We merge cache all shards since tests can get shuffled around one shard another based when we last updated our stats how long each test takes run This ensures even test moves different shard shard will know run first had failed previously bucket bucket = BUCKET isinstance pr_identifier PRIdentifier raise ValueError f pr_identifier must type PRIdentifier type pr_identifier obj_key_prefix = _get_s _key_prefix pr_identifier repo job_identifier zip_download_dir = temp_dir CACHE_ZIP_DOWNLOADS obj_key_prefix downloads cache zips all shards downloads = download_s _objects_with_prefix bucket obj_key_prefix zip_download_dir downloaded_zip downloads Unzip into random folder then merge current cache cache_dir_for_shard = temp_dir UNZIPPED_CACHES os urandom hex PYTEST_CACHE_DIR_NAME unzip_folder downloaded_zip cache_dir_for_shard print f Merging cache downloaded_zip _merge_pytest_caches cache_dir_for_shard dest_cache_dir _get_s _key_prefix pr_identifier PRIdentifier repo GithubRepo job_identifier str sha str = test_config str = shard str = - str The prefix any S object key pytest cache It s only prefix though full path object For example won t include file extension prefix = f PYTEST_CACHE_KEY_PREFIX repo owner repo name pr_identifier sanitize_for_s job_identifier sha prefix += f sha test_config prefix += f sanitize_for_s test_config shard prefix += f shard prefix _merge_pytest_caches pytest_cache_dir_to_merge_from Path pytest_cache_dir_to_merge_into Path - None LASTFAILED_FILE_PATH only file we actually care about cache since contains all tests failed The remaining files static supporting files don t really matter They make cache folder play nice other tools devs tend use e g git But since pytest doesn t recreate these files pytest_cache folder already exists we ll copy them over way protect against future bugs where certain tool may need those files exist work properly their combined file size negligible static_files_to_copy = gitignore CACHEDIR TAG README md Copy over static files These files never change so only copy them they don t already exist new cache static_file static_files_to_copy source_file = pytest_cache_dir_to_merge_from static_file source_file is_file continue dest_file = pytest_cache_dir_to_merge_into static_file dest_file exists copy_file source_file dest_file Handle v cache lastfailed file _merge_lastfailed_files pytest_cache_dir_to_merge_from pytest_cache_dir_to_merge_into _merge_additional_failures_files pytest_cache_dir_to_merge_from pytest_cache_dir_to_merge_into _merge_lastfailed_files source_pytest_cache Path dest_pytest_cache Path - None Simple cases where one files doesn t exist source_lastfailed_file = source_pytest_cache LASTFAILED_FILE_PATH dest_lastfailed_file = dest_pytest_cache LASTFAILED_FILE_PATH source_lastfailed_file exists dest_lastfailed_file exists copy_file source_lastfailed_file dest_lastfailed_file Both files exist so we need merge them from_lastfailed = load_json_file source_lastfailed_file to_lastfailed = load_json_file dest_lastfailed_file merged_content = _merged_lastfailed_content from_lastfailed to_lastfailed Save results write_json_file dest_lastfailed_file merged_content _merged_lastfailed_content from_lastfailed dict str bool to_lastfailed dict str bool - dict str bool The lastfailed files dictionaries where key test identifier Each entry s value appears always ` true ` let s count An empty dictionary represented single value empty string key If entry from_lastfailed doesn t exist to_lastfailed add s value key from_lastfailed key to_lastfailed to_lastfailed key = from_lastfailed key len to_lastfailed Remove empty entry exists since we have actual entries now to_lastfailed del to_lastfailed to_lastfailed _merge_additional_failures_files source_pytest_cache Path dest_pytest_cache Path - None Simple cases where one files doesn t exist source_lastfailed_file = source_pytest_cache TD_HEURISTIC_PREVIOUSLY_FAILED_ADDITIONAL dest_lastfailed_file = dest_pytest_cache TD_HEURISTIC_PREVIOUSLY_FAILED_ADDITIONAL source_lastfailed_file exists dest_lastfailed_file exists copy_file source_lastfailed_file dest_lastfailed_file Both files exist so we need merge them from_lastfailed = load_json_file source_lastfailed_file to_lastfailed = load_json_file dest_lastfailed_file merged_content = list set from_lastfailed + to_lastfailed Save results write_json_file dest_lastfailed_file merged_content