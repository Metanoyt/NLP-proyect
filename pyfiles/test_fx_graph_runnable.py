Owner s module dynamo io logging subprocess sys unittest torch torch _logging structured torch distributed dist torch _inductor codecache WritableTempFile torch _inductor test_case TestCase torch testing _internal common_utils IS_FBCODE IS_SANDCASTLE torch utils _triton has_triton torch distributed is_available torch distributed _tensor DeviceMesh DTensor Replicate Shard torch testing _internal distributed fake_pg FakeStore has_triton triton triton language tl init_to_zero name lambda nargs nargs name zero_ triton jit add_kernel x_ptr y_ptr output_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load x_ptr + offsets mask=mask y = tl load y_ptr + offsets mask=mask output = x + y tl atomic_add output_ptr + offsets output mask=mask triton autotune configs= triton Config BLOCK_SIZE num_warps= num_stages= pre_hook=init_to_zero output_ptr pre_hook=init_to_zero output_ptr post_hook=init_to_zero output_ptr key= n_elements triton jit add_kernel_autotune x_ptr y_ptr output_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load x_ptr + offsets mask=mask y = tl load y_ptr + offsets mask=mask output = x + y tl atomic_add output_ptr + offsets output mask=mask torch testing _internal inductor_utils GPU_TYPE torch testing _internal triton_utils requires_gpu FxGraphRunnableArtifactFilter logging Filter filter record artifact record metadata record metadata artifact name == fx_graph_runnable StructuredTracePayloadFormatter logging Formatter format record record payload strip trace_log = logging getLogger torch __trace ToyModel torch nn Module __init__ input_size= hidden_size= output_size= super __init__ linear = torch nn Linear input_size hidden_size linear = torch nn Linear hidden_size output_size relu = torch nn ReLU dropout = torch nn Dropout forward x x = linear x x = relu x x = dropout x x = linear x x unittest skipIf IS_FBCODE IS_SANDCASTLE Skip fbcode sandcastle FxGraphRunnableTest TestCase setUp super setUp torch _dynamo reset torch _logging structured INTERN_TABLE clear old_level = trace_log level trace_log setLevel logging DEBUG Create custom filter specifically fx_graph_runnable entries filter = FxGraphRunnableArtifactFilter Create separate buffer handler capturing fx_graph_runnable entries buffer = io StringIO handler = logging StreamHandler buffer handler setFormatter StructuredTracePayloadFormatter handler addFilter filter trace_log addHandler handler tearDown trace_log removeHandler handler trace_log setLevel old_level _exec_and_verify_payload Write captured payload run fresh Python process payload = buffer getvalue strip assertTrue payload Expected fx_graph_runnable payload got nothing assertIn forward payload sanity-check actual FX code WritableTempFile w suffix= py tmp tmp write payload tmp flush res = subprocess run sys executable tmp name capture_output=True text=True timeout= assertEqual res returncode f Standalone fx_graph_runnable failed \nSTDERR \n res stderr basic tests test_basic_tensor_add f x x + torch compile f torch randn _exec_and_verify_payload unittest skipUnless has_triton Triton available test_user_defined_triton_kernel_autotune add x torch Tensor y torch Tensor - torch Tensor output = torch ones x shape device=x device dtype=x dtype n_elements = output numel grid meta triton cdiv n_elements meta BLOCK_SIZE add_kernel_autotune grid x y output n_elements output x = torch ones device=GPU_TYPE dtype=torch float y = torch ones device=GPU_TYPE dtype=torch float torch compile add x y _exec_and_verify_payload unittest skipUnless has_triton Triton available requires_gpu test_user_defined_triton_kernel add x torch Tensor y torch Tensor - torch Tensor output = torch ones x shape device=x device dtype=x dtype n_elements = x numel add_kernel n_elements x y output n_elements BLOCK_SIZE= output x = torch ones device=GPU_TYPE dtype=torch float y = torch ones device=GPU_TYPE dtype=torch float torch compile add x y _exec_and_verify_payload test_two_inputs_matmul f b b relu b = torch randn torch randn torch compile f b _exec_and_verify_payload test_scalar_multiply f x x torch compile f torch randn _exec_and_verify_payload testing dynamic shapes test_dynamic_shapes_run f x x x transpose relu = torch randn torch _dynamo mark_dynamic torch _dynamo mark_dynamic torch compile f _exec_and_verify_payload test_broadcast_add_dynamic f x y x + y x = torch randn y = torch randn torch _dynamo mark_dynamic x torch _dynamo mark_dynamic y torch compile f x y _exec_and_verify_payload test_toy_model_basic model = ToyModel input_size= hidden_size= output_size= model eval Set eval mode avoid dropout randomness x = torch randn torch compile model x _exec_and_verify_payload test_toy_model_batch_processing model = ToyModel input_size= hidden_size= output_size= model eval x = torch randn torch compile model x _exec_and_verify_payload test_toy_model_dynamic_batch model = ToyModel input_size= hidden_size= output_size= model eval x = torch randn torch _dynamo mark_dynamic x torch compile model x _exec_and_verify_payload Distributed collectives tests FakeProcessGroup unittest skipIf torch distributed is_available Torch distributed available unittest skipIf IS_FBCODE IS_SANDCASTLE Skip fbcode sandcastle test_all_reduce_collective store = FakeStore dist init_process_group backend= fake rank= world_size= store=store f x dist all_reduce x x try x = torch randn torch compile f x finally dist destroy_process_group _exec_and_verify_payload unittest skipIf torch distributed is_available Torch distributed available unittest skipIf IS_FBCODE IS_SANDCASTLE Skip fbcode sandcastle test_all_gather_collective store = FakeStore dist init_process_group backend= fake rank= world_size= store=store f x output_tensors = torch empty_like x _ range dist all_gather output_tensors x output_tensors + output_tensors try x = torch randn torch compile f x finally dist destroy_process_group _exec_and_verify_payload unittest skipIf torch distributed is_available Torch distributed available unittest skipIf IS_FBCODE IS_SANDCASTLE Skip fbcode sandcastle test_broadcast_collective store = FakeStore dist init_process_group backend= fake rank= world_size= store=store f x dist broadcast x src= x sum try x = torch randn torch compile f x finally dist destroy_process_group _exec_and_verify_payload unittest skipIf torch distributed is_available Torch distributed available unittest skipIf IS_FBCODE IS_SANDCASTLE Skip fbcode sandcastle test_reduce_scatter_collective store = FakeStore dist init_process_group backend= fake rank= world_size= store=store f x input_list = x x clone output = torch empty_like x dist reduce_scatter output input_list output try x = torch randn torch compile f x finally dist destroy_process_group _exec_and_verify_payload unittest skipIf torch distributed is_available Torch distributed available unittest skipIf IS_FBCODE IS_SANDCASTLE Skip fbcode sandcastle test_dtensor_compile_redistribute store = FakeStore dist init_process_group backend= fake rank= world_size= store=store mesh = DeviceMesh cpu list range f x y dt = DTensor from_local x reshape mesh Shard run_check=False dt = DTensor from_local y reshape mesh Shard run_check=False dt_out = torch matmul dt dt dt_out_redistribute = dt_out redistribute mesh Replicate dt_out_redistribute to_local try x = torch arange dtype=torch float y = torch arange dtype=torch float torch compile f x y finally dist destroy_process_group _exec_and_verify_payload test_metrics_context When TORCH_COMPILE_DEBUG set provenance_tracking_level set generated fx_graph_runnable crashed RuntimeError Cannot add inductor_provenance outside MetricsContext torch _inductor config inductor_config f x x + Enable provenance tracking trigger code path adds metrics inductor_config patch trace enabled True trace provenance_tracking_level x = torch randn torch compile f x _exec_and_verify_payload torch _dynamo config patch assume_static_by_default=False test_dynamic_expression Test emitting something like s s = f x torch ops aten _adaptive_avg_pool d x torch ops aten _adaptive_avg_pool d x + x = torch randn torch compile f x _exec_and_verify_payload __name__ == __main__ torch _dynamo test_case run_tests IS_FBCODE IS_SANDCASTLE fbcode complains about being able find torch subprocess run_tests