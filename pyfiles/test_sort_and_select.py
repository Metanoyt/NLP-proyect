Owner s module tests random itertools permutations product numpy np torch torch nan torch testing make_tensor torch testing _internal common_device_type dtypes dtypesIfCPU dtypesIfCUDA instantiate_device_type_tests largeTensorTest onlyCPU onlyCUDA onlyNativeDeviceTypes torch testing _internal common_dtype all_types all_types_and floating_types_and integral_types torch testing _internal common_utils run_tests skipIfTorchDynamo slowTest TestCase TestSortAndSelect TestCase assertIsOrdered order x mxx ixx task SIZE = x size order == descending check_order b ` = ` because we put NaNs end ascending sorted lists beginning descending ones = &#124; = b all item order == ascending check_order b see above b = b &#124; = b all item raise ValueError f unknown order order must ascending descending k range SIZE assertTrue check_order mxx k - mxx k f torch sort order values unordered task seen = set size = x size size = x size x dim - x = x tolist mxx = mxx tolist ixx = ixx tolist k range size seen clear j range size assertEqual x k ixx k j mxx k j msg=f torch sort order indices wrong task seen add ixx k j assertEqual len seen size test_sort device CUDA vs have different code path dim being sorted SIZE x = torch rand SIZE device=device res val res ind = torch sort x Test inplace y = x clone y_inds = torch tensor dtype=torch int device=device torch sort y out= y y_inds x_vals x_inds = torch sort x assertEqual x_vals y assertEqual x_inds y_inds Test use result tensor res val = torch tensor device=device res ind = torch tensor device=device dtype=torch long torch sort x out= res val res ind assertEqual res val res val atol= rtol= assertEqual res ind res ind atol= rtol= assertEqual torch argsort x res ind assertEqual x argsort res ind Test sorting random numbers assertIsOrdered ascending x res val res ind random Test simple sort assertEqual torch sort torch tensor device=device torch tensor device=device atol= rtol= Test we still have proper sorting duplicate keys x = torch floor torch rand SIZE device=device torch sort x out= res val res ind assertIsOrdered ascending x res val res ind random duplicate keys DESCENDING SORT x = torch rand SIZE device=device res val res ind = torch sort x x dim - True Test use result tensor res val = torch tensor device=device res ind = torch tensor device=device dtype=torch long torch sort x x dim - True out= res val res ind assertEqual res val res val atol= rtol= assertEqual res ind res ind atol= rtol= assertEqual torch argsort x x dim - True res ind assertEqual x argsort x dim - True res ind Test sorting random numbers assertIsOrdered descending x res val res ind random Test simple sort task assertEqual torch sort torch tensor device=device True torch tensor device=device atol= rtol= Test we still have proper sorting duplicate keys assertIsOrdered descending x res val res ind random duplicate keys Test argument sorting without stable x = torch tensor assertEqual torch argsort x stable=True torch sort x stable=True indices assertEqual torch argsort x stable=False torch sort x stable=False indices assertEqual torch argsort x torch sort x indices Test sorting NaNs x = torch rand SIZE device=device x = float NaN x = float NaN torch sort x out= res val res ind assertIsOrdered ascending x res val res ind random NaNs torch sort x out= res val res ind descending=True assertIsOrdered descending x res val res ind random NaNs test_sort_stable_none Called sort stable=None used trigger assertion See https github com pytorch pytorch issues x = torch ones y = x sort stable=None values assertTrue torch all y == torch ones item onlyCPU test_complex_unsupported_cpu x = torch tensor + j + j assertRaisesRegex RuntimeError Sort does support complex dtypes CPU torch sort input=x onlyCUDA test_sort_large_slice device tests direct cub path x = torch randn device=device res val res ind = torch sort x stable=True torch cuda synchronize assertIsOrdered too slow so just compare cpu res val_cpu res ind_cpu = torch sort x cpu stable=True assertEqual res val res val_cpu cuda assertEqual res ind res ind_cpu cuda res val res ind = torch sort x descending=True stable=True torch cuda synchronize res val_cpu res ind_cpu = torch sort x cpu descending=True stable=True assertEqual res val res val_cpu cuda assertEqual res ind res ind_cpu cuda dtypes all_types_and torch bool torch half torch bfloat test_stable_sort device dtype sizes = ncopies sizes x = torch tensor ncopies dtype=dtype device=device _ idx = x sort stable=True assertEqual idx ncopies torch arange start= end= ncopies step= device=device assertEqual idx ncopies torch arange start= end= ncopies step= device=device onlyCUDA dtypes torch float largeTensorTest GB Unfortunately GB A large enough test_sort_large device dtype t = torch randperm device=device dtype t = t view expand + - contiguous v i = t sort del t iv im = torch var_mean i dtype dim= del i vv vm = torch var_mean v dtype dim= del v assertEqual vv torch zeros_like vv assertEqual iv torch zeros_like iv assertEqual vm torch arange dtype=dtype device=device assertEqual im t sort indices exact_dtype=False dtypes torch float test_sort_restride device dtype Input non-contiguous stride -element array tensor = torch randn dtype=dtype device=device Outputs -dim tensors They will need resized which means they will also restrided input tensor s strides base values = torch tensor dtype=dtype device=device indices = torch tensor dtype=torch long device=device torch sort tensor out= values indices Check outputs restrided dense strides assertEqual values stride assertEqual indices stride Check tensor indexed indices equal values assertEqual tensor indices values _test_sort_discontiguous device dtype CUDA vs have different code path dim being sorted sizes = shape permutations sizes perm permutations dim range t = torch randn shape device=device dtype=dtype permute perm r = t sort dim=dim r = t contiguous sort dim=dim assertEqual r r n = t size dim assert ordered assertTrue r values narrow dim n - = r values narrow dim n - all assert different segments does mix which can easily happen stride handled correctly assertTrue t unsqueeze - transpose dim - == r values unsqueeze - any dim=dim any dim=- all assert stride preserved device_type == cuda FIXME behavior should true all cases just one specified condition assertEqual r values stride t stride assertEqual r indices stride t stride onlyCUDA dtypes torch float test_sort_discontiguous device dtype _test_sort_discontiguous device dtype slowTest test slow CPU CUDA onlyCPU dtypes torch float test_sort_discontiguous_slow device dtype _test_sort_discontiguous device dtype dtypes torch float test_sort_ d_output_discontiguous device dtype tensor = torch randn device=device dtype=dtype values = torch empty_like tensor indices = torch empty device=device dtype=torch long torch sort tensor out= values indices values_cont indices_cont = tensor sort assertEqual indices indices_cont assertEqual values values_cont slowTest onlyCPU dtypes integral_types test_sort_ d_parallel device dtype low = dtype == torch uint - tensor = torch randint low=low high= size= device=device dtype=dtype vals _ = torch sort tensor stable=True assertEqual True torch all vals - = vals dtypes torch float test_topk_ d_output_discontiguous device dtype tensor = torch randn device=device dtype=dtype values = torch empty_like tensor indices = torch empty device=device dtype=torch long sorted True False outputs ` sorted=False ` test guaranteed same current implementation they torch topk tensor sorted=sorted out= values indices values_cont indices_cont = tensor topk sorted=sorted assertEqual indices indices_cont assertEqual values values_cont dtypes all_types_and torch bool torch half torch bfloat test_stable_sort_against_numpy device dtype dtype floating_types_and torch float torch bfloat inf = float inf neg_inf = -float inf nan = float nan dtype = torch bool no torch iinfo support torch bool inf = torch iinfo dtype max neg_inf = torch iinfo dtype min inf = True neg_inf = ~inf no nan integral types we use inf instead simplicity nan = inf generate_samples itertools chain combinations sizes size = sizes binary strings yield torch tensor size dtype=dtype device=device device_type == cuda yield torch tensor dtype=dtype device=device repeated_index_fill t dim idxs vals res = t idx val zip idxs vals res = res index_fill dim idx val res sizes size = min sizes x = torch randn sizes device=device size dtype yield x Generate tensors which being filled random locations values non-empty subsets set inf neg_inf nan each dimension n_fill_vals = cardinality inf neg_inf nan dim range len sizes idxs = torch randint high=size size= size i range n_fill_vals vals = inf neg_inf nan subsets = chain from_iterable combinations list zip idxs vals r r range n_fill_vals + subset subsets idxs_subset vals_subset = zip subset yield repeated_index_fill x dim idxs_subset vals_subset dim sample dim generate_samples _ idx_torch = sample sort dim=dim stable=True dtype torch bfloat sample_numpy = sample float cpu numpy sample_numpy = sample cpu numpy idx_numpy = np argsort sample_numpy axis=dim kind= stable assertEqual idx_torch idx_numpy dtypes all_types_and torch half torch bfloat test_msort device dtype test shape tensor = make_tensor shape dtype=dtype device=device low=- high= tensor size = torch Size dtype torch bfloat expected = torch from_numpy np sort tensor float cpu numpy axis= bfloat expected = torch from_numpy np sort tensor cpu numpy axis= expected = tensor numpy msort does support empty shapes tensor result = torch msort tensor assertEqual result expected out = torch empty_like result torch msort tensor out=out assertEqual out expected shapes = shape shapes test shape skipIfTorchDynamo Fails python dtypes torch float test_sort_expanded_tensor device dtype https github com pytorch pytorch issues data = torch scalar_tensor True device=device dtype=dtype data = data expand ref = torch Tensor True out = torch sort data stable=True dim= descending=True expected = torch sort ref stable=True dim= descending=True assertEqual out expected data = torch randn device=device dtype=dtype data = data expand ref = data contiguous out = torch sort data stable=True dim= descending=True expected = torch sort ref stable=True dim= descending=True assertEqual out expected test_topk device topKViaSort t k dim dir sorted indices = t sort dim dir sorted narrow dim k indices narrow dim k compareTensors t res ind res ind dim Values should exactly equivalent assertEqual res res atol= rtol= Indices might differ based implementation since there no guarantee relative order selection ind eq ind all To verify indices represent equivalent elements gather input using topk indices compare against sort indices vals = t gather dim ind assertEqual res vals atol= rtol= compare t k dim dir topKVal topKInd = t topk k dim dir True sortKVal sortKInd = topKViaSort t k dim dir compareTensors t sortKVal sortKInd topKVal topKInd dim SIZE = t = torch rand random randint SIZE random randint SIZE random randint SIZE device=device _kTries range _dimTries range transpose True False dir True False testTensor = t transpose dim = random randrange t ndimension dim = dim while dim == dim dim = random randrange t ndimension testTensor = t transpose dim dim dim = random randrange testTensor ndimension k = random randint testTensor size dim compare testTensor k dim dir This tests code path where CUDA topk implemented sort t = torch randn device=device compare t True compare t False This tests code path where CUDA topk implemented multiblock t = torch randn device=device compare t True compare t False test_topk_quantized_scalar_input Calling topk quantized scalar input used segfault see https github com pytorch pytorch issues x = torch quantize_per_tensor torch randn torch qint x topk test_topk_arguments device q = torch randn device=device Make sure True isn t mistakenly taken nd dimension interpreted assertRaises TypeError lambda q topk True test_unique_dim device assertFalse hasattr torch unique_dim run_test device dtype x = torch tensor dtype=dtype device=device x_empty = torch empty dtype=dtype device=device x_ill_formed_empty = torch empty dtype=dtype device=device x_ill_formed_empty_another = torch empty dtype=dtype device=device dtype floating_types_and torch float torch bfloat x_nan = torch tensor float nan float nan float nan dtype=dtype device=device expected_unique_dim = torch tensor dtype=dtype device=device expected_inverse_dim = torch tensor expected_counts_dim = torch tensor expected_unique_dim = torch tensor dtype=dtype device=device expected_unique_dim _bool = torch tensor False True True True False True True True dtype=torch bool device=device expected_inverse_dim = torch tensor expected_inverse_dim _bool = torch tensor expected_counts_dim = torch tensor expected_counts_dim _bool = torch tensor expected_unique_dim = torch tensor dtype=dtype device=device expected_inverse_dim = torch tensor expected_counts_dim = torch tensor expected_unique_empty = torch empty dtype=dtype device=device expected_inverse_empty = torch tensor dtype=torch long device=device expected_counts_empty = torch tensor dtype=torch long device=device dtype floating_types_and torch float torch bfloat expected_unique_nan = torch tensor float nan float nan float nan dtype=dtype device=device expected_inverse_nan = torch tensor dtype=torch long device=device expected_counts_nan = torch tensor dtype=torch long device=device dim x_unique = torch unique x dim= assertEqual expected_unique_dim x_unique x_unique x_inverse = torch unique x return_inverse=True dim= assertEqual expected_unique_dim x_unique assertEqual expected_inverse_dim x_inverse x_unique x_counts = torch unique x return_inverse=False return_counts=True dim= assertEqual expected_unique_dim x_unique assertEqual expected_counts_dim x_counts x_unique x_inverse x_counts = torch unique x return_inverse=True return_counts=True dim= assertEqual expected_unique_dim x_unique assertEqual expected_inverse_dim x_inverse assertEqual expected_counts_dim x_counts dim x_unique = torch unique x dim= x dtype == torch bool assertEqual expected_unique_dim _bool x_unique assertEqual expected_unique_dim x_unique x_unique x_inverse = torch unique x return_inverse=True dim= x dtype == torch bool assertEqual expected_unique_dim _bool x_unique assertEqual expected_inverse_dim _bool x_inverse assertEqual expected_unique_dim x_unique assertEqual expected_inverse_dim x_inverse x_unique x_counts = torch unique x return_inverse=False return_counts=True dim= x dtype == torch bool assertEqual expected_unique_dim _bool x_unique assertEqual expected_counts_dim _bool x_counts assertEqual expected_unique_dim x_unique assertEqual expected_counts_dim x_counts x_unique x_inverse x_counts = torch unique x return_inverse=True return_counts=True dim= x dtype == torch bool assertEqual expected_unique_dim _bool x_unique assertEqual expected_inverse_dim _bool x_inverse assertEqual expected_counts_dim _bool x_counts assertEqual expected_unique_dim x_unique assertEqual expected_inverse_dim x_inverse assertEqual expected_counts_dim x_counts dim x_unique = torch unique x dim= assertEqual expected_unique_dim x_unique x_unique x_inverse = torch unique x return_inverse=True dim= assertEqual expected_unique_dim x_unique assertEqual expected_inverse_dim x_inverse x_unique x_counts = torch unique x return_inverse=False return_counts=True dim= assertEqual expected_unique_dim x_unique assertEqual expected_counts_dim x_counts x_unique x_inverse x_counts = torch unique x return_inverse=True return_counts=True dim= assertEqual expected_unique_dim x_unique assertEqual expected_inverse_dim x_inverse assertEqual expected_counts_dim x_counts test empty tensor x_unique x_inverse x_counts = torch unique x_empty return_inverse=True return_counts=True dim= assertEqual expected_unique_empty x_unique assertEqual expected_inverse_empty x_inverse assertEqual expected_counts_empty x_counts test tensor nan dtype floating_types_and torch float torch bfloat x_unique x_inverse x_counts = torch unique x_nan return_inverse=True return_counts=True dim= assertEqual expected_unique_nan x_unique assertEqual expected_inverse_nan x_inverse assertEqual expected_counts_nan x_counts test well formed tensor Checking runtime error expected behaviour assertRaises RuntimeError torch unique x_ill_formed_empty return_inverse=True return_counts=True dim= test along dim assertRaises RuntimeError torch unique x_ill_formed_empty_another return_inverse=True return_counts=True dim= test consecutive version y = torch tensor dtype=dtype device=device test tensor nan dtype floating_types_and torch float torch bfloat y_nan = torch tensor float nan float nan float nan dtype=dtype device=device expected_y_unique = torch tensor noqa F dtype=dtype device=device expected_y_inverse = torch tensor dtype=torch int device=device expected_y_counts = torch tensor dtype=torch int device=device expected_y_inverse_bool = torch tensor dtype=torch int device=device expected_y_counts_bool = torch tensor dtype=torch int device=device dtype floating_types_and torch float torch bfloat expected_y_unique_nan = torch tensor float nan float nan float nan dtype=dtype device=device expected_y_inverse_nan = torch tensor dtype=torch long device=device expected_y_counts_nan = torch tensor dtype=torch long device=device y_unique y_inverse y_counts = torch unique_consecutive y return_inverse=True return_counts=True dim= x dtype == torch bool assertEqual expected_y_inverse_bool y_inverse assertEqual expected_y_counts_bool y_counts assertEqual expected_y_inverse y_inverse assertEqual expected_y_counts y_counts test tensor nan dtype floating_types_and torch float torch bfloat y_unique y_inverse y_counts = torch unique_consecutive y_nan return_inverse=True return_counts=True dim= assertEqual expected_y_unique_nan y_unique assertEqual expected_y_inverse_nan y_inverse assertEqual expected_y_counts_nan y_counts Test dim sorted same NumPy dims = x = torch tensor dtype=dtype device=device xn = x cpu numpy d range x dim t = torch unique x dim=d n = np unique xn axis=d assertEqual t cpu numpy n run_test device torch float run_test device torch double run_test device torch long run_test device torch uint run_test device torch bool onlyCUDA test_topk_noncontiguous_gpu device test different topk paths cuda single_block_t = torch randn device=device multi_block_t = torch randn device=device sort_t = torch randn device=device t single_block_t multi_block_t sort_t k k = t shape continue top idx = t topk k top idx = t contiguous topk k assertEqual top top assertEqual idx idx _test_topk_dtype device dtype integral size integral = torch randint torch iinfo dtype min torch iinfo dtype max size= size dtype=dtype device=device = torch randn size= size dtype=dtype device=device sort_topk = sort - size flip topk = topk size assertEqual sort_topk topk check values assertEqual sort_topk topk check indices dtypes torch int torch uint torch int torch int torch int test_topk_integral device dtype small = large = verylarge = multi_block topk cuda curr_size small large verylarge _test_topk_dtype device dtype True curr_size dtypes torch bfloat torch half test_topk_lower_precision device dtype small = large = verylarge = multi_block topk cuda curr_size small large verylarge _test_topk_dtype device dtype False curr_size dtypesIfCUDA floating_types_and torch half torch bfloat dtypes torch float torch double torch bfloat torch half test_topk_nonfinite device dtype x = torch tensor float nan float inf e - e -float inf device=device dtype=dtype val idx = x topk expect = torch tensor float nan float inf e device=device dtype=dtype assertEqual val expect assertEqual idx val idx = x topk largest=False expect = torch tensor -float inf - e e device=device dtype=dtype assertEqual val expect assertEqual idx test_topk_ d device small = large = size small large x = torch ones size device=device x = x = val ind = torch topk x k= dim= expected_ind = torch ones dtype=torch long device=device expected_ind = expected_val = torch ones device=device expected_val = expected_val = assertEqual val expected_val atol= rtol= assertEqual ind expected_ind atol= rtol= onlyNativeDeviceTypes dtypesIfCUDA all_types_and torch bfloat dtypes all_types_and torch bfloat torch half test_topk_zero device dtype https github com pytorch pytorch issues t = torch rand device=device dtype=dtype val idx = torch topk t k= largest=False assertEqual val size torch Size assertEqual idx size torch Size _test_unique_scalar_empty dtype device f test scalar x = torch tensor dtype=dtype device=device unique inverse counts = f x return_inverse=True return_counts=True expected_unique = torch tensor dtype=dtype device=device expected_inverse = torch tensor device=device expected_counts = torch tensor device=device assertEqual unique expected_unique assertEqual inverse expected_inverse assertEqual counts expected_counts test zero sized tensor x = torch zeros dtype=dtype device=device unique inverse counts = f x return_inverse=True return_counts=True expected_unique = torch tensor dtype=dtype device=device expected_inverse = torch empty dtype=torch long device=device expected_counts = torch tensor dtype=torch long device=device assertEqual unique expected_unique assertEqual inverse expected_inverse assertEqual counts expected_counts _test_unique_with_expects device dtype f x expected_unique expected_inverse expected_counts additional_shape ensure_tuple x isinstance x torch Tensor x x return_inverse True False return_counts True False test expected ret = ensure_tuple f x return_inverse=return_inverse return_counts=return_counts assertEqual len ret + int return_inverse + int return_counts assertEqual expected_unique ret return_inverse assertEqual expected_inverse ret return_counts count_index = + int return_inverse assertEqual expected_counts ret count_index tests per-element unique higher rank tensor y = x view additional_shape y_unique y_inverse y_counts = f y return_inverse=True return_counts=True assertEqual expected_unique y_unique assertEqual expected_inverse view additional_shape y_inverse assertEqual expected_counts y_counts dtypesIfCPU all_types_and torch bool torch float torch bfloat dtypes all_types_and torch half torch bool test_unique device dtype ensure_tuple x isinstance x torch Tensor x x dtype torch bool x = torch tensor True False False False True False True False dtype=torch bool device=device expected_unique = torch tensor False True dtype=torch bool device=device expected_inverse = torch tensor dtype=torch long device=device expected_counts = torch tensor dtype=torch long device=device x = torch tensor dtype=dtype device=device expected_unique = torch tensor dtype=dtype device=device expected_inverse = torch tensor device=device expected_counts = torch tensor device=device test sorted unique fs = lambda x kwargs torch unique x sorted=True kwargs lambda x kwargs x unique sorted=True kwargs x_sliced = torch empty x size dtype=dtype device=device copy_ x xs = x x_sliced f x product fs xs _test_unique_with_expects device dtype f x expected_unique expected_inverse expected_counts _test_unique_scalar_empty dtype device f test unsorted unique fs = lambda x kwargs torch unique x sorted=False kwargs lambda x kwargs x unique sorted=False kwargs f x product fs xs _test_unique_scalar_empty dtype device f return_inverse return_counts product True False repeat= ret = ensure_tuple f x return_inverse=return_inverse return_counts=return_counts assertEqual len ret + int return_inverse + int return_counts x_list = x tolist x_unique_list = ret tolist assertEqual expected_unique tolist sorted x_unique_list return_inverse x_inverse_list = ret tolist i j enumerate x_inverse_list assertEqual x_list i x_unique_list j return_counts count_index = + int return_inverse x_counts_list = ret count_index tolist i j zip x_unique_list x_counts_list count = k x_list k == i count += assertEqual j count dtypesIfCPU all_types_and torch bool torch float torch bfloat dtypes all_types_and torch half torch bool test_unique_consecutive device dtype dtype torch bool x = torch tensor True False False False True True False False False dtype=torch bool device=device expected_unique = torch tensor True False True False dtype=torch bool device=device expected_inverse = torch tensor dtype=torch long device=device expected_counts = torch tensor dtype=torch long device=device x = torch tensor dtype=dtype device=device expected_unique = torch tensor dtype=dtype device=device expected_inverse = torch tensor device=device expected_counts = torch tensor device=device f torch unique_consecutive lambda x kwargs x unique_consecutive kwargs _test_unique_with_expects device dtype f x expected_unique expected_inverse expected_counts _test_unique_scalar_empty dtype device f dtypes torch double test_kthvalue device dtype SIZE = x = torch rand SIZE SIZE SIZE dtype=dtype device=device x = x clone k = random randint SIZE res val res ind = torch kthvalue x k keepdim=False res val res ind = torch sort x assertEqual res val res val k - atol= rtol= assertEqual res ind res ind k - atol= rtol= test use result tensors k = random randint SIZE res val = torch tensor dtype=dtype device=device res ind = torch tensor dtype=torch long device=device torch kthvalue x k keepdim=False out= res val res ind res val res ind = torch sort x assertEqual res val res val k - atol= rtol= assertEqual res ind res ind k - atol= rtol= test non-default dim k = random randint SIZE res val res ind = torch kthvalue x k keepdim=False res val res ind = torch sort x assertEqual res val res val k - atol= rtol= assertEqual res ind res ind k - atol= rtol= non-contiguous y = x narrow y = y contiguous k = random randint SIZE res val res ind = torch kthvalue y k res val res ind = torch kthvalue y k assertEqual res val res val atol= rtol= assertEqual res ind res ind atol= rtol= non-contiguous Reference https github com pytorch pytorch issues non_contig_t = torch tensor - - dtype=dtype device=device expected_val expected_ind = non_contig_t contiguous kthvalue non_contig_cpu_t = non_contig_t cpu expected_val_cpu expected_ind_cpu = non_contig_cpu_t kthvalue out_val out_ind = non_contig_t kthvalue assertEqual expected_val out_val atol= rtol= assertEqual expected_ind out_ind atol= rtol= assertEqual expected_val_cpu out_val atol= rtol= assertEqual expected_ind_cpu out_ind atol= rtol= check input wasn t modified assertEqual x x atol= rtol= simple test case repetitions y = torch tensor dtype=dtype device=device assertEqual torch kthvalue y atol= rtol= assertEqual torch kthvalue y atol= rtol= simple test case NaN SIZE = x = torch rand SIZE SIZE SIZE dtype=dtype device=device x torch arange SIZE torch randint = nan ks = random randint SIZE SIZE SIZE - res val res ind = torch sort x k ks res val res ind = torch kthvalue x k keepdim=False assertEqual res val res val k - atol= rtol= assertEqual res ind res ind k - atol= rtol= dtypes torch float onlyNativeDeviceTypes Fails XLA test_kthvalue_scalar device dtype Test scalar input test case https github com pytorch pytorch issues Tests passing scalar tensor D tensor element work either way res = torch tensor device=device dtype=dtype kthvalue ref = torch tensor device=device dtype=dtype kthvalue assertEqual res ref squeeze assertEqual res ref squeeze dtypes all_types dtypesIfCUDA all_types_and torch half test_isin device dtype assert_isin_equal b Compare numpy reference implementation x = torch isin b = cpu numpy torch is_tensor np array b = b cpu numpy torch is_tensor b np array b y = np isin b assertEqual x y multi-dim tensor multi-dim tensor = torch arange device=device dtype=dtype reshape b = torch tensor device=device dtype=dtype assert_isin_equal b zero-dim tensor zero_d = torch tensor device=device dtype=dtype assert_isin_equal zero_d b assert_isin_equal zero_d assert_isin_equal zero_d zero_d empty tensor empty = torch tensor device=device dtype=dtype assert_isin_equal empty b assert_isin_equal empty assert_isin_equal empty empty scalar assert_isin_equal assert_isin_equal b define_expected lst invert=False expected = torch tensor lst device=device invert expected = expected logical_not expected Adapted numpy s d tests mult invert False True = torch tensor device=device dtype=dtype b = torch tensor mult device=device dtype=dtype ec = define_expected True False True True invert=invert c = torch isin b assume_unique=True invert=invert assertEqual c ec = ec = define_expected False False True True invert=invert c = torch isin b assume_unique=True invert=invert assertEqual c ec = ec = define_expected True False True False invert=invert c = torch isin b assume_unique=True invert=invert assertEqual c ec = torch tensor device=device dtype=dtype b = torch tensor mult device=device dtype=dtype ec = define_expected False True False True True True True True True False True False False False invert=invert c = torch isin b invert=invert assertEqual c ec b = torch tensor mult + mult device=device dtype=dtype ec = define_expected True True True True True True True True True True True False True True invert=invert c = torch isin b invert=invert assertEqual c ec = torch tensor device=device dtype=dtype b = torch tensor mult device=device dtype=dtype ec = define_expected True False True True invert=invert c = torch isin b invert=invert assertEqual c ec = torch tensor device=device dtype=dtype b = torch tensor mult device=device dtype=dtype ec = define_expected True False True True True invert=invert c = torch isin b invert=invert assertEqual c ec = torch tensor device=device dtype=dtype b = torch tensor mult device=device dtype=dtype ec = define_expected False False invert=invert c = torch isin b invert=invert assertEqual c ec multi-dimensional input case using sort-based algo assume_unique False True = torch arange device=device dtype=dtype reshape b = torch arange device=device dtype=dtype ec = define_expected False False False True True True invert=invert c = torch isin b invert=invert assume_unique=assume_unique assertEqual c ec test_isin_different_dtypes device supported_types = all_types device == cpu all_types_and torch half mult assume_unique False True dtype dtype product supported_types supported_types = torch tensor device=device dtype=dtype b = torch tensor mult device=device dtype=dtype ec = torch tensor False False True device=device c = torch isin b assume_unique=assume_unique assertEqual c ec onlyCUDA dtypes all_types test_isin_different_devices device dtype = torch arange device=device dtype=dtype reshape b = torch arange device= cpu dtype=dtype assertRaises RuntimeError torch isin b c = torch arange device= cpu dtype=dtype reshape d = torch arange device=device dtype=dtype assertRaises RuntimeError torch isin c d dtypes integral_types test_sort_overflow device dtype Regression test https github com pytorch pytorch issues prev_num_threads = torch get_num_threads try low = dtype == torch uint - x = torch full low dtype=dtype device=device x = torch iinfo x dtype max torch set_num_threads uv = x sort values unique assertEqual uv size finally torch set_num_threads prev_num_threads instantiate_device_type_tests TestSortAndSelect globals __name__ == __main__ run_tests