mypy allow-untyped-defs inspect types warnings collections abc Callable Sequence functools wraps types GenericAlias typing NamedTuple Optional overload TypeVar Union typing_extensions ParamSpec torch torch _prims_common utils torch _prims_common CustomOutParamAnnotation ELEMENTWISE_TYPE_PROMOTION_KIND Number NumberType ShapeType TensorLike TensorLikeType torch utils _pytree pytree torch utils _pytree tree_flatten tree_unflatten _T = TypeVar _T _P = ParamSpec _P overload pyrefly ignore bad-return _maybe_convert_to_dtype TensorLikeType dtype torch dtype - TensorLikeType pass overload pyrefly ignore bad-return _maybe_convert_to_dtype NumberType dtype torch dtype - NumberType pass overload pyrefly ignore bad-return _maybe_convert_to_dtype Sequence dtype torch dtype - Sequence pass overload _maybe_convert_to_dtype None dtype torch dtype - None pass TODO implement ref cast option enforce safe casting _maybe_convert_to_dtype dtype isinstance TensorLike dtype = dtype dtype isinstance Number utils dtype_to_type_ctor dtype type ignore arg-type isinstance Sequence tuple _maybe_convert_to_dtype x dtype x Passthrough None because some functions wrapped type promotion wrapper might have optional args None None raise ValueError f Received unsupported type type Expected TensorLike Number Sequence _maybe_convert_to_type NumberType typ type - NumberType isinstance Number msg = f Found unknown type type when trying convert scalars raise ValueError msg utils is_weakly_lesser_type type typ msg = f Scalar type type cannot safely cast type typ raise ValueError msg typ _annotation_has_type typ annotation hasattr annotation __args__ annotation __args__ _annotation_has_type typ=typ annotation=a True False typ annotation elementwise_type_promotion_wrapper Adds elementwise type promotion Python reference implementation Takes two kwargs type_promoting_args type_promotion_kind type_promoting_args must string Sequence specifying argument names all arguments participate type promotion should type promoted If arg specifies Sequence-type then every element Sequence will participate type promotion type_promotion_kind must one kinds specified ELEMENTWISE_TYPE_PROMOTION_KIND See its documentation details The return_dtype will coerced wrapped function s dtype arg available None Other type promotion behavior like validating Python type scalar arguments must handled separately __init__ type_promotion_kind ELEMENTWISE_TYPE_PROMOTION_KIND type_promoting_args Optional Sequence str = None type_promoting_arg_names = type_promoting_args type_promotion_kind = type_promotion_kind __call__ fn Callable - Callable sig = inspect signature fn TorchDynamo tracing inspect causes fake tensor dynamo_wrapped tests fail PYTORCH_TEST_WITH_DYNAMO= python test test_fake_tensor py FakeTensorTest test_basic torch _disable_dynamo wraps fn _fn args kwargs bound = sig bind args kwargs type_promoting_args = tuple bound arguments x x type_promoting_arg_names type ignore union-attr x bound arguments keys flattened_type_promoting_args = pytree arg_tree_leaves type_promoting_args compute_dtype result_dtype = utils elementwise_dtypes flattened_type_promoting_args type_promotion_kind=self type_promotion_kind promoted_args = x _maybe_convert_to_dtype bound arguments x compute_dtype x type_promoting_arg_names type ignore union-attr x bound arguments keys bound arguments update promoted_args result = fn bound arguments Override return_dtype dtype arg present None dtype bound arguments maybe_dtype = bound arguments dtype maybe_dtype dtype cannot None result_dtype = maybe_dtype isinstance result TensorLike _maybe_convert_to_dtype result result_dtype isinstance result Sequence tuple _maybe_convert_to_dtype x result_dtype x result raise AssertionError f Unhandled result type type result _fn __signature__ = sig type ignore attr-defined _fn Returns True resize necessary _resize_output_check out TensorLikeType shape ShapeType If shapes correct there s nothing do utils same_shape out shape shape False out numel = msg = f An output one more elements resized since had shape str out shape which does match required output shape str shape This behavior deprecated future PyTorch release outputs will resized unless they have zero elements You can explicitly reuse out tensor t resizing inplace zero elements t resize_ warnings warn msg stacklevel= True TODO handle tuples tensors _maybe_resize_out out TensorLikeType shape ShapeType memory_format Optional torch memory_format = None _resize_output_check out shape out resize_ shape memory_format=memory_format out is_cpu_scalar x TensorLikeType - bool x dim == x device type == cpu check_copy_devices copy_from TensorLikeType copy_to TensorLikeType - None copy_from device = copy_to device msg = f Attempting copy device copy_from device f device copy_to device cross-device copies allowed raise RuntimeError msg _safe_copy_out copy_from TensorLikeType copy_to TensorLikeType exact_dtype bool = False Checks same device is_cpu_scalar copy_from check_copy_devices copy_from=copy_from copy_to=copy_to Checks safe cast exact_dtype torch _check copy_from dtype == copy_to dtype lambda f Expected out tensor have dtype copy_from dtype f got copy_to dtype instead torch _check utils can_safe_cast_to cast_from=copy_from dtype cast_to=copy_to dtype lambda f Attempting cast copy_from dtype out tensor dtype copy_to dtype can t cast because safe copy_to copy_ copy_from out_wrapper out_names str exact_dtype bool = False pass_is_out bool = False preserve_memory_format bool = False - Callable Callable _P _T Callable _P _T The wrapped function needs convert output parameters ensure compatibility between Python API which always uses out parameter name may tuple Aten API which may have multiple output parameters use different parameter names such grad_input indices values default_out_names = out len out_names == Use default out name out_names = default_out_names is_tensor = len out_names == maybe_compute_memory_format t utils suggest_memory_format t preserve_memory_format None _out_wrapper fn Callable _P _T - Callable _P _T Adds out parameter Python reference out_type = TensorLikeType is_tensor GenericAlias tuple tuple TensorLikeType _ range len out_names For backward compatibility - should able remove once PEP conversion complete bc_out_type = TensorLikeType is_tensor types GenericAlias tuple tuple TensorLikeType _ range len out_names return_type = TensorLikeType is_tensor NamedTuple f return_types_ fn __name__ pyrefly ignore bad-argument-count o TensorLikeType o out_names sig = inspect signature fn factory_kwargs = device dtype is_factory_fn = all p sig parameters p factory_kwargs wraps fn _fn args _P args kwargs _P kwargs out = kwargs pop out None is_factory_fn out None k factory_kwargs out_attr = getattr out k k kwargs kwargs k = out_attr maybe_check_copy_devices out pyrefly ignore unsupported-operation isinstance out TensorLike isinstance args TensorLike check_copy_devices copy_from=args copy_to=out isinstance out tuple list o out maybe_check_copy_devices o maybe_check_copy_devices out pass_is_out result = fn args is_out= out None kwargs type ignore arg-type result = fn args kwargs result NotImplemented NotImplemented assert isinstance result TensorLike is_tensor isinstance result tuple type ignore arg-type len result == len out_names type ignore arg-type fn __name__ == unbind isinstance result list tuple type ignore arg-type unbind_copy special case see https github com pytorch pytorch issues out None Naively you might expect assert true s assert type out type result The reason functions under wrapper can get registered Meta dispatch key means they can executed context where tensor subclasses disabled no_dispatch which handy way is-a tensor subclass e g FakeTensor have normal meta backend create meta tensor wrapped once gets returned In situation you will get FakeTensor output tensor result -- which will normal meta tensor perfectly harmless is_tensor fn __name__ = unbind assert isinstance out TensorLike These two operations done in-place _maybe_resize_out out result shape type ignore union-attr maybe_compute_memory_format result _safe_copy_out copy_from=result type ignore arg-type copy_to=out exact_dtype=exact_dtype fn __name__ = unbind assert isinstance out tuple type ignore arg-type assert isinstance out list tuple type ignore arg-type torch _check_type len out == len result type ignore arg-type lambda f expected tuple len result elements got len out type ignore arg-type r o zip result out type ignore arg-type These two operations done in-place _maybe_resize_out o r shape maybe_compute_memory_format r _safe_copy_out copy_from=r copy_to=o exact_dtype=exact_dtype type ignore arg-type out = result mypy does see through definition out_type given s different scope out is_tensor return_type out type ignore operator out_param = inspect Parameter out kind=inspect Parameter KEYWORD_ONLY default=None annotation=out_type Mark function now returns tuple assert isinstance sig return_annotation str TypeVar sig return_annotation sig empty out_type bc_out_type params = sig parameters values out_param If there s Parameter VAR_KEYWORD parameter like kwds must appear after out= parameter which Parameter KEYWORD_ONLY Sorting Parameter kind guarantees all parameters legal order params = sorted params key=lambda p p kind _fn __signature__ = inspect Signature type ignore attr-defined parameters=params return_annotation=return_type type ignore arg-type _fn __annotations__ = dict getattr fn __annotations__ _fn __annotations__ out = out_type _fn __annotations__ = return_type In special case having single tensor out parameter name other than out add special annotation name parameter is_tensor out_names = default_out_names _fn __annotations__ CustomOutParamAnnotation = out_names Add indicator attribute can used special cases where having function wrapped ` out_wrapper ` desirable e g jit _fn _torch_decompositions_out_wrapper = type ignore attr-defined f This function wrapped out_wrapper __module__ out_wrapper _fn _out_wrapper _maybe_remove_out_wrapper fn Callable inspect unwrap fn stop=lambda f hasattr f _torch_decompositions_out_wrapper backwards_not_supported prim redispatch_prim args kwargs torch _C _AutoDispatchBelowAutograd prim args kwargs BackwardsNotSupported torch autograd Function staticmethod pyrefly ignore bad-override forward ctx args_spec flat_args args kwargs = tree_unflatten flat_args args_spec type ignore arg-type redispatch_prim args kwargs staticmethod backward ctx args raise RuntimeError backwards supported prim wraps prim _autograd_impl args kwargs flat_args args_spec = tree_flatten args kwargs torch is_grad_enabled any requires_grad flat_args isinstance torch Tensor TODO There subtle bug here prims like copy_to their input argument after mutating custom autograd function will incorrectly turn result into view which will fail test_python_ref_executor tests At moment we sidestep observing unit tests don t ever try run executor autograd so we don t exercise buggy case you ever want feed autograd through aware We need way properly implementing autograd mutating operations Python do BackwardsNotSupported apply args_spec flat_args redispatch_prim args kwargs _autograd_impl TODO when tracing will add torch tensors TensorMeta objects trace -- we should fix adding tracing context NumberMeta classes TODO wrapper currently untested elementwise_unary_scalar_wrapper fn Callable _P _T - Callable _P Union _T NumberType Allows unary operators accept tensors work Python numbers sig = inspect signature fn wraps fn _fn args kwargs len args isinstance args Number dtype = utils type_to_dtype type args args_ = list args args_ = torch tensor args dtype=dtype pyrefly ignore invalid-param-spec result = fn args_ kwargs assert isinstance result torch Tensor result item pyrefly ignore invalid-param-spec fn args kwargs _fn __signature__ = sig type ignore attr-defined pyrefly ignore bad-return _fn