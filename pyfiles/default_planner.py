mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates dataclasses io logging operator collections ChainMap functools reduce typing Any cast Optional Union torch torch distributed _shard _utils narrow_tensor_by_index torch distributed checkpoint _dedup_save_plans dedup_save_plans torch distributed checkpoint _nested_dict FLATTEN_MAPPING flatten_state_dict torch distributed checkpoint _sharded_tensor_utils _flatten_sharded_tensors torch distributed checkpoint _traverse set_element torch distributed checkpoint metadata BytesStorageMetadata ChunkStorageMetadata Metadata MetadataIndex STATE_DICT_TYPE STORAGE_TYPES StorageMeta TensorStorageMetadata torch distributed checkpoint planner LoadPlan LoadPlanner ReadItem SavePlan SavePlanner WriteItem WriteItemType torch distributed checkpoint planner_helpers _compare_save_plans _contains_usable_plan _create_default_metadata_only_plan _create_read_items _create_write_items _init_state_dict _merge_delta_local_plans torch distributed checkpoint utils find_state_dict_object torch distributed tensor DTensor _version logger logging Logger = logging getLogger __name__ __all__ = DefaultSavePlanner DefaultLoadPlanner create_default_local_load_plan create_default_global_load_plan create_default_local_save_plan create_default_global_save_plan TODO Update docstrings default_planner py DefaultSavePlanner SavePlanner mappings FLATTEN_MAPPING __init__ flatten_state_dict bool = True flatten_sharded_tensors bool = True dedup_replicated_tensors Optional bool = None dedup_save_to_lowest_rank bool = False enable_plan_caching bool = False - None flatten_state_dict = flatten_state_dict flatten_sharded_tensors = flatten_sharded_tensors mappings = dedup_save_to_lowest_rank = dedup_save_to_lowest_rank dedup_replicated_tensors None logger warning DefaultSavePlanner s ` dedup_replicated_tensors ` argument being deprecated no longer has any effect Please remove argument your call _cached_plans_key str = __class__ __name__ _enable_plan_caching = enable_plan_caching set_up_planner state_dict STATE_DICT_TYPE storage_meta Optional StorageMeta = None is_coordinator bool = False - None flatten_state_dict state_dict mappings = flatten_state_dict state_dict flatten_sharded_tensors state_dict = _flatten_sharded_tensors state_dict state_dict = state_dict is_coordinator = is_coordinator create_local_plan - SavePlan plan = create_default_local_save_plan state_dict is_coordinator flatten_state_dict plan = dataclasses replace plan planner_data=self mappings plan = plan _enable_plan_caching If plans equal we can skip sending plan coordinator _cached_plans_key SavePlanner _cached_save_plan _compare_save_plans plan SavePlanner _cached_save_plan _cached_plans_key logger info No change local plan Skipping sending plan coordinator SavePlan usable=False SavePlanner _cached_save_plan _cached_plans_key = plan plan _dedup_save_plans all_plans list SavePlan - list SavePlan dedup_save_plans all_plans dedup_save_to_lowest_rank _create_global_plan all_plans list SavePlan - tuple list SavePlan Metadata deduped_plans = _dedup_save_plans all_plans global_plan metadata = create_default_global_save_plan deduped_plans flatten_state_dict &#124; does work Python older version merged_mappings = reduce lambda x y x &#124; y p planner_data p global_plan planner_data_dict = p planner_data p global_plan merged_mappings = dict ChainMap planner_data_dict metadata = dataclasses replace metadata planner_data=merged_mappings _validate_global_plan global_plan metadata raise ValueError Failed validate global plan global_plan metadata _create_global_plan_with_caching all_plans list SavePlan - tuple list SavePlan list SavePlan Metadata Create global plan caching Returns tuple global_plan_delta global_plan metadata global_plan_delta list SavePlan = _cached_plans_key SavePlanner _cached_all_plans Case If plans cached cache will hydrated all_plans global_plans Deduped metadata Cache original all_plans SavePlanner _cached_all_plans _cached_plans_key = all_plans global_plan metadata = _create_global_plan all_plans Cache deduped validated global_plan SavePlanner _cached_global_plan _cached_plans_key = global_plan Cache metadata SavePlanner _cached_metadata _cached_plans_key = metadata If plans cached global_plan delta will same global plan global_plan global_plan metadata Case Plans cached _contains_usable_plan all_plans Case Plans cached local plans have NOT changed No usable plans Global plan delta will empty plans avoid collective overhead We can reuse deduped global plan metadata cache directly global_plan_delta = SavePlan usable=False len all_plans global_plan = SavePlanner _cached_global_plan _cached_plans_key metadata = SavePlanner _cached_metadata _cached_plans_key Case Plans cached local plans have changed We will merge changed local plans cached local plans Updated plans will overwrite cached plans New global plan metadata will created cached Global plan delta will created comparing new global plan cached global plan Only global plan delta updated ones will sent coordinator avoid collective overhead merged_plans = _merge_delta_local_plans SavePlanner _cached_all_plans _cached_plans_key all_plans Cache updated local plans SavePlanner _cached_all_plans _cached_plans_key = merged_plans global_plan metadata = _create_global_plan merged_plans _cached_plans_key _cached_global_plan cached_plan new_plan zip SavePlanner _cached_global_plan _cached_plans_key global_plan _compare_save_plans cached_plan new_plan global_plan_delta append SavePlan usable=False global_plan_delta append new_plan Cache new global plan metadata SavePlanner _cached_global_plan _cached_plans_key = global_plan SavePlanner _cached_metadata _cached_plans_key = metadata global_plan_delta global_plan metadata create_global_plan all_plans list SavePlan - tuple list SavePlan Metadata global_plan_delta list SavePlan = _enable_plan_caching If plans cached we only need send global plan delta scattered across ranks Ranks will use cached final plans instead global_plan_delta global_plan metadata = _create_global_plan_with_caching all_plans global_plan metadata = _create_global_plan all_plans If caching enabled global delta plan will always same new global plan global_plan_delta = global_plan global_plan = global_plan metadata = metadata global_plan_delta metadata _finish_plan_with_caching new_plan SavePlan - SavePlan finished_plan SavePlan = new_plan new_plan usable finished_plan = SavePlanner _cached_final_save_plan _cached_plans_key finished_plan = new_plan SavePlanner _cached_final_save_plan _cached_plans_key = new_plan finished_plan finish_plan new_plan SavePlan - SavePlan finished_plan SavePlan = new_plan _enable_plan_caching finished_plan = _finish_plan_with_caching new_plan plan = finished_plan plan resolve_data write_item WriteItem - Union torch Tensor io BytesIO object = lookup_object write_item index transform_object write_item object lookup_object index MetadataIndex - Any Extension planner interface make easy extend default planner find_state_dict_object state_dict index transform_object write_item WriteItem object Any Extension planner interface make easy extend default planner write_item type == WriteItemType BYTE_IO bytes = io BytesIO torch save object bytes object = bytes object DefaultLoadPlanner LoadPlanner DefaultLoadPlanner adds multiple features top LoadPlanner In particular adds following flatten_state_dict Handle state_dict nested dicts flatten_sharded_tensors For FSDP D parallel mode allow_partial_load If False will raise runtime error key present state_dict checkpoint original_state_dict STATE_DICT_TYPE mappings FLATTEN_MAPPING __init__ flatten_state_dict bool = True flatten_sharded_tensors bool = True allow_partial_load bool = False - None flatten_state_dict = flatten_state_dict flatten_sharded_tensors = flatten_sharded_tensors original_state_dict = mappings = allow_partial_load = allow_partial_load set_up_planner state_dict STATE_DICT_TYPE metadata Optional Metadata = None is_coordinator bool = False - None _init_state_dict state_dict original_state_dict = state_dict flatten_sharded_tensors state_dict = _flatten_sharded_tensors state_dict flatten_state_dict state_dict mappings = flatten_state_dict state_dict state_dict = state_dict metadata = metadata is_coordinator = is_coordinator create_local_plan - LoadPlan metadata None raise AssertionError metadata None flatten_state_dict To support checkpoints saved before v we have differentiate missing keys due old checkpoints The contracts There cases when we found missing key Actual missing key allow_partial_load False Actual missing key allow_partial load True Old checkpoint allow_partial_load False Old checkpoint allow_partial_load True If we found missing key we first convert keys back key format v If previous missing keys v keys we assume old checkpoint Pass state_dict ` create_default_local_load_plan ` which has logic check missing allow_partial_load So cases we delegate allow_partial_load check ` create_default_local_load_plan ` The logic here determine whether checkpoint belong before after current_keys = set state_dict keys load_keys = set metadata state_dict_metadata keys missing_keys = load_keys - current_keys missing_keys _version _derived_version = _ old_state_dict old_mappings = flatten_state_dict original_state_dict old_keys = set old_state_dict keys old_keys missing_keys state_dict mappings = old_state_dict old_mappings _derived_version only used flatten_state_dict now Set back None so later we can save new version _version _derived_version = None create_default_local_load_plan state_dict metadata allow_partial_load create_global_plan global_plan list LoadPlan - list LoadPlan create_default_global_load_plan global_plan finish_plan new_plan LoadPlan - LoadPlan new_plan load_bytes read_item ReadItem value io BytesIO - None flatten_state_dict set_element original_state_dict mappings read_item dest_index fqn torch load value weights_only=False state_dict read_item dest_index fqn = torch load value weights_only=False resolve_tensor read_item ReadItem tensor = lookup_tensor read_item dest_index transform_tensor read_item tensor commit_tensor read_item ReadItem tensor torch Tensor - None pass lookup_tensor index MetadataIndex - torch Tensor Extension planner interface make easy extend default planner find_state_dict_object state_dict index transform_tensor read_item ReadItem tensor torch Tensor Extension planner interface make easy extend default planner narrow_tensor_by_index tensor read_item dest_offsets read_item lengths _EmptyStateDictLoadPlanner DefaultLoadPlanner Extension DefaultLoadPlanner which rebuilds state_dict saved metadata Useful loading state_dict without first initializing model such when converting DCP checkpoint into Torch save file N B ` state_dict ` must empty dictionary when used LoadPlanner warning Because entire state dict initialized It s recommended only utilize LoadPlanner single rank process avoid OOM __init__ keys=None args kwargs keys = keys super __init__ args kwargs _should_include_key key str metadata Metadata - bool keys None True key keys True unflattened_keys list str = planner_data = metadata planner_data get key unflattened_key planner_data unflattened_keys unflattened_keys append join unflattened_keys - str unflattened_key unflattened_keys append unflattened_key any unflattened_key keys unflattened_key unflattened_keys True False set_up_planner state_dict STATE_DICT_TYPE metadata Optional Metadata = None is_coordinator bool = False - None state_dict raise AssertionError state_dict metadata None raise AssertionError metadata None rebuild state dict metadata k v metadata state_dict_metadata items _should_include_key k metadata continue isinstance v TensorStorageMetadata v = torch empty v size dtype=v properties dtype type ignore assignment metadata planner_data None k metadata planner_data set_element state_dict metadata planner_data k v state_dict k = v super set_up_planner state_dict metadata is_coordinator create_default_local_load_plan state_dict dict str Any metadata Metadata strict bool = True - LoadPlan requests = Create ` ` LoadPlan ` ` used DefaultLoadPlanner It produces one read item per value ` ` state_dict ` ` using metadata ` ` metadata ` ` The default behavior match key exactly between state_dict metadata It handles resharding issuing multiple read requests against storage order match load requirements fqn obj state_dict items ignore state_dict keys which do exist ` state_dict ` strict=False fqn metadata state_dict_metadata strict raise RuntimeError f Missing key checkpoint state_dict fqn continue md = metadata state_dict_metadata fqn isinstance md TensorStorageMetadata getattr obj size None None md size = obj size raise ValueError f Size mismatch between saved md size current obj size fqn Since DTensor supports submesh adding extra check ensure _create_read_items gets called only when current rank part mesh corresponding DTensor isinstance obj DTensor obj device_mesh get_coordinate None requests += _create_read_items fqn md obj requests += _create_read_items fqn md obj LoadPlan requests create_default_global_load_plan all_plans list LoadPlan - list LoadPlan Create global load plan used DefaultLoadPlanner The default load behavior involved no global coordination function currently doesn t change local plans all_plans create_default_local_save_plan state_dict dict str Any is_coordinator bool - SavePlan Create ` ` SavePlan ` ` used DefaultSavePlanner On non-coordinator ranks function ignores tensors non-tensor objects only producing writes ShardedTensor objects On coordinator rank produce writes all values requests = fqn obj state_dict items Since DTensor supports submesh adding extra check ensure _create_write_items gets called only when current rank part mesh corresponding DTensor isinstance obj DTensor obj device_mesh get_coordinate None requests += _create_write_items fqn obj For plain tensor non-tensor values add request all ranks Coordinator will decides whether deduplicate values based keys requests += _create_write_items fqn obj SavePlan requests create_default_global_save_plan all_plans list SavePlan rewrite_index_hints bool = True - tuple list SavePlan Metadata Create global plan metadata used DefaultSavePlanner Metadata produced concatenating metadata all ` ` WriteItem ` ` supplied plans The only global planning change update index hints all ` ` MetadataIndex ` ` objects ` ` rewrite_index_hints ` ` True md dict str STORAGE_TYPES = new_plans = plan all_plans new_items = item plan items item type = WriteItemType SHARD item index fqn md raise AssertionError item index fqn md item type == WriteItemType BYTE_IO md item index fqn = BytesStorageMetadata new_items append item item tensor_data None raise AssertionError item tensor_data None tensor_md = cast TensorStorageMetadata md setdefault item index fqn TensorStorageMetadata properties=item tensor_data properties size=item tensor_data size chunks= new_item = item rewrite_index_hints new_index = dataclasses replace item index index=len tensor_md chunks new_item = dataclasses replace item index=new_index new_items append new_item item tensor_data chunk None raise AssertionError f Cannot create MD tensor without bounds FQN item index fqn tensor_md chunks append item tensor_data chunk new_plans append dataclasses replace plan items=new_items new_plans Metadata md _create_default_local_metadata state_dict STATE_DICT_TYPE - Metadata Return ` ` Metadata ` ` DefaultSavePlanner used checkpoint ` ` state_dict ` ` plan = _create_default_metadata_only_plan state_dict _ md = create_default_global_save_plan plan md _check_box_overlap box ChunkStorageMetadata box ChunkStorageMetadata - bool Check two boxes overlap Tuples offset lengths For each dim each shard check one shard resides other end second shard respect dim As example D shard we would check one shard above left other shard ndims = len box offsets i range ndims box offsets i = box offsets i + box sizes i False box offsets i = box offsets i + box sizes i False True _check_box_bounds outer_box_size torch Size inner_box ChunkStorageMetadata - bool i range len outer_box_size inner_box offsets i False inner_box sizes i False inner_box offsets i + inner_box sizes i outer_box_size i False True _validate_global_plan global_plan list SavePlan metadata Metadata - bool all_good = True key value metadata state_dict_metadata items isinstance value BytesStorageMetadata continue len value size == continue chunks_volume = chunk_idx chunk enumerate value chunks Compute volume _check_box_bounds value size chunk logger warning key s has out bounds chunk tensor-size s chunk s key value size chunk all_good = False chunks_volume += reduce operator mul chunk sizes Check overlap chunk value chunks chunk_idx + _check_box_overlap chunk chunk logger warning key s has overlapping chunks s s key chunk chunk all_good = False Check whether combined chunk cover whole tensor tensor_volume = reduce operator mul value size len global_plan chunks_volume = tensor_volume logger warning key s invalid fill tensor-volume s chunks-volume s key tensor_volume chunks_volume all_good = False all_good