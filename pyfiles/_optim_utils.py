mypy allow-untyped-defs copy functools logging warnings collections abc Iterable Iterator Sequence contextlib ExitStack dataclasses dataclass field itertools chain typing Any cast NamedTuple no_type_check Optional TYPE_CHECKING Union torch torch distributed dist torch distributed fsdp _traversal_utils traversal_utils torch nn nn torch distributed _state_dict_utils _gather_state_dict torch distributed distributed_c d _get_pg_default_device torch distributed fsdp _common_utils _apply_to_modules _FSDPState _get_module_fsdp_state_if_fully_sharded_module _get_param_to_fqns _module_handle _named_parameters_with_duplicates clean_tensor_name torch distributed fsdp _debug_utils SimpleProfiler torch distributed fsdp _flat_param FlatParameter FlatParamHandle torch distributed fsdp _fsdp_extensions _ext_chunk_dtensor _ext_chunk_tensor torch distributed fsdp _runtime_utils _lazy_init _reset_flat_param_grad_info_if_needed torch distributed fsdp api ShardingStrategy StateDictSettings StateDictType torch distributed tensor DTensor Replicate torch utils _pytree tree_map_only TYPE_CHECKING torch distributed _shard sharded_tensor ShardedTensor logger = logging getLogger __name__ dataclass FSDPParamInfo state _FSDPState handle FlatParamHandle param_indices dict str int param_requires_grad list bool sorted_items dictionary dict str Any - Iterator tuple str Any keys = sorted dictionary keys k keys yield k dictionary k dataclass _ConsolidatedOptimState This holds consolidated optimizer state target rank Positive- dimension tensor state communicated across ranks while zero-dimension tensor state non-tensor state taken directly target rank PyTorch version moved using zero-dimension tensors scalar values user implemented optimizers may still use float i e non-tensor Thus we support both handle them identically Attributes tensor_state Dict str torch Tensor Mapping positive-dimension tensor state name unsharded flat tensor representing state zero_dim_tensor_state Dict str torch Tensor Mapping zero- dimension tensor state name its value non_tensor_state Dict str Any Mapping non-tensor state name its value tensor_state dict str torch Tensor = field default_factory=dict zero_dim_tensor_state dict str torch Tensor = field default_factory=dict non_tensor_state dict str Any = field default_factory=dict _PosDimTensorInfo NamedTuple Metadata positive-dimension tensors used internally meth ` scatter_full_optim_state_dict ` Attributes shape torch Size Sharded tensor shape which equal unsharded tensor shape tensor optimizer state non-FSDP parameter hence sharded dtype torch dtype Data type tensor shape torch Size dtype torch dtype _OptimStateKey NamedTuple This represents optimizer state key may used commonly across ranks It based unflattened parameter names rather than parameter IDs make independent each rank s own optimizer construction unflat_param_names tuple str is_fsdp_managed bool _unflatten_optim_state fsdp_param_info FSDPParamInfo flat_param_state dict str Any to_save bool shard_state bool cpu_offload bool - list dict str Any Unflattens optimizer state consisting state part param_groups part Unflattening state part involves consolidating state target rank remapping flattened unflattened parameter IDs param_groups part only involves remapping flattened unflattened parameter IDs Args fsdp_param_info FSDPParamInfo The FSDP state handle mapping FQN original parameter index flat_param_state Dict str Any Entry flat parameter state part optimizer state dict to_save bool Whether save state rank Returns List Dict str Any A ` list ` holding entries state part optimizer state dict corresponding unflattened parameters comprising flat parameter target rank empty ` list ` otherwise The final optimizer state dict will need map these entries using proper unflattened parameter IDs shard_state to_save raise AssertionError If ` ` shard_state ` ` True ` ` to_save ` ` has True consolidated_state = _communicate_optim_state fsdp_param_info flat_param_state to_save unflat_param_state = _unflatten_communicated_optim_state fsdp_param_info consolidated_state shard_state optim_state unflat_param_state We can t use items below cuz we d run into concurrent modification error cpu_offload key list optim_state keys state = optim_state key isinstance state torch Tensor continue optim_state key = state cpu unflat_param_state _is_zero_dim_tensor x Any - bool torch is_tensor x x dim == _communicate_optim_state fsdp_param_info FSDPParamInfo flat_param_state dict str Any - _ConsolidatedOptimState Communicates optimizer state flat parameter across ranks All ranks will hold entire non-sharded optimizer state GPU If ` ` N ` ` number tensor optimizer states optimizer state dict then communication complexity ` ` N = ` ` ` ` N + ` ` otherwise where plus comes all-gathering padding per rank Args fsdp_param_info FSDPParamInfo The FSDP state handle mapping FQN original parameter index flat_param_state Dict str Any The entry state part optimizer state dict corresponding flat parameter Returns ConsolidatedOptimState Consolidated optimizer state target flat parameter fsdp_state = fsdp_param_info state flat_param = fsdp_param_info handle flat_param state = _ConsolidatedOptimState tensor_state zero_dim_tensor_state non_tensor_state = state tensor_state state zero_dim_tensor_state state non_tensor_state state_name value sorted_items flat_param_state Positive-dimension tensor state communicate across ranks torch is_tensor value value dim If parameter sharded then neither positive-dimension tensor state so no need communicate -- we take target rank s value fsdp_state world_size == fsdp_state sharding_strategy == ShardingStrategy NO_SHARD tensor_state state_name = value continue fsdp_state compute_device None raise AssertionError compute_device has been initialized value device type = fsdp_state compute_device type value = value fsdp_state compute_device Assume positive-dimension tensor optimizer state has same shape sharded flat parameter buffer_size = flat_param _full_param_padded size type ignore attr-defined tensor_buffer = value new_zeros buffer_size dist all_gather_into_tensor tensor_buffer value group=fsdp_state process_group fsdp_state _device_handle synchronize unpadded_numel = cast nn Parameter flat_param _unpadded_unsharded_size numel tensor_state state_name = tensor_buffer unpadded_numel Zero-dimension tensor state non-tensor state take rank s value directly _is_zero_dim_tensor value zero_dim_tensor_state state_name = value detach clone non_tensor_state state_name = value state _unflatten_communicated_optim_state fsdp_param_info FSDPParamInfo state _ConsolidatedOptimState shard_state bool - list dict str Any Unflattens communicated optimizer state given ` ` tensor_state ` ` ` ` non_tensor_state ` ` ` ` zero_dim_tensor_state ` ` single flat parameter This should only called target rank Args fsdp_param_info FSDPParamInfo The FSDP state handle mapping FQN original parameter index state _ConsolidatedOptimState Consolidated optimizer state Returns List Dict str Any A ` list ` holding entries state part optimizer state dict corresponding unflattened parameters comprising flat parameter The final optimizer state dict will need map these entries using proper unflattened parameter IDs fsdp_state = fsdp_param_info state handle = fsdp_param_info handle flat_param = handle flat_param unflat_param_state list dict str Any = flat_param_views dict str Iterator = num_unflat_params = flat_param _num_params tensor_state zero_dim_tensor_state non_tensor_state = state tensor_state state zero_dim_tensor_state state non_tensor_state _ range num_unflat_params unflat_state_param = Add positive-dimension tensor state unflatten views state_name flat_tensor sorted_items tensor_state views_generated = state_name flat_param_views views_generated views = handle _get_unflat_views flat_tensor flat_param_views state_name = views views = flat_param_views state_name optim_state Union torch Tensor ShardedTensor DTensor = next views shard_state osd_config = fsdp_state _optim_state_dict_config getattr osd_config _use_dtensor False fsdp_state _device_mesh None raise AssertionError f Expected _device_mesh None got fsdp_state _device_mesh optim_state = _ext_chunk_dtensor optim_state fsdp_state rank fsdp_state _device_mesh fsdp_state _fsdp_extension fsdp_state process_group None raise AssertionError f Expected process_group None got fsdp_state process_group optim_state = _ext_chunk_tensor optim_state fsdp_state rank fsdp_state world_size fsdp_state _device_handle device_count fsdp_state process_group fsdp_state _fsdp_extension unflat_state_param state_name = optim_state Add zero-dimension tensor state take target rank s value unflat_state_param update sorted_items zero_dim_tensor_state Add non-tensor state take target rank s value unflat_state_param update sorted_items non_tensor_state unflat_param_state append unflat_state_param unflat_param_state _broadcast_processed_state fsdp_state _FSDPState optim_state dict str Any group Optional dist ProcessGroup - dict str Any objects list Any = None dist get_rank group == objects = tree_map_only torch Tensor lambda v v cpu v dim == _PosDimTensorInfo v shape v dtype type ignore union-attr optim_state dist broadcast_object_list objects src= group=group dist get_rank group == optim_state objects _broadcast_state fsdp_state _FSDPState state Any group Optional dist ProcessGroup - Any dist get_rank group == isinstance state torch Tensor state dim == state tensor = state fsdp_state compute_device isinstance state torch Tensor state dim = raise AssertionError For non-zero ranks tensor state should have zero dimension f got state shape state shape state isinstance state _PosDimTensorInfo state tensor = torch zeros state shape dtype=state dtype device=fsdp_state compute_device dist broadcast tensor src= group=group tensor _shard_orig_param_state fsdp_param_info FSDPParamInfo fqn str optim_state dict str Any - dict str Any Shard optimizer state original parameter name ` ` fqn ` ` This API should only used when ` ` use_orig_params ` ` True optim_state fsdp_state = fsdp_param_info state flat_param = fsdp_param_info handle flat_param param_idx = fsdp_param_info param_indices fqn shard_param_info = flat_param _shard_param_infos param_idx type ignore attr-defined optim_state = _gather_state_dict optim_state pg=fsdp_state process_group device=fsdp_state compute_device shard_param_info in_shard Flatten shard state new_optim_state dict str Any = intra_param_start_idx = shard_param_info intra_param_start_idx intra_param_end_idx = shard_param_info intra_param_end_idx state_name value optim_state items torch is_tensor value value dim fsdp_state sharding_strategy = ShardingStrategy NO_SHARD value = value flatten intra_param_start_idx intra_param_end_idx type ignore operator + clone new_optim_state state_name = value new_optim_state _flatten_optim_state_dict optim_state_dict dict str Any model nn Module use_orig_params bool = False optim Optional torch optim Optimizer = None rank _only bool = False group Optional dist ProcessGroup = None - dict str Any Flattens full optimizer state dict still keying unflattened parameter names If ` ` use_orig_params ` ` True each rank will have all FSDP-managed parameters some these parameters may empty due sharding For regular optim Optimizer states those empty parameters will initialized So when aggregating FQNs across ranks no assert will raised rank even does have all states -- valid FSDP know how aggregate them However FSDP has ignore handling those parameters managed FSDP do exist local rank -- managed other parallelism FSDP does know ho handle aggregate them Note ` ` _flatten_tensor_optim_state ` ` does need ` ` optim ` ` flatten shard state However NamedOptimizer KeyedOptimizer require all states even corresponding parameters empty To end ` ` optim ` ` will used get initial state empty parameters ` ` optim ` ` should only non-None ` ` optim ` KeyedOptimizer NamedOptimizer Returns Dict str Any The flattened optimizer state dict SimpleProfiler reset unflat_osd = optim_state_dict state unflat_osd rank _only raise ValueError ` optim_state_dict ` must have keys state valid optimizer state dict param_to_fqns = _get_param_to_fqns model fqn_to_fsdp_param_info = _get_fqn_to_fsdp_param_info model fsdp_state = next iter fqn_to_fsdp_param_info values state Broadcast unflat_osd without non-scalar tensor rank _only True rank _only unflat_osd = _broadcast_processed_state fsdp_state unflat_osd group=group Construct state part flat_osd_state dict Union _OptimStateKey str Any = unflat_osd_state = unflat_osd state all_state_keys = set unflat_osd_state keys param fqns param_to_fqns items fqn = fqns fqn unflat_osd_state continue all_state_keys difference_update fqns rank _only fqn fqns unflat_osd_state fqn continue state_name unflat_osd_state fqn keys unflat_osd_state fqn state_name = _broadcast_state fsdp_state unflat_osd_state fqn state_name group=group fqn = fqns fqn fqn_to_fsdp_param_info fsdp_param_info = fqn_to_fsdp_param_info fqn use_orig_params SimpleProfiler profile SimpleProfiler Type RESHARDING flat_state = _shard_orig_param_state fsdp_param_info fqn unflat_osd_state fqn flat_state = _flatten_optim_state fsdp_param_info unflat_osd_state fqns key = _OptimStateKey tuple fqns True Only include non-empty states since expected ` torch optim Optimizer ` s unless optimizer KeyedOptimizer NamedOptimizer flat_state flat_osd_state key = flat_state use_orig_params len fqns = raise AssertionError f use_orig_params True there multiple FQNs fqns optim None NamedOptimizer KeyedOptimizer case state = optim state get param None type ignore call-overload state None flat_osd_state key = copy deepcopy state warnings warn f optim_state key rank fsdp_state rank stacklevel= raise RuntimeError f The state key empty This should happen when use_orig_params=True do flatten non-FSDP parameters states len fqns = raise AssertionError f Expected len fqns == got len fqns key = _OptimStateKey tuple fqns False flat_osd_state key = copy copy unflat_osd_state fqn rank _only fqn fqns unflat_osd_state fqn continue state_name param_state list unflat_osd_state fqn items fsdp_state rank Deference tensor so PyTorch can collect memory del unflat_osd_state fqn state_name Move tensor original osd back CPU make original osd unaffected unflat_osd_state fqn state_name = param_state cpu Handle user-defined state states associated parameters key all_state_keys user_state = unflat_osd_state key isinstance user_state torch Tensor rank _only use_orig_params user_state = _broadcast_state fsdp_state user_state group=group flat_osd_state key = copy copy user_state SimpleProfiler dump_and_reset FSDP _flatten_optim_state_dict profiling Construct param_groups part -- copy since will rekeyed later according target rank s optimizer Only copy param_groups exists unflat_osd param_groups unflat_osd flat_osd_param_groups = copy deepcopy unflat_osd param_groups state flat_osd_state param_groups flat_osd_param_groups state flat_osd_state _flatten_optim_state fsdp_param_info FSDPParamInfo unflat_osd_state dict str dict str Any unflat_param_names list str - dict str Any Flattens optimizer state ` ` full_optim_state_dict ` ` single flat parameter ` ` fsdp_param_info ` ` corresponding unflattened parameter names ` ` unflat_param_names ` ` Args fsdp_param_info FSDPParamInfo The FSDP state handle mapping FQN original parameter index unflat_osd_state Dict str Dict str Any The state part optimizer state dict corresponding unflattened parameters unflat_param_names List str A ` list ` unflattened parameter names corresponding flat parameter ` ` flat_param ` ` Returns Dict str Any A ` dict ` mapping state names their values particular flat parameter The sharded optimizer state dict s state part will map key returned value fsdp_state = fsdp_param_info state handle = fsdp_param_info handle flat_param = handle flat_param num_unflat_params = len unflat_param_names num_unflat_params = raise AssertionError Expects least one unflattened parameter corresponding flat parameter unflat_param_shapes = flat_param _shapes num_unflat_param_shapes = len unflat_param_shapes num_unflat_params = num_unflat_param_shapes raise AssertionError f Expects num_unflat_params shapes got num_unflat_param_shapes Check these unflattened parameters have any optimizer state has_state = bool unflat_param_name unflat_osd_state unflat_param_name unflat_param_names If none unflattened parameters comprising flat parameter have any state then we do want entry optimizer state dict any has_state no need flatten any state There may still some unflattened parameters state some without unflat_param_states = _gather_state_dict unflat_osd_state unflat_param_name pg=fsdp_state process_group device=fsdp_state compute_device unflat_param_name unflat_osd_state None unflat_param_name unflat_param_names Check unflattened parameters have same state names state_names = None pyrefly ignore bad-assignment unflat_param_state unflat_param_states unflat_param_state None continue state_names None state_names = set unflat_param_state keys state_names = set unflat_param_state keys raise ValueError Differing optimizer state names unflattened f parameters unflat_param_names state_names None raise AssertionError f Expected state_names None got state_names Flatten state flat_state dict str Optional torch Tensor = state_name state_names state_values = unflat_param_state state_name unflat_param_state None None unflat_param_state unflat_param_states non_none_state_values = v v state_values v None If all ranks have None None value non_none_state_values flat_state state_name = None continue are_pos_dim_tensors = are_zero_dim_tensors = are_non_tensors = True v non_none_state_values are_pos_dim_tensors = torch is_tensor v v dim are_zero_dim_tensors = _is_zero_dim_tensor v are_non_tensors = torch is_tensor v types = type v v non_none_state_values len types = are_pos_dim_tensors are_zero_dim_tensors are_non_tensors raise ValueError f Differing optimizer state types state state_name f values non_none_state_values unflattened parameter f names unflat_param_names are_pos_dim_tensors flat_tensor = _flatten_tensor_optim_state state_name state_values type ignore arg-type unflat_param_names unflat_param_shapes handle Shard flattened tensor immediately minimize max memory usage fsdp_state world_size = fsdp_state sharding_strategy = ShardingStrategy NO_SHARD sharded_flat_tensor _ = FlatParamHandle _get_shard flat_tensor fsdp_state rank fsdp_state world_size sharded_flat_tensor = flat_tensor flat_state state_name = sharded_flat_tensor are_zero_dim_tensors flat_state state_name = _flatten_zero_dim_tensor_optim_state state_name state_values type ignore arg-type unflat_param_names are_non_tensors raise AssertionError f Expected are_non_tensors True got are_non_tensors flat_state state_name = _flatten_non_tensor_optim_state state_name state_values unflat_param_names flat_state _flatten_tensor_optim_state state_name str pos_dim_tensors list torch Tensor unflat_param_names list str unflat_param_shapes Sequence torch Size handle FlatParamHandle - torch Tensor Flattens positive-dimension tensor optimizer state given values ` ` tensors ` ` state ` ` state_name ` ` single flat parameter ` ` handle ` ` corresponding unflattened parameter names ` ` unflat_param_names ` ` unflatted parameter shapes ` ` unflat_param_shapes ` ` This flattens each unflattened parameter s tensor state into one tensor NOTE We use zero tensors any unflattened parameters without state since some value required fill those entries This assumes zero tensor mathematically equivalent having no state which true Adam s exp_avg exp_avg_sq may true all optimizers Args state_name str Optimizer state name pos_dim_tensors List torch Tensor Positive-dimension tensor optimizer state values unflattened parameters corresponding single flat parameter unflat_param_names List str A ` list ` unflattened parameter names corresponding single flat parameter unflat_param_shapes List torch Size Unflattened parameter shapes corresponding single flat parameter handle FlatParamHandle The flat parameter s handle Returns torch Tensor A flat tensor containing optimizer state corresponding ` ` state_name ` ` constructed concatenating unflattened parameter tensor states ` ` pos_dim_tensors ` ` using zero tensors any unflattened parameters without state flat_param = handle flat_param non_none_tensors = t t pos_dim_tensors t None Check all tensors same dtype dtypes = t dtype t non_none_tensors len dtypes = raise ValueError All unflattened parameters comprising single flat parameter must have positive-dimension tensor state f same dtype got dtypes dtypes state state_name f unflattened parameter names unflat_param_names dtype = next iter dtypes Check each tensor state matches its parameter s shape tensor shape zip pos_dim_tensors unflat_param_shapes tensor None len shape == raise ValueError Flattening zero-dimension parameter supported tensor None tensor shape = shape raise ValueError Tensor optimizer state does have same shape its f parameter tensor shape shape Flatten tensor states we do need add any right-hand-side padding since flat optimizer state tensor sharded via ` _get_shard ` which pads shard needed just like flat parameter cpu_device = torch device cpu tensors_to_flatten = torch flatten state_value cpu_device state_value None torch flatten torch zeros size=shape dtype=dtype device=cpu_device state_value shape zip pos_dim_tensors unflat_param_shapes flat_tensor = handle flatten_tensors tensors_to_flatten handle _aligned_numel flat_param_shape = flat_param _unpadded_unsharded_size type ignore attr-defined flat_tensor shape = flat_param_shape raise AssertionError f tensor optim state flat_tensor shape flat parameter flat_param_shape flat_tensor _flatten_zero_dim_tensor_optim_state state_name str zero_dim_tensors list torch Tensor unflat_param_names list str - torch Tensor Flattens zero-dimension tensor optimizer state given values ` ` zero_dim_tensors ` ` state ` ` state_name ` ` single flat parameter corresponding unflattened parameter names ` ` unflat_param_names ` ` enforcing all tensors same using common value NOTE The requirement tensors same across all unflattened parameters comprising flat parameter needed maintain invariant FSDP performs same computation its non-sharded equivalent This means none unflattened parameters can missing state since imposing value may differ having no value For example Adam s step no value means maximum bias correction while having some positive value means less bias correction Args state_name str Optimizer state name zero_dim_tensors List torch Tensor Zero-dimension optimizer state unflattened parameters corresponding single flat parameter unflat_param_names List str A ` list ` unflattened parameter names corresponding single flat parameter Returns torch Tensor A zero-dimensional tensor giving value state ` ` state_name ` ` all unflattened parameters corresponding names ` ` unflat_param_names ` ` non_none_tensors = t t zero_dim_tensors t None Enforce all have same value dtype values_set = t item t None None t zero_dim_tensors dtypes = t dtype t None None t zero_dim_tensors len non_none_tensors = len zero_dim_tensors len values_set = len dtypes = raise ValueError All unflattened parameters comprising single flat parameter must have scalar state same value dtype f got values values_set dtypes dtypes state f state_name unflattened parameter names f unflat_param_names value = next iter values_set dtype = next iter dtypes torch tensor value dtype=dtype device=torch device cpu _flatten_non_tensor_optim_state state_name str non_tensors list Any unflat_param_names list str - Any Flattens non-tensor optimizer state given values ` ` non_tensors ` ` state ` ` state_name ` ` single flat parameter corresponding unflattened parameter names ` ` unflat_param_names ` ` enforcing all values same using common value See note func ` _flatten_zero_dim_tensor_optim_state ` Args state_name str Optimizer state name non_tensors List Any Non-tensor optimizer state unflattened parameters corresponding single flat parameter unflat_param_names List str A ` list ` unflattened parameter names corresponding single flat parameter Returns Any A non-tensor giving value state ` ` state_name ` ` all unflattened parameters corresponding names ` ` unflat_param_names ` ` non_none_non_tensors = nt nt non_tensors nt None Enforce all have same value same type already checked non_tensor_set = set non_tensors len non_none_non_tensors = len non_tensors len non_tensor_set = raise ValueError All unflattened parameters comprising single flat parameter must have scalar state same value dtype f got values non_tensor_set state state_name f unflattened parameter names unflat_param_names non_tensor = next iter non_tensor_set non_tensor _rekey_sharded_optim_state_dict sharded_osd dict str Any model nn Module optim torch optim Optimizer optim_input Optional Union list dict str Any Iterable nn Parameter using_optim_input bool is_named_optimizer bool = False - dict str Any Rekeys optimizer state dict unflattened parameter names flat parameter IDs according calling rank s ` ` optim ` ` which may different across ranks In particular unflattened parameter names represented ` _OptimStateKey ` s param_to_fqns = _get_param_to_fqns model flat_param_to_fqn = _get_flat_param_to_fqn model param_to_param_key dict nn Parameter Union int str = cast dict nn Parameter Union int str _get_param_to_param_id_from_optim_input model optim_input using_optim_input _get_param_to_param_key optim model is_named_optimizer param_to_fqns flat_param_to_fqn All parameter keys ` param_to_param_key ` should ` param_to_fqns ` -- strict inequality follows when all parameters passed optimizer len param_to_param_key len param_to_fqns raise AssertionError f Expected len param_to_param_key = len param_to_fqns got len param_to_param_key len param_to_fqns unflat_param_names_to_flat_param_key dict tuple str Union int str = state unflat_param_name_to_flat_param_key dict str Union int str = param_groups param unflat_param_names param_to_fqns items param param_to_param_key This parameter passed optimizer continue flat_param_key = param_to_param_key param unflat_param_names_to_flat_param_key tuple unflat_param_names = flat_param_key unflat_param_name unflat_param_names unflat_param_name_to_flat_param_key unflat_param_name = flat_param_key sharded_osd_state = sharded_osd state rekeyed_osd_state dict Union str int Any = key param_state sharded_osd_state items isinstance key str rekeyed_osd_state key = param_state continue flat_param_key = unflat_param_names_to_flat_param_key get key unflat_param_names key unflat_param_names pyrefly ignore unsupported-operation rekeyed_osd_state flat_param_key = param_state Only process param_groups exists sharded_osd param_groups sharded_osd rekeyed_osd_param_groups list dict str Any = unflat_param_group sharded_osd param_groups flat_param_group = copy deepcopy unflat_param_group flat_param_keys = sorted unflat_param_name_to_flat_param_key unflat_param_name unflat_param_name unflat_param_group params flat_param_group params = flat_param_keys rekeyed_osd_param_groups append flat_param_group state rekeyed_osd_state param_groups rekeyed_osd_param_groups state rekeyed_osd_state _get_param_id_to_param_from_optim_input model nn Module optim_input Optional Union list dict str Any Iterable nn Parameter = None - dict int nn Parameter Constructs mapping parameter IDs parameters This may used both models ` ` FlatParameter ` ` s without NOTE This method only preserved backward compatibility The method meth ` _get_param_key_to_param ` preferred code path does rely ` ` optim_input ` ` NOTE We critically assume whether optimizer input list parameters list parameter groups ` torch optim Optimizer ` enumerates parameter IDs order In other words parameter list input parameter IDs should list order parameter groups input parameter IDs should order within each parameter group order across parameter groups Args model nn Module Model whose parameters passed into optimizer optim_input Optional Union List Dict str Any Iterable nn Parameter Input passed into optimizer representing either ` list ` parameter groups iterable parameters ` ` None ` ` then method assumes input ` ` model parameters ` ` Default ` ` None ` ` Returns List nn Parameter Mapping parameter IDs parameters where parameter ID implicitly index ` list ` Assume standard case passing ` model parameters ` optimizer ` optim_input ` specified optim_input None dict enumerate model parameters try pyrefly ignore no-matching-overload pyrefly ignore redundant-cast params = cast list nn Parameter list optim_input except TypeError e raise TypeError Optimizer input should iterable Tensors dicts f got optim_input e len params == raise ValueError Optimizer input should empty Check optimizer input represents tensors parameter groups all_tensors = True all_dicts = True param params all_tensors = isinstance param torch Tensor all_dicts = isinstance param dict all_tensors all_dicts raise TypeError Optimizer input should iterable Tensors dicts all_tensors dict enumerate params all_dicts raise AssertionError f Expected all_dicts True got all_dicts param_id_to_param list nn Parameter = param_group params has_params_key = params param_group type ignore operator has_params_key raise AssertionError A parameter group should map params list parameters group Implicitly map ` flat_param_id ` current length list ` param ` param_id_to_param extend param_group params type ignore index dict enumerate param_id_to_param _get_flat_param_to_fqn model torch nn Module - dict FlatParameter str Constructs mapping ` ` FlatParameter ` ` cleaned devoid prefixes wrappers fully qualified name FQN Note FQN non-canonical because ` ` FlatParameter ` ` s do come original module registered only after FSDP has been applied This function returns FSDP-given name ` ` FlatParameter ` ` usually module _flat_param opposed canonical FQNs returned ` ` FlatParameter ` ` s ` ` _common_utils _get_param_to_fqns ` ` Consequently function will only non-empty mapping FSDP applied ` ` use_orig_params=False ` ` otherwise original parameters used within module there would no ` ` FlatParameter ` ` s module module_fn module prefix tree_level flat_param_to_fqn param_name param _named_parameters_with_duplicates module recurse=False isinstance param FlatParameter continue fqn = clean_tensor_name prefix + param_name flat_param_to_fqn param = fqn return_fn flat_param_to_fqn flat_param_to_fqn flat_param_to_fqn_ret dict FlatParameter str = _apply_to_modules model module_fn return_fn fqn fqn _ _named_parameters_with_duplicates model flat_param_to_fqn_ret _get_param_key_to_param optim torch optim Optimizer model Optional nn Module = None is_named_optimizer bool = False param_to_fqns Optional dict nn Parameter list str = None flat_param_to_fqn Optional dict FlatParameter str = None - dict Union int str nn Parameter Constructs mapping parameter keys parameters For regular optimizers keys parameter IDs For NamedOptimizer keys FQNs This API may used both models ` ` FlatParameter ` ` s without clean_fqn_to_curr_fqn dict str str = is_named_optimizer param_to_fqns None flat_param_to_fqn None raise AssertionError The optimizer NamedOptimizer ` param_to_fqns ` must None model None raise AssertionError f Expected model None got model key _ _named_parameters_with_duplicates model clean_fqn_to_curr_fqn clean_tensor_name key = key param_key_to_param dict Union str int nn Parameter = pid = param_group optim param_groups is_named_optimizer param param_group params flat_param_to_fqn None raise AssertionError f Expected flat_param_to_fqn None got flat_param_to_fqn param flat_param_to_fqn FlatParameter case key = flat_param_to_fqn param param_to_fqns None raise AssertionError f Expected param_to_fqns None got param_to_fqns use_orig_params case len param_to_fqns param = raise AssertionError f Expected len param_to_fqns param == got len param_to_fqns param key = param_to_fqns param try key = clean_fqn_to_curr_fqn key except KeyError e raise KeyError f Can t find key list clean_fqn_to_curr_fqn keys e param_key_to_param key = param param param_group params param_key_to_param pid = param pid += param_key_to_param _get_param_to_param_key optim torch optim Optimizer model Optional nn Module = None is_named_optimizer bool = False param_to_fqns Optional dict nn Parameter list str = None flat_param_to_fqn Optional dict FlatParameter str = None - dict nn Parameter Union int str Constructs inverse mapping func ` _get_param_key_to_param ` This API only supports case where ` optim ` regular optimizer NamedOptimizer So parameter keys will parameter ids param_id_to_param = _get_param_key_to_param optim model is_named_optimizer param_to_fqns flat_param_to_fqn param param_id param_id param param_id_to_param items _get_param_to_param_id_from_optim_input model nn Module optim_input Optional Union list dict str Any Iterable nn Parameter = None - dict nn Parameter int Constructs inverse mapping func ` _get_param_id_to_param_from_optim_input ` param_id_to_param = _get_param_id_to_param_from_optim_input model optim_input param param_id param_id param param_id_to_param items _check_missing_keys_on_rank r _optim_state_keys list _OptimStateKey optim_state_key_to_param_key dict _OptimStateKey Union str int param_key_to_param dict Union str int nn Parameter group Optional dist ProcessGroup - None Ensure all ranks have least optimizer states needed rank s optimizer missing_keys list _OptimStateKey = r _optim_state_key r _optim_state_keys r _optim_state_key optim_state_key_to_param_key A parameter rank s optimizer does exist rank s optimizer missing_keys append r _optim_state_key continue param_key = optim_state_key_to_param_key r _optim_state_key isinstance param_key int param_key = param_key len param_key_to_param raise AssertionError Check ` param_key_to_param ` construction We cannot use FSDPState compute_device API global view device = _get_pg_default_device group num_missing = torch tensor len missing_keys dtype=torch int device=device dist all_reduce num_missing group=group num_missing item obj_list = None _ range dist get_world_size group dist all_gather_object obj_list missing_keys group=group error_msg = FSDP currently requires each rank have least optimizer states needed rank s optimizer some ranks missing some those states rank keys enumerate obj_list keys = cast list _OptimStateKey keys len keys error_msg += f \nRank rank missing states parameters f key unflat_param_names key keys raise RuntimeError error_msg _map_param_key_to_optim_keys optim_state_dict dict str Any group Optional dist ProcessGroup param_key_to_param dict Union int str nn Parameter param_to_fqns dict nn Parameter list str fqn_to_fsdp_param_info dict str FSDPParamInfo merge_keys bool = False - tuple list _OptimStateKey dict _OptimStateKey Union int str Construct local mapping between ` ` _OptimStateKey ` ` parameter keys all ` ` _OptimStateKey ` ` across ranks If ` ` merge_keys ` ` False rank must contain all ` ` _OptimStateKey ` ` exception will raised otherwise Note ` ` merge_keys ` ` should equal ` ` use_orig_params ` ` rank = dist get_rank group optim_state_key_to_param_key dict _OptimStateKey Union int str = local all_optim_state_keys list _OptimStateKey = param_key param param_key_to_param items Do include parameters without state avoid empty mappings just like normal ` torch optim Optimizer state_dict ` param_key optim_state_dict state continue fqns = param_to_fqns param is_fsdp_managed = isinstance param FlatParameter is_fsdp_managed fqns fqn_to_fsdp_param_info raise AssertionError f Expected fqns fqn_to_fsdp_param_info got keys list fqn_to_fsdp_param_info keys is_fsdp_managed = fqns fqn_to_fsdp_param_info optim_state_key = _OptimStateKey unflat_param_names=tuple fqns is_fsdp_managed=is_fsdp_managed rank == merge_keys all_optim_state_keys append optim_state_key optim_state_key_to_param_key optim_state_key = param_key merge_keys all_keys list list _OptimStateKey = _ range dist get_world_size group dist all_gather_object all_keys all_optim_state_keys group=group merge_all_optim_state_keys = chain from_iterable all_keys all_optim_state_keys = sorted set merge_all_optim_state_keys key_obj_list list Optional list _OptimStateKey = all_optim_state_keys rank == None dist broadcast_object_list key_obj_list src= group=group key_obj_list None raise AssertionError f Expected key_obj_list None got key_obj_list all_optim_state_keys = key_obj_list _check_missing_keys_on_rank all_optim_state_keys optim_state_key_to_param_key param_key_to_param group all_optim_state_keys optim_state_key_to_param_key _unflatten_param_groups state_dict dict str Any param_key_to_param dict Union int str nn Parameter param_to_fqns dict nn Parameter list str - list dict str Any param_groups list dict str Any = flat_param_group state_dict param_groups unflat_param_group = copy deepcopy flat_param_group param_group_params = param_key_to_param flat_param_key flat_param_key flat_param_group params nested_unflat_param_names = param_to_fqns param param param_group_params unflat_param_group params = chain from_iterable nested_unflat_param_names flatten list lists param_groups append unflat_param_group param_groups _is_named_optimizer optim_state_dict dict str Any - bool Returns whether state_dict NamedOptimizer This function checks keys state_dict state strings which usually FQNs versus integers which usually refer param_ids vanilla torch optim Optimizer state = optim_state_dict get state state If we cannot find state assume NamedOptimizer NamedOptimizer has eager initialization False try key = next iter state keys except Exception e raise Exception optim_state_dict e noqa TRY isinstance key str dataclass StateInfo The key these dictionaries state name e g ` exp_avg ` tensors dict str _PosDimTensorInfo scalar_tensors dict str torch Tensor non_tensors dict str Any _allgather_state_info fsdp_state _FSDPState input_states dict str Any - list dict str StateInfo Given ` ` input_states ` ` allgather StateInfo each state The function uses all_gather_object gather StateInfo so no GPU tensors sent processed_state_dict dict str StateInfo = gathered_state_info list dict str StateInfo = _ range fsdp_state world_size fqn optim_state input_states items Allgather scalar tensor state non-tensor states tensors metadata processed_state = StateInfo state_name value sorted_items optim_state torch is_tensor value value dim == Ensure ` step ` CPU processed_state scalar_tensors state_name = value cpu processed_state tensors state_name = _PosDimTensorInfo value shape value dtype processed_state non_tensors state_name = value processed_state_dict fqn = processed_state dist all_gather_object gathered_state_info processed_state_dict group=fsdp_state process_group gathered_state_info _convert_all_state_info fsdp_param_info FSDPParamInfo gathered_state_info list dict str StateInfo input_states dict str Any output_states dict str dict str Any - tuple Optional torch dtype dict str list Optional torch Tensor Given ` ` gathered_state_info ` ` ` ` input_states ` ` API converted StateInfo into original state state non-scalar tensor For multi-dimensional tensor local state will stored ` ` state_buffer ` ` correct order later allgather purpose state_buffers dict str list Optional torch Tensor = fqn gathered_state output_states items state_info = s fqn s gathered_state_info all_tensor_states = sorted n state state_info n state tensors keys empty_ranks set int = set dtype Optional torch dtype = None First check all non-scalar states get information states each rank state_name all_tensor_states numels = _empty_ranks set int = set rank object_state enumerate state_info numels append info = object_state tensors get state_name None info None numels - = info shape numel dtype dtype = info dtype dtype = info dtype raise AssertionError f Expected dtype == info dtype got dtype = info dtype numels - == _empty_ranks add rank empty_ranks empty_ranks == _empty_ranks raise AssertionError f Expected empty_ranks empty equal _empty_ranks got empty_ranks vs _empty_ranks empty_ranks = _empty_ranks state_name state_buffers state_buffers state_name = None _ fsdp_param_info param_indices local_state = input_states fqn get state_name None N B We need move state compute_device The reason yet clear we need figure out why state may different device local_state None local_state = local_state fsdp_param_info state compute_device state_buffers state_name fsdp_param_info param_indices fqn = local_state Restoring scalar non-tensor states If corresponding non-scalar states do exist rank we also skip scalar non-tensor states rank rank object_state enumerate state_info rank empty_ranks continue name non_tensor_value object_state non_tensors items curr_non_tensor_value = gathered_state get name None curr_non_tensor_value None curr_non_tensor_value == non_tensor_value raise AssertionError f Rank rank has different values name non_tensor_value + f Other ranks curr_non_tensor_value gathered_state name = non_tensor_value name scalar_tensor_value object_state scalar_tensors items curr_scalar_tensor_value = gathered_state get name None curr_scalar_tensor_value None torch equal scalar_tensor_value curr_scalar_tensor_value raise AssertionError f Rank rank has different values name scalar_tensor_value + f Other ranks curr_scalar_tensor_value gathered_state name = scalar_tensor_value dtype state_buffers type ignore possibly-undefined _unflatten_orig_param_states fsdp_param_info FSDPParamInfo output_states dict str dict str Any state_name str shard_state bool to_save bool cpu_offload bool - None Given output state dict ` ` output_states ` ` which keys FQNs original parameters FlatParameters nor parameter ID values gathered states unflatten states original dimensions This function performs unflattening process in-place to_save flat_param = fsdp_param_info handle flat_param fsdp_state = fsdp_param_info state fqn gathered_state output_states items value = gathered_state state_name param_idx = fsdp_param_info param_indices fqn TODO This solution general only apply PTD TP solution isinstance value DTensor placement = value placements If gathered state DTensor its TP placement Replicate we need gather tensor its TP dimension before chunking them into DTensor again placement = Replicate placement_dim = placement dim type ignore attr-defined value redistribute placements= Replicate reshape_size = list flat_param _shapes param_idx reshape_size placement_dim = value device_mesh size reshape_size = torch Size reshape_size value = value reshape reshape_size If gathered state replicate DTensor we directly reshape value = value reshape flat_param _shapes param_idx If gathered state tensor we directly reshape into unflatten state value = value reshape flat_param _shapes param_idx shard_state osd_config = fsdp_state _optim_state_dict_config getattr osd_config _use_dtensor False fsdp_state _device_mesh None raise AssertionError f Expected _device_mesh None got fsdp_state _device_mesh value = _ext_chunk_dtensor value fsdp_state rank fsdp_state _device_mesh fsdp_state _fsdp_extension fsdp_state process_group None raise AssertionError f Expected process_group None got fsdp_state process_group value = _ext_chunk_tensor value fsdp_state rank fsdp_state world_size fsdp_state _device_handle device_count fsdp_state process_group fsdp_state _fsdp_extension cpu_offload SimpleProfiler profile clone value = value detach clone cpu_offload SimpleProfiler profile SimpleProfiler Type D H value = value cpu gathered_state state_name = value _allgather_orig_param_states fsdp_param_info FSDPParamInfo gathered_state_info list dict str StateInfo input_states dict str Any shard_state bool to_save bool cpu_offload bool - dict str dict str Any Given ` ` gathered_state_info ` ` ` ` input_states ` ` API allgathers all tensor states restore non-tensor states ` ` gathered_state_info ` ` fsdp_state = fsdp_param_info state fsdp_state rank == dist get_debug_level == dist DebugLevel DETAIL logger info Memory Summary before calling _allgather_orig_param_states s fsdp_state _device_handle memory_summary output_states dict str dict str Any = fqn fqn input_states keys dtype state_buffers = _convert_all_state_info fsdp_param_info gathered_state_info input_states output_states len state_buffers == output_states has_state_params list bool = fqn output_states fqn idx fsdp_param_info param_indices items Loop through ` ` state_buffers ` ` construct flattened concatenated sharded states The size constructed state will same size flat_param also sharded Then we perform allgather_into_tensor get full flat_param state The full flat_param state result concatenation multiple states order flat_param _fqns The final step split flat_param state into original param states result flat_param = fsdp_param_info handle flat_param empty_func = functools partial torch empty dtype=dtype device=fsdp_state compute_device gathered_tensor = empty_func flat_param _padded_unsharded_size Synchronize can slow will easier us debug fsdp_state _device_handle synchronize state_name buffers state_buffers items local_buffers list torch Tensor = begin = fsdp_state rank flat_param _sharded_size numel End inclusive end = begin + flat_param _sharded_size numel - param_idx corresponds parameter index FlatParameter mem_offset param_idx = numel is_padding zip flat_param _numels_with_padding flat_param _is_padding_mask frozen_and_no_state = is_padding fsdp_param_info param_requires_grad param_idx has_state_params param_idx is_padding frozen_and_no_state This memory range padding param frozen does require gradient For later case we treat padding add empty values local_buffers padding_begin padding_end = mem_offset mem_offset + numel - padding_begin = begin = padding_end The range align padding before first parameter shard The shard includes parts align padding padding_len = padding_end - begin + end = padding_end end - begin + padding_begin = end = padding_end The range align padding after last parameter shard The shard includes parts align padding padding_len = end - padding_begin + begin = padding_begin end - begin + begin padding_begin = padding_end end The range align padding completely shard padding_len = numel padding_len = padding_len local_buffers append empty_func padding_len is_padding This memory range parameter FlatParameter So there should corresponding state optimizer unless parameter frozen which we treat padding above We need check rank owns buffer If None rank does own any part original parameter As result there no corresponding optimizer state rank well parameter frozen AND no optimizer state parameter If parameter frozen there can still optimizer state parameter frozen previous steps buffers param_idx None local_buffers append cast torch Tensor buffers param_idx param_idx += mem_offset += numel shard_numel_padded = flat_param _sharded_size numel - sum t numel t local_buffers flat_param _shard_numel_padded = shard_numel_padded raise AssertionError Manually calculated _sharded_numel_padded incorrect f _shard_numel_padded= flat_param _shard_numel_padded f shard_numel_padded= shard_numel_padded f _sharded_size numel= flat_param _sharded_size numel f _numels_with_padding= flat_param _numels_with_padding f begin= begin end= end shard_numel_padded Add right-handed padding local_buffers append empty_func shard_numel_padded local_shard = torch cat local_buffers local_shard numel fsdp_state world_size = gathered_tensor numel raise AssertionError The size local shard times world size should equal gathered tensor size The inconsistency may bug FlatParameter s metadata reconstruction logic optimizer state dict fsdp_state _device_handle synchronize SimpleProfiler profile SimpleProfiler Type ALLGATHER dist all_gather_into_tensor gathered_tensor local_shard group=fsdp_state process_group Synchronize can slow will easier us debug fsdp_state _device_handle synchronize unpadded_tensor = gathered_tensor flat_param _unpadded_unsharded_size numel flat_param_handle = fsdp_param_info handle orig_states = flat_param_handle _get_unflat_views_aligned unpadded_tensor len orig_states = len fsdp_param_info param_indices raise AssertionError The number parameters FlatParameter consistent number states used optimizer state dict reconstruction logic fqn idx fsdp_param_info param_indices items fsdp_param_info param_requires_grad idx fqn output_states output_states fqn state_name = orig_states idx _unflatten_orig_param_states fsdp_param_info output_states state_name shard_state to_save cpu_offload del gathered_tensor output_states _gather_all_orig_param_state fsdp_param_info FSDPParamInfo input_states dict str Any shard_state bool to_save bool cpu_offload bool - dict str Any Given optimizer state dict ` ` input_states ` ` which keys FQNs original parameters FlatParameters nor parameter ID gather all states unflatten them original dimensions Note all params referred ` ` input_states ` ` must managed FSDP fsdp_state = fsdp_param_info state fsdp_state world_size == fsdp_state sharding_strategy == ShardingStrategy NO_SHARD input_states to_save SimpleProfiler profile SimpleProfiler Type RESHARDING SimpleProfiler profile SimpleProfiler Type ALLGATHER_OBJ gathered_state_info = _allgather_state_info fsdp_state input_states output_states = _allgather_orig_param_states fsdp_param_info gathered_state_info input_states shard_state to_save cpu_offload to_save key idx fsdp_param_info param_indices items key output_states continue fsdp_param_info param_requires_grad idx continue raise RuntimeError f key output state The FSDPParamInfo has param keys f sorted fsdp_param_info param_indices keys while output_states has param keys f sorted output_states keys output_states _convert_state_with_orig_params all_optim_state_keys list _OptimStateKey optim_state_key_to_param_key dict _OptimStateKey Union int str fqn_to_fsdp_param_info dict str FSDPParamInfo optim_state_dict dict Union str int Any to_save bool shard_state bool cpu_offload bool = True - dict str Any fsdp_osd_state dict str Any = This variable used deduplicate FSDPParamInfo one FSDPParamInfo usually corresponds multiple parameters We could use FSDPParamInfo key because FSDPParamInfo hashable As result we fall back ` id FSDPParamInfo ` which type integer all_states dict int dict str Any = Iterate rank s flat parameter ID order ensure aligned all-gathers across ranks optim_state_key all_optim_state_keys param_key Union str int None = optim_state_key_to_param_key get optim_state_key param_key None optim_state_key is_fsdp_managed continue optim_state_key is_fsdp_managed fqn = optim_state_key unflat_param_names fsdp_param_info = fqn_to_fsdp_param_info get fqn fsdp_param_info None This can happen all FSDP instances have all parameters This can happen FSDP + some MPMD style parallelism TODO unclear we need do same check non-FSDP managed keys continue state = param_key None optim_state_dict param_key id fsdp_param_info all_states all_states id fsdp_param_info = all_states id fsdp_param_info fqn = state to_save len optim_state_key unflat_param_names = raise AssertionError f Expected len optim_state_key unflat_param_names == got len optim_state_key unflat_param_names unflat_param_name = optim_state_key unflat_param_names SimpleProfiler profile none_fsdp_managed_copy param_key = cast Union str int param_key fsdp_osd_state unflat_param_name = copy copy optim_state_dict param_key cpu_offload state_name value sorted_items fsdp_osd_state unflat_param_name torch is_tensor value continue fsdp_osd_state unflat_param_name state_name = value cpu Instead gathering state each parameter individually we perform gathering all once speed up process _all_states all_states values fqn = next iter _all_states keys fsdp_param_info = fqn_to_fsdp_param_info fqn len fsdp_param_info param_requires_grad = raise AssertionError With use_orig_params FSDPParamInfo should have requires_grad information However length zero key idx fsdp_param_info param_indices items key _all_states continue fsdp_param_info param_requires_grad idx continue raise RuntimeError f key optimizer state The FSDPParamInfo has param keys f sorted fsdp_param_info param_indices keys while optimizer has param keys f sorted _all_states keys fsdp_osd_state update _gather_all_orig_param_state fsdp_param_info _all_states shard_state to_save cpu_offload fsdp_osd_state _convert_state_with_flat_params all_optim_state_keys list _OptimStateKey optim_state_key_to_param_key dict _OptimStateKey Union int str fqn_to_fsdp_param_info dict str FSDPParamInfo optim_state_dict dict Union str int Any to_save bool shard_state bool cpu_offload bool = True - dict str Any fsdp_osd_state dict str Any = Iterate rank s flat parameter ID order ensure aligned all-gathers across ranks optim_state_key all_optim_state_keys param_key Union str int None = optim_state_key_to_param_key get optim_state_key param_key None raise AssertionError If use_orig_params False we must able find f corresponding param id optim_state_key param_key optim_state_key is_fsdp_managed If there multiple unflat_param_names use_orig_params they share same FSDPParamInfo So first unflat_param_name sufficient fetch FSDPParamInfo fqn = optim_state_key unflat_param_names fsdp_param_info = fqn_to_fsdp_param_info fqn unflat_state = _unflatten_optim_state fsdp_param_info optim_state_dict param_key to_save shard_state cpu_offload to_save len unflat_state = len optim_state_key unflat_param_names raise AssertionError f Expected len unflat_state == len optim_state_key unflat_param_names f got len unflat_state = len optim_state_key unflat_param_names fsdp_osd_state update zip optim_state_key unflat_param_names unflat_state to_save len optim_state_key unflat_param_names = raise AssertionError f Expected len optim_state_key unflat_param_names == got len optim_state_key unflat_param_names unflat_param_name = optim_state_key unflat_param_names fsdp_osd_state unflat_param_name = copy copy optim_state_dict param_key cpu_offload state_name value sorted_items fsdp_osd_state unflat_param_name torch is_tensor value continue fsdp_osd_state unflat_param_name state_name = value cpu fsdp_osd_state torch no_grad _optim_state_dict model nn Module optim torch optim Optimizer optim_state_dict dict str Any optim_input Optional Union list dict str Any Iterable nn Parameter rank _only bool shard_state bool group Optional dist ProcessGroup using_optim_input bool use_orig_params bool = False cpu_offload bool = True - dict str Any Consolidates optimizer state returns ` dict ` following convention meth ` torch optim Optimizer state_dict ` i e keys ` ` state ` ` ` ` param_groups ` ` The flat parameters ` ` FSDP ` ` modules contained ` ` model ` ` mapped back their unflattened parameters Parameter keys well-defined For regular optimizer optimizer state_dict contains mapping parameter IDs parameter states Parameter IDs order parameters ` ` optim param_groups ` ` across all groups This API also allows user pass ` ` optim_input ` ` mapping between parameters parameter IDs Using ` ` optim_input ` ` being deprecated If optimizer ` ` NamedOptimizer ` ` optimizer state_dict does contain parameter IDs mapping mapping parameter FQNs parameter states This API finds mapping FQNs parameters optimizer ` ` NamedOptimizer ` ` If ` ` use_orig_params ` ` True each rank will have all FSDP-managed parameters some these parameters may empty due sharding For regular optim Optimizer states those empty parameters will initialized So when aggregating FQNs across ranks no assert will raised rank even does have all states -- valid FSDP knows how aggregate them However FSDP has ignore handling those parameters managed FSDP do exist local rank -- those managed other parallelisms FSDP does know how handle aggregate them Args model nn Module Root module which may may ` FullyShardedDataParallel ` instance whose parameters passed into optimizer ` ` optim ` ` optim torch optim Optimizer Optimizer ` ` model ` ` s parameters rank _only bool If ` ` True ` ` saves populated ` dict ` only rank ` ` False ` ` saves all ranks Default ` ` True ` ` shard_state bool If ` ` True ` ` shard distribute all non-zero-dimension states Returns Dict str Any A ` dict ` containing optimizer state ` ` model ` ` s original unflattened parameters including keys state param_groups following convention meth ` torch optim Optimizer state_dict ` If ` ` rank _only=False ` ` then nonzero ranks empty ` dict ` SimpleProfiler reset cm = ExitStack cm enter_context SimpleProfiler profile SimpleProfiler Type ALL _reset_flat_param_grad_info_if_needed traversal_utils _get_fsdp_handles model to_save = rank _only dist get_rank group == shard_state SimpleProfiler profile preprocessing param_to_fqns = _get_param_to_fqns model flat_param_to_fqn = _get_flat_param_to_fqn model is_named_optimizer = _is_named_optimizer optim_state_dict param_key_to_param = cast dict Union int str nn Parameter _get_param_id_to_param_from_optim_input model optim_input using_optim_input _get_param_key_to_param optim model is_named_optimizer param_to_fqns flat_param_to_fqn fqn_to_fsdp_param_info = _get_fqn_to_fsdp_param_info model SimpleProfiler profile preprocessing_with_comm all_optim_state_keys optim_state_key_to_param_key = _map_param_key_to_optim_keys optim_state_dict group param_key_to_param param_to_fqns fqn_to_fsdp_param_info merge_keys=use_orig_params SimpleProfiler profile state_converting convert_fn = _convert_state_with_orig_params use_orig_params _convert_state_with_flat_params fsdp_osd_state = convert_fn all_optim_state_keys optim_state_key_to_param_key fqn_to_fsdp_param_info optim_state_dict state to_save shard_state cpu_offload At point communication complete ranks can early nothing will saved rank to_save fsdp_osd dict str Any = state fsdp_osd_state flat_param_fqns = set flat_param_to_fqn values key value optim_state_dict state items key fsdp_osd_state continue key flat_param_fqns continue key param_key_to_param continue This key recognized FSDP It may user-defined state some parameters state FSDP unable map ` ` optim param_groups ` ` warnings warn f Found optim state key FSDP cannot process FSDP will directly copy everything returned state_dict In most cases user-defined state associated any particular parameter Another possible case state managed TorchRec Otherwise there may mismatched assumption optim_state_dict mode stacklevel= fsdp_osd_state key = value param_groups optim_state_dict fsdp_osd param_groups = _unflatten_param_groups optim_state_dict param_key_to_param param_to_fqns cm close SimpleProfiler dump_and_reset FSDP _optim_state_dict profiling fsdp_osd _get_fqn_to_fsdp_param_info model nn Module - dict str FSDPParamInfo Construct mapping param s fqn its corresponding ` ` FSDPParamInfo ` ` param managed FSDP Shared parameters original parameters shared across multiple nn Modules required belong one only one FSDP instance thus correspond one ` ` FlatParameter ` ` Within one ` ` FlatParameter ` ` ` ` FlatParameter _fqns ` ` only stores first FQN shared parameter Thus keys mapping guaranteed map unique parameters module_fn module prefix tree_level fqn_to_param_info fsdp_state = _get_module_fsdp_state_if_fully_sharded_module module fsdp_state None _lazy_init fsdp_state module handle = _module_handle fsdp_state module handle flat_param = handle flat_param fsdp_param_info = FSDPParamInfo fsdp_state handle NOTE ` idx ` indexes into data structures without padding elements idx local_fqn enumerate flat_param _fqns fqn = clean_tensor_name prefix + local_fqn fqn fqn_to_param_info fqn_to_param_info fqn handle flat_param flat_param raise AssertionError f Expected fqn_to_param_info fqn handle flat_param flat_param fqn fqn_to_param_info fqn = fsdp_param_info fsdp_param_info param_indices fqn = idx flat_param _params None fsdp_param_info param_requires_grad append flat_param _params idx requires_grad return_fn fqn_to_param_info fqn_to_param_info fqn_to_param_info dict str FSDPParamInfo = FlatParameter _fqns stores local fqn starting root FSDP Using _apply_to_modules model may FSDP root module allows us construct global fqn _apply_to_modules model module_fn return_fn fqn fqn _ _named_parameters_with_duplicates model fqn_to_param_info no_type_check _set_optim_use_dtensor fsdp_state _FSDPState state_dict_settings StateDictSettings - None If device_mesh passed when initializing FSDP we automatically turn _use_dtensor flag true ShardedOptimStateDictConfig state_dict_type has set SHARDED_STATE_DICT getattr fsdp_state _device_mesh None state_dict_type = state_dict_settings state_dict_type state_dict_type == StateDictType LOCAL_STATE_DICT raise RuntimeError Found state_dict_type LOCAL_STATE_DICT DeviceMesh compatible LOCAL_STATE_DICT Please set state_dict_type SHARDED_STATE_DICT get DTensor state_dict state_dict_settings optim_state_dict_config _use_dtensor = True