Owner s module inductor functools logging os pickle shutil subprocess sys tempfile textwrap unittest contextlib contextmanager typing Optional Union typing_extensions override unittest mock torch torch _dynamo reset torch _dynamo package DynamoCache torch _dynamo precompile_context PrecompileContext torch _dynamo utils counters torch _functorch config functorch_config torch _functorch _aot_autograd autograd_cache AOTAutogradCache torch _inductor config metrics torch _inductor codecache BypassFxGraphCache cuda_compile_command CUDACodeCache FxGraphCachePickler FxGraphHashDetails PyCodeCache TensorMetadata TensorMetadataAndValues torch _inductor cpp_builder normalize_path_separator torch _inductor custom_graph_pass CustomGraphModulePass CustomGraphPass CustomPartitionerFn get_hash_for_files torch _inductor graph GraphLowering torch _inductor mock_cache global_stats PatchCaches Stats torch _inductor runtime runtime_utils cache_dir torch _inductor test_case run_tests TestCase torch _inductor utils clear_caches fresh_cache torch _library capture_triton torch compiler _cache CacheArtifact CacheArtifactFactory CacheArtifactManager torch testing _internal common_cuda SM OrLater TEST_MULTIGPU with_tf _off torch testing _internal common_device_type largeTensorTest torch testing _internal common_utils instantiate_parametrized_tests IS_FBCODE IS_SANDCASTLE parametrize TEST_WITH_ROCM torch testing _internal inductor_utils GPU_TYPE HAS_GPU HAS_MULTIGPU HAS_TRITON HAS_XPU_AND_TRITON patch_inductor_backend requires_gpu requires_triton torch testing _internal triton_utils requires_cuda_and_triton requires_gpu_and_triton try custom_inductor_config except ImportError custom_inductor_config HAS_TRITON triton manual torch testing _internal triton_utils add_kernel sub_kernel torch _dynamo config fake_tensor_cache_enabled = True torch _dynamo config fake_tensor_cache_crosscheck_enabled = True LogCaptureHandler logging Handler __init__ level super __init__ level records = emit record records append record contextmanager capture_logs log_name log_level try logger = logging getLogger log_name old_level = logger level handler = logging Handler logger setLevel log_level log_records = emit record log_records append record handler emit = emit logger addHandler handler yield log_records finally logger removeHandler handler logger setLevel old_level MyModelConv d torch nn Module __init__ dim= super __init__ conv = torch nn Conv d dim kernel_size= stride= bias=False conv = torch nn Conv d dim dim kernel_size= stride= bias=False forward x x = conv x torch _dynamo graph_break x = conv x x TestPyCodeCache TestCase test_linemaps_empty src = torch key path = PyCodeCache write src Load empty linemap PyCodeCache load_by_key_path key path linemap= stack_frames = PyCodeCache stack_frames_for_code path assertEqual stack_frames None unittest skipIf IS_FBCODE IS_SANDCASTLE Skip fbcode sandcastle test_editable_cached_wrapper tempfile TemporaryDirectory tmpdir env = os environ copy env TORCHINDUCTOR_CACHE_DIR = tmpdir step = textwrap dedent glob os torch warnings torch _inductor config warnings filterwarnings ignore config fx_graph_cache = True config fx_graph_remote_cache = False torch _dynamo reset torch compile backend= inductor f x x f torch ones cache_dir = os environ TORCHINDUCTOR_CACHE_DIR pyfiles = glob glob os path join cache_dir py recursive=True print pyfiles wrapper_path = subprocess check_output sys executable -c step env=env decode strip step = textwrap dedent torch warnings torch _dynamo utils counters torch _inductor config warnings filterwarnings ignore config fx_graph_cache = True config fx_graph_remote_cache = False torch _dynamo reset torch compile backend= inductor f x x f torch ones print counters inductor fxgraph_cache_hit hit = subprocess check_output sys executable -c step env=env decode strip assertEqual hit open wrapper_path f src = f read open wrapper_path w f f write src replace call args call args \n print debug step = textwrap dedent torch warnings torch _inductor config warnings filterwarnings ignore config fx_graph_cache = True config fx_graph_remote_cache = False torch _dynamo reset torch compile backend= inductor f x x f torch ones out = subprocess check_output sys executable -c step env=env decode assertIn debug out instantiate_parametrized_tests TestFxGraphCache TestCase device_type = GPU_TYPE setUp super setUp counters clear DynamoCache clear PrecompileContext clear AOTAutogradCache clear PatchCaches setUp CacheArtifactManager clear torch _dynamo reset tearDown super tearDown PatchCaches tearDown reset AOTAutogradCache clear DynamoCache clear PrecompileContext clear PyCodeCache cache_clear purge=True torch _dynamo reset clear_caches requires_triton config patch fx_graph_cache True config patch fx_graph_remote_cache False config patch compile_threads parametrize device GPU_TYPE cpu parametrize dtype torch float torch bfloat parametrize dynamic False True parametrize bundle_triton False True parametrize use_static_cuda_launcher False True parametrize grad False True test_cache_load_function device dtype dynamic bundle_triton use_static_cuda_launcher grad Verify we can populate load functions cache device == GPU_TYPE HAS_GPU raise unittest SkipTest f requires GPU_TYPE device == cuda dtype == torch bfloat SM OrLater raise unittest SkipTest requires SM later use_static_cuda_launcher device == cuda bundle_triton raise unittest SkipTest Static cuda launcher requires cuda triton bundling use_static_cuda_launcher TEST_WITH_ROCM raise unittest SkipTest Static cuda launcher doesn t work ROCM grad_multiplier = grad fn x y yy = y y x + yy view a_orig = torch rand dtype=dtype device=device b_orig = torch rand dtype=dtype device=device config patch bundle_triton_into_fx_graph_cache=bundle_triton use_static_cuda_launcher=use_static_cuda_launcher compiled_fn = torch compile fn dynamic=dynamic = a_orig clone requires_grad_ grad b = b_orig clone requires_grad_ grad = a_orig clone requires_grad_ grad b = b_orig clone requires_grad_ grad A first call should miss cache eager_result = fn b compiled_result = compiled_fn b assertEqual eager_result compiled_result grad eager_result sum backward compiled_result sum backward assertEqual grad grad assertEqual b grad b grad assertEqual counters inductor fxgraph_cache_miss grad_multiplier assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file we expect ttir ttgir llir ptx cuda spv xpu json __grp__ json optionally we can also get cubin CUDA only source new versions triton only triton-lang triton# avoid depending device triton version just assert we have least kernels save_and_read_min_artifact_count = bundle_triton device = cpu assertGreaterEqual counters inductor triton_bundler_save_kernel grad_multiplier save_and_read_min_artifact_count assertEqual counters inductor triton_bundler_read_and_emit_kernel use_static_cuda_launcher assertEqual counters inductor triton_bundler_save_static_autotuner grad_multiplier device == cuda assertEqual counters inductor triton_bundler_load_static_autotuner A second call should hit First reset so in-memory guards don t prevent compilation reset Clean triton kernels shutil rmtree os path join cache_dir triton ignore_errors=True = a_orig clone requires_grad_ grad b = b_orig clone requires_grad_ grad = a_orig clone requires_grad_ grad b = b_orig clone requires_grad_ grad eager_result = fn b compiled_result = compiled_fn b assertEqual eager_result compiled_result grad eager_result sum backward compiled_result sum backward assertEqual grad grad assertEqual b grad b grad assertEqual counters inductor fxgraph_cache_miss grad_multiplier assertEqual counters inductor fxgraph_cache_hit grad_multiplier assertEqual counters inductor fxgraph_lookup_write_file grad_multiplier bundle_triton device = cpu assertGreaterEqual counters inductor triton_bundler_save_kernel grad_multiplier save_and_read_min_artifact_count assertGreaterEqual counters inductor triton_bundler_read_and_emit_kernel grad_multiplier save_and_read_min_artifact_count use_static_cuda_launcher assertEqual counters inductor triton_bundler_save_static_autotuner grad_multiplier device == cuda assertEqual counters inductor triton_bundler_load_static_autotuner grad_multiplier device == cuda reset = a_orig clone requires_grad_ grad b = b_orig clone requires_grad_ grad = a_orig clone requires_grad_ grad b = b_orig clone requires_grad_ grad eager_result = fn b grad eager_result sum backward torch compiler config patch cache_key_tag test compiled_result = compiled_fn b grad compiled_result sum backward assertEqual eager_result compiled_result grad assertEqual grad grad assertEqual b grad b grad assertEqual counters inductor fxgraph_cache_miss grad_multiplier assertEqual counters inductor fxgraph_cache_hit grad_multiplier assertEqual counters inductor fxgraph_lookup_write_file grad_multiplier bundle_triton device = cpu assertGreaterEqual counters inductor triton_bundler_save_kernel grad_multiplier save_and_read_min_artifact_count assertGreaterEqual counters inductor triton_bundler_read_and_emit_kernel grad_multiplier save_and_read_min_artifact_count use_static_cuda_launcher assertEqual counters inductor triton_bundler_save_static_autotuner grad_multiplier device == cuda assertEqual counters inductor triton_bundler_load_static_autotuner grad_multiplier device == cuda requires_triton config patch fx_graph_remote_cache True parametrize device GPU_TYPE cpu parametrize dtype torch float torch bfloat parametrize dynamic False True parametrize bundle_triton False True parametrize use_static_cuda_launcher False True config patch compile_threads Can t check globalStats there workers test_remote_cache_load_function device dtype dynamic bundle_triton use_static_cuda_launcher unittest mock patch device == GPU_TYPE HAS_GPU raise unittest SkipTest f requires GPU_TYPE device == cuda dtype == torch bfloat SM OrLater raise unittest SkipTest requires SM later use_static_cuda_launcher device == cuda bundle_triton raise unittest SkipTest Static cuda launcher requires cuda triton bundling use_static_cuda_launcher TEST_WITH_ROCM raise unittest SkipTest Static cuda launcher doesn t work ROCM fn x y x y y = torch rand dtype=dtype device=device b = torch rand dtype=dtype device=device config patch fx_graph_remote_cache True bundle_triton_into_fx_graph_cache bundle_triton use_static_cuda_launcher use_static_cuda_launcher patch dict os environ PatchCaches os environ pop TRITON_CACHE_MANAGER None _ range fresh_cache compiled_fn = torch compile fn dynamic=dynamic assertEqual fn b compiled_fn b reset assertEqual global_stats fx_graph Stats torch compiler config patch cache_key_tag test fresh_cache compiled_fn = torch compile fn dynamic=dynamic assertEqual fn b compiled_fn b assertEqual global_stats fx_graph Stats Check cache entries seem reasonable k global_stats fx_graph cache keys assertRegex k r pt fx-graph-v - a-z c - + requires_triton config patch fx_graph_cache True fx_graph_remote_cache False autotune_local_cache True parametrize device GPU_TYPE cpu parametrize dtype torch float torch bfloat parametrize dynamic False True torch _functorch config patch enable_autograd_cache False test_cache_hot_load device dtype dynamic Verify we can populate hot load functions cache device == GPU_TYPE HAS_GPU raise unittest SkipTest f requires GPU_TYPE device == cuda dtype == torch bfloat SM OrLater raise unittest SkipTest requires SM later fn x y x sin y = torch rand dtype=dtype device=device b = torch rand dtype=dtype device=device Record artifacts fresh_cache compiled_fn = torch compile fn dynamic=dynamic A first call should miss cache eager_result = fn b compiled_result = compiled_fn b assertEqual eager_result compiled_result assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file artifacts = torch compiler save_cache_artifacts assertIsNotNone artifacts artifact_bytes cache_info = artifacts autotune_expect = device == GPU_TYPE assertEqual len cache_info inductor_artifacts assertEqual len cache_info autotune_artifacts autotune_expect assertEqual len cache_info aot_autograd_artifacts assertEqual len cache_info pgo_artifacts reset Clean triton kernels shutil rmtree os path join cache_dir triton ignore_errors=True We did load anything so dont hit yet fresh_cache eager_result = fn b compiled_result = compiled_fn b assertEqual eager_result compiled_result assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file reset Clean triton kernels shutil rmtree os path join cache_dir triton ignore_errors=True Hot load hit fresh_cache cache_info = torch compiler load_cache_artifacts artifact_bytes assertEqual len cache_info inductor_artifacts assertEqual len cache_info autotune_artifacts autotune_expect assertEqual len cache_info aot_autograd_artifacts assertEqual len cache_info pgo_artifacts eager_result = fn b compiled_result = compiled_fn b assertEqual eager_result compiled_result assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file requires_triton config patch fx_graph_cache True fx_graph_remote_cache False autotune_local_cache True torch _dynamo config patch caching_precompile True parametrize dynamic False True parametrize device GPU_TYPE cpu parametrize dtype torch float torch bfloat test_cache_hot_load_caching_precompile device dtype dynamic Verify we can populate hot load functions cache device == GPU_TYPE HAS_GPU raise unittest SkipTest f requires GPU_TYPE device == cuda dtype == torch bfloat SM OrLater raise unittest SkipTest requires SM later fn x y x sin y = torch rand dtype=dtype device=device requires_grad=True b = torch rand dtype=dtype device=device requires_grad=True Record artifacts fresh_cache compiled_fn = torch compile fn dynamic=dynamic A first call should miss cache eager_result = fn b compiled_result = compiled_fn b compiled_result sum backward assertEqual eager_result compiled_result assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters dynamo_cache dynamo_cache_miss assertEqual counters dynamo_cache dynamo_cache_hit artifacts = torch compiler save_cache_artifacts assertIsNotNone artifacts artifact_bytes cache_info = artifacts autotune_expect = device == GPU_TYPE assertEqual len cache_info inductor_artifacts assertEqual len cache_info autotune_artifacts autotune_expect assertEqual len cache_info aot_autograd_artifacts assertEqual len cache_info pgo_artifacts assertEqual len cache_info precompile_artifacts reset Clean triton kernels shutil rmtree os path join cache_dir triton ignore_errors=True We did load anything so dont hit yet fresh_cache eager_result = fn b With caching precompile we have re torch compile function trigger cache lookup compiled_fn = torch compile fn dynamic=dynamic compiled_result = compiled_fn b compiled_result sum backward assertEqual eager_result compiled_result assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters dynamo_cache dynamo_cache_miss assertEqual counters dynamo_cache dynamo_cache_hit reset Clean triton kernels shutil rmtree os path join cache_dir triton ignore_errors=True Hot load hit fresh_cache torch compiler set_stance fail_on_recompile cache_info = torch compiler load_cache_artifacts artifact_bytes assertEqual len cache_info inductor_artifacts assertEqual len cache_info autotune_artifacts autotune_expect assertEqual len cache_info aot_autograd_artifacts assertEqual len cache_info pgo_artifacts assertEqual len cache_info precompile_artifacts With caching precompile we have re torch compile function trigger cache lookup compiled_fn = torch compile fn dynamic=dynamic eager_result = fn b compiled_result = compiled_fn b compiled_result sum backward assertEqual eager_result compiled_result assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters dynamo_cache dynamo_cache_miss assertEqual counters dynamo_cache dynamo_cache_hit config patch fx_graph_cache True fx_graph_remote_cache False test_cache_hot_load_repeat fn x y x y sin compiled_fn = torch compile fn dynamic=False = torch randn b = torch randn = torch randn b = torch randn fresh_cache eager_result = fn b compiled_result = compiled_fn b assertEqual eager_result compiled_result assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit artifacts = torch compiler save_cache_artifacts assertFalse torch compiler _cache CacheArtifactManager need_serialize assertIsNotNone artifacts artifact_bytes cache_info = artifacts reset fresh_cache torch compiler load_cache_artifacts artifact_bytes eager_result = fn b compiled_result = compiled_fn b assertEqual eager_result compiled_result assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertFalse torch compiler _cache CacheArtifactManager need_serialize reset fresh_cache eager_result = fn b compiled_result = compiled_fn b assertEqual eager_result compiled_result assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertTrue torch compiler _cache CacheArtifactManager need_serialize torch _dynamo config patch automatic_dynamic_local_pgo=True torch _functorch config patch enable_autograd_cache False config patch fx_graph_cache True fx_graph_remote_cache False test_cache_hot_load_pgo Verify we can populate hot load functions cache pgo backend = torch _dynamo testing CompileCounterWithBackend inductor torch compile backend=backend fullgraph=True f x x Record artifacts torch compiler config patch job_id=self id fresh_cache f torch randn f torch randn assertEqual backend frame_count assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file artifacts = torch compiler save_cache_artifacts assertIsNotNone artifacts artifact_bytes cache_info = artifacts assertEqual len cache_info inductor_artifacts assertEqual len cache_info autotune_artifacts assertEqual len cache_info aot_autograd_artifacts assertEqual len cache_info pgo_artifacts reset backend clear Clean triton kernels shutil rmtree os path join cache_dir triton ignore_errors=True Hot load hit torch compiler config patch job_id id fresh_cache cache_info = torch compiler load_cache_artifacts artifact_bytes assertEqual len cache_info inductor_artifacts assertEqual len cache_info autotune_artifacts assertEqual len cache_info aot_autograd_artifacts assertEqual len cache_info pgo_artifacts f torch randn f torch randn assertEqual backend frame_count assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file torch _dynamo config patch automatic_dynamic_local_pgo=True torch _functorch config patch enable_autograd_cache False config patch fx_graph_cache True fx_graph_remote_cache False test_cache_hot_load_pgo_swap_file_names Verify we can populate hot load functions cache pgo file name swapping backend = torch _dynamo testing CompileCounterWithBackend inductor torch compile backend=backend fullgraph=True f x x Record artifacts mock patch torch _utils_internal get_mast_job_name_version return_value= foo fresh_cache f torch randn f torch randn assertEqual backend frame_count artifacts = torch compiler save_cache_artifacts assertIsNotNone artifacts artifact_bytes cache_info = artifacts assertEqual len cache_info pgo_artifacts reset backend clear Clean triton kernels shutil rmtree os path join cache_dir triton ignore_errors=True Hot load hit mock patch torch _utils_internal get_mast_job_name_version return_value= bar fresh_cache cache_info = torch compiler load_cache_artifacts artifact_bytes assertEqual len cache_info pgo_artifacts f torch randn f torch randn assertEqual backend frame_count test_cache_hot_load_empty assertIsNone torch compiler save_cache_artifacts test_cache_hot_load_generic CacheStub __init__ cache = lookup key content = cache get key content None None CacheArtifactManager record_artifact ArbitraryCacheArtifact type key content content save key content cache key = content CacheArtifactManager record_artifact ArbitraryCacheArtifact type key content clear cache clear cache_stub = CacheStub CacheArtifactFactory register ArbitraryCacheArtifact CacheArtifact override populate_cache - None cache_stub cache key = content decode override staticmethod type - str test override staticmethod encode content str - bytes content encode test_cache = foo bar foo bar k v test_cache items cache_stub save k v artifacts = torch compiler save_cache_artifacts assertIsNotNone artifacts artifact_bytes cache_info = artifacts assertEqual len cache_info test_artifacts cache_stub clear CacheArtifactManager clear cache_info = torch compiler load_cache_artifacts artifact_bytes assertEqual len cache_info test_artifacts assertEqual cache_stub cache test_cache CacheArtifactManager clear cache_stub lookup foo artifacts = torch compiler save_cache_artifacts assertIsNotNone artifacts _ cache_info = artifacts assertEqual len cache_info test_artifacts requires_triton config patch fx_graph_cache True config patch fx_graph_remote_cache False parametrize device GPU_TYPE cpu parametrize dtype torch float torch float parametrize dynamic False True test_cache_load_model device dtype dynamic Verify we can populate load models cache device == GPU_TYPE HAS_GPU raise unittest SkipTest f requires GPU_TYPE fn mod x mod zero_grad mod x sum backward p grad p mod parameters compiled_fn = torch compile fn dynamic=dynamic mod = MyModelConv d device=device dtype=dtype inp = torch randn device=device dtype=dtype The first call should see all cache misses counters clear grads = compiled_fn mod inp assertGreater counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit The second should see all hits First reset so in-memory guards don t prevent compilation counters clear reset grads = compiled_fn mod inp assertEqual counters inductor fxgraph_cache_miss assertGreater counters inductor fxgraph_cache_hit And results should same assertEqual grads grads largeTensorTest GB device=GPU_TYPE inductor=True config patch fx_graph_cache True config patch fx_graph_remote_cache False parametrize device GPU_TYPE parametrize dtype torch float torch bfloat test_cache_load_with_guards_int _bounds device dtype Test caching same graph under conditions introduce guards tensor sizes int device == GPU_TYPE HAS_GPU raise unittest SkipTest f requires GPU_TYPE device == cuda dtype == torch bfloat SM OrLater raise unittest SkipTest requires CUDA SM later fn x y x + x y + y compiled_fn = torch compile fn dynamic=True Iterate over different shapes varying whether total size below above int For each combination we expect different guards around whether symbolic sizes do do exceed int shapes = a_shape b_shape shapes = torch rand a_shape device=device dtype=dtype b = torch rand b_shape device=device dtype=dtype AVOID dynamo reset here We expect guards have been added will violated new shape We should see recompilation along cache miss counters clear res = compiled_fn b assertGreater counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit A second call should hit Reset here force compilation counters clear reset res = compiled_fn b assertEqual counters inductor fxgraph_cache_miss assertGreater counters inductor fxgraph_cache_hit assertEqual res res config patch fx_graph_cache True config patch fx_graph_remote_cache False parametrize device GPU_TYPE cpu parametrize dtype torch float torch bfloat test_cache_load_with_guards_static_bounds device dtype Test caching same graph under conditions introduce guards static bounds device == GPU_TYPE HAS_GPU raise unittest SkipTest f requires GPU_TYPE device == cuda dtype == torch bfloat SM OrLater raise unittest SkipTest requires SM later See lowering all pooling operators we always guard make height width static fn x torch nn functional adaptive_avg_pool d x compiled_fn = torch compile fn dynamic=True Iterate over different input shapes Each new shape should cause cache miss shapes = shape shapes x = torch rand shape device=device dtype=dtype AVOID dynamo reset here For each cache hit we expect guards have been added will violated each new shape We should see recompilation along cache miss counters clear res = compiled_fn x assertGreater counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit A second call should hit counters clear reset res = compiled_fn x assertEqual counters inductor fxgraph_cache_miss assertGreater counters inductor fxgraph_cache_hit assertEqual res res config patch fx_graph_cache True torch _functorch config patch enable_autograd_cache False config patch fx_graph_remote_cache False unittest skipIf TEST_MULTIGPU only one GPU detected requires_cuda_and_triton test_no_arguments_tensor_device_guards Usually when there example inputs device index inputs sufficient make sure we don t cache hit results different cuda devices When input has no arguments we still need have cuda device index cache key torch compile f y = torch randn device= cuda y torch cuda _DeviceGuard torch cuda set_device result = f assertEqual result device torch device cuda reset Should cache hit device guard torch cuda _DeviceGuard torch cuda set_device result = f assertEqual result device torch device cuda config patch fx_graph_cache True torch _functorch config patch enable_autograd_cache False config patch fx_graph_remote_cache False unittest skipIf TEST_MULTIGPU only one GPU detected requires_cuda_and_triton test_tensor_device_guards_cpu_tensor CPU tensor arguments should still cache hit torch compile f x x sin torch cuda _DeviceGuard torch cuda set_device result = f torch randn device= cpu assertEqual result device torch device cpu reset Should cache hit device guard torch cuda _DeviceGuard torch cuda set_device result = f torch randn device= cpu assertEqual result device torch device cpu assertEqual counters inductor fxgraph_cache_hit config patch fx_graph_cache True config patch fx_graph_remote_cache False parametrize device GPU_TYPE cpu test_constant_handling device Test different constants recognized correctly device == GPU_TYPE HAS_GPU raise unittest SkipTest f requires GPU_TYPE fn x x + torch tensor list range device=device fn x x + torch tensor list range device=device = torch rand device=device compiled_fn = torch compile fn compiled_fn = torch compile fn A call fn should miss cache assertEqual fn compiled_fn assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit A call fn should also miss constant different assertEqual fn compiled_fn assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit config patch fx_graph_cache True config patch fx_graph_remote_cache False parametrize variant v v test_auto_functionalized_caching variant variant == v patch = torch _inductor config patch enable_auto_functionalized_v =False assert variant == v patch = torch _inductor config patch enable_auto_functionalized_v =True torch library custom_op mylib sin_inplace mutates_args= x sin_inplace x torch Tensor - None x sin_ torch library custom_op mylib cos_inplace mutates_args= x cos_inplace x torch Tensor - None x cos_ torch compile fullgraph=True fn x op y = torch empty_like x op y y x = torch randn patch A first call should miss cache fn x sin_inplace reset assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file A second call should hit First reset so in-memory guards don t prevent compilation reset fn x sin_inplace assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file A third call different operator should have cache miss reset fn x cos_inplace assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file requires_gpu_and_triton config patch fx_graph_cache True config patch fx_graph_remote_cache False with_tf _off test_flex_attention_caching torch nn attention flex_attention create_block_mask flex_attention block_mask = create_block_mask lambda b h q kv q = kv None None score_mod score b h q kv score + q - kv fn q k v flex_attention q k v score_mod=score_mod block_mask=block_mask score_mod score b h q kv score fn q k v flex_attention q k v score_mod=score_mod block_mask=block_mask b c = torch randn GPU_TYPE _ range compiled_fn = torch compile fn compiled_fn = torch compile fn atol rtol = e- e- A first call should miss cache assertEqual fn b c compiled_fn b c atol=atol rtol=rtol assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file A second call should hit First reset so in-memory guards don t prevent compilation reset assertEqual fn b c compiled_fn b c atol=atol rtol=rtol assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file A third call different score_mod should have cache miss reset assertEqual fn b c compiled_fn b c atol=atol rtol=rtol assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_lookup_write_file requires_gpu requires_triton config patch fx_graph_cache True config patch fx_graph_remote_cache False parametrize bundle_triton False True test_higher_order_op_bypass bundle_triton Verify we bypass cache when we have higher order ops bundler start end works cache bypass fn x true_fn x torch Tensor x cos false_fn x torch Tensor x sin torch cond x shape true_fn false_fn x config patch bundle_triton_into_fx_graph_cache=bundle_triton compiled_fn = torch compile fn dynamic=True fullgraph=True x = torch randn device=GPU_TYPE compiled_fn x assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertGreater counters inductor fxgraph_cache_bypass requires_gpu requires_triton config patch fx_graph_cache True config patch fx_graph_remote_cache False parametrize bundle_triton False True test_triton_higher_order_op bundle_triton Verify we can cache user defined triton kernel higher order op fn x y n_elements = x numel grid = lambda meta noqa E triton cdiv n_elements meta BLOCK_SIZE add_kernel grid x y x n_elements BLOCK_SIZE= x fn x y n_elements = x numel grid = lambda meta noqa E triton cdiv n_elements meta BLOCK_SIZE sub_kernel grid x y x n_elements BLOCK_SIZE= x config patch bundle_triton_into_fx_graph_cache=bundle_triton compiled_fn = torch compile fn fullgraph=True compiled_fn = torch compile fn fullgraph=True x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE compiled_fn x y assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_cache_bypass A second call should hit First reset so in-memory guards don t prevent compilation reset Clean PyCodeCache triton kernels PyCodeCache cache_clear shutil rmtree os path join cache_dir triton ignore_errors=True compiled_fn x y assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_cache_bypass A second call should hit First reset so in-memory guards don t prevent compilation reset Clean PyCodeCache triton kernels PyCodeCache cache_clear shutil rmtree os path join cache_dir triton ignore_errors=True compiled_fn x y assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_cache_bypass requires_gpu requires_triton config patch fx_graph_cache True config patch fx_graph_remote_cache False parametrize bundle_triton False True test_triton_higher_order_op_different_configs bundle_triton Verify user defined triton kernel different configs cached separately add_kernel = triton autotune configs= triton Config BLOCK_SIZE num_stages= num_warps= triton Config BLOCK_SIZE num_stages= num_warps= key= add_kernel add_kernel = triton autotune configs= triton Config BLOCK_SIZE num_stages= num_warps= triton Config BLOCK_SIZE num_stages= num_warps= key= add_kernel fn x y n_elements = x numel grid = lambda meta noqa E triton cdiv n_elements meta BLOCK_SIZE add_kernel grid x y x n_elements x fn x y n_elements = x numel grid = lambda meta noqa E triton cdiv n_elements meta BLOCK_SIZE add_kernel grid x y x n_elements x config patch bundle_triton_into_fx_graph_cache=bundle_triton compiled_fn = torch compile fn fullgraph=True compiled_fn = torch compile fn fullgraph=True x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE compiled_fn x y assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_cache_bypass A second call should hit First reset so in-memory guards don t prevent compilation reset Clean PyCodeCache triton kernels PyCodeCache cache_clear shutil rmtree os path join cache_dir triton ignore_errors=True compiled_fn x y assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_cache_bypass A second call should hit First reset so in-memory guards don t prevent compilation reset Clean PyCodeCache triton kernels PyCodeCache cache_clear shutil rmtree os path join cache_dir triton ignore_errors=True compiled_fn x y assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_cache_bypass requires_gpu requires_triton config patch fx_graph_cache True config patch fx_graph_remote_cache False config patch compile_threads parametrize bundle_triton False True parametrize use_static_cuda_launcher False True test_triton_op bundle_triton use_static_cuda_launcher use_static_cuda_launcher TEST_WITH_ROCM raise unittest SkipTest Static cuda launcher doesn t work ROCM libname = my_cool_namespace opname = my_triton_operator torch _library triton_op f libname opname mutates_args= add x torch Tensor y torch Tensor - torch Tensor output = torch empty_like x n_elements = output numel grid meta triton cdiv n_elements meta BLOCK_SIZE capture_triton add_kernel grid x y output n_elements output f x y add x y compile_threads = use_static_cuda_launcher config compile_threads config patch bundle_triton_into_fx_graph_cache=bundle_triton use_static_cuda_launcher=use_static_cuda_launcher compile_threads=compile_threads compiled_fn = torch compile f fullgraph=True x = torch randn device=GPU_TYPE y = torch randn device=GPU_TYPE compiled_fn x y assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_cache_bypass A second call should hit First reset so in-memory guards don t prevent compilation reset Clean PyCodeCache triton kernels PyCodeCache cache_clear shutil rmtree os path join cache_dir triton ignore_errors=True compiled_fn x y assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual counters inductor fxgraph_cache_bypass config patch fx_graph_cache True config patch fx_graph_remote_cache False test_generated_kernel_count Test we bump generated_kernel_count metric cache hit torch _logging set_logs inductor_metrics=True fn x y x y + y = torch rand b = torch rand compiled_fn = torch compile fn metrics reset assertEqual metrics generated_kernel_count Verify miss case assertEqual fn b compiled_fn b assertEqual counters inductor fxgraph_cache_hit assertEqual metrics generated_kernel_count Verify hit case reset assertEqual fn b compiled_fn b assertEqual counters inductor fxgraph_cache_hit assertEqual metrics generated_kernel_count torch _logging set_logs config patch fx_graph_cache True config patch fx_graph_remote_cache False test_inductor_counters Test we bump inductor counters cache hit fn b torch mm b = torch rand device= cpu b = torch rand device= cpu compiled_fn = torch compile fn Verify miss case assertEqual fn b compiled_fn b assertEqual counters inductor fxgraph_cache_hit Verify hit case reset assertEqual fn b compiled_fn b assertEqual counters inductor fxgraph_cache_hit config patch fx_graph_cache True config patch fx_graph_remote_cache False test_cache_clear Test clearing cache fn x y x y = torch rand b = torch rand compiled_fn = torch compile fn A first call should miss cache assertEqual fn b compiled_fn b assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit A second call should hit counters clear reset assertEqual fn b compiled_fn b assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit Clear cache now we should miss counters clear reset torch _inductor codecache FxGraphCache clear assertEqual fn b compiled_fn b assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit config patch fx_graph_cache True config patch fx_graph_remote_cache False test_cache_with_nt gen_nt r values = torch randn r offsets = torch tensor r torch nested nested_tensor_from_jagged values offsets fn nt nt values size == nt sin nt cos inp = gen_nt inp = gen_nt counters clear torch compile fn inp torch compile fn inp assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit reset counters clear torch compile fn inp torch compile fn inp assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit config patch fx_graph_cache True config patch fx_graph_remote_cache False test_cache_with_symint_non_arg_guard fn x ref_id self_id = self_id == ref_id x = torch mul x x = torch mul x x x = torch ones counters clear torch compile fn fullgraph=True dynamic=True x assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit reset counters clear torch compile fn fullgraph=True dynamic=True x assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit config patch fx_graph_cache True config patch fx_graph_remote_cache False test_cache_guard f x val val x sin x cos x = torch ones = torch compile f dynamic=True x assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit reset counters clear b = torch compile f dynamic=True x assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertNotEqual b config patch fx_graph_cache False fx_graph_remote_cache False requires_cuda_and_triton unittest expectedFailure TODO pass optimize_mem runtime test_async_compile_cache SimpleFunction torch autograd Function staticmethod forward ctx x x staticmethod backward ctx grad_output grad_output x = torch rand requires_grad=True device= cuda counters clear sf = SimpleFunction out = torch compile sf apply x out sum backward assertEqual counters inductor async_compile_cache_miss assertEqual counters inductor async_compile_cache_hit config patch fx_graph_cache True test_cache_guard_overspec b = torch tensor torch compile MyModel torch nn Module forward x torch isin x b model = MyModel counters clear i range model torch arange i assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit reset counters clear i range model torch arange i assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit config patch fx_graph_cache True config patch fx_graph_remote_cache False config patch freezing True parametrize device GPU_TYPE cpu parametrize inlinable True False test_freezing device inlinable device == GPU_TYPE HAS_GPU raise unittest SkipTest f requires GPU_TYPE For machines mkldnn_fp support weight_pack mkldnn_fusion py causes creation mkldnn format tensor which current implementation does support device == cpu torch backends mkldnn is_available torch ops mkldnn _is_mkldnn_fp _supported raise unittest SkipTest mkldnn tensors unsupported The shape frozen constant determines will inlined shape = inlinable MM torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand shape forward x x param dtype = torch float Populate cache entry mod = MM device=device dtype=dtype torch no_grad x = torch rand shape device=device dtype=dtype out = mod x out = torch compile mod x assertEqual out out assertEqual counters inductor fxgraph_cache_bypass assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit counters clear reset Same nn Module different parameters In case param can inlined we should consider actual tensor value we expect cache miss because values different here If param cannot inlined then we consider only tensor metadata we expect cache hit mod = MM device=device dtype=dtype assertNotEqual mod param mod param torch no_grad x = torch rand shape device=device dtype=dtype out = mod x out = torch compile mod x assertEqual out out assertEqual counters inductor fxgraph_cache_bypass assertEqual counters inductor fxgraph_cache_miss inlinable assertEqual counters inductor fxgraph_cache_hit inlinable instantiate_parametrized_tests TestStandaloneCompile TestCase setUp super setUp counters clear PatchCaches setUp CacheArtifactManager clear tearDown super tearDown PatchCaches tearDown reset AOTAutogradCache clear PyCodeCache cache_clear purge=True torch _dynamo reset clear_caches capture fn dynamic=None inner args gm = None actual_args = None kwargs = None backend gm_ args_ kwargs_ nonlocal gm nonlocal actual_args nonlocal kwargs gm = gm_ actual_args = args_ kwargs = kwargs_ gm _ = torch compile fn fullgraph=True backend=backend dynamic=dynamic args gm actual_args kwargs inner config patch fx_graph_cache True config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True parametrize device GPU_TYPE cpu parametrize format binary unpacked parametrize dynamic False True parametrize graph_partition False True parametrize is_aot False True test_basic device str format str dynamic bool graph_partition bool is_aot bool - None device == GPU_TYPE HAS_GPU raise unittest SkipTest f requires GPU_TYPE AOT mode does support unpacked format is_aot format == unpacked raise unittest SkipTest AOT mode does support unpacked format mod = torch nn Linear device=device x = torch randn device=device dynamic torch _dynamo mark_dynamic x f x torch no_grad mod x x sin eager_out = f x tempfile TemporaryDirectory temp_dir config patch graph_partition=graph_partition path = temp_dir format == unpacked os path join temp_dir compiled_artifact bin fresh_cache gm args kwargs = capture f x assert kwargs compiled_artifact = torch _inductor standalone_compile gm args aot=is_aot compiled_artifact save path=path format=format assertEqual counters inductor fxgraph_cache_hit fresh_cache loaded = torch _inductor CompiledArtifact load path=path format=format dynamic concrete_args = isinstance torch SymInt args concrete_args = args compiled_out = loaded concrete_args assertEqual eager_out compiled_out is_aot assertEqual counters inductor fxgraph_cache_hit config patch fx_graph_cache True config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True parametrize dynamic False True parametrize is_aot False True test_call_in_backend dynamic bool is_aot bool - None mod = torch nn Linear x = torch randn dynamic torch _dynamo mark_dynamic x f x torch no_grad mod x eager_out = f x backend gm args kwargs torch _inductor standalone_compile gm args aot=is_aot fresh_cache compiled_out = torch compile f fullgraph=True backend=backend x assertEqual eager_out compiled_out config patch fx_graph_cache True config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True test_save_in_new_path - None mod = torch nn Linear x = torch randn f x torch no_grad mod x eager_out = f x tempfile TemporaryDirectory temp_dir path = os path join temp_dir new_dir fresh_cache gm args kwargs = capture f x assert kwargs compiled_artifact = torch _inductor standalone_compile gm args compiled_artifact save path=path format= unpacked assertEqual counters inductor fxgraph_cache_hit fresh_cache loaded = torch _inductor CompiledArtifact load path=path format= unpacked compiled_out = loaded args assertEqual eager_out compiled_out config patch fx_graph_cache True config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True parametrize device GPU_TYPE cpu test_modify_unpacked_file device str - None device == GPU_TYPE HAS_GPU raise unittest SkipTest f requires GPU_TYPE x = torch ones device=device f x torch no_grad x x sin eager_out = f x tempfile TemporaryDirectory temp_dir fresh_cache gm args kwargs = capture f x assert kwargs compiled_artifact = torch _inductor standalone_compile gm args compiled_out = compiled_artifact args assertEqual eager_out compiled_out compiled_artifact save path=temp_dir format= unpacked assertEqual counters inductor fxgraph_cache_hit fresh_cache Now modify output file expect see changes subdir os listdir temp_dir subdir aotautograd fxgraph continue subdir_path = os path join temp_dir subdir file os listdir subdir_path file_path = os path join subdir_path file assert os path isfile file_path open file_path f file_contents = f read device == GPU_TYPE file_contents = file_contents replace tmp = tmp = assert device == cpu file_contents = file_contents replace auto tmp = static_cast float auto tmp = static_cast float open file_path w f f write file_contents loaded = torch _inductor CompiledArtifact load path=temp_dir format= unpacked compiled_out = loaded args assertEqual eager_out compiled_out assertEqual counters inductor fxgraph_cache_hit unittest skipIf IS_FBCODE torch error config patch fx_graph_cache True config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True test_different_process x = torch ones f x x sin gm args kwargs = capture f x assert kwargs tempfile TemporaryDirectory temp_dir path = normalize_path_separator os path join temp_dir compiled_artifact bin fresh_cache compiled_artifact = torch _inductor standalone_compile gm args compiled_artifact save path=path script = f torch torch _inductor utils fresh_cache arg = torch ones fresh_cache loaded = torch _inductor CompiledArtifact load path= path compiled_result = loaded arg eager_result = arg sin torch allclose eager_result compiled_result atol= rtol= raise RuntimeError tensors do match try subprocess check_output sys executable -c script stderr=subprocess STDOUT cwd=os path dirname os path realpath __file__ except subprocess CalledProcessError e fail msg= Subprocess exception while attempting run test + e output decode utf- config patch fx_graph_cache True config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True parametrize is_aot False True test_dynamic_shapes_from_graph is_aot bool f x x shape x x = torch ones torch _dynamo mark_dynamic x fresh_cache captured graph lambda s x x s gm args kwargs = capture f x assert kwargs compiled_artifact = torch _inductor standalone_compile gm args dynamic_shapes= from_graph aot=is_aot x = torch ones result = compiled_artifact x assertEqual result x config patch fx_graph_cache True config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True functorch_config patch autograd_cache_normalize_inputs True parametrize is_aot False True test_split_module is_aot Mod torch nn Module forward x b b c c x = x + + x = x + b + b x = x + c + c x seen = splits = split n nonlocal seen seen splits seen += seen splits seen += seen += t torch randn x = t = t = t b = t b = t c = t c = t example_inputs = x b b c c gm inps _ = capture Mod example_inputs split = torch fx passes split_module split_module gm gm split Each split graphs only has one output ca = torch _inductor standalone_compile split submod_ x aot=is_aot ca = torch _inductor standalone_compile split submod_ b x b aot=is_aot ca = torch _inductor standalone_compile split submod_ c x c aot=is_aot y = ca x y = ca b y b y = ca c y c is_aot fx graph cache doesn t run AOT mode assertEqual counters inductor fxgraph_cache_bypass assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit TODO split_module causes ca ca have different type annotations parameter x so we can only AOTAutogradCache cache hit once instead twice assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_hit assertEqual counters aot_autograd autograd_cache_saved expected = Mod example_inputs assertEqual y expected config patch fx_graph_cache True config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True parametrize is_aot False True parametrize config_patches True False test_dynamic_shapes_from_example_inputs config_patches is_aot f x x shape x x = torch ones torch _dynamo mark_dynamic x fresh_cache captured graph lambda s x x s gm args kwargs = capture f x assert kwargs config_patches config_patches = fx_graph_cache True config_patches = None specialized example inputs compiled_artifact = torch _inductor standalone_compile gm torch ones dynamic_shapes= from_example_inputs options= config_patches config_patches aot=is_aot x = torch ones result = compiled_artifact x int baked assertEqual result x size baked assertRaisesRegex AssertionError expected size == x = torch randn result = compiled_artifact x config patch fx_graph_cache True config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True parametrize is_aot True False parametrize dynamic_shapes from_graph from_example_inputs test_static_shapes dynamic_shapes is_aot f x x shape x static_x = torch randn fresh_cache static_gm lambda x x static_gm args kwargs = capture f dynamic=False static_x assert kwargs compiled_artifact = torch _inductor standalone_compile static_gm static_x dynamic_shapes=dynamic_shapes aot=is_aot x = torch randn result = compiled_artifact x assertEqual result x assertRaisesRegex AssertionError expected size == x = torch randn result = compiled_artifact x config patch fx_graph_cache True config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True parametrize is_aot True False parametrize dynamic_shapes from_tracing_context from_graph test_backend dynamic_shapes is_aot f x x shape x x = torch randn torch _dynamo mark_dynamic x backend gm args kwargs compiled_artifact = torch _inductor standalone_compile gm args dynamic_shapes=dynamic_shapes aot=is_aot y = torch randn result = compiled_artifact y assertEqual result y compiled_artifact torch _dynamo reset _ = torch compile f backend=backend x config patch fx_graph_cache True config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True parametrize is_aot True False test_backend_dynamic_shapes_from_example_inputs is_aot f x x shape x x = torch ones torch _dynamo mark_dynamic x backend gm args kwargs compiled_artifact = torch _inductor standalone_compile gm torch ones dynamic_shapes= from_example_inputs aot=is_aot y = torch ones result = compiled_artifact y baked assertEqual result y shape y baked assertRaisesRegex AssertionError expected size == y = torch ones result = compiled_artifact y compiled_artifact torch _dynamo reset _ = torch compile f backend=backend x config patch fx_graph_cache True config patch fx_graph_remote_cache False functorch_config patch enable_autograd_cache True parametrize dynamic_shapes from_tracing_context from_graph from_example_inputs test_backend_static_shapes dynamic_shapes static_x all these options should produce static graph s bit hard tell so these just smoke tests static_x = torch randn f x x shape x backend gm args kwargs torch _inductor standalone_compile gm args dynamic_shapes=dynamic_shapes result = torch compile f backend=backend static_x assertEqual result static_x config patch fx_graph_cache True config patch fx_graph_remote_cache False test_custom_pass_handling Test properly-registered custom hooks allow caching TestCustomGraphPass CustomGraphPass __call__ graph torch fx graph Graph - None None uuid - Optional Union bytes str uuid fn b torch mm b = torch rand device= cpu b = torch rand device= cpu compiled_fn = torch compile fn The cache should bypassed custom hook doesn t use CustomGraphPass config patch post_grad_custom_pre_pass lambda x x assertEqual fn b compiled_fn b assertEqual counters inductor fxgraph_cache_bypass assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit With proper usage we expect normal caching custom_pass = TestCustomGraphPass config patch post_grad_custom_pre_pass custom_pass post_grad_custom_post_pass custom_pass joint_custom_pre_pass custom_pass joint_custom_post_pass custom_pass reset counters clear Cache miss assertEqual fn b compiled_fn b assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit reset counters clear Cache hit assertEqual fn b compiled_fn b assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit TestCustomPartitionerFn CustomPartitionerFn __init__ _uuid = None __call__ gm joint_inputs kwargs - tuple torch fx GraphModule torch fx GraphModule gm gm Dummy implementation uuid - Optional Union bytes str _uuid TestFxGraphCacheHashing TestCase test_parameter_constants Test hashing parameter constants small = torch nn Parameter torch rand large = torch nn Parameter torch rand assertTrue GraphLowering can_inline_constant small assertFalse GraphLowering can_inline_constant large By default we hash metadata values independent size gm = torch fx GraphModule torch fx Graph pickler = FxGraphCachePickler gm data = pickler dumps small assertIsInstance pickle loads data TensorMetadataAndValues data = pickler dumps large assertIsInstance pickle loads data TensorMetadataAndValues For frozen parameters we only hash values small tensors gm _has_frozen_params = True gm _frozen_param = small gm _frozen_param = large small _is_frozen_param = True large _is_frozen_param = True pickler = FxGraphCachePickler gm data = pickler dumps small assertIsInstance pickle loads data TensorMetadataAndValues data = pickler dumps large assertIsInstance pickle loads data TensorMetadata test_hash_fake_tensors Test hashing pickling FakeTensors various characteristics gm = torch fx GraphModule torch fx Graph pickler = FxGraphCachePickler gm torch _subclasses FakeTensorMode Verify FakeTensors get pickled into TensorMetadata data = pickler dumps torch randn assertIsInstance pickle loads data TensorMetadata Different shapes assertEqual pickler dumps torch randn pickler dumps torch randn assertNotEqual pickler dumps torch randn pickler dumps torch randn assertNotEqual pickler dumps torch randn pickler dumps torch randn assertEqual pickler dumps torch randn pickler dumps torch randn assertNotEqual pickler dumps torch randn pickler dumps torch randn assertNotEqual pickler dumps torch randn pickler dumps torch randn Different strides assertEqual pickler dumps torch randn pickler dumps torch randn transpose transpose assertNotEqual pickler dumps torch randn pickler dumps torch randn transpose Different storage offsets assertEqual pickler dumps torch randn pickler dumps torch randn assertEqual pickler dumps torch randn pickler dumps torch randn Different dtypes assertEqual pickler dumps torch randn dtype=torch float pickler dumps torch randn dtype=torch float assertNotEqual pickler dumps torch randn dtype=torch float pickler dumps torch randn dtype=torch float Different requires_grad assertEqual pickler dumps torch randn requires_grad=True pickler dumps torch randn requires_grad=True assertNotEqual pickler dumps torch randn requires_grad=True pickler dumps torch randn requires_grad=False Different memory formats assertNotEqual pickler dumps torch randn pickler dumps torch randn memory_format=torch channels_last Different devices assertEqual pickler dumps torch randn device= meta pickler dumps torch randn device= meta assertNotEqual pickler dumps torch randn device= meta pickler dumps torch randn device= cpu HAS_MULTIGPU assertEqual pickler dumps torch randn device=f GPU_TYPE pickler dumps torch randn device=f GPU_TYPE assertNotEqual pickler dumps torch randn device=f GPU_TYPE pickler dumps torch randn device=f GPU_TYPE test_hash_kwargs Test special handling kwargs when hashing i e ordering kwargs dict any set arguments gm = torch fx GraphModule torch fx Graph pickler = FxGraphCachePickler gm Dict order kwargs should affect hashes details = FxGraphHashDetails None z details = FxGraphHashDetails None z assertEqual pickler dumps details pickler dumps details Different kwarg values should affect hashes details = FxGraphHashDetails None details = FxGraphHashDetails None assertNotEqual pickler dumps details pickler dumps details Set order should affect hashes Sets unordered sorting creating new set seems change order set = b c d e f g set = set sorted set noqa C details = FxGraphHashDetails None set details = FxGraphHashDetails None set assertEqual pickler dumps details pickler dumps details But different set contents should affect hashes details = FxGraphHashDetails None details = FxGraphHashDetails None assertNotEqual pickler dumps details pickler dumps details test_hash_config_changes Test different config settings affect hashes config patch max_autotune False details = FxGraphHashDetails None details = FxGraphHashDetails None config patch max_autotune True details = FxGraphHashDetails None gm = torch fx GraphModule torch fx Graph pickler = FxGraphCachePickler gm assertEqual pickler dumps details pickler dumps details assertNotEqual pickler dumps details pickler dumps details test_hash_private_config_changes Test private config settings affect hashes config patch _micro_pipeline_tp False details = FxGraphHashDetails None details = FxGraphHashDetails None config patch _micro_pipeline_tp True details = FxGraphHashDetails None gm = torch fx GraphModule torch fx Graph pickler = FxGraphCachePickler gm assertEqual pickler dumps details pickler dumps details assertNotEqual pickler dumps details pickler dumps details test_non_serializable_custom_passes_causes_cache_miss Mod torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand forward x x param mod = Mod mod_compiled = torch compile mod torch no_grad x = torch rand miss mod_compiled x assertEqual counters inductor fxgraph_cache_bypass assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit hit torch _dynamo reset mod_compiled x assertEqual counters inductor fxgraph_cache_bypass assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit torch _dynamo reset counters clear hit mod_compiled x assertEqual counters inductor fxgraph_cache_bypass assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit config patch _fuse_ddp_communication_passes new_pass_foo_bar miss private config changed torch _dynamo reset mod_compiled x assertEqual counters inductor fxgraph_cache_bypass assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit torch _dynamo reset counters clear capture_logs torch _inductor codecache logging INFO logs config patch _fuse_ddp_communication_passes lambda args None bypass custom pass serializable mod_compiled x assertEqual counters inductor fxgraph_cache_bypass assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit counters clear assert our bypass explicit assertTrue any x getMessage == Bypassing FX Graph Cache because Unsupported _fuse_ddp_communication_pass x logs test_hash_custom_passes Test CustomGraphPass usage TestCustomGraphPass CustomGraphPass __init__ _uuid = None __call__ graph torch fx graph Graph - None None uuid - Optional Union bytes str _uuid custom_pass = TestCustomGraphPass config patch post_grad_custom_pre_pass custom_pass custom_pass _uuid = details = FxGraphHashDetails None details = FxGraphHashDetails None custom_pass _uuid = details = FxGraphHashDetails None gm = torch fx GraphModule torch fx Graph pickler = FxGraphCachePickler gm assertEqual pickler dumps details pickler dumps details assertNotEqual pickler dumps details pickler dumps details test_hash_custom_backend_pass Test CustomGraphModulePass usage TestCustomGraphModulePass CustomGraphModulePass __init__ _uuid = None __call__ gm torch fx GraphModule - None None uuid - Optional Union bytes str _uuid custom_pass = TestCustomGraphModulePass patch_inductor_backend cpu custom_pass=custom_pass custom_pass _uuid = details = FxGraphHashDetails None details = FxGraphHashDetails None custom_pass _uuid = details = FxGraphHashDetails None gm = torch fx GraphModule torch fx Graph pickler = FxGraphCachePickler gm assertEqual pickler dumps details pickler dumps details assertNotEqual pickler dumps details pickler dumps details test_hash_custom_backend_config Test cache correctness when custom inductor codegen config installed patch_inductor_backend cpu custom_backend_config=custom_inductor_config gm = torch fx GraphModule torch fx Graph pickler = FxGraphCachePickler gm details = FxGraphHashDetails None details = FxGraphHashDetails None assertEqual pickler dumps details pickler dumps details custom_inductor_config enable_optimisation = True details = FxGraphHashDetails None assertNotEqual pickler dumps details pickler dumps details torch _dynamo reset counters clear custom_inductor_config enable_optimisation = False x = torch zeros y = torch zeros compiled_fn = torch compile torch add compiled_fn x y assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit torch _dynamo reset counters clear compiled_fn x y assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit torch _dynamo reset counters clear Changing custom config should trigger recompilation custom_inductor_config enable_optimisation = True compiled_fn x y assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit test_hash_custom_partitioner_fn Test custom partitioner function s UUID properly used FX graph cache hashing custom_partitioner_fn = TestCustomPartitionerFn config patch custom_partitioner_fn custom_partitioner_fn custom_partitioner_fn _uuid = details = FxGraphHashDetails None details = FxGraphHashDetails None custom_partitioner_fn _uuid = details = FxGraphHashDetails None assertEqual details _custom_partitioner_fn assertEqual details _custom_partitioner_fn assertEqual details _custom_partitioner_fn gm = torch fx GraphModule torch fx Graph pickler = FxGraphCachePickler gm assertEqual pickler dumps details pickler dumps details assertNotEqual pickler dumps details pickler dumps details test_bypass_unsupported Test _reduce_unsupported gm = torch fx GraphModule torch fx Graph assertRaises BypassFxGraphCache FxGraphCachePickler gm dumps torch fx experimental _backward_state BackwardState test_stable_strings Test objects containing identical strings pickle same even they same id s = string s = strin codespell ignore s += g assertNotEqual id s id s gm = torch fx GraphModule torch fx Graph pickler = FxGraphCachePickler gm assertEqual pickler dumps s s pickler dumps s s test_get_hash_for_files Test get_hash_for_files helper delete=True does work Windows See https docs python org library tempfile html#tempfile NamedTemporaryFile tempfile NamedTemporaryFile delete=False temp try temp write b contents temp flush hash = get_hash_for_files temp name get_hash_for_files cache_clear hash = get_hash_for_files temp name temp write b temp flush get_hash_for_files cache_clear hash = get_hash_for_files temp name assertEqual hash hash assertNotEqual hash hash finally temp close os unlink temp name TestCudaCompileCommand TestCase requires_cuda_and_triton test_cuda_compile_command cmd_no_extra_args str = cuda_compile_command abc cu cu output so assert nvcc cmd_no_extra_args cmd_no_extra_args assert abc cu cmd_no_extra_args cmd_no_extra_args assert cu cmd_no_extra_args cmd_no_extra_args assert output cmd_no_extra_args cmd_no_extra_args cmd_extra_args str = cuda_compile_command abc cu cu output so -Wwhatever -nothing assert nvcc cmd_extra_args cmd_extra_args assert -Wwhatever cmd_extra_args cmd_extra_args assert -nothing cmd_extra_args cmd_extra_args assert abc cu cmd_extra_args cmd_extra_args assert cu cmd_extra_args cmd_extra_args assert output cmd_extra_args cmd_extra_args mock patch subprocess check_output check_output_mock CUDACodeCache compile test cu so -Wsomething check_output_mock assert_called cmd_parts list str = check_output_mock call_args assert cmd_parts endswith nvcc cmd_parts assert -Wsomething cmd_parts cmd_parts assert -DNDEBUG cmd_parts cmd_parts instantiate_parametrized_tests TestAutotuneCache TestCase device_type = GPU_TYPE setUp super setUp counters clear PatchCaches setUp tearDown super tearDown PatchCaches tearDown reset PyCodeCache cache_clear purge=True torch _dynamo reset clear_caches requires_cuda_and_triton unittest skipIf SM OrLater Requires SM + unittest skipIf TEST_WITH_ROCM Requires static cuda launcher which does support ROCM config patch use_static_cuda_launcher True config patch fx_graph_cache True config patch fx_graph_remote_cache False config patch autotune_local_cache False config patch autotune_remote_cache True config patch bundled_autotune_remote_cache False config patch max_autotune True config patch compile_threads Worker processes do register PatchCaches properly test_autotune_cache_warm_start Model torch nn Module forward x y b x + y + b f x y b Model x y b x = torch randn cuda y = torch randn cuda = torch randn cuda b = torch randn cuda f_compiled = torch compile f fullgraph=True PatchCaches = f_compiled x y b assertEqual global_stats autotune_remote Stats assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit Don t reset FxGraphCache see loads again torch _dynamo reset = f_compiled x y b assertEqual assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit assertEqual global_stats autotune_remote Stats Check cache entries seem reasonable k global_stats autotune_remote cache keys assertRegex k r - a-z k global_stats triton cache keys assertRegex k r triton - a-f - a-f c - + requires_gpu_and_triton unittest skipIf HAS_XPU_AND_TRITON SM OrLater Requires SM + config patch fx_graph_cache False config patch fx_graph_remote_cache False config patch autotune_local_cache False config patch autotune_remote_cache True config patch bundled_autotune_remote_cache False config patch max_autotune True config patch compile_threads Worker processes do register PatchCaches properly test_autotune_cache Model torch nn Module forward x y b x + y + b f x y b Model x y b x = torch randn GPU_TYPE y = torch randn GPU_TYPE = torch randn GPU_TYPE b = torch randn GPU_TYPE f_compiled = torch compile f fullgraph=True PatchCaches f_compiled x y b assertEqual global_stats autotune_remote Stats reset f_compiled x y b assertEqual global_stats autotune_remote Stats Check cache entries seem reasonable k global_stats autotune_remote cache keys assertRegex k r - a-z k global_stats triton cache keys assertRegex k r triton - a-f - a-f c - + requires_gpu_and_triton unittest skipIf HAS_XPU_AND_TRITON SM OrLater Requires SM + config patch fx_graph_cache False config patch fx_graph_remote_cache False config patch autotune_local_cache True config patch autotune_remote_cache False config patch bundled_autotune_remote_cache True config patch compile_threads config patch max_autotune True test_bundled_autotune_remote_cache Model torch nn Module forward b c d e f + b c + d e + f f b c d e f Model b c d e f f_compiled = torch compile f fullgraph=True = torch randn GPU_TYPE b = torch randn GPU_TYPE c = torch randn GPU_TYPE d = torch randn GPU_TYPE e = torch randn GPU_TYPE f = torch randn GPU_TYPE PatchCaches f_compiled b c d e f assertEqual global_stats autotune_local Stats assertEqual global_stats bundled_autotune Stats reset f_compiled b c d e f assertEqual global_stats autotune_local Stats assertEqual global_stats bundled_autotune Stats torch compiler config patch cache_key_tag test global_stats reset reset f_compiled b c d e f assertEqual global_stats autotune_local Stats assertEqual global_stats bundled_autotune Stats reset f_compiled b c d e f assertEqual global_stats autotune_local Stats assertEqual global_stats bundled_autotune Stats Check cache entries seem reasonable k global_stats autotune_local cache keys assertRegex k r tmp ^ ^ ^ \ best_config k global_stats bundled_autotune cache keys assertRegex k r pt bundled-autotune-v - a-z c - + k global_stats triton cache keys assertRegex k r triton - a-f - a-f c - + requires_triton requires_gpu_and_triton unittest skipIf HAS_XPU_AND_TRITON SM OrLater Requires SM + config patch fx_graph_cache False config patch fx_graph_remote_cache False config patch bundled_autotune_remote_cache False config patch max_autotune True config patch compile_threads Worker processes do register PatchCaches properly parametrize remote_cache True False test_modified_autotune_cache remote_cache If developer changes way autotune cache handled there s chance ll break cache This happened This test ensures torch code changes then old cache entries will invalidated mock_torch_key value str - bytes value encode utf- get_autotune_stats remote_cache global_stats autotune_remote global_stats autotune_local fn x y x + y relu x = torch randn GPU_TYPE y = torch randn GPU_TYPE config patch autotune_local_cache remote_cache autotune_remote_cache remote_cache PatchCaches mock patch torch _inductor codecache torch_key functools partial mock_torch_key torchkey f_compiled = torch compile fn fullgraph=True res = f_compiled x y assertEqual get_autotune_stats Stats torch _dynamo reset PyCodeCache cache_clear mock patch torch _inductor codecache torch_key functools partial mock_torch_key torchkey f_compiled = torch compile fn fullgraph=True res = f_compiled x y assertEqual get_autotune_stats Stats assertEqual res res TestRemoteAOTAutogradCache TestCase requires_gpu unittest skipIf HAS_XPU_AND_TRITON SM OrLater Requires SM + config patch fx_graph_cache False config patch fx_graph_remote_cache True torch _functorch config patch enable_autograd_cache False torch _functorch config patch enable_remote_autograd_cache True test_autograd_remote_cache f b + b f_compiled = torch compile f = torch randn device=GPU_TYPE requires_grad=False b = torch randn device=GPU_TYPE requires_grad=False PatchCaches f_compiled b assertEqual global_stats aot_autograd Stats assertEqual global_stats fx_graph Stats torch _dynamo reset f_compiled b assertEqual global_stats aot_autograd Stats assertEqual global_stats fx_graph Stats torch _dynamo reset torch compiler config patch cache_key_tag test f_compiled b assertEqual global_stats aot_autograd Stats assertEqual global_stats fx_graph Stats Check cache entries seem reasonable k global_stats aot_autograd cache keys assertRegex k r pt autograd-experimental - a-z c - + k global_stats fx_graph cache keys assertRegex k r pt fx-graph-v - a-z c - + requires_gpu_and_triton unittest skipIf HAS_XPU_AND_TRITON SM OrLater Requires SM + config patch fx_graph_cache False config patch fx_graph_remote_cache True torch _functorch config patch enable_autograd_cache False torch _functorch config patch enable_remote_autograd_cache True test_autograd_remote_lazy_backward Lazily compile backward lazily save cache fn b cos + b PatchCaches = torch randn requires_grad=True b = torch randn requires_grad=True = detach clone requires_grad_ True b = b detach clone requires_grad_ True compiled_fn = torch compile fn backend= inductor assertEqual fn b compiled_fn b assertEqual global_stats aot_autograd Stats Clear dynamo run again Should cache miss still because backward hasn t run torch _dynamo reset assertEqual fn b compiled_fn b assertEqual global_stats aot_autograd Stats Now let s run backward fn b sum backward compiled_fn b sum backward assertEqual grad grad assertEqual b grad b grad assertEqual global_stats aot_autograd Stats Clear dynamo rerun everything now there should cache hit torch _dynamo reset = torch randn requires_grad=True b = torch randn requires_grad=True = detach clone requires_grad_ True b = b detach clone requires_grad_ True assertEqual fn b compiled_fn b assertEqual global_stats aot_autograd Stats fn b sum backward compiled_fn b sum backward assertEqual grad grad assertEqual b grad b grad TestUtils TestCase config patch fx_graph_remote_cache False test_fresh_cache fn x y x + y = torch rand b = torch rand fresh_cache assertEqual len PyCodeCache modules res = torch compile fn b cache_dir = cache_dir torch _dynamo reset fresh_cache assertEqual len PyCodeCache modules res = torch compile fn b cache_dir = cache_dir assertEqual res res assertNotEqual cache_dir cache_dir This combination settings exposed bug where we cleared PyCodeCache disk artifacts while they still needed requires_gpu_and_triton config patch coordinate_descent_tuning True force_disable_caches True test_force_disable_coordinate_descent fn inp = torch randn device=GPU_TYPE weight = torch randn device=GPU_TYPE layer = torch nn LayerNorm device=GPU_TYPE layer inp weight torch compile fn __name__ == __main__ run_tests