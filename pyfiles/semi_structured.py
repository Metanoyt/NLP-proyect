mypy allow-untyped-defs warnings collections namedtuple collections abc Callable typing Any Optional torch torch sparse _semi_structured_conversions sparse_semi_structured_from_dense_cutlass sparse_semi_structured_to_dense_cutlass torch sparse _semi_structured_ops fallback_dispatcher semi_sparse_addmm semi_sparse_detach semi_sparse_indices semi_sparse_linear semi_sparse_mm semi_sparse_scaled_mm semi_sparse_t semi_sparse_values semi_sparse_view __all__ = SparseSemiStructuredTensor SparseSemiStructuredTensorCUTLASS SparseSemiStructuredTensorCUSPARSELT to_sparse_semi_structured _SEMI_STRUCTURED_SPARSE_CONFIG = namedtuple _SEMI_STRUCTURED_SPARSE_CONFIG sparse_min_rows sparse_min_cols dense_min_rows dense_min_cols SparseSemiStructuredTensor torch Tensor This implements semi-structured sparsity Tensor subclass Semi-structured sparsity describes sparsity pattern where n every n elements sparse depending datatype It also referred sparsity fine-grained structured sparsity There two backends available semi_structred sparsity either cuSPARSELt CUTLASS This meant serve base both implementations SparseSemiStructuredCUTLASS SparseSemiStructuredCUSPARSELT both inherit define three backend-specific items Note such cannot instantiated directly - ` _DTYPE_SHAPE_CONSTRAINTS ` - A dictionary holding backend specific dense sparse min shape constraints - ` from_dense ` - backend specific compression routines - ` _mm ` - backend specific mm op either torch _cslt_sparse_mm torch _sparse_semi_structured_ mm &#124; addmm _DEFAULT_ALG_ID int = _DTYPE_SHAPE_CONSTRAINTS dict torch dtype _SEMI_STRUCTURED_SPARSE_CONFIG _FORCE_CUTLASS bool = False _FUSE_TRANSPOSE bool = False _PROTOTYPE_WARNING_SHOWN bool = False BACKEND str SPARSE_DISPATCH dict Callable Callable packed Optional torch Tensor meta Optional torch Tensor packed_t Optional torch Tensor meta_t Optional torch Tensor compressed_swizzled_bitmask Optional torch Tensor fuse_transpose_cusparselt bool alg_id_cusparselt int __slots__ = packed meta packed_t meta_t compressed_swizzled_bitmask staticmethod __new__ noqa PYI cls shape torch Size packed Optional torch Tensor meta Optional torch Tensor packed_t Optional torch Tensor meta_t Optional torch Tensor compressed_swizzled_bitmask Optional torch Tensor fuse_transpose_cusparselt bool = False alg_id_cusparselt int = requires_grad bool = False Create new instance tensor subclass compressed sparse representation We have option create subclass compressed representations both X X training For inference we only need single representation either X X while corresponding other set will None Depending backend selected certain fields will set None CUSPARSELT vs CUTLASS Args shape The shape original dense tensor packed The compressed representation original dense tensor meta The metadata original dense tensor stored separately packed_t The compressed representation transposed original dense tensor meta_t The metadata transposed original dense tensor stored separately compressed_swizzled_bitmask The masks used CUTLASS backend determine which threads should participate computation Used pointwise ops fuse_transpose_cusparselt When running cuSPARSELt we have option fuse transposition matmul which useful case sparse training alg_id_cusparselt The algorithm id use when using cuSPARSELT will have effect performance Returns torch Tensor A torch Tensor wrapper subclass Raises ValueError If all tensor arguments None cls _PROTOTYPE_WARNING_SHOWN warnings warn The PyTorch API SparseSemiStructuredTensor prototype stage will change near future Please open Github issue features requests see our documentation torch sparse module further information about project UserWarning stacklevel= cls _PROTOTYPE_WARNING_SHOWN = True Because only runs once we also load dispatch table here well We can t define dispatch table explicitly because torch ops errors so we do instead But useful since allows users overload dispatch table debugging testing cls _load_dispatch_table we can also register classes dynamo when warning shown torch _dynamo allow_in_graph cls packed None previous_tensor = packed packed_t None previous_tensor = packed_t raise ValueError At least one packed packed_t must provided tensor = torch Tensor _make_wrapper_subclass cls shape device=previous_tensor device dtype=previous_tensor dtype layout=previous_tensor layout requires_grad=requires_grad tensor packed = packed tensor meta = meta tensor packed_t = packed_t tensor meta_t = meta_t tensor compressed_swizzled_bitmask = compressed_swizzled_bitmask tensor fuse_transpose_cusparselt = fuse_transpose_cusparselt tensor alg_id_cusparselt = alg_id_cusparselt tensor __repr__ - str type ignore override assert hasattr shape f __class__ __name__ shape= shape __tensor_flatten__ - tuple list str tuple torch Size bool int bool inner_tensors = list filter lambda x getattr x None __slots__ tensor_meta = shape fuse_transpose_cusparselt alg_id_cusparselt requires_grad inner_tensors tensor_meta classmethod __tensor_unflatten__ cls inner_tensors tensor_meta tuple torch Size bool int bool outer_size outer_stride - torch Tensor shape fuse_transpose_cusparselt alg_id_cusparselt requires_grad = tensor_meta pyrefly ignore no-matching-overload cls shape=shape packed=inner_tensors get packed None meta=inner_tensors get meta None packed_t=inner_tensors get packed_t None meta_t=inner_tensors get meta_t None compressed_swizzled_bitmask=inner_tensors get compressed_swizzled_bitmask None fuse_transpose_cusparselt=fuse_transpose_cusparselt alg_id_cusparselt=alg_id_cusparselt requires_grad=requires_grad __torch_function__ = torch _C _disabled_torch_function_impl type ignore assignment classmethod __torch_dispatch__ cls func types args kwargs - Any type ignore override func _overloadpacket cls SPARSE_DISPATCH raise NotImplementedError f cls __name__ only supports specific set operations f can t perform requested op func __name__ cls SPARSE_DISPATCH func _overloadpacket func types args kwargs classmethod _load_dispatch_table cls custom_dispatch_table=None - None Loads op overload sparse dispatch table current getattr cls SPARSE_DISPATCH None None cls SPARSE_DISPATCH = torch ops aten values semi_sparse_values torch ops aten indices semi_sparse_indices torch ops aten is_same_size fallback_dispatcher torch ops aten detach_ fallback_dispatcher torch ops aten detach semi_sparse_detach torch ops aten t semi_sparse_t torch ops aten view semi_sparse_view torch ops aten mm semi_sparse_mm torch ops aten matmul semi_sparse_mm torch ops aten addmm semi_sparse_addmm torch ops aten linear semi_sparse_linear torch ops aten _to_copy fallback_dispatcher torch ops aten _scaled_mm semi_sparse_scaled_mm custom_dispatch_table None cls SPARSE_DISPATCH update custom_dispatch_table classmethod _validate_device_dim_dtype_shape cls original_tensor torch Tensor - None Assert given tensor valid semi-structured sparse compression check device original_tensor is_cuda raise RuntimeError f Error original_tensor device= original_tensor device supported Only CUDA tensors currently supported check dim original_tensor dim = raise RuntimeError f Error original_tensor dim = original_tensor dim supported Only d tensors currently supported check contiguous original_tensor is_contiguous raise RuntimeError Error original_tensor contiguous Only contiguous tensors currently supported check dtype original_tensor dtype cls _DTYPE_SHAPE_CONSTRAINTS raise RuntimeError f Error original_tensor dtype original_tensor dtype supported dtype cls check shape m n = original_tensor shape min_rows = cls _DTYPE_SHAPE_CONSTRAINTS original_tensor dtype sparse_min_rows min_cols = cls _DTYPE_SHAPE_CONSTRAINTS original_tensor dtype sparse_min_cols m min_rows m min_rows n min_cols n min_cols TODO future we can add padding support sparse dimensions aren t perfect multiples raise RuntimeError f Error original_tensor shape original_tensor shape supported f Both dimensions must larger equal than multiple min_rows min_cols classmethod _pad_dense_input cls dense_input torch Tensor - torch Tensor Calculates padding dense tensor pads tensor necessary If padding required function returns original tensor only d matmul assert dense_input dim == check shape m n = dense_input shape min_rows = cls _DTYPE_SHAPE_CONSTRAINTS dense_input dtype dense_min_rows min_cols = cls _DTYPE_SHAPE_CONSTRAINTS dense_input dtype dense_min_cols calculate padding to_pad_m = -m min_rows m min_rows m min_rows to_pad_n = -n min_cols n min_cols n min_rows to_pad_m to_pad_n torch nn functional pad dense_input to_pad_n to_pad_m dense_input to_dense type ignore override col = shape - torch mm torch eye col dtype=self dtype device=self device classmethod from_dense cls original_tensor torch Tensor - SparseSemiStructuredTensor raise NotImplementedError _mm B torch Tensor bias Optional torch Tensor = None kwargs - torch Tensor raise NotImplementedError to_sparse_semi_structured original_tensor torch Tensor transposed bool = False - SparseSemiStructuredTensor This function converts dense tensor into sparse semi-structured tensor It will SparseSemiStructuredTensor subclass torch Tensor This function will check ensure dense tensor has right dtype size dims device We currently only support semi-structured sparse tensors d CUDA tensors Additionally your tensor must positive multiple minimum sparse block size given ` _DTYPE_TO_SHAPE_CONSTRAINTS ` each dtype float float bfloat int Args original_tensor Tensor dense tensor convert transposed bool optional deprecated arg removed another release Do use Returns SparseSemiStructuredTensor A sparse semi-structured tensor created given original_tensor Raises None Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA A = torch Tensor tile half cuda tensor device= cuda dtype=torch float A_sparse = to_sparse_semi_structured A SparseSemiStructuredTensor shape=torch Size A_sparse values tensor device= cuda dtype=torch float A_sparse indices tensor - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - device= cuda dtype=torch int transposed warnings warn Setting transpose ` to_sparse_semi_structured ` deprecated will removed future release ` SparseSemiStructuredTensor ` only support contiguous input tensors FutureWarning stacklevel= set _FORCE_CUTLASS flag SPARSE_SUBCLASS = torch sparse SparseSemiStructuredTensorCUTLASS SparseSemiStructuredTensor _FORCE_CUTLASS torch sparse SparseSemiStructuredTensorCUSPARSELT SPARSE_SUBCLASS from_dense original_tensor SparseSemiStructuredTensorCUTLASS SparseSemiStructuredTensor This implements semi-structured sparsity CUTLASS backend In implementation specified elements metadata stored separately packed meta respectively When _FORCE_CUTLASS set when cuSPARSELt available subclass calls into _sparse_semi_structured_ mm &#124; addmm sparse_semi_structured_from_dense conversion compressed format BACKEND = cutlass _DTYPE_SHAPE_CONSTRAINTS = torch int _SEMI_STRUCTURED_SPARSE_CONFIG torch float _SEMI_STRUCTURED_SPARSE_CONFIG torch bfloat _SEMI_STRUCTURED_SPARSE_CONFIG torch float _SEMI_STRUCTURED_SPARSE_CONFIG classmethod from_dense cls original_tensor torch Tensor - SparseSemiStructuredTensorCUTLASS cls _validate_device_dim_dtype_shape original_tensor sparse_tensor_cutlass meta_tensor_cutlass = sparse_semi_structured_from_dense_cutlass original_tensor pyrefly ignore no-matching-overload cls original_tensor shape packed=sparse_tensor_cutlass meta=meta_tensor_cutlass packed_t=None meta_t=None compressed_swizzled_bitmask=None requires_grad=original_tensor requires_grad to_dense type ignore override assert meta None packed None sparse_semi_structured_to_dense_cutlass packed meta meta ndim == super to_dense classmethod prune_dense_static_sort cls original_tensor torch Tensor algorithm= - SparseSemiStructuredTensor This function takes unpruned dense tensor runs branchless static sort across x tile It greedily picks largest values tile upholding sparsity constraint across both rows columns The algorithm used prune matrix implemented ` _sparse_semi_structured_tile ` Then creates packed meta tensors compressed sparse representation pruned dense tensor It also calculates packed_t meta_t tensors compressed sparse representation transposed pruned dense tensor Since we cannot transpose compressed representations we store both fw bw pass respectively Finally function also computes compressed swizzled bitmask encodes sparsity pattern This can used backward pass mask gradients - prune x tile - - pack CUTLASS semi-structured - packed - metadata - pack transposed CUTLASS - packed_t semi-structured representation - metadata_t - compute swizzled bitmask - compressed_swizzled_bitmask The equivalent PyTorch code create same five outputs dense tensor can found below ` ` ` torch sparse SparseSemiStructuredTensorCUTLASS torch sparse _semi_structured_conversions _sparse_semi_structured_tile _compute_compressed_swizzled_bitmask pruned = _sparse_semi_structured_tile dense packed_cutlass meta_cutlass = sparse_semi_structured_from_dense_cutlass pruned packed_t_cutlass meta_t_cutlass = sparse_semi_structured_from_dense_cutlass pruned t contiguous bitmask = _compute_compressed_swizzled_bitmask pruned SparseSemiStructuredTensorCUTLASS dense shape packed_cutlass meta_cutlass packed_t_cutlass meta_t_cutlass bitmask ` ` ` We can either pack CUTLASS cuSPARSELt representation depending use_cutlass flag packed meta packed_t meta_t compressed_swizzled_bitmask = torch _sparse_semi_structured_tile original_tensor algorithm=algorithm use_cutlass=True pyrefly ignore no-matching-overload cls original_tensor shape packed=packed meta=meta packed_t=packed_t meta_t=meta_t compressed_swizzled_bitmask=compressed_swizzled_bitmask requires_grad=False _mm B torch Tensor bias Optional torch Tensor = None kwargs - torch Tensor isinstance B SparseSemiStructuredTensor raise ValueError ` SparseSemiStructuredTensor SparseSemiStructuredTensor ` supported hardware cls_name = __class__ __name__ ndim = B ndim = raise NotImplementedError f ` cls_name ` matmul Broadcasting implemented packed None meta None raise NotImplementedError f ` cls_name ` matmul operation supported bias None res = torch _sparse_semi_structured_mm packed meta B res = torch _sparse_semi_structured_addmm bias packed meta B res shape SparseSemiStructuredTensorCUSPARSELT SparseSemiStructuredTensor The cuSPARSELt backend expects specified elements metadata stored single tensor packed = specified elements original tensor &#124; metadata For original tensor size m k we expect first m k elements kept elements The rest tensor metadata Since there only one tensor we only use packed packed_t attributes respectively cuSPARSELt also supports transposition fusion which necessary performant sparse training well specifying alg_id config affects performance matmul depending matmul sizes BACKEND = cusparselt _DTYPE_SHAPE_CONSTRAINTS = torch float _e m fn _SEMI_STRUCTURED_SPARSE_CONFIG torch int _SEMI_STRUCTURED_SPARSE_CONFIG torch float _SEMI_STRUCTURED_SPARSE_CONFIG torch bfloat _SEMI_STRUCTURED_SPARSE_CONFIG classmethod from_dense cls original_tensor torch Tensor - SparseSemiStructuredTensorCUSPARSELT cls _validate_device_dim_dtype_shape original_tensor pyrefly ignore no-matching-overload cls shape=original_tensor shape packed=torch _cslt_compress original_tensor meta=None packed_t=None meta_t=None compressed_swizzled_bitmask=None fuse_transpose_cusparselt=SparseSemiStructuredTensor _FUSE_TRANSPOSE alg_id_cusparselt=SparseSemiStructuredTensor _DEFAULT_ALG_ID requires_grad=original_tensor requires_grad classmethod prune_dense_static_sort cls original_tensor torch Tensor algorithm= - SparseSemiStructuredTensor This function does same thing described SparseSemiStructuredCUTLASS uses cuSPARSELt metadata layout sparse matmul The only functional difference cuSPARSELt stores ` metadata ` ` packed ` together into single tensor - prune x tile - - pack cuSPARSELT semi-structured - packed - pack transposed cuSPARSELt - packed_t semi-structured representation - compute swizzled bitmask - compressed_swizzled_bitmask The equivalent PyTorch code create same three outputs dense tensor can found below ` ` ` torch sparse SparseSemiStructuredTensorCUSPARSELT torch sparse _semi_structured_conversions _sparse_semi_structured_tile _compute_compressed_swizzled_bitmask pruned = _sparse_semi_structured_tile dense packed_cusparselt = torch _cslt_compress pruned packed_t_cusparselt = torch _cslt_compress pruned t contiguous bitmask = _compute_compressed_swizzled_bitmask pruned SparseSemiStructuredTensorCUSPARSELT dense shape packed_cutlass None packed_t_cutlass None bitmask ` ` ` packed meta packed_t meta_t compressed_swizzled_bitmask = torch _sparse_semi_structured_tile original_tensor algorithm=algorithm use_cutlass=False Map two -dim view packed data TODO proper cuSPARSELt metadata packed = packed view original_tensor shape - packed_t = packed_t view original_tensor shape - pyrefly ignore no-matching-overload cls original_tensor shape packed=packed meta=meta packed_t=packed_t meta_t=meta_t compressed_swizzled_bitmask=compressed_swizzled_bitmask requires_grad=False _mm B torch Tensor bias Optional torch Tensor = None kwargs - torch Tensor isinstance B SparseSemiStructuredTensor raise ValueError ` SparseSemiStructuredTensor SparseSemiStructuredTensor ` supported hardware ndim = B ndim = raise NotImplementedError f ` __class__ __name__ ` matmul Broadcasting implemented B dtype = dtype raise NotImplementedError f ` __class__ __name__ ` matmul trying do ` A= tuple shape B= tuple B shape ` f A dtype= dtype B dtype= B dtype This operation only supported when A B have same data type bias None bias dtype = dtype raise NotImplementedError f ` __class__ __name__ ` matmul trying do ` A= tuple shape B= tuple B shape + C ` f A dtype=B dtype= dtype C dtype= B dtype This operation only supported when A B C have same data type Force fp mm error consistent torch dtype == torch float _e m fn raise NotImplementedError f ` __class__ __name__ ` matmul trying do ` A= tuple shape B= tuple B shape ` f A dtype=B dtype= dtype mm supported float _e m fn please use ` torch _scaled_mm ` instead packed None raise NotImplementedError f ` __class__ __name__ ` matmul operation supported res = torch _cslt_sparse_mm packed B bias=bias transpose_result=self fuse_transpose_cusparselt alg_id=self alg_id_cusparselt res t fuse_transpose_cusparselt res