mypy allow-untyped-defs __future__ annotations functools logging operator typing Optional TYPE_CHECKING torch torch _prims Tensor TYPE_CHECKING collections abc Sequence log = logging getLogger __name__ make_prim schema str impl_aten return_type=_prims RETURN_TYPE NEW doc str = tags Optional Sequence torch Tag = None isinstance return_type tuple meta args kwargs tuple _prims TensorMeta o o impl_aten args kwargs meta args kwargs _prims TensorMeta impl_aten args kwargs _prims _make_prim schema=schema return_type=return_type meta=meta impl_aten=impl_aten doc=doc tags=tags eager_force_stride input_tensor Tensor stride - Tensor input_tensor stride == stride input_tensor new_tensor = input_tensor clone as_strided input_tensor shape stride new_tensor copy_ input_tensor new_tensor eager_prepare_softmax x Tensor dim int - tuple Tensor Tensor amax = torch amax x dim keepdim=True amax torch sum torch exp x - amax dim keepdim=True Custom prims used handling randomness seed = make_prim inductor_seed Device device - Tensor lambda device torch randint - device=device doc= create fresh seed one per call use inductor_rand tags= torch Tag nondeterministic_seeded seeds = make_prim inductor_seeds int count Device device - Tensor lambda count device torch randint - count device=device doc= Horizontal fusion many inductor_seed calls tags= torch Tag nondeterministic_seeded lookup_seed = make_prim inductor_lookup_seed changes update partitioners py inductor_lookup_seed Tensor seeds int index - Tensor lambda seeds index seeds index clone doc= Extract single seed result inductor_seeds inductor_random doesn t accept dtype instead its lowering always burns float conversions different type explicit graph We therefore need impl used during tracing hardcoded dtype so always faithfully produces float tensor during tracing even default dtype set something random = make_prim inductor_random SymInt size Tensor seed str mode - Tensor lambda size seed mode getattr torch mode size device=seed device dtype=torch float doc= torch rand torch randn using backend-specific RNG can fused randint = make_prim inductor_randint SymInt low SymInt high SymInt size Tensor seed - Tensor lambda low high size seed torch randint low high size device=seed device doc= torch randint using backend-specific RNG can fused force_stride_order = make_prim inductor_force_stride_order Tensor input SymInt stride - Tensor eager_force_stride doc= Force stride order input tensor No-op input tensor already has stride Do copy otherwise _unsafe_index_put_ = make_prim _unsafe_index_put_ Tensor Tensor indices Tensor values bool accumulate=False - Tensor lambda indices values accumulate=False torch ops aten index_put_ indices values accumulate doc= Unsafe index_put_ doesn t issue device asserts fma = make_prim fma Tensor Tensor b Tensor c - Tensor lambda b c b + c doc= Fused multiply add fma b c - b + c without rounding after multiplication tags= torch Tag pointwise prepare_softmax_online = make_prim prepare_softmax_online Tensor int dim - Tensor Tensor eager_prepare_softmax return_type= _prims RETURN_TYPE NEW _prims RETURN_TYPE NEW doc= Prepare softmax computing max sum _flattened_index_to_nd indices width sympy torch utils _sympy functions FloorDiv dim = len width dim == indices dim = m = functools reduce operator mul width isinstance indices sympy Expr isinstance m sympy Expr ih = FloorDiv indices m ih = indices m indices_new = indices - ih m ih _flattened_index_to_nd indices_new width raise ValueError f Unknown dim dim _flatten_index indices width result = indices d range len indices result = width d result + indices d result _low_memory_max_pool_with_offsets_aten kernel_size stride padding dilation ceil_mode dim = len kernel_size dim == vals indices = torch ops aten max_pool d_with_indices kernel_size stride padding dilation ceil_mode vals indices = torch ops aten max_pool d_with_indices kernel_size stride padding dilation ceil_mode idhw = _flattened_index_to_nd indices shape -dim dhw_inc = d range dim bh_shape = ndim bh_shape -dim + d = - bh = torch arange indices shape -dim + d dtype=torch int device=self device view bh_shape hbase = bh stride d - padding d h_inc = idhw d - hbase dilation d dhw_inc append h_inc offsets = _flatten_index dhw_inc kernel_size vals offsets torch int _low_memory_max_pool_offsets_to_indices_aten offsets kernel_size input_size stride padding dilation dim = len kernel_size offsets = offsets torch int dhw_inc = _flattened_index_to_nd offsets kernel_size idhw = d range dim bh_shape = offsets ndim bh_shape -dim + d = - bh = torch arange offsets shape -dim + d dtype=torch int device=offsets device view bh_shape hbase = bh stride d - padding d idhw append hbase + dhw_inc d dilation d _flatten_index idhw input_size _low_memory_max_pool_with_offsets = make_prim _low_memory_max_pool_with_offsets Tensor SymInt kernel_size SymInt stride SymInt padding SymInt dilation bool ceil_mode - Tensor Tensor noqa B _low_memory_max_pool_with_offsets_aten return_type= _prims RETURN_TYPE NEW _prims RETURN_TYPE NEW doc= Instead returning indices returns indices offsets _low_memory_max_pool_offsets_to_indices = make_prim _low_memory_max_pool_offsets_to_indices Tensor SymInt kernel_size SymInt input_size SymInt stride SymInt padding SymInt dilation - Tensor noqa B _low_memory_max_pool_offsets_to_indices_aten doc= Convert small int offsets regular indices