Owner s module fsdp functools gc typing Union torch torch nn nn torch distributed _composable checkpoint torch distributed _tools fsdp _mem_tracker FSDPMemTracker torch distributed algorithms _checkpoint checkpoint_wrapper apply_activation_checkpointing CheckpointWrapper torch distributed device_mesh init_device_mesh torch distributed fsdp CPUOffloadPolicy fully_shard MixedPrecisionPolicy OffloadPolicy torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest MLP torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor ModelArgs Transformer TransformerBlock _init_cublas_workspace dev torch device lin = torch nn Linear device=dev inp = torch randn device=dev lin inp sum backward del lin del inp _reset_mem_stats dev torch device mod = torch get_device_module dev mod empty_cache mod reset_accumulated_memory_stats dev mod reset_peak_memory_stats dev TestTrackerFullyShard DTrainingCore FSDPTest property world_size - int min torch accelerator device_count skip_if_lt_x_gpu test_tracker_multi_group_eager Tests tracker accuracy when using multiple parameter groups communication communication computation overlap plus memory reduction different mixed precision policies run_subtests reshard_after_forward True False offload_policy CPUOffloadPolicy pin_memory=False OffloadPolicy mp_policy MixedPrecisionPolicy param_dtype=torch float reduce_dtype=torch float _test_tracker_multi_group _test_tracker_multi_group reshard_after_forward Union bool int offload_policy OffloadPolicy mp_policy MixedPrecisionPolicy debug = False dev = torch device torch accelerator current_device_index _init_cublas_workspace dev gc collect _reset_mem_stats dev mod = torch get_device_module dev mem_stats = mod memory_stats dev pre_acc_active = mem_stats active_bytes all current torch manual_seed lin_dim bsz = torch device dev model = nn Sequential MLP dim=lin_dim device=dev _ range mesh = init_device_mesh dev type world_size fully_shard_fn = functools partial fully_shard mesh=mesh reshard_after_forward=reshard_after_forward offload_policy=offload_policy mp_policy=mp_policy mlp model fully_shard_fn mlp fully_shard_fn model optim = torch optim Adam model parameters lr= e- inp = torch randn bsz lin_dim device=dev fmt = FSDPMemTracker model optim fmt track_inputs inp fmt iter_idx range loss = model inp sum loss backward optim step optim zero_grad iter_idx == fmt reset_mod_stats mem_stats = mod memory_stats tracker_max = fmt get_tracker_snapshot peak dev Total acc_max = mem_stats active_bytes all peak - pre_acc_active accuracy = tracker_max acc_max rank == debug print f Accuracy accuracy Tracker Max tracker_max Accelerator Max acc_max assertAlmostEqual accuracy delta= msg=f Tracker Max tracker_max Accelerator Max acc_max del model del inp del optim skip_if_lt_x_gpu test_tracker_non_root_forward_backward Tests tracker accuracy when running forward backward through non-root debug = False dev = torch device torch accelerator current_device_index _init_cublas_workspace dev gc collect _reset_mem_stats dev mod = torch get_device_module dev mem_stats = mod memory_stats dev pre_acc_active = mem_stats active_bytes all current torch manual_seed lin_dim bsz = model = nn Sequential MLP lin_dim dev _ range mlp model fully_shard mlp fully_shard model optim = torch optim Adam model parameters lr= e- foreach=True torch manual_seed + rank inp = torch randn bsz lin_dim device=dev fmt = FSDPMemTracker model optim fmt track_inputs inp fmt iter_idx range nonroot_loss = model inp sum nonroot_loss backward optim step optim zero_grad iter_idx == fmt reset_mod_stats mem_stats = mod memory_stats tracker_max = fmt get_tracker_snapshot peak dev Total acc_max = mem_stats active_bytes all peak - pre_acc_active accuracy = tracker_max acc_max rank == debug print f Accuracy accuracy Tracker Max tracker_max Accelerator Max acc_max assertAlmostEqual accuracy delta= msg=f Tracker Max tracker_max Accelerator Max acc_max del inp del model del optim TestTrackerFullyShard DTrainingCompose FSDPTest property world_size - int min torch accelerator device_count skip_if_lt_x_gpu test_tracker_with_activation_checkpointing Tests tracker accuracy when composing activation checkpointing run_subtests reshard_after_forward True False checkpoint_impl composable wrapper _test_tracker_with_activation_checkpointing _test_tracker_with_activation_checkpointing reshard_after_forward Union bool int checkpoint_impl str assert checkpoint_impl composable wrapper debug = False dev = torch device torch accelerator current_device_index _init_cublas_workspace dev gc collect _reset_mem_stats dev mod = torch get_device_module dev mem_stats = mod memory_stats dev pre_acc_active = mem_stats active_bytes all current torch manual_seed vocab_size = bsz seq_len = torch device dev model_args = ModelArgs n_layers= n_heads= vocab_size=vocab_size max_seq_len=seq_len dropout_p= model = Transformer model_args foreach = False fully_shard_fn = functools partial fully_shard reshard_after_forward=reshard_after_forward checkpoint_impl == wrapper apply_activation_checkpointing model check_fn=lambda m isinstance m TransformerBlock module model modules Apply ` CheckpointWrapper ` which wraps ` TransformerBlock ` isinstance module CheckpointWrapper fully_shard_fn module module model modules isinstance module TransformerBlock checkpoint_impl == composable checkpoint module fully_shard_fn module fully_shard_fn model optim = torch optim Adam model parameters lr= e- foreach=foreach torch manual_seed + rank inp = torch randint vocab_size bsz seq_len device=dev fmt = FSDPMemTracker model optim fmt track_inputs inp fmt iter_idx range loss = model inp sum loss backward optim step optim zero_grad iter_idx == fmt reset_mod_stats mem_stats = mod memory_stats tracker_max = fmt get_tracker_snapshot peak dev Total acc_max = mem_stats active_bytes all peak - pre_acc_active accuracy = tracker_max acc_max rank == debug print f Accuracy accuracy Tracker Max tracker_max Accelerator Max acc_max assertAlmostEqual accuracy delta= msg=f Tracker Max tracker_max Accelerator Max acc_max del inp del model del optim __name__ == __main__ run_tests