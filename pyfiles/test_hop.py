Owner s oncall export flake noqa copy io unittest torch torch _dynamo torchdynamo torch utils _pytree pytree torch _dynamo test_case TestCase torch export export load save torch export _trace _export torch testing _internal common_device_type instantiate_device_type_tests ops torch testing _internal common_utils IS_WINDOWS run_tests torch testing _internal hop_db FIXME_hop_that_doesnt_have_opinfo_test_allowlist hop_db hop_tests = op_info hop_db op_info_hop_name = op_info name op_info_hop_name FIXME_hop_that_doesnt_have_opinfo_test_allowlist continue hop_tests append op_info unittest skipIf IS_WINDOWS Windows isn t supported case unittest skipIf torchdynamo is_dynamo_supported dynamo isn t support TestHOP TestCase _compare eager_model export args kwargs eager_args = copy deepcopy args eager_kwargs = copy deepcopy kwargs export_args = copy deepcopy args export_kwargs = copy deepcopy kwargs flat_orig_outputs = pytree tree_leaves eager_model eager_args eager_kwargs flat_loaded_outputs = pytree tree_leaves export module export_args export_kwargs orig loaded zip flat_orig_outputs flat_loaded_outputs assertEqual type orig type loaded assertEqual orig loaded ops hop_tests allowed_dtypes= torch float test_aot_export device dtype op Foo torch nn Module forward args op op args sample_inputs_itr = op sample_inputs device dtype requires_grad=True inp sample_inputs_itr model = Foo input = inp input isinstance inp input tuple inp input args = input inp args kwargs = inp kwargs ep = export model args kwargs strict=True _compare model ep args kwargs With PYTORCH_TEST_CUDA_MEM_LEAK_CHECK= memory leak occurs during strict-mode export We need manually reset cache backends Specifically ` cached_backends clear ` required Upon examining items ` cached_backends ` we notice under strict-mode export there exists ` dynamo_normalization_capturing_compiler ` which must cleared avoid memory leaks An educated guess ` dynamo_normalization_capturing_compiler ` references input tensors CUDA devices fails free them torchdynamo _reset_guarded_backend_cache ops hop_tests allowed_dtypes= torch float test_pre_dispatch_export device dtype op Foo torch nn Module forward args op op args sample_inputs_itr = op sample_inputs device dtype requires_grad=True inp sample_inputs_itr model = Foo input = inp input isinstance inp input tuple inp input args = input inp args kwargs = inp kwargs ep = _export model args kwargs pre_dispatch=True _compare model ep args kwargs torchdynamo _reset_guarded_backend_cache ops hop_tests allowed_dtypes= torch float test_retrace_export device dtype op Foo torch nn Module forward args op op args sample_inputs_itr = op sample_inputs device dtype requires_grad=True inp sample_inputs_itr model = Foo input = inp input isinstance inp input tuple inp input args = input inp args kwargs = inp kwargs ep = _export model args kwargs pre_dispatch=True ep = ep run_decompositions _compare model ep args kwargs torchdynamo _reset_guarded_backend_cache ops hop_tests allowed_dtypes= torch float test_serialize_export device dtype op Foo torch nn Module forward args op op args sample_inputs_itr = op sample_inputs device dtype requires_grad=True inp sample_inputs_itr model = Foo input = inp input isinstance inp input tuple inp input args = input inp args kwargs = inp kwargs ep = _export model args kwargs pre_dispatch=True ep = ep run_decompositions buffer = io BytesIO save ep buffer buffer seek ep = load buffer _compare model ep args kwargs torchdynamo _reset_guarded_backend_cache instantiate_device_type_tests TestHOP globals __name__ == __main__ run_tests