Owner s oncall distributed os sys datetime timedelta unittest mock patch torch torch distributed c d torch _C _distributed_c d _ProcessGroupWrapper c d is_available print c d available skipping tests file=sys stderr sys exit test_c d_common LOOPBACK torch testing _internal common_distributed create_device MultiProcessTestCase requires_gloo requires_nccl skip_if_lt_x_gpu with_dist_debug_levels torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN AbstractProcessGroupWrapperTest MultiProcessTestCase setUp super setUp _spawn_processes _validate_error exception op_type rank tensor verify_diff=True err = str exception assertTrue op_type err f Got err expected op_type error User doesn t call barrier tensor op_type = BARRIER assertTrue f list tensor shape err f Did find shapes list tensor shape error err For CUDA only assert device type index cuda str tensor device assertTrue cuda err f Did find cuda device error err assertTrue str tensor device err f Did find tensor device str tensor device error err C++ python type strings exactly same float str tensor dtype assertTrue Float err Expected Float type int str tensor dtype assertTrue Long err Expected Long type fail f Unexpected dtype str tensor dtype error err Ensure sequence number logged error assertTrue SequenceNumber err Ensure info about how collectives diff error verify_diff assertTrue Collectives differ following err f Got error err _test_collective_hang wrapper_pg use_cuda=False All ranks besides call allreduce wrapper_pg should detect hang report issue rank faulty_rank = rank = faulty_rank tensor = torch randn use_cuda tensor = tensor rank rank == Rank reports faulty ranks err = f Ranks faulty_rank failed pass monitoredBarrier err = Please check rank logs faulty rank Gloo can sometimes throw following error rank exits early before rank calls into allreduce err += &#124; Connection closed peer &#124; Connection reset peer assertRaisesRegex RuntimeError err wrapper_pg allreduce tensor _test_collectives_op_mismatch wrapper_pg use_cuda=False tensor = torch randn use_cuda tensor = tensor rank works = Run few successful collectives _ range work = wrapper_pg allreduce tensor works append work w works w wait Simulate mismatch allreduce vs reduce Error including info about inconsistent collective rank tensor shape device dtype should raised assertRaisesRegex RuntimeError cm rank == wrapper_pg allreduce tensor wrapper_pg reduce tensor _validate_error exception=cm exception op_type= ALLREDUCE rank == REDUCE rank=self rank tensor=tensor assertRaisesRegex RuntimeError cm rank == wrapper_pg reduce tensor wrapper_pg barrier _validate_error exception=cm exception op_type= REDUCE rank == BARRIER rank=self rank tensor=tensor assertRaisesRegex RuntimeError cm rank == wrapper_pg broadcast tensor output_tensors = torch zeros_like tensor _ range world_size wrapper_pg allgather output_tensors tensor _validate_error exception=cm exception op_type= BROADCAST rank == ALLGATHER rank=self rank tensor=tensor _test_collective_shape_mismatch wrapper_pg use_cuda=False wrapper_pg barrier dim = rank == tensor = torch randn dim use_cuda tensor = tensor rank assertRaisesRegex RuntimeError cm wrapper_pg allreduce tensor _validate_error exception=cm exception op_type= ALLREDUCE rank=self rank tensor=tensor Check errors raised when dimensionality shapes different tensor = torch randn rank == torch randn use_cuda tensor = tensor rank assertRaisesRegex RuntimeError cm wrapper_pg allreduce tensor _validate_error exception=cm exception op_type= ALLREDUCE rank=self rank tensor=tensor Check shape errors scatter input = torch tensor rank rank == rank rank device=self rank use_cuda cpu _ range world_size outputs = torch tensor - rank == - - device=self rank use_cuda cpu _ range world_size root_rank = opts = c d ScatterOptions opts rootRank = root_rank assertRaisesRegex RuntimeError cm rank == root_rank wrapper_pg scatter outputs rank input opts wait wrapper_pg scatter outputs rank opts wait _validate_error exception=cm exception op_type= SCATTER rank=self rank tensor=outputs rank ASAN safe since we spawning processes TEST_WITH_DEV_DBG_ASAN requires_gloo requires_nccl ProcessGroupNCCLWrapperTest AbstractProcessGroupWrapperTest setUp super AbstractProcessGroupWrapperTest setUp _spawn_processes TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests use TORCH_NCCL_BLOCKING_WAIT will test expected os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = property world_size - int _create_wrapper_pg with_new_group=False timeout= store = c d FileStore file_name world_size c d init_process_group backend= nccl rank=self rank world_size=self world_size store=store timeout=timedelta seconds=timeout with_new_group pg = c d new_group backend= nccl timeout=timedelta seconds=timeout _pg = c d ProcessGroupNCCL store rank world_size timeout=timedelta seconds=timeout pg = c d _create_process_group_wrapper _pg unused store rank world_size timeout=timeout pg requires_nccl skip_if_lt_x_gpu test_collective_hang pg = _create_wrapper_pg timeout= _test_collective_hang pg NOTE these tests separated debug level instead combined into one due https github com pytorch pytorch issues they can combined after resolved requires_nccl skip_if_lt_x_gpu with_dist_debug_levels levels= DETAIL test_collectives_op_mismatch_debug_mode pg = _create_wrapper_pg with_new_group=True _test_collectives_op_mismatch pg use_cuda=True _test_nccl_only_op_mismatch pg requires_nccl skip_if_lt_x_gpu with_dist_debug_levels levels= OFF test_collectives_op_mismatch pg = _create_wrapper_pg with_new_group=False _test_collectives_op_mismatch pg use_cuda=True _test_nccl_only_op_mismatch pg requires_nccl skip_if_lt_x_gpu with_dist_debug_levels levels= DETAIL test_collective_shape_mismatch_debug_mode_detail pg = _create_wrapper_pg with_new_group=True _test_collective_shape_mismatch pg use_cuda=True _test_nccl_only_shape_mismatch pg requires_nccl skip_if_lt_x_gpu with_dist_debug_levels levels= OFF test_collective_shape_mismatch_debug_mode_off pg = _create_wrapper_pg with_new_group=False _test_collective_shape_mismatch pg use_cuda=True _test_nccl_only_shape_mismatch pg _test_nccl_only_op_mismatch wrapper_pg device = f cuda rank assertRaisesRegex RuntimeError cm output = torch zeros + rank device=device input = torch ones world_size device=device rank == wrapper_pg _allgather_base output input wait wrapper_pg _reduce_scatter_base output input wait op_type = ALLGATHER_BASE rank == REDUCE_SCATTER_BASE _validate_error exception=cm exception op_type=op_type rank=self rank tensor=input _test_nccl_only_shape_mismatch wrapper_pg device = f cuda rank assertRaisesRegex RuntimeError cm output = torch zeros + rank device=device input = torch ones world_size + device=device wrapper_pg _reduce_scatter_base output input wait _validate_error exception=cm exception op_type= REDUCE_SCATTER_BASE rank=self rank tensor=input verify_diff=False assertRaisesRegex RuntimeError cm output = torch zeros device=device input = torch ones + rank world_size device=device wrapper_pg _reduce_scatter_base output input wait _validate_error exception=cm exception op_type= REDUCE_SCATTER_BASE rank=self rank tensor=input verify_diff=False requires_nccl skip_if_lt_x_gpu with_dist_debug_levels levels= DETAIL test_coalescing_manager_debug_mode_detail Tests coalescing manager w TORCH_DISTRIBUTED_DEBUG does crash https github com pytorch pytorch issues torch cuda set_device rank pg = _create_wrapper_pg with_new_group=True dev = torch cuda current_device pg _start_coalescing torch device dev pg allreduce torch ones device=dev pg _end_coalescing torch device dev requires_nccl skip_if_lt_x_gpu with_dist_debug_levels levels= DETAIL patch torch distributed distributed_c d _GLOO_AVAILABLE False test_debug_level_detail_no_gloo assertRaisesRegex AssertionError ProcessGroupWrapper unsupported without GLOO backend _create_wrapper_pg requires_nccl skip_if_lt_x_gpu patch torch distributed distributed_c d _GLOO_AVAILABLE False test_new_group_no_gloo patched_isinstance obj clazz clazz _ProcessGroupWrapper raise NameError isinstance obj clazz patch torch distributed distributed_c d isinstance side_effect=patched_isinstance _create_wrapper_pg with_new_group=True nothing assert isinstance pg _ProcessGroupWrapper should never invoked since proceeded _GLOO_AVAILABLE check test will fail unexpected NameError requires_gloo ProcessGroupGlooWrapperTest AbstractProcessGroupWrapperTest opts threads= timeout= opts = c d ProcessGroupGloo _Options opts _timeout = timeout opts _devices = create_device interface=LOOPBACK opts _threads = threads opts _create_wrapper_pg with_new_group=False timeout= store = c d FileStore file_name world_size c d init_process_group backend= gloo rank=self rank world_size=self world_size store=store with_new_group pg = c d new_group backend= gloo _pg = c d ProcessGroupGloo store rank world_size opts timeout=timeout pg = c d _create_process_group_wrapper _pg unused store rank world_size timeout=timeout pg test_collective_hang pg = _create_wrapper_pg timeout= _test_collective_hang pg NOTE these tests separated debug level instead combined into one due https github com pytorch pytorch issues they can combined after resolved with_dist_debug_levels levels= DETAIL test_collectives_op_mismatch_debug_mode pg = _create_wrapper_pg with_new_group=True _test_collectives_op_mismatch pg with_dist_debug_levels levels= OFF test_collectives_op_mismatch pg = _create_wrapper_pg with_new_group=False _test_collectives_op_mismatch pg with_dist_debug_levels levels= DETAIL test_collective_shape_mismatch_debug_mode pg = _create_wrapper_pg with_new_group=True _test_collective_shape_mismatch pg with_dist_debug_levels levels= OFF test_collective_shape_mismatch_debug_mode_off pg = _create_wrapper_pg with_new_group=False _test_collective_shape_mismatch pg skip_if_lt_x_gpu with_dist_debug_levels levels= DETAIL test_collectives_op_mismatch_cuda_debug_mode pg = _create_wrapper_pg with_new_group=True _test_collectives_op_mismatch pg use_cuda=True skip_if_lt_x_gpu with_dist_debug_levels levels= OFF test_collectives_op_mismatch_cuda pg = _create_wrapper_pg with_new_group=False _test_collectives_op_mismatch pg use_cuda=True skip_if_lt_x_gpu with_dist_debug_levels levels= DETAIL test_collective_shape_mismatch_cuda_debug_mode pg = _create_wrapper_pg with_new_group=True _test_collective_shape_mismatch pg use_cuda=True skip_if_lt_x_gpu with_dist_debug_levels levels= OFF test_collective_shape_mismatch_cuda pg = _create_wrapper_pg with_new_group=False _test_collective_shape_mismatch pg use_cuda=True __name__ == __main__ assert torch cuda _initialized test_pg_wrapper must have initialized CUDA context main process run_tests