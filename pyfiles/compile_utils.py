mypy ignore-errors operator collections abc Callable sympy torch torch fx fx torch fx experimental symbolic_shapes free_unbacked_symbols torch multiprocessing reductions StorageWeakRef torch utils _pytree pytree torch utils _pytree tree_flatten aten = torch ops aten get_aten_target node fx Node - Callable hasattr node target overloadpacket node target overloadpacket node target rand_ops = aten dropout aten _fused_dropout aten _standard_gamma aten bernoulli aten multinomial aten native_dropout aten normal aten poisson aten binomial aten rrelu aten rand_like aten rand aten randint aten randn aten randperm new copy torch fx graph Graph CSE applied input graph fx_graph_cse fx_g torch fx graph Graph new_graph = fx Graph env = map node old graph node new graph hash_env = map hash node new graph token_map = map hash token torch _inductor pattern_matcher compute_mutation_region_ids same_mutation_regions compute_mutation_region_ids fx_g type ignore arg-type Make set separate storages returned output which will preserved when pruning This prevents us deduplicating returned tensors which have experienced identical operations separate data structures eager mode output_node fx Node = list fx_g nodes - assert output_node op == output checkable_node node fx Node - bool We can evaluate only nodes represent tensors defined storage val node meta isinstance node meta val torch Tensor False try node meta val untyped_storage except NotImplementedError False True output_storages = StorageWeakRef n meta val untyped_storage n output_node all_input_nodes checkable_node n nodes_that_alias_outputs = n n fx_g nodes checkable_node n StorageWeakRef n meta val untyped_storage output_storages n fx_g nodes The placeholder output get_attr nodes copied new graph without change do CSE away random operations n op == placeholder n op == output n op == get_attr get_aten_target n rand_ops aten empty non-deterministic so don t CSE Also aten empty almost always fusible into its consumer so s worth CSEing get_aten_target n aten empty n nodes_that_alias_outputs This CSE pass currently doesn t handle re-propagation unbacked meta where ll sometimes eliminate _local_scalar_dense replace meta downstream users eg one bug we ve seen _local_scalar_dense_ Sym u = torch ops aten _local_scalar_dense default select_ sym_sum_ Sym u + u + u = torch sym_sum _local_scalar_dense_ _local_scalar_dense_ _local_scalar_dense_ noqa B Notice how _local_scalar_dense_ u sym_sum_ s meta incorrectly old pre-cse value u val n meta isinstance n meta val sympy Symbol free_unbacked_symbols n meta val new_node = new_graph node_copy n lambda x env x env n = new_node n op == call_function should never see n op == call_module call_method substitute args kwargs members their mapping env exists specs can used reconstruct nested list dictionaries substitute arg_list arg_list spec = tree_flatten arg_list i range len arg_list v = arg_list i isinstance v torch fx node Node v env arg_list i = env v isinstance v torch SymBool torch SymInt torch SymFloat arg_list i = v node tuple arg_list spec args args_spec = substitute n args kwargs kwargs_spec = substitute n kwargs each token corresponds unique node nodes same token can substituted token = target n target args args args_spec args_spec kwargs kwargs kwargs_spec kwargs_spec hash substituted args number do hash specs because specs hashable We need add type into hash avoid situations like hash primals_ == hash primals_ hash_arg = hash tuple type args tuple type kwargs hash_val = n target hash_arg check node has substitute can eliminated hash_val_in_hash_env = hash_val hash_env overwrite_due_to_mutation = False hash_val_in_hash_env token_map hash_val == token duplicate_n_prev = hash_env hash_val same_mutation_regions n duplicate_n_prev env n = duplicate_n_prev continue any futures duplicates should replace n duplicate_n_prev overwrite_due_to_mutation = True new_node = new_graph node_copy n lambda x env x env n = new_node overwrite_due_to_mutation hash_val_in_hash_env hash_env hash_val = new_node token_map hash_val = token new_graph raise_getitems gm fx GraphModule - fx GraphModule Pre-create list nodes iterate over modifying node order during loop can lead infinite loops handled properly getitem_nodes = list gm graph find_nodes op= call_function target=operator getitem loop through getitem nodes graph raise them parent node reverse order preserve their original relative order node reversed getitem_nodes assert len node all_input_nodes == parent = node all_input_nodes parent append node gm recompile gm strip_overloads gm Modifies target graph nodes attr ` gm ` strip overloads Args gm fx GraphModule The input Fx graph module modified node gm graph nodes isinstance node target torch _ops OpOverload node target = node target overloadpacket gm recompile get_placeholders graph graph find_nodes op= placeholder get_outputs graph node graph find_nodes op= output pytree tree_leaves node args raise AssertionError No output node found