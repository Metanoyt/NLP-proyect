mypy allow-untyped-defs Contains utils logging AOTAutograd including managing names graphs under compilation capturing user-friendly tracebacks debug messages collections contextlib contextmanager torch torch fx traceback fx_traceback This list since looking forward we can have arbitrarily nested graph_being_compiled list str = TODO It would nice reset numbering every time aot_id goes up annoying do right now because we don t know aot_id will come back dead so right now also happens globally unique number too cost wobbling you change how graphs compile nth_graph int = model_name str = model set_model_name name global model_name model_name = name get_aot_compilation_context - tuple list str str int list graph_being_compiled model_name nth_graph get_aot_graph_name - str Returns name graph being compiled global model_name graph_being_compiled nth_graph f model_name __ _ join graph_being_compiled _ nth_graph get_graph_being_compiled = get_aot_graph_name contextmanager track_graph_compiling aot_config graph_name global graph_being_compiled TODO Don t shove aot_id here set context graph_being_compiled = f aot_config aot_id _ graph_name old_name = None tracing_context = torch _guards TracingContext try_get old_name = tracing_context aot_graph_name tracing_context aot_graph_name = graph_being_compiled has_tracing_context = True has_tracing_context = False try yield finally global nth_graph nth_graph += graph_being_compiled = has_tracing_context tracing_context = torch _guards TracingContext try_get tracing_context aot_graph_name = old_name Set up hooks so during backward fx s stack_trace properly set callback_set = False setup_stacktrace_preservation_hooks roots list iter_graph roots roots seen = set q = collections deque type ignore var-annotated node roots node None node seen seen add node q append node while q node = q popleft fn _idx node next_functions fn seen fn None continue seen add fn q append fn yield node get_callback saved_stack_ callback global callback_set fx_traceback set_stack_trace saved_stack_ callback_set = False callback get_prehook stack_ seq_nr prehook grad_output global callback_set callback_set torch autograd variable Variable _execution_engine queue_callback type ignore attr-defined get_callback fx_traceback format_stack callback_set = True fx_traceback set_stack_trace stack_ fx_traceback set_grad_fn_seq_nr seq_nr prehook get_posthook special_stack_ seq_nr posthook grad_input grad_output fx_traceback set_stack_trace special_stack_ fx_traceback reset_grad_fn_seq_nr posthook node iter_graph roots forward_node_stack = node metadata get traceback_ node register_prehook get_prehook forward_node_stack node _sequence_nr special_stack = forward_node_stack copy special_stack append Gradient addition node due multiple use tensor around node register_hook get_posthook special_stack node _sequence_nr describe_input i aot_config i aot_config num_params_buffers f parameter buffer i f input i - aot_config num_params_buffers format_guard_bug_msg aot_config expected f At compilation time graph aot_config aot_id compiled under f assumption expected runtime case This indicates guard bug AOTAutograd Dynamo please file bug PyTorch