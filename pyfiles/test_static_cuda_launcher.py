Owner s module inductor os random tempfile unittest mock torch torch _dynamo device_interface get_interface_for_device torch _inductor codecache PyCodeCache torch _inductor runtime triton_helpers torch _inductor runtime static_cuda_launcher StaticallyLaunchedCudaKernel torch _inductor runtime triton_compat CompiledKernel tl triton torch _inductor runtime triton_helpers libdevice torch _inductor test_case TestCase torch testing _internal common_utils skipIfRocm torch testing _internal triton_utils requires_cuda_and_triton requires_cuda_and_triton TestStaticCudaLauncher TestCase setUp super setUp tmp_files = tearDown super tearDown tmp_file tmp_files try os remove tmp_file name except OSError pass write_cubin_to_tmp kernel CompiledKernel - str Only used tests where we don t have cubin path hasattr kernel _cubin_path Just used tests now TODO derive cubin_path wherever triton stores cubin file disk tmp_file = tempfile NamedTemporaryFile mode= wb delete=False tmp_file tmp_file write kernel asm cubin tmp_files append tmp_file tmp_file name _make_launcher compiled_kernel CompiledKernel - StaticallyLaunchedCudaKernel Compiles Triton kernel provided args writes its cubin temporary file returns file path cubin_file = write_cubin_to_tmp compiled_kernel compiled_kernel _cubin_path = cubin_file result = StaticallyLaunchedCudaKernel compiled_kernel Test reload cubin raw here old_cubin_path = result cubin_path assert old_cubin_path None result cubin_path = None result reload_cubin_from_raw old_cubin_path device_interface = get_interface_for_device cuda result load_kernel device_interface current_device result skipIfRocm test_basic triton jit simple_kernel arg arg x = tl load arg y = arg tl store arg x + y arg = torch zeros dtype=torch int device= cuda arg = args = arg arg compiled_kernel = simple_kernel args launcher = _make_launcher compiled_kernel assertEqual arg torch tensor dtype=torch int device= cuda assertEqual launcher arg_tys Oi new_arg = torch zeros dtype=torch int device= cuda device_interface = get_interface_for_device cuda stream = device_interface get_raw_stream device_interface current_device launcher run stream new_arg arg assertEqual new_arg arg I wish I could macro all int types into single unit test loop variables aren t allowed type annotations python triton relies inspect get_source get type annotations so I can t even use exec generate test cases So we ll just make few kernels hand skipIfRocm test_unsigned_integers triton jit unsigned_integers arg arg tl uint arg tl uint arg tl uint arg tl uint x = tl load arg y = arg + arg + arg + arg tl store arg x + y arg = torch zeros dtype=torch uint device= cuda Using small numbers creates Literal type which triton treats constant args = arg compiled_kernel = unsigned_integers args launcher = _make_launcher compiled_kernel assertEqual arg torch tensor dtype=torch uint device= cuda assertEqual launcher arg_tys OBHIK new_arg = torch zeros dtype=torch uint device= cuda device_interface = get_interface_for_device cuda stream = device_interface get_raw_stream device_interface current_device launcher run stream new_arg assertEqual new_arg arg skipIfRocm test_signed_integers triton jit signed_integers arg arg tl int arg tl int arg tl int arg tl int x = tl load arg y = arg + arg + arg + arg tl store arg x + y arg = torch zeros dtype=torch int device= cuda Using small numbers creates Literal type which triton treats constant args = arg compiled_kernel = signed_integers args launcher = _make_launcher compiled_kernel assertEqual arg torch tensor dtype=torch int device= cuda assertEqual launcher arg_tys Obhil new_arg = torch zeros dtype=torch int device= cuda device_interface = get_interface_for_device cuda stream = device_interface get_raw_stream device_interface current_device launcher run stream new_arg assertEqual new_arg arg skipIfRocm test_basic_ arg triton jit simple_kernel_ _arg arg x = tl load arg tl store arg x + arg = torch zeros dtype=torch int device= cuda compiled_kernel = simple_kernel_ _arg arg launcher = _make_launcher compiled_kernel assertEqual arg torch tensor dtype=torch int device= cuda assertEqual launcher arg_tys O new_arg = torch zeros dtype=torch int device= cuda device_interface = get_interface_for_device cuda stream = device_interface get_raw_stream device_interface current_device launcher run stream new_arg assertEqual new_arg arg skipIfRocm test_constexpr Constexprs compiled directly into cubin file so we never need pass StaticCudaLauncher triton jit kernel_constexpr arg CONSTANT tl constexpr x = tl load arg tl store arg x + CONSTANT Can t use make_launcher because constexpr needs constant arg = torch zeros dtype=torch int device= cuda compiled_kernel = kernel_constexpr arg CONSTANT= launcher = _make_launcher compiled_kernel assertEqual arg torch tensor dtype=torch int device= cuda assertEqual launcher arg_tys O new_arg = torch zeros dtype=torch int device= cuda device_interface = get_interface_for_device cuda stream = device_interface get_raw_stream device_interface current_device launcher run stream new_arg assertEqual new_arg arg skipIfRocm test_implied_constant xnumel unused kernel isn t explicitly marked constexpr This kernel generated inductor so has bunch unused arguments We don t change triton jit triton_red_fused_any_isinf_ in_ptr out_ptr xnumel noqa F r _numel XBLOCK tl constexpr R _BLOCK tl constexpr xnumel = noqa F rnumel = r _numel noqa F RBLOCK tl constexpr = R _BLOCK noqa F xoffset = tl program_id XBLOCK xindex = xoffset + tl arange XBLOCK None noqa F xmask = tl full XBLOCK R _BLOCK True tl int noqa F r _base = tl arange R _BLOCK None rbase = r _base noqa F _tmp = tl full XBLOCK R _BLOCK False tl int r _offset range r _numel R _BLOCK r _index = r _offset + r _base r _mask = r _index r _numel roffset = r _offset noqa F rindex = r _index noqa F r _ = r _index tmp = tl load in_ptr + r _ r _mask eviction_policy= evict_first other= tmp = libdevice isinf tmp tl int tmp = tl broadcast_to tmp XBLOCK R _BLOCK tmp = _tmp &#124; tmp _tmp = tl where r _mask tmp _tmp tmp = triton_helpers any _tmp tl int None tl int tl store out_ptr + tl full XBLOCK tl int tmp None arg = torch tensor float inf device= cuda arg = torch tensor False device= cuda arg = torch tensor False device= cuda compiled_kernel = triton_red_fused_any_isinf_ arg arg XBLOCK= R _BLOCK= launcher = _make_launcher compiled_kernel device_interface = get_interface_for_device cuda stream = device_interface get_raw_stream device_interface current_device Don t pass xnumel constant launcher run stream arg arg assertEqual arg arg skipIfRocm test_kernel_no_args Just easy way test incompatible number arguments triton jit kernel_no_op pass compiled_kernel = kernel_no_op launcher = _make_launcher compiled_kernel device_interface = get_interface_for_device cuda stream = device_interface get_raw_stream device_interface current_device launcher run stream skipIfRocm test_high_shared_mem triton jit simple_kernel arg arg x = tl load arg y = arg tl store arg x + y arg = torch zeros dtype=torch int device= cuda arg = args = arg arg compiled_kernel = simple_kernel args Allocate KB memory compiled_kernel shared = launcher = _make_launcher compiled_kernel assertEqual arg torch tensor dtype=torch int device= cuda assertEqual launcher arg_tys Oi new_arg = torch zeros dtype=torch int device= cuda device_interface = get_interface_for_device cuda stream = device_interface get_raw_stream device_interface current_device launcher slow_launch_kernel = True launcher run stream new_arg arg assertEqual new_arg arg skipIfRocm test_too_high_shared_mem triton jit simple_kernel arg arg x = tl load arg y = arg tl store arg x + y arg = torch zeros dtype=torch int device= cuda arg = args = arg arg compiled_kernel = simple_kernel args Allocate too much shared memory compiled_kernel shared = assertRaisesRegex RuntimeError out resource simple_kernel lambda _make_launcher compiled_kernel skipIfRocm test_kernel_empty_tensor Triton kernel generated torch compile following torch compile foo x y torch cat x y + Running example input torch _dynamo decorators mark_unbacked t x = torch rand device= cuda y = torch rand device= cuda triton jit triton_poi_fused_cat_ in_ptr in_ptr out_ptr ks xnumel XBLOCK tl constexpr xoffset = tl program_id tl int XBLOCK xindex = xoffset + tl arange XBLOCK tl int xmask = xindex xnumel x = xindex tmp = x tmp = ks tmp = tmp tmp tmp = tl load in_ptr + x xmask tmp eviction_policy= evict_last other= tmp = tmp = tmp tmp tmp = tl full tmp shape tmp dtype tmp = tl where tmp tmp tmp tmp = tmp = tmp tmp = tl load in_ptr + x + - ks xmask tmp eviction_policy= evict_last other= tmp = tmp = tmp + tmp tmp = tl full tmp shape tmp dtype tmp = tl where tmp tmp tmp tmp = tl where tmp tmp tmp tl store out_ptr + x tmp xmask arg = arg = torch randn device= cuda arg = torch randn device= cuda buf = torch empty device= cuda buf = torch empty device= cuda xnumel = + arg compiled_kernel = triton_poi_fused_cat_ arg arg buf arg xnumel XBLOCK= launcher = _make_launcher compiled_kernel device_interface = get_interface_for_device cuda stream = device_interface get_raw_stream device_interface current_device launcher run stream arg arg buf arg xnumel assertEqual buf buf skipIfRocm test_kernel_many_args N = Make arguments args = f arg_ i i range N decl = join args sums = f total += arg_ i i range N sums_str = \n join sums template = f torch _inductor runtime triton_compat tl triton triton jit kernel_many_args out_tensor decl out = tl load out_tensor total = out sums_str tl store out_tensor total result = PyCodeCache load template lstrip kernel_args = tuple random random _ range N buf = torch zeros device= cuda compiled_kernel = result kernel_many_args buf kernel_args launcher = _make_launcher compiled_kernel device_interface = get_interface_for_device cuda stream = device_interface get_raw_stream device_interface current_device buf = torch zeros device= cuda launcher run stream buf kernel_args assertEqual buf buf requires_cuda_and_triton torch _inductor config patch use_static_cuda_launcher True strict_static_cuda_launcher True TestStaticTritonCompileResult TestCase Tests static cuda launcher torch compile skipIfRocm test_basic_compile torch compile foo x y x + y x = torch randn device= cuda y = torch randn device= cuda assertEqual foo x y x + y skipIfRocm The error gets raised worker so we want use separate process torch _inductor config patch compile_threads test_incompatible_code User defined triton kernel triton jit custom_kernel arg_ arg_ x = tl load arg_ y = arg_ tl store arg_ x + y torch compile foo x custom_kernel x x x = torch randn device= cuda assertRaisesRegex torch _inductor exc InductorError CannotStaticallyLaunchKernel User defined triton kernel lambda foo x skipIfRocm The error gets raised worker so we want use separate process torch _inductor config patch compile_threads static_launch_user_defined_triton_kernels True test_static_launch_user_defined_triton_kernels User defined triton kernel triton jit custom_kernel arg_ arg_ x = tl load arg_ y = arg_ tl store arg_ x + y torch compile foo x custom_kernel x x x = torch randn device= cuda x = x clone detach_ assertEqual foo x x + skipIfRocm test_empty_tensor torch compile foo x y torch cat x y + x = torch rand device= cuda torch _dynamo decorators mark_unbacked x y = torch rand device= cuda result = foo x y assertEqual result torch cat x y + skipIfRocm test_any fn x x any - x isinf any torch all x isinf dim= torch all torch logical_not x isinf compiled_fn = torch compile fn arg = -torch rand device= cuda dtype=torch float eager_result = fn arg compiled_result = compiled_fn arg assertEqual eager_result compiled_result arg = float inf eager_result = fn arg compiled_result = compiled_fn arg assertEqual eager_result compiled_result skipIfRocm test_disable_static_cuda_launcher torch compile fn x y torch cat x y + Test static cuda launcher fact disabled torch _inductor config patch use_static_cuda_launcher False x = torch rand device= cuda y = torch rand device= cuda mock patch torch _inductor runtime triton_heuristics StaticTritonCompileResult make_launcher mocked result = fn x y mocked assert_not_called assertEqual result torch cat x y + __name__ == __main__ torch _inductor test_case run_tests run_tests