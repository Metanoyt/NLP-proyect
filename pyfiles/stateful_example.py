mypy allow-untyped-defs Owner s oncall distributed pyre-unsafe os shutil torch torch distributed dist torch distributed checkpoint dcp torch multiprocessing mp torch nn nn torch distributed checkpoint state_dict _patch_model_state_dict _patch_optimizer_state_dict torch distributed device_mesh init_device_mesh torch distributed fsdp FullyShardedDataParallel FSDP CHECKPOINT_DIR = f ~ os environ get LOGNAME checkpoint Model torch nn Module __init__ - None super __init__ torch manual_seed net = nn Sequential nn Linear nn ReLU net = nn Sequential nn Linear nn ReLU net = nn Linear net = nn Sequential nn ReLU nn Linear forward x net net net net x get_input torch rand device= cuda _make_stateful model optim _patch_model_state_dict model _patch_optimizer_state_dict model optimizers=optim _train model optim train_steps= torch manual_seed loss = None _ range train_steps loss = model model get_input sum loss backward optim step optim zero_grad loss _init_model device world_size device_mesh = init_device_mesh device world_size model = Model cuda model = FSDP model device_mesh=device_mesh use_orig_params=True optim = torch optim Adam model parameters lr= _make_stateful model optim model optim run rank world_size device= cuda Set up world pg os environ MASTER_ADDR = localhost os environ MASTER_PORT = dist init_process_group cpu gloo cuda nccl rank=rank world_size=world_size torch cuda set_device rank model optim = _init_model device world_size _train model optim train_steps= dcp save state_dict= model model optimizer optim checkpoint_id=CHECKPOINT_DIR presumably do something model optim = _init_model device world_size dcp load state_dict= model model optimizer optim checkpoint_id=CHECKPOINT_DIR _train model optim train_steps= __name__ == __main__ world_size = torch cuda device_count print f Running stateful checkpoint example world_size devices shutil rmtree CHECKPOINT_DIR ignore_errors=True mp spawn run args= world_size nprocs=world_size join=True