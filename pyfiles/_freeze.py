mypy allow-untyped-defs Freezing This intended imported directly please use exposed functionalities ` torch jit ` typing Optional torch torch jit _script RecursiveScriptModule ScriptModule freeze mod preserved_attrs Optional list str = None optimize_numerics bool = True r Freeze ScriptModule inline submodules attributes constants Freezing ` ScriptModule ` will clone attempt inline cloned module s submodules parameters attributes constants TorchScript IR Graph By default ` forward ` will preserved well attributes methods specified ` preserved_attrs ` Additionally any attribute modified within preserved method will preserved Freezing currently only accepts ScriptModules eval mode Freezing applies generic optimization will speed up your model regardless machine To further optimize using server-specific settings run ` optimize_for_inference ` after freezing Args mod ` ScriptModule ` module frozen preserved_attrs Optional List str list attributes preserve addition forward method Attributes modified preserved methods will also preserved optimize_numerics bool If ` ` True ` ` set optimization passes will run does strictly preserve numerics Full details optimization can found ` torch jit run_frozen_optimizations ` Returns Frozen ` ScriptModule ` Example Freezing simple module Parameter testcode torch MyModule torch nn Module __init__ N M super __init__ weight = torch nn Parameter torch rand N M linear = torch nn Linear N M forward input output = weight mm input output = linear output output scripted_module = torch jit script MyModule eval frozen_module = torch jit freeze scripted_module parameters have been removed inlined into Graph constants assert len list frozen_module named_parameters == See compiled graph Python code print frozen_module code Example Freezing module preserved attributes testcode torch MyModule torch nn Module __init__ - None super __init__ modified_tensor = torch tensor version = forward input modified_tensor += input + modified_tensor scripted_module = torch jit script MyModule eval frozen_module = torch jit freeze scripted_module preserved_attrs= version we ve manually preserved ` version ` so still exists frozen module can modified assert frozen_module version == frozen_module version = ` modified_tensor ` detected being mutated forward so freezing preserves retain model semantics assert frozen_module torch tensor == torch tensor now we ve run once next result will incremented one assert frozen_module torch tensor == torch tensor Note Freezing submodule attributes also supported frozen_module = torch jit freeze scripted_module preserved_attrs= submodule version Note If you re sure why attribute being inlined constant you can run ` dump_alias_db ` frozen_module forward graph see freezing has detected attribute being modified Note Because freezing makes weights constants removes module hierarchy ` ` other nn Module methods manipulate device dtype no longer work As workaround You can remap devices specifying ` map_location ` ` torch jit load ` however device-specific logic may have been baked into model isinstance mod ScriptModule raise RuntimeError Freezing expects ScriptModule input Please use torch jit script torch jit trace script your nn Module mod training raise RuntimeError Freezing currently only implemented modules eval mode Please call eval your module before freezing preserved_attrs = preserved_attrs preserved_attrs None out = RecursiveScriptModule torch _C _freeze_module mod _c preserved_attrs RecursiveScriptModule _finalize_scriptmodule out preserved_methods = x x preserved_attrs mod _c _has_method x run_frozen_optimizations out optimize_numerics preserved_methods out run_frozen_optimizations mod optimize_numerics bool = True preserved_methods Optional list str = None r Run series optimizations looking patterns occur frozen graphs The current set optimizations includes - Dropout Removal - Pretranspose Linear Layers - Concat Linear Layers same input Tensor - Conv - Batchnorm folding - Conv - Add Sub folding - Conv - Mul Div folding Args mod ` ScriptModule ` frozen module optimized optimize_numerics bool If ` ` True ` ` set optimization passes will run does strictly preserve numerics These optimizations preserve default rtol atol ` torch testing assert_close ` when applied single transformation however module where many transformations applied rtol atol may no longer fall within default ` assert_close ` tolerance Conv - Batchnorm folding Conv-Add Sub Conv - Mul Div folding all may alter numerics Returns None Note In rare occasions can result slower execution Example Freezing module Conv- Batchnorm code-block python torch in_channels out_channels = conv = torch nn Conv d in_channels out_channels kernel_size= stride= bias=True bn = torch nn BatchNorm d out_channels eps= mod = torch nn Sequential conv bn set optimize False here default freezing runs run_frozen_optimizations frozen_mod = torch jit freeze torch jit script mod eval optimize=False inspect frozen mod assert batch_norm str frozen_mod graph torch jit run_frozen_optimizations frozen_mod assert batch_norm str frozen_mod graph mod _c _has_method forward torch _C _jit_pass_optimize_frozen_graph mod graph optimize_numerics preserved_methods None preserved_methods = method preserved_methods torch _C _jit_pass_optimize_frozen_graph mod __getattr__ method graph optimize_numerics optimize_for_inference mod ScriptModule other_methods Optional list str = None - ScriptModule Perform set optimization passes optimize model purposes inference If model already frozen optimize_for_inference will invoke ` torch jit freeze ` automatically In addition generic optimizations should speed up your model regardless environment prepare inference will also bake build specific settings such presence CUDNN MKLDNN may future make transformations which speed things up one machine slow things down another Accordingly serialization implemented following invoking ` optimize_for_inference ` guaranteed This still prototype may have potential slow down your model Primary use cases have been targeted so far have been vision models cpu gpu lesser extent Example optimizing module Conv- Batchnorm torch in_channels out_channels = conv = torch nn Conv d in_channels out_channels kernel_size= stride= bias=True bn = torch nn BatchNorm d out_channels eps= mod = torch nn Sequential conv bn frozen_mod = torch jit optimize_for_inference torch jit script mod eval assert batch_norm str frozen_mod graph built MKLDNN convolution will run MKLDNN weights assert MKLDNN frozen_mod graph isinstance mod ScriptModule raise RuntimeError optimize_for_inference expects ScriptModule input Please use torch jit script torch jit trace script your nn Module other_methods None other_methods = hasattr mod training mod = freeze mod eval preserved_attrs=other_methods torch _C _jit_pass_optimize_for_inference mod _c other_methods mod