mypy allow-untyped-defs torch torch ao nn intrinsic torch ao nn intrinsic qat torch ao nn quantized nnq torch nn functional F _reverse_repeat_padding = nnq modules conv _reverse_repeat_padding ConvAdd d nnq Conv d r A ConvAdd d module fused module Conv d Add We adopt same interface ` torch ao nn quantized Conv d ` Attributes Same torch ao nn quantized Conv d _FLOAT_MODULE = torch ao nn intrinsic ConvAdd d type ignore assignment __init__ in_channels out_channels kernel_size stride= padding= dilation= groups= bias=True padding_mode= zeros device=None dtype=None super __init__ in_channels out_channels kernel_size stride=stride padding=padding dilation=dilation groups=groups bias=bias padding_mode=padding_mode device=device dtype=dtype forward input extra_input type ignore override Temporarily using len shape instead ndim due JIT issue https github com pytorch pytorch issues len input shape = raise ValueError Input shape must ` N C H W ` padding_mode = zeros _reversed_padding_repeated_twice = _reverse_repeat_padding padding input = F pad input _reversed_padding_repeated_twice mode=self padding_mode torch ops quantized conv d_add input extra_input _packed_params scale zero_point _get_name QuantizedConvAdd d classmethod from_float cls mod use_precomputed_fake_quant=False type ignore override super from_float mod use_precomputed_fake_quant=use_precomputed_fake_quant classmethod from_reference cls ref_qconv output_scale output_zero_point super from_reference ref_qconv output_scale output_zero_point ConvAddReLU d nnq Conv d r A ConvAddReLU d module fused module Conv d Add Relu We adopt same interface ` torch ao nn quantized Conv d ` Attributes Same torch ao nn quantized Conv d _FLOAT_MODULE = torch ao nn intrinsic ConvAddReLU d type ignore assignment __init__ in_channels out_channels kernel_size stride= padding= dilation= groups= bias=True padding_mode= zeros device=None dtype=None super __init__ in_channels out_channels kernel_size stride=stride padding=padding dilation=dilation groups=groups bias=bias padding_mode=padding_mode device=device dtype=dtype forward input extra_input type ignore override Temporarily using len shape instead ndim due JIT issue https github com pytorch pytorch issues len input shape = raise ValueError Input shape must ` N C H W ` padding_mode = zeros _reversed_padding_repeated_twice = _reverse_repeat_padding padding input = F pad input _reversed_padding_repeated_twice mode=self padding_mode torch ops quantized conv d_add_relu input extra_input _packed_params scale zero_point _get_name QuantizedConvAddReLU d classmethod from_float cls mod use_precomputed_fake_quant=False type ignore override super from_float mod use_precomputed_fake_quant=use_precomputed_fake_quant classmethod from_reference cls ref_qconv output_scale output_zero_point super from_reference ref_qconv output_scale output_zero_point