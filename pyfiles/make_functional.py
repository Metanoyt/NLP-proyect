mypy allow-untyped-defs Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree copy collections abc Callable Iterable Sequence typing Any NoReturn Union torch torch nn nn torch Tensor torch nn utils _named_member_accessor NamedMemberAccessor Utilities make nn Module functional In particular goal able provide function takes input parameters evaluate nn Module using fixed inputs raise_parameter_tying_error - NoReturn raise RuntimeError make_functional module we don t yet support models do parameter tying also sometimes known weight sharing Please try rewrite your model replacing all instances tied parameter another comment your support https github com pytorch functorch issues create_names_map named_params Union dict str Tensor Iterable tuple str Tensor tied_named_params Union dict str Tensor Iterable tuple str Tensor - dict str list str named_params dictionary tensors A A B B tied_named_params another dictionary tensors A A B B B_tied B potentially tied duplicated tensors This function creates mapping names named_params names tied_named_params A A B B B_tied pyrefly ignore no-matching-overload named_params = dict named_params pyrefly ignore no-matching-overload tied_named_params = dict tied_named_params tensors_dict_keys = set named_params keys tied_tensors_dict_keys = set tied_named_params keys assert tensors_dict_keys issubset tied_tensors_dict_keys tensor_to_mapping dict Tensor tuple str list str = key tensor named_params items pyrefly ignore unsupported-operation tensor_to_mapping tensor = key key tensor tied_named_params items assert tensor tensor_to_mapping pyrefly ignore bad-argument-type tensor_to_mapping tensor append key dict tensor_to_mapping values _extract_members mod nn Module named_members Callable Iterable tuple str Tensor subclass Callable Tensor Tensor - tuple tuple Tensor tuple str dict str list str all_named_members = tuple named_members remove_duplicate=False unique_named_members = tuple named_members remove_duplicate=True names_map = create_names_map unique_named_members all_named_members Remove all members model memo = accessor = NamedMemberAccessor mod name p all_named_members p memo memo p = subclass torch empty_like p device= meta replacement = memo p accessor set_tensor name replacement len unique_named_members == names params = names params = zip unique_named_members type ignore assignment params names names_map extract_weights mod nn Module - tuple tuple Tensor tuple str dict str list str This function removes all Parameters model them tuple well their original attribute names The weights must re-loaded ` load_weights ` before model can used again Note function modifies model place after call mod parameters will empty _extract_members mod mod named_parameters nn Parameter extract_buffers mod nn Module - tuple tuple Tensor tuple str dict str list str _extract_members mod mod named_buffers lambda x x load_weights mod nn Module names Sequence str params Sequence Tensor as_params bool = False - None Reload set weights so ` mod ` can used again perform forward pass Note ` params ` regular Tensors can have history so left Tensors This means mod parameters will still empty after call accessor = NamedMemberAccessor mod as_params params = nn Parameter p p params accessor set_tensors names params _swap_state mod nn Module names_map dict str list str elems Iterable Tensor - list Tensor result list Tensor = accessor = NamedMemberAccessor mod _ attr_names elem zip names_map items elems i attr_name enumerate attr_names i == result append accessor swap_tensor attr_name elem accessor set_tensor attr_name elem result load_buffers mod nn Module names Sequence str buffers Sequence Tensor as_params bool = False - None accessor = NamedMemberAccessor mod accessor set_tensors names buffers load_state model nn Module weights Sequence Tensor weight_names Sequence str buffers Sequence Tensor = buffer_names Sequence str = - nn Module load_state model weights weight_names buffers= buffer_names= - model load_state takes ` weights ` ` buffers ` assigns them model This inverse operation ` make_functional_deprecated_v ` assert len weight_names == len weights load_weights model weight_names weights len buffers assert len buffer_names == len buffers load_buffers model buffer_names buffers model make_functional_deprecated_v model nn Module make_functional_deprecated_v model - weights func weight_names Given nn Module make_functional_deprecated_v extracts state weights returns functional version model ` func ` This makes so possible use transforms over parameters ` model ` ` func ` can invoked follows ` ` ` x = torch randn model = nn Linear weights func _ = make_functional_deprecated_v model func weights x ` ` ` And here example applying grad transform ` ` ` x = torch randn model = nn Linear weights _ func = make_functional_deprecated_v model grad_weights = grad func weights x ` ` ` To put state back into model use ` load_state ` buffers = list model buffers len buffers raise RuntimeError make_functional_deprecated_v model ` model ` has buffers Please use make_functional_with_buffers_deprecated_v model instead weights descriptors _ = extract_weights model fun weights data mutable_model = copy deepcopy model load_weights mutable_model descriptors weights mutable_model data weights fun descriptors make_functional_with_buffers_deprecated_v model nn Module make_functional_with_buffers_deprecated_v model - weights buffers func weight_names buffer_names Given nn Module make_functional_with_buffers_deprecated_v extracts state weights buffers returns functional version model ` func ` ` func ` can invoked follows ` ` ` x = torch randn model = nn Linear weights buffers func _ _ = make_functional_with_buffers_deprecated_v model func weights buffers x ` ` ` And here example applying grad transform ` ` ` x = torch randn model = nn Linear weights buffers func _ _ = make_functional_with_buffers_deprecated_v model func weights buffers x grad_weights = grad func weights buffers x ` ` ` To put state back into model use ` load_state ` weights weight_descriptors _ = extract_weights model buffers buf_descriptors _ = extract_buffers model fun weights buffers data mutable_model = copy deepcopy model load_weights mutable_model weight_descriptors weights load_buffers mutable_model buf_descriptors buffers mutable_model data weights buffers fun weight_descriptors buf_descriptors FunctionalModuleWithBuffers nn Module This callable object returned func ` make_functional_with_buffers ` __init__ stateless_model nn Module param_names tuple str buffer_names tuple str param_names_map dict str list str buffer_names_map dict str list str - None super __init__ stateless_model = stateless_model param_names = param_names buffer_names = buffer_names all_names_map = dict param_names_map all_names_map update buffer_names_map staticmethod _create_from model nn Module disable_autograd_tracking bool = False - tuple FunctionalModuleWithBuffers tuple Tensor tuple Tensor TODO We don t need copy model create stateless copy model_copy = copy deepcopy model params param_names param_names_map = extract_weights model_copy buffers buffer_names buffer_names_map = extract_buffers model_copy disable_autograd_tracking param params param requires_grad_ False FunctionalModuleWithBuffers model_copy param_names buffer_names param_names_map buffer_names_map params buffers forward params Iterable Tensor buffers Iterable Tensor args kwargs - Any Temporarily load state back onto stateless_model old_state = _swap_state stateless_model all_names_map tuple params + tuple buffers try stateless_model args kwargs finally Remove loaded state stateless_model _swap_state stateless_model all_names_map old_state FunctionalModule nn Module This callable object returned func ` make_functional ` __init__ stateless_model nn Module param_names tuple str names_map dict str list str - None super __init__ stateless_model = stateless_model param_names = param_names names_map = names_map staticmethod _create_from model nn Module disable_autograd_tracking bool = False - tuple FunctionalModule tuple Tensor TODO We don t need copy model create stateless copy model_copy = copy deepcopy model params param_names names_map = extract_weights model_copy disable_autograd_tracking param params param requires_grad_ False FunctionalModule model_copy param_names names_map params forward params Iterable Tensor args kwargs - Any Temporarily load state back onto stateless_model old_state = _swap_state stateless_model names_map params try stateless_model args kwargs finally Remove loaded state stateless_model _swap_state stateless_model names_map old_state make_functional model nn Module disable_autograd_tracking bool = False - tuple FunctionalModule tuple Tensor make_functional model disable_autograd_tracking=False - func params Given ` ` torch nn Module ` ` func ` make_functional ` extracts state params returns functional version model ` ` func ` ` This makes so possible use transforms over parameters ` ` model ` ` ` ` func ` ` can invoked follows code-block python torch torch nn nn functorch make_functional x = torch randn model = nn Linear func params = make_functional model func params x And here example applying grad transform over parameters model code-block python torch torch nn nn functorch make_functional grad x = torch randn t = torch randn model = nn Linear func params = make_functional model compute_loss params x t y = func params x nn functional mse_loss y t grad_weights = grad compute_loss params x t If model has any buffers please use func ` make_functional_with_buffers ` instead Args model torch nn Module Input model disable_autograd_tracking bool Flag disable gradients tracking output parameters The returned params unrelated set params original model If False default params will have ` ` requires_grad=True ` ` them aka they will trackable regular PyTorch autograd matching requires_grad-ness params original model Otherwise returned params will have ` ` requires_grad=False ` ` Default False If you plan using regular PyTorch autograd e g you want call ` ` backward ` ` ` ` torch autograd grad ` ` then set ` ` disable_autograd_tracking=False ` ` Otherwise you re only planning using functorch s gradient transforms then please set ` ` disable_autograd_tracking=True ` ` avoid unnecessarily tracking history PyTorch autograd buffers = list model buffers len buffers raise RuntimeError make_functional model ` model ` has buffers Please use make_functional_with_buffers model instead FunctionalModule _create_from model disable_autograd_tracking=disable_autograd_tracking make_functional_with_buffers model nn Module disable_autograd_tracking bool = False - tuple FunctionalModuleWithBuffers tuple Tensor tuple Tensor make_functional_with_buffers model disable_autograd_tracking=False - func params buffers Given ` ` torch nn Module ` ` make_functional_with_buffers extracts state params buffers returns functional version model ` ` func ` ` can invoked like function ` ` func ` ` can invoked follows code-block python torch torch nn nn functorch make_functional_with_buffers x = torch randn model = nn Linear func params buffers = make_functional_with_buffers model func params buffers x And here example applying grad transform over parameters model code-block python torch torch nn nn functorch make_functional_with_buffers grad x = torch randn t = torch randn model = nn Linear func params buffers = make_functional_with_buffers model compute_loss params buffers x t y = func params buffers x nn functional mse_loss y t grad_weights = grad compute_loss params buffers x t Args model torch nn Module Input model disable_autograd_tracking bool Flag disable gradients tracking output parameters The returned params unrelated set params original model If False default params will have ` ` requires_grad=True ` ` them aka they will trackable regular PyTorch autograd matching requires_grad-ness params original model Otherwise returned params will have ` ` requires_grad=False ` ` Default False If you plan using regular PyTorch autograd e g you want call ` ` backward ` ` ` ` torch autograd grad ` ` then set ` ` disable_autograd_tracking=False ` ` Otherwise you re only planning using functorch s gradient transforms then please set ` ` disable_autograd_tracking=True ` ` avoid unnecessarily tracking history PyTorch autograd FunctionalModuleWithBuffers _create_from model disable_autograd_tracking=disable_autograd_tracking transpose_stack tuple_of_tuple_of_tensors tuple tuple Tensor - tuple Tensor tuple_of_tuple_of_tensors = tuple zip tuple_of_tuple_of_tensors results = tuple torch stack shards detach shards tuple_of_tuple_of_tensors results combine_state_for_ensemble models Sequence nn Module - tuple FunctionalModuleWithBuffers tuple Tensor tuple Tensor combine_state_for_ensemble models - func params buffers Prepares list torch nn Modules ensembling func ` vmap ` Given list ` ` M ` ` ` ` nn Modules ` ` same stacks all their parameters buffers together make ` ` params ` ` ` ` buffers ` ` Each parameter buffer result will have additional dimension size ` ` M ` ` func ` combine_state_for_ensemble ` also returns ` ` func ` ` functional version one models attr ` models ` One cannot directly run ` ` func params buffers args kwargs ` ` directly you probably want use ` ` vmap func params buffers args kwargs ` ` Here s example how ensemble over very simple model code-block python num_models = batch_size = in_features out_features = models = torch nn Linear in_features out_features i range num_models data = torch randn batch_size fmodel params buffers = combine_state_for_ensemble models output = vmap fmodel None params buffers data assert output shape == num_models batch_size out_features warning All modules being stacked together must same except values their parameters buffers For example they should same mode training vs eval This API subject change -- we re investigating better ways create ensembles would love your feedback how improve len models == raise RuntimeError combine_state_for_ensemble Expected least one model got all m training m models all m training m models raise RuntimeError combine_state_for_ensemble Expected all models have same training eval mode model _typ = type models all type m model _typ m models raise RuntimeError combine_state_for_ensemble Expected all models same funcs params buffers = zip make_functional_with_buffers model model models params = transpose_stack params buffers = transpose_stack buffers funcs params buffers functional_init model_class type nn Module ensemble_shape Union tuple tuple int = device torch types Device = cpu wrapped args kwargs len ensemble_shape = raise ValueError NYI ensemble_shape more than element len ensemble_shape == model = model_class args kwargs device make_functional_deprecated_v model num_models = ensemble_shape type ignore misc num_models = raise ValueError f num_models num_models should NB Not very efficient more POC models = tuple model_class args kwargs device _ range num_models _ fn names = make_functional_deprecated_v model_class args kwargs weights = tuple make_functional_deprecated_v model model models weights = tuple zip weights weights = tuple torch stack shards detach shards weights weights fn names wrapped functional_init_with_buffers model_class type nn Module ensemble_shape Union tuple tuple int = device torch types Device = cpu wrapped args kwargs len ensemble_shape = raise ValueError NYI ensemble_shape more than element len ensemble_shape == model = model_class args kwargs device make_functional_deprecated_v model num_models = ensemble_shape type ignore misc num_models = raise ValueError f num_models num_models should NB Not very efficient more POC models = tuple model_class args kwargs device _ range num_models _ _ fn weight_names buffer_names = make_functional_with_buffers_deprecated_v model_class args kwargs weights buffers = zip tuple make_functional_with_buffers_deprecated_v model model models weights = tuple zip weights weights = tuple torch stack shards detach shards weights buffers = tuple zip buffers buffers = tuple torch stack shards detach shards buffers weights buffers fn weight_names buffer_names wrapped