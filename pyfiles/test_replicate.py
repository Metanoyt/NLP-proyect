Owner s oncall distributed os unittest copy deepcopy torch torch distributed dist torch nn functional F torch nn torch distributed _composable replicate replicate torch distributed fsdp fully_shard torch distributed tensor DTensor torch testing _internal common_distributed MultiProcessTestCase skip_if_lt_x_gpu torch testing _internal common_utils run_tests TEST_XPU device_type = acc type acc = torch accelerator current_accelerator cpu device_module = torch get_device_module device_type Net nn Module __init__ - None super __init__ fc = nn Linear fc = nn Linear fc = nn Linear forward x fc fc fc x ReplicateStateDictTest MultiProcessTestCase setUp - None super setUp _spawn_processes tearDown super tearDown try os remove file_name except OSError pass _init_pg dist init_process_group backend= gloo rank=self rank world_size=self world_size store=dist FileStore file_name world_size _check_state_dict_parity sd_ sd_ k k zip sd_ keys sd_ keys assertEqual k k v v zip sd_ values sd_ values assertEqual v v test_replicate_single_module_save_load Tests replicate single module state_dict matches local module state_dict _init_pg model = Net replicate_model = replicate deepcopy model local_sd = model state_dict ddp_sd = replicate_model state_dict _check_state_dict_parity local_sd ddp_sd test_replicate_non_root_multiple_save_load Tests replicate multiple submodules matches local module state_dict _init_pg model = Net replicate_model = deepcopy model replicate replicate_model fc replicate replicate_model fc replicate replicate_model fc local_sd = model state_dict ddp_sd = replicate_model state_dict _check_state_dict_parity local_sd ddp_sd ReplicateTest MultiProcessTestCase property world_size - int setUp - None super setUp _spawn_processes tearDown super tearDown try os remove file_name except OSError pass _init_pg dist init_process_group backend= gloo rank=self rank world_size=self world_size store=dist FileStore file_name world_size _compare_module mod replicate_mod local_batch_size = global_batch_size = world_size local_batch_size input = torch randn global_batch_size target = torch randn global_batch_size step_model model input target model train output = model input loss = F mse_loss output target output device loss backward param model parameters torch no_grad param -= param grad param grad = None iteration range step_model mod input target step_model replicate_mod input rank local_batch_size rank + local_batch_size target rank local_batch_size rank + local_batch_size assertEqual len list mod parameters len list replicate_mod parameters i j zip mod parameters replicate_mod parameters assertEqual i j rtol= e- atol= e- Shuffle input so DDP input different torch manual_seed iteration input = input torch randperm global_batch_size test_replicate_single_module _init_pg model = Net replicate_model = replicate deepcopy model _compare_module model replicate_model skip_if_lt_x_gpu unittest skipIf TEST_XPU XPU does support gloo backend test_replicate_move_args_kwargs_to_device MyNet nn Module __init__ - None super __init__ = nn Linear forward inp kwarg=None kwarg None inp = inp kwarg inp _init_pg torch accelerator set_device_index rank model = MyNet device_type replicate model device_id=torch accelerator current_device_index CPU input ensures replicate can move arg kwargs device b = torch randn torch randn model kwarg=b sum backward skip_if_lt_x_gpu unittest skipIf TEST_XPU XPU does support gloo backend test_replicate_ignore_module _init_pg torch accelerator set_device_index rank Seed ensures diff input thus different local grads across ranks torch manual_seed rank device_module manual_seed rank model = Net device_type replicate model ignored_modules= model fc CPU input ensures replicate can move input GPU DDP does inp = torch randn device=device_type rank + out = model inp out sum backward FC grads should synchronized FC should fc _grad = model fc weight grad tensor_list = torch zeros_like fc _grad _ range dist get_world_size dist all_gather tensor_list fc _grad grad rest = tensor_list tensor_list g rest assertNotEqual grad g dp_grad model fc weight grad model fc weight grad tensor_list = torch zeros_like dp_grad _ range dist get_world_size dist all_gather tensor_list dp_grad grad rest = tensor_list tensor_list g rest assertEqual grad g test_replicate_multi_module _init_pg model = Net replicate_model = deepcopy model replicate replicate_model fc replicate replicate_model fc replicate replicate_model fc _compare_module model replicate_model test_replicate_with_kwargs _init_pg model = Net replicate_model = replicate deepcopy model bucket_cap_mb= gradient_as_bucket_view=True _compare_module model replicate_model skip_if_lt_x_gpu unittest skipIf TEST_XPU XPU does support gloo backend test_replicate_device_id _init_pg model = Net model_cuda = deepcopy model device_type model_cuda = deepcopy model_cuda replicate model device_id=torch device cpu DDP instance attached first pre forward model torch randn replicate_ddp_weakref = replicate state model _ddp_weakref Should None CPU training assertEqual None replicate_ddp_weakref device_ids replicate model_cuda device_id=torch device torch accelerator current_device_index DDP instance attached first pre forward model_cuda torch randn replicate_ddp_weakref = replicate state model_cuda _ddp_weakref assertEqual replicate_ddp_weakref device_ids Pass int device_id replicate model_cuda device_id=int torch accelerator current_device_index DDP instance attached first pre forward model_cuda torch randn replicate_ddp_weakref = replicate state model_cuda _ddp_weakref assertEqual replicate_ddp_weakref device_ids test_replicate_wrong_device_id_type _init_pg model = Net assertRaisesRegex RuntimeError Expected device_id int torch device replicate model device_id= torch device cpu ReplicateFullyShardInit ReplicateTest skip_if_lt_x_gpu unittest skipIf TEST_XPU XPU does support gloo backend test_replicate_fully_shard_init ToyModel nn Module __init__ dim int super __init__ linears = nn Sequential nn Linear dim dim bias=False nn Linear dim dim bias=False nn Linear dim dim bias=False proj = nn Linear dim dim bias=False forward x torch Tensor y = linears x y = proj y y _init_pg torch accelerator set_device_index rank dim = bz = model = ToyModel dim device_type linear model linears fully_shard linear fully_shard model linears replicate model device_id=torch accelerator current_device_index linear model linears assertTrue isinstance linear weight DTensor inp = torch rand bz dim trigger lazy init model inp sum linear model linears assertTrue isinstance linear weight DTensor __name__ == __main__ run_tests