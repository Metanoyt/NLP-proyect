mypy allow-untyped-defs This module one analysis modules - takes input function graph some preexisting properties returns some data useful deciding how further proceed compilation construct runtime wrappers In particular following analyses provided Refine view mutation metadata collected previously - removing duplicate inputs mapping views their bases We also analyze function signature export graphs contextlib itertools typing Any Optional Union torch torch utils _pytree pytree torch Tensor torch _C _dynamo guards compute_overlapping_tensors torch _functorch _aot_autograd schemas PlainTensorMeta torch _guards StorageOverlap torch _subclasses functional_tensor FunctionalTensor torch fx experimental symbolic_shapes is_concrete_int collect_metadata_analysis coerce_tangent_and_suggest_memory_format descriptors AOTInput InputMutationAOTOutput TangentAOTInput schemas BackwardSignature GraphSignature InputAliasInfo MemoryFormatMeta OutputAliasInfo OutputType ViewAndMutationMeta utils strict_zip zip = strict_zip remove_dupe_metadata m ViewAndMutationMeta keep_arg_mask list bool add_dupe_map list int - ViewAndMutationMeta assert len m input_info == len keep_arg_mask Easy invariant first argument should never dupe will kept assert len keep_arg_mask keep_arg_mask Filter dupe d mutated inputs out traced_tangents num_data_mutations = len x x m input_info x mutates_data other_traced_tangents = m traced_tangents num_data_mutations inp_traced_tangents = m traced_tangents num_data_mutations other_traced_tangents_descs = m traced_tangents_descs num_data_mutations inp_traced_tangents_descs = m traced_tangents_descs num_data_mutations filtered_inp_traced_tangents = See Note Tangents memory format x i x enumerate inp_traced_tangents keep_arg_mask m mutated_inp_runtime_indices i filtered_inp_traced_tangents_descs = x_desc i x_desc enumerate inp_traced_tangents_descs keep_arg_mask m mutated_inp_runtime_indices i traced_tangents = filtered_inp_traced_tangents + other_traced_tangents traced_tangents_descs = filtered_inp_traced_tangents_descs + other_traced_tangents_descs assert m subclass_tangent_meta None subclass_tangent_meta = PlainTensorMeta memory_format=MemoryFormatMeta memory_format=torch contiguous_format len filtered_inp_traced_tangents + m subclass_tangent_meta num_data_mutations ViewAndMutationMeta input_info= x i x enumerate m input_info keep_arg_mask i For outputs views inputs we store index input output generated Need update index account removed dupes output_info= OutputAliasInfo output_type=o output_type raw_type=o raw_type dynamic_dims=o dynamic_dims base_idx=None o base_idx None add_dupe_map o base_idx requires_grad=o requires_grad view_meta_sequence=o view_meta_sequence o m output_info num_intermediate_bases=m num_intermediate_bases keep_input_mutations=m keep_input_mutations traced_tangents=traced_tangents traced_tangents_descs=traced_tangents_descs We guaranteed get here since dupes supported today subclass inputs subclass_inp_meta= subclass_fw_graph_out_meta= subclass_tangent_meta=subclass_tangent_meta is_train=m is_train Given our ViewAndMutation metadata fn constructs new set metadata after adding synthetic base arguments function Most work fn slogging through all metadata corresponding inputs updating our synthetic base calling convention When config debug_assert set we automatically regenerate metadata compare output sanity In addition updated metadata also list input indices will need updated synthetic base epilogue create_synthetic_base_metadata m ViewAndMutationMeta Maps each outer argument idx its inner idx outer arg generated synthetic base you get tuple i TensorMeta telling you base tensor idx view metadata synthetic_base_info list Union int tuple int torch Tensor outer_args list Any inner_args list Any inner_args_desc list AOTInput - tuple ViewAndMutationMeta list int maps inner arg indices outer arg indices synthetic_base_to_indices dict int list int = inner_idx range len inner_args outer_aliased_indices_of_current_base_arg = outer_idx outer_idx inner_idx_or_tuple enumerate synthetic_base_info isinstance inner_idx_or_tuple int inner_idx_or_tuple == inner_idx isinstance inner_idx_or_tuple tuple inner_idx_or_tuple == inner_idx synthetic_base_to_indices inner_idx = outer_aliased_indices_of_current_base_arg given requires_grad info mutated inputs generate requires_grad info those same mutated inputs after constructing synthetic bases input_infos = outer_indices synthetic_base_to_indices values leaf-ness should all-or-nothing aliased tensor aka b views then is_leaf == b is_leaf any_leaf = any m input_info x is_leaf x outer_indices all_leaf = all m input_info x is_leaf x outer_indices assert any_leaf == all_leaf mutates_data = True len outer_indices m input_info outer_indices mutates_data mutates_metadata = False len outer_indices m input_info outer_indices mutates_metadata requires_grad = any m input_info x requires_grad x outer_indices mutations_under_no_grad_or_inference_mode = all m input_info x mutations_under_no_grad_or_inference_mode x outer_indices mutation_inductor_storage_resize = all m input_info x mutation_inductor_storage_resize x outer_indices inpt_info = InputAliasInfo If len outer_indices then input synthetic base The invariant rest aot autograd synthetic bases only show up one their aliases gets data mutation And any their aliases get metadata mutations they will hidden rest aot autograd mutates_data=mutates_data mutates_metadata=mutates_metadata mutations_hidden_from_autograd=all m input_info x mutations_hidden_from_autograd x outer_indices mutates_storage_metadata= False len outer_indices m input_info outer_indices mutates_storage_metadata mutations_under_no_grad_or_inference_mode=mutations_under_no_grad_or_inference_mode mutation_inductor_storage_resize=mutation_inductor_storage_resize is_leaf=any_leaf requires_grad=requires_grad keep_input_mutations=m keep_input_mutations input_infos append inpt_info Find any inputs fulfill following criteria They part synthetic base because they alias another input least one input experiences data mutation They experience metadata mutation outer_aliased_arg_idx_with_metadata_mutations = outer_idx outer_idx inpt_info enumerate m input_info inpt_info mutates_metadata isinstance synthetic_base_info outer_idx int grab original requires grad info outputs except ones mutated inputs input_metadata_output_info = OutputAliasInfo output_type=OutputType alias_of_input raw_type=FunctionalTensor dynamic_dims= i i s enumerate outer_args outer_idx shape is_concrete_int s base_idx=synthetic_base_info outer_idx type ignore index requires_grad=outer_args outer_idx requires_grad outer_idx outer_aliased_arg_idx_with_metadata_mutations existing_output_infos = o m output_info new_base_idx = None o base_idx None synthetic_base_info o base_idx isinstance synthetic_base_info o base_idx int synthetic_base_info o base_idx type ignore index If base_idx changed OutputType is_input we need update output type reflect change new_output_type = OutputType alias_of_input o output_type == OutputType is_input o base_idx = new_base_idx o output_type existing_output_infos append OutputAliasInfo output_type=new_output_type raw_type=o raw_type dynamic_dims=o dynamic_dims Map input idx pre-synthetic-bases new idx post-synthetic-bases base_idx=new_base_idx type ignore arg-type requires_grad=o requires_grad view_meta_sequence=o view_meta_sequence inner_mutated_tangents_and_memory_formats = See Note Tangents memory format coerce_tangent_and_suggest_memory_format x TangentAOTInput InputMutationAOTOutput x_desc inner_idx x x_desc enumerate zip inner_args inner_args_desc input_infos inner_idx mutates_data input_infos inner_idx requires_grad inner_mutated_tangents = x x inner_mutated_tangents_and_memory_formats inner_mutated_tangents_descs = x x inner_mutated_tangents_and_memory_formats inner_mutated_tangents_memory_formats = x x inner_mutated_tangents_and_memory_formats output_info = existing_output_infos + input_metadata_output_info Regenerate traced tangents include mutated inputs including synthetic bases traced_tangents = inner_mutated_tangents + m traced_tangents len inner_mutated_tangents traced_tangents_descs = inner_mutated_tangents_descs + m traced_tangents_descs len inner_mutated_tangents assert m subclass_tangent_meta None subclass_tangent_meta = PlainTensorMeta memory_format=x x inner_mutated_tangents_memory_formats + m subclass_tangent_meta len inner_mutated_tangents ViewAndMutationMeta input_info=input_infos output_info=output_info num_intermediate_bases=m num_intermediate_bases keep_input_mutations=m keep_input_mutations traced_tangents=traced_tangents traced_tangents_descs=traced_tangents_descs We guaranteed get here since synthetic_base codepaths supported today subclass inputs subclass_inp_meta= subclass_fw_graph_out_meta= subclass_tangent_meta=subclass_tangent_meta is_train=m is_train outer_aliased_arg_idx_with_metadata_mutations compute_overlapping_inputs aot_config fwd_inputs aliased_input_indices num_aliases = len aliased_input_indices shape_env = None maybe_suppress_guards = contextlib nullcontext tracing_context = torch _guards TracingContext try_get tracing_context None assert tracing_context fake_mode None shape_env = tracing_context fake_mode shape_env Check whether we can actually get dynamo sources within AOTAutograd aot_config aot_autograd_arg_pos_to_source shape_env None maybe_suppress_guards = shape_env suppress_guards type ignore assignment Check whether there any symbolic values being used We do reasons StorageOverlap guard only issued whenever dynamic shapes turned Triggers fast-path computing storage overlapping symbolic = any isinstance x torch SymInt i aliased_input_indices x fwd_inputs i shape fwd_inputs i stride fwd_inputs i storage_offset torch _inductor config is_fbcode symbolic num_aliases torch _subclasses fake_tensor UnsupportedMutationAliasingException torch _utils_internal justknobs_check msg = f Encountered num_aliases dynamic aliased mutated inputs consider setting dynamic=False justknobs_check pytorch compiler aliased_inputs_with_mutation_and_dyn_shapes_killswitch False raise UnsupportedMutationAliasingException msg maybe_suppress_guards aliased_fwd_inputs = fwd_inputs i i aliased_input_indices actual_aliased_indices = aliased_input_indices i i compute_overlapping_tensors aliased_fwd_inputs symbolic=symbolic Add StorageOverlap AOTAutograd guard only we actually keeping track dynamo sources inside AOTAutograd tracing_context None Make sure dynamic shapes currently being used symbolic We check we have more than aliased tensor which should true point anyway num_aliases aot_config aot_autograd_arg_pos_to_source no_overlap_indices = list set aliased_input_indices - actual_aliased_indices overlapping_sources = aot_config aot_autograd_arg_pos_to_source i i actual_aliased_indices non_overlapping_sources = aot_config aot_autograd_arg_pos_to_source i i no_overlap_indices tracing_context guards_context aotautograd_guards append StorageOverlap overlapping_sources non_overlapping_sources actual_aliased_indices _graph_input_names gm node name node gm graph find_nodes op= placeholder _graph_output_names gm output_node = next iter reversed gm graph nodes assert output_node op == output len output_node args == return_args = output_node args getattr return_arg name None return_arg return_args create_graph_signature fx_g torch fx GraphModule fw_metadata ViewAndMutationMeta in_spec pytree TreeSpec out_spec pytree TreeSpec user_args_flat list Tensor params_and_buffers_flat list Tensor param_names list str buffer_names list str trace_joint bool num_user_fw_outs Optional int loss_index Optional int - GraphSignature Retrieve graph input names graph_input_names = _graph_input_names fx_g Retrieve graph output names graph_output_names = _graph_output_names fx_g num_params_buffers = len param_names + len buffer_names num_tokens = len fw_metadata tokens We have enough restrictions graph no de-duping synthetic bases etc Such graph inps = user inps + params + buffers num_user_args = len graph_input_names - num_params_buffers - num_tokens trace_joint assert num_user_fw_outs None num_fw_outs = num_user_fw_outs + fw_metadata num_mutated_inp_runtime_indices backward_output_names = graph_output_names num_fw_outs grad_index = itertools count gradients_to_parameters = backward_output_names next grad_index param_names i i param enumerate params_and_buffers_flat param requires_grad gradients_to_user_inputs = backward_output_names next grad_index graph_input_names i + len params_and_buffers_flat i user_input enumerate user_args_flat user_input requires_grad assert len gradients_to_parameters + len gradients_to_user_inputs == len backward_output_names Check we have fully accounted all graph outputs backward_signature = BackwardSignature gradients_to_parameters gradients_to_user_inputs graph_output_names loss_index backward_signature = None num_user_fw_outs = len graph_output_names - fw_metadata num_mutated_inp_runtime_indices - num_tokens GraphSignature from_tracing_metadata in_spec=in_spec out_spec=out_spec graph_input_names=graph_input_names graph_output_names=graph_output_names view_mutation_metadata=fw_metadata named_parameters=param_names named_buffers=buffer_names num_user_inputs=num_user_args num_user_outputs=num_user_fw_outs trace_joint=trace_joint loss_index=loss_index backward_signature=backward_signature