Owner s module inductor ruff noqa F contextlib copy dataclasses functools gc importlib itertools math operator os random re subprocess sys threading time unittest unittest mock weakref collections abc Callable pathlib Path typing TypeVar typing_extensions ParamSpec unittest mock patch numpy np torch torch _dynamo config dynamo_config torch _inductor aoti_eager torch nn nn torch _C _dynamo guards assert_alignment assert_size_stride torch _dispatch python enable_python_dispatcher torch _dynamo debug_utils aot_graph_input_parser torch _dynamo device_interface get_interface_for_device torch _dynamo testing CompileCounterWithBackend expectedFailureCodegenDynamic rand_strided reset_rng_state same skipIfPy torch _dynamo utils ifdynstaticdefault torch _guards CompileContext CompileId torch _inductor lowering torch _inductor aoti_eager aoti_compile_with_persistent_cache aoti_eager_cache_dir load_aoti_eager_cache torch _inductor codegen common DataTypePropagation OptimizationContext torch _inductor fx_passes pad_mm torch _inductor test_case TestCase InductorTestCase torch _inductor utils add_scheduler_init_hook run_and_get_code run_and_get_cpp_code run_and_get_kernels run_and_get_triton_code run_fw_bw_and_get_code triton_version_uses_attrs_dict torch _inductor virtualized V torch _prims_common is_integer_dtype torch fx experimental proxy_tensor make_fx torch library _scoped_library torch nn functional F torch testing FileCheck make_tensor torch testing _internal common_cuda IS_SM PLATFORM_SUPPORTS_FLASH_ATTENTION PLATFORM_SUPPORTS_MEM_EFF_ATTENTION SM OrLater SM OrLater TEST_CUDNN tf _on_and_off with_tf _off torch testing _internal common_device_type expectedFailureXPU largeTensorTest torch testing _internal common_dtype all_types get_all_dtypes torch testing _internal common_quantization _dynamically_quantize_per_channel _group_quantize_tensor_symmetric torch testing _internal common_utils DeterministicGuard instantiate_parametrized_tests IS_FBCODE IS_MACOS IS_X MACOS_VERSION parametrize serialTest skipIfMPS skipIfRocm skipIfWindows skipIfXpu subtest TEST_WITH_ASAN TEST_WITH_ROCM xfailIfS X torch testing _internal logging_utils logs_to_string torch utils _pytree pytree torch utils _python_dispatch TorchDispatchMode torch utils _pytree tree_flatten tree_unflatten torch utils weak WeakTensorKeyDictionary DO_PERF_TEST = os environ get DO_PERF_TEST == importlib import_module functorch importlib import_module filelock torch _inductor config cpu_vec_isa test_operators torch _inductor compile_fx compile_fx compile_fx_inner complex_memory_overlap torch _inductor utils has_torchvision_roi_align torch testing _internal common_utils slowTest torch testing _internal inductor_utils clone_preserve_strides_offset GPU_TYPE HAS_CPU HAS_GPU HAS_MPS HAS_MULTIGPU IS_BIG_GPU requires_gpu RUN_CPU RUN_GPU skipCPUIf skipCUDAIf torch testing _internal triton_utils requires_cuda_and_triton _T = TypeVar _T _P = ParamSpec _P HAS_AVX = fbgemm torch backends quantized supported_engines TEST_WITH_ROCM torch _inductor config force_layout_optimization = os environ PYTORCH_MIOPEN_SUGGEST_NHWC = aten = torch ops aten requires_multigpu = functools partial unittest skipIf HAS_MULTIGPU f requires multiple GPU_TYPE devices skip_if_x _mac = functools partial unittest skipIf IS_MACOS IS_X Does work x Mac vec_dtypes = torch float torch bfloat torch float libtest = torch library Library test FRAGMENT noqa TOR ids = set f = torch float i = torch int i = torch int test_dtypes = torch float torch float torch float torch uint torch int torch int torch int torch int test_int_dtypes = torch uint torch int torch int torch int torch int SM OrLater MACOS_VERSION = test_dtypes append torch bfloat _large_cumprod_input shape dim dtype device Construct cumprod input which guarantees overflow underflow is_integer_dtype dtype Large products don t fit integers best we can do random + - values test sign result x = torch randint shape dtype=dtype device=device x - comp_dtype = torch _prims_common get_computation_dtype dtype batch_size = comp_dtype = dtype batch_size = math floor math log torch finfo dtype max Create random values uniform magnitude uniform exponent num_batches = shape dim + batch_size - batch_size batch_shape = shape dim + num_batches batch_size + shape dim + magnitude = + torch rand batch_shape dtype=comp_dtype device=device exponent = torch randint - batch_shape device=device comp_dtype batch = magnitude exponent exp Alternate each batch values their reciprocals so product never gets too far away t = torch cat batch batch reciprocal dim=dim + t = t flatten dim dim + t = aten slice t dim=dim start= end=shape dim Randomize sign sign = torch randint shape device=device - t sign dtype define_custom_op_for_test id_ fn fn_meta tags= global libtest global ids id_ ids libtest define f id_ Tensor - Tensor tags=tags libtest impl id_ fn CPU libtest impl id_ fn CUDA libtest impl id_ fn XPU libtest impl id_ fn MPS libtest impl id_ fn_meta Meta ids add id_ define_custom_op_ _for_test id_ fn fn_meta tags= global libtest global ids id_ ids libtest define f id_ Tensor float scale - Tensor Tensor tags=tags libtest impl id_ fn CPU libtest impl id_ fn CUDA libtest impl id_ fn XPU libtest impl id_ fn MPS libtest impl id_ fn_meta Meta ids add id_ define_custom_op_ _for_test id_ fn fn_meta tags= global libtest global ids id_ ids libtest define f id_ Tensor x - Tensor tags=tags libtest impl id_ fn CPU libtest impl id_ fn CUDA libtest impl id_ fn XPU libtest impl id_ fn MPS libtest impl id_ fn_meta Meta ids add id_ f = torch float register_ops_with_aoti_compile ns op_set dispatch_key torch_compile_op_lib_impl _op_name op_set qualified_op_name = f ns _op_name _ overload_names = torch _C _jit_get_operation qualified_op_name overload_name overload_names try reg_op_name = qualified_op_name schema = torch _C _get_schema qualified_op_name overload_name schema overload_name reg_op_name = f qualified_op_name schema overload_name torch_compile_op_lib_impl _impl_with_aoti_compile noqa F reg_op_name dispatch_key except Exception e continue get_divisible_by_ cfg attribute renamed between triton versions divisible_by_ divisibility_ hasattr cfg divisibility_ cfg divisibility_ hasattr cfg divisible_by_ cfg divisible_by_ ` cfg ` example tt divisibility tt divisibility tt divisibility key key value cfg items len key == value == tt divisibility get_post_grad_graph f inputs log_stream ctx = logs_to_string torch _inductor compile_fx post_grad_graphs ctx f inputs post_grad_graph = \n join log_stream getvalue strip split \n strip post_grad_graph TestCase InductorTestCase classmethod setUpClass cls super setUpClass cls _stack = contextlib ExitStack cls _stack enter_context config patch debug True debug_index_asserts True cpp min_chunk_size triton autotune_pointwise False too slow implicit_fallbacks False generate_intermediate_hooks True classmethod tearDownClass cls cls _stack close super tearDownClass setUp torch _dynamo reset torch _inductor metrics reset super setUp _start = time perf_counter tearDown super tearDown torch _dynamo reset os environ get ERROR_ON_SLOW == elapsed = time perf_counter - _start assert elapsed ToTuple torch nn Module forward x x dataclasses dataclass InputGen n int device str dense torch randn n n device=self device transposed dense transpose strided torch randn n n device=self device n n broadcast torch randn n device=self device broadcast torch randn n device=self device broadcast torch randn device=self device double device == mps raise unittest SkipTest MPS does support torch float torch randn n n device=self device dtype=torch double int torch arange n device=self device dtype=torch int compute_grads args kwrags results grads gather_leaf_tensors args kwargs args = pytree arg_tree_leaves args kwargs leaf_tensors = arg arg args isinstance arg torch Tensor arg requires_grad leaf_tensors flat_results = pytree tree_leaves results flat_diff_results = r r flat_results isinstance r torch Tensor r requires_grad assert len flat_diff_results leaf_tensors = gather_leaf_tensors args kwrags assert len leaf_tensors torch autograd grad flat_diff_results leaf_tensors grads allow_unused=True retain_graph=True check_model TestCase model example_inputs kwargs=None atol=None rtol=None grad_atol=None grad_rtol=None check_lowp=True exact_dtype=True nopython=True copy_to_gpu=True reference_in_float=True assert_equal=True check_gradient=False check_has_compiled=True output_process_fn_grad=lambda x x TODO enable all tests exact_stride=False kwargs = kwargs torch _dynamo reset ref_inputs = clone_preserve_strides_offset x x example_inputs ref_kwargs = kwargs has_lowp_args = False reference_in_float exact_dtype Store expected dtypes so we can check actual result gives correct types torch manual_seed try eager_result = model ref_inputs ref_kwargs except RuntimeError Eager model may fail dtype supported eager_result = None ref_inputs = clone_preserve_strides_offset x x example_inputs expect_dtypes = x dtype isinstance x torch Tensor None x pytree tree_leaves eager_result del eager_result ref_model = model reference_in_float check_lowp ignored here s kept just able call ` common ` extra arg upcast_fn x nonlocal has_lowp_args isinstance x torch Tensor x dtype == torch float x dtype == torch bfloat has_lowp_args = True Preserve strides when casting result = torch empty_strided x size x stride device=x device dtype=torch float result copy_ x result x We previously call upcast_fn example_inputs It s incorrect example_inputs already fp get inplace updated model Call cloned tensors instead ref_inputs = list map upcast_fn ref_inputs ref_kwargs = k upcast_fn v k v kwargs items has_lowp_args hasattr model ref_model = copy deepcopy model torch float torch manual_seed correct = ref_model ref_inputs ref_kwargs torch _inductor metrics reset called = False compile_fx_wrapper model_ example_inputs_ nonlocal called called = True compile_fx model_ example_inputs_ run ex kwargs model ex kwargs run = torch compile run backend=compile_fx_wrapper fullgraph=nopython torch manual_seed actual = run example_inputs kwargs called exp = torch _dynamo explain run example_inputs print Explain exp graph exp print Graph graph check_has_compiled assert called Ran graph without calling compile_fx assert type actual type correct isinstance actual tuple list assert len actual == len correct assert all type actual_item type correct_item actual_item correct_item zip actual correct correct_flat correct_spec = tree_flatten correct actual_flat = pytree tree_leaves actual reference_to_expect actual_flat correct_flat tuple y x dtype isinstance y torch Tensor y dtype is_floating_point y x y zip actual_flat correct_flat reference_in_float exact_dtype expect_dtype actual_result zip expect_dtypes actual_flat expect_dtype None assert actual_result dtype == expect_dtype f dtype mismatch expected expect_dtype got actual_result dtype reference_in_float correct_flat = reference_to_expect actual_flat correct_flat correct = tree_unflatten correct_flat correct_spec Allow assert_equal custom function instead True False cases where differences may indicate incorrectness assert_equal callable assert_equal custom_assert_with_self args kwargs assert_equal args kwargs assert_equal_fn = custom_assert_with_self assert_equal_fn = assertEqual assert_equal_fn actual correct atol=atol rtol=rtol equal_nan=True exact_dtype=exact_dtype exact_stride=exact_stride In case input mutations check inputs same This never uses custom assert_equal fn assertEqual ref_inputs example_inputs atol=atol rtol=rtol equal_nan=True our testing sometimes uses higher precision inputs reference exact_dtype=False exact_stride=exact_stride correct_val actual_val zip correct_flat actual_flat isinstance correct_val torch Tensor assert correct_val device == actual_val device assert correct_val size == actual_val size strides_equal _ = torch _prims_common check_significant_strides correct_val actual_val assert strides_equal assert correct_val layout == actual_val layout exact_dtype assert correct_val dtype == actual_val dtype exact_stride assert correct_val stride == actual_val stride check_gradient actual = output_process_fn_grad actual correct = output_process_fn_grad correct actual_flat = pytree tree_leaves actual correct_flat = pytree tree_leaves correct generate random unit norm gradients grads = torch randn_like r r correct_flat isinstance r torch Tensor r requires_grad g grads g = g norm correct_grad = compute_grads ref_inputs ref_kwargs correct grads all_none_grads = all x None x correct_grad tensor_args = x x pytree tree_flatten example_inputs isinstance x torch Tensor any_non_leaves = any x grad_fn None x tensor_args all_none_grads any_non_leaves See Note Detaching inputs never need gradients There handful ops can None gradients into zero gradients If all inputs AOTAutograd graph supposed get None gradients AOTAutograd will end up forcing all outputs forward require grad There s no easy fix see note above although one option force any derivative formulas core tensors zeros instead None flat_results = pytree tree_leaves actual results_that_require_grad = x x flat_results isinstance x torch Tensor x requires_grad assertEqual len results_that_require_grad actual_grad = compute_grads example_inputs kwargs actual grads reference_in_float expect_grad = reference_to_expect actual_grad correct_grad expect_grad = correct_grad assertEqual actual_grad expect_grad atol=grad_atol atol rtol=grad_rtol rtol equal_nan=True exact_dtype=exact_dtype exact_stride=exact_stride torch _dynamo reset torch _inductor config patch triton cudagraphs False check_model_gpu TestCase model example_inputs kwargs=None atol=None rtol=None grad_atol=None grad_rtol=None check_lowp=True exact_dtype=True nopython=True copy_to_gpu=True reference_in_float=True assert_equal=True check_gradient=False check_has_compiled=True output_process_fn_grad=lambda x x TODO enable all tests exact_stride=False kwargs = kwargs hasattr model model = model device=GPU_TYPE copy_to_gpu example_inputs = tuple clone_preserve_strides_offset x device=GPU_TYPE x example_inputs check_model model example_inputs kwargs atol=atol rtol=rtol grad_atol=grad_atol grad_rtol=grad_rtol exact_dtype=exact_dtype nopython=nopython reference_in_float=reference_in_float assert_equal=assert_equal check_gradient=check_gradient check_has_compiled=check_has_compiled output_process_fn_grad=output_process_fn_grad exact_stride=exact_stride check_lowp downcast_fn x isinstance x torch Tensor x dtype = torch float x torch empty_strided x size x stride device=GPU_TYPE dtype=torch half copy_ x example_inputs = list map downcast_fn example_inputs hasattr model model = model torch half rtol None rtol = max e- rtol check_model model example_inputs kwargs atol=atol rtol=rtol grad_atol=grad_atol grad_rtol=grad_rtol exact_dtype=exact_dtype nopython=nopython reference_in_float=reference_in_float assert_equal=assert_equal check_gradient=check_gradient check_has_compiled=check_has_compiled output_process_fn_grad=output_process_fn_grad exact_stride=exact_stride check_model_cuda = check_model_gpu _run_and_assert_no_indirect_indexing test_case func args has_wrapping=None has_assert=False kwargs result source_codes = run_and_get_code func args kwargs code source_codes line code split \n stmt = None Find indexing expressions load line stmt = line split load - tl store line stmt = line split store - stmt = join stmt split - Remove store value mask store line stmt = line split store - line stmt = line split - split tl make_block_ptr line continue stmt None continue indirect indexing involves ` tmp ` variable test_case assertTrue tmp stmt msg=f Found indirect indexing statement stmt code \n code has_wrapping None test_case assertTrue where code code has_wrapping msg=f Wanted has_wrapping= got\n code test_case assertTrue any device_assert code TORCH_CHECK code has_assert code source_codes result assertGeneratedKernelCountEqual TestCase expected int config triton multi_kernel when multi_kernel enabled we generated both persistent reduction non-persistent reduction kernels same node schedule That will mess up kernel count Just don t check assertEqual torch _inductor metrics generated_kernel_count expected SweepInputs input_gen_types = dense transposed strided broadcast broadcast broadcast double int input_gen_types = input_gen_types gen = None staticmethod kernel b + b classmethod gen_template cls name name test check_model cls kernel getattr cls gen name getattr cls gen name test __name__ = f test_ cls gen device _ name _ name setattr cls test __name__ test classmethod populate cls name cls input_gen_types name cls input_gen_types cls gen_template name name is_cpp_backend device getattr device type device == cpu config cpu_backend == cpp skip_if_cpu fn functools wraps fn wrapper args kwargs device == cpu raise unittest SkipTest cpu supported fn args kwargs wrapper skip_if_halide fn functools wraps fn wrapper args kwargs is_halide_backend device raise unittest SkipTest halide supported fn args kwargs wrapper xfail_if_mps fn functools wraps fn wrapper args kwargs is_mps_backend device fn args kwargs assertRaises Exception fn args kwargs wrapper Just alias track failures due missing eager ops xfail_if_mps_unimplemented = xfail_if_mps skip_if_triton fn functools wraps fn wrapper args kwargs is_triton_backend device raise unittest SkipTest triton supported fn args kwargs wrapper skip_if_not_triton fn functools wraps fn wrapper args kwargs is_triton_backend device raise unittest SkipTest f triton backend required device fn args kwargs wrapper skip_if_dynamic fn functools wraps fn wrapper args kwargs ifdynstaticdefault True False raise unittest SkipTest associtaive_scan doesn s support lifted SymInts fn args kwargs wrapper is_halide_backend device getattr device type device == cpu config cpu_backend == halide config cuda_backend == halide is_mps_backend device getattr device type device == mps is_triton_backend device device_type = getattr device type device device_type == cpu config cpu_backend == triton device_type == mps False config cuda_backend == triton is_triton_cpu_backend device getattr device type device == cpu config cpu_backend == triton skip_if_triton_cpu fn types reason = Triton CPU supported decorator fn functools wraps fn wrapper args kwargs is_triton_cpu_backend device raise unittest SkipTest reason fn args kwargs wrapper isinstance fn types FunctionType decorator fn reason = fn decorator xfail_if_triton_cpu fn fn _expected_failure_triton_cpu = True fn skip_if_gpu_halide fn functools wraps fn wrapper args kwargs is_halide_backend device getattr device type device == cuda raise unittest SkipTest halide supported fn args kwargs wrapper skip_if_cpp_wrapper __init__ reason str = - None reason = reason __call__ fn args kwargs functools wraps fn wrapper test_self config cpp_wrapper raise unittest SkipTest f cpp wrapper bug fixed reason fn test_self args kwargs wrapper is_dynamic_shape_enabled What s best way decide torch _dynamo config assume_static_by_default instantiate_parametrized_tests CommonTemplate is_dtype_supported dtype torch dtype - bool device_interface = get_interface_for_device device device_interface is_dtype_supported dtype test_bool fn b + b b b &#124; b ^ b torch logical_and b torch logical_or b torch logical_not torch sign b common fn torch tensor True False True False torch tensor False False True True skipCUDAIf SM OrLater Requires sm skip_if_halide aoti skip_if_triton_cpu aoti skipIfWindows msg= aoti support Windows test_aoti_eager_dtype_device_layout ns = aten op_name = tril_indices dispatch_key = CPU device = cpu device lower == cuda dispatch_key = CUDA device = cuda _scoped_library aten IMPL torch_compile_op_lib_impl row = col = offset = dtype = torch int layout = torch strided pin_memory = False ref = torch tril_indices row=row col=col offset=offset dtype=dtype layout=layout pin_memory=pin_memory device=device register_ops_with_aoti_compile ns op_name dispatch_key torch_compile_op_lib_impl res = torch tril_indices row=row col=col offset=offset dtype=dtype layout=layout pin_memory=pin_memory device=device assertEqual ref res skipCUDAIf SM OrLater Requires sm skip_if_halide aoti skip_if_triton_cpu aoti skipIfWindows msg= aoti support Windows test_aoti_eager_support_out ns = aten op_name = clamp dispatch_key = CPU device = cpu device lower == cuda dispatch_key = CUDA device = cuda inp_tensor = torch randn dtype=torch float device=device fill_ min_tensor = inp_tensor - max_tensor = inp_tensor + _scoped_library aten IMPL torch_compile_op_lib_impl ref_out_tensor = torch randn dtype=torch float device=device fill_ - ref_tensor = torch clamp max=max_tensor min=min_tensor input=inp_tensor out=ref_out_tensor ref_out_tensor = torch randn dtype=torch float device=device fill_ - ref_tensor = torch clamp max=max_tensor out=ref_out_tensor min=min_tensor input=inp_tensor register_ops_with_aoti_compile ns op_name dispatch_key torch_compile_op_lib_impl res_out_tensor = torch randn dtype=torch float device=device fill_ - res_tensor = torch clamp max=max_tensor min=min_tensor input=inp_tensor out=res_out_tensor assertEqual ref_tensor res_tensor assertEqual ref_out_tensor res_out_tensor res_out_tensor = torch randn dtype=torch float device=device fill_ - res_tensor = torch clamp max=max_tensor out=res_out_tensor min=min_tensor input=inp_tensor assertEqual ref_tensor res_tensor assertEqual ref_out_tensor res_out_tensor skipCUDAIf SM OrLater Requires sm skip_if_halide aoti skip_if_triton_cpu aoti skipIfWindows msg= aoti support Windows test_aoti_eager_support_str ns = aten op_name = div dispatch_key = CPU device = cpu device lower == cuda dispatch_key = CUDA device = cuda = torch randn dtype=torch float device=device b = torch randn dtype=torch float device=device rounding_mode_list = trunc floor _scoped_library aten IMPL torch_compile_op_lib_impl Get ref result eager ref_value_list = rounding_mode rounding_mode_list ref_value = getattr torch ops aten op_name b rounding_mode=rounding_mode ref_value_list append ref_value register_ops_with_aoti_compile ns op_name dispatch_key torch_compile_op_lib_impl Invoke pre-compiled kernel get result res_value_list = rounding_mode rounding_mode_list res_value = getattr torch ops aten op_name b rounding_mode=rounding_mode res_value_list append res_value ref_value res_value zip ref_value_list res_value_list assertEqual ref_value res_value skipCUDAIf SM OrLater Requires sm skip_if_halide aoti skip_if_triton_cpu aoti skipIfWindows msg= aoti support Windows test_aoti_eager_cache_hit ns = aten op_name = abs dispatch_key = CPU device = cpu device lower == cuda dispatch_key = CUDA device = cuda input_tensor = torch randn dtype=torch float device=device kernel_lib_path = aoti_compile_with_persistent_cache ns op_name device False getattr torch ops aten op_name input_tensor assertTrue Path kernel_lib_path exists unittest mock Patch aoti_compile_with_persistent_cache None ensure no new kernel generated mock patch torch _inductor aoti_eager aoti_compile_with_persistent_cache None _scoped_library aten IMPL torch_compile_op_lib_impl Get ref result eager ref_value = getattr torch ops aten op_name input_tensor register_ops_with_aoti_compile ns op_name dispatch_key torch_compile_op_lib_impl Invoke pre-compiled kernel get result res_value = getattr torch ops aten op_name input_tensor assertEqual ref_value res_value skipCUDAIf SM OrLater Requires sm skip_if_halide aoti skip_if_triton_cpu aoti skipIfWindows msg= aoti support Windows test_aoti_eager_with_persistent_cache fn torch abs ns = aten op_name = abs device = cpu device lower == cuda device = cuda input_tensor = torch randn dtype=torch float device=device kernel_lib_path = aoti_compile_with_persistent_cache ns op_name input_tensor device type False fn args= input_tensor kwargs= assertTrue len kernel_lib_path device_kernel_cache = aoti_eager_cache_dir ns device kernel_conf = device_kernel_cache f op_name json assertTrue kernel_conf exists json_data = load_aoti_eager_cache aten abs input_tensor device type assertTrue json_data None assertTrue isinstance json_data list assertTrue len json_data op_info = json_data assertTrue isinstance op_info dict assertTrue meta_info op_info assertTrue kernel_path op_info kernel_libs_abs_path = item json_data kernel_path = device_kernel_cache item kernel_path kernel_libs_abs_path append kernel_path as_posix assertTrue kernel_lib_path kernel_libs_abs_path skipCUDAIf SM OrLater Requires sm skip_if_halide aoti skip_if_triton_cpu aoti skipIfWindows msg= aoti support Windows test_aoti_eager_with_scalar namespace_name = aten op_name = add op_overload_name = Tensor op_name_with_overload = f op_name op_overload_name dispatch_key = CPU device = torch device cpu device lower == cuda dispatch_key = CUDA device = torch device cuda Test difference between scalar tensor scalar = torch scalar_tensor device=device b = torch scalar_tensor device=device kernel_lib_path = aoti_compile_with_persistent_cache namespace_name op_name_with_overload device type False torch ops aten add args= b kwargs= alpha assertTrue Path kernel_lib_path exists device_kernel_cache = aoti_eager_cache_dir namespace_name device type kernel_conf = device_kernel_cache f op_name_with_overload json assertTrue kernel_conf exists json_data = load_aoti_eager_cache namespace_name op_name_with_overload device type op_info = json_data assertTrue isinstance op_info dict assertTrue meta_info op_info assertTrue len op_info meta_info == Scalar Tensor assertTrue scalar_value op_info meta_info assertTrue op_info meta_info sizes == assertTrue op_info meta_info strides == Scalar Tensor assertTrue scalar_value op_info meta_info assertTrue op_info meta_info sizes == assertTrue op_info meta_info strides == Scalar assertTrue scalar_value op_info meta_info assertTrue sizes op_info meta_info assertTrue strides op_info meta_info _scoped_library aten IMPL torch_compile_op_lib_impl = torch randn device=device b = torch randn device=device scalar_values = ref_values = scalar_value scalar_values ref_values append torch add b alpha=scalar_value register_ops_with_aoti_compile namespace_name op_name dispatch_key torch_compile_op_lib_impl res_values = scalar_value scalar_values res_values append torch add b alpha=scalar_value assertEqual len ref_values len res_values assertEqual ref_values res_values skipCUDAIf SM OrLater Requires sm skip_if_halide aoti skip_if_triton_cpu aoti skipIfWindows msg= aoti support Windows test_aoti_eager_override_registration namespace_name = aten dispatch_key = CPU device = torch device cpu device lower == cuda dispatch_key = CUDA device = torch device cuda unary_op_set = abs acos fn x op_name= getattr torch op_name x Invoke torch compile directly get referent results x = torch randn device=device ref_array = unary_op_name unary_op_set opt_fn = torch compile functools partial fn op_name=unary_op_name ref = opt_fn x ref_array append ref _scoped_library aten IMPL torch_compile_op_lib_impl register_ops_with_aoti_compile namespace_name unary_op_set dispatch_key torch_compile_op_lib_impl res_array = unary_op_name unary_op_set res_array append getattr torch unary_op_name x ref res zip ref_array res_array assertEqual ref res = torch randn device=device min_tensor = torch randn device=device max_tensor = min_tensor + ref_with_min = torch ops aten clamp min_tensor ref_with_min_max = torch ops aten clamp min_tensor max_tensor _scoped_library aten IMPL torch_compile_op_lib_impl register_ops_with_aoti_compile namespace_name clamp dispatch_key torch_compile_op_lib_impl res_with_min = torch ops aten clamp min_tensor res_with_min_max = torch ops aten clamp min_tensor max_tensor assertEqual ref_with_min res_with_min assertEqual ref_with_min_max res_with_min_max test_add_const_int fn + torch add alpha= dtype torch float torch int torch int common fn torch arange dtype=dtype test_add_const_float fn + common fn torch randn test_add_inplace_permuted config cpu_backend == halide raise unittest SkipTest Halide cpu backend does work test case https github com pytorch pytorch issues fn x y x add_ y x = torch ones transpose y = torch randn common fn x y test_add_complex fn b alpha torch add b alpha=alpha x = torch tensor + j - + j - + j - j j - y = torch tensor + j - + j - + j - j j - common fn x y test_add_complex fix https github com pytorch pytorch issues torch compile fn args = torch neg args b = torch add args args b x = torch randn dtype=torch complex device=self device y = x clone should inplace write input fn x assertEqual x y test_add_complex torch compile fn b c = + b d = + b c + d dtype torch complex torch complex torch complex is_dtype_supported dtype continue x = torch tensor + j - + j - + j - j j - dtype=dtype device=self device y = torch tensor + j - + j - + j - j j - dtype=dtype device=self device _ code = run_and_get_code fn x y code = join code assert_keywords = assert_size_stride assert_alignment filtered_lines = line line code splitlines any assert_key line assert_key assert_keywords code = \n join filtered_lines assertGreaterEqual code count view_dtype config cpp_wrapper aten view test_add_complex_strided_fallback torch compile fn b + b is_dtype_supported torch complex raise unittest SkipTest complex supported device base = torch randn dtype=torch complex device=self device x = base transpose y = base transpose torch _inductor metrics reset _ code = run_and_get_code fn x y assertEqual torch _inductor metrics generated_kernel_count code = join code fallback_markers = extern_kernels add torch ops aten add Tensor config cpp_wrapper fallback_markers extend aoti_torch_cuda_add_Tensor aoti_torch_cpu_add_Tensor assertTrue any code count marker = marker fallback_markers msg=f Expected complex add strided inputs fall back extern kernels got \n code test_add_complex fn b alpha torch add b alpha=alpha x = torch tensor + j - + j - + j - j y = torch tensor + j - + j - + j - j common fn x y test_add_complex Fix https github com pytorch pytorch issues Add complex tensors broadcasting fn b alpha torch add b alpha=alpha x = torch tensor + j - + j - + j - j y = torch tensor + j common fn x y test_add_complex Fix https github com pytorch pytorch issues Test scalar -dimensional complex tensor addition D + D fn b alpha torch add b alpha=alpha x = torch rand dtype=torch complex device=self device y = torch rand dtype=torch complex device=self device common fn x y test_add_complex Fix https github com pytorch pytorch issues Test scalar complex addition D + D fn b alpha torch add b alpha=alpha x = torch rand dtype=torch complex device=self device y = torch rand dtype=torch complex device=self device common fn x y test_add_complex Fix https github com pytorch pytorch issues Test scalar complex addition D + D fn b alpha torch add b alpha=alpha x = torch rand dtype=torch complex device=self device y = torch rand dtype=torch complex device=self device common fn x y test_add_complex Fix https github com pytorch pytorch issues Test scalar complex broadcasting fn b alpha torch add b alpha=alpha x = torch randn dtype=torch complex device=self device y = torch rand dtype=torch complex device=self device common fn x y test_concat_add_inplace fn x y z torch cat x y dim= add_ z x = torch randn y = torch randn z = torch randn common fn x y z test_abs fn torch abs + common fn torch randn xfail_if_triton_cpu test_angle fn b c torch angle torch angle b torch angle c complex_input = torch tensor + j - + j - + j - j j - float nan real_input = torch tensor - float nan interger_real_input = torch tensor - common fn complex_input real_input interger_real_input test_sgn fn torch sgn torch sgn + - common fn torch linspace - skipCUDAIf SM OrLater uses bfloat which requires SM = test_scatter_bf fn inp src index inp scatter_add index src dtype torch int torch bool torch bfloat is_dtype_supported dtype continue common fn torch zeros dtype=dtype torch ones dtype=dtype torch tensor test_randn_generator fn generator torch randn generator=generator device=a device common fn torch linspace - None assert_equal=False generator yet supported dynamo assertRaisesRegex torch _dynamo exc Unsupported Generator common fn torch linspace - torch Generator device test_sgn_extremal fn torch sgn common fn torch tensor np nan np inf -np inf test_max_min fn b torch maximum b torch minimum b common fn torch randn torch randn t = torch randn t = float nan t = torch randn t = float nan common fn t t test_neg_max_uint https github com pytorch pytorch issues fn b c = torch neg torch maximum b c = torch randint dtype=torch uint b = torch randint dtype=torch uint common fn b test_compar fn x x gt x ge x eq x le x lt x ne = torch tensor common fn test_horizonal_fusion fn b c + b - c b c common fn torch randn torch randn torch randn test_horizonal_fusion fn b c + b + c + common fn torch randn torch randn torch randn test_vertical_fusion fn sa ct p From torchbench pyhpc_equation_of_state v = - e- v = - e- v = - e- v = e- t = v ct t = v + ct v + t + v sa t = t t = t p t + t common fn torch randn torch randn torch randn assertGeneratedKernelCountEqual config patch fx_graph_cache False skipIfWindows msg= torch _dynamo exc Unsupported test_forced_buffer_realize Test torch _test_inductor_realize forces buffer realized fn b = test_operators realize b common fn torch randn assertEqual torch _inductor metrics ir_nodes_pre_fusion config patch fx_graph_cache False skipIfWindows msg= torch _dynamo exc Unsupported test_scheduler_vertical_fusion realize = test_operators realize fn sa ct p From torchbench pyhpc_equation_of_state v = - e- v = - e- v = - e- v = e- t = realize v ct t = realize v + ct v + t + v sa t = realize t t = realize t p t + t common fn torch randn torch randn torch randn assertEqual torch _inductor metrics ir_nodes_pre_fusion assertGeneratedKernelCountEqual is_cpp_backend device test_index_propagation copy x i = torch arange x size device=x device x i x = torch randn device=self device copy_opt = torch compile copy backend= inductor expect = copy x actual = _run_and_assert_no_indirect_indexing copy_opt x assertEqual expect actual dynamo_config patch capture_dynamic_output_shape_ops True https github com halide Halide issues config patch halide scheduler_cpu Mullapudi config patch halide scheduler_cuda Li config patch implicit_fallbacks=True test_index_propagation_nested_indirect_indexing nested x repeats rank = torch arange repeats numel device=x device index = rank repeat_interleave repeats dim= torch index_select x index=index dim= example_inputs = torch randn device=self device repeats = torch tensor device=self device torch _dynamo mark_dynamic repeats create backed symint nested_opt = torch compile nested backend= inductor expect = nested example_inputs actual = nested_opt example_inputs assertEqual expect actual test_index_propagation_flip flip x i = torch arange x size - - - device=x device x i x = torch randn device=self device flip_opt = torch compile flip backend= inductor expect = flip x actual = _run_and_assert_no_indirect_indexing flip_opt x assertEqual expect actual test_index_propagation_floordiv repeat_interleave x n e g x= n= = returns i = torch arange x shape n device=x device x i n x = torch randn device=self device repeat_interleave_opt = torch compile repeat_interleave backend= inductor With static shapes we can prove bound our dynamic shapes reasoning good enough has_assert = ifdynstaticdefault False True should collapsed direct indexing actual = _run_and_assert_no_indirect_indexing repeat_interleave_opt x has_assert=has_assert expect = torch repeat_interleave x dim= assertEqual expect actual assertEqual actual repeat_interleave x test_index_propagation_remainder repeat x n e g x= n= = returns i = torch arange x shape n device=x device x i x shape x = torch randn device=self device repeat_opt = torch compile repeat backend= inductor With static shapes we can prove bound our dynamic shapes reasoning good enough has_assert = ifdynstaticdefault False True should collapsed direct indexing actual = _run_and_assert_no_indirect_indexing repeat_opt x has_wrapping=False has_assert=has_assert expect = x repeat assertEqual expect actual assertEqual actual repeat x test_index_propagation_abs reflection_pad_left x n e g x= n= = returns i = torch arange x shape + n device=x device x i - n abs x = torch randn device=self device opt_fn = torch compile reflection_pad_left backend= inductor With static shapes we can prove bound our dynamic shapes reasoning good enough has_assert = ifdynstaticdefault False True should collapsed direct indexing actual = _run_and_assert_no_indirect_indexing opt_fn x has_wrapping=False has_assert=has_assert expect = reflection_pad_left x assertEqual expect actual test_index_propagation_device_assert_masked fn idx = torch arange size device=a device padded_idx = torch constant_pad_nd idx padded_idx = torch where padded_idx = padded_idx padded_idx padded_idx common fn torch randn test_index_remainder fn x y x y common fn torch rand torch randint xfailIfS X config patch debug_index_asserts=False config patch cpp enable_tiling_heuristics False test_neg_index test fn inps has_assert bool has_wrapping bool vectorize bool = True fn_opt = torch compile fn is_halide_backend device pass no device asserts halide TODO remove once https github com pytorch pytorch issues fixed is_mps_backend device pass no device asserts MPS device == cpu is_triton_cpu_backend device _ code = run_and_get_cpp_code fn_opt inps assertTrue TORCH_CHECK code has_assert cpu_vec_isa valid_vec_isa_list os getenv ATEN_CPU_CAPABILITY = default assertTrue code blendv code has_wrapping Assert we always vectorize kernel regardless wrapping checks assertTrue loadu code vectorize code = run_and_get_triton_code fn_opt inps assertTrue tl where code has_wrapping assertTrue device_assert code has_assert indirect b b - = torch rand device=self device b = torch zeros dtype=torch long device=self device test indirect b has_assert=True has_wrapping=True direct x x - = torch rand device=self device Does even generate kernel s view test direct has_assert=False has_wrapping=False vectorize=False flip b b = torch rand device=self device b = torch arange start=- end=-a numel - step=- device=self device test flip b has_assert=True has_wrapping=True Constant propagate constant s negative flip_with_index_constant b = torch arange start=- end=-a numel - step=- device=a device b Wrapping constant-folded test flip_with_index_constant has_assert=False has_wrapping=False Operation where we can t prove index always positive negative pos_and_neg b = torch arange start= end=-a numel - step=- device=a device b It has wrapping no assert test pos_and_neg has_assert=False has_wrapping=True We currently don t do constant propagation float constants We cannot prove kind asserts just bounds We would need lift IndexPropagation shape_env accessible all Inductor flip_with_index b = torch arange start=- end=-a numel - step=- device=a device b = b int b test flip_with_index has_assert=ifdynstaticdefault False True has_wrapping=False vectorize=True unsafe_index b aten _unsafe_index b test unsafe_index b has_assert=False has_wrapping=True constant_propagation b = torch tensor device=a device b test constant_propagation has_assert=ifdynstaticdefault False True has_wrapping=False vectorize=False There s no loop vectorize constant_propagation_neg b = torch tensor - device=a device b In symbolic shapes we know we can access - so no assert necessary test constant_propagation_neg has_assert=False has_wrapping=False vectorize=False There s no loop vectorize test_computed_buffer_inlining flip x idx = torch arange x size - - - device=x device x idx idx flip_opt = torch compile flip backend= inductor x = torch randn device=self device expect = flip x actual = _run_and_assert_no_indirect_indexing flip_opt x assertEqual expect actual test__unsafe_masked_index fn mask idx aten _unsafe_masked_index mask idx common fn torch randn device=self device torch tensor True False True device=self device torch tensor device=self device test__unsafe_masked_index_put_accumulate fn mask idx values aten _unsafe_masked_index_put_accumulate mask idx values common fn torch randn device=self device torch tensor True False True device=self device torch tensor device=self device torch randn device=self device test_sum fn b + b sum - common fn torch randn torch randn test_sum fn b + b sum + b sum - common fn torch randn torch randn test_sum fn b r = + b r = r sum - r = torch squeeze b + r r r Mismatched elements Greatest absolute difference index up e- allowed Greatest relative difference index up allowed common fn torch randn torch randn atol= e- rtol= e- test_sum fn b = + c = b sum - d = c + e = d sum - f = e + f e d c b common fn torch randn test_sum fn b = + c = b sum - d = c + e = d sum - f = e + f common fn torch randn test_reduction fn sum max min argmax argmin common fn torch tensor float -inf float inf skip_if_x _mac test_reduction fn FIXME argmax sum max min argmin common fn torch full float inf skip_if_x _mac test_reduction fn FIXME argmin sum max min argmax common fn torch full float -inf test_reduction device == cpu raise unittest SkipTest Non-deterministic CPU results fn argmax - argmin - inputs = torch ones torch ones i inputs common fn i check_lowp=not is_halide_backend device config patch unroll_reductions_threshold= test_reduction device == cpu raise unittest SkipTest Non-deterministic CPU results fn sum max min argmax common fn torch full float -inf skip_if_not_triton test_reduction_config_limit This unit-test tests whether we exceed cudaDeviceProperties maxGridSize triton reduction configs large size hints introduced scaling XBLOCK feature resolve issue reduction configs which may exceed maxGridSize torch _inductor runtime runtime_utils next_power_of_ torch _inductor runtime triton_heuristics triton_config_reduction size_hints = x r _ _ range size_hints x = next_power_of_ size_hints x triton_config_reduction size_hints test_prod fn prod prod prod common fn torch rand common fn torch rand test_unroll_small_reduction fn x val index = x min - val index = x max - val index val index x sum - x any - x all - x argmin - x argmax - x amin - x amax - x aminmax config patch unroll_reductions_threshold= small sized reductions will get unrolled common fn torch randn torch _dynamo reset config patch unroll_reductions_threshold= make sure things also work they aren t unrolled common fn torch randn test_multilayer_sum_low_prec fp nyi cpu device == cpu raise unittest SkipTest f requires GPU_TYPE fn torch mean common fn torch rand dtype=torch float test_multilayer_prime_size fn torch max torch sum Requires masked loading intermediate reduction sample = torch full dtype=torch int sample - = common fn sample skip_if_gpu_halide skipCPUIf IS_MACOS fails macos test_multilayer_var fn torch var common fn torch rand dtype=torch float atol= e- rtol= e- common fn torch rand dtype=torch float atol= e- rtol= e- skipCPUIf IS_MACOS fails macos skip_if_halide accuracy off xfailIfS X accuracy failure test_multilayer_var_lowp fn torch var atol = None rtol = None device == cpu os getenv ATEN_CPU_CAPABILITY == default atol = e- rtol = e- common fn torch rand dtype=torch float atol=atol rtol=rtol common fn torch rand dtype=torch float atol=atol rtol=rtol test_split_cumsum fn torch cumsum - dtype get_all_dtypes include_bfloat =False include_bool=True include_complex=False include_half=False is_dtype_supported dtype continue Use low= since when mean value cumsum all points tends towards zero which makes relative error term blow up inp = make_tensor low= dtype=dtype device=self device common fn inp view - rtol= e- atol= e- check_lowp=False common fn inp view - rtol= e- atol= e- check_lowp=False skipCUDAIf SM OrLater Requires sm skip_if_gpu_halide accuracy issue test_split_cumsum_low_prec is_cpp_backend device raise unittest SkipTest ir Scan nyi CPU fn torch cumsum view - common fn torch rand dtype=torch float reference_in_float=True check_lowp=False test_consecutive_split_cumsum fn b = view - b = b view - torch cumsum + torch cumsum b dtype_a = torch float dtype_b = torch float ctx = contextlib nullcontext is_dtype_supported dtype_a is_dtype_supported dtype_b assertRaises TypeError ctx = make_tensor low= dtype=dtype_a device=self device b = make_tensor low= dtype=dtype_b device=self device common fn b rtol= e- atol= e- check_lowp=False config patch max_autotune_pointwise=True test_split_cumsum_index Split scan uses workspace needs zeroed before use data index does indirect indexing should catch issues workspace zeroed fn lengths data offsets = torch cumsum lengths data offsets lengths = torch full dtype=torch int device=self device lengths - = lengths - = data = make_tensor dtype=torch float device=self device common fn lengths data test_split_cumprod fn torch cumprod - dtype torch float torch float torch int torch int is_dtype_supported dtype continue inp = _large_cumprod_input dim= dtype=dtype device=self device common fn inp atol= e- rtol= e- check_lowp=False skipCUDAIf SM OrLater Requires sm skip_if_gpu_halide accuracy issue test_split_cumprod_low_prec is_cpp_backend device raise unittest SkipTest ir Scan nyi CPU fn torch cumprod view - dtype torch float torch bfloat is_dtype_supported dtype continue inp = _large_cumprod_input dim= dtype=dtype device=self device common fn inp reference_in_float=True check_lowp=False test_consecutive_split_cumprod fn b torch cumprod + torch cumprod b dtype_a = torch float dtype_b = torch float ctx = contextlib nullcontext is_dtype_supported dtype_a is_dtype_supported dtype_b assertRaises TypeError ctx = _large_cumprod_input dim= dtype=dtype_a device=self device b = _large_cumprod_input dim= dtype=dtype_b device=self device common fn b atol= e- rtol= e- check_lowp=False skip_if_halide scan ops TODO support lifted symints when dynamic torch _dynamo config patch dynamic_shapes False assume_static_by_default True test_custom_scan_op device = cuda raise unittest SkipTest associative_scan only supported GPU sum_combine b + b torch _higher_order_ops associative_scan associative_scan = torch randn device=self device expect = torch cumsum actual = associative_scan sum_combine assertEqual expect actual logcumsum_combine b min_v = torch minimum b max_v = torch maximum b mask = min_v = max_v &#124; ~min_v isinf torch where mask max_v + min_v - max_v exp log p expect = torch logcumsumexp actual = associative_scan logcumsum_combine assertEqual expect actual skip_if_halide scan ops TODO support lifted symints when dynamic torch _dynamo config patch dynamic_shapes False assume_static_by_default True test_custom_scan_op_compiled device = cuda raise unittest SkipTest associative_scan only supported GPU torch _higher_order_ops associative_scan associative_scan sum_combine b + b fn b dim diff = - b abs sad = associative_scan sum_combine diff dim sad sum dim = torch randn device=self device b = torch randn device=self device common fn b cfn = torch compile fn _ code = run_and_get_code cfn b Check everything fused into single kernel FileCheck check_not run check_regex r triton_ \ run\ arg _ arg _ buf check_not run run code skip_if_halide scan ops TODO support lifted symints when dynamic torch _dynamo config patch dynamic_shapes False assume_static_by_default True test_custom_scan_op_multi_input device = cuda raise unittest SkipTest associative_scan only supported GPU argmax_combine b a_value a_index = b_value b_index = b mask = a_value b_value &#124; a_value == b_value a_index b_index torch where mask a_value b_value torch where mask a_index b_index torch _higher_order_ops associative_scan associative_scan = torch randn device=self device expect = torch cummax idx = torch arange device=self device view expand actual = associative_scan argmax_combine idx assertEqual expect actual skip_if_halide scan ops TODO support lifted symints when dynamic torch _dynamo config patch dynamic_shapes False assume_static_by_default True test_custom_scan_would_split device = cuda raise unittest SkipTest associative_scan only supported GPU combine_linear_recurrence left right xl fl = left xr fr = right x = xl fr + xr f = fl fr x f eager_scan x g x g = x torch float g torch float x_out = torch empty_like x g_out = torch empty_like g x_out = x g_out = g i range x shape x_out i g_out i = combine_linear_recurrence x_out i - g_out i - x i g i x_out float g_out float torch compile compiled_scan x f torch _higher_order_ops associative_scan associative_scan x f = associative_scan combine_linear_recurrence x f dim= x f x = torch randn device=self device f = torch randn device=self device expect = eager_scan x f actual = compiled_scan x f assertEqual expect actual test_embedding_bag_byte_unpack device = cpu raise unittest SkipTest f No GPU_TYPE implementation returns empty fn torch ops quantized embedding_bag_byte_unpack M N = scales = torch randn M view torch uint offsets = torch randn M view torch uint data = torch randint M N dtype=torch uint packed = torch cat data scales offsets dim=- common fn packed xfail_if_mps_unimplemented skipIfXpu msg= No _weight_int pack_mm implementation XPU test_int _weight_only_quant convert_weight_to_int pack b b_int pack b_scales _ = _dynamically_quantize_per_channel b - torch int b_int pack b_scales fn b_int pack b_scales c res = torch _weight_int pack_mm b_int pack b_scales res = res + c res m = k = n = = torch rand m k dtype=torch bfloat b = torch rand n k dtype=torch bfloat c = torch rand m n dtype=torch bfloat b_int pack b_scales = convert_weight_to_int pack b common fn b_int pack b_scales c xfail_if_mps_unimplemented xfail_if_triton_cpu skipCUDAIf True No _dyn_quant_pack_ bit_weight implementation CUDA skipIfRocm skipIfXpu msg= No _dyn_quant_pack_ bit_weight implementation XPU test__dyn_quant_pack_ bit_weight q_group = k = n = torch manual_seed b = torch rand k n dtype=torch float in_features = b size out_features = b size dyn_quant_pack_ bit_weight b in_features out_features b_uint b_scales_and_zeros = _group_quantize_tensor_symmetric b n_bit= groupsize=q_group q_group == in_features b_scales_and_zeros = b_scales_and_zeros torch float b_scales_and_zeros = b_scales_and_zeros torch bfloat b_int pack = torch _dyn_quant_pack_ bit_weight b_uint b_scales_and_zeros None q_group in_features out_features b_int pack b_scales_and_zeros fn b in_features out_features b_int pack _ = dyn_quant_pack_ bit_weight b in_features out_features b_int pack common fn b in_features out_features xfail_if_mps_unimplemented xfail_if_triton_cpu skipCUDAIf True No _dyn_quant_matmul_ bit implementation CUDA skipIfRocm skipIfXpu msg= No _dyn_quant_matmul_ bit implementation XPU test__dyn_quant_matmul_ bit q_group = m = k = n = torch manual_seed = torch rand m k dtype=torch float b = torch rand k n dtype=torch float in_features = b size out_features = b size dyn_quant_pack_ bit_weight b in_features out_features b_uint b_scales_and_zeros = _group_quantize_tensor_symmetric b n_bit= groupsize=q_group q_group == in_features b_scales_and_zeros = b_scales_and_zeros torch float b_scales_and_zeros = b_scales_and_zeros torch bfloat b_int pack = torch _dyn_quant_pack_ bit_weight b_uint b_scales_and_zeros None q_group in_features out_features b_int pack b_scales_and_zeros fn q_group in_features out_features b_int pack _ = dyn_quant_pack_ bit_weight b in_features out_features res = torch _dyn_quant_matmul_ bit b_int pack q_group in_features out_features res common fn q_group in_features out_features test_expanded_reduction fn x y z = x y z sum atol = e- rtol = e- common fn torch randn torch randn atol=atol rtol=rtol skip_if_gpu_halide test_min_max_reduction fn b + b max + b min torch amax + keepdim=True torch amin b + keepdim=True dtypes = torch float torch float is_dtype_supported torch bfloat dtypes += torch bfloat dtype dtypes common fn torch randn dtype torch randn dtype skip_if_halide bug nan handling test_min_max_reduction_nan fn torch max torch min t = torch randn t = float nan common fn t skip_if_halide bug nan handling test_fmin_fmax fn b torch fmin b torch fmax b torch fmax + torch tensor common fn torch tensor - float nan float nan float nan torch tensor float nan float nan - float nan test_sum_int fn x x sum - + x sum dtypes = torch bool torch uint torch int inps = torch randint dtype=dtype dtype dtypes i inps common fn i check_lowp=False test_sum_dtype sum_dtype = torch double device = mps torch bfloat fn x x x sum - dtype=sum_dtype + x sum dtype=sum_dtype common fn torch ones skip_if_halide test_cummin fn x x cummin common fn torch rand check_lowp=not is_halide_backend device common fn torch rand check_lowp=not is_halide_backend device common fn torch rand check_lowp=not is_halide_backend device test_cumsum fn x x cumsum x cumsum Persistent reductions common fn torch rand check_lowp=not is_halide_backend device common fn torch rand check_lowp=not is_halide_backend device Non-persistent reduction common fn torch rand check_lowp=not is_halide_backend device atol= e- rtol= e- test_cumsum_zero_dim fn x x cumsum x cumsum - = torch rand common fn test_cumsum_no_mask fn x x cumsum - Persistent reduction = torch rand common fn check_lowp=not TEST_WITH_ROCM is_halide_backend device Non-persistent reduction b = torch rand common fn b check_lowp=not TEST_WITH_ROCM is_halide_backend device atol= e- rtol= e- test_cumprod_zero_dim fn x x cumprod x cumprod - = torch rand common fn test_cumsum_inf fn x x cumsum - _dtype = torch float make_tensor shape torch full shape float inf device=self device dtype=_dtype ctx = contextlib nullcontext is_dtype_supported _dtype assertRaises TypeError ctx cfn = torch compile fn n inp = torch full n float inf device=self device dtype=_dtype assertEqual cfn inp fn inp xfail_if_triton_cpu test_logcumsumexp fn x x logcumsumexp x logcumsumexp Persistent reductions common fn torch rand check_lowp=not TEST_WITH_ROCM is_halide_backend device common fn torch rand check_lowp=not TEST_WITH_ROCM is_halide_backend device Non-persistent reduction common fn torch rand check_lowp=not TEST_WITH_ROCM is_halide_backend device atol= e- rtol= e- test_logcumsumexp_zero_dim fn x x logcumsumexp x logcumsumexp - = torch rand common fn test_clamp fn b clamp - b clamp torch clamp + b max= common fn torch randn torch randn test_clamp_type_promotion tgt_dtype = torch double device = mps torch half fn b = torch tensor dtype=tgt_dtype device=self device c = torch full device=self device clamp min=b max=c common fn torch randint test_clamp_type_promotion_non_tensor fn clamp min= clamp min= common fn torch randint skip_if_gpu_halide xfail_if_triton_cpu test_dist fn b torch dist b torch dist b p= common fn torch randn torch randn xfail_if_mps skip_if_halide different pow accuracies xfail_if_triton_cpu test_norm_constant_overflow fn torch norm p=- dim= torch norm p=- dim= common fn torch randn skipCUDAIf SM OrLater Requires sm skip_if_gpu_halide https github com halide Halide issues test_dist_bf fn b torch dist torch bfloat b torch bfloat is_dtype_supported torch bfloat raise unittest SkipTest f torch bfloat supported device device common fn torch randn torch randn test_arange fn x rng = torch arange dtype=torch float device=x device view rng = torch arange device=x device tmp = x rng tmp tmp + rng common fn torch randn test_arange fn x rng = torch arange device=x device x + rng common fn torch randint check_lowp=False test_arange fn x x + torch ops aten arange start_step dtype=torch int device=x device common fn torch randn test_arange fn x x - torch arange - - device=x device common fn torch randn test_arange fn step device torch arange - step device=device compiled_fn = torch compile fn NOTE use assertEqual check dtypes which common doesn t do step - - expect = fn step device actual = compiled_fn step device assertEqual expect actual assertEqual expect actual test_arange fn x torch arange dtype=x dtype device=x device Test float arguments truncated int when dtype set explicitly make_arg = functools partial make_tensor device=self device requires_grad=False common fn make_arg dtype=torch float common fn make_arg dtype=torch int test_linspace fn x torch linspace device=x device + x common fn torch randn test_linspace fn x torch linspace device=x device + x common fn torch randn test_linspace fn x torch linspace device=x device common fn torch Tensor requires_multigpu test_linspace fn x torch linspace device=f GPU_TYPE common fn torch Tensor test_tensor fn x torch tensor device=x device + x torch tensor device=x device common fn torch randn test_tensor fn x torch tensor list range device=x device + x common fn torch randn test_tensor fn x torch tensor device=x device torch tensor device=x device + torch tensor device=x device + torch tensor device=x device + x common fn torch randn test_views fn x y x view size + y fn x y x + view size + y views = size size views common fn torch randn size torch randn size common fn torch randn size torch randn size size size views common fn torch randn size torch randn size common fn torch randn size torch randn size test_views fn x x view size + fn x x view size + size size - - - common fn torch randn size common fn torch randn size test_views example taken hf_BigBird forward arg arg index = torch ops aten index arg arg view_ = torch ops aten view index view_ = torch ops aten view view_ view_ common forward rand_strided torch float rand_strided torch int test_views example taken hf_BigBird forward arg arg arg = arg index_select arg arg = torch ops aten view arg arg = torch ops aten view arg - arg common forward torch randn torch randint test_views tensor shape any dimension forward x y = x y view len y - common forward torch randn test_views forward x x = torch ops aten relu x s = torch ops aten slice x s = torch ops aten slice s s = torch ops aten slice s y = torch ops aten view s - y common forward torch randn test_views x view dtype forward x y x = x + torch float y = y + torch int x view torch int y view torch float common forward torch rand dtype=torch float torch randint dtype=torch int test_torch_device_split fn x x split x = torch rand x device out = torch compile fn backend=lambda gm _ gm x ref = fn x b zip out ref assertTrue torch allclose b test_relu fn b torch relu torch relu + b common fn torch randn torch randn test_exp fn b torch exp torch exp + b common fn torch randn torch randn test_exp fn b torch exp torch exp + b torch pow -torch abs - b common fn torch randn torch randn skipIfXpu msg= logaddexp_xpu implemented ComplexFloat skipCUDAIf True Not implemented CUDA test_logaddexp common torch logaddexp torch randn dtype=torch complex torch randn dtype=torch complex test_sigmoid fn b torch sigmoid torch sigmoid + b common fn torch randn torch randn xfail_if_triton_cpu test_round fn b torch round torch round b + torch round decimals= without manual_seed there some chance test fails due https github com triton-lang triton issues torch manual_seed we always getting number exactly which we don t do right half common fn torch randn torch randn xfail_if_triton_cpu test_round_correctness device == cuda raise unittest SkipTest need debug tl libdevice A V fn torch round dtype = torch float device = mps torch float common fn torch arange - dtype=dtype check_lowp=False xfail_if_triton_cpu test_builtins_round fn x i x round i + + round i cfn = torch compile fullgraph=True dynamic=True fn x = torch zeros dtype=torch int device=self device torch no_grad i range assertEqual cfn x i fn x i xfail_if_triton_cpu test_builtins_round_float_ndigits_pos fn x i x + round i cfn = torch compile fullgraph=True dynamic=True fn x = torch zeros device=self device i = torch no_grad assertEqual cfn x i fn x i xfail_if_triton_cpu test_builtins_round_float_ndigits_zero fn x i x + round i cfn = torch compile fullgraph=True dynamic=True fn x = torch zeros device=self device i = torch no_grad assertEqual cfn x i fn x i xfail_if_triton_cpu test_builtins_round_float_ndigits_neg fn x i x + round i - cfn = torch compile fullgraph=True dynamic=True fn x = torch zeros device=self device i = torch no_grad assertEqual cfn x i fn x i test_builtins_round_int_ndigits_pos fn x i x + round i cfn = torch compile fullgraph=True dynamic=True fn x = torch zeros device=self device i = torch no_grad assertEqual cfn x i fn x i test_builtins_round_int_ndigits_zero fn x i x + round i cfn = torch compile fullgraph=True dynamic=True fn x = torch zeros device=self device i = torch no_grad assertEqual cfn x i fn x i test_silu fn torch nn functional silu common fn torch randn skip_if_halide halide has buggy nan handling test_nan_to_num fn torch nan_to_num torch nan_to_num nan= torch nan_to_num nan=None torch nan_to_num posinf= torch nan_to_num neginf= torch nan_to_num nan= posinf= neginf= common fn torch tensor float nan float inf float -inf check_lowp=False much more elaborate test required match finfo max s float half test_one_hot fn torch nn functional one_hot + common fn torch arange view check_lowp=False test_div fn b aten div b rounding_mode=None aten div b rounding_mode= floor aten div b rounding_mode= trunc b b common fn torch randn torch randn test_div fn b aten div b rounding_mode=None aten div b rounding_mode= floor aten div b rounding_mode= trunc b b common fn torch randint - torch randn test_div fn b aten div b rounding_mode=None aten div b rounding_mode= floor aten div b rounding_mode= trunc b b = torch randint common fn test_div fn b aten div b rounding_mode=None aten div b rounding_mode= floor aten div b rounding_mode= trunc b b common fn torch randint - torch randint test_div fn b aten div b rounding_mode=None aten div b rounding_mode= floor aten div b rounding_mode= trunc b b divide scalar common fn torch randint - test_div fn b aten div b rounding_mode=None aten div b rounding_mode= floor aten div b rounding_mode= trunc b b treat boolean integer common fn torch ones dtype=torch bool torch randint - - skip_if_triton_cpu divide zero cannot xfail because crashes process test_div fn b aten div b rounding_mode=None aten div b rounding_mode= floor aten div b rounding_mode= trunc b b common fn torch randint torch randint - - test_div fn b aten div b rounding_mode=None aten div b rounding_mode=None aten div b rounding_mode=None aten div b rounding_mode= floor aten div b rounding_mode= trunc b b common fn test_div fn x torch div x aten true_divide x aten div Tensor x common fn torch randn skip_if_triton_cpu divide zero cannot xfail because crashes process test_div_zero_dim fn b aten div b rounding_mode=None aten div b rounding_mode= floor aten div b rounding_mode= trunc b b dtype torch float torch int common fn make_tensor device=self device dtype=dtype make_tensor device=self device dtype=dtype exclude_zero=True common fn make_tensor device=self device dtype=dtype make_tensor device=self device dtype=dtype exclude_zero=True skip_if_triton_cpu divide zero cannot xfail because crashes process test_div_prim fn b torch ops prims div b dtype torch float torch int common fn make_tensor device=self device dtype=dtype make_tensor device=self device dtype=dtype exclude_zero=True test_floordiv fn_floor_input i n = i + n common fn_floor_input make_tensor device=self device dtype=torch float fn_int_input i n = i + n common fn_int_input make_tensor device=self device dtype=torch float test_div_precision Reproducer https github com pytorch pytorch issues forward x y z = x div y F softmax z dim=- query = torch randn key = torch randn x = torch matmul query key transpose - - common forward x e- x = torch tensor - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - x = torch matmul x x y = torch tensor - common forward x y test_div_softmax_symfloat forward x y z = x div y x shape - F softmax z dim=- query = torch randn key = torch randn x = torch matmul query key transpose - - cf = torch compile forward dynamic=True cf x e- cf x e- test_div_presicion_accuracy fix https github com pytorch pytorch issues forward x y x y sum x = torch rand y = common forward x y test_mul_softmax_symfloat forward x y z = x mul y x shape - F softmax z dim=- query = torch randn key = torch randn x = torch matmul query key transpose - - cf = torch compile forward dynamic=True cf x e- cf x e- test_div_by_zero fn x runtime_zero runtime_neg_zero zero = torch zeros_like x x x - zero x zero x -zero zero zero x runtime_zero NOTE -runtime_zero doesn t work - broken triton x runtime_neg_zero runtime_zero runtime_neg_zero = torch randn zero = torch zeros neg_zero = -zero common fn zero neg_zero test_both_scalars fn b aten add b aten add b aten sub b aten sub b aten mul b aten mul b common fn reference_in_float=False test_sum_keepdims fn b torch sum + b - keepdim=True common fn torch randn torch randn skip_if_cpu skip_if_halide only -bit indexing largeTensorTest GB inductor=True test_large_tensor_reduction Test -bit indexing works correctly fn torch max t = torch ones dtype=torch int device=self device t - = common OOMs here because copies inputs check mutations compiled_fn = torch compile fn actual = compiled_fn t expect = torch tensor dtype=torch int device=self device assertEqual actual expect skip_if_cpu skip_if_gpu_halide only -bit indexing test_large_broadcast_reduction Test -bit indexing works correctly when inputs less than -bit intermediate tensors require -bit indexing fn b torch max + b t = torch ones dtype=torch int device=self device t = torch ones dtype=torch int device=self device t - - = t - - = common OOMs here because copies inputs check mutations compiled_fn = torch compile fn actual = compiled_fn t t expect = torch tensor dtype=torch int device=self device assertEqual actual expect skip_if_halide only -bit indexing largeTensorTest GB inductor=True test_large_pointwise fn + t = torch ones + dtype=torch int device=self device compiled_fn = torch compile fn actual = compiled_fn t Can t use assertEqual expands broadcasted inputs del t torch device device type == GPU_TYPE getattr torch GPU_TYPE empty_cache assertTrue actual == all skip_if_halide only -bit indexing largeTensorTest GB inductor=True test_large_offset_pointwise Test -bit indexing used when input views tensor can indexed -bit strides storage offset pushes over INT_MAX fn + t = torch ones + dtype=torch int device=self device t = compiled_fn = torch compile fn actual = compiled_fn t assertTrue actual == all skip_if_halide only -bit indexing largeTensorTest GB inductor=True test_large_strided_reduction Test -bit indexing used when input numel less than INT_MAX stride calculations go above INT_MAX fn torch max storage = torch ones + dtype=torch int device=self device view = storage view - = compiled_fn = torch compile fn actual = compiled_fn view expect = torch tensor dtype=torch int device=self device assertEqual actual expect test_softmax fn b torch softmax + b - torch softmax torch softmax b common fn torch randn torch randn test_log_softmax fn b F log_softmax + b - F log_softmax F log_softmax b common fn torch randn torch randn test_transpose fn b torch t + b torch transpose b + common fn torch randn torch randn test_permute fn torch permute + + torch permute + common fn torch randn test_permute fn = unfold = torch unsqueeze = torch permute - common fn torch randn test_expand fn + expand + expand + expand - - common fn torch randn test_squeeze fn + squeeze + squeeze + common fn torch randn test_squeeze fn + squeeze - squeeze + squeeze + common fn torch randn test_squeeze_varargs fn x x squeeze clone = torch randn common fn test_simplify_loops fn b + b common fn torch randn torch randn permute test_unsqueeze fn torch unsqueeze + - + torch unsqueeze + torch unsqueeze + + torch unsqueeze - + common fn torch randn test_unsqueeze_inplace fn tmp = + aten unsqueeze_ tmp tmp = aten unsqueeze_ + + tmp tmp common fn torch randn test_addmm fn b c torch addmm + b + c + + common fn torch randn torch randn torch randn test_addmv fn b c torch addmv b c cfn = torch compile backend= inductor fn input = torch tensor dtype=torch int mat = torch tensor np random randn dtype=torch int vec = torch tensor torch no_grad assertEqual cfn input mat vec fn input mat vec https github com pytorch pytorch issues skipCUDAIf True cuda failed float linear skipIfXpu msg= Double complex datatype matmul supported oneDNN test_linear_float _dtype = torch float ctx = contextlib nullcontext is_dtype_supported _dtype assertRaises TypeError ctx mod = torch nn Sequential torch nn Linear _dtype eval torch no_grad common mod torch randn _dtype test_linear mod = torch nn Sequential torch nn Linear torch nn Sigmoid ToTuple common mod torch randn test_linear mod = torch nn Sequential torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU torch nn Linear torch nn ReLU common mod torch randn atol= e- rtol= test_bmm fn b torch bmm b torch bmm + b + + common fn torch randn torch randn check_lowp=False common fn torch randn torch randn check_lowp=False test_bmm fn b torch bmm permute b common fn torch randn torch randn check_lowp=False skipIfPy segfaults skipCUDAIf SM OrLater Requires sm test_mixed_mm fn b torch mm b dtype common fn torch randn torch randint - dtype=torch int check_lowp=True skipIfPy segfaults skipCUDAIf SM OrLater Requires sm test_mixed_mm fn b scale bias torch mm b dtype scale + bias common fn torch randn torch randint - dtype=torch int torch randn torch randn check_lowp=True skipIfPy segfaults skipCUDAIf SM OrLater Requires sm test_mixed_mm fn b torch mm b dtype so different block sizes tried out during autotuning common fn torch randn torch randint - dtype=torch int rtol= atol= with_tf _off test_uint x _mixed_mm fn b torch mm torch cat b xF b reshape - b shape dtype sub common fn torch randn torch randint dtype=torch uint check_lowp=True skipIfXpu test_mm_mixed_dtype fn b torch mm b t = torch arange dtype=torch float device=self device view t = torch arange dtype=torch int device=self device view msg = expected have same dtype got = assertRaisesRegex RuntimeError msg fn t t config cpp_wrapper msg = aoti_torch_ API call failed assertRaisesRegex RuntimeError msg torch compile fn t t skipIfXpu xfail_if_mps_unimplemented linear non-float inputs test_linear_mixed_dtype Net nn Module __init__ - None super Net __init__ noqa UP fc = nn Linear forward x x = fc x permute x fn = Net device t = torch arange device=self device view msg = expected have same dtype got = assertRaisesRegex RuntimeError msg fn t config cpp_wrapper msg = aoti_torch_ API call failed assertRaisesRegex RuntimeError msg torch no_grad torch compile fn t assertRaisesRegex RuntimeError Autograd support dtype torch compile fn t unittest skipIf IS_BIG_GPU Skipping triton backend only since big GPU enough SM config patch max_autotune True max_autotune_gemm_backends TRITON test_linear_dynamic_maxautotune device == cpu raise unittest SkipTest using triton backend only supported CPU torch compile dynamic=True Model torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x x = torch randn torch _dynamo mark_dynamic x common Model x test_scalar_input fn x y = torch div x y rounding_mode= floor common fn torch randint torch _dynamo config patch dynamic_shapes=True torch _dynamo config patch assume_static_by_default=False test_scalar_output fn arg _ arg _ arg _ = arg _ size view = torch ops aten view default arg _ - arg _ embedding = torch ops aten embedding default arg _ view full = torch ops aten full default arg _ dtype=torch float full arg _ embedding arg _ = rand_strided device= cpu dtype=torch float arg _ = rand_strided device= cpu dtype=torch int common fn arg _ arg _ test_shape_prop_torch_ones Model torch nn Module forward attention_scores extended_attention_mask = torch ones device=attention_scores device attention_scores = attention_scores + extended_attention_mask attention_scores mod = Model eval torch no_grad common mod torch randn slowTest expectedFailureCodegenDynamic config patch freezing True test_conv_bn_fuse For gpu path there accuracy issue device == GPU_TYPE raise unittest SkipTest only support cpu conv bn test fails dynamic check which bn fused there will have loops vars input_shapes = conv_modules = torch nn Conv d torch nn Conv d torch nn Conv d bn_modules = torch nn BatchNorm d torch nn BatchNorm d torch nn BatchNorm d options = itertools product True False dim bias kernel_size dilation groups options oC = groups iC = groups x_shape = iC + input_shapes dim mod = torch nn Sequential conv_modules dim iC oC kernel_size=kernel_size dilation=dilation groups=groups bias=bias bn_modules dim oC eval test_memory_format = torch contiguous_format TODO GPU path doesn t support channels_last now HAS_GPU dim channels_last = torch channels_last dim == torch channels_last_ d test_memory_format append channels_last memory_format test_memory_format v = torch randn x_shape dtype=torch float memory_format=memory_format torch no_grad common mod v test_conv_functional_bn_fuse For gpu path there accuracy issue device == GPU_TYPE raise unittest SkipTest only support cpu conv bn test Define BatchNorm using functional BN BatchNorm torch nn BatchNorm d __init__ num_features eps= e- momentum= affine=True track_running_stats=True device=None dtype=None factory_kwargs = device device dtype dtype super __init__ num_features eps=eps momentum=momentum affine=affine track_running_stats=track_running_stats factory_kwargs forward x momentum None exponential_average_factor = exponential_average_factor = momentum training track_running_stats TODO statement only here tell jit skip emitting when None num_batches_tracked None type ignore has-type num_batches_tracked = num_batches_tracked + type ignore has-type momentum None use cumulative moving average exponential_average_factor = float num_batches_tracked use exponential moving average exponential_average_factor = momentum training bn_training = True bn_training = running_mean None running_var None x = F batch_norm x If buffers tracked ensure they won t updated running_mean training track_running_stats None running_var training track_running_stats None weight bias bn_training exponential_average_factor eps x v = torch randn dtype=torch float mod = torch nn Sequential torch nn Conv d kernel_size= dilation= groups= bias=True BatchNorm eval torch no_grad common mod v skipIfRocm xfail_if_mps Expected find run test_conv_inference_heuristics device = GPU_TYPE raise unittest SkipTest f GPU_TYPE only test in_channels = out_channels = kernel_size = groups = grouped_conv = nn Conv d in_channels out_channels kernel_size groups=groups device input_tensor = torch randn in_channels device Perform forward pass torch compile foo m inp m inp device = xpu torch no_grad _ code = run_and_get_code foo grouped_conv input_tensor no channels last permuting before kernel config cpp_wrapper FileCheck check_not call_triton check _convolution run code FileCheck check_not run check convolution run code out should do channels last inference in_channels = out_channels = kernel_size = Create convolution layer conv_layer = nn Conv d in_channels out_channels kernel_size device input_tensor = torch randn in_channels device torch no_grad _ code = run_and_get_code foo conv_layer input_tensor should channels last permuting before kernel is_halide_backend device FileCheck check halide_kernel_ check convolution run code FileCheck check run check convolution run code test_upsample_cat_conv device == GPU_TYPE raise unittest SkipTest only support cpu upsample_cat_conv test M torch nn Module __init__ kwargs super __init__ upsample = torch nn UpsamplingNearest d scale_factor= conv = torch nn Conv d kernel_size= padding= stride= dilation= kwargs forward x y x = upsample x z = torch cat x y dim= z = conv z z v = torch randn v = torch randn torch no_grad common M eval v v test_aliased_buffer_reuse fn x y x = x y = y c = torch cat x y dim=- d = + c m = torch mm d d m + x common fn torch randn torch randn check_lowp=False test_slice_view_with_graph_break fn = torch tensor device=self device = b = squeeze = e pass = b expect = fn opt_fn = torch compile fn actual = opt_fn assertEqual expect actual test_view_detach fn detach common fn torch randn requires_grad=True test_gather fn b torch gather expand b + torch gather expand - b + common fn torch randn torch randint dtype=torch int test_gather d tensor fn b torch gather b + torch gather - b x = torch tensor y = torch tensor assertEqual fn x y x + x xfail_if_mps_unimplemented Sparse supported test_gather fn b torch gather b sparse_grad=True common fn torch randn requires_grad=True torch randint dtype=torch int test_device_assert fn x y x = torch sum x view int x shape dim= torch gather x torch trunc y torch int x = torch randn device=self device x = torch randn device=self device dtype = torch float device = mps torch float y = torch ones dtype=dtype device=self device assertEqual torch compile fn x y fn x y assertEqual torch compile fn x y fn x y test_slice fn + + + + - negative index out range - negative index out range common fn torch randn test_slice fn - - + - - + - - + + - - common fn torch randn It s view so doesn t generate kernel expectedFailureCodegenDynamic test_slice fn b torch ops aten slice Tensor -b x = torch rand common fn x expectedFailureCodegenDynamic test_slice empty slices require clamping start end fn aten slice Tensor aten slice Tensor shape shape + aten slice Tensor - aten slice Tensor - - x = torch rand common fn x test_split_with_list fn sizes t + t torch split sizes - common fn torch randn common fn torch randn common fn torch randn test_split_with_integer argument ` split_size_or_sections ` integer torch compile dynamic=True f x sizes torch split x sizes - split into equally sized chunks = + r r = f torch randn assertTrue r size == assertTrue r size == split into equally sized chunks = + + r r r = f torch randn assertTrue r size == assertTrue r size == assertTrue r size == split unevenly = + + + r r r r = f torch randn assertTrue r size == assertTrue r size == assertTrue r size == assertTrue r size == test_split_failed torch compile backend= inductor fn torch split dim= assertRaisesRegex RuntimeError fn torch randn test_inductor_assert torch compile backend= inductor dynamic=True fn assert shape = shape = cos inp = torch randn torch _dynamo mark_dynamic inp torch _dynamo mark_dynamic inp assertEqual fn inp inp cos test_split fn t = torch split - t t t t fn fn + common fn torch randn common fn torch randn parametrize dilation parametrize dim subtest subtest test_low_memory_max_pool dilation int dim int prims = torch ops prims fn x kernel_size = dim == stride = dim padding = dim ceil_mode = False vals offsets = prims _low_memory_max_pool_with_offsets x kernel_size stride padding dilation dim ceil_mode indices = prims _low_memory_max_pool_offsets_to_indices offsets kernel_size x shape -dim stride padding dilation= dilation dim vals indices offsets common fn torch randn dim test_to_dtype new_dtype = torch float device = mps torch bfloat fn b aten _to_copy dtype= aten _to_copy b + dtype= aten b new_dtype aten b torch bool common fn torch randn torch randn dtype=new_dtype requires_gpu test_to_device fn device type == cpu aten _to_copy device=torch device GPU_TYPE dtype= layout= aten _to_copy device=torch device cpu dtype= layout= common fn torch randn test_to_memory_format fn memory_format memory_format=memory_format common fn torch randn torch channels_last common fn torch randn memory_format=torch channels_last torch contiguous_format requires_gpu test_to_device_constant fn d = device type d == cpu d = GPU_TYPE d = cpu const = torch as_tensor list range device=d torch arange device=d d + const d const + d common fn torch randn requires_gpu xfail_if_triton_cpu test_multi_device fn x x = x + x = x + x = x device=GPU_TYPE x = x + x = x + x = x cpu x = x + x = x + x = x device=GPU_TYPE x = x + x = x + x = x cpu x = x + x = x + x common fn torch randn check_lowp=False cpu doesn t understand fp there explicit cpu calls skipIfRocm requires_multigpu test_multi_gpu_device TODO https github com pytorch pytorch issues x = torch rand device=GPU_TYPE fn x y r = torch ops aten div x y r = r f GPU_TYPE r common fn torch randn torch randn check_lowp=False requires_multigpu test_multi_gpu_recompile_on_index torch set_float _matmul_precision high gemm x y x y failed_guard = None fail guard nonlocal failed_guard failed_guard = guard gemm_opt = torch _dynamo optimize inductor guard_fail_fn=fail gemm x = torch randn device=f GPU_TYPE y = torch randn device=f GPU_TYPE gemm_opt x y x = torch randn device=f GPU_TYPE y = torch randn device=f GPU_TYPE gemm_opt x y assertTrue failed_guard None assertTrue tensor x Tensor device index mismatch Expected device index failed_guard reason test_unbind fn torch unbind torch unbind - common fn torch randn skipIfXpu msg= Incorrect reference XPU see issue test_conv d_with_permute fix https github com pytorch pytorch issues ConvModel nn Module __init__ super __init__ conv = nn Conv d kernel_size= padding= forward x x = x permute conv x common ConvModel torch randn check_lowp=False test_conv d_depthwise ConvModel nn Module __init__ super __init__ conv = nn Conv d kernel_size= stride= padding= groups= bias=False forward x conv x input_tensor = torch randn as_strided common ConvModel input_tensor check_lowp=False test_convolution m = torch nn Sequential torch nn Conv d torch nn ReLU ToTuple common m torch randn Mismatched elements Greatest absolute difference e- index up e- allowed Greatest relative difference index up allowed atol= e- rtol= Make sure we compute also fp reference Otherwise reference will compute fp cast back fp which causes numeric differences beyond tolerance reference_in_float=not torch version hip test_convolution fn x w b transposed conv aten convolution x w b True common fn torch randn torch randn torch randn check_lowp=False test_convolution Test stride padding dilation element list m = torch nn Sequential torch nn Conv d stride= padding= dilation= torch nn ReLU ToTuple common m torch randn atol= e- rtol= Make sure we compute also fp reference Otherwise reference will compute fp cast back fp which causes numeric differences beyond tolerance reference_in_float=not torch version hip skip_if_gpu_halide test_convolution fn x w x = F conv d x w groups=w shape x sum common fn torch randn torch randn test_convolution fn x w x = F conv d x w dilation= x size x sum x = torch randn w = torch randn torch _dynamo mark_dynamic x atol = None rtol = None device == xpu set float default tolerance check_model_gpu update rotl e- fp fix issue atol = e- rtol = e- common fn x w atol=atol rtol=rtol test_conv d m = torch nn Sequential torch nn Conv d kernel_size= ToTuple common m torch randn atol= e- rtol= Make sure we compute also fp reference Otherwise reference will compute fp cast back fp which causes numeric differences beyond tolerance reference_in_float=not torch version hip test_conv d_channels_last device == GPU_TYPE raise unittest SkipTest only support cpu conv d channels_last m = torch nn Sequential torch nn Conv d ToTuple only weight channels_last common m memory_format=torch channels_last torch randn check_lowp=False only activation channels_last common m torch randn memory_format=torch channels_last check_lowp=False activation weight all channels_last common m memory_format=torch channels_last torch randn memory_format=torch channels_last check_lowp=False test_conv d_backward_channels_last fn grad_output inp weight convolution_backward_ = torch ops aten convolution_backward default grad_output inp weight False True True True convolution_backward_ only weight channels_last common fn torch randn torch randn torch randn memory_format=torch channels_last check_lowp=False parametrize use_block_ptr subtest False subtest True decorators= skip_if_not_triton test_conv d_channels_last use_block_ptr bool device == GPU_TYPE raise unittest SkipTest only support cpu conv d channels_last m = torch nn Sequential torch nn Conv d ToTuple config patch triton use_block_ptr use_block_ptr only weight channels_last common m memory_format=torch channels_last_ d torch randn only activation channels_last common m torch randn memory_format=torch channels_last_ d activation weight all channels_last common m memory_format=torch channels_last_ d torch randn memory_format=torch channels_last_ d skip_if_gpu_halide slow xfail_if_mps Non-divisible input sizes implemented MPS device test_adaptive_avg_pool d fn x aten _adaptive_avg_pool d x aten _adaptive_avg_pool d x + common fn torch randn check_lowp=False lowering avg_pool d case common fn torch randn no-op case common fn torch randn xfail_if_mps Non-divisible input sizes implemented MPS device test_adaptive_avg_pool d Big kernel size use fallback fn x aten _adaptive_avg_pool d x torch _inductor metrics generated_kernel_count = common fn torch randn check_lowp=False assertGeneratedKernelCountEqual xfail_if_mps skip_if_gpu_halide slow test_adaptive_max_pool d fn x aten adaptive_max_pool d x common fn torch randn check_lowp=False common fn torch randn no-op case common fn torch randn skip_if_gpu_halide slow test_adaptive_max_pool d Big kernel size use fallback fn x aten adaptive_max_pool d x torch _inductor metrics generated_kernel_count = common fn torch randn check_lowp=False assertGeneratedKernelCountEqual skip_if_gpu_halide slow test_adaptive_max_pool d test when adaptive_max_pool d fallbacks max_pool d fn x aten adaptive_max_pool d x Big kernel common fn torch randn Small kernel common fn torch randn xfail_if_mps_unimplemented test_fractional_max_pool d fn x samples aten fractional_max_pool d x samples common fn torch randn torch rand check_lowp=False xfail_if_mps_unimplemented test_fractional_max_pool d large kernel size without unrolling fn x samples aten fractional_max_pool d x samples common fn torch randn torch rand check_lowp=False xfail_if_mps_unimplemented test_fractional_max_pool d fn x samples aten fractional_max_pool d x samples common fn torch randn torch rand check_lowp=False xfail_if_mps_unimplemented config patch fallback_random=True skip_if_halide Can only unroll loops over constant extent test_fractional_max_pool d random seed torch manual_seed check rectangular kernel output size fn x torch nn functional fractional_max_pool d_with_indices x common fn torch randn check_lowp=False xfail_if_mps_unimplemented test_fractional_max_pool d fn x samples aten fractional_max_pool d x samples common fn torch randn torch rand check_lowp=False test_multi_threading model = torch nn Linear eval inp = torch randn num_run = run_weights_sharing_model m inp torch no_grad _ range num_run y = m inp numb_instance = threads = compiled_m = torch compile model _ range numb_instance + thread = threading Thread target=run_weights_sharing_model args= compiled_m inp threads append thread thread start thread threads thread join unittest skipIf config is_fbcode fbcode triton error needs debugging skip_if_triton_cpu Flaky Triton CPU skip_if_gpu_halide https github com halide Halide issues test_adaptive_avg_pool d_low_prec Model torch nn Module __init__ - None super __init__ avgpool = torch nn AdaptiveAvgPool d forward x x = avgpool x x mod = Model device dtype torch half torch bfloat Skip bfloat MacOS- MPS tests is_dtype_supported dtype continue x = torch randn device=self device dtype=dtype opt_mod = torch compile mod res = opt_mod x expected = mod x assertTrue torch allclose res expected test_buffer_copied_in_graph MyModel torch nn Module __init__ - None super __init__ buf = torch nn Buffer torch zeros w = torch nn Parameter torch zeros w = torch nn Parameter torch zeros forward x buf add_ w x w sum + buf sum model_for_eager = MyModel device model_for_compile = copy deepcopy model_for_eager eager_version_counters = buffer _version _ buffer model_for_eager named_buffers compile_version_counters = buffer _version _ buffer model_for_compile named_buffers compiled_f = torch compile model_for_compile backend= inductor inp_ref = torch ones requires_grad=True device=self device inp_test = torch ones requires_grad=True device=self device out_ref = model_for_eager inp_ref clone out_test = compiled_f inp_test clone eager_version_counters_after = buffer _version _ buffer model_for_eager named_buffers compile_version_counters_after = buffer _version _ buffer model_for_compile named_buffers eager_delta = list map operator sub eager_version_counters_after eager_version_counters compile_delta = list map operator sub compile_version_counters_after compile_version_counters assertEqual eager_delta compile_delta skip_if_gpu_halide test_buffer_copied_in_graph_with_different_shapes MyModel torch nn Module __init__ - None super __init__ buf = torch nn Buffer torch ones w = torch nn Parameter torch Tensor forward x buf add_ w x sum + buf sum model_for_eager = MyModel device model_for_compile = copy deepcopy model_for_eager eager_version_counters = buffer _version _ buffer model_for_eager named_buffers compile_version_counters = buffer _version _ buffer model_for_compile named_buffers compiled_f = torch compile model_for_compile backend= inductor inp_ref = torch ones requires_grad=True device=self device inp_test = torch ones requires_grad=True device=self device out_ref = model_for_eager inp_ref clone out_test = compiled_f inp_test clone eager_version_counters_after = buffer _version _ buffer model_for_eager named_buffers compile_version_counters_after = buffer _version _ buffer model_for_compile named_buffers eager_delta = list map operator sub eager_version_counters_after eager_version_counters compile_delta = list map operator sub compile_version_counters_after compile_version_counters assertEqual eager_delta compile_delta test_buffer_batch_norm MyModel torch nn Module __init__ - None super __init__ m = torch nn BatchNorm d forward x m x model_for_eager = MyModel device model_for_compile = copy deepcopy model_for_eager eager_version_counters = buffer _version _ buffer model_for_eager named_buffers compile_version_counters = buffer _version _ buffer model_for_compile named_buffers compiled_f = torch compile model_for_compile backend= inductor inp_ref = torch ones requires_grad=True device=self device inp_test = torch ones requires_grad=True device=self device out_ref = model_for_eager inp_ref clone out_test = compiled_f inp_test clone eager_version_counters_after = TODO remove + after https github com pytorch pytorch issues fixed buffer _version + k m running_mean m running_var buffer _version k buffer model_for_eager named_buffers compile_version_counters_after = buffer _version _ buffer model_for_compile named_buffers eager_delta = list map operator sub eager_version_counters_after eager_version_counters compile_delta = list map operator sub compile_version_counters_after compile_version_counters assertEqual eager_delta compile_delta xfail_if_mps Non-divisible input sizes implemented MPS device test_adaptive_avg_pool_with_output_size_ m = nn AdaptiveAvgPool d common m torch randn m = nn AdaptiveAvgPool d common m torch randn test_max_pool d fn x aten max_pool d_with_indices x common fn torch randn skip_if_gpu_halide slow test_max_pool d fn x aten max_pool d_with_indices x common fn torch randn skip_if_gpu_halide slow test_max_pool d fn x padding aten max_pool d_with_indices x aten max_pool d_with_indices x common fn -torch arange dtype=torch float view skip_if_halide Can only unroll loops over constant extent test_max_pool d fn x padding aten max_pool d_with_indices x True common fn torch randn skip_if_gpu_halide slow test_max_pool d fn x aten max_pool d_with_indices x common fn torch randn skip_if_gpu_halide slow parametrize dilation test_max_pool d dilation int Big kernel size fn x aten max_pool d_with_indices x dilation= dilation common fn torch randn From https github com pytorch pytorch issues test_max_pool d ceil mode turns fn x torch nn functional max_pool d x stride= padding= ceil_mode=True common fn torch randn From https github com pytorch pytorch issues test_max_pool d dilation fn x aten max_pool d_with_indices x common fn torch randn test_avg_pool d fn x aten avg_pool d x common fn torch randn test_avg_pool d fn x aten avg_pool d x common fn torch randn test_avg_pool d fn x aten avg_pool d x aten avg_pool d x common fn -torch arange dtype=torch float view check_lowp=not is_halide_backend device misaligned addr fp test_avg_pool d fn x aten avg_pool d x True common fn torch randn test_avg_pool d fn x aten avg_pool d x count_include_pad=False common fn -torch arange dtype=torch float view check_lowp=not is_halide_backend device misaligned addr fp test_avg_pool d fn x aten avg_pool d x divisor_override= common fn -torch arange dtype=torch float view check_lowp=not is_halide_backend device misaligned addr fp test_avg_pool d Large kernel size use fallback fn x aten avg_pool d x torch _inductor metrics generated_kernel_count = common fn -torch arange dtype=torch float view assertGeneratedKernelCountEqual test_avg_pool d https github com pytorch pytorch issues fn x aten avg_pool d x kernel_size= stride= padding= ceil_mode=True common fn torch randn check_lowp=not is_halide_backend device misaligned addr fp tf _on_and_off skip_if_gpu_halide slow test_alexnet_prefix forward arg arg arg convolution = torch ops aten convolution arg arg arg False relu = torch ops aten relu convolution max_pool d_with_indices = torch ops aten max_pool d_with_indices relu getitem = max_pool d_with_indices getitem common forward rand_strided torch float cpu rand_strided torch float cpu rand_strided torch float cpu Mismatched elements Greatest absolute difference index up e- allowed Greatest relative difference index up allowed atol= e- rtol= test_elu fn x aten elu x + aten elu x + common fn torch randn rtol= e- atol= e- test_tan fn x aten tan x + aten tan x + common fn torch randn test_tanh fn x aten tanh x + aten tanh x + common fn torch randn skip_if_halide lgamma implemented xfail_if_triton_cpu test_lgamma fn x aten lgamma x + aten cos x + common fn torch randn test_cos fn x aten cos x + aten cos x + common fn torch randn test_sin fn x aten sin x + aten sin x + common fn torch randn test_repeat fn x x repeat x repeat x repeat x repeat common fn torch randn test_repeat_as_strided Reproducer fn x view_size = full = x repeat view = torch as_strided full view_size full stride result = view + view result common fn torch randn test_as_strided_on_views https github com pytorch pytorch issues fn c = view - convert float d = c view torch float e = d as_strided convert back bfloat f = e view torch bfloat g = f as_strided g = torch randn dtype=torch bfloat common fn reference_in_float=False test dtype separately out = fn assert out dtype == torch bfloat out = torch compile fn assert out dtype == torch bfloat test_repeat_interleave fn x x repeat_interleave x repeat_interleave dim= x repeat_interleave x size dim= common fn torch randn config patch implicit_fallbacks=True test_repeat_interleave_ fn x torch ops aten repeat_interleave Tensor x output_size= common fn torch tensor config patch fallback_random=True test_randn_with_dtype_and_device device == GPU_TYPE raise unittest SkipTest only support cpu randn_with_dtype_and_device test fn vectors rotations_shape = vectors shape - random_rotations = torch randn rotations_shape device=vectors device dtype=vectors dtype random_rotations += random_rotations common fn torch randn test_embedding m = torch nn Sequential torch nn Embedding padding_idx= torch nn ReLU ToTuple common m torch randint test_embedding_sparse Fix https github com pytorch pytorch issues fn weight indices F embedding indices weight sparse=True indices = torch randint weight = torch randn requires_grad=True common fn weight indices test_mean fn x x mean x mean - torch mean x - keepdim=True x mean common fn torch randn parametrize tile_reduction False True test_var_mean tile_reduction bool fn x torch var_mean x - torch var_mean x config patch triton prefer_nd_tiling tile_reduction triton tile_reductions tile_reduction common fn torch randn test_var_mean_div_by fn x var mean = torch var_mean x dim= keepdim=True x var var mean common fn torch rand test_var_correction fn x dim = - torch var x dim=dim correction= torch var x dim=dim correction= torch var x dim=dim correction= common fn torch randn Unrolled reduction common fn torch randn config patch pick_loop_orders=True test_transposed_propagates torch compile backend= inductor fullgraph=True fn x y x + y = torch randn device=self device permute b = torch randn device=self device permute c = fn b assertEqual stride c stride assertEqual c stride skip_if_gpu_halide test_std fn x torch var x True torch var x False torch var x - True torch var x - False torch std x False torch std x True torch std x False torch std x - True keepdim=True common fn torch randn test_embedding_bag fn w i o aten _embedding_bag w i o False False None common fn torch randn torch randint torch tensor test_batch_norm_ d m = torch nn Sequential torch nn BatchNorm d torch nn ReLU m eval common m torch randn check_lowp=False common m torch randn check_lowp=False too painful match types bn model From yolov with_tf _off test_batch_norm_ d_ device == cpu raise unittest SkipTest f requires GPU_TYPE Repro torch nn Module __init__ - None super __init__ self_ = torch nn Conv d kernel_size= stride= padding= bias=False self_ = torch nn BatchNorm d eps= momentum= affine=True track_running_stats=True self_ = torch nn LeakyReLU negative_slope= inplace=True forward l_input_ torch Tensor self_ = self_ l_input_ self_ = self_ self_ self_ = self_ self_ self_ inp = torch randn dtype=torch float device=GPU_TYPE mod = Repro device=GPU_TYPE o = mod inp o = torch compile mod inp assertEqual o o rtol= e- atol= e- patch object config trace enabled True test_layer_norm m = torch nn Sequential torch nn LayerNorm torch nn ReLU m eval torch no_grad common m torch randn check_lowp=False device = cpu assertGeneratedKernelCountEqual torch _functorch config patch donated_buffer True test_matmul_layer_norm batch_size = seq_length = hidden_size = inp = torch randn batch_size seq_length hidden_size requires_grad=True device=self device weight = torch randn hidden_size hidden_size requires_grad=True device=self device layer_norm = torch nn LayerNorm hidden_size device=self device foo inp weight matmul_output = inp weight final_output = layer_norm matmul_output final_output common foo inp weight check_lowp=False test_transpose_add fn b t + b common fn torch randn torch randn check_lowp=False device = cpu assertGeneratedKernelCountEqual patch object config triton persistent_reductions True test_softmax_one_kernel_persist fn x dim = x_max = torch amax x dim keepdim=True unnormalized = torch exp x - x_max result = unnormalized torch sum unnormalized dim keepdim=True result common fn torch randn check_lowp=False device = cpu assertGeneratedKernelCountEqual patch object config triton persistent_reductions False test_softmax_one_kernel_loop fn x x_max = torch amax x keepdim=True unnormalized = torch exp x - x_max result = unnormalized torch sum unnormalized keepdim=True result common fn torch randn check_lowp=False device = cpu assertGeneratedKernelCountEqual test_complex_fallback fn x x x + common fn torch randn dtype=torch complex assertGeneratedKernelCountEqual ToComplex nn Module forward x x + x + torch complex common ToComplex torch rand check_lowp=False device = cpu assertGeneratedKernelCountEqual test_complex_from_real_imag fn x y aten complex default x y = torch randn permute common fn exact_stride=True reference_in_float=False test_view_as_complex Repro torch nn Module __init__ - None super __init__ forward view_ clone = torch ops aten clone default view_ memory_format=torch contiguous_format view_ = None view_as_complex = torch ops aten view_as_complex default clone clone = None view_as_complex inp = torch empty_strided device mod = Repro o = mod inp o = torch compile mod inp assertEqual o o test_view_as_real fn x y = torch view_as_real x y + x = torch randn dtype=torch complex common fn x test_polar fn dist angle torch polar dist angle dtype = torch float device = mps torch float inp = torch tensor dtype=dtype torch tensor np pi np pi dtype=dtype common fn inp reference_in_float=self device = mps skip_if_gpu_halide incorrect result CUDA test_cauchy fn x y torch sum torch unsqueeze x - - y common fn torch randn torch randn Absolute difference up allowed Relative difference e- up e- allowed atol= e- rtol= e- check_lowp=False device = cpu assertGeneratedKernelCountEqual skip_if_gpu_halide misaligned address error test_fusing_write_into_disjoint_read test_flip copy_ torch flip common test_flip torch rand assertGeneratedKernelCountEqual issue only manifests cuda large tensors device = cpu f = + = + = torch rand device=self device common f test_inplace_flip f x y x copy_ x flip y = y sum dim= keepdim=True + y x + y x = torch randn y = torch randn common f x y atol= e- rtol= e- test_gather_scatter fn node_feat edge_index src_node_feat = node_feat edge_index dst_node_feat = node_feat edge_index edge_feat = src_node_feat - dst_node_feat + new_node_feat = torch zeros_like node_feat new_node_feat scatter_add_ edge_index unsqueeze - expand_as edge_feat edge_feat new_node_feat num_nodes = num_features = node_feat = torch randn num_nodes num_features edge_index = torch randint num_nodes size= num_nodes common fn node_feat edge_index check_lowp=False device = cpu assertGeneratedKernelCountEqual config patch max_fusion_size= test_no_mega_fusion_during_lowering n = fn args x = args i range n x = torch add x args i x common fn torch randn _ range n check_lowp=False print -- torch _inductor metrics generated_kernel_count device = cpu assertTrue torch _inductor metrics generated_kernel_count test_move_arange fn x torch arange len x device= cpu x device + x common fn torch randn check_lowp=False we have copy there will more than kernel assertGeneratedKernelCountEqual test_leaky_relu fn x aten leaky_relu x + aten leaky_relu x + common fn torch randn test_gelu fn x aten gelu x + aten gelu x + common fn torch randn test_clone fn x aten clone x + aten clone x + common fn torch randn test_masked_fill fn mask value aten masked_fill value mask - + aten masked_fill value torch logical_not mask common fn torch randint dtype=torch bool torch randn test_masked_fill_promotion fn mask value aten masked_fill value mask torch tensor opt_fn = torch compile fn backend= inductor inp torch randn dtype=torch float device == GPU_TYPE torch float device=self device torch randint device=self device inputs = torch randint dtype=torch bool device=self device inp assertEqual fn inputs opt_fn inputs xfail_if_mps NullHandler object has no attribute wrapper_code test_masked_scatter fn value mask source torch masked_scatter value mask source value = make_tensor dtype=torch float device=self device mask = make_tensor dtype=torch bool device=self device source = make_tensor mask count_nonzero dtype=torch float device=self device common fn value mask source test_fill fn x tmp = torch ones_like x tmp aten fill Scalar tmp common fn torch randn test_fill fn x tmp = torch ones_like x tmp aten fill Tensor tmp torch tensor common fn torch randn test_pow fn x aten pow x e e range - common fn torch randn xfail_if_triton_cpu test_pow fn x aten pow x aten pow x common fn torch randn dtype=torch float Mismatched elements Greatest absolute difference e+ index up e- allowed Greatest relative difference e- index up e- allowed atol= e- rtol= e- skip_if_gpu_halide https github com halide Halide issues config patch halide scheduler_cuda Li test_pow power special-cased arbitrary power would still produce triton codegen error fn x z = torch tensor device=self device w = z + x torch pow w opt = torch compile fn backend= inductor input = torch rand device=self device assertTrue same opt input fn input test_pow_int fn x y torch pow x x torch pow x y dtype torch uint torch int torch int torch int torch int intmax = torch iinfo dtype max make_arg = functools partial make_tensor dtype=dtype device=self device requires_grad=False common fn make_arg make_arg high=intmax xfail_if_triton_cpu test_pow_symfloat fn x r = math sqrt x size r = r x r cfn = torch compile fullgraph=True dynamic=True fn x = torch randn device=self device assertEqual cfn x fn x test_glu fn x aten glu x - aten glu x aten glu x common fn torch randn test_unsigned_constant_tensors fn x c = torch tensor dtype=torch uint c + x torch neg c torch neg c + x common fn torch randn Disable size_asserts test due https github com pytorch pytorch issues config patch size_asserts=os environ get TORCHINDUCTOR_SIZE_ASSERTS == torch _dynamo config patch capture_dynamic_output_shape_ops=True test_nonzero_unbacked_refinement fn x z = x nonzero torch _check z size == z + common fn torch tensor assertRaises RuntimeError torch compile fn torch tensor torch _dynamo config patch capture_scalar_outputs=True test_unbacked_floordiv_simplify fn x y z = y item torch _check z == x + x new_ones z common fn torch randn torch tensor common fn torch randn torch tensor torch _dynamo config patch capture_scalar_outputs=True test_unbacked_floordiv_simplify_errors fn x y z = y item torch _check z == x + x new_zeros z This little suboptimal we actually fail compiler way causes Dynamo graph break assertRaises RuntimeError torch compile fn torch randn torch tensor test_cat tgt_dtype = torch double device = mps torch half fn tmp = torch cat + + - torch cat tmp tmp torch cat tmp tmp dtype=tgt_dtype common fn torch randn common fn torch randn memory_format=torch channels_last test_cat_uint fn x batch_shape = x shape out = torch cat x new_zeros expand batch_shape + x dim=- out common fn torch randint size= dtype=torch uint test_cat_empty fn_ tensors torch cat tensors common fn_ torch randn torch ones common fn_ torch randn torch ones torch randn common fn_ torch ones torch randn test_cat_empty_index fn out x torch cat out x dim= common fn torch randn torch randn torch _dynamo config patch capture_scalar_outputs=True test_cat_unbacked_legacy_empty fn x y z = y item torch cat x x new_ones z assertRaisesRegex RuntimeError Expected -D tensors got -D tensor number list common fn torch randn torch tensor torch _dynamo config patch capture_scalar_outputs=True test_cat_unbacked_empty_ d fn x y z = y item torch cat x x new_ones z common fn torch randn torch tensor common fn torch randn torch tensor torch _dynamo config patch capture_scalar_outputs=True test_cat_unbacked_ d fn x y z = y item torch cat x x new_ones z x shape common fn torch randn torch tensor common fn torch randn torch tensor test_cat_negative_dim fn tensors torch cat tensors dim=- common fn torch randn torch randn common fn torch randn torch randn torch randn common fn torch randn torch randn torch randn expectedFailureCodegenDynamic test_cat_single_empty fails dynamic check has dynamic dimension fn_ tensors torch cat tensors common fn_ torch ones test_cat_upcasting fn arg _ slice_ cat_ = aten cat default arg _ slice_ cat_ common fn torch randn dtype=torch float torch randn dtype=torch float test_cat_extern_kernel fn x x x x x = torch mm x x s = torch narrow x x = torch mm s x c = torch cat x x c device == xpu atol = e- rtol = e- atol = e- rtol = e- MPS has correctness problem before MacOS contextlib nullcontext device = mps MACOS_VERSION = assertRaises AssertionError common fn torch randn torch randn torch randn torch randn atol=atol rtol=rtol check_lowp=False accuracy issues relatively large matmuls skip_if_gpu_halide Constant folding explicitly turned off due issue Turn back test unittest skipIf config triton native_matmul native matmul has better precision torch _inductor config patch joint_graph_constant_folding=True Numerical accuracy failure triton fp max_autotune_gemm_backends= ATEN test_remove_no_ops matmul_with_op x y fn fn x y foo_opt = torch compile matmul_with_op test no-op fns = lambda x x + torch zeros dtype=torch float device=x device noqa E lambda x x - torch zeros dtype=torch float device=x device noqa E lambda x x torch ones dtype=torch float device=x device noqa E lambda x x torch ones dtype=torch float device=x device noqa E inps = torch rand device=self device _ range fn fns out source_codes = run_and_get_code foo_opt inps inps fn assertEqual out matmul_with_op inps inps fn atol rtol = None None device == cpu FileCheck check_not cpp_fused run source_codes FileCheck check_not triton jit run source_codes test dtype conversion lowp_dtype torch float torch bfloat is_dtype_supported lowp_dtype continue inps = torch rand device=self device dtype=lowp_dtype _ range fn fns out source_codes = run_and_get_code foo_opt inps inps fn assertEqual out matmul_with_op inps inps fn atol=atol rtol=rtol test broadcasted shape bail fn = lambda x x + torch zeros noqa E dtype=lowp_dtype device=self device out source_codes = run_and_get_code foo_opt inps inps fn assertEqual out matmul_with_op inps inps fn atol=atol rtol=rtol test_remove_noop_copy fn x y x = x cos = x copy_ y sin common fn torch randn torch randn fn b abs_max = torch abs max b = abs_max dtype b common fn torch randn dtype=torch float torch randn dtype=torch float test_remove_noop_clone fn x y = x clone reshape - y = y y + x common fn torch randn test_remove_noop_slice f x x = x + size = x shape - y = torch ops aten slice x - size noop y + f = torch compile f x = torch ones device=self device torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x post_grad_graph = get_post_grad_graph f x expected_graph = f \ forward arg _ Sym s arg _ Sym s arg _ Sym s arg _ f s s s s s s str x device add f s s s s s s str x device = torch ops aten add Tensor arg _ arg _ = None add_ f s s s s s s str x device = torch ops aten add Tensor add add = None add_ noqa B assertExpectedInline post_grad_graph expected_graph ignore_comments=True ignore_empty_lines=True test_remove_noop_slice f x x = x + y = torch ops aten slice x - - noop y + f = torch compile f x = torch ones device=self device torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x post_grad_graph = get_post_grad_graph f x expected_graph = f \ forward arg _ Sym s arg _ Sym s arg _ f s s s str x device add f s s s str x device = torch ops aten add Tensor arg _ arg _ = None slice_ f s s s str x device = torch ops aten slice Tensor add - - add = None add_ f s s s str x device = torch ops aten add Tensor slice_ slice_ = None add_ noqa B assertExpectedInline post_grad_graph expected_graph ignore_comments=True ignore_empty_lines=True test_remove_noop_slice_scatter f x x = x + y = torch empty_like x size = x shape - out = torch ops aten slice_scatter y x - size noop out + f = torch compile f x = torch ones device=self device torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x post_grad_graph = get_post_grad_graph f x expected_graph = f \ forward arg _ Sym s arg _ Sym s arg _ Sym s arg _ f s s s s s s str x device empty f s s s s s s str x device = torch ops aten empty memory_format arg _ arg _ arg _ dtype = torch float layout = torch strided device = repr x device pin_memory = False arg _ = arg _ = arg _ = None permute f s s s s s s str x device = torch ops aten permute default empty empty = permute = None add f s s s s s s str x device = torch ops aten add Tensor arg _ arg _ = None add_ f s s s s s s str x device = torch ops aten add Tensor add add = None add_ noqa B assertExpectedInline post_grad_graph expected_graph ignore_comments=True ignore_empty_lines=True test_cat_of_loops_and_extern_kernel M torch nn Module __init__ kwargs super __init__ conv = torch nn Conv d kwargs max_pool d = torch nn MaxPool d forward x y x = conv x y = max_pool d y torch cat x y mod = M opt_mod = torch compile mod backend= inductor memory_format = torch channels_last inputs = torch randn memory_format=memory_format torch randn memory_format=memory_format y = mod inputs opt_y = opt_mod inputs assertEqual y opt_y assertEqual y stride opt_y stride test_cat_inplace fn x rt = torch cat x v = x sin_ rt can t use common because input modified inplace inp = torch ones opt_fn = torch compile fn res = opt_fn inp clone expected = fn inp clone assertEqual res expected test_stack fn b torch stack expand b expand common fn torch randn torch randn test_hardtanh fn x F hardtanh x F hardtanh x + F hardtanh x - common fn torch randn test_hardsigmoid fn x F hardsigmoid x F hardsigmoid x + F hardsigmoid x - common fn torch randn test_hardswish fn x F hardswish x F hardswish x + F hardswish x - common fn torch randn test_rsqrt fn x torch rsqrt x torch rsqrt x + - common fn torch randn test_expm fn x torch expm x torch expm x dtype torch float torch float torch double torch int torch int is_dtype_supported dtype continue common fn torch randn dtype=dtype common fn torch arange - e- e- e- dtype=dtype xfail_if_mps_unimplemented test_adaptive_pool_errors_with_long Model torch nn Module __init__ pool_operator super __init__ pool = pool_operator forward x x = torch argmax x dim= x = pool x x dim op_inst = eval f torch nn AdaptiveMaxPool dim d model = Model op_inst device x = torch randn dim + device model = torch compile model fullgraph=True assertRaisesRegex RuntimeError r implemented &#124; aoti_torch_ model x xfail_if_mps_unimplemented test_adaptive_avg_pool_errors_with_long Model torch nn Module __init__ pool_operator super __init__ pool = pool_operator forward x x = torch argmax x dim= x = pool x x dim op_inst = eval f torch nn AdaptiveAvgPool dim d model = Model op_inst device x = torch randn dim + device model = torch compile model fullgraph=True assertRaisesRegex RuntimeError r implemented &#124; aoti_torch_ model x torch _dynamo config patch recompile_limit= test_avg_pool_errors_with_uint dim dtype torch uint torch uint torch uint torch uint x = torch randn dim + dtype op = eval f torch nn functional avg_pool dim d c_op = torch compile op assertRaisesRegex RuntimeError r implemented &#124; aoti_torch_ c_op x kernel_size= stride= test_replication_pad_errors_with_bool dim fn x x = torch signbit x x = eval f nn ReplicationPad dim d padding= x x c_fn = torch compile fn x = torch randn dim + assertRaisesRegex RuntimeError r implemented &#124; aoti_torch_ c_fn x test_log p fn x torch log p x torch log p x dtype torch float torch float torch double torch int torch int is_dtype_supported dtype continue common fn torch randn dtype=dtype common fn torch arange - e- e- e- dtype=dtype config patch force_disable_caches=True skip_if_cpp_wrapper run_and_get_kernels issue test_deterministic_codegen cpu str device config is_fbcode raise unittest SkipTest cpp packaging wacky fbcode torch compile fullgraph=True x x cos sin softmax - torch compile fullgraph=True b x x sin cos softmax - torch compile fullgraph=True c x x cos sin softmax - x = torch randn device=self device _ coda_a = _run_and_get_stripped_kernels x _ coda_b = _run_and_get_stripped_kernels b x _ coda_c = _run_and_get_stripped_kernels c x assertEqual coda_a coda_c compile different order torch compiler reset _ coda_c = _run_and_get_stripped_kernels c x _ coda_a = _run_and_get_stripped_kernels x _ coda_b = _run_and_get_stripped_kernels b x assertEqual coda_a coda_a assertEqual coda_b coda_b assertEqual coda_c coda_c force different CompileId torch compiler reset CompileContext_init = CompileContext __init__ patch object CompileContext __init__ lambda _ CompileContext_init CompileId frame_id= frame_compile_id= _ coda_a = _run_and_get_stripped_kernels x _ coda_c = _run_and_get_stripped_kernels c x _ coda_b = _run_and_get_stripped_kernels b x assertEqual coda_a coda_a assertEqual coda_b coda_b assertEqual coda_c coda_c config patch force_disable_caches=True skip_if_cpp_wrapper run_and_get_kernels issue test_deterministic_codegen_on_graph_break cpu str device config is_fbcode raise unittest SkipTest cpp packaging wacky fbcode x x cos sin softmax - torch compile b x x = x torch _dynamo graph_break x = x x x = torch randn device=self device _ code code = _run_and_get_stripped_kernels b x assertEqual code code config patch force_disable_caches=True Test expects single fused kernel generated max_autotune_gemm_backends= ATEN skip_if_cpp_wrapper run_and_get_kernels issue unittest skipIf config triton native_matmul matmul now generated test_deterministic_codegen_with_suffix cpu str device config is_fbcode raise unittest SkipTest cpp packaging wacky fbcode torch compile fullgraph=True x x cos sin softmax - torch compile fullgraph=True b x y x = x cos sin softmax - x = torch matmul x y x x = torch randn device=self device y = torch randn device=self device _ code = _run_and_get_stripped_kernels x _ code = _run_and_get_stripped_kernels b x y assertEqual code code test_flip fn x torch flip x - torch flip x - common fn torch randn test_signbit fn x torch signbit x ~torch signbit -x common fn torch randn test_sign_dtype fn x y = torch sign x torch tanh y common fn torch randn xfail_if_triton_cpu test_fmod fn b torch fmod b torch fmod b - shape = common fn torch randn shape torch randn shape xfail_if_triton_cpu test_fmod_zero_dim fn b torch fmod b common fn make_tensor device=self device dtype=torch float make_tensor device=self device dtype=torch float common fn make_tensor device=self device dtype=torch float make_tensor device=self device dtype=torch float skip_if_halide log implemented halide test_log fn x torch log x torch log x + - common fn torch randn + test_logsumexp fn x torch logsumexp x - torch logsumexp x - common fn torch randn + skip_if_halide log implemented halide test_log_fp fn x torch log x torch log x _dtype = torch float ctx = contextlib nullcontext is_dtype_supported _dtype assertRaises TypeError ctx common fn torch randn dtype=_dtype + test_bitwise fn x y torch bitwise_not x torch bitwise_or x y torch bitwise_xor x y torch bitwise_and x y common fn torch randint dtype=torch int torch randint dtype=torch int test_bitwise again bool types fn x y torch bitwise_not x torch bitwise_or x y torch bitwise_xor x y torch bitwise_and x y common fn torch randint dtype=torch bool torch randint dtype=torch bool test_bitwise Repro https github com pytorch pytorch issues fn x y torch max torch bitwise_and x y y torch clamp_max torch bitwise_or x y y torch clamp_min torch bitwise_xor x y y common fn torch rand torch int torch rand torch int test_inf fn + float inf + float -inf -float inf common fn torch randn test_remainder fn b torch remainder b torch remainder + b - torch remainder - b + common fn torch randn torch randn test_zeros fn + torch zeros dtype=torch float device=a device torch zeros dtype=torch float device=a device torch zeros + torch ones device=a device torch full device=a device common fn torch randn test_new_ones fn aten new_ones device=a device dtype= layout= pin_memory=False aten new_zeros device=a device dtype= layout= pin_memory=False common fn torch randn test_full_like fn torch full_like - common fn torch randn test_full_like_transposed fn torch full_like common fn torch randn transpose - exact_stride=True test_full_like_sliced fn torch full_like common fn torch rand exact_stride=True test_full_truncation fn + torch full_like dtype all_types ctx = contextlib nullcontext is_dtype_supported dtype assertRaises TypeError ctx common fn make_tensor dtype=dtype device=self device check_lowp=False test_full_boolean fn n x = torch full n = device=self device x x + common fn common fn test_index fn b c aten index b c common fn torch randn torch tensor dtype=torch int torch tensor dtype=torch int common fn torch randn torch tensor dtype=torch int torch tensor dtype=torch int test_index fn b aten index b aten index None b common fn torch randn torch tensor dtype=torch int test_index fn x ia ib x ia None ib common fn torch randn torch tensor dtype=torch int torch tensor dtype=torch int test_output_strides fn x y = x permute contiguous torch _dynamo graph_break y view - inp = torch rand device=self device fn_opt = torch compile fn backend= inductor assertEqual fn inp fn_opt inp assertEqual fn inp stride fn_opt inp stride no redundant copy foo x x T squeeze foo_opt = torch compile foo backend= inductor out = foo_opt inp assertEqual inp storage out storage test_index_select fn b torch index_select b torch index_select b torch index_select torch index_select b b ind_dtype torch int torch int common fn torch randn torch tensor dtype=ind_dtype xfail_if_mps_unimplemented skipCUDAIf TEST_CUDNN CUDNN available skipIfXpu skipIfRocm test_cudnn_rnn device == cpu raise unittest SkipTest f requires GPU_TYPE fn b b b b b b b b b b b b b b b b = b b b b b b b b b b b b b b b b aten _cudnn_rnn False False True None common fn torch randn torch randn torch randn torch randn torch randn torch randn torch randn torch randn torch randn torch randn torch randn torch randn torch randn torch randn torch randn torch randn torch randn torch randn torch randn torch randn check_lowp=False difference rnn too large between half float inputs test_upsample_nearest d fn aten upsample_nearest d None aten upsample_nearest d None aten upsample_nearest d None aten upsample_nearest d None aten upsample_nearest d None common fn torch randn test_upsample_nearest d fn aten upsample_nearest d aten upsample_nearest d aten upsample_nearest d aten upsample_nearest d aten upsample_nearest d None common fn torch randn test_upsample_nearest d fn aten upsample_nearest d None aten upsample_nearest d None aten upsample_nearest d None aten upsample_nearest d None aten upsample_nearest d None common fn torch randn test_upsample_nearest d_backward func = torch ops aten upsample_nearest d_backward fn func output_size= input_size= func output_size= input_size= func output_size= input_size= func output_size= input_size= func output_size= input_size= common fn torch randn skip_if_x _mac test_upsample_bilinear d_a fn aten upsample_bilinear d False None aten upsample_bilinear d None True common fn torch randn atol= e- rtol= e- test_upsample_bilinear d_b fn aten upsample_bilinear d None True common fn torch randn atol= e- rtol= e- skip_if_gpu_halide accuracy issue test_reflection_pad d fn pad aten reflection_pad d aten reflection_pad d pad common fn torch randint size= dtype=torch float xfail_if_mps test_reflection_pad d_backward template size padding fn grad_output x aten reflection_pad d_backward grad_output x padding x = torch randint size=size dtype=torch float result = aten reflection_pad d x padding grad_output = torch randn_like result common fn grad_output x check_lowp=not is_halide_backend device template template template template - template - template - template - xfail_if_mps_unimplemented Unsupported Border padding mode test_grid_sampler_ d fn b aten grid_sampler_ d b True aten grid_sampler_ d b False common fn torch randn dtype=torch float torch rand dtype=torch float - check_lowp=False Mismatched elements Greatest absolute difference index up e- allowed Greatest relative difference index up e- allowed atol= rtol= e- requires_gpu test_grid_sampler_expand_preserves_view device startswith cuda skipTest requires CUDA torch manual_seed torch _dynamo reset repeats = batch = channels = img = grid_size = device = device ExpandGridSampler torch nn Module __init__ - None super __init__ grid = torch nn Parameter torch randn repeats grid_size grid_size device=device fc = torch nn Linear grid_size grid_size channels device forward x torch Tensor - torch Tensor per_channel = i range channels channel = x i expand repeats - - - patch = torch nn functional grid_sample channel grid mode= bilinear align_corners=False padding_mode= border patch = patch transpose flatten start_dim= per_channel append patch x = torch cat per_channel dim= fc x model = ExpandGridSampler device compiled = torch compile model backend= inductor inp = torch randn batch channels img img device=device out = compiled inp out sum backward assertIsNotNone model grid grad test_upsample_bicubic d fn aten upsample_bicubic d True aten upsample_bicubic d False Mismatched elements Greatest absolute difference e- index up e- allowed Greatest relative difference index up e- allowed common fn torch randn dtype=torch float atol= e- rtol= e- test_float_index_expression Test index propagation doesn t generate bad index_expr calls like ops index_expr x dtype where expression integral fn x aten upsample_bicubic d x False x = torch randn dtype=torch float device=self device _ source_codes = run_and_get_code fn x pattern = r \ \ ix \d code source_codes assertIsNone re search pattern code msg= Found bad index_expr code \n + code test_float_index_expression_type_promotion Test float indexing expressions participate type promotion fn x x + x size x = torch arange common fn x test_sort fn descending torch sort inp = torch randint size= dtype=torch float common fn inp False common fn inp True parametrize stable True False parametrize descending True False test_nan_sort descending stable test_sort x descending stable out = torch sort x descending=descending stable=stable stable out non stable idx may equal out tensor = torch tensor - torch nan torch nan - - device=self device inps = tensor descending stable = torch compile test_sort inps b = test_sort inps assertEqual b equal_nan=True test_sort_stable fn descending sort dim=- stable=True descending=descending Duplicates give deterministic indices when stable sorting inp = torch rand dtype=torch float inp = inp = common fn inp False common fn inp True Non-power two inp = inp common fn inp False common fn inp True test_sort_bool fn descending torch sort torch int stable=True descending=descending inp = torch randint size= dtype=torch bool common fn inp False common fn inp True skipIfWindows msg= Crash UT test_sort_transpose fn descending torch sort stable=True descending=descending MPS has correctness problem transposed sort before MacOS ctx = contextlib nullcontext device = mps MACOS_VERSION = assertRaises AssertionError inp = torch randn transpose ctx common fn inp False common fn inp True test_topk fn torch topk - common fn torch randint size= dtype=torch float test_long_tensor fn torch LongTensor device - torch as_tensor device + common fn torch randint size= skip_if_gpu_halide correctness issue test_constant_pad_ d fn aten constant_pad_nd aten constant_pad_nd common fn torch randint size= dtype=torch float test_constant_pad_fill_dtype fn b aten constant_pad_nd b aten constant_pad_nd b common fn torch randint dtype=torch bool torch ones dtype=torch bool skip_if_gpu_halide misaligned address test_constant_pad_ d fn aten constant_pad_nd aten constant_pad_nd common fn torch randint size= dtype=torch float test_constant_pad_ d_strides_nonpositive fn torch constant_pad_nd - common fn torch empty_strided dtype=torch float skip_if_gpu_halide misaligned address test_constant_pad_ d fn aten constant_pad_nd aten constant_pad_nd common fn torch randint size= dtype=torch float test_constant_pad_float Repro https github com pytorch pytorch issues fn input v = torch nn functional pad input pad= torch gt v input _dtype = torch float ctx = contextlib nullcontext is_dtype_supported _dtype assertRaises TypeError x = torch rand dtype=_dtype ctx common fn x test_constant_pad_nd_inplace fn aten constant_pad_nd x = torch randn device=self device fn_compiled = torch compile fn y = fn_compiled x assertTrue y x test_l _loss fn b torch nn functional l _loss b torch nn functional mse_loss b common fn torch randn torch randn check_lowp=False test_triu fn aten triu aten triu aten triu common fn torch randn test_no_op_reduction fn sum - torch amax + keepdim=True common fn torch randn test_inplace_add torch compile backend= inductor fn x y x add_ y inputs = rand_strided device=self device rand_strided device=self device inp_clone = inputs clone out = fn inputs assertTrue same out inp_clone + inputs assertTrue out inputs The following tests meant check logic drops xmask triton load store xnumel = requires_gpu test_single_elem fn b = + b common fn torch randn requires_gpu test_single_elem_indirect fn b c = b + c = torch randn b = torch tensor dtype=torch int common fn b This test meant check issues logic drops xmask trito load store XBLOCK divides xnumel requires_gpu test_xblock_divides_xnumel fn b = + b assumption XBLOCK always divisor so xmask will dropped iff xnumel multiple common fn torch randn common fn torch randn test_inplace_mixed_dtype_ops torch compile backend= inductor fn x y z = x + y float w = z add_ y w mul_ y tgt_dtype = torch double device = mps torch half inputs = rand_strided device=self device dtype=torch float rand_strided device=self device dtype=tgt_dtype out = fn inputs out_eager = inputs + inputs float add_ inputs mul_ inputs assertTrue same out out_eager config patch triton unique_kernel_names True triton descriptive_names False test_kernel_names torch compile backend= inductor fn x x inputs = rand_strided device=self device assertTrue same fn inputs inputs config patch triton cudagraphs True dynamo_config patch automatic_dynamic_shapes=True test_strided_inputs torch compile backend= inductor fn x y x + y inputs = rand_strided device=self device rand_strided device=self device assertTrue same fn inputs inputs + inputs config patch triton cudagraphs True dynamo_config patch automatic_dynamic_shapes=True test_input_mutation fn b = + copy_ b c = + b c arg = torch randn device=self device arg = arg clone arg = torch randn device=self device arg = arg clone correct = fn arg correct = fn arg opt_fn = torch _dynamo optimize_assert compile_fx fn actual = opt_fn arg actual = opt_fn arg assertTrue same actual correct assertTrue same actual correct assertTrue same arg arg assertTrue same arg arg test_input_mutation fn b = + view copy_ torch tensor device=a device c = + b c NOTE test fails when none inputs require grad That seems like inductor bug arg = torch randn device=self device requires_grad_ True add arg = arg clone correct = fn arg opt_fn = torch _dynamo optimize_assert compile_fx fn actual = opt_fn arg assertTrue same actual correct assertTrue same arg arg test_input_mutation fn += = aten sigmoid_ = view += = aten relu_ arg = torch randn device=self device arg = arg clone correct = fn arg opt_fn = torch _dynamo optimize_assert compile_fx fn actual = opt_fn arg assertTrue same actual correct assertTrue same arg arg test_input_mutation fn torch relu_ arg = torch randn device=self device arg = arg clone correct = fn arg opt_fn = torch _dynamo optimize_assert compile_fx fn actual = opt_fn arg assertTrue same actual correct assertTrue same arg arg test_input_mutation fn x tmp = x ceil x add_ tmp opt_fn = torch compile fn = torch zeros dtype=torch int device=self device a_expect = clone expect = fn a_expect a_actual = clone actual = opt_fn a_actual assertEqual a_expect a_actual assertEqual expect actual test_slice_mutation fn x = torch zeros_like b = x + x = c = torch clone x x = d = x + x b c d common fn torch randn skip_if_gpu_halide accuracy issue test_slice_mutation fn = + = + arg = torch randn device=self device arg = arg clone fn arg opt_fn = torch _dynamo optimize_assert compile_fx fn opt_fn arg assertTrue same arg arg test_slice_mutation fn fill_ opt_fn = torch _dynamo optimize_assert compile_fx fn x = torch randn device=self device x = x clone fn x opt_fn x assertEqual x x test_tensor_index_slice fn x = torch tensor device=self device y = torch tensor device=self device xx = torch tensor device=self device view yy = torch tensor device=self device view x y x y x y x y x y xx yy xx yy xx yy xx yy xx yy = torch arange device=self device view refs = fn tests = torch compile fn ref test zip refs tests torch testing assert_close ref test torch _dynamo config patch recompile_limit= test_tensor_index_put_slice fn version x = torch tensor device=self device dtype=torch int y = torch tensor device=self device dtype=torch int xx = torch tensor device=self device view yy = torch tensor device=self device view version == x y = torch zeros_like x y version == x y = torch zeros_like x y version == x y = torch zeros_like x y version == x y = torch zeros_like x y version == x y = torch zeros_like x y version == xx yy = torch zeros_like xx yy version == xx yy = torch zeros_like xx yy version == xx yy = torch zeros_like xx yy version == xx yy = torch zeros_like xx yy version == xx yy = torch zeros_like xx yy = torch arange device=self device dtype=torch int view i range ref = fn torch clone i test = torch compile fn torch clone i torch testing assert_close ref test test_indirect_load_broadcast fn in_ptr in_ptr in_ptr torch gather in_ptr in_ptr + in_ptr arg = rand_strided device=self device dtype=torch int arg fill_ arg = rand_strided device=self device dtype=torch float common fn torch randn arg arg test_roi_align has_torchvision_roi_align raise unittest SkipTest requires torchvision fn b torch ops torchvision roi_align b False common fn torch zeros torch zeros https github com halide Halide issues config patch halide scheduler_cuda Li test_nll_loss_forward fn b aten nll_loss_forward b None - labels = torch zeros dtype=torch int torch tensor - - - - dtype=torch int inps = torch randn torch randn b zip inps labels common fn b xfail_if_mps dtypes mismatch test_nll_loss_backward fn b c aten nll_loss_backward b c None - torch tensor device=self device labels = torch zeros dtype=torch int torch tensor - - - - dtype=torch int inps = torch randn torch randn grad_outs = torch randn torch randn b c zip grad_outs inps labels common fn b c test_isinf fn x x isinf x isnan values = float inf float -inf float nan dtype torch float torch float torch half torch bfloat ctx = contextlib nullcontext is_dtype_supported dtype assertRaises TypeError ctx common fn torch tensor values dtype=dtype check_lowp=False skip_if_halide different nan behavior == test_isinf fn x y = torch tensor float inf float -inf float nan device=self device x == y common fn torch tensor float inf float -inf float nan test_any fn x x any - x isinf any torch all x isinf dim= torch all torch logical_not x isinf common fn -torch rand tmp = torch randn tmp = float inf common fn tmp skip_if_gpu_halide test_multilayer_any fn x x isinf any x isfinite all sample = torch rand common fn sample sample view - - = float inf common fn sample test_inplace_activations fn x = aten hardswish_ x + b = aten hardtanh_ x + c = aten leaky_relu_ x + d = aten silu_ x + e = aten log p x + f = aten masked_fill_ x + torch zeros_like x dtype=torch bool h = aten masked_fill_ x + torch ones_like x dtype=torch bool b c d e f h common fn torch randn test_baddbmm fn b c beta aten baddbmm b c beta=beta b = torch randn c = torch randn options = itertools product torch randn torch randn fill_ torch nan beta options common fn b c beta Mismatched elements Greatest absolute difference index up e- allowed Greatest relative difference index up allowed atol= rtol= config patch triton max_tiles test_fuse_tiled fn b c + b c + common fn torch randn torch randn torch randn test_expand_as fn b aten expand_as b aten expand_as + b + + common fn torch randn torch randn test_index_put fn b c torch index_put b c torch index_put_ + b + c + + common fn torch randn torch randperm torch randn common fn torch randn torch arange torch randn test_index_put fn b c torch index_put b c True common fn torch randn torch randint size= dtype=torch int torch randn workaround https github com triton-lang triton issues check_lowp=False test_index_put fn b c torch ops aten index_put_ None b None c = + torch ops aten index_put_ None b + None c + common fn torch randn torch arange torch randn test_index_put b broadcastable https github com pytorch pytorch issues fn b c torch index_put b c common fn torch rand torch rand torch rand test_index_put_as_masked_fill fn b c d = clone torch ops aten index_put_ b c d common fn torch randn torch randn torch randn False common fn torch randn torch randn torch randn True test_index_put_fallback fn b c d = clone torch ops aten index_put_ b c d common fn torch randn torch as_tensor True True False torch randn False common fn torch randn torch as_tensor True True False torch randn True test_index_put_fallback fn b c d e = clone torch ops aten index_put_ None b c d e common fn torch randn torch as_tensor torch as_tensor True True False torch randn False common fn torch randn torch as_tensor torch as_tensor True True False torch randn True test_index_put_deterministic_fallback DeterministicGuard True fn b c torch index_put b c True common fn torch randn torch randint size= dtype=torch int torch randn check_lowp=False skip_if_gpu_halide https github com halide Halide issues test_index_put_index fn ind x src y = torch ops aten index_put default x ind src torch ops aten index Tensor y ind args = torch tensor dtype=torch int torch randn torch randn common fn args test_index_put_reinplace fn x idx src = torch ones idx size device=x device x index_put_ idx src x expand x shape = torch randn idx = torch arange torch _inductor metrics generated_kernel_count = common fn idx assertGeneratedKernelCountEqual test_index_put_failed_reinplace fn x idx src = torch ones idx size device=x device y = x index_put idx src x y = torch randn idx = torch arange torch _inductor metrics generated_kernel_count = common fn idx assertGeneratedKernelCountEqual test_adding_tensor_offsets torch compile fullgraph=True fn x x torch no_grad x = torch randn device=self device assertEqual fn x x assertEqual fn x x + test_index_float_zero fn arg arg arg t = torch tanh arg t = t clone t fill_ arg item t = torch clamp t arg size - torch long torch nn functional embedding t arg arg = torch randint dtype=torch int device=self device arg = torch randint dtype=torch int device=self device arg = torch rand dtype=torch float device=self device cfn = torch compile fullgraph=True dynamic=True fn assertEqual fn arg arg arg cfn arg arg arg GPT ForSequenceClassification skip_if_gpu_halide test_index_tensor fn x y ne = torch ops aten ne Scalar x sum = torch ops aten sum dim_IntList ne - sub = torch ops aten sub Tensor sum iota = torch ops prims iota default start= step= dtype=torch int device=x device requires_grad=False torch ops aten index Tensor y iota sub common fn torch randn torch randn config patch fallback_random=True test_bernoulli fn b = clone aten bernoulli_ uses aten bernoulli p behind scene so will decomposed aten bernoulli_ b sum torch prod torch tensor size p = common fn torch ones p atol=p rtol= skip_if_triton_cpu test_bernoulli fn aten bernoulli sum torch prod torch tensor size p = common fn torch ones p atol=p rtol= test_narrow fn x aten narrow x aten narrow x + + aten narrow_copy x common fn torch randn test_as_strided fn x aten as_strided x aten as_strided x + + fn_channels_last x aten as_strided x aten as_strided x + + common fn torch randn common fn_channels_last torch randn memory_format=torch channels_last test_exact_stride full = torch randn device=self device view = torch as_strided full full stride fn x result = x + x result_strided = torch empty_strided x size x stride device=self device result_strided = result result_strided common fn view reference_out = fn view compiled_fn = torch compile fn actual_out = compiled_fn view assertEqual reference_out stride actual_out stride test_like_channels_last foo randn = torch randn device=self device dtype=torch float xc = randn contiguous memory_format=torch channels_last clone = torch zeros_like xc memory_format=torch preserve_format rand_like = torch rand_like randn xc clone rand_like out = foo out_comp = torch compile foo t t_comp zip out out_comp assertEqual t stride t_comp stride test_as_strided_scatter fn b aten as_strided_scatter + b - size= shape shape stride= shape storage_offset= common fn torch randn torch randn test_select_scatter fn x b aten select_scatter x aten select_scatter x b common fn torch randn torch randn torch randn skip_if_gpu_halide accuracy issue test_slice_scatter fn x aten slice_scatter x - aten slice_scatter x - common fn torch randn torch randn test_slice_scatter fn b aten slice_scatter b common fn torch randn torch randn test_slice_scatter fn b aten slice_scatter default b common fn torch randn torch randn test_slice_scatter fn b aten slice_scatter default b common fn torch randn torch randn test_slice_scatter empty slices require clamping start end fn b aten slice_scatter default b aten slice_scatter default b shape shape + aten slice_scatter default b - aten slice_scatter default b - - = torch arange dtype=torch float b = torch empty common fn b with_tf _off test_slice_scatter_reinplace M nn Module __init__ device super __init__ linear = nn Linear bias=False cache_k = torch zeros device=device forward x start_pos bsz seqlen _ _ = x shape xk = linear x torch no_grad cache_k bsz start_pos start_pos + seqlen = xk keys = cache_k bsz start_pos + seqlen scores = torch matmul xk transpose keys transpose transpose scores kv_cache_module = M device inp = torch randn Test cache update reinplaced such cache updated inplace rather than copy-scatter-copy-back torch _inductor metrics generated_kernel_count = torch no_grad common kv_cache_module inp check_lowp=False config triton native_matmul config cuda_backend == triton device == cuda assertGeneratedKernelCountEqual assertGeneratedKernelCountEqual skipIfMPS test_slice_scatter_dtype_consistency Test dtype consistency slice_scatter fn x y torch slice_scatter y x dtype torch int torch float common fn torch tensor dtype=dtype torch tensor dtype=torch float skip_if_gpu_halide compile error gpu test_scatter fn dim index b aten scatter dim index b common fn torch zeros - torch tensor torch ones test_scatter device == cuda raise unittest SkipTest unstable sm check_lowp = True device == xpu check_lowp = False fn dim index b aten scatter reduce dim index b reduce= add common fn torch zeros torch zeros dtype=torch int torch ones check_lowp=check_lowp test_scatter fn dim index b aten scatter dim index b reduce= add check_lowp = True device == xpu check_lowp = False common fn torch randn torch tensor src can scalar Mismatched elements Greatest absolute difference index up e- allowed Greatest relative difference index up allowed atol= e- rtol= e- check_lowp=check_lowp test_scatter fn x ind src torch scatter x ind src check_lowp = True device == xpu check_lowp = False deterministic False True DeterministicGuard deterministic common fn torch randn torch randint torch randn check_lowp=check_lowp test_scatter fn dim index b reduce = clone scatter_ dim index b reduce=reduce = + scatter_ dim index b reduce=reduce check_lowp = True device == xpu check_lowp = False reduce add multiply common fn torch ones torch tensor dtype=torch int torch randn reduce check_lowp=check_lowp test_scatter fn dim index b aten scatter dim index b check_lowp = True device == xpu check_lowp = False deterministic False True DeterministicGuard deterministic common fn torch randn torch tensor src can scalar check_lowp=check_lowp unittest skip Flaky test needs debugging test_scatter_add fn dim index b aten scatter_add dim index b check_lowp = True device == xpu check_lowp = False common fn torch randn torch tensor torch randn check_lowp=check_lowp test_scatter_add fn dim index b aten scatter_add dim index b check_lowp = True device == xpu check_lowp = False common fn torch randn torch tensor torch randn check_lowp=check_lowp test_scatter_add fn dim index b aten scatter_add dim index b check_lowp = True device == xpu check_lowp = False deterministic False True deterministic device == xpu There no deterministic implementation scatter_add Intel GPU continue DeterministicGuard deterministic common fn torch randn torch tensor torch randn check_lowp=check_lowp test_scatter_reduce fn dim index b aten scatter_reduce dim index b sum check_lowp = True device == xpu check_lowp = False common fn torch randn torch tensor torch randn check_lowp=check_lowp test_scatter_reduce fn dim index b reduce aten scatter_reduce dim index b reduce include_self=False check_lowp = True device == xpu check_lowp = False reduce sum amax common fn torch randn torch zeros dtype=torch int torch randn reduce check_lowp=check_lowp test_scatter_reduce fn dim index b reduce = clone scatter_reduce_ dim index b reduce=reduce = + scatter_reduce_ dim index b reduce=reduce check_lowp = True device == xpu check_lowp = False reduce sum prod common fn torch ones torch tensor dtype=torch int torch randn reduce check_lowp=check_lowp skip_if_gpu_halide test_dense_mask_index r There will little difference reduce order between aten inductor https github com pytorch pytorch pull Absolute difference up e- allowed Relative difference e- up e- allowed kwargs = device == cpu kwargs atol = e- kwargs rtol = e- fn x y y = torch ops aten select int y z = x y z sum common fn torch randn torch randn kwargs test_empty fn torch empty common fn assert_equal=False test_empty fn aten empty common fn assert_equal=False test_new_empty fn aten new_empty common fn torch randn assert_equal=False test_empty_strided fn aten empty_strided common fn assert_equal=False test_new_empty_strided fn aten new_empty_strided common fn torch randn assert_equal=False test_dropout_trivial_ fn torch nn functional dropout True + common fn torch randn test_dropout_trivial_ fn torch nn functional dropout True + common fn torch randn config patch triton cudagraphs True dynamo_config patch automatic_dynamic_shapes=True test_dropout random seed torch manual_seed torch compile backend= inductor fn torch nn functional dropout x = torch ones device=self device dtype=torch float result = fn x assertTrue result nonzero shape assertTrue result mean item random seed torch manual_seed torch compile backend= inductor fn torch nn functional dropout True result = fn x assertTrue result nonzero shape assertTrue result mean item dynamo_config patch automatic_dynamic_shapes=True test_dropout_deterministic torch compile backend= inductor fn torch nn functional dropout True cg False True patch object config triton cudagraphs cg torch _dynamo reset x = torch ones device=self device dtype=torch float torch manual_seed = fn x clone = fn x clone = fn x clone torch manual_seed b = fn x clone b = fn x clone b = fn x clone same seed same values assertTrue torch allclose b assertTrue torch allclose b assertTrue torch allclose b different calls different values assertFalse torch allclose assertFalse torch allclose test_rand_like_deterministic torch compile backend= inductor fn torch rand_like torch rand_like x = torch ones device=self device dtype=torch float torch manual_seed = fn x clone = fn x clone = fn x clone torch manual_seed b = fn x clone b = fn x clone b = fn x clone same seed same values assertTrue torch allclose b assertTrue torch allclose b assertTrue torch allclose b different calls different values assertFalse torch allclose assertFalse torch allclose c d = fn x assertFalse torch allclose c d assertTrue c = all assertTrue c all assertTrue d = all assertTrue d all config patch implicit_fallbacks=True test_needs_contiguous_strides Construct custom op whose output strides contiguous torch library custom_op mylib myop mutates_args= myop x torch Tensor - torch Tensor torch zeros t myop register_fake _ x torch zeros t custom op needs contiguous inputs torch library custom_op mylib second_op mutates_args= tags= torch _C Tag needs_contiguous_strides second_op x torch Tensor - torch Tensor assert x is_contiguous torch ones second_op register_fake _ x torch ones f x y = myop x second_op y Check x is_contiguous assertion never gets triggered x = torch randn _ = torch compile f backend= inductor fullgraph=True x config patch implicit_fallbacks=True test_fallback_mutable_op_basic torch library _scoped_library mylib FRAGMENT m impl b c d e= add_ b c e d None d add_ b m define inplace_ Tensor Tensor b SymInt c Tensor b d SymInt e= - m impl inplace_ impl CompositeExplicitAutograd We do some clones copy_ test Inductor doesn t reorder copy_ w r t inplace_ f b b c d a_ = clone d_ = d d None d clone torch ops mylib inplace_ a_ b b c d=d_ copy_ a_ d None d copy_ d_ = torch tensor b = torch tensor torch tensor c = d = torch tensor args = b b c d cloned_args = pytree tree_map_only torch Tensor torch clone args mod = make_fx f cloned_args cloned_args = pytree tree_map_only torch Tensor torch clone args compiled_f = compile_fx_inner mod cloned_args cloned_args = pytree tree_map_only torch Tensor torch clone args compiled_f list cloned_args f args assertEqual cloned_args args skip_if_cpp_wrapper Without major redesign cpp_wrapper will support custom ops defined Python config patch implicit_fallbacks=True test_fallback_mutable_op_list_tensor torch library custom_op mylib mysin mutates_args= out_list schema= Tensor x Tensor out_list - Tensor mysin x out_list - torch Tensor r = x sin out_list None out_list copy_ r r mysin register_fake _ x out_list - torch Tensor torch empty_like x fn x x = x s = torch empty_like x x = mysin x s x = x x s x = torch randn requires_grad=False expected = fn x result = torch compile fn fullgraph=True x assertEqual result expected config patch implicit_fallbacks=True test_fallback_mutable_op_with_return torch library _scoped_library mylib FRAGMENT m impl b c d e= add_ b c e d None d add_ b b + b m define inplace_ Tensor Tensor b SymInt c Tensor b d SymInt e= - Tensor m impl inplace_ impl CompositeExplicitAutograd We do some clones copy_ test Inductor doesn t reorder copy_ w r t inplace_ f b b c d a_ = clone d_ = d d None d clone res = torch ops mylib inplace_ a_ b b c d=d_ copy_ a_ d None d copy_ d_ res = torch tensor b = torch tensor torch tensor c = d = torch tensor args = b b c d cloned_args = pytree tree_map_only torch Tensor torch clone args mod = make_fx f cloned_args cloned_args = pytree tree_map_only torch Tensor torch clone args compiled_f = compile_fx_inner mod cloned_args cloned_args = pytree tree_map_only torch Tensor torch clone args compiled_out = compiled_f list cloned_args out = f args assertEqual cloned_args args assertEqual compiled_out out config patch implicit_fallbacks=True test_fallback_mutable_op_no_mutated_tensors torch library _scoped_library mylib FRAGMENT m impl b b None b add_ m define inplace_ Tensor Tensor b b - m impl inplace_ impl CompositeExplicitAutograd f torch ops mylib inplace_ None = torch tensor args = cloned_args = pytree tree_map_only torch Tensor torch clone args mod = make_fx f cloned_args cloned_args = pytree tree_map_only torch Tensor torch clone args compiled_f = compile_fx_inner mod cloned_args cloned_args = pytree tree_map_only torch Tensor torch clone args compiled_f list cloned_args f args assertEqual cloned_args args config patch implicit_fallbacks=True skip_if_cpp_wrapper Without major redesign cpp_wrapper will support custom ops defined Python test_fallback_mutable_op_list torch library _scoped_library mylib FRAGMENT m impl b bi b bi add_ m define inplace_ Tensor Tensor b - m impl inplace_ impl CompositeExplicitAutograd f b torch ops mylib inplace_ b None = torch tensor b = torch tensor torch tensor args = b cloned_args = pytree tree_map_only torch Tensor torch clone args mod = make_fx f cloned_args cloned_args = pytree tree_map_only torch Tensor torch clone args compiled_f = compile_fx_inner mod cloned_args torch library custom_op mylib sin_out mutates_args= outs sin_out x torch Tensor outs list torch Tensor - None x_np = x numpy assert len outs == out_np = out numpy out_np = out numpy np sin x_np out=out_np np sin x_np out=out_np torch compile g x outs = torch empty_like x _ range sin_out x outs outs x = torch randn out = torch empty_like x _ range y = g x xfail_if_mps_unimplemented rng_prims supported MPS test_functionalize_rng_wrappers Ideally we would like use torch compile these operators But currently plan introduce these operators partitioner level obviating need support them fully through torch compile stack To ensure we have good enough debugging minifiers we have ensure they work make_fx This test uses make_fx do testing In future we can move torch compile fn rng_state = torch _prims rng_prims run_and_save_rng_state torch ops aten rand default dtype=torch float device=self device rng_state = torch _prims rng_prims run_and_save_rng_state torch ops aten rand default dtype=torch float device=self device b = torch _prims rng_prims run_with_rng_state rng_state torch ops aten rand default dtype=torch float device=self device b = torch _prims rng_prims run_with_rng_state rng_state torch ops aten rand default dtype=torch float device=self device b b mod = make_fx fn compiled_f = compile_fx_inner mod b b = compiled_f assertEqual b assertEqual b patch object torch _functorch config functionalize_rng_ops True expectedFailureXPU skip_if_gpu_halide rand xfail_if_mps test_philox_rand device == cpu raise unittest SkipTest f functionalization rng ops supported only GPU_TYPE torch compile backend= inductor fn x = torch rand_like x x = torch rand_like x check x torch manual_seed = fn x torch manual_seed b = fn x torch manual_seed c = fn x same seed same values assertTrue torch allclose c different calls different values assertFalse torch allclose b check torch ones device=self device dtype=torch float Need comment should we add _get_rng_state_offset common device interface assertEqual getattr torch device _get_rng_state_offset Check non-multiple numel check torch ones device=self device dtype=torch float assertEqual getattr torch device _get_rng_state_offset Already default just want make sure patch object torch _inductor config allow_buffer_reuse True test_reuse_buffers_with_aliasing f x z = x + z = torch view_as_complex z = torch view_as_real z out = + out torch view_as_real z + common f torch zeros code = run_and_get_triton_code torch compile f torch zeros Make sure we haven t added complex support made test invalid If we ve added complex support please update test use different set view ops we don t lower assertTrue aten view_as_real code f x z = x + z = torch view_as_complex z z = torch view_as_real z z = torch view_as_complex z = torch view_as_real z out = + out torch view_as_real z + common f torch zeros xfail_if_triton_cpu libdevice fma test_softmax_backward_data fn b aten _softmax_backward_data b dim= input_dtype=torch float common fn torch randn torch randn test_randn_like_empty Model torch nn Module __init__ super __init__ forward v torch Tensor vx = v min dim= values v = torch randn_like vx v model = Model x = torch rand common model x exact_stride=True test_randint torch compile fullgraph=True fn x torch randint device=x device torch randint - dtype=torch int device=x device torch randint_like x torch manual_seed b c = fn torch zeros device=self device assertEqual shape assertEqual b shape assertEqual c shape torch manual_seed b c = fn torch zeros device=self device assertEqual assertEqual b b assertEqual c c assertEqual min assertEqual max assertEqual b min - assertEqual b max assertGreaterEqual c min assertGreater c max assertLess c max test_randint_distribution torch compile fullgraph=True fn n_argsmax size torch randint n_max size device=self device bin index max_size index max_size n_bins size = _ _ n_max = int n_bins = res = fn n_max size bins = bin res n_max float cpu hist _ = bins histogram range= n_bins expected_bin = res shape expected_error = math sqrt expected_bin expected_bin error = hist - expected_bin abs max expected_bin assertTrue error expected_error config patch fallback_random=True xfail_if_mps close test_like_rands fn x torch rand_like x torch randn_like x torch randint_like x common fn torch zeros exact_stride=True config patch fallback_random=True xfail_if_mps close test_like_rands_sliced fn x torch randn_like x torch randn_like x torch randint_like x common fn torch zeros permute exact_stride=True config patch check_stack_no_cycles_TESTING_ONLY=True test_check_stack_no_cycles config cpp_wrapper device = cpu raise unittest SkipTest codegen gets called twice cpp_wrapper GPU compilation which causes test fail This can removed GPU compilation done single pass torch compile fn x x r = fn torch randn device=self device requires_grad=True Backward compilation isn t hooked into cprofile probably should r sum backward test_like_rands rand_like kwargs ` device ` str type d = device assert isinstance d str torch compile fn x torch rand_like x device=d x = torch ones device=self device dtype=torch float = fn x clone = fn x clone assertFalse torch allclose assertEqual shape shape assertEqual stride stride requires_gpu skip_if_triton_cpu Flaky Triton CPU test_like_rands rand_like ` device ` which different ` x device ` test_like_rands_on_different_device device device torch compile fn x device torch rand_like x device=device x = torch ones device=device dtype=torch float fn x device clone = test_like_rands_on_different_device cpu GPU_TYPE = test_like_rands_on_different_device GPU_TYPE cpu assertTrue device type == GPU_TYPE assertTrue device type == cpu assertEqual shape shape assertEqual stride stride test_max_pool d_with_indices_backward fn b c aten max_pool d_with_indices_backward b False c x = torch randn result indices = aten max_pool d_with_indices x False common fn torch randn_like result x indices xfail_if_mps Small tolerances bug skip_if_gpu_halide slow test_max_pool d_with_indices_backward fn b c aten max_pool d_with_indices_backward b True c x = torch randn result indices = aten max_pool d_with_indices x True common fn torch randn_like result x indices From https github com pytorch torchdynamo issues test_max_pool d_with_indices_backward fn b c aten max_pool d_with_indices_backward b False c x = torch randn result indices = aten max_pool d_with_indices x False common fn torch randn_like result x indices From https github com pytorch torchdynamo issues xfail_if_mps Small tolerances bug skip_if_halide hangs forever test_max_pool d_with_indices_backward fn b c aten max_pool d_with_indices_backward b False c torch _inductor metrics generated_kernel_count = x = torch randn result indices = aten max_pool d_with_indices x False common fn torch randn_like result x indices assertGeneratedKernelCountEqual expectedFailureXPU test_max_pool d_with_indices_backward Window size too big Should fallback fn b c aten max_pool d_with_indices_backward b False c torch _inductor metrics generated_kernel_count = x = torch randn result indices = aten max_pool d_with_indices x False common fn torch randn_like result x indices assertGeneratedKernelCountEqual From https github com pytorch pytorch issues test_max_pool d_with_indices_backward dilation Should fallback fn b c aten max_pool d_with_indices_backward b False c torch _inductor metrics generated_kernel_count = x = torch randn result indices = aten max_pool d_with_indices x False common fn torch randn_like result x indices assertGeneratedKernelCountEqual test_issue fn x x mean common fn torch rand test_avg_pool d_backward fn b aten avg_pool d_backward b True False None common fn torch randn torch randn skip_if_gpu_halide slow test_avg_pool d_backward fn b aten avg_pool d_backward b True False None common fn torch randn torch randn test_avg_pool d_backward fn b aten avg_pool d_backward b False False None torch _inductor metrics generated_kernel_count = common fn torch randn torch randn assertGeneratedKernelCountEqual test_avg_pool d_backward fn b aten avg_pool d_backward b True False None torch _inductor metrics generated_kernel_count = common fn torch randn torch randn check_lowp=False assertGeneratedKernelCountEqual test_avg_pool d_backward fn b aten avg_pool d_backward b True False None common fn torch randn torch randn skip_if_halide compiles + minutes test_avg_pool d_backward fn b aten avg_pool d_backward b True False None common fn torch randn torch randn test_avg_pool d_backward fn b aten avg_pool d_backward b False False None torch _inductor metrics generated_kernel_count = common fn torch randn torch randn assertGeneratedKernelCountEqual test_avg_pool d_backward fn b aten avg_pool d_backward b True False None torch _inductor metrics generated_kernel_count = common fn torch randn torch randn check_lowp=False assertGeneratedKernelCountEqual config patch search_autotune_cache=False test_mm_views fn b torch mm view b view common fn torch randn transpose torch randn transpose check_lowp=False config triton native_matmul config cuda_backend == triton device == cuda assertEqual torch _inductor metrics generated_kernel_count codegen mm kernel template assertEqual torch _inductor metrics generated_kernel_count torch _dynamo config patch assume_static_by_default=False test_dtype_sympy_expr torch _dynamo optimize_assert inductor fn y = - contiguous y result = fn torch randn requires_grad_ result sum backward xfail_if_mps test_dropout n = weight = torch ones n device=self device dtype=torch float requires_grad=True ones = torch ones n device=self device dtype=torch float torch _dynamo optimize_assert inductor run x train=True F dropout x weight train check r g rmean = r mean item gmean = g mean item rcount = len r nonzero gcount = len g nonzero dropped elements should match assertTrue same r nonzero g nonzero assertEqual rcount gcount dropped should close assertGreater rcount n assertGreater n rcount assertAlmostEqual rmean gmean assertAlmostEqual rmean places= r = run ones train=False r sum backward g = weight grad clone eval mode should all ones assertTrue same r torch ones_like r assertTrue same g torch ones_like g torch manual_seed weight grad zero_ r fw_code bw_code = run_fw_bw_and_get_code lambda run ones is_halide_backend device assertEqual fw_code count halide_helpers rand assertEqual bw_code count halide_helpers rand device == GPU_TYPE assertEqual fw_code count tl rand assertEqual bw_code count tl rand g = weight grad clone check r g torch manual_seed weight grad zero_ r = run ones r sum backward g = weight grad clone check r g second run same result first assertTrue same r r assertTrue same g g xfail_if_mps config patch search_autotune_cache=False unittest skipIf config triton native_matmul matmul count different test_dropout m = torch nn Sequential torch nn Linear bias=False torch nn Dropout torch nn Linear bias=False torch nn Dropout device torch _dynamo optimize_assert inductor run x m x torch _inductor metrics generated_kernel_count = result fw_code bw_code = run_fw_bw_and_get_code lambda run torch randn device=self device is_halide_backend device assertEqual fw_code count halide_helpers rand assertEqual bw_code count halide_helpers rand device == GPU_TYPE load_seed_offset arg can non- depending whether triton signature specializes vs non- you might get kernels In newer versions triton there s no specialization so we get only kernel assertEqual fw_code count tl rand assertEqual bw_code count tl rand assertEqual torch _inductor metrics generated_kernel_count xfail_if_mps Only works triton test_randint_kernel_count device = GPU_TYPE raise unittest SkipTest Only valid GPU torch _dynamo optimize_assert inductor fn random_tensor = torch randint device=self device random_tensor = torch randint device=self device random_tensor = torch randint device=self device random_tensor random_tensor random_tensor _ source_codes = run_and_get_code fn cpp_wrapper does -pass generation GPU assertEqual len source_codes load_seed_offset arg can non- depending whether triton signature specializes vs non- you might get kernels In newer versions triton there s no specialization so we get only kernel assertEqual source_codes count async_compile triton test_roll fn aten roll - aten roll common fn torch randn test_argmax_min_int https github com pytorch pytorch issues fn b c = argmax torch min b c = torch rand int b = torch rand int common fn b test_argmax_argmin fn x aten argmax x aten argmin x common fn torch randn skipIfXpu msg= Incorrect XPU reference test_argmax_argmin fn x aten argmax x aten argmin x aten argmax x aten argmin x common fn torch randn skipIfXpu msg= Incorrect XPU reference test_argmax_argmin_with_duplicates fn x aten argmax x aten argmin x aten argmax x aten argmin x Unrolled reduction t = torch randint size= common fn t Persistent reduction t = torch randint size= common fn t Non-persistent reduction t = torch randint size= common fn t skipIfXpu msg= Incorrect XPU reference xfail_if_mps eager nan wrong see https github com pytorch pytorch issues skip_if_halide nan behavior test_argmax_argmin_with_nan fn x aten argmax x aten argmin x aten argmax x aten argmin x Unrolled reduction t = torch randn t = float nan t = float nan common fn t Persistent reduction t = torch randn t = float nan t = float nan common fn t Non-persistent reduction t = torch randn t = float nan t = float nan common fn t test_conv_backward fn rank _inps rank _inps rank _inps out = aten convolution_backward rank _inps C False True True True out = aten convolution_backward rank _inps C False True False False out = aten convolution_backward rank _inps C False True True True out = aten convolution_backward rank _inps C False True True True out out out out B = C = H = grad_out = torch randn B C H - H - H - inp = torch randn B C H H H weight = torch randn C C shrink_rank x rank res = x while res dim rank res = torch select res - res contiguous rank _inps = shrink_rank x x grad_out inp weight rank _inps = shrink_rank x x grad_out inp weight rank _inps = shrink_rank x x grad_out inp weight torch backends cudnn flags enabled=True allow_tf =False common fn rank _inps rank _inps rank _inps skipIfXpu msg= Incorrect XPU reference test_argmax_argmin fn x aten argmax x aten argmin x aten argmax x - aten argmin x - common fn torch randint test_vdd_clamp fn x torch clamp_min x common fn torch randn requires_grad=True parametrize use_block_ptr subtest True decorators= skip_if_not_triton test_tmp_not_defined_issue use_block_ptr forward primals_ primals_ add_tensor convert_element_type_default div_default reciprocal_default var_default = torch ops aten var convert_element_type_default correction= sub_tensor = torch ops aten sub Tensor add_tensor div_default mul_tensor_ = torch ops aten mul Tensor sub_tensor reciprocal_default mul_tensor_ = torch ops aten mul Tensor mul_tensor_ primals_ add_tensor_ = torch ops aten add Tensor mul_tensor_ primals_ convert_element_type_default_ = add_tensor_ dtype=torch float convert_element_type_default_ = convert_element_type_default_ dtype=torch float var_default_ = torch ops aten var convert_element_type_default_ correction= broadcast_in_dim_default_ = var_default_ reshape sum_default_ = convert_element_type_default_ sum add_tensor_ = torch ops aten add Tensor broadcast_in_dim_default_ e- var_default sum_default_ add_tensor_ inps = torch Size torch float torch Size torch float torch Size torch float torch Size torch float torch Size torch float torch Size torch float inps = torch randn shape dtype=dtype shape dtype inps config patch triton use_block_ptr use_block_ptr common forward inps atol= e- rtol= e- unittest skipIf os environ get BUILD_ENVIRONMENT startswith parallelnative TODO debug asan skip_if_gpu_halide test_tmp_not_defined_issue forward arg _ arg _ getitem_ new_zeros_default_ div_tensor_ = torch ops aten div Tensor getitem_ arg _ mul_tensor_ = torch ops aten mul Tensor div_tensor_ arg _ sum_default_ = torch ops aten sum default mul_tensor_ new_zeros_default_ sum_default_ dtype = torch float args = dtype dtype dtype dtype args = rand_strided shape stride dtype requires_grad_ True add shape stride dtype args common forward args atol= e- rtol= e- xfail_if_mps_unimplemented embedding bag requires_gpu skip_if_halide cascading accuracy issues due rsqrt fallback test_tmp_not_defined_issue test_device = torch device type=self device test_device_ = torch device type=self device index= device = cpu test_device forward primals_ f primals_ f primals_ f primals_ f primals_ f primals_ f primals_ f _tensor_constant i = _tensor_constant lift_fresh_copy i = torch ops aten lift_fresh_copy default _tensor_constant index f = torch ops aten index Tensor primals_ None lift_fresh_copy _tensor_constant i = _tensor_constant lift_fresh_copy_ i = torch ops aten lift_fresh_copy default _tensor_constant index_ f = torch ops aten index Tensor primals_ None lift_fresh_copy_ primals_ = lift_fresh_copy_ = None permute f = torch ops aten permute default primals_ addmm f = torch ops aten addmm default primals_ index_ permute amax f = torch ops aten amax default addmm - True sub f = torch ops aten sub Tensor addmm amax exp f = torch ops aten exp default sub sum_ f = torch ops aten sum dim_IntList exp - True div f = torch ops aten div Tensor exp sum_ full_default i = torch ops aten full default dtype=torch int layout=torch strided device=test_device_ pin_memory=False iota i = torch ops prims iota default start= step= dtype=torch int device=test_device requires_grad=False mul i = torch ops aten mul Tensor full_default iota iota_ i = torch ops prims iota default start= step= dtype=torch int device=test_device_ requires_grad=False view i = torch ops aten reshape default mul - view_ f = torch ops aten reshape default div - _embedding_bag = torch ops aten _embedding_bag default primals_ view iota_ False False view_ getitem f = _embedding_bag getitem_ i = _embedding_bag getitem_ i = _embedding_bag getitem_ i = _embedding_bag unsqueeze f = torch ops aten unsqueeze default getitem var_mean = torch ops aten var_mean correction index correction= keepdim=True getitem_ f = var_mean getitem_ f = var_mean add f = torch ops aten add Tensor getitem_ e- rsqrt f = torch ops aten rsqrt default add sub_ f = torch ops aten sub Tensor index getitem_ mul_ f = torch ops aten mul Tensor sub_ rsqrt mul_ f = torch ops aten mul Tensor mul_ primals_ add_ f = torch ops aten add Tensor mul_ primals_ permute_ f = torch ops aten permute default primals_ index index_ addmm amax sum_ iota_ view view_ getitem_ getitem_ getitem_ unsqueeze getitem_ rsqrt add_ permute_ kwargs = aot_graph_input_parser forward device=self device common forward kwargs=kwargs skip_if_gpu_halide config patch halide scheduler_cpu Mullapudi test_misaligned_address_issue forward sub_tensor_ unsqueeze_default gather_default = torch ops aten gather default sub_tensor_ unsqueeze_default gather_default args = torch float torch int args = rand_strided shape stride dtype shape stride dtype args common forward args test_invalid_operand_issue forward arg _ arg _ arg _ squeeze view_ slice_ slice_scatter = torch ops aten slice_scatter default slice_ arg _ slice_scatter_ = torch ops aten slice_scatter default arg _ slice_scatter slice_ = torch ops aten slice Tensor slice_scatter_ select_scatter = torch ops aten select_scatter default slice_ squeeze slice_scatter_ = torch ops aten slice_scatter default slice_scatter_ select_scatter view = torch ops aten view default slice_scatter_ - embedding = torch ops aten embedding default arg _ view embedding view_ args = torch float torch int torch int torch int torch int torch int args = rand_strided shape stride dtype shape stride dtype args common forward args test_sizehint_issue forward x torch nn functional unfold x kernel_size= dilation= padding= stride= args = torch float False args = rand_strided sh st dt requires_grad_ rg sh st dt rg args common forward args test_zero_dim_reductions kd True False inps = torch zeros device=self device dtype=torch float kd failed_ops = aten argmin aten argmax aten max aten min op failed_ops assertRaisesRegex IndexError Expected reduction dim have non-zero size mod = make_fx op inps _ = compile_fx_inner mod inps pass_ops = lambda x fn x fn aten sum aten prod aten any aten all op pass_ops compiled = torch compile op backend= inductor expected = op inps actual = compiled inps assertTrue torch allclose actual expected atol= e- rtol= e- test_unfold_zero_dimension_tensor forward x torch unfold_copy dimension= input=x size= step= x = torch rand dtype=torch float y = forward x compiled_y = torch compile forward fullgraph=True x assertEqual y compiled_y test_zero_element_mutation CustomModel nn Module __init__ - None super __init__ layer = nn LeakyReLU negative_slope= inplace=True forward inputs layer inputs ip_size = input_tensor = torch randn ip_size mymodel = CustomModel common mymodel input_tensor test_lerp non-contiguous inputs lerp fn i i x = i transpose - - torch lerp i x contiguous inputs lerp fn i i torch lerp i i common fn torch rand torch rand common fn torch rand torch rand parametrize dtype test_dtypes test_unspec_inputs dtype device == cpu raise unittest SkipTest Testing mixed devices is_halide_backend device getattr device type device == cuda https github com halide Halide issues raise unittest SkipTest halide supported is_dtype_supported dtype raise unittest SkipTest f dtype dtype supported device device fn x y x + y x y x y opt = torch compile fn backend= inductor inputs = rand_strided dtype=torch float device=GPU_TYPE rand_strided dtype=dtype device= cpu assertTrue same opt inputs fn inputs inputs = inputs inputs assertTrue same opt inputs fn inputs dynamo_config patch automatic_dynamic_shapes=True test_list_clearing device == cpu contexts = contextlib nullcontext contexts = contextlib nullcontext lambda config patch triton cudagraphs True context contexts context inps = torch rand device torch rand device inp_refs = weakref ref inp inp inps fn x y = x + y fn_fx = make_fx fn inps inps fn_compiled = compile_fx_inner fn_fx inps test_self = matmul_seen = False TestRefMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None kwargs = kwargs kwargs nonlocal inps nonlocal inp_refs nonlocal test_self nonlocal matmul_seen matmul inputs should deallocated TODO should necessary ref-cycle gc collect func aten mm out matmul_seen = True test_self assertEqual len inps test_self assertIsNone inp_refs test_self assertIsNone inp_refs func args kwargs TestRefMode fn_compiled inps do extra run make sure we deallocating warmup record device == GPU_TYPE inps extend torch rand device torch rand device inp_refs extend weakref ref inp inp inps matmul_seen = False TestRefMode fn_compiled inps some reason TorchDispatch doesn t capture cuda mm call even without cudagraphs device == cpu assertTrue matmul_seen assertEqual len inps test_dtype_mismatch_issue fn x attn = torch nn functional pad x attn softmax dim=- x = torch rand common fn x test_vectorized_ops_masked fn x index = torch arange device=x device mask = index view indices = None None index torch ops aten _unsafe_masked_index x mask indices x = torch rand common fn x xfail_if_mps test_vectorized_ops_masked_var_novec fn x index = torch arange device=x device mask = index view indices = None None None index torch ops aten _unsafe_masked_index x mask indices x = torch rand common fn x test_diagonal_copy fn x torch diagonal_copy x x torch randn torch randn torch randn common fn x test_copy_with_scalar_src fn x buffer = torch zeros_like x buffer copy_ result = x + buffer result x = torch randn dtype=torch float device=self device common fn x test_kwargs device == GPU_TYPE raise unittest SkipTest histogramdd only supports cpu fn x y torch histogramdd x bins= weight=y common fn torch randn torch randn Shape padding causes inputs all get specialized so codegen test fails expectedFailureCodegenDynamic requires_gpu torch _inductor config patch shape_padding True test_shape_padding dtypes = torch float torch float b m n k = gen shape dtype=torch float torch randn shape device=GPU_TYPE dtype=dtype k + dtype dtypes x = gen m k dtype=dtype y = gen k n dtype=dtype z = gen n dtype=dtype common lambda x y torch mm x y x y common lambda x y torch matmul x y x y common lambda x y z torch addmm z x y x y z dtype dtypes x = gen b m k dtype=dtype y = gen b k n dtype=dtype z = gen n dtype=dtype common lambda x y torch bmm x y x y common lambda x y torch matmul x y x y common lambda x y z torch baddbmm z x y x y z requires_gpu torch _inductor config patch layout_optimization True tf _on_and_off test_inductor_layout_optimization_input_mutations channel dim must inductor do layout optimization use NHWC mod = nn Conv d stride= bias=False device f x x mul_ out = mod x out f_compiled = torch compile f x_ref = torch rand device=self device x_test = x_ref detach clone torch no_grad out_ref = f x_ref out_test = f_compiled x_test assertEqual out_ref out_test assertEqual out_ref shape out_test shape Importantly since inductor _config keep_output_stride True outputs should have matching strides here assertEqual out_ref stride out_test stride assertEqual x_ref x_test requires_gpu skip_if_not_triton unittest skipIf IS_BIG_GPU Skipping triton backend only since big GPU enough SM test_inductor_multiple_specializations torch compile options= max_autotune True max_autotune_gemm_backends TRITON dynamic=False inductor_matmul b torch _check shape == b shape m torch mm b m = k = dynamic_a = torch randn m k device=GPU_TYPE dtype=torch bfloat dynamic_specialized_a = dynamic_a clone b = torch randn k m device=GPU_TYPE dtype=torch bfloat torch _dynamo decorators mark_dynamic dynamic_a torch _dynamo decorators mark_dynamic dynamic_specialized_a specialize_on= lambda x x == torch _dynamo decorators mark_dynamic b dynamic = inductor_matmul dynamic_a b torch _dynamo reset dynamic_specialized = inductor_matmul dynamic_specialized_a b assertEqual dynamic dynamic_specialized requires_gpu skip_if_not_triton unittest skipIf IS_BIG_GPU Skipping triton backend only since big GPU enough SM config patch force_disable_caches True test_mark_dynamic_with_hint_override torch compile no_override x x sum dim= torch compile override x x sum dim= x_small = torch randn device=GPU_TYPE torch _dynamo decorators mark_dynamic x_small code = run_and_get_triton_code no_override x_small torch _dynamo reset_code_caches torch _dynamo decorators mark_dynamic x_small hint_override= code = run_and_get_triton_code override x_small assertNotEqual code code assertEqual no_override x_small override x_small requires_gpu skip_if_not_triton unittest skipIf IS_BIG_GPU Skipping triton backend only since big GPU enough SM config patch force_disable_caches True test_mark_unbacked_with_hint_override torch compile no_override x x sum dim= torch compile override x x sum dim= torch compile fullgraph=True branching x x shape x_small = torch randn device=GPU_TYPE torch _dynamo decorators mark_unbacked x_small code = run_and_get_triton_code no_override x_small torch _dynamo reset_code_caches torch _dynamo decorators mark_unbacked x_small hint_override= code = run_and_get_triton_code override x_small assertNotEqual code code assertEqual no_override x_small override x_small assertRaisesRegex RuntimeError Could guard data-dependent expression branching x_small requires_gpu test_stride_preservation_with_stride_modifying_fx_pass f x x + custom_pass g torch fx Graph - None Applies ` lambda x x t contiguous t ` output output_node = g find_nodes op= output assert len output_node args == output = output_node args g inserting_before output_node output = g call_function torch ops aten permute default args= output output = g call_function torch ops aten clone default args= output kwargs= memory_format torch contiguous_format output = g call_function torch ops aten permute default args= output output_node args = output g config patch post_grad_custom_post_pass=custom_pass f_compiled = torch compile f x = torch rand device=GPU_TYPE y = f x y_compiled = f_compiled x assertEqual y y_compiled assertEqual y stride y_compiled stride test_int_input_dynamic_shapes torch compile dynamic=True fn x i y = x i y Constant must get matched constant common fn torch randn test_float_repr_dynamic_shapes torch compile dynamic=True fn x F interpolate x scale_factor= mode= linear common fn torch randn torch _dynamo config patch capture_scalar_outputs True test_pattern_matcher_unbacked torch compile fullgraph=True get_mask W torch Tensor percentage_nonzeros torch Tensor total_elements = W numel k = total_elements percentage_nonzeros top_k_indices = torch topk torch abs W flatten k int mask = torch zeros total_elements dtype=torch bool device=W device mask scatter_ top_k_indices True mask = mask view W shape mask x = torch randn device=self device p = torch tensor device=self device get_mask x p test_flexible_layout_immutable_free_symbols sympy x = sympy Symbol x y = sympy Symbol y z = sympy Symbol z layout = torch _inductor ir FlexibleLayout device torch float size= x y pad_strides works since does add new symints layout pad_strides same symints different order should work layout size = y x adding new symints should fail assertRaisesRegex AssertionError Expected free symbols unchanged got layout size = z test_sqrt_dynamic_shapes TIMM convit_base model https github com pytorch pytorch issues TODO support cuda path device == GPU_TYPE raise unittest SkipTest sqrt dynamic shapes only supports cpu Model torch nn Module __init__ - None super __init__ forward x B N C = x shape get_rel_indices N get_rel_indices num_patches int - torch Tensor img_size = int num_patches ind = torch arange img_size ind common Model torch randn test_rsqrt_dynamic_shapes From HF hf_BigBird model torch compile dynamic=True fn b r = math sqrt size torch bmm b r common fn torch randn torch randn xfail_if_triton_cpu test_index_dynamic_shapes Repro vision_maskrcnn fn arg _ unsqueeze = arg _ unsqueeze sym_size = arg _ size ceil = math ceil sym_size iota = torch ops prims iota default ceil start= step= dtype=torch int device=arg _ device requires_grad=False convert_element_type_ = iota torch float sym_size_ = arg _ size floor_ = math floor sym_size_ ceil_ = math ceil floor_ iota_ = torch ops prims iota default ceil_ start= step= dtype=torch int device=arg _ device requires_grad=False convert_element_type_ = iota_ torch float sub_ = convert_element_type_ + sym_size ceil - clamp_min = sub_ clamp_min sub_ = convert_element_type_ + sym_size_ floor_ - clamp_min_ = sub_ clamp_min convert_element_type_ = clamp_min torch int sub_ = sym_size - clamp_max = clamp_min ceil clamp_max sub_ convert_element_type_ = clamp_max torch int convert_element_type_ = clamp_min_ torch int unsqueeze_ = convert_element_type_ unsqueeze index = torch ops aten index Tensor unsqueeze None None unsqueeze_ convert_element_type_ index_ = torch ops aten index Tensor unsqueeze None None convert_element_type_ unsqueeze convert_element_type_ sub_ = clamp_min unsqueeze - unsqueeze_ mul_ = index - sub_ + index_ sub_ - clamp_min_ - convert_element_type_ select = torch ops aten select int mul_ select x = torch randn common fn x skip_if_halide log yet implemented skip_if_triton_cpu log implemented only Dec test_pow_by_natural_log _dynamic_shapes torch compile dynamic=True fn x x + math floor math log x shape + common fn torch randn test_setitem_with_int_parameter x = torch zeros device=self device fn n n = - cnts = CompileCounterWithBackend inductor opt_fn = torch compile fn backend=cnts fullgraph=True n range x shape opt_fn n x assertEqual x n - If assume_static_by_default set calls above will trigger function compilation assuming n static equals making n dynamic guard end = x shape torch _inductor ir SliceView create frame_count = torch _dynamo config assume_static_by_default assertEqual cnts frame_count frame_count Negative index triggers new compilation opt_fn -x shape x assertEqual x - assertEqual cnts frame_count frame_count + config patch triton autotune_at_compile_time False config patch profiler_mark_wrapper_call=True test_profiler_mark_wrapper_call torch profiler profile torch compile backend= inductor fullgraph=True fn b + b = torch rand device=self device b = torch rand device=self device profile prof fn b assert any inductor_wrapper_call e name e prof profiler function_events test_insignificant_strides f x tmp = x + tmp view - x = torch arange device=self device dtype=torch float out = f x compiled_out = torch compile f x assertEqual out stride compiled_out stride assertEqual out compiled_out unittest skipIf IS_X HAS_AVX Requires AVX test_pixel_shuffle_channels_last fn x x = torch nn functional pixel_shuffle x x = torch nn functional relu x x common fn torch randn memory_format=torch channels_last test_where_broadcast https github com pytorch pytorch issues fn x p p o = torch where x p p o https github com pytorch pytorch issues Repro torch nn Module __init__ - None super __init__ _tensor_constant = nn Buffer torch randn dtype=torch float forward arg _ arg _ convert_element_type = torch ops prims convert_element_type default arg _ torch bool bitwise_not = torch ops aten bitwise_not default convert_element_type _tensor_constant = _tensor_constant lift_fresh_copy = torch ops aten lift_fresh_copy default _tensor_constant where = torch ops aten where bitwise_not lift_fresh_copy arg _ where bitwise_not common fn torch tensor True torch rand torch rand args = torch randn torch zeros dtype=torch uint args = eager_args = x clone x args eager_mod = Repro mod = make_fx eager_mod tracing_mode= real args compiled = compile_fx_inner mod args inductor_out = compiled args eager_out = eager_mod eager_args assertEqual inductor_out eager_out test_require_stride_expanded forward arg arg arg convolution = torch ops aten convolution arg unsqueeze arg arg False convolution common forward None rand_strided torch float device=self device memory_format=torch channels_last rand_strided torch float device=self device memory_format=torch channels_last squeeze atol= e- rtol= expanded dim should cause copy require_stride_order assertGeneratedKernelCountEqual requires_gpu parametrize prefer_nd_tiling False True parametrize use_block_ptr False True unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Does support SDPA pre-SM hardware test_sdpa use_block_ptr bool prefer_nd_tiling bool foo arg _ arg _ arg _ arg _ arg _ view = torch ops aten view default arg _ arg _ = None mm = torch ops aten mm default view arg _ view = arg _ = None view_ = torch ops aten view default mm mm = None view_ = torch ops aten view default view_ view_ = None permute = torch ops aten permute default view_ view_ = None view_ = torch ops aten view default permute permute = None clone = torch ops aten clone default view_ memory_format=torch contiguous_format view_ = None expand = torch ops aten expand default clone clone = None _scaled_dot_product_efficient_attention = torch ops aten _scaled_dot_product_efficient_attention default arg _ arg _ arg _ expand False arg _ = arg _ = arg _ = expand = None getitem = _scaled_dot_product_efficient_attention _scaled_dot_product_efficient_attention = None getitem device == cpu raise unittest SkipTest f requires GPU_TYPE DEVICE = torch device f GPU_TYPE DTYPE = torch float B = H = Q = K = D = C_bias = inputs query = torch randn B H Q D device=DEVICE dtype=DTYPE key = torch randn B H K D device=DEVICE dtype=DTYPE value = torch randn B H K D device=DEVICE dtype=DTYPE bias = torch randn B Q K C_bias device=DEVICE dtype=DTYPE weights = torch randn C_bias H device=DEVICE dtype=DTYPE inps = query key value bias weights config patch triton prefer_nd_tiling prefer_nd_tiling triton use_block_ptr use_block_ptr triton native_matmul False Check accuracy common foo inps atol= rtol= e Check code block pointers foo_opt = torch compile foo backend= inductor code = run_and_get_triton_code foo_opt inps have_block_ptr = code count tl make_block_ptr is_halide_backend device assertEqual have_block_ptr use_block_ptr requires_gpu unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Does support mem_eff_attention test_sdpa_unaligned_mask foo arg _ f arg _ f arg _ f arg _ f constant_pad_nd f = torch ops aten constant_pad_nd default arg _ arg _ = None slice_ f = torch ops aten slice Tensor constant_pad_nd - constant_pad_nd = None expand f = torch ops aten expand default slice_ slice_ = None _scaled_dot_product_efficient_attention = torch ops aten _scaled_dot_product_efficient_attention default arg _ arg _ arg _ expand False arg _ = arg _ = arg _ = expand = None getitem f = _scaled_dot_product_efficient_attention _scaled_dot_product_efficient_attention = None getitem query = torch rand device=GPU_TYPE key = torch rand device=GPU_TYPE value = torch rand device=GPU_TYPE bias = torch rand device=GPU_TYPE common foo query key value bias atol= rtol= e requires_gpu unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Does support mem_eff_attention config patch freezing=True test_sdpa_unaligned_mask_freezing Mod torch nn Module __init__ - None super __init__ arg _ = torch rand device=GPU_TYPE forward arg _ f arg _ f arg _ f arg _ = arg _ constant_pad_nd f = torch ops aten constant_pad_nd default arg _ arg _ = None slice_ f = torch ops aten slice Tensor constant_pad_nd - constant_pad_nd = None expand f = torch ops aten expand default slice_ slice_ = None _scaled_dot_product_efficient_attention = torch ops aten _scaled_dot_product_efficient_attention default arg _ arg _ arg _ expand False arg _ = arg _ = arg _ = expand = None getitem f = _scaled_dot_product_efficient_attention _scaled_dot_product_efficient_attention = None getitem query = torch rand device=GPU_TYPE key = torch rand device=GPU_TYPE value = torch rand device=GPU_TYPE mod = Mod out_eager = mod query key value torch no_grad out_compiled = torch compile mod query key value assertEqual out_eager out_compiled atol= rtol= e test_where_with_logical_op fn_and x y torch where torch logical_and x y fn_or x y torch where torch logical_or x y common fn_and torch randn torch randn common fn_or torch randn torch randn skipIfRocm test_conv_with_as_strided Model nn Module __init__ - None super __init__ kv = torch nn Conv d kernel_size= stride= bias=False forward x convolution = kv x constant_pad_nd = torch ops aten constant_pad_nd default convolution as_strided inputs depend input s size stide as_strided = torch ops aten as_strided default constant_pad_nd as_strided_ = torch ops aten as_strided default as_strided clone = torch ops aten clone default as_strided_ memory_format=torch contiguous_format clone common Model torch randn check_lowp=not is_halide_backend device test_inplace_where_pointwise https github com pytorch pytorch issues fn b = b common fn torch rand torch rand xfail_if_triton_cpu test_view_on_aliased https github com pytorch pytorch issues fn b = max values c = torch cat b c = c round b = noqa B c some_const = torch tensor fn = torch tensor ret = torch cat dim= some_const = noqa B ret common fn torch tensor torch tensor common fn test_argmax_to_float https github com pytorch pytorch issues fn = torch zeros b = argmax b float mean common fn test_const_int _to_float https github com pytorch pytorch issues fn = torch zeros dtype=torch int = + b = dtype=torch float b common fn test_getitem out_features = p p p p p in_feature = p fn out_features index in_feature x = torch rand device=self device torch rand device=self device torch rand device=self device opt_fn = torch compile fn backend= inductor same fn x opt_fn x test_pad_view fn y = torch nn functional pad y = y view y size - y size - y size - y x = torch rand common fn x test_pad_single fn y = torch nn functional pad y x = torch rand common fn x test_pad_cast fn x torch nn functional pad x torch float dtype torch int torch int common fn torch ones dtype=dtype unittest skipIf HAS_CPU requires C++ compiler skip_if_triton No inductor data type propagation pass scheduler nodes skip_if_halide bf test_data_type_propogation torch _dynamo utils detect_fake_mode torch _inductor codegen common boolean_ops torch _inductor compile_fx shape_env_from_inputs torch _inductor debug DebugContext torch _inductor decomposition decompositions torch _inductor graph GraphLowering torch _inductor virtualized V torch fx passes fake_tensor_prop FakeTensorProp get_data_type node torch fx Node OptimizationContext key node meta node meta OptimizationContext key dtype None func arg _ max_pool d_with_indices = torch ops aten max_pool d_with_indices default arg _ arg _ = None getitem = max_pool d_with_indices max_pool d_with_indices = None getitem example_inputs = torch randn dtype=torch bfloat memory_format=torch channels_last gm = make_fx func decomposition_table=decompositions tracing_mode= fake example_inputs shape_env = shape_env_from_inputs example_inputs fake_mode = detect_fake_mode example_inputs fake_mode fake_mode = torch _subclasses FakeTensorMode allow_non_fake_inputs=True FakeTensorProp gm mode=fake_mode propagate example_inputs FakeTensorProp gm mode=fake_mode propagate_dont_convert_inputs example_inputs V set_fake_mode fake_mode graph = GraphLowering gm shape_env=shape_env V set_graph_handler graph V set_debug_handler DebugContext graph run example_inputs graph compile_to_module scheduler_node = graph scheduler nodes DataTypePropagation propagate_scheduler_node scheduler_node root_graph = scheduler_node _body root_block graph node root_graph nodes node op == placeholder assertEqual get_data_type node None node target boolean_ops assertEqual get_data_type node torch bool node target constant to_dtype index_expr assertEqual get_data_type node node args - node target get_index index_expr assertEqual get_data_type node torch int node target load store assertEqual get_data_type node V graph get_dtype node args node target == reduction _ _ dtype _ _ _ _ = node args assertEqual get_data_type node dtype node target startswith masked_subblock masked_subblocks opcode name target args kwargs ----------- --------- --------- -------------------------- -------- placeholder ops ops call_module get_index get_index index call_method load load ops arg _ get_index call_method to_dtype to_dtype ops load torch float output output output to_dtype assertEqual get_data_type node torch float node target == and_ and_ s input boolean_ops ----------- --------- --------- -------------------------- -------- call_method and__ and_ ops ge_ lt_ ----------- --------- --------- -------------------------- -------- assertEqual get_data_type node torch bool node target == maximum maximum s input maximum masked_subblock ----------- --------- --------- -------------------------- -------- call_method maximum_ maximum ops masked_subblock maximum_ ----------- --------- --------- -------------------------- -------- assertEqual get_data_type node torch float node target == output assertEqual get_data_type node torch bfloat Calling div only torch SymInt arguments yet supported To support behavior we need allow const-propping tensors store symint data For now dynamo will explicitly graph break when encounters user code behavior expectedFailureCodegenDynamic xfailIfS X skip_if_gpu_halide accuracy error test_AllenaiLongformerBase_repro fn query scores window_overlap batch_size seq_len num_heads _ = query size chunks_count = torch div seq_len window_overlap rounding_mode= trunc - diagonal_attention_scores = scores new_zeros batch_size num_heads chunks_count + window_overlap window_overlap + diagonal_attention_scores - window_overlap = scores window_overlap window_overlap + input_tensor = diagonal_attention_scores view batch_size num_heads seq_len window_overlap + transpose beginning_input = input_tensor window_overlap window_overlap + input_tensor window_overlap window_overlap + = torch full_like beginning_input -float inf input_tensor args = args = rand_strided sh st sh st args args append is_cpp_backend device opt_fn = torch compile fn backend= inductor _ code = run_and_get_cpp_code opt_fn args num = cpu_vec_isa valid_vec_isa_list os getenv ATEN_CPU_CAPABILITY = default FileCheck check_count static_cast int _t num exactly=True run code common fn args test_cumsum_pattern_matcher_issue fn input_ids - torch Tensor input_shape = input_ids size input_ids = input_ids view - input_shape - batch_size seq_length = input_shape past_key_values_length = mask_seq_length = past_key_values_length + seq_length attention_mask = torch ones batch_size mask_seq_length device=input_ids device attention_mask = attention_mask long torch cumsum attention_mask dim= x = torch randn common fn x atol= rtol= staticmethod _check_resize_common fn x size_or_y memory_format inplace deterministic x = x device x_ref_arg = x clone x_opt_arg = x clone x_numel = x numel torch _dynamo reset_code_caches opt_fn = torch _dynamo optimize_assert compile_fx fn correct = fn x_ref_arg size_or_y memory_format actual = opt_fn x_opt_arg size_or_y memory_format get_numel size_or_y isinstance size_or_y torch Tensor size_or_y numel assume shape functools reduce lambda x y x y size_or_y deterministic nele_check = correct numel nele_check = min x_numel get_numel size_or_y correct_values = correct as_strided nele_check actual_values = actual as_strided nele_check assertTrue same correct_values actual_values equal_nan=deterministic correct_strides = correct stride actual_strides = actual stride assertEqual correct_strides actual_strides staticmethod _cases_resize_common sizes = x_size y_size sizes memory_formats = torch contiguous_format len y_size == memory_formats append torch channels_last len y_size == memory_formats append torch channels_last_ d memory_format memory_formats x = torch randn x_size yield x y_size memory_format check some non-contiguous tensors x numel == x_strided = x reshape transpose yield x_strided y_size memory_format test_resize fn x size memory_format NOTE Tensor resize = = aten resize torch ops aten resize x size memory_format=memory_format deterministic True False DeterministicGuard deterministic fill_uninitialized_memory=deterministic x y_size memory_format CommonTemplate _cases_resize_common CommonTemplate _check_resize_common fn x y_size memory_format inplace=False deterministic=deterministic staticmethod _cases_resize_as_common x y_size memory_format CommonTemplate _cases_resize_common each sizes memory_format combination tested ways y contiguous fn gets memory_format kwargs y has memory_format contiguity fn gets preserve kwarg y has some other strides contiguous channels last fn gets preserve yield x torch randn y_size memory_format yield x torch randn y_size contiguous memory_format=memory_format torch preserve_format yield x torch randn y_size permute tuple reversed range len y_size torch preserve_format skipIfXpu test_resize_as fn x y memory_format torch ops aten resize_as x y memory_format=memory_format deterministic True False DeterministicGuard deterministic fill_uninitialized_memory=deterministic x y memory_format CommonTemplate _cases_resize_as_common CommonTemplate _check_resize_common fn x y memory_format inplace=False deterministic=deterministic test_inplace_resize_as fn x y x resize_as_ y x x = torch randn y = torch randn x_clone = x clone opt_fn = torch compile fn backend= inductor same fn x y opt_fn x_clone y xfail_if_triton_cpu test_erfc fn x torch erfc x common fn torch randn skip_if_halide erfinv implemented xfail_if_triton_cpu test_erfinv fn x torch erfinv x domain erfinv - x = torch empty uniform_ - common fn x test_uint fn z x = torch tensor device=z device dtype=torch uint y = torch neg x x y common fn torch randn test_scaled_dot_product_attention device == cuda PLATFORM_SUPPORTS_FLASH_ATTENTION raise unittest SkipTest Can t run flash attention platform fn q k v torch nn functional scaled_dot_product_attention q transpose contiguous k transpose v transpose scale= common fn torch randn torch randn torch randn atol= e- pass lowp check GPU rtol= e- pass lowp check GPU xfail_if_mps_unimplemented expectedFailureXPU unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Some archs don t support mem eff SDPA test_scaled_dot_product_efficient_attention device == cpu raise unittest SkipTest f requires GPU_TYPE The first two values should same attention output logsumexp since dropout being set fn q k v attn_bias compute_log_sumexp aten _scaled_dot_product_efficient_attention q k v attn_bias compute_log_sumexp common fn torch randn torch randn torch randn torch randn False check_lowp=False test_fft_real_input fn x torch fft fftn x common fn torch randn check_lowp=False test_fft_real_input_real_output fn x torch fft fftn x real common fn torch randn check_lowp=False test_searchsorted fn sorted_sequence values out_int right side sorter torch searchsorted sorted_sequence values out_int =out_int right=right side=side sorter=sorter shapes = scalar sorted_sequence scalar values -D sorted_sequence N-D sorted_sequence prime dimensioned sequence flush out indexing bugs booleans = False True seq_shape value_shape out_int right itertools product shapes booleans booleans unsorted_sequence = torch rand seq_shape sorted_sequence sorting_indices = torch sort unsorted_sequence values = torch rand value_shape side = right right left common fn sorted_sequence values out_int right side None check_lowp=False common fn unsorted_sequence values out_int right side sorting_indices check_lowp=False requires_gpu skip_if_gpu_halide skip_if_not_triton test_searchsorted_broadcast fn sorted_sequence values torch searchsorted sorted_sequence values unsqueeze - expand - contiguous unsorted_sequence = torch rand sorted_sequence sorting_indices = torch sort unsorted_sequence values = torch rand common fn sorted_sequence values check_lowp=False cfn = torch compile fn _ code = run_and_get_code cfn sorted_sequence GPU_TYPE values GPU_TYPE make sure we did fuse broadcast bucketize because bucketize computationally expensive FileCheck check triton check triton run code parametrize nd_tiling False True test_bucketize nd_tiling bool fn input boundaries out_int right torch bucketize input boundaries out_int =out_int right=right input = torch rand - boundaries = torch tensor - - out_int True False right True False out_int = True right = False config patch triton prefer_nd_tiling nd_tiling common fn input boundaries out_int right check_lowp=False test_bucketize_default_kwargs fn input offsets torch bucketize input offsets input = torch tensor - - - - offsets = torch tensor - - common fn input offsets check_lowp=False parametrize dtype_input dtype_boundaries list itertools product test_int_dtypes test_int_dtypes test_bucketize_int dtype_input torch dtype dtype_boundaries torch dtype fn input offsets out_int right torch bucketize input offsets out_int =out_int right=right input = torch randint - dtype_input offsets = torch arange dtype=torch int - dtype_boundaries out_int True False right True False common fn input offsets out_int right check_lowp=False patch object config triton autotune_pointwise True test_bucketize_add_autotune Causes pointwise size_hints where size_hints D fn input offsets add_value torch bucketize input offsets + add_value input = torch rand boundaries = torch tensor - - add_value = torch randint memory_format=torch channels_last common fn input boundaries add_value check_lowp=False assertGeneratedKernelCountEqual test_bucketize_computed_offsets fn inp offsets torch bucketize inp offsets + inp = torch tensor - - - - offsets = torch tensor - - - common fn inp offsets check_lowp=False requires_gpu skip_if_gpu_halide skip_if_not_triton test_bucketize_broadcast fn input boundaries torch bucketize input boundaries unsqueeze - expand - - contiguous inp = torch rand - boundaries = torch tensor - - common fn inp boundaries check_lowp=False cfn = torch compile fn _ code = run_and_get_code cfn inp GPU_TYPE boundaries GPU_TYPE make sure we did fuse broadcast bucketize because bucketize computationally expensive FileCheck check triton check triton run code requires_gpu config patch assume_aligned_inputs=False test_config_option_dont_assume_alignment fn x torch Tensor - torch Tensor x sin + x cos Inductor specializes unguarded alignment initial input Make sure different configurations nothing breaks offset base = torch randn + dtype=torch float device=self device inp = torch as_strided base offset torch _dynamo reset fn_c = torch compile fn ref = fn inp res = fn_c inp assertEqual ref res offset base = torch randn + dtype=torch float device=self device inp = torch as_strided base offset ref = fn inp res = fn_c inp assertEqual ref res atol= e- rtol= e- requires_gpu config patch assume_aligned_inputs=False test_config_option_dont_assume_alignment_recompiles Inputs shape shape - causes recompile shape different storage offset - should NOT cause recompile failed_guards = fail guard nonlocal failed_guards failed_guards append guard fn x torch Tensor - torch Tensor x sin + x cos base = torch randn + dtype=torch float device=self device inp = torch as_strided base inp = torch as_strided base inp = torch as_strided base torch _dynamo reset fn_c = torch _dynamo optimize inductor guard_fail_fn=fail fn ref = fn inp res = fn_c inp assertEqual ref res assertEqual len failed_guards ref = fn inp res = fn_c inp assertEqual ref res dynamic shapes isn t already turned we might have guard failure we turn dynamic shapes assertLessEqual len failed_guards failed_guard_count_iteration_ = len failed_guards failed_guards = ref = fn inp res = fn_c inp assertEqual ref res we might still have dynamics shapes failure offset change shouldn t guarded see Note Input Alignment handling Inductor assertLessEqual len failed_guards failed_guard_count_iteration_ requires_gpu config patch assume_aligned_inputs=False test_config_option_dont_assume_alignment_cudagraphs fn x x cos x sin fn_c = torch compile fn mode= reduce-overhead dynamic=True size stride offset torch manual_seed base = torch randn + dtype=torch float device=self device torch manual_seed base_ref = torch randn + dtype=torch float device=self device inp = torch as_strided base size stride offset inp_ref = torch as_strided base_ref size stride offset inp requires_grad_ True inp_ref requires_grad_ True res = fn_c inp ref = fn inp_ref assertEqual ref res res sum backward ref sum backward assertEqual base grad base_ref grad config patch implicit_fallbacks=True test_custom_op_ torch library foo x x foo_meta x torch empty_like x define_custom_op_for_test foo foo foo_meta fn x = torch nn functional relu x b = torch ops test foo c = torch cos b c common fn torch randn check_lowp=False config patch implicit_fallbacks=True test_custom_op_ torch library foo x scale float scale x torch cos x foo_meta x scale float torch empty_like x torch empty_like x define_custom_op_ _for_test foo foo foo_meta fn x scale float = torch nn functional relu x torch ops test foo scale common fn torch randn check_lowp=False config patch implicit_fallbacks=True test_custom_op_ foo x result = torch zeros_like x t x result += t result foo_meta x torch empty_like x define_custom_op_ _for_test foo foo foo_meta fn x torch ops test foo x common fn torch randn torch randn torch randn check_lowp=False requires_gpu skip_if_not_triton skip_if_cpp_wrapper skip cpp_wrapper tests config patch implicit_fallbacks=True test_generated_code_has_size_stride_assert foo x x foo_meta x torch empty_like x define_custom_op_for_test foo foo foo_meta fn x = torch nn functional relu x b = torch ops test foo b = torch randn device=self device _ code = run_and_get_code torch compile fn is_dynamic_shape_enabled code len code assert_size_stride code try FileCheck check_regex r assert_size_stride\s \ \s ^ + \s \ ^\ \ \s \ ^\ \ \s ^ + \s \ run code except Exception e print f Failed regex match assert_size_stride e print code raise e print Skipping No assert_size_stride found requires_gpu skip_if_not_triton skip_if_cpp_wrapper skip cpp_wrapper tests config patch implicit_fallbacks=True test_generated_code_has_alignment_assert foo x x foo_meta x torch empty_like x define_custom_op_for_test foo foo foo_meta fn x = torch nn functional relu x b = torch ops test foo b = torch randn device=self device _ code = run_and_get_code torch compile fn is_dynamic_shape_enabled code len code assert_alignment code try FileCheck check_regex r assert_alignment\s \ \s ^ + \s ^ + \s ^ + \s \ run code except Exception e print f Failed regex match assert_alignment e print code raise e print Skipping No assert_alignment found test_assert_size_stride_op_name_pass tensor = torch empty assert_size_stride tensor torch ops dummy op_name test_assert_size_stride_op_name_fail tensor = torch empty assertRaisesRegex AssertionError torch ops dummy op_name assert_size_stride tensor torch ops dummy op_name test_assert_alignment_op_name_pass tensor = torch empty assert_alignment tensor torch ops dummy op_name test_assert_alignment_op_name_fail tensor = torch empty assertRaisesRegex AssertionError torch ops dummy op_name assert_alignment tensor torch ops dummy op_name torch _dynamo config patch capture_dynamic_output_shape_ops=True torch _inductor config patch implicit_fallbacks=True test_custom_op_unbacked_symints torch library custom_op test_unbacked_symints foo mutates_args= foo x torch Tensor - torch Tensor x clone foo register_fake _ x u = torch library get_ctx new_dynamic_size u = torch library get_ctx new_dynamic_size u = torch library get_ctx new_dynamic_size x new_empty u u u torch library custom_op test_unbacked_symints bar mutates_args= bar x torch Tensor - torch Tensor x clone bar register_fake _ x torch empty_like x x = torch randn torch compile fullgraph=True f x y = foo x z = bar y z No error f x requires_gpu torch _inductor config patch layout_optimization True torch _inductor config patch keep_output_stride False config patch implicit_fallbacks=True tf _on_and_off test_custom_op_fixed_layout_sequential torch library mod = nn Conv d stride= bias=False device=self device inp = torch rand device=self device expected_stride = mod inp stride bar x assertEqual x stride expected_stride x clone bar_meta x torch empty_like x define_custom_op_for_test bar bar bar_meta tags= torch _C Tag needs_fixed_stride_order fn x z = mod x output = torch ops test bar z output torch no_grad With keep_output_stride False inductor would normally have different layout eager execution But because our custom op needs fixed layout assertions custom op will pass common fn inp check_lowp=False requires_gpu config patch implicit_fallbacks=True skip_if_cpp_wrapper Without major redesign cpp_wrapper will support custom ops defined Python tf _on_and_off test_mutable_custom_op_fixed_layout torch library _scoped_library mylib DEF lib mod = nn Conv d stride= bias=False device=self device inp = torch rand device=self device expected_stride = mod inp clone stride lib define bar Tensor x bool is_compiling - Tensor tags=torch Tag flexible_layout bar_strides = torch library impl lib bar CompositeExplicitAutograd _ x is_compiling is_compiling bar_strides append x stride result = x clone assert x stride == result stride result torch library impl lib bar Meta _ x is_compiling x clone lib define add_one Tensor x - tags=torch Tag needs_fixed_stride_order torch library impl lib add_one CompositeExplicitAutograd _ x assertEqual x stride expected_stride x copy_ x + fn x Inductor changes conv channels-last z = mod x output = torch ops mylib bar z torch _dynamo is_compiling torch ops mylib add_one output output torch no_grad common fn inp check_lowp=False Dynamic shapes rocm invalidate test case torch _dynamo config assume_static_by_default TEST_WITH_ROCM For test valid Inductor must have changed conv channels-last If assertion ever fails then we need new test case assertEqual len bar_strides assertNotEqual bar_strides expected_stride config patch implicit_fallbacks=True skip_if_cpp_wrapper Without major redesign cpp_wrapper will support custom ops defined Python test_mutable_custom_op_fixed_layout torch library _scoped_library mylib DEF lib lib define copy_ Tensor dst Tensor src - tags=torch Tag needs_fixed_stride_order torch library impl lib copy_ Meta _ dst src None torch library impl lib copy_ CompositeExplicitAutograd _ dst src dst copy_ src f x full_default_ = torch full device= cpu chunk_cat_default_ = torch ops mylib copy_ default full_default_ x mul_out = torch mul full_default_ full_default_ mul_out x = torch arange dtype=torch float device= cpu eager_out = f x compiled_inductor_f = torch compile f backend= inductor fullgraph=True compiled_inductor_out = compiled_inductor_f x assertEqual compiled_inductor_out eager_out requires_gpu config patch implicit_fallbacks=True test_custom_op_fixed_layout_channels_last Block nn Module __init__ super __init__ in_layers = nn Sequential nn Dropout p= helper x out = F gelu x out = in_layers out out forward x out = helper x out = torch ops test baz out out model = Block model = model GPU_TYPE memory_format=torch channels_last input_t = torch randn dtype=torch float device=GPU_TYPE input_t = input_t memory_format=torch channels_last expected_strides = model helper input_t stride baz x assertEqual expected_strides x stride x clone baz_meta x torch empty_like x define_custom_op_for_test baz baz baz_meta tags= torch _C Tag needs_fixed_stride_order torch no_grad net = torch compile model out = net input_t skip_if_cpp_wrapper Without major redesign cpp_wrapper will support custom ops defined Python config patch implicit_fallbacks=True test_custom_op_default_layout_constraint torch library _scoped_library mylib DEF lib lib define copy_ Tensor dst Tensor src - No need pass explicit tag since default behavior custom op works tags=torch Tag needs_fixed_stride_order torch library impl lib copy_ Meta _ dst src None torch library impl lib copy_ CompositeExplicitAutograd _ dst src src is_contiguous dst copy_ src + dst copy_ src f x full_default_ = torch full device=self device chunk_cat_default_ = torch ops mylib copy_ default full_default_ x mul_out = torch mul full_default_ full_default_ mul_out x = torch arange dtype=torch float device=self device view t contiguous t eager_out = f x compiled_inductor_f = torch compile f backend= inductor fullgraph=True compiled_inductor_out = compiled_inductor_f x assertTrue torch allclose compiled_inductor_out eager_out skip_if_gpu_halide cuda error test_buffer_use_after_remove https github com pytorch pytorch issues rotvec_to_rotmat rotvec - torch Tensor Simplified rotvec rotmat code RoMa https github com naver roma blob e b cdc c bb c d c roma mappings py#L theta = torch norm rotvec dim=- axis = rotvec theta None kx ky kz = axis axis axis sin_theta = torch sin theta cos_theta = torch cos theta one_minus_cos_theta = - cos_theta xs = kx sin_theta ys = ky sin_theta zs = kz sin_theta xyc = kx ky one_minus_cos_theta xzc = kx kz one_minus_cos_theta yzc = ky kz one_minus_cos_theta xxc = kx one_minus_cos_theta yyc = ky one_minus_cos_theta zzc = kz one_minus_cos_theta R_rodrigues = torch stack - yyc - zzc xyc - zs xzc + ys xyc + zs - xxc - zzc -xs + yzc xzc - ys xs + yzc - xxc - yyc dim=- reshape - R = R_rodrigues R f coord rot trans rot_mat = rotvec_to_rotmat rot coord = torch einsum ij bj- bi rot_mat coord + trans coord sum foo_c = torch compile f dynamic=True run fn coord = torch ones device=self device rot = nn Parameter torch ones device=self device trans = nn Parameter torch ones device=self device U = fn coord rot trans U backward U rot trans U_e rot_e trans_e = run f U rot trans = run foo_c assertEqual U U_e assertEqual rot grad rot_e grad assertEqual trans grad trans_e grad If we serve cache init hook isn t called config patch fx_graph_cache False fx_graph_remote_cache False skipIfWindows msg= torch _dynamo exc Unsupported test_inner_fn_str_and_stride f x x = x + x = test_operators realize x x = x x = test_operators realize x x x = torch rand device=self device t ref = f x called = False hook_fn scheduler nodes nonlocal called called = True device = cpu assertEqual len nodes _ mul_buf _ = nodes assertTrue all V graph sizevars size_hints buf get_stride == buf nodes before fix wrong index expression i + i cached assertTrue i + i mul_buf data inner_fn_str i + i s mul_buf data inner_fn_str add_scheduler_init_hook hook_fn actual = torch compile f fullgraph=True x assertEqual ref actual assertTrue called skip_if_gpu_halide cuda error test_mutations_loop_fusion fn tensor index source out = tensor index_add index source alpha= out device = cpu dtype = torch double device = mps torch float tensor = torch rand dtype=dtype device=device index = torch tensor dtype=torch long device=device source = torch rand dtype=dtype device=device common fn tensor index source config patch triton autotune_pointwise True needed introduce config exceed max shared memory usage serialTest largeTensorTest GB inductor=True test_large_block_sizes Inductor will try triton configs like x = y = which will result out shared memory dtype fp Currently inductor will skip such bad configs pick best one remaining configs torch compile fn x y x t + y Use shape rather than potentially avoid OOM CI while still keep same up-rounded size-hints = torch randn device=self device b = torch randn device=self device fn b Skipped ROCm until https github com ROCm triton issues resolved slowTest test_fuse_large_params pt _optimizer_step optimizer torch compile f optimizer step f params = torch rand dtype=torch float device=self device _ range p params p grad = torch rand_like p o = torch optim AdamW params pt _optimizer_step o Skipped MPS because avgpool size divisible xfail_if_mps skip_if_gpu_halide test_adaptive_avg_pool d_argmax https github com pytorch pytorch issues fn x x = torch adaptive_avg_pool d input=x output_size= x = torch argmax input=x x x = torch rand dtype=torch float common fn x skipCUDAIf SM OrLater uses bfloat which requires SM = parametrize dtype_x dtype_y list itertools product test_dtypes test_dtypes test_dtypeview dtype_x dtype_y TEST_WITH_ASAN is_triton_cpu_backend device raise unittest SkipTest Compile time crash Triton CPU CI https github com pytorch pytorch issues fn x y x_dtype x x = x view x_dtype y = y view x_dtype + x = x view x_dtype + x y x x operation needs arguments same dtype view_dtype test_dtypes try x = rand_strided device=self device dtype=dtype_x y = rand_strided device=self device dtype=dtype_y x = x clone fn x y view_dtype x except Exception e continue common fn x y view_dtype x reference_in_float=False check_lowp=False test_dtypeview_fusion torch compile fn x x = x + x = torch ops aten view dtype x torch int x = x x torch _inductor metrics generated_kernel_count = x = torch randn dtype=torch float device=self device common fn x reference_in_float=False assertGeneratedKernelCountEqual expectedFailureCodegenDynamic test_reinterpret_dtypeview torch compile fn x x x view view torch int x view torch int view x = torch randn device=self device x = x clone common fn x x reference_in_float=False check_lowp=False The cpp_wrapper code significantly more complex so skip checking exact code lines config cpp_wrapper x = torch randn device=self device x = x clone _ code = run_and_get_code fn x x FileCheck check aten view dtype reinterpret_tensor run code xfail_if_triton_cpu requires_gpu test_scalar_cpu_tensor_arg fn x y x + y sum test_dtypes = torch float torch float torch float torch bfloat cpu_dtype test_dtypes is_dtype_supported cpu_dtype continue x = torch rand device=self device y = torch rand device= cpu dtype=cpu_dtype common fn x y check_lowp=False copy_to_gpu=False reference_in_float=False test_float _to_int fn x x_view = x view dtype=torch int x_view mul + x_view bitwise_and x = torch ones dtype=torch float device=self device ref = fn x actual = torch compile fn x assertEqual ref actual skipCUDAIf SM OrLater uses bfloat which requires SM = skip_if_gpu_halide https github com halide Halide issues test_bfloat _to_int fn b x = + b x_view = x view dtype=torch int x_view mul + x_view bitwise_and is_dtype_supported torch bfloat raise unittest SkipTest bfloat supported device = torch ones dtype=torch bfloat device=self device b = torch ones dtype=torch bfloat device=self device ref = fn b actual = torch compile fn b assertEqual ref actual test_float _to_int fn b x = + b x_view = x view dtype=torch int x_view mul + x_view bitwise_and = torch ones dtype=torch float device=self device b = torch ones dtype=torch float device=self device ref = fn b actual = torch compile fn b assertEqual ref actual test_randint_int _mod This used compile due wrong type randint _cpu See https github com pytorch pytorch issues fn n torch randint low=- high= size= n dtype=torch int device=self device res = torch compile fn assertTrue torch all res = res item torch _inductor config patch force_shape_pad=True skip_if_gpu_halide correctness issue test_should_pad_bench_for_bmm B = M = N = K = + size requires padding mat = torch rand B M K device=self device mat = torch rand B K N device=self device should_pad = pad_mm should_pad_bench None mat mat torch ops aten bmm assertTrue should_pad parametrize name op subtest name getattr torch special name name=name name torch special __all__ name softmax log_softmax logsumexp test_pointwise name op dtype = torch float check_lowp = True device == GPU_TYPE name airy_ai bessel_i bessel_i bessel_j bessel_j bessel_y bessel_y erfcx gammainc gammaincc i i e modified_bessel_i modified_bessel_i modified_bessel_k modified_bessel_k ndtri scaled_modified_bessel_k scaled_modified_bessel_k spherical_bessel_j zeta chebyshev_polynomial_t chebyshev_polynomial_v chebyshev_polynomial_u chebyshev_polynomial_w legendre_polynomial_p shifted_chebyshev_polynomial_t shifted_chebyshev_polynomial_u shifted_chebyshev_polynomial_v shifted_chebyshev_polynomial_w hermite_polynomial_h hermite_polynomial_he laguerre_polynomial_l func _cuda implemented Half check_lowp = False is_halide_backend device is_triton_cpu_backend device name erfinv airy_ai bessel_j bessel_j bessel_y bessel_y chebyshev_polynomial_t chebyshev_polynomial_u chebyshev_polynomial_v chebyshev_polynomial_w digamma gammainc gammaincc gammaln hermite_polynomial_h hermite_polynomial_he i i e i i e laguerre_polynomial_l legendre_polynomial_p modified_bessel_i modified_bessel_i modified_bessel_k modified_bessel_k multigammaln ndtri polygamma psi scaled_modified_bessel_k scaled_modified_bessel_k shifted_chebyshev_polynomial_t shifted_chebyshev_polynomial_u shifted_chebyshev_polynomial_v shifted_chebyshev_polynomial_w spherical_bessel_j zeta raise unittest SkipTest f Halide Triton CPU do support name is_triton_cpu_backend device name erfc erfcx round log_ndtr raise unittest SkipTest f Triton CPU does support name name gammainc gammaincc args = torch randn dtype=dtype device=self device torch empty dtype=dtype device=self device uniform_ fn x y op x y name xlog py xlogy zeta args = torch randn dtype=dtype device=self device torch empty dtype=dtype device=self device uniform_ fn x y op x y name == multigammaln args = torch empty dtype=dtype device=self device uniform_ fn x p op x p name == polygamma args = torch empty dtype=dtype device=self device uniform_ fn n x op n x _polynomial_ name args = torch randn dtype=dtype device=self device fn x n op x n args = torch randn dtype=dtype device=self device fn x op x ctx = contextlib nullcontext device = mps name airy_ai erfcx laguerre_polynomial_l legendre_polynomial_p log_ndtr ndtri assertRaises NotImplementedError ctx common fn args check_lowp=check_lowp atol= e- rtol= e- codegen test fails no dynamic loop dynamic shape tests expectedFailureCodegenDynamic test_view_uint _through_differing_bitwidths https github com pytorch pytorch issues fn x view_dtype x view view_dtype view torch uint view_dtypes = torch int torch int torch int dtype view_dtypes x = torch randint dtype=torch uint common fn x dtype torch _dynamo config patch capture_scalar_outputs=True test_split_with_sizes_with_unbacked_symints torch compile f sz x s s = sz tolist r r = torch ops aten split_with_sizes default x s s torch ops aten sort default r N = S = S = N - S result = f torch tensor S S torch randn N assertTrue len result == torch compile f x y = torch arange x item torch ops aten split_with_sizes default y result = f torch tensor assertTrue len result == torch _dynamo config patch capture_scalar_outputs=True test_split_with_unbacked_symints https github com pytorch pytorch issues torch compile f x y = torch arange x item torch split y result = f torch tensor assertTrue len result == test_complex_memory_overlap t = rand_strided device=self device assertFalse complex_memory_overlap t xfail_if_mps test_generate_rand_fp PyTorch can generate fp tensors normal distribution because missing needed kernels We work around rand_strided generating fp tensor first then do casting t = rand_strided device=self device dtype=torch float _e m fn assertTrue t dtype torch float _e m fn largeTensorTest GB inductor=True parametrize use_block_ptr subtest False subtest True decorators= skip_if_not_triton test_large_grid use_block_ptr https github com pytorch pytorch issues fn primals_ view = torch ops aten reshape default primals_ - primals_ = None permute = torch ops aten permute default view clone = torch ops aten clone default permute memory_format=torch contiguous_format clone s = s = config patch triton use_block_ptr use_block_ptr compiled_fn = torch compile fn actual = compiled_fn torch ones s s device=self device assertTrue actual == all skip_if_gpu_halide test_pattern_matcher_multi_user Reproducer https github com pytorch pytorch issues forward float_ view_ logits = float_ loss = torch nn functional cross_entropy logits view_ ignore_index= logsumexp = logits logsumexp dim=- loss logsumexp = torch randn requires_grad=True b = torch randint size= low= high= common forward b test_isin_tensor_scalar invert True False torch _dynamo reset elements = test_elements = torch tensor common torch isin elements test_elements invert invert torch _dynamo reset elements = torch tensor test_elements = common torch isin elements test_elements invert invert test_mul_index_expr Minified repro https github com pytorch pytorch issues forward iota = torch ops prims iota default start= step= dtype=torch int device=self device requires_grad=False unsqueeze = torch ops aten unsqueeze default iota - mul = torch ops aten mul Tensor unsqueeze iota unsqueeze = iota = None neg = torch ops aten neg default mul mul = None div = torch ops aten div Tensor neg neg = None div common forward test_flip_cat forward unsqueeze unsqueeze_ cat_ = torch ops aten cat default unsqueeze unsqueeze_ view = torch ops aten view default cat_ slice_ = torch ops aten slice Tensor view rev_ = torch ops aten flip default slice_ rev_ = torch randn requires_grad=True b = torch randn requires_grad=True common forward b config patch implicit_fallbacks=True test_weight_norm_bwd Weight norm backward eager kernel does support non-contiguous inputs Eager kernel silently produces incorrect results when inputs non-contiguous Inductor implicitly fallback eager weight norm backward Fix requiring contiguous inputs any implicit fallback kernels Check https github com pytorch pytorch issues Repro nn Module __init__ in_features super __init__ weight_normed_linear = nn utils parametrizations weight_norm nn Linear in_features out_features= linear = nn Linear in_features= out_features= forward x linear weight_normed_linear x f m x torch amp autocast device_type=self device dtype=torch half loss = m x sum loss backward loss odd number purpose trigger comprehensive padding in_features = x = torch randn in_features dtype=torch half requires_grad=True device=self device m = Repro in_features m = m device f m x ref_grad_list = p grad p m parameters p m parameters p grad = None opt_f = torch compile f opt_f m x act_grad_list = p grad p m parameters assertTrue same ref_grad_list act_grad_list tol= e- f Ref \n ref_grad_list \nAct \n act_grad_list test_chunk_recompiles f x x chunk Runs f its torch compile-d version fresh D tensor specific size checks result correct run size input = torch randn size expected_out = f input actual_out = optf input assertEqual expected_out actual_out cnts = CompileCounterWithBackend inductor optf = torch compile f backend=cnts fullgraph=True The first run should compile once static shapes run assertEqual cnts frame_count Varying input size should trigger recompilation Since input size multiple i e all runs shall generate output tensors there should no further recompilation i range run i assertEqual cnts frame_count Input size Not multiple still generates output tensors where last one has size run assertEqual cnts frame_count Input size Even though still generates output tensors last one has size falling into our specialization Thus one also triggers recompilation run assertEqual cnts frame_count Input size Yields one less output tensor which should trigger recompilation run assertEqual cnts frame_count dynamo_config patch error_on_recompile=True test_no_specization_over_symbolic_value fn x s = x shape y = torch full s x + y arg = torch ones arg = torch ones ref = fn arg ref = fn arg opt_fn = torch compile fn fullgraph=True dynamic=True backend= inductor res = opt_fn arg res = opt_fn arg assertEqual res ref assertEqual res ref test_conv_shape_check https github com pytorch pytorch issues Model torch nn Module __init__ dim super __init__ conv_t_cls = eval f torch nn ConvTranspose dim d conv_t = conv_t_cls kernel_size= dim padding= dim forward x x = conv_t x x = torch sigmoid x trigger condition x dim inputs = torch randn dim + model = Model dim assertRaisesRegex RuntimeError Output size too small _ = model inputs assertRaisesRegex RuntimeError Output size too small _ = torch compile model inputs requires_gpu config patch fallback_random=True unittest skipIf config cpp_wrapper cpp wrapper does support sort properly https gist github com shunting e f f f ad d cee e test_mix_device_index A tiny repro meta internal issue https fb workplace com groups posts whose root cause Inductor having wrong assumption index Tensor s output stride image_latent = torch randn device=GPU_TYPE memory_format=torch channels_last view f image_latent indices = torch argsort torch rand dim=- tar_latent = image_latent torch arange unsqueeze - indices The original model uses einops In unit test we use view op directly avoid importing einops tar_latent_rearranged = einops rearrange tar_latent b n c h w - b n c h w tar_latent_rearranged = tar_latent view - tar_latent size tar_latent_rearranged reset_rng_state ref = f image_latent opt_f = torch compile f code = run_and_get_triton_code opt_f image_latent reset_rng_state act = opt_f image_latent torch testing assert_close ref act atol= e- rtol= e- is_dynamic_shape_enabled size_assert_pattern = r assert_size_stride a-z + - + s s s \ s \ s \ s s \ s \ s s \ s s noqa B size_assert_pattern = r assert_size_stride a-z + - + FileCheck check_regex size_assert_pattern run code lowering force_fallback aten sort default unittest skipIf config cpp_wrapper Inductor does generate size stride asserts cpp_wrapper test_size_asserts_for_multi_output_fallback torch compile f x x sort x = torch randn device=self device code = run_and_get_triton_code f x is_dynamic_shape_enabled FileCheck check assert_size_stride buf s s s check assert_size_stride buf s s s run code FileCheck check assert_size_stride buf check assert_size_stride buf run code requires_cuda_and_triton config patch use_fast_math=True test_prepare_softmax_with_fast_math Measure A perf ms v s ms without flushing zero A speedup DO_PERF_TEST M = N = Use small shapes doing perf test M = N = x = torch randn M N dtype=torch bfloat device=GPU_TYPE f x Not calling softmax directly generate kernel just computation max sum If we call softmax directly computation final result will double membw usage In case saving computation does matter much In reality during training since max sum need saved bwd computation softmax result fused other kernels we do see such prepare_softmax kernel appear real models x_max = x amax dim=- keepdim=True x_sum = x - x_max exp sum dim=- keepdim=True log x_max x_sum opt_f = torch compile f ref = f x act = opt_f x assertTrue same ref act tol= e- f Ref \n ref \nAct \n act DO_PERF_TEST triton testing do_bench ms = do_bench lambda opt_f x print f ms= f torch _inductor config patch graph_partition True test_graph_partition_no_inputs foo torch manual_seed torch randint foo = torch compile foo foo torch _inductor config patch graph_partition True test_graph_partition_mutation_real_name f x y z other mul = x y diag = torch diagonal mul diag copy_ other force grah partition device copy u = diag cpu device torch mm mul z + u + diag inps = torch randn device=self device torch randn device=self device torch randn device=self device torch randn device=self device eager_out = f inps compiled_f = torch compile f compiled_out = compiled_f inps torch testing assert_close eager_out compiled_out torch _inductor config patch graph_partition True test_graph_partition_arange fn step device torch arange - step device=device compiled_fn = torch compile fn step - - expect = fn step cpu actual = compiled_fn step cpu assertEqual expect actual assertEqual expect actual torch _inductor config patch graph_partition True test_graph_partition_arange fn x torch arange dtype=x dtype device=x device make_arg = functools partial make_tensor device=self device requires_grad=False compiled_fn = torch compile fn x = make_arg dtype=torch float assertEqual fn x compiled_fn x x = make_arg dtype=torch int assertEqual fn x compiled_fn x torch _inductor config patch graph_partition True test_graph_partition_argmax fn = torch zeros b = argmax b float mean compiled_fn = torch compile fn assertEqual fn compiled_fn torch _inductor config patch graph_partition True test_graph_partition_both_scalars fn b aten add b aten add b aten sub b aten sub b aten mul b aten mul b compiled_fn = torch compile fn assertEqual fn compiled_fn torch _inductor config patch graph_partition True config patch assume_aligned_inputs=False test_graph_partition_misaligned_input fn x x cos x sin fn_c = torch compile fn mode= reduce-overhead dynamic=True size stride offset torch manual_seed base = torch randn + dtype=torch float device=self device requires_grad=True torch manual_seed base_ref = torch randn + dtype=torch float device=self device requires_grad=True inp = torch as_strided base size stride offset inp_ref = torch as_strided base_ref size stride offset inp requires_grad_ True inp_ref requires_grad_ True res = fn_c inp ref = fn inp_ref assertEqual ref res res sum backward ref sum backward assertEqual base grad base_ref grad torch _inductor config patch graph_partition True test_graph_partition_constant_tensor fn = torch zeros dtype=torch int = + b = dtype=torch float b compiled_fn = torch compile fn assertEqual fn compiled_fn torch _inductor config patch graph_partition True test_graph_partition_constant_tensor fn x torch tensor list range device=self device + x compiled_fn = torch compile fn x = torch randn device=self device assertEqual fn x compiled_fn x torch _inductor config patch graph_partition True test_graph_partition_scalar_inputs fn b aten div b rounding_mode=None aten div b rounding_mode=None aten div b rounding_mode=None aten div b rounding_mode= floor aten div b rounding_mode= trunc b b compiled_fn = torch compile fn assertEqual fn compiled_fn torch _inductor config patch graph_partition True test_graph_partition_unbacked_symint_as_output nested x repeats rank = torch arange repeats numel device=x device index = rank repeat_interleave repeats dim= torch index_select x index=index dim= example_inputs = torch randn device=self device repeats = torch tensor device=self device torch _dynamo mark_dynamic repeats nested_opt = torch compile nested backend= inductor expect = nested example_inputs actual = nested_opt example_inputs assertEqual expect actual torch _inductor config patch graph_partition True test_graph_partition_refcount contexts = contextlib nullcontext lambda torch _inductor config patch triton cudagraphs True context contexts context inps = torch rand device torch rand device inp_refs = weakref ref inp inp inps fn x y = x + y fn_fx = make_fx fn inps inps fn_compiled = compile_fx_inner fn_fx inps matmul_seen = False TestRefMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None kwargs = kwargs kwargs nonlocal inps nonlocal inp_refs nonlocal matmul_seen gc collect func aten mm out matmul_seen = True assert len inps == assert inp_refs None assert inp_refs None func args kwargs TestRefMode fn_compiled inps do extra run make sure we deallocating warmup record inps extend torch rand device torch rand device inp_refs extend weakref ref inp inp inps matmul_seen = False TestRefMode fn_compiled inps assert len inps == torch _inductor config patch graph_partition True test_graph_partition_pad_dynamic get_same_padding x int k int s int d int max math ceil x s - s + k - d + - x pad_same x k s d= value= ih iw = x size - pad_h pad_w = get_same_padding ih k s d get_same_padding iw k s d pad_h pad_w x = torch nn functional pad x pad_w pad_w - pad_w pad_h pad_h - pad_h value=value x x = torch randn device=self device opt = torch compile pad_same dynamic=True res = opt x ref = pad_same x assertEqual res ref atol= rtol= skip_if_halide only -bit indexing largeTensorTest GB inductor=True test_split_reduction_with_int _size torch _inductor config cpu_backend == triton raise unittest SkipTest Fail triton cpu backend error https gist github com shunting fb b b b f ae f device == cpu raise unittest SkipTest The test fails some times CI https github com pytorch pytorch actions runs job Skip now size = rand rather than randn since mean latter close which happens close value generated bug t = torch rand size dtype=torch float device=self device op = torch mean expected = op t actual = torch compile op t common takes more GPU memory Do check directly assertTrue torch allclose expected actual atol= e- rtol= e- f expected= actual= test_remove_noop_view_default f x batch_size = x shape x = x transpose batch_size x = x reshape batch_size noop x f = torch compile f x = torch randn device=self device expected_graph = f \ forward arg _ f str x device permute f str x device = torch ops aten permute default arg _ arg _ = None permute noqa B post_grad_graph = get_post_grad_graph f x assertExpectedInline post_grad_graph expected_graph ignore_comments=True ignore_empty_lines=True dynamic shape x = torch randn device=self device expected_graph = f \ forward arg _ Sym s arg _ f s str x device permute f s str x device = torch ops aten permute default arg _ arg _ = None permute noqa B post_grad_graph = get_post_grad_graph f x assertExpectedInline post_grad_graph expected_graph ignore_comments=True ignore_empty_lines=True test_remove_noop_view_dtype f x x = x transpose batch_size x = x view torch uint noop x f = torch compile f x = torch ones device=self device dtype=torch uint torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x post_grad_graph = get_post_grad_graph f x expected_graph = f \ forward arg _ Sym s arg _ Sym s arg _ Sym s arg _ u s s s s s s str x device permute u s s s s s s str x device = torch ops aten permute default arg _ arg _ = None permute noqa B assertExpectedInline post_grad_graph expected_graph ignore_comments=True ignore_empty_lines=True config patch min_num_split test_split_reduction_dynamic_shape torch _dynamo decorators mark_dynamic f x outer reduction x sum dim= N = x_small = torch randn N device=self device mark_dynamic x_small expect = f x_small opt_f = torch compile f dynamic=True actual = opt_f x_small assertTrue torch allclose expect actual atol= e- rtol= e- DO_PERF_TEST triton testing do_bench benchmark much larger input x_large = torch randn N device=self device ms = do_bench lambda opt_f x_large print f ms= f expectedFailureCodegenDynamic test_special_polygamma fn = torch special polygamma x = torch tensor dtype=torch float common fn x common fn x common fn x skip_if_triton skip_if_halide config patch freezing True test_dont_constant_fold torch _inductor constant_folding add_dont_constant_fold clear_dont_constant_fold m = M torch nn Module __init__ super __init__ w = torch randn m s = torch randn m forward x w s + x x = torch rand m mod = M dont_constant_fold True False clear_dont_constant_fold dont_constant_fold add_dont_constant_fold torch ops aten mul Tensor torch no_grad refe_out = mod x mod = torch compile mod test_out code = run_and_get_code mod x dont_constant_fold FileCheck check cpp_fused_add_mul run code FileCheck check cpp_fused_add_ run code assertEqual refe_out test_out test_triton_kernel_bool_param device = GPU_TYPE device == mps raise unittest SkipTest requires GPU torch testing _internal triton_utils add_kernel_with_boolean_param Model torch nn Module forward x out = torch zeros_like x add_kernel_with_boolean_param in_ptr =x in_ptr =x out_ptr=out n_elements=x numel add_xy=True BLOCK_SIZE= out inputs = torch randn device=self device common Model inputs requires_cuda_and_triton parametrize use_cat True False test_copy_non_blocking_is_pinned use_cat f a_list a_cpu_list = a_to_cpu_event_list = a_list a_cpu = device= cpu non_blocking=True a_to_cpu_event = torch Event a_to_cpu_event record a_cpu_list append a_cpu a_to_cpu_event_list append a_to_cpu_event e a_to_cpu_event_list e synchronize use_cat torch cat a_cpu_list a_cpu_list f_compiled = torch compile f inputs = torch rand dtype=torch float device=GPU_TYPE _ range outputs = f inputs warmup_compiled = f_compiled inputs torch profiler profile activities= getattr torch profiler ProfilerActivity GPU_TYPE upper p outputs_compiled = f_compiled inputs assertEqual outputs outputs_compiled profile_output = str p key_averages print profile_output assertFalse Pageable profile_output unittest skipIf config cpp_wrapper cpp_wrapper samples will lead invalid indexing test_inductor_triton_bucketize_respects_masking fn inp repeats output_size torch repeat_interleave inp repeats dim= output_size=output_size idx = torch searchsorted repeats cumsum torch arange output_size device=repeats device right=True torch index_select inp idx inp = torch arange device=self device repeats = torch tensor device=self device output_size = repeats sum item args = inp repeats output_size assertEqual fn args torch compile fn args parametrize dtype torch int torch int parametrize nd test_repeat_interleave_Tensor_decomp dtype nd https github com pytorch pytorch issues f input repeats torch repeat_interleave input repeats dim= output_size= + input = torch tensor dtype=dtype device=self device input = torch arange nd + dtype=dtype device=self device reshape nd repeat = torch tensor device=self device f_compiled = torch compile f output code = run_and_get_code f_compiled input repeat reference = f input repeat assertEqual output reference we don t lower when cpp_wrapper used because cannot generate proper examples during autotune can_lower = config cpp_wrapper input device type = mps has_lowered = re search r repeat_interleave Tensor code assertEqual has_lowered can_lower staticmethod _is_triggering_buffer_reuse fn inputs config patch allow_buffer_reuse=True _ code_allowed = run_and_get_code fn inputs config patch allow_buffer_reuse=False _ code_disallowed = run_and_get_code fn inputs code_allowed = re sub r AOT ID AOT ID test code_allowed code_disallowed = re sub r AOT ID AOT ID test code_disallowed code_allowed = code_disallowed If matmul implemented triton there more reuse config patch max_autotune_gemm_backends= ATEN unittest skipIf config triton native_matmul matmul now generated test_allow_reuse_disable_if_exceed_peak torch compile fn inp N^ = inp mean - N^ + N b = inp - N^ + N c = b b N^ since peak can reuse across d = c mean - N^ + N d N^ + N inp = torch randn device=self device assertFalse CommonTemplate _is_triggering_buffer_reuse fn inp test_allow_reuse_active_if_under_peak g inp inp - torch logsumexp inp - torch compile fn m inp inp = m g inp inp = m g inp inp = m g inp inp = m g inp inp = m g inp inp m = torch randn device=self device inp = torch randn device=self device assertTrue CommonTemplate _is_triggering_buffer_reuse fn m inp requires_cuda_and_triton test_cpu_scalar_with_gpu_tensor fn b + b = torch rand device=GPU_TYPE b = torch rand device= cpu torch _inductor metrics generated_kernel_count = eager = fn b compiled = torch compile fn backend= inductor b assertEqual eager compiled assertEqual torch _inductor metrics generated_kernel_count requires_cuda_and_triton torch _inductor config patch cpp_wrapper=True test_cpu_scalar_with_gpu_tensor_cpp fn b + b = torch rand device=GPU_TYPE b = torch rand device= cpu eager = fn b compiled = torch compile fn backend= inductor b assertEqual eager compiled requires_cuda_and_triton test_cpu_scalar_with_gpu_tensor_dynamic fn b + b = torch rand device=GPU_TYPE b = torch rand device= cpu eager = fn b compiled = torch compile fn backend= inductor dynamic=True b assertEqual eager compiled test_cpu_scalar_with_cpu_tensor fn b + b = torch rand device= cpu b = torch rand device= cpu torch _inductor metrics generated_kernel_count = eager = fn b compiled = torch compile fn backend= inductor b assertEqual eager compiled assertEqual torch _inductor metrics generated_kernel_count requires_cuda_and_triton test_gpu_scalar_with_gpu_tensor fn b + b = torch rand device=GPU_TYPE b = torch rand device=GPU_TYPE torch _inductor metrics generated_kernel_count = eager = fn b compiled = torch compile fn backend= inductor b assertEqual eager compiled assertEqual torch _inductor metrics generated_kernel_count requires_cuda_and_triton test_cpu_tensor_with_gpu_tensor fn b + b = torch rand device=GPU_TYPE b = torch rand device= cpu assertRaises RuntimeError compiled = torch compile fn backend= inductor b test_cpu_tensor_with_cpu_tensor fn b + b = torch rand device= cpu b = torch rand device= cpu eager = fn b compiled = torch compile fn backend= inductor b assertEqual eager compiled test_cpu_scalar_with_cpu_scalar fn b + b = torch rand device= cpu b = torch rand device= cpu eager = fn b compiled = torch compile fn backend= inductor b assertEqual eager compiled requires_cuda_and_triton test_gpu_scalar_with_cpu_tensor fn b + b = torch rand device=GPU_TYPE b = torch rand device= cpu assertRaises RuntimeError compiled = torch compile fn backend= inductor b requires_cuda_and_triton config patch emulate_precision_casts=True test_emulate_precision_triton_fp_fusion fn b + b = torch full device=GPU_TYPE dtype=torch float b = torch full - device=GPU_TYPE dtype=torch float compiled = torch compile fn out code = run_and_get_code compiled b assertTrue enable_fp_fusion False code torch testing assert_close out fn b atol= rtol= requires_cuda_and_triton config patch runtime_triton_nan_asserts=True test_nan_assert_inside_triton_kernel fn x x = x - Uncomment following line can trigger failure device size assertion x = torch log x torch where x isnan x compiled = torch compile fn x = torch randn device=GPU_TYPE out code = run_and_get_code compiled x assertTrue NaN Inf found code torch testing assert_close out fn x skip_if_cpp_wrapper skip cpp wrapper requires_cuda_and_triton test_repeat_interleave_decomposition_has_clamp repeat = torch ones dtype=torch int device=GPU_TYPE output_size = data = torch arange device=GPU_TYPE is_dynamic_shape_enabled raise unittest SkipTest repeat_interleave decomp doesn t support dynamic output size torch compile fn repeat output_size data indices = torch ops aten repeat_interleave Tensor repeat output_size=output_size data indices result code = run_and_get_code fn repeat output_size data assertEqual result shape output_size assertTrue torch all result = item assertTrue torch all result item code_str = \n join code torch version hip triton_str = tl minimum triton_str = triton_helpers minimum assertIn triton_str code_str Generated Triton code should use triton_helpers minimum clamping skip_if_halide requires_cuda_and_triton skip_if_cpp_wrapper skip cpp wrapper test_triton_argmin_argmax_transpose_logical_index fn x x tan_ x = x t x argmin common fn torch randn device=GPU_TYPE fn x x t argmin x t argmax common fn torch randn device=GPU_TYPE common fn torch randn device=GPU_TYPE common fn torch randn device=GPU_TYPE dtype=torch float fn x Permute A B C - C A B permuted = x permute permuted argmin permuted argmax common fn torch randn device=GPU_TYPE fn x sliced tensor gaps memory sliced = x sliced argmin sliced argmax common fn torch randn device=GPU_TYPE Test column major passed input fn x x argmin x argmax common fn torch randn device=GPU_TYPE t contiguous t end CommonTemplate - add new tests here dataclasses dataclass TestFailure suffixes tuple str is_skip bool = False __test__ bool = False copy_tests my_cls other_cls suffix test_failures=None xfail_prop=None noqa B name value my_cls __dict__ items name startswith test_ You cannot copy functions Python so we use closures here create objects different ids Otherwise unittest skip would modify all methods sharing same object id Also using default argument we create copy instead reference Otherwise we would lose access value functools wraps value new_test value=value value Copy __dict__ which may contain test metadata new_test __dict__ = copy deepcopy value __dict__ xfail_prop None hasattr value xfail_prop new_test = unittest expectedFailure new_test tf = test_failures test_failures get name tf suffix tf suffixes skip_func = unittest skip Skipped tf is_skip unittest expectedFailure new_test = skip_func new_test setattr other_cls f name _ suffix new_test Special case convenience routine hasattr my_cls is_dtype_supported other_cls is_dtype_supported = my_cls is_dtype_supported add_test_failures test_failures dict str TestFailure added_test_failures dict str TestFailure In-place modifies given dictionary ` test_failures ` add contents ` added_test_failures ` unioning test_failure suffixes or-ing is_skip value name new_failure added_test_failures items name test_failures orig_failure = test_failures name orig_failure suffixes = tuple set orig_failure suffixes union set new_failure suffixes orig_failure is_skip = orig_failure is_skip new_failure is_skip test_failures name = new_failure RUN_CPU SweepInputsCpuTest SweepInputs TestCase gen = InputGen cpu SweepInputsCpuTest populate CpuTests TestCase common = check_model device = cpu copy_tests CommonTemplate CpuTests cpu RUN_GPU HAS_MPS SweepInputsGPUTest SweepInputs TestCase gen = InputGen GPU_TYPE SweepInputsGPUTest populate GPUTests TestCase common = check_model_gpu device = GPU_TYPE copy_tests CommonTemplate GPUTests GPU_TYPE RUN_GPU instantiate_parametrized_tests TritonCodeGenTests TestCase torch _inductor runtime triton_heuristics CachingAutotuner device_type = GPU_TYPE device = GPU_TYPE NoOpCompilerBackend __init__ - None example_args = None model = None noop_backend model_ torch fx GraphModule example_inputs_ list torch Tensor The Noop backend does compile fx graph given Instead transforms fx graph so its functions aten operations It then saves graph torch _inductor decomposition select_decomp_table torch _subclasses FakeTensorMode torch fx Interpreter fake_mode = FakeTensorMode interpret args kwargs Interpreter model_ run args kwargs fake_flat_tensor_args = fake_mode from_tensor x x example_inputs_ fw_module = make_fx interpret select_decomp_table fake_flat_tensor_args model = fw_module example_args = fake_flat_tensor_args lambda x example_inputs_ get_kernels fn args - list CachingAutotuner torch _inductor debug DebugContext torch _inductor graph GraphLowering torch _inductor virtualized V cxt = TritonCodeGenTests NoOpCompilerBackend torch compile fn backend=cxt noop_backend args graph = GraphLowering cxt model kernels = V set_graph_handler graph V set_debug_handler DebugContext graph run cxt example_args mod = graph compile_to_module val mod __dict__ values isinstance val torch _inductor runtime triton_heuristics CachingAutotuner kernels append val kernels test_divisible_by_ _covers_numel_args torch _dynamo reset fn torch Tensor - torch Tensor torch sum kernels = get_kernels fn torch randn device=GPU_TYPE expected_divisible = kernel reduces xnumel= rnumel= which means reduces into array size accumulating elements once note rnumel equal so rnumel which slot should divisible descriptor kernel reduces elements single scalar Since multi-kernel generate variants each kernel The second persistent-reduction has index config triton multi_kernel assertEqual len kernels expected_divisible = expected_divisible pop config triton cooperative_reductions assertEqual len kernels expected_divisible = one kernel extra workspace semaphore args assertEqual len kernels kernel_id expected expected_divisible items divisible_by_ = get_divisible_by_ kernels kernel_id triton_meta configs assertEqual divisible_by_ expected torch _dynamo reset config patch assume_aligned_inputs=False test_codegen_config_option_dont_assume_alignment fn x torch Tensor - torch Tensor x sin + x cos We want code assumes alignment initial input -byte aligned offset base = torch randn + dtype=torch float device=GPU_TYPE inps = torch as_strided base offset torch _dynamo reset kernels = get_kernels fn inps arguments_that_are_divisible_by_ = get_divisible_by_ kernels triton_meta configs NO_ALIGN ALIGN ALIGN triton_ in_ptr out_ptr xnumel XBLOCK tl constexpr offset == expected_aligned = expected_aligned = assertEqual arguments_that_are_divisible_by_ expected_aligned If input isn t view storage offset = inductor will assume alignment torch _dynamo reset inp = torch randn device=GPU_TYPE kernels = get_kernels fn inp arguments_that_are_divisible_by_ = get_divisible_by_ kernels triton_meta configs assertEqual arguments_that_are_divisible_by_ test_optimize_indexing_dtype fn x torch Tensor - torch Tensor aten upsample_bilinear d vec x None True fn_opt = torch compile fn backend= inductor inps = torch randn device=GPU_TYPE code = run_and_get_triton_code fn_opt inps assertTrue tl int code assertFalse tl int code assertEqual fn_opt inps fn inps config patch fx_graph_remote_cache False test_optimize_indexing_dtype_with_constraint fn torch Tensor b torch Tensor - torch Tensor x = torch arange b shape device=GPU_TYPE y = x + x int y torch int fn torch Tensor b torch Tensor - torch Tensor torch _check b shape = torch _check b shape = fn b fn _opt = torch compile fn backend= inductor fn _opt = torch compile fn backend= inductor = torch rand device=GPU_TYPE b = torch rand device=GPU_TYPE b = torch rand device=GPU_TYPE torch _dynamo mark_dynamic b torch _dynamo mark_dynamic b inps = b inps = b Run fn first since has more restrictive bounds -- avoid cache hit code = run_and_get_triton_code fn _opt inps code = run_and_get_triton_code fn _opt inps The function constrained tensor should optimized other should assertTrue tl int code assertTrue tl int code assertFalse tl int code assertEqual fn _opt inps fn inps assertEqual fn _opt inps fn inps test_constant_folding_deallocation torch _inductor fn li = i range x = torch full i x = x + li append x li mod = make_fx fn live_tensors = WeakTensorKeyDictionary max_live_tensors = LiveTensors TorchDispatchMode __torch_dispatch__ func types args= kwargs=None nonlocal live_tensors nonlocal max_live_tensors kwargs = kwargs kwargs arg pytree arg_tree_leaves args kwargs isinstance arg torch Tensor live_tensors arg = True out = func args kwargs isinstance out torch Tensor out live_tensors out = True max_live_tensors = max max_live_tensors len live_tensors out mode = LiveTensors torch _inductor fx_passes joint_graph UniformValueConstantFolder mode UniformValueConstantFolder mod run there couple extra tensors created ` insertable_tensor_check ` assertTrue max_live_tensors == See https github com pytorch pytorch issues parametrize backend aot_eager inductor test_inductor_detach_view backend fn x torch Tensor - torch Tensor = x detach fn_opt = torch compile fn backend=backend inp = torch ones requires_grad=True device=GPU_TYPE inp_ref = inp detach clone requires_grad_ True out_ref = fn inp_ref out_ref sum backward out = fn_opt inp out sum backward assertEqual inp grad inp_ref grad requires_gpu unittest skipIf PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Does support mem_eff_attention test_sdpa_inference_mode_aot_compile TestSDPA torch nn Module __init__ - None super __init__ forward q torch Tensor k torch Tensor v torch Tensor attn_mask torch Tensor torch nn functional scaled_dot_product_attention q k v attn_mask=attn_mask dropout_p= is_causal=False q = torch rand device=GPU_TYPE dtype=torch bfloat k = torch rand device=GPU_TYPE dtype=torch bfloat v = torch rand device=GPU_TYPE dtype=torch bfloat attn_mask = torch rand device=GPU_TYPE dtype=torch bfloat inputs = q k v attn_mask torch export _trace export_trace torch inference_mode traced = export_trace _export_to_torch_ir TestSDPA inputs disable_constraint_solver=True restore_fqn=False torch _inductor aot_compile traced inputs skipCUDAIf SM OrLater Requires sm requires_cuda_and_triton unittest skipIf TEST_WITH_ROCM no grouped_mm support config patch implicit_fallbacks=True test_grouped_mm torch compile fullgraph=True f b offs out_dtype torch _grouped_mm b transpose - - offs=offs out_dtype=out_dtype device = cuda dtype = torch bfloat m n k n_groups = a_ref = torch randn m n_groups k device=device dtype=dtype k b_ref = torch randn n_groups n k device=device dtype=dtype k offs = torch arange m n_groups m + m device=device dtype=torch int a_ref requires_grad_ True b_ref requires_grad_ True a_test = a_ref clone detach requires_grad_ b_test = b_ref clone detach requires_grad_ out_ref = f a_ref b_ref offs out_dtype=torch bfloat out_ref sum backward out_test = f a_test b_test offs=offs out_dtype=torch bfloat out_test sum backward assertEqual out_ref out_test assertEqual a_ref grad a_test grad assertEqual b_ref grad b_test grad test_optimize_indexing_assert has_indirect code tl_fn str assertTrue tl_fn code msg=f tl_fn present \n code line code split \n tl_fn line stmt = line split tl_fn - indirect indexing involves ` tmp ` variable assertTrue tmp stmt msg=f Indirect indexing present code \n line has_assert code lower bool upper bool assertIn device_assert code msg=f No device assert found \n code line code split \n device_assert line assertTrue = line lower msg=f Lower bound lower elided line assertTrue line upper msg=f Upper bound upper elided line fn x torch Tensor - torch Tensor s = torch arange x shape device=x device x s long aten index dynamic False True fn_opt = torch compile fn dynamic=dynamic x = torch randn device=GPU_TYPE code = run_and_get_triton_code fn_opt x assertEqual fn_opt x fn x msg=f dynamic= Check there s indirect indexing has_indirect code tl_fn= tl load dynamic We elide assert static shapes assertNotIn device_assert code we generate upper bound dynamic shapes has_assert code lower=False upper=True fn z b idx idx idx = torch arange shape - device=a device index_put_ z idx idx idx b accumulate=True aten index_put dynamic False True fn_opt = torch compile fn dynamic=dynamic = torch randn device=GPU_TYPE z = torch zeros dtype=torch int device=GPU_TYPE b = torch randn device=GPU_TYPE idx = torch randint device=GPU_TYPE view idx = torch randint device=GPU_TYPE view inps = clone z b idx idx code = run_and_get_triton_code fn_opt inps Correctness out_opt = fn_opt clone z b idx idx out = fn clone z b idx idx assertEqual out_opt out msg=f dynamic= We have indirect store via atomic_add has_indirect code tl_fn= tl atomic_add We cannot elide he assert case has_assert code lower=True upper=True test_not_materialize_pointwise_reduction fn b - b sum dim=- amax dim=- N = K = fn_opt = torch compile fn backend= inductor inps = torch randn N K device=GPU_TYPE torch randn N K device=GPU_TYPE code = run_and_get_triton_code fn_opt inps assertEqual code count tl store config triton multi_kernel assertTrue out_ptr code assertFalse out_ptr code assertEqual fn_opt inps fn inps test_numpy_on_gpu x = np arange dtype=np float torch compile fn x np sin x fn_gpu x torch device GPU_TYPE fn x r = fn_gpu x code = run_and_get_triton_code fn_gpu x assertIn tl_math sin code assertEqual type r np ndarray assertEqual r np sin x config patch expand_dimension_for_pointwise_nodes=True test_rope_fusion batch_size seq_length hidden_dim = num_q_heads num_kv_heads = prepare_input batch_size seq_length q = torch randn batch_size num_q_heads seq_length hidden_dim device=GPU_TYPE k = torch randn batch_size num_kv_heads seq_length hidden_dim device=GPU_TYPE pos_ids = torch arange seq_length device=GPU_TYPE dtype=torch long unsqueeze dummy cos sin cos sin = torch randn seq_length hidden_dim device=GPU_TYPE torch randn seq_length hidden_dim device=GPU_TYPE q k cos sin pos_ids rotate_half x Rotates half hidden dims input x = x x shape - x = x x shape - torch cat -x x dim=- apply_rotary_pos_emb q k cos sin position_ids=None unsqueeze_dim= cos = cos unsqueeze unsqueeze_dim sin = sin unsqueeze unsqueeze_dim q_embed = q cos + rotate_half q sin k_embed = k cos + rotate_half k sin q_embed k_embed q k cos sin pos_ids = prepare_input batch_size seq_length compiled_fn = torch compile apply_rotary_pos_emb compiled_out = compiled_fn q k cos sin pos_ids eager_out = apply_rotary_pos_emb q k cos sin pos_ids assertEqual compiled_out eager_out make sure rope fused into kernel code = run_and_get_triton_code compiled_fn q k cos sin pos_ids assertEqual code count run test_numpy_autograd my_torch x y = torch cat torch sin x torch max x None y sum my_np x y = np concatenate np sin x np max x None np sum y torch compile wrapper x torch compiler wrap_numpy my_np x torch compile wrapper x x = x numpy y = my_np x torch from_numpy y x_np = torch arange dtype=torch float requires_grad=True x = torch arange dtype=torch float requires_grad=True out_np = wrapper x_np out = my_torch x assertEqual out out_np x _np = torch arange dtype=torch float requires_grad=True out _np = wrapper x _np assertEqual out out _np out_np backward out backward assertEqual x grad x_np grad out _np backward assertEqual x grad x _np grad Disable constant propagation so we isolate value range analysis patch object config constant_and_index_propagation False patch object config joint_graph_constant_folding False test_cant_optimize_compute ones torch ones device=GPU_TYPE suffix inp inp torch int + torch float ten = torch rand device=GPU_TYPE foo lambda x x + lambda x torch where x ones ones - - lambda x x + ten lambda x x + ten sum fn suffix foo ones fn_opt = torch compile fn backend= inductor code = run_and_get_triton_code fn_opt cannot optimized away value too large assertTrue tl int code assertEqual fn_opt fn Disable constant propagation so we isolate value range analysis patch object config constant_and_index_propagation False patch object config joint_graph_constant_folding False test_optimize_compute ones torch ones device=GPU_TYPE suffix inp inp torch int + torch float foo lambda x x + lambda x torch where x ones ones - - lambda x x fn suffix foo ones fn_opt = torch compile fn backend= inductor code = run_and_get_triton_code fn_opt can optimized away value too large assertTrue tl int code assertTrue tl int code assertEqual fn_opt fn https github com pytorch pytorch issues test_ctr_not_moved_to_cuda_when_used_in_index_put torch compile f x mask x mask = -math inf x x_tmp = torch randn device=GPU_TYPE x = x_tmp permute view - mask_tmp = torch ones dtype=torch int device=GPU_TYPE mask = mask_tmp == mask_tmp f x mask code = run_and_get_triton_code f x mask What we testing here inductor has pass move tensor constructors cpu cuda -math inf will become scalar-tensor input index_put_ we asserting when inductor allocates tensor does move tensor constructor cuda keeps CPU assertFalse empty_strided_cuda code only uncoalesced without config patch triton coalesce_tiling_analysis False config patch triton use_block_ptr False test_evict_last_non_coalesced_loads torch compile f b b sum dim=- N = inps = torch randn N N N device=GPU_TYPE permute torch randn N N N device=GPU_TYPE permute code = run_and_get_triton_code f inps lines = line line code split \n tl load line config triton multi_kernel first lines generated persistent reduction variant assertExpectedInline \n join lines \ tmp = tl load in_ptr + x + x + r rmask eviction_policy= evict_last other= tmp = tl load in_ptr + x + r rmask other= tmp = tl load in_ptr + x + x + r rmask eviction_policy= evict_last other= tmp = tl load in_ptr + x + r rmask eviction_policy= evict_first other= assertExpectedInline \n join lines \ tmp = tl load in_ptr + x + x + r _ r _mask eviction_policy= evict_last other= tmp = tl load in_ptr + x + r _ r _mask eviction_policy= evict_first other= config patch triton skip_l _cache True test_skip_l _cache torch compile f b + b N = inps = torch randn N device=GPU_TYPE torch randn N device=GPU_TYPE code = run_and_get_triton_code f inps lines = line line code split \n tl load line assertExpectedInline \n join lines \ tmp = tl load in_ptr + x xmask cache_modifier= cg tmp = tl load in_ptr + x xmask cache_modifier= cg config patch triton use_block_ptr True config patch triton coalesce_tiling_analysis False test_evict_last_non_coalesced_loads_block_ptr torch compile f b b sum dim=- N = inps = torch randn N N N device=GPU_TYPE permute torch randn N N N device=GPU_TYPE permute code = run_and_get_triton_code f inps lines = line line code split \n tl load line config triton multi_kernel first lines generated persistent reduction variant assertExpectedInline \n join lines \ tmp = tl load in_ptr + x + x + r _ rmask eviction_policy= evict_last other= tmp = tl load tl make_block_ptr in_ptr shape= strides= block_shape= XBLOCK R _BLOCK order= offsets= xoffset roffset boundary_check= padding_option= zero tmp = tl load in_ptr + x + x + r _ rmask eviction_policy= evict_last other= tmp = tl load block_ptr boundary_check= padding_option= zero eviction_policy= evict_first noqa B line too long assertExpectedInline \n join lines \ tmp = tl reshape tl load block_ptr boundary_check= padding_option= zero eviction_policy= evict_last XBLOCK R _BLOCK tmp = tl load block_ptr boundary_check= padding_option= zero eviction_policy= evict_first noqa B line too long Disable index propagation so indirect indexing isn t optimized away patch object config constant_and_index_propagation False test_computed_indirect_mask fn x n tmp = torch arange n device=x device x tmp + x = torch randn device=GPU_TYPE fn_opt = torch compile fn code = run_and_get_triton_code fn_opt x load should masked assertTrue tl load in_ptr + tmp xmask code tl load in_ptr + tmp xmask tl int code assertEqual fn x fn_opt x config patch triton prefer_nd_tiling True config patch triton max_tiles parametrize block_multiple ynumel_exceed_ygrid_size xdim has constant mask ydim does True True xdim ydim both have constant mask True False numel block multiple no constant mask False False TODO test zdim too test_has_constant_mask block_multiple ynumel_exceed_ygrid_size torch _inductor runtime hints TRITON_MAX_BLOCK torch _inductor runtime runtime_utils get_max_y_grid shape = TRITON_MAX_BLOCK Y TRITON_MAX_BLOCK X block_multiple shape = s + s shape ynumel_exceed_ygrid_size shape = shape math ceil get_max_y_grid shape + shape = torch zeros shape device=GPU_TYPE dtype=torch bool b = torch zeros shape device=GPU_TYPE dtype=torch bool opt_fn = torch compile torch add code = run_and_get_triton_code opt_fn b block_multiple assertTrue xmask = tl full code ynumel_exceed_ygrid_size assertTrue ymask = yindex ynumel code assertTrue ymask = tl full code assertTrue ymask = yindex ynumel code assertTrue xmask = xindex xnumel code config patch triton native_matmul False test_kernel_names_descriptive torch compile backend= inductor fn x x cos sin torch compile backend= inductor fn x x = torch mm x x x = torch softmax x dim= x mod = nn Sequential nn Linear nn LayerNorm nn ReLU device=GPU_TYPE torch compile backend= inductor fn x mod x func_and_kernel_aten = fn triton_poi_fused_cos_sin torch randn device=GPU_TYPE fn triton_poi_fused__softmax torch randn device=GPU_TYPE fn triton_poi_fused_native_layer_norm_relu torch randn device=GPU_TYPE func_and_kernel_torch = fn triton_poi_fused_cos_sin torch randn device=GPU_TYPE fn triton_poi_fused_softmax torch randn device=GPU_TYPE fn triton_poi_fused_LayerNorm_ReLU torch randn device=GPU_TYPE test_funcs func_and_kernel torch no_grad fn kernel_name inps func_and_kernel code = run_and_get_triton_code fn inps kernel_name code print code assertTrue kernel_name code test_funcs func_and_kernel_aten patch object config triton descriptive_names torch test_funcs func_and_kernel_torch patch object config profile_bandwidth True test_bandwidth_profiler torch compile backend= inductor fn x x = x cos x = x cos x = torch mm x x x = x sin x = x relu x inp = torch randn device=GPU_TYPE code = run_and_get_triton_code fn inp fn inp assertTrue start_graph code assertTrue end_graph code test_comment_graph_fragment torch compile backend= inductor fn x x = x sin x = x relu x inp = torch randn device=GPU_TYPE code = run_and_get_triton_code fn inp fn inp config cpp_wrapper assertTrue fused_relu_sin code assertTrue Graph fragment code assertTrue f sin Tensor f GPU_TYPE num_users= = call_function target=torch ops aten sin default code assertTrue f relu Tensor f GPU_TYPE num_users= = call_function target=torch ops aten relu default code test_split_op_with_sym fn x torch Tensor - torch Tensor split tensor sympy Integer split tensor sympy Expr torch split x x shape torch split x x shape dynamic_shapes True False torch _dynamo config patch dynamic_shapes=dynamic_shapes torch _dynamo reset fn_opt = torch compile fn backend= inductor dynamic=dynamic_shapes inps = torch randn fn_opt inps skipIfRocm unittest skipIf IS_FBCODE fbcode system python does provide torch test_indirect_device_assert dir_path = os path dirname os path realpath __file__ test_path = os path join dir_path indirect_assert_helper py fns = first_arg store second_arg same_pm_one same_pp_one test fn ndims dyn_shape one_size=False proc = subprocess Popen sys executable test_path fn str ndims str dyn_shape str one_size stdout=subprocess PIPE stderr=subprocess PIPE env= os environ MKL_THREADING_LAYER GNU stderr = proc communicate assertTrue any out bounds err decode utf- err stderr splitlines f fn ndims dyn_shape one_size fn ndims dyn_shape itertools product fns True False test fn ndims dyn_shape test first_arg False True fn dyn_shape itertools product upper upper lower lower True False test fn dyn_shape patch torch _inductor config comment_origin True patch torch _functorch config max_dist_from_bw test_inductor_sequence_nr Model torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= same bias=True bn = torch nn BatchNorm d num_features= relu = torch nn ReLU loss_fn = torch nn L Loss forward x target y = x x = conv x x = bn x x = relu x x = x + y x = torch flatten x output = loss_fn x target output get_triton_codegen optimized_module args run_with_backward result = optimized_module args result backward result res fwd_code bwd_code = run_and_get_code run_with_backward fwd_code bwd_code x = torch rand requires_grad=True device=GPU_TYPE target = torch rand device=GPU_TYPE args = x target model = Model device=GPU_TYPE opt_model = torch compile model fwd_code bwd_code = get_triton_codegen opt_model args bwd_seq_nr_set = set fwd_seq_nr_set = set idx code enumerate fwd_code bwd_code seq_nr_set = bwd_seq_nr_set idx fwd_seq_nr_set prefix = BWD idx FWD line code split \n seq_nr line res = re search r seq_nr \d+ line res seq_nr_set add int res group assertTrue bwd_seq_nr_set issubset fwd_seq_nr_set config patch coordinate_descent_tuning True triton unique_kernel_names True benchmark_kernel True skipIfRocm expectedFailureXPU unittest skipIf torch cuda is_available torch cuda get_device_capability Triton does support fp A test_red_followed_by_transposed_pointwise bs = dim = torch compile dynamic=False f b scale_a scale_b out = torch nn functional silu out_row = out out amax dim= keepdim=True torch float _e m fn out_col = out out amax dim= keepdim=True torch float _e m fn setup strides _scaled_mm out_row = out_row contiguous out_col = out_col t contiguous t torch _scaled_mm out_row scale_a scale_b out_dtype=torch bfloat torch _scaled_mm b out_col scale_a scale_b out_dtype=torch bfloat = torch randn bs dim dtype=torch bfloat device=GPU_TYPE = torch randn bs dim dtype=torch bfloat device=GPU_TYPE = torch randn dim dim dtype=torch bfloat device=GPU_TYPE t torch float _e m fn b = torch randn dim bs dtype=torch bfloat device=GPU_TYPE torch float _e m fn Scales scale_a = torch tensor device=GPU_TYPE scale_b = torch tensor device=GPU_TYPE warmup _ wrapper = run_and_get_code f b scale_a scale_b Previously indcutor decide reduction hint reduction kernel without considering pointwise nodes That will cause third reduction kernel wrapper persistent inner reduction cause bad perf We fix making third reduction non-persistent reduction improve perf x us - us assertEqual wrapper count triton_red_ assertEqual wrapper count triton_per_ DO_PERF_TEST torch profiler profile activities= torch profiler ProfilerActivity CUDA p _ range f b scale_a scale_b print p key_averages table max_name_column_width= test_non_blocking_copy_codegen Checks non_blocking arg present codegen see https github com pytorch pytorch issues fn x x device=self device non_blocking=True inp = torch randn _ code = run_and_get_code torch compile fn inp config cpp_wrapper cpp_wrapper passes True case so check more explicitly FileCheck check aoti_torch_copy_ check_same run code FileCheck check copy_ check_same True run code test_layer_norm_inplaces_after_matmul https github com pytorch pytorch issues batch_size = seq_length = hidden_size = layer_norm = torch nn LayerNorm hidden_size device=GPU_TYPE fn inp weight matmul_output = inp weight final_output = layer_norm matmul_output final_output inps = torch randn batch_size seq_length hidden_size device=GPU_TYPE torch randn hidden_size hidden_size device=GPU_TYPE fn_opt = torch compile fn code = run_and_get_triton_code fn_opt inps assertTrue len re findall r in_out_ptr\d+ code assertEqual fn_opt inps fn inps torch _functorch config patch donated_buffer True The inplace updating does happen after we fused layernorm backward torch _inductor config patch triton mix_order_reduction False test_donated_buffer_inplace batch_size = seq_length = hidden_size = inp = torch randn batch_size seq_length hidden_size requires_grad=True device=self device weight = torch randn hidden_size hidden_size requires_grad=True device=self device layer_norm = torch nn LayerNorm hidden_size device=self device fn inp weight matmul_output = inp weight final_output = layer_norm matmul_output final_output fn_opt = torch compile fn wrapper inp weight fn_opt inp weight sum backward _ code = run_and_get_code wrapper inp weight assertTrue in_out_ptr code TODO Enable case after pad_mm enabled XPU expectedFailureXPU torch _functorch config patch donated_buffer True torch _inductor config patch force_shape_pad True test_donated_buffer_inplace_gpt model implementation llm c https github com karpathy llm c blob master train_gpt py NewGELU nn Module forward input input + torch tanh math sqrt math pi input + torch pow input CausalSelfAttention nn Module __init__ config super __init__ assert config n_embd config n_head == key query value projections all heads batch c_attn = nn Linear config n_embd config n_embd output projection c_proj = nn Linear config n_embd config n_embd c_proj LLMC_RESIDUAL_SCALE_FLAG = regularization n_head = config n_head n_embd = config n_embd really bias more mask following OpenAI HF naming though register_buffer bias torch tril torch ones config block_size config block_size view config block_size config block_size forward x B T C = x size batch size sequence length embedding dimensionality n_embd calculate query key values all heads batch move head forward batch dim qkv = c_attn x q k v = qkv split n_embd dim= k = k view B T n_head C n_head transpose B nh T hs q = q view B T n_head C n_head transpose B nh T hs v = v view B T n_head C n_head transpose B nh T hs y = F scaled_dot_product_attention q k v is_causal=True y = y transpose contiguous view B T C re-assemble all head outputs side side output projection y = c_proj y y MLP nn Module __init__ config super __init__ c_fc = nn Linear config n_embd config n_embd gelu = NewGELU c_proj = nn Linear config n_embd config n_embd c_proj LLMC_RESIDUAL_SCALE_FLAG = forward x x = c_fc x x = gelu x x = c_proj x x Block nn Module __init__ config super __init__ ln_ = nn LayerNorm config n_embd attn = CausalSelfAttention config ln_ = nn LayerNorm config n_embd mlp = MLP config forward x x = x + attn ln_ x x = x + mlp ln_ x x GPTConfig block_size int = vocab_size int = n_layer int = n_head int = n_embd int = GPT nn Module __init__ config super __init__ config = config transformer = nn ModuleDict dict wte=nn Embedding config vocab_size config n_embd wpe=nn Embedding config block_size config n_embd h=nn ModuleList Block config _ range config n_layer ln_f=nn LayerNorm config n_embd lm_head = nn Linear config n_embd config vocab_size bias=False lm_head LLMC_SKIP_INIT = don t init one we will tie weights transformer wte weight = lm_head weight https paperswithcode com method weight-tying forward idx targets device = idx device b t = idx size assert t = config block_size f Cannot forward sequence length t block size only config block_size pos = torch arange t dtype=torch long device=device shape t forward GPT model itself tok_emb = transformer wte idx token embeddings shape b t n_embd pos_emb = transformer wpe pos position embeddings shape t n_embd x = tok_emb + pos_emb block transformer h x = block x x = transformer ln_f x logits = lm_head x loss = F cross_entropy logits view - logits size - targets view - ignore_index=- loss B T = ctx = torch amp autocast device_type=GPU_TYPE dtype=torch bfloat model = GPT GPTConfig model train model GPU_TYPE model = torch compile model x = torch randint B T dtype=torch int device=GPU_TYPE y = torch randint B T dtype=torch int device=GPU_TYPE wrapper x y ctx loss = model x y loss backward _ code = run_and_get_code wrapper x y The cpp_wrapper code significantly more complex so skip checking exact code lines config cpp_wrapper FileCheck check_regex r reinterpret_tensor\ \ \ reuse run code unittest skipIf triton_version_uses_attrs_dict Test only applies newer triton versions test_triton_attrs_dict_constexpr_signature fn x x sin fn_c = torch compile fn x = torch rand device=GPU_TYPE _ code = run_and_get_code fn_c x FileCheck check triton_meta check signature check XBLOCK constexpr run code unittest skipIf TEST_WITH_ROCM IS_SM no scaled_grouped_mm support test_respect_scaled_grouped_mm_layout_tag scaled_grouped_mm needs ` mat ` column-major M K N = K N must divisible num_groups = E = num_groups B_t batch size must match number groups group_size = M num_groups A = torch randn M K dtype=torch bfloat device=GPU_TYPE Row-major default Create B_t proper column-major layout B_t_transposed = torch randn E N K dtype=torch bfloat device=GPU_TYPE contiguous B_t = B_t_transposed transpose - - E K N B_t = B_t transpose - - contiguous transpose - - Verify column-major layout _is_column_major x torch Tensor - bool Check tensor column-major stride - == stride - assert x ndim == x ndim == input tensor must D D x stride - == x stride - assertTrue _is_column_major B_t offs = torch tensor group_size M dtype=torch int device=GPU_TYPE out_dtype = torch bfloat torch compile fn A_scales = torch ones M dtype=torch float device=GPU_TYPE A_scaled = A torch float A_scales unsqueeze - A_fp _row_major = A_scaled torch float _e m fn B_t_scales = torch ones E N dtype=torch float device=GPU_TYPE B_t_scaled = B_t torch float B_t_scales unsqueeze B_t_fp _col_major = B_t_scaled torch float _e m fn A_scales_reciprocal = A_scales reciprocal B_t_scales_reciprocal = B_t_scales reciprocal torch ops aten _scaled_grouped_mm A_fp _row_major B_t_fp _col_major A_scales_reciprocal B_t_scales_reciprocal offs out_dtype=out_dtype use_fast_accum=True fn config patch implicit_fallbacks=True torch _inductor config patch graph_partition True test_graph_partition_default_device_context torch library custom_op mylib cg_unsafe_op mutates_args= schema= Tensor x - Tensor device_types=GPU_TYPE tags= torch _C Tag cudagraph_unsafe cg_unsafe_op x - torch Tensor x + cg_unsafe_op register_fake _ x - torch Tensor torch empty_like x f x x += y = cg_unsafe_op x y += y f = torch compile f mode= reduce-overhead inp = torch randn device=GPU_TYPE _ code = run_and_get_code f inp config cpp_wrapper FileCheck check_count AOTICudaGuard device_guard exactly=True run code FileCheck check_count f torch GPU_TYPE _DeviceGuard exactly=True run code RNNTest TestCase device_type = GPU_TYPE Model torch nn Module __init__ - None super __init__ gru = torch nn GRU batch_first=True forward x gru x test_rnn_compile_safe device = torch device GPU_TYPE model = RNNTest Model device model = torch compile model backend= inductor x = torch rand device model x NanCheckerTest TestCase config patch nan_asserts True test_nan_checker_pass f x torch softmax x dim=- x = torch randn device=GPU_TYPE ref = f x actual code = run_and_get_code torch compile f x assertTrue torch allclose ref actual code = code config cpp_wrapper assertIn aoti_torch_check_inf_and_nan code assertIn make sure graph inputs nan inf code assertRegex code r return_vars = assertIn var return_vars code assertIn isinstance var torch Tensor code assertRegex code r assert \ isnan\ \ \ any\ \ item\ \ assertRegex code r assert \ isinf\ \ \ any\ \ item\ \ config patch nan_asserts True test_nan_checker_fail f x torch softmax x dim=- x = torch randn device=GPU_TYPE x = float nan assertRaises AssertionError config cpp_wrapper RuntimeError torch compile f x RUN_CPU TestFull TestCase test_full_dtype pytypes = bool int float TODO Triton s JITFunction _type_of has no support complex complex dtypes = torch bool torch int torch int torch float torch float None torch complex torch complex fn pytype dtype pytype bool fill_value = True pytype int fill_value = pytype float fill_value = raise AssertionError f Unexpected Python type pytype torch full fill_value dtype=dtype device=torch device cpu fn_opt = torch compile fn backend= inductor pytype dtype itertools product pytypes dtypes enable_python_dispatcher torch no_grad ret_opt = fn_opt pytype dtype assertEqual ret_opt fn pytype dtype _strip_tmp_path code str - str Canonicalize things look like tmp path so they can compared re sub #include #include tmppath code _run_and_get_stripped_kernels fn Callable _P _T args _P args kwargs _P kwargs - tuple _T list str result codes = run_and_get_kernels fn args kwargs result _strip_tmp_path code code codes __name__ == __main__ torch _inductor test_case run_tests RUN_CPU RUN_GPU run_tests needs= filelock