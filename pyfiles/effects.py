mypy allow-untyped-defs enum Enum typing Any Optional Union weakref WeakKeyDictionary torch torch utils _pytree pytree torch _C DispatchKey torch _higher_order_ops torchbind call_torchbind torch _library fake_class_registry FakeScriptObject torch _ops HigherOrderOperator torch _subclasses fake_tensor FakeTensorMode torch fx experimental proxy_tensor disable_proxy_modes_tracing ProxyTorchDispatchMode track_tensor_tree _EffectType Enum ORDERED = Ordered OpType = Union torch _ops HigherOrderOperator torch _ops OpOverload SIDE_EFFECTS = WeakKeyDictionary OpType _EffectType torch ops aten _print default _EffectType ORDERED call_torchbind _EffectType ORDERED _register_effectful_op op OpType effect _EffectType assert isinstance op torch _ops OpOverload torch _ops HigherOrderOperator has_aliasing op op SIDE_EFFECTS SIDE_EFFECTS op = effect raise RuntimeError f Already registered effect type SIDE_EFFECTS op op op f trying register different effect type effect SIDE_EFFECTS op = effect _deregister_effectful_op op OpType op SIDE_EFFECTS raise RuntimeError f Op op registered effectful del SIDE_EFFECTS op WithEffects HigherOrderOperator with_effects token op args kwargs - new_token op_results This HOP helps ensure ordering between side effectful ops like prints ops using torchbind objects This needed ensure traced graph AOTAutograd functional so future optimization passes do reorder these operators This done through threading effect tokens through graph enforce data dependence between side effectful ops The tokens basically dummy values torch tensor We create token per effect type which enumerated _EffectType enum __init__ - None super __init__ with_effects __call__ token op OpType args tuple Any kwargs dict str Any - tuple Any assert isinstance op torch _ops HigherOrderOperator torch _ops OpOverload assert has_aliasing op Ops aliasing supported assert has_effects op args kwargs assert isinstance kwargs dict super __call__ token op args kwargs with_effects = WithEffects has_aliasing op OpType NOT FOR PUBLIC USE isinstance op torch _ops HigherOrderOperator op SIDE_EFFECTS arg op _schema arguments arg alias_info None True arg op _schema returns arg alias_info None True False has_effects op args kwargs - bool Skip over profiler s RecordFunction they should show up graph _skip_ops = torch ops profiler _record_function_exit _RecordFunction op _skip_ops False isinstance op torch _ops HigherOrderOperator torch _ops OpOverload has_aliasing op get_effect_key op args kwargs None get_effect_key op args kwargs - Optional _EffectType op SIDE_EFFECTS SIDE_EFFECTS op arg args isinstance arg torch ScriptObject FakeScriptObject Add table so next time we see same op we don t have parse through args again SIDE_EFFECTS op = _EffectType ORDERED _EffectType ORDERED arg kwargs values isinstance arg torch ScriptObject FakeScriptObject Add table so next time we see same op we don t have parse through args again SIDE_EFFECTS op = _EffectType ORDERED _EffectType ORDERED None new_token_tensor - torch Tensor torch tensor with_effects py_impl DispatchKey CompositeExplicitAutograd with_effects_dense token torch Tensor op torch _ops OpOverload args tuple Any kwargs dict str Any - tuple torch Tensor out = op args kwargs new_token = new_token_tensor NOTE with_effects type Note we should only do out tuple type list type This match schema op For tuple output length schema output same length out For list output length schema output e g Tensor regardless length list isinstance out tuple new_token out new_token out with_effects py_impl FakeTensorMode with_effects_fake mode token torch Tensor op torch _ops OpOverload args tuple Any kwargs dict str Any - tuple torch Tensor mode result = with_effects_dense token op args kwargs result with_effects py_impl ProxyTorchDispatchMode with_effects_proxy mode token torch Tensor op torch _ops OpOverload args tuple Any kwargs dict str Any - tuple torch Tensor disable_proxy_modes_tracing out = with_effects token op args kwargs proxy_token = mode tracer unwrap_proxy token proxy_args = pytree tree_map mode tracer unwrap_proxy args proxy_kwargs = pytree tree_map mode tracer unwrap_proxy kwargs torch fx node has_side_effect To avoid being DCEed graph eliminate_dead_code they don t have output their outputs used has_side_effect op out_proxy = mode tracer create_proxy call_function with_effects proxy_token op proxy_args proxy_kwargs result = track_tensor_tree out out_proxy constant=None tracer=mode tracer result with_effects fallthrough DispatchKey AutogradCPU with_effects fallthrough DispatchKey AutogradCUDA _get_schema op args - torch FunctionSchema isinstance op torch _ops OpOverload op _schema op == call_torchbind getattr args args schema raise RuntimeError f Unable get schema op op handle_effects allow_token_discovery bool tokens dict _EffectType torch Tensor op OpType args tuple Any kwargs dict str Any - Any Args allow_token_discovery Whether we discovering tokens If true we will create token every side effect type seen does have token assigned yet If false tokens should ve all been created ahead time so we will error there no token mapping every effect type tokens Map effect type tokens This chain operators same effects together so they do get reordered later optimization passes Get token We can t do ` tokens get op torch tensor ` because will create empty tensor during proxy mode tracing token doesn t exist But tokens should always exist during proxy mode tracing key = get_effect_key op args kwargs assert key None key tokens assert allow_token_discovery f Could find token effect key which came function op proxy_tensor_mode = torch _C _get_dispatch_mode torch _C _TorchDispatchModeKey PROXY proxy_tensor_mode None If we discovered new token during tracing we backward Then we patch graph adding additional tangents_token input joint graph tracer = proxy_tensor_mode tracer torch fx experimental proxy_tensor disable_proxy_modes_tracing track_tensor_tree disable_proxy_modes_tracing token_tensor = new_token_tensor token_proxy = proxy_tensor_mode tracer create_proxy placeholder tangents_token name= tangents_token track_tensor_tree token_tensor token_proxy constant=None tracer=tracer tokens key = token_tensor tokens key = new_token_tensor token = tokens key torch _subclasses functional_tensor PythonFunctionalizeAPI ctx = PythonFunctionalizeAPI unwrapped_token = ctx unwrap_tensors token unwrapped_args = ctx unwrap_tensors args unwrapped_kwargs = ctx unwrap_tensors kwargs type ignore arg-type ctx redispatch_to_next new_token unwrapped_outs = with_effects unwrapped_token op unwrapped_args unwrapped_kwargs schema = _get_schema op unwrapped_args len schema returns == assert unwrapped_outs None unwrapped_outs = None type ignore assignment len schema returns == assert len unwrapped_outs == unwrapped_outs = unwrapped_outs assert len unwrapped_outs == len schema returns Add newly created token into tokens map following call use token wrapped_token = ctx wrap_tensors new_token assert isinstance wrapped_token torch Tensor tokens key = wrapped_token pyrefly ignore bad-argument-type ctx wrap_tensors unwrapped_outs