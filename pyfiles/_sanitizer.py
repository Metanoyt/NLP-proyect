mypy allow-untyped-defs r This module introduces CUDA Sanitizer tool detecting synchronization errors between kernels ran different streams It stores information accesses tensors determine they synchronized When enabled python program possible data race detected detailed warning will printed program will exit It can enabled either importing module calling func ` enable_cuda_sanitizer ` exporting ` ` TORCH_CUDA_SANITIZER ` ` environment variable enum functools inspect io logging re sys textwrap traceback collections abc Iterator dataclasses dataclass field typing Any Optional TypeVar torch torch cuda _gpu_trace gpu_trace torch utils _pytree pytree torch utils _python_dispatch TorchDispatchMode DEFAULT_STREAM_ID = TK = TypeVar TK TVa = TypeVar TVa TVb = TypeVar TVb DataPtr = int StreamId = int EventId = int SeqNum = int logger = logging getLogger __name__ Note only factories take Tensor input they ones we care about FACTORY_FUNCTION_REGEX = re compile new_ &#124; _like AccessType enum Enum READ = enum auto WRITE = enum auto __str__ reading AccessType READ writing dataclass Access r Stores information about single access tensor kernel Args type either AccessType READ AccessType Write seq_num sequential number kernel performing access stream stream id stream executing kernel operator schema launched kernel which lists arguments type aliases arguments schema access corresponds is_output Whether tensor output kernel stack_trace stack summary object captured during access type AccessType seq_num SeqNum stream StreamId operator str aliases list str is_output bool stack_trace traceback StackSummary SynchronizationError Exception Base errors detected CUDA Sanitizer UnsynchronizedAccessError SynchronizationError Stores information about two unsynchronized accesses one data pointer __init__ data_ptr DataPtr allocation_stack_trace Optional traceback StackSummary current_access Access previous_access Access data_ptr = data_ptr allocation_stack_trace = allocation_stack_trace current_access = current_access previous_access = previous_access __str__ format_access access Access message write f access operator \n access type access aliases message write argument s + join access aliases access is_output message write access is_output message write output message write f \nWith stack trace \n join access stack_trace format \n io StringIO message message write textwrap dedent f \ ============================ CSAN detected possible data race tensor data pointer data_ptr Access stream current_access stream during kernel format_access current_access message write f Previous access stream previous_access stream during kernel \n format_access previous_access allocation_stack_trace message write Tensor allocated stack trace \n f join allocation_stack_trace format message write Trace tensor allocation found message getvalue CUDASanitizerErrors Exception Wrapper errors reported CUDA Sanitizer __init__ errors list SynchronizationError errors = errors __str__ f detected len errors errors dataclass TensorInfo r Stores information about single tensor recent accesses Args allocation_stack_trace stack summary object captured during tensor allocation Can ` ` None ` ` allocation wasn t caught CSAN reads list read accesses tensor performed since last write write last write access tensor allocation_stack_trace Optional traceback StackSummary reads list Access = field default_factory=list write Optional Access = None _TensorsAccessed __init__ - None accesses dict DataPtr TensorInfo = ensure_tensor_exists data_ptr DataPtr - None data_ptr accesses logger info Found tensor pointer s no matching tensor allocation trace Backfilling trace now Perhaps sanitizer enabled after some torch operations data_ptr create_tensor data_ptr None ensure_tensor_does_not_exist data_ptr DataPtr - None data_ptr accesses logger info Found duplicate tensor allocation trace tensor pointer s Assuming trace tensor deallocation wasn t caught backfilling now Perhaps sanitizer enabled after some torch operations data_ptr delete_tensor data_ptr create_tensor data_ptr DataPtr stack_trace Optional traceback StackSummary - None accesses data_ptr = TensorInfo stack_trace delete_tensor data_ptr DataPtr - None del accesses data_ptr were_there_reads_since_last_write data_ptr DataPtr - bool bool accesses data_ptr reads get_allocation_stack_trace data_ptr DataPtr - Optional traceback StackSummary accesses data_ptr allocation_stack_trace get_write data_ptr DataPtr - Optional Access accesses data_ptr write get_reads data_ptr DataPtr - list Access accesses data_ptr reads add_read data_ptr DataPtr access Access - None accesses data_ptr reads append access set_write data_ptr DataPtr access Access - None accesses data_ptr write = access accesses data_ptr reads = StreamSynchronizations __init__ - None current_sync_states dict StreamId dict StreamId SeqNum = recorded_sync_states dict EventId dict StreamId SeqNum = host_sync_state dict StreamId SeqNum = create_stream DEFAULT_STREAM_ID _ensure_stream_exists stream StreamId - None stream current_sync_states logger info Found Stream id s no matching stream creation trace Backfilling trace now Perhaps sanitizer enabled after some torch operations stream create_stream stream _ensure_event_exists event EventId - None event recorded_sync_states logger info Found Event id s no matching event creation trace Backfilling trace now Perhaps sanitizer enabled after some torch operations event create_event event _ensure_event_does_not_exist event EventId - None event recorded_sync_states logger info Found duplicate event creation trace event id s Assuming trace event deletion wasn t caught backfilling now Perhaps sanitizer enabled after some torch operations event delete_event event create_stream stream StreamId - None stream current_sync_states logger info Found duplicate Stream creation trace Stream id s PyTorch Streams only created once so trace entry ignored stream host_sync_state stream = current_sync_states stream = host_sync_state copy create_event event EventId - None _ensure_event_does_not_exist event recorded_sync_states event = delete_event event EventId - None _ensure_event_exists event del recorded_sync_states event update_seq_num stream StreamId seq_num SeqNum - None _ensure_stream_exists stream current_sync_states stream stream = seq_num record_state event EventId stream StreamId - None _ensure_event_exists event _ensure_stream_exists stream recorded_sync_states event = current_sync_states stream copy _state_wait_for_other state dict StreamId SeqNum other dict StreamId SeqNum - None stream seq_num other items state stream = max state get stream - seq_num stream_wait_for_event stream StreamId event EventId - None _ensure_stream_exists stream _ensure_event_exists event _state_wait_for_other current_sync_states stream recorded_sync_states event all_streams_wait_for_event event EventId - None _ensure_event_exists event stream current_sync_states keys stream_wait_for_event stream event _state_wait_for_other host_sync_state recorded_sync_states event all_streams_wait_for_stream stream StreamId - None _ensure_stream_exists stream state current_sync_states values _state_wait_for_other state current_sync_states stream _state_wait_for_other host_sync_state current_sync_states stream sync_all_streams - None stream state current_sync_states items host_sync_state stream = state stream state current_sync_states values _state_wait_for_other state host_sync_state is_ordered_after current_stream StreamId seq_num SeqNum other_stream StreamId - bool _ensure_stream_exists current_stream _ensure_stream_exists other_stream seq_num = current_sync_states current_stream get other_stream - EventHandler Analyzes CSAN trace synchronization errors Stores information each stream s synchronizations other streams well tensor accesses determine whether given kernel launch might cause data race __init__ - None tensors_accessed = _TensorsAccessed syncs = StreamSynchronizations seq_num SeqNum = _handle_kernel_launch stream StreamId read_only set DataPtr read_write set DataPtr outputs set DataPtr operator str tensor_aliases dict int list str - list SynchronizationError check_conflict data_ptr DataPtr current_access Access previous_access Optional Access - None previous_access None syncs is_ordered_after current_access stream previous_access seq_num previous_access stream error_list append UnsynchronizedAccessError data_ptr tensors_accessed get_allocation_stack_trace data_ptr current_access previous_access error_list list SynchronizationError = seq_num += syncs update_seq_num stream seq_num stack_trace = traceback StackSummary extract traceback walk_stack inspect currentframe lookup_lines=False The stack trace generated way inverse order so must reversed stack_trace reverse data_ptr read_only tensors_accessed ensure_tensor_exists data_ptr current_access = Access AccessType READ seq_num stream operator tensor_aliases data_ptr data_ptr outputs stack_trace check_conflict data_ptr current_access tensors_accessed get_write data_ptr tensors_accessed add_read data_ptr current_access data_ptr read_write tensors_accessed ensure_tensor_exists data_ptr current_access = Access AccessType WRITE seq_num stream operator tensor_aliases data_ptr data_ptr outputs stack_trace tensors_accessed were_there_reads_since_last_write data_ptr previous_access tensors_accessed get_reads data_ptr check_conflict data_ptr current_access previous_access check_conflict data_ptr current_access tensors_accessed get_write data_ptr tensors_accessed set_write data_ptr current_access error_list _handle_event_creation event EventId - None syncs create_event event _handle_event_deletion event EventId - None syncs delete_event event _handle_event_record event EventId stream StreamId - None syncs record_state event stream _handle_event_wait event EventId stream StreamId - None syncs stream_wait_for_event stream event _handle_memory_allocation data_ptr DataPtr - None tensors_accessed ensure_tensor_does_not_exist data_ptr stack_trace = traceback StackSummary extract traceback walk_stack inspect currentframe lookup_lines=False The stack trace generated way inverse order so must reversed stack_trace reverse tensors_accessed create_tensor data_ptr stack_trace _handle_memory_deallocation data_ptr DataPtr - None tensors_accessed ensure_tensor_exists data_ptr tensors_accessed delete_tensor data_ptr _handle_stream_creation stream StreamId - None syncs create_stream stream _handle_device_synchronization - None syncs sync_all_streams _handle_stream_synchronization stream StreamId - None syncs all_streams_wait_for_stream stream _handle_event_synchronization event EventId - None syncs all_streams_wait_for_event event zip_by_key dict TK TVa b dict TK TVb - Iterator tuple TK TVa TVb arg value items arg b yield arg value b arg zip_arguments schema torch FunctionSchema args tuple Any kwargs dict str Any - Iterator tuple torch Argument Any schema_args = schema arguments len args schema_kwargs = arg name arg arg schema arguments len args yield zip schema_args args _ argument value zip_by_key schema_kwargs kwargs yield argument value ArgumentHandler __init__ - None dataptrs_read set DataPtr = set dataptrs_written set DataPtr = set tensor_aliases dict DataPtr list str = outputs set DataPtr = set _handle_argument value Any is_write bool metadata_only bool name Optional str = None is_output bool = False - None isinstance value torch Tensor value is_cuda data_ptr = value data_ptr is_write dataptrs_written add data_ptr metadata_only dataptrs_read add data_ptr tensor_aliases setdefault data_ptr name None tensor_aliases data_ptr append name is_output outputs add data_ptr parse_inputs schema torch FunctionSchema args tuple Any kwargs dict str Any is_factory bool - None argument value zip_arguments schema args kwargs is_write = argument alias_info None argument alias_info is_write A change metadata only view factory function reads only metadata metadata_only = is_factory argument alias_info None argument alias_info is_write pytree tree_map_ functools partial _handle_argument is_write=is_write name=argument name metadata_only=metadata_only value parse_outputs schema torch FunctionSchema outputs Any is_factory bool - None res value zip schema returns outputs metadata_only = is_factory res alias_info None res alias_info is_write pytree tree_map_ functools partial _handle_argument is_write=not metadata_only is_output=True metadata_only=metadata_only value CUDASanitizerDispatchMode TorchDispatchMode __init__ - None event_handler = EventHandler torch _C _activate_gpu_trace gpu_trace register_callback_for_event_creation event_handler _handle_event_creation gpu_trace register_callback_for_event_deletion event_handler _handle_event_deletion gpu_trace register_callback_for_event_record event_handler _handle_event_record gpu_trace register_callback_for_event_wait event_handler _handle_event_wait gpu_trace register_callback_for_memory_allocation event_handler _handle_memory_allocation gpu_trace register_callback_for_memory_deallocation event_handler _handle_memory_deallocation gpu_trace register_callback_for_stream_creation event_handler _handle_stream_creation gpu_trace register_callback_for_device_synchronization event_handler _handle_device_synchronization gpu_trace register_callback_for_stream_synchronization event_handler _handle_stream_synchronization gpu_trace register_callback_for_event_synchronization event_handler _handle_event_synchronization __torch_dispatch__ func types args= kwargs=None kwargs None kwargs = is_factory = bool FACTORY_FUNCTION_REGEX match func _schema name argument_handler = ArgumentHandler argument_handler parse_inputs func _schema args kwargs is_factory=is_factory outputs = func args kwargs argument_handler parse_outputs func _schema outputs is_factory=is_factory errors = event_handler _handle_kernel_launch torch cuda current_stream cuda_stream argument_handler dataptrs_read - argument_handler dataptrs_written argument_handler dataptrs_written argument_handler outputs func _schema argument_handler tensor_aliases errors error errors print error file=sys stderr raise CUDASanitizerErrors errors outputs CUDASanitizer Manages lifetime CUDASanitizer dispatch mode object The CUDASanitizer wraps entering exiting functions dispatch mode context manager enable function destructor respectively This explicitly set lifetime dispatch mode object application This approach deemed more elegant than using atexit module __init__ - None dispatch = CUDASanitizerDispatchMode enabled = False enable dispatch __enter__ enabled = True disable dispatch __exit__ None None None enabled = False __del__ Since object lifetime linked ` torch cuda _sanitizer ` python module often gets deleted part overall ` torch ` module cleanup At time depending CPython version torch module might different states being already cleaned up Similarly other imports might already have been cleaned up so ` sys ` might already gone well Skip exiting mode outlived runtime sys None sys is_finalizing enabled disable enable_cuda_sanitizer Enable CUDA Sanitizer The sanitizer will begin analyze low-level CUDA calls invoked torch functions synchronization errors All data races found will printed standard error output along stack traces suspected causes For best results sanitizer should enabled very beginning program cuda_sanitizer enable cuda_sanitizer = CUDASanitizer