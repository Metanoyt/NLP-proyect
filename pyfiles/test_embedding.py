Owner s module nn itertools random unittest itertools product torch torch nn nn torch nn functional F torch testing _internal common_cuda TEST_CUDA torch testing _internal common_device_type dtypes dtypesIfCUDA instantiate_device_type_tests largeTensorTest onlyCUDA onlyNativeDeviceTypes skipCUDAIf skipMeta TEST_WITH_ROCM torch testing _internal common_nn NNTestCase torch testing _internal common_utils _assertGradAndGradgradChecks dtype prec_DONTUSE instantiate_parametrized_tests IS_JETSON parametrize parametrize_test run_tests set_default_dtype skipIfTorchDynamo TestEmbeddingNN NNTestCase _do_cuda_memory_leak_check = True _do_cuda_non_default_stream = True unittest skipIf TEST_CUDA CUDA unavailable test_embedding_max_norm_unsorted_repeating_indices create_embedding device Seed RNG so we get same Embedding each time torch manual_seed torch nn Embedding num_embeddings= embedding_dim= max_norm= device ix = torch arange device= cpu dtype=torch long repeat out_cpu = create_embedding cpu ix ix = ix cuda out = create_embedding cuda ix assertEqual out cpu out_cpu test_embedding_sparse_basic embedding = nn Embedding sparse=True input = torch tensor dtype=torch long embedding input sum backward assertTrue embedding weight grad is_sparse assertEqual embedding weight grad shape embedding weight shape test_embedding_sparse_empty_tensor embedding = nn Embedding sparse=True input = torch tensor dtype=torch int embedding input sum backward assertTrue embedding weight grad is_sparse assertEqual embedding weight grad shape embedding weight shape embedding = nn Embedding sparse=True input = torch LongTensor embedding input sum backward assertTrue embedding weight grad is_sparse assertEqual embedding weight grad shape embedding weight shape test_move_sparse_half_embedding embedding = nn Embedding sparse=True assertEqual embedding weight device type cpu assertEqual embedding weight dtype torch get_default_dtype embedding torch float assertEqual embedding weight dtype torch float assertEqual embedding embedding_dim assertEqual embedding num_embeddings torch cuda is_available embedding cuda assertEqual embedding weight device type cuda embedding cpu assertEqual embedding weight device type cpu test_embedding_max_norm embedding = nn Embedding max_norm= input = torch tensor dtype=torch long output = embedding input assertEqual output output assertTrue output data norm p= dim= le all parametrize_test dtype torch uint torch int torch int torch int torch int torch float torch double test_embedding_from_pretrained dtype = torch tensor dtype=dtype embedding = nn Embedding from_pretrained assertEqual embedding weight data input = torch LongTensor output = embedding input assertEqual output test_embedding_bag_from_pretrained = torch tensor embedding = nn EmbeddingBag from_pretrained assertEqual embedding weight input = torch tensor dtype=torch long output = embedding input torch arange input size assertEqual output test_embedding_from_pretrained_padding_idx padding_idx = padding_vec = torch ones embeddings = torch rand requires_grad=True torch no_grad embeddings padding_idx = padding_vec embedding_nn = nn Embedding from_pretrained embeddings padding_idx=padding_idx assertEqual embedding_nn weight padding_idx padding_vec test_embedding_bag_from_pretrained_padding_idx padding_idx = embeddings = torch rand requires_grad=True embedding_nn = nn EmbeddingBag from_pretrained embeddings padding_idx=padding_idx assertEqual embedding_nn weight embeddings test_embedding_from_pretrained_options set_default_dtype torch double = torch tensor opts = max_norm norm_type scale_grad_by_freq False sparse True embedding = nn Embedding from_pretrained opts input = torch LongTensor output = embedding input test output weight matrix renormalized assertEqual output assertTrue ne torch arange dtype=a dtype view all assertTrue output data norm p=opts norm_type dim= le opts max_norm all test_embedding_functional = torch tensor dtype=torch long embeddings = torch rand requires_grad=True embed_old = torch nn Embedding embed_old weight data = embeddings data A silly test eager test useful when we run under PYTORCH_TEST_WITH_DYNAMO= ensures setattr correctly works assertEqual embed_old weight data embeddings data res_old = embed_old res_F = F embedding embeddings assertEqual res_old res_F embed_old = torch nn Embedding embed_old = embed_old from_pretrained embeddings padding_idx= res_old = embed_old res_F = F embedding embeddings padding_idx= assertEqual res_old res_F https github com pytorch pytorch issues unittest skipIf TEST_CUDA CUDA available largeTensorTest GB device= cuda test_large_tensors input = torch randint low= high= size= device= cuda w = torch randn device= cuda out = torch nn functional embedding input w assertEqual out dim assertEqual out numel test_embedding_bag_functional = torch tensor dtype=torch long embeddings = torch rand requires_grad=True embed_old = torch nn EmbeddingBag embed_old weight = torch nn Parameter embeddings res_old = embed_old res_F = F embedding_bag embeddings assertEqual res_old res_F embed_old = torch nn EmbeddingBag embed_old = embed_old from_pretrained embeddings padding_idx= res_old = embed_old res_F = F embedding_bag embeddings padding_idx= assertEqual res_old res_F Make sure error thrown padding_idx out bounds test_embedding_bag_padding_idx_error = torch tensor dtype=torch long num_embeddings = num_features = embeddings = torch rand num_embeddings num_features requires_grad=True functional_err_msg = r padding_idx must within number embeddings module_err_msg = r padding_idx must within num_embeddings padding_idx range - num_embeddings + num_embeddings + padding_idx -num_embeddings padding_idx = num_embeddings assertRaisesRegex RuntimeError functional_err_msg F embedding_bag embeddings padding_idx=padding_idx assertRaisesRegex AssertionError module_err_msg torch nn EmbeddingBag num_embeddings num_features padding_idx=padding_idx F embedding_bag embeddings padding_idx=padding_idx torch nn EmbeddingBag num_embeddings num_features padding_idx=padding_idx test_embeddingbag_from_pretrained = torch tensor embeddingbag = nn EmbeddingBag from_pretrained assertEqual embeddingbag weight data input = torch LongTensor output = embeddingbag input assertEqual mean keepdim=True output test_embeddingbag_from_pretrained_options set_default_dtype torch double = torch tensor opts = max_norm norm_type scale_grad_by_freq False mode max sparse False embeddingbag = nn EmbeddingBag from_pretrained opts input = torch LongTensor output = embeddingbag input assertEqual max keepdim=True output assertTrue ne torch arange dtype=a dtype view all assertTrue norm p=opts norm_type dim= le opts max_norm all test_embeddingbag_include_last_offset Test case https github com pytorch pytorch issues embeddingbag = nn EmbeddingBag include_last_offset=True padding_idx= input = torch tensor out = embeddingbag input torch tensor out = embeddingbag input torch tensor weight = embeddingbag weight row = weight mean row = weight ref_out = torch stack row row assertEqual ref_out out assertEqual ref_out out TestEmbeddingNNDeviceType NNTestCase test_embedding_dense_grad device set_default_dtype torch double embd = nn Embedding device weight = embd weight fn_wrapper device fn weight inp = torch tensor dtype=torch long device torch nn functional embedding inp weight fn fn = fn_wrapper device _assertGradAndGradgradChecks fn weight test_embedding_scalar_weight_error device indices = torch rand device=device long weights = torch tensor device=device torch tensor device=device reshape weight weights assertRaisesRegex RuntimeError weight must -D torch nn functional embedding indices weight dtypesIfCUDA torch float torch float dtypes torch float test_embedding_backward device dtype embedding = nn Embedding sparse=True tensor = torch tensor ones = torch tensor dtype=dtype expand tensorTwice = tensor repeat onesTwice = torch cat ones ones embedding = embedding dtype=dtype device tensor = tensor device ones = ones device tensorTwice = tensorTwice device onesTwice = onesTwice device embedding zero_grad embedding tensor sum backward assertEqual embedding weight grad _indices tensor assertEqual embedding weight grad _values ones embedding zero_grad embedding tensor sum backward embedding tensor sum backward assertEqual embedding weight grad _indices tensorTwice assertEqual embedding weight grad _values onesTwice embedding zero_grad embedding tensor sum backward tensor = embedding tensor sum backward tensorTwice = assertEqual embedding weight grad _indices tensorTwice assertEqual embedding weight grad _values onesTwice dtypesIfCUDA torch float torch double torch bfloat torch half TEST_WITH_ROCM torch float torch double torch half dtypes torch float test_embedding_max_norm_backward device dtype can t use gradcheck since place renorm makes analytical gradients different produced ones weight = torch randn device=device dtype=dtype weight requires_grad_ inp_list = inp = torch tensor inp_list device=device out = nn functional embedding inp weight max_norm= sum out backward expected_grad = torch tensor device=device dtype=dtype transpose expand assertEqual weight grad expected_grad dtypesIfCUDA torch float torch double torch bfloat torch half TEST_WITH_ROCM torch float torch double torch half dtypes torch float test_embedding_max_norm_fwd_AD device dtype torch device device type == xla skipTest forward AD doesn t work xla can t use gradcheck since place renorm makes analytical gradients different produced ones weight = torch randn device=device dtype=dtype tangent = torch ones device=device dtype=dtype inp = torch tensor device=device torch autograd forward_ad dual_level dual_weight = torch autograd forward_ad make_dual weight tangent out = nn functional embedding inp dual_weight max_norm= jvp = torch autograd forward_ad unpack_dual out tangent expected_grad = torch ones device=device dtype=dtype assertEqual jvp expected_grad dtypesIfCUDA torch float torch double torch bfloat torch half TEST_WITH_ROCM torch float torch double torch half dtypes torch float test_embedding_padding_idx device dtype embedding = nn Embedding padding_idx= device dtype input = torch tensor dtype=torch long device output = embedding input assertEqual output sum assertEqual output sum embedding = nn Embedding padding_idx= sparse=True device dtype input = torch tensor dtype=torch long device output = embedding input assertEqual output sum assertEqual output sum negative indexing check padding_idx padding_idx=- num_embeddings= == index padded embedding = nn Embedding padding_idx=- device dtype input = torch tensor dtype=torch long device output = embedding input assertEqual output sum assertEqual output sum embedding = nn Embedding padding_idx=- sparse=True device dtype input = torch tensor dtype=torch long device output = embedding input assertEqual output sum assertEqual output sum change padding vector padding_vector = torch ones dtype=dtype device=device embedding = nn Embedding padding_idx= sparse=True device dtype torch no_grad embedding weight = padding_vector input = torch tensor dtype=torch long device output = embedding input assertEqual output padding_vector out bounds check padding_idx assertRaises AssertionError nn Embedding num_embeddings= embedding_dim= padding_idx= assertRaises AssertionError nn Embedding num_embeddings= embedding_dim= padding_idx=- padding_idx = embedding = nn Embedding padding_idx=padding_idx device dtype n Need large N trigger all methods we have implemented other_indices indices = torch tensor other_indices + padding_idx n dtype=torch long device pre = embedding weight padding_idx clone embedding indices sum backward after = embedding weight + embedding weight grad padding_idx embedding zero_grad assertEqual after pre test double backward emb_sum = embedding indices sum emb_grad = torch autograd grad outputs=emb_sum inputs=list embedding parameters retain_graph=True scalar = emb_grad sum + emb_sum scalar backward after = embedding weight + embedding weight grad padding_idx embedding zero_grad assertEqual after pre Check correctness torch nn functional embedding_bag forward backward functions padding_idx given D input separated into bags offset array Compare against equivalent D input uses padding indices fill gaps indicated offset array skipIfTorchDynamo see https github com pytorch pytorch pull onlyNativeDeviceTypes dtypes torch float torch float dtypesIfCUDA torch half torch bfloat test_embedding_bag_ D_padding_idx device dtype num_features = max_indices_per_bag = num_bags = num_words = gen_ D_indices_offsets include_last_offset allpad indices = offsets = cur_offset = Make one bag full one bag empty extra coverage empty_bag = random randint num_bags - full_bag = empty_bag while full_bag == empty_bag full_bag = random randint num_bags - bag range num_bags offsets append cur_offset bag == full_bag bag_size = max_indices_per_bag bag == empty_bag bag_size = bag_size = random randint max_indices_per_bag - indices += allpad random randint num_words - _ range bag_size cur_offset += bag_size embedding_bag requires first entry offsets assert offsets == indices = torch tensor indices device=device include_last_offset offsets append indices size offsets = torch tensor offsets device=device indices offsets Convert -D indices-offsets representation into -D Fill any empty indices padding_idx gen_ D_indices_from_ D indices_ D offsets include_last_offset padding_idx assert offsets == include_last_offset offsets = offsets - indices_ D = torch empty num_bags max_indices_per_bag device=device dtype=torch long bag range num_bags Determine start end position bag within indices_ D start = offsets bag end = len indices_ D bag + == num_bags offsets bag + end = min len indices_ D end Pull out bag s indices indices_ D fill any remaining space padding indices indices_in_bag = item_pos range max_indices_per_bag start + item_pos end indices_in_bag append indices_ D start + item_pos indices_in_bag append padding_idx indices_ D bag = torch tensor indices_in_bag device=device indices_ D test_cases = product max mean sum False True False True False True mode sparse include_last_offset allpad test_cases Max sparse bfloat supported mode == max sparse dtype == torch bfloat continue indices_ D offsets = gen_ D_indices_offsets include_last_offset allpad padding_idx_ D list set indices_ D tolist + None msg = f mode mode sparse sparse include_last_offset include_last_offset f padding_idx_ D padding_idx_ D If D input does use padding index we still need one D input so we can add one dummy word weights act padded word padding_idx_ D = padding_idx_ D padding_idx_ D None num_words num_words_with_padding = num_words padding_idx_ D None num_words + indices_ D = gen_ D_indices_from_ D indices_ D offsets include_last_offset padding_idx_ D weights = torch randn num_words_with_padding num_features dtype=dtype device=device requires_grad=True weights_check = weights detach clone requires_grad_ True bag = torch nn functional embedding_bag indices_ D weights offsets padding_idx=padding_idx_ D mode=mode sparse=sparse include_last_offset=include_last_offset bag_check = torch nn functional embedding_bag indices_ D weights_check padding_idx=padding_idx_ D mode=mode sparse=sparse assertEqual bag bag_check msg=msg bag sum backward bag_check sum backward Sometimes half dtype gradients mismatch greater amount than other dtypes dtype torch half torch bfloat atol = rtol = atol = None rtol = None assertEqual weights grad weights_check grad msg=msg atol=atol rtol=rtol onlyCUDA dtypes torch bfloat largeTensorTest GB device= cuda test_embedding_backward_large_batch_overflow device dtype Test embedding_dense_backward handles large batches exceed INT _MAX thread IDs This reproduces bug where gid = blockIdx x blockDim x + threadIdx x overflows when declared int causing negative indices illegal memory access Parameters chosen GUARANTEE int overflow num_indices = _ _ embedding_dim = num_weights = padding_idx = - scale_grad_by_freq = False Verify parameters guarantee overflow NROWS_PER_THREAD = max_segments = min num_indices num_weights min_partial_for_overflow = required_indices = min_partial_for_overflow - max_segments NROWS_PER_THREAD assert num_indices required_indices f Test bug num_indices= num_indices too small Need required_indices Generate indices create many partial segments Strategy ~ unique indices each appearing many times num_unique = unique_indices = torch randint num_unique dtype=torch int device=device counts = torch randint num_unique dtype=torch int device=device Normalize exactly num_indices counts = counts float counts float sum num_indices long counts - = num_indices - counts - sum indices = torch repeat_interleave unique_indices counts assert indices numel == num_indices Verify we ll trigger overflow approx_partial_segments = num_indices NROWS_PER_THREAD + max_segments stride_warped = embedding_dim + total_threads = approx_partial_segments stride_warped assert total_threads - f Test bug threads= total_threads = INT _MAX won t trigger overflow Create gradient output grad_output = torch randn num_indices embedding_dim dtype=dtype device=device This should complete without error after fix Before fix RuntimeError illegal memory access grad_weight = torch ops aten embedding_dense_backward grad_output indices num_weights padding_idx scale_grad_by_freq Verify output shape assert grad_weight shape == num_weights embedding_dim assert grad_weight dtype == torch bfloat Check correctness torch nn functional embedding_bag forward backward functions padding_idx given D indices input Compare against torch nn functional embedding followed reduction onlyNativeDeviceTypes dtypes torch float torch float dtypesIfCUDA torch half torch bfloat test_embedding_bag_ D_padding_idx device dtype Use Python implementation embedding_bag padding_idx support check torch nn functional embedding_bag correctness embedding_bag_check indices weights mode sparse padding_idx assert padding_idx None embedding = torch nn functional embedding indices weights padding_idx=padding_idx sparse=sparse reduction_dim = indices dim - mode == sum mode == mean We must avoid including elements padding_idx sum mean so multiply those elements multiply all other elements per_sample_weights = indices ne padding_idx dtype unsqueeze - res = embedding mul per_sample_weights sum dim=reduction_dim mode == mean weights_sum = per_sample_weights sum dim=reduction_dim res = res div weights_sum mode == max We must avoid allowing elements padding_idx chosen max so set those elements negative infinity res = embedding masked_fill indices unsqueeze - == padding_idx -float inf amax dim=reduction_dim raise RuntimeError f mode mode available If row all padding set its corresponding result row This needed because above mean max mode implementations set these elements nan -inf respectively mode mean max res = res masked_fill indices eq padding_idx all dim=- unsqueeze - res num_features = num_words = indices_dim = mode sparse allpad indices_dim product max mean sum False True False True Max sparse bfloat supported mode == max sparse dtype == torch bfloat continue allpad indices = torch empty indices_dim indices_dim dtype=torch long device=device fill_ indices = torch randint num_words indices_dim indices_dim device=device indices_dim Fill one row duplicate index so we can test fully padded row duplicate_row = random randint indices_dim - indices duplicate_row = indices duplicate_row padding_idx list set indices flatten - tolist weights = torch randn num_words num_features dtype=dtype device=device requires_grad=True weights_check = weights detach clone requires_grad_ True msg = f mode mode sparse sparse padding_idx padding_idx f allpad allpad indices size indices size Check forward Python implementation padding_idx embedding_bag bag_check = embedding_bag_check indices weights_check mode sparse padding_idx bag = torch nn functional embedding_bag indices weights padding_idx=padding_idx mode=mode sparse=sparse assertEqual bag bag_check msg=msg bag_check sum backward grad_check = weights_check grad bag sum backward grad = weights grad Sometimes half dtype gradients mismatch greater amount than other dtypes dtype torch half torch bfloat atol = rtol = atol = None rtol = None assertEqual grad grad_check msg=msg atol=atol rtol=rtol onlyCUDA dtypes torch float torch double torch bfloat torch half TEST_WITH_ROCM torch float torch double torch half test_embedding_max_norm_device device dtype embedding = nn Embedding max_norm= device dtype=dtype nn Embedding only takes LongTensor input input = torch tensor device=device dtype=torch long output = embedding input assertEqual output output assertTrue output data norm p= dim= le all dtypes itertools product torch int torch long torch int torch long test_embedding_bag_empty_input device dtypes m = n = x = torch tensor device=device dtype=dtypes sparse True False Embed = torch nn EmbeddingBag m n sparse=sparse Embed device output = Embed input=x offsets=torch tensor device=device dtype=dtypes assertEqual output torch zeros_like output output = Embed input=x offsets=torch tensor device=device dtype=dtypes assertEqual output torch zeros_like output skipCUDAIf True no out-of-bounds check CUDA perf dtypes itertools product torch float torch double torch int torch long parametrize_test padding_idx None parametrize_test mode sum mean max test_embedding_bag_out_of_bounds_idx device dtypes padding_idx mode padding_idx = w_dtype idx_dtype = dtypes negative out-of-bound idx = torch tensor - device=device dtype=idx_dtype positive out-of-bound idx = torch tensor device=device dtype=idx_dtype weight = torch randn device=device dtype=w_dtype mode == sum Only ` sum ` supports per_sample_weight per_sample_weights = None torch randn_like idx device=device dtype=w_dtype per_sample_weights = None p_s_weights idx itertools product per_sample_weights idx idx msg = Expected idx = idx num_embeddings assertRaisesRegex RuntimeError msg torch nn functional embedding_bag idx weight per_sample_weights=p_s_weights padding_idx=padding_idx mode=mode test_embedding_bag_dimension_errors device funcs = lambda x y z torch nn functional embedding_bag y x z torch embedding_bag torch _embedding_bag torch _embedding_bag_forward_only i f enumerate funcs err_type = ValueError RuntimeError i == RuntimeError weight = torch full dtype=torch float device=device indices = torch full dtype=torch int device=device offsets = torch full dtype=torch int device=device i == error_msg = input has D D Tensor error_msg = input has D D Tensor torch _dynamo disable assertRaisesRegex err_type error_msg lambda f weight indices offsets weight = torch full dtype=torch float device=device indices = torch full dtype=torch int device=device torch _dynamo disable assertRaisesRegex err_type offsets has D Tensor lambda f weight indices offsets weight = torch full dtype=torch float device=device indices = torch full dtype=torch int device=device offsets = torch full dtype=torch int device=device torch _dynamo disable assertRaisesRegex err_type weight has D Tensor lambda f weight indices offsets dtypes itertools product torch int torch long torch int torch long test_EmbeddingBag_per_sample_weights_failures device dtypes Failure mismatched embeddings per_sample_weights dtype only CPU device es = nn EmbeddingBag mode= sum dtype=torch float device=device input = torch tensor dtype=dtypes device=device offsets = torch tensor dtype=dtypes device=device per_sample_weights = torch randn_like input dtype=torch double device=device device == cpu assertRaisesRegex RuntimeError have same type es input offsets per_sample_weights Failure input per_sample_weights have different sizes d input input = torch tensor dtype=dtypes device=device offsets = torch tensor dtype=dtypes device=device per_sample_weights = torch randn dtype=torch float device=device assertRaisesRegex ValueError same shape input es input offsets per_sample_weights Failure input per_sample_weights have different sizes d input input = torch randint dtype=dtypes device=device offsets = None per_sample_weights = torch randn dtype=torch float device=device assertRaisesRegex ValueError same shape input es input offsets per_sample_weights Failure Unsupported per_sample_weights mode= max mean unsupported_mode max mean es = nn EmbeddingBag mode=unsupported_mode dtype=torch float device=device input = torch randint dtype=dtypes device=device offsets = None per_sample_weights = torch randn dtype=torch float device=device assertRaisesRegex NotImplementedError only supported mode= sum es input offsets per_sample_weights _embedding_bag_reference_impl input weight offsets=None mode= sum per_sample_weights=None include_last_offset=False assert mode == sum per_sample_weights None assert offsets None per_sample_weights None per_sample_weights = torch ones input size dtype=weight dtype device=weight device assert input numel == per_sample_weights numel bags = long_input = input torch long embeddings = weight index_select long_input per_sample_weights unsqueeze include_last_offset index range len offsets - offset = offsets index next_offset = offsets index + length = next_offset - offset length == bags append torch tensor weight size dtype=embeddings dtype device=embeddings device mode == sum bags append embeddings narrow offset length sum mode == mean bags append embeddings narrow offset length sum div length assert mode == max bags append embeddings narrow offset length max index offset enumerate offsets index + len offsets next_offset = offsets index + next_offset = len long_input length = next_offset - offset length == bags append torch tensor weight size dtype=embeddings dtype device=embeddings device mode == sum bags append embeddings narrow offset length sum mode == mean bags append embeddings narrow offset length sum div length assert mode == max bags append embeddings narrow offset length max torch stack bags skipMeta dtypes itertools product torch int torch long torch int torch long torch half torch bfloat torch float torch double dtypesIfCUDA itertools product torch int torch long torch int torch long torch float torch double torch half test_EmbeddingBag_empty_per_sample_weights_and_offsets device dtypes Test empty input per sample weight backward pass There CUDA invalid configuration bug more context test_per_sample_weights mode trainable_scale es = nn EmbeddingBag mode=mode dtype=dtypes device=device es weight data copy_ torch arange device=device view_as es weight dtypes input = torch tensor device=device dtype=dtypes offsets = torch tensor device=device dtype=dtypes per_sample_weights = torch randn_like input dtype=dtypes requires_grad_ trainable_scale ref_per_sample_weights = per_sample_weights detach requires_grad_ trainable_scale reference_weights = es weight detach requires_grad_ expected = _embedding_bag_reference_impl input reference_weights offsets mode ref_per_sample_weights result = es input offsets per_sample_weights assertEqual result expected atol=dtype prec_DONTUSE dtypes rtol= grad = torch randn_like expected result backward grad reference impl doesn t have grad fn empty input grad should simply zero tensor ref_weights_grad = torch zeros_like es weight assertEqual es weight grad ref_weights_grad atol=dtype prec_DONTUSE dtypes rtol= trainable_scale ref_per_sample_weights_grad = torch empty_like per_sample_weights assertEqual per_sample_weights grad ref_per_sample_weights_grad atol=dtype prec_DONTUSE dtypes rtol= modes = sum trainable_scale = True False mode trainable itertools product modes trainable_scale test_per_sample_weights mode trainable skipMeta dtypes itertools product torch int torch long torch int torch long torch float torch double torch half torch bfloat dtypesIfCUDA itertools product torch int torch long torch int torch long torch float torch double torch half test_EmbeddingBag_per_sample_weights_and_offsets device dtypes test_per_sample_weights mode trainable_scale es = nn EmbeddingBag mode=mode dtype=dtypes device=device es weight data copy_ torch arange device=device view_as es weight dtypes input = torch tensor device=device dtype=dtypes offsets = torch tensor device=device dtype=dtypes per_sample_weights = torch randn_like input dtype=dtypes requires_grad_ trainable_scale ref_per_sample_weights = per_sample_weights detach requires_grad_ trainable_scale reference_weights = es weight detach requires_grad_ expected = _embedding_bag_reference_impl input reference_weights offsets mode ref_per_sample_weights result = es input offsets per_sample_weights assertEqual result expected atol=dtype prec_DONTUSE dtypes rtol= grad = torch randn_like expected dtype=dtypes device=device result backward grad expected backward grad assertEqual es weight grad reference_weights grad atol=dtype prec_DONTUSE dtypes rtol= trainable_scale assertEqual per_sample_weights grad ref_per_sample_weights grad atol=dtype prec_DONTUSE dtypes rtol= modes = sum trainable_scale = True False mode trainable itertools product modes trainable_scale test_per_sample_weights mode trainable skipMeta dtypes itertools product torch int torch long torch int torch long torch float torch double torch half torch bfloat dtypesIfCUDA itertools product torch int torch long torch int torch long torch float torch double torch half test_EmbeddingBag_per_sample_weights_and_new_offsets device dtypes test_per_sample_weights_new_offsets mode trainable_scale include_last_offset has_weight=True es = nn EmbeddingBag mode=mode include_last_offset=include_last_offset dtype=dtypes device=device es weight data copy_ torch arange device=device view_as es weight dtypes input = torch tensor device=device dtype=dtypes offsets = torch tensor device=device dtype=dtypes include_last_offset offsets = torch cat offsets torch tensor input size device=device dtype=dtypes has_weight per_sample_weights = torch randn_like input device=device dtype=dtypes requires_grad_ trainable_scale ref_per_sample_weights = per_sample_weights detach requires_grad_ trainable_scale per_sample_weights = None ref_per_sample_weights = None reference_weights = es weight detach requires_grad_ expected = _embedding_bag_reference_impl input reference_weights offsets mode ref_per_sample_weights include_last_offset result = es input offsets per_sample_weights assertEqual result expected atol=dtype prec_DONTUSE dtypes rtol= grad = torch randn_like expected result backward grad expected backward grad assertEqual es weight grad reference_weights grad atol=dtype prec_DONTUSE dtypes rtol= has_weight trainable_scale assertEqual per_sample_weights grad ref_per_sample_weights grad atol=dtype prec_DONTUSE dtypes rtol= trainable_scale = True False include_last_offset_list = True False modes = sum False sum True max False mean False mode has_weight trainable include_last_offset itertools product modes trainable_scale include_last_offset_list test_per_sample_weights_new_offsets mode trainable include_last_offset has_weight _test_EmbeddingBag_vs_Embedding N D B L max_norm=None mode= mean device= cpu wdtype=torch float dtype=torch long test_per_sample_weights=False trainable_per_sample_weights=False sparse=False test_backward=True backward_prec=None es = nn EmbeddingBag N D mode=mode sparse=sparse max_norm=max_norm device wdtype e = nn Embedding N D max_norm=max_norm device wdtype e weight data copy_ es weight input = torch randint N B L device=device dtype=dtype offsets = torch arange B device=device dtype=dtype mul_ L grad_output = torch rand B D device=device dtype=wdtype test_per_sample_weights To prevent large gradients weights should sum each bag per_sample_weights = torch randn B L device=device dtype=wdtype softmax dim=- per_sample_weights_reference = per_sample_weights clone requires_grad_ trainable_per_sample_weights per_sample_weights requires_grad_ trainable_per_sample_weights output = es input view - offsets per_sample_weights view - output = es input view - offsets per_sample_weights = None per_sample_weights_reference = None mode == sum test_per_sample_weights ref_output = e input per_sample_weights_reference unsqueeze - sum ref_output = e input sum mode == mean assert test_per_sample_weights ref_output = e input mean mode == max assert test_per_sample_weights ref_output = e input max assertEqual output ref_output atol=dtype prec_DONTUSE wdtype rtol= test_backward output backward grad_output ref_output backward grad_output es_weight_grad = es weight grad sparse es_weight_grad = es weight grad to_dense We have more floating point error here because we dealing larger numbers backward_prec None needed_prec = dtype prec_DONTUSE wdtype rtol = wdtype == torch half needed_prec = backward_prec rtol = assertEqual es_weight_grad e weight grad atol=needed_prec rtol=rtol test_per_sample_weights trainable_per_sample_weights assertEqual per_sample_weights grad per_sample_weights_reference grad atol=dtype prec_DONTUSE wdtype rtol= dtypesIfCUDA itertools product torch int torch long torch half torch float torch double dtypes itertools product torch int torch long torch float torch double test_EmbeddingBag_per_sample_weights_and_no_offsets device dtypes run_tests mode sparse trainable_per_sample_weights kwargs = dict test_per_sample_weights=True device=device mode=mode wdtype=dtypes dtype=dtypes sparse=sparse trainable_per_sample_weights=trainable_per_sample_weights Simple case _test_EmbeddingBag_vs_Embedding kwargs B L _test_EmbeddingBag_vs_Embedding kwargs Large num_embedding _test_EmbeddingBag_vs_Embedding kwargs Large embedding_dim _test_EmbeddingBag_vs_Embedding kwargs modes = sum sparsity = True False trainable_scale = True False mode sparse trainable_per_sample_weights itertools product modes sparsity trainable_scale run_tests mode sparse trainable_per_sample_weights Test CUDA Dense half precision device == cuda modes = sum sparsity = False trainable_scale = True False mode sparse trainable_per_sample_weights itertools product modes sparsity trainable_scale run_tests mode sparse trainable_per_sample_weights _test_EmbeddingBag device mode sparse wdtype=torch double dtype=torch long odtype=torch long test_backward=True check known test example es = nn EmbeddingBag mode=mode sparse=sparse device wdtype es weight data copy_ torch arange device=device view_as es weight wdtype input = torch tensor device=device dtype=dtype offsets = torch tensor device=device dtype=odtype grad_output = torch tensor device=device dtype=wdtype view grad_output_with_empty = torch tensor device=device dtype=wdtype view mode == sum mode == mean denominator = mode == sum expected_output = torch tensor device=device dtype=wdtype denominator expected_output_with_empty = torch tensor device=device dtype=wdtype denominator expected_grad_weight = torch tensor device=device dtype=wdtype denominator mode == max expected_output = torch tensor device=device dtype=wdtype expected_output_with_empty = torch tensor device=device dtype=wdtype expected_grad_weight = torch tensor device=device dtype=wdtype output = es input offsets output backward grad_output_with_empty es_weight_grad = es weight grad sparse es_weight_grad = es weight grad to_dense assertEqual output expected_output_with_empty assertEqual es_weight_grad expected_grad_weight atol=dtype prec_DONTUSE wdtype rtol= check same example except D x input = input view - es zero_grad output = es input output backward grad_output es_weight_grad = es weight grad sparse es_weight_grad = es weight grad to_dense assertEqual output expected_output assertEqual es_weight_grad expected_grad_weight atol=dtype prec_DONTUSE wdtype rtol= test all empty bags es zero_grad inputs = torch tensor dtype=dtype device=device offsets = torch tensor dtype=odtype device=device es inputs offsets sum backward dense_grad = es weight grad dense_grad is_sparse dense_grad = dense_grad to_dense assertEqual dense_grad torch zeros_like es weight now compare EmbeddingBag vs Embedding + Sum Mean constant bag length N D B L = random randint random randint random randint random randint kwargs = dict mode=mode sparse=sparse device=device wdtype=wdtype dtype=dtype test_backward=test_backward _test_EmbeddingBag_vs_Embedding N D B L kwargs max_norm None p itertools product repeat= _test_EmbeddingBag_vs_Embedding p max_norm=max_norm kwargs check giving illegal input combos raises error es = nn EmbeddingBag mode=mode sparse=sparse input = torch ones dtype=dtype offset = torch arange dtype=odtype torch _dynamo disable assertRaises ValueError lambda es input offset torch _dynamo disable assertRaises ValueError lambda es input view - offset = device_type == cpu torch _dynamo disable assertRaises RuntimeError lambda es input view - offset offset = offset - = torch _dynamo disable assertRaises RuntimeError lambda es input view - offset skipMeta dtypes itertools product torch int torch long torch int torch long torch float torch double torch half torch bfloat dtypesIfCUDA itertools product torch int torch long torch int torch long torch float torch double torch half test_embedding_bag_device device dtypes IS_JETSON torch bfloat dtypes device == cpu skipTest bfloat supported Jetson cpu set_default_dtype torch double _test_EmbeddingBag device sum False wdtype=dtypes dtype=dtypes odtype=dtypes _test_EmbeddingBag device mean False wdtype=dtypes dtype=dtypes odtype=dtypes _test_EmbeddingBag device max False wdtype=dtypes dtype=dtypes odtype=dtypes test_backward = False device_type == cuda see todo test_embedding_bag test_backward = dtypes torch float device_type == cpu TODO figure out why precision sparse embeddings isn t same dense test_backward = dtypes torch float dtypes torch float _test_EmbeddingBag device sum True wdtype=dtypes dtype=dtypes odtype=dtypes test_backward=test_backward _test_EmbeddingBag device mean True wdtype=dtypes dtype=dtypes odtype=dtypes test_backward=test_backward skipMeta dtypes itertools product torch int torch long torch int torch long torch float torch double torch half torch bfloat dtypesIfCUDA itertools product torch int torch long torch int torch long torch float torch double torch half test_embedding_bag_non_contiguous_weight device dtypes weight_tensor = torch randn dtype=dtypes device=device weight_tensor_non_contig = weight_tensor This non-contiguous strided weight_tensor_contig = weight_tensor_non_contig clone contiguous Contig-strided index = torch tensor dtype=dtypes device=device offsets = torch tensor dtype=dtypes device=device mode sum mean max output_non_contig = F embedding_bag input=index weight=weight_tensor_non_contig offsets=offsets mode=mode output_contig = F embedding_bag input=index weight=weight_tensor_contig offsets=offsets mode=mode assertEqual output_non_contig output_contig onlyNativeDeviceTypes currently fails XLA dtypes itertools product torch int torch long torch int torch long test_embedding_bag_bfloat device dtypes set_default_dtype torch double _test_EmbeddingBag device sum True wdtype=torch bfloat dtype=dtypes odtype=dtypes test_backward=True _test_EmbeddingBag device mean True wdtype=torch bfloat dtype=dtypes odtype=dtypes test_backward=True onlyNativeDeviceTypes currently fails XLA dtypes itertools product torch int torch long torch int torch long test_embedding_bag_half device dtypes _test_EmbeddingBag device sum True wdtype=torch float dtype=dtypes odtype=dtypes test_backward=True parametrize_test bag_use_grad per_sample_weights_use_grad True True True False False True False False test_embedding_bag_per_sample_weights_grad device bag_use_grad bool per_sample_weights_use_grad bool bag = torch nn EmbeddingBag mode= sum device=device bag requires_grad_ bag_use_grad x = torch arange device=device expand - w = torch rand device=device requires_grad=per_sample_weights_use_grad bag x per_sample_weights=F softmax w dim=- instantiate_device_type_tests TestEmbeddingNNDeviceType globals instantiate_parametrized_tests TestEmbeddingNN __name__ == __main__ run_tests