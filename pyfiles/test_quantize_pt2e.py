Owner s oncall quantization ruff noqa F torch torch Tensor torch ao quantization observer ObserverOrFakeQuantize QConfigMapping torch ao quantization qconfig default_per_channel_symmetric_qnnpack_qconfig float_qparams_weight_only_qconfig per_channel_weight_observer_range_neg_ _to_ QConfig weight_observer_range_neg_ _to_ torch ao quantization quantize_pt e convert_pt e prepare_pt e prepare_qat_pt e torch ao quantization quantizer DerivedQuantizationSpec EdgeOrNode FixedQParamsQuantizationSpec QuantizationAnnotation QuantizationSpec Quantizer SharedQuantizationSpec torch ao quantization quantizer composable_quantizer noqa F ComposableQuantizer torch ao quantization quantizer embedding_quantizer noqa F EmbeddingQuantizer torch ao quantization quantizer xnnpack_quantizer get_symmetric_quantization_config XNNPACKQuantizer torch ao quantization quantizer xnnpack_quantizer_utils OP_TO_ANNOTATOR QuantizationConfig torch export export torch fx Node torch testing _internal common_quantization NodeSpec ns PT EQuantizationTestCase skipIfNoQNNPACK TestHelperModules torch testing _internal common_utils instantiate_parametrized_tests parametrize skipIfHpu TemporaryFileName TEST_CUDA TEST_HPU skipIfNoQNNPACK TestQuantizePT E PT EQuantizationTestCase test_simple_quantizer TODO use OP_TO_ANNOTATOR BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule node model graph nodes node op == call_function node target == torch ops aten conv d default input_act = node args assert isinstance input_act Node weight = node args assert isinstance weight Node bias = node args assert isinstance bias Node act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer weight_qspec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_weight_observer bias_qspec = QuantizationSpec dtype=torch float is_dynamic=False observer_or_fake_quant_ctr=observer PlaceholderObserver node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec weight weight_qspec bias bias_qspec output_qspec=act_qspec _annotated=True validate model torch fx GraphModule - None pass example_inputs = torch randn node_occurrence = two input first conv one output first conv torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default node_list = torch ops quantized_decomposed dequantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default _test_quantizer TestHelperModules ConvWithBNRelu relu=False bn=False example_inputs BackendAQuantizer node_occurrence node_list test_wo_annotate_conv_output_quantizer TODO use OP_TO_ANNOTATOR BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer weight_qspec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_weight_observer bias_qspec = QuantizationSpec dtype=torch float is_dynamic=False observer_or_fake_quant_ctr=observer PlaceholderObserver node model graph nodes node op == call_function node target == torch ops aten conv d default input_act = node args assert isinstance input_act Node weight = node args assert isinstance weight Node bias = node args assert isinstance bias Node node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec weight weight_qspec bias bias_qspec _annotated=True validate model torch fx GraphModule - None pass m = torch nn Conv d x = torch rand example_inputs = x m = _quantize m BackendAQuantizer example_inputs Ensure conv has no observer inserted output node_occurrence = two input conv ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default node_list = ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops aten conv d default checkGraphModuleNodes m expected_node_list=node_list expected_node_occurrence=node_occurrence test_max_pool d_quantizer TODO use OP_TO_ANNOTATOR BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer weight_qspec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_weight_observer bias_qspec = QuantizationSpec dtype=torch float is_dynamic=False observer_or_fake_quant_ctr=observer PlaceholderObserver node model graph nodes node op == call_function node target == torch ops aten conv d default input_act = node args assert isinstance input_act Node weight = node args assert isinstance weight Node bias = node args assert isinstance bias Node node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec weight weight_qspec bias bias_qspec _annotated=True node op == call_function node target == torch ops aten max_pool d default maxpool_node = node input_act = maxpool_node args assert isinstance input_act Node maxpool_node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec output_qspec=SharedQuantizationSpec input_act maxpool_node _annotated=True validate model torch fx GraphModule - None pass m = TestHelperModules ConvMaxPool d x = torch rand example_inputs = x m = _quantize m BackendAQuantizer example_inputs node_occurrence = two input conv one input maxpool one output maxpool ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default node_list = ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops aten conv d default ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops aten max_pool d default checkGraphModuleNodes m expected_node_list=node_list expected_node_occurrence=node_occurrence test_derived_qspec TODO use OP_TO_ANNOTATOR BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule node model graph nodes node op == call_function node target == torch ops aten conv d default input_act = node args assert isinstance input_act Node weight = node args assert isinstance weight Node bias = node args assert isinstance bias Node act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer weight_qspec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_weight_observer derive_qparams_fn obs_or_fqs list ObserverOrFakeQuantize - tuple Tensor Tensor assert len obs_or_fqs == f Expecting two obs fqs one activation one weight got len obs_or_fqs act_obs_or_fq = obs_or_fqs weight_obs_or_fq = obs_or_fqs act_scale act_zp = act_obs_or_fq calculate_qparams weight_scale weight_zp = weight_obs_or_fq calculate_qparams torch tensor act_scale weight_scale torch float torch tensor torch int bias_qspec = DerivedQuantizationSpec derived_from= input_act node weight node derive_qparams_fn=derive_qparams_fn dtype=torch int quant_min=- quant_max= - qscheme=torch per_tensor_symmetric node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec weight weight_qspec bias bias_qspec output_qspec=act_qspec _annotated=True validate model torch fx GraphModule - None pass m = TestHelperModules ConvWithBNRelu relu=False bn=False eval example_inputs = torch randn m = _quantize m BackendAQuantizer example_inputs node_occurrence = input weight bias output conv note quantize op weight bias const propagated ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default node_list = ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops aten conv d default ns call_function torch ops quantized_decomposed quantize_per_tensor default checkGraphModuleNodes m expected_node_list=node_list expected_node_occurrence=node_occurrence test_derived_qspec_per_channel BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule node model graph nodes node op == call_function node target == torch ops aten conv d default input_act = node args assert isinstance input_act Node weight = node args assert isinstance weight Node bias = node args assert isinstance bias Node act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer weight_qspec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_channel_affine is_dynamic=False ch_axis= observer_or_fake_quant_ctr=observer default_per_channel_weight_observer derive_qparams_fn obs_or_fqs list ObserverOrFakeQuantize - tuple Tensor Tensor assert len obs_or_fqs == f Expecting one weight obs fq got len obs_or_fqs weight_obs_or_fq = obs_or_fqs weight_scale weight_zp = weight_obs_or_fq calculate_qparams weight_scale torch zeros_like weight_scale bias_qspec = DerivedQuantizationSpec derived_from= weight node derive_qparams_fn=derive_qparams_fn dtype=torch int quant_min=- quant_max= - qscheme=torch per_channel_symmetric ch_axis= node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec weight weight_qspec bias bias_qspec output_qspec=act_qspec _annotated=True validate model torch fx GraphModule - None pass m = TestHelperModules ConvWithBNRelu relu=False bn=False eval example_inputs = torch randn m = _quantize m BackendAQuantizer example_inputs node_occurrence = input output conv ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default weight bias conv note quantize op weight bias const propagated ns call_function torch ops quantized_decomposed quantize_per_channel default ns call_function torch ops quantized_decomposed dequantize_per_channel default node_list = ns call_function torch ops quantized_decomposed dequantize_per_channel default ns call_function torch ops quantized_decomposed dequantize_per_channel default ns call_function torch ops aten conv d default ns call_function torch ops quantized_decomposed quantize_per_tensor default checkGraphModuleNodes m expected_node_list=node_list expected_node_occurrence=node_occurrence test_fixed_qparams_qspec_ptq _test_fixed_qparams_qspec is_qat=False TODO refactor move test_quantize_pt _qat py test_fixed_qparams_qspec_qat _test_fixed_qparams_qspec is_qat=True _test_fixed_qparams_qspec is_qat bool M torch nn Module forward x torch sigmoid x BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule node model graph nodes node op == call_function node target == torch ops aten sigmoid default input_act = node args assert isinstance input_act Node act_qspec = FixedQParamsQuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine scale= zero_point= node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec output_qspec=act_qspec _annotated=True validate model torch fx GraphModule - None pass m = M eval example_inputs = torch randn m = _quantize m BackendAQuantizer example_inputs is_qat fixed_scale = fixed_zero_point = n m graph nodes n op == call_function n target == torch ops quantized_decomposed quantize_per_tensor default scale_ = n args zero_point_ = n args n target == torch ops quantized_decomposed dequantize_per_tensor default scale_ = n args zero_point_ = n args assertEqual scale_ fixed_scale assertEqual zero_point_ fixed_zero_point assertEqual scale_ fixed_scale assertEqual zero_point_ fixed_zero_point node_occurrence = two input first conv one output first conv ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default node_list = ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops aten sigmoid default ns call_function torch ops quantized_decomposed quantize_per_tensor default checkGraphModuleNodes m expected_node_list=node_list expected_node_occurrence=node_occurrence test_fixed_qparams_qspec_observer_dedup BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule act_qspec = FixedQParamsQuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine scale= zero_point= node model graph nodes node op == call_function node target == torch ops aten sigmoid default input_act = node args assert isinstance input_act Node node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec output_qspec=act_qspec _annotated=True node op == call_function node target == torch ops aten add Tensor input_act = node args assert isinstance input_act Node input_act = node args assert isinstance input_act Node node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec input_act act_qspec output_qspec=act_qspec _annotated=True validate model torch fx GraphModule - None pass M torch nn Module forward x y torch sigmoid x + y example_inputs torch randn torch randn m = M eval example_inputs = m example_inputs m = _quantize m BackendAQuantizer example_inputs is_qat=False node_occurrence = two input first conv one output first conv ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default node_list = ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops aten sigmoid default ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops aten add Tensor ns call_function torch ops quantized_decomposed quantize_per_tensor default checkGraphModuleNodes m expected_node_list=node_list expected_node_occurrence=node_occurrence test_shared_qspec BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule node model graph nodes node op == call_function node target == torch ops aten conv d default input_act = node args assert isinstance input_act Node weight = node args assert isinstance weight Node bias = node args assert isinstance bias Node act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer weight_qspec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_weight_observer bias_qspec = QuantizationSpec dtype=torch float is_dynamic=False observer_or_fake_quant_ctr=observer PlaceholderObserver node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec weight weight_qspec bias bias_qspec output_qspec=act_qspec _annotated=True node target torch ops aten cat default cat_node = node input_nodes = cat_node args first_input_node = input_nodes input_qspec_map = act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer input_qspec_map first_input_node = act_qspec share_qparams_with_input_act _qspec = SharedQuantizationSpec first_input_node cat_node input_node input_nodes input_qspec_map input_node = share_qparams_with_input_act _qspec cat_node meta quantization_annotation = QuantizationAnnotation input_qspec_map=input_qspec_map output_qspec=share_qparams_with_input_act _qspec _annotated=True validate model torch fx GraphModule - None pass m = TestHelperModules Conv dWithCat eval example_inputs = torch randn torch randn program capture m = export m example_inputs strict=True module m = prepare_pt e m BackendAQuantizer make sure two observers input shared conv_output_obs = n m graph nodes n op == call_function n target == torch ops aten conv d default conv_output_obs append getattr m next iter n users target n op == call_function n target == torch ops aten cat default inputs = n args input = inputs input = inputs assert input op == call_module assert input op == call_module obs_ins = getattr m input target obs_ins = getattr m input target assert obs_ins == obs_ins assert len conv_output_obs == expecting two observer follows conv d ops checking output observers two convs shared well assert conv_output_obs == conv_output_obs m example_inputs m = convert_pt e m node_occurrence = two input first conv one output first conv ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default node_list = ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops aten cat default ns call_function torch ops quantized_decomposed quantize_per_tensor default checkGraphModuleNodes m expected_node_list=node_list expected_node_occurrence=node_occurrence _test_transitive_sharing_with_cat_helper quantizer m = TestHelperModules Conv dWithTwoCat eval example_inputs = torch randn torch randn torch randn torch randn program capture m = export m example_inputs strict=True module m = prepare_pt e m quantizer m example_inputs make sure two input observers output shared conv_output_obs = n m graph nodes n op == call_function n target == torch ops aten conv d default conv_output_obs append getattr m next iter n users target n op == call_function n target == torch ops aten cat default inputs = n args input = inputs input = inputs assert input op == call_module assert input op == call_module obs_ins = getattr m input target obs_ins = getattr m input target assert obs_ins == obs_ins output_obs = next iter n users assert output_obs op == call_module obs_ins = getattr m output_obs target assert obs_ins == obs_ins input observer does match output assert len conv_output_obs == expecting two observer follows conv d ops checking output observers two convs shared well assert conv_output_obs == conv_output_obs m example_inputs m = convert_pt e m node_occurrence = two input first conv one output first conv ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default node_list = ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops aten cat default ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops aten cat default ns call_function torch ops quantized_decomposed quantize_per_tensor default checkGraphModuleNodes m expected_node_list=node_list expected_node_occurrence=node_occurrence test_shared_qspec_transitivity This tests transitivity SharedQuantizationSpec A shared B B shared C then C should shared A well x - conv - cat ----- cat x - conv - x - add x both cat has shared input output because cat cat - cat same Tensor so there implicit sharing here all tensors connect cat cat same sharing group after transitive sharing TODO refactor common util BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule node model graph nodes node op == call_function node target == torch ops aten conv d default input_act = node args assert isinstance input_act Node weight = node args assert isinstance weight Node bias = node args assert isinstance bias Node act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer weight_qspec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_weight_observer bias_qspec = QuantizationSpec dtype=torch float is_dynamic=False observer_or_fake_quant_ctr=observer PlaceholderObserver node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec weight weight_qspec bias bias_qspec output_qspec=act_qspec _annotated=True node target torch ops aten cat default cat_node = node input_nodes = cat_node args first_input_node = input_nodes input_qspec_map = act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer input_qspec_map first_input_node = act_qspec share_qparams_with_input_act _qspec = SharedQuantizationSpec first_input_node cat_node input_node input_nodes input_qspec_map input_node = share_qparams_with_input_act _qspec cat_node meta quantization_annotation = QuantizationAnnotation input_qspec_map=input_qspec_map output_qspec=share_qparams_with_input_act _qspec _annotated=True validate model torch fx GraphModule - None pass _test_transitive_sharing_with_cat_helper BackendAQuantizer test_shared_qspec_transitivity_case_ This tests transitivity SharedQuantizationSpec A shared B B shared C then C should shared A well x - conv - cat ----- cat x - conv - x - add x both cat has shared input output because cat cat - cat same Tensor so there implicit sharing here all tensors connect cat cat same sharing group after transitive sharing difference one all edges nodes shared second input edge cat instead first input edge cat previous example TODO refactor common util BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule node model graph nodes node op == call_function node target == torch ops aten conv d default input_act = node args assert isinstance input_act Node weight = node args assert isinstance weight Node bias = node args assert isinstance bias Node act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer weight_qspec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_weight_observer bias_qspec = QuantizationSpec dtype=torch float is_dynamic=False observer_or_fake_quant_ctr=observer PlaceholderObserver node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec weight weight_qspec bias bias_qspec output_qspec=act_qspec _annotated=True node target torch ops aten cat default cat_node = node input_nodes = cat_node args first_input_node = input_nodes second_input_node = input_nodes input_qspec_map = act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer input_qspec_map second_input_node = act_qspec share_qparams_with_input_act _qspec = SharedQuantizationSpec second_input_node cat_node input_qspec_map first_input_node = share_qparams_with_input_act _qspec cat_node meta quantization_annotation = QuantizationAnnotation input_qspec_map=input_qspec_map output_qspec=share_qparams_with_input_act _qspec _annotated=True validate model torch fx GraphModule - None pass _test_transitive_sharing_with_cat_helper BackendAQuantizer test_allow_implicit_sharing This tests allow_transitive_sharing flag QuantizationAnnotation node configured allow_implicit_sharing=False we will have implicit sharing node node consumer even they refer same Tensor x - add ----- add x - x - add x - all add has shared input output second input using shared quantization spec pointing first input we set allow_implicit_sharing False all add nodes so input output add add add will each belong one sharing group so we ll have x - obs - add - obs - obs -- add - obs x - obs - x - obs - add - obs - obs x - obs - TODO refactor common util BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule node model graph nodes node target torch ops aten add Tensor add_node = node first_input_node = add_node args second_input_node = add_node args input_qspec_map = act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer input_qspec_map second_input_node = act_qspec share_qparams_with_input_act _qspec = SharedQuantizationSpec second_input_node add_node input_qspec_map first_input_node = share_qparams_with_input_act _qspec add_node meta quantization_annotation = QuantizationAnnotation input_qspec_map=input_qspec_map output_qspec=share_qparams_with_input_act _qspec allow_implicit_sharing=False _annotated=True validate model torch fx GraphModule - None pass m = TestHelperModules ThreeAdd eval example_inputs = torch randn torch randn torch randn torch randn program capture m = export m example_inputs strict=True module quantizer = BackendAQuantizer m = prepare_pt e m quantizer m example_inputs observers = n m graph nodes n target == torch ops aten add Tensor input_obs = getattr m n args target input_obs = getattr m n args target output_obs = getattr m next iter n users target assertIs input_obs input_obs assertIs input_obs output_obs observers append input_obs assert len observers == assertIsNot observers observers assertIsNot observers observers assertIsNot observers observers skipIfHpu parametrize dtype torch float torch bfloat parametrize quant_dtype torch int torch float _e m torch float _e m fn test_quantization_dtype dtype quant_dtype DtypeActQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule info_fun = torch iinfo quant_dtype == torch int torch finfo activate_qspec = QuantizationSpec dtype=quant_dtype quant_min=int info_fun quant_dtype min quant_max=int info_fun quant_dtype max qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer int _qspec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_tensor_symmetric is_dynamic=False observer_or_fake_quant_ctr=observer default_weight_observer quantization_config = QuantizationConfig input_activation=activate_qspec weight=int _qspec bias=None output_activation=activate_qspec OP_TO_ANNOTATOR conv model quantization_config validate model torch fx GraphModule - None pass M torch nn Module __init__ dtype super __init__ conv = torch nn Conv d dtype=dtype forward x conv x quantizer = DtypeActQuantizer node_occurrence = one input first conv one output first conv torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default node_list = torch ops quantized_decomposed dequantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default example_inputs = torch randn dtype=dtype m = _test_quantizer M dtype eval example_inputs quantizer node_occurrence node_list verify_quant_dequant_iotypes m node m graph nodes node op == call_function node target __name__ == dequantize_per_tensor default Check dequantize node dequant_node = node dequant_in_dtype = dequant_node args dequant_out_dtype = torch float out_dtype dequant_node kwargs dequant_out_dtype = dequant_node kwargs out_dtype Check preceding quantize node Depending fold_quantize flag quantize node may absent quant_node = node args quant_node op == call_function quant_node target __name__ == quantize_per_tensor default quant_in_dtype = torch float val quant_node args meta quant_in_dtype = quant_node args meta val dtype quant_out_dtype = quant_node args assert quant_in_dtype == dequant_out_dtype quant_out_dtype == dequant_in_dtype quant dequant io dtype check failed verify_quant_dequant_iotypes m test_input_edge_sanity_check M torch nn Module forward x x + BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule node model graph nodes node op == call_function node target == torch ops aten add Tensor input_act = node args constant so valid annotation input_act = node args act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec supposed error out input_act act_qspec output_qspec=act_qspec _annotated=True validate model torch fx GraphModule - None pass m = M eval example_inputs = torch randn m = export m example_inputs strict=True module assertRaises Exception m = prepare_pt e m BackendAQuantizer test_fold_quantize Test make sure quantized model gets quantized weight quantize_per_tensor op folded m = _get_pt e_quantized_linear node_occurrence = quantize op weight node folded ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default checkGraphModuleNodes m expected_node_occurrence=node_occurrence test_fold_quantize_per_channel Test make sure quantized model gets quantized weight quantize_per_channel op folded m = _get_pt e_quantized_linear is_per_channel=True node_occurrence = quantize op weight node folded ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_channel default ns call_function torch ops quantized_decomposed dequantize_per_tensor default checkGraphModuleNodes m expected_node_occurrence=node_occurrence test_dont_fold_other_constant Make sure constant propagation does apply things unrelated quantization M torch nn Module __init__ - None super __init__ linear = torch nn Linear dont_fold_me = torch nn Parameter torch randn forward x t = dont_fold_me t linear x + t quantizer = XNNPACKQuantizer operator_config = get_symmetric_quantization_config is_per_channel=False only quantize linear so add quantized constant Tensor should folded quantizer set_module_type torch nn Linear operator_config example_inputs = torch randn m = M eval m = _quantize m quantizer example_inputs node_occurrence = quantize op weight node folded ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default transpose op folded ns call_function torch ops aten t default checkGraphModuleNodes m expected_node_occurrence=node_occurrence test_fold_all_ops_before_quantize Test folding all ops s before quantized operator Before get_attr weight - transpose - quantize - dequantize After get_attr folded_weight - dequantize M torch nn Module __init__ - None super __init__ weight = torch randn forward x t = weight t torch nn functional linear x t quantizer = XNNPACKQuantizer operator_config = get_symmetric_quantization_config is_per_channel=False quantizer set_global operator_config example_inputs = torch randn m = M eval m = _quantize m quantizer example_inputs node_occurrence = quantize op weight node folded ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default checkGraphModuleNodes m expected_node_occurrence=node_occurrence test_constant_prop_preserve_metadata Test make sure get_attr node const propagated weight Tensor gets correct metadata original get_attr node weight M torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x quantizer = XNNPACKQuantizer operator_config = get_symmetric_quantization_config quantizer set_global operator_config example_inputs = torch randn m = M eval m = export m example_inputs strict=True module weight_meta = None n m graph nodes n op == get_attr next iter n users target == torch ops aten linear default weight_meta = n meta break assert weight_meta None Expect find metadata weight node m = prepare_pt e m quantizer m example_inputs m = convert_pt e m n m graph nodes n op == get_attr frozen_param n target key n meta assertEqual n meta key weight_meta key test_save_load Test save load quantized model m = _get_pt e_quantized_linear example_inputs = torch randn ref_res = m example_inputs TemporaryFileName fname serialization quantized_ep = torch export export m example_inputs strict=True torch export save quantized_ep fname deserialization loaded_ep = torch export load fname loaded_quantized_model = loaded_ep module res = loaded_quantized_model example_inputs assertEqual ref_res res test_composable_quantizer_throw BadQuantizer Quantizer annotate gm torch fx GraphModule - torch fx GraphModule n gm graph nodes n meta quantization_annotation = None validate model torch fx GraphModule - None pass quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config bad_quantizer = BadQuantizer composable_quantizer = ComposableQuantizer quantizer bad_quantizer m_eager = TestHelperModules ConvLinearWPermute eval example_inputs = torch randn assertRaises RuntimeError lambda _test_quantizer m_eager example_inputs composable_quantizer test_transform_for_annotation TestQuantizer Quantizer transform_for_annotation model torch fx GraphModule - torch fx GraphModule Make copy graph ensure we using value function graph = torch fx Graph graph graph_copy model graph n graph nodes n target == torch ops aten add Tensor n target = torch ops aten mul Tensor model = torch fx GraphModule model graph model annotate model torch fx GraphModule - torch fx GraphModule model validate model torch fx GraphModule - None pass M torch nn Module forward x x + m = M eval quantizer = TestQuantizer example_inputs = torch randn m = export m example_inputs strict=True module m = prepare_pt e m quantizer m example_inputs node_occurrence = ns call_function torch ops aten add Tensor ns call_function torch ops aten mul Tensor checkGraphModuleNodes m expected_node_occurrence=node_occurrence test_composable_quantizer_transform_for_annotation TestQuantizer Quantizer transform_for_annotation model torch fx GraphModule - torch fx GraphModule n model graph nodes n target == torch ops aten add Tensor n target = torch ops aten mul Tensor model annotate model torch fx GraphModule - torch fx GraphModule model validate model torch fx GraphModule - None pass TestQuantizer Quantizer transform_for_annotation model torch fx GraphModule - torch fx GraphModule n model graph nodes n target == torch ops aten sub Tensor n target = torch ops aten div Tensor model annotate model torch fx GraphModule - torch fx GraphModule model validate model torch fx GraphModule - None pass M torch nn Module forward x y z x + y - z m = M eval quantizer = ComposableQuantizer TestQuantizer TestQuantizer example_inputs = torch randn torch randn torch randn m = export m example_inputs strict=True module m = prepare_pt e m quantizer m example_inputs node_occurrence = ns call_function torch ops aten add Tensor ns call_function torch ops aten sub Tensor ns call_function torch ops aten mul Tensor ns call_function torch ops aten div Tensor checkGraphModuleNodes m expected_node_occurrence=node_occurrence test_embedding_quantizer m_eager = TestHelperModules EmbeddingModule eval indices = torch tensor example_inputs = indices quantizer = EmbeddingQuantizer node_occurrence = note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed dequantize_per_channel default torch ops aten embedding default Compare against short term workflow cannot compare against fx quant because numerical differences coming quantize dequantize ops qconfig = default_per_channel_symmetric_qnnpack_qconfig qconfig_mapping = QConfigMapping set_global qconfig qconfig_mapping = qconfig_mapping set_object_type torch nn Embedding float_qparams_weight_only_qconfig _test_quantizer m_eager example_inputs quantizer node_occurrence node_list True qconfig_mapping test_composable_quantizer_linear_conv dynamic_quantizer = XNNPACKQuantizer quantization_config_dynamic = get_symmetric_quantization_config is_per_channel=False is_dynamic=True dynamic_quantizer set_global quantization_config_dynamic static_quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True static_quantizer set_global quantization_config Note dynamic quantization must applied first here because static quantizer also quantizes linear static qspec we apply static_quantizer first then dynamic_quantizer cannot applied composable_quantizer = ComposableQuantizer dynamic_quantizer static_quantizer m_eager = TestHelperModules ConvLinearWPermute eval node_occurrence = torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_tensor tensor note quantize op weights const propagated torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default act_affine_quant_obs = observer PlaceholderObserver with_args dtype=torch qint qscheme=torch per_tensor_affine quant_min=- quant_max= eps= - is_dynamic=True dynamic_qconfig = QConfig activation=act_affine_quant_obs weight=weight_observer_range_neg_ _to_ Test d inputs example_inputs = torch randn qconfig = default_per_channel_symmetric_qnnpack_qconfig qconfig_mapping = QConfigMapping set_global qconfig qconfig_mapping set_object_type torch nn Linear dynamic_qconfig Had turn off check against fx because fx quant workflow does seem propagate observers permute node model Surprisingly does propagate EmbeddingConvLinearModule TODO Figure out right behavior propagation _test_quantizer m_eager example_inputs composable_quantizer node_occurrence False qconfig_mapping test_embedding_conv_linear_quantization m_eager = TestHelperModules EmbeddingConvLinearModule eval indices = torch tensor indices = torch unsqueeze indices example_inputs = indices embedding_quantizer = EmbeddingQuantizer dynamic_quantizer = XNNPACKQuantizer quantization_config_dynamic = get_symmetric_quantization_config is_per_channel=True is_dynamic=True dynamic_quantizer set_global quantization_config_dynamic static_quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True static_quantizer set_global quantization_config composed_quantizer = ComposableQuantizer embedding_quantizer dynamic_quantizer static_quantizer act_affine_quant_obs = observer PlaceholderObserver with_args dtype=torch qint qscheme=torch per_tensor_affine quant_min=- quant_max= eps= - is_dynamic=True dynamic_qconfig = QConfig activation=act_affine_quant_obs weight=per_channel_weight_observer_range_neg_ _to_ qconfig = default_per_channel_symmetric_qnnpack_qconfig qconfig_mapping = QConfigMapping set_global qconfig qconfig_mapping set_object_type torch nn Linear dynamic_qconfig qconfig_mapping = qconfig_mapping set_object_type torch nn Embedding float_qparams_weight_only_qconfig node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_tensor tensor note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default _test_quantizer m_eager example_inputs composed_quantizer node_occurrence True qconfig_mapping _get_node m torch fx GraphModule target torch _ops OpOverload Return first node matching specified target throwing exception no such batch norm node found n m graph nodes n target == target n raise ValueError Did find node target target _test_move_exported_model_dropout inplace bool Test switching dropout behavior between train eval modes using ` move_exported_model_to_eval ` ` move_exported_model_to_train ` APIs M torch nn Module __init__ - None super __init__ dropout = torch nn Dropout inplace=inplace forward x dropout x example_inputs = torch randn m = M train m = export m example_inputs strict=True module inplace target = torch ops aten dropout_ default target = torch ops aten dropout default Assert dropout op exists train mode dropout_node = _get_node m target assertTrue dropout_node None assertTrue dropout_node args Move eval torch ao quantization move_exported_model_to_eval m Assert dropout op now eval mode dropout_node = _get_node m target assertTrue dropout_node None assertTrue dropout_node args Move back train torch ao quantization move_exported_model_to_train m Assert dropout op now train mode again dropout_node = _get_node m target assertTrue dropout_node None assertTrue dropout_node args test_move_exported_model_dropout _test_move_exported_model_dropout inplace=False test_move_exported_model_dropout_inplace _test_move_exported_model_dropout inplace=True _get_bn_train_eval_ops torch ops aten batch_norm default torch ops aten batch_norm default parametrize device cpu + cuda TEST_CUDA + hpu TEST_HPU test_move_exported_model_bn device Test switching batch_norm behavior between train eval modes using ` move_exported_model_to_eval ` ` move_exported_model_to_train ` APIs M torch nn Module __init__ - None super __init__ bn = torch nn BatchNorm d forward x bn x TEST_CUDA TEST_HPU m = M train device example_inputs = torch randn device=device m = M train example_inputs = torch randn bn_train_op bn_eval_op = _get_bn_train_eval_ops m = export m example_inputs strict=True module Assert batch norm op exists train mode bn_node = _get_node m bn_train_op assertTrue bn_node None assertTrue bn_node args Move eval torch ao quantization move_exported_model_to_eval m Assert batch norm op now eval mode bn_node = _get_node m bn_eval_op assertTrue bn_node None Move train torch ao quantization move_exported_model_to_train m Assert batch norm op now train mode again bn_node = _get_node m bn_train_op assertTrue bn_node None assertTrue bn_node args test_disallow_eval_train m = TestHelperModules ConvWithBNRelu relu=True example_inputs = torch rand Before export OK m eval m train After export OK m = export m example_inputs strict=True module assertRaises NotImplementedError m eval assertRaises NotImplementedError m train After prepare still OK quantizer = XNNPACKQuantizer m = prepare_qat_pt e m quantizer assertRaises NotImplementedError m eval assertRaises NotImplementedError m train After convert still OK m = convert_pt e m assertRaises NotImplementedError m eval assertRaises NotImplementedError m train skipIfHpu test_allow_exported_model_train_eval M torch nn Module __init__ - None super __init__ bn = torch nn BatchNorm d dropout = torch nn Dropout forward x x = bn x x = dropout x x TEST_CUDA m = M train cuda example_inputs = torch randn cuda m = M train example_inputs = torch randn bn_train_op bn_eval_op = _get_bn_train_eval_ops m = export m example_inputs strict=True module _assert_ops_are_correct m torch fx GraphModule train bool targets = n target n m graph nodes bn_op = bn_train_op train bn_eval_op bn_node = _get_node m bn_op assertTrue bn_node None TEST_CUDA assertEqual bn_node args train dropout_node = _get_node m torch ops aten dropout default assertEqual dropout_node args train Before wrapping OK assertRaises NotImplementedError m eval assertRaises NotImplementedError m train After wrapping does error swaps ops accordingly torch ao quantization allow_exported_model_train_eval m m eval _assert_ops_are_correct m train=False m train _assert_ops_are_correct m train=True After prepare before wrapping OK quantizer = XNNPACKQuantizer m = prepare_qat_pt e m quantizer assertRaises NotImplementedError m eval assertRaises NotImplementedError m train After prepare after wrapping does error swaps ops accordingly torch ao quantization allow_exported_model_train_eval m m eval _assert_ops_are_correct m train=False m train _assert_ops_are_correct m train=True After convert before wrapping OK m = convert_pt e m fold_quantize=True assertRaises NotImplementedError m eval assertRaises NotImplementedError m train After convert after wrapping does error swaps ops accordingly torch ao quantization allow_exported_model_train_eval m m eval _assert_ops_are_correct m train=False m train _assert_ops_are_correct m train=True test_allow_exported_model_train_eval_idempotent M torch nn Module __init__ - None super __init__ bn = torch nn BatchNorm d forward x x = bn x x m = M train example_inputs = torch randn m = export m example_inputs strict=True module torch ao quantization allow_exported_model_train_eval m Mock m recompile count how many times s been called m _recompile_count = _fake_recompile m _recompile_count += m recompile = _fake_recompile First train after export should always recompile m train assertNotEqual m _recompile_count count = m _recompile_count Train - train should recompile m train assertEqual m _recompile_count count Train - eval should recompile m eval assertNotEqual m _recompile_count count count = m _recompile_count Eval - eval should recompile m eval assertEqual m _recompile_count count test_model_is_exported m = TestHelperModules ConvWithBNRelu relu=True example_inputs = torch rand exported_gm = export m example_inputs strict=True module fx_traced_gm = torch fx symbolic_trace m example_inputs assertTrue torch ao quantization pt e export_utils model_is_exported exported_gm assertFalse torch ao quantization pt e export_utils model_is_exported fx_traced_gm assertFalse torch ao quantization pt e export_utils model_is_exported m test_reentrant Test we can safely call quantization apis multiple times m = TestHelperModules ConvBnReLU dAndLinearReLU example_inputs = torch randn quantizer = XNNPACKQuantizer set_global get_symmetric_quantization_config is_per_channel=True is_qat=True m conv_bn_relu = export m conv_bn_relu example_inputs strict=True module m conv_bn_relu = prepare_qat_pt e m conv_bn_relu quantizer m example_inputs m conv_bn_relu = convert_pt e m conv_bn_relu quantizer = XNNPACKQuantizer set_module_type torch nn Linear get_symmetric_quantization_config is_per_channel=False m = export m example_inputs strict=True module m = prepare_pt e m quantizer m = convert_pt e m node_occurrence = ns call_function torch ops quantized_decomposed quantize_per_tensor default one weight ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_channel default node_list = ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops aten conv d default ns call_function torch ops aten relu default ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default ns call_function torch ops aten linear default ns call_function torch ops quantized_decomposed quantize_per_tensor default checkGraphModuleNodes m expected_node_occurrence=node_occurrence expected_node_list=node_list test_groupwise_per_channel_quant m = TestHelperModules GroupwiseConv d quantizer = XNNPACKQuantizer operator_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global operator_config example_inputs = m example_inputs m = _quantize m quantizer example_inputs make sure runs m example_inputs test_observer_callback torch library custom_op custom_op test_int quantize_per_tensor_int mutates_args= quantize_per_tensor_int input torch Tensor scale float zero_point int - torch Tensor inv_scale = scale torch clamp torch round input inv_scale + zero_point torch uint view torch bits custom_op test_int dequantize_per_tensor_int mutates_args= dequantize_per_tensor_int input torch Tensor scale float zero_point int - torch Tensor input view torch uint torch float - zero_point scale torch ao quantization observer ObserverBase Int Observer ObserverBase __init__ args kwargs just faking dtype here super __init__ dtype=torch int forward x x calculate_qparams kwargs pass convert model torch fx GraphModule observer_node Node model graph inserting_before observer_node q_node = model graph call_function torch ops test_int quantize_per_tensor_int observer_node args dq_node = model graph call_function torch ops test_int dequantize_per_tensor_int q_node observer_node replace_all_uses_with dq_node model graph erase_node observer_node BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule node model graph nodes node op == call_function node target == torch ops aten add Tensor input_act = node args assert isinstance input_act Node input_act = node args assert isinstance input_act Node act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=Int Observer node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec input_act act_qspec output_qspec=act_qspec _annotated=True validate model torch fx GraphModule - None pass M torch nn Module forward x x x + x example_inputs = torch randn torch randn node_occurrence = two input first conv one output first conv torch ops test_int quantize_per_tensor_int torch ops test_int dequantize_per_tensor_int node_list = torch ops test_int dequantize_per_tensor_int torch ops test_int dequantize_per_tensor_int torch ops aten add Tensor torch ops test_int quantize_per_tensor_int _test_quantizer M eval example_inputs BackendAQuantizer node_occurrence node_list test_speed time dynamic_quantize_pt e model example_inputs torch _dynamo reset model = export model example_inputs strict=True module Per channel quantization weight Dynamic quantization activation Please read detail https fburl com code zds q embedding_quantizer = EmbeddingQuantizer dynamic_quantizer = XNNPACKQuantizer operator_config_dynamic = get_symmetric_quantization_config is_per_channel=True is_dynamic=True dynamic_quantizer set_global operator_config_dynamic composed_quantizer = ComposableQuantizer embedding_quantizer dynamic_quantizer prev = time time model = prepare_qat_pt e model composed_quantizer cur = time time print prepare time cur - prev Without Calibration scale zero value will have initialized value Per channel quantization needs proper scale zero shape value work properly So we need run calibration before converting quantized model model example_inputs prev = time time model = convert_pt e model cur = time time uncomment see time print convert time cur - prev model M torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x m = M eval example_inputs = torch randn _ = dynamic_quantize_pt e m example_inputs test_conv_transpose_bn_relu BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule int _qspec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_tensor_symmetric is_dynamic=False observer_or_fake_quant_ctr=observer default_weight_observer quantization_config = QuantizationConfig input_activation=int _qspec weight=int _qspec bias=None output_activation=int _qspec conv_transpose + bn fused automatically PTQ configurable so we just need annotate conv_transpose + relu conv_transpose + bn + relu pattern OP_TO_ANNOTATOR conv_transpose_relu model quantization_config validate model torch fx GraphModule - None pass example_inputs = torch randn node_occurrence = two input first conv one output first conv torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default node_list = torch ops quantized_decomposed dequantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv_transpose d input torch ops aten relu default torch ops quantized_decomposed quantize_per_tensor default _test_quantizer TestHelperModules ConvTWithBNRelu relu=True bn=True example_inputs BackendAQuantizer node_occurrence node_list test_conv_padding_bn_relu BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer weight_qspec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_weight_observer bias_qspec = QuantizationSpec dtype=torch float is_dynamic=False observer_or_fake_quant_ctr=observer PlaceholderObserver n model graph nodes n op = call_function n target = torch ops aten relu default continue relu_node = n n = n args Check any conv operations conv_ops = torch ops aten conv d padding torch ops aten conv d padding torch ops aten conv d padding n op = call_function n target conv_ops continue conv_node = n input_act = conv_node args weight = conv_node args bias = conv_node args conv_node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec weight weight_qspec bias bias_qspec _annotated=True relu_node meta quantization_annotation = QuantizationAnnotation output_qspec=act_qspec _annotated=True validate model torch fx GraphModule - None pass Test cases Conv d Conv d Conv d test_cases = dim example_input torch randn conv_op torch ops aten conv d padding dim example_input torch randn conv_op torch ops aten conv d padding dim example_input torch randn conv_op torch ops aten conv d padding test_case test_cases subTest dim=test_case dim model = TestHelperModules ConvWithBNRelu relu=True dim=test_case dim bn=True bias=True padding= same This will trigger padding variants eval node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default node_list = torch ops quantized_decomposed dequantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default test_case conv_op torch ops aten relu default torch ops quantized_decomposed quantize_per_tensor default _test_quantizer model test_case example_input BackendAQuantizer node_occurrence node_list test_multi_users_without_output_observer Test case which node used multiple users had its output observer removed M torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x x = conv x x x + example_inputs = torch randn m = M m = export m example_inputs strict=True module quantizer = XNNPACKQuantizer set_global get_symmetric_quantization_config m = prepare_pt e m quantizer m example_inputs Remove output observer observer_to_remove = None n m graph nodes n op == output observer_to_remove = n args assert observer_to_remove op == call_module assert observer_to_remove target startswith activation_post_process_ break assert observer_to_remove None observer_to_remove replace_all_uses_with observer_to_remove args m graph erase_node observer_to_remove m recompile Convert should succeed m = convert_pt e m m example_inputs test_prepare_obs_or_fq_callback Model torch nn Module forward x x = torch nn functional max_pool d x x = torch nn functional pixel_shuffle x x permute BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=observer default_observer node model graph nodes node op == call_function node target torch ops aten max_pool d default torch ops aten permute default torch ops aten pixel_shuffle default node meta quantization_annotation = QuantizationAnnotation input_qspec_map= node args act_qspec output_qspec=SharedQuantizationSpec node args node _annotated=True validate model torch fx GraphModule - None pass prepare_obs_or_fq_callback model torch fx GraphModule edge_or_node_to_obs_or_fq dict EdgeOrNode ObserverOrFakeQuantize - None hard code output quant updating entire sharing group output_node = next n n model graph nodes n op == output output_value = output_node args old_observer = edge_or_node_to_obs_or_fq output_value sharing_group = k k v edge_or_node_to_obs_or_fq items v old_observer new_observer = observer FixedQParamsObserver scale= zero_point= dtype=torch uint quant_min= quant_max= qscheme=torch per_tensor_affine x sharing_group edge_or_node_to_obs_or_fq x = new_observer example_inputs = torch rand gm = export Model eval example_inputs strict=True module gm = prepare_pt e gm BackendAQuantizer gm = convert_pt e gm n gm graph nodes n op == call_function n target torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default Entire graph share same qspec which overridden FixedQParamsObserver assertEqual n args assertEqual n args test_preserve_nn_module_stack Test we can preserve nn_module_stack replaced pattern s nodes m = TestHelperModules ConvBnReLU dAndLinearReLU example_inputs = torch randn quantizer = XNNPACKQuantizer set_global get_symmetric_quantization_config is_per_channel=True is_qat=True check_nn_module node assertTrue nn_module_stack node meta assertTrue ConvWithBNRelu node meta nn_module_stack L__self__ m conv_bn_relu = export m conv_bn_relu example_inputs strict=True module node m conv_bn_relu graph nodes node op placeholder output get_attr check_nn_module node m conv_bn_relu = prepare_qat_pt e m conv_bn_relu quantizer node m conv_bn_relu graph nodes node name == mul check_nn_module node skipIfNoQNNPACK TestQuantizePT EAffineQuantization PT EQuantizationTestCase test_channel_group_quantization torch ao quantization observer MappingType PerGroup PerToken torch ao quantization pt e _affine_quantization AffineQuantizedMinMaxObserver BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule node model graph nodes node op == call_function node target == torch ops aten linear default input_act = node args assert isinstance input_act Node weight = node args assert isinstance weight Node act_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=None is_dynamic=False observer_or_fake_quant_ctr=AffineQuantizedMinMaxObserver with_args TODO maybe align arg name here target_dtype=torch uint mapping_type=MappingType SYMMETRIC granularity=PerToken weight_qspec = QuantizationSpec dtype=torch uint quant_min= quant_max= qscheme=None is_dynamic=False observer_or_fake_quant_ctr=AffineQuantizedMinMaxObserver with_args target_dtype=torch uint mapping_type=MappingType SYMMETRIC granularity=PerGroup group_size= node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec weight weight_qspec _annotated=True validate model torch fx GraphModule - None pass M torch nn Module __init__ super __init__ linear = torch nn Linear forward x linear x node_occurrence = torch ops pt e_quant quantize_affine torch ops pt e_quant dequantize_affine node_list = torch ops pt e_quant quantize_affine torch ops pt e_quant dequantize_affine example_inputs = torch randn _test_quantizer M eval example_inputs BackendAQuantizer node_occurrence node_list is_debug_mode=True test_dynamic_affine_act_per_channel_weights operator torch ao quantization observer MappingType PerChannelMinMaxObserver PerToken torch ao quantization pt e _affine_quantization AffineQuantizedMovingAverageMinMaxObserver BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule node model graph nodes node op == call_function node target == torch ops aten linear default input_act = node args assert isinstance input_act Node weight = node args assert isinstance weight Node activation_dtype = torch int act_qspec = QuantizationSpec dtype=activation_dtype quant_min=- quant_max= qscheme=None is_dynamic=True observer_or_fake_quant_ctr=AffineQuantizedMovingAverageMinMaxObserver with_args TODO maybe align arg name here target_dtype=activation_dtype mapping_type=MappingType SYMMETRIC granularity=PerToken averaging_constant= weight_qspec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_channel_symmetric ch_axis= is_dynamic=False observer_or_fake_quant_ctr=PerChannelMinMaxObserver with_args node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec weight weight_qspec _annotated=True validate model torch fx GraphModule - None pass M torch nn Module __init__ super __init__ linear = torch nn Linear forward x linear x node_occurrence = torch ops pt e_quant choose_qparams_affine operator getitem torch ops pt e_quant quantize_affine torch ops pt e_quant dequantize_affine torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed dequantize_per_channel default torch ops pt e_quant choose_qparams_affine operator getitem torch ops pt e_quant quantize_affine torch ops pt e_quant dequantize_affine example_inputs = torch randn _test_quantizer M eval example_inputs BackendAQuantizer node_occurrence node_list is_debug_mode=True test_dynamic_per_tok_act_per_group_weights operator torch ao quantization observer MappingType PerGroup PerToken torch ao quantization pt e _affine_quantization AffineQuantizedMinMaxObserver AffineQuantizedPlaceholderObserver BackendAQuantizer Quantizer annotate model torch fx GraphModule - torch fx GraphModule node model graph nodes node op == call_function node target == torch ops aten linear default input_act = node args assert isinstance input_act Node weight = node args assert isinstance weight Node activation_dtype = torch int act_qspec = QuantizationSpec dtype=activation_dtype quant_min=- quant_max= qscheme=None is_dynamic=True observer_or_fake_quant_ctr=AffineQuantizedPlaceholderObserver with_args TODO maybe align arg name here target_dtype=activation_dtype mapping_type=MappingType SYMMETRIC granularity=PerToken weight_qspec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_channel_symmetric ch_axis= is_dynamic=False observer_or_fake_quant_ctr=AffineQuantizedMinMaxObserver with_args target_dtype=torch int mapping_type=MappingType SYMMETRIC granularity=PerGroup group_size= node meta quantization_annotation = QuantizationAnnotation input_qspec_map= input_act act_qspec weight weight_qspec _annotated=True validate model torch fx GraphModule - None pass M torch nn Module __init__ super __init__ linear = torch nn Linear forward x linear x node_occurrence = torch ops pt e_quant choose_qparams_affine operator getitem torch ops pt e_quant quantize_affine torch ops pt e_quant dequantize_affine node_list = torch ops pt e_quant dequantize_affine torch ops pt e_quant choose_qparams_affine operator getitem torch ops pt e_quant quantize_affine torch ops pt e_quant dequantize_affine example_inputs = torch randn _test_quantizer M eval example_inputs BackendAQuantizer node_occurrence node_list is_debug_mode=True instantiate_parametrized_tests TestQuantizePT E