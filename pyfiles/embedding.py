mypy allow-untyped-defs torch torch distributed dist torch distributed _shard sharded_tensor ShardedTensor torch distributed _shard sharding_spec ChunkShardingSpec torch distributed _shard sharding_spec api custom_sharding_spec_op torch distributed nn functional all_gather reduce_scatter _common _all_gather_base_input _handle_col_wise_sharding_base _handle_max_norm_col_wise _handle_row_wise_mask custom_sharding_spec_op ChunkShardingSpec torch nn functional embedding sharded_embedding types args kwargs pg Handles ` ` __torch_function__ ` ` dispatch ` ` torch nn functional embedding ` ` This method computes sharded embedding lookup has following limitations Supports only sharding ` ` weight ` ` Supports only ` ` ChunkShardingSpec ` ` Supports only single local shard per rank Supports all specs except scale_grad_by_freq sparse etc Based dimension weight sharded there two algorithms ROWWISE SHARDING ================ For row-wise sharding weight sharded dimension The overall algorithm can best explained example Let s assume dims input x W x W sharded across GPUs creating shard x shard x The algorithm follows First input all gathered all ranks since SPMD input actually sharded across all ranks The inputs then become x tensor each rank For example given input tensor rank Then every rank we will have tensor If input itself already replicated no all-gather will done Next we mask ID which stored rank For example rank we store ID We only keep ID inside set numbers The rest them will masked extra row The masked matrix will used embedding look up like tensor The reason having extra row aka number example because when max_norm specified only weight which has looked will re-normed so mask IDs whose embeddings stored current rank will extra row will ensure max_norm still works expected If max_norm specified extra row guarantees mask ID will affect behavior weigh re-norm COLWISE SHARDING ================ For col-wise sharding weight sharded dimension The overall algorithm can best explained example Let s assume dims input x W x W sharded across GPUs creating shards x shard x The algorithm follows First input broadcasted all ranks since SPMD we actually do all_gather all inputs resulting x inputs each rank Next we perform local embedding lookup operation apply each input x local shard x x last This results x x x x last matrices each rank We transpose dim dim Next we concat these matrices perform all all share appropriate x x x x matrices each rank Now each rank receives x x matrix which basically size result we need If placements order any appropriate rearrangement columns done x x matrix finally we transpose dim dim again If max_norm specified we manually sum up norm renorm Because renorm must place we need override local_shard mimic behavior Validate input params _validate_embedding_param args kwargs input = args weight = args max_norm = kwargs get max_norm norm_type = kwargs get norm_type padding_idx = kwargs get padding_idx local_shard = weight local_tensor contiguous sharding_dim = weight _sharding_spec dim world_size = dist get_world_size pg rank = dist get_rank pg sharding_dim == output local_shard = _handle_col_wise_sharding input world_size weight local_shard max_norm norm_type padding_idx pg weight local_shards tensor = local_shard output sharding_dim == _handle_row_wise_sharding input world_size weight local_shard max_norm norm_type padding_idx rank pg raise RuntimeError f nn Embedding weight sharded dim sharding_dim supported _validate_embedding_param args kwargs Validate input params sharded embedding op Args input list ID used lookup weight sharded weight tensor kwargs same normal Embedding Return None input = args weight = args max_norm = kwargs get max_norm scale_grad_by_freq = kwargs get scale_grad_by_freq sparse = kwargs get sparse Validate types isinstance input torch Tensor raise TypeError input need torch Tensor isinstance weight ShardedTensor raise TypeError weight needs ShardedTensor weight_size = weight size len weight_size = raise ValueError Weight needs have exactly dims int torch min input item raise ValueError Index out range Input d d int torch min input item weight_size int torch max input item = weight_size raise ValueError Index out range Input d d int torch max input item weight_size scale_grad_by_freq raise RuntimeError nn Embedding weight sharded flag scale_grad_by_freq supported sparse raise RuntimeError nn Embedding weight sharded flag sparse supported max_norm max_norm = raise ValueError max_norm must larger than zero isinstance weight _sharding_spec ChunkShardingSpec raise ValueError Only ChunkShardingSpec supported ShardedTensor ops len weight local_shards = raise ValueError Only one local shard supported _handle_col_wise_sharding input world_size weight local_shard max_norm norm_type padding_idx pg Entry-point function handle logic col-wise sharding weight embedding Detailed explanations logic can found comment sharded_embedding Args input list ID used lookup aggregation world_size number ranks weight sharded weight tensor local_shard col-wise shared local weight used lookup max_norm If given each embedding vector norm larger than max_norm renormalized have norm max_norm Note will modify weight in-place norm_type The p p-norm compute max_norm option padding_idx If specified entries padding_idx do contribute gradient therefore embedding vector padding_idx updated during training i e remains fixed pad pg process group Returns final result lookup allgather inputs first non Replicated Tensor gathered_inputs = all_gather input group=pg max_norm None max_norm changes weight in-place local_shard = _handle_max_norm_col_wise max_norm norm_type local_shard input world_size gathered_inputs pg output = _handle_col_wise_sharding_base torch nn functional embedding len input size input world_size weight local_shard pg gathered_inputs padding_idx=padding_idx output local_shard _handle_row_wise_sharding input world_size weight local_shard max_norm norm_type padding_idx rank pg Entry-point function handle logic row-wise sharding weight embedding Detailed explanations logic can found comment sharded_embedding Args input list ID used lookup aggregation world_size number ranks weight sharded weight tensor local_shard row-wise shared local weight used lookup max_norm If given each embedding vector norm larger than max_norm renormalized have norm max_norm Note will modify weight in-place norm_type The p p-norm compute max_norm option padding_idx If specified entries padding_idx do contribute gradient therefore embedding vector padding_idx updated during training i e remains fixed pad rank cuda process pg process group Returns final result lookup allgather inputs first non Replicated Tensor gather_inp = _all_gather_base_input input pg Mask input according sharding spec lookup_input padding_idx padding_row = _handle_row_wise_mask gather_inp padding_idx weight world_size rank When input large tensor value weight changed This walk-around now GH issue max_norm None torch nn functional embedding torch unique lookup_input - local_shard padding_idx=padding_idx max_norm=max_norm norm_type=norm_type max_norm = None local_input_embeddings = torch nn functional embedding lookup_input torch cat local_shard padding_row padding_idx=padding_idx max_norm=max_norm norm_type=norm_type TODO Make result PartialTensor local_shards = local_input_embeddings chunk pg size reduce_scatter torch empty_like local_shards list local_shards group=pg