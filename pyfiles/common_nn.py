mypy ignore-errors abc abstractmethod tempfile unittest copy deepcopy functools reduce partial itertools product operator mul torch torch cuda torch nn nn torch nn functional F torch nn _reduction _Reduction torch testing _internal common_utils torch testing _internal common_utils TestCase to_gpu freeze_rng_state is_iterable \ gradcheck gradgradcheck set_default_dtype skipIfTorchDynamo TEST_WITH_ROCM torch testing _internal common_cuda TEST_CUDA SM OrLater torch autograd gradcheck _get_numerical_jacobian _iter_tensors torch autograd Variable torch types _TensorOrTensors torch backends cudnn typing Union Any collections abc Callable collections abc Sequence TemporaryFile = tempfile TemporaryFile PRECISION = e- get_reduction m result = getattr m reduction None result None result = _Reduction legacy_get_string getattr m sizeAverage None True emit_warning=False assert result None result get_weight m result = getattr m weight None result None result getattr m weights None NOTE How check NN module functional API parity between Python C++ frontends The way check API parity add parity tests NN module functional interest Here detailed steps For NN module Make sure you already have test dict module configuration you want test Add ` cpp_constructor_args ` entry test dict its value exactly matching Python module constructor arguments For example test dict we pass ` ` ` torch nn Linear ` constructor then we should pass ` torch nn LinearOptions ` corresponding C++ constructor argument ` torch nn Linear ` If process performing above step you referenced any variables ` cpp_constructor_args ` entry you must add ` cpp_var_map ` entry test dict make sure those variables populated right Python values For example Python constructor call ` torch nn FractionalMaxPool d output_ratio= _random_samples=random_samples ` corresponding C++ constructor argument ` torch nn FractionalMaxPool dOptions output_ratio _random_samples random_samples ` ` cpp_var_map ` entry must ` random_samples random_samples ` order populate C++ variable ` random_samples ` used C++ constructor argument Python tensor value ` random_samples ` For NN functional Make sure you already have test dict functional configuration you want test If test dict s ` constructor ` entry looks like ` wrap_functional F some_functional_name ` then you must add ` cpp_options_args ` entry test dict its value exactly matching Python functional optional arguments For example test dict s ` constructor ` entry ` wrap_functional F interpolate size= scale_factor=None mode= nearest ` then ` cpp_options_args ` entry should F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kNearest Otherwise test dict s ` constructor ` entry looks like ` wrap_functional lambda i F some_functional_name ` then you must add ` cpp_function_call ` entry test dict its value exactly matching Python functional function call For example test dict s ` constructor ` entry ` wrap_functional lambda i F poisson_nll_loss i t type_as i reduction= none ` then ` cpp_function_call ` entry should F poisson_nll_loss i t i options F PoissonNLLLossFuncOptions reduction torch kNone If process performing above two steps you referenced any variables ` cpp_options_args ` ` cpp_function_call ` entry you must add ` cpp_var_map ` entry test dict make sure those variables populated right Python values For example test dict s ` constructor ` entry ` wrap_functional lambda i F poisson_nll_loss i t type_as i reduction= none ` then ` cpp_function_call ` entry should F poisson_nll_loss i t i options F PoissonNLLLossFuncOptions reduction torch kNone Notice there two variables ` i ` ` t ` need have their values provided way do so add ` cpp_var_map ` entry ` cpp_var_map= i _get_input t t ` Note ` i ` since we want take Python input value we pass _get_input string value C++ parity test mechanism will populate ` i ` Python input value correctly There also few optional flags test dict control C++ parity test behavior - ` test_cpp_api_parity ` ` False ` skips C++ parity test test dict Default True - ` has_parity ` ` False ` expects test dict fail C++ parity test Default True module_tests = dict module_name= Linear constructor_args= cpp_constructor_args= torch nn LinearOptions input_size= reference_fn=lambda i p _ torch mm i p t + p view - expand with_tf =True tf _precision= default_dtype=torch double dict module_name= Linear constructor_args= False cpp_constructor_args= torch nn LinearOptions bias false input_size= desc= no_bias reference_fn=lambda i p _ torch mm i p t with_tf =True tf _precision= ROCM skipping tf test gfx archs due tolerance issue test_cuda=not TEST_WITH_ROCM gfx torch cuda get_device_properties gcnArchName default_dtype=torch double dict module_name= RReLU input_size= test_cuda=False default_dtype=torch double dict module_name= RReLU constructor_args= cpp_constructor_args= torch nn RReLUOptions lower upper input_size= desc= with_up_down test_cuda=False default_dtype=torch double dict module_name= Flatten input_size= reference_fn=lambda i _ torch flatten i default_dtype=torch double TODO reference function dict module_name= CrossMapLRN d constructor_args= e- e- cpp_constructor_args= torch nn CrossMapLRN dOptions alpha e- beta e- k input_size= check_gradgrad=False TODO Figure out error RuntimeError Unrecognized tensor type ID Batched check_batched_grad=False default_dtype=torch double Generates rand tensor non-equal values This ensures duplicate values won t causing test failure modules like MaxPooling size should small otherwise randperm fails long overflows _rand_tensor_non_equal size total = reduce mul size torch randperm total view size double wrap_functional fn kwargs FunctionalModule nn Module forward args fn args kwargs FunctionalModule poissonnllloss_no_reduce_test t = torch randn dict fullname= PoissonNLLLoss_no_reduce constructor=wrap_functional lambda i F poisson_nll_loss i t type_as i reduction= none cpp_function_call= F poisson_nll_loss i t i options F PoissonNLLLossFuncOptions reduction torch kNone input_fn=lambda torch rand cpp_var_map= i _get_input t t reference_fn=lambda i _ i exp - t mul i pickle=False default_dtype=torch double bceloss_no_reduce_test t = Variable torch randn gt torch double dict fullname= BCELoss_no_reduce constructor=wrap_functional lambda i F binary_cross_entropy i t type_as i reduction= none cpp_function_call= F binary_cross_entropy i t i options F BinaryCrossEntropyFuncOptions reduction torch kNone input_fn=lambda torch rand clamp_ e- - e- cpp_var_map= i _get_input t t reference_fn=lambda i _ - t i log + - t - i log pickle=False precision= e- default_dtype=torch double bceloss_no_reduce_scalar_test t = torch randn gt torch double dict fullname= BCELoss_no_reduce_scalar constructor=wrap_functional lambda i F binary_cross_entropy i t type_as i reduction= none cpp_function_call= F binary_cross_entropy i t i options F BinaryCrossEntropyFuncOptions reduction torch kNone input_fn=lambda torch rand clamp_ e- - e- cpp_var_map= i _get_input t t reference_fn=lambda i _ - t i log + - t - i log pickle=False default_dtype=torch double bceloss_weights_no_reduce_test t = Variable torch randn dtype=torch double gt torch double weights = torch rand dtype=torch double dict fullname= BCELoss_weights_no_reduce constructor=wrap_functional lambda i F binary_cross_entropy i t type_as i weight=weights type_as i reduction= none cpp_function_call= F binary_cross_entropy i t i options F BinaryCrossEntropyFuncOptions weight weights i options reduction torch kNone input_fn=lambda torch rand clamp_ e- - e- cpp_var_map= i _get_input t t weights weights reference_fn=lambda i p m - t i log + - t - i log weights pickle=False precision= e- default_dtype=torch double bceloss_weights_no_reduce_scalar_test t = torch randn gt torch double weights = torch rand dtype=torch double dict fullname= BCELoss_weights_no_reduce_scalar constructor=wrap_functional lambda i F binary_cross_entropy i t type_as i weight=weights type_as i reduction= none cpp_function_call= F binary_cross_entropy i t i options F BinaryCrossEntropyFuncOptions weight weights i options reduction torch kNone cpp_var_map= i _get_input t t weights weights input_fn=lambda torch rand clamp_ e- - e- reference_fn=lambda i _ - t i log + - t - i log weights pickle=False default_dtype=torch double bce_with_logistic_legacy_enum_test t = Variable torch randn gt torch double sigmoid = nn Sigmoid dict fullname= BCEWithLogitsLoss_legacy_enum constructor=wrap_functional lambda i F binary_cross_entropy_with_logits i t type_as i reduce=False cpp_function_call= F binary_cross_entropy_with_logits i t i options F BinaryCrossEntropyWithLogitsFuncOptions reduction torch kNone input_fn=lambda torch rand clamp_ e- - e- cpp_var_map= i _get_input t t reference_fn=lambda i _ - t sigmoid i log + - t - sigmoid i log check_gradgrad=False pickle=False default_dtype=torch double bce_with_logistic_no_reduce_test t = Variable torch randn gt torch double sigmoid = nn Sigmoid dict fullname= BCEWithLogitsLoss_no_reduce constructor=wrap_functional lambda i F binary_cross_entropy_with_logits i t type_as i reduction= none cpp_function_call= F binary_cross_entropy_with_logits i t i options F BinaryCrossEntropyWithLogitsFuncOptions reduction torch kNone input_fn=lambda torch rand clamp_ e- - e- cpp_var_map= i _get_input t t reference_fn=lambda i _ - t sigmoid i log + - t - sigmoid i log check_gradgrad=False pickle=False default_dtype=torch double bce_with_logistic_no_reduce_scalar_test t = torch randn gt torch double sigmoid = nn Sigmoid dict fullname= BCEWithLogitsLoss_no_reduce_scalar constructor=wrap_functional lambda i F binary_cross_entropy_with_logits i t type_as i reduction= none cpp_function_call= F binary_cross_entropy_with_logits i t i options F BinaryCrossEntropyWithLogitsFuncOptions reduction torch kNone input_fn=lambda torch rand clamp_ e- - e- cpp_var_map= i _get_input t t reference_fn=lambda i _ - t sigmoid i log + - t - sigmoid i log check_gradgrad=False pickle=False default_dtype=torch double kldivloss_with_target_no_reduce_test t = torch rand dtype=torch double dict fullname= KLDivLoss_with_target_no_reduce constructor=wrap_functional lambda i F kl_div i t type_as i reduction= none cpp_function_call= F kl_div i t i options F KLDivFuncOptions reduction torch kNone input_fn=lambda torch rand log cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns KLDivLoss i t type_as i reduction= none supports_forward_ad=True pickle=False default_dtype=torch double kldivloss_no_reduce_test t = torch rand dtype=torch double dict fullname= KLDivLoss_no_reduce constructor=wrap_functional lambda i F kl_div i t type_as i reduction= none cpp_function_call= F kl_div i t i options F KLDivFuncOptions reduction torch kNone input_fn=lambda torch rand log cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns KLDivLoss i t type_as i reduction= none supports_forward_ad=True pickle=False default_dtype=torch double kldivloss_no_reduce_scalar_test t = torch rand dtype=torch double dict fullname= KLDivLoss_no_reduce_scalar constructor=wrap_functional lambda i F kl_div i t type_as i reduction= none cpp_function_call= F kl_div i t i options F KLDivFuncOptions reduction torch kNone input_fn=lambda torch rand log cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns KLDivLoss i t type_as i reduction= none supports_forward_ad=True pickle=False default_dtype=torch double kldivloss_with_log_target_no_reduce_test t = torch rand dtype=torch double log dict fullname= KLDivLoss_with_log_target_no_reduce constructor=wrap_functional lambda i F kl_div i t type_as i reduction= none log_target=True cpp_function_call= F kl_div i t i options F KLDivFuncOptions reduction torch kNone log_target true input_fn=lambda torch rand log cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns KLDivLoss_log_target i t type_as i reduction= none supports_forward_ad=True pickle=False default_dtype=torch double kldivloss_no_reduce_log_target_test t = torch rand dtype=torch double log dict fullname= KLDivLoss_no_reduce_log_target constructor=wrap_functional lambda i F kl_div i t type_as i reduction= none log_target=True cpp_function_call= F kl_div i t i options F KLDivFuncOptions reduction torch kNone log_target true input_fn=lambda torch rand log cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns KLDivLoss_log_target i t type_as i reduction= none supports_forward_ad=True pickle=False default_dtype=torch double kldivloss_no_reduce_scalar_log_target_test t = torch rand dtype=torch double log dict fullname= KLDivLoss_no_reduce_scalar_log_target constructor=wrap_functional lambda i F kl_div i t type_as i reduction= none log_target=True cpp_function_call= F kl_div i t i options F KLDivFuncOptions reduction torch kNone log_target true input_fn=lambda torch rand log cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns KLDivLoss_log_target i t type_as i reduction= none supports_forward_ad=True pickle=False default_dtype=torch double l loss_no_reduce_test t = torch randn dtype=torch double dict fullname= L Loss_no_reduce constructor=wrap_functional lambda i F l _loss i t type_as i reduction= none cpp_function_call= F l _loss i t i options F L LossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ i - t type_as i abs supports_forward_ad=True pickle=False default_dtype=torch double l loss_no_reduce_complex_test t = torch randn dtype=torch cdouble dict fullname= L Loss_no_reduce_complex constructor=wrap_functional lambda i F l _loss i t type_as i reduction= none cpp_function_call= F l _loss i t i options F L LossFuncOptions reduction torch kNone input_fn=lambda torch randn dtype=torch cdouble cpp_var_map= i _get_input t t reference_fn=lambda i _ i - t type_as i abs supports_forward_ad=True pickle=False l loss_no_reduce_scalar_test t = torch randn dtype=torch double dict fullname= L Loss_no_reduce_scalar constructor=wrap_functional lambda i F l _loss i t type_as i reduction= none cpp_function_call= F l _loss i t i options F L LossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ i - t type_as i abs supports_forward_ad=True pickle=False default_dtype=torch double mseloss_no_reduce_test input_size = target = torch randn input_size dtype=torch double dict fullname= MSELoss_no_reduce constructor=wrap_functional lambda i F mse_loss i target type_as i reduction= none cpp_function_call= F mse_loss i target i options F MSELossFuncOptions reduction torch kNone input_size=input_size cpp_var_map= i _get_input target target reference_fn=lambda i _ i - target pow supports_forward_ad=True pickle=False default_dtype=torch double mseloss_no_reduce_scalar_test input_size = target = torch randn input_size dtype=torch double dict fullname= MSELoss_no_reduce_scalar constructor=wrap_functional lambda i F mse_loss i target type_as i reduction= none cpp_function_call= F mse_loss i target i options F MSELossFuncOptions reduction torch kNone input_size=input_size cpp_var_map= i _get_input target target reference_fn=lambda i _ i - target pow supports_forward_ad=True pickle=False default_dtype=torch double nllloss_no_reduce_test t = Variable torch empty uniform_ mul floor long kwargs = reduction none dict fullname= NLLLoss_no_reduce constructor=wrap_functional lambda i F nll_loss i t type_as i long reduction=kwargs reduction cpp_function_call= F nll_loss i t i options torch kLong F NLLLossFuncOptions reduction torch kNone input_fn=lambda torch rand log cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns NLLLoss i t type_as i long kwargs pickle=False default_dtype=torch double nllloss_no_reduce_ignore_index_test t = Variable torch empty uniform_ mul floor long kwargs dict str Union int str = ignore_index reduction none dict fullname= NLLLoss_no_reduce_ignore_index constructor=wrap_functional lambda i F nll_loss i t type_as i long ignore_index=int kwargs ignore_index reduction=str kwargs reduction cpp_function_call= F nll_loss i t i options torch kLong F NLLLossFuncOptions ignore_index reduction torch kNone input_fn=lambda torch rand log cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns NLLLoss i t type_as i long kwargs pickle=False default_dtype=torch double nllloss_no_reduce_weights_test t = Variable torch empty uniform_ mul floor long weight = torch rand kwargs i weight weight type_as i reduction none dict fullname= NLLLoss_no_reduce_weights constructor=wrap_functional lambda i F nll_loss i t type_as i long kwargs i cpp_function_call= F nll_loss i t i options torch kLong F NLLLossFuncOptions weight weight i options reduction torch kNone input_fn=lambda torch rand add e- log cpp_var_map= i _get_input t t weight weight reference_fn=lambda i _ loss_reference_fns NLLLoss i t type_as i long kwargs i pickle=False default_dtype=torch double nllloss_no_reduce_weights_ignore_index_test t = Variable torch empty uniform_ mul floor long weight = torch rand kwargs i weight weight type_as i reduction none ignore_index dict fullname= NLLLoss_no_reduce_weights_ignore_index constructor=wrap_functional lambda i F nll_loss i t type_as i long kwargs i data cpp_function_call= F nll_loss i t i options torch kLong F NLLLossFuncOptions weight weight i options reduction torch kNone ignore_index input_fn=lambda torch rand add e- log cpp_var_map= i _get_input t t weight weight reference_fn=lambda i _ loss_reference_fns NLLLoss i t type_as i long kwargs i pickle=False default_dtype=torch double nllloss_no_reduce_weights_ignore_index_neg_test t = Variable torch empty uniform_ mul floor long weight = torch rand kwargs i weight weight type_as i reduction none ignore_index - dict fullname= NLLLoss_no_reduce_weights_ignore_index_neg constructor=wrap_functional lambda i F nll_loss i t type_as i long kwargs i cpp_function_call= F nll_loss i t i options torch kLong F NLLLossFuncOptions weight weight i options reduction torch kNone ignore_index - input=torch rand dtype=torch double add e- log cpp_var_map= i _get_input t t weight weight reference_fn=lambda i _ loss_reference_fns NLLLoss i t type_as i long kwargs i pickle=False default_dtype=torch double nllloss d_no_reduce_test t = Variable torch rand mul floor long kwargs = reduction none dict fullname= NLLLoss d_no_reduce constructor=wrap_functional lambda i F nll_loss i t type_as i long reduction=kwargs reduction cpp_function_call= F nll_loss i t i options torch kLong F NLLLossFuncOptions reduction torch kNone input_fn=lambda torch rand log cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns NLLLossNd i t type_as i long kwargs pickle=False default_dtype=torch double nllloss d_no_reduce_ignore_index_test t = Variable torch rand mul floor long kwargs dict str Union int str = ignore_index reduction none dict fullname= NLLLoss d_no_reduce_ignore_index constructor=wrap_functional lambda i F nll_loss i t type_as i long ignore_index=int kwargs ignore_index reduction=str kwargs reduction cpp_function_call= F nll_loss i t i options torch kLong F NLLLossFuncOptions ignore_index reduction torch kNone input_fn=lambda torch rand log cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns NLLLossNd i t type_as i long kwargs pickle=False default_dtype=torch double nllloss d_no_reduce_weights_test t = Variable torch rand mul floor long weight = torch rand kwargs i weight weight type_as i reduction none dict fullname= NLLLoss d_no_reduce_weights constructor=wrap_functional lambda i F nll_loss i t type_as i long kwargs i cpp_function_call= F nll_loss i t i options torch kLong F NLLLossFuncOptions weight weight i options reduction torch kNone input_fn=lambda torch rand log cpp_var_map= i _get_input t t weight weight reference_fn=lambda i _ loss_reference_fns NLLLossNd i t type_as i long kwargs i pickle=False default_dtype=torch double nlllossNd_no_reduce_test t = Variable torch rand mul floor long kwargs = reduction none dict fullname= NLLLossNd_no_reduce constructor=wrap_functional lambda i F nll_loss i t type_as i long reduction=kwargs reduction cpp_function_call= F nll_loss i t i options torch kLong F NLLLossFuncOptions reduction torch kNone input_fn=lambda torch rand log cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns NLLLossNd i t type_as i long kwargs pickle=False default_dtype=torch double nlllossNd_no_reduce_ignore_index_test t = Variable torch rand mul floor long kwargs dict str Union int str = ignore_index reduction none dict fullname= NLLLossNd_no_reduce_ignore_index constructor=wrap_functional lambda i F nll_loss i t type_as i long ignore_index=int kwargs ignore_index reduction=str kwargs reduction cpp_function_call= F nll_loss i t i options torch kLong F NLLLossFuncOptions ignore_index reduction torch kNone input_fn=lambda torch rand log cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns NLLLossNd i t type_as i long kwargs pickle=False default_dtype=torch double nlllossNd_no_reduce_weights_test t = Variable torch rand mul floor long weight = torch rand kwargs i weight weight type_as i reduction none dict fullname= NLLLossNd_no_reduce_weights constructor=wrap_functional lambda i F nll_loss i t type_as i long kwargs i cpp_function_call= F nll_loss i t i options torch kLong F NLLLossFuncOptions weight weight i options reduction torch kNone input_fn=lambda torch rand log cpp_var_map= i _get_input t t weight weight reference_fn=lambda i _ loss_reference_fns NLLLossNd i t type_as i long kwargs i pickle=False default_dtype=torch double smoothl loss_no_reduce_test t = torch randn dtype=torch double dict fullname= SmoothL Loss_no_reduce constructor=wrap_functional lambda i F smooth_l _loss i t type_as i reduction= none cpp_function_call= F smooth_l _loss i t i options F SmoothL LossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns SmoothL Loss i t type_as i reduction= none supports_forward_ad=True pickle=False default_dtype=torch double smoothl loss_no_reduce_scalar_test t = torch randn dtype=torch double dict fullname= SmoothL Loss_no_reduce_scalar constructor=wrap_functional lambda i F smooth_l _loss i t type_as i reduction= none cpp_function_call= F smooth_l _loss i t i options F SmoothL LossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns SmoothL Loss i t type_as i reduction= none supports_forward_ad=True pickle=False default_dtype=torch double smoothl loss_beta_test t = torch randn dtype=torch double dict fullname= SmoothL Loss_beta constructor=wrap_functional lambda i F smooth_l _loss i t type_as i reduction= none beta= cpp_function_call= F smooth_l _loss i t i options F SmoothL LossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns SmoothL Loss i t type_as i reduction= none beta= supports_forward_ad=True pickle=False default_dtype=torch double smoothl loss_zero_beta_test t = torch randn dtype=torch double dict fullname= SmoothL Loss_zero_beta constructor=wrap_functional lambda i F smooth_l _loss i t type_as i reduction= none beta= cpp_function_call= F smooth_l _loss i t i options F SmoothL LossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns SmoothL Loss i t type_as i reduction= none beta= supports_forward_ad=True pickle=False default_dtype=torch double huberloss_delta_test t = torch randn dict fullname= HuberLoss_delta constructor=wrap_functional lambda i F huber_loss i t type_as i reduction= none delta= cpp_function_call= F huber_loss i t i options F HuberLossFuncOptions reduction torch kNone delta input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns HuberLoss i t type_as i reduction= none delta= supports_forward_ad=True pickle=False default_dtype=torch double multilabelmarginloss_ d_no_reduce_test t = torch zeros long dict fullname= MultiLabelMarginLoss_ d_no_reduce constructor=wrap_functional lambda i F multilabel_margin_loss i t type_as i long reduction= none cpp_function_call= F multilabel_margin_loss i t i options torch kLong F MultilabelMarginLossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns MultiLabelMarginLoss i t data type_as i long reduction= none check_sum_reduction=True check_gradgrad=False pickle=False multilabelmarginloss_ d_no_reduce_test t = Variable torch rand mul floor long dict fullname= MultiLabelMarginLoss_ d_no_reduce constructor=wrap_functional lambda i F multilabel_margin_loss i t type_as i long reduction= none cpp_function_call= F multilabel_margin_loss i t i options torch kLong F MultilabelMarginLossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns MultiLabelMarginLoss i t data type_as i long reduction= none check_sum_reduction=True check_gradgrad=False pickle=False default_dtype=torch double multilabelmarginloss_index_neg_test t = Variable torch clamp torch rand add - mul floor long min=- dict fullname= MultiLabelMarginLoss_index_neg constructor=wrap_functional lambda i F multilabel_margin_loss i t type_as i long reduction= none cpp_function_call= F multilabel_margin_loss i t i options torch kLong F MultilabelMarginLossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns MultiLabelMarginLoss i t data type_as i long reduction= none check_sum_reduction=True check_gradgrad=False pickle=False default_dtype=torch double multilabelmarginloss_no_reduce_test t = Variable torch rand mul floor long dict fullname= MultiLabelMarginLoss_no_reduce constructor=wrap_functional lambda i F multilabel_margin_loss i t type_as i long reduction= none cpp_function_call= F multilabel_margin_loss i t i options torch kLong F MultilabelMarginLossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns MultiLabelMarginLoss i t data type_as i long reduction= none check_sum_reduction=True check_gradgrad=False pickle=False default_dtype=torch double hingeembeddingloss_no_reduce_test t = Variable torch randn gt torch double mul_ sub dict fullname= HingeEmbeddingLoss_no_reduce constructor=wrap_functional lambda i F hinge_embedding_loss i t type_as i reduction= none cpp_function_call= F hinge_embedding_loss i t i options F HingeEmbeddingLossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns HingeEmbeddingLoss i t type_as i reduction= none check_sum_reduction=True pickle=False default_dtype=torch double hingeembeddingloss_margin_no_reduce_test t = Variable torch randn gt torch double mul_ sub dict fullname= HingeEmbeddingLoss_margin_no_reduce constructor=wrap_functional lambda i F hinge_embedding_loss i t type_as i margin= reduction= none cpp_function_call= F hinge_embedding_loss i t i options F HingeEmbeddingLossFuncOptions margin reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns HingeEmbeddingLoss i t type_as i margin= reduction= none check_sum_reduction=True pickle=False default_dtype=torch double softmarginloss_no_reduce_test t = torch randn dtype=torch double dict fullname= SoftMarginLoss_no_reduce constructor=wrap_functional lambda i F soft_margin_loss i t type_as i reduction= none cpp_function_call= F soft_margin_loss i t i options F SoftMarginLossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns SoftMarginLoss i t type_as i reduction= none supports_forward_ad=True pickle=False default_dtype=torch double multilabelsoftmarginloss_no_reduce_test t = torch rand mul floor dict fullname= MultiLabelSoftMarginLoss_no_reduce constructor=wrap_functional lambda i F multilabel_soft_margin_loss i t type_as i reduction= none cpp_function_call= F multilabel_soft_margin_loss i t i options F MultilabelSoftMarginLossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ - t i sigmoid log + - t -i sigmoid log sum dim= i size check_gradgrad=False pickle=False default_dtype=torch double multilabelsoftmarginloss_weights_no_reduce_test t = torch rand mul floor weights = torch rand dict fullname= MultiLabelSoftMarginLoss_weights_no_reduce constructor=wrap_functional lambda i F multilabel_soft_margin_loss i t type_as i weight=weights type_as i reduction= none cpp_function_call= F multilabel_soft_margin_loss i t i options F MultilabelSoftMarginLossFuncOptions weight weights i options reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t weights weights reference_fn=lambda i _ - t i sigmoid log + - t -i sigmoid log weights sum dim= i size check_sum_reduction=True check_gradgrad=False pickle=False default_dtype=torch double multimarginloss_no_reduce_test t = torch rand mul floor long dict fullname= MultiMarginLoss_no_reduce constructor=wrap_functional lambda i F multi_margin_loss i t type_as i long reduction= none cpp_function_call= F multi_margin_loss i t i options torch kLong F MultiMarginLossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns MultiMarginLoss i t data type_as i long reduction= none check_sum_reduction=True check_gradgrad=False pickle=False default_dtype=torch double multimarginloss_ d_no_reduce_test t = torch rand mul floor long dict fullname= MultiMarginLoss_ d_no_reduce constructor=wrap_functional lambda i F multi_margin_loss i t type_as i long reduction= none cpp_function_call= F multi_margin_loss i t i options torch kLong F MultiMarginLossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns MultiMarginLoss i t data type_as i long reduction= none check_sum_reduction=True check_gradgrad=False pickle=False default_dtype=torch double multimarginloss_ d_input_ d_target_no_reduce_test t = torch rand mul floor long dict fullname= multimarginloss_ d_input_ d_target_no_reduce constructor=wrap_functional lambda i F multi_margin_loss i t type_as i long reduction= none cpp_function_call= F multi_margin_loss i t i options torch kLong F MultiMarginLossFuncOptions reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns MultiMarginLoss i t data type_as i long reduction= none check_sum_reduction=True check_gradgrad=False pickle=False default_dtype=torch double multimarginloss_p_no_reduce_test t = torch rand mul floor long dict fullname= MultiMarginLoss_p_no_reduce constructor=wrap_functional lambda i F multi_margin_loss i t type_as i long p= reduction= none cpp_function_call= F multi_margin_loss i t i options torch kLong F MultiMarginLossFuncOptions p reduction torch kNone input_fn=lambda torch randn clamp_ e- - e- cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns MultiMarginLoss i t data type_as i long p= reduction= none check_sum_reduction=True check_gradgrad=False pickle=False default_dtype=torch double multimarginloss_margin_no_reduce_test t = torch rand mul floor long dict fullname= MultiMarginLoss_margin_no_reduce constructor=wrap_functional lambda i F multi_margin_loss i t type_as i long margin= reduction= none cpp_function_call= F multi_margin_loss i t i options torch kLong F MultiMarginLossFuncOptions margin reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t reference_fn=lambda i _ loss_reference_fns MultiMarginLoss i t data type_as i long margin= reduction= none check_sum_reduction=True check_gradgrad=False pickle=False default_dtype=torch double multimarginloss_weights_no_reduce_test t = torch rand mul floor long weights = torch rand dtype=torch double dict fullname= MultiMarginLoss_weights_no_reduce constructor=wrap_functional lambda i F multi_margin_loss i t type_as i long weight=weights type_as i reduction= none cpp_function_call= F multi_margin_loss i t i options torch kLong F MultiMarginLossFuncOptions weight weights i options reduction torch kNone input_fn=lambda torch randn cpp_var_map= i _get_input t t weights weights reference_fn=lambda i _ loss_reference_fns MultiMarginLoss i t data type_as i long weight=weights reduction= none check_sum_reduction=True check_gradgrad=False pickle=False default_dtype=torch double single_batch_reference_fn input parameters module Reference function modules supporting no batch dimensions The module passed input target batched form single item The output squeezed compare no-batch input unsqueeze_inp inp isinstance inp list tuple t unsqueeze t inp inp unsqueeze single_batch_input = unsqueeze_inp input single_batch_input = single_batch_input isinstance single_batch_input torch Tensor single_batch_input freeze_rng_state module single_batch_input squeeze get_new_module_tests common_utils set_rng_seed new_module_tests = poissonnllloss_no_reduce_test bceloss_no_reduce_test bceloss_weights_no_reduce_test bce_with_logistic_legacy_enum_test bce_with_logistic_no_reduce_test bceloss_no_reduce_scalar_test bceloss_weights_no_reduce_scalar_test bce_with_logistic_no_reduce_scalar_test kldivloss_with_target_no_reduce_test kldivloss_no_reduce_test kldivloss_no_reduce_scalar_test kldivloss_with_log_target_no_reduce_test kldivloss_no_reduce_log_target_test kldivloss_no_reduce_scalar_log_target_test l loss_no_reduce_test l loss_no_reduce_complex_test l loss_no_reduce_scalar_test mseloss_no_reduce_test mseloss_no_reduce_scalar_test nllloss_no_reduce_test nllloss_no_reduce_ignore_index_test nllloss_no_reduce_weights_test nllloss_no_reduce_weights_ignore_index_test nllloss_no_reduce_weights_ignore_index_neg_test nllloss d_no_reduce_test nllloss d_no_reduce_weights_test nllloss d_no_reduce_ignore_index_test nlllossNd_no_reduce_test nlllossNd_no_reduce_weights_test nlllossNd_no_reduce_ignore_index_test smoothl loss_no_reduce_test smoothl loss_no_reduce_scalar_test smoothl loss_beta_test smoothl loss_zero_beta_test huberloss_delta_test multilabelmarginloss_ d_no_reduce_test multilabelmarginloss_ d_no_reduce_test multilabelmarginloss_index_neg_test multilabelmarginloss_no_reduce_test hingeembeddingloss_no_reduce_test hingeembeddingloss_margin_no_reduce_test softmarginloss_no_reduce_test multilabelsoftmarginloss_no_reduce_test multilabelsoftmarginloss_weights_no_reduce_test multimarginloss_no_reduce_test multimarginloss_ d_no_reduce_test multimarginloss_ d_input_ d_target_no_reduce_test multimarginloss_p_no_reduce_test multimarginloss_margin_no_reduce_test multimarginloss_weights_no_reduce_test dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions input_size= cudnn=True with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions stride input_size= cudnn=True desc= stride with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions stride padding input_size= cudnn=True desc= pad with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions stride padding input_size= cudnn=True desc= pad with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions stride padding input_size= cudnn=True desc= pad size with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions stride padding input_size= cudnn=True desc= pad size with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions input_size= cudnn=True desc= zero_batch with_tf =True tf _precision= dict fullname= Conv d_dilated constructor=lambda nn Conv d kernel_size= dilation= cpp_constructor_args= torch nn Conv dOptions dilation input_size= with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_groups constructor=lambda nn Conv d kernel_size= groups= cpp_constructor_args= torch nn Conv dOptions groups input_size= cudnn=True with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_pad_valid constructor=lambda nn Conv d padding= valid cpp_constructor_args= torch nn Conv dOptions padding torch kValid input_size= cudnn=True with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_pad_same constructor=lambda nn Conv d padding= same cpp_constructor_args= torch nn Conv dOptions padding torch kSame input_size= cudnn=True with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_pad_same constructor=lambda nn Conv d padding= same cpp_constructor_args= torch nn Conv dOptions padding torch kSame input_size= cudnn=True with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_pad_same_dilated constructor=lambda nn Conv d padding= same dilation= cpp_constructor_args= torch nn Conv dOptions padding torch kSame dilation input_size= cudnn=True with_tf =True tf _precision= default_dtype=torch double dict fullname= ConvTranspose d constructor=lambda nn ConvTranspose d kernel_size= stride= padding= output_padding= cpp_constructor_args= torch nn ConvTranspose dOptions stride padding output_padding cudnn=True input_size= with_tf =True tf _precision= default_dtype=torch double dict module_name= ConvTranspose d constructor_args= False cpp_constructor_args= torch nn ConvTranspose dOptions stride padding output_padding groups bias false input_size= cudnn=True desc= no_bias with_tf =True tf _precision= default_dtype=torch double dict module_name= ConvTranspose d constructor_args= True cpp_constructor_args= torch nn ConvTranspose dOptions stride padding output_padding groups bias true dilation input_size= cudnn=True desc= dilated with_tf =True tf _precision= default_dtype=torch double dict fullname= ConvTranspose d_groups constructor=lambda nn ConvTranspose d stride= padding= output_padding= groups= cpp_constructor_args= torch nn ConvTranspose dOptions stride padding output_padding groups cudnn=True input_size= with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions input_size= cudnn=True check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions stride input_size= cudnn=True desc= strided check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions stride padding input_size= cudnn=True desc= padding check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions stride padding dilation input_size= cudnn=True desc= dilated check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= False cpp_constructor_args= torch nn Conv dOptions stride padding dilation groups bias false input_size= cudnn=True desc= no_bias check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions input_size= cudnn=True desc= zero_batch check_with_long_tensor=True with_tf =True dict fullname= Conv d_groups constructor=lambda nn Conv d groups= cpp_constructor_args= torch nn Conv dOptions groups input_size= cudnn=True check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_groups_thnn constructor=lambda nn Conv d groups= cpp_constructor_args= torch nn Conv dOptions groups input_size= check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_pad_valid constructor=lambda nn Conv d padding= valid cpp_constructor_args= torch nn Conv dOptions padding torch kValid input_size= cudnn=True with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_pad_same constructor=lambda nn Conv d padding= same cpp_constructor_args= torch nn Conv dOptions padding torch kSame input_size= cudnn=True with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_pad_same_dilated constructor=lambda nn Conv d padding= same dilation= cpp_constructor_args= torch nn Conv dOptions padding torch kSame dilation input_size= cudnn=True with_tf =True tf _precision= default_dtype=torch double dict module_name= ConvTranspose d constructor_args= cpp_constructor_args= torch nn ConvTranspose dOptions stride padding output_padding cudnn=True input_size= check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict module_name= ConvTranspose d constructor_args= False cpp_constructor_args= torch nn ConvTranspose dOptions stride padding output_padding groups bias false dilation input_size= cudnn=True desc= dilated check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict module_name= ConvTranspose d constructor_args= False cpp_constructor_args= torch nn ConvTranspose dOptions stride padding output_padding groups bias false input_size= cudnn=True desc= no_bias check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict fullname= ConvTranspose d_groups constructor=lambda nn ConvTranspose d groups= cpp_constructor_args= torch nn ConvTranspose dOptions groups input_size= cudnn=True check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_depthwise constructor=lambda nn Conv d groups= cpp_constructor_args= torch nn Conv dOptions groups input_size= with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_depthwise_with_multiplier constructor=lambda nn Conv d groups= cpp_constructor_args= torch nn Conv dOptions groups input_size= with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_depthwise_strided constructor=lambda nn Conv d stride= groups= cpp_constructor_args= torch nn Conv dOptions stride groups input_size= with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_depthwise_padded constructor=lambda nn Conv d padding= groups= cpp_constructor_args= torch nn Conv dOptions padding groups input_size= with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_depthwise_dilated constructor=lambda nn Conv d dilation= groups= cpp_constructor_args= torch nn Conv dOptions dilation groups input_size= with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions input_size= cudnn=True check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= False cpp_constructor_args= torch nn Conv dOptions stride padding dilation groups bias false input_size= cudnn=True desc= no_bias check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= False cpp_constructor_args= torch nn Conv dOptions stride padding dilation groups bias false input_size= cudnn=True desc= x x _no_bias check_with_long_tensor=False with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions stride input_size= cudnn=True desc= stride check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions stride padding input_size= cudnn=True desc= stride_padding check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict module_name= Conv d constructor_args= cpp_constructor_args= torch nn Conv dOptions input_size= cudnn=True check_with_long_tensor=True desc= zero_batch with_tf =True dict fullname= Conv d_groups constructor=lambda nn Conv d kernel_size= groups= cpp_constructor_args= torch nn Conv dOptions groups input_size= cudnn=True check_with_long_tensor=True with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_dilated constructor=lambda nn Conv d kernel_size= dilation= cpp_constructor_args= torch nn Conv dOptions dilation input_size= with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_dilated_strided constructor=lambda nn Conv d kernel_size= dilation= stride= cpp_constructor_args= torch nn Conv dOptions dilation stride input_size= with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_pad_valid constructor=lambda nn Conv d padding= valid cpp_constructor_args= torch nn Conv dOptions padding torch kValid input_size= cudnn=True with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_pad_same constructor=lambda nn Conv d padding= same cpp_constructor_args= torch nn Conv dOptions padding torch kSame input_size= cudnn=True with_tf =True tf _precision= default_dtype=torch double dict fullname= Conv d_pad_same_dilated constructor=lambda nn Conv d padding= same dilation= cpp_constructor_args= torch nn Conv dOptions padding torch kSame dilation input_size= cudnn=True with_tf =True tf _precision= default_dtype=torch double dict module_name= ConvTranspose d constructor_args= cpp_constructor_args= torch nn ConvTranspose dOptions cudnn=True input_size= with_tf =True tf _precision= default_dtype=torch double dict module_name= ConvTranspose d constructor_args= True cpp_constructor_args= torch nn ConvTranspose dOptions stride padding output_padding groups bias true dilation cudnn=True input_size= desc= dilated with_tf =True tf _precision= default_dtype=torch double dict module_name= ReplicationPad d constructor_args= cpp_constructor_args= torch nn ReplicationPad dOptions input_size= default_dtype=torch double dict module_name= ReplicationPad d constructor_args= cpp_constructor_args= torch nn ReplicationPad dOptions input_size= reference_fn=single_batch_reference_fn desc= no_batch_dim default_dtype=torch double dict module_name= ReplicationPad d constructor_args= cpp_constructor_args= torch nn ReplicationPad dOptions input_fn=lambda torch rand dtype=torch complex requires_grad=True skip_half=True desc= complex dict module_name= Embedding constructor_args= cpp_constructor_args= torch nn EmbeddingOptions input_fn=lambda torch empty dtype=torch long random_ check_gradgrad=False default_dtype=torch double decorator=skipIfTorchDynamo https github com pytorch pytorch issues dict module_name= Embedding constructor_args= cpp_constructor_args= torch nn EmbeddingOptions input_fn=lambda torch empty dtype=torch long random_ expand check_gradgrad=False desc= discontiguous default_dtype=torch double decorator=skipIfTorchDynamo https github com pytorch pytorch issues dict module_name= EmbeddingBag constructor_args= cpp_constructor_args= torch nn EmbeddingBagOptions input_fn=lambda torch empty dtype=torch long random_ check_gradgrad=False desc= mean default_dtype=torch double dict module_name= EmbeddingBag constructor_args= cpp_constructor_args= torch nn EmbeddingBagOptions input_fn=lambda torch empty dtype=torch long random_ expand check_gradgrad=False desc= discontiguous default_dtype=torch double dict module_name= EmbeddingBag constructor_args= None False sum cpp_constructor_args= torch nn EmbeddingBagOptions max_norm std nullopt norm_type scale_grad_by_freq false mode torch kSum input_fn=lambda torch empty dtype=torch long random_ check_gradgrad=False desc= sum default_dtype=torch double dict module_name= EmbeddingBag constructor_args= None False max cpp_constructor_args= torch nn EmbeddingBagOptions max_norm std nullopt norm_type scale_grad_by_freq false mode torch kMax input_fn=lambda torch empty dtype=torch long random_ check_gradgrad=False desc= max default_dtype=torch double dict fullname= EmbeddingBag_mean_padding_idx constructor=lambda nn EmbeddingBag padding_idx= cpp_constructor_args= torch nn EmbeddingBagOptions padding_idx input_fn=lambda torch stack torch randperm torch randperm check_gradgrad=False default_dtype=torch double dict fullname= EmbeddingBag_sum_padding_idx constructor=lambda nn EmbeddingBag None False sum padding_idx= cpp_constructor_args= torch nn EmbeddingBagOptions max_norm std nullopt norm_type scale_grad_by_freq false mode torch kSum padding_idx input_fn=lambda torch stack torch randperm torch randperm check_gradgrad=False default_dtype=torch double dict fullname= EmbeddingBag_max_padding_idx constructor=lambda nn EmbeddingBag None False max padding_idx= cpp_constructor_args= torch nn EmbeddingBagOptions max_norm std nullopt norm_type scale_grad_by_freq false mode torch kMax padding_idx input_fn=lambda torch stack torch randperm torch randperm check_gradgrad=False default_dtype=torch double dict fullname= EmbeddingBag_sparse constructor=lambda nn EmbeddingBag sparse=True dtype=torch double cpp_constructor_args= torch nn EmbeddingBagOptions sparse true _weight torch rand torch kFloat input_fn=lambda torch randperm repeat check_gradgrad=False has_sparse_gradients=True dict constructor=lambda nn Embedding dtype=torch double sparse=True cpp_constructor_args= torch nn EmbeddingOptions sparse true _weight torch rand torch kFloat input_fn=lambda torch randperm repeat fullname= Embedding_sparse check_gradgrad=False has_sparse_gradients=True dict module_name= PixelShuffle constructor_args= cpp_constructor_args= torch nn PixelShuffleOptions input_size= default_dtype=torch double dict module_name= PixelUnshuffle constructor_args= cpp_constructor_args= torch nn PixelUnshuffleOptions input_size= default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= nearest cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kNearest input_size= fullname= interpolate_nearest_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= nearest cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kNearest input_size= fullname= interpolate_nearest_ d_zero_dim pickle=False dict constructor=wrap_functional F interpolate size= scale_factor=None mode= nearest cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kNearest input_size= fullname= interpolate_nearest_tuple_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size=None scale_factor= mode= nearest cpp_options_args= F InterpolateFuncOptions size std nullopt scale_factor std vector double mode torch kNearest input_size= fullname= interpolate_nearest_scale_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= linear align_corners=False cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kLinear align_corners false input_size= fullname= interpolate_linear_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= linear align_corners=False cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kLinear align_corners false input_size= fullname= interpolate_linear_tuple_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size=None scale_factor= mode= linear align_corners=False cpp_options_args= F InterpolateFuncOptions size std nullopt scale_factor std vector double mode torch kLinear align_corners false input_size= fullname= interpolate_linear_scale_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= linear align_corners=False cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kLinear align_corners false input_size= fullname= interpolate_linear_ d_zero_dim pickle=False dict constructor=wrap_functional F interpolate size= scale_factor=None mode= linear align_corners=True cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kLinear align_corners true input_size= fullname= interpolate_linear_ d_align_corners pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size=None scale_factor= mode= linear align_corners=True cpp_options_args= F InterpolateFuncOptions size std nullopt scale_factor std vector double mode torch kLinear align_corners true input_size= fullname= interpolate_linear_scale_ d_align_corners pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= nearest cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kNearest input_size= fullname= interpolate_nearest_ d_launch_configs pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= nearest cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kNearest input_size= fullname= interpolate_nearest_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= nearest cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kNearest input_size= fullname= interpolate_nearest_tuple_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size=None scale_factor= mode= nearest cpp_options_args= F InterpolateFuncOptions size std nullopt scale_factor std vector double mode torch kNearest input_size= fullname= interpolate_nearest_scale_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= nearest cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kNearest input_size= fullname= interpolate_nearest_ d_zero_dim pickle=False dict constructor=wrap_functional F interpolate size= scale_factor=None mode= bilinear align_corners=False cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kBilinear align_corners false input_size= fullname= interpolate_bilinear_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= bilinear align_corners=False cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kBilinear align_corners false input_size= fullname= interpolate_bilinear_ d_zero_dim pickle=False dict constructor=wrap_functional F interpolate size= scale_factor=None mode= bilinear align_corners=False cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kBilinear align_corners false input_size= fullname= interpolate_bilinear_tuple_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size=None scale_factor= mode= bilinear align_corners=False cpp_options_args= F InterpolateFuncOptions size std nullopt scale_factor std vector double mode torch kBilinear align_corners false input_size= fullname= interpolate_bilinear_scale_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size=None scale_factor= mode= bilinear align_corners=False cpp_options_args= F InterpolateFuncOptions size std nullopt scale_factor std vector double mode torch kBilinear align_corners false input_size= fullname= interpolate_bilinear_scale_tuple_shared_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size=None scale_factor= mode= bilinear align_corners=False cpp_options_args= F InterpolateFuncOptions size std nullopt scale_factor std vector double mode torch kBilinear align_corners false input_size= fullname= interpolate_bilinear_scale_tuple_skewed_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= bilinear align_corners=True cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kBilinear align_corners true input_size= fullname= interpolate_bilinear_tuple_ d_align_corners pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size=None scale_factor= mode= bilinear align_corners=True cpp_options_args= F InterpolateFuncOptions size std nullopt scale_factor std vector double mode torch kBilinear align_corners true input_size= fullname= interpolate_bilinear_scale_tuple_skewed_ d_align_corners pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= bicubic align_corners=False cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kBicubic align_corners false input_size= fullname= interpolate_bicubic_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= bicubic align_corners=False cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kBicubic align_corners false input_size= fullname= interpolate_bicubic_ d_zero_dim pickle=False dict constructor=wrap_functional F interpolate size= scale_factor=None mode= bicubic align_corners=False cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kBicubic align_corners false input_size= fullname= interpolate_bicubic_tuple_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size=None scale_factor= mode= bicubic align_corners=False cpp_options_args= F InterpolateFuncOptions size std nullopt scale_factor std vector double mode torch kBicubic align_corners false input_size= fullname= interpolate_bicubic_scale_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size=None scale_factor= mode= bicubic align_corners=False cpp_options_args= F InterpolateFuncOptions size std nullopt scale_factor std vector double mode torch kBicubic align_corners false input_size= fullname= interpolate_bicubic_scale_tuple_shared_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size=None scale_factor= mode= bicubic align_corners=False cpp_options_args= F InterpolateFuncOptions size std nullopt scale_factor std vector double mode torch kBicubic align_corners false input_size= fullname= interpolate_bicubic_scale_tuple_skewed_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= bicubic align_corners=True cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kBicubic align_corners true input_size= fullname= interpolate_bicubic_tuple_ d_align_corners pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size=None scale_factor= mode= bicubic align_corners=True cpp_options_args= F InterpolateFuncOptions size std nullopt scale_factor std vector double mode torch kBicubic align_corners true input_size= fullname= interpolate_bicubic_scale_tuple_skewed_ d_align_corners pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= nearest cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kNearest input_size= fullname= interpolate_nearest_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= nearest cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kNearest input_size= fullname= interpolate_nearest_ d_zero_dim pickle=False dict constructor=wrap_functional F interpolate size= scale_factor=None mode= nearest cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kNearest input_size= fullname= interpolate_nearest_tuple_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size=None scale_factor= mode= nearest cpp_options_args= F InterpolateFuncOptions size std nullopt scale_factor std vector double mode torch kNearest input_size= fullname= interpolate_nearest_scale_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= trilinear align_corners=False cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kTrilinear align_corners false input_size= fullname= interpolate_trilinear_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= trilinear align_corners=False cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kTrilinear align_corners false input_size= fullname= interpolate_trilinear_ d_zero_dim pickle=False dict constructor=wrap_functional F interpolate size= scale_factor=None mode= trilinear align_corners=False cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kTrilinear align_corners false input_size= fullname= interpolate_trilinear_tuple_ d pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size=None scale_factor= mode= trilinear align_corners=False cpp_options_args= F InterpolateFuncOptions size std nullopt scale_factor std vector double mode torch kTrilinear align_corners false input_size= fullname= interpolate_trilinear_scale_ d See https github com pytorch pytorch issues precision= e- pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size= scale_factor=None mode= trilinear align_corners=True cpp_options_args= F InterpolateFuncOptions size std vector int _t scale_factor std nullopt mode torch kTrilinear align_corners true input_size= fullname= interpolate_trilinear_tuple_ d_align_corners pickle=False default_dtype=torch double dict constructor=wrap_functional F interpolate size=None scale_factor= mode= trilinear align_corners=True cpp_options_args= F InterpolateFuncOptions size std nullopt scale_factor std vector double mode torch kTrilinear align_corners true input_size= fullname= interpolate_trilinear_scale_ d_align_corners See https github com pytorch pytorch issues precision= e- pickle=False default_dtype=torch double dict constructor=wrap_functional F softmax dim=- cpp_options_args= F SoftmaxFuncOptions - input_size= trigger last-dim algo CUDA fullname= softmax_lastdim pickle=False default_dtype=torch double dict constructor=wrap_functional F softmax dim= dtype=torch float cpp_options_args= F SoftmaxFuncOptions dtype torch kFloat input_size= fullname= softmax_lastdim_dtype pickle=False test_cuda=False default_dtype=torch double dict constructor=wrap_functional F softmax dim= cpp_options_args= F SoftmaxFuncOptions input_size= trigger special case spatial CUDA algo fullname= softmax_spatial_special pickle=False default_dtype=torch double dict constructor=wrap_functional F softmax dim= cpp_options_args= F SoftmaxFuncOptions input_size= regular spatial algorithm fullname= softmax_spatial pickle=False default_dtype=torch double dict constructor=wrap_functional F softmax dim= dtype=torch float cpp_options_args= F SoftmaxFuncOptions dtype torch kFloat input_size= regular spatial algorithm fullname= softmax_spatial_dtype pickle=False test_cuda=False default_dtype=torch double dict constructor=wrap_functional F softmax dim= cpp_options_args= F SoftmaxFuncOptions input_size= fullname= softmax_functional_dim test_cuda=False pickle=False default_dtype=torch double dict constructor=wrap_functional F softmax dim= cpp_options_args= F SoftmaxFuncOptions input_size= fullname= softmax_functional_dim test_cuda=False pickle=False default_dtype=torch double dict constructor=wrap_functional F softmax dim=- cpp_options_args= F SoftmaxFuncOptions - input_size= fullname= softmax_functional_scalar test_cuda=False pickle=False dict constructor=wrap_functional F log_softmax dim=- cpp_options_args= F LogSoftmaxFuncOptions - input_size= trigger last-dim algo CUDA fullname= log_softmax_lastdim pickle=False default_dtype=torch double dict constructor=wrap_functional F log_softmax dim= cpp_options_args= F LogSoftmaxFuncOptions input_size= trigger special case spatial CUDA algo fullname= log_softmax_spatial_special pickle=False default_dtype=torch double dict constructor=wrap_functional F log_softmax dim= cpp_options_args= F LogSoftmaxFuncOptions input_size= regular spatial algorithm fullname= log_softmax_spatial pickle=False default_dtype=torch double dict constructor=wrap_functional F log_softmax dim= cpp_options_args= F LogSoftmaxFuncOptions input_size= fullname= log_softmax_dim pickle=False default_dtype=torch double dict constructor=wrap_functional F log_softmax dim= cpp_options_args= F LogSoftmaxFuncOptions input_size= fullname= log_softmax_dim pickle=False default_dtype=torch double dict constructor=wrap_functional F log_softmax dim= cpp_options_args= F LogSoftmaxFuncOptions input_size= fullname= log_softmax_scalar pickle=False dict fullname= Unfold constructor=lambda nn Unfold cpp_constructor_args= torch nn UnfoldOptions dilation padding stride input_size= check_gradgrad=False test_cuda=True default_dtype=torch double dict fullname= Fold constructor=lambda nn Fold cpp_constructor_args= torch nn FoldOptions dilation padding stride input_size= check_gradgrad=False test_cuda=True default_dtype=torch double dict fullname= Fold_no_batch_dim_input constructor=lambda nn Fold cpp_constructor_args= torch nn FoldOptions dilation padding stride input_size= check_gradgrad=False ref=single_batch_reference_fn test_cuda=True default_dtype=torch double dict fullname= Unfold_int_input constructor=lambda nn Unfold cpp_constructor_args= torch nn UnfoldOptions dilation padding stride input_size= check_gradgrad=False test_cuda=True default_dtype=torch double dict fullname= Fold_int_input constructor=lambda nn Fold cpp_constructor_args= torch nn FoldOptions dilation padding stride input_size= check_gradgrad=False test_cuda=True default_dtype=torch double dict fullname= Fold_no_batch_dim_int_input constructor=lambda nn Fold cpp_constructor_args= torch nn FoldOptions dilation padding stride input_size= ref=single_batch_reference_fn check_gradgrad=False test_cuda=True default_dtype=torch double dict module_name= RReLU constructor_args= cpp_constructor_args= torch nn RReLUOptions lower upper input_size= desc= with_up_down_scalar test_cuda=False default_dtype=torch double dict module_name= PairwiseDistance input_fn=lambda torch randn torch randn default_dtype=torch double dict module_name= PairwiseDistance input_fn=lambda torch randn torch randn desc= broadcast_lhs default_dtype=torch double dict module_name= PairwiseDistance input_fn=lambda torch randn torch randn desc= broadcast_rhs default_dtype=torch double dict module_name= PairwiseDistance constructor_args= e- True cpp_constructor_args= torch nn PairwiseDistanceOptions p eps e- keepdim true input_fn=lambda torch randn torch randn desc= with_non_default_args default_dtype=torch double dict module_name= PairwiseDistance input_fn=lambda torch randn torch randn reference_fn=single_batch_reference_fn desc= no_batch_dim default_dtype=torch double dict module_name= TransformerEncoderLayer constructor_args= cpp_constructor_args= torch nn TransformerEncoderLayerOptions dim_feedforward dropout input_size= desc= relu_activation with_tf =True tf _precision= TODO figure out error RuntimeError The size tensor must match size tensor b non-singleton dimension check_batched_grad=False check_gradgrad=False default_dtype=torch double dict module_name= TransformerEncoderLayer constructor_args= F gelu cpp_constructor_args= torch nn TransformerEncoderLayerOptions dim_feedforward dropout activation torch kGELU input_size= check_gradgrad=False desc= gelu_activation with_tf =True tf _precision= SM OrLater default_dtype=torch double dict module_name= TransformerDecoderLayer constructor_args= cpp_constructor_args= torch nn TransformerDecoderLayerOptions dim_feedforward dropout input_fn=lambda torch rand torch rand check_gradgrad=False desc= relu_activation with_tf =True tf _precision= default_dtype=torch double dict module_name= TransformerDecoderLayer constructor_args= F gelu cpp_constructor_args= torch nn TransformerDecoderLayerOptions dim_feedforward dropout activation torch kGELU input_fn=lambda torch rand torch rand check_gradgrad=False desc= gelu_activation with_tf =True tf _precision= default_dtype=torch double dict module_name= Transformer constructor_args= F relu cpp_constructor_args= torch nn TransformerOptions d_model nhead num_encoder_layers num_decoder_layers dim_feedforward dropout activation torch kReLU input_fn=lambda torch rand torch rand torch rand check_gradgrad=False desc= multilayer_coder with_tf =True tf _precision= SM OrLater default_dtype=torch double dict module_name= Linear constructor_args= cpp_constructor_args= torch nn LinearOptions input_fn=lambda torch rand reference_fn=lambda i p _ torch mm i view - p t view - + p desc= no_batch_dim with_tf =True tf _precision= default_dtype=torch double dict module_name= Flatten cpp_constructor_args= torch nn FlattenOptions start_dim - end_dim - constructor_args= - - input_size= reference_fn=single_batch_reference_fn desc= no_batch_dim default_dtype=torch double dict module_name= Unflatten cpp_constructor_args= torch nn UnflattenOptions - constructor_args= - torch Size input_size= reference_fn=single_batch_reference_fn desc= no_batch_dim default_dtype=torch double dict module_name= LayerNorm constructor_args= e- False cpp_constructor_args= torch nn LayerNormOptions eps e- elementwise_affine false input_size= cudnn=True check_eval=True gradcheck_fast_mode=True check_half=True desc= d_no_affine_large_feature add conv padding mode tests padding_mode cpp_padding_mode zip reflect circular replicate zeros torch kReflect torch kCircular torch kReplicate torch kZeros strict=True conv signature in_channels out_channels kernel_size stride= padding= dilation= groups= bias=True padding_mode= zeros d d == padding_mode == reflect FIXME remove after implementing reflection pad d https github com pytorch pytorch issues continue padding = tuple range d + cpp_padding = + join map str padding + input_size = + d output_size = + tuple p + p padding simplified ` + p - + ` new_module_tests append dict module_name=f Conv d d constructor_args= padding True padding_mode cpp_constructor_args=f torch nn Conv d dOptions stride padding cpp_padding dilation groups bias true padding_mode cpp_padding_mode input_size=input_size output_size=output_size cudnn=True desc=f padding_mode _stride _pad with_tf =True tf _precision= default_dtype=torch double Check non linear activations work no batch dimensions non_linear_activations_no_batch = ELU Hardshrink Hardsigmoid Hardtanh Hardswish LeakyReLU LogSigmoid PReLU ReLU ReLU RReLU SELU CELU GELU GLU Sigmoid SiLU Mish Softplus Softshrink Softsign Tanh Tanhshrink Threshold non_linear_activations_extra_info dict str dict = CELU constructor_args default_dtype torch double Threshold constructor_args Hardsigmoid check_gradgrad False check_jit False default_dtype torch double Hardswish check_gradgrad False check_jit False default_dtype torch double For RRelu test compare CPU GPU results fail because RNG different between CPU GPU RReLU test_cuda False default_dtype torch double ELU default_dtype torch double GELU default_dtype torch double GLU default_dtype torch double Hardshrink default_dtype torch double Hardtanh default_dtype torch double LeakyReLU default_dtype torch double LogSigmoid default_dtype torch double Mish default_dtype torch double PReLU default_dtype torch double ReLU default_dtype torch double ReLU default_dtype torch double SELU default_dtype torch double SiLU default_dtype torch double Sigmoid default_dtype torch double Softplus default_dtype torch double Softshrink default_dtype torch double Softsign default_dtype torch double Tanh default_dtype torch double Tanhshrink default_dtype torch double non_linear_activation non_linear_activations_no_batch activation_test_info = dict module_name=non_linear_activation input_size= reference_fn=single_batch_reference_fn desc= no_batch_dim test_cpp_api_parity=False extra_info = non_linear_activations_extra_info get non_linear_activation activation_test_info update extra_info new_module_tests append activation_test_info new_module_tests kldivloss_reference input target reduction= mean log_target=False log_target result = torch exp target target - input result = target target log - input reduction == mean result mean reduction == sum result sum reduction == batchmean result dim = result sum result size result nlllossNd_reference input target weight=None ignore_index=- reduction= mean assert input dim = N = input size C = input size out_size = N + input size output = torch zeros out_size type_as input weight None weight = torch ones C type_as input total_weight = tup product range size size out_size t_nx = target tup norm = ignore_index == t_nx weight t_nx item input_index = list tup input_index insert t_nx output tup = -input tuple input_index norm total_weight += norm reduction == mean output sum total_weight reduction == sum output sum output cross_entropy_loss_prob_target_reference input target weight=None reduction= mean label_smoothing= assert input dim = input = torch log_softmax input C = input size weight None weight = torch ones C type_as input weight = weight view C _ input shape label_smoothing assert label_smoothing = target = target - label_smoothing + label_smoothing C output = - input target weight sum dim= reduction == mean output mean reduction == sum output sum output cross_entropy_loss_indices_target_reference input target weight=None ignore_index=- reduction= mean label_smoothing= log_softmax_input = torch log_softmax input nllloss = F nll_loss log_softmax_input target weight ignore_index=ignore_index reduction=reduction label_smoothing == nllloss assert label_smoothing = input = torch log_softmax input C = input size weight None input = input weight view C _ input shape smooth_loss = -torch sum input ignore_mask = target == ignore_index smooth_loss masked_fill_ ignore_mask reduction == mean weight None TODO This code can path can removed resolved loss normalized weights consistent nll_loss_nd ret = torch sum smooth_loss weight gather target masked_select ignore_mask logical_not flatten sum ret = torch mean smooth_loss masked_select ignore_mask logical_not reduction == sum ret = torch sum smooth_loss ret = smooth_loss - label_smoothing nllloss + ret label_smoothing C cross_entropy_loss_reference input target weight=None ignore_index=- reduction= mean label_smoothing= input shape == target shape cross_entropy_loss_prob_target_reference input target weight=weight reduction=reduction label_smoothing=label_smoothing cross_entropy_loss_indices_target_reference input target weight=weight reduction=reduction ignore_index=ignore_index label_smoothing=label_smoothing nllloss_reference input target weight=None ignore_index=- reduction= mean nll_loss_helper input target weight ignore_index target == ignore_index norm = weight None weight target result = -input target norm result norm losses_and_weights = nll_loss_helper i t weight ignore_index i t zip input target strict=True losses weights = zip losses_and_weights strict=True losses_tensor = input new_tensor losses reduction == mean sum losses_tensor sum weights reduction == sum sum losses_tensor losses_tensor smoothl loss_reference input target reduction= mean beta= abs_diff = input - target abs ge_beta_mask = abs_diff = beta type_as abs_diff lt_beta_mask = abs_diff beta type_as abs_diff when beta = we should just use l _loss beta == output = abs_diff output = ge_beta_mask abs_diff - beta + lt_beta_mask abs_diff beta reduction == mean output mean reduction == sum output sum output huberloss_reference input target reduction= mean delta= abs_diff = input - target abs ge_delta_mask = abs_diff = delta lt_delta_mask = abs_diff delta output = ge_delta_mask delta abs_diff - delta + lt_delta_mask abs_diff reduction == mean output mean reduction == sum output sum output _multilabelmarginloss_reference input target targets = target_index target target_index break targets append target_index sum = target_index targets i range len input i targets sum += max - input target_index + input i sum multilabelmarginloss_reference input target reduction= mean make everything -dimensional input_dim = input dim input dim assert target dim input = input unsqueeze input dim == input unsqueeze unsqueeze target = target unsqueeze target dim == target unsqueeze unsqueeze n = input size dim = input size output = input new n zero_ i range n output i = _multilabelmarginloss_reference input i target i reduction == mean output mean dim reduction == sum output sum dim input_dim we know we have C X C - so squeeze will get us back correct dimensionality output squeeze dim output dim hingeembeddingloss_reference input target margin= reduction= mean margin_clamp = margin - input clamp min= type_as input output = torch where target == input margin_clamp reduction == mean output mean reduction == sum output sum output softmarginloss_reference input target reduction= mean output = + -input target exp log reduction == mean output mean reduction == sum output sum output _multimarginloss_reference input target_idx p margin weight weight None weight = input new len input fill_ output = i range len input i = target_idx output += weight target_idx max margin - input target_idx + input i p output multimarginloss_reference input target p= margin= weight=None reduction= mean input dim input = input unsqueeze input dim == input unsqueeze unsqueeze target_dim = target dim target dim == target = target unsqueeze n = input size dim = input size output = input new n x range n output x = _multimarginloss_reference input x target x p margin weight reduction == mean output mean dim reduction == sum output sum dim target_dim == output squeeze dim output dim cosineembeddingloss_reference input input target margin= reduction= mean _cos b cos = new size i range size cos i = i b i sum i i sum + e- b i b i sum + e- cos output = torch where target == - _cos input input _cos input input - margin clamp min= reduction == mean output mean reduction == sum output sum output tripletmarginloss_reference anchor positive negative margin= p= eps= e- swap=False reduction= mean d_p = torch pairwise_distance anchor positive p eps d_n = torch pairwise_distance anchor negative p eps swap d_s = torch pairwise_distance positive negative p eps d_n = torch min d_n d_s output = torch clamp margin + d_p - d_n min= reduction == mean output mean reduction == sum output sum output marginrankingloss_reference input input target margin= reduction= mean output = -target input - input + margin clamp min= reduction == mean output mean reduction == sum output sum output directly follows Graves et al s paper contrast production implementation does use log-space ctcloss_reference log_probs targets input_lengths target_lengths blank= reduction= mean input_lengths = torch as_tensor input_lengths dtype=torch long target_lengths = torch as_tensor target_lengths dtype=torch long dt = log_probs dtype log_probs = log_probs double we need accuracy we logspace targets = targets long cum_target_lengths = target_lengths cumsum losses = i range log_probs size input_length = input_lengths i item target_length = target_lengths i item cum_target_length = cum_target_lengths i item targets_prime = targets new_full target_length + blank targets dim == targets_prime = targets i target_length targets_prime = targets cum_target_length - target_length cum_target_length probs = log_probs input_length i exp alpha = log_probs new_zeros target_length + alpha = probs blank alpha = probs targets_prime mask_third = targets_prime - = targets_prime t range input_length alpha_next = alpha clone alpha_next += alpha - alpha_next += torch where mask_third alpha - alpha new_zeros alpha = probs t targets_prime alpha_next losses append -alpha - sum log None output = torch cat losses reduction == mean output = output target_lengths dtype=output dtype device=output device mean reduction == sum output = output sum output = output dt output loss_reference_fns dict str Callable = KLDivLoss kldivloss_reference KLDivLoss_log_target partial kldivloss_reference log_target=True NLLLoss nllloss_reference NLLLossNd nlllossNd_reference SmoothL Loss smoothl loss_reference HuberLoss huberloss_reference MultiLabelMarginLoss multilabelmarginloss_reference HingeEmbeddingLoss hingeembeddingloss_reference SoftMarginLoss softmarginloss_reference MultiMarginLoss multimarginloss_reference CosineEmbeddingLoss cosineembeddingloss_reference TripletMarginLoss tripletmarginloss_reference MarginRankingLoss marginrankingloss_reference CTCLoss ctcloss_reference CrossEntropyLoss cross_entropy_loss_reference criterion_tests = single_batch_reference_criterion_fn args Reference function criterion supporting no batch dimensions The criterion passed input target batched form single item The output squeezed compare no-batch input criterion = args - unsqueeze_inp inp isinstance inp list tuple t unsqueeze t inp inp unsqueeze flatten xs result = isinstance xs list tuple x xs result extend flatten x result append xs result single_batch_input_args = flatten unsqueeze_inp input input args - output = criterion single_batch_input_args reduction = get_reduction criterion reduction == none output squeeze reduction sum mean which results scalar output Check regression criterion work no batch dimensions regression_criterion_no_batch = L Loss MSELoss PoissonNLLLoss HuberLoss SmoothL Loss reductions = none mean sum name reduction product regression_criterion_no_batch reductions regression_test_info = dict fullname=f name _no_batch_dim_ reduction constructor=lambda args name=name getattr nn name reduction=reduction input_size= target_size= reference_fn=single_batch_reference_criterion_fn test_cpp_api_parity=False default_dtype=torch double criterion_tests append regression_test_info reduction reductions regression_test_info = dict fullname=f KLDivLoss_no_batch_dim_ reduction constructor=lambda nn KLDivLoss reduction=reduction input_fn=lambda torch rand log target_fn=lambda torch rand reference_fn=single_batch_reference_criterion_fn test_cpp_api_parity=False default_dtype=torch double criterion_tests append regression_test_info Check classification criterion work no batch dimensions List tuples name input_fn target_fn classification_criterion_no_batch = BCELoss lambda torch sigmoid torch randn dtype=torch double lambda torch randn dtype=torch double gt torch double BCEWithLogitsLoss lambda torch randn dtype=torch double lambda torch randn dtype=torch double HingeEmbeddingLoss lambda torch randn dtype=torch double lambda torch tensor - MultiLabelMarginLoss lambda torch randn dtype=torch double lambda torch tensor - SoftMarginLoss lambda torch randn dtype=torch double lambda torch tensor - NLLLoss lambda F log_softmax torch randn dtype=torch double dim= lambda torch tensor CosineEmbeddingLoss lambda torch randn dtype=torch double torch randn dtype=torch double lambda torch tensor dtype=torch double For MarginRankingLoss input_fn x x target_fn target MarginRankingLoss lambda torch randn torch randn lambda torch randn sign For TripletMarginLoss input_fn anchor positive target_fn negative TripletMarginLoss lambda torch randn dtype=torch double torch randn dtype=torch double lambda torch randn dtype=torch double MultiLabelSoftMarginLoss lambda torch randn dtype=torch double lambda torch randn classification_criterion_no_batch_extra_info dict str dict = MultiLabelMarginLoss check_gradgrad False TODO Fix these discrepancies classification_cpp_parity = BCELoss False BCEWithLogitsLoss False HingeEmbeddingLoss False NLLLoss False SoftMarginLoss False reductions = none mean sum name input_fn target_fn reduction product classification_criterion_no_batch reductions classification_test_info = dict fullname=f name _no_batch_dim_ reduction constructor=lambda args name=name getattr nn name reduction=reduction input_fn=lambda f=input_fn f target_fn=lambda f=target_fn f reference_fn=single_batch_reference_criterion_fn test_cpp_api_parity=True has_parity=classification_cpp_parity get name True extra_info = classification_criterion_no_batch_extra_info get name classification_test_info update extra_info criterion_tests append classification_test_info NNTestCase TestCase _forward defined classes inheriting NNTestCase abstractmethod _forward args kwargs raise NotImplementedError abstractmethod _get_parameters module nn Module - tuple list nn Parameter list nn Parameter raise NotImplementedError abstractmethod _zero_grad_parameters module nn Module - None raise NotImplementedError abstractmethod _backward module nn Module input _TensorOrTensors output torch Tensor grad_output Union torch Tensor Sequence torch Tensor create_graph bool = False raise NotImplementedError _jacobian input num_out isinstance input tuple tuple _jacobian elem num_out elem input isinstance input list _jacobian elem num_out elem input torch zeros input nelement num_out _flatten_tensors x isinstance x torch Tensor x is_sparse x to_dense view - x view - tuple _flatten_tensors x _zero_grad_input input isinstance input torch Tensor input requires_grad input grad None input grad zero_ input grad detach_ i input _zero_grad_input i _analytical_jacobian module input _TensorOrTensors jacobian_input=True jacobian_parameters=True output = _forward module input output_size = output nelement jacobian_input jacobian_inp = _jacobian input output_size flat_jacobian_input = list _iter_tensors jacobian_inp jacobian_parameters num_param = sum p numel p _get_parameters module jacobian_param = torch zeros num_param output_size i range output_size param d_param = _get_parameters module make non grad zeros d_param = torch zeros_like p d None d p d zip param d_param strict=True d_out = torch zeros_like output flat_d_out = d_out view - flat_d_out i = jacobian_parameters _zero_grad_parameters module Tensors will accumulate gradient multiple steps jacobian_input _zero_grad_input input d_input = _backward module input output d_out jacobian_input jacobian_x d_x zip flat_jacobian_input _iter_tensors d_input strict=True jacobian_x i = d_x contiguous view - jacobian_parameters jacobian_param i = torch cat _flatten_tensors d_param res tuple torch Tensor = jacobian_input res += jacobian_inp jacobian_parameters res += jacobian_param res _numerical_jacobian module input _TensorOrTensors jacobian_input=True jacobian_parameters=True fw input _forward module input detach res tuple torch Tensor = jacobian_input res += _get_numerical_jacobian fw input eps= e- jacobian_parameters param _ = _get_parameters module to_cat = p param jacobian = _get_numerical_jacobian fw input target=p eps= e- get_numerical_jacobian returns list tuples we require tensor to_cat append jacobian res += torch cat to_cat res check_jacobian module input _TensorOrTensors jacobian_input=True jacobian_parameters = bool _get_parameters module analytical = _analytical_jacobian module input jacobian_input jacobian_parameters numerical = _numerical_jacobian module input jacobian_input jacobian_parameters analytical_t = list _iter_tensors analytical numerical_t = list _iter_tensors numerical differences = n zip analytical_t numerical_t strict=True numel = differences append add n alpha=- abs max TODO compare structure ensure analytic jacobian has correct shape len differences assertLessEqual max differences PRECISION type ignore type-var TestBase _required_arg_names = constructor_args input extra_args __init__ constructor desc= reference_fn=None fullname=None kwargs desc = desc fullname = fullname constructor = constructor reference_fn = reference_fn name _required_arg_names name kwargs name + _fn kwargs name + _size kwargs name constructor_args extra_args kwargs name = raise ValueError f get_name Specify name value function generate s size _extra_kwargs = kwargs _arg_cache = get_name fullname None test_ + fullname test_name = test_ + constructor __name__ desc test_name += _ + desc test_name _unpack value isinstance value torch Tensor value is_iterable value type value _unpack v v value value property constructor_args _get_arg constructor_args True property extra_args _get_arg extra_args True _get_arg name unpack assert name _required_arg_names name _arg_cache fn_name = name + _fn size_name = name + _size name _extra_kwargs _arg_cache name = _extra_kwargs name fn_name _extra_kwargs _arg_cache name = _extra_kwargs fn_name assert size_name _extra_kwargs \ f Missing ` name ` ` size_name ` ` fn_name ` get_name map_tensor_sizes sizes isinstance sizes list map_tensor_sizes s s sizes isinstance sizes torch Tensor sizes double torch randn sizes _arg_cache name = map_tensor_sizes _extra_kwargs size_name _unpack _arg_cache name unpack _arg_cache name _get_input unpack=True _get_arg input unpack __call__ test_case raise NotImplementedError ModuleTest TestBase abstractmethod _do_test test_case Any module nn Module input Any - Any raise NotImplementedError __init__ args kwargs super __init__ args kwargs jacobian_input = kwargs get jacobian_input True should_test_cuda = kwargs get test_cuda True should_test_pickle = kwargs get pickle True check_gradgrad = kwargs get check_gradgrad True FIXME_no_cuda_gradgrad_comparison = \ kwargs get FIXME_no_cuda_gradgrad_comparison False precision = kwargs get precision e- check_forward_only = kwargs get check_forward_only False default_dtype = kwargs get default_dtype default_dtype None default_dtype = torch get_default_dtype __call__ test_case set_default_dtype default_dtype module = constructor constructor_args input = _get_input reference_fn None out = test_case _forward module input ref_input = deepcopy input ref_module = deepcopy module expected_out = reference_fn ref_input test_case _get_parameters module ref_module test_case assertEqual out expected_out exact_dtype=False check_forward_only test_noncontig test_case module input should_test_pickle TODO do in-memory files soon torch save will support tempfile TemporaryFile f test_case _forward module input torch save module f f seek weights_only=False legacy code saves model module_copy = torch load f weights_only=False test_case assertEqual test_case _forward module input test_case _forward module_copy input _do_test test_case module input noncontiguize obj isinstance obj list noncontiguize o o obj isinstance obj tuple tuple noncontiguize o o obj tensor = obj ndim = tensor dim Always making only last dimension noncontiguous easy hide bugs because view - will still work So try find dim size make non-contiguous i e stack + select dimension directly after dim = ndim d range ndim tensor size d dim = d + break noncontig = torch stack torch empty_like tensor tensor dim select dim detach assert noncontig numel == noncontig numel == noncontig is_contiguous noncontig requires_grad = tensor requires_grad noncontig test_noncontig test_case module input check no scalars can t make non-contig isinstance input torch Tensor input dim == any i dim == i input isinstance i torch Tensor test_case _zero_grad_parameters module test_case _zero_grad_input input freeze_rng_state output = test_case _forward module input getattr module return_indices False output = output grad_output = output new output shape normal_ output = output clone d_input = deepcopy test_case _backward module input output grad_output d_param = deepcopy test_case _get_parameters module nc_input = noncontiguize input nc_grad_output = noncontiguize grad_output contig_i contig_g product True False repeat= i = input contig_i nc_input Some ops e g nn Flatten gradient shares storage grad_output Hence we copy here go = deepcopy grad_output contig_g nc_grad_output test_case _zero_grad_parameters module test_case _zero_grad_input i freeze_rng_state out = test_case _forward module i getattr module return_indices False out = out grad = test_case _backward module i out go test_case assertEqual out output test_case assertEqual grad d_input atol= e- rtol= test_case assertEqual test_case _get_parameters module d_param test_cuda test_case TEST_CUDA should_test_cuda raise unittest SkipTest Excluded CUDA tests set_default_dtype default_dtype cpu_input = _get_input type_map = torch double torch float cpu_input_tuple = cpu_input isinstance cpu_input tuple cpu_input is_any_input_complex = any isinstance t torch Tensor t dtype is_complex t cpu_input_tuple gpu_input_tuple = to_gpu cpu_input_tuple type_map=type_map cpu_module = constructor constructor_args gpu_module = constructor constructor_args float cuda cpu_param = test_case _get_parameters cpu_module gpu_param = test_case _get_parameters gpu_module cpu_p gpu_p zip cpu_param gpu_param strict=True gpu_p data copy_ cpu_p test_case _zero_grad_input cpu_input_tuple test_case _zero_grad_input gpu_input_tuple test_case _zero_grad_parameters cpu_module test_case _zero_grad_parameters gpu_module cpu_output = test_case _forward cpu_module cpu_input_tuple gpu_output = test_case _forward gpu_module gpu_input_tuple getattr cpu_module return_indices False cpu_output = cpu_output gpu_output = gpu_output test_case assertEqual cpu_output gpu_output atol=self precision rtol= exact_dtype=False Run backwards CPU GPU compare results _ range cpu_gradOutput = cpu_output clone normal_ gpu_gradOutput = cpu_gradOutput type_as gpu_output cpu_gradInput = test_case _backward cpu_module cpu_input_tuple cpu_output cpu_gradOutput gpu_gradInput = test_case _backward gpu_module gpu_input_tuple gpu_output gpu_gradOutput test_case assertEqual cpu_gradInput gpu_gradInput atol=self precision rtol= exact_dtype=False cpu_d_p gpu_d_p zip cpu_param gpu_param strict=True test_case assertEqual cpu_d_p gpu_d_p atol=self precision rtol= Run double-backwards CPU GPU compare results check_gradgrad FIXME_no_cuda_gradgrad_comparison cpu_output = cpu_module cpu_input_tuple gpu_output = gpu_module gpu_input_tuple getattr cpu_module return_indices False cpu_output = cpu_output gpu_output = gpu_output cpu_gradOutput = torch randn_like cpu_output requires_grad=True gpu_gradOutput = cpu_gradOutput type_as gpu_output detach gpu_gradOutput requires_grad = True cpu_gradInputs = torch autograd grad cpu_output cpu_input_tuple + tuple cpu_module parameters cpu_gradOutput create_graph=True gpu_gradInputs = torch autograd grad gpu_output gpu_input_tuple + tuple gpu_module parameters gpu_gradOutput create_graph=True cpu_d_i gpu_d_i zip cpu_gradInputs gpu_gradInputs strict=True test_case assertEqual cpu_d_i gpu_d_i atol=self precision rtol= exact_dtype=False We mix output into second backwards computation so torch autograd grad doesn t complain some inputs unreachable which can happen you differentiate only gradient is_any_input_complex outputs_cpu = cpu_output sum abs + sum x sum abs x cpu_gradInputs outputs_gpu = gpu_output sum abs + sum x sum abs x gpu_gradInputs outputs_cpu = cpu_output sum + sum x sum x cpu_gradInputs outputs_gpu = gpu_output sum + sum x sum x gpu_gradInputs cpu_gg = torch autograd grad outputs_cpu cpu_input_tuple + cpu_gradOutput + tuple cpu_module parameters retain_graph=True gpu_gg = torch autograd grad outputs_gpu gpu_input_tuple + gpu_gradOutput + tuple gpu_module parameters retain_graph=True test_case assertEqual cpu_gradInput gpu_gradInput atol=self precision rtol= exact_dtype=False cpu_d_p gpu_d_p zip cpu_gg gpu_gg strict=True test_case assertEqual cpu_d_p gpu_d_p atol=self precision rtol= exact_dtype=False test_noncontig test_case gpu_module gpu_input_tuple InputVariableMixin _get_input input = TestBase _get_input False type ignore arg-type map_variables i isinstance i torch Tensor i is_floating_point i is_complex i requires_grad = True i type i map_variables elem elem i map_variables input NewModuleTest InputVariableMixin ModuleTest type ignore misc __init__ args kwargs super __init__ args kwargs cudnn = kwargs get cudnn False check_inplace = kwargs get check_inplace False check_gradgrad = kwargs get check_gradgrad True skip_double = kwargs get skip_double False skip_half = kwargs get skip_half False with_tf = kwargs get with_tf False tf _precision = kwargs get tf _precision test_cpu = kwargs get test_cpu True has_sparse_gradients = kwargs get has_sparse_gradients False check_batched_grad = kwargs get check_batched_grad True gradcheck_fast_mode = kwargs get gradcheck_fast_mode supports_forward_ad = kwargs get supports_forward_ad False supports_fwgrad_bwgrad = kwargs get supports_fwgrad_bwgrad False _check_gradients test_case module input_tuple params = tuple x x module parameters num_inputs = len input_tuple fn_to_gradcheck inputs_and_params kwargs assert kwargs test_case _forward module inputs_and_params num_inputs gradcheck doesn t support operators take dense inputs sparse parameters This only happens case nn Embedding nn EmbeddingBag Instead we call ` check_jacobian ` which slightly different version gradcheck can handle has_sparse_gradients assert num_inputs == test_input_jacobian = torch is_floating_point input_tuple test_case check_jacobian module input_tuple test_input_jacobian test_case assertTrue gradcheck fn_to_gradcheck input_tuple + params check_batched_grad=self check_batched_grad fast_mode=self gradcheck_fast_mode check_forward_ad=self supports_forward_ad check_gradgrad test_case assertTrue gradgradcheck fn_to_gradcheck input_tuple + params check_batched_grad=self check_batched_grad fast_mode=self gradcheck_fast_mode check_fwd_over_rev=self supports_fwgrad_bwgrad _do_test test_case module input num_threads = torch get_num_threads torch set_num_threads input_tuple = input isinstance input tuple input _check_gradients test_case module input_tuple check module can printed module __repr__ check_inplace check inplace variant module gives same result out-of-place check_inplace doesn t support multiple input tensors since we don t have any modules modify inputs in-place accept more than one input assert len input_tuple == input = input_tuple module_ip = constructor constructor_args inplace=True input_version = input _version freeze_rng_state output = module input test_case assertEqual input _version input_version input_ip = deepcopy input input_ip_clone = input_ip clone freeze_rng_state output_ip = module_ip input_ip_clone test_case assertNotEqual input_ip_clone _version input_version test_case assertEqual output output_ip grad = output data clone normal_ input grad None torch no_grad input grad zero_ input_ip grad None torch no_grad input_ip grad zero_ output backward grad output_ip backward grad test_case assertEqual input grad input_ip grad assert_module_parameters_are tensor_type device_id=None p module parameters test_case assertIsInstance p tensor_type device_id None test_case assertEqual p get_device device_id all isinstance t torch LongTensor t input_tuple TEST_CUDA check cuda moves module parameters correct GPU device float casts parameters correctly input_tuple = tuple t cuda t input_tuple module float cuda module input_tuple assert_module_parameters_are torch cuda FloatTensor type ignore attr-defined torch cuda device_count input_tuple = tuple t cuda t input_tuple module cuda torch cuda device module input_tuple assert_module_parameters_are torch cuda FloatTensor type ignore attr-defined check float double casters work correctly to_type tensor real complex tensor is_complex tensor complex tensor is_floating_point tensor real tensor to_half x TODO torch complex when properly supported to_type x torch float None to_single x to_type x torch float torch complex to_double x to_type x torch float torch complex float input_tuple = tuple to_single t t input_tuple module float module input_tuple assert_module_parameters_are torch FloatTensor back double input_tuple = tuple to_double t t input_tuple module double module input_tuple assert_module_parameters_are torch DoubleTensor TEST_CUDA should_test_cuda check cuda moves module parameters correct GPU device float casts parameters correctly GPU input_tuple = tuple to_single t cuda t input_tuple module float cuda module input_tuple assert_module_parameters_are torch cuda FloatTensor type ignore attr-defined CPU input_tuple = tuple t cpu t input_tuple module cpu module input_tuple assert_module_parameters_are torch FloatTensor back GPU input_tuple = tuple t cuda t input_tuple module cuda module input_tuple assert_module_parameters_are torch cuda FloatTensor type ignore attr-defined test forwards module runs correctly without cuDNN cudnn torch backends cudnn flags enabled=False module input_tuple assert_module_parameters_are torch cuda FloatTensor type ignore attr-defined torch cuda device_count = test cross-GPU transfer works GPU input_tuple = tuple t cuda t input_tuple module cuda torch cuda device module input_tuple assert_module_parameters_are torch cuda FloatTensor type ignore attr-defined skip_double test double input_tuple = tuple to_double t cuda t input_tuple module double cuda module input_tuple assert_module_parameters_are torch cuda DoubleTensor type ignore attr-defined test half skip_half input_tuple = tuple to_half t cuda t input_tuple module half cuda module input_tuple assert_module_parameters_are torch cuda HalfTensor type ignore attr-defined torch set_num_threads num_threads _get_target _get_arg target False property constructor_args _get_arg constructor_args False CriterionTest InputVariableMixin TestBase type ignore misc TODO check criterions don t ignore grad_output _required_arg_names = TestBase _required_arg_names union target __init__ args kwargs super __init__ args kwargs should_test_cuda = kwargs get test_cuda True check_forward_only = kwargs get check_forward_only False check_gradgrad = kwargs get check_gradgrad True check_half = kwargs get check_half True check_bfloat = kwargs get check_bfloat False check_complex = kwargs get check_complex False test_cpu = kwargs get test_cpu True with_tf = kwargs get with_tf True tf _precision = kwargs get tf _precision check_batched_grad = kwargs get check_batched_grad True default_dtype = kwargs get default_dtype default_dtype None default_dtype = torch get_default_dtype __call__ test_case set_default_dtype default_dtype module = constructor constructor_args input = _get_input Check these methods don t raise errors module __repr__ str module target = _get_target reference_fn None out = test_case _forward_criterion module input target extra_args=self extra_args ref_args = deepcopy input deepcopy target + extra_args + module expected_out = reference_fn ref_args test_case assertEqual out expected_out check_forward_only params = tuple x x module parameters isinstance input tuple inputs = input + params + target apply_fn input target params module input target inputs = input + params + target apply_fn input input target params type ignore misc module input input target gradcheck apply_fn inputs check_batched_grad=self check_batched_grad check_gradgrad gradgradcheck apply_fn inputs check_batched_grad=self check_batched_grad test_cuda test_case dtype extra_args=None convert_dtype obj dtype requires_grad=False isinstance obj torch Tensor obj detach dtype=dtype requires_grad_ requires_grad isinstance obj tuple tuple convert_dtype o dtype requires_grad o obj obj TEST_CUDA should_test_cuda raise unittest SkipTest Excluded CUDA tests set_default_dtype default_dtype cpu_input = _get_input cpu_target = _get_target cpu_module = constructor constructor_args gpu_module = constructor constructor_args Convert input target module parameters dtype cpu_input = convert_dtype cpu_input dtype True cpu_target is_floating_point cpu_target is_complex cpu_target = convert_dtype cpu_target dtype cpu_module type dtype gpu_module type dtype GPU setup gpu_input = to_gpu cpu_input gpu_target = to_gpu cpu_target gpu_module cuda torch HalfTensor doesn t support most operations converting back default dtype torch half torch bfloat cpu_input = _get_input cpu_target = _get_target Loss modules weights require consistent input module weight types cpu_module = constructor constructor_args cpu_output = test_case _forward_criterion cpu_module cpu_input cpu_target extra_args=extra_args gpu_output = test_case _forward_criterion gpu_module gpu_input gpu_target extra_args=extra_args dtype used able None so set precision way instead precision map test_case assertEqual cpu_output gpu_output atol= e- dtype torch half torch bfloat e- rtol= exact_dtype=False cpu_gradInput = test_case _backward_criterion cpu_module cpu_input cpu_output cpu_target extra_args=extra_args gpu_gradInput = test_case _backward_criterion gpu_module gpu_input gpu_output gpu_target extra_args=extra_args dtype used able None so set precision way instead precision map test_case assertEqual cpu_gradInput gpu_gradInput atol= e- dtype torch half torch bfloat e- rtol= exact_dtype=False _get_target _get_arg target False property constructor_args _get_arg constructor_args False property extra_args _get_arg extra_args False _test_bfloat _ops test_case op device inp_dims= prec= e- scale_factor=None fp compute input = torch randn inp_dims dtype=torch float device=device requires_grad=True scale_factor None input = torch rand inp_dims dtype=torch bfloat device=device scale_factor float requires_grad_ out = op input grad_input = torch randn_like out device=device out backward grad_input bfloat compute op_bfp = op bfloat input = input detach bfloat requires_grad_ grad_input = grad_input bfloat out = op_bfp input out backward grad_input test_case assertEqual out out atol=prec rtol=prec exact_dtype=False test_case assertEqual input grad data input grad data atol=prec rtol=prec exact_dtype=False _test_module_empty_input test_case module inp check_size=True inference=False inference inp requires_grad_ True out = module inp inference gO = torch rand_like out out backward gO check_size test_case assertEqual out size inp size inference p module parameters p requires_grad test_case assertEqual p grad torch zeros_like p grad test_case assertEqual inp grad torch zeros_like inp _create_basic_net Layer nn Module __init__ - None super __init__ layer_dummy_param = nn Parameter torch empty layer_dummy_buf = nn Buffer torch zeros Net nn Module __init__ - None super __init__ l = Layer dummy_param = nn Parameter torch empty dummy_buf = nn Buffer torch zeros l = Layer n = Net s = nn Sequential n n l n s