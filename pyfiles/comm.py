mypy allow-untyped-defs warnings torch torch _utils _flatten_dense_tensors _get_device_index _handle_complex _reorder_tensors_as _take_tensors _unflatten_dense_tensors torch cuda nccl broadcast tensor devices=None out=None r Broadcasts tensor specified GPU devices Args tensor Tensor tensor broadcast Can CPU GPU devices Iterable torch device str int optional iterable GPU devices among which broadcast out Sequence Tensor optional keyword-only GPU tensors store output results note Exactly one attr ` devices ` attr ` out ` must specified Returns - If attr ` devices ` specified tuple containing copies attr ` tensor ` placed attr ` devices ` - If attr ` out ` specified tuple containing attr ` out ` tensors each containing copy attr ` tensor ` tensor = _handle_complex tensor devices None ^ out None raise RuntimeError f Exactly one devices out must specified got devices= devices out= out devices None devices = _get_device_index d d devices torch _C _broadcast tensor devices pyrefly ignore bad-argument-type torch _C _broadcast_out tensor out broadcast_coalesced tensors devices buffer_size= Broadcast sequence tensors specified GPUs Small tensors first coalesced into buffer reduce number synchronizations Args tensors sequence tensors broadcast Must same device either CPU GPU devices Iterable torch device str int iterable GPU devices among which broadcast buffer_size int maximum size buffer used coalescing Returns A tuple containing copies attr ` tensor ` placed attr ` devices ` devices = _get_device_index d d devices tensors = _handle_complex t t tensors torch _C _broadcast_coalesced tensors devices buffer_size reduce_add inputs destination=None Sum tensors multiple GPUs All inputs should have matching shapes dtype layout The output tensor will same shape dtype layout Args inputs Iterable Tensor iterable tensors add destination int optional device which output will placed default current device Returns A tensor containing elementwise sum all inputs placed attr ` destination ` device destination = _get_device_index destination optional=True input_size = inputs size root_index = None index input tensor already correct device i inp enumerate inputs assert inp device type = cpu reduce_add expects all inputs GPUs inp get_device == destination root_index = i inp size = input_size got = x join str x x inp size expected = x join str x x input_size raise ValueError f input i has invalid size got got expected expected root_index None raise RuntimeError reduce_add expects destination same GPU one tensors len inputs == inputs nccl is_available inputs result = torch empty_like inputs root_index nccl reduce inputs output=result root=root_index destination_device = torch device inputs root_index device type destination nonroot = t i t enumerate inputs i = root_index make new tensor w o clone result = inputs root_index + nonroot device=destination_device non_blocking=True other nonroot result add_ other device=destination_device non_blocking=True result reduce_add_coalesced inputs destination=None buffer_size= Sum tensors multiple GPUs Small tensors first coalesced into buffer reduce number synchronizations Args inputs Iterable Iterable Tensor iterable iterables contain tensors single device destination int optional device which output will placed default current device buffer_size int maximum size buffer used coalescing Returns A tuple tensors containing elementwise sum each group inputs placed ` ` destination ` ` device TODO When ` len inputs == ` all inputs ` destination ` just ` inputs ` dense_tensors list list = _ inputs shape num_gpus num_tensors output = ref_order = process sparse ones first since they may have different sizes different gpus tensor_at_gpus zip inputs strict=True all t is_sparse t tensor_at_gpus result = reduce_add tensor_at_gpus destination will sparse too output append result ref_order append tensor_at_gpus coll t zip dense_tensors tensor_at_gpus strict=True coll append t to_dense t is_sparse t ref_order append dense_tensors - itrs = _take_tensors tensors buffer_size tensors dense_tensors now dense ones which have consistent sizes chunks zip itrs strict=True flat_tensors = _flatten_dense_tensors chunk chunk chunks num_gpus flat_result = reduce_add flat_tensors destination t _unflatten_dense_tensors flat_result chunks The unflattened tensors do share storage we don t expose base flat tensor anyways so give them different version counters See NOTE Version Counter comm _coalesced output append t data tuple _reorder_tensors_as output ref_order scatter tensor devices=None chunk_sizes=None dim= streams=None out=None Scatters tensor across multiple GPUs Args tensor Tensor tensor scatter Can CPU GPU devices Iterable torch device str int optional iterable GPU devices among which scatter chunk_sizes Iterable int optional sizes chunks placed each device It should match attr ` devices ` length sums ` ` tensor size dim ` ` If specified attr ` tensor ` will divided into equal chunks dim int optional A dimension along which chunk attr ` tensor ` Default ` ` ` ` streams Iterable torch cuda Stream optional iterable Streams among which execute scatter If specified default stream will utilized out Sequence Tensor optional keyword-only GPU tensors store output results Sizes these tensors must match attr ` tensor ` except attr ` dim ` where total size must sum ` ` tensor size dim ` ` note Exactly one attr ` devices ` attr ` out ` must specified When attr ` out ` specified attr ` chunk_sizes ` must specified will inferred sizes attr ` out ` Returns - If attr ` devices ` specified tuple containing chunks attr ` tensor ` placed attr ` devices ` - If attr ` out ` specified tuple containing attr ` out ` tensors each containing chunk attr ` tensor ` tensor = _handle_complex tensor out None pyrefly ignore not-iterable devices = _get_device_index d d devices tuple torch _C _scatter tensor devices chunk_sizes dim streams devices None raise RuntimeError f devices must specified when out specified got devices= devices chunk_sizes None raise RuntimeError f chunk_sizes must specified when out specified got chunk_sizes= chunk_sizes tuple torch _C _scatter_out tensor out dim streams gather tensors dim= destination=None out=None r Gathers tensors multiple GPU devices Args tensors Iterable Tensor iterable tensors gather Tensor sizes all dimensions other than attr ` dim ` have match dim int optional dimension along which tensors will concatenated Default ` ` ` ` destination torch device str int optional output device Can CPU CUDA Default current CUDA device out Tensor optional keyword-only tensor store gather result Its sizes must match those attr ` tensors ` except attr ` dim ` where size must equal ` ` sum tensor size dim tensor tensors ` ` Can CPU CUDA note attr ` destination ` must specified when attr ` out ` specified Returns - If attr ` destination ` specified tensor located attr ` destination ` device result concatenating attr ` tensors ` along attr ` dim ` - If attr ` out ` specified attr ` out ` tensor now containing results concatenating attr ` tensors ` along attr ` dim ` tensors = _handle_complex t t tensors out None destination == - warnings warn Using - represent CPU tensor deprecated Please use device object string instead e g cpu FutureWarning stacklevel= destination = _get_device_index destination allow_cpu=True optional=True torch _C _gather tensors dim destination destination None raise RuntimeError f destination must specified when out specified got destination= destination torch _C _gather_out tensors out dim