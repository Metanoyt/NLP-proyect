Owner s oncall mobile inspect io tempfile TemporaryFileName torch torch utils bundled_inputs torch jit mobile _export_operator_list _load_for_lite_interpreter torch testing FileCheck torch testing _internal common_quantization AnnotatedNestedModel AnnotatedSingleLayerLinearModel QuantizationLiteTestCase TwoLayerLinearModel torch testing _internal common_utils run_tests TestCase TestLiteScriptModule TestCase getScriptExportImportCopy m save_mobile_debug_info=True also_test_file=False m_scripted = torch jit script m also_test_file buffer = io BytesIO m_scripted _save_to_buffer_for_lite_interpreter _save_mobile_debug_info=save_mobile_debug_info buffer seek mobile_module = _load_for_lite_interpreter buffer mobile_module TemporaryFileName fname m_scripted _save_for_lite_interpreter fname _save_mobile_debug_info=save_mobile_debug_info mobile_module = _load_for_lite_interpreter fname mobile_module test_load_mobile_module MyTestModule torch nn Module forward x x + input = torch tensor script_module = torch jit script MyTestModule script_module_result = script_module input buffer = io BytesIO script_module _save_to_buffer_for_lite_interpreter buffer seek mobile_module = _load_for_lite_interpreter buffer mobile_module_result = mobile_module input torch testing assert_close script_module_result mobile_module_result mobile_module_forward_result = mobile_module forward input torch testing assert_close script_module_result mobile_module_forward_result mobile_module_run_method_result = mobile_module run_method forward input torch testing assert_close script_module_result mobile_module_run_method_result test_save_mobile_module_with_debug_info_with_trace A torch nn Module forward x y x y B torch nn Module __init__ - None super __init__ A = A A = A forward x y z A x y + A y z export_method trace script x = torch rand y = torch rand z = torch rand export_method == trace trace_module = torch jit trace B x y z trace_module = torch jit script B exported_module = trace_module _save_to_buffer_for_lite_interpreter _save_mobile_debug_info=True buffer = io BytesIO exported_module buffer seek assert b callstack_debug_map pkl exported_module mobile_module = _load_for_lite_interpreter buffer assertRaisesRegex RuntimeError r Module hierarchy top\ B\ unknown A \ A\ forward aten mul x = torch rand y = torch rand z = torch rand mobile_module x y z assertRaisesRegex RuntimeError r Module hierarchy top\ B\ unknown A \ A\ forward aten mul x = torch rand y = torch rand z = torch rand mobile_module x y z test_load_mobile_module_with_debug_info MyTestModule torch nn Module forward x x + input = torch tensor script_module = torch jit script MyTestModule script_module_result = script_module input buffer = io BytesIO script_module _save_to_buffer_for_lite_interpreter _save_mobile_debug_info=True buffer seek mobile_module = _load_for_lite_interpreter buffer mobile_module_result = mobile_module input torch testing assert_close script_module_result mobile_module_result mobile_module_forward_result = mobile_module forward input torch testing assert_close script_module_result mobile_module_forward_result mobile_module_run_method_result = mobile_module run_method forward input torch testing assert_close script_module_result mobile_module_run_method_result test_find_and_run_method MyTestModule torch nn Module forward arg arg input = torch tensor script_module = torch jit script MyTestModule script_module_result = script_module input buffer = io BytesIO script_module _save_to_buffer_for_lite_interpreter buffer seek mobile_module = _load_for_lite_interpreter buffer has_bundled_inputs = mobile_module find_method get_all_bundled_inputs assertFalse has_bundled_inputs torch utils bundled_inputs augment_model_with_bundled_inputs script_module input buffer = io BytesIO script_module _save_to_buffer_for_lite_interpreter buffer seek mobile_module = _load_for_lite_interpreter buffer has_bundled_inputs = mobile_module find_method get_all_bundled_inputs assertTrue has_bundled_inputs bundled_inputs = mobile_module run_method get_all_bundled_inputs mobile_module_result = mobile_module forward bundled_inputs torch testing assert_close script_module_result mobile_module_result test_method_calls_with_optional_arg A torch nn Module __init__ - None super __init__ opt arg script-to-script invocation forward x two int = x + two B torch nn Module __init__ - None super __init__ A = A opt arg Python-to-script invocation forward x one int = A x + one script_module = torch jit script B buffer = io BytesIO script_module _save_to_buffer_for_lite_interpreter mobile_module = _load_for_lite_interpreter buffer input = torch tensor script_module_forward_result = script_module forward input mobile_module_forward_result = mobile_module forward input torch testing assert_close script_module_forward_result mobile_module_forward_result change ref only script_module_forward_result = script_module forward input assertFalse script_module_forward_result == mobile_module_forward_result all item now both match again mobile_module_forward_result = mobile_module forward input torch testing assert_close script_module_forward_result mobile_module_forward_result test_unsupported_classtype Foo __init__ - None func x int y int x + y MyTestModule torch nn Module forward arg f = Foo f func script_module = torch jit script MyTestModule assertRaisesRegex RuntimeError r Workaround instead using arbitrary type \ Foo\ \ \ r define pytorch \ Foo\ torch\ nn\ Module\ \ \ r The problematic type script_module _save_to_buffer_for_lite_interpreter test_unsupported_return_list_with_module_class Foo torch nn Module pass MyTestModuleForListWithModuleClass torch nn Module __init__ - None super __init__ foo = Foo forward my_list list Foo = foo my_list script_module = torch jit script MyTestModuleForListWithModuleClass assertRaisesRegex RuntimeError r ^Returning list dictionary pytorch type r supported mobile module r \ List\ Foo\ Dict\ int\ Foo\ Foo\ torch\ nn\ Module\ \ \ r Workaround\ instead using pytorch their element type\ r use combination list\ dictionary\ single types\ $ script_module _save_to_buffer_for_lite_interpreter test_unsupported_return_dict_with_module_class Foo torch nn Module pass MyTestModuleForDictWithModuleClass torch nn Module __init__ - None super __init__ foo = Foo forward my_dict dict int Foo = foo my_dict script_module = torch jit script MyTestModuleForDictWithModuleClass assertRaisesRegex RuntimeError r ^Returning list dictionary pytorch type r supported mobile module r \ List\ Foo\ Dict\ int\ Foo\ Foo\ torch\ nn\ Module\ \ \ r Workaround\ instead using pytorch their element type\ r use combination list\ dictionary\ single types\ $ script_module _save_to_buffer_for_lite_interpreter test_module_export_operator_list Foo torch nn Module __init__ - None super __init__ weight = torch ones bias = torch ones forward input x = torch zeros x = torch empty_like torch empty x = torch _convolution input weight bias False False False True True x x x m = torch jit script Foo buffer = io BytesIO m _save_to_buffer_for_lite_interpreter buffer seek mobile_module = _load_for_lite_interpreter buffer expected_ops = aten _convolution aten empty memory_format aten empty_like aten zeros actual_ops = _export_operator_list mobile_module assertEqual actual_ops expected_ops test_source_range_simple FooTest torch jit ScriptModule torch jit script_method forward x w torch mm x w t ft = FooTest loaded = getScriptExportImportCopy ft _ lineno = inspect getsourcelines FooTest assertRaisesRegex RuntimeError f test_lite_script_module py line lineno + loaded torch rand torch rand test_source_range_raise_exception FooTest torch jit ScriptModule torch jit script_method forward raise RuntimeError foo _ _ = inspect getsourcelines FooTest In C++ code type exception thrown torch jit JITException which does extend c Error hence isn t possible add additional context exception message preserve correct C++ stack trace symbolication i e isn t possible add debug handle string show where Python code exception occurred w o first changing torch jit JITException extend c Error assertRaisesRegex torch jit Error foo ft = FooTest loaded = getScriptExportImportCopy ft loaded test_source_range_function_call FooTest torch jit ScriptModule torch jit script_method add_method x w x + w torch jit script_method forward x y w x = x y x = x + add_method x w ft = FooTest loaded = getScriptExportImportCopy ft _ lineno = inspect getsourcelines FooTest try loaded torch rand torch rand torch rand except RuntimeError e error_message = f e assertTrue f test_lite_script_module py line lineno + error_message assertTrue f test_lite_script_module py line lineno + error_message assertTrue top FooTest error_message test_source_range_no_debug_info FooTest torch jit ScriptModule torch jit script_method forward x w torch mm x w t ft = FooTest loaded = getScriptExportImportCopy ft save_mobile_debug_info=False try loaded torch rand torch rand except RuntimeError e error_message = f e assertTrue test_lite_script_module py error_message test_source_range_raise_exc FooTest torch jit ScriptModule __init__ val int super __init__ val = val torch jit script_method add_method val int x w val == val raise RuntimeError val val same x + w torch jit script_method forward val int x y w x = x y x = x + add_method val x w ft = FooTest loaded = getScriptExportImportCopy ft _ _ = inspect getsourcelines FooTest try loaded torch rand torch rand torch rand except torch jit Error e error_message = f e In C++ code type exception thrown torch jit JITException which does extend c Error hence isn t possible add additional context exception message preserve correct C++ stack trace symbolication i e isn t possible add debug handle string show where Python code exception occurred w o first changing torch jit JITException extend c Error assertTrue val val same error_message test_stacktrace_interface_call torch jit interface Forward torch nn Module forward x - torch Tensor pass forwardError x - torch Tensor pass B torch nn Module forward x x forwardError x call + x call torch ones - A torch nn Module b Forward __init__ - None super __init__ b = B forward b forward torch ones b forwardError torch ones = torch jit script A torch _C _enable_mobile_interface_call_export buffer = io BytesIO _save_to_buffer_for_lite_interpreter _save_mobile_debug_info=True buffer seek mobile_module = _load_for_lite_interpreter buffer try mobile_module assertTrue False except RuntimeError exp FileCheck check Trying create tensor negative dimension check Traceback TorchScript check b forwardError check_next ~~~~~~~~~~~~~~~~~~~ --- HERE check call check_next ~~~~~~~~~ --- HERE check torch ones check_next ~~~~~~~~~~ --- HERE run str exp TestLiteScriptQuantizedModule QuantizationLiteTestCase test_single_layer input = torch rand dtype=torch float quantized_model = _create_quantized_model model_class=AnnotatedSingleLayerLinearModel qengine= qnnpack _compare_script_and_mobile model=quantized_model input=input test_two_layer input = torch rand dtype=torch float quantized_model = _create_quantized_model model_class=TwoLayerLinearModel _compare_script_and_mobile model=quantized_model input=input test_annotated_nested input = torch rand dtype=torch float quantized_model = _create_quantized_model model_class=AnnotatedNestedModel qengine= qnnpack _compare_script_and_mobile model=quantized_model input=input test_quantization_example From example Static Quantization section https pytorch org docs stable quantization html M torch nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub conv = torch nn Conv d relu = torch nn ReLU dequant = torch ao quantization DeQuantStub forward x x = quant x x = conv x x = relu x x = dequant x x model_fp = M model_fp eval model_fp qconfig = torch ao quantization get_default_qconfig qnnpack model_fp _fused = torch ao quantization fuse_modules model_fp conv relu model_fp _prepared = torch ao quantization prepare model_fp _fused input_fp = torch randn model_fp _prepared input_fp model_int = torch ao quantization convert model_fp _prepared input = torch randn _compare_script_and_mobile model=model_int input=input test_bundled_input_with_dynamic_type Model torch nn Module forward x dict int torch Tensor y dict int torch Tensor z dict int torch Tensor x model = Model script_module = torch jit script model sample_input = script_module forward torch ones torch ones torch ones bundled_model = torch utils bundled_inputs bundle_inputs script_module sample_input buf = bundled_model _save_to_buffer_for_lite_interpreter mobile_module = _load_for_lite_interpreter io BytesIO buf i = mobile_module run_method get_all_bundled_inputs assertEqual i torch ones torch ones torch ones __name__ == __main__ run_tests