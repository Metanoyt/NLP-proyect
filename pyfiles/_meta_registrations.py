mypy allow-untyped-defs math collections abc Callable Sequence enum Enum functools wraps typing Optional TypeVar Union typing_extensions ParamSpec torch torch _prims_common utils torch SymBool SymFloat Tensor torch _decomp _add_op_to_registry _convert_out_params global_decomposition_table meta_table torch _ops OpOverload torch _prims _prim_elementwise_meta ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND torch _prims_common BoolLike corresponding_complex_dtype corresponding_real_dtype elementwise_dtypes ELEMENTWISE_TYPE_PROMOTION_KIND FloatLike IntLike make_contiguous_strides_for Number suggest_memory_format TensorLike torch _prims_common wrappers _maybe_convert_to_dtype _maybe_resize_out _resize_output_check _safe_copy_out out_wrapper torch _refs _broadcast_shapes _maybe_broadcast torch fx experimental _config exp_config torch utils _pytree pytree _T = TypeVar _T _P = ParamSpec _P aten = torch ops aten _meta_lib_dont_use_me_use_register_meta = torch library Library aten IMPL Meta MODE_SUM MODE_MEAN MODE_MAX = range register_meta op - Callable Callable _P _T Callable _P _T wrapper fn fn = _convert_out_params fn register op _add_op_to_registry meta_table op fn pytree tree_map_ register op fn wrapper elementwise_meta args type_promotion ELEMENTWISE_TYPE_PROMOTION_KIND Perform type promotion expected prim_metafunction _ result_dtype = utils elementwise_dtypes args type_promotion_kind=type_promotion args = _maybe_convert_to_dtype x result_dtype x args Broadcast args = _maybe_broadcast args Perform prim checks _prim_elementwise_meta args type_promotion=ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND DEFAULT toRealValueType dtype from_complex = torch complex torch half torch cfloat torch float torch cdouble torch double from_complex get dtype dtype check_inplace_broadcast self_shape args_shape broadcasted_shape = tuple _broadcast_shapes self_shape args_shape torch _check broadcasted_shape == self_shape lambda f output shape self_shape doesn t match broadcast shape broadcasted_shape register_meta aten linspace aten logspace out_wrapper meta_linspace_logspace start end steps base=None dtype=None device=None layout=torch strided pin_memory=False requires_grad=False isinstance start torch Tensor torch _check start dim == lambda linspace only supports -dimensional start end tensors isinstance end torch Tensor torch _check end dim == lambda linspace only supports -dimensional start end tensors any isinstance arg complex arg start end steps default_complex_dtype = utils corresponding_complex_dtype torch get_default_dtype dtype None dtype = default_complex_dtype torch _check utils is_complex_dtype dtype lambda f linspace inferred dtype default_complex_dtype can t safely cast passed dtype dtype dtype = dtype torch get_default_dtype assert isinstance dtype torch dtype steps does participate computation dtype torch _check_type isinstance steps IntLike lambda f received invalid combination arguments - got \ type start __name__ type end __name__ type steps __name__ assert isinstance steps IntLike mypy torch _check steps = lambda number steps must non-negative torch empty steps type ignore arg-type dtype=dtype layout=layout device= meta pin_memory=pin_memory requires_grad=requires_grad register_meta aten take default aten take out out_wrapper meta_take index Type device checks torch _check index dtype == torch long lambda f take Expected long tensor index got index dtype Index checks torch _check_index numel == index numel = lambda take tried take empty tensor new_empty index shape register_meta aten linalg_cross default aten linalg_cross out out_wrapper linalg_cross other dim=- x_d = ndim y_d = other ndim torch _check x_d == y_d lambda linalg cross inputs must have same number dimensions torch _check size dim == other size dim == lambda f linalg cross inputs dimension dim must have length f Got size dim other size dim out_shape = _broadcast_shapes shape other shape new_empty out_shape register_meta aten linalg_matrix_exp out_wrapper linalg_matrix_exp squareCheckInputs linalg matrix_exp checkFloatingOrComplex linalg matrix_exp torch empty_like memory_format=torch contiguous_format register_meta aten cummax default aten cummax out aten cummin default aten cummin out out_wrapper values indices cummaxmin dim values = torch empty shape device=self device dtype=self dtype indices = torch empty shape device=self device dtype=torch int numel = ndim = Checks dim within bounds maybe_wrap_dim dim ndim values indices register_meta aten logcumsumexp default aten logcumsumexp out out_wrapper logcumsumexp dim Checks dim within bounds maybe_wrap_dim dim ndim torch empty_like memory_format=torch contiguous_format Stride-related code _exec_fft aten src ATen native mkl SpectralOps cpp aten src ATen cuda SpectralOps cpp Although actual FFT launch different all permuting code appears same _exec_fft out out_sizes dim forward ndim = ndim signal_ndim = len dim batch_dims = ndim - signal_ndim Permute dimensions so batch dimensions come first stride order dim_permute = list range ndim is_transformed_dim = False _ range ndim d dim is_transformed_dim d = True std partition left right = d dim_permute is_transformed_dim d left append d right append d dim_permute = left + right batch_end = len left self_strides = stride tmp = dim_permute batch_end tmp sort key=lambda x self_strides x reverse=True dim_permute = tmp + dim_permute batch_end input = permute dim_permute Collapse batch dimensions into single dimension batched_sizes = - + list input shape batch_dims input = input reshape batched_sizes batch_size = input size batched_sizes = batch_size batched_out_sizes = list batched_sizes i range len dim batched_out_sizes i + = out_sizes dim i out resize_ batched_out_sizes memory_format=torch contiguous_format Inplace reshaping original batch shape inverting dimension permutation out_strides = _ range ndim batch_numel = i = batch_dims - while i = out_strides dim_permute i = batch_numel out stride batch_numel = out_sizes dim_permute i i -= i range batch_dims ndim out_strides dim_permute i = out stride + i - batch_dims out as_strided_ out_sizes out_strides out storage_offset out _sort_dims Tensor dim list int exclude_last bool = False sorted_dims = list dim self_strides = stride sorted_dims len sorted_dims - int exclude_last sort key=lambda i self_strides i sorted_dims See _fft_c c_cufft aten src ATen native cuda SpectralOps cpp _fft_c c_mkl aten src ATen native mkl SpectralOps cpp register_meta aten _fft_c c default aten _fft_c c out out_wrapper meta_fft_c c dim normalization forward torch _check dtype is_complex dim clone sorted_dims = _sort_dims dim out = new_empty size _exec_fft out size sorted_dims forward=forward cufft_max_ndim = use_optimized_cufft_path dim list int len dim cufft_max_ndim len dim = dim == dim == False True register_meta aten _fft_r c default aten _fft_r c out out_wrapper meta_fft_r c dim normalization onesided torch _check dtype is_floating_point input_sizes = list size out_sizes = list input_sizes last_dim = dim - last_dim_halfsize = input_sizes last_dim + onesided_sizes = list input_sizes onesided_sizes last_dim = last_dim_halfsize onesided out_sizes last_dim = last_dim_halfsize device_hint == cuda device_hint == xpu _fft_r c_cufft aten src ATen native cuda SpectralOps cpp _fft_r c_xpu torch-xpu-ops src ATen native xpu SpectralOps cpp output = new_empty out_sizes dtype=utils corresponding_complex_dtype dtype working_tensor = device_hint == cuda use_optimized_cufft_path dim _exec_fft output working_tensor out_sizes dim forward=True First do R C transform last dimension target_sizes = out_sizes len dim == onesided_sizes _exec_fft output working_tensor target_sizes last_dim forward=True len dim working_tensor = new_empty out_sizes dtype=utils corresponding_complex_dtype dtype Then any remaining C C transforms sorted_dims = dim - while sorted_dims output working_tensor = working_tensor output strides = working_tensor stride sorted_dims sort key=lambda i strides i reverse=True NB reverse Not sure og bug max_dims = min cufft_max_ndim len sorted_dims last_dims = sorted_dims len sorted_dims - max_dims _exec_fft output working_tensor onesided_sizes last_dims forward=True sorted_dims = sorted_dims len sorted_dims - max_dims onesided output size last_dim = out_sizes last_dim working_tensor resize_ out_sizes memory_format=torch contiguous_format output = working_tensor output new_empty out_sizes dtype=utils corresponding_complex_dtype dtype register_meta aten randperm generator_out meta_randperm n generator=None out _maybe_resize_out out torch Size n register_meta aten randperm default meta_randperm_default n dtype=torch long layout=None device=None pin_memory=None torch empty n dtype=dtype layout=layout device=device pin_memory=pin_memory register_meta aten randint default aten randint out out_wrapper meta_randint high size dtype=torch long layout=None device=None pin_memory=None low = torch _check high low lambda f random_ expects less than got from= low = to= high torch empty size dtype=dtype layout=layout device=device pin_memory=pin_memory register_meta aten randint low aten randint low_out out_wrapper meta_randint_low low high size dtype=torch long layout=None device=None pin_memory=None torch _check high low lambda f random_ expects less than got from= low = to= high torch empty size dtype=dtype layout=layout device=device pin_memory=pin_memory register_meta aten rand default aten rand out out_wrapper meta_rand_default size dtype=None layout=None device=None pin_memory=None torch empty size dtype=dtype layout=layout device=device pin_memory=pin_memory register_meta aten _fft_c r default aten _fft_c r out out_wrapper meta_fft_c r Tensor dim list int normalization int lastdim int _fft_c r_mkl torch _check dtype is_complex device_hint == cuda out_sizes = list size out_sizes dim - = lastdim output = new_empty out_sizes dtype=toRealValueType dtype use_optimized_cufft_path dim _exec_fft output clone memory_format=torch contiguous_format out_sizes dim forward=False First complete any C C transforms len dim temp = meta_fft_c c dim - lastdim fft_norm_mode none temp = clone memory_format=torch contiguous_format _exec_fft output temp out_sizes dim - forward=False input = len dim c c_dims = dim - input = meta_fft_c c c c_dims normalization forward=False dim = dim - out_sizes = list input size out_sizes dim - = lastdim out = new_empty out_sizes dtype=toRealValueType dtype _exec_fft out input out_sizes dim forward=False register_meta aten copy_ default meta_copy_ src non_blocking=False This code simulates original decomp inductor which runs most meta checks we care about In theory we should make more robust carefully auditing our C++ copy_ kernel copying checks here torch fx experimental symbolic_shapes free_unbacked_symbols TODO Ideally we d insert deferred runtime assert here we calling actual copy_ you ll get automatically https github com pytorch pytorch issues free_unbacked_symbols torch _debug_has_internal_overlap == == MemOverlap Yes raise RuntimeError more than one element written-to tensor refers single memory location isinstance src Tensor intermediate = src non_blocking size = intermediate size aten expand_copy default intermediate size inferUnsqueezeGeometry tensor dim result_sizes = list tensor size result_strides = list tensor stride pyrefly ignore unsupported-operation new_stride = dim = tensor dim result_sizes dim result_strides dim pyrefly ignore bad-argument-type result_sizes insert dim pyrefly ignore bad-argument-type result_strides insert dim new_stride result_sizes result_strides register_meta aten unsqueeze_ default meta_unsqueeze_ dim dim = maybe_wrap_dim dim dim + g_sizes g_strides = inferUnsqueezeGeometry dim as_strided_ g_sizes g_strides register_meta aten _sparse_semi_structured_linear meta_sparse_structured_linear input Tensor weight Tensor _meta Tensor bias Optional Tensor = None _activation_opt Optional str = None out_dtype Optional torch dtype = None output_sizes = list input shape bias None assert weight size == bias size output size mismatch assert weight size == input size - output_sizes - = weight size see https github com pytorch pytorch pull #issuecomment- We assume we have already squashed inputs into -D tensor Then output transposed we need propagate transposed stride information output tensor assert len input shape == we can only handle squashed input case transposed_strides = input size out_dtype None assert input dtype == torch int out_dtype == torch int out_dtype only supported i i - i linear operator output = input new_empty output_sizes dtype=input dtype out_dtype None out_dtype as_strided output_sizes transposed_strides output register_meta aten _sparse_semi_structured_mm meta_sparse_structured_mm mat Tensor mat _meta Tensor mat Tensor out_dtype Optional torch dtype = None assert len mat shape == assert len mat _meta shape == assert len mat shape == assert mat size == mat size output_sizes = mat size mat size out_dtype None assert mat dtype == torch int out_dtype == torch int out_dtype only supported i i - i linear operator output = mat new_empty output_sizes dtype=mat dtype out_dtype None out_dtype output register_meta aten _sparse_semi_structured_addmm meta_sparse_structured_addmm input Tensor mat Tensor mat _meta Tensor mat Tensor alpha= beta= out_dtype Optional torch dtype = None assert len input shape == only input broadcasted columns mat mat product supported assert len mat shape == assert len mat _meta shape == assert len mat shape == assert input size == mat size only input broadcasted columns mat mat product supported assert mat size == mat size output_sizes = mat size mat size out_dtype None assert mat dtype == torch int out_dtype == torch int out_dtype only supported i i - i linear operator output = mat new_empty output_sizes dtype=mat dtype out_dtype None out_dtype output register_meta aten _cslt_sparse_mm meta__cslt_sparse_mm compressed_A torch Tensor dense_B torch Tensor bias Optional Tensor = None alpha Optional Tensor = None out_dtype Optional torch dtype = None transpose_result bool = False alg_id int = split_k int = split_k_mode int = - assert dense_B dtype torch float torch float torch bfloat torch int torch float _e m fn _cslt_sparse_mm only supports fp bf int fp e m assert compressed_A dtype == dense_B dtype inputs must have same dtype assert len dense_B shape == _cslt_sparse_mm only supports d inputs is_ bit_input_type = compressed_A dtype torch int torch float _e m fn is_ bit_input_type assert dense_B is_contiguous dense input must transposed bit dtypes n = dense_B size m = compressed_A size bias None assert m == bias size out_dtype None assert is_ bit_input_type out_dtype torch float torch bfloat torch int torch float _e m fn f out_dtype supported compressed_A dtype x dense_B dtype - out_dtype matmul output_shape = n m transpose_result m n dense_B new_empty output_shape dtype=out_dtype register_meta aten index_reduce default meta_index_reduce Tensor dim int index Tensor source torch Tensor reduce str include_self bool = True - Tensor torch empty_like memory_format=torch contiguous_format register_meta aten index_reduce_ default meta_index_reduce_ Tensor dim int index Tensor source torch Tensor reduce str include_self bool = True - Tensor Implementations below taken https github com albanD subclass_zoo blob main python_meta_tensor py out_wrapper register_meta aten index_select default meta_index_select dim index result_size = list size dim result_size dim = index numel new_empty result_size register_meta aten segment_reduce default meta_segment_reduce data Tensor reduce str lengths Optional Tensor = None indices Optional Tensor = None offsets Optional Tensor = None axis int = unsafe bool = False initial=None - Tensor indices None raise NotImplementedError segment_reduce indices based reduction supported yet segment_reduce_lengths_tensor lengths_shape torch empty lengths_shape + data shape axis + dtype=data dtype device= meta memory_format=torch contiguous_format lengths None segment_reduce_lengths_tensor lengths shape FIXME should probably check lengths offset aren t both set ATen implementation neglects too offsets None lengths == torch diff offsets lengths_shape = offsets shape - + offsets shape - - segment_reduce_lengths_tensor lengths_shape raise RuntimeError segment_reduce Either lengths offsets must defined register_meta aten max default aten max unary_out out_wrapper meta_max new_empty register_meta aten max dim meta_max_dim dim keepdim=False dim = utils reduction_dims shape dim output_shape = _compute_reduction_shape dim keepdim new_empty output_shape new_empty output_shape dtype=torch long register_meta aten min default aten min unary_out out_wrapper meta_min new_empty register_meta aten min dim meta_min_dim dim keepdim=False dim = utils reduction_dims shape dim output_shape = _compute_reduction_shape dim keepdim new_empty output_shape new_empty output_shape dtype=torch long register_meta aten angle default meta_angle is_complex result_dtype = corresponding_real_dtype dtype _ result_dtype = elementwise_dtypes type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT torch empty_like dtype=result_dtype register_meta aten angle out meta_angle_out out torch _resize_output_ out size device out copy_ torch angle register_meta aten _assert_async default assert_async val register_meta aten _assert_async msg assert_async_meta val assert_msg register_meta aten _print default print_meta s register_meta aten _make_dep_token default make_dep_token dtype=None layout=None device=None pin_memory=None memory_format=None torch empty device= meta register_meta aten sym_constrain_range default sym_constrain_range size min=None max=None Avoid importing sympy module level torch fx experimental symbolic_shapes constrain_range isinstance size SymFloat SymBool raise ValueError Constraining SymFloat Symbool nyi constrain_range size min=min max=max register_meta aten _functional_sym_constrain_range default functional_sym_constrain_range size min=None max=None dep_token=None aten sym_constrain_range size min=min max=max dep_token register_meta aten sym_constrain_range_for_size default sym_constrain_range_for_size size min=None max=None Avoid importing sympy module level torch fx experimental symbolic_shapes _constrain_range_for_size min None max None torch _check size = isinstance size SymFloat SymBool raise ValueError Constraining SymFloat Symbool nyi type size int min None torch _check size = min max None torch _check size = max _constrain_range_for_size size min=min max=max register_meta aten _functional_sym_constrain_range_for_size default functional_sym_constrain_range_for_size size min max dep_token aten sym_constrain_range_for_size size min=min max=max dep_token register_meta aten _functional_assert_async msg functional_assert_async_meta val assert_msg dep_token dep_token From aten src ATen native LinearAlgebraUtils h squareCheckInputs Tensor f_name str assert dim = f f_name The input tensor must have least dimensions assert size - == size - f f_name A must batches square matrices they size - size - matrices Validates input shapes devices linear solve methods solve cholesky_solve lu_solve triangular_solve From aten src ATen native LinearAlgebraUtils h linearSolveCheckInputs Tensor A Tensor name str torch _check device == A device lambda f Expected b A same device found b f device A A device instead torch _check dtype == A dtype lambda f Expected b A have same dtype found b type f dtype A type A dtype instead torch _check A size - == A size - lambda f A must batches square matrices f they A size - A size - matrices torch _check A size - == size - lambda f Incompatible matrix sizes name each A f matrix A size - A size - f each b matrix size - size - From aten src ATen native LinearAlgebraUtils h checkFloatingOrComplex t Tensor f_name str allow_low_precision_dtypes bool = True dtype = t dtype torch _check t is_floating_point t is_complex lambda f f_name Expected floating point complex tensor input Got dtype allow_low_precision_dtypes torch _check dtype torch float torch double torch cfloat torch cdouble lambda f f_name Low precision dtypes supported Got dtype From aten src ATen native LinearAlgebraUtils h checkIsMatrix A Tensor f_name str arg_name str = A torch _check A dim = lambda f f_name The input tensor arg_name must have least dimensions checkInputsSolver A Tensor B Tensor left bool f_name str squareCheckInputs A f_name checkIsMatrix B f_name torch _check A size - == B size - left A size - == B size - lambda f f_name Incompatible shapes A B equation f AX = B left XA = B f A size - x A size - B size - x B size - checkSameDevice fn_name str result Tensor input Tensor result_name str = result torch _check result device == input device lambda f fn_name Expected result_name input tensors same device got f result_name result device input input device checkUplo UPLO str UPLO_uppercase = UPLO upper torch _check len UPLO == UPLO_uppercase == U UPLO_uppercase == L lambda f Expected UPLO argument L U got UPLO register_meta aten _linalg_eigh default aten _linalg_eigh eigenvalues out_wrapper eigenvalues eigenvectors meta__linalg_eigh A Tensor UPLO str = L compute_v bool = True squareCheckInputs A linalg eigh checkUplo UPLO shape = list A shape compute_v vecs = A new_empty shape vecs as_strided_ shape make_contiguous_strides_for shape row_major=False vecs = A new_empty shape pop vals = A new_empty shape dtype=toRealValueType A dtype vals vecs register_meta aten _linalg_eigvals default aten linalg_eigvals out out_wrapper meta__linalg_eigvals input Tensor - Tensor squareCheckInputs input linalg eigvals complex_dtype = input dtype utils is_complex_dtype input dtype utils corresponding_complex_dtype input dtype input new_empty input shape - dtype=complex_dtype register_meta aten linalg_eig out_wrapper eigenvalues eigenvectors meta_linalg_eig input Tensor squareCheckInputs input linalg eig complex_dtype = input dtype utils is_complex_dtype input dtype utils corresponding_complex_dtype input dtype values = input new_empty input shape - dtype=complex_dtype vectors = input new_empty input shape dtype=complex_dtype values vectors cloneBatchedColumnMajor src Tensor - Tensor src mT clone memory_format=torch contiguous_format transpose - - register_meta aten _cholesky_solve_helper out_wrapper _cholesky_solve_helper Tensor A Tensor upper bool - Tensor cloneBatchedColumnMajor register_meta aten cholesky_solve out_wrapper cholesky_solve Tensor A Tensor upper bool = False - Tensor torch _check ndim = lambda f b should have least dimensions has ndim dimensions instead torch _check A ndim = lambda f u should have least dimensions has A ndim dimensions instead self_broadcasted A_broadcasted = _linalg_broadcast_batch_dims_name A cholesky_solve _cholesky_solve_helper self_broadcasted A_broadcasted upper register_meta aten cholesky out_wrapper cholesky Tensor upper bool = False - Tensor numel == torch empty_like memory_format=torch legacy_contiguous_format squareCheckInputs cholesky cloneBatchedColumnMajor register_meta aten cholesky_inverse out_wrapper cholesky_inverse Tensor upper bool = False - Tensor squareCheckInputs cholesky_inverse cloneBatchedColumnMajor From aten src ATen native BatchLinearAlgebra cpp register_meta aten linalg_cholesky_ex default linalg_cholesky_ex A Tensor upper bool = False check_errors bool = False squareCheckInputs A linalg cholesky checkFloatingOrComplex A linalg cholesky A_shape = A shape ndim = len A_shape L L_strides = make_contiguous_strides_for A_shape False L = A new_empty A_shape L as_strided_ A_shape L_strides infos infos = A new_empty A_shape ndim - dtype=torch int L infos register_meta aten linalg_householder_product default aten linalg_householder_product out out_wrapper linalg_householder_product input Tensor tau Tensor - Tensor torch _check input ndim = lambda torch linalg householder_product input must have least dimensions torch _check input size - = input size - lambda torch linalg householder_product input shape - must greater than equal input shape - torch _check input size - = tau size - lambda torch linalg householder_product input shape - must greater than equal tau shape - torch _check input ndim - tau ndim == lambda f torch linalg householder_product Expected tau have one dimension less than input f got tau ndim equal tau ndim input ndim equal input ndim input ndim expected_batch_tau_shape = input shape - actual_batch_tau_shape = tau shape - torch _check actual_batch_tau_shape == expected_batch_tau_shape lambda f torch linalg householder_product Expected batch dimensions tau f equal input shape - got actual_batch_tau_shape torch _check tau dtype == input dtype lambda f torch linalg householder_product tau dtype tau dtype f does match input dtype input dtype checkSameDevice torch linalg householder_product tau input tau torch empty_strided size=input shape stride=make_contiguous_strides_for input shape row_major=False dtype=input dtype device=input device From aten src ATen native BatchLinearAlgebra cpp register_meta aten linalg_inv_ex default linalg_inv_ex_meta A Tensor check_errors bool = False squareCheckInputs A linalg inv_ex checkFloatingOrComplex A linalg inv_ex allow_low_precision_dtypes=False L = A new_empty A shape L as_strided_ A shape make_contiguous_strides_for A shape row_major=False infos = A new_empty A shape - dtype=torch int L infos register_meta aten linalg_ldl_factor_ex default aten linalg_ldl_factor_ex out out_wrapper LD pivots info linalg_ldl_factor_ex_meta Tensor hermitian bool = False check_errors bool = False - tuple Tensor Tensor Tensor squareCheckInputs torch linalg ldl_factor_ex checkFloatingOrComplex torch linalg ldl_factor_ex LD = torch empty_strided size=self shape stride=make_contiguous_strides_for shape row_major=False dtype=self dtype device=self device pivots = new_empty shape - dtype=torch int info = new_empty shape - dtype=torch int LD pivots info register_meta aten linalg_ldl_solve default aten linalg_ldl_solve out out_wrapper linalg_ldl_solve_meta LD Tensor pivots Tensor B Tensor hermitian bool = False - Tensor squareCheckInputs LD torch linalg ldl_solve checkFloatingOrComplex LD torch linalg ldl_solve linearSolveCheckInputs B LD torch linalg ldl_solve torch _check B ndim = lambda f torch linalg ldl_solve Expected B have least dimensions f has B ndim dimensions instead expected_pivots_shape = LD shape - torch _check expected_pivots_shape == pivots shape lambda f torch linalg ldl_solve Expected LD shape - pivots shape same f got pivots shape pivots shape instead torch _check utils is_integer_dtype pivots dtype lambda f torch linalg ldl_solve Expected pivots integers Got pivots dtype torch _check LD dtype == B dtype lambda f torch linalg ldl_solve LD dtype LD dtype does match b dtype B dtype B_broadcast_size _ = _linalg_broadcast_batch_dims B LD torch empty_strided size=B_broadcast_size stride=make_contiguous_strides_for B_broadcast_size row_major=False dtype=B dtype device=B device register_meta aten linalg_lu default aten linalg_lu out out_wrapper P L U linalg_lu_meta A Tensor pivot bool = True - tuple Tensor Tensor Tensor torch _check A ndim = lambda f linalg lu Expected tensor more dimensions Got size A shape instead sizes = list A shape m = sizes - n = sizes - k = min m n sizes - = m pivot P = A new_empty sizes P = A new_empty sizes - = k L = A new_empty sizes sizes - = k sizes - = n U = A new_empty sizes P L U register_meta aten linalg_lu_factor_ex default aten linalg_lu_factor_ex out out_wrapper LU pivots info linalg_lu_factor_ex_meta A Tensor pivot bool = True check_errors bool = False - tuple Tensor Tensor Tensor torch _check A ndim = lambda f torch lu_factor Expected tensor more dimensions Got size A shape instead sizes = list A shape m = sizes - n = sizes - LU = torch empty_strided size=sizes stride=make_contiguous_strides_for sizes row_major=False dtype=A dtype device=A device Sets sizes size pivots sizes pop sizes - = min m n pivots = A new_empty sizes dtype=torch int Sets sizes size info sizes pop info = A new_empty sizes dtype=torch int LU pivots info register_meta aten linalg_lu_solve default aten linalg_lu_solve out out_wrapper linalg_lu_solve_meta LU Tensor pivots Tensor B Tensor left bool = True adjoint bool = False - Tensor dtype checkFloatingOrComplex LU torch linalg lu_solve torch _check LU dtype == B dtype lambda f linalg lu_solve Expected LU B have same dtype f found LU type LU dtype B type B dtype instead torch _check pivots dtype == torch int lambda linalg lu_solve pivots should Tensor scalar type torch int matrix shapes squareCheckInputs LU torch linalg lu_solve checkInputsSolver LU B left linalg lu_solve torch _check LU size - == pivots size - lambda linalg lu_solve Number pivots per batch should same dimension matrix batches torch _check LU shape - == pivots shape lambda f linalg lu_solve Expected LU shape - pivots shape same f got pivots shape pivots shape instead B_broadcast_size _ = _linalg_broadcast_batch_dims B LU result = torch empty_strided size=B_broadcast_size stride=make_contiguous_strides_for B_broadcast_size row_major=not left dtype=B dtype device=B device result numel = left result is_complex result = result conj result register_meta aten lu_unpack out_wrapper P L U lu_unpack_meta LU Tensor pivots Tensor unpack_data bool = True unpack_pivots bool = True - tuple Tensor Tensor Tensor torch _check LU ndim = lambda f torch lu_unpack Expected tensor more dimensions Got size LU shape instead unpack_pivots torch _check pivots dtype == torch int lambda torch lu_unpack LU_pivots expected contiguous tensor torch int dtype \n Note function intended used output produced torch linalg lu_factor sizes = list LU shape m = sizes - n = sizes - k = min m n sizes - = m unpack_pivots P = LU new_empty sizes P = LU new_empty unpack_data sizes - = k L = LU new_empty sizes sizes - = k sizes - = n U = LU new_empty sizes L = LU new_empty U = LU new_empty P L U parse mode param linalg_qr tuple bools compute_q reduced _parse_qr_mode mode str - tuple bool bool mode == reduced compute_q = True reduced = True mode == complete compute_q = True reduced = False mode == r compute_q = False reduced = True actually irrelevant mode torch _check False lambda f qr received unrecognized mode mode f expected one reduced default r complete compute_q reduced type ignore possibly-undefined register_meta aten linalg_qr default aten linalg_qr out out_wrapper Q R linalg_qr_meta A Tensor mode str = reduced - tuple Tensor Tensor checkIsMatrix A linalg qr checkFloatingOrComplex A linalg qr compute_q reduced_mode = _parse_qr_mode mode m = A shape - n = A shape - k = min m n compute_q Q_shape = list A shape Q_shape - = k reduced_mode m Q = A new_empty Q_shape Q as_strided_ Q_shape make_contiguous_strides_for Q_shape row_major=False Q = A new_empty For readability R_shape = list A shape R_shape - = k reduced_mode compute_q m R = A new_empty R_shape R as_strided_ R_shape make_contiguous_strides_for R_shape row_major=False Q R register_meta aten _linalg_slogdet default aten _linalg_slogdet sign out_wrapper sign logabsdet LU pivots _linalg_slogdet A Tensor - tuple Tensor Tensor Tensor Tensor squareCheckInputs A linalg slogdet checkFloatingOrComplex A linalg slogdet False shape = A shape sign = A new_empty shape - logabsdet = A new_empty shape - dtype=toRealValueType A dtype LU = torch empty_strided size=shape stride=make_contiguous_strides_for shape False dtype=A dtype device=A device pivots = A new_empty shape - dtype=torch int sign logabsdet LU pivots From aten src ATen native BatchLinearAlgebra cpp NOTE matching defaults aten src ATen native native_functions yaml register_meta aten _linalg_svd default _linalg_svd_meta A Tensor full_matrices bool = False compute_uv bool = True driver Optional str = None checkIsMatrix A linalg svd checkFloatingOrComplex A linalg svd batch_dims = list A shape - m = A shape - n = A shape - k = min m n compute_uv U_shape = batch_dims + m m full_matrices k U = A new_empty U_shape U as_strided_ U_shape make_contiguous_strides_for U_shape row_major=False V_shape = batch_dims + n full_matrices k n V = A new_empty V_shape NB This checks CUDA since there no way check cuSolver Also might work correctly CPU when fake_device available device_hint just defaults CUDA case See _linalg_svd meta core is_cuda = device_hint A == cuda V as_strided_ V_shape make_contiguous_strides_for V_shape row_major=is_cuda doesn t matter U = A new_empty V = A new_empty S always real even when A complex S = A new_empty batch_dims + k dtype=toRealValueType A dtype U S V _linalg_broadcast_batch_dims arg Tensor arg Tensor - tuple list int list int broadcast batch dimensions arg arg arg _batch_sizes = arg shape - arg _batch_sizes = arg shape - expand_batch_portion = _broadcast_shapes arg _batch_sizes arg _batch_sizes arg _expand_size = list expand_batch_portion arg _expand_size += arg size - arg size - arg _expand_size = list expand_batch_portion arg _expand_size += arg size - arg size - arg _expand_size arg _expand_size _linalg_broadcast_batch_dims_name arg Tensor arg Tensor name Optional str - tuple Tensor Tensor If there s no name we assume we don t want check errors name linearSolveCheckInputs arg arg name arg _expand_size arg _expand_size = _linalg_broadcast_batch_dims arg arg arg _broadcasted = arg arg _expand_size == arg shape arg expand arg _expand_size arg _broadcasted = arg arg _expand_size == arg shape arg expand arg _expand_size arg _broadcasted arg _broadcasted linalg_solve_is_vector_rhs input Tensor other Tensor - bool expected_batched_rhs_shape = input shape - vector_case = other ndim == input ndim - == other ndim other shape == expected_batched_rhs_shape vector_case register_meta aten _linalg_solve_ex _linalg_solve_ex A Tensor B Tensor left bool = True check_errors bool = False result Optional Tensor = None LU Optional Tensor = None pivots Optional Tensor = None info Optional Tensor = None - tuple Tensor Tensor Tensor Tensor checkFloatingOrComplex A linalg solve torch _check A dtype == B dtype lambda f linalg solve Expected A B have same dtype found A type f A dtype B type B dtype instead vector_case = linalg_solve_is_vector_rhs A B B_ = B unsqueeze - vector_case B checkInputsSolver A B_ left linalg solve B_broad_shape _ = _linalg_broadcast_batch_dims B_ A torch _check left vector_case lambda linalg solve Vector broadcasting left hand side supported left=False In case linalg solve equivalent B A squeeze - result_shape = B_broad_shape - vector_case B_broad_shape result_ = torch empty_strided size=result_shape stride=make_contiguous_strides_for result_shape left dtype=B dtype device=B device shape = A shape LU_ = torch empty_strided size=shape stride=make_contiguous_strides_for shape False dtype=A dtype device=A device pivots_ = A new_empty shape - dtype=torch int info_ = A new_empty shape - dtype=torch int out = result LU pivots info res = result_ LU_ pivots_ info_ all x None x out r o zip res out resize copy operations done in-place _maybe_resize_out o r shape type ignore arg-type strides copied out_wrapper o as_strided_ r shape r stride type ignore union-attr _safe_copy_out copy_from=r copy_to=o exact_dtype=False type ignore arg-type res register_meta aten linalg_solve_triangular default aten linalg_solve_triangular out linalg_solve_triangular_meta A Tensor B Tensor upper bool left bool = True unitriangular bool = False out Optional Tensor = None - Tensor out None out = A new_empty assert isinstance out TensorLike checkInputsSolver A B left linalg solve_triangular B_ A_ = _linalg_broadcast_batch_dims_name B A None avoid_copy_A = A_ transpose - - is_contiguous A_ is_conj avoid_copy_A out = _maybe_resize_out out B_ shape reimplementation resize_output result F-contig _resize_output_check out B_ shape out resize_ B_ transpose - - shape out transpose_ - - out type ignore return-value register_meta aten triangular_solve out_wrapper X M exact_dtype=True triangular_solve_meta Tensor A Tensor upper bool = True transpose bool = False unitriangular bool = False - tuple Tensor Tensor torch _check ndim = lambda f torch triangular_solve Expected b have least dimensions f has ndim dimensions instead torch _check A ndim = lambda f torch triangular_solve Expected A have least dimensions f has A ndim dimensions instead linearSolveCheckInputs A triangular_solve A layout == torch strided self_broadcast_size A_broadcast_size = _linalg_broadcast_batch_dims A solution = torch empty_strided size=self_broadcast_size stride=make_contiguous_strides_for self_broadcast_size row_major=False dtype=self dtype device=self device cloned_coefficient = torch empty_strided size=A_broadcast_size stride=make_contiguous_strides_for A_broadcast_size row_major=False dtype=A dtype device=A device A layout == torch sparse_csr A layout == torch sparse_bsr solution = torch empty_like cloned_coefficient = new_empty torch _check False lambda triangular_solve Got unexpected layout solution cloned_coefficient type ignore possibly-undefined From aten src ATen native LinearAlgebra cpp register_meta aten _linalg_det default _linalg_det_meta A squareCheckInputs A linalg det checkFloatingOrComplex A linalg det det = A new_empty A shape - LU = A new_empty A shape LU as_strided_ A shape make_contiguous_strides_for A shape row_major=False pivots = A new_empty A shape - dtype=torch int det LU pivots register_meta aten ormqr out_wrapper ormqr input Tensor tau Tensor other Tensor left bool = True transpose bool = False - Tensor torch _check input ndim = lambda torch ormqr input must have least dimensions torch _check other ndim = lambda torch ormqr other must have least dimensions left_size_condition = - left - torch _check other shape left_size_condition = tau shape - lambda f torch ormqr other shape left_size_condition must greater than equal tau shape - torch _check other shape left_size_condition == input shape - lambda f torch ormqr other shape left_size_condition must equal input shape - torch _check tau shape - = input shape - lambda torch ormqr tau shape - must less than equal input shape - torch _check input ndim - tau ndim == lambda f torch ormqr Expected tau have one dimension less than input f got tau ndim equal tau ndim input ndim equal input ndim torch _check input ndim == other ndim lambda f torch ormqr Expected other have same number dimensions input f got other ndim equal other ndim input ndim equal input ndim input ndim expected_batch_shape = input shape - actual_batch_tau_shape = tau shape - torch _check actual_batch_tau_shape == expected_batch_shape lambda f torch ormqr Expected batch dimensions tau f equal input shape - got actual_batch_tau_shape actual_batch_other_shape = other shape - torch _check actual_batch_other_shape == expected_batch_shape lambda f torch ormqr Expected batch dimensions other f equal input shape - got actual_batch_other_shape torch _check tau dtype == input dtype lambda f torch ormqr Expected input tau have same dtype f input has dtype input dtype tau has dtype tau dtype torch _check other dtype == input dtype lambda f torch ormqr Expected input other have same dtype f input has dtype input dtype other has dtype other dtype checkSameDevice torch ormqr tau input tau checkSameDevice torch ormqr other input other torch empty_strided size=other shape stride=make_contiguous_strides_for other shape row_major=False dtype=other dtype device=other device _padding_check_valid_input input padding dim torch _check len padding == dim lambda f padding size expected dim got len padding input_dim = input ndim is_batch_mode = input_dim == dim + valid_batch_mode = is_batch_mode valid_non_batch_mode = is_batch_mode is_batch_mode allow batch size -dim d range input_dim valid_batch_mode = valid_batch_mode input size d = d range input_dim valid_non_batch_mode = valid_non_batch_mode input size d = allow empty batch size other dimensions torch _check valid_batch_mode valid_non_batch_mode lambda f Expected dim + D dim + D batch mode tensor possibly batch size f other non-zero dimensions input got input shape _pad d_common input padding is_reflection dim_plane = dim_w = nbatch = input ndim == nbatch = input size dim_w += dim_plane += _padding_check_valid_input input padding dim= pad_l pad_r = padding nplane = input size dim_plane input_w = input size dim_w output_w = input_w + pad_l + pad_r is_reflection torch _check pad_l input_w pad_r input_w lambda f Argument Padding size should less than corresponding input dimension f got padding pad_l pad_r dimension dim_w input input shape torch _check output_w = lambda f input W input_w too small Calculated output W output_w input ndim == input new_empty nplane output_w input new_empty nbatch nplane output_w register_meta aten reflection_pad d out_wrapper meta_reflection_pad d input padding _pad d_common input padding is_reflection=True register_meta aten replication_pad d out_wrapper meta_replication_pad d input padding torch _check input dtype = torch bool lambda f replication_pad d implemented input dtype __str__ _pad d_common input padding is_reflection=False _pad d_backward_common grad_output input padding is_reflection dim_w = is_reflection torch _check len padding == lambda padding size expected input ndim == dim_w += pad_l pad_r = padding input_w = input size dim_w output_w = input_w + pad_l + pad_r is_reflection torch _check pad_l input_w pad_r input_w lambda f Argument Padding size should less than corresponding input dimension f got padding pad_l pad_r dimension dim_w input input shape torch _check output_w == grad_output size dim_w lambda f grad_output width unexpected Expected output_w Got grad_output size dim_w input new_empty input shape register_meta aten reflection_pad d_backward out_wrapper grad_input meta_reflection_pad d_backward grad_output input padding _pad d_backward_common grad_output input padding is_reflection=True register_meta aten replication_pad d_backward out_wrapper grad_input meta_replication_pad d_backward grad_output input padding _pad d_backward_common grad_output input padding is_reflection=False _pad d_common input padding is_reflection dim_w = dim_h = dim_slices = nbatch = _padding_check_valid_input input padding dim= ndim = input ndim ndim == nbatch = input size dim_w += dim_h += dim_slices += pad_l pad_r pad_t pad_b = padding nplane = input size dim_slices input_h = input size dim_h input_w = input size dim_w output_h = input_h + pad_t + pad_b output_w = input_w + pad_l + pad_r is_reflection torch _check pad_l input_w pad_r input_w lambda f Argument Padding size should less than corresponding input dimension f got padding pad_l pad_r dimension dim_w input input shape torch _check pad_t input_h pad_b input_h lambda f Argument Padding size should less than corresponding input dimension f got padding pad_t pad_b dimension dim_h input input shape torch _check output_w = output_h = lambda f input H input_h W input_w too small f Calculated output H output_h W output_w input ndim == input new_empty nplane output_h output_w input new_empty nbatch nplane output_h output_w register_meta aten reflection_pad d out_wrapper meta_reflection_pad d input padding _pad d_common input padding is_reflection=True register_meta aten replication_pad d out_wrapper meta_replication_pad d input padding torch _check input dtype = torch bool lambda f replication_pad d implemented input dtype __str__ _pad d_common input padding is_reflection=False register_meta aten reflection_pad d_backward default aten reflection_pad d_backward grad_input aten replication_pad d_backward default aten replication_pad d_backward grad_input out_wrapper grad_input meta_pad d_backward grad_output padding dim_w = dim_h = dim_plane = self_shape = shape dim == dim_w += dim_h += dim_plane += pad_l pad_r pad_t pad_b = padding input_h = self_shape dim_h input_w = self_shape dim_w output_h = input_h + pad_t + pad_b output_w = input_w + pad_l + pad_r torch _check output_w == grad_output size dim_w lambda f grad_output width unexpected Expected output_w Got grad_output size dim_w torch _check output_h == grad_output size dim_h lambda f grad_output height unexpected Expected output_h Got grad_output size dim_h new_empty shape _pad d_common input padding is_reflection dim_w = dim_h = dim_d = dim_plane = _padding_check_valid_input input padding dim= batch_mode = input ndim == batch_mode nbatch = input size dim_w += dim_h += dim_d += dim_plane += pad_l pad_r pad_t pad_b pad_f pad_bk = padding nplane = input size dim_plane input_d = input size dim_d input_h = input size dim_h input_w = input size dim_w output_d = input_d + pad_f + pad_bk output_h = input_h + pad_t + pad_b output_w = input_w + pad_l + pad_r is_reflection torch _check pad_l input_w pad_r input_w lambda f Argument Padding size should less than corresponding input dimension f got padding pad_l pad_r dimension dim_w input input shape torch _check pad_t input_h pad_b input_h lambda f Argument Padding size should less than corresponding input dimension f got padding pad_t pad_b dimension dim_h input input shape torch _check pad_f input_d pad_bk input_d lambda f Argument Padding size should less than corresponding input dimension f got padding pad_f pad_bk dimension dim_d input input shape torch _check output_w = output_h = output_d = lambda f input D input_d H input_h W input_w too small f Calculated output D output_d H output_h W output_w batch_mode input new_empty nbatch nplane output_d output_h output_w type ignore possibly-undefined input new_empty nplane output_d output_h output_w register_meta aten reflection_pad d out_wrapper meta_reflection_pad d input padding _pad d_common input padding is_reflection=True register_meta aten replication_pad d out_wrapper meta_replication_pad d input padding torch _check input dtype = torch bool lambda f replication_pad d implemented input dtype __str__ _pad d_common input padding is_reflection=False register_meta aten reflection_pad d_backward default aten reflection_pad d_backward grad_input aten replication_pad d_backward default aten replication_pad d_backward grad_input out_wrapper grad_input meta_pad d_backward grad_output input padding torch _check len padding == lambda padding size expected assert input ndim assert grad_output ndim == input ndim dim_w = dim_h = dim_d = input ndim == dim_w += dim_h += dim_d += pad_l pad_r pad_t pad_b pad_f pad_bk = padding input_d = input size dim_d input_h = input size dim_h input_w = input size dim_w output_d = input_d + pad_f + pad_bk output_h = input_h + pad_t + pad_b output_w = input_w + pad_l + pad_r torch _check output_w == grad_output size dim_w lambda f grad_output width unexpected Expected output_w Got grad_output size dim_w torch _check output_h == grad_output size dim_h lambda f grad_output height unexpected Expected output_h Got grad_output size dim_h torch _check output_d == grad_output size dim_d lambda f grad_output depth unexpected Expected output_d Got grad_output size dim_d input new_empty input shape register_meta aten _pdist_forward out_wrapper meta__pdist_forward Tensor p float = - Tensor torch _check is_contiguous lambda _pdist_forward requires contiguous input n = size n = new_empty memory_format=torch legacy_contiguous_format type ignore call-overload new_empty n n - memory_format=torch legacy_contiguous_format type ignore call-overload register_meta aten _pdist_backward out_wrapper meta__pdist_backward grad Tensor Tensor p float pdist Tensor - Tensor torch _check is_contiguous lambda _pdist_backward requires contiguous torch _check pdist is_contiguous lambda _pdist_backward requires pdist contiguous torch empty_like memory_format=torch legacy_contiguous_format register_meta aten baddbmm default aten baddbmm out out_wrapper exact_dtype=True meta_baddbmm batch batch beta= alpha= torch fx experimental symbolic_shapes guard_or_true sym_eq dim = batch size dim = batch size dim = batch size guard_or_true torch sym_not sym_eq shape dim dim dim = expand dim dim dim torch _check batch dim == lambda batch must D tensor torch _check batch dim == lambda batch must D tensor exp_config skip_dtype_check_in_meta_registrations torch _check dtype == batch dtype == batch dtype lambda f Input dtypes must same got input dtype batch batch dtype batch batch dtype batch _sizes = batch shape batch _sizes = batch shape bs = batch _sizes contraction_size = batch _sizes torch _check batch _sizes == bs batch _sizes == contraction_size lambda f Expected size first two dimensions batch tensor f bs contraction_size got batch _sizes batch _sizes new_empty size register_meta aten bernoulli default aten bernoulli out out_wrapper meta_bernoulli generator=None https github com pytorch pytorch issues torch empty_like memory_format=torch contiguous_format register_meta aten bernoulli_ float meta_bernoulli_ p= generator=None register_meta aten bernoulli p meta_bernoulli_p p= generator=None https github com pytorch pytorch issues torch empty_like memory_format=torch contiguous_format register_meta aten poisson default aten poisson out out_wrapper meta_poisson generator=None torch empty_like register_meta aten _fused_moving_avg_obs_fq_helper default meta__fused_moving_avg_obs_fq_helper observer_on fake_quant_on running_min running_max scale zero_point averaging_const quant_min quant_max ch_axis per_row_fake_quant=False symmetric_quant=False torch _check ch_axis dim lambda Error fused_moving_avg_obs_fake_quant_cpu ch_axis must dim mask = torch empty_like dtype=torch bool torch empty_like mask register_meta aten mm out_wrapper exact_dtype=True meta_mm b out_dtype Optional torch dtype = None torch _check dim == lambda must D torch _check b dim == lambda b must D N M = shape M P = b shape torch _check M == M lambda f b must have same reduction dim got N M X M P out_dtype None torch _check out_dtype == dtype out_dtype == torch float dtype torch float torch bfloat lambda out_dtype must same input dtype fp fp bf inputs result_dtype = dtype out_dtype None out_dtype new_empty N P dtype=result_dtype _compute_reduction_shape dims keepdim keepdim tuple shape i i dims i range ndim utils compute_reduction_output_shape shape dims FakeTensors meta tensors device will report device meta when running meta kernels Here access fake device FakeTensor exists so meta kernels which have diverge per device will more accurate when run FakeTensors device_hint tensor - str isinstance tensor torch _subclasses FakeTensor tensor fake_device type hasattr tensor device hasattr tensor device type tensor device type = meta tensor device type cuda default cuda calc_conv_nd_return_shape input_tensor torch Tensor weight torch Tensor stride Union list int int padding Union list int int dilation Union list int int is_transposed bool groups int output_padding Optional Union list int int = None _formula ln int p int d int k int s int - int Formula apply calculate length some dimension output See https pytorch org docs stable generated torch nn Conv d html Args ln length dimension p padding dim d dilation dim k kernel size dim s stride dim Returns The output length ln + p - d k - - s + _formula_transposed ln int p int d int k int s int op int - int Formula apply calculate length some dimension output transposed convolution used See https pytorch org docs stable generated torch nn ConvTranspose d html Args ln length dimension p padding dim d dilation dim k kernel size dim s stride dim op output padding dim Returns The output length ln - s - p + d k - + op + kernel_size = weight shape dims = input_tensor shape is_transposed out_channels = groups weight shape out_channels = weight shape weight shape groups = input_tensor shape raise RuntimeError Invalid channel dimensions ret_shape = input_tensor shape out_channels isinstance stride IntLike pyrefly ignore bad-assignment stride = stride len dims len stride == stride = stride len dims isinstance padding IntLike pyrefly ignore bad-assignment padding = padding len dims len padding == padding = padding len dims isinstance dilation IntLike pyrefly ignore bad-assignment dilation = dilation len dims len dilation == dilation = dilation len dims output_padding_list Optional list int = None output_padding isinstance output_padding IntLike pyrefly ignore bad-assignment output_padding_list = output_padding len dims len output_padding == output_padding_list = output_padding len dims output_padding_list = output_padding i range len dims If output_padding present we dealing transposed convolution output_padding_list ret_shape append _formula_transposed dims i pyrefly ignore index-error padding i pyrefly ignore index-error dilation i kernel_size i pyrefly ignore index-error stride i output_padding_list i ret_shape append pyrefly ignore index-error _formula dims i padding i dilation i kernel_size i stride i torch fx experimental symbolic_shapes sym_or torch _check sym_or x x ret_shape lambda f Given input size per channel list dims f Calculated output size per channel ret_shape f Output size too small ret_shape is_channels_last ten torch _prims_common suggest_memory_format ten == torch channels_last register_meta aten miopen_batch_norm default meta_miopen_batch_norm input_tensor torch Tensor weight torch Tensor bias Optional torch Tensor running_mean Optional torch Tensor running_var Optional torch Tensor training bool exponential_average_factor float epsilon float In batch norm output same shape input out_shape = input_tensor shape If tensor provided running_mean running_var then use If these provided then we shape weight tensor Similar how handled decomposition save_mean_shape = running_mean shape running_mean None weight shape save_var_shape = running_var shape running_var None weight shape pick_memory_format is_channels_last input_tensor torch channels_last input_tensor is_contiguous memory_format=torch contiguous_format torch contiguous_format torch contiguous_format out = input_tensor new_empty out_shape memory_format=pick_memory_format training save_mean = input_tensor new_empty save_mean_shape save_var = input_tensor new_empty save_var_shape save_mean = input_tensor new_empty save_var = input_tensor new_empty out save_mean save_var register_meta aten convolution default meta_conv input_tensor torch Tensor weight torch Tensor bias torch Tensor stride list int padding list int dilation list int is_transposed bool output_padding list int groups int shape_out = calc_conv_nd_return_shape input_tensor weight stride padding dilation is_transposed groups output_padding is_transposed None input_channels_dim = output_channels_dim = input_tensor size input_channels_dim == shape_out output_channels_dim = out = input_tensor new_empty shape_out out torch _C _has_mkldnn _meta_lib_dont_use_me_use_register_meta_for_mkldnn = torch library Library mkldnn IMPL Meta register_meta torch ops mkldnn _convolution_pointwise default meta_mkldnn_convolution_default input_tensor weight bias padding stride dilation groups attr scalars algorithm shape_out = calc_conv_nd_return_shape input_tensor weight stride padding dilation False groups out = input_tensor new_empty shape_out out_memory_format = torch channels_last input_tensor dim == out_memory_format = torch channels_last_ d out = out memory_format=out_memory_format type ignore call-overload out register_meta torch ops mkldnn _linear_pointwise default meta_linear_pointwise_default input_tensor weight bias attr scalars algorithm input_tensor new_empty input_tensor shape - weight shape torch _C has_mkl _meta_lib_dont_use_me_use_register_meta_for_mkl = torch library Library mkl IMPL Meta register_meta torch ops mkl _mkl_linear meta_mkl_linear input_tensor packed_weight orig_weight bias batch_size input_tensor new_empty input_tensor shape - orig_weight shape _meta_lib_dont_use_me_use_register_meta_for_onednn = torch library Library onednn IMPL Meta register_meta torch ops onednn qconv d_pointwise default register_meta torch ops onednn qconv_pointwise default meta_qconv_pointwise x x_scale x_zp w prepacked_weight w_scale w_zp bias stride padding dilation groups output_scale output_zero_point output_dtype attr scalars algorithm shape_out = calc_conv_nd_return_shape x w stride padding dilation False groups None output_dtype None output_dtype = x dtype assert output_dtype torch float torch bfloat torch uint torch int torch float _e m fn out = x new_empty shape_out dtype=output_dtype assert len shape_out Expect output d d d conv d d d format = torch contiguous_format torch channels_last torch channels_last_ d len shape_out out = out memory_format=format out register_meta torch ops onednn qconv d_pointwise binary meta_qconv d_pointwise_binary x x_scale x_zp w w_scale w_zp accum bias stride padding dilation groups output_scale output_zero_point output_dtype accum_scale accum_zero_point binary_op_name alpha unary_op_name unary_op_args unary_op_algorithm assert binary_op_name == sum accum register_meta torch ops onednn qlinear_pointwise default register_meta torch ops onednn qlinear_pointwise tensor meta_qlinear_pointwise x x_scale x_zp w w_scale w_zp bias output_scale output_zero_point output_dtype post_op_name post_op_args post_op_algorithm output_shape = list x shape The weight has been transposed during qlinear weight prepack process output_shape - = w shape assert output_dtype torch float torch bfloat torch int torch uint torch float _e m fn out = x new_empty output_shape dtype=output_dtype out register_meta torch ops onednn qlinear_pointwise binary register_meta torch ops onednn qlinear_pointwise binary_tensor meta_qlinear_pointwise_binary x x_scale x_zp w w_scale w_zp x_ bias output_scale output_zero_point output_dtype x _scale x _zp binary_op_name alpha unary_op_name unary_op_args unary_op_algorithm binary_op_name == sum x_ output_shape = list x shape The weight has been transposed during qlinear weight prepack process output_shape - = w shape assert output_dtype torch float torch bfloat torch uint torch int torch float _e m fn out = x new_empty output_shape dtype=output_dtype out register_meta torch ops onednn linear_dynamic_fp default register_meta torch ops onednn linear_relu_dynamic_fp default meta_linear_dynamic_fp x w bias output_shape = list x shape The weight has been transposed during qlinear weight prepack process output_shape - = w shape out = x new_empty output_shape out _meta_lib_dont_use_me_use_register_meta_for_quantized = torch library Library quantized IMPL Meta register_meta torch ops quantized max_pool d meta_quantized_max_pool d input kernel_size stride= padding= dilation= ceil_mode=False nInputPlane outputHeight outputWidth = max_pool d_checks_and_compute_shape input kernel_size stride padding dilation ceil_mode nbatch = input size - input dim == memory_format = torch channels_last input dim == size = nInputPlane outputHeight outputWidth size = nbatch nInputPlane outputHeight outputWidth torch empty size dtype=input dtype device=input device memory_format=memory_format register_meta torch ops quantized int mm_packed_weight_cpu meta_int mm_packed_weight_cpu x w q_group_size q_scale_and_zeros torch _check x dim == lambda f x must D tensor got x dim D torch _check w dim == lambda f w must D tensor got w dim D torch _check x dtype torch float torch float torch bfloat lambda f expected x f f bf got x dtype torch _check w dtype == torch uint lambda f expected w uint got w dtype torch _check q_group_size dtype == torch int lambda f q_group_size must int got q_group_size dtype torch _check q_scale_and_zeros dtype == x dtype lambda f q_scale_and_zeros must have same dtype x got q_scale_and_zeros dtype x new_empty x size w size dtype=x dtype check_dim_size aten src ATen TensorUtils cpp check_dim_size tensor dim dim_size size torch _check tensor dim == dim tensor shape dim_size == size lambda f Expected tensor dimension dim tensor size dim_size == size + f got dimension tensor dim tensor size dim_size = tensor shape dim_size register_meta aten avg_pool d default meta_avg_pool d input kernel_size stride= padding= ceil_mode=False count_include_pad=True divisor_override=None unpack name val torch _check len val lambda f avg_pool d name must either single int tuple two ints H = val W = H len val == val H W kH kW = unpack kernel_size kernel_size torch _check len stride lambda avg_pool d stride must either omitted single int tuple two ints torch _check input dtype torch uint torch uint torch uint torch uint lambda f avg_pool d implemented input dtype __str__ len stride == dH dW = kH kW len stride == dH dW = stride stride dH dW = unpack stride stride padH padW = unpack padding padding torch _check divisor_override None divisor_override = lambda divisor must zero nbatch = input size - input dim == nInputPlane = input size - inputHeight = input size - inputWidth = input size - outputHeight = pooling_output_shape inputHeight kH padH dH ceil_mode outputWidth = pooling_output_shape inputWidth kW padW dW ceil_mode memory_format = utils suggest_memory_format input pool d_shape_check input kH kW dH dW padH padW nInputPlane inputHeight inputWidth outputHeight outputWidth memory_format input dim == size = nInputPlane outputHeight outputWidth size = nbatch nInputPlane outputHeight outputWidth torch empty size dtype=input dtype device=input device memory_format=memory_format avg_pool d_backward_shape_check aten src ATen native Pool h avg_pool d_backward_shape_check input gradOutput nbatch kH kW dH dW padH padW nInputPlane inputHeight inputWidth outputHeight outputWidth mem_format pool d_shape_check input kH kW dH dW padH padW nInputPlane inputHeight inputWidth outputHeight outputWidth mem_format ndim = input dim nOutputPlane = nInputPlane check_dim_size gradOutput ndim ndim - nOutputPlane check_dim_size gradOutput ndim ndim - outputHeight check_dim_size gradOutput ndim ndim - outputWidth Don t override C++ registration register_meta aten avg_pool d_backward default meta_avg_pool d_backward gradOutput_ input kernel_size stride padding ceil_mode count_include_pad divisor_override From aten src ATen native AveragePool d cpp structured kernel meta func torch _check len kernel_size == len kernel_size == lambda avg_pool d kernel_size must either single int tuple two ints kH = kernel_size kW = kH len kernel_size == kernel_size torch _check len stride == len stride == len stride == lambda avg_pool d stride must either omitted single int tuple two ints dH = kH len stride == stride dW = kW len stride == dH len stride == stride torch _check len padding == len padding == lambda avg_pool d padding must either single int tuple two ints padH = padding padW = padH len padding == padding torch _check divisor_override None divisor_override = lambda divisor must zero input_size = input shape nbatch = input_size - input dim == nInputPlane = input_size - inputHeight = input_size - inputWidth = input_size - outputHeight = pooling_output_shape inputHeight kH padH dH ceil_mode outputWidth = pooling_output_shape inputWidth kW padW dW ceil_mode mem_format = utils suggest_memory_format input avg_pool d_backward_shape_check input gradOutput_ nbatch kH kW dH dW padH padW nInputPlane inputHeight inputWidth outputHeight outputWidth mem_format torch empty input_size dtype=input dtype device=input device memory_format=mem_format register_meta aten avg_pool d out_wrapper meta_avg_pool d input kernel_size stride= padding= ceil_mode=False count_include_pad=True divisor_override=None torch _check len kernel_size lambda avg_pool d kernel_size must single int tuple three ints kT = kernel_size kH = kT len kernel_size == kernel_size kW = kT len kernel_size == kernel_size torch _check stride len stride lambda avg_pool d stride must omitted single int tuple three ints torch _check input dtype torch uint torch uint torch uint torch uint lambda f avg_pool d implemented input dtype __str__ dT = kT stride stride dH = kH stride dT len stride == stride dW = kW stride dT len stride == stride torch _check len padding lambda avg_pool d padding must single int tuple three ints padT = padding padH = padT len padding == padding padW = padT len padding == padding torch _check input ndim lambda non-empty D D batch mode tensor expected input torch _check divisor_override divisor_override = lambda divisor must zero nbatch = input size nslices = input size - itime = input size - iheight = input size - iwidth = input size - otime = pooling_output_shape itime kT padT dT ceil_mode oheight = pooling_output_shape iheight kH padH dH ceil_mode owidth = pooling_output_shape iwidth kW padW dW ceil_mode pool d_shape_check input nslices kT kH kW dT dH dW padT padH padW itime iheight iwidth otime oheight owidth avg_pool d check_input_size=True input ndim == input new_empty nslices otime oheight owidth input new_empty nbatch nslices otime oheight owidth register_meta aten avg_pool d_backward out_wrapper grad_input meta_avg_pool d_backward grad_output input kernel_size stride padding ceil_mode count_include_pad divisor_override torch _check len kernel_size lambda avg_pool d kernel_size must single int tuple three ints kT = kernel_size kH = kT len kernel_size == kernel_size kW = kT len kernel_size == kernel_size torch _check stride len stride lambda avg_pool d stride must omitted single int tuple three ints dT = kT stride stride dH = kH stride dT len stride == stride dW = kW stride dT len stride == stride torch _check len padding lambda avg_pool d padding must single int tuple three ints padT = padding padH = padT len padding == padding padW = padT len padding == padding torch _check input ndim lambda non-empty D D batch mode tensor expected input torch _check divisor_override divisor_override = lambda divisor must zero nslices = input size - itime = input size - iheight = input size - iwidth = input size - otime_for_shape_check = pooling_output_shape itime kT padT dT ceil_mode oheight_for_shape_check = pooling_output_shape iheight kH padH dH ceil_mode owidth_for_shape_check = pooling_output_shape iwidth kW padW dW ceil_mode avg_pool d_backward_shape_check input grad_output nslices kT kH kW dT dH dW padT padH padW itime iheight iwidth otime_for_shape_check oheight_for_shape_check owidth_for_shape_check avg_pool d_backward input new_empty input shape register_meta aten _adaptive_avg_pool d default meta_adaptive_avg_pool d output_size torch _check ndim == ndim == lambda f Expected D D tensor got shape output_shape = shape - + tuple output_size memory_format = utils suggest_memory_format need set memory_format preserve memory format input channel last input should have channel last output torch empty output_shape dtype=self dtype device=self device memory_format=memory_format register_meta aten _adaptive_avg_pool d default meta_adaptive_avg_pool d output_size torch _check ndim == ndim == lambda f Expected D D tensor got shape new_empty shape - + tuple output_size register_meta aten _adaptive_avg_pool d_backward default meta__adaptive_avg_pool d_backward grad_out ndim = grad_out ndim i range ndim torch _check grad_out size i lambda f adaptive_avg_pool d_backward Expected grad_output have non-zero \ size non-batch dimensions grad_out shape dimension i being empty torch _check ndim == ndim == lambda f adaptive_avg_pool d_backward Expected D D tensor got shape torch _check dtype == grad_out dtype lambda f expected dtype dtype ` grad_output ` got dtype grad_out dtype memory_format = torch contiguous_format is_channels_last memory_format = torch channels_last new_empty shape memory_format=memory_format register_meta aten _adaptive_avg_pool d_backward out_wrapper grad_input meta__adaptive_avg_pool d_backward grad_output _adaptive_pool_empty_output_check grad_output adaptive_avg_pool d_backward torch empty_like memory_format=torch legacy_contiguous_format _adaptive_pool_empty_output_check grad_output Tensor arg_name str ndim = grad_output ndim i range ndim torch _check grad_output size i lambda f arg_name Expected grad_output have non-zero size non-batch dimensions f grad_output has sizes grad_output shape dimension i being empty register_meta aten adaptive_max_pool d out_wrapper out indices meta_adaptive_max_pool d input output_size ndim = input ndim torch _check ndim lambda f adaptive_max_pool d Expected D D tensor got input shape i range ndim torch _check input size i lambda f adaptive_max_pool d Expected input have non-zero size non-batch dimensions f input has sizes input shape dimension i being empty torch _check len output_size == lambda adaptive_max_pool d internal error output_size size must dimH = sizeB = sizeD = input ndim == sizeB = input size dimH += sizeD = input size dimH - osizeH osizeW = output_size input ndim == out_shape = sizeD osizeH osizeW out = input new_empty out_shape indices = input new_empty out_shape dtype=torch int out indices out_shape = sizeB sizeD osizeH osizeW type ignore assignment memory_format = utils suggest_memory_format input out = input new_empty out_shape memory_format=memory_format indices = input new_empty out_shape dtype=torch int memory_format=memory_format out indices register_meta aten adaptive_max_pool d_backward out_wrapper grad_input meta_adaptive_max_pool d_backward grad_output input indices ndim = grad_output ndim torch _check ndim lambda f adaptive_max_pooling d_backward Expected D D grad_output got grad_output shape _adaptive_pool_empty_output_check grad_output adaptive_max_pool d_backward torch _check input dtype == grad_output dtype lambda f expected dtype input dtype ` grad_output ` got dtype grad_output dtype memory_format = utils suggest_memory_format input input new_empty input shape memory_format=memory_format register_meta aten adaptive_max_pool d out_wrapper out indices meta_adaptive_max_pool d input output_size ndim = input ndim torch _check ndim lambda f adaptive_max_pool d Expected D D tensor got input shape i range ndim torch _check input size i lambda f adaptive_max_pool d Expected input have non-zero size non-batch dimensions f input has sizes input shape dimension i being empty torch _check len output_size == lambda adaptive_max_pool d internal error output_size size must dimD = sizeB = sizeD = ndim == sizeB = input size dimD += sizeD = input size dimD osizeT osizeH osizeW = output_size ndim == out_shape = sizeD osizeT osizeH osizeW out_shape = sizeB sizeD osizeT osizeH osizeW type ignore assignment out = input new_empty out_shape indices = input new_empty out_shape dtype=torch int out indices register_meta aten adaptive_max_pool d_backward out_wrapper grad_input meta_adaptive_max_pool d_backward grad_output input indices _adaptive_pool_empty_output_check grad_output adaptive_max_pool d_backward input new_empty input shape register_meta aten repeat_interleave Tensor meta_repeat_interleave_Tensor repeats output_size=None output_size None raise RuntimeError cannot repeat_interleave meta tensor without output_size repeats new_empty output_size register_meta aten complex default aten complex out out_wrapper meta_complex real imag assert real dtype is_floating_point assert imag dtype is_floating_point result = elementwise_meta real corresponding_complex_dtype real dtype imag corresponding_complex_dtype imag dtype type_promotion=ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT result register_meta aten nonzero_static default aten nonzero_static out out_wrapper nonzero_static size fill_value int = - new_empty size dim dtype=torch long register_meta torch ops aten nonzero default torch ops aten nonzero out out_wrapper nonzero torch _check_not_implemented exp_config meta_nonzero_assume_all_nonzero lambda The register_meta function torch nonzero raises unimplemented default correct data-independent implementation does exist This implementation returns fake value assuming all elements tensor non-zero To enable registration please set torch fx experimental _config meta_nonzero_assume_all_nonzero True torch empty_strided numel dim numel dtype=torch long device=self device register_meta aten index Tensor aten _unsafe_index Tensor meta_index_Tensor indices torch _check bool indices lambda least one index must provided aten index internal advanced indexing implementation checkIndexTensorTypes expandTensors result list Optional Tensor = i index enumerate indices index None torch _check index dtype torch long torch int torch int torch bool lambda tensors used indices must long int byte bool tensors index dtype torch int torch bool nonzero = index nonzero k = len result torch _check_index k + index ndim = ndim lambda f too many indices tensor dimension ndim j range index ndim torch _check_index index shape j == shape k + j lambda f The shape mask index shape index i f does match shape indexed tensor shape index k + j result append nonzero select j result append index result append index indices = result torch _check len indices = ndim lambda f too many indices tensor dimension ndim got len indices expand_outplace torch _refs refs avoid cycle mypy indices = list refs _maybe_broadcast indices add missing null tensors while len indices ndim indices append None hasContiguousSubspace true all non-null tensors adjacent See https numpy org doc stable user basics indexing html#combining-advanced-and-basic-indexing https stackoverflow com questions why-does-numpy-mixed-basic-advanced-indexing-depend-on-slice-adjacency state = has_contiguous_subspace = False index indices state == index None state = state == index None state = index None break has_contiguous_subspace = True transposeToFront This logic causes newly inserted dimensions show up beginning tensor they re contiguous has_contiguous_subspace dims = transposed_indices = i index enumerate indices index None dims append i transposed_indices append index i index enumerate indices index None dims append i transposed_indices append index = permute dims indices = transposed_indices AdvancedIndex AdvancedIndex Now we can assume indices have contiguous subspace This simplified AdvancedIndex which goes more effort put input indices form so TensorIterator can take them If we write ref probably logic should get implemented before_shape list int = after_shape list int = replacement_shape list int = dim index enumerate indices index None replacement_shape after_shape append shape dim before_shape append shape dim replacement_shape = list index shape _restride_src This follows restride_src TensorAdvancedIndexing cpp shape = before_shape + replacement_shape + after_shape strides = list stride pyrefly ignore unsupported-operation strides len before_shape len shape - len after_shape = len replacement_shape as_strided shape strides out = new_empty before_shape + replacement_shape + after_shape torch fx experimental symbolic_shapes guard_or_false guard_or_false numel == No need worry about output strides empty out Try follow eager decide output stride based Note perm here reverse perm_ decided TensorIteratorBase reorder_dimensions restrided_self = _restride_src perm _ = utils compute_elementwise_output_logical_to_physical_perm restrided_self Follow TensorIteratorBase allocate_or_resize_outputs list perm = list range len perm perm_shape = utils apply_perm out shape perm new_stride = utils make_contiguous_strides_for perm_shape new_stride = utils apply_perm new_stride utils invert_perm perm out = out as_strided out size new_stride out register_meta aten convolution_backward default meta_convolution_backward grad_output_ input_ weight_ bias_sizes_opt stride padding dilation transposed output_padding groups output_mask High level logic taken slow_conv d_backward_cpu which should representative all convolution_backward impls backend_grad_input = None backend_grad_weight = None backend_grad_bias = None output_mask backend_grad_input = grad_output_ new_empty input_ size output_mask backend_grad_weight = grad_output_ new_empty weight_ size output_mask backend_grad_bias = grad_output_ new_empty bias_sizes_opt backend_grad_input backend_grad_weight backend_grad_bias register_meta aten addbmm default aten addbmm out out_wrapper exact_dtype=True meta_addbmm batch batch beta= alpha= dim = batch size dim = batch size = expand dim dim torch _check batch dim == lambda batch must D tensor torch _check batch dim == lambda batch must D tensor torch _check batch size == batch size lambda f batch batch must have same number batches got batch size batch size torch _check batch size == batch size lambda f Incompatible matrix sizes bmm batch size x batch size f batch size x batch size torch _check size == dim size == dim lambda tensor does match matmul output shape new_empty size register_meta aten randint_like Tensor meta_randint_like high kwargs new_empty size register_meta aten _fused_adam_ default aten _fused_adamw_ default meta__fused_adam_ grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps lr beta beta weight_decay eps amsgrad maximize grad_scale=None found_inf=None l grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps torch _check isinstance l list lambda f exponent must tensor list got type l register_meta aten _fused_adam default meta__fused_adam grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps lr beta beta weight_decay eps amsgrad maximize grad_scale=None found_inf=None l grads exp_avgs exp_avg_sqs max_exp_avg_sqs state_steps torch _check isinstance l list lambda f exponent must tensor list got type l empty_like_list tensor_list torch empty_like t t tensor_list empty_like_list empty_like_list grads empty_like_list exp_avgs empty_like_list exp_avg_sqs empty_like_list max_exp_avg_sqs register_meta aten _int_mm out_wrapper meta__int_mm b torch _check dim == lambda must D tensor torch _check b dim == lambda b must D tensor torch _check dtype torch int lambda f expected int got dtype torch _check b dtype torch int lambda f expected mat int got b dtype torch _check size == b size lambda f Incompatible matrix sizes _int_mm size x size f b size x b size new_empty size b size dtype=torch int register_meta aten _convert_weight_to_int pack meta__convert_weight_to_int pack w inner_k_tiles torch _check w dim == lambda w must D tensor torch _check w dtype torch uint lambda f expected w uint got w dtype n = w size k = w size w n k uint w new_empty n k inner_k_tiles inner_k_tiles dtype=torch int register_meta aten _convert_weight_to_int pack_for_cpu meta__convert_weight_to_int pack_for_cpu w inner_k_tiles torch _check w dim == lambda w must D tensor torch _check w dtype torch int lambda f expected w int got w dtype n = w size k = w size w n k int w new_empty n k dtype=torch uint register_meta aten _weight_int pack_mm meta__weight_int pack_mm x w q_group_size q_scale_and_zeros torch _check x dim == lambda x must D tensor torch _check w dim == lambda w must D tensor torch _check x dtype torch float torch float torch bfloat lambda f expected x f f bf got x dtype torch _check w dtype torch int lambda f expected w int got w dtype x new_empty x size w size dtype=x dtype register_meta aten _weight_int pack_mm_for_cpu meta__weight_int pack_mm_for_cpu x w q_group_size q_scale_and_zeros torch _check x dim == lambda x must D tensor torch _check w dim == lambda w must D tensor torch _check x dtype torch float torch float torch bfloat lambda f expected x f f bf got x dtype torch _check w dtype torch uint lambda f expected w uint got w dtype x new_empty x size w size dtype=x dtype register_meta aten _weight_int pack_mm_with_scales_and_zeros _weight_int pack_mm_with_scales_and_zeros x w q_group_size qScale qZeros torch _check x dim == lambda x must D tensor torch _check w dim == lambda w must D tensor torch _check x dtype torch float torch float torch bfloat lambda f expected x f f bf got x dtype torch _check w dtype torch int lambda f expected w int got w dtype x new_empty x size w size dtype=x dtype kai_roundup int b int - int + b - b b get_kai_packed_weight_size n_bits N K groupsize n_bits == groupsize == K channelwise dotprod params only x x _neon_dotprod kai_nr = kai_kr = kai_sr = kai_num_bytes_sum_rhs = sizeof int _t kai_num_bytes_multiplier_rhs = sizeof float kai_num_bytes_bias = sizeof float kai_k_roundedup k kr sr Since we pack float int value end row we must make sure k multiple alignment kr_sr_roundedup = kai_roundup kr sr kai_roundup k kr_sr_roundedup kai_get_rhs_packed_stride_rhs_pack_nxk_qsi cxp_qsu cxs s k nr kr sr k_internal = kai_k_roundedup k kr sr assert k_internal == k_internal must even nr k_internal + kai_num_bytes_multiplier_rhs + kai_num_bytes_sum_rhs + kai_num_bytes_bias kai_get_rhs_packed_size_rhs_pack_nxk_qsi cxp_qsu cxs s n k nr kr sr num_rows = kai_roundup n nr nr num_rows kai_get_rhs_packed_stride_rhs_pack_nxk_qsi cxp_qsu cxs s k nr kr sr kai_get_rhs_packed_size_rhs_pack_nxk_qsi cxp_qsu cxs s N K kai_nr kai_kr kai_sr groupsize == K groupsize == groupwise kai_nr = kai_kr = kai_sr = kai_num_bytes_sum_rhs = kai_num_bytes_bias = kai_nr_multiple_of = kai_bl_multiple_of = kai_get_rhs_packed_size_rhs_pack_nxk_qsi c p_qsu c s s n k nr kr sr bl assert bl kr == assert nr kai_nr_multiple_of == assert bl kai_bl_multiple_of == num_rows = kai_roundup n nr nr num_rows kai_get_rhs_packed_stride_rhs_pack_nxk_qsi c p_qsu c s s k nr kr sr bl kai_get_rhs_packed_stride_rhs_pack_nxk_qsi c p_qsu c s s k nr kr sr bl assert bl kr == assert nr kai_nr_multiple_of == assert bl kai_bl_multiple_of == kr sr unused calculation num_bytes_multiplier_rhs = kai_get_bf _datatype_size_in_bytes num_blocks_per_row = kai_num_blocks_per_row k bl num_bytes_per_block = kai_num_bytes_per_block bl num_bytes_multiplier_rhs nr num_bytes_per_block num_blocks_per_row + kai_num_bytes_sum_rhs + kai_num_bytes_bias This function returns size these datatypes stored enum We modify just bf datatype https gitlab arm com kleidi kleidiai - blob main kai kai_common h ref_type=heads#L kai_get_bf _datatype_size_in_bytes bytes kai_num_blocks_per_row k bl assert bl kai_bl_multiple_of == kai_roundup k bl bl kai_num_bytes_per_block bl num_bytes_multiplier_rhs assert bl kai_bl_multiple_of == bl + num_bytes_multiplier_rhs kai_get_rhs_packed_size_rhs_pack_nxk_qsi c p_qsu c s s N K kai_nr kai_kr kai_sr groupsize register_meta aten _dyn_quant_pack_ bit_weight meta__dyn_quant_pack_ bit_weight weights scales_zeros bias Optional Tensor block_size in_features out_features torch _check weights dtype torch uint lambda f expected w uint got weights dtype torch backends kleidiai is_available block_size == in_features scales_zeros dtype == torch float block_size in_features block_size == in_features block_size == scales_zeros dtype == torch bfloat packed_weight_size = get_kai_packed_weight_size out_features in_features block_size weights new_empty int packed_weight_size dtype=torch uint packed_weight_size = weights numel + scales_zeros numel weights new_empty packed_weight_size dtype=torch float register_meta aten _dyn_quant_matmul_ bit meta__dyn_quant_matmul_ bit inp packed_weights block_size in_features out_features torch _check inp dim == lambda input must D tensor torch _check inp dtype == torch float lambda f expected input f got inp dtype M = inp size inp new_empty M out_features dtype=inp dtype register_meta aten _weight_int pack_mm meta__weight_int pack_mm x w q_scales torch _check x dim == lambda x must D tensor torch _check x dtype torch float torch float torch bfloat lambda f expected x f f bf got x dtype torch _check w dim == lambda w must D tensor torch _check w dtype torch int lambda f expected w int got w dtype x new_empty x size w size dtype=x dtype register_meta aten _cdist_forward default meta_cdist_forward x x p compute_mode torch _check x dim = lambda f cdist only supports least D tensors X got x dim D torch _check x dim = lambda f cdist only supports least D tensors X got x dim D torch _check x size - == x size - lambda f X X must have same number columns X x size - X x size - torch _check utils is_float_dtype x dtype lambda f cdist only supports floating-point dtypes X got x dtype torch _check utils is_float_dtype x dtype lambda f cdist only supports floating-point dtypes X got x dtype torch _check p = lambda cdist only supports non-negative p values torch _check compute_mode None lambda f possible modes None compute_mode r = x size - r = x size - batch_tensor = x shape - batch_tensor = x shape - output_shape = list torch broadcast_shapes batch_tensor batch_tensor output_shape extend r r x new_empty output_shape register_meta aten _cdist_backward out_wrapper meta_cdist_backward grad x x p cdist c = x shape - r = x shape - r = x shape - batch_tensor = x shape - batch_tensor = x shape - expand_batch_portion = list torch broadcast_shapes batch_tensor batch_tensor tensor _expand_size = expand_batch_portion copy tensor _expand_size extend r c batch_product = math prod expand_batch_portion r == r == c == batch_product == torch zeros_like x tensor _expand_size = list x shape x = x expand tensor _expand_size torch empty_like x memory_format=torch contiguous_format NB This meta function accepts non-meta arguments When behavior originally introduced accidental now load bearing people using so they can conveniently test code involving embeddings feeding CPU tensor inputs meta device EmbeddingBag module register_meta aten _embedding_bag default meta_embedding_bag weight indices offsets scale_grad_by_freq=False mode= sparse=False per_sample_weights=None include_last_offset=False padding_idx=- torch _check indices dtype torch long torch int lambda f expected indices long int got indices dtype torch _check offsets dtype torch long torch int lambda f expected offsets long int got offsets dtype torch _check utils is_float_dtype weight dtype lambda f expected weight floating point type got weight dtype num_bags = offsets size include_last_offset torch _check num_bags = lambda include_last_offset numBags should least num_bags -= output = weight new_empty num_bags weight size per_sample_weights None torch _check mode == MODE_SUM lambda embedding_bag per_sample_weights only supported mode= sum torch _check per_sample_weights ndim == lambda f expected per_sample_weights D tensor got per_sample_weights ndim D torch _check per_sample_weights numel == indices numel lambda f expected per_sample_weights numel per_sample_weights numel f same indices numel indices numel is_fast_path_index_select_scale src scale output padding_idx is_fast_path_index_select src output padding_idx scale stride == is_fast_path_index_select src output padding_idx src dtype == torch float src dtype == torch half src stride == output stride == padding_idx is_fast_path src scale output padding_idx scale None is_fast_path_index_select_scale src scale output padding_idx is_fast_path_index_select src output padding_idx device_hint offsets = cpu offset bag = indices new_empty indices size bag_size = indices new_empty offsets size mode == MODE_MAX max_indices = indices new_empty num_bags weight size max_indices = indices new_empty fast_path_sum = is_fast_path weight per_sample_weights output padding_idx mode MODE_MEAN MODE_MAX fast_path_sum offset bag = offsets new_empty indices size offset bag = offsets new_empty bag_size = offsets new_empty num_bags This part logic comes make_max_indices_out EmbeddingBag cpp numBags = offsets shape mode == MODE_MAX include_last_offset torch _check numBags = lambda include_last_offset numBags should least numBags -= max_indices = offsets new_empty numBags weight shape max_indices = offsets new_empty bag_size size output offset bag bag_size max_indices register_meta aten _embedding_bag_forward_only default meta_embedding_bag_forward_only weight indices offsets args output offset bag bag_size max_indices = meta_embedding_bag weight indices offsets args device_hint offsets == cpu bag_size = offsets new_empty offsets size output offset bag bag_size max_indices _get_reduction_dtype input dtype promote_int_to_long=True specified dtype takes precedence dtype dtype input dtype is_floating_point input dtype is_complex input dtype promote_int_to_long torch long input dtype register_meta aten nansum default aten nansum out out_wrapper meta_nansum input dims=None keepdim=False dtype=None output_dtype = _get_reduction_dtype input dtype promote_int_to_long=True dims = utils reduction_dims input shape dims output_shape = _compute_reduction_shape input dims keepdim input new_empty output_shape dtype=output_dtype register_meta aten median default aten nanmedian default meta_median input output_shape = utils compute_reduction_output_shape input shape tuple range input dim input new_empty output_shape register_meta aten median dim aten median dim_values aten nanmedian dim aten nanmedian dim_values aten mode default aten mode values out_wrapper values indices meta_median_mode_dim input dim=- keepdim=False device_hint input == cuda utils alert_not_deterministic median CUDA indices output dim = utils reduction_dims input shape dim output_shape = _compute_reduction_shape input dim keepdim input new_empty output_shape input new_empty output_shape dtype=torch long register_meta aten logical_not_ default meta_logical_not_ register_meta aten repeat default meta_repeat repeats torch _check len repeats = dim lambda Number dimensions repeat dims can smaller than number dimensions tensor i rep enumerate repeats torch _check rep = lambda f Repeats cannot negative found rep index i Add new leading dimensions tensor number target dimensions larger than number source dimensions num_new_dimensions = len repeats - dim padded_size = num_new_dimensions + tuple shape target_size = padded_size i repeats i i range len repeats new_empty target_size register_meta aten zero_ default meta_zero_ register_meta aten mul_ Scalar aten div_ Scalar aten mul_ Tensor aten div_ Tensor aten logical_and_ default aten logical_or_ default aten logical_xor_ default meta_binop_inplace other isinstance other torch Tensor check_inplace_broadcast shape other shape register_meta aten add_ Scalar aten sub_ Scalar aten add_ Tensor aten sub_ Tensor meta_binop_inplace_alpha other alpha= Some checks inplace ops Checks promotion rules some dtypes int add sub_ float bool add sub_ others rejected Promoting these in-place operations would require reallocating copying over elements hence allowed Checks alpha param is_integeric arg isinstance arg TensorLike utils is_integer_dtype arg dtype isinstance arg IntLike is_floatic arg isinstance arg TensorLike utils is_float_dtype arg dtype isinstance arg FloatLike is_booleanic arg isinstance arg TensorLike utils is_boolean_dtype arg dtype isinstance arg BoolLike Do allow int+float- int in-place is_integeric is_floatic other raise RuntimeError Promotion int add sub_ float in-place ops possible due element size change Do allow bool+other- bool in-place is_booleanic is_booleanic other raise RuntimeError Promotion book add sub_ others in-place ops possible due element size change isinstance other torch Tensor check_inplace_broadcast shape other shape register_meta aten add Scalar aten sub Scalar meta_binop_alpha other alpha= elementwise_meta other type_promotion=ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT register_meta aten round default aten round decimals meta_round kwargs elementwise_meta type_promotion=ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT shift_dtype_check fn_name val torch _check utils is_integer_dtype dtype lambda f fn_name Expected input tensor have integral dtype Got dtype isinstance val torch Tensor torch _check utils is_integer_dtype val dtype lambda f fn_name Expected shift value have integral dtype Got val dtype torch _check isinstance val IntLike lambda f fn_name Expected shift value int Got val register_meta aten __rshift__ Tensor aten __rshift__ Scalar meta_rshifts other shift_dtype_check rshift other elementwise_meta other type_promotion=ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT register_meta aten __lshift__ Tensor aten __lshift__ Scalar meta_lshifts other shift_dtype_check lshift other elementwise_meta other type_promotion=ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT register_meta aten zero default meta_zero new_empty shape register_meta aten fill_ Tensor aten fill_ Scalar meta_fill_ val register_meta aten fill Tensor aten fill Scalar meta_fill val torch empty_like register_meta aten relu_ default meta_relu_ register_meta aten _add_relu Tensor out_wrapper meta__add_relu other alpha= - Tensor elementwise_meta other type_promotion=ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT register_meta aten rrelu_with_noise out_wrapper meta_rrelu_with_noise noise lower= upper= training=False generator=None torch empty_like register_meta aten rrelu_with_noise_functional meta_rrelu_with_noise_functional noise lower= upper= training=False generator=None torch empty_like torch empty_like noise register_meta aten rrelu_with_noise_ meta_rrelu_with_noise_ lower= upper= training=False generator=None register_meta aten index_put default aten _unsafe_index_put default meta_index_put indices values accumulate=False torch empty_like register_meta aten masked_fill_ Scalar meta_masked_fill_ mask value check_inplace_broadcast shape mask shape register_meta aten _masked_scale default meta__masked_scale mask scale masked_scale = new_empty size memory_format=utils suggest_memory_format masked_scale register_meta aten masked_scatter_ meta_masked_scatter_ mask source torch _check mask dtype torch bool torch uint lambda Mask must bool uint torch _check dtype == source dtype lambda masked_scatter expected source have same f dtypes got dtype source dtype register_meta aten masked_scatter out_wrapper meta_masked_scatter mask source mask = _maybe_broadcast mask output = torch empty_like memory_format=torch contiguous_format meta_masked_scatter_ output mask source register_meta aten masked_scatter_backward meta_masked_scatter_backward mask sizes new_empty sizes register_meta aten index_put_ default meta_index_put_ indices values accumulate=False common_meta_baddbmm_bmm batch batch is_bmm self_baddbmm=None out_dtype=None torch fx experimental symbolic_shapes sym_and sym_eq torch _check batch dim == lambda batch must D tensor torch _check batch dim == lambda batch must D tensor batch _sizes = batch size batch _sizes = batch size bs = batch _sizes contraction_size = batch _sizes res_rows = batch _sizes res_cols = batch _sizes output_size = bs res_rows res_cols torch _check sym_and sym_eq batch _sizes bs sym_eq batch _sizes contraction_size lambda f Expected size first two dimensions batch tensor bs f contraction_size got batch _sizes batch _sizes out_dtype supported_out_dtype = batch dtype == torch float batch dtype == torch bfloat out_dtype == torch float torch _check out_dtype == batch dtype supported_out_dtype lambda out_dtype only supported torch float output float bfloat inputs same input dtypes output = batch new_empty output_size out_dtype TODO handle out output = batch new_empty output_size is_bmm self_baddbmm None torch _check self_baddbmm dim == lambda must D tensor torch _check sym_eq self_baddbmm size output_size lambda f Expected input tensor shape shape output_size got shape self_baddbmm size output register_meta aten bmm default meta_bmm mat common_meta_baddbmm_bmm mat True register_meta aten bmm dtype meta_bmm_dtype mat out_dtype common_meta_baddbmm_bmm mat True out_dtype=out_dtype div_rtn x y q = x y r = x y WARNING explicit bool conversion here necessary would fixed SymBool r = bool r = bool y q -= q pooling_output_shape_pad_lr inputSize kernelSize pad_l pad_r stride dilation ceil_mode outputSize = div_rtn inputSize + pad_l + pad_r - dilation kernelSize - - + stride - ceil_mode stride + ceil_mode outputSize - stride = inputSize + pad_l outputSize -= outputSize pooling_output_shape inputSize kernelSize pad stride dilation ceil_mode torch _check stride = lambda stride should zero torch _check pad = lambda f pad must non-negative got pad pad torch _check pad = kernelSize - dilation + lambda f pad should most half effective kernel size got pad= pad f kernel_size= kernelSize dilation= dilation pooling_output_shape_pad_lr inputSize kernelSize pad pad stride dilation ceil_mode pool d_shape_check input kH kW dH dW padH padW dilationH dilationW nInputPlane inputHeight inputWidth outputHeight outputWidth memory_format ndim = input dim nOutputPlane = nInputPlane torch _check kW kH lambda f kernel size should greater than zero got kH kH kW kW torch _check dW dH lambda f stride should greater than zero got dH dH dW dW torch _check dilationH dilationW lambda f dilation should greater than zero got dilationH dilationH dilationW dilationW valid_dims = input size = input size = memory_format == torch channels_last torch _check ndim == valid_dims input size = lambda Expected D batch mode tensor expected input channels_last layout f optional dim batch size input got input size torch _check ndim == input size = valid_dims ndim == valid_dims input size = lambda f Expected D D batch mode tensor optional dim batch size input got input size torch _check kW = padW kH = padH lambda pad should smaller than equal half kernel size got f padW = padW padH = padH kW = kW kH = kH torch _check outputWidth = outputHeight = lambda f Given input size nInputPlane x inputHeight x inputWidth f Calculated output size nOutputPlane x outputHeight x outputWidth Output size too small pool d_shape_check input Tensor nslices int kT int kH int kW int dT int dH int dW int pT int pH int pW int dilationT int dilationH int dilationW int itime int iheight int iwidth int otime int oheight int owidth int fn_name str check_input_size bool = False ndim = input ndim torch _check kT kW kH lambda f kernel size should greater than zero got f kT kT kH kH kW kW torch _check dT dW dH lambda f stride should greater than zero got dT dT dH dH dW dW torch _check dilationT dilationW dilationH lambda f dilation should greater than zero got f dilationT dilationT dilationH dilationH dilationW dilationW torch _check ndim lambda f fn_name Expected D D tensor input got input shape i range ndim ndim == i == size batch-dim can continue torch _check input size i lambda f fn_name Expected input s non-batch dimensions have positive length f input has shape input shape f non-batch dimension input size i has length zero check_input_size AveragePool d torch _check itime = kT iheight = kH iwidth = kW lambda f input image T itime H iheight W iwidth smaller than f kernel size kT kT kH kH kW kW torch _check kT = pT kW = pW kH = pH lambda f pad should smaller than equal half kernel size got f kT kT kW kW kH kH padT pT padW pW padH pH torch _check otime = owidth = oheight = lambda f Given input size nslices x itime x iheight x iwidth f Calculated output size nslices x otime x oheight x owidth f Output size too small max_pool d_backward_shape_check input grad_output indices nslices kT kH kW dT dH dW pT pH pW dilationT dilationH dilationW itime iheight iwidth otime oheight owidth fn_name ndim = input ndim pool d_shape_check input nslices kT kH kW dT dH dW pT pH pW dilationT dilationH dilationW itime iheight iwidth otime oheight owidth fn_name check_dim_size grad_output ndim ndim - nslices check_dim_size grad_output ndim ndim - otime check_dim_size grad_output ndim ndim - oheight check_dim_size grad_output ndim ndim - owidth check_dim_size indices ndim ndim - nslices check_dim_size indices ndim ndim - otime check_dim_size indices ndim ndim - oheight check_dim_size indices ndim ndim - owidth avg_pool d_backward_shape_check input Tensor grad_output Tensor nslices int kT int kH int kW int dT int dH int dW int pT int pH int pW int itime int iheight int iwidth int otime int oheight int owidth int fn_name str ndim = input ndim pool d_shape_check input nslices kT kH kW dT dH dW pT pH pW itime iheight iwidth otime oheight owidth fn_name True check_dim_size grad_output ndim ndim - nslices check_dim_size grad_output ndim ndim - otime check_dim_size grad_output ndim ndim - oheight check_dim_size grad_output ndim ndim - owidth max_pool d_checks_and_compute_shape input kernel_size stride padding dilation ceil_mode Reference aten src ATen native DilatedMaxPool d cpp unpack name val torch _check len val lambda f max_pool d name must either single int tuple two ints H = val W = H len val == val H W kH kW = unpack kernel_size kernel_size torch _check len stride lambda max_pool d stride must either omitted single int tuple two ints len stride == dH dW = kH kW dH dW = unpack stride stride padH padW = unpack padding padding dilationH dilationW = unpack dilation dilation nInputPlane = input size - inputHeight = input size - inputWidth = input size - memory_format = utils suggest_memory_format input memory_format == torch channels_last torch _check input dim == lambda non-empty D batch mode tensor expected input channels_last layout memory_format == torch contiguous_format torch _check input dim lambda non-empty D D batch mode tensor expected input torch _check False lambda Unsupported memory format Supports only ChannelsLast Contiguous outputHeight = pooling_output_shape inputHeight kH padH dH dilationH ceil_mode outputWidth = pooling_output_shape inputWidth kW padW dW dilationW ceil_mode pool d_shape_check input kH kW dH dW padH padW dilationH dilationW nInputPlane inputHeight inputWidth outputHeight outputWidth memory_format nInputPlane outputHeight outputWidth register_meta aten max_pool d_with_indices_backward default meta_max_pool d_with_indices_backward grad_output kernel_size stride padding dilation ceil_mode indices nInputPlane outputHeight outputWidth = max_pool d_checks_and_compute_shape kernel_size stride padding dilation ceil_mode torch _check dtype == grad_output dtype lambda f Expected dtype dtype ` gradOutput ` got dtype grad_output dtype nOutputPlane = nInputPlane ndim = ndim _check_dim_size t check_dim_size t ndim ndim - nOutputPlane check_dim_size t ndim ndim - outputHeight check_dim_size t ndim ndim - outputWidth _check_dim_size grad_output _check_dim_size indices memory_format = utils suggest_memory_format torch empty shape dtype=self dtype device=self device memory_format=memory_format register_meta aten max_pool d_with_indices default meta_max_pool d_with_indices input kernel_size stride= padding= dilation= ceil_mode=False nInputPlane outputHeight outputWidth = max_pool d_checks_and_compute_shape input kernel_size stride padding dilation ceil_mode nbatch = input size - input dim == memory_format = utils suggest_memory_format input input dim == size = nInputPlane outputHeight outputWidth size = nbatch nInputPlane outputHeight outputWidth torch empty size dtype=input dtype device=input device memory_format=memory_format torch empty size dtype=torch int device=input device memory_format=memory_format register_meta aten fractional_max_pool d default meta_fractional_max_pool d kernel_size output_size random_samples torch _check ndim lambda f fractional_max_pool d Expected D D tensor got ndim ndim = ndim d range ndim - ndim torch _check size d lambda f fractional_max_pool d Expected input have non-zero f size non-batch dimensions got size dimension d empty check message out sync matches structured meta torch _check len kernel_size == lambda fractional_max_pool d kernel_size must either single int tuple Ints torch _check len output_size == lambda fractional_max_pool d output_size must either single int tuple Ints input_channels = size - input_height = size - input_width = size - ndim == input_batch = size input_batch = torch _check dtype == random_samples dtype lambda Expect _random_samples have same dtype input torch _check random_samples ndim == lambda f Expect _random samples have dimensions got random_samples ndim n = random_samples size c = random_samples size d = random_samples size torch _check n = input_batch lambda Expect _random_samples size no less then input batch size torch _check c == input_channels lambda Expect _random_samples size equals input channel size torch _check d == lambda f Expect _random_samples size equals got d torch _check output_size + kernel_size - = input_height lambda f fractional_max_pool d kernel height kernel_size too large relative input height input_height torch _check output_size + kernel_size - = input_width lambda f fractional_max_pool d kernel width kernel_size too large relative input width input_width dim == size = input_batch input_channels output_size output_size size = input_channels output_size output_size torch empty size dtype=self dtype device=self device torch empty size dtype=torch int device=self device register_meta aten max_pool d_with_indices out_wrapper out indices meta_max_pool d_with_indices input kernel_size stride= padding= dilation= ceil_mode=False torch _check len kernel_size lambda max_pool d kernel_size must either single int tuple three ints kT = kernel_size kH = kT len kernel_size == kernel_size kW = kT len kernel_size == kernel_size torch _check stride len stride lambda max_pool d stride must either omitted single int tuple three ints dT = kT stride stride dH = kH stride dT len stride == stride dW = kW stride dT len stride == stride torch _check len padding lambda max_pool d padding must either single int tuple three ints pT = padding pH = pT len padding == padding pW = pT len padding == padding torch _check len dilation lambda max_pool d dilation must either single int tuple three ints dilationT = dilation dilationH = dilationT len dilation == dilation dilationW = dilationT len dilation == dilation torch _check input ndim lambda non-empty D D batch mode tensor expected input nbatch = input size - input ndim == nslices = input size - itime = input size - iheight = input size - iwidth = input size - otime = pooling_output_shape itime kT pT dT dilationT ceil_mode oheight = pooling_output_shape iheight kH pH dH dilationH ceil_mode owidth = pooling_output_shape iwidth kW pW dW dilationW ceil_mode pool d_shape_check input nslices kT kH kW dT dH dW pT pH pW dilationT dilationH dilationW itime iheight iwidth otime oheight owidth max_pool d_with_indices channels_last = input ndim == utils suggest_memory_format input == torch channels_last_ d input ndim == input_channels_last_check = input unsqueeze channels_last = input_channels_last_check is_contiguous input_channels_last_check is_contiguous memory_format=torch channels_last_ d out_shape = nslices otime oheight owidth out_shape = nbatch nslices otime oheight owidth type ignore assignment out = input new_empty out_shape indices = input new_empty out_shape dtype=torch int channels_last out = out memory_format=torch channels_last_ d indices = indices memory_format=torch channels_last_ d out indices register_meta aten max_pool d_with_indices_backward out_wrapper grad_input meta_max_pool d_with_indices_backward grad_output input kernel_size stride padding dilation ceil_mode indices torch _check len kernel_size lambda max_pool d kernel_size must either single int tuple three ints kT = kernel_size kH = kT len kernel_size == kernel_size kW = kT len kernel_size == kernel_size torch _check stride len stride lambda max_pool d stride must either omitted single int tuple three ints dT = kT stride stride dH = kH stride dT len stride == stride dW = kW stride dT len stride == stride torch _check len padding lambda max_pool d padding must either single int tuple three ints pT = padding pH = pT len padding == padding pW = pT len padding == padding torch _check len dilation lambda max_pool d dilation must either single int tuple three ints dilationT = dilation dilationH = dilationT len dilation == dilation dilationW = dilationT len dilation == dilation torch _check input ndim lambda non-empty D D batch mode tensor expected input nslices = input size - itime = input size - iheight = input size - iwidth = input size - otime = grad_output size - oheight = grad_output size - owidth = grad_output size - max_pool d_backward_shape_check input grad_output indices nslices kT kH kW dT dH dW pT pH pW dilationT dilationH dilationW itime iheight iwidth otime oheight owidth max_pool d_with_indices_backward channels_last = input ndim == utils suggest_memory_format input == torch channels_last_ d input ndim == input_channels_last_check = input unsqueeze channels_last = input_channels_last_check is_contiguous input_channels_last_check is_contiguous memory_format=torch channels_last_ d grad_input = input new_empty input shape channels_last grad_input = grad_input memory_format=torch channels_last_ d grad_input check_grid_sampler_common input Tensor grid Tensor torch _check input device == grid device lambda f grid_sampler expected input grid same device input f input device grid grid device torch _check input layout == torch strided grid layout == torch strided lambda f grid_sampler expected input grid have torch strided layout f input has input layout grid has grid layout torch _check input shape == grid shape lambda f grid_sampler expected grid input have same batch size got f input sizes input shape grid sizes grid shape torch _check grid shape - == input ndim - lambda f grid_sampler expected grid have size input ndim - last f dimension got grid sizes grid shape i range input ndim torch _check input shape i lambda f grid_sampler expected input have non-empty spatial dimensions f input has sizes input shape dimension i being empty GridSamplerInterpolation Enum BILINEAR = NEAREST = BICUBIC = check_grid_sampler_ d input Tensor grid Tensor interpolation_mode int torch _check input ndim == input ndim == grid ndim lambda f grid_sampler expected D input grid same number f dimensions got input sizes input shape f grid sizes grid shape torch _check input ndim == interpolation_mode == GridSamplerInterpolation BICUBIC value lambda grid_sampler bicubic interpolation only supports D input register_meta aten grid_sampler_ d_backward default grid_sampler_ d_backward_meta grad_output input grid interpolation_mode padding_mode align_corners output_mask input_requires_grad = output_mask input_requires_grad grad_input = torch zeros_like input memory_format=torch contiguous_format grad_input = None grad_grid = torch empty_like grid memory_format=torch contiguous_format grad_input grad_grid register_meta aten grid_sampler_ d out_wrapper grid_sampler_ d input grid interpolation_mode padding_mode align_corners check_grid_sampler_common input grid check_grid_sampler_ d input grid interpolation_mode N = input shape C = input shape out_D = grid shape out_H = grid shape out_W = grid shape input new_empty N C out_D out_H out_W register_meta aten grid_sampler_ d_backward out_wrapper grad_input grad_grid grid_sampler_ d_backward grad_output input grid interpolation_mode padding_mode align_corners output_mask check_grid_sampler_common input grid check_grid_sampler_ d input grid interpolation_mode input_requires_grad = output_mask input_requires_grad grad_input = torch zeros_like input memory_format=torch legacy_contiguous_format grad_input = None grad_grid = torch empty_like grid memory_format=torch legacy_contiguous_format grad_input grad_grid register_meta aten full default full size fill_value args kwargs dtype = kwargs get dtype dtype dtype = utils get_dtype fill_value kwargs dtype = dtype pyrefly ignore not-iterable torch empty size args kwargs zeros_like special cased work sparse register_meta aten zeros_like default zeros_like dtype=None layout=None device=None pin_memory=None memory_format=None layout == torch sparse_coo torch _check memory_format None lambda memory format option only supported strided tensors res = torch empty dtype=self dtype dtype None dtype layout=layout device=self device device None device pin_memory=pin_memory is_sparse res sparse_resize_and_clear_ size sparse_dim dense_dim res sparse_resize_and_clear_ size dim res _coalesced_ True res res = aten empty_like default dtype=dtype layout=layout device=device pin_memory=pin_memory memory_format=memory_format device can meta res fill_ res register_meta aten ones default aten ones out out_wrapper meta_ones size dtype=None layout=None device=None pin_memory=None requires_grad=False dtype None dtype = torch get_default_dtype device None device = torch get_default_device layout None layout = torch strided torch empty size dtype=dtype layout=layout device=device pin_memory=pin_memory register_meta aten zeros default aten zeros out out_wrapper meta_zeros size dtype=None layout=None device=None pin_memory=None requires_grad=False dtype None dtype = torch get_default_dtype device None device = torch get_default_device layout None layout = torch strided torch empty size dtype=dtype layout=layout device=device pin_memory=pin_memory register_meta aten select_scatter default meta_select_scatter src dim index utils clone_preserve_strides register_meta aten slice_scatter default meta_slice_scatter src dim= start=None end=None step= utils clone_preserve_strides TODO Deduplicate canonicalize_dim maybe_wrap_dim dim int dim_post_expr int wrap_scalar bool = True dim_post_expr = assert wrap_scalar dim_post_expr = min = -dim_post_expr max = dim_post_expr - assert dim min dim max f dim dim out bounds min max dim dim += dim_post_expr dim ensure_nonempty_size t dim t dim == t shape dim From aten src ATen native ScatterGatherChecks h gather_shape_check dim index self_dims = max dim index_dims = max index dim torch _check self_dims == index_dims lambda Index tensor must have same number dimensions input tensor i range self_dims i = dim torch _check ensure_nonempty_size index i = ensure_nonempty_size i lambda f Size does match dimension i expected index index shape + f no larger than shape apart dimension dim register_meta aten gather default meta_gather dim index sparse_grad=False torch fx experimental symbolic_shapes guard_or_false wrapped_dim = maybe_wrap_dim dim dim is_index_empty = guard_or_false index numel == is_index_empty torch _check index dtype == torch long index dtype == torch int lambda f gather Expected dtype int int index got index dtype gather_shape_check wrapped_dim index new_empty index shape From aten src ATen native TensorAdvancedIndexing cpp get_operator_enum reduce_ use_new_options=False use_new_options reduce_ == sum REDUCE_ADD reduce_ == prod REDUCE_MULTIPLY reduce_ == mean REDUCE_MEAN reduce_ == amax REDUCE_MAXIMUM reduce_ == amin REDUCE_MINIMUM torch _check False lambda reduce argument must either sum prod mean amax amin reduce_ == add REDUCE_ADD reduce_ == multiply REDUCE_MULTIPLY torch _check False lambda reduce argument must either add multiply From aten src ATen native ScatterGatherChecks h scatter_gather_dtype_check method_name index src_opt=None torch fx experimental symbolic_shapes guard_or_true guard_or_true index numel = torch _check index dtype == torch long index dtype == torch int lambda f method_name Expected dtype int int index src_opt None torch _check dtype == src_opt dtype lambda f method_name Expected dtype equal src dtype ensure_nonempty_dim dim max dim From aten src ATen native ScatterGatherChecks h scatter_shape_check dim index src_opt=None torch fx experimental symbolic_shapes guard_or_false guard_or_false index numel == torch _check ensure_nonempty_dim dim == ensure_nonempty_dim index dim lambda Index tensor must have same number dimensions tensor is_wrong_shape = False self_dims = ensure_nonempty_dim dim Check index size d = size d all d = dim d range self_dims index_d_size = ensure_nonempty_size index d d == dim continue index_d_size ensure_nonempty_size d is_wrong_shape = True break Check index size d = src size d all d src Tensor is_wrong_shape src_opt None d range self_dims index_d_size = ensure_nonempty_size index d index_d_size ensure_nonempty_size src_opt d is_wrong_shape = True break src_opt None torch _check ensure_nonempty_dim dim == ensure_nonempty_dim index dim lambda Index tensor must have same number dimensions tensor torch _check is_wrong_shape lambda f Expected index index shape no larger than shape + f apart dimension dim no larger than src src_opt shape torch _check is_wrong_shape lambda f Expected index index shape no larger than shape + f apart dimension dim From aten src ATen native TensorAdvancedIndexing cpp scatter_meta_impl dim index src=None reduce_=None use_new_options=False wrapped_dim = maybe_wrap_dim dim dim scatter_gather_dtype_check scatter index src scatter_shape_check wrapped_dim index src reduce_ None Check we have valid reduce operator get_operator_enum reduce_ use_new_options register_meta aten scatter_add default meta_scatter_add dim index src scatter_meta_impl dim index src add new_empty shape register_meta aten scatter_add_ meta_scatter_add_ dim index src scatter_meta_impl dim index src add register_meta aten scatter src aten scatter value aten scatter reduce aten scatter value_reduce out_wrapper meta_scatter dim index src_or_value reduce=None src = src_or_value isinstance src_or_value torch Tensor None scatter_meta_impl dim index src reduce new_empty shape register_meta aten scatter_ src aten scatter_ value aten scatter_ reduce aten scatter_ value_reduce meta_scatter_ dim index src_or_value reduce=None src = src_or_value isinstance src_or_value torch Tensor None scatter_meta_impl dim index src reduce register_meta aten _scaled_dot_product_flash_attention meta__scaled_dot_product_flash_attention query Tensor key Tensor value Tensor dropout_p float = is_causal bool = False return_debug_mask bool = False scale Optional float = None batch_size = query size num_heads = query size max_seqlen_batch_q = query size head_dim = query size max_seqlen_batch_k = key size query_t = query transpose attention = torch empty_like query_t transpose logsumexp = torch empty batch_size num_heads max_seqlen_batch_q dtype=torch float device=query device return_debug_mask blocksize_c = head_dim max_seqlen_k = math ceil max_seqlen_batch_q blocksize_c max_seqlen_batch_k = max_seqlen_k = max_seqlen_batch_k = max_seqlen_k = debug_mask = torch empty batch_size num_heads max_seqlen_batch_q max_seqlen_k dtype=query dtype device=query device debug_mask = torch empty dtype=query dtype device=query device Note Seed Offset device seed offset below depends whether we capturing time tracing we don t know we going use cudagraphs so we meta tensors here s possible we ll need have some special handling inductor sdpa See Note BC breaking change flash seed offset torch version hip torch cuda is_available Maintain old path AMD seed = torch empty dtype=torch long device= meta offset = torch empty dtype=torch long device= meta seed = torch empty dtype=torch uint device= meta offset = torch empty dtype=torch uint device= meta attention logsumexp None None max_seqlen_batch_q max_seqlen_batch_k seed offset debug_mask alloc_with_matching_layout query Tensor res_shape tuple int tuple query shape == res_shape query_t = query transpose res = torch empty_like query_t transpose dim_order = sorted key=lambda idx query stride idx reverse=True permuted_shape = res_shape idx idx dim_order final_permute = dim_order index i i range len dim_order res = torch empty permuted_shape dtype=query dtype device=query device permute final_permute res register_meta aten _scaled_dot_product_cudnn_attention meta__scaled_dot_product_cudnn_attention query Tensor key Tensor value Tensor attn_bias Optional Tensor compute_log_sumexp bool dropout_p float = is_causal bool = False return_debug_mask bool = False scale Optional float = None B = query size H = query size S_Q = query size S_KV = key size D_V = value size - res_shape = B H S_Q D_V res = alloc_with_matching_layout query res_shape logsum_exp = torch empty B H S_Q dtype=torch float device=query device See Note Seed Offset seed = torch empty dtype=torch long device= meta offset = torch empty dtype=torch long device= meta res logsum_exp None None S_Q S_KV seed offset None register_meta aten _scaled_dot_product_fused_attention_overrideable meta__scaled_dot_product_fused_attention_overrideable query Tensor key Tensor value Tensor attn_bias Optional Tensor = None dropout_p float = is_causal bool = False return_debug_mask bool = False scale Optional float = None B = query size H_Q = query size S_Q = query size S_KV = key size D_V = value size - res_shape = B H_Q S_Q D_V res = alloc_with_matching_layout query res_shape logsum_exp = torch empty B H_Q S_Q dtype=torch float device=query device See Note Seed Offset seed = torch empty dtype=torch long device= meta offset = torch empty dtype=torch long device= meta res logsum_exp None None S_Q S_KV seed offset None register_meta aten _scaled_dot_product_flash_attention_backward meta__scaled_dot_product_flash_backward grad_out Tensor query Tensor key Tensor value Tensor out Tensor logsumexp Tensor cum_seq_q Tensor cum_seq_k Tensor max_q int max_k int dropout_p float is_causal bool philox_seed Tensor philox_offset Tensor scale Optional float = None grad_q = torch empty_like query transpose transpose grad_k = torch empty_like key transpose transpose grad_v = torch empty_like value transpose transpose grad_q grad_k grad_v register_meta aten _scaled_dot_product_flash_attention_for_cpu meta__scaled_dot_product_flash_attention_for_cpu query Tensor key Tensor value Tensor dropout_p float = is_causal bool = False attn_mask Optional Tensor = None scale Optional float = None batch_size = query size num_heads = query size max_seqlen_batch_q = query size attention = torch empty_like query logsumexp = torch empty batch_size max_seqlen_batch_q num_heads dtype=torch float device=query device transpose attention logsumexp register_meta aten _scaled_dot_product_flash_attention_for_cpu_backward meta__scaled_dot_product_flash_attention_for_cpu_backward grad_out Tensor query Tensor key Tensor value Tensor out Tensor logsumexp Tensor dropout_p float is_causal bool attn_mask Optional Tensor = None scale Optional float = None cpus s grad layout different cuda s i e batch_size seq_len num_heads head_dim grad_q = torch empty_permuted query size dtype=query dtype device=query device grad_k = torch empty_permuted key size dtype=key dtype device=key device grad_v = torch empty_permuted value size dtype=value dtype device=value device grad_q grad_k grad_v register_meta aten _scaled_dot_product_attention_math_for_mps meta__scaled_dot_product_attention_math_for_mps query Tensor key Tensor value Tensor attn_mask Optional Tensor = None dropout_p float = is_causal bool = False dropout_mask Optional Tensor = None scale Optional float = None - tuple Tensor Tensor ensure_ d x x dim == x unsqueeze True x dim batch_size = i range x dim - batch_size = x shape i x view batch_size x size - x size - x size - True x False q_ unsqueezed = ensure_ d query k_ _ = ensure_ d key v_ _ = ensure_ d value batch_size num_head q_size head_size = q_ shape _ k_size max_seq_length _ = k_ shape sdpa_vector_fast_mps out = q_ new_empty q_ shape unsqueezed out = out view_as query attn = q_ new_empty batch_size num_head q_size max_seq_length unsqueezed query dim == attn = attn squeeze shape = list query shape - + attn shape attn = attn view shape out attn sdpa_vector_ pass_mps blocks = out = q_ new_empty q_ shape intermediate = q_ new_empty batch_size num_head q_size blocks head_size out intermediate max_seq_length = k_size q_size max_seq_length = sdpa_vector_ pass_mps sdpa_vector_fast_mps register_meta aten _scaled_dot_product_efficient_attention meta__scaled_dot_product_efficient_attention query Tensor key Tensor value Tensor attn_bias Optional Tensor compute_log_sumexp bool dropout_p= is_causal bool = False scale Optional float = None query = query transpose key = key transpose value = value transpose B = query size M = query size num_heads = query size - Kv = value size - res = torch empty B M num_heads Kv dtype=query dtype device=query device torch version hip torch cuda is_available Please see https github com pytorch pytorch issues longsumexp last dim should seq length logsumexp_dim = M compute_log_sumexp logsumexp_dim = math ceil M compute_log_sumexp logsum_exp = torch empty B num_heads logsumexp_dim dtype=torch float device=query device res = res transpose See Note Seed Offset seed = torch empty dtype=torch long device= meta offset = torch empty dtype=torch long device= meta res logsum_exp seed offset register_meta aten _scaled_dot_product_efficient_attention_backward meta__scaled_dot_product_efficient_backward grad_out Tensor query Tensor key Tensor value Tensor attn_bias Optional Tensor out Tensor logsumexp Tensor philox_seed Tensor philox_offset Tensor dropout_p float grad_input_mask list bool is_causal bool = False scale Optional float = None batch_size = query size num_heads = query size max_q = query size head_dim = query size head_dim_v = value size max_k = key size grad_q = torch empty_permuted batch_size num_heads max_q head_dim dtype=query dtype device=query device grad_k = torch empty_permuted batch_size num_heads max_k head_dim dtype=key dtype device=key device grad_v = torch empty_permuted batch_size num_heads max_k head_dim_v dtype=value dtype device=value device grad_bias = None attn_bias None grad_input_mask lastDim = attn_bias size - lastDimAligned = lastDim lastDim == lastDim + - lastDim new_sizes = list attn_bias size new_sizes - = lastDimAligned grad_bias = torch empty new_sizes dtype=attn_bias dtype device=attn_bias device grad_bias = grad_bias lastDim grad_q grad_k grad_v grad_bias register_meta aten _scaled_dot_product_cudnn_attention_backward meta__scaled_dot_product_cudnn_backward grad_out Tensor query Tensor key Tensor value Tensor out Tensor logsumexp Tensor philox_seed Tensor philox_offset Tensor attn_bias Tensor cum_seq_q Tensor cum_seq_k Tensor max_q int max_k int dropout_p float is_causal bool scale Optional float = None grad_q = torch empty_like query grad_k = torch empty_like key grad_v = torch empty_like value grad_q grad_k grad_v register_meta aten _flash_attention_forward meta__flash_attention_forward query Tensor key Tensor value Tensor cum_seq_q Optional Tensor cum_seq_k Optional Tensor max_q int max_k int dropout_p float is_causal bool return_debug_mask bool scale Optional float = None window_size_left Optional int = None window_size_right Optional int = None seqused_k Optional Tensor = None alibi_slopes Optional Tensor = None NB there two underlying paths normal dense path expect D inputs shape batch_size seqlen num_heads head_dim varseqlen path expect D inputs shape total num_heads head_dim where total includes all batch item sequences cum_seq_q cum_seq_k contain offsets into total batch_size = query size cum_seq_q None cum_seq_q numel - max_seqlen_batch_q = query size cum_seq_q None max_q max_seqlen_batch_k = key size cum_seq_k None max_k num_heads = query size - head_dim = query size - Cuda Path attention = torch empty_like query cum_seq_q None logsumexp = torch empty batch_size num_heads max_seqlen_batch_q dtype=torch float device=query device total_q = query size logsumexp = torch empty num_heads total_q dtype=torch float device=query device return_debug_mask blocksize_c = head_dim max_seqlen_k = math ceil max_seqlen_batch_q blocksize_c max_seqlen_batch_k = max_seqlen_k = max_seqlen_batch_k = max_seqlen_k = debug_mask = torch empty batch_size num_heads max_seqlen_batch_q max_seqlen_k dtype=query dtype device=query device debug_mask = torch empty dtype=query dtype device=query device See Note Seed Offset See Note BC breaking change flash seed offset seed offset = None None torch version hip torch cuda is_available Maintain old path AMD seed = torch empty dtype=torch long device= meta offset = torch empty dtype=torch long device= meta seed = torch empty dtype=torch uint device= meta offset = torch empty dtype=torch uint device= meta attention logsumexp seed offset debug_mask register_meta aten _flash_attention_backward meta__flash_attention_backward grad_out Tensor query Tensor key Tensor value Tensor out Tensor logsumexp Tensor cum_seq_q Tensor cum_seq_k Tensor max_q int max_k int dropout_p float is_causal bool philox_seed Tensor philox_offset Tensor scale Optional float = None window_size_left Optional int = None window_size_right Optional int = None grad_query = torch empty_like query grad_key = torch empty_like key grad_value = torch empty_like value grad_query grad_key grad_value register_meta aten _efficient_attention_forward meta__efficient_attention_forward query Tensor key Tensor value Tensor bias Optional Tensor cu_seqlens_q Optional Tensor cu_seqlens_k Optional Tensor max_seqlen_q Optional int max_seqlen_k Optional int dropout_p float custom_mask_type int compute_log_sumexp bool = False scale Optional float = None causal_diagonal Optional Tensor = None seqlen_k Optional Tensor = None window_size Optional int = None B = query size M = query size N = key size num_heads = query size - Kv = value size - res = torch empty B M num_heads Kv dtype=query dtype device=query device logsumexp_batch_dim = cu_seqlens_q size - cu_seqlens_q None B actual_max_seqlen_q = M cu_seqlens_q None assert max_seqlen_q None actual_max_seqlen_q = max_seqlen_q actual_max_seqlen_k = max_seqlen_k max_seqlen_k None N logsumexp_dim = math ceil actual_max_seqlen_q compute_log_sumexp logsum_exp = torch empty logsumexp_batch_dim num_heads logsumexp_dim dtype=torch float device=query device See Note Seed Offset seed = torch empty dtype=torch long device= meta offset = torch empty dtype=torch long device= meta res logsum_exp seed offset actual_max_seqlen_q actual_max_seqlen_k register_meta aten _efficient_attention_backward meta__efficient_attention_backward grad_out Tensor query Tensor key Tensor value Tensor bias Optional Tensor cu_seqlens_q Optional Tensor cu_seqlens_k Optional Tensor max_seqlen_q torch SymInt max_seqlen_k torch SymInt logsumexp Tensor dropout_p float philox_seed Tensor philox_offset Tensor custom_mask_type int bias_requires_grad bool scale Optional float = None num_splits_key Optional int = None shared_storage_dqdkdv bool = False shared_storage_dqdkdv torch _check query shape == key shape lambda seqlen must match ` shared_storage_dqdkdv torch _check query shape == key shape lambda embedding dim must match ` shared_storage_dqdkdv chunk = torch empty query shape - query shape - query shape - dtype=query dtype device=query device grad_query = chunk select - grad_key = chunk select - grad_value = chunk select - grad_query = torch empty_like query grad_key = torch empty_like key grad_value = torch empty_like value bias None lastDim = bias size - lastDimAligned = lastDim lastDim == lastDim + - lastDim new_sizes = list bias size new_sizes - = lastDimAligned grad_bias = torch empty new_sizes dtype=bias dtype device=bias device grad_bias = grad_bias lastDim grad_bias = torch empty device=query device grad_query grad_key grad_value grad_bias register_meta aten _scaled_mm default meta_scaled_mm torch Tensor mat torch Tensor scale_a torch Tensor scale_b torch Tensor bias Optional torch Tensor = None scale_result Optional torch Tensor = None out_dtype Optional torch dtype = None use_fast_accum bool = False is_fp _or_fp _type dtype dtype torch float _e m fn torch float _e m torch float _e m fnuz torch float _e m fnuz torch float _e m fn_x torch _check dim == mat dim == lambda f Inputs must D got dim = dim mat dim = mat dim torch _check is_fp _or_fp _type dtype is_fp _or_fp _type mat dtype lambda f Expected both inputs fp fp types got dtype= dtype mat dtype= mat dtype device_hint == cuda is_row_major stride stride stride stride == is_col_major stride stride == stride has_zero_dim tensor_ d tensor_ d size == tensor_ d size == torch _check is_row_major stride has_zero_dim lambda f must row_major got stride stride torch _check is_col_major mat stride has_zero_dim mat lambda f mat must col_major got stride mat stride torch _check size == lambda f Expected size divisible got size = size torch _check mat size == mat size == lambda f Expected both dimensions mat divisible got mat shape determine scaling type check input dimensions refer Blas cpp op m _k = shape n = mat size is_blockwise_scaling = scale_a dtype == torch float _e m fnu scale_b dtype == torch float _e m fnu scale_a dtype == torch float _e m fn scale_b dtype == torch float _e m fn scale_a numel == scale_b numel == tensorwise scaling torch _check scale_a dtype == torch float scale_b dtype == torch float lambda For tensorwise scaling both scale_a scale_b must float fp tensors is_blockwise_scaling blockwise scaling scale_a dtype == torch float _e m fn NVIDIA s nvfp recipe block size elements packed unpacked _k needs translated unpacked version block_size_k = _k = _k block_size_k = block_size_mn = ceil_div b + b - b num_k_blocks = ceil_div _k block_size_k padded_num_k_blocks = ceil_div num_k_blocks expected_a_size = block_size_mn ceil_div m block_size_mn padded_num_k_blocks expected_b_size = block_size_mn ceil_div n block_size_mn padded_num_k_blocks scale_a numel == expected_a_size scale_b numel == expected_b_size torch _check scale_a is_contiguous lambda scale_a must contiguous torch _check scale_b is_contiguous lambda scale_b must contiguous torch _check False lambda Invalid blockwise scaling configuration f For blockwise scaling scale_a should have expected_a_size elements got scale_a numel f scale_b should have expected_b_size elements got scale_b numel torch _check scale_a dtype == torch float scale_b dtype == torch float lambda For rowwise scaling both scale_a scale_b must float fp tensors rowwise scaling enforce D input tensors torch _check scale_a dim == scale_b dim == lambda f For non-tensorwise scaling scale tensors must D got scale_a dim = scale_b dim = scale_a size == m scale_a size == scale_b size == scale_b size == n rowwise scaling torch _check scale_a is_contiguous scale_b is_contiguous lambda Both scale_a scale_b must contiguous rowwise scaling scale_a size == m scale_a size == scale_b size == _k + - scale_b size == n + - BlockWise x BlockWise x pass do nothing do error does match any valid scaling type torch _check False lambda Invalid scaling configuration For tensorwise scaling both scales should scalar f For rowwise scaling scale_a should m scale_b should n f For BlockWise x BlockWise x scale_a should m _k + - + f scale_b should _k + - n + - f Got scale_a size = scale_a size scale_a size f scale_b size = scale_b size scale_b size _out_dtype = out_dtype out_dtype None dtype torch empty size mat size dtype=_out_dtype device=self device register_meta aten scatter_reduce two aten scatter_reduce two_out out_wrapper meta_scatter_reduce_two dim index src reduce include_self=True scatter_meta_impl dim index src reduce use_new_options=True new_empty shape register_meta aten scatter_reduce_ two meta_scatter_reduce__two dim index src reduce include_self=True scatter_meta_impl dim index src reduce use_new_options=True register_meta aten multinomial default aten multinomial out out_wrapper meta_multinomial input num_samples replacement=False generator=None torch _check input dim = lambda f The probability distributions dimensions must got input dim input dim == torch empty num_samples dtype=torch long device=input device torch empty input size num_samples dtype=torch long device=input device multiply_integers vs r = v vs r = v r upsample_common_check input_size output_size num_spatial_dims torch _check len output_size == num_spatial_dims lambda f It expected output_size equals num_spatial_dims got size len output_size expected_input_dims = num_spatial_dims + N C torch _check len input_size == expected_input_dims lambda f It expected input_size equals expected_input_dims got size len input_size torch _check all s s input_size all s s output_size lambda f Input output sizes should greater than got f input size input_size output size output_size nbatch channels = input_size nbatch channels output_size register_meta aten upsample_nearest d default aten _upsample_nearest_exact d default upsample_nearest d input output_size scales=None torch _check input numel = multiply_integers input size lambda f Non-empty D data tensor expected got tensor sizes input size full_output_size = upsample_common_check input size output_size num_spatial_dims= input new_empty full_output_size memory_format=utils suggest_memory_format input register_meta aten upsample_nearest d default aten _upsample_nearest_exact d default upsample_nearest d input output_size scales_h=None scales_w=None torch _check input numel = multiply_integers input size lambda f Non-empty D data tensor expected got tensor sizes input size full_output_size = upsample_common_check input size output_size num_spatial_dims= output = input new_empty full_output_size convert output correct memory format necessary memory_format = utils suggest_memory_format input following heuristic only use channels_last path when s faster than contiguous path _ n_channels _ _ = input shape input device type == cuda n_channels memory_format = torch contiguous_format output = output contiguous memory_format=memory_format output register_meta aten upsample_nearest d_backward default aten _upsample_nearest_exact d_backward default upsample_nearest d_backward grad_output Tensor output_size Sequence Union int torch SymInt input_size Sequence Union int torch SymInt scales_h Optional float = None scales_w Optional float = None full_output_size = upsample_common_check input_size output_size num_spatial_dims= torch _check grad_output ndim == lambda f Expected grad_output tensor dimension got dimension grad_output ndim i range torch _check grad_output size i == full_output_size i lambda f Expected grad_output have same shape output f output size i = full_output_size i f got grad_output size i = grad_output size i grad_output new_empty input_size memory_format=utils suggest_memory_format grad_output type ignore call-overload register_meta aten upsample_nearest d default aten _upsample_nearest_exact d default upsample_nearest d input output_size scales_d=None scales_h=None scales_w=None torch _check input numel = multiply_integers input size lambda f Non-empty D data tensor expected got tensor sizes input size full_output_size = upsample_common_check input size output_size num_spatial_dims= input new_empty full_output_size memory_format=utils suggest_memory_format input register_meta aten sort default aten sort stable aten sort values aten sort values_stable meta_sort stable=None dim=- descending=False values=None indices=None v i = torch empty_like torch empty_like dtype=torch int values None indices None assert isinstance values TensorLike assert isinstance indices TensorLike Makes sure values indices have same strides For cases where these have different shapes like msort out_shape = v shape out_stride = v stride values = _maybe_resize_out values out_shape indices = _maybe_resize_out indices out_shape values as_strided_ out_shape out_stride indices as_strided_ out_shape out_stride _safe_copy_out copy_from=v copy_to=values type ignore arg-type _safe_copy_out copy_from=i copy_to=indices type ignore arg-type values indices v i rnn_cell_checkSizes input_gates hidden_gates input_bias hidden_bias factor prev_hidden torch _check input_gates ndim == lambda f input_gates ndim = torch _check input_gates shape == hidden_gates shape lambda f input_gates shape = hidden_gates shape gates_size = input_gates size input_bias None torch _check input_bias ndim == lambda f input_bias ndim = torch _check input_bias numel == gates_size lambda f input_bias numel = gates_size torch _check input_bias shape == hidden_bias shape lambda f input_bias shape = hidden_bias shape torch _check prev_hidden ndim == lambda f prev_hidden ndim = expected_prev_hidden_numel = input_gates size gates_size factor torch _check prev_hidden numel == expected_prev_hidden_numel lambda f prev_hidden numel = input_gates size gates_size factor aka expected_prev_hidden_numel torch _check all pyrefly ignore missing-attribute x device == input_gates device x hidden_gates input_bias hidden_bias prev_hidden lambda expected all inputs same device register_meta aten _thnn_fused_lstm_cell default _thnn_fused_lstm_cell_meta input_gates hidden_gates cx input_bias=None hidden_bias=None rnn_cell_checkSizes input_gates hidden_gates input_bias hidden_bias cx workspace = torch empty_like input_gates memory_format=torch contiguous_format hy = torch empty_like cx memory_format=torch contiguous_format cy = torch empty_like cx memory_format=torch contiguous_format hy cy workspace register_meta aten _cudnn_rnn default _cudnn_rnn input weight weight_stride weight_buf hx cx mode hidden_size proj_size num_layers batch_first dropout train bidirectional batch_sizes dropout_state is_input_packed = len batch_sizes = is_input_packed seq_length = len batch_sizes mini_batch = batch_sizes batch_sizes_sum = input shape seq_length = input shape batch_first input shape mini_batch = input shape batch_first input shape batch_sizes_sum = - num_directions = bidirectional out_size = proj_size proj_size = hidden_size is_input_packed out_shape = batch_sizes_sum out_size num_directions out_shape = mini_batch seq_length out_size num_directions batch_first seq_length mini_batch out_size num_directions output = input new_empty out_shape cell_shape = num_layers num_directions mini_batch hidden_size cx None cy = torch empty device=input device cy = cx new_empty cell_shape hy = hx new_empty num_layers num_directions mini_batch out_size TODO Query cudnnGetRNNTrainingReserveSize expose python reserve_shape = train reserve = input new_empty reserve_shape dtype=torch uint output hy cy reserve weight_buf register_meta aten mkldnn_rnn_layer default mkldnn_rnn_layer input w w w w hx_ cx_ reverse batch_sizes mode hidden_size num_layers has_biases bidirectional batch_first train seq_length = input shape batch_first input shape mini_batch = input shape batch_first input shape output_chanels = hidden_size out_shape = mini_batch seq_length output_chanels batch_first seq_length mini_batch output_chanels output = input new_empty out_shape hx_ None hy = torch empty device=input device hy = hx_ new_empty hx_ shape cx_ None cy = torch empty device=input device cy = cx_ new_empty cx_ shape workspace = torch empty device=input device dtype=torch uint output hy cy workspace zero_numel_check_dims dim fn_name ndim == torch _check_index dim == dim == - lambda f fn_name Expected reduction dim - scalar got dim torch _check_index size dim = lambda f fn_name Expected reduction dim dim have non-zero size From aten src ATen native ReduceOps cpp check_argmax_argmin name dim dim None dim = maybe_wrap_dim dim dim zero_numel_check_dims dim name torch _check numel = lambda f name Expected reduction dim specified input numel == register_meta aten argmax default aten argmin default argmax_argmin_meta dim=None keepdim=False check_argmax_argmin argmax dim dims = utils reduction_dims shape dim dim None None shape = _compute_reduction_shape dims keepdim new_empty shape dtype=torch int register_meta aten scalar_tensor default scalar_tensor s dtype=None layout=None device=None pin_memory=None NB It s always wrong try create scalar tensor jagged layout Rather than fix everywhere just use strided layout let NJT handle scalar tensor broadcasting layout == torch jagged layout = torch strided torch empty dtype=dtype layout=layout device=device pin_memory=pin_memory register_meta aten topk default topk_meta k dim=- largest=True sorted=True From aten src ATen native Sorting cpp dim = maybe_wrap_dim dim dim wrap_scalar=True sliceSize = dim == size dim torch _check k = torch _check k = sliceSize lambda k range dimension topKSize = list shape len topKSize topKSize dim = k new_empty topKSize new_empty topKSize dtype=torch int register_meta aten _segment_reduce_backward out_wrapper meta__segment_reduce_backward grad output data reduce lengths=None offsets=None axis= initial=None assert lengths None offsets None segment_reduce Either lengths offsets must defined data_contig = data contiguous grad_contig = grad contiguous torch empty_like data_contig dtype=grad_contig dtype device=grad_contig device layout=grad_contig layout register_meta aten kthvalue default aten kthvalue values out_wrapper values indices kthvalue_meta k dim=- keepdim=False torch fx experimental symbolic_shapes sym_and dim = maybe_wrap_dim dim dim wrap_scalar=True dimSize = size dim dim torch _check sym_and k = k = dimSize lambda f kthvalue selected number k out range dimension dim shape = list shape dim + shape dim + keepdim dim shape insert dim new_empty shape new_empty shape dtype=torch int legacy_contiguous_memory_format = torch contiguous_format From aten src ATen native cuda RNN cu checkLSTMBackwardSizes grad_hy grad_cy cx cy workspace defined_grad = grad_hy grad_hy None grad_cy torch _check defined_grad dim == lambda exp_size = defined_grad size grad_hy None torch _check grad_hy size == exp_size lambda grad_cy None torch _check grad_cy size == exp_size lambda torch _check cx size == exp_size lambda torch _check cy size == exp_size lambda torch _check workspace dim == lambda torch _check workspace numel == exp_size exp_size lambda From aten src ATen native cuda RNN cu register_meta aten _thnn_fused_lstm_cell_backward_impl default _thnn_fused_lstm_cell_backward_impl grad_hy grad_cy cx cy workspace has_bias grad_hy None grad_cy None None None None checkLSTMBackwardSizes grad_hy grad_cy cx cy workspace grad_gates = torch empty_like workspace memory_format=legacy_contiguous_memory_format grad_cx = torch empty_like cx memory_format=legacy_contiguous_memory_format grad_bias = grad_gates sum keepdim=False has_bias None grad_gates grad_cx grad_bias From aten src ATen native mps operations Linear mm register_meta aten linear_backward default linear_backward input_ grad_output_ weight_ output_mask grad_input = None grad_weight = None grad_bias = None output_mask grad_input = grad_output_ new_empty input_ size output_mask output_mask grad_weight = grad_output_ new_empty grad_output_ size - input_ size - grad_bias = grad_output_ new_empty grad_output_ size - grad_input grad_weight grad_bias register_meta aten pixel_shuffle default meta_pixel_shuffle upscale_factor assert len shape shape - upscale_factor upscale_factor == f Invalid input shape pixel_shuffle shape upscale_factor = upscale_factor is_channels_last ten torch _prims_common suggest_memory_format ten == torch channels_last pick_memory_format is_channels_last device_hint == cuda torch contiguous_format torch channels_last is_contiguous memory_format=torch contiguous_format torch contiguous_format is_contiguous memory_format=torch preserve_format torch preserve_format C = shape - upscale_factor upscale_factor Hr = shape - upscale_factor Wr = shape - upscale_factor out_shape = shape - C Hr Wr out = new_empty out_shape out = out memory_format=pick_memory_format type ignore call-overload out register_meta aten mkldnn_rnn_layer_backward default mkldnn_rnn_layer_backward input weight weight weight weight hx_ cx_tmp output hy_ cy_ grad_output_r_opt grad_hy_r_opt grad_cy_r_opt reverse mode hidden_size num_layers has_biases train bidirectional batch_sizes batch_first workspace diff_x = input new_empty input shape diff_hx = hx_ new_empty hx_ shape diff_cx = cx_tmp new_empty cx_tmp shape diff_w = weight new_empty weight shape diff_w = weight new_empty weight shape diff_b = weight new_empty weight shape diff_x diff_w diff_w diff_b diff_b diff_hx diff_cx register_meta aten bucketize Tensor aten bucketize Tensor_out out_wrapper meta_bucketize boundaries out_int =False right=False torch empty_like dtype=torch int out_int torch int memory_format=torch contiguous_format register_meta aten histc out_wrapper meta_histc input bins= min= max= fn_name = histc device_hint input == cpu torch _check input is_floating_point lambda f \ histogram_cpu\ implemented input dtype device_hint input == cuda input is_floating_point utils alert_not_deterministic _histc_cuda floating point input torch _check isinstance bins IntLike lambda f fn_name argument bins must int type bins torch _check bins lambda f fn_name bins must got bins torch _check isinstance min Number lambda f fn_name argument min must Number type min torch _check isinstance max Number lambda f fn_name argument max must Number type max torch _check max = min lambda f fn_name max must larger than min torch empty bins device=input device dtype=input dtype register_meta aten _upsample_bilinear d_aa default aten _upsample_bicubic d_aa default meta_upsample_bimode d_aa input output_size align_corners scales_h=None scales_w=None full_output_size = upsample_common_check input size output_size num_spatial_dims= torch _check input numel = all size size input size lambda f Non-empty D data tensor expected got tensor sizes input size input new_empty full_output_size memory_format=utils suggest_memory_format input register_meta aten _upsample_bilinear d_aa_backward default meta_upsample_bimode d_aa_backward grad_output output_size input_size align_corners scales_h=None scales_w=None full_output_size = upsample_common_check input_size output_size num_spatial_dims= torch _check grad_output ndim == lambda f Expected grad_output tensor dimension got dimension grad_output ndim i range torch _check grad_output shape i == full_output_size i lambda f Expected grad_output have same shape output output size i = full_output_size i got grad_output_size i = grad_output size i grad_output new_empty input_size memory_format=utils suggest_memory_format grad_output From aten src ATen native cuda AmpKernels cu register_meta aten _amp_foreach_non_finite_check_and_unscale_ default _amp_foreach_non_finite_check_and_unscale_ found_inf inv_scale torch _check found_inf numel == lambda found_inf must -element tensor torch _check inv_scale numel == lambda inv_scale must -element tensor torch _check found_inf dtype is_floating_point lambda found_inf must float tensor torch _check inv_scale dtype is_floating_point lambda inv_scale must float tensor From aten src ATen native UnaryOps cpp register_meta aten nan_to_num default aten nan_to_num out out_wrapper nan_to_num nan=None posinf=None neginf=None torch empty_like register_meta torch ops aten transpose_ transpose_ dim dim assert layout torch sparse_csr torch sparse_csc torch sparse_bsr torch sparse_bsc f torch transpose_ in-place transposition supported layout layout ndims = ndim dim = maybe_wrap_dim dim ndims dim = maybe_wrap_dim dim ndims dim == dim size = list size stride = list stride stride dim stride dim = stride dim stride dim size dim size dim = size dim size dim as_strided_ size stride register_meta torch ops aten t_ t_ ndims = ndim is_sparse sparse_dim = sparse_dim dense_dim = dense_dim assert sparse_dim = dense_dim == f t_ expects tensor = sparse dense dimensions f got sparse_dim sparse dense_dim dense dimensions assert dim = f t_ expects tensor = dimensions ndims D transpose_ ndims register_meta aten searchsorted out_wrapper meta_searchsorted sorted_sequence out_int =False right=False side=None sorter=None If sorted_sequence one-dimensional its shape must match values all last dimension torch _check len sorted_sequence shape = sorted_sequence shape - == shape - lambda torch searchsorted boundaries tensor should dimension first N- dimensions boundaries tensor input value tensor must f match we got boundaries tensor list sorted_sequence shape f input value tensor list shape If sorter array provided its dimensions must exactly match sorted_sequence torch _check sorter None sorted_sequence shape == sorter shape lambda torch searchsorted boundary sorter must have same size f got boundary tensor list sorted_sequence shape got sorter tensor f list sorter shape sorter None Per docs side == left right True we error torch _check side = left right lambda torch searchsorted side right can t set opposites got side left while right True dtype = torch int out_int torch int isinstance torch Tensor torch empty_like dtype=dtype memory_format=torch contiguous_format Scalar torch empty dtype=dtype device=sorted_sequence device _check_for_unsupported_isin_dtype dtype torch _check dtype torch bool torch complex torch complex lambda f Unsupported input type encountered isin dtype register_meta aten embedding_dense_backward meta_embedding_dense_backward grad_output indices num_weights padding_idx scale_grad_by_freq grad_weight = grad_output new_empty num_weights grad_output size - grad_weight register_meta aten _embedding_bag_backward meta_embedding_bag_backward grad indices offsets offset bag bag_size maximum_indices num_weights scale_grad_by_freq mode sparse per_sample_weights padding_idx=- sparse aten _embedding_bag_sparse_backward grad indices offsets offset bag bag_size num_weights scale_grad_by_freq mode per_sample_weights padding_idx meta_embedding_bag_dense_backward grad indices offset bag bag_size maximum_indices num_weights scale_grad_by_freq mode per_sample_weights padding_idx register_meta aten _embedding_bag_dense_backward meta_embedding_bag_dense_backward grad indices offset bag bag_size maximum_indices num_weights scale_grad_by_freq mode per_sample_weights padding_idx=- torch _check grad dtype torch float torch bfloat torch float torch float lambda f Unsupported input type encountered grad dtype mode == MODE_MAX torch _check maximum_indices None index_grad_weight = grad new_empty num_weights grad size index_grad_weight register_meta aten _embedding_bag_per_sample_weights_backward meta_embedding_bag_per_sample_weights_backward grad weight indices offsets offset bag mode padding_idx=- embedding_features = grad size torch _check mode == MODE_SUM lambda embedding_bag_backward per_sample_weights only supported mode= sum torch _check grad dim == torch _check indices dim == num_samples = indices size torch _check weight dim == torch _check weight size == embedding_features output = grad new_empty num_samples output register_meta aten isin out_wrapper meta_isin elements test_elements assume_unique=False invert=False torch _check isinstance elements Tensor isinstance test_elements Tensor lambda At least one elements test_elements must Tensor isinstance elements Tensor elements = torch tensor elements device=test_elements device isinstance test_elements Tensor test_elements = torch tensor test_elements device=elements device _check_for_unsupported_isin_dtype elements dtype _check_for_unsupported_isin_dtype test_elements dtype torch empty_like elements dtype=torch bool register_meta aten polygamma out_wrapper meta_polygamma n int Tensor - Tensor torch _check n = lambda polygamma n x does support negative n _ result_dtype = elementwise_dtypes type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT torch empty_like dtype=result_dtype register_meta aten _local_scalar_dense meta_local_scalar_dense Tensor raise RuntimeError Tensor item cannot called meta tensors register_meta aten silu out_wrapper exact_dtype=True silu Tensor - Tensor torch empty_like register_meta aten sigmoid out_wrapper sigmoid Tensor - Tensor _ result_dtype = elementwise_dtypes type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT torch empty_like dtype=result_dtype _create_grouped_mm_output_tensor mat mat offs out_dtype mat _is_ d = mat dim == mat _is_ d = mat dim == mat _is_ d mat _is_ d out_size = offs size mat size mat size torch _check offs size == mat size lambda matrix batch sizes have match out_size = mat size mat size - mat _is_ d torch _check offs size == mat size lambda matrix batch sizes have match out_size = mat size mat size regular bmm torch _check mat size == mat size lambda batched dimension has match out_size = mat size mat size mat size - out_dtype = out_dtype mat dtype torch version cuda alignment = out_dtype itemsize size_padded = out_size - + alignment - alignment alignment mat _is_ d == mat _is_ d out_stride = out_size size_padded size_padded out_stride = size_padded out = torch empty_strided out_size out_stride dtype=out_dtype device=mat device out = torch empty out_size dtype=out_dtype device=mat device out _meta_grouped_mm_common mat_a Tensor mat_b Tensor scale_a Optional torch Tensor scale_b Optional torch Tensor offs Optional Tensor = None bias Optional Tensor = None scale_result Optional torch Tensor = None out_dtype Optional torch dtype = None use_fast_accum bool = False torch _check scale_a None == scale_b None lambda Either both scale factors given none scaled = scale_a None scale_b None Implementing all checks _grouped_mm_cuda _scaled_grouped_mm_cuda code aten src ATen native cuda Blas cpp scaled fp _dtype = torch float _e m fnuz torch version hip torch float _e m fn torch _check mat_a dtype == fp _dtype mat_b dtype == fp _dtype lambda f Expected inputs E M FP type got mat_a dtype= mat_a dtype mat_b dtype= mat_b dtype noqa B torch _check mat_a dtype == torch bfloat mat_b dtype == torch bfloat lambda f Expected inputs BF type got mat_a dtype= mat_a dtype mat_b dtype= mat_b dtype noqa B torch _check mat_a dim mat_b dim lambda f Multiplicands must D D got mat_a dim = mat_a dim mat_b dim = mat_b dim noqa B mat_a_is_ d = mat_a dim == mat_b_is_ d = mat_b dim == mat_a_is_ d mat_b_is_ d torch _check mat_a size - == mat_b size - lambda contraction dimension mat_a mat_b must match scaled is_row_major mat mat_stride = mat stride mat_stride - mat_stride - == is_col_major mat mat_stride = mat stride mat_stride - == mat_stride - torch _check is_row_major mat_a lambda f Expected mat_a tensor row major last two dimensions got strides mat_a stride - noqa B torch _check is_col_major mat_b lambda f Expected mat_b tensor column major last two dimensions got strides mat_b stride - noqa B check_valid_strides mat_name mat end_dim = mat dim - alignment = mat element_size mat_stride = mat stride mat_stride end_dim - == mat_stride end_dim = max mat shape end_dim - torch _check mat_stride end_dim alignment == lambda f Expected mat_name stride along end_dim dim multiple bytes got mat_stride end_dim noqa B mat_stride end_dim == mat_stride end_dim - = max mat shape end_dim torch _check mat_stride end_dim - alignment == lambda f Expected mat_name stride along end_dim - dim multiple bytes got mat_stride end_dim - noqa B torch _check False lambda f Invalid strides sizes got mat_stride strides mat shape sizes noqa B check_valid_strides mat_a mat_a check_valid_strides mat_b mat_b scale_a None scale_b None torch _check scale_a dtype == torch float scale_b dtype == torch float scale_a dtype == torch float _e m fnu scale_b dtype == torch float _e m fnu lambda f For FP scales must both float MXFP both scales must float _e m fnu Got scale_a dtype= scale_a dtype scale_b dtype= scale_b dtype noqa B is_mxfp = scale_a dtype == torch float _e m fnu scale_b dtype == torch float _e m fnu round_up x y Rounds up x nearest multiple y x + y - y y check_scale scale_name scale mat scaled_dim scale_multiplier= mat dim == torch _check scale is_contiguous lambda f Expected scale_name contiguous For MXFP d tensors have variable size groups represented subtensors converted blocked padded format individually At compile time we don t know group sizes yet so we don t know expect size blocked format scale This limits what we can check here is_mxfp torch _check scale dim == mat dim lambda f For MXFP scale must have same number dimensions target tensor scale_name has mat ndim= mat ndim scale ndim= scale ndim noqa B torch _check scale dim == lambda f Expected scale_name D tensor got scale dim D tensor torch _check scale shape == mat shape scaled_dim scale_multiplier lambda f Expected scale_name have mat shape scaled_dim scale_multiplier elements got scale shape elements noqa B torch _check scale stride - == lambda f Expected scale_name contiguous last dimension torch _check scale shape == mat shape lambda f Expected scale_name batch dimension mat shape got scale shape For MXFP d tensors have static groups stack d tensors so we can know expected blocked scale sizes compile time is_mxfp torch _check scale ndim == mat ndim - lambda f For MXFP d tensor should have d scales scale_name has mat ndim= mat ndim scale ndim= scale ndim noqa B TODO This logic only holds RHS tensor d- d case We ll need update handle LHS d tensor d- d d- d cases G K N = mat shape block_size = blocked_K = round_up K block_size blocked_N = round_up N torch _check scale shape == G scale shape == blocked_K blocked_N lambda f For MXFP expected mat shape= mat shape have scale shape G blocked_K blocked_N got scale shape noqa B torch _check scale dim == lambda f Expected scale_name D tensor got scale dim D tensor torch _check scale shape == mat shape + scaled_dim lambda f Expected scale_name non-batch dimension mat shape + scaled_dim got scale shape noqa B scale_multiplier = offs shape offs None mat_a_is_ d mat_b_is_ d check_scale scale_a scale_a mat_a scale_multiplier check_scale scale_b scale_b mat_b scale_multiplier torch _check scale_result None lambda Scale result tensor provided supported yet mat_a_is_ d mat_b_is_ d torch _check offs None lambda f Offsets tensor provided needed mat_a dim D mat_b dim D multiplicand layouts offs None silence Mypy torch _check offs dim == lambda f Offsets tensor must D got offs dim = offs dim torch _check offs dtype == torch int lambda f Offsets tensor must integer int tensor got offs dtype torch _check offs None lambda Offsets tensor provided needed D D multiplicand layouts torch _check bias None lambda Bias tensor provided supported yet torch _check out_dtype None out_dtype == torch bfloat lambda If output dtype provided must torch bfloat _create_grouped_mm_output_tensor mat_a mat_b offs out_dtype register_meta aten _grouped_mm out_wrapper meta_grouped_mm mat_a Tensor mat_b Tensor offs Optional Tensor = None bias Optional Tensor = None out_dtype Optional torch dtype = None - Tensor _meta_grouped_mm_common mat_a mat_b scale_a=None scale_b=None offs=offs bias=bias scale_result=None out_dtype=out_dtype register_meta aten _scaled_grouped_mm meta_scaled_grouped_mm mat_a torch Tensor mat_b torch Tensor scale_a torch Tensor scale_b torch Tensor offs Optional torch Tensor = None bias Optional torch Tensor = None scale_result Optional torch Tensor = None out_dtype Optional torch dtype = None use_fast_accum bool = False matching _scaled_grouped_mm_cuda Blas cpp implementation out_dtype = out_dtype torch bfloat _meta_grouped_mm_common mat_a mat_b scale_a=scale_a scale_b=scale_b offs=offs bias=bias scale_result=scale_result out_dtype=out_dtype use_fast_accum=use_fast_accum register_meta aten _softmax out_wrapper softmax x Tensor dim int half_to_float bool - Tensor half_to_float assert x dtype == torch half computation_dtype result_dtype = utils elementwise_dtypes x type_promotion_kind=utils ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT result_dtype = result_dtype half_to_float computation_dtype res = torch empty_like x dtype=result_dtype memory_format=torch contiguous_format res register_meta aten constant_pad_nd out_wrapper _constant_pad_nd_meta input pad value= same checks decomposition torch _refs __init__ py constant_pad_nd torch _check len pad == lambda f Length pad must even instead equals len pad input_sizes = input shape l_inp = len input_sizes l_pad = len pad l_diff = l_inp - l_pad torch _check l_inp = l_pad lambda Length pad should no more than twice number f dimensions input Pad length len pad while input has f l_inp dimensions all isinstance p utils IntWithoutSymInt p = p pad c_input = input i range l_diff l_inp pad_idx = l_inp - i - pad pad_idx c_input = c_input narrow i -pad pad_idx c_input shape i + pad pad_idx pad pad_idx + c_input = c_input narrow i c_input shape i + pad pad_idx + c_input clone new_shape = list input_sizes l_diff i range l_pad pad_idx = len pad - i + new_dim = input_sizes l_diff + i + pad pad_idx + pad pad_idx + torch _check new_dim = lambda f The input size input_sizes l_diff + i plus negative padding f pad pad_idx pad pad_idx + resulted negative output size f which invalid Check dimension l_diff + i your input new_shape append new_dim torch empty new_shape dtype=input dtype device=input device requires_grad=input requires_grad memory_format=suggest_memory_format input register_meta aten embedding out_wrapper embedding weight Tensor indices Tensor padding_idx int = - scale_grad_by_freq bool = False sparse bool = False - Tensor assert weight dim == weight must -D weight_shape = weight shape indices_shape = indices shape indices ndim == out_shape tuple int = weight_shape indices ndim == out_shape = indices_shape weight_shape out_shape = indices_shape weight_shape out_dtype = weight dtype weight new_empty out_shape dtype=out_dtype register_meta aten _jagged_to_padded_dense_forward default meta__jagged_to_padded_dense_forward values Tensor offsets list Tensor max_lengths list int padding_value float = only one jagged dim supported now assert len offsets == assert len max_lengths == B = offsets shape - S = max_lengths output_shape = B S values shape values new_empty output_shape _create_unary_float_meta_func func register_meta func out_wrapper _f x elementwise_meta x type_promotion=ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT _f Implementation follows cuda implementation native_multi_head_attention_cuda register_meta aten _native_multi_head_attention default native_multi_head_attention_fake query key value embed_dim num_head qkv_weight qkv_bias proj_weight proj_bias mask=None need_weights=True average_attn_weights=True mask_type=None query is_nested key is_nested value is_nested raise NotImplementedError _native_multi_head_attention fake implementation does support nested tensors query numel == query new_empty query shape query new_empty B = query size B batch size T = query size T target sequence length In native_multi_head_attention_cuda we have proj = transform _gemm_nt_bias attn_ctx proj_weight proj_bias query which does attn_ctx proj_weight T + proj_bias so last dim output shape proj_weight size output_dim = proj_weight size output = query new_empty B T output_dim need_weights average_attn_weights When averaging attention weights shape B T T averaged over heads T = query seq len S = key value seq len attn_weights = query new_empty B T T When averaging shape B num_head T T T = query seq len S = key value seq len attn_weights = query new_empty B num_head T T attn_weights = query new_empty output attn_weights _create_binary_float_meta_func func register_meta func out_wrapper _f x y elementwise_meta x y type_promotion=ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT _f _create_unary_float_meta_func aten special_airy_ai _create_unary_float_meta_func aten special_bessel_y _create_unary_float_meta_func aten special_bessel_y _create_unary_float_meta_func aten special_modified_bessel_i _create_unary_float_meta_func aten special_modified_bessel_i _create_unary_float_meta_func aten special_modified_bessel_k _create_unary_float_meta_func aten special_modified_bessel_k _create_unary_float_meta_func aten special_scaled_modified_bessel_k _create_unary_float_meta_func aten special_scaled_modified_bessel_k _create_binary_float_meta_func aten special_chebyshev_polynomial_t _create_binary_float_meta_func aten special_chebyshev_polynomial_u _create_binary_float_meta_func aten special_chebyshev_polynomial_v _create_binary_float_meta_func aten special_chebyshev_polynomial_w _create_binary_float_meta_func aten special_shifted_chebyshev_polynomial_t _create_binary_float_meta_func aten special_shifted_chebyshev_polynomial_u _create_binary_float_meta_func aten special_shifted_chebyshev_polynomial_v _create_binary_float_meta_func aten special_shifted_chebyshev_polynomial_w _create_binary_float_meta_func aten special_hermite_polynomial_h _create_binary_float_meta_func aten special_hermite_polynomial_he _create_binary_float_meta_func aten special_laguerre_polynomial_l _create_binary_float_meta_func aten special_legendre_polynomial_p _register_inplace_meta fn wraps fn _fn args kwargs out = fn args kwargs check_inplace_broadcast shape out shape inplace_name = f fn __name__ _ _fn __name__ = inplace_name _fn = register_meta getattr aten inplace_name _fn type ignore assignment _fn register_meta aten lerp out_wrapper lerp start end weight torch _check start dtype == end dtype lambda f expected dtype start dtype ` end ` got dtype end dtype args = start end isinstance weight TensorLike weight ndim = torch _check start dtype == weight dtype lambda f expected dtype start dtype ` weight ` got dtype weight dtype args append weight elementwise_meta args type_promotion=ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT register_meta aten addcmul out_wrapper addcmul input tensor tensor value= elementwise_meta input tensor tensor type_promotion=ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT register_meta aten addcdiv out_wrapper addcdiv input tensor tensor value= torch _check utils is_integer_dtype tensor dtype utils is_integer_dtype tensor dtype lambda Integer division addcdiv no longer supported future release addcdiv will perform true division tensor tensor The historic addcdiv behavior can implemented input + value torch trunc tensor tensor input dtype integer inputs input + value tensor tensor float inputs The future addcdiv behavior just latter implementation input + value tensor tensor all dtypes elementwise_meta input tensor tensor type_promotion=ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT lerp_ = _register_inplace_meta aten lerp addcmul_ = _register_inplace_meta aten addcmul addcdiv_ = _register_inplace_meta aten addcdiv We must also trigger meta registrations PrimTorch ref decompositions torch _refs torch _refs nn functional torch _refs special activate_meta activate_meta_table = For given op we pick most specific decomp function global_decomp_table precedence order meta post_autograd pre_autograd type meta post_autograd pre_autograd registry = global_decomposition_table type opo registry opo activate_meta_table activate_meta_table opo = registry opo op_overload fn activate_meta_table items Don t register meta HigherOrderOp s decomp We can reconsider future general way you do meta HigherOrderOp different OpOverload isinstance op_overload torch _ops HigherOrderOperator continue assert isinstance op_overload OpOverload op_overload py_impl torch _C DispatchKey Meta fn torch _C _dispatch_has_kernel_for_dispatch_key op_overload name CompositeImplicitAutograd Internally we shouldn t registering meta kernels any operators have CompositeImplicitAutograd kernels Instead we should letting those decompositions run writing meta kernels only base operators op_overload global_decomposition_table meta raise RuntimeError f op_overload CompositeImplicitAutograd op we shouldn t register meta function Instead we should let decomposition run write meta kernels base operators op_overload is_view Attempting register python meta kernel view operator We shouldn t do because output will report having aliased storages All view ops have meta kernels C++ today so we should use those instead pass op_overload name aten empty_strided causing infinite recursion test_meta py aten clone causing infinite recursion aten _to_copy causing infinite recursion test_serialization py -k test_tensor_subclass_getstate_overwrite noqa B aten copy_ Exception raised test_torch py -k test_storage_meta_errors_cpu_int noqa B aten constant_pad_nd requires_grad mismatch test_ops py -k test_fake_crossref_backward_amp_istft_cuda_float noqa B aten rot requires_grad mismatch test_ops py -k test_fake_crossref_backward_amp_rot _cuda_float noqa B aten as_strided_scatter requires_grad mismatch test_ops py -k test_fake_crossref_backward_no_amp_as_strided_scatter_cuda_float noqa B pass mkldnn op_overload name _meta_lib_dont_use_me_use_register_meta_for_mkldnn impl op_overload fn mkl op_overload name _meta_lib_dont_use_me_use_register_meta_for_mkl impl op_overload fn onednn op_overload name _meta_lib_dont_use_me_use_register_meta_for_onednn impl op_overload fn quantized op_overload name _meta_lib_dont_use_me_use_register_meta_for_quantized impl op_overload fn _meta_lib_dont_use_me_use_register_meta impl op_overload fn activate_meta