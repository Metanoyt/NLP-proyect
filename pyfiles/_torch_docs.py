mypy allow-untyped-defs Adds docstrings functions defined torch _C module re torch _C torch _C _add_docstr add_docstr parse_kwargs desc r Map description args dictionary argname description Input weight Tensor weight tensor\n + Some optional description Output weight \ weight Tensor weight tensor\n Some optional description Split exactly spaces after newline regx = re compile r \n\s \s kwargs = section strip section regx split desc kwargs = section section kwargs len section desc split desc desc kwargs merge_dicts dicts Merge dictionaries into single dictionary x d x d dicts x d common_args = parse_kwargs input Tensor input tensor generator ` torch Generator ` optional pseudorandom number generator sampling out Tensor optional output tensor memory_format ` torch memory_format ` optional desired memory format returned tensor Default ` ` torch preserve_format ` ` reduceops_common_args = merge_dicts common_args parse_kwargs dtype ` torch dtype ` optional desired data type returned tensor If specified input tensor casted attr ` dtype ` before operation performed This useful preventing data type overflows Default None keepdim bool whether output tensor has attr ` dim ` retained opt_keepdim keepdim bool optional whether output tensor has attr ` dim ` retained Default ` ` False ` ` multi_dim_common = merge_dicts reduceops_common_args parse_kwargs dim int tuple ints dimension dimensions reduce keepdim_details If attr ` keepdim ` ` ` True ` ` output tensor same size attr ` input ` except dimension s attr ` dim ` where size Otherwise attr ` dim ` squeezed see func ` torch squeeze ` resulting output tensor having ` ` len dim ` ` fewer dimension s opt_dim dim int tuple ints optional dimension dimensions reduce opt_dim_all_reduce dim int tuple ints optional dimension dimensions reduce If ` ` None ` ` all dimensions reduced single_dim_common = merge_dicts reduceops_common_args parse_kwargs dim int dimension reduce opt_dim dim int optional dimension reduce opt_dim_all_reduce dim int optional dimension reduce If ` ` None ` ` all dimensions reduced opt_dim_without_none dim int optional dimension reduce If omitted all dimensions reduced Explicit ` ` None ` ` supported keepdim_details If attr ` keepdim ` ` ` True ` ` output tensor same size attr ` input ` except dimension attr ` dim ` where size Otherwise attr ` dim ` squeezed see func ` torch squeeze ` resulting output tensor having fewer dimension than attr ` input ` factory_common_args = merge_dicts common_args parse_kwargs dtype ` torch dtype ` optional desired data type returned tensor Default ` ` None ` ` uses global default see func ` torch set_default_dtype ` layout ` torch layout ` optional desired layout returned Tensor Default ` ` torch strided ` ` device ` torch device ` optional desired device returned tensor Default ` ` None ` ` uses current device default tensor type see func ` torch set_default_device ` attr ` device ` will CPU CPU tensor types current CUDA device CUDA tensor types requires_grad bool optional If autograd should record operations returned tensor Default ` ` False ` ` pin_memory bool optional If set returned tensor would allocated pinned memory Works only CPU tensors Default ` ` False ` ` memory_format ` torch memory_format ` optional desired memory format returned Tensor Default ` ` torch contiguous_format ` ` check_invariants bool optional If sparse tensor invariants checked Default returned func ` torch sparse check_sparse_tensor_invariants is_enabled ` initially False sparse_factory_device_note \ note If ` ` device ` ` argument specified device given attr ` values ` indices tensor s must match If however argument specified input Tensors will converted given device turn determine device constructed sparse tensor factory_like_common_args = parse_kwargs input Tensor size attr ` input ` will determine size output tensor layout ` torch layout ` optional desired layout returned tensor Default ` ` None ` ` defaults layout attr ` input ` dtype ` torch dtype ` optional desired data type returned Tensor Default ` ` None ` ` defaults dtype attr ` input ` device ` torch device ` optional desired device returned tensor Default ` ` None ` ` defaults device attr ` input ` requires_grad bool optional If autograd should record operations returned tensor Default ` ` False ` ` pin_memory bool optional If set returned tensor would allocated pinned memory Works only CPU tensors Default ` ` False ` ` memory_format ` torch memory_format ` optional desired memory format returned Tensor Default ` ` torch preserve_format ` ` factory_data_common_args = parse_kwargs data array_like Initial data tensor Can list tuple NumPy ` ` ndarray ` ` scalar other types dtype ` torch dtype ` optional desired data type returned tensor Default ` ` None ` ` infers data type attr ` data ` device ` torch device ` optional desired device returned tensor Default ` ` None ` ` uses current device default tensor type see func ` torch set_default_device ` attr ` device ` will CPU CPU tensor types current CUDA device CUDA tensor types requires_grad bool optional If autograd should record operations returned tensor Default ` ` False ` ` pin_memory bool optional If set returned tensor would allocated pinned memory Works only CPU tensors Default ` ` False ` ` tf _notes = tf _note This operator supports ref ` TensorFloat tf _on_ampere ` rocm_fp _notes = rocm_fp _note On certain ROCm devices when using float inputs module will use \ ref ` different precision fp _on_mi ` backward reproducibility_notes dict str str = forward_reproducibility_note This operation may behave nondeterministically when given tensors \ CUDA device See doc ` notes randomness ` more information backward_reproducibility_note This operation may produce nondeterministic gradients when given tensors \ CUDA device See doc ` notes randomness ` more information cudnn_reproducibility_note In some circumstances when given tensors CUDA device \ using CuDNN operator may select nondeterministic algorithm increase performance If \ undesirable you can try make operation deterministic potentially \ performance cost setting ` ` torch backends cudnn deterministic = True ` ` \ See doc ` notes randomness ` more information sparse_support_notes = sparse_beta_warning warning Sparse support beta feature some layout s dtype device combinations may supported may have autograd support If you notice missing functionality please open feature request add_docstr torch abs r abs input Tensor out Optional Tensor - Tensor Computes absolute value each element attr ` input ` math \text out _ i = &#124; \text input _ i &#124; + r Args input Keyword args out Example torch abs torch tensor - - tensor format common_args add_docstr torch absolute r absolute input Tensor out Optional Tensor - Tensor Alias func ` torch abs ` add_docstr torch acos r acos input Tensor out Optional Tensor - Tensor Returns new tensor arccosine radians each element attr ` input ` math \text out _ i = \cos^ - \text input _ i + r Args input Keyword args out Example = torch randn tensor - - torch acos tensor format common_args add_docstr torch arccos r arccos input Tensor out Optional Tensor - Tensor Alias func ` torch acos ` add_docstr torch acosh r acosh input Tensor out Optional Tensor - Tensor Returns new tensor inverse hyperbolic cosine elements attr ` input ` math \text out _ i = \cosh^ - \text input _ i Note The domain inverse hyperbolic cosine ` inf ` values outside range will mapped ` ` NaN ` ` except ` + INF ` which output mapped ` + INF ` + r Args input Keyword arguments out Example = torch randn uniform_ tensor torch acosh tensor format common_args add_docstr torch arccosh r arccosh input Tensor out Optional Tensor - Tensor Alias func ` torch acosh ` add_docstr torch index_add r index_add input Tensor dim int index Tensor source Tensor alpha Union Number _complex = out Optional Tensor - Tensor noqa B See meth ` ~Tensor index_add_ ` function description add_docstr torch index_copy r index_copy input Tensor dim int index Tensor source Tensor out Optional Tensor - Tensor See meth ` ~Tensor index_add_ ` function description add_docstr torch index_reduce r index_reduce input Tensor dim int index Tensor source Tensor reduce str include_self bool = True out Optional Tensor - Tensor noqa B See meth ` ~Tensor index_reduce_ ` function description add_docstr torch add r add input other alpha= out=None - Tensor Adds attr ` other ` scaled attr ` alpha ` attr ` input ` math \text out _i = \text input _i + \text alpha \times \text other _i + r Supports ref ` broadcasting common shape broadcasting-semantics ` ref ` type promotion type-promotion-doc ` integer float complex inputs Args input other Tensor Number tensor number add attr ` input ` Keyword arguments alpha Number multiplier attr ` other ` out Examples = torch randn tensor - torch add tensor b = torch randn b tensor - - c = torch randn c tensor - - - torch add b c alpha= tensor - - - - - - - - - - - - format common_args add_docstr torch addbmm r addbmm input batch batch beta= alpha= out=None - Tensor Performs batch matrix-matrix product matrices stored attr ` batch ` attr ` batch ` reduced add step all matrix multiplications get accumulated along first dimension attr ` input ` added final result attr ` batch ` attr ` batch ` must -D tensors each containing same number matrices If attr ` batch ` math ` b \times n \times m ` tensor attr ` batch ` math ` b \times m \times p ` tensor attr ` input ` must ref ` broadcastable broadcasting-semantics ` math ` n \times p ` tensor attr ` out ` will math ` n \times p ` tensor math out = \beta\ \text input + \alpha\ \sum_ i= ^ b- \text batch _i \mathbin \text batch _i If attr ` beta ` then content attr ` input ` will ignored ` nan ` ` inf ` will propagated + r For inputs type ` FloatTensor ` ` DoubleTensor ` arguments attr ` beta ` attr ` alpha ` must real numbers otherwise they should integers tf _note rocm_fp _note Args input Tensor matrix added batch Tensor first batch matrices multiplied batch Tensor second batch matrices multiplied Keyword args beta Number optional multiplier attr ` input ` math ` \beta ` alpha Number optional multiplier ` batch batch ` math ` \alpha ` out Example M = torch randn batch = torch randn batch = torch randn torch addbmm M batch batch tensor - - - - - - - format common_args tf _notes rocm_fp _notes add_docstr torch addcdiv r addcdiv input tensor tensor value= out=None - Tensor Performs element-wise division attr ` tensor ` attr ` tensor ` multiplies result scalar attr ` value ` adds attr ` input ` warning Integer division addcdiv no longer supported future release addcdiv will perform true division tensor tensor The historic addcdiv behavior can implemented input + value torch trunc tensor tensor input dtype integer inputs input + value tensor tensor float inputs The future addcdiv behavior just latter implementation input + value tensor tensor all dtypes math \text out _i = \text input _i + \text value \times \frac \text tensor _i \text tensor _i + r The shapes attr ` input ` attr ` tensor ` attr ` tensor ` must ref ` broadcastable broadcasting-semantics ` For inputs type ` FloatTensor ` ` DoubleTensor ` attr ` value ` must real number otherwise integer Args input Tensor tensor added tensor Tensor numerator tensor tensor Tensor denominator tensor Keyword args value Number optional multiplier math ` \text tensor \text tensor ` out Example t = torch randn t = torch randn t = torch randn torch addcdiv t t t value= tensor - - - - - - format common_args add_docstr torch addcmul r addcmul input tensor tensor value= out=None - Tensor Performs element-wise multiplication attr ` tensor ` attr ` tensor ` multiplies result scalar attr ` value ` adds attr ` input ` math \text out _i = \text input _i + \text value \times \text tensor _i \times \text tensor _i + r The shapes attr ` tensor ` attr ` tensor ` attr ` tensor ` must ref ` broadcastable broadcasting-semantics ` For inputs type ` FloatTensor ` ` DoubleTensor ` attr ` value ` must real number otherwise integer Args input Tensor tensor added tensor Tensor tensor multiplied tensor Tensor tensor multiplied Keyword args value Number optional multiplier math ` tensor tensor ` out Example t = torch randn t = torch randn t = torch randn torch addcmul t t t value= tensor - - - - - - format common_args add_docstr torch addmm r addmm input mat mat out_dtype=None beta= alpha= out=None - Tensor Performs matrix multiplication matrices attr ` mat ` attr ` mat ` The matrix attr ` input ` added final result If attr ` mat ` math ` n \times m ` tensor attr ` mat ` math ` m \times p ` tensor then attr ` input ` must ref ` broadcastable broadcasting-semantics ` math ` n \times p ` tensor attr ` out ` will math ` n \times p ` tensor attr ` alpha ` attr ` beta ` scaling factors matrix-vector product between attr ` mat ` attr ` mat ` added matrix attr ` input ` respectively math \text out = \beta\ \text input + \alpha\ \text mat _i \mathbin \text mat _i If attr ` beta ` then content attr ` input ` will ignored ` nan ` ` inf ` will propagated + r For inputs type ` FloatTensor ` ` DoubleTensor ` arguments attr ` beta ` attr ` alpha ` must real numbers otherwise they should integers This operation has support arguments ref ` sparse layouts sparse-docs ` If attr ` input ` sparse result will have same layout attr ` out ` provided must have same layout attr ` input ` sparse_beta_warning tf _note rocm_fp _note Args input Tensor matrix added mat Tensor first matrix matrix multiplied mat Tensor second matrix matrix multiplied out_dtype dtype optional dtype output tensor Supported only CUDA torch float given torch float torch bfloat input dtypes Keyword args beta Number optional multiplier attr ` input ` math ` \beta ` alpha Number optional multiplier math ` mat mat ` math ` \alpha ` out Example M = torch randn mat = torch randn mat = torch randn torch addmm M mat mat tensor - - - - format common_args tf _notes rocm_fp _notes sparse_support_notes add_docstr torch adjoint r adjoint input Tensor - Tensor Returns view tensor conjugated last two dimensions transposed ` ` x adjoint ` ` equivalent ` ` x transpose - - conj ` ` complex tensors ` ` x transpose - - ` ` real tensors Args input Example x = torch arange dtype=torch float A = torch complex x x reshape A tensor + j + j + j + j A adjoint tensor - j - j - j - j A adjoint == A mH all tensor True add_docstr torch sspaddmm r sspaddmm input mat mat beta= alpha= out=None - Tensor Matrix multiplies sparse tensor attr ` mat ` dense tensor attr ` mat ` then adds sparse tensor attr ` input ` result Note This function equivalent func ` torch addmm ` except attr ` input ` attr ` mat ` sparse Args input Tensor sparse matrix added mat Tensor sparse matrix matrix multiplied mat Tensor dense matrix matrix multiplied Keyword args beta Number optional multiplier attr ` mat ` math ` \beta ` alpha Number optional multiplier math ` mat mat ` math ` \alpha ` out format common_args add_docstr torch smm r smm input mat - Tensor Performs matrix multiplication sparse matrix attr ` input ` dense matrix attr ` mat ` Args input Tensor sparse matrix matrix multiplied mat Tensor dense matrix matrix multiplied add_docstr torch addmv r addmv input mat vec beta= alpha= out=None - Tensor Performs matrix-vector product matrix attr ` mat ` vector attr ` vec ` The vector attr ` input ` added final result If attr ` mat ` math ` n \times m ` tensor attr ` vec ` -D tensor size ` m ` then attr ` input ` must ref ` broadcastable broadcasting-semantics ` -D tensor size ` n ` attr ` out ` will -D tensor size ` n ` attr ` alpha ` attr ` beta ` scaling factors matrix-vector product between attr ` mat ` attr ` vec ` added tensor attr ` input ` respectively math \text out = \beta\ \text input + \alpha\ \text mat \mathbin \text vec If attr ` beta ` then content attr ` input ` will ignored ` nan ` ` inf ` will propagated + r For inputs type ` FloatTensor ` ` DoubleTensor ` arguments attr ` beta ` attr ` alpha ` must real numbers otherwise they should integers Args input Tensor vector added mat Tensor matrix matrix multiplied vec Tensor vector matrix multiplied Keyword args beta Number optional multiplier attr ` input ` math ` \beta ` alpha Number optional multiplier math ` mat vec ` math ` \alpha ` out Example M = torch randn mat = torch randn vec = torch randn torch addmv M mat vec tensor - - format common_args add_docstr torch addr r addr input vec vec beta= alpha= out=None - Tensor Performs outer-product vectors attr ` vec ` attr ` vec ` adds matrix attr ` input ` Optional values attr ` beta ` attr ` alpha ` scaling factors outer product between attr ` vec ` attr ` vec ` added matrix attr ` input ` respectively math \text out = \beta\ \text input + \alpha\ \text vec \otimes \text vec If attr ` beta ` then content attr ` input ` will ignored ` nan ` ` inf ` will propagated + r If attr ` vec ` vector size ` n ` attr ` vec ` vector size ` m ` then attr ` input ` must ref ` broadcastable broadcasting-semantics ` matrix size math ` n \times m ` attr ` out ` will matrix size math ` n \times m ` Args input Tensor matrix added vec Tensor first vector outer product vec Tensor second vector outer product Keyword args beta Number optional multiplier attr ` input ` math ` \beta ` alpha Number optional multiplier math ` \text vec \otimes \text vec ` math ` \alpha ` out Example vec = torch arange vec = torch arange M = torch zeros torch addr M vec vec tensor format common_args add_docstr torch allclose r allclose input Tensor other Tensor rtol float = e- atol float = e- equal_nan bool = False - bool This function checks attr ` input ` attr ` other ` satisfy condition math \lvert \text input _i - \text other _i \rvert \leq \texttt atol + \texttt rtol \times \lvert \text other _i \rvert + r elementwise all elements attr ` input ` attr ` other ` The behaviour function analogous ` numpy allclose https numpy org doc stable reference generated numpy allclose html ` _ Args input Tensor first tensor compare other Tensor second tensor compare atol float optional absolute tolerance Default e- rtol float optional relative tolerance Default e- equal_nan bool optional ` ` True ` ` then two ` ` NaN ` ` s will considered equal Default ` ` False ` ` Example torch allclose torch tensor e- torch tensor e- False torch allclose torch tensor e- torch tensor e- True torch allclose torch tensor float nan torch tensor float nan False torch allclose torch tensor float nan torch tensor float nan equal_nan=True True add_docstr torch all r all input Tensor out=None - Tensor Tests all elements attr ` input ` evaluate ` True ` note This function matches behaviour NumPy returning output dtype ` bool ` all supported dtypes except ` uint ` For ` uint ` dtype output ` uint ` itself Args input Keyword args out Example = torch rand bool tensor False True dtype=torch bool torch all tensor False dtype=torch bool = torch arange tensor torch all tensor False function all input dim keepdim=False out=None - Tensor noindex For each row attr ` input ` given dimension attr ` dim ` returns ` True ` all elements row evaluate ` True ` ` False ` otherwise keepdim_details Args input opt_dim_all_reduce opt_keepdim Keyword args out Example = torch rand bool tensor True True True False True True True True dtype=torch bool torch all dim= tensor True False True True dtype=torch bool torch all dim= tensor True False dtype=torch bool format multi_dim_common add_docstr torch any r any input Tensor out Optional Tensor - Tensor Tests any element attr ` input ` evaluates ` True ` note This function matches behaviour NumPy returning output dtype ` bool ` all supported dtypes except ` uint ` For ` uint ` dtype output ` uint ` itself Args input Keyword args out Example = torch rand bool tensor False True dtype=torch bool torch any tensor True dtype=torch bool = torch arange tensor torch any tensor True function any input dim keepdim=False out=None - Tensor noindex For each row attr ` input ` given dimension attr ` dim ` returns ` True ` any element row evaluate ` True ` ` False ` otherwise keepdim_details Args input opt_dim_all_reduce opt_keepdim Keyword args out Example = torch randn tensor True True False True True True False False torch any tensor True True True False torch any tensor True True format multi_dim_common add_docstr torch angle r angle input Tensor out Optional Tensor - Tensor Computes element-wise angle radians given attr ` input ` tensor math \text out _ i = angle \text input _ i + r Args input Keyword args out note Starting PyTorch angle returns pi negative real numbers zero non-negative real numbers propagates NaNs Previously function would zero all real numbers propagate floating-point NaNs Example torch angle torch tensor - + j - + j - j tensor - format common_args add_docstr torch as_strided r as_strided input size stride storage_offset=None - Tensor Create view existing ` torch Tensor ` attr ` input ` specified attr ` size ` attr ` stride ` attr ` storage_offset ` warning Prefer using other view functions like meth ` torch Tensor view ` meth ` torch Tensor expand ` setting view s strides manually ` as_strided ` function will throw error non-standard Pytorch backends do have concept stride result will depend current layout memory The constructed view must only refer elements within Tensor s storage runtime error will thrown If generated view overlapped multiple indices referring same element memory behavior inplace operations view undefined might throw runtime errors Args input size tuple ints shape output tensor stride tuple ints stride output tensor storage_offset int optional offset underlying storage output tensor If ` ` None ` ` storage_offset output tensor will match input tensor Example x = torch randn x tensor - - - t = torch as_strided x t tensor t = torch as_strided x tensor format common_args add_docstr torch as_tensor r as_tensor data Any dtype Optional dtype = None device Optional DeviceLikeType - Tensor Converts attr ` data ` into tensor sharing data preserving autograd history possible If attr ` data ` already tensor requested dtype device then attr ` data ` itself returned attr ` data ` tensor different dtype device then s copied using ` data dtype=dtype device=device ` If attr ` data ` NumPy array ndarray same dtype device then tensor constructed using func ` torch from_numpy ` If attr ` data ` CuPy array returned tensor will located same device CuPy array unless specifically overwritten attr ` device ` default device The device CuPy array inferred pointer array using ` cudaPointerGetAttributes ` unless attr ` device ` provided explicit device index seealso func ` torch tensor ` never shares its data creates new leaf tensor see doc ` notes autograd ` Args data dtype device ` torch device ` optional device constructed tensor If None data tensor then device data used If None data tensor then result tensor constructed current device Example = numpy array t = torch as_tensor t tensor t = - array - = numpy array t = torch as_tensor device=torch device cuda t tensor t = - array format factory_data_common_args add_docstr torch asin r asin input Tensor out Optional Tensor - Tensor Returns new tensor arcsine elements radians attr ` input ` tensor math \text out _ i = \sin^ - \text input _ i + r Args input Keyword args out Example = torch randn tensor - - torch asin tensor - nan - nan format common_args add_docstr torch arcsin r arcsin input Tensor out Optional Tensor - Tensor Alias func ` torch asin ` add_docstr torch asinh r asinh input Tensor out Optional Tensor - Tensor Returns new tensor inverse hyperbolic sine elements attr ` input ` math \text out _ i = \sinh^ - \text input _ i + r Args input Keyword arguments out Example = torch randn tensor - - - torch asinh tensor - - - format common_args add_docstr torch arcsinh r arcsinh input Tensor out Optional Tensor - Tensor Alias func ` torch asinh ` add_docstr torch atan r atan input Tensor out Optional Tensor - Tensor Returns new tensor arctangent elements radians attr ` input ` tensor math \text out _ i = \tan^ - \text input _ i + r Args input Keyword args out Example = torch randn tensor - - torch atan tensor - - format common_args add_docstr torch arctan r arctan input Tensor out Optional Tensor - Tensor Alias func ` torch atan ` add_docstr torch atan r atan input Tensor other Tensor out Optional Tensor - Tensor Element-wise arctangent math ` \text input _ i \text other _ i ` consideration quadrant Returns new tensor signed angles radians between vector math ` \text other _ i \text input _ i ` vector math ` ` Note math ` \text other _ i ` second parameter x-coordinate while math ` \text input _ i ` first parameter y-coordinate The shapes ` ` input ` ` ` ` other ` ` must ref ` broadcastable broadcasting-semantics ` Args input Tensor first input tensor other Tensor second input tensor Keyword args out Example = torch randn tensor - - torch atan torch randn tensor - - format common_args add_docstr torch arctan r arctan input Tensor other Tensor out Optional Tensor - Tensor Alias func ` torch atan ` add_docstr torch atanh r atanh input Tensor out Optional Tensor - Tensor Returns new tensor inverse hyperbolic tangent elements attr ` input ` Note The domain inverse hyperbolic tangent ` - ` values outside range will mapped ` ` NaN ` ` except values ` ` ` - ` which output mapped ` + -INF ` respectively math \text out _ i = \tanh^ - \text input _ i + r Args input Keyword arguments out Example = torch randn uniform_ - tensor - - - torch atanh tensor - - - format common_args add_docstr torch arctanh r arctanh input Tensor out Optional Tensor - Tensor Alias func ` torch atanh ` add_docstr torch asarray r asarray obj Any dtype Optional dtype device Optional DeviceLikeType copy Optional bool = None requires_grad bool = False - Tensor noqa B Converts attr ` obj ` tensor attr ` obj ` can one tensor NumPy array NumPy scalar DLPack capsule object implements Python s buffer protocol scalar sequence scalars When attr ` obj ` tensor NumPy array DLPack capsule returned tensor will default require gradient have same datatype attr ` obj ` same device share memory These properties can controlled attr ` dtype ` attr ` device ` attr ` copy ` attr ` requires_grad ` keyword arguments If returned tensor different datatype different device copy requested then will share its memory attr ` obj ` If attr ` requires_grad ` ` ` True ` ` then returned tensor will require gradient attr ` obj ` also tensor autograd history then returned tensor will have same history When attr ` obj ` tensor NumPy array DLPack capsule implements Python s buffer protocol then buffer interpreted array bytes grouped according size datatype passed attr ` dtype ` keyword argument If no datatype passed then default floating point datatype used instead The returned tensor will have specified datatype default floating point datatype none specified default CPU device share memory buffer When attr ` obj ` NumPy scalar returned tensor will -dimensional tensor CPU doesn t share its memory i e ` ` copy=True ` ` By default datatype will PyTorch datatype corresponding NumPy s scalar s datatype When attr ` obj ` none above scalar sequence scalars then returned tensor will default infer its datatype scalar values current default device share its memory seealso func ` torch tensor ` creates tensor always copies data input object func ` torch from_numpy ` creates tensor always shares memory NumPy arrays func ` torch frombuffer ` creates tensor always shares memory objects implement buffer protocol func ` torch from_dlpack ` creates tensor always shares memory DLPack capsules Args obj object tensor NumPy array DLPack Capsule object implements Python s buffer protocol scalar sequence scalars Keyword args dtype ` torch dtype ` optional datatype returned tensor Default ` ` None ` ` which causes datatype returned tensor inferred attr ` obj ` copy bool optional controls whether returned tensor shares memory attr ` obj ` Default ` ` None ` ` which causes returned tensor share memory attr ` obj ` whenever possible If ` ` True ` ` then returned tensor does share its memory If ` ` False ` ` then returned tensor shares its memory attr ` obj ` error thrown cannot device ` torch device ` optional device returned tensor Default ` ` None ` ` which causes device attr ` obj ` used Or attr ` obj ` Python sequence current default device will used requires_grad bool optional whether returned tensor requires grad Default ` ` False ` ` which causes returned tensor require gradient If ` ` True ` ` then returned tensor will require gradient attr ` obj ` also tensor autograd history then returned tensor will have same history Example = torch tensor Shares memory tensor b = torch asarray data_ptr == b data_ptr True Forces memory copy c = torch asarray copy=True data_ptr == c data_ptr False = torch tensor requires_grad=True b = + b tensor grad_fn= AddBackward Shares memory tensor b no grad c = torch asarray b c tensor Shares memory tensor b retaining autograd history d = torch asarray b requires_grad=True d tensor grad_fn= AddBackward array = numpy array Shares memory array array t = torch asarray array array __array_interface__ data == t data_ptr True Copies memory due dtype mismatch t = torch asarray array dtype=torch float array __array_interface__ data == t data_ptr False scalar = numpy float torch asarray scalar tensor dtype=torch float add_docstr torch baddbmm r baddbmm input batch batch out_dtype=None beta= alpha= out=None - Tensor Performs batch matrix-matrix product matrices attr ` batch ` attr ` batch ` attr ` input ` added final result attr ` batch ` attr ` batch ` must -D tensors each containing same number matrices If attr ` batch ` math ` b \times n \times m ` tensor attr ` batch ` math ` b \times m \times p ` tensor then attr ` input ` must ref ` broadcastable broadcasting-semantics ` math ` b \times n \times p ` tensor attr ` out ` will math ` b \times n \times p ` tensor Both attr ` alpha ` attr ` beta ` mean same scaling factors used meth ` torch addbmm ` math \text out _i = \beta\ \text input _i + \alpha\ \text batch _i \mathbin \text batch _i If attr ` beta ` then content attr ` input ` will ignored ` nan ` ` inf ` will propagated + r For inputs type ` FloatTensor ` ` DoubleTensor ` arguments attr ` beta ` attr ` alpha ` must real numbers otherwise they should integers tf _note rocm_fp _note Args input Tensor tensor added batch Tensor first batch matrices multiplied batch Tensor second batch matrices multiplied out_dtype dtype optional dtype output tensor Supported only CUDA torch float given torch float torch bfloat input dtypes Keyword args beta Number optional multiplier attr ` input ` math ` \beta ` alpha Number optional multiplier math ` \text batch \mathbin \text batch ` math ` \alpha ` out Example M = torch randn batch = torch randn batch = torch randn torch baddbmm M batch batch size torch Size format common_args tf _notes rocm_fp _notes add_docstr torch bernoulli r bernoulli input Tensor generator Optional Generator out Optional Tensor - Tensor Draws binary random numbers Bernoulli distribution The attr ` input ` tensor should tensor containing probabilities used drawing binary random number Hence all values attr ` input ` have range math ` \leq \text input _i \leq ` The math ` \text i ^ th ` element output tensor will draw value math ` ` according math ` \text i ^ th ` probability value given attr ` input ` math \text out _ i \sim \mathrm Bernoulli p = \text input _ i + r The returned attr ` out ` tensor only has values same shape attr ` input ` attr ` out ` can have integral ` ` dtype ` ` attr ` input ` must have floating point ` ` dtype ` ` Args input Tensor input tensor probability values Bernoulli distribution Keyword args generator out Example = torch empty uniform_ generate uniform random matrix range tensor torch bernoulli tensor = torch ones probability drawing torch bernoulli tensor = torch zeros probability drawing torch bernoulli tensor format common_args add_docstr torch bincount r bincount input weights=None minlength= - Tensor Count frequency each value array non-negative ints The number bins size one larger than largest value attr ` input ` unless attr ` input ` empty which case result tensor size If attr ` minlength ` specified number bins least attr ` minlength ` attr ` input ` empty then result tensor size attr ` minlength ` filled zeros If ` ` n ` ` value position ` ` i ` ` ` ` out n += weights i ` ` attr ` weights ` specified ` ` out n += ` ` Note backward_reproducibility_note Arguments input Tensor -d int tensor weights Tensor optional weight each value input tensor Should same size input tensor minlength int optional minimum number bins Should non-negative Returns output Tensor tensor shape ` ` Size max input + ` ` attr ` input ` non-empty ` ` Size ` ` Example input = torch randint dtype=torch int weights = torch linspace steps= input weights tensor tensor torch bincount input tensor input bincount weights tensor format reproducibility_notes add_docstr torch bitwise_not r bitwise_not input out=None - Tensor Computes bitwise NOT given input tensor The input tensor must integral Boolean types For bool tensors computes logical NOT Args input Keyword args out Example torch bitwise_not torch tensor - - dtype=torch int tensor - dtype=torch int format common_args add_docstr torch bmm r bmm input mat out_dtype=None out=None - Tensor Performs batch matrix-matrix product matrices stored attr ` input ` attr ` mat ` attr ` input ` attr ` mat ` must -D tensors each containing same number matrices If attr ` input ` math ` b \times n \times m ` tensor attr ` mat ` math ` b \times m \times p ` tensor attr ` out ` will math ` b \times n \times p ` tensor math \text out _i = \text input _i \mathbin \text mat _i + r tf _note rocm_fp _note note This function does ref ` broadcast broadcasting-semantics ` For broadcasting matrix products see func ` torch matmul ` Args input Tensor first batch matrices multiplied mat Tensor second batch matrices multiplied out_dtype dtype optional dtype output tensor Supported only CUDA torch float given torch float torch bfloat input dtypes Keyword Args out Example input = torch randn mat = torch randn res = torch bmm input mat res size torch Size format common_args tf _notes rocm_fp _notes add_docstr torch bitwise_and r bitwise_and input other out=None - Tensor Computes bitwise AND attr ` input ` attr ` other ` The input tensor must integral Boolean types For bool tensors computes logical AND Args input first input tensor other second input tensor Keyword args out Example torch bitwise_and torch tensor - - dtype=torch int torch tensor dtype=torch int tensor dtype=torch int torch bitwise_and torch tensor True True False torch tensor False True False tensor False True False format common_args add_docstr torch bitwise_or r bitwise_or input Tensor other Tensor out Optional Tensor - Tensor Computes bitwise OR attr ` input ` attr ` other ` The input tensor must integral Boolean types For bool tensors computes logical OR Args input first input tensor other second input tensor Keyword args out Example torch bitwise_or torch tensor - - dtype=torch int torch tensor dtype=torch int tensor - - dtype=torch int torch bitwise_or torch tensor True True False torch tensor False True False tensor True True False format common_args add_docstr torch bitwise_xor r bitwise_xor input other out=None - Tensor Computes bitwise XOR attr ` input ` attr ` other ` The input tensor must integral Boolean types For bool tensors computes logical XOR Args input first input tensor other second input tensor Keyword args out Example torch bitwise_xor torch tensor - - dtype=torch int torch tensor dtype=torch int tensor - - dtype=torch int torch bitwise_xor torch tensor True True False torch tensor False True False tensor True False False format common_args add_docstr torch bitwise_left_shift r bitwise_left_shift input other out=None - Tensor Computes left arithmetic shift attr ` input ` attr ` other ` bits The input tensor must integral type This operator supports ref ` broadcasting common shape broadcasting-semantics ` ref ` type promotion type-promotion-doc ` The operation applied math \text out _i = \text input _i \text other _i Args input Tensor Scalar first input tensor other Tensor Scalar second input tensor Keyword args out Example torch bitwise_left_shift torch tensor - - dtype=torch int torch tensor dtype=torch int tensor - - dtype=torch int format common_args add_docstr torch bitwise_right_shift r bitwise_right_shift input other out=None - Tensor Computes right arithmetic shift attr ` input ` attr ` other ` bits The input tensor must integral type This operator supports ref ` broadcasting common shape broadcasting-semantics ` ref ` type promotion type-promotion-doc ` In any case value right operand negative greater equal number bits promoted left operand behavior undefined The operation applied math \text out _i = \text input _i \text other _i Args input Tensor Scalar first input tensor other Tensor Scalar second input tensor Keyword args out Example torch bitwise_right_shift torch tensor - - dtype=torch int torch tensor dtype=torch int tensor - - dtype=torch int format common_args add_docstr torch broadcast_to r broadcast_to input shape - Tensor Broadcasts attr ` input ` shape attr ` \shape ` Equivalent calling ` ` input expand shape ` ` See meth ` ~Tensor expand ` details Args input shape list tuple ` torch Size ` new shape Example x = torch tensor torch broadcast_to x tensor format common_args add_docstr torch stack r stack tensors dim= out=None - Tensor Concatenates sequence tensors along new dimension All tensors need same size seealso func ` torch cat ` concatenates given sequence along existing dimension Arguments tensors sequence Tensors sequence tensors concatenate dim int optional dimension insert Has between number dimensions concatenated tensors inclusive Default Keyword args out Example x = torch randn x tensor - - torch stack x x same torch stack x x dim= tensor - - - - torch stack x x size torch Size torch stack x x dim= tensor - - - - torch stack x x dim= tensor - - - - torch stack x x dim=- tensor - - - - format common_args add_docstr torch hstack r hstack tensors out=None - Tensor Stack tensors sequence horizontally column wise This equivalent concatenation along first axis -D tensors along second axis all other tensors Args tensors sequence Tensors sequence tensors concatenate Keyword args out Example = torch tensor b = torch tensor torch hstack b tensor = torch tensor b = torch tensor torch hstack b tensor format common_args add_docstr torch vstack r vstack tensors out=None - Tensor Stack tensors sequence vertically row wise This equivalent concatenation along first axis after all -D tensors have been reshaped func ` torch atleast_ d ` Args tensors sequence Tensors sequence tensors concatenate Keyword args out Example = torch tensor b = torch tensor torch vstack b tensor = torch tensor b = torch tensor torch vstack b tensor format common_args add_docstr torch dstack r dstack tensors out=None - Tensor Stack tensors sequence depthwise along third axis This equivalent concatenation along third axis after -D -D tensors have been reshaped func ` torch atleast_ d ` Args tensors sequence Tensors sequence tensors concatenate Keyword args out Example = torch tensor b = torch tensor torch dstack b tensor = torch tensor b = torch tensor torch dstack b tensor format common_args add_docstr torch tensor_split r tensor_split input indices_or_sections dim= - List Tensors Splits tensor into multiple sub-tensors all which views attr ` input ` along dimension attr ` dim ` according indices number sections specified attr ` indices_or_sections ` This function based NumPy s func ` numpy array_split ` Args input Tensor tensor split indices_or_sections Tensor int list tuple ints If attr ` indices_or_sections ` integer ` ` n ` ` zero dimensional long tensor value ` ` n ` ` attr ` input ` split into ` ` n ` ` sections along dimension attr ` dim ` If attr ` input ` divisible ` ` n ` ` along dimension attr ` dim ` each section will equal size code ` input size dim n ` If attr ` input ` divisible ` ` n ` ` sizes first code ` int input size dim n ` sections will have size code ` int input size dim n + ` rest will have size code ` int input size dim n ` If attr ` indices_or_sections ` list tuple ints one-dimensional long tensor then attr ` input ` split along dimension attr ` dim ` each indices list tuple tensor For instance code ` indices_or_sections= ` code ` dim= ` would result tensors code ` input ` code ` input ` code ` input ` If attr ` indices_or_sections ` tensor must zero-dimensional one-dimensional long tensor CPU dim int optional dimension along which split tensor Default ` ` ` ` Example x = torch arange torch tensor_split x tensor tensor tensor x = torch arange torch tensor_split x tensor tensor tensor torch tensor_split x tensor tensor tensor x = torch arange reshape x tensor torch tensor_split x dim= tensor tensor tensor torch tensor_split x dim= tensor tensor tensor add_docstr torch chunk r chunk input Tensor chunks int dim int = - Tuple Tensor Attempts split tensor into specified number chunks Each chunk view input tensor note This function may fewer than specified number chunks seealso func ` torch tensor_split ` function always returns exactly specified number chunks If tensor size along given dimension attr ` dim ` divisible attr ` chunks ` all returned chunks will same size If tensor size along given dimension attr ` dim ` divisible attr ` chunks ` all returned chunks will same size except last one If such division possible function may fewer than specified number chunks Arguments input Tensor tensor split chunks int number chunks dim int dimension along which split tensor Example torch arange chunk tensor tensor tensor tensor tensor tensor torch arange chunk tensor tensor tensor tensor tensor tensor torch arange chunk tensor tensor tensor tensor tensor add_docstr torch unsafe_chunk r unsafe_chunk input chunks dim= - List Tensors Works like func ` torch chunk ` without enforcing autograd restrictions inplace modification outputs warning This function safe use long only input only outputs modified inplace after calling function It user s responsibility ensure case If both input one more outputs modified inplace gradients computed autograd will silently incorrect add_docstr torch unsafe_split r unsafe_split tensor split_size_or_sections dim= - List Tensors Works like func ` torch split ` without enforcing autograd restrictions inplace modification outputs warning This function safe use long only input only outputs modified inplace after calling function It user s responsibility ensure case If both input one more outputs modified inplace gradients computed autograd will silently incorrect add_docstr torch hsplit r hsplit input indices_or_sections - List Tensors Splits attr ` input ` tensor one more dimensions into multiple tensors horizontally according attr ` indices_or_sections ` Each split view attr ` input ` If attr ` input ` one dimensional equivalent calling torch tensor_split input indices_or_sections dim= split dimension zero attr ` input ` has two more dimensions s equivalent calling torch tensor_split input indices_or_sections dim= split dimension except attr ` indices_or_sections ` integer must evenly divide split dimension runtime error will thrown This function based NumPy s func ` numpy hsplit ` Args input Tensor tensor split indices_or_sections int list tuple ints See argument func ` torch tensor_split ` Example t = torch arange reshape t tensor torch hsplit t tensor tensor torch hsplit t tensor tensor tensor size= add_docstr torch vsplit r vsplit input indices_or_sections - List Tensors Splits attr ` input ` tensor two more dimensions into multiple tensors vertically according attr ` indices_or_sections ` Each split view attr ` input ` This equivalent calling torch tensor_split input indices_or_sections dim= split dimension except attr ` indices_or_sections ` integer must evenly divide split dimension runtime error will thrown This function based NumPy s func ` numpy vsplit ` Args input Tensor tensor split indices_or_sections int list tuple ints See argument func ` torch tensor_split ` Example t = torch arange reshape t tensor torch vsplit t tensor tensor torch vsplit t tensor tensor tensor size= add_docstr torch dsplit r dsplit input indices_or_sections - List Tensors Splits attr ` input ` tensor three more dimensions into multiple tensors depthwise according attr ` indices_or_sections ` Each split view attr ` input ` This equivalent calling torch tensor_split input indices_or_sections dim= split dimension except attr ` indices_or_sections ` integer must evenly divide split dimension runtime error will thrown This function based NumPy s func ` numpy dsplit ` Args input Tensor tensor split indices_or_sections int list tuple ints See argument func ` torch tensor_split ` Example t = torch arange reshape t tensor torch dsplit t tensor tensor torch dsplit t tensor tensor tensor size= add_docstr torch can_cast r can_cast from_ - bool Determines type conversion allowed under PyTorch casting rules described type promotion ref ` documentation type-promotion-doc ` Args from\_ dtype The original ` torch dtype ` dtype The target ` torch dtype ` Example torch can_cast torch double torch float True torch can_cast torch float torch int False add_docstr torch corrcoef r corrcoef input - Tensor Estimates Pearson product-moment correlation coefficient matrix variables given attr ` input ` matrix where rows variables columns observations note The correlation coefficient matrix R computed using covariance matrix C given math ` R_ ij = \frac C_ ij \sqrt C_ ii C_ jj ` note Due floating point rounding resulting array may Hermitian its diagonal elements may The real imaginary values clipped interval - attempt improve situation Args input Tensor A D matrix containing multiple variables observations Scalar D vector representing single variable Returns Tensor The correlation coefficient matrix variables seealso func ` torch cov ` covariance matrix Example x = torch tensor torch corrcoef x tensor - - x = torch randn x tensor - - - - torch corrcoef x tensor torch corrcoef x tensor add_docstr torch cov r cov input correction= fweights=None aweights=None - Tensor Estimates covariance matrix variables given attr ` input ` matrix where rows variables columns observations A covariance matrix square matrix giving covariance each pair variables The diagonal contains variance each variable covariance variable itself By definition attr ` input ` represents single variable Scalar D then its variance returned The sample covariance variables math ` x ` math ` y ` given math \text cov x y = \frac \sum^ N _ i = x_ i - \bar x y_ i - \bar y \max ~N~-~\delta N where math ` \bar x ` math ` \bar y ` simple means math ` x ` math ` y ` respectively math ` \delta N ` attr ` correction ` If attr ` fweights ` attr ` aweights ` provided weighted covariance calculated which given math \text cov _w x y = \frac \sum^ N _ i = w_i x_ i - \mu_x^ y_ i - \mu_y^ \max ~\sum^ N _ i = w_i~-~\frac \sum^ N _ i = w_ia_i \sum^ N _ i = w_i ~\delta N where math ` w ` denotes attr ` fweights ` attr ` aweights ` ` ` f ` ` ` ` ` ` brevity based whichever provided math ` w = f \times ` both provided math ` \mu_x^ = \frac \sum^ N _ i = w_ix_ i \sum^ N _ i = w_i ` weighted mean variable If provided ` ` f ` ` ` ` ` ` can seen math ` \mathbb ` vector appropriate size Args input Tensor A D matrix containing multiple variables observations Scalar D vector representing single variable Keyword Args correction int optional difference between sample size sample degrees freedom Defaults Bessel s correction ` ` correction = ` ` which returns unbiased estimate even both attr ` fweights ` attr ` aweights ` specified ` ` correction = ` ` will simple average Defaults ` ` ` ` fweights tensor optional A Scalar D tensor observation vector frequencies representing number times each observation should repeated Its numel must equal number columns attr ` input ` Must have integral dtype Ignored ` ` None ` ` Defaults ` ` None ` ` aweights tensor optional A Scalar D array observation vector weights These relative weights typically large observations considered important smaller observations considered less important Its numel must equal number columns attr ` input ` Must have floating point dtype Ignored ` ` None ` ` Defaults ` ` None ` ` Returns Tensor The covariance matrix variables seealso func ` torch corrcoef ` normalized covariance matrix Example x = torch tensor T x tensor torch cov x tensor - - torch cov x correction= tensor - - fw = torch randint fw tensor aw = torch rand aw tensor torch cov x fweights=fw aweights=aw tensor - - add_docstr torch cat r cat tensors dim= out=None - Tensor Concatenates given sequence tensors attr ` tensors ` given dimension All tensors must either have same shape except concatenating dimension -D empty tensor size ` ` ` ` func ` torch cat ` can seen inverse operation func ` torch split ` func ` torch chunk ` func ` torch cat ` can best understood via examples seealso func ` torch stack ` concatenates given sequence along new dimension Args tensors sequence Tensors Non-empty tensors provided must have same shape except cat dimension dim int optional dimension over which tensors concatenated Keyword args out Example x = torch randn x tensor - - - - torch cat x x x tensor - - - - - - - - - - - - torch cat x x x tensor - - - - - - - - - - - - format common_args add_docstr torch concat r concat tensors dim= out=None - Tensor Alias func ` torch cat ` add_docstr torch concatenate r concatenate tensors axis= out=None - Tensor Alias func ` torch cat ` add_docstr torch ceil r ceil input out=None - Tensor Returns new tensor ceil elements attr ` input ` smallest integer greater than equal each element For integer inputs follows array-api convention returning copy input tensor math \text out _ i = \left\lceil \text input _ i \right\rceil + r Args input Keyword args out Example = torch randn tensor - - - torch ceil tensor - - - format common_args add_docstr torch real r real input - Tensor Returns new tensor containing real values attr ` ` tensor The returned tensor attr ` ` share same underlying storage Args input Example x=torch randn dtype=torch cfloat x tensor + j - - j - - j - - j x real tensor - - - format common_args add_docstr torch imag r imag input - Tensor Returns new tensor containing imaginary values attr ` ` tensor The returned tensor attr ` ` share same underlying storage warning func ` imag ` only supported tensors complex dtypes Args input Example x=torch randn dtype=torch cfloat x tensor + j - - j - - j - - j x imag tensor - - - format common_args add_docstr torch view_as_real r view_as_real input - Tensor Returns view attr ` input ` real tensor For input complex tensor attr ` size ` math ` m m \dots mi ` function returns new real tensor size math ` m m \dots mi ` where last dimension size represents real imaginary components complex numbers warning func ` view_as_real ` only supported tensors ` ` complex dtypes ` ` Args input Example x=torch randn dtype=torch cfloat x tensor - j - - j - j - - j torch view_as_real x tensor - - - - - - format common_args add_docstr torch view_as_complex r view_as_complex input - Tensor Returns view attr ` input ` complex tensor For input complex tensor attr ` size ` math ` m m \dots mi ` function returns new complex tensor attr ` size ` math ` m m \dots mi ` where last dimension input tensor expected represent real imaginary components complex numbers warning func ` view_as_complex ` only supported tensors ` torch dtype ` ` ` torch float ` ` ` ` torch float ` ` The input expected have last dimension attr ` size ` In addition tensor must have ` stride ` its last dimension The strides all other dimensions must even numbers Args input Example x=torch randn x tensor - - - - - - torch view_as_complex x tensor - j - - j - j - - j format common_args add_docstr torch reciprocal r reciprocal input out=None - Tensor Returns new tensor reciprocal elements attr ` input ` math \text out _ i = \frac \text input _ i note Unlike NumPy s reciprocal torch reciprocal supports integral inputs Integral inputs reciprocal automatically ref ` promoted type-promotion-doc ` default scalar type + r Args input Keyword args out Example = torch randn tensor - - - torch reciprocal tensor - - - format common_args add_docstr torch cholesky r cholesky input upper=False out=None - Tensor Computes Cholesky decomposition symmetric positive-definite matrix math ` A ` batches symmetric positive-definite matrices If attr ` upper ` ` ` True ` ` returned matrix ` ` U ` ` upper-triangular decomposition has form math A = U^TU If attr ` upper ` ` ` False ` ` returned matrix ` ` L ` ` lower-triangular decomposition has form math A = LL^T If attr ` upper ` ` ` True ` ` math ` A ` batch symmetric positive-definite matrices then returned tensor will composed upper-triangular Cholesky factors each individual matrices Similarly when attr ` upper ` ` ` False ` ` returned tensor will composed lower-triangular Cholesky factors each individual matrices warning func ` torch cholesky ` deprecated favor func ` torch linalg cholesky ` will removed future PyTorch release ` ` L = torch cholesky A ` ` should replaced code python L = torch linalg cholesky A ` ` U = torch cholesky A upper=True ` ` should replaced code python U = torch linalg cholesky A mH This transform will produce equivalent results all valid symmetric positive definite inputs Args input Tensor input tensor math ` A ` size math ` n n ` where ` ` zero more batch dimensions consisting symmetric positive-definite matrices upper bool optional flag indicates whether upper lower triangular matrix Default ` ` False ` ` Keyword args out Tensor optional output matrix Example = torch randn = mT + e- make symmetric positive-definite l = torch cholesky tensor - - l tensor - l l mT tensor - - = torch randn Example batched input = mT + e- make symmetric positive-definite l = torch cholesky z = l l mT torch dist z tensor e- add_docstr torch cholesky_solve r cholesky_solve B L upper=False out=None - Tensor Computes solution system linear equations complex Hermitian real symmetric positive-definite lhs given its Cholesky decomposition Let math ` A ` complex Hermitian real symmetric positive-definite matrix math ` L ` its Cholesky decomposition such math A = LL^ \text H where math ` L^ \text H ` conjugate transpose when math ` L ` complex transpose when math ` L ` real-valued Returns solution math ` X ` following linear system math AX = B Supports inputs float double cfloat cdouble dtypes Also supports batches matrices math ` A ` math ` B ` batch matrices then output has same batch dimensions Args B Tensor right-hand side tensor shape ` n k ` where math ` ` zero more batch dimensions L Tensor tensor shape ` n n ` where ` ` zero more batch dimensions consisting lower upper triangular Cholesky decompositions symmetric Hermitian positive-definite matrices upper bool optional flag indicates whether math ` L ` lower triangular upper triangular Default ` ` False ` ` Keyword args out Tensor optional output tensor Ignored ` None ` Default ` None ` Example A = torch randn A = A A T + torch eye e- Creates symmetric positive-definite matrix L = torch linalg cholesky A Extract Cholesky decomposition B = torch randn torch cholesky_solve B L tensor - - - A inverse B tensor - - - A = torch randn dtype=torch complex A = A A mH + torch eye e- Batch Hermitian positive-definite matrices L = torch linalg cholesky A B = torch randn dtype=torch complex X = torch cholesky_solve B L torch dist X A inverse B tensor e- add_docstr torch cholesky_inverse r cholesky_inverse L upper=False out=None - Tensor Computes inverse complex Hermitian real symmetric positive-definite matrix given its Cholesky decomposition Let math ` A ` complex Hermitian real symmetric positive-definite matrix math ` L ` its Cholesky decomposition such math A = LL^ \text H where math ` L^ \text H ` conjugate transpose when math ` L ` complex transpose when math ` L ` real-valued Computes inverse matrix math ` A^ - ` Supports input float double cfloat cdouble dtypes Also supports batches matrices math ` A ` batch matrices then output has same batch dimensions Args L Tensor tensor shape ` n n ` where ` ` zero more batch dimensions consisting lower upper triangular Cholesky decompositions symmetric Hermitian positive-definite matrices upper bool optional flag indicates whether math ` L ` lower triangular upper triangular Default ` ` False ` ` Keyword args out Tensor optional output tensor Ignored ` None ` Default ` None ` Example A = torch randn A = A A T + torch eye e- Creates symmetric positive-definite matrix L = torch linalg cholesky A Extract Cholesky decomposition torch cholesky_inverse L tensor - - A inverse tensor - - A = torch randn dtype=torch complex A = A A mH + torch eye e- Batch Hermitian positive-definite matrices L = torch linalg cholesky A torch dist torch inverse A torch cholesky_inverse L tensor e- add_docstr torch clone r clone input memory_format=torch preserve_format - Tensor Returns copy attr ` input ` note This function differentiable so gradients will flow back result operation attr ` input ` To create tensor without autograd relationship attr ` input ` see meth ` ~Tensor detach ` In addition when ` ` torch preserve_format ` ` used If input tensor dense i e non-overlapping strided its memory format including strides retained Otherwise e g non-dense view like stepped slice output converted dense contiguous format Args input Keyword args memory_format format common_args add_docstr torch clamp r clamp input min=None max=None out=None - Tensor Clamps all elements attr ` input ` into range ` ` attr ` min ` attr ` max ` ` ` Letting min_value max_value attr ` min ` attr ` max ` respectively returns math y_i = \min \max x_i \text min\_value _i \text max\_value _i If attr ` min ` ` ` None ` ` there no lower bound Or attr ` max ` ` ` None ` ` there no upper bound + r note If attr ` min ` greater than attr ` max ` func ` torch clamp min max torch clamp ` sets all elements attr ` input ` value attr ` max ` Args input min Number Tensor optional lower-bound range clamped max Number Tensor optional upper-bound range clamped Keyword args out Example = torch randn tensor - - - torch clamp min=- max= tensor - - - min = torch linspace - steps= torch clamp min=min tensor - format common_args add_docstr torch clip r clip input min=None max=None out=None - Tensor Alias func ` torch clamp ` add_docstr torch column_stack r column_stack tensors out=None - Tensor Creates new tensor horizontally stacking tensors attr ` tensors ` Equivalent ` ` torch hstack tensors ` ` except each zero one dimensional tensor ` ` t ` ` attr ` tensors ` first reshaped into ` ` t numel ` ` column before being stacked horizontally Args tensors sequence Tensors sequence tensors concatenate Keyword args out Example = torch tensor b = torch tensor torch column_stack b tensor = torch arange b = torch arange reshape torch column_stack b b tensor format common_args add_docstr torch complex r complex real imag out=None - Tensor Constructs complex tensor its real part equal attr ` real ` its imaginary part equal attr ` imag ` Args real Tensor The real part complex tensor Must half float double imag Tensor The imaginary part complex tensor Must same dtype attr ` real ` Keyword args out Tensor If inputs ` ` torch float ` ` must ` ` torch complex ` ` If inputs ` ` torch float ` ` must ` ` torch complex ` ` Example real = torch tensor dtype=torch float imag = torch tensor dtype=torch float z = torch complex real imag z tensor + j + j z dtype torch complex add_docstr torch polar r polar abs angle out=None - Tensor Constructs complex tensor whose elements Cartesian coordinates corresponding polar coordinates absolute value attr ` abs ` angle attr ` angle ` math \text out = \text abs \cdot \cos \text angle + \text abs \cdot \sin \text angle \cdot j note ` torch polar ` similar ` std polar https en cppreference com w cpp numeric complex polar ` _ does compute polar decomposition complex tensor like Python s ` cmath polar ` SciPy s ` linalg polar ` do The behavior function undefined ` abs ` negative NaN ` angle ` infinite + r Args abs Tensor The absolute value complex tensor Must float double angle Tensor The angle complex tensor Must same dtype attr ` abs ` Keyword args out Tensor If inputs ` ` torch float ` ` must ` ` torch complex ` ` If inputs ` ` torch float ` ` must ` ` torch complex ` ` Example numpy np abs = torch tensor dtype=torch float angle = torch tensor np pi np pi dtype=torch float z = torch polar abs angle z tensor + j - - j dtype=torch complex add_docstr torch conj_physical r conj_physical input out=None - Tensor Computes element-wise conjugate given attr ` input ` tensor If attr ` input ` has non-complex dtype function just returns attr ` input ` note This performs conjugate operation regardless fact conjugate bit set warning In future func ` torch conj_physical ` may non-writeable view attr ` input ` non-complex dtype It s recommended programs modify tensor returned func ` torch conj_physical ` when attr ` input ` non-complex dtype compatible change math \text out _ i = conj \text input _ i + r Args input Keyword args out Example torch conj_physical torch tensor - + j - + j - j tensor - - j - - j + j format common_args add_docstr torch conj r conj input - Tensor Returns view attr ` input ` flipped conjugate bit If attr ` input ` has non-complex dtype function just returns attr ` input ` note func ` torch conj ` performs lazy conjugation actual conjugated tensor can materialized any time using func ` torch resolve_conj ` warning In future func ` torch conj ` may non-writeable view attr ` input ` non-complex dtype It s recommended programs modify tensor returned func ` torch conj_physical ` when attr ` input ` non-complex dtype compatible change Args input Example x = torch tensor - + j - + j - j x is_conj False y = torch conj x y is_conj True format common_args add_docstr torch resolve_conj r resolve_conj input - Tensor Returns new tensor materialized conjugation attr ` input ` s conjugate bit set ` True ` returns attr ` input ` The output tensor will always have its conjugate bit set ` False ` Args input Example x = torch tensor - + j - + j - j y = x conj y is_conj True z = y resolve_conj z tensor - - j - - j + j z is_conj False format common_args add_docstr torch resolve_neg r resolve_neg input - Tensor Returns new tensor materialized negation attr ` input ` s negative bit set ` True ` returns attr ` input ` The output tensor will always have its negative bit set ` False ` Args input Example x = torch tensor - + j - + j - j y = x conj z = y imag z is_neg True out = z resolve_neg out tensor - - out is_neg False format common_args add_docstr torch copysign r copysign input other out=None - Tensor Create new floating-point tensor magnitude attr ` input ` sign attr ` other ` elementwise math \text out _ i = \begin cases - &#124; \text input _ i &#124; \text \text other _ i \leq - \\ &#124; \text input _ i &#124; \text \text other _ i \geq \\ \end cases + r Supports ref ` broadcasting common shape broadcasting-semantics ` integer float inputs Args input Tensor magnitudes other Tensor Number contains value s whose signbit s applied magnitudes attr ` input ` Keyword args out Example = torch randn tensor - - - - torch copysign tensor = torch randn tensor - - - - - - - - - - b = torch randn tensor - torch copysign b tensor - - - - = torch tensor b = torch tensor - torch copysign b tensor - note copysign handles signed zeros If other argument has negative zero - corresponding output value will negative format common_args add_docstr torch cos r cos input out=None - Tensor Returns new tensor cosine elements attr ` input ` given radians math \text out _ i = \cos \text input _ i + r Args input Keyword args out Example = torch randn tensor - torch cos tensor format common_args add_docstr torch cosh r cosh input out=None - Tensor Returns new tensor hyperbolic cosine elements attr ` input ` math \text out _ i = \cosh \text input _ i + r Args input Keyword args out Example = torch randn tensor - - torch cosh tensor note When attr ` input ` CPU implementation torch cosh may use Sleef library which rounds very large results infinity negative infinity See ` here https sleef org purec xhtml ` _ details format common_args add_docstr torch cross r cross input other dim=None out=None - Tensor Returns cross product vectors dimension attr ` dim ` attr ` input ` attr ` other ` Supports input float double cfloat cdouble dtypes Also supports batches vectors which computes product along dimension attr ` dim ` In case output has same batch dimensions inputs warning If attr ` dim ` given defaults first dimension found size Note might unexpected This behavior deprecated will changed match func ` torch linalg cross ` future release seealso func ` torch linalg cross ` which has dim=- default Args input other Tensor second input tensor dim int optional dimension take cross-product Keyword args out Example = torch randn tensor - - - - - - b = torch randn b tensor - - - - - - - - - - torch cross b dim= tensor - - - - - - torch cross b tensor - - - - - - format common_args add_docstr torch logcumsumexp r logcumsumexp input dim out=None - Tensor Returns logarithm cumulative summation exponentiation elements attr ` input ` dimension attr ` dim ` For summation index math ` j ` given ` dim ` other indices math ` i ` result math \text logcumsumexp x _ ij = \log \sum\limits_ k= ^ j \exp x_ ik Args input dim int dimension do operation over Keyword args out Example = torch randn torch logcumsumexp dim= tensor - - format reduceops_common_args add_docstr torch cummax r cummax input dim out=None - Tensor LongTensor Returns namedtuple ` ` values indices ` ` where ` ` values ` ` cumulative maximum elements attr ` input ` dimension attr ` dim ` And ` ` indices ` ` index location each maximum value found dimension attr ` dim ` math y_i = max x_ x_ x_ \dots x_i Args input dim int dimension do operation over Keyword args out tuple optional result tuple two output tensors values indices Example = torch randn tensor - - - - - - torch cummax dim= torch return_types cummax values=tensor - - indices=tensor format reduceops_common_args add_docstr torch cummin r cummin input dim out=None - Tensor LongTensor Returns namedtuple ` ` values indices ` ` where ` ` values ` ` cumulative minimum elements attr ` input ` dimension attr ` dim ` And ` ` indices ` ` index location each maximum value found dimension attr ` dim ` math y_i = min x_ x_ x_ \dots x_i Args input dim int dimension do operation over Keyword args out tuple optional result tuple two output tensors values indices Example = torch randn tensor - - - - - torch cummin dim= torch return_types cummin values=tensor - - - - - - - - - - indices=tensor format reduceops_common_args add_docstr torch cumprod r cumprod input dim dtype=None out=None - Tensor Returns cumulative product elements attr ` input ` dimension attr ` dim ` For example attr ` input ` vector size N result will also vector size N elements math y_i = x_ \times x_ \times x_ \times \dots \times x_i Args input dim int dimension do operation over Keyword args dtype out Example = torch randn tensor - - - torch cumprod dim= tensor - - - - - - - = torch cumprod dim= tensor - - - - - - - format reduceops_common_args add_docstr torch cumsum r cumsum input dim dtype=None out=None - Tensor Returns cumulative sum elements attr ` input ` dimension attr ` dim ` For example attr ` input ` vector size N result will also vector size N elements math y_i = x_ + x_ + x_ + \dots + x_i Args input dim int dimension do operation over Keyword args dtype out Example = torch randint tensor torch cumsum dim= tensor format reduceops_common_args add_docstr torch count_nonzero r count_nonzero input dim=None - Tensor Counts number non-zero values tensor attr ` input ` along given attr ` dim ` If no dim specified then all non-zeros tensor counted Args input dim int tuple ints optional Dim tuple dims along which count non-zeros Example x = torch zeros x torch randn = x tensor torch count_nonzero x tensor torch count_nonzero x dim= tensor format reduceops_common_args add_docstr torch dequantize r dequantize tensor - Tensor Returns fp Tensor dequantizing quantized Tensor Args tensor Tensor A quantized Tensor function dequantize tensors - sequence Tensors noindex Given list quantized Tensors dequantize them list fp Tensors Args tensors sequence Tensors A list quantized Tensors add_docstr torch diag r diag input diagonal= out=None - Tensor - If attr ` input ` vector -D tensor then returns -D square tensor elements attr ` input ` diagonal - If attr ` input ` matrix -D tensor then returns -D tensor diagonal elements attr ` input ` The argument attr ` diagonal ` controls which diagonal consider - If attr ` diagonal ` = main diagonal - If attr ` diagonal ` above main diagonal - If attr ` diagonal ` below main diagonal Args input diagonal int optional diagonal consider Keyword args out seealso func ` torch diagonal ` always returns diagonal its input func ` torch diagflat ` always constructs tensor diagonal elements specified input Examples Get square matrix where input vector diagonal = torch randn tensor - torch diag tensor - torch diag tensor - Get k-th diagonal given matrix = torch randn tensor - - - - - torch diag tensor - - - torch diag tensor format common_args add_docstr torch diag_embed r diag_embed input offset= dim =- dim =- - Tensor Creates tensor whose diagonals certain D planes specified attr ` dim ` attr ` dim ` filled attr ` input ` To facilitate creating batched diagonal matrices D planes formed last two dimensions returned tensor chosen default The argument attr ` offset ` controls which diagonal consider - If attr ` offset ` = main diagonal - If attr ` offset ` above main diagonal - If attr ` offset ` below main diagonal The size new matrix will calculated make specified diagonal size last input dimension Note attr ` offset ` other than math ` ` order attr ` dim ` attr ` dim ` matters Exchanging them equivalent changing sign attr ` offset ` Applying meth ` torch diagonal ` output function same arguments yields matrix identical input However meth ` torch diagonal ` has different default dimensions so those need explicitly specified Args input Must least -dimensional offset int optional which diagonal consider Default main diagonal dim int optional first dimension respect which take diagonal Default - dim int optional second dimension respect which take diagonal Default - Example = torch randn torch diag_embed tensor - - - - torch diag_embed offset= dim = dim = tensor - - - - format common_args add_docstr torch diagflat r diagflat input offset= - Tensor - If attr ` input ` vector -D tensor then returns -D square tensor elements attr ` input ` diagonal - If attr ` input ` tensor more than one dimension then returns -D tensor diagonal elements equal flattened attr ` input ` The argument attr ` offset ` controls which diagonal consider - If attr ` offset ` = main diagonal - If attr ` offset ` above main diagonal - If attr ` offset ` below main diagonal Args input offset int optional diagonal consider Default main diagonal Examples = torch randn tensor - - torch diagflat tensor - - torch diagflat tensor - - = torch randn tensor - - torch diagflat tensor - - format common_args add_docstr torch diagonal r diagonal input offset= dim = dim = - Tensor Returns partial view attr ` input ` its diagonal elements respect attr ` dim ` attr ` dim ` appended dimension end shape The argument attr ` offset ` controls which diagonal consider - If attr ` offset ` = main diagonal - If attr ` offset ` above main diagonal - If attr ` offset ` below main diagonal Applying meth ` torch diag_embed ` output function same arguments yields diagonal matrix diagonal entries input However meth ` torch diag_embed ` has different default dimensions so those need explicitly specified Args input Must least -dimensional offset int optional which diagonal consider Default main diagonal dim int optional first dimension respect which take diagonal Default dim int optional second dimension respect which take diagonal Default note To take batch diagonal pass dim =- dim =- Examples = torch randn tensor - - - - - torch diagonal tensor - - - torch diagonal tensor b = torch randn b tensor - - - - - - torch diagonal b tensor x = torch randn torch diagonal x offset=- dim = dim = tensor - - - - - - - - - - format common_args add_docstr torch diagonal_scatter r diagonal_scatter input src offset= dim = dim = - Tensor Embeds values attr ` src ` tensor into attr ` input ` along diagonal elements attr ` input ` respect attr ` dim ` attr ` dim ` This function returns tensor fresh storage does view The argument attr ` offset ` controls which diagonal consider - If attr ` offset ` = main diagonal - If attr ` offset ` above main diagonal - If attr ` offset ` below main diagonal Args input Must least -dimensional src Tensor tensor embed into attr ` input ` offset int optional which diagonal consider Default main diagonal dim int optional first dimension respect which take diagonal Default dim int optional second dimension respect which take diagonal Default note attr ` src ` must proper size order embedded into attr ` input ` Specifically should have same shape ` ` torch diagonal input offset dim dim ` ` Examples = torch zeros tensor torch diagonal_scatter torch ones tensor torch diagonal_scatter torch ones tensor format common_args add_docstr torch as_strided_scatter r as_strided_scatter input src size stride storage_offset=None - Tensor Embeds values attr ` src ` tensor into attr ` input ` along elements corresponding result calling input as_strided size stride storage_offset This function returns tensor fresh storage does view Args input size tuple ints shape output tensor stride tuple ints stride output tensor storage_offset int optional offset underlying storage output tensor note attr ` src ` must proper size order embedded into attr ` input ` Specifically should have same shape ` torch as_strided input size stride storage_offset ` Example = torch arange reshape + tensor b = torch zeros b tensor torch as_strided_scatter b tensor format common_args add_docstr torch diff r diff input n= dim=- prepend=None append=None - Tensor Computes n-th forward difference along given dimension The first-order differences given ` out i = input i + - input i ` Higher-order differences calculated using func ` torch diff ` recursively Args input Tensor tensor compute differences n int optional number times recursively compute difference dim int optional dimension compute difference along Default last dimension prepend append Tensor optional values prepend append attr ` input ` along attr ` dim ` before computing difference Their dimensions must equivalent input their shapes must match input s shape except attr ` dim ` Keyword args out Example = torch tensor torch diff tensor - b = torch tensor torch diff append=b tensor - c = torch tensor torch diff c dim= tensor torch diff c dim= tensor format common_args add_docstr torch digamma r digamma input out=None - Tensor Alias func ` torch special digamma ` add_docstr torch dist r dist input other p= - Tensor Returns p-norm attr ` input ` - attr ` other ` The shapes attr ` input ` attr ` other ` must ref ` broadcastable broadcasting-semantics ` Args input other Tensor Right-hand-side input tensor p float optional norm computed Example x = torch randn x tensor - - y = torch randn y tensor - torch dist x y tensor torch dist x y tensor torch dist x y tensor torch dist x y tensor format common_args add_docstr torch div r div input other rounding_mode=None out=None - Tensor Divides each element input ` ` input ` ` corresponding element attr ` other ` math \text out _i = \frac \text input _i \text other _i note By default performs true division like Python See attr ` rounding_mode ` argument floor division Supports ref ` broadcasting common shape broadcasting-semantics ` ref ` type promotion type-promotion-doc ` integer float complex inputs Always promotes integer types default scalar type Args input Tensor dividend other Tensor Number divisor Keyword args rounding_mode str optional Type rounding applied result None - default behavior Performs no rounding both attr ` input ` attr ` other ` integer types promotes inputs default scalar type Equivalent true division Python ` ` ` ` operator NumPy s ` ` np true_divide ` ` ` ` trunc ` ` - rounds results division towards zero Equivalent C-style integer division ` ` floor ` ` - rounds results division down Equivalent floor division Python ` ` ` ` operator NumPy s ` ` np floor_divide ` ` out Examples x = torch tensor - - torch div x tensor - - = torch tensor - - - - - - - - - - b = torch tensor - - torch div b tensor - - - - - - - - torch div b rounding_mode= trunc tensor - - - - - - - - torch div b rounding_mode= floor tensor - - - - - - - - format common_args add_docstr torch divide r divide input other rounding_mode=None out=None - Tensor Alias func ` torch div ` add_docstr torch dot r dot input tensor out=None - Tensor Computes dot product two D tensors note Unlike NumPy s dot torch dot intentionally only supports computing dot product two D tensors same number elements Args input Tensor first tensor dot product must D tensor Tensor second tensor dot product must D Keyword args out Example torch dot torch tensor torch tensor tensor t t = torch tensor torch tensor torch dot t t tensor format common_args add_docstr torch vdot r vdot input other out=None - Tensor Computes dot product two D vectors along dimension In symbols function computes math \sum_ i= ^n \overline x_i y_i where math ` \overline x_i ` denotes conjugate complex vectors identity real vectors note Unlike NumPy s vdot torch vdot intentionally only supports computing dot product two D tensors same number elements seealso func ` torch linalg vecdot ` computes dot product two batches vectors along dimension Args input Tensor first tensor dot product must D Its conjugate used s complex other Tensor second tensor dot product must D Keyword args + rf note common_args out + r Example torch vdot torch tensor torch tensor tensor = torch tensor + j - j b = torch tensor + j - j torch vdot b tensor + j torch vdot b tensor - j add_docstr torch eq r eq input other out=None - Tensor Computes element-wise equality The second argument can number tensor whose shape ref ` broadcastable broadcasting-semantics ` first argument Args input Tensor tensor compare other Tensor float tensor value compare Keyword args out Returns A boolean tensor True where attr ` input ` equal attr ` other ` False elsewhere Example torch eq torch tensor torch tensor tensor True False False True format common_args add_docstr torch equal r equal input other - bool ` ` True ` ` two tensors have same size elements ` ` False ` ` otherwise note Tensors containing NaNs never equal each other Additionally function does differentiate between data types tensors during comparison For more thorough tensor checks use meth ` torch testing assert_close ` Example torch equal torch tensor torch tensor True torch equal torch tensor torch nan torch tensor torch nan False torch equal torch tensor dtype=torch int torch tensor dtype=torch float True add_docstr torch erf r erf input out=None - Tensor Alias func ` torch special erf ` add_docstr torch erfc r erfc input out=None - Tensor Alias func ` torch special erfc ` add_docstr torch erfinv r erfinv input out=None - Tensor Alias func ` torch special erfinv ` add_docstr torch exp r exp input out=None - Tensor Returns new tensor exponential elements input tensor attr ` input ` math y_ i = e^ x_ i + r Args input Keyword args out Example torch exp torch tensor math log tensor format common_args add_docstr torch exp r exp input out=None - Tensor Alias func ` torch special exp ` add_docstr torch expm r expm input out=None - Tensor Alias func ` torch special expm ` add_docstr torch eye r eye n m=None out=None dtype=None layout=torch strided device=None requires_grad=False - Tensor Returns -D tensor ones diagonal zeros elsewhere Args n int number rows m int optional number columns default being attr ` n ` Keyword arguments out dtype layout device requires_grad Returns Tensor A -D tensor ones diagonal zeros elsewhere Example torch eye tensor format factory_common_args add_docstr torch floor r floor input out=None - Tensor Returns new tensor floor elements attr ` input ` largest integer less than equal each element For integer inputs follows array-api convention returning copy input tensor math \text out _ i = \left\lfloor \text input _ i \right\rfloor + r Args input Keyword args out Example = torch randn tensor - - - torch floor tensor - - - format common_args add_docstr torch floor_divide r floor_divide input other out=None - Tensor note Before PyTorch func ` torch floor_divide ` incorrectly performed truncation division To restore previous behavior use func ` torch div ` ` ` rounding_mode= trunc ` ` Computes attr ` input ` divided attr ` other ` elementwise floors result math \text out _i = \text floor \left \frac \text input _i \text other _i \right + r Supports broadcasting common shape type promotion integer float inputs Args input Tensor Number dividend other Tensor Number divisor Keyword args out Example = torch tensor b = torch tensor torch floor_divide b tensor torch floor_divide tensor format common_args add_docstr torch fmod r fmod input other out=None - Tensor Applies C++ s ` std fmod https en cppreference com w cpp numeric math fmod ` _ entrywise The result has same sign dividend attr ` input ` its absolute value less than attr ` other ` This function may defined terms func ` torch div ` code python torch fmod b == - div b rounding_mode= trunc b Supports ref ` broadcasting common shape broadcasting-semantics ` ref ` type promotion type-promotion-doc ` integer float inputs note When divisor zero returns ` ` NaN ` ` floating point dtypes both CPU GPU raises ` ` RuntimeError ` ` integer division zero CPU Integer division zero GPU may any value note Complex inputs supported In some cases mathematically possible satisfy definition modulo operation complex numbers seealso func ` torch remainder ` which implements Python s modulus operator This one defined using division rounding down result Args input Tensor dividend other Tensor Scalar divisor Keyword args out Example torch fmod torch tensor - - - tensor - - - torch fmod torch tensor - tensor format common_args add_docstr torch frac r frac input out=None - Tensor Computes fractional portion each element attr ` input ` math \text out _ i = \text input _ i - \left\lfloor &#124; \text input _ i &#124; \right\rfloor \operatorname sgn \text input _ i Example torch frac torch tensor - tensor - add_docstr torch frexp r frexp input out=None - Tensor mantissa Tensor exponent Decomposes attr ` input ` into mantissa exponent tensors such math ` \text input = \text mantissa \times ^ \text exponent ` The range mantissa open interval - Supports float inputs Args input Tensor input tensor Keyword args out tuple optional output tensors Example x = torch arange mantissa exponent = torch frexp x mantissa tensor exponent tensor dtype=torch int torch ldexp mantissa exponent tensor add_docstr torch from_numpy r from_numpy ndarray - Tensor Creates ` Tensor ` ` numpy ndarray ` The returned tensor attr ` ndarray ` share same memory Modifications tensor will reflected attr ` ndarray ` vice versa The returned tensor resizable It currently accepts attr ` ndarray ` dtypes ` ` numpy float ` ` ` ` numpy float ` ` ` ` numpy float ` ` ` ` numpy complex ` ` ` ` numpy complex ` ` ` ` numpy int ` ` ` ` numpy int ` ` ` ` numpy int ` ` ` ` numpy int ` ` ` ` numpy uint ` ` ` ` bool ` ` warning Writing tensor created read-only NumPy array supported will result undefined behavior Example = numpy array t = torch from_numpy t tensor t = - array - add_docstr torch frombuffer r frombuffer buffer dtype count=- offset= requires_grad=False - Tensor Creates -dimensional ` Tensor ` object implements Python buffer protocol Skips first attr ` offset ` bytes buffer interprets rest raw bytes -dimensional tensor type attr ` dtype ` attr ` count ` elements Note either following must true attr ` count ` positive non-zero number total number bytes buffer more than attr ` offset ` plus attr ` count ` times size bytes attr ` dtype ` attr ` count ` negative length number bytes buffer subtracted attr ` offset ` multiple size bytes attr ` dtype ` The returned tensor buffer share same memory Modifications tensor will reflected buffer vice versa The returned tensor resizable note This function increments reference count object owns shared memory Therefore such memory will deallocated before returned tensor goes out scope warning This function s behavior undefined when passed object implementing buffer protocol whose data CPU Doing so likely cause segmentation fault warning This function does try infer attr ` dtype ` hence optional Passing different attr ` dtype ` than its source may result unexpected behavior Args buffer object Python object exposes buffer interface Keyword args dtype ` torch dtype ` desired data type returned tensor count int optional number desired elements read If negative all elements until end buffer will read Default - offset int optional number bytes skip start buffer Default requires_grad Example array = array array i t = torch frombuffer dtype=torch int t tensor t = - array - Interprets signed char bytes -bit integers Each signed char elements will interpreted signed -bit integer array = array array b - torch frombuffer dtype=torch int tensor dtype=torch int format factory_common_args add_docstr torch from_file r from_file filename shared=None size= dtype=None layout=None device=None pin_memory=False Creates CPU tensor storage backed memory-mapped file If ` ` shared ` ` True then memory shared between processes All changes written file If ` ` shared ` ` False then changes tensor do affect file ` ` size ` ` number elements Tensor If ` ` shared ` ` ` ` False ` ` then file must contain least ` ` size sizeof dtype ` ` bytes If ` ` shared ` ` ` ` True ` ` file will created needed note Only CPU tensors can mapped files note For now tensors storages backed memory-mapped file cannot created pinned memory Args filename str file name map shared bool whether share memory whether ` ` MAP_SHARED ` ` ` ` MAP_PRIVATE ` ` passed underlying ` mmap call https man org linux man-pages man mmap html ` _ size int number elements tensor Keyword args dtype layout device pin_memory Example t = torch randn dtype=torch float t numpy tofile storage pt t_mapped = torch from_file storage pt shared=False size= dtype=torch float format factory_common_args add_docstr torch flatten r flatten input start_dim= end_dim=- - Tensor Flattens attr ` input ` reshaping into one-dimensional tensor If attr ` start_dim ` attr ` end_dim ` passed only dimensions starting attr ` start_dim ` ending attr ` end_dim ` flattened The order elements attr ` input ` unchanged Unlike NumPy s flatten which always copies input s data function may original object view copy If no dimensions flattened then original object attr ` input ` returned Otherwise input can viewed flattened shape then view returned Finally only input cannot viewed flattened shape input s data copied See meth ` torch Tensor view ` details when view will returned note Flattening zero-dimensional tensor will one-dimensional view Args input start_dim int first dim flatten end_dim int last dim flatten Example t = torch tensor torch flatten t tensor torch flatten t start_dim= tensor format common_args add_docstr torch unflatten r unflatten input dim sizes - Tensor Expands dimension input tensor over multiple dimensions seealso func ` torch flatten ` inverse function It coalesces several dimensions into one Args input dim int Dimension unflattened specified index into ` ` input shape ` ` sizes Tuple int New shape unflattened dimension One its elements can ` - ` which case corresponding output dimension inferred Otherwise product ` ` sizes ` ` must equal ` ` input shape dim ` ` Returns A View input specified dimension unflattened Examples torch unflatten torch randn shape torch Size torch unflatten torch randn - shape torch Size torch unflatten torch randn - shape torch Size format common_args add_docstr torch gather r gather input dim index sparse_grad=False out=None - Tensor Gathers values along axis specified ` dim ` For -D tensor output specified out i j k = input index i j k j k dim == out i j k = input i index i j k k dim == out i j k = input i j index i j k dim == attr ` input ` attr ` index ` must have same number dimensions It also required ` ` index size d = input size d ` ` all dimensions ` ` d = dim ` ` attr ` out ` will have same shape attr ` index ` Note ` ` input ` ` ` ` index ` ` do broadcast against each other When attr ` index ` empty we always empty output same shape without further error checking Args input Tensor source tensor dim int axis along which index index LongTensor indices elements gather Keyword arguments sparse_grad bool optional If ` ` True ` ` gradient w r t attr ` input ` will sparse tensor out Tensor optional destination tensor Example t = torch tensor torch gather t torch tensor tensor add_docstr torch gcd r gcd input other out=None - Tensor Computes element-wise greatest common divisor GCD attr ` input ` attr ` other ` Both attr ` input ` attr ` other ` must have integer types note This defines math ` gcd = ` Args input other Tensor second input tensor Keyword arguments out Example = torch tensor b = torch tensor torch gcd b tensor c = torch tensor torch gcd c tensor format common_args add_docstr torch ge r ge input other out=None - Tensor Computes math ` \text input \geq \text other ` element-wise + r The second argument can number tensor whose shape ref ` broadcastable broadcasting-semantics ` first argument Args input Tensor tensor compare other Tensor float tensor value compare Keyword args out Returns A boolean tensor True where attr ` input ` greater than equal attr ` other ` False elsewhere Example torch ge torch tensor torch tensor tensor True True False True format common_args add_docstr torch greater_equal r greater_equal input other out=None - Tensor Alias func ` torch ge ` add_docstr torch gradient r gradient input spacing= dim=None edge_order= - List Tensors Estimates gradient function math ` g \mathbb R ^n \rightarrow \mathbb R ` one more dimensions using ` second-order accurate central differences method https www ams org journals mcom - - S - - - - S - - - - pdf ` _ either first second order estimates boundaries The gradient math ` g ` estimated using samples By default when attr ` spacing ` specified samples entirely described attr ` input ` mapping input coordinates output same tensor s mapping indices values For example three-dimensional attr ` input ` function described math ` g \mathbb R ^ \rightarrow \mathbb R ` math ` g \ == input ` When attr ` spacing ` specified modifies relationship between attr ` input ` input coordinates This detailed Keyword Arguments section below The gradient estimated estimating each partial derivative math ` g ` independently This estimation accurate math ` g ` math ` C^ ` has least continuous derivatives estimation can improved providing closer samples Mathematically value each interior point partial derivative estimated using ` Taylor s theorem remainder https en wikipedia org wiki Taylor s_theorem ` _ Letting math ` x ` interior point math ` x-h_l ` math ` x+h_r ` points neighboring left right respectively math ` f x+h_r ` math ` f x-h_l ` can estimated using math \begin aligned f x+h_r = f x + h_r f x + h_r ^ \frac f x + h_r ^ \frac f \xi_ \xi_ \in x x+h_r \\ f x-h_l = f x - h_l f x + h_l ^ \frac f x - h_l ^ \frac f \xi_ \xi_ \in x x-h_l \\ \end aligned Using fact math ` f \in C^ ` solving linear system we derive math f x \approx \frac h_l ^ f x+h_r - h_r ^ f x-h_l + h_r ^ - h_l ^ f x h_r h_l ^ + h_r ^ h_l note We estimate gradient functions complex domain math ` g \mathbb C ^n \rightarrow \mathbb C ` same way The value each partial derivative boundary points computed differently See edge_order below Args input ` ` Tensor ` ` tensor represents values function Keyword args spacing ` ` scalar ` ` ` ` list scalar ` ` ` ` list Tensor ` ` optional attr ` spacing ` can used modify how attr ` input ` tensor s indices relate sample coordinates If attr ` spacing ` scalar then indices multiplied scalar produce coordinates For example attr ` spacing= ` indices become coordinates If attr ` spacing ` list scalars then corresponding indices multiplied For example attr ` spacing= - ` indices become coordinates - Finally attr ` spacing ` list one-dimensional tensors then each tensor specifies coordinates corresponding dimension For example indices tensors t t t then coordinates t t t dim ` ` int ` ` ` ` list int ` ` optional dimension dimensions approximate gradient over By default partial gradient every dimension computed Note when attr ` dim ` specified elements attr ` spacing ` argument must correspond specified dims edge_order ` ` int ` ` optional ` first-order https www ams org journals mcom - - S - - - - S - - - - pdf ` _ ` second-order https www ams org journals mcom - - S - - - - S - - - - pdf ` _ estimation boundary edge values respectively Note when attr ` edge_order ` specified each dimension size attr ` input ` should least edge_order+ Examples Estimates gradient f x =x^ points - - coordinates = torch tensor - - values = torch tensor torch gradient values spacing = coordinates tensor - - Estimates gradient R^ - R function whose samples described tensor t Implicit coordinates outermost dimension innermost dimension function estimates partial derivative both dimensions t = torch tensor torch gradient t tensor tensor A scalar value spacing modifies relationship between tensor indices input coordinates multiplying indices find coordinates For example below indices innermost translate coordinates indices outermost dimension translate coordinates torch gradient t spacing = dim = None implicitly tensor tensor doubling spacing between samples halves estimated partial gradients Estimates only partial derivative dimension torch gradient t dim = spacing = None implicitly tensor When spacing list scalars relationship between tensor indices input coordinates changes based dimension For example below indices innermost dimension translate coordinates indices outermost dimension translate coordinates torch gradient t spacing = tensor tensor The following example replication previous one explicit coordinates coords = torch tensor torch tensor torch gradient t spacing = coords tensor tensor add_docstr torch geqrf r geqrf input out=None - Tensor Tensor This low-level function calling LAPACK s geqrf directly This function returns namedtuple tau defined ` LAPACK documentation geqrf ` _ Computes QR decomposition attr ` input ` Both ` Q ` ` R ` matrices stored same output tensor ` ` The elements ` R ` stored above diagonal Elementary reflectors Householder vectors implicitly defining matrix ` Q ` stored below diagonal The results function can used together func ` torch linalg householder_product ` obtain ` Q ` matrix func ` torch ormqr ` which uses implicit representation ` Q ` matrix efficient matrix-matrix multiplication See ` LAPACK documentation geqrf ` _ further details note See also func ` torch linalg qr ` which computes Q R matrices func ` torch linalg lstsq ` ` ` driver= gels ` ` option function can solve matrix equations using QR decomposition Args input Tensor input matrix Keyword args out tuple optional output tuple Tensor Tensor Ignored ` None ` Default ` None ` _LAPACK documentation geqrf http www netlib org lapack explore-html df dc group__variants_g_ecomputational_ga ea b cf f ec b html add_docstr torch inner r inner input other out=None - Tensor Computes dot product D tensors For higher dimensions sums product elements attr ` input ` attr ` other ` along their last dimension note If either attr ` input ` attr ` other ` scalar result equivalent ` torch mul input other ` If both attr ` input ` attr ` other ` non-scalars size their last dimension must match result equivalent ` torch tensordot input other dims= - - ` Args input Tensor First input tensor other Tensor Second input tensor Keyword args out Tensor optional Optional output tensor write result into The output shape ` input shape - + other shape - ` Example Dot product torch inner torch tensor torch tensor tensor Multidimensional input tensors = torch randn tensor b = torch randn b tensor - - - - - - - - - torch inner b tensor - - - - - - Scalar input torch inner torch tensor tensor add_docstr torch outer r outer input vec out=None - Tensor Outer product attr ` input ` attr ` vec ` If attr ` input ` vector size math ` n ` attr ` vec ` vector size math ` m ` then attr ` out ` must matrix size math ` n \times m ` note This function does ref ` broadcast broadcasting-semantics ` Args input Tensor -D input vector vec Tensor -D input vector Keyword args out Tensor optional optional output matrix Example v = torch arange v = torch arange torch outer v v tensor add_docstr torch ger r ger input vec out=None - Tensor Alias func ` torch outer ` warning This function deprecated will removed future PyTorch release Use func ` torch outer ` instead add_docstr torch get_default_dtype r get_default_dtype - torch dtype Get current default floating point ` torch dtype ` Example torch get_default_dtype initial default floating point torch float torch float torch set_default_dtype torch float torch get_default_dtype default now changed torch float torch float add_docstr torch get_num_threads r get_num_threads - int Returns number threads used parallelizing CPU operations add_docstr torch get_num_interop_threads r get_num_interop_threads - int Returns number threads used inter-op parallelism CPU e g JIT interpreter add_docstr torch gt r gt input other out=None - Tensor Computes math ` \text input \text other ` element-wise + r The second argument can number tensor whose shape ref ` broadcastable broadcasting-semantics ` first argument Args input Tensor tensor compare other Tensor float tensor value compare Keyword args out Returns A boolean tensor True where attr ` input ` greater than attr ` other ` False elsewhere Example torch gt torch tensor torch tensor tensor False True False False format common_args add_docstr torch greater r greater input other out=None - Tensor Alias func ` torch gt ` add_docstr torch hash_tensor r hash_tensor input mode= - Tensor Returns hash all elements attr ` input ` tensor Currently only mode= reduction via xor supported The output will always type ` ` torch uint ` ` The elements ` ` input ` ` upcasted their bit float integer equivalent bitcasted ` ` torch uint ` ` before reduction via xor Args input Keyword Args mode int The hash use Default xor_reduction Example = torch randn tensor - torch hash_tensor tensor dtype=torch uint function hash_tensor input dim keepdim=False mode= - Tensor noindex Returns hash each row attr ` input ` tensor given dimension attr ` dim ` given mode If attr ` dim ` list dimensions reduce over all them keepdim_details Args input opt_dim_all_reduce opt_keepdim Keyword Args mode int The hash use Default xor_reduction Example = torch randn tensor - - - - torch hash_tensor tensor dtype=torch uint format multi_dim_common add_docstr torch histc r histc input bins= min= max= out=None - Tensor Computes histogram tensor The elements sorted into equal width bins between attr ` min ` attr ` max ` If attr ` min ` attr ` max ` both zero minimum maximum values data used Elements lower than min higher than max ` ` NaN ` ` elements ignored Args input bins int number histogram bins min Scalar lower end range inclusive max Scalar upper end range inclusive Keyword args out Returns Tensor Histogram represented tensor Example torch histc torch tensor bins= min= max= tensor format common_args add_docstr torch histogram r histogram input bins range=None weight=None density=False out=None - Tensor Tensor Computes histogram values tensor attr ` bins ` can integer D tensor If attr ` bins ` int specifies number equal-width bins By default lower upper range bins determined minimum maximum elements input tensor The attr ` range ` argument can provided specify range bins If attr ` bins ` D tensor specifies sequence bin edges including rightmost edge It should contain least elements its elements should increasing Args input bins int D Tensor If int defines number equal-width bins If tensor defines sequence bin edges including rightmost edge Keyword args range tuple float Defines range bins weight Tensor If provided weight should have same shape input Each value input contributes its associated weight towards its bin s result density bool If False result will contain count total weight each bin If True result value probability density function over bins normalized such integral over range bins out tuple optional The result tuple two output tensors hist bin_edges Returns hist Tensor D Tensor containing values histogram bin_edges Tensor D Tensor containing edges histogram bins Example torch histogram torch tensor bins= range= weight=torch tensor tensor tensor torch histogram torch tensor bins= range= weight=torch tensor density=True tensor tensor format common_args add_docstr torch histogramdd r histogramdd input bins range=None weight=None density=False out=None - Tensor Tensor Computes multi-dimensional histogram values tensor Interprets elements input tensor whose innermost dimension has size N collection N-dimensional points Maps each points into set N-dimensional bins returns number points total weight each bin attr ` input ` must tensor least dimensions If input has shape M N each its M rows defines point N-dimensional space If input has three more dimensions all last dimension flattened Each dimension independently associated its own strictly increasing sequence bin edges Bin edges may specified explicitly passing sequence D tensors Alternatively bin edges may constructed automatically passing sequence integers specifying number equal-width bins each dimension For each N-dimensional point input - Each its coordinates binned independently among bin edges corresponding its dimension - Binning results combined identify N-dimensional bin any into which point falls - If point falls into bin bin s count total weight incremented - Points which do fall into any bin do contribute output attr ` bins ` can sequence N D tensors sequence N ints single int If attr ` bins ` sequence N D tensors explicitly specifies N sequences bin edges Each D tensor should contain strictly increasing sequence least one element A sequence K bin edges defines K- bins explicitly specifying left right edges all bins Every bin inclusive its left edge Only rightmost bin inclusive its right edge If attr ` bins ` sequence N ints specifies number equal-width bins each dimension By default leftmost rightmost bin edges each dimension determined minimum maximum elements input tensor corresponding dimension The attr ` range ` argument can provided manually specify leftmost rightmost bin edges each dimension If attr ` bins ` int specifies number equal-width bins all dimensions note See also func ` torch histogram ` which specifically computes D histograms While func ` torch histogramdd ` infers dimensionality its bins binned values shape attr ` input ` func ` torch histogram ` accepts flattens attr ` input ` any shape Args input bins Tensor int int If Tensor defines sequences bin edges If int defines number equal-width bins each dimension If int defines number equal-width bins all dimensions Keyword args range sequence float Defines leftmost rightmost bin edges each dimension weight Tensor By default each value input has weight If weight tensor passed each N-dimensional coordinate input contributes its associated weight towards its bin s result The weight tensor should have same shape attr ` input ` tensor excluding its innermost dimension N density bool If False default result will contain count total weight each bin If True each count weight divided total count total weight then divided volume its associated bin Returns hist Tensor N-dimensional Tensor containing values histogram bin_edges Tensor sequence N D Tensors containing bin edges Example torch histogramdd torch tensor bins= weight=torch tensor torch return_types histogramdd hist=tensor bin_edges= tensor tensor torch histogramdd torch tensor bins= range= density=True torch return_types histogramdd hist=tensor bin_edges= tensor tensor format common_args TODO Fix via https github com pytorch pytorch issues torch histogramdd __module__ = torch add_docstr torch hypot r hypot input other out=None - Tensor Given legs right triangle its hypotenuse math \text out _ i = \sqrt \text input _ i ^ + \text other _ i ^ The shapes ` ` input ` ` ` ` other ` ` must ref ` broadcastable broadcasting-semantics ` + r Args input Tensor first input tensor other Tensor second input tensor Keyword args out Example = torch hypot torch tensor torch tensor tensor format common_args add_docstr torch i r i input out=None - Tensor Alias func ` torch special i ` add_docstr torch igamma r igamma input other out=None - Tensor Alias func ` torch special gammainc ` add_docstr torch igammac r igammac input other out=None - Tensor Alias func ` torch special gammaincc ` add_docstr torch index_select r index_select input dim index out=None - Tensor Returns new tensor which indexes attr ` input ` tensor along dimension attr ` dim ` using entries attr ` index ` The returned tensor has same number dimensions original tensor attr ` input ` The attr ` dim ` \ th dimension has same size length attr ` index ` other dimensions have same size original tensor note The returned tensor does use same storage original tensor If attr ` out ` has different shape than expected we silently change correct shape reallocating underlying storage necessary Args input dim int dimension which we index index IntTensor LongTensor -D tensor containing indices index Keyword args out Example x = torch randn x tensor - - - - - - - - indices = torch tensor torch index_select x indices tensor - - - - - torch index_select x indices tensor - - - - format common_args add_docstr torch inverse r inverse input out=None - Tensor Alias func ` torch linalg inv ` add_docstr torch isin r isin elements test_elements assume_unique=False invert=False - Tensor Tests each element attr ` elements ` attr ` test_elements ` Returns boolean tensor same shape attr ` elements ` True elements attr ` test_elements ` False otherwise note One attr ` elements ` attr ` test_elements ` can scalar both Args elements Tensor Scalar Input elements test_elements Tensor Scalar Values against which test each input element assume_unique bool optional If True assumes both attr ` elements ` attr ` test_elements ` contain unique elements which can speed up calculation Default False invert bool optional If True inverts boolean tensor resulting True values elements attr ` test_elements ` Default False Returns A boolean tensor same shape attr ` elements ` True elements attr ` test_elements ` False otherwise Example torch isin torch tensor torch tensor tensor False True True False add_docstr torch isinf r isinf input - Tensor Tests each element attr ` input ` infinite positive negative infinity note Complex values infinite when their real imaginary part infinite Args input Returns A boolean tensor True where attr ` input ` infinite False elsewhere Example torch isinf torch tensor float inf float -inf float nan tensor False True False True False format common_args add_docstr torch isposinf r isposinf input out=None - Tensor Tests each element attr ` input ` positive infinity Args input Keyword args out Example = torch tensor -float inf float inf torch isposinf tensor False True False format common_args add_docstr torch isneginf r isneginf input out=None - Tensor Tests each element attr ` input ` negative infinity Args input Keyword args out Example = torch tensor -float inf float inf torch isneginf tensor True False False format common_args add_docstr torch isclose r isclose input other rtol= e- atol= e- equal_nan=False - Tensor Returns new tensor boolean elements representing each element attr ` input ` close corresponding element attr ` other ` Closeness defined math \lvert \text input _i - \text other _i \rvert \leq \texttt rtol \times \lvert \text other _i \rvert + \texttt atol + r where attr ` input ` attr ` other ` finite Where attr ` input ` attr ` other ` nonfinite they close only they equal NaNs being considered equal each other when attr ` equal_nan ` True Args input Tensor first tensor compare other Tensor second tensor compare rtol float optional relative tolerance Default e- atol float optional absolute tolerance Default e- equal_nan bool optional ` ` True ` ` then two ` ` NaN ` ` s will considered equal Default ` ` False ` ` Examples torch isclose torch tensor torch tensor + e- tensor True False False torch isclose torch tensor float inf torch tensor float inf rtol= tensor True True add_docstr torch isfinite r isfinite input - Tensor Returns new tensor boolean elements representing each element ` finite ` Real values finite when they NaN negative infinity infinity Complex values finite when both their real imaginary parts finite Args input Returns A boolean tensor True where attr ` input ` finite False elsewhere Example torch isfinite torch tensor float inf float -inf float nan tensor True False True False False format common_args add_docstr torch isnan r isnan input - Tensor Returns new tensor boolean elements representing each element attr ` input ` NaN Complex values considered NaN when either their real imaginary part NaN Arguments input Returns A boolean tensor True where attr ` input ` NaN False elsewhere Example torch isnan torch tensor float nan tensor False True False format common_args add_docstr torch isreal r isreal input - Tensor Returns new tensor boolean elements representing each element attr ` input ` real-valued All real-valued types considered real Complex values considered real when their imaginary part Arguments input Returns A boolean tensor True where attr ` input ` real False elsewhere Example torch isreal torch tensor + j + j tensor True False True format common_args add_docstr torch is_floating_point r is_floating_point input Tensor - bool Returns True data type attr ` input ` floating point data type i e one ` ` torch float ` ` ` ` torch float ` ` ` ` torch float ` ` ` ` torch bfloat ` ` Args input Example torch is_floating_point torch tensor True torch is_floating_point torch tensor dtype=torch int False torch is_floating_point torch tensor dtype=torch float True torch is_floating_point torch tensor dtype=torch complex False format common_args add_docstr torch is_complex r is_complex input Tensor - bool Returns True data type attr ` input ` complex data type i e one ` ` torch complex ` ` ` ` torch complex ` ` Args input Example torch is_complex torch tensor dtype=torch complex True torch is_complex torch tensor dtype=torch complex True torch is_complex torch tensor dtype=torch int False torch is_complex torch tensor dtype=torch float False format common_args add_docstr torch is_grad_enabled r is_grad_enabled - bool Returns True grad mode currently enabled format common_args add_docstr torch is_inference_mode_enabled r is_inference_mode_enabled - bool Returns True inference mode currently enabled format common_args add_docstr torch is_inference r is_inference input - bool Returns True attr ` input ` inference tensor A non-view tensor inference tensor only allocated during inference mode A view tensor inference tensor only tensor view inference tensor For details inference mode please see ` Inference Mode https pytorch org cppdocs notes inference_mode html ` _ Args input format common_args add_docstr torch is_conj r is_conj input - bool Returns True attr ` input ` conjugated tensor i e its conjugate bit set ` True ` Args input format common_args add_docstr torch is_nonzero r is_nonzero input - bool Returns True attr ` input ` single element tensor which equal zero after type conversions i e equal ` ` torch tensor ` ` ` ` torch tensor ` ` ` ` torch tensor False ` ` Throws ` ` RuntimeError ` ` ` ` torch numel = ` ` even case sparse tensors Args input Examples torch is_nonzero torch tensor False torch is_nonzero torch tensor True torch is_nonzero torch tensor False False torch is_nonzero torch tensor True torch is_nonzero torch tensor Traceback most recent call last RuntimeError Boolean value Tensor more than one value ambiguous torch is_nonzero torch tensor Traceback most recent call last RuntimeError Boolean value Tensor no values ambiguous format common_args add_docstr torch kron r kron input other out=None - Tensor Computes Kronecker product denoted math ` \otimes ` attr ` input ` attr ` other ` If attr ` input ` math ` a_ \times a_ \times \dots \times a_n ` tensor attr ` other ` math ` b_ \times b_ \times \dots \times b_n ` tensor result will math ` a_ b_ \times a_ b_ \times \dots \times a_n b_n ` tensor following entries math \text input \otimes \text other _ k_ k_ \dots k_n = \text input _ i_ i_ \dots i_n \text other _ j_ j_ \dots j_n where math ` k_t = i_t b_t + j_t ` math ` \leq t \leq n ` If one tensor has fewer dimensions than other unsqueezed until has same number dimensions Supports real-valued complex-valued inputs note This function generalizes typical definition Kronecker product two matrices two tensors described above When attr ` input ` math ` m \times n ` matrix attr ` other ` math ` p \times q ` matrix result will math ` p m \times q n ` block matrix math \mathbf A \otimes \mathbf B =\begin bmatrix a_ \mathbf B \cdots a_ n \mathbf B \\ \vdots \ddots \vdots \\ a_ m \mathbf B \cdots a_ m n \mathbf B \end bmatrix where attr ` input ` math ` \mathbf A ` attr ` other ` math ` \mathbf B ` Arguments input Tensor other Tensor Keyword args out Tensor optional The output tensor Ignored ` ` None ` ` Default ` ` None ` ` Examples mat = torch eye mat = torch ones torch kron mat mat tensor mat = torch eye mat = torch arange reshape torch kron mat mat tensor add_docstr torch kthvalue r kthvalue input k dim=None keepdim=False out=None - Tensor LongTensor Returns namedtuple ` ` values indices ` ` where ` ` values ` ` attr ` k ` th smallest element each row attr ` input ` tensor given dimension attr ` dim ` And ` ` indices ` ` index location each element found If attr ` dim ` given last dimension ` input ` chosen If attr ` keepdim ` ` ` True ` ` both attr ` values ` attr ` indices ` tensors same size attr ` input ` except dimension attr ` dim ` where they size Otherwise attr ` dim ` squeezed see func ` torch squeeze ` resulting both attr ` values ` attr ` indices ` tensors having fewer dimension than attr ` input ` tensor note When attr ` input ` CUDA tensor there multiple valid attr ` k ` th values function may nondeterministically attr ` indices ` any them Args input k int k k-th smallest element dim int optional dimension find kth value along opt_keepdim Keyword args out tuple optional output tuple Tensor LongTensor can optionally given used output buffers Example x = torch arange x tensor torch kthvalue x torch return_types kthvalue values=tensor indices=tensor x=torch arange resize_ x tensor torch kthvalue x True torch return_types kthvalue values=tensor indices=tensor format single_dim_common add_docstr torch lcm r lcm input other out=None - Tensor Computes element-wise least common multiple LCM attr ` input ` attr ` other ` Both attr ` input ` attr ` other ` must have integer types note This defines math ` lcm = ` math ` lcm = ` Args input other Tensor second input tensor Keyword arguments out Example = torch tensor b = torch tensor torch lcm b tensor c = torch tensor torch lcm c tensor format common_args add_docstr torch ldexp r ldexp input other out=None - Tensor Multiplies attr ` input ` attr ` other ` math \text out _i = \text input _i ^\text other _i + r Typically function used construct floating point numbers multiplying mantissas attr ` input ` integral powers two created exponents attr ` other ` Args input other Tensor tensor exponents typically integers Keyword args out Example torch ldexp torch tensor torch tensor tensor torch ldexp torch tensor torch tensor tensor format common_args add_docstr torch le r le input other out=None - Tensor Computes math ` \text input \leq \text other ` element-wise + r The second argument can number tensor whose shape ref ` broadcastable broadcasting-semantics ` first argument Args input Tensor tensor compare other Tensor Scalar tensor value compare Keyword args out Returns A boolean tensor True where attr ` input ` less than equal attr ` other ` False elsewhere Example torch le torch tensor torch tensor tensor True False True True format common_args add_docstr torch less_equal r less_equal input other out=None - Tensor Alias func ` torch le ` add_docstr torch lerp r lerp input end weight out=None Does linear interpolation two tensors attr ` start ` given attr ` input ` attr ` end ` based scalar tensor attr ` weight ` returns resulting attr ` out ` tensor math \text out _i = \text start _i + \text weight _i \times \text end _i - \text start _i + r The shapes attr ` start ` attr ` end ` must ref ` broadcastable broadcasting-semantics ` If attr ` weight ` tensor then shapes attr ` weight ` attr ` start ` attr ` end ` must ref ` broadcastable broadcasting-semantics ` Args input Tensor tensor starting points end Tensor tensor ending points weight float tensor weight interpolation formula Keyword args out Example start = torch arange end = torch empty fill_ start tensor end tensor torch lerp start end tensor torch lerp start end torch full_like start tensor format common_args add_docstr torch lgamma r lgamma input out=None - Tensor Computes natural logarithm absolute value gamma function attr ` input ` math \text out _ i = \ln &#124; \Gamma \text input _ i &#124; + Args input Keyword args out Example = torch arange torch lgamma tensor - format common_args add_docstr torch linspace r linspace start end steps out=None dtype=None layout=torch strided device=None requires_grad=False - Tensor Creates one-dimensional tensor size attr ` steps ` whose values evenly spaced attr ` start ` attr ` end ` inclusive That value math \text start \text start + \frac \text end - \text start \text steps - \ldots \text start + \text steps - \frac \text end - \text start \text steps - \text end + From PyTorch linspace requires steps argument Use steps= restore previous behavior Args start float Tensor starting value set points If ` Tensor ` must -dimensional end float Tensor ending value set points If ` Tensor ` must -dimensional steps int size constructed tensor Keyword arguments out dtype torch dtype optional data type perform computation Default None uses global default dtype see torch get_default_dtype when both attr ` start ` attr ` end ` real corresponding complex dtype when either complex layout device requires_grad Example torch linspace steps= tensor torch linspace - steps= tensor - - torch linspace start=- end= steps= tensor - - torch linspace start=- end= steps= tensor - format factory_common_args add_docstr torch log r log input out=None - Tensor Returns new tensor natural logarithm elements attr ` input ` math y_ i = \log_ e x_ i + r Args input Keyword args out Example = torch rand tensor torch log tensor - format common_args add_docstr torch log r log input Tensor out Optional Tensor - Tensor Returns new tensor logarithm base elements attr ` input ` math y_ i = \log_ x_ i + r Args input Keyword args out Example = torch rand tensor torch log tensor - - - - - format common_args add_docstr torch log p r log p input out=None - Tensor Returns new tensor natural logarithm + attr ` input ` math y_i = \log_ e x_i + + r note This function more accurate than func ` torch log ` small values attr ` input ` Args input Keyword args out Example = torch randn tensor - - - torch log p tensor nan - - format common_args add_docstr torch log r log input Tensor out Optional Tensor - Tensor Returns new tensor logarithm base elements attr ` input ` math y_ i = \log_ x_ i + r Args input Keyword args out Example = torch rand tensor torch log tensor - - - - - format common_args add_docstr torch logaddexp r logaddexp input other out=None - Tensor Logarithm sum exponentiations inputs Calculates pointwise math ` \log\left e^x + e^y\right ` This function useful statistics where calculated probabilities events may so small exceed range normal floating point numbers In such cases logarithm calculated probability stored This function allows adding probabilities stored such fashion This op should disambiguated func ` torch logsumexp ` which performs reduction single tensor Args input other Tensor second input tensor Keyword arguments out Example torch logaddexp torch tensor - torch tensor - - - tensor - - - torch logaddexp torch tensor - - - torch tensor - - - tensor - - - torch logaddexp torch tensor torch tensor - - - tensor e+ e+ e+ format common_args add_docstr torch logaddexp r logaddexp input other out=None - Tensor Logarithm sum exponentiations inputs base- Calculates pointwise math ` \log_ \left ^x + ^y\right ` See func ` torch logaddexp ` more details Args input other Tensor second input tensor Keyword arguments out format common_args add_docstr torch xlogy r xlogy input other out=None - Tensor Alias func ` torch special xlogy ` add_docstr torch logical_and r logical_and input other out=None - Tensor Computes element-wise logical AND given input tensors Zeros treated ` ` False ` ` nonzeros treated ` ` True ` ` Args input other Tensor tensor compute AND Keyword args out Example torch logical_and torch tensor True False True torch tensor True False False tensor True False False = torch tensor dtype=torch int b = torch tensor dtype=torch int torch logical_and b tensor False False True False torch logical_and double b double tensor False False True False torch logical_and double b tensor False False True False torch logical_and b out=torch empty dtype=torch bool tensor False False True False format common_args add_docstr torch logical_not r logical_not input out=None - Tensor Computes element-wise logical NOT given input tensor If specified output tensor will have bool dtype If input tensor bool tensor zeros treated ` ` False ` ` non-zeros treated ` ` True ` ` Args input Keyword args out Example torch logical_not torch tensor True False tensor False True torch logical_not torch tensor - dtype=torch int tensor True False False torch logical_not torch tensor - dtype=torch double tensor True False False torch logical_not torch tensor - dtype=torch double out=torch empty dtype=torch int tensor dtype=torch int format common_args add_docstr torch logical_or r logical_or input other out=None - Tensor Computes element-wise logical OR given input tensors Zeros treated ` ` False ` ` nonzeros treated ` ` True ` ` Args input other Tensor tensor compute OR Keyword args out Example torch logical_or torch tensor True False True torch tensor True False False tensor True False True = torch tensor dtype=torch int b = torch tensor dtype=torch int torch logical_or b tensor True True True False torch logical_or double b double tensor True True True False torch logical_or double b tensor True True True False torch logical_or b out=torch empty dtype=torch bool tensor True True True False format common_args add_docstr torch logical_xor r logical_xor input Tensor other Tensor out Optional Tensor - Tensor Computes element-wise logical XOR given input tensors Zeros treated ` ` False ` ` nonzeros treated ` ` True ` ` Args input other Tensor tensor compute XOR Keyword args out Example torch logical_xor torch tensor True False True torch tensor True False False tensor False False True = torch tensor dtype=torch int b = torch tensor dtype=torch int torch logical_xor b tensor True True False False torch logical_xor double b double tensor True True False False torch logical_xor double b tensor True True False False torch logical_xor b out=torch empty dtype=torch bool tensor True True False False format common_args add_docstr torch logspace logspace start end steps base= \ out=None dtype=None layout=torch strided device=None requires_grad=False - Tensor + r Creates one-dimensional tensor size attr ` steps ` whose values evenly spaced math ` \text base ^ \text start ` math ` \text base ^ \text end ` inclusive logarithmic scale base attr ` base ` That values math \text base ^ \text start \text base ^ \text start + \frac \text end - \text start \text steps - \ldots \text base ^ \text start + \text steps - \frac \text end - \text start \text steps - \text base ^ \text end + From PyTorch logspace requires steps argument Use steps= restore previous behavior Args start float Tensor starting value set points If ` Tensor ` must -dimensional end float Tensor ending value set points If ` Tensor ` must -dimensional steps int size constructed tensor base float optional base logarithm function Default ` ` ` ` Keyword arguments out dtype torch dtype optional data type perform computation Default None uses global default dtype see torch get_default_dtype when both attr ` start ` attr ` end ` real corresponding complex dtype when either complex layout device requires_grad Example torch logspace start=- end= steps= tensor e- e- e+ e+ e+ torch logspace start= end= steps= tensor torch logspace start= end= steps= tensor torch logspace start= end= steps= base= tensor format factory_common_args add_docstr torch logsumexp r logsumexp input dim keepdim=False out=None Returns log summed exponentials each row attr ` input ` tensor given dimension attr ` dim ` The computation numerically stabilized For summation index math ` j ` given ` dim ` other indices math ` i ` result math \text logsumexp x _ i = \log \sum_j \exp x_ ij keepdim_details Args input dim opt_keepdim Keyword args out Example = torch randn torch logsumexp tensor torch dist torch logsumexp torch log torch sum torch exp tensor e- format multi_dim_common add_docstr torch lt r lt input other out=None - Tensor Computes math ` \text input \text other ` element-wise + r The second argument can number tensor whose shape ref ` broadcastable broadcasting-semantics ` first argument Args input Tensor tensor compare other Tensor float tensor value compare Keyword args out Returns A boolean tensor True where attr ` input ` less than attr ` other ` False elsewhere Example torch lt torch tensor torch tensor tensor False False True False format common_args add_docstr torch lu_unpack r lu_unpack LU_data LU_pivots unpack_data=True unpack_pivots=True out=None - Tensor Tensor Tensor Unpacks LU decomposition returned func ` ~linalg lu_factor ` into ` P L U ` matrices seealso func ` ~linalg lu ` returns matrices LU decomposition Its gradient formula more efficient than doing func ` ~linalg lu_factor ` followed func ` ~linalg lu_unpack ` Args LU_data Tensor packed LU factorization data LU_pivots Tensor packed LU factorization pivots unpack_data bool flag indicating data should unpacked If ` ` False ` ` then returned ` ` L ` ` ` ` U ` ` empty tensors Default ` ` True ` ` unpack_pivots bool flag indicating pivots should unpacked into permutation matrix ` ` P ` ` If ` ` False ` ` then returned ` ` P ` ` empty tensor Default ` ` True ` ` Keyword args out tuple optional output tuple three tensors Ignored ` None ` Returns A namedtuple ` ` P L U ` ` Examples A = torch randn LU pivots = torch linalg lu_factor A P L U = torch lu_unpack LU pivots We can recover A factorization A_ = P L U torch allclose A A_ True LU factorization rectangular matrix A = torch randn LU pivots = torch linalg lu_factor A P L U = torch lu_unpack LU pivots P L U same returned linalg lu P_ L_ U_ = torch linalg lu A torch allclose P P_ torch allclose L L_ torch allclose U U_ True format common_args add_docstr torch less r less input other out=None - Tensor Alias func ` torch lt ` add_docstr torch lu_solve r lu_solve b LU_data LU_pivots out=None - Tensor Returns LU solve linear system math ` Ax = b ` using partially pivoted LU factorization A func ` ~linalg lu_factor ` This function supports ` ` float ` ` ` ` double ` ` ` ` cfloat ` ` ` ` cdouble ` ` dtypes attr ` input ` warning func ` torch lu_solve ` deprecated favor func ` torch linalg lu_solve ` func ` torch lu_solve ` will removed future PyTorch release ` ` X = torch lu_solve B LU pivots ` ` should replaced code python X = linalg lu_solve LU pivots B Arguments b Tensor RHS tensor size math ` m k ` where math ` ` zero more batch dimensions LU_data Tensor pivoted LU factorization A meth ` ~linalg lu_factor ` size math ` m m ` where math ` ` zero more batch dimensions LU_pivots IntTensor pivots LU factorization meth ` ~linalg lu_factor ` size math ` m ` where math ` ` zero more batch dimensions The batch dimensions attr ` LU_pivots ` must equal batch dimensions attr ` LU_data ` Keyword args out Example A = torch randn b = torch randn LU pivots = torch linalg lu_factor A x = torch lu_solve b LU pivots torch dist A x b tensor e- format common_args add_docstr torch masked_select r masked_select input mask out=None - Tensor Returns new -D tensor which indexes attr ` input ` tensor according boolean mask attr ` mask ` which ` BoolTensor ` The shapes attr ` mask ` tensor attr ` input ` tensor don t need match they must ref ` broadcastable broadcasting-semantics ` note The returned tensor does use same storage original tensor Args input mask BoolTensor tensor containing binary mask index Keyword args out Example x = torch randn x tensor - - - - mask = x ge mask tensor False False False False False True True True False False False True torch masked_select x mask tensor format common_args add_docstr torch matrix_power r matrix_power input n out=None - Tensor Alias func ` torch linalg matrix_power ` add_docstr torch matrix_exp r matrix_exp A - Tensor Alias func ` torch linalg matrix_exp ` add_docstr torch max r max input out=None - Tensor Returns maximum value all elements ` ` input ` ` tensor note The difference between ` ` max ` ` ` ` min ` ` ` ` amax ` ` ` ` amin ` ` - ` ` amax ` ` ` ` amin ` ` supports reducing multiple dimensions - ` ` amax ` ` ` ` amin ` ` does indices Both ` ` amax ` ` ` ` amin ` ` evenly distribute gradients between equal values when there multiple input elements same minimum maximum value For ` ` max ` ` ` ` min ` ` - If reduce over all dimensions no dim specified gradients evenly distribute between equally ` ` max ` ` ` ` min ` ` values - If reduce over one specified axis only propagate indexed element Args input Keyword args out Example = torch randn tensor - torch max tensor function max input dim keepdim=False out=None - Tensor LongTensor noindex Returns namedtuple ` ` values indices ` ` where ` ` values ` ` maximum value each row attr ` input ` tensor given dimension attr ` dim ` And ` ` indices ` ` index location each maximum value found argmax If ` ` keepdim ` ` ` ` True ` ` output tensors same size ` ` input ` ` except dimension ` ` dim ` ` where they size Otherwise ` ` dim ` ` squeezed see func ` torch squeeze ` resulting output tensors having fewer dimension than ` ` input ` ` note If there multiple maximal values reduced row then indices first maximal value returned Args input opt_dim_without_none opt_keepdim Keyword args out tuple optional result tuple two output tensors max max_indices Example = torch randn tensor - - - - - - - - - - - torch max torch return_types max values=tensor indices=tensor = torch tensor max dim= keepdim=True torch return_types max values=tensor indices=tensor max dim= keepdim=False torch return_types max values=tensor indices=tensor function max input other out=None - Tensor noindex See func ` torch maximum ` format single_dim_common add_docstr torch maximum r maximum input other out=None - Tensor Computes element-wise maximum attr ` input ` attr ` other ` note If one elements being compared NaN then element returned func ` maximum ` supported tensors complex dtypes Args input other Tensor second input tensor Keyword args out Example = torch tensor - b = torch tensor torch maximum b tensor format common_args add_docstr torch fmax r fmax input other out=None - Tensor Computes element-wise maximum attr ` input ` attr ` other ` This like func ` torch maximum ` except handles NaNs differently exactly one two elements being compared NaN then non-NaN element taken maximum Only both elements NaN NaN propagated This function wrapper around C++ s ` ` std fmax ` ` similar NumPy s ` ` fmax ` ` function Supports ref ` broadcasting common shape broadcasting-semantics ` ref ` type promotion type-promotion-doc ` integer floating-point inputs Args input other Tensor second input tensor Keyword args out Example = torch tensor float nan float nan b = torch tensor - float nan float nan torch fmax b tensor nan format common_args add_docstr torch amax r amax input dim keepdim=False out=None - Tensor Returns maximum value each slice attr ` input ` tensor given dimension s attr ` dim ` note The difference between ` ` max ` ` ` ` min ` ` ` ` amax ` ` ` ` amin ` ` - ` ` amax ` ` ` ` amin ` ` supports reducing multiple dimensions - ` ` amax ` ` ` ` amin ` ` does indices Both ` ` amax ` ` ` ` amin ` ` evenly distribute gradients between equal values when there multiple input elements same minimum maximum value For ` ` max ` ` ` ` min ` ` - If reduce over all dimensions no dim specified gradients evenly distribute between equally ` ` max ` ` ` ` min ` ` values - If reduce over one specified axis only propagate indexed element keepdim_details Args input opt_dim_all_reduce opt_keepdim Keyword args out Example = torch randn tensor - - - - - - - torch amax tensor format multi_dim_common add_docstr torch argmax r argmax input - LongTensor Returns indices maximum value all elements attr ` input ` tensor This second value returned meth ` torch max ` See its documentation exact semantics method note If there multiple maximal values then indices first maximal value returned Args input Example = torch randn tensor - - - - - - - - - - torch argmax tensor function argmax input dim keepdim=False - LongTensor noindex Returns indices maximum values tensor across dimension This second value returned meth ` torch max ` See its documentation exact semantics method Args input opt_dim If ` ` None ` ` argmax flattened input returned opt_keepdim Example = torch randn tensor - - - - - - - - - - torch argmax dim= tensor format single_dim_common add_docstr torch argwhere r argwhere input - Tensor Returns tensor containing indices all non-zero elements attr ` input ` Each row result contains indices non-zero element attr ` input ` The result sorted lexicographically last index changing fastest C-style If attr ` input ` has math ` n ` dimensions then resulting indices tensor attr ` out ` size math ` z \times n ` where math ` z ` total number non-zero elements attr ` input ` tensor note This function similar NumPy s ` argwhere ` When attr ` input ` CUDA function causes host-device synchronization Args input Example t = torch tensor torch argwhere t tensor t = torch tensor torch argwhere t tensor add_docstr torch mean r mean input dtype=None - Tensor note If ` input ` tensor empty ` ` torch mean ` ` returns ` ` nan ` ` This behavior consistent NumPy follows definition mean over empty set undefined Returns mean value all elements attr ` input ` tensor Input must floating point complex Args input Tensor input tensor either floating point complex dtype Keyword args dtype Example = torch randn tensor - torch mean tensor function mean input dim keepdim=False dtype=None out=None - Tensor noindex Returns mean value each row attr ` input ` tensor given dimension attr ` dim ` If attr ` dim ` list dimensions reduce over all them keepdim_details Args input opt_dim_all_reduce opt_keepdim Keyword args dtype out seealso func ` torch nanmean ` computes mean value ` non-NaN ` elements Example = torch randn tensor - - - - - - - - - torch mean tensor - - - torch mean True tensor - - - format multi_dim_common add_docstr torch nanmean r nanmean input dim=None keepdim=False dtype=None out=None - Tensor Computes mean all ` non-NaN ` elements along specified dimensions Input must floating point complex This function identical func ` torch mean ` when there no ` NaN ` values attr ` input ` tensor In presence ` NaN ` func ` torch mean ` will propagate ` NaN ` output whereas func ` torch nanmean ` will ignore ` NaN ` values ` torch nanmean ` equivalent ` torch mean ~a isnan ` keepdim_details Args input Tensor input tensor either floating point complex dtype opt_dim_all_reduce opt_keepdim Keyword args dtype out seealso func ` torch mean ` computes mean value propagating ` NaN ` Example x = torch tensor torch nan x mean tensor nan x nanmean tensor x mean dim= tensor nan x nanmean dim= tensor If all elements reduced dimensions NaN then result NaN torch tensor torch nan nanmean tensor nan format multi_dim_common add_docstr torch median r median input - Tensor Returns median values attr ` input ` note The median unique attr ` input ` tensors even number elements In case lower two medians returned To compute mean both medians use func ` torch quantile ` ` ` q= ` ` instead warning This function produces deterministic sub gradients unlike ` ` median dim= ` ` Args input Example = torch randn tensor - torch median tensor function median input dim=- keepdim=False out=None - Tensor LongTensor noindex Returns namedtuple ` ` values indices ` ` where ` ` values ` ` contains median each row attr ` input ` dimension attr ` dim ` ` ` indices ` ` contains index median values found dimension attr ` dim ` By default attr ` dim ` last dimension attr ` input ` tensor If attr ` keepdim ` ` ` True ` ` output tensors same size attr ` input ` except dimension attr ` dim ` where they size Otherwise attr ` dim ` squeezed see func ` torch squeeze ` resulting outputs tensor having fewer dimension than attr ` input ` note The median unique attr ` input ` tensors even number elements dimension attr ` dim ` In case lower two medians returned To compute mean both medians attr ` input ` use func ` torch quantile ` ` ` q= ` ` instead warning ` ` indices ` ` does necessarily contain first occurrence each median value found unless unique The exact implementation details device-specific Do expect same result when run CPU GPU general For same reason do expect gradients deterministic Args input opt_dim_all_reduce opt_keepdim Keyword args out Tensor Tensor optional The first tensor will populated median values second tensor which must have dtype long their indices dimension attr ` dim ` attr ` input ` Example = torch randn tensor - - - - - - - torch median torch return_types median values=tensor - indices=tensor format single_dim_common add_docstr torch nanmedian r nanmedian input - Tensor Returns median values attr ` input ` ignoring ` ` NaN ` ` values This function identical func ` torch median ` when there no ` ` NaN ` ` values attr ` input ` When attr ` input ` has one more ` ` NaN ` ` values func ` torch median ` will always ` ` NaN ` ` while function will median non- ` ` NaN ` ` elements attr ` input ` If all elements attr ` input ` ` ` NaN ` ` will also ` ` NaN ` ` Args input Example = torch tensor float nan median tensor nan nanmedian tensor function nanmedian input dim=- keepdim=False out=None - Tensor LongTensor noindex Returns namedtuple ` ` values indices ` ` where ` ` values ` ` contains median each row attr ` input ` dimension attr ` dim ` ignoring ` ` NaN ` ` values ` ` indices ` ` contains index median values found dimension attr ` dim ` This function identical func ` torch median ` when there no ` ` NaN ` ` values reduced row When reduced row has one more ` ` NaN ` ` values func ` torch median ` will always reduce ` ` NaN ` ` while function will reduce median non- ` ` NaN ` ` elements If all elements reduced row ` ` NaN ` ` then will reduced ` ` NaN ` ` too Args input opt_dim_all_reduce opt_keepdim Keyword args out Tensor Tensor optional The first tensor will populated median values second tensor which must have dtype long their indices dimension attr ` dim ` attr ` input ` Example = torch tensor float nan float nan tensor nan nan median torch return_types median values=tensor nan nan indices=tensor nanmedian torch return_types nanmedian values=tensor indices=tensor format single_dim_common add_docstr torch quantile r quantile input q dim=None keepdim=False interpolation= linear out=None - Tensor Computes q-th quantiles each row attr ` input ` tensor along dimension attr ` dim ` To compute quantile we map q range indices n find location quantile sorted input If quantile lies between two data points ` ` b ` ` indices ` ` i ` ` ` ` j ` ` sorted order result computed according given attr ` interpolation ` method follows - ` ` linear ` ` ` ` + b - fraction ` ` where ` ` fraction ` ` fractional part computed quantile index - ` ` lower ` ` ` ` ` ` - ` ` higher ` ` ` ` b ` ` - ` ` nearest ` ` ` ` ` ` ` ` b ` ` whichever s index closer computed quantile index follows func ` torch round ` - ` ` midpoint ` ` ` ` + b ` ` If attr ` q ` D tensor first dimension output represents quantiles has size equal size attr ` q ` remaining dimensions what remains reduction note By default attr ` dim ` ` ` None ` ` resulting attr ` input ` tensor being flattened before computation Args input q float Tensor scalar D tensor values range opt_dim opt_keepdim Keyword arguments interpolation str optional interpolation method use when desired quantile lies between two data points Can ` ` linear ` ` ` ` lower ` ` ` ` higher ` ` ` ` midpoint ` ` ` ` nearest ` ` Default ` ` linear ` ` out Example = torch randn tensor - q = torch tensor torch quantile q dim= keepdim=True tensor - torch quantile q dim= keepdim=True shape torch Size = torch arange tensor torch quantile interpolation= linear tensor torch quantile interpolation= lower tensor torch quantile interpolation= higher tensor torch quantile interpolation= midpoint tensor torch quantile interpolation= nearest tensor torch quantile interpolation= nearest tensor format single_dim_common add_docstr torch nanquantile r nanquantile input q dim=None keepdim=False interpolation= linear out=None - Tensor This variant func ` torch quantile ` ignores ` ` NaN ` ` values computing quantiles attr ` q ` ` ` NaN ` ` values attr ` input ` did exist If all values reduced row ` ` NaN ` ` then quantiles reduction will ` ` NaN ` ` See documentation func ` torch quantile ` Args input q float Tensor scalar D tensor quantile values range opt_dim_all_reduce opt_keepdim Keyword arguments interpolation str interpolation method use when desired quantile lies between two data points Can ` ` linear ` ` ` ` lower ` ` ` ` higher ` ` ` ` midpoint ` ` ` ` nearest ` ` Default ` ` linear ` ` out Example t = torch tensor float nan t quantile tensor nan t nanquantile tensor t = torch tensor float nan float nan t tensor nan nan t nanquantile dim= tensor t nanquantile dim= tensor nan format single_dim_common add_docstr torch min r min input out=None - Tensor Returns minimum value all elements attr ` input ` tensor note The difference between ` ` max ` ` ` ` min ` ` ` ` amax ` ` ` ` amin ` ` - ` ` amax ` ` ` ` amin ` ` supports reducing multiple dimensions - ` ` amax ` ` ` ` amin ` ` does indices Both ` ` amax ` ` ` ` amin ` ` evenly distribute gradients between equal values when there multiple input elements same minimum maximum value For ` ` max ` ` ` ` min ` ` - If reduce over all dimensions no dim specified gradients evenly distribute between equally ` ` max ` ` ` ` min ` ` values - If reduce over one specified axis only propagate indexed element Args input Keyword args out Example = torch randn tensor torch min tensor function min input dim keepdim=False out=None - Tensor LongTensor noindex Returns namedtuple ` ` values indices ` ` where ` ` values ` ` minimum value each row attr ` input ` tensor given dimension attr ` dim ` And ` ` indices ` ` index location each minimum value found argmin If attr ` keepdim ` ` ` True ` ` output tensors same size attr ` input ` except dimension attr ` dim ` where they size Otherwise attr ` dim ` squeezed see func ` torch squeeze ` resulting output tensors having fewer dimension than attr ` input ` note If there multiple minimal values reduced row then indices first minimal value returned Args input opt_dim_without_none opt_keepdim Keyword args out tuple optional tuple two output tensors min min_indices Example = torch randn tensor - - - - - - - torch min torch return_types min values=tensor - - - indices=tensor function min input other out=None - Tensor noindex See func ` torch minimum ` format single_dim_common add_docstr torch minimum r minimum input other out=None - Tensor Computes element-wise minimum attr ` input ` attr ` other ` note If one elements being compared NaN then element returned func ` minimum ` supported tensors complex dtypes Args input other Tensor second input tensor Keyword args out Example = torch tensor - b = torch tensor torch minimum b tensor - format common_args add_docstr torch fmin r fmin input other out=None - Tensor Computes element-wise minimum attr ` input ` attr ` other ` This like func ` torch minimum ` except handles NaNs differently exactly one two elements being compared NaN then non-NaN element taken minimum Only both elements NaN NaN propagated This function wrapper around C++ s ` ` std fmin ` ` similar NumPy s ` ` fmin ` ` function Supports ref ` broadcasting common shape broadcasting-semantics ` ref ` type promotion type-promotion-doc ` integer floating-point inputs Args input other Tensor second input tensor Keyword args out Example = torch tensor float nan float nan b = torch tensor - float nan float nan torch fmin b tensor - nan format common_args add_docstr torch amin r amin input dim keepdim=False out=None - Tensor Returns minimum value each slice attr ` input ` tensor given dimension s attr ` dim ` note The difference between ` ` max ` ` ` ` min ` ` ` ` amax ` ` ` ` amin ` ` - ` ` amax ` ` ` ` amin ` ` supports reducing multiple dimensions - ` ` amax ` ` ` ` amin ` ` does indices Both ` ` amax ` ` ` ` amin ` ` evenly distribute gradients between equal values when there multiple input elements same minimum maximum value For ` ` max ` ` ` ` min ` ` - If reduce over all dimensions no dim specified gradients evenly distribute between equally ` ` max ` ` ` ` min ` ` values - If reduce over one specified axis only propagate indexed element keepdim_details Args input opt_dim_all_reduce opt_keepdim Keyword args out Example = torch randn tensor - - - - - - - torch amin tensor - - - - format multi_dim_common add_docstr torch aminmax r aminmax input dim=None keepdim=False out=None - Tensor min Tensor max Computes minimum maximum values attr ` input ` tensor Args input Tensor The input tensor Keyword Args dim Optional int The dimension along which compute values If ` None ` computes values over entire attr ` input ` tensor Default ` None ` keepdim bool If ` True ` reduced dimensions will kept output tensor dimensions size broadcasting otherwise they will removed calling func ` torch squeeze ` Default ` False ` out Optional Tuple Tensor Tensor Optional tensors which write result Must have same shape dtype expected output Default ` None ` Returns A named tuple ` min max ` containing minimum maximum values Raises RuntimeError If any dimensions compute values over has size note NaN values propagated output least one value NaN seealso func ` torch amin ` computes just minimum value func ` torch amax ` computes just maximum value Example torch aminmax torch tensor - torch return_types aminmax min=tensor - max=tensor aminmax propagates NaNs torch aminmax torch tensor - torch nan torch return_types aminmax min=tensor nan max=tensor nan t = torch arange view t tensor t aminmax dim= keepdim=True torch return_types aminmax min=tensor max=tensor add_docstr torch argmin r argmin input dim=None keepdim=False - LongTensor Returns indices minimum value s flattened tensor along dimension This second value returned meth ` torch min ` See its documentation exact semantics method note If there multiple minimal values then indices first minimal value returned Args input opt_dim If ` ` None ` ` argmin flattened input returned opt_keepdim Example = torch randn tensor - - - - - - - - torch argmin tensor torch argmin dim= tensor torch argmin dim= keepdim=True tensor format single_dim_common add_docstr torch mm r mm input mat out_dtype=None out=None - Tensor Performs matrix multiplication matrices attr ` input ` attr ` mat ` If attr ` input ` math ` n \times m ` tensor attr ` mat ` math ` m \times p ` tensor attr ` out ` will math ` n \times p ` tensor note This function does ref ` broadcast broadcasting-semantics ` For broadcasting matrix products see func ` torch matmul ` Supports strided sparse -D tensors inputs autograd respect strided inputs This operation has support arguments ref ` sparse layouts sparse-docs ` If attr ` out ` provided its layout will used Otherwise result layout will deduced attr ` input ` sparse_beta_warning tf _note rocm_fp _note Args input Tensor first matrix matrix multiplied mat Tensor second matrix matrix multiplied out_dtype dtype optional dtype output tensor Supported only CUDA torch float given torch float torch bfloat input dtypes Keyword args out Example mat = torch randn mat = torch randn torch mm mat mat tensor - - - format common_args tf _notes rocm_fp _notes sparse_support_notes add_docstr torch hspmm r hspmm mat mat out=None - Tensor Performs matrix multiplication ref ` sparse COO matrix sparse-coo-docs ` attr ` mat ` strided matrix attr ` mat ` The result + -dimensional ref ` hybrid COO matrix sparse-hybrid-coo-docs ` Args mat Tensor first sparse matrix matrix multiplied mat Tensor second strided matrix matrix multiplied Keyword args out format common_args add_docstr torch matmul r matmul input other out=None - Tensor Matrix product two tensors The behavior depends dimensionality tensors follows - If both tensors -dimensional dot product scalar returned - If both arguments -dimensional matrix-matrix product returned - If first argument -dimensional second argument -dimensional prepended its dimension purpose matrix multiply After matrix multiply prepended dimension removed - If first argument -dimensional second argument -dimensional matrix-vector product returned - If both arguments least -dimensional least one argument N-dimensional where N then batched matrix multiply returned If first argument -dimensional prepended its dimension purpose batched matrix multiply removed after If second argument -dimensional appended its dimension purpose batched matrix multiply removed after The first N- dimensions each argument batch dimensions ref ` broadcast broadcasting-semantics ` thus must broadcastable The last matrix dimensions handled matrix-matrix product For example attr ` input ` math ` j \times \times n \times m ` tensor attr ` other ` math ` k \times m \times p ` tensor batch dimensions math ` j \times ` math ` k ` matrix dimensions math ` n \times m ` math ` m \times p ` attr ` out ` will math ` j \times k \times n \times p ` tensor This operation has support arguments ref ` sparse layouts sparse-docs ` In particular matrix-matrix both arguments -dimensional supports sparse arguments same restrictions func ` torch mm ` sparse_beta_warning tf _note rocm_fp _note note The -dimensional dot product version function does support attr ` out ` parameter Arguments input Tensor first tensor multiplied other Tensor second tensor multiplied Keyword args out Example vector x vector tensor = torch randn tensor = torch randn torch matmul tensor tensor size torch Size matrix x vector tensor = torch randn tensor = torch randn torch matmul tensor tensor size torch Size batched matrix x broadcasted vector tensor = torch randn tensor = torch randn torch matmul tensor tensor size torch Size batched matrix x batched matrix tensor = torch randn tensor = torch randn torch matmul tensor tensor size torch Size batched matrix x broadcasted matrix tensor = torch randn tensor = torch randn torch matmul tensor tensor size torch Size format common_args tf _notes rocm_fp _notes sparse_support_notes add_docstr torch mode r mode input dim=- keepdim=False out=None - Tensor LongTensor Returns namedtuple ` ` values indices ` ` where ` ` values ` ` mode value each row attr ` input ` tensor given dimension attr ` dim ` i e value which appears most often row ` ` indices ` ` index location each mode value found By default attr ` dim ` last dimension attr ` input ` tensor If attr ` keepdim ` ` ` True ` ` output tensors same size attr ` input ` except dimension attr ` dim ` where they size Otherwise attr ` dim ` squeezed see func ` torch squeeze ` resulting output tensors having fewer dimension than attr ` input ` Args input opt_dim opt_keepdim Keyword args out tuple optional result tuple two output tensors values indices Example b = torch tensor torch mode b torch return_types mode values=tensor indices=tensor format single_dim_common add_docstr torch mul r mul input other out=None - Tensor Multiplies attr ` input ` attr ` other ` math \text out _i = \text input _i \times \text other _i + r Supports ref ` broadcasting common shape broadcasting-semantics ` ref ` type promotion type-promotion-doc ` integer float complex inputs Args input other Tensor Number tensor number multiply input Keyword args out Examples = torch randn tensor - torch mul tensor - b = torch randn b tensor - c = torch randn c tensor - torch mul b c tensor - - - - - - format common_args add_docstr torch multiply r multiply input other out=None Alias func ` torch mul ` add_docstr torch multinomial r multinomial input num_samples replacement=False generator=None out=None - LongTensor Returns tensor where each row contains attr ` num_samples ` indices sampled multinomial stricter definition would multivariate refer ` torch distributions multinomial Multinomial ` more details probability distribution located corresponding row tensor attr ` input ` note The rows attr ` input ` do need sum one which case we use values weights must non-negative finite have non-zero sum Indices ordered left right according when each sampled first samples placed first column If attr ` input ` vector attr ` out ` vector size attr ` num_samples ` If attr ` input ` matrix ` m ` rows attr ` out ` matrix shape math ` m \times \text num\_samples ` If replacement ` ` True ` ` samples drawn replacement If they drawn without replacement which means when sample index drawn row cannot drawn again row note When drawn without replacement attr ` num_samples ` must lower than number non-zero elements attr ` input ` min number non-zero elements each row attr ` input ` matrix Args input Tensor input tensor containing probabilities num_samples int number samples draw replacement bool optional whether draw replacement Keyword args generator out Example weights = torch tensor dtype=torch float create tensor weights torch multinomial weights tensor torch multinomial weights ERROR RuntimeError cannot sample n_sample prob_dist size - samples without replacement torch multinomial weights replacement=True tensor format common_args add_docstr torch mv r mv input vec out=None - Tensor Performs matrix-vector product matrix attr ` input ` vector attr ` vec ` If attr ` input ` math ` n \times m ` tensor attr ` vec ` -D tensor size math ` m ` attr ` out ` will -D size math ` n ` note This function does ref ` broadcast broadcasting-semantics ` Args input Tensor matrix multiplied vec Tensor vector multiplied Keyword args out Example mat = torch randn vec = torch randn torch mv mat vec tensor - format common_args add_docstr torch mvlgamma r mvlgamma input p out=None - Tensor Alias func ` torch special multigammaln ` add_docstr torch movedim r movedim input source destination - Tensor Moves dimension s attr ` input ` position s attr ` source ` position s attr ` destination ` Other dimensions attr ` input ` explicitly moved remain their original order appear positions specified attr ` destination ` Args input source int tuple ints Original positions dims move These must unique destination int tuple ints Destination positions each original dims These must also unique Examples t = torch randn t tensor - - - - torch movedim t shape torch Size torch movedim t tensor - - - - torch movedim t shape torch Size torch movedim t tensor - - - - format common_args add_docstr torch moveaxis r moveaxis input source destination - Tensor Alias func ` torch movedim ` This function equivalent NumPy s moveaxis function Examples t = torch randn t tensor - - - - torch moveaxis t shape torch Size torch moveaxis t tensor - - - - torch moveaxis t shape torch Size torch moveaxis t tensor - - - - format common_args add_docstr torch swapdims r swapdims input dim dim - Tensor Alias func ` torch transpose ` This function equivalent NumPy s swapaxes function Examples x = torch tensor x tensor torch swapdims x tensor torch swapdims x tensor format common_args add_docstr torch swapaxes r swapaxes input axis axis - Tensor Alias func ` torch transpose ` This function equivalent NumPy s swapaxes function Examples x = torch tensor x tensor torch swapaxes x tensor torch swapaxes x tensor format common_args add_docstr torch narrow r narrow input dim start length - Tensor Returns new tensor narrowed version attr ` input ` tensor The dimension attr ` dim ` input attr ` start ` ` ` start + length ` ` The returned tensor attr ` input ` tensor share same underlying storage Args input Tensor tensor narrow dim int dimension along which narrow start int Tensor index element start narrowed dimension Can negative which means indexing end ` dim ` If ` Tensor ` must -dim integral ` Tensor ` bools allowed length int length narrowed dimension must weakly positive Example x = torch tensor torch narrow x tensor torch narrow x tensor torch narrow x - torch tensor - tensor add_docstr torch narrow_copy r narrow_copy input dim start length out=None - Tensor Same meth ` Tensor narrow ` except returns copy rather than shared storage This primarily sparse tensors which do have shared-storage narrow method Args input Tensor tensor narrow dim int dimension along which narrow start int index element start narrowed dimension Can negative which means indexing end ` dim ` length int length narrowed dimension must weakly positive Keyword args out Example x = torch tensor torch narrow_copy x tensor torch narrow_copy x tensor s = torch arange reshape to_sparse torch narrow_copy s tensor indices=tensor values=tensor size= nnz= layout=torch sparse_coo seealso func ` torch narrow ` non copy variant format common_args add_docstr torch nan_to_num r nan_to_num input nan= posinf=None neginf=None out=None - Tensor Replaces literal ` NaN ` positive infinity negative infinity values attr ` input ` values specified attr ` nan ` attr ` posinf ` attr ` neginf ` respectively By default literal ` NaN ` \ s replaced zero positive infinity replaced greatest finite value representable attr ` input ` s dtype negative infinity replaced least finite value representable attr ` input ` s dtype Args input nan Number optional value replace literal ` NaN ` \s Default zero posinf Number optional Number value replace positive infinity values If None positive infinity values replaced greatest finite value representable attr ` input ` s dtype Default None neginf Number optional Number value replace negative infinity values If None negative infinity values replaced lowest finite value representable attr ` input ` s dtype Default None Keyword args out Example x = torch tensor float nan float inf -float inf torch nan_to_num x tensor e+ e+ - e+ e+ torch nan_to_num x nan= tensor e+ e+ - e+ e+ torch nan_to_num x nan= posinf= tensor e+ e+ - e+ e+ format common_args add_docstr torch ne r ne input other out=None - Tensor Computes math ` \text input \neq \text other ` element-wise + r The second argument can number tensor whose shape ref ` broadcastable broadcasting-semantics ` first argument Args input Tensor tensor compare other Tensor float tensor value compare Keyword args out Returns A boolean tensor True where attr ` input ` equal attr ` other ` False elsewhere Example torch ne torch tensor torch tensor tensor False True True False format common_args add_docstr torch not_equal r not_equal input other out=None - Tensor Alias func ` torch ne ` add_docstr torch neg r neg input out=None - Tensor Returns new tensor negative elements attr ` input ` math \text out = - \times \text input + r Args input Keyword args out Example = torch randn tensor - - - torch neg tensor - - format common_args add_docstr torch negative r negative input out=None - Tensor Alias func ` torch neg ` add_docstr torch nextafter r nextafter input other out=None - Tensor Return next floating-point value after attr ` input ` towards attr ` other ` elementwise The shapes ` ` input ` ` ` ` other ` ` must ref ` broadcastable broadcasting-semantics ` Args input Tensor first input tensor other Tensor second input tensor Keyword args out Example eps = torch finfo torch float eps torch nextafter torch tensor torch tensor == torch tensor eps + - eps tensor True True format common_args add_docstr torch nonzero r nonzero input out=None as_tuple=False - LongTensor tuple LongTensors note func ` torch nonzero as_tuple=False torch nonzero ` default returns -D tensor where each row index nonzero value func ` torch nonzero as_tuple=True torch nonzero ` returns tuple -D index tensors allowing advanced indexing so ` ` x x nonzero as_tuple=True ` ` gives all nonzero values tensor ` ` x ` ` Of returned tuple each index tensor contains nonzero indices certain dimension See below more details two behaviors When attr ` input ` CUDA func ` torch nonzero torch nonzero ` causes host-device synchronization When attr ` as_tuple ` ` ` False ` ` default Returns tensor containing indices all non-zero elements attr ` input ` Each row result contains indices non-zero element attr ` input ` The result sorted lexicographically last index changing fastest C-style If attr ` input ` has math ` n ` dimensions then resulting indices tensor attr ` out ` size math ` z \times n ` where math ` z ` total number non-zero elements attr ` input ` tensor When attr ` as_tuple ` ` ` True ` ` Returns tuple -D tensors one each dimension attr ` input ` each containing indices dimension all non-zero elements attr ` input ` If attr ` input ` has math ` n ` dimensions then resulting tuple contains math ` n ` tensors size math ` z ` where math ` z ` total number non-zero elements attr ` input ` tensor As special case when attr ` input ` has zero dimensions nonzero scalar value treated one-dimensional tensor one element Args input Keyword args out LongTensor optional output tensor containing indices Returns LongTensor tuple LongTensor If attr ` as_tuple ` ` ` False ` ` output tensor containing indices If attr ` as_tuple ` ` ` True ` ` one -D tensor each dimension containing indices each nonzero element along dimension Example torch nonzero torch tensor tensor torch nonzero torch tensor - tensor torch nonzero torch tensor as_tuple=True tensor torch nonzero torch tensor - as_tuple=True tensor tensor torch nonzero torch tensor as_tuple=True tensor format common_args add_docstr torch normal r normal mean std generator=None out=None - Tensor Returns tensor random numbers drawn separate normal distributions whose mean standard deviation given The attr ` mean ` tensor mean each output element s normal distribution The attr ` std ` tensor standard deviation each output element s normal distribution The shapes attr ` mean ` attr ` std ` don t need match total number elements each tensor need same note When shapes do match shape attr ` mean ` used shape returned output tensor note When attr ` std ` CUDA tensor function synchronizes its device CPU Args mean Tensor tensor per-element means std Tensor tensor per-element standard deviations Keyword args generator out Example torch normal mean=torch arange std=torch arange - tensor function normal mean= std out=None - Tensor noindex Similar function above means shared among all drawn elements Args mean float optional mean all distributions std Tensor tensor per-element standard deviations Keyword args out Example torch normal mean= std=torch arange tensor - - - - function normal mean std= out=None - Tensor noindex Similar function above standard deviations shared among all drawn elements Args mean Tensor tensor per-element means std float optional standard deviation all distributions Keyword args out Tensor optional output tensor Example torch normal mean=torch arange tensor function normal mean std size out=None - Tensor noindex Similar function above means standard deviations shared among all drawn elements The resulting tensor has size given attr ` size ` Args mean float mean all distributions std float standard deviation all distributions size int sequence integers defining shape output tensor Keyword args out Example torch normal size= tensor - - format common_args add_docstr torch numel r numel input Tensor - int Returns total number elements attr ` input ` tensor Args input Example = torch randn torch numel = torch zeros torch numel format common_args add_docstr torch ones r ones size out=None dtype=None layout=torch strided device=None requires_grad=False - Tensor Returns tensor filled scalar value ` ` shape defined variable argument attr ` size ` Args size int sequence integers defining shape output tensor Can variable number arguments collection like list tuple Keyword arguments out dtype layout device requires_grad Example torch ones tensor torch ones tensor format factory_common_args add_docstr torch ones_like r ones_like input dtype=None layout=None device=None requires_grad=False memory_format=torch preserve_format - Tensor Returns tensor filled scalar value ` ` same size attr ` input ` ` ` torch ones_like input ` ` equivalent ` ` torch ones input size dtype=input dtype layout=input layout device=input device ` ` warning As function does support attr ` out ` keyword As alternative old ` ` torch ones_like input out=output ` ` equivalent ` ` torch ones input size out=output ` ` Args input Keyword arguments dtype layout device requires_grad memory_format Example input = torch empty torch ones_like input tensor format factory_like_common_args add_docstr torch orgqr r orgqr input tau - Tensor Alias func ` torch linalg householder_product ` add_docstr torch ormqr r ormqr input tau other left=True transpose=False out=None - Tensor Computes matrix-matrix multiplication product Householder matrices general matrix Multiplies math ` m \times n ` matrix ` C ` given attr ` other ` matrix ` Q ` where ` Q ` represented using Householder reflectors ` input tau ` See ` Representation Orthogonal Unitary Matrices ` _ further details If attr ` left ` ` True ` then ` op Q ` times ` C ` computed otherwise result ` C ` times ` op Q ` When attr ` left ` ` True ` implicit matrix ` Q ` has size math ` m \times m ` It has size math ` n \times n ` otherwise If attr ` transpose ` ` True ` then ` op ` conjugate transpose operation otherwise s no-op Supports inputs float double cfloat cdouble dtypes Also supports batched inputs input batched output batched same dimensions seealso func ` torch geqrf ` can used form Householder representation ` input tau ` matrix ` Q ` QR decomposition note This function supports backward only fast when ` ` input tau ` ` do require gradients ` ` tau size - ` ` very small ` ` Args input Tensor tensor shape ` mn k ` where ` ` zero more batch dimensions ` mn ` equals ` m ` ` n ` depending attr ` left ` tau Tensor tensor shape ` min mn k ` where ` ` zero more batch dimensions other Tensor tensor shape ` m n ` where ` ` zero more batch dimensions left bool controls order multiplication transpose bool controls whether matrix ` Q ` conjugate transposed Keyword args out Tensor optional output Tensor Ignored ` None ` Default ` None ` _Representation Orthogonal Unitary Matrices https www netlib org lapack lug node html add_docstr torch permute r permute input dims - Tensor Returns view original tensor attr ` input ` its dimensions permuted Args input dims tuple int The desired ordering dimensions Example x = torch randn x size torch Size torch permute x size torch Size format common_args add_docstr torch poisson r poisson input generator=None - Tensor Returns tensor same size attr ` input ` each element sampled Poisson distribution rate parameter given corresponding element attr ` input ` i e math \text out _i \sim \text Poisson \text input _i attr ` input ` must non-negative Args input Tensor input tensor containing rates Poisson distribution Keyword args generator Example rates = torch rand rate parameter between torch poisson rates tensor format common_args add_docstr torch polygamma r polygamma n input out=None - Tensor Alias func ` torch special polygamma ` add_docstr torch positive r positive input - Tensor Returns attr ` input ` Throws runtime error attr ` input ` bool tensor + r Args input Example t = torch randn t tensor - - - torch positive t tensor - - - format common_args add_docstr torch pow r pow input exponent out=None - Tensor Takes power each element attr ` input ` attr ` exponent ` returns tensor result attr ` exponent ` can either single ` ` float ` ` number ` Tensor ` same number elements attr ` input ` When attr ` exponent ` scalar value operation applied math \text out _i = x_i ^ \text exponent When attr ` exponent ` tensor operation applied math \text out _i = x_i ^ \text exponent _i + r When attr ` exponent ` tensor shapes attr ` input ` attr ` exponent ` must ref ` broadcastable broadcasting-semantics ` Args input exponent float tensor exponent value Keyword args out Example = torch randn tensor - torch pow tensor exp = torch arange = torch arange tensor exp tensor torch pow exp tensor function pow exponent out=None - Tensor noindex attr ` ` scalar ` ` float ` ` value attr ` exponent ` tensor The returned tensor attr ` out ` same shape attr ` exponent ` The operation applied math \text out _i = \text ^ \text exponent _i Args float scalar base value power operation exponent Tensor exponent tensor Keyword args out Example exp = torch arange base = torch pow base exp tensor format common_args add_docstr torch float_power r float_power input exponent out=None - Tensor Raises attr ` input ` power attr ` exponent ` elementwise double precision If neither input complex returns ` ` torch float ` ` tensor one more inputs complex returns ` ` torch complex ` ` tensor note This function always computes double precision unlike func ` torch pow ` which implements more typical ref ` type promotion type-promotion-doc ` This useful when computation needs performed wider more precise dtype results computation may contain fractional values representable input dtypes like when integer base raised negative integer exponent Args input Tensor Number base value s exponent Tensor Number exponent value s Keyword args out Example = torch randint tensor torch float_power tensor dtype=torch float = torch arange tensor exp = torch tensor - - exp tensor - - torch float_power exp tensor e+ e- e+ e- dtype=torch float format common_args add_docstr torch prod r prod input Tensor dtype Optional _dtype - Tensor Returns product all elements attr ` input ` tensor Args input Keyword args dtype Example = torch randn tensor - - torch prod tensor function prod input dim keepdim=False dtype=None - Tensor noindex Returns product each row attr ` input ` tensor given dimension attr ` dim ` keepdim_details Args input opt_dim_all_reduce opt_keepdim Keyword args dtype Example = torch randn tensor - - - - torch prod tensor - - - - format single_dim_common add_docstr torch promote_types r promote_types type type - dtype Returns ` torch dtype ` smallest size scalar kind smaller nor lower kind than either ` type ` ` type ` See type promotion ref ` documentation type-promotion-doc ` more information type promotion logic Args type ` torch dtype ` type ` torch dtype ` Example torch promote_types torch int torch float torch float torch promote_types torch uint torch long torch long add_docstr torch qr r qr input Tensor some bool = True out Union Tensor Tuple Tensor List Tensor None - Tensor Tensor Computes QR decomposition matrix batch matrices attr ` input ` returns namedtuple Q R tensors such math ` \text input = Q R ` math ` Q ` being orthogonal matrix batch orthogonal matrices math ` R ` being upper triangular matrix batch upper triangular matrices If attr ` some ` ` ` True ` ` then function returns thin reduced QR factorization Otherwise attr ` some ` ` ` False ` ` function returns complete QR factorization warning func ` torch qr ` deprecated favor func ` torch linalg qr ` will removed future PyTorch release The boolean parameter attr ` some ` has been replaced string parameter attr ` mode ` ` ` Q R = torch qr A ` ` should replaced code python Q R = torch linalg qr A ` ` Q R = torch qr A some=False ` ` should replaced code python Q R = torch linalg qr A mode= complete warning If you plan backpropagate through QR note current backward implementation only well-defined when first math ` \min input size - input size - ` columns attr ` input ` linearly independent This behavior will probably change once QR supports pivoting note This function uses LAPACK CPU inputs MAGMA CUDA inputs may produce different valid decompositions different device types different platforms Args input Tensor input tensor size math ` m n ` where ` ` zero more batch dimensions consisting matrices dimension math ` m \times n ` some bool optional Set ` ` True ` ` reduced QR decomposition ` ` False ` ` complete QR decomposition If ` k = min m n ` then ` ` some=True ` ` returns ` Q R ` dimensions m k k n default ` ` some=False ` ` returns ` Q R ` dimensions m m m n Keyword args out tuple optional tuple ` Q ` ` R ` tensors The dimensions ` Q ` ` R ` detailed description attr ` some ` above Example = torch tensor - - - - q r = torch qr q tensor - - - - - r tensor - - - - torch mm q r round tensor - - - - torch mm q t q round tensor - - = torch randn q r = torch qr some=False torch allclose torch matmul q r True torch allclose torch matmul q mT q torch eye True add_docstr torch rad deg r rad deg input Tensor out Optional Tensor - Tensor Returns new tensor each elements attr ` input ` converted angles radians degrees Args input Keyword arguments out Example = torch tensor - - - torch rad deg tensor - - - format common_args add_docstr torch deg rad r deg rad input out=None - Tensor Returns new tensor each elements attr ` input ` converted angles degrees radians Args input Keyword arguments out Example = torch tensor - - - torch deg rad tensor - - - format common_args add_docstr torch heaviside r heaviside input values out=None - Tensor Computes Heaviside step function each element attr ` input ` The Heaviside step function defined math \text heaviside input values = \begin cases \text input \\ values \text input == \\ \text input \end cases + r Args input values Tensor The values use where attr ` input ` zero Keyword arguments out Example input = torch tensor - values = torch tensor torch heaviside input values tensor values = torch tensor - torch heaviside input values tensor - format common_args add_docstr torch rand rand size generator=None out=None dtype=None layout=torch strided device=None \ requires_grad=False pin_memory=False - Tensor + r Returns tensor filled random numbers uniform distribution interval math ` ` The shape tensor defined variable argument attr ` size ` Args size int sequence integers defining shape output tensor Can variable number arguments collection like list tuple Keyword args generator out dtype layout device requires_grad pin_memory Example torch rand tensor torch rand tensor format factory_common_args add_docstr torch rand_like r rand_like input dtype=None layout=None device=None requires_grad=False memory_format=torch preserve_format - Tensor Returns tensor same size attr ` input ` filled random numbers uniform distribution interval math ` ` ` ` torch rand_like input ` ` equivalent ` ` torch rand input size dtype=input dtype layout=input layout device=input device ` ` Args input Keyword args dtype layout device requires_grad memory_format format factory_like_common_args add_docstr torch randint randint low= high size \\ generator=None out=None \ dtype=None layout=torch strided device=None requires_grad=False - Tensor Returns tensor filled random integers generated uniformly between attr ` low ` inclusive attr ` high ` exclusive The shape tensor defined variable argument attr ` size ` note With global dtype default ` ` torch float ` ` function returns tensor dtype ` ` torch int ` ` Args low int optional Lowest integer drawn distribution Default high int One above highest integer drawn distribution size tuple tuple defining shape output tensor Keyword args generator out dtype torch dtype optional desired data type returned tensor Default ` ` None ` ` function returns tensor dtype ` ` torch int ` ` layout device requires_grad Example torch randint tensor torch randint tensor torch randint tensor format factory_common_args add_docstr torch randint_like randint_like input low= high \\ dtype=None layout=torch strided device=None requires_grad=False \ memory_format=torch preserve_format - Tensor Returns tensor same shape Tensor attr ` input ` filled random integers generated uniformly between attr ` low ` inclusive attr ` high ` exclusive note With global dtype default ` ` torch float ` ` function returns tensor dtype ` ` torch int ` ` Args input low int optional Lowest integer drawn distribution Default high int One above highest integer drawn distribution Keyword args dtype layout device requires_grad memory_format format factory_like_common_args add_docstr torch randn randn size generator=None out=None dtype=None layout=torch strided device=None requires_grad=False \ pin_memory=False - Tensor + r Returns tensor filled random numbers normal distribution mean ` ` variance ` ` also called standard normal distribution math \text out _ i \sim \mathcal N For complex dtypes tensor i i d sampled ` complex normal distribution ` _ zero mean unit variance math \text out _ i \sim \mathcal CN This equivalent separately sampling real math ` \operatorname Re ` imaginary math ` \operatorname Im ` part math ` \text out _i ` math \operatorname Re \text out _ i \sim \mathcal N \frac \quad \operatorname Im \text out _ i \sim \mathcal N \frac The shape tensor defined variable argument attr ` size ` Args size int sequence integers defining shape output tensor Can variable number arguments collection like list tuple Keyword args generator out dtype layout device requires_grad pin_memory Example torch randn tensor - - torch randn tensor - - - _complex normal distribution https en wikipedia org wiki Complex_normal_distribution format factory_common_args add_docstr torch randn_like r randn_like input dtype=None layout=None device=None requires_grad=False memory_format=torch preserve_format - Tensor Returns tensor same size attr ` input ` filled random numbers normal distribution mean variance Please refer func ` torch randn ` sampling process complex dtypes ` ` torch randn_like input ` ` equivalent ` ` torch randn input size dtype=input dtype layout=input layout device=input device ` ` Args input Keyword args dtype layout device requires_grad memory_format format factory_like_common_args add_docstr torch randperm randperm n generator=None out=None dtype=torch int layout=torch strided \ device=None requires_grad=False pin_memory=False - Tensor + r Returns random permutation integers ` ` ` ` ` ` n - ` ` Args n int upper bound exclusive Keyword args generator out dtype ` torch dtype ` optional desired data type returned tensor Default ` ` torch int ` ` layout device requires_grad pin_memory Example torch randperm tensor format factory_common_args add_docstr torch tensor r tensor data dtype=None device=None requires_grad=False pin_memory=False - Tensor Constructs tensor no autograd history also known leaf tensor see doc ` notes autograd ` copying attr ` data ` warning When working tensors prefer using func ` torch Tensor clone ` func ` torch Tensor detach ` func ` torch Tensor requires_grad_ ` readability Letting ` t ` tensor ` ` torch tensor t ` ` equivalent ` ` t detach clone ` ` ` ` torch tensor t requires_grad=True ` ` equivalent ` ` t detach clone requires_grad_ True ` ` seealso func ` torch as_tensor ` preserves autograd history avoids copies where possible func ` torch from_numpy ` creates tensor shares storage NumPy array Args data Keyword args dtype device ` torch device ` optional device constructed tensor If None data tensor then device data used If None data tensor then result tensor constructed current device requires_grad pin_memory Example torch tensor tensor torch tensor Type inference data tensor torch tensor dtype=torch float device=torch device cuda creates double tensor CUDA device tensor dtype=torch float device= cuda torch tensor Create zero-dimensional scalar tensor tensor torch tensor Create empty tensor size tensor format factory_data_common_args add_docstr torch range r range start= end step= out=None dtype=None layout=torch strided device=None requires_grad=False - Tensor Returns -D tensor size math ` \left\lfloor \frac \text end - \text start \text step \right\rfloor + ` values attr ` start ` attr ` end ` step attr ` step ` Step gap between two values tensor math \text out _ i+ = \text out _i + \text step + r warning This function deprecated will removed future release because its behavior inconsistent Python s range builtin Instead use func ` torch arange ` which produces values start end Args start float optional starting value set points Default ` ` ` ` end float ending value set points step float optional gap between each pair adjacent points Default ` ` ` ` Keyword args out dtype If ` dtype ` given infer data type other input arguments If any ` start ` ` end ` ` step ` floating-point ` dtype ` inferred default dtype see meth ` ~torch get_default_dtype ` Otherwise ` dtype ` inferred ` torch int ` layout device requires_grad Example torch range tensor torch range tensor format factory_common_args add_docstr torch arange r arange start= end step= out=None dtype=None layout=torch strided device=None requires_grad=False - Tensor Returns -D tensor size math ` \left\lceil \frac \text end - \text start \text step \right\rceil ` values interval ` ` start end ` ` taken common difference attr ` step ` beginning ` start ` Note When using floating-point dtypes especially reduced precision types like ` ` bfloat ` ` results may affected floating-point rounding behavior Some values sequence might exactly representable certain floating-point formats which can lead repeated values unexpected rounding For precise sequences recommended use integer dtypes instead floating-point dtypes Note non-integer attr ` step ` subject floating point rounding errors when comparing against attr ` end ` avoid inconsistency we advise subtracting small epsilon attr ` end ` such cases math \text out _ i+ = \text out _ i + \text step + r Args start Number optional starting value set points Default ` ` ` ` end Number ending value set points step Number optional gap between each pair adjacent points Default ` ` ` ` Keyword args out dtype If ` dtype ` given infer data type other input arguments If any ` start ` ` end ` ` stop ` floating-point ` dtype ` inferred default dtype see meth ` ~torch get_default_dtype ` Otherwise ` dtype ` inferred ` torch int ` layout device requires_grad Example torch arange tensor torch arange tensor torch arange tensor format factory_common_args add_docstr torch ravel r ravel input - Tensor Return contiguous flattened tensor A copy made only needed Args input Example t = torch tensor torch ravel t tensor format common_args add_docstr torch remainder r remainder input other out=None - Tensor Computes ` Python s modulus operation https docs python org reference expressions html#binary-arithmetic-operations ` _ entrywise The result has same sign divisor attr ` other ` its absolute value less than attr ` other ` It may also defined terms func ` torch div ` code python torch remainder b == - div b rounding_mode= floor b Supports ref ` broadcasting common shape broadcasting-semantics ` ref ` type promotion type-promotion-doc ` integer float inputs note Complex inputs supported In some cases mathematically possible satisfy definition modulo operation complex numbers See func ` torch fmod ` how division zero handled seealso func ` torch fmod ` which implements C++ s ` std fmod https en cppreference com w cpp numeric math fmod ` _ This one defined terms division rounding towards zero Args input Tensor Scalar dividend other Tensor Scalar divisor Keyword args out Example torch remainder torch tensor - - - tensor torch remainder torch tensor - tensor - - - - format common_args add_docstr torch renorm r renorm input p dim maxnorm out=None - Tensor Returns tensor where each sub-tensor attr ` input ` along dimension attr ` dim ` normalized such ` p ` -norm sub-tensor lower than value attr ` maxnorm ` note If norm row lower than ` maxnorm ` row unchanged Args input p float power norm computation dim int dimension slice over get sub-tensors maxnorm float maximum norm keep each sub-tensor under Keyword args out Example x = torch ones x fill_ tensor x fill_ tensor x tensor torch renorm x tensor format common_args add_docstr torch reshape r reshape input shape - Tensor Returns tensor same data number elements attr ` input ` specified shape When possible returned tensor will view attr ` input ` Otherwise will copy Contiguous inputs inputs compatible strides can reshaped without copying you should depend copying vs viewing behavior See meth ` torch Tensor view ` when possible view A single dimension may - which case s inferred remaining dimensions number elements attr ` input ` Args input Tensor tensor reshaped shape tuple int new shape Example = torch arange torch reshape tensor b = torch tensor torch reshape b - tensor add_docstr torch result_type r result_type tensor tensor - dtype Returns ` torch dtype ` would result performing arithmetic operation provided input tensors See type promotion ref ` documentation type-promotion-doc ` more information type promotion logic Args tensor Tensor Number input tensor number tensor Tensor Number input tensor number Example torch result_type torch tensor dtype=torch int torch float torch result_type torch tensor dtype=torch uint torch tensor torch uint add_docstr torch row_stack r row_stack tensors out=None - Tensor Alias func ` torch vstack ` add_docstr torch round r round input decimals= out=None - Tensor Rounds elements attr ` input ` nearest integer For integer inputs follows array-api convention returning copy input tensor The type output same input s dtype note This function implements round half even break ties when number equidistant two integers e g ` round ` When attr \ ` decimals\ ` argument specified algorithm used similar NumPy s ` around ` This algorithm fast inexact can easily overflow low precision dtypes Eg ` round tensor dtype=torch float decimals= ` ` inf ` seealso func ` torch ceil ` which rounds up func ` torch floor ` which rounds down func ` torch trunc ` which rounds towards zero Args input decimals int Number decimal places round default If decimals negative specifies number positions left decimal point Keyword args out Example torch round torch tensor - - tensor - - Values equidistant two integers rounded towards nearest even value zero treated even torch round torch tensor - tensor - A positive decimals argument rounds decimal place torch round torch tensor decimals= tensor A negative decimals argument rounds left decimal torch round torch tensor decimals=- tensor format common_args add_docstr torch rsqrt r rsqrt input out=None - Tensor Returns new tensor reciprocal square-root each elements attr ` input ` math \text out _ i = \frac \sqrt \text input _ i + r Args input Keyword args out Example = torch randn tensor - - torch rsqrt tensor nan nan format common_args add_docstr torch scatter r scatter input dim index src - Tensor Out-of-place version meth ` torch Tensor scatter_ ` add_docstr torch scatter_add r scatter_add input dim index src - Tensor Out-of-place version meth ` torch Tensor scatter_add_ ` add_docstr torch scatter_reduce r scatter_reduce input dim index src reduce include_self=True - Tensor Out-of-place version meth ` torch Tensor scatter_reduce_ ` add_docstr torch segment_reduce r segment_reduce data Tensor reduce str lengths Tensor &#124; None = None indices Tensor &#124; None = None offsets Tensor &#124; None = None axis _int = unsafe _bool = False initial Number &#124; _complex &#124; None = None - Tensor noqa B Perform segment reduction operation input tensor along specified axis Args data Tensor The input tensor which segment reduction operation will performed reduce str The type reduction operation Supported values ` ` sum ` ` ` ` mean ` ` ` ` max ` ` ` ` min ` ` ` ` prod ` ` Keyword args lengths Tensor optional Length each segment Default ` ` None ` ` offsets Tensor optional Offset each segment Default ` ` None ` ` axis int optional The axis perform reduction Default ` ` ` ` unsafe bool optional Skip validation If ` True ` Default ` ` False ` ` initial Number optional The initial value reduction operation Default ` ` None ` ` Example data = torch tensor dtype=torch float device= cuda lengths = torch tensor device= cuda torch segment_reduce data max lengths=lengths tensor device= cuda add_docstr torch select r select input dim index - Tensor Slices attr ` input ` tensor along selected dimension given index This function returns view original tensor given dimension removed note If attr ` input ` sparse tensor returning view tensor possible RuntimeError exception raised In case consider using func ` torch select_copy ` function Args input dim int dimension slice index int index select note meth ` select ` equivalent slicing For example ` ` tensor select index ` ` equivalent ` ` tensor index ` ` ` ` tensor select index ` ` equivalent ` ` tensor index ` ` format common_args add_docstr torch select_scatter r select_scatter input src dim index - Tensor Embeds values attr ` src ` tensor into attr ` input ` given index This function returns tensor fresh storage does create view Args input src Tensor The tensor embed into attr ` input ` dim int dimension insert slice into index int index select note attr ` src ` must proper size order embedded into attr ` input ` Specifically should have same shape ` ` torch select input dim index ` ` Example = torch zeros b = torch ones select_scatter b tensor format common_args add_docstr torch slice_scatter r slice_scatter input src dim= start=None end=None step= - Tensor Embeds values attr ` src ` tensor into attr ` input ` given dimension This function returns tensor fresh storage does create view Args input src Tensor The tensor embed into attr ` input ` dim int dimension insert slice into start Optional int start index where insert slice end Optional int end index where insert slice step int how many elements skip Example = torch zeros b = torch ones slice_scatter b start= tensor b = torch ones slice_scatter b dim= start= end= step= tensor format common_args add_docstr torch set_flush_denormal r set_flush_denormal mode - bool Disables denormal floating numbers CPU Returns ` ` True ` ` your system supports flushing denormal numbers successfully configures flush denormal mode meth ` ~torch set_flush_denormal ` supported x architectures supporting SSE AArch architecture Args mode bool Controls whether enable flush denormal mode Example torch set_flush_denormal True True torch tensor e- dtype=torch float tensor dtype=torch float torch set_flush_denormal False True torch tensor e- dtype=torch float tensor e- dtype=torch float add_docstr torch set_num_threads r set_num_threads int Sets number threads used intraop parallelism CPU warning To ensure correct number threads used set_num_threads must called before running eager JIT autograd code add_docstr torch set_num_interop_threads r set_num_interop_threads int Sets number threads used interop parallelism e g JIT interpreter CPU warning Can only called once before any inter-op parallel work started e g JIT execution add_docstr torch sigmoid r sigmoid input out=None - Tensor Alias func ` torch special expit ` add_docstr torch logit r logit input eps=None out=None - Tensor Alias func ` torch special logit ` add_docstr torch sign r sign input out=None - Tensor Returns new tensor signs elements attr ` input ` math \text out _ i = \operatorname sgn \text input _ i + r Args input Keyword args out Example = torch tensor - tensor - torch sign tensor - format common_args add_docstr torch signbit r signbit input out=None - Tensor Tests each element attr ` input ` has its sign bit set Args input Keyword args out Example = torch tensor - torch signbit tensor False True False False = torch tensor - torch signbit tensor True False note signbit handles signed zeros so negative zero - returns True format common_args add_docstr torch sgn r sgn input out=None - Tensor This function extension torch sign complex tensors It computes new tensor whose elements have same angles corresponding elements attr ` input ` absolute values i e magnitudes one complex tensors equivalent torch sign non-complex tensors math \text out _ i = \begin cases &#124; \text input _i &#124; == \\ \frac \text input _i &#124; \text input _i &#124; \text otherwise \end cases + r Args input Keyword args out Example t = torch tensor + j - j + j t sgn tensor + j - j + j + j format common_args add_docstr torch sin r sin input out=None - Tensor Returns new tensor sine elements attr ` input ` tensor where each value input tensor radians math \text out _ i = \sin \text input _ i + r Args input Keyword args out Example = torch randn tensor - - - torch sin tensor - - - format common_args add_docstr torch sinc r sinc input out=None - Tensor Alias func ` torch special sinc ` add_docstr torch sinh r sinh input out=None - Tensor Returns new tensor hyperbolic sine elements attr ` input ` math \text out _ i = \sinh \text input _ i + r Args input Keyword args out Example = torch randn tensor - - torch sinh tensor - - note When attr ` input ` CPU implementation torch sinh may use Sleef library which rounds very large results infinity negative infinity See ` here https sleef org purec xhtml ` _ details format common_args add_docstr torch sort r sort input dim=- descending=False stable=False out=None - Tensor LongTensor Sorts elements attr ` input ` tensor along given dimension ascending order value If attr ` dim ` given last dimension ` input ` chosen If attr ` descending ` ` ` True ` ` then elements sorted descending order value If attr ` stable ` ` ` True ` ` then sorting routine becomes stable preserving order equivalent elements A namedtuple values indices returned where ` values ` sorted values ` indices ` indices elements original ` input ` tensor Args input dim int optional dimension sort along descending bool optional controls sorting order ascending descending Keyword args stable bool optional makes sorting routine stable which guarantees order equivalent elements preserved out tuple optional output tuple ` Tensor ` ` LongTensor ` can optionally given used output buffers Example x = torch randn sorted indices = torch sort x sorted tensor - - - indices tensor sorted indices = torch sort x sorted tensor - - - indices tensor x = torch tensor x sort torch return_types sort values=tensor indices=tensor x sort stable=True torch return_types sort values=tensor indices=tensor format common_args add_docstr torch argsort r argsort input dim=- descending=False stable=False - Tensor Returns indices sort tensor along given dimension ascending order value This second value returned meth ` torch sort ` See its documentation exact semantics method If attr ` stable ` ` ` True ` ` then sorting routine becomes stable preserving order equivalent elements If ` ` False ` ` relative order values which compare equal guaranteed ` ` True ` ` slower Args input dim int optional dimension sort along descending bool optional controls sorting order ascending descending Keyword args stable bool optional controls relative order equivalent elements Example = torch randn tensor - - - - - - - torch argsort dim= tensor format common_args add_docstr torch msort r msort input Tensor out Optional Tensor - Tensor Sorts elements attr ` input ` tensor along its first dimension ascending order value note ` torch msort t ` equivalent ` torch sort t dim= ` See also func ` torch sort ` Args input Keyword args out Example t = torch randn t tensor - - - - - - - - torch msort t tensor - - - - - - - - format common_args add_docstr torch sparse_compressed_tensor r sparse_compressed_tensor compressed_indices plain_indices values size=None r dtype=None layout=None device=None pin_memory=False requires_grad=False check_invariants=None - Tensor Constructs ref ` sparse tensor Compressed Sparse format - CSR CSC BSR BSC - sparse-compressed-docs ` specified values given attr ` compressed_indices ` attr ` plain_indices ` Sparse matrix multiplication operations Compressed Sparse format typically faster than sparse tensors COO format Make you have look ref ` note data type indices sparse-compressed-docs ` sparse_factory_device_note Args compressed_indices array_like B+ -dimensional array size ` ` batchsize compressed_dim_size + ` ` The last element each batch number non-zero elements blocks This tensor encodes index ` ` values ` ` ` ` plain_indices ` ` depending where given compressed dimension row column starts Each successive number tensor subtracted number before denotes number elements blocks given compressed dimension plain_indices array_like Plain dimension column row coordinates each element block values B+ -dimensional tensor same length values values array_list Initial values tensor Can list tuple NumPy ` ` ndarray ` ` scalar other types represents +K -dimensional CSR CSC layouts + +K -dimensional tensor BSR BSC layouts where ` ` K ` ` number dense dimensions size list tuple ` torch Size ` optional Size sparse tensor ` ` batchsize nrows blocksize ncols blocksize densesize ` ` where ` ` blocksize == blocksize == ` ` CSR CSC formats If provided size will inferred minimum size big enough hold all non-zero elements blocks Keyword args dtype ` torch dtype ` optional desired data type returned tensor Default None infers data type attr ` values ` layout ` torch layout ` required desired layout returned tensor attr ` torch sparse_csr ` attr ` torch sparse_csc ` attr ` torch sparse_bsr ` attr ` torch sparse_bsc ` device ` torch device ` optional desired device returned tensor Default None uses current device default tensor type see func ` torch set_default_device ` attr ` device ` will CPU CPU tensor types current CUDA device CUDA tensor types pin_memory requires_grad check_invariants Example compressed_indices = plain_indices = values = torch sparse_compressed_tensor torch tensor compressed_indices dtype=torch int torch tensor plain_indices dtype=torch int torch tensor values dtype=torch double layout=torch sparse_csr tensor crow_indices=tensor col_indices=tensor values=tensor size= nnz= dtype=torch float layout=torch sparse_csr format factory_common_args add_docstr torch sparse_csr_tensor r sparse_csr_tensor crow_indices col_indices values size=None r dtype=None device=None pin_memory=False requires_grad=False check_invariants=None - Tensor Constructs ref ` sparse tensor CSR Compressed Sparse Row sparse-csr-docs ` specified values given attr ` crow_indices ` attr ` col_indices ` Sparse matrix multiplication operations CSR format typically faster than sparse tensors COO format Make you have look ref ` note data type indices sparse-csr-docs ` sparse_factory_device_note Args crow_indices array_like B+ -dimensional array size ` ` batchsize nrows + ` ` The last element each batch number non-zeros This tensor encodes index values col_indices depending where given row starts Each successive number tensor subtracted number before denotes number elements given row col_indices array_like Column coordinates each element values B+ -dimensional tensor same length values values array_list Initial values tensor Can list tuple NumPy ` ` ndarray ` ` scalar other types represents +K -dimensional tensor where ` ` K ` ` number dense dimensions size list tuple ` torch Size ` optional Size sparse tensor ` ` batchsize nrows ncols densesize ` ` If provided size will inferred minimum size big enough hold all non-zero elements Keyword args dtype ` torch dtype ` optional desired data type returned tensor Default None infers data type attr ` values ` device ` torch device ` optional desired device returned tensor Default None uses current device default tensor type see func ` torch set_default_device ` attr ` device ` will CPU CPU tensor types current CUDA device CUDA tensor types pin_memory requires_grad check_invariants Example crow_indices = col_indices = values = torch sparse_csr_tensor torch tensor crow_indices dtype=torch int torch tensor col_indices dtype=torch int torch tensor values dtype=torch double tensor crow_indices=tensor col_indices=tensor values=tensor size= nnz= dtype=torch float layout=torch sparse_csr format factory_common_args add_docstr torch sparse_csc_tensor r sparse_csc_tensor ccol_indices row_indices values size=None r dtype=None device=None pin_memory=False requires_grad=False check_invariants=None - Tensor Constructs ref ` sparse tensor CSC Compressed Sparse Column sparse-csc-docs ` specified values given attr ` ccol_indices ` attr ` row_indices ` Sparse matrix multiplication operations CSC format typically faster than sparse tensors COO format Make you have look ref ` note data type indices sparse-csc-docs ` sparse_factory_device_note Args ccol_indices array_like B+ -dimensional array size ` ` batchsize ncols + ` ` The last element each batch number non-zeros This tensor encodes index values row_indices depending where given column starts Each successive number tensor subtracted number before denotes number elements given column row_indices array_like Row coordinates each element values B+ -dimensional tensor same length values values array_list Initial values tensor Can list tuple NumPy ` ` ndarray ` ` scalar other types represents +K -dimensional tensor where ` ` K ` ` number dense dimensions size list tuple ` torch Size ` optional Size sparse tensor ` ` batchsize nrows ncols densesize ` ` If provided size will inferred minimum size big enough hold all non-zero elements Keyword args dtype ` torch dtype ` optional desired data type returned tensor Default None infers data type attr ` values ` device ` torch device ` optional desired device returned tensor Default None uses current device default tensor type see func ` torch set_default_device ` attr ` device ` will CPU CPU tensor types current CUDA device CUDA tensor types pin_memory requires_grad check_invariants Example ccol_indices = row_indices = values = torch sparse_csc_tensor torch tensor ccol_indices dtype=torch int torch tensor row_indices dtype=torch int torch tensor values dtype=torch double tensor ccol_indices=tensor row_indices=tensor values=tensor size= nnz= dtype=torch float layout=torch sparse_csc format factory_common_args add_docstr torch sparse_bsr_tensor r sparse_bsr_tensor crow_indices col_indices values size=None r dtype=None device=None pin_memory=False requires_grad=False check_invariants=None - Tensor Constructs ref ` sparse tensor BSR Block Compressed Sparse Row sparse-bsr-docs ` specified -dimensional blocks given attr ` crow_indices ` attr ` col_indices ` Sparse matrix multiplication operations BSR format typically faster than sparse tensors COO format Make you have look ref ` note data type indices sparse-bsr-docs ` sparse_factory_device_note Args crow_indices array_like B+ -dimensional array size ` ` batchsize nrowblocks + ` ` The last element each batch number non-zeros This tensor encodes block index values col_indices depending where given row block starts Each successive number tensor subtracted number before denotes number blocks given row col_indices array_like Column block coordinates each block values B+ -dimensional tensor same length values values array_list Initial values tensor Can list tuple NumPy ` ` ndarray ` ` scalar other types represents + + K -dimensional tensor where ` ` K ` ` number dense dimensions size list tuple ` torch Size ` optional Size sparse tensor ` ` batchsize nrows blocksize ncols blocksize densesize ` ` where ` ` blocksize == values shape ` ` If provided size will inferred minimum size big enough hold all non-zero blocks Keyword args dtype ` torch dtype ` optional desired data type returned tensor Default None infers data type attr ` values ` device ` torch device ` optional desired device returned tensor Default None uses current device default tensor type see func ` torch set_default_device ` attr ` device ` will CPU CPU tensor types current CUDA device CUDA tensor types pin_memory requires_grad check_invariants Example crow_indices = col_indices = values = torch sparse_bsr_tensor torch tensor crow_indices dtype=torch int torch tensor col_indices dtype=torch int torch tensor values dtype=torch double tensor crow_indices=tensor col_indices=tensor values=tensor size= nnz= dtype=torch float layout=torch sparse_bsr format factory_common_args add_docstr torch sparse_bsc_tensor r sparse_bsc_tensor ccol_indices row_indices values size=None r dtype=None device=None pin_memory=False requires_grad=False check_invariants=None - Tensor Constructs ref ` sparse tensor BSC Block Compressed Sparse Column sparse-bsc-docs ` specified -dimensional blocks given attr ` ccol_indices ` attr ` row_indices ` Sparse matrix multiplication operations BSC format typically faster than sparse tensors COO format Make you have look ref ` note data type indices sparse-bsc-docs ` sparse_factory_device_note Args ccol_indices array_like B+ -dimensional array size ` ` batchsize ncolblocks + ` ` The last element each batch number non-zeros This tensor encodes index values row_indices depending where given column starts Each successive number tensor subtracted number before denotes number elements given column row_indices array_like Row block coordinates each block values B+ -dimensional tensor same length values values array_list Initial blocks tensor Can list tuple NumPy ` ` ndarray ` ` other types represents + + K -dimensional tensor where ` ` K ` ` number dense dimensions size list tuple ` torch Size ` optional Size sparse tensor ` ` batchsize nrows blocksize ncols blocksize densesize ` ` If provided size will inferred minimum size big enough hold all non-zero blocks Keyword args dtype ` torch dtype ` optional desired data type returned tensor Default None infers data type attr ` values ` device ` torch device ` optional desired device returned tensor Default None uses current device default tensor type see func ` torch set_default_device ` attr ` device ` will CPU CPU tensor types current CUDA device CUDA tensor types pin_memory requires_grad check_invariants Example ccol_indices = row_indices = values = torch sparse_bsc_tensor torch tensor ccol_indices dtype=torch int torch tensor row_indices dtype=torch int torch tensor values dtype=torch double tensor ccol_indices=tensor row_indices=tensor values=tensor size= nnz= dtype=torch float layout=torch sparse_bsc format factory_common_args add_docstr torch sparse_coo_tensor r sparse_coo_tensor indices values size=None r dtype=None device=None pin_memory=False requires_grad=False check_invariants=None is_coalesced=None - Tensor Constructs ref ` sparse tensor COO rdinate format sparse-coo-docs ` specified values given attr ` indices ` note This function returns ref ` uncoalesced tensor sparse-uncoalesced-coo-docs ` when attr ` is_coalesced ` unspecified ` ` None ` ` sparse_factory_device_note Args indices array_like Initial data tensor Can list tuple NumPy ` ` ndarray ` ` scalar other types Will cast ` torch LongTensor ` internally The indices coordinates non-zero values matrix thus should two-dimensional where first dimension number tensor dimensions second dimension number non-zero values values array_like Initial values tensor Can list tuple NumPy ` ` ndarray ` ` scalar other types size list tuple ` torch Size ` optional Size sparse tensor If provided size will inferred minimum size big enough hold all non-zero elements Keyword args dtype ` torch dtype ` optional desired data type returned tensor Default None infers data type attr ` values ` device ` torch device ` optional desired device returned tensor Default None uses current device default tensor type see func ` torch set_default_device ` attr ` device ` will CPU CPU tensor types current CUDA device CUDA tensor types pin_memory requires_grad check_invariants is_coalesced bool optional When ` ` True ` ` caller responsible providing tensor indices correspond coalesced tensor If attr ` check_invariants ` flag False no error will raised prerequisites met will lead silently incorrect results To force coalescion please use meth ` coalesce ` resulting Tensor Default None except trivial cases e g nnz resulting Tensor has is_coalesced set ` ` False ` ` ` Example i = torch tensor v = torch tensor dtype=torch float torch sparse_coo_tensor i v tensor indices=tensor values=tensor size= nnz= layout=torch sparse_coo torch sparse_coo_tensor i v Shape inference tensor indices=tensor values=tensor size= nnz= layout=torch sparse_coo torch sparse_coo_tensor i v dtype=torch float device=torch device cuda tensor indices=tensor values=tensor device= cuda size= nnz= dtype=torch float layout=torch sparse_coo Create empty sparse tensor following invariants sparse_dim + dense_dim = len SparseTensor shape SparseTensor _indices shape = sparse_dim nnz SparseTensor _values shape = nnz SparseTensor shape sparse_dim For instance create empty sparse tensor nnz = dense_dim = sparse_dim = hence indices D tensor shape = S = torch sparse_coo_tensor torch empty tensor indices=tensor size= values=tensor size= size= nnz= layout=torch sparse_coo create empty sparse tensor nnz = dense_dim = sparse_dim = S = torch sparse_coo_tensor torch empty torch empty tensor indices=tensor size= values=tensor size= size= nnz= layout=torch sparse_coo _torch sparse https pytorch org docs stable sparse html format factory_common_args add_docstr torch sqrt r sqrt input out=None - Tensor Returns new tensor square-root elements attr ` input ` math \text out _ i = \sqrt \text input _ i + r Args input Keyword args out Example = torch randn tensor - torch sqrt tensor nan format common_args add_docstr torch square r square input Tensor out Optional Tensor - Tensor Returns new tensor square elements attr ` input ` Args input Keyword args out Example = torch randn tensor - torch square tensor format common_args add_docstr torch squeeze r squeeze input Tensor dim Optional Union int List int - Tensor Returns tensor all specified dimensions attr ` input ` size ` ` removed For example ` input ` shape math ` A \times \times B \times C \times \times D ` then ` input squeeze ` will shape math ` A \times B \times C \times D ` When attr ` dim ` given squeeze operation done only given dimension s If ` input ` shape math ` A \times \times B ` ` ` squeeze input ` ` leaves tensor unchanged ` ` squeeze input ` ` will squeeze tensor shape math ` A \times B ` note The returned tensor shares storage input tensor so changing contents one will change contents other warning If tensor has batch dimension size then ` squeeze input ` will also remove batch dimension which can lead unexpected errors Consider specifying only dims you wish squeezed Args input dim int tuple ints optional given input will squeezed only specified dimensions versionchanged attr ` dim ` now accepts tuples dimensions Example x = torch zeros x size torch Size y = torch squeeze x y size torch Size y = torch squeeze x y size torch Size y = torch squeeze x y size torch Size y = torch squeeze x torch Size format common_args add_docstr torch std r std input dim=None correction= keepdim=False out=None - Tensor Calculates standard deviation over dimensions specified attr ` dim ` attr ` dim ` can single dimension list dimensions ` ` None ` ` reduce over all dimensions The standard deviation math ` \sigma ` calculated math \sigma = \sqrt \frac \max ~N - \delta N \sum_ i= ^ N- x_i-\bar x ^ where math ` x ` sample set elements math ` \bar x ` sample mean math ` N ` number samples math ` \delta N ` attr ` correction ` + r keepdim_details Args input opt_dim_all_reduce Keyword args correction int difference between sample size sample degrees freedom Defaults ` Bessel s correction ` _ ` ` correction= ` ` versionchanged Previously argument called ` ` unbiased ` ` boolean ` ` True ` ` corresponding ` ` correction= ` ` ` ` False ` ` being ` ` correction= ` ` opt_keepdim out Example = torch tensor - - - - - - fmt skip torch std dim= keepdim=True tensor _Bessel s correction https en wikipedia org wiki Bessel s_correction format multi_dim_common add_docstr torch std_mean r std_mean input dim=None correction= keepdim=False out=None - Tensor Tensor Calculates standard deviation mean over dimensions specified attr ` dim ` attr ` dim ` can single dimension list dimensions ` ` None ` ` reduce over all dimensions The standard deviation math ` \sigma ` calculated math \sigma = \sqrt \frac \max ~N - \delta N \sum_ i= ^ N- x_i-\bar x ^ where math ` x ` sample set elements math ` \bar x ` sample mean math ` N ` number samples math ` \delta N ` attr ` correction ` + r keepdim_details Args input opt_dim_all_reduce Keyword args correction int difference between sample size sample degrees freedom Defaults ` Bessel s correction ` _ ` ` correction= ` ` versionchanged Previously argument called ` ` unbiased ` ` boolean ` ` True ` ` corresponding ` ` correction= ` ` ` ` False ` ` being ` ` correction= ` ` opt_keepdim out Returns A tuple std mean containing standard deviation mean Example = torch tensor - - - - - - fmt skip torch std_mean dim= keepdim=True tensor tensor - _Bessel s correction https en wikipedia org wiki Bessel s_correction format multi_dim_common add_docstr torch sub r sub input other alpha= out=None - Tensor Subtracts attr ` other ` scaled attr ` alpha ` attr ` input ` math \text out _i = \text input _i - \text alpha \times \text other _i + r Supports ref ` broadcasting common shape broadcasting-semantics ` ref ` type promotion type-promotion-doc ` integer float complex inputs Args input other Tensor Number tensor number subtract attr ` input ` Keyword args alpha Number multiplier attr ` other ` out Example = torch tensor b = torch tensor torch sub b alpha= tensor format common_args add_docstr torch subtract r subtract input other alpha= out=None - Tensor Alias func ` torch sub ` add_docstr torch sum r sum input dtype=None - Tensor Returns sum all elements attr ` input ` tensor Args input Keyword args dtype note Use ` dtype ` argument you need result specific tensor type Otherwise result type may automatically promoted e g ` torch int ` ` torch int ` Example = torch randn tensor - torch sum tensor - function sum input dim keepdim=False dtype=None - Tensor noindex Returns sum each row attr ` input ` tensor given dimension attr ` dim ` If attr ` dim ` list dimensions reduce over all them keepdim_details Args input opt_dim_all_reduce opt_keepdim Keyword args dtype Example = torch randn tensor - - - - - - - - torch sum tensor - - - b = torch arange view torch sum b tensor format multi_dim_common add_docstr torch nansum r nansum input dtype=None - Tensor Returns sum all elements treating Not Numbers NaNs zero Args input Keyword args dtype Example = torch tensor float nan torch nansum tensor function nansum input dim keepdim=False dtype=None - Tensor noindex Returns sum each row attr ` input ` tensor given dimension attr ` dim ` treating Not Numbers NaNs zero If attr ` dim ` list dimensions reduce over all them keepdim_details Args input opt_dim_all_reduce opt_keepdim Keyword args dtype Example torch nansum torch tensor float nan tensor = torch tensor float nan torch nansum tensor torch nansum dim= tensor torch nansum dim= tensor format multi_dim_common add_docstr torch svd r svd input some=True compute_uv=True out=None - Tensor Tensor Tensor Computes singular value decomposition either matrix batch matrices attr ` input ` The singular value decomposition represented namedtuple ` U S V ` such attr ` input ` math ` = U \text diag S V^ \text H ` where math ` V^ \text H ` transpose ` V ` real inputs conjugate transpose ` V ` complex inputs If attr ` input ` batch matrices then ` U ` ` S ` ` V ` also batched same batch dimensions attr ` input ` If attr ` some ` ` True ` default method returns reduced singular value decomposition In case last two dimensions attr ` input ` ` m ` ` n ` then returned ` U ` ` V ` matrices will contain only ` min n m ` orthonormal columns If attr ` compute_uv ` ` False ` returned ` U ` ` V ` will zero-filled matrices shape ` m m ` ` n n ` respectively same device attr ` input ` The argument attr ` some ` has no effect when attr ` compute_uv ` ` False ` Supports attr ` input ` float double cfloat cdouble data types The dtypes ` U ` ` V ` same attr ` input ` s ` S ` will always real-valued even attr ` input ` complex warning func ` torch svd ` deprecated favor func ` torch linalg svd ` will removed future PyTorch release ` ` U S V = torch svd A some=some compute_uv=True ` ` default should replaced code python U S Vh = torch linalg svd A full_matrices=not some V = Vh mH ` ` _ S _ = torch svd A some=some compute_uv=False ` ` should replaced code python S = torch linalg svdvals A note Differences func ` torch linalg svd ` attr ` some ` opposite func ` torch linalg svd ` s attr ` full_matrices ` Note default value both ` True ` so default behavior effectively opposite func ` torch svd ` returns ` V ` whereas func ` torch linalg svd ` returns ` Vh ` math ` V^ \text H ` If attr ` compute_uv ` ` False ` func ` torch svd ` returns zero-filled tensors ` U ` ` Vh ` whereas func ` torch linalg svd ` returns empty tensors note The singular values returned descending order If attr ` input ` batch matrices then singular values each matrix batch returned descending order note The ` S ` tensor can only used compute gradients attr ` compute_uv ` ` True ` note When attr ` some ` ` False ` gradients ` U min m n ` ` V min m n ` will ignored backward pass those vectors can arbitrary bases corresponding subspaces note The implementation func ` torch linalg svd ` CPU uses LAPACK s routine ` gesdd ` divide-and-conquer algorithm instead ` gesvd ` speed Analogously GPU uses cuSOLVER s routines ` gesvdj ` ` gesvdjBatched ` CUDA later MAGMA s routine ` gesdd ` earlier versions CUDA note The returned ` U ` will contiguous The matrix batch matrices will represented column-major matrix i e Fortran-contiguous warning The gradients respect ` U ` ` V ` will only finite when input does have zero nor repeated singular values warning If distance between any two singular values close zero gradients respect ` U ` ` V ` will numerically unstable they depends math ` \frac \min_ i \neq j \sigma_i^ - \sigma_j^ ` The same happens when matrix has small singular values these gradients also depend ` S^ - ` warning For complex-valued attr ` input ` singular value decomposition unique ` U ` ` V ` may multiplied arbitrary phase factor math ` e^ i \phi ` every column The same happens when attr ` input ` has repeated singular values where one may multiply columns spanning subspace ` U ` ` V ` rotation matrix ` resulting vectors will span same subspace ` _ Different platforms like NumPy inputs different device types may produce different ` U ` ` V ` tensors Args input Tensor input tensor size ` m n ` where ` ` zero more batch dimensions consisting ` m n ` matrices some bool optional controls whether compute reduced full decomposition consequently shape returned ` U ` ` V ` Default ` True ` compute_uv bool optional controls whether compute ` U ` ` V ` Default ` True ` Keyword args out tuple optional output tuple tensors Example = torch randn tensor - - - - - - u s v = torch svd u tensor - - - - s tensor v tensor - - - - torch dist torch mm torch mm u torch diag s v t tensor e- a_big = torch randn u s v = torch svd a_big torch dist a_big torch matmul torch matmul u torch diag_embed s v mT tensor e- _the resulting vectors will span same subspace https en wikipedia org wiki Singular_value_decomposition#Singular_values _singular_vectors _and_their_relation_to_the_SVD add_docstr torch t r t input - Tensor Expects attr ` input ` = -D tensor transposes dimensions -D -D tensors returned When input -D tensor equivalent ` ` transpose input ` ` Args input Example x = torch randn x tensor torch t x tensor x = torch randn x tensor - torch t x tensor - x = torch randn x tensor - - torch t x tensor - - See also func ` torch transpose ` format common_args add_docstr torch flip r flip input dims - Tensor Reverse order n-D tensor along given axis dims note ` torch flip ` makes copy attr ` input ` s data This different NumPy s ` np flip ` which returns view constant time Since copying tensor s data more work than viewing data ` torch flip ` expected slower than ` np flip ` Args input dims list tuple axis flip Example x = torch arange view x tensor torch flip x tensor format common_args add_docstr torch fliplr r fliplr input - Tensor Flip tensor left right direction returning new tensor Flip entries each row left right direction Columns preserved appear different order than before Note Requires tensor least -D note ` torch fliplr ` makes copy attr ` input ` s data This different NumPy s ` np fliplr ` which returns view constant time Since copying tensor s data more work than viewing data ` torch fliplr ` expected slower than ` np fliplr ` Args input Tensor Must least -dimensional Example x = torch arange view x tensor torch fliplr x tensor format common_args add_docstr torch flipud r flipud input - Tensor Flip tensor up down direction returning new tensor Flip entries each column up down direction Rows preserved appear different order than before Note Requires tensor least -D note ` torch flipud ` makes copy attr ` input ` s data This different NumPy s ` np flipud ` which returns view constant time Since copying tensor s data more work than viewing data ` torch flipud ` expected slower than ` np flipud ` Args input Tensor Must least -dimensional Example x = torch arange view x tensor torch flipud x tensor format common_args add_docstr torch roll r roll input shifts dims=None - Tensor Roll tensor attr ` input ` along given dimension s Elements shifted beyond last position re-introduced first position If attr ` dims ` ` None ` tensor will flattened before rolling then restored original shape Args input shifts int tuple ints The number places which elements tensor shifted If shifts tuple dims must tuple same size each dimension will rolled corresponding value dims int tuple ints Axis along which roll Example x = torch tensor view x tensor torch roll x tensor torch roll x tensor torch roll x - tensor torch roll x shifts= dims= tensor format common_args add_docstr torch rot r rot input k= dims= - Tensor Rotate n-D tensor degrees plane specified dims axis Rotation direction first towards second axis k second towards first k Args input k int number times rotate Default value dims list tuple axis rotate Default value Example x = torch arange view x tensor torch rot x tensor x = torch arange view x tensor torch rot x tensor format common_args add_docstr torch take r take input index - Tensor Returns new tensor elements attr ` input ` given indices The input tensor treated viewed -D tensor The result takes same shape indices Args input index LongTensor indices into tensor Example src = torch tensor torch take src torch tensor tensor format common_args add_docstr torch take_along_dim r take_along_dim input indices dim=None out=None - Tensor Selects values attr ` input ` -dimensional indices attr ` indices ` along given attr ` dim ` If attr ` dim ` None input array treated has been flattened d Functions indices along dimension like func ` torch argmax ` func ` torch argsort ` designed work function See examples below note This function similar NumPy s ` take_along_axis ` See also func ` torch gather ` Args input indices LongTensor indices into attr ` input ` Must have long dtype dim int optional dimension select along Default Keyword args out Example t = torch tensor max_idx = torch argmax t torch take_along_dim t max_idx tensor sorted_idx = torch argsort t dim= torch take_along_dim t sorted_idx dim= tensor format common_args add_docstr torch tan r tan input out=None - Tensor Returns new tensor tangent elements attr ` input ` tensor where each value input tensor radians math \text out _ i = \tan \text input _ i + r Args input Keyword args out Example = torch randn tensor - - - torch tan tensor - - format common_args add_docstr torch tanh r tanh input out=None - Tensor Returns new tensor hyperbolic tangent elements attr ` input ` math \text out _ i = \tanh \text input _ i + r Args input Keyword args out Example = torch randn tensor - torch tanh tensor - format common_args add_docstr torch softmax doc str Point torch nn functional softmax torch softmax r softmax input dim dtype=None - Tensor Alias func ` torch nn functional softmax ` add_docstr torch topk r topk input k dim=None largest=True sorted=True out=None - Tensor LongTensor Returns attr ` k ` largest elements given attr ` input ` tensor along given dimension If attr ` dim ` given last dimension ` input ` chosen If attr ` largest ` ` ` False ` ` then ` k ` smallest elements returned A namedtuple ` values indices ` returned ` values ` ` indices ` largest ` k ` elements each row ` input ` tensor given dimension ` dim ` The boolean option attr ` sorted ` ` ` True ` ` will make sure returned ` k ` elements themselves sorted note When using ` torch topk ` indices tied elements guaranteed stable may vary across different invocations Args input k int k top-k dim int optional dimension sort along largest bool optional controls whether largest smallest elements sorted bool optional controls whether elements sorted order Keyword args out tuple optional output tuple Tensor LongTensor can optionally given used output buffers Example x = torch arange x tensor torch topk x torch return_types topk values=tensor indices=tensor format common_args add_docstr torch trace r trace input - Tensor Returns sum elements diagonal input -D matrix Example x = torch arange view x tensor torch trace x tensor add_docstr torch transpose r transpose input dim dim - Tensor Returns tensor transposed version attr ` input ` The given dimensions attr ` dim ` attr ` dim ` swapped If attr ` input ` strided tensor then resulting attr ` out ` tensor shares its underlying storage attr ` input ` tensor so changing content one would change content other If attr ` input ` ref ` sparse tensor sparse-docs ` then resulting attr ` out ` tensor does share underlying storage attr ` input ` tensor If attr ` input ` ref ` sparse tensor sparse-docs ` compressed layout SparseCSR SparseBSR SparseCSC SparseBSC arguments attr ` dim ` attr ` dim ` must both batch dimensions must both sparse dimensions The batch dimensions sparse tensor dimensions preceding sparse dimensions note Transpositions which interchange sparse dimensions ` SparseCSR ` ` SparseCSC ` layout tensor will result layout changing between two options Transposition sparse dimensions ` SparseBSR ` ` SparseBSC ` layout tensor will likewise generate result opposite layout Args input dim int first dimension transposed dim int second dimension transposed Example x = torch randn x tensor - - torch transpose x tensor - - See also func ` torch t ` format common_args add_docstr torch triangular_solve r triangular_solve b A upper=True transpose=False unitriangular=False out=None - Tensor Tensor Solves system equations square upper lower triangular invertible matrix math ` A ` multiple right-hand sides math ` b ` In symbols solves math ` AX = b ` assumes math ` A ` square upper-triangular lower-triangular attr ` upper ` \ ` = False ` does have zeros diagonal ` torch triangular_solve b A ` can take D inputs ` b A ` inputs batches D matrices If inputs batches then returns batched outputs ` X ` If diagonal attr ` A ` contains zeros elements very close zero attr ` unitriangular ` \ ` = False ` default input matrix badly conditioned result may contain ` NaN ` s Supports input float double cfloat cdouble data types warning func ` torch triangular_solve ` deprecated favor func ` torch linalg solve_triangular ` will removed future PyTorch release func ` torch linalg solve_triangular ` has its arguments reversed does copy one inputs ` ` X = torch triangular_solve B A solution ` ` should replaced code python X = torch linalg solve_triangular A B Args b Tensor multiple right-hand sides size math ` m k ` where math ` ` zero more batch dimensions A Tensor input triangular coefficient matrix size math ` m m ` where math ` ` zero more batch dimensions upper bool optional whether math ` A ` upper lower triangular Default ` ` True ` ` transpose bool optional solves ` op A X = b ` where ` op A = A^T ` flag ` ` True ` ` ` op A = A ` ` ` False ` ` Default ` ` False ` ` unitriangular bool optional whether math ` A ` unit triangular If True diagonal elements math ` A ` assumed referenced math ` A ` Default ` ` False ` ` Keyword args out Tensor Tensor optional tuple two tensors write output Ignored ` None ` Default ` None ` Returns A namedtuple ` solution cloned_coefficient ` where ` cloned_coefficient ` clone math ` A ` ` solution ` solution math ` X ` math ` AX = b ` whatever variant system equations depending keyword arguments Examples A = torch randn triu A tensor - b = torch randn b tensor - - - torch triangular_solve b A torch return_types triangular_solve solution=tensor - - cloned_coefficient=tensor - add_docstr torch tril r tril input diagonal= out=None - Tensor Returns lower triangular part matrix -D tensor batch matrices attr ` input ` other elements result tensor attr ` out ` set The lower triangular part matrix defined elements below diagonal The argument attr ` diagonal ` controls which diagonal consider If attr ` diagonal ` = all elements below main diagonal retained A positive value includes just many diagonals above main diagonal similarly negative value excludes just many diagonals below main diagonal The main diagonal set indices math ` \lbrace i i \rbrace ` math ` i \in \min\ d_ d_ \ - ` where math ` d_ d_ ` dimensions matrix + r Args input diagonal int optional diagonal consider Keyword args out Example = torch randn tensor - - - - torch tril tensor - - - b = torch randn b tensor - - - - - - - - - - torch tril b diagonal= tensor - - - - - - - - torch tril b diagonal=- tensor - - - format common_args docstr split two parts avoid format mis-captureing math braces common args add_docstr torch tril_indices r tril_indices row col offset= dtype=torch long device= cpu layout=torch strided - Tensor Returns indices lower triangular part attr ` row ` -by- attr ` col ` matrix -by-N Tensor where first row contains row coordinates all indices second row contains column coordinates Indices ordered based rows then columns The lower triangular part matrix defined elements below diagonal The argument attr ` offset ` controls which diagonal consider If attr ` offset ` = all elements below main diagonal retained A positive value includes just many diagonals above main diagonal similarly negative value excludes just many diagonals below main diagonal The main diagonal set indices math ` \lbrace i i \rbrace ` math ` i \in \min\ d_ d_ \ - ` where math ` d_ d_ ` dimensions matrix note When running CUDA ` ` row col ` ` must less than math ` ^ ` prevent overflow during calculation + r Args row ` ` int ` ` number rows -D matrix col ` ` int ` ` number columns -D matrix offset ` ` int ` ` diagonal offset main diagonal Default provided Keyword args dtype ` torch dtype ` optional desired data type returned tensor only support ` ` torch int ` ` ` ` torch long ` ` Default ` ` None ` ` ` ` torch long ` ` device layout ` torch layout ` optional currently only support ` ` torch strided ` ` Example = torch tril_indices tensor = torch tril_indices - tensor = torch tril_indices tensor format factory_common_args add_docstr torch triu r triu input diagonal= out=None - Tensor Returns upper triangular part matrix -D tensor batch matrices attr ` input ` other elements result tensor attr ` out ` set The upper triangular part matrix defined elements above diagonal The argument attr ` diagonal ` controls which diagonal consider If attr ` diagonal ` = all elements above main diagonal retained A positive value excludes just many diagonals above main diagonal similarly negative value includes just many diagonals below main diagonal The main diagonal set indices math ` \lbrace i i \rbrace ` math ` i \in \min\ d_ d_ \ - ` where math ` d_ d_ ` dimensions matrix + r Args input diagonal int optional diagonal consider Keyword args out Example = torch randn tensor - - - torch triu tensor - - torch triu diagonal= tensor torch triu diagonal=- tensor - - - b = torch randn b tensor - - - - - - - - - - - torch triu b diagonal= tensor - - - - - - - torch triu b diagonal=- tensor - - - - - - - - - - format common_args docstr split two parts avoid format mis-capturing math braces common args add_docstr torch triu_indices r triu_indices row col offset= dtype=torch long device= cpu layout=torch strided - Tensor Returns indices upper triangular part attr ` row ` attr ` col ` matrix -by-N Tensor where first row contains row coordinates all indices second row contains column coordinates Indices ordered based rows then columns The upper triangular part matrix defined elements above diagonal The argument attr ` offset ` controls which diagonal consider If attr ` offset ` = all elements above main diagonal retained A positive value excludes just many diagonals above main diagonal similarly negative value includes just many diagonals below main diagonal The main diagonal set indices math ` \lbrace i i \rbrace ` math ` i \in \min\ d_ d_ \ - ` where math ` d_ d_ ` dimensions matrix note When running CUDA ` ` row col ` ` must less than math ` ^ ` prevent overflow during calculation + r Args row ` ` int ` ` number rows -D matrix col ` ` int ` ` number columns -D matrix offset ` ` int ` ` diagonal offset main diagonal Default provided Keyword args dtype ` torch dtype ` optional desired data type returned tensor only support ` ` torch int ` ` ` ` torch long ` ` Default ` ` None ` ` ` ` torch long ` ` device layout ` torch layout ` optional currently only support ` ` torch strided ` ` Example = torch triu_indices tensor = torch triu_indices - tensor = torch triu_indices tensor format factory_common_args add_docstr torch true_divide r true_divide dividend divisor out - Tensor Alias func ` torch div ` ` ` rounding_mode=None ` ` add_docstr torch trunc r trunc input out=None - Tensor Returns new tensor truncated integer values elements attr ` input ` For integer inputs follows array-api convention returning copy input tensor Args input Keyword args out Example = torch randn tensor - - torch trunc tensor - - format common_args add_docstr torch fake_quantize_per_tensor_affine r fake_quantize_per_tensor_affine input scale zero_point quant_min quant_max - Tensor Returns new tensor data attr ` input ` fake quantized using attr ` scale ` attr ` zero_point ` attr ` quant_min ` attr ` quant_max ` math \text output = min \text quant\_max max \text quant\_min \text std nearby\_int \text input \text scale + \text zero\_point - \text zero\_point \times \text scale Args input Tensor input value s ` ` torch float ` ` tensor scale double scalar ` ` float ` ` Tensor quantization scale zero_point int scalar ` ` int ` ` Tensor quantization zero_point quant_min int lower bound quantized domain quant_max int upper bound quantized domain Returns Tensor A newly fake_quantized ` ` torch float ` ` tensor Example x = torch randn x tensor - torch fake_quantize_per_tensor_affine x tensor torch fake_quantize_per_tensor_affine x torch tensor torch tensor tensor add_docstr torch fake_quantize_per_channel_affine r fake_quantize_per_channel_affine input scale zero_point axis quant_min quant_max - Tensor Returns new tensor data attr ` input ` fake quantized per channel using attr ` scale ` attr ` zero_point ` attr ` quant_min ` attr ` quant_max ` across channel specified attr ` axis ` math \text output = min \text quant\_max max \text quant\_min \text std nearby\_int \text input \text scale + \text zero\_point - \text zero\_point \times \text scale Args input Tensor input value s ` ` torch float ` ` scale Tensor quantization scale per channel ` ` torch float ` ` zero_point Tensor quantization zero_point per channel ` ` torch int ` ` ` ` torch half ` ` ` ` torch float ` ` axis int channel axis quant_min int lower bound quantized domain quant_max int upper bound quantized domain Returns Tensor A newly fake_quantized per channel ` ` torch float ` ` tensor Example x = torch randn x tensor - - - - - scales = torch randn + scales tensor zero_points = torch zeros torch int zero_points tensor torch fake_quantize_per_channel_affine x scales zero_points tensor add_docstr torch fix r fix input out=None - Tensor Alias func ` torch trunc ` add_docstr torch unsqueeze r unsqueeze input dim - Tensor Returns new tensor dimension size one inserted specified position The returned tensor shares same underlying data tensor A attr ` dim ` value within range ` ` -input dim - input dim + ` ` can used Negative attr ` dim ` will correspond meth ` unsqueeze ` applied attr ` dim ` = ` ` dim + input dim + ` ` Args input dim int index which insert singleton dimension Example x = torch tensor torch unsqueeze x tensor torch unsqueeze x tensor format common_args add_docstr torch var r var input dim=None correction= keepdim=False out=None - Tensor Calculates variance over dimensions specified attr ` dim ` attr ` dim ` can single dimension list dimensions ` ` None ` ` reduce over all dimensions The variance math ` \sigma^ ` calculated math \sigma^ = \frac \max ~N - \delta N \sum_ i= ^ N- x_i-\bar x ^ where math ` x ` sample set elements math ` \bar x ` sample mean math ` N ` number samples math ` \delta N ` attr ` correction ` + r keepdim_details Args input opt_dim_all_reduce Keyword args correction int difference between sample size sample degrees freedom Defaults ` Bessel s correction ` _ ` ` correction= ` ` versionchanged Previously argument called ` ` unbiased ` ` boolean ` ` True ` ` corresponding ` ` correction= ` ` ` ` False ` ` being ` ` correction= ` ` opt_keepdim out Example = torch tensor - - - - - - fmt skip torch var dim= keepdim=True tensor _Bessel s correction https en wikipedia org wiki Bessel s_correction format multi_dim_common add_docstr torch var_mean r var_mean input dim=None correction= keepdim=False out=None - Tensor Tensor Calculates variance mean over dimensions specified attr ` dim ` attr ` dim ` can single dimension list dimensions ` ` None ` ` reduce over all dimensions The variance math ` \sigma^ ` calculated math \sigma^ = \frac \max ~N - \delta N \sum_ i= ^ N- x_i-\bar x ^ where math ` x ` sample set elements math ` \bar x ` sample mean math ` N ` number samples math ` \delta N ` attr ` correction ` + r keepdim_details Args input opt_dim_all_reduce Keyword args correction int difference between sample size sample degrees freedom Defaults ` Bessel s correction ` _ ` ` correction= ` ` versionchanged Previously argument called ` ` unbiased ` ` boolean ` ` True ` ` corresponding ` ` correction= ` ` ` ` False ` ` being ` ` correction= ` ` opt_keepdim out Returns A tuple var mean containing variance mean Example = torch tensor - - - - - - fmt skip torch var_mean dim= keepdim=True tensor tensor - _Bessel s correction https en wikipedia org wiki Bessel s_correction format multi_dim_common add_docstr torch zeros r zeros size out=None dtype=None layout=torch strided device=None requires_grad=False - Tensor Returns tensor filled scalar value ` ` shape defined variable argument attr ` size ` Args size int sequence integers defining shape output tensor Can variable number arguments collection like list tuple Keyword args out dtype layout device requires_grad Example torch zeros tensor torch zeros tensor format factory_common_args add_docstr torch zeros_like r zeros_like input dtype=None layout=None device=None requires_grad=False memory_format=torch preserve_format - Tensor Returns tensor filled scalar value ` ` same size attr ` input ` ` ` torch zeros_like input ` ` equivalent ` ` torch zeros input size dtype=input dtype layout=input layout device=input device ` ` warning As function does support attr ` out ` keyword As alternative old ` ` torch zeros_like input out=output ` ` equivalent ` ` torch zeros input size out=output ` ` Args input Keyword args dtype layout device requires_grad memory_format Example input = torch empty torch zeros_like input tensor format factory_like_common_args add_docstr torch empty empty size out=None dtype=None layout=torch strided device=None requires_grad=False pin_memory=False \ memory_format=torch contiguous_format - Tensor Returns tensor filled uninitialized data The shape tensor defined variable argument attr ` size ` note If func ` torch use_deterministic_algorithms ` attr ` torch utils deterministic fill_uninitialized_memory ` both set ` ` True ` ` output tensor initialized prevent any possible nondeterministic behavior using data input operation Floating point complex tensors filled NaN integer tensors filled maximum value Args size int sequence integers defining shape output tensor Can variable number arguments collection like list tuple Keyword args out dtype layout device requires_grad pin_memory memory_format Example torch empty dtype=torch int tensor e+ e+ e+ e+ e+ e+ format factory_common_args add_docstr torch empty_like r empty_like input dtype=None layout=None device=None requires_grad=False memory_format=torch preserve_format - Tensor Returns uninitialized tensor same size attr ` input ` ` ` torch empty_like input ` ` equivalent ` ` torch empty input size dtype=input dtype layout=input layout device=input device ` ` note If func ` torch use_deterministic_algorithms ` attr ` torch utils deterministic fill_uninitialized_memory ` both set ` ` True ` ` output tensor initialized prevent any possible nondeterministic behavior using data input operation Floating point complex tensors filled NaN integer tensors filled maximum value When ` ` torch preserve_format ` ` used If input tensor dense i e non-overlapping strided its memory format including strides retained Otherwise e g non-dense view like stepped slice output converted dense format Args input Keyword args dtype layout device requires_grad memory_format Example a=torch empty dtype=torch int device = cuda torch empty_like tensor device= cuda dtype=torch int format factory_like_common_args add_docstr torch empty_strided r empty_strided size stride dtype=None layout=None device=None requires_grad=False pin_memory=False - Tensor Creates tensor specified attr ` size ` attr ` stride ` filled undefined data warning If constructed tensor overlapped multiple indices referring same element memory its behavior undefined note If func ` torch use_deterministic_algorithms ` attr ` torch utils deterministic fill_uninitialized_memory ` both set ` ` True ` ` output tensor initialized prevent any possible nondeterministic behavior using data input operation Floating point complex tensors filled NaN integer tensors filled maximum value Args size tuple int shape output tensor stride tuple int strides output tensor Keyword args dtype layout device requires_grad pin_memory Example = torch empty_strided tensor e- e- e+ e+ e+ e- stride size torch Size format factory_common_args add_docstr torch empty_permuted r empty_permuted size physical_layout dtype=None layout=None device=None requires_grad=False pin_memory=False - Tensor Creates uninitialized non-overlapping dense tensor specified attr ` size ` attr ` physical_layout ` specifying how dimensions physically laid out memory each logical dimension listed outermost innermost attr ` physical_layout ` generalization NCHW NHWC notation each dimension assigned number according what order they occur size N= C= H= W= then NCHW ` ` ` ` while NHWC ` ` ` ` Equivalently strides output tensor ` ` t ` ` such ` ` t stride physical_layout i == contiguous_strides i ` ` notably function equivalent ` ` torch empty size permute physical_layout ` ` Unlike func ` torch empty_strided ` guaranteed produce dense tensor no overlaps If possible prefer using function over func ` torch empty_strided ` manual use func ` torch as_strided ` note If func ` torch use_deterministic_algorithms ` attr ` torch utils deterministic fill_uninitialized_memory ` both set ` ` True ` ` output tensor initialized prevent any possible nondeterministic behavior using data input operation Floating point complex tensors filled NaN integer tensors filled maximum value Args size tuple int shape output tensor physical_layout tuple int ordering dimensions physically memory Keyword args dtype layout device requires_grad pin_memory Examples torch empty stride torch empty_permuted stride torch empty memory_format=torch channels_last stride torch empty_permuted stride torch empty_permuted dim_order format factory_common_args add_docstr torch full r full size fill_value out=None dtype=None layout=torch strided device=None requires_grad=False - Tensor Creates tensor size attr ` size ` filled attr ` fill_value ` The tensor s dtype inferred attr ` fill_value ` Args size int list tuple ` torch Size ` integers defining shape output tensor fill_value Scalar value fill output tensor Keyword args out dtype layout device requires_grad Example torch full tensor format factory_common_args add_docstr torch full_like full_like input fill_value \\ dtype=None layout=torch strided device=None requires_grad=False \ memory_format=torch preserve_format - Tensor Returns tensor same size attr ` input ` filled attr ` fill_value ` ` ` torch full_like input fill_value ` ` equivalent ` ` torch full input size fill_value dtype=input dtype layout=input layout device=input device ` ` Args input fill_value number fill output tensor Keyword args dtype layout device requires_grad memory_format Example x = torch ones torch full_like x tensor torch full_like x tensor torch full_like x dtype=torch int tensor dtype=torch int y = torch randn dtype=torch float torch full_like y - tensor - - - - - - - - - - - - dtype=torch float format factory_like_common_args add_docstr torch det r det input - Tensor Alias func ` torch linalg det ` add_docstr torch where r where condition input other out=None - Tensor Return tensor elements selected either attr ` input ` attr ` other ` depending attr ` condition ` The operation defined math \text out _i = \begin cases \text input _i \text \text condition _i \\ \text other _i \text otherwise \\ \end cases + r note The tensors attr ` condition ` attr ` input ` attr ` other ` must ref ` broadcastable broadcasting-semantics ` Arguments condition BoolTensor When True nonzero yield input otherwise yield other input Tensor Scalar value attr ` input ` scalar values selected indices where attr ` condition ` ` ` True ` ` other Tensor Scalar value attr ` other ` scalar values selected indices where attr ` condition ` ` ` False ` ` Keyword args out Returns Tensor A tensor shape equal broadcasted shape attr ` condition ` attr ` input ` attr ` other ` Example x = torch randn y = torch ones x tensor - - - torch where x tensor torch where x x y tensor x = torch randn dtype=torch double x tensor - - dtype=torch float torch where x x tensor dtype=torch float function where condition - tuple LongTensor noindex ` ` torch where condition ` ` identical ` ` torch nonzero condition as_tuple=True ` ` note See also func ` torch nonzero ` format common_args add_docstr torch logdet r logdet input - Tensor Calculates log determinant square matrix batches square matrices It returns ` ` -inf ` ` input has determinant zero ` ` NaN ` ` has negative determinant note Backward through meth ` logdet ` internally uses SVD results when attr ` input ` invertible In case double backward through meth ` logdet ` will unstable when attr ` input ` doesn t have distinct singular values See func ` torch linalg svd ` details seealso func ` torch linalg slogdet ` computes sign resp angle natural logarithm absolute value determinant real-valued resp complex square matrices Arguments input Tensor input tensor size ` ` n n ` ` where ` ` ` ` zero more batch dimensions Example A = torch randn torch det A tensor torch logdet A tensor - A tensor - - - - - - A det tensor A det log tensor - - add_docstr torch slogdet r slogdet input - Tensor Tensor Alias func ` torch linalg slogdet ` add_docstr torch pinverse r pinverse input rcond= e- - Tensor Alias func ` torch linalg pinv ` add_docstr torch hann_window hann_window window_length periodic=True dtype=None \ layout=torch strided device=None requires_grad=False - Tensor + r Hann window function math w n = \frac \ \left - \cos \left \frac \pi n N - \right \right = \sin^ \left \frac \pi n N - \right where math ` N ` full window size The input attr ` window_length ` positive integer controlling returned window size attr ` periodic ` flag determines whether returned window trims off last duplicate value symmetric window ready used periodic window functions like meth ` torch stft ` Therefore attr ` periodic ` true math ` N ` above formula fact math ` \text window\_length + ` Also we always have ` ` torch hann_window L periodic=True ` ` equal ` ` torch hann_window L + periodic=False - ` ` note If attr ` window_length ` math ` = ` returned window contains single value + r Arguments window_length int size returned window periodic bool optional If True returns window used periodic function If False symmetric window Keyword args dtype Only floating point types supported layout ` torch layout ` optional desired layout returned window tensor Only ` ` torch strided ` ` dense layout supported device requires_grad Returns Tensor A -D tensor size math ` \text window\_length ` containing window format factory_common_args add_docstr torch hamming_window hamming_window window_length dtype=None layout=None device=None pin_memory=False \ requires_grad=False - Tensor + r Hamming window function math w n = \alpha - \beta\ \cos \left \frac \pi n N - \right where math ` N ` full window size The input attr ` window_length ` positive integer controlling returned window size attr ` periodic ` flag determines whether returned window trims off last duplicate value symmetric window ready used periodic window functions like meth ` torch stft ` Therefore attr ` periodic ` true math ` N ` above formula fact math ` \text window\_length + ` Also we always have ` ` torch hamming_window L periodic=True ` ` equal ` ` torch hamming_window L + periodic=False - ` ` note If attr ` window_length ` math ` = ` returned window contains single value note This generalized version meth ` torch hann_window ` + r Arguments window_length int size returned window Keyword args dtype Only floating point types supported layout ` torch layout ` optional desired layout returned window tensor Only ` ` torch strided ` ` dense layout supported device pin_memory requires_grad Returns Tensor A -D tensor size math ` \text window\_length ` containing window function hamming_window window_length periodic dtype=None layout=None device=None \ pin_memory=False requires_grad=False - Tensor noindex Hamming window function periodic specified Arguments window_length int size returned window periodic bool If True returns window used periodic function If False symmetric window Keyword args dtype Only floating point types supported layout ` torch layout ` optional desired layout returned window tensor Only ` ` torch strided ` ` dense layout supported device pin_memory requires_grad Returns Tensor A -D tensor size math ` \text window\_length ` containing window function hamming_window window_length periodic float alpha dtype=None layout=None device=None \ pin_memory=False requires_grad=False - Tensor noindex Hamming window function periodic alpha specified Arguments window_length int size returned window periodic bool If True returns window used periodic function If False symmetric window alpha float The coefficient math ` \alpha ` equation above Keyword args dtype Only floating point types supported layout ` torch layout ` optional desired layout returned window tensor Only ` ` torch strided ` ` dense layout supported device pin_memory requires_grad Returns Tensor A -D tensor size math ` \text window\_length ` containing window function hamming_window window_length periodic float alpha float beta dtype=None layout=None \ device=None pin_memory=False requires_grad=False - Tensor noindex Hamming window function periodic alpha beta specified Arguments window_length int size returned window periodic bool If True returns window used periodic function If False symmetric window alpha float The coefficient math ` \alpha ` equation above beta float The coefficient math ` \beta ` equation above Keyword args dtype Only floating point types supported layout ` torch layout ` optional desired layout returned window tensor Only ` ` torch strided ` ` dense layout supported device pin_memory requires_grad Returns Tensor A -D tensor size math ` \text window\_length ` containing window format factory_common_args add_docstr torch bartlett_window bartlett_window window_length periodic=True dtype=None \ layout=torch strided device=None requires_grad=False - Tensor + r Bartlett window function math w n = - \left &#124; \frac n N- - \right &#124; = \begin cases \frac n N - \text \leq n \leq \frac N - \\ - \frac n N - \text \frac N - n N \\ \end cases where math ` N ` full window size The input attr ` window_length ` positive integer controlling returned window size attr ` periodic ` flag determines whether returned window trims off last duplicate value symmetric window ready used periodic window functions like meth ` torch stft ` Therefore attr ` periodic ` true math ` N ` above formula fact math ` \text window\_length + ` Also we always have ` ` torch bartlett_window L periodic=True ` ` equal ` ` torch bartlett_window L + periodic=False - ` ` note If attr ` window_length ` math ` = ` returned window contains single value + r Arguments window_length int size returned window periodic bool optional If True returns window used periodic function If False symmetric window Keyword args dtype Only floating point types supported layout ` torch layout ` optional desired layout returned window tensor Only ` ` torch strided ` ` dense layout supported device requires_grad Returns Tensor A -D tensor size math ` \text window\_length ` containing window format factory_common_args add_docstr torch blackman_window blackman_window window_length periodic=True dtype=None \ layout=torch strided device=None requires_grad=False - Tensor + r Blackman window function math w n = - \cos \left \frac \pi n N - \right + \cos \left \frac \pi n N - \right where math ` N ` full window size The input attr ` window_length ` positive integer controlling returned window size attr ` periodic ` flag determines whether returned window trims off last duplicate value symmetric window ready used periodic window functions like meth ` torch stft ` Therefore attr ` periodic ` true math ` N ` above formula fact math ` \text window\_length + ` Also we always have ` ` torch blackman_window L periodic=True ` ` equal ` ` torch blackman_window L + periodic=False - ` ` note If attr ` window_length ` math ` = ` returned window contains single value + r Arguments window_length int size returned window periodic bool optional If True returns window used periodic function If False symmetric window Keyword args dtype Only floating point types supported layout ` torch layout ` optional desired layout returned window tensor Only ` ` torch strided ` ` dense layout supported device requires_grad Returns Tensor A -D tensor size math ` \text window\_length ` containing window format factory_common_args add_docstr torch kaiser_window kaiser_window window_length periodic=True beta= dtype=None \ layout=torch strided device=None requires_grad=False - Tensor + r Computes Kaiser window window length attr ` window_length ` shape parameter attr ` beta ` Let I_ zeroth order modified Bessel function first kind see func ` torch i ` ` ` N = L - ` ` attr ` periodic ` False ` ` L ` ` attr ` periodic ` True where ` ` L ` ` attr ` window_length ` This function computes math out_i = I_ \left \beta \sqrt - \left \frac i - N N \right ^ \right I_ \beta Calling ` ` torch kaiser_window L B periodic=True ` ` equivalent calling ` ` torch kaiser_window L + B periodic=False - ` ` The attr ` periodic ` argument intended helpful shorthand produce periodic window input functions like func ` torch stft ` note If attr ` window_length ` one then returned window single element tensor containing one + r Args window_length int length window periodic bool optional If True returns periodic window suitable use spectral analysis If False returns symmetric window suitable use filter design beta float optional shape parameter window Keyword args dtype layout ` torch layout ` optional desired layout returned window tensor Only ` ` torch strided ` ` dense layout supported device requires_grad format factory_common_args add_docstr torch vander vander x N=None increasing=False - Tensor + r Generates Vandermonde matrix The columns output matrix elementwise powers input vector math ` x^ N- x^ N- x^ ` If increasing True order columns reversed math ` x^ x^ x^ N- ` Such matrix geometric progression each row named Alexandre-Theophile Vandermonde Arguments x Tensor -D input tensor N int optional Number columns output If N specified square array returned math ` N = len x ` increasing bool optional Order powers columns If True powers increase left right False default they reversed Returns Tensor Vandermonde matrix If increasing False first column math ` x^ N- ` second math ` x^ N- ` so forth If increasing True columns math ` x^ x^ x^ N- ` Example x = torch tensor torch vander x tensor torch vander x N= tensor torch vander x N= increasing=True tensor format factory_common_args add_docstr torch unbind r unbind input dim= - seq Removes tensor dimension Returns tuple all slices along given dimension already without Arguments input Tensor tensor unbind dim int dimension remove Example torch unbind torch tensor tensor tensor tensor add_docstr torch combinations r combinations input Tensor r int = with_replacement bool = False - seq Compute combinations length math ` r ` given tensor The behavior similar python s ` itertools combinations ` when ` with_replacement ` set ` False ` ` itertools combinations_with_replacement ` when ` with_replacement ` set ` True ` Arguments input Tensor D vector r int optional number elements combine with_replacement bool optional whether allow duplication combination Returns Tensor A tensor equivalent converting all input tensors into lists do ` itertools combinations ` ` itertools combinations_with_replacement ` these lists finally convert resulting list into tensor Example = list itertools combinations r= list itertools combinations r= list itertools combinations_with_replacement r= tensor_a = torch tensor torch combinations tensor_a tensor torch combinations tensor_a r= tensor torch combinations tensor_a with_replacement=True tensor add_docstr torch trapezoid r trapezoid y x=None dx=None dim=- - Tensor Computes ` trapezoidal rule https en wikipedia org wiki Trapezoidal_rule ` _ along attr ` dim ` By default spacing between elements assumed attr ` dx ` can used specify different constant spacing attr ` x ` can used specify arbitrary spacing along attr ` dim ` Only one attr ` x ` attr ` dx ` should specified Assuming attr ` y ` one-dimensional tensor elements math ` y_ y_ y_n ` default computation math \begin aligned \sum_ i = ^ n \frac y_i + y_ i- \end aligned When attr ` dx ` specified computation becomes math \begin aligned \sum_ i = ^ n \frac \Delta x y_i + y_ i- \end aligned effectively multiplying result attr ` dx ` When attr ` x ` specified assuming attr ` x ` also one-dimensional tensor elements math ` x_ x_ x_n ` computation becomes math \begin aligned \sum_ i = ^ n \frac x_i - x_ i- y_i + y_ i- \end aligned When attr ` x ` attr ` y ` have same size computation described above no broadcasting needed The broadcasting behavior function follows when their sizes different For both attr ` x ` attr ` y ` function computes difference between consecutive elements along dimension attr ` dim ` This effectively creates two tensors ` x_diff ` ` y_diff ` have same shape original tensors except their lengths along dimension attr ` dim ` reduced After those two tensors broadcast together compute final output part trapezoidal rule See examples below details note The trapezoidal rule technique approximating definite integral function averaging its left right Riemann sums The approximation becomes more accurate resolution partition increases Arguments y Tensor Values use when computing trapezoidal rule x Tensor If specified defines spacing between values specified above Keyword arguments dx float constant spacing between values If neither attr ` x ` attr ` dx ` specified then defaults Effectively multiplies result its value dim int The dimension along which compute trapezoidal rule The last inner-most dimension default Examples Computes trapezoidal rule D spacing implicitly y = torch tensor torch trapezoid y tensor Computes same trapezoidal rule directly verify + + Computes trapezoidal rule D constant spacing NOTE result same before multiplied torch trapezoid y dx= Computes trapezoidal rule D arbitrary spacing x = torch tensor torch trapezoid y x Computes same trapezoidal rule directly verify - + + - + Computes trapezoidal rule each row x matrix y = torch arange reshape tensor torch trapezoid y tensor Computes trapezoidal rule each column matrix torch trapezoid y dim= tensor Computes trapezoidal rule each row x ones matrix same arbitrary spacing y = torch ones x = torch tensor torch trapezoid y x array Computes trapezoidal rule each row x ones matrix different arbitrary spacing per row y = torch ones x = torch tensor torch trapezoid y x array add_docstr torch trapz r trapz y x dim=- - Tensor Alias func ` torch trapezoid ` add_docstr torch cumulative_trapezoid r cumulative_trapezoid y x=None dx=None dim=- - Tensor Cumulatively computes ` trapezoidal rule https en wikipedia org wiki Trapezoidal_rule ` _ along attr ` dim ` By default spacing between elements assumed attr ` dx ` can used specify different constant spacing attr ` x ` can used specify arbitrary spacing along attr ` dim ` For more details please read func ` torch trapezoid ` The difference between func ` torch trapezoid ` function func ` torch trapezoid ` returns value each integration where function returns cumulative value every spacing within integration This analogous how ` sum ` returns value ` cumsum ` returns cumulative sum Arguments y Tensor Values use when computing trapezoidal rule x Tensor If specified defines spacing between values specified above Keyword arguments dx float constant spacing between values If neither attr ` x ` attr ` dx ` specified then defaults Effectively multiplies result its value dim int The dimension along which compute trapezoidal rule The last inner-most dimension default Examples Cumulatively computes trapezoidal rule D spacing implicitly y = torch tensor torch cumulative_trapezoid y tensor Computes same trapezoidal rule directly up each element verify + + + Cumulatively computes trapezoidal rule D constant spacing NOTE result same before multiplied torch cumulative_trapezoid y dx= tensor Cumulatively computes trapezoidal rule D arbitrary spacing x = torch tensor torch cumulative_trapezoid y x tensor Computes same trapezoidal rule directly up each element verify - + - + + - + Cumulatively computes trapezoidal rule each row x matrix y = torch arange reshape tensor torch cumulative_trapezoid y tensor Cumulatively computes trapezoidal rule each column matrix torch cumulative_trapezoid y dim= tensor Cumulatively computes trapezoidal rule each row x ones matrix same arbitrary spacing y = torch ones x = torch tensor torch cumulative_trapezoid y x tensor Cumulatively computes trapezoidal rule each row x ones matrix different arbitrary spacing per row y = torch ones x = torch tensor torch cumulative_trapezoid y x tensor add_docstr torch repeat_interleave r repeat_interleave input repeats dim=None output_size=None - Tensor Repeat elements tensor warning This different meth ` torch Tensor repeat ` similar ` ` numpy repeat ` ` Args input repeats Tensor int The number repetitions each element repeats broadcasted fit shape given axis dim int optional The dimension along which repeat values By default use flattened input array flat output array Keyword args output_size int optional Total output size given axis e g sum repeats If given will avoid stream synchronization needed calculate output shape tensor Returns Tensor Repeated tensor which has same shape input except along given axis Example x = torch tensor x repeat_interleave tensor y = torch tensor torch repeat_interleave y tensor torch repeat_interleave y dim= tensor torch repeat_interleave y torch tensor dim= tensor torch repeat_interleave y torch tensor dim= output_size= tensor If ` repeats ` ` tensor n n n ` then output will ` tensor ` where ` ` appears ` n ` times ` ` appears ` n ` times ` ` appears ` n ` times etc function repeat_interleave repeats - Tensor noindex Repeats repeats times repeats times repeats times etc Args repeats Tensor The number repetitions each element Returns Tensor Repeated tensor size ` sum repeats ` Example torch repeat_interleave torch tensor tensor format common_args add_docstr torch tile r tile input dims - Tensor Constructs tensor repeating elements attr ` input ` The attr ` dims ` argument specifies number repetitions each dimension If attr ` dims ` specifies fewer dimensions than attr ` input ` has then ones prepended attr ` dims ` until all dimensions specified For example attr ` input ` has shape attr ` dims ` then attr ` dims ` treated Analogously attr ` input ` has fewer dimensions than attr ` dims ` specifies then attr ` input ` treated unsqueezed dimension zero until has many dimensions attr ` dims ` specifies For example attr ` input ` has shape attr ` dims ` then attr ` input ` treated had shape note This function similar NumPy s tile function Args input Tensor tensor whose elements repeat dims tuple number repetitions per dimension Example x = torch tensor x tile tensor y = torch tensor torch tile y tensor add_docstr torch quantize_per_tensor r quantize_per_tensor input scale zero_point dtype - Tensor Converts float tensor quantized tensor given scale zero point Arguments input Tensor float tensor list tensors quantize scale float Tensor scale apply quantization formula zero_point int Tensor offset integer value maps float zero dtype ` torch dtype ` desired data type returned tensor Has one quantized dtypes ` ` torch quint ` ` ` ` torch qint ` ` ` ` torch qint ` ` Returns Tensor A newly quantized tensor list quantized tensors Example torch quantize_per_tensor torch tensor - torch quint tensor - size= dtype=torch quint quantization_scheme=torch per_tensor_affine scale= zero_point= torch quantize_per_tensor torch tensor - torch quint int_repr tensor dtype=torch uint torch quantize_per_tensor torch tensor - torch tensor - torch tensor torch tensor torch quint tensor - size= dtype=torch quint quantization_scheme=torch per_tensor_affine scale= zero_point= tensor - size= dtype=torch quint quantization_scheme=torch per_tensor_affine scale= zero_point= torch quantize_per_tensor torch tensor - torch tensor torch tensor torch quint tensor - size= dtype=torch quint quantization_scheme=torch per_tensor_affine scale= zero_point= add_docstr torch quantize_per_tensor_dynamic r quantize_per_tensor_dynamic input dtype reduce_range - Tensor Converts float tensor quantized tensor scale zero_point calculated dynamically based input Arguments input Tensor float tensor list tensors quantize dtype ` torch dtype ` desired data type returned tensor Has one quantized dtypes ` ` torch quint ` ` ` ` torch qint ` ` reduce_range bool flag indicate whether reduce range quantized data bit s required avoid instruction overflow some hardwares Returns Tensor A newly dynamically quantized tensor Example t = torch quantize_per_tensor_dynamic torch tensor - torch quint False print t tensor - size= dtype=torch quint quantization_scheme=torch per_tensor_affine scale= zero_point= t int_repr tensor dtype=torch uint add_docstr torch quantize_per_channel r quantize_per_channel input scales zero_points axis dtype - Tensor Converts float tensor per-channel quantized tensor given scales zero points Arguments input Tensor float tensor quantize scales Tensor float D tensor scales use size should match ` ` input size axis ` ` zero_points int integer D tensor offset use size should match ` ` input size axis ` ` axis int dimension which apply per-channel quantization dtype ` torch dtype ` desired data type returned tensor Has one quantized dtypes ` ` torch quint ` ` ` ` torch qint ` ` ` ` torch qint ` ` Returns Tensor A newly quantized tensor Example x = torch tensor - torch quantize_per_channel x torch tensor torch tensor torch quint tensor - size= dtype=torch quint quantization_scheme=torch per_channel_affine scale=tensor dtype=torch float zero_point=tensor axis= torch quantize_per_channel x torch tensor torch tensor torch quint int_repr tensor dtype=torch uint add_docstr torch quantized_batch_norm r quantized_batch_norm input weight=None bias=None mean var eps output_scale output_zero_point - Tensor Applies batch normalization D NCHW quantized tensor math y = \frac x - \mathrm E x \sqrt \mathrm Var x + \epsilon \gamma + \beta Arguments input Tensor quantized tensor weight Tensor float tensor corresponds gamma size C bias Tensor float tensor corresponds beta size C mean Tensor float mean value batch normalization size C var Tensor float tensor variance size C eps float value added denominator numerical stability output_scale float output quantized tensor scale output_zero_point int output quantized tensor zero_point Returns Tensor A quantized tensor batch normalization applied Example qx = torch quantize_per_tensor torch rand torch quint torch quantized_batch_norm qx torch ones torch zeros torch rand torch rand tensor - - - - - - - - - - - - size= dtype=torch quint quantization_scheme=torch per_tensor_affine scale= zero_point= add_docstr torch quantized_max_pool d r quantized_max_pool d input kernel_size stride= padding= dilation= ceil_mode=False - Tensor Applies D max pooling over input quantized tensor composed several input planes Arguments input Tensor quantized tensor kernel_size list int size sliding window stride ` ` list int ` ` optional stride sliding window padding ` ` list int ` ` optional padding added both sides must = = kernel_size dilation ` ` list int ` ` optional The stride between elements within sliding window must Default ceil_mode bool optional If True will use ceil instead floor compute output shape Defaults False Returns Tensor A quantized tensor max_pool d applied Example qx = torch quantize_per_tensor torch rand torch quint torch quantized_max_pool d qx tensor size= dtype=torch quint quantization_scheme=torch per_tensor_affine scale= zero_point= add_docstr torch quantized_max_pool d r quantized_max_pool d input kernel_size stride= padding= dilation= ceil_mode=False - Tensor Applies D max pooling over input quantized tensor composed several input planes Arguments input Tensor quantized tensor kernel_size ` ` list int ` ` size sliding window stride ` ` list int ` ` optional stride sliding window padding ` ` list int ` ` optional padding added both sides must = = kernel_size dilation ` ` list int ` ` optional The stride between elements within sliding window must Default ceil_mode bool optional If True will use ceil instead floor compute output shape Defaults False Returns Tensor A quantized tensor max_pool d applied Example qx = torch quantize_per_tensor torch rand torch quint torch quantized_max_pool d qx tensor size= dtype=torch quint quantization_scheme=torch per_tensor_affine scale= zero_point= add_docstr torch Stream r Stream device priority - Stream An in-order queue executing respective tasks asynchronously first first out FIFO order It can control synchronize execution other Stream block current host thread ensure correct task sequencing It supports statement context manager ensure operators within block running corresponding stream See in-depth description CUDA behavior ref ` cuda-semantics ` details exact semantic applies all devices Arguments device ` torch device ` optional desired device Stream If given current ref ` accelerator accelerators ` type will used priority int optional priority stream should negative where negative numbers indicate higher priority By default streams have priority Returns Stream An torch Stream object Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA torch Stream device= cuda s_cuda = torch randn device= cuda b = torch randn device= cuda c = torch mm b add_docstr torch Stream query r Stream query - bool Check all work submitted has been completed Returns bool A boolean indicating all kernels stream completed Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA s_cuda = torch Stream device= cuda s_cuda query True add_docstr torch Stream record_event r Stream record_event event - Event Record event En-queuing into Stream allow further synchronization current point FIFO queue Arguments event ` torch Event ` optional event record If given new one will allocated Returns Event Recorded event Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA s_cuda = torch Stream device= cuda e_cuda = s_cuda record_event add_docstr torch Stream synchronize r Stream synchronize - None Wait all kernels stream complete Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA s_cuda = torch Stream device= cuda s_cuda synchronize add_docstr torch Stream wait_event r Stream wait_event event - None Make all future work submitted stream wait event Arguments event ` torch Event ` event wait Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA s _cuda = torch Stream device= cuda s _cuda = torch Stream device= cuda e_cuda = s _cuda record_event s _cuda wait_event e_cuda add_docstr torch Stream wait_stream r Stream wait_stream stream - None Synchronize another stream All future work submitted stream will wait until all kernels already submitted given stream completed Arguments stream ` torch Stream ` stream synchronize Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA s _cuda = torch Stream device= cuda s _cuda = torch Stream device= cuda s _cuda wait_stream s _cuda add_docstr torch Event r Event device=None enable_timing=False blocking=False interprocess=False Query record Stream status identify control dependencies across Stream measure timing Arguments device ` torch device ` optional desired device Event If given current ref ` accelerator accelerators ` type will used enable_timing bool optional indicates event should measure time default ` ` False ` ` blocking bool optional ` ` True ` ` meth ` wait ` will blocking default ` ` False ` ` interprocess bool ` ` True ` ` event can shared between processes default ` ` False ` ` warning Both blocking interprocess supported right now noops Returns Event An torch Event object Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA event = torch Event e_cuda = torch Event device= cuda add_docstr torch Event elapsed_time r Event elapsed_time end_event - float Returns elapsed time milliseconds between when event attr ` end_event ` each recorded via func ` torch Stream record_event ` Arguments end_event ` torch Event ` The ending event has been recorded Returns float Time between starting ending event milliseconds Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA s_cuda = torch Stream device= cuda e _cuda = s_cuda record_event e _cuda = s_cuda record_event ms = e _cuda elapsed_time e _cuda add_docstr torch Event query r Event query - bool Check stream where event recorded already moved past point where event recorded Always returns ` ` True ` ` Event recorded Returns bool A boolean indicating all work currently captured event has completed Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA s_cuda = torch Stream device= cuda e_cuda = s_cuda record_event e_cuda query True add_docstr torch Event record r Event record stream=None - None Record event given stream The stream s device must match event s device This function equivalent ` ` stream record_event ` ` Arguments stream ` torch Stream ` optional A stream recorded If given current stream will used Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA e_cuda = torch Event device= cuda e_cuda record add_docstr torch Event synchronize r Event synchronize - None Wait event complete This prevents CPU thread proceeding until event completes Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA s_cuda = torch Stream device= cuda e_cuda = s_cuda record_event e_cuda synchronize add_docstr torch Event wait r Event wait stream=None - None Make all future work submitted given stream wait event Arguments stream ` torch Stream ` optional A stream synchronize If given current stream will used Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA s _cuda = torch Stream device= cuda s _cuda = torch Stream device= cuda e_cuda = s _cuda record e_cuda wait s add_docstr torch Generator r Generator device= cpu - Generator Creates returns generator object manages state algorithm which produces pseudo random numbers Used keyword argument many ref ` inplace-random-sampling ` functions Arguments device ` torch device ` optional desired device generator Returns Generator An torch Generator object Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA g_cpu = torch Generator g_cuda = torch Generator device= cuda add_docstr torch Generator set_state r Generator set_state new_state - void Sets Generator state Arguments new_state torch ByteTensor The desired state Example g_cpu = torch Generator g_cpu_other = torch Generator g_cpu set_state g_cpu_other get_state add_docstr torch Generator get_state r Generator get_state - Tensor Returns Generator state ` ` torch ByteTensor ` ` Returns Tensor A ` ` torch ByteTensor ` ` which contains all necessary bits restore Generator specific point time Example g_cpu = torch Generator g_cpu get_state add_docstr torch Generator graphsafe_set_state r Generator graphsafe_set_state state - None Sets state generator specified state manner safe use graph capture This method crucial ensuring generator s state can captured CUDA graph Arguments state torch Generator A Generator point new state generator typically obtained ` graphsafe_get_state ` Example g_cuda = torch Generator device= cuda g_cuda_other = torch Generator device= cuda current_state = g_cuda_other graphsafe_get_state g_cuda graphsafe_set_state current_state add_docstr torch Generator graphsafe_get_state r Generator graphsafe_get_state - torch Generator Retrieves current state generator manner safe graph capture This method crucial ensuring generator s state can captured CUDA graph Returns torch Generator A Generator point current state generator Example g_cuda = torch Generator device= cuda current_state = g_cuda graphsafe_get_state add_docstr torch Generator clone_state r Generator clone_state - torch Generator Clones current state generator returns new generator pointing cloned state This method beneficial preserving particular state generator restore later point Returns torch Generator A Generator pointing newly cloned state Example g_cuda = torch Generator device= cuda cloned_state = g_cuda clone_state add_docstr torch Generator manual_seed r Generator manual_seed seed - Generator Sets seed generating random numbers Returns ` torch Generator ` object Any -bit integer valid seed Arguments seed int The desired seed Value must within inclusive range ` - x _ _ _ xffff_ffff_ffff_ffff ` Otherwise RuntimeError raised Negative inputs remapped positive values formula ` xffff_ffff_ffff_ffff + seed ` Returns Generator An torch Generator object Example g_cpu = torch Generator g_cpu manual_seed add_docstr torch Generator initial_seed r Generator initial_seed - int Returns initial seed generating random numbers Example g_cpu = torch Generator g_cpu initial_seed add_docstr torch Generator seed r Generator seed - int Gets non-deterministic random number std random_device current time uses seed Generator Example g_cpu = torch Generator g_cpu seed add_docstr torch Generator device r Generator device - device Gets current device generator Example g_cpu = torch Generator g_cpu device device type= cpu add_docstr torch _assert_async r _assert_async tensor - void Asynchronously assert contents tensor nonzero For CPU tensors equivalent ` ` assert tensor ` ` ` ` assert tensor is_nonzero ` ` CUDA tensors we DO NOT synchronize you may only find out assertion failed later CUDA kernel launch Asynchronous assertion can helpful testing invariants CUDA tensors without giving up performance This function NOT intended used regular error checking will trash your CUDA context assert fails forcing you restart your PyTorch process Args tensor Tensor one element tensor test see nonzero Zero elements including False boolean tensors cause assertion failure raised add_docstr torch searchsorted r searchsorted sorted_sequence values out_int =False right=False side=None out=None sorter=None - Tensor Find indices innermost dimension attr ` sorted_sequence ` such corresponding values attr ` values ` inserted before indices when sorted order corresponding innermost dimension within attr ` sorted_sequence ` would preserved Return new tensor same size attr ` values ` More formally returned index satisfies following rules list-table widths header-rows - attr ` sorted_sequence ` - attr ` right ` - returned index satisfies - -D - False - ` ` sorted_sequence i- values m n l x = sorted_sequence i ` ` - -D - True - ` ` sorted_sequence i- = values m n l x sorted_sequence i ` ` - N-D - False - ` ` sorted_sequence m n l i- values m n l x = sorted_sequence m n l i ` ` - N-D - True - ` ` sorted_sequence m n l i- = values m n l x sorted_sequence m n l i ` ` Args sorted_sequence Tensor N-D -D tensor containing monotonically increasing sequence innermost dimension unless attr ` sorter ` provided which case sequence does need sorted values Tensor Scalar N-D tensor Scalar containing search value s Keyword args out_int bool optional indicate output data type torch int True torch int otherwise Default value False i e default output data type torch int right bool optional False first suitable location found If True last such index If no suitable index found non-numerical value eg nan inf size innermost dimension within attr ` sorted_sequence ` one pass last index innermost dimension In other words False gets lower bound index each value attr ` values ` corresponding innermost dimension attr ` sorted_sequence ` If True gets upper bound index instead Default value False attr ` side ` does same preferred It will error attr ` side ` set left while True side str optional same attr ` right ` preferred left corresponds False attr ` right ` right corresponds True attr ` right ` It will error set left while attr ` right ` True Default value None out Tensor optional output tensor must same size attr ` values ` provided sorter LongTensor optional provided tensor matching shape unsorted attr ` sorted_sequence ` containing sequence indices sort ascending order innermost dimension Example sorted_sequence = torch tensor sorted_sequence tensor values = torch tensor values tensor torch searchsorted sorted_sequence values tensor torch searchsorted sorted_sequence values side= right tensor sorted_sequence_ d = torch tensor sorted_sequence_ d tensor torch searchsorted sorted_sequence_ d values tensor add_docstr torch bucketize r bucketize input boundaries out_int =False right=False out=None - Tensor Returns indices buckets which each value attr ` input ` belongs where boundaries buckets set attr ` boundaries ` Return new tensor same size attr ` input ` If attr ` right ` False default then left boundary open Note behavior opposite behavior ` numpy digitize https numpy org doc stable reference generated numpy digitize html ` _ More formally returned index satisfies following rules list-table widths header-rows - attr ` right ` - returned index satisfies - False - ` ` boundaries i- input m n l x = boundaries i ` ` - True - ` ` boundaries i- = input m n l x boundaries i ` ` Args input Tensor Scalar N-D tensor Scalar containing search value s boundaries Tensor -D tensor must contain strictly increasing sequence value undefined Keyword args out_int bool optional indicate output data type torch int True torch int otherwise Default value False i e default output data type torch int right bool optional determines behavior values attr ` boundaries ` See table above out Tensor optional output tensor must same size attr ` input ` provided Example boundaries = torch tensor boundaries tensor v = torch tensor v tensor torch bucketize v boundaries tensor torch bucketize v boundaries right=True tensor add_docstr torch view_as_real_copy r Performs same operation func ` torch view_as_real ` all output tensors freshly created instead aliasing input add_docstr torch view_as_complex_copy r Performs same operation func ` torch view_as_complex ` all output tensors freshly created instead aliasing input add_docstr torch as_strided_copy r Performs same operation func ` torch as_strided ` all output tensors freshly created instead aliasing input add_docstr torch diagonal_copy r Performs same operation func ` torch diagonal ` all output tensors freshly created instead aliasing input add_docstr torch expand_copy r Performs same operation func ` torch Tensor expand ` all output tensors freshly created instead aliasing input add_docstr torch permute_copy r Performs same operation func ` torch permute ` all output tensors freshly created instead aliasing input add_docstr torch select_copy r Performs same operation func ` torch select ` all output tensors freshly created instead aliasing input add_docstr torch detach_copy r Performs same operation func ` torch detach ` all output tensors freshly created instead aliasing input add_docstr torch slice_copy r Performs same operation func ` torch slice ` all output tensors freshly created instead aliasing input add_docstr torch split_copy r Performs same operation func ` torch split ` all output tensors freshly created instead aliasing input add_docstr torch split_with_sizes_copy r Performs same operation func ` torch split_with_sizes ` all output tensors freshly created instead aliasing input add_docstr torch squeeze_copy r Performs same operation func ` torch squeeze ` all output tensors freshly created instead aliasing input add_docstr torch t_copy r Performs same operation func ` torch t ` all output tensors freshly created instead aliasing input add_docstr torch transpose_copy r Performs same operation func ` torch transpose ` all output tensors freshly created instead aliasing input add_docstr torch unsqueeze_copy r Performs same operation func ` torch unsqueeze ` all output tensors freshly created instead aliasing input add_docstr torch indices_copy r Performs same operation func ` torch indices ` all output tensors freshly created instead aliasing input add_docstr torch values_copy r Performs same operation func ` torch values ` all output tensors freshly created instead aliasing input add_docstr torch crow_indices_copy r Performs same operation func ` torch crow_indices ` all output tensors freshly created instead aliasing input add_docstr torch col_indices_copy r Performs same operation func ` torch col_indices ` all output tensors freshly created instead aliasing input add_docstr torch unbind_copy r Performs same operation func ` torch unbind ` all output tensors freshly created instead aliasing input add_docstr torch view_copy r Performs same operation func ` torch view ` all output tensors freshly created instead aliasing input add_docstr torch unfold_copy r Performs same operation func ` torch unfold ` all output tensors freshly created instead aliasing input add_docstr torch alias_copy r Performs same operation func ` torch alias ` all output tensors freshly created instead aliasing input unary_base_func_name exp sqrt abs acos asin atan ceil cos cosh erf erfc expm floor log log log p log neg tan tanh sin sinh round lgamma frac reciprocal sigmoid trunc zero unary_foreach_func_name = f _foreach_ unary_base_func_name hasattr torch unary_foreach_func_name add_docstr getattr torch unary_foreach_func_name rf unary_foreach_func_name List Tensor - List Tensor Apply func ` torch unary_base_func_name ` each Tensor input list unary_inplace_foreach_func_name = f unary_foreach_func_name _ hasattr torch unary_inplace_foreach_func_name add_docstr getattr torch unary_inplace_foreach_func_name rf unary_inplace_foreach_func_name List Tensor - None Apply func ` torch unary_base_func_name ` each Tensor input list