Owner s oncall distributed copy os typing TYPE_CHECKING torch torch distributed checkpoint dcp torch nn nn torch nn functional F torch distributed _composable replicate_with_fsdp replicate torch distributed checkpoint FileSystemReader torch distributed checkpoint default_planner _EmptyStateDictLoadPlanner torch distributed checkpoint state_dict get_state_dict set_state_dict torch distributed checkpoint state_dict_loader _load_state_dict torch distributed checkpoint stateful Stateful torch distributed device_mesh DeviceMesh init_device_mesh torch distributed fsdp fully_shard MixedPrecisionPolicy torch distributed pipelining PipelineStage torch distributed pipelining schedules PipelineScheduleSingle Schedule F B ScheduleGPipe ScheduleInterleaved F B ScheduleInterleavedZeroBubble ScheduleLoopedBFS torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel torch testing _internal common_cuda TEST_MULTIGPU torch testing _internal common_distributed MultiProcessTestCase requires_accelerator_dist_backend skip_if_lt_x_gpu torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests skip_but_pass_in_sandcastle_if TEST_XPU torch testing _internal distributed checkpoint_utils with_temp_dir TYPE_CHECKING torch distributed checkpoint metadata STATE_DICT_TYPE device_type = acc type acc = torch accelerator current_accelerator cpu backend = torch distributed get_default_backend_for_device device_type MLP Layer MLPModule torch nn Module __init__ d_hid int super __init__ net = torch nn Linear d_hid d_hid relu = torch nn ReLU net = torch nn Linear d_hid d_hid forward x x = net x x = relu x x = net x x MLPModuleEven torch nn Module __init__ d_hid int super __init__ net = nn Linear d_hid d_hid net = nn Linear d_hid d_hid net = nn Linear d_hid d_hid forward x x = F relu net x x = F relu net x x = F relu net x x ComposabilityTest MultiProcessTestCase classmethod backend_str cls - str Testing NCCL backend backend setUp super setUp _spawn_processes tearDown super tearDown try os remove file_name except OSError pass property world_size property device rank requires_accelerator_dist_backend nccl xccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if TEST_MULTIGPU TEST_XPU Test requires + GPUs test_pp_and_dcp Test pipeline parallelism distributed checkpointing can used together saved correct FQNs AppState Stateful __init__ model optimizer model = model optimizer = optimizer state_dict line automatically manages FSDP FQN s well sets default state dict type FSDP SHARDED_STATE_DICT model_state_dict optimizer_state_dict = get_state_dict model optimizer model model_state_dict optim optimizer_state_dict load_state_dict state_dict sets our state dicts model optimizer now we ve loaded set_state_dict model optimizer model_state_dict=state_dict model optim_state_dict=state_dict optim PPModelChunk nn Module __init__ layers nn ModuleDict start_index int end_index int super __init__ Filter layers based start_index end_index layers = nn ModuleDict str i layers str i i range start_index end_index forward x layer layers values x = layer x x device = torch device device_type device torch accelerator set_device_index device store = torch distributed FileStore file_name world_size torch distributed init_process_group backend=backend store=store rank=self rank world_size=self world_size device_id=device create entire model total_layers = dim = full_model = nn ModuleDict f i MLPModule dim i range total_layers Calculate start end indices based rank start_index = rank end_index = start_index + pp_model = PPModelChunk full_model start_index end_index pp_model device opt = torch optim Adam pp_model parameters lr= perform work temp dir cleaned up after test with_temp_dir _dcp_test state_dict = app AppState pp_model opt dcp save state_dict checkpoint_id=self temp_dir temp checkpoint sd STATE_DICT_TYPE = _load_state_dict sd storage_reader=FileSystemReader temp_dir planner=_EmptyStateDictLoadPlanner Check parameter names sd compare pp_model pp_model_param_names = set pp_model state_dict keys sd_param_names = set sd app model keys Verify each parameter name pp_model contained sd param_name pp_model_param_names assertIn param_name sd_param_names f Parameter name param_name found state_dict _dcp_test requires_accelerator_dist_backend nccl xccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if TEST_MULTIGPU TEST_XPU Test requires + GPUs parametrize ScheduleClass ScheduleGPipe Schedule F B ScheduleInterleaved F B ScheduleLoopedBFS ScheduleInterleavedZeroBubble parametrize MixedPrecisionParam torch bfloat torch float test_ d_with_tp_dp_pp ScheduleClass MixedPrecisionParam torch accelerator set_device_index device store = torch distributed FileStore file_name world_size torch distributed init_process_group backend=backend store=store rank=self rank world_size=self world_size dim = tp_size = pp_size = num_microbatches = dp_size = world_size tp_size pp_size device_mesh = init_device_mesh device_type mesh_shape= dp_size pp_size tp_size mesh_dim_names= dp pp tp dp_mesh = device_mesh dp tp_mesh = device_mesh tp pp_mesh = device_mesh pp pp_group = device_mesh pp get_group create entire model total_layers = full_model = nn ModuleList MLPModuleEven dim _ range total_layers dummy loss needed just force backwards run schedule step loss_fn y target y sum Apply DP stage module apply_fsdp partial_model apply FSDP mp_policy = MixedPrecisionPolicy param_dtype=MixedPrecisionParam reduce_dtype=torch float fsdp_config = mesh dp_mesh mp_policy mp_policy layer_id range len partial_model fully_shard partial_model layer_id fsdp_config reshard_after_forward=False dp_model = fully_shard partial_model fsdp_config dp_model apply_tp model nn Module tp_mesh DeviceMesh parallelize_plan = net ColwiseParallel net RowwiseParallel net ColwiseParallel layer model parallelize_module layer tp_mesh parallelize_plan model issubclass ScheduleClass PipelineScheduleSingle n_virtual = n_virtual = num_stages = pp_group size n_virtual layers_per_stage = total_layers num_stages stages = i range n_virtual stage_idx = pp_group rank + pp_group size i start_layer = stage_idx layers_per_stage end_layer = start_layer + layers_per_stage divide model layers number stages partial_model = nn Sequential full_model start_layer end_layer partial_model device tp_model = apply_tp partial_model tp_mesh dp_model = apply_fsdp tp_model stage = PipelineStage dp_model stage_idx num_stages device group=pp_group stages append stage partial_models = pipeline_stage submod pipeline_stage stages issubclass ScheduleClass PipelineScheduleSingle stages = stages pipeline_schedule = ScheduleClass stages n_microbatches=num_microbatches loss_fn=loss_fn scale_grads=False optimizer_kwargs = lr betas weight_decay fused False foreach True optimizers = torch optim AdamW model parameters optimizer_kwargs model partial_models _train_step range optimizer optimizers optimizer zero_grad inputs = torch rand num_microbatches dim device=self device labels = torch rand num_microbatches dim device=self device is_last_stage = pp_mesh get_local_rank == pp_mesh size - pp_mesh get_local_rank == pipeline_schedule step inputs is_last_stage losses = pipeline_schedule step target=labels losses=losses pipeline_schedule step optimizer optimizers optimizer step torch distributed destroy_process_group requires_accelerator_dist_backend nccl xccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if TEST_MULTIGPU TEST_XPU Test requires + GPUs parametrize ScheduleClass ScheduleGPipe Schedule F B ScheduleInterleaved F B ScheduleLoopedBFS ScheduleInterleavedZeroBubble parametrize MixedPrecisionParam torch bfloat torch float test_replicate_pp ScheduleClass MixedPrecisionParam torch accelerator set_device_index device store = torch distributed FileStore file_name world_size torch distributed init_process_group backend=backend store=store rank=self rank world_size=self world_size dim = pp_size = num_microbatches = replicate_size = world_size pp_size device_mesh = init_device_mesh device_type mesh_shape= replicate_size pp_size mesh_dim_names= replicate shard pp torch manual_seed dp_mesh = device_mesh replicate shard pp_mesh = device_mesh pp pp_group = device_mesh pp get_group create entire model total_layers = full_model = nn ModuleList MLPModule dim _ range total_layers ref_full_model = copy deepcopy full_model dummy loss needed just force backwards run schedule step loss_fn y target y sum Apply DP stage module apply_replicate partial_model apply replicate mp_policy = MixedPrecisionPolicy param_dtype=MixedPrecisionParam reduce_dtype=torch float replicate_config = mp_policy mp_policy layer_id range len partial_model replicate partial_model layer_id device_mesh=dp_mesh replicate_config reshard_after_forward=False dp_model = replicate partial_model device_mesh=dp_mesh replicate_config dp_model Apply same precision reference model without replicate apply_same_precision partial_model MixedPrecisionParam = torch float Cast same precision pipeline model partial_model = partial_model dtype=MixedPrecisionParam partial_model issubclass ScheduleClass PipelineScheduleSingle n_virtual = n_virtual = num_stages = pp_group size n_virtual layers_per_stage = total_layers num_stages stages = ref_stages = i range n_virtual stage_idx = pp_group rank + pp_group size i start_layer = stage_idx layers_per_stage end_layer = start_layer + layers_per_stage divide model layers number stages partial_model = nn Sequential full_model start_layer end_layer partial_model device ref_partial_model = nn Sequential ref_full_model start_layer end_layer ref_partial_model device dp_model = apply_replicate partial_model ref_dp_model = apply_same_precision ref_partial_model stage = PipelineStage dp_model stage_idx num_stages device group=pp_group ref_stage = PipelineStage ref_dp_model stage_idx num_stages device group=pp_group stages append stage ref_stages append ref_stage partial_models = pipeline_stage submod pipeline_stage stages ref_partial_models = pipeline_stage submod pipeline_stage ref_stages issubclass ScheduleClass PipelineScheduleSingle stages = stages ref_stages = ref_stages pipeline_schedule = ScheduleClass stages n_microbatches=num_microbatches loss_fn=loss_fn scale_grads=False ref_pipeline_schedule = ScheduleClass ref_stages n_microbatches=num_microbatches loss_fn=loss_fn scale_grads=False optimizer_kwargs = lr betas weight_decay fused False foreach True optimizers = torch optim AdamW model parameters optimizer_kwargs model partial_models ref_optimizers = torch optim AdamW model parameters optimizer_kwargs model ref_partial_models _train_step range optimizer optimizers optimizer zero_grad ref_optimizer ref_optimizers ref_optimizer zero_grad inputs = torch rand num_microbatches dim device=self device dtype=MixedPrecisionParam labels = torch rand num_microbatches dim device=self device dtype=MixedPrecisionParam is_last_stage = pp_mesh get_local_rank == pp_mesh size - pp_mesh get_local_rank == pipeline_schedule step inputs ref_pipeline_schedule step inputs is_last_stage losses = ref_losses = pipeline_schedule step target=labels losses=losses ref_pipeline_schedule step target=labels losses=ref_losses loss ref_loss zip losses ref_losses assertEqual loss ref_loss pipeline_schedule step ref_pipeline_schedule step optimizer optimizers optimizer step ref_optimizer ref_optimizers ref_optimizer step torch distributed destroy_process_group requires_accelerator_dist_backend nccl xccl skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if TEST_MULTIGPU TEST_XPU Test requires + GPUs parametrize ScheduleClass ScheduleGPipe Schedule F B ScheduleInterleaved F B ScheduleLoopedBFS ScheduleInterleavedZeroBubble test_replicate_pp_grads ScheduleClass torch accelerator set_device_index device store = torch distributed FileStore file_name world_size torch distributed init_process_group backend=backend store=store rank=self rank world_size=self world_size dim = pp_size = num_microbatches = replicate_size = world_size pp_size device_mesh = init_device_mesh device_type mesh_shape= replicate_size pp_size mesh_dim_names= replicate shard pp torch manual_seed dp_mesh = device_mesh replicate shard pp_mesh = device_mesh pp pp_group = device_mesh pp get_group dp_group = device_mesh replicate get_group create entire model total_layers = full_model = nn ModuleList MLPModule dim _ range total_layers ref_model = nn Sequential copy deepcopy full_model device dummy loss needed just force backwards run schedule step loss_fn y target y sum Simulate microbatch processing reference model simulate_stage_forward_backward model inputs labels Simulate forward backward passes through stages microbatch processing batch_size _ = inputs shape total_loss = Split inputs into microbatches microbatch_size = batch_size num_microbatches mb_idx range num_microbatches start_idx = mb_idx microbatch_size end_idx = start_idx + microbatch_size mb_input = inputs start_idx end_idx mb_label = labels start_idx end_idx labels None None Simulate stage-by-stage processing issubclass ScheduleClass PipelineScheduleSingle num_stages = pp_group size layers_per_stage = total_layers pp_group size = n_virtual = num_stages = pp_group size n_virtual layers_per_stage = total_layers num_stages Forward pass through all stages x = mb_input stage range num_stages start_layer = stage layers_per_stage end_layer = start_layer + layers_per_stage Process layers stage layer_idx range start_layer min end_layer len model x = model layer_idx x mb_loss = loss_fn x mb_label total_loss += mb_loss Backward pass mb_loss backward total_loss num_microbatches Apply replicate stage module apply_replicate partial_model layer_id range len partial_model replicate partial_model layer_id device_mesh=dp_mesh reshard_after_forward=False dp_model = replicate partial_model device_mesh=dp_mesh dp_model pipelined_models_parameters start_layer model layer_idx = start_layer layer model children name param layer named_parameters updated_param_name = f layer_idx name pipeline_model_parameter_dict updated_param_name = param layer_idx += check_gradient_parity pipeline_model_parameter_dict ref_model_parameter_dict parameter pipeline_model_parameter_dict assert parameter ref_model_parameter_dict pipeline_parameter = pipeline_model_parameter_dict parameter pipeline_parameter grad None pipeline_parameter_grad = pipeline_parameter grad to_local ref_parameter = ref_model_parameter_dict parameter ref_parameter grad None torch testing assert_close pipeline_parameter_grad ref_parameter grad rtol= e- atol= e- assert pipeline_parameter grad None pipeline_model_parameter_dict = issubclass ScheduleClass PipelineScheduleSingle n_virtual = n_virtual = num_stages = pp_group size n_virtual layers_per_stage = total_layers num_stages stages = i range n_virtual stage_idx = pp_group rank + pp_group size i start_layer = stage_idx layers_per_stage end_layer = start_layer + layers_per_stage divide model layers number stages partial_model = nn Sequential full_model start_layer end_layer partial_model device dp_model = apply_replicate partial_model pipelined_models_parameters start_layer dp_model stage = PipelineStage dp_model stage_idx num_stages device group=pp_group stages append stage partial_models = pipeline_stage submod pipeline_stage stages issubclass ScheduleClass PipelineScheduleSingle stages = stages pipeline_schedule = ScheduleClass stages n_microbatches=num_microbatches loss_fn=loss_fn scale_grads=False optimizer_kwargs = lr betas weight_decay fused False foreach True optimizers = torch optim AdamW model parameters optimizer_kwargs model partial_models ref_optimizer = torch optim AdamW ref_model parameters optimizer_kwargs Helper function simulate all-reduce reference model gradients simulate_all_reduce_grads model group Simulate all-reduce operation gradients like replicate does param model parameters param grad None Scale number replicas like replicate does param grad div_ group size Simulate all-reduce torch distributed all_reduce param grad group=group ref_model_parameter_dict = ref_model_parameter_dict = dict ref_model named_parameters torch manual_seed + rank _ range optimizer optimizers optimizer zero_grad ref_optimizer zero_grad inputs = torch rand num_microbatches dim device=self device labels = torch rand num_microbatches dim device=self device Ensure all ranks use same inputs labels comparison torch distributed broadcast inputs torch distributed broadcast labels is_last_stage = pp_mesh get_local_rank == pp_mesh size - Run pipeline schedule pp_mesh get_local_rank == pipeline_schedule step inputs is_last_stage losses = pipeline_schedule step target=labels losses=losses pipeline_schedule step Run reference model simulation is_last_stage ref_loss = simulate_stage_forward_backward ref_model inputs labels Simulate all-reduce reference model gradients simulate_all_reduce_grads ref_model dp_group Compare losses - only check last stage where we have losses losses locals len losses Average microbatch losses match ref_loss avg_pipeline_loss = sum losses len losses torch testing assert_close avg_pipeline_loss ref_loss rtol= e- atol= e- For non-last stages still run ref model generate gradients simulate_stage_forward_backward ref_model inputs None simulate_all_reduce_grads ref_model dp_group Step optimizers optimizer optimizers optimizer step ref_optimizer step check_gradient_parity pipeline_model_parameter_dict ref_model_parameter_dict torch distributed destroy_process_group instantiate_parametrized_tests ComposabilityTest __name__ == __main__ run_tests