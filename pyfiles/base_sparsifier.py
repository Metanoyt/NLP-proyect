mypy allow-untyped-defs abc copy collections defaultdict typing Any Optional torch torch nn torch nn utils parametrize torch nn utils parametrize type_before_parametrizations utils FakeSparsity get_arg_info_from_tensor_fqn module_contains_param module_to_fqn swap_module __all__ = BaseSparsifier SUPPORTED_MODULES = nn Linear KEYS_NOT_IN_STATE_DICT = module module_fqn tensor_name TODO update desc new config args BaseSparsifier abc ABC r Base all sparsifiers Abstract methods need implemented - update_mask Function compute new mask all keys ` groups ` Args - model nn Module model configure The model itself saved used state_dict saving loading - config list configuration elements should dict map includes ` tensor_fqn ` tensors sparsify - defaults dict default configurations will attached configuration Only keys don t exist ` config ` will updated Example xdoctest +SKIP Can t instantiate abstract BaseSparsifier abstract method update_mask config = tensor_fqn layer weight tensor_fqn linear weight sparsity_level defaults = sparsity_level model layer weight will have ` sparsity_level ` = getting default sparsifier = BaseSparsifier config defaults __init__ defaults Optional dict str Any = None super __init__ defaults dict str Any = defaults state dict str dict = defaultdict dict groups list dict str Any = enable_mask_update = True __getstate__ - dict str Any defaults defaults state state groups groups __setstate__ state dict str dict str Any - None __dict__ update state __repr__ format_string = __class__ __name__ + i sparse_args enumerate groups module = sparse_args module format_string += \n format_string += f \tGroup i \n format_string += f \t module module \n key sorted sparse_args keys key == module continue format_string += f \t key sparse_args key \n format_string += format_string state_dict - dict str Any r Returns state optimizer ` dict ` It contains state - current state sparsification groups - list containing all sparsity configuration groups key tensor_fqn specifying path sparsified tensor within model TODO Need clean way loading state prepared module groups list dict str Any = dict filter lambda key_value key_value KEYS_NOT_IN_STATE_DICT mg items mg groups state state groups groups load_state_dict state_dict dict str Any strict bool = True groups = copy deepcopy state_dict groups states = state_dict state tensor_fqn s states items arg_info = get_arg_info_from_tensor_fqn model tensor_fqn module = arg_info module tensor_name = arg_info tensor_name strict module None raise RuntimeError f Error loading tensor_fqn into model found = False p module parametrizations tensor_name isinstance p FakeSparsity found = True break found p = FakeSparsity torch ones getattr module tensor_name shape parametrize register_parametrization module tensor_name p s get mask None None mask = s pop mask p mask = mask mg groups mg tensor_fqn == tensor_fqn mg update arg_info __setstate__ state states groups groups make_config_from_model model nn Module SUPPORTED_MODULES set type nn Linear = SUPPORTED_MODULES - None config = stack = model while stack module = stack pop _name child module named_children type child SUPPORTED_MODULES module_fqn = module_to_fqn model child isinstance module_fqn str raise AssertionError module_fqn must string config append tensor_fqn module_fqn + weight stack append child prepare model config r Prepares model adding parametrizations Note The model modified inplace If you need preserve original model use copy deepcopy model = model TODO Need figure out how load without config = config If no config -- try getting all supported layers config None make_config_from_model model TODO Remove configuration reference module pyrefly ignore not-iterable module_config config isinstance module_config dict raise AssertionError config elements should dicts modules i e ` tensor_fqn ` ` foo bar weight ` ` tensor_fqn ` isinstance defaults dict raise AssertionError defaults must dict local_args = copy deepcopy defaults local_args update module_config tensor_fqn = local_args get tensor_fqn None tensor_fqn None raise AssertionError tensor_fqn required argument sparsity config which replaces previous ` module ` module ` fqn ` arguments populate all information tensor_fqn info_from_tensor_fqn = get_arg_info_from_tensor_fqn model tensor_fqn check whatever put into local_args agrees what obtained tensor_fqn key info_from_tensor_fqn keys key local_args info_from_tensor_fqn key == local_args key key == tensor_fqn + info_from_tensor_fqn key == local_args key info_from_tensor_fqn will chop leading tensor_fqn so ignore raise AssertionError f Given both ` key ` ` tensor_fqn ` config expected them agree local_args update info_from_tensor_fqn groups append local_args _prepare _prepare args kwargs r Adds mask parametrization layer weight config groups module = config module tensor_name = config tensor_name parametrization = config get parametrization FakeSparsity mask = config get mask torch ones_like getattr module tensor_name state config tensor_fqn mask = mask parametrize register_parametrization module tensor_name parametrization mask squash_mask params_to_keep Optional tuple str = None params_to_keep_per_layer Optional dict str tuple str = None args kwargs r Squashes sparse masks into appropriate tensors If either ` params_to_keep ` ` params_to_keep_per_layer ` set module will have ` sparse_params ` dict attached Args params_to_keep List keys save module dict representing modules keys will have sparsity parameters saved params_to_keep_per_layer Dict specify params should saved specific layers The keys dict should module fqn while values should list strings names variables save ` sparse_params ` Examples xdoctest +SKIP locals undefined Don t save any sparse params sparsifier squash_mask hasattr model submodule sparse_params False Keep sparse params per layer sparsifier squash_mask params_to_keep_per_layer= submodule linear foo bar submodule linear baz print model submodule linear sparse_params foo bar print model submodule linear sparse_params baz Keep sparse params all layers sparsifier squash_mask params_to_keep= foo bar print model submodule linear sparse_params foo bar print model submodule linear sparse_params foo bar Keep some sparse params all layers specific ones some other layers sparsifier squash_mask params_to_keep= foo bar params_to_keep_per_layer= submodule linear baz print model submodule linear sparse_params foo bar print model submodule linear sparse_params foo bar baz config groups module = config module tensor_name = config tensor_name parametrize remove_parametrizations module tensor_name leave_parametrized=True sparse_params = params_to_keep None global_params = k config k k params_to_keep sparse_params update global_params params_to_keep_per_layer None params = params_to_keep_per_layer get config module_fqn None params None per_layer_params = k config k k params sparse_params update per_layer_params sparse_params TODO handle multiple tensor being quantized single module where store sparse_params module sparse_params = sparse_params convert module nn Module mapping Optional dict type nn Module type nn Module = None inplace bool = False parameterization type nn Module = FakeSparsity r Converts submodules input module different module according ` mapping ` calling ` from_dense ` method target module Args module input module mapping dictionary maps source module type target module type can overwritten allow swapping user defined Modules inplace carry out model transformations in-place original module mutated mapping None raise NotImplementedError Need auto generate mapping inplace module = copy deepcopy module reassign = name mod module named_children leaf node module_contains_param mod parameterization type_before_parametrizations mod mapping reassign name = swap_module mod mapping recurse reassign name = convert mod mapping=mapping inplace=True parameterization=parameterization key value reassign items module _modules key = value module step use_path bool = True - None enable_mask_update torch no_grad config groups update_mask config abc abstractmethod update_mask module nn Module tensor_name str kwargs pass