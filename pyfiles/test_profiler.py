Owner s oncall profiler ruff noqa F collections gc json mmap os pickle random re struct subprocess sys tempfile threading time unittest dataclasses dataclass field typing Optional TYPE_CHECKING unittest mock patch expecttest torch torch nn nn torch optim torch utils data torch _C _profiler _ExperimentalConfig _ExtraFields_PyCall torch _inductor utils is_big_gpu torch autograd profiler KinetoStepTracker profile _profile torch autograd profiler_legacy profile _profile_legacy torch profiler _utils DeviceType kineto_available profile ProfilerAction ProfilerActivity record_function supported_activities torch profiler _pattern_matcher Conv dBiasFollowedByBatchNorm dPattern ExtraCUDACopyPattern ForLoopIndexingPattern FP MatMulPattern GradNotSetToNonePattern MatMulDimInFP Pattern NamePattern OptimizerSingleTensorPattern Pattern report_all_anti_patterns SynchronizedDataLoaderPattern torch testing _internal common_cuda TEST_MULTIGPU torch testing _internal common_utils instantiate_parametrized_tests IS_ARM IS_JETSON IS_LINUX IS_WINDOWS parametrize run_tests serialTest skipIfTorchDynamo TemporaryDirectoryName TemporaryFileName TEST_CUDA TEST_WITH_CROSSREF TEST_WITH_ROCM TEST_XPU TestCase TYPE_CHECKING torch autograd profiler_util FunctionEvent tqdm shutdown properly will leave monitor thread alive This causes issue multithreading test because we check all events test their tids The events correspond these lingering threads all have TID uint _t - which invalid The work around turning off monitoring thread when tqdm loaded Since these unit tests safe turn off monitor thread try tqdm tqdm tqdm monitor_interval = except ImportError pass try psutil HAS_PSUTIL = True except ModuleNotFoundError HAS_PSUTIL = False psutil = None unittest skipIf HAS_PSUTIL Requires psutil run unittest skipIf IS_WINDOWS Test flaky Windows unittest skipIf torch cuda is_available CUDA required TestProfilerCUDA TestCase test_mem_leak Checks there s no memory leak when using profiler CUDA t = torch rand cuda p = psutil Process last_rss = collections deque maxlen= _ range _profile use_cuda=True _ range t = torch mm t t gc collect torch cuda empty_cache last_rss append p memory_info rss CUDA events leaking increase memory ~ MB between profiler invocations above is_increasing = all last_rss idx last_rss idx - idx range len last_rss max_diff = - idx range len last_rss max_diff = max max_diff last_rss idx - last_rss idx - assertTrue is_increasing max_diff msg=f memory usage increasing str last_rss test_custom_module_input_op_ids MyFunc torch autograd Function staticmethod forward ctx x ctx save_for_backward x x staticmethod backward ctx gO x = ctx saved_tensors x custom_layer input_ten MyFunc apply input_ten Only testing emit_nvtx runs when record_shapes option enabled torch autograd profiler emit_nvtx record_shapes=True prof x = torch randn requires_grad=True y = torch randn requires_grad=True z = x + y s = custom_layer z q = s sum q backward unittest skipIf torch cuda is_available CUDA required test_cudagraph_profiling_workaround subprocess repro taken Launch separate process catch hanging illegal memory errors make sure CUPTI isn t already initialized p = subprocess check_call sys executable -c os torch torch profiler ProfilerActivity profile add_one in_ torch Tensor in_ + sample_arg = torch zeros device= cuda requires_grad_ True add before cuda graphs created torch profiler _utils _init_for_cuda_graphs add_one_graphed = torch cuda graphs make_graphed_callables add_one sample_args= sample_arg zeros = torch zeros device= cuda out = add_one_graphed zeros assert out == profile activities= ProfilerActivity CPU add_one_graphed zeros profile activities= ProfilerActivity CUDA add_one_graphed zeros universal_newlines=True timeout= ^ will throw exception script fails unittest skipIf torch profiler itt is_available ITT required TestProfilerITT TestCase test_custom_module_input_op_ids MyFunc torch autograd Function staticmethod forward ctx x ctx save_for_backward x x staticmethod backward ctx gO x = ctx saved_tensors x custom_layer input_ten MyFunc apply input_ten Only testing emit_itt runs when record_shapes option enabled torch autograd profiler emit_itt record_shapes=True prof x = torch randn requires_grad=True y = torch randn requires_grad=True z = x + y s = custom_layer z q = s sum q backward instantiate_parametrized_tests TestProfiler TestCase unittest skipIf TEST_WITH_CROSSREF crossref intercepts calls changes callsite test_source Checks source code attribution works eager TS autograd mode avoid automatic inlining prev_opt = torch _C _get_graph_executor_optimize torch _C _set_graph_executor_optimize False torch jit script ts_method_ x y torch matmul x y torch jit script ts_method_ x y z = x + z w = ts_method_ x y + w sum DummyModule nn Module __init__ - None super __init__ conv = torch nn Conv d kernel_size= stride= padding= bias=False forward x conv x mod = DummyModule call_module x mod x _profile with_stack=True use_kineto=kineto_available experimental_config=_ExperimentalConfig verbose=True p x = torch randn requires_grad=True y = torch randn requires_grad=True z = x + y w = ts_method_ x y z v = w v backward = torch randn requires_grad=True b = call_module c = b sum c backward e p function_events aten add e name AddBackward e name assertTrue any test_profiler entry entry e stack assertTrue any test_source entry ts_method_ entry ts_method_ entry entry e stack kineto_available TemporaryFileName mode= w+ fname p export_chrome_trace fname open fname f events = json load f traceEvents extract pattern str matches = e e events re search pattern e name assertEqual len matches repr e name e matches matches module_event = extract r DummyModule_ wrapper_event = extract r call_module assertEqual module_event args Python parent id wrapper_event args Python id torch _C _set_graph_executor_optimize prev_opt parametrize name thread_spec basic False False multiple_preexisting False False open_in_scope True False close_in_scope False True complex Large number background threads False False False False False False False False some which finish during profiling False True False True And profiled section also multithreaded True False True True items name_fn=lambda name thread_spec name serialTest parametrize work_in_main_thread True False skipIfTorchDynamo profiler gets ignored dynamo activated test_source_multithreaded name thread_spec work_in_main_thread Test various threading configurations ` thread_spec ` Tuple Tuple bool bool where each pair thread The first bool indicates thread should started under profiler context second should joined under profiler context timeout = num_threads = len thread_spec + Main thread start_barrier = threading Barrier num_threads timeout=timeout end_barrier = threading Barrier num_threads timeout=timeout Task threading Thread __init__ - None _end_gate = threading Event super __init__ daemon=True start finished = False run _run _end_gate release _end_gate set staticmethod _run end_gate=None known_preexisting_function start_barrier wait Fixed point we can use test capture functions which already running when profiling enabled known_preexisting_function model = torch nn Sequential torch nn Linear torch nn ReLU invoked_during_run pass invoked_during_run _ = model torch rand end_barrier wait end_gate None end_gate wait timeout=timeout threads = add_threads context bool idx start_under_profiler _ enumerate thread_spec start_under_profiler == context assert idx threads threads idx = Task join_threads context bool idx _ end_under_profiler enumerate thread_spec end_under_profiler == context threads idx release idx _ end_under_profiler enumerate thread_spec t = threads idx end_under_profiler == context t join timeout=timeout try add_threads False torch profiler profile with_stack=True prof Threads added while profiler running will observed since there no way hook into Python s thread start call register observer These here purely verify safety add_threads True work_in_main_thread Task _run start_barrier wait end_barrier wait join_threads True join_threads False finally It very important we clean up everything because Python tracer will detect ALL active threads Even orphans prior failed tests If we don t clean up properly we can contaminate subsequent tests start_barrier abort end_barrier abort t threads values t release t threads values t join timeout=timeout t threads values assertFalse t is_alive roots = prof profiler kineto_results experimental_event_tree nodes = node node _utils traverse_dfs roots isinstance node extra_fields _ExtraFields_PyCall tid_counts = collections Counter node start_tid node nodes prior_threads = sum start_under_profiler start_under_profiler _ thread_spec expected_threads = prior_threads + assertEqual len tid_counts expected_threads f expected_threads tid_counts assertEqual len nodes sum tid_counts values Profiler uses uint _t max placeholder until TID can determined no_tid = - assertFalse no_tid tid_counts worker_threads = prior_threads + work_in_main_thread observed_preexisting = node start_tid node nodes known_preexisting_function node name assertEqual len observed_preexisting worker_threads assertEqual len observed_preexisting len set observed_preexisting observed_during_run = node start_tid node nodes invoked_during_run node name assertEqual len observed_during_run worker_threads assertEqual len observed_during_run len set observed_during_run payload use_cuda=False x = torch randn use_cuda x = x cuda y = torch randn use_cuda y = y cuda z = torch mm x y z = z + y use_cuda z = z cpu _check_stats profiler_stats assertGreater profiler_stats profiling_window_duration_sec assertGreater profiler_stats number_of_events assertGreater profiler_stats profiler_prepare_call_duration_us assertGreater profiler_stats profiler_enable_call_duration_us assertGreater profiler_stats profiler_disable_call_duration_us assertGreater profiler_stats parse_kineto_call_duration_us assertGreater profiler_stats function_events_build_tree_call_duration_us unittest skipIf kineto_available Kineto required test_kineto use_cuda = torch profiler ProfilerActivity CUDA supported_activities _profile use_cuda=use_cuda use_kineto=True payload use_cuda=use_cuda rerun avoid initial start overhead _profile use_cuda=use_cuda use_kineto=True p payload use_cuda=use_cuda assertTrue aten mm str p output = p key_averages table sort_by= self_cuda_time_total use_cuda self_cpu_time_total row_limit=- print output found_gemm = False found_memcpy = False found_mm = False e p function_events aten mm e name found_mm = True gemm e name lower Cijk e name found_gemm = True memcpy e name lower __amd_rocclr_copyBuffer e name found_memcpy = True use_cuda assertTrue found_gemm assertTrue found_memcpy assertTrue found_mm _check_stats p _stats p export_chrome_trace tmp test_trace json unittest skipIf kineto_available Kineto required unittest skipIf TEST_MULTIGPU Multiple GPUs needed unittest skipIf TEST_WITH_ROCM Not supported ROCm test_kineto_multigpu profile activities= ProfilerActivity CPU ProfilerActivity CUDA prof gpu_id x = torch randn cuda gpu_id y = torch randn cuda gpu_id z = x matmul y found_gemm_ = False found_gemm_ = False found_cuda = False evt prof events gemm evt name lower evt device_type == DeviceType CUDA evt device_index == found_gemm_ = True evt device_index == found_gemm_ = True cuda evt name lower evt device_type == DeviceType CPU found_cuda = True assertTrue found_gemm_ assertTrue found_gemm_ assertTrue found_cuda _check_stats prof _stats test_memory_profiler run_profiler tensor_creation_fn collecting allocs deallocs _profile profile_memory=True record_shapes=True use_kineto=kineto_available prof x = None record_function test_user_scope_alloc x = tensor_creation_fn record_function test_user_scope_dealloc del x prof key_averages group_by_input_shape=True check_metrics stats metric allocs=None deallocs=None stat_metrics = print stats stat stats stat_metrics stat key = getattr stat metric print stat_metrics allocs None alloc_fn allocs assertTrue alloc_fn stat_metrics assertGreater stat_metrics alloc_fn f alloc_fn = alloc_fn deallocs None dealloc_fn deallocs assertTrue dealloc_fn stat_metrics assertLess stat_metrics dealloc_fn f alloc_fn = dealloc_fn create_cpu_tensor torch rand create_cuda_tensor torch rand cuda create_xpu_tensor torch rand xpu create_mkldnn_tensor torch rand dtype=torch float to_mkldnn stats = run_profiler create_cpu_tensor check_metrics stats cpu_memory_usage allocs= aten empty aten rand test_user_scope_alloc deallocs= test_user_scope_dealloc kineto_available TemporaryFileName mode= w+ fname profile profile_memory=True prof x = None record_function test_user_scope_alloc x = create_cpu_tensor record_function test_user_scope_dealloc del x prof export_chrome_trace fname open fname f trace = json load f assert traceEvents trace events = trace traceEvents found_memory_events = False evt events assert name evt evt name == memory found_memory_events = True assert args evt assert Addr evt args assert Device Type evt args assert Device Id evt args assert Bytes evt args Memory should instantaneous event assert dur evt args assert cat evt args assert found_memory_events torch cuda is_available create_cuda_tensor stats = run_profiler create_cuda_tensor check_metrics stats device_memory_usage allocs= test_user_scope_alloc aten aten empty_strided deallocs= test_user_scope_dealloc check_metrics stats cpu_memory_usage allocs= aten rand aten empty torch xpu is_available create_xpu_tensor stats = run_profiler create_xpu_tensor check_metrics stats device_memory_usage allocs= test_user_scope_alloc aten aten empty_strided deallocs= test_user_scope_dealloc check_metrics stats cpu_memory_usage allocs= aten rand aten empty torch backends mkldnn is_available create_mkldnn_tensor stats = run_profiler create_mkldnn_tensor check_metrics stats cpu_memory_usage allocs= test_user_scope_alloc aten rand aten empty aten to_mkldnn deallocs= test_user_scope_dealloc check top-level memory events _profile profile_memory=True use_kineto=kineto_available prof x = torch rand del x torch cuda is_available y = torch rand cuda del y torch xpu is_available y = torch rand xpu del y gc collect stats = prof key_averages group_by_input_shape=True check_metrics stats cpu_memory_usage allocs= aten rand aten empty deallocs= memory torch cuda is_available check_metrics stats device_memory_usage deallocs= memory torch xpu is_available check_metrics stats device_memory_usage deallocs= memory unittest skipIf IS_JETSON Jetson has guard against OOM since host gpu memory shared test_oom_tracing run_profiler tensor_creation_fn _profile profile_memory=True record_shapes=True prof assertRaisesRegex RuntimeError tT ried allocate x = tensor_creation_fn prof create_cuda_tensor_oom device = torch device cuda torch empty dtype=torch float device=device check_trace fname prof export_chrome_trace fname open fname f trace = json load f assertTrue traceEvents trace events = trace traceEvents found_out_of_memory_events = False evt events assertTrue name evt evt name == OutOfMemory found_out_of_memory_events = True assertTrue args evt assertTrue Device Type evt args assertTrue Device Id evt args assertTrue Bytes evt args Memory should instantaneous event assertTrue dur evt args assertTrue cat evt args assertTrue found_out_of_memory_events torch cuda is_available TemporaryFileName mode= w+ fname prof = run_profiler create_cuda_tensor_oom check_trace fname unittest skipIf kineto_available Kineto required test_module_hierarchy A nn Module my_new_method x x forward_impl_ x y my_new_method x + y forward x y y = y - forward_impl_ x y B nn Module forward x x + C nn Module __init__ - None super __init__ A = A B = B call_b x B forward x forward x y A forward x y + call_b x model = C model = torch jit script model input_a = torch rand input_b = torch rand op_to_module_hierarchy = op_to_module_hierarchy aten sub = TOP C forward A A forward op_to_module_hierarchy aten mul = TOP C forward A A forward SELF A forward_impl_ SELF A my_new_method op_to_module_hierarchy aten add = TOP C forward A A forward SELF A forward_impl_ TOP C forward SELF C call_b B B forward TOP C forward TemporaryFileName mode= w+ fname profile activities= torch profiler ProfilerActivity CPU with_modules=True prof model input_a input_b prof export_chrome_trace fname open fname f trace = json load f assert traceEvents trace events = trace traceEvents found_memory_events = False evt events assert name evt args evt op_name = evt name Module Hierarchy evt args hierarchy = evt args Module Hierarchy op_name op_to_module_hierarchy assert hierarchy op_to_module_hierarchy op_name test_high_level_trace Checks python side high level events recorded RepeatedDataset torch utils data Dataset __init__ N D_in D_out N = N x = torch randn N D_in y = torch randn N D_out __len__ N __getitem__ idx x y TwoLayerNet torch nn Module __init__ D_in H D_out super __init__ linear = torch nn Linear D_in H linear = torch nn Linear H D_out forward x h_relu = linear x clamp min= y_pred = linear h_relu y_pred CustomSGD torch optim SGD __init__ args kwargs super __init__ args kwargs train data dataloader x y = data data y_pred = model x loss = criterion y_pred y optimizer zero_grad loss backward optimizer step N D_in H D_out = model = TwoLayerNet D_in H D_out criterion = torch nn MSELoss reduction= sum optimizer = torch optim SGD model parameters lr= e- ds = RepeatedDataset N D_in D_out dataloader = torch utils data DataLoader ds batch_size= try train except Exception assertTrue False Expected no exception without profiling Create multiple instances expect each func hooked only one time Nested wrappers repeated patching will make following test fail optimizer_duplicate = torch optim SGD model parameters lr= e- dataloader_duplicate = torch utils data DataLoader ds batch_size= judge expected_event_count prof actual_event_count = e prof function_events e name key = e name key expected_event_count keys actual_event_count key = actual_event_count setdefault key + key count expected_event_count items assertTrue key actual_event_count keys count == actual_event_count key _profile use_kineto=kineto_available prof train expected_event_count = + because final iteration will enter __next__ skip loop body enumerate DataLoader #_SingleProcessDataLoaderIter __next__ N + Optimizer step#SGD step N Optimizer zero_grad#SGD zero_grad N judge expected_event_count prof Test pickle unpickle Expect work multi-processing optimizer = pickle loads pickle dumps optimizer _profile use_kineto=kineto_available prof train judge expected_event_count prof Test customized optimizer optimizer = CustomSGD model parameters lr= e- _profile use_kineto=kineto_available prof train expected_event_count = enumerate DataLoader #_SingleProcessDataLoaderIter __next__ N + Optimizer step#CustomSGD step N Optimizer zero_grad#CustomSGD zero_grad N judge expected_event_count prof test_flops model = torch nn Sequential nn Conv d nn ReLU nn Linear nn ReLU inputs = torch randn nested_tensor = torch nested nested_tensor torch randn torch randn layout=torch jagged _profile record_shapes=True with_flops=True use_kineto=kineto_available prof model inputs test nested tensor won t cause exception during flop compute nested_tensor = nested_tensor + nested_tensor profiler_output = prof key_averages group_by_input_shape=True table sort_by= cpu_time_total row_limit= assertRegex profiler_output Total M FLOPs kineto_available torch cuda is_available profile activities= torch profiler ProfilerActivity CPU torch profiler ProfilerActivity CUDA record_shapes=True with_flops=True kineto_profiler model inputs profiler_output = kineto_profiler key_averages table sort_by= self_cuda_time_total row_limit=- assertRegex profiler_output Total M FLOPs test_override_time_units US_IN_SECOND = US_IN_MS = model = torch nn Sequential nn Conv d nn ReLU nn Linear nn ReLU inputs = torch randn _profile prof model inputs profiler_output = prof key_averages table time_unit= s assertRegex profiler_output r \ - s assertNotRegex profiler_output r \ - ms assertNotRegex profiler_output r \ - us event prof key_averages cpu_time_str_s = f event cpu_time US_IN_SECOND f s cpu_time_total_str_s = f event cpu_time_total US_IN_SECOND f s assertTrue cpu_time_str_s profiler_output assertTrue cpu_time_total_str_s profiler_output profiler_output = prof key_averages table time_unit= ms assertNotRegex profiler_output r \ - s assertRegex profiler_output r \ - ms assertNotRegex profiler_output r \ - us event prof key_averages cpu_time_str_ms = f event cpu_time US_IN_MS f ms cpu_time_total_str_ms = f event cpu_time_total US_IN_MS f ms assertTrue cpu_time_str_ms profiler_output assertTrue cpu_time_total_str_ms profiler_output profiler_output = prof key_averages table time_unit= us assertNotRegex profiler_output r \ - s assertNotRegex profiler_output r \ - ms assertRegex profiler_output r \ - us event prof key_averages cpu_time_str_us = f event cpu_time f us cpu_time_total_str_us = f event cpu_time_total f us assertTrue cpu_time_str_us profiler_output assertTrue cpu_time_total_str_us profiler_output patch dict os environ KINETO_USE_DAEMON patch dict os environ KINETO_DAEMON_INIT_DELAY_S test_kineto_profiler_api called_num = use_cuda = torch profiler ProfilerActivity CUDA supported_activities profile activities=supported_activities payload use_cuda=use_cuda trace_handler p output = p key_averages table sort_by= self_cuda_time_total use_cuda self_cpu_time_total row_limit=- print output p export_chrome_trace tmp test_trace_ + str called_num + json called_num += initial_step = KinetoStepTracker current_step profile activities=supported_activities schedule=torch profiler schedule wait= warmup= active= on_trace_ready=trace_handler p _ range payload use_cuda=use_cuda p step assertEqual called_num assertEqual KinetoStepTracker current_step initial_step + case without schedule profile activities=supported_activities p payload use_cuda=use_cuda payload use_cuda=use_cuda output = p key_averages table sort_by= self_cuda_time_total use_cuda self_cpu_time_total row_limit=- print output test_schedule = torch profiler schedule skip_first= wait= warmup= active= repeat= test_schedule_expected_outputs = skip first ProfilerAction NONE ProfilerAction NONE ProfilerAction NONE ---- repeat No begin wait ProfilerAction NONE ProfilerAction NONE warmup ProfilerAction WARMUP active begin ProfilerAction RECORD ProfilerAction RECORD ProfilerAction RECORD ProfilerAction RECORD_AND_SAVE active end repeat No end --- repeat No begin wait ProfilerAction NONE ProfilerAction NONE warmup ProfilerAction WARMUP active begin ProfilerAction RECORD ProfilerAction RECORD ProfilerAction RECORD ProfilerAction RECORD_AND_SAVE active end repeat No end ProfilerAction NONE ProfilerAction NONE ProfilerAction NONE ProfilerAction NONE step range len test_schedule_expected_outputs assertEqual test_schedule step test_schedule_expected_outputs step patch dict os environ KINETO_USE_DAEMON patch dict os environ KINETO_DAEMON_INIT_DELAY_S test_kineto_profiler_multiple_steppers niters = use_cuda = torch profiler ProfilerActivity CUDA supported_activities net = SimpleNet opt = torch optim SGD net parameters lr= momentum= opt zero_grad inputs = torch rand profile activities=supported_activities payload use_cuda=use_cuda optimizer_step This simulates step hook optimizer KinetoStepTracker increment_step yet_another_step initial_step = KinetoStepTracker current_step run_batch out = net inputs loss = torch nn functional cross_entropy out torch rand loss backward opt step Manually call hook TODO Remove once we add profiler step hooks Optimizer will get triggered above See https github com pytorch pytorch issues optimizer_step _ range niters run_batch profile activities=supported_activities schedule=torch profiler schedule wait= warmup= active= p _ range niters run_batch p step assertEqual KinetoStepTracker current_step initial_step + niters test_export_stacks _profile with_stack=True use_kineto=kineto_available experimental_config=_ExperimentalConfig verbose=True p x = torch randn y = torch randn z = torch mm x y z = z + y TemporaryFileName mode= w+ fname p export_stacks fname open fname f lines = f readlines assert len lines Empty stacks file line lines is_int = False try assert int line split - Invalid stacks record is_int = True except ValueError pass assert is_int Invalid stacks record unittest skipIf kineto_available Kineto required test_tensorboard_trace_handler use_cuda = torch profiler ProfilerActivity CUDA supported_activities _profile use_cuda=use_cuda use_kineto=True payload use_cuda=use_cuda TemporaryDirectoryName dname profile activities= torch profiler ProfilerActivity CPU + torch profiler ProfilerActivity CUDA use_cuda schedule=torch profiler schedule wait= warmup= active= repeat= on_trace_ready=torch profiler tensorboard_trace_handler dname p _ range payload use_cuda=use_cuda p step assertTrue os path exists dname file_num = file_name os listdir dname parts = file_name split assertTrue len parts assertTrue parts - isdigit int parts - Wrong tracing file name pattern assertEqual parts - pt trace json file_num += assertEqual file_num test case gzip file format TemporaryDirectoryName dname p = profile activities= torch profiler ProfilerActivity CPU + torch profiler ProfilerActivity CUDA use_cuda schedule=torch profiler schedule wait= warmup= active= repeat= on_trace_ready=torch profiler tensorboard_trace_handler dname use_gzip=True p start _ range payload use_cuda=use_cuda p step p stop assertTrue os path exists dname file_num = file_name os listdir dname parts = file_name split assertTrue len parts assertTrue parts - isdigit int parts - Wrong tracing file name pattern assertEqual parts - pt trace json gz file_num += assertEqual file_num unittest skipIf kineto_available Kineto required test_profiler_metadata t t = torch ones torch ones profile prof torch add t t prof add_metadata test_key test_value prof add_metadata_json test_key TemporaryFileName mode= w+ fname prof export_chrome_trace fname open fname f trace = json load f assert test_key trace assert trace test_key == test_value assert test_key trace assert trace test_key == _test_profiler_tracing use_kineto _profile use_kineto=use_kineto prof t t = torch ones torch ones torch add t t TemporaryFileName mode= w+ fname prof export_chrome_trace fname read trace expect valid json JSON generated export_chrome_trace valid will throw fail test open fname f json load f test empty trace _profile use_kineto=use_kineto prof pass saving empty trace TemporaryFileName mode= w+ fname prof export_chrome_trace fname use_kineto open fname f contents = json load f Some builds may have logger observer so skip WARNING contents found_empty_warning = False warning contents WARNING No Valid Trace Events warning found_empty_warning = True assertTrue found_empty_warning Same test cuda use_cuda = torch profiler ProfilerActivity CUDA supported_activities use_cuda device = torch device cuda _profile use_cuda=True use_kineto=use_kineto prof t t = torch ones device=device torch ones device=device torch add t t TemporaryFileName mode= w+ fname prof export_chrome_trace fname Now validate json open fname f json load f test_profiler_tracing _test_profiler_tracing False kineto_available _test_profiler_tracing True test_profiler_op_event_args torch _C _profiler _set_record_concrete_inputs_enabled_val True _profile record_shapes=True prof = torch ones dtype=torch float c = torch cat sin TemporaryFileName mode= w+ fname prof export_chrome_trace fname open fname f j = json load f op_events = e e j traceEvents e get cat == cpu_op e op_events args = e args e name == aten ones assertEqual args Input type ScalarList Scalar Scalar assertEqual args Concrete Inputs False e name == aten cat assertEqual args Input Dims assertEqual args Input type TensorList Scalar check each op has record function id assertGreaterEqual args get Record function id - f Failed finding record funciont op = e test_profiler_strides torch _C _profiler _set_record_concrete_inputs_enabled_val True base_tensor = torch randn dtype=torch float = base_tensor as_strided b = base_tensor as_strided _profile record_shapes=True prof c = torch add b TemporaryFileName mode= w+ fname prof export_chrome_trace fname open fname f j = json load f op_events = e e j traceEvents e get cat == cpu_op e op_events args = e args e name == aten add assertEqual args Input Strides test_profiler_fwd_bwd_link _profile use_kineto=True prof t t = torch ones requires_grad=True torch ones requires_grad=True z = torch add t t y = torch ones loss = torch nn functional binary_cross_entropy_with_logits z y loss backward TemporaryFileName mode= w+ fname prof export_chrome_trace fname open fname f j = json load f events = j traceEvents ts_to_name = flow_s_to_ts = flow_f_to_ts = e events e ph == X ts_to_name e ts = e name cat e name e e cat == fwdbwd e name == fwdbwd e ph == s flow_s_to_ts e id = e ts e ph == f flow_f_to_ts e id = e ts assertEqual len flow_s_to_ts assertEqual len flow_f_to_ts assertIn flow_s_to_ts assertIn flow_f_to_ts assertIn flow_s_to_ts assertIn flow_f_to_ts s_ts_ = flow_s_to_ts f_ts_ = flow_f_to_ts s_ts_ = flow_s_to_ts f_ts_ = flow_f_to_ts assertTrue all ts ts_to_name keys ts s_ts_ f_ts_ s_ts_ f_ts_ assertTrue ts_to_name s_ts_ == aten binary_cross_entropy_with_logits assertTrue ts_to_name s_ts_ == aten add test_profiler_disable_fwd_bwd_link try torch _C _profiler _set_fwd_bwd_enabled_val False _profile use_kineto=True prof t t = torch ones requires_grad=True torch ones requires_grad=True z = torch add t t y = torch ones loss = torch nn functional binary_cross_entropy_with_logits z y loss backward TemporaryFileName mode= w+ fname prof export_chrome_trace fname open fname f j = json load f events = j traceEvents e events assertNotEqual e get cat None fwdbwd finally torch _C _profiler _set_fwd_bwd_enabled_val True unittest skipIf kineto_available Kineto required unittest skipIf torch cuda is_available CUDA required test_profiler_cuda_sync_events device = torch device cuda t t = torch ones device=device torch ones device=device workload - None torch add t t torch cuda synchronize torch add t t trace_and_check exp_config Optional _ExperimentalConfig - None _profile use_kineto=True use_cuda=True experimental_config=exp_config prof workload TemporaryFileName mode= w+ fname fname = tmp kineto_out json prof export_chrome_trace fname open fname f j = json load f cats = e get cat None e j traceEvents assertTrue cuda_sync cats f Expected find cuda_sync event found = cats print Testing enable_cuda_sync_events _ExperimentalConfig trace_and_check exp_config=_ExperimentalConfig enable_cuda_sync_events=True print Testing _profiler _set_cuda_sync_enabled_val try torch _C _profiler _set_cuda_sync_enabled_val True trace_and_check exp_config=None finally torch _C _profiler _set_cuda_sync_enabled_val False test_profiler_type profiler_type = torch _C _autograd _profiler_type ActiveProfilerType = torch _C _profiler ActiveProfilerType assertEqual profiler_type ActiveProfilerType NONE Autograd profiler _profile_legacy assertEqual profiler_type ActiveProfilerType LEGACY Kineto profiler profile assertEqual profiler_type ActiveProfilerType KINETO test_profiler_correlation_id We expect correlation_id unique across multiple invokation profiler So we will reuse id_uniqueness_set id_uniqueness_set = set model = torch nn Sequential nn Conv d nn ReLU nn Linear nn ReLU inputs = torch randn uint _max = - _ range profile prof model inputs event prof profiler kineto_results events corr_id = event correlation_id corr_id event device_type == DeviceType CPU assertTrue corr_id id_uniqueness_set id_uniqueness_set add corr_id assertTrue corr_id uint _max test_nested_tensor_with_shapes = torch randn b = torch randn c = torch randn inp = torch nested nested_tensor b torch profiler profile record_shapes=True prof torch nn functional linear inp c None e prof events e name aten mm aten addmm intentionally vague tests protect against possible future changes mm addmm other impl changing internal order args assertTrue len e input_shapes assertTrue len e input_shapes patch dict os environ KINETO_USE_DAEMON patch dict os environ KINETO_DAEMON_INIT_DELAY_S skipIfTorchDynamo profiler gets ignored dynamo activated test_kineto_profiler_with_environment_variable script = torch torch nn nn torch profiler supported_activities profile torch autograd profiler KinetoStepTracker SimpleNet nn Module __init__ - None super __init__ fc = nn Linear fc = nn Linear forward x fc fc x payload use_cuda=False x = torch randn use_cuda x = x cuda y = torch randn use_cuda y = y cuda z = torch mm x y z = z + y use_cuda z = z cpu niters = use_cuda = torch profiler ProfilerActivity CUDA supported_activities net = SimpleNet opt = torch optim SGD net parameters lr= opt zero_grad inputs = torch rand profile activities=supported_activities payload use_cuda=use_cuda initial_step = KinetoStepTracker current_step run_batch out = net inputs loss = torch nn functional cross_entropy out torch rand loss backward opt step _ range niters run_batch profile activities=supported_activities schedule=torch profiler schedule wait= warmup= active= p _ range niters run_batch p step assert KinetoStepTracker current_step == initial_step + niters try subprocess check_output sys executable -W always -c script cwd=os path dirname os path realpath __file__ except subprocess CalledProcessError e e returncode = assertTrue False Kineto working properly Dynolog environment variable test_concrete_inputs_profiling x = torch rand profile record_shapes=True p y = x as_strided found = False e p events e name aten as_strided found = True assertTrue len e input_shapes assertTrue len e concrete_inputs assertEqual e input_shapes assertEqual e concrete_inputs assertEqual e concrete_inputs assertTrue found Expected find aten as_strided did test_concrete_inputs_profiling_toggling try before after True False False True x = torch rand torch _C _profiler _set_record_concrete_inputs_enabled_val before profile record_shapes=True p y = x as_strided torch _C _profiler _set_record_concrete_inputs_enabled_val after found = False e p events e name aten as_strided found = True assertTrue len e input_shapes assertTrue found Expected find aten as_strided did finally torch _C _profiler _set_record_concrete_inputs_enabled_val True test_record_function_fast x y = torch rand _ range profile record_shapes=True p _ range Test first no optional args torch _C _profiler _RecordFunctionFast add_test_fast_rf x add y assertGreaterEqual len e e p events e name == add_test_fast_rf e p events e name == add_test_fast_rf assertTrue e input_shapes == assertTrue e kwinputs == profile record_shapes=True p add optional args cm = torch _C _profiler _RecordFunctionFast add_test_fast_rf x y stream grid lambda x x + _ range cm x add y assertGreaterEqual len e e p events e name == add_test_fast_rf e p events e name == add_test_fast_rf assertTrue e input_shapes == assertTrue e kwinputs == stream grid lambda x x + profile record_shapes=True p cm = torch _C _profiler _RecordFunctionFast add_test_fast_rf input_values= hi keyword_values= hi hello _ range try cm x add y raise ValueError x relu except ValueError pass assertGreaterEqual len e e p events e name == add_test_fast_rf assertFalse any e name relu e name e p events e p events e name == add_test_fast_rf assertTrue e input_shapes == profile p _ range torch _C _profiler _RecordFunctionFast add_test_fast_rf x y x add y torch _C _profiler _RecordFunctionFast add_test_fast_rf x relu assertGreaterEqual len e e p events e name == add_test_fast_rf e p events e name == add_test_fast_rf assertTrue e input_shapes == assertGreaterEqual len e e p events e name == add_test_fast_rf profile record_shapes=True p test optional args tuple cm = torch _C _profiler _RecordFunctionFast add_test_fast_rf x y _ range cm x add y assertGreaterEqual len e e p events e name == add_test_fast_rf e p events e name == add_test_fast_rf assertTrue e input_shapes == skipIfTorchDynamo profiler gets ignored dynamo activated test_profiler_op_event_kwargs x y = torch rand _ range profile record_shapes=True p cm = torch _C _profiler _RecordFunctionFast add_test_kwinputs x y stream grid lambda x x + debug debug boolean True _ range cm x add y TemporaryFileName mode= w+ fname p export_chrome_trace fname open fname f j = json load f op_events = e e j traceEvents e get name == add_test_kwinputs assertTrue len op_events e op_events args = e args assertTrue stream args assertTrue grid args assertTrue boolean args assertTrue args stream == assertTrue args grid == lambda x x + assertTrue args debug == None assertTrue args boolean assertTrue e cat == cpu_op profile record_shapes=True p cm = torch _C _profiler _RecordFunctionFast add_test_kwinputs x y stream test grid scope user_scope _ range cm x add y TemporaryFileName mode= w+ fname p export_chrome_trace fname open fname f j = json load f op_events = e e j traceEvents e get name == add_test_kwinputs assertTrue len op_events e op_events args = e args assertTrue stream args assertTrue grid args assertTrue e cat == user_annotation skipIfTorchDynamo profiler gets ignored dynamo activated test_profiler_op_event_kwargs_list_of_strings x y = torch rand _ range profile record_shapes=True p cm = torch _C _profiler _RecordFunctionFast add_test_kwinputs_string_list x y string_list hello world test int_param string_param single_string _ range cm x add y TemporaryFileName mode= w+ fname p export_chrome_trace fname open fname f j = json load f op_events = e e j traceEvents e get name == add_test_kwinputs_string_list assertTrue len op_events e op_events args = e args assertTrue string_list args assertTrue int_param args assertTrue string_param args Check list strings properly serialized The list should formatted JSON array ivalueListToStr assertEqual args string_list hello world test assertEqual args int_param assertEqual args string_param single_string assertTrue e cat == cpu_op Test mixed types should filtered out profile record_shapes=True p cm = torch _C _profiler _RecordFunctionFast add_test_kwinputs_string_list_filtered x y valid_string_list valid valid mixed_list string Should filtered out non_string_list Should filtered out valid_int _ range cm x add y TemporaryFileName mode= w+ fname p export_chrome_trace fname open fname f j = json load f op_events = e e j traceEvents e get name == add_test_kwinputs_string_list_filtered assertTrue len op_events e op_events args = e args Only valid types should present assertTrue valid_string_list args assertTrue valid_int args Invalid lists should filtered out assertTrue mixed_list args assertTrue non_string_list args Check values assertEqual args valid_string_list valid valid assertEqual args valid_int assertTrue e cat == cpu_op test_is_profiler_enabled assertFalse torch autograd profiler _is_profiler_enabled profile p assertTrue torch autograd profiler _is_profiler_enabled assertFalse torch autograd profiler _is_profiler_enabled torch autograd profiler profile p assertTrue torch autograd profiler _is_profiler_enabled assertFalse torch autograd profiler _is_profiler_enabled test_guarded_record_function_fast x y = torch rand _ range profile p cm = torch _C _profiler _RecordFunctionFast guarded_rff _ range torch autograd profiler _is_profiler_enabled cm x add y x add y assertGreaterEqual len e e p events e name == guarded_rff unittest skipIf torch cuda is_available CUDA required test_event_list AFAIK event list part legacy profiler used when kineto available This test has basic sanity checks test against obvious regressions x y = torch rand requires_grad=True device= cuda _ range profile with_stack=True p z = x y relu sum z backward event_list = torch autograd profiler_util EventList p events event_list _build_tree TemporaryFileName mode= w+ fname event_list export_chrome_trace fname open fname f json load f event_list table _check_all_gpu_present gpu_dict max_gpu_count i range max_gpu_count assertEqual gpu_dict GPU + str i Do json sanity testing Checks all events between profiler start end also checks see GPU values present trace cuda used _validate_basic_json traceEvents cuda_available=False MAX_GPU_COUNT = PROFILER_IDX = - RECORD_END = - RECORD_START = - traceEventProfiler = traceEvents PROFILER_IDX assertTrue traceEventProfiler name == PyTorch Profiler assertTrue traceEvents RECORD_END name == Record Window End assertTrue traceEvents RECORD_START name == Iteration Start PyTorch Profiler check profiler starts ends within record interval assertGreaterEqual traceEventProfiler ts traceEvents RECORD_START ts Profiler starts before record assertLessEqual traceEventProfiler ts + traceEventProfiler dur traceEvents RECORD_END ts Profiler ends after record end gpu_dict = collections defaultdict int i traceEvent enumerate traceEvents i == len traceEvents + RECORD_END i == len traceEvents + RECORD_START continue make sure all valid trace events within bounds profiler ts traceEvent assertGreaterEqual traceEvent ts traceEventProfiler ts Trace event out bounds some python events seem go little past record end probably because some clock inaccuracies so just compare events ending RECORD_END dur traceEvent assertLessEqual traceEvent ts + traceEvent dur traceEvents RECORD_END ts Trace event ends too late gpu_value = traceEvent get args get labels None gpu_value GPU gpu_value gpu_dict gpu_value += Max PID offset M based pytorch kineto include header https github com pytorch kineto blob ff e fa da c c eddd b libkineto include output_base h#L kExceedMaxPid = assertTrue traceEvents i + args sort_index == kExceedMaxPid + int gpu_value split TODO add checking gpu count cpuOnly_ true _test_chrome_trace_basic_helper with_cuda=False with_cuda device = cuda device = cpu x y = torch rand device _ range profile with_stack=True p torch add x y TemporaryFileName mode= w+ fname p export_chrome_trace fname open fname f report = json load f _validate_basic_json report traceEvents with_cuda unittest skipIf kineto_available Kineto required skipIfTorchDynamo profiler gets ignored dynamo activated test_basic_chrome_trace _test_chrome_trace_basic_helper torch cuda is_available _test_chrome_trace_basic_helper with_cuda=True skipIfTorchDynamo profiler gets ignored dynamo activated test_profiler_time_scale MARGIN_ERROR = SEC_TO_US = WAIT_TIME = profile p torch profiler record_function test_span _ range WAIT_TIME torch rand time sleep events = p events make sure function events scaled appropriately assertTrue events name == test_span test_span = events assertGreaterEqual test_span cpu_time SEC_TO_US WAIT_TIME - MARGIN_ERROR event out range assertLessEqual test_span cpu_time SEC_TO_US WAIT_TIME + MARGIN_ERROR event out range make sure tracing scaled appropriately TemporaryFileName mode= w+ fname p export_chrome_trace fname open fname f report = json load f events = report traceEvents event events event name == test_span assertGreaterEqual event dur SEC_TO_US WAIT_TIME - MARGIN_ERROR profiling out range assertLessEqual event dur SEC_TO_US WAIT_TIME + MARGIN_ERROR profiling out range _schedule_helper warmup active repeat acc_events=True profile schedule=torch profiler schedule skip_first= wait= warmup=warmup active=active repeat=repeat acc_events=acc_events prof _ range torch add prof step print prof key_averages ev prof key_averages ev key == aten add ev count skipIfTorchDynamo profiler gets ignored dynamo activated test_schedule_function_count assertEqual _schedule_helper warmup= active= repeat= assertEqual _schedule_helper warmup= active= repeat= assertEqual _schedule_helper warmup= active= repeat= assertEqual _schedule_helper warmup= active= repeat= assertEqual _schedule_helper warmup= active= repeat= assertEqual _schedule_helper warmup= active= repeat= assertEqual _schedule_helper warmup= active= repeat= acc_events=False assertEqual _schedule_helper warmup= active= repeat= acc_events=False _step_helper_func prof time sleep torch randn prof step _partial_overlap prof_step step_helper_func p_start = prof_step ts p_end = prof_step ts + prof_step dur h_start = step_helper_func ts h_end = step_helper_func ts + step_helper_func dur p_start h_start p_end h_end p_end h_start True p_start h_start p_start h_end p_end h_end True False skipIfTorchDynamo profiler gets ignored dynamo activated test_cpu_annotation_overlap torch profiler profile activities= ProfilerActivity CPU ProfilerActivity CUDA record_shapes=True with_stack=True schedule=torch profiler schedule wait= warmup= active= repeat= experimental_config=torch _C _profiler _ExperimentalConfig adjust_profiler_step=True prof _ range _step_helper_func prof TemporaryFileName mode= w+ fname prof export_chrome_trace fname prof_steps = step_helper_funcs = open fname f report = json load f event report traceEvents ProfilerStep event name prof_steps append event step_helper_func event name step_helper_funcs append event assertEqual len prof_steps assertEqual len step_helper_funcs i range len step_helper_funcs j range len step_helper_funcs assertTrue _partial_overlap prof_steps i step_helper_funcs j skipIfTorchDynamo profiler gets ignored dynamo activated test_user_annotation use_cuda = torch profiler ProfilerActivity CUDA supported_activities profile activities=supported_activities p torch profiler record_function test_user_annotation payload use_cuda=use_cuda evt p key_averages evt key == test_user_annotation assertTrue evt is_user_annotation assertFalse evt is_user_annotation unittest skipUnless TEST_CUDA TEST_XPU requires gpu skipIfTorchDynamo profiler gets ignored dynamo activated test_basic_profile test really basic profile make sure no erroneous aten ops run x = torch randn device= cuda torch profiler profile with_stack=True p x = names = e name e p events name names name startswith aten name = aten mul_ assertTrue False Found unexpected event + name assertTrue aten mul_ names unittest skipIf torch cuda is_available CUDA required skipIfTorchDynamo profiler gets ignored dynamo activated test_dynamic_toggle acc = torch accelerator current_accelerator assertIsNotNone acc device = acc type gpu_activity = getattr ProfilerActivity device upper None assertIsNotNone gpu_activity activities = ProfilerActivity CPU gpu_activity profile activities=activities p torch profiler record_function test_user_annotation x y = torch rand device _ range torch add x y assertTrue any aten e name e p events assertTrue any device e name e p events assertTrue any kernel e name lower e p events profile activities=activities p p toggle_collection_dynamic False gpu_activity torch profiler record_function test_user_annotation x y = torch rand device _ range torch add x y assertTrue any aten e name e p events assertTrue all device e name e p events assertTrue all kernel e name lower e p events profile activities=activities p p toggle_collection_dynamic False activities torch profiler record_function test_user_annotation x y = torch rand device _ range torch add x y assertTrue len p events == skipIfTorchDynamo profiler gets ignored dynamo activated test_lazy_build_tree profile p payload stats = p _stats Test tree built assertEqual stats function_events_build_tree_call_duration_us assertEqual stats number_of_events Test tree built demand p events assertGreater stats function_events_build_tree_call_duration_us assertGreater stats number_of_events skipIfTorchDynamo profiler gets ignored dynamo activated unittest skipIf torch cuda is_available CUDA complains about forking after init unittest skipIf IS_WINDOWS can t use os fork Windows test_forked_process Induce pid cache running profiler payload validate_forked_json profiler nonlocal cpu_op_found parent_tid child_pid TemporaryFileName mode= w+ fname profiler export_chrome_trace fname open fname f events = json load f traceEvents event events cat event event cat == cpu_op assertEqual event pid child_pid assertNotEqual event tid parent_tid cpu_op_found = True cpu_op_found = False parent_tid = threading current_thread ident profile p payload pid = os fork pid == child_pid = os getpid profile p payload validate_forked_json p assertTrue cpu_op_found os _exit os waitpid pid skipIfTorchDynamo profiler gets ignored dynamo activated test_skip_first_wait Other tests test when skip_first_wait false default so just test true case test_schedule = torch profiler schedule skip_first= wait= warmup= active= repeat= skip_first_wait= test_schedule_expected_outputs = repeat No begin skip first ProfilerAction NONE ProfilerAction NONE ProfilerAction NONE warmup ProfilerAction WARMUP active begin ProfilerAction RECORD ProfilerAction RECORD_AND_SAVE active end repeat No end --- repeat No begin wait ProfilerAction NONE ProfilerAction NONE ProfilerAction NONE ProfilerAction NONE ProfilerAction NONE warmup ProfilerAction WARMUP active begin ProfilerAction RECORD ProfilerAction RECORD_AND_SAVE active end repeat No end ProfilerAction NONE ProfilerAction NONE ProfilerAction NONE ProfilerAction NONE step range len test_schedule_expected_outputs assertEqual test_schedule step test_schedule_expected_outputs step skipIfTorchDynamo profiler gets ignored dynamo activated unittest skipIf torch cuda is_available CUDA required unittest skipIf kineto_available Kineto required test_disable_external_correlation cuda_external_id_events = cuda_runtime gpu_memcpy kernel activities = ProfilerActivity CPU ProfilerActivity CUDA check_correlations event disable_external_correlation cat event event cat cuda_external_id_events disable_external_correlation assertTrue External id event args event name = cudaDeviceSynchronize assertTrue External id event args assertTrue event args External id validate_json prof disable_external_correlation TemporaryFileName mode= w+ fname prof export_chrome_trace fname open fname f events = json load f traceEvents seen_event_types = set event events check_correlations event disable_external_correlation cat event seen_event_types add event cat assertTrue cuda_external_id_events issubset seen_event_types Run External Id CUDA events off disable_external_correlation False True profile activities=activities experimental_config=torch _C _profiler _ExperimentalConfig disable_external_correlation=disable_external_correlation prof payload use_cuda=True validate_json prof disable_external_correlation skipIfTorchDynamo profiler gets ignored dynamo activated unittest skipIf torch cuda is_available CUDA required unittest skipIf kineto_available Kineto required unittest skipIf RelWithAssert torch __config__ show failing debug build see https github com pytorch pytorch pull example test_profile_all_threads profiling_started = threading Event profiling_ended = threading Event n_rep = prep_inputs torch randn device= cuda _ range main_thread_fn profile_all_threads returned_events x y = prep_inputs experimental_config = torch _C _profiler _ExperimentalConfig profile_all_threads=profile_all_threads torch profiler profile experimental_config=experimental_config record_shapes=True p profiling_started set _ range n_rep _ = x y profiling_ended wait returned_events append p events side_thread_fn x y = prep_inputs profiling_started wait _ range n_rep _ = x y profiling_ended set main_with_thread_fn profile_all_threads x y = prep_inputs experimental_config = torch _C _profiler _ExperimentalConfig profile_all_threads=profile_all_threads torch profiler profile experimental_config=experimental_config record_shapes=True p side_thread = threading Thread target=side_thread_fn side_thread start _ range n_rep _ = x y side_thread join p events profile_all_threads True False returned_events = main_thread = threading Thread target=main_thread_fn args= profile_all_threads returned_events side_thread = threading Thread target=side_thread_fn main_thread start side_thread start main_thread join side_thread join verify_events events mm_events = collections defaultdict int e events e name == aten mm mm_events e thread += assertEqual e input_shapes assertEqual len mm_events + int profile_all_threads v mm_events values assertEqual v n_rep verify_events returned_events test spawning thread within profiled region events = main_with_thread_fn profile_all_threads verify_events events skipIfTorchDynamo profiler gets ignored dynamo activated unittest skipIf kineto_available Kineto required test_python_gc_event activities = ProfilerActivity CPU payload x = torch randn y = torch randn record_function pre_gc torch mm x y gc collect record_function post_gc torch mm x y validate_json prof gc_collection_on TemporaryFileName mode= w+ fname prof export_chrome_trace fname open fname f events = json load f traceEvents Find required events gc_collection_on pre_gc = next e e events e name == pre_gc None post_gc = next e e events e name == post_gc None python_gc_events = e e events e name == Python GC Assert all required events present assertIsNotNone pre_gc pre_gc event missing assertIsNotNone post_gc post_gc event missing assertTrue len python_gc_events No Python GC events found Calculate boundaries pre_gc_end = pre_gc ts + pre_gc get dur post_gc_start = post_gc ts Assert each Python GC event correctly placed python_gc python_gc_events python_gc_start = python_gc ts python_gc_end = python_gc ts + python_gc get dur assertTrue python_gc_start pre_gc_end python_gc_end post_gc_start f Python GC event python_gc_start correctly placed python_gc_events = e e events e name == Python GC assertTrue len python_gc_events == Python GC event found when flag off gc_flag True False profile activities=activities experimental_config=torch _C _profiler _ExperimentalConfig record_python_gc_info=gc_flag with_stack=True prof payload validate_json prof gc_flag SimpleNet nn Module __init__ - None super __init__ fc = nn Linear fc = nn Linear forward x fc fc x dataclass frozen=True MockKinetoEvent _name str _start_us int _duration_us int _linked_correlation_id int _device_type int property name - str _name start_ns - int _start_us duration_ns - int _duration_us linked_correlation_id - int _linked_correlation_id device_type - DeviceType DeviceType CUDA _device_type == DeviceType CPU dataclass frozen=True MockProfilerEvent _name str id int start_time_ns int duration_time_ns int correlation_id int = children list MockProfilerEvent = field default_factory=list parent Optional MockProfilerEvent = None property end_time_ns start_time_ns + duration_time_ns property name - str _name __post__init__ parent children object __setattr__ parent parent object __setattr__ children children MockNode __init__ name children - None name = name children = MockNode name i name i children items TestExperimentalUtils TestCase make_tree - list MockNode tree = root_ root_ MockNode name i name i tree items test_dfs - None assertEqual join i name i _utils traverse_dfs make_tree root_ root_ test_bfs - None assertEqual join i name i _utils traverse_bfs make_tree root_ root_ staticmethod generate_mock_profile cuda_events = MockKinetoEvent cudaLaunchKernel MockKinetoEvent cudaLaunchKernel MockKinetoEvent cudaLaunchKernel MockKinetoEvent cudaLaunchKernel MockKinetoEvent cudaLaunchKernel MockKinetoEvent cudaLaunchKernel MockKinetoEvent GPU MockKinetoEvent GPU MockKinetoEvent GPU MockKinetoEvent GPU MockKinetoEvent GPU MockKinetoEvent GPU cpu_events = MockProfilerEvent CPU Before cudaLaunchKernel MockProfilerEvent CPU Before cudaLaunchKernel MockProfilerEvent CPU Before cudaLaunchKernel MockProfilerEvent CPU Before cudaLaunchKernel MockProfilerEvent CPU After cudaLaunchKernel MockProfilerEvent CPU After cudaLaunchKernel MockProfilerEvent CPU After cudaLaunchKernel MockProfilerEvent CPU After cudaLaunchKernel MockProfilerEvent CPU After GPU MockProfilerEvent CPU After GPU MockProfilerEvent CPU After GPU MockProfilerEvent CPU After GPU profiler = unittest mock Mock profiler kineto_results = unittest mock Mock profiler kineto_results events = unittest mock Mock return_value=cuda_events profiler kineto_results experimental_event_tree = unittest mock Mock return_value=cpu_events profiler staticmethod load_mock_profile accept = expecttest ACCEPT json_file_path = os path join os path dirname os path realpath __file__ profiler_utils_mock_events json accept torch cuda is_available garbage_code x i range x i = i x = torch ones device= cuda x = x x profile activities= ProfilerActivity CPU ProfilerActivity CUDA record_shapes=True with_stack=True prof _ range x = x x garbage_code x _ range x = x x kineto_events = _name e name _start_ns e start_ns _duration_ns e duration_ns _linked_correlation_id e linked_correlation_id _device_type e device_type == DeviceType CUDA e prof profiler kineto_results events EventTreeDFS event_tree collections deque stack = deque event_tree while stack curr_event = stack pop yield curr_event child_event curr_event children stack append child_event profiler_events = _name e name id e id start_time_ns e start_time_ns duration_time_ns e duration_time_ns correlation_id e correlation_id children child id child e children parent e parent id e parent None e EventTreeDFS prof profiler kineto_results experimental_event_tree open json_file_path w f json dump kineto_events profiler_events f assert os path exists json_file_path open json_file_path f kineto_events profiler_events = json load f cuda_events = MockKinetoEvent event values event kineto_events cpu_events = id_map = e profiler_events event = MockProfilerEvent e id_map event id = event cpu_events append event event cpu_events parent = None event parent None id_map event parent children = id_map child child event children event __post__init__ parent children cpu_events = event event cpu_events event parent None profiler = unittest mock Mock profiler kineto_results = unittest mock Mock profiler kineto_results events = unittest mock Mock return_value=cuda_events profiler kineto_results experimental_event_tree = unittest mock Mock return_value=cpu_events profiler test_utils_compute_self_time profile prof t t = torch ones requires_grad=True torch ones requires_grad=True z = torch add t t y = torch ones loss = torch nn functional binary_cross_entropy_with_logits z y loss backward basic_eval = _utils BasicEvaluation prof profiler metrics = basic_eval metrics assertTrue len metrics event_key event_metrics metrics items assertEqual event_metrics self_time_ns event_key event duration_time_ns - sum child duration_time_ns child event_key event children test_utils_intervals_overlap event = _utils EventKey MockProfilerEvent Event intervals = _utils Interval _utils Interval _utils Interval _utils Interval _utils Interval _utils Interval print event intervals_overlap intervals assertEqual event intervals_overlap intervals test_utils_compute_queue_depth format_queue_depth queue_depth_list events res = data event zip queue_depth_list events res += f data queue_depth event name \n res We have use Mock because time series data too flaky test profiler = generate_mock_profile basic_evaluation = _utils BasicEvaluation profiler assertExpectedInline format_queue_depth basic_evaluation queue_depth_list basic_evaluation cuda_events \ cudaLaunchKernel cudaLaunchKernel cudaLaunchKernel cudaLaunchKernel cudaLaunchKernel GPU GPU GPU GPU GPU cudaLaunchKernel GPU assertExpectedInline format_queue_depth basic_evaluation metrics k k basic_evaluation event_keys basic_evaluation events \ CPU Before cudaLaunchKernel CPU Before cudaLaunchKernel CPU Before cudaLaunchKernel CPU Before cudaLaunchKernel CPU After cudaLaunchKernel CPU After cudaLaunchKernel CPU After cudaLaunchKernel CPU After cudaLaunchKernel CPU After GPU CPU After GPU CPU After GPU CPU After GPU test_utils_compute_queue_depth_when_no_cuda_events For traces only cpu events we expect empty queue depth list x = torch ones profile prof _ range x = x x basic_evaluation = _utils BasicEvaluation prof profiler assertFalse basic_evaluation compute_queue_depth test_utils_compute_idle_time profiler = generate_mock_profile basic_evaluation = _utils BasicEvaluation profiler expected_output = \n join f basic_evaluation metrics event_key idle_time_ns event_key event name event_key basic_evaluation event_keys assertExpectedInline expected_output \ CPU Before cudaLaunchKernel CPU Before cudaLaunchKernel CPU Before cudaLaunchKernel CPU Before cudaLaunchKernel CPU After cudaLaunchKernel CPU After cudaLaunchKernel CPU After cudaLaunchKernel CPU After cudaLaunchKernel CPU After GPU CPU After GPU CPU After GPU CPU After GPU unittest skipIf IS_JETSON JSON behaving expected Jetson test_utils_get_optimizable_events basic_evaluation = _utils BasicEvaluation load_mock_profile optimizable_events = basic_evaluation get_optimizable_events print_enable=False expected_output = \n join f event_key event name event_key optimizable_events assertExpectedInline expected_output \ built-in function _cuda_synchronize aten copy_ test_profiler_name_pattern x = torch ones profile prof _ range x = x x x = x + x matched_events = NamePattern prof aten mm matched_events output = \n join f event name event matched_events assertExpectedInline output \ aten mm aten mm aten mm aten mm aten mm TODO Add logic CUDA version test unittest skipIf torch cuda is_available Test working CUDA test_profiler_pattern_match_helper x = torch ones profile prof _ range x = x x x = x + x event_tree = prof profiler kineto_results experimental_event_tree pattern = Pattern prof assertEqual pattern siblings_of event_tree assertEqual event_tree pattern siblings_of event_tree child_nodes = event_tree children assertEqual pattern siblings_of child_nodes assertEqual child_nodes pattern siblings_of child_nodes assertEqual event_tree pattern root_of event_tree children children assertEqual None pattern next_of event_tree - assertEqual event_tree pattern next_of event_tree assertEqual event_tree pattern prev_of event_tree unittest skipIf TEST_WITH_CROSSREF crossref intercepts calls changes callsite unittest skipIf torch cuda is_available CUDA required test_profiler_extra_cuda_copy_pattern cases = lambda torch ones device= cuda lambda torch ones cuda lambda torch zeros cuda lambda torch empty fill_ cuda lambda torch ones cuda lambda torch zeros cuda lambda torch empty fill_ cuda lambda torch rand cuda lambda torch randn cuda lambda torch full cuda lambda torch rand dtype=torch float lambda torch rand half lambda torch rand device= cuda half num_matched = _ fn cases profile with_stack=True record_shapes=True prof fn pattern = ExtraCUDACopyPattern prof num_matched append len pattern matched_events assertEqual num_matched i i _ cases unittest skipIf TEST_WITH_CROSSREF crossref intercepts calls changes callsite test_profiler_for_loop_indexing_pattern x = torch ones case i range x i = i case y = i range y += x i case y = i range y = x i case y = x _ range y = y x case i range x i = torch arange + i cases = case case case case case num_matched = _ fn cases profile with_stack=True prof fn pattern = ForLoopIndexingPattern prof num_matched append len pattern matched_events assertEqual num_matched i i _ cases unittest skipIf torch cuda is_available CUDA required test_profiler_fp _matmul_pattern x = torch ones device= cuda profile with_stack=True prof x = x x pattern = FP MatMulPattern prof has_tf = pattern skip num_matched = len pattern matched_events assertEqual num_matched has_tf unittest skipIf torch cuda is_available CUDA required test_profiler_extra_cuda_copy_pattern_benchmark profile with_stack=True record_shapes=True prof x = torch ones cuda x = torch ones cuda pattern = ExtraCUDACopyPattern prof shapes_factor_map = pattern benchmark pattern matched_events assertEqual len shapes_factor_map test_profiler_optimizer_single_tensor_pattern x = torch ones cases = lambda torch optim Adam model parameters lambda torch optim SGD model parameters lr= lambda torch optim AdamW model parameters lambda torch optim Adam model parameters foreach=True lambda torch optim SGD model parameters lr= foreach=True lambda torch optim AdamW model parameters foreach=True num_matched = _ fn cases profile with_stack=True prof model = nn Sequential nn Linear nn ReLU nn Linear optimizer = fn optimizer zero_grad y_hat = model x loss = torch nn functional cross_entropy y_hat torch randint loss backward optimizer step pattern = OptimizerSingleTensorPattern prof num_matched append len pattern matched_events assertEqual num_matched i i _ cases test_profiler_synchronized_dataloader_pattern dataset = torch rand sync_dataloader = torch utils data DataLoader dataset batch_size= async_dataloader = torch utils data DataLoader dataset batch_size= num_workers= profile with_stack=True prof next iter sync_dataloader next iter async_dataloader pattern = SynchronizedDataLoaderPattern prof num_matched = len pattern matched_events assertEqual num_matched skipIfTorchDynamo pattern checks aten _zero op which might there torch compile d graph test_profiler_grad_not_set_to_none_pattern x = torch ones model = nn Sequential nn Linear nn ReLU nn Linear optimizer = torch optim Adam model parameters cases = lambda optimizer zero_grad lambda model zero_grad lambda optimizer zero_grad set_to_none=False lambda model zero_grad set_to_none=False num_matched = _ fn cases profile with_stack=True prof y_hat = model x loss = torch nn functional cross_entropy y_hat torch randint loss backward optimizer step fn pattern = GradNotSetToNonePattern prof num_matched append len pattern matched_events assertEqual num_matched i i _ cases test_profiler_conv d_bias_followed_by_batchnorm d_pattern x = torch randn cases = nn Sequential nn Conv d nn BatchNorm d nn Sequential nn Conv d bias=False nn BatchNorm d nn Sequential nn Conv d num_matched = _ model cases profile with_stack=True record_shapes=True prof model x pattern = Conv dBiasFollowedByBatchNorm dPattern prof num_matched append len pattern matched_events assertEqual num_matched i i _ cases unittest skipIf torch cuda is_available CUDA required test_profiler_matmul_dim_fp _pattern cases = torch randn device= cuda dtype=torch float torch randn device= cuda dtype=torch float torch randn device= cuda dtype=torch float torch randn device= cuda dtype=torch float num_matched = _ x cases profile with_stack=True record_shapes=True prof x x pattern = MatMulDimInFP Pattern prof num_matched append len pattern matched_events assertEqual num_matched i i _ cases skipIfTorchDynamo profiler gets ignored dynamo activated test_profiler_pattern_matcher_json_report x = torch ones model = nn Sequential nn Linear nn ReLU nn Linear optimizer = torch optim Adam model parameters profile with_stack=True record_shapes=True prof y_hat = model x loss = torch nn functional cross_entropy y_hat torch randint loss backward optimizer step optimizer zero_grad tempfile TemporaryDirectory tmpdir report_all_anti_patterns prof json_report_dir=tmpdir print_enable=False open os path join tmpdir torchtidy_report json f report = json load f It platform dependent whether path will include profiler keys = k k report keys k endswith test_profiler py assertEqual len keys f keys entry = report keys assertTrue len entry expected_fields = sorted line_number name url message event entry actual_fields = sorted event keys assertEqual expected_fields actual_fields unittest skipIf IS_ARM IS_LINUX x linux only cpp unwinding test_fuzz_symbolize generate some random addresses text section make sure symbolizers do throw exceptions crash get_text_sections text_sections = seen = set filename os listdir proc map_files library = os readlink proc map_files + filename so library library seen continue seen add library open os path join proc map_files library rb f mm = mmap mmap f fileno prot=mmap PROT_READ unpack fmt offset struct unpack fmt mm offset offset + struct calcsize fmt mm = b \x fELF continue section_headers_start = unpack Q section_header_size = unpack H num_section_headers = unpack H shstrndx = unpack H shstrtab_offset = unpack Q section_headers_start + shstrndx section_header_size + i range num_section_headers section_name_offset = unpack I section_headers_start + i section_header_size name_start = shstrtab_offset + section_name_offset section_name = mm name_start name_start + section_name = b text\ continue section_offset = unpack Q section_headers_start + i section_header_size + section_size = unpack Q section_headers_start + i section_header_size + start = int filename split - + section_offset text_sections append start section_size break mm close text_sections r = random Random r seed text_sections = get_text_sections addrs = _ range s = r randrange len text_sections start size = text_sections s addr = r randrange start start + size addrs append addr fast = torch _C _profiler symbolize_addresses addrs fast dladdr = torch _C _profiler symbolize_addresses addrs dladdr addr line = torch _C _profiler symbolize_addresses addrs addr line assertEqual len fast len addrs assertEqual len addr line len fast test_profiler_overload_names torch library _scoped_library fallthrough_kernel validate_json prof print TemporaryFileName mode= w+ fname prof export_chrome_trace fname open fname f events = json load f traceEvents assertTrue any aten add Tensor e name e events assertTrue any aten add out e name e events _scoped_library aten IMPL my_lib my_lib impl add Tensor fallthrough_kernel CPU experimental_config = torch _C _profiler _ExperimentalConfig capture_overload_names=True profile experimental_config=experimental_config activities= ProfilerActivity CPU prof torch add The following execution trace expected Dispatch trace call op= aten add Tensor key= AutogradCPU redispatch op= aten add Tensor key= Undefined call op= aten empty memory_format key= BackendSelect redispatch op= aten empty memory_format key= CPU call op= aten add out key= CPU prof table --------------- --------------- ------------ ------------ ------------ ------------ ------------ ------------ Name Overload Name Self CPU Self CPU CPU total CPU total CPU time avg Calls --------------- --------------- ------------ ------------ ------------ ------------ ------------ ------------ aten add Tensor us us us aten empty memory_format us us us aten add out us us us --------------- --------------- ------------ ------------ ------------ ------------ ------------ ------------ aten add out aten empty memory_format children aten add Tensor aten_add_parent list FunctionEvent = event event prof events len event cpu_children == assert len aten_add_parent == aten_add_parent = aten_add_parent assert aten_add_parent overload_name == Tensor aten_add_out_event = c c aten_add_parent cpu_children c overload_name == out assert len aten_add_out_event == Without group_by_overload_name overload name ignored key averages key_averages = prof key_averages assert len key_averages == assert Overload Name key_averages table key_averages = prof key_averages group_by_overload_name=True assert len key_averages == assert Overload Name key_averages table validate_json prof test_expose_kineto_event_metadata check_metadata prof op_name metadata_key TemporaryFileName mode= w+ fname prof export_chrome_trace fname open fname f events = json load f traceEvents found_op = False e events name e args e e name == op_name assert metadata_key e args f Metadata op_name Chrome trace did contain metadata_key found_op = True assert found_op f Could find op op_name Chrome trace found_op = False event prof events event name == op_name assert metadata_key event metadata_json f Metadata op_name FunctionEvent did contain metadata_key found_op = True assert found_op f Could find op op_name prof events experimental_config = torch _C _profiler _ExperimentalConfig expose_kineto_event_metadata=True profile experimental_config=experimental_config activities= ProfilerActivity CPU prof torch add check_metadata prof op_name= aten add metadata_key= Ev Idx unittest skipIf torch cuda is_available requries CUDA test_profiler_debug_autotuner This test makes sure profiling events will present when kernel run using DebugAutotuner is_big_gpu raise unittest SkipTest requires large gpu max-autotune = torch randn device= cuda dtype=torch float = torch randn device= cuda dtype=torch float mm torch mm pb_mm = torch compile mm options= benchmark_kernel True max_autotune True max_autotune_gemm_backends TRITON profile_bandwidth True comp_mm = torch compile mm options= benchmark_kernel True max_autotune True max_autotune_gemm_backends TRITON profile prof pb_mm profile prof comp_mm names prof ev name ev prof events mm ev name triton ev name n = names prof n = names prof assertEqual n n __name__ == __main__ run_tests