Owner s module dynamo enum functools pprint re unittest warnings copy deepcopy functorch experimental control_flow control_flow torch torch _dynamo config config torch _dynamo test_case torch _functorch config torch nn nn torch utils _pytree pytree torch utils checkpoint torch _dynamo backends common aot_autograd torch _dynamo testing check_dynamic_shape_capture CompileCounter CompileCounterWithBackend EagerAndRecordGraphs empty_line_normalizer normalize_gm torch _dynamo utils counters ifdynstaticdefault torch _higher_order_ops hints_wrap hints_wrapper torch _higher_order_ops wrap wrap torch testing _internal common_device_type instantiate_device_type_tests ops torch testing _internal common_utils munge_exc parametrize TEST_WITH_TORCHDYNAMO xfailIfTorchDynamo torch testing _internal hop_db hop_db torch testing _internal logging_utils LoggingTestCase make_logging_test torch testing _internal triton_utils requires_cuda_and_triton count_ops gm args freq op actual = node target node gm graph nodes count op assert actual == freq f expected= freq actual= actual gm Obj pass MyModule nn Module __init__ - None super __init__ existing = torch nn Parameter torch ones forward x existing x global_obj = Obj global_module = MyModule global_var = torch randn global_num = global_list = find_first_node gm func node gm graph nodes node target func node None op_count gm result = node gm graph nodes call node op result += result Checks dict matches dict regex keys That keys regex expressions assert_dict_matches_regex dct dct_with_regex_keys regex_keys = dct_with_regex_keys keys regex_key_to_actual_key = regex_key regex_keys key dct re match regex_key key regex_key regex_key_to_actual_key raise AssertionError f Single key regex mapped multiple keys Please improve your f regex Got regex= regex_key f keys= regex_key_to_actual_key regex_key f key regex_key_to_actual_key regex_key = key new_dct = regex_key regex_keys regex_key regex_key_to_actual_key raise AssertionError f Got regex regex_key could match any key dict f keys dct keys new_dct regex_key_to_actual_key regex_key = dct_with_regex_keys regex_key assertEqual dct new_dct default_args_generator seed_value flat_args args_spec = pytree tree_flatten seed_value i range new_flat_arg = val flat_args isinstance val torch Tensor new_val = val + i isinstance val int new_val = val + i isinstance val float new_val = val + i isinstance val enum Enum new_val = val raise AssertionError unexpected arg type new_flat_arg append new_val new_args = pytree tree_unflatten new_flat_arg args_spec yield new_args HigherOrderOpTests torch _dynamo test_case TestCase _assert_wrap_fallback func args setup=lambda None counters clear backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend setup expected = func args setup result = torch compile func backend=cnt fullgraph=False args num_graph_breaks = len counters graph_break keys assertGreater num_graph_breaks gm backend graphs node gm graph nodes assertFalse node target wrap assertEqual result expected _test_wrap_simple func args_generator expected_num_wrap_args expected_opcount= return_graph=False Given ` func ` has single call ` wrap ` we check - there no graph breaks - eager vs torch compile has same result correctness - other compilation metrics e g ops dynamo captured graph wrap has expected number args etc we have one multiple runs through each args args_generator we will check - correctness no graph breaks every run - other compilation metrics only first run since automatic_dynamic_shapes may compile another dynamic version graph later runs graph = None i args enumerate args_generator backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend expected = func args result = torch compile func fullgraph=True backend=cnt args check correctness no graph breaks assertEqual result expected assertEqual cnt frame_count assertEqual len backend graphs check other compilation metrics i == assertEqual cnt op_count expected_opcount graph = backend graphs wrap_node = find_first_node graph wrap assertEqual len wrap_node args expected_num_wrap_args We always check graph first run return_graph = True return_graph normalize_gm graph print_readable print_output=False test_error_message_sane foo = inner x foo append x x clone torch compile backend= eager fullgraph=True f x wrap inner x x = torch randn assertRaisesRegex torch _dynamo exc Unsupported r HigherOrderOperator Mutating variable current scope \ SideEffects\ f x test_no_freevars f x wrap lambda x torch sin x x x = torch randn arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x arg_count test_enum_arg SomeEnum enum Enum A = B = g x val val == SomeEnum A torch sin x torch cos x f x val wrap g x val x = torch randn arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x SomeEnum A arg_count test_return_captured_var freevar = torch randn test x freevar fn x wrap test x x = torch randn Since ` x ` unused we don t lift input when testing dynamic shape symbols lifted input arg_count = ifdynstaticdefault _test_wrap_simple fn default_args_generator x arg_count test_return_captured_vars freevar = torch randn freevar = torch randn test x freevar freevar freevar fn x wrap test x x = torch randn Since ` x ` unused we don t lift input when testing dynamic shape symbol lifted input arg_count = ifdynstaticdefault _test_wrap_simple fn default_args_generator x arg_count test_return_captured_var_used_multiple_times freevar = torch randn test x y = x + freevar y freevar fn x wrap test x x = torch randn when testing dynamic shape symbol lifted input arg_count = ifdynstaticdefault _test_wrap_simple fn default_args_generator x arg_count test_capture_untracked_global f x wrap lambda x x + global_var x x = torch randn when testing dynamic shape symbol lifted input arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x arg_count test_allow_python_side_effects_utility torch _dynamo utils _disable_side_effect_safety_checks_for_current_subtracer torch _higher_order_ops wrap dynamo_bypassing_wrapper wrapper fn fn count = does_side_effect x nonlocal count count += x sin does_side_effect_wrapped args kwargs _disable_side_effect_safety_checks_for_current_subtracer does_side_effect args kwargs torch compile backend= eager fullgraph=True fn x dynamo_bypassing_wrapper wrapper does_side_effect_wrapped x x = torch tensor fn x inner_does_side_effect x nonlocal count count += x Test any nested HOPs unaffected outer x dynamo_bypassing_wrapper wrapper inner_does_side_effect x outer_wrapped args kwargs _disable_side_effect_safety_checks_for_current_subtracer outer args kwargs torch compile backend= eager fullgraph=True fn_nested x dynamo_bypassing_wrapper wrapper outer_wrapped x x = torch tensor assertRaisesRegex RuntimeError Mutating variable current scope fn_nested x test_symint_input f x i = x size wrap lambda x i x view i x i x = torch randn _test_wrap_simple f default_args_generator x ifdynstaticdefault expected_opcount= test_symint_in_slice f x i = x size - j = x size - k = x size wrap lambda x x i j k x x = torch randn _test_wrap_simple f default_args_generator x basic symbols compound symbols ifdynstaticdefault more sym expression computation expected_opcount=ifdynstaticdefault test_wrap_pytree_args_nested f x y z fn d d x sin + d y cos - d y sin wrap fn d x = torch tensor y = torch tensor z = torch tensor d = x x y y x y z my_args_generator t yield t yield t + t t yield t t + t actual_graph = _test_wrap_simple f my_args_generator x y z return_graph=True assertExpectedInline actual_graph \ GraphModule torch nn Module forward L_d_x_ f L_d_y_ _ f L_d_y_ _ _ f l_d_x_ = L_d_x_ l_d_y_ _ = L_d_y_ _ l_d_y_ _ _ = L_d_y_ _ _ wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ l_d_x_ l_d_y_ _ l_d_y_ _ _ wrap_body_ = l_d_x_ = l_d_y_ _ = l_d_y_ _ _ = None getitem f = wrap wrap = None getitem wrap_body_ torch nn Module forward l_d_x_ f l_d_y_ _ f l_d_y_ _ _ f sin f = l_d_x_ sin l_d_x_ = None cos f = l_d_y_ _ cos l_d_y_ _ = None add f = sin + cos sin = cos = None sin_ f = l_d_y_ _ _ sin l_d_y_ _ _ = None sub f = add - sin_ add = sin_ = None sub NOQA B test_wrap_pytree_args_with_symint_constant f x y i = x size wrap lambda t t view t + t x y i x = torch randn y = actual_graph = _test_wrap_simple f default_args_generator x y ifdynstaticdefault expected_opcount= return_graph=True torch _dynamo config assume_static_by_default assertExpectedInline actual_graph \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ l_x_ wrap_body_ = l_x_ = None getitem f = wrap wrap = None getitem wrap_body_ torch nn Module forward l_x_ f view f = l_x_ view l_x_ = None add f = view + view = None add assertExpectedInline actual_graph \ GraphModule torch nn Module forward s Sym s L_x_ f s l_x_ = L_x_ wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ s l_x_ wrap_body_ = s = l_x_ = None getitem f s = wrap wrap = None getitem wrap_body_ torch nn Module forward s Sym s l_x_ f s view f s = l_x_ view s l_x_ = s = None add f s = view + view = None add test_wrap_pytree_kwargs f x y z fn x y z z _ = z x + y + z wrap fn x=x y=y z=z x = torch randn y = torch randn my_args_generator t yield t x = t + y = t + yield x y x y x = t + y = t + yield x y x y arg_count = ifdynstaticdefault _test_wrap_simple f my_args_generator x y x y arg_count test_wrap_pytree_args_not_const_symint_tensor MyClass __init__ x val = x f x y wrap lambda z z sin z val cos x y x = torch tensor y = MyClass torch tensor _test_wrap_simple f x y test_capture_constants x = torch randn fn x y z z x + y x y f x y z wrap fn x y z args = x None opt_f = torch compile f fullgraph=True backend=CompileCounter expected = f args result = opt_f args assertEqual result expected Ensure we recompile here args = x None expected = f args result = opt_f args assertEqual result expected test_capture_untracked_global_nested backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend torch compile backend=cnt fullgraph=True f x wrap lambda x wrap lambda x x + global_var x x x = torch randn result = f x assertEqual result x + global_var assertEqual cnt frame_count assertEqual cnt op_count assertEqual len backend graphs wrap_node = find_first_node backend graphs wrap assertTrue len wrap_node args body_function = getattr backend graphs wrap_node args name assertEqual op_count body_function inner_wrap_node = find_first_node body_function wrap assertTrue len inner_wrap_node args test_capture_untracked_nonlocal x = torch randn y = torch randn f x y g x wrap lambda x x + y x when testing dynamic shape symbol lifted input arg_count = ifdynstaticdefault _test_wrap_simple g default_args_generator x arg_count g x f x y test_capture_tracked x = torch randn y = torch randn f x y wrap lambda x x + y x when testing dynamic shape symbol lifted input arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x y arg_count test_capture_tracked_nested x = torch randn y = torch randn f x y wrap lambda x wrap lambda x x + y x x when testing dynamic shape symbol lifted input arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x y arg_count test_inlined_functions g x y x + y f x y wrap lambda x g x y x x = torch randn y = torch randn when testing dynamic shape symbol lifted input arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x y arg_count test_same_freevar_twice free = torch randn g x y = free sin z = free cos y z f x wrap g x x = torch randn Since ` x ` unused we don t lift input when testing dynamic shape symbol lifted input arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x arg_count torch _dynamo config patch capture_scalar_outputs=True test_unbacked_symbol_closure f x c = x sum item g x k x x + c wrap k x wrap g x x = torch randn arg_count = ifdynstaticdefault out_graph = _test_wrap_simple f default_args_generator x arg_count return_graph=True check_dynamic_shape_capture assertExpectedInline out_graph \ GraphModule torch nn Module forward s Sym s L_x_ f s l_x_ = L_x_ sum_ f = l_x_ sum item Sym zuf = sum_ item sum_ = None wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ s l_x_ item wrap_body_ = s = l_x_ = item = None getitem f s = wrap wrap = None getitem wrap_body_ torch nn Module forward s Sym s l_x_ f s item Sym zuf wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ s l_x_ item wrap_body_ = s = l_x_ = item = None getitem f s = wrap wrap = None getitem wrap_body_ torch nn Module forward s Sym s l_x_ f s item Sym zuf add f s = l_x_ + item l_x_ = item = None add assertExpectedInline out_graph \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ sum_ f = l_x_ sum item Sym zuf = sum_ item sum_ = None wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ l_x_ item wrap_body_ = l_x_ = item = None getitem f = wrap wrap = None getitem wrap_body_ torch nn Module forward l_x_ f item Sym zuf wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ l_x_ item wrap_body_ = l_x_ = item = None getitem f = wrap wrap = None getitem wrap_body_ torch nn Module forward l_x_ f item Sym zuf add f = l_x_ + item l_x_ = item = None add torch _dynamo config patch capture_dynamic_output_shape_ops=True test_tensor_with_unbacked_shape_closure f x c = x nonzero g x k x x sin c sin wrap k x wrap g x x = torch randn arg_count = ifdynstaticdefault when compiled dynamic we don t have upper bound runtime assertions u expected_op_count = ifdynstaticdefault out_graph = _test_wrap_simple f default_args_generator x arg_count expected_op_count return_graph=True check_dynamic_shape_capture assertExpectedInline out_graph \ GraphModule torch nn Module forward s Sym s L_x_ f s l_x_ = L_x_ c i u = l_x_ nonzero sym_size_int_ Sym u = torch ops aten sym_size int c _check_is_size = torch _check_is_size sym_size_int_ _check_is_size = None ge Sym u = = sym_size_int_ = _assert_scalar_default = torch ops aten _assert_scalar default ge Runtime assertion failed expression u = node ge ge = _assert_scalar_default = None wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ s l_x_ sym_size_int_ c wrap_body_ = s = l_x_ = sym_size_int_ = c = None getitem f s = wrap getitem_ f u = wrap wrap = None getitem getitem_ wrap_body_ torch nn Module forward s Sym s l_x_ f s u Sym u c i u wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ s l_x_ u c wrap_body_ = s = l_x_ = u = c = None child f s = wrap child_ f u = wrap wrap = None child child_ wrap_body_ torch nn Module forward s Sym s l_x_ f s u Sym u c i u child f s = l_x_ sin l_x_ = None child_ f u = c sin c = None child child_ assertExpectedInline out_graph \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ c i u = l_x_ nonzero sym_size_int_ Sym u = torch ops aten sym_size int c _check_is_size = torch _check_is_size sym_size_int_ _check_is_size = None ge Sym u = = sym_size_int_ = _assert_scalar_default = torch ops aten _assert_scalar default ge Runtime assertion failed expression u = node ge ge = _assert_scalar_default = None le Sym u = = sym_size_int_ = _assert_scalar_default_ = torch ops aten _assert_scalar default le Runtime assertion failed expression u = node le le = _assert_scalar_default_ = None wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ l_x_ sym_size_int_ c wrap_body_ = l_x_ = sym_size_int_ = c = None getitem f = wrap getitem_ f u = wrap wrap = None getitem getitem_ wrap_body_ torch nn Module forward l_x_ f u Sym u c i u wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ l_x_ u c wrap_body_ = l_x_ = u = c = None child f = wrap child_ f u = wrap wrap = None child child_ wrap_body_ torch nn Module forward l_x_ f u Sym u c i u child f = l_x_ sin l_x_ = None child_ f u = c sin c = None child child_ torch _dynamo config patch capture_dynamic_output_shape_ops=True capture_scalar_outputs=True test_tensor_to_list_closure f x li = x tolist g x k x li + x wrap k x wrap g x x = torch tensor dtype=torch int arg_count = ifdynstaticdefault out_graph = _test_wrap_simple f x arg_count return_graph=True tolist will specialize input shapes so dynamic static tests have same graph assertExpectedInline out_graph \ GraphModule torch nn Module forward L_x_ i l_x_ = L_x_ getitem = l_x_ item Sym u = getitem item getitem = None wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ item l_x_ wrap_body_ = item = l_x_ = None getitem_ i = wrap wrap = None getitem_ wrap_body_ torch nn Module forward item Sym u l_x_ i wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ item l_x_ wrap_body_ = item = l_x_ = None getitem i = wrap wrap = None getitem wrap_body_ torch nn Module forward item Sym u l_x_ i add i = item + l_x_ item = l_x_ = None add torch _dynamo config patch capture_dynamic_output_shape_ops=True test_tensor_and_unbacked_symbol_closure f x c = x nonzero sz = c size g x k x x sin + sz c sin wrap k x wrap g x x = torch randn arg_count = ifdynstaticdefault when compiled dynamic we don t have upper bound runtime assertions u expected_op_count = ifdynstaticdefault out_graph = _test_wrap_simple f default_args_generator x arg_count expected_op_count return_graph=True Note u accessed sz shape c We cached via symbol u de-duplicate them check_dynamic_shape_capture assertExpectedInline out_graph \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ c i u = l_x_ nonzero sym_size_int Sym u = torch ops aten sym_size int c _check_is_size = torch _check_is_size sym_size_int _check_is_size = None ge Sym u = = sym_size_int = _assert_scalar_default = torch ops aten _assert_scalar default ge Runtime assertion failed expression u = node ge ge = _assert_scalar_default = None le Sym u = = sym_size_int = _assert_scalar_default_ = torch ops aten _assert_scalar default le Runtime assertion failed expression u = node le le = _assert_scalar_default_ = None wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ l_x_ sym_size_int c wrap_body_ = l_x_ = sym_size_int = c = None getitem f = wrap getitem_ f u = wrap wrap = None getitem getitem_ wrap_body_ torch nn Module forward l_x_ f size Sym u c i u wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ l_x_ size c wrap_body_ = l_x_ = size = c = None child f = wrap child_ f u = wrap wrap = None child child_ wrap_body_ torch nn Module forward l_x_ f size Sym u c i u sin f = l_x_ sin l_x_ = None child f = sin + size sin = size = None child_ f u = c sin c = None child child_ torch _dynamo config patch capture_dynamic_output_shape_ops=True test_concat_unbacked_shape_tensor f x y c = x nonzero d = y nonzero cat = torch cat c d g x k x cat sum + x wrap k x wrap g x x = torch randn y = torch randn arg_count = ifdynstaticdefault when compiled dynamic we don t have upper bound runtime assertions u u expected_op_count = ifdynstaticdefault out_graph = _test_wrap_simple f default_args_generator x y arg_count expected_op_count return_graph=True check_dynamic_shape_capture assertExpectedInline out_graph \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ c i u = l_x_ nonzero sym_size_int_ Sym u = torch ops aten sym_size int c _check_is_size = torch _check_is_size sym_size_int_ _check_is_size = None ge Sym u = = sym_size_int_ = _assert_scalar_default = torch ops aten _assert_scalar default ge Runtime assertion failed expression u = node ge ge = _assert_scalar_default = None le Sym u = = sym_size_int_ = _assert_scalar_default_ = torch ops aten _assert_scalar default le Runtime assertion failed expression u = node le le = _assert_scalar_default_ = None d i u = l_y_ nonzero l_y_ = None sym_size_int_ Sym u = torch ops aten sym_size int d _check_is_size_ = torch _check_is_size sym_size_int_ _check_is_size_ = None ge_ Sym u = = sym_size_int_ = _assert_scalar_default_ = torch ops aten _assert_scalar default ge_ Runtime assertion failed expression u = node ge_ ge_ = _assert_scalar_default_ = None le_ Sym u = = sym_size_int_ = _assert_scalar_default_ = torch ops aten _assert_scalar default le_ Runtime assertion failed expression u = node le_ le_ = _assert_scalar_default_ = None cat i u + u = torch cat c d c = d = None wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ sym_size_int_ sym_size_int_ cat l_x_ wrap_body_ = sym_size_int_ = sym_size_int_ = cat = l_x_ = None getitem f = wrap wrap = None getitem wrap_body_ torch nn Module forward u Sym u u Sym u cat i u + u l_x_ f wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ u u cat l_x_ wrap_body_ = u = u = cat = l_x_ = None getitem f = wrap wrap = None getitem wrap_body_ torch nn Module forward u Sym u u Sym u cat i u + u l_x_ f sum_ i = cat sum cat = None add f = sum_ + l_x_ sum_ = l_x_ = None add torch _dynamo config patch assume_static_by_default=False dynamic_shapes=True test_lift_tensors_with_shared_symbols f x y g x k x x y wrap k x wrap g x x = torch randn y = torch randn out_graph = _test_wrap_simple f default_args_generator x y return_graph=True assertExpectedInline out_graph \ GraphModule torch nn Module forward s Sym s s Sym s L_x_ f s s s Sym s L_y_ f s s l_x_ = L_x_ l_y_ = L_y_ wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ s s l_x_ s l_y_ wrap_body_ = s = s = l_x_ = s = l_y_ = None getitem f s s = wrap wrap = None getitem wrap_body_ torch nn Module forward s Sym s s Sym s l_x_ f s s s Sym s l_y_ f s s wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ s s l_x_ s l_y_ wrap_body_ = s = s = l_x_ = s = l_y_ = None getitem f s s = wrap wrap = None getitem wrap_body_ torch nn Module forward s Sym s s Sym s l_x_ f s s s Sym s l_y_ f s s matmul f s s = l_x_ l_y_ l_x_ = l_y_ = None matmul torch _dynamo config patch assume_static_by_default=False dynamic_shapes=True capture_dynamic_output_shape_ops=True test_lift_tensors_with_compound_expressions f x y x = x view - c = y nonzero d = torch concat x c g x k x d sum + x wrap k x wrap g x x = torch randn y = torch randn f x y check_dynamic_shape_capture out_graph = _test_wrap_simple f default_args_generator x y return_graph=True assertExpectedInline out_graph \ GraphModule torch nn Module forward s Sym s s Sym s L_x_ f s s s Sym s L_y_ f s s l_x_ = L_x_ l_y_ = L_y_ x f s s s s s s = l_x_ view - l_x_ = None c i u = l_y_ nonzero l_y_ = None sym_size_int_ Sym u = torch ops aten sym_size int c _check_is_size = torch _check_is_size sym_size_int_ _check_is_size = None ge Sym u = = sym_size_int_ = _assert_scalar_default = torch ops aten _assert_scalar default ge Runtime assertion failed expression u = node ge ge = _assert_scalar_default = None d f u + s s s s s s = torch concat x c c = None wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ sym_size_int_ s s d x wrap_body_ = sym_size_int_ = s = s = d = x = None getitem f s s s s s s = wrap wrap = None getitem wrap_body_ torch nn Module forward u Sym u s Sym s s Sym s d f u + s s s s s s x f s s s s s s wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ u s s d x wrap_body_ = u = s = s = d = x = None getitem f s s s s s s = wrap wrap = None getitem wrap_body_ torch nn Module forward u Sym u s Sym s s Sym s d f u + s s s s s s x f s s s s s s sum_ f = d sum d = None add f s s s s s s = sum_ + x sum_ = x = None add test_register_subclass torch _higher_order_ops cond cond_op torch testing _internal two_tensor TwoTensor = torch tensor b = torch randn t = TwoTensor b assertRaisesRegex NotImplementedError no rule registered HOP cond subclass TwoTensor res = cond_op sum torch sin torch cos t called = Using cond py_impl cond_op py_impl TwoTensor _ pred true_fn false_fn operands nonlocal called called += assert len operands == = cond_op pred true_fn false_fn operands b = cond_op pred true_fn false_fn operands b TwoTensor b res = cond_op sum torch sin torch cos t assertEqual res torch sin assertEqual res b torch sin b assertEqual called test_register_mode torch _higher_order_ops cond cond_op torch_dispatch_called = MyMode torch utils _python_dispatch TorchDispatchMode __torch_dispatch__ func types args= kwargs=None nonlocal torch_dispatch_called torch_dispatch_called += func args kwargs = torch tensor pred = sum assertRaisesRegex NotImplementedError no rule registered HigherOrderOperator cond mode MyMode MyMode res = cond_op pred torch sin torch cos py_impl_called = Using cond py_impl cond_op py_impl MyMode _ mode pred true_fn false_fn operands nonlocal py_impl_called py_impl_called += cond_op pred true_fn false_fn operands = torch tensor pred = sum MyMode res = cond_op pred torch sin torch cos assertEqual res sin test_capture_value_created_in_subgraph backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend x = torch randn y = torch randn inner x y z = x + y wrap lambda x wrap lambda x x + z x x torch compile backend=cnt fullgraph=True f x y wrap inner x y result = f x y assertEqual result x + y + x assertEqual cnt frame_count assertEqual cnt op_count assertEqual len backend graphs No changes args outer wrap gm = backend graphs wrap_node = find_first_node gm wrap assertTrue len wrap_node args z lifted arg inner wrap body_function = getattr gm wrap_node args name addition + wrap + getitem assertEqual op_count body_function inner_wrap_node = find_first_node body_function wrap assertTrue len inner_wrap_node args Innermost body function z also lifted arg body_function = getattr body_function inner_wrap_node args name assertEqual op_count body_function inner_wrap_node = find_first_node body_function wrap assertTrue len inner_wrap_node args test_side_effect_set_new_attr_global_obj setup global global_obj global_obj = Obj f x h x g x global_obj foo = x + x clone y = wrap g x y + global_obj foo h x x = torch zeros _assert_wrap_fallback f x setup=setup test_side_effect_set_existing_attr_global_obj setup global global_obj global_obj = Obj global_obj foo = nn Parameter torch tensor f x h x g x global_obj foo = x + x clone y = wrap g x y + global_obj foo h x x = torch zeros _assert_wrap_fallback f x setup=setup test_side_effect_del_existing_attr_global_obj setup global global_obj global_obj = Obj global_obj foo = torch tensor f x h x g x del global_obj foo x clone y = wrap g x y h x x = torch zeros _assert_wrap_fallback f x setup=setup test_side_effect_set_new_attr_global_module setup global global_module global_module = MyModule h x g x global_module foo = nn Parameter x + x clone y = wrap g x y + global_module foo x = torch zeros _assert_wrap_fallback h x setup=setup test_side_effect_set_existing_attr_global_module setup global global_module global_module = MyModule h x g x global_module existing = nn Parameter torch tensor global_module x y = wrap g x y x = torch zeros _assert_wrap_fallback h x setup=setup test_side_effect_del_existing_attr_global_module setup global global_module global_module = MyModule h x g x del global_module existing x clone y = wrap g x y x = torch zeros _assert_wrap_fallback h x setup=setup test_side_effect_mutate_global_num setup global global_num global_num = f x g x global global_num global_num = global_num + x + global_num y = wrap g x y + global_num x = torch zeros _assert_wrap_fallback f x setup=setup test_side_effect_mutate_global_num_builtin setup global global_num global_num = f x g x global global_num global_num += x + global_num y = wrap g x y + global_num x = torch zeros _assert_wrap_fallback f x setup=setup test_side_effect_mutate_global_tensor setup global global_var global_var = torch ones f x g x global global_var global_var = global_var + x + global_var y = wrap g x y + global_var x = torch zeros _assert_wrap_fallback f x setup=setup test_side_effect_mutate_global_tensor_builtin setup global global_var global_var = torch ones f x g x global global_var global_var += x + global_var y = wrap g x y + global_var x = torch zeros _assert_wrap_fallback f x setup=setup test_side_effect_mutate_global_list setup global global_list global_list = f x g x val = x + global_list append val global_list - y = wrap g x z = y + global_list - z x = torch zeros _assert_wrap_fallback f x setup=setup test_side_effect_mutate_nonlocal_num f x h x val = g x nonlocal val val = val + x + val y = wrap g x z = y + val z h x x = torch zeros _assert_wrap_fallback f x test_side_effect_set_new_attr_nonlocal_obj f x h x obj = Obj g x obj val = x dim x clone y = wrap g x z = y + obj val z h x x = torch zeros _assert_wrap_fallback f x test_side_effect_set_existing_attr_nonlocal_obj f x h x obj = Obj obj val = g x obj val = x dim x clone y = wrap g x z = y + obj val z h x x = torch zeros _assert_wrap_fallback f x test_side_effect_del_existing_attr_nonlocal_obj f x h x obj = Obj obj val = g x del obj val x clone y = wrap g x y h x x = torch zeros _assert_wrap_fallback f x test_side_effect_set_new_attr_nonlocal_module h x obj = MyModule g x obj val = x dim x clone y = wrap g x z = y + obj val z x = torch zeros _assert_wrap_fallback h x test_side_effect_set_existing_attr_nonlocal_module h x obj = MyModule g x obj existing = nn Parameter torch tensor obj x y = wrap g x y x = torch zeros _assert_wrap_fallback h x test_side_effect_del_existing_attr_nonlocal_module h x obj = MyModule g x del obj existing x clone y = wrap g x y x = torch zeros _assert_wrap_fallback h x test_side_effect_mutate_nonlocal_tensor f x h x val = torch tensor g x nonlocal val val = val + x + val y = wrap g x z = y + val z h x x = torch zeros _assert_wrap_fallback f x test_side_effect_mutate_nonlocal_num_builtin f x h x val = g x nonlocal val val += x + val y = wrap g x z = y + val z h x x = torch zeros _assert_wrap_fallback f x test_side_effect_mutate_nonlocal_tensor_builtin f x h x val = torch tensor g x nonlocal val val += x + val y = wrap g x z = y + val z h x x = torch zeros _assert_wrap_fallback f x test_side_effect_nonlocal_list_append_graph_break g x y = f k m = k + y append m k wrap f x y x = torch randn _assert_wrap_fallback g x test_side_effect_nested_nonlocal_list_append_graph_break g x h x y = f k m = k + y append m k wrap f x y h x x = torch randn _assert_wrap_fallback g x test_side_effect_local_list_append_no_graph_break g x f k y = y append k + y wrap f x x = torch randn arg_count = ifdynstaticdefault _test_wrap_simple g default_args_generator x arg_count test_wrap_kwarg f x y wrap lambda x y x + y x y=y x = torch randn y = torch randn arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x y arg_count test_wrap_kwarg_int f x y wrap lambda x y x + y x y=y x = torch randn y = arg_count = ifdynstaticdefault + check_dynamic_shape_capture ifdynstaticdefault _test_wrap_simple f default_args_generator x y arg_count test_wrap_all_kwarg f y x wrap lambda x y x + y x=x y=y x = torch randn y = torch randn arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x y arg_count test_wrap_kwarg_only f x y fn x y x + y wrap fn x=x y=y x = torch randn y = torch randn arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x y arg_count test_wrap_kwarg_default f x y fn x y z= x + y + z wrap fn x=x y=y x = torch randn y = torch randn arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x y arg_count test_wrap_kwarg_default_if_branch f x y fn x y z=None z None x + y x wrap fn x=x y=y x = torch randn y = torch randn arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x y arg_count test_wrap_kwarg_recompile f x y z=None fn x y z=None z None x + y x wrap fn x=x y=y z=z x = torch randn y = torch randn counters clear opt = torch compile f backend= eager fullgraph=True opt x y assertEqual counters stats calls_captured verify we ` don t ` recompile opt x y assertEqual counters stats calls_captured output = opt x y assertEqual counters stats calls_captured assertEqual output x test_wrap_kwarg_default_else_branch f x y z fn x y z=None z None x + y x wrap fn x=x y=y z=z x = torch randn y = torch randn arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x y arg_count test_map_subgraph_name_is_valid xs = torch randn y = torch randn map_f xs y inner x y inner x y x + y control_flow map inner x y control_flow map inner xs y graphs = _check_map_graph_and_extract map_f xs y graphs graph body_graph = graphs assertExpectedInline graph \ forward L_xs_ torch Tensor L_y_ torch Tensor l_xs_ = L_xs_ l_y_ = L_y_ map_body_ = map_body_ map_impl = torch ops higher_order map_impl map_body_ l_xs_ l_y_ map_body_ = l_xs_ = l_y_ = None getitem = map_impl map_impl = None getitem assertExpectedInline body_graph \ forward child torch Tensor l_y_ torch Tensor map_body_ = map_body_ map_impl = torch ops higher_order map_impl map_body_ child l_y_ map_body_ = child = l_y_ = None getitem = map_impl map_impl = None getitem test_map_multi_return f x control_flow map lambda x x sin x sin x x = torch randn graphs = _check_map_graph_and_extract f x graphs graph body_graph = graphs assertExpectedInline graph \ forward L_x_ torch Tensor l_x_ = L_x_ map_body_ = map_body_ map_impl = torch ops higher_order map_impl map_body_ l_x_ map_body_ = l_x_ = None getitem = map_impl getitem_ = map_impl map_impl = None getitem getitem_ assertExpectedInline body_graph \ forward child torch Tensor child_ = child sin child_ = child sin child = None child_ child_ test_map_pytree_return _construct_pytree clone clone clone clone clone clone clone f x inner_f xs _construct_pytree xs control_flow map inner_f x x = torch randn graphs = _check_map_graph_and_extract f x graphs graph body_graph = graphs assertExpectedInline graph \ forward L_x_ torch Tensor l_x_ = L_x_ map_body_ = map_body_ map_impl = torch ops higher_order map_impl map_body_ l_x_ map_body_ = l_x_ = None getitem = map_impl getitem_ = map_impl getitem_ = map_impl getitem_ = map_impl getitem_ = map_impl getitem_ = map_impl value = map_impl map_impl = None getitem getitem_ getitem_ getitem_ getitem_ getitem_ value assertExpectedInline body_graph \ forward child torch Tensor child_ = child clone child_ = child clone child_ = child clone child_ = child clone child_ = child clone child_ = child clone child_ = child clone child = None child_ child_ child_ child_ child_ child_ child_ test_map_kwargs cnt = CompileCounter torch compile backend=cnt f x control_flow map lambda x x sin x=x x = torch randn assertRaises TypeError lambda f x assertEqual cnt frame_count test_map_symint_input fn x y inner x y torch sin x + y control_flow map inner x y size x = torch randn y = torch randn graphs = _check_map_graph_and_extract fn x y graphs graph body_graph = graphs assertExpectedInline graph \ forward L_x_ torch Tensor l_x_ = L_x_ map_body_ = map_body_ map_impl = torch ops higher_order map_impl map_body_ l_x_ map_body_ = l_x_ = None getitem = map_impl map_impl = None getitem assertExpectedInline body_graph \ forward child torch Tensor const_unused int add = child + child = None sin = torch sin add add = None sin test_map_lowers_to_graph fn x y inner x y torch sin x + y control_flow map inner x y size x = torch randn y = torch randn graphs = _check_map_graph_and_extract fn x y graphs graph body_graph = graphs assertExpectedInline graph \ forward L_x_ torch Tensor l_x_ = L_x_ map_body_ = map_body_ map_impl = torch ops higher_order map_impl map_body_ l_x_ map_body_ = l_x_ = None getitem = map_impl map_impl = None getitem assertExpectedInline body_graph \ forward child torch Tensor const_unused int add = child + child = None sin = torch sin add add = None sin test_map_example_value_metadata_consistent_with_eager torch _higher_order_ops map map_dense backend = EagerAndRecordGraphs inner x x sin x cos T x sin view - rand_ = torch randn inps = torch randn torch randn torch randn requires_grad=True torch randn requires_grad=True permute torch randn requires_grad=True detach torch randn requires_grad=True narrow rand_ T rand_ rand_ rand_ rand_ T rand_ unsqueeze rand_ squeeze rand_ reshape x inps compiled_ret = torch compile noqa F control_flow map backend=backend fullgraph=True inner x eager_sin eager_transpose eager_view = map_dense inner x map_node = next node node backend graphs graph nodes node op == call_function map node name fake_sin fake_transpose fake_view = map_node meta example_value _check_size_stride_contiguous x y assertEqual y size x size assertEqual y stride x stride assertEqual y requires_grad x requires_grad assertEqual x is_contiguous True assertEqual y is_contiguous True _check_size_stride_contiguous eager_sin fake_sin _check_size_stride_contiguous eager_transpose fake_transpose _check_size_stride_contiguous eager_view fake_view torch _dynamo reset backend graphs clear test_cond_subgraph_name_is_valid backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend pred = torch tensor True pred = torch tensor False xs = torch randn y = torch randn torch compile backend=cnt fullgraph=True cond_f pred pred x y true_fn pred x y x + y false_fn pred x y true_fn x y x sin - y cos false_fn x y x cos - y sin control_flow cond pred true_fn false_fn x y control_flow cond pred true_fn false_fn pred x y result = cond_f pred pred xs y assertEqual result xs + y cond_gm = backend graphs name_set = set name_set update name name _ cond_gm named_modules assertEqual name_set cond_true_ cond_false_ cond_false_ cond_false_ cond_false_ cond_true_ torch _dynamo config patch assume_static_by_default=True dynamic_shapes=True test_cond_graph_break_in_one_branch backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend Foo torch nn Module __init__ - None super __init__ buffer = torch nn Buffer torch ones forward x true_fn x buffer += buffer sum + x sum false_fn x x - sum control_flow cond x sum true_fn false_fn x mod_for_compile = torch compile Foo backend=cnt dynamic=True mod_for_eager = Foo assertRaisesRegex torch _dynamo exc UncapturedHigherOrderOpError r Cond doesn t work unless captured completely torch compile mod_for_eager torch ones assertRaisesRegex torch _dynamo exc UncapturedHigherOrderOpError r Cond doesn t work unless captured completely torch compile mod_for_compile torch ones test_cond_free_variable_in_both_branches backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend z = torch ones Foo torch nn Module __init__ - None super __init__ buffer = torch nn Buffer torch ones forward x y true_fn x x sum + buffer sum + z sum false_fn x x sum - z sum - buffer sum control_flow cond y true_fn false_fn x mod_for_compile = torch compile Foo backend=cnt dynamic=True fullgraph=True mod_for_eager = Foo assertEqual mod_for_compile torch tensor True torch tensor mod_for_eager torch tensor True torch tensor node backend graphs graph nodes node op == call_function node target == torch ops higher_order cond _ _ _ operands = node args Since we compile dynamic each branch takes inputs buffer x z s assertEqual len operands node op == get_attr str node target cond_true_ cond_false_ num_placeholders = len node node getattr backend graphs str node target graph nodes node op == placeholder assertEqual num_placeholders _check_cond_graph_and_extract fn args backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend out = torch compile fn backend=cnt fullgraph=True args assertEqual out fn args assertEqual cnt frame_count assertEqual len backend graphs Dynamic shapes produce slightly different graph check_dynamic_shape_capture gm = backend graphs graph = gm code strip true_graph = gm cond_true_ code strip false_graph = gm cond_false_ code strip graph true_graph false_graph _check_map_graph_and_extract fn args backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend out = torch compile fn backend=cnt fullgraph=True args assertEqual out fn args assertEqual cnt frame_count assertEqual len backend graphs Dynamic shapes produce slightly different graph check_dynamic_shape_capture gm = backend graphs graph = gm code strip subgraphs = module_name gm _modules keys subgraphs append getattr gm module_name code strip graph subgraphs test_cond_branches_no_arguments fn x true_fn torch sin x false_fn torch cos x control_flow cond x sum true_fn false_fn graphs = _check_cond_graph_and_extract fn torch randn graphs None graph true_graph false_graph = graphs assertExpectedInline graph \ forward L_x_ torch Tensor l_x_ = L_x_ sum_ = l_x_ sum gt = sum_ sum_ = None cond_true_ = cond_true_ cond_false_ = cond_false_ cond = torch ops higher_order cond gt cond_true_ cond_false_ l_x_ gt = cond_true_ = cond_false_ = l_x_ = None getitem = cond cond = None getitem assertExpectedInline true_graph \ forward l_x_ l_x__ = l_x_ sin = torch sin l_x__ l_x__ = None sin assertExpectedInline false_graph \ forward l_x_ l_x__ = l_x_ cos = torch cos l_x__ l_x__ = None cos test_cond_branches_no_arguments_no_closure fn x true_fn torch ones false_fn torch ones sin control_flow cond x sum true_fn false_fn _check_cond_graph_and_extract fn torch randn graphs = _check_cond_graph_and_extract fn torch randn graphs None graph true_graph false_graph = graphs assertExpectedInline graph \ forward L_x_ torch Tensor l_x_ = L_x_ sum_ = l_x_ sum l_x_ = None gt = sum_ sum_ = None cond_true_ = cond_true_ cond_false_ = cond_false_ cond = torch ops higher_order cond gt cond_true_ cond_false_ gt = cond_true_ = cond_false_ = None getitem = cond cond = None getitem assertExpectedInline true_graph \ forward ones = torch ones ones assertExpectedInline false_graph \ forward ones = torch ones sin = ones sin ones = None sin test_cond_side_effect_in_one_branches backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend z = torch ones Foo torch nn Module __init__ - None super __init__ forward y x true_fn x z append x z append x z pop x sum + z - sum false_fn x x sum - z sum control_flow cond y true_fn false_fn x mod_for_eager = Foo mod_for_compile = torch compile Foo backend=cnt dynamic=True fullgraph=False assertRaisesRegex torch _dynamo exc UncapturedHigherOrderOpError r Cond doesn t work unless captured completely torch compile mod_for_eager torch tensor True torch tensor assertRaisesRegex torch _dynamo exc UncapturedHigherOrderOpError r Cond doesn t work unless captured completely torch compile mod_for_compile torch tensor True torch tensor test_cond_with_constant_pred test pred x true_fn x x false_fn x -x control_flow cond pred true_fn false_fn x opt_test = torch compile test backend= eager inp = torch ones assertTrue torch allclose test True inp opt_test True inp assertTrue torch allclose test False inp opt_test False inp test_map_graph_break backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend Module torch nn Module __init__ - None super __init__ w = torch nn Buffer torch ones forward xs body x w += x control_flow map body xs mod = Module mod_for_compile = torch compile mod backend=cnt dynamic=True fullgraph=False assertRaisesRegex torch _dynamo exc UncapturedHigherOrderOpError map doesn t work unless captured completely torch compile mod_for_compile torch Tensor test_map_side_effect backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend z = torch ones Module torch nn Module __init__ - None super __init__ w = torch nn Buffer torch ones forward xs body x z append x z append x z pop x + z - sum control_flow map body xs mod = Module mod_for_compile = torch compile mod backend=cnt dynamic=True fullgraph=False assertRaisesRegex torch _dynamo exc UncapturedHigherOrderOpError map doesn t work unless captured completely torch compile mod_for_compile torch Tensor test_wrap_subgraph_name_is_valid backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend x = torch randn y = torch randn inner x y z = x + y wrap lambda x wrap lambda x x + z x x torch compile backend=cnt fullgraph=True f x y wrap inner x y result = f x y assertEqual result x + y + x wrap_gm = backend graphs names = set names update mod_name mod_name _ wrap_gm named_modules assertEqual names wrap_body_ wrap_body_ wrap_body_ wrap_body_ wrap_body_ wrap_body_ test_wrap_allow_local_assign_in_body_fn f arg arg inner_f arg arg = arg b = arg ret = x ret append x + x b ret append x + ret wrap inner_f arg arg x = torch ones my_args_generator yield x x sin yield x x sin arg_count = ifdynstaticdefault actual_graph = _test_wrap_simple f my_args_generator arg_count return_graph=True Dynamic shapes produce slightly different graph check_dynamic_shape_capture assertExpectedInline actual_graph \ GraphModule torch nn Module forward L_arg _ _ f L_arg _ _ f l_arg _ _ = L_arg _ _ l_arg _ _ = L_arg _ _ wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ l_arg _ _ l_arg _ _ wrap_body_ = l_arg _ _ = l_arg _ _ = None getitem f = wrap getitem_ f = wrap wrap = None getitem getitem_ wrap_body_ torch nn Module forward l_arg _ _ f l_arg _ _ f child f = l_arg _ _ + l_arg _ _ = None child_ f = l_arg _ _ + l_arg _ _ = None child child_ test_capture_global_num f x wrap lambda x x + global_num x x = torch zeros Numbers don t get lifted so args still _test_wrap_simple f default_args_generator x test_capture_global_num_adds_guard torch compile backend= eager fullgraph=True f x wrap lambda x x + global_num x global global_num x = torch zeros result = f x assertEqual result x + global_num global_num = torch randn item result = f x assertEqual result x + global_num test_capture_input_num f x y wrap lambda x x + y x x = torch zeros y = Numbers don t get lifted so args still _test_wrap_simple f default_args_generator x y test_side_effect_in_body counters clear backend = EagerAndRecordGraphs x = torch randn y = torch randn inner x nonlocal y y = x x clone torch compile backend=backend f x wrap inner x f x assertEqual y x assert_dict_matches_regex dict counters graph_break r HigherOrderOperator Mutating variable current scope \ SideEffects\ test_fallback_on_graph_break_simple In future there should per-HigherOrderOperator switch whether fallback raise loud error For now we just fallback default cnt = CompileCounter x = torch randn inner x y = x sin torch _dynamo graph_break z = y sin z torch compile backend=cnt f x wrap inner x result = f x assertEqual result inner x assertEqual cnt frame_count test_fallback_on_graph_break_complicated cnt = CompileCounter x = torch randn inner x y = x sin y = y global_var torch _dynamo graph_break z = y sin z torch compile backend=cnt f x x = x clone result = wrap inner x result clone result = f x assertEqual result inner x assertEqual cnt frame_count test_modules counters clear backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend mod = torch nn Linear x = torch randn torch compile backend=cnt fullgraph=True f x wrap lambda x mod x x result = f x assertEqual result mod x assertEqual cnt frame_count assertEqual len backend graphs wrap_node = find_first_node backend graphs wrap args - input other weight bias assertTrue len wrap_node args Check linear bias weight getattr outer graph torch _dynamo config inline_inbuilt_nn_modules assertTrue len dict backend graphs named_parameters == Check inner function has one op its linear op body_function = getattr backend graphs wrap_node args name assertEqual op_count body_function linear_node = find_first_node body_function torch _C _nn linear assertTrue linear_node None Check innermost graph does have any params assertTrue len dict body_function named_parameters == assertTrue len dict body_function named_children == test_flat_list_output f x wrap lambda x torch sin x torch cos x x x = torch randn arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x arg_count expected_opcount= test_support_float_in_output counters clear cnt = CompileCounter torch compile backend=cnt fullgraph=True f x wrap lambda x torch sin x x x = torch randn result = f x assertEqual result torch sin x test_nested_tuple_output f x b = wrap lambda x x sin x cos x + b x = torch randn counters clear arg_count = ifdynstaticdefault graph = _test_wrap_simple f default_args_generator x arg_count return_graph=True assertEqual len counters graph_break check_dynamic_shape_capture assertExpectedInline graph \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ l_x_ wrap_body_ = l_x_ = None f = wrap b f = wrap wrap = None add f = + b = b = None add wrap_body_ torch nn Module forward l_x_ f child f = l_x_ sin child_ f = l_x_ cos l_x_ = None child child_ test_output_with_dict f x wrap lambda x -x x x = torch randn counters clear arg_count = ifdynstaticdefault graph = _test_wrap_simple f default_args_generator x arg_count return_graph=True assertEqual len counters graph_break check_dynamic_shape_capture assertExpectedInline graph \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ wrap_body_ = wrap_body_ wrap = torch ops higher_order wrap wrap_body_ l_x_ wrap_body_ = l_x_ = None value f = wrap wrap = None value wrap_body_ torch nn Module forward l_x_ f child f = -l_x_ l_x_ = None child test_access_module_attr counters clear backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend mod = torch nn Linear x = torch randn torch compile backend=cnt fullgraph=True f x y = mod x wrap lambda y y - mod bias y result = f x assertEqual result mod x - mod bias assertEqual cnt frame_count assertEqual len backend graphs wrap_node = find_first_node backend graphs wrap assertTrue len wrap_node args Check linear bias weight getattr outer graph torch _dynamo config inline_inbuilt_nn_modules assertTrue len dict backend graphs named_parameters == Check inner function has one op its linear op body_function = getattr backend graphs wrap_node args name assertEqual op_count body_function Check innermost graph does have any params assertTrue len dict body_function named_parameters == assertTrue len dict body_function named_children == test_make_closure f x y g x x + y g x h x y wrap f x y x = torch randn y = torch randn arg_count = ifdynstaticdefault _test_wrap_simple h default_args_generator x y arg_count test_internal_nonlocal f x y w = g x nonlocal w w = x x h x nonlocal w w = w + x g x h x w + y h x y wrap f x y x = torch randn y = torch randn arg_count = ifdynstaticdefault _test_wrap_simple h default_args_generator x y arg_count test_capture_numpy_number numpy np y = np float f x wrap lambda x x + y x x = torch randn np number lifted graph inputs arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x arg_count test_freevars_as_inputs_to_wrap y = torch randn f x wrap lambda x y x + y x y x = torch randn arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x arg_count test_lift_tensor_constant f x y = torch tensor wrap lambda x x + y x x = torch randn arg_count = ifdynstaticdefault _test_wrap_simple f default_args_generator x arg_count expected_opcount= test_nested_wrap MockModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x mod = MockModule Two levels wrap ops gn x torch cos x + wrap mod x fn x wrap gn x arg_count = ifdynstaticdefault _test_wrap_simple fn default_args_generator torch randn arg_count test_fn_with_kwargs_in_torch_ops fn x wrap lambda z torch cos input=z x x = torch randn arg_count = ifdynstaticdefault _test_wrap_simple fn default_args_generator x arg_count test_hooks ToyModel torch nn Module __init__ - None super __init__ net = torch nn Linear forward x net x model = ToyModel forward_handles = activations = save_activations mod inp out activations name = inp name module model named_children forward_handles name = module register_forward_hook save_activations torch compile backend= eager fn x wrap lambda x model x x _ range second iteration key hooks would have fired during aot trace first iter activations clear x = torch randn pred = fn x loss = pred sum loss backward assertTrue activations keys == forward_handles keys _get_source_fn_stack gm node_names ret = mod gm modules node mod graph nodes node name node_names actual_stack = name name _ node meta get source_fn_stack ret node name = actual_stack ret test_wrap_source_fn_stack MockModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x mod = MockModule gn x torch cos x + wrap mod x fn x wrap gn x backend = EagerAndRecordGraphs inp = torch randn torch compile fn backend=backend fullgraph=True inp gm = backend graphs actual_stack = _get_source_fn_stack gm cos add linear assertExpectedInline pprint pformat actual_stack \ add wrap add cos wrap cos linear wrap wrap linear test_cond_source_fn_stack backend = EagerAndRecordGraphs torch compile backend=backend fullgraph=True cond_f pred pred x y true_fn pred x y x + y false_fn pred x y true_fn x y x sin - y cos false_fn x y x cos - y sin control_flow cond pred true_fn false_fn x y control_flow cond pred true_fn false_fn pred x y pred = torch tensor True pred = torch tensor False xs = torch randn y = torch randn cond_f pred pred xs y gm = backend graphs actual_stack = _get_source_fn_stack gm cos add sin sub assertExpectedInline pprint pformat actual_stack \ add cond add cos cond cond cos sin cond cond sin sub cond cond sub test_map_source_fn_stack backend = EagerAndRecordGraphs xs = torch randn y = torch randn torch compile backend=backend fullgraph=True map_f xs y inner x y inner x y x + y control_flow map inner x y y cos control_flow map inner xs y sin map_f xs y gm = backend graphs actual_stack = _get_source_fn_stack gm cos add sin assertExpectedInline pprint pformat actual_stack \ add map_impl map_impl add cos map_impl cos sin sin test_grad_source_fn_stack backend = EagerAndRecordGraphs fn x x sin sum torch compile backend=backend fullgraph=False wrapper_fn x torch func grad torch func grad fn x x = torch randn wrapper_fn x gm = backend graphs actual_stack = _get_source_fn_stack gm sum_ sin assertExpectedInline pprint pformat actual_stack sin sin test_vmap_multiply_scalar torch compile backend= inductor fullgraph=True g x torch vmap torch mul in_dims= None x x = torch randn y = g x assertEqual y x torch compile backend= inductor fullgraph=True f x torch vmap torch mul in_dims= None x x = torch randn y = f x assertEqual y x test_vmap_source_fn_stack backend = EagerAndRecordGraphs inner_fn x torch func vmap lambda x x sum + x sum x torch compile backend=backend fullgraph=True fn x torch func vmap lambda x inner_fn x cos x x = torch randn fn x gm = backend graphs actual_stack = _get_source_fn_stack gm sum_ sum_ batched_output assertExpectedInline pprint pformat actual_stack sum_ sum_ sum_ sum_ https github com pytorch pytorch issues test_dynamic_shapes_over_vmap_batch_size gn b c d + b + c + d fn func b c d = torch arange b = torch arange b c = torch arange c d = torch arange d func = torch vmap func in_dims= None None None func = torch vmap func in_dims= None None None func = torch vmap func in_dims= None None None func = torch vmap func in_dims= None None None func b c d cnt = CompileCounterWithBackend eager We generate corresponding dynamic shapes test case ` test dynamo test_dynamic_shapes py ` automatically compiled_fn = torch compile fn backend=cnt b c d = assertEqual fn gn b c d compiled_fn gn b c d assertEqual cnt frame_count b c d = assertEqual fn gn b c d compiled_fn gn b c d Ensure no recompile dynamic shapes enabled assertEqual cnt frame_count ifdynstaticdefault graph = cnt graphs Check dynamic shapes generates correct graph check_dynamic_shape_capture assertExpectedInline graph code strip \ forward L_a_ torch SymInt L_b_ torch SymInt L_c_ torch SymInt L_d_ torch SymInt l_a_ = L_a_ l_b_ = L_b_ l_c_ = L_c_ l_d_ = L_d_ = torch arange l_a_ b = torch arange l_b_ c = torch arange l_c_ d = torch arange l_d_ lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting l_d_ error _vmap_increment_nesting = None child = torch _functorch predispatch _add_batch_dim d d = None lazy_load_decompositions_ = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions_ = None _vmap_increment_nesting_ = torch _functorch predispatch _vmap_increment_nesting l_c_ error _vmap_increment_nesting_ = None child_ = torch _functorch predispatch _add_batch_dim c c = None lazy_load_decompositions_ = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions_ = None _vmap_increment_nesting_ = torch _functorch predispatch _vmap_increment_nesting l_b_ error _vmap_increment_nesting_ = None child_ = torch _functorch predispatch _add_batch_dim b b = None lazy_load_decompositions_ = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions_ = None _vmap_increment_nesting_ = torch _functorch predispatch _vmap_increment_nesting l_a_ error _vmap_increment_nesting_ = None _add_batch_dim_ = torch _functorch predispatch _add_batch_dim = None add = _add_batch_dim_ + child_ _add_batch_dim_ = child_ = None add_ = add + child_ add = child_ = None batched_outputs = add_ + child add_ = child = None batched_outputs_ = torch _functorch predispatch _remove_batch_dim batched_outputs l_a_ batched_outputs = l_a_ = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None batched_outputs_ = torch _functorch predispatch _remove_batch_dim batched_outputs_ l_b_ batched_outputs_ = l_b_ = None _vmap_decrement_nesting_ = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting_ = None batched_outputs_ = torch _functorch predispatch _remove_batch_dim batched_outputs_ l_c_ batched_outputs_ = l_c_ = None _vmap_decrement_nesting_ = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting_ = None _remove_batch_dim_ = torch _functorch predispatch _remove_batch_dim batched_outputs_ l_d_ batched_outputs_ = l_d_ = None _vmap_decrement_nesting_ = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting_ = None _remove_batch_dim_ noqa B test_cond_pytree_operands _construct_pytree = torch randn b = torch randn c = torch randn d = torch randn e = torch randn f = torch randn g = torch randn b c d e f g g pred = torch tensor True inp = _construct_pytree _reduce_sum flattened init = val flattened init += val init _reduce_max flattened init = flattened val flattened init = max val init init true_fn pytree_in flattened spec = pytree tree_flatten pytree_in _reduce_sum flattened false_fn pytree_in flattened spec = pytree tree_flatten pytree_in _reduce_max flattened fn pred pytree_in torch cond pred true_fn false_fn pytree_in backend = EagerAndRecordGraphs compiled_res = torch compile fn backend=backend pred inp eager_res = fn pred inp assertEqual compiled_res eager_res graph = backend graphs Dynamic shapes produce slightly different graph check_dynamic_shape_capture assertExpectedInline graph code strip \ forward L_pred_ torch Tensor L_pytree_in_ _ torch Tensor L_pytree_in_ _ _ _ _ torch Tensor L_pytree_in_ _ torch Tensor L_pytree_in_ _ _ torch Tensor L_pytree_in_ _ _ _ torch Tensor L_pytree_in_ _ _ torch Tensor L_pytree_in_ _g_ torch Tensor l_pred_ = L_pred_ l_pytree_in_ _ = L_pytree_in_ _ l_pytree_in_ _ _ _ _ = L_pytree_in_ _ _ _ _ l_pytree_in_ _ = L_pytree_in_ _ l_pytree_in_ _ _ = L_pytree_in_ _ _ l_pytree_in_ _ _ _ = L_pytree_in_ _ _ _ l_pytree_in_ _ _ = L_pytree_in_ _ _ l_pytree_in_ _g_ = L_pytree_in_ _g_ cond_true_ = cond_true_ cond_false_ = cond_false_ cond = torch ops higher_order cond l_pred_ cond_true_ cond_false_ l_pytree_in_ _ l_pytree_in_ _ _ _ _ l_pytree_in_ _ l_pytree_in_ _ _ l_pytree_in_ _ _ _ l_pytree_in_ _ _ l_pytree_in_ _g_ l_pred_ = cond_true_ = cond_false_ = l_pytree_in_ _ = l_pytree_in_ _ _ _ _ = l_pytree_in_ _ = l_pytree_in_ _ _ = l_pytree_in_ _ _ _ = l_pytree_in_ _ _ = l_pytree_in_ _g_ = None getitem = cond cond = None getitem noqa B test_cond_pytree_operands_with_non_tensor_leaves fn pred pytree_in torch cond pred lambda x x + lambda x x pytree_in pred = torch tensor True pytree_in string assertRaisesRegex RuntimeError r Expect operands tuple possibly nested dict list tuple fn pred pytree_in pytree_in string assertRaisesRegex torch _dynamo exc UncapturedHigherOrderOpError r Cond doesn t work unless captured completely torch compile torch compile fn backend= eager pred pytree_in test_cond_with_empty_operands torch compile fullgraph=True fn x y z true_fn y + false_fn z + torch cond x true_fn false_fn zeros = torch zeros ones = torch ones assertEqual fn zeros ones ones torch tensor assertEqual fn ones ones ones torch tensor test_hopify_generic_wrap torch _higher_order_ops wrap dynamo_bypassing_wrapper my_hop_fn_impl fn args k= kwargs wrapper args kwargs out = fn args kwargs isinstance out tuple out + k out + k wrapper my_hop_fn fn args k= kwargs dynamo_bypassing_wrapper functools partial my_hop_fn_impl k=k fn args kwargs my_hop_fn_ _impl fn args g=None wrapper args kwargs assert g None out = fn args isinstance out tuple g out g out wrapper my_hop_fn_ fn args g=None kwargs dynamo_bypassing_wrapper functools partial my_hop_fn_ _impl g=g fn args kwargs gn x h= x sin + h fn x b out = my_hop_fn gn x h=b k= out = torch rand requires_grad=True b = torch rand compiled_fn = torch compile fn backend= aot_eager_decomp_partition fullgraph=True assertEqual compiled_fn b fn b g x x cos fn_ x b out = my_hop_fn_ fn x b g=g out = torch rand requires_grad=True compiled_fn_ = torch compile fn_ backend= aot_eager_decomp_partition fullgraph=True assertEqual compiled_fn_ b fn_ b test_hints_wrapper ref_fn x y x = x + y x = torch relu x x = x + y torch abs x fn_with_hints x y x = x + y inner_body_fn x y x = torch relu x x = x + y x outer_body_fn x y x = hints_wrapper inner_body_fn x y hints= inner_body True x = torch abs x x res = hints_wrapper outer_body_fn x y hints= outer_body True res backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend x = torch randn y = torch ones eager_res = fn_with_hints x y compiled_res = torch compile fn_with_hints backend=cnt x y ref_res = ref_fn x y assertEqual eager_res ref_res assertEqual compiled_res ref_res assertEqual len cnt graphs Dynamic shapes produce slightly different graph check_dynamic_shape_capture graph = backend graphs assertExpectedInline normalize_gm graph print_readable print_output=False \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ x f = l_x_ + l_y_ l_x_ = None hints_wrapper_body_ = hints_wrapper_body_ hints_wrapper = torch ops higher_order hints_wrapper hints_wrapper_body_ x l_y_ hints = outer_body True hints_wrapper_body_ = x = l_y_ = None res f = hints_wrapper hints_wrapper = None res hints_wrapper_body_ torch nn Module forward x f l_y_ f hints_wrapper_body_ = hints_wrapper_body_ hints_wrapper = torch ops higher_order hints_wrapper hints_wrapper_body_ x l_y_ hints = inner_body True hints_wrapper_body_ = x = l_y_ = None x_ f = hints_wrapper hints_wrapper = None x_ f = torch abs x_ x_ = None x_ hints_wrapper_body_ torch nn Module forward x f l_y_ f x_ f = torch relu x x = None x_ f = x_ + l_y_ x_ = l_y_ = None x_ test_hints_wrapper_no_hints fn_with_hints x y outer_body_fn x y x = torch add x y x res = hints_wrapper outer_body_fn x y res backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend x = torch randn y = torch ones msg = hints_wrapper - key hints provided assertRaisesRegex RuntimeError msg torch compile fn_with_hints backend=cnt x y test_hints_wrapper_incorrect_type fn_with_hints x y outer_body_fn x y x = torch add x y x res = hints_wrapper outer_body_fn x y hints= test True res backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend x = torch randn y = torch ones msg = r hints must dict containing int float bool str value assertRaisesRegex RuntimeError msg torch compile fn_with_hints backend=cnt x y test_hints_wrapper_pytree_inputs fn_with_hints x y outer_body_fn x res = torch add x x test res res = hints_wrapper outer_body_fn x test y hints= test True res x = torch randn y = torch ones msg = r args must tuple tensors ints floats bools assertRaisesRegex RuntimeError msg fn_with_hints x y HigherOrderOpVmapGuardTests LoggingTestCase make_logging_test recompiles=True test_vmap_grad_guard_ok records vmap = torch vmap grad = torch func grad g x vmap grad torch sin x torch compile backend= eager fn x vmap g x x = torch randn y = fn x sanity check assertEqual len records assertEqual x cos y Calling same function again won t have any effect guards fn x assertEqual len records xfailIfTorchDynamo make_logging_test recompiles=True test_grad_guard_fail records grad = torch func grad torch compile backend= eager fn x grad torch sin x sum x = torch randn fn x assertEqual len records calling again should invalidate graph fn x assertEqual len records call grad should retrigger compilation x = torch randn grad fn x assertGreater len records record = getRecord records pyfunctorch assertIn torch _functorch pyfunctorch compare_functorch_state munge_exc record getMessage make_logging_test recompiles=True test_dual_level_guard records fwAD = torch autograd forward_ad torch compile backend= eager fullgraph=True fn foo tangent fwAD dual_level dual = fwAD make_dual foo tangent dual foo = torch rand tangent = torch rand fn foo tangent assertEqual len records calling again should invalidate graph fn foo tangent assertEqual len records assertRaises only here because Nested forward mode AD supported assertRaises torch _dynamo exc InternalTorchDynamoError fwAD dual_level fn foo tangent assertGreater len records record = getRecord records forward_ad assertIn torch autograd forward_ad _current_level == - munge_exc record getMessage xfailIfTorchDynamo make_logging_test recompiles=True test_jvp_guard_fail records jvp = torch func jvp vmap = torch func vmap torch compile backend= eager fn x jvp torch sin x x x = torch randn fn x assertEqual len records calling again should invalidate graph fn x assertEqual len records call jvp should retrigger compilation x = torch randn jvp vmap fn x x assertGreater len records hasRecord records pyfunctorch record = getRecord records pyfunctorch assertIn torch _functorch pyfunctorch compare_functorch_state munge_exc record getMessage hasRecord records forward_ad record = getRecord records forward_ad assertIn torch autograd forward_ad _current_level == - munge_exc record getMessage make_logging_test recompiles=True test_vmap_guard_ok records torch compile backend= eager fn x torch vmap lambda x x sin x x = torch randn y = fn x sanity check assertEqual len records assertEqual x sin y Calling same function again won t have any effect guards z = fn x assertEqual len records assertEqual x sin z calling different object will also affect guards w = fn z assertEqual len records assertEqual z sin w xfailIfTorchDynamo make_logging_test recompiles=True test_vmap_guard_fail_different_state records torch compile backend= eager fn x torch vmap lambda x x sin x x = torch zeros y = torch vmap fn randomness= same x assertEqual x sin y assertEqual len records call vmap vmap fn x should retrigger compilation y = torch vmap fn randomness= different x assertEqual x sin y assertGreater len records record = getRecord records pyfunctorch assertIn torch _functorch pyfunctorch compare_functorch_state Vmap same record getMessage xfailIfTorchDynamo make_logging_test recompiles=True test_vmap_guard_fail records torch compile backend= eager fn x torch vmap lambda x x sin x x = torch zeros y = torch vmap fn x assertEqual x sin y assertEqual len records call vmap vmap fn x should retrigger compilation _functorch current_level same x = torch zeros y = torch vmap torch vmap fn x assertEqual x sin y assertGreater len records record = getRecord records pyfunctorch assertIn torch _functorch pyfunctorch compare_functorch_state Vmap error record getMessage xfailIfTorchDynamo make_logging_test recompiles=True test_vmap_grad_vmap_guard_fail records vmap = torch vmap grad = torch func grad g x y = vmap torch sin randomness= same x y sum torch compile backend= eager fn x grad g x x = torch randn y = vmap fn randomness= error x assertEqual x cos y previous FX graph should invalidated x = torch randn y = vmap vmap fn randomness= different x assertGreater len records record = getRecord records pyfunctorch assertIn torch _functorch pyfunctorch compare_functorch_state Vmap error munge_exc record getMessage xfailIfTorchDynamo make_logging_test recompiles=True test_vmap_recompile_different_states records torch compile backend= eager fn x torch vmap lambda x x sin x x = torch zeros torch vmap fn randomness= same x assertEqual len records sanity check torch vmap fn randomness= different x assertGreater len records record = getRecord records pyfunctorch assertIn torch _functorch pyfunctorch compare_functorch_state Vmap same munge_exc record getMessage make_logging_test guards=True test_emit_functorch_guard_if_active records torch compile backend= eager fn x torch sin x x = torch randn _ = fn x assertFalse hasRecord records pyfunctorch sanity check _ = torch vmap fn x assertTrue hasRecord records pyfunctorch record = getRecord records pyfunctorch assertIn torch _functorch pyfunctorch compare_functorch_state Vmap error munge_exc record getMessage make_logging_test recompiles=True test_linearize_recompiles records torch compile backend= eager fn x out jvp_fn = torch func linearize torch sin x out jvp_fn x x = torch randn fn x assertEqual len records z = torch randn fn z assertEqual len records y = torch randn fn y assertGreater len records FuncTorchHigherOrderOpTests torch _dynamo test_case TestCase tearDown Ensure case test failure next test won t fail because previous call _vmap_increment_nesting wasn t undone i e test_vmap_free_tensor fails when PYTORCH_TEST_WITH_DYNAMO= call increment nesting undone TEST_WITH_TORCHDYNAMO warn = False while ci = torch _C _functorch peek_interpreter_stack ci key == torch _C _functorch TransformType Vmap warn = True torch _C _functorch _vmap_decrement_nesting break warn msg = Interpreter stack empty Test should have called torch _C _functorch _vmap_decrement_nesting warnings warn msg _compile_check fn inputs fullgraph=True graph_idx= backend = EagerAndRecordGraphs actual = fn inputs expected = torch compile fn backend=backend fullgraph=fullgraph inputs assertEqual actual expected wrapped_gm = backend graphs graph_idx wrapped_gm test_hessian counters clear wrapper_fn x torch func hessian torch sin x x = torch randn wrapped_gm = _compile_check wrapper_fn x Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ tensor i = torch tensor cumsum i = tensor cumsum dim = tensor = None getitem i = cumsum slice None - None cumsum = None neg i = getitem neg getitem = None unbind = neg unbind neg = unbind = None chunk f = l_x_ new_zeros diagonal f = chunk diagonal fill_ f = diagonal fill_ diagonal = fill_ = None child f = chunk view chunk = None lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None child_ f = torch _functorch predispatch _add_batch_dim child child = None _jvp_increment_nesting = torch _C _functorch _jvp_increment_nesting _jvp_increment_nesting = None _set_fwd_grad_enabled = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled = None _enter_dual_level = torch _C _enter_dual_level _enter_dual_level = None _maybe_load_decompositions = torch autograd forward_ad _maybe_load_decompositions _maybe_load_decompositions = None child_ f = torch _make_dual l_x_ child_ level = child_ = None _wrap_for_grad f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = _wrap_for_grad = None _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None diff_primals f = torch _C _functorch _wrap_for_grad child_ child_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None _set_tensor_requires_grad f = torch _functorch eager_transforms _set_tensor_requires_grad diff_primals _set_tensor_requires_grad = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None primals_out f = torch sin diff_primals results f = torch _C _functorch _unwrap_for_grad primals_out _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None tensor_ i = torch tensor cumsum_ i = tensor_ cumsum dim = tensor_ = None getitem_ i = cumsum_ slice None - None cumsum_ = None neg_ i = getitem_ neg getitem_ = None unbind_ = neg_ unbind neg_ = unbind_ = None chunk_ f = results new_zeros results = None diagonal_ f = chunk_ diagonal fill__ f = diagonal_ fill_ diagonal_ = fill__ = None basis f = chunk_ view chunk_ = None lazy_load_decompositions_ = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions_ = None _vmap_increment_nesting_ = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting_ = None _add_batch_dim_ f = torch _functorch predispatch _add_batch_dim basis basis = None _autograd_grad = torch _functorch eager_transforms _autograd_grad primals_out diff_primals _add_batch_dim_ retain_graph = True create_graph = True primals_out = diff_primals = _add_batch_dim_ = None batched_outputs f = _autograd_grad _autograd_grad = None chunked_result f = torch _functorch predispatch _remove_batch_dim batched_outputs batched_outputs = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None split = chunked_result split dim = chunked_result = None split_ f = split split = None output_input f = split_ view split_ = None _unpack_dual = torch _unpack_dual output_input level = output_input = None primal f = _unpack_dual dual f = _unpack_dual _unpack_dual = None primals_out_unflatten f = torch _C _functorch _unwrap_for_grad primal primal = primals_out_unflatten = None tangents_out_unflatten f = torch _C _functorch _unwrap_for_grad dual dual = None _exit_dual_level = torch _C _exit_dual_level _exit_dual_level = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _jvp_decrement_nesting = torch _C _functorch _jvp_decrement_nesting _jvp_decrement_nesting = None results_ f = torch _functorch predispatch _remove_batch_dim tangents_out_unflatten tangents_out_unflatten = None _vmap_decrement_nesting_ = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting_ = None movedim f = results_ movedim - results_ = None split_ = movedim split dim = - movedim = None jac_out_in f = split_ split_ = None unflatten f = jac_out_in unflatten - jac_out_in = None unflatten test_hessian_argnums counters clear fn x y x sin wrapper_fn x y torch func hessian fn argnums= x y x = torch randn y = torch randn wrapped_gm = _compile_check wrapper_fn x y Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline \n join actual split \n - \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ tensor i = torch tensor cumsum i = tensor cumsum dim = tensor = None getitem i = cumsum slice None - None cumsum = None neg i = getitem neg getitem = None unbind = neg unbind neg = unbind = None chunk f = l_y_ new_zeros diagonal f = chunk diagonal fill_ f = diagonal fill_ diagonal = fill_ = None child f = chunk view chunk = None lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None child_ f = torch _functorch predispatch _add_batch_dim child child = None _jvp_increment_nesting = torch _C _functorch _jvp_increment_nesting _jvp_increment_nesting = None _set_fwd_grad_enabled = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled = None _enter_dual_level = torch _C _enter_dual_level _enter_dual_level = None _maybe_load_decompositions = torch autograd forward_ad _maybe_load_decompositions _maybe_load_decompositions = None child_ f = torch _make_dual l_y_ child_ level = child_ = None child_ f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None _wrap_for_grad_ f = torch _C _functorch _wrap_for_grad l_y_ l_y_ = _wrap_for_grad_ = None _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None _wrap_for_grad_ f = torch _C _functorch _wrap_for_grad child_ child_ = None child_ f = torch _C _functorch _wrap_for_grad child_ child_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None _set_tensor_requires_grad f = torch _functorch eager_transforms _set_tensor_requires_grad child_ _set_tensor_requires_grad = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None primals_out f = _wrap_for_grad_ sin _wrap_for_grad_ = None results f = torch _C _functorch _unwrap_for_grad primals_out _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None tensor_ i = torch tensor cumsum_ i = tensor_ cumsum dim = tensor_ = None getitem_ i = cumsum_ slice None - None cumsum_ = None neg_ i = getitem_ neg getitem_ = None unbind_ = neg_ unbind neg_ = unbind_ = None chunk_ f = results new_zeros results = None diagonal_ f = chunk_ diagonal fill__ f = diagonal_ fill_ diagonal_ = fill__ = None basis f = chunk_ view chunk_ = None lazy_load_decompositions_ = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions_ = None _vmap_increment_nesting_ = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting_ = None _add_batch_dim_ f = torch _functorch predispatch _add_batch_dim basis basis = None _autograd_grad = torch _functorch eager_transforms _autograd_grad primals_out child_ _add_batch_dim_ retain_graph = True create_graph = True primals_out = child_ = _add_batch_dim_ = None child_ f = _autograd_grad _autograd_grad = None child_ f = torch _functorch predispatch _remove_batch_dim child_ child_ = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None split = child_ split dim = child_ = None split_ f = split split = None child_ f = split_ view split_ = None _unpack_dual = torch _unpack_dual child_ level = child_ = None primal f = _unpack_dual _unpack_dual = None tangent f = torch zeros_like primal child_ f = torch _C _functorch _unwrap_for_grad primal primal = child_ = None child_ f = torch _C _functorch _unwrap_for_grad tangent tangent = None _exit_dual_level = torch _C _exit_dual_level _exit_dual_level = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _jvp_decrement_nesting = torch _C _functorch _jvp_decrement_nesting _jvp_decrement_nesting = None child_ f = torch _functorch predispatch _remove_batch_dim child_ child_ = None _vmap_decrement_nesting_ = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting_ = None movedim f = child_ movedim - child_ = None split_ = movedim split dim = - movedim = None jac_out_in f = split_ split_ = None unflatten f = jac_out_in unflatten - jac_out_in = None assertExpectedInline actual split \n - unflatten test_jacrev counters clear wrapper_fn x torch func jacrev torch sin x x = torch randn wrapped_gm = _compile_check wrapper_fn x Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None diff_primals f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None _set_tensor_requires_grad f = torch _functorch eager_transforms _set_tensor_requires_grad diff_primals _set_tensor_requires_grad = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None primals_out f = torch sin diff_primals results f = torch _C _functorch _unwrap_for_grad primals_out _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None tensor i = torch tensor cumsum i = tensor cumsum dim = tensor = None getitem i = cumsum slice None - None cumsum = None neg i = getitem neg getitem = None unbind = neg unbind neg = unbind = None chunk f = results new_zeros results = None diagonal f = chunk diagonal fill_ f = diagonal fill_ diagonal = fill_ = None basis f = chunk view chunk = None lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None _add_batch_dim f = torch _functorch predispatch _add_batch_dim basis basis = None _autograd_grad = torch _functorch eager_transforms _autograd_grad primals_out diff_primals _add_batch_dim retain_graph = True create_graph = True primals_out = diff_primals = _add_batch_dim = None batched_outputs f = _autograd_grad _autograd_grad = None chunked_result f = torch _functorch predispatch _remove_batch_dim batched_outputs batched_outputs = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None split = chunked_result split dim = chunked_result = None split_ f = split split = None output_input f = split_ view split_ = None output_input test_jacrev_two_tensors_argnums counters clear fn x y y sin wrapper_fn x y torch func jacrev fn argnums= x y x = torch randn y = torch randn wrapped_gm = _compile_check wrapper_fn x y Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None _wrap_for_grad f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = _wrap_for_grad = None diff_primals f = torch _C _functorch _wrap_for_grad l_y_ l_y_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None _set_tensor_requires_grad f = torch _functorch eager_transforms _set_tensor_requires_grad diff_primals _set_tensor_requires_grad = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None primals_out f = diff_primals sin results f = torch _C _functorch _unwrap_for_grad primals_out _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None tensor i = torch tensor cumsum i = tensor cumsum dim = tensor = None getitem i = cumsum slice None - None cumsum = None neg i = getitem neg getitem = None unbind = neg unbind neg = unbind = None chunk f = results new_zeros results = None diagonal f = chunk diagonal fill_ f = diagonal fill_ diagonal = fill_ = None basis f = chunk view chunk = None lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None _add_batch_dim f = torch _functorch predispatch _add_batch_dim basis basis = None _autograd_grad = torch _functorch eager_transforms _autograd_grad primals_out diff_primals _add_batch_dim retain_graph = True create_graph = True primals_out = diff_primals = _add_batch_dim = None batched_outputs f = _autograd_grad _autograd_grad = None chunked_result f = torch _functorch predispatch _remove_batch_dim batched_outputs batched_outputs = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None split = chunked_result split dim = chunked_result = None split_ f = split split = None output_input f = split_ view split_ = None output_input test_jacrev_has_aux counters clear fn x y y sin x wrapper_fn x y torch func jacrev fn argnums= has_aux=True x y x = torch randn y = torch randn wrapped_gm = _compile_check wrapper_fn x y Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None aux f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None diff_primals f = torch _C _functorch _wrap_for_grad l_y_ l_y_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None _set_tensor_requires_grad f = torch _functorch eager_transforms _set_tensor_requires_grad diff_primals _set_tensor_requires_grad = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None primals_out f = diff_primals sin aux_ f = torch _C _functorch _unwrap_for_grad aux aux = None results f = torch _C _functorch _unwrap_for_grad primals_out _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None tensor i = torch tensor cumsum i = tensor cumsum dim = tensor = None getitem i = cumsum slice None - None cumsum = None neg i = getitem neg getitem = None unbind = neg unbind neg = unbind = None chunk f = results new_zeros results = None diagonal f = chunk diagonal fill_ f = diagonal fill_ diagonal = fill_ = None basis f = chunk view chunk = None lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None _add_batch_dim f = torch _functorch predispatch _add_batch_dim basis basis = None _autograd_grad = torch _functorch eager_transforms _autograd_grad primals_out diff_primals _add_batch_dim retain_graph = True create_graph = True primals_out = diff_primals = _add_batch_dim = None batched_outputs f = _autograd_grad _autograd_grad = None chunked_result f = torch _functorch predispatch _remove_batch_dim batched_outputs batched_outputs = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None split = chunked_result split dim = chunked_result = None split_ f = split split = None output_input f = split_ view split_ = None output_input aux_ test_vjp counters clear fn x x sin sum wrapper_fn x v out vjpfunc = torch func vjp fn x out x = torch randn v = torch randn wrapped_gm = _compile_check wrapper_fn x v Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None child f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None child_ f = torch _functorch eager_transforms _set_tensor_requires_grad child child_ = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None sin f = child sin child = None primals_out f = sin sum sin = None results f = torch _C _functorch _unwrap_for_grad primals_out primals_out = None _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None results test_vjp_multiple_outputs counters clear wrapper_fn x v fn = lambda x x sin x cos noqa E out vjpfunc = torch func vjp fn x vjps = vjpfunc v v out vjps x = torch randn v = torch randn wrapped_gm = _compile_check wrapper_fn x v Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_v_ f l_x_ = L_x_ l_v_ = L_v_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None child f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None child_ f = torch _functorch eager_transforms _set_tensor_requires_grad child set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None child_ f = child sin child_ f = child cos child = None _unwrap_for_grad f = torch _C _functorch _unwrap_for_grad child_ _unwrap_for_grad_ f = torch _C _functorch _unwrap_for_grad child_ _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None _autograd_grad = torch _functorch eager_transforms _autograd_grad child_ child_ child_ l_v_ l_v_ retain_graph = True create_graph = True child_ = child_ = child_ = l_v_ = None getitem f = _autograd_grad _autograd_grad = None _unwrap_for_grad _unwrap_for_grad_ getitem test_vjp_multiple_outputs_python_struct counters clear wrapper_fn x v fn = lambda x first x sin second x cos noqa E out vjpfunc = torch func vjp fn x vjps = vjpfunc first v second v sin out vjps x = torch randn v = torch randn wrapped_gm = _compile_check wrapper_fn x v Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_v_ f l_x_ = L_x_ l_v_ = L_v_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None child f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None child_ f = torch _functorch eager_transforms _set_tensor_requires_grad child set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None child_ f = child sin child_ f = child cos child = None value f = torch _C _functorch _unwrap_for_grad child_ value_ f = torch _C _functorch _unwrap_for_grad child_ _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None child_ f = l_v_ sin _autograd_grad = torch _functorch eager_transforms _autograd_grad child_ child_ child_ l_v_ child_ retain_graph = True create_graph = True child_ = child_ = child_ = l_v_ = child_ = None getitem f = _autograd_grad _autograd_grad = None value value_ getitem test_vjp_has_aux counters clear fn x x sin sum x wrapper_fn x v out vjpfunc _ = torch func vjp fn x has_aux=True out x = torch randn v = torch randn wrapped_gm = _compile_check wrapper_fn x v Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None child f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None child_ f = torch _functorch eager_transforms _set_tensor_requires_grad child child_ = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None sin f = child sin primals_out f = sin sum sin = None aux f = torch _C _functorch _unwrap_for_grad child child = aux = None results f = torch _C _functorch _unwrap_for_grad primals_out primals_out = None _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None results config patch inline_inbuilt_nn_modules=True test_functional_call wrapper_fn model params inputs targets prediction = torch func functional_call model params inputs torch nn functional mse_loss prediction targets model = torch nn Linear params = dict model named_parameters inputs = torch randn targets = torch randn wrapped_gm = _compile_check wrapper_fn model params inputs targets Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False torch _dynamo config inline_inbuilt_nn_modules assertExpectedInline actual \ GraphModule torch nn Module forward L_model_parameters_weight_ f L_model_parameters_bias_ f L_inputs_ f L_targets_ f l_model_parameters_weight_ = L_model_parameters_weight_ l_model_parameters_bias_ = L_model_parameters_bias_ l_inputs_ = L_inputs_ l_targets_ = L_targets_ prediction f = torch _C _nn linear l_inputs_ l_model_parameters_weight_ l_model_parameters_bias_ l_inputs_ = l_model_parameters_weight_ = l_model_parameters_bias_ = None mse_loss f = torch nn functional mse_loss prediction l_targets_ prediction = l_targets_ = None mse_loss assertExpectedInline actual \ GraphModule torch nn Module forward L_inputs_ f L_targets_ f l_inputs_ = L_inputs_ l_targets_ = L_targets_ prediction f = model l_inputs_ l_inputs_ = None mse_loss f = torch nn functional mse_loss prediction l_targets_ prediction = l_targets_ = None mse_loss config patch inline_inbuilt_nn_modules=True test_functional_call_sequential_params_and_buffers copied test test_stateless py MockModule torch nn Module __init__ - None super __init__ l = torch nn Linear register_buffer buffer torch ones foo = forward x l x + buffer wrapper_fn model params buffers inputs two separate dictionaries torch func functional_call model params buffers inputs model = MockModule params = dict model named_parameters buffers = dict model named_buffers inputs = torch tensor wrapped_gm = _compile_check wrapper_fn model params buffers inputs fullgraph=False Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False torch _dynamo config inline_inbuilt_nn_modules expected = \ GraphModule torch nn Module forward L_inputs_ f L_model_modules_l _parameters_weight_ f L_model_modules_l _parameters_bias_ f L_model_buffers_buffer_ f l_inputs_ = L_inputs_ l_model_modules_l _parameters_weight_ = L_model_modules_l _parameters_weight_ l_model_modules_l _parameters_bias_ = L_model_modules_l _parameters_bias_ l_model_buffers_buffer_ = L_model_buffers_buffer_ linear f = torch _C _nn linear l_inputs_ l_model_modules_l _parameters_weight_ l_model_modules_l _parameters_bias_ l_inputs_ = l_model_modules_l _parameters_weight_ = l_model_modules_l _parameters_bias_ = None add f = linear + l_model_buffers_buffer_ linear = l_model_buffers_buffer_ = None add We found Windows Linux have some empty line difference empty_line_normalizer will help fix assertExpectedInline empty_line_normalizer actual empty_line_normalizer normalize_gm expected assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ l__self___l f = L__self___l l_x_ l_x_ = None l__self___buffer f = L__self___buffer add f = l__self___l + l__self___buffer l__self___l = l__self___buffer = None add config patch inline_inbuilt_nn_modules=False test_functional_call_disable_inline_nn_module counters clear wrapper_fn model params inputs targets prediction = torch func functional_call model params inputs torch nn functional mse_loss prediction targets model = torch nn Linear params = dict model named_parameters inputs = torch randn targets = torch randn actual = wrapper_fn model params inputs targets expected = torch compile wrapper_fn backend= aot_eager fullgraph=False model params inputs targets assertEqual len counters graph_break assertEqual torch func functional_call capture disabled can turned setting ` torch _dynamo config inline_inbuilt_nn_modules=True ` dict counters graph_break assertEqual actual expected test_grad counters clear fn x x sin sum wrapper_fn x torch func grad fn x x = torch randn wrapped_gm = _compile_check wrapper_fn x Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None diff_args f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None _set_tensor_requires_grad f = torch _functorch eager_transforms _set_tensor_requires_grad diff_args _set_tensor_requires_grad = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None sin f = diff_args sin output f = sin sum sin = None _autograd_grad = torch _functorch eager_transforms _autograd_grad output diff_args create_graph = True diff_args = None grad_input f = _autograd_grad _autograd_grad = None grad_input_ f = torch _C _functorch _unwrap_for_grad grad_input grad_input = None output_ f = torch _C _functorch _unwrap_for_grad output output = output_ = None _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None grad_input_ test_grad_freevar_tensor counters clear y = torch randn fn x x sin + y sum wrapper_fn x torch func grad fn x x = torch randn expected = wrapper_fn x actual = torch compile wrapper_fn backend= aot_eager fullgraph=True x assertEqual actual expected test_grad_freevar_python_scalar counters clear y = fn x x sin + y sum wrapper_fn x torch func grad fn x x = torch randn wrapped_gm = _compile_check wrapper_fn x Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None diff_args f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None _set_tensor_requires_grad f = torch _functorch eager_transforms _set_tensor_requires_grad diff_args _set_tensor_requires_grad = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None sin f = diff_args sin add f = sin + sin = None output f = add sum add = None _autograd_grad = torch _functorch eager_transforms _autograd_grad output diff_args create_graph = True diff_args = None grad_input f = _autograd_grad _autograd_grad = None grad_input_ f = torch _C _functorch _unwrap_for_grad grad_input grad_input = None output_ f = torch _C _functorch _unwrap_for_grad output output = output_ = None _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None grad_input_ test_grad_capture_tensor counters clear wrapper_fn x y = torch randn fn x x sin + y sum torch func grad fn x x = torch randn wrapped_gm = _compile_check wrapper_fn x Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ y f = torch randn _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None diff_args f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None _set_tensor_requires_grad f = torch _functorch eager_transforms _set_tensor_requires_grad diff_args _set_tensor_requires_grad = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None sin f = diff_args sin add f = sin + y sin = y = None output f = add sum add = None _autograd_grad = torch _functorch eager_transforms _autograd_grad output diff_args create_graph = True diff_args = None grad_input f = _autograd_grad _autograd_grad = None grad_input_ f = torch _C _functorch _unwrap_for_grad grad_input grad_input = None output_ f = torch _C _functorch _unwrap_for_grad output output = output_ = None _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None grad_input_ test_grad_closure_scalar counters clear wrapper_fn x y = fn x x sin + y sum torch func grad fn x x = torch randn Graph break because dynamo unable get source ` fn ` functools wraps ` grad ` leads graph-break wrapped_gm = _compile_check wrapper_fn x fullgraph=False Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None diff_args f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None _set_tensor_requires_grad f = torch _functorch eager_transforms _set_tensor_requires_grad diff_args _set_tensor_requires_grad = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None sin f = diff_args sin add f = sin + sin = None output f = add sum add = None _autograd_grad = torch _functorch eager_transforms _autograd_grad output diff_args create_graph = True diff_args = None grad_input f = _autograd_grad _autograd_grad = None grad_input_ f = torch _C _functorch _unwrap_for_grad grad_input grad_input = None output_ f = torch _C _functorch _unwrap_for_grad output output = output_ = None _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None grad_input_ test_grad_has_aux counters clear y = fn x x sin + y sum x cos wrapper_fn x torch func grad fn has_aux=True x x = torch randn wrapped_gm = _compile_check wrapper_fn x Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None diff_args f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None _set_tensor_requires_grad f = torch _functorch eager_transforms _set_tensor_requires_grad diff_args _set_tensor_requires_grad = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None sin f = diff_args sin add f = sin + sin = None output f = add sum add = None aux f = diff_args cos _autograd_grad = torch _functorch eager_transforms _autograd_grad output diff_args create_graph = True diff_args = None grad_input f = _autograd_grad _autograd_grad = None grad_input_ f = torch _C _functorch _unwrap_for_grad grad_input grad_input = None output_ f = torch _C _functorch _unwrap_for_grad output output = output_ = None aux_ f = torch _C _functorch _unwrap_for_grad aux aux = None _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None grad_input_ aux_ test_grad_two_tensor_has_aux counters clear fn x y x sin + y sum x cos wrapper_fn x y torch func grad fn has_aux=True x y y = torch randn x = torch randn wrapped_gm = _compile_check wrapper_fn x y Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None diff_args f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None _wrap_for_grad_ f = torch _C _functorch _wrap_for_grad l_y_ l_y_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None _set_tensor_requires_grad f = torch _functorch eager_transforms _set_tensor_requires_grad diff_args _set_tensor_requires_grad = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None sin f = diff_args sin add f = sin + _wrap_for_grad_ sin = _wrap_for_grad_ = None output f = add sum add = None aux f = diff_args cos _autograd_grad = torch _functorch eager_transforms _autograd_grad output diff_args create_graph = True diff_args = None grad_input f = _autograd_grad _autograd_grad = None grad_input_ f = torch _C _functorch _unwrap_for_grad grad_input grad_input = None output_ f = torch _C _functorch _unwrap_for_grad output output = output_ = None aux_ f = torch _C _functorch _unwrap_for_grad aux aux = None _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None grad_input_ aux_ test_grad_two_tensor_all_grad_has_aux counters clear nums = fn x y x sin + y sum x cos wrapper_fn_const_var x y torch func grad fn argnums= has_aux=True x y wrapper_fn_tuple_var x y torch func grad fn argnums=nums has_aux=True x y y = torch randn x = torch randn wrapped_gm_const_var = _compile_check wrapper_fn_const_var x y wrapped_gm_tuple_var = _compile_check wrapper_fn_tuple_var x y Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual_const_var = normalize_gm wrapped_gm_const_var print_readable print_output=False actual_tuple_var = normalize_gm wrapped_gm_tuple_var print_readable print_output=False assertExpectedInline actual_const_var \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None child f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None child_ f = torch _C _functorch _wrap_for_grad l_y_ l_y_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None _set_tensor_requires_grad f = torch _functorch eager_transforms _set_tensor_requires_grad child _set_tensor_requires_grad = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed_ = None _set_tensor_requires_grad_ f = torch _functorch eager_transforms _set_tensor_requires_grad child_ _set_tensor_requires_grad_ = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None sin f = child sin add f = sin + child_ sin = None output f = add sum add = None aux f = child cos _autograd_grad = torch _functorch eager_transforms _autograd_grad output child child_ create_graph = True child = child_ = None child_ f = _autograd_grad child_ f = _autograd_grad _autograd_grad = None _unwrap_for_grad f = torch _C _functorch _unwrap_for_grad child_ child_ = None _unwrap_for_grad_ f = torch _C _functorch _unwrap_for_grad child_ child_ = None output_ f = torch _C _functorch _unwrap_for_grad output output = output_ = None aux_ f = torch _C _functorch _unwrap_for_grad aux aux = None _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None _unwrap_for_grad _unwrap_for_grad_ aux_ assertExpectedInline actual_tuple_var \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None child f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None child_ f = torch _C _functorch _wrap_for_grad l_y_ l_y_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None _set_tensor_requires_grad f = torch _functorch eager_transforms _set_tensor_requires_grad child _set_tensor_requires_grad = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed_ = None _set_tensor_requires_grad_ f = torch _functorch eager_transforms _set_tensor_requires_grad child_ _set_tensor_requires_grad_ = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None sin f = child sin add f = sin + child_ sin = None output f = add sum add = None aux f = child cos _autograd_grad = torch _functorch eager_transforms _autograd_grad output child child_ create_graph = True child = child_ = None child_ f = _autograd_grad child_ f = _autograd_grad _autograd_grad = None _unwrap_for_grad f = torch _C _functorch _unwrap_for_grad child_ child_ = None _unwrap_for_grad_ f = torch _C _functorch _unwrap_for_grad child_ child_ = None output_ f = torch _C _functorch _unwrap_for_grad output output = output_ = None aux_ f = torch _C _functorch _unwrap_for_grad aux aux = None _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None _unwrap_for_grad _unwrap_for_grad_ aux_ test_grad_over_grad counters clear fn x x sin sum wrapper_fn x torch func grad torch func grad fn x x = torch randn wrapped_gm = _compile_check wrapper_fn x fullgraph=False check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None diff_args f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None _set_tensor_requires_grad f = torch _functorch eager_transforms _set_tensor_requires_grad diff_args _set_tensor_requires_grad = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None _saved_tensors_hooks_disable_ = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable_ = None _grad_increment_nesting_ = torch _C _functorch _grad_increment_nesting _grad_increment_nesting_ = None diff_args_ f = torch _C _functorch _wrap_for_grad diff_args set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed_ = None _set_tensor_requires_grad_ f = torch _functorch eager_transforms _set_tensor_requires_grad diff_args_ _set_tensor_requires_grad_ = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None sin f = diff_args_ sin output f = sin sum sin = None _autograd_grad = torch _functorch eager_transforms _autograd_grad output diff_args_ create_graph = True diff_args_ = None grad_input f = _autograd_grad _autograd_grad = None grad_input_ f = torch _C _functorch _unwrap_for_grad grad_input grad_input = None output_ f = torch _C _functorch _unwrap_for_grad output output = output_ = None _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_disable_ = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable_ = None _autograd_grad_ = torch _functorch eager_transforms _autograd_grad grad_input_ diff_args create_graph = True diff_args = None grad_input_ f = _autograd_grad_ _autograd_grad_ = None grad_input_ f = torch _C _functorch _unwrap_for_grad grad_input_ grad_input_ = None output_ f = torch _C _functorch _unwrap_for_grad grad_input_ grad_input_ = output_ = None _grad_decrement_nesting_ = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting_ = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None grad_input_ test_grad_with_graph_break counters clear fn x torch _dynamo graph_break x sin sum wrapper_fn x torch func grad fn x x = torch randn actual = wrapper_fn x expected = torch compile wrapper_fn backend= aot_eager fullgraph=False x assertEqual len counters graph_break assertEqual actual expected test_grad_with_side_effect counters clear foo = fn x foo append x sin sum wrapper_fn x torch func grad fn x x = torch randn actual = wrapper_fn x expected = torch compile wrapper_fn backend= aot_eager fullgraph=False x assertEqual len counters graph_break assertEqual actual expected test_grad_pytree counters clear fn x x x = x x sin sum + x wrapper_fn x torch func grad fn x x = torch randn x = torch randn actual = wrapper_fn x x expected = torch compile wrapper_fn backend= aot_eager fullgraph=False x x assertEqual len counters graph_break assertEqual actual expected test_grad_non_tensor_input counters clear fn x y x sin sum + y wrapper_fn x y torch func grad fn x y x = torch randn y = wrapped_gm = _compile_check wrapper_fn x y Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ _saved_tensors_hooks_disable = torch _C _autograd _saved_tensors_hooks_disable torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case _saved_tensors_hooks_disable = None _grad_increment_nesting = torch _C _functorch _grad_increment_nesting _grad_increment_nesting = None diff_args f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None set_inplace_requires_grad_allowed = torch _C _functorch set_inplace_requires_grad_allowed True set_inplace_requires_grad_allowed = None _set_tensor_requires_grad f = torch _functorch eager_transforms _set_tensor_requires_grad diff_args _set_tensor_requires_grad = None set_inplace_requires_grad_allowed_ = torch _C _functorch set_inplace_requires_grad_allowed False set_inplace_requires_grad_allowed_ = None sin f = diff_args sin sum_ f = sin sum sin = None output f = sum_ + sum_ = None _autograd_grad = torch _functorch eager_transforms _autograd_grad output diff_args create_graph = True diff_args = None grad_input f = _autograd_grad _autograd_grad = None grad_input_ f = torch _C _functorch _unwrap_for_grad grad_input grad_input = None output_ f = torch _C _functorch _unwrap_for_grad output output = output_ = None _grad_decrement_nesting = torch _C _functorch _grad_decrement_nesting _grad_decrement_nesting = None _saved_tensors_hooks_enable = torch _C _autograd _saved_tensors_hooks_enable _saved_tensors_hooks_enable = None grad_input_ test_grad_fn_with_kwargs fn x y x + y sum wrapper_fn x y torch func grad fn x y=y x = torch randn y = torch randn actual = wrapper_fn x y expected = torch compile wrapper_fn backend= aot_eager fullgraph=False x y assertEqual len counters graph_break assertEqual actual expected test_jacfwd counters clear wrapper_fn x torch func jacfwd torch sin x x = torch randn wrapped_gm = _compile_check wrapper_fn x Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ tensor i = torch tensor cumsum i = tensor cumsum dim = tensor = None getitem i = cumsum slice None - None cumsum = None neg i = getitem neg getitem = None unbind = neg unbind neg = unbind = None chunk f = l_x_ new_zeros diagonal f = chunk diagonal fill_ f = diagonal fill_ diagonal = fill_ = None child f = chunk view chunk = None lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None child_ f = torch _functorch predispatch _add_batch_dim child child = None _jvp_increment_nesting = torch _C _functorch _jvp_increment_nesting _jvp_increment_nesting = None _set_fwd_grad_enabled = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled = None _enter_dual_level = torch _C _enter_dual_level _enter_dual_level = None _maybe_load_decompositions = torch autograd forward_ad _maybe_load_decompositions _maybe_load_decompositions = None _make_dual f = torch _make_dual l_x_ child_ level = child_ = None _wrap_for_grad f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = _wrap_for_grad = None result_duals f = torch sin _make_dual _make_dual = None _unpack_dual = torch _unpack_dual result_duals level = result_duals = None primal f = _unpack_dual dual f = _unpack_dual _unpack_dual = None primals_out_unflatten f = torch _C _functorch _unwrap_for_grad primal primal = primals_out_unflatten = None tangents_out_unflatten f = torch _C _functorch _unwrap_for_grad dual dual = None _exit_dual_level = torch _C _exit_dual_level _exit_dual_level = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _jvp_decrement_nesting = torch _C _functorch _jvp_decrement_nesting _jvp_decrement_nesting = None results f = torch _functorch predispatch _remove_batch_dim tangents_out_unflatten tangents_out_unflatten = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None movedim f = results movedim - results = None split = movedim split dim = - movedim = None jac_out_in f = split split = None unflatten f = jac_out_in unflatten - jac_out_in = None unflatten test_jacfwd_two_tensors_argnums counters clear fn x y y sin wrapper_fn x y torch func jacfwd fn argnums= x y x = torch randn y = torch randn wrapped_gm = _compile_check wrapper_fn x y Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ tensor i = torch tensor cumsum i = tensor cumsum dim = tensor = None getitem i = cumsum slice None - None cumsum = None neg i = getitem neg getitem = None unbind = neg unbind neg = unbind = None chunk f = l_y_ new_zeros diagonal f = chunk diagonal fill_ f = diagonal fill_ diagonal = fill_ = None child f = chunk view chunk = None lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None child_ f = torch _functorch predispatch _add_batch_dim child child = None _jvp_increment_nesting = torch _C _functorch _jvp_increment_nesting _jvp_increment_nesting = None _set_fwd_grad_enabled = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled = None _enter_dual_level = torch _C _enter_dual_level _enter_dual_level = None _maybe_load_decompositions = torch autograd forward_ad _maybe_load_decompositions _maybe_load_decompositions = None _make_dual f = torch _make_dual l_y_ child_ level = child_ = None _wrap_for_grad f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = _wrap_for_grad = None _wrap_for_grad_ f = torch _C _functorch _wrap_for_grad l_y_ l_y_ = _wrap_for_grad_ = None result_duals f = _make_dual sin _make_dual = None _unpack_dual = torch _unpack_dual result_duals level = result_duals = None primal f = _unpack_dual dual f = _unpack_dual _unpack_dual = None primals_out_unflatten f = torch _C _functorch _unwrap_for_grad primal primal = primals_out_unflatten = None tangents_out_unflatten f = torch _C _functorch _unwrap_for_grad dual dual = None _exit_dual_level = torch _C _exit_dual_level _exit_dual_level = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _jvp_decrement_nesting = torch _C _functorch _jvp_decrement_nesting _jvp_decrement_nesting = None results f = torch _functorch predispatch _remove_batch_dim tangents_out_unflatten tangents_out_unflatten = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None movedim f = results movedim - results = None split = movedim split dim = - movedim = None jac_out_in f = split split = None unflatten f = jac_out_in unflatten - jac_out_in = None unflatten test_jacfwd_has_aux counters clear fn x y y sin x wrapper_fn x y torch func jacfwd fn argnums= has_aux=True x y x = torch randn y = torch randn wrapped_gm = _compile_check wrapper_fn x y Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ tensor i = torch tensor cumsum i = tensor cumsum dim = tensor = None getitem i = cumsum slice None - None cumsum = None neg i = getitem neg getitem = None unbind = neg unbind neg = unbind = None chunk f = l_y_ new_zeros diagonal f = chunk diagonal fill_ f = diagonal fill_ diagonal = fill_ = None child f = chunk view chunk = None lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None child_ f = torch _functorch predispatch _add_batch_dim child child = None _jvp_increment_nesting = torch _C _functorch _jvp_increment_nesting _jvp_increment_nesting = None _set_fwd_grad_enabled = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled = None _enter_dual_level = torch _C _enter_dual_level _enter_dual_level = None _maybe_load_decompositions = torch autograd forward_ad _maybe_load_decompositions _maybe_load_decompositions = None _make_dual f = torch _make_dual l_y_ child_ level = child_ = None aux f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = None _wrap_for_grad_ f = torch _C _functorch _wrap_for_grad l_y_ l_y_ = _wrap_for_grad_ = None result_duals f = _make_dual sin _make_dual = None aux_ f = torch _C _functorch _unwrap_for_grad aux aux = None _unpack_dual = torch _unpack_dual result_duals level = result_duals = None primal f = _unpack_dual dual f = _unpack_dual _unpack_dual = None primals_out_unflatten f = torch _C _functorch _unwrap_for_grad primal primal = primals_out_unflatten = None tangents_out_unflatten f = torch _C _functorch _unwrap_for_grad dual dual = None _exit_dual_level = torch _C _exit_dual_level _exit_dual_level = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _jvp_decrement_nesting = torch _C _functorch _jvp_decrement_nesting _jvp_decrement_nesting = None results f = torch _functorch predispatch _remove_batch_dim tangents_out_unflatten tangents_out_unflatten = None aux_ f = torch _functorch predispatch _remove_batch_dim aux_ aux_ = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None aux_ f = aux_ aux_ = None movedim f = results movedim - results = None split = movedim split dim = - movedim = None jac_out_in f = split split = None unflatten f = jac_out_in unflatten - jac_out_in = None unflatten aux_ test_jacfwd_randomness counters clear fn x y y sin x wrapper_fn x y torch func jacfwd fn randomness= same x y x = torch randn y = torch randn wrapped_gm = _compile_check wrapper_fn x y Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ tensor i = torch tensor cumsum i = tensor cumsum dim = tensor = None getitem i = cumsum slice None - None cumsum = None neg i = getitem neg getitem = None unbind = neg unbind neg = unbind = None chunk f = l_x_ new_zeros diagonal f = chunk diagonal fill_ f = diagonal fill_ diagonal = fill_ = None child f = chunk view chunk = None lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting same _vmap_increment_nesting = None child_ f = torch _functorch predispatch _add_batch_dim child child = None _jvp_increment_nesting = torch _C _functorch _jvp_increment_nesting _jvp_increment_nesting = None _set_fwd_grad_enabled = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled = None _enter_dual_level = torch _C _enter_dual_level _enter_dual_level = None _maybe_load_decompositions = torch autograd forward_ad _maybe_load_decompositions _maybe_load_decompositions = None child_ f = torch _make_dual l_x_ child_ level = child_ = None _wrap_for_grad f = torch _C _functorch _wrap_for_grad l_x_ l_x_ = _wrap_for_grad = None _wrap_for_grad_ f = torch _C _functorch _wrap_for_grad l_y_ l_y_ = None child_ f = _wrap_for_grad_ sin _wrap_for_grad_ = None _unpack_dual = torch _unpack_dual child_ level = child_ = None primal f = _unpack_dual _unpack_dual = None tangent f = torch zeros_like primal _unpack_dual_ = torch _unpack_dual child_ level = child_ = None primal_ f = _unpack_dual_ dual f = _unpack_dual_ _unpack_dual_ = None child_ f = torch _C _functorch _unwrap_for_grad primal primal = child_ = None child_ f = torch _C _functorch _unwrap_for_grad primal_ primal_ = child_ = None child_ f = torch _C _functorch _unwrap_for_grad tangent tangent = None child_ f = torch _C _functorch _unwrap_for_grad dual dual = None _exit_dual_level = torch _C _exit_dual_level _exit_dual_level = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _jvp_decrement_nesting = torch _C _functorch _jvp_decrement_nesting _jvp_decrement_nesting = None child_ f = torch _functorch predispatch _remove_batch_dim child_ child_ = None child_ f = torch _functorch predispatch _remove_batch_dim child_ child_ = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None movedim f = child_ movedim - child_ = None split = movedim split dim = - movedim = None jac_out_in f = split split = None unflatten f = jac_out_in unflatten - jac_out_in = None movedim_ f = child_ movedim - child_ = None split_ = movedim_ split dim = - movedim_ = None jac_out_in_ f = split_ split_ = None unflatten_ f = jac_out_in_ unflatten - jac_out_in_ = None unflatten unflatten_ test_jvp_simple counters clear fn x x sin sum wrapper_fn x v torch func jvp fn x v x = torch randn v = torch randn wrapped_gm = _compile_check wrapper_fn x v Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_v_ f l_x_ = L_x_ l_v_ = L_v_ _jvp_increment_nesting = torch _C _functorch _jvp_increment_nesting _jvp_increment_nesting = None _set_fwd_grad_enabled = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled = None _enter_dual_level = torch _C _enter_dual_level _enter_dual_level = None _maybe_load_decompositions = torch autograd forward_ad _maybe_load_decompositions _maybe_load_decompositions = None _make_dual f = torch _make_dual l_x_ l_v_ level = l_x_ = l_v_ = None sin f = _make_dual sin _make_dual = None result_duals f = sin sum sin = None _unpack_dual = torch _unpack_dual result_duals level = result_duals = None primal f = _unpack_dual dual f = _unpack_dual _unpack_dual = None primals_out_unflatten f = torch _C _functorch _unwrap_for_grad primal primal = None tangents_out_unflatten f = torch _C _functorch _unwrap_for_grad dual dual = None _exit_dual_level = torch _C _exit_dual_level _exit_dual_level = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _jvp_decrement_nesting = torch _C _functorch _jvp_decrement_nesting _jvp_decrement_nesting = None primals_out_unflatten tangents_out_unflatten test_jvp_has_aux counters clear fn x x sin sum x wrapper_fn x v torch func jvp fn x v has_aux=True x = torch randn v = torch randn wrapped_gm = _compile_check wrapper_fn x v Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_v_ f l_x_ = L_x_ l_v_ = L_v_ _jvp_increment_nesting = torch _C _functorch _jvp_increment_nesting _jvp_increment_nesting = None _set_fwd_grad_enabled = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled = None _enter_dual_level = torch _C _enter_dual_level _enter_dual_level = None _maybe_load_decompositions = torch autograd forward_ad _maybe_load_decompositions _maybe_load_decompositions = None aux f = torch _make_dual l_x_ l_v_ level = l_x_ = l_v_ = None sin f = aux sin result_duals f = sin sum sin = None aux_ f = torch _C _functorch _unwrap_for_grad aux aux = None _unpack_dual = torch _unpack_dual result_duals level = result_duals = None primal f = _unpack_dual dual f = _unpack_dual _unpack_dual = None primals_out_unflatten f = torch _C _functorch _unwrap_for_grad primal primal = None tangents_out_unflatten f = torch _C _functorch _unwrap_for_grad dual dual = None _exit_dual_level = torch _C _exit_dual_level _exit_dual_level = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _jvp_decrement_nesting = torch _C _functorch _jvp_decrement_nesting _jvp_decrement_nesting = None primals_out_unflatten tangents_out_unflatten aux_ test_jvp_two_tensors_has_aux counters clear fn x y x sin sum + y cos x wrapper_fn x y v torch func jvp fn x y v v has_aux=True x = torch randn y = torch randn v = torch randn wrapped_gm = _compile_check wrapper_fn x y v Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_y_ f L_v_ f l_x_ = L_x_ l_y_ = L_y_ l_v_ = L_v_ _jvp_increment_nesting = torch _C _functorch _jvp_increment_nesting _jvp_increment_nesting = None _set_fwd_grad_enabled = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled = None _enter_dual_level = torch _C _enter_dual_level _enter_dual_level = None _maybe_load_decompositions = torch autograd forward_ad _maybe_load_decompositions _maybe_load_decompositions = None aux f = torch _make_dual l_x_ l_v_ level = l_x_ = None _maybe_load_decompositions_ = torch autograd forward_ad _maybe_load_decompositions _maybe_load_decompositions_ = None _make_dual_ f = torch _make_dual l_y_ l_v_ level = l_y_ = l_v_ = None sin f = aux sin sum_ f = sin sum sin = None cos f = _make_dual_ cos _make_dual_ = None result_duals f = sum_ + cos sum_ = cos = None aux_ f = torch _C _functorch _unwrap_for_grad aux aux = None _unpack_dual = torch _unpack_dual result_duals level = result_duals = None primal f = _unpack_dual dual f = _unpack_dual _unpack_dual = None primals_out_unflatten f = torch _C _functorch _unwrap_for_grad primal primal = None tangents_out_unflatten f = torch _C _functorch _unwrap_for_grad dual dual = None _exit_dual_level = torch _C _exit_dual_level _exit_dual_level = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _jvp_decrement_nesting = torch _C _functorch _jvp_decrement_nesting _jvp_decrement_nesting = None primals_out_unflatten tangents_out_unflatten aux_ test_jvp_two_tensors_disable_grad counters clear fn x x sin sum wrapper_fn x v torch autograd forward_ad _set_fwd_grad_enabled False torch func jvp fn x v x = torch randn v = torch randn wrapped_gm = _compile_check wrapper_fn x v Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_v_ f l_x_ = L_x_ l_v_ = L_v_ _set_fwd_grad_enabled = torch _C _set_fwd_grad_enabled False _set_fwd_grad_enabled = None _jvp_increment_nesting = torch _C _functorch _jvp_increment_nesting _jvp_increment_nesting = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _enter_dual_level = torch _C _enter_dual_level _enter_dual_level = None _maybe_load_decompositions = torch autograd forward_ad _maybe_load_decompositions _maybe_load_decompositions = None _make_dual f = torch _make_dual l_x_ l_v_ level = l_x_ = l_v_ = None sin f = _make_dual sin _make_dual = None result_duals f = sin sum sin = None _unpack_dual = torch _unpack_dual result_duals level = result_duals = None primal f = _unpack_dual dual f = _unpack_dual _unpack_dual = None primals_out_unflatten f = torch _C _functorch _unwrap_for_grad primal primal = None tangents_out_unflatten f = torch _C _functorch _unwrap_for_grad dual dual = None _exit_dual_level = torch _C _exit_dual_level _exit_dual_level = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled False _set_fwd_grad_enabled_ = None _jvp_decrement_nesting = torch _C _functorch _jvp_decrement_nesting _jvp_decrement_nesting = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None primals_out_unflatten tangents_out_unflatten test_jvp_two_tensors_disable_enable_disable_grad counters clear fn x x sin sum wrapper_fn x v torch autograd forward_ad _set_fwd_grad_enabled False torch autograd forward_ad _set_fwd_grad_enabled True torch autograd forward_ad _set_fwd_grad_enabled False torch func jvp fn x v Start True False True False True True undo False undo True undo x = torch randn v = torch randn wrapped_gm = _compile_check wrapper_fn x v Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_v_ f l_x_ = L_x_ l_v_ = L_v_ _set_fwd_grad_enabled = torch _C _set_fwd_grad_enabled False _set_fwd_grad_enabled = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled False _set_fwd_grad_enabled_ = None _jvp_increment_nesting = torch _C _functorch _jvp_increment_nesting _jvp_increment_nesting = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _enter_dual_level = torch _C _enter_dual_level _enter_dual_level = None _maybe_load_decompositions = torch autograd forward_ad _maybe_load_decompositions _maybe_load_decompositions = None _make_dual f = torch _make_dual l_x_ l_v_ level = l_x_ = l_v_ = None sin f = _make_dual sin _make_dual = None result_duals f = sin sum sin = None _unpack_dual = torch _unpack_dual result_duals level = result_duals = None primal f = _unpack_dual dual f = _unpack_dual _unpack_dual = None primals_out_unflatten f = torch _C _functorch _unwrap_for_grad primal primal = None tangents_out_unflatten f = torch _C _functorch _unwrap_for_grad dual dual = None _exit_dual_level = torch _C _exit_dual_level _exit_dual_level = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled False _set_fwd_grad_enabled_ = None _jvp_decrement_nesting = torch _C _functorch _jvp_decrement_nesting _jvp_decrement_nesting = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled False _set_fwd_grad_enabled_ = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None primals_out_unflatten tangents_out_unflatten test_jvp_freevar_tensor counters clear y = torch randn fn x x sin + y sum wrapper_fn x torch func jvp fn x x x = torch randn expected = wrapper_fn x actual = torch compile wrapper_fn backend= aot_eager fullgraph=True x assertEqual actual expected test_jvp_jvp counters clear check_dynamic_shape_capture skipTest test fails dynamic shapes fn x torch func jvp torch sin x x wrapper_fn x torch func jvp fn x x x = torch randn wrapped_gm = _compile_check wrapper_fn x Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ _jvp_increment_nesting = torch _C _functorch _jvp_increment_nesting _jvp_increment_nesting = None _set_fwd_grad_enabled = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled = None _enter_dual_level = torch _C _enter_dual_level _enter_dual_level = None _maybe_load_decompositions = torch autograd forward_ad _maybe_load_decompositions _maybe_load_decompositions = None child f = torch _make_dual l_x_ l_x_ level = l_x_ = None _jvp_increment_nesting_ = torch _C _functorch _jvp_increment_nesting _jvp_increment_nesting_ = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _maybe_load_decompositions_ = torch autograd forward_ad _maybe_load_decompositions _maybe_load_decompositions_ = None _make_dual_ f = torch _make_dual child child level = child = None result_duals f = torch sin _make_dual_ _make_dual_ = None _unpack_dual = torch _unpack_dual result_duals level = result_duals = None primal f = _unpack_dual dual f = _unpack_dual _unpack_dual = None primals_out_unflatten f = torch _C _functorch _unwrap_for_grad primal primal = None tangents_out_unflatten f = torch _C _functorch _unwrap_for_grad dual dual = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _jvp_decrement_nesting = torch _C _functorch _jvp_decrement_nesting _jvp_decrement_nesting = None _unpack_dual_ = torch _unpack_dual primals_out_unflatten level = primals_out_unflatten = None primal_ f = _unpack_dual_ dual_ f = _unpack_dual_ _unpack_dual_ = None _unpack_dual_ = torch _unpack_dual tangents_out_unflatten level = tangents_out_unflatten = None primal_ f = _unpack_dual_ dual_ f = _unpack_dual_ _unpack_dual_ = None _unwrap_for_grad_ f = torch _C _functorch _unwrap_for_grad primal_ primal_ = None _unwrap_for_grad_ f = torch _C _functorch _unwrap_for_grad primal_ primal_ = None _unwrap_for_grad_ f = torch _C _functorch _unwrap_for_grad dual_ dual_ = None _unwrap_for_grad_ f = torch _C _functorch _unwrap_for_grad dual_ dual_ = None _exit_dual_level = torch _C _exit_dual_level _exit_dual_level = None _set_fwd_grad_enabled_ = torch _C _set_fwd_grad_enabled True _set_fwd_grad_enabled_ = None _jvp_decrement_nesting_ = torch _C _functorch _jvp_decrement_nesting _jvp_decrement_nesting_ = None _unwrap_for_grad_ _unwrap_for_grad_ _unwrap_for_grad_ _unwrap_for_grad_ test_jvp_freevar_python_scalar counters clear y = fn x x sin + y sum wrapper_fn x torch func jvp fn x x x = torch randn expected = wrapper_fn x actual = torch compile wrapper_fn backend= aot_eager fullgraph=True x assertEqual actual expected test_linearize_jvp_fn counters clear wrapper_fn x output jvp_fn = torch func linearize torch sin x output jvp_fn x x = torch randn wrapped_gm = _compile_check wrapper_fn x fullgraph=False graph_idx= Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_self_buffers_tensor_constant _ f l_self_buffers_tensor_constant _ = L_self_buffers_tensor_constant _ alias_default f = torch ops aten alias default l_self_buffers_tensor_constant _ l_self_buffers_tensor_constant _ = None sin_default f = torch ops aten sin default alias_default alias_default_ f = torch ops aten alias default alias_default cos_default f = torch ops aten cos default alias_default_ alias_default_ = None alias_default_ f = torch ops aten alias default sin_default alias_default_ = None alias_default cos_default sin_default wrapped_gm = _compile_check wrapper_fn x fullgraph=False graph_idx= actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_self_modules_FX_CONST_FOLDED_ATTRS_parameters_ _ f L_self_modules_FX_CONST_FOLDED_ATTRS_parameters_ _ f L_flat_tangents_ _ f l_self_modules_fx_const_folded_attrs_parameters_ _ = L_self_modules_FX_CONST_FOLDED_ATTRS_parameters_ _ l_self_modules_fx_const_folded_attrs_parameters_ _ = L_self_modules_FX_CONST_FOLDED_ATTRS_parameters_ _ l_flat_tangents_ _ = L_flat_tangents_ _ _new_zeros_with_same_feature_meta_default f = torch ops aten _new_zeros_with_same_feature_meta default l_flat_tangents_ _ l_self_modules_fx_const_folded_attrs_parameters_ _ l_self_modules_fx_const_folded_attrs_parameters_ _ = None copy__default f = torch ops aten copy_ default _new_zeros_with_same_feature_meta_default l_flat_tangents_ _ _new_zeros_with_same_feature_meta_default = l_flat_tangents_ _ = None mul_tensor f = torch ops aten mul Tensor copy__default l_self_modules_fx_const_folded_attrs_parameters_ _ copy__default = l_self_modules_fx_const_folded_attrs_parameters_ _ = None mul_tensor config patch error_on_recompile=True test_vmap_recompile torch compile backend= eager fn x torch vmap lambda x x sin x x = torch zeros torch vmap fn x should recompile second call See Pytorch issue torch vmap fn x xfailIfTorchDynamo config patch error_on_recompile=True test_vmap_recompile_different_config torch compile backend= eager fn x torch vmap lambda x x sin x x = torch zeros torch vmap fn x assertRaises torch _dynamo exc RecompileError fn x config patch error_on_recompile=True test_vmap_recompile_same_config torch compile backend= eager fn x torch vmap lambda x x sin x x = torch zeros torch vmap torch vmap fn randomness= same randomness= same x assertRaises torch _dynamo exc RecompileError torch vmap torch vmap fn randomness= same randomness= error x config patch error_on_recompile=True test_vmap_recompile_with_randomness torch compile backend= eager fn x torch vmap lambda x x sin x x = torch zeros torch vmap fn randomness= same x assertRaises torch _dynamo exc RecompileError torch vmap fn randomness= different x test_vmap_call_torch_compile_fn wrapped_fn x x sin x = torch randn fn = torch compile backend= aot_eager fullgraph=True wrapped_fn assertRaisesRegex torch _dynamo exc Unsupported Calling torch func vmap\\ compiled_fn\\ function eager mode supported torch func vmap fn x test_vmap_call_compiled_backward_fn See PyTorch issue torch compile f x x x = torch randn requires_grad=True y = f x get_vjp v torch autograd grad y x v assertRaisesRegex RuntimeError It looks like you re trying call compiled backward function within vmap grad vjp which isn t supported torch func vjp get_vjp x test_vjp_call_compiled_backward_fn See PyTorch issue torch compile f x x x = torch randn requires_grad=True y = f x get_vjp v torch autograd grad y x v assertRaisesRegex RuntimeError It looks like you re trying call compiled backward function within vmap grad vjp which isn t supported torch func vjp get_vjp x test_grad_call_compiled_backward_fn See PyTorch issue torch compile f x x x = torch randn requires_grad=True y = f x get_vjp v torch autograd grad y x v assertRaisesRegex RuntimeError It looks like you re trying call compiled backward function within vmap grad vjp which isn t supported torch func grad get_vjp x test_grad_call_torch_compile_fn wrapped_fn x x sin sum x = torch randn fn = torch compile backend= aot_eager fullgraph=True wrapped_fn assertRaisesRegex torch _dynamo exc Unsupported Calling torch func grad\\ compiled_fn\\ function eager mode supported torch func grad fn x test_jvp_call_torch_compile_fn wrapped_fn x x sin sum x = torch randn fn = torch compile backend= aot_eager fullgraph=True wrapped_fn assertRaisesRegex torch _dynamo exc Unsupported Calling torch func jvp\\ compiled_fn\\ function eager mode supported torch func jvp fn x x config patch error_on_recompile=True test_grad_recompile torch compile backend= eager fn x torch func grad torch sin x x = torch randn torch func grad fn x should recompile second call torch func grad fn x test_vmap_get_wrapped counters clear g x x sin torch compile backend= aot_eager fullgraph=True fn torch vmap g x = torch randn expected = torch vmap g x wrapper = fn got = wrapper x assertEqual expected got test_vmap_with_conditional_graph_break g x len x shape torch _dynamo graph_break x sin x cos torch compile backend= aot_eager fn x torch vmap g x counters clear x = torch randn expected = x sin got = fn x assertEqual expected got assertEqual len counters graph_break counters clear y = torch randn expected = y cos got = fn y assertEqual expected got assertEqual len counters graph_break test_vmap_with_graph_break counters clear g x y = x cos print hi y sin fn x torch vmap g x x = torch randn opt = torch compile fn backend= aot_eager fullgraph=False expected = fn x got = opt x assertEqual len counters graph_break assertEqual expected got test_vmap_with_graph_break_ counters clear cos x print cos x cos sin x print sin x sin g x y = cos x sin y fn x torch vmap g randomness= same x x = torch randn opt = torch compile fn backend= aot_eager fullgraph=False expected = fn x got = opt x assertEqual len counters graph_break assertEqual expected got test_vmap_with_graph_break_lambda counters clear sin x print sin x sin fn x torch vmap lambda x sin x x x = torch randn opt = torch compile fn backend= aot_eager fullgraph=False expected = fn x got = opt x assertEqual len counters graph_break assertEqual expected got test_vmap fn x torch func vmap lambda x x sum + x sum x x = torch randn wrapped_gm = _compile_check fn x Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None _add_batch_dim f = torch _functorch predispatch _add_batch_dim l_x_ l_x_ = None sum_ f = _add_batch_dim sum sum_ f = _add_batch_dim sum _add_batch_dim = None batched_outputs f = sum_ + sum_ sum_ = sum_ = None _remove_batch_dim f = torch _functorch predispatch _remove_batch_dim batched_outputs batched_outputs = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None _remove_batch_dim test_vmap_free_const y = fn x torch func vmap lambda x x sum + x sum + y x x = torch randn wrapped_gm = _compile_check fn x Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None _add_batch_dim f = torch _functorch predispatch _add_batch_dim l_x_ l_x_ = None sum_ f = _add_batch_dim sum sum_ f = _add_batch_dim sum _add_batch_dim = None add f = sum_ + sum_ sum_ = sum_ = None batched_outputs f = add + add = None _remove_batch_dim f = torch _functorch predispatch _remove_batch_dim batched_outputs batched_outputs = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None _remove_batch_dim test_vmap_free_tensor y = torch randn fn x torch func vmap lambda x x sum + x sum + y x x = torch randn wrapped_gm = _compile_check fn x Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None _add_batch_dim f = torch _functorch predispatch _add_batch_dim l_x_ l_x_ = None sum_ f = _add_batch_dim sum sum_ f = _add_batch_dim sum _add_batch_dim = None add f = sum_ + sum_ sum_ = sum_ = None batched_outputs f = add + l_y_ add = l_y_ = None _remove_batch_dim f = torch _functorch predispatch _remove_batch_dim batched_outputs batched_outputs = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None _remove_batch_dim test_vmap_two_inputs fn x y torch func vmap lambda x y x sum + x sum + y in_dims= x y x = torch randn y = torch randn wrapped_gm = _compile_check fn x y Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None _add_batch_dim f = torch _functorch predispatch _add_batch_dim l_x_ l_x_ = None _add_batch_dim_ f = torch _functorch predispatch _add_batch_dim l_y_ l_y_ = None sum_ f = _add_batch_dim sum sum_ f = _add_batch_dim sum _add_batch_dim = None add f = sum_ + sum_ sum_ = sum_ = None batched_outputs f = add + _add_batch_dim_ add = _add_batch_dim_ = None _remove_batch_dim f = torch _functorch predispatch _remove_batch_dim batched_outputs batched_outputs = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None _remove_batch_dim test_vmap_two_inputs_tuple_in_dims in_dims = fn x y torch func vmap lambda x y x sum + x sum + y in_dims=in_dims x y x = torch randn y = torch randn wrapped_gm = _compile_check fn x y Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None _add_batch_dim f = torch _functorch predispatch _add_batch_dim l_x_ l_x_ = None _add_batch_dim_ f = torch _functorch predispatch _add_batch_dim l_y_ l_y_ = None sum_ f = _add_batch_dim sum sum_ f = _add_batch_dim sum _add_batch_dim = None add f = sum_ + sum_ sum_ = sum_ = None batched_outputs f = add + _add_batch_dim_ add = _add_batch_dim_ = None _remove_batch_dim f = torch _functorch predispatch _remove_batch_dim batched_outputs batched_outputs = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None _remove_batch_dim test_vmap_over_vmap_two_inputs fn x y torch func vmap torch func vmap lambda x y x + y in_dims= x y x = torch randn y = torch randn wrapped_gm = _compile_check fn x y Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None child f = torch _functorch predispatch _add_batch_dim l_x_ l_x_ = None child_ f = torch _functorch predispatch _add_batch_dim l_y_ l_y_ = None lazy_load_decompositions_ = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions_ = None _vmap_increment_nesting_ = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting_ = None _add_batch_dim_ f = torch _functorch predispatch _add_batch_dim child child = None _add_batch_dim_ f = torch _functorch predispatch _add_batch_dim child_ child_ = None batched_outputs f = _add_batch_dim_ + _add_batch_dim_ _add_batch_dim_ = _add_batch_dim_ = None batched_outputs_ f = torch _functorch predispatch _remove_batch_dim batched_outputs batched_outputs = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None _remove_batch_dim_ f = torch _functorch predispatch _remove_batch_dim batched_outputs_ batched_outputs_ = None _vmap_decrement_nesting_ = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting_ = None _remove_batch_dim_ test_vmap_over_vmap_captured x = torch ones y = torch ones fn x torch func vmap torch func vmap lambda y x y y wrapped_gm = _compile_check fn x Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_y_ f L_x_ f l_y_ = L_y_ l_x_ = L_x_ lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None child f = torch _functorch predispatch _add_batch_dim l_y_ l_y_ = None lazy_load_decompositions_ = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions_ = None _vmap_increment_nesting_ = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting_ = None _add_batch_dim_ f = torch _functorch predispatch _add_batch_dim child child = None batched_outputs f = l_x_ _add_batch_dim_ l_x_ = _add_batch_dim_ = None batched_outputs_ f = torch _functorch predispatch _remove_batch_dim batched_outputs batched_outputs = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None _remove_batch_dim_ f = torch _functorch predispatch _remove_batch_dim batched_outputs_ batched_outputs_ = None _vmap_decrement_nesting_ = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting_ = None _remove_batch_dim_ test_vmap_multiple_outputs x = torch ones fn x torch vmap lambda x x sum x sum x wrapped_gm = _compile_check fn x Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None _add_batch_dim f = torch _functorch predispatch _add_batch_dim l_x_ l_x_ = None child f = _add_batch_dim sum child_ f = _add_batch_dim sum _add_batch_dim = None _remove_batch_dim f = torch _functorch predispatch _remove_batch_dim child child = None _remove_batch_dim_ f = torch _functorch predispatch _remove_batch_dim child_ child_ = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None _remove_batch_dim _remove_batch_dim_ test_vmap_multiple_outputs_diff_dims x = torch ones fn x torch vmap lambda x x sum x sum out_dims= x wrapped_gm = _compile_check fn x Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None _add_batch_dim f = torch _functorch predispatch _add_batch_dim l_x_ l_x_ = None child f = _add_batch_dim sum child_ f = _add_batch_dim sum _add_batch_dim = None _remove_batch_dim f = torch _functorch predispatch _remove_batch_dim child child = None _remove_batch_dim_ f = torch _functorch predispatch _remove_batch_dim child_ child_ = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None _remove_batch_dim _remove_batch_dim_ test_vmap_multiple_outputs_out_dims_tuple x = torch ones out_dims = fn x torch vmap lambda x x sum x sum out_dims=out_dims x wrapped_gm = _compile_check fn x Dynamic shapes produce slightly different graph check_dynamic_shape_capture actual = normalize_gm wrapped_gm print_readable print_output=False assertExpectedInline actual \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ lazy_load_decompositions = torch _functorch predispatch lazy_load_decompositions lazy_load_decompositions = None _vmap_increment_nesting = torch _functorch predispatch _vmap_increment_nesting error _vmap_increment_nesting = None _add_batch_dim f = torch _functorch predispatch _add_batch_dim l_x_ l_x_ = None child f = _add_batch_dim sum child_ f = _add_batch_dim sum _add_batch_dim = None _remove_batch_dim f = torch _functorch predispatch _remove_batch_dim child child = None _remove_batch_dim_ f = torch _functorch predispatch _remove_batch_dim child_ child_ = None _vmap_decrement_nesting = torch _functorch predispatch _vmap_decrement_nesting _vmap_decrement_nesting = None _remove_batch_dim _remove_batch_dim_ test_vmap_kwargs counters clear x = torch ones y = torch randn fn x y torch func vmap lambda x y x + y x y=y actual = fn x y expected = torch compile fn backend= aot_eager fullgraph=False x y assertEqual len counters graph_break assertEqual actual expected test_vmap_pytree_inputs counters clear x = torch ones y = torch randn vmap_fn inps x = inps x y = inps y x + y fn x y torch func vmap vmap_fn x x y y actual = fn x y expected = torch compile fn backend= aot_eager fullgraph=False x y assertEqual len counters graph_break assertEqual actual expected test_vmap_side_effects counters clear x = torch ones y = torch randn some_list = f x y some_list append x + y wrapper_fn x y torch func vmap f x y actual = wrapper_fn x y expected = torch compile wrapper_fn backend= aot_eager fullgraph=False x y assertEqual len counters graph_break assertEqual actual expected assertEqual some_list unittest expectedFailure test_vmap_side_effects_append_input counters clear x = torch ones y = torch randn some_list = f x y some_list append x x + y wrapper_fn x y torch func vmap f x y actual = wrapper_fn x y expected = torch compile wrapper_fn backend= aot_eager fullgraph=False x y assertEqual len counters graph_break assertEqual actual expected test_vmap_previous_illegal_op_no_graph_break counters clear calling stride would previously graph break bad_fn x y = x view y stride y wrapper_fn x torch func vmap bad_fn x x = torch randn actual = wrapper_fn x expected = torch compile wrapper_fn backend= aot_eager fullgraph=False x assertEqual len counters graph_break assertEqual actual expected test_vmap_multiple_invocation_in_dims counters clear wrapper_fn x in_dims torch func vmap torch sum in_dims x x = torch randn cnt = CompileCounter opt = torch compile wrapper_fn backend=cnt fullgraph=False dynamic=True expected = wrapper_fn x wrapper_fn x wrapper_fn x Third invocation ` opt ` makes ` in_dims ` SymInt actual = opt x opt x opt x assertEqual expected actual assertEqual cnt frame_count assertEqual cnt op_count test_vmap_multiple_invocation_out_dims counters clear wrapper_fn x out_dims torch func vmap lambda x torch sum x out_dims=out_dims x x = torch randn cnt = CompileCounter opt = torch compile wrapper_fn backend=cnt fullgraph=False dynamic=True expected = wrapper_fn x wrapper_fn x wrapper_fn x Third invocation ` opt ` makes ` in_dims ` SymInt actual = opt x opt x opt x assertEqual expected actual assertEqual cnt frame_count assertEqual cnt op_count test_vmap_out_dims_None issue https github com pytorch pytorch issues fn x y x y wrapper_fn x y torch func vmap fn in_dims= None out_dims= None x y x y = torch randn torch randn expected = wrapper_fn x y got = torch compile wrapper_fn backend= aot_eager fullgraph=True x y assertEqual expected got test_vmap_new_tensor_in_body fn x x + torch ones wrapper_fn x torch func vmap fn x x = torch randn opt = torch compile wrapper_fn backend= aot_eager fullgraph=True expected = wrapper_fn x actual = opt x assertEqual expected actual test_vmap_new_tensor_unused_in_body fn x torch tensor wrapper_fn x torch func vmap fn x x = torch randn opt = torch compile wrapper_fn backend= aot_eager fullgraph=True expected = wrapper_fn x actual = opt x assertEqual expected actual test_vmap_new_tensor_implicit_via_op wrapper_fn x torch func vmap lambda t torch add t x x = torch randn opt = torch compile wrapper_fn backend= aot_eager fullgraph=True expected = wrapper_fn x actual = opt x assertEqual expected actual ActivationCheckpointingTests torch _dynamo test_case TestCase _validate fn backend args skip_check=False fullgraph=True cloned_args = arg args cloned_args append arg detach clone requires_grad_ arg requires_grad torch manual_seed expected = fn args expected sum backward opt_fn = torch compile fn fullgraph=fullgraph backend=backend torch manual_seed result = opt_fn cloned_args result sum backward skip_check assertEqual result expected arg cloned_arg zip args cloned_args assertEqual arg grad cloned_arg grad requires_cuda_and_triton torch _functorch config patch functionalize_rng_ops=True test_function gn x y torch sigmoid torch matmul x y fn x y torch utils checkpoint checkpoint gn torch sin x y use_reentrant=True x = torch randn requires_grad=True y = torch randn requires_grad=True fw_compiler = functools partial count_ops freq= op=torch ops aten mm default bw_compiler = functools partial count_ops freq= op=torch ops aten mm default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler _validate fn backend x y requires_cuda_and_triton torch _functorch config patch functionalize_rng_ops=True test_function_with_kwargs gn x y torch sigmoid torch matmul x y fn x y torch utils checkpoint checkpoint gn torch sin x y use_reentrant=True preserve_rng_state=False x = torch randn requires_grad=True y = torch randn requires_grad=True fw_compiler = functools partial count_ops freq= op=torch ops aten mm default bw_compiler = functools partial count_ops freq= op=torch ops aten mm default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler _validate fn backend x y requires_cuda_and_triton torch _functorch config patch functionalize_rng_ops=True test_dropout gn x y torch nn functional dropout torch matmul x y p= fn x y torch utils checkpoint checkpoint gn torch sin x y use_reentrant=True x = torch randn device= cuda requires_grad=True y = torch randn device= cuda requires_grad=True fw_compiler = functools partial count_ops freq= op=torch ops rngprims philox_rand default philox_rand passed fwd bw_compiler = functools partial count_ops freq= op=torch ops rngprims philox_rand default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler _validate fn backend x y skip_check=True dropout decomp known diverge eager requires_cuda_and_triton torch _functorch config patch functionalize_rng_ops=True test_dropout_inductor gn x y torch nn functional dropout torch matmul x y p= fn x y torch utils checkpoint checkpoint gn torch sin x y use_reentrant=True x = torch randn device= cuda requires_grad=True y = torch randn device= cuda requires_grad=True backend = inductor _validate fn backend x y skip_check=True dropout decomp known diverge eager requires_cuda_and_triton torch _functorch config patch functionalize_rng_ops=True test_fallback gn x y torch _dynamo graph_break torch sigmoid torch matmul x y fn x y torch cos torch utils checkpoint checkpoint gn torch sin x y use_reentrant=True x = torch randn requires_grad=True y = torch randn requires_grad=True args = x y backend = EagerAndRecordGraphs cnt = CompileCounterWithBackend backend expected = fn args result = torch compile fn backend=cnt args assertEqual result expected One graph torch sin input other torch cos assertEqual cnt frame_count assertEqual cnt op_count assertEqual len backend graphs requires_cuda_and_triton torch _functorch config patch functionalize_rng_ops=True test_module MockModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x torch sigmoid linear x mod = MockModule fn x torch utils checkpoint checkpoint mod torch sin x use_reentrant=True x = torch randn requires_grad=True fw_compiler = functools partial count_ops freq= op=torch ops aten sigmoid default sigmoid passed fwd bw_compiler = functools partial count_ops freq= op=torch ops aten sigmoid default backend = aot_autograd fw_compiler=fw_compiler bw_compiler=bw_compiler _validate fn backend x test_override_fallthrough_dispatch_key _FallthroughTestOnly torch _ops HigherOrderOperator __init__ super __init__ _fallthrough_test_only __call__ args kwargs super __call__ args kwargs test_op = _FallthroughTestOnly default_keys = torch _ops _HIGHER_ORDER_OP_DEFAULT_FALLTHROUGH_DISPATCH_KEYS assertTrue any test_op non_fallthrough_keys has key key default_keys foos = lambda x=i x i k enumerate default_keys foo fallthrough_key zip foos default_keys test_op py_impl fallthrough_key foo assertTrue all test_op non_fallthrough_keys has key key default_keys assertEqual list range len default_keys test_op py_kernels key key default_keys test_cond_with_kwargs torch _higher_order_ops cond cond_op test pred x true_fn x x clone false_fn x -x cond_op pred=pred true_fn=true_fn false_fn=false_fn operands= x cnt = CompileCounter opt_test = torch compile test backend=cnt fullgraph=True inp = torch ones true_pred = torch Tensor True false_pred = torch Tensor False assertTrue torch allclose test true_pred inp opt_test true_pred inp assertEqual cnt frame_count assertTrue torch allclose test false_pred inp opt_test false_pred inp assertEqual cnt frame_count test_cond_with_invalid_kwargs torch _higher_order_ops cond cond_op test pred mode x true_fn x x clone false_fn x -x mode cond_op pred=pred true_fn=true_fn false_fn=false_fn operands= x invalid=True cond_op pred pred=pred true_fn=true_fn false_fn=false_fn operands= x cnt = CompileCounter opt_test = torch compile test backend=cnt inp = torch ones assertRaises torch _dynamo exc UncapturedHigherOrderOpError opt_test True True inp assertRaises AssertionError opt_test True False inp test_cond_with_mismatched_output output_mismatch_test x true_fn torch concat x x false_fn x sin torch cond x sum true_fn false_fn x = torch randn output_mismatch_test x torch compile output_mismatch_test backend= eager x test_non_aliasing_util torch _dynamo variables higher_order_ops _assert_tensors_nonaliasing = torch tensor torch tensor b = torch tensor _assert_tensors_nonaliasing b assertRaisesRegex AssertionError inputs function body cannot alias outputs _assert_tensors_nonaliasing test_flop_counter_for_cond torch utils flop_counter FlopCounterMode Mod torch nn Module __init__ super __init__ linear = torch nn Linear forward x torch cond torch tensor True lambda x linear x lambda x linear linear x x mod = Mod FlopCounterMode mod display=False mode mod torch randn assertEqual mode get_flop_counts Global torch ops aten addmm Mod torch ops aten addmm Mod linear torch ops aten addmm test_flop_counter_for_nested_cond torch utils flop_counter FlopCounterMode Mod torch nn Module __init__ super __init__ linear = torch nn Linear linear = torch nn Linear forward x true_branch x Nested cond inside true branch torch cond torch tensor True lambda x linear x lambda x linear x x false_branch x linear linear x torch cond torch tensor True true_branch false_branch x mod = Mod FlopCounterMode mod display=False mode mod torch randn assertEqual mode get_flop_counts Global torch ops aten addmm Mod torch ops aten addmm Mod linear torch ops aten addmm Mod linear torch ops aten addmm test_flop_counter_for_cond_unbalanced_branches torch utils flop_counter FlopCounterMode Mod torch nn Module __init__ super __init__ linear = torch nn Linear forward x true_branch x linear x false_branch x x clone torch cond torch tensor True true_branch false_branch x mod = Mod FlopCounterMode mod display=False mode mod torch randn assertEqual mode get_flop_counts Global torch ops aten addmm Mod torch ops aten addmm Mod linear torch ops aten addmm xfail_hops_compile = aot_eager map assert type args realize TensorVariable scan scan OpOverload local_map_hop can t retrace inductor while_loop LoweringException AssertionError flex_attention LoweringException AssertionError flex_attention_backward AssertionError Input shapes should have M = N = K = TestHigherOrderOpsOpInfo torch _dynamo test_case TestCase requires_cuda_and_triton parametrize backend aot_eager inductor ops list filter lambda op op name xfail_hops_compile hop_db allowed_dtypes= torch float test_hops_compile device dtype op backend Ensure HOPs can compiled backend == aot_eager op name == invoke_quant raise unittest SkipTest TODO partitioner fails migrate canonicalization aot eager backend sample_inputs_itr = op sample_inputs device dtype requires_grad=op supports_autograd inp sample_inputs_itr input = inp input isinstance inp input tuple inp input eager_args = input inp args eager_kwargs = inp kwargs compiled_args = deepcopy eager_args compiled_kwargs = deepcopy eager_kwargs fn args kwargs op op args kwargs compiled_fn = torch compile fn backend=backend fullgraph=True eager_out = fn eager_args eager_kwargs compiled_out = compiled_fn compiled_args compiled_kwargs assertEqual eager_out compiled_out instantiate_device_type_tests TestHigherOrderOpsOpInfo globals only_for= cuda __name__ == __main__ torch _dynamo test_case run_tests run_tests