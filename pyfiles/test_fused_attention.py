Owner s module inductor functools itertools math torch torch _inductor config torch utils checkpoint torch _dynamo debug_utils aot_graph_input_parser torch _dynamo utils counters torch _inductor test_case run_tests TestCase torch _inductor utils run_and_get_code torch testing _internal common_cuda PLATFORM_SUPPORTS_FUSED_ATTENTION SM OrLater torch testing _internal common_utils IS_LINUX skipIfRocm torch testing _internal inductor_utils GPU_TYPE HAS_CPU HAS_CUDA_AND_TRITON HAS_XPU_AND_TRITON checkpoint_wrapper fn inner args torch utils checkpoint checkpoint fn args use_reentrant=True inner TestSDPAPatternRewriterTemplate TestCase use_static_shapes = True _clone_inputs inputs clone x isinstance x torch Tensor x x clone clone x x inputs _check_common dot_prod_attention args =None contains=True atol= e- has_fuse_pattern=True has_dropout=False check_train=True override_check_equal=False dtype=torch float rtol= e- args None tensor_shape = args = torch randn tensor_shape device=self device dtype=dtype torch randn tensor_shape device=self device dtype=dtype torch randn tensor_shape device=self device dtype=dtype args = list args args = _clone_inputs args training False True check_train False training device == xpu Intel GPU have implemented sdpa backward yet mode TODO remove when sdpa backward implemented XPU continue x itertools chain args args isinstance x torch Tensor x is_floating_point x requires_grad = training use_static_shapes torch _dynamo mark_dynamic args torch _dynamo mark_dynamic args torch _dynamo mark_dynamic args dropout_arg = training has_dropout torch manual_seed result = dot_prod_attention args + dropout_arg counters clear torch manual_seed result source_code = run_and_get_code torch compile dot_prod_attention fullgraph=True args + dropout_arg source_code = \n join source_code has_fuse_pattern assertGreaterEqual counters inductor fuse_attention contains many patterns get re-expanded dispatcher assertIn aten _scaled_dot_product source_code some tests configured very low dropout where we still want check equality has_dropout override_check_equal assertEqual result result atol=atol rtol= e- training result sum backward result sum backward arg arg zip args args isinstance arg torch Tensor arg is_floating_point has_dropout override_check_equal assertEqual arg grad arg grad atol=atol rtol=rtol _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor - torch Tensor Input tensors assumed have shape batch_size n_head seq_len embed_dim torch matmul query key transpose - - div math sqrt key shape - softmax dim=- matmul value dtype torch float torch half atol = rtol = e- dtype == torch float device cpu xpu dtype == torch half atol = e- rtol = e- _check_common dot_prod_attention dtype=dtype atol=atol rtol=rtol _check_common checkpoint_wrapper dot_prod_attention dtype=dtype atol=atol rtol=rtol torch _inductor config patch freezing True _test_sdpa_rewriter_ _freezing dot_prod_attention query torch Tensor key torch Tensor value torch Tensor - torch Tensor Input tensors assumed have shape batch_size n_head seq_len embed_dim torch matmul query key transpose - - div math sqrt key shape - softmax dim=- matmul value dtype torch half atol = rtol = e- dtype == torch float device cpu xpu dtype == torch half atol = e- rtol = e- torch no_grad _check_common dot_prod_attention dtype=dtype atol=atol rtol=rtol check_train=False _test_insignificant_strides device == xpu skipTest The operator aten _scaled_dot_product_efficient_attention currently implemented XPU device f = torch float repro taken https github com pytorch pytorch issues constant_pad_nd single element tensor gets expanded forward permute_ f permute_ f permute_ f permute_ f mul_ f cat = torch ops aten cat default permute_ permute_ permute_ = None cos = torch ops aten cos default cat sin = torch ops aten sin default cat unsqueeze_ = torch ops aten unsqueeze default cos cos = None unsqueeze_ = torch ops aten unsqueeze default sin sin = None mul_ = torch ops aten mul Tensor permute_ unsqueeze_ slice_ = torch ops aten slice Tensor permute_ slice_ = torch ops aten slice Tensor permute_ permute_ = None neg = torch ops aten neg default slice_ slice_ = None cat_ = torch ops aten cat default neg slice_ neg = slice_ = None mul_ = torch ops aten mul Tensor cat_ unsqueeze_ cat_ = None add_ = torch ops aten add Tensor mul_ mul_ mul_ = mul_ = None mul_ = torch ops aten mul Tensor permute_ unsqueeze_ unsqueeze_ = None slice_ = torch ops aten slice Tensor permute_ slice_ = torch ops aten slice Tensor permute_ permute_ = None neg_ = torch ops aten neg default slice_ slice_ = None cat_ = torch ops aten cat default neg_ slice_ neg_ = slice_ = None mul_ = torch ops aten mul Tensor cat_ unsqueeze_ cat_ = unsqueeze_ = None add_ = torch ops aten add Tensor mul_ mul_ mul_ = mul_ = None slice_ = torch ops aten slice Tensor mul_ mul_ = None slice_ = torch ops aten slice Tensor slice_ slice_ = None slice_ = torch ops aten slice Tensor slice_ slice_ = None constant_pad_nd = torch ops aten constant_pad_nd default slice_ slice_ = None slice_ = torch ops aten slice Tensor constant_pad_nd - constant_pad_nd = None expand_ = torch ops aten expand default slice_ _scaled_dot_product_efficient_attention = torch ops aten _scaled_dot_product_efficient_attention default add_ add_ permute_ expand_ True _scaled_dot_product_efficient_attention kwargs = aot_graph_input_parser forward device=GPU_TYPE runs successfully out_eager = forward kwargs out_c = torch compile forward kwargs dont compare philox_seed offset torch testing assert_close out_eager out_c _test_pattern_fails_with_reuse This test checks replacement done when intermediate result being used returned downstream torch compile fullgraph=True dot_prod_attention query torch Tensor key torch Tensor value torch Tensor - torch Tensor attn_weights = torch matmul query key transpose - - div math sqrt key shape - softmax dim=- attn_weights matmul value attn_weights tensor_shape = args = torch randn tensor_shape device=self device torch randn tensor_shape device=self device torch randn tensor_shape device=self device _ source_code = run_and_get_code dot_prod_attention args assertNotIn aten _scaled_dot_product_efficient_attention source_code _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor - torch Tensor torch matmul query key transpose - - mul math sqrt key shape - softmax dim=- matmul value _check_common dot_prod_attention _check_common checkpoint_wrapper dot_prod_attention _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor training bool - torch Tensor torch nn functional dropout torch matmul query key transpose - - div softmax dim=- p= training=training inplace=False matmul value _check_common dot_prod_attention contains=False has_dropout=True _check_common checkpoint_wrapper dot_prod_attention contains=False has_dropout=True _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor training bool - torch Tensor torch nn functional dropout torch matmul query key transpose - - mul softmax dim=- p= inplace=False training=training matmul value _check_common dot_prod_attention contains=False has_dropout=True _check_common checkpoint_wrapper dot_prod_attention contains=False has_dropout=True _test_sdpa_rewriter_ sfdp_pattern_ _v query key value attn_mask = torch ones query size - key size - dtype=torch bool device=query device tril diagonal= attn_mask = attn_mask masked_fill torch logical_not attn_mask -float inf attn_weight = torch softmax query key transpose - - math sqrt query size - + attn_mask dim=- attn_weight value sfdp_pattern_ _v query key value https github com pytorch pytorch issues attn_mask = torch zeros query size - key size - dtype=torch bool device=query device bool attn_weight = torch softmax query key transpose - - math sqrt query size - + attn_mask dim=- attn_weight value _check_common sfdp_pattern_ _v contains=False _check_common checkpoint_wrapper sfdp_pattern_ _v contains=False _check_common sfdp_pattern_ _v contains=False _check_common checkpoint_wrapper sfdp_pattern_ _v contains=False _test_sdpa_rewriter_ sfdp_pattern_ query key value training attn_mask = torch ones query size - key size - dtype=torch bool device=query device tril diagonal= attn_mask = attn_mask masked_fill torch logical_not attn_mask -float inf attn_weight = torch softmax query key transpose - - math sqrt query size - + attn_mask dim=- attn_weight = torch nn functional dropout attn_weight training attn_weight value _check_common sfdp_pattern_ contains=False has_dropout=True _check_common checkpoint_wrapper sfdp_pattern_ contains=False has_dropout=True _test_sdpa_rewriter_ sfdp_pattern_ query key value training q = query permute k = key permute v = value permute div = q k transpose - - math sqrt q size - div = div torch float attn_weight = torch softmax div dim=- Set False attn_weight = torch dropout attn_weight training attn_weight = attn_weight torch float attn_weight v args = torch randn device=self device dtype=torch half torch randn device=self device dtype=torch half torch randn device=self device dtype=torch half _check_common sfdp_pattern_ args contains=SM OrLater has_dropout=True override_check_equal=True atol= e- args = torch randn device=GPU_TYPE dtype=torch half torch randn device=GPU_TYPE dtype=torch half torch randn device=GPU_TYPE dtype=torch half _check_common checkpoint_wrapper sfdp_pattern_ args contains=SM OrLater has_dropout=True override_check_equal=True atol= e- _test_sdpa_rewriter_ sfdp_pattern_ query key value q = query permute k = key permute v = value permute div = q k transpose - - math sqrt q size - div = div torch float attn_weight = torch softmax div dim=- attn_weight = attn_weight torch float attn_weight v args = torch randn device=self device dtype=torch half torch randn device=self device dtype=torch half torch randn device=self device dtype=torch half _check_common sfdp_pattern_ args atol= e- args = torch randn device=GPU_TYPE dtype=torch half torch randn device=GPU_TYPE dtype=torch half torch randn device=GPU_TYPE dtype=torch half _check_common checkpoint_wrapper sfdp_pattern_ args atol= e- _test_sdpa_rewriter_ sfdp_pattern_ query key value training q = query permute k = key permute v = value permute q = q math sqrt q size - div = q k transpose - - div = div torch float attn_weight = torch softmax div dim=- very low dropout make test pass attn_weight = torch dropout attn_weight training attn_weight = attn_weight torch float attn_weight v args = torch randn device=self device dtype=torch half torch randn device=self device dtype=torch half torch randn device=self device dtype=torch half _check_common sfdp_pattern_ args contains=SM OrLater has_dropout=True override_check_equal=True atol= e- args = torch randn device=GPU_TYPE dtype=torch half torch randn device=GPU_TYPE dtype=torch half torch randn device=GPU_TYPE dtype=torch half _check_common checkpoint_wrapper sfdp_pattern_ args contains=SM OrLater has_dropout=True override_check_equal=True atol= e- _test_sdpa_rewriter_ sfdp_pattern_ query key value q = query permute k = key permute v = value permute q = q math sqrt q size - div = q k transpose - - div = div torch float attn_weight = torch softmax div dim=- attn_weight = attn_weight torch float attn_weight v args = torch randn device=self device dtype=torch half torch randn device=self device dtype=torch half torch randn device=self device dtype=torch half _check_common sfdp_pattern_ args atol= e- args = torch randn device=GPU_TYPE dtype=torch half torch randn device=GPU_TYPE dtype=torch half torch randn device=GPU_TYPE dtype=torch half _check_common checkpoint_wrapper sfdp_pattern_ args atol= e- _test_pattern_fails_with_tensor_factor https github com pytorch pytorch issues Model torch nn Module __init__ is_inv_factor super __init__ is_inv_factor = is_inv_factor forward query key value scale_factor - torch Tensor Dividing scale_factor makes scale_factor gradients very unstable scale_factor = scale_factor detach y = torch matmul query key transpose - - is_inv_factor y = y div scale_factor y = y mul scale_factor y softmax dim=- matmul value tensor_shape = is_inv_factor True False args = torch randn tensor_shape device=self device torch randn tensor_shape device=self device torch randn tensor_shape device=self device torch randn device=self device model = Model is_inv_factor eval The training path has accuracy gap compared eager mode _check_common model args =args contains=False atol= e- has_fuse_pattern=False _test_pattern_fails_with_unsupported_mask use_static_shapes skipTest Causes shape specialization TODO investigate https github com pytorch pytorch issues Model torch nn Module __init__ super __init__ forward query key value attn_mask - torch Tensor attn_weight = torch softmax query key transpose - - math sqrt query size - + attn_mask dim=- attn_weight value tensor_shape = upsupported_masks = torch randn device=self device dtype=torch int atte_mask upsupported_masks args = torch randn tensor_shape device=self device torch randn tensor_shape device=self device torch randn tensor_shape device=self device atte_mask model = Model eval The training path has accuracy gap compared eager mode _check_common model args =args contains=False atol= e- has_fuse_pattern=False _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor - torch Tensor Input tensors assumed have shape batch_size seq_len n_head embed_dim q = query transpose k = key transpose v = value transpose torch matmul q k transpose - - div math sqrt key shape - softmax dim=- matmul v _check_common dot_prod_attention _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor training bool - torch Tensor Input tensors assumed have shape batch_size seq_len n_head embed_dim q = query transpose k = key transpose v = value transpose torch nn functional dropout torch matmul q k transpose - - div math sqrt key shape - softmax dim=- matmul v p= training=training inplace=False _check_common dot_prod_attention contains=False has_dropout=True _test_sdpa_prev_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor - torch Tensor Input tensors assumed have shape batch_size n_head seq_len embed_dim torch matmul query key transpose - - div math sqrt key shape - softmax dim=- clone matmul value _check_common dot_prod_attention check_train=False _check_common checkpoint_wrapper dot_prod_attention check_train=False _test_sdpa_prev_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor - torch Tensor torch matmul query key transpose - - mul math sqrt key shape - softmax dim=- clone matmul value _check_common dot_prod_attention check_train=False _check_common checkpoint_wrapper dot_prod_attention check_train=False _test_sdpa_prev_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor - torch Tensor Input tensors assumed have shape batch_size seq_len n_head embed_dim q = query transpose k = key transpose v = value transpose torch matmul q k transpose - - div math sqrt key shape - softmax dim=- clone matmul v _check_common dot_prod_attention check_train=False _test_sdpa_rewriter_ dtype dot_prod_attention query torch Tensor key torch Tensor value torch Tensor training bool - torch Tensor Input tensors assumed have shape batch_size seq_len n_head embed_dim attn_weight = torch bmm query key transpose softmax dim=- attn_weight = torch nn functional dropout attn_weight p= training=training torch bmm attn_weight value tensor_shape = args = torch randn tensor_shape device=self device dtype=dtype torch randn tensor_shape device=self device dtype=dtype torch randn tensor_shape device=self device dtype=dtype _check_common dot_prod_attention check_train=False args =args has_dropout=True override_check_equal=True atol= e- rtol= e- _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor - torch Tensor Input tensors assumed have shape batch_size seq_len n_head embed_dim attn_mask = torch ones query size key size dtype=torch bool device=query device tril diagonal= attn_mask = attn_mask masked_fill torch logical_not attn_mask -float inf q = query permute k = key permute v = value permute torch matmul q k transpose - - div + attn_mask softmax dim=- matmul v _check_common dot_prod_attention _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor - torch Tensor Input tensors assumed have shape batch_size seq_len n_head embed_dim q = query transpose k = key transpose v = value transpose bs = q size k_len = k size - attn_mask = torch ones bs k_len dtype=torch bool device=query device tril diagonal= scores = torch matmul q k transpose - - attn_mask = attn_mask == view bs k_len expand_as scores scores = scores masked_fill attn_mask -float inf weights = torch nn functional softmax scores dim=- torch matmul weights v _check_common dot_prod_attention check_train=False skipIfRocm _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor training - torch Tensor Input tensors assumed have shape batch_size seq_len n_head embed_dim attn_mask = torch ones query size key size dtype=torch bool device=query device tril diagonal= attn_mask = attn_mask masked_fill torch logical_not attn_mask -float inf q = query permute k = key permute v = value permute torch nn functional dropout torch matmul q k transpose - - div + attn_mask softmax dim=- p= training=training inplace=False matmul v _check_common dot_prod_attention contains=False has_dropout=True also check batch_size= because graph slightly different tensor_shape = args = torch randn tensor_shape device=self device torch randn tensor_shape device=self device torch randn tensor_shape device=self device _check_common dot_prod_attention args =args contains=False has_dropout=True skipIfRocm _test_sdpa_rewriter_ _fp _mask dot_prod_attention query torch Tensor key torch Tensor value torch Tensor training - torch Tensor Input tensors assumed have shape batch_size seq_len n_head embed_dim attn_mask = torch randn query size key size dtype=torch float device=query device tril diagonal= q = query permute k = key permute v = value permute torch nn functional dropout torch matmul q k transpose - - div + attn_mask softmax dim=- p= training=training inplace=False matmul v _check_common dot_prod_attention contains=False has_dropout=True also check batch_size= because graph slightly different tensor_shape = args = torch randn tensor_shape device=self device torch randn tensor_shape device=self device torch randn tensor_shape device=self device _check_common dot_prod_attention args =args contains=False has_dropout=True _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor training - torch Tensor Input tensors assumed have shape batch_size seq_len n_head embed_dim q = query transpose k = key transpose v = value transpose bs = q size k_len = k size - attn_mask = torch ones bs k_len dtype=torch bool device=query device tril diagonal= scores = torch matmul q k transpose - - attn_mask = attn_mask == view bs k_len expand_as scores scores = scores masked_fill attn_mask -float inf weights = torch nn functional softmax scores dim=- weights = torch nn functional dropout weights p= training=training inplace=False torch matmul weights v _check_common dot_prod_attention check_train=False has_dropout=True skipIfRocm _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor causal_mask torch Tensor - torch Tensor hf_GPT dropout query = query permute key = key permute value = value permute attn_weights = torch matmul query key permute inv_scale = torch full math sqrt value size - dtype=query dtype device=query device attn_weights = attn_weights div inv_scale causal_mask_value = torch full torch finfo query dtype min dtype=query dtype device=query device attn_weights = torch where causal_mask attn_weights causal_mask_value torch nn functional dropout attn_weights softmax dim=- matmul value key permute value permute tensor_shape = causal_mask = torch ones dtype=torch bool device=self device tril diagonal= args = torch randn tensor_shape device=self device torch randn tensor_shape device=self device torch randn tensor_shape device=self device causal_mask _check_common dot_prod_attention args =args contains=False has_dropout=False check_train=False also check batch_size= because graph slightly different tensor_shape = args = torch randn tensor_shape device=self device torch randn tensor_shape device=self device torch randn tensor_shape device=self device causal_mask _check_common dot_prod_attention args =args contains=False has_dropout=False check_train=False _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor causal_mask torch Tensor attn_mask torch Tensor training - torch Tensor attn_weights = torch matmul query key permute inv_scale = torch full math sqrt value size - dtype=attn_weights dtype device=attn_weights device attn_weights = attn_weights div inv_scale causal_mask_value = torch full torch finfo query dtype min dtype=query dtype device=query device attn_weights = torch where causal_mask attn_weights causal_mask_value attn_weights = attn_weights + attn_mask attn_weights = attn_weights softmax dim=- type value dtype torch nn functional dropout attn_weights p= training=training inplace=False matmul value tensor_shape = causal_mask = torch ones dtype=torch bool device=self device tril diagonal= attn_mask = torch randn dtype=torch float device=self device args = torch randn tensor_shape device=self device torch randn tensor_shape device=self device torch randn tensor_shape device=self device causal_mask attn_mask _check_common dot_prod_attention args =args contains=False has_dropout=True check_train=False _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor training - torch Tensor Input tensors assumed have shape batch_size seq_len n_head embed_dim q = query transpose k = key transpose v = value transpose bs = q size k_len = k size - q = q math sqrt q size - scores = torch matmul q k transpose - - attn_mask = torch ones bs k_len dtype=torch bool device=query device tril diagonal= attn_mask = attn_mask == view bs k_len expand_as scores scores = scores masked_fill attn_mask -float inf weights = torch nn functional softmax scores dim=- weights = torch nn functional dropout weights p= training=training inplace=False torch matmul weights v _check_common dot_prod_attention check_train=False has_dropout=True _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor attn_mask torch Tensor - torch Tensor query = query transpose key = key transpose value = value transpose scores = torch matmul query key permute scores += attn_mask attn_weights = scores float softmax dim=- type value dtype attn_weights matmul value tensor_shapes = tensor_shape tensor_shapes attn_mask = torch randn dtype=torch float device=self device args = torch randn tensor_shape device=self device torch randn tensor_shape device=self device torch randn tensor_shape device=self device attn_mask _check_common dot_prod_attention args =args has_dropout=False check_train=False _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor attn_mask torch Tensor - torch Tensor query = query transpose key = key transpose value = value transpose scores = torch matmul query key permute scores += attn_mask attn_weights = scores float softmax dim=- type value dtype attn_weights matmul value key value tensor_shapes = tensor_shape tensor_shapes attn_mask = torch randn dtype=torch float device=self device args = torch randn tensor_shape device=self device torch randn tensor_shape device=self device torch randn tensor_shape device=self device attn_mask _check_common dot_prod_attention args =args has_dropout=False check_train=False test attn_mask stride last dim = attn_mask_ = attn_mask transpose args = attn_mask_ _check_common dot_prod_attention args =args has_dropout=False check_train=False contains=self device == cpu _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor - torch Tensor attn_mask = torch full device=query device query = query transpose key = key transpose value = value transpose scores = torch matmul query key permute scores += attn_mask attn_weights = scores float softmax dim=- type value dtype attn_weights matmul value key value tensor_shapes = tensor_shape tensor_shapes args = torch randn tensor_shape device=self device torch randn tensor_shape device=self device torch randn tensor_shape device=self device _check_common dot_prod_attention args =args has_dropout=False check_train=False _test_sdpa_rewriter_ dot_prod_attention query torch Tensor key torch Tensor value torch Tensor attn_mask torch Tensor - torch Tensor Input tensors assumed have shape batch_size n_head seq_len embed_dim bs = query size n_head = query size seq_len = query size embed_dim = query size q = query view bs n_head seq_len embed_dim k = key reshape bs n_head seq_len embed_dim v = value reshape bs n_head seq_len embed_dim attn_weights = torch bmm q k transpose attn_weights = attn_weights view bs n_head seq_len seq_len + attn_mask attn_weights = attn_weights view bs n_head seq_len seq_len attn_weights = torch nn functional softmax attn_weights dim=- attn_output = torch bmm attn_weights v attn_output = attn_output view bs n_head seq_len embed_dim attn_output tensor_shape = attn_mask = torch randn dtype=torch float device=self device args = torch randn tensor_shape device=self device dtype=torch float torch randn tensor_shape device=self device dtype=torch float torch randn tensor_shape device=self device dtype=torch float attn_mask _check_common dot_prod_attention args =args has_dropout=False check_train=False HAS_XPU_AND_TRITON HAS_CUDA_AND_TRITON PLATFORM_SUPPORTS_FUSED_ATTENTION SDPAPatternRewriterGpuTests TestSDPAPatternRewriterTemplate device = GPU_TYPE test_sdpa_rewriter_ _gpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _freezing = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ _freezing test_insignificant_strides = TestSDPAPatternRewriterTemplate _test_insignificant_strides test_pattern_fails_with_reuse_gpu = TestSDPAPatternRewriterTemplate _test_pattern_fails_with_reuse test_sdpa_rewriter_ _gpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_pattern_fails_with_tensor_factor_gpu = TestSDPAPatternRewriterTemplate _test_pattern_fails_with_tensor_factor test_pattern_fails_with_unsupported_mask_gpu = TestSDPAPatternRewriterTemplate _test_pattern_fails_with_unsupported_mask test_sdpa_rewriter_ _gpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_prev_ _gpu = TestSDPAPatternRewriterTemplate _test_sdpa_prev_ test_sdpa_prev_ _gpu = TestSDPAPatternRewriterTemplate _test_sdpa_prev_ test_sdpa_prev_ _gpu = TestSDPAPatternRewriterTemplate _test_sdpa_prev_ test_sdpa_rewriter_ _gpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ dtype=torch half test_sdpa_rewriter_ _gpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _gpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ SDPAPatternRewriterGpuDynamicTests SDPAPatternRewriterGpuTests use_static_shapes = False HAS_CPU SDPAPatternRewriterCpuTests TestSDPAPatternRewriterTemplate device = cpu test_sdpa_rewriter_ _cpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_pattern_fails_with_reuse_cpu = TestSDPAPatternRewriterTemplate _test_pattern_fails_with_reuse test_sdpa_rewriter_ _cpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _cpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_pattern_fails_with_tensor_factor_cpu = TestSDPAPatternRewriterTemplate _test_pattern_fails_with_tensor_factor test_pattern_fails_with_unsupported_mask_cpu = TestSDPAPatternRewriterTemplate _test_pattern_fails_with_unsupported_mask test_sdpa_rewriter_ _cpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _cpu = TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_prev_ _cpu = TestSDPAPatternRewriterTemplate _test_sdpa_prev_ test_sdpa_prev_ _cpu = TestSDPAPatternRewriterTemplate _test_sdpa_prev_ test_sdpa_prev_ _cpu = TestSDPAPatternRewriterTemplate _test_sdpa_prev_ test_sdpa_rewriter_ _cpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ dtype=torch float test_sdpa_rewriter_ _cpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _cpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _cpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _fp _mask_cpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ _fp _mask test_sdpa_rewriter_ _cpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _cpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _cpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _cpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _cpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _cpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _cpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ test_sdpa_rewriter_ _cpu = functools partialmethod TestSDPAPatternRewriterTemplate _test_sdpa_rewriter_ SDPAPatternRewriterCpuDynamicTests SDPAPatternRewriterCpuTests use_static_shapes = False __name__ == __main__ IS_LINUX run_tests