argparse inspect os sys time datetime timedelta datasets load_dataset load_metric transformers AutoModelForSequenceClassification AutoTokenizer torch torch _dynamo torch utils data DataLoader torch backends cuda matmul allow_tf = True You will download around G dataset you run end end training evaluation example os environ TOKENIZERS_PARALLELISM = false device = torch device cuda torch cuda is_available torch device cpu data_processing num_samples batch_size dataset = load_dataset yelp_review_full tokenizer = AutoTokenizer from_pretrained bert-base-cased tokenize_function examples tokenizer examples text padding= max_length truncation=True tokenized_datasets = dataset map tokenize_function batched=True tokenized_datasets = tokenized_datasets remove_columns text tokenized_datasets = tokenized_datasets rename_column label labels tokenized_datasets set_format torch small_train_dataset = tokenized_datasets train select range num_samples small_eval_dataset = tokenized_datasets test select range num_samples train_dataloader = DataLoader small_train_dataset batch_size=batch_size eval_dataloader = DataLoader small_eval_dataset batch_size=batch_size train_dataloader eval_dataloader training_iter_fn batch model optimizer outputs = model batch loss = outputs loss loss backward optimizer step optimizer zero_grad loss model_training_evaluation backend train_dataloader eval_dataloader model optimizer num_epochs evaluation model device model train loss_history = backend Run native Pytorch opt_training_iter_fn = training_iter_fn Support backends eager aot_eager aot_nvfuser inductor opt_training_iter_fn = torch _dynamo optimize backend training_iter_fn epoch range num_epochs running_loss = i batch enumerate train_dataloader batch = k v device k v batch items loss = opt_training_iter_fn batch model optimizer running_loss += loss item i == loss_history append running_loss running_loss = evaluation metric = load_metric accuracy model eval backend opt_model = model opt_model = torch _dynamo optimize backend model batch eval_dataloader batch = k v device k v batch items torch no_grad outputs = opt_model batch logits = outputs logits predictions = torch argmax logits dim=- metric add_batch predictions=predictions references=batch labels loss_history metric compute loss_history None check_loss ref_loss res_loss assert len ref_loss == len res_loss length = len ref_loss x = min length sum res_loss -x = sum ref_loss -x + parse_args parser = argparse ArgumentParser description= TorchDynamo end end training evaluation benchmark parser add_argument -- epochs type=int default= help= number epochs train default parser add_argument -- num-samples type=int default= help= number samples train eval default parser add_argument -- batch-size type=int default= help= input batch size training default parser add_argument -- lr type=float default= e- help= learning rate default e- parser add_argument -- backend choices=torch _dynamo list_backends exclude_tags=None default= inductor help= train evaluate model given backend default inductor parser add_argument -- optimizer default= Adam help= train model using given optimizer default Adam parser add_argument -- evaluation action= store_true help= running evaluation after model training args = parser parse_args args main args = parse_args train_dataloader eval_dataloader = data_processing args num_samples args batch_size model = AutoModelForSequenceClassification from_pretrained bert-base-cased num_labels= optimizer_cls = getattr sys modules torch optim args optimizer capturable inspect signature optimizer_cls parameters keys optimizer = optimizer_cls model parameters lr=args lr capturable=True optimizer = optimizer_cls model parameters lr=args lr native_start = time time ref_loss accuracy = model_training_evaluation None train_dataloader eval_dataloader model optimizer args epochs args evaluation native_end = time time res_loss accuracy = model_training_evaluation args backend train_dataloader eval_dataloader model optimizer args epochs args evaluation dynamo_end = time time check_loss ref_loss res_loss print PASSED TorchDynamo end end training loss less than equal native PyTorch print FAILED TorchDynamo end end training loss greater than native Pytorch args evaluation print f Model accuracy accuracy native_elapsed = native_end - native_start dynamo_elapsed = dynamo_end - native_end print f Train model args epochs epochs backend args backend optimizer args optimizer print f PyTorch spent timedelta seconds=native_elapsed args epochs per epoch print f TorchDynamo spent timedelta seconds=dynamo_elapsed args epochs per epoch __name__ == __main__ main