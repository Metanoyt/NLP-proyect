Owner s module mps ruff noqa F io sys math random unittest warnings shutil subprocess tempfile os copy gc threading torch torch nn nn torch nn functional F itertools collections defaultdict torch inf torch nn Buffer Parameter torch testing _internal opinfo torch testing _internal common_utils \ gradcheck gradgradcheck parametrize run_tests TestCase download_file MACOS_VERSION IS_CI NoTest skipIfSlowGradcheckEnv suppress_warnings serialTest instantiate_parametrized_tests xfailIf torch testing _internal common_mps mps_ops_modifier mps_ops_grad_modifier mps_ops_error_inputs_modifier torch testing make_tensor torch testing _internal common_dtype get_all_dtypes integral_types torch backends mps torch distributions Uniform Exponential torch utils _python_dispatch TorchDispatchMode functools partial torch testing _internal common_methods_invocations op_db UnaryUfuncInfo ReductionOpInfo SpectralFuncInfo BinaryUfuncInfo torch testing _internal common_device_type ops dtypes instantiate_device_type_tests OpDTypes torch testing _internal common_nn NNTestCase torch testing _internal common_quantization _group_quantize_tensor _dynamically_quantize_per_channel numpy np torch torch utils _pytree pytree itertools product operator test_consistency_op_db = copy deepcopy op_db test_error_inputs_op_db = copy deepcopy op_db Add bicubic d_aa test_consistency_op_db op op_db op name = _upsample_bilinear d_aa continue op = copy deepcopy op op name = _upsample_bicubic d_aa op op = torch ops aten _upsample_bicubic d_aa test_consistency_op_db append op break Copied ` test_ops py ` purposes duplicating ` test_numpy_ref ` _ref_test_ops = tuple filter lambda op isinstance op UnaryUfuncInfo ReductionOpInfo SpectralFuncInfo BinaryUfuncInfo op ref None op_db Same logic test_cuda py torch backends mps is_available print MPS available skipping tests file=sys stderr TestCase = NoTest noqa F NNTestCase = NoTest noqa F total_memory = int subprocess check_output sysctl -n hw memsize MPS_UNSUPPORTED_TYPES = torch double torch cdouble MPS_DTYPES = t t get_all_dtypes t MPS_UNSUPPORTED_TYPES Determine whether enable MPS memory leak check uses same code CUDA TEST_MPS_MEM_LEAK_CHECK = os getenv PYTORCH_TEST_MPS_MEM_LEAK_CHECK == skipMPSMemoryLeakCheckIf condition dec fn getattr fn _do_mps_memory_leak_check True fn _do_mps_memory_leak_check = condition fn dec MpsMemoryLeakCheck __init__ testcase name=None name = testcase id name None name testcase = testcase __enter__ Performs gc required required any memory held caching_allocator_mem_allocated = torch mps current_allocated_memory caching_allocator_mem_allocated gc collect torch mps empty_cache Acquires caching allocator driver statistics before test run caching_allocator_before = torch mps current_allocated_memory driver_before = torch mps driver_allocated_memory __exit__ exc_type exc_value traceback Don t check leaks exception thrown exc_type None Compares caching allocator before after statistics An increase allocated memory discrepancy indicating possible memory leak discrepancy_detected = False caching_allocator_mem_allocated = torch mps current_allocated_memory caching_allocator_mem_allocated caching_allocator_before discrepancy_detected = True Short-circuits no discrepancy detected discrepancy_detected Validates discrepancy persists after garbage collection confirmed driver API gc collect torch mps empty_cache discrepancy_detected = True Query memory multiple items ensure leak transient _ range caching_allocator_mem_allocated = torch mps current_allocated_memory driver_mem_allocated = torch mps driver_allocated_memory caching_allocator_discrepancy = False driver_discrepancy = False caching_allocator_mem_allocated caching_allocator_before caching_allocator_discrepancy = True driver_mem_allocated driver_before driver_discrepancy = True caching_allocator_discrepancy driver_discrepancy Leak false positive exit loop discrepancy_detected = False break caching_allocator_discrepancy driver_discrepancy Just raises warning leak validated driver API msg = MPS caching allocator reports memory leak f verified driver API name f Caching allocator allocated memory caching_allocator_before f now reported caching_allocator_mem_allocated f MPS driver allocated memory driver_before now driver_mem_allocated warnings warn msg caching_allocator_discrepancy driver_discrepancy A caching allocator discrepancy validated driver API failure msg = f MPS driver API confirmed leak name f Caching allocator allocated memory caching_allocator_before f now reported caching_allocator_mem_allocated f MPS driver allocated memory driver_before now driver_mem_allocated raise RuntimeError msg TestAutocastMPS TestCase test_matmul_autocast autocast_tensor_A = torch rand device= mps autocast_tensor_B = torch rand device= mps tensor_A = autocast_tensor_A detach clone tensor_B = autocast_tensor_B detach clone autocast_output_tensor = torch empty output_tensor = autocast_output_tensor detach clone torch autocast device_type= mps autocast_output_tensor = torch mm autocast_tensor_A autocast_tensor_B autocast_output_tensor = torch mm autocast_tensor_A autocast_output_tensor output_tensor = torch mm tensor_A tensor_B output_tensor = torch mm tensor_A output_tensor assertEqual autocast_output_tensor dtype torch float Autocast output tensor expected type float assertEqual autocast_output_tensor output_tensor torch float f Autocast non-autocast tensors did match \ got \n autocast_output_tensor \n output_tensor torch float parametrize dtype torch float torch bfloat torch float test_scaled_dot_product_attention_autocast dtype Regression test https github com pytorch pytorch issues query = torch rand dtype=torch float device= mps key = torch rand dtype=torch float device= mps value = torch rand dtype=dtype device= mps torch amp autocast device_type= mps y_autocast = F scaled_dot_product_attention query key value y = F scaled_dot_product_attention query key value torch float assertEqual y y_autocast dtype y_autocast test_conv_transpose d_autocast_fp m = nn ConvTranspose d stride= mps x = torch randn device= mps torch amp autocast device_type= mps y = m x assertEqual y dtype torch float test_conv d_autocast Regression test https github com pytorch pytorch issues Foo nn Module __init__ super __init__ c = nn Conv d c = nn Conv d forward x x = c x x = c x x x = torch randn device= mps model = Foo mps torch amp autocast device_type= mps y = model x assertEqual y dtype torch float test_gradscaler_mps big model force chunking depth gradscaler dispatch Model nn Module __init__ super __init__ fc = nn Linear fc = nn Linear fc = nn Linear fc = nn Linear fc = nn Linear relu = nn ReLU forward x x = relu fc x x = relu fc x x = relu fc x x = relu fc x fc x torch manual_seed helper model_cpu model_mps dtype iterations batch_size atol= e- rtol= e- optimizer_cpu = torch optim SGD model_cpu parameters lr= optimizer_mps = torch optim SGD model_mps parameters lr= loss_fn = nn MSELoss input_cpu = torch randn batch_size target_cpu = torch randn batch_size input_mps = input_cpu mps target_mps = target_cpu mps scaler_cpu = torch amp GradScaler device= cpu scaler_mps = torch amp GradScaler device= mps _ range iterations optimizer_cpu zero_grad optimizer_mps zero_grad torch amp autocast device_type= cpu dtype=dtype output_cpu = model_cpu input_cpu loss_cpu = loss_fn output_cpu target_cpu scaler_cpu scale loss_cpu backward scaler_cpu step optimizer_cpu scaler_cpu update torch autocast device_type= mps dtype=dtype output_mps = model_mps input_mps loss_mps = loss_fn output_mps target_mps scaler_mps scale loss_mps backward scaler_mps step optimizer_mps scaler_mps update p_cpu p_mps zip model_cpu parameters model_mps parameters assertEqual p_mps cpu p_cpu rtol=rtol atol=atol model_cpu = Model cpu model_mps = Model mps model_mps load_state_dict model_cpu state_dict helper model_cpu model_mps torch float iterations= batch_size= helper model_cpu model_mps torch bfloat iterations= batch_size= test_non_fast_path_amp_unscale torch manual_seed Model nn Module __init__ super __init__ linear = nn Linear linear = nn Linear forward x x = linear x x = F relu x x = linear x x = x mean dim= x cpu_model = Model cpu mps_model = copy deepcopy cpu_model mps cpu_optimizer = torch optim SGD cpu_model parameters lr= mps_optimizer = torch optim SGD mps_model parameters lr= cpu_scaler = torch amp GradScaler device= cpu mps_scaler = torch amp GradScaler device= mps helper model optimizer scaler device input target apply_grad_transform=False optimizer zero_grad torch autocast device_type=device dtype=torch bfloat output = model input loss = nn MSELoss output target scaler scale loss backward apply_grad_transform p model parameters p grad None p grad dim = p grad = p grad as_strided p grad size p grad dim scaler unscale_ optimizer scaler step optimizer scaler update CPU forward backward pass input_cpu = torch randn device= cpu target_cpu = torch randn device= cpu helper cpu_model cpu_optimizer cpu_scaler cpu input_cpu target_cpu MPS forward backward pass input_mps = input_cpu mps target_mps = target_cpu mps helper mps_model mps_optimizer mps_scaler mps input_mps target_mps apply_grad_transform=True updated_linear _weight_cpu = cpu_model linear weight detach updated_linear _weight_cpu = cpu_model linear weight detach updated_linear _weight_mps = mps_model linear weight detach cpu updated_linear _weight_mps = mps_model linear weight detach cpu assertEqual updated_linear _weight_cpu updated_linear _weight_mps atol= e- rtol= e- assertEqual updated_linear _weight_cpu updated_linear _weight_mps atol= e- rtol= e- Expand TestCase Memory Leak Detection MPS device TestCaseMPS TestCase _do_mps_memory_leak_check = True __init__ method_name= runTest super __init__ method_name test_method = getattr method_name None test_method None Wraps tested method we should do MPS memory check TEST_MPS_MEM_LEAK_CHECK _do_mps_memory_leak_check wrap_with_mps_policy method_name assertLeaksNoMpsTensors assertLeaksNoMpsTensors name=None name = id name None name MpsMemoryLeakCheck name wrap_with_mps_policy method_name policy test_method = getattr method_name setattr method_name super wrap_method_with_policy test_method policy checks leaks even TEST_MPS_MEM_LEAK_CHECK wrap_with_mps_memory_check method super wrap_method_with_policy method assertLeaksNoMpsTensors TestMemoryLeak TestCaseMPS test_mps_memory_leak_detection l = wrap_with_mps_memory_check no_leak pass Trigger intentional memory leak wrap_with_mps_memory_check leak_gpu increasing MB force acquiring new block overcome blocksize differences across platforms l append torch randn device=torch device mps no_leak check runtime error memory leak emitted which would confirm whether memory leak detection worked successfully assertRaisesRegex RuntimeError r MPS driver API confirmed + leak_gpu test_copy_cast_no_leak step x x = x device= cpu dtype=torch float x = x device= mps dtype=torch float = torch randn device= mps dtype=torch float Warm up prebuild MPS shaders otherwise check fails step torch mps empty_cache driver_before = torch mps driver_allocated_memory step torch mps empty_cache driver_after = torch mps driver_allocated_memory assertEqual driver_before driver_after f Detected driver_after - driver_before bytes leak GPU memory TestPixelShuffle TestCaseMPS test_pixel_shuffle_unshuffle _test_pixel_shuffle_unshuffle_helper num_input_dims valid_channels_dim=True upscale_factor=None is_contiguous=True generate_input If valid_channels_dim=False add make channels dim indivisible upscale_factor channels = random randint upscale_factor + valid_channels_dim height = random randint width = random randint num_input_dims == input = torch rand channels requires_grad=True device= mps assert is_contiguous num_input_dims == input = torch rand width height requires_grad=True device= mps T is_contiguous input = input contiguous batch_sizes = random randint _ range num_input_dims - input = torch rand batch_sizes channels width height requires_grad=True device= mps input = input transpose - - is_contiguous input = input contiguous is_contiguous len input reshape - assert input is_contiguous input = input detach clone input requires_grad = True input Function imperatively ensure pixels shuffled correct locations Used validate batch operations pixel_shuffle _verify_pixel_shuffle input output upscale_factor c range output size - h range output size - w range output size - height_idx = h upscale_factor weight_idx = w upscale_factor channel_idx = upscale_factor h upscale_factor + w upscale_factor + \ c upscale_factor assertEqual output c h w input channel_idx height_idx weight_idx upscale_factor = random randint upscale_factor None upscale_factor input = generate_input ps = nn PixelShuffle upscale_factor pus = nn PixelUnshuffle downscale_factor=upscale_factor num_input_dims = valid_channels_dim upscale_factor output = ps input _verify_pixel_shuffle input output upscale_factor output backward output data assertEqual input data input grad data Ensure unshuffle properly inverts shuffle unshuffle_output = pus output assertEqual input unshuffle_output assertRaises RuntimeError lambda ps input _test_pixel_unshuffle_error_case_helper num_input_dims valid_height_dim=True valid_width_dim=True downscale_factor=None downscale_factor = random randint downscale_factor None downscale_factor channels = random randint If valid_height_dim=False add make height dim indivisible downscale_factor height = random randint abs downscale_factor + valid_height_dim If valid_width_dim=False add make width dim indivisible downscale_factor width = random randint abs downscale_factor + valid_width_dim num_input_dims == input = torch rand channels requires_grad=True device= mps num_input_dims == input = torch rand height width requires_grad=True device= mps batch_sizes = random randint _ range num_input_dims - input = torch rand batch_sizes channels height width requires_grad=True device= mps pus = nn PixelUnshuffle downscale_factor assertRaises RuntimeError lambda pus input _test_pixel_shuffle_unshuffle_for_input_dims num_input_dims For D - D error case For D - D success case pixel_shuffle + pixel_unshuffle is_contiguous_check = True False num_input_dims True is_contiguous is_contiguous_check _test_pixel_shuffle_unshuffle_helper num_input_dims=num_input_dims is_contiguous=is_contiguous _test_pixel_shuffle_unshuffle_helper num_input_dims=num_input_dims valid_channels_dim=False is_contiguous=is_contiguous _test_pixel_shuffle_unshuffle_helper num_input_dims=num_input_dims upscale_factor= is_contiguous=is_contiguous _test_pixel_shuffle_unshuffle_helper num_input_dims=num_input_dims upscale_factor=- is_contiguous=is_contiguous Error cases pixel_unshuffle _test_pixel_unshuffle_error_case_helper num_input_dims=num_input_dims valid_height_dim=False _test_pixel_unshuffle_error_case_helper num_input_dims=num_input_dims valid_width_dim=False _test_pixel_unshuffle_error_case_helper num_input_dims=num_input_dims downscale_factor= _test_pixel_unshuffle_error_case_helper num_input_dims=num_input_dims downscale_factor=- test_pixel_shuffle_large_upscale_factor assertRaises ValueError ps = nn PixelShuffle ps torch randn test_pixel_shuffle_unshuffle_ D _test_pixel_shuffle_unshuffle_for_input_dims num_input_dims= test_pixel_shuffle_unshuffle_ D _test_pixel_shuffle_unshuffle_for_input_dims num_input_dims= test_pixel_shuffle_unshuffle_ D _test_pixel_shuffle_unshuffle_for_input_dims num_input_dims= test_pixel_shuffle_unshuffle_ D _test_pixel_shuffle_unshuffle_for_input_dims num_input_dims= test_pixel_shuffle_unshuffle_ D _test_pixel_shuffle_unshuffle_for_input_dims num_input_dims= test_pixel_shuffle_large_upscale_factor test_pixel_shuffle_unshuffle_ D test_pixel_shuffle_unshuffle_ D test_pixel_shuffle_unshuffle_ D test_pixel_shuffle_unshuffle_ D test_pixel_shuffle_unshuffle_ D MPSReluTest TestCaseMPS _npRelu np_features np maximum np_features np zeros np_features shape astype np_features dtype testNpRelu torch testing assert_close np array _npRelu np array - - - - - _testRelu np_features device np_relu = _npRelu np_features Convert numpy array PyTorch Tensor move Tensor CPU GPU based device parameter py_tensor = torch from_numpy np_features device py_relu = torch nn ReLU inplace=False py_tensor py_relu_cpu = py_relu cpu assertEqual np_relu py_relu_cpu _testReluInPlace np_features device np_relu = _npRelu np_features Convert numpy array PyTorch Tensor move Tensor CPU GPU based device parameter py_tensor = torch from_numpy np_features device py_relu = torch nn ReLU inplace=True py_tensor py_relu_cpu = py_relu cpu assertEqual np_relu py_relu_cpu Inplace Relu modifies initial input should match output Relu assertEqual np_relu py_tensor cpu testNumbersCPU t np int Force execution CPU even GPU kernel available type _testRelu np array - - - - - astype t device= cpu _testReluInPlace np array - - - - - astype t device= cpu testNumbersGPU t np float np float _testRelu np array - - - - - astype t device= mps _testReluInPlace np array - - - - - astype t device= mps _testRelu np array astype t device= mps _testReluInPlace np array astype t device= mps MatmulTest TestCaseMPS _helper shape_tensor_ shape_tensor_ expand_tensor_ _shape=None expand_tensor_ _shape=None expand_tensor_ _shape tensor _mps = torch randn shape_tensor_ device= mps expand expand_tensor_ _shape tensor _mps = torch randn shape_tensor_ device= mps expand_tensor_ _shape tensor _mps = torch randn shape_tensor_ device= mps expand expand_tensor_ _shape tensor _mps = torch randn shape_tensor_ device= mps tensor _cpu = tensor _mps cpu tensor _cpu = tensor _mps cpu matmul_cpu = torch matmul tensor _cpu tensor _cpu matmul_mps = torch matmul tensor _mps tensor _mps assertEqual matmul_cpu matmul_mps cpu test_vector_x_vector uses ` dot ` _helper test_matrix_x_vector uses ` addmv ` _helper test_batched_matrix_x_broadcasted_vector _helper test_batched_matrix_x_batched_matrix uses ` bmm out ` _helper test_batched_matrix_x_broadcasted_matrix _helper serialTest test_large_matmul Issue tensor _mps = torch randn dtype=torch half device= mps tensor _mps = torch randn dtype=torch half device= mps matmul_mps = torch matmul tensor _mps tensor _mps tensor _cpu = tensor _mps cpu tensor _cpu = tensor _mps cpu matmul_cpu = torch matmul tensor _cpu tensor _cpu assertEqual matmul_cpu matmul_mps cpu MPSLeakyReluTest TestCaseMPS _npLeakyRelu np_features negative_slope= np maximum np_features negative_slope np_features astype np_features dtype testNpLeakyRelu torch testing assert_close np array - - - - - _npLeakyRelu np array - - - - - negative_slope= _testLeakyRelu shape dtype negative_slope contiguous cpu_x = torch randn shape device= cpu dtype=dtype mps_x = cpu_x detach clone mps contiguous shape len shape Transposing will make tensor non-contiguous cpu_x = cpu_x transpose mps_x = mps_x transpose assert mps_x is_contiguous cpu_x requires_grad_ mps_x requires_grad_ relu_op = torch nn LeakyReLU negative_slope cpu_leaky_relu = relu_op cpu_x mps_leaky_relu = relu_op mps_x torch testing assert_close cpu_leaky_relu mps_leaky_relu cpu test backward pass cpu_grad = torch ones_like cpu_leaky_relu mps_grad = cpu_grad mps mps_leaky_relu backward gradient=mps_grad cpu_leaky_relu backward gradient=cpu_grad assert cpu_x grad None Check grad well-populated assertEqual cpu_x grad mps_x grad testNumbersCPU t torch float torch half shape contiguous True False _testLeakyRelu shape dtype=t negative_slope= contiguous=contiguous TestAvgPool TestCaseMPS _sum_pool d x kernel_size windows = torch nn functional unfold x kernel_size=kernel_size stride=kernel_size torch sum windows dim= _sum_pool d x kernel_size Because unfold does support D sliding window we will split tensor multiple tensors calculate sum h = kernel_size splited_x = t sum t x split h t size == h sum_pool d assumes tensor n m view so unsqueeze two times splited_x = _sum_pool d t unsqueeze unsqueeze kernel_size t splited_x joined_x = torch cat splited_x joined_x view joined_x numel _avg_pool d x kernel_size size = reduce operator mul kernel_size noqa F _sum_pool d x kernel_size size _avg_pool d x kernel_size size = reduce operator mul kernel_size noqa F _sum_pool d x kernel_size size test_avg_pool d_with_zero_divisor assertRaisesRegex RuntimeError divisor must zero lambda F avg_pool d torch zeros divisor_override= test_doubletensor_avg_pool d_with_divisor n m = input = torch rand n m i range n + j range m + divisor i j actual = F avg_pool d input i j divisor_override=divisor actual = actual view actual numel expected = _sum_pool d input i j divisor assertEqual actual expected rtol= atol= e- test_avg_pool d_ceil_mode Regression test gh- x = torch randn y = torch nn functional avg_pool d x ceil_mode=True count_include_pad=True kernel_size= padding= stride= assertFalse torch isnan y any y = torch nn functional avg_pool d x mps ceil_mode=True count_include_pad=True kernel_size= padding= stride= assertFalse torch isnan y any Test some cases avg_pool d which used mismatch CPU results Addresses issue https github com pytorch pytorch issues test_avg_pool d_ceil_mode_mismatch sizes = kwargs = dict kernel_size= stride= ceil_mode=True divisor_override= input_size sizes model = torch nn AvgPool d kwargs x = torch arange math prod input_size dtype=torch float reshape input_size out_cpu = model x out_mps = model x mps msg = f input_size= kwargs= assertEqual out_mps out_cpu msg=msg TestMPS TestCaseMPS test_exp device= mps dtype=torch float v - + j + j dtype is_complex b = torch arange dtype=dtype device=device math pi = torch tensor v dtype=dtype device= mps b compare_with_numpy torch exp np exp xfailIf MACOS_VERSION test_conv_raises_error device= mps dtype=torch float conv = nn Conv d padding= mps x = torch ones assertRaises NotImplementedError y = conv x mps xfailIf MACOS_VERSION test_conv_high_channel_size out_channels = weight = torch randn out_channels x = torch ones y_cpu = F conv d x cpu weight cpu y_mps = F conv d x mps weight mps assertEqual y_cpu y_mps test_triu_inf device= mps dtype=torch float diag - mask = torch full float -inf mask_mps = mask detach clone mps cpu_ref = torch triu mask diagonal=diag mps_out = torch triu mask_mps diagonal=diag assertEqual cpu_ref mps_out test_exp device= mps dtype=torch float input = torch tensor - - device=device dtype=dtype output = torch exp input output_cpu = torch exp input cpu If exponentWithTensor MPS call used M running test will fail Mismatched elements Greatest absolute difference e- index up e- allowed Greatest relative difference e- index up e- allowed assertEqual output output_cpu atol= e- rtol= e- test_exp_strided_output x = torch rand device= mps x_cpu = x cpu x = x permute x_cpu = x_cpu permute res = x exp res_cpu = x_cpu exp assertEqual res res_cpu _testLeakyRelu np_features negative_slope device cpu_x = torch from_numpy np_features requires_grad_ mps_x = torch from_numpy np_features mps requires_grad_ relu_op = torch nn LeakyReLU negative_slope cpu_leaky_relu = relu_op cpu_x mps_leaky_relu = relu_op mps_x torch testing assert_close cpu_leaky_relu mps_leaky_relu cpu test backward pass cpu_grad = torch ones_like cpu_leaky_relu mps_grad = cpu_grad mps cpu_leaky_relu backward gradient=cpu_grad mps_leaky_relu backward gradient=mps_grad torch testing assert_close cpu_x grad mps_x grad cpu testNumbersGPU t np float _testLeakyRelu np array - - - - - astype t negative_slope= device= mps test_fill helper val shape dtype tensor = torch zeros shape device= mps dtype=dtype tensor_mps = tensor fill_ val tensor_ = torch zeros shape device= cpu dtype=dtype tensor_cpu = tensor_ fill_ val assertEqual tensor_mps tensor_cpu helper torch float helper torch float helper + j torch complex test_fill_storage_offset shape = val = tensor = torch ones shape device= mps tensor_mps = tensor fill_ val tensor_ = torch ones shape device= cpu tensor_cpu = tensor_ fill_ val assertEqual tensor_mps tensor_cpu assertEqual tensor tensor_ shape = val = tensor = torch ones shape device= mps val_tensor_mps = torch tensor val device= mps tensor_mps = tensor fill_ val_tensor_mps Regression test https github com pytorch pytorch issues tensor fill_ val_tensor_mps tensor_ = torch ones shape device= cpu val_tensor_cpu = torch tensor val device= cpu tensor_cpu = tensor_ fill_ val_tensor_cpu tensor_ fill_ val_tensor_cpu assertEqual tensor_mps device= cpu tensor_cpu assertEqual tensor device= cpu tensor_ test_cdist_large device= mps cm use_mm_for_euclid_dist_if_necessary use_mm_for_euclid_dist donot_use_mm_for_euclid_dist x = torch randn device=device y = torch randn device=device actual = torch cdist x y p= compute_mode=cm expected = _brute_cdist x y p= assertEqual expected actual test_cdist_large_batch device= mps cm use_mm_for_euclid_dist_if_necessary use_mm_for_euclid_dist donot_use_mm_for_euclid_dist x = torch randn device=device y = torch randn device=device actual = torch cdist x y p= compute_mode=cm expected = _brute_cdist x y p= assertEqual expected actual test_cdist_non_contiguous device= mps cm use_mm_for_euclid_dist donot_use_mm_for_euclid_dist x = torch randn device=device mT y = torch randn device=device mT actual = torch cdist x y p= compute_mode=cm expected = _brute_cdist x y p= assertFalse x is_contiguous assertFalse y is_contiguous assertEqual expected actual x = torch randn device=device y = torch randn device=device t actual = torch cdist x y p= compute_mode=cm expected = _brute_cdist x y p= assertTrue x is_contiguous assertFalse y is_contiguous assertEqual expected actual x = torch randn device=device t y = torch randn device=device actual = torch cdist x y p= compute_mode=cm expected = _brute_cdist x y p= assertFalse x is_contiguous assertTrue y is_contiguous assertEqual expected actual test_cdist_non_contiguous_batch device= mps cm use_mm_for_euclid_dist donot_use_mm_for_euclid_dist x = torch randn device=device mT y = torch randn device=device mT actual = torch cdist x y p= compute_mode=cm expected = _brute_cdist x y p= assertFalse x is_contiguous assertFalse y is_contiguous assertEqual expected actual x = torch randn device=device y = torch randn device=device mT actual = torch cdist x y p= compute_mode=cm expected = _brute_cdist x y p= assertTrue x is_contiguous assertFalse y is_contiguous assertEqual expected actual x = torch randn device=device mT y = torch randn device=device actual = torch cdist x y p= compute_mode=cm expected = _brute_cdist x y p= assertFalse x is_contiguous assertTrue y is_contiguous assertEqual expected actual test_cdist_euclidean_large device= mps _test_euclidean_large_cdist sizex sizey=None sizey None sizey = sizex x = torch randn sizex device=device dtype=torch float y = torch randn sizey device=device dtype=torch float eps = e- avoid extremum x = x - x - y eps float eps x requires_grad = True y requires_grad = True dist = torch cdist x y p= Do backward pass check valid large matrices loss = dist sum loss backward _test_euclidean_large_cdist test_cdist_same_inputs device= mps Test detect issues cdist gradient calculation When distances sizex = p float inf x = torch randn sizex device=device dtype=torch float dist_grad = torch randn device=device dtype=torch float y = x clone eps = e- x requires_grad = True d = torch cdist x y d backward dist_grad Check backward pass does contain invalid values such nan inf assert torch isfinite x grad all _brute_cdist x y p= r = x shape - r = y shape - r == r == torch empty r r device=x device torch norm x None - y None p=p dim=- test_cdist_norm device= mps r m r p float inf x = torch randn r m device=device y = torch randn r m device=device p == cm use_mm_for_euclid_dist donot_use_mm_for_euclid_dist actual = torch cdist x y p= compute_mode=cm expected = _brute_cdist x y p= assertEqual expected actual rtol= atol= actual = torch cdist x y p=p expected = _brute_cdist x y p=p assertEqual expected actual test_cdist_norm_batch device= mps r m r p float inf x = torch randn r m device=device y = torch randn r m device=device p == cm use_mm_for_euclid_dist donot_use_mm_for_euclid_dist actual = torch cdist x y p= compute_mode=cm expected = _brute_cdist x y p= assertEqual expected actual rtol= atol= actual = torch cdist x y p=p expected = _brute_cdist x y p=p assertEqual expected actual test_mm B = torch ones mps C = torch ones mps D = torch mm B C cpu torch testing assert_close D torch full test_linalg_cross helper dtype device = mps dtype torch int dtype torch int x = torch randint dtype=dtype device=device y = torch randint dtype=dtype device=device x = torch rand dtype=dtype device=device y = torch rand dtype=dtype device=device x_cpu = x cpu y_cpu = y cpu res = torch linalg cross x y dim= res = torch tensor dtype=dtype device=device res _cpu = torch linalg cross x_cpu y_cpu dim= res _cpu = torch tensor dtype=dtype device= cpu torch linalg cross x y dim= out=res torch linalg cross x_cpu y_cpu dim= out=res _cpu assertEqual res res assertEqual res res _cpu assertEqual res res _cpu test broadcastable inputs dtype torch int dtype torch int x = torch randint dtype=dtype device=device y = torch randint dtype=dtype device=device x = torch rand dtype=dtype device=device y = torch rand dtype=dtype device=device x_cpu = x cpu y_cpu = y cpu res = torch linalg cross x y dim= res = torch tensor dtype=dtype device=device res _cpu = torch linalg cross x_cpu y_cpu dim= res _cpu = torch tensor dtype=dtype device= cpu torch linalg cross x y dim= out=res torch linalg cross x_cpu y_cpu dim= out=res _cpu assertEqual res res assertEqual res res _cpu assertEqual res res _cpu helper dtype dtype torch int torch int torch float test_cross = torch randn device= mps b = torch randn device= mps a_cpu = cpu b_cpu = b cpu res = torch cross b dim= res_cpu = torch cross a_cpu b_cpu dim= assertEqual res res_cpu test_addmm A = torch ones mps B = torch ones mps C = torch ones mps D = torch addmm A B C cpu torch testing assert_close D torch full test_bmm batch _cpu = torch randn batch _cpu = torch randn batch _mps = batch _cpu detach clone mps batch _mps = batch _cpu detach clone mps output_cpu = torch bmm batch _cpu batch _cpu output_mps = torch bmm batch _mps batch _mps assertEqual output_cpu output_mps assertEqual output_cpu size output_mps size xfailIf MACOS_VERSION parametrize dtype torch float torch bfloat test_large_bmm dtype B M N = batch = torch randn B M N dtype=dtype device= mps batch = torch randn B N M dtype=dtype device= mps output_mps = torch bmm batch batch For performance reasons check only one non-first batch correctness TODO Check two when https github com pytorch pytorch issues fixed batch_idx = torch randint B size= item output_cpu = torch mm batch batch_idx cpu batch batch_idx cpu Using low precision comparison FP tol = e- dtype == torch float None assertEqual output_cpu output_mps batch_idx atol=tol rtol=tol parametrize dtype torch float torch float torch bfloat test_take_along_dim dtype x = torch tensor - dtype=dtype inds = torch tensor ref = torch take_along_dim x inds x_mps = x detach clone mps inds_mps = inds detach clone mps res = torch take_along_dim x_mps inds_mps assertEqual res ref test_addr A = torch ones mps B = torch ones mps C = torch ones mps D = torch addr A B C cpu torch testing assert_close D torch full test_trace M_cpu = torch randn M_mps = M_cpu detach clone mps output_cpu = torch trace M_cpu output_mps = torch trace M_mps assertEqual output_cpu output_mps assertEqual output_cpu size output_mps size test_addbmm M_cpu = torch randn batch _cpu = torch randn batch _cpu = torch randn M_mps = M_cpu detach clone mps batch _mps = batch _cpu detach clone mps batch _mps = batch _cpu detach clone mps output_cpu = torch addbmm M_cpu batch _cpu batch _cpu output_mps = torch addbmm M_mps batch _mps batch _mps assertEqual output_cpu output_mps assertEqual output_cpu size output_mps size test_baddbmm helper input_shape batch _shape batch _shape M_cpu = torch randn input_shape batch _cpu = torch randn batch _shape batch _cpu = torch randn batch _shape alpha = beta = M_mps = M_cpu detach clone mps batch _mps = batch _cpu detach clone mps batch _mps = batch _cpu detach clone mps output_cpu = torch baddbmm M_cpu batch _cpu batch _cpu beta=beta alpha=alpha output_mps = torch baddbmm M_mps batch _mps batch _mps beta=beta alpha=alpha assertEqual output_cpu output_mps assertEqual output_cpu size output_mps size helper input_shape= batch _shape= batch _shape= helper input_shape= batch _shape= batch _shape= helper input_shape= batch _shape= batch _shape= test_local_scalar_dense_mps x_cpu = torch randn y_mps = x_cpu mps torch testing assert_close x_cpu item y_mps item test_linear_ d_weight device = cpu projected = torch rand device x = torch rand device x_mps = x mps projected_mps = projected mps linear = F linear x projected linear_mps = F linear x_mps projected_mps assertEqual linear linear_mps projected = torch rand device x = torch rand device x_mps = x mps projected_mps = projected mps linear = F linear x projected linear_mps = F linear x_mps projected_mps assertEqual linear linear_mps test_linear_bias helper bias_shape device = cpu x = torch randn device=device linear = torch nn Linear device=device linear bias = torch nn Parameter torch randn bias_shape dtype=torch float device=device y = linear x device = mps x_mps = x device linear device y_mps = linear x_mps assertEqual y y_mps helper helper test_linear_errors Mixed CPU - MPS tensors size = Unsupported dtypes assertRaisesRegex RuntimeError does support linear non-float weights torch nn functional linear torch rand size device= mps torch randint - size dtype=torch int device= mps Weights wrong device assertRaisesRegex RuntimeError argument weight cpu expected mps torch nn functional linear torch rand size device= mps torch rand size device= cpu Input wrong device assertRaisesRegex RuntimeError argument input cpu expected mps torch nn functional linear torch rand size device= cpu torch rand size device= mps test_linear_non_contiguous Regression test https github com pytorch pytorch issues Slice tensors force non-contiguity large_weight = torch randn device= mps weight_sliced = large_weight weight_contiguous_equiv = weight_sliced contiguous input_s = torch randn device= mps result_sliced = torch nn functional linear input_s weight_sliced result_contig = torch nn functional linear input_s weight_contiguous_equiv assertEqual result_contig result_sliced _linear_helper in_features out_features shape bias=True backward_pass=False cpu_linear = torch nn Linear in_features=in_features out_features=out_features device= cpu bias=bias mps_linear = torch nn Linear in_features=in_features out_features=out_features device= mps bias=bias Use same weights bias ones cpu mps_linear weight data = cpu_linear weight data detach clone mps bias mps_linear bias data = cpu_linear bias data detach clone mps linear_mps_input = torch randn shape mps linear_cpu_input = linear_mps_input detach clone cpu backward_pass linear_mps_input = linear_mps_input requires_grad_ linear_cpu_input = linear_cpu_input requires_grad_ linear_cpu_output = cpu_linear linear_cpu_input linear_mps_output = mps_linear linear_mps_input assertEqual linear_cpu_output linear_mps_output cpu assertEqual linear_cpu_output size linear_mps_output size backward_pass cpu_grad = torch rand_like linear_cpu_output requires_grad=True grad = cpu_grad detach mps requires_grad_ linear_cpu_output backward gradient=cpu_grad create_graph=True linear_mps_output backward gradient=grad create_graph=True assertEqual linear_cpu_input grad size linear_mps_input grad size assertEqual linear_cpu_input grad linear_mps_input grad cpu atol= e- rtol= e- assertEqual cpu_linear weight grad size mps_linear weight grad size assertEqual cpu_linear weight grad mps_linear weight grad cpu atol= e- rtol= e- bias assertEqual cpu_linear bias grad size mps_linear bias grad size assertEqual cpu_linear bias grad mps_linear bias grad cpu atol= e- rtol= e- test gradgrad x_grad_out = torch rand_like linear_cpu_input x_grad_out_mps = x_grad_out mps w_grad_out = torch rand_like cpu_linear weight w_grad_out_mps = w_grad_out mps linear_cpu_input grad detach zero_ linear_mps_input grad detach zero_ cpu_linear weight grad detach zero_ mps_linear weight grad detach zero_ bias b_grad_out = torch rand_like cpu_linear bias b_grad_out_mps = b_grad_out mps cpu_linear bias grad detach zero_ mps_linear bias grad detach zero_ linear_cpu_input grad backward x_grad_out retain_graph=True linear_mps_input grad backward x_grad_out_mps retain_graph=True cpu_linear weight grad backward w_grad_out retain_graph=True mps_linear weight grad backward w_grad_out_mps retain_graph=True bias cpu_linear bias grad backward b_grad_out retain_graph=True mps_linear bias grad backward b_grad_out_mps retain_graph=True assertEqual cpu_grad grad grad grad assertEqual linear_cpu_input grad linear_mps_input grad assertEqual cpu_linear weight grad mps_linear weight grad bias assertEqual cpu_linear bias grad mps_linear bias grad test_linear D _linear_helper in_features= out_features= shape= bias=True backward_pass=False test_linear D_backward _linear_helper in_features= out_features= shape= bias=True backward_pass=True test_linear D _linear_helper in_features= out_features= shape= bias=True backward_pass=False test_linear D_backward _linear_helper in_features= out_features= shape= bias=True backward_pass=True test_linear D_no_bias _linear_helper in_features= out_features= shape= bias=False backward_pass=False test_linear D_no_bias_backward _linear_helper in_features= out_features= shape= bias=False backward_pass=True test_linear D _linear_helper in_features= out_features= shape= bias=True backward_pass=False test_linear D_backward _linear_helper in_features= out_features= shape= bias=True backward_pass=True test_linear D_no_bias _linear_helper in_features= out_features= shape= bias=True backward_pass=False test_linear D_no_bias_backward _linear_helper in_features= out_features= shape= bias=True backward_pass=True test_linear_large Regression test https github com pytorch pytorch issues x_cpu = torch randn device= cpu w_cpu = torch randn device= cpu x_mps = x_cpu detach clone mps w_mps = w_cpu detach clone mps out_cpu = F linear x_cpu w_cpu None out_mps = F linear x_mps w_mps None assertEqual out_cpu out_mps test_uniform low = torch zeros requires_grad=True high = torch ones requires_grad_ low_ d = torch zeros requires_grad=True high_ d = torch ones requires_grad_ assertEqual Uniform low high sample size assertEqual Uniform low high sample size assertEqual Uniform low_ d high_ d sample size assertEqual Uniform low_ d high_ d sample size assertEqual Uniform sample size Check log_prob computation when value outside range uniform = Uniform low_ d high_ d validate_args=False above_high = torch tensor below_low = torch tensor - assertEqual uniform log_prob above_high item -inf assertEqual uniform log_prob below_low item -inf check cdf computation when value outside range assertEqual uniform cdf below_low item assertEqual uniform cdf above_high item state = torch get_rng_state rand = low new low size uniform_ torch set_rng_state state u = Uniform low high rsample u backward torch ones_like u assertEqual low grad - rand assertEqual high grad rand low grad zero_ high grad zero_ test_randperm device= mps rng_device = None n dtype torch long torch half torch float n dtype == torch half Large n torch half will raise exception do test here continue n dtype == torch bfloat continue torch random fork_rng devices=rng_device res = torch randperm n dtype=dtype device=device res = torch empty dtype=dtype device=device torch randperm n out=res dtype=dtype device=device assertEqual res cpu sort values long torch arange n device=device Default type long n assertEqual torch randperm n device=device dtype torch long randperm elements empty tensor res = torch randperm res = torch tensor dtype=dtype device=device torch randperm out=res assertEqual res numel assertEqual res numel Test non-contiguous tensors n non_contiguous_tensor = torch zeros dtype=torch long device=device t assertFalse non_contiguous_tensor is_contiguous torch random fork_rng devices=rng_device res = torch randperm n dtype=torch long device=device torch randperm n out=non_contiguous_tensor assertEqual res cpu sort values long torch arange n device=device Test forward maxpool d test_max_pool d helper shape ks padding= dilation= ceil_mode=False return_indices=False test_ties=False cpu_x = None test_ties cpu_x = torch ones shape device= cpu dtype=torch float requires_grad=True cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ pool = torch nn MaxPool d kernel_size=ks padding=padding dilation=dilation ceil_mode=ceil_mode return_indices=return_indices return_indices False y = pool x ref_y = pool cpu_x cpu_grad = torch ones_like ref_y grad = cpu_grad mps y backward gradient=grad ref_y backward gradient=cpu_grad assertEqual y ref_y assertEqual x grad cpu_x grad y idx = pool x ref_y ref_idx = pool cpu_x cpu_grad = torch ones_like ref_y grad = cpu_grad mps y backward gradient=grad ref_y backward gradient=cpu_grad assertEqual y ref_y assertEqual idx ref_idx assertEqual x grad cpu_x grad Test no batch dimension helper ks= helper ks= helper ks= helper ks= test max_pool d Test padding helper ks= padding= helper ks= padding= test max_pool d Test dilation helper ks= dilation= helper ks= padding= test max_pool d Test ceil mode helper ks= ceil_mode=True helper ks= ceil_mode=True test max_pool d Test indices test_ties False True Test no batch dimension helper ks= return_indices=True test_ties=test_ties helper ks= return_indices=True test_ties=test_ties helper ks= return_indices=True test_ties=test_ties helper ks= return_indices=True test_ties=test_ties test max_pool d Test padding helper ks= padding= return_indices=True test_ties=test_ties helper ks= padding= return_indices=True test_ties=test_ties test max_pool d Test dilation helper ks= dilation= return_indices=True test_ties=test_ties helper ks= padding= return_indices=True test_ties=test_ties test max_pool d Test ceil mode helper ks= ceil_mode=True return_indices=True test_ties=test_ties helper ks= ceil_mode=True return_indices=True test_ties=test_ties test max_pool d test_adaptive_avg_pool d_output_size_one helper size memory_format x = torch randint size dtype=torch float device= mps requires_grad=True memory_format == non_contiguous x = x x = x memory_format=memory_format net = torch nn AdaptiveAvgPool d out = net x ref_out = x contiguous mean - - view x size x size out sum backward make sure doesn t crash assertEqual out ref_out memory_format == torch channels_last assertTrue out is_contiguous memory_format=torch channels_last c = out size assertEqual out stride c c c assertTrue out is_contiguous c = out size assertEqual out stride c helper torch contiguous_format test_masked_scatter helper shape x_mps = torch randn shape device= mps x_cpu = x_mps detach clone cpu mask_mps = torch rand shape device= mps mask_cpu = mask_mps detach clone cpu y_mps = torch randn shape device= mps y_cpu = y_mps detach clone cpu y_mps masked_scatter_ mask_mps x_mps y_cpu masked_scatter_ mask_cpu x_cpu assertEqual y_mps y_cpu helper helper helper helper helper test_masked_fill device = mps dtype = torch float mask_dtype = torch bool num_dest = dst = torch zeros num_dest dtype=dtype device=device mask = torch randint num_dest dtype=mask_dtype device=device val = random random dst = torch zeros num_dest dtype=dtype mask_cpu = mask cpu dst masked_fill_ mask val i range num_dest mask_cpu i dst i = val assertEqual dst cpu dst atol= rtol= Regression test https github com pytorch pytorch issues Allocating x x x tensor crashes MacOS- mask_bool = torch triu torch ones device=device diagonal= bool attn_scores = torch rand device=device attn_scores masked_fill_ mask_bool test_masked_fill__non_contiguous shape = x_mps = torch randn shape device= mps x_cpu = x_mps detach clone cpu mask_mps = torch zeros shape device= mps dtype=torch bool mask_cpu = mask_mps detach clone cpu x_mps_strided = x_mps T x_cpu_strided = x_cpu T x_mps_strided masked_fill_ mask_mps T float -inf x_cpu_strided masked_fill_ mask_cpu T float -inf assertEqual x_mps_strided x_cpu_strided assertFalse x_mps_strided == float -inf any test_nhwc_operation helper shape channels_last=False numpy np np random seed arr = - np random random_sample size=shape + cpu_x = torch tensor arr device= cpu dtype=torch float requires_grad=True channels_last cpu_x = cpu_x memory_format=torch channels_last cpu_x retain_grad x = cpu_x detach clone mps requires_grad_ This passes assertEqual x cpu_x helper True Test forward batch norm test_batch_norm helper shape eps= momentum= wts=False training=False channels_last=False track_running_stats=True test_module=False numpy np np random seed arr = - np random random_sample size=shape + cpu_x = torch tensor arr device= cpu dtype=torch float requires_grad=True channels_last cpu_x = cpu_x memory_format=torch channels_last cpu_x retain_grad x = cpu_x detach clone mps requires_grad_ mean_shape = shape cpu_running_mean = None cpu_running_var = None running_mean = None running_var = None track_running_stats mean_arr = - np random random_sample size=mean_shape + cpu_running_mean = torch tensor mean_arr device= cpu dtype=torch float var_arr = np random random_sample size=mean_shape cpu_running_var = torch tensor var_arr device= cpu dtype=torch float running_mean = cpu_running_mean detach clone mps running_var = cpu_running_var detach clone mps weight = None cpu_weight = None bias = None cpu_bias = None wts cpu_weight = torch randn mean_shape device= cpu dtype=torch float requires_grad=True weight = cpu_weight detach clone mps requires_grad_ cpu_bias = torch randn mean_shape device= cpu dtype=torch float requires_grad=True bias = cpu_bias detach clone mps requires_grad_ y = None ref_y = None test_module y = torch nn functional batch_norm x running_mean running_var weight=weight bias=bias training=training momentum=momentum eps=eps ref_y = torch nn functional batch_norm cpu_x cpu_running_mean cpu_running_var weight=cpu_weight bias=cpu_bias training=training momentum=momentum eps=eps batchnorm_op = None mps_batchnorm_op = None len shape == batchnorm_op = torch nn BatchNorm d shape eps=eps momentum=momentum affine=wts track_running_stats=track_running_stats device= cpu mps_batchnorm_op = torch nn BatchNorm d shape eps=eps momentum=momentum affine=wts track_running_stats=track_running_stats device= mps len shape == batchnorm_op = torch nn BatchNorm d shape eps=eps momentum=momentum affine=wts track_running_stats=track_running_stats device= cpu mps_batchnorm_op = torch nn BatchNorm d shape eps=eps momentum=momentum affine=wts track_running_stats=track_running_stats device= mps len shape == batchnorm_op = torch nn BatchNorm d shape eps=eps momentum=momentum affine=wts track_running_stats=track_running_stats device= cpu mps_batchnorm_op = torch nn BatchNorm d shape eps=eps momentum=momentum affine=wts track_running_stats=track_running_stats device= mps track_running_stats batchnorm_op running_mean = cpu_running_mean batchnorm_op running_var = cpu_running_var mps_batchnorm_op running_mean = running_mean mps_batchnorm_op running_var = running_var wts batchnorm_op weight = torch nn Parameter cpu_weight batchnorm_op bias = torch nn Parameter cpu_bias mps_batchnorm_op weight = torch nn Parameter weight mps_batchnorm_op bias = torch nn Parameter bias ref_y = batchnorm_op cpu_x y = mps_batchnorm_op x assertEqual y ref_y test_module assertEqual running_mean cpu_running_mean assertEqual running_var cpu_running_var assertEqual mps_batchnorm_op running_mean batchnorm_op running_mean assertEqual mps_batchnorm_op running_var batchnorm_op running_var cpu_grad = torch randn ref_y shape grad = cpu_grad mps ref_y backward gradient=cpu_grad y backward gradient=grad assertEqual x grad cpu_x grad wts test_module assertEqual weight grad cpu_weight grad assertEqual bias grad cpu_bias grad assertEqual mps_batchnorm_op weight grad batchnorm_op weight grad assertEqual mps_batchnorm_op bias grad batchnorm_op bias grad shape test_module False True track_running_stats True False channels_last False channels_last len shape = continue Running stats must tracked eval mode track_running_stats helper shape eps= momentum= channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= e- momentum= wts=False training=False channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= momentum= wts=False training=False channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= momentum= wts=True training=False channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= momentum= wts=True training=False channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= e- momentum= wts=False training=True channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= momentum= wts=False training=True channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= momentum= wts=True training=True channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= momentum= wts=True training=True channels_last=channels_last track_running_stats=track_running_stats test_module=test_module test_batch_norm_backward inputs = torch rand device= mps requires_grad=True x = torch nn BatchNorm d mps y = torch nn BatchNorm d mps y weight requires_grad = False y bias requires_grad = False outputs = y x inputs This used crash see https github com pytorch pytorch issues outputs sum backward test_batch_norm_slices Regression test https github com pytorch pytorch issues bn_cpu = nn BatchNorm d affine=False device= cpu bn_mps = nn BatchNorm d affine=False device= mps x_cpu = torch randn cpu x_mps = x_cpu mps res_cpu = bn_cpu x_cpu res_mps = bn_mps x_mps assertEqual res_cpu res_mps test_batch_norm_backward_weight_bias_gradients See issue https github com pytorch pytorch issues N C L = x = torch randn N C L y = torch randn N C L bn_cpu = nn BatchNorm d C affine=True cpu train bn_mps = nn BatchNorm d C affine=True mps train bn_mps load_state_dict bn_cpu state_dict out_cpu = bn_cpu x out_mps = bn_mps x mps loss_cpu = out_cpu - y mean loss_mps = out_mps - y mps mean loss_cpu backward loss_mps backward assertEqual bn_cpu weight grad bn_mps weight grad atol= e- rtol= e- assertEqual bn_cpu bias grad bn_mps bias grad atol= e- rtol= e- test_layer_norm_backward inputs = torch rand device= mps requires_grad=True x = torch nn LayerNorm mps y = torch nn LayerNorm mps y weight requires_grad = False y bias requires_grad = False outputs = y x inputs This used crash see https github com pytorch pytorch issues outputs sum backward test_norm = torch arange dtype=torch float device= mps - b = reshape a_cpu = torch arange dtype=torch float device= cpu - b_cpu = a_cpu reshape res = torch norm res_cpu = torch norm a_cpu assertEqual res res_cpu res = torch norm b res_cpu = torch norm b_cpu assertEqual res res_cpu res = torch norm float inf res_cpu = torch norm a_cpu float inf assertEqual res res_cpu res = torch norm b float inf res_cpu = torch norm b_cpu float inf assertEqual res res_cpu c = torch tensor - dtype=torch float device= mps c_cpu = torch tensor - dtype=torch float device= cpu res = torch norm c dim= res_cpu = torch norm c_cpu dim= assertEqual res res_cpu res = torch norm c dim= res_cpu = torch norm c_cpu dim= assertEqual res res_cpu res = torch norm c p= dim= res_cpu = torch norm c_cpu p= dim= assertEqual res res_cpu d = torch arange dtype=torch float device= mps reshape d_cpu = torch arange dtype=torch float device= cpu reshape res = torch norm d dim= res_cpu = torch norm d_cpu dim= assertEqual res res_cpu res = torch norm d torch norm d res_cpu = torch norm d_cpu torch norm d_cpu assertEqual res res_cpu test_linalg_vector_norm x_mps = torch tensor dtype=torch float device= mps x_cpu = x_mps detach clone cpu res_mps = torch linalg vector_norm x_mps ord= res_cpu = torch linalg vector_norm x_cpu ord= assertEqual res_mps res_cpu a_mps = torch arange dtype=torch float device= mps - a_cpu = torch arange dtype=torch float device= cpu - B_mps = a_mps reshape B_cpu = a_cpu reshape res_mps = torch linalg vector_norm a_mps ord= res_cpu = torch linalg vector_norm a_cpu ord= assertEqual res_mps res_cpu res_mps = torch linalg vector_norm B_mps ord= res_cpu = torch linalg vector_norm B_cpu ord= assertEqual res_mps res_cpu dim range B_mps dim res_mps = torch linalg vector_norm B_mps ord= dim=dim res_cpu = torch linalg vector_norm B_cpu ord= dim=dim assertEqual res_mps res_cpu test_linalg_lu_factor_ex torch testing _internal common_utils make_fullrank_matrices_with_distinct_singular_values make_fullrank = make_fullrank_matrices_with_distinct_singular_values make_arg = partial make_fullrank device= cpu dtype=torch float run_lu_factor_ex_test size batch_dims check_errors atol= e- rtol= e- input_cpu = make_arg batch_dims size size input_mps = input_cpu mps out_cpu = torch linalg lu_factor_ex input_cpu check_errors=check_errors out_mps = torch linalg lu_factor_ex input_mps check_errors=check_errors assertEqual out_cpu out_mps atol=atol rtol=rtol out_cpu = torch linalg lu_factor_ex input_cpu mT check_errors=check_errors out_mps = torch linalg lu_factor_ex input_mps mT check_errors=check_errors assertEqual out_cpu out_mps atol=atol rtol=rtol test different even odd matrix sizes matrix_sizes = even odd batch sizes batch_sizes = check_errors True False size matrix_sizes batch_size batch_sizes run_lu_factor_ex_test size batch_size check_errors=check_errors test D matrices run_lu_factor_ex_test check_errors=False run_lu_factor_ex_test check_errors=True big matrix check batch size run_lu_factor_ex_test check_errors=False atol= e- rtol= e- test_linalg_lu_factor_singular Explicit singular matrix A = torch tensor device= mps assertRaisesRegex RuntimeError result division zero torch linalg lu_factor A test_linalg_solve torch testing _internal common_utils make_fullrank_matrices_with_distinct_singular_values make_fullrank = make_fullrank_matrices_with_distinct_singular_values make_arg = partial make_fullrank device= cpu dtype=torch float run_linalg_solve_test size batch_dims A_cpu = make_arg batch_dims size size A_mps = A_cpu mps left True False left b_cpu = torch randn batch_dims size device= cpu dtype=torch float b_cpu = torch randn batch_dims size device= cpu dtype=torch float b_mps = b_cpu mps Solve system X_cpu = torch linalg solve A_cpu b_cpu left=left X_mps = torch linalg solve A_mps b_mps left=left assertEqual X_cpu X_mps Test transposed matrices X_cpu_t = torch linalg solve A_cpu mT b_cpu left=left X_mps_t = torch linalg solve A_mps mT b_mps left=left assertEqual X_cpu_t X_mps_t test different even odd matrix sizes matrix_sizes = even odd batch sizes batch_sizes = size matrix_sizes batch_size batch_sizes run_linalg_solve_test size batch_size test D matrices run_linalg_solve_test run_linalg_solve_test test_linalg_solve_singular Regression test https github com pytorch pytorch issues Explicit singular matrix A = torch tensor device= mps b = torch rand_like A assertRaisesRegex RuntimeError input matrix singular torch linalg solve A b test_linalg_solve_with_broadcasting functools partial torch torch testing _internal common_utils make_fullrank_matrices_with_distinct_singular_values make_fullrank = make_fullrank_matrices_with_distinct_singular_values make_arg = partial make_fullrank device= cpu dtype=torch float batch_size = size = A_cpu = make_arg batch_size size size A_mps = A_cpu mps left True False b_cpu = torch randn batch_size size device= cpu dtype=torch float b_mps = b_cpu mps left b_cpu = b_cpu unsqueeze - b_mps = b_mps unsqueeze - b_cpu = b_cpu view batch_size size b_mps = b_mps view batch_size size X_cpu = torch linalg solve A_cpu b_cpu left=left X_mps = torch linalg solve A_mps b_mps left=left assertEqual X_cpu X_mps X_cpu_t = torch linalg solve A_cpu mT b_cpu left=left X_mps_t = torch linalg solve A_mps mT b_mps left=left assertEqual X_cpu_t X_mps_t test_linalg_det torch testing _internal common_utils make_fullrank_matrices_with_distinct_singular_values make_fullrank = make_fullrank_matrices_with_distinct_singular_values make_arg = partial make_fullrank device= cpu dtype=torch float run_det_test size batch_dims input_cpu = make_arg batch_dims size size input_mps = input_cpu mps out_cpu = torch linalg det input_cpu out_mps = torch linalg det input_mps assertEqual out_cpu out_mps non-contiguous matrices input_cpu_T = input_cpu mT input_mps_T = input_mps mT out_cpu_T = torch linalg det input_cpu_T out_mps_T = torch linalg det input_mps_T assertEqual out_cpu_T out_mps_T test different even odd matrix sizes matrix_sizes = even odd batch sizes batch_sizes = size matrix_sizes batch_size batch_sizes run_det_test size batch_size test D matrices run_det_test run_det_test test_layer_norm helper input_shape normalized_shape eps= e- elementwise_affine=True dtype=torch float non_contiguous=False cpu_x = torch randn input_shape device= cpu dtype=dtype requires_grad=True x = cpu_x detach clone mps requires_grad_ non_contiguous x = x mT cpu_x = cpu_x mT normalized_shape - normalized_shape - = normalized_shape - normalized_shape - cpu_op = torch nn LayerNorm normalized_shape eps=eps elementwise_affine=elementwise_affine device= cpu dtype=dtype mps_op = torch nn LayerNorm normalized_shape eps=eps elementwise_affine=elementwise_affine device= mps dtype=dtype cpu_wt = torch randn normalized_shape device= cpu dtype=dtype requires_grad=True wt = cpu_wt detach clone mps requires_grad_ cpu_bias = torch randn normalized_shape device= cpu dtype=dtype requires_grad=True bias = cpu_bias detach clone mps requires_grad_ elementwise_affine cpu_op weight = torch nn Parameter cpu_wt mps_op weight = torch nn Parameter wt cpu_op bias = torch nn Parameter cpu_bias mps_op bias = torch nn Parameter bias cpu_result = cpu_op cpu_x result = mps_op x cpu_grad = torch randn cpu_result shape grad = cpu_grad mps cpu_result backward cpu_grad result backward grad assertEqual result cpu_result assertEqual x grad cpu_x grad elementwise_affine assertEqual mps_op weight grad cpu_op weight grad assertEqual mps_op bias grad cpu_op bias grad elementwise_affine non_contiguous itertools product True False True False helper elementwise_affine=elementwise_affine non_contiguous=non_contiguous helper elementwise_affine=elementwise_affine non_contiguous=non_contiguous helper elementwise_affine=elementwise_affine non_contiguous=non_contiguous Regression test https github com pytorch pytorch issues torch nn LayerNorm elementwise_affine=True mps torch randn mps dtype=torch float test_ifft See https github com pytorch pytorch issues device = torch device mps N = signal = torch rand N device=device fft_result = torch fft rfft signal ifft_result = torch fft irfft fft_result n=signal shape Expecting inverted yield original signal assertEqual ifft_result signal test_fftfreq Regression test https github com pytorch pytorch issues freq_cpu = torch fft fftfreq device= cpu freq_mps = torch fft fftfreq device= mps assertEqual freq_cpu freq_mps test_instance_norm helper shape eps= momentum= wts=False channels_last=False track_running_stats=True test_module=False numpy np np random seed arr = - np random random_sample size=shape + cpu_x = torch tensor arr device= cpu dtype=torch float requires_grad=True channels_last cpu_x = cpu_x memory_format=torch channels_last cpu_x retain_grad x = cpu_x detach clone mps requires_grad_ mean_shape = shape cpu_running_mean = None cpu_running_var = None running_mean = None running_var = None track_running_stats mean_arr = - np random random_sample size=mean_shape + cpu_running_mean = torch tensor mean_arr device= cpu dtype=torch float var_arr = np random random_sample size=mean_shape cpu_running_var = torch tensor var_arr device= cpu dtype=torch float running_mean = cpu_running_mean detach clone mps running_var = cpu_running_var detach clone mps weight = None cpu_weight = None bias = None cpu_bias = None wts cpu_weight = torch randn mean_shape device= cpu dtype=torch float requires_grad=True weight = cpu_weight detach clone mps requires_grad_ cpu_bias = torch randn mean_shape device= cpu dtype=torch float requires_grad=True bias = cpu_bias detach clone mps requires_grad_ y = None ref_y = None test_module ref_y = torch nn functional instance_norm cpu_x cpu_running_mean cpu_running_var weight=cpu_weight bias=cpu_bias momentum=momentum eps=eps y = torch nn functional instance_norm x running_mean running_var weight=weight bias=bias momentum=momentum eps=eps instancenorm_op = None mps_instancenorm_op = None len shape == instancenorm_op = torch nn InstanceNorm d shape eps=eps momentum=momentum affine=wts track_running_stats=track_running_stats device= cpu mps_instancenorm_op = torch nn InstanceNorm d shape eps=eps momentum=momentum affine=wts track_running_stats=track_running_stats device= mps len shape == instancenorm_op = torch nn InstanceNorm d shape eps=eps momentum=momentum affine=wts track_running_stats=track_running_stats device= cpu mps_instancenorm_op = torch nn InstanceNorm d shape eps=eps momentum=momentum affine=wts track_running_stats=track_running_stats device= mps len shape == instancenorm_op = torch nn InstanceNorm d shape eps=eps momentum=momentum affine=wts track_running_stats=track_running_stats device= cpu mps_instancenorm_op = torch nn InstanceNorm d shape eps=eps momentum=momentum affine=wts track_running_stats=track_running_stats device= mps track_running_stats instancenorm_op running_mean = cpu_running_mean instancenorm_op running_var = cpu_running_var mps_instancenorm_op running_mean = running_mean mps_instancenorm_op running_var = running_var wts instancenorm_op weight = torch nn Parameter cpu_weight instancenorm_op bias = torch nn Parameter cpu_bias mps_instancenorm_op weight = torch nn Parameter weight mps_instancenorm_op bias = torch nn Parameter bias ref_y = instancenorm_op cpu_x y = mps_instancenorm_op x assertEqual y ref_y test_module assertEqual running_mean cpu_running_mean assertEqual running_var cpu_running_var assertEqual mps_instancenorm_op running_mean instancenorm_op running_mean assertEqual mps_instancenorm_op running_var instancenorm_op running_var cpu_grad = torch randn ref_y shape grad = cpu_grad mps ref_y backward gradient=cpu_grad y backward gradient=grad assertEqual x grad cpu_x grad wts test_module assertEqual weight grad cpu_weight grad assertEqual bias grad cpu_bias grad assertEqual mps_instancenorm_op weight grad instancenorm_op weight grad assertEqual mps_instancenorm_op bias grad instancenorm_op bias grad shape test_module False True track_running_stats True False channels_last False channels_last len shape = continue Running stats must tracked eval mode track_running_stats helper shape eps= momentum= channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= e- momentum= wts=False channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= momentum= wts=False channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= momentum= wts=True channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= momentum= wts=True channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= e- momentum= wts=False channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= momentum= wts=False channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= momentum= wts=True channels_last=channels_last track_running_stats=track_running_stats test_module=test_module helper shape eps= momentum= wts=True channels_last=channels_last track_running_stats=track_running_stats test_module=test_module test_weight_norm validate_weight_norm_equality model cpu_model x cpu_x dim cpu_norm = torch nn utils parametrizations weight_norm cpu_model dim=dim norm = torch nn utils parametrizations weight_norm model dim=dim cpu_out = cpu_norm cpu_x out = norm x assertEqual cpu_out out cpu_grad = torch randn cpu_out shape grad = cpu_grad mps cpu_out backward gradient=cpu_grad out backward gradient=grad assertEqual cpu_model parametrizations weight original grad model parametrizations weight original grad assertEqual cpu_model parametrizations weight original grad model parametrizations weight original grad assertEqual x grad cpu_x grad helper dim layer= linear dtype=torch float linear layer layer == linear cpu_x = torch randn device= cpu dtype=dtype requires_grad=True x = cpu_x detach clone mps requires_grad_ cpu_weight = torch randn device= cpu dtype=dtype requires_grad=True weight = cpu_weight detach clone mps requires_grad_ cpu_bias = torch randn device= cpu dtype=dtype requires_grad=True bias = cpu_bias detach clone mps requires_grad_ cpu_linear = torch nn Linear device= cpu linear = torch nn Linear device= mps torch no_grad cpu_linear weight copy_ cpu_weight cpu_linear bias copy_ cpu_bias linear weight copy_ weight linear bias copy_ bias validate_weight_norm_equality linear cpu_linear x cpu_x dim conv layer layer == conv cpu_x = torch randn device= cpu dtype=dtype requires_grad=True x = cpu_x detach clone mps requires_grad_ cpu_conv = torch nn Conv d device= cpu conv = torch nn Conv d device= mps torch no_grad conv weight copy_ cpu_conv weight conv bias copy_ cpu_conv bias validate_weight_norm_equality conv cpu_conv x cpu_x dim conv d layer layer == conv d cpu_x = torch randn device= cpu dtype=dtype requires_grad=True x = cpu_x detach clone mps requires_grad_ cpu_conv = torch nn Conv d device= cpu conv = torch nn Conv d device= mps torch no_grad conv weight copy_ cpu_conv weight conv bias copy_ cpu_conv bias validate_weight_norm_equality conv cpu_conv x cpu_x dim helper layer= linear helper layer= linear helper - layer= linear helper layer= conv helper layer= conv helper layer= conv helper layer= conv helper - layer= conv Conv d only available MacOS onwards helper layer= conv d helper layer= conv d helper layer= conv d helper layer= conv d helper layer= conv d helper - layer= conv d Test conv d test_conv d_unit helper input_shape wt_shape stride= padding= dilation= groups= bias_shape=None cpu_x = torch randn input_shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ cpu_wt = torch randn wt_shape device= cpu dtype=torch float requires_grad=True wt = cpu_wt detach clone mps requires_grad_ cpu_bias = None bias = None bias_shape None cpu_bias = torch randn bias_shape device= cpu dtype=torch float requires_grad=True bias = cpu_bias detach clone mps requires_grad_ y = torch nn functional conv d x wt bias=bias stride=stride padding=padding dilation=dilation groups=groups ref_y = torch nn functional conv d cpu_x cpu_wt bias=cpu_bias stride=stride padding=padding dilation=dilation groups=groups cpu_grad = torch ones_like ref_y grad = cpu_grad mps y backward gradient=grad ref_y backward gradient=cpu_grad assertEqual y ref_y rtol= e- atol= e- assertEqual x grad cpu_x grad rtol= e- atol= e- assertEqual wt grad cpu_wt grad atol= e- rtol= e- bias_shape None assertEqual bias grad cpu_bias grad atol= e- rtol= e- N = C_in = C_out = H = W = kH = kW = stride = padding = helper N C_in H W C_out C_in kH kW stride=stride padding=padding N = C_in = H = W = C_out = kH = kW = groups helper N C_in H W C_out C_in groups kH kW groups=groups helper N C_in H W C_out C_in groups kH kW groups=groups helper N C_in H W C_out C_in groups kH kW bias_shape= C_out groups=groups helper N C_in H W C_out C_in groups kH kW bias_shape= C_out groups=groups helper N C_in H W C_out C_in groups kH + kW + groups=groups helper N C_in H W C_out C_in groups kH + kW + groups=groups helper N C_in H W C_out C_in groups kH + kW + bias_shape= C_out groups=groups helper N C_in H W C_out C_in groups kH + kW + bias_shape= C_out groups=groups Test conv transpose d test_conv_transpose d helper input_shape wt_shape stride= padding= output_padding= dilation= groups= bias_shape=None cpu_x = torch randn input_shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ cpu_wt = torch randn wt_shape device= cpu dtype=torch float requires_grad=True wt = cpu_wt detach clone mps requires_grad_ cpu_bias = None bias = None bias_shape None cpu_bias = torch randn bias_shape device= cpu dtype=torch float requires_grad=True bias = cpu_bias detach clone mps requires_grad_ y = torch nn functional conv_transpose d x wt bias=bias stride=stride padding=padding output_padding=output_padding groups=groups dilation=dilation ref_y = torch nn functional conv_transpose d cpu_x cpu_wt bias=cpu_bias stride=stride padding=padding output_padding=output_padding groups=groups dilation=dilation cpu_grad = torch randn ref_y shape grad = cpu_grad mps y backward gradient=grad ref_y backward gradient=cpu_grad assertEqual y ref_y rtol= e- atol= e- assertEqual x grad cpu_x grad rtol= e- atol= e- assertEqual wt grad cpu_wt grad atol= e- rtol= e- bias_shape None print cpu_bias grad print bias grad cpu assertEqual bias grad cpu_bias grad N = C_in = H = W = C_out = groups = kH = kW = stride padding output_padding dilation output_padding = stride output_padding = dilation continue helper N C_out H W C_out C_in kH kW stride=stride padding=padding output_padding=output_padding dilation=dilation helper N C_out H W C_out C_in kH kW stride=stride padding=padding output_padding=output_padding dilation=dilation helper N C_out H W C_out C_in kH kW bias_shape= C_in stride=stride padding=padding output_padding=output_padding dilation=dilation helper N C_out H W C_out C_in kH kW bias_shape= C_in stride=stride padding=padding output_padding=output_padding dilation=dilation Test sigmoid test_sigmoid helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ sigmoid_op = torch nn Sigmoid y = sigmoid_op x ref_y = sigmoid_op cpu_x cpu_grad = torch ones_like ref_y grad = cpu_grad mps y backward gradient=grad ref_y backward gradient=cpu_grad assertEqual y ref_y assertEqual x grad cpu_x grad helper helper helper Test tanh test_tanh helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ tanh_op = torch nn Tanh y = tanh_op x ref_y = tanh_op cpu_x cpu_grad = torch ones_like ref_y grad = cpu_grad mps y backward gradient=grad ref_y backward gradient=cpu_grad assertEqual y ref_y assertEqual x grad cpu_x grad helper helper helper test_threshold helper threshold value num_elems inplace=False requires_grad=True m = nn Threshold threshold=threshold value=value inplace=inplace input_cpu = torch randn num_elems requires_grad=requires_grad dtype=torch float input_mps = input_cpu detach clone mps requires_grad_ requires_grad output_cpu = m input_cpu output_mps = m input_mps cpu_grad = torch ones_like output_cpu mps_grad = cpu_grad mps assertEqual output_cpu output_mps requires_grad output_cpu backward gradient=cpu_grad output_mps backward gradient=mps_grad assertEqual input_cpu grad input_mps grad helper threshold= value= num_elems= helper threshold=- value= num_elems= helper threshold= value=- num_elems= helper threshold= value= num_elems= inplace=True requires_grad=False Test pow test_pow helper shape aten pow Tensor_Tensor cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps cpu_y = torch randn shape device= cpu dtype=torch float requires_grad=False y = cpu_y detach clone mps z = torch pow x y ref_z = torch pow cpu_x cpu_y assertEqual z ref_z aten pow Tensor_Scalar cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps exp = random random z = torch pow x exp ref_z = torch pow cpu_x exp assertEqual z ref_z aten pow Scalar x = random random cpu_y = torch randn shape device= cpu dtype=torch float requires_grad=False y = cpu_y detach clone mps z = torch pow x y ref_z = torch pow x cpu_y assertEqual z ref_z helper Test addcmul test_addcmul helper shape value xtype=torch float ytype=None ztype=None rand_helper dtype dtype is_floating_point torch randn shape device= cpu dtype=dtype requires_grad=False torch randint shape dtype=dtype device= cpu requires_grad=False cpu_x = rand_helper xtype x = cpu_x detach clone mps cpu_y = rand_helper ytype ytype None xtype y = cpu_y detach clone mps cpu_z = rand_helper ztype ztype None xtype z = cpu_z detach clone mps y = torch addcmul x y z value=value ref_y = torch addcmul cpu_x cpu_y cpu_z value=value assertEqual y ref_y helper helper helper helper Integral types helper xtype=torch int helper xtype=torch int Mixed types helper xtype=torch float ytype=torch float helper ytype=torch float helper ztype=torch float helper xtype=torch int ytype=torch int ztype=torch uint helper ytype=torch int ztype=torch uint Test addcdiv test_addcdiv helper shape value cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False cpu_y = torch randn shape device= cpu dtype=torch float requires_grad=False clamp avoid division cpu_z = torch randn shape device= cpu dtype=torch float requires_grad=False clamp_min_ cpu_out = torch randn shape device= cpu dtype=torch float requires_grad=False mps_x = cpu_x detach clone mps mps_y = cpu_y detach clone mps mps_z = cpu_z detach clone mps mps_out = cpu_out detach clone mps result_div_mps = torch addcdiv mps_x mps_y mps_z value=value result_div_cpu = torch addcdiv cpu_x cpu_y cpu_z value=value assertEqual result_div_mps result_div_cpu test out variant assertEqual torch addcdiv mps_x mps_y mps_z out=mps_out value=value result_div_cpu helper helper helper value should ignored internally test_addcdiv_transpose Regression test issue https github com pytorch pytorch issues Testing continuity all input tensors helper shape value shape_t = shape - i range j range k range x = torch rand shape device= cpu i == torch rand shape_t device= cpu t y = torch rand shape device= cpu j == torch rand shape_t device= cpu t z = torch rand shape device= cpu k == torch rand shape_t device= cpu t x_mps = x detach clone device= mps y_mps = y detach clone device= mps z_mps = z detach clone device= mps result_cpu = x addcdiv_ y z value=value result_mps = x_mps addcdiv y_mps z_mps value=value result_mps_out = result_cpu detach clone mps torch addcdiv x_mps y_mps z_mps out=result_mps_out value=value assertEqual result_cpu result_mps assertEqual result_cpu result_mps_out helper helper helper helper test_buffer_size_match test shouldn t cause any crash size = cpu_A = torch rand size device= cpu cpu_F = torch rand size size size device= cpu mps_A = cpu_A mps mps_F = cpu_F mps assertEqual cpu_A cpu_F mps_A mps_F test_transpose_inplace values = cpu_x = torch tensor values device= cpu mps_x = torch tensor values device= mps cpu_x transpose_ mps_x transpose_ assertEqual cpu_x mps_x cpu test_expand_cpu_to_mps_copy https github com pytorch pytorch issues x = torch tensor expand mps x_cpu = torch tensor expand assertEqual x_cpu x cpu test_cpu_to_strided_mps_copy https github com pytorch pytorch issues = torch Tensor torch device mps b = torch Tensor - - = b = torch Tensor torch device mps b = torch Tensor - - torch device mps = b assertEqual test_view_slice_reshape x = torch randn device= mps y = x x_cpu = x cpu y_cpu = x_cpu r = y + r_cpu = y_cpu + assertEqual r r_cpu test_slice_reshape x = torch randn dtype=torch float device= mps x_cpu = x detach clone cpu x = x view x_cpu = x_cpu view assertEqual x x_cpu x = x + x_cpu = x_cpu + assertEqual x x_cpu Regression test https github com pytorch pytorch issues slice_and_reshape t t reshape x = torch rand dtype=torch cfloat device= mps x_cpu = x detach clone cpu assertEqual slice_and_reshape x_cpu slice_and_reshape x cpu test_reshape_storage_offset https github com pytorch pytorch issues B = T = lin_cpu = nn Linear lin_mps = nn Linear device= mps Use same weights bias ones cpu lin_mps weight data = lin_cpu weight data detach clone mps requires_grad_ lin_mps bias data = lin_cpu bias data detach clone mps requires_grad_ x_mps = torch rand B T device= mps requires_grad=True x_cpu = x_mps detach clone cpu requires_grad_ x_mps = lin_mps x_mps x_cpu = lin_cpu x_cpu assertEqual x_mps shape B T assertEqual x_cpu shape B T cls_token_mps = torch rand device= mps requires_grad=True repeat B cls_token_cpu = cls_token_mps detach clone cpu x_mps = torch cat cls_token_mps x_mps dim= x_cpu = torch cat cls_token_cpu x_cpu dim= x_mps = x_mps transpose x_cpu = x_cpu transpose target_mps = torch rand_like x_mps target_cpu = target_mps detach clone cpu loss_mps = F mse_loss x_mps target_mps loss_cpu = F mse_loss x_cpu target_cpu assertEqual loss_mps loss_cpu loss_mps backward loss_cpu backward assertEqual x_mps grad x_cpu grad test_stack_storage_offset https github com pytorch pytorch issues x_cpu = torch tensor x_mps = x_cpu detach clone mps y_cpu = torch stack x_cpu x_cpu - dim=- y_mps = torch stack x_mps x_mps - dim=- assertEqual y_cpu y_mps t_mps = torch tensor device= mps t_cpu = t_mps detach cpu detach x_mps = t_mps y_mps = t_mps x_cpu = t_cpu y_cpu = t_cpu res_mps = torch stack y_mps x_mps dim=- res_cpu = torch stack y_cpu x_cpu dim=- assertEqual res_mps res_cpu test_unsafe_chunk https github com pytorch pytorch issues = torch rand dtype=torch float device= cpu ret = unsafe_chunk y = ret ret a_mps = mps ret_mps = a_mps unsafe_chunk y_mps = ret_mps ret_mps assertEqual y y_mps test_slice_casting generate random binary numbers cpu_in = torch bernoulli torch empty uniform_ torch uint mps_in = cpu_in detach clone mps check copy_cast unit - bool tensors storage offset cpu_out = cpu_in torch bool mps_out = mps_in torch bool assertEqual cpu_out mps_out test_slice_reshape_contg_view torch x_mps = torch randn device= mps x_cpu = x_mps detach clone cpu r_mps = x_mps + r_cpu = x_cpu + assertEqual r_mps r_cpu test_contiguous_slice_ d helper shape i range shape j range shape t_mps = torch randn shape device= mps t_cpu = t_mps detach clone cpu y_mps = t_mps i j y_cpu = t_cpu i j assertEqual y_mps + y_cpu + y_mps = t_mps i j y_cpu = t_cpu i j assertEqual y_mps + y_cpu + y_mps = t_mps i j y_cpu = t_cpu i j assertEqual y_mps + y_cpu + y_mps = t_mps i j y_cpu = t_cpu i j assertEqual y_mps + y_cpu + y_mps = t_mps i j y_cpu = t_cpu i j assertEqual y_mps + y_cpu + y_mps = t_mps i j y_cpu = t_cpu i j assertEqual y_mps + y_cpu + l = N range l append N C range l append C helper l D range l append D helper l H range l append H helper l W range l append W helper l l pop l pop l pop l pop l pop helper helper helper helper test_contiguous_slice_ d x = torch randn device= mps x_cpu = x detach clone cpu x = x x_cpu = x_cpu out = x x out_cpu = x_cpu x_cpu assertEqual out out_cpu test_view_slice https github com pytorch pytorch issues NUM_SAMPLES = s = X = torch rand dtype=torch float device= cpu X_mps = X detach clone cpu idx = torch randint X shape repeat len s pts = torch randint X shape NUM_SAMPLES X shape idx_mps = idx mps pts_mps = pts mps pts s = idx pts_mps s = idx_mps actual_pts = torch zeros NUM_SAMPLES X shape dtype=torch float actual_pts_mps = torch zeros NUM_SAMPLES X shape dtype=torch float device= mps i range NUM_SAMPLES j range X shape actual_pts_mps i j = X_mps pts_mps i j j actual_pts i j = X pts i j j assertEqual actual_pts i j actual_pts_mps i j test_slice_scatter shape = tensor = torch randint shape device= mps tensor_before = tensor clone torch empty shape shape device= mps copy_ tensor torch testing assert_close tensor tensor_before test_slice values = cpu_x = torch tensor values device= cpu mps_x = torch tensor values device= mps dtype=torch float cpu_slice = cpu_x mps_slice = mps_x assertEqual cpu_slice mps_slice cpu_slice = cpu_x mps_slice = mps_x assertEqual cpu_slice mps_slice cpu_slice = cpu_x mps_slice = mps_x assertEqual cpu_slice mps_slice cpu cpu_slice = cpu_x mps_slice = mps_x cpu assertEqual cpu_slice mps_slice parametrize torch_type arg_values= torch float torch float torch bfloat test_slice_view_api torch_type torch dtype helper x_tensor y_func z_func r_func=None x_mps = x_tensor detach clone mps y = y_func x_tensor y_mps = y_func x_mps assertEqual y y_mps z = z_func y z_mps = z_func y_mps assertEqual z z_mps assertEqual z storage_offset z_mps storage_offset r_func r = r_func z r_mps = r_func z_mps assertEqual r r_mps Skip bfloat before MacOS MACOS_VERSION torch_type == torch bfloat Tests previously encountered MPS bugs helper torch randn dtype=torch_type lambda x x lambda y y reshape lambda z z + helper torch randn dtype=torch_type lambda x x lambda y y + torch ones device=y device helper torch randn dtype=torch_type lambda x x lambda y y reshape t lambda z z + helper torch arange dtype=torch_type resize lambda x x permute lambda y y + helper torch randn dtype=torch_type lambda x x transpose reshape - lambda y y lambda z z + helper torch randn dtype=torch_type lambda x x expand lambda y y + torch ones device=y device test_slice_reshape_contiguous x = torch randn x_mps = x detach clone mps y = x y_mps = x_mps assertEqual y y_mps z = y reshape z_mps = y_mps reshape assertEqual z z_mps assertEqual z storage_offset z_mps storage_offset test_scalar_from_slice_unary https github com pytorch pytorch issues tensor_list = torch tensor device= mps scalar tensor_list r_mps = torch ceil scalar r_cpu = torch ceil scalar cpu assertEqual r_mps cpu r_cpu test_scalar_from_slice_binary https github com pytorch pytorch issues helper binary_op tensor_list = torch tensor device= mps scalar tensor_list r_mps = binary_op scalar r_cpu = binary_op scalar cpu assertEqual r_mps cpu r_cpu helper torch sub helper torch add helper torch not_equal helper torch eq test_slice_contiguous_view https github com pytorch pytorch issues helper operator t_mps = torch tensor device= mps t_cpu = torch tensor device= cpu contiguous view x_mps = t_mps y_mps = t_mps x_cpu = t_cpu y_cpu = t_cpu res_mps = res_cpu = None operator == = res_mps = x_mps = y_mps res_cpu = x_cpu = y_cpu operator == res_mps = x_mps y_mps res_cpu = x_cpu y_cpu operator == = res_mps = x_mps = y_mps res_cpu = x_cpu = y_cpu operator == res_mps = x_mps = y_mps res_cpu = x_cpu = y_cpu operator == == res_mps = x_mps == y_mps res_cpu = x_cpu == y_cpu operator == = res_mps = x_mps = y_mps res_cpu = x_cpu = y_cpu operator == stack res_mps = torch stack y_mps x_mps dim=- res_cpu = torch stack y_cpu x_cpu dim=- assertEqual res_mps res_cpu op = = == = stack helper op test_slice_of_slice x = torch tensor device= cpu x_mps = torch tensor device= mps tensor = x None tensor_mps = x_mps None res = tensor ne res_mps = tensor_mps ne assertEqual res res_mps test_index_storage_offset https github com pytorch pytorch issues = torch tensor e- - e+ b_cpu = c_cpu = both b c views b has storage offset while c has storage offset when copying cpu mps c will have storage_offset which needs taking into account otherwise ends same value b b = b_cpu mps c = c_cpu mps res_mps = b c res_cpu = b_cpu c_cpu assertEqual res_mps res_cpu res_mps = c b res_cpu = c_cpu b_cpu assertEqual res_mps res_cpu test_flatten values = cpu_x = torch tensor values device= cpu mps_x = torch tensor values device= mps cpu_flatten = cpu_x flatten mps_flatten = mps_x flatten cpu assertEqual cpu_flatten mps_flatten cpu_flatten = cpu_x flatten start_dim= mps_flatten = mps_x flatten start_dim= cpu assertEqual cpu_flatten mps_flatten cpu_flatten = cpu_x flatten end_dim= mps_flatten = mps_x flatten end_dim= cpu assertEqual cpu_flatten mps_flatten Test repeat test_repeat helper shape repeats cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ y = x repeat repeats ref_y = cpu_x repeat repeats cpu_grad = torch randn ref_y shape grad = cpu_grad mps y backward gradient=grad ref_y backward gradient=cpu_grad assertEqual y ref_y assertEqual x grad cpu_x grad helper helper helper helper test_torch_repeat_interleave device= mps y = torch tensor device=device exercise single argument function signature temp = y repeat_interleave assertEqual torch Size temp size dtype torch int torch long lengths = torch tensor dtype=dtype device= mps output_size = torch sum lengths = torch repeat_interleave y lengths dim= assertEqual dtype y dtype assertEqual size torch Size a_with_output = torch repeat_interleave y lengths dim= output_size=output_size assertEqual a_with_output dtype y dtype assertEqual a_with_output size torch Size test_repeat_interleave device= mps x = torch tensor device=device expected = torch tensor device=device assertEqual torch repeat_interleave x expected assertRaises RuntimeError torch repeat_interleave torch arange device=device reshape assertRaises RuntimeError torch repeat_interleave torch arange device=device assertRaises RuntimeError torch repeat_interleave torch tensor - device=device y = torch tensor device=device y _v = torch repeat_interleave y y _v = torch repeat_interleave y torch tensor device=device y _v = torch repeat_interleave y torch tensor device=device y _expect = torch tensor device=device assertEqual y _v y _expect assertEqual y _v y _expect assertEqual y _v y _expect y = torch repeat_interleave y dim= y _expect = torch tensor device=device assertEqual y y _expect y = torch repeat_interleave y torch tensor device=device dim= y _expect = torch tensor device=device assertEqual y y _expect assertRaises RuntimeError torch repeat_interleave y torch tensor device=device dim= assertRaises RuntimeError torch repeat_interleave y torch arange device=device reshape dim= test zero sized dimension x = torch zeros device=device y = torch repeat_interleave x repeats= dim= assertEqual y x new_zeros device=device x = torch tensor dtype=torch int device=device y = torch repeat_interleave x x assertEqual y x test_repeat_interleave_simple helper shape dtype=torch float num_repeats=torch Tensor dim=None x = torch randn shape dtype=dtype device= mps x_cpu = x detach clone cpu num_repeats_cpu = num_repeats detach clone cpu repeats = torch repeat_interleave x num_repeats dim repeats_cpu = torch repeat_interleave x_cpu num_repeats_cpu dim assertEqual repeats repeats_cpu helper shape= num_repeats=torch tensor device= mps helper shape= num_repeats=torch tensor device= mps dim= helper shape= num_repeats=torch arange device= mps dim= helper shape= num_repeats=torch randint device= mps dim= helper shape= num_repeats=torch randint device= mps dim= test_count_nonzero helper dtype n = - cpu_x = torch tensor n dtype=dtype mps_x = torch tensor n dtype=dtype mps All non-zeros assertEqual torch count_nonzero cpu_x torch count_nonzero mps_x dim= assertEqual torch count_nonzero cpu_x dim= torch count_nonzero mps_x dim= dim= assertEqual torch count_nonzero cpu_x dim= torch count_nonzero mps_x dim= helper torch int helper torch int helper torch float helper torch float _test_module_empty_input module inp check_size=True inp requires_grad_ True out = module inp gO = torch rand_like out out backward gO check_size assertEqual out size inp size p module parameters p requires_grad assertEqual p grad torch zeros_like p grad assertEqual inp grad torch zeros_like inp Test dtype casting without simultaneous device change test_to values = cpu_x = torch tensor values device= cpu mps_x = torch tensor values device= mps assertEqual cpu_x int mps_x int cpu assertEqual cpu_x bool mps_x bool cpu assertEqual cpu_x float mps_x float cpu assertEqual torch tensor device= mps int cpu torch tensor dtype=torch int assertEqual torch tensor device= mps bool cpu torch tensor False assertEqual torch tensor device= mps bool cpu torch tensor True assertEqual torch tensor device= mps bool int cpu torch tensor dtype=torch int assertEqual torch tensor device= mps bool int float cpu torch tensor assertEqual torch tensor device= mps cpu torch int torch tensor dtype=torch int assertEqual torch tensor device= cpu mps torch int cpu torch tensor dtype=torch int assertEqual torch tensor - device= cpu mps torch int torch tensor - device= cpu mps torch int Cast int uint float compare results See https github com pytorch pytorch issues more details cpu_byte = torch tensor dtype=torch uint cpu_char = torch tensor - - dtype=torch uint x_cpu cpu_byte cpu_char x_mps = x_cpu mps assertEqual x_mps torch float x_cpu torch float test_setitem_scalar - None device = mps dtype torch int torch float torch int i range j range t = torch zeros i j dtype=dtype device=device assertEqual t sum t = t = j t = i assertEqual t assertEqual t i assertEqual t j assertEqual t sum + i + j test_stride_of_strides - None x = torch rand device= mps y = x as_strided size= stride= Casting stride strided tensor CPU use crash buffer large enough assert See https github com pytorch pytorch issues #issuecomment- z = y as_strided size= stride= cpu assertEqual x cpu as_strided size= stride= z test_type_casting https github com pytorch pytorch issues helper data to_dtype a_cpu = torch tensor data a_mps = a_cpu torch device mps res_cpu = a_cpu type to_dtype res_mps = a_mps type to_dtype assertEqual res_cpu res_mps helper torch LongTensor helper torch FloatTensor helper torch IntTensor helper torch ShortTensor helper torch HalfTensor helper torch CharTensor helper torch ByteTensor test_to_casting https github com pytorch pytorch issues helper data to_dtype a_cpu = torch tensor data a_mps = a_cpu torch device mps res_cpu = a_cpu to_dtype res_mps = a_mps to_dtype assertEqual res_cpu res_mps helper torch int helper torch float helper torch int helper torch short helper torch half helper torch int helper torch uint test_storage_offset_greater_than_src_nbytes https github com pytorch pytorch issues n_tensors = n_tensor_elems = elems = torch arange n_tensors n_tensor_elems dtype=torch float tensor_list = i range n_tensors - create list contiguous view tensors view tensor created slice op t = elems n_tensor_elems i n_tensor_elems i + tensor_list append t i range n_tensors - t = tensor_list i view n_tensor_elems t_mps = t mps assertEqual t t_mps cpu f i= i See https github com pytorch pytorch issues https github com pytorch pytorch issues test_full_bugs Test should crash x = torch full True device= mps torch full should work uint y_mps = torch full device= mps dtype=torch uint y_cpu = torch full device= cpu dtype=torch uint assertEqual y_mps y_cpu test_div_bugs dtype mode itertools product integral_types trunc floor x = torch tensor list range device= mps dtype=dtype y = torch div x rounding_mode=mode assertEqual y sum See https github com pytorch pytorch issues test_bool_expand x = torch tensor dtype=torch bool device= mps y = torch tensor dtype=torch bool device= mps assertFalse torch equal x expand y expand test_int_expand x = torch tensor dtype=torch int device= mps y = torch tensor dtype=torch int device= mps assertFalse torch equal x expand y expand Empty unary op should tensor same size test_empty_neg x = torch tensor device= mps y = -x assertEqual x y _test_unique_scalar_empty dtype device f test scalar x = torch tensor dtype=dtype device=device unique inverse counts = f x return_inverse=True return_counts=True expected_unique = torch tensor dtype=dtype device=device expected_inverse = torch tensor device=device expected_counts = torch tensor device=device assertEqual unique expected_unique assertEqual inverse expected_inverse assertEqual counts expected_counts test zero sized tensor x = torch zeros dtype=dtype device=device unique inverse counts = f x return_inverse=True return_counts=True expected_unique = torch tensor dtype=dtype device=device expected_inverse = torch empty dtype=torch long device=device expected_counts = torch tensor dtype=torch long device=device assertEqual unique expected_unique assertEqual inverse expected_inverse assertEqual counts expected_counts _test_unique_with_expects device dtype f x expected_unique expected_inverse expected_counts additional_shape ensure_tuple x isinstance x torch Tensor x x return_inverse True False return_counts True False test expected ret = ensure_tuple f x return_inverse=return_inverse return_counts=return_counts assertEqual len ret + int return_inverse + int return_counts assertEqual expected_unique ret return_inverse assertEqual expected_inverse ret return_counts count_index = + int return_inverse assertEqual expected_counts ret count_index tests per-element unique higher rank tensor y = x view additional_shape y_unique y_inverse y_counts = f y return_inverse=True return_counts=True assertEqual expected_unique y_unique assertEqual expected_inverse view additional_shape y_inverse assertEqual expected_counts y_counts test_unique_all_dtypes device= mps helper dtype ensure_tuple x isinstance x torch Tensor x x dtype torch bool x = torch tensor True False False False True False True False dtype=torch bool device=device expected_unique = torch tensor False True dtype=torch bool device=device expected_inverse = torch tensor dtype=torch long device=device expected_counts = torch tensor dtype=torch long device=device x = torch tensor dtype=dtype device=device expected_unique = torch tensor dtype=dtype device=device expected_inverse = torch tensor device=device expected_counts = torch tensor device=device test sorted unique fs = lambda x kwargs torch unique x sorted=True kwargs lambda x kwargs x unique sorted=True kwargs x_sliced = torch empty x size dtype=dtype device=device copy_ x xs = x x_sliced f x product fs xs _test_unique_with_expects device dtype f x expected_unique expected_inverse expected_counts _test_unique_scalar_empty dtype device f test unsorted unique fs = lambda x kwargs torch unique x sorted=False kwargs lambda x kwargs x unique sorted=False kwargs f x product fs xs _test_unique_scalar_empty dtype device f return_inverse return_counts product True False repeat= ret = ensure_tuple f x return_inverse=return_inverse return_counts=return_counts assertEqual len ret + int return_inverse + int return_counts x_list = x tolist x_unique_list = ret tolist assertEqual expected_unique tolist sorted x_unique_list return_inverse x_inverse_list = ret tolist i j enumerate x_inverse_list assertEqual x_list i x_unique_list j return_counts count_index = + int return_inverse x_counts_list = ret count_index tolist i j zip x_unique_list x_counts_list count = k x_list k == i count += assertEqual j count helper dtype dtype torch float torch int torch int torch int torch uint test_unique helper x return_inverse return_counts cpu_x = x x = cpu_x detach clone mps result = torch unique x return_inverse=return_inverse return_counts=return_counts result_cpu = torch unique cpu_x return_inverse=return_inverse return_counts=return_counts assertEqual result result_cpu helper torch tensor False False helper torch randint False False helper torch randint True False helper torch randint False True helper torch randint True True helper torch randint True True helper torch randint True True Regression test https github com pytorch pytorch issues x = torch arange device= mps assertEqual x reshape unique x test_unique_consecutive helper x dim return_inverse return_counts cpu_x = x x = cpu_x detach clone mps result = torch unique_consecutive x dim=dim return_inverse=return_inverse return_counts=return_counts result_cpu = torch unique_consecutive cpu_x dim=dim return_inverse=return_inverse return_counts=return_counts assertEqual result result_cpu helper torch tensor False False helper torch randint False False helper torch randint True False helper torch randint False True helper torch randint True True helper torch randint True True helper torch randint True True helper torch randint True True helper torch tensor False False helper torch tensor True True helper torch randint True True helper torch randint True True helper torch randint True True helper torch tensor False False helper torch tensor True True helper torch randint True True helper torch randint True True helper torch randint True True See https github com pytorch pytorch issues test_cat_non_contiguous rotate_subset data dim x = data x = data assertFalse x is_contiguous assertFalse x is_contiguous torch concat x x dim=dim dtype MPS_DTYPES dtype == torch bool continue data = torch arange dtype=dtype reshape data = data memory_format=torch channels_last mps_data = data mps assertEqual data mps_data dim range data dim cpu_result = rotate_subset data dim mps_result = rotate_subset mps_data dim assertEqual cpu_result mps_result cpu TODO enable memory format test assertEqual cpu_result is_contiguous mps_result is_contiguous Skip test needs more memory than system has _skip_if_exceeds_total_memory required_memory total_memory required_memory skipTest f Needs required_memory f GiB RAM f only total_memory f GiB available parametrize dtype MPS_DTYPES test_cat_large_tensor dtype a_shape = + b_shape = Assume up extra overhead memory might required required_memory = math prod a_shape + math prod a_shape dtype itemsize _skip_if_exceeds_total_memory required_memory a_cpu = make_tensor dtype=dtype device= cpu expand a_shape b_cpu = make_tensor b_shape dtype=dtype device= cpu r_cpu = torch cat a_cpu b_cpu dim= Pick subset output elements compare because comparing all them takes too long rand_indices = torch randint a_cpu shape + b_cpu shape _ r_cpu_part = r_cpu rand_indices clone r_cpu_part = r_cpu - clone r_cpu_part = r_cpu clone Delete CPU result free up memory MPS run del r_cpu a_mps = torch empty dtype=dtype device= mps set_ a_cpu untyped_storage mps as_strided size=a_cpu size stride=a_cpu stride b_mps = b_cpu mps try r_mps = torch cat a_mps b_mps dim= except RuntimeError e Invalid buffer size str e skipTest f Exceeds max buffer size MPS str e raise e assertEqual r_mps rand_indices r_cpu_part assertEqual r_mps - r_cpu_part assertEqual r_mps r_cpu_part test_large_tensor_to_string shape = Assume up extra overhead memory might required required_memory = math prod shape _skip_if_exceeds_total_memory required_memory assertEqual str torch ones shape dtype=torch int device= mps tensor \n device= mps dtype=torch int See https github com pytorch pytorch issues test_jacfwd_cat fn x y torch cat x y x = torch rand device= mps y = torch rand device= mps rc = torch func jacfwd fn x y assertEqual rc shape See https github com pytorch pytorch issues test_from_numpy_non_contiguous = np arange reshape t_cpu = torch tensor device= cpu t_mps = torch tensor device= mps assertEqual t_cpu t_mps cpu See https github com pytorch pytorch issues test_copy_non_contiguous x = torch arange reshape permute assertFalse x is_contiguous y = x mps assertFalse y is_contiguous assertEqual x y cpu x = torch arange reshape permute y = x mps assertEqual x y cpu x = torch full device= cpu y = torch full device= mps z = torch arange reshape permute x permute = z As y MPS z CPU dispatches copy operator y permute = z assertEqual x y cpu See https github com pytorch pytorch issues test_copy_storage_offset x_cpu = torch zeros device= cpu dtype=torch float x_mps = torch zeros device= mps dtype=torch float update_cpu = torch tensor device= cpu dtype=torch int update_mps = torch tensor device= mps dtype=torch int x_cpu = update_cpu x_mps = update_mps implicit type casting copy assertEqual x_cpu x_mps x_cpu = update_mps implicit device moving copy assertEqual x_cpu x_mps test_copy_broadcasting helper src_shape dst_shape src_dtype dst_dtype cpu_src = torch randint src_shape src_dtype cpu_dst = torch randint dst_shape dst_dtype cpu_result = cpu_dst copy_ cpu_src mps_src = cpu_src mps mps_dst = cpu_dst mps mps_result = mps_dst copy_ mps_src assertEqual cpu_result mps_result test_dtypes = torch float torch int torch int torch int src_dtype dst_dtype itertools product test_dtypes test_dtypes helper src_dtype dst_dtype helper src_dtype dst_dtype helper src_dtype dst_dtype helper src_dtype dst_dtype helper src_dtype dst_dtype helper src_dtype dst_dtype helper src_dtype dst_dtype helper src_dtype dst_dtype helper src_dtype dst_dtype helper src_dtype dst_dtype Regression test https github com pytorch pytorch issues assertEqual torch tensor device= mps item See https github com pytorch pytorch pull https github com pytorch pytorch pull parametrize binop add sub mul div test_binops_dtype_precedence binop Test dtype precedence casting order binary operations comparing CPU result Example values all dtypes supported MPS backend sample_vals = torch bool False True torch int - torch int - torch int - torch float - torch float - Test all combinations dtypes operations dimensionality dtype dtype itertools product sample_vals repeat= bool minus bool generally unsupported so skip binop == sub dtype == torch bool dtype == torch bool continue full_shape = val val itertools product sample_vals dtype sample_vals dtype print f dtype dtype val binop val print getattr torch tensor val dtype=dtype device= mps binop torch tensor val dtype=dtype device= mps print getattr torch tensor val dtype=dtype device= cpu binop torch tensor val dtype=dtype device= cpu assertEqual getattr torch tensor val dtype=dtype device= mps binop torch tensor val dtype=dtype device= mps getattr torch tensor val dtype=dtype device= cpu binop torch tensor val dtype=dtype device= cpu assertEqual getattr torch tensor val dtype=dtype device= mps binop torch tensor val dtype=dtype device= mps getattr torch tensor val dtype=dtype device= cpu binop torch tensor val dtype=dtype device= cpu assertEqual getattr torch tensor val dtype=dtype device= mps binop torch tensor val dtype=dtype device= mps getattr torch tensor val dtype=dtype device= cpu binop torch tensor val dtype=dtype device= cpu assertEqual getattr torch tensor val dtype=dtype device= mps binop torch tensor val dtype=dtype device= mps getattr torch tensor val dtype=dtype device= cpu binop torch tensor val dtype=dtype device= cpu Test tensors created torch full x = torch full full_shape val dtype=dtype device= mps y = torch tensor val dtype=dtype device= mps x = torch full full_shape val dtype=dtype device= cpu y = torch tensor val dtype=dtype device= cpu assertEqual getattr x binop y getattr x binop y x = torch tensor val dtype=dtype device= mps y = torch full full_shape val dtype=dtype device= mps x = torch tensor val dtype=dtype device= cpu y = torch full full_shape val dtype=dtype device= cpu assertEqual getattr x binop y getattr x binop y assertEqual getattr torch tensor val dtype=dtype device= mps binop torch full full_shape val dtype=dtype device= mps getattr torch tensor val dtype=dtype device= cpu binop torch full full_shape val dtype=dtype device= cpu test_xor_non_contigous See https github com pytorch pytorch issues x_mps = torch randint - dtype=torch int device= mps x_cpu = x_mps detach cpu x_mps ^= x_cpu ^= assertEqual x_mps cpu x_cpu test_nansum helper dtype noncontiguous dim zero_cpu = torch zeros dtype=dtype Randomly scale values scale = random randint x_cpu torch Tensor = make_tensor dtype=dtype device= cpu low=-scale high=scale noncontiguous=noncontiguous dtype is_floating_point nan_mask_cpu = x_cpu scale x_no_nan_cpu = torch where nan_mask_cpu zero_cpu x_cpu x_cpu nan_mask_cpu = np nan x_no_nan_cpu = x_cpu x_mps = x_cpu mps actual_out_mps = torch empty dtype=dtype device= mps expect_out_cpu = torch empty dtype=dtype dim_kwargs = dim dim dim None expect = torch sum x_no_nan_cpu dim_kwargs actual_cpu = torch nansum x_cpu dim_kwargs Sanity check CPU assertEqual expect actual_cpu Test MPS actual_mps = torch nansum x_mps dim_kwargs Test out= variant torch nansum x_mps out=actual_out_mps dim_kwargs torch nansum x_cpu out=expect_out_cpu dim_kwargs assertEqual expect actual_mps assertEqual expect_out_cpu actual_out_mps args = itertools product torch float torch float torch int torch int dtype True False noncontiguous None dim dtype noncontiguous dim args subTest dtype=dtype noncontiguous=noncontiguous dim=dim helper dtype noncontiguous dim test_cumsum_all_dtypes helper dtype t = torch tensor device= mps dtype=dtype t_cpu = torch tensor device= cpu = t cumsum dtype=dtype a_cpu = t_cpu cumsum dtype=dtype assertEqual cpu a_cpu helper dtype dtype torch int torch int torch int torch int torch float test_cumsum_bool = torch ones dtype=torch bool t_cpu = cumsum t_mps = mps cumsum assertEqual t_cpu t_mps test_cumsum_minus_one_axis helper dtype Test axis - cpu_x = None dtype == torch float cpu_x = torch randn device= cpu dtype=torch float cpu_x = torch randint device= cpu dtype=torch float x = cpu_x detach clone mps cpu_y = cpu_x cumsum - y = x cumsum - assertEqual y cpu_y helper dtype dtype torch float torch int torch int torch uint test_cumprod_all_dtypes helper dtype t = torch tensor device= mps dtype=dtype t_cpu = torch tensor device= cpu = t cumprod dtype=dtype a_cpu = t_cpu cumprod dtype=dtype assertEqual cpu a_cpu helper dtype dtype torch int torch int torch int torch int torch float test_cumprod_minus_one_axis helper dtype Test axis - cpu_x = None dtype == torch float cpu_x = torch randn device= cpu dtype=torch float cpu_x = torch randint device= cpu dtype=torch float x = cpu_x detach clone mps cpu_y = cpu_x cumprod - y = x cumprod - assertEqual y cpu_y helper dtype dtype torch float torch int torch int torch uint test_median_int helper shape dtype cpu_x = torch randint - shape device= cpu dtype=dtype x = cpu_x detach clone mps median_result = torch median x median_result_cpu = torch median cpu_x assertEqual median_result median_result_cpu helper torch int test_activation_checkpoint_does_not_error torch utils checkpoint checkpoint use_reentrant True False = torch tensor device= mps requires_grad=True fn x x sin cos exp out = checkpoint fn use_reentrant=use_reentrant out backward test_as_strided values = values_ = cpu_x = torch tensor values device= cpu ones = torch tensor values_ device= mps x = cpu_x detach clone mps requires_grad_ strided_cpu = torch as_strided cpu_x strided_mps = torch as_strided x assertEqual strided_mps strided_cpu strided_cpu_out = strided_cpu + ones cpu strided_mps_out = strided_mps + ones assertEqual strided_cpu_out strided_mps_out test storage offsets cpu_x = torch rand device= cpu mps_x = cpu_x mps strided_cpu = torch as_strided cpu_x strided_mps = torch as_strided mps_x strided_cpu = torch as_strided cpu_x strided_mps = torch as_strided mps_x strided_cpu_out = strided_cpu - strided_cpu strided_mps_out = strided_mps - strided_mps assertEqual strided_cpu_out strided_mps_out test_unfold x = torch arange x_mps = torch arange device= mps y = x unfold y_mps = x_mps unfold assertEqual y y_mps test_unfold_all_devices_and_dtypes supported_dtypes = torch float torch float torch int torch int torch int torch uint dt supported_dtypes x = torch empty dtype=dt device= mps assertEqual x unfold shape test_unfold_scalars x = torch tensor device= mps unfold -dimensional tensor should always -d dimensional tensor shape size i e second parameter unfold assertEqual torch empty device= mps x unfold assertEqual torch empty device= mps x unfold assertEqual torch tensor device= mps x unfold test_bincount_simple input = torch randint dtype=torch int device= mps input_cpu = input cpu weights = torch linspace steps= device= mps dtype=torch float weights_cpu = weights cpu x = torch bincount input x_cpu = torch bincount input_cpu assertEqual x x_cpu y = input bincount weights y_cpu = input_cpu bincount weights_cpu assertEqual y y_cpu test_bincount_reduction device = mps negative input throws assertRaisesRegex RuntimeError -d non-negative integral torch bincount torch tensor - device=device dtype=torch int n-d input n throws assertRaisesRegex RuntimeError -d non-negative integral torch bincount torch tensor device=device minlength throws assertRaisesRegex RuntimeError minlength should = torch bincount torch tensor device=device torch tensor device=device minlength=- n-d weights n throws assertRaisesRegex RuntimeError -d torch bincount torch tensor device=device dtype=torch int torch tensor device=device dtype=torch float input weights dim mismatch assertRaisesRegex RuntimeError same length torch bincount torch tensor device=device dtype=torch int torch tensor device=device dtype=torch float -d input no elements default minlength assertEqual torch bincount torch tensor device=device dtype=torch long torch zeros dtype=torch long device=device -d input no elements specified minlength assertEqual torch bincount torch tensor device=device dtype=torch long minlength= torch zeros dtype=torch long device=device test tensor method without weights long_counts = torch tensor dtype=torch uint device=device bincount assertEqual torch tensor dtype=torch int device=device long_counts test avoiding overflow uint count_uint = torch tensor dtype=torch uint device=device bincount count_int = torch tensor dtype=torch int device=device bincount assertEqual count_uint count_int test minlength functionality int_counts = torch bincount torch tensor device=device dtype=torch int minlength= assertEqual torch tensor dtype=torch int device=device int_counts test weights byte_counts = torch bincount torch tensor device=device dtype=torch int torch tensor device=device assertEqual torch tensor device=device byte_counts byte_counts = torch bincount torch tensor device=device dtype=torch int torch tensor dtype=torch int device=device assertEqual torch tensor device=device dtype=torch int byte_counts test non-contiguous inputs weights inputs = torch tensor device=device dtype=torch int weights = torch tensor device=device i assert inputs i is_contiguous Inputs supposed non-contiguous assert weights i is_contiguous Weights supposed non-contiguous inputs non-contiguous weights contiguous assertEqual inputs bincount torch tensor inputs weights non-contiguous assertEqual inputs bincount weights torch tensor dtype=torch float weights non-contiguous inputs contiguous assertEqual inputs contiguous bincount weights torch tensor dtype=torch float test bincount non-contiguous slices all s = torch zeros dtype=torch int device=device assertEqual all s bincount torch tensor all s = torch ones dtype=torch int device=device assertEqual all s bincount torch tensor test large number bins - global memory use big_exp = torch zeros device=device big_exp - = big_w = torch tensor device=device big_out = torch tensor device=device dtype=torch int bincount big_w assertEqual big_exp big_out test large input size big_exp = torch zeros device=device dtype=torch int big_exp = big_out = torch ones dtype=torch int device=device bincount assertEqual big_exp big_out test_bincount device = mps input_size = w = torch randn input_size dtype=torch float device=device w_cpu = w cpu t = torch randint input_size dtype=torch int device=device assertEqual t cpu bincount t bincount assertEqual t cpu bincount w_cpu t bincount w t = torch randint input_size dtype=torch int device=device assertEqual t cpu bincount t bincount assertEqual t cpu bincount w_cpu t bincount w t = torch randint input_size dtype=torch int device=device assertEqual t cpu bincount t bincount assertEqual t cpu bincount w_cpu t bincount w t = torch zeros dtype=torch int device=device t = counted = t bincount minlength= assertEqual torch sum counted test_sum_backward helper n c values = cpu_x = torch tensor values device= cpu requires_grad=True x = cpu_x detach clone mps requires_grad_ all_sum = torch sum x all_sum_cpu = torch sum cpu_x all_sum backward all_sum_cpu backward assertEqual all_sum all_sum_cpu assertEqual x grad cpu_x grad helper L loss test_l _loss helper shape reduction create criterion loss = torch nn L Loss reduction=reduction inputCPU = torch randn shape device= cpu dtype=torch float requires_grad=True targetCPU = torch randn shape device= cpu dtype=torch float requires_grad=False inputMPS = inputCPU detach clone mps requires_grad_ targetMPS = targetCPU detach clone mps forward pass outputCPU = loss inputCPU targetCPU outputMPS = loss inputMPS targetMPS assertEqual outputCPU outputMPS backward pass reduction = none chose just make grad_output backward pass outputCPU backward gradient=torch full_like outputCPU outputMPS backward gradient=torch full_like outputMPS assertEqual inputCPU grad inputMPS grad helper none helper sum verify changes shape would cause cached graph lookup problems helper sum helper mean Mean Squared Error test_mse_loss helper shape reduction create criterion loss = torch nn MSELoss reduction=reduction inputCPU = torch randn shape device= cpu dtype=torch float requires_grad=True targetCPU = torch randn shape device= cpu dtype=torch float requires_grad=False inputMPS = inputCPU detach clone mps requires_grad_ targetMPS = targetCPU detach clone mps forward pass outputCPU = loss inputCPU targetCPU outputMPS = loss inputMPS targetMPS assertEqual outputCPU outputMPS backward pass reduction = none chose just make grad_output backward pass outputCPU backward gradient=torch full_like outputCPU outputMPS backward gradient=torch full_like outputMPS assertEqual inputCPU grad inputMPS grad helper none helper sum verify changes shape would cause cached graph lookup problems helper sum helper mean helper sum helper mean helper none test_mse_loss_strided_output https github com pytorch pytorch issues lf = nn MSELoss reduction= none model_cpu = nn Sequential nn Conv d model_mps = copy deepcopy model_cpu mps x = torch randn x = x permute x_mps = x detach clone mps permute x_mps = x_mps permute y = model_cpu x y_mps = model_mps x_mps y = y permute y_mps = y_mps permute y_hat = torch randn y_hat_mps = y_hat detach clone mps loss = lf y y_hat loss_mps = lf y_mps y_hat_mps assertEqual loss loss_mps test_mse_loss_unsupported_types loss = nn MSELoss dtype MPS_DTYPES a_mps = torch tensor dtype=dtype device= mps a_cpu = torch tensor dtype=dtype device= cpu dtype is_floating_point assertEqual loss a_mps a_mps loss a_cpu a_cpu continue assertRaises RuntimeError lambda loss a_mps a_mps assertRaises RuntimeError lambda loss a_cpu a_cpu Binary Cross Enropy test_bce_loss_simple helper shape reduction create criterion loss = torch nn BCELoss reduction=reduction input target must within input_t = np random random_sample size=shape astype np float target_t = np random random_sample size=shape astype np float inputCPU = torch tensor input_t device= cpu dtype=torch float requires_grad=True targetCPU = torch tensor target_t device= cpu dtype=torch float requires_grad=False inputMPS = inputCPU detach clone mps requires_grad_ targetMPS = targetCPU detach clone mps forward pass outputCPU = loss inputCPU targetCPU outputMPS = loss inputMPS targetMPS assertEqual outputCPU outputMPS backward pass reduction = none chose just have grad_output = outputCPU backward gradient=torch full_like outputCPU outputMPS backward gradient=torch full_like outputMPS assertEqual inputCPU grad inputMPS grad helper none helper sum verify changes shape would cause cached graph lookup problems helper sum helper mean helper mean test_bce_loss_always_nonnegative target = torch ones device= mps input = torch ones device= mps assertEqual nn BCELoss input target sum target = torch zeros device= mps input = torch zeros device= mps assertEqual nn BCELoss input target sum test_bce_loss_size_mismatch bceloss = nn BCELoss = torch rand device= mps b = torch rand device= mps assertRaisesRegex ValueError r Using target size \ bceloss b test_bce_with_logits_gives_same_result_as_sigmoid_and_bce_loss_large_tensors_with_grad x_size = y_size = target = torch rand x_size y_size device= mps reduction none mean sum output_sig = torch rand x_size y_size device= mps - output_logits = output_sig detach clone output_sig requires_grad = True output_logits requires_grad = True weight = torch rand y_size device= mps loss_sig = nn BCELoss weight reduction=reduction torch sigmoid output_sig target loss_logits = nn BCEWithLogitsLoss weight reduction=reduction output_logits target assertEqual loss_logits loss_sig reduction == none grad = torch rand x_size y_size device= mps loss_sig backward grad loss_logits backward grad loss_sig backward loss_logits backward assertEqual output_sig grad output_logits grad test_bce_with_logits_has_correct_grad_at_zero output = torch zeros requires_grad=True device= mps target = torch zeros device= mps nn BCEWithLogitsLoss reduction= sum output target backward expected_grad = torch empty device= mps fill_ assertEqual output grad expected_grad test_bce_with_logits_broadcasts_weights target = torch rand device= mps output = torch rand device= mps - weight = torch rand device= mps out = nn BCEWithLogitsLoss weight output target weight = weight expand contiguous out = nn BCEWithLogitsLoss weight output target assertEqual out out weight = torch rand device= mps out = nn BCEWithLogitsLoss weight output target weight = weight expand contiguous out = nn BCEWithLogitsLoss weight output target assertEqual out out test_bce_with_logits_ones_in_pos_weights_are_the_same_as_none target = torch rand device= mps output = torch rand device= mps - pos_weight = torch ones device= mps assertEqual nn BCEWithLogitsLoss output target nn BCEWithLogitsLoss pos_weight=pos_weight output target test_bce_with_logits_broadcasts_pos_weights target = torch rand device= mps output = torch rand device= mps - pos_weight = torch rand device= mps out = nn BCEWithLogitsLoss pos_weight=pos_weight output target pos_weight = pos_weight expand out = nn BCEWithLogitsLoss pos_weight=pos_weight output target pos_weight = pos_weight expand out = nn BCEWithLogitsLoss pos_weight=pos_weight output target assertEqual out out assertEqual out out test_bce_with_logits_with_pos_weight_has_correct_grad_at_zero output = torch zeros requires_grad=True device= mps target = torch zeros device= mps pos_weight = torch ones device= mps nn BCEWithLogitsLoss pos_weight=pos_weight reduction= sum output target backward expected_grad = torch empty device= mps fill_ grad = output grad assertEqual grad expected_grad test_bce_with_logits_stability output = torch tensor - device= mps target = torch tensor device= mps pos_weight = torch tensor device= mps out = nn BCEWithLogitsLoss output target assertTrue torch isfinite out all item out = nn BCEWithLogitsLoss pos_weight=pos_weight output target assertTrue torch isfinite out all item test_bce_loss_broadcasts_weights sigmoid = nn Sigmoid target = torch rand device= mps output = torch rand device= mps - weight = torch rand device= mps out = nn BCELoss weight sigmoid output target weight = weight expand contiguous out = nn BCELoss weight sigmoid output target assertEqual out out weight = torch rand device= mps out = nn BCELoss weight sigmoid output target weight = weight expand contiguous out = nn BCELoss weight sigmoid output target assertEqual out out test_cross_entropy_loss Regression test https github com pytorch pytorch issues loss = nn CrossEntropyLoss pred = torch randn requires_grad=True dtype=torch float device= mps target = torch ones dtype=torch long device= mps output = loss pred target output backward test_log_softmax values = cpu_x = torch tensor values device= cpu requires_grad=True mps_x = torch tensor values device= mps requires_grad=True cpu_log_softmax = F log_softmax cpu_x dim= mps_log_softmax = F log_softmax mps_x dim= assertEqual cpu_log_softmax mps_log_softmax cpu cpu_grad = torch ones_like cpu_log_softmax mps_grad = torch ones_like cpu_log_softmax mps cpu_log_softmax backward gradient=cpu_grad mps_log_softmax backward gradient=mps_grad assertEqual cpu_x grad mps_x grad cpu test_log_softmax_large_numbers values = - - - - - - cpu_x = torch tensor values device= cpu requires_grad=True mps_x = torch tensor values device= mps requires_grad=True cpu_log_softmax = F log_softmax cpu_x dim=- mps_log_softmax = F log_softmax mps_x dim=- assertEqual cpu_log_softmax mps_log_softmax cpu cpu_grad = torch ones_like cpu_log_softmax mps_grad = torch ones_like cpu_log_softmax mps cpu_log_softmax backward gradient=cpu_grad mps_log_softmax backward gradient=mps_grad assertEqual cpu_x grad mps_x grad cpu test_eq values = values = mps_x = torch tensor values device= mps mps_y = torch tensor values device= mps cpu_x = torch tensor values device= cpu cpu_y = torch tensor values device= cpu result_mps = torch eq mps_x mps_y result_cpu = torch eq cpu_x cpu_y assertEqual result_cpu result_mps cpu test_signed_vs_unsigned_comparison cpu_x = torch tensor - device= cpu dtype=torch uint mps_x = torch tensor - device= mps dtype=torch uint comparison signed vs unsigned we should always cast unsigned assertEqual cpu_x == - mps_x == - assertEqual cpu_x - mps_x - assertEqual cpu_x - mps_x - test_eq_int values = values = mps_x = torch tensor values device= mps mps_y = torch tensor values device= mps cpu_x = torch tensor values device= cpu cpu_y = torch tensor values device= cpu result_mps = torch eq mps_x mps_y result_cpu = torch eq cpu_x cpu_y assertEqual result_cpu result_mps cpu test_ne helper shape cpu_x = torch randn shape device= cpu dtype=torch float cpu_y = torch randn shape device= cpu dtype=torch float mps_x = cpu_x detach clone mps mps_y = cpu_y detach clone mps result_mps = torch ne mps_x mps_y result_cpu = torch ne cpu_x cpu_y assertEqual result_cpu result_mps cpu helper test_ne_scalar helper shape cpu_x = torch randn shape device= cpu dtype=torch float mps_x = cpu_x detach clone mps result_mps = torch ne mps_x result_cpu = torch ne cpu_x assertEqual result_cpu result_mps cpu helper test_lt helper shape cpu_x = torch randn shape device= cpu dtype=torch float cpu_y = torch randn shape device= cpu dtype=torch float mps_x = cpu_x detach clone mps mps_y = cpu_y detach clone mps result_mps = torch lt mps_x mps_y result_cpu = torch lt cpu_x cpu_y assertEqual result_cpu result_mps cpu helper test_lt_scalar helper shape cpu_x = torch randn shape device= cpu dtype=torch float mps_x = cpu_x detach clone mps result_mps = torch lt mps_x result_cpu = torch lt cpu_x assertEqual result_cpu result_mps cpu helper test_le helper shape cpu_x = torch randn shape device= cpu dtype=torch float cpu_y = torch randn shape device= cpu dtype=torch float mps_x = cpu_x detach clone mps mps_y = cpu_y detach clone mps result_mps = torch le mps_x mps_y result_cpu = torch le cpu_x cpu_y assertEqual result_cpu result_mps cpu helper test_le_scalar helper shape cpu_x = torch randn shape device= cpu dtype=torch float mps_x = cpu_x detach clone mps result_mps = torch le mps_x result_cpu = torch le cpu_x assertEqual result_cpu result_mps cpu helper test_ge helper shape cpu_x = torch randn shape device= cpu dtype=torch float cpu_y = torch randn shape device= cpu dtype=torch float mps_x = cpu_x detach clone mps mps_y = cpu_y detach clone mps result_mps = torch ge mps_x mps_y result_cpu = torch ge cpu_x cpu_y assertEqual result_cpu result_mps cpu helper test_ge_scalar helper shape cpu_x = torch randn shape device= cpu dtype=torch float mps_x = cpu_x detach clone mps result_mps = torch ge mps_x result_cpu = torch ge cpu_x assertEqual result_cpu result_mps cpu helper test_gt helper shape cpu_x = torch randn shape device= cpu dtype=torch float cpu_y = torch randn shape device= cpu dtype=torch float mps_x = cpu_x detach clone mps mps_y = cpu_y detach clone mps result_mps = torch gt mps_x mps_y result_cpu = torch gt cpu_x cpu_y assertEqual result_cpu result_mps cpu helper test_gt_scalar helper shape cpu_x = torch randn shape device= cpu dtype=torch float mps_x = cpu_x detach clone mps result_mps = torch gt mps_x result_cpu = torch gt cpu_x assertEqual result_cpu result_mps cpu helper test_argmax https github com pytorch pytorch issues cpu_tensor = torch tensor res_cpu = torch argmax cpu_tensor dim= mps_tensor = cpu_tensor torch device mps res_mps = torch argmax mps_tensor dim= assertEqual res_cpu res_mps https github com pytorch pytorch issues mps_tensor = torch randn device= mps dtype=torch float cpu_tensor = mps_tensor detach clone cpu res_mps = torch argmax mps_tensor dim= res_cpu = torch argmax cpu_tensor dim= assertEqual res_cpu res_mps Test forward argmin argmax test_argmin_argmax helper n c h w reduction_type dtype=torch float reduction_type == max arg_reduction_fn = torch argmax arg_reduction_fn = torch argmin cpu_x = None x = None dtype torch float torch bool cpu_x = torch randint n c h w device= cpu dtype=dtype requires_grad=False x = cpu_x detach clone mps dtype == torch bool cpu_x = torch randint n c h w device= cpu dtype=dtype requires_grad=False x = cpu_x detach clone mps cpu_x = torch randn n c h w device= cpu dtype=dtype requires_grad=True x = cpu_x detach clone mps requires_grad_ y = arg_reduction_fn x ref_y = arg_reduction_fn cpu_x assertEqual y ref_y y_ = arg_reduction_fn x dim= refy_ = arg_reduction_fn cpu_x dim= assertEqual y_ refy_ y_ dim = arg_reduction_fn x dim= keepdim=True refy_ dim = arg_reduction_fn cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim y_ = arg_reduction_fn x dim= refy_ = arg_reduction_fn cpu_x dim= assertEqual y_ refy_ y_ dim = arg_reduction_fn x dim= keepdim=True refy_ dim = arg_reduction_fn cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim y_ = arg_reduction_fn x dim= refy_ = arg_reduction_fn cpu_x dim= assertEqual y_ refy_ y_ dim = arg_reduction_fn x dim= keepdim=True refy_ dim = arg_reduction_fn cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim y_ = arg_reduction_fn x dim= refy_ = arg_reduction_fn cpu_x dim= assertEqual y_ refy_ y_ dim = arg_reduction_fn x dim= keepdim=True refy_ dim = arg_reduction_fn cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim helper max torch float helper max torch int helper max torch float helper max torch int helper min torch float helper min torch int helper min torch float helper min torch int test_reduction_sum_max_long_val x_mps = torch tensor sys maxsize sys maxsize - sys maxsize - sys maxsize - device= mps x_cpu = x_mps detach clone cpu res_mps = torch sum x_mps res_cpu = torch sum x_cpu assertEqual res_mps res_cpu Test forward max Note - don t test grad now test_max_el helper n c h w dtype=torch float dtype torch float torch bool cpu_x = torch randint n c h w device= cpu dtype=dtype requires_grad=False x = cpu_x detach clone mps dtype == torch bool cpu_x = torch randint n c h w device= cpu dtype=dtype requires_grad=False x = cpu_x detach clone mps cpu_x = torch randn n c h w device= cpu dtype=dtype requires_grad=True x = cpu_x detach clone mps ref_y = torch max cpu_x y = torch max x assertEqual y ref_y dim keepdim True False y idx = torch max x dim=dim keepdim=keepdim refy refidx = torch max cpu_x dim=dim keepdim=keepdim assertEqual y refy assertEqual idx refidx y_ = torch ones c h w device= mps dtype=dtype idx_ = torch ones c h w device= mps dtype=torch int torch max x dim= out= y_ idx_ refy_ refidx_ = torch max cpu_x dim= assertEqual y_ refy_ assertEqual idx_ refidx_ y_ dim = torch ones c h w device= mps dtype=dtype idx_ dim = torch ones c h w device= mps dtype=torch int torch max x dim= keepdim=True out= y_ dim idx_ dim refy_ dim refidx_ dim = torch max cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim assertEqual idx_ dim refidx_ dim y_ = torch ones n h w device= mps dtype=dtype idx_ = torch ones n h w device= mps dtype=torch int torch max x dim= out= y_ idx_ refy_ refidx_ = torch max cpu_x dim= assertEqual y_ refy_ assertEqual idx_ refidx_ y_ dim = torch ones n h w device= mps dtype=dtype idx_ dim = torch ones n h w device= mps dtype=torch int torch max x dim= keepdim=True out= y_ dim idx_ dim refy_ dim refidx_ dim = torch max cpu_x keepdim=True dim= assertEqual y_ dim refy_ dim assertEqual idx_ dim refidx_ dim y_ = torch ones n c w device= mps dtype=dtype idx_ = torch ones n c w device= mps dtype=torch int torch max x dim= out= y_ idx_ refy_ refidx_ = torch max cpu_x dim= assertEqual y_ refy_ assertEqual idx_ refidx_ y_ dim = torch ones n c w device= mps dtype=dtype idx_ dim = torch ones n c w device= mps dtype=torch int torch max x dim= keepdim=True out= y_ dim idx_ dim refy_ dim refidx_ dim = torch max cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim assertEqual idx_ dim refidx_ dim y_ = torch ones n c h device= mps dtype=dtype idx_ = torch ones n c h device= mps dtype=torch int torch max x dim= out= y_ idx_ refy_ refidx_ = torch max cpu_x dim= assertEqual y_ refy_ assertEqual idx_ refidx_ y_ dim = torch ones n c h device= mps dtype=dtype idx_ dim = torch ones n c h device= mps dtype=torch int torch max x dim= keepdim=True out= y_ dim idx_ dim refy_ dim refidx_ dim = torch max cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim assertEqual idx_ dim refidx_ dim helper torch float helper torch int helper torch int test_median helper_dtype_int n n n cpu_x = torch randint n n n device= cpu dtype=torch int mps_x = cpu_x detach clone mps result_cpu = torch median cpu_x result_mps = torch median mps_x assertEqual result_cpu result_mps dim keepdim True False y idx = torch median cpu_x dim=dim keepdim=keepdim refy refidx = torch median mps_x dim=dim keepdim=keepdim assertEqual y refy assertEqual idx refidx helper_dtype_float n n n cpu_x = torch randn n n n device= cpu dtype=torch float mps_x = cpu_x detach clone mps result_cpu = torch median cpu_x result_mps = torch median mps_x assertEqual result_cpu result_mps dim keepdim True False y idx = torch median cpu_x dim=dim keepdim=keepdim refy refidx = torch median mps_x dim=dim keepdim=keepdim assertEqual y refy assertEqual idx refidx helper_dtype_int median even place helper_dtype_int median odd place helper_dtype_int helper_dtype_int helper_dtype_float helper_dtype_float helper_dtype_float test_any helper shape input_xs = prod = i range len shape prod = shape i input_xs append torch randn prod dtype=torch float reshape shape input_xs append torch arange prod dtype=torch float reshape shape input_xs append torch ones prod dtype=torch float reshape shape input_xs append torch zeros prod dtype=torch float reshape shape input_xs append torch arange prod dtype=torch int reshape shape input_xs append torch ones prod dtype=torch int reshape shape input_xs append torch zeros prod dtype=torch int reshape shape input_xs append torch arange prod dtype=torch int reshape shape bool input_xs append torch ones prod dtype=torch int reshape shape bool input_xs append torch zeros prod dtype=torch int reshape shape bool i cpu_x enumerate input_xs x = cpu_x detach clone mps y = torch any x ref_y = torch any cpu_x assertEqual y ref_y y_ = torch any x dim= refy_ = torch any cpu_x dim= assertEqual y_ refy_ y_ dim = torch any x dim= keepdim=True refy_ dim = torch any cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim y_ dim = torch any x dim= keepdim=True refy_ dim = torch any cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim y_ = torch any x dim= refy_ = torch any cpu_x dim= assertEqual y_ refy_ y_ dim = torch any x dim= keepdim=True refy_ dim = torch any cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim len shape y_ = torch any x dim= refy_ = torch any cpu_x dim= assertEqual y_ refy_ y_ dim = torch any x dim= keepdim=True refy_ dim = torch any cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim y_ = torch any x dim= refy_ = torch any cpu_x dim= assertEqual y_ refy_ y_ dim = torch any x dim= keepdim=True refy_ dim = torch any cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim helper helper helper helper test_reduction_ops_ D helper fn dim shape = x_cpu = fn torch zeros shape dim=dim x_mps = fn torch zeros shape device= mps dim=dim assertEqual x_cpu x_mps cpu fn torch any torch all dim range helper fn dim D tensor reductions Regression test https github com pytorch pytorch issues x = torch rand device= mps - relu assertEqual x all x cpu all i range - assertEqual x all dim=i x cpu all dim=i test_all helper shape input_xs = prod = i range len shape prod = shape i input_xs append torch randn prod dtype=torch float reshape shape input_xs append torch arange prod dtype=torch float reshape shape input_xs append torch ones prod dtype=torch float reshape shape input_xs append torch zeros prod dtype=torch float reshape shape input_xs append torch arange prod dtype=torch int reshape shape input_xs append torch ones prod dtype=torch int reshape shape input_xs append torch zeros prod dtype=torch int reshape shape input_xs append torch arange prod dtype=torch int reshape shape bool input_xs append torch ones prod dtype=torch int reshape shape bool input_xs append torch zeros prod dtype=torch int reshape shape bool cpu_x input_xs x = cpu_x detach clone mps y = torch all x ref_y = torch all cpu_x assertEqual y ref_y y_ = torch all x dim= refy_ = torch all cpu_x dim= assertEqual y_ refy_ y_ dim = torch all x dim= keepdim=True refy_ dim = torch all cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim y_ dim = torch all x dim= keepdim=True refy_ dim = torch all cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim y_ = torch all x dim= refy_ = torch all cpu_x dim= assertEqual y_ refy_ y_ dim = torch all x dim= keepdim=True refy_ dim = torch all cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim len shape y_ = torch all x dim= refy_ = torch all cpu_x dim= assertEqual y_ refy_ y_ dim = torch all x dim= keepdim=True refy_ dim = torch all cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim y_ = torch all x dim= refy_ = torch all cpu_x dim= assertEqual y_ refy_ y_ dim = torch all x dim= keepdim=True refy_ dim = torch all cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim helper helper helper helper Empty tensor x_cpu = torch tensor dtype=torch bool x_mps = x_cpu mps assertEqual x_cpu all x_mps all cpu Test forward min test_min_el helper n c h w cpu_x = torch randn n c h w device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps y = torch min x ref_y = torch min cpu_x assertEqual y ref_y y_ idx_ = torch min x dim= refy_ refidx_ = torch min cpu_x dim= assertEqual y_ refy_ assertEqual idx_ refidx_ y_ = torch ones c h w device= mps dtype=torch float idx_ = torch ones c h w device= mps dtype=torch int torch min x dim= out= y_ idx_ refy_ refidx_ = torch min cpu_x dim= assertEqual y_ refy_ assertEqual idx_ refidx_ y_ dim idx_ dim = torch min x dim= keepdim=True refy_ dim refidx_ dim = torch min cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim assertEqual idx_ dim refidx_ dim y_ dim = torch ones c h w device= mps dtype=torch float idx_ dim = torch ones c h w device= mps dtype=torch int torch min x dim= keepdim=True out= y_ dim idx_ dim refy_ dim refidx_ dim = torch min cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim assertEqual idx_ dim refidx_ dim y_ idx_ = torch min x dim= refy_ refidx_ = torch min cpu_x dim= assertEqual y_ refy_ assertEqual idx_ refidx_ y_ = torch ones n h w device= mps dtype=torch float idx_ = torch ones n h w device= mps dtype=torch int torch min x dim= out= y_ idx_ refy_ refidx_ = torch min cpu_x dim= assertEqual y_ refy_ assertEqual idx_ refidx_ y_ dim idx_ dim = torch min x dim= keepdim=True refy_ dim refidx_ dim = torch min cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim assertEqual idx_ dim refidx_ dim y_ dim = torch ones n h w device= mps dtype=torch float idx_ dim = torch ones n h w device= mps dtype=torch int torch min x dim= keepdim=True out= y_ dim idx_ dim refy_ dim refidx_ dim = torch min cpu_x keepdim=True dim= assertEqual y_ dim refy_ dim assertEqual idx_ dim refidx_ dim y_ idx_ = torch min x dim= refy_ refidx_ = torch min cpu_x dim= assertEqual y_ refy_ assertEqual idx_ refidx_ y_ = torch ones n c w device= mps dtype=torch float idx_ = torch ones n c w device= mps dtype=torch int torch min x dim= out= y_ idx_ refy_ refidx_ = torch min cpu_x dim= assertEqual y_ refy_ assertEqual idx_ refidx_ y_ dim idx_ dim = torch min x dim= keepdim=True refy_ dim refidx_ dim = torch min cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim assertEqual idx_ dim refidx_ dim y_ dim = torch ones n c w device= mps dtype=torch float idx_ dim = torch ones n c w device= mps dtype=torch int torch min x dim= keepdim=True out= y_ dim idx_ dim refy_ dim refidx_ dim = torch min cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim assertEqual idx_ dim refidx_ dim y_ idx_ = torch min x dim= refy_ refidx_ = torch min cpu_x dim= assertEqual y_ refy_ assertEqual idx_ refidx_ y_ = torch ones n c h device= mps dtype=torch float idx_ = torch ones n c h device= mps dtype=torch int torch min x dim= out= y_ idx_ refy_ refidx_ = torch min cpu_x dim= assertEqual y_ refy_ assertEqual idx_ refidx_ y_ dim idx_ dim = torch min x dim= keepdim=True refy_ dim refidx_ dim = torch min cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim assertEqual idx_ dim refidx_ dim y_ dim = torch ones n c h device= mps dtype=torch float idx_ dim = torch ones n c h device= mps dtype=torch int torch min x dim= keepdim=True out= y_ dim idx_ dim refy_ dim refidx_ dim = torch min cpu_x dim= keepdim=True assertEqual y_ dim refy_ dim assertEqual idx_ dim refidx_ dim helper test_fmin Regression test https github com pytorch pytorch issues scalar = torch tensor x_mps = torch rand device= mps x_cpu = x_mps detach cpu assertEqual torch fmin x_mps scalar torch fmin x_cpu scalar Test forward sum test_sum helper n c h w dtype=torch float cpu_x = None x = None dtype torch float torch bool cpu_x = torch randint n c h w device= cpu dtype=dtype requires_grad=False x = cpu_x detach clone mps dtype == torch bool cpu_x = torch randint n c h w device= cpu dtype=dtype requires_grad=False x = cpu_x detach clone mps cpu_x = torch randn n c h w device= cpu dtype=dtype requires_grad=True x = cpu_x detach clone mps requires_grad_ all_sum = torch sum x all_sum_cpu = torch sum cpu_x assertEqual all_sum all_sum_cpu nil_dim_sum = torch sum x dim= nil_dim_sum_cpu = torch sum cpu_x dim= assertEqual nil_dim_sum nil_dim_sum_cpu nil_dim_sum_keepdim = torch sum x dim= keepdim=True nil_dim_sum_cpu_keepdim = torch sum cpu_x dim= keepdim=True assertEqual nil_dim_sum_keepdim nil_dim_sum_cpu_keepdim zero_dim_sum = torch sum x dim= zero_dim_sum_cpu = torch sum cpu_x dim= assertEqual zero_dim_sum zero_dim_sum_cpu zero_dim_sum_keepdim = torch sum x dim= keepdim=True zero_dim_sum_cpu_keepdim = torch sum cpu_x dim= keepdim=True assertEqual zero_dim_sum_keepdim zero_dim_sum_cpu_keepdim zero_one_dim_sum = torch sum x dim= zero_one_dim_sum_cpu = torch sum cpu_x dim= assertEqual zero_one_dim_sum zero_one_dim_sum_cpu zero_one_dim_sum_keepdim = torch sum x dim= keepdim=True zero_one_dim_sum_cpu_keepdim = torch sum cpu_x dim= keepdim=True assertEqual zero_one_dim_sum_keepdim zero_one_dim_sum_cpu_keepdim two_three_dim_sum = torch sum x dim= two_three_dim_sum_cpu = torch sum cpu_x dim= assertEqual two_three_dim_sum two_three_dim_sum_cpu two_three_keepdim_sum = torch sum x dim= keepdim=True two_three_dim_keepsum_cpu = torch sum cpu_x dim= keepdim=True assertEqual two_three_keepdim_sum two_three_dim_keepsum_cpu helper helper dtype=torch int helper dtype=torch int helper dtype=torch bool Regression test https github com pytorch pytorch issues x = torch ones device= mps sum dim=- assertEqual x numel assertEqual x max item Test forward prod test_prod helper shape dtype=torch float cpu_x = None x = None dtype torch float torch bool cpu_x = torch randint shape device= cpu dtype=dtype requires_grad=False x = cpu_x detach clone mps dtype == torch bool cpu_x = torch randint shape device= cpu dtype=dtype requires_grad=False x = cpu_x detach clone mps cpu_x = torch randn shape device= cpu dtype=dtype requires_grad=True x = cpu_x detach clone mps requires_grad_ all_prod = torch prod x all_prod_cpu = torch prod cpu_x assertEqual all_prod all_prod_cpu dim range len shape dim_prod = torch prod x dim=dim dim_prod_cpu = torch prod cpu_x dim=dim assertEqual dim_prod dim_prod_cpu dim_prod_keepdim = torch prod x dim=dim keepdim=True dim_prod_cpu_keepdim = torch prod cpu_x dim=dim keepdim=True assertEqual dim_prod_keepdim dim_prod_cpu_keepdim dtype torch float torch int torch int torch bool helper dtype Test forward mean test_mean helper n c h w cpu_x = torch randn n c h w device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ all_mean = torch mean x all_mean_cpu = torch mean cpu_x assertEqual all_mean all_mean_cpu nil_dim_mean = torch mean x dim= nil_dim_mean_cpu = torch mean cpu_x dim= assertEqual nil_dim_mean nil_dim_mean_cpu nil_dim_mean_keepdim = torch mean x dim= keepdim=True nil_dim_mean_cpu_keepdim = torch mean cpu_x dim= keepdim=True assertEqual nil_dim_mean_keepdim nil_dim_mean_cpu_keepdim zero_dim_mean = torch mean x dim= zero_dim_mean_cpu = torch mean cpu_x dim= assertEqual zero_dim_mean zero_dim_mean_cpu zero_dim_mean_keepdim = torch mean x dim= keepdim=True zero_dim_mean_cpu_keepdim = torch mean cpu_x dim= keepdim=True assertEqual zero_dim_mean_keepdim zero_dim_mean_cpu_keepdim zero_one_dim_mean = torch mean x dim= zero_one_dim_mean_cpu = torch mean cpu_x dim= assertEqual zero_one_dim_mean zero_one_dim_mean_cpu zero_one_dim_mean_keepdim = torch mean x dim= keepdim=True zero_one_dim_mean_cpu_keepdim = torch mean cpu_x dim= keepdim=True assertEqual zero_one_dim_mean_keepdim zero_one_dim_mean_cpu_keepdim two_three_dim_mean = torch mean x dim= two_three_dim_mean_cpu = torch mean cpu_x dim= assertEqual two_three_dim_mean two_three_dim_mean_cpu two_three_keepdim_mean = torch mean x dim= keepdim=True two_three_dim_keepmean_cpu = torch mean cpu_x dim= keepdim=True assertEqual two_three_keepdim_mean two_three_dim_keepmean_cpu helper Test std test_std helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps all_std = torch std x unbiased=False all_std_cpu = torch std cpu_x unbiased=False assertEqual all_std all_std_cpu nil_dim_std = torch std x dim= unbiased=False nil_dim_std_cpu = torch std cpu_x dim= unbiased=False assertEqual nil_dim_std nil_dim_std_cpu nil_dim_std_keepdim = torch std x dim= keepdim=True unbiased=False nil_dim_std_cpu_keepdim = torch std cpu_x dim= keepdim=True unbiased=False assertEqual nil_dim_std_keepdim nil_dim_std_cpu_keepdim zero_dim_std = torch std x dim= unbiased=False zero_dim_std_cpu = torch std cpu_x dim= unbiased=False assertEqual zero_dim_std zero_dim_std_cpu zero_dim_std_keepdim = torch std x dim= keepdim=True unbiased=False zero_dim_std_cpu_keepdim = torch std cpu_x dim= keepdim=True unbiased=False assertEqual zero_dim_std_keepdim zero_dim_std_cpu_keepdim zero_one_dim_std = torch std x dim= unbiased=False zero_one_dim_std_cpu = torch std cpu_x dim= unbiased=False assertEqual zero_one_dim_std zero_one_dim_std_cpu zero_one_dim_std_keepdim = torch std x dim= keepdim=True unbiased=False zero_one_dim_std_cpu_keepdim = torch std cpu_x dim= keepdim=True unbiased=False assertEqual zero_one_dim_std_keepdim zero_one_dim_std_cpu_keepdim two_three_dim_std = torch std x dim= unbiased=False two_three_dim_std_cpu = torch std cpu_x dim= unbiased=False assertEqual two_three_dim_std two_three_dim_std_cpu two_three_keepdim_std = torch std x dim= keepdim=True unbiased=False two_three_dim_keepstd_cpu = torch std cpu_x dim= keepdim=True unbiased=False assertEqual two_three_keepdim_std two_three_dim_keepstd_cpu all_std = torch std x unbiased=True all_std_cpu = torch std cpu_x unbiased=True assertEqual all_std all_std_cpu nil_dim_std = torch std x dim= unbiased=True nil_dim_std_cpu = torch std cpu_x dim= unbiased=True assertEqual nil_dim_std nil_dim_std_cpu nil_dim_std_keepdim = torch std x dim= keepdim=True unbiased=True nil_dim_std_cpu_keepdim = torch std cpu_x dim= keepdim=True unbiased=True assertEqual nil_dim_std_keepdim nil_dim_std_cpu_keepdim zero_dim_std = torch std x dim= unbiased=True zero_dim_std_cpu = torch std cpu_x dim= unbiased=True assertEqual zero_dim_std zero_dim_std_cpu zero_dim_std_keepdim = torch std x dim= keepdim=True unbiased=True zero_dim_std_cpu_keepdim = torch std cpu_x dim= keepdim=True unbiased=True assertEqual zero_dim_std_keepdim zero_dim_std_cpu_keepdim zero_one_dim_std = torch std x dim= unbiased=True zero_one_dim_std_cpu = torch std cpu_x dim= unbiased=True assertEqual zero_one_dim_std zero_one_dim_std_cpu zero_one_dim_std_keepdim = torch std x dim= keepdim=True unbiased=True zero_one_dim_std_cpu_keepdim = torch std cpu_x dim= keepdim=True unbiased=True assertEqual zero_one_dim_std_keepdim zero_one_dim_std_cpu_keepdim two_three_dim_std = torch std x dim= unbiased=True two_three_dim_std_cpu = torch std cpu_x dim= unbiased=True assertEqual two_three_dim_std two_three_dim_std_cpu two_three_keepdim_std = torch std x dim= keepdim=True unbiased=True two_three_dim_keepstd_cpu = torch std cpu_x dim= keepdim=True unbiased=True assertEqual two_three_keepdim_std two_three_dim_keepstd_cpu helper verify change shape input would cause problems graph caching helper Test var test_var_simple helper shape = cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps unbiased False True keepdim False True zero_dim_var = x var - keepdim=keepdim unbiased=unbiased zero_dim_var_cpu = cpu_x var - keepdim=keepdim unbiased=unbiased assertEqual zero_dim_var zero_dim_var_cpu all_var = torch var x unbiased=unbiased all_var_cpu = torch var cpu_x unbiased=unbiased assertEqual all_var all_var_cpu nil_dim_var = torch var x dim= keepdim=keepdim unbiased=unbiased nil_dim_var_cpu = torch var cpu_x dim= keepdim=keepdim unbiased=unbiased assertEqual nil_dim_var nil_dim_var_cpu zero_dim_var = torch var x dim= keepdim=keepdim unbiased=unbiased zero_dim_var_cpu = torch var cpu_x dim= keepdim=keepdim unbiased=unbiased assertEqual zero_dim_var zero_dim_var_cpu zero_one_dim_var = torch var x dim= - keepdim=keepdim unbiased=unbiased zero_one_dim_var_cpu = torch var cpu_x dim= - keepdim=keepdim unbiased=unbiased assertEqual zero_one_dim_var zero_one_dim_var_cpu two_three_dim_var = torch var x dim= keepdim=keepdim unbiased=unbiased two_three_dim_var_cpu = torch var cpu_x dim= keepdim=keepdim unbiased=unbiased assertEqual two_three_dim_var two_three_dim_var_cpu helper Regression test https github com pytorch pytorch issues assertTrue torch var torch tensor device= mps dim= isnan item Test forward amax test_amax helper shape dim keepdim cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ result = torch amax x dim=dim keepdim=keepdim result_cpu = torch amax cpu_x dim=dim keepdim=keepdim cpu_grad = torch randn result_cpu shape grad = cpu_grad mps result_cpu backward gradient=cpu_grad result backward gradient=grad assertEqual result result_cpu assertEqual x grad cpu_x grad dim keepdim False True helper dim keepdim Test forward amin test_amin helper shape dim keepdim cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ result = torch amin x dim=dim keepdim=keepdim result_cpu = torch amin cpu_x dim=dim keepdim=keepdim cpu_grad = torch randn result_cpu shape grad = cpu_grad mps result_cpu backward gradient=cpu_grad result backward gradient=grad assertEqual result result_cpu assertEqual x grad cpu_x grad dim keepdim False True helper dim keepdim Test minimum maximum test_minimum_maximum helper n c h w cpu_x = torch randn n c h w device= cpu dtype=torch float requires_grad=False cpu_y = torch randn n c h w device= cpu dtype=torch float requires_grad=False mps_x = cpu_x detach clone mps mps_y = cpu_y detach clone mps minimum_result_cpu = torch minimum cpu_x cpu_y minimum_result_mps = torch minimum mps_x mps_y assertEqual minimum_result_cpu minimum_result_mps maximum_result_cpu = torch maximum cpu_x cpu_y maximum_result_mps = torch maximum mps_x mps_y assertEqual maximum_result_cpu maximum_result_mps helper test_minimum_maximum_nan_propagation x = torch rand device= mps y = torch rand device= mps x = torch nan y = torch nan assertTrue torch minimum x y isnan any item assertTrue torch maximum x y isnan any item test_clamp_fp _fp cpu_x = torch randn device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps dtype = torch float clamp_min_vals_mps = torch ones device= mps torch float clamp_max_vals_mps = torch ones device= mps torch float clamp_result_mps = torch clamp x clamp_min_vals_mps clamp_max_vals_mps clamp_min_vals_cpu = torch ones device= cpu torch float clamp_max_vals_cpu = torch ones device= cpu torch float clamp_result_cpu = torch clamp cpu_x clamp_min_vals_cpu clamp_max_vals_cpu assertEqual clamp_result_mps clamp_result_cpu test_clamp_nan t_mps = torch tensor torch nan device= mps t_cpu = torch tensor torch nan device= cpu clamp_min_max_mps = torch clamp t_mps min=- max= clamp_min_max_cpu = torch clamp t_cpu min=- max= assertEqual clamp_min_max_mps clamp_min_max_cpu clamp_min_mps = torch clamp t_mps min=- clamp_min_cpu = torch clamp t_cpu min=- assertEqual clamp_min_mps clamp_min_cpu clamp_max_mps = torch clamp t_mps max= clamp_max_cpu = torch clamp t_cpu max= assertEqual clamp_max_mps clamp_max_cpu Test clamp_min test_clamp_min helper n c h w cpu_x = torch randn n c h w device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps cpu_min_t = torch randn n c h w device= cpu dtype=torch float requires_grad=False min_t = cpu_min_t detach clone mps clamp_min_result = torch clamp_min x min= clamp_min_result_cpu = torch clamp_min cpu_x min= assertEqual clamp_min_result clamp_min_result_cpu clamp_min_t_result = torch clamp_min x min=min_t clamp_min_t_result_cpu = torch clamp_min cpu_x min=cpu_min_t assertEqual clamp_min_t_result clamp_min_t_result_cpu helper Test clamp_max test_clamp_max helper n c h w cpu_x = torch randn n c h w device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps cpu_max_t = torch randn n c h w device= cpu dtype=torch float requires_grad=False max_t = cpu_max_t detach clone mps clamp_max_result = torch clamp_max x max= clamp_max_result_cpu = torch clamp_max cpu_x max= assertEqual clamp_max_result clamp_max_result_cpu clamp_max_t_result = torch clamp_max x max=max_t clamp_max_t_result_cpu = torch clamp_max cpu_x max=cpu_max_t assertEqual clamp_max_t_result clamp_max_t_result_cpu helper Test clamp test_clamp helper n c h w numpy np upper_bound = half_upper_bound = upper_bound x= x_arr = upper_bound np random random_sample size= n c h w astype np float cpu_x = torch tensor x_arr device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps x= min_arr = half_upper_bound np random random_sample size= n c h w astype np float cpu_min_t = torch tensor min_arr device= cpu dtype=torch float requires_grad=False min_t = cpu_min_t detach clone mps x= ensure max s greater than mins max_arr = half_upper_bound np random random_sample size= n c h w astype np float + half_upper_bound cpu_max_t = torch tensor max_arr device= cpu dtype=torch float requires_grad=False max_t = cpu_max_t detach clone mps just arbitrary range between clamp_result = torch clamp x min= max= clamp_result_cpu = torch clamp cpu_x min= max= assertEqual clamp_result clamp_result_cpu test optional scalar refs cached graph keys passing only max clamp_opt_result = torch clamp x max= clamp_opt_result_cpu = torch clamp cpu_x max= assertEqual clamp_opt_result clamp_opt_result_cpu clamp_t_result = torch clamp x min=min_t max=max_t clamp_t_result_cpu = torch clamp cpu_x min=cpu_min_t max=cpu_max_t assertEqual clamp_t_result clamp_t_result_cpu test optional tensor refs cached graph keys passing only max clamp_topt_result = torch clamp x max=max_t clamp_topt_result_cpu = torch clamp cpu_x max=cpu_max_t assertEqual clamp_topt_result clamp_topt_result_cpu test strided x clamp_result = torch clamp x movedim - min= max= clamp_result_cpu = torch clamp cpu_x movedim - min= max= assertEqual clamp_result clamp_result_cpu test strided x min_t max_t clamp_result = torch clamp x movedim - min=min_t movedim - max=max_t movedim - clamp_result_cpu = torch clamp cpu_x movedim - min=cpu_min_t movedim - max=cpu_max_t movedim - assertEqual clamp_result clamp_result_cpu test strided min_t max_t clamp_result = torch clamp x movedim - clone memory_format=torch contiguous_format min=min_t movedim - max=max_t movedim - clamp_result_cpu = torch clamp cpu_x movedim - clone memory_format=torch contiguous_format min=cpu_min_t movedim - max=cpu_max_t movedim - assertEqual clamp_result clamp_result_cpu test inplace clamping x clamp_ min= max= cpu_x clamp_ min= max= assertEqual cpu_x x helper test_divmode helper shape rounding_mode dtype torch float torch float torch int torch int rounding_mode None floor rounding_mode dtype == torch int rounding_mode None trunc rounding_mode dtype == torch float False cpu_x = None cpu_y = None dtype torch float torch float cpu_x = torch randn shape device= cpu dtype=dtype requires_grad=False cpu_y = torch randn shape device= cpu dtype=dtype requires_grad=False cpu_x = torch randint - shape device= cpu dtype=dtype requires_grad=False cpu_y = torch randint - shape device= cpu dtype=dtype requires_grad=False mps_x = cpu_x detach clone mps clamp avoid division mps_y = cpu_y detach clone mps rounding_mode == floor_divide result_div_cpu = torch floor_divide cpu_x cpu_y result_div_mps = torch floor_divide mps_x mps_y assertEqual result_div_mps result_div_cpu result_div_cpu = torch div cpu_x cpu_y rounding_mode=rounding_mode result_div_mps = torch div mps_x mps_y rounding_mode=rounding_mode assertEqual result_div_mps result_div_cpu helper None helper floor helper trunc helper floor_divide test_rounding helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False mps_x = cpu_x detach clone mps result_floor_cpu = torch floor cpu_x result_floor_mps = torch floor mps_x assertEqual result_floor_mps result_floor_cpu result_ceil_cpu = torch ceil cpu_x result_ceil_mps = torch ceil mps_x assertEqual result_ceil_mps result_ceil_cpu result_trunc_cpu = torch trunc cpu_x result_trunc_mps = torch trunc mps_x assertEqual result_trunc_mps result_trunc_cpu result_round_cpu = torch round cpu_x result_round_mps = torch round mps_x assertEqual result_round_mps result_round_cpu helper helper test_remainder res_cpu = torch remainder torch tensor - - - dtype=torch int device= cpu torch tensor device= cpu dtype=torch int res_mps = torch remainder torch tensor - - - dtype=torch int device= mps torch tensor device= mps dtype=torch int assertEqual res_cpu res_mps res_cpu = torch remainder torch tensor dtype=torch int device= cpu - res_mps = torch remainder torch tensor dtype=torch int device= mps - assertEqual res_cpu res_mps Regression test https github com pytorch pytorch issues Essentially remained over integral types should rely integers ops assertEqual torch tensor device= mps torch tensor device= mps torch tensor device= mps test_expand helper n c values = cpu_x = torch tensor values device= cpu x = cpu_x detach clone mps strided_cpu = torch as_strided cpu_x strided_mps = torch as_strided x assertEqual strided_mps strided_cpu helper test_im col helper x torch nn functional unfold x kernel_size= dilation= padding= stride= x_cpu = torch rand x = x_cpu detach clone mps assertEqual helper x_cpu helper x test_col im helper shapes output_size kernel_size padding stride contiguous dtype=torch float test_bool=False atol = e- dtype == torch float e- rtol = e- dtype == torch float e- x_cpu = torch rand shapes dtype=dtype test_bool x_cpu = x_cpu x_mps = x_cpu clone mps contiguous x_cpu = x_cpu mT x_mps = x_mps mT out_cpu = torch nn functional fold x_cpu output_size=output_size kernel_size=kernel_size padding=padding stride=stride out_mps = torch nn functional fold x_mps output_size=output_size kernel_size=kernel_size padding=padding stride=stride assertEqual out_cpu out_mps atol=atol rtol=rtol helper True helper True helper True helper True helper True helper False helper True helper True helper True helper True helper False helper True helper True helper False helper False torch bfloat helper False torch float helper False test_bool=True test_select helper n c cpu_x = torch randn n c device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ strided_cpu = torch as_strided cpu_x strided_mps = torch as_strided x assertEqual strided_mps strided_cpu strided_cpu = torch as_strided cpu_x strided_mps = torch as_strided x assertEqual strided_mps strided_cpu strided_cpu = torch as_strided cpu_x storage_offset= strided_mps = torch as_strided x storage_offset= assertEqual strided_mps strided_cpu helper test_sort SIZE device = mps x = torch rand SIZE device=device res val res ind = torch sort x res val = torch tensor device=device res ind = torch tensor device=device dtype=torch long torch sort x out= res val res ind assertEqual res val res val atol= rtol= assertEqual res ind res ind atol= rtol= assertEqual torch argsort x res ind assertEqual x argsort res ind assertEqual torch sort torch tensor device=device torch tensor device=device atol= rtol= test_linalg_cholesky torch testing _internal common_utils random_hermitian_pd_matrix run_cholesky_test size batch_dims upper=False check_errors=False check_errors expect failure non-positive definite matrix input_mps = torch eye size dtype=torch float device= mps input_mps = - error_msg = r The factorization could completed because input positive-definite assertRaisesRegex RuntimeError error_msg torch linalg cholesky_ex input_mps upper=upper check_errors=check_errors output checks positive definite matrix input_cpu = random_hermitian_pd_matrix size batch_dims dtype=torch float device= cpu input_mps = input_cpu mps output_cpu = torch linalg cholesky_ex input_cpu upper=upper output_mps = torch linalg cholesky_ex input_mps upper=upper assertEqual output_cpu output_mps atol= e- rtol= e- test different even odd matrix sizes matrix_sizes = even odd batch sizes batch_sizes = upper True False size matrix_sizes batch_size batch_sizes run_cholesky_test size batch_size upper=upper test D matrices run_cholesky_test upper=False run_cholesky_test upper=True run_cholesky_test upper=False check_errors=True run_cholesky_test upper=True check_errors=True test_linalg_cholesky_info non psd matrix leading minor order being positive definite A = torch tensor - device= mps assertRaisesRegex RuntimeError r leading minor order positive-definite torch linalg cholesky_ex A check_errors=True test_upsample_nearest d helper N C H W memory_format inputCPU = torch arange N C H W device= cpu dtype=torch float requires_grad=True reshape N C H W memory_format=memory_format inputCPU retain_grad inputMPS = inputCPU detach mps requires_grad_ values = i values j values upsample_nearest d = nn UpsamplingNearest d scale_factor= i j outputCPU = upsample_nearest d inputCPU outputMPS = upsample_nearest d inputMPS assertEqual outputCPU outputMPS upsample_nearest d = nn UpsamplingNearest d i H j W outputCPU = upsample_nearest d inputCPU outputMPS = upsample_nearest d inputMPS assertEqual outputCPU outputMPS outputCPU backward gradient=torch full_like outputCPU outputMPS backward gradient=torch full_like outputMPS assertEqual inputCPU grad inputMPS grad memory_format torch channels_last torch contiguous_format helper memory_format=memory_format helper memory_format=memory_format test_upsample_bilinear d helper N C H W inputCPU = torch arange N C H W device= cpu dtype=torch float requires_grad=True reshape N C H W inputCPU retain_grad inputMPS = inputCPU detach clone mps requires_grad_ values = i values j values upsample_bilinear d = nn UpsamplingBilinear d scale_factor= i j outputCPU = upsample_bilinear d inputCPU outputMPS = upsample_bilinear d inputMPS assertEqual outputCPU outputMPS upsample_bilinear d = nn UpsamplingBilinear d i H j W outputCPU = upsample_bilinear d inputCPU outputMPS = upsample_bilinear d inputMPS assertEqual outputCPU outputMPS outputCPU backward gradient=torch full_like outputCPU outputMPS backward gradient=torch full_like outputMPS assertEqual inputCPU grad inputMPS grad helper helper test_interpolate helper shape output_size scales mode align_corners=False inputCPU = torch randn shape device= cpu dtype=torch float requires_grad=True inputCPU retain_grad inputMPS = inputCPU detach clone mps requires_grad_ align_corners used D interpolation only align_corners True len shape mode == bilinear scales None outputCPU = nn functional interpolate inputCPU scale_factor=scales mode=mode align_corners=align_corners outputMPS = nn functional interpolate inputMPS scale_factor=scales mode=mode align_corners=align_corners outputCPU = nn functional interpolate inputCPU size=output_size mode=mode align_corners=align_corners outputMPS = nn functional interpolate inputMPS size=output_size mode=mode align_corners=align_corners scales None outputCPU = nn functional interpolate inputCPU scale_factor=scales mode=mode outputMPS = nn functional interpolate inputMPS scale_factor=scales mode=mode outputCPU = nn functional interpolate inputCPU size=output_size mode=mode outputMPS = nn functional interpolate inputMPS size=output_size mode=mode assertEqual outputCPU outputMPS backward pass chose just have grad_output = outputCPU backward gradient=torch full_like outputCPU outputMPS backward gradient=torch full_like outputMPS assertEqual inputCPU grad inputMPS grad D interpolation mode nearest nearest-exact helper None mode downsample size helper None mode upsample size helper None mode downsample scale factor helper None mode upsample scale factor D interpolation mode nearest nearest-exact bilinear helper None mode downsample_nearest size helper None mode upsample_nearest size helper None mode downsample_nearest scale factor helper None mode upsample_nearest scale factor align_corners=True helper None bilinear True helper None bilinear True Regression test https github com pytorch pytorch issues inp = torch tensor device= mps align_corners True False interp x F interpolate x mode= linear align_corners=align_corners assertEqual interp inp cpu interp inp cpu Test concat forward test_cat helper shape_x shape_y shape_z cpu_x = torch randn shape_x device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps cpu_y = torch randn shape_y device= cpu dtype=torch float requires_grad=False y = cpu_y detach clone mps cpu_z = torch randn shape_z device= cpu dtype=torch float requires_grad=False z = cpu_z detach clone mps cat = torch cat x y z dim= cat_cpu = torch cat cpu_x cpu_y cpu_z dim= assertEqual cat cat_cpu helper helper helper helper helper helper helper helper helper Test stack forward test_stack All shapes must same helper shape dtype=torch float x cpu_x = None None y cpu_y = None None z cpu_z = None None dtype torch float torch bool cpu_x = torch randint shape device= cpu dtype=dtype requires_grad=False x = cpu_x detach clone mps cpu_y = torch randint shape device= cpu dtype=dtype requires_grad=False y = cpu_y detach clone mps cpu_z = torch randint shape device= cpu dtype=dtype requires_grad=False z = cpu_z detach clone mps dtype == torch bool cpu_x = torch randint shape device= cpu dtype=dtype requires_grad=False x = cpu_x detach clone mps cpu_y = torch randint shape device= cpu dtype=dtype requires_grad=False y = cpu_y detach clone mps cpu_z = torch randint shape device= cpu dtype=dtype requires_grad=False z = cpu_z detach clone mps cpu_x = torch randn shape device= cpu dtype=dtype requires_grad=True x = cpu_x detach clone mps requires_grad_ cpu_y = torch randn shape device= cpu dtype=dtype requires_grad=True y = cpu_y detach clone mps requires_grad_ cpu_z = torch randn shape device= cpu dtype=dtype requires_grad=True z = cpu_z detach clone mps requires_grad_ stack = torch stack x y z dim= stack_cpu = torch stack cpu_x cpu_y cpu_z dim= assertEqual stack stack_cpu helper helper dtype=torch float helper dtype=torch int helper dtype=torch int helper dtype=torch bool Empty test - Currently failing Empty tensor handled helper Test abs test_abs helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps abs_result = torch abs x abs_result_cpu = torch abs cpu_x assertEqual abs_result abs_result_cpu helper test_angle helper shape dtype cpu_x = torch randn shape device= cpu dtype=dtype requires_grad=False cpu_x flatten = torch nan Test NaN propagated correctly x = cpu_x detach clone mps angle_result = torch angle x angle_result_cpu = torch angle cpu_x assertEqual angle_result angle_result_cpu helper torch float helper torch float helper torch complex test_log helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps log_result = torch log x log_result_cpu = torch log cpu_x assertEqual log_result log_result_cpu helper test_log_ten helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps log_ten_result = torch log x log_ten_result_cpu = torch log cpu_x assertEqual log_ten_result log_ten_result_cpu helper test_log_two helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps log_two_result = torch log x log_two_result_cpu = torch log cpu_x assertEqual log_two_result log_two_result_cpu helper parametrize dtype torch float torch half torch bfloat test_log p dtype eps = torch finfo dtype eps Small values cpu_x = torch arange - eps eps e- eps dtype=dtype requires_grad=False x = cpu_x detach clone mps log_result = torch log p x log_result_cpu = torch log p cpu_x assertEqual log_result log_result_cpu atol= rtol= e- Fallback log cpu_x = torch arange - e- dtype=dtype requires_grad=False x = cpu_x detach clone mps log_result = torch log p x log_result_cpu = torch log p cpu_x assertEqual log_result log_result_cpu atol= rtol= e- test_logaddexp helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps cpu_y = torch randn shape device= cpu dtype=torch float requires_grad=False y = cpu_y detach clone mps log_result = torch logaddexp x y log_result_cpu = torch logaddexp cpu_x cpu_y assertEqual log_result log_result_cpu helper test_logaddexp helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps cpu_y = torch randn shape device= cpu dtype=torch float requires_grad=False y = cpu_y detach clone mps log_result = torch logaddexp x y log_result_cpu = torch logaddexp cpu_x cpu_y assertEqual log_result log_result_cpu helper test_logsumexp helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps log_result = torch logsumexp x - log_result_cpu = torch logsumexp cpu_x - assertEqual log_result log_result_cpu helper Test concat forward test_cat helper shape_x shape_y shape_z shape_w cpu_x = torch randn shape_x device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps cpu_y = torch randn shape_y device= cpu dtype=torch float requires_grad=False y = cpu_y detach clone mps cpu_z = torch randn shape_z device= cpu dtype=torch float requires_grad=False z = cpu_z detach clone mps cpu_w = torch randn shape_w device= cpu dtype=torch float requires_grad=False w = cpu_w detach clone mps cat = torch cat x y z w dim= cat_cpu = torch cat cpu_x cpu_y cpu_z cpu_w dim= assertEqual cat cat_cpu helper shape_x shape_y shape_z cpu_x = torch randn shape_x device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps cpu_y = torch randn shape_y device= cpu dtype=torch float requires_grad=False y = cpu_y detach clone mps cpu_z = torch randn shape_z device= cpu dtype=torch float requires_grad=False z = cpu_z detach clone mps cat = torch cat x y z dim= cat_cpu = torch cat cpu_x cpu_y cpu_z dim= assertEqual cat cat_cpu helper helper Empty test - Currently failing Empty tensor handled helper Test isnan test_isnan helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False nan_index = random randrange shape make selected row inf cpu_x index_put_ indices= torch tensor nan_index values=torch tensor float nan x = cpu_x detach clone mps isnan_result = torch isnan x isnan_result_cpu = torch isnan cpu_x assertEqual isnan_result isnan_result_cpu helper Test reciprocal test_reciprocal helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ reciprocal_result = torch reciprocal x reciprocal_result_cpu = torch reciprocal cpu_x cpu_grad = torch ones_like reciprocal_result_cpu grad = cpu_grad mps reciprocal_result backward gradient=grad reciprocal_result_cpu backward gradient=cpu_grad assertEqual reciprocal_result reciprocal_result_cpu assertEqual x grad cpu_x grad helper Test sqrt test_sqrt helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ sqrt_result = torch sqrt x sqrt_result_cpu = torch sqrt cpu_x cpu_grad = torch ones_like sqrt_result_cpu grad = cpu_grad mps sqrt_result backward gradient=grad sqrt_result_cpu backward gradient=cpu_grad assertEqual sqrt_result sqrt_result_cpu assertEqual x grad cpu_x grad helper Test complex half x = torch rand device= mps dtype=torch chalf rc_h = x sqrt rc_f = x cfloat sqrt chalf assertEqual rc_h rc_f Test selu elu celu test_elu helper shape alpha= memory_format=torch contiguous_format cpu_x = torch randn shape device= cpu dtype=torch float cpu_x = cpu_x memory_format=memory_format requires_grad_ x = cpu_x detach clone mps requires_grad_ True activation_func torch nn ELU alpha=alpha torch nn CELU alpha=alpha torch nn SELU elu_result = activation_func x elu_result_cpu = activation_func cpu_x cpu_grad = torch randn elu_result_cpu shape grad = cpu_grad mps elu_result backward gradient=grad elu_result_cpu backward gradient=cpu_grad assertEqual elu_result elu_result_cpu assertEqual x grad cpu_x grad Test empty shape too memory_fromat torch channels_last torch contiguous_format shape alpha helper shape alpha memory_fromat test_elu_strided_output https github com pytorch pytorch issues elu_input = torch randn alpha = float inplace = False elu_input_noncontiguous = elu_input transpose assertEqual F elu elu_input_noncontiguous cpu alpha inplace F elu elu_input_noncontiguous mps alpha inplace Test glu test_glu helper shape dim= cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ activation_func torch nn GLU dim=dim glu_result = activation_func x glu_result_cpu = activation_func cpu_x cpu_grad = torch randn glu_result_cpu shape grad = cpu_grad mps glu_result backward gradient=grad glu_result_cpu backward gradient=cpu_grad assertEqual glu_result glu_result_cpu assertEqual x grad cpu_x grad shape dim range len shape helper shape dim Test softplus test_softplus helper shape beta threshold dtype cpu_x = torch randn shape device= cpu dtype=dtype requires_grad=True x = cpu_x detach clone mps requires_grad_ softplus_result = torch nn Softplus beta=beta threshold=threshold x softplus_result_cpu = torch nn Softplus beta=beta threshold=threshold cpu_x cpu_grad = torch randn softplus_result shape grad = cpu_grad mps softplus_result backward gradient=grad softplus_result_cpu backward gradient=cpu_grad assertEqual softplus_result softplus_result_cpu assertEqual x grad cpu_x grad Test empty shape too shape beta threshold dtype product torch float torch float helper shape beta threshold dtype Test silu test_silu helper shape contiguous=True cpu_x = torch randn shape device= cpu dtype=torch float x = cpu_x detach clone mps contiguous shape len shape = Transposing will make tensor non-contiguous cpu_x = cpu_x transpose x = x transpose assert x is_contiguous cpu_x requires_grad_ x requires_grad_ silu_result = torch nn SiLU x silu_result_cpu = torch nn SiLU cpu_x cpu_grad = torch randn silu_result_cpu shape grad = cpu_grad mps silu_result backward gradient=grad silu_result_cpu backward gradient=cpu_grad assertEqual silu_result silu_result_cpu assertEqual x grad cpu_x grad Test empty shape too shape contiguous True False helper shape contiguous test_cast_mps_to_cpu helper src_dtype dst_dtype input = torch rand dtype=src_dtype input_cast_mps = input mps input_cast_cpu = input_cast_mps cpu dtype=dst_dtype needs match initial Tensor assertEqual input_cast_cpu input dtype=dst_dtype helper torch half torch float helper torch float torch half test_cast_mps_to_mps helper src_dtype dst_dtype input_cpu = torch rand dtype=src_dtype input_mps = input_cpu mps output_mps = input_mps dtype=dst_dtype output_cpu = input_cpu dtype=dst_dtype assertEqual output_mps cpu output_cpu helper torch half torch float helper torch float torch half helper torch half torch long helper torch float torch int test_avg_pool d_count_include_pad cpu_x = torch randn device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ pool = torch nn AvgPool d kernel_size= padding= stride= ceil_mode=True count_include_pad=True ref_y = pool cpu_x y = pool x assertEqual y ref_y cpu_grad = torch randn ref_y shape grad = cpu_grad mps ref_y backward gradient=cpu_grad y backward gradient=grad assertEqual x grad cpu_x grad Test adaptive avg pool d - when input size multiple output size Not testing channels last right now test_adaptive_avg_pool d_simple helper input_shape out_shape channels_last cpu_x = torch randn input_shape device= cpu dtype=torch float requires_grad=True channels_last cpu_x = cpu_x memory_format=torch channels_last cpu_x retain_grad x = cpu_x detach clone mps requires_grad_ avg_result = torch nn AdaptiveAvgPool d out_shape x avg_result_cpu = torch nn AdaptiveAvgPool d out_shape cpu_x cpu_grad = torch randn avg_result_cpu shape grad = cpu_grad mps avg_result backward gradient=grad avg_result_cpu backward gradient=cpu_grad assertEqual avg_result avg_result_cpu assertEqual x grad cpu_x grad helper False helper False helper False helper False helper False helper False Output shape larger than input shape helper False helper False helper False helper False helper False helper False try helper False except Exception e pass Test max avg pool d - when input size multiple output size Not testing channels last right now test_adaptive_max_pool d_simple helper input_shape out_shape return_indices dtype channels_last=False cpu_x = None dtype torch float torch float cpu_x = torch randn input_shape device= cpu dtype=dtype requires_grad=True cpu_x = torch randint input_shape device= cpu dtype=dtype requires_grad=True channels_last cpu_x = cpu_x memory_format=torch channels_last cpu_x retain_grad x = cpu_x detach clone mps requires_grad_ max_result max_indices = None None max_result_cpu max_indices_cpu = None None return_indices max_result max_indices = torch nn AdaptiveMaxPool d out_shape return_indices x max_result_cpu max_indices_cpu = torch nn AdaptiveMaxPool d out_shape return_indices cpu_x max_result = torch nn AdaptiveMaxPool d out_shape return_indices x max_result_cpu = torch nn AdaptiveMaxPool d out_shape return_indices cpu_x cpu_grad = torch randn max_result_cpu shape grad = cpu_grad mps max_result backward gradient=grad max_result_cpu backward gradient=cpu_grad assertEqual max_result max_result_cpu return_indices assertEqual max_indices max_indices_cpu assertEqual x grad cpu_x grad dtype torch float return_indices False True helper return_indices dtype helper return_indices dtype helper return_indices dtype helper return_indices dtype helper return_indices dtype helper return_indices dtype test_gelu_simple helper shape dtype=torch float contiguous=True cpu_x = torch randn shape device= cpu dtype=dtype x = cpu_x detach clone mps contiguous shape len shape = Transposing will make tensor non-contiguous cpu_x = cpu_x transpose x = x transpose assert x is_contiguous cpu_x requires_grad_ x requires_grad_ gelu_result = torch nn GELU x GELU supported CPU so cast float gelu_result_cpu = torch nn GELU cpu_x torch float cpu_grad = torch ones_like gelu_result_cpu grad = cpu_grad mps gelu_result backward gradient=grad gelu_result_cpu backward gradient=cpu_grad atol = e- dtype == torch float e- rtol = e- dtype == torch float e- assertEqual gelu_result gelu_result_cpu dtype atol=atol rtol=rtol assert x grad None Check grad well-populated assertEqual x grad cpu_x grad atol=atol rtol=rtol Test empty shape too dtype torch float torch half shape contiguous True False helper shape dtype contiguous Test gelu would raise assert integral types dtype torch int torch int torch int torch int assertRaises RuntimeError lambda torch nn GELU torch randint dtype=dtype device= mps test_mish_simple helper shape dtype=torch float contiguous=True cpu_x = torch randn shape device= cpu dtype=dtype x = cpu_x detach clone mps contiguous shape len shape = Transposing will make tensor non-contiguous cpu_x = cpu_x transpose x = x transpose assert x is_contiguous cpu_x requires_grad_ x requires_grad_ mish_result = torch nn Mish x mish_result_cpu = torch nn Mish cpu_x cpu_grad = torch ones_like mish_result_cpu grad = cpu_grad mps mish_result backward gradient=grad mish_result_cpu backward gradient=cpu_grad atol = e- dtype == torch float e- rtol = e- dtype == torch float e- assertEqual mish_result mish_result_cpu dtype atol=atol rtol=rtol assert x grad None Check grad well-populated assertEqual x grad cpu_x grad atol=atol rtol=rtol Test empty shape too dtype torch float torch half shape contiguous True False helper shape dtype contiguous test_gelu _test_gelu n m dtype contiguous atol=None rtol=None numpy_dtype = torch bfloat torch float torch float torch float torch double torch double dtype devices = cpu devices += mps _gelu_ref X X stats norm cdf X noqa F d devices X = torch rand n m dtype=dtype requires_grad=True device=d res = X ref = X numpy_dtype cpu detach numpy assertEqual res ref rtol=rtol atol=atol exact_dtype=False n m _test_gelu n m torch float True _test_gelu n m torch float False Test multi threaded num_threads = torch get_num_threads torch set_num_threads try _test_gelu torch float False finally torch set_num_threads num_threads test_gelu_tanh helper shape cpu_x = torch randn shape device= cpu dtype=torch float x = cpu_x detach clone mps gelu_tanh_result = torch nn functional gelu x approximate= tanh gelu_tanh_result_cpu = torch nn functional gelu cpu_x approximate= tanh assertEqual gelu_tanh_result gelu_tanh_result_cpu helper Test hardtanh test_hardtanh helper shape min_val max_val inplace=False cpu_x = None x = None inplace cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps hardtanh_result = torch nn Hardtanh min_val=min_val max_val=max_val inplace=inplace x hardtanh_result_cpu = torch nn Hardtanh min_val=min_val max_val=max_val inplace=inplace cpu_x assertEqual hardtanh_result hardtanh_result_cpu inplace cpu_grad = torch randn hardtanh_result_cpu shape grad = cpu_grad mps hardtanh_result backward gradient=grad hardtanh_result_cpu backward gradient=cpu_grad assertEqual x grad cpu_x grad Test empty shape too shape min_val max_val zip - - - helper shape min_val max_val helper shape min_val max_val inplace=True test_hardswish helper shape inplace=False requires_grad=True m = nn Hardswish inplace=inplace input_cpu = torch randn shape device= cpu dtype=torch float requires_grad=requires_grad input_mps = input_cpu detach clone mps requires_grad_ requires_grad inplace requires_grad check both raise runtime error assertRaises RuntimeError lambda m input_cpu assertRaises RuntimeError lambda m input_mps output_cpu = m input_cpu output_mps = m input_mps cpu_grad = torch ones_like output_cpu mps_grad = cpu_grad mps assertEqual output_cpu output_mps requires_grad output_cpu backward gradient=cpu_grad output_mps backward gradient=mps_grad assertEqual input_cpu grad input_mps grad shape helper shape inplace=False requires_grad=False helper shape inplace=True requires_grad=False helper shape inplace=False requires_grad=True helper shape inplace=True requires_grad=True test_transpose_ D values = values = cpu_x = torch tensor values device= cpu mps_x = torch tensor values device= mps mps_x = torch tensor values device= mps cpu_transpose = torch transpose cpu_x mps_transpose = torch transpose mps_x assertEqual cpu_transpose mps_transpose cpu test_transpose_ D values = cpu_x = torch tensor values device= cpu mps_x = torch tensor values device= mps cpu_transpose = torch transpose cpu_x mps_transpose = torch transpose mps_x cpu assertEqual cpu_transpose mps_transpose cpu_transpose = torch transpose cpu_x mps_transpose = torch transpose mps_x cpu assertEqual cpu_transpose mps_transpose cpu_transpose = torch transpose cpu_x mps_transpose = torch transpose mps_x cpu assertEqual cpu_transpose mps_transpose test_transpose_ D values = cpu_x = torch tensor values device= cpu mps_x = torch tensor values device= mps cpu_transpose = torch transpose cpu_x mps_transpose = torch transpose mps_x cpu assertEqual cpu_transpose mps_transpose cpu_transpose = torch transpose cpu_x mps_transpose = torch transpose mps_x cpu assertEqual cpu_transpose mps_transpose cpu_transpose = torch transpose cpu_x mps_transpose = torch transpose mps_x cpu assertEqual cpu_transpose mps_transpose cpu_transpose = torch transpose cpu_x mps_transpose = torch transpose mps_x cpu assertEqual cpu_transpose mps_transpose cpu_transpose = torch transpose cpu_x mps_transpose = torch transpose mps_x cpu assertEqual cpu_transpose mps_transpose cpu_transpose = torch transpose cpu_x mps_transpose = torch transpose mps_x cpu assertEqual cpu_transpose mps_transpose Test sign test_sign helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ sign_result = torch sign x sign_result_cpu = torch sign cpu_x cpu_grad = torch ones_like sign_result_cpu grad = cpu_grad mps sign_result backward gradient=grad sign_result_cpu backward gradient=cpu_grad assertEqual sign_result sign_result_cpu helper test_signbit helper shape dtype cpu_x = torch randn shape device= cpu dtype x = cpu_x clone mps signbit_result = torch signbit x signbit_result_cpu = torch signbit cpu_x assertEqual signbit_result signbit_result_cpu helper torch int helper torch float helper torch int Test neg test_neg helper shape cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ neg_result = torch neg x neg_result_cpu = torch neg cpu_x cpu_grad = torch ones_like neg_result_cpu grad = cpu_grad mps neg_result backward gradient=grad neg_result_cpu backward gradient=cpu_grad assertEqual neg_result neg_result_cpu helper test_neg_strided_input See https github com pytorch pytorch issues #issuecomment- x = torch arange device= mps reshape y = x permute z = y + y neg assertEqual z abs max item Test index add test_index_add helper shape dim index source_shape alpha x_dtype=torch float idx_dtype=torch int cpu_x = torch randn shape device= cpu dtype=x_dtype requires_grad=False x = cpu_x detach clone mps cpu_idx = torch tensor index device= cpu dtype=idx_dtype idx = cpu_idx detach clone mps cpu_source = torch randn source_shape device= cpu dtype=x_dtype requires_grad=False source = cpu_source detach clone mps idx_result = torch index_add x dim=dim index=idx source=source alpha=alpha idx_result_cpu = torch index_add cpu_x dim=dim index=cpu_idx source=cpu_source alpha=alpha assertEqual idx_result idx_result_cpu helper helper helper helper helper helper - test result dim= helper helper test float helper x_dtype=torch float test_index_ bit Test index operations work Gb+ tensors Cleanup memory gc collect torch mps empty_cache Check index operations work +GB tensors x = torch rand device= mps assertGreater x element_size x numel idx = torch arange device= mps x_sampled = x idx assertEqual x x_sampled Reclaim memory after running tests del x gc collect torch mps empty_cache test_mm_large Test MM works matrices index larger than K x = torch rand device= mps y = torch rand device= mps This used crash error subRange start less than length dimension See https github com pytorch pytorch issues #issuecomment- assertNotEqual torch mm x y abs max item compare_mm m n k dtype=torch float x = torch rand m n device= mps dtype=dtype y = torch rand n k device= mps dtype=dtype z = torch mm x y cpu z_cpu = torch mm x cpu y cpu assertEqual z z_cpu Used produce incorrect results MPS M running MacOS correct Metal compare_mm one more time dimensions inverted see https github com pytorch pytorch issues #issuecomment- compare_mm Test bfloat mm compare_mm torch bfloat unittest skipIf total_memory _ _ _ Needs least Gb RAM run test unittest skipIf IS_CI May fixes https github com pytorch pytorch issues test_copy_large Test copy Gb+ tensors works x = torch ones + dtype=torch float y = x device= mps assertTrue torch all y == torch tensor device= mps del y del x Test flip test_flip helper shape dims cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps flip_result = torch flip x dims=dims flip_result_cpu = torch flip cpu_x dims=dims assertEqual flip_result flip_result_cpu helper helper helper helper - empty dims helper input numel == helper input numel == helper none dims needs flipped helper Test index select test_index_select helper shape dim index idx_dtype=torch int cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps cpu_idx = torch tensor index device= cpu dtype=idx_dtype idx = cpu_idx detach clone mps idx_result = torch index_select x dim=dim index=idx idx_result_cpu = torch index_select cpu_x dim=dim index=cpu_idx assertEqual idx_result idx_result_cpu helper helper helper helper helper helper - helper helper test_index_copy_non_contiguous helper shape dim index dest_cpu = torch randn shape dest = dest_cpu clone mps dest_cpu = dest_cpu transpose dest = dest transpose dim = dim == dim == dim src_shape = list dest_cpu shape src_shape dim = len index src_cpu = torch randn src_shape src = src_cpu clone mps idx_cpu = torch tensor index dtype=torch long idx_mps = idx_cpu clone mps dest_cpu index_copy_ dim idx_cpu src_cpu dest index_copy_ dim idx_mps src assertEqual dest dest_cpu test_cases = - args test_cases helper args test_index_select_scalar helper value dim index idx_dtype=torch int cpu_x = torch tensor value device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps cpu_idx = torch tensor index device= cpu dtype=idx_dtype idx = cpu_idx detach clone mps idx_result = torch index_select x dim=dim index=idx idx_result_cpu = torch index_select cpu_x dim=dim index=cpu_idx assertEqual idx_result idx_result_cpu helper assertRaisesRegex RuntimeError Index scalar can have only value helper test_embedding_dense_backward helper n d m idx embeddingMPS = nn Embedding n d max_norm=True device= mps embedding_weight = embeddingMPS weight detach cpu W_MPS = torch randn m d requires_grad=True device= mps idx_MPS = torch tensor idx device= mps a_MPS = embeddingMPS weight clone W_MPS t weight must cloned differentiable a_MPS retain_grad b_MPS = embeddingMPS idx_MPS W_MPS t modifies weight in-place b_MPS retain_grad out_MPS = a_MPS unsqueeze + b_MPS loss_MPS = out_MPS sigmoid prod loss_MPS backward embeddingCPU = nn Embedding n d max_norm=True _weight=embedding_weight W_CPU = W_MPS cpu idx_CPU = torch tensor idx a_CPU = embeddingCPU weight clone W_CPU t weight must cloned differentiable a_CPU retain_grad b_CPU = embeddingCPU idx_CPU W_CPU t modifies weight in-place b_CPU retain_grad out_CPU = a_CPU unsqueeze + b_CPU loss_CPU = out_CPU sigmoid prod loss_CPU backward assertEqual b_CPU grad b_MPS grad assertEqual a_CPU grad a_MPS grad helper helper verify changes shape would cause cached graph lookup problems helper test scalar index Test pytorch gather test_gather helper shape dim idx_shape idx_dtype=torch int cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ Indices should taken range axis along which gathering done idx_np = np random randint shape dim idx_shape cpu_idx = torch tensor idx_np device= cpu dtype=idx_dtype idx = cpu_idx detach clone mps gather_result = torch gather x dim=dim index=idx gather_result_cpu = torch gather cpu_x dim=dim index=cpu_idx cpu_grad = torch randn idx_shape device= cpu dtype=torch float grad = cpu_grad mps gather_result backward gradient=grad gather_result_cpu backward gradient=cpu_grad assertEqual gather_result gather_result_cpu assertEqual cpu_x grad x grad helper helper helper helper helper helper helper helper helper Test pytorch gather test_gather_scalar idx_dtype = torch int cpu_x = torch tensor device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ idx_np = cpu_idx = torch tensor idx_np device= cpu dtype=idx_dtype idx = cpu_idx detach clone mps gather_result = torch gather x dim= index=idx gather_result_cpu = torch gather cpu_x dim= index=cpu_idx cpu_grad = torch randn device= cpu dtype=torch float grad = cpu_grad mps gather_result backward gradient=grad gather_result_cpu backward gradient=cpu_grad assertEqual gather_result gather_result_cpu assertEqual cpu_x grad x grad Test pytorch scatter_add scatter test_scatter_add helper shape dim idx_shape src_shape idx_dtype=torch int do_add=True cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ cpu_src = torch randn src_shape device= cpu dtype=torch float requires_grad=True src = cpu_src detach clone mps requires_grad_ Indices should taken range axis along which gathering done idx_np = None do_add idx_np = np random randint shape dim idx_shape idx_np = np array cpu_idx = torch tensor idx_np device= cpu dtype=idx_dtype idx = cpu_idx detach clone mps scatter_result = None scatter_result_cpu = None do_add scatter_result = torch scatter_add x dim=dim index=idx src=src scatter_result_cpu = torch scatter_add cpu_x dim=dim index=cpu_idx src=cpu_src scatter_result = torch scatter x dim=dim index=idx src=src scatter_result_cpu = torch scatter cpu_x dim=dim index=cpu_idx src=cpu_src cpu_grad = None grad = None idx_shape == src_shape cpu_grad = torch randn shape device= cpu dtype=torch float grad = cpu_grad mps scatter_result backward gradient=grad scatter_result_cpu backward gradient=cpu_grad assertEqual scatter_result scatter_result_cpu idx_shape == src_shape assertEqual cpu_x grad x grad assertEqual cpu_src grad src grad helper helper helper helper helper helper helper helper helper helper helper helper helper Test scatter src helper do_add=False helper do_add=False Test pytorch scatter_add scatter scalar input test_scatter_add_scalar helper idx_dtype=torch int do_add=True cpu_x = torch tensor device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ cpu_src = torch tensor device= cpu dtype=torch float requires_grad=True src = cpu_src detach clone mps requires_grad_ Indices should taken range axis along which gathering done idx_np = cpu_idx = torch tensor idx_np device= cpu dtype=idx_dtype idx = cpu_idx detach clone mps scatter_result = None scatter_result_cpu = None do_add scatter_result = torch scatter_add x dim= index=idx src=src scatter_result_cpu = torch scatter_add cpu_x dim= index=cpu_idx src=cpu_src scatter_result = torch scatter x dim= index=idx src=src scatter_result_cpu = torch scatter cpu_x dim= index=cpu_idx src=cpu_src cpu_grad = None grad = None cpu_grad = torch tensor device= cpu dtype=torch float grad = cpu_grad mps scatter_result backward gradient=grad scatter_result_cpu backward gradient=cpu_grad assertEqual scatter_result scatter_result_cpu assertEqual cpu_x grad x grad assertEqual cpu_src grad src grad helper helper do_add=False Test pytorch scatter_reduce test_scatter_reduce helper shape dim idx_shape src_shape idx_dtype=torch int reduce_str= sum cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ cpu_src = torch randn src_shape device= cpu dtype=torch float requires_grad=True src = cpu_src detach clone mps requires_grad_ Indices should taken range axis along which gathering done idx_np = np random randint shape dim idx_shape cpu_idx = torch tensor idx_np device= cpu dtype=idx_dtype idx = cpu_idx detach clone mps scatter_result = torch scatter x dim=dim index=idx src=src reduce=reduce_str scatter_result_cpu = torch scatter cpu_x dim=dim index=cpu_idx src=cpu_src reduce=reduce_str assertEqual scatter_result scatter_result_cpu reduce sum prod amax amin reduce_type add multiply helper reduce_str=reduce_type helper reduce_str=reduce_type helper reduce_str=reduce_type helper reduce_str=reduce_type helper reduce_str=reduce_type helper reduce_str=reduce_type helper reduce_str=reduce_type helper reduce_str=reduce_type helper reduce_str=reduce_type helper reduce_str=reduce_type helper reduce_str=reduce_type helper reduce_str=reduce_type helper reduce_str=reduce_type test_is_nonzero assertFalse torch is_nonzero torch tensor mps assertTrue torch is_nonzero torch tensor mps assertFalse torch is_nonzero torch tensor False mps assertTrue torch is_nonzero torch tensor mps Test triu test_triu helper shape diag= cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ triu_result = torch triu x diag triu_result_cpu = torch triu cpu_x diag cpu_grad = torch randn triu_result_cpu shape grad = cpu_grad mps triu_result backward gradient=grad triu_result_cpu backward gradient=cpu_grad assertEqual triu_result triu_result_cpu assertEqual x grad cpu_x grad helper helper diag= helper diag= helper diag= helper diag=- helper diag=- helper diag=- Test inplace x_mps = torch arange device= mps reshape t triu x_cpu = torch arange device= cpu reshape t triu assertEqual x_cpu x_mps assertEqual x_cpu stride x_mps stride Test inverse test_inverse helper n atol= e- rtol= e- cpu_input = torch randn n n device= cpu mps_input = cpu_input mps cpu_result = torch linalg inv cpu_input mps_result = torch linalg inv mps_input assertEqual cpu_result mps_result atol=atol rtol=rtol helper helper helper helper helper atol= e- Test tril test_tril helper shape diag= cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ tril_result = torch tril x diag tril_result_cpu = torch tril cpu_x diag cpu_grad = torch randn tril_result_cpu shape grad = cpu_grad mps tril_result backward gradient=grad tril_result_cpu backward gradient=cpu_grad assertEqual tril_result tril_result_cpu assertEqual x grad cpu_x grad diag - - - helper diag=diag helper_nans_infs value diag_vals= - For nans infs mps_tensor = torch full value device= mps cpu_tensor = torch full value device= cpu diag diag_vals mps_result = torch tril mps_tensor diagonal=diag cpu_result = torch tril cpu_tensor diagonal=diag assertEqual mps_result cpu_result f Mismatch diag= diag helper_nans_infs float inf helper_nans_infs float -inf helper_nans_infs float nan test eye test_eye helper n m dtype cpu_result = None result = None n == m cpu_result = torch eye n dtype=dtype device= cpu result = torch eye n dtype=dtype device= mps cpu_result = torch eye n m device= cpu result = torch eye n m device= mps assertEqual result cpu_result dtype torch bool torch float torch float torch uint torch int torch int torch int helper dtype helper dtype helper dtype helper dtype helper dtype helper dtype Test diag test_diag helper shape diag= cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ diag_result = torch diag x diag diag_result_cpu = torch diag cpu_x diag cpu_grad = torch randn diag_result_cpu shape grad = cpu_grad mps diag_result backward gradient=grad diag_result_cpu backward gradient=cpu_grad assertEqual diag_result diag_result_cpu assertEqual x grad cpu_x grad shape diag - - - - helper shape diag=diag Test linspace test_linspace helper start end steps dtype=torch float cpu_result = torch tensor np linspace start end steps dtype=dtype result = torch linspace start end steps dtype=dtype device= mps assertEqual cpu_result result dtype torch float torch int torch uint torch int helper dtype helper dtype helper dtype helper dtype Test argange test_arange assertEqual np arange torch arange device= mps assertEqual np arange - torch arange - device= mps assertEqual np arange dtype=np float torch arange device= mps assertEqual np arange dtype=np float torch arange device= mps do_arange start= end= dtype=torch bfloat device= cpu torch arange start end device=device dtype=dtype assertEqual do_arange device= mps do_arange device= cpu test_arange_empty out_mps = torch tensor device= mps out_cpu = torch tensor device= cpu y_mps = torch arange out=out_mps y_cpu = torch arange out=out_cpu assertEqual y_mps y_cpu Test rgange test_range assertEqual np arange dtype=np float torch range device= mps assertEqual np arange - dtype=np float torch range - device= mps assertEqual np array dtype=np float torch range device= mps assertEqual np arange dtype=np float torch arange device= mps Test softmax test_softmax helper shape dim channels_last=False cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=True channels_last cpu_x = cpu_x memory_format=torch channels_last cpu_x retain_grad x = cpu_x detach clone mps requires_grad_ softmax_result = torch nn functional softmax x dim=dim softmax_result_cpu = torch nn functional softmax cpu_x dim=dim Currently NOT testing backward channels last backward cpu_grad = None grad = None channels_last cpu_grad = torch randn shape device= cpu dtype=torch float grad = cpu_grad mps softmax_result backward gradient=grad softmax_result_cpu backward gradient=cpu_grad assertEqual softmax_result softmax_result_cpu channels_last assertEqual x grad cpu_x grad helper dim cpu_x = torch tensor device= cpu dtype=torch float requires_grad=True x = cpu_x detach clone mps requires_grad_ softmax_result = torch nn functional softmax x dim=dim softmax_result_cpu = torch nn functional softmax cpu_x dim=dim cpu_grad = torch tensor device= cpu dtype=torch float grad = cpu_grad mps softmax_result backward gradient=grad softmax_result_cpu backward gradient=cpu_grad assertEqual softmax_result softmax_result_cpu assertEqual x grad cpu_x grad helper channels_last False shape len shape = channels_last continue dim - - - helper shape dim channels_last test_nan_to_num inputCPU = torch tensor float nan float inf -float inf inputMPS = inputCPU detach clone mps requires_grad_ outputCPU = torch nan_to_num inputCPU nan= posinf= neginf=- outputMPS = torch nan_to_num inputMPS nan= posinf= neginf=- assertEqual outputMPS outputCPU Test where test_where helper shape x_shape y_shape cond_dtype=torch bool x_dtype=torch float cpu_cond = torch randint shape device= cpu dtype=cond_dtype requires_grad=False cond = cpu_cond detach clone mps cpu_x = torch randn x_shape device= cpu dtype=x_dtype requires_grad=True x = cpu_x detach clone mps requires_grad_ cpu_y = torch randn y_shape device= cpu dtype=x_dtype requires_grad=True y = cpu_y detach clone mps requires_grad_ cpu_out = torch where cpu_cond cpu_x cpu_y out = torch where cond x y cpu_grad = torch randn cpu_out shape grad = cpu_grad mps cpu_out backward gradient=cpu_grad out backward gradient=grad assertEqual out cpu_out assertEqual x grad cpu_x grad assertEqual y grad cpu_y grad shape helper shape shape shape helper helper helper helper helper helper helper helper helper Test output correctly resizes TODO Remove me when out OpInfo testing enabled MPS output = torch tensor device= mps cond = torch randint dtype=torch bool device= mps inp = torch rand device= mps other = torch rand device= mps out = torch where cond inp other out=output assertEqual id out id output assertEqual out shape Test normal test_normal helper shape mean= std= dtype=torch float mps_out = torch normal mean std shape device= mps dtype=dtype mean_array = np ones shape mean_array = mean cpu_mean_tensor = torch tensor mean_array device= cpu dtype=dtype requires_grad=False mean_tensor = cpu_mean_tensor detach clone mps std_array = np ones shape std_array = std cpu_std_tensor = torch tensor std_array device= cpu dtype=dtype requires_grad=False std_tensor = cpu_std_tensor detach clone mps test out mps_out = torch zeros shape device= mps dtype=dtype torch normal mean_tensor std out=mps_out mps_out = torch zeros shape device= mps dtype=dtype torch normal mean std_tensor out=mps_out mps_out = torch zeros shape device= mps dtype=dtype torch normal mean_tensor std_tensor out=mps_out test without out mps_out = torch normal mean_tensor std assertEqual mps_out size mean_tensor size mps_out = torch normal mean std_tensor assertEqual mps_out size std_tensor size inferred_shape = torch broadcast_shapes mean_tensor size std_tensor size mps_out = torch normal mean_tensor std_tensor assertEqual mps_out size inferred_shape helper helper helper dtype=torch bfloat Test invalid inputs assertRaises TypeError helper dtype=torch int test_bernoulli shape = all_ones = torch ones shape device= mps all_zeros = torch zeros shape device= mps prob_tensor = all_ones probability drawing mps_out = torch bernoulli prob_tensor We can t check reliably mean std Just make sure we don t constant values assertNotEqual mps_out cpu mean assertNotEqual mps_out cpu std probability drawing mps_out = torch bernoulli all_zeros assertEqual mps_out all_zeros probability drawing mps_out = torch bernoulli all_ones assertEqual mps_out all_ones Check works different dtypes dtype torch float torch int torch int torch int torch int mps_out = torch zeros shape device= mps dtype=dtype bernoulli Check output all zeros ones uniq = mps_out unique assertEqual uniq torch arange device= mps dtype=dtype parametrize dtype torch float torch bfloat torch float test_dropout dtype shapes = _ p_list = shape p train itertools product shapes p_list False True input = torch randn shape device= mps dtype=dtype requires_grad=True output mask = torch native_dropout input p train=train p_actual_mps = - mask sum mask numel train assertEqual p_actual_mps p atol= e- rtol= e- assertTrue output mask logical_not == all assertEqual output mask input mask - p assertEqual output input assertTrue mask all output_grad = torch randn_like output output backward output_grad grad_scale = p == - p train assertEqual input grad output_grad mask grad_scale assertEqual input grad output_grad test_mps_generator explicit manual seeding creating MPS Generator g_mps = torch Generator device= mps g_mps manual_seed mps_x = torch randn device= mps generator=g_mps g_mps manual_seed generate random numbers offset ` ` mps_y = torch randn device= mps generator=g_mps seed values same so random tensor contents should match assertEqual mps_x mps_y save generator s state offset = restore later g_state = g_mps get_state generate random numbers offset ` ` mps_x = torch randn device= mps generator=g_mps case random results must differ last generated random results assertNotEqual mps_x mps_y mps_x produced g_state we use our reference mps_y mps_y = mps_x restore previously saved state results should match again g_mps set_state g_state mps_x = torch randn device= mps generator=g_mps assertEqual mps_x mps_y serialTest test_default_mps_generator manual seeding default MPS generator using global torch manual_seed torch manual_seed mps_x = torch randn device= mps manual seeding using torch mps manual_seed which should set default MPS generator like global torch manual_seed torch mps manual_seed generate random numbers offset ` ` mps_y = torch randn device= mps seed values same so random tensor contents should match assertEqual mps_x mps_y save default generator s state offset = restore later g_state = torch mps get_rng_state generate random numbers offset ` ` mps_x = torch randn device= mps case random results must differ last generated random results assertNotEqual mps_x mps_y since we called randn twice after seeding offset should assertEqual torch mps _get_default_mps_generator get_offset mps_x produced g_state we use our reference mps_y mps_y = mps_x restore previously saved state default MPS generator results should match again torch mps set_rng_state g_state mps_x = torch randn device= mps assertEqual mps_x mps_y test_device_synchronize just running some ops each followed synchronize wait MPS stream finish running each them net = torch nn ConvTranspose d kernel_size= stride= padding= output_padding= \ device= mps dtype=torch float x = torch rand device= mps dtype=torch float requires_grad=True torch mps synchronize x = net x torch mps synchronize x backward torch randn_like x torch mps synchronize serialTest test_mps_allocator_module first garbage collect empty cached blocks gc collect torch mps empty_cache measure memory allocations MPSAllocator current_alloc_before = torch mps current_allocated_memory after garbage collection emptying cache current_allocated_memory must zero assertEqual current_alloc_before measure total memory allocations Metal driver driver_alloc_before = torch mps driver_allocated_memory allocate new MB tensor force allocation new Metal Heap x = torch ones device= mps get memory allocations after allocating tensor x current_alloc_after = torch mps current_allocated_memory driver_alloc_after = torch mps driver_allocated_memory current driver memory allocations must have grown point assertGreater current_alloc_after current_alloc_before assertGreater driver_alloc_after driver_alloc_before test_mps_allocator_stats max_memory = torch mps recommended_max_memory print f Recommended Max Memory max_memory GB assertGreater max_memory verify test run XCode Instruments Metal System Trace Logging tool press record then run python test press stop Next expand os_signposts- PyTorchMPS check events intervals logged like example aten mps_convolution_backward_input f f id=G run= test_mps_profiler_module torch mps profiler profile mode= event wait_until_completed=False p just running some ops capture OS Signposts traces profiling net = torch nn ConvTranspose d kernel_size= stride= padding= output_padding= \ device= mps dtype=torch float x = torch rand device= mps dtype=torch float requires_grad=True x = net x torch mps profiler start mode= interval wait_until_completed=True just running some ops capture OS Signposts traces profiling x = torch rand device= mps dtype=torch float requires_grad=True x = net x torch mps profiler stop test_mps_event_module startEvent = torch mps Event enable_timing=True startEvent record net = torch nn ConvTranspose d kernel_size= stride= padding= output_padding= \ device= mps dtype=torch float x = torch rand device= mps dtype=torch float requires_grad=True x = net x endEvent = torch mps Event enable_timing=True endEvent record elapsedTime = startEvent elapsed_time endEvent assertGreater elapsedTime test_generic_event startEvent = torch Event mps enable_timing=True startEvent record net = torch nn ConvTranspose d kernel_size= stride= padding= output_padding= \ device= mps dtype=torch float x = torch rand device= mps dtype=torch float requires_grad=True x = net x endEvent = torch Event mps enable_timing=True endEvent record elapsedTime = startEvent elapsed_time endEvent assertGreater elapsedTime test_generic_device_synchronize event = torch Event mps = torch randn b = torch randn c = + b a_acc = mps non_blocking=True b_acc = b mps non_blocking=True event record event synchronize c_acc = a_acc + b_acc event record torch accelerator synchronize assertTrue event query assertEqual c_acc cpu c test_jit_save_load m = torch nn Module m x = torch rand device= mps buffer = io BytesIO torch jit save torch jit script m buffer buffer seek n = torch jit load buffer assertEqual n x m x Test random_ random_ random_ test_random helper shape low high dtype=torch int mps_out = torch randint low high shape dtype=dtype device= mps We can t check reliably mean std Just make sure we don t constant values assertNotEqual mps_out float mean item assertNotEqual mps_out float std item helper helper helper dtype=torch float helper dtype=torch int helper dtype=torch bool Test random_ dtype torch bool torch int torch uint torch int torch float torch float x = torch empty dtype=dtype device= mps x random_ assertNotEqual x max item test_random_ d See https github com pytorch pytorch issues FB shape = x = torch rand shape device= mps assertNotEqual x x Check normal distributions affected same y = torch normal torch zeros shape device= mps torch ones shape device= mps assertNotEqual y y test_random_ops_noncontiguous Test random in-place operations non-contiguous tensors All random in-place operations should work non-contiguous tensors See issues Test each random in-place operation ops = normal_ lambda t t normal_ uniform_ lambda t t uniform_ exponential_ lambda t t exponential_ bernoulli_ lambda t t bernoulli_ random_ lambda t t random_ random_with_to lambda t t random_ random_with_range lambda t t random_ name op_func ops subTest operation=name Create non-contiguous tensor via transpose t_mps = torch zeros device= mps T clone assertFalse t_mps is_contiguous f name tensor should non-contiguous Apply operation op_func t_mps Verify tensor modified all zeros max_val = t_mps max item assertNotEqual max_val f name operation failed modify non-contiguous tensor Test rand_like specifically issue t = torch ones device= mps permute assertFalse t is_contiguous rand_like input should non-contiguous result = torch rand_like t assertFalse result is_contiguous rand_like result should non-contiguous assertNotEqual result max item rand_like should generate non-zero values Test exponential unittest skip This does test anything test_exponential helper shape lambda_ dtype=torch float mps_out = torch zeros shape device= mps dtype=dtype mps_out exponential_ lambda_ print mps_out cpu float mean lambda_ print mps_out cpu float std lambda_ dtype torch float torch float helper dtype helper dtype helper dtype helper dtype test_exponential_ rate = torch randn abs requires_grad_ rate_ d = torch randn abs requires_grad_ assertEqual Exponential rate sample size assertEqual Exponential rate sample size assertEqual Exponential rate_ d sample size assertEqual Exponential rate_ d sample size assertEqual Exponential sample size assertEqual Exponential sample size parametrize dtype torch float torch bfloat torch float test_exponential_nonzero dtype _ range = torch empty _ device= mps dtype=dtype exponential_ assertTrue = all Test add test_add_sub helper shape alpha op_name inplace op_name == add op = torch Tensor add_ inplace torch add op_name == sub op = torch Tensor sub_ inplace torch sub dtype torch float torch float cpu_x = torch randn shape device= cpu dtype=dtype requires_grad=False mps_x = cpu_x detach clone mps cpu_y = torch randn shape device= cpu dtype=dtype requires_grad=False mps_y = cpu_y detach clone mps cpu_out = op cpu_x cpu_y alpha=alpha mps_out = op mps_x mps_y alpha=alpha fp isn t accurate when alpha passed TODO remove fix tol when we fix problems fp tol = e- dtype torch float None assertEqual mps_out cpu_out rtol=tol atol=tol cpu_y shape = inplace in-place output cannot broadcasted create scalar tensor cpu_s = torch tensor device= cpu dtype=dtype requires_grad=False mps_s = cpu_s detach clone mps primary tensor scalar assertEqual op cpu_s cpu_y op mps_s mps_y create scalar tensor cpu_s = torch tensor device= cpu dtype=dtype requires_grad=False mps_s = cpu_s detach clone mps secondary tensor scalar assertEqual op cpu_x cpu_s op mps_x mps_s rtol=tol atol=tol op_name inplace product add sub True False helper op_name inplace helper op_name inplace helper op_name inplace helper op_name inplace helper op_name inplace helper op_name inplace Test float int alpha See https github com pytorch pytorch issues x = torch rand device= mps dtype=torch float y = torch arange device= mps dtype=torch int assertEqual torch add x y alpha= cpu torch add x cpu y cpu alpha= assertEqual torch add x alpha= cpu torch add x cpu alpha= Regression test https github com pytorch pytorch issues assertEqual torch add y x alpha= cpu torch add y cpu x cpu alpha= Test add test_add_scalars helper alpha dtype torch float torch float cpu_x = torch tensor device= cpu dtype=dtype requires_grad=False x = cpu_x detach clone mps cpu_y = torch tensor device= cpu dtype=dtype requires_grad=False y = cpu_y detach clone mps cpu_out = torch add cpu_x cpu_y alpha=alpha out = torch add x y alpha=alpha fp isn t accurate when alpha passed tol = e- dtype torch float None assertEqual out cpu_out rtol=tol atol=tol helper helper helper helper Test int tensor + int scalar add see https github com pytorch pytorch issues #issuecomment- x = torch ones dtype=torch int device= mps assertEqual x + torch full dtype=torch int device= mps assertTrue torch equal x + torch full device= mps test_types_binary_op Float Bool cpu_x = torch arange dtype=torch float device= cpu torch tensor True False True False True device= cpu mps_x = torch arange dtype=torch float device= mps torch tensor True False True False True device= mps assertEqual cpu_x mps_x Float Int cpu_y = torch arange dtype=torch float device= cpu torch tensor device= cpu mps_y = torch arange dtype=torch float device= mps torch tensor device= mps assertEqual cpu_y mps_y test_unary_ops helper shape op dtypef torch float cpu_x = torch randn shape device= cpu dtype=dtypef requires_grad=False mps_x = cpu_x detach clone mps assertEqual op cpu_x op mps_x dtypei torch int torch int cpu_x = torch randint shape device= cpu dtype=dtypei requires_grad=False mps_x = cpu_x mps assertEqual op cpu_x op mps_x rtol= e- atol= e- test slice dtypef torch float cpu_x = torch randn shape device= cpu dtype=dtypef requires_grad=False mps_x = cpu_x detach clone mps cpu_slice = cpu_x mps_slice = mps_x assertEqual op cpu_slice op mps_slice test view dtypef torch float cpu_x = torch randn shape device= cpu dtype=dtypef requires_grad=False mps_x = cpu_x detach clone mps create view tensor reducing rd th dimension combined_dim = shape - shape - reshaped_dims = list shape - + combined_dim cpu_view = cpu_x view reshaped_dims mps_view = mps_x view reshaped_dims assertEqual op cpu_view op mps_view helper torch exp helper torch exp helper torch expm helper torch log helper torch cos helper torch erfinv test_non_dense_in_storage_unary_ops helper op dtypef torch float cpu_x = torch randn device= cpu dtype=dtypef requires_grad=False mps_x = cpu_x detach clone mps assertEqual op cpu_x op mps_x dtypei torch int torch int torch int cpu_x = torch randint device= cpu size= dtype=dtypei requires_grad=False mps_x = cpu_x mps assertEqual op cpu_x op mps_x rtol= e- atol= e- helper torch exp helper torch exp helper torch expm helper torch log helper torch cos test_unary_ops_storage_offset_strided helper shape op inplace dtype=torch float test in-place storage_offset cpu_x = torch randn shape device= cpu dtype=dtype mps_x = cpu_x detach clone mps y = op mps_x cpu_y = op cpu_x assertEqual y cpu_y See https github com pytorch pytorch issues inplace cpu_x = torch randn shape device= cpu dtype=dtype mps_x = cpu_x detach clone mps cpu_y = torch empty shape device= cpu dtype=dtype t mps_y = cpu_y detach clone mps op cpu_x out=cpu_y op mps_x out=mps_y assertEqual mps_y cpu_y test non contiguous dense input output similar strides cpu_x = torch randn shape device= cpu dtype=dtype mT mps_x = cpu_x mps cpu_y = torch empty_like cpu_x mps_y = cpu_y mps op cpu_x out=cpu_y op mps_x out=mps_y assertEqual mps_y cpu_y test sliced inputs outputs similar strides mps_x mps_y = torch randn shape shape device= mps dtype=dtype unbind op mps_x out=mps_y assertEqual mps_y op mps_x contiguous helper torch exp False helper torch cos False helper torch neg False helper torch tanh False helper torch tanh_ True helper lambda x kwargs torch round x decimals= kwargs False test_atan helper shape input_cpu = torch randn shape input_mps = input_cpu detach clone mps other_cpu = torch randn shape other_mps = other_cpu detach clone mps atan _cpu = torch atan input_cpu other_cpu atan _mps = torch atan input_mps other_mps assertEqual atan _cpu atan _mps cpu helper helper helper unittest skip This does test anything test_multinomial Test num_dist = helper probs compare_mean compare_var num_samples= replacement=True cpu_prob_tensor = torch tensor probs device= cpu dtype=torch float requires_grad=False prob_tensor = cpu_prob_tensor detach clone mps mps_out = torch multinomial prob_tensor num_samples replacement=replacement replacement print mps_out cpu Compare real theoretical values print mps_out cpu float mean compare_mean print mps_out cpu float std compare_var TODO Add tests data types helper np array + - helper np array + + + + - helper np array + + + + - helper np array + + + + - helper np array False test_non_contiguous_sampling_variation torch manual_seed transpose so s made non-contiguous probs = torch tensor T mps samples = torch multinomial probs flatten item _ range we should get different samples rather than same value repeated indicating sampling working properly non-contiguous tensors assertNotEqual len samples test_cumsum_dim_check x = torch rand device= mps assertEqual x cumsum x cumsum - assertEqual x cumsum x cumsum - assertRaises IndexError lambda x cumsum assertRaises IndexError lambda x cumsum - test_cumprod_dim_check x = torch rand device= mps assertEqual x cumprod x cumprod - assertEqual x cumprod x cumprod - assertRaises IndexError lambda x cumprod assertRaises IndexError lambda x cumprod - test_do_sync_thrice_its_all_right Regression test https github com pytorch pytorch commit bc d cdb d f d d d That caused sync calls deadlock x = torch nextafter torch ones device= mps torch zeros device= mps _ range torch mps synchronize assertLess x sum item x numel parametrize dtype torch int torch int torch int torch int torch uint test_inplace_bitwise_not dtype Start bitwise here reported qqaatw x_mps x_cpu = torch arange device=device dtype=dtype device cpu mps x x_mps x_cpu x bitwise_not_ assertEqual x_mps cpu x_cpu test_empty_posneginf just check doesnt crash input_tensor = torch empty device= mps out_pos = torch isposinf input_tensor out_neg = torch isposinf input_tensor assertEqual out_pos numel assertEqual out_neg numel test_empty_dot just check doesnt crash = torch rand device= mps b = torch rand device= mps assertEqual dot b cpu dot b cpu TestLargeTensors TestCaseMPS serialTest test_ bit_binops torch mps recommended_max_memory _ _ _ raise unittest SkipTest Needs least Gb RAM = torch rand dtype=torch float device= mps b = torch rand dtype=torch float device= mps rc = + b sin slice_idx = - rc_slice = rc slice_idx rc_slice_cpu = cpu + b cpu slice_idx sin assertEqual rc_slice rc_slice_cpu serialTest test_ bit_index_select torch mps recommended_max_memory _ _ _ raise unittest SkipTest Needs least Gb RAM B N = x = torch empty B N N dtype=torch float device= mps i range B x i = i batch_idx = torch tensor device= mps y = x batch_idx assertEqual y item Reclaim memory after running tests del y del x gc collect torch mps empty_cache serialTest test_rand_ b_raises int _max = torch iinfo torch int max assertRaises RuntimeError This used crash NDArray dimension length INT_MAX x = torch randint int _max + dtype=torch int device= mps x = torch randint int _max dtype=torch int device= mps assertEqual x numel int _max del x TestLogical TestCaseMPS _wrap_tensor x device= cpu dtype=None requires_grad=False torch tensor x device=device dtype=dtype requires_grad=requires_grad test_logical_not helper x cpu_x = x x = cpu_x detach clone mps result = torch logical_not x result_cpu = torch logical_not cpu_x assertEqual result result_cpu helper _wrap_tensor helper _wrap_tensor dtype=torch float requires_grad=True helper _wrap_tensor True True False False helper _wrap_tensor helper _wrap_tensor helper _wrap_tensor True helper _wrap_tensor False test_logical_and helper x other cpu_x = x x = cpu_x detach clone mps cpu_other = other other = cpu_other detach clone mps result = torch logical_and x other result_cpu = torch logical_and cpu_x cpu_other assertEqual result result_cpu helper _wrap_tensor _wrap_tensor helper _wrap_tensor dtype=torch float requires_grad=True _wrap_tensor dtype=torch float helper _wrap_tensor True True False False _wrap_tensor True False False True helper _wrap_tensor _wrap_tensor helper _wrap_tensor _wrap_tensor helper _wrap_tensor _wrap_tensor True helper _wrap_tensor _wrap_tensor False test_logical_or helper x other cpu_x = x x = cpu_x detach clone mps cpu_other = other other = cpu_other detach clone mps result = torch logical_or x other result_cpu = torch logical_or cpu_x cpu_other assertEqual result result_cpu helper _wrap_tensor _wrap_tensor helper _wrap_tensor dtype=torch float requires_grad=True _wrap_tensor dtype=torch float helper _wrap_tensor True True False False _wrap_tensor True False False True helper _wrap_tensor _wrap_tensor helper _wrap_tensor _wrap_tensor helper _wrap_tensor _wrap_tensor True helper _wrap_tensor _wrap_tensor False test_logical_xor helper x other cpu_x = x x = cpu_x detach clone mps cpu_other = other other = cpu_other detach clone mps result = torch logical_xor x other result_cpu = torch logical_xor cpu_x cpu_other assertEqual result result_cpu helper _wrap_tensor _wrap_tensor helper _wrap_tensor dtype=torch float requires_grad=True _wrap_tensor dtype=torch float helper _wrap_tensor True True False False _wrap_tensor True False False True helper _wrap_tensor _wrap_tensor helper _wrap_tensor _wrap_tensor helper _wrap_tensor _wrap_tensor True helper _wrap_tensor _wrap_tensor False parametrize dtype torch float torch float torch int torch int torch uint torch int torch bool test_min_max dtype _ range dtype == torch float dtype == torch float x = torch randn device= mps dtype=dtype x = torch randint device= mps dtype=dtype x_cpu = x cpu y = x max y_cpu = x_cpu max assertEqual y y_cpu z = x min z_cpu = x_cpu min assertEqual z z_cpu parametrize dtype torch float torch float torch bfloat test_min_max_nan_propagation dtype cpu_x = torch tensor float nan device= cpu dtype=dtype mps_x = cpu_x detach clone mps cpu_max = torch max cpu_x mps_max = torch max mps_x cpu cpu_amax = torch amax cpu_x mps_amax = torch amax mps_x cpu cpu_min = torch min cpu_x mps_min = torch min mps_x cpu cpu_amin = torch amin cpu_x mps_amin = torch amin mps_x cpu assertEqual cpu_max mps_max assertEqual cpu_amax mps_amax assertEqual cpu_min mps_min assertEqual cpu_amin mps_amin test_isin helper dtype shapes = shape_tuple shapes inverted True False dtype is_floating_point Half supported CPU isin Compute reference FP A = torch randn size=shape_tuple device= cpu dtype=torch float B = torch randn size=shape_tuple device= cpu dtype=torch float A = torch randint size=shape_tuple device= cpu dtype=dtype B = torch randint size=shape_tuple device= cpu dtype=dtype A_mps = A detach clone mps B_mps = B detach clone mps cpu_ref = torch isin A B invert=inverted dtype torch float torch bfloat cpu_ref type dtype mps_out = torch isin A_mps B_mps invert=inverted assertEqual mps_out cpu_ref dtypes = torch float torch float torch bfloat torch int torch int torch uint torch int helper dtype dtype dtypes Mixed dtypes see https github com pytorch pytorch issues x = torch arange device= mps y = torch tensor device= mps dtype=torch float assertEqual torch isin x y torch tensor False True False True device= mps Tensor Scalar variant aliases eq covered OpInfo assertEqual torch isin x torch tensor False False True False device= mps assertEqual torch isin x invert=True torch tensor True False True True device= mps assertEqual torch isin x torch tensor False False False False device= mps Scalar Tensor variant alaises Scalar Scalar covered OpInfo assertEqual torch isin x torch tensor True device= mps test_isin_asserts C = torch randn size= device= mps dtype=torch float D = torch randn size= device= cpu dtype=torch float assertRaisesRegex RuntimeError Expected elements is_mps out = torch isin C D parametrize dtype torch int torch int torch int torch int torch uint torch bool test_shifts dtype x = make_tensor device= mps dtype=dtype dtype torch bool x = torch iinfo dtype max x = torch iinfo dtype min x_cpu = x cpu assertEqual x cpu x_cpu assertEqual x cpu x_cpu Regression test https github com pytorch pytorch issues x = x clamp x_cpu = x cpu assertEqual x cpu x_cpu assertEqual x cpu x_cpu TestSmoothL Loss TestCaseMPS parametrize reduction none mean sum parametrize requires_grad False True test_smooth_l _loss reduction requires_grad helper sizes CPU input_cpu = torch randn sizes requires_grad=requires_grad target_cpu = torch randn sizes MPS input_mps = input_cpu detach clone mps requires_grad_ target_mps = target_cpu detach clone mps smooth_l _loss_cpu = F smooth_l _loss input_cpu target_cpu beta= reduction=reduction smooth_l _loss_mps = F smooth_l _loss input_mps target_mps beta= reduction=reduction assertEqual smooth_l _loss_cpu smooth_l _loss_mps requires_grad reduction == none grad_cpu = torch zeros_like smooth_l _loss_cpu grad_mps = grad_cpu mps smooth_l _loss_cpu backward grad_cpu smooth_l _loss_mps backward grad_mps smooth_l _loss_cpu backward smooth_l _loss_mps backward assertEqual input_cpu grad input_mps grad cpu helper helper helper helper TestNLLLoss TestCaseMPS test_nll_loss_mismatched_batch device= mps x = torch randn requires_grad=True device=device t should have size t = torch zeros dtype=torch int device=device assertRaisesRegex ValueError Expected batch_size F nll_loss x t test_nll_loss_out_of_bounds_ignore_index test_nll_loss_out_of_bounds_ignore_index_helper device output = x = torch tensor device=device t = torch tensor dtype=torch int device=device t = torch tensor - dtype=torch int device=device reduction mean none out bound ignore_index output append F nll_loss x t ignore_index= reduction=reduction default ignore_index output append F nll_loss x t reduction=reduction output output_cpu = test_nll_loss_out_of_bounds_ignore_index_helper device= cpu output_mps = test_nll_loss_out_of_bounds_ignore_index_helper device= mps cpu mps zip output_cpu output_mps assertEqual cpu mps test_nll_loss_invalid_target_dim _test_nll_loss_invalid_target_dim device output = x = torch tensor device=device t = torch zeros dtype=torch int device=device assertRaisesRegex RuntimeError D target tensor expected F nll_loss x t _test_nll_loss_invalid_target_dim device= cpu _test_nll_loss_invalid_target_dim device= mps test_nll_loss_invalid_weights _test_nll_loss_invalid_weights device x = torch tensor device=device t = torch tensor dtype=torch int device=device invalid_weights = torch zeros device=device torch zeros device=device msg = weight tensor should defined either all classes no classes weight invalid_weights assertRaisesRegex RuntimeError msg F nll_loss x t weight=weight _test_nll_loss_invalid_weights device= cpu _test_nll_loss_invalid_weights device= mps _nll_loss_helper input_size reduction expected CPU input = torch rand input_size requires_grad=True device= cpu num_channels = input_size target_size = input_size + tuple input_size target = torch randint num_channels target_size device= cpu weights = torch randn num_channels MPS input_mps = input detach clone mps requires_grad_ target_mps = target detach clone mps weights_mps = weights mps output_cpu = F nll_loss input target weight=weights reduction=reduction output_mps = F nll_loss input_mps target_mps weight=weights_mps reduction=reduction assertEqual output_cpu output_mps cpu output_cpu sum backward output_mps sum backward assertEqual input grad input_mps grad cpu _nll_loss_ d_helper input_size reduction CPU input = torch rand input_size requires_grad=True device= cpu num_channels = input_size target = torch randint num_channels device= cpu MPS input_mps = input detach clone mps requires_grad_ target_mps = target detach clone mps output_cpu = F nll_loss input target reduction=reduction output_mps = F nll_loss input_mps target_mps reduction=reduction assertEqual output_cpu output_mps cpu output_cpu sum backward output_mps sum backward assertEqual input grad input_mps grad cpu test_nll_loss_ d device= cpu _nll_loss_ d_helper none _nll_loss_ d_helper mean _nll_loss_ d_helper sum test_nll_loss_empty_tensor_reduction_none device= cpu _nll_loss_helper none torch empty device=device _nll_loss_helper none torch empty device=device _nll_loss_helper none torch empty device=device _nll_loss_helper none torch empty device=device _nll_loss_helper none torch empty device=device test_nll_loss_empty_tensor_reduction_mean device= cpu nan = torch tensor float nan device=device _nll_loss_helper mean nan _nll_loss_helper mean nan _nll_loss_helper mean nan _nll_loss_helper mean nan _nll_loss_helper mean nan test_nll_loss_empty_tensor_reduction_sum device= cpu zero = torch tensor device=device _nll_loss_helper sum zero _nll_loss_helper sum zero _nll_loss_helper sum zero _nll_loss_helper sum zero _nll_loss_helper sum zero test_nll_loss_byte_target_matches_long device= cpu N C = input = torch randn N C device=device requires_grad=True target = torch empty N dtype=torch long device=device random_ C compute_result_and_gradient reduction target_dtype result grad = dev cpu mps input_dev = input dev input_ = input_dev detach input_ requires_grad_ target_dev = target dev prob = F log_softmax input_ dim=- loss = nn NLLLoss reduction=reduction result dev = loss prob target_dev target_dtype result dev sum backward grad dev = input_ grad result grad reduction none mean sum result_long grad_long = compute_result_and_gradient reduction torch long result_byte grad_byte = compute_result_and_gradient reduction torch uint assertEqual result_long mps cpu result_long cpu assertEqual grad_long mps cpu grad_long cpu test_nll_loss_backward Copy-n-pasted similar test_torchinductor py test Used crash ` error mps divide op requires same element type all operands results ` labels = torch zeros dtype=torch int device= mps torch tensor - - - - dtype=torch int device= mps label labels inp = torch rand device= mps dtype=torch half grad_out = torch empty device=inp device dtype=inp dtype total_weight = torch tensor device=inp device torch ops aten nll_loss_backward grad_out inp label None - total_weight TestTopK TestCase _test_topk shape largest cpu_x = torch randn shape device= cpu dtype=torch float requires_grad=False x = cpu_x detach clone mps isinstance shape tuple curr_dim dim_size enumerate shape k range dim_size + topk_values topk_indices = torch topk x k dim=curr_dim largest=largest topk_values_cpu topk_indices_cpu = torch topk cpu_x k dim=curr_dim largest=largest assertEqual topk_values topk_values_cpu assertEqual topk_indices topk_indices_cpu k range shape topk_values topk_indices = torch topk x k dim= largest=largest topk_values_cpu topk_indices_cpu = torch topk cpu_x k dim= largest=largest assertEqual topk_values topk_values_cpu assertEqual topk_indices topk_indices_cpu test_topk largest_vals = True False shapes = Zero Element Tensors Multiple Element Tensors shape shapes largest_val largest_vals subTest shape=shape largest_val=largest_val _test_topk shape largest_val test_topk_gt_ d = torch ones dtype=torch float mps try t_mps = torch ops aten topk k= dim= except Exception e e_string = str e assertEqual e_string On-going issue MPSGraph topk when ndims - axis see issue TestNNMPS NNTestCase _create_basic_net Layer nn Module __init__ - None super __init__ layer_dummy_param = Parameter torch empty layer_dummy_buf = Buffer torch zeros Net nn Module __init__ - None super __init__ l = Layer dummy_param = Parameter torch empty dummy_buf = Buffer torch zeros l = Layer n = Net s = nn Sequential n n l n s test_requires_grad_ m = _create_basic_net - assert len list m buffers invalid test assert all b requires_grad b m buffers invalid test assert len list m parameters invalid test assert all p requires_grad p m parameters invalid test requires_grad False True assertIs m requires_grad_ requires_grad m p m parameters assertEqual p requires_grad requires_grad b m buffers assertFalse b requires_grad test_module_backcompat torch serialization SourceChangeWarning path = download_file https download pytorch org test_data linear pt warnings catch_warnings warnings simplefilter ignore SourceChangeWarning weights_only=False legacy use case loads module m = torch load path weights_only=False input = torch randn dtype=torch float assertEqual m input size test_conv_backcompat torch serialization SourceChangeWarning This file generated running PyTorch Python torch torch nn m = nn Conv d torch save m legacy_conv d pt NB This Pickle also contains some Unicode data path = download_file https download pytorch org test_data legacy_conv d pt warnings catch_warnings warnings simplefilter ignore SourceChangeWarning weights_only=False legacy use case loads module m = torch load path encoding= utf- weights_only=False input = torch randn dtype=torch float assertEqual m input size test_conv_expand device = mps input_ = torch rand device=device kernel = torch rand device=device tmp_kernel = kernel expand - - - output = F conv d input_ tmp_kernel groups= padding= stride= The test should crash test_permute M_cpu = torch randn M_mps = M_cpu mps output_cpu = M_cpu permute output_mps = M_mps permute assertEqual output_cpu output_mps assertEqual output_cpu size output_mps size Printing non_contiguous should crash test_print_non_contiguous print obj equivalent calling ` x=str obj print x ` Use assertTrue case make sure non-empty string returned assertTrue str torch ones device= mps nonzero assertTrue str torch ones device= mps nonzero contiguous test_zero_grad i = torch randn requires_grad=True module = nn Linear p module parameters p requires_grad = False module zero_grad module weight requires_grad = True module zero_grad assertIsNone module weight grad uninitialized grad module i sum backward assertIsNotNone module weight grad assertGreater module weight grad data abs sum module zero_grad assertIsNone module weight grad module bias requires_grad = True module zero_grad assertIsNone module weight grad assertIsNone module bias grad module i sum backward assertIsNotNone module weight grad assertIsNotNone module bias grad assertGreater module weight grad data abs sum assertGreater module bias grad data abs sum Force set zeros module zero_grad set_to_none=False assertEqual module weight grad data module weight data clone zero_ assertEqual module bias grad data module bias data clone zero_ module zero_grad assertIsNone module weight grad assertIsNone module bias grad test_no_grad dtype torch bfloat torch float torch double module = nn Conv d kernel_size= padding= dtype input = torch randn dtype x = input y = input clone output = module x assertTrue output requires_grad output backward torch ones torch no_grad output = module y assertFalse output requires_grad assertRaises RuntimeError lambda output backward torch ones test_invalid_conv d dtype torch bfloat torch float torch double module = nn Conv d in_channels= out_channels= kernel_size= stride= bias=True dtype input = torch randn dtype assertRaisesRegex RuntimeError r Calculated padded input size per channel \ \ + r Kernel size \ \ Kernel size can\ t greater than actual input size module input Negative stride check module = nn Conv d in_channels= out_channels= kernel_size= stride=- bias=True dtype input = torch randn dtype assertRaisesRegex RuntimeError non-positive stride supported module input test_conv d_discontiguous_weight Test https github com pytorch pytorch issues x = torch ones weight = torch arange reshape assertFalse weight is_contiguous y = torch nn functional conv d x weight None torch backends mkldnn is_available Disable MKLDNN explicitly so either NNPACK THCNN will used torch backends mkldnn flags enabled=False y_ = torch nn functional conv d x weight None assertEqual y y_ assertEqual y sum test_invalid_conv d dtype torch bfloat torch float torch double module = torch nn Conv d kernel_size= dilation= stride= dtype input = torch empty dtype assertRaises RuntimeError lambda module input module = nn Conv d in_channels= out_channels= kernel_size= stride= bias=True input = torch randn assertRaisesRegex RuntimeError r Calculated padded input size per channel \ x \ + r Kernel size \ x \ Kernel size can\ t greater than actual input size module input Negative stride check module = nn Conv d in_channels= out_channels= kernel_size= stride=- bias=True dtype input = torch randn dtype assertRaisesRegex RuntimeError non-positive stride supported module input Zero stride check module = nn Conv d in_channels= out_channels= kernel_size= stride= bias=True dtype input = torch randn dtype assertRaisesRegex RuntimeError non-positive stride supported module input Input weights different devices assertRaisesRegex RuntimeError must same device lambda torch conv d torch rand torch rand device= mps assertRaisesRegex RuntimeError Input type \\ MPSFloatType\\ weight type \\ torch\\ FloatTensor\\ should same lambda torch conv d torch rand device= mps torch rand test_conv d_valid_padding device= mps Test F conv d padding= valid same no padding x = torch rand device=device torch float y = torch rand device=device torch float expect = F conv d x y actual = F conv d x y padding= valid assertEqual expect cpu actual cpu test_conv d_backward_collision Test https github com pytorch pytorch issues x = torch rand device= mps requires_grad=True m = nn Conv d stride= padding= mps m = nn Conv d stride= padding= mps y y = m x m x assertEqual y shape y shape y sum backward This used crash MPSNDArrayConvolutionA mm failed assertion y sum backward test_conv d_backward_collision Conv D only available MacOS onwards x = torch rand device= mps requires_grad=True m = nn Conv d stride= padding= mps m = nn Conv d stride= padding= mps y y = m x m x assertEqual y shape y shape y sum backward This used crash MPSNDArrayConvolutionA mm failed assertion y sum backward Regression test https github com pytorch pytorch issues test_conv d_channels_last_ d m_cpu = nn Conv d stride= padding= device= cpu m_mps = copy deepcopy m_cpu mps x_cpu = torch randn device= cpu memory_format=torch channels_last_ d x_mps = x_cpu detach clone mps res_cpu = m_cpu x_cpu res_mps = m_mps x_mps assertEqual res_cpu res_mps test_gemm_permute_transpose batch_size = n = hidden = num_attention_heads = attention_head_size = hidden num_attention_heads transpose_for_scores x torch Tensor - torch Tensor new_x_shape = x size - + num_attention_heads attention_head_size x = x view new_x_shape x permute attention key workaround=False device key = transpose_for_scores key res = key transpose - - res A = torch randn batch_size n hidden A_mps = A detach clone mps r = attention A device= cpu r = attention A_mps device= mps r _cpu = r cpu assertEqual r r _cpu test_group_norm_backward device= mps See https github com pytorch pytorch issues more detail shape = x = torch full shape device=device target = torch ones device=device conv_in = nn Conv d kernel_size= stride= padding= device=device conv_out = nn Conv d kernel_size= stride= padding= device=device norm = nn GroupNorm eps= e- affine=True device=device torch enable_grad x = x detach requires_grad_ out = x out = conv_in out out = out + norm out out = out + norm out out = out + norm out out = F interpolate out scale_factor= mode= nearest out = norm out out = conv_out out loss = out - target norm dim=- sum grad = -torch autograd grad loss x assertFalse grad detach isnan any item NaN gradients returned autograd test_conv d_same_padding device= mps x = torch rand device=device y = torch rand device=device expect = F conv d x y padding= actual = F conv d x y padding= same assertEqual expect cpu actual cpu With dilation y = torch rand device=device expect = F conv d x y padding= dilation= actual = F conv d x y padding= same dilation= assertEqual expect actual Dilation asymmetric padding y = torch rand device=device expect = F conv d x y padding= dilation= actual = F conv d x y padding= same dilation= assertEqual expect actual TestPad TestCaseMPS test_constant_pad m = torch nn ConstantPad d - - - - input_cpu = torch randn input_mps = input_cpu detach clone mps r_cpu = m input_cpu r_mps = m input_mps assertEqual r_cpu r_mps cpu Arbitrary input dimensions pad = value = input_cpu = torch randn input_mps = input_cpu detach clone mps r_cpu = F pad input_cpu pad=pad value=value r_mps = F pad input_mps pad=pad value=value assertEqual r_cpu r_mps cpu test_circular_pad https github com pytorch pytorch issues k_cpu = torch ones k_mps = k_cpu detach clone mps x_cpu = torch rand x_mps = x_cpu detach clone mps x_pad_cpu = F pad x_cpu mode= circular x_pad_mps = F pad x_mps mode= circular y_cpu = F conv d x_pad_cpu k_cpu y_mps = F conv d x_pad_mps k_mps assertEqual y_cpu y_mps cpu test_constant_pad_ d_warning inputCPU = torch rand inputMPS = inputCPU detach clone mps outputCPU = F pad inputCPU outputMPS = F pad inputMPS assertEqual outputCPU outputMPS test_pad helper shape padding op value= inputCPU = torch randn shape device= cpu dtype=torch float requires_grad=True inputCPU retain_grad inputMPS = inputCPU detach clone mps requires_grad_ op nn ConstantPad d nn ConstantPad d nn ConstantPad d padCriteria = op padding value padCriteria = op padding outputCPU = padCriteria inputCPU outputMPS = padCriteria inputMPS assertEqual outputCPU outputMPS backward pass chose just have grad_output = outputCPU backward gradient=torch full_like outputCPU outputMPS backward gradient=torch full_like outputMPS assertEqual inputCPU grad inputMPS grad D Padding helper nn ReflectionPad d verify change shape input would cause problems graph caching helper nn ReflectionPad d Replication D helper nn ReplicationPad d Constant Pad D helper nn ConstantPad d Constant Pad D single dimension input helper nn ConstantPad d D Padding helper nn ReflectionPad d verify change shape input would cause problems graph caching helper nn ReflectionPad d should make padding helper nn ReplicationPad d verify change shape padding would cause problems graph caching helper nn ReplicationPad d Constant Pad D helper nn ConstantPad d input size pad size helper nn ConstantPad d pad dims input dims helper nn ConstantPad d pad dims == input dims helper nn ConstantPad d input numel == output numel helper nn ConstantPad d pad dims input dims - helper nn ConstantPad d D Padding helper nn ReflectionPad d verify change shape padding would cause problems graph caching helper nn ReplicationPad d case where input_d == pad_front back ReplicationPad d helper nn ReplicationPad d Constant Pad D helper nn ConstantPad d input size pad size helper nn ConstantPad d check workaround right padding bug Monterey helper nn ConstantPad d test_constant_pad_nd_preserves_memory_format nchw_tensor = torch rand nchw_padded = torch constant_pad_nd nchw_tensor assertTrue nchw_padded is_contiguous memory_format=torch contiguous_format nhwc_tensor = nchw_tensor contiguous memory_format=torch channels_last nhwc_padded = torch constant_pad_nd nhwc_tensor assertTrue nhwc_padded is_contiguous memory_format=torch channels_last test_constant_pad_nd_with_empty_pad Empty constant pad no-op See https github com pytorch pytorch issues input_mps = torch randn device= mps output_mps = torch constant_pad_nd input_mps assertEqual output_mps input_mps TestLinalgMPS TestCaseMPS _test_addmm_addmv f t m v alpha=None beta=None transpose_out=False dtype = t dtype numpy_dtype = dtype alpha = alpha None alpha beta = beta None beta res = f t m v alpha=alpha beta=beta res = torch full_like res math nan transpose_out res = res t clone memory_format=torch contiguous_format t f t m v alpha=alpha beta=beta out=res res = alpha m numpy_dtype cpu numpy v numpy_dtype cpu numpy beta = res += torch mul t beta numpy_dtype cpu numpy res = torch from_numpy res dtype assertEqual res res assertEqual res res test_addmm device= mps dtype=torch float M = torch randn device=device dtype m = torch randn device=device dtype m = torch randn device=device dtype _test_addmm_addmv torch addmm M m m Test beta= M=nan M = torch full math nan device=device dtype m = torch randn device=device dtype m = torch randn device=device dtype _test_addmm_addmv torch addmm M m m beta= Test transpose t t t t itertools product True False repeat= maybe_transpose cond m cond m m t clone memory_format=torch contiguous_format t M = maybe_transpose t torch randn device=device dtype m = maybe_transpose t torch randn device=device dtype m = maybe_transpose t torch randn device=device dtype _test_addmm_addmv torch addmm M m m transpose_out=t _test_addr f t m v alpha=None beta=None dtype = t dtype numpy_dtype = dtype alpha = alpha None alpha beta = beta None beta res = f t m v alpha=alpha beta=beta res = alpha np outer m numpy_dtype cpu numpy v numpy_dtype cpu numpy beta = res += torch mul t beta numpy_dtype cpu numpy res = torch from_numpy res dtype assertEqual res res test_addr device= mps dtype=torch float M = torch randn device=device dtype m = torch randn device=device dtype m = torch randn device=device dtype _test_addr torch addr M m m Test beta= M=nan M = torch full math nan device=device dtype m = torch randn device=device dtype m = torch randn device=device dtype _test_addr torch addr M m m beta= test_matrix_rank device= mps dtype=torch float matrix_rank = torch linalg matrix_rank run_test shape shape batch = torch randn batch shape shape dtype=dtype device=device rank_a = matrix_rank assertEqual rank_a matrix_rank mH aaH = torch matmul mH rank_aaH = matrix_rank aaH rank_aaH_hermitian = matrix_rank aaH hermitian=True assertEqual rank_aaH rank_aaH_hermitian aHa = torch matmul mH assertEqual matrix_rank aHa matrix_rank aHa hermitian=True check against NumPy assertEqual rank_a np linalg matrix_rank cpu numpy assertEqual matrix_rank np linalg matrix_rank cpu numpy assertEqual rank_aaH np linalg matrix_rank aaH cpu numpy assertEqual matrix_rank aaH np linalg matrix_rank aaH cpu numpy hermitian flag NumPy added np lib NumpyVersion np __version__ = assertEqual rank_aaH_hermitian np linalg matrix_rank aaH cpu numpy hermitian=True assertEqual matrix_rank aaH True np linalg matrix_rank aaH cpu numpy True check out= variant out = torch empty shape - dtype=torch int device=device ans = matrix_rank out=out assertEqual ans out assertEqual ans rank_a shapes = batches = shape shape batch zip itertools product shapes reversed shapes batches escape only when NotImplementedError downstream function raised TODO remove once required function implemented try run_test shape shape batch except NotImplementedError e assertRaisesRegex NotImplementedError The operator aten _linalg_svd U currently implemented MPS device raise e test_pinv device= mps dtype=torch float precision= e- torch testing _internal common_utils random_hermitian_pd_matrix run_test_main A hermitian Testing against definition pseudo-inverses A_pinv = torch linalg pinv A hermitian=hermitian np_A = A cpu numpy np_A_pinv = A_pinv cpu numpy A numel assertEqual A np_A np_A_pinv np_A atol=precision rtol=precision assertEqual A_pinv np_A_pinv np_A np_A_pinv atol=precision rtol=precision assertEqual np_A np_A_pinv np_A np_A_pinv conj swapaxes - - atol=precision rtol=precision assertEqual np_A_pinv np_A np_A_pinv np_A conj swapaxes - - atol=precision rtol=precision assertEqual A shape A_pinv shape - + A_pinv shape - A_pinv shape - Check out= variant out = torch empty_like A_pinv ans = torch linalg pinv A hermitian=hermitian out=out assertEqual ans out assertEqual ans A_pinv run_test_numpy A hermitian Check against NumPy output Test float rcond specific value each matrix rconds = float torch rand Test different types rcond tensor rcond_type MPS_DTYPES TODO Figure out why s supported complex Skip test bfloat numpy does support type rcond_type is_complex rcond_type == torch bfloat continue rconds append torch rand A shape - dtype=torch float device=device rcond_type Test broadcasting rcond A ndim rconds append torch rand A shape - device=device rcond rconds actual = torch linalg pinv A rcond=rcond hermitian=hermitian torch_rtol = torch linalg pinv A rtol=rcond hermitian=hermitian assertEqual actual torch_rtol atol=precision rtol=precision numpy_rcond = rcond isinstance rcond float rcond cpu numpy expected = np linalg pinv A cpu numpy rcond=numpy_rcond hermitian=hermitian assertEqual actual expected atol=precision rtol=precision sizes square matrices fat matrices thin matrices zero numel matrices A = torch randn sizes dtype=dtype device=device hermitian = False run_test_main A hermitian run_test_numpy A hermitian Check hermitian = True sizes square matrices zero numel square matrices A = random_hermitian_pd_matrix sizes - sizes - dtype=dtype device=device hermitian = True escape only when NotImplementedError downstream function raised TODO remove once required function implemented try run_test_main A hermitian except NotImplementedError e assertRaisesRegex NotImplementedError The operator aten _linalg_eigh eigenvalues currently implemented MPS device raise e try run_test_numpy A hermitian except NotImplementedError e assertRaisesRegex NotImplementedError The operator aten _linalg_eigh eigenvalues currently implemented MPS device raise e parametrize m parametrize n parametrize q_group parametrize num_groups test__int _mm m n q_group num_groups k = q_group num_groups inner_k_tiles = torch manual_seed a_f = torch rand m k device= mps b_f = torch rand k n device= mps convert_weight_to_int pack b b_int b_scales_and_zeros = _group_quantize_tensor b n_bit= q_group_size=q_group b_scales_and_zeros = b_scales_and_zeros mps b_int pack = torch _convert_weight_to_int pack b_int inner_k_tiles b_int pack b_scales_and_zeros weight_int pack_mm b_int pack b_scales_and_zeros torch _weight_int pack_mm b_int pack q_group b_scales_and_zeros b_int pack b_scales_and_zeros_f = convert_weight_to_int pack b_f dtype torch float torch float torch bfloat = a_f dtype=dtype b = b_f dtype=dtype b_scales_and_zeros = b_scales_and_zeros_f dtype=dtype ref = torch mm b res = weight_int pack_mm b_int pack b_scales_and_zeros mean_err = res - ref abs ref mean assertLess mean_err parametrize m parametrize k parametrize n test__int _mm m k n torch manual_seed a_f = torch rand m k device= mps b_f = torch rand n k device= mps convert_weight_to_int pack b b_int pack b_scales _ = _dynamically_quantize_per_channel b - torch int b_int pack b_scales weight_int pack_mm b_int pack b_scales torch _weight_int pack_mm b_int pack b_scales b_int pack b_scales_f = convert_weight_to_int pack b_f dtype torch float torch float torch bfloat = a_f dtype=dtype b = b_f dtype=dtype b_scales = b_scales_f dtype=dtype res = weight_int pack_mm b_int pack b_scales ref = torch mm b transpose mean_err = res - ref abs ref mean assertLess mean_err TestSDPA TestCaseMPS _compare_tensors y ref denom = torch maximum ref abs torch tensor e- device=ref device dtype=ref dtype err = y - ref abs denom mean item assertLess err _test_sdpa_no_mask is_causal bool dtype torch dtype L int = S int = NH int = HS int = requires_grad bool = False torch manual_seed torch nn attention sdpa_kernel torch nn attention SDPBackend MATH q = torch randn NH L HS dtype=dtype device= mps requires_grad=requires_grad k = torch randn NH S HS dtype=q dtype device= mps v = torch randn NH S HS dtype=q dtype device= mps q_cpu = q cpu detach cpu requires_grad_ requires_grad k_cpu = k cpu v_cpu = v cpu y = F scaled_dot_product_attention q k v dropout_p= is_causal=is_causal y_ref = F scaled_dot_product_attention q_cpu k_cpu v_cpu dropout_p= is_causal=is_causal _compare_tensors y cpu y_ref requires_grad torch is_grad_enabled y sum backward y_ref sum backward _compare_tensors q grad cpu q_cpu grad test_sdpa_no_mask_no_causal_fp _test_sdpa_no_mask False torch float test_sdpa_no_mask_no_causal_fp _test_sdpa_no_mask False torch float test_sdpa_no_mask_causal_fp _test_sdpa_no_mask True torch float test_sdpa_no_mask_causal_fp _test_sdpa_no_mask True torch float test_sdpa_no_mask_causal_fp _L _test_sdpa_no_mask True torch float test_sdpa_no_mask_causal_fp _L _S _test_sdpa_no_mask True torch float test_sdpa_no_mask_causal_fp _L _S _NH _HS _test_sdpa_no_mask True torch float test_sdpa_no_mask_no_causal_fp _grad _test_sdpa_no_mask False torch float requires_grad=True torch no_grad _test_sdpa_no_mask False torch float requires_grad=True _test_sdpa_mask dtype torch dtype L int = S int = NH int = HS int = torch manual_seed causal_mask = torch tril torch ones S S dtype=torch bool device= mps torch nn attention sdpa_kernel torch nn attention SDPBackend MATH i = q = torch randn NH L HS dtype=dtype device= mps k = torch randn NH S HS dtype=q dtype device= mps v = torch randn NH S HS dtype=q dtype device= mps input_pos = torch tensor i dtype=torch int device= mps mask = causal_mask None None input_pos y = F scaled_dot_product_attention q k v attn_mask=mask dropout_p= is_causal=False y_ref = F scaled_dot_product_attention q cpu k cpu v cpu attn_mask=mask cpu dropout_p= is_causal=False _compare_tensors y cpu y_ref test_sdpa_mask_fp _test_sdpa_mask torch float Test twice catch https github com pytorch pytorch issues _test_sdpa_mask torch float test_sdpa_mask_fp _test_sdpa_mask torch float test_sdpa_mask_fp _L _test_sdpa_mask torch float test_sdpa_mask_fp _L _S _NH _HS _test_sdpa_mask torch float Regression test https github com pytorch pytorch issues parametrize dtype torch float torch float test_sdpa_full_mask dtype q = torch randn dtype=dtype k = torch randn dtype=dtype v = torch randn dtype=dtype mask = torch tensor False False True True dtype=torch bool out_cpu = F scaled_dot_product_attention q k v attn_mask=mask out_mps = F scaled_dot_product_attention q mps k mps v mps attn_mask=mask mps _compare_tensors out_mps cpu out_cpu parametrize dtype torch float torch float test_sdpa_ d_input dtype head_num seq_len embed_dim = q = torch randn head_num seq_len embed_dim dtype=dtype k = torch randn head_num seq_len embed_dim dtype=dtype v = torch randn head_num seq_len embed_dim dtype=dtype attention_mask = torch ones seq_len seq_len dtype=dtype torch nn attention sdpa_kernel torch nn attention SDPBackend MATH y = F scaled_dot_product_attention q mps k mps v mps attention_mask mps dropout_p= y_ref = F scaled_dot_product_attention q cpu k cpu v cpu attention_mask cpu dropout_p= _compare_tensors y cpu y_ref parametrize dtype torch float torch float test_sdpa_no_mask_ d dtype torch dtype B int = extra int = NH int = L int = HS int = requires_grad bool = False torch manual_seed q = torch randn B extra NH L HS dtype=dtype device= mps requires_grad=requires_grad k = torch randn B extra NH L HS dtype=dtype device= mps v = torch randn B extra NH L HS dtype=dtype device= mps torch nn attention sdpa_kernel torch nn attention SDPBackend MATH y = F scaled_dot_product_attention q k v dropout_p= is_causal=False y_ref = F scaled_dot_product_attention q cpu k cpu v cpu dropout_p= is_causal=False _compare_tensors y cpu y_ref requires_grad torch is_grad_enabled y sum backward y_ref sum backward _compare_tensors q grad cpu q cpu grad parametrize dtype torch float torch float test_sdpa_mask_ d dtype torch dtype B int = extra int = NH int = L int = HS int = torch manual_seed q = torch randn B extra NH L HS dtype=dtype device= mps k = torch randn B extra NH L HS dtype=dtype device= mps v = torch randn B extra NH L HS dtype=dtype device= mps mask = torch tril torch ones L L dtype=torch bool device= mps unsqueeze unsqueeze torch nn attention sdpa_kernel torch nn attention SDPBackend MATH y = F scaled_dot_product_attention q k v attn_mask=mask dropout_p= is_causal=False y_ref = F scaled_dot_product_attention q cpu k cpu v cpu attn_mask=mask cpu dropout_p= is_causal=False _compare_tensors y cpu y_ref parametrize dtype torch float torch float parametrize is_causal True False test_sdpa_enable_gqa dtype is_causal q_heads = key_heads = L = S = HS = q = torch randn q_heads L HS dtype=dtype device= mps k = torch randn key_heads S HS dtype=dtype device= mps v = torch randn key_heads S HS dtype=dtype device= mps y_ref = F scaled_dot_product_attention q cpu k cpu v cpu dropout_p= is_causal=is_causal enable_gqa=True torch nn attention sdpa_kernel torch nn attention SDPBackend MATH y = F scaled_dot_product_attention q k v dropout_p= is_causal=is_causal enable_gqa=True _compare_tensors y cpu y_ref serialTest test_sdpa_fp _no_memory_leak get_mps_memory_usage torch mps current_allocated_memory torch mps driver_allocated_memory batch_size seq_len num_heads head_dim = query = torch randn batch_size num_heads seq_len head_dim device= mps dtype=torch float key = torch randn batch_size num_heads seq_len head_dim device= mps dtype=torch float value = torch randn batch_size num_heads seq_len head_dim device= mps dtype=torch float memory_footprints = _ range output = F scaled_dot_product_attention query key value current_mem driver_mem = get_mps_memory_usage memory_footprints append current_mem driver_mem MB different maximum allowed value could decreased even more torch testing assert_close memory_footprints - memory_footprints atol= rtol= generate_qkv batch int NH int q_len int s_len int head_dim int layout str dtype torch dtype layout == contiguous q = torch randn batch NH q_len head_dim dtype=dtype device= mps k = torch randn batch NH s_len head_dim dtype=dtype device= mps layout == mT Transpose head dimension length q = torch randn batch NH head_dim q_len dtype=dtype device= mps mT k = torch randn batch NH head_dim s_len dtype=dtype device= mps mT layout == transpose_seq_head Transpose length number heads q = torch randn batch q_len NH head_dim dtype=dtype device= mps transpose k = torch randn batch s_len NH head_dim dtype=dtype device= mps transpose layout == permute Permute head dimension length q = torch randn batch head_dim NH q_len dtype=dtype device= mps permute k = torch randn batch head_dim NH s_len dtype=dtype device= mps permute raise ValueError f Unknown layout layout v = torch randn batch NH s_len head_dim dtype=dtype device= mps q k v run_fast_attention_test q torch Tensor k torch Tensor v torch Tensor with_mask bool dropout_p float = is_causal bool = False q_len = q shape s_len = k shape with_mask attn_mask = torch zeros q shape q shape q_len s_len dtype=torch bool device=q device attn_mask s_len = True torch nn attention sdpa_kernel torch nn attention SDPBackend MATH y = F scaled_dot_product_attention q k v attn_mask=attn_mask dropout_p=dropout_p is_causal=is_causal y_ref = F scaled_dot_product_attention q cpu k cpu v cpu attn_mask=attn_mask cpu dropout_p=dropout_p is_causal=is_causal torch nn attention sdpa_kernel torch nn attention SDPBackend MATH y = F scaled_dot_product_attention q k v dropout_p=dropout_p is_causal=is_causal y_ref = F scaled_dot_product_attention q cpu k cpu v cpu dropout_p=dropout_p is_causal=is_causal _compare_tensors y cpu y_ref parametrize dtype torch float torch float parametrize layout contiguous mT transpose_seq_head permute parametrize head_dim fast kernel parametrize with_mask True False test_fast_vector_attention dtype torch dtype layout str head_dim int with_mask bool torch manual_seed batch = NH = q_len = so vector fast eligible s_len = smaller than so we use one  pass variant q k v = generate_qkv batch NH q_len s_len head_dim layout dtype run_fast_attention_test q k v with_mask parametrize dtype torch float float underflows sometimes which leads flaky tests parametrize layout contiguous mT transpose_seq_head permute parametrize with_mask True False test_fast_vector_attention_ pass dtype torch dtype layout str with_mask bool torch manual_seed batch = NH = q_len = s_len = large enough trigger two  pass path head_dim = supported head dimension vector attention q k v = generate_qkv batch NH q_len s_len head_dim layout dtype run_fast_attention_test q k v with_mask unittest skip Full attention fast kernel implemented yet parametrize dtype torch float torch float parametrize layout contiguous mT parametrize head_dim fast kernel parametrize with_mask True False test_fast_full_attention dtype torch dtype layout str head_dim int with_mask bool torch manual_seed batch = NH = q_len = threshold trigger full fast attention path s_len = q k v = generate_qkv batch NH q_len s_len head_dim layout dtype run_fast_attention_test q k v with_mask TestSDPAMetaDispatchMode TorchDispatchMode TorchDispatchMode which intercepts _scaled_dot_product_attention_math_for_mps aten operator check meta kernel correct __init__ test test = test super __init__ __torch_dispatch__ func types args kwargs=None kwargs = kwargs res = func args kwargs func = torch ops aten _scaled_dot_product_attention_math_for_mps default res meta_args meta_kwargs = pytree tree_map_only torch Tensor lambda t t device= meta args kwargs meta_res = func meta_args meta_kwargs format_res res t shape t stride t dtype isinstance t torch Tensor t t pytree tree_flatten res Format output so we only look tensor metadata test assertEqual format_res res format_res meta_res res create_sdpa_meta_test Creates new which takes every test TestSDPA adds TestSDPAMetaDispatchMode context order test scaled_dot_product_attention_for_mps meta kernel This allows us test all branches sdpa op If there changes sdpa kernel without changing meta kernel torch compile guard will catch issue necessarily export orig_test_cls = TestSDPA new_test_cls = type f orig_test_cls __name__ Meta orig_test_cls __bases__ new_test_cls __qualname__ = new_test_cls __name__ name dir orig_test_cls name startswith test_ fn = getattr orig_test_cls name callable fn setattr new_test_cls name getattr orig_test_cls name continue new_name = f name _meta new_fn args kwargs TestSDPAMetaDispatchMode fn args kwargs new_fn __name__ = new_name setattr new_test_cls new_name new_fn hasattr new_test_cls name setattr new_test_cls name getattr orig_test_cls name new_test_cls TestSDPAMeta = create_sdpa_meta_test instantiate_parametrized_tests TestSDPAMeta TestGatherScatter TestCaseMPS test_slicing_with_step Slicing step https github com pytorch pytorch issues x_mps = torch zeros dtype=torch float device= mps x_mps = x_cpu = torch zeros dtype=torch float device= cpu x_cpu = assertEqual x_cpu x_mps test_cast_gather_scatter _ range input = np random randint size= dtype=np uint torch no_grad s = torch tensor input dtype=torch uint device= mps unsqueeze s_cpu = torch tensor input dtype=torch uint device= cpu unsqueeze s = s long s_cpu = s_cpu long assertEqual s cpu s_cpu s = s float s_cpu = s_cpu float assertEqual s cpu s_cpu s = s_cpu = assertEqual s cpu s_cpu test_slicing_replace_column https github com pytorch pytorch issues _helper tensor_data x_cpu = torch tensor tensor_data x_mps = x_cpu mps x_cpu = x_mps = assertEqual x_cpu x_mps _helper _helper _helper test_inplace_scatter https github com pytorch pytorch issues a_mps = torch ones torch device mps b_mps = torch ones torch device mps a_cpu = torch ones torch device cpu b_cpu = torch ones torch device cpu a_mps += b_mps a_cpu += b_cpu assertEqual a_cpu a_mps a_mps = a_mps + b_mps a_cpu = a_cpu + b_cpu assertEqual a_cpu a_mps These tests taken test test_view_ops py They subset those tests currently only subset working This whole ` ` will removed when we add generic device testing There no additional tests added apart what part test_view_ops py TestViewOpsMPS TestCaseMPS exact_dtype = True test_permute_slicing test fix crash reported https github com pytorch pytorch issues cpu_x = torch randn float mps_x = cpu_x detach clone mps cpu_out = cpu_x permute mps_out = mps_x permute print caused crash prior fix PR# print torch zeros_like mps_out test fix fill_scalar_mps mentioned issue assertEqual torch zeros_like cpu_out torch zeros_like mps_out assertEqual cpu_x fill_ mps_x fill_ is_view_of base other other _is_view other base other _base base base device = other device False Note only validates storage native device types because some accelerators like XLA do expose storage base device type == mps base untyped_storage data_ptr = other untyped_storage data_ptr False True Returns true v v views same base is_view_of_same_base v v v _is_view v v False is_view_of v _base v Performs transpose contiguous=True returns input tensor _do_transpose x contiguous=False dim = dim = contiguous x x transpose dim dim test_diagonal_view device= mps t = torch ones device=device v = torch diagonal t assertTrue is_view_of t v v = assertEqual t v t = torch ones device= mps v = torch diagonal t offset= dim = dim = assertTrue is_view_of t v v = assertEqual t v test_select_view device= mps - None t = torch ones device=device v = t select assertTrue is_view_of t v v = assertEqual t v test_unbind_view device= mps - None t = torch zeros device=device tup = torch unbind t idx v enumerate tup assertTrue is_view_of t v v = idx + assertEqual t idx v test_expand_view device= mps - None t = torch ones device=device v = t expand assertTrue is_view_of t v v = assertEqual t v test_expand_as_view device= mps t = torch ones device=device e = torch empty device=device v = t expand_as e assertTrue is_view_of t v v = assertEqual t v test_narrow_view device= mps t = torch ones device=device v = torch narrow t assertTrue is_view_of t v v = assertEqual t v test_permute_view device= mps - None t = torch ones device=device v = t permute assertTrue is_view_of t v v = assertEqual t v test_transpose_view device= mps fn torch swapdims torch swapaxes torch transpose t = torch ones device=device v = fn t assertTrue is_view_of t v v = assertEqual t v test_transpose_inplace_view device= mps t = torch ones device=device v = t view_as t v = v swapdims_ assertTrue is_view_of t v v = assertEqual t v t = torch ones device=device v = t view_as t v = v swapaxes_ assertTrue is_view_of t v v = assertEqual t v t = torch ones device=device v = t view_as t v = v transpose_ assertTrue is_view_of t v v = assertEqual t v test_t_view device= mps t = torch ones device=device v = t t assertTrue is_view_of t v v = assertEqual t v test_inplace_view_add https github com pytorch pytorch issues t_mps = torch ones device= mps reshape t_cpu = torch ones device= cpu reshape t_mps = t_mps + t_cpu = t_cpu + assertEqual t_mps t_cpu test_t_inplace_view device= mps t = torch ones device=device v = t view_as t v = v t_ assertTrue is_view_of t v v = assertEqual t v test_T_view device= mps op T H mT mH t = torch ones device=device v = getattr t op assertTrue is_view_of t v v = assertEqual t v test_unfold_view device= mps t = torch ones device=device v = t unfold assertTrue is_view_of t v v = assertEqual t v test_squeeze_view device= mps t = torch ones device=device v = torch squeeze t assertTrue is_view_of t v v = assertIs t v _base test_squeeze_inplace_view device= mps t = torch ones device=device v = t view_as t v = v squeeze_ assertTrue is_view_of t v v = assertIs t v _base test_unsqueeze_view device= mps t = torch ones device=device v = torch unsqueeze t assertTrue is_view_of t v v = assertEqual t v test_unsqueeze_inplace_view device= mps t = torch ones device=device v = t view_as t v = v unsqueeze_ assertTrue is_view_of t v v = assertEqual t v test_as_strided_view device= mps t = torch ones device=device v = torch as_strided t assertTrue is_view_of t v v = assertEqual t v test_as_strided_inplace_view device= mps t = torch ones device=device v = t view_as t v = v as_strided_ assertTrue is_view_of t v v = assertEqual t v test_view_view device= mps t = torch ones device=device v = t view assertTrue is_view_of t v v = assertEqual t v test_view_as_view device= mps t = torch ones device=device e = torch empty v = t view_as e assertTrue is_view_of t v v = assertEqual t v test_contiguous_self device= mps t = torch ones device=device s = t contiguous assertIs s t test_contiguous_nonview device= mps t = torch ones device=device nv = t t contiguous assertFalse is_view_of t nv nv = assertNotEqual t nv test_reshape_view device= mps t = torch ones device=device v = torch reshape t assertTrue is_view_of t v v = assertEqual t v test_reshape_as_view device= mps t = torch ones device=device e = torch empty device=device v = t reshape_as e assertTrue is_view_of t v v = assertEqual t v test_reshape_nonview device= mps t = torch ones device=device nv = torch reshape t t assertFalse is_view_of t nv nv = assertNotEqual t nv test_flatten_view device= mps test_writes_propagate t v idx_t = t ndim idx_v = v ndim v idx_v = assertEqual t idx_t v idx_v t = torch ones device=device v = t flatten assertTrue is_view_of t v test_writes_propagate t v zero-dimensional tensor t = torch tensor device=device v = t flatten test_writes_propagate t v assertTrue is_view_of t v t = torch ones device=device transpose v = t flatten test_writes_propagate t v assertTrue is_view_of_same_base t v stride i = stride i + size i + satisfied groups t = torch ones device=device \ as_strided -- -- &#124; --- --- &#124; - - -- -- &#124; ---- --- &#124; - - v = t flatten v = v flatten v = v flatten test_writes_propagate t v assertTrue is_view_of_same_base t v test_writes_propagate t v assertTrue is_view_of_same_base t v test_writes_propagate t v assertTrue is_view_of_same_base t v test_flatten_nonview device= mps assert_is_nonview t nv idx_t = t ndim idx_nv = nv ndim assertFalse nv _is_view nv idx_nv = assertNotEqual t idx_t nv idx_nv t = torch ones device=device transpose nv = t flatten assert_is_nonview t nv t = torch ones device=device T nv = t flatten assert_is_nonview t nv flatten returns original object start_dim=end_dim t = torch ones device=device nv = t flatten assertIs t nv test_basic_indexing_slice_view device= mps t = torch ones device=device v = t assertTrue is_view_of t v v = assertEqual t v test_basic_indexing_ellipses_view device= mps t = torch ones device=device v = t assertTrue is_view_of t v v = assertEqual t v test_basic_indexing_newaxis_view device= mps t = torch ones device=device v = t None assertTrue is_view_of t v v = assertEqual t v test_chunk_view device= mps t = torch zeros device=device l = torch chunk t idx v enumerate l assertTrue is_view_of t v v = idx + assertEqual t idx v test_split_view device= mps t = torch zeros device=device l = torch split t idx v enumerate l assertTrue is_view_of t v v = idx + assertEqual t idx v test_movedim_view device= mps run_test device op t = torch zeros device=device out = op t assertTrue is_view_of t out Randomly change values output verify original changed well _ range idx_ idx_ = random randint random randint out idx_ idx_ = random random assertEqual t idx_ idx_ out idx_ idx_ fn torch movedim torch moveaxis op = partial fn source= destination= run_test device op op = partial fn source= destination= run_test device op Testing generated view_copy kernel its derivative implemented correctly test_view_copy device= mps = torch randn device=device requires_grad=True a_ref = detach clone requires_grad_ a_view = a_ref view a_view_copy = torch view_copy view_copy ops don t preserve view relationship assertTrue is_view_of a_ref a_view assertFalse is_view_of a_view_copy a_view_copy sum backward a_view sum backward forward backward give same shape + result assertEqual a_view_copy a_view assertEqual grad a_ref grad test_view_copy_out device= mps = torch randn device=device out = torch empty device=device torch diagonal_copy out=out expected = torch diagonal_copy assertEqual expected out = torch randn device=device out = torch empty device=device out = torch empty device=device torch split_copy out= out out expected expected = torch split_copy assertEqual expected out assertEqual expected out test_detached_view_copy device= mps https github com pytorch pytorch issues x = torch arange detach makes y view contig tensor non-zero offset y = x detach z = y device assertEqual y z cpu test_empty_reshape device= mps x = torch randn device=device assertEqual x reshape shape should viewable -- i e data_ptr same assertEqual x data_ptr x reshape data_ptr match NumPy semantics -- don t infer size dimension degree freedom assertRaises RuntimeError lambda x reshape - test_expand device= mps tensor = torch rand device=device tensor = torch rand device=device template = torch rand device=device target = template size assertEqual tensor expand_as template size target assertEqual tensor expand size target assertEqual tensor expand target size target assertEqual tensor expand_as template size target assertEqual tensor expand size target assertEqual tensor expand target size target test double expand assertEqual tensor expand expand tensor repeat test non-contiguous noncontig = torch randn device=device assertFalse noncontig is_contiguous assertEqual noncontig expand noncontig contiguous repeat make sure s compatible unsqueeze expanded = tensor expand unsqueezed = tensor unsqueeze unsqueeze assertEqual expanded unsqueezed assertEqual expanded stride unsqueezed stride test - target size assertEqual tensor expand - tensor expand assertRaises RuntimeError lambda tensor expand - - test expanding empty empty assertEqual torch zeros device=device expand torch zeros device=device test_view_empty device= mps x = torch randn device=device assertEqual x view shape test_reshape device= mps x = torch randn device=device assertEqual x data_ptr x reshape - data_ptr assertEqual x data_ptr x reshape data_ptr assertEqual torch reshape x x reshape assertRaises RuntimeError lambda x reshape - - y = torch randn device=device data_ptr meta tensors always so they equal regardless reshape device = meta assertNotEqual y data_ptr y reshape - data_ptr assertEqual y contiguous view - y reshape - assertEqual y reshape data_ptr y data_ptr s = torch randn device=device assertEqual s data_ptr s reshape data_ptr assertEqual s reshape - shape assertRaises RuntimeError lambda s reshape empty = torch tensor device=device assertEqual empty empty reshape - assertEqual empty empty reshape TODO fix these once we have multi-dimensional empty tensors assertEqual empty reshape shape assertEqual empty reshape - shape assertRaises RuntimeError lambda empty reshape x = torch randn device=device assertEqual x data_ptr x reshape_as torch rand data_ptr assertEqual x data_ptr x reshape_as torch rand data_ptr assertRaises RuntimeError lambda x reshape_as torch rand device=device test_narrow device= mps x = torch tensor assertEqual x narrow torch tensor assertEqual x narrow torch tensor assertEqual x narrow torch tensor assertEqual x narrow - torch tensor assertEqual x narrow - torch tensor assertEqual x narrow - torch tensor assertEqual x narrow - - torch tensor assertEqual x narrow - - torch tensor test_narrow_tensor device= mps x = torch tensor assertEqual x narrow torch tensor torch tensor assertRaises Exception x narrow torch tensor assertRaises Exception x narrow torch tensor assertRaises Exception x narrow torch tensor test_t device= mps Test D tensors x = torch randn assertEqual x x t x = x to_sparse assertEqual x x t Test D tensors x = torch arange assertEqual x x t x = x to_sparse assertEqual x x t Test D tensors x = torch rand assertEqual x t x transpose x = x to_sparse assertEqual x t x transpose Test D tensor x = torch rand assertRaisesRegex RuntimeError expects tensor = dimensions D x t x = x to_sparse assertRaisesRegex RuntimeError expects tensor = sparse dense dimensions x t test_split device= mps tensor = torch rand split_size = dim = target_sizes = splits = tensor split split_size dim start = target_size split zip target_sizes splits assertEqual split size target_size assertEqual tensor narrow dim start target_size dim split atol= rtol= start = start + target_size dim Variable sections split tensor = torch randn dim = split_sizes = target_sizes = splits = tensor split split_sizes dim start = target_size split zip target_sizes splits assertEqual split size target_size assertEqual tensor narrow dim start target_size dim split atol= rtol= start = start + target_size dim split_sizes = target_sizes = dim = splits = tensor split split_sizes dim start = target_size split zip target_sizes splits assertEqual split size target_size assertEqual tensor narrow dim start target_size dim split atol= rtol= start = start + target_size dim test_chunk device= mps tensor = torch rand num_chunks = dim = target_sizes = splits = tensor chunk num_chunks dim start = target_size split zip target_sizes splits assertEqual split size target_size assertEqual tensor narrow dim start target_size dim split atol= rtol= start = start + target_size dim Invalid chunk sizes error_regex = chunk expects greater than assertRaisesRegex RuntimeError error_regex tensor chunk assertRaisesRegex RuntimeError error_regex tensor chunk - test_unsqueeze device= mps - None x = torch randn y = x unsqueeze assertEqual y x view y = x clone unsqueeze_ assertEqual y x view x = x assertFalse x is_contiguous y = x unsqueeze assertEqual y x contiguous view y = x clone unsqueeze_ assertEqual y x contiguous view unit test special case transposed copy see ATen native Copy cpp details test_big_transpose device= mps t = torch rand device=device t = t t contiguous t = torch from_numpy t cpu numpy transpose assertEqual t t test_T device= mps = torch randn device=device t = T t = permute assertEqual t t b = torch randn device=device assertEqual b b T test_transposes device= mps dtype=torch float op T H mT mH adjoint shapes = op == m op == adjoint shape shapes = make_tensor shape device=device dtype=dtype t = getattr op op == adjoint t = t t = ndim = t = t transpose - - op - == H op == adjoint t = t conj assertEqual t t test_transposes_errors device= mps dtype=torch float op H mT mH adjoint shapes = op == H shape shapes = make_tensor shape device=device dtype=dtype assertRaisesRegex RuntimeError only supported matrices t = getattr op op == adjoint t = t test_python_types device= mps = torch randn device=device dtype=torch float = torch randn device=device dtype=torch float assertEqual dtype dtype b = torch arange dtype=torch int device=device b = torch arange dtype=int device=device assertEqual b dtype b dtype c = torch tensor True False dtype=torch bool device=device c = torch tensor True False dtype=bool device=device assertEqual c dtype c dtype TODO resize best put test_view_ops test_resize_as_preserves_strides device= mps x = torch empty t old_strides = x stride x resize_as_ x assertEqual x stride old_strides test_memory_format_resize_as device= mps test_helper shape memory_format device= mps xc = torch randn shape device=device contiguous memory_format=memory_format flat = torch randn xc numel device=device flat resize_as_ xc memory_format=torch preserve_format assertTrue flat is_contiguous memory_format=memory_format test_helper torch channels_last device= mps test_helper torch channels_last_ d device= mps test_memory_format_resize_ device= mps test_helper shape numel memory_format device= mps flat = torch randn numel device=device flat resize_ shape memory_format=memory_format assertTrue flat is_contiguous memory_format=memory_format test_helper torch channels_last device= mps test_helper torch channels_last_ d device= mps TODO OpInfo _test_atleast device torch_fn -dim s = torch tensor dtype=torch double requires_grad=True gradcheck lambda x torch_fn x s gradgradcheck lambda x torch_fn x s -dim = torch rand dtype=torch double requires_grad=True gradcheck lambda x torch_fn x gradgradcheck lambda x torch_fn x -dim b = torch rand dtype=torch double requires_grad=True c = torch rand dtype=torch double requires_grad=True d = torch rand dtype=torch double requires_grad=True input_tuple = s b c d gradcheck lambda s w x y z torch_fn s w x y z input_tuple gradgradcheck lambda s w x y z torch_fn s w x y z input_tuple test_atleast_gradient device= mps _test_atleast device torch atleast_ d _test_atleast device torch atleast_ d _test_atleast device torch atleast_ d test_view device= mps tensor = torch rand device=device template = torch rand device=device empty = torch empty device=device target = template size assertEqual tensor view_as template size target assertEqual tensor view size target assertEqual tensor view torch Size size target assertEqual tensor view - size target assertEqual tensor view - size target tensor_view = tensor view tensor_view fill_ random uniform assertEqual empty view_as empty empty assertEqual empty view empty assertEqual empty view size torch Size assertEqual empty view view empty test size inference empty tensors assertEqual empty view - size torch Size assertEqual empty view - size torch Size assertRaisesRegex RuntimeError r because unspecified dimension size - can any value empty view - assertRaisesRegex RuntimeError r because unspecified dimension size - can any value empty view - assertRaises RuntimeError lambda tensor view assertRaises RuntimeError lambda tensor view - assertRaises RuntimeError lambda tensor view - - test_contiguous device= mps x = torch randn device=device assertTrue x is_contiguous stride = list x stride stride = change stride dimension tensor still contiguous because size x set_ x storage x size stride assertTrue x is_contiguous test_resize_mps_dtypes device= mps shape = dt MPS_DTYPES x = torch tensor dtype=dt device=device x resize_ shape assertEqual shape x shape test_resize_as_mps_dtypes device= mps dt MPS_DTYPES x = torch tensor dtype=dt device=device y = torch tensor dtype=dt device=device x resize_as_ y assertEqual y shape x shape test_resize_overflow device= mps x = torch empty dtype=torch float assertRaisesRegex RuntimeError Storage size calculation overflowed x resize_ assertRaisesRegex RuntimeError overflow x resize_ test_view_all_dtypes_and_devices device= mps dt torch float torch bool x = torch tensor dtype=dt device=device assertEqual x view shape TestConvolutionMPS TestCaseMPS test_conv d_all_strides_paddings https github com pytorch pytorch issues helper stride padding y_cpu = torch randn conv_cpu = nn Conv d stride=stride padding=padding kernel_size= bias=False conv_gpu = copy deepcopy conv_cpu device= mps x_cpu = conv_cpu y_cpu y_gpu = y_cpu device= mps x_gpu = conv_gpu y_gpu assertEqual x_cpu x_gpu cpu stride range padding range helper stride padding test_conv d_channels_last https github com pytorch pytorch issues model_cpu = torch nn Conv d a_cpu = torch arange dtype=torch float a_cpu = a_cpu view permute out_cpu = model_cpu a_cpu a_mps = a_cpu detach clone mps model_mps = model_cpu mps out_mps = model_mps a_mps assertEqual out_cpu out_mps cpu rtol= e- atol= e- test_conv_transpose_ d_all_strides https github com pytorch pytorch issues helper stride y_cpu = torch ones deconv_cpu = nn ConvTranspose d in_channels= out_channels= kernel_size= stride=stride bias=False padding= deconv_cpu weight data = torch ones deconv_gpu = copy deepcopy deconv_cpu device= mps x_cpu = deconv_cpu y_cpu y_gpu = y_cpu device= mps x_gpu = deconv_gpu y_gpu assertEqual x_cpu x_gpu cpu helper stride stride test_conv_transpose_ d_nn_functional https github com pytorch pytorch issues tin = torch rand dtype=torch float tparams = torch rand dtype=torch float tbias = torch rand dtype=torch float device = cpu tcpu = torch nn functional conv_transpose d tin device tparams device tbias device stride= padding= device = mps tgpu = torch nn functional conv_transpose d tin device tparams device tbias device stride= padding= assertEqual tcpu tgpu cpu rtol= e- atol= e- test_conv_backward_ d_channels_last helper shape in_channels= out_channels= kernel_size= groups= https github com pytorch pytorch issues conv_cpu = torch nn Conv d in_channels=in_channels out_channels=out_channels kernel_size=kernel_size groups=groups requires_grad_ conv_mps = torch nn Conv d in_channels=in_channels out_channels=out_channels kernel_size=kernel_size groups=groups mps conv_mps weight data = conv_cpu weight data detach clone mps requires_grad_ True conv_mps bias data = conv_cpu bias data detach clone mps requires_grad_ True data = torch rand shape dtype=torch float x_cpu = data permute contiguous requires_grad_ True x_mps = data permute detach clone mps contiguous requires_grad_ True res_cpu = conv_cpu x_cpu res_mps = conv_mps x_mps assertEqual res_cpu res_mps res_cpu = res_cpu sum backward res_mps = res_mps sum backward assertEqual conv_cpu weight grad conv_mps weight grad rtol= e- atol= e- assertEqual x_cpu grad x_mps grad helper shape= helper shape= helper shape= helper shape= helper shape= in_channels= out_channels= groups= helper shape= in_channels= out_channels= groups= Regression test https github com pytorch pytorch issues And https github com pytorch pytorch issues adding grad input ic oc ks f = conv = torch nn Conv d ic oc kernel_size=ks padding= mps inp = torch rand ic f device= mps requires_grad=True out = conv inp grad_in = torch rand oc f device= mps grad_in_cl = torch empty f oc device= mps transpose grad_in_cl = grad_in It does matter whether grad_in contiguous channels last results should equal each other grad_rc = torch autograd grad out inp conv weight conv bias grad_in retain_graph=True grad_rc_cl = torch autograd grad out inp conv weight conv bias grad_in_cl retain_graph=True assertEqual grad_rc grad_rc_cl assertEqual grad_rc grad_rc_cl assertEqual grad_rc grad_rc_cl test_conv d_contiguous model_cpu = torch nn Conv d a_cpu = torch ones out_cpu = model_cpu a_cpu a_mps = a_cpu detach clone mps model_mps = model_cpu mps out_mps = model_mps a_mps assertEqual out_cpu shape out_mps shape assertEqual out_cpu out_mps cpu test_conv d_all_strides_paddings https github com pytorch pytorch issues helper N C H W groups input_mem_format weight_mem_format permute_data x_cpu = torch randn N C H W memory_format=input_mem_format requires_grad_ x_mps = x_cpu detach clone device= mps requires_grad_ permute_data x_cpu permute x_mps permute strideX range strideY range conv_cpu = torch nn Conv d in_channels=N out_channels=C kernel_size=H groups=groups stride= strideX strideY requires_grad_ conv_cpu weight data = conv_cpu weight memory_format=weight_mem_format requires_grad_ conv_mps = torch nn Conv d in_channels=N out_channels=C kernel_size=H groups=groups stride= strideX strideY device= mps conv_mps weight data = conv_cpu weight data detach clone mps requires_grad_ conv_mps bias data = conv_cpu bias data detach clone mps requires_grad_ res_cpu = conv_cpu x_cpu res_mps = conv_mps x_mps assertEqual res_cpu res_mps cpu rtol= e- atol= e- res_cpu = res_cpu sum backward res_mps = res_mps sum backward assertEqual res_cpu res_mps rtol= e- atol= e- assertEqual conv_cpu weight grad conv_mps weight grad rtol= e- atol= e- assertEqual conv_cpu bias grad conv_mps bias grad assertEqual x_cpu grad x_mps grad mem_format_input torch contiguous_format torch channels_last mem_format_weight torch contiguous_format torch channels_last permute_data True False helper mem_format_input mem_format_weight permute_data helper mem_format_input mem_format_weight permute_data helper mem_format_input mem_format_weight permute_data test_conv_transpose_ d_strided helper m_cpu memory_format m_mps = copy deepcopy m_cpu requires_grad_ m_mps weight data = m_cpu weight data detach clone mps requires_grad_ m_mps bias data = m_cpu bias data detach clone mps requires_grad_ input_cpu = torch randn memory_format=memory_format requires_grad_ input_mps = input_cpu detach clone mps output_cpu = m_cpu input_cpu output_mps = m_mps input_mps assertEqual output_cpu output_mps mem_format_input torch contiguous_format torch channels_last With square kernels equal stride helper nn ConvTranspose d stride= requires_grad_ mem_format_input non-square kernels unequal stride padding helper nn ConvTranspose d stride= padding= requires_grad_ mem_format_input test_conv_transpose_ d_specified_output input_cpu = torch randn input_mps = input_cpu detach clone mps downsample_cpu = nn Conv d stride= padding= downsample_mps = nn Conv d stride= padding= device= mps downsample_mps weight data = downsample_cpu weight data detach clone mps requires_grad_ downsample_mps bias data = downsample_cpu bias data detach clone mps requires_grad_ upsample_cpu = nn ConvTranspose d stride= padding= upsample_mps = nn ConvTranspose d stride= padding= device= mps upsample_mps weight data = upsample_cpu weight data detach clone mps requires_grad_ upsample_mps bias data = upsample_cpu bias data detach clone mps requires_grad_ h_cpu = downsample_cpu input_cpu h_mps = downsample_mps input_mps assertEqual h_cpu h_mps size_cpu = h_cpu size size_mps = h_mps size assertEqual size_cpu size_mps output_cpu = upsample_cpu h_cpu output_size=input_cpu size output_mps = upsample_mps h_mps output_size=input_mps size assertEqual output_cpu output_mps assertEqual output_cpu size output_mps size test_conv d_single_stride y_cpu = torch randn y_gpu = y_cpu device= mps stride range conv_cpu = torch nn Conv d in_channels= out_channels= kernel_size= stride=stride conv_gpu = copy deepcopy conv_cpu device= mps x_cpu = conv_cpu y_cpu x_gpu = conv_gpu y_gpu assertEqual x_cpu x_gpu cpu rtol= e- atol= e- test_conv d_single_stride Conv d only available MacOS onwards y_cpu = torch randn y_gpu = y_cpu device= mps stride range conv_cpu = torch nn Conv d in_channels= out_channels= kernel_size= stride=stride conv_gpu = copy deepcopy conv_cpu device= mps x_cpu = conv_cpu y_cpu x_gpu = conv_gpu y_gpu assertEqual x_cpu x_gpu cpu rtol= e- atol= e- test_grid_sample test N C H W mode padding_mode align_corners input_requires_grad test_shape N C IH IW H W mode padding_mode align_corners grid_dim_contig_order grid_dim_contig_order specifies dimension order can make grid contiguous i e grid permute grid_dim_contig_order contiguous e g grid_dim_contig_order= grid should initialized contiguous tensor shape N H W permuted N H W afterwards grid_shape = N H W grid_init_shape = grid_shape d d grid_dim_contig_order grid_fwd_permute = None None None None i d enumerate grid_dim_contig_order grid_fwd_permute d = i get_grid device= cpu data=None data None assert list data shape == grid_shape data = data permute grid_dim_contig_order device data = torch randn grid_init_shape device=device grid = data permute grid_fwd_permute assert grid permute grid_dim_contig_order is_contiguous grid input_cpu = torch randn C N IH IW transpose requires_grad_ input_requires_grad grid_cpu = get_grid requires_grad_ out_cpu = F grid_sample input_cpu grid_cpu mode=mode padding_mode=padding_mode align_corners=align_corners assertEqual out_cpu size torch Size N C H W gradients = torch randn_like out_cpu out_cpu backward gradients Compare against unvectorized CPU fallback NOTE grid_sample CPU fallback grid_sample uses AVX d images requires -bit indexing -bit floats So we also have fallback used only float tensors requiring -bit indexing That requires too much memory run CI so we also export fallback test here ensure feature parity vectorized version input_fallback = input_cpu float detach_ requires_grad_ grid_fallback = grid_cpu float detach_ requires_grad_ out_fallback = torch _grid_sampler_ d_cpu_fallback input_fallback grid_fallback F GRID_SAMPLE_INTERPOLATION_MODES mode F GRID_SAMPLE_PADDING_MODES padding_mode align_corners assertEqual out_fallback out_cpu float atol= e- rtol= e- out_fallback backward gradients float input_requires_grad assertEqual input_fallback grad input_cpu grad float atol= e- rtol= e- assertEqual grid_fallback grad grid_cpu grad float atol= e- rtol= e- input_mps = input_cpu detach transpose mps transpose requires_grad_ input_requires_grad grid_mps = get_grid mps grid_cpu detach requires_grad_ out_mps = F grid_sample input_mps grid_mps mode=mode padding_mode=padding_mode align_corners=align_corners assertEqual out_cpu out_mps out_mps backward gradients mps input_requires_grad assertEqual input_cpu grad input_mps grad assertEqual grid_cpu grad grid_mps grad atol= e- rtol= check zero-dimensional input strides don t error out base_input = torch randn N C IW input_cpu = base_input expand_as input_mps requires_grad_ input_requires_grad out_cpu = F grid_sample input_cpu grid_cpu mode=mode padding_mode=padding_mode align_corners=align_corners input_mps = base_input mps expand_as input_mps requires_grad_ input_requires_grad out_mps = F grid_sample input_mps grid_mps mode=mode padding_mode=padding_mode align_corners=align_corners assertEqual out_cpu out_mps test same size output test_shape N C H W H W mode padding_mode align_corners test larger output N = random randint C = random randint IH = random randint IW = random randint H = random randint IH + W = random randint IW + test_shape N C IH IW H W mode padding_mode align_corners test smaller output N = random randint C = random randint IH = random randint IW = random randint H = random randint IH W = random randint IW test_shape N C IH IW H W mode padding_mode align_corners test x inpput N = random randint C = random randint IH = IW = H = random randint W = random randint test_shape N C IH IW H W mode padding_mode align_corners testing empty grid N = random randint C = random randint IH = random randint IW = random randint W = random randint IW + test_shape N C IH IW W mode padding_mode align_corners testing empty channel N = random randint IH = random randint IW = random randint H = random randint IH + W = random randint IW + test_shape N IH IW H W mode padding_mode align_corners testing empty batch C = random randint IH = random randint IW = random randint H = random randint IH + W = random randint IW + test_shape C IH IW H W mode padding_mode align_corners mode bilinear nearest padding_mode zeros reflection align_corners True False test known input input = torch arange device= mps view grid = torch tensor - - - - e- - - - - e- device= mps view mode == bilinear padding_mode == zeros align_corners groundtruth = torch tensor device= mps view groundtruth = torch tensor device= mps view padding_mode == border align_corners groundtruth = torch tensor device= mps view groundtruth = torch tensor device= mps view padding_mode == reflection align_corners groundtruth = torch tensor device= mps view groundtruth = torch tensor device= mps view raise AssertionError f missing groundtruth test padding mode padding_mode mode == nearest padding_mode == zeros align_corners groundtruth = torch tensor device= mps view groundtruth = torch tensor device= mps view padding_mode == border align_corners groundtruth = torch tensor device= mps view groundtruth = torch tensor device= mps view padding_mode == reflection align_corners groundtruth = torch tensor device= mps view groundtruth = torch tensor device= mps view raise AssertionError f missing groundtruth test padding mode padding_mode mode == bicubic padding_mode == zeros align_corners groundtruth = torch tensor - device= mps view groundtruth = torch tensor - device= mps view padding_mode == border align_corners groundtruth = torch tensor device= mps view groundtruth = torch tensor device= mps view padding_mode == reflection align_corners groundtruth = torch tensor device= mps view groundtruth = torch tensor device= mps view raise AssertionError f missing groundtruth test padding mode padding_mode raise AssertionError f missing groundtruth test interpolation mode mode output = F grid_sample input grid mode=mode padding_mode=padding_mode align_corners=align_corners assertEqual output groundtruth atol= e- rtol= msg=f groundtruth comparison failed mode= mode f padding_mode= padding_mode TestAdvancedIndexing TestCaseMPS supported_dtypes = torch float torch float torch int torch int torch int torch uint supported_np_dtypes = np float np float np int np int np int np uint test_nonzero_no_warning device = mps t = torch randn device=device warnings catch_warnings record=True w warnings simplefilter always torch nonzero t t nonzero assertEqual len w test_nonzero helper dtype device = mps shapes = torch Size torch Size torch Size torch Size torch Size torch Size gen_nontrivial_input shape dtype device dtype = torch bfloat torch randint shape device=device dtype=dtype windows does work bfloat randing torch randint shape device=device dtype=torch float dtype shape shapes tensor = gen_nontrivial_input shape dtype device dst = torch nonzero tensor as_tuple=False dst = tensor nonzero as_tuple=False dst = torch empty dtype=torch long device=device dst = dst resize_ torch nonzero tensor out=dst np_array = tensor cpu numpy dtype = torch bfloat tensor float cpu numpy np_result = torch from_numpy np stack np_array nonzero t assertEqual dst cpu np_result atol= rtol= assertEqual dst cpu np_result atol= rtol= assertEqual dst cpu np_result atol= rtol= tup = torch nonzero tensor as_tuple=True tup = tensor nonzero as_tuple=True tup = torch stack tup t cpu tup = torch stack tup t cpu assertEqual tup np_result atol= rtol= assertEqual tup np_result atol= rtol= helper dtype dtype supported_dtypes test_nonzero_astuple_out device = mps t = torch randn device=device out = torch empty dtype=torch long device=device out = out resize_ assertRaises RuntimeError torch nonzero t as_tuple=True out=out assertEqual torch nonzero t as_tuple=False out=out torch nonzero t out=out Verifies JIT script cannot handle as_tuple kwarg See Issue https github com pytorch pytorch issues _foo t tuple_result = torch nonzero t as_tuple=True nontuple_result = torch nonzero t as_tuple=False out = torch empty_like nontuple_result torch nonzero t as_tuple=False out=out tuple_result nontuple_result out assertRaises RuntimeError scripted_foo = torch jit script _foo Verifies JIT tracing works fine traced_foo = torch jit trace _foo t traced_tuple traced_nontuple traced_out = traced_foo t expected_tuple = torch nonzero t as_tuple=True expected_nontuple = torch nonzero t assertEqual traced_tuple expected_tuple assertEqual traced_nontuple expected_nontuple assertEqual traced_out expected_nontuple test_nonzero_discontiguous device = mps shape = tensor = torch randint shape device=device tensor_nc = torch empty shape shape device=device copy_ tensor dst = tensor nonzero as_tuple=False dst = tensor_nc nonzero as_tuple=False assertEqual dst dst atol= rtol= dst = torch empty_like dst data_ptr = dst data_ptr expect dst storage reused torch nonzero tensor out=dst assertEqual data_ptr dst data_ptr assertEqual dst dst atol= rtol= discontiguous out dst = torch empty dst size dst size dtype=torch long device=device data_ptr = dst data_ptr strides = dst stride torch nonzero tensor out=dst assertEqual data_ptr dst data_ptr assertEqual dst dst atol= rtol= assertEqual strides dst stride test_nonzero_non_diff device = mps x = torch randn requires_grad=True device=device nz = x nonzero assertFalse nz requires_grad test_nonzero_multi_threading Test MPS doesn t crash nonzero called concurrently See https github com pytorch pytorch issues x = torch rand device= mps t = threading Thread target=torch nonzero args= x t = threading Thread target=torch nonzero args= x t start t start test_sliced_view_cast This used crash MacOS Sequoia See https github com pytorch pytorch issues x = torch rand device= mps dtype=torch float y = x view torch float + test_masked_select x = torch randn x_mps = x mps mask = x ge mask_mps = x_mps ge res = torch masked_select x mask res_mps = torch masked_select x_mps mask_mps assertEqual res res_mps examples https www tutorialspoint com numpy numpy_advanced_indexing htm test_indexing_get helper dtype x_cpu = torch tensor dtype=dtype x_mps = x_cpu detach clone mps y_cpu = x_cpu y_mps = x_mps assertEqual y_cpu y_mps str dtype helper dtype dtype supported_dtypes test_indexing_select_corners helper dtype x_cpu = torch tensor dtype=dtype x_mps = x_cpu detach clone mps rows_cpu = torch tensor rows_mps = rows_cpu detach clone mps cols_cpu = torch tensor cols_mps = cols_cpu detach clone mps res_cpu = x_cpu rows_cpu cols_cpu res_mps = x_mps rows_mps cols_mps assertEqual res_cpu res_mps str dtype helper dtype dtype supported_dtypes FIXME uint fails testcase needs further debugging test_slicing_using_advanced_index_for_column helper dtype x_cpu = torch tensor dtype=dtype x_mps = x_cpu detach clone mps z_cpu = x_cpu z_mps = x_mps assertEqual z_cpu z_mps str dtype using advanced index column y_cpu = x_cpu y_mps = x_mps assertEqual y_cpu y_mps str dtype FIXME use supported_dtypes once uint fixed helper dtype dtype torch float torch float torch int torch int torch int test_boolean_array_indexing helper dtype x_cpu = torch tensor dtype=dtype x_mps = x_cpu detach clone mps res_cpu = x_cpu x_cpu res_mps = x_mps x_mps assertEqual res_cpu res_mps str dtype dtype supported_dtypes helper dtype test_advanced_indexing_ D_get helper x_cpu x_mps = x_cpu detach clone mps assertEqual x_cpu x_mps assertEqual x_cpu x_mps assertEqual x_cpu x_mps x_cpu = torch tensor device= cpu dtype=torch float helper x_cpu idx range len supported_np_dtypes torch randn torch rand don t work all dtypes Generate input data all dtypes Numpy them move torch input_t = np random random_sample size= astype supported_np_dtypes idx inputCPU = torch tensor input_t device= cpu dtype=self supported_dtypes idx helper inputCPU test_advanced_indexing_ D_put helper x_cpu dtype = x_cpu dtype x_mps = x_cpu detach clone mps out_tensor_cpu = torch tensor dtype=dtype device= cpu out_tensor_cpu_view = out_tensor_cpu out_tensor_mps = torch tensor dtype=dtype device= mps out_tensor_mps_view = out_tensor_mps x_cpu = out_tensor_cpu_view x_mps = out_tensor_mps_view assertEqual x_cpu x_mps x_cpu = out_tensor_cpu_view x_mps = out_tensor_mps_view assertEqual x_cpu x_mps x_cpu = out_tensor_cpu_view x_mps = out_tensor_mps_view assertEqual x_cpu x_mps x_cpu = torch tensor device= cpu dtype=torch float helper x_cpu idx range len supported_np_dtypes torch randn torch rand don t work all dtypes Generate input data all dtypes Numpy them move torch input_t = np random random_sample size= astype supported_np_dtypes idx inputCPU = torch tensor input_t device= cpu dtype=self supported_dtypes idx helper inputCPU test_index_put_with_view_indices helper dtype target_cpu = torch zeros device= cpu dtype=dtype target_mps = torch zeros device= mps dtype=dtype indices_cpu = torch tensor dtype=torch int device= cpu indices_mps = torch tensor dtype=torch int device= mps value_cpu = torch ones indices_cpu shape device= cpu dtype=dtype value_mps = torch ones indices_mps shape device= mps dtype=dtype target_cpu index_put_ tuple indices_cpu t value_cpu accumulate=True target_mps index_put_ tuple indices_mps t value_mps accumulate=True assertEqual target_cpu target_mps helper dtype dtype torch int torch float tests test_indexing py test_advancedindex_big device= mps reference = torch arange dtype=torch int device=device assertEqual reference torch tensor dtype=torch int test_set_item_to_scalar_tensor device= mps m = random randint n = random randint z = torch randn m n device=device = w = torch tensor requires_grad=True device=device z = w z sum backward assertEqual w grad m test_single_int device= mps v = torch randn device=device assertEqual v shape test_multiple_int device= mps v = torch randn device=device assertEqual v shape assertEqual v shape test_none device= mps v = torch randn device=device assertEqual v None shape assertEqual v None shape assertEqual v None None shape assertEqual v None shape test_step device= mps v = torch arange device=device assertEqual v v assertEqual v tolist assertEqual v tolist assertEqual v tolist assertEqual v tolist test_step_assignment device= mps v = torch zeros device=device v = torch tensor device=device assertEqual v tolist assertEqual v sum test_bool_indices device= mps v = torch randn device=device boolIndices = torch tensor True False True True False dtype=torch bool device=device assertEqual v boolIndices shape assertEqual v boolIndices torch stack v v v v = torch tensor True False True dtype=torch bool device=device boolIndices = torch tensor True False False dtype=torch bool device=device uint Indices = torch tensor dtype=torch uint device=device warnings catch_warnings record=True w assertEqual v boolIndices shape v uint Indices shape assertEqual v boolIndices v uint Indices assertEqual v boolIndices torch tensor True dtype=torch bool device=device assertEqual len w test_bool_indices_accumulate device= mps mask = torch zeros size= dtype=torch uint device=device mask = mask y = torch ones size= device=device y index_put_ mask y mask accumulate=True assertEqual y torch ones size= device=device test_multiple_bool_indices device= mps v = torch randn device=device note these broadcast together transposed first dim mask = torch tensor dtype=torch bool device=device mask = torch tensor dtype=torch bool device=device assertEqual v mask mask shape test_byte_mask device= mps v = torch randn device=device mask = torch ByteTensor device warnings catch_warnings record=True w assertEqual v mask shape assertEqual v mask torch stack v v v assertEqual len w v = torch tensor device=device assertEqual v v == torch tensor device=device test_byte_mask_accumulate device= mps mask = torch zeros size= dtype=torch uint device=device y = torch ones size= device=device warnings catch_warnings record=True w warnings simplefilter always y index_put_ mask y mask accumulate=True assertEqual y torch ones size= device=device assertEqual len w test_index_put_accumulate_expanded_values device= mps t = torch zeros t_dev = t device indices = torch tensor torch tensor indices_dev = i device i indices values d = torch tensor values d = torch tensor out_mps = t_dev index_put_ indices_dev values d device accumulate=True out_cpu = t index_put_ indices values d accumulate=True assertEqual out_mps cpu out_cpu out_mps = t_dev index_put_ indices_dev values d device accumulate=True out_cpu = t index_put_ indices values d accumulate=True assertEqual out_mps cpu out_cpu t = torch zeros t_dev = t device indices = torch tensor torch arange None torch arange None indices_dev = i device i indices values d = torch tensor - - values d = torch tensor - - out_mps = t_dev index_put_ indices_dev values d device accumulate=True out_cpu = t index_put_ indices values d accumulate=True assertEqual out_mps cpu out_cpu out_mps = t_dev index_put_ indices_dev values d device accumulate=True out_cpu = t index_put_ indices values d accumulate=True assertEqual out_mps cpu out_cpu test_index_put_accumulate_non_contiguous device= mps t = torch zeros t_dev = t device t = t_dev t = t assertFalse t is_contiguous assertFalse t is_contiguous indices = torch tensor indices_dev = i device i indices value = torch randn out_mps = t index_put_ indices_dev value device accumulate=True out_cpu = t index_put_ indices value accumulate=True assertFalse t is_contiguous assertFalse t is_contiguous assertEqual out_mps cpu out_cpu test_index_put_accumulate_with_optional_tensors device= mps TODO replace better solution Currently here using torchscript put None into indices C++ gives indices list optional tensors first null second valid tensor torch jit script func x i v idx = None i x index_put_ idx v accumulate=True x n = t = torch arange n dtype=torch float reshape n t_dev = t device indices = torch tensor indices_dev = indices device value d = torch tensor value d = torch tensor out_mps = func t_dev indices_dev value d mps out_cpu = func t indices value d assertEqual out_mps cpu out_cpu out_mps = func t_dev indices_dev value d mps out_cpu = func t indices value d assertEqual out_mps cpu out_cpu test_index_put_accumulate_duplicate_indices device= mps i range generate indices random walk will create indices lots duplicates interleaved each other delta = torch empty i dtype=torch float device=device uniform_ - indices = delta cumsum long mps abs int supported mps fallback cpu calculate input = torch randn indices cpu abs max mps + device=device values = torch randn indices size device=device output = input index_put indices values accumulate=True input_list = input tolist indices_list = indices tolist values_list = values tolist i v zip indices_list values_list input_list i += v assertEqual output input_list test_index_put_deterministic device= mps helper dtype accumulate deterministic num_tests= acc_expected = torch tensor device=device dtype=dtype non_acc_expected = torch tensor device=device dtype=dtype t_idx = torch tensor _ range num_tests try torch use_deterministic_algorithms deterministic t = torch zeros dtype=dtype device=device t index_put_ t_idx torch arange len t_idx device=device dtype=dtype accumulate=accumulate accumulate assertEqual t acc_expected assertEqual t non_acc_expected finally torch use_deterministic_algorithms False accumulate deterministic product False True False True dtype = torch float accumulate torch long accumulate deterministic assertRaisesRegex AssertionError Tensor-likes equal helper dtype accumulate deterministic helper dtype accumulate deterministic test_multiple_byte_mask device= mps v = torch randn device=device note these broadcast together transposed first dim mask = torch ByteTensor device mask = torch ByteTensor device warnings catch_warnings record=True w warnings simplefilter always assertEqual v mask mask shape assertEqual len w test_byte_mask d device= mps v = torch randn device=device c = torch randn device=device num_ones = c sum r = v c assertEqual r shape num_ones test_jit_indexing device= mps fn x x x = x fn x x = x scripted_fn = torch jit script fn scripted_fn = torch jit script fn data = torch arange device=device dtype=torch float out = scripted_fn data detach clone ref = torch tensor np concatenate np ones np arange device=device dtype=torch float assertEqual out ref out = scripted_fn data detach clone assertEqual out ref test_int_indices device= mps v = torch randn device=device assertEqual v shape assertEqual v shape assertEqual v shape test_index_put_src_datatype helper device dtype src = torch ones device=device dtype=dtype vals = torch ones device=device dtype=dtype indices = torch tensor res = src index_put_ indices vals accumulate=True assertEqual res shape src shape helper device= mps dtype=dtype dtype torch float torch int test_index_src_datatype helper device dtype orig_dtype = dtype dtype torch bool dtype = torch uint src = torch ones device=device dtype=dtype orig_dtype torch bool src = src == test index res = src assertEqual res shape src shape test index_put no accum src = res assertEqual res shape src shape helper device= mps dtype=dtype dtype torch float torch float torch long torch bool test_int_indices d device= mps From NumPy indexing example x = torch arange device=device view rows = torch tensor device=device columns = torch tensor device=device assertEqual x rows columns tolist test_int_indices_broadcast device= mps From NumPy indexing example x = torch arange device=device view rows = torch tensor device=device columns = torch tensor device=device result = x rows None columns assertEqual result tolist test_empty_index device= mps x = torch arange device=device view idx = torch tensor dtype=torch long device=device assertEqual x idx numel empty assignment should have no effect throw exception y = x clone y idx = - assertEqual x y mask = torch zeros device=device bool y mask = - assertEqual x y test_empty_ndim_index device= mps x = torch randn device=device assertEqual torch empty device=device x torch empty dtype=torch int device=device x = torch randn device=device assertEqual torch empty device=device x torch empty dtype=torch int device=device x = torch empty device=device assertEqual x shape assertEqual x shape assertRaisesRegex IndexError dimension size x test_empty_ndim_index_bool device= mps x = torch randn device=device assertRaises IndexError lambda x torch empty dtype=torch uint device=device test_empty_slice device= mps x = torch randn device=device y = x z = y assertEqual z shape isn t technically necessary matches NumPy stride calculations assertEqual z stride assertTrue z is_contiguous test_empty_reduce device= mps x = torch rand device=device assertTrue x mean isnan assertTrue x nanmean isnan assertTrue x median isnan assertTrue x nanmedian isnan assertEqual x count_nonzero assertEqual x sum assertEqual x nansum assertRaises RuntimeError lambda x amax assertRaises IndexError lambda x amax dim= assertRaises RuntimeError lambda x amin assertRaises IndexError lambda x amin dim= test_index_getitem_copy_bools_slices device= mps true = torch tensor dtype=torch uint device=device false = torch tensor dtype=torch uint device=device tensors = torch randn device=device torch tensor device=device tensors assertNotEqual data_ptr True data_ptr assertEqual torch empty shape False assertNotEqual data_ptr true data_ptr assertEqual torch empty shape false assertEqual data_ptr None data_ptr assertEqual data_ptr data_ptr test_index_setitem_bools_slices device= mps true = torch tensor dtype=torch uint device=device false = torch tensor dtype=torch uint device=device tensors = torch randn device=device torch tensor device=device tensors prefix ensure we compatible numpy which cuts off prefix s some these ops already prefix size neg_ones = torch ones_like - neg_ones_expanded = neg_ones unsqueeze unsqueeze True = neg_ones_expanded assertEqual neg_ones False = assertEqual neg_ones true = neg_ones_expanded assertEqual neg_ones false = assertEqual neg_ones None = neg_ones_expanded assertEqual neg_ones = neg_ones_expanded assertEqual neg_ones dim == assertRaises IndexError = neg_ones_expanded test_index_scalar_with_bool_mask device= mps = torch tensor device=device uintMask = torch tensor True dtype=torch uint device=device boolMask = torch tensor True dtype=torch bool device=device assertEqual uintMask boolMask assertEqual uintMask dtype boolMask dtype = torch tensor True dtype=torch bool device=device assertEqual uintMask boolMask assertEqual uintMask dtype boolMask dtype test_setitem_expansion_error device= mps true = torch tensor True device=device = torch randn device=device check prefix non- s doesn t work a_expanded = expand torch Size + size NumPy ValueError assertRaises RuntimeError True = a_expanded assertRaises RuntimeError true = a_expanded test_getitem_scalars device= mps zero = torch tensor dtype=torch int device=device one = torch tensor dtype=torch int device=device non-scalar indexed scalars = torch randn device=device assertEqual zero assertEqual zero one assertEqual zero one assertEqual one zero indexing scalar should slice copy assertEqual data_ptr zero one data_ptr assertEqual data_ptr one int data_ptr assertEqual data_ptr one short data_ptr scalar indexed scalar r = torch randn device=device assertRaises IndexError r assertRaises IndexError r zero assertEqual r r test_setitem_scalars device= mps zero = torch tensor dtype=torch int non-scalar indexed scalars = torch randn device=device a_set_with_number = clone a_set_with_scalar = clone b = torch randn device=device a_set_with_number = b a_set_with_scalar zero = b assertEqual a_set_with_number a_set_with_scalar zero = assertEqual scalar indexed scalars r = torch randn device=device assertRaises IndexError r = assertRaises IndexError r zero = r = assertEqual r test_basic_advanced_combined device= mps From NumPy indexing example x = torch arange device=device view assertEqual x x assertEqual x tolist Check copy unmodified = x clone x zero_ assertEqual x unmodified But assignment should modify original unmodified = x clone x = assertNotEqual x unmodified test_int_assignment device= mps x = torch arange device=device view x = assertEqual x tolist x = torch arange device=device view x = torch arange device=device assertEqual x tolist test_byte_tensor_assignment device= mps x = torch arange device=device view b = torch ByteTensor True False True False device value = torch tensor device=device warnings catch_warnings record=True w x b = value assertEqual len w assertEqual x value assertEqual x torch arange device=device assertEqual x value assertEqual x torch arange device=device test_variable_slicing device= mps x = torch arange device=device view indices = torch IntTensor device i j = indices assertEqual x i j x test_ellipsis_tensor device= mps x = torch arange device=device view idx = torch tensor device=device assertEqual x idx tolist assertEqual x idx tolist test_invalid_index device= mps x = torch arange device=device view assertRaisesRegex TypeError slice indices lambda x test_out_of_bound_index device= mps x = torch arange device=device view assertRaisesRegex IndexError index out bounds dimension size lambda x assertRaisesRegex IndexError index out bounds dimension size lambda x assertRaisesRegex IndexError index out bounds dimension size lambda x assertRaisesRegex IndexError index out bounds dimension size lambda x test_zero_dim_index device= mps x = torch tensor device=device assertEqual x x item runner print x x assertRaisesRegex IndexError invalid index runner test_cpu_indices device= mps idx = torch tensor b = torch zeros device=device x = torch ones device=device x idx = b index_put_ ref = torch ones device=device ref = assertEqual x ref atol= rtol= out = x idx index assertEqual out torch zeros device=device atol= rtol= test_nextafter device= mps dtype torch float torch float x = torch tensor - - device=device dtype=dtype y = torch tensor - - - device=device dtype=dtype na = torch nextafter x y na_cpu = torch nextafter x cpu y cpu na_ge_x_mps = na cpu x cpu greater broken MPS see https github com pytorch pytorch issues na_ge_x_cpu = na_cpu x cpu assertEqual na_ge_x_mps na_ge_x_cpu TestRNNMPS TestCaseMPS _lstm_helper num_layers dtype device bidirectional=False bias=True batch_first=False seq_len= batch_size= hidden_size= input_size= backward=False rnn = nn LSTM input_size=input_size hidden_size=hidden_size num_layers=num_layers bias=bias bidirectional=bidirectional batch_first=batch_first device= cpu bidirectional_mul = bidirectional batch_first input = torch randn batch_size seq_len input_size device= cpu dtype=dtype requires_grad=backward hx = torch randn num_layers bidirectional_mul batch_size hidden_size device= cpu dtype=dtype requires_grad=backward cx = torch randn num_layers bidirectional_mul batch_size hidden_size device= cpu dtype=dtype requires_grad=backward input = torch randn seq_len batch_size input_size device= cpu dtype=dtype requires_grad=backward hx = torch randn num_layers bidirectional_mul batch_size hidden_size device= cpu dtype=dtype requires_grad=backward cx = torch randn num_layers bidirectional_mul batch_size hidden_size device= cpu dtype=dtype requires_grad=backward cpu_output cpu_hn cpu_cn = rnn input hx cx rnn = rnn device input = input device hx = hx device cx = cx device output hn cn = rnn input hx cx assertEqual cpu_output output assertEqual cpu_hn hn assertEqual cpu_cn cn get_backward_results rnn device inp hx cx output_grad_presented=True states_grad_presented=True rnn = rnn device inp hx cx = inp device hx device cx device output hx_out cx_out = rnn inp hx cx assert output_grad_presented states_grad_presented At least some outputs must used f = output_grad_presented f = f + output sum states_grad_presented f = f + hx_out cx_out sum param_names params = zip rnn named_parameters param_grads = zip param_names torch autograd grad f params retain_graph=True input_grad hx_grad cx_grad = torch autograd grad f inp hx cx output param_grads input_grad hx_grad cx_grad backward grad_cases = dict output_grad_presented=True states_grad_presented=True dict output_grad_presented=False states_grad_presented=True dict output_grad_presented=True states_grad_presented=False grad_case grad_cases cpu_output cpu_weights_grad cpu_input_grad cpu_hx_grad cpu_cx_grad =\ get_backward_results rnn cpu input hx cx grad_case mps_output mps_weights_grad mps_input_grad mps_hx_grad mps_cx_grad =\ get_backward_results rnn device input hx cx grad_case assertEqual cpu_hx_grad mps_hx_grad assertEqual cpu_cx_grad mps_cx_grad assertEqual cpu_output mps_output assertEqual cpu_input_grad mps_input_grad cpu_name cpu_weight_grad mps_name mps_weight_grad zip cpu_weights_grad mps_weights_grad assertEqual cpu_weight_grad mps_weight_grad f mismatch cpu cpu_name vs mps mps_name layers num_layers LSTM_TEST_CASES = default dict batch_first=True dict bias=False dict bidirectional=True dict batch_first=True bias=False dict bidirectional=True bias=False dict bidirectional=True batch_first=True dict bidirectional=True batch_first=True bias=False test_lstm_forward device= mps dtype=torch float num_layers test_options LSTM_TEST_CASES _lstm_helper num_layers=num_layers dtype=dtype device=device test_options test_lstm_backward device= mps dtype=torch float num_layers test_options LSTM_TEST_CASES _lstm_helper num_layers=num_layers dtype=dtype device=device backward=True test_options test_RNN_cell_no_broadcasting test cell_module input hx input_size hidden_size cell = cell_module input_size hidden_size device= mps assertRaises RuntimeError lambda cell input hx test_all hidden_size bad_hx good_hx input_size input test nn RNNCell input bad_hx input_size hidden_size test nn GRUCell input bad_hx input_size hidden_size test nn LSTMCell input bad_hx good_hx input_size hidden_size test nn LSTMCell input good_hx bad_hx input_size hidden_size hidden_size = input_size = input = torch randn input_size device= mps bad_hx = torch randn hidden_size device= mps good_hx = torch randn hidden_size device= mps Test hidden input batch size broadcasting test_all hidden_size bad_hx good_hx input_size input Test hx s hidden_size vs module s hidden_size broadcasting bad_hx = torch randn test_all hidden_size bad_hx good_hx input_size input Test input s input_size vs module s input_size broadcasting bad_input = torch randn test_all hidden_size good_hx good_hx input_size bad_input test_LSTM_cell just smoke test these modules implemented through autograd so no Jacobian test needed bias True False input = torch randn device= mps hx = torch randn device= mps cx = torch randn device= mps lstm = nn LSTMCell bias=bias device= mps _ range hx cx = lstm input hx cx hx + cx sum backward test_LSTM_cell_forward_input_size input = torch randn device= mps hx = torch randn device= mps cx = torch randn device= mps lstm = nn LSTMCell device= mps assertRaises Exception lambda lstm input hx cx test_LSTM_cell_forward_hidden_size input = torch randn device= mps hx = torch randn device= mps cx = torch randn device= mps lstm = nn LSTMCell device= mps assertRaises Exception lambda lstm input hx cx assertRaises Exception lambda lstm input cx hx TestFallbackWarning TestCase TODO Remove once test_testing py running MPS devices test_no_warning_on_import out = subprocess check_output sys executable -W always -c torch stderr=subprocess STDOUT On Windows opening subprocess default CWD makes ` torch ` fail so just set CWD script s directory cwd=os path dirname os path realpath __file__ decode utf- assertEqual out _get_not_implemented_op This can changed once we actually implement lcm Should fn args kwargs string_version torch lcm torch tensor device= mps torch tensor device= mps torch lcm torch tensor device= mps torch tensor device= mps test_error_on_not_implemented fn args kwargs _ = _get_not_implemented_op assertRaisesRegex NotImplementedError currently implemented MPS device fn args kwargs test_warn_on_not_implemented_with_fallback _ _ _ op = _get_not_implemented_op script = f os MUST happen before pytorch s os environ PYTORCH_ENABLE_MPS_FALLBACK = warnings warnings catch_warnings record=True w torch len w print w exit This should run just fine raise warning about perf warnings catch_warnings record=True w op len w = print w exit try subprocess check_output sys executable -W always -c script stderr=subprocess STDOUT On Windows opening subprocess default CWD makes ` torch ` fail so just set CWD script s directory cwd=os path dirname os path realpath __file__ except subprocess CalledProcessError e e returncode == assertTrue False There warning when importing torch when PYTORCH_ENABLE_MPS_FALLBACK set + e output decode utf- e returncode == assertTrue False There wasn t exactly one warning when running implemented op f PYTORCH_ENABLE_MPS_FALLBACK set e output assertTrue False Running implemented op failed even though PYTORCH_ENABLE_MPS_FALLBACK set + e output decode utf- TestNoRegression TestCase test_assert_close = torch ones device= mps b = torch zeros device= mps inf = b nan = b b assertRaisesRegex AssertionError Tensor-likes close torch testing assert_close inf TODO The NaN test failing when all tests test_mps run together passes when run separately There seems memory corruption which needs fixed test enabled assertRaisesRegex AssertionError Tensor-likes close torch testing assert_close nan test_double_error assertRaisesRegex TypeError MPS framework doesn t support float = torch ones dtype=torch float device= mps = torch ones device= mps assertRaisesRegex TypeError MPS framework doesn t support float = double test_legacy_constructor = torch ones device= mps b = new test_serialization_map_location Ensures cpu Tensor can loaded mps tempfile NamedTemporaryFile f x = torch rand torch save x f f seek x = torch load f map_location= mps assertEqual x x assertEqual x device type mps Ensures mps Tensors can loaded mps tempfile NamedTemporaryFile f x = torch rand device= mps torch save x f f seek x = torch load f assertEqual x x assertEqual x device type mps Ensures mps Tensors can loaded cpu tempfile NamedTemporaryFile f x = torch rand device= mps torch save x f f seek x = torch load f map_location= cpu assertEqual x x assertEqual x device type cpu Ensures ` mps ` Tensors can loaded mps tempfile NamedTemporaryFile f x = torch rand device= mps torch save x f f seek x = torch load f map_location= mps assertEqual x x assertEqual x device type mps MPS_GRAD_DTYPES = torch float torch float transform_opinfo_sample_to_cpu sample Transforms opinfo core SampleInput MPS CPU transform_sample x isinstance x torch Tensor x requires_grad = x requires_grad conjugated = x is_conj rc = x detach rc = rc cpu conjugated x conj cpu conj rc requires_grad_ x requires_grad cpu_sample = sample transform transform_sample Transform kwargs ` device= mps ` ` device= cpu ` cpu_sample kwargs get device == mps cpu_sample kwargs device = cpu cpu_sample TestConsistency TestCaseMPS TODO This only used while some ops being added This list should contain all ops dtypes eventually This can generated automatically ` new_mps_allowlist txt ` file doing ` EXPECTTEST_ACCEPT= python test_mps py TestConsistencyCPU ` You most likely do NOT want modify manually BF _LOW_PRECISION_LIST = nn functional linear nn functional gaussian_nll_loss FP _LOW_PRECISION_LIST = add sub div addcdiv __rdiv__ __rmul__ nn functional huber_loss true_divide kron gradient var std std_mean ldexp linalg vector_norm lerp addr var_mean var_mean_unbiased acosh asinh asin masked std nn functional avg_pool d NS Only backward pass nn functional normalize nn functional triplet_margin_loss nn functional triplet_margin_with_distance_loss nn functional batch_norm NOTE nn functional group_norm here because ULP difference mean output forward pass tolerable blew up into ULP difference backward pass MPS uses fp accumulation anyway nn functional group_norm nn functional instance_norm round xlogy addcmul nn functional cross_entropy nn functional binary_cross_entropy nn functional nll_loss nn functional max_pool d nn functional gelu nn functional glu _native_batch_norm_legit _batch_norm_with_update native_batch_norm softmax _softmax_backward_data log_softmax masked softmax masked log_softmax masked softmin nn functional kl_div nn functional softmin cross linalg cross prod masked prod nextafter native_layer_norm nn functional layer_norm nn functional interpolate nn functional upsample_nearest norm masked normalize arange linspace special xlog py CPU accumulates sequantially GPU does parallel _unsafe_masked_index_put_accumulate FP _LOW_PRECISION_LIST = conv d conv_transpose d conv_transpose d results have very small difference compared CPU CUDA so we use lower precision FP nn functional conv d nn functional conv_transpose d nn functional conv_transpose d matmul __rmatmul__ linalg multi_dot addbmm _compute_tolerances op dtype op name FP _LOW_PRECISION_LIST dtype torch float torch complex e- e- op name FP _LOW_PRECISION_LIST dtype torch float torch bfloat e- e- dtype == torch float e- e- op name BF _LOW_PRECISION_LIST dtype == torch bfloat e- e- op name nn functional conv_transpose d nn functional conv_transpose d nn functional conv_transpose d __rmatmul__ addbmm addmv baddbmm cov matmul mv dtype torch float torch bfloat e- e- dtype == torch float e- e- op name == masked mean e- e- op name == native_layer_norm e- e- op name fft rfftn fft hfftn fft hfft fft fft fft fftn fft rfft TODO Investigate why needed See https github com pytorch pytorch issues e- e- TODO Rounding broken linspace see https github com pytorch pytorch issues op name == linspace dtype torch int torch uint torch int torch int torch int None None Used accept mode only NEW_ALLOW_LIST = defaultdict list NEW_ALLOW_LIST_GRAD = defaultdict list ops mps_ops_modifier test_consistency_op_db allowed_dtypes=MPS_DTYPES test_output_match device dtype op assertEqual device mps include_conjugated_inputs = dtype is_complex op test_conjugated_samples get_samples op sample_inputs device dtype requires_grad= dtype is_floating_point dtype is_complex include_conjugated_inputs=include_conjugated_inputs set_seed=True mps_sample get_samples Forward check cpu_sample = transform_opinfo_sample_to_cpu mps_sample cpu_args = cpu_sample input + list cpu_sample args cpu_kwargs = cpu_sample kwargs mps_args = mps_sample input + list mps_sample args mps_kwargs = mps_sample kwargs tensor_split second tensor arg tensor_indices_or_sections must CPU only op name == tensor_split isinstance mps_args torch Tensor mps_args = cpu_args Order ops index_put guaranteed which can lead large errors inputs normalized op name == _unsafe_masked_index_put_accumulate dtype torch bfloat torch float mps_args = F normalize mps_args cpu_args = F normalize cpu_args warnings catch_warnings warnings filterwarnings ignore category=UserWarning cpu_out = op cpu_args cpu_kwargs mps_out = op mps_args mps_kwargs atol rtol = _compute_tolerances op dtype op name == nn functional interpolate dtype == torch uint cpu_kwargs get mode == bilinear cpu_kwargs get recompute_scale_factor True cpu_kwargs get scale_factor == For scale factors results will match CPU ones As MPS compute scales floats CPU always used doubles which results slight numerical differences atol rtol = op name _upsample_bilinear d_aa _upsample_bicubic d_aa cpu_kwargs get scale_factors == Similar above float vs double precision aresults slight error atol rtol = e- e- op name grid_sampler_ d asinh atol rtol = e- e- op name == kthvalue assertEqual cpu_out mps_out atol=atol rtol=rtol kthvalue non-deterministic input has repeated values dim = cpu_args len cpu_args - keep_dim = cpu_args len cpu_args False values = torch gather mps_sample input dim mps_out keep_dim mps_out unsqueeze dim assertEqual values keep_dim values squeeze dim mps_out continue assertEqual cpu_out mps_out atol=atol rtol=rtol ops mps_ops_grad_modifier copy deepcopy test_consistency_op_db allowed_dtypes=MPS_GRAD_DTYPES test_output_grad_match device dtype op assertEqual device mps get_samples op sample_inputs device dtype requires_grad= dtype is_floating_point dtype is_complex TODO Enable per-sample seed setting tweak tolerances fix xfails set_seed=False mps_sample get_samples Forward check cpu_sample = transform_opinfo_sample_to_cpu mps_sample cpu_args = cpu_sample input + list cpu_sample args cpu_kwargs = cpu_sample kwargs mps_args = mps_sample input + list mps_sample args mps_kwargs = mps_sample kwargs tensor_split second tensor arg tensor_indices_or_sections must CPU only op name == tensor_split isinstance mps_args torch Tensor mps_args = cpu_args Order ops index_put guaranteed which can lead large errors inputs normalized op name == _unsafe_masked_index_put_accumulate dtype torch bfloat torch float mps_args = F normalize mps_args cpu_args = F normalize cpu_args warnings catch_warnings warnings filterwarnings ignore category=UserWarning cpu_out = op cpu_args cpu_kwargs mps_out = op mps_args mps_kwargs op name == unique cpu_kwargs sorted False continue atol rtol = _compute_tolerances op dtype op name renorm norm linalg norm dtype == torch float atol = e- rtol = e- assertEqual cpu_out mps_out atol=atol rtol=rtol Backward check cpu_out = cpu_out isinstance cpu_out torch Tensor tuple cpu_out mps_out = mps_out isinstance mps_out torch Tensor tuple mps_out req_grad t isinstance t torch Tensor t requires_grad diff_cpu_out = tuple t t cpu_out req_grad t diff_mps_out = tuple t t mps_out req_grad t diff_cpu_arg = tuple t t pytree tree_leaves cpu_args cpu_kwargs req_grad t diff_mps_arg = tuple t t pytree tree_leaves mps_args mps_kwargs req_grad t assertEqual len diff_cpu_out len diff_mps_out assertEqual len diff_cpu_arg len diff_mps_arg len diff_cpu_out == continue rand_like does work certain dtypes so cast double cast back cpu_grad_outputs = tuple torch rand_like t dtype=torch double dtype=t dtype t diff_cpu_out mps_grad_outputs = tuple t mps t cpu_grad_outputs Compare computed gradients cpu given random grad_output vector Sometimes when derivative we just don t bother creating graph allow_unused needed those cases cpu_grad_inputs = torch autograd grad diff_cpu_out diff_cpu_arg grad_outputs=cpu_grad_outputs allow_unused=True mps_grad_inputs = torch autograd grad diff_mps_out diff_mps_arg grad_outputs=mps_grad_outputs allow_unused=True op name == nn functional pad op variant_test_name replicate reflect dtype == torch float atol = e- rtol = e- op name == nn functional unfold dtype == torch float atol rtol = e- e- Order ops unsafe_masked_index backward guaranteed which leads larger errors op name == _unsafe_masked_index dtype == torch float atol rtol = e- e- op name == logcumsumexp atol rtol = e- e- op name == nn functional max_pool d dtype == torch float In few cases where stride smaller than kernel size several output grad elements similar magnitudes get summed together introducing significant error float atol rtol = e- e- op name == nn functional embedding_bag dtype == torch float atol rtol = e- e- assertEqual cpu_grad_inputs mps_grad_inputs atol=atol rtol=rtol The CPU impl grid_sampler_ d gives large amount error half precision types So instead testing MPS-vs-CPU outputs test full-vs-half precision dtypes MPS dtypes torch float torch bfloat test_grid_sampler_ d_half_precision device dtype op = next op op test_consistency_op_db op name == grid_sampler_ d None include_conjugated_inputs = dtype is_complex op test_conjugated_samples get_samples op sample_inputs device dtype requires_grad= dtype is_floating_point dtype is_complex include_conjugated_inputs=include_conjugated_inputs set_seed=True half_sample get_samples half_input = half_sample input half_grid mode padding_mode align_corners = half_sample args full_input = half_input torch float detach full_grid = half_grid torch float detach warnings catch_warnings warnings filterwarnings ignore category=UserWarning half_out = op half_input half_grid mode padding_mode align_corners full_out = op full_input full_grid mode padding_mode align_corners atol rtol = e- e- assertEqual half_out full_out dtype atol=atol rtol=rtol test_grid_sampler_ d_nan device input = torch ones grid_nan = torch tensor torch nan out_cpu = torch grid_sampler_ d input grid_nan True out_mps = torch grid_sampler_ d input device grid_nan device True assertEqual out_mps out_cpu test_fmax_mixed_dtypes device Regression testing https github com pytorch pytorch issues fmax fmin implemented binary metal shaders they implemented assumption both args have same dtype x = torch rand device=device dtype=torch float x_int = torch randint - device=device dtype=torch int y = torch rand device=device dtype=torch float op torch fmax torch fmin assertEqual op x y op x mps y mps cpu assertEqual op x_int y op x_int mps y mps cpu Stride assertEqual op x t y op x mps t y mps cpu Broadcast assertEqual op x y op x mps y mps cpu TestErrorInputs TestCase _ignore_not_implemented_error = True ops mps_ops_error_inputs_modifier op op test_error_inputs_op_db op error_inputs_func None dtypes=OpDTypes none test_error_inputs device op assertEqual device mps TODO Enable per-sample seed setting tweak tolerances fix xfails mps_samples = op error_inputs device set_seed=False mps_sample mps_samples mps_sample_input = mps_sample sample_input error_type = mps_sample error_type error_regex = mps_sample error_regex mps_args = mps_sample_input input + list mps_sample_input args mps_kwargs = mps_sample_input kwargs tensor_split second tensor arg tensor_indices_or_sections must CPU only op name == tensor_split isinstance mps_args torch Tensor mps_args = mps_args cpu assertRaisesRegex error_type error_regex op mps_args mps_kwargs TestComplex TestCase test_tensor_scalar_binops Regression test https github com pytorch pytorch issues to_cpu x x cpu isinstance x torch Tensor x Allocate tensors mps torch device mps inputs = torch rand dtype=dtype dtype torch float torch half torch cfloat assertTrue all x device type == mps x inputs Add scalars inputs extend + j torch tensor + j dtype=torch chalf Iterate over all permutations types int float complex half ops excluding div x y itertools product inputs inputs op_name __add__ __sub__ __mul__ x_cpu y_cpu = map to_cpu x y res = getattr x op_name y res_cpu = getattr x_cpu op_name y_cpu assertEqual to_cpu res res_cpu f op_name x y produces different results res vs res_cpu Copied ` TestCommon ` ` test_ops py ` just enough duplicate ` test_numpy_ref ` MPS skipIfSlowGradcheckEnv TestCommon TestCase exact_dtype = True Verifies teardown no OpInfo still using dynamic dtypes CI classmethod tearDownClass cls super tearDownClass IS_CI err_msg = The operator s below using dynamic_dtypes OpInfo entries This OK testing sure set dtypes manually before landing your PR Assure no opinfo entry has dynamic_dtypes filtered_ops = list filter opinfo utils is_dynamic_dtype_set op_db op filtered_ops fmt_str = opinfo utils str_format_dynamic_dtype op err_msg += \n + fmt_str assert len filtered_ops == err_msg This MPS equivalent ` test_numpy_ref ` ` test_ops py ` It lives over here while MPS still requires some fairly heavy special casing test framework When MPS becomes more consistent can probably merged test using ` dtypesIfMPS torch float ` now assertions themselves need loosened suppress_warnings MPS only supports float ops _ref_test_ops allowed_dtypes= torch float test_numpy_ref_mps device dtype op Unlike ` test_numpy_ref ` test compares ` float ` since time test s creation MPS does support float Tensors TODO Enable per-sample seed setting tweak tolerances fix xfails inputs = op reference_inputs device dtype set_seed=False sample_input inputs compare_with_reference op op ref sample_input dtypes get_all_dtypes test_tensor_creation device dtype ones device torch ones dtype=dtype device=device dtype MPS_DTYPES assertRaises TypeError ones device mps_tensor = ones device cpu_tensor = ones cpu assertEqual mps_tensor cpu cpu_tensor TestMetalLibrary TestCaseMPS test_metal_arange x = torch zeros device= mps dtype=torch half lib = torch mps compile_shader kernel void arange device half x uint idx thread_position_in_grid x idx = idx lib arange x assertEqual x torch arange x numel device= mps dtype=x dtype test_metal_dispatch_ d x = torch empty device= mps y = torch empty_like x z = torch empty_like x lib = torch mps compile_shader kernel void arange_x device float x uint idx thread_position_in_grid x idx x + idx y + idx z = idx x kernel void arange_y device float x uint idx thread_position_in_grid x idx x + idx y + idx z = idx y kernel void arange_z device float x uint idx thread_position_in_grid x idx x + idx y + idx z = idx z Check one can enumerate all shaders assertEqual set dir lib f arange_ i i x y z lib arange_x x lib arange_y y threads= y numel lib arange_z z threads= z numel assertEqual x torch arange x numel device= mps dtype=x dtype assertEqual x y assertEqual x z test_metal_arange_with_arg start= step= x = torch zeros device= mps lib = torch mps compile_shader kernel void arange device float x constant float start constant float step uint idx thread_position_in_grid x idx = start + idx step lib arange x start step assertEqual x torch arange start device= mps test_metal_arange_with_arg_and_scalar_tensor test_metal_arange_with_arg step=torch tensor test_metal_arange_with_arg_and_scalar_tensor_float test_metal_arange_with_arg step=torch tensor dtype=torch float test_metal_arange_with_arg_and_cast x = torch zeros device= mps dtype=torch half y = torch zeros device= mps dtype=torch half lib = torch mps compile_shader kernel void arange_all_half device half x constant half start_step uint idx thread_position_in_grid x idx = start_step x + idx start_step y kernel void arange_half_float device half x constant half start constant float step uint idx thread_position_in_grid x idx = start + idx step lib arange_all_half x arg_casts= fp lib arange_half_float y arg_casts= fp assertEqual x torch arange device= mps dtype=x dtype assertEqual x y test_metal_error_checking Syntax error asserts assertRaises SyntaxError lambda torch mps compile_shader Syntax error cpu_tensor = torch rand mps_tensor = torch rand device= mps lib = torch mps compile_shader kernel void full device half x x = Passing CPU tensor asserts assertRaises RuntimeError lambda lib full cpu_tensor Passing invalid shader name asserts assertRaises RuntimeError lambda lib non_existing mps_tensor Passing no tensors asserts assertRaises RuntimeError lambda lib full Exceeing thread group size asserts max_thread_group_size = lib full max_threads_per_threadgroup assertRaises ValueError lambda lib full mps_tensor group_size=max_thread_group_size + assertRaises ValueError lambda lib full mps_tensor threads= max_thread_group_size group_size= max_thread_group_size test_metal_include Checks includes embedding works lib = torch mps compile_shader #include c metal special_math h assertIsNotNone lib parametrize dtype torch float torch float torch bfloat torch int torch int test_reduction_utils dtype torch _inductor codegen mps DTYPE_TO_METAL lib = torch mps compile_shader f #include c metal reduction_utils h kernel void do_sum device DTYPE_TO_METAL dtype out constant DTYPE_TO_METAL dtype inp uint idx thread_position_in_grid out idx = c metal simd_sum inp idx kernel void do_max device DTYPE_TO_METAL dtype out device int out constant DTYPE_TO_METAL dtype inp uint idx thread_position_in_grid auto rc = c metal simd_argmax inp idx out idx = rc first out idx = rc second x = torch testing make_tensor device= mps dtype=dtype y = torch empty_like x z = torch empty_like x z = torch empty_like x dtype=torch int lib do_sum y x lib do_max z z x x_sum = x sum x_max x_max_idx = x max dim= max_err = y - x_sum abs max item assertLess max_err e- dtype == torch float e- f results y all elements should have been x_sum item assertTrue z == x_max all item f results z all elements should have been x_max item assertTrue z == x_max_idx all item f results z all elements should have been x_max_idx item Test nan propagation dtype is_floating_point idx = x idx = torch nan lib do_max z z x assertTrue z isnan all item f results z all elements shold have been nan assertTrue z == idx all item f results z all elements shold have been idx parametrize dtype torch float torch float torch int torch bfloat test_atomic_add dtype torch _inductor codegen mps DTYPE_TO_METAL mdtype = DTYPE_TO_METAL dtype lib = torch mps compile_shader f #include c metal atomic h using namespace c metal kernel void atomic_add device AtomicType mdtype type out constant mdtype inc uint idx thread_position_in_grid AtomicType mdtype atomic_add out idx inc idx x = torch arange device= mps dtype=dtype y = torch arange device= mps dtype=dtype lib atomic_add x y assertEqual x assertEqual x test_argument_buffers lib = torch mps compile_shader constant constexpr auto nbuffers = struct Inputs metal array device float nbuffers args kernel void sum_all device float output constant Inputs inputs uint idx thread_position_in_grid auto rc = inputs args idx auto i = i nbuffers ++i rc += inputs args i idx output idx = rc inputs = torch rand device= mps unbind output = torch empty_like inputs lib sum_all output inputs correct = torch zeros_like inputs inp inputs correct += inp assertEqual correct output unittest skipIf torch mps profiler is_metal_capture_enabled Set MTL_CAPTURE_ENABLED try again test_metal_capture lib = torch mps compile_shader kernel void full device float x uint idx thread_position_in_grid x idx = mps_tensor = torch rand device= mps capture_name = f lib_full join random choice i range capture_dirname = f - capture_name gputrace os path exists capture_dirname shutil rmtree capture_dirname torch mps profiler metal_capture capture_name assertTrue torch mps profiler is_capturing_metal lib full mps_tensor assertEqual mps_tensor sum item assertTrue os path exists capture_dirname f Capture file capture_dirname has been generated capture_listdir = os listdir capture_dirname shutil rmtree capture_dirname assertGreater len capture_listdir f Capture file capture_dirname contains only metadata i e capture_listdir TODO Actually instantiate test mps device better reflect what doing This requires mps properly registered device generic test framework which case right now We can probably use ` allow_mps ` introduced https github com pytorch pytorch pull achieve instantiate_device_type_tests TestConsistency globals allow_mps=True only_for= mps instantiate_device_type_tests TestErrorInputs globals allow_mps=True only_for= mps instantiate_device_type_tests TestCommon globals allow_mps=True only_for= mps instantiate_device_type_tests TestLinalgMPS globals allow_mps=True only_for= mps instantiate_parametrized_tests TestAutocastMPS instantiate_parametrized_tests TestLogical instantiate_parametrized_tests TestMPS instantiate_parametrized_tests TestSDPA instantiate_parametrized_tests TestSmoothL Loss instantiate_parametrized_tests TestMetalLibrary __name__ == __main__ run_tests