Owner s module onnx Test consistency between output values torch onnx exported operators torch operators given same inputs Usage pytest test onnx test_op_consistency py To run tests specific operator e g torch ceil pytest test onnx test_op_consistency py -k ceil pytest test onnx test_op_consistency py -k nn_functional_scaled_dot_product_attention Read more Running writing tests https github com pytorch pytorch wiki Running-and-writing-tests Note When new ops supported please scroll down modify EXPECTED_SKIPS_OR_FAILS TESTED_OPS lists See Modify section __future__ annotations copy typing Optional onnx_test_common parameterized For readability these two allowed imported function onnx_test_common skip xfail torch torch testing _internal common_device_type common_methods_invocations common_utils OPS_DB = copy deepcopy common_methods_invocations op_db Modify section ########################################################## NOTE Modify section more ops supported The list should sorted alphabetically For example add test torch ceil Add ceil TESTED_OPS then run pytest If test fails fix error add new entry EXPECTED_SKIPS_OR_FAILS TODO Directly modify DecorateInfo each OpInfo ob_db when all ops enabled Ops tested numerical consistency between onnx pytorch TODO https github com pytorch pytorch issues TESTED_OPS frozenset str = frozenset atan atan atleast_ d How support list input atleast_ d atleast_ d broadcast_to ceil expand flatten hstack logical_not logit nn functional scaled_dot_product_attention repeat round scatter_add scatter_reduce sqrt stft t tile unflatten vstack fmt off Turn off black formatting keep list compact Expected failures onnx export The list should sorted alphabetically op name Q When should I use fixme vs vs skip vs xfail A Prefer xfail over skip when possible If test now failing because xpass because some previous errors now fixed removed corresponding xfail b If test failing consistently use skip EXPECTED_SKIPS_OR_FAILS tuple onnx_test_common DecorateMeta = skip atan dtypes=onnx_test_common BOOL_TYPES + onnx_test_common INT_TYPES reason=onnx_test_common reason_onnx_does_not_support Atan xfail atan dtypes= torch float reason=onnx_test_common reason_onnx_runtime_does_not_support Atan f skip atan dtypes=onnx_test_common BOOL_TYPES + onnx_test_common INT_TYPES reason=onnx_test_common reason_onnx_does_not_support Atan xfail atan dtypes= torch float reason=onnx_test_common reason_onnx_runtime_does_not_support Atan f xfail ceil dtypes=onnx_test_common BOOL_TYPES + onnx_test_common INT_TYPES reason=onnx_test_common reason_onnx_does_not_support Ceil skip hstack opsets= onnx_test_common opsets_before reason=onnx_test_common reason_onnx_does_not_support ConcatFromSequence xfail logit dtypes=onnx_test_common BOOL_TYPES + onnx_test_common INT_TYPES reason=onnx_test_common reason_onnx_does_not_support Log bool int skip nn functional scaled_dot_product_attention opsets= onnx_test_common opsets_before reason= Need Trilu skip nn functional scaled_dot_product_attention reason= fixme ORT crashes Windows segfaults randomly Linux xfail round opsets= onnx_test_common opsets_before reason=onnx_test_common reason_onnx_does_not_support Round xfail round variant_name= decimals_ opsets= onnx_test_common opsets_before reason=onnx_test_common reason_onnx_does_not_support Round xfail round variant_name= decimals_ opsets= onnx_test_common opsets_before reason=onnx_test_common reason_onnx_does_not_support Round xfail round variant_name= decimals_neg_ opsets= onnx_test_common opsets_before reason=onnx_test_common reason_onnx_does_not_support Round skip scatter_reduce variant_name= amin opsets= onnx_test_common opsets_before reason=onnx_test_common reason_onnx_does_not_support ScatterElements reduction skip scatter_reduce variant_name= amax opsets= onnx_test_common opsets_before reason=onnx_test_common reason_onnx_does_not_support ScatterElements reduction skip scatter_reduce variant_name= prod opsets= onnx_test_common opsets_before reason=onnx_test_common reason_onnx_does_not_support ScatterElements reduction xfail scatter_reduce variant_name= mean reason=onnx_test_common reason_onnx_does_not_support ScatterElements reduction=mean skip scatter_reduce variant_name= sum opsets= onnx_test_common opsets_before reason=onnx_test_common reason_onnx_does_not_support ScatterElements reduction xfail scatter_reduce variant_name= sum dtypes= torch float reason=onnx_test_common reason_onnx_runtime_does_not_support ScatterElements reduction=sum float xfail scatter_reduce variant_name= prod dtypes= torch float reason=onnx_test_common reason_onnx_runtime_does_not_support ScatterElements reduction=prod float xfail scatter_reduce variant_name= amin dtypes=onnx_test_common BOOL_TYPES + torch float reason=onnx_test_common reason_onnx_runtime_does_not_support ScatterElements reduction=amin float xfail scatter_reduce variant_name= amax dtypes=onnx_test_common BOOL_TYPES + torch float reason=onnx_test_common reason_onnx_runtime_does_not_support ScatterElements reduction=amax float xfail scatter_reduce variant_name= mean reason= ONNX doesn t support reduce= mean option skip sqrt dtypes=onnx_test_common BOOL_TYPES reason=onnx_test_common reason_onnx_does_not_support Sqrt skip stft opsets= onnx_test_common opsets_before reason=onnx_test_common reason_onnx_does_not_support STFT xfail stft reason=onnx_test_common reason_onnx_runtime_does_not_support STFT Regression ORT= percent difference skip tile opsets= onnx_test_common opsets_before reason=onnx_test_common reason_onnx_does_not_support Tile xfail unflatten opsets= onnx_test_common opsets_before reason= Helper function needed support legacy ops skip vstack opsets= onnx_test_common opsets_before reason=onnx_test_common reason_onnx_does_not_support ConcatFromSequence fmt SKIP_XFAIL_SUBTESTS tuple onnx_test_common DecorateMeta = skip nn functional scaled_dot_product_attention matcher=lambda sample sample kwargs get dropout_p = reason= dropout random so results do match skip repeat reason= Empty repeats value leads invalid graph matcher=lambda sample sample args skip scatter_reduce ONNX has include_self parameter default include_self=True mode matcher=lambda sample sample kwargs get include_self False reason= ONNX does t support include_self=False option skip stft reason= ONNX STFT does support complex results matcher=lambda sample sample kwargs get return_complex True skip tile matcher=lambda sample any dim == dim sample input shape sample input shape reason= Logic implemented size inputs op Reshape skip unflatten reason= Logic implemented size inputs op Reshape matcher=lambda sample any dim == dim sample input shape END OF SECTION TO MODIFY ##################################################### OP_WITH_SKIPPED_XFAIL_SUBTESTS = frozenset meta op_name meta SKIP_XFAIL_SUBTESTS ALL_OPS_IN_DB = frozenset op_info name op_info OPS_DB Assert all ops OPINFO_FUNCTION_MAPPING OPS_DB assert TESTED_OPS issubset ALL_OPS_IN_DB f TESTED_OPS - ALL_OPS_IN_DB OPS_DB SingleOpModel torch nn Module Test model wrap around single op export __init__ op kwargs super __init__ operator = op kwargs = kwargs forward args operator args kwargs _should_skip_xfail_test_sample op_name str sample - tuple Optional str Optional str Returns reason test sample should skipped op_name OP_WITH_SKIPPED_XFAIL_SUBTESTS None None decorator_meta SKIP_XFAIL_SUBTESTS Linear search ops_test_data SKIP_XFAIL_SUBTESTS That s fine because list small decorator_meta op_name == op_name assert decorator_meta matcher None Matcher must defined decorator_meta matcher sample decorator_meta test_behavior decorator_meta reason None None _get_test_class_name cls num params_dict - str del cls unused del num unused params_dict name parameterized parameterized_class name f TestOnnxModelOutputConsistency_opset opset opset_version opset opset onnx_test_common TESTED_OPSETS class_name_func=_get_test_class_name TestOnnxModelOutputConsistency onnx_test_common _TestONNXRuntime Test output consistency between exported ONNX models PyTorch eager mode This parameterized test suite opset_version = - common_device_type ops op op OPS_DB op name TESTED_OPS allowed_dtypes=onnx_test_common INT_TYPES + onnx_test_common FLOAT_TYPES + onnx_test_common BOOL_TYPES test_output_match device str dtype torch dtype op Test ONNX exporter device provided instantiate_device_type_tests we only want run cpu assert device == cpu samples = op sample_inputs device dtype requires_grad=False i cpu_sample enumerate samples inputs = cpu_sample input cpu_sample args Provide repr subtest because tensors serializable parallel test runs subTest opset=self opset_version sample_num=i inputs=repr inputs kwargs=repr cpu_sample kwargs test_behavior reason = _should_skip_xfail_test_sample op name cpu_sample onnx_test_common normal_xfail_skip_test_behaviors test_behavior reason model = SingleOpModel op cpu_sample kwargs model eval dtype == torch float Relax atol rtol float based empirical results The current most relaxed values aten stft rtol = e- atol = e- dtype == torch float The current most relaxed values aten stft rtol = e- atol = e- rtol = None atol = None Run test run_test model inputs rtol=rtol atol=atol opset onnx_test_common TESTED_OPSETS The name needs match parameterized_class name test_class_name = f TestOnnxModelOutputConsistency_opset opset onnx_test_common add_decorate_info OPS_DB test_class_name test_output_match opset=opset skip_or_xfails=EXPECTED_SKIPS_OR_FAILS common_device_type instantiate_device_type_tests globals test_class_name globals only_for= cpu __name__ == __main__ common_utils run_tests