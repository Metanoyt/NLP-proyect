Owner s module nn math copy torch torch testing _internal common_device_type dtypes dtypesIfCUDA instantiate_device_type_tests onlyCUDA skipMeta torch testing _internal common_utils parametrize run_tests TestCase TEST_WITH_ROCM TestMHADeviceType TestCase torch no_grad _test_transform_bias_rescale_qkv_impl device dtype use_nt use_padding=False tests = dim_per_head = does divide evenly CPU vectorization length Make sure CUDA can handle small input sizes dim_per_head = does divide evenly CUDA vectorization length causes alignment issues embed_dim num_heads bs sl tests subTest embed_dim=embed_dim num_heads=num_heads bs=bs sl=sl torch manual_seed dense_x = x = torch randn bs sl embed_dim device=device dtype=dtype use_padding x - = torch full x - shape float -Inf use_nt xs = list torch unbind x use_padding xs = xs - x = torch nested nested_tensor xs device=device dtype=dtype qkv = torch nn Linear embed_dim embed_dim device=device dtype=dtype We have use inference_mode here because q k v all views same Tensor which autograd doesn t like This fine because function only exposed Python purposes writing test torch inference_mode q k v = torch _transform_bias_rescale_qkv x qkv bias num_heads=num_heads simple_transform_bias_rescale_qkv qkv bias q k v = torch split qkv embed_dim dim=- q_bias k_bias v_bias = torch split bias embed_dim dim=- embiggen x use_nt x b t d = x size t = t + - t newsize = b t d new_x = torch zeros newsize device=device dtype=dtype new_x x size x size x size = x new_x tuple embiggen x reshape bs - num_heads embed_dim num_heads transpose x q + q_bias math sqrt embed_dim num_heads k + k_bias v + v_bias correct_q correct_k correct_v = simple_transform_bias_rescale_qkv dense_x qkv bias use_nt use_padding t correct_q correct_k correct_v t t == float -Inf = assertEqual q size correct_q size torch testing assert_close q correct_q torch testing assert_close k correct_k torch testing assert_close v correct_v dtypesIfCUDA torch float dtypes torch float skipMeta test_transform_bias_rescale_qkv device dtype use_padding False True subTest use_padding=use_padding _test_transform_bias_rescale_qkv_impl device dtype use_nt=False use_padding=use_padding dtypesIfCUDA torch float dtypes torch float skipMeta onlyCUDA test_transform_bias_rescale_qkv_nested device dtype use_padding False True subTest use_padding=use_padding _test_transform_bias_rescale_qkv_impl device dtype use_nt=True use_padding=use_padding _test_multihead_attention_impl device dtype mode use_nt need_weights average_attn_weights use_padding=False pad_all=False embed_dim = num_heads = bs = sl = q = torch rand bs sl embed_dim device=device dtype=torch float - use_padding pad_all q_i q q_i - = torch zeros_like q - device=device dtype=torch float mask = torch zeros q shape - device=device dtype=torch bool mask_i mask mask_i - = True q - = torch zeros_like q - device=device dtype=torch float mask = torch zeros q shape - device=device dtype=torch bool mask - = True mode == k = q v = q mode == encdec k = torch rand bs sl embed_dim device=device dtype=torch float - v = k mode == generic k = torch rand bs sl embed_dim device=device dtype=torch float - v = torch rand bs sl embed_dim device=device dtype=torch float - fail f invalid mode ` mode ` qkv = torch nn Linear embed_dim embed_dim device=device dtype=torch float native_qkv = copy deepcopy qkv dtype=dtype proj = torch nn Linear embed_dim embed_dim device=device dtype=torch float native_proj = copy deepcopy proj dtype=dtype pt = torch nn MultiheadAttention embed_dim num_heads batch_first=True device=device dtype=torch float pt in_proj_weight = qkv weight pt in_proj_bias = qkv bias pt out_proj weight = proj weight pt out_proj bias = proj bias NativeMHA torch nn Module __init__ embed_dim num_heads qkv proj super __init__ qkv = qkv proj = proj embed_dim = embed_dim num_heads = num_heads forward q k v key_padding_mask torch _native_multi_head_attention q k v embed_dim num_heads qkv weight qkv bias proj weight proj bias key_padding_mask need_weights=need_weights average_attn_weights=average_attn_weights mask_type= mask_type = = src_key_padding_mask mask_type = = src_mask npt = NativeMHA embed_dim=embed_dim num_heads=num_heads qkv=native_qkv proj=native_proj dtype device == cuda pt = pt cuda npt = npt cuda ypt weight_pt = pt q k v need_weights=need_weights average_attn_weights=average_attn_weights key_padding_mask=mask use_padding None use_nt qs = list torch unbind q use_padding pad_all qs = x - x qs qs = qs - q = torch nested nested_tensor qs device=device dtype=dtype mode == k = v = q mode == encdec k = torch nested nested_tensor torch unbind k device=device dtype=dtype v = k k = torch nested nested_tensor torch unbind k device=device dtype=dtype v = torch nested nested_tensor torch unbind v device=device dtype=dtype native_q = q dtype=dtype native_k = k dtype=dtype native_v = v dtype=dtype ynpt weight_npt = npt native_q native_k native_v key_padding_mask=mask use_padding use_nt None use_nt ynpt = ynpt to_padded_tensor pad_all ynpt_final = torch zeros_like ypt ynpt_final ynpt shape = ynpt ynpt = ynpt_final do_pad_all tensors t tensors t_i t t_i - = torch zeros_like t_i - device=device dtype=dtype PyTorch implementation returns non-zero junk padding locations overwrite so comparison works out use_padding ypt - = torch zeros_like ypt - device=device dtype=dtype ynpt - = torch zeros_like ynpt - device=device dtype=dtype pad_all do_pad_all ypt ynpt Zero last row each TxT weight matrix need_weights average_attn_weights weight_pt - = torch zeros_like weight_pt - device=device dtype=dtype weight_npt - = torch zeros_like weight_npt - device=device dtype=dtype pad_all do_pad_all weight_pt weight_npt nh range num_heads weight_pt nh - = torch zeros_like weight_pt nh - device=device dtype=dtype weight_npt nh - = torch zeros_like weight_npt nh - device=device dtype=dtype dtype == torch half torch testing assert_close ypt ynpt torch float atol= e- rtol= e- High rtol seems necessary test_native_multihead_attention_cpu_float Windows otherwise e- would likely fine torch testing assert_close ypt ynpt atol= e- rtol= e- need_weights torch testing assert_close weight_pt weight_npt torch float atol= e- rtol= e- assertEqual weight_pt weight_npt dtypesIfCUDA torch float torch half dtypes torch float skipMeta parametrize use_nt False True parametrize use_padding pad_all False False True False True True parametrize need_weights False parametrize average_attn_weights False True parametrize fused False True torch no_grad test_native_multihead_self_attention device dtype use_nt need_weights average_attn_weights use_padding pad_all fused TEST_WITH_ROCM use_nt use_padding pad_all skipTest Large numerical errors ROCM investigate use_padding pad_all fused skipTest Large numerical errors ROCM investigate need_weights False pad_all subTest use_padding=use_padding pad_all=pad_all use_nt=use_nt need_weights=need_weights average_attn_weights=average_attn_weights torch backends cuda sdp_kernel enable_flash=False enable_mem_efficient=False fused torch backends cuda sdp_kernel enable_flash=True enable_mem_efficient=True _test_multihead_attention_impl device dtype use_nt=use_nt use_padding=use_padding pad_all=pad_all need_weights=need_weights average_attn_weights=average_attn_weights dtypesIfCUDA torch float torch half dtypes torch float skipMeta torch no_grad test_native_multihead_encoder_decoder_attention device dtype _test_multihead_attention_impl device dtype encdec use_nt=False need_weights=False average_attn_weights=False dtypesIfCUDA torch float torch half dtypes torch float skipMeta torch no_grad test_native_multihead_attention device dtype _test_multihead_attention_impl device dtype generic use_nt=False need_weights=False average_attn_weights=False instantiate_device_type_tests TestMHADeviceType globals __name__ == __main__ run_tests