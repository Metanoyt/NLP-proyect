mypy allow-untyped-defs contextlib functools collections abc Callable typing Any Union torch torch utils _pytree pytree torch _C DispatchKey torch _higher_order_ops utils _maybe_run_with_interpreter _set_compilation_env autograd_not_implemented check_input_alias_and_mutation_return_outputs check_meta_consistency fill_none_with_masks filter_with_masks materialize_as_graph reenter_make_fx validate_subgraph_args_types torch _ops HigherOrderOperator torch _subclasses fake_tensor FakeTensorMode torch fx experimental proxy_tensor _temp_remove_metadata_torch_function_mode disable_proxy_modes_tracing ProxyTorchDispatchMode track_tensor_tree WhileLoopOp HigherOrderOperator __init__ - None super __init__ while_loop __call__ cond_fn Callable body_fn Callable carried_inputs tuple Union torch Tensor int float bool additional_inputs tuple Union torch Tensor torch SymInt int isinstance carried_inputs tuple list raise RuntimeError f carried_inputs must tuple list got type carried_inputs isinstance additional_inputs tuple list raise RuntimeError f additional_inputs must tuple list got type additional_inputs validate_subgraph_args_types carried_inputs validate_subgraph_args_types additional_inputs super __call__ cond_fn body_fn carried_inputs additional_inputs pyrefly ignore bad-override gen_schema cond_fn body_fn carried_inputs additional_inputs torch _higher_order_ops schema HopSchemaGenerator torch _higher_order_ops utils materialize_as_graph all_inputs = carried_inputs + additional_inputs cond_gm torch fx GraphModule = cond_fn isinstance cond_fn torch fx GraphModule materialize_as_graph cond_fn all_inputs body_gm torch fx GraphModule = body_fn isinstance body_fn torch fx GraphModule materialize_as_graph body_fn all_inputs _find_example_value n real_inp val n meta n meta val example_value n meta n meta example_value assert isinstance real_inp torch Tensor real_inp _ _ _ body_mutated_inputs body_outputs = check_input_alias_and_mutation_return_outputs body_gm _ _ _ cond_mutated_inputs _ = check_input_alias_and_mutation_return_outputs cond_gm mutated_inputs = set body_mutated_inputs &#124; set cond_mutated_inputs schema_gen = HopSchemaGenerator schema_gen add_arg cond_fn cond_gm schema_gen add_arg body_fn body_gm idx arg enumerate carried_inputs schema_gen add_arg f carried_input idx arg is_mutated=idx mutated_inputs idx arg enumerate additional_inputs additional_idx = len carried_inputs + idx schema_gen add_arg f additional_input idx arg is_mutated=additional_idx mutated_inputs out body_outputs schema_gen add_output out schema_gen add_schema_tree_spec cond_fn body_fn carried_inputs additional_inputs schema_gen gen_schema while_loop_op = WhileLoopOp while_loop cond_fn body_fn carried_inputs r Run body_fn carried_inputs while cond_fn carried_inputs returns True scalar tensor Returns output body_fn initial carried_inputs warning ` torch while_loop ` prototype feature PyTorch It has limited support input output types doesn t support training currently Please look forward more stable implementation future version PyTorch Read more about feature classification https pytorch org blog pytorch-feature-classification-changes #prototype ` while_loop ` structured control flow operator It preserves loop semantic across torch compile torch export ` while_loop ` equivalent following while_loop cond_fn body_fn carried_inputs val = carried_inputs while cond_fn val val = body_fn val val Args cond_fn Callable A callable function returns boolean Scalar tensor python boolean body_fn Callable A callable function takes same inputs ` cond_fn ` returns tuple tensors ints carried_inputs Tuple possibly nested dict list tuple tensors ints A tuple inputs cond_fn body_fn It s also initial value states carried across iterations Note when pass integer carry corresponding while_loop will another int unknown values because we don t know how many iterations while_loop will run Example cond_fn iter x iter sum body_fn iter x iter + x sin while_loop cond_fn body_fn torch zeros torch randn Example cond_fn int_iter x int_iter x shape body_fn int_iter x int_iter + x + int_iter while_loop cond _fn body_fn torch randn Restrictions - body_fn must tensors int same metadata e g shape dtype inputs - body_fn cond_fn must in-place mutate carried_inputs A clone before mutation required - body_fn cond_fn must mutate python variables e g list dict created outside body_fn - body_fn cond_fn s output cannot alias any inputs A clone required warning Temporal Limitations - while_loop only supports inference right now Autograd will supported future torch _dynamo backends debugging make_eager_backend_with_torch_function_mode Currently additional_inputs user-facing input It will automatically set dynamo parameters buffers accessed cond_fn body_fn tensor closures will become additional_inputs additional_inputs tuple = The reason we flatten output before calling into dynamo we want create consistent input ordering cond_fn body_fn we also want input ordering matches output ordering Also see NOTE why we cannot use automatic while_loop Construct flat cond_fn flat_body_fn which takes flattened inputs flat_inputs in_spec = pytree tree_flatten carried_inputs additional_inputs flat_cond_fn flat_args carried additional = pytree tree_unflatten flat_args in_spec cond_fn carried additional flat_body_fn flat_args carried additional = pytree tree_unflatten flat_args in_spec body_fn carried additional torch compiler is_dynamo_compiling while_loop_op flat_cond_fn flat_body_fn tuple flat_inputs tuple _validate_input cond_fn body_fn carried_inputs torch _higher_order_ops utils validate_subgraph_args_types callable cond_fn callable body_fn raise RuntimeError Expect cond_fn body_fn callable validate_subgraph_args_types flat_inputs pytree tree_all lambda t isinstance t torch Tensor torch SymInt int carried_inputs raise RuntimeError Expect carried_inputs tuple possibly nested dict list tuple only f consists tensor int leaves got carried_inputs _validate_input cond_fn body_fn carried_inputs Dynamo expecting callable __code__ attribute We cannot directly pass cond_op So we wrap dummy function _while_loop_op_wrapper args kwargs while_loop_op args kwargs _set_compilation_env torch _dynamo utils disable_cache_limit _temp_remove_metadata_torch_function_mode metadata_mode _temp_remove_metadata_torch_function_mode metadata_mode metadata_mode backend Union str Callable Any = make_eager_backend_with_torch_function_mode metadata_mode backend = eager torch compile _while_loop_op_wrapper backend=backend fullgraph=True flat_cond_fn flat_body_fn tuple flat_inputs tuple while_loop_op py_impl DispatchKey CompositeExplicitAutograd while_loop_dense cond_fn body_fn carried_inputs additional_inputs stack_output=False carried_vals = carried_inputs _validate_cond_output pred isinstance pred torch Tensor pred size == torch Size pred dtype == torch bool isinstance pred bool raise RuntimeError f cond_fn must boolean scalar tensor boolean got pred isinstance carried_inputs tuple list raise RuntimeError f carried_inputs must tuple list got type carried_inputs Check condition set up flag should_loop = cond_fn carried_vals additional_inputs _validate_cond_output should_loop should_loop stack_output tuple val unsqueeze clone isinstance val torch Tensor val val carried_vals tuple val clone isinstance val torch Tensor val val carried_vals outputs list list torch Tensor = _ carried_vals while should_loop out = body_fn carried_vals additional_inputs stack_output i o enumerate out outputs i append o assert isinstance out tuple f body_fn should tuple got type out assert len out == len carried_inputs body_fn should same number elements carried_inputs carried_vals = out should_loop = cond_fn carried_vals additional_inputs stack_output outs list torch Tensor = out outputs outs append torch stack out dim= tuple outs carried_vals while_loop_op py_autograd_impl while_loop_autograd cond_fn body_fn operands additional_inputs WhileLoopAutogradOp apply cond_fn body_fn len operands len additional_inputs operands additional_inputs _find_or_create_fake_mode - FakeTensorMode torch fx experimental symbolic_shapes ShapeEnv fake_mode = torch _guards detect_fake_mode fake_mode None fake_mode = FakeTensorMode shape_env=ShapeEnv fake_mode _create_unbacked_symint fake_mode FakeTensorMode ignore_fresh_unbacked_symbols bool - torch SymInt assert fake_mode None fake_mode shape_env None Must provide fake_mode shape_env ctx = contextlib nullcontext ignore_fresh_unbacked_symbols fake_mode shape_env ignore_fresh_unbacked_symbols ctx fake_mode shape_env create_unbacked_symint while_loop_op py_impl ProxyTorchDispatchMode while_loop_tracing mode cond_fn body_fn carried_inputs additional_inputs stack_output=False op = while_loop_stack_output_op stack_output while_loop_op _trace_while_loop proxy_mode op cond_fn body_fn carried_inputs additional_inputs NOTE unspecialize int carry unbacked symints When we support int carry we ll also need support int output body_fn because previous iteration s output next iteration s input they must match For carries when we start tracing while_loop they can - constants e g - backed symints x shape x shape + x stride x shape - unbacked symints e g u u + u u We choose most conservative design all cases we create new unbacked symints trace subgraph It s possible do some analysis initial carry output first iteration determine better range output unbacked symbol e g when input unbacked symint = before while_loop general difficult because we don t know number iterations Users would have re-constrain unbacked symint subgraph needed For output fake cond_fn could constant bool SymBool e g x shape where x shape can either static dynamic In case constant bool we should do specialization NYI For output fake body_fn could all three types though user s point view they re all integers e g init_carry = s u t body_fn u s u t t shape t shape t shape y + It may seem constant output isn t possible users shouldn t write while_loop always But could shape set dynamic properly e g automatic dynamic hasn t been triggered For reason we treat int symint outputs same way - they can match against any int symint carry - we unspecialize them new unbacked symints fake while_loop Similarly we could do some analysis refine output ranges s easier start fresh unbacked symints One surprising case can input unbacked symint constrained users = either before while_loop inside body_fn increments each iteration Ideally we should know final output = we didn t constrain unbacked symint output subgraph today because requires smart range analysis fake_mode FakeTensorMode = _find_or_create_fake_mode _unspecialize_carried_inputs x isinstance x int torch SymInt _create_unbacked_symint fake_mode ignore_fresh_unbacked_symbols=True Note unspecialize constant tensor carry We need disable constant specialization tensor inputs become loop carries Here s problem when user creates constant tensor e g torch tensor PyTorch calls aten lift_fresh_copy create safe copy avoiding aliasing issues which creates FakeTensor constant=True But when FakeTensor becomes loop carry we have problem - Operations like item will read constant value bake into traced code - This incorrect because carry variables change between loop iterations - The traced code would use wrong constant value all iterations Solution We clone constant tensors mark cloned tensor non-constant so they won t specialized fixed values during tracing body_fn cond_fn isinstance x torch Tensor x = x clone hasattr x constant x constant None pyrefly ignore missing-attribute x constant = None x disable_proxy_modes_tracing unspecialized_carried_inputs = pytree tree_map_only int torch SymInt torch Tensor For temporarily created unbacked symints we don t need bind them any proxy lambda x _unspecialize_carried_inputs x carried_inputs produce_graph fn cloned_carried_inputs = pytree tree_map_only torch Tensor lambda x x clone unspecialized_carried_inputs reenter_make_fx fn cloned_carried_inputs additional_inputs cond_graph = produce_graph cond_fn body_graph = produce_graph body_fn next_name = None i = pyrefly ignore bad-assignment while next_name candidate = f while_loop_cond_graph_ i hasattr proxy_mode tracer root candidate i += next_name = candidate cond_graph_name = next_name body_graph_name = f while_loop_body_graph_ i assert hasattr proxy_mode tracer root body_graph_name proxy_mode tracer root register_module cond_graph_name cond_graph proxy_mode tracer root register_module body_graph_name body_graph args = cond_graph body_graph carried_inputs additional_inputs proxy_args = pytree tree_map proxy_mode tracer unwrap_proxy args out_proxy = proxy_mode tracer create_proxy call_function op proxy_args name=op _name out = op cond_graph body_graph unspecialized_carried_inputs additional_inputs track_tensor_tree out out_proxy constant=None tracer=proxy_mode tracer _trace_while_loop mode op cond_fn body_fn carried_inputs additional_inputs while_loop_op py_impl FakeTensorMode while_loop_fake_tensor_mode mode cond_fn body_fn carried_inputs additional_inputs stack_output=False mode NOTE Handling unback symints subgraph while_loop The idea scope unbacked symints limited subgraph We re implementing fake tensor mode while_loop operator we run body_fn once get fake output Let s first consider case unbacked symints tensor shapes Case unbacked symints local subgraph e g body_fn x nz = x nonzero it+ nz sum we can just ignore newly created unbacked symints because has no effect output while_loop s tracked when we tracing subgraph Case unbacked symints shape output while_loop e g body_fn x nz = x nonzero it+ nz This will fail shape check because each iteration carried_input s shape must match output shape nz shape contains newly allocated unbacked symint won t match carried_input s shape Case unbacked symints shape carried_inputs e g nz = nonzero body_fn nz it+ nz sin + There s no new unbacked symints allocated subgraph so we re safe mode shape_env ignore_fresh_unbacked_symbols body_fn output same pytree tensor meta data carried_inputs so we could just output after one iteration body_outs = body_fn carried_inputs additional_inputs check_meta_consistency carried_inputs body_outs carried_inputs body_output include_contiguity=False stack_output n_iter = _create_unbacked_symint mode ignore_fresh_unbacked_symbols=False assert all isinstance x torch Tensor x carried_inputs fake_outputs = tuple out clone unsqueeze repeat n_iter + tuple _ range out dim out body_outs pytree tree_map_only int torch SymInt For while_loop s unbacked symint output we want them bound proxy while_loop s output lambda _ _create_unbacked_symint mode ignore_fresh_unbacked_symbols=False fake_outputs See NOTE unspecialize int carry unbacked symints pytree tree_map_only int torch SymInt For while_loop s unbacked symint output we want them bound proxy while_loop s output lambda _ _create_unbacked_symint mode ignore_fresh_unbacked_symbols=False body_outs while_loop_op py_functionalize_impl while_loop_func ctx cond_fn body_fn carried_inputs additional_inputs stack_output=False torch _higher_order_ops utils _check_alias_and_mutation op = while_loop_stack_output_op stack_output while_loop_op unwrapped_carried_inputs = ctx unwrap_tensors carried_inputs unwrapped_additional_inputs = ctx unwrap_tensors additional_inputs unwrapped_inputs = unwrapped_carried_inputs + unwrapped_additional_inputs ctx redispatch_to_next functional_cond_fn = ctx functionalize _maybe_run_with_interpreter cond_fn functional_body_fn = ctx functionalize _maybe_run_with_interpreter body_fn pre_dispatch = hasattr ctx mode ctx mode pre_dispatch fn fn_name cond_fn cond_fn body_fn body_fn _check_alias_and_mutation fn unwrapped_inputs fn_name pre_dispatch ret = op functional_cond_fn functional_body_fn unwrapped_carried_inputs unwrapped_additional_inputs ctx wrap_tensors ret WhileLoopStackOutputOp HigherOrderOperator while_loop_stack_output variant while_loop returns stack outputs Its semantic can illurated using python code while_loop_stack_output cond_fn body_fn carried_inputs additional_inputs outs = while cond_fn carried_inputs additional_inputs out = body_fn carried_inputs additional_inputs outs append out torch stack outs It s useful supporting autograd while_loop __init__ - None super __init__ while_loop_stack_output __call__ cond_fn Callable body_fn Callable carried_inputs tuple Union torch Tensor int float bool additional_inputs tuple Union torch Tensor torch SymInt int isinstance carried_inputs tuple list raise RuntimeError f carried_inputs must tuple list got type carried_inputs isinstance additional_inputs tuple list raise RuntimeError f additional_inputs must tuple list got type additional_inputs validate_subgraph_args_types carried_inputs validate_subgraph_args_types additional_inputs super __call__ cond_fn body_fn carried_inputs additional_inputs Note while_loop autograd Consider wthe following while_loop can visualized additional_inputs ┌─────┬─────┼─────┬─────┐ &#124; &#124; &#124; &#124; &#124; ↓ ↓ ↓ ↓ ↓ x ──→ y ─→ y ─→ y ─→ y ─→ y The bacwkard can visualized follows g_additional_inputs ┌──────┬──────┼──────┬──────┐ &#124; &#124; &#124; &#124; &#124; &#124; &#124; &#124; &#124; &#124; gx ── gy ─ gy ─ gy ─ gy ─ gy We can compute gx using chain rule gx = gy bw y x where gy denotes gradient loss respect y bw y x denotes gradient y respect x Note bw can computed forward body_fn easily using torch autograd grad We could substitute unknowns gy gy chain rule until gy gx = gy bw y y bw y x = gy bw y y bw y y bw y x = = gy bw y y bw y y bw y y bw y y bw y x since gy graient final output which given backward input we ve got formula compute gx A abbr formula gy bw x In similar way we can compute g_additional_inputs using chain rule g_additional_inputs = gy bw y addi + gy bw y addi + gy bw y addi + + gy bw y addi Notice gy = gy bw gy = gy bw etc we now also get formula g_additional_inputs Implementation The idea implementation construct while_loop calculate both gx g_additional_inputs Specifically we can implement backward while_loop follows cond_fn idx grad_carries grad_additional_inputs fw_additional_inputs fw_inps idx fw_inps size body_fn idx grad_carries grad_additional_inputs fw_additional_inputs fw_inps reversed_idx = fw_inps size - - idx next_grad_carry next_grad_additional_inputs = bw fw_inps reversed_idx fw_additional_inputs grad_carries idx + next_grad_carry next_grad_additional_inputs + grad_additional_inputs idx = init_grad_carries = grads init_grad_additional_inputs = torch zeros_like g_additional_inputs fw_inps = torch cat ctx fw_carried_inputs fw_outputs - while_loop cond_fn body_fn idx init_grad_carries init_grad_additional_inputs fw_additional_inputs fw_inps WhileLoopAutogradOp torch autograd Function staticmethod pyrefly ignore bad-override forward ctx cond_fn body_fn num_carried_inputs num_additional_inputs carries_and_inputs torch _higher_order_ops scan split_into_chunks carries additional_inputs = split_into_chunks carries_and_inputs num_carried_inputs num_additional_inputs torch _C _AutoDispatchBelowAutograd fw_outputs = while_loop_stack_output_op cond_fn body_fn carries additional_inputs assert hasattr ctx fw_cond_fn assert hasattr ctx fw_body_fn assert hasattr ctx carries assert hasattr ctx additional_inputs assert hasattr ctx fw_outputs ctx fw_cond_fn = cond_fn ctx fw_body_fn = body_fn ctx carries = carries ctx additional_inputs = additional_inputs ctx fw_outputs = fw_outputs loop_count = None pyrefly ignore bad-assignment out fw_outputs isinstance out torch Tensor loop_count None assert out size == loop_count loop_count = out size assert loop_count None Remove loop_count pending_fresh_unbacked_symbols because s part forward output s impossible bind proxy forward graph anyways isinstance loop_count torch SymInt shape_env = loop_count node shape_env loop_count shape_env pending_fresh_unbacked_symbols shape_env pending_fresh_unbacked_symbols remove loop_count Even when body function executed we clone unsqueeze input avoid aliasing therefore loop_count always = torch _check loop_count = We snapshot dispatch keys forward materializing bw_graph backward ctx _fw_include_key_set = torch _C _dispatch_tls_local_include_set ctx _fw_exclude_key_set = torch _C _dispatch_tls_local_exclude_set assert len fw_outputs fw_outputs shouldn t empty Only last output fw_outputs need returned tuple ckp - ckp fw_outputs staticmethod backward ctx grads torch _higher_order_ops cond create_bw_fn torch _higher_order_ops scan split_into_chunks set up single step bw fn bw_body_fn = create_bw_fn ctx fw_body_fn ctx carries + ctx additional_inputs Note Handle inputs re differentiable When forward input non-differentiable e g symint integer tensor their gradients will None However we don t want None subgraph because complicates inductor codegen where we need do non-uniform treatment None tensors So we set up masks filter None gradients so only tensors returned each step carries_tensor_masks = bool isinstance t torch Tensor t dtype is_floating_point t ctx carries additional_inputs_tensor_masks = bool isinstance t torch Tensor t dtype is_floating_point t ctx additional_inputs init_idx = torch zeros dtype=torch int init_grad_carries = filter_with_masks grads carries_tensor_masks type ignore arg-type init_grad_additional_inputs = tuple torch zeros_like t need_keep t zip additional_inputs_tensor_masks ctx additional_inputs need_keep We need forward inputs each iteration compute backward which concatenation first iteraiton input i e ctx carries all iterations s output except last iteration fw_carries = torch cat carry unsqueeze carries - carry carries zip ctx carries ctx fw_outputs fw_carry carry zip fw_carries ctx carries fw_carry requires_grad_ carry requires_grad _ spec = pytree tree_flatten init_idx init_grad_carries init_grad_additional_inputs ctx fw_outputs ctx additional_inputs cond_fn flat_args idx grad_carries grad_additional_inputs fw_carries additional_inputs = pytree tree_unflatten flat_args spec assert isinstance fw_carries torch Tensor fw_carries excluding last iteration s output idx fw_carries size body_fn flat_args idx grad_carries grad_additional_inputs fw_carries additional_inputs = pytree tree_unflatten flat_args spec reversed_idx = fw_carries size - idx - selected_fw_carries = ckp select reversed_idx item ckp fw_carries cur_grad_carries cur_grad_additional_inputs = split_into_chunks bw_body_fn selected_fw_carries additional_inputs grad_carries len ctx carries len ctx additional_inputs assert all isinstance t torch Tensor t cur_grad_carries cur_grad_carries_tensors = filter_with_masks cur_grad_carries carries_tensor_masks cur_grad_additional_inputs_tensors = filter_with_masks cur_grad_additional_inputs additional_inputs_tensor_masks idx + cur_grad_carries_tensors cur_grad + grad cur_grad grad zip cur_grad_additional_inputs_tensors grad_additional_inputs args_single_step_bw = init_idx init_grad_carries init_grad_additional_inputs fw_carries ctx additional_inputs cond_gm = materialize_as_graph cond_fn args_single_step_bw ctx _fw_include_key_set ctx _fw_exclude_key_set force_enable_grad=True body_gm = materialize_as_graph body_fn args_single_step_bw ctx _fw_include_key_set ctx _fw_exclude_key_set force_enable_grad=True _ final_grad_carries final_grad_additional_inputs = split_into_chunks while_loop_op cond_gm body_gm pyrefly ignore bad-argument-type init_idx init_grad_carries init_grad_additional_inputs fw_carries ctx additional_inputs len init_grad_carries len init_grad_additional_inputs None None None None fill_none_with_masks final_grad_carries carries_tensor_masks fill_none_with_masks final_grad_additional_inputs additional_inputs_tensor_masks while_loop_stack_output_op = WhileLoopStackOutputOp while_loop_stack_output_op py_impl DispatchKey CompositeExplicitAutograd functools partial while_loop_dense stack_output=True while_loop_stack_output_op py_impl ProxyTorchDispatchMode functools partial while_loop_tracing stack_output=True while_loop_stack_output_op py_impl FakeTensorMode functools partial while_loop_fake_tensor_mode stack_output=True while_loop_stack_output_op py_functionalize_impl functools partial while_loop_func stack_output=True while_loop_stack_output_op py_autograd_impl autograd_not_implemented while_loop_stack_output_op deferred_error=True