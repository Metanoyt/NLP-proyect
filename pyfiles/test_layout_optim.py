Owner s module inductor copy os random torch torch nn torch _dynamo utils same torch _inductor config torch _inductor test_case run_tests TestCase torch testing _internal common_cuda tf _off torch testing _internal common_utils skipIfXpu torch testing _internal inductor_utils GPU_TYPE HAS_GPU USE_DDP_WRAPPER = os environ get USE_DDP_WRAPPER == Model Conv nn Module __init__ dim= manual_graph_break=False super __init__ conv = nn Conv d dim kernel_size= stride= bias=False conv = nn Conv d dim dim kernel_size= stride= bias=False manual_graph_break = manual_graph_break forward x x = conv x manual_graph_break torch _dynamo graph_break x = conv x x get_example_inputs torch rand skipIfXpu msg= ccl doesn t currently work XPU stack TestLayoutOptim TestCase classmethod setUpClass cls super setUpClass torch distributed dist use fixed port stress test tot_retry = retry_no range tot_retry try port = random randint GPU_TYPE == cuda backend = nccl GPU_TYPE == xpu backend = ccl dist init_process_group backend=backend init_method=f tcp localhost port world_size= rank= break except RuntimeError retry_no == tot_retry - raise continue verify_accuracy model_class use_ddp_wrapper=USE_DDP_WRAPPER is_train=False there potential ways introduce graph breaks manually using DDP we using DDP introduce graph breaks do manually wrap_mod m is_train f inp x = m inp x sum backward grads = _ param m named_parameters grad = param grad param grad None grad = torch zeros_like param grads append grad grads f m manual_graph_break = use_ddp_wrapper mod = model_class manual_graph_break=manual_graph_break GPU_TYPE inp = t GPU_TYPE t mod get_example_inputs expected_out = wrap_mod mod inp fp _mod = copy deepcopy mod torch float fp _inp = t torch float t copy deepcopy inp fp _out = wrap_mod fp _mod fp _inp use_ddp_wrapper torch nn parallel DistributedDataParallel DDP ddp_wrapped_mod = DDP mod opt_mod = torch compile wrap_mod ddp_wrapped_mod opt_mod = torch compile wrap_mod mod actual_out = opt_mod inp is_train assertTrue same expected_out actual_out fp _ref=fp _out expected_sum = expected_out sum actual_sum = actual_out sum print f Expected sum expected_sum actual sum actual_sum assertTrue same expected_out actual_out fp _ref=fp _out verify_accuracy_for_infer args kwargs verify_accuracy args kwargs is_train=False verify_accuracy_for_train args kwargs verify_accuracy args kwargs is_train=True test_ conv_with_graph_break Make sure graph break does cause any accuracy issue verify_accuracy_for_infer Model Conv test_ conv_with_graph_break Model nn Module __init__ dim= patch_size= kernel_size= manual_graph_break=False super __init__ seq = nn Sequential nn Conv d dim kernel_size=patch_size stride=patch_size bias=False nn Conv d dim dim kernel_size groups=dim padding= same bias=False conv = nn Conv d dim dim kernel_size= bias=False manual_graph_break = manual_graph_break forward x x = seq x manual_graph_break torch _dynamo graph_break x = conv x x get_example_inputs torch randn verify_accuracy_for_infer Model torch no_grad test_keep_output_layout_infer Model nn Module __init__ - None super __init__ conv = nn Conv d kernel_size= padding= stride= bias=False forward x x = conv x x get_example_inputs torch randn mod = Model GPU_TYPE inp = t GPU_TYPE t mod get_example_inputs out = mod inp opt_mod = torch compile mod opt_out = opt_mod inp We should able do view eager output out view - We should able do view output optimized module Note output channels last view op will fail opt_out view - test_keep_output_layout_with_freezing config patch freezing True test_keep_output_layout_infer test_training_acc verify_accuracy_for_train Model Conv test_mutate_view The GraphModule passed GraphLowering init method like https gist github com shunting fd e ff edc d It shows we will call copy_ update argument end This guarantees correctnesss torch compile f x y = x view y mul_ x = torch ones GPU_TYPE f x assertTrue torch equal x torch ones GPU_TYPE test_mutate_base The GraphModule passed GraphLowering init method like https gist github com shunting fd fe d f c db aba b bc It shows output graph mul node which contains update we applied base tensor torch compile f x y = x view x mul_ y x = torch ones GPU_TYPE y = f x assertTrue torch equal y torch ones GPU_TYPE tf _off test_mutate_base_for_conv_output Model nn Module __init__ manual_graph_break=False super __init__ conv = nn Conv d kernel_size= stride= bias=False forward x x = conv x y = x view - x mul_ y get_example_inputs torch rand verify_accuracy_for_infer Model tf _off test_mutate_view_for_conv_output Model nn Module __init__ manual_graph_break=False super __init__ conv = nn Conv d kernel_size= stride= bias=False forward x x = conv x y = x view - y mul_ x get_example_inputs torch rand verify_accuracy_for_infer Model test_dynamic_shape_specialization Previously aot_autograd py we compare strides FakeTensor real tensor That cause dynamic dimensions FakeTensor being specialized static shapes This test protects against f b x = sin y = b cos z = x + y z size = torch randn size requires_grad=True GPU_TYPE b = torch randn size GPU_TYPE actual = torch compile f dynamic=True b assertTrue torch allclose f b actual Trigger compiling backward graph actual sum backward test_nll_loss_backward Repro issue https github com pytorch pytorch issues The CUDA implementation aten nll_loss d_backward default requires tensor whose layout will used create grad_input contiguous Layout optimization may change tensor s layout cause failure We fix adding layout constraints fallback aten nll_loss d_backward default MyModel torch nn Module __init__ input_dim num_classes super __init__ conv = torch nn Conv d num_classes padding= same out = torch nn Linear input_dim num_classes num_classes forward x torch Tensor targets torch Tensor - torch Tensor x = conv x b c t f = x size x = out x reshape b t c f logits = x reshape x size x size x size loss = torch nn functional cross_entropy logits targets loss device = GPU_TYPE batch_size = seq_len = input_dim = num_classes = model = MyModel input_dim num_classes model device opt_model = torch compile model noqa F x = torch ones batch_size seq_len input_dim device=device targets = torch randint num_classes - batch_size seq_len device=device dtype=torch int loss = model x targets loss backward ref = model x targets assertTrue torch allclose ref loss __name__ == __main__ HAS_GPU run_tests