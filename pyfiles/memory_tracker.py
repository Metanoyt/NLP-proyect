mypy allow-untyped-defs operator pickle collections defaultdict collections abc Callable Sequence itertools chain typing Any no_type_check TYPE_CHECKING torch torch nn nn torch utils _python_dispatch TorchDispatchMode TYPE_CHECKING torch utils hooks RemovableHandle BYTES_PER_MB = MemoryProfileDispatchMode TorchDispatchMode Run ` ` TorchDispatchMode ` ` get memory stats operator level __init__ memory_tracker - None memory_tracker = memory_tracker __torch_dispatch__ func types args= kwargs=None rs = func args kwargs func torch ops aten detach default rs func_name str = memory_tracker _cur_module_name + + func __name__ + _ + str memory_tracker _operator_names func __name__ memory_tracker _operator_names func __name__ = memory_tracker _operator_names func __name__ + memory_tracker _record_memory_stats func_name rs MemoryTracker Collect plot memory stats operator level Includes ` ` memories_allocated ` ` ` ` memories_active ` ` ` ` memories_reserved ` ` It also prints summary top operators generate most memories Example usage xdoctest +SKIP failing net cuda input = input cuda mem_tracker = MemoryTracker mem_tracker start_monitor net net zero_grad True loss = net input isinstance loss dict loss = loss out loss sum backward net zero_grad set_to_none=True mem_tracker stop mem_tracker summary mem_tracker show_traces __init__ - None torch _C _log_api_usage_once torch distributed memory_tracker _hooks list RemovableHandle = _operator_names dict str int = defaultdict int memories_allocated dict int dict str float = defaultdict memories_active dict int dict str float = defaultdict memories_reserved dict int dict str float = defaultdict _markers dict str int = defaultdict int _cur_module_name str = _op_index int = _num_alloc_retries int = _device_module = torch get_device_module no_type_check start_monitor root_module nn Module - None Register module hooks entering ` ` MemoryProfileDispatchMode ` ` This enables operator level memory stats can tracked during module runtime _clear_state root_module __setattr__ _memory_tracker_is_root True name m root_module named_modules m root_module m __setattr__ _memory_tracker_is_root False fused_proxy_group does support hooks fused_proxy_grouped_embedding_bag name continue hook ordering other hooks added users managed so memory stats tracked here may completely accurate h = m register_forward_pre_hook _create_pre_forward_hook name h = m register_forward_hook _create_post_forward_hook name does work well jagged tensor somehow root cause clear remove now does really capture important info h = m register_backward_hook _create_backward_hook name _hooks extend h h _device_module empty_cache assert getattr profile_mode None None profile_mode = MemoryProfileDispatchMode profile_mode __enter__ no_type_check stop - None Remove module hooks exit ` ` MemoryProfileDispatchMode ` ` stop tracking memory stats operator level Get some aggregated stats when memory_tracker enabled like ` ` num_alloc_retries ` ` _num_alloc_retries = _device_module memory_stats get num_alloc_retries h _hooks h remove _hooks clear assert getattr profile_mode None None profile_mode __exit__ None None None profile_mode = None no_type_check summary top int = - None Print out top operators generate most memories The number top operators can configured op_diff dict str float = defaultdict float op_name previous_allocated_memory = memories_allocated i range _op_index op_name current_allocated_memory = memories_allocated i op_diff op_name = current_allocated_memory - previous_allocated_memory previous_allocated_memory = current_allocated_memory print ------------------------------------------------ print f The number alloc retries _num_alloc_retries print f Top top ops generates memory k v sorted op_diff items key=operator itemgetter reverse=True top print f k v MB print ------------------------------------------------ no_type_check show_traces path str = - None matplotlib pyplot plt _plot_figure x y_values labels min_val = min chain from_iterable y_values max_val = max chain from_iterable y_values plt figure y label zip y_values labels plt plot x y label=label plt xlabel Operator Calls plt ylabel Memory MB plt legend marker_name marker _markers items marker_name == fw_bw_boundary plt plot marker marker min_val max_val r lw= label=marker_name plt plot marker marker min_val max_val k- lw= label=marker_name path = load path y_ = gb name gb memories_allocated values y_ = gb name gb memories_active values y_ = gb name gb memories_reserved values x = list range len y_ Split figures when there big difference between reserved_memory allocated_memory active_memory _plot_figure x list y_ list y_ list y_ allocated_memory active_memory reserved_memory _plot_figure x list y_ allocated_memory _plot_figure x list y_ active_memory _plot_figure x list y_ reserved_memory save_stats path str - None Save stats using pickle during runtime users want plot traces other places like notebook stats = memories_allocated memories_allocated memories_active memories_active memories_reserved memories_reserved markers _markers num_alloc_retries _num_alloc_retries open path wb f pickle dump stats f pickle HIGHEST_PROTOCOL load path str - None Load pickled memory stats plot traces print summary open path rb f stats = pickle load f memories_allocated = stats memories_allocated memories_active = stats memories_active memories_reserved = stats memories_reserved _markers = stats markers _num_alloc_retries = stats num_alloc_retries _create_pre_forward_hook name str - Callable Prefix operator name current module forward insert fw_start marker forward pass start _pre_forward_hook module nn Module inputs Any - None _cur_module_name = f name forward pyrefly ignore invalid-argument hasattr module _memory_tracker_is_root pyrefly ignore not-callable module _memory_tracker_is_root _add_marker fw_start _pre_forward_hook _create_post_forward_hook name str - Callable Insert marker fw_bw_boundary boundary forward backward pass _post_forward_hook module nn Module inputs Sequence torch Tensor outputs Sequence torch Tensor - None pyrefly ignore invalid-argument hasattr module _memory_tracker_is_root pyrefly ignore not-callable module _memory_tracker_is_root _add_marker fw_bw_boundary _post_forward_hook _create_backward_hook name str - Callable Insert current module name backward prefix operator name _backward_hook module nn Module grad_input torch Tensor grad_output torch Tensor - None _cur_module_name = f name backward _backward_hook no_type_check _record_memory_stats fn_name str - None Record current memory allocated current memory active current memory reserved The memory stats dict indexed ` ` _op_index ` ` memory_allocated float = _device_module memory_allocated BYTES_PER_MB memory_reserved float = _device_module memory_reserved BYTES_PER_MB memory_active float = _device_module memory_stats get active_bytes all current BYTES_PER_MB memories_allocated _op_index = fn_name memory_allocated memories_reserved _op_index = fn_name memory_reserved memories_active _op_index = fn_name memory_active _op_index += _add_marker marker_name str - None Set marker s x-axis value marker_val = len memories_allocated values _markers marker_name = marker_val _clear_state - None Clear states when start_monitor called _operator_names clear memories_allocated clear memories_active clear memories_reserved clear _markers clear _cur_module_name = _op_index = _num_alloc_retries =