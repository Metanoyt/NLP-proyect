mypy allow-untyped-defs __future__ annotations contextlib dataclasses functools itertools logging math operator os textwrap warnings collections defaultdict collections abc Iterable Sequence typing Any Callable cast Optional TYPE_CHECKING TypeVar Union typing_extensions ParamSpec unittest mock patch sympy torch torch ao quantization fx _decomposed torch fx torch utils _pytree pytree torch _dynamo utils counters torch _higher_order_ops associative_scan associative_scan_op torch _higher_order_ops triton_kernel_wrap triton_kernel_wrapper_mutation torch _library utils get_layout_constraint_tag torch _prims_common pyrefly ignore deprecated canonicalize_dim canonicalize_dims check dtype_to_type elementwise_dtypes ELEMENTWISE_TYPE_PROMOTION_KIND get_computation_dtype is_boolean_dtype is_float_dtype is_integer_dtype Number torch fx experimental sym_node magic_methods method_to_operator torch fx experimental symbolic_shapes free_unbacked_symbols has_free_unbacked_symbols resolve_unbacked_bindings torch utils _ordered_set OrderedSet torch utils _sympy functions CeilDiv FloorDiv Identity Mod ModularIndexing _dynamo utils import_submodule config inductor_prims ir test_operators NOQA F decomposition decompositions get_decompositions ir BaseView DtypeView ExpandView IndexingConstant IRNode is_triton MutableBox OnlineSoftmaxReduction ops_wrapper PermuteView Pointwise Reduction ShapeAsConstantBuffer SqueezeView TensorBox validate_ir View utils ceildiv decode_device is_dynamic is_gpu is_pointwise_use is_view needs_fallback_due_to_atomic_add_limitations pad_listlike register_op_dtype_propagation_rules register_op_requires_libdevice_fp sympy_product use_scatter_fallback virtualized ops V TYPE_CHECKING ops_handler ReductionType _T = TypeVar _T _P = ParamSpec _P TODO jansel we should implement decomps lowerings these https github com pytorch torchdynamo issues FALLBACK_ALLOW_LIST = OrderedSet torchvision roi_align aten index_add log = logging getLogger __name__ lowerings dict Union Callable Any str Callable Any = Use maybe_layout_constraints access dict we lazily register tag-based layout constraints _maybe_layout_constraints dict torch _ops OpOverload Optional Callable Any = fallbacks = OrderedSet torch _ops OpOverload aten = torch ops aten tr_c d = torch ops tr_c d prims = torch ops prims needs_realized_inputs = OrderedSet torch _ops OpOverload foreach_ops = OrderedSet torch _ops OpOverload torch _higher_order_ops _foreach_map type ignore list-item TODO rec torch _higher_order_ops _foreach_map OpOverload so why foreach_ops inplace_foreach_ops = OrderedSet torch _ops OpOverload inplaceable_foreach_ops dict torch _ops OpOverload torch _ops OpOverload = quantized_decomposed = torch ops quantized_decomposed cur_node_has_non_foreach_users node V graph current_node users user node users user op == call_function user target foreach_ops True False group device whether any inputs dynamic note arg_pairs may may pair foreach_map example just passes output buffers here group_foreach_args arg_pairs Iterable Union tuple Any Any Any out = defaultdict list unpack_args = False i args enumerate arg_pairs isinstance args Iterable unpack_args = True args = args use_foreach = is_dynamic args config combo_kernel_foreach_dynamic_shapes device = None t args isinstance t TensorBox device = t data get_device break assert device None foreach op should have least one tensor arg unpack_args pyrefly ignore bad-unpacking args = args out device use_foreach append i args out maybe_layout_constraints fn Callable Any - Optional Callable Any Get layout constraints Returns None there no layout constraints isinstance fn torch _ops OpOverload Only OpOverloads have layout constraints None maybe_layout_tag = get_layout_constraint_tag fn with_default=False tag_to_layout_constraint maybe_layout_tag fn _maybe_layout_constraints _maybe_layout_constraints fn None tag_to_layout_constraint tag tag == torch _C Tag needs_exact_strides constrain_to_fake_tensors tag == torch _C Tag needs_contiguous_strides type ignore attr-defined require_contiguous_strides tag == torch _C Tag needs_fixed_stride_order constrain_to_fx_strides tag == torch _C Tag flexible_layout None raise AssertionError f Unknown layout constraint tag tag assert_nyi cond msg cond raise NotImplementedError f inductor does support msg add_needs_realized_inputs fn isinstance fn list set tuple OrderedSet noqa set_linter add_needs_realized_inputs x x fn needs_realized_inputs add fn isinstance fn torch _ops OpOverloadPacket needs_realized_inputs update getattr fn overload overload fn overloads add_layout_constraint fn constraint isinstance fn torch _ops OpOverloadPacket overload fn overloads _maybe_layout_constraints getattr fn overload = constraint _maybe_layout_constraints fn = constraint add_needs_realized_inputs aten as_strided aten as_strided_copy aten avg_pool d aten avg_pool d_backward aten bmm aten convolution aten convolution_backward aten max_pool d_with_indices aten max_pool d_with_indices aten max_pool d_with_indices_backward aten mm aten upsample_nearest d aten _upsample_nearest_exact d aten _int_mm TODO jansel ezyang says we won t need future try removing based https github com pytorch pytorch blob e eb df f c core ScalarType h#L DTYPE_ID_LOOKUP = torch uint torch int torch int torch int torch int torch float torch float torch float torch complex torch complex torch complex torch bool torch bfloat TODO jansel add quantized types _ c qint QInt _ c quint QUInt _ c qint QInt _ c quint x QUInt x _ c quint x QUInt x decode_dtype dtype int isinstance dtype int dtype assert dtype DTYPE_ID_LOOKUP f id dtype missing DTYPE_ID_LOOKUP pyrefly ignore bad-assignment dtype = DTYPE_ID_LOOKUP dtype dtype is_integer_type x isinstance x TensorBox is_integer_dtype x get_dtype is_boolean_dtype x get_dtype isinstance x sympy Expr x is_integer True type ignore attr-defined isinstance x int is_boolean_type x isinstance x TensorBox is_boolean_dtype x get_dtype isinstance x bool get_promoted_dtype args type_promotion_kind ELEMENTWISE_TYPE_PROMOTION_KIND construct_input inp isinstance inp Number sympy Basic inp dim = len inp get_size construct tmp tensor feed into torch result_type torch zeros dim dtype=inp get_dtype inps = construct_input arg arg args _ dtype = elementwise_dtypes inps type_promotion_kind=type_promotion_kind dtype get_overloads aten_fn isinstance aten_fn list tuple aten_fn = aten_fn aten_fn = list aten_fn fn list aten_fn isinstance fn torch _ops OpOverloadPacket overload fn overloads other_fn = getattr fn overload other_fn lowerings aten_fn append other_fn aten_fn in_namespace op namespace isinstance op torch _ops OpOverloadPacket namespace op _qualified_op_name isinstance op torch _ops OpOverload namespace op name False maybe_copy_cpu_scalar x TensorBox device torch device - TensorBox Copy cpu scalar doesn t match given ` device ` isinstance x data ir ReinterpretView has_free_unbacked_symbols x get_size x size = V graph sizevars size_hint_or_throw s s x get_size cur_device = x get_device cur_device None cur_device type == cpu cur_device = device len size == len size == size == TensorBox ir StorageBox ir DeviceCopy create x cur_device False x transform_args args list Any kwargs dict str Any broadcast bool type_promotion_kind Optional ELEMENTWISE_TYPE_PROMOTION_KIND convert_input_to_bool bool - tuple list Any dict str Any Transforms arguments broadcasting type promotion args_indices = i i x enumerate args isinstance x TensorBox kwargs_indices = k k v kwargs items isinstance v TensorBox check there s something transform args_indices kwargs_indices args kwargs type_promotion_kind convert_input_to_bool convert_input_to_bool dtype = torch bool FIXME crude approximation promoting args promoting_args = args isinstance Number sympy Basic hasattr dtype only consider tensor kwargs promotion now promoting_args extend kwargs values hasattr dtype dtype = get_promoted_dtype promoting_args type_promotion_kind=type_promotion_kind type ignore arg-type device = args args_indices args_indices kwargs kwargs_indices get_device i args_indices args i = maybe_copy_cpu_scalar args i device k kwargs_indices kwargs k = maybe_copy_cpu_scalar kwargs k device sometimes args immutable list so we can t mutate them promote arg isinstance arg TensorBox to_dtype arg dtype isinstance arg ir Constant ir Constant value=arg value dtype=dtype device=device arg args = promote args kwargs = k promote v k v kwargs items broadcast broadcasted = broadcast_tensors list itertools chain args i i args_indices kwargs k k kwargs_indices size = list broadcasted get_size i x zip args_indices broadcasted len args_indices args i = x k x zip kwargs_indices broadcasted len args_indices kwargs k = x i range len args isinstance args i ir Constant args i = ExpandView create args i size k kwargs isinstance kwargs k ir Constant kwargs k = ExpandView create kwargs k size args kwargs _register_foreach_lowering aten_fn decomp_fn Add foreach lowering lowerings dict Arguments aten_fn torch ops aten fn we lowering decomp_fn alternate implementation our IR broadcast True apply broadcasting tensor inputs type_promotion_kind kind type promotion applied tensor inputs ` None ` means no type promotion convert_input_to_bool some logical ops require inputs converted bool functools wraps decomp_fn wrapped args kwargs assert len args = out = decomp_fn args kwargs validate_ir out out aten_fns = get_overloads aten_fn foreach_ops update aten_fns lowerings update dict fromkeys aten_fns wrapped wrapped _register_lowering aten_fn decomp_fn broadcast type_promotion_kind Optional ELEMENTWISE_TYPE_PROMOTION_KIND convert_input_to_bool lowering_dict Add lowering lowerings dict Arguments aten_fn torch ops aten fn we lowering decomp_fn alternate implementation our IR broadcast True apply broadcasting tensor inputs type_promotion_kind kind type promotion applied tensor inputs ` None ` means no type promotion convert_input_to_bool some logical ops require inputs converted bool functools wraps decomp_fn wrapped args kwargs args list Any = list args kwargs dict str Any = dict kwargs unpacked = False TODO maybe we need use pytrees here len args == isinstance args list tuple unpacked = True args = list args all fn fallbacks in_namespace fn _c d_functional fn aten_fn explicitly assert out= ops better error messages assert any x == out x kwargs keys out= ops aren t yet supported args kwargs = transform_args args kwargs broadcast type_promotion_kind convert_input_to_bool unpacked args = args out = decomp_fn args kwargs validate_ir out out aten_fn = get_overloads aten_fn lowering_dict update dict fromkeys aten_fn wrapped wrapped register_lowering aten_fn broadcast=False type_promotion_kind Optional ELEMENTWISE_TYPE_PROMOTION_KIND = ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT convert_input_to_bool=False lowering_dict=lowerings - Callable Callable _P _T Callable _P _T Shim support decorator syntax functools partial _register_lowering aten_fn broadcast=broadcast type_promotion_kind=type_promotion_kind convert_input_to_bool=convert_input_to_bool lowering_dict=lowering_dict broadcast_symbolic_shapes b Broadcasting logic based symbolic shapes We give shapes concrete values while all other shapes symbolic sympy formulas output = x y itertools zip_longest reversed reversed b fillvalue=sympy S One V graph sizevars is_size_one_or_false y output append x V graph sizevars is_size_one_or_false x output append y V graph sizevars check_equals x y len sympy expand y free_symbols len sympy expand x free_symbols output append y prefer shorter formula output append x tuple reversed output promote_constants inputs override_return_dtype=None type_promotion_kind=None assert override_return_dtype None type_promotion_kind None only one override_return_dtype type_promotion_kind may given override_return_dtype None type_promotion_kind None type_promotion_kind = ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT any isinstance x sympy Basic int float x inputs inputs all isinstance x int float sympy Basic x inputs dtype = override_return_dtype get_promoted_dtype inputs pyrefly ignore bad-argument-type type_promotion_kind=type_promotion_kind const_func x isinstance x sympy Basic ir IndexingConstant index=x dtype=dtype device=decode_device None ir Constant value=x dtype=dtype device=decode_device None const_func x x inputs ex = next x x inputs isinstance x TensorBox ExpandView ir Constant out = x inputs isinstance x int float out append ExpandView create ir Constant value=x dtype=ex get_dtype device=ex get_device_or_error list ex get_size isinstance x sympy Basic out append ExpandView create IndexingConstant index=x dtype=ex get_dtype device=ex get_device_or_error list ex get_size out append x out make_pointwise fn override_return_dtype=None override_device=None override_fn_when_input_bool=None allow_alpha=False triton_fallback=None inner inputs TensorBox alpha=None triton_fallback None any isinstance inp IRNode is_triton inp inp inputs assert allow_alpha implemented triton_fallback inputs inputs = promote_constants inputs override_return_dtype allow_alpha alpha None alpha = pyrefly ignore bad-assignment inputs = list inputs pyrefly ignore unsupported-operation inputs - = mul inputs - alpha assert alpha None loaders = x make_loader x inputs ranges = inputs get_size dtype = override_return_dtype inputs get_dtype other inputs assert isinstance other ir BaseConstant len ranges == len other get_size f ndim mismatch fn ranges other get_size tracing we will annotate pointwise nodes correspond output pointwise node would have been run eager intermediary pointwise nodes during decompositions annotated low_pr_fp = torch bfloat torch float emulate_precision_casts = V graph None getattr V graph current_node None None V graph current_node meta None V graph current_node meta get low_precision_pointwise_barrier False emulate_output_cast = emulate_precision_casts dtype low_pr_fp inner_fn index assert len index == len ranges f wrong ndim index ranges dtype == torch bool override_fn_when_input_bool None override_fn_when_input_bool load index load loaders inputs_loaded = inp_index load enumerate loaders out = load index inp_dtype = inputs inp_index get_dtype emulate_precision_casts inp_dtype low_pr_fp downcast = ops to_dtype out inp_dtype use_compute_types=False out = ops to_dtype downcast inp_dtype inputs_loaded append out out = fn inputs_loaded emulate_output_cast fp bf kernels computed fp Casting down fp bf here then upcasting again emulate casts eager would do downcast = ops to_dtype out dtype use_compute_types=False ops to_dtype downcast dtype out override_device device = None i inputs pyrefly ignore missing-attribute is_gpu i get_device type device = i get_device break device device = inputs get_device pyrefly ignore unbound-name device = override_device device Pointwise create device=device type ignore arg-type dtype=dtype inner_fn=inner_fn ranges=ranges inner make_foreach_pointwise pw_fn allow_alpha=False inner inputs list list TensorBox alpha= realize_outputs = len V graph current_node users == V graph current_node target inplace_foreach_ops cur_node_has_non_foreach_users a_list_input = None input inputs isinstance input list tuple a_list_input = input break assert a_list_input None least one input must list foreach op broadcast scalar inputs match length list inputs broadcast_inputs = input inputs isinstance input list tuple broadcast_inputs append input len a_list_input broadcast_inputs append input groups = group_foreach_args zip broadcast_inputs outputs = None len a_list_input device use_foreach group groups items operation_list list str = output_ind args group allow_alpha output = pw_fn args alpha=alpha output = pw_fn args outputs output_ind = output pyrefly ignore unbound-name V graph has_feature device BackendFeature FOREACH use_foreach realize_outputs output realize operation_list append output get_operation_name operation_list pyrefly ignore unbound-name V graph register_operation_list operation_list assert all x None x outputs outputs inner to_dtype x Union TensorBox ShapeAsConstantBuffer dtype torch dtype copy bool = False src_dtype = x get_dtype src_dtype == dtype clone x copy x _to_dtype x ops to_dtype x dtype src_dtype=src_dtype make_pointwise _to_dtype override_return_dtype=dtype x register_lowering torch _higher_order_ops _foreach_map type_promotion_kind=None _foreach_map subgraph args kwargs This lowers invocation foreach_map The way works arbitrary N-arg func provided user looped over polyfill same semantics foreach op loop applying n-ary function n args then traced into subgraph dynamo This code allows us inline subgraph into main graph lowering using PontwiseSubgraphLowering The graph outputs represent vertically fused sequence ops then register_operation_list below registers buffers horizontally fuseable scheduler subgraph_lowering PointwiseSubgraphLowering inputs = args gm = subgraph graph_module pw_subgraph = PointwiseSubgraphLowering gm root_graph_lowering=V graph V set_graph_handler pw_subgraph type ignore arg-type pw_subgraph run inputs sub_outputs = pw_subgraph graph_outputs group outputs device register foreach assert sub_outputs mypy lol groups = group_foreach_args sub_outputs outputs = None len sub_outputs device use_foreach group groups items operation_list list str = output_ind output group outputs output_ind = output V graph has_feature device BackendFeature FOREACH use_foreach output realize operation_list append output get_operation_name operation_list V graph register_operation_list operation_list assert all x None x outputs outputs register_lowering prims convert_element_type type_promotion_kind=None _convert_element_type x TensorBox dtype torch dtype dtype is_complex x get_dtype is_complex x get_size Decompose since aa aten fallback more friendly c++ codegen This decomposition doesn t work empty tensor which needs more investigation dst = empty_like x dtype=dtype ir InplaceCopyFallback create dst x dst fallback_handler prims convert_element_type default add_to_fallback_set=False x dtype to_dtype x dtype copy=True to_dtype_bitcast x TensorBox dtype torch dtype copy=False x_dtype = x get_dtype x_dtype == dtype clone x copy x _get_primitive_bitwidth dtype dtype is_floating_point torch finfo dtype bits torch iinfo dtype bits src_bits = _get_primitive_bitwidth x_dtype dst_bits = _get_primitive_bitwidth dtype src_bits = dst_bits fallback aten eager implementation differing bitwidths fallback_handler aten view dtype x dtype TensorBox DtypeView create x dtype register_lowering aten view dtype type_promotion_kind=None _view_dtype x TensorBox dtype torch dtype dtype is_complex x get_dtype is_complex TensorBox create ir ComplexView create torch ops aten view dtype x dtype to_dtype_bitcast x dtype to_device x TensorBox device torch device copy=False non_blocking=False device = decode_device device x get_device == device clone x copy x TensorBox create ir DeviceCopy create x device non_blocking register_lowering prims device_put type_promotion_kind=None _device_put x TensorBox device torch device non_blocking=False to_device x device copy=True non_blocking=non_blocking register_pointwise aten_fn name=None broadcast=True type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT convert_input_to_bool=False override_return_dtype=None override_fn_when_input_bool=None allow_alpha=False triton_fallback=None A pointwise function maps ops name inputs name = name aten_fn __name__ fn = ops_wrapper name register_op_dtype_propagation_rules name type_promotion_kind override_return_dtype override_fn_when_input_bool None override_fn_when_input_bool = ops_wrapper override_fn_when_input_bool fn = make_pointwise fn override_return_dtype=override_return_dtype override_fn_when_input_bool=override_fn_when_input_bool allow_alpha=allow_alpha triton_fallback=triton_fallback fn = register_lowering aten_fn broadcast=broadcast type_promotion_kind=type_promotion_kind convert_input_to_bool=convert_input_to_bool fn hasattr prims name register_lowering getattr prims name type_promotion_kind=None convert_input_to_bool=convert_input_to_bool fn fn register_frexp A pointwise function maps ops frexp inputs name = frexp frexp = ops_wrapper frexp frexp args kwargs frexp args kwargs type ignore index frexp args kwargs frexp args kwargs type ignore index pw_fns = make_pointwise frexp make_pointwise frexp override_return_dtype=torch int fn args kwargs pw_fns args kwargs pw_fns args kwargs fn = register_lowering aten frexp fn hasattr prims name register_lowering getattr prims name type_promotion_kind=None fn fn register_frexp register_foreach_pointwise aten_fn pointwise_lowering_fn allow_alpha=False fn = make_foreach_pointwise pointwise_lowering_fn allow_alpha=allow_alpha fn = _register_foreach_lowering aten_fn fn fn register_lowering aten where broadcast=False type_promotion_kind=None where cond b fn args ops where args isinstance float int = constant_like b isinstance b float int b = constant_like b args = cond b dtype = get_promoted_dtype args args type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT indices = i i x enumerate args isinstance x TensorBox i x zip indices broadcast_tensors args i i indices args i = x i range len args isinstance args i ir Constant args i = ExpandView create args i list args indices get_size make_pointwise fn override_return_dtype=dtype args to_dtype args dtype to_dtype args dtype register_lowering aten broadcast_tensors broadcast=False type_promotion_kind=None broadcast_tensors inputs len inputs == isinstance inputs list tuple broadcast_tensors inputs target list sympy Expr = functools reduce broadcast_symbolic_shapes x get_size x inputs outputs = x inputs sizes = x get_size len sizes = len target any V graph sizevars is_size_one_or_false = V graph sizevars is_size_one_or_false b b zip sizes target x = expand x target outputs append x outputs register_lowering aten alias aten detach aten detach_ aten lift prims view_of nop x x AOT autograd handles us hasattr aten lift_fresh register_lowering aten lift_fresh nop register_lowering aten squeeze type_promotion_kind=None squeeze x dim=None assert isinstance x TensorBox dim None TensorBox SqueezeView create x data dim = V graph sizevars guard_int dim isinstance dim int sympy Expr tuple V graph sizevars guard_int d d dim dim = canonicalize_dims len x get_size dim type ignore call-overload dims = OrderedSet dim isinstance dim tuple dim new_shape = d s enumerate x get_size d dims V graph sizevars guard_or_false sympy Eq s new_shape append s squeeze does nothing size isn t view x new_shape new_shape = x get_size x register_lowering aten squeeze_copy type_promotion_kind=None squeeze_copy x dim=None clone squeeze x dim register_lowering aten squeeze_ squeeze_ x dim=None val = squeeze x dim assert isinstance x TensorBox assert isinstance val TensorBox x data = val data x register_lowering aten isinf isinf x is_integer_type x full_like x False dtype=torch bool fn = ops_wrapper isinf make_pointwise fn override_return_dtype=torch bool x register_lowering aten isnan isnan x is_integer_type x full_like x False dtype=torch bool fn = ops_wrapper isnan make_pointwise fn override_return_dtype=torch bool x register_lowering aten ceil ceil x is_integer_type x clone x fn = ops_wrapper ceil make_pointwise fn x register_lowering aten floor floor x is_integer_type x clone x fn = ops_wrapper floor make_pointwise fn x register_lowering aten round default round x is_integer_type x clone x fn = ops_wrapper round make_pointwise fn x register_lowering aten trunc trunc x is_integer_type x clone x fn = ops_wrapper trunc make_pointwise fn x register_lowering aten expand type_promotion_kind=None expand x sizes x = promote_constants x isinstance x ir BaseConstant ExpandView create x tuple sizes assert isinstance x TensorBox assert isinstance sizes list tuple tuple x get_size == tuple sizes x free_unbacked_symbols x get_size x_size_product = V graph sizevars size_hint_or_throw sympy_product x get_size TODO It would better realize input any its sizes unbacked because typically size will non-zero However cannot done directly below we ll choke size_hint here x_size_product free_unbacked_symbols sizes maybe realize input before broadcasting x mark_reuse V graph sizevars size_hint_or_throw sympy_product sizes x_size_product TensorBox ExpandView create x data tuple sizes register_lowering prims broadcast_in_dim type_promotion_kind=None broadcast_in_dim shape broadcast_dimensions s = list shape broadcast_dimension broadcast_dimensions s broadcast_dimension = - v = idx x enumerate s x = - v = unsqueeze v idx expand v shape register_lowering aten expand_as type_promotion_kind=None expand_as x y expand x y get_size register_lowering aten repeat repeat x repeats old_size = list x get_size len repeats len old_size old_size = sympy S One len repeats - len old_size + old_size x = view x list old_size assert len repeats == len x get_size new_size = list x get_size zero_tensor = False i range len repeats repeats i == zero_tensor = True new_size i = new_size i repeats i zero_tensor empty new_size dtype=x get_dtype device=x get_device all == b == b zip repeats old_size clone expand x new_size x_loader Callable Any Any inner_fn index assert len index == len repeats index = list index i range len repeats repeats i = old_size i == index i = sympy S Zero index i = ModularIndexing index i old_size i x_loader index free_unbacked_symbols old_size free_unbacked_symbols new_size old_size_product = V graph sizevars size_hint_or_throw sympy_product old_size old_size_product maybe realize input skip unbacked symints since ll choke size hint x mark_reuse V graph sizevars size_hint_or_throw sympy_product new_size old_size_product x_loader = x make_loader Pointwise create device=x get_device dtype=x get_dtype inner_fn=inner_fn ranges=list new_size register_lowering aten _unsafe_view type_promotion_kind=None register_lowering aten view type_promotion_kind=None register_lowering aten reshape type_promotion_kind=None view x TensorBox sizes Sequence sympy Expr - TensorBox TensorBox View create x data sizes register_lowering aten permute type_promotion_kind=None permute x dims assert isinstance x TensorBox assert isinstance dims list tuple TensorBox PermuteView create x data tuple dims register_lowering aten slice type_promotion_kind=None slice_ x dim= start= end= step= clamp=True Lowers slice call creating ExternKernels output size storage offset symbols indices unbacked appropriate semantics aren t known If they known indices static backed unbacked info SliceView created torch fx experimental symbolic_shapes CallMethodKey resolve_unbacked_bindings assert isinstance x TensorBox dim = _validate_dim x dim size = x get_size dim step = sympy expand step assert isinstance step sympy Expr step step maybe apply slice optimization try start == V graph sizevars statically_known_leq size end step == x except TypeError pass try avoid dynamic unbacked slice compute_slice_index index size default=None index None default fn = lambda x V graph sizevars guard_or_false x noqa E index = sympy expand index size = sympy expand size fn sympy Ge index fn sympy Le index size index fn sympy Lt index fn sympy Ge index -size index + size fn sympy Gt index size size fn sympy Lt index -size None start_index end_index = None None ambiguous_slice = clamp ambiguous_slice start_index = compute_slice_index start size end_index = compute_slice_index end size size start_index None end_index None start end = start_index end_index ambiguous_slice = False ambiguous_slice=False means we know what semantics slice call follows don t need generate extern kernel represent output size This assumed True clamp=False meant follow standard indexing semantics = index size ambiguous_slice TensorBox ir SliceView create x data dim start end step clamp=clamp go SliceView ReinterpretView unbacked territory create DynamicSlice ExternKernel clamp True unbacked start end assert clamp unbacked_bindings = resolve_unbacked_bindings V graph sizevars shape_env V graph current_node meta unbacked_bindings assert unbacked_bindings None assert len unbacked_bindings = unbacked_bindings sym_size sym_storage = None None sym keypath unbacked_bindings items keypath == CallMethodKey size pytree SequenceKey dim sym_size = sym keypath == CallMethodKey storage_offset sym_storage = sym assert start_index None end_index None b_size = ir DynamicSliceSize sym_size start end step x get_size dim b_size name = V graph register_buffer b_size V graph register_operation b_size new_size = sym_size start_index None x realize we shouldn t have allocated storage offset symbol start index determinable assert sym_storage None new_storage_offset = x get_layout offset + start_index x get_stride dim b_storage = ir DynamicSelectStorageOffset sym_storage start x get_layout offset x get_stride dim x get_size dim clamp=True b_storage name = V graph register_buffer b_storage V graph register_operation b_storage new_storage_offset = sym_storage new_sizes = list x get_size new_strides = list x get_stride new_sizes dim = new_size new_strides dim = step as_strided x new_sizes new_strides new_storage_offset register_lowering aten as_strided type_promotion_kind=None as_strided x size stride storage_offset=None new_device = None new_dtype = None isinstance x TensorBox isinstance x data ir BaseView Note Merging views When we use as_strided we can rewrite size stride offset incoming buffer x If x view we would overwrite its metadata Except dtype which we need propagate Technically device needed because possible have cross-device view today new_device = x get_device new_dtype = x dtype x = x data unwrap_view x realize ir is_storage_and_layout x raise NotImplementedError f unrealized as_strided x storage old_layout = ir as_storage_and_layout x new_layout = ir FixedLayout new_device new_device old_layout device new_dtype new_dtype old_layout dtype sympy expand s s size sympy expand s s stride sympy expand storage_offset TensorBox ir ReinterpretView data=storage layout=new_layout register_lowering aten as_strided_ type_promotion_kind=None as_strided_ x size stride storage_offset=None assert isinstance x TensorBox x data = as_strided x size stride storage_offset data x register_lowering aten as_strided_copy type_promotion_kind=None as_strided_copy x size stride storage_offset=None result = as_strided x size stride storage_offset clone result pointwise_cat inputs dim= inclusive exclusive inputs_ranges list tuple sympy Expr sympy Expr = prev_end = inp inputs inputs_ranges append prev_end prev_end + inp get_size dim type ignore arg-type prev_end = inputs_ranges - - type ignore assignment inputs_loaders = inp make_loader inp inputs inner_fn idx idx_dim = ops index_expr idx dim torch int masks = masked_loads = i range len inputs start = ops constant torch int i == ops index_expr inputs_ranges i torch int end = ops index_expr inputs_ranges i torch int start_cond = ops ge idx_dim start end_cond = ops lt idx_dim end i == mask = end_cond i == len inputs - mask = start_cond mask = ops and_ start_cond end_cond masks append mask idx_load = list idx we re concatting when we index second tensor we want index - Use Identity prevent expansion index stride keep expression same int bitwidth shape idx_load dim = Identity idx_load dim - inputs_ranges i masked_loads append ops masked mask lambda inputs_loaders i idx_load value should unused next_val = masked_loads - i range len inputs - - - next_val = ops where masks i masked_loads i next_val next_val new_size = list inputs get_size new_size dim = inputs_ranges - - Pointwise create device=inputs get_device dtype=inputs get_dtype inner_fn=inner_fn ranges=new_size register_lowering quantized_decomposed quantize_per_channel type_promotion_kind=None quantized_decomposed_quantize_per_channel input TensorBox scales TensorBox zero_points TensorBox axis int quant_min int quant_max int dtype torch dtype - Union TensorBox ShapeAsConstantBuffer assert len scales get_size == expect scales dim assert len zero_points get_size == expect zero_points dim input get_dtype == torch bfloat input = to_dtype input torch float assert input get_dtype == torch float f Expecting input have dtype torch float got dtype input get_dtype assert axis len input get_size f Expecting axis len input get_size input_loader = input make_loader scales_loader = scales make_loader zero_points_loader = zero_points make_loader inner_fn idx channel_idx = idx axis input = input_loader idx scale = scales_loader channel_idx zero_point = zero_points_loader channel_idx qmin qmax = _create_constants quant_min quant_max dtype=torch float scales dtype = torch float scale = ops to_dtype scale torch float zero_points dtype = torch int zero_point = ops to_dtype zero_point torch int inv_scale = ops reciprocal scale val = ops round input inv_scale + zero_point clamped = ops maximum qmin ops minimum qmax val ops to_dtype clamped dtype Pointwise create device=input get_device dtype=dtype inner_fn=inner_fn ranges=input get_size _assert_async cond msg cond realize cond = to_dtype cond torch bool inner_fn index ir ComputedBuffer force_realize ops device_assert_async cond make_loader index msg assertion_op = Pointwise create device=cond get_device dtype=cond get_dtype inner_fn=inner_fn ranges=list cond get_size assertion_op realize assertion_op register_lowering aten _assert_async msg lower_assert_async cond msg _assert_async cond msg register_lowering aten _functional_assert_async msg lower_assert_functional_async cond msg _assert_async cond msg register_lowering quantized_decomposed dequantize_per_channel type_promotion_kind=None quantized_decomposed_dequantize_per_channel input TensorBox scales TensorBox zero_points TensorBox axis int quant_min int quant_max int dtype torch dtype out_dtype Optional torch dtype = None - Union TensorBox ShapeAsConstantBuffer assert len scales get_size == expect scales dim assert len zero_points get_size == expect zero_points dim assert input get_dtype == dtype f Expecting input have dtype dtype got dtype input get_dtype assert axis len input get_size f Expecting axis len input get_size out_dtype None out_dtype = torch float input_loader = input make_loader scales_loader = scales make_loader zero_points_loader = zero_points make_loader inner_fn idx channel_idx = idx axis input = input_loader idx scale = scales_loader channel_idx zero_point = zero_points_loader channel_idx scales dtype = torch float scale = ops to_dtype scale torch float zero_points dtype = torch float zero_point = ops to_dtype zero_point torch float val = ops sub ops to_dtype input torch float zero_point scale val = ops to_dtype val out_dtype val Pointwise create device=input get_device dtype=out_dtype inner_fn=inner_fn ranges=input get_size register_lowering quantized_decomposed quantize_per_tensor default type_promotion_kind=None quantized_decomposed_quantize_per_tensor_default input TensorBox scale float zero_point int quant_min int quant_max int dtype torch dtype - Union TensorBox ShapeAsConstantBuffer input get_dtype == torch bfloat input = to_dtype input torch float assert input get_dtype == torch float f Expecting input have dtype torch float got dtype input get_dtype input_loader = input make_loader inner_fn idx scale zero_point input = input_loader idx inv_scale zero_point = _create_constants scale zero_point dtype=torch float val = ops round input inv_scale + zero_point qmin qmax = _create_constants quant_min quant_max dtype=torch float clamped = ops minimum ops maximum val qmin qmax ops to_dtype clamped dtype Pointwise create device=input get_device dtype=dtype inner_fn=functools partial inner_fn scale=float scale zero_point=int zero_point ranges=input get_size register_lowering quantized_decomposed dequantize_per_tensor default type_promotion_kind=None quantized_decomposed_dequantize_per_tensor_default input TensorBox scale float zero_point int quant_min int quant_max int dtype torch dtype out_dtype Optional torch dtype = None - Union TensorBox ShapeAsConstantBuffer assert input get_dtype == dtype f Expecting input have dtype dtype got dtype input get_dtype out_dtype None out_dtype = torch float input_loader = input make_loader inner_fn idx scale zero_point input = input_loader idx scale zero_point = _create_constants scale zero_point dtype=torch float val = ops sub ops to_dtype input torch float zero_point scale val = ops to_dtype val out_dtype val Pointwise create device=input get_device dtype=out_dtype inner_fn=functools partial inner_fn scale=float scale zero_point=int zero_point ranges=input get_size register_lowering quantized_decomposed quantize_per_tensor tensor type_promotion_kind=None quantized_decomposed_quantize_per_tensor_tensor input TensorBox scale TensorBox zero_point TensorBox quant_min int quant_max int dtype torch dtype - Union TensorBox ShapeAsConstantBuffer input get_dtype == torch bfloat input = to_dtype input torch float assert input get_dtype == torch float f Expecting input have dtype torch float got dtype input get_dtype assert len scale get_size == len scale get_size == scale get_size == expect scale scalar tensor assert len zero_point get_size == len zero_point get_size == zero_point get_size == expect zero_point scalar tensor input_loader = input make_loader scale_loader = scale make_loader zero_point_loader = zero_point make_loader inner_fn idx input = input_loader idx _scale = scale_loader len scale get_size == _zero_point = zero_point_loader len scale get_size == scale dtype = torch float _scale = ops to_dtype _scale torch float zero_point dtype = torch float _zero_point = ops to_dtype _zero_point torch float val = ops round input ops reciprocal _scale + _zero_point qmin qmax = _create_constants quant_min quant_max dtype=torch float clamped = ops minimum ops maximum val qmin qmax ops to_dtype clamped dtype Pointwise create device=input get_device dtype=dtype inner_fn=inner_fn ranges=input get_size register_lowering quantized_decomposed dequantize_per_tensor tensor type_promotion_kind=None quantized_decomposed_dequantize_per_tensor_tensor input TensorBox scale TensorBox zero_point TensorBox quant_min int quant_max int dtype torch dtype out_dtype Optional torch dtype = None - Union TensorBox ShapeAsConstantBuffer assert len scale get_size == len scale get_size == scale get_size == expect scale scalar tensor assert len zero_point get_size == len zero_point get_size == zero_point get_size == expect zero_point scalar tensor assert input get_dtype == dtype f Expecting input have dtype dtype got dtype input get_dtype out_dtype None out_dtype = torch float input_loader = input make_loader scale_loader = scale make_loader zero_point_loader = zero_point make_loader inner_fn idx input = input_loader idx _scale = scale_loader len scale get_size == _zero_point = zero_point_loader len scale get_size == scale dtype = torch float _scale = ops to_dtype _scale torch float zero_point dtype = torch float _zero_point = ops to_dtype _zero_point torch float val = ops sub ops to_dtype input torch float _zero_point _scale val = ops to_dtype val out_dtype val Pointwise create device=input get_device dtype=out_dtype inner_fn=inner_fn ranges=input get_size register_lowering aten cat cat inputs dim= cpu_device = inputs get_device type == cpu cpu_device all input get_dtype torch int torch uint input inputs TODO leslie Remove fallback when we support vectorization code gen uint data type directly input inputs input realize all len input get_size == input inputs inputs _ = require_channels_last aten cat inputs fallback_handler aten cat default inputs dim len inputs == clone inputs dim = _validate_dim inputs dim dtype = get_promoted_dtype inputs type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT inputs = to_dtype inp dtype inp inputs unwrap_tensor x Union TensorBox ir StorageBox - ir IRNode isinstance x TensorBox isinstance x data ir BaseView x data unwrap_view x data isinstance x ir StorageBox x data x is_reduction t isinstance t ir ComputedBuffer isinstance t data ir Reduction can_fuse_reduction t isinstance t TensorBox ir StorageBox can_fuse_reduction unwrap_tensor t is_reduction t isinstance t ir Pointwise any can_fuse_reduction V graph get_buffer read read t get_read_names fusing reducutions into computed concat buffer can cause regressions fusable_reduction = any can_fuse_reduction t t inputs should_lower_cat_input x - bool Unrealized inputs will storage layouts we dont want realize them case we want fuse ir is_storage_and_layout x storage _ = ir as_storage_and_layout x freeze=False ir ConcatKernel can_realize_into_without_copy storage isinstance x TensorBox ir StorageBox should_lower_cat_input unwrap_tensor x isinstance x ir Pointwise True False config force_pointwise_cat pointwise_cat inputs dim TODO We observed negative performance impact pointwise_cat optimization CPU so disabled We will revisit later after enabling vectorization index_expr cpu_device TensorBox ir ConcatKernel create inputs dim op_count x isinstance x TensorBox ir StorageBox op_count unwrap_tensor x will correspond direct memory read isinstance x ir Pointwise count = x inner_fn_opcount num_ops read x get_read_names count += op_count V graph get_buffer read count inputs increase possibility register spilling also increases past certain threshold inputs we only fuse input kernels simple sure we want expose users via config since logic may change future MAX_COMPLEX_POINTWISE_CAT = MAX_SIMPLE_OP_COUNT = additional_pointwise_ops op torch _ops OpOverload op aten cat default aten constant_pad_nd default len inputs = MAX_COMPLEX_POINTWISE_CAT len inputs = config max_pointwise_cat_inputs all op_count t = MAX_SIMPLE_OP_COUNT t inputs pointwise_uses = all is_pointwise_use use additional_pointwise_ops use V current_node users fuse case we will used pointwise node there any inputs we we can prevent materialization fuse_pointwise_use = any should_lower_cat_input inp inp inputs pointwise_uses horizontal fuse case all inputs will require copy kernel anyway only horizontally fuse pointwise kernels horizontal_fuse_cat = all should_lower_cat_input inp inp inputs any can_fuse_reduction t t inputs fuse_pointwise_use horizontal_fuse_cat fusable_reduction pointwise_cat inputs dim TensorBox ir ConcatKernel create inputs dim register_lowering aten diagonal type_promotion_kind=None diagonal input offset int = dim int = dim int = original_shape = input get_size num_dims = len original_shape dim = canonicalize_dim idx=dim rank=num_dims dim = canonicalize_dim idx=dim rank=num_dims check dim = dim lambda f diagonal dimensions cannot identical dim dim offset_negative = V graph sizevars evaluate_expr sympy Lt offset offset_negative diag_size = V graph sizevars evaluate_max V graph sizevars evaluate_min original_shape dim + offset original_shape dim type ignore arg-type diag_size = V graph sizevars evaluate_max V graph sizevars evaluate_min original_shape dim original_shape dim - offset type ignore arg-type base_idx = offset_negative base_idx = -offset base_idx = offset sizes = s i s enumerate original_shape i dim dim sizes append diag_size reindexer idx diag_idx = idx - original_idx = len original_shape cur_dim = d range num_dims d == dim original_idx d = diag_idx + base_idx d == dim original_idx d = diag_idx + base_idx original_idx d = idx cur_dim cur_dim += assert cur_dim == len original_shape - original_idx TensorBox ir GenericView create input sizes reindexer register_lowering aten diagonal_copy type_promotion_kind=None diagonal_copy input offset int = dim int = dim int = clone diagonal input offset dim dim register_lowering aten diagonal_scatter type_promotion_kind=None diagonal_scatter input src offset int = dim int = dim int = output = clone input target = diagonal output offset dim dim mutate_to target src output register_lowering aten select type_promotion_kind=None select x dim idx idx = sympy expand idx size = sympy expand x get_size dim actual_index = None V graph sizevars guard_or_false sympy Lt idx actual_index = idx + size V graph sizevars guard_or_false sympy Ge idx actual_index = idx actual_index None has_free_unbacked_symbols idx Inductor could generate incorrect views tensors unbacked symbols here Squeeze operations translated views resulting incorrect strides Additionally we want avoid accidental unbacked unsqueeze semantics To resolve we use as_strided instead Removing branch will cause test_unbacked_select_index_with_check fail before accessing size stride offset we need realize x realize new_size = x get_size new_stride = x get_stride new_storage_offset = x get_layout offset + new_stride dim actual_index del new_size dim del new_stride dim as_strided x new_size new_stride new_storage_offset no need clamp function handles negative indexing itself slice_result = slice_ x dim actual_index actual_index + clamp=False squeeze slice_result dim Unbacked Semantics When index idx unbacked e g u we compute index dynamically during lowering select operation using DynamicSelectStorageOffset unbacked_bindings = resolve_unbacked_bindings V graph sizevars shape_env V graph current_node meta unbacked_bindings assert unbacked_bindings None assert len unbacked_bindings == unbacked_bindings unbacked_offset_sym _ = next iter unbacked_bindings items before accessing size stride offset we need realize x realize new_size = x get_size new_stride = x get_stride new_storage_offset = unbacked_offset_sym buffer = ir DynamicSelectStorageOffset unbacked_offset_sym idx x get_layout offset new_stride dim x get_size dim clamp=False buffer name = V graph register_buffer buffer V graph register_operation buffer del new_size dim del new_stride dim as_strided x new_size new_stride new_storage_offset register_lowering aten split type_promotion_kind=None split x sizes dim= dim = _validate_dim x dim sizes_ = sizes If sizes integer SymInt we turn into list sizes computing what actual size each chunk should isinstance sizes list tuple x_size = x get_size dim chunks = V graph sizevars guard_int FloorDiv x_size + sizes - sizes sizes_ = sizes chunks The last chunk might have smaller size than rest sizes_ - = x_size - chunks - sizes From point we assume sum sizes all chunks equals size base tensor result = start = size sizes_ end = start + size No need clamping here since we compute exact start end values result append slice_ x dim start end clamp=False start = end result register_lowering aten split_with_sizes type_promotion_kind=None split_with_sizes x sizes dim= split x sizes dim register_lowering aten unbind type_promotion_kind=None unbind x dim= dim = _validate_dim x dim x_size = V graph sizevars guard_int x get_size dim result = select x dim i i range x_size result register_lowering aten unfold type_promotion_kind=None unfold x dimension size step sizes = x get_size ndim = len sizes dim = canonicalize_dim ndim dimension ndim == slice_ unsqueeze x end=size clamp=False dim_size = sizes dim sizevars = V graph sizevars sizevars check_leq size dim_size sizevars check_lt step type ignore arg-type new_dim_size = FloorDiv dim_size - size step + sizevars size_hint_or_throw dim_size x mark_reuse sizevars size_hint_or_throw CeilDiv new_dim_size size dim_size out_size = sizes dim new_dim_size sizes dim + size reindexer idx dim_idx = idx - + idx dim step idx dim dim_idx idx dim + - TensorBox ir GenericView create x out_size reindexer register_lowering aten unsqueeze type_promotion_kind=None unsqueeze x dim dim = _validate_dim x dim new_shape = list x get_size new_shape insert dim sympy S One view x new_shape register_lowering aten unsqueeze_ type_promotion_kind=None unsqueeze_ x dim val = unsqueeze x dim assert isinstance x TensorBox assert isinstance val TensorBox x data = val data x _validate_dim x dim offset= dim = V graph sizevars shape_env evaluate_expr sympy sympify dim ndim = len x get_size dim dim += ndim + offset assert = dim ndim + offset dim register_lowering aten glu glu x dim=- dim = _validate_dim x dim TODO don t guard static shape here new_len = V graph sizevars guard_int x get_size dim no need clamp index int based input size = slice_ x dim new_len clamp=False b = slice_ x dim new_len new_len clamp=False mul sigmoid b fallback_handler kernel add_to_fallback_set=True add_to_fallback_set fallbacks add kernel handler args kwargs wrap_tensors x TensorBox create x isinstance x ir IRNode x pytree tree_map wrap_tensors ir FallbackKernel create kernel args kwargs This lets us detect lowering fallback handler handler _is_fallback_handler = True type ignore attr-defined handler functools cache _warn_complex_not_supported warnings warn Torchinductor does support code generation complex operators Performance may worse than eager There some types CPU which we accept input output unsupported_input_tensor t torch Tensor node=None Do support reading writing tensor t is_complex Complex views supported IR ComplexView _warn_complex_not_supported True t is_meta True t is_sparse True t dtype == torch float _e m fnu node True allow bitcast views memory movement arithmetic TODO delete once triton adds native support isinstance node target torch _ops OpOverload node target aten view dtype aten cat default aten clone default aten _scaled_mm default isinstance node target torch _ops OpOverload is_view node target False unsupported_output_tensor t torch Tensor node=None Do support writing tensor can read supported_complex_views = aten view dtype torch ops prims convert_element_type default node None node target supported_complex_views t is_complex False unsupported_input_tensor t node True t is_cpu config disable_cpp_codegen fallback_node_due_to_unsupported_type node torch fx Node allow_cpu_inputs=True Custom fallback lowering node target aten view_as_complex default False node op == placeholder False We should able remove special case once ` disable_cpp_codegen ` killed node target aten lift_fresh_copy default False check_skip_condition inp_out_node is_output isinstance inp_out_node torch fx Node False val inp_out_node meta False meta pytree tree_leaves inp_out_node meta val isinstance meta torch _subclasses FakeTensor continue is_output unsupported_output_tensor meta node True unsupported_input_tensor meta node True False only skip codegen there cpu output input arg pytree arg_tree_leaves node args node kwargs check_skip_condition arg is_output=False True check_skip_condition node is_output=True make_fallback op layout_constraint=None warn=True override_decomp=False assert op decompositions override_decomp f both fallback decomp same op op warn bool os getenv CI get_decompositions op fallback_random we allow decomposing random config fallback_random op torch _decomp decompositions_for_rng extra_random_decomps override_decomp Note warn holdover when warning ops previously set warn=False we do want CI error Ignore suppress errors configs CI particular warning happens startup anyway likely triggered preferentially one CI config over another torch _dynamo config suppress_errors torch _dynamo config suppress_errors = False log warning A make_fallback error occurred suppress_errors config suppress_errors being disabled surface raise AssertionError f make_fallback op decomposition exists we should switch To fix error either add decomposition core_aten_decompositions preferred inductor_decompositions delete corresponding ` make_fallback ` line Get help inductor team unsure don t pick arbitrarily unblock yourself register_fallback op_overload add_needs_realized_inputs op_overload layout_constraint None add_layout_constraint op_overload layout_constraint register_lowering op_overload type_promotion_kind=None fallback_handler op_overload isinstance op torch _ops OpOverloadPacket ol op overloads op_overload = getattr op ol register_fallback op_overload isinstance op torch _ops OpOverload torch _ops HigherOrderOperator register_fallback op raise RuntimeError f Unsupported fallback op type type op philox_rand_offset shape TorchInductor offset calculation differs PyTorch eager offset calculation random ops tl rand vs torch rand In future we should strive same impl tl rand torch rand numel = s shape numel = numel s tensor numel dtype=torch int register_lowering torch ops rngprims philox_rand type_promotion_kind=None philox_rand size seed offset stride device dtype stride arg optional will used future distributed random ops Currently its unused random_pos = ir FixedLayout device dtype size ir FlexibleLayout contiguous_strides size make_indexer seed_loader = seed make_loader offset_loader = offset make_loader inner_fn index Both seed offset philox_rand op tensors torch seed offsets type int tl rand accepts int seed_index_expr = ops to_dtype seed_loader torch int offset_index_expr = ops to_dtype offset_loader torch int Get offset d position rand_index_expr = ops add ops index_expr random_pos index torch int offset_index_expr result = ops rand seed_index_expr rand_index_expr ops to_dtype result dtype random_values_node = Pointwise create device=device dtype=dtype inner_fn=inner_fn ranges=list size offset_node = philox_rand_offset size random_values_node offset_node register_lowering aten native_dropout type_promotion_kind=None native_dropout x p train config fallback_random pytree tree_map TensorBox create ir FallbackKernel create aten native_dropout default x p train raise AssertionError should handled replace_random py register_lowering aten bernoulli_ type_promotion_kind=None bernoulli_ x args assert config fallback_random x get_device == torch device cpu should handled decomps unless config fallback_random device CPU x realize op_overload = aten bernoulli_ float len args == isinstance args float aten bernoulli_ Tensor ir InplaceBernoulliFallback op_overload x args x register_lowering aten bernoulli p type_promotion_kind=None bernoulli_p x args assert config fallback_random x get_device == torch device cpu should handled decomps unless config fallback_random device CPU bernoulli_ clone x args This shouldn t called general register_lowering aten _foobar _foobar _ raise AssertionError functools lru_cache _warn_triton_random salt log info using triton random expect difference eager warn_triton_random only warn once per graph _warn_triton_random V graph creation_time fallback_rand_default = fallback_handler aten rand default fallback_rand_generator = fallback_handler aten rand generator fallback_randn_default = fallback_handler aten randn default fallback_randn_generator = fallback_handler aten randn generator make_fallback aten randint register_lowering aten rand rand args kwargs kwargs get generator None fallback_rand_generator args kwargs config fallback_random kwargs pop generator None fallback_rand_default args kwargs raise AssertionError should have been handled replace_random py register_lowering aten randn randn args kwargs kwargs get generator None fallback_randn_generator args kwargs config fallback_random kwargs pop generator None fallback_randn_default args kwargs raise AssertionError should have been handled replace_random py register_lowering inductor_prims force_stride_order type_promotion_kind=None inductor_force_stride_order input_tensor stride stride_order = ir get_stride_order stride ir ExternKernel require_stride_order input_tensor stride_order register_lowering inductor_prims seed type_promotion_kind=None inductor_seed device torch device raise AssertionError should handled fuse_seed_creation_pass register_lowering inductor_prims seeds type_promotion_kind=None inductor_seeds count device warn_triton_random TensorBox create ir RandomSeeds count decode_device device register_lowering inductor_prims lookup_seed type_promotion_kind=None inductor_lookup_seed seeds index inner_fn _ ops load_seed seeds get_name index Pointwise create device=seeds get_device dtype=seeds get_dtype inner_fn=inner_fn ranges= register_lowering inductor_prims random type_promotion_kind=None inductor_random size list int seed TensorBox mode str offset int = assert config fallback_random assert mode rand randn size = size dtype = torch float device = seed get_device_or_error random_pos = ir FixedLayout device dtype size ir FlexibleLayout contiguous_strides size offset=offset make_indexer seed_loader = seed make_loader inner_fn index getattr ops mode seed_loader ops index_expr random_pos index torch int result = Pointwise create device=device dtype=dtype inner_fn=inner_fn ranges= size result realize result register_lowering inductor_prims randint type_promotion_kind=None inductor_randint low int high int size list int seed TensorBox offset int = assert config fallback_random size = size dtype = torch int device = seed get_device_or_error random_pos = ir FixedLayout device dtype size ir FlexibleLayout contiguous_strides size offset=offset make_indexer seed_loader = seed make_loader inner_fn index ops randint seed_loader ops index_expr random_pos index torch int ops index_expr low torch int ops index_expr high torch int Pointwise create device=device dtype=dtype inner_fn=inner_fn ranges= size _boundaries_helper tb TensorBox - tuple str sympy Expr sympy Expr sympy Expr Calculate maximum offset boundaries tensor For strided tensor sum size i - stride i + stride - This ensures mask check bucketize_binary_search works correctly both contiguous non-contiguous tensors size = tb get_size stride = tb get_stride max_offset = sum s - st s st zip size stride + stride - tb get_name size - max_offset stride - _sorter_helper tb TensorBox - tuple str sympy Expr tb get_name tb get_stride - register_lowering aten searchsorted Tensor type_promotion_kind=None searchsorted sorted_sequence TensorBox TensorBox out_int bool = False right bool = False side Optional str = None sorter Optional TensorBox = None - Union TensorBox ShapeAsConstantBuffer validate_bucketize = lambda tb V graph has_feature noqa E tb BackendFeature BUCKETIZE validate_bucketize sorted_sequence validate_bucketize sorter None validate_bucketize sorter fallback_handler aten searchsorted Tensor add_to_fallback_set=False sorted_sequence out_int =out_int right=right side=side sorter=sorter If side present override value right needed This assumes validation two options being non-contradictory already done searchsorted meta-function side None side == right right = True index_dtype = torch int out_int torch int values_loader = make_loader The entire sorted_sequence tensor needs used ops bucketize so we need realize into global memory other words we can t guarantee sorted_sequence get_name used below will exist unless we call sorted_sequence realize sorted_sequence realize sorter None sorter realize len sorted_sequence get_size == inner_fn idx val = values_loader idx ops bucketize val _boundaries_helper sorted_sequence index_dtype right sorter=None sorter None _sorter_helper sorter sorter_indices=None sorter None inner_fn idx val = values_loader idx Get index beginning sorted sequence within flattened version array get_flattened_index tb TensorBox strides = tb get_stride ops index_expr functools reduce operator add s i s i zip strides - idx - index_dtype ops bucketize val _boundaries_helper sorted_sequence get_flattened_index sorted_sequence index_dtype right sorter=None sorter None _sorter_helper sorter sorter_indices=None sorter None get_flattened_index sorter device = get_device result = Pointwise create device=device dtype=index_dtype inner_fn=inner_fn ranges=self shape see NOTE inductor bucketize realize result realize result register_lowering aten bucketize type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND NO_OPMATH bucketize input TensorBox boundaries TensorBox out_int bool = False right bool = False assert len boundaries get_size == V graph has_feature input BackendFeature BUCKETIZE V graph has_feature boundaries BackendFeature BUCKETIZE fallback_handler aten bucketize Tensor add_to_fallback_set=False input boundaries out_int =out_int right=right The entire boundaries tensor needs used ops bucketize so we need realize into global memory other words we can t guarantee boundaries get_name used below will exist unless we call boundaries realize boundaries realize device = input get_device input_loader = input make_loader index_dtype = torch int out_int torch int inner_fn index val = input_loader index indices = ops bucketize val _boundaries_helper boundaries index_dtype right indices result = Pointwise create device=device dtype=index_dtype inner_fn=inner_fn ranges=input get_size NOTE inductor bucketize realize bucketize_binary_search relatively expensive so we don t want re-compute unnecessarily If we run bucketize then broadcast result we don t want fused into large number duplicate bucketize computations each elements result If no broadcasting occurs fusions can still occur scheduler py result realize result require_dense _ args kwargs args kwargs = pytree tree_map_only ir IRNode ir ExternKernel require_stride args kwargs args kwargs require_contiguous _ args kwargs args kwargs = pytree tree_map_only ir IRNode ir ExternKernel require_contiguous args kwargs args kwargs require_contiguous_strides _ args kwargs TODO combine require_contiguous after https github com pytorch pytorch pull lands args kwargs = pytree tree_map_only ir IRNode ir ExternKernel require_contiguous_strides args kwargs args kwargs require_channels_last _ args kwargs args kwargs = pytree tree_map_only ir IRNode ir ExternKernel require_channels_last args kwargs args kwargs constrain_to_fake_tensor arg fake_arg isinstance arg ir IRNode meta_stride_expr = s node expr isinstance s torch SymInt s s fake_arg stride ir ExternKernel require_exact_strides arg meta_stride_expr isinstance arg dict key constrain_to_fake_tensor arg key fake_arg key key arg keys isinstance arg tuple list type arg constrain_to_fake_tensor f_a f_a zip arg fake_arg arg constrain_to_fake_tensors args kwargs fake_args fake_kwargs args = tuple constrain_to_fake_tensor arg fake_arg arg fake_arg zip args fake_args kwargs = k constrain_to_fake_tensor v fake_kwargs k k v kwargs items args kwargs constrain_to_fx_strides fx_node args kwargs apply_constraint arg fx_arg isinstance arg ir IRNode stride_order = ir get_stride_order fx_arg meta val stride V graph sizevars shape_env ir ExternKernel require_stride_order arg stride_order isinstance arg dict key apply_constraint arg key fx_arg key key arg keys arg args = tuple apply_constraint arg fx_arg arg fx_arg zip args fx_node args kwargs = k apply_constraint v fx_node kwargs k k v kwargs items args kwargs sdpa_constraint fx_node args kwargs sdpa requires dense last dimension apply_constraint idx arg fx_arg isinstance arg ir IRNode arg meta_val = fx_arg meta val meta_stride_expr = s node expr isinstance s torch SymInt s s meta_val stride shape_env = V graph sizevars shape_env stride_order = ir get_stride_order meta_val stride shape_env stride_order stride_order - = contiguous stride order stride_order = list reversed range len arg get_size fx_node target == aten _scaled_dot_product_efficient_attention_backward default idx assert len stride_order == The th arguments aten _scaled_dot_product_efficient_attention_backward default out gradient_out They have stride order Otherwise kernel will crash Check https github com pytorch pytorch issues stride_order = meta_val is_cuda ir ExternKernel require_stride_order arg stride_order This minimum alignment required SDPA kernels attention_bias This value can found pytorch aten src ATen native transformers attention cpp preprocess_mask ALIGNMENT = effn_attn_fwd does requires dense last dim just alignment effn_attn_fwd_bias = fx_node target == torch ops aten _scaled_dot_product_efficient_attention default idx == assert isinstance arg TensorBox len arg get_size arg is_aligned_tensor = ir is_aligned_realized_tensor arg ALIGNMENT is_aligned_tensor ir try_match_insignificant_strides ir ExternKernel realize_input arg meta_stride_expr isinstance arg IRNode arg maybe_get_stride None is_aligned_tensor ir try_match_insignificant_strides ir ExternKernel realize_input arg meta_stride_expr effn_attn_fwd_bias out_size = list arg get_size expanded_dims = We require dense last dimension other strides can expanded which results smaller tensor maybe_stride = arg maybe_get_stride i range len arg get_size - V graph sizevars statically_known_equals meta_stride_expr i maybe_stride None V graph sizevars statically_known_equals maybe_stride i expanded_dims append i Now pad strides alignment out_strides = - len out_size out_strides - = stride = i range len out_size - - - out_strides i + = stride = stride out_size i + expanded dims still need aligned they we can make them expanded setting stride equal i expanded_dims V graph sizevars statically_known_equals out_strides i + ALIGNMENT out_strides i = continue V graph sizevars statically_known_equals stride ALIGNMENT stride = ceildiv stride ALIGNMENT ALIGNMENT out_strides i = stride ir ExternKernel require_exact_strides arg out_strides is_aligned_tensor ir try_match_insignificant_strides ir ExternKernel realize_input arg meta_stride_expr isinstance arg IRNode arg maybe_get_stride None is_aligned_tensor ir try_match_insignificant_strides ir ExternKernel realize_input arg meta_stride_expr is_aligned x V graph sizevars guard_or_false sympy Eq Mod x get_size - ALIGNMENT isinstance arg data ir BaseView is_aligned arg is_aligned arg unwrap_view ir try_match_insignificant_strides ir ExternKernel realize_input arg meta_stride_expr ir ExternKernel require_stride_order arg stride_order args = tuple apply_constraint idx arg fx_arg idx arg fx_arg enumerate zip args fx_node args kwargs = k apply_constraint - v fx_node kwargs k k v kwargs items args kwargs WIP make_fallback aten _adaptive_avg_pool d isuruf make_fallback aten adaptive_max_pool d isuruf make_fallback aten _scaled_dot_product_attention_math_for_mps malfet Easy make_fallback aten uniform warn=False make_fallback aten exponential default warn=False fails accuracy test_torch py make_fallback aten _pdist_forward Has decomp Needs benchmarks make_fallback aten soft_margin_loss_backward warn=False py_impl make_fallback aten _fused_rms_norm warn=False MPS-only faster than decomp torch xpu is_available make_fallback aten embedding_dense_backward warn=False XPU-only faster than decomp Easy Impossible make_fallback aten _cdist_forward p= should feasible make_fallback aten _cdist_backward Medium make_fallback aten _trilinear Difficult Scans See discussion https dev-discuss pytorch org t pytorch-sparse-gnn-compiler-rfc make_fallback aten segment_reduce default make_fallback aten _segment_reduce_backward default Histogram need implement Histogram IR make_fallback aten histc make_fallback aten histogram bin_ct make_fallback aten _histogramdd_bin_edges default make_fallback aten _histogramdd_from_bin_cts default Need templated kernel make_fallback aten addbmm make_fallback aten _addmm_activation warn=False make_fallback aten _grouped_mm require_dense Need templated kernel Probably impossible write efficiently make_fallback aten convolution_backward constrain_to_fx_strides make_fallback aten _cudnn_rnn require_dense make_fallback aten _cudnn_rnn_backward require_contiguous Haven t checked sound difficult impossible make_fallback aten _embedding_bag require_contiguous make_fallback aten _embedding_bag_forward_only require_contiguous make_fallback aten _embedding_bag_backward make_fallback aten _embedding_bag_per_sample_weights_backward make_fallback aten _embedding_bag_per_sample_weights_backward make_fallback aten _fused_moving_avg_obs_fq_helper make_fallback aten _fused_moving_avg_obs_fq_helper_functional Backwards try py_impl ing them when fwd written decomp make_fallback aten max_pool d_with_indices_backward make_fallback aten _adaptive_avg_pool d_backward require_dense make_fallback aten _adaptive_avg_pool d_backward make_fallback aten adaptive_max_pool d_backward make_fallback aten adaptive_max_pool d_backward make_fallback aten fractional_max_pool d_backward make_fallback aten fractional_max_pool d_backward make_fallback aten replication_pad d_backward make_fallback aten replication_pad d_backward make_fallback aten upsample_linear d_backward make_fallback aten upsample_bicubic d_backward require_contiguous make_fallback aten upsample_trilinear d_backward make_fallback aten grid_sampler_ d_backward make_fallback aten _pdist_backward Impossible missing triton CPU features Sorting Sorting-like make_fallback aten sort make_fallback aten sort stable make_fallback aten kthvalue make_fallback aten topk make_fallback aten mode make_fallback aten median make_fallback aten nanmedian make_fallback aten randperm see https github com pytorch pytorch pull make_fallback aten resize_ make_fallback aten resize_as_ Linalg make_fallback aten _linalg_det make_fallback aten linalg_householder_product make_fallback aten linalg_inv_ex make_fallback aten linalg_ldl_factor_ex make_fallback aten linalg_ldl_solve make_fallback aten linalg_lu make_fallback aten linalg_lu_factor_ex make_fallback aten linalg_lu_solve make_fallback aten linalg_matrix_exp make_fallback aten linalg_qr make_fallback aten _linalg_slogdet make_fallback aten _linalg_solve_ex make_fallback aten linalg_solve_triangular make_fallback aten _linalg_svd make_fallback aten lu_unpack make_fallback aten ormqr make_fallback aten _linalg_check_errors make_fallback aten linalg_pinv atol_rtol_tensor make_fallback aten _linalg_eigh make_fallback aten triangular_solve make_fallback aten linalg_cholesky_ex make_fallback aten cholesky_inverse make_fallback aten cholesky_solve make_fallback aten geqrf make_fallback aten _fft_r c needs complex well Data dependent these necessary make_fallback aten nonzero default Misc make_fallback aten gcd default warn=False make_fallback aten _thnn_fused_lstm_cell require_dense make_fallback torch _prims rng_prims run_and_save_rng_state make_fallback torch _prims rng_prims run_with_rng_state make_fallback torch _prims rng_prims graphsafe_run_with_rng_state Implemented Half implemented Scans Implemented CUDA missing CPU make_fallback aten masked_scatter make_fallback aten masked_scatter_backward Complex number support make_fallback aten view_as_complex require_contiguous make_fallback aten angle needs complex Needs efficentzerotensor make_fallback aten _efficientzerotensor Needs Sparse make_fallback aten _sparse_coo_tensor_with_dims_and_tensors make_fallback aten to_sparse make_fallback aten _to_sparse Needs dimname support make_fallback aten zeros names Pattern-matched make_fallback aten _scaled_dot_product_efficient_attention default sdpa_constraint warn=False make_fallback aten _scaled_dot_product_efficient_attention_backward default sdpa_constraint warn=False make_fallback aten _scaled_dot_product_flash_attention default sdpa_constraint warn=False make_fallback aten _scaled_dot_product_flash_attention_backward default sdpa_constraint warn=False make_fallback aten _scaled_dot_product_cudnn_attention default sdpa_constraint warn=False make_fallback aten _scaled_dot_product_cudnn_attention_backward default sdpa_constraint warn=False make_fallback aten _scaled_dot_product_flash_attention_for_cpu default sdpa_constraint warn=False make_fallback aten _scaled_dot_product_flash_attention_for_cpu_backward default sdpa_constraint warn=False make_fallback aten _scaled_dot_product_fused_attention_overrideable default sdpa_constraint warn=False make_fallback aten _scaled_dot_product_fused_attention_overrideable_backward default sdpa_constraint warn=False make_fallback aten _flash_attention_forward default sdpa_constraint make_fallback aten _flash_attention_backward default sdpa_constraint make_fallback aten _efficient_attention_forward default sdpa_constraint make_fallback aten _efficient_attention_backward default sdpa_constraint index_reduce requires fallback when use_scatter_fallback returns True make_fallback aten index_reduce make_fallback aten repeat_interleave Tensor override_decomp=True Register type_promotion_kind None For example fp copy_ fp should promote first input s dtype register_lowering aten copy type_promotion_kind=None copy src non_blocking=False isinstance src ir IRNode src = tensor src dtype=self get_dtype device=self get_device x = src get_device = src get_device pyrefly ignore bad-argument-type x = to_device x get_device get_dtype = src get_dtype pyrefly ignore bad-argument-type x = to_dtype x get_dtype get_size = src get_size out = expand x get_size clone out clone x register_lowering aten clone clone x memory_format=None TODO jansel memory format Pointwise create device=x get_device dtype=x get_dtype inner_fn=x make_loader ranges=list x get_size clone_preserve_reinterpret_view x reinterpret_view_layouts = isinstance x TensorBox isinstance x data ir ReinterpretView x = x data unwrap TensorBox pyrefly ignore bad-assignment while isinstance x ir ReinterpretView reinterpret_view_layouts append x get_layout x = x data x = TensorBox x x = clone x reinterpret_view_layouts x = x data unwrap TensorBox layout reinterpret_view_layouts - x = ir ReinterpretView data=x layout=layout x = TensorBox x x hasattr aten lift_fresh_copy register_lowering aten lift_fresh_copy clone register_lowering prims iota iota length start step dtype device requires_grad fn index ops index_expr step index + start dtype=dtype Pointwise create device=decode_device device dtype=dtype inner_fn=fn ranges= length register_lowering aten select_scatter type_promotion_kind=None select_scatter x src dim int index int assert x get_dtype == src get_dtype x_loader = x make_loader dim = _validate_dim x dim V graph sizevars guard_or_false sympy Lt index index = index + x get_size dim V graph sizevars guard_or_false sympy Ge index pass unbacked index fallback_handler aten select_scatter default x src dim index V graph sizevars check_leq index type ignore arg-type V graph sizevars check_lt index x get_size dim type ignore arg-type src = expand unsqueeze src dim x get_size src_loader = src make_loader inner_fn idx ops where ops eq ops index_expr idx dim torch int ops index_expr index torch int src_loader idx x_loader idx Pointwise create device=x get_device dtype=x get_dtype inner_fn=inner_fn ranges=list x get_size register_lowering aten slice_scatter type_promotion_kind=None slice_scatter x src dim= start=None end=None step= src = to_dtype src x get_dtype x_loader = x make_loader dim = _validate_dim x dim dim_size = x get_size dim pyrefly ignore bad-argument-type start end = ir SliceView normalize_start_end x dim start end src_size = list x get_size src_size dim = FloorDiv end - start + step - step src = expand src src_size src_loader = src make_loader inner_fn idx start == end == dim_size step == selecting every element same just src clone src_loader idx idx_dim = ops index_expr idx dim torch int src_idx = list idx src_idx dim = FloorDiv idx dim - start step mask = start = mask append ops ge idx_dim ops index_expr sympy expand start torch int end = dim_size mask append ops lt idx_dim ops index_expr sympy expand end torch int step = mask append ops eq ops index_expr ModularIndexing idx dim - start step torch int ops constant torch int assert mask mask = functools reduce ops and_ mask src_val = ops masked mask lambda src_loader src_idx is_integer_type x ops where mask src_val x_loader idx Pointwise create device=x get_device dtype=x get_dtype inner_fn=inner_fn ranges=list x get_size _unwrap x isinstance x list tuple len x _unwrap x x register_lowering torch tensor aten scalar_tensor tensor data dtype=None device=None layout=None pin_memory=False assert_nyi layout None torch strided f layout= layout assert_nyi pin_memory pin_memory isinstance _unwrap data int dtype = dtype torch int dtype = dtype torch get_default_dtype ranges list sympy Expr = isinstance data sympy Basic inner_fn index ops index_expr data dtype isinstance data float int inner_fn index ops constant data dtype len data == isinstance data float int len data = inline small tensors ranges append sympy Integer len data inner_fn index binary_search start end assert start end end - start == ops constant data start dtype mid = end - start + start ops where ops lt ops index_expr index torch int ops constant mid torch int binary_search start mid binary_search mid end len data == ops constant dtype binary_search len data V graph add_tensor_constant torch tensor data dtype=dtype device=device Pointwise create device=decode_device device dtype=dtype inner_fn=inner_fn ranges=ranges register_lowering torch as_tensor as_tensor data dtype=None device=None isinstance data TensorBox dtype None data = to_dtype data dtype device None data = to_device data device data tensor data dtype=dtype device=device register_lowering torch LongTensor long_tensor data tensor data dtype=torch int register_lowering aten _local_scalar_dense _local_scalar_dense data This interesting Most lowerings tensors so you can just buffer you allocated will get used used s dead But _local_scalar_dense aka item returns int Tensor so you would have type mismatch you buffer we obligated sympy expression instead However we need actually codegen item call somehow We do registering faux buffer DynamicScalar IR node which solely responsible generating item The buffer used anything notice we discard codegen time buffer just gets assigned None unbacked_bindings = resolve_unbacked_bindings V graph sizevars shape_env V graph current_node meta unbacked_bindings assert unbacked_bindings None assert len unbacked_bindings == unbacked_bindings NB Have very careful here V graph current_node meta val seemingly also contains symbol which you want do binding actually isn t In particular we have later performed deferred runtime assert saying u == s you will actually see s expr This bad because we need actually generate assert says u == s so we need know where get u call In particular we must use unbacked_bindings which guaranteed have original unreplaced symbol question NB Another thing we have very careful about symbol bindings require nontrivial refinement e g when you have binding site x Sym u = y item Here code generation must do division order appropriately bind u This communicated via keypath unbacked_bindings we need hold onto order generate code appropriately case binding_sym keypath = next iter unbacked_bindings items buffer = ir DynamicScalar binding_sym keypath data buffer name = V graph register_buffer buffer V graph register_operation buffer NB replaced expr OK use directly downstream we want simplifications case val = V graph current_node meta val isinstance val torch SymInt torch SymFloat torch SymBool val node expr sympy sympify val register_lowering aten _assert_scalar _assert_scalar data msg NB These will handled codegen time Not sure we guaranteed able serve out truth deferred_runtime_asserts TODO try assert out See NOTE Codegen runtime asserts Inductor assert bool data scalar data None register_lowering aten _assert_tensor_metadata _assert_tensor_metadata size=None stride=None dtype=None device=None layout=None None _full fill_value device dtype size value = fill_value isinstance fill_value int float hasattr value value value = value value isinstance value int float inner_fn index ops constant value dtype isinstance value sympy Basic inner_fn index ops index_expr value dtype assert len value get_size == value_loader = value make_loader inner_fn index value_loader Pointwise create device=device dtype=dtype inner_fn=inner_fn ranges=list size full_like x fill_value kwargs create_tensor_like tensor_constructor fill_value x kwargs tensor_constructor fill_value torch zeros torch ones etc inner size names=None dtype=None device=None layout=None pin_memory=False memory_format=None assert_nyi names None named tensors assert_nyi layout None torch strided f layout= layout assert_nyi pin_memory pin_memory device = decode_device device dtype = dtype torch get_default_dtype len size == isinstance size list tuple torch Size size = tuple size See https github com pytorch pytorch issues All sizes lowering time should sympy Symbol SymInt s size assert isinstance s torch SymInt size = sympy expand s s size _full fill_value device dtype size inner register_lowering torch empty aten empty empty size names=None dtype=None layout=None device=None pin_memory=None memory_format=None assert_nyi names None named tensors device = decode_device device len size == isinstance size list tuple torch Size size = tuple size empty_strided size None dtype=dtype layout=layout device=device pin_memory=pin_memory create_tensor_like creation_fn Shim convert X_like into X For example zeros_like into zeros _constant_like x dtype=None device=None layout=None pin_memory=False memory_format=None assert_nyi pin_memory pin_memory assert_nyi layout None torch strided f layout= layout dtype None dtype = x get_dtype dtype = decode_dtype dtype device = device x get_device size = list x get_size creation_fn size dtype=dtype device=device layout=layout pin_memory=pin_memory _constant_like constant_like fill_value create_tensor_like tensor_constructor fill_value empty_like = register_lowering aten empty_like create_tensor_like empty ones_like = create_tensor_like tensor_constructor zeros_like = create_tensor_like tensor_constructor new_constant fill_value _new_constant x size dtype=None layout=None device=None pin_memory=None assert isinstance size list tuple assert_nyi pin_memory pin_memory assert_nyi layout None torch strided f layout= layout pyrefly ignore bad-argument-type dtype = decode_dtype dtype x get_dtype device = device x get_device size = sympy Integer s s size _full fill_value decode_device device dtype size _new_constant register_lowering aten new_empty new_empty x size dtype=None layout=None device=None pin_memory=None dtype None dtype = x get_dtype device None device = x get_device empty_strided size None dtype=dtype layout=layout device=decode_device device pin_memory=pin_memory register_lowering aten empty_strided empty_strided size stride dtype=None layout=None device=None pin_memory=None assert isinstance size list tuple assert isinstance stride list tuple type None assert_nyi pin_memory pin_memory assert_nyi layout None torch strided f layout= layout pyrefly ignore bad-argument-type dtype = decode_dtype dtype torch get_default_dtype device = device torch tensor device device = decode_device device pointwise = _full fill_value= device=device dtype=dtype size=size pointwise realize buffer = pointwise data data explicitly set ranges zeros order make NopKernelSchedulerNode buffer data = dataclasses replace buffer data ranges= len size assert isinstance buffer ir ComputedBuffer size = sympy expand s s size stride = sympy expand s s stride stride ir FlexibleLayout contiguous_strides size buffer layout = ir FixedLayout device=device dtype=dtype size=size stride=stride pointwise register_lowering aten new_empty_strided new_empty_strided x size stride dtype=None layout=None device=None pin_memory=None dtype None dtype = x get_dtype device None device = x get_device empty_strided size stride dtype=dtype layout=layout device=decode_device device pin_memory=pin_memory register_lowering prims copy_strided default copy_strided x stride stride = V graph sizevars size_hint_or_throw s s stride stride_order = sorted range len stride key=stride __getitem__ ir ExternKernel require_stride_order x stride_order register_lowering torch full aten full full size fill_value kwargs assert kwargs get dtype None dtype should handled decomposition tensor_constructor fill_value size kwargs register_lowering aten gather type_promotion_kind=None gather x dim index sparse_grad=False sparse_grad doesn t affect forward computation backward tracing taken care AOT Autograd assert isinstance x TensorBox index get_numel == Empty index case Return empty array same shape new_empty x index get_size size = x get_size offset = len size == dim = _validate_dim x dim offset offset x = expand x size = x_loader = x make_loader index_loader = index make_loader fn idx idx = list idx gather_idx = ops indirect_indexing index_loader idx size dim len idx == idx = gather_idx idx dim = gather_idx x_loader idx Pointwise create device=x get_device dtype=x get_dtype inner_fn=fn ranges=index get_size register_lowering aten embedding type_promotion_kind=None embedding weight indices padding_idx=- scale_grad_by_freq=False sparse=False sparse fallback_handler aten embedding default weight indices padding_idx scale_grad_by_freq sparse assert sparse assert isinstance weight TensorBox assert isinstance indices TensorBox assert int str indices get_dtype weight_loader = weight make_loader indices_loader = indices make_loader indices_ndim = len indices get_size weight_size = weight get_size new_size = indices get_size weight_size fn idx assert len idx == len new_size f idx = new_size var_index = indices_loader idx indices_ndim weight_idx = ops indirect_indexing var_index weight_size + idx indices_ndim weight_loader weight_idx Pointwise create device=weight get_device dtype=weight get_dtype inner_fn=fn ranges=new_size check_and_broadcast_indices indices device assert all i get_dtype torch int torch int torch bool torch uint i indices i None f indices must int byte bool Got i get_dtype i indices i None any i get_dtype torch bool torch uint i indices i None raise NotImplementedError Fallback bool indices valid_idxs = i i x enumerate indices isinstance x TensorBox assert len valid_idxs requires least non-None index new_indices = None len indices i x zip valid_idxs broadcast_tensors indices i i valid_idxs Eager allows indices CPU tensor when running CUDA FIXME Calling to_device x device should work test_advancedindex_mixed_cpu_devices still fails x get_device = device raise NotImplementedError Fallback when indices different device new_indices i = x new_indices valid_idxs index_output_size_and_inner_fn x_size indices tensor_indices tensor_size indices_loaders indexed_size x_loader check wrap_neg=True Note behavior indexing differs when there non consecutive tensors In case tensor index pulled beginning Suppose = torch arange view x = torch tensor Then x x will have shape due x x then will pulled front non_consecutive_tensors = False previous current itertools pairwise tensor_indices current - previous = non_consecutive_tensors = True output_size = x_size i i val enumerate indices val None output_size = output_size x_size len output_size + len tensor_indices first_tensor_index = tensor_indices non_consecutive_tensors output_size = tensor_size + output_size output_size = output_size first_tensor_index + tensor_size + output_size first_tensor_index fn idx assert len idx == len output_size assert len indices_loaders == len indexed_size rank = len tensor_size new_index = first_tensor_index = tensor_indices start_offset = non_consecutive_tensors first_tensor_index next_idx = i range tensor_indices - + i == start_offset next_idx += rank indices i None assert next_idx len idx new_index append idx next_idx next_idx += loader = indices_loaders i assert loader None size = indexed_size i new_index append ops indirect_indexing loader idx start_offset start_offset + rank size check=check wrap_neg=wrap_neg new_index = new_index idx next_idx new_index x_loader None x_loader new_index output_size fn index_impl x indices check output_size inner_fn _ = index_impl_helper x indices check Pointwise create device=x get_device dtype=x get_dtype inner_fn=inner_fn ranges=output_size index_impl_helper x indices check wrap_neg=True assert isinstance indices list tuple x_loader = x make_loader indices tensor_indices = check_and_broadcast_indices indices x get_device assert len tensor_indices Must have least one valid idx indices_loaders = i make_loader i None None i indices no guards output size all guards set broadcast_tensors We can use first one since they all required same size tensor_size = list indices tensor_indices get_size x_size = x get_size indexed_size = x_size i i range len indices indices i None check indexed_size tensor_size raise IndexError index out bounds dimension size indexed_size = x_size i i range len indices output_size index_inner_fn = index_output_size_and_inner_fn x_size indices tensor_indices tensor_size indices_loaders indexed_size None check=check wrap_neg=wrap_neg inner_fn idx x_loader index_inner_fn idx output_size inner_fn index_inner_fn register_lowering aten index type_promotion_kind=None index x indices try index_impl x indices check=True except NotImplementedError Fallback ATen boolean indexing x realize fallback_handler aten index Tensor add_to_fallback_set=False x indices register_lowering aten _unsafe_index type_promotion_kind=None _unsafe_index x indices index_impl x indices check=False All indexing decompositions written terms index index_put index_put_ We cannot have lowering decomposition introduces mutation graph which bad Aot Autograd Aot Autograd runs dead code elimination common subexpression elimination optimizations which assume graphs side-effect free More details https github com pytorch torchdynamo issues https github com pytorch torchdynamo issues register_lowering aten index_put type_promotion_kind=None index_put x indices values accumulate=False index_put_impl_ clone x indices values accumulate check=True may_realize=False register_lowering aten _unsafe_index_put _unsafe_index_put x indices values accumulate=False index_put_impl_ clone x indices values accumulate check=False may_realize=False index_put_as_masked_fill indices value accumulate value get_device = get_device value = to_device value get_device accumulate value = add value mutate_to where indices value index_put_fallback indices values accumulate op_overload = getattr aten index_put_ V graph current_node target _overloadname type ignore union-attr ir IndexPutFallback op_overload indices values accumulate register_lowering aten index_put_ type_promotion_kind=None index_put_ indices values accumulate=False index_put_impl_ indices values accumulate check=True may_realize=True register_lowering inductor_prims _unsafe_index_put_ type_promotion_kind=None _unsafe_index_put_ indices values accumulate=False index_put_impl_ indices values accumulate check=False may_realize=True index_put_impl_ indices values accumulate check may_realize=False may_realize try_get_name x isinstance x ir TensorBox x = x data isinstance x ir BaseView x = x unwrap_view isinstance x ir StorageBox x = x data x get_name isinstance x ir Buffer None indice_slice_from_randperm indice Refer https github com pytorch pytorch pull #discussion_r For specific pattern indices unique coming torch randperm However content indices unknown we have check specific pattern isinstance indice TensorBox isinstance indice data ir BaseView indice = indice data unwrap_view isinstance indice ir StorageBox isinstance indice data ir ExternKernel getattr indice data fx_node None indice data fx_node target torch ops aten randperm default False try_get_name values get_read_names all indice_slice_from_randperm indice indice indices Fix issue https github com pytorch pytorch issues When values have memory overlapping indices may contain duplicate values potentially causing incorrect results since load ` values ` might contain modified value store ` ` To address store values temporary buffer such cases values realize Dispatch masked fill single boolean index single value values get_numel == len indices == indices get_dtype torch bool torch uint mask = indices _ range len mask get_size len get_size mask = unsqueeze mask - index_put_as_masked_fill mask values accumulate Fallback torch deterministic mode torch are_deterministic_algorithms_enabled index_put_fallback indices values accumulate Fallback there boolean index index indices index None index get_dtype torch bool torch uint index_put_fallback indices values accumulate x_size = get_size x_ndim = len x_size accumulate needs_fallback_due_to_atomic_add_limitations get_dtype scalar Tensor x_ndim == = view = index_put_fallback indices values accumulate x_ndim == = view values = to_dtype values get_dtype try Note code will only get here when dtype uint indices tensor_indices = check_and_broadcast_indices indices get_device except NotImplementedError index_put_fallback indices values accumulate indices_loaders = i make_loader i None None i indices assert isinstance TensorBox realize scalar Tensor x_ndim == = view We can use first one since they all required same size tensor_size = list indices tensor_indices get_size indexed_size = x_size i i range len indices expected_vals_size inner_fn = index_output_size_and_inner_fn x_size indices tensor_indices tensor_size indices_loaders indexed_size None check=check values = expand values expected_vals_size all guards set above during broadcast_tensors expand device = get_device assert device None scatter = ir Scatter device=device dtype=self get_dtype inner_fn=values make_loader ranges=expected_vals_size iter_ranges output_indexer=inner_fn scatter_mode= atomic_add accumulate None buffer = ir ComputedBuffer name=None layout=ir MutationLayoutSHOULDREMOVE data=scatter buffer name = V graph register_buffer buffer V graph register_operation buffer x_ndim == = view fallback__unsafe_masked_index = fallback_handler aten _unsafe_masked_index default add_to_fallback_set=False fallback__unsafe_masked_index_put_accumulate = fallback_handler aten _unsafe_masked_index_put_accumulate default add_to_fallback_set=False register_lowering aten _unsafe_masked_index type_promotion_kind=None _unsafe_masked_index mask indices fill ranges _ _unsafe_index_fn = index_impl_helper indices check=False wrap_neg=False mask_loader = mask make_loader self_loader = make_loader inner_fn idx mask dtype = torch bool mask_val = ops to_dtype mask_loader idx torch bool mask_val = mask_loader idx ops masked mask_val lambda self_loader _unsafe_index_fn idx fill Pointwise create device=self get_device dtype=self get_dtype inner_fn=inner_fn ranges=ranges register_lowering aten _unsafe_masked_index_put_accumulate type_promotion_kind=None _unsafe_masked_index_put_accumulate x mask indices values masked_value = where mask values shape = x get_size clamped_indices = clamp indices i -shape i shape i - indices i None i range len indices TODO use masked store currently only triton supports masked stores cpp backend does _unsafe_index_put x clamped_indices masked_value accumulate=True make_pointwise clamp min max ops maximum min ops minimum max register_lowering aten as_strided_scatter type_promotion_kind=None as_strided_scatter src size stride storage_offset=None output = clone output_view = as_strided output size stride storage_offset copy_ output_view src output register_lowering aten scatter type_promotion_kind=None scatter x dim int index src kwargs scatter_ clone x dim index src kwargs scatter_fallback op_overload torch _ops OpOverload dim int index src reduce Optional str = None include_self bool = True src_is_tensor = isinstance src TensorBox use_scatter_fallback op_overload reduce get_dtype cast torch dtype src get_dtype src_is_tensor type src src get_device type src_is_tensor impl src_is_tensor ir ScatterFallback op_overload dim index src reduce=reduce include_self=include_self None register_lowering aten scatter_ type_promotion_kind=None scatter_ dim int index src reduce Optional str = None assert reduce None add multiply reduce None op_overload = getattr aten scatter_ V graph current_node target _overloadname type ignore union-attr fallback_result = scatter_fallback op_overload dim index src reduce=reduce fallback_result None fallback_result reduce == add reduce = sum reduce == multiply reduce = prod scatter_reduce_ dim index src reduce register_lowering aten scatter_add type_promotion_kind=None scatter_add x dim int index src scatter_add_ clone x dim index src register_lowering aten scatter_add_ type_promotion_kind=None scatter_add_ x dim int index src scatter_reduce_ x dim index src sum register_lowering aten scatter_reduce type_promotion_kind=None scatter_reduce x dim int index src reduction_type kwargs scatter_reduce_ clone x dim index src reduction_type kwargs register_lowering aten scatter_reduce_ type_promotion_kind=None scatter_reduce_ dim int index src reduce include_self bool = True assert reduce None sum prod mean amax amin assert len aten scatter_reduce_ overloads == two aten scatter_reduce_ overloads aten scatter_reduce_ two unique overload aten scatter_reduce_ isinstance src Number src = full_like src fallback_result = scatter_fallback aten scatter_reduce_ two dim index src reduce=reduce include_self=include_self fallback_result fallback_result assert isinstance TensorBox assert int str index get_dtype ndim = len get_size ndim == = view isinstance src TensorBox len src get_size == src = view src isinstance index TensorBox len index get_size == index = view index index get_numel == dim = _validate_dim dim realize index_loader = index make_loader src_loader = src make_loader isinstance src TensorBox None output_indexer idx captured end function so may have dim shape = get_size ndim = len shape indirect_idx = list idx indirect_idx dim = ops indirect_indexing index_loader idx ndim == shape dim wrap_neg=False indirect_idx fn idx src_loader src_loader idx src scalar pyrefly ignore bad-argument-type ops constant src get_dtype backend_reduce_str reduce reduce == sum atomic_add TODO Need support more reduction type assert reduce None None device = get_device assert device None include_self zero out corresponding elements first zero_out = ir Scatter device=device dtype=self get_dtype inner_fn=lambda index ops constant get_dtype ranges=index get_size output_indexer=output_indexer scatter_mode=None buffer = ir ComputedBuffer name=None layout=ir MutationLayoutSHOULDREMOVE data=zero_out buffer name = V graph register_buffer buffer V graph register_operation buffer index i j k j k += src i j k dim == i index i j k k += src i j k dim == i j index i j k += src i j k dim == scatter = ir Scatter device=device dtype=self get_dtype inner_fn=fn ranges=index get_size output_indexer=output_indexer scatter_mode=backend_reduce_str reduce buffer = ir ComputedBuffer name=None layout=ir MutationLayoutSHOULDREMOVE data=scatter buffer name = V graph register_buffer buffer V graph register_operation buffer ndim == = view upsample_nearestnd x output_size scales_x tuple Optional float n int = exact bool = False x realize_hint elements reused x_loader = x make_loader i_sizes = x get_size -n batch = x get_size -n i_sizes = V graph sizevars guard_int i i i_sizes assert len scales_x == n o_sizes = output_size inv_scales = i o i o zip i_sizes o_sizes i scale enumerate scales_x scale None inv_scales i = scale scale_fn x scale size Nearest Exact input_index = round scale output_index + - = floor scale output_index + Nearest input_index = floor scale output_index x = ops index_expr x torch float exact x = ops add x ops constant torch float x = ops mul x ops constant scale torch float x = ops to_dtype x torch int ops indirect_indexing x size check=False fn idx x = idx -n b = idx -n x_loader b scale_fn i s size i s size zip x inv_scales i_sizes Pointwise create device=x get_device dtype=x get_dtype inner_fn=fn ranges= batch o_sizes register_lowering aten upsample_nearest d default upsample_nearest d x output_size scales Optional float = None upsample_nearestnd x output_size scales n= register_lowering aten _upsample_nearest_exact d default _upsample_nearest_exact d x output_size scales Optional float = None upsample_nearestnd x output_size scales n= exact=True register_lowering aten upsample_nearest d default upsample_nearest d x output_size scales_h Optional float = None scales_w Optional float = None upsample_nearestnd x output_size scales_h scales_w n= register_lowering aten _upsample_nearest_exact d default _upsample_nearest_exact d x output_size scales_h Optional float = None scales_w Optional float = None upsample_nearestnd x output_size scales_h scales_w n= exact=True register_lowering aten upsample_nearest d default upsample_nearest d x output_size scales_d Optional float = None scales_h Optional float = None scales_w Optional float = None upsample_nearestnd x output_size scales_d scales_h scales_w n= register_lowering aten _upsample_nearest_exact d default _upsample_nearest_exact d x output_size scales_d Optional float = None scales_h Optional float = None scales_w Optional float = None upsample_nearestnd x output_size scales_d scales_h scales_w n= exact=True _create_constants args dtype tuple ops constant dtype args register_lowering prims rev default rev x dims note - dims pre-canonicalized x_loader = x make_loader sizes = x get_size loader idx idx = list idx assert len idx == len sizes dim dims idx dim = sizes dim - - idx dim x_loader idx Pointwise create device=x get_device dtype=x get_dtype inner_fn=loader ranges=sizes inplace_constant_pad_nd x TensorBox padding Sequence int fill_value float - Optional TensorBox This optimization changes semantics padding clone style view style Thanks functionalization change can still maintain numerical correctness _padding_can_be_fused Conservatively check padding can fused downstream op downstream op sum then there little benefit do inplace padding downstream op matmul doing inplace padding can save membw current_node = V graph current_node current_node None True conservative users = tuple current_node users len users == users target aten mm default aten addmm default False True conservative _padding_can_be_fused None Only handle D case now len padding = len x get_size = None No harm realize since we already know op can fused into single user It need realized later anyways x realize If x view e g SliceView realizing just realizing underlying storage x itself still view isinstance x ir TensorBox isinstance x data ir StorageBox isinstance x data data ir ComputedBuffer config can_inplace_pad_graph_input isinstance x data data ir InputBuffer x data data name None x freeze_layout _ layout = ir as_storage_and_layout x strides = layout stride strides = None padding = padding = padding = None npad = padding npad == None stride = strides rowsize = layout size stride rowsize + npad None bufname = x data data name padded_size = layout size layout size + npad V graph buffer_to_padded_size bufname = padded_size resized_x = as_strided x padded_size layout stride layout offset sliced_x = slice_ resized_x dim= start=rowsize end=rowsize + npad clamp=False fill_ sliced_x fill_value counters inductor inplace_padding += resized_x register_lowering aten constant_pad_nd type_promotion_kind=None constant_pad_nd x padding fill_value= assert len padding == all p == p padding clone x config inplace_padding out = inplace_constant_pad_nd x padding fill_value out out fall through can inplace padding sizes = x get_size bounds = list reversed list zip padding padding n = len sizes - len bounds padding complicated expression hoist bounds_precomp list tuple sympy Symbol Any = l h bounds bounds_precomp append V graph sizevars lookup_precomputed_size l h type ignore arg-type output_size = list sizes n mask_sizes = low high size zip bounds sizes n mask_sizes append size output_size append sympy expand size + low + high assert len output_size == len sizes fill_value = dtype_to_type x get_dtype fill_value mask index mask = idx low high length zip index n bounds mask_sizes low = mask append range_mask_low idx high = mask append range_mask_high idx length mask = functools reduce ops and_ mask ops masked mask lambda x_loader index fill_value offset_fn index new_index = list index n idx low _high zip index n bounds_precomp new_index append idx - low assert len new_index == len index mask new_index x_loader = x make_loader Pointwise create device=x get_device dtype=x get_dtype inner_fn=offset_fn ranges=output_size range_mask_low i sympy Expr low Union sympy Expr int ops ge ops index_expr i torch int ops index_expr sympy Integer low torch int range_mask_high i sympy Expr high sympy Expr ops lt ops index_expr i torch int ops index_expr high torch int range_mask i sympy Expr high sympy Expr low sympy Expr ops and_ range_mask_low i low range_mask_high i high constant_boundary_condition x fill_value padding=None pad_fill_value= dim=None h = x get_size -dim x_loader = x make_loader pyrefly ignore unsupported-operation padding_h = padding dim load index prefix = index -dim ih = index -dim mask = functools reduce ops and_ pyrefly ignore no-matching-overload range_mask ih i h i + padding_h i -padding_h i i range dim ops masked mask lambda constant_boundary_condition x pad_fill_value dim=dim prefix ih fill_value padding ops masked mask lambda x_loader prefix ih fill_value load pooling_size x i kernel_size stride padding ceil_mode dilation=None dilation None dilation = len padding x_out = FloorDiv x + padding i - dilation i kernel_size i - + stride i - stride i ceil_mode x_alt = FloorDiv x + padding i - dilation i kernel_size i - + stride i - stride i V graph sizevars size_hint x_alt - stride i - x - padding i = Sliding windows must start within input left padding x_alt -= type ignore assignment V graph sizevars check_leq x_alt stride i - x - padding i type ignore arg-type V graph sizevars size_hint x_out - x_alt == ceil mode actually no-op lets guard V graph sizevars check_equals x_out x_alt ceil_mode = False x_out = x_alt x_out ceil_mode should_fallback_max_pool_with_indices kernel_size n_dim kernel_size = pad_listlike kernel_size n_dim window_size = functools reduce operator mul kernel_size window_size max_pool_checks x kernel_size stride padding dilation n_dim assert_fallback=None padding == padding = n_dim dilation == dilation = n_dim stride stride = kernel_size kernel_size = pad_listlike kernel_size n_dim stride = pad_listlike stride n_dim padding = pad_listlike padding n_dim dilation = pad_listlike dilation n_dim assert isinstance x TensorBox assert len kernel_size == n_dim assert len stride == n_dim assert len padding == n_dim assert len dilation == n_dim assert len x get_size n_dim + n_dim + use_fallback = should_fallback_max_pool_with_indices kernel_size n_dim=n_dim assert_fallback None assert use_fallback == assert_fallback kernel_size stride padding dilation use_fallback _max_pool_with_offsets x kernel_size stride padding dilation ceil_mode n_dim x realize_hint batch = x shape -n_dim dhw = x shape -n_dim dhw_out ceil_mode = zip pooling_size dhw d d kernel_size stride padding ceil_mode dilation=dilation d range n_dim dtype = x dtype min_value = False dtype torch bool float -inf dtype is_floating_point torch iinfo dtype min new_size = list batch + list dhw_out any padding any ceil_mode any d d dilation x_loader = constant_boundary_condition x min_value dim=n_dim x_loader = x make_loader fn_inner idx reduction_idx prefix = idx -n_dim bh = idx -n_dim ih = bh i stride i + reduction_idx i dilation i - padding i i range n_dim x_loader prefix ih result = Reduction create reduction_type= max input_node=x device=x get_device dst_dtype=dtype src_dtype=dtype inner_fn=fn_inner ranges=new_size reduction_ranges=kernel_size offsets = Reduction create reduction_type= argmax input_node=x device=x get_device dst_dtype=torch int src_dtype=dtype inner_fn=fn_inner ranges=new_size reduction_ranges=kernel_size isinstance result data data Reduction type ignore attr-defined union-attr Only realize reduction isn t unrolled result realize isinstance offsets data data Reduction type ignore attr-defined union-attr Only realize reduction isn t unrolled offsets realize result offsets register_lowering prims _low_memory_max_pool_with_offsets type_promotion_kind=None _low_memory_max_pool_with_offsets x kernel_size stride padding dilation ceil_mode=False n_dim = len kernel_size assert we fallback path inductor decomp should have guaranteed kernel_size stride padding dilation _ = max_pool_checks x kernel_size stride padding dilation n_dim assert_fallback=False config patch unroll_reductions_threshold= result offsets = _max_pool_with_offsets x kernel_size stride padding dilation ceil_mode n_dim=n_dim result to_dtype offsets torch int _pool_offsets_to_indices offsets TensorBox kernel_size Sequence Union int torch SymInt input_size Sequence Union int torch SymInt increments_to_index Callable Sequence Union int torch SymInt Sequence Union int torch SymInt torch _inductor virtualized OpsValue - Union TensorBox ShapeAsConstantBuffer n_dim = len kernel_size offsets_loader = offsets make_loader window_size = sympy sympify functools reduce operator mul kernel_size offsets_to_indices idx offset = offsets_loader idx offset_sympy = ops indirect_indexing offset window_size reduction_idx = inductor_prims _flattened_index_to_nd offset_sympy kernel_size idhw = increments_to_index idx reduction_idx ops index_expr inductor_prims _flatten_index idhw input_size -n_dim torch int indices = Pointwise create device=offsets get_device dtype=torch int inner_fn=offsets_to_indices ranges=offsets get_size indices register_lowering prims _low_memory_max_pool_offsets_to_indices type_promotion_kind=None _low_memory_max_pool_offsets_to_indices offsets kernel_size input_size stride padding dilation TODO Generalize other max pooling flavors n_dim = len kernel_size increments_to_index idx reduction_idx bh = idx -n_dim bh i stride i + reduction_idx i dilation i - padding i i range n_dim _pool_offsets_to_indices offsets kernel_size input_size increments_to_index _max_pool_with_indices x kernel_size stride padding dilation ceil_mode n_dim kernel_size stride padding dilation _ = max_pool_checks x kernel_size stride padding dilation n_dim=n_dim out offsets = _max_pool_with_offsets x kernel_size stride padding dilation ceil_mode n_dim=n_dim indices = _low_memory_max_pool_offsets_to_indices offsets kernel_size x shape -n_dim stride padding dilation out indices Fallback when we do decompose low-memory path register_lowering aten max_pool d_with_indices type_promotion_kind=None max_pool d_with_indices x kernel_size stride=None padding= dilation= ceil_mode=False _max_pool_with_indices x kernel_size stride padding dilation ceil_mode n_dim= Fallback when we do decompose low-memory path register_lowering aten max_pool d_with_indices type_promotion_kind=None max_pool d_with_indices x kernel_size stride=None padding= dilation= ceil_mode=False _max_pool_with_indices x kernel_size stride padding dilation ceil_mode n_dim= fallback_max_pool d_with_indices_backward = fallback_handler aten max_pool d_with_indices_backward default add_to_fallback_set=False register_lowering aten max_pool d_with_indices_backward type_promotion_kind=None max_pool d_with_indices_backward grad_output x kernel_size stride padding dilation ceil_mode indices padding == padding = dilation == dilation = stride stride = kernel_size assert isinstance x TensorBox assert len kernel_size == assert len stride == assert len padding == assert len dilation == assert len x get_size we will read many times so make sure computed grad_output realize_hint gO_stride = grad_output maybe_get_stride x_stride Optional Sequence Any isinstance x TensorBox isinstance x data data Pointwise type ignore attr-defined data = x data data type ignore attr-defined device = data get_device assert device None x_buffer = ir ComputedBuffer name=None layout=ir FlexibleLayout device=device dtype=data get_dtype size=data get_size data=data x_buffer decide_layout x_stride = x_buffer get_stride x_stride = x maybe_get_stride is_channels_last = x_stride None x_stride == gO_stride None gO_stride == any d = d dilation dilation NYI fallback_max_pool d_with_indices_backward grad_output x kernel_size stride padding dilation ceil_mode indices _batch _height width = x get_size _ pooled_height pooled_width = grad_output get_size indices_loader = indices make_loader grad_loader = grad_output make_loader new_size = list x get_size h_window_size = max max FloorDiv h stride - max FloorDiv h - kernel_size stride h range kernel_size w_window_size = max max FloorDiv w stride - max FloorDiv w - kernel_size stride w range kernel_size window_size = h_window_size w_window_size window_size Kernel size too big Results hard-to-optimize Triton code Use fallback fallback_max_pool d_with_indices_backward grad_output x kernel_size stride padding dilation ceil_mode indices indices_size = indices get_size fn idx prefix h w = idx index_test = ops index_expr h width + w torch int h = h + padding w = w + padding phstart = ops index_expr FloorDiv h - kernel_size + stride stride torch int pwstart = ops index_expr FloorDiv w - kernel_size + stride stride torch int phend = ops index_expr FloorDiv h stride + torch int pwend = ops index_expr FloorDiv w stride + torch int phstart = ops maximum phstart ops constant torch int pwstart = ops maximum pwstart ops constant torch int phend = ops minimum phend ops index_expr pooled_height torch int pwend = ops minimum pwend ops index_expr pooled_width torch int gradient = None ph_ range h_window_size pw_ range w_window_size ph = ops add phstart ops constant ph_ torch int pw = ops add pwstart ops constant pw_ torch int grad_index = prefix ops indirect_indexing ops minimum ph ops sub phend ops constant torch int indices_size - check=False ops indirect_indexing ops minimum pw ops sub pwend ops constant torch int indices_size - check=False index_actual = indices_loader grad_index grad_part = grad_loader grad_index check = ops eq index_actual index_test gradient None don t need mask gradient = ops where check grad_part ops constant torch float mask = ops and_ ops and_ ops lt ph phend ops lt pw pwend check gradient = ops where mask ops add gradient grad_part gradient assert gradient None gradient out = Pointwise create device=grad_output get_device dtype=grad_output get_dtype inner_fn=fn ranges=new_size is_channels_last ir ExternKernel require_channels_last out out pad_adaptive_loader x pad_val= x_loader = x make_loader load prefix increments start_indices end_indices ih iw = increments h_start_index w_start_index = start_indices h_end_index w_end_index = end_indices mask = ops and_ ops lt ops index_expr h_start_index + ih torch int ops index_expr h_end_index torch int ops lt ops index_expr w_start_index + iw torch int ops index_expr w_end_index torch int ops masked mask lambda x_loader prefix h_start_index + ih w_start_index + iw pad_val load compute_indices_adaptive_pooling start_index end_index h_in w_in h_out w_out h_start_index = functools partial start_index out_dim=h_out inp_dim=h_in h_end_index = functools partial end_index out_dim=h_out inp_dim=h_in w_start_index = functools partial start_index out_dim=w_out inp_dim=w_in w_end_index = functools partial end_index out_dim=w_out inp_dim=w_in h_start_index h_end_index w_start_index w_end_index _adaptive_pooling_fn start_index end_index kernel_maxes in_sizes out_sizes pooling_fn h_in w_in = in_sizes h_out w_out = out_sizes h_start_index_fn h_end_index_fn w_start_index_fn w_end_index_fn = compute_indices_adaptive_pooling start_index end_index h_in w_in h_out w_out fn idx loader prefix bh bw = idx h_start_index = h_start_index_fn bh h_end_index = h_end_index_fn bh w_start_index = w_start_index_fn bw w_end_index = w_end_index_fn bw result = None ih iw itertools product range kernel_maxes range kernel_maxes val = loader prefix ih iw h_start_index w_start_index h_end_index w_end_index result None result = val result = pooling_fn val result result fn _adaptive_pooling_fn_with_idx start_index end_index kernel_maxes in_sizes out_sizes pooling_fn h_in w_in = in_sizes h_out w_out = out_sizes h_start_index_fn h_end_index_fn w_start_index_fn w_end_index_fn = compute_indices_adaptive_pooling start_index end_index h_in w_in h_out w_out fn idx loader prefix bh bw = idx h_start_index = h_start_index_fn bh h_end_index = h_end_index_fn bh w_start_index = w_start_index_fn bw w_end_index = w_end_index_fn bw maxval = None maxindex = None ih iw itertools product range kernel_maxes range kernel_maxes val = loader prefix ih iw h_start_index w_start_index h_end_index w_end_index index = ops index_expr h_start_index + ih w_in + w_start_index + iw torch int maxindex None maxindex = index maxindex = ops where ops gt val maxval index maxindex maxval None maxval = val maxval = pooling_fn val maxval maxindex fn fallback_adaptive_avg_pool d = fallback_handler aten _adaptive_avg_pool d default add_to_fallback_set=False register_lowering aten _adaptive_avg_pool d _adaptive_avg_pool d x output_size x get_dtype == torch int supported eager raise RuntimeError adaptive_avg_pool d implemented Long assert isinstance x TensorBox assert len output_size == x realize_hint batch h_in w_in = x get_size h_in = V graph sizevars guard_int h_in w_in = V graph sizevars guard_int w_in h_out w_out = output_size no-op same input output h_in == h_out w_in == w_out clone x h_out == w_out == o_size = batch h_out w_out empty o_size dtype=x get_dtype device=x get_device h_in h_out == w_in w_out == kernel_size = FloorDiv h_in h_out FloorDiv w_in w_out avg_pool d x kernel_size h_kernel_max = ceildiv h_in + h_out - h_out w_kernel_max = ceildiv w_in + w_out - w_out new_size = list batch + h_out w_out dtype = x get_dtype window_size = h_kernel_max w_kernel_max window_size Kernel size too big Results hard-to-optimize Triton code Use fallback fallback_adaptive_avg_pool d x output_size start_index index out_dim inp_dim FloorDiv index inp_dim out_dim end_index index out_dim inp_dim FloorDiv index + inp_dim + out_dim - out_dim fn_sum = _adaptive_pooling_fn start_index=start_index end_index=end_index kernel_maxes= h_kernel_max w_kernel_max in_sizes= h_in w_in out_sizes= h_out w_out pooling_fn=ops add ones_loader = pad_adaptive_loader ones_like x fn idx ops truediv fn_sum idx pad_adaptive_loader x fn_sum idx ones_loader rv = Pointwise create device=x get_device dtype=dtype inner_fn=fn ranges=new_size TODO should we force these realized rv fallback_adaptive_max_pool d = fallback_handler aten adaptive_max_pool d default add_to_fallback_set=False register_lowering aten adaptive_max_pool d adaptive_max_pool d x output_size x get_dtype == torch int supported eager raise RuntimeError adaptive_max_pool d implemented Long assert isinstance x TensorBox assert len output_size == x realize_hint batch h_in w_in = x get_size h_in = V graph sizevars guard_int h_in w_in = V graph sizevars guard_int w_in h_out w_out = output_size h_out == w_out == o_size = batch h_out w_out empty o_size dtype=x get_dtype device=x get_device empty o_size dtype=torch int device=x get_device h_in h_out == w_in w_out == This handled decomposition raise ValueError h_kernel_max = ceildiv h_in + h_out - h_out w_kernel_max = ceildiv w_in + w_out - w_out new_size = list batch + h_out w_out dtype = x get_dtype window_size = h_kernel_max w_kernel_max window_size Kernel size too big Results hard-to-optimize Triton code Use fallback fallback_adaptive_max_pool d x output_size start_index index out_dim inp_dim FloorDiv index inp_dim out_dim end_index index out_dim inp_dim FloorDiv index + inp_dim + out_dim - out_dim inner_func_max_val = _adaptive_pooling_fn start_index=start_index end_index=end_index kernel_maxes= h_kernel_max w_kernel_max in_sizes= h_in w_in out_sizes= h_out w_out pooling_fn=ops maximum inner_func_max_idx = _adaptive_pooling_fn_with_idx start_index=start_index end_index=end_index kernel_maxes= h_kernel_max w_kernel_max in_sizes= h_in w_in out_sizes= h_out w_out pooling_fn=ops maximum inner_fn_max_val idx inner_func_max_val idx pad_adaptive_loader x float -inf inner_fn_max_idx idx inner_func_max_idx idx pad_adaptive_loader x float -inf rv = Pointwise create device=x get_device dtype=dtype inner_fn=inner_fn_max_val ranges=new_size ri = Pointwise create device=x get_device dtype=torch int inner_fn=inner_fn_max_idx ranges=new_size rv ri _fractional_pooling_offsets samples in_sz out_sz kernel_sz dim ndims out_sz = out_sz dim in_sz = in_sz dim kernel_sz = kernel_sz dim samples_loader = samples make_loader load prefix i Handle indexing samples tensor correctly different input dimensions samples tensor always has shape N C fractional_max_pool d where - N= D inputs C H W N=batch_size D inputs N C H W - C=num_channels - two spatial dimensions height width samples_shape = samples get_size len samples_shape == Expected N C len prefix == D input case prefix= channel samples= C Access samples channel dim sample = samples_loader prefix ndims - - dim len prefix = D+ input case prefix= batch channel samples= batch C Access samples batch channel dim sample = samples_loader prefix prefix ndims - - dim Edge case - shouldn t happen valid fractional pooling sample = samples_loader ndims - - dim Fallback unexpected tensor shapes sample = samples_loader prefix ndims - - dim i_expr = ops index_expr i samples get_dtype diff = ops index_expr in_sz - kernel_sz torch int out_sz_expr = ops index_expr out_sz - torch int alpha = ops truediv ops to_dtype diff torch float ops to_dtype out_sz_expr torch float alpha = ops where ops eq out_sz_expr alpha seq_i = ops trunc i_expr + sample alpha - ops trunc sample alpha seq_i = ops to_dtype seq_i torch int mask = ops lt i_expr out_sz_expr ops indirect_indexing ops where mask seq_i diff sympy sympify in_sz load register_lowering aten fractional_max_pool d fractional_max_pool d x kernel_size output_size random_samples _fractional_max_pool x kernel_size output_size random_samples n_dim= register_lowering aten fractional_max_pool d fractional_max_pool d x kernel_size output_size random_samples _fractional_max_pool x kernel_size output_size random_samples n_dim= _fractional_max_pool x kernel_size output_size random_samples n_dim x realize_hint batch inp_dhw = x shape -n_dim x shape -n_dim config patch unroll_reductions_threshold= dhw_index_fn = _fractional_pooling_offsets samples=random_samples in_sz=inp_dhw out_sz=output_size kernel_sz=kernel_size ndims=n_dim dim=d d range n_dim x_loader = x make_loader fn_inner idx reduction_idx prefix = idx -n_dim x_loader prefix increments_to_index idx reduction_idx increments_to_index idx reduction_idx prefix = idx -n_dim bdhw = idx -n_dim dhw_index_fn d prefix bdhw d + reduction_idx d d range n_dim new_size = list batch + list output_size dtype = x get_dtype result = Reduction create reduction_type= max input_node=x device=x get_device dst_dtype=dtype src_dtype=dtype inner_fn=fn_inner ranges=new_size reduction_ranges=kernel_size offsets = Reduction create reduction_type= argmax input_node=x device=x get_device dst_dtype=torch int src_dtype=dtype inner_fn=fn_inner ranges=new_size reduction_ranges=kernel_size assert isinstance result TensorBox result isinstance result data data Reduction type ignore attr-defined Only realize reduction isn t unrolled result realize assert isinstance offsets TensorBox offsets isinstance offsets data data Reduction type ignore attr-defined Only realize reduction isn t unrolled offsets realize indices = _pool_offsets_to_indices offsets kernel_size x shape increments_to_index result indices register_lowering aten upsample_nearest d_backward default upsample_nearest d_backward x output_size=None input_size=None scales_h=None scales_w=None x realize_hint _batch inp_h inp_w = x get_size inp_h = V graph sizevars guard_int inp_h inp_w = V graph sizevars guard_int inp_w pyrefly ignore not-iterable _batch out_h out_w = input_size inp_h out_h == inp_w out_w == avg_pool d x FloorDiv inp_h out_h FloorDiv inp_w out_w divisor_override= h_kernel_max = ceildiv inp_h out_h w_kernel_max = ceildiv inp_w out_w start_index index out_dim inp_dim CeilDiv index inp_dim sympy sympify out_dim end_index index out_dim inp_dim start_index index + out_dim inp_dim fn_sum = _adaptive_pooling_fn start_index=start_index end_index=end_index kernel_maxes= h_kernel_max w_kernel_max in_sizes= inp_h inp_w out_sizes= out_h out_w pooling_fn=ops add fn idx fn_sum idx pad_adaptive_loader x rv = Pointwise create device=x get_device dtype=x get_dtype inner_fn=fn pyrefly ignore no-matching-overload ranges=list input_size rv fallback_avg_pool d = fallback_handler aten avg_pool d default add_to_fallback_set=False fallback_avg_pool d = fallback_handler aten avg_pool d default add_to_fallback_set=False register_lowering aten avg_pool d type_promotion_kind=None avg_pool d x kernel_size stride= padding= ceil_mode=False count_include_pad=True divisor_override=None _avg_poolnd x kernel_size stride padding ceil_mode count_include_pad divisor_override dim= register_lowering aten avg_pool d type_promotion_kind=None avg_pool d x kernel_size stride= padding= ceil_mode=False count_include_pad=True divisor_override=None _avg_poolnd x kernel_size stride padding ceil_mode count_include_pad divisor_override dim= _avg_poolnd x kernel_size stride padding ceil_mode count_include_pad divisor_override dim stride stride = kernel_size padding padding = dim kernel_size = pad_listlike kernel_size dim stride = pad_listlike stride dim padding = pad_listlike padding dim assert isinstance x TensorBox assert len kernel_size == dim assert len stride == dim assert len padding == dim assert len x get_size dim + dim + x realize_hint batch = x get_size -dim h = x get_size -dim h_out ceil_modes = zip pooling_size h i i kernel_size stride padding ceil_mode i range dim any padding any ceil_modes x_loader = constant_boundary_condition x dim=dim had_padding = True x_loader = x make_loader had_padding = False new_size = list batch + list h_out dtype = x get_dtype window_size = functools reduce operator mul kernel_size window_size Kernel size too big Results hard-to-optimize Triton code Use fallback dim == fallback = fallback_avg_pool d dim == fallback = fallback_avg_pool d raise ValueError f Unknown dim dim fallback x kernel_size stride padding ceil_mode count_include_pad divisor_override fn_sum idx loader prefix = idx -dim b = idx -dim total = None ih itertools product range kernel_size i i range dim inp = b i stride i + ih i - padding i i range dim val = loader prefix inp total None total = val total = ops add val total total had_padding divisor_override divisor = divisor_override divisor_override window_size dtype is_floating_point scale = divisor fn idx ops mul fn_sum idx x_loader ops constant scale dtype fn idx C style integer division done native cpu AvgPoolKernel cpp ops truncdiv fn_sum idx x_loader ops constant divisor dtype fn idx bh = idx -dim divide_factors = i range dim hstart = bh i stride i - padding i hend = sympy Min hstart + kernel_size i h i + padding i count_include_pad hstart = sympy Max hstart hend = sympy Min hend h i factor = ops index_expr hend - hstart torch int divide_factors append factor divide_factor = functools reduce ops mul divide_factors dtype is_floating_point ops truediv fn_sum idx x_loader divide_factor C style integer division done native cpu AvgPoolKernel cpp ops truncdiv fn_sum idx x_loader divide_factor rv = Pointwise create device=x get_device dtype=dtype inner_fn=fn ranges=new_size TODO jansel should we force these realized rv fallback_avg_pool d_backward = fallback_handler aten avg_pool d_backward default add_to_fallback_set=False register_lowering aten avg_pool d_backward type_promotion_kind=None avg_pool d_backward grad_output x kernel_size stride padding ceil_mode count_include_pad divisor_override=None assert divisor_override None divisor_override = divisor must zero stride stride = kernel_size padding padding = assert isinstance grad_output TensorBox assert isinstance x TensorBox assert len kernel_size == assert len stride == assert len padding == assert len x get_size grad_output realize_hint we will read many times so make sure computed _ height width = x get_size _h_out ceil_mode = pooling_size height kernel_size stride padding ceil_mode _w_out ceil_mode = pooling_size width kernel_size stride padding ceil_mode grad_loader = grad_output make_loader had_padding = padding padding ceil_mode ceil_mode _ pooled_height pooled_width = grad_output get_size new_size = list x get_size dtype = x get_dtype h_window_size = max max FloorDiv h stride - max FloorDiv h - kernel_size stride h range kernel_size w_window_size = max max FloorDiv w stride - max FloorDiv w - kernel_size stride w range kernel_size window_size = h_window_size w_window_size window_size Kernel size too big Results hard-to-optimize Triton code Use fallback fallback_avg_pool d_backward grad_output x kernel_size stride padding ceil_mode count_include_pad divisor_override compute_pool_size_without_padding ph pw This computes scaling factor we will divide element when ` count_include_pad=False ` stride_h = ops constant stride torch int stride_w = ops constant stride torch int pad_h = ops constant padding torch int pad_w = ops constant padding torch int kernel_h = ops constant kernel_size torch int kernel_w = ops constant kernel_size torch int hstart = ops sub ops mul ph stride_h pad_h wstart = ops sub ops mul pw stride_w pad_w hend = ops minimum ops add hstart kernel_h ops add ops index_expr height torch int pad_h wend = ops minimum ops add wstart kernel_w ops add ops index_expr width torch int pad_w hstart = ops maximum hstart ops constant torch int wstart = ops maximum wstart ops constant torch int hend = ops minimum hend ops index_expr height torch int wend = ops minimum wend ops index_expr width torch int divide_factor = ops mul ops sub hend hstart ops sub wend wstart divide_factor fn idx prefix h w = idx h = h + padding w = w + padding phstart = ops index_expr FloorDiv h - kernel_size + stride stride torch int pwstart = ops index_expr FloorDiv w - kernel_size + stride stride torch int phend = ops index_expr FloorDiv h stride + torch int pwend = ops index_expr FloorDiv w stride + torch int phstart = ops maximum phstart ops constant torch int pwstart = ops maximum pwstart ops constant torch int phend = ops minimum phend ops index_expr pooled_height torch int pwend = ops minimum pwend ops index_expr pooled_width torch int gradient = None ph_ range h_window_size pw_ range w_window_size ph = ops add phstart ops constant ph_ torch int pw = ops add pwstart ops constant pw_ torch int divisor_override None scale = divisor_override count_include_pad had_padding scale = kernel_size kernel_size scale = compute_pool_size_without_padding ph pw part = ops truediv grad_loader prefix ops indirect_indexing ops minimum ph ops sub phend ops constant torch int pooled_height check=False ops indirect_indexing ops minimum pw ops sub pwend ops constant torch int pooled_width check=False scale mask = ops and_ ops lt ph phend ops lt pw pwend gradient None gradient = ops where mask part ops constant torch float gradient = ops where mask ops add gradient part gradient assert gradient None gradient rv = Pointwise create device=grad_output get_device dtype=dtype inner_fn=fn ranges=new_size rv fallback_avg_pool d_backward = fallback_handler aten avg_pool d_backward default add_to_fallback_set=False register_lowering aten avg_pool d_backward type_promotion_kind=None avg_pool d_backward grad_output x kernel_size stride padding ceil_mode count_include_pad divisor_override=None assert divisor_override None divisor_override = divisor must zero stride stride = kernel_size padding padding = assert isinstance grad_output TensorBox assert isinstance x TensorBox assert len kernel_size == assert len stride == assert len padding == assert len x get_size grad_output realize_hint _batch depth height width = x get_size _d_out ceil_mode_d = pooling_size depth kernel_size stride padding ceil_mode _h_out ceil_mode_h = pooling_size height kernel_size stride padding ceil_mode _w_out ceil_mode_w = pooling_size width kernel_size stride padding ceil_mode grad_loader = grad_output make_loader had_padding = any padding ceil_mode_d ceil_mode_h ceil_mode_w _ pooled_depth pooled_height pooled_width = grad_output get_size new_size = list x get_size dtype = x get_dtype d_window_size h_window_size w_window_size = max max d stride i - max d - kernel_size i stride i d range kernel_size i i range window_size = d_window_size h_window_size w_window_size window_size Kernel size too big Results hard-to-optimize Triton code fallback_avg_pool d_backward grad_output x kernel_size stride padding ceil_mode count_include_pad divisor_override compute_pool_size_without_padding pd ph pw stride_d stride_h stride_w = ops constant s torch int s stride pad_d pad_h pad_w = ops constant p torch int p padding kernel_d kernel_h kernel_w = ops constant k torch int k kernel_size dstart hstart wstart = ops sub ops mul p s pad p s pad zip pd ph pw stride_d stride_h stride_w pad_d pad_h pad_w dend hend wend = ops minimum ops add start k ops add ops index_expr dim torch int pad start k dim pad zip dstart hstart wstart kernel_d kernel_h kernel_w depth height width pad_d pad_h pad_w dstart hstart wstart = ops maximum start ops constant torch int start dstart hstart wstart dend hend wend = ops minimum end ops index_expr dim torch int end dim zip dend hend wend depth height width divide_factor = ops mul ops mul ops sub dend dstart ops sub hend hstart ops sub wend wstart divide_factor fn idx prefix d h w = idx d h w = v + pad v pad zip d h w padding pdstart phstart pwstart = ops index_expr FloorDiv v - k + s s torch int v k s zip d h w kernel_size stride pdend phend pwend = ops index_expr FloorDiv v s + torch int v s zip d h w stride pdstart phstart pwstart = ops maximum pstart ops constant torch int pstart pdstart phstart pwstart pdend phend pwend = ops minimum pend ops index_expr pooled_dim torch int pend pooled_dim zip pdend phend pwend pooled_depth pooled_height pooled_width gradient = None Iterate over D region accumulate gradients pd_ range d_window_size ph_ range h_window_size pw_ range w_window_size pd ph pw = ops add pstart ops constant p_ torch int pstart p_ zip pdstart phstart pwstart pd_ ph_ pw_ divisor_override None scale = divisor_override count_include_pad had_padding scale = kernel_size kernel_size kernel_size scale = compute_pool_size_without_padding pd ph pw part = ops truediv grad_loader prefix ops indirect_indexing ops minimum pd ops sub pdend ops constant torch int pooled_depth check=False ops indirect_indexing ops minimum ph ops sub phend ops constant torch int pooled_height check=False ops indirect_indexing ops minimum pw ops sub pwend ops constant torch int pooled_width check=False scale mask = ops and_ ops and_ ops lt pd pdend ops lt ph phend ops lt pw pwend gradient None gradient = ops where mask part ops constant torch float gradient = ops where mask ops add gradient part gradient assert gradient None gradient rv = Pointwise create device=grad_output get_device dtype=dtype inner_fn=fn ranges=new_size rv _validate_reduction_axis x axis size = x get_size isinstance axis int axis = axis axis axis = range len size len size == assert tuple axis - f invalid axis axis axis = list axis i range len axis axis i axis i += len size len size assert = axis i len size len size == axis i == assert len OrderedSet axis == len axis reduction axis unique axis _make_reduction_inner x axis keepdims dtype override_return_dtype reduction_type=None dtype None x = to_dtype x dtype size = x get_size axis = OrderedSet int _validate_reduction_axis x axis kept_sizes = kept_idx = reduced_sizes = reduced_idx = i range len size i axis reduced_idx append i reduced_sizes append size i kept_idx append i kept_sizes append size i For argmax argmin compute logical indices when tensor has non-contiguous layout should_compute_logical_index = False reduction_type argmax argmin len reduced_sizes is_triton x isinstance x data PermuteView should_compute_logical_index = True isinstance x data ir ReinterpretView isinstance x data ir StorageBox isinstance x data data ir Buffer layout = x get_layout should_compute_logical_index = layout is_transposed layout is_contiguous loader index reduction_index assert len reduction_index == len reduced_idx keepdims assert len index == len size index = index i i kept_idx assert len index == len kept_idx new_index = None len index + len reduction_index idx var itertools chain zip kept_idx index zip reduced_idx reduction_index new_index idx = var value = inner_loader new_index For argmax argmin tuple logical linear index needed should_compute_logical_index rindex = sympy expand i i reduction_index Compute linear index row-major order For reduction_ranges = linear_index = r + r linear_idx = rindex i range len rindex linear_idx = linear_idx reduced_sizes i + rindex i value ops index_expr linear_idx torch int value keepdims new_size = list size i reduced_idx new_size i = sympy S One new_size = kept_sizes inner_loader = x make_loader dict device=x get_device dst_dtype=override_return_dtype x get_dtype src_dtype=x get_dtype inner_fn=loader ranges=new_size reduction_ranges=reduced_sizes make_reduction reduction_type ReductionType override_return_dtype=None inner x axis=None keepdims=False dtype=None kwargs = _make_reduction_inner x axis=axis keepdims=keepdims dtype=dtype override_return_dtype=override_return_dtype reduction_type=reduction_type result = Reduction create reduction_type=reduction_type input_node=x kwargs isinstance result data data type ignore attr-defined attr-type union-attr Reduction Only realize reduction isn t unrolled result realize result inner _make_scan_inner x axis dtype dtype None x = to_dtype x dtype axis = _validate_dim x axis dict device=x get_device dtypes= x get_dtype inner_fns= x make_loader size=x get_size axis=axis register_lowering aten mean mean x axis=None keepdim=False dtype=None dtype None x = to_dtype x dtype size = x get_size axis = _validate_reduction_axis x axis compute higher-precision until end mean lowering output_dtype = x get_dtype output_dtype torch float torch bfloat x = to_dtype x torch float sum_result = sum_ x axis keepdim denom = sympy_product size i i axis denom = ir IndexingConstant index=denom dtype=x get_dtype device=x get_device denom = ExpandView create denom list sum_result get_size to_dtype div sum_result denom output_dtype var_mean_sum_ x axis correction keepdim return_mean correction None correction = size = x get_size axis = _validate_reduction_axis x axis x_mean = mean x axis keepdim=True return_mean x_mean realize diffs = square sub x x_mean sum_result = sum_ diffs axis keepdim denom = sympy_product size i i axis correction denom = sympy Max denom - correction denom = ir IndexingConstant index=denom dtype=x get_dtype device=x get_device denom = ExpandView create denom list sum_result get_size x_var = div sum_result denom return_mean x_var x_mean = x_mean keepdim squeeze x_mean axis x_var x_mean use_two_step_variance x axis keepdim Instead unrolling welford just unroll simpler two-step var axis = _validate_reduction_axis x axis kwargs = _make_reduction_inner x axis=axis keepdims=keepdim dtype=None override_return_dtype=None ranges = kwargs ranges reduction_numel = sympy_product kwargs reduction_ranges isinstance reduction_numel sympy Integer int reduction_numel config unroll_reductions_threshold sympy_product ranges = var_mean_welford_ x axis correction keepdim return_mean correction None correction = kwargs = _make_reduction_inner x axis=axis keepdims=keepdim dtype=None override_return_dtype=None loader = kwargs pop inner_fn kwargs pop dst_dtype kwargs pop src_dtype mean m _ = ir WelfordReduction create inner_fns= loader reduction_type= welford_reduce dtype=x get_dtype kwargs m realize dtype = x get_dtype size = x get_size axis = _validate_reduction_axis x axis rnumel = sympy_product size i i axis get_constant_or_index_expr x dtype isinstance x sympy Expr x is_number ops to_dtype ops index_expr x torch int dtype ops constant x dtype scale_fn data c = get_constant_or_index_expr correction dtype N = get_constant_or_index_expr rnumel dtype zero = ops constant dtype data ops maximum zero N - c var = make_pointwise scale_fn m return_mean mean realize var mean var var_mean_helper_ x axis correction keepdim return_mean out_dtype = x get_dtype compute_dtype = get_computation_dtype out_dtype x = to_dtype x compute_dtype copy=False kwargs = dict x=x axis=axis correction=correction keepdim=keepdim return_mean=return_mean output = var_mean_sum_ kwargs use_two_step_variance x axis=axis keepdim=keepdim var_mean_welford_ kwargs output = tuple to_dtype x out_dtype copy=False x output output return_mean output register_lowering aten var prims var var_ x axis=None correction=None keepdim=False var_mean_helper_ x axis=axis correction=correction keepdim=keepdim return_mean=False register_lowering aten var_mean var_mean x axis=None correction=None keepdim=False var_mean_helper_ x axis=axis correction=correction keepdim=keepdim return_mean=True pow_recursive x y dtype y pow_recursive ops reciprocal x -y dtype y == ops constant dtype y == x result = pow_recursive x y dtype result = ops mul result result y == result = ops mul result x result make_pointwise pow_native b ops pow b fallback_pow_tensor_tensor = fallback_handler aten pow Tensor_Tensor add_to_fallback_set=False fallback_pow_scalar = fallback_handler aten pow Scalar add_to_fallback_set=False fallback_pow_tensor_scalar = fallback_handler aten pow Tensor_Scalar add_to_fallback_set=False register_lowering aten pow broadcast=True pow b isinstance b float b == int b pow int b isinstance b float b == sqrt isinstance b int b == clone Type promotion ensures all tensor arguments have same type dtype = next x get_dtype x b isinstance x ir TensorBox is_integer_pow = is_integer_dtype dtype Optimize away small fixed powers integers avoid falling back ATen embed_exponent = isinstance b int - b is_integer_pow b = embed_exponent loader = make_loader fn idx pow_recursive loader idx b get_dtype Pointwise create device=a get_device dtype=a get_dtype inner_fn=fn ranges=a get_size isinstance Number == full_like b pyrefly ignore missing-attribute == is_float_dtype b get_dtype exp b is_integer_pow ops pow doesn t work integers isinstance Number fallback_pow_scalar b isinstance b Number fallback_pow_tensor_scalar b fallback_pow_tensor_tensor b pow_native b mutate_to changed val unsafe_alias=False isinstance changed TensorBox changed_data = changed data changed_data = changed isinstance val TensorBox val = val data isinstance val ir StorageBox introduce copy handle views node = Pointwise create device=changed get_device dtype=changed get_dtype inner_fn=val make_loader ranges=changed get_size assert isinstance node BaseView MutableBox val = node data assert isinstance val ir StorageBox isinstance changed_data ir StorageBox changed_data is_input_buffer In AOTI module parameters buffers lifted graph inputs changed_data is_module_buffer isinstance changed_data data ir NopKernel Fast path just swing data pointer val realize changed_data data = val data changed ir MutationLayoutSHOULDREMOVE realize_into val changed_data unsafe_alias=unsafe_alias changed register_lowering aten fill_ fill_ x fill_value mutate_to x full_like x fill_value register_lowering aten copy_ type_promotion_kind=None copy_ dst src non_blocking=False dst src dst copy_ dst can happen reinplacing pass dst src = to_device src dst get_device src = to_dtype src dst get_dtype src = expand src dst get_size mutate_to dst src make_pointwise floordiv b ops floordiv b make_pointwise truncdiv b ops truncdiv b register_lowering aten div broadcast=True div_mode b rounding_mode=None both_integer = is_integer_type is_integer_type b both_boolean = is_boolean_type is_boolean_type b floordiv truncdiv need special handling integer tensors Triton see discussion https github com triton-lang triton issues rounding_mode == floor assert both_boolean floordiv operands can boolean same time floordiv b both_integer floor div b rounding_mode == trunc assert both_boolean truncdiv operands can boolean same time truncdiv b both_integer trunc div b div b register_lowering aten mul broadcast=True mul b both_bool = is_boolean_type is_boolean_type b both_bool logical_and b fn = ops_wrapper aten mul __name__ make_pointwise fn b get_constant_value x ir IRNode - Optional ir Constant Try convert arbitrary IR node into ir Constant value First try unwrapping IRNode see already ir Constant Optional step avoids unnecessary inner_fn evaluation isinstance x ir MutableBox get_constant_value x data isinstance x ir BaseView get_constant_value x unwrap_view isinstance x ir Constant x If unwrapped node ir Constant try evaluating inner_fn see returned value ` ops constant ` call isinstance x ir Loops None handler = torch _inductor ops_handler ExtractConstantsHandler x get_device V set_ops_handler handler patch object ir FlexibleLayout allow_indexing True out = x inner_fn x inner_fn_args assert isinstance out torch _inductor virtualized OpsValue isinstance out value ir Constant out value None NOTE prims div maps b C so performs truncation division integer inputs true division floating complex inputs register_lowering prims div broadcast=True div_prim b is_integral = all is_boolean_type x is_integer_type x x b is_integral truncdiv b Disable CPU optimization avoid precision issues see https github com pytorch pytorch issues divisor = get_constant_value b None get_device type = cpu Replace divide constant multiply reciprocal divisor value == reciprocal = math copysign float inf divisor value reciprocal = divisor value mul reciprocal fn args ops truediv args make_pointwise fn b register_lowering aten true_divide aten div Tensor broadcast=True type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT div b b = promote_constants b type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT div_prim b register_lowering aten fmod prims fmod broadcast=True fmod b is_integral = is_boolean_type is_integer_type is_integral fn b ops mod b fn b ops fmod b make_pointwise fn b register_lowering aten sum prims sum sum_ x axis=None keepdims=False dtype=None is_integer_dtype x get_dtype is_boolean_dtype x get_dtype dtype None dtype = torch int fn = make_reduction sum override_return_dtype=dtype fn x axis keepdims dtype=dtype fallback_cumsum = fallback_handler aten cumsum default fallback_cumprod = fallback_handler aten cumprod default fallback_logcumsumexp = fallback_handler aten logcumsumexp default fallback_cummax = fallback_handler aten cummax default fallback_cummin = fallback_handler aten cummin default register_lowering aten cumsum cumsum x axis=None dtype=None is_integer_dtype x get_dtype is_boolean_dtype x get_dtype dtype None dtype = torch int len x get_size == assert axis - dtype = dtype x get_dtype to_dtype x dtype copy=True combine_fn a_tuple b_tuple = a_tuple b = b_tuple ops add b kwargs = _make_scan_inner x axis=axis dtype=dtype result = ir Scan create kwargs combine_fn=combine_fn result None fallback_cumsum x dim=axis dtype=dtype result register_lowering aten cumprod cumprod x axis=None dtype=None is_integer_dtype x get_dtype is_boolean_dtype x get_dtype dtype None dtype = torch int len x get_size == assert axis - dtype = dtype x get_dtype to_dtype x dtype copy=True combine_fn a_tuple b_tuple = a_tuple b = b_tuple ops mul b kwargs = _make_scan_inner x axis=axis dtype=dtype result = ir Scan create kwargs combine_fn=combine_fn result None fallback_cumprod x dim=axis dtype=dtype result register_lowering aten logcumsumexp logcumsumexp x dim log_add_exp_helper a_tuple b_tuple = a_tuple b = b_tuple min_v = ops minimum b max_v = ops maximum b mask = min_v = max_v &#124; ~ops isinf min_v ops where mask ops log p ops exp min_v - max_v + max_v dtype = x get_dtype len x get_size == assert dim - clone x kwargs = _make_scan_inner x axis=dim dtype=dtype result = ir Scan create kwargs combine_fn=log_add_exp_helper result None fallback_logcumsumexp x dim=dim result register_lowering aten cummax type_promotion_kind=None cummax x axis=None len x get_size == assert axis - clone x empty_like x dtype=torch int dtype = x get_dtype combine_fn = ir get_reduction_combine_fn argmax dtype=dtype arg_break_ties_left=False kwargs = _make_scan_inner x axis=axis dtype=dtype kwargs dtypes = dtype torch int kwargs inner_fns = x make_loader lambda idx ops index_expr idx axis torch int values indices = ir Scan create kwargs combine_fn=combine_fn type ignore arg-type values None fallback_cummax x dim=axis values indices register_lowering aten cummin type_promotion_kind=None cummin x axis=None len x get_size == assert axis - clone x empty_like x dtype=torch int dtype = x get_dtype combine_fn = ir get_reduction_combine_fn argmin dtype=dtype arg_break_ties_left=False kwargs = _make_scan_inner x axis=axis dtype=dtype kwargs dtypes = dtype torch int kwargs inner_fns = x make_loader lambda idx ops index_expr idx axis torch int values indices = ir Scan create kwargs combine_fn=combine_fn type ignore arg-type values None fallback_cummin x dim=axis values indices register_lowering aten prod prod x axis=None keepdims=False dtype=None is_integer_dtype x get_dtype is_boolean_dtype x get_dtype dtype None dtype = torch int fn = make_reduction prod override_return_dtype=dtype fn x axis keepdims dtype=dtype register_lowering aten any reduce_any x dim=None keepdim=False x = to_dtype x torch bool make_reduction any x axis=dim keepdims=keepdim register_lowering aten max type_promotion_kind=None reduce_max x dim=None keepdim=False dim None reduce_amax x axis=dim keepdims=keepdim reduce_argmax x axis=dim keepdims=keepdim reduce_amax x axis=None keepdims=keepdim register_lowering aten min type_promotion_kind=None reduce_min x dim=None keepdim=False dim None reduce_amin x axis=dim keepdims=keepdim reduce_argmin x axis=dim keepdims=keepdim reduce_amin x axis=None keepdims=keepdim register_lowering prims xor_sum make_reduction xor_sum reduce_amax = register_lowering aten amax make_reduction max reduce_amin = register_lowering aten amin make_reduction min reduce_argmax = register_lowering aten argmax make_reduction argmax override_return_dtype=torch int reduce_argmin = register_lowering aten argmin make_reduction argmin override_return_dtype=torch int add = register_pointwise aten add allow_alpha=True override_fn_when_input_bool= logical_or sort_fallback = fallback_handler aten sort stable add_to_fallback_set=False register_lowering aten sort stable type_promotion_kind=None sort_stable x stable=None dim=- descending=False stable None stable = False shape = x get_size device = x get_device dim = canonicalize_dim len shape dim len shape == clone x _full device torch int shape dim_size = shape dim len shape V graph sizevars statically_known_lt dim_size torch iinfo torch int max sort_fallback x stable=stable dim=dim descending=descending indices = iota dim_size start= step= dtype=torch int device=device requires_grad=False view_shape = len shape len shape view_shape dim = dim_size indices = view indices view_shape indices = expand indices shape values indices = ir Sort create device=device dtypes= x dtype indices dtype inner_fns= x make_loader indices make_loader size=shape axis=dim stable=stable descending=descending values None sort_fallback x stable=stable dim=dim descending=descending assert indices None values to_dtype indices torch int register_lowering aten sort default type_promotion_kind=None sort x dim=- descending=False sort_stable x stable=False dim=dim descending=descending register_pointwise_numeric op name=None triton_fallback=None register_pointwise op name=name type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT triton_fallback=triton_fallback register_pointwise_numeric_ldf op torch _ops OpOverloadPacket register_op_requires_libdevice_fp op __name__ register_pointwise op type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT rsqrt = register_pointwise_numeric aten rsqrt exp = register_pointwise_numeric_ldf aten exp exp = register_pointwise_numeric aten exp expm = register_pointwise_numeric aten expm relu = register_pointwise aten relu sigmoid = register_pointwise_numeric_ldf aten sigmoid sqrt = register_pointwise_numeric_ldf aten sqrt square = register_pointwise aten square sub = register_pointwise aten sub allow_alpha=True register_pointwise_numeric_ldf aten cos register_pointwise_numeric_ldf aten sin abs = register_pointwise aten abs bitwise_and = register_pointwise aten bitwise_and bitwise_left_shift = register_pointwise aten bitwise_left_shift bitwise_not = register_pointwise aten bitwise_not override_fn_when_input_bool= logical_not bitwise_or = register_pointwise aten bitwise_or bitwise_right_shift = register_pointwise aten bitwise_right_shift bitwise_xor = register_pointwise aten bitwise_xor register_pointwise_numeric aten lgamma erf = register_pointwise_numeric aten erf register_lowering aten special_erf type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT erf register_pointwise_numeric aten log p register_pointwise_numeric aten tan register_pointwise_numeric aten tanh register_pointwise_numeric_ldf aten log logical_and = register_pointwise aten logical_and type_promotion_kind=None convert_input_to_bool=True override_return_dtype=torch bool logical_not = register_pointwise aten logical_not type_promotion_kind=None convert_input_to_bool=True override_return_dtype=torch bool logical_or = register_pointwise aten logical_or type_promotion_kind=None convert_input_to_bool=True override_return_dtype=torch bool logical_xor = register_pointwise aten logical_xor type_promotion_kind=None convert_input_to_bool=True override_return_dtype=torch bool maximum = register_pointwise aten maximum minimum = register_pointwise aten minimum register_lowering aten clamp_min maximum register_lowering aten clamp_max minimum neg = register_pointwise aten neg abs = register_pointwise aten abs reciprocal = register_pointwise_numeric aten reciprocal register_pointwise aten remainder sign = register_pointwise aten sign override_fn_when_input_bool= identity register_pointwise aten ceil register_pointwise aten signbit override_return_dtype=torch bool register_lowering aten _neg_view neg register_pointwise aten le override_return_dtype=torch bool register_pointwise aten lt override_return_dtype=torch bool register_pointwise aten ge override_return_dtype=torch bool gt = register_pointwise aten gt override_return_dtype=torch bool register_pointwise aten eq override_return_dtype=torch bool register_pointwise aten ne override_return_dtype=torch bool register_pointwise_numeric aten cosh register_pointwise_numeric aten sinh register_pointwise_numeric aten acos register_pointwise_numeric aten acosh register_pointwise_numeric aten asin register_pointwise_numeric aten asinh register_pointwise_numeric aten atan register_pointwise_numeric aten atan register_pointwise_numeric aten atanh register_pointwise_numeric aten copysign register_pointwise_numeric aten erfc register_pointwise_numeric aten erfinv register_pointwise_numeric aten hypot register_pointwise_numeric aten log register_pointwise_numeric aten log register_pointwise_numeric aten nextafter codegen common BackendFeature pointwise_overrides_data _get_pointwise_overrides ns name data = pointwise_overrides_data name op = getattr ns data name None op None make_triton_fallback op data triton None fallback_handler op isinstance op torch _ops OpOverloadPacket olname op overloads ol = getattr op olname yield ol data type_promotion_kind make_triton_fallback ol yield op data type_promotion_kind make_triton_fallback op name pointwise_overrides_data op type_promotion_kind triton_fallback _get_pointwise_overrides aten name register_pointwise op name=name type_promotion_kind=type_promotion_kind triton_fallback=triton_fallback op type_promotion_kind triton_fallback _get_pointwise_overrides prims name register_pointwise op name=name type_promotion_kind=type_promotion_kind triton_fallback=triton_fallback foreach_add_list = register_foreach_pointwise aten _foreach_add List add allow_alpha=True foreach_add_scalar = register_foreach_pointwise aten _foreach_add Scalar add allow_alpha=True register_foreach_pointwise aten _foreach_add Tensor add allow_alpha=True foreach_mul_list = register_foreach_pointwise aten _foreach_mul List mul register_foreach_pointwise aten _foreach_mul Tensor mul foreach_mul_scalar = register_foreach_pointwise aten _foreach_mul Scalar mul register_foreach_pointwise aten _foreach_sub List sub register_foreach_pointwise aten _foreach_sub Scalar sub register_foreach_pointwise aten _foreach_neg default neg register_foreach_pointwise aten _foreach_abs default abs register_foreach_pointwise aten _foreach_pow Scalar pow register_foreach_pointwise aten _foreach_pow List pow register_foreach_pointwise aten _foreach_pow ScalarAndTensor pow foreach_div_list = register_foreach_pointwise aten _foreach_div List div register_foreach_pointwise aten _foreach_div Tensor div foreach_div_scalar = register_foreach_pointwise aten _foreach_div Scalar div register_foreach_pointwise aten _foreach_sqrt sqrt register_foreach_pointwise aten _foreach_rsqrt rsqrt register_foreach_pointwise aten _foreach_maximum List maximum register_foreach_pointwise aten _foreach_maximum Scalar maximum register_foreach_pointwise aten _foreach_minimum List minimum register_foreach_pointwise aten _foreach_minimum Scalar minimum register_foreach_pointwise aten _foreach_clamp_min List maximum register_foreach_pointwise aten _foreach_clamp_min Scalar maximum register_foreach_pointwise aten _foreach_clamp_max List minimum register_foreach_pointwise aten _foreach_clamp_max Scalar minimum register_foreach_pointwise aten _foreach_reciprocal reciprocal register_foreach_pointwise aten _foreach_sign sign foreach_copy = register_foreach_pointwise aten _foreach_copy copy these only encountered outputs graph reinplacing epilogue copies improves compile time removing extra buffers sent scheduler register_foreach_inplace aten_op outplace_aten_op outplace_op inplaceable_foreach_ops outplace_aten_op = aten_op inplace_foreach_ops add aten_op fn args kwargs results = outplace_op args kwargs mut_results = arg result zip args results mut_results append mutate_to arg result unsafe_alias=True mut_results _register_foreach_lowering aten_op fn register_foreach_inplace aten _foreach_add_ List aten _foreach_add List foreach_add_list register_foreach_inplace aten _foreach_add_ Scalar aten _foreach_add Scalar foreach_add_scalar register_foreach_inplace aten _foreach_mul_ List aten _foreach_mul List foreach_mul_list register_foreach_inplace aten _foreach_mul_ Scalar aten _foreach_mul Scalar foreach_mul_scalar register_foreach_inplace aten _foreach_div_ List aten _foreach_div List foreach_div_list register_foreach_inplace aten _foreach_div_ Scalar aten _foreach_div Scalar foreach_div_scalar register_foreach_inplace aten _foreach_copy_ default aten _foreach_copy default foreach_copy register_inplace aten_op outplace_op register_lowering aten_op type_promotion_kind=None fn args kwargs result = outplace_op args kwargs result = to_dtype result args get_dtype mutate_to args result fn register_inplace aten add_ add register_inplace aten bitwise_and_ bitwise_and register_inplace aten bitwise_left_shift_ bitwise_left_shift register_inplace aten bitwise_not_ bitwise_not register_inplace aten bitwise_or_ bitwise_or register_inplace aten bitwise_right_shift_ bitwise_right_shift register_inplace aten bitwise_xor_ bitwise_xor register_inplace aten mul_ mul register_inplace aten div_ Tensor div register_inplace aten div_ Tensor_mode div_mode register_inplace aten logical_and_ logical_and register_inplace aten logical_not_ logical_not register_inplace aten logical_or_ logical_or register_inplace aten logical_xor_ logical_xor register_inplace aten sub_ sub register_inplace aten relu_ relu register_inplace aten sigmoid_ sigmoid register_lowering aten __and__ bitwise_and register_lowering aten __lshift__ bitwise_left_shift register_lowering aten __or__ bitwise_or register_lowering aten __rshift__ bitwise_right_shift register_lowering aten __xor__ bitwise_xor register_inplace aten __iand__ aten __and__ register_inplace aten __ilshift__ aten __lshift__ register_inplace aten __ior__ aten __or__ register_inplace aten __irshift__ aten __rshift__ register_inplace aten __ixor__ aten __xor__ register_lowering aten sym_constrain_range sym_constrain_range min=None max=None None register_lowering aten sym_size int sym_size dim val = V graph current_node meta val Note Can val int ~~~~~~~~~~~~~~~~~~~~~~~~~ In principle someone could construct FX graph where call size stride has val plain int SymInt However we will maintain invariant possible you constructing FX graph where there call size stride returns int you KNOW int must always constant then you do need trace call all just constant propagate integer assert isinstance val torch SymInt f Expect val torch SymInt got val= val val node expr register_lowering aten sym_stride int sym_stride dim val = V graph current_node meta val See Note Can val int assert isinstance val torch SymInt f Expect val torch SymInt got val= val val node expr register_lowering aten sym_numel sym_numel get_numel method func magic_methods items register_lowering method_to_operator method func type ignore arg-type register_lowering torch sym_sum sym_sum args sympy Add args register_lowering aten _foobar foobar args kwargs raise NotImplementedError Helpful debugging register_lowering torch ops _inductor_test realize _realize x x realize clone x register_lowering torch ops inductor resize_storage_bytes_ resize_storage_bytes_ variable new_size variable realize ir ResizeStorageBytes variable new_size variable register_lowering torch ops aten set_ source_Tensor set__source_tensor source_tensor realize source_tensor realize TensorBox create ir SetSourceTensorKernel source_tensor hasattr torch ops fsdp copy_ register_lowering torch ops fsdp copy_ default fsdp_copy_ dst src dst src dst copy_ dst can happen reinplacing pass dst src = to_device src dst get_device src = to_dtype src dst get_dtype src = expand src dst get_size mutate_to dst src register_lowering torch ops aten resize resize x size memory_format=None assert isinstance x TensorBox assert isinstance size list tuple memory_format None memory_format = torch contiguous_format memory_format == torch preserve_format raise RuntimeError f unsupported memory format memory_format memory_format == torch channels_last assert len size == memory_format == torch channels_last_ d assert len size == old_numel = x get_numel dtype = x get_dtype device = x get_device_or_error isinstance x data ir BaseView x data = x data unwrap_view torch are_deterministic_algorithms_enabled torch utils deterministic fill_uninitialized_memory type ignore attr-defined is_float_dtype dtype uninitialized_val = float nan is_integer_dtype dtype uninitialized_val = torch iinfo dtype max uninitialized_val = True using zero what empty does uninitialized_val = V graph sizevars statically_known_equals old_numel type ignore arg-type full size uninitialized_val dtype=dtype device=device x_flat = as_strided x old_numel flat_loader = x_flat make_loader out_stride = ir FlexibleLayout stride_ordered_for_memory_format size memory_format out_indexer = ir FixedLayout device dtype size out_stride make_indexer inner_fn idx flat_index = out_indexer idx flat_index_expr = ops index_expr flat_index torch int limit = ops index_expr old_numel torch int mask = ops lt flat_index_expr limit ops masked mask lambda flat_loader flat_index uninitialized_val out = Pointwise create device=device dtype=dtype inner_fn=inner_fn ranges=list size out torch _higher_order_ops auto_functionalize auto_functionalized make_fallback auto_functionalized register_lowering triton_kernel_wrapper_mutation triton_kernel_wrap_ kernel_idx constant_args_idx grid tma_descriptor_metadata kwargs torch _higher_order_ops triton_kernel_wrap kernel_side_table constant_args = kernel_side_table get_constant_args constant_args_idx ir UserDefinedTritonKernel kernel_idx=kernel_idx grid=grid tma_descriptor_metadata=tma_descriptor_metadata kernel_args= kwargs constant_args key val key val kwargs items isinstance val TensorBox register_lowering torch ops higher_order cond type_promotion_kind=None cond pred true_fn false_fn operands any isinstance x IRNode is_triton x x pred operands msg = control flow operator torch cond stack_trace = V graph current_node meta get stack_trace None msg = f msg Found \n stack_trace V graph disable_cudagraphs_reason = msg result = ir Conditional create pred true_fn false_fn operands list map TensorBox create result register_lowering torch ops higher_order while_loop type_promotion_kind=None while_loop cond_fn body_fn carried_inputs additional_inputs stack_output=False any isinstance x IRNode is_triton x x carried_inputs + additional_inputs msg = control flow operator torch while_loop stack_trace = V graph current_node meta get stack_trace None msg = f msg Found \n stack_trace V graph disable_cudagraphs_reason = msg result = ir WhileLoop create cond_fn body_fn carried_inputs additional_inputs stack_output assert isinstance result Sequence list map ir WhileLoop _maybe_wrap_as_tensor_box result register_lowering torch ops higher_order while_loop_stack_output type_promotion_kind=None functools partial while_loop stack_output=True register_lowering torch ops higher_order invoke_subgraph type_promotion_kind=None invoke_subgraph subgraph_fn ir Subgraph identifier str operands result = ir InvokeSubgraph create subgraph_fn operands list map TensorBox create result type ignore call-overload Import control_deps_op HOP lowering torch _inductor fx_passes control_dependencies control_deps register_lowering control_deps type_promotion_kind=None control_deps_op_lowering additional_deps subgraph_fn args Lower control_deps_op ensuring dependencies realized tracking them The control_deps_op HOP makes dependencies explicit graph During lowering Realize all additional dependencies ensure they re computed Execute target operation normally Track dependencies scheduler Realize all additional dependencies dep_names = dep additional_deps isinstance dep IRNode continue dep realize dep_names append dep get_name original_args = V graph current_node args arg_offset = first two args additional_deps subgraph assert len args + arg_offset == len original_args output = None operation_len = len V graph operations assert len subgraph_fn graph_module graph find_nodes op= placeholder == len args i node enumerate subgraph_fn graph_module graph nodes node op == placeholder assert node V graph env V graph env node = args i continue node op == output args kwargs = V graph fetch_args_kwargs_from_env node output = torch fx Interpreter output V graph node args kwargs assert node V graph env V graph env node = V graph run_node node assert output None additional_deps some operators like wait_tensor just their input so its more robust add dep operation itself otherwise you can have cycle = coll b = control_deps mm c = control_deps b wait c == then you have cycle op V graph operations operation_len dep_name dep_names op_name = op operation_name assert op_name None V graph additional_buffer_deps op_name add dep_name output register_lowering torch _higher_order_ops invoke_quant type_promotion_kind=None invoke_quant_tracer subgraph_fn ir Subgraph operands scheme=None output = None quant_options = V graph current_node meta get quant_options None assert quant_options None i node enumerate subgraph_fn graph_module graph nodes node op == placeholder V graph env node = operands i continue todo getattr node op == output args kwargs = V graph fetch_args_kwargs_from_env node v itertools chain args kwargs values v realize quant_options codegen_low_precision V graph low_precision_codegen_ops add v get_operation_name V graph invoke_quant_ops add v get_operation_name output = torch fx Interpreter output V graph node args kwargs V graph env node = V graph run_node node output register_lowering associative_scan_op type_promotion_kind=None associative_scan combine_fn ir Subgraph xs additional_inputs tuple torch Tensor subgraph_lowering InputDescriptor lower_pointwise_subgraph len additional_inputs raise RuntimeError Unable generate code associative_scan op because there lifted arguments subgraph_inputs = InputDescriptor dtype=x get_dtype device=x get_device x itertools chain xs xs lowered_combine_fn = lower_pointwise_subgraph combine_fn subgraph_inputs type ignore var-annotated wrapped_combine_fn lhs rhs lowered_combine_fn pytree tree_leaves lhs pytree tree_leaves rhs kwargs = _make_scan_inner xs axis= dtype=None kwargs dtypes = tuple x get_dtype x xs kwargs inner_fns = tuple x make_loader x xs result = ir Scan create combine_fn=wrapped_combine_fn can_fallback_to_aten=False kwargs result None raise RuntimeError Unable generate code associative_scan op result register_lowering torch ops prims _sink_tokens default _sink_tokens tokens None register_lowering torch ops higher_order with_effects type_promotion_kind=None with_effects token op args kwargs result = ir EffectfulKernel create op args kwargs torch _higher_order_ops effects get_effect_key effect_type = get_effect_key op args kwargs assert effect_type None effectful_kernel = V graph effectful_ops effect_type result None effectful_kernel result = pytree tree_map_only ir MultiOutput TensorBox create result See NOTE with_effects type Only ` result ` tuple list isinstance result tuple effectful_kernel result effectful_kernel result comm_lowering register_comm_lowerings register_comm_lowerings register_lowering inductor_prims prepare_softmax_online type_promotion_kind=None prepare_softmax_online x dim Lowering inductor_prims prepare_softmax_online compute max sum one pass no split needed kwargs = _make_reduction_inner x axis=dim keepdims=True dtype=None override_return_dtype=None reduction_ranges = kwargs reduction_ranges rnumel = V graph sizevars simplify sympy_product reduction_ranges hint num_split = ir Reduction num_splits kwargs reduction_type= online_softmax_reduce type ignore arg-type reduction_numel=rnumel num_split == V graph sizevars statically_known_geq rnumel config unroll_reductions_threshold max_tensor sum_tensor = OnlineSoftmaxReduction create input_node=x num_output= reduction_hint=hint kwargs max_tensor sum_tensor Note Split online_softmax_reduce We don t split reduction online_softmax_reduce now On one hand supporting split reduction makes things complex since split out reuctions requires inputs rather than one On other hand during training online_softmax_reduce should usually don t requires split due large batch size more specifically batch size times sequence length We should support split reduction we find legit use cases motivate work TODO does inference need split online_softmax_reduce warnings warn textwrap dedent Online softmax disabled fly since Inductor decides split reduction Cut issue PyTorch important use case you want speed up online softmax amax = reduce_amax x dim keepdims=True exp = lowerings aten exp sub x amax xsum = sum_ exp dim keepdims=True amax xsum populate lowerings defined kernel kernel import_submodule kernel quantized_lowerings quantized_lowerings register_quantized_ops quantized_lowerings register_woq_mm_ops mkldnn_lowerings mkldnn_lowerings register_onednn_fusion_ops jagged_lowerings jagged_lowerings register_jagged_ops contextlib contextmanager force_fallback op torch _ops OpOverload A context manager force fallback op Used unit test FallbackKernel assert isinstance op torch _ops OpOverload Only OpOverload make clean up easier old_handler = lowerings get op try register_lowering op fallback_handler op yield finally old_handler lowerings op = old_handler lowerings pop op