__future__ annotations gzip io json math os time zipfile functools lru_cache pathlib Path typing Any cast Optional TYPE_CHECKING boto type ignore requests TYPE_CHECKING collections abc Callable PYTORCH_REPO = https api github com repos pytorch pytorch lru_cache get_s _resource - Any boto resource s GHA_ARTIFACTS_BUCKET = gha-artifacts NB In CI flaky test usually retried times then test file would rerun more times MAX_RETRY_IN_NON_DISABLED_MODE = _get_request_headers - dict str str Accept application vnd github v +json Authorization token + os environ GITHUB_TOKEN _get_artifact_urls prefix str workflow_run_id int - dict Path str Get all workflow artifacts test-report name response = requests get f PYTORCH_REPO actions runs workflow_run_id artifacts per_page= headers=_get_request_headers artifacts = response json artifacts while next response links keys response = requests get response links next url headers=_get_request_headers artifacts extend response json artifacts artifact_urls = artifact artifacts artifact name startswith prefix artifact_urls Path artifact name = artifact archive_download_url artifact_urls _download_artifact artifact_name Path artifact_url str workflow_run_attempt int - Path Artifact run attempt All artifacts workflow share single namespace However we can re-run workflow produce new set artifacts To avoid name collisions we add ` -runattempt run - ` somewhere artifact name This code parses out run attempt number artifact name If doesn t match one specified command line skip atoms = str artifact_name split - atom atoms atom startswith runattempt found_run_attempt = int atom len runattempt workflow_run_attempt = found_run_attempt print f Skipping artifact_name invalid run attempt f Expected workflow_run_attempt found found_run_attempt print f Downloading artifact_name response = requests get artifact_url headers=_get_request_headers open artifact_name wb f f write response content artifact_name download_s _artifacts prefix str workflow_run_id int workflow_run_attempt int job_id Optional int = None - list Path bucket = get_s _resource Bucket GHA_ARTIFACTS_BUCKET objs = bucket objects filter Prefix=f pytorch pytorch workflow_run_id workflow_run_attempt artifact prefix found_one = False paths = obj objs object_name = Path obj key name target artifact specific job_id provided otherwise skip download job_id None str job_id object_name continue found_one = True p = Path Path obj key name print f Downloading p open p wb f f write obj get Body read paths append p found_one print warning title=s artifacts found Didn t find any test reports s there might bug paths download_gha_artifacts prefix str workflow_run_id int workflow_run_attempt int - list Path artifact_urls = _get_artifact_urls prefix workflow_run_id paths = name url artifact_urls items paths append _download_artifact Path name url workflow_run_attempt paths upload_to_dynamodb dynamodb_table str repo str docs list Any generate_partition_key Optional Callable str dict str Any str - None print f Writing len docs documents DynamoDB dynamodb_table https boto amazonaws com v documentation api latest guide dynamodb html#batch-writing boto resource dynamodb Table dynamodb_table batch_writer batch doc docs generate_partition_key doc dynamoKey = generate_partition_key repo doc This move away _event_time field Rockset which we cannot use when reimport data doc timestamp = int round time time batch put_item Item=doc upload_to_s bucket_name str key str docs list dict str Any - None print f Writing len docs documents S bucket_name key body = io StringIO doc docs json dump doc body body write \n get_s _resource Object f bucket_name f key put Body=gzip compress body getvalue encode ContentEncoding= gzip ContentType= application json print f Done Finish writing document S bucket_name key read_from_s bucket_name str key str - list dict str Any print f Reading s bucket_name key body = get_s _resource Object f bucket_name f key get Body read results = gzip decompress body decode split \n json loads result result results result remove_nan_inf old Any - Any Casta NaN inf -inf string float since json dumps outputs invalid json them _helper o Any - Any isinstance o float math isinf o math isnan o str o isinstance o list _helper v v o isinstance o dict _helper k _helper v k v o items isinstance o tuple tuple _helper v v o o _helper old upload_workflow_stats_to_s workflow_run_id int workflow_run_attempt int collection str docs list dict str Any - None bucket_name = ossci-raw-job-status key = f collection workflow_run_id workflow_run_attempt upload_to_s bucket_name key docs upload_file_to_s file_name str bucket str key str - None Upload local file S print f Upload file_name s bucket key boto client s upload_file file_name bucket key unzip p Path - None Unzip provided zipfile similarly-named directory Returns None ` p ` zipfile Looks like tmp test-reports zip - tmp unzipped-test-reports assert p is_file unzipped_dir = p with_name unzipped- + p stem print f Extracting p unzipped_dir zipfile ZipFile p r zip zip extractall unzipped_dir is_rerun_disabled_tests report Path workflow_run_id int workflow_run_attempt int tests dict str dict str int - bool Check test report coming rerun_disabled_tests workflow where each test run multiple times all t get num_green + t get num_red MAX_RETRY_IN_NON_DISABLED_MODE t tests values True job_id = get_job_id report job_name = get_job_name job_id workflow_run_id workflow_run_attempt job_name None rerun_disabled_tests job_name get_job_id report Path - int &#124; None Job id artifacts Retrieve job id report path In our GHA workflows we append job id end report name so ` report ` looks like unzipped-test-reports-foo_ test test-reports foo TEST-foo xml we want get ` ` out try int report parts rpartition _ except ValueError None lru_cache get_job_name id int &#124; None workflow_id int &#124; None workflow_run_attempt int &#124; None - str &#124; None id None None try workflow_id None response = requests get f PYTORCH_REPO actions jobs id headers=_get_request_headers response status_code = None cast str response json name lru_cache _get_jobs workflow_id int - dict int str jobs dict int str = Paginate page = while True response = requests get f PYTORCH_REPO actions runs workflow_id attempts workflow_run_attempt jobs headers=_get_request_headers params= page page per_page response status_code = jobs job response json jobs jobs job id = job name next response links break page += jobs jobs = _get_jobs workflow_id jobs id except Exception None