Owner s module inductor os sys unittest torch torch nn torch _dynamo utils same torch _inductor test_case run_tests TestCase torch _inductor utils run_and_get_code torch testing FileCheck torch testing _internal common_utils serialTest torch testing _internal inductor_utils GPU_TYPE HAS_GPU requires_cuda_with_enough_memory Make helper files test importable pytorch_test_dir = os path dirname os path dirname os path realpath __file__ sys path append pytorch_test_dir TODO move check_model common module since s quite often used new test cases inductor test_torchinductor check_model torch _dynamo testing rand_strided torch _inductor config inductor_config aten = torch ops aten num_inplace_padding torch _dynamo utils counters counters inductor inplace_padding enable_inplace_padding = True os environ get TORCHINDUCTOR_INPLACE_PADDING None enable_inplace_padding = os environ get TORCHINDUCTOR_INPLACE_PADDING == DO_PERF_TEST = os environ get DO_PERF_TEST == inductor_config patch inplace_padding=enable_inplace_padding InplacePaddingTest TestCase test_skip_pad_due_to_fusion If padding can fused downstream op there would little benefit do inplace padding f x x = aten constant_pad_nd x x sum dim=- M N = x = rand_strided M N N + device=GPU_TYPE check_model f x atol= e- rtol= e- assertEqual num_inplace_padding test_skip_pad_input Don t apply padding graph input since Inductor does allocatae input can guarantee enough trailing space padding f x y x = aten constant_pad_nd x x y M N = x = rand_strided M N N + device=GPU_TYPE y = torch randn N + M device=GPU_TYPE check_model f x y atol= e- rtol= e- assertEqual num_inplace_padding test_pad_non_zero f x x = x + x = aten constant_pad_nd x x x odd shape purpose pad intermediate buffer s strides x = torch randn device=GPU_TYPE ref = f x act code = run_and_get_code torch compile f x When we allocate x tensor output x + Instead doing empty_strided_cuda torch float note stride already padded We do empty_strided_cuda torch float as_strided This will allocate extra item last row so inplace padding would safe without accessing out bound memory FileCheck check_regex r empty_strided \ \ \ \ \ torch float \ r as_strided\ \ \ \ \ \ run code assertTrue torch allclose ref act atol= e- rtol= e- assertEqual num_inplace_padding inductor_config patch cpp_wrapper=True test_pad_non_zero_cpp_wrapper f x x = x + x = aten constant_pad_nd x x x odd shape purpose pad intermediate buffer s strides x = torch randn device=GPU_TYPE ref = f x torch _inductor codegen cpp_wrapper_gpu CppWrapperGpu orig_generate_and_run_autotune_block = CppWrapperGpu generate_and_run_autotune_block compile_time_autotune_called = False mock_generate_and_run_autotune_block wrapper nonlocal compile_time_autotune_called compile_time_autotune_called = True out = orig_generate_and_run_autotune_block wrapper call_code = wrapper kernel_autotune_calls getvalue FileCheck check f buf = generate_example_value GPU_TYPE torch float run call_code out unittest mock patch object CppWrapperGpu generate_and_run_autotune_block mock_generate_and_run_autotune_block act code = run_and_get_code torch compile f x Buf should over-allocated then strided FileCheck check_regex r aoti_torch_as_strided\ buf _handle buf _handle_restrided\ run code assertTrue torch allclose ref act atol= e- rtol= e- assertEqual num_inplace_padding assertTrue compile_time_autotune_called test_pad_too_large f x y x = aten constant_pad_nd x x y M N = x = rand_strided M N N + device=GPU_TYPE y = torch randn N + M device=GPU_TYPE check_model f x y atol= e- rtol= e- assertEqual num_inplace_padding inductor_config patch can_inplace_pad_graph_input=True test_mutating_padding_input Even ` aten constant_pad_nd ` input get inplace updated doing inplace-padding still generates correct result f x y x = aten constant_pad_nd x x add_ x y M N = x = rand_strided M N + N + device=GPU_TYPE as_strided M N N + y = torch randn N + M device=GPU_TYPE check_model f x y atol= e- rtol= e- assertEqual num_inplace_padding test_mutating_padding_output Inplace padding does take effect since ` aten add_ ` op cause user padding output matmul We skip inplace-padding case f x y x = aten constant_pad_nd x x add_ x y M N = x = rand_strided M N N + device=GPU_TYPE y = torch randn N + M device=GPU_TYPE e- tolerance may fail CI A G GPU check_model f x y atol= e- rtol= e- assertEqual num_inplace_padding requires_cuda_with_enough_memory e inductor_config patch force_shape_pad=True serialTest test_linear_and_cel Use nan torch empty torch use_deterministic_algorithms True torch utils deterministic fill_uninitialized_empty = True os environ CUBLAS_WORKSPACE_CONFIG = B T C V = linear = nn Linear C V bfloat device=GPU_TYPE ce = torch nn CrossEntropyLoss f x y x grad = None linear weight grad = None linear bias grad = None loss = ce linear x y loss backward loss x = torch randn B T C requires_grad=True GPU_TYPE bfloat x retain_grad y = torch randint V B T GPU_TYPE opt_f = torch compile f expect = f x y x grad linear weight grad linear bias grad actual = opt_f x y x grad linear weight grad linear bias grad assert same expect actual tol= e- f ref \n expect \nact \n actual We may disable inplace_padding via env-var test perf assertEqual num_inplace_padding int inductor_config inplace_padding DO_PERF_TEST triton testing do_bench ms = do_bench lambda opt_f x y print f inductor_config inplace_padding= ms= f Enable Max-Autotune repro test failure https github com pytorch pytorch pull #issuecomment- inductor_config patch max_autotune=True test_linear_and_cel_max_autotune test_linear_and_cel __name__ == __main__ HAS_GPU run_tests