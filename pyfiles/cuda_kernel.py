mypy allow-untyped-defs functools itertools logging collections defaultdict dataclasses dataclass typing Any Callable Literal Optional TYPE_CHECKING Union sympy Expr symbols torch _inductor config config torch dtype torch_dtype torch _inductor codegen cpp_wrapper_cpu CppWrapperCpu torch _inductor scheduler BaseSchedulerNode torch _inductor utils do_bench_using_profiling OrderedSet Placeholder torch utils _sympy value_ranges ValueRanges cutlass_utils DTYPE_TO_CUTLASS_TYPE TYPE_CHECKING cuda_template ArgInfo autotune_process CUDABenchmarkRequest ir Buffer ChoiceCaller CUDATemplateBuffer IRNode Layout PrimitiveInfoType ShapeAsConstantBuffer TensorBox utils sympy_product virtualized V common CSEVariable IndentedBuffer Kernel OpOverrides WorkspaceArg WorkspaceZeroMode cpp_utils CppPrinter DTYPE_TO_CPP TYPE_CHECKING torch _inductor codegen cuda cuda_template CUDATemplate log = logging getLogger __name__ cexpr = CppPrinter doprint _normalize_idx index int total_length int - int index index = index + total_length ValidLayoutSymbols = Literal M N K B lda ldb ldc ldd ValidLayoutAttrs = Literal size stride dataclass frozen=True LayoutArg node IRNode symbol ValidLayoutSymbols attr ValidLayoutAttrs dim int matches node attr dim - bool node == node attr == attr dim == dim CUDAKernel Kernel Baseclass CUDA Cutlass based Kernels overrides = OpOverrides type ignore assignment __init__ args kwargs - None super __init__ args kwargs layout_args dict str list LayoutArg = defaultdict list size_args list Union Expr int = Mapping arg name IRNode named_nodes dict str IRNode = find_symbol node IRNode attr ValidLayoutAttrs dim int - Optional str arg = find_layout_arg node attr dim arg symbol arg None find_layout_arg node IRNode attr ValidLayoutAttrs dim int - Optional LayoutArg matches = arg arg itertools chain from_iterable layout_args values arg matches node attr dim len matches = Verify all matches have same node attribute dimension And they come same node whichever symbol we use fine runtime logic changes would trigger guard first_match = matches all match node == first_match node match attr == first_match attr match dim == first_match dim match matches raise AssertionError All matching layout args should identical first_match None add_layout_arg symbol ValidLayoutSymbols node IRNode attr ValidLayoutAttrs dim int arg = LayoutArg node symbol attr dim layout_args symbol append arg init_layout_args - None X = named_nodes X W = named_nodes W Y = named_nodes Y Bias = named_nodes get Bias None x_mdim = _normalize_idx - len X get_size x_kdim = _normalize_idx - len X get_size w_kdim = _normalize_idx - len W get_size w_ndim = _normalize_idx - len W get_size y_mdim = _normalize_idx - len Y get_size y_ndim = _normalize_idx - len Y get_size add_layout_arg M X size x_mdim add_layout_arg K X size x_kdim add_layout_arg K W size w_kdim add_layout_arg N W size w_ndim add_layout_arg M Y size y_mdim add_layout_arg N Y size y_ndim len X get_size add_layout_arg B X size lda_dim = find_ld_idx X ldb_dim = find_ld_idx W ldc_dim = find_ld_idx Bias Bias None ldd_dim = find_ld_idx Y add_layout_arg lda X stride lda_dim add_layout_arg ldb W stride ldb_dim Bias None ldc_dim None add_layout_arg ldc Bias stride ldc_dim add_layout_arg ldd Y stride ldd_dim get_layout_args - tuple Union Expr int X = named_nodes X W = named_nodes W Y = named_nodes Y Bias = named_nodes get Bias None mdim = _normalize_idx - len X get_size ndim = _normalize_idx - len W get_size kdim = _normalize_idx - len X get_size get_ld node - Union Expr int dim = find_ld_idx node node get_stride dim M = X get_size mdim N = W get_size ndim K = X get_size kdim B = X get_size len X get_size LDA = get_ld X LDB = get_ld W LDC = get_ld Bias Bias LDD = get_ld Y M N K B LDA LDB LDC LDD get_dynamic_shape_args - list Union Expr int get_layout_args size_args get_offset_args - list Expr node get_layout offset node named_nodes values staticmethod find_ld_idx node IRNode - int strides = node get_stride Handle D tensor case V graph sizevars statically_known_equals strides - _normalize_idx - len strides assert V graph sizevars statically_known_equals strides - strides - _normalize_idx - len strides CUDATemplateKernel CUDAKernel Template kernels defined CUDA Cutlass C++ _EXTRA_CPP_ARGS = size_t workspace_size uint _t workspace cudaStream_t stream __init__ kernel_name str runtime_arg_info list ArgInfo runtime_arg_values list Any - None Initializes new instance CUDATemplateKernel Args kernel_name str The name kernel super __init__ kernel_name = kernel_name runtime_arg_info = runtime_arg_info runtime_arg_values = runtime_arg_values check_not_null node IRNode - str Generates code check node null node None size_str = size node - name_str = arg_name node name_str None res = IndentedBuffer initial_indent= res tabwidth = res splice f name_str int _t name_str _size = size_str name_str _size throw std runtime_error input name_str null size res getvalue get_signature - str signature def_kernel inputs list IRNode outputs list IRNode names_str str = input_reorder Optional list int = None - str Hook called template code generate function definition needed args Args inputs List input IRNodes outputs List output IRNodes names_str Comma separated list input + output argument names input_reorder The actual order input nodes e g The template might have input argument defined X W Bias actual input passed into template could Bias X W In case ` input_reorder ` would additional_size_args Additional size arguments epilogue inputs NB name order matters here s used match up offsets names = x strip x names_str strip split len inputs + len outputs = len names raise RuntimeError f len inputs + len outputs = = len names = inputs= outputs= names= input_reorder None assert len inputs == len input_reorder input_reorder = list range len inputs idx input_reorder name = names idx node = inputs idx node None named_nodes name = node args input_buffers node get_name = name free_symbols OrderedSet Expr = OrderedSet name node zip names len inputs len inputs + len outputs outputs node None NB named nodes must populated order names named_nodes name = node args output_buffers node get_name = name name X W Bias Y we handle these symbolic shapes explicitly expr itertools chain node get_size node get_stride isinstance expr Expr s expr free_symbols free_symbols add s type ignore arg-type arg_defs _ = args cpp_argdefs DTYPE_TO_CUTLASS_TYPE init_layout_args size_vars = M N K B lda ldb ldc ldd size_vars extend str s s free_symbols size_args extend free_symbols size_args = f const int s s size_vars offset_args = f const int name _offset name named_nodes keys runtime_arg_decls = join f arg ty arg name arg runtime_arg_info runtime_arg_decls runtime_arg_decls += signature = f int kernel_name join arg_defs + size_args + offset_args \ runtime_arg_decls _EXTRA_CPP_ARGS signature = signature signature call_kernel name str node CUDATemplateBuffer type ignore name-defined - None Generates code call kernel through V graph wrapper_code used within torch _inductor wrapper PythonWrapperCodegen name Name kernel function node The CUDATemplateBuffer node which contains information about kernel s fused epilogue nodes well all required inputs outputs wrapper = V graph wrapper_code arg_types list Any V graph cpp_wrapper Make sure we initialize these kernels since they re exported C-style symbol names assert isinstance wrapper CppWrapperCpu wrapper initialized_kernels name = We always originally initialize name KERNEL_NAME So we we replace real kernel name passed arg function signature = signature replace str Placeholder KERNEL_NAME name _ call_args arg_types = args cpp_argdefs DTYPE_TO_CUTLASS_TYPE _ call_args _ arg_types = args python_argdefs dynamic_shape_args = get_dynamic_shape_args offset_args = get_offset_args call_args extend dynamic_shape_args type ignore arg-type call_args extend offset_args type ignore arg-type arg runtime_arg_values call_args append str arg arg_types extend const int _ dynamic_shape_args arg_types extend const int _ offset_args arg runtime_arg_info arg_types append arg ty dynamo wraps unspec variable d CPU tensor need convert scalar i range len call_args V graph is_unspec_arg call_args i call_args i = call_args i + item isinstance arg_types i torch_dtype call_args i = call_args i V graph cpp_wrapper f c_void_p call_args i data_ptr workspace_size ptr NULL mark call intended retrieving workspace_size workspace_size should have already been retrieved prior call workspace_size here call_args append nullptr V graph cpp_wrapper None V graph cpp_wrapper arg_types append size_t node get_workspace_size ws = WorkspaceArg count=node get_workspace_size device=V graph get_current_device_or_throw zero_mode=WorkspaceZeroMode UNINITIALIZED outer_name=WorkspaceArg unique_name wrapper generate_workspace_allocation ws workspace = str ws outer_name call_args append workspace V graph cpp_wrapper f c_void_p workspace data_ptr ws = None call_args append nullptr V graph cpp_wrapper None V graph cpp_wrapper arg_types append uint _t wrapper generate_kernel_call name call_args triton=False arg_types=arg_types ws wrapper generate_workspace_deallocation ws dtype node IRNode - Optional str Generates code which represents dtype given node node None void DTYPE_TO_CPP get node get_layout dtype cutlass_dtype node IRNode default_dtype= void - Optional str Helper method called into CUTLASSGemmTemplate node None default_dtype torch _inductor codegen cuda cuda_template CUTLASSTemplate CUTLASSTemplate _DTYPE_TO_CUTLASS node get_layout dtype max_valid_index node IRNode default=- Helper method called into CUTLASSGemmTemplate node None default max_valid_offset = i range len node get_size max_valid_offset += node get_size i - node get_stride i max_valid_offset ptr node IRNode - str Generates code which represents pointer given node node None nullptr arg_name = arg_name node arg_name None nullptr f arg_name + arg_name _offset size node IRNode start_index int end_index Optional int = None default_value int = - str Hook called template code get size arg Generates code which represents size given node start_index end_index If node None returns default_value TODO Will add needed args pass dynamic node None str default_value start_index = _normalize_idx start_index len node get_size end_index None end_index = start_index end_index = _normalize_idx end_index len node get_size sizes = find_symbol node size dim=i node get_size i i range start_index end_index + len sizes == str default_value sizes = symbols v isinstance v str v v sizes val = sympy_product sizes val stride node IRNode index int default_value int = - str Hook called template code get stride arg Generates code which represents stride given node index If node None returns default_value TODO Will add needed args pass dynamic node None str default_value index = _normalize_idx index len node get_size index str default_value stride = node get_stride index V graph sizevars statically_known_leq stride str stride find_symbol node stride dim=index str stride batch_stride node IRNode default_value int = - str Hook called template code get batch stride arg Returns batch dim present This method assumes batch stride largest stride node None str default_value len node get_size str default_value batch_stride = node get_stride V graph sizevars statically_known_leq batch_stride str batch_stride format find_symbol node size dim= node get_size find_symbol node size dim= node get_size row_or_column_stride node IRNode default_value int = - str Hook called template code get row column stride arg This required some CUTLASS X APIs If node row_major returns stride - If node column_major returns stride - TODO Will add needed args pass dynamic node None len node get_stride str default_value stride = node get_stride - stride = node get_stride - stride == cexpr rename_indexing stride stride == cexpr rename_indexing stride raise RuntimeError f At least stride should Strides node get_stride = load name str index Expr mode Any = None - CSEVariable Mock load function memory planning optimize allocations properly create_cse_var name bounds=ValueRanges unknown store name str index Expr value Any mode Any = None - None Mock store function memory planning optimize allocations properly store_buffer_names add name CUDATemplateCaller ChoiceCaller CUDATemplateCaller This represents caller CUDA template kernels It subclass ChoiceCaller Attributes name str The name caller category str The category caller bmreq CUDABenchmarkRequest The benchmark request caller template_buffer CUDATemplateBuffer The template buffer caller __init__ name str category str input_nodes list Buffer layout Layout make_kernel_render Callable CUDATemplateBuffer Optional list BaseSchedulerNode tuple CUDATemplateKernel functools partial str bmreq CUDABenchmarkRequest supports_epilogue_fusion bool template CUDATemplate type ignore name-defined info_kwargs Optional dict str Union PrimitiveInfoType list PrimitiveInfoType type ignore type-arg description str - None super __init__ name input_nodes layout description category = category make_kernel_render = make_kernel_render bmreq = bmreq supports_epilogue_fusion = supports_epilogue_fusion template = template info_kwargs = info_kwargs precompile - None assert bmreq None bmreq precompile benchmark args out - float assert bmreq None config profile_bandwidth_with_do_bench_using_profiling algo = bmreq make_run_fn args out=out do_bench_using_profiling algo bmreq benchmark args out=out __str__ - str f CUDATemplateCaller source_file= bmreq source_file call_name - str f cuda_template_kernels name kernel_hash_key - str Return kernel hash key does depend swizzle - join category bmreq hash_key hash_key - str Return kernel hash key does depend swizzle swizzle_str str = str info_kwargs get swizzle isinstance info_kwargs dict None - join category bmreq hash_key swizzle_str info_dict - dict str Union PrimitiveInfoType list PrimitiveInfoType Information returned here logged autotune log file when enabled In general we should avoid calling function expensive compute can add up very fast info_kwargs None op info_kwargs op Any = info_kwargs op backend CUDA op_type type op __name__ op_conf_name str op configuration_name op_arch str op arch tile_shape str op tile_description tile_shape epilogue_schedule str op epilogue_schedule kernel_schedule str op kernel_schedule element_accumulator str op accumulator_type op_name str op procedural_name instruction_shape str op tile_description math_instruction instruction_shape swizzle str info_kwargs swizzle backend CUDA op_type unknown output_node - Union TensorBox ShapeAsConstantBuffer bmreq update_workspace_size TensorBox create CUDATemplateBuffer layout=self layout inputs=self input_nodes make_kernel_render=self make_kernel_render workspace_size=self bmreq workspace_size supports_epilogue_fusion=self supports_epilogue_fusion template=self template