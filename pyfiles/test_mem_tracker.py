Owner s oncall distributed gc unittest torch torch nn nn torch distributed _tools mem_tracker MemTracker torch testing _internal common_utils run_tests skipIfTorchDynamo TEST_CUDA TEST_XPU TestCase torch utils checkpoint checkpoint TestMemTracker TestCase _init_cublas_workspace dev torch device lin = torch nn Linear device=dev inp = torch randn device=dev lin inp sum backward del lin del inp _reset_mem_stats dev torch device mod = torch get_device_module dev mod empty_cache mod reset_accumulated_memory_stats dev mod reset_peak_memory_stats dev skipIfTorchDynamo https github com pytorch pytorch issues unittest skipIf TEST_CUDA TEST_XPU Neither CUDA XPU available test_accelerator_tracker_equivalence Tests tracker correctly calculates peak memory dev = torch device torch accelerator current_device_index _init_cublas_workspace dev gc collect _reset_mem_stats dev mod = torch get_device_module dev mem_stats = mod memory_stats dev pre_acc_active = mem_stats active_bytes all current bsz n_layers dim dtype = torch bfloat DummyModel nn Module __init__ n_layers int dim int dtype torch dtype super __init__ linears = nn ModuleList _ range n_layers linears append nn Linear dim dim dtype=dtype linears append nn ReLU forward x layer linears x = layer x x torch device dev model = DummyModel n_layers dim dtype=dtype optim = torch optim Adam model parameters foreach=True input_batch = torch randn bsz dim device=dev dtype=dtype mem_tracker = MemTracker mem_tracker track_external model optim input_batch mem_tracker mt iter_idx range model input_batch sum backward optim step optim zero_grad iter_idx == mt reset_mod_stats Check accuracy peak memory tracker_max = mt get_tracker_snapshot peak dev Total mem_stats = mod memory_stats dev acc_max = mem_stats active_bytes all peak - pre_acc_active accuracy = tracker_max acc_max assertAlmostEqual accuracy delta= skipIfTorchDynamo https github com pytorch pytorch issues unittest skipIf TEST_CUDA TEST_XPU Neither CUDA XPU available test_tracker_with_activation_checkpointing Tests tracker correctly computes peak memory during activation checkpointing dev = torch device torch accelerator current_device_index _init_cublas_workspace dev gc collect _reset_mem_stats dev mod = torch get_device_module dev mem_stats = mod memory_stats dev pre_acc_active = mem_stats active_bytes all current bsz n_layers dim dtype = torch float MLPBlock nn Module __init__ dim int dtype torch dtype super __init__ mlp_block = nn Sequential nn Linear dim dim dtype=dtype nn ReLU nn Linear dim dim dtype=dtype forward x mlp_block x MyModule nn Module __init__ n_layers int dim int dtype torch dtype use_ac bool = False super __init__ mlp_blocks = nn ModuleList use_ac = use_ac _ range n_layers mlp_blocks append MLPBlock dim dtype=dtype forward x i block enumerate mlp_blocks i = use_ac x = checkpoint block x preserve_rng_state=True use_reentrant=False x = block x x torch device dev model = MyModule n_layers dim dtype True optim = torch optim Adam model parameters foreach=True mem_tracker = MemTracker mem_tracker track_external model optim mem_tracker mt input_batch = torch randn bsz dim dim device=dev dtype=dtype iter_idx range model input_batch sum backward optim step optim zero_grad iter_idx == mt reset_mod_stats Check accuracy peak memory tracker_max = mt get_tracker_snapshot peak dev Total mem_stats = mod memory_stats dev acc_max = mem_stats active_bytes all peak - pre_acc_active accuracy = tracker_max acc_max assertAlmostEqual accuracy delta= skipIfTorchDynamo https github com pytorch pytorch issues test_tracker_attribution Tests tracker correctly categorizes params gradients optimizer states dev = torch device torch get_default_device gc collect bsz n_layers dim dtype = torch float get_param_grad_optstate_actual_bytes model nn Module opt torch optim Optimizer - tuple int int int param_bytes = grad_bytes = opt_state_bytes = param model parameters param device == dev param_bytes += param numel param element_size param grad None param grad device == dev grad_bytes += param grad numel param grad element_size state opt state values v state values isinstance v torch Tensor v device == dev opt_state_bytes += v numel v element_size param_bytes grad_bytes opt_state_bytes get_param_grad_optstate_bytes_from_tracker tracker MemTracker - tuple int int int snapshot = tracker get_tracker_snapshot param_bytes = snapshot dev Parameter grad_bytes = snapshot dev Gradient opt_state_bytes = snapshot dev Optstate param_bytes grad_bytes opt_state_bytes test_attribution_equivalence mt MemTracker model nn Module opt torch optim Optimizer - None actual = get_param_grad_optstate_actual_bytes model opt tracker = get_param_grad_optstate_bytes_from_tracker mt b zip actual tracker == assertEqual b assertAlmostEqual b delta= DummyModel nn Module __init__ n_layers int dim int dtype torch dtype super __init__ MLP_layers = nn ModuleList _ range n_layers MLP_layers extend nn Linear dim dim dtype=dtype nn GELU MLP_layers extend nn Linear dim dim dtype=dtype nn GELU forward x layer MLP_layers x = layer x x torch device dev model = DummyModel n_layers dim dtype=dtype optim = torch optim Adam model parameters foreach=True mem_tracker = MemTracker mem_tracker track_external model optim mem_tracker mt input_batch = torch randn bsz dim device=dev dtype=dtype Before forward Only parameters input allocated test_attribution_equivalence mt model optim output = model input_batch output sum backward After backward Gradients allocated test_attribution_equivalence mt model optim output = None optim step After step Optimizer state allocated test_attribution_equivalence mt model optim optim zero_grad After zero_grad Gradients deallocated test_attribution_equivalence mt model optim __name__ == __main__ run_tests