Owner s module optimizer module LrScheduler ruff noqa F copy math pickle tempfile types warnings functools partial torch torch nn functional F torch nn Parameter torch optim Adam Rprop SGD torch optim lr_scheduler ChainedScheduler ConstantLR CosineAnnealingLR CosineAnnealingWarmRestarts CyclicLR EPOCH_DEPRECATION_WARNING ExponentialLR LambdaLR LinearLR LRScheduler MultiplicativeLR MultiStepLR OneCycleLR PolynomialLR ReduceLROnPlateau SequentialLR StepLR torch optim swa_utils SWALR torch testing _internal common_utils instantiate_parametrized_tests load_tests parametrize skipIfTorchDynamo TestCase load_tests common_utils used automatically filter tests sharding sandcastle This line silences flake warnings load_tests = load_tests noqa PLW TestLRScheduler TestCase SchedulerTestNet torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv = torch nn Conv d forward x conv F relu conv x LambdaLRTestObject __init__ value value = value __call__ epoch value epoch __eq__ other isinstance other __class__ __dict__ == other __dict__ False exact_dtype = True setUp super setUp net = SchedulerTestNet opt = SGD params net conv parameters params net conv parameters lr torch tensor lr= _check_warning_is_epoch_deprecation_warning w num_warnings int = This function swallows epoch deprecation warning which produced when we call ` scheduler step epoch ` some ` None ` value ` epoch ` deprecated function will need removed updated when schedulers no longer accept parameter all assertEqual len w num_warnings warning w assertEqual len warning message args assertEqual warning message args EPOCH_DEPRECATION_WARNING test_error_when_getlr_has_epoch MultiStepLR torch optim lr_scheduler LRScheduler __init__ optimizer gamma milestones last_epoch=- init_lr = group lr group optimizer param_groups gamma = gamma milestones = milestones super __init__ optimizer last_epoch get_lr step global_step = last_epoch gamma_power = + i + i m enumerate milestones global_step = m - init_lr gamma gamma_power init_lr init_lr optimizer = SGD torch rand lr= assertRaises TypeError scheduler = MultiStepLR optimizer gamma= milestones= skipIfTorchDynamo Torchdynamo keeps references optim guards stack graph break frames test_no_cyclic_references gc param = Parameter torch empty optim = SGD param lr= scheduler = LambdaLR optim lambda epoch del scheduler assertTrue len gc get_referrers optim == Optimizer should contain no cyclic references gc collect del optim assertEqual gc collect msg= Optimizer should garbage-collected __del__ skipIfTorchDynamo Torchdynamo keeps references optim guards stack graph break frames test_no_cyclic_references_in_step gc weakref run param = torch empty requires_grad=True optim = SGD params= param lr= scheduler = LambdaLR optim lambda epoch param sum backward optim step scheduler step weakref ref scheduler To ensure there no reference cycles scheduler we need turn off garbage collector Since gc will automatically collect unreachable objects gc disable ref = run assert ref None gc enable restore test_old_pattern_warning epochs = warnings catch_warnings record=True ws warnings simplefilter always allow any warning raised scheduler = StepLR opt gamma= step_size= assertTrue len ws == No warning should raised old_pattern _ range epochs scheduler step opt step assertWarnsRegex UserWarning r how-to-adjust-learning-rate old_pattern test_old_pattern_warning_with_arg epochs = warnings catch_warnings record=True ws warnings simplefilter always allow any warning raised scheduler = StepLR opt gamma= step_size= assertTrue len ws == No warning should raised old_pattern _ range epochs scheduler step opt step assertWarnsRegex UserWarning r how-to-adjust-learning-rate old_pattern test_old_pattern_warning_resuming epochs = group opt param_groups group initial_lr = warnings catch_warnings record=True ws warnings simplefilter always allow any warning raised scheduler = StepLR opt gamma= step_size= last_epoch= assertTrue len ws == No warning should raised old_pattern _ range epochs scheduler step opt step assertWarnsRegex UserWarning r how-to-adjust-learning-rate old_pattern test_old_pattern_warning_resuming_with_arg epochs = group opt param_groups group initial_lr = warnings catch_warnings record=True ws warnings simplefilter always allow any warning raised scheduler = StepLR opt gamma= step_size= last_epoch= assertTrue len ws == No warning should raised old_pattern _ range epochs scheduler step opt step assertWarnsRegex UserWarning r how-to-adjust-learning-rate old_pattern test_old_pattern_warning_with_overridden_optim_step epochs = group opt param_groups group initial_lr = warnings catch_warnings record=True ws warnings simplefilter always allow any warning raised scheduler = StepLR opt gamma= step_size= last_epoch= assertTrue len ws == No warning should raised emulate use-case optimizer step overridden types old_step = opt step new_step o args kwargs retval = old_step args kwargs retval opt step = types MethodType new_step opt old_pattern _ range epochs scheduler step opt step assertWarnsRegex UserWarning r how-to-adjust-learning-rate old_pattern test_new_pattern_no_warning epochs = warnings catch_warnings record=True ws warnings simplefilter always allow any warning raised scheduler = StepLR opt gamma= step_size= assertTrue len ws == No warning should raised warnings catch_warnings record=True ws warnings simplefilter always allow any warning raised _ range epochs opt step scheduler step assertTrue len ws == No warning should raised test_new_pattern_no_warning_with_arg epochs = warnings catch_warnings record=True ws warnings simplefilter always allow any warning raised scheduler = StepLR opt gamma= step_size= assertTrue len ws == No warning should raised warnings catch_warnings record=True ws warnings simplefilter always allow any warning raised _ range epochs opt step scheduler step assertTrue len ws == No warning should raised test_new_pattern_no_warning_with_overridden_optim_step epochs = warnings catch_warnings record=True ws warnings simplefilter always allow any warning raised scheduler = StepLR opt gamma= step_size= assertTrue len ws == No warning should raised emulate use-case optimizer step overridden types old_step = opt step new_step o args kwargs retval = old_step args kwargs retval opt step = types MethodType new_step opt new_pattern _ range epochs opt step scheduler step assertWarnsRegex UserWarning r ` optimizer step\ \ ` has been overridden new_pattern _test_lr_is_constant_for_constant_epoch scheduler l = _ range scheduler optimizer step warnings catch_warnings record=True w scheduler step _check_warning_is_epoch_deprecation_warning w l append opt param_groups lr assertEqual min l max l test_step_lr_is_constant_for_constant_epoch scheduler = StepLR opt _test_lr_is_constant_for_constant_epoch scheduler test_exponential_lr_is_constant_for_constant_epoch scheduler = ExponentialLR opt gamma= _test_lr_is_constant_for_constant_epoch scheduler test_constantlr_is_constant_for_constant_epoch scheduler = ConstantLR opt _test_lr_is_constant_for_constant_epoch scheduler test_linear_linearlr_is_constant_for_constant_epoch scheduler = LinearLR opt _test_lr_is_constant_for_constant_epoch scheduler test_polynomial_lr_is_constant_for_constant_epoch scheduler = PolynomialLR opt power= _test_lr_is_constant_for_constant_epoch scheduler test_step_lr lr = epoch lr = = epoch lr = epoch = epochs = single_targets = + + + targets = single_targets x epochs x single_targets scheduler = StepLR opt gamma= step_size= _test scheduler targets epochs test_get_last_lr_step_lr torch nn Parameter epochs = optimizer = SGD Parameter torch randn requires_grad=True targets = + + + scheduler = torch optim lr_scheduler StepLR optimizer gamma= _test_get_last_lr scheduler targets epochs test_get_last_lr_multi_step_lr lr = epoch lr = = epoch lr = = epoch lr = = epoch epochs = single_targets = + + + targets = single_targets x epochs x single_targets scheduler = MultiStepLR opt gamma= milestones= _test_get_last_lr scheduler targets epochs test_raise_error_when_last_epoch_is_greater_than_ _and_initial_lr_is_not_specified optimizer = SGD Parameter torch randn requires_grad=True assertRaisesRegex KeyError r param \ initial_lr\ specified param_groups\ \ when resuming scheduler last_epoch = StepLR optimizer step_size= gamma= last_epoch= test_multi_step_lr lr = epoch lr = = epoch lr = epoch lr = epoch = epochs = single_targets = + + + targets = single_targets x epochs x single_targets scheduler = MultiStepLR opt gamma= milestones= _test scheduler targets epochs test_multi_step_lr_with_epoch lr = epoch lr = = epoch lr = epoch lr = epoch = epochs = single_targets = + + + targets = single_targets x epochs x single_targets scheduler = MultiStepLR opt gamma= milestones= _test_with_epoch scheduler targets epochs test_get_last_lr_constantlr lr = epoch lr = = epoch epochs = single_targets = + targets = single_targets x epochs x single_targets scheduler = ConstantLR opt factor= total_iters= _test_get_last_lr scheduler targets epochs test_get_last_lr_linearlr lr = epoch == lr = epoch == lr = epoch == lr = epoch == lr = = epoch epochs = start_factor = end_factor = iters = interpolation = start_factor + i end_factor - start_factor iters i range iters single_targets = x x interpolation + end_factor epochs - iters targets = single_targets x epochs x single_targets scheduler = LinearLR opt start_factor=start_factor end_factor=end_factor total_iters=iters _test_get_last_lr scheduler targets epochs test_constantlr lr = epoch lr = = epoch epochs = single_targets = + targets = single_targets x epochs x single_targets scheduler = ConstantLR opt factor= total_iters= _test scheduler targets epochs test_linearlr lr = epoch == lr = epoch == lr = epoch == lr = epoch == lr = = epoch epochs = start_factor = iters = interpolation = start_factor + i - start_factor iters i range iters single_targets = x x interpolation + epochs - iters targets = single_targets x epochs x single_targets scheduler = LinearLR opt start_factor=start_factor total_iters=iters _test scheduler targets epochs test_linearlr_start_factor_limits start_factor = iters = assertRaises ValueError LinearLR opt start_factor=start_factor total_iters=iters test_linearlr_start_factor_limits start_factor = iters = assertRaises ValueError LinearLR opt start_factor=start_factor total_iters=iters test_constantlr_with_epoch lr = epoch lr = = epoch epochs = single_targets = + targets = single_targets x epochs x single_targets scheduler = ConstantLR opt factor= total_iters= _test_with_epoch scheduler targets epochs test_linearlr_with_epoch lr = epoch == lr = epoch == lr = epoch == lr = epoch == lr = = epoch epochs = start_factor = end_factor = iters = interpolation = start_factor + i end_factor - start_factor iters i range iters single_targets = x x interpolation + epochs - iters targets = single_targets x epochs x single_targets scheduler = LinearLR opt start_factor=start_factor total_iters=iters _test_with_epoch scheduler targets epochs test_exp_lr epochs = single_targets = x x range epochs targets = single_targets x epochs x single_targets scheduler = ExponentialLR opt gamma= _test scheduler targets epochs test_poly_lr epochs = power = total_iters = single_targets = - x total_iters power x range total_iters + epochs - total_iters targets = single_targets x epochs x single_targets scheduler = PolynomialLR opt power=power total_iters=total_iters _test scheduler targets epochs test_cos_anneal_lr epochs = eta_min = e- single_targets = eta_min + - eta_min + math cos math pi x epochs x range epochs targets = single_targets x epochs x single_targets scheduler = CosineAnnealingLR opt T_max=epochs eta_min=eta_min _test scheduler targets epochs test_closed_form_step_lr scheduler = StepLR opt gamma= step_size= closed_form_scheduler = StepLR opt gamma= step_size= _test_against_closed_form scheduler closed_form_scheduler test_closed_form_linearlr scheduler = LinearLR opt start_factor= end_factor= total_iters= closed_form_scheduler = LinearLR opt start_factor= end_factor= total_iters= _test_against_closed_form scheduler closed_form_scheduler test_closed_form_constantlr scheduler = ConstantLR opt factor= total_iters= closed_form_scheduler = ConstantLR opt factor= total_iters= _test_against_closed_form scheduler closed_form_scheduler test_closed_form_multi_step_lr scheduler = MultiStepLR opt gamma= milestones= closed_form_scheduler = MultiStepLR opt gamma= milestones= _test_against_closed_form scheduler closed_form_scheduler test_closed_form_exp_lr scheduler = ExponentialLR opt gamma= closed_form_scheduler = ExponentialLR opt gamma= _test_against_closed_form scheduler closed_form_scheduler test_closed_form_poly_lr scheduler = PolynomialLR opt power= closed_form_scheduler = PolynomialLR opt power= _test_against_closed_form scheduler closed_form_scheduler test_closed_form_cos_anneal_lr eta_min = e- epochs = T_max = scheduler = CosineAnnealingLR opt T_max=T_max eta_min=eta_min closed_form_scheduler = CosineAnnealingLR opt T_max=T_max eta_min=eta_min _test_against_closed_form scheduler closed_form_scheduler epochs test_cos_anneal_lr_continue eta_min = T_max = scheduler = CosineAnnealingLR opt T_max=T_max eta_min=eta_min opt step scheduler step original_lrs = scheduler _last_lr new_scheduler = CosineAnnealingLR opt T_max=T_max eta_min=eta_min last_epoch= new_lrs = new_scheduler _last_lr torch testing assert_close original_lrs new_lrs rtol= e- atol= e- test_reduce_lr_on_plateau epochs = param_group opt param_groups param_group lr = targets = metrics = - i i range scheduler = ReduceLROnPlateau opt threshold_mode= abs mode= min threshold= patience= cooldown= _test_reduce_lr_on_plateau scheduler targets metrics epochs test_reduce_lr_on_plateau epochs = param_group opt param_groups param_group lr = targets = + + + metrics = - i i range scheduler = ReduceLROnPlateau opt patience= cooldown= threshold_mode= abs mode= min threshold= _test_reduce_lr_on_plateau scheduler targets metrics epochs test_reduce_lr_on_plateau epochs = param_group opt param_groups param_group lr = targets = + + + + metrics = - + - scheduler = ReduceLROnPlateau opt mode= max patience= cooldown= threshold_mode= abs _test_reduce_lr_on_plateau scheduler targets metrics epochs test_reduce_lr_on_plateau epochs = param_group opt param_groups param_group lr = targets = metrics = i i range scheduler = ReduceLROnPlateau opt mode= max patience= threshold_mode= rel threshold= _test_reduce_lr_on_plateau scheduler targets metrics epochs test_reduce_lr_on_plateau epochs = param_group opt param_groups param_group lr = targets = + + + metrics = i i range scheduler = ReduceLROnPlateau opt mode= max threshold_mode= rel threshold= patience= cooldown= _test_reduce_lr_on_plateau scheduler targets metrics epochs test_reduce_lr_on_plateau epochs = param_group opt param_groups param_group lr = targets = metrics = i i range scheduler = ReduceLROnPlateau opt mode= min threshold_mode= rel threshold= _test_reduce_lr_on_plateau scheduler targets metrics epochs test_reduce_lr_on_plateau epochs = param_group opt param_groups param_group lr = targets = + + + metrics = + + scheduler = ReduceLROnPlateau opt mode= min threshold_mode= rel threshold= patience= cooldown= _test_reduce_lr_on_plateau scheduler targets metrics epochs test_reduce_lr_on_plateau epochs = param_group opt param_groups param_group lr = targets = + + metrics = i i range scheduler = ReduceLROnPlateau opt mode= max threshold_mode= rel min_lr= threshold= patience= cooldown= _test_reduce_lr_on_plateau scheduler targets metrics epochs test_reduce_lr_on_plateau_get_last_lr_before_step param_group opt param_groups param_group lr = scheduler = ReduceLROnPlateau opt assertEqual scheduler get_last_lr param_group opt param_groups test_reduce_lr_on_plateau_preserves_lr_type Ensures tensor lrs preserved preventing recompilations types = type group lr group opt param_groups scheduler = ReduceLROnPlateau opt mode= min patience= scheduler step scheduler step Triggers scheduler _reduce_lr group type_ zip opt param_groups types assertEqual type group lr type_ test_sequentiallr epochs = schedulers = None targets = + x range + x range + x range + x range milestones = schedulers = ExponentialLR opt gamma= schedulers = StepLR opt gamma= step_size= scheduler = SequentialLR opt schedulers=schedulers milestones=milestones _test scheduler targets epochs test_sequentiallr epochs = schedulers = None targets = + x x range milestones = schedulers = ConstantLR opt factor= total_iters= schedulers = ExponentialLR opt gamma= scheduler = SequentialLR opt schedulers=schedulers milestones=milestones _test scheduler targets epochs test_sequentiallr epochs = schedulers = None targets = + + milestones = schedulers = ConstantLR opt factor= total_iters= schedulers = ExponentialLR opt gamma= schedulers = StepLR opt gamma= step_size= scheduler = SequentialLR opt schedulers=schedulers milestones=milestones _test scheduler targets epochs test_sequentiallr optimizer = SGD torch tensor lr= prev_lr = optimizer param_groups lr schedulers = torch optim lr_scheduler ConstantLR optimizer factor= torch optim lr_scheduler ConstantLR optimizer factor= scheduler = torch optim lr_scheduler SequentialLR optimizer schedulers milestones= new_lr = optimizer param_groups lr Ensure multiple schedulers does affect initial learning rate assertEqual prev_lr new_lr test_sequentiallr Test SequentialLR ChainedScheduler epochs = schedulers = milestones = targets = + const_sched = ConstantLR optimizer=self opt factor= total_iters= lin_sched = LinearLR optimizer=self opt start_factor= total_iters= milestones append chained = ChainedScheduler lin_sched const_sched schedulers append chained const_sched = ConstantLR optimizer=self opt factor= total_iters= schedulers append const_sched scheduler = SequentialLR opt schedulers=schedulers milestones=milestones _test scheduler targets epochs test_sequentiallr_no_warnings scheduler = LinearLR opt start_factor= end_factor= total_iters= scheduler = ExponentialLR opt gamma= scheduler = SequentialLR opt schedulers= scheduler scheduler milestones= _ range opt step warnings catch_warnings record=True ws scheduler step assertTrue len ws == No warning should raised test_get_last_lr_sequentiallr epochs = milestones = schedulers = None schedulers = ConstantLR opt factor= total_iters= schedulers = ExponentialLR opt gamma= schedulers = StepLR opt gamma= step_size= scheduler = SequentialLR opt schedulers=schedulers milestones=milestones constant_lr_target = exponential_lr_target = step_lr_target = single_targets = constant_lr_target + exponential_lr_target + step_lr_target targets = single_targets x x single_targets _test_get_last_lr scheduler targets epochs test_sequentiallr_does_not_alias_lr_and_initial_lr The TestLRScheduler object uses opt avoid instantiating new optimizer each test opt has float lr we need use Tensor lr ensure former SequentialLR bug fixed For more context see https github com pytorch pytorch issues old_opt = opt lr = torch tensor opt = SGD net parameters lr=lr milestone = epochs = start end = schedulers = LinearLR opt start end total_iters=milestone LinearLR opt end start total_iters=epochs - milestone targets = scheduler = SequentialLR opt schedulers milestones= milestone _test scheduler targets epochs opt = old_opt test_chained_lr _get_last_lr_before_step schedulers = LinearLR opt start_factor= total_iters= MultiStepLR opt milestones= gamma= scheduler = ChainedScheduler schedulers assertEqual scheduler get_last_lr schedulers - get_last_lr test_chained_lr epochs = schedulers = None targets = + + + schedulers = StepLR opt gamma= step_size= scheduler = ChainedScheduler schedulers _test scheduler targets epochs assertEqual scheduler get_last_lr schedulers - get_last_lr test_chained_lr epochs = schedulers = None targets = + schedulers = LinearLR opt start_factor= total_iters= scheduler = ChainedScheduler schedulers _test scheduler targets epochs assertEqual scheduler get_last_lr schedulers - get_last_lr test_chained_lr epochs = schedulers = None targets = + + + schedulers = LinearLR opt start_factor= total_iters= schedulers = MultiStepLR opt milestones= gamma= scheduler = ChainedScheduler schedulers _test scheduler targets epochs assertEqual scheduler get_last_lr schedulers - get_last_lr test_chained_lr epochs = schedulers = None targets = x x range + + x x range + x x range schedulers = ExponentialLR opt gamma= schedulers = ConstantLR opt factor= total_iters= schedulers = StepLR opt gamma= step_size= scheduler = ChainedScheduler schedulers _test scheduler targets epochs assertEqual scheduler get_last_lr schedulers - get_last_lr test_chained_lr poly_lr lr float lr - x total_iters power x range total_iters + epochs - total_iters schedulers = None epochs = power = total_iters = const_factor = single_targets = x const_factor x poly_lr lr= targets = single_targets x const_factor x poly_lr schedulers = PolynomialLR opt power=power total_iters=total_iters schedulers = ConstantLR opt factor=const_factor scheduler = ChainedScheduler schedulers _test scheduler targets epochs assertEqual scheduler get_last_lr schedulers - get_last_lr test_compound_step_and_multistep_lr epochs = schedulers = None schedulers = StepLR opt gamma= step_size= schedulers = MultiStepLR opt gamma= milestones= targets = + + e- + e- + e- + e- _test schedulers targets epochs test_compound_step_and_exp_lr epochs = schedulers = None single_targets = x x range single_targets += x x range single_targets += x x range single_targets += x x range targets = single_targets x epochs x single_targets schedulers = StepLR opt gamma= step_size= schedulers = ExponentialLR opt gamma= _test schedulers targets epochs test_compound_exp_and_multistep_lr epochs = schedulers = None single_targets = x x range single_targets += x x range single_targets += x x range single_targets += x x range targets = single_targets x epochs x single_targets schedulers = MultiStepLR opt gamma= milestones= schedulers = ExponentialLR opt gamma= _test schedulers targets epochs test_compound_exp_and_linearlr epochs = iters = start_factor = end_factor = schedulers = None single_targets = x x range i range iters single_targets i = start_factor + i iters end_factor - start_factor i range iters single_targets i = end_factor targets = single_targets x epochs x single_targets schedulers = LinearLR opt start_factor=start_factor end_factor=end_factor total_iters=iters schedulers = ExponentialLR opt gamma= _test schedulers targets epochs test_compound_step_and_constantlr epochs = iters = factor = schedulers = None single_targets = + + + + targets = single_targets x epochs x single_targets schedulers = StepLR opt gamma= step_size= schedulers = ConstantLR opt factor= total_iters= _test schedulers targets epochs test_compound_linearlr_and_multistep_lr epochs = iters = start_factor = schedulers = None single_targets = + + + i range iters single_targets i = start_factor + i iters - start_factor targets = single_targets x epochs x single_targets schedulers = MultiStepLR opt gamma= milestones= schedulers = LinearLR opt start_factor=start_factor total_iters=iters _test schedulers targets epochs test_compound_cosanneal_and_step_lr epochs = eta_min = e- single_targets = eta_min + - eta_min + math cos math pi x epochs x range epochs single_targets = x i i x enumerate single_targets targets = single_targets x epochs x single_targets schedulers = None schedulers = CosineAnnealingLR opt T_max=epochs eta_min=eta_min schedulers = StepLR opt gamma= step_size= _test schedulers targets epochs test_compound_cosanneal_and_multistep_lr epochs = eta_min = e- single_targets = eta_min + - eta_min + math cos math pi x epochs x range epochs multipliers = + + + single_targets = x y x y zip single_targets multipliers targets = single_targets x epochs x single_targets schedulers = None schedulers = CosineAnnealingLR opt T_max=epochs eta_min=eta_min schedulers = MultiStepLR opt gamma= milestones= _test schedulers targets epochs test_compound_cosanneal_and_linearlr epochs = iters = start_factor = eta_min = e- schedulers = None single_targets = eta_min + - eta_min + math cos math pi x epochs x range epochs i range iters single_targets i = start_factor + i iters - start_factor targets = single_targets x epochs x single_targets schedulers = LinearLR opt start_factor=start_factor total_iters=iters schedulers = CosineAnnealingLR opt T_max=epochs eta_min=eta_min _test schedulers targets epochs test_compound_cosanneal_and_exp_lr epochs = eta_min = e- single_targets = eta_min + - eta_min + math cos math pi x epochs x range epochs multipliers = i i range epochs single_targets = x y x y zip single_targets multipliers targets = single_targets x epochs x single_targets schedulers = None schedulers = CosineAnnealingLR opt T_max=epochs eta_min=eta_min schedulers = ExponentialLR opt gamma= _test schedulers targets epochs test_compound_reduce_lr_on_plateau epochs = param_group opt param_groups param_group lr = single_targets = multipliers = i i range single_targets = x y x y zip multipliers single_targets targets = single_targets targets = targets test runs step before checking lr metrics = - i i range schedulers = None None schedulers = ReduceLROnPlateau opt threshold_mode= abs mode= min threshold= patience= cooldown= schedulers = StepLR opt gamma= step_size= _test_reduce_lr_on_plateau schedulers targets metrics epochs test_compound_reduce_lr_on_plateau epochs = param_group opt param_groups param_group lr = single_targets = + + + multipliers = + + + single_targets = x y x y zip single_targets multipliers targets = single_targets targets = targets test runs step before checking lr metrics = - i i range schedulers = None schedulers = ReduceLROnPlateau opt patience= cooldown= threshold_mode= abs mode= min threshold= schedulers = MultiStepLR opt gamma= milestones= _test_reduce_lr_on_plateau schedulers targets metrics epochs test_compound_reduce_lr_on_plateau epochs = param_group opt param_groups param_group lr = single_targets = + + + + multipliers = i i range epochs single_targets = x y x y zip multipliers single_targets targets = single_targets targets = targets test runs step before checking lr metrics = - + - schedulers = None None schedulers = ReduceLROnPlateau opt mode= max patience= cooldown= threshold_mode= abs schedulers = ExponentialLR opt gamma= _test_reduce_lr_on_plateau schedulers targets metrics epochs test_compound_reduce_lr_on_plateau epochs = param_group opt param_groups param_group lr = epochs = eta_min = e- single_targets = eta_min + - eta_min + math cos math pi x epochs x range epochs targets = single_targets targets = targets test runs step before checking lr metrics = i i range schedulers = None None schedulers = ReduceLROnPlateau opt mode= max patience= threshold_mode= rel threshold= schedulers = CosineAnnealingLR opt epochs eta_min _test_reduce_lr_on_plateau schedulers targets metrics epochs test_compound_reduce_lr_on_plateau iters = start_factor = epochs = param_group opt param_groups param_group lr = single_targets = + + + multipliers = i range iters multipliers i = start_factor + i iters - start_factor single_targets = x y x y zip single_targets multipliers targets = single_targets targets = targets test runs step before checking lr metrics = - i i range schedulers = None schedulers = ReduceLROnPlateau opt patience= cooldown= threshold_mode= abs mode= min threshold= schedulers = LinearLR opt start_factor=start_factor total_iters=iters _test_reduce_lr_on_plateau schedulers targets metrics epochs test_cycle_lr_invalid_mode assertRaises ValueError scheduler = CyclicLR opt base_lr= max_lr= mode= CATS test_cycle_lr_triangular_mode_one_lr lr_target = momentum_target = lr_targets = lr_target lr_target momentum_targets = momentum_target momentum_target scheduler = CyclicLR opt base_lr= max_lr= step_size_up= cycle_momentum=True base_momentum= max_momentum= mode= triangular _test_cycle_lr scheduler lr_targets momentum_targets len lr_target test_cycle_lr_triangular_mode_one_lr_no_momentum lr_target = lr_targets = lr_target lr_target momentum_target = opt defaults momentum len lr_target momentum_targets = momentum_target momentum_target scheduler = CyclicLR opt base_lr= max_lr= step_size_up= cycle_momentum=False mode= triangular _test_cycle_lr scheduler lr_targets momentum_targets len lr_target test_cycle_lr_triangular _mode_one_lr lr_target = momentum_target = lr_targets = lr_target lr_target momentum_targets = momentum_target momentum_target scheduler = CyclicLR opt base_lr= max_lr= step_size_up= cycle_momentum=True base_momentum= max_momentum= mode= triangular _test_cycle_lr scheduler lr_targets momentum_targets len lr_target test_cycle_lr_exp_range_mode_one_lr base_lr max_lr = diff_lr = max_lr - base_lr gamma = xs = lr_target = base_lr + x diff_lr gamma i i x enumerate xs momentum_target = max_lr - x diff_lr gamma i i x enumerate xs lr_targets = lr_target lr_target momentum_targets = momentum_target momentum_target scheduler = CyclicLR opt base_lr=base_lr max_lr=max_lr step_size_up= cycle_momentum=True base_momentum=base_lr max_momentum=max_lr mode= exp_range gamma=gamma _test_cycle_lr scheduler lr_targets momentum_targets len lr_target test_cycle_lr_triangular_mode lr_target_ = lr_target_ = x + x lr_target_ lr_targets = lr_target_ lr_target_ momentum_target_ = momentum_target_ = x + x momentum_target_ momentum_targets = momentum_target_ momentum_target_ scheduler = CyclicLR opt base_lr= max_lr= step_size_up= cycle_momentum=True base_momentum= max_momentum= mode= triangular _test_cycle_lr scheduler lr_targets momentum_targets len lr_target_ test_cycle_lr_triangular _mode lr_target_ = lr_target_ = x + x lr_target_ lr_targets = lr_target_ lr_target_ momentum_target_ = momentum_target_ = x + x momentum_target_ momentum_targets = momentum_target_ momentum_target_ scheduler = CyclicLR opt base_lr= max_lr= step_size_up= cycle_momentum=True base_momentum= max_momentum= mode= triangular _test_cycle_lr scheduler lr_targets momentum_targets len lr_target_ test_cycle_lr_exp_range_mode base_lr_ max_lr_ = base_lr_ max_lr_ = diff_lr_ = max_lr_ - base_lr_ diff_lr_ = max_lr_ - base_lr_ gamma = xs = lr_target_ = base_lr_ + x diff_lr_ gamma i i x enumerate xs lr_target_ = base_lr_ + x diff_lr_ gamma i i x enumerate xs lr_targets = lr_target_ lr_target_ momentum_target_ = max_lr_ - x diff_lr_ gamma i i x enumerate xs momentum_target_ = max_lr_ - x diff_lr_ gamma i i x enumerate xs momentum_targets = momentum_target_ momentum_target_ scheduler = CyclicLR opt base_lr= base_lr_ base_lr_ max_lr= max_lr_ max_lr_ step_size_up= cycle_momentum=True base_momentum= base_lr_ base_lr_ max_momentum= max_lr_ max_lr_ mode= exp_range gamma=gamma _test_cycle_lr scheduler lr_targets momentum_targets len lr_target_ test_cycle_lr_triangular_mode_step_size_up_down lr_target = lr_targets = lr_target lr_target momentum_target = momentum_targets = momentum_target momentum_target scheduler = CyclicLR opt base_lr= max_lr= step_size_up= step_size_down= cycle_momentum=True base_momentum= max_momentum= mode= triangular _test_cycle_lr scheduler lr_targets momentum_targets len lr_target test_cycle_lr_triangular _mode_step_size_up_down lr_base_target = momentum_base_target = deltas = i i range base_lrs = + delta delta deltas max_lrs = + delta delta deltas lr_targets = x + delta x lr_base_target delta deltas momentum_targets = x + delta x momentum_base_target delta deltas scheduler = CyclicLR opt base_lr=base_lrs max_lr=max_lrs step_size_up= step_size_down= cycle_momentum=True base_momentum=base_lrs max_momentum=max_lrs mode= triangular _test_cycle_lr scheduler lr_targets momentum_targets len lr_base_target test_cycle_lr_exp_range_mode_step_size_up_down base_lr max_lr = diff_lr = max_lr - base_lr gamma = xs = lr_target = base_lr + x diff_lr gamma i i x enumerate xs lr_targets = lr_target lr_target momentum_target = max_lr - x diff_lr gamma i i x enumerate xs momentum_targets = momentum_target momentum_target scheduler = CyclicLR opt base_lr=base_lr max_lr=max_lr step_size_up= step_size_down= cycle_momentum=True base_momentum=base_lr max_momentum=max_lr mode= exp_range gamma=gamma _test_cycle_lr scheduler lr_targets momentum_targets len lr_target test_cycle_lr_with_momentumless_optimizer Note Temporarily set optimizer Adam ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ The TestLRScheduler object carries around SGD optimizer avoid having instantiate one every test This gets way our very specific case which we need use Adam really any optimizer doesn t use momentum order test momentum bug CyclicLR fixed bug described more detail https github com pytorch pytorch issues old_opt = opt opt = Adam params net conv parameters params net conv parameters lr lr= lr_target = lr_targets = lr_target lr_target momentum_target = None len lr_target momentum_targets = momentum_target momentum_target scheduler = CyclicLR opt base_lr= max_lr= step_size_up= cycle_momentum=False mode= triangular _test_cycle_lr scheduler lr_targets momentum_targets len lr_target opt = old_opt set optimizer back SGD test_cycle_lr_cycle_momentum_fail_with_momentumless_optimizer assertRaises ValueError rprop_opt = Rprop net parameters scheduler = CyclicLR rprop_opt base_lr= max_lr= cycle_momentum=True test_cycle_lr_cycle_momentum_with_beta _optimizer adam_opt = Adam net parameters scheduler = CyclicLR adam_opt base_lr= max_lr= cycle_momentum=True test_cycle_lr_removed_after_out_of_scope gc weakref gc disable test adam_opt = Adam net parameters scheduler = CyclicLR adam_opt base_lr= max_lr= cycle_momentum=False weakref ref scheduler ref = test assert ref None gc enable test_cycle_lr_state_dict_picklable adam_opt = Adam net parameters Case Built-in mode scheduler = CyclicLR adam_opt base_lr= max_lr= cycle_momentum=False assertIsInstance scheduler _scale_fn_ref types FunctionType state = scheduler state_dict assertNotIn _scale_fn_ref state assertIs state _scale_fn_custom None pickle dumps state Case Custom ` scale_fn ` function object scale_fn _ scheduler = CyclicLR adam_opt base_lr= max_lr= cycle_momentum=False scale_fn=scale_fn state = scheduler state_dict assertNotIn _scale_fn_ref state assertIs state _scale_fn_custom None pickle dumps state Case Custom ` scale_fn ` callable ScaleFn __init__ - None x = __call__ _ x scale_fn = ScaleFn scheduler = CyclicLR adam_opt base_lr= max_lr= cycle_momentum=False scale_fn=scale_fn state = scheduler state_dict assertNotIn _scale_fn_ref state assertEqual state _scale_fn_custom scale_fn __dict__ pickle dumps state test_cycle_lr_scale_fn_restored_from_state_dict adam_opt = Adam net parameters Case Built-in mode scheduler = CyclicLR adam_opt base_lr= max_lr= cycle_momentum=False mode= triangular restored_scheduler = CyclicLR adam_opt base_lr= max_lr= cycle_momentum=False restored_scheduler load_state_dict scheduler state_dict assertTrue restored_scheduler mode == scheduler mode == triangular assertIsNotNone restored_scheduler _scale_fn_ref assertIsNotNone scheduler _scale_fn_ref assertIs restored_scheduler _scale_fn_custom None assertIs scheduler _scale_fn_custom None Case Custom ` scale_fn ` scale_fn _ scheduler = CyclicLR adam_opt base_lr= max_lr= cycle_momentum=False scale_fn=scale_fn restored_scheduler = CyclicLR adam_opt base_lr= max_lr= cycle_momentum=False scale_fn=scale_fn restored_scheduler load_state_dict scheduler state_dict assertIs scheduler _scale_fn_custom scale_fn assertIs restored_scheduler _scale_fn_custom scale_fn test_onecycle_lr_invalid_anneal_strategy assertRaises ValueError scheduler = OneCycleLR opt max_lr= e- total_steps= anneal_strategy= CATS test_onecycle_lr_invalid_pct_start assertRaises ValueError scheduler = OneCycleLR opt max_lr= e- total_steps= pct_start= test_onecycle_lr_cannot_calculate_total_steps assertRaises ValueError scheduler = OneCycleLR opt max_lr= e- test_onecycle_lr_linear_annealing lr_target = momentum_target = lr_targets = lr_target lr_target momentum_targets = momentum_target momentum_target scheduler = OneCycleLR opt max_lr= final_div_factor= base_momentum= max_momentum= total_steps= anneal_strategy= linear _test_cycle_lr scheduler lr_targets momentum_targets test_onecycle_lr_linear_annealing_three_phases lr_target = momentum_target = lr_targets = lr_target lr_target momentum_targets = momentum_target momentum_target scheduler = OneCycleLR opt max_lr= div_factor= base_momentum= max_momentum= total_steps= anneal_strategy= linear pct_start= final_div_factor= three_phase=True _test_cycle_lr scheduler lr_targets momentum_targets test_onecycle_lr_cosine_annealing annealing_cos start end pct cos_out = math cos math pi pct + end + start - end cos_out lr_target = annealing_cos annealing_cos annealing_cos annealing_cos annealing_cos annealing_cos momentum_target = annealing_cos annealing_cos annealing_cos annealing_cos annealing_cos annealing_cos lr_targets = lr_target lr_target momentum_targets = momentum_target momentum_target scheduler = OneCycleLR opt max_lr= final_div_factor= base_momentum= max_momentum= total_steps= _test_cycle_lr scheduler lr_targets momentum_targets test_onecycle_lr_legacy_state_dict scheduler = OneCycleLR opt max_lr= final_div_factor= base_momentum= max_momentum= total_steps= anneal_strategy= cos delattr scheduler _anneal_func_type state_dict = scheduler state_dict assertNotIn anneal_func_type state_dict state_dict anneal_func = OneCycleLR _annealing_cos scheduler load_state_dict state_dict annealing_cos start end pct cos_out = math cos math pi pct + end + start - end cos_out lr_target = annealing_cos annealing_cos annealing_cos annealing_cos annealing_cos annealing_cos momentum_target = annealing_cos annealing_cos annealing_cos annealing_cos annealing_cos annealing_cos lr_targets = lr_target lr_target momentum_targets = momentum_target momentum_target _test_cycle_lr scheduler lr_targets momentum_targets test_cycle_lr_with_adam old_opt = opt opt = Adam params net conv parameters params net conv parameters lr lr= lr_target = momentum_target = lr_targets = lr_target lr_target momentum_targets = momentum_target momentum_target scheduler = OneCycleLR opt max_lr= final_div_factor= base_momentum= max_momentum= total_steps= anneal_strategy= linear _test_cycle_lr scheduler lr_targets momentum_targets use_beta =True opt = old_opt set optimizer back SGD test_lambda_lr epochs = opt param_groups lr = opt param_groups lr = targets = x x range epochs x x range epochs scheduler = LambdaLR opt lr_lambda= lambda x x lambda x x _test scheduler targets epochs test_multiplicative_lr epochs = opt param_groups lr = opt param_groups lr = targets = x x range epochs x x range epochs scheduler = MultiplicativeLR opt lr_lambda= lambda x lambda x _test scheduler targets epochs test_multiplicative_lr_with_lr_lambda lr_lambda = assertRaisesRegex TypeError lr_lambda should function MultiplicativeLR opt lr_lambda lr_lambda = assertRaisesRegex TypeError lr_lambda should function MultiplicativeLR opt lr_lambda lr_lambda parametrize T_mult test_CosineAnnealingWarmRestarts_lr T_mult iters = eta_min = e- T_i = T_cur = targets = scheduler = CosineAnnealingWarmRestarts opt T_ =T_i T_mult=T_mult eta_min=eta_min _ range iters T_cur += T_cur = T_i T_cur = T_cur - T_i T_i = int T_mult T_i targets += eta_min + - eta_min + math cos math pi T_cur T_i targets += eta_min + - eta_min + math cos math pi T_cur T_i _test scheduler targets iters test_CosineAnnealingWarmRestarts_lr iters = eta_min = e- T_mults = T_mult T_mults T_i = T_cur = targets = scheduler = CosineAnnealingWarmRestarts opt T_ =T_i T_mult=T_mult eta_min=eta_min _ torch arange iters T_cur = round T_cur + T_cur = T_i T_cur = T_cur - T_i T_i = int T_mult T_i targets += eta_min + - eta_min + math cos math pi T_cur T_i targets += eta_min + - eta_min + math cos math pi T_cur T_i _test_CosineAnnealingWarmRestarts scheduler targets iters test_CosineAnnealingWarmRestarts_lr epochs_for_T_mults = T_curs_for_T_mults = T_is_for_T_mults = eta_min = e- T_mults = epochs T_mult T_curs T_is zip epochs_for_T_mults T_mults T_curs_for_T_mults T_is_for_T_mults targets = scheduler = CosineAnnealingWarmRestarts opt T_ = T_mult=T_mult eta_min=eta_min T_cur T_i zip T_curs T_is targets += eta_min + - eta_min + math cos math pi T_cur T_i targets += eta_min + - eta_min + math cos math pi T_cur T_i _test_interleaved_CosineAnnealingWarmRestarts scheduler targets epochs test_CosineAnnealingWarmRestarts_T_cur_reset sch = CosineAnnealingWarmRestarts opt T_ = epoch sch T_cur = epoch sch step expect_T_cur = epoch + sch T_ assertEqual sch T_cur expect_T_cur test_swalr_no_anneal epochs swa_start swa_lr = initial_lrs = group lr group opt param_groups targets = lr swa_start + + swa_lr epochs - swa_start - lr initial_lrs swa_scheduler = SWALR opt anneal_epochs= swa_lr=swa_lr _test_swalr swa_scheduler None targets swa_start epochs test_swalr_cosine_anneal_after_multiplicative same swa_lr different param_groups epochs swa_start swa_lr anneal_epochs = mult_factor = scheduler = MultiplicativeLR opt lr_lambda=lambda epoch mult_factor swa_scheduler = SWALR opt anneal_epochs=anneal_epochs swa_lr=swa_lr anneal_coef t t + = anneal_epochs + math cos math pi t + anneal_epochs initial_lrs = group lr group opt param_groups targets_before_swa = lr mult_factor i i range swa_start + lr initial_lrs swa_epochs = epochs - swa_start - targets = lrs + lrs - anneal_coef t + swa_lr - anneal_coef t t range swa_epochs lrs targets_before_swa _test_swalr swa_scheduler scheduler targets swa_start epochs test_swalr_linear_anneal_after_multiplicative separate swa_lr different param_groups epochs swa_start swa_lrs anneal_epochs = mult_factor = scheduler = MultiplicativeLR opt lr_lambda=lambda epoch mult_factor swa_scheduler = SWALR opt anneal_epochs=anneal_epochs anneal_strategy= linear swa_lr=swa_lrs anneal_coef t t + = anneal_epochs - t + anneal_epochs initial_lrs = group lr group opt param_groups targets_before_swa = lr mult_factor i i range swa_start + lr initial_lrs swa_epochs = epochs - swa_start - targets = lrs + lrs - anneal_coef t + swa_lr - anneal_coef t t range swa_epochs lrs swa_lr zip targets_before_swa swa_lrs _test_swalr swa_scheduler scheduler targets swa_start epochs _test_swalr swa_scheduler scheduler targets swa_start epochs epoch range epochs param_group target zip opt param_groups targets assertEqual target epoch param_group lr msg= LR wrong epoch expected got format epoch target epoch param_group lr atol= e- rtol= epoch = swa_start opt step swa_scheduler step scheduler None opt step scheduler step test_swalr_hypers Test SWALR raises errors incorrect hyper-parameters assertRaisesRegex ValueError anneal_strategy must swa_scheduler = SWALR opt anneal_strategy= exponential swa_lr= assertRaisesRegex ValueError anneal_epochs must swa_scheduler = SWALR opt anneal_epochs=- swa_lr= assertRaisesRegex ValueError anneal_epochs must swa_scheduler = SWALR opt anneal_epochs= swa_lr= assertRaisesRegex ValueError swa_lr must swa_scheduler = SWALR opt swa_lr= test_step_lr_state_dict _check_scheduler_state_dict lambda StepLR opt gamma= step_size= lambda StepLR opt gamma= step_size= test_multi_step_lr_state_dict _check_scheduler_state_dict lambda MultiStepLR opt gamma= milestones= lambda MultiStepLR opt gamma= milestones= test_exp_step_lr_state_dict _check_scheduler_state_dict lambda ExponentialLR opt gamma= lambda ExponentialLR opt gamma= test_cosine_lr_state_dict epochs = eta_min = e- _check_scheduler_state_dict lambda CosineAnnealingLR opt T_max=epochs eta_min=eta_min lambda CosineAnnealingLR opt T_max=epochs eta_min=eta_min epochs=epochs test_reduce_lr_on_plateau_state_dict scheduler = ReduceLROnPlateau opt mode= min factor= patience= score scheduler step score scheduler_copy = ReduceLROnPlateau opt mode= max factor= patience= scheduler_copy load_state_dict scheduler state_dict key scheduler __dict__ keys key optimizer is_better assertEqual scheduler __dict__ key scheduler_copy __dict__ key test_lambda_lr_state_dict_fn scheduler = LambdaLR opt lr_lambda=lambda x x state = scheduler state_dict assertIsNone state lr_lambdas scheduler_copy = LambdaLR opt lr_lambda=lambda x x scheduler_copy load_state_dict state key scheduler __dict__ keys key optimizer lr_lambdas assertEqual scheduler __dict__ key scheduler_copy __dict__ key test_lambda_lr_state_dict_obj scheduler = LambdaLR opt lr_lambda=self LambdaLRTestObject state = scheduler state_dict assertIsNotNone state lr_lambdas scheduler_copy = LambdaLR opt lr_lambda=self LambdaLRTestObject - scheduler_copy load_state_dict state key scheduler __dict__ keys key optimizer assertEqual scheduler __dict__ key scheduler_copy __dict__ key test_CosineAnnealingWarmRestarts_lr_state_dict _check_scheduler_state_dict lambda CosineAnnealingWarmRestarts opt T_ = T_mult= lambda CosineAnnealingWarmRestarts opt T_ = test_swa_lr_state_dict _check_scheduler_state_dict lambda SWALR opt anneal_epochs= swa_lr= lambda SWALR opt anneal_epochs= anneal_strategy= linear swa_lr= _check_scheduler_state_dict constr constr epochs= scheduler = constr _ range epochs scheduler optimizer step scheduler step scheduler_copy = constr scheduler_copy load_state_dict scheduler state_dict key scheduler __dict__ keys key = optimizer assertEqual scheduler __dict__ key scheduler_copy __dict__ key assertEqual scheduler get_last_lr scheduler_copy get_last_lr _test_get_last_lr schedulers targets epochs= isinstance schedulers LRScheduler schedulers = schedulers optimizers = scheduler optimizer scheduler schedulers epoch range epochs result = scheduler get_last_lr scheduler schedulers optimizer step optimizer optimizers scheduler step scheduler schedulers target = t epoch t targets len schedulers t r zip target result assertEqual t r msg=f LR wrong epoch epoch expected t got r atol= e- rtol= _test_with_epoch schedulers targets epochs= isinstance schedulers LRScheduler schedulers = schedulers optimizers = scheduler optimizer scheduler schedulers epoch range epochs optimizer step optimizer optimizers warnings catch_warnings record=True w scheduler step epoch scheduler schedulers step before assert skip initial lr _check_warning_is_epoch_deprecation_warning w num_warnings=len schedulers param_group target zip opt param_groups targets assertEqual target epoch param_group lr msg= LR wrong epoch expected got format epoch target epoch param_group lr atol= e- rtol= _test schedulers targets epochs= isinstance schedulers LRScheduler schedulers = schedulers epoch range epochs param_group target zip opt param_groups targets assertEqual target epoch param_group lr msg= LR wrong epoch expected got format epoch target epoch param_group lr atol= e- rtol= scheduler step scheduler schedulers _test_CosineAnnealingWarmRestarts scheduler targets epochs= index epoch enumerate torch arange epochs epoch = round epoch item scheduler step epoch param_group target zip opt param_groups targets assertEqual target index param_group lr msg= LR wrong epoch expected got format epoch target index param_group lr atol= e- rtol= _test_interleaved_CosineAnnealingWarmRestarts scheduler targets epochs index epoch enumerate epochs scheduler step epoch param_group target zip opt param_groups targets assertEqual target index param_group lr msg= LR wrong epoch expected got format epoch target index param_group lr atol= e- rtol= _test_against_closed_form scheduler closed_form_scheduler epochs= setUp targets = epoch range epochs closed_form_scheduler optimizer step warnings catch_warnings record=True w closed_form_scheduler step epoch _check_warning_is_epoch_deprecation_warning w targets append group lr group opt param_groups setUp epoch range epochs opt step scheduler step i param_group enumerate opt param_groups assertEqual targets epoch i param_group lr msg= LR wrong epoch expected got format epoch targets epoch i param_group lr atol= e- rtol= _test_reduce_lr_on_plateau schedulers targets metrics epochs= verbose=False isinstance schedulers LRScheduler ReduceLROnPlateau schedulers = schedulers epoch range epochs opt step scheduler schedulers isinstance scheduler ReduceLROnPlateau scheduler step metrics epoch scheduler step verbose print epoch \tlr= format epoch opt param_groups lr param_group target zip opt param_groups targets assertEqual target epoch param_group lr msg= LR wrong epoch expected got format epoch target epoch param_group lr atol= e- rtol= _test_cycle_lr scheduler lr_targets momentum_targets batch_iterations verbose=False use_beta =False batch_num range batch_iterations verbose momentum opt param_groups keys print batch \tlr= momentum= format batch_num opt param_groups lr opt param_groups momentum use_beta betas opt param_groups keys print batch \tlr= beta = format batch_num opt param_groups lr opt param_groups betas print batch \tlr= format batch_num opt param_groups lr param_group lr_target momentum_target zip opt param_groups lr_targets momentum_targets assertEqual lr_target batch_num param_group lr msg= LR wrong batch_num expected got format batch_num lr_target batch_num param_group lr atol= e- rtol= use_beta betas param_group keys assertEqual momentum_target batch_num param_group betas msg= Beta wrong batch_num expected got format batch_num momentum_target batch_num param_group betas atol= e- rtol= momentum param_group keys assertEqual momentum_target batch_num param_group momentum msg= Momentum wrong batch_num expected got format batch_num momentum_target batch_num param_group momentum atol= e- rtol= opt step scheduler step test_cosine_then_cyclic https github com pytorch pytorch issues max_lr = base_lr = optim_lr = model = torch nn Linear optimizer = SGD model parameters lr=optim_lr lr_scheduler_ = torch optim lr_scheduler CosineAnnealingLR optimizer T_max= eta_min= lr_scheduler_ = torch optim lr_scheduler CyclicLR optimizer base_lr=base_lr max_lr=max_lr step_size_up= step_size_down= i range optimizer step i = lr_scheduler_ T_max lr_scheduler_ step lr_scheduler_ step last_lr = optimizer param_groups lr assertLessEqual last_lr max_lr parametrize LRClass partial LambdaLR lr_lambda=lambda e e partial MultiplicativeLR lr_lambda=lambda partial StepLR step_size= partial MultiStepLR milestones= ConstantLR LinearLR partial ExponentialLR gamma= PolynomialLR partial CosineAnnealingLR T_max= lambda opt kwargs ChainedScheduler schedulers= ConstantLR opt ConstantLR opt kwargs lambda opt kwargs SequentialLR opt schedulers= ConstantLR opt ConstantLR opt milestones= kwargs ReduceLROnPlateau partial CyclicLR base_lr= max_lr= partial OneCycleLR max_lr= total_steps= anneal_strategy= linear partial CosineAnnealingWarmRestarts T_ = partial SWALR swa_lr= parametrize weights_only True False test_lr_scheduler_state_dict_load LRClass weights_only scheduler = LRClass opt state_dict = scheduler state_dict tempfile TemporaryFile f torch save state_dict f f seek state_dict_loaded = torch load f weights_only=weights_only assertEqual state_dict state_dict_loaded Make sure state_dict can loaded scheduler = LRClass opt scheduler load_state_dict state_dict_loaded assertEqual scheduler state_dict state_dict parametrize min_lr scalar list test_add_param_group_does_not_break_reduce_lr_on_plateau min_lr epochs = param_group opt param_groups param_group lr = targets = + + + metrics = + + scheduler = ReduceLROnPlateau opt mode= min threshold_mode= rel threshold= patience= cooldown= min_lr= min_lr == scalar e- e- epoch range epochs Point test use case epoch == param = torch nn Parameter torch rand opt add_param_group params param lr min_lr == list scheduler min_lrs append e- opt step scheduler step metrics epoch param_group target zip opt param_groups targets assertEqual target epoch param_group lr msg= LR wrong epoch expected got format epoch target epoch param_group lr atol= e- rtol= test_add_param_group_errors_reduce_lr_on_plateau scheduler = ReduceLROnPlateau opt mode= min threshold_mode= rel threshold= e- patience= cooldown= min_lr= e- e- param = torch nn Parameter torch rand opt add_param_group params param lr opt step scheduler step assertRaisesRegex RuntimeError The number param groups opt step scheduler step parametrize LRClass partial LambdaLR lr_lambda=lambda e e partial MultiplicativeLR lr_lambda=lambda e partial StepLR step_size= partial MultiStepLR milestones= ConstantLR LinearLR partial ExponentialLR gamma= PolynomialLR partial CosineAnnealingLR T_max= partial CosineAnnealingWarmRestarts T_ = test_constant_initial_lr LRClass Test initial learning rate constant does alias base_lrs lr = torch as_tensor opt = SGD torch nn Parameter torch randn lr=lr sch = LRClass opt ori_param_groups = copy deepcopy opt param_groups i range opt step sch step i lr multiply_ group ori_group zip opt param_groups ori_param_groups assertEqual group initial_lr ori_group initial_lr assertEqual sch base_lrs assertIsNot sch base_lrs group initial_lr test_constant_initial_params_cyclelr Test initial learning rate constant lr = torch as_tensor max_lr = torch as_tensor base_momentum = torch as_tensor max_momentum = torch as_tensor opt = SGD torch nn Parameter torch randn lr=lr sch = CyclicLR opt base_lr=lr max_lr=max_lr base_momentum=base_momentum max_momentum=max_momentum ori_param_groups = copy deepcopy opt param_groups i range lr multiply_ max_lr multiply_ base_momentum multiply_ max_momentum multiply_ opt step sch step i group ori_group zip opt param_groups ori_param_groups assertEqual group initial_lr ori_group initial_lr assertEqual group max_momentum ori_group max_momentum assertEqual group base_momentum ori_group base_momentum assertEqual sch base_lrs assertEqual sch max_lrs assertEqual group max_momentum assertEqual group base_momentum test_constant_initial_params_onecyclelr Test initial learning rate constant lr = torch as_tensor base_momentum = torch as_tensor max_momentum = torch as_tensor opt = SGD torch nn Parameter torch randn lr=lr sch = OneCycleLR opt max_lr=lr total_steps= base_momentum=base_momentum max_momentum=max_momentum ori_param_groups = copy deepcopy opt param_groups i range lr multiply_ base_momentum multiply_ max_momentum multiply_ opt step sch step i group ori_group zip opt param_groups ori_param_groups assertEqual group initial_lr ori_group initial_lr assertEqual group max_lr ori_group max_lr assertEqual group min_lr ori_group min_lr assertEqual group max_momentum ori_group max_momentum assertEqual group base_momentum ori_group base_momentum assertEqual group max_momentum assertEqual group base_momentum test_constant_initial_params_swalr Test initial learning rate constant lr = torch as_tensor swa_lr = torch as_tensor opt = SGD torch nn Parameter torch randn lr=lr sch = SWALR opt swa_lr=swa_lr ori_param_groups = copy deepcopy opt param_groups _ range lr multiply_ swa_lr multiply_ opt step sch step group ori_group zip opt param_groups ori_param_groups assertEqual group initial_lr ori_group initial_lr assertEqual group swa_lr ori_group swa_lr assertEqual group swa_lr assertEqual sch base_lrs parametrize LRClass partial ExponentialLR gamma= partial LambdaLR lr_lambda=lambda epoch epoch partial MultiplicativeLR lr_lambda=lambda epoch partial StepLR step_size= partial MultiStepLR milestones= ConstantLR LinearLR PolynomialLR partial CosineAnnealingLR T_max= partial CosineAnnealingWarmRestarts T_ = partial CyclicLR base_lr= max_lr= partial OneCycleLR max_lr= total_steps= partial SWALR swa_lr= test_lr_scheduler_checkpoint LRClass model = torch nn Linear optim = torch optim AdamW model parameters sch = LRClass optim optim step sch step optim = torch optim AdamW model parameters optim load_state_dict optim state_dict sch = LRClass optim last_epoch= assertEqual sch _get_closed_form_lr hasattr _get_closed_form_lr sch get_last_lr optim param_groups lr test_lr_scheduler_checkpoint_on_plateau model = torch nn Linear optim = torch optim AdamW model parameters sch = ReduceLROnPlateau optim mode= min optim step sch step optim = torch optim AdamW model parameters optim load_state_dict optim state_dict sch = ReduceLROnPlateau optim mode= min assertEqual sch _get_closed_form_lr hasattr _get_closed_form_lr sch get_last_lr optim param_groups lr instantiate_parametrized_tests TestLRScheduler __name__ == __main__ print These tests should run through test test_optim py instead