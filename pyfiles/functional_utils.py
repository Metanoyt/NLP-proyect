mypy allow-untyped-defs This file contains utilities related functionalization AOTAutograd converting functional tensors detecting Tensor mutations - both metadata Tensor value regenerating replaying views their base checking graph functional i e whether contains any mutation ops __future__ annotations dataclasses dataclass torch torch Tensor torch _C _functionalization torch _logging getArtifactLogger torch _subclasses fake_tensor FakeTensor torch _subclasses functional_tensor FunctionalTensor torch _subclasses meta_utils is_sparse_any torch fx experimental symbolic_shapes guard_or_false sym_eq SymIntEqByExpr torch multiprocessing reductions StorageWeakRef torch utils _python_dispatch is_traceable_wrapper_subclass transform_subclass aot_joint_log = getArtifactLogger __name__ aot_joint_graph to_fun t isinstance t Tensor is_traceable_wrapper_subclass t See Note Functionalization always runs last This means we want functionalize subclass we need ensure functional wrapper goes bottom recurse here so we can support nested wrapper subclasses out = transform_subclass t lambda _ inner_t to_fun inner_t torch _mirror_autograd_meta_to t out type ignore attr-defined out FunctionalTensor to_functional t t sync_functional_tensor t is_traceable_wrapper_subclass t attrs _ctx = t __tensor_flatten__ type ignore attr-defined attr attrs sync_functional_tensor getattr t attr torch _sync t When subclasses involved t here will usually look something like SubclassA SubclassB FunctionalTensor _to_fun_tensor FakeTensor from_fun t isinstance t Tensor is_traceable_wrapper_subclass t See Note Functionalization always runs last This means we want functionalize subclass we need ensure functional wrapper goes bottom recurse here so we can support nested wrapper subclasses out = transform_subclass t lambda _ inner_t from_fun inner_t torch _mirror_autograd_meta_to t out type ignore attr-defined out isinstance t FunctionalTensor quick sanity assert isinstance t torch Tensor assert torch _is_functional_tensor t type ignore attr-defined t sync_functional_tensor t torch _from_functional_tensor t elem is_fun t isinstance t Tensor is_traceable_wrapper_subclass t See Note Functionalization always runs last This means we want functionalize subclass we need ensure functional wrapper goes bottom recurse here so we can support nested wrapper subclasses t_attrs _ = t __tensor_flatten__ type ignore attr-defined t_inners = getattr t attr attr t_attrs any_fun = any is_fun x x t_inners all_fun = all is_fun x x t_inners assert any_fun == all_fun any_fun isinstance t FunctionalTensor t here either A FunctionalTensor _to_functional_tensor FakeTensor A traceable tensor subclass holds FunctionalTensor Not tensor has_data_mutation t is_traceable_wrapper_subclass t attrs _ = t __tensor_flatten__ A tensor subclass updated any its inner elements updated any has_data_mutation getattr t attr attr attrs isinstance t torch Tensor assert isinstance t FunctionalTensor torch _functionalize_has_data_mutation t elem type ignore attr-defined False are_all_mutations_hidden_from_autograd t is_traceable_wrapper_subclass t attrs _ = t __tensor_flatten__ If all inner elements mutations hidden autograd then mutation hidden autograd all are_all_mutations_hidden_from_autograd getattr t attr attr attrs isinstance t torch Tensor assert isinstance t FunctionalTensor torch _functionalize_are_all_mutations_hidden_from_autograd t elem False are_all_mutations_under_no_grad_or_inference_mode t is_traceable_wrapper_subclass t attrs _ = t __tensor_flatten__ all are_all_mutations_under_no_grad_or_inference_mode getattr t attr attr attrs assert isinstance t FunctionalTensor torch _functionalize_are_all_mutations_under_no_grad_or_inference_mode t elem was_inductor_storage_resized t is_traceable_wrapper_subclass t attrs _ = t __tensor_flatten__ any was_inductor_storage_resized getattr t attr attr attrs raise RuntimeError f storage resizing supported tensor subclass type t isinstance t torch Tensor False assert isinstance t FunctionalTensor torch _functionalize_was_inductor_storage_resized t elem f_arg here either A FunctionalTensor _to_functional_tensor FakeTensor A traceable tensor subclass holds FunctionalTensor Not tensor Assumption arg promises original tensor wrapped f_arg Note storage mutations coming set_ type metadata mutation So - check_only_storage_mutation=True only true there storage mutation - check_only_storage_mutation=Flse true there any metadata mutation including storage mutation has_metadata_mutation f_arg arg check_only_storage_mutation bool is_traceable_wrapper_subclass f_arg attrs _ = f_arg __tensor_flatten__ A tensor subclass updated any its inner elements updated f_inner_ts = getattr f_arg attr attr attrs inner_ts = getattr arg attr attr attrs any has_metadata_mutation f_inner_t inner_t check_only_storage_mutation=check_only_storage_mutation f_inner_t inner_t zip f_inner_ts inner_ts isinstance f_arg torch Tensor assert isinstance arg torch Tensor False assert isinstance f_arg FunctionalTensor assert isinstance arg FakeTensor arg_after = torch _from_functional_tensor f_arg elem This true current tensor experienced least one set_ call maybe_storage_changed = torch _functionalize_was_storage_changed f_arg elem type ignore attr-defined However multiple set_ calls can cancel out So we also check whether storage tensor has changed Note input experienced two set_ calls cancel out experiences data mutation we pessimistically think set_ call necessary here We could theory fix will hopefully never happen user code needed fsdp is_sparse_any arg TODO add sparse tensors support functionalization same_storages = False same_storages = StorageWeakRef arg untyped_storage == StorageWeakRef arg_after untyped_storage has_storage_metadata_mutation = maybe_storage_changed same_storages check_only_storage_mutation has_storage_metadata_mutation storage metadata mutation type metadata mutation so true we saw one has_storage_metadata_mutation True maybe_metadata_mutated = torch _functionalize_has_metadata_mutation f_arg elem type ignore attr-defined This true current tensor experienced least one metadata mutation So false we know there no metadata mutation maybe_metadata_mutated False However multi metadata mutations can cancel out So we also check concrete sizes strides tensor have changed same_sizes = arg shape == arg_after shape same_strides = arg stride == arg_after stride same_offsets = arg storage_offset == arg_after storage_offset has_metadata_mutation_ = maybe_metadata_mutated same_sizes same_strides same_offsets We consider tensor have been metadata mutated its storage mutated through set_ call has_metadata_mutation_ gen_alias_from_base aliased_base_tensor target_meta_tensor target_requires_grad target_view_meta_sequence ViewMetaSequence &#124; None = None replay_views bool Patch correct requires_grad field output tensor depending whether i reconstructed output out came tensor requires grad ii concrete returned output does require grad patch_requires_grad out aliased_base_tensor requires_grad target_requires_grad out = out detach aliased_base_tensor requires_grad target_requires_grad out requires_grad_ True out If provided use target functional tensor replaying views In summary we use fact FunctionalTensorWrapper saves view functions applied itself collected during functionalization so replay them view functions aliased_base_tensor replay_views target_view_meta_sequence None any vm has_symbolic_inputs vm target_view_meta_sequence sequence out = _functionalization apply_view_meta_sequence aliased_base_tensor target_view_meta_sequence sequence If re-applying ViewMeta sequence succeeded there should no more problems going forward We just check we got target shape patch requires_grad flag assert out shape == target_meta_tensor shape incorrect out shape after application ViewMeta sequence f tuple out shape actual vs tuple target_meta_tensor shape expected patch_requires_grad out Try do view-replay possible fall back as_strided we can t target_meta_tensor _base None The base we want replay our view off might have different shape than view s original base b = target_meta_tensor _base abt = aliased_base_tensor Don t unnecessarily call as_strided nothing changed as_strided s backward poorly implemented slow abt b abt size = b size abt stride = b stride abt storage_offset = b storage_offset reshaped_base_tensor = aliased_base_tensor as_strided b size b stride b storage_offset reshaped_base_tensor = aliased_base_tensor out = target_meta_tensor _view_func reshaped_base_tensor This shape mismatch can happen due bug inplace view handling autograd Try putting breakpoint here running ` test functorch test_aotdispatch TestAOTAutograd test_output_all_alias_types ` Also https github com pytorch pytorch issues As stopgap we ll fall back as_strided out None out shape == target_meta_tensor shape patch_requires_grad out size = target_meta_tensor size stride = target_meta_tensor stride storage_offset = target_meta_tensor storage_offset aliased_base_tensor is_complex target_meta_tensor is_complex aliased_out = torch view_as_real aliased_base_tensor as_strided size stride storage_offset aliased_base_tensor is_complex target_meta_tensor is_complex aliased_out = torch view_as_complex aliased_base_tensor as_strided size stride storage_offset aliased_out = aliased_base_tensor as_strided size stride storage_offset For outputs aliasing inputs we need check requires-gradness has changed aliased_out = patch_requires_grad aliased_out For outputs aliasing inputs we need check dtype has changed as_strided most generic view does cover cross-dtype views aliased_out dtype = target_meta_tensor dtype aliased_out = aliased_out view target_meta_tensor dtype aliased_out has_same_metadata t t guard_or_false sym_eq t size t size guard_or_false t layout == t layout is_sparse_any t guard_or_false sym_eq t stride t stride guard_or_false t storage_offset == t storage_offset t is_conj == t is_conj t is_neg == t is_neg dataclass frozen=True MetadataKey This should equal whenever has_same_metadata would True size tuple SymIntEqByExpr layout torch layout is_sparse bool these empty when is_sparse stride tuple SymIntEqByExpr &#124; None storage_offset SymIntEqByExpr &#124; None is_conj bool is_neg bool staticmethod make t is_sparse = is_sparse_any t MetadataKey size=tuple SymIntEqByExpr s s t size layout=t layout is_sparse=is_sparse stride=None is_sparse tuple SymIntEqByExpr s s t stride storage_offset=None is_sparse SymIntEqByExpr t storage_offset is_conj=t is_conj is_neg=t is_neg ViewMeta sequence wrapper equality comparisons Even though we can compare each ViewMeta instance we compare resulting tensor metadata instead That s because creation synthetic bases + re-generation input views might end-up creating different sequence ViewMeta semantically equivalent i e gets tensor same metadata Therefore we store what end result should look like serializable metadata When logging should look like ViewMetaSequence view select_int slice_Tensor i e parenthesized list view operations within ViewMeta sequence ViewMetaSequence __init__ tensor FunctionalTensor - None assert torch _is_functional_tensor tensor elem sequence = _functionalization get_view_meta_sequence tensor elem metadata = MetadataKey make tensor __repr__ - str suffix = len _ViewMeta types = join type vm __name__ -suffix vm sequence f ViewMetaSequence types __eq__ other object - bool If other None then probably means we weren t able recreate ViewMeta sequence One example when we update view metadata calling create_synthetic_base_metadata other None True Comparison against any other type implemented isinstance other ViewMetaSequence NotImplemented metadata == other metadata new_arg arg here either both FakeTensor both traceable tensor subclass holds FakeTensor Pre-condition two args old new inputs running functionalization When we run functionalization wrap our inputs into FunctionalTensors we can detect whether input mutated checking see inner tensor has changed Normally would enough just check arg new_arg which normally enough functionalization confirm inputs mutated when running user s model functionalization But when we have subclass inputs we can t rely ` from_fun to_fun x x ` will False because call ` from_fun ` constructs brand new subclass instance we calling __tensor_unflatten__ going Subclass FakeTensor Subclass FunctionalTensor FakeTensor was_tensor_updated arg new_arg is_traceable_wrapper_subclass arg assert is_traceable_wrapper_subclass new_arg attrs _ = arg __tensor_flatten__ new_attrs _ = new_arg __tensor_flatten__ assert attrs == new_attrs A tensor subclass updated any its inner elements updated any was_tensor_updated getattr arg attr getattr new_arg attr attr attrs arg new_arg new_arg arg here either both FakeTensor both traceable tensor subclass holds FakeTensor Pre-condition two args old new inputs running functionalization When we run functionalization wrap our inputs into FunctionalTensors we can detect whether input mutated checking see inner tensor has changed shares storage old input was_tensor_metadata_updated arg new_arg is_traceable_wrapper_subclass arg assert is_traceable_wrapper_subclass new_arg attrs _ = arg __tensor_flatten__ new_attrs _ = new_arg __tensor_flatten__ assert attrs == new_attrs A tensor subclass updated any its inner elements updated any was_tensor_metadata_updated getattr arg attr getattr new_arg attr attr attrs arg new_arg StorageWeakRef arg untyped_storage == StorageWeakRef new_arg untyped_storage Returns number detected copy_ assert_functional_graph fx_g torch fx Graph - int allowed_mutation_ops = torch ops aten copy_ default torch ops aten set_ source_Tensor hasattr torch ops fsdp copy_ allowed_mutation_ops append torch ops fsdp copy_ default placeholders = set mutation_count = NB It would also nice verify mutations all happen end we also do some administrative views after mutations so isn t actually true TODO Could cause problems Inductor n fx_g nodes n op == placeholder placeholders add n isinstance n target torch _ops OpOverload n target allowed_mutation_ops Can only copy_ set_ into input mostly hack avoid failing XLA tests See https github com pytorch pytorch pull #issuecomment- set_buffer_donor_ str n args assert n args placeholders f n= str n n args = str n args placeholders= str placeholders graph= str fx_g mutation_count += assert n target _schema is_mutable f aot_autograd expected have entirely functional graph found n format_node mutation_count propagate_input_mutation_stacktraces fx_g torch fx Graph - None placeholders = set n fx_g nodes n op == placeholder placeholders add n isinstance n target torch _ops OpOverload n target torch ops aten copy_ default Can only copy_ into input can only do so once set_buffer_donor_ str n args assert n args placeholders f n= str n n args = str n args placeholders= str placeholders graph= str fx_g placeholders remove n args copy_from_node = n args Pre-condition every node has stack_trace field its meta copy_ nodes do since we manually added them during functionalization Instead we manually propagate here stack_trace copy_from_node meta n meta stack_trace = copy_from_node meta stack_trace _check_if_mutation_can_be_in_graph keep_input_mutations bool mutates_data mutates_metadata mutations_hidden_from_autograd mutations_under_no_grad_or_inference_mode mutates_storage_metadata mutation_inductor_storage_resize requires_grad keep_input_mutations in_graph = mutates_data mutates_storage_metadata mutation_inductor_storage_resize mutates_metadata requires_grad mutations_hidden_from_autograd mutations_under_no_grad_or_inference_mode in_graph = False See Note set_ Input Mutations AOTAutograd If there ` set_ ` we require all mutations under no_grad so we can safely emit set_ graph runtime resize_ gets same treatment mutation_inductor_storage_resize mutates_storage_metadata op_name = resize_ mutation_inductor_storage_resize set_ assert in_graph f \ Encountered op_name graph input input has other mutations we cannot keep graph This supported today Current state keep_input_mutations= keep_input_mutations mutates_data= mutates_data mutates_metadata= mutates_metadata mutations_hidden_from_autograd= mutations_hidden_from_autograd mutations_under_no_grad_or_inference_mode= mutations_under_no_grad_or_inference_mode mutation_inductor_storage_resize= mutation_inductor_storage_resize requires_grad= requires_grad in_graph