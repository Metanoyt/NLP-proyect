Owner s oncall export ruff noqa F flake noqa itertools subprocess sys unittest torch torch _subclasses fake_tensor FakeTensor FakeTensorMode torch testing _internal common_device_type instantiate_device_type_tests ops torch testing _internal common_methods_invocations onlyCUDA op_db skip skipOps xfail torch testing _internal common_utils run_tests skipIfRocm TestCase torch utils _pytree pytree following failing regular torch export export export_failures = xfail allclose xfail combinations xfail corrcoef xfail cov xfail equal xfail linalg lstsq xfail linalg lstsq grad_oriented xfail nn functional ctc_loss xfail nn functional gaussian_nll_loss xfail sparse sampled_addmm xfail tensor_split following failing fake export cuda device fake_export_failures = xfail geqrf xfail histogram xfail masked amax xfail masked amin xfail masked argmax xfail masked argmin xfail masked logaddexp xfail masked logsumexp xfail masked mean xfail masked prod xfail masked std xfail masked sum xfail masked var xfail nn functional grid_sample xfail to_sparse following failing due OptionalDeviceGuard xfail __getitem__ xfail nn functional batch_norm xfail nn functional instance_norm xfail nn functional multi_margin_loss xfail nonzero fake_decomposition_failures = xfail linalg matrix_rank xfail nn functional binary_cross_entropy_with_logits xfail nn functional instance_norm xfail nn functional multi_margin_loss xfail repeat_interleave xfail take _test_export_helper dtype op sample_inputs_itr = op sample_inputs cpu dtype requires_grad=False mode = FakeTensorMode allow_non_fake_inputs=True target_device = cuda to_fake_device x x target_device Limit first inputs so tests don t take too long sample_input itertools islice sample_inputs_itr args = tuple sample_input input + list sample_input args kwargs = sample_input kwargs hack skip non-tensor args export doesn t support any isinstance arg torch Tensor arg args continue device kwargs kwargs device = target_device mode args kwargs = pytree tree_map_only torch Tensor to_fake_device args kwargs Module torch nn Module forward args op op args kwargs m = Module ep = torch export export m args node ep graph nodes node op == call_function fake_tensor = node meta get val None isinstance fake_tensor FakeTensor assertEqual fake_tensor device torch device target_device TestExportOpInfo TestCase ops op_db allowed_dtypes= torch float skipOps TestExportOpInfo test_fake_export export_failures &#124; fake_export_failures test_fake_export device dtype op _test_export_helper dtype op instantiate_device_type_tests TestExportOpInfo globals only_for= cpu selected_ops = __getitem__ nn functional batch_norm nn functional conv d nn functional instance_norm nn functional multi_margin_loss nn functional scaled_dot_product_attention nonzero selected_op_db = op op op_db op name selected_ops TestExportOnFakeCuda TestCase In CI test runs CUDA machine cuda build We set CUDA_VISIBLE_DEVICES= simulate CPU machine cuda build Running all ops op_db too slow so we only run selected subset onlyCUDA skipIfRocm ops selected_op_db allowed_dtypes= torch float test_fake_export device dtype op test_script = f \ torch itertools torch testing _internal common_methods_invocations op_db torch _subclasses fake_tensor FakeTensor FakeTensorMode torch utils _pytree pytree ops = op op op_db op name == op name assert len ops op ops sample_inputs_itr = op sample_inputs cpu torch float requires_grad=False mode = FakeTensorMode allow_non_fake_inputs=True target_device = cuda to_fake_device x x target_device Limit first inputs so tests don t take too long sample_input itertools islice sample_inputs_itr args = tuple sample_input input + list sample_input args kwargs = sample_input kwargs hack skip non-tensor args export doesn t support any isinstance arg torch Tensor arg args continue device kwargs kwargs device = target_device mode args kwargs = pytree tree_map_only torch Tensor to_fake_device args kwargs Module torch nn Module forward args op op args kwargs m = Module ep = torch export export m args node ep graph nodes node op == call_function fake_tensor = node meta get val None isinstance fake_tensor FakeTensor assert fake_tensor device == torch device target_device r = subprocess check_output sys executable -c test_script env= CUDA_VISIBLE_DEVICES decode ascii strip assertEqual r unittest skipIf torch backends cuda is_built requires CUDA build skipIfRocm test_preserve_original_behavior test_script = f \ torch torch _subclasses fake_tensor FakeTensor FakeTensorMode cuda_calls_behavior_unchanged exception_count = try cpu_x = torch randn cuda_x = cpu_x cuda except Exception e exception_count += try torch randn device= cuda except Exception e exception_count += try torch cuda get_device_capability except Exception e exception_count += try torch cuda set_device except Exception e exception_count += try torch cuda current_device except Exception e exception_count += assert torch cuda is_available == False assert torch cuda device_count == assert exception_count == cuda_calls_behavior_unchanged cpu_x = torch randn FakeTensorMode allow_non_fake_inputs=True mode cuda_x = mode from_tensor cpu_x cuda_x fake_device = torch device cuda cuda_y = cuda_x + cuda_x assert cuda_y device type == cuda should fail again after exiting fake mode identical error message cuda_calls_behavior_unchanged r = subprocess check_output sys executable -c test_script env= CUDA_VISIBLE_DEVICES decode ascii strip assertEqual r instantiate_device_type_tests TestExportOnFakeCuda globals only_for= cuda __name__ == __main__ run_tests