This module contains utility functions explicitly allowed called during TorchDynamo compilation These functions carefully vetted ensure they work correctly within TorchDynamo tracing compilation process Key functionality groups - Compilation State Functions checking compilation state is_compiling - Function Wrapping Utilities wrapping functions wrap_inline wrap_numpy work TorchDynamo compilation - Autograd Hooks Functions classes handling autograd hooks backward passes call_hook FakeBackwardCFunction etc - Tensor Operations Utility functions tensor operations transformations functools warnings collections abc Callable typing Any Optional TYPE_CHECKING TypeVar Union typing_extensions deprecated ParamSpec torch torch utils _pytree pytree try numpy np except ModuleNotFoundError np = None type ignore assignment _P = ParamSpec _P _R = TypeVar _R TYPE_CHECKING TorchScript does support ` deprecated ` This workaround avoid breaking TorchScript deprecated ` torch _dynamo external_utils is_compiling ` deprecated Use ` torch compiler is_compiling ` instead category=FutureWarning is_compiling - bool torch compiler is_compiling is_compiling - bool Indicates whether we tracing compiling torch compile torch export NOTE With ` torch compile backend= eager ` torch _dynamo is_compiling will get traced true torch compiler is_compiling skipped will false torch compiler is_compiling wrap_inline fn Callable _P _R - Callable _P _R Create extra frame around fn skipfiles functools wraps fn inner args _P args kwargs _P kwargs - _R fn args kwargs inner call_hook hook Callable Optional torch Tensor args Any kwargs Any - torch Tensor Used compiled autograd handle hook returning None result = hook args result None args kwargs get hook_type == post_acc_grad_hook raise RuntimeError Tensor post accumulate grad hooks should None result wrap_numpy f Callable _P _R - Callable _P _R r Decorator turns function ` ` np ndarray ` ` s ` ` np ndarray ` ` s into function ` ` torch Tensor ` ` s ` ` torch Tensor ` ` s np f functools wraps f wrap args _P args kwargs _P kwargs - pytree PyTree args kwargs = pytree tree_map_only torch Tensor lambda x x numpy args kwargs pyrefly ignore invalid-param-spec out = f args kwargs pyrefly ignore missing-attribute pytree tree_map_only np ndarray lambda x torch as_tensor x out wrap FakeBackwardCFunction __init__ real torch autograd function BackwardCFunction saved_tensors list torch Tensor - None real = real saved_tensors = saved_tensors __getattr__ name str - Any name == saved_variables warnings warn saved_variables deprecated use saved_tensors DeprecationWarning saved_tensors getattr real name call_backward backward_c_function torch autograd function BackwardCFunction saved_tensors list torch Tensor args Any - Union torch Tensor tuple torch Tensor fake = FakeBackwardCFunction backward_c_function saved_tensors grads = fake _forward_cls backward fake args type ignore attr-defined isinstance grads tuple grads = grads grads normalize_as_list x Any - list Any isinstance x tuple list x isinstance x list x x untyped_storage_size x torch Tensor - int x untyped_storage size FakeCompiledAutogradEngine staticmethod queue_callback final_callbacks list Callable None cb Callable None - None final_callbacks append cb staticmethod exec_final_callbacks final_callbacks list Callable None - None i = while i len final_callbacks cb = final_callbacks i cb i += final_callbacks clear staticmethod _exec_final_callbacks_stub - None pass call_hook_from_backward_state args Any bw_state Any hook_name str kwargs Any - Any getattr bw_state hook_name args kwargs call_module_hooks_from_backward_state _ Any result Any args Any bw_state Any hooks_name str module_name str - Any module = getattr bw_state module_name hooks = getattr bw_state hooks_name hook hooks new_result = hook module result args new_result None result = new_result result used torch _dynamo disable recursive=False get_nonrecursive_disable_wrapper fn Callable _P _R - Callable _P _R wrap function get right error message function external_utils so convert_frame doesn t skip functools wraps fn nonrecursive_disable_wrapper args _P args kwargs _P kwargs - _R fn args kwargs nonrecursive_disable_wrapper wrap_dunder_call_ctx_manager Any func Callable _P _R - Callable _P _R Apply ctx manager around call func NOTE do functools wraps func because we don t ever want frame skipped inner args _P args kwargs _P kwargs - _R func args kwargs inner Use only ints marked dynamic via torch empty integer Currently only way mark ints dynamic https github com pytorch pytorch issues unwrap_maybe_dynamic_int x Union torch Tensor int - int isinstance x torch Tensor x size expected dynamic_int x size x call_accumulate_grad variable torch Tensor grad torch Tensor has_post_hooks bool - None updated_grad = torch _dynamo compiled_autograd ops AccumulateGrad type ignore attr-defined grad variable variable grad has_post_hooks variable grad = updated_grad wrap_inline_with_error_on_graph_break fn Callable _P _R error_on_graph_break bool - Callable _P _R NB need multiple definitions order prevent ` fullgraph ` being freevar wrapper NOTE do functools wraps fn because we don t ever want these wrappers skipped error_on_graph_break wrapper args _P args kwargs _P kwargs - _R torch _dynamo error_on_graph_break True fn args kwargs wrapper args _P args kwargs _P kwargs - _R torch _dynamo error_on_graph_break False fn args kwargs wrapper filter_out_const_values tup tuple Any masks list bool - tuple Any masks list bools where True means corresponding element tup const value Filter out const values out = mask_idx mask enumerate masks mask out append tup mask_idx tuple out insert_const_values_with_mask tup tuple Any masks list bool values tuple Any - tuple Any masks values same length For indices where mask True use const_values fill out = idx = mask_idx mask enumerate masks mask out append values mask_idx out append tup idx idx += tuple out