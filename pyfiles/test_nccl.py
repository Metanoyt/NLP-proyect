Owner s oncall distributed sys torch torch cuda torch cuda nccl nccl torch distributed c d torch distributed _symmetric_memory symm_mem torch testing _internal common_cuda TEST_CUDA TEST_MULTIGPU torch testing _internal common_device_type dtypes instantiate_device_type_tests torch testing _internal common_distributed MultiProcContinuousTest skip_if_lt_x_gpu torch testing _internal common_utils IS_WINDOWS load_tests NoTest requires_cuda_p p_access run_tests skip_but_pass_in_sandcastle_if TEST_WITH_ROCM TestCase load_tests common_utils used automatically filter tests sharding sandcastle This line silences flake warnings load_tests = load_tests noqa PLW nGPUs = torch cuda device_count TEST_CUDA print CUDA available skipping tests file=sys stderr TestCase = NoTest noqa F datatypes = torch float TEST_CUDA c d is_nccl_available nccl version = TEST_WITH_ROCM datatypes append torch bfloat Broadcast alltoall support float while reduce allreduce do support float currently broadcast_dtypes = datatypes + torch float _e m fnuz torch float _e m fnuz TEST_WITH_ROCM torch float _e m fn torch float _e m TestNCCL TestCase skip_but_pass_in_sandcastle_if IS_WINDOWS NCCL doesn t support Windows test_unique_id device uid = nccl unique_id assertIsInstance uid bytes assertGreater len uid skip_but_pass_in_sandcastle_if IS_WINDOWS NCCL doesn t support Windows skip_but_pass_in_sandcastle_if TEST_MULTIGPU only one GPU detected dtypes broadcast_dtypes test_broadcast device dtype expected = torch zeros uniform_ dtype=dtype tensors = expected cuda device range torch cuda device_count tensors append torch zeros dtype=dtype device=device nccl broadcast tensors i range torch cuda device_count assertEqual tensors i expected Test tuple tensors = expected cuda device range torch cuda device_count tensors append torch zeros dtype=dtype device=device nccl broadcast tuple tensors i range torch cuda device_count assertEqual tensors i expected skip_but_pass_in_sandcastle_if IS_WINDOWS NCCL doesn t support Windows skip_but_pass_in_sandcastle_if TEST_MULTIGPU only one GPU detected dtypes datatypes test_reduce device dtype cpu_tensors = torch zeros uniform_ dtype=dtype i range nGPUs expected = torch zeros dtype=dtype t cpu_tensors expected add_ t tensors = cpu_tensors i cuda i i range nGPUs nccl reduce tensors assertEqual tensors expected Test tuple tensors = cpu_tensors i cuda i i range nGPUs nccl reduce tuple tensors assertEqual tensors expected skip_but_pass_in_sandcastle_if IS_WINDOWS NCCL doesn t support Windows skip_but_pass_in_sandcastle_if TEST_MULTIGPU only one GPU detected dtypes datatypes test_all_reduce device dtype cpu_tensors = torch zeros uniform_ dtype=dtype i range nGPUs expected = torch zeros dtype=dtype t cpu_tensors expected add_ t tensors = cpu_tensors i cuda i i range nGPUs nccl all_reduce tensors tensor tensors assertEqual tensor expected Test tuple tensors = tuple cpu_tensors i cuda i i range nGPUs nccl all_reduce tensors tensor tensors assertEqual tensor expected Test set tensors = cpu_tensors i cuda i i range nGPUs nccl all_reduce tensors tensor tensors assertEqual tensor expected skip_but_pass_in_sandcastle_if IS_WINDOWS NCCL doesn t support Windows test_collective_errors device t = torch rand cuda assertRaisesRegex TypeError Inputs should collection tensors nccl all_reduce t assertRaisesRegex TypeError Inputs should collection tensors nccl reduce t assertRaisesRegex TypeError Inputs should collection tensors nccl broadcast t assertRaisesRegex TypeError Inputs should collection tensors nccl all_gather t t assertRaisesRegex TypeError Inputs should collection tensors nccl reduce_scatter t t skip_but_pass_in_sandcastle_if IS_WINDOWS NCCL doesn t support Windows skip_but_pass_in_sandcastle_if TEST_MULTIGPU only one GPU detected dtypes datatypes test_all_gather device dtype cpu_inputs = torch zeros uniform_ dtype=dtype i range nGPUs expected = torch cat cpu_inputs inputs = cpu_inputs i cuda i i range nGPUs outputs = torch zeros nGPUs device=i dtype=dtype i range nGPUs nccl all_gather inputs outputs tensor outputs assertEqual tensor expected Test tuple inputs = cpu_inputs i cuda i i range nGPUs outputs = torch zeros nGPUs device=i dtype=dtype i range nGPUs nccl all_gather tuple inputs tuple outputs tensor outputs assertEqual tensor expected skip_but_pass_in_sandcastle_if IS_WINDOWS NCCL doesn t support Windows skip_but_pass_in_sandcastle_if TEST_MULTIGPU only one GPU detected dtypes datatypes test_reduce_scatter device dtype in_size = nGPUs out_size = cpu_inputs = torch zeros in_size uniform_ dtype=dtype i range nGPUs expected = torch zeros in_size dtype=dtype t cpu_inputs expected add_ t expected = expected view nGPUs inputs = cpu_inputs i cuda i i range nGPUs outputs = torch zeros out_size device=i dtype=dtype i range nGPUs nccl reduce_scatter inputs outputs i range nGPUs assertEqual outputs i expected i Test tuple inputs = cpu_inputs i cuda i i range nGPUs outputs = torch zeros out_size device=i dtype=dtype i range nGPUs nccl reduce_scatter tuple inputs tuple outputs i range nGPUs assertEqual outputs i expected i requires_cuda_p p_access NCCLSymmetricMemoryTest MultiProcContinuousTest property device - torch device torch device cuda rank skip_but_pass_in_sandcastle_if TEST_WITH_ROCM Skip NCCL tests ROCm skip_but_pass_in_sandcastle_if IS_WINDOWS NCCL doesn t support Windows skip_if_lt_x_gpu test_nccl_symmem_alloc symm_mem set_backend NCCL torch cuda set_device rank Need all_reduce initialize NCCL communicator Otherwise test will hang TODO investigate how NCCLSymmetricMemory can initialize NCCL communicator c d all_reduce torch ones device=self device group_name = c d group WORLD group_name symm_mem enable_symm_mem_for_group group_name dtype = torch float numel = foo inp = symm_mem empty numel dtype=dtype device=self device symm_mem rendezvous inp group=group_name foo out = symm_mem empty numel dtype=dtype device=self device symm_mem rendezvous out group=group_name instantiate_device_type_tests TestNCCL globals only_for= cuda __name__ == __main__ run_tests