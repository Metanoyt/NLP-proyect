Owner s oncall distributed __future__ annotations json sys dataclasses dataclass typing Any Optional unittest skipUnless unittest mock mock_open patch torch torch _utils_internal signpost_event torch distributed elastic multiprocessing DefaultLogsSpecs start_processes torch distributed elastic multiprocessing api _wrap torch numa binding _bind_all_threads_in_current_process_to_logical_cpus _get_ranges_str_from_ints _get_set_of_int_from_ranges_str AffinityMode NumaOptions torch testing _internal common_utils run_tests TestCase dataclass frozen=True MockDeviceProperties name str major int minor int total_memory str multi_processor_count int uuid str pci_bus_id int pci_device_id int pci_domain_id int L _cache_size str _real_open = open skipUnless sys platform == linux Only linux currently supported skipUnless torch distributed is_available Need access some distributed submodules NumaBindingTest TestCase setUp - None super setUp _mock_file_path_to_contents dict str str = _mock_device_properties list MockDeviceProperties = _mock_num_logical_cpus = _mock_num_numa_nodes = _mock_num_sockets = _context_managers_to_apply_to_all_tests = patch torch cuda device_count _mock_device_count patch torch cuda get_device_properties _mock_get_device_properties patch torch cuda is_available _mock_is_available Implicitly used dynamo patch torch cuda get_rng_state patch builtins open new=self _mock_open patch os listdir new=self _mock_listdir patch os sched_getaffinity new=self _mock_sched_getaffinity patch torch numa binding signpost_event _mock_signpost_event patch torch numa binding shutil which return_value= usr bin numactl context_manager _context_managers_to_apply_to_all_tests context_manager __enter__ tearDown - None context_manager _context_managers_to_apply_to_all_tests context_manager __exit__ None None None super tearDown _mock_signpost_event args kwargs - None Please keep these parameters JSON serializable logging purposes json dumps kwargs parameters signpost_event args kwargs _add_mock_hardware num_sockets int num_numa_nodes_per_socket int num_gpus_per_numa_node int num_l _caches_per_numa_node int num_physical_core_per_l _cache int - None It s fun we mock everything down sysfs level make sure we get really thorough coverage socket_index range num_sockets numa_node_index range _mock_num_numa_nodes _mock_num_numa_nodes + num_numa_nodes_per_socket _mock_file_contents file_path=f sys devices system node node numa_node_index cpulist contents=f _mock_num_logical_cpus - + f _mock_num_logical_cpus + num_l _caches_per_numa_node num_physical_core_per_l _cache - gpu_index range len _mock_device_properties len _mock_device_properties + num_gpus_per_numa_node device_properties = MockDeviceProperties name=f mock_gpu_ gpu_index major= minor= total_memory= GB multi_processor_count= uuid=f mock_gpu_uuid_ gpu_index pci_bus_id=gpu_index pci_device_id=gpu_index pci_domain_id=gpu_index L _cache_size= MB _mock_device_properties append device_properties pci_numa_node_path = _get_corresponding_pci_numa_node_file_path device_properties=device_properties _mock_file_contents file_path=pci_numa_node_path contents=str numa_node_index _ range num_l _caches_per_numa_node lowest_logical_cpu_index_on_l = _mock_num_logical_cpus highest_logical_cpu_index_on_l = _mock_num_logical_cpus + num_physical_core_per_l _cache - logical_cpu_index range _mock_num_logical_cpus _mock_num_logical_cpus Assume hyperthreaded + num_physical_core_per_l _cache thread_siblings_range_str = f logical_cpu_index - - logical_cpu_index logical_cpu_index f logical_cpu_index - logical_cpu_index + _mock_file_contents file_path=f sys devices system cpu cpu logical_cpu_index topology thread_siblings_list contents=thread_siblings_range_str Unrelated file our logic should know skip _mock_file_contents file_path=f sys devices system cpu cpu logical_cpu_index cache paulwuzhere contents= Data _mock_file_contents file_path=f sys devices system cpu cpu logical_cpu_index topology physical_package_id contents=str socket_index cache_level range _mock_file_contents file_path=f sys devices system cpu cpu logical_cpu_index cache index cache_level type contents= ShouldSkip cache_level == Data _mock_file_contents file_path=f sys devices system cpu cpu logical_cpu_index cache index cache_level level contents=str cache_level _mock_file_contents file_path=f sys devices system cpu cpu logical_cpu_index cache index cache_level shared_cpu_list contents= f lowest_logical_cpu_index_on_l - highest_logical_cpu_index_on_l cache_level == Assume L - per physical core thread_siblings_range_str _mock_num_logical_cpus += _mock_num_numa_nodes += _mock_num_sockets += _mock_file_contents file_path= sys devices system node possible contents=f - _mock_num_numa_nodes - _mock_is_available - bool len _mock_device_properties _get_corresponding_pci_numa_node_file_path device_properties MockDeviceProperties - str pci_addr = f device_properties pci_domain_id x + f device_properties pci_bus_id x device_properties pci_device_id x f sys bus pci devices pci_addr numa_node _mock_file_contents file_path str contents str - None _mock_file_path_to_contents file_path = contents _mock_device_count - int len _mock_device_properties _mock_get_device_properties index int - MockDeviceProperties _mock_device_properties index _mock_open path str args kwargs - Any path _mock_file_path_to_contents mock_open read_data=self _mock_file_path_to_contents path isinstance path str path startswith sys raise FileNotFoundError f File path mocked Looks like CI calling open intending open actual file some places Need make CI pass _real_open path args kwargs _mock_listdir target_path str - set str target_path endswith target_path += mock_path split target_path split mock_path _mock_file_path_to_contents mock_path startswith target_path _mock_sched_getaffinity pid int - set int set range _mock_num_logical_cpus _start_processes_for_str_entrypoint_and_get_command_args numa_options Optional NumaOptions target_local_rank int - tuple str patch torch distributed elastic multiprocessing subprocess_handler subprocess_handler Popen mock_popen start_processes name= test_process entrypoint= echo args=dict fromkeys range _mock_device_count Hello world envs= i LOCAL_RANK str i i range _mock_device_count logs_specs=DefaultLogsSpecs numa_options=numa_options call_args = next call_args call_args mock_popen call_args_list call_args kwargs get env get LOCAL_RANK == str target_local_rank call_args kwargs args _start_processes_for_callable_entrypoint_and_get_sched_setaffinity_cpus numa_options Optional NumaOptions target_local_rank int - Optional set int target_sched_setaffinity_logical_cpu_indices = None mock_sched_setaffinity args kwargs - None nonlocal target_sched_setaffinity_logical_cpu_indices target_sched_setaffinity_logical_cpu_indices = args dummy_fn pass torch multiprocessing mp ctx = mp get_context mock_queue = ctx SimpleQueue mock_event = ctx Event patch os sched_setaffinity mock_sched_setaffinity mock_event set Prevent hanging This entrypoint subprocesses Callable entrypoints _wrap local_rank=target_local_rank fn=dummy_fn args= target_local_rank envs= target_local_rank stdout_redirects= target_local_rank stderr_redirects= target_local_rank ret_vals= target_local_rank mock_queue queue_finished_reading_event=mock_event numa_options=numa_options target_sched_setaffinity_logical_cpu_indices test_node_numa_binding - None _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= command_args = _start_processes_for_str_entrypoint_and_get_command_args numa_options=NumaOptions affinity_mode=AffinityMode NODE target_local_rank= assertEqual command_args There numa nodes GPUs per numa node so GPU would numa node = Each numa node has = logical CPUs Numa node has CPUs - numactl -- physcpubind= - echo Hello world test_no_numa_binding_if_numa_options_not_provided - None _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= command_args = _start_processes_for_str_entrypoint_and_get_command_args numa_options=None target_local_rank= assertEqual command_args echo Hello world test_default_numa_binding - None Inner avoid crashing torch distributed is_available torch distributed launcher api LaunchConfig _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= patch torch distributed launcher api get_default_numa_options return_value=NumaOptions affinity_mode=AffinityMode NODE should_fall_back_if_binding_fails=True launch_config = LaunchConfig min_nodes= max_nodes= nproc_per_node= Don t provide numa_options assertEqual launch_config numa_options NumaOptions affinity_mode=AffinityMode NODE should_fall_back_if_binding_fails=True test_fallback - None _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= patch torch numa binding signpost_event signpost_patch patch torch numa binding _get_numa_node_index_for_gpu_index side_effect=Exception Mock exception command_args = _start_processes_for_str_entrypoint_and_get_command_args numa_options=NumaOptions affinity_mode=AffinityMode NODE should_fall_back_if_binding_fails=True target_local_rank= assertIn Mock exception signpost_patch call_args kwargs parameters traceback assertEqual command_args echo Hello world test_fallback_if_numactl_not_available - None _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= patch torch numa binding signpost_event signpost_patch patch torch numa binding shutil which return_value=None command_args = _start_processes_for_str_entrypoint_and_get_command_args numa_options=NumaOptions affinity_mode=AffinityMode NODE should_fall_back_if_binding_fails=True target_local_rank= assertIn numactl CLI required NUMA binding signpost_patch call_args kwargs parameters traceback assertEqual command_args echo Hello world test_explicit_numa_options_overrides_default - None Inner avoid crashing torch distributed is_available torch distributed launcher api LaunchConfig patch torch distributed launcher api get_default_numa_options return_value=NumaOptions affinity_mode=AffinityMode NODE launch_config = LaunchConfig min_nodes= max_nodes= nproc_per_node= numa_options=NumaOptions affinity_mode=AffinityMode EXCLUSIVE assertEqual launch_config numa_options NumaOptions affinity_mode=AffinityMode EXCLUSIVE test_nproc_must_equal_cuda_device_count_to_use_default_numa_options - None Inner avoid crashing torch distributed is_available torch distributed launcher api LaunchConfig _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= patch torch distributed launcher api get_default_numa_options mock_get_default_numa_options launch_config = LaunchConfig min_nodes= max_nodes= nproc_per_node= mock_get_default_numa_options assert_not_called assertIsNone launch_config numa_options test_socket_numa_binding_with_multiple_numa_per_socket - None _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= command_args = _start_processes_for_str_entrypoint_and_get_command_args numa_options=NumaOptions affinity_mode=AffinityMode SOCKET target_local_rank= assertEqual command_args GPU numa node = which socket numa nodes Each numa node has = logical CPUs Numa nodes have CPUs - - numactl -- physcpubind= - echo Hello world test_socket_numa_binding_with_single_numa_per_socket - None _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= command_args = _start_processes_for_str_entrypoint_and_get_command_args numa_options=NumaOptions affinity_mode=AffinityMode SOCKET target_local_rank= assertEqual command_args GPU numa node = which socket itself Each numa node has = logical CPUs Numa node has CPUs - numactl -- physcpubind= - echo Hello world test_exclusive_numa_binding - None _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= command_args_ = _start_processes_for_str_entrypoint_and_get_command_args numa_options=NumaOptions affinity_mode=AffinityMode EXCLUSIVE target_local_rank= assertEqual command_args_ Gets extra physical core due odd number physical cores numa node physical cores total GPUs GPU gets physical cores CPUs - numactl -- physcpubind= - echo Hello world command_args_ = _start_processes_for_str_entrypoint_and_get_command_args numa_options=NumaOptions affinity_mode=AffinityMode EXCLUSIVE target_local_rank= assertEqual command_args_ Does get extra physical core since st GPU already took extra GPU gets physical core CPUs - numactl -- physcpubind= - echo Hello world test_exclusive_raises_if_too_few_physical_cores - None _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= assertRaisesRegex RuntimeError There only physical cores numa_node_index= there GPUs associated NUMA node _start_processes_for_str_entrypoint_and_get_command_args numa_options=NumaOptions affinity_mode=AffinityMode EXCLUSIVE target_local_rank= test_core_complex_numa_binding_with_extra_l - None _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= command_args = _start_processes_for_str_entrypoint_and_get_command_args numa_options=NumaOptions affinity_mode=AffinityMode CORE_COMPLEX target_local_rank= assertEqual command_args GPU numa node = relative GPU index = The second L second numa node numa node Second numa node starts CPU second L cache CPUs - numactl -- physcpubind= - echo Hello world test_core_complex_numa_binding_with_fewer_l _than_gpu - None _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= command_args = _start_processes_for_str_entrypoint_and_get_command_args numa_options=NumaOptions affinity_mode=AffinityMode CORE_COMPLEX target_local_rank= assertEqual command_args GPU numa node = relative GPU index = With L cache per numa node GPU uses L cache index = only cache Second numa node starts CPU single L cache spans CPUs - numactl -- physcpubind= - echo Hello world test_core_complex_prefers_caches_with_more_cpus - None _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= Only some subset CPUs available time patch os sched_getaffinity return_value= command_args = _start_processes_for_str_entrypoint_and_get_command_args numa_options=NumaOptions affinity_mode=AffinityMode CORE_COMPLEX target_local_rank= Binds second L because has most available CPUs assertEqual command_args numactl -- physcpubind= - echo Hello world test_core_complex_tiebreak_prefers_lower_cache_key - None When several maxâ€‘level caches expose same number logical CPUs prioritize binding caches lower cpu indices first _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= command_args = _start_processes_for_str_entrypoint_and_get_command_args numa_options=NumaOptions affinity_mode=AffinityMode CORE_COMPLEX target_local_rank= numa node L caches physical core per L cache = logical CPUs per cache L cache CPUs - L cache CPUs - Both have same number CPUs so prefer lower cache key assertEqual command_args numactl -- physcpubind= - echo Hello world test_binds_to_node_ _if_node_stored_as_minus_one - None _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= device_ _properties = _mock_get_device_properties Overwrite existing mock file _mock_file_contents file_path=self _get_corresponding_pci_numa_node_file_path device_properties=device_ _properties contents= - command_args = _start_processes_for_str_entrypoint_and_get_command_args numa_options=NumaOptions affinity_mode=AffinityMode NODE target_local_rank= GPU has numa node stored - which treated numa node Each numa node has = logical CPUs Numa node has CPUs - assertEqual command_args numactl -- physcpubind= - echo Hello world test_callable_entrypoint_basic - None _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= bound_logical_cpu_indices = _start_processes_for_callable_entrypoint_and_get_sched_setaffinity_cpus numa_options=NumaOptions affinity_mode=AffinityMode NODE target_local_rank= assertEqual bound_logical_cpu_indices There numa nodes GPUs per numa node so GPU would numa node = Each numa node has = logical CPUs Numa node has CPUs - set range test_raises_if_binding_to_empty_set - None _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= patch torch numa binding _get_logical_cpus_to_bind_to return_value=set assertRaisesRegex RuntimeError Must bind non-empty set CPU indices _start_processes_for_callable_entrypoint_and_get_sched_setaffinity_cpus numa_options=NumaOptions affinity_mode=AffinityMode NODE target_local_rank= test_get_set_of_int_from_ranges_str - None assertEqual _get_set_of_int_from_ranges_str - - test_get_range_str_from_ints - None assertEqual _get_ranges_str_from_ints - - test_bind_all_threads_in_current_process_to_logical_cpus - None _add_mock_hardware num_sockets= num_numa_nodes_per_socket= num_gpus_per_numa_node= num_l _caches_per_numa_node= num_physical_core_per_l _cache= _mock_file_contents file_path= proc task contents= _mock_file_contents file_path= proc task contents= _mock_file_contents The exception casting not_an_integer int should get silenced file_path= proc task not_an_integer contents= Mock original affinity all threads original_main_thread_affinity = set range manually_reaffinitized_thread_affinity = mock_sched_getaffinity_impl tid int - set int tid == manually_reaffinitized_thread_affinity original_main_thread_affinity call_order = mock_sched_setaffinity_impl tid int cpus set int - None call_order append tid cpus patch os sched_getaffinity side_effect=mock_sched_getaffinity_impl patch os sched_setaffinity side_effect=mock_sched_setaffinity_impl mock_sched_setaffinity _bind_all_threads_in_current_process_to_logical_cpus logical_cpu_indices= arbitrary Should set affinity main thread thread same affinity main Should NOT set affinity thread manually reaffinitized assertEqual mock_sched_setaffinity call_count mock_sched_setaffinity assert_any_call mock_sched_setaffinity assert_any_call Verify thread NOT reaffinitized tid _ call_order assertNotEqual tid Thread should have been reaffinitized __name__ == __main__ run_tests