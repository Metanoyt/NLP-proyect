torch torch nn nn configurable bsz = imgsz = nz = ngf = ndf = nc = custom weights initialization called netG netD weights_init m classname = m __class__ __name__ classname find Conv = - m weight data normal_ classname find BatchNorm = - m weight data normal_ m bias data fill_ _netG nn Module __init__ ngpu super __init__ ngpu = ngpu main = nn Sequential input Z going into convolution nn ConvTranspose d nz ngf bias=False nn BatchNorm d ngf nn ReLU True state size ngf x x nn ConvTranspose d ngf ngf bias=False nn BatchNorm d ngf nn ReLU True state size ngf x x nn ConvTranspose d ngf ngf bias=False nn BatchNorm d ngf nn ReLU True state size ngf x x nn ConvTranspose d ngf ngf bias=False nn BatchNorm d ngf nn ReLU True state size ngf x x nn ConvTranspose d ngf nc bias=False nn Tanh state size nc x x forward input ngpu isinstance input data torch cuda FloatTensor output = nn parallel data_parallel main input range ngpu output = main input output _netD nn Module __init__ ngpu super __init__ ngpu = ngpu main = nn Sequential input nc x x nn Conv d nc ndf bias=False nn LeakyReLU inplace=True state size ndf x x nn Conv d ndf ndf bias=False nn BatchNorm d ndf nn LeakyReLU inplace=True state size ndf x x nn Conv d ndf ndf bias=False nn BatchNorm d ndf nn LeakyReLU inplace=True state size ndf x x nn Conv d ndf ndf bias=False nn BatchNorm d ndf nn LeakyReLU inplace=True state size ndf x x nn Conv d ndf bias=False nn Sigmoid forward input ngpu isinstance input data torch cuda FloatTensor output = nn parallel data_parallel main input range ngpu output = main input output view -