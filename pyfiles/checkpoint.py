mypy allow-untyped-defs contextlib platform uuid warnings weakref collections defaultdict typing noqa F enum weakref ReferenceType torch torch fx traceback fx_traceback torch utils _pytree tree_map torch testing _internal logging_tensor capture_logs LoggingTensorMode torch utils _python_dispatch TorchDispatchMode __all__ = checkpoint checkpoint_sequential CheckpointError CheckpointFunction check_backward_validity detach_variable get_device_states set_device_states noop_context_fn set_checkpoint_early_stop DefaultDeviceType set_checkpoint_debug_enabled CheckpointPolicy SelectiveCheckpointContext create_selective_checkpoint_contexts SAC_IGNORED_OPS _DEFAULT_DETERMINISM_MODE = default _checkpoint_debug_enabled Optional bool = None contextlib contextmanager set_checkpoint_debug_enabled enabled Optional bool Context manager sets whether checkpoint should print additional debug information when running See ` ` debug ` ` flag func ` ~torch utils checkpoint checkpoint ` more information Note when set context manager overrides value ` ` debug ` ` passed checkpoint To defer local setting pass ` ` None ` ` context Args enabled bool Whether checkpoint should print debug information Default None global _checkpoint_debug_enabled try prev = _checkpoint_debug_enabled _checkpoint_debug_enabled = enabled yield finally _checkpoint_debug_enabled = prev detach_variable inputs Tuple Any - Tuple torch Tensor isinstance inputs tuple out = inp inputs isinstance inp torch Tensor out append inp continue x = inp detach x requires_grad = inp requires_grad out append x tuple out raise RuntimeError Only tuple tensors supported Got Unsupported input type type inputs __name__ check_backward_validity inputs Iterable Any - None any inp requires_grad inp inputs isinstance inp torch Tensor warnings warn None inputs have requires_grad=True Gradients will None stacklevel= _get_device_module device= cuda device == meta torch device meta device_module = getattr torch device device_module DefaultDeviceType r A manages default device type checkpointing If no non-CPU tensors present default device type will used The default value cuda The device type used checkpointing process when determining which device states save restore recomputation _default_device_type = cuda staticmethod set_device_type device str = cuda Set default device type checkpointing Args device str The device type set default Default cuda DefaultDeviceType _default_device_type = device staticmethod get_device_type - str Get current default device type checkpointing Returns str The current default device type DefaultDeviceType _default_device_type _infer_device_type args device_types = add_device_types arg nonlocal device_types isinstance arg torch Tensor arg device type = cpu device_types append arg device type tree_map add_device_types args device_types_set = set device_types len device_types_set warnings warn Tensor arguments excluding CPU tensors detected least two types devices Device state will only saved devices single device type remaining devices will ignored Consequently any checkpointed functions involve randomness may result incorrect gradients Note CUDA devices among devices detected will prioritized otherwise first device encountered will selected f \nDevice types sorted device_types_set first device type device_types stacklevel= len device_types == DefaultDeviceType get_device_type cuda device_types_set cuda device_types We can t know run_fn will internally move some args different devices which would require logic preserve rng states those devices well We could paranoically stash restore ALL rng states all visible devices seems very wasteful most cases Compromise Stash RNG state device all Tensor args To consider maybe get_device_states set_device_states should reside torch random py get_device_states args - Tuple List int List torch Tensor This will error out arg CPU tensor non-tensor type because conditionals short-circuit fwd_device_ids = add_device_ids arg nonlocal fwd_device_ids isinstance arg torch Tensor arg device type cpu meta fwd_device_ids append arg get_device tree_map add_device_ids args fwd_device_states = device_module = _get_device_module _infer_device_type args device_id fwd_device_ids device_module device device_id fwd_device_states append device_module get_rng_state fwd_device_ids fwd_device_states set_device_states devices states device_type=None - None Sets random number generator states specified devices Args devices Device ids set states states States set device_type ` ` device_type ` ` devices set states Default device returned call ` ` DefaultDeviceType get_device_type ` ` which ` ` cuda ` ` changed calling ` ` DefaultDeviceType set_device_type ` ` device_type None device_type = DefaultDeviceType get_device_type device_type == meta device_module = _get_device_module device_type device state zip devices states strict=False device_module device device device_module set_rng_state state _get_autocast_kwargs device_type= cuda torch amp is_autocast_available device_type device_autocast_kwargs = enabled torch is_autocast_enabled device_type dtype torch get_autocast_dtype device_type cache_enabled torch is_autocast_cache_enabled device_autocast_kwargs = None cpu_autocast_kwargs = enabled torch is_autocast_enabled cpu dtype torch get_autocast_dtype cpu cache_enabled torch is_autocast_cache_enabled device_autocast_kwargs cpu_autocast_kwargs CheckpointFunction torch autograd Function staticmethod pyrefly ignore bad-override forward ctx run_function preserve_rng_state args check_backward_validity args ctx run_function = run_function ctx preserve_rng_state = preserve_rng_state Accommodates remote possibility autocast enabled cpu AND gpu ctx device_type = _infer_device_type args ctx device_autocast_kwargs ctx cpu_autocast_kwargs = _get_autocast_kwargs ctx device_type preserve_rng_state ctx fwd_cpu_state = torch get_rng_state Don t eagerly initialize cuda context accident If user intends context initialized later within their run_function we SHOULD actually stash cuda state here Unfortunately we have no way anticipate will happen before we run function ctx had_device_in_fwd = False device_module = _get_device_module ctx device_type getattr device_module _initialized False ctx had_device_in_fwd = True ctx fwd_devices ctx fwd_device_states = get_device_states args Save non-tensor inputs ctx keep placeholder None tensors filled out during backward ctx inputs = ctx tensor_indices = tensor_inputs = i arg enumerate args torch is_tensor arg tensor_inputs append arg ctx tensor_indices append i ctx inputs append None ctx inputs append arg ctx save_for_backward tensor_inputs torch no_grad outputs = run_function args outputs staticmethod backward ctx args torch autograd _is_checkpoint_valid raise RuntimeError When use_reentrant=True torch utils checkpoint incompatible grad passing ` inputs ` parameter backward To resolve error you can either set use_reentrant=False call backward without passing ` inputs ` argument Copy list avoid modifying original list inputs = list ctx inputs tensor_indices = ctx tensor_indices tensors = ctx saved_tensors Fill inputs appropriate saved tensors i idx enumerate tensor_indices inputs idx = tensors i Stash surrounding rng state mimic state present time during forward Restore surrounding state when we re done rng_devices = ctx preserve_rng_state ctx had_device_in_fwd rng_devices = ctx fwd_devices torch random fork_rng devices=rng_devices enabled=ctx preserve_rng_state device_type=ctx device_type ctx preserve_rng_state torch set_rng_state ctx fwd_cpu_state ctx had_device_in_fwd set_device_states ctx fwd_devices ctx fwd_device_states device_type=ctx device_type detached_inputs = detach_variable tuple inputs device_autocast_ctx = torch amp autocast device_type=ctx device_type ctx device_autocast_kwargs torch amp is_autocast_available ctx device_type contextlib nullcontext torch enable_grad device_autocast_ctx torch amp autocast cpu ctx cpu_autocast_kwargs type ignore attr-defined outputs = ctx run_function detached_inputs isinstance outputs torch Tensor outputs = outputs run backward only tensor requires grad outputs_with_grad = args_with_grad = i range len outputs torch is_tensor outputs i outputs i requires_grad outputs_with_grad append outputs i args_with_grad append args i len outputs_with_grad == raise RuntimeError none output has requires_grad=True checkpoint necessary torch autograd backward outputs_with_grad args_with_grad grads = tuple inp grad isinstance inp torch Tensor None inp detached_inputs None None + grads noop_context_fn contextlib nullcontext contextlib nullcontext Note torch compile checkpoint TorchDynamo does step inside utils checkpoint function The flow looks likes TorchDynamo tries wrap utils checkpoint HigherOrderOp speculatively checking forward function safe trace If yes then Dynamo-generated Fx graph has wrapped higher order op As result TorchDynamo does look inside utils checkpoint If then TorchDynamo falls back eager performing graph break And here following disable wrapper ensures TorchDynamo does trigger again frames created utils checkpoint innards torch _disable_dynamo checkpoint function args use_reentrant Optional bool = None context_fn Callable Tuple ContextManager ContextManager = noop_context_fn determinism_check str = _DEFAULT_DETERMINISM_MODE debug bool = False early_stop bool = True kwargs r Checkpoint model part model Activation checkpointing technique trades compute memory Instead keeping tensors needed backward alive until they used gradient computation during backward forward computation checkpointed regions omits saving tensors backward recomputes them during backward pass Activation checkpointing can applied any part model There currently two checkpointing implementations available determined attr ` use_reentrant ` parameter It recommended you use ` ` use_reentrant=False ` ` Please refer note below discussion their differences warning If attr ` function ` invocation during backward pass differs forward pass e g due global variable checkpointed version may equivalent potentially causing error being raised leading silently incorrect gradients warning The ` ` use_reentrant ` ` parameter should passed explicitly In version we will raise exception ` ` use_reentrant ` ` passed If you using ` ` use_reentrant=True ` ` variant please refer note below important considerations potential limitations note The reentrant variant checkpoint ` ` use_reentrant=True ` ` non-reentrant variant checkpoint ` ` use_reentrant=False ` ` differ following ways Non-reentrant checkpoint stops recomputation soon all needed intermediate activations have been recomputed This feature enabled default can disabled func ` set_checkpoint_early_stop ` Reentrant checkpoint always recomputes attr ` function ` its entirety during backward pass The reentrant variant does record autograd graph during forward pass runs forward pass under func ` torch no_grad ` The non-reentrant version does record autograd graph allowing one perform backward graph within checkpointed regions The reentrant checkpoint only supports func ` torch autograd backward ` API backward pass without its ` inputs ` argument while non-reentrant version supports all ways performing backward pass At least one input output must have ` ` requires_grad=True ` ` reentrant variant If condition unmet checkpointed part model will have gradients The non-reentrant version does have requirement The reentrant version does consider tensors nested structures e g custom objects lists dicts etc participating autograd while non-reentrant version does The reentrant checkpoint does support checkpointed regions detached tensors computational graph whereas non-reentrant version does For reentrant variant checkpointed segment contains tensors detached using ` ` detach ` ` func ` torch no_grad ` backward pass will raise error This because ` ` checkpoint ` ` makes all outputs require gradients causes issues when tensor defined have no gradient model To avoid detach tensors outside ` ` checkpoint ` ` function Args function describes what run forward pass model part model It should also know how handle inputs passed tuple For example LSTM user passes ` ` activation hidden ` ` attr ` function ` should correctly use first input ` ` activation ` ` second input ` ` hidden ` ` args tuple containing inputs attr ` function ` Keyword args preserve_rng_state bool optional Omit stashing restoring RNG state during each checkpoint Note under torch compile flag doesn t take effect we always preserve RNG state Default ` ` True ` ` use_reentrant bool specify whether use activation checkpoint variant requires reentrant autograd This parameter should passed explicitly In version we will raise exception ` ` use_reentrant ` ` passed If ` ` use_reentrant=False ` ` ` ` checkpoint ` ` will use implementation does require reentrant autograd This allows ` ` checkpoint ` ` support additional functionality such working expected ` ` torch autograd grad ` ` support keyword arguments input into checkpointed function context_fn Callable optional A callable returning tuple two context managers The function its recomputation will run under first second context managers respectively This argument only supported ` ` use_reentrant=False ` ` determinism_check str optional A string specifying determinism check perform By default set ` ` default ` ` which compares shapes dtypes devices recomputed tensors against those saved tensors To turn off check specify ` ` none ` ` Currently these only two supported values Please open issue you would like see more determinism checks This argument only supported ` ` use_reentrant=False ` ` ` ` use_reentrant=True ` ` determinism check always disabled debug bool optional If ` ` True ` ` error messages will also include trace operators ran during original forward computation well recomputation This argument only supported ` ` use_reentrant=False ` ` early_stop bool optional If ` ` True ` ` non-reentrant checkpoint stops recomputation soon has computed all needed Tensors This argument ignored ` ` use_reentrant=True ` ` Can overridden globally using func ` set_checkpoint_early_stop ` context manager Default ` ` True ` ` Returns Output running attr ` function ` attr ` args ` use_reentrant None warnings warn torch utils checkpoint use_reentrant parameter should passed explicitly Starting PyTorch calling checkpoint without use_reentrant will raise exception use_reentrant=False recommended you need preserve current default behavior you can pass use_reentrant=True Refer docs more details differences between two variants stacklevel= use_reentrant = True Hack mix args kwargs python -compliant way preserve = kwargs pop preserve_rng_state True kwargs use_reentrant raise ValueError Unexpected keyword arguments + join arg arg kwargs use_reentrant context_fn noop_context_fn debug False raise ValueError Passing ` context_fn ` ` debug ` only supported when use_reentrant=False CheckpointFunction apply function preserve args gen = _checkpoint_without_reentrant_generator function preserve context_fn determinism_check debug early_stop args kwargs Runs pre-forward logic next gen ret = function args kwargs Runs post-forward logic try next gen except StopIteration ret checkpoint_sequential functions segments input use_reentrant=None kwargs r Checkpoint sequential model save memory Sequential models execute list modules functions order sequentially Therefore we can divide such model various segments checkpoint each segment All segments except last will store intermediate activations The inputs each checkpointed segment will saved re-running segment backward pass warning The ` ` use_reentrant ` ` parameter should passed explicitly In version we will raise exception ` ` use_reentrant ` ` passed If you using ` ` use_reentrant=True ` variant please see func ` ~torch utils checkpoint checkpoint ` important considerations limitations variant It recommended you use ` ` use_reentrant=False ` ` warning Since PyTorch allows only one Tensor input intermediate outputs just like ` torch nn Sequential ` Args functions A ` torch nn Sequential ` list modules functions comprising model run sequentially segments Number chunks create model input A Tensor input attr ` functions ` preserve_rng_state bool optional Omit stashing restoring RNG state during each checkpoint Default ` ` True ` ` use_reentrant bool specify whether use activation checkpoint variant requires reentrant autograd This parameter should passed explicitly In version we will raise exception ` ` use_reentrant ` ` passed If ` ` use_reentrant=False ` ` ` ` checkpoint ` ` will use implementation does require reentrant autograd This allows ` ` checkpoint ` ` support additional functionality such working expected ` ` torch autograd grad ` ` support keyword arguments input into checkpointed function Returns Output running attr ` functions ` sequentially attr ` inputs ` Example xdoctest +SKIP stub model = nn Sequential input_var = checkpoint_sequential model chunks input_var use_reentrant None warnings warn torch utils checkpoint checkpoint_sequential use_reentrant parameter should passed explicitly In version we will raise exception use_reentrant passed use_reentrant=False recommended you need preserve current default behavior you can pass use_reentrant=True Refer docs more details differences between two variants stacklevel= use_reentrant = True Hack keyword-only parameter python -compliant way preserve = kwargs pop preserve_rng_state True kwargs raise ValueError Unexpected keyword arguments + join arg arg kwargs run_function start end functions forward input j range start end + input = functions j input input forward isinstance functions torch nn Sequential functions = list functions children segment_size = len functions segments last chunk has non-volatile end = - start range segment_size segments - segment_size end = start + segment_size - input = checkpoint run_function start end functions input use_reentrant=use_reentrant preserve_rng_state=preserve run_function end + len functions - functions input _internal_assert cond cond raise AssertionError Something went unexpectedly wrong activation checkpoint Please report bug filing issue PyTorch NOTE Nestable Checkpoint The semantics nested checkpoint can defined two basic rules Following two rules leads important implication central motivating design Rule Saved tensors managed inner-most checkpoint only hidden any outer layers checkpoint Rule The inputs inner checkpoints treated tensors saved its parent checkpoint Implication To recompute any given saved tensor we need recompute all checkpoints wrapping Why implied To unpack saved tensor X during backward we need recompute inner-most checkpoint order recompute checkpoint I need have its inputs which managed checkpoint s parent which thus also needs recomputed first Continue line reasoning we realize order unpack X all checkpoints active time X saved need recomputed unless we have already done so backward some other saved tensor In practice we use noop autograd Function save inputs saved tensors During unpack calling ctx saved_tensor triggers parent checkpoint recompute Rule We should start recomputation there no checkpoints currently active Checkpoints encountered during recomputation still respected When we start recomputation we push saved variable hook meant recomputation stack See examples Rule more context Beyond basic semantics specific nested checkpoint we impose several more constraints may apply checkpointing general Rule Lifetime recomputed tensors Recomputed tensors considered specific particular invocations backward always cleared immediately they unpacked Particularly we require happen even retain_graph=True Implementation details Rule If we okay recomputed tensors staying alive after backward run retain_graph=True we would store recomputed variables values WeakKeyDictionary pack strong references keys so we backward those packed keys would cleared long retain_graph=False Clearing packed key clears corresponding entry WKD If we wish recomputed variables immediately cleared we unpack them retain_graph=True case we cannot rely packed keys cleared backward automatically Instead packing strong reference key directly we pack container object which we manually clear we unpack An important detail second backward happens second recomputation needs reset container newly created key Rule Stop recomputation soon we ve recomputed saved tensors we know we need Implementation details Rule During recomputation raise exception number recomputed tensors matches number tensors we expected recompute We wrap recomputation call try-catch catch specific exception See Rule below some examples Rule We support doing backward inside checkpoint context retain_graph True fn x y = x sin z = y cos gx = torch autograd grad z x retains_grad=True gx z out = checkpoint fn inp out backward Because z saved cos while checkpoint enabled would actually saved so grad call inside must trigger recomputation During recomputation inner pack hook has two responsibilities As usual populating WeakKeyDictionary storing recomputed tensors Pack actual tensor detached so one may perform backward recomputed graph The tensors saved graph will live until end recomputation die earlier someone performs backward retain_graph=False More generally performing backward recomputed graph occurs following cases - If backward performed inside forward - During original forward IF early-stop disabled - During original backward - If there multiple grad backward calls we would perform backward recomputed graph even early-stop enabled see example below retain_graph False The example below shows what happens during recomputation we find some tensors we trying recompute have already been cleared Spoiler we don t do anything special we just skip over them fn x y = x sin z = y cos gx = torch autograd grad z x x cos gx out = checkpoint fn inp out backward Don t save x y since we inside checkpoint Trigger recompute fn since x y weren t saved And depending whether early stop enabled either stop continue running function Because we running backward retain_graph=False we clear x y s holders Don t save x since we inside checkpoint Calling backward triggers another recompute fn During recompute we see x y have already been cleared original graph indicated holder=None We skip over them We still save x since its holder still alive _enable_checkpoint_early_stop Optional bool = None contextlib contextmanager set_checkpoint_early_stop enable bool Context manager sets whether checkpoint should stop recomputation early By default non-reentrant checkpoint stops recomputation soon has computed all needed Tensors This context manager can used disable feature problematic your specific application This context manager only needs active when forward run It does need active during backward Example xdoctest +SKIP failing message = saved tensors default hooks disabled set_checkpoint_early_stop False Any checkpoint under context manager will respect context manager even its backward performed outside out = checkpoint fn inputs out backward global _enable_checkpoint_early_stop try prev = _enable_checkpoint_early_stop _enable_checkpoint_early_stop = enable yield finally _enable_checkpoint_early_stop = prev _Handle pass _Holder __init__ handles Dict int Optional _Handle = _NoopSaveInputs torch autograd Function staticmethod pyrefly ignore bad-override forward args torch empty staticmethod setup_context ctx Any inputs Tuple Any output Any - None Only tensors can saved ctx save_for_backward everything captured get_args which saved directly ctx tensor_indices tensors = zip i o i o enumerate inputs isinstance o torch Tensor strict=False idx saved_idx = b b enumerate tensor_indices args tensors replaced None placeholders args = None isinstance o torch Tensor o o inputs get_args saved_tensors restore placeholders original tensors grabbed ctx saved_tensors which may saved parent checkpoint checkpoint nested would trigger recursive unpack ret = saved_tensors idx saved_idx i i tensor_indices o i o enumerate args grab tail since we also saved dummy avoid having explicitly handle case where there no tensor inputs ret ctx get_args = get_args ctx save_for_backward tensors staticmethod backward ctx grad_outputs raise AssertionError Did expect backward graph _CheckpointFrame __init__ recompute_fn early_stop unpack_error_cb metadata_fn recompute_fn = recompute_fn input_saver = None weak_holders List ReferenceType = We store weakkeydictionary so case partial backward entries dict cleared alongside Holder which will removed when SavedVariable cleared recomputed DefaultDict int weakref WeakKeyDictionary _Handle torch Tensor = defaultdict weakref WeakKeyDictionary We need both recomp_counter recomputed since they can diverge https github com pytorch pytorch pull #discussion_r recomp_counter DefaultDict int int = defaultdict int is_recomputed DefaultDict int bool = defaultdict bool See Rule early_stop = early_stop Debugging metadata_fn = metadata_fn unpack_error_cb = unpack_error_cb x_metadatas = forward_completed = False ignore_saved_mismatch = False check_recomputed_tensors_match gid ignore_saved_mismatch TODO we can probably make check stricter checking metadata first tensors still match NOTE Error handling checkpoint At high level we need check tensors saved during original forward matches tensors saved during recompute This means handling cases During recompute more tensors saved Usually hidden due StopRecomputationError early stop enabled we would have errored anyway because there aren t enough weak_holders But we do want have nice error See _recomputation_hook details len weak_holders == recomp_counter gid During recompute fewer tensors saved We know every time we save something do original forward we append weak_holder every time we save tensor during recompute we increment recompute_counter raise CheckpointError torch utils checkpoint A different number tensors saved during original forward recomputation \n f Number tensors saved during forward len weak_holders \n f Number tensors saved during recomputation recomp_counter gid \n f _debug_tip_msg During recompute same tensors saved they have different metadata nb_meta_different = idx weak_holder enumerate weak_holders holder = weak_holder holder None continue We ve seen all holders since we iterate over them order For every holder still alive now must ve been alive when we saw during recompute therefore gid must set _internal_assert gid holder handles We know first unpack so couldn t have been set None yet _internal_assert holder handles gid None We always set these together recomputation hook _internal_assert holder handles gid recomputed gid see pack hook x_metadata weak_holders x_meta = x_metadatas idx recomputed_x = recomputed gid holder handles gid x_meta = metadata_fn recomputed_x nb_meta_different append idx x_meta metadata_fn recomputed_x len nb_meta_different mismatched_tensors = idx x_meta recomputed_meta nb_meta_different mismatched_tensors += f tensor position idx \n f saved metadata x_meta \n f recomputed metadata recomputed_meta \n raise CheckpointError torch utils checkpoint Recomputed values following tensors have different metadata than during forward pass \n f mismatched_tensors \n f _debug_tip_msg _debug_tip_msg = Tip To see more detailed error message either pass ` debug=True ` ` torch utils checkpoint checkpoint ` wrap code block ` torch utils checkpoint set_checkpoint_debug_enabled True ` enable checkpointâ€‘debug mode globally _checkpoint_error_template = \ An error happened while unpacking tensors dumping logs latest computation because you passed ` debug=True ` ` torch utils checkpoint checkpoint ` Scroll all way down guidance how navigate these logs +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ &#124; Stack traces operators ran original forward &#124; + ------------------------------------------------------------------------------ + forward_traces +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ &#124; Stack traces operators ran during recomputation &#124; + ------------------------------------------------------------------------------ + recompute_traces +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ &#124; Log operators original forward recomputation &#124; + ------------------------------------------------------------------------------ + Scroll up correlate stack traces each operation listed below This helps identify their source code IMPORTANT Differences detach calls between original forward recomputation expected They introduced checkpointing mechanism can ignored Operations executed during original forward forward_ops Operations executed during recomputation recompute_ops + ------------------------------------------------------------------------------ + ERROR Detected non-determinism while running activation checkpointing You seeing error because you passed ` debug=True ` checkpoint tensors saved during original forward differ between those saved during recomputation This can happen different operators ran original forward recomputation To identify where mismatch may coming you can do following Compare operators ran during original forward recomputation see where they differ These operators printed above order they executed Review stack trace each operator locate its invocation source Each operator s stack trace printed their execution order Note logs can quite long Here s how they structured Tip you can Ctrl-f these headers Stack traces operators ran original forward Stack traces operators ran during recomputation Log operators original forward recomputation Error message --- You here -------------------------------------------------------------------------------- CheckpointError RuntimeError pass _get_debug_context_and_cb - Tuple Callable Any Callable CheckpointError None This function returns context_fn error_cb used checkpointing mechanism error_cb invoked when error detected during unpack record_context_cpp support non-linux non-x _ platforms cpp_tb = platform machine == x _ platform system == Linux CaptureLogs __init__ logs = None tbs = None get_context_manager contextlib contextmanager logging_mode LoggingTensorMode \ capture_logs True python_tb=True script_tb=True cpp_tb=cpp_tb logs_and_tb pyrefly ignore bad-assignment logs tbs = logs_and_tb yield logs_and_tb logging_mode capture_logs_fwd = CaptureLogs capture_logs_recompute = CaptureLogs unpack_error_cb e CheckpointError get_str_tb label capture_logs out = total_len = len capture_logs logs i log tb enumerate zip capture_logs logs capture_logs tbs strict=False out += f log i + total_len label \n\n found_torch_dispatch = False line tb Start printing stack trace only after __torch_dispatch__ found is_torch_dispatch = line name == __torch_dispatch__ found_torch_dispatch is_torch_dispatch continue is_torch_dispatch found_torch_dispatch = True continue out += f line filename line line line name \n out += \n\n out capture_logs_fwd logs None raise AssertionError capture_logs_fwd logs None capture_logs_recompute logs None raise AssertionError capture_logs_recompute logs None raise CheckpointError _checkpoint_error_template format forward_traces=get_str_tb original capture_logs_fwd recompute_traces=get_str_tb recompute capture_logs_recompute forward_ops= \n join capture_logs_fwd logs recompute_ops= \n join capture_logs_recompute logs e context_fn capture_logs_fwd get_context_manager capture_logs_recompute get_context_manager context_fn unpack_error_cb _default_meta_extractor x torch Tensor - Dict str Any These properties fast check easy understand shape x shape dtype x dtype device x device _allowed_determinism_checks_to_fns Dict str Callable torch Tensor Any = _DEFAULT_DETERMINISM_MODE _default_meta_extractor none lambda _ None See Rule _StopRecomputationError Exception pass _recomputation_hook torch autograd graph saved_tensors_hooks __init__ target_frame_ref ReferenceType gid int pack_hook x x = x detach x requires_grad x target_frame = target_frame_ref target_frame None raise AssertionError Internal error target_frame reference None recomp_idx = target_frame recomp_counter gid target_frame recomp_counter gid += recomp_idx = len target_frame weak_holders target_frame early_stop raise AssertionError Unexpected state target_frame early_stop set target_frame forward_completed We run into case when early stop enabled do grad within checkpoint We need set flag so we don t error out later when we check number tensors saved during forward recomputation match target_frame ignore_saved_mismatch = True x raise CheckpointError torch utils checkpoint trying save more tensors during recomputation than during original forward pass \n f _debug_tip_msg holder = target_frame weak_holders recomp_idx This holder may have been cleared because someone may have called backward within forward If so we don t need save holder None _internal_assert holder handles get gid None None holder handles gid = _Handle target_frame recomputed gid holder handles gid = x target_frame early_stop target_frame recomp_counter gid == len target_frame weak_holders raise _StopRecomputationError See Rule retain_graph True above x unpack_hook x See Rule retain_graph True above example when graph created during recomputation could backwarded x super __init__ pack_hook unpack_hook torch _disable_dynamo creates reference cycle decorated function This function used ensure decorated function does have closure so other objects aren t also kept alive https github com pytorch pytorch issues Note does work when fn compiled torch _disable_dynamo _run_fn_with_dynamo_disabled fn args kwargs fn args kwargs _checkpoint_hook torch autograd graph saved_tensors_hooks __init__ frame pack_hook x See Rule above holder = _Holder frame weak_holders append weakref ref holder Save metadata detect non-determinism frame metadata_fn None torch no_grad frame x_metadatas append frame metadata_fn x holder unpack_hook holder gid = torch _C _current_graph_task_id gid == - generate temporary id we trigger unpack outside backward call gid = int uuid uuid frame is_recomputed gid ctx = frame input_saver grad_fn args = ctx get_args ctx saved_tensors try _recomputation_hook weakref ref frame gid torch autograd enable_grad See Note compiled autograd checkpoint unpack hook _run_fn_with_dynamo_disabled frame recompute_fn args except _StopRecomputationError pass frame is_recomputed gid = True frame check_recomputed_tensors_match gid _internal_assert gid holder handles holder handles gid None raise CheckpointError torch utils checkpoint Unpack being triggered tensor already unpacked once If you calling ctx saved_tensors backward make sure do so only once Otherwise please open issue details your use case _internal_assert holder handles gid frame recomputed gid ret = frame recomputed gid holder handles gid holder handles gid = None ret frame unpack_error_cb None unpack_hook_with_error_cb holder try unpack_hook holder except CheckpointError e frame unpack_error_cb e super __init__ pack_hook unpack_hook_with_error_cb super __init__ pack_hook unpack_hook _is_compiling func args kwargs Check we under AOTAutograd tracing Checking functional mode active should always do what we want torch _C _get_dispatch_mode torch _C _TorchDispatchModeKey PROXY None _VersionWrapper Check cached tensors mutated __init__ val val Union torch Tensor Any = val version Optional int = val _version isinstance val torch Tensor None get_val allow_cache_entry_mutation version None allow_cache_entry_mutation val _version = version Can we give user stack trace where mutation happened raise RuntimeError Tensor cached during selective activation checkpoint has been mutated val _maybe_detach x any_ret_has_alias_info We detach two separate reasons - For view ops we need ensure when tensor returned CachedDispatchMode as_view sees AutogradMeta nullptr - Avoid reference cycles For case enough check whether x has differentiable dtype because non-differentiable dtype can have non-nullptr AutogradMeta e g when tensor view isinstance x torch Tensor x is_floating_point x is_complex any_ret_has_alias_info torch _C _SetExcludeDispatchKeyGuard torch _C DispatchKey ADInplaceOrView False Ensure view performed beneath autograd properly propagates version counter TODO Use reentrant_dispatch instead manually manipulating dispatch keys Using reentrant_dispatch would respect inference_mode though relevant case x = x detach x SelectiveCheckpointContext Context passed policy function during selective checkpointing This used pass relevant metadata policy function during selective checkpointing The metadata includes whether current invocation policy function during recomputation Example xdoctest +SKIP stub policy_fn ctx op args kwargs print ctx is_recompute context_fn = functools partial create_selective_checkpoint_contexts policy_fn out = torch utils checkpoint checkpoint fn x y use_reentrant=False context_fn=context_fn __init__ is_recompute is_recompute = is_recompute CheckpointPolicy enum Enum Enum specifying policy checkpointing during backpropagation The following policies supported - ` ` MUST PREFER _SAVE ` ` The operation s output will saved during forward pass will recomputed during backward pass - ` ` MUST PREFER _RECOMPUTE ` ` The operation s output will saved during forward pass will recomputed during backward pass Use ` ` MUST_ ` ` over ` ` PREFER_ ` ` indicate policy should overridden other subsystems like ` torch compile ` note A policy function always returns ` ` PREFER_RECOMPUTE ` ` equivalent vanilla checkpointing A policy function returns ` ` PREFER_SAVE ` ` every op NOT equivalent using checkpointing Using such policy would save additional tensors limited ones actually needed gradient computation MUST_SAVE = PREFER_SAVE = MUST_RECOMPUTE = PREFER_RECOMPUTE = _policy_from_bool b For backward compatibility CheckpointPolicy MUST_SAVE b CheckpointPolicy PREFER_RECOMPUTE SAC_IGNORED_OPS = AC inserts different number detach during forward recompute torch ops aten detach default AC s determinism check invokes additional metadata ops during forward With subclasses involved these metadata ops become dispatchable can result incorrectness these ops selected cached torch ops prim device default &#124; set torch _subclasses functional_tensor FunctionalTensor metadata_fns type ignore has-type _CachingTorchDispatchMode TorchDispatchMode Used together _CachedTorchDispatchMode implement SAC __init__ policy_fn storage policy_fn = policy_fn storage = storage __torch_dispatch__ func types args= kwargs=None func SAC_IGNORED_OPS func args kwargs kwargs = kwargs None kwargs policy = policy_fn SelectiveCheckpointContext is_recompute=False func args kwargs isinstance policy bool policy = _policy_from_bool policy is_compiling = _is_compiling func args kwargs is_compiling Overwrite each node s recompute tag add user annotation fx_traceback current_meta recompute = policy out = func args kwargs HOPs don t support func _schema HOPs don t alias - always true today will always true long time TODO HOPs don t mutate - always true today will true forever isinstance func torch _ops HigherOrderOperator any_ret_has_alias_info = False any_ret_has_alias_info = any ret alias_info None ret func _schema returns policy CheckpointPolicy MUST_SAVE CheckpointPolicy PREFER_SAVE is_compiling storage func append tree_map lambda x _VersionWrapper _maybe_detach x any_ret_has_alias_info out out _CachedTorchDispatchMode TorchDispatchMode Used together _CachedTorchDispatchMode implement SAC __init__ policy_fn storage allow_cache_entry_mutation policy_fn = policy_fn storage = storage allow_cache_entry_mutation = allow_cache_entry_mutation __torch_dispatch__ func types args= kwargs=None func SAC_IGNORED_OPS func args kwargs kwargs = kwargs None kwargs policy = policy_fn SelectiveCheckpointContext is_recompute=True func args kwargs isinstance policy bool policy = _policy_from_bool policy is_compiling = _is_compiling func args kwargs policy CheckpointPolicy MUST_SAVE CheckpointPolicy PREFER_SAVE is_compiling storage = storage get func storage None raise RuntimeError f func encountered during backward found storage len storage == raise RuntimeError Trying backward extra time You only allowed backward once any region computed under selective activation checkpoint out = tree_map lambda x x get_val allow_cache_entry_mutation storage pop out = func args kwargs out create_selective_checkpoint_contexts policy_fn_or_list allow_cache_entry_mutation=False Helper avoid recomputing certain ops during activation checkpointing Use ` torch utils checkpoint checkpoint ` control which operations recomputed during backward pass Args policy_fn_or_list Callable List - If policy function provided should accept ` SelectiveCheckpointContext ` ` OpOverload ` args kwargs op ` CheckpointPolicy ` enum value indicating whether execution op should recomputed - If list operations provided equivalent policy returning ` CheckpointPolicy MUST_SAVE ` specified operations ` CheckpointPolicy PREFER_RECOMPUTE ` all other operations allow_cache_entry_mutation bool optional By default error raised any tensors cached selective activation checkpoint mutated order ensure correctness If set ` True ` check disabled Returns A tuple two context managers Example xdoctest +REQUIRES LINUX functools x = torch rand requires_grad=True y = torch rand requires_grad=True ops_to_save = torch ops aten mm default policy_fn ctx op args kwargs op ops_to_save CheckpointPolicy MUST_SAVE CheckpointPolicy PREFER_RECOMPUTE context_fn = functools partial create_selective_checkpoint_contexts policy_fn equivalently context_fn = functools partial create_selective_checkpoint_contexts ops_to_save fn x y torch sigmoid torch matmul torch matmul x y y y out = torch utils checkpoint checkpoint fn x y use_reentrant=False context_fn=context_fn NB If grad_mode disabled checkpoint would run forward under context_fn anyway so proceed usual isinstance policy_fn_or_list list op policy_fn_or_list isinstance op torch _ops OpOverload torch _ops HigherOrderOperator _extra_msg = Please update OpOverloadPacket specific OpOverload For example you have ` torch ops aten mm ` change ` torch ops aten mm default ` isinstance op torch _ops OpOverloadPacket raise ValueError f Expected op ` op_list ` OpOverload got op f type type op _extra_msg policy_fn ctx op args kwargs op policy_fn_or_list CheckpointPolicy MUST_SAVE CheckpointPolicy PREFER_RECOMPUTE callable policy_fn_or_list policy_fn = policy_fn_or_list raise TypeError policy_fn_or_list must either function list ops storage Dict Any List Any = defaultdict list _CachingTorchDispatchMode policy_fn storage _CachedTorchDispatchMode policy_fn storage allow_cache_entry_mutation NB helper wraps fn before calling checkpoint_impl kwargs saving restoring global state handled here _checkpoint_without_reentrant_generator fn preserve_rng_state=True context_fn Callable Tuple ContextManager ContextManager = noop_context_fn determinism_check str = _DEFAULT_DETERMINISM_MODE debug bool = False early_stop bool = True args kwargs Checkpointing without reentrant autograd Args fn describes what run forward pass model part model It should also know how handle inputs passed tuple For example LSTM user passes ` ` activation hidden ` ` attr ` function ` should correctly use first input ` ` activation ` ` second input ` ` hidden ` ` preserve_rng_state bool optional Omit stashing restoring RNG state during each checkpoint Default ` ` True ` ` context_fn Callable optional A callable returning tuple two context managers The function its recomputation will run under first second context managers respectively determinism_check str optional A string specifying determinism check perform By default set ` ` default ` ` which compares shapes dtypes devices recomputed tensors against those saved tensors To turn off check specify ` ` none ` ` Currently these only two supported values Please open issue you would like see more determinism checks debug bool optional If ` ` True ` ` error messages will also include trace operators ran during original forward computation well recomputation early_stop bool optional If ` ` True ` ` non-reentrant checkpoint stops recomputation soon has computed all needed Tensors Can overridden globally using func ` set_checkpoint_early_stop ` context manager Default ` ` True ` ` args Arguments pass given ` ` function ` ` kwargs Keyword arguments pass into given ` ` function ` ` unpack_error_cb = None _checkpoint_debug_enabled _checkpoint_debug_enabled None debug context_fn noop_context_fn raise ValueError debug=True incompatible non-default context_fn context_fn unpack_error_cb = _get_debug_context_and_cb determinism_check _allowed_determinism_checks_to_fns metadata_fn = _allowed_determinism_checks_to_fns determinism_check raise ValueError f determinism_check should one list _allowed_determinism_checks_to_fns keys f got determinism_check device_type = _infer_device_type args device_module = _get_device_module device_type forward_context recompute_context = context_fn _is_compiling fn args kwargs context_fn noop_context_fn isinstance forward_context TorchDispatchMode isinstance recompute_context TorchDispatchMode raise AssertionError In torch compile mode ` context_fn ` arg passed ` torch utils checkpoint ` must generate tuple two ` TorchDispatchMode ` s Accommodates remote possibility autocast enabled cpu AND gpu device_autocast_kwargs cpu_autocast_kwargs = _get_autocast_kwargs device_type=device_type preserve_rng_state fwd_cpu_state = torch get_rng_state Don t eagerly initialize cuda context accident If user intends context initialized later within their run_function we SHOULD actually stash cuda state here Unfortunately we have no way anticipate will happen before we run function If they do so we raise error had_device_in_fwd = False getattr device_module _initialized False had_device_in_fwd = True fwd_devices fwd_device_states = get_device_states args recompute_fn inputs kwargs args = inputs This will called later during recomputation This wrapping enables necessary global state captured rng_devices = preserve_rng_state had_device_in_fwd rng_devices = fwd_devices torch random fork_rng devices=rng_devices enabled=preserve_rng_state device_type=device_type preserve_rng_state torch set_rng_state fwd_cpu_state had_device_in_fwd set_device_states fwd_devices fwd_device_states device_type=device_type device_autocast_ctx = torch amp autocast device_type=device_type device_autocast_kwargs torch amp is_autocast_available device_type contextlib nullcontext device_autocast_ctx torch amp autocast cpu cpu_autocast_kwargs recompute_context type ignore attr-defined fn args kwargs new_frame = _CheckpointFrame recompute_fn _enable_checkpoint_early_stop _enable_checkpoint_early_stop None early_stop unpack_error_cb metadata_fn dummy = torch empty requires_grad=True new_frame input_saver = _NoopSaveInputs apply dummy kwargs args When ambient grad_mode False new_frame input_saver grad_fn None yield _checkpoint_hook new_frame forward_context yield new_frame forward_completed = True getattr device_module _initialized False \ preserve_rng_state had_device_in_fwd type ignore possibly-undefined Device initialized before running forward so we didn t stash device state raise RuntimeError PyTorch s device state initialized forward pass Checkpoint which allowed Please open issue you need feature Note compiled autograd checkpoint unpack hook When tracing via compiled autograd hook will visible compiler forward checkpointed region ran eager If forward had ran under compile would have been wrapped higher order op See Note torch compile checkpoint Since we run recomputation hook under enable_grad context AOTDispatch will trace joint graph hook may save different activations than eager This conflicts strict activation count checks ` frame check_recomputed_tensors_match ` So we disable hook force recompute eager checkpointed regions eager This could removed we can disable partitioner graph segment