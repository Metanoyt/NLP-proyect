Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD license found LICENSE file root directory source tree r Zero Redundancy Optimizer collections copy enum inspect io logging collections abc Callable itertools chain typing Any Optional Union torch torch distributed dist torch distributed algorithms join Join Joinable JoinHook torch distributed optim utils functional_optim_map torch optim Optimizer __all__ = ZeroRedundancyOptimizer logger = logging getLogger __name__ Credits classy_vision generic distributed_util py _recursive_copy_to_device value Any non_blocking bool device torch device - Any r Recursively searches lists tuples dicts copies tensors device possible Non-tensor values passed as-is result note These all copies so there two objects reference same object then after call there will two different objects referenced device isinstance value torch Tensor value device non_blocking=non_blocking isinstance value list tuple values = _recursive_copy_to_device val non_blocking=non_blocking device=device val value values isinstance value list tuple values isinstance value collections abc Mapping key _recursive_copy_to_device val non_blocking=non_blocking device=device key val value items value _is_trainable param torch Tensor - bool r Return parameter trainable where trainability equivalent requiring gradient param requires_grad _broadcast_object obj Any src_rank int group object = dist group WORLD device torch device = torch device cpu - Any r Broadcasts object given group It will sending object called source rank receiving object otherwise Arguments obj object broadcast only used called source rank src_rank int source rank group ` ` ProcessGroup ` ` optional group used broadcast default ` ` dist group WORLD ` ` device ` ` torch device ` ` optional device send receive default ` ` torch device cpu ` ` Returns The broadcasted object dist get_rank == src_rank Send object buffer = io BytesIO torch save obj buffer data = bytearray buffer getbuffer length_tensor = torch LongTensor len data device data_send_tensor = torch ByteTensor data device pyrefly ignore bad-argument-type dist broadcast length_tensor src=src_rank group=group async_op=False pyrefly ignore bad-argument-type dist broadcast data_send_tensor src=src_rank group=group async_op=False Receive object length_tensor = torch LongTensor device pyrefly ignore bad-argument-type dist broadcast length_tensor src=src_rank group=group async_op=False data_recv_tensor = torch empty int length_tensor item dtype=torch uint device=device pyrefly ignore bad-argument-type dist broadcast data_recv_tensor src=src_rank group=group async_op=False buffer = io BytesIO data_recv_tensor cpu numpy obj = torch load buffer map_location=device weights_only=False obj _ZeROJoinHook JoinHook __init__ zero assert isinstance zero ZeroRedundancyOptimizer ZeRO join hook requires passing ZeroRedundancyOptimizer instance state zero = zero super __init__ main_hook Perform optimizer step This step updates joined process s shard parameters broadcasts those parameters zero step _DDPBucketAssignment r Represent ` DistributedDataParallel ` bucket assignment This means possibly non-strict subset parameters corresponding DDP bucket assigned rank update Attributes bucket_index int index bucket determined DDP gradient bucket all-reduce order parameters List torch Tensor model parameters bucket assigned rank offset int offset into ` GradBucket ` s meth ` parameters ` giving index first element passed-in ` ` parameters ` ` equivalently indexes into ` GradBucket ` s meth ` gradients ` device torch device device which parameters stored tensor torch Tensor flattened tensor giving data parameter subset assigned rank __init__ bucket_index int parameters list torch Tensor offset int bucket_index = bucket_index parameters = parameters offset = offset len parameters == raise ValueError Empty bucket assignment DDP guarantees all parameters bucket have same device pyrefly ignore read-only device torch device = parameters device tensor Optional torch Tensor = None _OverlapStatus enum IntEnum r Define possible statuses ` ZeroRedundancyOptimizer ` can when overlapping ` DistributedDataParallel ` Attributes ` ` UNINITIALIZED ` ` The ZeRO instance effectively uninitialized waiting DDP finalize its bucketing ` ` DDP_HAS_REBUILT_BUCKETS ` ` DDP has rebuilt its buckets meaning its bucketing finalized The ZeRO instance can now collect necessary information about DDP bucketing ` ` INITIALIZED ` ` The ZeRO instance fully initialized can now optimize parameters UNINITIALIZED = DDP_HAS_REBUILT_BUCKETS = INITIALIZED = _OverlapInfo r Information needed ` ZeroRedundancyOptimizer ` overlap ` DistributedDataParallel ` Arguments world_size int world size process group being used Attributes shard_buckets bool ` ` True ` ` then assignment each ` DistributedDataParallel ` bucket partitioned across possibly multiple ` ZeroRedundancyOptimizer ` instances i e across possibly multiple ranks approximate uniformity following threshold given total parameter size divided world size ` ` False ` ` then each bucket wholly assigned single ` ZeroRedundancyOptimizer ` instance i e single rank should set value passed into hook constructor status _OverlapStatus current status see ` _OverlapStatus ` more information params_per_bucket List List torch Tensor ` ` params_per_bucket i ` ` gives model parameters ` ` i ` ` th bucket params_per_rank List List torch Tensor ` ` params_per_rank i ` ` gives model parameters assigned ` ` i ` ` th rank where parameters grouped increasing bucket indices offsets Dict int int maps bucket index offset ` ` params_per_rank rank ` ` giving index first parameter bucket where ` ` rank ` ` process s own rank keys ` dict ` bucket indices assigned rank num_bucket_assignments int total number bucket assignments across all ranks equal number ` DistributedDataParallel ` gradient buckets ` ` shard_buckets=False ` ` possibly greater otherwise total_size int optional total size all buckets i e sum ` ` param numel ` ` all ` ` param ` ` across all buckets ` ` shard_buckets=True ` ` otherwise ` ` None ` ` broadcast_handles List Work ` list ` async work handles parameter broadcasts bucket_index_to_future Dict int torch futures Future ` dict ` mapping bucket index corresponding all-reduce future bucket_index_to_bucket Dict int dist GradBucket ` dict ` mapping bucket index corresponding bucket bucket_indices_seen List int ` list ` bucket indices seen iteration __init__ world_size - None status _OverlapStatus = _OverlapStatus UNINITIALIZED shard_buckets bool = False Modified per bucket reconstruction params_per_bucket list list torch Tensor = params_per_rank list list torch Tensor = _ range world_size offsets dict int int = Group Ranks assigned_ranks_per_bucket list set int = num_bucket_assignments int = total_size Optional int = None Modified per iteration broadcast_handles list Any = bucket_indices_seen list int = Used ` hook_with_zero_step ` bucket_index_to_future dict int torch futures Future = bucket_index_to_bucket dict int dist GradBucket = wait_for_broadcasts - None r Wait all parameter broadcasts This function should called once all broadcasts have been scheduled meaning ` ` broadcast_handles ` ` filled This clears ` ` broadcast_handles ` ` preparation next iteration assert len broadcast_handles == num_bucket_assignments f Missing least one broadcast handle rank dist get_rank _ = x wait x broadcast_handles broadcast_handles clear clear_per_iter_info - None r Clear data structures modified per-iteration This function should called end iteration bucket_indices_seen clear bucket_index_to_future clear bucket_index_to_bucket clear ZeroRedundancyOptimizer Optimizer Joinable r Wrap arbitrary ` optim Optimizer torch optim Optimizer ` shards its states across ranks group The sharing done described ` ZeRO https arxiv org abs ` _ The local optimizer instance each rank only responsible updating approximately ` ` world_size ` ` parameters hence only needs keep ` ` world_size ` ` optimizer states After parameters updated locally each rank will broadcast its parameters all other peers keep all model replicas same state ` ` ZeroRedundancyOptimizer ` ` can used conjunction ` torch nn parallel DistributedDataParallel ` reduce per-rank peak memory consumption ` ` ZeroRedundancyOptimizer ` ` uses sorted-greedy algorithm pack number parameters each rank Each parameter belongs single rank divided among ranks The partition arbitrary might match parameter registration usage order Arguments params ` ` Iterable ` ` ` ` Iterable ` ` ` torch Tensor ` s ` dict ` s giving all parameters which will sharded across ranks Keyword Args optimizer_class ` torch nn Optimizer ` local optimizer process_group ` ` ProcessGroup ` ` optional ` ` torch distributed ` ` ` ` ProcessGroup ` ` default ` ` dist group WORLD ` ` initialized meth ` torch distributed init_process_group ` parameters_as_bucket_view bool optional ` ` True ` ` parameters packed into buckets speed up communication ` ` param data ` ` fields point bucket views different offsets ` ` False ` ` each individual parameter communicated separately each ` ` params data ` ` stays intact default ` ` False ` ` overlap_with_ddp bool optional ` ` True ` ` meth ` step ` overlapped ` DistributedDataParallel ` s gradient synchronization requires either functional optimizer ` ` optimizer_class ` ` argument one functional equivalent registering DDP communication hook constructed one functions ` ` ddp_zero_hook py ` ` parameters packed into buckets matching those ` DistributedDataParallel ` meaning ` ` parameters_as_bucket_view ` ` argument ignored If ` ` False ` ` meth ` step ` runs disjointly after backward pass per normal default ` ` False ` ` defaults any trailing arguments which forwarded local optimizer Example xdoctest +SKIP torch nn nn torch distributed optim ZeroRedundancyOptimizer torch nn parallel DistributedDataParallel DDP model = nn Sequential nn Linear rank _ range ddp = DDP model device_ids= rank opt = ZeroRedundancyOptimizer ddp parameters optimizer_class=torch optim Adam lr= ddp inputs sum backward opt step warning Currently ` ` ZeroRedundancyOptimizer ` ` requires all passed-in parameters same dense type warning If you pass ` ` overlap_with_ddp=True ` ` wary following Given way overlapping ` DistributedDataParallel ` ` ZeroRedundancyOptimizer ` currently implemented first two three training iterations do perform parameter updates optimizer step depending ` ` static_graph=False ` ` ` ` static_graph=True ` ` respectively This because needs information about gradient bucketing strategy used ` DistributedDataParallel ` which finalized until second forward pass ` ` static_graph=False ` ` until third forward pass ` ` static_graph=True ` ` To adjust one option prepend dummy inputs warning ZeroRedundancyOptimizer experimental subject change __init__ params optimizer_class type Optimizer process_group Optional Any = None parameters_as_bucket_view bool = False overlap_with_ddp bool = False defaults Any r Init Perform type assumption checks input parameters params = _verify_and_init_params params _verify_same_dense_param_type NOTE The parent constructor uses ` add_param_group ` which partially overloaded ZeroRedundancyOptimizer so we use ` initialized ` flag dissociate behaviour ` add_param_group ` between parent child initialized = False Optimizer __init__ params defaults Joinable __init__ Now all parameters held both ` _all_params ` ` param_groups ` Internal data structures ` _cache ` indicates lazily evaluated _param_to_rank_cache dict torch Tensor int = _param_to_index_cache dict torch Tensor int = _partition_parameters_cache list list dict = _index_to_param_cache list torch Tensor = _device_to_params_per_rank_cache dict torch device list list torch Tensor = _bucket_assignments_per_rank_cache list dict int _DDPBucketAssignment = _is_trainable_mask = _get_is_trainable_mask Default device collective communication buckets _default_device = _all_params device process_group = process_group process_group None dist group WORLD world_size int = dist get_world_size process_group rank int = dist get_rank process_group global_rank int = dist distributed_c d get_global_rank pyrefly ignore bad-argument-type process_group rank _overlap_with_ddp bool = overlap_with_ddp _optim_defaults = defaults _optim_constructor = _get_optimizer_constructor optimizer_class If ` overlap_with_ddp=True ` local optimizer initialization delayed run time after necessary information has been collected overlap_with_ddp _init_local_optimizer _overlap_info _OverlapInfo = _OverlapInfo world_size parameters_as_bucket_view logger warning ` parameters_as_bucket_view=True ` will ignored since ` overlap_with_ddp=True ` instead different bucketing strategy will used ` _buckets ` used ` parameters_as_bucket_view=True ` which case parameter data flattened into contiguous bucket tensors parameters_as_bucket_view = parameters_as_bucket_view _buckets list list torch Tensor = _build_param_buckets Optional consolidated optimizer state only populated rank target ` consolidate_state_dict ` _all_state_dicts list dict str Any = initialized = True _clear_cache - None r Clear cached data structures giving partition information _partition_parameters_cache clear _param_to_rank_cache clear _index_to_param_cache clear _param_to_index_cache clear _device_to_params_per_rank_cache clear _bucket_assignments_per_rank_cache clear add_param_group param_group dict str Any - None r Add parameter group ` Optimizer ` s ` ` param_groups ` ` This can useful when fine tuning pre-trained network frozen layers can made trainable added ` Optimizer ` training progresses Arguments param_group dict specifies parameters optimized group-specific optimization options warning This method handles updating shards all partitions needs called all ranks Calling subset ranks will cause training hang because communication primitives called depending managed parameters expect all ranks participate same set parameters initialized _overlap_with_ddp raise RuntimeError ZeroRedundancyOptimizer ` overlap_with_ddp=True ` only supports single parameter group super add_param_group param_group NOTE The rest method assumes call parent s ` add_param_group ` appends new parameter group preserves previous parameter-group ordering initialized Force re-partitioning parameters _clear_cache param_groups = _partition_parameters rank NOTE All parameters old parameter groups should assigned same ranks so local optimizers do need reinitialized Add parameters assigned rank new parameter group local optimizer any len param_groups == len optim param_groups + optim add_param_group param_groups - Update bucketing strategy accordingly parameters_as_bucket_view _build_param_buckets consolidate_state_dict int = - None r Consolidate list ` ` state_dict ` ` s one per rank target rank Arguments int rank receives optimizer states default Raises RuntimeError ` ` overlap_with_ddp=True ` ` method called before ` ZeroRedundancyOptimizer ` instance has been fully initialized which happens once ` DistributedDataParallel ` gradient buckets have been rebuilt warning This needs called all ranks _check_overlap_initialized Sync exposed ` param_groups ` attributes local optimizer case they have been updated _sync_param_groups param_groups optim param_groups Pull sharded state all ranks store them rank order empty_messenger = torch tensor dtype=torch uint device=self _default_device NOTE We wastefully use ` broadcast ` e g instead ` gather ` due compatibility issues NCCL backend possible follow-up move all sharded state management RPC RRef _all_state_dicts = rank range world_size global_rank = dist distributed_c d get_global_rank pyrefly ignore bad-argument-type process_group rank rank == Consolidate all local ` state_dict ` s rank storing CPU save GPU memory rank == rank Directly append own optimizer state _all_state_dicts append _recursive_copy_to_device optim state_dict non_blocking=True device=torch device cpu Receive optimizer state source rank local_state_dict = _broadcast_object empty_messenger src_rank=global_rank group=self process_group device=self _default_device _all_state_dicts append _recursive_copy_to_device local_state_dict non_blocking=True device=torch device cpu rank == rank Send optimizer state target rank _ = _broadcast_object optim state_dict src_rank=self global_rank group=self process_group device=self _default_device rank = Discard received object ` broadcast ` used compatibility reasons _ = _broadcast_object empty_messenger src_rank=global_rank group=self process_group device=self _default_device _verify_params_per_rank params_per_rank list list torch Tensor - None r Verify ` ` params_per_rank ` ` meth ` _partition_parameters ` The verification done checking ` ` params_per_rank ` ` has length equal world size does contain any parameters passed into ` ZeroRedundancyOptimizer ` constructor The parameters ` ` params_per_rank ` ` being strict subset those passed into constructor valid since some parameters may frozen Raises ValueError ` ` params_per_rank ` ` does have length equal world size contains parameter passed into ` ZeroRedundancyOptimizer ` constructor len params_per_rank = world_size raise ValueError ` params_per_rank ` must have length equal world size all_params_set = set _all_params params params_per_rank param params param all_params_set raise ValueError Passing new parameter ` params_per_rank ` passed into ZeroRedundancyOptimizer constructor _partition_param_group param_group dict str Any params_per_rank list list torch Tensor - None r Partition parameter group ` ` param_group ` ` according ` ` params_per_rank ` ` The partition will modify ` ` _partition_parameters_cache ` ` This method should only used subroutine meth ` _partition_parameters ` Arguments param_group dict str Any parameter group normally defined optimizer state params_per_rank list list torch Tensor ` list ` length world size containing ` list ` s parameters assign each rank rank params enumerate params_per_rank rank_param_group = copy copy param_group rank_param_group params = params _partition_parameters_cache rank append rank_param_group _partition_parameters params_per_rank Optional list list torch Tensor = None - list list dict r Partitions parameters across distributed data parallel ranks Arguments params_per_rank list list torch Tensor optional ` list ` length world size containing ` list ` s parameters assign each rank provides way specify partition manually If ` ` None ` ` parameters partitioned according internal algorithm default ` ` None ` ` Returns A ` list ` where each element list contains ` ` param_groups ` ` rank which itself ` list ` ` dict ` element corresponds rank etc each rank stores ` ` param_groups ` ` all ranks collective communication meth ` step ` Raises ValueError see meth ` _validate_params_per_rank ` RuntimeError ` ` params_per_rank ` ` ` ` None ` ` ` ZeroRedundancyOptimizer ` instance using more than one parameter group params_per_rank None Partition parameters optimizing uniformity len _partition_parameters_cache == _partition_parameters_cache = _ range world_size sizes = world_size param_group param_groups param_group_params_per_rank list list = _ range world_size Sort parameters size largest first params_sorted = sorted param_group params key=lambda t t numel reverse=True param params_sorted Greedily add parameter rank smallest size so far rank = _get_min_index sizes param_group_params_per_rank rank append param sizes rank += param numel Apply constructed partition parameter group _partition_param_group param_group param_group_params_per_rank _partition_parameters_cache Partition parameters according ` params_per_rank ` assert len _partition_parameters_cache == Specifying ` params_per_rank ` should only done when parameters have been partitioned yet len param_groups = raise RuntimeError Specifying ` params_per_rank ` only supports single parameter group _verify_params_per_rank params_per_rank _partition_parameters_cache = _ range world_size Apply passed-in partition parameter group param_group = param_groups _partition_param_group param_group params_per_rank _partition_parameters_cache property _param_to_rank - dict torch Tensor int r ` dict ` mapping parameters their assigned data parallel rank partition len _param_to_rank_cache == rank param_groups enumerate _partition_parameters param_group param_groups param param_group params _param_to_rank_cache param = rank _param_to_rank_cache property _param_to_index - dict torch Tensor int r ` dict ` mapping parameters their indices global optimizer state NOTE This assumes global optimizer state s indexing ` ` state_dict ` ` follows linear ordering over parameter groups len _param_to_index_cache == _param_to_index_cache = p i i p enumerate chain from_iterable g params g param_groups _param_to_index_cache property _index_to_param - list torch Tensor r List mapping parameter indices global optimizer scheme actual params len _index_to_param_cache == _index_to_param_cache = list chain from_iterable g params g param_groups _index_to_param_cache _broadcast_params_from_rank rank int r Broadcast shard parameters given rank all other ranks asynchronously Arguments rank int source rank Returns A ` list ` async work handles ` ` broadcast ` ` s performed synchronize parameters assert _overlap_with_ddp ` _broadcast_params_from_rank ` should used ` overlap_with_ddp=True ` instead broadcasting should happen DDP communication hook handles = parameters_as_bucket_view dev_i_buckets _buckets bucket = dev_i_buckets rank global_rank = dist distributed_c d get_global_rank pyrefly ignore bad-argument-type process_group rank handles append dist broadcast tensor=bucket src=global_rank group=self process_group async_op=True param_groups = _partition_parameters rank global_rank = dist distributed_c d get_global_rank pyrefly ignore bad-argument-type process_group rank param_group param_groups handles extend dist broadcast tensor=param data src=global_rank group=self process_group async_op=True param param_group params handles _sync_params r Sync all parameter shards across ranks This rank sends its shard parameters all other ranks receives shard each other rank This done using ` ` broadcast ` ` Parameters sent bucket-by-bucket ` ` parameters_as_bucket_view=True ` ` sent parameter-by-parameter otherwise handles = rank range world_size handles extend _broadcast_params_from_rank rank _ = x wait x handles property _device_to_params_per_rank - dict torch device list list torch Tensor r Return device parameters assigned per rank ` dict ` mapping each device ` list ` per-rank parameter lists filtered only include parameters stored device Each per-rank parameter list gives parameters assigned rank update This used constructing parameter buckets ` ` parameters_as_bucket_view=True ` ` Let ` ` dev_i ` ` denote ` ` i ` ` th device rank Then ` ` dev_ ` ` maps list containing rank s assigned parameters stored ` ` dev_ ` ` rank s assigned parameters stored ` ` dev_ ` ` ` ` dev_ ` ` maps list containing rank s assigned parameters stored ` ` dev_ ` ` rank s assigned parameters stored ` ` dev_ ` ` assert parameters_as_bucket_view ` _device_to_params_per_rank ` should only used ` parameters_as_bucket_view=True ` len _device_to_params_per_rank_cache == rank param_groups enumerate _partition_parameters param_group param_groups param param_group params device = param device device _device_to_params_per_rank_cache _device_to_params_per_rank_cache device = _ range world_size _device_to_params_per_rank_cache device rank append param _device_to_params_per_rank_cache _get_min_index values list int disallowed_indices Optional set int = None - int r Return ` ` values index min values ` ` except only uses one pass It also excludes any indices ` ` disallowed_indices ` ` provided Arguments values List int ` list ` values disallowed_indices Optional set int indices disallowed being returned min index min_index = - min_value = float inf i value enumerate values disallowed_indices i disallowed_indices continue value min_value min_value = value min_index = i assert min_index = All indices disallowed min_index _assign_bucket_subset_to_rank bucket_index int bucket_params list torch Tensor bucket_offset int assigned_rank int assigned_ranks_per_bucket list set int - None r Assign ` ` bucket_params ` ` rank least size assigned so far collects relevant information The model parameters given ` ` bucket_params ` ` represents possibly non-strict subset parameters corresponding ` DistributedDataParallel ` bucket Arguments bucket_index int index ` DistributedDataParallel ` gradient bucket bucket_params List torch Tensor subset parameters corresponding bucket assign bucket_offset int offset giving index first element ` ` bucket_params ` ` bucket s full parameter list assigned_rank int group rank assign assigned_ranks_per_bucket list set int ` set ` group ranks assigned each bucket overlap_info = _overlap_info len bucket_params == raise ValueError Empty bucket assignment params_per_rank = overlap_info params_per_rank offsets = overlap_info offsets _bucket_assignments_per_rank_cache assigned_rank bucket_index = _DDPBucketAssignment bucket_index bucket_params bucket_offset global_rank == assigned_rank offsets bucket_index = len params_per_rank assigned_rank params_per_rank assigned_rank extend bucket_params assigned_ranks_per_bucket bucket_index add assigned_rank _overlap_info num_bucket_assignments += property _bucket_assignments_per_rank - list dict int _DDPBucketAssignment r Return DDP bucket parameters assigned per rank ` list ` length world size consisting ` dict ` s mapping bucket indices ` _DDPBucketAssignment ` s each rank assert _overlap_with_ddp ` _bucket_assignments_per_rank ` only used ` overlap_with_ddp=True ` len _bucket_assignments_per_rank_cache _bucket_assignments_per_rank_cache overlap_info = _overlap_info assert overlap_info status == _OverlapStatus INITIALIZED _bucket_assignments_per_rank_cache = _ range world_size params_per_bucket = overlap_info params_per_bucket overlap_info shard_buckets Define assignment threshold approximate uniformity assert overlap_info total_size None ` total_size ` computed threshold = overlap_info total_size world_size type ignore operator size_per_rank = _ range world_size num_buckets = len params_per_bucket overlap_info assigned_ranks_per_bucket = set _ range num_buckets assigned_ranks_per_bucket = overlap_info assigned_ranks_per_bucket overlap_info shard_buckets Assign each DDP bucket entirely single rank bucket_index bucket_params enumerate params_per_bucket assert len bucket_params Empty bucket assigned_rank = _get_assigned_rank bucket_index _assign_bucket_subset_to_rank bucket_index bucket_params assigned_rank assigned_ranks_per_bucket Assign each DDP bucket possibly multiple ranks Specifically sort DDP buckets increasing size each bucket iteratively assign maximal unassigned subset size less than ` threshold ` rank least total size so far -- each such assignment represented ` _DDPBucketAssignment ` instance only contains parameters single DDP bucket params_per_bucket_enum = sorted enumerate params_per_bucket key=lambda x sum p numel p x bucket_index bucket_params params_per_bucket_enum assert len bucket_params Empty bucket bucket_offset = assignment_size = param_index param enumerate bucket_params param_numel = param numel pyrefly ignore unbound-name assignment_size + param_numel = threshold param_index bucket_offset assigned_rank = _get_min_index pyrefly ignore unbound-name size_per_rank assigned_ranks_per_bucket bucket_index Include up including parameter exceeded threshold _assign_bucket_subset_to_rank bucket_index bucket_params bucket_offset param_index bucket_offset assigned_rank assigned_ranks_per_bucket pyrefly ignore unbound-name size_per_rank assigned_rank += assignment_size bucket_offset = param_index assignment_size = assignment_size += param_numel Assign remainder bucket so no assignment spans across two buckets assigned_rank = _get_min_index pyrefly ignore unbound-name size_per_rank assigned_ranks_per_bucket bucket_index _assign_bucket_subset_to_rank bucket_index bucket_params bucket_offset bucket_offset assigned_rank assigned_ranks_per_bucket pyrefly ignore unbound-name size_per_rank assigned_rank += assignment_size _bucket_assignments_per_rank_cache _local_step gradients Optional list Optional torch Tensor = None closure Optional Callable float = None kwargs Any - Optional float r Perform single optimizer step without syncing parameters across ranks Arguments gradients list Optional torch Tensor optional ` list ` length equal number parameters assigned rank containing gradient tensors ` ` None ` ` its elements ` ` None ` ` ` list ` indicates corresponding parameter should updated If argument itself ` ` None ` ` then all parameters updated gradients assumed already populated default ` ` None ` ` closure Callable closure re-evaluates model returns loss optional most optimizers should ` ` None ` ` ` ` gradients ` ` ` ` None ` ` default ` ` None ` ` Returns Optional loss depending underlying local optimizer warning The argument ` ` gradients ` ` should only specified i e ` ` None ` ` ` ` overlap_with_ddp=True ` ` which case ` ZeroRedundancyOptimizer ` wraps functional optimizer Join notify_join_context Check model trainability has changed is_trainable_mask = _get_is_trainable_mask is_trainable_mask = _is_trainable_mask _overlap_with_ddp raise RuntimeError ZeroRedundancyOptimizer ` overlap_with_ddp=True ` does support changing parameter trainability run time logger warning ZeroRedundancyOptimizer detected trainable parameters changed rebuilding parameter buckets enabled _build_param_buckets _is_trainable_mask = is_trainable_mask Sync exposed ` param_groups ` attributes local optimizer case they have been updated _sync_param_groups param_groups optim param_groups Run optimizer step shard only gradients None loss = optim step kwargs closure None optim step closure=closure kwargs assert _overlap_with_ddp Specifying ` gradients ` should used when ` overlap_with_ddp=False ` assert closure None ` closure ` supported when using local functional optimizer loss = optim step gradients=gradients Sync any updated attributes local optimizer exposed ` param_groups ` _sync_param_groups optim param_groups param_groups loss pyrefly ignore bad-override step closure Optional Callable float = None kwargs Any - Optional float r Perform single optimizer step syncs parameters across all ranks Arguments closure Callable closure re-evaluates model returns loss optional most optimizers Returns Optional loss depending underlying local optimizer note Any extra parameters passed base optimizer as-is _overlap_with_ddp logger warning ` step ` should included training loop when ` overlap_with_ddp=True ` None Perform local optimizer step loss = _local_step closure=closure kwargs Sync all updated parameter shards across ranks _sync_params loss join_hook kwargs r Return ZeRO join hook It enables training uneven inputs shadowing collective communications optimizer step Gradients must properly set before hook called Arguments kwargs dict ` dict ` containing any keyword arguments modify behavior join hook run time all ` Joinable ` instances sharing same join context manager forwarded same value ` ` kwargs ` ` This hook does support any keyword arguments i e ` ` kwargs ` ` unused _ZeROJoinHook property join_device - torch device r Return default device _default_device property join_process_group - Any r Return process group process_group load_state_dict state_dict dict str Any - None r Load state pertaining given rank input ` ` state_dict ` ` updating local optimizer needed Arguments state_dict dict optimizer state should object returned call meth ` state_dict ` Raises RuntimeError ` ` overlap_with_ddp=True ` ` method called before ` ZeroRedundancyOptimizer ` instance has been fully initialized which happens once ` DistributedDataParallel ` gradient buckets have been rebuilt _check_overlap_initialized index value state_dict state items param = _index_to_param index _param_to_rank param = rank Clear any state irrelevant rank state_dict state index = None Load parameter state local optimizer optim state param = _recursive_copy_to_device value non_blocking=True device=param device Force zero-dimensional tensors like Adam step CPU state_name state_value optim state param items torch is_tensor state_value state_value dim == optim state param state_name = state_value cpu super load_state_dict state_dict Sync input state exposed local optimizer states _sync_param_groups state_dict param_groups param_groups _sync_param_groups param_groups optim param_groups state_dict - dict str Any r Return last global optimizer state known rank warning If state has been consolidated rank raises runtime error even has state may up-to-date depending when meth ` consolidate_state_dict ` last called Raises RuntimeError ` ` overlap_with_ddp=True ` ` method called before ` ZeroRedundancyOptimizer ` instance has been fully initialized which happens once ` DistributedDataParallel ` gradient buckets have been rebuilt method called without preceding call meth ` consolidate_state_dict ` _check_overlap_initialized len _all_state_dicts == raise RuntimeError Optimizer state has been consolidated rank f Please call ` consolidate_state_dict to= rank ` all ranks beforehand you meant save global state Get possibly-stale global optimizer state uses global parameter indexing state_dict = super state_dict Update global optimizer state local state information factoring translation local global indexing rank local_state_dict enumerate _all_state_dicts local_param_groups = local_state_dict param_groups global_param_groups = _partition_parameters rank assert len local_param_groups == len global_param_groups Mismatch between number local global parameter groups local_param_group global_param_group zip local_param_groups global_param_groups ` local_param_group ` stores local indices while ` global_param_group ` stores tensors directly local_param_indices = local_param_group params global_params = global_param_group params assert len local_param_indices == len global_params Mismatch between number local global parameters parameter group local_param_index global_param zip local_param_indices global_params Update global parameter state any local_param_index local_state_dict state global_param_index = _param_to_index global_param state_dict state global_param_index = local_state_dict state local_param_index Sort parameters state state_dict state = dict sorted state_dict state items state_dict staticmethod _sync_param_groups src_param_groups list dict Any Any dst_param_groups list dict Any Any - None r Sync attributes source parameter groups destination parameter groups Example attributes include learning rate scheduler attributes The two parameter groups should have same length i e same number parameter groups Arguments src_param_groups list dict parameter groups giving attribute settings copy dst_param_groups list dict parameter groups giving attribute settings set assert len src_param_groups == len dst_param_groups Mismatch between number source destination parameter groups src_param_group dst_param_group zip src_param_groups dst_param_groups Sync all attributes except parameters attr filter lambda x x = params src_param_group keys dst_param_group attr = src_param_group attr _build_param_buckets - None r Build parameter buckets ` ` parameters_as_bucket_view=True ` ` For each device stores rank s parameters there bucket represented tensor containing all parameters device assigned given rank parameter update partition This method called constructor any time parameter trainability changed warning The current implementation assumes all parameters bucket same dense type when allocating bucket s tensor warning If model parameters stored across more than one device then storage partitioning must same across all processes order parameter synchronization work parameters_as_bucket_view _overlap_with_ddp ` _buckets i j ` parameters stored device i assigned rank j num_devices = len _device_to_params_per_rank _buckets = _ range num_devices type ignore assignment dev_i device params_per_rank enumerate _device_to_params_per_rank items params params_per_rank bucket_size = dtype = None trainable_params = param params _is_trainable param Clone case parameter previously part bucket avoid data being destroyed param data = param data detach clone bucket_size += param numel trainable_params append param dtype = param dtype assumes all same dtype bucket_size == Create dummy bucket there no parameters bucket = torch zeros device=device Construct bucket assuming all dense same dtype bucket = torch empty bucket_size dtype=dtype device=device offset = param trainable_params offset_next = offset + param numel bucket offset offset_next copy_ param data flatten param data = bucket offset offset_next view_as param data offset = offset_next _buckets dev_i append bucket type ignore arg-type _build_ddp_param_buckets - None r Build DDP bucket parameters assigned rank For each DDP bucket parameters assigned rank flattens data those parameters into single tensor saves tensor ` ` tensor ` ` attribute corresponding ` _DDPBucketAssignment ` instance stored ` ` _bucket_assignments_per_rank ` ` ` DistributedDataParallel ` guarantees parameters corresponding gradient bucket have same device same dtype bucket_assignments _bucket_assignments_per_rank bucket_assignment bucket_assignments values params = bucket_assignment parameters bucket_size = dtype = None param params assert _is_trainable param Model parameter corresponding gradient DDP bucket should require gradient bucket_size += param numel dtype = param dtype assumes all same dtype assert bucket_size Empty bucket Construct bucket tensor assuming all dense same dtype tensor = torch empty bucket_size dtype=dtype device=bucket_assignment device offset = param params offset_next = offset + param numel tensor offset offset_next copy_ param data flatten param data = tensor offset offset_next view_as param data offset = offset_next bucket_assignment tensor = tensor _verify_and_init_params params Any - Union list torch Tensor list dict r Verify type ` ` params ` ` initializes ` ` _all_params ` ` ` list ` all parameters The initializagtion will first make sure provided ` ` params ` ` valid Arguments params Any Candidate parameter list parameter groups verify Raises TypeError ` ` params ` ` has invalid type ValueError ` ` params ` ` empty Returns The persistent form ` ` params ` ` passed into parent ` Optimizer ` constructor -- i e returns ` ` params ` ` ` list ` ensure can iterated over again isinstance params torch Tensor raise TypeError ` params ` argument should iterable f Tensors got torch typename params try all_params = list params except TypeError e raise TypeError ` params ` argument should iterable Tensors f dicts got torch typename params e len all_params == raise ValueError ZeroRedundancyOptimizer got empty parameter list all_tensors = True all_dicts = True param all_params all_tensors = isinstance param torch Tensor all_dicts = isinstance param dict all_tensors all_dicts raise TypeError ` params ` argument should iterable Tensors dicts Ensure ` _all_params ` contains list all parameters all_tensors _all_params = all_params all_dicts _all_params = ` all_params ` contains parameter groups parameters param_group all_params params param_group raise ValueError Each parameter group passed-in via ` params ` must have params key mapping parameters group _all_params extend param_group params all_params _verify_same_dense_param_type - None r Verify all parameters same dense type The method assumes ` ` _all_params ` ` has been initialized non-empty Raises ValueError ` ` params ` ` contains sparse parameters parameters varying dense types NOTE This method can removed once support sparse parameters varying parameter types added typename = torch typename _all_params _all_params is_sparse raise ValueError ZeroRedundancyOptimizer only supports using same dense type all parameters got f typename param _all_params other_typename = torch typename param other_typename = typename raise ValueError ZeroRedundancyOptimizer only supports using same dense type all f parameters got both typename f other_typename _get_is_trainable_mask - list bool r Return boolean mask indicating each parameter trainable ` ` requires_grad ` ` list map _is_trainable _all_params _init_local_optimizer - None r Initialize rank s local optimizer responsible its subset parameters The local optimizer saved ` ` optim ` ` assert _optim_constructor None The local optimizer has been set param_groups = _partition_parameters rank ` overlap_with_ddp=True ` requires local functional optimizer _overlap_with_ddp Functional optimizers only support single parameter group require passing parameters list assert len param_groups == Initializing local functional optimizer more than one parameter group params = param_groups params Try pass ` _allow_empty_param_list=True ` avoid erroring _allow_empty_param_list inspect signature _optim_constructor parameters optim Any = _optim_constructor params _optim_defaults _allow_empty_param_list=True logger warning s does support argument ` _allow_empty_param_list ` ZeroRedundancyOptimizer may error due empty parameter list _optim_constructor optim Any = _optim_constructor params _optim_defaults type ignore no-redef Log information about DDP ZeRO bucketing dist get_debug_level = dist DebugLevel OFF local_numel = sum p numel p params num_assigned_buckets = len _bucket_assignments_per_rank global_rank logger info rank s s parameters across s buckets global_rank local_numel num_assigned_buckets global_rank == logger info s DDP buckets s bucket assignments len _overlap_info params_per_bucket _overlap_info num_bucket_assignments NOTE Passing ` param_groups ` into local optimizer constructor bypasses empty parameter list check optim Optimizer = _optim_constructor param_groups _optim_defaults type ignore no-redef TODO Manually add ` param_groups ` using functional optimizer remove when functional optimizers support multiple parameter groups _overlap_with_ddp hasattr optim param_groups assert hasattr optim param_group The functional optimizer should set least one attributes ` param_group ` ` param_groups ` optim param_groups = optim param_group type ignore attr-defined _sync_param_groups optim param_groups param_groups _init_zero_for_overlap - None r Perform delayed initialization local optimizer supporting data structures assert _overlap_with_ddp ` _init_zero_for_overlap ` should only called when ` overlap_with_ddp=True ` _overlap_info status = _OverlapStatus INITIALIZED _clear_cache _partition_parameters _overlap_info params_per_rank _build_ddp_param_buckets _init_local_optimizer _get_assigned_rank bucket_index int - int r Return single rank assigned ` DistributedDataParallel ` gradient bucket Arguments bucket_index int index ` DistributedDataParallel ` bucket which get assigned rank assert _overlap_info shard_buckets The bucket assignment requires global bucket information will computed later there should no need use method bucket_index world_size _check_overlap_initialized r Check delayed initialization depending value ` ` overlap_with_ddp ` ` The delayed initialization has occurred see meth ` _init_zero_for_overlap ` ` ` overlap_with_ddp=True ` ` raises ` ` RuntimeError ` ` This should preface methods should run before delayed initialization Raises RuntimeError ` ` overlap_with_ddp=True ` ` meth ` _init_zero_for_overlap ` has been called _overlap_with_ddp _overlap_info status = _OverlapStatus INITIALIZED raise RuntimeError This method should called until ZeroRedundancyOptimizer instance has been fully initialized _get_optimizer_constructor optimizer_class Any - Any r Return optimizer constructor using validation transformation depending ` ` overlap_with_ddp ` ` Returns - ` ` optimizer_class ` ` ` ` overlap_with_ddp=False ` ` ` ` optimizer_class ` ` functional optimizer - ` ` optimizer_class ` ` ` ` overlap_with_ddp=True ` ` ` ` optimizer_class ` ` already functional optimizer - The functional equivalent ` ` optimizer_class ` ` ` ` overlap_with_ddp=True ` ` ` ` optimizer_class ` ` already functional optimizer assuming equivalent exists Raises ValueError - ` ` overlap_with_ddp=True ` ` ` ` optimizer_class ` ` neither functional optimizer nor translatable functional optimizer - ` ` overlap_with_ddp=False ` ` ` ` optimizer_class ` ` functional optimizer functional_optims = functional_optim_map values _overlap_with_ddp optimizer_class functional_optims Using functional optimizer only supported when ` overlap_with_ddp=True ` raise ValueError f Passing functional optimizer optimizer_class when ` overlap_with_ddp=False ` optimizer_class optimizer_class functional_optims Already functional optimizer optimizer_class optimizer_class functional_optim_map Translate passed-in optimizer its functional equivalent ` overlap_with_ddp=True ` optim_constructor = functional_optim_map optimizer_class logger info Using functional optimizer s instead s since ` overlap_with_ddp=True ` optim_constructor optimizer_class optim_constructor raise ValueError Using ` ddp_with_overlap=True ` requires using functional optimizer there no supported functional f optimizer equivalent optimizer_class