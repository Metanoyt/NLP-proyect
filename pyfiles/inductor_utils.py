mypy ignore-errors contextlib functools logging os re sys unittest subprocess CalledProcessError torch torch _inductor async_compile noqa F required warm up AsyncCompile pools torch _inductor codecache CppCodeCache torch _inductor codegen common get_custom_backend_config_for_device get_custom_backend_pass_for_device get_scheduling_for_device get_wrapper_codegen_for_device init_backend_registration register_backend_for_device torch _inductor codegen wrapper PythonWrapperCodegen torch _inductor compile_fx shape_env_from_inputs torch _inductor custom_graph_pass CustomGraphModulePass torch _inductor graph GraphLowering torch _inductor utils get_gpu_shared_memory get_gpu_type GPU_TYPES is_big_gpu is_gpu OrderedSet torch fx experimental proxy_tensor make_fx torch testing _internal common_device_type get_desired_device_type_test_bases torch testing _internal common_utils IS_CI IS_FBCODE IS_WINDOWS LazyVal TestCase torch utils _config_module ConfigModule torch utils _helion has_helion torch utils _triton has_triton log logging Logger = logging getLogger __name__ test_cpu try CppCodeCache load IS_FBCODE except CalledProcessError OSError torch _inductor exc InvalidCxxCompiler torch _inductor exc CppCompileError False HAS_CPU = LazyVal test_cpu HAS_TRITON = has_triton HAS_HELION = has_helion HAS_TRITON triton TRITON_HAS_CPU = cpu triton backends backends TRITON_HAS_CPU = False HAS_CUDA_AND_TRITON = torch cuda is_available HAS_TRITON HAS_XPU_AND_TRITON = torch xpu is_available HAS_TRITON HAS_MPS = torch mps is_available HAS_GPU = HAS_CUDA_AND_TRITON HAS_XPU_AND_TRITON HAS_GPU_AND_TRITON = HAS_GPU GPU_TYPE = get_gpu_type HAS_MULTIGPU = any getattr torch gpu is_available getattr torch gpu device_count = gpu GPU_TYPES _desired_test_bases = get_desired_device_type_test_bases allow_xpu=True RUN_GPU = HAS_GPU any is_gpu getattr x device_type x _desired_test_bases RUN_CPU = HAS_CPU any getattr x device_type == cpu x _desired_test_bases _check_has_dynamic_shape TestCase code for_loop_found = False has_dynamic = False lines = code split \n line lines line for_loop_found = True re search r ks line None has_dynamic = True break assertTrue has_dynamic msg=f Failed find dynamic loop variable\n code assertTrue for_loop_found f Failed find loop\n code skipDeviceIf cond msg device cond decorate_fn fn functools wraps fn inner args kwargs hasattr device warn_msg = Expect test have attribute device found hasattr device_type warn_msg += Consider using skip device decorators common_device_type py log warning warn_msg device == device raise unittest SkipTest msg fn args kwargs inner decorate_fn fn fn decorate_fn skip_windows_ci name str file str - None IS_WINDOWS IS_CI module = os path basename file strip py sys stderr write f Windows CI does have necessary dependencies module tests yet\n name == __main__ sys exit raise unittest SkipTest requires sympy functorch filelock TODO Remove HAS_MPS condition when ` HAS_GPU ` includes HAS_MPS requires_gpu = functools partial unittest skipIf HAS_GPU HAS_MPS requires gpu requires_triton = functools partial unittest skipIf HAS_TRITON requires triton requires_helion = functools partial unittest skipIf HAS_HELION requires helion requires_cuda_with_enough_memory min_mem_required inner fn torch cuda is_available torch cuda get_device_properties total_memory min_mem_required unittest skip f Only CUDA device has least min_mem_required e f GB memory safe fn fn inner skipCUDAIf = functools partial skipDeviceIf device= cuda skipXPUIf = functools partial skipDeviceIf device= xpu skipCPUIf = functools partial skipDeviceIf device= cpu IS_A = LazyVal lambda HAS_CUDA_AND_TRITON get_gpu_shared_memory == IS_H = LazyVal lambda HAS_CUDA_AND_TRITON get_gpu_shared_memory == IS_BIG_GPU = LazyVal lambda HAS_GPU_AND_TRITON is_big_gpu dummy_graph - GraphLowering Create graph This useful unit testing code which accesses V graph sizevars example_inputs = torch randn _ range gm = make_fx torch add tracing_mode= fake example_inputs shape_env = shape_env_from_inputs example_inputs graph = GraphLowering gm shape_env=shape_env graph maybe_skip_size_asserts op For certain ops there meta eager implementation returns different strides This cause size strides assert fail Skip adding those asserts now op aten_name fft_hfftn fft_hfft fft_hfft fft_ihfftn fft_fft fft_fft fft_fftn fft_ifft fft_ifft fft_ifftn fft_irfft fft_irfft fft_irfftn fft_ihfft fft_ihfft fft_rfft fft_rfft fft_rfftn linalg_eig linalg_eigvals TORCHINDUCTOR_SIZE_ASSERTS os environ torch _inductor config patch size_asserts=False contextlib nullcontext get_func_call - str void inductor_entry_impl torch _inductor config cpp_wrapper call get_kernel_launch - str call_triton_ torch _inductor config cpp_wrapper run clone_preserve_strides_offset x device=None isinstance x torch Tensor x buffer = torch as_strided x x untyped_storage size x element_size device buffer = buffer clone buffer = buffer device copy=True out = torch as_strided buffer x size x stride x storage_offset out define e m e m constants E M _MAX_POS = torch finfo torch float _e m fn max E M _MAX_POS = torch finfo torch float _e m max E M FNUZ_MAX_POS = torch finfo torch float _e m fnuz max E M FNUZ_MAX_POS = torch finfo torch float _e m fnuz max FP _MAX_POS float = torch finfo torch float max EPS float = e- Tensor = torch Tensor _to_fp _saturated x Tensor float _dtype torch dtype - Tensor The default behavior PyTorch casting ` float _e m fn ` ` e m ` saturate In context we should saturate A common case where we want saturate when history tensor has maximum value ` amax ` current amax value ` amax ` where ` amax amax ` This common when using delayed scaling float _dtype == torch float _e m fn x = x clamp min=- E M _MAX_POS max=E M _MAX_POS float _dtype == torch float _e m x = x clamp min=- E M _MAX_POS max=E M _MAX_POS float _dtype == torch float _e m fnuz x = x clamp min=- E M FNUZ_MAX_POS max=E M FNUZ_MAX_POS float _dtype == torch float _e m fnuz x = x clamp min=- E M FNUZ_MAX_POS max=E M FNUZ_MAX_POS raise TypeError f Unsupported float _dtype float _dtype x float _dtype torch no_grad _amax_to_scale amax torch Tensor float _dtype torch dtype orig_dtype torch dtype - torch Tensor To make scale dtype fp accuracy amax = amax float float _dtype == torch float _e m fn res = E M _MAX_POS torch clamp amax min=EPS e m res = E M _MAX_POS torch clamp amax min=EPS Ensure scale representable float helps when amax small We assuming we don t need care about float bfloat orig_dtype torch float res = torch clamp res max=FP _MAX_POS res _quantize_tensorwise x Tensor float _dtype torch dtype amax = torch max torch abs x scale = _amax_to_scale amax float _dtype x dtype x_fp = _to_fp _saturated x scale float _dtype inverse_scale = scale reciprocal x_fp inverse_scale _quantize_rowwise x Tensor float _dtype torch dtype amax = torch max torch abs x dim= keepdim=True values scale = _amax_to_scale amax float _dtype x dtype x_fp = _to_fp _saturated x scale float _dtype inverse_scale = scale reciprocal x_fp inverse_scale _quantize_blockwise x Tensor float _dtype torch dtype block_outer int block_inner int min_outer = min block_outer x shape min_inner = min block_inner x shape x = x unflatten - min_inner unflatten - min_outer amax = x abs amax dim= keepdim=True float scale = _amax_to_scale amax float _dtype x dtype x = x flatten flatten scale = scale flatten flatten scale_expanded = scale repeat_interleave min_outer dim= repeat_interleave min_inner dim= x_fp = _to_fp _saturated x scale_expanded Ensures scaling doesn t cause inf nan values float _dtype inverse_scale = scale reciprocal x_fp inverse_scale MockGraphHandler GraphLowering Minimal mock graph handler testing virtualized context __init__ name_to_buffer=None torch _inductor sizevars sizevars = torch _inductor sizevars SizeVarAllocator name_to_buffer = name_to_buffer graph_inputs = mutated_buffers = OrderedSet removed_buffers = OrderedSet constants = scheduler = None get_dtype buffer_name str - torch dtype noqa ARG Return default dtype any buffer testing torch float contextlib contextmanager patch_inductor_backend device str python_wrapper_codegen PythonWrapperCodegen = None custom_pass CustomGraphModulePass = None custom_backend_config ConfigModule = None Patch inductor backend specific device Make sure backend already registered init_backend_registration Get original registration parameters original_scheduling = get_scheduling_for_device device original_python_wrapper = get_wrapper_codegen_for_device device False original_cpp_wrapper = get_wrapper_codegen_for_device device True original_fx_wrapper = get_wrapper_codegen_for_device device fx_wrapper=True original_custom_pass = get_custom_backend_pass_for_device device original_custom_backend_config = get_custom_backend_config_for_device device try Register modified backend device register_backend_for_device device original_scheduling python_wrapper_codegen python_wrapper_codegen None original_python_wrapper original_cpp_wrapper original_fx_wrapper custom_pass custom_pass None original_custom_pass custom_backend_config custom_backend_config None original_custom_backend_config yield finally Restore original backend register_backend_for_device device original_scheduling original_python_wrapper original_cpp_wrapper original_fx_wrapper original_custom_pass original_custom_backend_config