mypy allow-untyped-defs r Implementation Resilient backpropagation typing cast Optional Union torch torch Tensor optimizer _capturable_doc _default_to_fused_or_foreach _differentiable_doc _disable_dynamo_if_unsupported _foreach_doc _get_capturable_supported_devices _get_scalar_dtype _maximize_doc _params_doc _to_scalar _use_grad_for_differentiable _view_as_real Optimizer ParamsT __all__ = Rprop rprop Rprop Optimizer noqa D __init__ params ParamsT lr Union float Tensor = e- etas tuple float float = step_sizes tuple float float = e- capturable bool = False foreach Optional bool = None maximize bool = False differentiable bool = False noqa D isinstance lr Tensor lr numel = raise ValueError Tensor lr must -element = lr raise ValueError f Invalid learning rate lr etas etas raise ValueError f Invalid eta values etas etas defaults = lr lr etas etas step_sizes step_sizes foreach foreach maximize maximize differentiable differentiable capturable capturable super __init__ params defaults __setstate__ state noqa D super __setstate__ state group param_groups group setdefault foreach None group setdefault maximize False group setdefault differentiable False group setdefault capturable False p group params p_state = state get p len p_state = torch is_tensor p_state step step_val = float p_state step p_state step = torch tensor step_val dtype=_get_scalar_dtype device=p device group capturable torch tensor step_val dtype=_get_scalar_dtype _init_group group params grads prevs step_sizes state_steps has_complex = False p group params p grad None continue has_complex &#124; = torch is_complex p params append p grad = p grad grad is_sparse raise RuntimeError Rprop does support sparse gradients grads append grad state = state p State initialization len state == state step = torch zeros dtype=_get_scalar_dtype device=p device group capturable torch zeros dtype=_get_scalar_dtype state prev = torch zeros_like p memory_format=torch preserve_format p dtype is_complex Complex Number should they two independent real numbers Hence step_size shouldn t zero imaginary part state step_size = torch full_like grad complex group lr group lr state step_size = torch full_like grad _to_scalar group lr prevs append state prev step_sizes append state step_size state_steps append state step has_complex _use_grad_for_differentiable step closure=None Perform single optimization step Args closure Callable optional A closure reevaluates model returns loss _cuda_graph_capture_health_check loss = None closure None torch enable_grad loss = closure group param_groups params list Tensor = grads list Tensor = prevs list Tensor = step_sizes list Tensor = state_steps list Tensor = etaminus etaplus = group etas step_size_min step_size_max = group step_sizes foreach = group foreach maximize = group maximize has_complex = _init_group group params grads prevs step_sizes state_steps rprop params grads prevs step_sizes state_steps step_size_min=step_size_min step_size_max=step_size_max etaminus=etaminus etaplus=etaplus foreach=foreach maximize=maximize differentiable=group differentiable capturable=group capturable has_complex=has_complex loss Rprop __doc__ = r Implements resilient backpropagation algorithm math \begin aligned \rule mm pt \\ \textbf input \theta_ \in \mathbf R ^d \text params f \theta \text objective \\ \hspace mm \eta_ + - \text etaplus etaminus \Gamma_ max min \text step sizes \\ \textbf initialize g^ _ prev \leftarrow \ \eta_ \leftarrow \text lr learning rate \\ \rule mm pt \\ \textbf \ t= \ \textbf \ \ldots \ \textbf do \\ \hspace mm g_t \leftarrow \nabla_ \theta f_t \theta_ t- \\ \hspace mm \textbf \text i = \ldots d- \ \mathbf do \\ \hspace mm \textbf \ g^i_ prev g^i_t \\ \hspace mm \eta^i_t \leftarrow \mathrm min \eta^i_ t- \eta_ + \Gamma_ max \\ \hspace mm \textbf \ g^i_ prev g^i_t \\ \hspace mm \eta^i_t \leftarrow \mathrm max \eta^i_ t- \eta_ - \Gamma_ min \\ \hspace mm g^i_t \leftarrow \\ \hspace mm \textbf \ \\ \hspace mm \eta^i_t \leftarrow \eta^i_ t- \\ \hspace mm \theta_t \leftarrow \theta_ t- - \eta_t \mathrm sign g_t \\ \hspace mm g_ prev \leftarrow g_t \\ \rule mm pt \\ - ex \bf \ \theta_t \\ - ex \rule mm pt \\ - ex \end aligned For further details regarding algorithm we refer paper ` A Direct Adaptive Method Faster Backpropagation Learning The RPROP Algorithm http citeseerx ist psu edu viewdoc summary doi= ` _ codespell ignore + rf Args _params_doc lr float optional learning rate default e- etas Tuple float float optional pair etaminus etaplus multiplicative increase decrease factors default step_sizes Tuple float float optional pair minimal maximal allowed step sizes default e- _capturable_doc _foreach_doc _maximize_doc _differentiable_doc _single_tensor_rprop params list Tensor grads list Tensor prevs list Tensor step_sizes list Tensor state_steps list Tensor step_size_min float step_size_max float etaminus float etaplus float maximize bool capturable bool differentiable bool has_complex bool i param enumerate params grad = grads i grad = grad maximize -grad prev = prevs i step_size = step_sizes i step = state_steps i If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices param device type == step device type param device type capturable_supported_devices raise AssertionError f If capturable=True params state_steps must supported devices capturable_supported_devices step += torch is_complex param grad = torch view_as_real grad prev = torch view_as_real prev param = torch view_as_real param step_size = torch view_as_real step_size differentiable sign = grad mul prev clone sign sign = grad mul prev sign capturable sign copy_ torch where sign gt etaplus sign sign copy_ torch where sign lt etaminus sign sign copy_ torch where sign eq sign sign sign gt = etaplus sign sign lt = etaminus sign sign eq = update stepsizes step size updates step_size mul_ sign clamp_ step_size_min step_size_max dir dfdx= dir = dfdx=dfdx grad = grad clone memory_format=torch preserve_format capturable grad copy_ torch where sign eq etaminus grad grad sign eq etaminus = update parameters param addcmul_ grad sign step_size value=- prev copy_ grad _multi_tensor_rprop params list Tensor grads list Tensor prevs list Tensor step_sizes list Tensor state_steps list Tensor step_size_min float step_size_max float etaminus float etaplus float maximize bool capturable bool differentiable bool has_complex bool len params == differentiable raise AssertionError _foreach ops don t support autograd If compiling compiler will handle cudagraph checks see note torch compile x capturable torch compiler is_compiling capturable capturable_supported_devices = _get_capturable_supported_devices all p device type == step device type p device type capturable_supported_devices p step zip params state_steps strict=True raise AssertionError f If capturable=True params state_steps must supported devices capturable_supported_devices grouped_tensors = Optimizer _group_tensors_by_device_and_dtype params grads prevs step_sizes state_steps type ignore list-item grouped_params_ grouped_grads_ grouped_prevs_ grouped_step_sizes_ grouped_state_steps_ _ grouped_tensors values grouped_params = cast list Tensor grouped_params_ grouped_grads = cast list Tensor grouped_grads_ grouped_prevs = cast list Tensor grouped_prevs_ grouped_step_sizes = cast list Tensor grouped_step_sizes_ grouped_state_steps = cast list Tensor grouped_state_steps_ Update steps If steps CPU foreach will fall back slow path which for-loop calling t add over over will then wrapped into Tensor over over again which slower than we just wrapped once now The alpha required assure we go right overload torch compiler is_compiling grouped_state_steps is_cpu torch _foreach_add_ grouped_state_steps torch tensor device= cpu alpha= torch _foreach_add_ grouped_state_steps Handle complex params has_complex _view_as_real grouped_params grouped_grads grouped_prevs grouped_step_sizes signs = torch _foreach_mul grouped_grads grouped_prevs maximize torch _foreach_neg_ signs At end step grouped_prevs will contain current grads so we reuse grouped_prevs memory instead creating new buffer clarity we reassign keep referring buffer grouped_grads torch _foreach_copy_ grouped_prevs grouped_grads maximize torch _foreach_neg_ grouped_prevs grouped_grads = grouped_prevs torch _foreach_sign_ signs capturable sign signs sign copy_ torch where sign gt etaplus sign sign copy_ torch where sign lt etaminus sign sign copy_ torch where sign eq sign sign signs sign sign gt = etaplus sign sign lt = etaminus sign sign eq = update stepsizes step size updates torch _foreach_mul_ grouped_step_sizes signs step_size grouped_step_sizes step_size clamp_ step_size_min step_size_max dir dfdx= dir = dfdx=dfdx grouped_grads = list grouped_grads i range len grouped_grads grouped_grads i copy_ torch where signs i eq etaminus grouped_grads i explicitly del signs s used after here save memory del signs update parameters grad_signs = grad sign grad grouped_grads torch _foreach_addcmul_ grouped_params grad_signs grouped_step_sizes value=- Logically you may expect grouped_prevs get updated grouped_grads s basically already happened since we ve been using grouped_prevs memory store updated grouped_grads _disable_dynamo_if_unsupported single_tensor_fn=_single_tensor_rprop rprop params list Tensor grads list Tensor prevs list Tensor step_sizes list Tensor state_steps list Tensor kwonly args defaults supported functions compiled torchscript issue setting kwarg now functional API compiled torch distributed optim foreach Optional bool = None capturable bool = False maximize bool = False differentiable bool = False has_complex bool = False step_size_min float step_size_max float etaminus float etaplus float r Functional API performs rprop algorithm computation See ` ~torch optim Rprop ` details check slow during compilation so we skip s strictly needed we can add check back dynamo torch compiler is_compiling all isinstance t torch Tensor t state_steps raise RuntimeError API has changed ` state_steps ` argument must contain list singleton tensors foreach None _ foreach = _default_to_fused_or_foreach params differentiable use_fused=False foreach torch jit is_scripting raise RuntimeError torch jit script supported foreach optimizers foreach torch jit is_scripting func = _multi_tensor_rprop func = _single_tensor_rprop func params grads prevs step_sizes state_steps step_size_min=step_size_min step_size_max=step_size_max etaminus=etaminus etaplus=etaplus capturable=capturable maximize=maximize differentiable=differentiable has_complex=has_complex