mypy allow-untyped-defs collections OrderedDict contextlib typing Any tensorboard compat proto config_pb RunMetadata tensorboard compat proto graph_pb GraphDef tensorboard compat proto step_stats_pb StepStats DeviceStepStats tensorboard compat proto versions_pb VersionDef torch _proto_graph node_proto methods_OP = attributeNames hasMultipleOutputs hasUses inputs kind outputs outputsSize scopeName Some additional methods explure methods_IO unique type int type type Tensor torch _C Type But below sufficient now methods_IO = node offset debugName GETATTR_KIND = prim GetAttr CLASSTYPE_KIND = ClassType NodeBase __init__ debugName=None inputs=None scope=None tensor_size=None op_type= UnSpecified attributes= TODO Specify __slots__ potentially used namedtuple instead debugName = debugName inputs = inputs tensor_size = tensor_size kind = op_type attributes = attributes scope = scope __repr__ repr = repr append str type repr extend m + + str getattr m + str type getattr m m dir __ m \n join repr + \n\n NodePy NodeBase __init__ node_cpp valid_methods super __init__ node_cpp valid_methods = valid_methods inputs = m valid_methods m == inputs m == outputs list_of_node = list getattr node_cpp m io_unique_names = io_tensor_sizes = n list_of_node io_unique_names append n debugName n isCompleteTensor io_tensor_sizes append n type sizes io_tensor_sizes append None setattr m io_unique_names setattr m + tensor_size io_tensor_sizes setattr m getattr node_cpp m NodePyIO NodePy __init__ node_cpp input_or_output=None super __init__ node_cpp methods_IO try tensor_size = node_cpp type sizes except RuntimeError tensor_size = fail when constant model used tensor_size = tensor_size Kind attribute string purely descriptive will shown detailed information node TensorBoard s graph plugin NodePyOP nodes get their kind method kind = Parameter input_or_output input_or_output = input_or_output kind = IO Node NodePyOP NodePy __init__ node_cpp super __init__ node_cpp methods_OP Replace single quote which causes strange behavior TensorBoard TODO See we can remove future attributes = str k _node_get node_cpp k k node_cpp attributeNames replace kind = node_cpp kind GraphPy Helper convert torch nn Module GraphDef proto visualization TensorBoard GraphDef generation operates two passes In first pass all nodes read saved two lists One list input output nodes nodes_io which only have inbound outbound connections both Another list internal operator nodes nodes_op The first pass also saves all scope name appeared nodes scope_name_appeared list later processing In second pass scope names fully applied all nodes debugNameToScopedName mapping node s ID its fully qualified scope name e g Net Linear Unfortunately torch jit doesn t have totally correct scope output so nontrivial The function populate_namespace_from_OP_to_IO find_common_root used assign scope name node based connection between nodes heuristic kind way Bookkeeping done shallowest_scope_name scope_name_appeared __init__ nodes_op = nodes_io = OrderedDict unique_name_to_scoped_name = shallowest_scope_name = default scope_name_appeared = append x isinstance x NodePyIO nodes_io x debugName = x isinstance x NodePyOP nodes_op append x printall print all nodes node nodes_op print node key nodes_io print nodes_io key find_common_root fullscope scope_name_appeared fullscope shallowest_scope_name = fullscope split populate_namespace_from_OP_to_IO node nodes_op node_output outputSize zip node outputs node outputstensor_size strict=True scope_name_appeared append node scopeName nodes_io node_output = NodeBase node_output node inputs node scopeName outputSize op_type=node kind attributes=node attributes find_common_root node nodes_op input_node_id node inputs unique_name_to_scoped_name input_node_id = node scopeName + + input_node_id key node nodes_io items type node NodeBase pyrefly ignore unsupported-operation unique_name_to_scoped_name key = node scope + + node debugName hasattr node input_or_output unique_name_to_scoped_name key = node input_or_output + + node debugName hasattr node scope node scope None unique_name_to_scoped_name key = node scope + + node debugName node scope == shallowest_scope_name unique_name_to_scoped_name node debugName = pyrefly ignore unsupported-operation shallowest_scope_name + + node debugName replace name key node nodes_io items nodes_io key inputs = unique_name_to_scoped_name node_input_id node_input_id node inputs node debugName unique_name_to_scoped_name nodes_io key debugName = unique_name_to_scoped_name node debugName to_proto Convert graph representation GraphPy object TensorBoard required format TODO compute correct memory usage CPU time once PyTorch supports nodes = node_proto v debugName input=v inputs outputsize=v tensor_size op=v kind attributes=v attributes v nodes_io values nodes parse graph trace args=None omit_useless_nodes=True Parse optimized PyTorch model graph produces list nodes node stats Useful eventual conversion TensorBoard protobuf format Args graph PyTorch module The model graph parsed trace PyTorch JIT TracedModule The model trace parsed args tuple input tensor s model omit_useless_nodes boolean Whether remove nodes graph nodes_py = GraphPy node graph inputs omit_useless_nodes len node uses == number user node = number outputs fanout continue node type kind = CLASSTYPE_KIND nodes_py append NodePyIO node input attr_to_scope dict Any str = node graph nodes node kind == GETATTR_KIND attr_name = node s name attr_key = node output debugName parent = node input node parent kind == GETATTR_KIND If parent node top-level node parent_attr_key = parent output debugName parent_scope = attr_to_scope parent_attr_key attr_scope = parent_scope split - attr_to_scope attr_key = f parent_scope attr_scope attr_name attr_to_scope attr_key = f __module attr_name We don t need classtype nodes scope will provide information node output type kind = CLASSTYPE_KIND node_py = NodePyOP node node_py scopeName = attr_to_scope attr_key type ignore attr-defined nodes_py append node_py nodes_py append NodePyOP node i node enumerate graph outputs Create sink nodes output ops node_pyio = NodePyIO node output node_pyio debugName = f output i + node_pyio inputs = node debugName nodes_py append node_pyio parse_traced_name module isinstance module torch jit TracedModule module_name = module _name module_name = getattr module original_name Module module_name alias_to_name = base_name = parse_traced_name trace name module trace named_modules prefix= __module mod_name = parse_traced_name module attr_name = name split - alias_to_name name = f mod_name attr_name node nodes_py nodes_op module_aliases = node scopeName split replacements = alias_to_name alias alias alias_to_name alias split - alias module_aliases node scopeName = base_name any replacements node scopeName += + join replacements nodes_py populate_namespace_from_OP_to_IO nodes_py to_proto graph model args verbose=False use_strict_trace=True Process PyTorch model produces ` GraphDef ` proto can logged TensorBoard Args model PyTorch module The model parsed args tuple input tensor s model verbose bool Whether print out verbose information while processing use_strict_trace bool Whether pass keyword argument ` strict ` ` torch jit trace ` Pass False when you want tracer record your mutable container types list dict _set_model_to_eval model try trace = torch jit trace model args strict=use_strict_trace graph = trace graph torch _C _jit_pass_inline graph except RuntimeError e print e print Error occurs No graph saved raise e verbose print graph list_of_nodes = parse graph trace args We hardcoding run CPU even though might have actually run GPU Note what shown TensorBoard has no bearing actual execution TODO See we can extract GPU vs CPU information PyTorch model pass correctly TensorBoard Definition StepStats DeviceStepStats can found https github com tensorflow tensorboard blob master tensorboard plugins graph tf_graph_common proto ts https github com tensorflow tensorboard blob master tensorboard compat proto step_stats proto stepstats = RunMetadata step_stats=StepStats dev_stats= DeviceStepStats device= device CPU GraphDef node=list_of_nodes versions=VersionDef producer= stepstats The producer version has been reverse engineered standard TensorBoard logged data contextlib contextmanager _set_model_to_eval model Context manager temporarily set training mode ` ` model ` ` eval isinstance model torch jit ScriptFunction originally_training = model training model train False try yield finally model train originally_training Do nothing ScriptFunction try yield finally pass _node_get node torch _C Node key str Get attributes node which polymorphic over type sel = node kindOf key getattr node sel key