mypy allow-untyped-defs The following example demonstrates how train ConvNeXt model intermediate activations sharded across multiple GPUs via DTensor To run example use following command torchrun -- standalone -- nnodes= -- nproc-per-node= convnext_example py os time torch torch distributed dist torch nn nn torch distributed tensor DeviceMesh distribute_module distribute_tensor init_device_mesh Replicate Shard WORLD_SIZE = ITER_TIME = LayerNorm nn Module __init__ normalized_shape eps= e- data_format=torch contiguous_format super __init__ weight = nn Parameter torch ones normalized_shape bias = nn Parameter torch zeros normalized_shape eps = eps data_format = data_format data_format = torch contiguous_format raise NotImplementedError normalized_shape = normalized_shape forward x u = x mean keepdim=True s = x - u pow mean keepdim=True x = x - u torch sqrt s + eps x = weight None None x + bias None None x Block nn Module __init__ dim drop_path= layer_scale_init_value= e- super __init__ dwconv = nn Conv d dim dim kernel_size= padding= groups=dim depthwise conv norm = LayerNorm dim eps= e- data_format=torch contiguous_format pwconv = nn Conv d dim dim kernel_size= stride= nn Linear dim dim pointwise x convs implemented linear layers act = nn GELU pwconv = nn Conv d dim dim kernel_size= stride= nn Linear dim dim gamma = nn Parameter layer_scale_init_value torch ones dim requires_grad=True layer_scale_init_value None drop_path = nn Identity forward x input_x = x x = dwconv x x = norm x x = pwconv x x = act x x = pwconv x gamma None x = gamma drop_path x x = input_x + x x DownSampling nn Module __init__ dim_in= dim_out= down_scale= norm_first=False super __init__ norm_first = norm_first norm_first norm = LayerNorm dim_in eps= e- data_format=torch contiguous_format conv = nn Conv d dim_in dim_out kernel_size=down_scale stride=down_scale conv = nn Conv d dim_in dim_out kernel_size=down_scale stride=down_scale norm = LayerNorm dim_out eps= e- data_format=torch contiguous_format forward x norm_first conv norm x norm conv x torch no_grad init_weights m type m nn Conv d type m nn Linear nn init ones_ m weight m bias None nn init zeros_ m bias ConvNeXt nn Module __init__ in_chans= num_classes= depths= noqa B dims= noqa B drop_path_rate= layer_scale_init_value= e- head_init_scale= super __init__ downsample_layers = nn ModuleList stem = DownSampling in_chans dims norm_first=False downsample_layers append stem i range len dims - downsample_layer = DownSampling dims i dims i + norm_first=True downsample_layers append downsample_layer stages = nn ModuleList dp_rates = x item x torch linspace drop_path_rate sum depths cur = i range len dims stage = nn Sequential Block dim=dims i drop_path=dp_rates cur + j layer_scale_init_value=layer_scale_init_value j range depths i stages append stage cur += depths i head = nn Linear dims - num_classes apply init_weights forward x i range len stages x = downsample_layers i x x = stages i x x = x mean - - x = head x x _conv_fn name str module nn Module device_mesh DeviceMesh - None name param module named_parameters dist_spec = Replicate dist_param = torch nn Parameter distribute_tensor param device_mesh dist_spec dist_param register_hook lambda grad grad redistribute placements=dist_spec name = _ join name split module register_parameter name dist_param train_convnext_example device_type = cuda world_size = int os environ WORLD_SIZE mesh = init_device_mesh device_type world_size rank = mesh get_rank in_shape = output_shape = torch manual_seed model = ConvNeXt depths= dims= drop_path_rate= num_classes= device_type model = distribute_module model mesh _conv_fn input_fn=None output_fn=None criterion = torch nn CrossEntropyLoss optimizer = torch optim Adam model parameters lr= e- amsgrad=False x = torch randn in_shape device_type requires_grad_ y_target = torch empty output_shape dtype=torch long random_ output_shape device_type x = distribute_tensor x mesh Shard y_target = distribute_tensor y_target mesh Replicate warm up y = model x loss = criterion y y_target optimizer zero_grad loss backward optimizer step torch cuda synchronize forward_time = backward_time = start = time time _ range ITER_TIME t = time time y = model x torch cuda synchronize t = time time loss = criterion y y_target optimizer zero_grad t = time time loss backward torch cuda synchronize t = time time optimizer step forward_time += t - t backward_time += t - t torch cuda synchronize end = time time max_reserved = torch cuda max_memory_reserved max_allocated = torch cuda max_memory_allocated print f rank rank ITER_TIME iterations f average latency end - start ITER_TIME f ms print f rank rank forward forward_time ITER_TIME f ms f backward backward_time ITER_TIME f ms print f rank rank max reserved max_reserved f GiB f max allocated max_allocated f GiB dist destroy_process_group train_convnext_example