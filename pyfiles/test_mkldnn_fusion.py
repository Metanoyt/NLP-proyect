Owner s module mkldnn itertools unittest typing NamedTuple torch torch nn torch testing _internal common_utils run_tests skipIfTorchDynamo torch testing _internal jit_utils JitTestCase test_tensorexpr warmup_and_run_forward FUSION_GROUP = prim TensorExprGroup PointwisePostOp NamedTuple attr str pointwise_module nn Module scalars list = algorithm str = CONV_MODULES = torch nn Conv d torch nn Conv d CONV_TRANSPOSE_MODULES = torch nn ConvTranspose d skipIfTorchDynamo too slow unittest skipIf torch backends mkldnn is_available MKL-DNN build disabled TestMkldnnFusion JitTestCase assertFused graph fused_patterns pat fused_patterns assertGraphContainsExactly graph pat _check_model m x trace=False old_fusion_inlining = torch _C _debug_get_fusion_group_inlining torch _C _debug_set_fusion_group_inlining False old_cpu_fuser_state = torch _C _jit_can_fuse_on_cpu torch _C _jit_override_can_fuse_on_cpu True old_te_must_use_llvm_cpu = torch _C _jit_get_te_must_use_llvm_cpu torch _C _jit_set_te_must_use_llvm_cpu False m eval torch no_grad trace script = torch jit trace m x script = torch jit script m script = torch jit freeze script torch no_grad y = warmup_and_run_forward script x y = script x y_ref = m x graph = script graph_for x assertEqual y y_ref torch _C _debug_set_fusion_group_inlining old_fusion_inlining torch _C _jit_override_can_fuse_on_cpu old_cpu_fuser_state torch _C _jit_set_te_must_use_llvm_cpu old_te_must_use_llvm_cpu graph test_single_conv M nn Module __init__ in_channels out_channels bias kwargs super __init__ conv = torch nn Conv d in_channels out_channels bias=bias kwargs forward x res = conv x res memory_format enabled torch contiguous_format False torch channels_last True trace True False input_size = batch_size = kernel_size = options = itertools product True False bias dilation groups options iC = groups oC = groups m = M iC oC bias kernel_size= kernel_size kernel_size stride= padding= dilation=dilation groups=groups memory_format=memory_format x = torch randn batch_size iC input_size input_size memory_format=memory_format graph = _check_model m x trace conv_node_name = aten _convolution trace aten conv d enabled assertFused graph conv_node_name assertGraphContainsExactly graph FUSION_GROUP assertGraphContains graph kind=conv_node_name test_conv_unary_fusion_nnc M nn Module __init__ unary_fn in_channels out_channels bias kwargs super __init__ conv = torch nn Conv d in_channels out_channels bias=bias kwargs unary = unary_fn forward x x = conv x x = unary x x memory_format enabled torch contiguous_format False torch channels_last True unary_fn torch relu bias True False oC m = M unary_fn oC bias kernel_size= memory_format=memory_format x = torch randn memory_format=memory_format graph = _check_model m x enabled assertFused graph aten conv d aten + unary_fn __name__ assertGraphContainsExactly graph FUSION_GROUP assertGraphContains graph kind= aten conv d test_unsupported_conv M nn Module __init__ m in_channels out_channels bias kwargs super __init__ conv = m in_channels out_channels bias=bias kwargs forward x res = conv x res module dim memory_format nn Conv d torch contiguous_format nn Conv d torch channels_last_ d nn ConvTranspose d torch contiguous_format nn ConvTranspose d torch channels_last trace = True input_size = batch_size = kernel_size = groups = bias = True iC = groups oC = groups dilation = m = M module iC oC bias kernel_size=kernel_size stride= padding= dilation=dilation groups=groups memory_format=memory_format input_sizes = batch_size iC input_size input_size dim == input_sizes append input_size x = torch randn input_sizes memory_format=memory_format graph = _check_model m x trace assertGraphContains graph kind= aten _convolution _unary_list unary_list = relu PointwisePostOp relu nn ReLU sigmoid PointwisePostOp sigmoid nn Sigmoid tanh PointwisePostOp tanh nn Tanh hardswish PointwisePostOp hardswish nn Hardswish leaky_relu PointwisePostOp leaky_relu nn LeakyReLU inplace=False scalars= hardtanh PointwisePostOp hardtanh nn Hardtanh min_val=- max_val= inplace=False scalars= - gelu_none PointwisePostOp gelu nn GELU approximate= none algorithm= none gelu_tanh PointwisePostOp gelu nn GELU approximate= tanh algorithm= tanh unary_list _binary_list binary_list = add torch add sub torch sub mul torch mul div torch div binary_list test_linear_unary_fusion_ops M nn Module __init__ unary_fn in_channels out_channels bias kwargs super __init__ linear = torch nn Linear in_channels out_channels bias=bias kwargs unary = unary_fn forward x x = linear x x = unary x x pointwise_info _unary_list values Tensor size = stride = contiguous tensor s strides default contiguous strides options = itertools product None None True False input_shape input_stride bias options torch no_grad mod = M pointwise_info pointwise_module input_shape - bias eval v = torch randn input_shape input_stride None v = v as_strided input_shape input_stride ref = mod v attr = pointwise_info attr scalars = pointwise_info scalars algorithm = pointwise_info algorithm fused = torch ops mkldnn _linear_pointwise v mod linear weight mod linear bias attr scalars algorithm assertEqual ref fused test_conv_unary_fusion_ops M nn Module __init__ unary_fn dim in_channels out_channels dilation groups bias kwargs super __init__ conv = CONV_MODULES dim in_channels out_channels dilation=dilation groups=groups bias=bias kwargs unary = unary_fn forward x x = conv x x = unary x x input_shapes = pointwise_info _unary_list values dim channels_last = torch channels_last dim == torch channels_last_ d options = itertools product True False torch contiguous_format channels_last bias dilation groups memory_format options oC = groups iC = groups x_shape = iC + input_shapes dim x = torch randn x_shape dtype=torch float memory_format=memory_format mod = M pointwise_info pointwise_module dim iC oC dilation groups bias kernel_size= mod = mod memory_format=memory_format eval torch no_grad ref = mod x attr = pointwise_info attr scalars = pointwise_info scalars algorithm = pointwise_info algorithm fused = torch ops mkldnn _convolution_pointwise x mod conv weight mod conv bias mod conv padding mod conv stride mod conv dilation mod conv groups attr scalars algorithm assertEqual ref fused test_conv_binary_fusion_ops M nn Module __init__ binary_fn dim in_channels out_channels dilation groups bias kwargs super __init__ conv = CONV_MODULES dim in_channels out_channels dilation=dilation groups=groups bias=bias kwargs binary = binary_fn forward x other x = conv x x = binary x other x input_shapes = pointwise_name pointwise_fn _binary_list items dim channels_last = torch channels_last dim == torch channels_last_ d options = itertools product False True True False torch contiguous_format channels_last fuse_relu bias dilation groups memory_format options oC = groups iC = groups x_shape = iC + input_shapes dim x = torch randn x_shape dtype=torch float memory_format=memory_format mod = M pointwise_fn dim iC oC dilation groups bias kernel_size= mod = mod memory_format=memory_format eval other = torch randn_like mod conv x torch no_grad ref = mod x other unary_attr = None fuse_relu ref relu_ unary_attr = relu attr = pointwise_name fused = torch ops mkldnn _convolution_pointwise x other mod conv weight mod conv bias mod conv padding mod conv stride mod conv dilation mod conv groups attr None unary_attr None binary add we support inplace version attr == add fused_inplace = torch ops mkldnn _convolution_pointwise_ other x mod conv weight mod conv bias mod conv padding mod conv stride mod conv dilation mod conv groups attr None unary_attr None assertEqual ref other assertEqual ref fused_inplace assertEqual ref fused atol= e- rtol= e- test_linear_binary_fusion_ops M nn Module __init__ binary_fn in_channels out_channels bias kwargs super __init__ linear = torch nn Linear in_channels out_channels bias=bias kwargs binary = binary_fn forward x other x = linear x x = binary x other x out_feature = pointwise_name pointwise_fn _binary_list items Tensor size = stride = contiguous tensor s strides default contiguous strides options = itertools product None None True False input_shape input_stride bias options torch no_grad mod = M pointwise_fn input_shape - out_feature bias eval v = torch randn input_shape input_stride None v = v as_strided input_shape input_stride other = torch randn input_shape - + out_feature ref = mod v other attr = pointwise_name fused = torch ops mkldnn _linear_pointwise v other mod linear weight mod linear bias attr assertEqual ref fused test_conv_transpose_unary_fusion_ops M nn Module __init__ unary_fn dim in_channels out_channels kernel_size kwargs super __init__ conv_transpose = CONV_TRANSPOSE_MODULES dim in_channels out_channels kernel_size kwargs unary = unary_fn forward x x = conv_transpose x x = unary x x input_shapes = kernel_size = pointwise_info _unary_list values dim channels_last = torch channels_last dim == torch channels_last_ d options = itertools product True False torch contiguous_format channels_last False True bias dilation groups memory_format prepack_weight options oC = groups iC = groups x_shape = iC + input_shapes dim x = torch randn x_shape dtype=torch float memory_format=memory_format mod = M pointwise_info pointwise_module dim iC oC kernel_size dilation=dilation groups=groups bias=bias mod = mod memory_format=memory_format eval torch no_grad ref = mod x attr = pointwise_info attr scalars = pointwise_info scalars algorithm = pointwise_info algorithm prepack_weight packed_weight = torch ops mkldnn _reorder_convolution_transpose_weight mod conv_transpose weight mod conv_transpose padding mod conv_transpose output_padding mod conv_transpose stride mod conv_transpose dilation mod conv_transpose groups x size mod conv_transpose weight = torch nn Parameter packed_weight requires_grad=mod conv_transpose weight requires_grad fused = torch ops mkldnn _convolution_transpose_pointwise x mod conv_transpose weight mod conv_transpose bias mod conv_transpose padding mod conv_transpose output_padding mod conv_transpose stride mod conv_transpose dilation mod conv_transpose groups attr scalars algorithm assertEqual ref fused __name__ == __main__ run_tests