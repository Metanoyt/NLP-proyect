mypy allow-untyped-defs functools logging math os pathlib typing Any Optional Union torch _inductor ir MultiTemplateBuffer torch _inductor metrics get_metric_table is_metric_table_enabled torch utils _ordered_set OrderedSet config codecache code_hash CodeCacheFuture get_path write_atomic runtime benchmarking benchmarker utils cache_on_self IndentedBuffer virtualized V common TensorArg WorkspaceArg log = logging getLogger __name__ MultiKernelState Maintain state multi-kernel compilation so we don t define duplicated multi-kernel same set sub-kernels V graph wrapper_code has reference MultiKernelState instance __init__ subkernel_to_kernel_name = kernel_defs = IndentedBuffer define_kernel kernels list Any kernel_shape_keys Optional list Union None tuple tuple int = None - str Previously we name multi kernel multi_kernel_ kernel_names This has some minor issue E g persistent reduction https gist github com shunting e c ff bb ed d ca there flavors non-persistent reduction https gist github com shunting d d e efb b c d https gist github com shunting ee b c c e afe bd The only different cache eviction policy We should name multi-kernel differently these cases kernels A list kernels kernel_shape_keys Specified size-hint multi-kernels Each list element shape key corresponding concrete input output size hints each kernel tuned Prevent circular select_algorithm TritonTemplateKernel kernel_names = tuple k kernel_name k kernels kernel_names subkernel_to_kernel_name subkernel_to_kernel_name kernel_names name multi kernel based first kernel multi_kernel_name = f multi_kernel_ len subkernel_to_kernel_name subkernel_to_kernel_name kernel_names = multi_kernel_name V graph cpp_wrapper config triton autotune_at_compile_time we should generate any python code multi-kernel during second pass cpp-wrapper multi_kernel_name arg_index dict int list slice = _ call_args _ arg_types = kernels args python_argdefs isinstance kernels TritonTemplateKernel isinstance kernels output_node MultiTemplateBuffer i kernel enumerate kernels additional_call_args _ = kernel additional_call_args_and_types i arg_index arg_index i = arg_index i append slice len call_args arg_index i append slice len call_args + i len additional_call_args len call_args + i + len additional_call_args kernels add_numel_to_call_args multi_kernel_name call_args arg_types i range len kernels arg_index i = slice len call_args keyed_by_sizes = kernel_shape_keys None buf = kernel_defs buf writeline buf writeline arg_index = key slice_list arg_index items slice_reprs = join repr s s slice_list buf writeline f key slice_reprs buf writeline keyed_by_sizes no size hint keys just call list kernels buf writeline f multi_kernel_name = async_compile multi_kernel multi_kernel_name r buf indent name kernel_names buf writeline f name buf writeline arg_index=arg_index call dict size hint key kernel assert isinstance kernels TritonTemplateKernel assert isinstance kernel_shape_keys list assert len kernels == len kernel_shape_keys buf writeline f multi_kernel_name = async_compile size_hint_multi_kernel multi_kernel_name r buf indent shape_key name zip kernel_shape_keys kernel_names buf writeline f shape_key name buf writeline arg_index=arg_index config triton autotune_at_compile_time V graph wrapper_code src_to_kernel \n join kernel_names = multi_kernel_name multi_kernel_name MultiKernel This maintains compile time state multi kernels Assume we do codegen MultiKernel encapsulating kernel kernel The generated definition multi-kernel will looks like ` ` ` multi_kernel_kernel = MultiKernelCall kernel kernel multi_kernel_definition_code ` ` ` Here concrete example https gist github com shunting d f fb bc cee dbae ca d __init__ kernels assert len kernels = kernels = kernels kernel_name = V graph wrapper_code multi_kernel_state define_kernel kernels need since some code inductor check kernel object has args attribute decide s non-null kernel args = object staticmethod _merge_workspace_args left list WorkspaceArg right list WorkspaceArg left == right left result = x inner_name x x left arg right arg inner_name result result arg inner_name = WorkspaceArg maximum result arg inner_name arg result arg inner_name = arg result values staticmethod merge_workspaces_inplace kernels len kernels All kernels must share same workspace workspace_args = functools reduce MultiKernel _merge_workspace_args kernel args workspace_args kernel kernels kernel kernels kernel args workspace_args = workspace_args workspace_args call_kernel kernel_name Collect union arguments all subkernels arguments multi-kernel Prevent circular select_algorithm TritonTemplateKernel assert kernel_name == kernel_name V graph wrapper_code write_triton_header_once _ call_args _ arg_types = kernels args python_argdefs kernel kernels _ other_call_args _ other_arg_types = kernel args python_argdefs assert call_args == other_call_args call_args other_call_args assert arg_types == other_arg_types V graph cpp_wrapper config triton autotune_at_compile_time second pass cpp-wrapper codegen we should call fast kernel directly kernel_name = MultiKernelCall lookup_choice kernel_name isinstance kernels TritonTemplateKernel isinstance kernels output_node MultiTemplateBuffer For matmuls grid arguments passed additional arguments kernel run method These grids change based various parameters matmul So we need pass each kernel s grid into multi call kernel multi_call_args = call_args multi_call_arg_types = arg_types kernel kernels additional_call_args additional_arg_types = kernel additional_call_args_and_types multi_call_args extend list additional_call_args multi_call_arg_types extend list additional_arg_types numels all subkernels should same Use kernels here kernels add_numel_to_call_args kernel_name call_args arg_types multi_call_args = call_args multi_call_arg_types = arg_types ws kernels args workspace_args V graph wrapper_code generate_workspace_allocation ws V graph cpp_wrapper We have already selected best kernel compile time so we only have one set call args NB currently doesn t work MultiTemplateBuffer kernels bobrenjc will add subsequent PR V graph wrapper_code generate_kernel_call kernel_name call_args arg_types=arg_types V graph wrapper_code generate_kernel_call kernel_name multi_call_args arg_types=multi_call_arg_types ws reversed kernels args workspace_args V graph wrapper_code generate_workspace_deallocation ws codegen_nan_check wrapper = V graph wrapper_code seen OrderedSet str = OrderedSet k kernels _ call_args precompile_args _ = k args python_argdefs arg precompile_arg zip call_args precompile_args arg seen continue seen add arg isinstance precompile_arg TensorArg line = f assert arg isnan any item wrapper writeline line line = f assert arg isinf any item wrapper writeline line property removed_buffers OrderedSet intersection k removed_buffers k kernels property inplaced_to_remove OrderedSet intersection k inplaced_to_remove k kernels property cache_on_self inplace_update_buffers Make sure all kernels have same inplace update mappings k kernels assert k inplace_update_buffers == kernels inplace_update_buffers kernels inplace_update_buffers warn_mix_layout kernel_name str pass MultiKernelCall This called run time actually run kernel __init__ multi_kernel_name kernels arg_index assert len kernels = _kernels = kernels multi_kernel_name = multi_kernel_name disable_cache = os environ get TORCHINDUCTOR_DISABLE_MULTI_KERNEL_CACHE == is_metric_table_enabled persistent_red_perf picked_kernel = None arg_index = arg_index config triton multi_kernel manually force subkernel ease perf testing picked_by_config = config triton multi_kernel - assert picked_by_config len _kernels pyrefly ignore bad-assignment picked_kernel = picked_by_config disable_cache load_cache _recorded = False cache_file_path key = code_hash join f k fn cache_key k size_hints r k triton_meta r k kernels _ _ path = get_path key picked_kernel pathlib Path path load_cache assert picked_kernel None path = cache_file_path path exists path open fd pyrefly ignore bad-assignment picked_kernel = int fd read pyrefly ignore unsupported-operation assert picked_kernel = picked_kernel len _kernels log debug Load picked kernel d cache file s picked_kernel path store_cache assert picked_kernel None path = cache_file_path path parent mkdir parents=True exist_ok=True write_atomic path str picked_kernel log debug Store picked kernel d cache file s picked_kernel path property kernels Read results future This should called after parallel compilation done In case you call before compilation done may slow down parallel compilation i kernel enumerate _kernels isinstance kernel CodeCacheFuture _kernels i = kernel result _kernels benchmark_sub_kernels args kwargs Benchmark all sub kernels execution time milliseconds each time Unit test may mock method force specific kernel picked wrap_fn kernel index inner filtered_args = _get_filtered_args args index args_clone kwargs_clone = kernel clone_args filtered_args kwargs kernel run args_clone kwargs_clone inner benchmarker benchmark_gpu wrap_fn kernel index rep= index kernel enumerate kernels _get_filtered_args args index We pass all arguments all kernels into MultiKernelCall so when invoking particular kernel we need filter only arguments specific kernel This sometimes invoked runtime where V graph NullHandler hasattr V graph cpp_wrapper V graph cpp_wrapper cpp-wrapper we should filter args since we already have chosen single kernel arg set args item s arg_index index item args s record_choice lookup_choice helper functions cpp-wrapper codegen The first pass use record_choice keep choice second pass do lookup calling lookup_choice An alternative reused multi-kernel cache does work well since during codegen second pass s very hard know path cache file Also reading cache file need do some IO which can slower staticmethod record_choice multi_kernel_name str picked_kernel_name str Record multi-kernel choice cpp-wrapper after autotuning We should do nothing function called during codegen torch _inductor graph GraphLowering isinstance V graph GraphLowering V graph record_multi_kernel_choice V graph multi_kernel_to_choice multi_kernel_name = picked_kernel_name staticmethod lookup_choice multi_kernel_name str - str should always been done during cpp-wrapper codegen assert V graph record_multi_kernel_choice multi_kernel_name V graph multi_kernel_to_choice there should no miss V graph multi_kernel_to_choice multi_kernel_name run args kwargs picked_kernel None timings = benchmark_sub_kernels args kwargs picked_kernel = timings index min timings k = kernels log debug pick dth sub-kernel s Size hints s Reduction hint s Timings s picked_kernel k inductor_meta get kernel_name k kernels k size_hints k inductor_meta get reduction_hint timings get_metric_table persistent_red_perf add_row functools partial _metrics_table_row timings disable_cache store_cache _recorded _recorded = True picked_kernel_name = kernels picked_kernel inductor_meta get kernel_name assert picked_kernel_name None record_choice multi_kernel_name picked_kernel_name run = kernels picked_kernel run type ignore method-assign filtered_args = _get_filtered_args args picked_kernel run filtered_args kwargs _metrics_table_row timings get_kernel_path k k fn fn __code__ co_filename k = kernels row = size_hints k size_hints reduction_hint k inductor_meta get reduction_hint max_kernels = assert len timings = max_kernels i range max_kernels i len kernels row f kernel i _path = get_kernel_path kernels i row f kernel i _latency = timings i row f kernel i _path = row f kernel i _latency = row SizeHintMultiKernel MultiKernel Version multi-kernel generates kernels based specified size hints Currently only performs -d search over hints doesn t perform combinatorial n-d search n dynamic dimensions specified e g matmul s s s s size-hints only generates kernels based tuning shapes __init__ kernels assert isinstance kernels dict len kernels = kernels kernel_shape_keys = shape_key kernel kernels items kernels append kernel kernel_shape_keys append shape_key kernel_name = V graph wrapper_code multi_kernel_state define_kernel kernels kernel_shape_keys need since some code inductor check kernel object has args attribute decide s non-null kernel args = object SizeHintMultiKernelCall MultiKernelCall Runtime size-hint multi-kernels Instead having plain list kernels benchmark over keys them input output shapes optionally perform shape-based selection The pre-generated kernel chosen based shape keys heuristic being log l distance between pre-generated runtime input output shapes __init__ multi_kernel_name kernels arg_index super __init__ multi_kernel_name list kernels values arg_index _kernel_hints = list kernels keys Caches results unique shapes _shape_cache = _get_shape_cache_key args kwargs Generate cache key based tensor shapes shape-specialized dispatch shapes = arg args hasattr arg shape shapes append tuple arg shape tuple shapes _get_cached_shape_choice cache_key Get cached kernel choice specific shape _shape_cache get cache_key _cache_shape_choice cache_key kernel_idx Cache kernel choice specific shape _shape_cache cache_key = kernel_idx _dist_heuristic k k log L distance heuristic kernel selection dist x y lx = math log x x - ly = math log y y - abs lx - ly out = s s zip k k out += sum dist x y x y zip s s out run args kwargs cache_key = _get_shape_cache_key args kwargs cached_choice = _get_cached_shape_choice cache_key cached_choice None picked_kernel = cached_choice log debug using cached shape-specialized choice dth sub-kernel s Cache key s picked_kernel k inductor_meta get kernel_name k kernels cache_key _select_kernel_by_shape args kwargs _recorded _recorded = True picked_kernel_name = kernels picked_kernel inductor_meta get kernel_name assert picked_kernel_name None record_choice multi_kernel_name picked_kernel_name run = kernels picked_kernel run type ignore method-assign filtered_args = _get_filtered_args args picked_kernel run filtered_args kwargs _select_kernel_by_shape args kwargs Benchmark kernels particular shape best kernel shape shape_key = _get_shape_cache_key args kwargs dists = _dist_heuristic shape_key key key None key _kernel_hints pyrefly ignore bad-assignment picked_kernel = dists index min dists _cache_shape_choice shape_key picked_kernel