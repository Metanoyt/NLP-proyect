Owner s module inductor sys unittest unittest mock mock torch torch _inductor torch _higher_order_ops foreach_map torch _inductor test_case TestCase torch _inductor utils run_fw_bw_and_get_code torch testing _internal common_utils instantiate_parametrized_tests IS_FBCODE parametrize torch testing _internal inductor_utils GPU_TYPE HAS_CPU HAS_GPU torch testing _internal triton_utils requires_gpu torch utils _pytree tree_flatten aten = torch ops aten try try test_torchinductor check_model check_model_gpu except ImportError test_torchinductor manual=fbcode caffe test inductor test_inductor-library check_model check_model_gpu except unittest SkipTest ImportError e sys stderr write f type e e \n __name__ == __main__ sys exit raise foreach_map_wrapper op wrapper args kwargs foreach_map op args kwargs wrapper __name__ = foreach_map_ + op __name__ wrapper original_op = op wrapper add_op x y torch add x y add_inplace_op x y x add_ y x sin addrecip_op x y torch reciprocal torch add x y addcmul_op x y z torch mul torch add x y z recipaddmul_op x y z torch mul torch add torch reciprocal x y z Foreach map bin op defs which support scalar arg foreach_map_add = foreach_map_wrapper torch add foreach_map_mul = foreach_map_wrapper torch mul foreach_map_sub = foreach_map_wrapper torch sub foreach_map_div = foreach_map_wrapper torch div foreach_map_addrecip = foreach_map_wrapper addrecip_op foreach_map_clamp_max = foreach_map_wrapper torch clamp_max foreach_map_clamp_min = foreach_map_wrapper torch clamp_min No scalar args due limitations op itself foreach_map_max = foreach_map_wrapper torch maximum foreach_map_min = foreach_map_wrapper torch minimum foreach_map_copy = foreach_map_wrapper aten copy More general functions foreach_map_add_fn = foreach_map_wrapper add_op foreach_map_add_inplace = foreach_map_wrapper add_inplace_op foreach_map_recipaddmul = foreach_map_wrapper addrecip_op foreach_map_addcmul = foreach_map_wrapper addcmul_op foreach_map_recipaddmul = foreach_map_wrapper recipaddmul_op Foreach map unary op defs foreach_map_recip = foreach_map_wrapper torch reciprocal foreach_map_neg = foreach_map_wrapper torch neg foreach_map_sign = foreach_map_wrapper torch sign foreach_map_abs = foreach_map_wrapper torch abs inplace_bin_ops_under_test = torch _foreach_add_ torch _foreach_mul_ torch _foreach_sub_ torch _foreach_div_ ternary_ops_under_test = foreach_map_addcmul foreach_map_recipaddmul foreach_map_bin_ops_under_test = foreach_map_add foreach_map_mul foreach_map_sub foreach_map_div foreach_map_addrecip foreach_map_clamp_max foreach_map_clamp_min foreach_map_add_fn foreach_map_max foreach_map_min foreach_map_un_ops_under_test = foreach_map_recip foreach_map_neg foreach_map_sign foreach_map_abs bin_ops_under_test = torch _foreach_add torch _foreach_mul torch _foreach_sub torch _foreach_div torch _foreach_maximum torch _foreach_minimum torch _foreach_clamp_max torch _foreach_clamp_min aten _foreach_copy foreach_map_copy aten copy doesn t support backward foreach_map_bin_ops_under_test scalar_bin_ops_under_test = op op bin_ops_under_test op foreach_map_max foreach_map_min foreach_map_copy aten _foreach_copy un_ops_under_test = torch _foreach_reciprocal torch _foreach_neg torch _foreach_sign torch _foreach_abs torch _foreach_sqrt torch _foreach_rsqrt foreach_map_un_ops_under_test compose_ops = torch _foreach_addcdiv torch _foreach_addcmul all_ops = parametrize op ternary_ops_under_test + bin_ops_under_test + un_ops_under_test name_fn=lambda f f __name__ bin_ops = parametrize op bin_ops_under_test name_fn=lambda f f __name__ inplace_bin_ops = parametrize op inplace_bin_ops_under_test name_fn=lambda f f __name__ scalar_bin_ops = parametrize op scalar_bin_ops_under_test name_fn=lambda f f __name__ scalar_tensor_bin_ops = parametrize op scalar_bin_ops_under_test name_fn=lambda f f __name__ foreach_map_bin_ops = parametrize op foreach_map_bin_ops_under_test name_fn=lambda f f __name__ foreach_map_un_ops = parametrize op foreach_map_un_ops_under_test name_fn=lambda f f __name__ decomp_ops = parametrize op compose_ops name_fn=lambda f f __name__ gen_args op op un_ops_under_test torch rand device=GPU_TYPE torch rand device=GPU_TYPE op bin_ops_under_test torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE instantiate_parametrized_tests ForeachTests TestCase check_model_gpu = check_model_gpu check_model_cpu = check_model check_kernel_count = True setUp super setUp torch _inductor metrics reset tearDown super tearDown torch _inductor metrics reset _test_single_list op op un_ops_under_test fn op op bin_ops_under_test fn b b op b b fn b b c c op b b c c check_model_gpu fn gen_args op _test_single_scalar op fn op check_model_gpu fn torch rand device=GPU_TYPE torch rand device=GPU_TYPE _test_single_scalar_tensor op fn op torch tensor device=GPU_TYPE check_model_gpu fn torch rand device=GPU_TYPE torch rand device=GPU_TYPE called test_gpu_cpp_wrapper py requires_gpu test_foreach_cpp_wrapper_cuda _test_single_list op=torch _foreach_add called test_gpu_cpp_wrapper py test_foreach_cpp_wrapper_xpu = test_foreach_cpp_wrapper_cuda requires_gpu all_ops test_single_list op _test_single_list op assertEqual torch _inductor metrics generated_kernel_count requires_gpu scalar_bin_ops test_single_scalar op _test_single_scalar op assertEqual torch _inductor metrics generated_kernel_count requires_gpu scalar_tensor_bin_ops test_single_scalar_tensor op _test_single_scalar_tensor op assertEqual torch _inductor metrics generated_kernel_count requires_gpu all_ops test_scheduler_fusion_list op op un_ops_under_test fn c = op torch _foreach_sqrt c op bin_ops_under_test fn b b c = op b b c torch _foreach_add c fn b b c c c = op b b c c c torch _foreach_add c check_model_gpu fn gen_args op assertEqual torch _inductor metrics generated_kernel_count requires_gpu scalar_bin_ops test_scheduler_fusion_scalar op fn c = op c torch _foreach_add c check_model_gpu fn torch rand device=GPU_TYPE torch rand device=GPU_TYPE assertEqual torch _inductor metrics generated_kernel_count requires_gpu scalar_bin_ops test_broadcasting op fn b b op b b fn_opt = torch compile fn inputs = torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE actual = fn_opt inputs expected = fn inputs assertEqual actual expected assertEqual torch _inductor metrics generated_kernel_count requires_gpu all_ops test_singleton_lists op op un_ops_under_test fn op args = torch rand device=GPU_TYPE op bin_ops_under_test fn b op b args = torch rand device=GPU_TYPE torch rand device=GPU_TYPE fn b c op b c args = torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE check_model_gpu fn args assertEqual torch _inductor metrics generated_kernel_count requires_gpu bin_ops test_type_promotion op fn b b op b b fn_opt = torch compile fn max = torch iinfo torch int max max = torch iinfo torch int max inputs = torch randint max device=GPU_TYPE dtype=torch int torch randint max device=GPU_TYPE dtype=torch int torch randint max device=GPU_TYPE dtype=torch int torch randint max device=GPU_TYPE dtype=torch int actual = fn_opt inputs expected = fn inputs assertEqual actual expected assertEqual torch _inductor metrics generated_kernel_count requires_gpu scalar_bin_ops test_kernel_split_arg_limit_list op NB foeach_copy won t pass test because will dce one set buffers fn b op b fn_opt = torch compile fn max_args = max_list_len = max_args + inputs = torch rand device=GPU_TYPE _ range max_list_len torch rand device=GPU_TYPE _ range max_list_len actual = fn_opt inputs expected = fn inputs assertEqual actual expected assertEqual torch _inductor metrics generated_kernel_count requires_gpu scalar_bin_ops unittest skip Triton recursion depth exceeded https github com triton-lang triton issues test_kernel_split_arg_limit_scalar op fn op fn_opt = torch compile fn max_args = max_list_len = max_args + inputs = torch rand device=GPU_TYPE _ range max_list_len actual = fn_opt inputs expected = fn inputs assertEqual actual expected assertEqual torch _inductor metrics generated_kernel_count requires_gpu bin_ops test_fusion_duplicate_buffer_list op fn b b c = op b b op b c c check_model_gpu fn torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE reference_in_float=False check_lowp=False kernel_count = foreach_map op __name__ kernel_count = assertEqual torch _inductor metrics generated_kernel_count kernel_count requires_gpu all_ops test_non_foreach_consumer_list op op un_ops_under_test fn c = op torch mul c op bin_ops_under_test fn b b c = op b b torch mul c fn b b c c c = op b b c c torch mul c check_model_gpu fn gen_args op assertEqual torch _inductor metrics generated_kernel_count requires_gpu scalar_bin_ops test_non_foreach_consumer_scalar op fn c = op torch mul c check_model_gpu fn torch rand device=GPU_TYPE torch rand device=GPU_TYPE assertEqual torch _inductor metrics generated_kernel_count requires_gpu all_ops test_non_foreach_producer_list op op un_ops_under_test fn c = torch add c = torch add op c c op bin_ops_under_test fn b b c = torch add b c = torch add b op c c fn b b c c c = torch add b c = torch add b op b b c c check_model_gpu fn gen_args op reference_in_float=False check_lowp=False assertEqual torch _inductor metrics generated_kernel_count requires_gpu scalar_bin_ops test_non_foreach_producer_scalar op fn b b c = torch mul b c = torch mul b op c c check_model_gpu fn torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE assertEqual torch _inductor metrics generated_kernel_count requires_gpu all_ops test_non_foreach_consumer_producer_list op op un_ops_under_test fn c = torch add c = torch mul d = op c c e = torch mul d e = torch mul d e e op bin_ops_under_test fn b b c = torch add b c = torch add b d = op c c e = torch mul d e = torch mul d e e fn b b c c c = torch add b c = torch add b d = op b b c c e = torch mul d e = torch mul d e e check_model_gpu fn gen_args op reference_in_float=False check_lowp=False assertEqual torch _inductor metrics generated_kernel_count requires_gpu scalar_bin_ops test_non_foreach_consumer_producer_scalar op fn b b c = torch add b c = torch add b d = op c c e = torch mul d e = torch mul d e e check_model_gpu fn torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE reference_in_float=False check_lowp=False assertEqual torch _inductor metrics generated_kernel_count requires_gpu bin_ops torch _dynamo config patch automatic_dynamic_shapes False torch _dynamo config patch assume_static_by_default False torch _inductor config patch combo_kernel_foreach_dynamic_shapes False test_dynamic_shapes_fallback op fn b b op b b inputs = torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE check_model_gpu fn inputs assertEqual torch _inductor metrics generated_kernel_count requires_gpu torch _dynamo config patch automatic_dynamic_shapes False torch _dynamo config patch assume_static_by_default False torch _inductor config patch combo_kernel_foreach_dynamic_shapes True test_enable_dynamic_shapes_python_wrapper op=torch _foreach_add fn b b op b b inputs = torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE check_model_gpu fn inputs assertEqual torch _inductor metrics generated_kernel_count requires_gpu torch _dynamo config patch automatic_dynamic_shapes False torch _dynamo config patch assume_static_by_default False torch _inductor config patch combo_kernel_foreach_dynamic_shapes True torch _inductor config patch cpp_wrapper True test_enable_dynamic_shapes_cpp_wrapper_cuda op=torch _foreach_add fn b b op b b inputs = torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE check_model_gpu fn inputs called test_gpu_cpp_wrapper py test_enable_dynamic_shapes_cpp_wrapper_xpu = test_enable_dynamic_shapes_cpp_wrapper_cuda unittest skipIf IS_FBCODE cpp compile supported fbcode bin_ops test_cpu_cpp_fallback op fn b b op b b inputs = torch rand device= cpu torch rand device= cpu torch rand device= cpu torch rand device= cpu check_model_cpu fn inputs assertEqual torch _inductor metrics generated_kernel_count requires_gpu decomp_ops test_decomp op fn b b c c op b b c c value= check_model_gpu fn torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE assertEqual torch _inductor metrics generated_kernel_count requires_gpu test_fuse_concat fn x x x w w w x = torch stack x x x w = torch stack w w w y = torch bmm x w y x = torch randn GPU_TYPE x = x + x = x + w = torch randn GPU_TYPE w = w + w = w + args = x x x w w w check_model_gpu fn args assertEqual torch _inductor metrics generated_kernel_count requires_gpu test_zero_elems fn b b torch _foreach_add b b check_model_gpu fn torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE assertEqual torch _inductor metrics generated_kernel_count requires_gpu bin_ops test_ d_blocking op fn b b op b b check_model_gpu fn torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE t torch rand device=GPU_TYPE t assertEqual torch _inductor metrics generated_kernel_count requires_gpu bin_ops test_ d_blocking_partitioning op fn b b op b b check_model_gpu fn torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE t assertEqual torch _inductor metrics generated_kernel_count requires_gpu bin_ops test_ d_blocking_partitioning_elems op D blocking should grouped number yelems fn b b b op b b b check_model_gpu fn torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE t torch rand device=GPU_TYPE t torch rand device=GPU_TYPE t assertEqual torch _inductor metrics generated_kernel_count requires_gpu bin_ops torch _inductor config patch combo_kernel_allow_mixed_sizes test_ d_blocking_partitioning_mixed_sizes op D blocking mixed sizes should group together fn b b b op b b b check_model_gpu fn torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE t torch rand device=GPU_TYPE t torch rand device=GPU_TYPE t assertEqual torch _inductor metrics generated_kernel_count requires_gpu inplace_bin_ops test_reinplacing op fn b b op b b inputs = torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE check_model_gpu fn inputs check_lowp=False assertEqual torch _inductor metrics generated_kernel_count requires_gpu inplace_bin_ops test_reinplacing_mut_before op fn b b add_ torch ones device=GPU_TYPE op b b inputs = torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE check_model_gpu fn inputs check_lowp=False assertEqual torch _inductor metrics generated_kernel_count requires_gpu inplace_bin_ops test_reinplacing_mut_after op fn b b op b b add_ torch ones device=GPU_TYPE inputs = torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE check_model_gpu fn inputs check_lowp=False assertEqual torch _inductor metrics generated_kernel_count requires_gpu test_multi_device test_foreach_add b b torch _foreach_add b b inps = torch ones device=GPU_TYPE torch ones device= cpu torch zeros device=GPU_TYPE torch zeros device= cpu out_eager = test_foreach_add inps out_compiled = torch compile test_foreach_add inps assertEqual out_eager out_compiled assertEqual torch _inductor metrics generated_kernel_count requires_gpu test_aliasing test_foreach_add b b b torch _foreach_add_ b b b input = torch ones device=GPU_TYPE input = torch ones device=GPU_TYPE inps = input input view input view input input view input view out_eager = test_foreach_add inps out_compiled = torch compile test_foreach_add inps assertEqual out_eager out_compiled assertEqual torch _inductor metrics generated_kernel_count requires_gpu torch _inductor config patch combo_kernel_allow_mixed_sizes test_ d_block_no_mixed_sizes_no_mask D blocking no mixed sizes constant mask fn b b b torch _foreach_add b b b check_model_gpu fn torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE t torch rand device=GPU_TYPE t torch rand device=GPU_TYPE t assertEqual torch _inductor metrics generated_kernel_count requires_gpu torch _inductor config patch combo_kernel_allow_mixed_sizes test_ d_block_mixed_sizes_with_mask D blocking mixed sizes should have mask fn b b b torch _foreach_add b b b check_model_gpu fn torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand device=GPU_TYPE t torch rand device=GPU_TYPE t torch rand device=GPU_TYPE t assertEqual torch _inductor metrics generated_kernel_count requires_gpu foreach_map_bin_ops test_foreach_map_backward_binary op torch _dynamo polyfills foreach_map_fn fn xs ys outs = op xs ys outs sum + outs sum + outs sum ref_fn xs ys outs = foreach_map_fn torch add xs ys outs sum + outs sum + outs sum ref_inps = torch rand device=GPU_TYPE requires_grad=True torch rand device=GPU_TYPE requires_grad=True torch rand device=GPU_TYPE requires_grad=True torch rand device=GPU_TYPE requires_grad=True torch rand device=GPU_TYPE requires_grad=True torch rand device=GPU_TYPE requires_grad=True inps = x clone detach requires_grad_ True x ref_inps y clone detach requires_grad_ True y ref_inps out_ref = ref_fn ref_inps out_ref backward unpacking result fw_code bw_code _ _ _ = run_fw_bw_and_get_code lambda torch compile fn inps ref act zip tree_flatten ref_inps tree_flatten inps torch allclose ref grad act grad assertEqual torch _inductor metrics generated_kernel_count requires_gpu test_foreach_map_input_mutation fn xs ys outs = foreach_map_add_inplace xs ys outs sum + outs sum + outs sum ref_inps = torch rand device=GPU_TYPE requires_grad=True torch rand device=GPU_TYPE requires_grad=True torch rand device=GPU_TYPE requires_grad=True torch rand device=GPU_TYPE requires_grad=True torch rand device=GPU_TYPE requires_grad=True torch rand device=GPU_TYPE requires_grad=True Set requires_grad False avoid mutating leaf variable inps = x clone detach requires_grad_ False x ref_inps y clone detach requires_grad_ False y ref_inps TODO after decomposing auto_functionalized we re getting functional subgraph inlined epilogue assertRaisesRegex torch _inductor exc InductorError Buffer mutation detected during lowering aten copy_ default mock patch torch _dynamo variables higher_order_ops BaseHOPVariable supports_input_mutation True _ = run_fw_bw_and_get_code lambda torch compile fn inps requires_gpu foreach_map_un_ops test_foreach_map_backward_unary op torch _dynamo polyfills foreach_map_fn fn xs outs = op xs outs sum + outs sum + outs sum ref_fn xs outs = foreach_map_fn op original_op xs outs sum + outs sum + outs sum ref_inp = torch rand device=GPU_TYPE requires_grad=True torch rand device=GPU_TYPE requires_grad=True torch rand device=GPU_TYPE requires_grad=True inp = x clone detach requires_grad_ True x ref_inp out_ref = ref_fn ref_inp out_ref backward unpacking result fw_code bw_code _ _ _ = run_fw_bw_and_get_code lambda torch compile fn inp ref act zip ref_inp inp torch allclose ref grad act grad assertEqual torch _inductor metrics generated_kernel_count __name__ == __main__ torch _inductor test_case run_tests HAS_CPU HAS_GPU run_tests needs= filelock