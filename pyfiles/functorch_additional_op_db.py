itertools unittest functools partial torch torch testing _internal common_dtype all_types_and_complex_and floating_types floating_types_and torch testing _internal common_methods_invocations DecorateInfo OpInfo SampleInput torch testing _internal common_utils make_tensor List OpInfos aren t PyTorch Core yet They here because we wanted fast way writing OpInfos may correct w r t dtypes other options TODO Figure out how upstream these delete them when they re upstreamed additional_op_db = https github com pytorch pytorch pull sample_inputs_conv d has_bias device dtype requires_grad extra_args= groups= in_ch out_ch = inp = make_tensor in_ch groups device=device dtype=dtype requires_grad=requires_grad low=- high= weight = make_tensor out_ch groups in_ch device=device dtype=dtype requires_grad=requires_grad low=- high= bias = None has_bias bias = make_tensor out_ch groups device=device dtype=dtype requires_grad=requires_grad low=- high= SampleInput inp args= weight bias + extra_args additional_op_db extend OpInfo nn functional conv d aten_name= conv d variant_test_name= no_bias supports_autograd=True supports_forward_ad=True sample_inputs_func=partial sample_inputs_conv d False dtypes=floating_types dtypesIfCUDA=floating_types_and torch half torch bfloat supports_out=False OpInfo nn functional conv d aten_name= conv d variant_test_name= with_bias supports_autograd=True supports_forward_ad=True sample_inputs_func=partial sample_inputs_conv d True dtypesIfCUDA=floating_types_and torch half torch bfloat dtypes=floating_types supports_out=False OpInfo nn functional conv d aten_name= conv d variant_test_name= stride_with_bias supports_autograd=True supports_forward_ad=True sample_inputs_func=partial sample_inputs_conv d True extra_args= dtypesIfCUDA=floating_types_and torch half torch bfloat dtypes=floating_types supports_out=False OpInfo nn functional conv d aten_name= conv d variant_test_name= stride_no_bias supports_autograd=True supports_forward_ad=True sample_inputs_func=partial sample_inputs_conv d False extra_args= dtypesIfCUDA=floating_types_and torch half torch bfloat dtypes=floating_types supports_out=False OpInfo nn functional conv d aten_name= conv d variant_test_name= stride_padding_with_bias supports_autograd=True supports_forward_ad=True sample_inputs_func=partial sample_inputs_conv d True extra_args= dtypesIfCUDA=floating_types_and torch half torch bfloat dtypes=floating_types supports_out=False OpInfo nn functional conv d aten_name= conv d variant_test_name= stride_padding_no_bias supports_autograd=True supports_forward_ad=True sample_inputs_func=partial sample_inputs_conv d False extra_args= dtypesIfCUDA=floating_types_and torch half torch bfloat dtypes=floating_types supports_out=False OpInfo nn functional conv d aten_name= conv d variant_test_name= strided_padding_dilation_with_bias supports_autograd=True supports_forward_ad=True sample_inputs_func=partial sample_inputs_conv d True extra_args= dtypesIfCUDA=floating_types_and torch half torch bfloat dtypes=floating_types supports_out=False OpInfo nn functional conv d aten_name= conv d variant_test_name= strided_padding_dilation_no_bias supports_autograd=True supports_forward_ad=True sample_inputs_func=partial sample_inputs_conv d True extra_args= dtypesIfCUDA=floating_types_and torch half torch bfloat dtypes=floating_types supports_out=False OpInfo nn functional conv d aten_name= conv d variant_test_name= stride_groups_with_bias supports_autograd=True supports_forward_ad=True sample_inputs_func=partial sample_inputs_conv d True extra_args= groups= dtypesIfCUDA=floating_types_and torch half torch bfloat dtypes=floating_types supports_out=False OpInfo nn functional conv d aten_name= conv d variant_test_name= stride_depthwise_with_bias supports_autograd=True supports_forward_ad=True sample_inputs_func=partial sample_inputs_conv d True extra_args= groups= dtypesIfCUDA=floating_types_and torch half torch bfloat dtypes=floating_types supports_out=False TODO PyTorch core has check requires_grad=True We actually want test more things backward here which why we have our own sample_inputs_embedding op_info device dtype requires_grad kwargs make_input shape make_tensor shape device=device dtype=dtype requires_grad=requires_grad make_long_input shape low high make_tensor shape device=device dtype=torch long low=low high=high M = S = generator -D index tensor idx = make_long_input low= high=M yield SampleInput make_input M S args= idx -D index tensor idx = make_long_input S low= high=M yield SampleInput make_input M S args= idx -D index tensor idx = make_long_input S S low= high=M yield SampleInput make_input M S args= idx idx = make_long_input low= high=S idx = idx = yield SampleInput make_input S S args= idx kwargs= padding_idx idx = make_long_input low= high=S idx = idx = yield SampleInput make_input S S args= idx kwargs= padding_idx - Scale gradient based inverse frequency particular index idx = make_long_input low= high=S idx = idx = weights = make_input S S yield SampleInput weights args= idx kwargs= scale_grad_by_freq True list generator additional_op_db append OpInfo nn functional embedding variant_test_name= functorch We use lambda reshuffle positional arguments This because currently only ` input ` field SampleInput tested gradient tests op=lambda weight idx kwargs torch nn functional embedding idx weight kwargs dtypes=floating_types_and torch bfloat torch float sample_inputs_func=sample_inputs_embedding supports_forward_ad=True supports_fwgrad_bwgrad=True supports_out=False sample_inputs_mse_loss op_info device dtype requires_grad kwargs make_input shape requires_grad=requires_grad make_tensor shape device=device dtype=dtype requires_grad=requires_grad rhs_requires_grad = kwargs get rhs_requires_grad requires_grad S = shapes = S S S S S S S S S reductions = none mean sum shape reduction itertools product shapes reductions yield SampleInput make_input shape args= make_input shape requires_grad=rhs_requires_grad kwargs= reduction reduction additional_op_db append OpInfo nn functional mse_loss variant_test_name= functorch sample_inputs_func=sample_inputs_mse_loss supports_out=False supports_forward_ad=True supports_fwgrad_bwgrad=True dtypes=floating_types_and torch float backward_dtypes=floating_types dtypesIfCUDA=floating_types_and torch bfloat torch float backward_dtypesIfCUDA=floating_types_and torch bfloat torch float TODO upstream sample inputs pytorch pytorch We more comprehensive sample_inputs_getitem op_info device dtype requires_grad kwargs Short advanced index adv_idx = torch LongTensor S = self_dim indices test_args = slice slice slice None slice None slice None slice None slice None slice None slice None slice None Ellipsis torch LongTensor slice None adv_idx adv_idx slice None slice None adv_idx slice None adv_idx adv_idx slice None slice None adv_idx slice None slice None adv_idx adv_idx Ellipsis adv_idx adv_idx slice None slice None adv_idx slice None adv_idx slice None slice None adv_idx adv_idx slice None slice None slice None adv_idx None adv_idx slice None slice None slice None slice None adv_idx adv_idx slice None slice None adv_idx adv_idx adv_idx slice None slice None None adv_idx adv_idx adv_idx get_shape dim tuple S + i i range dim tuple SampleInput make_tensor get_shape self_dim device=device dtype=dtype low=None high=None requires_grad=requires_grad args=args self_dim args test_args TODO split PyTorch s __getitem__ The problem we don t support indexing masks vmap additional_op_db append OpInfo __getitem__ variant_test_name= functorch dtypes=all_types_and_complex_and torch bool torch float torch bfloat supports_out=False supports_inplace_autograd=False supports_scripting=False op=torch Tensor __getitem__ assert_jit_shape_analysis=False TODO support index Tensor supports_forward_ad=True sample_inputs_func=sample_inputs_getitem Turns out index_put different torch index_put TODO figure out how upstream sample_inputs_aten_index_put op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad inputs = adv_idx = torch LongTensor self_shape indices additional = None adv_idx adv_idx None None adv_idx None adv_idx adv_idx None None adv_idx None None adv_idx adv_idx None None adv_idx None adv_idx None None adv_idx adv_idx None None None None adv_idx adv_idx None None adv_idx adv_idx adv_idx self_shape indices additional broadcast_value False True inp = make_arg self_shape tmp_indices = tuple slice None idx None idx idx indices values_shape = inp tmp_indices shape broadcast_value values_shape = values_shape values = make_arg values_shape inputs append SampleInput inp args= tuple indices values inputs sample_inputs_index_put op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad make_idx = partial make_tensor dtype=torch long device=device requires_grad=False S = inputs = accumulate False True putting vectors indexed locations inputs append SampleInput make_arg S S args= make_idx low= high= make_arg S kwargs=dict accumulate=accumulate putting multi-dim tensors indexed locations inputs append SampleInput make_arg S S args= make_idx low= high= make_arg S kwargs=dict accumulate=accumulate value size ` ` dim inputs append SampleInput make_arg S args= make_idx low= high= make_arg kwargs=dict accumulate=accumulate scalar value inputs append SampleInput make_arg S args= make_idx low= high=S make_arg kwargs=dict accumulate=accumulate cuda accumulate don t work well Reference https github com pytorch pytorch issues accumulate device == cuda Broadcast ` values ` inputs append SampleInput make_arg S S args= make_idx low= high=S make_arg S kwargs=dict accumulate=accumulate inputs additional_op_db append OpInfo index_put variant_test_name= functorch dtypes=all_types_and_complex_and torch bool torch float torch bfloat supports_out=False sample_inputs_func=sample_inputs_index_put supports_forward_ad=True additional_op_db append OpInfo ops aten index_put variant_test_name= functorch dtypes=all_types_and_complex_and torch bool torch float torch bfloat supports_out=False sample_inputs_func=sample_inputs_aten_index_put supports_forward_ad=True sample_inputs_masked_fill op_info device dtype requires_grad kwargs S = make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg S S args= torch randn S S device=device yield SampleInput make_arg S S args= torch randn S device=device yield SampleInput make_arg args= torch randn device=device yield SampleInput make_arg S S args= torch randn device=device yield SampleInput make_arg S args= torch randn S S device=device broadcasts_input=True additional_op_db append OpInfo masked_fill variant_test_name= functorch_Scalar_only dtypes=all_types_and_complex_and torch bool torch half torch bfloat torch chalf sample_inputs_func=sample_inputs_masked_fill supports_forward_ad=True supports_fwgrad_bwgrad=True check_batched_forward_grad=False supports_out=False sample_inputs_new_zeros_with_same_feature_meta op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad matrix = tangent base num_tangent_bdims results = tangent_shape base_shape num_tangent_bdims matrix tangent = make_arg tangent_shape base = make_arg base_shape results append SampleInput tangent args= base kwargs=dict self_num_batch_dims=num_tangent_bdims results additional_op_db append OpInfo ops aten _new_zeros_with_same_feature_meta variant_test_name= functorchonly dtypes=all_types_and_complex_and torch bool torch float torch bfloat supports_out=False supports_autograd=False supports_forward_ad=False sample_inputs_func=sample_inputs_new_zeros_with_same_feature_meta sample_inputs_conversion op_info device dtype requires_grad kwargs make_arg = partial make_tensor dtype=dtype device=device requires_grad=requires_grad shapes = memory_format_options = None torch contiguous_format shape memory_format itertools product shapes memory_format_options yield SampleInput make_arg shape kwargs= memory_format memory_format memory_format additional_op_db extend OpInfo bfloat op=lambda x args kwargs x bfloat args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_out=False variant_test_name= functorch_no_channels_last sample_inputs_func=sample_inputs_conversion skips= autograd tests don t handle operators change dtype DecorateInfo unittest expectedFailure TestFwdGradients DecorateInfo unittest expectedFailure TestBwdGradients DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit DecorateInfo unittest skip Skipped TestNNCOpInfo test_nnc_correctness OpInfo bool op=lambda x args kwargs x bool args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_out=False variant_test_name= functorch_no_channels_last sample_inputs_func=sample_inputs_conversion supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit OpInfo byte op=lambda x args kwargs x byte args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_out=False variant_test_name= functorch_no_channels_last sample_inputs_func=sample_inputs_conversion The autograd test runner cannot handle functions change dtype supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit OpInfo char op=lambda x args kwargs x char args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_out=False variant_test_name= functorch_no_channels_last sample_inputs_func=sample_inputs_conversion The autograd test runner cannot handle functions change dtype supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit OpInfo double op=lambda x args kwargs x double args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_out=False variant_test_name= functorch_no_channels_last sample_inputs_func=sample_inputs_conversion supports_forward_ad=True supports_fwgrad_bwgrad=True skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit OpInfo float op=lambda x args kwargs x float args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_out=False variant_test_name= functorch_no_channels_last sample_inputs_func=sample_inputs_conversion skips= autograd tests don t handle operators change dtype DecorateInfo unittest expectedFailure TestFwdGradients DecorateInfo unittest expectedFailure TestBwdGradients DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit OpInfo half op=lambda x args kwargs x half args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_out=False variant_test_name= functorch_no_channels_last sample_inputs_func=sample_inputs_conversion skips= autograd tests don t handle operators change dtype DecorateInfo unittest expectedFailure TestFwdGradients DecorateInfo unittest expectedFailure TestBwdGradients DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit OpInfo int op=lambda x args kwargs x int args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_out=False variant_test_name= functorch_no_channels_last sample_inputs_func=sample_inputs_conversion supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit OpInfo long op=lambda x args kwargs x long args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_out=False variant_test_name= functorch_no_channels_last sample_inputs_func=sample_inputs_conversion supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit OpInfo short op=lambda x args kwargs x short args kwargs dtypes=all_types_and_complex_and torch bool torch half torch bfloat supports_out=False variant_test_name= functorch_no_channels_last sample_inputs_func=sample_inputs_conversion supports_autograd=False skips= DecorateInfo unittest expectedFailure TestNormalizeOperators test_normalize_operator_exhaustive RuntimeError attribute lookup defined builtin DecorateInfo unittest expectedFailure TestJit test_variant_consistency_jit