__future__ annotations typing Any TYPE_CHECKING torch _dim_entry DimEntry TYPE_CHECKING Dim Tensor EnableAllLayers RAII-style context manager enabling functorch vmap layers It manages creation cleanup functorch dynamic layers This probably one more algorithmically important parts first dims Intuitively FCD can thought another way using vmap where you don t actually have vmap top level instead vmaps implicitly determined inspecting bound dimensions FCD tensors involved compute similar our concept non-lexical modes we spent long time talking about years ago But under hood you still need actually enable vmap mode So once FCD has determined all dims we batching over needs enable all those layers so functorch can actually apply batching rules Therefore enable all layers levels_start int levels_to_dim list Dim __init__ levels list DimEntry Initialize push dynamic layers all first-class dimensions Args levels List dimension entries create layers Dim levels_start = levels_to_dim = l levels l is_positional d = l dim assert isinstance d Dim levels_to_dim append d Sort level stable ordering levels_to_dim sort key=lambda d d _level __enter__ - EnableAllLayers noqa PYI Create functorch dynamic layers i dim enumerate levels_to_dim batch_size = dim size level = torch _C _functorch _vmap_increment_nesting batch_size different i == levels_start = level __exit__ exc_type Any exc_val Any exc_tb Any - None Clean up dynamic layers reverse order to_remove = levels_start + len levels_to_dim - i range len levels_to_dim popped = torch _C _functorch _vmap_decrement_nesting assert popped == to_remove - i f Expected layer to_remove - i got popped from_batched batchedtensor torch Tensor has_device bool - Tensor Create Tensor batched tensor unwrapping functorch layers Args batchedtensor Batched tensor functorch operation has_device Whether tensor has device info Returns Tensor appropriate levels Create positional levels base dimensions levels list DimEntry = i range -batchedtensor dim levels append DimEntry i tensor = batchedtensor while torch _C _functorch is_batchedtensor tensor level = torch _C _functorch maybe_get_level tensor assert level None assert level = levels_start level levels_start + len levels_to_dim dim = DimEntry levels_to_dim level - levels_start bdim = torch _C _functorch maybe_get_bdim tensor assert bdim None levels insert bdim dim tensor = torch _C _functorch get_unwrapped tensor Tensor result = Tensor result _tensor = tensor result _batchtensor = batchedtensor result _has_device = has_device result _levels = levels result inplace_update_layers batchtensor torch Tensor levels list DimEntry - None Update levels batched tensor place This requires _maybe_unsafe_set_level binding we ll add functorch Args batchtensor Batched tensor update levels New levels set Check tensor batched torch _C _functorch is_batchedtensor batchtensor impl = batchtensor i reversed range len levels_to_dim impl None break any l == DimEntry levels_to_dim i l levels This very interesting The level batch tensor meaningless We set RIGHT before we go into vmap torch _C _functorch _maybe_unsafe_set_level impl levels_start + i impl = torch _C _functorch get_unwrapped impl