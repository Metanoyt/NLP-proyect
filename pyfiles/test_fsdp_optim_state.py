Owner s oncall distributed bisect sys collections abc Callable copy deepcopy enum auto Enum typing Any Optional torch torch nn nn torch distributed dist torch distributed _shard sharded_tensor ShardedTensor torch distributed _state_dict_utils _gather_state_dict torch distributed algorithms _checkpoint checkpoint_wrapper _CHECKPOINT_WRAPPED_MODULE apply_activation_checkpointing torch distributed fsdp FullyShardedDataParallel FSDP torch distributed fsdp api ShardingStrategy torch distributed fsdp fully_sharded_data_parallel FullOptimStateDictConfig FullStateDictConfig OptimStateKeyType ShardedOptimStateDictConfig ShardedStateDictConfig StateDictSettings StateDictType torch distributed optim _NamedOptimizer torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp DEVICEInitMode FSDPInitMode FSDPTest TransformerWithSharedParams torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests TEST_WITH_DEV_DBG_ASAN STATE_DICT_TYPES = StateDictType FULL_STATE_DICT StateDictType SHARDED_STATE_DICT dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit _OSDCommMethod Enum Method communicating optimizer state dict internal tests BROADCAST_OBJECT_LIST = auto SCATTER_FULL_OSD = auto FLATTEN_SHARDED_OSD = auto OPTIM_STATE_DICT = auto _ModelClass Enum Different model type test NESTED = auto TRANSFORMER = auto Bias torch nn Module This module applies D additive bias dimension ` ` dim ` ` __init__ dim int - None super __init__ assert dim torch manual_seed bias = torch nn Parameter torch randn dim forward x x + bias BlockA torch nn Module Used define interesting nested structure FSDP wrapping BlockA Bias bias weight Bias bias __init__ in_dim int out_dim int - None super __init__ assert all v v in_dim out_dim torch manual_seed bias_module = Bias out_dim weight = torch nn Parameter torch randn in_dim out_dim bias_module = Bias out_dim relu = torch nn ReLU forward x x = x weight x = bias_module x x = relu x ensure biases have different gradients x = bias_module x x BlockB torch nn Module Used define interesting nested structure FSDP wrapping BlockB weight Bias bias Bias bias __init__ in_dim int out_dim int - None super __init__ assert all v v in_dim out_dim torch manual_seed weight = torch nn Parameter torch randn in_dim out_dim bias_module = Bias out_dim bias_module = Bias out_dim relu = torch nn ReLU forward x x = x weight x = bias_module x x = relu x ensure biases have different gradients x = bias_module x x NestedModel torch nn Module __init__ - None super __init__ block = BlockB block = BlockB bias = torch nn Parameter torch randn block = torch nn Sequential BlockA BlockA BlockB relu = torch nn ReLU forward x - torch Tensor x = relu block x x = relu block x x = relu block x x = x + bias x get_input device BATCH_SIZE = torch randn BATCH_SIZE device get_loss inp output output sum run_backward loss loss backward staticmethod wrap model torch nn Module group Optional dist ProcessGroup = None ignore_modules bool = False fsdp_kwargs Optional dict str Any = None - torch nn Module fsdp_kwargs None fsdp_kwargs = Flatten Bias then flatten weight Bias together into ` block ` model block bias_module = FSDP model block bias_module process_group=group fsdp_kwargs model block = FSDP model block process_group=group fsdp_kwargs Flatten Bias flatten Bias then flatten weight into ` block ` model block bias_module = FSDP model block bias_module process_group=group fsdp_kwargs model block bias_module = FSDP model block bias_module process_group=group fsdp_kwargs model block = FSDP model block process_group=group fsdp_kwargs Flatten weight Bias bias into ` block ` ignored_modules = model block bias_module ignore_modules None model block = FSDP model block process_group=group ignored_modules=ignored_modules fsdp_kwargs model staticmethod wrap_alt model torch nn Module group Optional dist ProcessGroup = None fsdp_kwargs Optional dict str Any = None - torch nn Module fsdp_kwargs None fsdp_kwargs = model block bias_module = FSDP model block bias_module process_group=group fsdp_kwargs model block = FSDP model block process_group=group fsdp_kwargs model staticmethod wrap_with_unmanaged_params model add_to_fsdp_module bool group=None - tuple torch nn Module list torch nn Parameter Registers unmanaged parameters before wrapping meth ` wrap ` device = next model parameters device unmanaged_param = torch nn Parameter torch randn device=device Either register parameter module wrapped FSDP ` model block ` module wrapped FSDP ` model ` register_module = model block add_to_fsdp_module model register_module register_parameter unmanaged_param unmanaged_param For simplicity we only add single unmanaged parameter should easy generalize needed NestedModel wrap model group unmanaged_param staticmethod add_unmanaged_param_entry osd unmanaged_param step - None Adds entry unmanaged parameter ` ` unmanaged_param ` ` assuming Adam optimizer single parameter group The unmanaged parameters should passed method ` model parameters ` order since their parameter IDs will assigned order skipped IDs Assign parameter ID unmanaged parameter unmanaged_param_id = - param_ids = osd param_groups params i range len param_ids diff = param_ids i - param_ids i - diff = assert diff f Invalid IDs param_ids i - param_ids i unmanaged_param_id = param_ids i - + break unmanaged_param_id == - unmanaged_param_id = len param_ids last ID skipped assert unmanaged_param_id = One parameter ID should skipped Add state entry unmanaged parameter state_device = next iter next iter osd state values values device osd state unmanaged_param_id = step torch tensor float step device=state_device exp_avg torch randn unmanaged_param shape device=state_device exp_avg_sq torch randn unmanaged_param shape device=state_device Insert ID into parameter group order bisect insort osd param_groups params unmanaged_param_id NOTE We exclude ` bias ` either parameter group test case where optimizer input does include all model parameters param_group - list torch nn Parameter Use ` block ` s parameters first parameter group deviate ` model parameters ` order list block parameters param_group - list torch nn Parameter Deviate ` model parameters ` order further rearranging ` block ` s parameters before ` block ` s parameters list block parameters + list block parameters Simple boring model test interface some corner cases do require complicated wrapping strategy TestDummyModel torch nn Module __init__ no_grad bool = False super __init__ torch manual_seed net = nn Sequential nn Linear nn ReLU net weight requires_grad = no_grad net bias requires_grad = no_grad net = nn Sequential nn Linear nn ReLU net = nn Linear net = nn Sequential nn ReLU nn Linear forward x net net net net x get_input torch rand device= cuda TestFSDPOptimState FSDPTest __init__ args kwargs super __init__ args kwargs _model_class = _ModelClass NESTED _init_nested_model _ModelClass TRANSFORMER _init_transformer_model _init_nested_model wrap bool wrap_alt bool = False ignored ` wrap=False ` device torch device = torch device cuda group=None optim_class type torch optim Optimizer = torch optim Adam use_multiple_param_groups bool = False use_diff_optim_inputs bool = False fsdp_kwargs Optional dict str Any = None model = NestedModel device wrap model = NestedModel wrap_alt model group fsdp_kwargs wrap_alt NestedModel wrap model group fsdp_kwargs=fsdp_kwargs use_multiple_param_groups optim_input = list model parameters optim_input = params model param_group params model param_group weight_decay Use reversed parameter order optimizer input odd ranks use_diff_optim_inputs rank == isinstance optim_input dict param_group optim_input param_group params = list reversed param_group params optim_input = list reversed optim_input optim = optim_class optim_input lr= model optim optim_input _init_transformer_model wrap bool device torch device = torch device cuda group=None optim_class type torch optim Optimizer = torch optim Adam use_multiple_param_groups bool = False use_diff_optim_inputs bool = False use_multiple_param_groups use_diff_optim_inputs Keep these arguments parity ` _init_nested_model ` these settings implemented since transformer wrapped FSDP top-level which means there only single flat parameter making these booleans vacuous raise NotImplementedError group None group = dist distributed_c d _get_default_group model = TransformerWithSharedParams init group FSDPInitMode RECURSIVE wrap FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE deterministic=True optim = optim_class model parameters lr= model optim None _step_model model torch nn Module optim torch optim Optimizer device torch device = torch device cuda num_iters int = - list float Performs forward pass backward pass optimizer step ` ` num_iters ` ` -many times returns per-iteration losses torch manual_seed set seed determinism losses = module = getattr model module model _ range num_iters optim zero_grad inp = module get_input device output = model inp loss = module get_loss inp output device losses append loss item module run_backward loss optim step losses _broadcast_full_osd full_osd dict str Any group=None Broadcasts full optimizer state dict place using ` ` torch save ` ` ` ` torch load ` ` so all ranks can have obj_list = full_osd dist broadcast_object_list obj_list src= group=group full_osd = obj_list full_osd _are_equal_states state dict str Any state dict str Any - bool Checks ` ` state ` ` ` ` state ` ` contain same mappings set state keys = set state keys False state_name value state items value = state state_name type value type value False torch is_tensor value tensor state assert torch is_tensor value Check values CPU device-agnostic value = value cpu value = value cpu value shape = value shape torch all torch isclose value value False non-tensor state value = value False True _check_same_state fsdp_osd ref_osd check_same_param_keys bool Checks ` ` full_osd ` ` ` ` ref_osd ` ` have same state part If ` ` check_same_param_keys=True ` ` then checks parameter keys match e g when both should parameter names does check parameter keys otherwise assert state ref_osd assertTrue state fsdp_osd ref_osd_state = ref_osd state fsdp_osd_state = k _gather_state_dict v k v fsdp_osd state items check_same_param_keys Check parameter keys same first earlier erroring ref_osd_param_ids = set ref_osd_state keys fsdp_osd_param_ids = set fsdp_osd_state keys assertTrue ref_osd_param_ids == fsdp_osd_param_ids f Rank rank ref_osd_param_ids fsdp_osd_param_ids Check state values same param_id param_state fsdp_osd_state items state_name value param_state items ref_value = ref_osd_state param_id state_name assertEqual value ref_value Otherwise only require parameter keys isomorphic e g between IDs names ref_osd_states = list ref_osd_state values fsdp_osd_states = list fsdp_osd_state values assertEqual len ref_osd_states len fsdp_osd_states Use brute-force quadratic-time comparison since hard hash tensor value instead object fsdp_osd_state fsdp_osd_states Check least one match may toy edge cases e g multiple biases nonetheless each having = match two lists having equal length imply list contents equal assertTrue any _are_equal_states fsdp_osd_state ref_osd_state ref_osd_state ref_osd_states _check_same_param_groups full_osd ref_osd check_same_param_keys bool Checks ` ` full_osd ` ` ` ` ref_osd ` ` have same param_groups part If ` ` check_same_param_keys=True ` then checks parameter keys match e g when both should parameter names does check parameter keys otherwise assert param_groups ref_osd assertTrue param_groups full_osd ref_osd_param_groups = ref_osd param_groups full_osd_param_groups = full_osd param_groups assertTrue len full_osd_param_groups len ref_osd_param_groups full_osd_pg ref_osd_pg zip full_osd_param_groups ref_osd_param_groups assertEqual set full_osd_pg keys set ref_osd_pg keys name full_osd_value full_osd_pg items name == params check_same_param_keys continue assertEqual full_osd_value ref_osd_pg name skip_if_lt_x_gpu parametrize state_dict_type STATE_DICT_TYPES parametrize use_multiple_param_groups False True parametrize rank _only False True parametrize use_diff_optim_inputs False True test_optim_state_dict_nested state_dict_type StateDictType use_multiple_param_groups bool rank _only bool use_diff_optim_inputs bool - None Tests meth ` full_optim_state_dict ` meth ` sharded_optim_state_dict ` comparing returned dict FSDP-wrapped model equivalent non-wrapped model The test checks equivalence excluding parameter keys since FSDP normal optimizer state dicts key names IDs respectively This means test can pass even parameter keys incorrectly mapped values Their correct mapping tested other tests exercise save load workflow run_subtests use_optim_input False True _test_optim_state_dict_nested state_dict_type=state_dict_type use_multiple_param_groups=use_multiple_param_groups rank _only=rank _only use_diff_optim_inputs=use_diff_optim_inputs _test_optim_state_dict_nested state_dict_type StateDictType use_multiple_param_groups bool rank _only bool use_diff_optim_inputs bool use_optim_input bool - None rank _only state_dict_type == StateDictType SHARDED_STATE_DICT supported NUM_ITERS = model optim optim_input = _init_nested_model wrap=True use_multiple_param_groups=use_multiple_param_groups use_diff_optim_inputs=use_diff_optim_inputs losses = _step_model model optim num_iters=NUM_ITERS state_dict_type == StateDictType FULL_STATE_DICT use_optim_input fsdp_osd = FSDP full_optim_state_dict model optim optim_input rank _only=rank _only fsdp_osd = FSDP full_optim_state_dict model optim rank _only=rank _only fsdp_osd = FSDP sharded_optim_state_dict model optim Non-target ranks get empty state dict rank _only rank = assertEqual len fsdp_osd model optim _ = _init_nested_model wrap=False use_multiple_param_groups=use_multiple_param_groups use_diff_optim_inputs=use_diff_optim_inputs losses = _step_model model optim num_iters=NUM_ITERS ref_osd = optim state_dict Check losses eliminate model drift source error i l l enumerate zip losses losses assert l == l f Losses differ iter i l f l f Do check parameter keys since full sharded optimizer state dict uses parameter names while non-wrapped equivalent uses parameter IDs check_same_param_keys = False _check_same_param_groups fsdp_osd ref_osd check_same_param_keys=check_same_param_keys _check_same_state fsdp_osd ref_osd check_same_param_keys=check_same_param_keys skip_if_lt_x_gpu test_full_optim_state_dict_keys Tests parameter keys returned meth ` full_optim_state_dict ` match those meth ` state_dict ` full ` ` state_dict_type ` ` non-FSDP-root model nested FSDP instances ignored modules device = torch device cuda model = NestedModel device wrapped_model = NestedModel wrap model ignore_modules=True Add checkpointing ensure optim_state_dict state_dict strip out checkpointing prefixes apply_activation_checkpointing model check_fn=lambda module isinstance module torch nn Sequential optim = torch optim Adam wrapped_model parameters lr= e- _step_model model optim device optim_state_dict = FSDP full_optim_state_dict wrapped_model optim rank _only=False FSDP state_dict_type wrapped_model StateDictType FULL_STATE_DICT state_dict = wrapped_model state_dict assertEqual optim_state_dict state keys state_dict keys Check checkpointing prefix indeed stripped key optim_state_dict state assertNotIn _CHECKPOINT_WRAPPED_MODULE key skip_if_lt_x_gpu test_full_optim_state_dict_nested_invalid Tests meth ` full_optim_state_dict ` raises error when nonzero ranks missing optimizer state parameters rank device = torch device cuda model = NestedModel wrap NestedModel device None optim_input = list model parameters rank = Exclude parameter so nonzero ranks missing state optim_input = optim_input - optim = torch optim Adam optim_input lr= e- _step_model model optim num_iters= error_regex = FSDP currently requires each rank have least optimizer states needed rank s optimizer some ranks missing some those states assertRaisesRegex RuntimeError error_regex FSDP full_optim_state_dict model optim skip_if_lt_x_gpu parametrize use_multiple_param_groups False True parametrize wrap_alt False True parametrize use_diff_optim_inputs False True test_shard_full_optim_state_dict_nested use_multiple_param_groups bool wrap_alt bool use_diff_optim_inputs bool Tests meth ` shard_full_optim_state_dict ` non-FSDP-root model nested FSDP instances run_subtests use_optim_input False True _test_load_optim_state model_class=_ModelClass NESTED use_multiple_param_groups=use_multiple_param_groups halve_world_size=False osd_comm_method=_OSDCommMethod BROADCAST_OBJECT_LIST use_diff_optim_inputs=use_diff_optim_inputs wrap_alt=wrap_alt num_iters= _test_load_optim_state_with_optim_state_dict _ModelClass NESTED state_dict_settings=StateDictSettings StateDictType FULL_STATE_DICT FullStateDictConfig FullOptimStateDictConfig use_multiple_param_groups=False halve_world_size=False use_diff_optim_inputs=use_diff_optim_inputs wrap_alt=wrap_alt num_iters= skip_if_lt_x_gpu test_shard_full_optim_state_dict_nested_halve_world_size Tests meth ` shard_full_optim_state_dict ` non-FSDP-root model nested FSDP instances when loading into new process group halved world size To save CI costs we test harder settings use_multiple_param_groups = True use_diff_optim_inputs = True wrap_alt = True run_subtests use_optim_input False True _test_load_optim_state model_class=_ModelClass NESTED use_multiple_param_groups=use_multiple_param_groups halve_world_size=True osd_comm_method=_OSDCommMethod BROADCAST_OBJECT_LIST use_diff_optim_inputs=use_diff_optim_inputs wrap_alt=wrap_alt num_iters= _test_load_optim_state_with_optim_state_dict _ModelClass NESTED state_dict_settings=StateDictSettings StateDictType FULL_STATE_DICT FullStateDictConfig FullOptimStateDictConfig use_multiple_param_groups=use_multiple_param_groups halve_world_size=True use_diff_optim_inputs=use_diff_optim_inputs wrap_alt=wrap_alt num_iters= skip_if_lt_x_gpu test_shard_full_optim_state_dict_transformer - None Tests meth ` shard_full_optim_state_dict ` FSDP-root transformer model shared parameters run_subtests use_optim_input False True _test_load_optim_state model_class=_ModelClass TRANSFORMER use_multiple_param_groups=False halve_world_size=True osd_comm_method=_OSDCommMethod BROADCAST_OBJECT_LIST use_diff_optim_inputs=False num_iters= _test_load_optim_state_with_optim_state_dict _ModelClass TRANSFORMER state_dict_settings=StateDictSettings StateDictType FULL_STATE_DICT FullStateDictConfig FullOptimStateDictConfig use_multiple_param_groups=False halve_world_size=True use_diff_optim_inputs=False num_iters= skip_if_lt_x_gpu parametrize use_multiple_param_groups False True parametrize wrap_alt False True parametrize use_diff_optim_inputs False True test_scatter_full_optim_state_dict_nested use_multiple_param_groups bool wrap_alt bool use_diff_optim_inputs bool Tests meth ` scatter_full_optim_state_dict ` non-FSDP-root model nested FSDP instances run_subtests use_optim_input False True _test_load_optim_state model_class=_ModelClass NESTED use_multiple_param_groups=use_multiple_param_groups halve_world_size=False osd_comm_method=_OSDCommMethod SCATTER_FULL_OSD use_diff_optim_inputs=use_diff_optim_inputs wrap_alt=wrap_alt num_iters= _test_load_optim_state_with_optim_state_dict _ModelClass NESTED state_dict_settings=StateDictSettings StateDictType FULL_STATE_DICT FullStateDictConfig FullOptimStateDictConfig rank _only=True use_multiple_param_groups=use_multiple_param_groups halve_world_size=False use_diff_optim_inputs=use_diff_optim_inputs wrap_alt=wrap_alt num_iters= skip_if_lt_x_gpu test_scatter_full_optim_state_dict_nested_halve_world_size Tests meth ` scatter_full_optim_state_dict ` non-FSDP-root model nested FSDP instances when loading into new process group halved world size To save CI costs we test harder settings use_multiple_param_groups = True use_diff_optim_inputs = True wrap_alt = True run_subtests use_optim_input False True _test_load_optim_state model_class=_ModelClass NESTED use_multiple_param_groups=use_multiple_param_groups halve_world_size=True osd_comm_method=_OSDCommMethod SCATTER_FULL_OSD use_diff_optim_inputs=use_diff_optim_inputs wrap_alt=wrap_alt num_iters= _test_load_optim_state_with_optim_state_dict _ModelClass NESTED state_dict_settings=StateDictSettings StateDictType FULL_STATE_DICT FullStateDictConfig FullOptimStateDictConfig rank _only=True use_multiple_param_groups=use_multiple_param_groups halve_world_size=True use_diff_optim_inputs=use_diff_optim_inputs wrap_alt=wrap_alt num_iters= skip_if_lt_x_gpu test_scatter_full_optim_state_dict_transformer - None Tests meth ` scatter_full_optim_state_dict ` FSDP-root transformer model shared parameters run_subtests use_optim_input False True _test_load_optim_state model_class=_ModelClass TRANSFORMER use_multiple_param_groups=False halve_world_size=True osd_comm_method=_OSDCommMethod SCATTER_FULL_OSD use_diff_optim_inputs=False num_iters= _test_load_optim_state_with_optim_state_dict _ModelClass TRANSFORMER state_dict_settings=StateDictSettings StateDictType FULL_STATE_DICT FullStateDictConfig FullOptimStateDictConfig rank _only=True use_multiple_param_groups=False halve_world_size=True use_diff_optim_inputs=False num_iters= skip_if_lt_x_gpu test_flatten_sharded_optim_state_dict_nested - None Tests meth ` flatten_sharded_optim_state_dict ` FSDP-root nested model _test_load_optim_state _ModelClass NESTED use_multiple_param_groups=False halve_world_size=False osd_comm_method=_OSDCommMethod FLATTEN_SHARDED_OSD use_diff_optim_inputs=False use_optim_input=False wrap_alt=True num_iters= _test_load_optim_state_with_optim_state_dict _ModelClass NESTED state_dict_settings=StateDictSettings StateDictType SHARDED_STATE_DICT ShardedStateDictConfig ShardedOptimStateDictConfig use_multiple_param_groups=False halve_world_size=False use_diff_optim_inputs=False wrap_alt=True num_iters= skip_if_lt_x_gpu test_flatten_sharded_optim_state_dict_transformer - None Tests meth ` flatten_sharded_optim_state_dict ` FSDP-root transformer model _test_load_optim_state _ModelClass TRANSFORMER use_multiple_param_groups=False halve_world_size=False osd_comm_method=_OSDCommMethod FLATTEN_SHARDED_OSD use_diff_optim_inputs=False use_optim_input=False num_iters= _test_load_optim_state_with_optim_state_dict _ModelClass TRANSFORMER state_dict_settings=StateDictSettings StateDictType SHARDED_STATE_DICT ShardedStateDictConfig ShardedOptimStateDictConfig use_multiple_param_groups=False halve_world_size=False use_diff_optim_inputs=False num_iters= skip_if_lt_x_gpu test_use_orig_params - None Tests meth ` optim_state_dict ` FSDP-root nested model run_subtests halve_world_size True False wrap_alt True False _test_load_optim_state_with_optim_state_dict model_class=_ModelClass NESTED state_dict_settings=StateDictSettings StateDictType FULL_STATE_DICT FullStateDictConfig FullOptimStateDictConfig use_multiple_param_groups=False use_diff_optim_inputs=False num_iters= fsdp_kwargs= use_orig_params True run_subtests halve_world_size True False wrap_alt True False _test_load_optim_state_with_optim_state_dict model_class=_ModelClass NESTED state_dict_settings=StateDictSettings StateDictType FULL_STATE_DICT FullStateDictConfig FullOptimStateDictConfig rank _only=True use_multiple_param_groups=False use_diff_optim_inputs=False num_iters= fsdp_kwargs= use_orig_params True run_subtests wrap_alt True False _test_load_optim_state_with_optim_state_dict model_class=_ModelClass NESTED state_dict_settings=StateDictSettings StateDictType SHARDED_STATE_DICT ShardedStateDictConfig ShardedOptimStateDictConfig use_multiple_param_groups=False We cannot test halve_world_size SHARDED_STATE_DICT halve_world_size=False use_diff_optim_inputs=False num_iters= fsdp_kwargs= use_orig_params True _test_load_optim_state model_class _ModelClass use_multiple_param_groups bool halve_world_size bool osd_comm_method _OSDCommMethod use_diff_optim_inputs bool use_optim_input bool num_iters int new_model_kwargs Runs model full world size K iterations generate full sharded optimizer state dict initializes model halved world size possibly different FSDP wrapping scheme based ` ` new_model_kwargs ` ` loads full sharded optimizer state dict according halved-world-size model runs halved-world-size model K iterations checks sharded optimizer state dict matches halved-world-size model s local optimizer state dict meaning former could have equivalently been loaded into local optimizer initializer = _model_class model_class osd_comm_method == _OSDCommMethod OPTIM_STATE_DICT osd_method = FSDP optim_state_dict osd_comm_method == _OSDCommMethod FLATTEN_SHARDED_OSD osd_method = FSDP sharded_optim_state_dict osd_method = FSDP full_optim_state_dict First run wrapped model full world size few iterations model optim optim_input = initializer wrap=True use_multiple_param_groups=use_multiple_param_groups _step_model model optim num_iters=num_iters fsdp_osd = osd_method model optim optim_input use_optim_input osd_method model optim halve_world_size Create new process group halved world size new_group_ranks = r r range world_size r == new_group = dist new_group ranks=new_group_ranks rank new_group_ranks Continue using same group hence world size new_group = dist distributed_c d _get_default_group Second run wrapped model possibly halved world size possibly differing ` optim_input ` across ranks model optim optim_input = initializer wrap=True group=new_group use_multiple_param_groups=use_multiple_param_groups use_diff_optim_inputs=use_diff_optim_inputs new_model_kwargs specify ` wrap_alt ` change wrapping _step_model model optim num_iters=num_iters fsdp_osd = osd_method model optim optim_input group=new_group use_optim_input osd_method model optim group=new_group Compute two sharded optim state dicts first model according second model second model according second model osd_comm_method == _OSDCommMethod BROADCAST_OBJECT_LIST fsdp_osd = _broadcast_full_osd fsdp_osd group=new_group sharded_osd = FSDP shard_full_optim_state_dict fsdp_osd model optim_input=optim_input use_optim_input FSDP shard_full_optim_state_dict fsdp_osd model optim=optim fsdp_osd = _broadcast_full_osd fsdp_osd group=new_group sharded_osd = FSDP shard_full_optim_state_dict fsdp_osd model optim_input=optim_input use_optim_input FSDP shard_full_optim_state_dict fsdp_osd model optim=optim osd_comm_method == _OSDCommMethod SCATTER_FULL_OSD sharded_osd = FSDP scatter_full_optim_state_dict fsdp_osd rank == None model optim_input=optim_input group=new_group use_optim_input FSDP scatter_full_optim_state_dict fsdp_osd rank == None model optim=optim group=new_group sharded_osd = FSDP scatter_full_optim_state_dict fsdp_osd rank == None model optim_input=optim_input group=new_group use_optim_input FSDP scatter_full_optim_state_dict fsdp_osd rank == None model optim=optim group=new_group osd_comm_method == _OSDCommMethod FLATTEN_SHARDED_OSD sharded_osd = FSDP flatten_sharded_optim_state_dict fsdp_osd model optim=optim sharded_osd = FSDP flatten_sharded_optim_state_dict fsdp_osd model optim=optim osd_comm_method == _OSDCommMethod OPTIM_STATE_DICT sharded_osd = FSDP optim_state_dict_to_load model optim fsdp_osd sharded_osd = FSDP optim_state_dict_to_load model optim fsdp_osd As sanity check check sharding second model s full sharded optimizer state dict according itself equivalent its local optimizer s state dict local_osd = optim state_dict check_same_param_keys = True should all have matching parameter IDs _check_same_param_groups sharded_osd local_osd check_same_param_keys=check_same_param_keys _check_same_state sharded_osd local_osd check_same_param_keys=check_same_param_keys Check sharding first model s full sharded optimizer state dict according second model equivalent second model s local optimizer state dict _check_same_param_groups sharded_osd local_osd check_same_param_keys=check_same_param_keys _check_same_state sharded_osd local_osd check_same_param_keys=check_same_param_keys As sanity check check we can load run few iterations optim load_state_dict sharded_osd _step_model model optim num_iters=num_iters skip_if_lt_x_gpu parametrize state_dict_type STATE_DICT_TYPES parametrize add_to_fsdp_module False True test_shard_full_optim_state_dict_unmanaged_params state_dict_type StateDictType add_to_fsdp_module bool Tests meth ` shard_full_optim_state_dict ` when there unmanaged parameters - If ` ` add_to_fsdp_module=True ` ` then unmanaged parameters added module wrapped FSDP which case there should error since we require all unflattened parameter comprising flat parameter have same scalar state e g Adam step added parameter missing its entry - If ` ` add_to_fsdp_module=False ` ` then unmanaged parameters added module wrapped FSDP which case there should no error emulating model parallel use cases where some parameters may managed externally FSDP We do separately test unmanaged parameters meth ` scatter_full_optim_state_dict ` ` flatten_sharded_optim_state_dict ` save CI cost since call into same subroutine meth ` _flatten_optim_state_dict ` state_dict_type == StateDictType SHARDED_STATE_DICT use_optim_input = False use_optim_input = False True run_subtests use_optim_input use_optim_input _test_shard_full_optim_state_dict_unmanaged_params state_dict_type=state_dict_type add_to_fsdp_module=add_to_fsdp_module _test_shard_full_optim_state_dict_unmanaged_params state_dict_type StateDictType add_to_fsdp_module bool use_optim_input bool NUM_ITERS = Create normal wrapped model model optim optim_input = _init_nested_model wrap=True _step_model model optim num_iters=NUM_ITERS state_dict_type == StateDictType FULL_STATE_DICT fsdp_osd = FSDP full_optim_state_dict model optim optim_input rank _only=False use_optim_input FSDP full_optim_state_dict model optim rank _only=False save all ranks avoid having broadcast rank fsdp_osd = FSDP sharded_optim_state_dict model optim Create new model same structure additional unmanaged parameters representing model which we want load device = torch device cuda model = NestedModel device model unmanaged_params = NestedModel wrap_with_unmanaged_params model add_to_fsdp_module optim_input = list model parameters optim = torch optim Adam optim_input lr= e- add_to_fsdp_module If we add unmanaged parameters module wrapped FSDP then flat parameter will comprised some unflattened parameters zero-dimensional tensor state i e Adam step others without i e unmanaged parameters which triggers error we have ensure correctness error_prefix = ^ All unflattened parameters comprising single flat parameter must have scalar state same value dtype assertRaisesRegex ValueError error_prefix state_dict_type == StateDictType FULL_STATE_DICT FSDP shard_full_optim_state_dict fsdp_osd model optim_input=optim_input use_optim_input FSDP shard_full_optim_state_dict fsdp_osd model optim=optim FSDP flatten_sharded_optim_state_dict fsdp_osd model optim=optim If we add unmanaged parameters module wrapped FSDP then we simply ignore them without erroring enable model parallelism use cases where some parameters managed externally FSDP state_dict_type == StateDictType FULL_STATE_DICT flattened_osd = FSDP shard_full_optim_state_dict fsdp_osd model optim_input=optim_input use_optim_input FSDP shard_full_optim_state_dict fsdp_osd model optim=optim flattened_osd = FSDP flatten_sharded_optim_state_dict fsdp_osd model optim=optim Add entries unmanaged parameters able load unmanaged_param unmanaged_params NestedModel add_unmanaged_param_entry flattened_osd unmanaged_param NUM_ITERS Check we can load optimizer state dict optim load_state_dict flattened_osd skip_if_lt_x_gpu parametrize state_dict_type STATE_DICT_TYPES parametrize use_multiple_param_groups False True test_rekey_optim_state_dict_to_ids state_dict_type StateDictType use_multiple_param_groups bool Tests meth ` rekey_optim_state_dict ` new keys being parameter IDs checking wrapped model i e FSDP modules can rekey its optimizer state dict match equivalent non-wrapped model i e without FSDP modules state_dict_type == StateDictType SHARDED_STATE_DICT use_optim_input = False use_optim_input = False True run_subtests use_optim_input use_optim_input _test_rekey_optim_state_dict_to_ids state_dict_type=state_dict_type use_multiple_param_groups=use_multiple_param_groups skip_if_lt_x_gpu _test_rekey_optim_state_dict_to_ids state_dict_type StateDictType use_multiple_param_groups bool use_optim_input bool NUM_ITERS = Run wrapped model few iterations model optim optim_input = _init_nested_model wrap=True use_multiple_param_groups=use_multiple_param_groups _step_model model optim num_iters=NUM_ITERS state_dict_type == StateDictType FULL_STATE_DICT fsdp_osd = FSDP full_optim_state_dict model optim optim_input use_optim_input FSDP full_optim_state_dict model optim Broadcast instead ` torch save ` ` torch load ` so all ranks have full state dict fsdp_osd = _broadcast_full_osd fsdp_osd fsdp_osd = FSDP sharded_optim_state_dict model optim Run non-wrapped model few iterations model optim optim_input = _init_nested_model wrap=False use_multiple_param_groups=use_multiple_param_groups _step_model model optim num_iters=NUM_ITERS Re-key wrapped model s optimizer state dict using parameter IDs according non-wrapped model rekeyed_osd = FSDP rekey_optim_state_dict fsdp_osd OptimStateKeyType PARAM_ID model optim_input=optim_input use_optim_input FSDP rekey_optim_state_dict fsdp_osd OptimStateKeyType PARAM_ID model optim=optim Check re-keyed dict actual dict same osd = optim state_dict check_same_param_keys = True _check_same_param_groups rekeyed_osd osd check_same_param_keys=check_same_param_keys _check_same_state rekeyed_osd osd check_same_param_keys=check_same_param_keys As sanity check check we can load run few iterations state_dict_type = StateDictType SHARDED_STATE_DICT optim load_state_dict rekeyed_osd _step_model model optim num_iters=NUM_ITERS skip_if_lt_x_gpu test_rekey_optim_state_dict_to_names Tests meth ` rekey_optim_state_dict ` new keys being parameter names checking non-wrapped model i e without FSDP modules can rekey its optimizer state dict match expected output meth ` full_optim_state_dict ` hence sharded using meth ` shard_full_optim_state_dict ` finally match per-rank optimizer state dict wrapped model i e FSDP modules run_subtests use_optim_input False True _test_rekey_optim_state_dict_to_names use_multiple_param_groups=False _test_rekey_optim_state_dict_to_names use_multiple_param_groups bool use_optim_input bool NUM_ITERS = Run wrapped model few iterations model optim optim_input = _init_nested_model wrap=True use_multiple_param_groups=use_multiple_param_groups _step_model model optim num_iters=NUM_ITERS Run non-wrapped model few iterations model optim optim_input = _init_nested_model wrap=False use_multiple_param_groups=use_multiple_param_groups _step_model model optim num_iters=NUM_ITERS Re-key non-wrapped model s optimizer state dict using parameter names still according itself osd = optim state_dict rekeyed_osd = FSDP rekey_optim_state_dict osd OptimStateKeyType PARAM_NAME model optim_input=optim_input use_optim_input FSDP rekey_optim_state_dict osd OptimStateKeyType PARAM_NAME model optim=optim Shard non-wrapped model s re-keyed optimizer state dict which maps back flattened parameter IDs sharded_osd = FSDP shard_full_optim_state_dict rekeyed_osd model optim_input=optim_input use_optim_input FSDP shard_full_optim_state_dict rekeyed_osd model optim=optim Check sharded optimizer state dict matches wrapped model s per-rank optimizer state dict osd = optim state_dict check_same_param_keys = True _check_same_param_groups sharded_osd osd check_same_param_keys=check_same_param_keys _check_same_state sharded_osd osd check_same_param_keys=check_same_param_keys As sanity check check we can load run few iterations optim load_state_dict sharded_osd _step_model model optim num_iters=NUM_ITERS skip_if_lt_x_gpu test_optim_input_warning Tests passing ` ` optim_input ` ` argument into optimizer state checkpointing APIs issues warning should_check_method method_name str Check every method since they all accept ` optim_input ` method_name sharded_optim_state_dict flatten_sharded_optim_state_dict get_warning_context warning_regex = ` optim_input ` argument deprecated assertWarnsRegex expected_warning=FutureWarning expected_regex=warning_regex _run_on_all_optim_state_apis should_check_method get_warning_context fsdp_kwargs=None _run_on_all_optim_state_apis should_check_method_fn Callable str bool context_fn Callable fsdp_kwargs Optional dict str Any Runs through all optimizer state checkpointing APIs context manager instantiated ` ` context_fn ` ` Certain APIs can skipped via ` ` should_check_method_fn ` ` which gets passed string name method wrapped_model wrapped_optim wrapped_optim_input = _init_nested_model wrap=True use_multiple_param_groups=False fsdp_kwargs=fsdp_kwargs _step_model wrapped_model wrapped_optim num_iters= Sharded optim state dict should_check_method_fn sharded_optim_state_dict context_fn fsdp_osd = FSDP sharded_optim_state_dict wrapped_model wrapped_optim fsdp_osd locals fsdp_osd = may defined due previous method erroring should_check_method_fn flatten_sharded_optim_state_dict context_fn FSDP flatten_sharded_optim_state_dict fsdp_osd wrapped_model wrapped_optim Full optim state dict should_check_method_fn full_optim_state_dict context_fn fsdp_osd = FSDP full_optim_state_dict wrapped_model wrapped_optim optim_input=wrapped_optim_input rank _only=False should_check_method_fn shard_full_optim_state_dict context_fn FSDP shard_full_optim_state_dict fsdp_osd wrapped_model optim_input=wrapped_optim_input should_check_method_fn scatter_full_optim_state_dict context_fn FSDP scatter_full_optim_state_dict fsdp_osd wrapped_model optim_input=wrapped_optim_input Rekey optim state dict nonwrapped_model nonwrapped_optim nonwrapped_optim_input = _init_nested_model wrap=False use_multiple_param_groups=False should_check_method_fn rekey_optim_state_dict context_fn FSDP rekey_optim_state_dict fsdp_osd ` full_optim_state_dict ` OptimStateKeyType PARAM_ID nonwrapped_model optim_input=nonwrapped_optim_input _step_model nonwrapped_model nonwrapped_optim num_iters= osd = nonwrapped_optim state_dict should_check_method_fn rekey_optim_state_dict context_fn FSDP rekey_optim_state_dict osd OptimStateKeyType PARAM_NAME nonwrapped_model optim_input=nonwrapped_optim_input skip_if_lt_x_gpu parametrize state_dict_type STATE_DICT_TYPES test_save_load_without_ th_param_state state_dict_type StateDictType Tests saving loading optim state dict Adam optimizer i e any optimizer step key its state when first parameter does have optimizer state e g unused frozen Model nn Module __init__ - None super __init__ lin = nn Linear lin = nn Linear relu = nn ReLU forward x torch Tensor - torch Tensor Do use ` lin ` which parameter passed optimizer one checked step state see tensor float relu lin x model = Model cuda model lin = FSDP model lin model lin = FSDP model lin fsdp_model = FSDP model optim = torch optim Adam fsdp_model parameters lr= e- any optimizer step Run iteration construct optimizer state device = torch device cuda inp = torch randn device=device loss = fsdp_model inp sum loss backward optim step Check save load does error state_dict_type == StateDictType FULL_STATE_DICT fsdp_osd = FSDP full_optim_state_dict fsdp_model optim rank _only=False flattened_osd = FSDP shard_full_optim_state_dict fsdp_osd fsdp_model state_dict_type == StateDictType SHARDED_STATE_DICT fsdp_osd = FSDP sharded_optim_state_dict fsdp_model optim flattened_osd = FSDP flatten_sharded_optim_state_dict fsdp_osd fsdp_model optim optim load_state_dict flattened_osd ` __setstate__ ` will check th parameter see step represented tensor float so imperative its state non-empty Run iteration sanity check inp = torch randn device=device loss = fsdp_model inp sum loss backward optim step skip_if_lt_x_gpu test_compatible_with_trec DenseModel torch nn Module __init__ - None super __init__ net = nn Sequential nn Linear nn ReLU net = nn Sequential nn Linear nn ReLU net = nn Linear net = nn Sequential nn ReLU nn Linear forward x net net net net x FakeMPModel torch nn Module __init__ - None super __init__ torch manual_seed dense = FSDP DenseModel cuda use_orig_params=True dist get_rank == sparse = nn Sequential nn Linear nn ReLU sparse = nn Sequential nn Linear nn ReLU forward x dist get_rank == sparse = sparse x sparse = sparse x dist all_reduce sparse dense sparse models = FakeMPModel cuda FakeMPModel cuda optims = torch optim Adam models parameters lr= e- _NamedOptimizer models named_parameters torch optim Adam params models parameters models lr= e- state_dicts = Train one batch see optim_state_dict same batch = torch rand device=torch device cuda model optim zip models optims Eagerly initialize states param model parameters param requires_grad t = torch zeros_like param param grad = torch autograd Variable t optim step loss = model batch sum loss backward optim step state_dicts append deepcopy FSDP optim_state_dict model optim _check_same_param_groups state_dicts state_dicts check_same_param_keys=False _check_same_state state_dicts state_dicts check_same_param_keys=True Make optim has different state _ range batch = torch rand cuda loss = models batch sum loss backward optims step Load state back see load_optim_state_dict works state_dict_to_load = FSDP optim_state_dict_to_load models optims state_dicts is_named_optimizer=True optims load_state_dict state_dict_to_load state_dicts = FSDP optim_state_dict models optims _check_same_param_groups state_dicts state_dicts check_same_param_keys=False _check_same_state state_dicts state_dicts check_same_param_keys=True skip_if_lt_x_gpu test_optim_state_without_param_groups SimpleModel torch nn Module __init__ - None super __init__ torch manual_seed net = nn Sequential nn Linear nn ReLU forward x net x model = FSDP SimpleModel cuda optim = torch optim Adam model parameters lr= e- Train one step save original optimizer state dict original optimizer param groups batch = torch rand device=torch device cuda param model parameters param requires_grad t = torch zeros_like param param grad = torch autograd Variable t optim step loss = model batch sum loss backward original_osd = deepcopy optim state_dict original_osd_no_param_groups = deepcopy original_osd manually remove param_groups optimizer state dict original_param_groups = deepcopy original_osd_no_param_groups pop param_groups passing osd without param_groups FSDP original_fsdp_optim_state_dict = deepcopy FSDP optim_state_dict model optim optim_state_dict=original_osd_no_param_groups check state_dict sharded FSDP does contain param_groups assertEqual None original_fsdp_optim_state_dict get param_groups train another step make optim different state param model parameters param requires_grad t = torch zeros_like param param grad = torch autograd Variable t optim step loss = model batch sum loss backward state_dict_to_load = FSDP optim_state_dict_to_load model optim original_fsdp_optim_state_dict manually add param_groups state_dict_to_load before loading optimizer state state_dict_to_load param_groups = original_param_groups optim load_state_dict state_dict_to_load assertEqual original_osd optim state_dict fsdp_optim_state = FSDP optim_state_dict model optim _check_same_state original_fsdp_optim_state_dict fsdp_optim_state check_same_param_keys=True assertEqual original_param_groups optim state_dict param_groups skip_if_lt_x_gpu test_with_empty_optimizer_state model = FSDP TestDummyModel cuda optim = torch optim Adam model parameters lr= e- state_dict = optim state_dict gathered_state_dict = FSDP optim_state_dict model optim assertEqual gathered_state_dict state state_dict state _test_load_optim_state_with_optim_state_dict model_class _ModelClass state_dict_settings StateDictSettings use_multiple_param_groups bool halve_world_size bool use_diff_optim_inputs bool num_iters int new_model_kwargs Runs model full world size K iterations generate full sharded optimizer state dict initializes model halved world size possibly different FSDP wrapping scheme based ` ` new_model_kwargs ` ` loads full sharded optimizer state dict according halved-world-size model runs halved-world-size model K iterations checks sharded optimizer state dict matches halved-world-size model s local optimizer state dict meaning former could have equivalently been loaded into local optimizer initializer = _model_class model_class First run wrapped model full world size few iterations model optim _ = initializer wrap=True use_multiple_param_groups=use_multiple_param_groups FSDP set_state_dict_type model state_dict_settings state_dict_type state_dict_settings state_dict_config state_dict_settings optim_state_dict_config _step_model model optim num_iters=num_iters fsdp_osd = FSDP optim_state_dict model optim halve_world_size Create new process group halved world size new_group_ranks = r r range world_size r == new_group = dist new_group ranks=new_group_ranks rank new_group_ranks Continue using same group hence world size new_group = dist distributed_c d _get_default_group Second run wrapped model possibly halved world size possibly differing ` optim_input ` across ranks model optim _ = initializer wrap=True group=new_group use_multiple_param_groups=use_multiple_param_groups use_diff_optim_inputs=use_diff_optim_inputs new_model_kwargs specify ` wrap_alt ` change wrapping FSDP set_state_dict_type model state_dict_settings state_dict_type state_dict_settings state_dict_config state_dict_settings optim_state_dict_config _step_model model optim num_iters=num_iters fsdp_osd = FSDP optim_state_dict model optim group=new_group Compute two sharded optim state dicts first model according second model second model according second model sharded_osd = FSDP optim_state_dict_to_load model optim fsdp_osd group=new_group As sanity check check sharding second model s full sharded optimizer state dict according itself equivalent its local optimizer s state dict local_osd = optim state_dict _check_same_param_groups sharded_osd local_osd check_same_param_keys=True _check_same_state sharded_osd local_osd check_same_param_keys=True Check sharding first model s full sharded optimizer state dict according second model equivalent second model s local optimizer state dict sharded_osd = FSDP optim_state_dict_to_load model optim fsdp_osd group=new_group _check_same_param_groups sharded_osd local_osd check_same_param_keys=True _check_same_state sharded_osd local_osd check_same_param_keys=True As sanity check check we can load run few iterations optim load_state_dict sharded_osd _step_model model optim num_iters=num_iters skip_if_lt_x_gpu test_interface_arguments model = FSDP TestDummyModel cuda optim = torch optim Adam model parameters lr= e- step loss = model model get_input loss backward loss optim step step original_osd = deepcopy optim state_dict osd = FSDP optim_state_dict model optim optim_state_dict=original_osd _check_same_state FSDP optim_state_dict model optim osd check_same_param_keys=True step FSDP optim_state_dict_to_load model optim osd load_directly=True _check_same_state optim state_dict original_osd check_same_param_keys=True Test default setting osd = FSDP optim_state_dict model optim optim_state_dict=original_osd state osd state values s state values assertFalse isinstance s ShardedTensor assertFalse s is_cuda Test sharded state_dict without offload_to_cpu FSDP state_dict_type model StateDictType SHARDED_STATE_DICT ShardedStateDictConfig ShardedOptimStateDictConfig offload_to_cpu=False osd = FSDP optim_state_dict model optim optim_state_dict=original_osd state osd state values s state values s dim == continue assertTrue isinstance s ShardedTensor s _local_shards assertTrue s _local_shards tensor is_cuda Test full state_dict rank _only FSDP state_dict_type model StateDictType FULL_STATE_DICT FullStateDictConfig FullOptimStateDictConfig offload_to_cpu=True rank _only=True osd = FSDP optim_state_dict model optim optim_state_dict=original_osd dist get_rank assertEqual osd state osd state values s state values s dim == continue assertFalse s is_cuda assertFalse isinstance s ShardedTensor skip_if_lt_x_gpu test_state_dict_with_none_tensor_state _run_test use_orig_params optimizer_has_tensor_state model = FSDP TestDummyModel cuda use_orig_params=use_orig_params optimizer_cls = torch optim Adam optimizer_has_tensor_state torch optim SGD optim = optimizer_cls model parameters lr= e- step loss = model model get_input loss backward loss optim step step original_osd = deepcopy optim state_dict state original_osd state values Add customized value state value = state value = None osd = FSDP optim_state_dict model optim optim_state_dict=original_osd osd_to_load = FSDP optim_state_dict_to_load model optim osd state osd_to_load state values assertEqual state value assertEqual state value None run_subtests use_orig_params False True optimizer_has_tensor_state False True _run_test skip_if_lt_x_gpu test_with_no_shard _run_test use_orig_params bool - None model = FSDP TestDummyModel cuda sharding_strategy=ShardingStrategy NO_SHARD use_orig_params=use_orig_params optim = torch optim Adam model parameters lr= e- step loss = model model get_input loss backward loss optim step step original_osd = deepcopy optim state_dict osd = FSDP optim_state_dict model optim osd_to_load = FSDP optim_state_dict_to_load model optim osd optim load_state_dict osd_to_load new_osd = optim state_dict assertEqual original_osd new_osd run_subtests use_orig_params False True _run_test skip_if_lt_x_gpu test_no_grad model = TestDummyModel no_grad=True cuda fsdp_model = FSDP deepcopy model use_orig_params=True fsdp_optim = torch optim Adam fsdp_model parameters lr= e- i range i == fsdp_model net weight requires_grad = True fsdp_model net bias requires_grad = True fsdp_model net weight requires_grad = False fsdp_model net bias requires_grad = False batch = fsdp_model get_input loss = fsdp_model batch sum loss backward fsdp_optim step orig_state_dict = deepcopy fsdp_optim state_dict FSDP optim_state_dict fsdp_model fsdp_optim FSDP optim_state_dict_to_load fsdp_model fsdp_optim FSDP optim_state_dict fsdp_model fsdp_optim load_directly=True _check_same_state fsdp_optim state_dict orig_state_dict check_same_param_keys=True instantiate_parametrized_tests TestFSDPOptimState __name__ == __main__ run_tests