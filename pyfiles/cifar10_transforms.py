usr bin env python Copyright c Facebook Inc its affiliates All Rights Reserved Runs CIFAR training differential privacy argparse logging shutil sys datetime datetime timedelta numpy np torchvision models transforms torchvision datasets CIFAR tqdm tqdm torch torch nn nn torch optim optim torch utils data torch func functional_call grad_and_value vmap logging basicConfig format= asctime s levelname s message s datefmt= m d Y H M S stream=sys stdout logger = logging getLogger ddp logger setLevel level=logging INFO save_checkpoint state is_best filename= checkpoint tar torch save state filename is_best shutil copyfile filename model_best pth tar accuracy preds labels preds == labels mean compute_norms sample_grads batch_size = sample_grads shape norms = sample_grad view batch_size - norm dim=- sample_grad sample_grads norms = torch stack norms dim= norm dim= norms batch_size clip_and_accumulate_and_add_noise model max_per_sample_grad_norm= noise_multiplier= sample_grads = tuple param grad_sample param model parameters step compute norms sample_norms batch_size = compute_norms sample_grads step compute clipping factors clip_factor = max_per_sample_grad_norm sample_norms + e- clip_factor = clip_factor clamp max= step clip grads = tuple torch einsum i i clip_factor sample_grad sample_grad sample_grads step add gaussian noise stddev = max_per_sample_grad_norm noise_multiplier noises = tuple torch normal stddev grad_param shape device=grad_param device grad_param grads grads = tuple noise + grad_param noise grad_param zip noises grads step assign new grads delete sample grads param param_grad zip model parameters grads param grad = param_grad batch_size del param grad_sample train args model train_loader optimizer epoch device start_time = datetime now criterion = nn CrossEntropyLoss losses = top _acc = i images target enumerate tqdm train_loader images = images device target = target device Step compute per-sample-grads To use vmap+grad compute per-sample-grads forward pass must re-formulated single example We use ` grad ` operator compute forward+backward single example finally ` vmap ` do forward+backward multiple examples compute_loss_and_output weights image target images = image unsqueeze targets = target unsqueeze output = functional_call model weights images loss = criterion output targets loss output squeeze ` grad f ` functional API returns function ` f ` computes gradients running both forward backward pass We want extract some intermediate values computation i e loss output To extract loss we use ` grad_and_value ` API returns gradient weights w r t loss loss To extract output we use ` has_aux=True ` flag ` has_aux=True ` assumes ` f ` returns tuple two values where first differentiated second auxiliary value differentiated ` f ` returns gradient w r t loss loss auxiliary value grads_loss_output = grad_and_value compute_loss_and_output has_aux=True weights = dict model named_parameters detaching weights since we don t need track gradients outside transforms more performant detached_weights = k v detach k v weights items sample_grads sample_loss output = vmap grads_loss_output None detached_weights images target loss = sample_loss mean name grad_sample sample_grads items weights name grad_sample = grad_sample detach Step Clip per-sample-grads sum them form grads add noise clip_and_accumulate_and_add_noise model args max_per_sample_grad_norm args sigma preds = np argmax output detach cpu numpy axis= labels = target detach cpu numpy losses append loss item measure accuracy record loss acc = accuracy preds labels top _acc append acc make sure we take step after processing last mini-batch epoch ensure we start next epoch clean state optimizer step optimizer zero_grad i args print_freq == print f \tTrain Epoch epoch \t f Loss np mean losses f f Acc np mean top _acc f train_duration = datetime now - start_time train_duration test args model test_loader device model eval criterion = nn CrossEntropyLoss losses = top _acc = torch no_grad images target tqdm test_loader images = images device target = target device output = model images loss = criterion output target preds = np argmax output detach cpu numpy axis= labels = target detach cpu numpy acc = accuracy preds labels losses append loss item top _acc append acc top _avg = np mean top _acc print f \tTest set Loss np mean losses f Acc top _avg f np mean top _acc flake noqa C main args = parse_args args debug = logger setLevel level=logging DEBUG device = args device args secure_rng try torchcsprng prng except ImportError e msg = To use secure RNG you must install torchcsprng package Check out instructions here https github com pytorch csprng#installation raise ImportError msg e generator = prng create_random_device_generator dev urandom generator = None normalize = transforms ToTensor transforms Normalize train_transform = transforms Compose normalize test_transform = transforms Compose normalize train_dataset = CIFAR root=args data_root train=True download=True transform=train_transform train_loader = torch utils data DataLoader train_dataset batch_size=int args sample_rate len train_dataset generator=generator num_workers=args workers pin_memory=True test_dataset = CIFAR root=args data_root train=False download=True transform=test_transform test_loader = torch utils data DataLoader test_dataset batch_size=args batch_size_test shuffle=False num_workers=args workers best_acc = model = models __dict__ args architecture pretrained=False norm_layer= lambda c nn GroupNorm args gn_groups c model = model device args optim == SGD optimizer = optim SGD model parameters lr=args lr momentum=args momentum weight_decay=args weight_decay args optim == RMSprop optimizer = optim RMSprop model parameters lr=args lr args optim == Adam optimizer = optim Adam model parameters lr=args lr raise NotImplementedError Optimizer recognized Please check spelling Store some logs accuracy_per_epoch = time_per_epoch = epoch range args start_epoch args epochs + args lr_schedule == cos lr = args lr + np cos np pi epoch args epochs + param_group optimizer param_groups param_group lr = lr train_duration = train args model train_loader optimizer epoch device top _acc = test args model test_loader device remember best acc save checkpoint is_best = top _acc best_acc best_acc = max top _acc best_acc time_per_epoch append train_duration accuracy_per_epoch append float top _acc save_checkpoint epoch epoch + arch Convnet state_dict model state_dict best_acc best_acc optimizer optimizer state_dict is_best filename=args checkpoint_file + tar time_per_epoch_seconds = t total_seconds t time_per_epoch avg_time_per_epoch = sum time_per_epoch_seconds len time_per_epoch_seconds metrics = accuracy best_acc accuracy_per_epoch accuracy_per_epoch avg_time_per_epoch_str str timedelta seconds=int avg_time_per_epoch time_per_epoch time_per_epoch_seconds logger info \nNote \n- total_time includes data loading time training time testing time \n- time_per_epoch measures training time only \n logger info metrics parse_args parser = argparse ArgumentParser description= PyTorch CIFAR DP Training parser add_argument -j -- workers default= type=int metavar= N help= number data loading workers default parser add_argument -- epochs default= type=int metavar= N help= number total epochs run parser add_argument -- start-epoch default= type=int metavar= N help= manual epoch number useful restarts parser add_argument -b -- batch-size-test default= type=int metavar= N help= mini-batch size test dataset default parser add_argument -- sample-rate default= type=float metavar= SR help= sample rate used batch construction default parser add_argument -- lr -- learning-rate default= type=float metavar= LR help= initial learning rate dest= lr parser add_argument -- momentum default= type=float metavar= M help= SGD momentum parser add_argument -- wd -- weight-decay default= type=float metavar= W help= SGD weight decay dest= weight_decay parser add_argument -p -- print-freq default= type=int metavar= N help= print frequency default parser add_argument -- resume default= type=str metavar= PATH help= path latest checkpoint default none parser add_argument -e -- evaluate dest= evaluate action= store_true help= evaluate model validation set parser add_argument -- seed default=None type=int help= seed initializing training parser add_argument -- sigma type=float default= metavar= S help= Noise multiplier default parser add_argument -c -- max-per-sample-grad_norm type=float default= metavar= C help= Clip per-sample gradients norm default parser add_argument -- secure-rng action= store_true default=False help= Enable Secure RNG have trustworthy privacy guarantees Comes performance cost Opacus will emit warning secure rng off indicating production use s recommender turn parser add_argument -- delta type=float default= e- metavar= D help= Target delta default e- parser add_argument -- checkpoint-file type=str default= checkpoint help= path save check points parser add_argument -- data-root type=str default= cifar help= Where CIFAR will stored parser add_argument -- log-dir type=str default= tmp stat tensorboard help= Where Tensorboard log will stored parser add_argument -- optim type=str default= SGD help= Optimizer use Adam RMSprop SGD parser add_argument -- lr-schedule type=str choices= constant cos default= cos parser add_argument -- device type=str default= cpu help= Device which run code parser add_argument -- architecture type=str default= resnet help= model torchvision run parser add_argument -- gn-groups type=int default= help= Number groups GroupNorm parser add_argument -- clip-per-layer -- clip_per_layer action= store_true default=False help= Use static per-layer clipping same clipping threshold each layer Necessary DDP If ` False ` default uses flat clipping parser add_argument -- debug type=int default= help= debug level default parser parse_args __name__ == __main__ main