mypy allow-untyped-defs copy dataclasses itertools operator collections abc Callable typing Any Optional TYPE_CHECKING torch torch nn functional F torch ao quantization fx _decomposed quantized_decomposed_lib noqa F torch ao quantization pt e export_utils _WrapperModule torch ao quantization quantizer DerivedQuantizationSpec EdgeOrNode QuantizationSpecBase SharedQuantizationSpec torch fx Graph GraphModule Node torch fx subgraph_rewriter replace_pattern_with_filters ReplacedPatterns utils _get_aten_graph_module_for_pattern _is_bn_node _is_conv_or_conv_transpose_node _is_conv_transpose_fn fold_bn_weights_into_conv_node TYPE_CHECKING torch fx passes utils matcher_with_name_node_map_utils InternalMatch __all__ = type ignore var-annotated _get_quantized_conv_bn_example_inputs_kwargs is_per_channel bool has_bias bool bias_is_quantized bool is_cuda bool - dict str Any Optional example inputs quantized folded conv-bn patterns used convert expressed kwargs kwargs = Per tensor quantization uses literals represent scale zero point so there no need include them here kwargs is_per_channel kwargs weight_scale = torch tensor dtype=torch float kwargs weight_zero_point = torch tensor dtype=torch int has_bias bias_is_quantized kwargs bias_scale = torch tensor dtype=torch float kwargs bias_zero_point = torch tensor dtype=torch int has_bias kwargs conv_bias = torch randn is_cuda k v kwargs items isinstance v torch Tensor kwargs k = v cuda kwargs _get_conv_bn_pattern conv_fn Callable - Callable _conv_bn_pattern x torch Tensor conv_weight torch Tensor conv_bias torch Tensor bn_weight torch Tensor bn_bias torch Tensor bn_running_mean torch Tensor bn_running_var torch Tensor - torch Tensor x = conv_fn x conv_weight conv_bias x = F batch_norm x bn_running_mean bn_running_var bn_weight bn_bias training=True x _WrapperModule _conv_bn_pattern TODO merge ` no_conv_bias ` case _get_qat_conv_bn_pattern conv_fn Callable - Callable _qat_conv_bn_pattern x torch Tensor conv_weight torch Tensor conv_bias torch Tensor bn_weight torch Tensor bn_bias torch Tensor bn_running_mean torch Tensor bn_running_var torch Tensor - torch Tensor Approximated method fuse conv bn It requires only one forward pass conv_orig = conv scale_factor where scale_factor = bn weight running_std This based ` nniqat ConvBn d _forward_approximate ` TODO allow setting eps bn_eps = e- running_std = torch sqrt bn_running_var + bn_eps scale_factor = bn_weight running_std weight_shape = len conv_weight shape weight_in_channel_axis = _is_conv_transpose_fn conv_fn weight_shape weight_in_channel_axis = - bias_shape = len conv_weight shape bias_shape = - scaled_weight = conv_weight scale_factor reshape weight_shape zero_bias = torch zeros_like conv_bias dtype=x dtype x = conv_fn x scaled_weight zero_bias x = x scale_factor reshape bias_shape x = x + conv_bias reshape bias_shape x = F batch_norm x bn_running_mean bn_running_var bn_weight bn_bias training=True eps=bn_eps x _WrapperModule _qat_conv_bn_pattern _get_qat_conv_bn_pattern_no_conv_bias conv_fn Callable - Callable _qat_conv_bn_pattern_no_conv_bias x torch Tensor conv_weight torch Tensor Not used only matching convenience conv_bias torch Tensor bn_weight torch Tensor bn_bias torch Tensor bn_running_mean torch Tensor bn_running_var torch Tensor - torch Tensor Same ` _get_qat_conv_bn_pattern ` handles case no conv bias TODO allow setting eps bn_eps = e- running_std = torch sqrt bn_running_var + bn_eps scale_factor = bn_weight running_std weight_shape = len conv_weight shape weight_in_channel_axis = _is_conv_transpose_fn conv_fn weight_shape weight_in_channel_axis = - bias_shape = len conv_weight shape bias_shape = - scaled_weight = conv_weight scale_factor reshape weight_shape x = conv_fn x scaled_weight None x = x scale_factor reshape bias_shape x = F batch_norm x bn_running_mean bn_running_var bn_weight bn_bias training=True eps=bn_eps x _WrapperModule _qat_conv_bn_pattern_no_conv_bias _append_qdq x is_per_channel is_bias kwargs Helper function append q-dq ops after ` x ` using dummy values qparams qmin qmax We use dummy values here because we match ` ignore_literals=True ` will manually replace these values after subgraph rewriting Return dq node Dummy args passed into q-dq ops per_channel_axis = scale_key = bias_scale is_bias weight_scale zp_key = bias_zero_point is_bias weight_zero_point scale = kwargs scale_key is_per_channel zp = kwargs zp_key is_per_channel qmin = - qmax = dtype = torch int qd = torch ops quantized_decomposed is_per_channel x = qd quantize_per_channel x scale zp per_channel_axis qmin qmax dtype x = qd dequantize_per_channel x scale zp per_channel_axis qmin qmax dtype x = qd quantize_per_tensor x scale zp qmin qmax dtype x = qd dequantize_per_tensor x scale zp qmin qmax dtype x _get_quantized_qat_conv_bn_pattern is_per_channel bool has_bias bool bias_is_quantized bool conv_fn Callable bn_is_training bool - Callable Return quantized version QAT conv + BN pattern This based ` nniqat ConvBn d _forward_approximate ` used QAT convert We first match pattern replace normal conv - bn pattern then fold BN weights into conv TODO allow setting eps bn_eps = e- _quantized_qat_conv_bn_pattern x torch Tensor conv_weight torch Tensor bn_weight torch Tensor bn_bias torch Tensor bn_running_mean torch Tensor bn_running_var torch Tensor kwargs - torch Tensor running_std = torch sqrt bn_running_var + bn_eps scale_factor = bn_weight running_std weight_shape = len conv_weight shape weight_shape = - bias_shape = len conv_weight shape bias_shape = - scaled_weight = conv_weight scale_factor reshape weight_shape scaled_weight = _append_qdq scaled_weight is_per_channel is_bias=False kwargs=kwargs has_bias zero_bias = torch zeros_like kwargs conv_bias dtype=x dtype bias_is_quantized zero_bias = _append_qdq zero_bias is_per_channel is_bias=True kwargs=kwargs x = conv_fn x scaled_weight zero_bias x = conv_fn x scaled_weight None x = x scale_factor reshape bias_shape has_bias x = x + kwargs conv_bias reshape bias_shape x = F batch_norm x bn_running_mean bn_running_var bn_weight bn_bias training=bn_is_training eps=bn_eps x _WrapperModule _quantized_qat_conv_bn_pattern _get_folded_quantized_qat_conv_bn_pattern is_per_channel bool has_bias bool bias_is_quantized bool conv_fn Callable bn_is_training bool - Callable Quantized QAT conv - bn pattern bn weights being folded into conv TODO allow setting eps bn_eps = e- _folded_quantized_qat_conv_bn_pattern x torch Tensor conv_weight torch Tensor bn_weight torch Tensor bn_bias torch Tensor bn_running_mean torch Tensor bn_running_var torch Tensor kwargs - torch Tensor conv_weight = _append_qdq conv_weight is_per_channel is_bias=False kwargs=kwargs has_bias bias = kwargs conv_bias bias_is_quantized bias = _append_qdq bias is_per_channel is_bias=True kwargs=kwargs bias = None x = conv_fn x conv_weight bias x = F batch_norm x bn_running_mean bn_running_var bn_weight bn_bias training=bn_is_training eps=bn_eps x _WrapperModule _folded_quantized_qat_conv_bn_pattern _has_conv_bias_filter match InternalMatch original_graph Graph pattern_graph Graph - bool Match filter subgraph rewriter returns True conv node original graph has bias n match nodes_map values _is_conv_or_conv_transpose_node n len n args n args None raise ValueError Could find conv node matched conv + bn pattern _no_conv_bias_filter match InternalMatch original_graph Graph pattern_graph Graph - bool Match filter subgraph rewriter returns True conv node original graph does NOT have bias _has_conv_bias_filter match original_graph pattern_graph _is_quantize n Node - bool n target torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed quantize_per_channel default _is_dequantize n Node - bool n target torch ops quantized_decomposed dequantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_channel default _get_conv_bn_pattern_nodes r ReplacedPatterns - dict str tuple Node Node Helper function extract nodes conv-bn fusion pattern after subgraph rewriting form map name original_node replacement_node The following names must exist map conv conv_weight conv_input bn getitem The following names may exist map conv_weight_q conv_weight_dq conv_bias conv_bias_q conv_bias_dq _get_nodes nodes list Node - tuple Node Node Optional Node Return -tuple conv_node bn_node getitem_node This asserts match contains exactly one each node conv_node bn_node getitem_node = None None None n nodes n op = call_function continue _is_conv_or_conv_transpose_node n conv_node None raise AssertionError f Found multiple conv nodes match previous conv_node new n conv_node = n _is_bn_node n bn_node None raise AssertionError f Found multiple bn nodes match previous bn_node new n bn_node = n n target operator getitem getitem_node None raise AssertionError f Found multiple getitem nodes match previous getitem_node new n getitem_node = n conv_node None raise AssertionError Expected exactly one conv node match found none bn_node None raise AssertionError Expected exactly one bn node match found none conv_node bn_node getitem_node _get_q_dq_nodes n Node - tuple Node Node Node Return -tuple orig_node q_node dq_node _is_dequantize n raise AssertionError f Expected dequantize node got n q_node = n args isinstance q_node Node raise AssertionError f Expected quantize node torch fx Node got type q_node _is_quantize q_node raise AssertionError f Expected q_node quantize node got target= q_node target orig_node = q_node args isinstance orig_node Node raise AssertionError f Expected original node torch fx Node got type orig_node orig_node q_node n original_nodes = list _filter_nodes_map r nodes_map values o_conv o_bn o_getitem = _get_nodes original_nodes r_conv r_bn r_getitem = _get_nodes r replacements Create mapping original node replacement node o_getitem None raise AssertionError f Expected o_getitem None got o_getitem r_getitem None raise AssertionError f Expected r_getitem None got r_getitem mapping = conv o_conv r_conv bn o_bn r_bn Extract conv input weight Note here we extract original nodes indirectly through pattern nodes because args original nodes no longer available after replacement p_conv _ _ = _get_nodes list r nodes_map keys p_conv_input p_conv_weight _ = p_conv args r_conv_input r_conv_weight _ = r_conv args isinstance p_conv_input Node raise AssertionError f Expected p_conv_input Node got type p_conv_input isinstance p_conv_weight Node raise AssertionError f Expected p_conv_weight Node got type p_conv_weight isinstance r_conv_input Node raise AssertionError f Expected r_conv_input Node got type r_conv_input isinstance r_conv_weight Node raise AssertionError f Expected r_conv_weight Node got type r_conv_weight o_conv_input = r nodes_map p_conv_input o_conv_weight = r nodes_map p_conv_weight If conv weight quantized extract q - dq nodes _is_dequantize p_conv_weight p_conv_weight p_conv_weight_q p_conv_weight_dq = _get_q_dq_nodes p_conv_weight r_conv_weight r_conv_weight_q r_conv_weight_dq = _get_q_dq_nodes r_conv_weight o_conv_weight = r nodes_map p_conv_weight o_conv_weight_q = r nodes_map p_conv_weight_q o_conv_weight_dq = r nodes_map p_conv_weight_dq mapping conv_weight_q = o_conv_weight_q r_conv_weight_q mapping conv_weight_dq = o_conv_weight_dq r_conv_weight_dq mapping conv_input = o_conv_input r_conv_input mapping conv_weight = o_conv_weight r_conv_weight Extract conv bias len p_conv args len r_conv args p_conv_bias = p_conv args r_conv_bias = r_conv args isinstance p_conv_bias Node raise AssertionError f Expected p_conv_bias Node got type p_conv_bias isinstance r_conv_bias Node raise AssertionError f Expected r_conv_bias Node got type r_conv_bias o_conv_bias = r nodes_map p_conv_bias If conv bias quantized extract q - dq nodes _is_dequantize p_conv_bias p_conv_bias p_conv_bias_q p_conv_bias_dq = _get_q_dq_nodes p_conv_bias r_conv_bias r_conv_bias_q r_conv_bias_dq = _get_q_dq_nodes r_conv_bias o_conv_bias = r nodes_map p_conv_bias o_conv_bias_q = r nodes_map p_conv_bias_q o_conv_bias_dq = r nodes_map p_conv_bias_dq mapping conv_bias_q = o_conv_bias_q r_conv_bias_q mapping conv_bias_dq = o_conv_bias_dq r_conv_bias_dq mapping conv_bias = o_conv_bias r_conv_bias mapping _filter_nodes_map nodes_map dict Node Node - dict Node Node Return filtered ` nodes_map ` returned subgraph rewriter The filtered ` nodes_map ` will contain only nodes actually matched pattern excluding None placeholder nodes new_nodes_map dict Node Node = pattern_node graph_node nodes_map items bias can None graph_node None continue skip pattern placeholder nodes pattern_node op == placeholder continue new_nodes_map pattern_node = graph_node new_nodes_map TODO error prone use replace_literals_with_placeholders hack instead _copy_over_literal_conv_args original_node Node new_node Node Copy over literal args conv such stride padding matched node original graph its replacement new graph This needed due following limitation subgraph rewriter when used dynamo export literal non-tensor args supported match replacement patterns This because dynamo export automatically inlines these literal args making them dead placeholder nodes In future we should check dynamo export can optionally disable inlining subgraph rewriter can do copying us See https github com pytorch pytorch issues Note Unlike other tensor args like conv weights biases literal args preserved original nodes after replacement so we can access them here _is_conv_or_conv_transpose_node original_node raise AssertionError f Expected original_node conv node got original_node _is_conv_or_conv_transpose_node new_node raise AssertionError f Expected new_node conv node got new_node x weight bias stride padding dilation transposed output_padding groups new_args = list new_node args len new_args bias optional when present means None new_args append None new_node args = tuple new_args + original_node args _update_conv_input_qspec_map_after_replacement original_node Node replacement_node Node Update ` input_qspec_map ` annotation after subgraph rewriting The original annotation referred nodes original graph so keys ` input_qspec_map ` will need updated reflect corresponding nodes replacement graph _is_conv_or_conv_transpose_node original_node raise AssertionError f Expected original_node conv node got original_node _is_conv_or_conv_transpose_node replacement_node raise AssertionError f Expected replacement_node conv node got replacement_node quantization_annotation original_node meta original_input_qspec_map = original_node meta quantization_annotation input_qspec_map input_qspec_map = get list configs should ordered input weight bias note really hacky we need better solution hopefully subgraph_rewriter issue tracking problem https github com pytorch pytorch issues all_configs = list original_input_qspec_map items input activation input_qspec_map replacement_node args = all_configs weight input_qspec_map replacement_node args = all_configs bias len replacement_node args len all_configs input_qspec_map replacement_node args = all_configs replacement_node meta quantization_annotation input_qspec_map = input_qspec_map _update_special_qspecs_after_replacement node Node original_to_replacement_node dict Node Node Update ` SharedQuantizationSpec ` s ` DerivedQuantizationSpec ` s used ` node ` s quantization annotation after subgraph rewriting The original annotation referred nodes original graph so nodes used these special quantization specs will need updated corresponding nodes replacement graph _get_new_edge_or_node edge_or_node EdgeOrNode isinstance edge_or_node Node _node = edge_or_node original_to_replacement_node get _node _node isinstance edge_or_node tuple len edge_or_node == all isinstance x Node x edge_or_node src dest = edge_or_node original_to_replacement_node get src src original_to_replacement_node get dest dest raise ValueError unexpected type edge_or_node type edge_or_node _get_new_qspec qspec QuantizationSpecBase isinstance qspec SharedQuantizationSpec new_edge_or_node = _get_new_edge_or_node qspec edge_or_node SharedQuantizationSpec new_edge_or_node isinstance qspec DerivedQuantizationSpec new_derived_from = _get_new_edge_or_node x x qspec derived_from dataclasses replace qspec derived_from=new_derived_from qspec quantization_annotation node meta annotation = node meta quantization_annotation input_node qspec annotation input_qspec_map items annotation input_qspec_map input_node = _get_new_qspec qspec annotation output_qspec = _get_new_qspec annotation output_qspec _fuse_conv_bn_qat m GraphModule - GraphModule Example inputs conv-bn d patterns _conv d_bn_example_inputs = torch randn x torch randn conv_weight torch randn conv_bias torch randn bn_weight torch randn bn_bias torch randn bn_running_mean torch randn bn_running_var Example inputs conv-bn d patterns _conv d_bn_example_inputs = torch randn x torch randn conv_weight torch randn conv_bias torch randn bn_weight torch randn bn_bias torch randn bn_running_mean torch randn bn_running_var has_bn = any _is_bn_node n n m graph nodes has_bn m is_cuda_options = True False torch cuda is_available False is_cuda is_cuda_options m = _fuse_conv_bn_qat_helper m F conv d _conv d_bn_example_inputs is_cuda=is_cuda m = _fuse_conv_bn_qat_helper m F conv d _conv d_bn_example_inputs is_cuda=is_cuda m = _fuse_conv_bn_qat_helper m F conv_transpose d _conv d_bn_example_inputs is_cuda=is_cuda m = _fuse_conv_bn_qat_helper m F conv_transpose d _conv d_bn_example_inputs is_cuda=is_cuda m _fuse_conv_bn_qat_helper m GraphModule conv_fn Callable example_inputs tuple Any is_cuda bool - GraphModule Given graph decomposed aten ops replace conv + bn pattern fused QAT subgraph equivalent The input graph should already annotated The annotations original nodes will preserved corresponding nodes new subgraph Note This also handles conv + bn + relu pattern m graph eliminate_dead_code m recompile conv_bn_pattern = _get_conv_bn_pattern conv_fn match_pattern = _get_aten_graph_module_for_pattern conv_bn_pattern example_inputs is_cuda Step Replace patterns conv bias Here we do replacement separately cases without conv bias since replacement patterns these two cases substantially different TODO use public replace_pattern API once also returns replacement nodes qat_conv_bn_pattern = _get_qat_conv_bn_pattern conv_fn replacement_pattern_with_conv_bias = _get_aten_graph_module_for_pattern qat_conv_bn_pattern example_inputs is_cuda replacements_with_conv_bias = replace_pattern_with_filters m match_pattern replacement_pattern_with_conv_bias match_filters= _has_conv_bias_filter ignore_literals=True m recompile Step Replace patterns without conv bias qat_conv_bn_pattern_no_conv_bias = _get_qat_conv_bn_pattern_no_conv_bias conv_fn replacement_pattern_no_conv_bias = _get_aten_graph_module_for_pattern qat_conv_bn_pattern_no_conv_bias example_inputs is_cuda replacements_no_conv_bias = replace_pattern_with_filters m match_pattern replacement_pattern_no_conv_bias match_filters= _no_conv_bias_filter ignore_literals=True m recompile Step Post processing Due limited functionality subgraph rewriter here we manually update replacement graph follows Copy over metadata original subgraph This ensures stack traces annotations preserved new subgraph b Copy over literal args conv original subgraph TODO do literal args batchnorm well c Update all references old nodes original subgraph refer corresponding nodes new subgraph annotations In future we should try push much functionality into subgraph rewriter possible so we don t have manually copy anything over For more detail see https github com pytorch pytorch issues all_original_to_replacement_nodes = r replacements_with_conv_bias + replacements_no_conv_bias replacement_dict = _get_conv_bn_pattern_nodes r The original conv node s nn_module_stack conv_nn_module = replacement_dict conv meta get nn_module_stack None k node_tuple replacement_dict items original_node replacement_node = node_tuple Step Copy over metadata all nodes conv - bn - getitem replacement_node meta = original_node meta If original_node get_attr node doesn t have nn_module_stack In case we copy nn_module_stack original conv node k conv_input conv_weight conv_nn_module nn_module_stack replacement_node meta replacement_node meta nn_module_stack = copy deepcopy conv_nn_module _is_conv_or_conv_transpose_node original_node Step b Copy over conv literal args _copy_over_literal_conv_args original_node replacement_node Step c Update old references conv node s input_qspec_map _update_conv_input_qspec_map_after_replacement original_node replacement_node all_original_to_replacement_nodes original_node = replacement_node Step c Update old references special qspecs all nodes graph n m graph nodes _update_special_qspecs_after_replacement n all_original_to_replacement_nodes m _duplicate_dequantize_node m GraphModule Helper function duplicate all dequantize nodes graph node has more than one user For example Before quantize - dequantize - \\ -- b \\ -- c After quantize - dequantize_ - \\ -- dequantize_ - b \\ -- dequantize_ - c This useful subgraph rewriting E g we wish match pattern dequantize - above subgraph matching would fail because dequantize node has users outside matched portion graph Instead we match dequantize_ - which safe dq_op = torch ops quantized_decomposed dequantize_per_tensor n m graph nodes n op = call_function n target = dq_op len n users == continue user list n users m graph inserting_before n new_node = m graph create_node call_function dq_op n args n kwargs user replace_input_with n new_node m graph erase_node n m recompile _remove_extra_dequantize m GraphModule Removes duplicate dequant nodes graph operator has multiple dequant nodes user Replace them single dequant node can shared across all uses This should seen reverse ` _duplicate_dequantize_node ` dq_op = torch ops quantized_decomposed dequantize_per_tensor n m graph nodes dq_users = user user n users user op == call_function user target == dq_op len dq_users m graph inserting_after dq_users new_node = m graph create_node call_function dq_op dq_users args dq_user dq_users dq_user replace_all_uses_with new_node m graph erase_node dq_user m recompile _copy_over_q_dq_args original_node Node replacement_node Node Given pair quantize dequantize nodes copy over all literal args original node replacement node For quantize_per_tensor scale zp literals need copied For quantize_per_channel scale zp get_attr nodes should skipped original_node target = replacement_node target raise AssertionError Expected original replacement nodes have same target got f original_node target = replacement_node target original_node target torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default Args input scale zp qmin qmax dtype start_copy_arg_index = original_node target torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default Args input scale zp axis qmin qmax dtype start_copy_arg_index = raise ValueError f Expected quantize dequantize nodes got original_node target replacement_node args = replacement_node args start_copy_arg_index + original_node args start_copy_arg_index _fold_conv_bn_qat m GraphModule - GraphModule Example inputs quantized folded conv-bn d patterns used convert _quantized_conv d_bn_example_inputs = torch randn x torch randn conv_weight torch randn bn_weight torch randn bn_bias torch randn bn_running_mean torch randn bn_running_var Example inputs quantized folded conv-bn d patterns used convert _quantized_conv d_bn_example_inputs = torch randn x torch randn conv_weight torch randn bn_weight torch randn bn_bias torch randn bn_running_mean torch randn bn_running_var has_bn = any _is_bn_node n n m graph nodes has_bn m is_cuda_options = True False torch cuda is_available False is_cuda is_cuda_options m = _fold_conv_bn_qat_helper m F conv d _quantized_conv d_bn_example_inputs is_cuda=is_cuda m = _fold_conv_bn_qat_helper m F conv d _quantized_conv d_bn_example_inputs is_cuda=is_cuda m = _fold_conv_bn_qat_helper m F conv_transpose d _quantized_conv d_bn_example_inputs is_cuda=is_cuda m = _fold_conv_bn_qat_helper m F conv_transpose d _quantized_conv d_bn_example_inputs is_cuda=is_cuda remove place add batchnorm tracking training stats node m graph nodes node target torch ops aten add_ Tensor node args op == get_attr node args == torch nn modules batchnorm BatchNorm d val val node meta source_fn_stack torch nn modules batchnorm BatchNorm d val val node meta source_fn_stack m graph erase_node node m graph eliminate_dead_code m recompile m _fold_conv_bn_qat_helper m GraphModule conv_fn Callable example_inputs tuple Any is_cuda bool - GraphModule Replace quantized conv + bn pattern conv bn weights folded into weights conv m graph eliminate_dead_code m recompile _duplicate_dequantize_node m Step Replace QAT pattern simple conv - bn pattern replacements = replacement_options = itertools product True False is_per_channel True False has_bias True False bias_is_quantized True False bn_is_training is_per_channel has_bias bias_is_quantized bn_is_training replacement_options For cases without bias ` bias_is_quantized ` irrelevant so here we arbitrarily filter out one values flag avoid having duplicate patterns has_bias bias_is_quantized continue kwargs = _get_quantized_conv_bn_example_inputs_kwargs is_per_channel has_bias bias_is_quantized is_cuda match_pattern = _get_quantized_qat_conv_bn_pattern is_per_channel has_bias bias_is_quantized conv_fn bn_is_training match_pattern = _get_aten_graph_module_for_pattern match_pattern example_inputs is_cuda kwargs replacement_pattern = _get_folded_quantized_qat_conv_bn_pattern is_per_channel has_bias bias_is_quantized conv_fn bn_is_training replacement_pattern = _get_aten_graph_module_for_pattern replacement_pattern example_inputs is_cuda kwargs replacements extend replace_pattern_with_filters m match_pattern replacement_pattern ignore_literals=True m recompile _remove_extra_dequantize m r replacements node_map = _get_conv_bn_pattern_nodes r Step Copy over metadata original subgraph original_node replacement_node node_map values replacement_node meta = original_node meta Step Copy over args weight optionally bias q - dq nodes _copy_over_q_dq_args node_map conv_weight_q _copy_over_q_dq_args node_map conv_weight_dq conv_bias_q node_map conv_bias_dq node_map raise AssertionError Expected conv_bias_dq present node_map when conv_bias_q present _copy_over_q_dq_args node_map conv_bias_q _copy_over_q_dq_args node_map conv_bias_dq Step Fold BN weights into conv conv_bias = None _ conv_node = node_map conv _ bn_node = node_map bn _ conv_weight = node_map conv_weight conv_bias node_map _ conv_bias = node_map conv_bias fold_bn_weights_into_conv_node conv_node conv_weight conv_bias bn_node m Copy over literal args conv original_node _filter_nodes_map r nodes_map values _is_conv_or_conv_transpose_node original_node _copy_over_literal_conv_args original_node conv_node m graph eliminate_dead_code m recompile m