mypy allow-untyped-defs __future__ annotations itertools logging weakref typing Any Optional torch torch utils _pytree pytree torch _dynamo utils dynamo_timed lazy_format_graph_code torch _functorch aot_autograd MutationType torch _functorch compile_utils fx_graph_cse torch _inductor constant_folding constant_fold replace_node_with_constant torch _inductor freezing_utils enter_freezing record_has_frozen_params torch _inductor fx_passes freezing_patterns freezing_passes torch _inductor fx_passes post_grad view_to_reshape config aten = torch ops aten prims = torch ops prims log = logging getLogger __name__ replace_params_with_constants gm torch fx GraphModule flat_params list Any fw_metadata torch _functorch aot_autograd ViewAndMutationMeta - list int Replaces parameters PyTorch GraphModule constants wherever possible Returns list indices representing input parameters converted constants params = gm graph find_nodes op= placeholder fake_inp_nodes = params len params preserved_arg_indices = aliased_input_args = out_info base_idx out_info fw_metadata output_info out_info base_idx None TODO tmanlaibaatar figure out why different mutated_inp_runtime_indices mutated_inps = i i m enumerate fw_metadata input_info m mutation_type MutationType MUTATED_IN_GRAPH MutationType MUTATED_OUT_GRAPH static_indices_new = static_indices_offset = i real_input node enumerate zip flat_params fake_inp_nodes i mutated_inps i aliased_input_args preserved_arg_indices append i i fw_metadata static_input_indices new_static_index = i - static_indices_offset static_indices_new append new_static_index replace_node_with_constant gm node real_input static_indices_offset += add non param inputs preserved_arg_indices extend range len flat_params len params necessary fw_metadata static_input_indices = static_indices_new gm recompile preserved_arg_indices freeze dynamo_gm torch fx GraphModule aot_autograd_gm torch fx GraphModule example_inputs list torch _subclasses FakeTensor - tuple torch fx GraphModule list int Inlines parameters mutated into constants optimizes graph through constant propagation other techniques If enabled function also discards original parameters module memory efficiency Assumes function run dynamo tracing post aot_autograd Args dynamo_gm torch fx GraphModule The Dynamo constructed GraphModule aot_autograd_gm torch fx GraphModule The aot_autograd constructed GraphModule frozen example_inputs List torch Tensor A list example input tensors used freezing process Returns Tuple torch fx GraphModule List int A tuple containing frozen GraphModule list indices inputs preserved turned into constants enter_freezing _freeze dynamo_gm aot_autograd_gm example_inputs _freeze dynamo_gm torch fx GraphModule aot_autograd_gm torch fx GraphModule example_inputs list torch _subclasses FakeTensor - tuple torch fx GraphModule list int We have convert conv s weight channels last which may meet error view when doing fake_tensor_prop So we need convert view reshape first See details fx_codegen_and_compile compile_fx py view_to_reshape aot_autograd_gm tracing_context = torch _guards TracingContext try_get fw_metadata = tracing_context fw_metadata assert tracing_context params_flat_unwrap_subclasses None params_flat = tracing_context params_flat_unwrap_subclasses assert fw_metadata None params_flat None preserved_arg_indices = replace_params_with_constants aot_autograd_gm params_flat fw_metadata inputs = aot_autograd_gm graph find_nodes op= placeholder preserved_arg_indices = list range len inputs TODO - further restrict cse right now needed dedup aliasing ops cse_graph = fx_graph_cse aot_autograd_gm graph aot_autograd_gm graph = cse_graph aot_autograd_gm recompile aot_example_inputs = example_inputs ind ind preserved_arg_indices freezing_passes aot_autograd_gm aot_example_inputs constant_fold aot_autograd_gm invalidate nn Modules config freezing_discard_parameters invalidate_eager_modules discard_traced_gm_params dynamo_gm log debug s lazy_format_graph_code FROZEN GRAPH aot_autograd_gm colored=True record_has_frozen_params aot_autograd_gm aot_autograd_gm preserved_arg_indices ErasedTensor torch Tensor staticmethod __new__ cls elem name owning_mod super __new__ cls elem device= meta __init__ elem name Optional str mod - None erased_name = name owning_mod_ref = weakref ref mod classmethod __torch_dispatch__ cls func types args= kwargs=None type ignore override erased_tensors = e pyrefly ignore bad-unpacking e pytree arg_tree_leaves args kwargs isinstance e ErasedTensor assert len erased_tensors e = erased_tensors raise RuntimeError f Trying run Pytorch Eager Module after Dynamo Freezing The original parameters have been discarded memory efficiency f Found op func erased parameter e erased_name e owning_mod_ref invalidate_eager_modules torch utils _python_dispatch _disable_current_modes mod torch _guards TracingContext get module_context nn_modules values isinstance mod torch nn Module continue attr_name tensor list itertools chain mod named_parameters recurse=False pyrefly ignore bad-argument-type mod named_buffers recurse=False torch _dispatch python no_python_dispatcher e_t = ErasedTensor tensor attr_name mod isinstance tensor torch nn Parameter e_t requires_grad_ True e_t _is_param = True setattr mod attr_name e_t discard_traced_gm_params mod torch fx GraphModule torch utils _python_dispatch _disable_current_modes attr_name tensor list itertools chain mod named_parameters recurse=False pyrefly ignore bad-argument-type mod named_buffers recurse=False torch _dispatch python no_python_dispatcher e_t = ErasedTensor tensor attr_name mod isinstance tensor torch nn Parameter e_t requires_grad_ True e_t _is_param = True setattr mod attr_name e_t enforce_output_layout gm torch fx GraphModule Make sure output node s layout does change due compiler optimizations adding aten as_strided nodes expected strides Only used inference so we can assume all graph outputs model outputs _ output_node = gm graph nodes out_list = output_node args gm graph inserting_before output_node n out_list isinstance n meta val torch Tensor torch _prims_common is_non_overlapping_and_dense n meta val continue add node enforce eager layout ft = n meta val new_node = gm graph call_function prims inductor_force_stride_order default n ft stride can call n replace_all_uses_with new_node since will replace usage n new_node itself output_node replace_input_with n new_node gm graph lint gm recompile enforce_as_strided_input_layout gm torch fx GraphModule Make sure as_strided node s input s layout does change due compiler optimizations because as_strided strides info depends input tensor stride info as_strided_ops = torch ops aten as_strided default torch ops aten as_strided_ default torch ops aten as_strided_scatter default strided_nodes = n n gm graph nodes n target as_strided_ops n strided_nodes gm graph inserting_before n add node enforce eager layout ft = n args meta val new_node = gm graph call_function prims inductor_force_stride_order default n args ft stride n replace_input_with n args new_node gm graph lint gm recompile convert_conv_weights_to_channels_last gm torch fx GraphModule Convert d convolution weight tensor channels last format This pass performed before freezing so added nodes can constant folded freezing dynamo_timed convert_conv_weights_to_channels_last convs = n n gm graph nodes n target aten convolution default conv convs weight_node = conv args len weight_node meta val size = weight_node meta val is_contiguous memory_format=torch channels_last d tensor already channels last skip continue gm graph inserting_before conv new_node = gm graph call_function aten clone default weight_node memory_format torch channels_last conv replace_input_with weight_node new_node enforce_as_strided_input_layout gm enforce_output_layout gm