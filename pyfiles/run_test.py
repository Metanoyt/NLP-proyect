usr bin env python argparse contextlib copy glob json os platform re shutil signal subprocess sys sysconfig tempfile time collections defaultdict collections abc Sequence contextlib ExitStack datetime datetime importlib metadata PackageNotFoundError version pathlib Path typing Any cast NamedTuple Optional Union torch torch distributed dist torch multiprocessing current_process get_context torch testing _internal common_utils get_report_path IS_CI IS_MACOS retry_shell set_cwd shell TEST_CUDA TEST_SAVE_XML TEST_WITH_ASAN TEST_WITH_ROCM TEST_WITH_SLOW_GRADCHECK TEST_XPU using tools optimize test run REPO_ROOT = Path __file__ resolve parent parent sys path insert str REPO_ROOT tools stats import_test_stats ADDITIONAL_CI_FILES_FOLDER TEST_CLASS_TIMES_FILE TEST_TIMES_FILE tools stats upload_metrics add_global_metric emit_metric tools testing discover_tests CPP_TEST_PATH CPP_TEST_PREFIX CPP_TESTS_DIR parse_test_module TESTS tools testing do_target_determination_for_s import_results tools testing target_determination gen_artifact gen_ci_artifact tools testing target_determination heuristics previously_failed_in_pr gen_additional_test_failures_file tools testing target_determination heuristics utils get_pr_number tools testing test_run TestRun tools testing test_selections calculate_shards get_test_case_configs NUM_PROCS ShardedTest THRESHOLD tools testing upload_artifacts zip_and_upload_artifacts Make sure remove REPO_ROOT after done sys path remove str REPO_ROOT HAVE_TEST_SELECTION_TOOLS = True TEST_CONFIG = os getenv TEST_CONFIG BUILD_ENVIRONMENT = os getenv BUILD_ENVIRONMENT RERUN_DISABLED_TESTS = os getenv PYTORCH_TEST_RERUN_DISABLED_TESTS == DISTRIBUTED_TEST_PREFIX = distributed INDUCTOR_TEST_PREFIX = inductor IS_SLOW = slow TEST_CONFIG slow BUILD_ENVIRONMENT IS_S X = platform machine == s x Note ROCm parallel CI testing https github com pytorch pytorch pull added file-granularity parallel testing In ci pytorch test sh TEST_CONFIG == default CUDA HIP_VISIBLE_DEVICES set This results multiple test files sharing same GPU This should supported use case ROCm exposed issues kernel driver resulting hangs See https github com pytorch pytorch issues Further ROCm self-hosted runners have up GPUs Device visibility set match CUDA test behavior wasting available GPU resources Assigning each Pool worker their own dedicated GPU avoids ROCm oversubscription issues This should also result better overall wall clock time since all GPUs can utilized maybe_set_hip_visible_devies Special handling ROCm GHA runners parallel file granularity tests torch version hip p = current_process p name = MainProcess Process parallel Pool MainProcess os environ HIP_VISIBLE_DEVICES = str p _identity NUM_PROCS strtobool s s lower false off TestChoices list __init__ args kwargs super __init__ args __contains__ item list __contains__ parse_test_module item FSDP_TEST = test test TESTS test startswith distributed fsdp WINDOWS_BLOCKLIST = distributed nn jit test_instantiator distributed rpc test_faulty_agent distributed rpc test_tensorpipe_agent distributed rpc test_share_memory distributed rpc cuda test_tensorpipe_agent distributed pipeline sync skip test_api distributed pipeline sync skip test_gpipe distributed pipeline sync skip test_inspect_skip_layout distributed pipeline sync skip test_leak distributed pipeline sync skip test_portal distributed pipeline sync skip test_stash_pop distributed pipeline sync skip test_tracker distributed pipeline sync skip test_verify_skippables distributed pipeline sync test_balance distributed pipeline sync test_bugs distributed pipeline sync test_checkpoint distributed pipeline sync test_copy distributed pipeline sync test_deferred_batch_norm distributed pipeline sync test_dependency distributed pipeline sync test_inplace distributed pipeline sync test_microbatch distributed pipeline sync test_phony distributed pipeline sync test_pipe distributed pipeline sync test_pipeline distributed pipeline sync test_stream distributed pipeline sync test_transparency distributed pipeline sync test_worker distributed elastic agent server test api_test distributed elastic multiprocessing api_test distributed _shard checkpoint test_checkpoint distributed _shard checkpoint test_file_system_checkpoint distributed _shard sharding_spec test_sharding_spec distributed _shard sharding_plan test_sharding_plan distributed _shard sharded_tensor test_sharded_tensor distributed _shard sharded_tensor test_sharded_tensor_reshard distributed _shard sharded_tensor ops test_embedding distributed _shard sharded_tensor ops test_embedding_bag distributed _shard sharded_tensor ops test_binary_cmp distributed _shard sharded_tensor ops test_init distributed _shard sharded_optim test_sharded_optim + FSDP_TEST ROCM_BLOCKLIST = distributed rpc test_faulty_agent distributed rpc test_tensorpipe_agent distributed rpc test_share_memory distributed rpc cuda test_tensorpipe_agent inductor test_max_autotune taking excessive time many tests min test_determination test_jit_legacy test_cuda_nvml_based_avail test_jit_cuda_fuser test_openreg S X_BLOCKLIST = these tests fail due various reasons dynamo test_misc inductor test_cpu_repro inductor test_cpu_select_algorithm inductor test_torchinductor_codegen_dynamic_shapes lazy test_meta_kernel onnx test_utility_funs profiler test_profiler test_jit dynamo test_utils test_nn these tests run long fail addition dynamo test_dynamic_shapes test_quantization inductor test_torchinductor inductor test_torchinductor_dynamic_shapes inductor test_torchinductor_opinfo these tests fail when cuda available inductor test_aot_inductor inductor test_best_config inductor test_cudacodecache inductor test_inductor_utils inductor test_inplacing_pass inductor test_kernel_benchmark inductor test_max_autotune inductor test_move_constructors_to_cuda inductor test_multi_kernel inductor test_pattern_matcher inductor test_perf inductor test_select_algorithm inductor test_snode_runtime inductor test_triton_wrapper these tests fail when mkldnn available inductor test_custom_post_grad_passes inductor test_mkldnn_pattern_matcher test_metal lacks quantization support onnx test_models_quantized_onnxruntime onnx test_pytorch_onnx_onnxruntime sysctl -n hw memsize available test_mps https github com pytorch pytorch issues test_decomp https github com pytorch pytorch issues test_model_exports_to_core_aten runs very long skip now inductor test_layout_optim test_fx some false errors doctests new failures investigate fix test_tensorboard onnx + protobuf failure see https github com protocolbuffers protobuf issues dynamo test_backends dynamo test_modules inductor test_config test_public_bindings test_testing depend z -solver fx test_z _gradual_types test_proxy_tensor test_openreg XPU_BLOCKLIST = test_autograd profiler test_cpp_thread profiler test_execution_trace profiler test_memory_profiler profiler test_profiler profiler test_profiler_tree profiler test_record_function profiler test_torch_tidy test_openreg XPU_TEST = test_xpu The tests inside these files should never run parallel each other RUN_PARALLEL_BLOCKLIST = test_extension_utils test_cpp_extensions_jit test_cpp_extensions_stream_and_event test_cpp_extensions_mtia_backend test_jit_disabled test_mobile_optimizer test_multiprocessing test_multiprocessing_spawn test_namedtuple_return_api test_openreg test_overrides test_show_pickle test_tensorexpr test_cuda_primary_ctx test_cuda_trace inductor test_benchmark_fusion test_cuda_nvml_based_avail temporarily sets global config test_autograd_fallback inductor test_compiler_bisector test_privateuseone_python_backend + FSDP_TEST Test files should always run serially other test files s okay tests inside them run parallel each other CI_SERIAL_LIST = test_nn test_fake_tensor test_cpp_api_parity test_reductions test_fx_backends test_cpp_extensions_jit test_torch test_tensor_creation_ops test_dispatch test_python_dispatch torch library creation deletion must serialized test_spectral_ops Cause CUDA illegal memory access https github com pytorch pytorch issues nn test_pooling nn test_convolution Doesn t respect set_per_process_memory_fraction results OOM other tests slow gradcheck distributions test_distributions test_fx gets SIGKILL functorch test_memory_efficient_fusion Cause CUDA OOM ROCm test_utils OOM test_sort_and_select OOM test_backward_compatible_arguments OOM test_autocast OOM test_native_mha OOM test_module_hooks OOM inductor test_max_autotune inductor test_cutlass_backend slow due many nvcc compilation steps inductor test_flex_attention OOM A subset onnx tests cannot run parallel due high memory usage ONNX_SERIAL_LIST = onnx test_models onnx test_models_quantized_onnxruntime onnx test_models_onnxruntime onnx test_custom_ops onnx test_utility_funs A subset our TEST list validates PyTorch s ops modules autograd function expected CORE_TEST_LIST = test_autograd test_autograd_fallback test_modules test_nn test_ops test_ops_gradients test_ops_fwd_gradients test_ops_jit test_torch test file takes longer than min we add TARGET_DET_LIST SLOW_TEST_THRESHOLD = DISTRIBUTED_TESTS_CONFIG = dist is_available num_gpus = torch cuda device_count DISTRIBUTED_TESTS_CONFIG test = WORLD_SIZE TEST_WITH_ROCM dist is_mpi_available DISTRIBUTED_TESTS_CONFIG mpi = WORLD_SIZE dist is_nccl_available num_gpus DISTRIBUTED_TESTS_CONFIG nccl = WORLD_SIZE f num_gpus dist is_gloo_available DISTRIBUTED_TESTS_CONFIG gloo = TODO retire testing gloo CUDA WORLD_SIZE f num_gpus num_gpus del num_gpus Test UCC backend deprecated See https github com pytorch pytorch pull dist is_ucc_available DISTRIBUTED_TESTS_CONFIG ucc = WORLD_SIZE f torch cuda device_count UCX_TLS tcp cuda UCC_TLS nccl ucp cuda UCC_TL_UCP_TUNE cuda don t use UCP TL CUDA well supported UCC_EC_CUDA_USE_COOPERATIVE_LAUNCH n CI nodes M fail https stackoverflow com questions get-signal-names-from-numbers-in-python SIGNALS_TO_NAMES_DICT = getattr signal n n n dir signal n startswith SIG _ n CPP_EXTENSIONS_ERROR = Ninja https ninja-build org required some C++ extensions tests could found Install ninja ` pip install ninja ` ` conda install ninja ` Alternatively disable said tests ` run_test py -- exclude test_cpp_extensions_aot_ninja test_cpp_extensions_jit ` PYTORCH_COLLECT_COVERAGE = bool os environ get PYTORCH_COLLECT_COVERAGE JIT_EXECUTOR_TESTS = test_jit_profiling test_jit_legacy test_jit_fuser_legacy INDUCTOR_TESTS = test test TESTS test startswith INDUCTOR_TEST_PREFIX DISTRIBUTED_TESTS = test test TESTS test startswith DISTRIBUTED_TEST_PREFIX TORCH_EXPORT_TESTS = test test TESTS test startswith export AOT_DISPATCH_TESTS = test test TESTS test startswith functorch test_aotdispatch FUNCTORCH_TESTS = test test TESTS test startswith functorch ONNX_TESTS = test test TESTS test startswith onnx QUANTIZATION_TESTS = test test TESTS test startswith test_quantization _is_cpp_test test Note tests underneath cpp_extensions different other cpp tests they utilize usual python test infrastructure test startswith CPP_TEST_PREFIX test startswith cpp_extensions CPP_TESTS = test test TESTS _is_cpp_test test TESTS_REQUIRING_LAPACK = distributions test_constraints distributions test_distributions These just slowest ones isn t exhaustive list TESTS_NOT_USING_GRADCHECK = Note you should use skipIfSlowGradcheckEnv you do wish skip all tests file e g test_mps doctests test_meta test_hub test_fx test_decomp test_cpp_extensions_jit test_jit test_matmul_cuda test_ops test_ops_jit dynamo test_recompile_ux inductor test_compiled_optimizers inductor test_cutlass_backend inductor test_max_autotune inductor test_select_algorithm inductor test_smoke test_quantization print_to_stderr message print message file=sys stderr get_executable_command options disable_coverage=False is_cpp_test=False options coverage disable_coverage is_cpp_test executable = coverage run -- parallel-mode -- source=torch TODO C++ coverage yet supported executable = is_cpp_test executable = sys executable -bb executable = pytest executable run_test test_module ShardedTest test_directory options launcher_cmd=None extra_unittest_args=None env=None print_log=True - int scribe_token = os getenv SCRIBE_GRAPHQL_ACCESS_TOKEN scribe_token print_to_stderr SCRIBE_GRAPHQL_ACCESS_TOKEN set print_to_stderr SCRIBE_GRAPHQL_ACCESS_TOKEN NOT set env = env os environ copy maybe_set_hip_visible_devies unittest_args = options additional_args copy test_file = test_module name stepcurrent_key = test_file is_distributed_test = test_file startswith DISTRIBUTED_TEST_PREFIX is_cpp_test = _is_cpp_test test_file NB Rerun disabled tests depends pytest-flakefinder doesn t work pytest-cpp atm We also don t have support disable C++ test yet so s ok just successfully here is_cpp_test RERUN_DISABLED_TESTS print_to_stderr Skipping C++ tests when running under RERUN_DISABLED_TESTS mode is_cpp_test stepcurrent_key = f test_file _ os urandom hex unittest_args extend f -- shard-id= test_module shard f -- num-shards= test_module num_shards stepcurrent_key = f test_file _ test_module shard _ os urandom hex options verbose unittest_args append f - v options verbose case pytest test_file RUN_PARALLEL_BLOCKLIST unittest_args = arg arg unittest_args arg startswith -- run-parallel extra_unittest_args assert isinstance extra_unittest_args list unittest_args extend extra_unittest_args If using pytest replace -f equivalent -x options pytest unittest_args extend get_pytest_args options is_cpp_test=is_cpp_test is_distributed_test=is_distributed_test unittest_args extend test_module get_pytest_args replacement = -f -x -dist=loadfile -- dist=loadfile unittest_args = replacement get arg arg arg unittest_args options showlocals options pytest unittest_args extend -- showlocals -- tb=long -- color=yes unittest_args append -- locals NB These features available C++ tests there little incentive implement because we have never seen flaky C++ test before IS_CI is_cpp_test ci_args = -- import-slow-tests -- import-disabled-tests RERUN_DISABLED_TESTS ci_args append -- rerun-disabled-tests use downloaded test cases configuration supported pytest unittest_args extend ci_args test_file PYTEST_SKIP_RETRIES options pytest raise RuntimeError A test running without pytest cannot skip retries using PYTEST_SKIP_RETRIES set unittest_args = arg arg unittest_args -- reruns arg Extra arguments supported pytest executable = get_executable_command options is_cpp_test=is_cpp_test executable If there no eligible executable returning here means unsupported case such coverage C++ test So just returning ok makes sense is_cpp_test C++ tests regular test directory CPP_TESTS_DIR cpp_test = os path join CPP_TESTS_DIR test_file replace f CPP_TEST_PREFIX cpp_test = os path join Path test_directory parent CPP_TEST_PATH test_file replace f CPP_TEST_PREFIX argv = cpp_test sys platform = win cpp_test + exe + unittest_args Can t call ` python -m unittest test_ ` here because doesn t run code ` __name__ == __main__ ` So call ` python test_ py ` instead argv = test_file + py + unittest_args os makedirs REPO_ROOT test test-reports exist_ok=True options pipe_logs log_fd log_path = tempfile mkstemp dir=REPO_ROOT test test-reports prefix=f sanitize_file_name str test_module _ suffix= _toprint log os close log_fd command = launcher_cmd + executable + argv should_retry = -- subprocess command RERUN_DISABLED_TESTS is_cpp_test -n command timeout = None options enable_timeout THRESHOLD IS_SLOW THRESHOLD should_retry isinstance test_module ShardedTest test_module time None THRESHOLD is_cpp_test None print_to_stderr f Executing command datetime now ExitStack stack output = None options pipe_logs output = stack enter_context open log_path w should_retry ret_code was_rerun = run_test_retries command test_directory env timeout stepcurrent_key output options continue_through_error test_file command extend f -- sc= stepcurrent_key -- print-items ret_code was_rerun = retry_shell command test_directory stdout=output stderr=output env=env timeout=timeout retries= Pytest code means no test collected Exit code returned when binary C++ test executable can also returned file fails before running any tests All binary files under build bin C++ test time writing have been excluded new ones should added list exclusions tools testing discover_tests py ret_code = ret_code == ret_code options pipe_logs print_log handle_log_file test_module log_path failed= ret_code = was_rerun=was_rerun ret_code install_cpp_extensions extensions_dir env=os environ Wipe build folder exists already build_dir = os path join extensions_dir build os path exists build_dir shutil rmtree build_dir Build test cpp extensions modules cmd = sys executable -m pip install -- no-build-isolation -- root install return_code = shell cmd cwd=extensions_dir env=env return_code = None return_code Get site-packages directory prepared PYTHONPATH platlib_path = sysconfig get_paths platlib platlib_rel = os path relpath platlib_path os path splitdrive platlib_path + os sep install_directory = os path join extensions_dir install platlib_rel assert install_directory install_directory must empty install_directory contextlib contextmanager extend_python_path install_directories python_path = os environ get PYTHONPATH try os environ PYTHONPATH = os pathsep join install_directories + python_path yield finally os environ PYTHONPATH = python_path try_set_cpp_stack_traces env command set=True Print full c++ stack traces during retries env = env env TORCH_SHOW_CPP_STACKTRACES = set env run_test_retries command test_directory env timeout stepcurrent_key output continue_through_error test_file Run test -x stop first failure Rerun test itself If succeeds move rest tests new process If still fails see below If continue through error set then we fail fast If continue through error set then we skip test keep going Basically same test fails times row skip test next run still fail end I take advantage value saved stepcurrent keep track most recently run test which one failed there failure print_to_file s print s file=output flush=True num_failures = defaultdict int print_items = -- print-items sc_command = f -- sc= stepcurrent_key while True ret_code _ = retry_shell command + sc_command + print_items test_directory stdout=output stderr=output env=env timeout=timeout retries= no retries here we do ourselves because handles timeout exceptions well ret_code = ret_code == ret_code ret_code == sc_command startswith -- rs= break Got end test suite successfully signal_name = f SIGNALS_TO_NAMES_DICT -ret_code ret_code print_to_file f Got exit code ret_code signal_name Read what just failed ran try open REPO_ROOT pytest_cache v cache stepcurrent stepcurrent_key f current_failure = f read current_failure == null current_failure = f test_file except FileNotFoundError print_to_file No stepcurrent file found Either pytest didn t get run e g error + file got deleted contact dev infra break env = try_set_cpp_stack_traces env command set=False ret_code = num_failures current_failure += ret_code == Rerunning previously failing test succeeded so now we can skip move sc_command = f -- scs= stepcurrent_key print_to_file Test succeeeded new process continuing rest tests num_failures current_failure = This log classifier so can prioritize consistently failing tests instead reruns - remove quotes print_to_file f FAILED CONSISTENTLY current_failure - continue_through_error print_to_file Stopping first consistent failure break sc_command = f -- scs= stepcurrent_key print_to_file Test failed consistently continuing rest tests due continue-through-error being set env = try_set_cpp_stack_traces env command set=True sc_command = f -- rs= stepcurrent_key print_to_file Retrying single test print_items = do continue printing them massive waste space consistent_failures = x - x num_failures keys num_failures x = flaky_failures = x - x num_failures keys num_failures x len flaky_failures print_to_file The following tests failed then succeeded when run new process + f flaky_failures len consistent_failures print_to_file f The following tests failed consistently consistent_failures True ret_code any x x num_failures values run_test_with_subprocess test_module test_directory options run_test test_module test_directory options extra_unittest_args= -- subprocess _test_cpp_extensions_aot test_directory options use_ninja use_ninja try torch utils cpp_extension cpp_extension verify_ninja_availability except RuntimeError print_to_stderr CPP_EXTENSIONS_ERROR Wipe build folder exists already cpp_extensions_test_dir = os path join test_directory cpp_extensions cpp_extensions_test_build_dir = os path join cpp_extensions_test_dir build os path exists cpp_extensions_test_build_dir shutil rmtree cpp_extensions_test_build_dir Build test cpp extensions modules shell_env = os environ copy shell_env USE_NINJA = str use_ninja install_cmd = sys executable -m pip install -- no-build-isolation -- root install wheel_cmd = sys executable -m build -- wheel -- no-isolation return_code = shell install_cmd cwd=cpp_extensions_test_dir env=shell_env return_code = return_code sys platform = win exts_to_build = install_cmd no_python_abi_suffix_test TEST_CUDA TEST_XPU exts_to_build append wheel_cmd python_agnostic_extension TEST_CUDA exts_to_build append install_cmd libtorch_agnostic_extension cmd extension_dir exts_to_build return_code = shell cmd cwd=os path join cpp_extensions_test_dir extension_dir env=shell_env return_code = return_code shutil copyfile os environ USE_NINJA = shell_env USE_NINJA test_module = test_cpp_extensions_aot + _ninja use_ninja _no_ninja copyfile test_directory + test_cpp_extensions_aot py test_directory + + test_module + py try cpp_extensions = os path join test_directory cpp_extensions install_directories = install directory one named site-packages root directories _ os walk os path join cpp_extensions install directory directories -packages directory install_directories append os path join root directory root directories _ os walk os path join cpp_extensions libtorch_agnostic_extension install directory directories -packages directory install_directories append os path join root directory extend_python_path install_directories run_test ShardedTest test_module test_directory options finally os path exists test_directory + + test_module + py os remove test_directory + + test_module + py os environ pop USE_NINJA test_cpp_extensions_aot_ninja test_module test_directory options _test_cpp_extensions_aot test_directory options use_ninja=True test_cpp_extensions_aot_no_ninja test_module test_directory options _test_cpp_extensions_aot test_directory options use_ninja=False test_autoload_enable test_module test_directory options _test_autoload test_directory options enable=True test_autoload_disable test_module test_directory options _test_autoload test_directory options enable=False _test_autoload test_directory options enable=True cpp_extensions_test_dir = os path join test_directory cpp_extensions install_directory return_code = install_cpp_extensions cpp_extensions_test_dir return_code = return_code try os environ TORCH_DEVICE_BACKEND_AUTOLOAD = str int enable extend_python_path install_directory cmd = sys executable test_autoload py return_code = shell cmd cwd=test_directory env=os environ return_code finally os environ pop TORCH_DEVICE_BACKEND_AUTOLOAD test_openreg designed run all tests under torch_openreg which torch backend similar CUDA MPS implemented using third-party accelerator integration mechanism Therefore all tests under torch_openreg passing can means mechanism mentioned above working expected test_openreg test_module test_directory options openreg_dir = os path join test_directory cpp_extensions open_registration_extension torch_openreg install_dir return_code = install_cpp_extensions openreg_dir return_code = return_code extend_python_path install_dir cmd = sys executable -m unittest discover -s os path join openreg_dir tests -v shell cmd cwd=test_directory env=os environ test_distributed test_module test_directory options mpi_available = shutil which mpiexec options verbose mpi_available print_to_stderr MPI available -- MPI backend tests will skipped config = DISTRIBUTED_TESTS_CONFIG backend env_vars config items sys platform == win backend = gloo continue backend == mpi mpi_available continue with_init_file True False sys platform == win with_init_file continue tmp_dir = tempfile mkdtemp init_method = file with_init_file env options verbose with_init = f init_method init_method print_to_stderr f Running distributed tests backend backend with_init old_environ = dict os environ os environ TEMP_DIR = tmp_dir os environ BACKEND = backend os environ update env_vars report_tag = f dist- backend backend = test report_tag += f -init- init_method os environ TEST_REPORT_SOURCE_OVERRIDE = report_tag try os mkdir os path join tmp_dir barrier os mkdir os path join tmp_dir test_dir backend == mpi test mpiexec -- noprefix option open os devnull w devnull allowrunasroot_opt = -- allow-run-as-root subprocess call mpiexec -- allow-run-as-root -n bash -c shell=True stdout=devnull stderr=subprocess STDOUT == noprefix_opt = -- noprefix subprocess call f mpiexec allowrunasroot_opt -n -- noprefix bash -c shell=True stdout=devnull stderr=subprocess STDOUT == mpiexec = mpiexec -n noprefix_opt allowrunasroot_opt return_code = run_test test_module test_directory options launcher_cmd=mpiexec return_code = run_test test_module test_directory options extra_unittest_args= -- subprocess return_code = return_code finally shutil rmtree tmp_dir os environ clear os environ update old_environ run_doctests test_module test_directory options Assumes incoming test module called doctest simply executes xdoctest runner torch library itself xdoctest pkgpath = Path torch __file__ parent exclude_module_list = torch _vendor enabled = TODO expose these options user For now disable all feature-conditional tests lapack auto cuda auto cuda auto qengine auto lapack cuda cuda qengine autograd_profiler cpp_ext monitor onnx auto Resolve auto based test determine feature available enabled cuda == auto torch cuda is_available enabled cuda = True enabled cuda == auto torch cuda is_available torch cuda device_count enabled cuda = True enabled lapack == auto torch _C has_lapack enabled lapack = True enabled qengine == auto try Is there better check quantization enabled torch ao nn quantized nnq NOQA F torch backends quantized engine = qnnpack torch backends quantized engine = fbgemm except ImportError RuntimeError enabled qengine = True enabled onnx == auto try onnx NOQA F onnxruntime NOQA F onnxscript NOQA F except ImportError exclude_module_list append torch onnx enabled onnx = False enabled onnx = True Set doctest environment variables enabled cuda os environ TORCH_DOCTEST_CUDA = enabled cuda os environ TORCH_DOCTEST_CUDA = enabled lapack os environ TORCH_DOCTEST_LAPACK = enabled qengine os environ TORCH_DOCTEST_QENGINE = enabled autograd_profiler os environ TORCH_DOCTEST_AUTOGRAD_PROFILER = enabled cpp_ext os environ TORCH_DOCTEST_CPP_EXT = enabled monitor os environ TORCH_DOCTEST_MONITOR = enabled onnx os environ TORCH_DOCTEST_ONNX = torch mps is_available os environ TORCH_DOCTEST_MPS = torch distributed is_available os environ TORCH_DOCTEST_DISTRIBUTED = TODO could try enable some these os environ TORCH_DOCTEST_QUANTIZED_DYNAMIC = os environ TORCH_DOCTEST_ANOMALY = os environ TORCH_DOCTEST_AUTOGRAD = os environ TORCH_DOCTEST_HUB = os environ TORCH_DOCTEST_DATALOADER = os environ TORCH_DOCTEST_FUTURES = pkgpath = os path dirname torch __file__ xdoctest_config = global_exec r \n join torch nn torch nn functional F torch analysis static set auto test doctests compiled modules style google options +IGNORE_WHITESPACE xdoctest_verbose = max options verbose run_summary = xdoctest runner doctest_module os fspath pkgpath config=xdoctest_config verbose=xdoctest_verbose command=options xdoctest_command argv= exclude=exclude_module_list result = run_summary get n_failed result sanitize_file_name file str file replace \\ replace replace _ handle_log_file test ShardedTest file_path str failed bool was_rerun bool - None test = str test open file_path errors= ignore f full_text = f read new_file = test test-reports + sanitize_file_name f test _ os urandom hex _ log os rename file_path REPO_ROOT new_file failed was_rerun === RERUNS === full_text If success + no retries idk how check test level retries other than reparse xml print only what tests ran print_to_stderr f \n test successful full logs can found artifacts path new_file line full_text splitlines re search Running items shard line print_to_stderr line rstrip print_to_stderr otherwise print entire file print_to_stderr f \nPRINTING LOG FILE test new_file print_to_stderr full_text print_to_stderr f FINISHED PRINTING LOG FILE test new_file \n get_pytest_args options is_cpp_test=False is_distributed_test=False is_distributed_test Distributed tests do support rerun see https github com pytorch pytorch issues rerun_options = -x -- reruns= RERUN_DISABLED_TESTS ASAN tests too slow so running them x will cause jobs timeout after + hours So let s opt less number reruns We need least instances test every weeks satisfy SQL query x = count = TEST_WITH_ASAN When under rerun-disabled-tests mode run same tests multiple times determine their flakiness status Default re-runs rerun_options = -- flake-finder f -- flake-runs= count When under normal mode retry failed test more times -x means stop first failure rerun_options = -x -- reruns= pytest_args = -vv -rfEX is_cpp_test C++ tests need run pytest directly via python We have custom pytest shard conflicts normal plugin pytest_args extend -p no xdist -- use-pytest Use pytext-dist run C++ tests parallel running them sequentially using run_test much slower than running them directly pytest_args extend -n str NUM_PROCS TEST_SAVE_XML Add option generate XML test report here C++ tests won t go into common_utils test_report_path = get_report_path pytest=True pytest_args extend -- junit-xml-reruns test_report_path options pytest_k_expr pytest_args extend -k options pytest_k_expr pytest_args extend rerun_options pytest_args run_ci_sanity_check test ShardedTest test_directory options assert test name == test_ci_sanity_check_fail f This handler only works test_ci_sanity_check_fail got test name ret_code = run_test test test_directory options print_log=False This test should fail ret_code = test_reports_dir = str REPO_ROOT test test-reports Delete log files xmls generated test file glob glob f test_reports_dir test name log os remove file dirname glob glob f test_reports_dir test name shutil rmtree dirname CUSTOM_HANDLERS = test_cuda_primary_ctx run_test_with_subprocess test_cuda_nvml_based_avail run_test_with_subprocess test_cuda_trace run_test_with_subprocess test_cpp_extensions_aot_no_ninja test_cpp_extensions_aot_no_ninja test_cpp_extensions_aot_ninja test_cpp_extensions_aot_ninja distributed test_distributed_spawn test_distributed distributed algorithms quantization test_quantization test_distributed distributed test_c d_nccl run_test_with_subprocess distributed test_c d_gloo run_test_with_subprocess distributed test_c d_ucc run_test_with_subprocess distributed test_c d_common run_test_with_subprocess distributed test_c d_spawn_gloo run_test_with_subprocess distributed test_c d_spawn_nccl run_test_with_subprocess distributed test_c d_spawn_ucc run_test_with_subprocess distributed test_store run_test_with_subprocess distributed test_pg_wrapper run_test_with_subprocess distributed rpc test_faulty_agent run_test_with_subprocess distributed rpc test_tensorpipe_agent run_test_with_subprocess distributed rpc test_share_memory run_test_with_subprocess distributed rpc cuda test_tensorpipe_agent run_test_with_subprocess doctests run_doctests test_ci_sanity_check_fail run_ci_sanity_check test_autoload_enable test_autoload_enable test_autoload_disable test_autoload_disable test_openreg test_openreg PYTEST_SKIP_RETRIES = test_public_bindings parse_args parser = argparse ArgumentParser description= Run PyTorch unit test suite epilog= where TESTS any format join TESTS formatter_class=argparse RawTextHelpFormatter parser add_argument -v -- verbose action= count default= help= Print verbose information test-by-test results parser add_argument -- showlocals action=argparse BooleanOptionalAction default=strtobool os environ get TEST_SHOWLOCALS False help= Show local variables tracebacks default True parser add_argument -- jit -- jit action= store_true help= run all jit tests parser add_argument -- distributed-tests -- distributed-tests action= store_true help= Run all distributed tests parser add_argument -- functorch -- functorch action= store_true help= If flag present we will only run functorch tests If flag present we will run all tests including functorch tests parser add_argument -- einops -- einops action= store_true help= If flag present we will only run einops tests If flag present we will run all tests including einops tests parser add_argument -- mps -- mps action= store_true help= If flag present we will only run test_mps test_metal parser add_argument -- xpu -- xpu action= store_true help= If flag present we will run xpu tests except XPU_BLOCK_LIST parser add_argument -- cpp -- cpp action= store_true help= If flag present we will only run C++ tests parser add_argument -core -- core action= store_true help= Only run core tests tests validate PyTorch s ops modules autograd They defined CORE_TEST_LIST parser add_argument -- onnx -- onnx action= store_true help= Only run ONNX tests tests validate PyTorch s ONNX export If flag present we will exclude ONNX tests parser add_argument -k -- pytest-k-expr default= help= Pass pytest its -k expr argument parser add_argument -c -- coverage action= store_true help= enable coverage default=PYTORCH_COLLECT_COVERAGE parser add_argument -i -- include nargs= + choices=TestChoices TESTS default=TESTS metavar= TESTS help= select set tests include defaults ALL tests tests must part TESTS list defined run_test py parser add_argument -x -- exclude nargs= + choices=TESTS metavar= TESTS default= help= select set tests exclude parser add_argument -- ignore-win-blocklist action= store_true help= always run blocklisted windows tests NS Disable target determination until can made more reliable parser add_argument -- determine-from help= File affected source filenames determine which tests run parser add_argument -- continue-through-error -- keep-going action= store_true help= Runs full test suite despite one tests failing default=strtobool os environ get CONTINUE_THROUGH_ERROR False parser add_argument -- pipe-logs action= store_true help= Print logs output file while running tests True CI env var set default=IS_CI strtobool os environ get VERBOSE_TEST_LOGS False parser add_argument -- enable-timeout action= store_true help= Set timeout based test times json file Only works there test times available default=IS_CI strtobool os environ get NO_TEST_TIMEOUT False parser add_argument -- enable-td action= store_true help= Enables removing tests based TD default=IS_CI get_pr_number None strtobool os environ get NO_TD False IS_MACOS xpu BUILD_ENVIRONMENT onnx BUILD_ENVIRONMENT os environ get GITHUB_WORKFLOW slow trunk pull rocm rocm-mi parser add_argument -- shard nargs= type=int help= runs shard tests taking into account other selections e g -- shard will break up selected tests into shards run tests nd shard first number should exceed second parser add_argument -- exclude-jit-executor action= store_true help= exclude tests run specific jit config parser add_argument -- exclude-torch-export-tests action= store_true help= exclude torch export tests parser add_argument -- exclude-aot-dispatch-tests action= store_true help= exclude aot dispatch tests parser add_argument -- exclude-distributed-tests action= store_true help= exclude distributed tests parser add_argument -- exclude-inductor-tests action= store_true help= exclude inductor tests parser add_argument -- exclude-quantization-tests action= store_true help= exclude quantization tests parser add_argument -- dry-run action= store_true help= Only list test will run parser add_argument -- xdoctest-command default= all help= Control specific doctest action Use list simply parse doctests check syntax Use all execute all doctests specify specific doctest run parser add_argument -- no-translation-validation action= store_false help= Run tests without translation validation parser add_argument -- upload-artifacts-while-running action= store_true default=IS_CI group = parser add_mutually_exclusive_group group add_argument -- dynamo action= store_true help= Run tests TorchDynamo+EagerBackend turned group add_argument -- inductor action= store_true help= Run tests TorchInductor turned args extra = parser parse_known_args -- extra extra remove -- args additional_args = extra args exclude_tests exclude_list selected_tests exclude_message=None exact_match=False exclude_test exclude_list tests_copy = selected_tests test tests_copy exact_match test startswith exclude_test test == exclude_test exclude_message None print_to_stderr f Excluding test exclude_message selected_tests remove test selected_tests must_serial file Union str ShardedTest - bool isinstance file ShardedTest file = file name os getenv PYTORCH_TEST_RUN_EVERYTHING_IN_SERIAL == DISTRIBUTED_TEST_PREFIX os getenv TEST_CONFIG DISTRIBUTED_TEST_PREFIX file file CUSTOM_HANDLERS file RUN_PARALLEL_BLOCKLIST file CI_SERIAL_LIST file JIT_EXECUTOR_TESTS file ONNX_SERIAL_LIST NUM_PROCS == can_run_in_pytest test os getenv PYTORCH_TEST_DO_NOT_USE_PYTEST == get_selected_tests options - list str selected_tests = options include filter there s JIT only distributed only test options options jit selected_tests = list filter lambda test_name jit test_name selected_tests options distributed_tests selected_tests = list filter lambda test_name test_name DISTRIBUTED_TESTS selected_tests Filter only run core tests when -- core option specified options core selected_tests = list filter lambda test_name test_name CORE_TEST_LIST selected_tests Filter only run functorch tests when -- functorch option specified options functorch selected_tests = list filter lambda test_name test_name FUNCTORCH_TESTS selected_tests Filter only run einops tests when -- einops option specified options einops selected_tests = list filter lambda test_name test_name startswith dynamo test_einops selected_tests options cpp selected_tests = list filter lambda test_name test_name CPP_TESTS selected_tests Exclude all C++ tests otherwise they still handled differently than Python test moment options exclude extend CPP_TESTS options mps selected_tests = test_mps test_metal test_modules nn test_convolution nn test_dropout nn test_pooling test_view_ops test_nn inductor test_mps_basic inductor test_torchinductor inductor test_aot_inductor inductor test_torchinductor_dynamic_shapes Exclude all mps tests otherwise options exclude extend test_mps test_metal options xpu selected_tests = exclude_tests XPU_BLOCKLIST selected_tests XPU Exclude all xpu specific tests otherwise options exclude extend XPU_TEST Filter only run onnx tests when -- onnx option specified onnx_tests = tname tname selected_tests tname ONNX_TESTS options onnx selected_tests = onnx_tests Exclude all onnx tests otherwise options exclude extend onnx_tests process exclusion options exclude_jit_executor options exclude extend JIT_EXECUTOR_TESTS options exclude_distributed_tests options exclude extend DISTRIBUTED_TESTS options exclude_inductor_tests options exclude extend INDUCTOR_TESTS options exclude_torch_export_tests options exclude extend TORCH_EXPORT_TESTS options exclude_aot_dispatch_tests options exclude extend AOT_DISPATCH_TESTS options exclude_quantization_tests options exclude extend QUANTIZATION_TESTS these tests failing CUDA temporary disabling issue https github com pytorch pytorch issues torch version cuda None options exclude extend distributions test_constraints these tests failing Python temporarily disabling sys version_info = options exclude extend functorch test_dims functorch test_rearrange functorch test_parsing functorch test_memory_efficient_fusion torch_np numpy_tests core test_multiarray sys version_info Skip tests older Python versions they may use syntax features supported those versions options exclude extend test test selected_tests test startswith dynamo cpython _ selected_tests = exclude_tests options exclude selected_tests sys platform == win options ignore_win_blocklist target_arch = os environ get VSCMD_ARG_TGT_ARCH target_arch = x WINDOWS_BLOCKLIST append cpp_extensions_aot_no_ninja WINDOWS_BLOCKLIST append cpp_extensions_aot_ninja WINDOWS_BLOCKLIST append cpp_extensions_jit WINDOWS_BLOCKLIST append jit WINDOWS_BLOCKLIST append jit_fuser selected_tests = exclude_tests WINDOWS_BLOCKLIST selected_tests Windows TEST_WITH_ROCM selected_tests = exclude_tests ROCM_BLOCKLIST selected_tests ROCm IS_S X selected_tests = exclude_tests S X_BLOCKLIST selected_tests s x selected_tests = exclude_tests DISTRIBUTED_TESTS selected_tests Skip distributed tests s x skip all distributed tests distributed package available dist is_available selected_tests = exclude_tests DISTRIBUTED_TESTS selected_tests PyTorch built without distributed support skip tests require LAPACK when s available torch _C has_lapack selected_tests = exclude_tests TESTS_REQUIRING_LAPACK selected_tests PyTorch built without LAPACK support TEST_WITH_SLOW_GRADCHECK selected_tests = exclude_tests TESTS_NOT_USING_GRADCHECK selected_tests Running slow gradcheck mode skipping tests don t use gradcheck exact_match=True selected_tests = parse_test_module x x selected_tests selected_tests load_test_times_from_file file str - dict str Any Load previous test times make sharding decisions path = os path join str REPO_ROOT file os path exists path print_to_stderr f warning Failed find test times file ` path ` Using round robin sharding open path f test_times_file = cast dict str Any json load f job_name = os environ get JOB_NAME job_name None job_name == If job name isn t available use build environment backup job_name = os environ get BUILD_ENVIRONMENT job_name = job_name split test test_config = os environ get TEST_CONFIG test_config test_times_file get job_name print_to_stderr Found test times artifacts test_times_file job_name test_config test_config test_times_file default print_to_stderr f warning Gathered no stats artifacts job_name build env f test_config test config Using default job name test_config test config instead test_times_file default test_config print_to_stderr f warning Gathered no stats artifacts job name job_name build env f test_config test config Using default job name default test config instead test_times_file default default load_test_file_times file str = ADDITIONAL_CI_FILES_FOLDER TEST_TIMES_FILE - dict str float cast dict str float load_test_times_from_file file load_test_class_times file str = ADDITIONAL_CI_FILES_FOLDER TEST_CLASS_TIMES_FILE - dict str dict str float cast dict str dict str float load_test_times_from_file file get_sharding_opts options - tuple int int which_shard num_shards = options shard assert len options shard == Unexpected shard format assert min options shard Shards must positive numbers which_shard num_shards = options shard assert which_shard = num_shards Selected shard must less than equal total number shards which_shard num_shards do_sharding options selected_tests Sequence TestRun test_file_times dict str float test_class_times dict str dict str float sort_by_time bool = True - tuple float list ShardedTest which_shard num_shards = get_sharding_opts options Do sharding shards = calculate_shards num_shards selected_tests test_file_times test_class_times=test_class_times must_serial=must_serial sort_by_time=sort_by_time shards which_shard - TestFailure NamedTuple test TestRun message str run_test_module test ShardedTest test_directory str options - Optional TestFailure try maybe_set_hip_visible_devies test_name = test name Printing date here can help diagnose which tests slow print_to_stderr f Running str test datetime now handler = CUSTOM_HANDLERS get test_name run_test return_code = handler test test_directory options assert isinstance return_code int isinstance return_code bool f While running str test got non integer code return_code return_code == None message = f str test failed return_code subprocess Popen returns child process exit signal code -N where N signal number signal_name = SIGNALS_TO_NAMES_DICT -return_code message += f Received signal signal_name TestFailure test test message except Exception e TestFailure test test f str test failed e run_tests selected_tests list ShardedTest test_directory str options failures list TestFailure - None len selected_tests == parallel = parallel other files serial = file s own The file might still run parallel itself ex test_ops selected_tests_parallel = x x selected_tests must_serial x selected_tests_serial = x x selected_tests x selected_tests_parallel NB This hack make conftest py files depends available CPP_TESTS_DIR We should see file could turned into full-fledge ptest plugin instead conftest_files = conftest py pytest_shard_custom py conftest_file conftest_files cpp_file = os path join CPP_TESTS_DIR conftest_file options cpp os path exists CPP_TESTS_DIR os path isdir CPP_TESTS_DIR os path exists cpp_file shutil copy os path join test_directory conftest_file cpp_file handle_complete failure Optional TestFailure failed = failure None IS_CI options upload_artifacts_while_running zip_and_upload_artifacts failed failed False failures append failure print_to_stderr failure message True keep_going_message = \n\nTip You can keep running tests even failure passing -- keep-going run_test py \n If running CI add keep-going label your PR rerun your jobs pool = None try test selected_tests_serial options_clone = copy deepcopy options can_run_in_pytest test options_clone pytest = True failure = run_test_module test test_directory options_clone test_failed = handle_complete failure test_failed options continue_through_error RERUN_DISABLED_TESTS raise RuntimeError failure message + keep_going_message Run tests marked serial first test selected_tests_parallel options_clone = copy deepcopy options can_run_in_pytest test options_clone pytest = True options_clone additional_args extend -m serial failure = run_test_module test test_directory options_clone test_failed = handle_complete failure test_failed options continue_through_error RERUN_DISABLED_TESTS raise RuntimeError failure message + keep_going_message This used later constrain memory per proc GPU On ROCm number procs number GPUs so we don t need do os environ NUM_PARALLEL_PROCS = str torch version hip NUM_PROCS See Note ROCm parallel CI testing pool = get_context spawn Pool NUM_PROCS maxtasksperchild=None torch version hip parallel_test_completion_callback failure test_failed = handle_complete failure test_failed options continue_through_error RERUN_DISABLED_TESTS pool terminate test selected_tests_parallel options_clone = copy deepcopy options can_run_in_pytest test options_clone pytest = True options_clone additional_args extend -m serial pool apply_async run_test_module args= test test_directory options_clone callback=parallel_test_completion_callback pool close pool join del os environ NUM_PARALLEL_PROCS finally pool pool terminate pool join check_pip_packages - None packages = pytest-rerunfailures pytest-flakefinder pytest-xdist try pkg packages version pkg except PackageNotFoundError print_to_stderr f Missing pip dependency pkg please run ` pip install -r ci docker requirements-ci txt ` sys exit main check_pip_packages options = parse_args Include sharding info all metrics which_shard num_shards = get_sharding_opts options add_global_metric shard which_shard add_global_metric num_shards num_shards test_directory = str REPO_ROOT test selected_tests = get_selected_tests options test_prioritizations = import_results len test_prioritizations get_all_tests == options enable_td = False test_prioritizations amend_tests selected_tests os makedirs REPO_ROOT test test-reports exist_ok=True options coverage PYTORCH_COLLECT_COVERAGE shell coverage erase IS_CI downloading test cases configuration local environment get_test_case_configs dirpath=test_directory test_file_times_dict = load_test_file_times test_class_times_dict = load_test_class_times TestBatch Defines set tests similar priority should run together current shard name str sharded_tests list ShardedTest failures list TestFailure __init__ name str raw_tests Sequence TestRun should_sort_shard bool name = name failures = time sharded_tests = do_sharding options raw_tests test_file_times_dict test_class_times_dict sort_by_time=should_sort_shard __str__ s = f Name name est time round time min \n serial = test test sharded_tests must_serial test parallel = test test sharded_tests must_serial test s += f Serial tests len serial \n s += join f test \n test serial s += f Parallel tests len parallel \n s += join f test \n test parallel s strip percent_to_run = options enable_td print_to_stderr f Running percent_to_run tests based TD options enable_td Running all tests include exclude = test_prioritizations get_top_per_tests percent_to_run test_batch = TestBatch tests run include False test_batch_exclude = TestBatch excluded exclude True IS_CI gen_ci_artifact x to_json x include x to_json x exclude print_to_stderr f Running parallel tests NUM_PROCS processes print_to_stderr test_batch print_to_stderr test_batch_exclude options dry_run options dynamo os environ PYTORCH_TEST_WITH_DYNAMO = options inductor os environ PYTORCH_TEST_WITH_INDUCTOR = options no_translation_validation os environ PYTORCH_TEST_WITH_TV = try Actually run tests start_time = time time run_tests test_batch sharded_tests test_directory options test_batch failures elapsed_time = time time - start_time print_to_stderr f Running test batch test_batch name cost round elapsed_time seconds finally options coverage coverage Coverage set_cwd test_directory cov = Coverage PYTORCH_COLLECT_COVERAGE cov load cov combine strict=False cov save PYTORCH_COLLECT_COVERAGE cov html_report all_failures = test_batch failures IS_CI test _ all_failures test_stats = test_prioritizations get_test_stats test print_to_stderr Emiting td_test_failure_stats_v emit_metric td_test_failure_stats_v selected_tests selected_tests failure str test test_stats gen_additional_test_failures_file test test_file test _ all_failures len all_failures _ err all_failures print_to_stderr err A disabled test expected fail so there no need report failure here RERUN_DISABLED_TESTS sys exit __name__ == __main__ main