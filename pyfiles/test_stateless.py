Owner s module nn contextlib os re subprocess sys unittest torch torch nn utils stateless stateless torch testing _internal common_cuda TEST_MULTIGPU torch testing _internal common_utils run_tests TestCase parametrize instantiate_parametrized_tests \ subtest MockModule torch nn Module __init__ - None super __init__ l = torch nn Linear buffer = torch nn Buffer torch ones foo = forward x l x + buffer MockTiedModule torch nn Module __init__ - None super __init__ l = torch nn Linear tied_bias = l bias buffer = torch nn Buffer torch ones tied_buffer = buffer forward x l x + tied_bias + buffer + tied_buffer TestStatelessFunctionalAPI TestCase _run_call_with_mock_module module functional_call device= cpu prefix= x = torch rand device weight = torch tensor device=device bias = torch tensor device=device buffer = torch tensor device=device prefix = parameters = f prefix l weight weight f prefix l bias bias f prefix buffer buffer parameters = l weight weight l bias bias buffer buffer to_check = module prefix = to_check = getattr module prefix prev_weight = to_check l weight clone prev_buffer = to_check buffer clone parameters represent identity function contrary existing params module So here we expect result same input weight swapping went well res = functional_call module parameters x assertEqual x res check weight remain unmodified cur_weight = to_check l weight cur_buffer = to_check buffer assertEqual cur_weight prev_weight assertEqual cur_buffer prev_buffer contextlib contextmanager _ensure_module_unchanged module message orig_parameters orig_buffers = tuple module parameters tuple module buffers orig_tensors = orig_parameters + orig_buffers orig_tensors_values = tuple t clone t orig_tensors try yield module finally parameters buffers = tuple module parameters tuple module buffers assertTrue len parameters == len orig_parameters len buffers == len orig_buffers all t t torch allclose t t t t t zip orig_tensors parameters + buffers orig_tensors_values message parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_functional_call functional_call module = MockModule _run_call_with_mock_module module functional_call parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_functional_call_with_jit functional_call module = MockModule jit_module = torch jit script module assertRaisesRegex RuntimeError r used Jitted modules _run_call_with_mock_module jit_module functional_call x = torch rand traced_module = torch jit trace module x assertRaisesRegex RuntimeError r used Jitted modules _run_call_with_mock_module traced_module functional_call unittest skipIf TEST_MULTIGPU multi-GPU supported unittest skip This doesn t work right now parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_functional_call_with_data_parallel functional_call module = MockModule module cuda dp_module = torch nn DataParallel module _run_call_with_mock_module dp_module functional_call device= cuda prefix= module unittest skipIf TEST_MULTIGPU multi-GPU supported parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_functional_call_with_data_parallel_error functional_call module = MockModule module cuda dp_module = torch nn DataParallel module assertRaisesRegex RuntimeError r used nn DataParallel module functional_call dp_module module weight torch zeros device= cuda torch ones device= cuda parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_functional_call_with_gradient functional_call module = MockModule x = torch rand weight = torch tensor requires_grad=True bias = torch tensor requires_grad=True buffer = torch tensor parameters = l weight weight l bias bias buffer buffer res = functional_call module parameters x Check backward step calculates gradient supplied parameters res backward assertIsNotNone weight grad assertIsNotNone bias grad assertIsNone buffer grad Gradient calculated module stated buffers assertIsNone module l weight grad assertIsNone module l bias grad assertIsNone module buffer grad parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_functional_batch_norm functional_call module = torch nn BatchNorm d module train Allow stats update lets replace running_mean buffer check its correctly updated x = torch full rm = torch zeros parameters = running_mean rm prev_rm = module running_mean clone functional_call module parameters x cur_rm = module running_mean assertEqual cur_rm prev_rm assertEqual rm torch full Now run functional without reparameterization check module has been updated functional_call module x assertEqual module running_mean torch full parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_circular_references functional_call module = MockModule Add circular reference module l m = module x = torch rand weight = torch tensor bias = torch tensor buffer = torch tensor parameters = l m l weight weight l bias bias l m buffer buffer prev_weight = module l weight clone prev_buffer = module buffer clone res = functional_call module parameters x tie_weights=False assertEqual x res check weights remain unmodified correctly accessed cur_weight = module l weight cur_buffer = module buffer assertEqual cur_weight prev_weight assertEqual cur_buffer prev_buffer parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_reparametrized_module_change_parametrization_original functional_call module = MockModule torch nn utils parametrizations spectral_norm module l assertTrue l parametrizations weight original dict module named_parameters orig_sn_weight = module l weight clone x = torch rand We substitute parameter inside parametrization parametrization itself overwritten so will applied different value original tensor parameters = l parametrizations weight original torch nn Parameter torch tensor l bias torch tensor buffer torch tensor res = functional_call module parameters x assertEqual x res verify spectral normalization still applied assertTrue l parametrizations weight original dict module named_parameters assertEqual orig_sn_weight module l weight parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_reparametrize_module_fail_reset_to_original functional_call module = MockModule torch nn utils parametrizations spectral_norm module l assertTrue l parametrizations weight original dict module named_parameters orig_sn_weight = module l weight clone We substitute parameter inside parametrization parametrization itself overwritten so will applied different value original tensor parameters = l parametrizations weight original torch nn Parameter torch tensor l bias torch tensor buffer torch tensor assertRaisesRegex RuntimeError shapes cannot multiplied torch _dynamo disable _error_case x = torch rand work should size functional_call module parameters x call will fail because x wrong size _error_case verify spectral normalization still applied assertTrue l parametrizations weight original dict module named_parameters assertEqual orig_sn_weight module l weight parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_reparametrize_some_weights functional_call module = MockModule weight = torch tensor extra = torch tensor parameters = l weight weight x = torch randn out = functional_call module parameters x assertEqual out x weight + module l bias + module buffer parameters = l weight weight extra extra x = torch randn out = functional_call module parameters x assertEqual out x weight + module l bias + module buffer parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_reparametrize_strict functional_call module = MockModule weight = torch tensor bias = torch tensor buffer = torch tensor extra = torch tensor All weights no error parameters = l weight weight l bias bias buffer buffer x = torch randn _ensure_module_unchanged module module should have been modified successful call out = functional_call module parameters x strict=True assertEqual out x weight + bias + buffer Some weights parameters = l weight weight x = torch randn _ensure_module_unchanged module module should have been modified failed call assertRaisesRegex RuntimeError re escape Missing key s buffer l bias out = functional_call module parameters x strict=True Extra keys parameters = l weight weight l bias bias buffer buffer extra extra x = torch randn _ensure_module_unchanged module module should have been modified failed call assertRaisesRegex RuntimeError re escape Unexpected key s extra out = functional_call module parameters x strict=True Some weights extra keys parameters = l weight weight extra extra x = torch randn _ensure_module_unchanged module module should have been modified failed call assertRaisesRegex RuntimeError re escape Unexpected key s extra + r \s+ + re escape Missing key s buffer l bias out = functional_call module parameters x strict=True parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_reparametrize_special functional_call NonTensor __repr__ f __class__ __name__ module = MockModule weight = torch tensor bias = torch tensor buffer = torch tensor non_tensor = NonTensor Set None parameters = l weight weight l bias None buffer buffer x = torch randn _ensure_module_unchanged module module should have been modified successful call out = functional_call module parameters x assertEqual out x weight + buffer Set non-tensor parameters = l weight non_tensor x = torch randn _ensure_module_unchanged module module should have been modified failed call assertRaisesRegex TypeError re escape NonTensor instance torch Tensor out = functional_call module parameters x Set non-tensor attribute parameters = l weight weight foo torch tensor x = torch randn _ensure_module_unchanged module module should have been modified failed call assertRaisesRegex TypeError re escape attribute ` foo ` instance torch Tensor out = functional_call module parameters x Set non-exist submodule parameters = l weight weight l bias bias x = torch randn _ensure_module_unchanged module module should have been modified failed call assertRaisesRegex AttributeError re escape MockModule has no attribute ` l ` out = functional_call module parameters x parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_tied_weights_warns functional_call module = MockModule module tied_bias = module l bias module tied_buffer = torch nn Buffer module buffer parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_reparametrize_tie_weights functional_call module = MockTiedModule weight = torch tensor bias = torch tensor buffer = torch tensor extra = torch tensor parameters = l weight weight l bias bias buffer buffer x = torch randn out = functional_call module parameters x tie_weights=True assertEqual out x weight + bias + bias + buffer + buffer parameters = l weight weight l bias bias buffer buffer extra extra x = torch randn out = functional_call module parameters x tie_weights=True assertEqual out x weight + bias + bias + buffer + buffer parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_reparametrize_tie_some_weights functional_call module = MockTiedModule weight = torch tensor buffer = torch tensor parameters = l weight weight buffer buffer x = torch randn out = stateless functional_call module parameters x tie_weights=True assertEqual out x + module l bias + module tied_bias + buffer + buffer parametrize functional_call subtest torch func functional_call torch_func subtest stateless _functional_call stateless test_tied_weights_errors functional_call module = MockTiedModule weight = torch tensor bias = torch tensor buffer = torch tensor parameters = l weight weight l bias bias buffer buffer x = torch randn assertNotWarn lambda functional_call module parameters x tie_weights=True tied values same tensors shouldn t warn parameters tied_bias = bias parameters tied_buffer = buffer assertNotWarn lambda functional_call module parameters x tie_weights=True del parameters tied_bias del parameters tied_buffer assertRaisesRegex ValueError re escape functional_call got multiple values keys l bias tied_bias parameters tied_bias = torch tensor functional_call module parameters x tie_weights=True del parameters tied_bias assertRaisesRegex ValueError re escape functional_call got multiple values keys buffer tied_buffer parameters tied_buffer = torch tensor functional_call module parameters x tie_weights=True test_tied_weights_no_error_without_flag module = MockTiedModule weight = torch tensor bias = torch tensor buffer = torch tensor parameters = l weight weight l bias bias buffer buffer x = torch randn assertNotWarn lambda stateless _functional_call module parameters x tie_weights=False parameters tied_bias = torch tensor assertNotWarn lambda stateless _functional_call module parameters x tie_weights=False del parameters tied_bias parameters tied_buffer = torch tensor assertNotWarn lambda stateless _functional_call module parameters x tie_weights=False parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_reparametrize_tie_weights_strict functional_call module = MockTiedModule weight = torch tensor bias = torch tensor buffer = torch tensor extra = torch tensor Tie weights no error parameters = l weight weight l bias bias buffer buffer x = torch randn _ensure_module_unchanged module module should have been modified successful call out = functional_call module parameters x tie_weights=True strict=True assertEqual out x weight + bias + bias + buffer + buffer Tie weights without flag parameters = l weight weight l bias bias buffer buffer x = torch randn _ensure_module_unchanged module module should have been modified failed call assertRaisesRegex RuntimeError re escape Missing key s tied_bias tied_buffer out = functional_call module parameters x tie_weights=False strict=True Tie some weights parameters = l weight weight buffer buffer x = torch randn _ensure_module_unchanged module module should have been modified failed call assertRaisesRegex RuntimeError re escape Missing key s l bias tied_bias out = stateless functional_call module parameters x tie_weights=True strict=True Tie weights extra keys parameters = l weight weight l bias bias buffer buffer extra extra x = torch randn _ensure_module_unchanged module module should have been modified failed call assertRaisesRegex RuntimeError re escape Unexpected key s extra out = stateless functional_call module parameters x tie_weights=True strict=True Tie weights extra keys without flag parameters = l weight weight l bias bias buffer buffer extra extra x = torch randn _ensure_module_unchanged module module should have been modified failed call assertRaisesRegex RuntimeError re escape Unexpected key s extra + r \s+ + re escape Missing key s tied_bias tied_buffer out = stateless functional_call module parameters x tie_weights=False strict=True Tie some weights extra keys parameters = l weight weight buffer buffer extra extra x = torch randn _ensure_module_unchanged module module should have been modified failed call assertRaisesRegex RuntimeError re escape Unexpected key s extra + r \s+ + re escape Missing key s l bias tied_bias out = stateless functional_call module parameters x tie_weights=True strict=True parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_setattr functional_call Foo torch nn Module __init__ - None super __init__ foo = torch nn Buffer torch tensor forward x foo = foo + x + foo foo = torch tensor x = torch randn = foo foo mod = Foo functional_call mod x assertEqual mod foo torch tensor assertEqual foo torch tensor assertEqual foo torch tensor assertTrue foo foo parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_in_place_operator functional_call Foo torch nn Module __init__ - None super __init__ foo = torch nn Buffer torch tensor forward x foo add_ x + foo foo = torch tensor x = torch randn = foo foo mod = Foo functional_call mod x assertEqual mod foo torch tensor assertEqual foo torch tensor assertEqual foo torch tensor assertTrue foo foo parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_setattr_strict functional_call Bar torch nn Module __init__ - None super __init__ assert hasattr extra forward x x + extra = extra torch zeros mod = Bar assertTrue hasattr mod extra out = functional_call mod torch ones assertEqual out torch ones assertTrue hasattr mod extra = extra torch zeros assertRaisesRegex RuntimeError re escape Unexpected key s extra out = functional_call mod torch ones strict=True assertTrue hasattr mod extra = assertRaisesRegex AttributeError re escape Bar object has no attribute extra out = functional_call mod torch ones assertTrue hasattr mod extra = assertRaisesRegex AttributeError re escape Bar object has no attribute extra out = functional_call mod torch ones strict=True assertTrue hasattr mod extra parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_functional_call_with_kwargs functional_call Foo torch nn Module __init__ x super __init__ x = x forward inp other_inp inp x + other_inp = x torch zeros mod = Foo torch randn inp other_inp = torch randn torch randn assertRaisesRegex TypeError missing required keyword-only argument other_inp functional_call mod inp res = functional_call mod inp other_inp other_inp assertEqual res other_inp res_ = functional_call mod inp inp other_inp other_inp assertEqual res res_ res_ = functional_call mod kwargs= inp inp other_inp other_inp assertEqual res res_ test_functional_call_tuple_dicts mod = MockModule x = torch rand parameters = k torch ones_like v k v mod named_parameters buffers = k torch zeros_like v k v mod named_buffers two dictionaries res = torch func functional_call mod parameters buffers x assertEqual res x + no dictionaries res = torch func functional_call mod x assertEqual res mod x three dictionaries = l weight torch ones l bias torch ones buffer torch zeros res = torch func functional_call mod x assertEqual res x + test_functional_call_multiple_dicts_error mod = MockModule x = torch rand parameters = l weight torch zeros l bias torch zeros repeated_parameters = l weight torch ones assertRaisesRegex ValueError re escape l weight appeared multiple dictionaries torch func functional_call mod parameters repeated_parameters x parametrize functional_call subtest torch func functional_call torch_func subtest stateless functional_call stateless test_functional_call_member_reference functional_call Module torch nn Module __init__ - None super __init__ l = torch nn Linear buffer = torch nn Buffer torch ones forward x parameters = tuple parameters buffers = tuple buffers l x + buffer parameters buffers module = Module weight = torch tensor bias = torch tensor buffer = torch tensor extra = torch tensor extra_p = torch nn Parameter extra All weights parameters = l weight weight l bias bias buffer buffer x = torch randn out parameters buffers = functional_call module parameters x assertEqual out x weight + bias + buffer assertEqual parameters weight bias assertEqual buffers buffer assertTrue all t t t t zip parameters weight bias assertTrue all t t t t zip buffers buffer Some weights parameters = l weight weight x = torch randn out parameters buffers = functional_call module parameters x assertEqual out x weight + module l bias + module buffer assertEqual parameters weight module l bias assertEqual buffers module buffer assertTrue all t t t t zip parameters weight module l bias assertTrue all t t t t zip buffers module buffer All weights extra keys parameters = l weight weight l bias bias buffer buffer l extra extra x = torch randn out parameters buffers = functional_call module parameters x assertEqual out x weight + bias + buffer assertEqual parameters weight bias assertEqual buffers buffer assertTrue all t t t t zip parameters weight bias assertTrue all t t t t zip buffers buffer All weights extra keys parameters parameters = l weight weight l bias bias buffer buffer l extra extra_p x = torch randn out parameters buffers = functional_call module parameters x assertEqual out x weight + bias + buffer assertEqual parameters weight bias extra_p assertEqual buffers buffer assertTrue all t t t t zip parameters weight bias extra_p assertTrue all t t t t zip buffers buffer Some weights extra keys parameters = l weight weight l extra extra x = torch randn out parameters buffers = functional_call module parameters x assertEqual out x weight + module l bias + module buffer assertEqual parameters weight module l bias assertEqual buffers module buffer assertTrue all t t t t zip parameters weight module l bias assertTrue all t t t t zip buffers module buffer Some weights extra keys parameters parameters = l weight weight l extra extra_p x = torch randn out parameters buffers = functional_call module parameters x assertEqual out x weight + module l bias + module buffer assertEqual parameters weight module l bias extra_p assertEqual buffers module buffer assertTrue all t t t t zip parameters weight module l bias extra_p assertTrue all t t t t zip buffers module buffer Set None parameters = l weight weight l bias None x = torch randn out parameters buffers = functional_call module parameters x assertEqual out x weight + module buffer assertEqual parameters weight assertEqual buffers module buffer assertTrue all t t t t zip parameters weight assertTrue all t t t t zip buffers module buffer TestStatelessDeprecation TestCase test_private_stateless_warns script = torch warnings warnings catch_warnings record=True w torch nn utils _stateless exit len w try subprocess check_output sys executable -W always -c script stderr=subprocess STDOUT On Windows opening subprocess default CWD makes ` torch ` fail so just set CWD script s directory cwd=os path dirname os path realpath __file__ except subprocess CalledProcessError e assertEqual e returncode assertTrue False No warning raised test_stateless_functional_call_warns m = torch nn Linear params = dict m named_parameters x = torch randn assertWarnsRegex FutureWarning Please use ` torch func functional_call ` stateless functional_call m params x TestPythonOptimizeMode TestCase test_runs_with_optimize_flag script = torch torch _functorch deprecated try subprocess check_output sys executable -OO -c script stderr=subprocess STDOUT On Windows opening subprocess default CWD makes ` torch ` fail so just set CWD script s directory cwd=os path dirname os path realpath __file__ except subprocess CalledProcessError e assertFalse e returncode Import failed while running python optimized mode instantiate_parametrized_tests TestStatelessFunctionalAPI __name__ == __main__ run_tests