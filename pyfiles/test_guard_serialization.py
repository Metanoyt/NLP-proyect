Owner s module dynamo dataclasses pickle sys tempfile types unittest weakref collections abc Iterator unittest mock patch torch torch _dynamo testing torch _inductor config torch _inductor test_case torch onnx operators torch utils cpp_extension torch _dynamo bytecode_transformation transform_code_object torch _dynamo exc PackageError torch _dynamo guards CheckFunctionManager CompileId torch _dynamo package CompilePackage torch _dynamo symbolic_convert ExceptionStack InstructionTranslator SpeculationLog torch _dynamo utils dynamo_timed get_metrics_context torch _guards compile_context CompileContext tracing torch overrides TorchFunctionMode torch testing _internal common_utils IS_MACOS torch testing _internal inductor_utils HAS_GPU torch utils _pytree pytree dataclasses dataclass _FrameState f_locals dict f_globals dict f_code types CodeType f_builtins dict GlobalModule torch nn Module forward x x + GlobalNestedModule torch nn Module __init__ submodule=None super __init__ linear = torch nn Linear param = torch nn Parameter torch randn nested = submodule GlobalModule forward x linear x + global_func x x + ModuleNotSerializable torch nn Module __init__ super __init__ param = torch nn Parameter torch randn __getstate__ raise NotImplementedError serialzable forward x x + param GlobalTorchFunctionMode TorchFunctionMode __torch_function__ func types args= kwargs=None kwargs None kwargs = func args kwargs MyClass __getstate__ raise RuntimeError Cannot pickle add x x + MyClassNotSerializable __getstate__ raise NotImplementedError add x x + Inputs __init__ x unused x = x unused = unused _global_func_wrong_fqn x x + global_func_wrong_fqn = _global_func_wrong_fqn del _global_func_wrong_fqn FlatModule torch nn Module forward x x + ModWithDict torch nn Module __init__ d super __init__ d = d SubclassWithMeta torch Tensor staticmethod __new__ cls extra outer_size=None outer_stride=None outer_size None outer_size = size outer_stride None outer_stride = stride shape = outer_size kwargs = kwargs strides = outer_stride kwargs storage_offset = storage_offset kwargs device = device kwargs layout = layout kwargs requires_grad = requires_grad kwargs dtype = dtype torch Tensor _make_wrapper_subclass cls shape kwargs __init__ extra outer_size=None outer_stride=None = extra = extra classmethod __torch_dispatch__ cls func types args kwargs kwargs None kwargs = args_a = pytree tree_map_only SubclassWithMeta lambda x x args kwargs_a = pytree tree_map_only SubclassWithMeta lambda x x kwargs out_a = func args_a kwargs_a isinstance out_a torch Tensor assert isinstance args SubclassWithMeta SubclassWithMeta out_a extra=args extra out_a __tensor_flatten__ store extra meta extra extra staticmethod __tensor_unflatten__ inner_tensors meta outer_size outer_stride assert isinstance meta dict = inner_tensors pull out extra meta extra = meta extra type torch Tensor assert outer_size None assert outer_stride None SubclassWithMeta extra outer_size outer_stride SubclassWithCustomMetadataGuard torch Tensor staticmethod __new__ cls extra outer_size=None outer_stride=None outer_size None outer_size = size outer_stride None outer_stride = stride shape = outer_size kwargs = kwargs strides = outer_stride kwargs storage_offset = storage_offset kwargs device = device kwargs layout = layout kwargs requires_grad = requires_grad kwargs dtype = dtype torch Tensor _make_wrapper_subclass cls shape kwargs __init__ extra outer_size=None outer_stride=None = extra = extra classmethod __torch_dispatch__ cls func types args kwargs kwargs None kwargs = args_a = pytree tree_map_only SubclassWithCustomMetadataGuard lambda x x args kwargs_a = pytree tree_map_only SubclassWithCustomMetadataGuard lambda x x kwargs out_a = func args_a kwargs_a isinstance out_a torch Tensor assert isinstance args SubclassWithCustomMetadataGuard SubclassWithCustomMetadataGuard out_a extra=args extra out_a classmethod __metadata_guard__ cls meta meta Define custom metadata guard logic only looks bar determine metadata equivalence This more purposefully more lax than default guard behavior meta extra bar == meta extra bar __tensor_flatten__ store extra meta extra extra staticmethod __tensor_unflatten__ inner_tensors meta outer_size outer_stride assert isinstance meta dict = inner_tensors pull out extra meta extra = meta extra type torch Tensor assert outer_size None assert outer_stride None SubclassWithCustomMetadataGuard extra outer_size outer_stride SubclassWithSubclassInnerTensor torch Tensor staticmethod __new__ cls extra outer_size=None outer_stride=None outer_size None outer_size = size outer_stride None outer_stride = stride shape = outer_size kwargs = kwargs strides = outer_stride kwargs storage_offset = storage_offset kwargs device = device kwargs layout = layout kwargs requires_grad = requires_grad kwargs dtype = dtype torch Tensor _make_wrapper_subclass cls shape kwargs __init__ extra outer_size=None outer_stride=None = inner_sub = SubclassWithMeta + extra=extra classmethod __torch_dispatch__ cls func types args kwargs kwargs None kwargs = args_a = pytree tree_map_only SubclassWithSubclassInnerTensor lambda x x args kwargs_a = pytree tree_map_only SubclassWithSubclassInnerTensor lambda x x kwargs out_a = func args_a kwargs_a isinstance out_a torch Tensor assert isinstance args SubclassWithSubclassInnerTensor SubclassWithSubclassInnerTensor out_a extra=args inner_sub extra out_a __tensor_flatten__ inner_sub None staticmethod __tensor_unflatten__ inner_tensors meta outer_size outer_stride assert meta None = inner_tensors extra = inner_tensors inner_sub extra type torch Tensor assert outer_size None assert outer_stride None SubclassWithSubclassInnerTensor extra outer_size outer_stride defines custom __eq__ __hash__ registered pytree constant type CustomConstantType __init__ b = b = b __eq__ other custom eq ignores b == other __hash__ custom hash ignores b hash pytree register_constant CustomConstantType TestGuardSerializationBase torch _inductor test_case TestCase _tracefunc frame event arg event = call _frame_state None _frame_state = _FrameState f_locals=dict frame f_locals f_globals=frame f_globals f_code=frame f_code f_builtins=frame f_builtins _test_serialization guard_type fn args kwargs kwargs might contain callable generates kwargs torch _dynamo reset kwarg_gen_fn = kwargs get _gen_fn kwarg_gen_fn None kwargs = kwarg_gen_fn _frame_state = None sys settrace _tracefunc isinstance fn torch nn Module fn = fn forward try fn args kwargs finally sys settrace None assert _frame_state None Set f_locals regenerated kwargs handle exhausted input iterators NB This super janky might cause unforeseen problems kwarg_gen_fn None kwargs = kwarg_gen_fn key _frame_state f_locals keys key kwargs isinstance kwargs key Iterator _frame_state f_locals key = kwargs key guard_filter_fn guards ret = g guard_type == guard_type guard_type g derived_guard_types g guards assertTrue any ret ret ref_gm = None loaded_gm = None transform instructions list code_options dict str object The goal here reimplement dynamo just have simplified version extract state symbolic convert Should work all cases should work simple functions test file nonlocal ref_gm nonlocal loaded_gm torch _dynamo convert_frame initial_global_state = torch _C _dynamo guards GlobalStateGuard tracer = InstructionTranslator instructions _frame_state f_code _frame_state f_locals _frame_state f_globals _frame_state f_builtins fn __closure__ torch overrides _get_current_function_mode_stack code_options torch _dynamo lookup_backend eager one_graph=False export=False export_constraints=None frame_state=None speculation_log=SpeculationLog exn_vt_stack=ExceptionStack distributed_state=None package=None compile_context CompileContext CompileId frame_id= frame_compile_id= tracing tracer output tracing_context tracer set_current_tx get_metrics_context dynamo_timed tracer run ref_gm = CheckFunctionManager _frame_state f_code tracer output guard_filter_fn=guard_filter_fn guard_manager check_fn_manager = CheckFunctionManager _frame_state f_code tracer output guard_filter_fn=guard_filter_fn save_guards=True guards_state = check_fn_manager guards_state _cached_guards_state = guards_state _cached_f_code = _frame_state f_code assertIsNotNone guards_state guards_state = torch _dynamo package load_guards_state guards_state loaded_gm = torch _dynamo package load_guard_manager guards_state _frame_state f_code _frame_state f_globals try transform_code_object _frame_state f_code transform finally torch _dynamo convert_frame initial_global_state = None _frame_state = None assertIsNotNone ref_gm assertIsNotNone loaded_gm ref_gm loaded_gm _test_check_fn ref loaded inputs expected assertIsInstance inputs dict assertEqual ref check inputs expected assertEqual ref check inputs loaded check inputs torch _dynamo config patch strict_precompile True TestGuardSerialization TestGuardSerializationBase test_function_locals foo x x + fn x g g x + _test_serialization TENSOR_MATCH fn torch randn foo test_tensor_match f x torch Tensor x + ref loaded = _test_serialization TENSOR_MATCH f torch ones dtype=torch float _test_check_fn ref loaded x torch randn dtype=torch float True _test_check_fn ref loaded x torch randn dtype=torch float False _test_check_fn ref loaded x torch randn dtype=torch float False _test_check_fn ref loaded x None False test_not_present_in_generic_dict Module torch nn Module forward x torch Tensor x + m = Module fn x m x ref loaded = _test_serialization NOT_PRESENT_IN_GENERIC_DICT fn torch ones dtype=torch float _test_check_fn ref loaded m m True m forward = types MethodType lambda x x + m _test_check_fn ref loaded m m False test_hasattr_serialization Module torch nn Module __init__ super __init__ = forward x torch Tensor hasattr x + x + m = Module fn x m x ref loaded = _test_serialization HASATTR fn torch randn _test_check_fn ref loaded m m True delattr m _test_check_fn ref loaded m m False test_type_match LocalModule torch nn Module forward x torch Tensor x + m = LocalModule fn m x m x assertRaisesRegex TypeError Please define global scope _test_serialization TYPE_MATCH fn m torch randn m = GlobalModule ref loaded = _test_serialization TYPE_MATCH fn m torch randn _test_check_fn ref loaded m m True _test_check_fn ref loaded m GlobalModule True _test_check_fn ref loaded m torch nn Module False test_tensor_subclass_metadata_match LocalSubclass torch Tensor staticmethod __new__ cls outer_size=None outer_stride=None outer_size None outer_size = size outer_stride None outer_stride = stride shape = outer_size kwargs = kwargs strides = outer_stride kwargs storage_offset = storage_offset kwargs device = device kwargs layout = layout kwargs requires_grad = requires_grad kwargs dtype = dtype torch Tensor _make_wrapper_subclass cls shape kwargs __init__ outer_size=None outer_stride=None = classmethod __torch_dispatch__ cls func types args kwargs kwargs None kwargs = args_a = pytree tree_map_only LocalSubclass lambda x x args kwargs_a = pytree tree_map_only LocalSubclass lambda x x kwargs out_a = func args_a kwargs_a isinstance out_a torch Tensor LocalSubclass out_a out_a __tensor_flatten__ None staticmethod __tensor_unflatten__ inner_tensors meta outer_size outer_stride assert meta None = inner_tensors type torch Tensor assert outer_size None assert outer_stride None LocalSubclass outer_size outer_stride fn x x === example subclass defined locally error === local_sub = LocalSubclass torch randn assertRaisesRegex PackageError Please define global scope _test_serialization TENSOR_SUBCLASS_METADATA_MATCH fn local_sub === example subclass None extra metadata === torch testing _internal two_tensor TwoTensor tt = TwoTensor torch randn torch randn ref loaded = _test_serialization TENSOR_SUBCLASS_METADATA_MATCH fn tt _test_check_fn ref loaded x tt True _test_check_fn ref loaded x torch ones_like tt True used below convenience returned func accepts some metadata whether guard expected pass given subclass type _get_meta_test_check_fn ref loaded subclass_type _f meta expected ref=ref loaded=loaded subclass_type=subclass_type _test_check_fn ref loaded x subclass_type torch randn extra=meta expected _f === example subclass extra metadata === extra_meta = foo bar hello sub = SubclassWithMeta torch randn extra=extra_meta ref loaded = _test_serialization TENSOR_SUBCLASS_METADATA_MATCH fn sub _test_check_fn ref loaded x sub True check_with_meta = _get_meta_test_check_fn ref loaded SubclassWithMeta check_with_meta dict extra_meta True different foo check_with_meta foo bar hello False different bar check_with_meta foo bar world False === example subclass custom metadata guard logic === sub = SubclassWithCustomMetadataGuard torch randn extra=extra_meta ref loaded = _test_serialization TENSOR_SUBCLASS_METADATA_MATCH fn sub _test_check_fn ref loaded x sub True check_with_meta = _get_meta_test_check_fn ref loaded SubclassWithCustomMetadataGuard check_with_meta dict extra_meta True different foo custom logic says okay check_with_meta foo bar hello True different bar check_with_meta foo bar world False === example subclass subclass inner tensor === sub = SubclassWithSubclassInnerTensor torch randn extra=extra_meta ref loaded = _test_serialization TENSOR_SUBCLASS_METADATA_MATCH fn sub _test_check_fn ref loaded x sub True check_with_meta = _get_meta_test_check_fn ref loaded SubclassWithSubclassInnerTensor check_with_meta dict extra_meta True different foo check_with_meta foo bar hello False different bar check_with_meta foo bar world False test_equals_match fn x y CustomConstantType registered pytree constant so should result EQUALS_MATCH guard x y torch zeros torch ones x = CustomConstantType y = CustomConstantType CustomConstantType ref loaded = _test_serialization EQUALS_MATCH fn x y _test_check_fn ref loaded x x y y True custom __eq__ says CustomConstantType == CustomConstantType _test_check_fn ref loaded x CustomConstantType y CustomConstantType CustomConstantType True _test_check_fn ref loaded x x y False _test_check_fn ref loaded x x y CustomConstantType CustomConstantType False test_constant_match === bool constant === fn x y y x + x + x = torch randn y = True ref loaded = _test_serialization CONSTANT_MATCH fn x y _test_check_fn ref loaded x x y y True _test_check_fn ref loaded x torch randn y True True _test_check_fn ref loaded x torch randn y True True guard should fail different y value _test_check_fn ref loaded x torch randn y False False === None constant === fn x y y None x + x + x = torch randn y = None ref loaded = _test_serialization CONSTANT_MATCH fn x y _test_check_fn ref loaded x x y y True _test_check_fn ref loaded x torch randn y None True _test_check_fn ref loaded x torch randn y None True guard should fail non-None y value _test_check_fn ref loaded x torch randn y False _test_check_fn ref loaded x torch randn y True False === int constant === fn x y x + y x = torch randn y = ref loaded = _test_serialization CONSTANT_MATCH fn x y _test_check_fn ref loaded x x y y True _test_check_fn ref loaded x torch randn y True _test_check_fn ref loaded x torch randn y True guard should fail different y value _test_check_fn ref loaded x torch randn y False test_nn_module fn m x m x m = GlobalModule x = torch randn config setting controls whether NN_MODULE guard installed patch torch _dynamo config inline_inbuilt_nn_modules False we don t support NN_MODULE because adds ID_MATCH guard we don t support serialization assertRaisesRegex PackageError NN_MODULE guard cannot serialized _test_serialization NN_MODULE fn m x test_class_match fn x usage context manager installs FUNCTION_MATCH guard torch no_grad y = x y x = torch randn we don t support FUNCTION_MATCH because adds ID_MATCH guard we don t support serialization assertRaisesRegex PackageError CLASS_MATCH guard cannot serialized _test_serialization CLASS_MATCH fn x test_closure_match fn x usage global function installs CLOSURE_MATCH guard global_func x x = torch randn we don t support CLOSURE_MATCH because adds FUNCTION_MATCH guard we don t support serialization assertRaisesRegex PackageError CLOSURE_MATCH guard cannot serialized _test_serialization CLOSURE_MATCH fn x test_sequence_length tuple input installs SEQUENCE_LENGTH guard fn t x t + x t = tuple torch randn _ range x = torch randn ref loaded = _test_serialization SEQUENCE_LENGTH fn t x _test_check_fn ref loaded x x t t True _test_check_fn ref loaded x torch randn t tuple torch randn _ range True different types tuple same length shouldn t fail SEQUENCE_LENGTH guard should fail separate TYPE_MATCH guard isn t tested here _test_check_fn ref loaded x torch randn t True different length tuple _test_check_fn ref loaded x torch randn t tuple torch randn _ range False test_tuple_iterator_len fn t x len list t x x + tup = x = torch randn func generate kwargs useful avoiding iterator exhaustion issues _gen_kwargs tup=tup x=x t iter tup x x ref loaded = _test_serialization TUPLE_ITERATOR_LEN fn _gen_fn=_gen_kwargs same tuple _test_check_fn ref loaded t iter tup x x True _test_check_fn ref loaded t iter tup x torch randn True same length tuple different contents _test_check_fn ref loaded t iter x x True _test_check_fn ref loaded t iter x torch randn True different tuple lengths _test_check_fn ref loaded t iter x x False _test_check_fn ref loaded t iter x torch randn False _test_check_fn ref loaded t iter x x False _test_check_fn ref loaded t iter x torch randn False test_range_iterator_match fn x r y = x val r y = x + val y x = torch randn _gen_kwargs x=x x x r iter range ref loaded = _test_serialization RANGE_ITERATOR_MATCH fn _gen_fn=_gen_kwargs same range _test_check_fn ref loaded x x r iter range True _test_check_fn ref loaded x torch randn r iter range True equivalent even different end _test_check_fn ref loaded x x r iter range True _test_check_fn ref loaded x torch randn r iter range True different start _test_check_fn ref loaded x x r iter range False _test_check_fn ref loaded x torch randn r iter range False different end resulting different values _test_check_fn ref loaded x x r iter range False _test_check_fn ref loaded x torch randn r iter range False different step _test_check_fn ref loaded x x r iter range False _test_check_fn ref loaded x torch randn r iter range False test_dict_version fn x pytree tree_leaves x + assertRaisesRegex PackageError DICT_VERSION guard cannot serialized _test_serialization DICT_VERSION fn t torch randn test_dict_contains fn x x __contains__ t x t + torch ones ref loaded = _test_serialization DICT_CONTAINS fn t torch randn _test_check_fn ref loaded x t torch randn True _test_check_fn ref loaded x False _test_check_fn ref loaded x t torch randn d torch randn True test_bool_match fn x b b x + x + ref loaded = _test_serialization BOOL_MATCH fn torch randn True _test_check_fn ref loaded x torch randn b True True _test_check_fn ref loaded x torch randn b False False _test_check_fn ref loaded x torch randn b None False test_none_match fn x b b None x + x + ref loaded = _test_serialization NONE_MATCH fn torch randn None _test_check_fn ref loaded x torch randn b None True _test_check_fn ref loaded x torch randn b False False _test_check_fn ref loaded x torch randn b True False test_id_match fn x x + id x assertRaisesRegex PackageError ID_MATCH guard cannot serialized _test_serialization ID_MATCH fn torch randn torch _dynamo config patch caching_precompile=True test_id_match_with_config fn x x + id x ref loaded = _test_serialization ID_MATCH fn torch randn _test_check_fn ref loaded x torch randn True fn x usage context manager installs CLASS_MATCH guard torch no_grad y = x y ref loaded = _test_serialization CLASS_MATCH fn torch randn _test_check_fn ref loaded x torch randn True test_dispatch_key_set_match fn x dks dks has CPU torch sin x + torch sin x - x = torch randn dks = torch _C _dispatch_keys x ref loaded = _test_serialization DISPATCH_KEY_SET_MATCH fn x dks _test_check_fn ref loaded x x dks dks True x = torch randn device= meta dks = torch _C _dispatch_keys x _test_check_fn ref loaded x x dks dks False test_dual_level fn x torch autograd forward_ad dual_level x + x = torch randn ref loaded = _test_serialization DUAL_LEVEL fn x _test_check_fn ref loaded x x True torch autograd forward_ad dual_level _test_check_fn ref loaded x x False test_functorch_stack_match Test when functorch stack empty fn x torch func jvp torch sin x x x = torch randn ref loaded = _test_serialization FUNCTORCH_STACK_MATCH fn x _test_check_fn ref loaded x x True torch _functorch vmap vmap_increment_nesting error _test_check_fn ref loaded x x False fn x g x torch vmap torch func grad torch sin x torch vmap g x x = torch randn ref loaded = _test_serialization FUNCTORCH_STACK_MATCH fn x _test_check_fn ref loaded x x True torch _functorch eager_transforms grad_increment_nesting _test_check_fn ref loaded x x False Test when there more than functorch layers Simulate case where torch compile nested inside eager transforms Case vmap fn x x sum ref = loaded = None run x nonlocal ref loaded Turn off automatic dynamic shape so functionalization doesn t produce extra SymInt serialize torch _dynamo config patch automatic_dynamic_shapes=False ref loaded = _test_serialization FUNCTORCH_STACK_MATCH fn x fn x torch vmap run x _test_check_fn ref loaded x x False torch _functorch vmap vmap_increment_nesting error _test_check_fn ref loaded x x True torch _functorch vmap vmap_increment_nesting error _test_check_fn ref loaded x x False torch _functorch eager_transforms grad_increment_nesting _test_check_fn ref loaded x x False Case grad x = torch randn ref = loaded = None torch func grad run x _test_check_fn ref loaded x x False torch _functorch eager_transforms grad_increment_nesting _test_check_fn ref loaded x x True torch _functorch eager_transforms grad_increment_nesting _test_check_fn ref loaded x x False torch _functorch vmap vmap_increment_nesting error _test_check_fn ref loaded x x False Case jvp + vmap x = torch randn ref = loaded = None fn x torch func jvp torch sin x x torch func jvp torch vmap run x x _test_check_fn ref loaded x x False torch _functorch eager_transforms jvp_increment_nesting torch _functorch vmap vmap_increment_nesting error _test_check_fn ref loaded x x True torch _functorch vmap vmap_increment_nesting error torch _functorch eager_transforms jvp_increment_nesting _test_check_fn ref loaded x x False Case functionalize x = torch randn ref = loaded = None torch func functionalize run x _test_check_fn ref loaded x x False torch _C _functorch _func_increment_nesting True try _test_check_fn ref loaded x x True finally torch _C _functorch _func_decrement_nesting torch _functorch eager_transforms jvp_increment_nesting _test_check_fn ref loaded x x False Case vmap + grad fn x x sum x = torch randn ref = loaded = None torch vmap torch func grad run x _test_check_fn ref loaded x x False torch _functorch vmap vmap_increment_nesting error torch _functorch eager_transforms grad_increment_nesting _test_check_fn ref loaded x x True torch _functorch eager_transforms grad_increment_nesting torch _functorch vmap vmap_increment_nesting error _test_check_fn ref loaded x x False torch _functorch vmap vmap_increment_nesting error _test_check_fn ref loaded x x False torch _functorch eager_transforms grad_increment_nesting _test_check_fn ref loaded x x False test_duplicate_input fn x x_ x + x_ x = torch randn ref loaded = _test_serialization DUPLICATE_INPUT fn x x _test_check_fn ref loaded x x x_ x True _test_check_fn ref loaded x x x_ torch randn False test_weakref_alive mod = torch nn Linear bias=False p mod parameters p grad = torch rand_like p opt = torch optim SGD mod parameters lr= fn params = opt _init_group opt param_groups params params sum assertRaisesRegex PackageError WEAKREF_ALIVE guard cannot serialized torch set_grad_enabled False _test_serialization WEAKREF_ALIVE fn test_mapping_keys_check fn mp mp + mp = types MappingProxyType torch randn b torch randn ref loaded = _test_serialization MAPPING_KEYS_CHECK fn mp _test_check_fn ref loaded mp mp True _test_check_fn ref loaded mp types MappingProxyType b torch randn torch randn False _test_check_fn ref loaded mp types MappingProxyType torch randn False test_dict_keys_match fn x ret = k x ret += x k ret x = torch randn b torch randn ref loaded = _test_serialization DICT_KEYS_MATCH fn x _test_check_fn ref loaded x x True _test_check_fn ref loaded x b torch randn torch randn False _test_check_fn ref loaded x torch randn False torch _dynamo config patch skip_nnmodule_hook_guards False test_empty_nn_module_hooks_dict Module torch nn Module forward x torch Tensor x + m = Module fn x m x x = torch ones dtype=torch float ref loaded = _test_serialization EMPTY_NN_MODULE_HOOKS_DICT fn x _test_check_fn ref loaded m m x x True h = m register_forward_hook lambda args kwargs None _test_check_fn ref loaded m m x x False h remove h = m register_forward_pre_hook lambda args kwargs None _test_check_fn ref loaded m m x x False h remove h = m register_backward_hook lambda args kwargs None _test_check_fn ref loaded m m x x False h remove test_grad_mode fn x x + x = torch randn torch enable_grad ref loaded = _test_serialization GRAD_MODE fn x torch no_grad _test_check_fn ref loaded x x False torch enable_grad _test_check_fn ref loaded x x True test_grad_mode_loading fn x x + x = torch randn torch enable_grad ref _ = _test_serialization GRAD_MODE fn x torch no_grad Ensure guards state loading affected current global grad mode guards_state = pickle loads _cached_guards_state check_fn_manager = CheckFunctionManager _cached_f_code guards_state output_graph shape_code_parts=guards_state shape_code_parts loaded = check_fn_manager guard_manager _test_check_fn ref loaded x x False test_deterministic_algorithms fn x x + deterministic_restore = torch are_deterministic_algorithms_enabled try x = torch randn torch use_deterministic_algorithms True ref loaded = _test_serialization DETERMINISTIC_ALGORITHMS fn x torch use_deterministic_algorithms False _test_check_fn ref loaded x x False torch use_deterministic_algorithms True _test_check_fn ref loaded x x True finally torch use_deterministic_algorithms deterministic_restore test_torch_function_state fn x x + x = torch randn LocalTorchFunctionMode TorchFunctionMode __torch_function__ func types args= kwargs=None kwargs None kwargs = func args kwargs GlobalTorchFunctionMode ref loaded = _test_serialization TORCH_FUNCTION_STATE fn x _test_check_fn ref loaded x x True _test_check_fn ref loaded x x False GlobalTorchFunctionMode torch _C DisableTorchFunction _test_check_fn ref loaded x x False assertRaisesRegex PackageError defined local scope Please define global scope LocalTorchFunctionMode ref loaded = _test_serialization TORCH_FUNCTION_STATE fn x unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_fsdp_training_state torch distributed fsdp _fully_shard _fsdp_common TrainingState torch distributed fsdp _fully_shard _fsdp_param_group FSDPParamGroup param_group = FSDPParamGroup params List nn Parameter torch nn Linear module nn Module None mesh_info FSDPMeshInfo None post_forward_mesh_info Optional FSDPMeshInfo torch device cpu device torch device None shard_placement_fn Optional Callable None mp_policy MixedPrecisionPolicy None offload_policy OffloadPolicy fn x param_group use_training_state TrainingState FORWARD param_group _training_state == TrainingState FORWARD x + x - x = torch randn torch enable_grad ref loaded = _test_serialization FSDP_TRAINING_STATE fn x torch no_grad _test_check_fn ref loaded x x False torch enable_grad _test_check_fn ref loaded x x True test_default_device device = torch get_default_device fn x x + x = torch randn try torch set_default_device cpu ref loaded = _test_serialization DEFAULT_DEVICE fn x torch set_default_device meta _test_check_fn ref loaded x x False torch set_default_device cpu _test_check_fn ref loaded x x True finally torch set_default_device device test_shape_env fn x x + x = torch randn ref loaded = _test_serialization SHAPE_ENV fn x _test_check_fn ref loaded x x True x = torch randn torch _dynamo mark_dynamic x min= max= ref loaded = _test_serialization SHAPE_ENV fn x _test_check_fn ref loaded x torch randn True _test_check_fn ref loaded x torch randn True _test_check_fn ref loaded x torch randn False _test_check_fn ref loaded x torch randn False x = torch randn torch _dynamo mark_dynamic x min= max= ref loaded = _test_serialization SHAPE_ENV fn x _test_check_fn ref loaded x torch randn True _test_check_fn ref loaded x torch randn True _test_check_fn ref loaded x torch randn False _test_check_fn ref loaded x torch randn False test_builtin_match fn x usage getattr here installs BUILTIN_MATCH guard s = getattr x shape noqa B x + s x = torch randn ref loaded = _test_serialization BUILTIN_MATCH fn x _test_check_fn ref loaded x x True getattr_original = getattr getattr_new args kwargs getattr_original args kwargs builtins_dict = __builtins__ isinstance __builtins__ dict __builtins__ __dict__ builtins_dict getattr = getattr_new try _test_check_fn ref loaded x x False finally builtins_dict getattr = getattr_original test_skipped_objects foo pass Module torch nn Module __init__ super __init__ code = foo __code__ foo = foo p = torch nn Parameter torch randn forward x z = x + p parameters z += p z m = Module ref loaded = _test_serialization TENSOR_MATCH m torch randn _test_check_fn ref loaded m x torch randn True test_bound_method_input MyModule torch nn Module forward foo x x + id type foo m = MyModule ref loaded = _test_serialization TYPE_MATCH m MyClass add torch randn _test_check_fn ref loaded m foo MyClass add x torch randn True test_bound_methods_missing MyClass __getstate__ raise NotImplementedError add x x + foo x torch Tensor y list MyClass assert len y == x + ref loaded = _test_serialization TYPE_MATCH foo torch randn MyClass _test_check_fn ref loaded x torch randn y MyClass True test_bound_methods_empty foo x y assert callable y x + ref loaded = _test_serialization TYPE_MATCH foo torch randn MyClassNotSerializable add _test_check_fn ref loaded x torch randn y MyClassNotSerializable add True test_ddp_module torch distributed dist dist is_available skipTest Torch distributed available torch nn parallel DistributedDataParallel DDP tmpfile = tempfile NamedTemporaryFile dist init_process_group backend= gloo rank= world_size= init_method=f file tmpfile name try ddp_model = DDP GlobalNestedModule foo ddp x ddp x x = torch randn package = CompilePackage foo torch _dynamo optimize package=package guard_filter_fn=lambda gs x guard_type CLOSURE_MATCH ID_MATCH CLASS_MATCH x gs foo ddp_model x assertEqual len package _codes foo __code__ guarded_codes torch _dynamo package load_guards_state package _codes foo __code__ guarded_codes guards_state finally dist destroy_process_group test_dict_keys_serialization d = foo x y k y x += k x ref loaded = _test_serialization TYPE_MATCH foo torch randn d keys _test_check_fn ref loaded x torch randn y d keys True test_unserializable_sharded_tensor torch distributed dist dist is_available skipTest Torch distributed available tmpfile = tempfile NamedTemporaryFile dist init_process_group backend= gloo rank= world_size= init_method=f file tmpfile name try ChunkShardingSpec = dist _shard sharding_spec ChunkShardingSpec ShardedTensor = dist _shard sharded_tensor ShardedTensor tensor = torch arange dtype=torch int local_tensor = torch unsqueeze torch cat tensor tensor + sharding_dim = sharding_spec = ChunkShardingSpec dim=sharding_dim placements= rank cpu st = ShardedTensor _init_from_local_tensor local_tensor sharding_spec foo inputs inputs x + ref loaded = _test_serialization TENSOR_MATCH foo Inputs torch randn st _test_check_fn ref loaded inputs Inputs torch randn st True finally dist destroy_process_group test_function_with_wrong_fqn foo inputs inputs x + x = torch randn ref loaded = _test_serialization TENSOR_MATCH foo Inputs x global_func_wrong_fqn _test_check_fn ref loaded inputs Inputs x global_func_wrong_fqn True test_c d_work torch distributed dist dist is_available skipTest Torch distributed available Work = dist distributed_c d Work DummyWork Work __init__ should_succeed=True super __init__ _done = False _should_succeed = should_succeed is_completed _done is_success _should_succeed wait timeout=None _done = True _should_succeed raise RuntimeError DummyWork failed result _should_succeed raise RuntimeError DummyWork failed dummy_result foo inputs inputs x + x = torch randn ref loaded = _test_serialization TENSOR_MATCH foo Inputs x DummyWork _test_check_fn ref loaded inputs Inputs x DummyWork True test_unused_weakref foo inputs inputs x + x = torch randn ref loaded = _test_serialization TENSOR_MATCH foo Inputs x weakref ref x _test_check_fn ref loaded inputs Inputs x weakref ref x True test_unused_stream torch cuda is_available skipTest CUDA available foo inputs inputs x + x = torch randn ref loaded = _test_serialization TENSOR_MATCH foo Inputs x torch cuda Stream _test_check_fn ref loaded inputs Inputs x torch cuda Stream True test_unused_process_group torch distributed dist dist is_available skipTest Torch distributed available foo inputs inputs x + tmpfile = tempfile NamedTemporaryFile dist init_process_group backend= gloo init_method=f file tmpfile name rank= world_size= try pg = dist distributed_c d _get_default_group x = torch randn ref loaded = _test_serialization TENSOR_MATCH foo Inputs x pg _test_check_fn ref loaded inputs Inputs x pg True finally dist destroy_process_group test_unserializable_submodule foo mod x mod x x = torch randn mod = GlobalNestedModule ModuleNotSerializable ref loaded = _test_serialization TENSOR_MATCH foo mod x _test_check_fn ref loaded mod mod x x True test_closure_var_missing captured = torch randn bar x x + captured foo f x f x x = torch randn ref loaded = _test_serialization TENSOR_MATCH foo bar x _test_check_fn ref loaded f bar x x True test_bound_method_patched_forward forward x x + m = FlatModule m_forward = m forward m forward = forward foo f x assert callable f f x x = torch randn ref loaded = _test_serialization TYPE_MATCH foo m_forward x _test_check_fn ref loaded f m_forward x x True test_guard_on_key_order_with_cache foo x mod y mod d values x = y x x = torch randn d = e b e- ref loaded = _test_serialization DICT_KEYS_MATCH foo x ModWithDict d _test_check_fn ref loaded x x d ModWithDict b e- e False SimpleModule torch nn Module __init__ c super __init__ c = c p = torch nn Parameter torch randn forward x z = x + p parameters z += p z IS_MACOS torch testing _internal common_fsdp FSDPTestMultiThread torch _dynamo config patch strict_precompile True TestGuardSerializationFSDP TestGuardSerializationBase FSDPTestMultiThread setUp TestGuardSerializationBase setUp FSDPTestMultiThread setUp test_guard_serialization_fsdp_module torch distributed _tensor distribute_tensor Replicate torch distributed device_mesh init_device_mesh torch distributed fsdp fully_shard mesh = init_device_mesh str torch get_default_device m = SimpleModule m = fully_shard m mesh=mesh inputs = distribute_tensor torch randn mesh Replicate ref loaded = _test_serialization TENSOR_MATCH m inputs _test_check_fn ref loaded m x inputs True __name__ == __main__ torch _dynamo test_case run_tests run_tests