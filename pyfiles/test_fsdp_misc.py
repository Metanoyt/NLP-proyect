Owner s oncall distributed functools os sys warnings collections namedtuple contextlib nullcontext copy deepcopy itertools chain typing Any torch torch distributed dist torch distributed fsdp _traversal_utils traversal_utils torch nn nn torch distributed fsdp CPUOffload FlatParameter FullyShardedDataParallel FSDP ShardingStrategy torch distributed fsdp _flat_param _FSDP_USE_UNSAFE_SETATTR torch distributed fsdp _runtime_utils HOMOGENEOUS_ATTR_NAMES torch distributed fsdp wrap always_wrap_policy ModuleWrapPolicy transformer_auto_wrap_policy torch distributed optim _apply_optimizer_in_backward torch nn TransformerDecoderLayer TransformerEncoderLayer torch nn parallel DistributedDataParallel DDP torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp _assert_module_states DEVICEInitMode FSDPInitMode FSDPTest FSDPTestMultiThread MLP NestedWrappedModule TransformerWithSharedParams torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests TEST_WITH_DEV_DBG_ASAN dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit device_type = acc type acc = torch accelerator current_accelerator True cpu MyModel nn Module __init__ - None super __init__ = nn Linear b = nn Linear forward x y b x + y TestFSDPMiscMultiProcess FSDPTest property world_size property process_group dist distributed_c d _get_default_group skip_if_lt_x_gpu parametrize use_index True False test_fsdp_device_id use_index Tests FSDP ` ` device_id ` ` argument - Wrapping CPU module should move module GPU matching ` ` device_id ` ` - Wrapping GPU module already GPU matching ` ` device_id ` ` should raise error - Wrapping GPU module already GPU passing GPU device without specifying device ID i e ` ` torch device cuda ` ` warns dev_id = torch accelerator current_device_index use_index torch device device_type torch accelerator current_device_index _check_device_matches module device_id Checks ` ` FlatParameter ` ` s ` ` module ` ` have device matching ` ` device_id ` ` devices = p device p module parameters isinstance p FlatParameter assert len devices assertEqual len devices found_device = devices pop use_index isinstance device_id torch device device = torch device device_type device_id device = device_id assertEqual found_device device Check FSDP parameters moved ` device_id ` CPU module nested_wrapped_module = NestedWrappedModule init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_NEVER fsdp_kwargs= device_id dev_id _check_device_matches nested_wrapped_module dev_id Check specifying ` device_id ` GPU module already device does raise error nested_wrapped_module = NestedWrappedModule init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE fsdp_kwargs= device_id dev_id _check_device_matches nested_wrapped_module dev_id Check passing ` torch device cuda ` GPU module warns regex = does have explicit index context = assertWarnsRegex expected_warning=UserWarning expected_regex=regex context nested_wrapped_module = NestedWrappedModule init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE fsdp_kwargs= device_id torch device device_type _check_device_matches nested_wrapped_module torch device device_type torch accelerator current_device_index skip_if_lt_x_gpu test_fsdp_zero _eval_with_prefetch Test FSDP validation SHARD_GRAD_OP forward_prefetch Mnist nn Module __init__ - None super __init__ conv = nn Conv d conv = nn Conv d dropout = nn Dropout dropout = nn Dropout fc = nn Linear fc = nn Linear ln = nn LayerNorm forward x y x = conv x x = torch nn functional relu x x = conv x x = torch nn functional relu x x = torch nn functional max_pool d x x = dropout x x = torch flatten x x = ln x x = fc x x = torch nn functional relu x x = dropout x x = fc x output = torch nn functional log_softmax x dim= loss = torch nn functional cross_entropy output y loss model = Mnist device=device_type model = Mnist device=device_type model load_state_dict model state_dict fsdp_model = FSDP model sharding_strategy=ShardingStrategy SHARD_GRAD_OP forward_prefetch=True use_orig_params=True auto_wrap_policy=ModuleWrapPolicy nn Linear nn Conv d ddp_model = torch nn parallel DistributedDataParallel model fsdp_opt = torch optim SGD fsdp_model parameters lr= e- ddp_opt = torch optim SGD ddp_model parameters lr= e- seed = rank + torch manual_seed seed torch get_device_module device_type manual_seed seed losses = grads = i range x = torch randn device=device_type requires_grad_ y = torch randint low= high= size= device=device_type model opt fsdp_model fsdp_opt ddp_model ddp_opt seed = rank + i torch manual_seed seed torch get_device_module device_type manual_seed seed loss = model x y sum losses append loss loss backward opt step grads append x grad opt zero_grad assert torch allclose losses losses assert torch allclose grads grads losses clear grads clear torch no_grad fsdp_model eval ddp_model eval _ range x = torch randn device=device_type requires_grad_ y = torch randint low= high= size= device=device_type fsdp_loss = fsdp_model x y ddp_loss = ddp_model x y assert torch allclose fsdp_loss ddp_loss fsdp_model train ddp_model train i range x = torch randn device=device_type requires_grad_ y = torch randint low= high= size= device=device_type model opt fsdp_model fsdp_opt ddp_model ddp_opt seed = rank + i torch manual_seed seed torch get_device_module device_type manual_seed seed loss = model x y sum losses append loss loss backward opt step grads append x grad opt zero_grad assert torch allclose losses losses assert torch allclose grads grads losses clear grads clear skip_if_lt_x_gpu parametrize use_second_layer True False parametrize sharding_strategy ShardingStrategy NO_SHARD None test_fsdp_module_no_compute_grad use_second_layer sharding_strategy When use_second_layer=True b involved forward computation does receive grad backward Otherwise b involved forward computation MyModel nn Module __init__ - None super __init__ = nn Linear b = nn Linear forward x y out = x use_second_layer out = b y out out out fsdp = FSDP MyModel device=device_type sharding_strategy=sharding_strategy auto_wrap_policy=always_wrap_policy x = torch randn device=device_type y = torch randn device=device_type _ range use_second_layer _ = fsdp x y = fsdp x y loss = sum loss backward receives grad b does a_grad = fsdp module _handle flat_param grad b_grad = fsdp module b _handle flat_param grad assertIsNotNone a_grad assertIsNone b_grad skip_if_lt_x_gpu test_fsdp_not_all_outputs_used_in_loss run_subtests sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP ShardingStrategy NO_SHARD _test_fsdp_not_all_outputs_used_in_loss _test_fsdp_not_all_outputs_used_in_loss sharding_strategy ShardingStrategy MyModule nn Module __init__ - None super __init__ lin = nn Linear lin = nn Linear forward x = lin x b = lin x b _check_resharded fsdp_module handle = fsdp_module _handle handle param = handle flat_param handle uses_sharded_strategy full_param = param _full_param_padded assertEqual full_param storage size assertEqual param data_ptr param _local_shard data_ptr _check_equal local fsdp FSDP summon_full_params fsdp p p zip fsdp parameters local parameters torch testing assert_close p p fsdp_ctor = functools partial FSDP sharding_strategy=sharding_strategy m = MyModule device=device_type m_local = deepcopy m local_m = m_local prev_params = p clone p m_local parameters m lin = fsdp_ctor m lin m = fsdp_ctor m _check_equal m_local m opt = torch optim SGD m parameters lr= e- opt_local = torch optim SGD local_m parameters lr= e- i range t = torch ones device=device_type b = m t local_a local_b = local_m t i use both params loss computation Later b will go unused we check grads same local training loss = b sum loss_local = local_a local_b sum loss = sum loss_local = local_a sum loss backward loss_local backward _check_resharded m opt step opt_local step _check_equal m_local m Ensure least some change previous params otherwise above check would vacuously true assertTrue any torch equal p p p p zip prev_params m_local parameters prev_params = p clone p local_m parameters opt zero_grad opt_local zero_grad dist barrier skip_if_lt_x_gpu test_fsdp_optim_overlap_no_use_orig_params_error fsdp_overlap = FSDP MyModel device=device_type auto_wrap_policy=always_wrap_policy use_orig_params=False optim_cls = torch optim SGD optim_kwargs = lr _apply_optimizer_in_backward optimizer_class=optim_cls params=fsdp_overlap parameters optimizer_kwargs=optim_kwargs register_hook=False inp = torch randn device=device_type assertRaisesRegex RuntimeError only supported use_orig_params=True fsdp_overlap inp inp skip_if_lt_x_gpu test_fsdp_optimizer_overlap torch manual_seed cpu_offload True False offload = CPUOffload offload_params=cpu_offload model = MyModel device=device_type model_overlap = deepcopy model fsdp = FSDP model device=device_type auto_wrap_policy=always_wrap_policy use_orig_params=True cpu_offload=offload fsdp_overlap = FSDP model_overlap device=device_type auto_wrap_policy=always_wrap_policy use_orig_params=True cpu_offload=offload optim_cls = torch optim SGD optim_kwargs = lr _apply_optimizer_in_backward optimizer_class=optim_cls params=fsdp_overlap parameters optimizer_kwargs=optim_kwargs register_hook=False p fsdp_overlap parameters assert hasattr p _in_backward_optimizers optim = optim_cls fsdp parameters optim_kwargs Verify params initially equal p p zip fsdp parameters fsdp_overlap parameters assertEqual p p FSDP summon_full_params fsdp_overlap fsdp_overlap_prev_params = n p clone n p fsdp_overlap named_parameters i range inp = torch randn device=device_type torch no_grad inp_clone = inp clone fsdp inp inp sum backward fsdp_overlap inp_clone inp_clone sum backward optim step optim zero_grad Overlapped optimizer FSDP module should have sharded_grad None fsdp_unit FSDP fsdp_modules fsdp_overlap handle = fsdp_unit _handle handle handle_grad = handle sharded_grad assertEqual None handle_grad Overlapped FSDP sharded_grad None Note FSDP without optimizer overlap won t set sharded_grad None until next pre-forward since needs run FSDP specific logic picks up set_to_none=True has been called gradients have been otherwise set None Verify parameters different than prev iteration FSDP summon_full_params fsdp_overlap with_grads=True n p n_prev p_prev zip fsdp_overlap named_parameters fsdp_overlap_prev_params assertNotEqual p p_prev f n_prev Params iter i same previous iter Verify overlap non overlapped same FSDP summon_full_params fsdp_overlap FSDP summon_full_params fsdp n_overlap p_overlap n p zip fsdp_overlap named_parameters fsdp named_parameters assertEqual n_overlap n assertEqual p p_overlap f Rank rank Params equal iteration i n_overlap - p vs p_overlap assertEqual None p grad f Expected param n grad None assertEqual None p_overlap grad f Expected param n_overlap grad None fsdp_overlap_prev_params = n p clone n p fsdp_overlap named_parameters skip_if_lt_x_gpu test_fsdp_cpu_training Tests FSDP training CPU gloo_pg = dist new_group backend= gloo ss ShardingStrategy NO_SHARD ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP torch manual_seed model = MyModel ref_model = DDP deepcopy model process_group=gloo_pg model = FSDP model sharding_strategy=ss auto_wrap_policy=always_wrap_policy process_group=gloo_pg device_id=torch device cpu ref_optim = torch optim Adam ref_model parameters lr= e- optim = torch optim Adam model parameters lr= e- torch manual_seed + rank inp = torch randn _ range losses = _model _optim ref_model ref_optim model optim loss = _model inp inp sum losses append loss loss backward _optim step _optim zero_grad assertEqual losses losses skip_if_lt_x_gpu test_fsdp_cpu_init_stays_on_cpu Move me MT test once warning logging backward collective issue resolved Tests passing CPU module FSDP preserves wrapped module CPU after FSDP initialization albeit after logging warning FSDP moves CPU input GPU before forward torch accelerator set_device_index rank regex = passed-in ` module ` CPU context = assertWarnsRegex expected_warning=UserWarning expected_regex=regex context nested_wrapped_module = NestedWrappedModule init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_NEVER fsdp_model = FSDP nested_wrapped_module process_group devices = p device p fsdp_model parameters assertEqual len devices assertEqual torch device cpu devices pop fsdp_model = fsdp_model device=device_type Ensure fwd + backward can performed after moving CUDA CPU input also tests input correctly moved appropriate CUDA device inp = fsdp_model module get_input device=torch device cpu fsdp_model inp sum backward skip_if_lt_x_gpu test_cpu_init_with_sync_module_states Tests passing ` ` sync_module_states=True ` ` raises error CPU module since synchronization requires GPU communication while additionally passing ` ` device_id ` ` does raise error even when model has CPU buffers init_nested_wrapped_module NestedWrappedModule init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_NEVER assertRaisesRegex ValueError The module has CPU parameters buffers when ` sync_module_states=True ` FSDP init_nested_wrapped_module process_group sync_module_states=True Check ` device_id ` ` sync_module_states=True ` works nested_wrapped_module = init_nested_wrapped_module nested_wrapped_module buf = nn Buffer torch ones device= cpu rank nested_wrapped_module module buf = nn Buffer torch ones device= cpu rank nested_wrapped_module = FSDP nested_wrapped_module process_group auto_wrap_policy=ModuleWrapPolicy nn Linear device_id=torch accelerator current_device_index sync_module_states=True Each rank s buffers should s since rank source they should GPU since we specified ` device_id ` assertEqual nested_wrapped_module buf device torch device device_type torch accelerator current_device_index assertEqual nested_wrapped_module buf torch zeros assertEqual nested_wrapped_module module module buf device torch device device_type torch accelerator current_device_index assertEqual nested_wrapped_module module module buf torch zeros TestFSDPMiscMultiThread FSDPTestMultiThread property world_size property process_group dist distributed_c d _get_default_group skip_if_lt_x_gpu test_fsdp_namedtuple MyModule nn Module __init__ - None super __init__ lin = nn Linear forward x x m = MyModule device=device_type m = FSDP m t = torch ones device=device_type requires_grad=True MyOutputType = namedtuple MyOutputType b c d defaults= t t t t inp = MyOutputType out = m inp Ensure hooks registered x out assertNotEqual list x _backward_hooks values TODO we should check backward param resharded well blocked https github com pytorch pytorch issues https github com pytorch pytorch issues skip_if_lt_x_gpu test_device_id_auto_wrap Tests ` ` auto_wrap_policy ` ` propagates ` ` device_id ` ` all nested FSDP instances run_subtests use_callable False True _test_device_id_auto_wrap _test_device_id_auto_wrap use_callable bool module_classes = TransformerEncoderLayer TransformerDecoderLayer use_callable auto_wrap_policy = functools partial transformer_auto_wrap_policy transformer_layer_cls=module_classes auto_wrap_policy = ModuleWrapPolicy module_classes fsdp_kwargs = auto_wrap_policy auto_wrap_policy device_id torch accelerator current_device_index fsdp_model = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE fsdp_kwargs fsdp_module FSDP fsdp_modules fsdp_model assertEqual fsdp_module compute_device torch device device_type torch accelerator current_device_index skip_if_lt_x_gpu test_fsdp_device_id_cpu_offload Tests FSDP when specifying both ` ` device_id ` ` parameter CPU offloading run_subtests use_orig_params False True _test_fsdp_device_id_cpu_offload _test_fsdp_device_id_cpu_offload use_orig_params bool MyModel nn Module __init__ - None super __init__ seq = nn Sequential nn Linear nn Linear lin = nn Linear forward x lin seq x model = MyModel Choose wrapping policy such there nested FSDP instances parent FSDP instance has managed parameters auto_wrap_policy = ModuleWrapPolicy nn Sequential fsdp_model = FSDP model auto_wrap_policy=auto_wrap_policy cpu_offload=CPUOffload offload_params=True device_id=torch accelerator current_device_index use_orig_params=use_orig_params cpu_device = torch device cpu handle traversal_utils _get_fsdp_handles fsdp_model assertEqual handle flat_param device cpu_device skip_if_lt_x_gpu test_module_device_mismatches_device_id Tests specifying ` ` device_id ` ` argument FSDP GPU module does match GPU device ID raises error TODO override FSDP MT Thread _run set instead here every test torch accelerator set_device_index rank context = assertRaisesRegex ValueError f device_type rank vs device_type rank = nullcontext context NestedWrappedModule init process_group FSDPInitMode RECURSIVE Move wrapped modules CUDA before wrapping FSDP device_init_mode=DEVICEInitMode DEVICE_BEFORE Should raise error since rank given ` device_id= ` when model cuda fsdp_kwargs= device_id skip_if_lt_x_gpu test_cpu_gpu_module Tests CPU + GPU module supported device_id passed errors device_id torch accelerator set_device_index rank CPUGPUModule nn Module __init__ - None super __init__ = nn Linear device=device_type b = nn Linear cpu_gpu = CPUGPUModule fsdp = FSDP cpu_gpu device_id=torch accelerator current_device_index param fsdp parameters assertEqual param device torch device torch accelerator current_device_index without device_id we hit error assertRaisesRegex RuntimeError please pass device_id FSDP CPUGPUModule skip_if_lt_x_gpu test_fsdp_ignored_module_meta torch accelerator set_device_index rank CPUGPUModule nn Module __init__ - None super __init__ = nn Linear b = nn Linear torch device meta m = CPUGPUModule m = FSDP m device_id=self rank ignored_modules= m use_orig_params=True meta_device = torch device meta assertEqual meta_device next m parameters device Test param_init_fn torch device meta m = CPUGPUModule m = FSDP m device_id=torch accelerator current_device_index ignored_modules= m use_orig_params=True param_init_fn=lambda m m to_empty device=torch accelerator current_device_index recurse=False assertEqual meta_device next m parameters device skip_if_lt_x_gpu test_fsdp_device_id_no_move_ignored_params_and_bufs CPUGPUModule nn Module __init__ - None super __init__ = nn Linear b = nn Linear buf = torch nn Buffer torch ones m = CPUGPUModule m = FSDP m device_id=self rank ignored_modules= m use_orig_params=True ignored_params = m parameters ignored_bufs = m buffers t chain ignored_params ignored_bufs assertEqual torch device cpu t device skip_if_lt_x_gpu test_multigpu_module Module multiple GPUs wrapped FSDP should raise error MultiGPUModule nn Module __init__ rank super __init__ rank = rank = nn Linear cuda rank b = nn Linear cuda rank + dist get_world_size assertRaisesRegex RuntimeError FSDP only supports single device modules FSDP MultiGPUModule rank skip_if_lt_x_gpu test_no_params Test device_id cpu init work module has no params they effective noops ensure FSDP does assume module has parameters during init TODO override FSDP MT Thread _run set instead here every test torch accelerator set_device_index rank Test CPU no_params = nn ReLU FSDP no_params Test CUDA no_params = nn ReLU device=device_type FSDP no_params Test CPU + device_id no_params = nn ReLU FSDP no_params device_id=torch accelerator current_device_index For modules no params wrong device_id will raise error about inconsistency between compute_device device_id since compute_device computed torch cuda current_device when there no params no_params = nn ReLU device=device_type context = assertRaisesRegex ValueError f Inconsistent cuda rank vs cuda rank = nullcontext context FSDP no_params device_id= skip_if_lt_x_gpu test_fsdp_same_model_across_ranks FSDP broadcasts model rank ensure starts off same values MyModel nn Module __init__ rank super __init__ Seed via rank make model different across ranks torch manual_seed rank torch get_device_module device_type manual_seed rank lin = nn Linear bias=False buffer = nn Buffer torch ones rank m = MyModel rank device=device_type _assert_module_states m process_group=self process_group assert_fn=self assertNotEqual Passing sync_module_states into FSDP makes model same during init fsdp = FSDP m sync_module_states=True fsdp summon_full_params fsdp _assert_module_states fsdp process_group=self process_group assert_fn=self assertEqual sync_module_states also works CPU module device_id passed m = MyModel rank _assert_module_states m process_group=self process_group assert_fn=self assertNotEqual Passing sync_module_states into FSDP makes model same during init fsdp = FSDP m device_id=torch accelerator current_device_index sync_module_states=True fsdp summon_full_params fsdp _assert_module_states fsdp process_group=self process_group assert_fn=self assertEqual skip_if_lt_x_gpu test_homogeneous_attributes Tests passing heterogeneous values attributes designated homogeneous raises error Manually construct list verify against global list homogeneous attribute names all_attr_name_and_values = _use_orig_params False True limit_all_gathers False True _use_full_prec_in_eval False True assertEqual attr_name_and_values attr_name_and_values all_attr_name_and_values HOMOGENEOUS_ATTR_NAMES run_subtests attr_name_and_values all_attr_name_and_values _test_homogeneous_attributes _test_homogeneous_attributes attr_name_and_values tuple str Any Any model = NestedWrappedModule init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE attr_name = attr_name_and_values _use_full_prec_in_eval == attr_name model module = FSDP model module os environ FSDP_USE_FULL_PREC_IN_EVAL = fsdp_model = FSDP model fsdp_kwargs_inner = attr_name lstrip _ attr_name_and_values fsdp_kwargs_outer = attr_name lstrip _ attr_name_and_values model module = FSDP model module fsdp_kwargs_inner fsdp_model = FSDP model fsdp_kwargs_outer Run forward trigger lazy initialization error assertRaisesRegex ValueError f Expects one homogeneous value attr_name inp = fsdp_model module get_input torch device device_type fsdp_model inp skip_if_lt_x_gpu test_fsdp_unsupported_module_cls regex = r FSDP will all-gather parameters containers do implement forward model = nn ModuleList MLP torch device cpu _ range assertWarnsRegex UserWarning regex FSDP model device_id=device_type model = nn ModuleDict MLP torch device cpu MLP torch device cpu assertWarnsRegex UserWarning regex FSDP model TestFSDPMiscWorldSize FSDPTestMultiThread property world_size - int skip_if_lt_x_gpu test_world_size_ _sharding_strategy_warning Tests FSDP issues warning when switches using ` ` NO_SHARD ` ` when world size warning_prefix = FSDP switching use ` NO_SHARD ` instead If user already passes ` NO_SHARD ` then there should warning warnings catch_warnings record=True w warnings simplefilter always trigger all warnings FSDP nn Linear device=device_type sharding_strategy=ShardingStrategy NO_SHARD warning w assertTrue warning category UserWarning str warning message startswith warning_prefix Check warning issued warning_suffix = since world size - Pass ` FULL_SHARD ` ` None ` expected_regex_full_shard = warning_prefix + + str ShardingStrategy FULL_SHARD + warning_suffix assertWarnsRegex UserWarning expected_regex_full_shard FSDP nn Linear device=device_type sharding_strategy=ShardingStrategy FULL_SHARD assertWarnsRegex UserWarning expected_regex_full_shard FSDP nn Linear device=device_type - Pass ` SHARD_GRAD_OP ` expected_regex_shard_grad_op = warning_prefix + + str ShardingStrategy SHARD_GRAD_OP + warning_suffix assertWarnsRegex UserWarning expected_regex_shard_grad_op FSDP nn Linear device=device_type sharding_strategy=ShardingStrategy SHARD_GRAD_OP skip_if_lt_x_gpu test_training_device_mismatch_errors Tests when training starts FSDP parameters expected device then informative error raised This applies both no parameter CPU offloading parameter CPU offloading Incorrectly moving CPU - GPU model = torch nn Linear fsdp_model = FSDP model inp = torch randn assertRaisesRegex RuntimeError An FSDP-managed module unexpectedly has parameters cpu Make sure move module cuda before training fsdp_model inp Incorrectly moving CPU - GPU model = torch nn Linear fsdp_model = FSDP model cpu_offload=CPUOffload offload_params=True fsdp_model torch device device_type inp = torch randn assertRaisesRegex RuntimeError An FSDP-managed module parameter CPU offloading enabled has parameters cuda Make sure move module CPU when offloading parameters fsdp_model inp skip_if_lt_x_gpu test_unsafe_setattr Tests environment variable using unsafe setattr gates expected run_subtests use_orig_params False True _test_unsafe_setattr _test_unsafe_setattr use_orig_params bool called_setattr_override = False SetattrLinear nn Module __init__ in_dim int out_dim int device torch device - None super __init__ weight = nn Parameter torch randn in_dim out_dim device=device forward x torch Tensor - torch Tensor x weight __setattr__ name str value Any - None nonlocal called_setattr_override called_setattr_override = True super __setattr__ name value Construct FSDP module without changing any environment variables run forward which triggers both unsharded sharded view setting module = SetattrLinear torch device device_type fsdp_module = FSDP module use_orig_params=use_orig_params inp = torch randn device=torch device device_type called_setattr_override = False fsdp_module inp assertTrue called_setattr_override Repeat unsafe setattr explicitly enabled os environ _FSDP_USE_UNSAFE_SETATTR = module = SetattrLinear torch device device_type fsdp_module = FSDP module use_orig_params=use_orig_params called_setattr_override = False fsdp_module inp assertFalse called_setattr_override Repeat unsafe setattr explicitly disabled os environ _FSDP_USE_UNSAFE_SETATTR = module = SetattrLinear torch device device_type fsdp_module = FSDP module use_orig_params=use_orig_params called_setattr_override = False fsdp_module inp assertTrue called_setattr_override instantiate_parametrized_tests TestFSDPMiscMultiThread instantiate_parametrized_tests TestFSDPMiscMultiProcess __name__ == __main__ run_tests