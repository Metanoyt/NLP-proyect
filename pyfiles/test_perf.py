Owner s module inductor contextlib re unittest mock patch functorch torch torch _inductor config config torch autograd torch _inductor metrics torch _inductor compile_fx compile_fx compile_fx_inner torch _inductor test_case TestCase InductorTestCase torch _inductor utils run_and_get_code ######################## Explanation Tests ######################## These tests all testing memory accesses TorchInductor They intended deterministic performance tests The expect tests all measuring number memory bytes read written code Inductor has generated If test failing because number became smaller feel free lower On other hand test failing because number became larger means your change leading more memory accesses test That may still aceeptable aware you likely lowering performance setting Defines all kernels tests torch testing _internal inductor_utils GPU_TYPE HAS_GPU_AND_TRITON torch testing _internal triton_utils requires_gpu_and_triton set so metrics appear torch _logging set_logs inductor_metrics=True HAS_GPU_AND_TRITON triton manual triton language tl manual torch testing _internal triton_utils add_kernel aten = torch ops aten compile_but_use_eager gm example_inputs inner_compile gm args kwargs compile_fx_inner gm args kwargs gm compile_fx gm example_inputs inner_compile=inner_compile count_numel f args Assumes all inputs fp metrics reset torch compile f backend=compile_but_use_eager args print metrics nodes_num_elem str metrics num_bytes_accessed count_numel_train f args Assumes all inputs fp metrics reset f = torch compile f backend=compile_but_use_eager out = f args res = o out res += o mean res backward print metrics nodes_num_elem str metrics num_bytes_accessed DEVICE = GPU_TYPE T size dtype=torch float device=DEVICE grad=False torch randn size dtype=dtype device=device requires_grad=grad TI size mx= dtype=torch int device=DEVICE torch randint mx size dtype=dtype device=device TestCase InductorTestCase device = DEVICE NumBytesMetricTests TestCase Primarily used sanity testing num_bytes_accessed metrics correct test_pointwise f x x cos inp = T assertExpectedInline count_numel f inp f x y x + y inp = T T assertExpectedInline count_numel f inp f x y x + y inp = T T assertExpectedInline count_numel f inp f x x + x inp = T assertExpectedInline count_numel f inp f x x + x t inp = T assertExpectedInline count_numel f inp f b c cos b sin + c sin inp = T T T assertExpectedInline count_numel f inp test_reduction f x x sum dim= inp = T assertExpectedInline count_numel f inp f x x sum dim= inp = T assertExpectedInline count_numel f inp test_extern f x torch mm x x inp = T assertExpectedInline count_numel f inp f b torch mm b inp = T T assertExpectedInline count_numel f inp f x x = x cos x = torch mm x x x = x cos x inp = T assertExpectedInline count_numel f inp f x = x cos b = x sin x = torch mm b x inp = T assertExpectedInline count_numel f inp test_cat f b torch cat sin b sin inp = T T assertExpectedInline count_numel f inp f b torch cat b inp = T T assertExpectedInline count_numel f inp f b torch cat cos b inp = T T assertExpectedInline count_numel f inp f torch cat cos sin inp = T assertExpectedInline count_numel f inp f b torch cat torch mm b sin inp = T T assertExpectedInline count_numel f inp f b c torch cat + b + c + + inp = T T T assertExpectedInline count_numel f inp f b c d e torch cat + b + c + d + e + + inp = T _ range assertExpectedInline count_numel f inp f b torch cat sum dim= b sum dim= + inp = T T assertExpectedInline count_numel f inp test_cat_pointwise f b torch cat torch softmax dim=- torch softmax b dim=- inp = T T assertExpectedInline count_numel f inp f b torch cat torch softmax dim=- torch softmax b dim=- cos inp = T T assertExpectedInline count_numel f inp Should turn into pointwise even only some inputs pointwise f b out = torch cat cos torch mm b b out cos inp = T T assertExpectedInline count_numel f inp Should turn into pointwise all inputs pointwise f b out = torch cat torch mm torch mm b b out cos inp = T T assertExpectedInline count_numel f inp f b out = torch cat b out cos inp = T T assertExpectedInline count_numel f inp f b b = b cos torch cat b inp = T T assertExpectedInline count_numel f inp f b = torch constant_pad_nd torch cat b inp = T T assertExpectedInline count_numel f inp patch object config split_cat_fx_passes False patch object config pre_grad_fusion_options batch_linear batch_linear_lhs batch_layernorm batch_tanh batch_relu batch_sigmoid patch object config post_grad_fusion_options test_cat_pointwise_many_complex_inputs f inputs input = torch nn functional gelu val val inputs torch cat input + inp = T _ range assertExpectedInline count_numel f inp patch object config split_cat_fx_passes False patch object config pre_grad_fusion_options batch_linear batch_linear_lhs batch_layernorm batch_tanh batch_relu batch_sigmoid patch object config post_grad_fusion_options test_cat_pointwise_many_simple_inputs f inputs input = torch nn functional relu val val inputs torch cat input + inp = T _ range assertExpectedInline count_numel f inp patch object config max_pointwise_cat_inputs test_cat_pointwise_config_option f b torch cat + b + + inp = T T assertExpectedInline count_numel f inp test_index f b b inp = T TI mx= assertExpectedInline count_numel f inp FusionTests TestCase Tests things can fused into single kernel test_horizontal_reduction_pointwise f b = sum dim= c = cos b c inp = T assertExpectedInline count_numel f inp test_horizontal_reduction_reduction f b = sum dim= c = amax dim= b c inp = T assertExpectedInline count_numel f inp test_horizontal_reduction_pointwise f b c = sum dim= b = b cos b + c inp = T T assertExpectedInline count_numel f inp test_horizontal_reduction_outer_pointwise f b c = sum dim= b = b cos b + c inp = T T assertExpectedInline count_numel f inp test_horizontal_sum_pw_broadcast f b = sum dim= keepdim=True b = b cos b inp = T T assertExpectedInline count_numel f inp test_vertical_sum_pw f = cos = sum dim= cos inp = T assertExpectedInline count_numel f inp test_norm_chain f b = sum dim= keepdim=True = b b = sum dim= keepdim=True = b b = sum dim= keepdim=True = b inp = T assertExpectedInline count_numel f inp test_softmax_inner f torch softmax dim= inp = T assertExpectedInline count_numel f inp test_layer_norm TODO Suboptimal We shouldn t need save normalization stats mod = torch nn LayerNorm device=self device f x mod x inp = T torch no_grad assertExpectedInline count_numel f inp test_double_softmax f x x = torch softmax x dim= x = torch softmax x dim= x inp = T assertExpectedInline count_numel f inp test_softmax_backward f grad_out out aten _softmax_backward_data grad_out out torch float inp = T T assertExpectedInline count_numel f inp test_neighbor f b - b sum dim=- amax dim= inp = T T assertExpectedInline count_numel f inp test_factory_reduction f = torch ones device=self device b = torch ones device=self device + b sum dim=- inp = assertExpectedInline count_numel f inp test_index_pointwise f b b cos inp = T TI mx= assertExpectedInline count_numel f inp test_index_reduction f b b cos sum dim= inp = T TI mx= assertExpectedInline count_numel f inp test_mutation_fusion f b c = add c b = b add b copy_ b copy_ inp = T T T assertExpectedInline count_numel f inp test_reduction_pointwise_multi_level_reduction hidden_size = layer_norm = torch nn LayerNorm hidden_size GPU_TYPE float torch inference_mode f x scale amax_keep_dim x = layer_norm x dtype=torch float amax = torch amax torch abs x keepdim=amax_keep_dim x_scaled = x scale y = torch nn functional sigmoid x_scaled y amax inp = T hidden_size dtype=torch float T dtype=torch float kernels kernel input = X scale LN scale LN bias output = LN_pointwise X first-level amax split-reduction kernel input = first-level amax output = final amax scale + X hidden_size + LN scale hidden_size + LN bias hidden_size + amax + expected_numel = + hidden_size + hidden_size + + config triton cooperative_reductions expected_numel = assertExpectedInline count_numel f inp True str expected_numel assertExpectedInline count_numel f inp False str expected_numel test_pointwise_multi_level_reduction TODO can optimized having first pointwise kernel leveraging block sizes first-level reduction kernel hidden_size = f x scale amax_keep_dim x = x amax = torch amax torch abs x keepdim=amax_keep_dim x_scaled = x scale y = torch nn functional sigmoid x_scaled y amax inp = T hidden_size dtype=torch float T dtype=torch float compiled_f = torch compile f compiled_f inp True kernels kernel input = X scale output = pointwise X kernel input = X output = first-level amax kernel input = first-level amax output = final amax scale + X hidden_size + amax num_splits + num_splits depends SM architectures expected_numel = + hidden_size + actual_numel_amax_keep_dim = count_numel f inp True actual_numel_amax_no_keep_dim = count_numel f inp False assertEqual actual_numel_amax_keep_dim actual_numel_amax_no_keep_dim assertGreaterAlmostEqual actual_numel_amax_keep_dim str expected_numel test_create_block_mask mk_ d_flex_natten_mask dims kernel_size T H W = dims K_T K_H K_W = kernel_size spatial = H W get_x_y_t idx int - tuple int int int t = idx spatial s = idx spatial x = s W y = s W x y t get_mask b h q_idx kv_idx q_x q_y q_t = get_x_y_t q_idx kv_x kv_y kv_t = get_x_y_t kv_idx kernel_x = q_x clamp K_W W - - K_W kernel_y = q_y clamp K_H H - - K_H kernel_t = q_t clamp K_T T - - K_T hori_mask = kernel_x - kv_x abs = K_W vert_mask = kernel_y - kv_y abs = K_H temp_mask = kernel_t - kv_t abs = K_T hori_mask vert_mask temp_mask get_mask T = H = W = t = h = w = data_size = T H W kernel_size = t h w S = T H W torch nn attention flex_attention create_block_mask mask_mod = mk_ d_flex_natten_mask data_size kernel_size torch compile create_block_mask mask_mod None None S S numel = int count_numel create_block_mask mask_mod None None S S We should writing way less than quadratic amount bytes here With fusion we should only writing linear number bytes assertLess numel S S SchedulerFusionTests TestCase Testing fusion group creation heuristic i e cases where we can t fuse everything into single kernel Disables inductor rematerialization easier reasoning tests classmethod setUpClass cls super setUpClass cls _stack = contextlib ExitStack cls _stack enter_context patch object config realize_opcount_threshold classmethod tearDownClass cls cls _stack close super tearDownClass patch object config pattern_matcher False test_fusion_choice Doesn t matter where we break fusion group here f c = cos d = torch mm c c e = c cos d + e inp = T assertExpectedInline count_numel f inp patch object config pattern_matcher False test_fusion_choice We should materialize e s smaller c e f d f c = cos d = torch mm c c e = c sum dim= f = d + e f inp = T assertExpectedInline count_numel f inp patch object config pattern_matcher False test_fusion_choice We should materialize e c e f d f c = cos d = torch mm c c e = c + f = d + e f e inp = T assertExpectedInline count_numel f inp patch object config pattern_matcher False test_fusion_choice _cpu Fuse nodes same number elements compatible original var ranges buf d d buf d - buf _buf f x w o = x w output = o + output inp = T device= cpu T device= cpu assertExpectedInline count_numel f inp buf _buf d d buf d - buf _buf _buf f x w w o = x w o = x w output = o + o output inp = T device= cpu T device= cpu T device= cpu assertExpectedInline count_numel f inp TilingTests TestCase test_tiling_simple f b + b t inp = T T assertExpectedInline count_numel f inp f b t + b inp = T T assertExpectedInline count_numel f inp test_tiling_three f b c + b permute + c permute inp = T T T assertExpectedInline count_numel f inp MinCutPartitioningTests TestCase test_partitioning_full_remat f x x cos cos cos inp = T grad=True assertExpectedInline count_numel_train f inp test_partitioning_partial_remat f b c d x = + b + c + d x cos cos inp = T grad=True T grad=True T grad=True T grad=True assertExpectedInline count_numel_train f inp test_partitioning_dtype f x x x inp = T grad=True assertExpectedInline count_numel_train f inp patch object functorch compile config max_dist_from_bw test_partitioning_unremat_bw f x torch mm x x new_ones x shape tanh tanh inp = T grad=True assertExpectedInline count_numel_train f inp patch object config pattern_matcher False test_partitioning_unremat_bw f = torch mm = + b = + c = torch mm b c inp = T grad=True assertExpectedInline count_numel_train f inp test_partitioning_keops f b b cos sum dim= inp = T grad=True T grad=True assertExpectedInline count_numel_train f inp test_partitioning_cat f b = torch tanh torch cat b inp = T grad=True T grad=True assertExpectedInline count_numel_train f inp test_partitioning_relu f x torch relu x inp = T grad=True assertExpectedInline count_numel_train f inp test_partitioning_with_view Foo torch autograd Function staticmethod forward ctx x y = x sin x = x cos x = x view ctx save_for_backward x y x = x cos x staticmethod backward ctx gradOut x y = ctx saved_tensors torch mm gradOut x view y f Foo apply inp = T grad=True We do want recompute x cos view chain s materialized backwards assertExpectedInline count_numel_train f inp patch object config pattern_matcher False test_partitioning_long_chain_add f x orig = x _ range x = x x x = torch mm x x x = x x = orig + x orig = x x inp = T grad=True assertExpectedInline count_numel_train f inp unfusible x For purpose noop tests we want inductor fall back eager mode so below we must use aten operator does have decomposition nor lowering aten _lazy_clone x NoopTests TestCase test_noop_clones f b = clone b = unfusible b b inp = T assertExpectedInline count_numel f inp f b = clone c = unfusible b b c assertExpectedInline count_numel f inp test_noop_slice_scatter f b = aten slice_scatter c = unfusible b c inp = T assertExpectedInline count_numel f inp test_noop_dtype_conversion f b = torch ops prims convert_element_type torch float c = unfusible b c inp = T assertExpectedInline count_numel f inp test_noop_device_conversion f b = torch ops prims device_put DEVICE c = unfusible b c inp = T assertExpectedInline count_numel f inp test_noop_int_ops f b = torch ceil c = unfusible b c f d = torch floor e = unfusible d e f f = torch round g = unfusible f g f f = torch pow g = unfusible f g inp = TI assertExpectedInline count_numel f inp assertExpectedInline count_numel f inp assertExpectedInline count_numel f inp assertExpectedInline count_numel f inp test_noop_cat f b = torch cat unfusible b inp = T assertExpectedInline count_numel f inp f b = torch cat c = torch cat b c assertExpectedInline count_numel f inp InplacingTests TestCase test_inplace_scatter f b = cos b = inp = T TI mx= assertExpectedInline count_numel f inp f b out = aten index_put b torch tensor copy_ out inp = T TI mx= assertExpectedInline count_numel f inp f b out = aten _unsafe_index_put b torch tensor copy_ out inp = T TI mx= assertExpectedInline count_numel f inp test_inplace_scatter_noop_view f b b = inp = T TI mx= assertExpectedInline count_numel f inp requires_gpu_and_triton test_inplace_triton_kernel_training triton jit sin_kernel in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask output = tl sin x tl store out_ptr + offsets output mask=mask sin_triton x out n_elements = x numel sin_kernel n_elements x out n_elements BLOCK_SIZE= factory_op = torch empty_like MySin torch autograd Function staticmethod forward ctx x out = factory_op x sin_triton x out ctx save_for_backward out out staticmethod backward ctx grad saved = ctx saved_tensors out = factory_op grad sin_triton saved out out f x MySin apply x x = T grad=True assertExpectedInline count_numel_train f x requires_gpu_and_triton test_triton_kernel_not_fusable_with_users triton jit _sin_kernel in_ptr out_ptr out _ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask output = tl sin x tl store out_ptr + offsets output mask=mask tl store out _ptr + offsets output mask=mask torch _library capture_triton triton_op triton_op mylib sin_kernel mutates_args= sin_kernel x torch Tensor - list torch Tensor n_elements = x numel out = torch empty_like x out = torch empty_like x capture_triton _sin_kernel n_elements x out out n_elements BLOCK_SIZE= out out MySin torch autograd Function staticmethod forward ctx x out saved = tuple torch ops mylib sin_kernel x ctx save_for_backward x saved out staticmethod backward ctx grad x saved = ctx saved_tensors grad saved sigmoid x f x MySin apply x x = T grad=True Important bit saved sigmoid can fused into its consumer mul its producer user triton kernel So we should compute fw save backward will cost extra kernel assertExpectedInline count_numel_train f x requires_gpu_and_triton test_inplace_custom_op_training_two_mutated_inputs torch library custom_op _reinplacing sin_cos mutates_args= out_sin out_cos sin_cos x torch Tensor out_sin torch Tensor out_cos torch Tensor - None out_sin copy_ x sin out_cos copy_ x cos f x out = torch empty_like x out = torch empty_like x sin_cos x out out x clone out out x = T grad=True assertExpectedInline count_numel f x requires_gpu_and_triton test_inplace_custom_op_training torch library custom_op _reinplacing sin mutates_args= result sin x torch Tensor result torch Tensor - None result copy_ x sin factory_op = torch empty_like MySin torch autograd Function staticmethod forward ctx x out = factory_op x sin x out ctx save_for_backward out out staticmethod backward ctx grad saved = ctx saved_tensors out = factory_op grad sin saved out out f x MySin apply x x = T grad=True assertExpectedInline count_numel_train f x requires_gpu_and_triton test_inplace_custom_op torch library _scoped_library mylib FRAGMENT m m define foo Tensor x Tensor out - foo x torch Tensor out torch Tensor - None out copy_ x sin m impl foo foo CompositeExplicitAutograd f x out torch ops mylib foo x out torch ops mylib foo out out torch ops mylib foo out out out x = T out = T compiled_out code = run_and_get_code torch compile f fullgraph=True x out assertEqual compiled_out x sin sin sin Check we allocating minimum number intermediate buffers matches = re findall r empty_strided_\w+\ code assertEqual len matches assertExpectedInline count_numel f x out requires_gpu_and_triton test_inplace_custom_op_intermediate torch library _scoped_library mylib FRAGMENT m m define foo Tensor x Tensor out - foo x torch Tensor out torch Tensor - None out copy_ x sin m impl foo foo CompositeExplicitAutograd f x out out = torch empty_like x torch ops mylib foo x out torch ops mylib foo out out torch ops mylib foo out out out x = T out = T compiled_out code = run_and_get_code torch compile f fullgraph=True x out assertEqual compiled_out x sin sin sin Check we allocating minimum number intermediate buffers matches = re findall r empty_strided_\w+\ code assertEqual len matches assertExpectedInline count_numel f x out requires_gpu_and_triton test_inplace_custom_op_two_mutated_inputs torch library _scoped_library mylib FRAGMENT m m define foo Tensor q Tensor k_cache Tensor b v_cache - Tensor foo q k_cache v_cache k_cache add_ v_cache add_ q + m impl foo foo CompositeExplicitAutograd q = T k_cache = T v_cache = torch rand_like k_cache f x = _ range x = x + torch ops mylib foo q k_cache v_cache x _ code = run_and_get_code torch compile f fullgraph=True Check we allocate intermediate buffers which can reused matches = re findall r empty_strided_\w+\ code assertEqual len matches assertEqual in_out code True assertExpectedInline count_numel f requires_gpu_and_triton test_inplace_triton_kernel_v f x torch Tensor y torch Tensor output = torch zeros_like x n_elements = output numel grid = n_elements add_kernel grid x y output n_elements BLOCK_SIZE= output inp = T T assertExpectedInline count_numel f inp requires_gpu_and_triton test_inplace_triton_kernel_v f x torch Tensor y torch Tensor output = torch zeros_like x n_elements = output numel grid = n_elements tmp = torch add x add_kernel grid x y output n_elements BLOCK_SIZE= output tmp inp = T T assertExpectedInline count_numel f inp requires_gpu_and_triton test_inplace_triton_kernel_v f x torch Tensor y torch Tensor output = torch zeros_like x n_elements = output numel grid = n_elements add_kernel grid x y output n_elements BLOCK_SIZE= x add_ output inp = T T assertExpectedInline count_numel f inp requires_gpu_and_triton test_inplace_triton_kernel_v f x torch Tensor y torch Tensor x_view = x view - output = torch zeros_like x n_elements = output numel grid = n_elements add_kernel grid x y output n_elements BLOCK_SIZE= output = x_view mul output output inp = T T assertExpectedInline count_numel f inp requires_gpu_and_triton test_inplace_triton_kernel_v f x torch Tensor y torch Tensor x_view = x view - output = torch zeros_like x n_elements = output numel grid = n_elements add_kernel grid x y output n_elements BLOCK_SIZE= x_view mul_ output inp = T T assertExpectedInline count_numel f inp requires_gpu_and_triton test_inplace_triton_kernel_v f x torch Tensor y torch Tensor output = torch zeros_like x n_elements = output numel grid = n_elements add_kernel grid x y output n_elements BLOCK_SIZE= output t = T inp = t t view - assertExpectedInline count_numel f inp test_inplace_randperm_scatter scaled_index_add x y scale_y index = torch randperm x shape device=x device y shape out = x index_add_ dim= source=y scale_y index=index out inp = T T T assertExpectedInline count_numel scaled_index_add inp Test cases where we don t do right thing yet WouldBeNiceIfItWorked test_horizontal f b = sum dim= c = cos b c inp = T assertExpectedInline count_numel f inp TODO We aren t fusing outer dim softmaxes test_softmax_outer f torch softmax dim= inp = T assertExpectedInline count_numel f inp TODO The greedy fusion strategy results suboptimal grouping patch object config realize_opcount_threshold test_fusion_choice f b b c = + b d = torch mm c c e = c + b + b f = d + e + b f e inp = T T dtype=torch float T assertExpectedInline count_numel f inp TODO We materialize intermediate we don t unroll reduction test_neighbor f b - b sum dim=- amax dim= inp = T T assertExpectedInline count_numel f inp __name__ == __main__ torch _inductor test_case run_tests HAS_GPU_AND_TRITON run_tests needs= filelock