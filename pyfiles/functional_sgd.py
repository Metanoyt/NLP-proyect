mypy allow-untyped-defs typing Optional torch torch optim _functional F torch Tensor torch distributed optim _deprecation_warning _scripted_functional_optimizer_deprecation_warning __all__ list str = Define TorchScript compatible Functional SGD Optimizer where we use these optimizer functional way Instead using ` param grad ` when updating parameters we explicitly allow distributed optimizer pass gradients ` step ` function In way we could separate gradients parameters allow multithreaded trainer update parameters without data traces accumulating same grad NOTE This should only used distributed optimizer internals meant expose user torch jit script _FunctionalSGD __init__ params list Tensor lr float = e- momentum float = dampening float = weight_decay float = nesterov bool = False maximize bool = False foreach bool = False fused bool = False _allow_empty_param_list bool = False _scripted_functional_optimizer_deprecation_warning stacklevel= defaults = lr lr momentum momentum dampening dampening weight_decay weight_decay nesterov = nesterov maximize = maximize foreach = foreach fused = fused state = torch jit annotate dict torch Tensor dict str torch Tensor len params == _allow_empty_param_list raise ValueError optimizer got empty parameter list NOTE we only have one param_group don t allow user add additional param group s common use case param_group = params params step_param param Tensor grad Optional Tensor Similar step operates single parameter its gradient TODO Once step_param interface robust refactor step call step param each param weight_decay = defaults weight_decay momentum = defaults momentum dampening = defaults dampening lr = defaults lr params = param momentum_buffer_list list Optional Tensor = grads = has_sparse_grad = False grad None grads append grad grad is_sparse has_sparse_grad = True param state state param = state = state param momentum_buffer state momentum_buffer_list append None momentum_buffer_list append state momentum_buffer torch no_grad F sgd params grads momentum_buffer_list weight_decay=weight_decay momentum=momentum lr=lr dampening=dampening nesterov=self nesterov maximize=self maximize has_sparse_grad=has_sparse_grad foreach=self foreach fused=self fused grad_scale=None found_inf=None update momentum_buffer state state = state param momentum_buffer = momentum_buffer_list momentum_buffer None state momentum_buffer = momentum_buffer step gradients list Optional Tensor params = param_group params params_with_grad = grads = momentum_buffer_list list Optional Tensor = lr = defaults lr weight_decay = defaults weight_decay momentum = defaults momentum dampening = defaults dampening len params = len gradients raise ValueError gradients passed does equal size parameters + f Params length len params + f Gradients length len gradients has_sparse_grad = False param gradient zip params gradients gradient None params_with_grad append param grads append gradient gradient is_sparse has_sparse_grad = True param state state param = state = state param momentum_buffer state momentum_buffer_list append None momentum_buffer_list append state momentum_buffer torch no_grad F sgd params_with_grad grads momentum_buffer_list weight_decay=weight_decay momentum=momentum lr=lr dampening=dampening nesterov=self nesterov maximize=self maximize has_sparse_grad=has_sparse_grad foreach=self foreach fused=self fused grad_scale=None found_inf=None update momentum_buffers state i p enumerate params_with_grad state = state p momentum_buffer = momentum_buffer_list i momentum_buffer None state momentum_buffer = momentum_buffer