Owner s oncall distributed sys torch torch distributed dist torch distributed fsdp FullyShardedDataParallel FSDP torch nn Linear torch optim SGD torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest get_devtype torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit device_type = torch device get_devtype TestUnevenParamShard FSDPTest _get_ref_results device model input my_lr torch no_grad Compute one iteration local output weight = model weight T clone device_type v = torch Tensor input rank device_type ref_forward_output_my_rank = torch matmul v weight Compute one iteration global weight update v = torch Tensor input world_size device_type grad = v float sum repeat weight shape div world_size ref_weight_out = weight - grad T my_lr ref_forward_output_my_rank ref_weight_out skip_if_lt_x_gpu test_one_iteration device Test FSDP uneven divide parameter shards model = Linear bias=False input = torch rand world_size my_lr = ref_forward_output_my_rank ref_weight_out = _get_ref_results device model input my_lr model device_type model = FSDP model optim = SGD model parameters lr=my_lr assertTrue len input = world_size in_data = torch Tensor input rank device_type out = model in_data out float sum backward optim step optim zero_grad model summon_full_params model weight_out = model module weight T clone assertEqual ref_forward_output_my_rank out assertEqual ref_weight_out weight_out devices = cuda hpu xpu instantiate_device_type_tests TestUnevenParamShard globals only_for=devices allow_xpu=True __name__ == __main__ run_tests