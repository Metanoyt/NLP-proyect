Owner s oncall export flake noqa copy types unittest dataclasses dataclass typing Dict List Tuple torch torch _dynamo torch _dynamo functional_export dynamo_graph_capture_for_export torch _dynamo test_case run_tests TestCase torch _functorch aot_autograd aot_export_module torch export export torch export experimental _export_forward_backward _sticky_export torch export graph_signature OutputKind torch testing FileCheck torch testing _internal common_utils TEST_CUDA unittest skipIf torch _dynamo is_dynamo_supported dynamo isn t supported TestExperiment TestCase test_joint_basic - None Module torch nn Module __init__ - None super __init__ linear = torch nn Linear loss = torch nn CrossEntropyLoss forward x loss linear x softmax dim= torch tensor m = Module example_inputs = torch randn m example_inputs torch _export config patch use_new_tracer_experimental=True ep = torch export export m example_inputs strict=True joint_ep = _export_forward_backward ep assertExpectedInline str joint_ep graph_module code strip \ forward p_linear_weight p_linear_bias c_lifted_tensor_ x view = torch ops aten view default x x = None permute = torch ops aten permute default p_linear_weight p_linear_weight = None addmm = torch ops aten addmm default p_linear_bias view permute p_linear_bias = permute = None view_ = torch ops aten view default addmm addmm = None _softmax = torch ops aten _softmax default view_ False view_ = None alias = torch ops aten alias default _softmax clone = torch ops aten clone default c_lifted_tensor_ c_lifted_tensor_ = None _log_softmax = torch ops aten _log_softmax default _softmax False _softmax = None alias_ = torch ops aten alias default _log_softmax mul = torch ops aten mul Tensor _log_softmax clone _log_softmax = None sum_ = torch ops aten sum dim_IntList mul mul = None neg = torch ops aten neg default sum_ sum_ = None div = torch ops aten div Scalar neg neg = None full_like = torch ops aten full_like default div pin_memory = False memory_format = torch preserve_format div_ = torch ops aten div Scalar full_like full_like = None neg_ = torch ops aten neg default div_ div_ = None expand = torch ops aten expand default neg_ neg_ = None mul_ = torch ops aten mul Tensor expand clone expand = clone = None alias_ = torch ops aten alias default alias_ alias_ = None exp = torch ops aten exp default alias_ alias_ = None sum_ = torch ops aten sum dim_IntList mul_ True mul_ = torch ops aten mul Tensor exp sum_ exp = sum_ = None sub = torch ops aten sub Tensor mul_ mul_ mul_ = mul_ = None alias_ = torch ops aten alias default alias alias = None mul_ = torch ops aten mul Tensor sub alias_ sub = None sum_ = torch ops aten sum dim_IntList mul_ True mul_ = torch ops aten mul Tensor alias_ sum_ alias_ = sum_ = None sub_ = torch ops aten sub Tensor mul_ mul_ mul_ = mul_ = None view_ = torch ops aten view default sub_ sub_ = None permute_ = torch ops aten permute default view_ mm = torch ops aten mm default permute_ view permute_ = view = None permute_ = torch ops aten permute default mm mm = None sum_ = torch ops aten sum dim_IntList view_ True view_ = None view_ = torch ops aten view default sum_ sum_ = None permute_ = torch ops aten permute default permute_ permute_ = None div permute_ view_ ep = joint_ep run_decompositions assertExpectedInline str ep graph_module code strip \ forward p_linear_weight p_linear_bias c_lifted_tensor_ x view = torch ops aten view default x x = None permute = torch ops aten permute default p_linear_weight p_linear_weight = None addmm = torch ops aten addmm default p_linear_bias view permute p_linear_bias = permute = None view_ = torch ops aten view default addmm addmm = None _softmax = torch ops aten _softmax default view_ False view_ = None alias = torch ops aten alias default _softmax clone = torch ops aten clone default c_lifted_tensor_ c_lifted_tensor_ = None _log_softmax = torch ops aten _log_softmax default _softmax False _softmax = None alias_ = torch ops aten alias default _log_softmax mul = torch ops aten mul Tensor _log_softmax clone _log_softmax = None sum_ = torch ops aten sum dim_IntList mul mul = None neg = torch ops aten neg default sum_ sum_ = None div = torch ops aten div Scalar neg neg = None full_like = torch ops aten full_like default div pin_memory = False memory_format = torch preserve_format div_ = torch ops aten div Scalar full_like full_like = None neg_ = torch ops aten neg default div_ div_ = None expand = torch ops aten expand default neg_ neg_ = None mul_ = torch ops aten mul Tensor expand clone expand = clone = None alias_ = torch ops aten alias default alias_ alias_ = None exp = torch ops aten exp default alias_ alias_ = None sum_ = torch ops aten sum dim_IntList mul_ True mul_ = torch ops aten mul Tensor exp sum_ exp = sum_ = None sub = torch ops aten sub Tensor mul_ mul_ mul_ = mul_ = None alias_ = torch ops aten alias default alias alias = None mul_ = torch ops aten mul Tensor sub alias_ sub = None sum_ = torch ops aten sum dim_IntList mul_ True mul_ = torch ops aten mul Tensor alias_ sum_ alias_ = sum_ = None sub_ = torch ops aten sub Tensor mul_ mul_ mul_ = mul_ = None view_ = torch ops aten view default sub_ sub_ = None permute_ = torch ops aten permute default view_ mm = torch ops aten mm default permute_ view permute_ = view = None permute_ = torch ops aten permute default mm mm = None sum_ = torch ops aten sum dim_IntList view_ True view_ = None view_ = torch ops aten view default sum_ sum_ = None permute_ = torch ops aten permute default permute_ permute_ = None div permute_ view_ test_joint_dynamic - None torch export Dim Module torch nn Module __init__ - None super __init__ y = torch nn Parameter torch randn forward x x = torch ones x shape y + x sum m = Module example_inputs = torch randn m example_inputs ep = torch export export m example_inputs dynamic_shapes= x Dim x strict=True _export_forward_backward ep test_joint_cifar _backwards - None torch nn nn torch nn functional F From Pytorch s CIFAR example https pytorch org tutorials beginner blitz cifar _tutorial html Net nn Module __init__ super __init__ conv = nn Conv d pool = nn MaxPool d conv = nn Conv d fc = nn Linear fc = nn Linear fc = nn Linear loss = nn CrossEntropyLoss forward x labels x = pool F relu conv x x = pool F relu conv x x = torch flatten x flatten all dimensions except batch x = F relu fc x x = F relu fc x x = fc x loss x labels net = Net x = torch randn labels = torch ones dtype=torch int inputs = x labels ep = export net inputs strict=True ep = _export_forward_backward ep test_joint_loss_index Foo torch nn Module __init__ index super __init__ l = torch nn Linear index = index forward x x = l x x = x sum index == x -x detach x detach x inputs = torch randn i ep = export Foo i inputs strict=True ep_joint = _export_forward_backward ep joint_loss_index=i j spec enumerate ep_joint graph_signature output_specs i == j assertTrue spec kind == OutputKind LOSS_OUTPUT assertTrue spec kind = OutputKind LOSS_OUTPUT test_joint_buffer_input_mutations Foo torch nn Module __init__ super __init__ l = torch nn Linear register_buffer buf torch randn loss = torch nn CrossEntropyLoss forward x label x add_ buf x = l x buf add_ loss x label inputs = torch randn torch randint ep = export Foo inputs ep_joint = _export_forward_backward ep assertEqual len ep_joint graph_signature output_specs assertEqual ep_joint graph_signature output_specs kind OutputKind BUFFER_MUTATION assertEqual ep_joint graph_signature output_specs target buf assertEqual ep_joint graph_signature output_specs kind OutputKind USER_INPUT_MUTATION assertEqual ep_joint graph_signature output_specs target x assertEqual ep_joint graph_signature output_specs kind OutputKind LOSS_OUTPUT test_sticky_export Model torch nn Module __init__ super __init__ linear = torch nn Linear forward x linear x Pipeline __init__ model model = model generate args kwargs model args kwargs inp = torch randn p = Pipeline Model orig_forward = p model forward p model forward = _sticky_export p model forward res = p generate inp p model forward = orig_forward res = p generate inp assertTrue torch allclose res res test_sticky_export_dynamic Model torch nn Module __init__ super __init__ linear = torch nn Linear forward x x shape linear x x sin Pipeline __init__ model model = model generate args kwargs model args kwargs inp = torch randn callback args kwargs I think bit weird use forward arg name here so lets just use ShapeCollections flat_args _ = torch utils _pytree tree_flatten args kwargs collections = torch export ShapesCollection arg flat_args isinstance arg torch Tensor collections arg = i torch export Dim AUTO i range len arg shape collections p = Pipeline Model p model forward = _sticky_export p model forward dynamic_shapes_callback=callback _ = p generate inp assertExpectedInline str p model forward _exported_artifact code strip \ forward x x = fx_pytree tree_flatten_spec x _in_spec linear_weight = linear weight linear_bias = linear bias _guards_fn = _guards_fn x _guards_fn = None linear = torch ops aten linear default x linear_weight linear_bias x = linear_weight = linear_bias = None pytree tree_unflatten linear _out_spec test_sticky_export_nested_inp Model torch nn Module __init__ super __init__ linear = torch nn Linear forward inputs linear inputs + linear inputs Pipeline __init__ model model = model generate input_tensor input_tensor inputs = input_tensor input_tensor model inputs=inputs inp = torch randn inp = torch randn p = Pipeline Model orig_forward = p model forward p model forward = _sticky_export p model forward res = p generate input_tensor=inp input_tensor =inp p model forward = orig_forward res = p generate input_tensor=inp input_tensor =inp assertTrue torch allclose res res test_export_add_in_out_info Foo torch nn Module forward dct lst bleh x = dct lst y = dct b lst out_dict = Mutate get new entry there lst_copy = lst copy lst_copy append lst out_dict = x out_dict b = y dct out_dict b bleh lst_copy - out_dict dct = torch randn b torch randn lst = torch randn torch randn torch randn export_inputs = dct lst eager_inputs = copy deepcopy export_inputs torch _dynamo functional_export _dynamo_graph_capture_for_export graph_module = _dynamo_graph_capture_for_export Foo export_inputs export_inputs res_export = graph_module export_inputs export_inputs res_eager = Foo eager_inputs eager_inputs assertEqual res_export res_eager test_export_leaf Foo torch nn Module forward x x sin export_inputs = torch randn eager_inputs = copy deepcopy export_inputs torch _dynamo functional_export _dynamo_graph_capture_for_export graph_module = _dynamo_graph_capture_for_export Foo export_inputs export_inputs res_export = graph_module export_inputs export_inputs res_eager = Foo eager_inputs eager_inputs assertEqual res_export res_eager test_dynamo_graph_capture Foo torch nn Module forward dct lst bleh x = dct lst y = dct b lst out_dict = Mutate get new entry there lst_copy = lst copy lst_copy append lst out_dict = x out_dict b = y dct out_dict b bleh lst_copy - out_dict foo = Foo make_inputs torch randn b torch randn torch randn torch randn torch randn trace_inputs = make_inputs gm = dynamo_graph_capture_for_export foo trace_inputs test_inputs = make_inputs assertEqual gm test_inputs foo test_inputs test_dynamo_graph_capture_custom_pytree_type torch utils _pytree pytree dataclass Bar x torch Tensor y torch Tensor Foo torch nn Module forward bar Bar bar x + bar y foo = Foo make_inputs Bar torch randn torch randn pytree register_dataclass Bar try trace_inputs = make_inputs gm = dynamo_graph_capture_for_export foo trace_inputs test_inputs = make_inputs assertExpectedInline gm _in_shuffle_graph code strip \r\n \ forward arg _ arg _ arg _ arg _ arg _ assertExpectedInline gm code strip \r\n \ forward args_ _tree_leaf_ _tree_leaf_ _tree_leaf_ = pytree tree_leaves args_ L_bar_x L_bar_y = _in_shuffle_graph _tree_leaf_ _tree_leaf_ _tree_leaf_ l_bar_x = L_bar_x l_bar_y = L_bar_y add = l_bar_x + l_bar_y l_bar_x = l_bar_y = None pytree tree_unflatten _out_shuffle_graph _tree_leaf_ _tree_leaf_ _tree_leaf_ add _out_spec assertEqual gm test_inputs foo test_inputs finally pytree _deregister_pytree_node Bar test_dynamo_graph_capture_closure torch export Dim N = outer = torch randn MyModel torch nn Module forward x z = x + outer y = z - s - stacked = torch stack y N dim= N s - reshaped = stacked reshape - N s - N reshaped inps = torch randn ep = dynamo_graph_capture_for_export MyModel inps assertExpectedInline ep _in_shuffle_graph code strip \r\n \ forward arg _ arg _ _tensor_constant = _tensor_constant arg _ _tensor_constant assertExpectedInline ep code strip \r\n \ forward args_ _tree_leaf_ _tree_leaf_ = pytree tree_leaves args_ L_x_ L_outer_ = _in_shuffle_graph _tree_leaf_ _tree_leaf_ l_x_ = L_x_ l_outer_ = L_outer_ z = l_x_ + l_outer_ l_x_ = l_outer_ = None y = z slice None - None slice None None None z = None stacked = torch stack y y y dim = y = None reshaped = stacked reshape - stacked = None pytree tree_unflatten _out_shuffle_graph _tree_leaf_ _tree_leaf_ reshaped _out_spec assertEqual ep inps MyModel inps unittest skipIf TEST_CUDA CUDA available test_dynamo_graph_capture_fx_graph_annotate_overlap_pass DummyOp torch autograd Function staticmethod forward ctx x scalar ctx save_for_backward x x + scalar staticmethod backward ctx grad_out grad_out None mock_fw_compute x fx_traceback annotate compute DummyOp apply x mock_bw_comm x fx_traceback annotate comm DummyOp apply x mock_bw_compute x DummyOp apply x Model torch nn Module forward fw_in bw_in fw_out = mock_fw_compute fw_in bw_in blocks bw_out bw_in = mock_bw_comm bw_in bw_out = mock_bw_compute bw_in fw_out bw_out input_fn inputs = torch rand device= cuda requires_grad=True grad_ins = torch rand device= cuda inputs grad_ins torch device meta model = Model torch fx traceback fx_traceback fx_traceback preserve_node_meta gm = dynamo_graph_capture_for_export model input_fn forward args_ args_ _tree_leaf_ _tree_leaf_ _tree_leaf_ = pytree tree_leaves args_ args_ L_fw_in_ L_bw_in_ = _in_shuffle_graph _tree_leaf_ _tree_leaf_ _tree_leaf_ l_fw_in_ = L_fw_in_ l_bw_in_ = L_bw_in_ fwd_body_ = fwd_body_ bwd_body_ = bwd_body_ fw_out = torch ops higher_order autograd_function_apply fwd_body_ bwd_body_ l_fw_in_ args_tensor_mask = True False non_differentiable_idx = fwd_body_ = bwd_body_ = l_fw_in_ = None bw_in = l_bw_in_ + l_bw_in_ = None bw_out = bw_in + bw_in = None pytree tree_unflatten _out_shuffle_graph _tree_leaf_ _tree_leaf_ _tree_leaf_ fw_out bw_out _out_spec test_inputs = input_fn assertEqual gm test_inputs model test_inputs test_dynamo_graph_capture_default_args Module torch nn Module forward x y= x + y m = Module ep = dynamo_graph_capture_for_export m torch randn test_inputs = torch randn assertEqual ep test_inputs m test_inputs __name__ == __main__ run_tests