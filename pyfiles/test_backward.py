Copyright c Meta Platforms Inc affiliates Owner s oncall distributed copy model_registry MLPModule torch torch distributed pipelining _backward stage_backward stage_backward_input stage_backward_weight torch testing _internal common_device_type instantiate_device_type_tests skipXPUIf torch testing _internal common_utils run_tests TestCase d_hid = batch_size = StageBackwardTests TestCase skipXPUIf True https github com intel torch-xpu-ops issues test_stage_backward device MLP stage module mod = MLPModule d_hid device x = torch randn batch_size d_hid device=device As pipeline stage inputs stage requires gradients x requires_grad_ True target = torch randn batch_size d_hid device=device loss_fn = torch nn MSELoss reduction= sum Make copy ref_mod = copy deepcopy mod device ref_x = x detach requires_grad_ x requires_grad device ref_target = target detach device Forward backward stage manner out = mod x loss = loss_fn out target grad_inputs = stage_backward stage_output=loss output_grads=None input_values= x Run reference ref_out = ref_mod ref_x ref_loss = loss_fn ref_out ref_target ref_loss backward torch testing assert_close grad_inputs ref_x grad Every rank checks gradients name p mod named_parameters ref_p = ref_mod get_parameter name try torch testing assert_close p grad ref_p grad except AssertionError print f Gradient test failed name p grad vs ref_p grad raise test_stage_backward_input device MLP stage module mod = MLPModule d_hid device x = torch randn batch_size d_hid device=device As pipeline stage inputs stage requires gradients x requires_grad_ True target = torch randn batch_size d_hid device=device loss_fn = torch nn MSELoss reduction= sum Make copy ref_mod = copy deepcopy mod device ref_x = x detach requires_grad_ x requires_grad device ref_target = target detach device Forward then backward loss respect inputs out = mod x loss = loss_fn out target dinputs _param_groups = stage_backward_input stage_outputs_or_loss= loss output_grads=None input_values= x weights=mod parameters Run reference ref_out = ref_mod ref_x ref_loss = loss_fn ref_out ref_target ref_loss backward torch testing assert_close x grad ref_x grad torch testing assert_close dinputs ref_x grad _ p mod named_parameters Check weight gradients updated assertEqual p grad None skipXPUIf True https github com intel torch-xpu-ops issues test_stage_backward_weight device MLP stage module mod = MLPModule d_hid device x = torch randn batch_size d_hid device=device As pipeline stage inputs stage requires gradients x requires_grad_ True target = torch randn batch_size d_hid device=device loss_fn = torch nn MSELoss reduction= sum Make copy ref_mod = copy deepcopy mod device ref_x = x detach requires_grad_ x requires_grad device ref_target = target detach device Forward then backward loss respect inputs out = mod x loss = loss_fn out target _dinputs param_groups = stage_backward_input stage_outputs_or_loss= loss output_grads=None input_values= x weights=mod parameters backward loss respect weights stage_backward_weight mod parameters param_groups retain_graph=True Run reference ref_out = ref_mod ref_x ref_loss = loss_fn ref_out ref_target ref_loss backward Every rank checks gradients name p mod named_parameters ref_p = ref_mod get_parameter name try torch testing assert_close p grad ref_p grad except AssertionError print f Gradient test failed name p grad vs ref_p grad raise skipXPUIf True https github com intel torch-xpu-ops issues test_stage_backward_weight_multiple_iters device MLP stage module mod = MLPModule d_hid device inputs = _ range x = torch randn batch_size d_hid device=device inputs append x As pipeline stage inputs stage requires gradients x requires_grad_ True target = torch randn batch_size d_hid device=device loss_fn = torch nn MSELoss reduction= sum Make copy ref_mod = copy deepcopy mod device ref_inputs = x inputs ref_x = x detach requires_grad_ x requires_grad device ref_inputs append ref_x ref_target = target detach device Forward then backward loss respect inputs x inputs out = mod x loss = loss_fn out target _dinputs param_groups = stage_backward_input stage_outputs_or_loss= loss output_grads=None input_values= x weights=mod parameters backward loss respect weights stage_backward_weight mod parameters param_groups Run reference ref_x ref_inputs ref_out = ref_mod ref_x ref_loss = loss_fn ref_out ref_target ref_loss backward Every rank checks gradients name p mod named_parameters ref_p = ref_mod get_parameter name try torch testing assert_close p grad ref_p grad except AssertionError print f Gradient test failed name p grad vs ref_p grad raise test_stage_backward_weight_grad_validation device test_cases = size = lambda torch randn batch_size d_hid device=device torch randn batch_size d_hid device=device size = lambda torch randn batch_size d_hid device=device grad None lambda torch randn batch_size d_hid device=device None description mock_grads_factory test_cases subTest description=description mod = MLPModule d_hid device x = torch randn batch_size d_hid device=device x requires_grad_ True out = mod x loss = torch sum out dinputs param_groups = stage_backward_input stage_outputs_or_loss= loss output_grads=None input_values= x weights=mod parameters Set up mock grads param_group param_groups param_group grads = mock_grads_factory stage_backward_weight mod parameters param_groups devices = cpu cuda hpu xpu instantiate_device_type_tests StageBackwardTests globals only_for=devices allow_xpu=True __name__ == __main__ run_tests