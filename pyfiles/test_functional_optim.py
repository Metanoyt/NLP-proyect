Owner s oncall distributed unittest typing Optional torch torch distributed torch nn nn torch nn functional F torch Tensor torch optim Adam AdamW SGD torch testing _internal common_utils run_tests TestCase MyModule torch nn Module __init__ - None super __init__ torch manual_seed lin = nn Linear bias=False lin = nn Linear bias=False forward t lin F relu lin t dummy showcase custom optimizer registration functional wrapper MyDummyFnOptimizer __init__ params list Tensor lr float = e- betas tuple float float = eps float = e- weight_decay float = _allow_empty_param_list bool = False = lr raise ValueError f Invalid learning rate lr = eps raise ValueError f Invalid epsilon value eps = betas raise ValueError f Invalid beta parameter index betas = betas raise ValueError f Invalid beta parameter index betas weight_decay raise ValueError f Invalid weight_decay value weight_decay defaults = lr lr eps eps beta betas beta betas weight_decay weight_decay len params == _allow_empty_param_list raise ValueError optimizer got empty parameter list step_param param Tensor grad Optional Tensor call custom optimizer step_param implementation torch no_grad raise RuntimeError MyDummyFnOptimizer does support step_param now step gradients list Optional Tensor call custom optimizer step implementation torch no_grad raise RuntimeError MyDummyFnOptimizer does support step now torch distributed is_available torch distributed optim utils functional_optim_map register_functional_optim unittest skipIf torch distributed is_available These testing distributed functions TestFunctionalOptimParity TestCase _validate_parameters params_ params_ p p zip params_ params_ assertEqual p p Dynamo fails compiling python Since passes while compiling actual code under test we disable dynamo here torch _disable_dynamo recursive=False _test_functional_optim_parity optim_cls args kwargs module_optim = MyModule module_functional = MyModule optim_params = module_optim parameters optim = optim_cls optim_params args kwargs functional_optim_cls = functional_optim_map get optim_cls None functional_optim_cls raise ValueError f Functional optimizer implemented optim_cls optim_functional = functional_optim_cls args kwargs _allow_empty_param_list=True hasattr optim_functional step_param raise ValueError f Functional optimizer optim_functional must implement step_param method Initial weights should match _validate_parameters module_optim parameters module_functional parameters Save old parameters verify optimizer modifies them old_module_optim_params = param detach clone param module_optim parameters old_module_functional_params = param detach clone param module_functional parameters t = torch randn _ range module_optim zero_grad module_functional zero_grad Forward + Backward optim_out = module_optim t sum functional_out = module_functional t sum optim_out backward functional_out backward Optimizer step optim step Functional optimizer step_param param module_functional parameters grad = param grad optim_functional step_param param grad Validate parameters equal optim_param functional_param zip module_optim parameters module_functional parameters assertEqual optim_param functional_param Validate parameters modified i optim_param functional_param enumerate zip module_optim parameters module_functional parameters assertNotEqual old_module_optim_params i optim_param assertNotEqual old_module_functional_params i functional_param _test_functional_optim_registration fn_map_key = MyDummyFnOptimizer fn_optim = MyDummyFnOptimizer register_functional_optim fn_map_key fn_optim functional_optim_cls = functional_optim_map get fn_map_key None functional_optim_cls raise ValueError f Functional optimizer registered fn_map_key test_functional_optim_registration _test_functional_optim_registration test_functional_optim_parity_sgd _test_functional_optim_parity SGD e- momentum= weight_decay= test_functional_optim_parity_adam _test_functional_optim_parity Adam e- betas= eps= e- test_functional_optim_parity_adam_w _test_functional_optim_parity AdamW e- betas= eps= e- __name__ == __main__ run_tests