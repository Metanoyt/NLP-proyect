Owner s module nn ruff noqa F contextlib math random unittest io itertools warnings os pickle re copy deepcopy itertools product functools partial collections OrderedDict unittest SkipTest torch torch inf nan torch autograd forward_ad fwAD torch backends cudnn cudnn torch nn nn torch nn functional F torch nn utils rnn rnn_utils torch nn utils clip_grad_norm_ clip_grad_value_ clip_grads_with_norm_ get_total_norm torch nn utils parameters_to_vector vector_to_parameters torch nn utils fusion fuse_conv_bn_weights torch nn utils fusion fuse_linear_bn_weights torch nn Buffer Parameter torch nn parallel _functions Broadcast torch testing _internal common_dtype integral_types get_all_math_dtypes floating_types torch testing _internal common_utils dtype_name freeze_rng_state run_tests TestCase \ skipIfNoLapack skipIfRocm MI _ARCH skipIfRocmArch \ TEST_NUMPY TEST_SCIPY TEST_WITH_CROSSREF TEST_WITH_ROCM \ download_file get_function_arglist load_tests skipIfMPS \ IS_PPC \ parametrize parametrize_test subtest instantiate_parametrized_tests \ skipIfTorchDynamo gcIfJetson set_default_dtype torch testing _internal common_cuda TEST_CUDA TEST_MULTIGPU TEST_CUDNN \ _get_torch_rocm_version torch testing _internal common_nn NNTestCase NewModuleTest CriterionTest \ module_tests criterion_tests loss_reference_fns _create_basic_net \ ctcloss_reference get_new_module_tests single_batch_reference_fn _test_bfloat _ops _test_module_empty_input torch testing _internal common_device_type dtypesIfMPS instantiate_device_type_tests dtypes \ dtypesIfCUDA precisionOverride onlyCUDA onlyCPU \ skipCUDAIfRocm skipCUDAIf skipCUDAIfNotRocm \ onlyNativeDeviceTypes deviceCountAtLeast largeTensorTest expectedFailureMeta expectedFailureMPS \ skipMeta get_all_device_types hypothesis given torch testing _internal hypothesis_utils hu torch testing _internal common_utils _assertGradAndGradgradChecks gradcheck gradgradcheck \ GRADCHECK_NONDET_TOL torch testing _internal common_utils dtype prec_DONTUSE torch testing _internal common_cuda tf _on_and_off tf _off tf _on torch types _TensorOrTensors torch testing _internal common_mkldnn reduced_f _on_and_off AMPERE_OR_ROCM = TEST_WITH_ROCM torch cuda is_tf _supported TEST_WITH_ROCM os environ PYTORCH_MIOPEN_SUGGEST_NHWC = os environ PYTORCH_MIOPEN_SUGGEST_NHWC_BATCHNORM = load_tests common_utils used automatically filter tests sharding sandcastle This line silences flake warnings load_tests = load_tests noqa PLW TEST_SCIPY scipy signal scipy ndimage TEST_NUMPY numpy np WARNING If you add new top-level test case file you MUST update test run_test py list otherwise will NOT run CI TestNN NNTestCase _do_cuda_memory_leak_check = True _do_cuda_non_default_stream = True _forward module input _TensorOrTensors freeze_rng_state isinstance input tuple module input module input _backward module input _TensorOrTensors output grad_output create_graph=False output backward grad_output retain_graph=True create_graph=create_graph isinstance input tuple tuple i grad data i grad None None i input input grad data input grad None None _forward_criterion criterion input target extra_args=None extra_args None extra_args = isinstance input tuple args = input + target + extra_args output = criterion args output = criterion input target extra_args output _backward_criterion criterion input output target gradOutput=None extra_args=None extra_args None extra_args = input_tuple = input isinstance input tuple input output_tuple = output isinstance output tuple output i input_tuple i grad None i grad data zero_ args = input_tuple + target + extra_args gradOutput None gradOutput = torch ones criterion args backward gradOutput output_tuple isinstance input tuple tuple i grad data i input input grad data _zero_grad_parameters module p module parameters p grad None torch no_grad p grad zero_ p grad detach_ _get_parameters module params = d_params = p module parameters params append p d_params append p grad params d_params test_parse_to Test buggy use THPMemoryFormat_New assertEqual repr torch _C _nn _parse_to memory_format=torch contiguous_format torch contiguous_format test_requires_grad_ m = _create_basic_net - assert len list m buffers invalid test assert all b requires_grad b m buffers invalid test assert len list m parameters invalid test assert all p requires_grad p m parameters invalid test requires_grad False True assertIs m requires_grad_ requires_grad m p m parameters assertEqual p requires_grad requires_grad b m buffers assertFalse b requires_grad test_module_backcompat torch serialization SourceChangeWarning path = download_file https download pytorch org test_data linear pt warnings catch_warnings warnings simplefilter ignore SourceChangeWarning weights_only=False legacy code saves model m = torch load path weights_only=False input = torch randn dtype=torch float assertEqual m input size test_module_super_init MyMixin __init__ kw super __init__ kw mixin_init = True MyModuleWithMixinBefore MyMixin nn Module pass MyModuleWithMixinAfter nn Module MyMixin pass assertTrue hasattr MyModuleWithMixinBefore mixin_init assertFalse hasattr MyModuleWithMixinAfter mixin_init nn Module call_super_init = True assertTrue hasattr MyModuleWithMixinBefore mixin_init assertTrue hasattr MyModuleWithMixinAfter mixin_init nn Module call_super_init = False MyModuleWithMixinBefore call_super_init = True MyModuleWithMixinAfter call_super_init = True assertTrue hasattr MyModuleWithMixinBefore mixin_init assertTrue hasattr MyModuleWithMixinAfter mixin_init MyModuleWithMixinBefore call_super_init = False MyModuleWithMixinAfter call_super_init = False test_share_memory Net nn Module __init__ - None super __init__ p = nn Parameter torch eye par = nn ParameterList par append nn Parameter torch randn forward inp NB dead code inp clone net = Net p net parameters assertFalse p storage is_shared b net buffers assertFalse b storage is_shared net share_memory p net parameters assertTrue p storage is_shared b net buffers assertTrue b storage is_shared test_to m = nn Linear assertIs m m cpu assertIs m m cpu dtype=torch float assertEqual m double m torch float assertRaises RuntimeError lambda m cpu copy=True torch cuda is_available cuda cuda cuda torch cuda device_count == cuda m = m cuda device=cuda assertIs m m cuda assertEqual m m cpu assertEqual m m cuda assertIs m m dtype=torch float assertEqual m double m dtype=torch float test_zero_grad i = torch randn requires_grad=True module = nn Linear p module parameters p requires_grad = False module zero_grad module weight requires_grad = True module zero_grad assertIsNone module weight grad uninitialized grad module i sum backward assertIsNotNone module weight grad assertGreater module weight grad data abs sum module zero_grad assertIsNone module weight grad module bias requires_grad = True module zero_grad assertIsNone module weight grad assertIsNone module bias grad module i sum backward assertIsNotNone module weight grad assertIsNotNone module bias grad assertGreater module weight grad data abs sum assertGreater module bias grad data abs sum module zero_grad set_to_none=False Force set zeros assertEqual module weight grad data module weight data clone zero_ assertEqual module bias grad data module bias data clone zero_ module zero_grad assertIsNone module weight grad assertIsNone module bias grad test_no_grad dtype torch bfloat torch float torch double module = nn Conv d kernel_size= padding= dtype input = torch randn dtype x = input y = input clone output = module x assertTrue output requires_grad output backward torch ones torch no_grad output = module y assertFalse output requires_grad assertRaises RuntimeError lambda output backward torch ones test_parameters_and_named_parameters names named_parameters k k _ named_parameters l n s = _create_basic_net assertEqual len list l parameters assertEqual names l named_parameters layer_dummy_param assertEqual len list n parameters assertEqual names n named_parameters dummy_param l layer_dummy_param assertEqual len list n parameters recurse=False assertEqual names n named_parameters recurse=False dummy_param assertEqual len list s parameters assertEqual names s named_parameters dummy_param l layer_dummy_param test_named_parameters_remove_duplicate names named_parameters k k _ named_parameters M nn Module __init__ - None super __init__ param = nn Parameter torch empty param = param m = M assertEqual names m named_parameters param assertEqual names m named_parameters remove_duplicate=False param param M nn Module __init__ - None super __init__ mod = nn Linear bias=False mod = mod m = M assertEqual names m named_parameters mod weight assertEqual names m named_parameters remove_duplicate=False mod weight mod weight test_buffers_and_named_buffers names named_buffers k k _ named_buffers l n s = _create_basic_net assertEqual len list l buffers assertEqual names l named_buffers layer_dummy_buf assertEqual len list n buffers assertEqual names n named_buffers dummy_buf l layer_dummy_buf assertEqual len list n buffers recurse=False assertEqual names n named_buffers recurse=False dummy_buf assertEqual len list s buffers assertEqual names s named_buffers dummy_buf l layer_dummy_buf test remove_duplicate M nn Module __init__ - None super __init__ buffer = Buffer torch empty buffer = buffer m = M assertEqual names m named_buffers buffer assertEqual names m named_buffers remove_duplicate=False buffer buffer test_buffer_bad_module_subclass MyBadModule nn Linear __init__ - None super __init__ bar = Buffer torch rand register_buffer name value persistent explicitly missing super register_buffer name value True foo = MyBadModule assertIsNotNone foo bar test_call_supports_python_dict_output Net nn Module __init__ - None super __init__ l = nn Linear register_backward_hook hook check_backward_hook_flag = False hook module grad_out grad_in check_backward_hook_flag = True forward inputs output l inputs sum net = Net model_output = net torch randn model_output output backward assertTrue net check_backward_hook_flag test_children l = nn Linear l = nn Linear l = nn Linear l = nn Linear subnet = nn Sequential l l s = nn Sequential l l l l subnet assertEqual list s children l l subnet test_train_errors_for_invalid_mode SubclassNet nn Module __init__ - None super __init__ l = nn Linear forward inputs l inputs subclass_net = SubclassNet sequential_net = nn Sequential nn Linear nn Linear error_modes = invalid_str torch device cpu modules_to_check = subclass_net sequential_net error_mode module itertools product error_modes modules_to_check assertRaises ValueError module train error_mode test_dir linear = nn Linear linear _test_submodule = nn Linear linear _test_parameter = Parameter torch empty linear _test_buffer = Buffer torch empty keys = dir linear assertIn _test_submodule keys assertIn _test_parameter keys assertIn _test_buffer keys key keys assertTrue hasattr linear key test_repr no extra information sub-modules empty_sequential = nn Sequential expected_repr_empty = Sequential assertEqual repr empty_sequential expected_repr_empty one liner extra information linear = nn Linear expected_repr_linear = Linear in_features= out_features= bias=True assertEqual repr linear expected_repr_linear sub-modules repr sequential = nn Sequential linear expected_repr_sequential = Sequential \n \ Linear in_features= out_features= bias=True \n \ assertEqual repr sequential expected_repr_sequential test_dir_digit model = nn Sequential nn Linear keys = dir model assertNotIn keys test_named_children l = nn Linear l = nn Linear l = nn Linear l = nn Linear subnet = nn Sequential l l s = nn Sequential assertRaises KeyError s add_module l assertRaises KeyError s add_module name dot l s add_module layer l s add_module layer l s add_module layer l s add_module layer l s add_module subnet subnet assertEqual list s named_children layer l layer l subnet subnet test_modules Net nn Module __init__ - None super __init__ l = l l = l param = torch empty l = nn Linear n = Net s = nn Sequential n n n n assertEqual list s modules s n l test_named_modules Net nn Module __init__ - None super __init__ l = l l = l param = torch empty block = block l = nn Linear l = nn Linear l = nn Linear block = nn Sequential block add_module linear l block add_module linear l n = Net s = nn Sequential n n assertEqual list s named_modules s n l l block block block linear l block linear l test option remove duplicate module instances assertEqual list s named_modules remove_duplicate=False s n l l l l block block block linear l block linear l n l l l l block block block linear l block linear l test_register_buffer_raises_error_if_name_is_not_string m = nn Module expected_error = buffer name should string Got assertRaisesRegex TypeError expected_error + int m register_buffer torch rand assertRaisesRegex TypeError expected_error + NoneType m register_buffer None torch rand test_register_buffer_raises_error_if_attr_exists m = nn Module m attribute_name = assertRaises KeyError m register_buffer attribute_name torch rand assertRaises KeyError m attribute_name = Buffer torch rand del m attribute_name m register_parameter attribute_name nn Parameter assertRaises KeyError m register_buffer attribute_name torch rand del m attribute_name m add_module attribute_name nn Module assertRaises KeyError m register_buffer attribute_name torch rand test_register_buffer_raises_error_if_not_tensor m = nn Module assertRaises TypeError m register_buffer attribute_name test_register_buffer_allows_overwriting_with_same_name m = nn Module buffer = torch rand buffer = buffer + buffer = None m register_buffer buffer_name buffer assertEqual m buffer_name buffer m register_buffer buffer_name buffer assertEqual m buffer_name buffer m register_buffer buffer_name buffer assertEqual m buffer_name buffer m buffer_name = Buffer buffer assertEqual m buffer_name Buffer buffer m buffer_name = Buffer buffer assertEqual m buffer_name Buffer buffer m buffer_name = Buffer buffer assertEqual m buffer_name Buffer buffer test_register_buffer_allows_tensor_like_object TensorLike classmethod __torch_function__ cls func types args= kwargs=None raise NotImplementedError f TensorLike __torch_function__ func buffer = TensorLike buffer = TensorLike m = nn Module m register_buffer buffer_name buffer assertEqual m buffer_name buffer assertEqual m get_buffer buffer_name buffer m buffer_name = buffer assertEqual m buffer_name buffer assertEqual m get_buffer buffer_name buffer test_get_buffer m = nn Module buffer = torch randn buffer = torch randn m foo = Buffer buffer m register_buffer bar buffer assertEqual buffer m get_buffer foo assertEqual buffer m get_buffer bar test_get_buffer_from_submodules MyModule nn Module __init__ foo bar super __init__ sub = Sub foo bar Sub nn Module __init__ foo bar super __init__ foo = Buffer foo subsub = SubSub bar SubSub nn Module __init__ bar super __init__ bar = Buffer bar foo = torch randn bar = torch randn m = MyModule foo bar assertEqual foo m get_buffer sub foo assertEqual bar m get_buffer sub subsub bar test_buffer_not_persistent m = nn Module m buf = nn Buffer torch rand persistent=False assertTrue len list m buffers == assertTrue len m state_dict == test_buffer_not_persistent_del m = nn Module m buf = nn Buffer torch rand persistent=False del m buf assertTrue len list m buffers == test_buffer_not_persistent_overwrite m = nn Module m buf = nn Buffer torch rand persistent=False m buf = nn Buffer torch rand can we overwrite non-persistent buffer persistent one assertTrue len list m buffers == assertTrue len m state_dict == can we overwrite persistent buffer non-persistent one m buf = nn Buffer torch rand persistent=False assertTrue len list m buffers == assertTrue len m state_dict == test_buffer_not_persistent_assign m = nn Module m buf = nn Buffer torch rand persistent=False assertTrue len list m buffers == assertTrue len m state_dict == Assigning None removes buffer we then assign new Tensor same property should still marked buffer m buf = None assertTrue len list m buffers == assertTrue len m state_dict == m buf = torch rand assertTrue len list m buffers == assertTrue len m state_dict == Assigning Parameter removes buffer m buf = nn Parameter torch rand assertTrue len list m buffers == assertTrue len m state_dict == test_buffer_not_persistent_load m = nn Module m buf = nn Buffer torch rand persistent=False m load_state_dict test_register_parameter_raises_error_if_name_is_not_string m = nn Module expected_error = parameter name should string Got assertRaisesRegex TypeError expected_error + int m register_parameter nn Parameter assertRaisesRegex TypeError expected_error + NoneType m register_parameter None nn Parameter test_register_parameter_raises_error_if_attr_exists m = nn Module m attribute_name = assertRaises KeyError m register_parameter attribute_name nn Parameter del m attribute_name m register_buffer attribute_name torch rand assertRaises KeyError m register_parameter attribute_name nn Parameter del m attribute_name m attribute_name = Buffer torch rand assertRaises KeyError m register_parameter attribute_name nn Parameter del m attribute_name m add_module attribute_name nn Module assertRaises KeyError m register_parameter attribute_name nn Parameter test_register_parameter_allows_overwriting_with_same_name m = nn Module param = nn Parameter torch rand param = nn Parameter param data + param = None m register_parameter param_name param assertEqual m param_name param m register_parameter param_name param assertEqual m param_name param m register_parameter param_name param assertEqual m param_name param test_add_module_raises_error_if_attr_exists methods_to_test = add_module register_module fn methods_to_test m = nn Module m attribute_name = assertRaises KeyError getattr m fn attribute_name nn Module del m attribute_name m register_buffer attribute_name torch rand assertRaises KeyError getattr m fn attribute_name nn Module del m attribute_name m register_parameter attribute_name nn Parameter assertRaises KeyError getattr m fn attribute_name nn Module unittest expectedFailure test_getattr_with_property Model nn Module property some_property something_that_doesnt_exist model = Model assertRaisesRegex AttributeError r Model object has no attribute something_that_doesnt_exist model some_property test_Sequential_getitem l = nn Linear l = nn Linear l = nn Linear l = nn Linear n = nn Sequential l l l l assertIs n l assertIs n l assertIs n l assertIs n l assertIs n torch tensor dtype=torch int l assertEqual n nn Sequential l l l assertEqual n nn Sequential l assertEqual n - nn Sequential l l l assertEqual n - nn Sequential l assertEqual n - nn Sequential l l l l test_Sequential_setitem l = nn Linear l = nn Linear l = nn Linear l = nn Linear n = nn Sequential l l l n = l n - = l n torch tensor dtype=torch int = l assertIs n l assertIs n l assertIs n l test_Sequential_setitem_named l = nn Linear l = nn Linear l = nn Linear l = nn Linear n = nn Sequential OrderedDict linear l linear l linear l n = l n - = l assertEqual n linear l assertEqual n linear l test_Sequential_delitem l = nn Linear l = nn Linear l = nn Linear l = nn Linear n = nn Sequential l l l l del n - assertEqual n nn Sequential l l l del n assertEqual n nn Sequential l l test_Sequential_add l = nn Linear l = nn Linear l = nn Linear l = nn Linear n = nn Sequential l l other = nn Sequential l l assertEqual n + other nn Sequential l l l l test_Sequential_iadd l = nn Linear l = nn Linear l = nn Linear l = nn Linear n = nn Sequential l l l n = nn Sequential l n += n n += n assertEqual n nn Sequential l l l l assertEqual n nn Sequential l l l l l test_Sequential_mul l = nn Linear l = nn Linear l = nn Linear l = nn Linear n = nn Sequential l l l l n = n assertEqual n nn Sequential l l l l l l l l test_Sequential_rmul l = nn Linear l = nn Linear l = nn Linear l = nn Linear n = nn Sequential l l l l n = n assertEqual n nn Sequential l l l l l l l l test_Sequential_imul l = nn Linear l = nn Linear l = nn Linear l = nn Linear n = nn Sequential l l l l n = assertEqual n nn Sequential l l l l l l l l n = assertEqual n nn Sequential l l l l l l l l l l l l l l l l test_Sequential_append l = nn Linear l = nn Linear l = nn Linear l = nn Linear n = nn Sequential l l l n = n append l assertEqual n nn Sequential l l l l assertEqual n nn Sequential l l l l assertEqual nn Sequential l append l append l nn Sequential l l l test_Sequential_pop l = nn Linear l = nn Linear l = nn Linear l = nn Linear n = nn Sequential l l l l assertEqual l n pop n = nn Sequential l l l assertEqual n n check order index k mod zip range len n n assertIs n k mod test_Sequential_insert l = nn Linear l = nn Linear l = nn Linear n = nn Sequential l l l module_ = nn Linear n = nn Sequential l module_ l l assertEqual n insert module_ n test negative support n = nn Sequential l l l module_ = nn Linear n = nn Sequential l module_ l l assertEqual n insert - module_ n test_Sequential_insert_fail_case l = nn Linear l = nn Linear l = nn Linear module = nn Linear test error case n = nn Sequential l l l assertRaises IndexError n insert - module assertRaises AssertionError n insert nn Linear test_Sequential_extend l = nn Linear l = nn Linear l = nn Linear l = nn Linear n = nn Sequential l l n = nn Sequential l l n = nn Sequential l l l n n append l n extend n assertEqual n n test_ModuleList modules = nn ReLU nn Linear module_list = nn ModuleList modules check assertEqual len module_list len modules m m zip modules module_list assertIs m m m m zip modules module_list children assertIs m m i range len modules assertIs module_list i modules i check modules += nn Conv d module_list += modules - check modules = modules + nn Conv d bias=False nn GELU module_list = module_list + nn ModuleList modules - check modules insert nn Linear module_list insert modules check modules append nn Tanh module_list append modules - check next_modules = nn Linear nn Sigmoid modules extend next_modules module_list extend next_modules check modules = nn Conv d module_list = modules check modules - = nn Conv d module_list - = modules - check idx = torch tensor dtype=torch int modules = nn Conv d module_list idx = modules assertIs module_list idx modules check assertEqual module_list nn ModuleList modules assertEqual module_list nn ModuleList modules assertEqual module_list - nn ModuleList modules - assertEqual module_list - nn ModuleList modules - assertEqual module_list - nn ModuleList modules - del module_list - assertEqual module_list nn ModuleList modules - del module_list assertEqual module_list nn ModuleList modules - assertRaises TypeError module_list += nn ReLU assertRaises TypeError module_list extend nn ReLU l = nn Linear l = nn Linear l = nn Linear l = nn Linear subnet = nn Sequential l l s = nn Sequential OrderedDict layer l layer l layer l layer l subnet_layer subnet modules = list s modules module_list = nn ModuleList module_list extend s modules check modules = nn ReLU nn Linear nn Conv d module_list = nn ModuleList modules assertEqual modules pop module_list pop assertEqual modules module_list check order index k mod zip range len module_list module_list assertIs module_list k mod verify right exception thrown when trying forward through ModuleList assertRaises NotImplementedError module_list assertRaises NotImplementedError module_list torch rand test_ModuleDict modules = OrderedDict act nn ReLU conv nn Conv d fc nn Linear module_dict = nn ModuleDict modules check assertEqual len module_dict len modules k m zip modules module_dict children assertIs modules k m k k zip modules module_dict assertIs modules k module_dict k k module_dict assertIs module_dict k modules k k module_dict keys assertIs module_dict k modules k k v module_dict items assertIs modules k v k m zip modules module_dict values assertIs modules k m k modules keys assertTrue k module_dict check modules conv = nn Conv d module_dict conv = modules conv check next_modules = fc nn Linear act nn Sigmoid modules update next_modules module_dict update next_modules check next_modules = OrderedDict fc nn Linear act nn Sigmoid modules update next_modules module_dict update next_modules check next_modules = fc nn Linear act nn Sigmoid modules update next_modules items module_dict update next_modules check next_modules = nn ModuleDict fc nn Linear act nn Sigmoid modules update next_modules module_dict update next_modules check del module_dict fc del modules fc check assertRaises TypeError module_dict update nn ReLU assertRaises TypeError module_dict update nn ReLU assertRaises ValueError module_dict update nn ReLU assertRaises TypeError module_dict = nn ReLU s = nn Sequential modules module_dict = nn ModuleDict s named_children check c = module_dict pop conv assertIs c modules conv modules pop conv check module_dict clear assertEqual len module_dict modules clear check verify right exception thrown when trying forward through ModuleDict assertRaises NotImplementedError module_dict assertRaises NotImplementedError module_dict torch rand skipIfTorchDynamo test_ParameterList make_param Parameter torch randn parameters = make_param make_param param_list = nn ParameterList parameters check assertEqual len parameters len param_list p p zip parameters param_list assertIs p p p p zip filter lambda x isinstance x Parameter parameters param_list parameters assertIs p p i range len parameters assertIs parameters i param_list i check parameters += make_param param_list += parameters - check parameters append make_param param_list append parameters - check next_params = make_param make_param parameters extend next_params param_list extend next_params check parameters = make_param param_list = parameters check parameters - = make_param param_list - = parameters - check idx = torch tensor dtype=torch int parameters = make_param param_list idx = parameters assertIs param_list idx parameters check assertEqual param_list nn ParameterList parameters assertEqual param_list nn ParameterList parameters assertEqual param_list - nn ParameterList parameters - assertEqual param_list - nn ParameterList parameters - assertEqual param_list - nn ParameterList parameters - assertRaises TypeError param_list += make_param assertRaises TypeError param_list extend make_param l = nn Linear l = nn Linear l = nn Linear l = nn Linear subnet = nn Sequential l l s = nn Sequential OrderedDict layer l layer l layer l layer l subnet_layer subnet parameters = list s parameters param_list = nn ParameterList param_list extend s parameters check param_list append torch rand assertIsInstance param_list - Parameter parameters append param_list - param_list extend torch rand foo assertIsInstance param_list - Parameter assertIsInstance param_list - str parameters extend param_list - param_list += bar torch rand assertIsInstance param_list - str assertIsInstance param_list - Parameter parameters += param_list - check test_ParameterList_meta p = torch nn Parameter torch empty device= meta assertExpectedInline str p \ Parameter containing tensor device= meta size= requires_grad=True pl = torch nn ParameterList p assertExpectedInline str pl ParameterList Parameter containing torch float size test_ParameterList_replication The actual replication code DP cannot used CPU so doing manually here make_param Parameter torch randn parameters = make_param make_param param_list = nn ParameterList parameters new_param_list = param_list _replicate_for_data_parallel n p param_list named_parameters Do view here so we can check base later setattr new_param_list n p view_as p p p zip param_list new_param_list assertEqual p p assertIsNotNone p grad_fn assertIs p _base p test_ParameterDict parameters = OrderedDict p Parameter torch randn p Parameter torch randn p Parameter torch randn parameter_dict = nn ParameterDict parameters check assertEqual len parameter_dict len parameters k k m zip parameters parameter_dict named_parameters assertEqual k k assertIs parameters k m k k zip parameters parameter_dict assertIs parameters k parameter_dict k k parameter_dict assertIs parameter_dict k parameters k k parameter_dict keys assertIs parameter_dict k parameters k k v parameter_dict items assertIs v parameters k k m zip parameters parameter_dict values assertIs parameters k m k parameters keys assertTrue k parameter_dict check parameters p = Parameter torch randn parameter_dict p = parameters p check next_parameters = p Parameter torch randn p Parameter torch randn parameters update next_parameters parameter_dict update next_parameters check next_parameters = OrderedDict p Parameter torch randn p Parameter torch randn parameters update next_parameters parameter_dict update next_parameters check next_parameters = p Parameter torch randn p Parameter torch randn parameters update sorted next_parameters items parameter_dict update next_parameters check next_parameters = nn ParameterDict p Parameter torch randn p Parameter torch randn parameters update next_parameters parameter_dict update next_parameters check del parameter_dict p del parameters p check assertRaises TypeError parameter_dict update assertRaises TypeError parameter_dict update assertRaises ValueError parameter_dict update Parameter torch randn p_pop = parameter_dict pop p assertIs p_pop parameters p parameters pop p check Check reverse works forward = list iter parameter_dict backward = list reversed parameter_dict assertEqual len forward len backward n = len forward i range n assertIs forward i backward n - i - check Check copy works copy = parameter_dict copy Check all keys present have shallow copied values key parameter_dict assertTrue key copy assertEqual parameter_dict key copy key assertIs parameter_dict key copy key check parameter_dict p = Parameter torch randn copy p = Parameter torch randn assertTrue p parameter_dict assertFalse p copy assertFalse p parameter_dict assertTrue p copy parameter_dict pop p check p = Parameter torch randn parameter_dict p = p p_popitem = parameter_dict popitem assertEqual p_popitem p assertIs p_popitem p check Unit test set_default Ensure parameter correctly inserted when key present ` ParameterDict ` assert p parameter_dict assert p parameters parameters p = Parameter torch randn p_setdefault = parameter_dict setdefault p parameters p assertIs p_setdefault parameters p assertIs p_setdefault parameter_dict p check Ensure parameter NOT inserted when key already present ` ParameterDict ` p = Parameter torch randn assertFalse parameter_dict setdefault p p p check Ensure ` None ` inserted when key present ` Parameter ` parameter specified assertIs parameter_dict setdefault p None del parameter_dict p check parameters = OrderedDict p Parameter torch randn p Parameter torch randn p Parameter torch randn parameter_dict = nn ParameterDict parameters parameters update parameters parameter_dict &#124; = parameter_dict check parameters = OrderedDict parameter_dict = nn ParameterDict parameters parameters update parameters parameter_dict &#124; = parameter_dict check parameters = OrderedDict p Parameter torch randn p Parameter torch randn p Parameter torch randn parameter_dict = nn ParameterDict parameters parameters update parameters parameter_dict &#124; = parameter_dict check Check __or__ __ror__ works parameters = OrderedDict p Parameter torch randn p Parameter torch randn p Parameter torch randn parameter_dict = nn ParameterDict parameters parameters update parameters parameter_dict = parameter_dict &#124; parameter_dict check parameters = OrderedDict p Parameter torch randn p Parameter torch randn p Parameter torch randn parameter_dict = nn ParameterDict parameters parameters update parameters parameters = parameters parameter_dict = parameter_dict &#124; parameter_dict check parameters p = Parameter torch randn parameter_dict p = parameters p assertIs parameters p parameter_dict get p temp_param = Parameter torch randn assertIs parameters p parameter_dict get p temp_param assertIs None parameter_dict get p assertIs temp_param parameter_dict get p temp_param check parameter_dict clear assertEqual len parameter_dict parameters clear check parameter_dict = parameter_dict fromkeys p p assertEqual p None p None parameter_dict check parameter_dict = parameter_dict fromkeys p p temp_param assertEqual p temp_param p temp_param parameter_dict check parameter_dict p = torch rand assertIsInstance parameter_dict p Parameter parameters p = parameter_dict p parameter_dict update p torch rand foo bar assertIsInstance parameter_dict p Parameter assertIsInstance parameter_dict foo str parameters p = parameter_dict p parameters foo = parameter_dict foo test_ParameterDict_replication The actual replication code DP cannot used CPU so doing manually here make_param Parameter torch randn parameters = foo make_param bar make_param param_dict = nn ParameterDict parameters new_param_dict = param_dict _replicate_for_data_parallel n p param_dict named_parameters Do view here so we can check base later setattr new_param_dict n p view_as p k p k p zip param_dict items new_param_dict items assertEqual k k assertEqual p p assertIsNotNone p grad_fn assertIs p _base p assertEqual param_dict foo new_param_dict foo test_add_module methods_to_test = add_module register_module fn methods_to_test l = nn Linear net = nn Module net l = l net l = l getattr net fn empty None assertEqual net l l assertEqual net l l assertEqual net empty None getattr net fn l l assertEqual net l l l = nn Linear getattr net fn l l assertEqual net l l assertRaises TypeError lambda getattr net fn x non-module assertRaisesRegex TypeError module name should string Got int lambda getattr net fn l assertRaisesRegex TypeError module name should string Got NoneType lambda getattr net fn None l test_set_submodule test docstring example A = nn Module A set_submodule net_b nn Module A set_submodule net_b net_c nn Module A set_submodule net_b net_c conv nn Conv d A set_submodule net_b linear nn Linear new_linear = nn Linear A set_submodule net_b net_c conv new_linear assertEqual A get_submodule net_b net_c conv new_linear new_linear = nn Linear A set_submodule net_b net_c conv new_linear True assertEqual A get_submodule net_b net_c conv new_linear new_conv = nn Conv d assertRaises AttributeError A set_submodule net_b conv new_conv True A set_submodule net_b conv new_conv assertEqual A get_submodule net_b conv new_conv more tests net = nn Module net t = nn Module l = nn Linear target = t l net t l = l assertEqual net get_submodule target l l = nn Linear net set_submodule target l assertEqual net get_submodule target l assertRaises ValueError net set_submodule l assertRaises AttributeError net set_submodule l l assertRaises AttributeError net set_submodule l True net set_submodule l False assertEqual net get_submodule l l = nn Linear net set_submodule l True assertEqual net get_submodule l net foo = bar assertRaises AttributeError net set_submodule foo l assertRaises ValueError net set_submodule t l bazz test_module_to_argparse net = nn Sequential nn Linear cpu = torch device cpu assertRaises TypeError net cpu True assertRaises TypeError net torch long assertRaises TypeError net None True assertRaises TypeError net cpu torch long True assertRaises TypeError net cpu dtype=torch long non_blocking=True assertRaises TypeError net assertRaises TypeError net non_blocking=True assertRaises TypeError net torch tensor dtype=torch long non_blocking=True assertRaises TypeError net cpu torch tensor dtype=torch long non_blocking=True test_RNN_nonlinearity rnn = torch nn RNN assertEqual rnn nonlinearity tanh rnn = torch nn RNN nonlinearity= relu assertEqual rnn nonlinearity relu assertRaisesRegex ValueError Unknown nonlinearity rnn = torch nn RNN nonlinearity= garbage test_RNN_nonlinearity_passed_as_arg rnn = torch nn RNN relu assertEqual rnn nonlinearity relu test_module_apply_inplace_op add_one_inplace t t add_ Test applying in-place operation module would bump module s parameters version counter m = nn Linear pvm = m weight mul m weight m_weight_version_saved = m weight _version m = m _apply add_one_inplace assertGreater m weight _version m_weight_version_saved assertRaisesRegex RuntimeError modified inplace operation pvm backward torch randn Test applying in-place operation module would bump module s parameters gradients version counter m = nn Linear m weight grad = torch randn requires_grad_ pgm = m weight grad mul m weight grad m_weight_grad_version_saved = m weight grad _version m = m _apply add_one_inplace assertGreater m weight grad _version m_weight_grad_version_saved assertRaisesRegex RuntimeError modified inplace operation pgm backward torch randn test_overwrite_module_params_on_conversion Test conversion function passed ` module _apply ` changes TensorImpl type ` module ` s parameters ` module ` s parameters always overwritten regardless value ` torch __future__ get_overwrite_module_params_on_conversion ` m = nn Linear m weight grad = torch randn weight_ref = m weight weight_grad_ref = m weight grad m = m _apply lambda t torch sparse_coo_tensor torch zeros torch ones torch Size assertNotEqual weight_ref layout m weight layout assertNotEqual weight_grad_ref layout m weight grad layout Test under current default settings ` torch __future__ get_overwrite_module_params_on_conversion == False ` view module s parameters pointing same storage its base variable after converting module different dtype m = nn Linear float mw = m weight m double torch no_grad mw = assertTrue mw dtype == torch float assertTrue mw _base dtype == torch double try torch __future__ set_overwrite_module_params_on_conversion True Test ` torch __future__ get_overwrite_module_params_on_conversion == True ` view module s parameters still pointing same storage its base variable after converting module different dtype m = nn Linear float mw = m weight m double torch no_grad mw = assertTrue mw == mw _base Test ` torch __future__ get_overwrite_module_params_on_conversion == True ` ` float_module double ` doesn t preserve previous references ` float_module ` s parameters gradients m = nn Linear float m weight grad = torch randn float weight_ref = m weight weight_grad_ref = m weight grad m double assertNotEqual weight_ref dtype m weight dtype assertNotEqual weight_grad_ref dtype m weight grad dtype add_one_inplace t t add_ Test ` torch __future__ get_overwrite_module_params_on_conversion == True ` applying in-place operation module would bump module s original parameters version counter m = nn Linear pvm = m weight mul m weight weight_ref = m weight m_weight_version_saved = weight_ref _version m = m _apply add_one_inplace Test in-place operation bumps original parameter s version counter assertGreater weight_ref _version m_weight_version_saved assertRaisesRegex RuntimeError modified inplace operation pvm backward torch randn Test ` torch __future__ get_overwrite_module_params_on_conversion == True ` applying in-place operation module would bump module s original parameters gradients version counter m = nn Linear m weight grad = torch randn requires_grad_ pgm = m weight grad mul m weight grad weight_grad_ref = m weight grad m_weight_grad_version_saved = weight_grad_ref _version m = m _apply add_one_inplace assertGreater weight_grad_ref _version m_weight_grad_version_saved assertRaisesRegex RuntimeError modified inplace operation pgm backward torch randn Test ` torch __future__ get_overwrite_module_params_on_conversion == True ` applying out-of-place operation module doesn t bump module s original parameters version counter m = nn Linear weight_ref = m weight m_weight_version_saved = weight_ref _version m = m _apply lambda t torch randn t shape assertEqual weight_ref _version m_weight_version_saved Test ` torch __future__ get_overwrite_module_params_on_conversion == True ` applying out-of-place operation module doesn t bump module s original parameters gradients version counter m = nn Linear m weight grad = torch randn requires_grad_ weight_grad_ref = m weight grad m_weight_grad_version_saved = weight_grad_ref _version m = m _apply lambda t torch randn t shape assertEqual weight_grad_ref _version m_weight_grad_version_saved finally torch __future__ set_overwrite_module_params_on_conversion False test_swap_module_params_poisons_acc_grad try torch __future__ set_swap_module_params_on_conversion True backward cannot run after _apply forward will init AccumulateGrad nodes which bumps use_count parameters Tensors additionally any Tensors saved backward their use_count will bumped m = torch nn Linear inp = torch randn out = m inp m half assertTrue all p dtype == torch float p m parameters assertRaisesRegex RuntimeError Trying execute AccumulateGrad node poisoned swap_tensors out sum backward _apply can run after backward After running backward all references generated save backward will cleared So use_count will Tensor itself AccumulateGrad node swap_tensors should allow inp = torch randn dtype=torch half out = m inp out sum backward m float assertTrue all p dtype == torch float p m parameters out = m inp finally torch __future__ set_swap_module_params_on_conversion False test_type l = nn Linear net = nn Module net l = l net l = l net add_module empty None net indices = Buffer torch LongTensor net float assertIsInstance l weight data torch FloatTensor assertIsInstance l bias data torch FloatTensor assertIsInstance net indices torch LongTensor net double assertIsInstance l weight data torch DoubleTensor assertIsInstance l bias data torch DoubleTensor assertIsInstance net indices torch LongTensor net torch half assertIsInstance l weight data torch HalfTensor assertIsInstance l bias data torch HalfTensor assertIsInstance net indices torch LongTensor TEST_CUDA net float cuda assertIsInstance l weight data torch cuda FloatTensor assertIsInstance l bias data torch cuda FloatTensor assertIsInstance net indices torch cuda LongTensor net cpu assertIsInstance l weight data torch FloatTensor assertIsInstance l bias data torch FloatTensor assertIsInstance net indices torch LongTensor net cuda torch double True assertIsInstance l weight data torch cuda DoubleTensor assertIsInstance l bias data torch cuda DoubleTensor assertIsInstance net indices torch cuda LongTensor net torch empty device= cuda dtype=torch half assertIsInstance l weight data torch cuda HalfTensor assertIsInstance l bias data torch cuda HalfTensor assertIsInstance net indices torch cuda LongTensor net torch device cpu non_blocking=True assertIsInstance l weight data torch HalfTensor assertIsInstance l bias data torch HalfTensor assertIsInstance net indices torch LongTensor net torch float assertIsInstance l weight data torch FloatTensor assertIsInstance l bias data torch FloatTensor net torch DoubleTensor assertIsInstance l weight data torch DoubleTensor assertIsInstance l bias data torch DoubleTensor TEST_CUDA net device= cuda dtype=torch float assertIsInstance l weight data torch cuda FloatTensor assertIsInstance l bias data torch cuda FloatTensor test_non_leaf_parameters l = nn Linear l = nn Linear assign_weight l weight = l weight + assertRaises TypeError assign_weight This should work though l weight = Parameter torch randn test_parameters_to_vector conv = nn Conv d fc = nn Linear model = nn Sequential conv fc vec = parameters_to_vector model parameters assertEqual vec size test_vector_to_parameters conv = nn Conv d fc = nn Linear model = nn Sequential conv fc vec = torch arange vector_to_parameters vec model parameters sample = next model parameters assertTrue torch equal sample data vec data test_rnn_weight_norm check_weight_norm l name num_params This Module has parameters called weight_ih_l weight_hh_l bias_ih_l bias_hh_l weight_hr_l Applying weight norm one them causes become tensor l = torch nn utils weight_norm l name=name assertEqual sum isinstance p torch nn Parameter p l _flat_weights num_params - Removing weight norm reparameterization restores Parameter l = torch nn utils remove_weight_norm l name=name assertEqual sum isinstance p torch nn Parameter p l _flat_weights num_params Make sure upon removal reparameterization ` _parameters ` ` named_parameters ` contain right params Specifically original weight weight_ih_l should placed back parameters while reparameterization components weight_ih_l _v weight_ih_l _g should removed assertTrue name l _parameters assertIsNotNone l _parameters name assertTrue name + _v l _parameters assertTrue name + _g l _parameters assertTrue name dict l named_parameters assertIsNotNone dict l named_parameters name assertTrue name + _v dict l named_parameters assertTrue name + _g dict l named_parameters check_weight_norm torch nn LSTM weight_ih_l check_weight_norm torch nn LSTM proj_size= weight_hr_l test_weight_norm dtype torch float torch bfloat torch float input = torch randn dtype=dtype m = nn Linear dtype=dtype expected_output = m input add weight normalization m = torch nn utils weight_norm m assertEqual m weight_v size m weight size assertEqual m weight_g size assertEqual m input expected_output atol=dtype prec_DONTUSE dtype rtol= remove weight norm m = torch nn utils remove_weight_norm m assertFalse hasattr m weight_g assertFalse hasattr m weight_v assertEqual m input expected_output atol=dtype prec_DONTUSE dtype rtol= test dim= m = torch nn utils weight_norm m dim= assertEqual m weight_v size m weight size assertEqual m weight_g size assertEqual m input expected_output atol=dtype prec_DONTUSE dtype rtol= test dim=None m = nn Linear dtype=dtype expected_output = m input m = torch nn utils weight_norm m dim=None assertEqual m input expected_output assertRaisesRegex RuntimeError register two weight_norm hooks m = torch nn utils weight_norm m m = torch nn utils weight_norm m For float forward Module doesn t work we must still able register weight norm often done before sending Module CUDA m = nn Linear dtype=torch float m = torch nn utils weight_norm m test_parameterlistdict_setting_attributes warnings catch_warnings record=True w mod = nn ParameterList map nn Parameter torch rand torch rand assertTrue len w == warnings catch_warnings record=True w mod train mod eval assertTrue len w == warnings catch_warnings record=True w mod = nn ParameterDict nn Parameter torch rand b nn Parameter torch rand assertTrue len w == warnings catch_warnings record=True w mod train mod eval assertTrue len w == test_parameterlistdict_pickle m = nn ParameterList map nn Parameter torch rand torch rand warnings catch_warnings record=True w m = pickle loads pickle dumps m assertTrue len w == Test whether loading older checkpoints works without triggering warnings m = nn ParameterList map nn Parameter torch rand torch rand del m _forward_pre_hooks m _state_dict_hooks m _load_state_dict_pre_hooks m _non_persistent_buffers_set warnings catch_warnings record=True w m = pickle loads pickle dumps m assertTrue len w == m = nn ParameterDict nn Parameter torch rand b nn Parameter torch rand warnings catch_warnings record=True w m = pickle loads pickle dumps m assertTrue len w == Test whether loading older checkpoints works without triggering warnings m = nn ParameterDict nn Parameter torch rand b nn Parameter torch rand del m _forward_pre_hooks m _state_dict_hooks m _load_state_dict_pre_hooks m _non_persistent_buffers_set warnings catch_warnings record=True w m = pickle loads pickle dumps m assertTrue len w == test_weight_norm_pickle m = torch nn utils weight_norm nn Linear m = pickle loads pickle dumps m assertIsInstance m nn Linear skipIfTorchDynamo TorchDynamo fails here unknown reasons set_default_dtype torch double test_spectral_norm input = torch randn m = nn Linear m = torch nn utils spectral_norm m assertEqual m weight_u size torch Size m weight size weight_orig should trainable assertTrue hasattr m weight_orig assertTrue weight_orig m _parameters weight_u should just reused buffer assertTrue hasattr m weight_u assertTrue weight_u m _buffers assertTrue weight_v m _buffers weight should plain attribute counted buffer param assertFalse weight m _buffers assertFalse weight m _parameters should also sharing storage ` weight_orig ` assertEqual m weight_orig storage m weight storage assertEqual m weight_orig size m weight size assertEqual m weight_orig stride m weight stride m = torch nn utils remove_spectral_norm m assertFalse hasattr m weight_orig assertFalse hasattr m weight_u weight should converted back parameter assertTrue hasattr m weight assertTrue weight m _parameters assertRaisesRegex RuntimeError register two spectral_norm hooks m = torch nn utils spectral_norm m m = torch nn utils spectral_norm m test correctness training eval modes cpu multi-gpu settings apply_dp True False apply_dp TEST_MULTIGPU continue device = torch device cuda maybe_wrap m torch nn DataParallel m device = torch device cpu maybe_wrap m m requires_grad True False m = nn Linear device m weight requires_grad_ requires_grad m = torch nn utils spectral_norm m wrapped_m = maybe_wrap m assertTrue hasattr m weight_u u = m weight_u clone v = m weight_v clone TEST TRAINING BEHAVIOR assert u v updated input = torch randn device=device out = wrapped_m input assertNotEqual u m weight_u assertNotEqual v m weight_v assert backprop reaches weight_orig can t use gradcheck because function changes we activate through training mode requires_grad torch autograd grad out sum m weight_orig test backward works multiple forwards uses training mode so we need reset ` u ` ` v ` vectors same value beginning finite difference test pass saved_u = m weight_u clone saved_v = m weight_v clone fn input m weight_u data copy_ saved_u m weight_v data copy_ saved_v out = wrapped_m input out = wrapped_m input out + out gradcheck fn input clone requires_grad_ check_batched_grad=False test removing pre_remove_out = wrapped_m input m = torch nn utils remove_spectral_norm m assertEqual wrapped_m input pre_remove_out m = torch nn utils spectral_norm m _ range pre_remove_out = wrapped_m input m = torch nn utils remove_spectral_norm m assertEqual wrapped_m input pre_remove_out TEST EVAL BEHAVIOR m = torch nn utils spectral_norm m wrapped_m input last_train_out = wrapped_m input last_train_u = m weight_u clone last_train_v = m weight_v clone wrapped_m zero_grad wrapped_m eval eval_out = wrapped_m input assert eval gives same result last training iteration assertEqual eval_out last_train_out assert doing more iteration eval don t change things assertEqual eval_out wrapped_m input assertEqual last_train_u m weight_u assertEqual last_train_v m weight_v FIXME code below flaky when executed DataParallel see https github com pytorch pytorch issues apply_dp continue test backward works multiple forwards mixed training eval modes uses training mode so we need reset ` u ` ` v ` vectors same value beginning finite difference test pass saved_u = m weight_u clone saved_v = m weight_v clone fn input m weight_u data copy_ saved_u m weight_v data copy_ saved_v wrapped_m train out = wrapped_m input wrapped_m eval out = wrapped_m input wrapped_m train out = wrapped_m input wrapped_m eval out = wrapped_m input out + out + out + out gradcheck fn input clone requires_grad_ assert backprop reaches weight_orig eval requires_grad fn weight wrapped_m input gradcheck fn m weight_orig skipIfNoLapack test_spectral_norm_load_state_dict inp = torch randn activate_times Test backward compatibility At version None - weight becomes buffer v vector becomes buffer m = nn Linear snm = torch nn utils spectral_norm m snm train _ range activate_times snm inp version_latest_ref_state_dict = deepcopy snm state_dict assertEqual weight_orig bias weight_u weight_v set version_latest_ref_state_dict keys test non-strict loading works non_strict_state_dict = deepcopy version_latest_ref_state_dict non_strict_state_dict nonsense = nonsense assertRaisesRegex RuntimeError r Unexpected key\ s\ state_dict nonsense snm load_state_dict non_strict_state_dict strict=True snm load_state_dict non_strict_state_dict strict=False del non_strict_state_dict weight_orig snm load_state_dict non_strict_state_dict strict=False del non_strict_state_dict weight_u snm load_state_dict non_strict_state_dict strict=False del non_strict_state_dict weight_v snm load_state_dict non_strict_state_dict strict=False non_strict_state_dict weight = snm weight detach clone set W buffer snm load_state_dict non_strict_state_dict strict=False del non_strict_state_dict _metadata spectral_norm remove metadata info snm load_state_dict non_strict_state_dict strict=False del non_strict_state_dict weight remove W buffer snm load_state_dict non_strict_state_dict strict=False del non_strict_state_dict bias snm load_state_dict non_strict_state_dict strict=False craft version None state_dict version_none_state_dict = deepcopy version_latest_ref_state_dict assertIn spectral_norm version_none_state_dict _metadata del version_none_state_dict _metadata spectral_norm remove metadata info del version_none_state_dict weight_v remove v vector version_none_state_dict weight = snm weight detach clone set W buffer normal state_dict version_latest_with_metadata True False version_latest_state_dict = deepcopy version_latest_ref_state_dict version_latest_with_metadata We want still load user-crafted state_dict one without metadata del version_latest_state_dict _metadata spectral_norm test re-wrapping does matter m = torch nn utils remove_spectral_norm snm snm = torch nn utils spectral_norm m snm load_state_dict version_latest_ref_state_dict torch no_grad snm eval out _eval = snm inp snm train out _train = snm inp out _train = snm inp snm eval out _eval = snm inp test re-wrapping does matter m = torch nn utils remove_spectral_norm snm snm = torch nn utils spectral_norm m snm load_state_dict version_none_state_dict activate_times since loading version None state dict we assume values state dict have gone through lease one forward we only test equivalence when activate_times torch no_grad snm eval assertEqual out _eval snm inp snm train assertEqual out _train snm inp assertEqual out _train snm inp snm eval assertEqual out _eval snm inp test re-wrapping does matter m = torch nn utils remove_spectral_norm snm snm = torch nn utils spectral_norm m Test normal loading snm load_state_dict version_latest_state_dict torch no_grad snm eval assertEqual out _eval snm inp snm train assertEqual out _train snm inp assertEqual out _train snm inp snm eval assertEqual out _eval snm inp test_spectral_norm_dim inp = torch randn m = nn ConvTranspose d m = torch nn utils spectral_norm m should run into incompatible shapes x = m inp check u refers same dimension assertEqual m weight_u shape m weight_orig shape test_spectral_norm_forward input = torch randn m = nn Linear m = torch nn utils spectral_norm m naive forward _weight _bias _u = m weight_orig m bias m weight_u _weight_mat = _weight view _weight size - _v = torch mv _weight_mat t _u _v = F normalize _v dim= eps= e- _u = torch mv _weight_mat _v _u = F normalize _u dim= eps= e- _weight data = torch dot _u torch matmul _weight_mat _v out_hat = torch nn functional linear input _weight _bias expect_out = m input assertEqual expect_out out_hat test_spectral_norm_pickle m = torch nn utils spectral_norm nn Linear m = pickle loads pickle dumps m assertIsInstance m nn Linear test_threshold_int x = torch tensor - - - expected = torch tensor assertEqual F threshold x expected test_threshold_bfloat _half x = torch randn dtype torch bfloat torch half threshold - float inf float -inf float nan expected = F threshold x threshold dtype=dtype float res_bf = F threshold x dtype=dtype threshold float assertEqual res_bf expected unittest skipUnless fbgemm torch backends quantized supported_engines Linear_FP _weight requires FBGEMM FBGEMM only optimized CPUs instruction set support avx newer test_fb_fc_packed X = np random rand astype np float - W = np random rand astype np float - b = np random rand astype np float - fc_op X W b np dot X W T + b x_tensor = torch tensor X w_tensor = torch tensor W b_tensor = torch tensor b packed_w_tensor = torch fbgemm_pack_gemm_matrix_fp w_tensor actual_output = torch fbgemm_linear_fp _weight x_tensor packed_w_tensor b_tensor expected_output = fc_op X W b torch testing assert_close torch from_numpy expected_output actual_output cpu atol= e- rtol= e- test_pad_scalar_error inputs = torch tensor requires_grad=True assertRaises RuntimeError lambda F pad inputs assertRaises RuntimeError lambda F pad inputs test_nested_tensor_from_mask N L D = input = torch rand N L D mask = torch ones N L dtype=torch bool Leave first row all True maintain nt s size unchanged i range N end = torch randint L size= item mask i end = False nt = torch _nested_tensor_from_mask input mask input_convert = nt to_padded_tensor input masked_fill_ mask reshape N L logical_not assertEqual input input_convert test_nested_tensor_from_mask_error N L D = input = torch rand N L D Mask bool mask = torch zeros N L dtype=torch float assertRaises RuntimeError lambda torch _nested_tensor_from_mask input mask Mask size mask = torch zeros N L D dtype=torch bool assertRaises RuntimeError lambda torch _nested_tensor_from_mask input mask Input size mask = torch zeros N L dtype=torch bool input = torch rand N L assertRaises RuntimeError lambda torch _nested_tensor_from_mask input mask Mask size does match input mask = torch zeros N + L + dtype=torch bool input = torch rand N L D assertRaises RuntimeError lambda torch _nested_tensor_from_mask input mask Mask padding format mask = torch ones N L dtype=torch bool mask = False mask = False assertRaises RuntimeError lambda torch _nested_tensor_from_mask input mask test_normalize inputs = torch randn requires_grad=True dtype=torch double assertTrue gradcheck lambda x F normalize x p= dim=- inputs assertTrue gradcheck lambda x F normalize x p= dim=- inputs inputs = torch randn requires_grad=True assertTrue gradcheck lambda x F normalize x p= dim=- inputs unittest skipIf TEST_MULTIGPU multi-GPU supported Skip test ROCm per https github com pytorch pytorch issues skipIfRocm test_broadcast_double_backwards_gpu tensors = torch randn device= cuda requires_grad=True dtype=torch double torch randn device= cuda requires_grad=True dtype=torch double torch randn device= cuda requires_grad=True dtype=torch double TODO following segfaults check_batched_grad=True _assertGradAndGradgradChecks lambda i Broadcast apply i tensors check_batched_grad=False unittest skipIf TEST_MULTIGPU multi-GPU supported test_broadcast_not_requiring_grad variables = torch randn device= cuda requires_grad=True torch randn device= cuda requires_grad=False torch randn device= cuda requires_grad=False torch randn device= cuda requires_grad=True torch randn device= cuda requires_grad=True broadcasted_variables = Broadcast apply variables output_idx broadcasted_var enumerate broadcasted_variables input_var = variables output_idx len variables assertEqual input_var requires_grad broadcasted_var requires_grad unittest skipIf TEST_MULTIGPU multi-GPU supported test_broadcast_no_grad x = torch randn dtype=torch float requires_grad=True device= cuda torch no_grad broadcasted = Broadcast apply x assertTrue x requires_grad output broadcasted assertFalse output requires_grad test_state_dict l = nn Linear block = nn Module block conv = nn Conv d bias=False net = nn Module net linear = l net linear = l net bn = nn BatchNorm d net block = block net add_module empty None state_dict = net state_dict assertEqual len state_dict assertEqual len state_dict _metadata assertIn state_dict _metadata assertIn linear state_dict _metadata assertIn linear weight state_dict assertIn linear bias state_dict assertIn linear state_dict _metadata assertIn linear weight state_dict assertIn linear bias state_dict assertIn block state_dict _metadata assertIn block conv state_dict _metadata assertIn block conv weight state_dict assertIn block conv weight state_dict assertNotIn block conv bias state_dict assertIn bn state_dict _metadata assertIn bn weight state_dict assertIn bn bias state_dict assertIn bn running_var state_dict assertIn bn running_mean state_dict assertIn bn num_batches_tracked state_dict assertFalse any k startswith empty k state_dict keys k v state_dict items param = net component k split param = getattr param component isinstance param Parameter param = param data assertEqual v data_ptr param data_ptr l = nn Linear state_dict = l state_dict assertEqual len state_dict assertEqual len state_dict _metadata assertIn state_dict _metadata assertTrue state_dict _metadata version = assertEqual state_dict weight data_ptr l weight data_ptr assertEqual state_dict bias data_ptr l bias data_ptr Reference https github com pytorch pytorch pull #issuecomment- assertNotWarn lambda l state_dict destination= Should warn kwarg destination w o _metadata test_extra_state SubModule torch nn Module __init__ foo super __init__ foo = foo get_extra_state foo foo set_extra_state state foo = state foo MyModule torch nn Module __init__ foo bar super __init__ sub = SubModule foo bar = bar get_extra_state bar bar set_extra_state state bar = state bar Ensure state_dict contains extra state loading into another module m = MyModule something m = MyModule something m load_state_dict m state_dict assertEqual m state_dict m state_dict assertEqual m bar m bar assertEqual m sub foo m sub foo test_extra_state_non_dict MyModule torch nn Module __init__ foo super __init__ foo = foo get_extra_state foo set_extra_state state foo = state Test various types extra state state something MyModule m = MyModule state m = MyModule something m load_state_dict m state_dict assertEqual m state_dict m state_dict assertEqual m foo m foo test_extra_state_missing_set_extra_state MyModule torch nn Module get_extra_state foo m = MyModule assertRaisesRegex RuntimeError Unexpected key m load_state_dict m state_dict test_extra_state_missing_get_extra_state MyModule torch nn Module set_extra_state pass m = MyModule assertRaisesRegex RuntimeError Missing key m load_state_dict m state_dict skipIfTorchDynamo TorchDynamo fails here unknown reasons test_parameter_assignment l = nn Linear num_params len list l parameters assertEqual num_params new_param = Parameter torch randn l param_name = new_param assertEqual num_params assertObjectIn new_param l parameters var = torch randn l var_name = var assertEqual num_params assertNotIn id var map id l parameters Make sure Variables saved parameters l variable_attr = torch empty assertEqual num_params l param_attr = Parameter torch empty assertEqual num_params It shouldn t possible replace parameter Variable assign_var l param_attr = torch empty assertRaises TypeError assign_var But replacing None should fine l param_attr = None assertEqual num_params test_assignment l = nn Module = nn Parameter torch randn b = nn Parameter torch randn c = nn Parameter torch randn q = nn Linear r = nn Linear w = nn Linear test_assignments get_list b c Check None can shadowed l = None assertIsNone l assertIn l __dict__ l = assertIs l assertEqual get_list assertNotIn l __dict__ Assign second object l b = None assertIsNone l b assertIn b l __dict__ l b = b assertIs l b b assertEqual get_list b assertNotIn b l __dict__ Remove add object back Order should unchanged l = None assertIsNone l assertEqual get_list b l = assertIs l assertEqual get_list b Replace object another one Order should unchanged l = c assertIs l c assertEqual get_list c b Remove reassign attribute It should appear end list now del l assertFalse hasattr l l = assertIs l assertEqual get_list b test_assignments lambda list l parameters b c del l l b assertEqual list l parameters test_assignments lambda list l children q r w del l l b assertEqual list l children buf = Buffer torch randn l buf = buf assertIs l buf buf l buf = None assertIs l buf None assertNotIn buf l __dict__ should stored l _buffers l buf = buf assertIn buf l state_dict assertEqual l state_dict buf buf test_container_copy Model nn Module __init__ - None super __init__ linear = nn Linear forward input linear input input = torch randn model = Model model_cp = deepcopy model assertEqual model input data model_cp input data model_cp linear weight data = assertNotEqual model input data model_cp input data test_RNN_cell just smoke test these modules implemented through autograd so no Jacobian test needed module nn RNNCell nn GRUCell bias True False input = torch randn hx = torch randn cell = module bias=bias _ range hx = cell input hx hx sum backward test_RNN_cell_forward_zero_hidden_size input = torch randn hx = torch randn cell_shared_param = cell nn RNNCell cell_shared_param nonlinearity= relu nn RNNCell cell_shared_param nonlinearity= tanh nn GRUCell cell_shared_param assertEqual cell input hx shape torch Size _test_loss_equal_input_target_shape cast Tests losses whose inputs should have same size losses = mse_loss lambda x y F mse_loss x y l _loss lambda x y F l _loss x y smooth_l _loss lambda x y F smooth_l _loss x y huber_loss lambda x y F huber_loss x y kl_div lambda x y F kl_div x y poisson_nll_loss lambda x y F poisson_nll_loss x y input = cast torch randn target = cast torch randn fn losses values assertRaises Exception lambda fn input target test_loss_equal_input_target_shape _test_loss_equal_input_target_shape lambda x x test_mse_loss_size_warning i = torch randn requires_grad=True t = torch randn warnings catch_warnings record=True w Ensure warnings being shown warnings simplefilter always Trigger Warning F mse_loss i t Check warning occurs assertEqual len w assertIn Please ensure they have same size str w test_weighted_mse_loss inputs = torch tensor requires_grad=True targets = torch tensor weight = torch tensor loss = F mse_loss inputs targets weight=weight reduction= mean expected_loss = torch tensor assertTrue torch isclose loss expected_loss f Expected expected_loss got loss test_weighted_l _loss_with_weights inputs = torch tensor requires_grad=True targets = torch tensor weight = torch tensor loss = F l _loss inputs targets weight=weight reduction= mean expected_loss = torch tensor assertTrue torch isclose loss expected_loss f Expected expected_loss got loss test_weighted_huber_loss inputs = torch tensor requires_grad=True targets = torch tensor weight = torch tensor loss = F huber_loss input=inputs target=targets weight=weight reduction= mean delta= expected_loss = torch tensor print torch isclose loss expected_loss atol= e- f Expected expected_loss got loss test_gaussian_nll_loss_broadcasting input = torch tensor target_full = torch tensor target_part = torch tensor var_full = torch tensor var_part = torch tensor var_part = torch tensor component_wise_loss = torch log var_full + input - target_full var_full assertEqual component_wise_loss F gaussian_nll_loss input target_part var_full reduction= none assertEqual component_wise_loss F gaussian_nll_loss input target_full var_part reduction= none assertEqual component_wise_loss F gaussian_nll_loss input target_full var_part reduction= none assertEqual component_wise_loss F gaussian_nll_loss input target_part var_part reduction= none assertEqual component_wise_loss F gaussian_nll_loss input target_part var_part reduction= none test_gaussian_nll_loss_args input = torch randn assertRaisesRegex ValueError var incorrect size target = torch randn var = torch ones torch nn functional gaussian_nll_loss input target var assertRaisesRegex ValueError var has negative entry entries var = - torch ones torch nn functional gaussian_nll_loss input target var assertRaisesRegex ValueError var has negative entry entries var = - torch nn functional gaussian_nll_loss input target var test_gaussian_nll_loss_scalar_var input = torch tensor target = torch tensor var = var_tensor = var torch ones_like input component_wise_loss = torch log var_tensor + input - target var_tensor assertEqual component_wise_loss F gaussian_nll_loss input target var reduction= none assertEqual F gaussian_nll_loss input target var_tensor reduction= none F gaussian_nll_loss input target var reduction= none test_KLDivLoss_batch_mean input_shape = log_prob = F log_softmax torch randn input_shape prob = F softmax torch randn input_shape loss = nn KLDivLoss reduction= batchmean l = loss log_prob prob loss_none_reduce = nn KLDivLoss reduction= sum log_prob prob expected = loss_none_reduce input_shape assertEqual l expected test_KLDivLoss_batch_mean_log_target input_shape = log_prob = F log_softmax torch randn input_shape log_prob = F log_softmax torch randn input_shape loss = nn KLDivLoss reduction= batchmean log_target=True l = loss log_prob log_prob loss_none_reduce = nn KLDivLoss reduction= sum log_target=True log_prob log_prob expected = loss_none_reduce input_shape assertEqual l expected test_CTCLoss_typechecks target_lengths = torch tensor input_lengths = torch tensor targets = torch randint sum target_lengths dtype=torch int log_probs = torch randn dtype=torch float log_softmax assertRaises RuntimeError _input_lengths = input_lengths dtype=torch float torch nn functional ctc_loss log_probs targets _input_lengths target_lengths assertRaises RuntimeError target_lengths = target_lengths dtype=torch float torch nn functional ctc_loss log_probs targets input_lengths target_lengths unittest skipIf TEST_CUDA CUDA available test_CTCLoss_lengthchecks_cuda target_lengths - - - input_lengths - - - targets = torch randint dtype=torch long device= cuda log_probs = torch randn dtype=torch float device= cuda log_softmax assertRaises RuntimeError torch nn functional ctc_loss log_probs targets input_lengths target_lengths test_CTCLoss_lengthchecks_cpu target_lengths - - - input_lengths - - - targets = torch randint dtype=torch int log_probs = torch randn dtype=torch float log_softmax assertRaises RuntimeError torch nn functional ctc_loss log_probs targets input_lengths target_lengths unittest skipIf TEST_CUDA CUDA available test_CTCLoss_long_targets input_length = vocab_size = batch_size = target_length = log_probs = torch randn input_length batch_size vocab_size dtype=torch double log_softmax requires_grad_ targets = torch randint low= high=vocab_size - size= batch_size target_length dtype=torch long input_lengths = batch_size input_length target_lengths = batch_size target_length res_cpu = torch nn functional ctc_loss log_probs targets input_lengths target_lengths reduction= sum zero_infinity=True grad_out = torch randn_like res_cpu grad_cpu = torch autograd grad res_cpu log_probs grad_out torch backends cudnn flags enabled=False res_gpu = torch nn functional ctc_loss log_probs cuda targets cuda input_lengths target_lengths reduction= sum zero_infinity=True grad_gpu = torch autograd grad res_gpu log_probs grad_out cuda assertEqual res_cpu res_gpu atol= e- rtol= assertEqual grad_cpu grad_gpu atol= e- rtol= unittest skipIf TEST_CUDA CUDA available test_CTCLoss_critical_target_len cudnn has unexpected problem target length see issue N = S = C = T = target = torch randint low= high=C size= S dtype=torch int input_lengths = torch full size= N fill_value=T dtype=torch int target_lengths = torch tensor S dtype=torch int inp = torch randn T N C dtype=torch float device= cuda log_softmax requires_grad_ cudnn flags enabled=True res_gpu = torch nn functional ctc_loss inp target input_lengths target_lengths reduction= none res_cpu = torch nn functional ctc_loss inp cpu target input_lengths target_lengths reduction= none assertEqual res_cpu res_gpu atol= e- rtol= test_CTCLoss_zero_lengths devices = cpu devices += cuda TEST_CUDA N = S = C = T = target = torch randint low= high=C size= N S dtype=torch int input_lengths = torch full size= N fill_value= dtype=torch int target_lengths = torch full size= N fill_value= dtype=torch int device devices inp = torch randn T N C dtype=torch float device=device log_softmax requires_grad_ res = torch nn functional ctc_loss inp target input_lengths target_lengths reduction= none assertTrue res == all item res sum backward assertTrue inp grad == all item target_lengths = torch full size= N fill_value= dtype=torch int device devices inp = torch randn T N C dtype=torch float device=device log_softmax requires_grad_ res = torch nn functional ctc_loss inp target input_lengths target_lengths reduction= none assertTrue res == torch inf all item res sum backward assertTrue inp grad == all item unittest skipIf TEST_CUDA CUDA available test_CTCLoss_zero_infinity target_lengths = input_lengths = targets = torch randint sum target_lengths dtype=torch int device= cuda log_probs = torch randn dtype=torch float device= cuda log_softmax requires_grad_ res = torch nn functional ctc_loss log_probs targets input_lengths target_lengths reduction= sum zero_infinity=True torch backends cudnn flags enabled=False res = torch nn functional ctc_loss log_probs targets cuda long input_lengths target_lengths reduction= sum zero_infinity=True res_cpu = torch nn functional ctc_loss log_probs cpu targets cpu input_lengths target_lengths reduction= sum zero_infinity=True assertEqual res res atol= e- rtol= assertEqual res_cpu res cpu atol= e- rtol= g = torch autograd grad res log_probs g = torch autograd grad res log_probs g = torch autograd grad res_cpu log_probs assertEqual g g atol= e- rtol= assertEqual g g atol= e- rtol= assertTrue g == g all item check we don t have NaN test_RNN_cell_no_broadcasting test cell_module input hx input_size hidden_size cell = cell_module input_size hidden_size assertRaises RuntimeError lambda cell input hx test_all hidden_size bad_hx good_hx input_size input test nn RNNCell input bad_hx input_size hidden_size test nn GRUCell input bad_hx input_size hidden_size test nn LSTMCell input bad_hx good_hx input_size hidden_size test nn LSTMCell input good_hx bad_hx input_size hidden_size hidden_size = input_size = input = torch randn input_size bad_hx = torch randn hidden_size good_hx = torch randn hidden_size Test hidden input batch size broadcasting test_all hidden_size bad_hx good_hx input_size input Test hx s hidden_size vs module s hidden_size broadcasting bad_hx = torch randn test_all hidden_size bad_hx good_hx input_size input Test input s input_size vs module s input_size broadcasting bad_input = torch randn test_all hidden_size good_hx good_hx input_size bad_input test_LSTM_cell just smoke test these modules implemented through autograd so no Jacobian test needed bias True False input = torch randn hx = torch randn cx = torch randn lstm = nn LSTMCell bias=bias _ range hx cx = lstm input hx cx hx + cx sum backward test_LSTM_cell_forward_input_size input = torch randn hx = torch randn cx = torch randn lstm = nn LSTMCell assertRaises Exception lambda lstm input hx cx test_LSTM_cell_forward_hidden_size input = torch randn hx = torch randn cx = torch randn lstm = nn LSTMCell assertRaises Exception lambda lstm input hx cx assertRaises Exception lambda lstm input cx hx unittest skipIf TEST_CUDA CUDA available test_pack_sequence_batch_sizes_throw assertRaisesRegex ValueError r batch_sizes should always CPU m = nn LSTM bidirectional=True num_layers= cuda = torch rand device= cuda b = torch tensor device= cuda input = nn utils rnn PackedSequence b test_Transformer_cell just smoke test these modules implemented through autograd so no Jacobian test needed d_model = nhead = num_encoder_layers = num_decoder_layers = dim_feedforward = dropout = bsz = seq_length = tgt_length = batch_first src_size tgt_size zip True False bsz seq_length d_model seq_length bsz d_model bsz tgt_length d_model tgt_length bsz d_model transformer = nn Transformer d_model nhead num_encoder_layers num_decoder_layers dim_feedforward dropout batch_first=batch_first dtype=torch double src = torch randn src_size dtype=torch double src_mask = transformer generate_square_subsequent_mask seq_length double tgt = torch randn tgt_size dtype=torch double tgt_mask = transformer generate_square_subsequent_mask tgt_length double memory_mask = torch randn tgt_length seq_length double src_key_padding_mask = torch rand bsz seq_length = tgt_key_padding_mask = torch rand bsz tgt_length = memory_key_padding_mask = torch rand bsz seq_length = output = transformer src tgt src_mask=src_mask tgt_mask=tgt_mask memory_mask=memory_mask src_key_padding_mask=src_key_padding_mask tgt_key_padding_mask=tgt_key_padding_mask memory_key_padding_mask=memory_key_padding_mask output sum backward test_transformerdecoderlayer deterministic test TransformerDecoderLayer d_model = nhead = dim_feedforward = dropout = bsz = seq_length = tgt_length = batch_first False True perm_fn x x transpose batch_first x model = nn TransformerDecoderLayer d_model nhead dim_feedforward dropout batch_first=batch_first set constant weights model p model parameters x = p data sz = x view - size shape = x shape x = torch cos torch arange sz float view shape p data copy_ x deterministic input decoder_input = torch tensor memory_input = torch tensor result = model decoder_input memory_input ref_output = torch tensor - result = result detach numpy ref_output = ref_output detach numpy assertEqual tuple result shape tuple ref_output shape np testing assert_allclose result ref_output atol= e- deterministic input decoder_input = perm_fn torch tensor memory_input = torch tensor result = model decoder_input memory_input result = result detach numpy ref_output = perm_fn torch tensor - - - - ref_output = ref_output detach numpy assertEqual tuple result shape tuple ref_output shape np testing assert_allclose result ref_output atol= e- deterministic input decoder_input = perm_fn torch tensor memory_input = perm_fn torch tensor result = model decoder_input memory_input ref_output = perm_fn torch tensor - - result = result detach numpy ref_output = ref_output detach numpy assertEqual tuple result shape tuple ref_output shape np testing assert_allclose result ref_output atol= e- deterministic input decoder_input = perm_fn torch tensor memory_input = perm_fn torch tensor result = model decoder_input memory_input ref_output = perm_fn torch tensor - - - - - - - - - - - - result = result detach numpy ref_output = ref_output detach numpy assertEqual tuple result shape tuple ref_output shape np testing assert_allclose result ref_output atol= e- key_padding_mask key_padding_mask = torch zeros == result = model decoder_input memory_input tgt_key_padding_mask=key_padding_mask ref_output = perm_fn torch tensor - - - - - - - - - - - - result = result detach numpy ref_output = ref_output detach numpy assertEqual tuple result shape tuple ref_output shape np testing assert_allclose result ref_output atol= e- key_padding_mask key_padding_mask = key_padding_mask = key_padding_mask = result = model decoder_input memory_input tgt_key_padding_mask=key_padding_mask ref_output = perm_fn torch tensor - - - - - - - - - - - - result = result detach numpy ref_output = ref_output detach numpy assertEqual tuple result shape tuple ref_output shape np testing assert_allclose result ref_output atol= e- memory_key_padding_mask key_padding_mask = torch zeros == result = model decoder_input memory_input memory_key_padding_mask=key_padding_mask ref_output = perm_fn torch tensor - - - - - - - - - - - - result = result detach numpy ref_output = ref_output detach numpy assertEqual tuple result shape tuple ref_output shape np testing assert_allclose result ref_output atol= e- memory_key_padding_mask key_padding_mask = key_padding_mask = key_padding_mask = result = model decoder_input memory_input memory_key_padding_mask=key_padding_mask ref_output = perm_fn torch tensor - - - - - - - - - - - - result = result detach numpy ref_output = ref_output detach numpy assertEqual tuple result shape tuple ref_output shape np testing assert_allclose result ref_output atol= e- set_default_dtype torch double test_transformerdecoderlayer_gelu deterministic test TransformerDecoderLayer gelu activation d_model = nhead = dim_feedforward = dropout = bsz = seq_length = tgt_length = activation batch_first product gelu F gelu nn GELU True False perm_fn x x transpose batch_first x model = nn TransformerDecoderLayer d_model nhead dim_feedforward dropout activation batch_first=batch_first set constant weights model p model parameters x = p data sz = x view - size shape = x shape x = torch cos torch arange sz float view shape p data copy_ x deterministic input decoder_input = torch tensor memory_input = torch tensor result = model decoder_input memory_input ref_output = torch tensor - torch testing assert_close result ref_output rtol= e- atol= deterministic input decoder_input = perm_fn torch tensor memory_input = perm_fn torch tensor result = model decoder_input memory_input ref_output = perm_fn torch tensor - - - - torch testing assert_close result ref_output rtol= e- atol= deterministic input decoder_input = perm_fn torch tensor memory_input = perm_fn torch tensor result = model decoder_input memory_input ref_output = perm_fn torch tensor - - torch testing assert_close result ref_output rtol= e- atol= deterministic input decoder_input = perm_fn torch tensor memory_input = perm_fn torch tensor result = model decoder_input memory_input ref_output = perm_fn torch tensor - - - - - - - - - - - - torch testing assert_close result ref_output rtol= e- atol= test_transformerdecoder get_a_test_layer use_cuda activation batch_first=False d_model = nhead = dim_feedforward = dropout = device = torch device cuda use_cuda cpu layer = nn TransformerDecoderLayer d_model nhead dim_feedforward=dim_feedforward dropout=dropout activation=activation batch_first=batch_first device torch no_grad set constant weights model p layer parameters x = p data sz = x view - size shape = x shape x = torch cos torch arange sz float view shape p data copy_ x layer deterministic test TransformerDecoder batch_first False True perm_fn x x transpose batch_first x activation = F relu use_cuda = torch cuda is_available device = torch device cuda use_cuda cpu decoder_layer = get_a_test_layer use_cuda=use_cuda activation=activation batch_first=batch_first model = nn TransformerDecoder decoder_layer device deterministic input decoder_input = torch tensor device memory_input = torch tensor device result = model decoder_input memory_input ref_output = torch tensor - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- deterministic input decoder_input = perm_fn torch tensor device memory_input = perm_fn torch tensor device result = model decoder_input memory_input ref_output = perm_fn torch tensor - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- deterministic input decoder_input = perm_fn torch tensor device memory_input = perm_fn torch tensor device result = model decoder_input memory_input ref_output = perm_fn torch tensor - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- deterministic input decoder_input = perm_fn torch tensor device memory_input = perm_fn torch tensor device result = model decoder_input memory_input ref_output = perm_fn torch tensor - - - - - - - - - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- key_padding_mask key_padding_mask = torch zeros device == result = model decoder_input memory_input tgt_key_padding_mask=key_padding_mask ref_output = perm_fn torch tensor - - - - - - - - - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- key_padding_mask key_padding_mask = key_padding_mask = key_padding_mask = result = model decoder_input memory_input tgt_key_padding_mask=key_padding_mask ref_output = perm_fn torch tensor - - - - - - - - - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- memory_key_padding_mask key_padding_mask = torch zeros device == result = model decoder_input memory_input memory_key_padding_mask=key_padding_mask ref_output = perm_fn torch tensor - - - - - - - - - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- memory_key_padding_mask key_padding_mask = key_padding_mask = key_padding_mask = result = model decoder_input memory_input memory_key_padding_mask=key_padding_mask ref_output = perm_fn torch tensor - - - - - - - - - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- multiple layers no norm model = nn TransformerDecoder decoder_layer device deterministic input decoder_input = torch tensor device memory_input = torch tensor device result = model decoder_input memory_input ref_output = torch tensor - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- multiple layers no norm model = nn TransformerDecoder decoder_layer device deterministic input decoder_input = perm_fn torch tensor device memory_input = perm_fn torch tensor device result = model decoder_input memory_input ref_output = perm_fn torch tensor - - - - - - - - - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- multiple layers norm d_model = norm = nn LayerNorm model = nn TransformerDecoder decoder_layer norm=norm device deterministic input decoder_input = torch tensor device memory_input = torch tensor device result = model decoder_input memory_input ref_output = torch tensor - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- multiple layers norm model = nn TransformerDecoder decoder_layer norm=norm device deterministic input decoder_input = perm_fn torch tensor device memory_input = perm_fn torch tensor device result = model decoder_input memory_input ref_output = perm_fn torch tensor - - - - - - - - - - - - - - - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- gelu activation test cases activation = gelu use_cuda = torch cuda is_available device = torch device cuda use_cuda cpu decoder_layer = get_a_test_layer use_cuda=use_cuda activation=activation batch_first=batch_first model = nn TransformerDecoder decoder_layer device deterministic input decoder_input = torch tensor device memory_input = torch tensor device result = model decoder_input memory_input ref_output = torch tensor - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- deterministic input decoder_input = perm_fn torch tensor device memory_input = perm_fn torch tensor device result = model decoder_input memory_input ref_output = perm_fn torch tensor - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- deterministic input decoder_input = perm_fn torch tensor device memory_input = perm_fn torch tensor device result = model decoder_input memory_input ref_output = perm_fn torch tensor - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- deterministic input decoder_input = perm_fn torch tensor device memory_input = perm_fn torch tensor device result = model decoder_input memory_input ref_output = perm_fn torch tensor - - - - - - - - - - - - device assertEqual tuple result shape tuple ref_output shape torch testing assert_close result ref_output rtol= e- atol= e- unittest skipIf TEST_CUDNN TEST_MULTIGPU CUDNN multi-gpu available test_cudnn_rnn_dropout_states_device rnn = nn RNN num_layers= dropout= device = input = torch randn cuda device rnn cuda device hx = torch randn cuda device output = rnn input hx test_cudnn_forward_exception rnns = nn LSTM batch_first=True torch zeros torch zeros nn LSTM batch_first=True proj_size= torch zeros torch zeros nn GRU batch_first=True torch zeros nn RNN batch_first=True torch zeros x_wrong = torch randn x_right = torch randn rnn hidden rnns assertRaisesRegex RuntimeError Expected hidden size got rnn x_right hidden assertRaisesRegex RuntimeError re escape input size - must equal input_size rnn x_wrong unittest skipIf TEST_CUDNN CUDNN available test_cudnn_weight_format rnns = nn LSTM batch_first=True nn LSTM batch_first=True proj_size= nn GRU batch_first=True nn RNN batch_first=True ROCm RNN does issue warning about single contig chunk memory so don t assert first_warn = torch version hip rnn rnns rnn cuda input = torch randn requires_grad=True device= cuda hx = torch randn requires_grad=True device= cuda all_vars = input hx + list rnn parameters isinstance rnn nn LSTM LSTM projections has different hx size rnn proj_size hx = torch randn requires_grad=True device= cuda all_vars = hx cx = torch randn requires_grad=True device= cuda all_vars = cx hx = hx cx output = rnn input hx output sum backward grads = v grad data clone v all_vars v all_vars v grad data zero_ Weights will no longer view onto same chunk memory weight = all_vars weight_data = weight data clone torch no_grad weight set_ weight_data _ range warnings catch_warnings record=True w output_noncontig = rnn input hx first_warn assertEqual len w assertIn weights part single contiguous chunk memory w message args first_warn = False warnings resetwarnings output_noncontig sum backward grads_noncontig = v grad data clone v all_vars v all_vars v grad data zero_ assertEqual output output_noncontig assertEqual grads_noncontig grads Make sure these still share storage weight_data = assertEqual weight_data all_vars data unittest skipIf TEST_CUDNN CUDNN available tf _on_and_off test_cudnn_weight_tying rnns = nn LSTM batch_first=True bidirectional=True nn LSTM batch_first=True bidirectional=True proj_size= nn GRU batch_first=True bidirectional=True nn RNN batch_first=True bidirectional=True rnn rnns rnn bias_ih_l _reverse = rnn bias_ih_l rnn cuda input = torch randn requires_grad=True device= cuda hx = torch randn requires_grad=True device= cuda all_vars = input hx + list rnn parameters opt = torch optim SGD rnn parameters lr= opt zero_grad isinstance rnn nn LSTM LSTM projections has different hx size rnn proj_size hx = torch randn requires_grad=True device= cuda all_vars = hx cx = torch randn requires_grad=True device= cuda all_vars = cx hx = hx cx warnings catch_warnings record=True w output = rnn input hx output sum backward opt step warnings catch_warnings record=True w output_cuda = rnn input hx rnn cpu hx = hx cpu hx cpu isinstance rnn nn LSTM hx cpu output_cpu = rnn input cpu hx assertEqual output_cuda output_cpu test_transformer_args_check model_name = Transformer d_model = nhead = num_encoder_layers = num_decoder_layers = dim_feedforward = dropout = bsz = seq_len = tgt_len = activations = F relu F gelu wrong_bsz = wrong_d_model = wrong_nhead = wrong_activation = abc test encoder_input_shape decoder_input_shape src_mask_len=None tgt_mask_len=None memory_mask_size=None src_key_padding_mask_size=None tgt_key_padding_mask_size=None memory_key_padding_mask_size=None src_is_causal=False tgt_is_causal=False memory_is_causal=False encoder_input = torch randn encoder_input_shape decoder_input = torch randn decoder_input_shape model = getattr nn model_name d_model nhead num_encoder_layers num_decoder_layers dim_feedforward dropout src_mask_len None src_mask = model generate_square_subsequent_mask src_mask_len src_mask = None tgt_mask_len None tgt_mask = model generate_square_subsequent_mask tgt_mask_len tgt_mask = None memory_mask_size None memory_task = torch rand memory_mask_size memory_task = None src_key_padding_mask_size None src_key_padding_mask = torch rand src_key_padding_mask_size = src_key_padding_mask = None tgt_key_padding_mask_size None tgt_key_padding_mask = torch rand tgt_key_padding_mask_size = tgt_key_padding_mask = None memory_key_padding_mask_size None memory_key_padding_mask = torch rand memory_key_padding_mask_size = memory_key_padding_mask = None assertRaises RuntimeError model encoder_input decoder_input src_mask=src_mask tgt_mask=tgt_mask memory_mask=memory_task src_key_padding_mask=src_key_padding_mask tgt_key_padding_mask=tgt_key_padding_mask memory_key_padding_mask=memory_key_padding_mask src_is_causal=src_is_causal tgt_is_causal=tgt_is_causal memory_is_causal=memory_is_causal correct_encoder_input_shape = seq_len bsz d_model correct_decoder_input_shape = tgt_len bsz d_model update_shape shape dim new_dim_size new_shape = list shape new_shape dim = new_dim_size tuple new_shape Incorrect encoder_input batch size encoder_input_shape = update_shape correct_encoder_input_shape wrong_bsz decoder_input_shape = correct_decoder_input_shape test encoder_input_shape decoder_input_shape Incorrect decoder_input batch size encoder_input_shape = correct_encoder_input_shape decoder_input_shape = update_shape correct_decoder_input_shape wrong_bsz test encoder_input_shape decoder_input_shape Incorrect encoder_input input size encoder_input_shape = update_shape correct_encoder_input_shape wrong_d_model decoder_input_shape = correct_decoder_input_shape test encoder_input_shape decoder_input_shape Incorrect decoder_input input size encoder_input_shape = correct_encoder_input_shape decoder_input_shape = update_shape correct_decoder_input_shape wrong_d_model test encoder_input_shape decoder_input_shape Incorrect nhead encoder_input_shape = correct_encoder_input_shape decoder_input_shape = correct_decoder_input_shape assertRaises AssertionError model = getattr nn model_name d_model wrong_nhead num_encoder_layers num_decoder_layers dim_feedforward dropout Incorrect src_mask encoder_input_shape = correct_encoder_input_shape decoder_input_shape = correct_decoder_input_shape wrong_src_mask_size = seq_len + test encoder_input_shape decoder_input_shape src_mask_len=wrong_src_mask_size Incorrect tgt_mask encoder_input_shape = correct_encoder_input_shape decoder_input_shape = correct_decoder_input_shape wrong_tgt_mask_size = tgt_len + test encoder_input_shape decoder_input_shape tgt_mask_len=wrong_tgt_mask_size Incorrect memory_mask encoder_input_shape = correct_encoder_input_shape decoder_input_shape = correct_decoder_input_shape wrong_tgt_mask_size = tgt_len + test encoder_input_shape decoder_input_shape memory_mask_size= wrong_tgt_mask_size wrong_src_mask_size Incorrect src_key_padding_mask encoder_input_shape = correct_encoder_input_shape decoder_input_shape = correct_decoder_input_shape assertRaises AssertionError test encoder_input_shape decoder_input_shape src_key_padding_mask_size= wrong_bsz wrong_src_mask_size Incorrect tgt_key_padding_mask encoder_input_shape = correct_encoder_input_shape decoder_input_shape = correct_decoder_input_shape assertRaises AssertionError test encoder_input_shape decoder_input_shape tgt_key_padding_mask_size= wrong_bsz wrong_tgt_mask_size Incorrect memory_key_padding_mask encoder_input_shape = correct_encoder_input_shape decoder_input_shape = correct_decoder_input_shape assertRaises AssertionError test encoder_input_shape decoder_input_shape memory_key_padding_mask_size= wrong_bsz wrong_src_mask_size Correct activations activation activations model = getattr nn model_name d_model nhead num_encoder_layers num_decoder_layers dim_feedforward dropout activation Incorrect activation assertRaises RuntimeError model = getattr nn model_name d_model nhead num_encoder_layers num_decoder_layers dim_feedforward dropout wrong_activation test_transformer_layer_args_check model_names = TransformerEncoderLayer TransformerDecoderLayer d_model = nhead = dim_feedforward = dropout = bsz = seq_len = tgt_len = activations = F relu F gelu wrong_activation = abc encoder_input_shape = seq_len bsz d_model decoder_input_shape = tgt_len bsz d_model encoder_input = torch randn encoder_input_shape decoder_input = torch randn decoder_input_shape model_name model_names activation activations model = getattr nn model_name d_model nhead dim_feedforward dropout activation Incorrect activation model_name model_names assertRaises RuntimeError model = getattr nn model_name d_model nhead dim_feedforward dropout wrong_activation test_rnn_args_check input_size = hidden_size = num_layers = batch_size = seq_len = num_directions = bad_size = prime number so no size can divide test input_shape hidden_shape mode input hidden get_inputs input_shape hidden_shape mode model = getattr nn mode input_size hidden_size num_layers assertRaises RuntimeError lambda model input hidden correct_input_shape = seq_len batch_size input_size correct_hidden_shape = num_layers num_directions batch_size hidden_size update_shape shape dim new_dim_size new_shape = list shape new_shape dim = new_dim_size tuple new_shape get_inputs input_shape hidden_shape mode returns list tuple input hidden where input hidden inputs model input = torch randn input_shape hidden = torch randn hidden_shape mode = LSTM input hidden hidden_shape == correct_hidden_shape input hidden hidden good_hidden = torch randn correct_hidden_shape input hidden good_hidden input good_hidden hidden rnn_modes = RNN GRU LSTM mode rnn_modes Incorrect input batch size input_shape = update_shape correct_input_shape bad_size hidden_shape = correct_hidden_shape test input_shape hidden_shape mode Incorrect hidden batch size input_shape = correct_input_shape hidden_shape = update_shape correct_hidden_shape bad_size test input_shape hidden_shape mode Incorrect input size input_shape = update_shape correct_input_shape bad_size hidden_shape = correct_hidden_shape test input_shape hidden_shape mode Incorrect hidden size input_shape = correct_input_shape hidden_shape = update_shape correct_hidden_shape bad_size test input_shape hidden_shape mode Incorrect hidden input_shape = correct_input_shape hidden_shape = update_shape correct_hidden_shape bad_size test input_shape hidden_shape mode test_projections_lstm_args_check input_size = hidden_size = proj_size = num_layers = batch_size = seq_len = num_directions = bad_size = prime number so no size can divide test input_shape hidden_h_shape hidden_c_shape input hidden get_inputs input_shape hidden_h_shape hidden_c_shape model = nn LSTM input_size hidden_size num_layers proj_size=proj_size assertRaises RuntimeError lambda model input hidden correct_input_shape = seq_len batch_size input_size correct_hidden_h_shape = num_layers num_directions batch_size proj_size correct_hidden_c_shape = num_layers num_directions batch_size hidden_size update_shape shape dim new_dim_size new_shape = list shape new_shape dim = new_dim_size tuple new_shape get_inputs input_shape hidden_h_shape hidden_c_shape returns list tuple input hidden where input hidden inputs model input = torch randn input_shape hidden_h = torch randn hidden_h_shape hidden_c = torch randn hidden_c_shape input hidden_h hidden_c Incorrect input batch size input_shape = update_shape correct_input_shape bad_size test input_shape correct_hidden_h_shape correct_hidden_c_shape Incorrect hidden batch size input_shape = correct_input_shape hidden_h_shape = update_shape correct_hidden_h_shape bad_size hidden_c_shape = update_shape correct_hidden_c_shape bad_size test input_shape hidden_h_shape hidden_c_shape Incorrect input size input_shape = update_shape correct_input_shape bad_size test input_shape correct_hidden_h_shape correct_hidden_c_shape Incorrect hidden size input_shape = correct_input_shape hidden_h_shape = update_shape correct_hidden_h_shape bad_size hidden_c_shape = update_shape correct_hidden_c_shape bad_size test input_shape hidden_h_shape hidden_c_shape Incorrect hidden input_shape = correct_input_shape hidden_h_shape = update_shape correct_hidden_h_shape bad_size hidden_c_shape = update_shape correct_hidden_c_shape bad_size test input_shape hidden_h_shape hidden_c_shape Incorrect proj size = hidden size input_shape = correct_input_shape hidden_h_shape = update_shape correct_hidden_h_shape hidden_size hidden_c_shape = correct_hidden_c_shape test input_shape hidden_h_shape hidden_c_shape Incorrect proj size = hidden size input_shape = correct_input_shape hidden_h_shape = update_shape correct_hidden_h_shape bad_size hidden_c_shape = correct_hidden_c_shape test input_shape hidden_h_shape hidden_c_shape Incorrect cell size = hidden size input_shape = correct_input_shape hidden_h_shape = correct_hidden_h_shape hidden_c_shape = update_shape correct_hidden_c_shape bad_size test input_shape hidden_h_shape hidden_c_shape unittest skipIf TEST_MULTIGPU multi-GPU supported test_rnn_check_device copy input_size = hidden_size = num_layers = batch_size = seq_len = num_directions = correct_input_shape = seq_len batch_size input_size correct_hidden_shape = num_layers num_directions batch_size hidden_size rnn_modes = RNN GRU LSTM mode rnn_modes model = getattr nn mode input_size hidden_size num_layers model_cuda = copy deepcopy model cuda input = torch randn correct_input_shape hidden = torch randn correct_hidden_shape input weights same device assertRaisesRegex RuntimeError Input parameter tensors same device model input cuda assertRaisesRegex RuntimeError Input parameter tensors same device model_cuda input input hiddens same device assertRaisesRegex RuntimeError r Input hidden tensors same device mode == LSTM model input hidden cuda hidden cuda model input hidden cuda assertRaisesRegex RuntimeError r Input hidden tensors same device mode == LSTM model_cuda input cuda hidden hidden model_cuda input cuda hidden hidden tensors same CUDA device mode == LSTM assertRaisesRegex RuntimeError Input hidden tensors same device model input cuda hidden cuda hidden cuda unittest skipIf TEST_MULTIGPU multi-GPU supported test_projections_lstm_check_device input_size = hidden_size = proj_size = num_layers = batch_size = seq_len = num_directions = correct_input_shape = seq_len batch_size input_size correct_hidden_h_shape = num_layers num_directions batch_size proj_size correct_hidden_c_shape = num_layers num_directions batch_size hidden_size model = nn LSTM input_size hidden_size num_layers proj_size=proj_size input = torch randn correct_input_shape hidden_h = torch randn correct_hidden_h_shape hidden_c = torch randn correct_hidden_c_shape input weights same device assertRaisesRegex RuntimeError Input parameter tensors same device model input cuda input hiddens same device assertRaisesRegex RuntimeError r Input hidden tensors same device model input hidden_h cuda hidden_c cuda hidden tensors same CUDA device assertRaisesRegex RuntimeError Input hidden tensors same device model input cuda hidden_h cuda hidden_c cuda test_rnn_initial_hidden_state rnn_modes = RNN GRU LSTM mode rnn_modes rnn = getattr nn mode input = torch randn hidden = torch zeros mode == LSTM hidden = hidden hidden output hidden = rnn input hidden output hidden = rnn input assertEqual output output assertEqual hidden hidden test_projections_lstm_initial_hidden_state bidir False True rnn = nn LSTM bidirectional=bidir proj_size= num_dirs = bidir input = torch randn hidden_h = torch zeros num_dirs hidden_c = torch zeros num_dirs hidden = hidden_h hidden_c output hidden = rnn input hidden output hidden = rnn input assertEqual output output assertEqual hidden hidden test_projections_errors_on_gru_and_rnn error_msg = proj_size argument only supported LSTM RNN GRU mode RNN GRU assertRaisesRegex ValueError error_msg rnn = getattr nn mode proj_size= _test_RNN_cpu_vs_cudnn dropout dtype=torch double forward_backward cuda rnn input_val grad_output weights_val hx_val grad_hy cx_val=None grad_cy=None is_lstm = isinstance rnn nn LSTM x_layer y_layer zip rnn all_weights weights_val x y zip x_layer y_layer x data copy_ y data isinstance input_val rnn_utils PackedSequence input = rnn_utils PackedSequence input_val data data requires_grad_ True input_val batch_sizes input_var = input data input = input_val clone requires_grad_ True input_var = input is_lstm cx_val None hx = hx_val clone requires_grad_ True hx_val add requires_grad_ True hx = hx_val clone requires_grad_ True cx_val add requires_grad_ True hx = hx_val clone requires_grad_ True cuda rnn cuda input_var data = input_var data cuda is_lstm hx data = hx data cuda hx data = hx data cuda hx data = hx data cuda grad_hy = grad_hy cuda grad_cy None grad_cy = grad_cy cuda grad_output = grad_output cuda output hy = rnn input hx isinstance output rnn_utils PackedSequence output = output data is_lstm grad_cy None torch autograd backward output hy hy grad_output grad_hy grad_hy + torch autograd backward output hy hy grad_output grad_hy grad_cy + torch autograd backward output hy grad_output grad_hy output output data hy hy data is_lstm hy data weights rnn all_weights grad_input input_var grad data grad_hx hx grad data is_lstm hx grad data cy hy data is_lstm None grad_cx hx grad data is_lstm None input_size = hidden_size = proj_size = num_layers = seq_length = batch = make_noncontig tensor ndim = tensor dim torch stack tensor clone zero_ tensor ndim select ndim compare_cpu_gpu outputs_cpu outputs_gpu assertEqual list outputs_cpu keys list outputs_gpu keys key outputs_cpu keys key = weights assertEqual outputs_cpu key outputs_gpu key atol= e- rtol= msg=key check grad weights separately nested dict cpu_layer_weight gpu_layer_weight zip outputs_cpu weights outputs_gpu weights cpu_weight gpu_weight zip cpu_layer_weight gpu_layer_weight assertEqual cpu_weight grad data gpu_weight grad data atol= e- rtol= module nn RNN nn LSTM nn GRU bias bidirectional batch_first contig variable_len lens_as_tensor \ product True False repeat= num_directions = bidirectional batch_first input_val = torch randn batch seq_length input_size dtype=dtype grad_output = torch randn batch seq_length hidden_size num_directions dtype=dtype input_val = torch randn seq_length batch input_size dtype=dtype grad_output = torch randn seq_length batch hidden_size num_directions dtype=dtype hx_val = torch randn num_layers num_directions batch hidden_size dtype=dtype grad_hy = torch randn num_layers num_directions batch hidden_size dtype=dtype contig grad_output = make_noncontig grad_output grad_hy = make_noncontig grad_hy input_var = make_noncontig input_val hx_val = make_noncontig hx_val variable_len lengths = lens_as_tensor lengths = torch tensor lengths dtype=torch long input_val = rnn_utils pack_padded_sequence input_val lengths batch_first=batch_first grad_output = rnn_utils pack_padded_sequence grad_output lengths batch_first=batch_first data rnn = module input_size hidden_size num_layers bias=bias dropout=dropout bidirectional=bidirectional batch_first=batch_first dtype outputs_cpu = forward_backward False rnn input_val grad_output rnn all_weights hx_val grad_hy rnn_gpu = module input_size hidden_size num_layers bias=bias dropout=dropout bidirectional=bidirectional batch_first=batch_first dtype outputs_gpu = forward_backward True rnn_gpu input_val grad_output rnn all_weights hx_val grad_hy compare_cpu_gpu outputs_cpu outputs_gpu nonlinearity tanh relu hx_val = torch randn num_layers batch hidden_size dtype=dtype input_val = torch randn seq_length batch input_size dtype=dtype grad_output = torch randn seq_length batch hidden_size num_directions dtype=dtype grad_hy = torch randn num_layers num_directions batch hidden_size dtype=dtype rnn = nn RNN input_size hidden_size num_layers bias=bias nonlinearity=nonlinearity dtype outputs_cpu = forward_backward False rnn input_val grad_output rnn all_weights hx_val grad_hy rnn_gpu = nn RNN input_size hidden_size num_layers bias=bias nonlinearity=nonlinearity dtype outputs_gpu = forward_backward True rnn_gpu input_val grad_output rnn all_weights hx_val grad_hy compare_cpu_gpu outputs_cpu outputs_gpu checking LSTM projections bias bidirectional batch_first contig variable_len lens_as_tensor \ product True False repeat= num_directions = bidirectional batch_first input_val = torch randn batch seq_length input_size dtype=dtype grad_output = torch randn batch seq_length proj_size num_directions dtype=dtype input_val = torch randn seq_length batch input_size dtype=dtype grad_output = torch randn seq_length batch proj_size num_directions dtype=dtype hx_val = torch randn num_layers num_directions batch proj_size dtype=dtype cx_val = torch randn num_layers num_directions batch hidden_size dtype=dtype grad_hy = torch randn num_layers num_directions batch proj_size dtype=dtype grad_cy = torch randn num_layers num_directions batch hidden_size dtype=dtype contig grad_output = make_noncontig grad_output grad_hy = make_noncontig grad_hy grad_cy = make_noncontig grad_cy input_var = make_noncontig input_val hx_val = make_noncontig hx_val cx_val = make_noncontig cx_val variable_len lengths = lens_as_tensor lengths = torch tensor lengths dtype=torch long input_val = rnn_utils pack_padded_sequence input_val lengths batch_first=batch_first grad_output = rnn_utils pack_padded_sequence grad_output lengths batch_first=batch_first data rnn = nn LSTM input_size hidden_size num_layers bias=bias dropout=dropout bidirectional=bidirectional batch_first=batch_first proj_size=proj_size dtype outputs_cpu = forward_backward False rnn input_val grad_output rnn all_weights hx_val grad_hy cx_val grad_cy rnn_gpu = nn LSTM input_size hidden_size num_layers bias=bias dropout=dropout bidirectional=bidirectional batch_first=batch_first proj_size=proj_size dtype outputs_gpu = forward_backward True rnn_gpu input_val grad_output rnn all_weights hx_val grad_hy cx_val grad_cy compare_cpu_gpu outputs_cpu outputs_gpu unittest skipIf TEST_CUDNN needs cudnn test_RNN_cpu_vs_cudnn_no_dropout dtype = torch double _test_RNN_cpu_vs_cudnn dtype unittest skipIf TEST_CUDNN needs cudnn test_RNN_cpu_vs_cudnn_with_dropout Because dropout randomness can only compare dropout= dropout= _test_RNN_cpu_vs_cudnn unittest skipIf TEST_CUDNN needs cudnn tf _on_and_off test_RNN_cudnn_weight_norm input_size = hidden_size = num_layers = seq_length = batch = runs CPU acquire expected output check_weight_norm m name input = torch randn seq_length batch input_size expected_output = m input adds weight normalization m = torch nn utils weight_norm m name=name moves CUDA m = m cuda input = input cuda otherwise subsequent warnings will hidden further tests rely them warnings simplefilter always assertEqual m input expected_output remove weight norm m = torch nn utils remove_weight_norm m name=name assertEqual m input expected_output check_weight_norm nn LSTM input_size hidden_size num_layers weight_hh_l check_weight_norm nn LSTM input_size hidden_size num_layers proj_size= weight_hr_l unittest skipIf TEST_CUDA CUDA available test_partial_flat_weights input_size = hidden_size = num_layers = m = nn LSTM input_size hidden_size num_layers inp = torch randn out_expected = m inp deletes attribute original LSTM weight_orig = m weight_hh_l del m weight_hh_l assertFalse hasattr m weight_hh_l verifies moving CUDA only some attributes defined does throw error m cuda recompute weight make sure module can used m weight_hh_l = weight_orig cuda inp = inp cuda otherwise subsequent warnings will hidden further tests rely them warnings simplefilter always assertEqual m inp cpu out_expected unittest skipIf TEST_CUDNN needs cudnn set_default_dtype torch double test_RNN_dropout checking assumption cuDNN sticks dropout between RNN layers p train True False cuda True False rnn = nn RNN bias=False dropout=p nonlinearity= relu cuda rnn cuda train rnn train rnn eval rnn weight_ih_l data fill_ rnn weight_hh_l data fill_ rnn weight_ih_l data fill_ rnn weight_hh_l data fill_ input = torch ones hx = torch zeros cuda input = input cuda hx = hx cuda output hy = rnn input hx assertEqual output data min output data max output_val = output data p == train assertEqual output_val p == assertEqual output_val assertGreater output_val assertLess output_val denorm_mod = output_val - p assertLess min denorm_mod - denorm_mod e- assertEqual hy data min hy data max assertEqual hy data min hy data max assertEqual hy data assertEqual hy data output_val unittest skipIf TEST_CUDNN needs cudnn set_default_dtype torch double test_error_RNN_seq_len_zero checking error message when RNN has seq_len = module nn RNN nn LSTM nn GRU bidirectional True False device get_all_device_types input = torch ones rnn = module bidirectional=bidirectional device == cuda rnn cuda input = input cuda assertRaisesRegex RuntimeError Expected sequence length larger than RNN rnn input test_RNN_input_size_zero module nn RNN nn LSTM nn GRU device get_all_device_types input = torch zeros rnn = module input_size= hidden_size= device == cuda rnn cuda input = input cuda outs = rnn input assertEqual outs shape torch Size Check backward does cause hard error outs sum backward unittest skipIf TEST_CUDNN needs cudnn test_RNN_dropout_state p train True False cuda True False rnn = nn RNN bias=False dropout=p nonlinearity= relu cuda rnn cuda train rnn train rnn eval input = torch rand hx = torch rand cuda input = input cuda hx = hx cuda output hy = rnn input hx output hy = rnn input hx buf = io BytesIO rnn_pickle = torch save rnn buf buf seek weights_only=False legacy code saves model rnn = torch load buf weights_only=False rnn flatten_parameters output hy = rnn input hx p == train assertEqual output output assertEqual output output assertEqual hy hy assertEqual hy hy assertNotEqual output output assertNotEqual output output assertNotEqual hy hy assertNotEqual hy hy unittest skipIf TEST_CUDNN needs cudnn set_default_dtype torch double test_RNN_change_dropout train cuda product True False repeat= rnn = nn RNN dropout= nonlinearity= relu input = torch rand cuda input data = input data cuda rnn cuda train rnn train rnn eval prev_output = None p rnn dropout = p output hy = rnn input output hy = rnn input p == p == train assertEqual output output assertEqual hy hy assertNotEqual output output assertNotEqual hy hy prev_output None train assertEqual output data prev_output assertEqual output data prev_output assertNotEqual output data prev_output assertNotEqual output data prev_output prev_output = output data test_inplace_thnn modules = nn ReLU nn ELU nn SELU nn CELU nn RReLU mod modules r = mod inplace=True input = torch randn requires_grad=True output = r input + grad_output = torch randn grad_output_clone = grad_output clone output backward grad_output assertEqual grad_output grad_output_clone test_pixel_shuffle_unshuffle _test_pixel_shuffle_unshuffle_helper num_input_dims valid_channels_dim=True upscale_factor=None Function imperatively ensure pixels shuffled correct locations Used validate batch operations pixel_shuffle _verify_pixel_shuffle input output upscale_factor c range output size - h range output size - w range output size - height_idx = h upscale_factor weight_idx = w upscale_factor channel_idx = upscale_factor h upscale_factor + w upscale_factor + \ c upscale_factor assertEqual output c h w input channel_idx height_idx weight_idx upscale_factor = random randint upscale_factor None upscale_factor If valid_channels_dim=False add make channels dim indivisible upscale_factor channels = random randint upscale_factor + valid_channels_dim height = random randint width = random randint num_input_dims == input = torch rand channels requires_grad=True num_input_dims == input = torch rand height width requires_grad=True batch_sizes = random randint _ range num_input_dims - input = torch rand batch_sizes channels height width requires_grad=True ps = nn PixelShuffle upscale_factor pus = nn PixelUnshuffle downscale_factor=upscale_factor num_input_dims = valid_channels_dim upscale_factor output = ps input _verify_pixel_shuffle input output upscale_factor output backward output data assertEqual input data input grad data Ensure unshuffle properly inverts shuffle unshuffle_output = pus output assertEqual input unshuffle_output assertRaises RuntimeError lambda ps input _test_pixel_unshuffle_error_case_helper num_input_dims valid_height_dim=True valid_width_dim=True downscale_factor=None downscale_factor = random randint downscale_factor None downscale_factor channels = random randint If valid_height_dim=False add make height dim indivisible downscale_factor height = random randint abs downscale_factor + valid_height_dim If valid_width_dim=False add make width dim indivisible downscale_factor width = random randint abs downscale_factor + valid_width_dim num_input_dims == input = torch rand channels requires_grad=True num_input_dims == input = torch rand height width requires_grad=True batch_sizes = random randint _ range num_input_dims - input = torch rand batch_sizes channels height width requires_grad=True pus = nn PixelUnshuffle downscale_factor assertRaises RuntimeError lambda pus input _test_pixel_shuffle_unshuffle_for_input_dims num_input_dims For D - D error case For D - D success case pixel_shuffle + pixel_unshuffle _test_pixel_shuffle_unshuffle_helper num_input_dims=num_input_dims Error cases pixel_shuffle _test_pixel_shuffle_unshuffle_helper num_input_dims=num_input_dims valid_channels_dim=False _test_pixel_shuffle_unshuffle_helper num_input_dims=num_input_dims upscale_factor= _test_pixel_shuffle_unshuffle_helper num_input_dims=num_input_dims upscale_factor=- Error cases pixel_unshuffle _test_pixel_unshuffle_error_case_helper num_input_dims=num_input_dims valid_height_dim=False _test_pixel_unshuffle_error_case_helper num_input_dims=num_input_dims valid_width_dim=False _test_pixel_unshuffle_error_case_helper num_input_dims=num_input_dims downscale_factor= _test_pixel_unshuffle_error_case_helper num_input_dims=num_input_dims downscale_factor=- test_pixel_shuffle_large_upscale_factor assertRaises ValueError ps = nn PixelShuffle ps torch randn test_pixel_shuffle_unshuffle_ D _test_pixel_shuffle_unshuffle_for_input_dims num_input_dims= test_pixel_shuffle_unshuffle_ D _test_pixel_shuffle_unshuffle_for_input_dims num_input_dims= test_pixel_shuffle_unshuffle_ D _test_pixel_shuffle_unshuffle_for_input_dims num_input_dims= test_pixel_shuffle_unshuffle_ D _test_pixel_shuffle_unshuffle_for_input_dims num_input_dims= test_pixel_shuffle_unshuffle_ D _test_pixel_shuffle_unshuffle_for_input_dims num_input_dims= test_pixel_shuffle_large_upscale_factor test_pixel_shuffle_unshuffle_ D test_pixel_shuffle_unshuffle_ D test_pixel_shuffle_unshuffle_ D test_pixel_shuffle_unshuffle_ D test_pixel_shuffle_unshuffle_ D set_default_dtype torch double test_pixel_shuffle_nhwc_cpu input = torch randn device= cpu input = input contiguous memory_format=torch channels_last requires_grad_ grad = torch randn device= cpu ps = torch nn PixelShuffle pus = torch nn PixelUnshuffle ref_input = input detach clone contiguous requires_grad_ True ref_grad = grad detach clone contiguous ref_ps = torch nn PixelShuffle ref_pus = torch nn PixelUnshuffle out = pus ps input out backward grad ref_out = ref_pus ref_ps ref_input ref_out backward ref_grad assertTrue out is_contiguous memory_format=torch channels_last assertTrue ref_out is_contiguous assertEqual out ref_out assertEqual input grad ref_input grad These tests should OpInfo d test_elu_inplace_on_view v = torch tensor - - requires_grad=True dtype=torch double func root x = root clone view = x narrow res = F elu view inplace=True assertIs res view x gradcheck func v gradgradcheck func v test_elu_inplace_gradgrad v = torch randn requires_grad=True dtype=torch double func root x = root clone F elu x inplace=True gradcheck func v gradgradcheck func v test_relu_inplace_on_view v = torch tensor - - requires_grad=True dtype=torch double func root x = root clone view = x narrow res = F relu view inplace=True assertIs res view x gradcheck func v gradgradcheck func v test_PReLU_backward_requires_grad_false devices = cpu devices += cuda TEST_CUDA d devices m = nn PReLU d x = torch randn device=d requires_grad=False y = m x y mean backward assertEqual x grad None test_bce_loss_always_nonnegative target = torch ones input = torch ones assertEqual nn BCELoss input target sum target = torch zeros input = torch zeros assertEqual nn BCELoss input target sum test_bce_with_logits_raises_if_target_and_input_are_different_size target = torch rand input = torch rand assertRaises ValueError nn BCEWithLogitsLoss input target target = torch rand input = torch rand assertRaises ValueError nn BCEWithLogitsLoss input target test_bce_with_logits_gives_same_result_as_sigmoid_and_bce_loss sigmoid = nn Sigmoid target = torch rand output = torch rand - assertEqual nn BCEWithLogitsLoss output target nn BCELoss sigmoid output target weight = torch rand assertEqual nn BCEWithLogitsLoss weight output target nn BCELoss weight sigmoid output target target = torch zeros dtype=torch float output = torch empty dtype=torch float fill_ - assertEqual nn BCEWithLogitsLoss output target nn BCELoss sigmoid output target assertEqual nn BCEWithLogitsLoss reduction= none output target nn BCELoss reduction= none sigmoid output target weight = torch rand dtype=torch float assertEqual nn BCEWithLogitsLoss weight output target nn BCELoss weight sigmoid output target test_bce_loss_input_range bceloss = nn BCELoss target = torch rand output_valid = torch rand output_too_negative = output_valid - output_too_positive = output_valid + loss_valid = bceloss output_valid target assertRaisesRegex RuntimeError between loss_too_negative = bceloss output_too_negative target assertRaisesRegex RuntimeError between loss_too_positive = bceloss output_too_positive target test_bce_loss_size_mismatch bceloss = nn BCELoss = torch rand b = torch rand assertRaisesRegex ValueError r Using target size \ bceloss b test_bce_with_logits_gives_same_result_as_sigmoid_and_bce_loss_large_tensors_with_grad x_size = y_size = target = torch rand x_size y_size reduction none mean sum output_sig = torch rand x_size y_size - output_logits = output_sig detach clone output_sig requires_grad = True output_logits requires_grad = True weight = torch rand y_size loss_sig = nn BCELoss weight reduction=reduction torch sigmoid output_sig target loss_logits = nn BCEWithLogitsLoss weight reduction=reduction output_logits target assertEqual loss_logits loss_sig reduction == none grad = torch rand x_size y_size loss_sig backward grad loss_logits backward grad loss_sig backward loss_logits backward assertEqual output_sig grad output_logits grad test_bce_with_logits_has_correct_forward_grad output = torch randn requires_grad=True dtype=torch double target = torch randn dtype=torch double reduction sum mean none gradcheck lambda target nn BCEWithLogitsLoss reduction=reduction target output target check_forward_ad=True test_bce_with_logits_has_correct_grad_at_zero output = torch zeros requires_grad=True target = torch zeros nn BCEWithLogitsLoss reduction= sum output target backward expected_grad = torch empty fill_ assertEqual output grad expected_grad test_bce_with_logits_broadcasts_weights target = torch rand output = torch rand - weight = torch rand out = nn BCEWithLogitsLoss weight output target weight = weight expand contiguous out = nn BCEWithLogitsLoss weight output target assertEqual out out weight = torch rand out = nn BCEWithLogitsLoss weight output target weight = weight expand contiguous out = nn BCEWithLogitsLoss weight output target assertEqual out out test_bce_with_logits_ones_in_pos_weights_are_the_same_as_none target = torch rand output = torch rand - pos_weight = torch ones assertEqual nn BCEWithLogitsLoss output target nn BCEWithLogitsLoss pos_weight=pos_weight output target test_bce_with_logits_broadcasts_pos_weights target = torch rand output = torch rand - pos_weight = torch rand out = nn BCEWithLogitsLoss pos_weight=pos_weight output target pos_weight = pos_weight expand out = nn BCEWithLogitsLoss pos_weight=pos_weight output target pos_weight = pos_weight expand out = nn BCEWithLogitsLoss pos_weight=pos_weight output target assertEqual out out assertEqual out out test_bce_with_logits_with_pos_weight_has_correct_grad_at_zero output = torch zeros requires_grad=True target = torch zeros pos_weight = torch ones nn BCEWithLogitsLoss pos_weight=pos_weight reduction= sum output target backward expected_grad = torch empty fill_ grad = output grad assertEqual grad expected_grad test_bce_with_logits_stability output = torch tensor - target = torch tensor pos_weight = torch tensor out = nn BCEWithLogitsLoss output target assertTrue torch isfinite out all item out = nn BCEWithLogitsLoss pos_weight=pos_weight output target assertTrue torch isfinite out all item test_bce_loss_broadcasts_weights sigmoid = nn Sigmoid target = torch rand output = torch rand - weight = torch rand out = nn BCELoss weight sigmoid output target weight = weight expand contiguous out = nn BCELoss weight sigmoid output target assertEqual out out weight = torch rand out = nn BCELoss weight sigmoid output target weight = weight expand contiguous out = nn BCELoss weight sigmoid output target assertEqual out out test_hardtanh_inplace_gradgrad v = torch randn requires_grad=True dtype=torch double func root x = root clone F hardtanh x inplace=True gradcheck func v gradgradcheck func v test hardtanh backward large tensor test_hardtanh_backward x = torch randn requires_grad=True grad = torch randn z = torch zeros y = F hardtanh x y backward grad ref backward path hardtanh mask = x - x x_grad_ref = torch where mask grad z assertEqual x grad x_grad_ref test_batchnorm_nhwc_cpu helper mod size dtype mixed_dtype=False format=torch channels_last precision=None channels = size input = torch randn size dtype=dtype device= cpu requires_grad=True input = input contiguous memory_format=format dtype input retain_grad grad = torch randn size dtype=dtype device= cpu grad = grad contiguous memory_format=format bn = mod channels cpu dtype bn weight data uniform_ bn bias data uniform_ ref_input = input detach clone contiguous requires_grad_ True ref_grad = grad detach clone contiguous ref_bn = mod channels cpu dtype ref_bn load_state_dict bn state_dict mixed_dtype bn float ref_bn float out = bn input out backward grad ref_out = ref_bn ref_input ref_out backward ref_grad assertTrue out is_contiguous memory_format=format assertTrue ref_out is_contiguous assertEqual out ref_out assertEqual bn weight grad ref_bn weight grad atol=precision rtol=precision assertEqual bn bias grad ref_bn bias grad assertEqual input grad ref_input grad test NC N HW test mixed dtype shape dtype torch float torch bfloat torch float mixed_dtype False True dtype == torch float mixed_dtype = False helper nn BatchNorm d shape dtype mixed_dtype torch channels_last precisons = torch float e- torch bfloat e- torch float None shape dtype torch float torch bfloat torch float mixed_dtype False True dtype == torch float mixed_dtype = False helper nn BatchNorm d shape dtype mixed_dtype torch channels_last_ d precisons dtype test_batchnorm_half_overflow helper mod size param_dtype fwd_format bwd_format channels = size input = torch randn size dtype=torch half device= cpu input = input contiguous memory_format=fwd_format requires_grad_ True bn = mod channels cpu param_dtype out = bn input ref_input = input detach clone requires_grad_ True ref_bn = mod channels cpu torch float ref_bn load_state_dict bn torch float state_dict ref_out = ref_bn ref_input assertFalse out isinf any assertFalse out isnan any assertEqual out ref_out param_dtype = torch half grad_input = torch empty size=ref_out shape uniform_ dtype=torch half grad_input = grad_input contiguous memory_format=bwd_format ref_grad_input = grad_input clone out backward grad_input ref_out backward ref_grad_input assertFalse input grad isinf any assertFalse input grad isnan any assertEqual input grad ref_input grad format torch contiguous_format torch channels_last helper nn BatchNorm d torch half format format format torch contiguous_format torch channels_last_ d helper nn BatchNorm d torch half format format formats = torch contiguous_format torch channels_last torch contiguous_format torch channels_last_ d fwd_format bwd_format itertools product formats formats helper nn BatchNorm d torch float fwd_format bwd_format fwd_format bwd_format itertools product formats formats helper nn BatchNorm d torch float fwd_format bwd_format parametrize_test bn_module subtest torch nn BatchNorm d name= BatchNorm d subtest torch nn SyncBatchNorm name= SyncBatchNorm test_batchnorm_non_contig_cpu bn_module helper dtype input = torch arange dtype=torch float reshape cpu input = input permute bn = bn_module cpu float eval bn weight data uniform_ bn bias data uniform_ ref_input = input detach clone contiguous ref_bn = nn BatchNorm d cpu float eval ref_bn load_state_dict bn state_dict out = bn input ref_out = ref_bn ref_input assertTrue out is_contiguous memory_format=torch channels_last assertTrue ref_out is_contiguous assertEqual out ref_out input_bf = torch arange dtype=dtype reshape input_bf = input_bf permute input_f = input_bf float bn_mix = bn_module float eval ref_bn_f = deepcopy bn_mix out_bf = bn_mix input_bf ref_out_bf = ref_bn_f input_f assertEqual ref_out_bf out_bf float atol= rtol= helper torch bfloat helper torch float unittest skipIf TEST_CUDA CUDA unavailable unittest skipIf TEST_CUDNN needs cudnn test_batchnorm_cudnn_nhwc run_test input grad_output c = input size mod = nn BatchNorm d c cuda float mod weight data uniform_ mod bias data uniform_ ref_input = input detach clone contiguous requires_grad_ True ref_grad = grad detach clone contiguous ref_mod = nn BatchNorm d c cuda float ref_mod load_state_dict mod state_dict out = mod input out backward grad_output ref_out = ref_mod ref_input ref_out backward ref_grad assertTrue out is_contiguous memory_format=torch channels_last assertTrue ref_out is_contiguous assertEqual out ref_out assertEqual mod weight grad ref_mod weight grad assertEqual mod bias grad ref_mod bias grad assertEqual input grad ref_input grad input = torch randint dtype=torch float device= cuda input = input contiguous memory_format=torch channels_last detach requires_grad_ grad = torch randint dtype=torch float device= cuda grad = grad contiguous memory_format=torch channels_last run_test input grad see grad channels_last contiguous grad suggest_memory_format rightly contiguous channels_last input = torch randint dtype=torch float device= cuda input = input contiguous memory_format=torch channels_last detach requires_grad_ grad = torch randint dtype=torch float device= cuda grad = grad permute run_test input grad unittest skipIf TEST_CUDA CUDA unavailable test_batchnorm_cudnn_half THNN input = torch randint dtype=torch half device= cuda requires_grad=True m = nn BatchNorm d half cuda thnn_output = m input thnn_output sum backward thnn_input_grad = input grad data clone assertEqualTypeString thnn_output input cuDNN TEST_CUDNN input grad = None m = m float cudnn_output = m input cudnn_output sum backward cudnn_input_grad = input grad data clone assertEqualTypeString cudnn_output input assertEqual cudnn_output thnn_output assertEqual cudnn_input_grad thnn_input_grad atol= e- rtol= unittest skipIf TEST_CUDA CUDA unavailable test_batchnorm_nonaffine_cuda_half_input input = torch randn dtype=torch half device= cuda m = nn BatchNorm d affine=False cuda float keep running stats FP output = m input assertEqualTypeString output input m eval output = m input assertEqualTypeString output input test_batchnorm_raises_error_if_less_than_one_value_per_channel x = torch rand None None assertRaises ValueError torch nn BatchNorm d x test_batchnorm_raises_error_if_running_mean_is_not_same_size_as_input input = torch rand running_var = torch rand wrong_sizes = size wrong_sizes assertRaises RuntimeError F batch_norm input torch rand size running_var test_batchnorm_raises_error_if_running_var_is_not_same_size_as_input input = torch rand running_mean = torch rand wrong_sizes = size wrong_sizes assertRaises RuntimeError F batch_norm input running_mean torch rand size test_batchnorm_raises_error_if_weight_is_not_same_size_as_input input = torch rand running_mean = torch rand running_var = torch rand wrong_sizes = size wrong_sizes assertRaises RuntimeError F batch_norm input running_mean running_var weight=Parameter torch rand size test_batchnorm_raises_error_if_bias_is_not_same_size_as_input input = torch rand running_mean = torch rand running_var = torch rand wrong_sizes = size wrong_sizes assertRaises RuntimeError F batch_norm input running_mean running_var bias=Parameter torch rand size test_batchnorm_raises_error_if_running_var_or_running_mean_have_forward_grad args = torch randn input torch randn running_mean torch randn running_var kwargs = training False momentum - fn = partial F batch_norm kwargs dual_indices tangents = tuple torch rand_like x x args fwAD dual_level duals = fwAD make_dual primal tangent i dual_indices primal i primal tangent enumerate zip args tangents msg = batch_norm differentiable wrt running_mean running_var needs have forward grad because otherwise we won t even run batch_norm_jvp dual_indices dual_indices dual_indices assertRaisesRegex RuntimeError msg fn duals fn duals test_batchnorm_buffer_update_when_stats_are_not_tracked input_size = Instantiate BN buffers None bn = nn BatchNorm d input_size track_running_stats=True Use buffers normalization don t update them bn track_running_stats = False Store initial values num_batches = bn num_batches_tracked clone running_mean = bn running_mean clone running_var = bn running_var clone Forward random tensor _ = bn torch rand input_size Ensure none buffers has been updated assertTrue torch equal num_batches bn num_batches_tracked assertTrue torch equal running_mean bn running_mean assertTrue torch equal running_var bn running_var unittest skipIf torch cuda is_available CUDA available parametrize_test dims name_fn=lambda x f x D parametrize_test mode train inference name_fn=lambda x x parametrize_test test verifies cudnn miopen batchnorm reference backend memory format memory_format - one NCHW NHWC ref_backend - one cpu native NCHW NHWC cpu - cpu backend same memory_format will used reference native - native backend ` torch backends cudnn flags enabled=False ` same memory_format will used NCHW NHWC - same backend will used another memory format mixed - True False Mixed batchnorm mode where inputs -bit batchnorm fp memory_format ref_backend mixed dtype NCHW cpu False torch float NCHW cpu True torch half NCHW cpu True torch bfloat NCHW native False torch float NCHW native True torch half NCHW native True torch bfloat NHWC cpu False torch float NHWC cpu True torch half NHWC cpu True torch bfloat NHWC native False torch float NHWC native True torch half NHWC native True torch bfloat NHWC NCHW False torch float NHWC NCHW True torch half NHWC NCHW True torch bfloat name_fn=lambda f b m t f f _vs_ b _mixed m _ dtype_name t test_batchnorm dims mode memory_format ref_backend mixed dtype torch version cuda _testMethodName test_batchnorm_ D_train_NCHW_vs_cpu_mixed_bfloat test_batchnorm_ D_train_NCHW_vs_cpu_mixed_bfloat test_batchnorm_ D_train_NHWC_vs_NCHW_mixed_bfloat test_batchnorm_ D_train_NHWC_vs_NCHW_mixed_bfloat test_batchnorm_ D_train_NCHW_vs_native_mixed_float skipTest Failed CUDA torch version hip _testMethodName test_batchnorm_ D_train_NCHW_vs_cpu_mixed_bfloat test_batchnorm_ D_train_NCHW_vs_cpu_mixed_bfloat test_batchnorm_ D_train_NHWC_vs_NCHW_mixed_bfloat test_batchnorm_ D_train_NHWC_vs_NCHW_mixed_bfloat \ _get_torch_rocm_version NCHW bfloat path uses native kernels rocm = train failed rocm = due native accuracy issue https github com pytorch pytorch issues skipTest bfloat NHWC train failed ROCm = _testMethodName test_batchnorm_ D_train_NCHW_vs_native_mixed_bfloat test_batchnorm_ D_train_NCHW_vs_native_mixed_bfloat \ _get_torch_rocm_version = https github com pytorch pytorch issues skipTest bfloat NCHW train failed due native tolerance issue _testMethodName == test_batchnorm_ D_train_NCHW_vs_native_mixed_float skipTest D float NCHW train failed ROCm dims == memory_format NHWC NCHW memory_format = memory_format + D _create_tensor size memory_format dtype device t = torch empty size=size memory_format=memory_format dtype=dtype device=device t = t random_ t _get_ref_device backend str device str If backend specifies memory format device arg otherwise device matches backend backend NHWC NHWC D NCHW NCHW D device backend == native cuda backend == cpu cpu raise ValueError Unknown backend _get_backend_memory_format backend str memory_format torch memory_format - torch memory_format If backend specifies memory format otherwise look memory_format arg backend == NHWC torch channels_last backend == NHWC D torch channels_last_ d backend NCHW NCHW D torch contiguous_format memory_format torch contiguous_format torch channels_last torch channels_last_ d memory_format raise ValueError f Unable detect memory format backend= backend memory_format= memory_format _get_memory_format t torch Tensor - torch memory_format t is_contiguous memory_format=torch contiguous_format torch contiguous_format t is_contiguous memory_format=torch channels_last torch channels_last t is_contiguous memory_format=torch channels_last_ d torch channels_last_ d ValueError Unsupported memory_format _get_memory_format_from_name memory_format_name str - torch memory_format memory_format_name == NHWC torch channels_last memory_format_name == NHWC D torch channels_last_ d memory_format_name NCHW NCHW D torch contiguous_format ValueError Unsupported memory_format _create_backend inp torch Tensor mixed bool = False inp dim == nn BatchNorm d inp size device=inp device dtype=torch float mixed inp dtype nn BatchNorm d inp size device=inp device dtype=torch float mixed inp dtype _test_batchnorm_train inp grad mixed ref_inp ref_grad ref_backend mod = _create_backend inp mixed train mod weight data uniform_ mod bias data uniform_ ref_mod = _create_backend ref_inp mixed train ref_mod load_state_dict mod state_dict out = mod inp out backward grad torch backends cudnn flags enabled=False ref_backend == native contextlib nullcontext ref_out = ref_mod ref_inp ref_out backward ref_grad assertTrue out is_contiguous memory_format=_get_memory_format inp assertTrue ref_out is_contiguous memory_format=_get_memory_format ref_inp assertEqual out ref_out assertEqual mod weight grad ref_mod weight grad assertEqual mod bias grad ref_mod bias grad assertEqual mod running_mean ref_mod running_mean assertEqual mod running_var ref_mod running_var assertEqual inp grad ref_inp grad _train memory_format_name ref_backend mixed dtype memory_format = _get_memory_format_from_name memory_format_name ref_memory_format = _get_backend_memory_format ref_backend memory_format ref_device = _get_ref_device ref_backend device= cuda size = memory_format_name NCHW D NHWC D inp = _create_tensor size memory_format dtype device= cuda detach requires_grad_ grad = _create_tensor size memory_format dtype device= cuda ref_inp = inp detach clone memory_format=ref_memory_format device=ref_device requires_grad_ ref_grad = grad detach clone memory_format=ref_memory_format device=ref_device _test_batchnorm_train inp=inp grad=grad mixed=mixed ref_inp=ref_inp ref_grad=ref_grad ref_backend=ref_backend _inference memory_format_name ref_backend mixed dtype memory_format = _get_memory_format_from_name memory_format_name ref_memory_format = _get_backend_memory_format ref_backend memory_format ref_device = _get_ref_device ref_backend device= cuda size = memory_format_name NCHW D NHWC D inp = _create_tensor size memory_format dtype device= cuda ref_inp = inp detach clone memory_format=ref_memory_format device=ref_device mod = _create_backend inp mixed eval ref_mod = _create_backend ref_inp mixed eval out = mod inp torch backends cudnn flags enabled=False ref_backend == native contextlib nullcontext ref_out = ref_mod ref_inp assertEqual out ref_out mode == train _train memory_format ref_backend mixed dtype _inference memory_format ref_backend mixed dtype unittest skipIf torch cuda is_available CUDA available test_batchnorm_nhwc_cuda dtype torch half torch float N C H W = model = torch nn BatchNorm d C eps= e- momentum= affine=True track_running_stats=True model = model eval cuda dtype inp = torch randn N C H W device=torch device cuda dtype=dtype inp = inp contiguous memory_format=torch channels_last out = model inp out = model inp assertTrue torch equal out out test_batchnorm_load_state_dict bn = torch nn BatchNorm d assertEqual bn state_dict num_batches_tracked torch tensor bn num_batches_tracked = torch tensor assertEqual bn state_dict num_batches_tracked torch tensor empty_dict = OrderedDict bn load_state_dict empty_dict strict=False assertEqual bn state_dict num_batches_tracked torch tensor test when ` num_batches_tracked ` loaded state_dict meta num_batches_tracked still replaced singleton tensor torch device meta meta_bn = torch nn BatchNorm d assertTrue meta_bn num_batches_tracked device == torch device meta meta_bn load_state_dict empty_dict assign=True strict=False assertEqual meta_bn state_dict num_batches_tracked torch tensor test_batch_norm_update_stats input = torch rand running_mean = torch rand running_var = torch rand assertRaisesRegex RuntimeError re escape input tensor must have least one element got input_sizes = torch batch_norm_update_stats input=input momentum= running_mean=running_mean running_var=running_var test_pairwise_distance input = torch randn requires_grad=True dtype=torch double input = torch randn requires_grad=True dtype=torch double assertTrue gradcheck lambda x y F pairwise_distance x y input input TODO Create OpInfo pdist test_pdist device trans itertools product device_ False True inp = torch randn dtype=torch double device=device requires_grad=True trans inp = inp transpose p float inf assertTrue gradcheck lambda x F pdist x p inp test_pdist_zeros Test grad still valid when dist device device_ inp = torch randn dtype=torch double device=device requires_grad=True repeat p float inf assertTrue gradcheck lambda x F pdist x p inp test_pdist_empty_row device device_ inp = torch randn dtype=torch double device=device requires_grad=True assertTrue gradcheck F pdist inp test_pdist_empty_col device device_ inp = torch randn dtype=torch double device=device requires_grad=True assertTrue gradcheck F pdist inp unittest expectedFailure test_pdist_cpu_gradgrad_unimplemented inp = torch randn requires_grad=True gradgradcheck F pdist inp unittest expectedFailure test_pdist_cuda_gradgrad_unimplemented inp = torch randn device= cuda requires_grad=True gradgradcheck F pdist inp Merge into OpInfo test backward https github com pytorch pytorch issues test_pdist_large device device_ func x torch pdist x p= shape should able roughly arbitrarily large kernel currently limited smaller sizes see issue above just testing floor shape = x = torch randn shape device=device requires_grad_ output = torch pdist x p= just run single backward gradcheck gradgradcheck expensive here output sum backward test_cosine_embedding_loss_with_diff_type device device_ input = torch tensor dtype=torch double device=device input = torch tensor dtype=torch double device=device target = torch tensor - dtype=torch int device=device expected = torch nn functional cosine_embedding_loss input input target dt get_all_math_dtypes device dt get_all_math_dtypes device dt get_all_math_dtypes device dt used dtype target = - so let s skip unsigned type dt == torch uint continue dt is_complex dt is_complex dt is_complex continue input = input dt input = input dt target = target dt result = torch nn functional cosine_embedding_loss input input target assertEqual result item expected item atol= rtol= test_cosine_embedding_loss_error_on_diff_shapes device device_ input = torch empty dtype=torch double device=device input = torch empty dtype=torch double device=device target = torch empty dtype=torch int device=device assertRaisesRegex RuntimeError expects D torch nn functional cosine_embedding_loss input input target test_cosine_embedding_loss_error_on_nonexpandable_shapes device device_ input = torch empty dtype=torch double device=device input = torch empty dtype=torch double device=device target = torch ones dtype=torch int device=device assertRaisesRegex RuntimeError must match size torch nn functional cosine_embedding_loss input input target test_kl_div_with_diff_type device device_ input = torch tensor dtype=torch double device=device target = torch tensor dtype=torch double device=device expected = torch nn functional kl_div input target real_dtypes = torch float torch float torch float input_dtype target_dtype product real_dtypes repeat= torch device device type == cpu target_dtype == torch float continue input = input input_dtype target = target target_dtype result = torch nn functional kl_div input target assertEqual result item expected item atol= rtol= test_kl_div_with_diff_type_log_target device device_ input = torch tensor dtype=torch double device=device target = torch tensor dtype=torch double device=device log expected = torch nn functional kl_div input target log_target=True real_dtypes = torch float torch float torch float input_dtype target_dtype product real_dtypes repeat= torch device device type == cpu target_dtype == torch float continue input = input input_dtype target = target target_dtype result = torch nn functional kl_div input target log_target=True assertEqual result item expected item atol= rtol= test_kl_div_log_softmax_target device device_ = torch tensor device=device b = torch tensor device=device assertEqual F kl_div F log_softmax F log_softmax b reduction= none log_target=True torch zeros_like test_cosine_embedding_loss_no_reduce input = torch randn requires_grad=True dtype=torch double input = torch randn requires_grad=True dtype=torch double target = torch randn dtype=torch double sign assertTrue gradcheck lambda x y z F cosine_embedding_loss x y z reduction= none input input target assertEqual F cosine_embedding_loss input input target reduction= none loss_reference_fns CosineEmbeddingLoss input input target reduction= none test_cosine_embedding_loss_margin_no_reduce input = torch randn requires_grad=True dtype=torch double input = torch randn requires_grad=True dtype=torch double target = torch randn dtype=torch double sign assertTrue gradcheck lambda x y z F cosine_embedding_loss x y z margin= reduction= none input input target assertEqual F cosine_embedding_loss input input target margin= reduction= none loss_reference_fns CosineEmbeddingLoss input input target margin= reduction= none test_cosine_embedding_loss_invalid_shape input = torch randn input = torch randn target = torch randn sign assertRaisesRegex RuntimeError D target tensor expected F cosine_embedding_loss input input target assertRaisesRegex RuntimeError D target tensor expects D input tensors F cosine_embedding_loss torch randn torch randn torch randn assertRaisesRegex RuntimeError D target tensor expects D input tensors F cosine_embedding_loss torch randn torch randn torch randn test_margin_ranking_loss_no_reduce input = torch randn dtype=torch double mul_ requires_grad_ input = torch randn dtype=torch double mul_ requires_grad_ target = torch randn dtype=torch double sign assertTrue gradcheck lambda x y z F margin_ranking_loss x y z reduction= none input input target assertEqual F margin_ranking_loss input input target reduction= none loss_reference_fns MarginRankingLoss input input target reduction= none test_margin_ranking_loss_margin_no_reduce input = torch randn dtype=torch double mul_ requires_grad_ input = torch randn dtype=torch double mul_ requires_grad_ target = torch randn dtype=torch double sign assertTrue gradcheck lambda x y z F margin_ranking_loss x y z margin= reduction= none input input target assertEqual F margin_ranking_loss input input target margin= reduction= none loss_reference_fns MarginRankingLoss input input target margin= reduction= none test_triplet_margin_loss input = torch randn requires_grad=True dtype=torch double input = torch randn requires_grad=True dtype=torch double input = torch randn requires_grad=True dtype=torch double assertTrue gradcheck lambda x x x F triplet_margin_loss x x x input input input assertEqual F triplet_margin_loss input input input loss_reference_fns TripletMarginLoss input input input test_triplet_margin_loss_swap input = torch randn requires_grad=True dtype=torch double input = torch randn requires_grad=True dtype=torch double input = torch randn requires_grad=True dtype=torch double assertTrue gradcheck lambda x x x F triplet_margin_loss x x x swap=True input input input assertEqual F triplet_margin_loss input input input swap=True loss_reference_fns TripletMarginLoss input input input swap=True test_triplet_margin_loss_no_reduce input = torch randn requires_grad=True dtype=torch double input = torch randn requires_grad=True dtype=torch double input = torch randn requires_grad=True dtype=torch double assertTrue gradcheck lambda x x x F triplet_margin_loss x x x reduction= none input input input assertEqual F triplet_margin_loss input input input reduction= none loss_reference_fns TripletMarginLoss input input input reduction= none test_triplet_margin_loss_swap_no_reduce input = torch randn requires_grad=True dtype=torch double input = torch randn requires_grad=True dtype=torch double input = torch randn requires_grad=True dtype=torch double assertTrue gradcheck lambda x x x F triplet_margin_loss x x x swap=True reduction= none input input input assertEqual F triplet_margin_loss input input input swap=True reduction= none loss_reference_fns TripletMarginLoss input input input swap=True reduction= none test_pointwise_loss_target_grad_none_reduction i = torch randn t = torch randn requires_grad=True assertEqual F mse_loss i t reduction= none size t size assertEqual F l _loss i t reduction= none size t size test_pointwise_loss_broadcast losses = mse_loss lambda x y r F mse_loss x y reduction=r l _loss lambda x y r F l _loss x y reduction=r smooth_l _loss lambda x y r F smooth_l _loss x y reduction=r huber_loss lambda x y r F huber_loss x y reduction=r input = torch randn requires_grad=True dtype=torch double fn losses values requires_grad True False When target requires_grad=True its impl Python while other TH target = torch randn requires_grad=requires_grad dtype=torch double reduction none mean sum l = fn input target reduction reduction == none assertEqual l size target size assertTrue gradcheck fn input target reduction https github com pytorch pytorch issues reports l _loss get wrong result big batch size test_l _loss_correct dtype torch float torch cfloat N range input = torch rand N dtype=dtype assertEqual torch nn L Loss input torch zeros_like input input abs mean test_smoothl loss_intergral_target _input_grad input target reduction output = F smooth_l _loss input target reduction=reduction beta= output sum backward input grad device dtype reduction product device_ integral_types none sum mean input = torch randn device=device requires_grad=True target = torch randint device=device dtype=dtype input_grad_with_float_target = _input_grad input target float reduction input_grad = _input_grad input detach clone requires_grad_ True target reduction assertEqual input_grad input_grad_with_float_target test_smoothl loss_negative_beta_not_supported assertRaises RuntimeError F smooth_l _loss torch randn torch randn beta=- test_huber_loss_invalid_delta _test_huber_loss_delta_error_helper delta input target = torch randn torch randn loss = torch nn HuberLoss delta=delta assertRaises RuntimeError loss input target test_huber_loss_negative_delta _test_huber_loss_delta_error_helper delta=- test_huber_loss_zero_delta _test_huber_loss_delta_error_helper delta= test_huber_loss_negative_delta test_huber_loss_zero_delta set_default_dtype torch double test_cosine_similarity Check cosine_similarity input output shapes input_size = expected_size = input = torch randn input_size requires_grad=True input = torch randn input_size requires_grad=True assertEqual F cosine_similarity input input dim= size expected_size Check numerical precision issue vv = torch tensor float i i range unsqueeze vv = torch tensor float i i range unsqueeze out = F cosine_similarity vv vv assertLessEqual out Check dividing previous behavior x y max eps &#124; &#124; x &#124; &#124; &#124; &#124; y &#124; &#124; current x max eps &#124; &#124; x &#124; &#124; y max eps &#124; &#124; y &#124; &#124; f x y cosine similarity then df dx = y &#124; &#124; x &#124; &#124; &#124; &#124; y &#124; &#124; - x x y &#124; &#124; y &#124; &#124; &#124; &#124; x &#124; &#124; &#124; &#124; x &#124; &#124; &#124; &#124; y &#124; &#124; ^ tests below check division zero backward formula when x = input = y = input = For these inputs gradient wrt x simplifies g x y = y &#124; &#124; x &#124; &#124; &#124; &#124; y &#124; &#124; Previous test checks g x y == y eps Current test checks g x y == y &#124; &#124; y &#124; &#124; eps input = torch randn requires_grad_ input = torch zeros_like input requires_grad_ torch cosine_similarity input input sum backward assertEqual input grad torch zeros_like input assertEqual input grad input input norm e Check type promotion issue input = torch tensor out = F cosine_similarity input torch int input dim=- assertEqual out Check broadcasting = torch ones dtype=torch float b = torch ones dtype=torch float out = F cosine_similarity b assertEqual out torch ones dtype=torch float = torch ones dtype=torch float b = torch ones dtype=torch float out = F cosine_similarity b assertEqual out torch ones dtype=torch float test_grid_sample_error_checking input = torch empty grid = torch empty assert no error F grid_sample input grid align_corners=False assertRaisesRegex ValueError got garbage F grid_sample input grid mode= garbage align_corners=False assertRaisesRegex ValueError got garbage F grid_sample input grid padding_mode= garbage align_corners=False assertRaisesRegex RuntimeError expected grid have size last dimension F grid_sample input grid align_corners=False assertRaisesRegex RuntimeError expected grid have size last dimension F grid_sample input torch empty align_corners=False assertRaisesRegex RuntimeError expected grid input have same batch size F grid_sample input torch empty align_corners=False assertRaisesRegex RuntimeError expected grid have size last dimension F grid_sample input torch empty align_corners=False assertRaisesRegex RuntimeError expected input have non-empty spatial dimensions F grid_sample torch empty grid align_corners=False assertRaisesRegex RuntimeError bicubic interpolation only supports D input F grid_sample torch empty torch empty mode= bicubic TEST_CUDA assertRaisesRegex RuntimeError Expected all tensors same device F grid_sample input cuda grid align_corners=False test_affine_grid_error_checking D affine theta = torch empty dtype=torch double size = torch Size assert no error F affine_grid theta size align_corners=False check warning empty span along dimension warnings catch_warnings record=True w Ensure warnings being shown warnings simplefilter always Should trigger warning F affine_grid theta torch Size align_corners=False Check no warning occurs assertNotIn See documentation affine_grid details join map str w Should trigger warning F affine_grid theta torch Size align_corners=True Check warning occurs assertIn See documentation affine_grid details join map str w assertRaisesRegex ValueError Expected theta have floating point type F affine_grid theta int size align_corners=False assertRaisesRegex ValueError Expected batch D affine matrices shape Nx x F affine_grid theta size align_corners=False assertRaisesRegex ValueError Expected batch D affine matrices shape Nx x F affine_grid theta unsqueeze size align_corners=False assertRaisesRegex ValueError Expected batch D affine matrices shape Nx x F affine_grid theta repeat size align_corners=False assertRaisesRegex ValueError Expected batch D affine matrices shape Nx x F affine_grid theta repeat size align_corners=False D affine theta = torch empty dtype=torch double size = torch Size assert no error F affine_grid theta size align_corners=False check warning empty span along dimension warnings catch_warnings record=True w Ensure warnings being shown warnings simplefilter always Should trigger warning F affine_grid theta torch Size align_corners=False Check no warning occurs assertNotIn See documentation affine_grid details join map str w Should trigger warning F affine_grid theta torch Size align_corners=True Check warning occurs assertIn See documentation affine_grid details join map str w assertRaisesRegex ValueError Expected batch D affine matrices shape Nx x F affine_grid theta size align_corners=False assertRaisesRegex ValueError Expected batch D affine matrices shape Nx x F affine_grid theta unsqueeze size align_corners=False assertRaisesRegex ValueError Expected batch D affine matrices shape Nx x F affine_grid theta repeat size align_corners=False assertRaisesRegex ValueError Expected batch D affine matrices shape Nx x F affine_grid theta repeat size align_corners=False assertRaisesRegex NotImplementedError affine_grid only supports D D sizes F affine_grid theta torch Size align_corners=False assertRaisesRegex NotImplementedError affine_grid only supports D D sizes F affine_grid theta torch Size align_corners=False parametrize_test device cpu + cuda TEST_CUDA parametrize_test nd test_affine_grid_backward_cl_cf_consistency device nd Test based reported issue https github com pytorch pytorch issues theta = torch rand nd nd + requires_grad=True device=device size = nd == grid = torch nn functional affine_grid theta size align_corners=False grad_tensor = torch rand grid shape device=device memory_format_cl = torch channels_last nd == torch channels_last_ d grad_tensor_cl = grad_tensor contiguous memory_format=memory_format_cl assert theta grad None grid backward grad_tensor_cl theta_grad_cl = theta grad clone contiguous theta grad zero_ grid backward grad_tensor theta_grad_cf = theta grad assertEqual theta_grad_cf theta_grad_cl set_default_dtype torch double test_grid_sample Backward pass native C++ CUDA kernels branch depending whether input requires gradient so we test both cases test N C H W mode padding_mode align_corners input_requires_grad test_shape N C IH IW H W mode padding_mode align_corners grid_dim_contig_order grid_dim_contig_order specifies dimension order can make grid contiguous i e grid permute grid_dim_contig_order contiguous e g grid_dim_contig_order= grid should initialized contiguous tensor shape N H W permuted N H W afterwards grid_shape = N H W grid_init_shape = grid_shape d d grid_dim_contig_order grid_fwd_permute = None None None None i d enumerate grid_dim_contig_order grid_fwd_permute d = i get_grid device= cpu data=None data None assert list data shape == grid_shape data = data permute grid_dim_contig_order device data = torch randn grid_init_shape device=device grid = data permute grid_fwd_permute assert grid permute grid_dim_contig_order is_contiguous grid input_cpu = torch randn C N IH IW transpose requires_grad_ input_requires_grad grid_cpu = get_grid requires_grad_ out_cpu = F grid_sample input_cpu grid_cpu mode=mode padding_mode=padding_mode align_corners=align_corners assertTrue out_cpu size == torch Size N C H W gradients = torch randn_like out_cpu out_cpu backward gradients Compare against unvectorized CPU fallback NOTE grid_sample CPU fallback grid_sample uses AVX d images requires -bit indexing -bit floats So we also have fallback used only float tensors requiring -bit indexing That requires too much memory run CI so we also export fallback test here ensure feature parity vectorized version input_fallback = input_cpu float detach_ requires_grad_ grid_fallback = grid_cpu float detach_ requires_grad_ out_fallback = torch _grid_sampler_ d_cpu_fallback input_fallback grid_fallback F GRID_SAMPLE_INTERPOLATION_MODES mode F GRID_SAMPLE_PADDING_MODES padding_mode align_corners assertEqual out_fallback out_cpu float atol= e- rtol= e- out_fallback backward gradients float input_requires_grad assertEqual input_fallback grad input_cpu grad float atol= e- rtol= e- assertEqual grid_fallback grad grid_cpu grad float atol= e- rtol= e- TEST_CUDA input_cuda = input_cpu detach transpose cuda transpose requires_grad_ input_requires_grad grid_cuda = get_grid cuda grid_cpu detach requires_grad_ out_cuda = F grid_sample input_cuda grid_cuda mode=mode padding_mode=padding_mode align_corners=align_corners assertEqual out_cpu out_cuda out_cuda backward gradients cuda input_requires_grad assertEqual input_cpu grad input_cuda grad assertEqual grid_cpu grad grid_cuda grad atol= e- rtol= check zero-dimensional input strides don t error out base_input = torch randn N C IW input_cpu = base_input expand_as input_cuda requires_grad_ input_requires_grad out_cpu = F grid_sample input_cpu grid_cpu mode=mode padding_mode=padding_mode align_corners=align_corners input_cuda = base_input cuda expand_as input_cuda requires_grad_ input_requires_grad out_cuda = F grid_sample input_cuda grid_cuda mode=mode padding_mode=padding_mode align_corners=align_corners assertEqual out_cpu out_cuda test same size output test_shape N C H W H W mode padding_mode align_corners test larger output N = random randint C = random randint IH = random randint IW = random randint H = random randint IH + W = random randint IW + test_shape N C IH IW H W mode padding_mode align_corners test smaller output N = random randint C = random randint IH = random randint IW = random randint H = random randint IH W = random randint IW test_shape N C IH IW H W mode padding_mode align_corners test x inpput N = random randint C = random randint IH = IW = H = random randint W = random randint test_shape N C IH IW H W mode padding_mode align_corners testing empty grid N = random randint C = random randint IH = random randint IW = random randint W = random randint IW + test_shape N C IH IW W mode padding_mode align_corners testing empty channel N = random randint IH = random randint IW = random randint H = random randint IH + W = random randint IW + test_shape N IH IW H W mode padding_mode align_corners testing empty batch C = random randint IH = random randint IW = random randint H = random randint IH + W = random randint IW + test_shape C IH IW H W mode padding_mode align_corners mode bilinear nearest bicubic padding_mode zeros border reflection align_corners True False test known input CPU input = torch arange view grid = torch tensor - - - - e- - - - - e- view mode == bilinear padding_mode == zeros align_corners groundtruth = torch tensor view groundtruth = torch tensor view padding_mode == border align_corners groundtruth = torch tensor view groundtruth = torch tensor view padding_mode == reflection align_corners groundtruth = torch tensor view groundtruth = torch tensor view raise AssertionError f missing groundtruth test padding mode padding_mode mode == nearest padding_mode == zeros align_corners groundtruth = torch tensor view groundtruth = torch tensor view padding_mode == border align_corners groundtruth = torch tensor view groundtruth = torch tensor view padding_mode == reflection align_corners groundtruth = torch tensor view groundtruth = torch tensor view raise AssertionError f missing groundtruth test padding mode padding_mode mode == bicubic padding_mode == zeros align_corners groundtruth = torch tensor - view groundtruth = torch tensor - view padding_mode == border align_corners groundtruth = torch tensor view groundtruth = torch tensor view padding_mode == reflection align_corners groundtruth = torch tensor view groundtruth = torch tensor view raise AssertionError f missing groundtruth test padding mode padding_mode raise AssertionError f missing groundtruth test interpolation mode mode output = F grid_sample input grid mode=mode padding_mode=padding_mode align_corners=align_corners assertEqual output groundtruth atol= e- rtol= msg=f groundtruth comparison failed mode= mode f padding_mode= padding_mode See NOTE grid_sample CPU fallback output = torch _grid_sampler_ d_cpu_fallback input float grid float F GRID_SAMPLE_INTERPOLATION_MODES mode F GRID_SAMPLE_PADDING_MODES padding_mode align_corners assertEqual output groundtruth float atol= e- rtol= explicit check gradient edge cases input = torch arange expand grid = torch tensor - - - - - - - - view requires_grad_ mode == bilinear padding_mode == zeros align_corners groundtruth = torch tensor - - - view groundtruth = torch tensor - - - - - - view padding_mode == border align_corners groundtruth = torch tensor - - - view groundtruth = torch tensor - - - - - - view padding_mode == reflection align_corners groundtruth = torch tensor - - - view groundtruth = torch tensor - - - - - - view raise AssertionError f missing gradient groundtruth test padding mode padding_mode mode == nearest groundtruth = torch tensor - - - - - - view mode == bicubic padding_mode == zeros align_corners groundtruth = torch tensor - - - - - view groundtruth = torch tensor - - - - - - - - - view padding_mode == border align_corners groundtruth = torch tensor view groundtruth = torch tensor - - - - view padding_mode == reflection align_corners groundtruth = torch tensor view groundtruth = torch tensor view raise AssertionError f missing gradient groundtruth test padding mode padding_mode raise AssertionError f missing gradient groundtruth test interpolation mode mode input_requires_grad False True input = input requires_grad_ input_requires_grad F grid_sample input grid mode=mode padding_mode=padding_mode align_corners=align_corners sum backward assertEqual grid grad groundtruth atol= e- rtol= msg=f gradient groundtruth comparison failed mode= mode f padding_mode= padding_mode input_requires_grad= input_requires_grad grid grad zero_ See NOTE grid_sample CPU fallback torch _grid_sampler_ d_cpu_fallback input float grid float F GRID_SAMPLE_INTERPOLATION_MODES mode F GRID_SAMPLE_PADDING_MODES padding_mode align_corners sum backward assertEqual grid grad groundtruth atol= e- rtol= do gradcheck N = random randint C = random randint H = random randint W = random randint input = torch randn N C H W requires_grad=True grid = torch randn N H W requires_grad=True input_requires_grad False True input requires_grad_ input_requires_grad assertTrue gradcheck lambda inp grd F grid_sample inp grd mode=mode padding_mode=padding_mode align_corners=align_corners input grid test N C H W mode padding_mode align_corners input_requires_grad TEST_CUDNN cudnn flags enabled=False test N C H W mode padding_mode align_corners input_requires_grad set_default_dtype torch double test_grid_sample_ d Backward pass native C++ CUDA kernels branch depending whether input requires gradient so we test both cases test N C D H W mode padding_mode align_corners input_requires_grad test_shape N C ID IH IW D H W mode padding_mode align_corners input_cpu = torch randn C N ID IH IW transpose requires_grad_ input_requires_grad grid_cpu = torch randn D N H W transpose requires_grad_ out_cpu = F grid_sample input_cpu grid_cpu mode=mode padding_mode=padding_mode align_corners=align_corners assertTrue out_cpu size == torch Size N C D H W gradients = torch randn_like out_cpu out_cpu backward gradients TEST_CUDA input_cuda = input_cpu detach transpose cuda transpose requires_grad_ input_requires_grad grid_cuda = grid_cpu detach transpose cuda transpose requires_grad_ out_cuda = F grid_sample input_cuda grid_cuda mode=mode padding_mode=padding_mode align_corners=align_corners assertEqual out_cpu out_cuda out_cuda backward gradients cuda input_requires_grad assertEqual input_cpu grad input_cuda grad assertEqual grid_cpu grad grid_cuda grad atol= e- rtol= check zero-dimensional input strides don t error out base_input = torch randn N C IH IW input_cpu = base_input expand_as input_cuda requires_grad_ input_requires_grad grid_cpu = torch randn N D H W requires_grad=True out_cpu = F grid_sample input_cpu grid_cpu mode=mode padding_mode=padding_mode align_corners=align_corners input_cuda = base_input cuda expand_as input_cuda requires_grad_ input_requires_grad grid_cuda = grid_cpu detach cuda requires_grad_ out_cuda = F grid_sample input_cuda grid_cuda mode=mode padding_mode=padding_mode align_corners=align_corners assertEqual out_cpu out_cuda test same size output test_shape N C D H W D H W mode padding_mode align_corners test larger output N = random randint C = random randint ID = random randint IH = random randint IW = random randint D = random randint ID + H = random randint IH + W = random randint IW + test_shape N C ID IH IW D H W mode padding_mode align_corners test smaller output N = random randint C = random randint ID = random randint IH = random randint IW = random randint D = random randint ID H = random randint IH W = random randint IW test_shape N C ID IH IW D H W mode padding_mode align_corners test x inpput N = random randint C = random randint ID = IH = IW = H = random randint W = random randint test_shape N C ID IH IW D H W mode padding_mode align_corners testing empty grid N = random randint C = random randint ID = random randint IH = random randint IW = random randint D = random randint ID + W = random randint IW + test_shape N C ID IH IW D W mode padding_mode align_corners testing empty channel N = random randint ID = random randint IH = random randint IW = random randint D = random randint ID + H = random randint IH + W = random randint IW + test_shape N ID IH IW D H W mode padding_mode align_corners testing empty batch C = random randint ID = random randint IH = random randint IW = random randint D = random randint ID + H = random randint IH + W = random randint IW + test_shape C ID IH IW D H W mode padding_mode align_corners mode bilinear nearest padding_mode zeros border reflection align_corners True False do gradcheck N = random randint C = random randint D = random randint H = random randint W = random randint input = torch randn N C D H W requires_grad=True grid = torch randn N D H W requires_grad=True assertTrue gradcheck lambda inp grid F grid_sample inp grid mode=mode padding_mode=padding_mode align_corners=align_corners input grid input = input requires_grad_ False assertTrue gradcheck lambda grid F grid_sample input grid mode=mode padding_mode=padding_mode align_corners=align_corners grid input_requires_grad False True test N C D H W mode padding_mode align_corners input_requires_grad test_grid_sample_nearest_neighbor_rounding_mode_consistency device_list = cpu TEST_CUDA device_list append cuda normalize_indices indices_unnormalized torch Tensor dim_size int align_corners bool align_corners indices_normalized = indices_unnormalized dim_size - - indices_normalized = indices_unnormalized + dim_size - indices_normalized test_dim_size = non_test_dim_size = step_size = batch_size = channel_size = mode = nearest device device_list padding_mode zeros border reflection align_corners True False Unnormalized inquiry indices inquiry_indices_unnormalized = torch arange test_dim_size - + step_size step_size dtype=torch float device=device Note even though we trying create normalized indices which results x x indices after unnormalization because numerical error rounding direction might always expected designed The best we could do ensure rounding behaviors across different implementations different dimensions exactly same inquiry_indices = normalize_indices indices_unnormalized=inquiry_indices_unnormalized dim_size=test_dim_size align_corners=align_corners num_inqueries = inquiry_indices shape inquiry_fixed_indices = torch full num_inqueries dtype=torch float device=device array_data = torch rand test_dim_size dtype=torch float device=device D grid sample x-dim interpolation The input_tensor_ d_x shape batch_size channel_size non_test_dim_size test_dim_size input_tensor_ d_x = array_data reshape test_dim_size repeat batch_size channel_size non_test_dim_size The grid_tensor_ d_x shape batch_size num_inqueries grid_tensor_ d_x = torch cat tensors= inquiry_indices reshape num_inqueries inquiry_fixed_indices reshape num_inqueries dim= repeat batch_size The output_tensor_ d_x shape batch_size channel_size num_inqueries output_tensor_ d_x = F grid_sample input=input_tensor_ d_x grid=grid_tensor_ d_x mode=mode padding_mode=padding_mode align_corners=align_corners D grid sample y-dim interpolation The input_tensor_ d_y shape batch_size channel_size test_dim_size non_test_dim_size input_tensor_ d_y = torch transpose input_tensor_ d_x The grid_tensor_ d_y shape batch_size num_inqueries grid_tensor_ d_y = torch index_select grid_tensor_ d_x - torch tensor dtype=torch int device=device The output_tensor_ d_y shape batch_size channel_size num_inqueries output_tensor_ d_y = F grid_sample input=input_tensor_ d_y grid=grid_tensor_ d_y mode=mode padding_mode=padding_mode align_corners=align_corners assertEqual output_tensor_ d_x output_tensor_ d_y atol= rtol= D grid sample x-dim interpolation The input_tensor_ d_x shape batch_size channel_size non_test_dim_size non_test_dim_size test_dim_size input_tensor_ d_x = array_data reshape test_dim_size repeat batch_size channel_size non_test_dim_size non_test_dim_size The grid_tensor_ d_x shape batch_size num_inqueries grid_tensor_ d_x = torch cat tensors= inquiry_indices reshape num_inqueries inquiry_fixed_indices reshape num_inqueries inquiry_fixed_indices reshape num_inqueries dim= repeat batch_size The output_tensor_ d_x shape batch_size channel_size num_inqueries output_tensor_ d_x = F grid_sample input=input_tensor_ d_x grid=grid_tensor_ d_x mode=mode padding_mode=padding_mode align_corners=align_corners assertEqual output_tensor_ d_x output_tensor_ d_x atol= rtol= D grid sample y-dim interpolation The input_tensor_ d_y shape batch_size channel_size non_test_dim_size test_dim_size non_test_dim_size input_tensor_ d_y = torch transpose input_tensor_ d_x The grid_tensor_ d_y shape batch_size num_inqueries grid_tensor_ d_y = torch index_select grid_tensor_ d_x - torch tensor dtype=torch int device=device The output_tensor_ d_y shape batch_size channel_size num_inqueries output_tensor_ d_y = F grid_sample input=input_tensor_ d_y grid=grid_tensor_ d_y mode=mode padding_mode=padding_mode align_corners=align_corners assertEqual output_tensor_ d_x output_tensor_ d_y atol= rtol= D grid sample z-dim interpolation The input_tensor_ d_z shape batch_size channel_size non_test_dim_size non_test_dim_size test_dim_size input_tensor_ d_z = torch transpose input_tensor_ d_x The grid_tensor_ d_z shape batch_size num_inqueries grid_tensor_ d_z = torch index_select grid_tensor_ d_x - torch tensor dtype=torch int device=device The output_tensor_ d_z shape batch_size channel_size num_inqueries output_tensor_ d_z = F grid_sample input=input_tensor_ d_z grid=grid_tensor_ d_z mode=mode padding_mode=padding_mode align_corners=align_corners assertEqual output_tensor_ d_x output_tensor_ d_z atol= rtol= set_default_dtype torch double test_affine_grid test known input CPU input = torch arange view output = F affine_grid input torch Size align_corners=True groundtruth = torch tensor - view assertEqual output groundtruth output = F affine_grid input torch Size align_corners=False groundtruth = torch tensor view assertEqual output groundtruth align_corners True False do gradcheck N = random randint C = random randint H = random randint W = random randint sz = torch Size N C H W inp = torch randn N requires_grad=True warnings catch_warnings record=True warnings simplefilter always python requires so other tests can trigger assertTrue gradcheck lambda inp F affine_grid inp sz align_corners=align_corners inp check_forward_ad=True test CPU against CUDA TEST_CUDA N = random randint C = random randint H = random randint W = random randint sz = torch Size N C H W align_corners True False input_cpu = torch randn N requires_grad=True warnings catch_warnings record=True warnings simplefilter always python requires so other tests can trigger out_cpu = F affine_grid input_cpu sz align_corners=align_corners gradients = torch randn out_cpu size out_cpu backward gradients input_gpu = input_cpu detach cuda requires_grad_ warnings catch_warnings record=True warnings simplefilter always python requires so other tests can trigger out_cuda = F affine_grid input_gpu sz align_corners=align_corners out_cuda backward gradients cuda assertEqual out_cpu out_cuda assertEqual input_cpu grad input_gpu grad set_default_dtype torch double test_affine_grid_ d test known input CPU input = torch arange view output = F affine_grid input torch Size align_corners=True groundtruth = torch tensor - - - view assertEqual output groundtruth output = F affine_grid input torch Size align_corners=False groundtruth = torch tensor - - view assertEqual output groundtruth align_corners True False do gradcheck N = random randint C = random randint D = random randint H = random randint W = random randint sz = torch Size N C D H W inp = torch randn N requires_grad=True warnings catch_warnings record=True warnings simplefilter always python requires so other tests can trigger assertTrue gradcheck lambda inp F affine_grid inp sz align_corners=align_corners inp check_forward_ad=True test CPU against CUDA TEST_CUDA N = random randint C = random randint D = random randint H = random randint W = random randint sz = torch Size N C D H W align_corners True False input_cpu = torch randn N requires_grad=True warnings catch_warnings record=True warnings simplefilter always python requires so other tests can trigger out_cpu = F affine_grid input_cpu sz align_corners=align_corners gradients = torch randn out_cpu size out_cpu backward gradients input_gpu = input_cpu detach cuda requires_grad_ warnings catch_warnings record=True warnings simplefilter always python requires so other tests can trigger out_cuda = F affine_grid input_gpu sz align_corners=align_corners out_cuda backward gradients cuda assertEqual out_cpu out_cuda assertEqual input_cpu grad input_gpu grad test_channel_shuffle_return_alias_of_self gh- nn ChannelShuffle will alias empty input tensor groups = input_tensor = torch rand output = torch nn ChannelShuffle groups input_tensor torch testing assert_close output input_tensor test_channel_shuffle_input_checks input_tensor = torch rand assertRaisesRegex RuntimeError Number groups divide channels must positive groups = torch native_channel_shuffle input_tensor groups assertRaisesRegex RuntimeError Number channels must divisible groups groups = torch native_channel_shuffle input_tensor groups assertRaisesRegex RuntimeError channel_shuffle expects input dims input_tensor = torch rand groups = torch native_channel_shuffle input_tensor groups skipIfTorchDynamo TorchDynamo fails here unknown reasons test_native_channel_shuffle_return_alias_of_self groups = input_tensor = torch rand output = torch native_channel_shuffle input_tensor groups torch testing assert_close output input_tensor set_default_dtype torch double test_upsamplingLinear d align_corners True False recompute_scale_factor True False kwargs = dict mode= linear align_corners=align_corners recompute_scale_factor=recompute_scale_factor test float scale factor up downsampling scale_factor m = nn Upsample scale_factor=scale_factor kwargs in_t = torch ones out_size = int math floor in_t shape - scale_factor warnings catch_warnings record=True w out_t = m in_t assertEqual torch ones out_size out_t data input = torch randn requires_grad=True recompute_scale_factor gradcheck lambda x F interpolate x out_size kwargs input gradcheck lambda x F interpolate x scale_factor=scale_factor kwargs input test_upsamplingLinear d_spatial_invariance m = nn Upsample scale_factor= mode= linear align_corners=False in_t_ = torch zeros in_t_ normal_ warnings catch_warnings record=True w out_t_ = m in_t_ out_t_ = m in_t_ assertEqual out_t_ out_t_ set_default_dtype torch double test_upsampling_not_recompute_scale_factor test output against known input result must match opencv in_t = torch arange view expected_out_t = torch tensor - - IS_PPC Both OpenCV PyTorch give slightly different result PPC expected_out_t = torch tensor - - out_t = F interpolate in_t scale_factor= mode= bicubic align_corners=False recompute_scale_factor=False torch set_printoptions precision= assertEqual out_t expected_out_t atol= e- rtol= device_list = cpu TEST_CUDA device_list append cuda align_corners True False kwargs = dict mode= bicubic align_corners=align_corners test float scale factor up downsampling device device_list scale_factor in_t = torch ones device out_t = F interpolate in_t scale_factor=scale_factor kwargs out_size = int math floor in_t shape - scale_factor assertEqual torch ones out_size out_size out_t data atol= e- rtol= input = torch randn requires_grad=True gradcheck lambda x F interpolate x out_size kwargs input test_upsamplingBilinear d_spatial_invariance m = nn Upsample scale_factor= mode= bilinear align_corners=False in_t_ = torch zeros in_t_ normal_ warnings catch_warnings record=True w out_t_ = m in_t_ out_t_ = m in_t_ assertEqual out_t_ out_t_ test_upsamplingTrilinear d_spatial_invariance m = nn Upsample scale_factor= mode= trilinear align_corners=False in_t_ = torch zeros in_t_ normal_ warnings catch_warnings record=True w out_t_ = m in_t_ out_t_ = m in_t_ assertEqual out_t_ out_t_ test_upsampling_small_scale m = torch nn Upsample scale_factor= mode= bilinear in_t = torch arange dtype=torch get_default_dtype reshape out_t = m in_t expected_out_t = torch tensor assertEqual expected_out_t out_t test_upsampling_bfloat dtype=torch bfloat helper size scale_factor mode device memory_format=torch contiguous_format input = torch randn size device=device dtype=dtype memory_format=memory_format detach requires_grad_ True inputf = input torch float memory_format=torch contiguous_format detach requires_grad_ True m = nn Upsample scale_factor=scale_factor mode=mode outf = m inputf out = m input assertEqual out torch float outf atol= rtol= ginput = torch randn out shape device=device dtype=dtype memory_format=memory_format ginputf = ginput torch float memory_format=torch contiguous_format out backward ginput outf backward ginputf assertEqual input grad torch float inputf grad atol= rtol= device cpu helper nearest device helper nearest device torch channels_last helper nearest device helper linear device helper bilinear device helper bilinear device torch channels_last helper bicubic device helper bicubic device torch channels_last helper trilinear device helper nearest device helper nearest device helper nearest device helper nearest device torch channels_last helper nearest device helper nearest device torch channels_last helper nearest device torch channels_last_ d helper linear device helper linear device helper bilinear device helper bilinear device torch channels_last helper bicubic device helper bicubic device torch channels_last helper bicubic device helper trilinear device helper trilinear device torch channels_last_ d unittest skipIf TEST_CUDA CUDA unavailable test_interpolate_illegal_memory_access in_s = out_s = input = torch ones in_s device= cuda requires_grad=True note we allocated grad_output larger so out bound access would visible grad_input grad = torch ones out_s device= cuda requires_grad=True grad = grad out_s input_ref = input detach cpu requires_grad_ grad_ref = grad cpu out = F interpolate input size= out_s mode= nearest out backward grad out_ref = F interpolate input_ref size= out_s mode= nearest out_ref backward grad_ref assertEqual out_ref out assertEqual input_ref grad input grad test_interpolate_undefined_behavior_casting x = torch ones assertRaises RuntimeError lambda F interpolate x scale_factor=- e mode= bilinear assertRaises RuntimeError lambda F interpolate x scale_factor= e mode= bilinear test_interpolate_buffer_overflow Test buffer overflow issue due inaccurate floating point representation integer values See issue below details https github com pytorch pytorch issues helper size dtype mode device is_channels_last input = torch ones size dtype=dtype device=device is_channels_last len size == input = input transpose contiguous transpose len size == input = input memory_format=torch channels_last input = input memory_format=torch channels_last_ d output = F interpolate input mode=mode align_corners=True reset corner value expect output changed well output won t changed buffer overflow input - len size = output = F interpolate input mode=mode align_corners=True assertNotEqual output output size_dtype_list = We set size larger than floating point exactly representable range float exact representable range - size_dtype_list append + torch float size_dtype_list append + torch float size_dtype_list append + torch float bfloat exact representable range - size_dtype_list append + torch bfloat size_dtype_list append + torch bfloat size_dtype_list append + torch bfloat half exact representable range - size_dtype_list append + torch half size_dtype_list append + torch half size_dtype_list append + torch half TODO turn cuda test after buffer overflow issue fixed cuda kernel devices = cpu + cuda torch cuda is_available devices = cpu mode linear bilinear bicubic trilinear size_dtype size_dtype_list size dtype = size_dtype mode == linear len size = mode == bilinear len size = mode == bicubic len size = mode == trilinear len size = continue device devices device == cpu dtype == torch half device == cuda dtype == torch bfloat no half precision support cpu bfloat cuda yet continue is_channels_last True False helper size dtype mode device is_channels_last set_default_dtype torch double test_interpolate _test_interpolate_non_integer_size_warning in_t out_size dim kwargs test_sizes = float out_size torch tensor out_size dtype=torch float size test_sizes assertRaisesRegex TypeError expected size one int F interpolate in_t size= size dim kwargs _test_interpolate_helper in_t scale_factor layer out_size = int math floor in_t shape - scale_factor dim = len in_t shape - out_shape = + out_size dim warnings catch_warnings record=True w out_t = layer in_t assertEqual torch ones out_shape out_t assertEqual F interpolate in_t out_size dim kwargs F interpolate in_t scale_factor=scale_factor kwargs gradcheck lambda x F interpolate x out_size kwargs in_t nondet_tol=GRADCHECK_NONDET_TOL gradgradcheck lambda x F interpolate x out_size kwargs in_t nondet_tol=GRADCHECK_NONDET_TOL _test_interpolate_non_integer_size_warning in_t out_size dim kwargs _make_input dim device size = size += dim torch ones size requires_grad=True device=device device_list = cpu TEST_CUDA device_list append cuda device device_list scale_factor mode nearest area kwargs = dict mode=mode m = nn Upsample scale_factor=scale_factor kwargs device input _make_input device _make_input device _make_input device _test_interpolate_helper input scale_factor m align_corners True False kwargs = dict mode= linear align_corners=align_corners m = nn Upsample scale_factor=scale_factor kwargs device _test_interpolate_helper _make_input device scale_factor m kwargs = dict mode= bilinear align_corners=align_corners m = nn Upsample scale_factor=scale_factor kwargs device _test_interpolate_helper _make_input device scale_factor m kwargs = dict mode= bicubic align_corners=align_corners m t F interpolate t scale_factor=scale_factor kwargs device _test_interpolate_helper _make_input device scale_factor m kwargs = dict mode= trilinear align_corners=align_corners m = nn Upsample scale_factor=scale_factor kwargs device _test_interpolate_helper _make_input device scale_factor m test_linear_broadcasting m = nn Linear inp = torch randn expected = m inp view view assertEqual expected m inp test_linear_raise_on_scalar_input This used cause int underflow issue when reshaping input see https github com pytorch pytorch issues m = nn Linear inp = torch ones squeeze assertRaisesRegex RuntimeError both arguments D m inp tf _on_and_off parametrize_test device cpu + cuda TEST_CUDA parametrize_test bias subtest False name= nobias subtest True name= bias parametrize_test weight_layout subtest torch strided name= weightStrided subtest torch sparse_coo name= weightCOO subtest torch sparse_csr name= weightCSR subtest torch sparse_csc name= weightCSC TODO addmm computation CPU implemented Strided + Strided SparseBsr subtest torch sparse_bsr name= weightBSR subtest torch sparse_bsc name= weightBSC test_linear_autograd device bias weight_layout module = nn Linear bias=bias device=device weight_layout == torch strided pass weight_layout == torch sparse_csr module weight = nn Parameter module weight to_sparse_csr weight_layout == torch sparse_csc module weight = nn Parameter module weight to_sparse_csc weight_layout == torch sparse_bsr module weight = nn Parameter module weight to_sparse_bsr weight_layout == torch sparse_bsc module weight = nn Parameter module weight to_sparse_bsc weight_layout == torch sparse_coo module weight = nn Parameter module weight to_sparse_coo raise AssertionError inp = torch randn requires_grad=True device=device res = module inp bias expected = torch einsum i ji- j inp module weight to_dense + module bias expected = torch einsum i ji- j inp module weight to_dense assertEqual res expected grad_output = torch randn device=device grads = torch autograd grad res module weight inp grad_output grads_expected = torch autograd grad expected module weight inp grad_output assertEqual grads_expected layout weight_layout g ge zip grads grads_expected assertEqual g ge test_bilinear module = nn Bilinear input = torch randn requires_grad=True input = torch randn requires_grad=True grad_output = torch randn res = module input input expected = torch einsum bi kij bj- bk input module weight input + module bias assertEqual res expected grads = torch autograd grad res module weight module bias input input grad_output grads_expected = torch autograd grad expected module weight module bias input input grad_output g ge zip grads grads_expected assertEqual g ge test_bilinear_non_contiguous module = nn Bilinear input = torch randn requires_grad=True input = torch randn requires_grad=True input _tp = input transpose input _tp = input transpose grad_output = torch randn run input _tp input _tp input grad = input grad = None output = module input _tp input _tp output backward grad_output output data input grad data input grad data out_nc g _nc g _nc = run input _tp input _tp input _tp = input _tp contiguous input _tp = input _tp contiguous out g g = run input _tp input _tp assertEqual out out_nc assertEqual g g _nc assertEqual g g _nc test_bilinear_no_bias module = nn Bilinear dtype=torch double module_no_bias = nn Bilinear False dtype=torch double module bias data zero_ module weight data copy_ module_no_bias weight input = torch randn requires_grad=True dtype=torch double input = torch randn requires_grad=True dtype=torch double grad_output = torch randn dtype=torch double run net input grad = input grad = None output = net input input output backward grad_output output data input grad data input grad data out g g = run module out_nb g _nb g _nb = run module_no_bias assertEqual out out_nb assertEqual g g _nb assertEqual g g _nb _assertGradAndGradgradChecks lambda x x F bilinear x x module_no_bias weight module_no_bias bias input input test_bilinear_broadcasting m = nn Bilinear input = torch randn input = torch randn expected = m input view input view view assertEqual expected m input input test_bilinear_value_error assertRaisesRegex ValueError _features must nn Bilinear test_fold_invalid_arg input size divisible \prod kernel_size fold = nn Fold output_size= kernel_size= assertRaisesRegex RuntimeError r divisible product kernel_size fold torch randn assertRaisesRegex RuntimeError r divisible product kernel_size fold torch randn input size matching total number sliding blocks assertRaisesRegex RuntimeError r match calculated number sliding blocks fold = nn Fold output_size= kernel_size= fold torch randn assertRaisesRegex RuntimeError r match calculated number sliding blocks fold = nn Fold output_size= kernel_size= stride= fold torch randn assertRaisesRegex RuntimeError r match calculated number sliding blocks fold = nn Fold output_size= kernel_size= stride= dilation= padding= fold torch randn should = sliding blocks fold = nn Fold output_size= kernel_size= stride= dilation= padding= assertRaisesRegex RuntimeError r calculated shape array sliding blocks fold torch randn test_unfold_invalid_arg input wrong dimension unfold = nn Unfold kernel_size= calculated output shape too small assertRaisesRegex RuntimeError r its components must least one unfold = nn Unfold kernel_size= unfold torch randn assertRaisesRegex RuntimeError r its components must least one unfold = nn Unfold kernel_size= padding= unfold torch randn assertRaisesRegex RuntimeError r its components must least one unfold = nn Unfold kernel_size= padding= dilation= unfold torch randn assertRaisesRegex RuntimeError r product kernel_width kernel_height overflowed tensor_data = torch tensor e- - e- - e- - e- e- e+ e- - e- e- - e- - e- - e- - e- - e- - e- - e- - e- e+ e+ e+ e+ F fold tensor_data test_softmin x = torch randn assertEqual F softmin x F softmax -x assertEqual F softmin x F softmax -x test_adaptive_log_softmax args validation assertRaises ValueError _ = nn AdaptiveLogSoftmaxWithLoss div_value= assertRaises ValueError _ = nn AdaptiveLogSoftmaxWithLoss div_value= assertRaises ValueError _ = nn AdaptiveLogSoftmaxWithLoss div_value= assertRaisesRegex ValueError cutoffs should sequence unique _ = nn AdaptiveLogSoftmaxWithLoss div_value= raise _ = nn AdaptiveLogSoftmaxWithLoss div_value= input shapes assertRaisesRegex RuntimeError r Input target should have same size asfm = nn AdaptiveLogSoftmaxWithLoss div_value= x = torch randn y = torch tensor asfm x y out-of-bound targets assertRaisesRegex RuntimeError r Target values should asfm = nn AdaptiveLogSoftmaxWithLoss div_value= x = torch randn y = torch tensor asfm x y cluster sizes asfm = nn AdaptiveLogSoftmaxWithLoss div_value= x = torch randn y = torch tensor assertEqual asfm head weight size + targets head clusters dimensionality assertEqual asfm tail weight size targets cluster dimensionality assertEqual asfm tail weight size assertEqual asfm tail weight size assertEqual asfm x y output size test no_batch_dim support asfm = nn AdaptiveLogSoftmaxWithLoss div_value= x = torch randn y = torch tensor x = x squeeze y = y squeeze assertEqual asfm x y output squeeze asfm x y output log_probs actually returns log_proba asfm = nn AdaptiveLogSoftmaxWithLoss div_value= x = torch randn logprob_out = asfm log_prob x assertEqual torch exp logprob_out data sum torch ones forward returns same thing log_probs v y = torch full v dtype=torch long out loss = asfm x y assertEqual out logprob_out gather y unsqueeze squeeze assertEqual loss F nll_loss logprob_out y predict x = torch randn abs_ argmax shortlist asfm = nn AdaptiveLogSoftmaxWithLoss div_value= head_bias=True asfm head weight data abs_ asfm head bias data abs_ asfm head weight data asfm shortlist_size zero_ out = asfm predict x assertEqual out asfm log_prob x argmax dim= argmax outside shortlist asfm = nn AdaptiveLogSoftmaxWithLoss div_value= head_bias=True asfm head weight data abs_ asfm head bias data abs_ asfm head weight data asfm shortlist_size zero_ out = asfm predict x assertEqual out asfm log_prob x argmax dim= half argmax shortlist half clusters asfm = nn AdaptiveLogSoftmaxWithLoss div_value= head_bias=True asfm head weight data abs_ asfm head bias data abs_ x asfm shortlist_size zero_ x asfm shortlist_size zero_ asfm head weight data asfm shortlist_size asfm shortlist_size zero_ asfm head weight data asfm shortlist_size asfm shortlist_size zero_ out = asfm predict x assertEqual out asfm log_prob x argmax dim= test_cross_entropy_loss dtype=torch bfloat loss_cpu = nn CrossEntropyLoss cpu inputf = torch randn device= cpu dtype=torch float requires_grad=True input = inputf dtype detach requires_grad_ True target = torch empty dtype=torch long random_ outf = loss_cpu inputf target out = loss_cpu input target assertEqual out outf dtype=dtype atol= e- rtol= outf backward out backward assertEqual input grad inputf grad dtype=dtype atol= e- rtol= test_cross_entropy_loss_precision Regression test loss_cpu = nn CrossEntropyLoss cpu inputf = torch randn device= cpu dtype=torch float inputd = inputf double target = torch randint dtype=torch long outf = loss_cpu inputf target outd = loss_cpu inputd target assertEqual outf outd exact_dtype=False test_cross_entropy_loss_zero_div Test issue input_ = torch rand dtype=torch float input_ = torch rand dtype=torch float torch nn CrossEntropyLoss input_ input_ unittest skipIf torch cuda is_available CUDA available test_convert_sync_batchnorm module = torch nn Sequential torch nn BatchNorm d torch nn InstanceNorm d cuda necessary have anchor point comparison case convert_sync_batchnorm updates place comp_module = torch nn Sequential torch nn BatchNorm d torch nn InstanceNorm d cuda comp_module load_state_dict module state_dict sync_bn_module = torch nn SyncBatchNorm convert_sync_batchnorm module children = list sync_bn_module children assertEqual children __class__ torch nn SyncBatchNorm assertEqual children __class__ torch nn InstanceNorm d layer converted_layer zip comp_module children sync_bn_module children key layer state_dict keys assertEqual layer state_dict key device converted_layer state_dict key device assertEqual layer state_dict key converted_layer state_dict key unittest skipIf TEST_CUDA CUDA available test_sync_batchnorm_backward_elemt device = cuda saved_input = torch rand device=device grad_output = torch rand device=device mean = torch rand device=device invstd = torch rand device=device weight = torch rand device=device sum_dy = torch rand device=device sum_dy_xmu = torch rand device=device count_tensor = torch tensor dtype=torch int device=device gI_contiguous = torch batch_norm_backward_elemt grad_output saved_input mean invstd weight sum_dy sum_dy_xmu count_tensor Test batch_norm_backward_element gives same answer all combinations contiguous channels_last input b torch channels_last torch contiguous_format torch contiguous_format torch channels_last torch channels_last torch channels_last gI_actual = torch batch_norm_backward_elemt grad_output contiguous memory_format=a saved_input contiguous memory_format=b mean invstd weight sum_dy sum_dy_xmu count_tensor assertEqual gI_actual gI_contiguous unittest skipIf TEST_CUDA CUDA available test_sync_batchnorm_accuracy_cuda The target test test functionality accuracy those single-GPU cuda kernels used SyncBatchNorm They fwd torch batch_norm_stats torch batch_norm_gather_stats_with_counts torch batch_norm_elemt bwd torch batch_norm_backward_reduce torch batch_norm_backward_elemt _batch_norm_stats data memory_format mean_axes mean _ = torch batch_norm_stats data e- mean _ = torch batch_norm_stats data memory_format=memory_format e- mean_ref = torch mean data mean_axes keepdim=False assertEqual mean_ref mean assertEqual mean_ref mean _batch_norm_stats torch randn dtype=torch float device= cuda torch channels_last _batch_norm_stats torch randn dtype=torch float device= cuda torch channels_last_ d test_flatten tensor_input = torch randn Flatten Tensor flatten = nn Flatten start_dim= end_dim=- tensor_output = flatten tensor_input assertEqual tensor_output size torch Size test_unflatten tensor_input = torch randn Unflatten Tensor unflattened_size tuple ints list ints us unflatten = nn Unflatten dim= unflattened_size=us tensor_output = unflatten tensor_input assertEqual tensor_output size torch Size Unflatten NamedTensor unflatten = nn Unflatten dim= features unflattened_size= C H W named_tensor_input = tensor_input refine_names N features named_tensor_output = unflatten named_tensor_input assertEqual named_tensor_output size torch Size test_unflatten_invalid_arg Wrong type unflattened_size tuple floats assertRaisesRegex TypeError r unflattened_size must tuple ints found element type float pos nn Unflatten dim= unflattened_size= Wrong type unflattened_size list lists list tuples us C W H C W H assertRaisesRegex TypeError r unflattened_size must tuple tuples found type list nn Unflatten dim= features unflattened_size=us Wrong type unflattened_size tuple lists assertRaisesRegex TypeError r unflattened_size must tuple tuples found element type list pos nn Unflatten dim= features unflattened_size= C W H Wrong type unflattened_size tuple dicts assertRaisesRegex TypeError r unflattened_size must tuple tuples found element type dict pos nn Unflatten dim= features unflattened_size= C W H test_layer_norm_grads_with_create_graph_flag atol = e- rtol = e- x = torch randn requires_grad=True layer_norm = nn LayerNorm e- True torch no_grad layer_norm weight = torch nn Parameter torch ones_like layer_norm weight grads = torch autograd grad layer_norm x sum x create_graph=False grads = torch autograd grad layer_norm x sum x create_graph=True assertEqual grads grads rtol=rtol atol=atol TEST_CUDA x = x cuda layer_norm = layer_norm cuda grads = torch autograd grad layer_norm x sum x create_graph=False grads = torch autograd grad layer_norm x sum x create_graph=True assertEqual grads grads rtol=rtol atol=atol test_layer_norm_eps test https github com pytorch pytorch issues x = torch Tensor ln = torch nn LayerNorm eps= e- elementwise_affine=False assertEqual ln forward x torch zeros_like x unittest skipIf TEST_CUDA CUDA available test_layer_norm_backwards_eps dtype = torch float m_x_n_list = boolean = True False combinations = itertools product boolean repeat= elementwise_affine bias combinations m n m_x_n_list x = torch randn m n dtype=dtype requires_grad=True grad_output = torch rand_like x x_cuda = x clone detach cuda requires_grad_ grad_output_cuda = grad_output clone detach cuda ln = nn LayerNorm n dtype=dtype elementwise_affine=elementwise_affine bias=bias ln_cuda = nn LayerNorm n device= cuda dtype=dtype elementwise_affine=elementwise_affine bias=bias ln_out = ln x ln_out_cuda = ln_cuda x_cuda ln_out backward grad_output ln_out_cuda backward grad_output_cuda atol = e- rtol = e- m atol = e- rtol = e- elementwise_affine assertEqual ln weight grad ln_cuda weight grad f weight grad failed m= n= rtol=rtol atol=atol bias elementwise_affine assertEqual ln bias grad ln_cuda bias grad f bias grad failed m= n= rtol=rtol atol=atol unittest skipIf TEST_CUDA CUDA available largeTensorTest GB device= cuda test_layer_norm_large_tensor test https github com pytorch pytorch issues device = torch device cuda b n dp = pairwise_repr = torch randn b n n dp attn_bias_norm = nn LayerNorm dp device=device pairwise_repr = pairwise_repr dtype=torch float device=device we want smaller copy compare results pairwise_small = pairwise_repr - - - detach clone norm = attn_bias_norm pairwise_repr norm_small = attn_bias_norm pairwise_small assertEqual norm shape torch Size Check output make sure correct torch testing assert_close norm_small norm - - - test_padding_list Padding can list tuple regression test gh- x = torch randn net = torch nn ConvTranspose d kernel_size= padding= y = net x net = torch nn ConvTranspose d kernel_size= padding= y = net x test_fractional_max_pool d_invalid_output_ratio arg_ = arg_ = arg_class = torch nn FractionalMaxPool d kernel_size=arg_ output_ratio=arg_ arg_ _ _tensor = torch rand dtype=torch float arg_ _ = arg_ _ _tensor clone arg_ = arg_ _ assertRaisesRegex ValueError fractional_max_pool d requires output_ratio either single Int tuple Ints res = arg_class arg_ unittest skipIf TEST_CUDA CUDA available largeTensorTest GB device= cuda test_large_max_pool d_ch_last https github com pytorch pytorch issues N C H W = dims extend int device = torch device cuda x_cuda = torch randn N C H W device=device dtype=torch float x_cuda = x_cuda memory_format=torch channels_last pool = nn MaxPool d kernel_size= stride= padding= y_cuda_ch_last = pool x_cuda y_cuda_contig = pool x_cuda contiguous assertEqual y_cuda_ch_last y_cuda_contig test_max_pool d_invalid_output_size arg_ = arg_ = arg_ = False arg_class = torch nn MaxPool d kernel_size=arg_ stride=arg_ return_indices=arg_ arg_ _ = torch as_tensor arg_ = arg_ _ assertRaises RuntimeError res = arg_class arg_ test_pickle_module_no_weights_only_warning warnings catch_warnings record=True w pickle loads pickle dumps torch nn Linear assertEqual len w TestFusionEval TestCase set_default_dtype torch double given X=hu tensor shapes= dtype=np double running_mean=hu tensor shapes= dtype=np double running_var=hu tensor shapes= dtype=np double test_fuse_module_eval_numerics X running_mean running_var inputs _ = X iC oC = inputs shape len running_mean inputs = torch from_numpy inputs kernel_size = conv_ref = torch nn Conv d iC oC bias=True kernel_size=kernel_size bn_ref = torch nn BatchNorm d oC bn_ref running_mean = torch from_numpy running_mean bn_ref running_var = torch from_numpy running_var conv_ref eval bn_ref eval Y_ref = bn_ref conv_ref inputs conv_bn_fused = torch nn utils fusion fuse_conv_bn_eval conv_ref bn_ref Y_hat = conv_bn_fused inputs assertEqual Y_ref Y_hat msg= Conv+BN fusion results off na_bn_ref = torch nn BatchNorm d oC affine=False na_bn_ref running_mean = torch from_numpy running_mean na_bn_ref running_var = torch from_numpy running_var na_bn_ref eval Y_ref = na_bn_ref conv_ref inputs conv_na_bn_fused = torch nn utils fusion fuse_conv_bn_eval conv_ref na_bn_ref Y_hat = conv_na_bn_fused inputs assertEqual Y_ref Y_hat msg= Conv+BN non-affine fusion results off TestConstantPadNd TestCase test_constant_pad_nd = torch tensor res = torch constant_pad_nd expected = torch tensor assertEqual res expected test_preserves_memory_format nchw_tensor = torch rand nchw_padded = torch constant_pad_nd nchw_tensor assertTrue nchw_padded is_contiguous memory_format=torch contiguous_format nhwc_tensor = nchw_tensor contiguous memory_format=torch channels_last nhwc_padded = torch constant_pad_nd nhwc_tensor assertTrue nhwc_padded is_contiguous memory_format=torch channels_last TestAddRelu TestCase test_add_relu = torch rand b = torch rand = float b = b float = - = + add_res = + b relu_res = torch relu add_res add_relu_res = torch _VF _add_relu b assertEqual add_relu_res relu_res test_add_relu_broadcasting = torch rand b = b_scalar = torch ones res = torch _VF _add_relu b broadcasted_res = torch _VF _add_relu b_scalar assertEqual broadcasted_res res add_test test decorator=None add test_name fn hasattr TestNN test_name raise RuntimeError Found two tests same name + test_name decorator None fn = decorator fn setattr TestNN test_name fn test_name = test get_name hasattr test test_cpu test test_cpu add test_name lambda test=test test cuda_test_name = test_name + _cuda With dtype enable s good enough test against three floating types kwargs = extra_args get_function_arglist test test_cuda kwargs extra_args = test extra_args dtype get_function_arglist test test_cuda torch cuda is_tf _supported test with_tf with_tf _off test=test kwargs=kwargs tf _off test test_cuda dtype=torch float kwargs add cuda_test_name + _fp with_tf _off with_tf _on test=test kwargs=kwargs tf _on test tf _precision test test_cuda dtype=torch float kwargs add cuda_test_name + _tf with_tf _on add cuda_test_name + _float lambda test=test kwargs=kwargs test test_cuda dtype=torch float kwargs add cuda_test_name + _double lambda test=test kwargs=kwargs test test_cuda dtype=torch double kwargs test_half test=test kwargs=kwargs test test_cuda dtype=torch half kwargs getattr test check_half True add cuda_test_name + _half test_half test_bfloat test=test kwargs=kwargs test test_cuda dtype=torch bfloat kwargs getattr test check_bfloat True add cuda_test_name + _bfloat test_bfloat test_cfloat test=test kwargs=kwargs test test_cuda dtype=torch cfloat kwargs test_cdouble test=test kwargs=kwargs test test_cuda dtype=torch cdouble kwargs getattr test check_complex False add cuda_test_name + _cfloat test_cfloat add cuda_test_name + _cdouble test_cdouble with_tf _off test=test kwargs=kwargs tf _off test test_cuda kwargs torch cuda is_tf _supported test with_tf add cuda_test_name + _fp with_tf _off with_tf _on test=test kwargs=kwargs tf _on test tf _precision test test_cuda kwargs add cuda_test_name + _tf with_tf _on add cuda_test_name with_tf _off test_params module_tests + get_new_module_tests TODO CUDA implemented yet constructor test_params name = test_params pop module_name test_params constructor = getattr nn name decorator = test_params pop decorator None test = NewModuleTest test_params add_test test decorator check_eval test_params create new test identical sets module training False desc = test_params get desc None test_params desc = eval desc None desc + _eval gen_eval_constructor constructor eval_constructor args kwargs cons = constructor args kwargs cons training = False cons eval_constructor __name__ = constructor __name__ eval_constructor test_params constructor = gen_eval_constructor test_params constructor test = NewModuleTest test_params add_test test decorator check_with_long_tensor test_params fullname = test_params get fullname None fullname test_params fullname = fullname + _with_long_tensor desc = test_params get desc None test_params desc = with_long_tensor desc None desc + _with_long_tensor double_equivalent_of_long_tensor size torch randint - size=size double apply_to_cons t t is_floating_point isinstance t Parameter Parameter double_equivalent_of_long_tensor t size isinstance t torch Tensor double_equivalent_of_long_tensor t size t gen_long_tensor_constructor constructor long_tensor_constructor args kwargs cons = constructor args kwargs cons _apply apply_to_cons cons long_tensor_constructor __name__ = constructor __name__ long_tensor_constructor gen_long_tensor_input input_size input_func double_equivalent_of_long_tensor input_size input_func reference_fn i p m For bad reasons would create LongTensors requires gradients Remove requires_grad avoid p m parameters p requires_grad_ False m _apply lambda t t long input = i long out = m forward input out test_params constructor = gen_long_tensor_constructor test_params constructor test_params input_fn = gen_long_tensor_input test_params input_size test_params reference_fn = reference_fn test_params check_forward_only = True Currently we don t support conv d conv d LongTensor CUDA test_params test_cuda = False test = NewModuleTest test_params add_test test decorator test_params criterion_tests constructor test_params name = test_params pop module_name test_params constructor = getattr nn name test = CriterionTest test_params decorator = test_params pop decorator None add_test test decorator check_sum_reduction test_params desc = test_params get desc None test_params desc = sum_reduction desc None desc + _sum_reduction gen_sum_reduction_constructor constructor sum_reduction_constructor args kwargs cons = constructor args reduction= sum kwargs cons sum_reduction_constructor __name__ = constructor __name__ sum_reduction_constructor test_params constructor = gen_sum_reduction_constructor test_params constructor test = CriterionTest test_params add_test test decorator UnpoolingNet nn Module __init__ pool unpool super __init__ pool = pool unpool = unpool forward input unpool pool input add_test NewModuleTest constructor=lambda UnpoolingNet nn MaxPool d return_indices=True nn MaxUnpool d input_size= fullname= MaxUnpool d_net default_dtype=torch double add_test NewModuleTest constructor=lambda UnpoolingNet nn MaxPool d return_indices=True nn MaxUnpool d input_size= fullname= MaxUnpool d_net default_dtype=torch double add_test NewModuleTest constructor=lambda UnpoolingNet nn MaxPool d return_indices=True nn MaxUnpool d input_size= fullname= MaxUnpool d_net check_gradgrad=False default_dtype=torch double add_test NewModuleTest constructor=lambda UnpoolingNet nn MaxPool d return_indices=True nn MaxUnpool d input_size= reference_fn=single_batch_reference_fn fullname= MaxUnpool d_net_no_batch_dim default_dtype=torch double add_test NewModuleTest constructor=lambda UnpoolingNet nn MaxPool d return_indices=True nn MaxUnpool d input_size= reference_fn=single_batch_reference_fn fullname= MaxUnpool d_net_no_batch_dim default_dtype=torch double add_test NewModuleTest constructor=lambda UnpoolingNet nn MaxPool d return_indices=True nn MaxUnpool d input_size= reference_fn=single_batch_reference_fn fullname= MaxUnpool d_net_no_batch_dim check_gradgrad=False default_dtype=torch double _AdaptiveLogSoftmaxWithLoss nn AdaptiveLogSoftmaxWithLoss __call__ input t = torch tensor input device nn AdaptiveLogSoftmaxWithLoss __call__ input t output add_test NewModuleTest constructor=lambda _AdaptiveLogSoftmaxWithLoss input_size= fullname= AdaptiveLogSoftmax with_tf =True tf _precision= default_dtype=torch double The following helpers TestNN test_affine_ torch cuda is_available device_ cpu cuda device_ cpu angle_rad_ r math pi r random random axis_vector_ t = random random random random random random l = sum x x t tuple x l x t input_size d_ output_size d_ input_size dsq_ output_size dsq_ input_size d_ input_size dsq_ output_size dsq_ output_size d_ _buildEquivalentAffineTransforms d device input_size output_size angle_rad input_center = x - x input_size output_center = x - x output_size s = math sin angle_rad c = math cos angle_rad intrans_ary = np array input_center input_center dtype=np float inscale_ary = np array input_center input_center dtype=np float rotation_ary = np array c -s s c dtype=np float outscale_ary = np array output_center output_center dtype=np float outtrans_ary = np array -output_center -output_center dtype=np float reorder_ary = np array dtype=np float transform_ary = np dot np dot np dot np dot intrans_ary inscale_ary rotation_ary T outscale_ary outtrans_ary grid_ary = np dot np dot np dot reorder_ary rotation_ary T outscale_ary outtrans_ary transform_tensor = torch from_numpy rotation_ary device torch float transform_tensor = transform_tensor unsqueeze transform_tensor transform_ary grid_ary _buildEquivalentAffineTransforms d device input_size output_size angle_rad axis_vector input_center = x - x input_size output_center = x - x output_size s = math sin angle_rad c = math cos angle_rad c = - c intrans_ary = np array input_center input_center input_center dtype=np float inscale_ary = np array input_center input_center input_center dtype=np float l m n = axis_vector scipyRotation_ary = np array l l c + c m l c - n s n l c + m s l m c + n s m m c + c n m c - l s l n c - m s m n c + l s n n c + c dtype=np float z y x = axis_vector torchRotation_ary = np array x x c + c y x c - z s z x c + y s x y c + z s y y c + c z y c - x s x z c - y s y z c + x s z z c + c dtype=np float outscale_ary = np array output_center output_center output_center dtype=np float outtrans_ary = np array -output_center -output_center -output_center dtype=np float reorder_ary = np array dtype=np float transform_ary = np dot np dot np dot np dot intrans_ary inscale_ary np linalg inv scipyRotation_ary outscale_ary outtrans_ary grid_ary = np dot np dot np dot reorder_ary np linalg inv scipyRotation_ary outscale_ary outtrans_ary transform_tensor = torch from_numpy torchRotation_ary device torch float transform_tensor = transform_tensor unsqueeze transform_tensor transform_ary grid_ary end TestNN test_affine_ helpers TestNNDeviceType NNTestCase _test_InstanceNorm_general cls input device dtype=torch float default case track_running_stats=False b c = input size input size input_var = input device=device dtype=dtype requires_grad_ IN = cls c eps= device dtype output = IN input_var out_reshaped = output view b c - mean = out_reshaped mean var = out_reshaped var unbiased=False assertEqual torch abs mean data mean atol= e- rtol= assertEqual torch abs var data mean atol= e- rtol= check eval mode doesn t change behavior grad_out = torch randn_like output res = output data clone output backward grad_out grad = input_var grad data clone IN eval output = IN input_var input_var grad = None output backward grad_out res = output data grad = input_var grad data assertEqual res res assertEqual grad grad If track_running_stats=True momentum= running_mean var should equal mean var input unbias correction IN = cls c momentum= eps= track_running_stats=True device dtype output = IN input_var input_reshaped = input_var transpose reshape c - mean = input_reshaped mean input_reshaped = input_var transpose reshape c b - var = input_reshaped var unbiased=True assertEqual torch abs mean data - IN running_mean mean atol= e- rtol= assertEqual torch abs var data mean - IN running_var mean atol= e- rtol= eval mode adding X std channel input should make corresponding channel output have mean X IN eval delta = IN running_var sqrt torch arange c device=device dtype=dtype delta = delta view - _ range input dim output = IN input_var + delta assertEqual output transpose reshape c - mean torch arange c dtype=dtype _test_InstanceNorm_cuda_half cls input device THNN input = input device=device dtype=torch half random_ requires_grad_ True m = cls input size affine=True track_running_stats=True device torch half thnn_output = m input thnn_output sum backward thnn_input_grad = input grad data clone assertEqualTypeString thnn_output input cuDNN TEST_CUDNN input grad = None m = m float cudnn_output = m input cudnn_output sum backward cudnn_input_grad = input grad data clone assertEqualTypeString cudnn_output input assertEqual cudnn_output thnn_output atol= e- rtol= assertEqual cudnn_input_grad thnn_input_grad atol= e- rtol= _test_LayerNorm_general device dtype=torch float i range shape = torch randint i dtype=torch long tolist x = torch empty shape device=device dtype=dtype uniform_ normalized_ndim = random randint i - inclusive normalized_shape = shape -normalized_ndim unnormalized_shape = shape -normalized_ndim test LN normalizes mean stddev ln = nn LayerNorm normalized_shape eps= device dtype ln weight data fill_ ln bias data fill_ output = ln x out_reshaped = output view unnormalized_shape + - mean = out_reshaped mean - var = out_reshaped var - unbiased=False delta = e- dtype == torch bfloat dtype == torch half e- assertEqual torch abs mean data mean atol=delta rtol= assertEqual torch abs var data mean atol=delta rtol= test LN applies weight bias correctly scale bias = torch empty uniform_ tolist ln weight data fill_ scale ln bias data fill_ bias output = ln x out_reshaped = output view unnormalized_shape + - mean = out_reshaped mean - var = out_reshaped var - unbiased=False assertEqual torch abs mean data mean bias atol=delta rtol= assertEqual torch abs var data mean scale atol=delta rtol= bad_norm_shape_input_shape = norm_shape input_shape bad_norm_shape_input_shape items ln = nn LayerNorm norm_shape input = torch empty input_shape device=device dtype=dtype uniform_ assertRaises RuntimeError lambda ln input _test_LayerNorm_cuda_half device input = torch empty device=device dtype=torch half random_ requires_grad_ True m = nn LayerNorm device torch half output = m input output sum backward assertEqualTypeString output input _test_LayerNorm_cpu_mixed_dtype device dtype elementwise_affine True False layer norm input shape normalized m x n cpu vectorized n so make sure n exceeds vector length input = torch empty device=device dtype=dtype random_ m = nn LayerNorm elementwise_affine=elementwise_affine device dtype fp m_fp = deepcopy m device torch float x_fp = input detach clone float requires_grad_ out_fp = m_fp x_fp out_fp sum backward bf half m_bf = deepcopy m x_bf = input detach clone requires_grad_ out_bf = m_bf x_bf out_bf sum backward bf half mixed type m_mix = deepcopy m device torch float x_mix = input detach clone requires_grad_ out_mix = m_mix x_mix out_mix sum backward assertEqual out_fp dtype=dtype out_bf assertEqual out_fp dtype=dtype out_mix assertEqual x_fp grad dtype=dtype x_bf grad atol= e- rtol= e- assertEqual x_fp grad dtype=dtype x_mix grad atol= e- rtol= e- _test_GroupNorm_general device dtype=torch float good_shape_g = shape_g grad product good_shape_g items True False shape g = shape_g x = torch empty shape device=device dtype=dtype uniform_ x requires_grad_ grad b = shape c = shape test GN normalizes mean stddev gn = nn GroupNorm g c eps= device dtype gn weight data fill_ gn bias data fill_ output = gn x out_reshaped = output view b g - mean = out_reshaped mean - var = out_reshaped var - unbiased=False assertEqual torch abs mean mean atol= e- rtol= assertEqual torch abs var mean atol= e- rtol= output backward torch randn_like output output is_cuda torch cuda synchronize test GN applies weight bias correctly scale = torch empty c device=device dtype=dtype uniform_ bias = torch empty c device=device dtype=dtype uniform_ gn weight data copy_ scale gn bias data copy_ bias output = gn x out_reshaped = output view b c - out_normed = out_reshaped - bias view c scale view c out_normed_reshaped = out_normed view b g - mean = out_normed_reshaped mean - var = out_normed_reshaped var - unbiased=False assertEqual torch abs mean mean atol= e- rtol= assertEqual torch abs var mean atol= e- rtol= bad_shape_g = shape g bad_shape_g items assertRaises ValueError gn = nn GroupNorm g shape _test_GroupNorm_cuda_half input = torch zeros requires_grad=True cuda half random_ m = nn GroupNorm cuda torch half output = m input output sum backward assertEqualTypeString output input _test_GroupNorm_cpu_mixed_dtype helper size groups memory_format dtype channels = size input = torch randn size cpu dtype=dtype input_bf = input contiguous memory_format=memory_format detach requires_grad_ True input_bf = input_bf detach clone requires_grad_ True input_f = input_bf float detach requires_grad_ True m_bf = nn GroupNorm groups channels cpu dtype=dtype m_f = deepcopy m_bf float m_f = deepcopy m_f bfloat input bfloat parameters out = m_bf input_bf bfloat input float parameters out = m_f input_bf float input float parameters out = m_f input_f assertEqual out out atol= e- rtol= e- assertEqual out float out atol= e- rtol= e- grad_out = torch randn out shape cpu dtype=dtype grad_out_bf = grad_out contiguous memory_format=memory_format detach requires_grad_ True grad_out_bf = grad_out_bf detach clone requires_grad_ True grad_out_f = grad_out_bf clone float detach requires_grad_ True bfloat half input grad float parameters out backward grad_out_bf retain_graph=True float input grad float parameters out backward grad_out_f retain_graph=True bfloat half input grad bfloat half parameters out backward grad_out_bf retain_graph=True Need higher tolerances atol= e- rtol= e- macos assertEqual m_f weight grad m_f weight grad atol= e- rtol= e- assertEqual m_f bias grad m_f bias grad atol= e- rtol= e- assertEqual input_bf grad float input_f grad atol= e- rtol= e- Full bf half has lower precision compared mixed bf half fp Use Amp keep module parameters acc dtype i e float better numerical stability atol = None rtol = None dtype == torch bfloat atol = e- rtol = e- assert dtype == torch half atol = e- rtol = e- assertEqual m_bf weight grad m_f weight grad dtype=dtype atol=atol rtol=rtol assertEqual m_bf bias grad m_f bias grad dtype=dtype atol=atol rtol=rtol assertEqual input_bf grad input_bf grad atol=atol rtol=rtol cl_formats = torch channels_last torch channels_last_ d dtype torch bfloat torch half shape g is_cl False True format = cl_formats len shape is_cl torch contiguous_format helper shape g format dtype _test_module_empty_inputs module inputs _inp inputs _inp requires_grad_ True out = module inputs gO = torch rand_like out out backward gO p module parameters p requires_grad assertEqual p grad torch zeros_like p grad _inp inputs assertEqual _inp grad torch zeros_like _inp unittest skipIf TEST_NUMPY TEST_SCIPY scipy __version__ Scipy v numpy found expectedFailureMPS Unsupported Border padding mode https github com pytorch pytorch issues tf _on_and_off reduced_f _on_and_off test_affine_ d_rotate device scipy before do support homogeneous coordinate scipy ndimage affine_transform so we need skip input_size = input_ary = np array np random random input_size dtype=np float output_size = angle_rad = transform_tensor transform_ary offset = \ _buildEquivalentAffineTransforms d device input_size output_size angle_rad scipy_ary = torch from_numpy scipy ndimage affine_transform input_ary transform_ary offset=offset output_shape=output_size order= mode= nearest prefilter=False affine_tensor = torch nn functional affine_grid transform_tensor torch Size output_size align_corners=True gridsample_ary = torch nn functional grid_sample torch tensor input_ary device=device device affine_tensor padding_mode= border align_corners=True cpu assertEqual scipy_ary mean gridsample_ary mean assertEqual scipy_ary gridsample_ary reshape_as scipy_ary unittest skipIf TEST_NUMPY TEST_SCIPY scipy __version__ Scipy v numpy found skipIfRocmArch MI _ARCH expectedFailureMPS Unsupported Border padding mode https github com pytorch pytorch issues tf _on_and_off reduced_f _on_and_off test_affine_ d_rotate device scipy before do support homogeneous coordinate scipy ndimage affine_transform so we need skip input_size dsq output_size dsq \ itertools product input_size dsq_ output_size dsq_ input_size = input_size dsq input_ary = np array np random random input_size dtype=np float output_size = output_size dsq angle_rad = math pi transform_tensor transform_ary offset = \ _buildEquivalentAffineTransforms d device input_size output_size angle_rad scipy_ary = torch from_numpy scipy ndimage affine_transform input_ary transform_ary offset=offset output_shape=output_size order= mode= nearest prefilter=True input_size dsq == output_size dsq assertEqual scipy_ary mean input_ary mean assertEqual scipy_ary input_ary - assertEqual scipy_ary - input_ary - - assertEqual scipy_ary - - input_ary - assertEqual scipy_ary - input_ary affine_tensor = torch nn functional affine_grid transform_tensor torch Size output_size align_corners=True gridsample_ary = torch nn functional grid_sample torch tensor input_ary device=device device affine_tensor padding_mode= border align_corners=True cpu assertEqual scipy_ary mean gridsample_ary mean assertEqual scipy_ary gridsample_ary reshape_as scipy_ary unittest skipIf TEST_NUMPY TEST_SCIPY scipy __version__ Scipy v numpy found expectedFailureMPS Unsupported Border padding mode https github com pytorch pytorch issues tf _on_and_off reduced_f _on_and_off test_affine_ d_rotate device scipy before do support homogeneous coordinate scipy ndimage affine_transform so we need skip input_size = input_ary = np array np zeros input_size dtype=np float input_ary = input_ary = output_size = angle_rad = math pi transform_tensor transform_ary offset = \ _buildEquivalentAffineTransforms d device input_size output_size angle_rad scipy_ary = torch from_numpy scipy ndimage affine_transform input_ary transform_ary offset=offset output_shape=output_size order= mode= nearest prefilter=False affine_tensor = torch nn functional affine_grid transform_tensor torch Size output_size align_corners=True gridsample_ary = torch nn functional grid_sample torch tensor input_ary device=device device affine_tensor padding_mode= border align_corners=True cpu assertEqual scipy_ary gridsample_ary reshape_as scipy_ary onlyCUDA largeTensorTest GB cpu largeTensorTest GB cuda test_avg_pool_large_tensor device test https github com pytorch pytorch issues = torch randn dtype=torch half device=device requires_grad=True a_cpu = detach cpu float m = torch nn AvgPool d o = m a_cpu requires_grad = True o sum backward o_cpu = m a_cpu o_cpu sum backward workaround memory usage overhead assertEqual assertTrue torch allclose grad cpu a_cpu grad half onlyCUDA largeTensorTest GB device= cuda test_large_max_pool d_ch_last device https github com pytorch pytorch issues N C H W = dims extend int x_cuda = torch randn N C H W device=device dtype=torch float x_cuda = x_cuda memory_format=torch channels_last pool = nn MaxPool d kernel_size= stride= padding= y_cuda_ch_last = pool x_cuda y_cuda_contig = pool x_cuda contiguous assertEqual y_cuda_ch_last y_cuda_contig onlyCUDA test_large_reflect_pad device https github com pytorch pytorch issues x = torch rand device= cuda c = F pad x mode= reflect c_cpu = F pad x cpu mode= reflect assertEqual c c_cpu onlyCUDA largeTensorTest GB cpu largeTensorTest GB cuda test_avg_pool_large_tensor device test https github com pytorch pytorch issues out_size = size = inp = torch randn size device=device requires_grad=True dtype=torch float inp_cpu = inp detach cpu m = torch nn AvgPool d False True None o = m inp inp_cpu requires_grad = True o sum backward o_cpu = m inp_cpu o_cpu sum backward assertEqual o shape out_size assertEqual o_cpu shape out_size reduce memory usage assertEqual inp grad sum inp_cpu grad sum unittest skipIf TEST_NUMPY TEST_SCIPY scipy __version__ Scipy v numpy found skipIfRocmArch MI _ARCH expectedFailureMPS Unsupported Border padding mode https github com pytorch pytorch issues tf _on_and_off reduced_f _on_and_off test_affine_ d_rotateRandom device scipy before do support homogeneous coordinate scipy ndimage affine_transform so we need skip angle_rad input_size d output_size d \ itertools product angle_rad_ input_size d_ output_size d_ input_size = input_size d input_ary = np array np random random input_size dtype=np float round output_size = output_size d input_ary = input_ary - = input_ary - = input_ary - - = transform_tensor transform_ary grid_ary = \ _buildEquivalentAffineTransforms d device input_size output_size angle_rad scipy_ary = torch from_numpy scipy ndimage affine_transform input_ary transform_ary output_shape=output_size order= mode= nearest prefilter=False affine_tensor = torch nn functional affine_grid transform_tensor torch Size output_size align_corners=True gridsample_ary = torch nn functional grid_sample torch tensor input_ary device=device device affine_tensor padding_mode= border align_corners=True cpu affine_tensor = affine_tensor cpu r range affine_tensor size c range affine_tensor size grid_out = np dot grid_ary r c assertEqual affine_tensor r c grid_out exact_dtype=False assertEqual scipy_ary gridsample_ary reshape_as scipy_ary unittest skipIf TEST_NUMPY TEST_SCIPY scipy __version__ Scipy v numpy found skipIfRocmArch MI _ARCH tf _on_and_off reduced_f _on_and_off test_affine_ d_rotateRandom device scipy before do support homogeneous coordinate scipy ndimage affine_transform so we need skip angle_rad axis_vector input_size d output_size d \ itertools product angle_rad_ axis_vector_ input_size d_ output_size d_ input_size = input_size d input_ary = np array np random random input_size dtype=np float output_size = output_size d input_ary = input_ary - = input_ary - = input_ary - - = input_ary - = input_ary - - = input_ary - - = input_ary - - - = transform_tensor transform_ary grid_ary = \ _buildEquivalentAffineTransforms d device input_size output_size angle_rad axis_vector scipy_ary = torch from_numpy scipy ndimage affine_transform input_ary transform_ary output_shape=output_size order= mode= nearest prefilter=False affine_tensor = torch nn functional affine_grid transform_tensor torch Size output_size align_corners=True gridsample_ary = torch nn functional grid_sample torch tensor input_ary device=device device affine_tensor padding_mode= border align_corners=True cpu affine_tensor = affine_tensor cpu i range affine_tensor size r range affine_tensor size c range affine_tensor size grid_out = np dot grid_ary i r c assertEqual affine_tensor i r c grid_out exact_dtype=False assertEqual scipy_ary gridsample_ary reshape_as scipy_ary onlyCUDA dtypes torch float torch half test_batchnorm_large_batch device dtype bn = nn BatchNorm d device dtype data = torch rand device=device dtype=dtype out = bn data sum backward dtypesIfCUDA torch float torch double torch half torch complex dtypesIfMPS torch float torch half torch complex dtypes torch float torch double torch bfloat torch complex test_conv_empty_input device dtype help input conv memory_format ref_out = conv input conv_cl = conv memory_format=memory_format out_cl = conv_cl input assertEqual ref_out out_cl input_cl = input memory_format=memory_format out_cl = conv input_cl assertEqual out_cl out_cl out_cl = conv_cl input_cl assertEqual out_cl out_cl channels_last case input d = torch randn device=device dtype=dtype conv d = torch nn Conv d device=device dtype=dtype help input d conv d torch channels_last channels_last_ d case input d = torch randn device=device dtype=dtype conv d = torch nn Conv d device=device dtype=dtype help input d conv d torch channels_last_ d non-contiguous case weight = torch rand device=device dtype=dtype bias = torch rand device=device dtype=dtype out = F conv d input d weight bias weight = weight contiguous out_ref = F conv d input d weight bias assertEqual out_ref out sigfpe reported https github com pytorch pytorch issues assertRaises RuntimeError inp = torch empty dtype=dtype device=device weight = torch empty dtype=dtype device=device torch _C _nn slow_conv d inp weight assertRaisesRegex RuntimeError re escape D kernel_size expected torch _C _nn thnn_conv d torch rand kernel_size= padding= stride= weight=torch rand assertRaisesRegex RuntimeError re escape D stride expected torch _C _nn thnn_conv d torch rand kernel_size= padding= stride= weight=torch rand assertRaisesRegex RuntimeError re escape D padding expected torch _C _nn thnn_conv d torch rand kernel_size= padding= stride= weight=torch rand test_InstanceNorm d_general device b = random randint c = random randint d = random randint input = torch rand b c d _test_InstanceNorm_general nn InstanceNorm d input device device_type == cuda _test_InstanceNorm_cuda_half nn InstanceNorm d input device test_InstanceNorm d_general device b = random randint c = random randint w = random randint h = random randint input = torch rand b c h w _test_InstanceNorm_general nn InstanceNorm d input device device_type == cuda _test_InstanceNorm_cuda_half nn InstanceNorm d input device test_InstanceNorm d_general device b = random randint c = random randint w = random randint h = random randint d = random randint input = torch rand b c h w d _test_InstanceNorm_general nn InstanceNorm d input device device_type == cuda _test_InstanceNorm_cuda_half nn InstanceNorm d input device parametrize_test instance_norm_cls nn InstanceNorm d nn InstanceNorm d nn InstanceNorm d name_fn=lambda c c __name__ parametrize_test no_batch_dim True False parametrize_test affine True False test_instancenorm_raises_error_if_input_channels_is_not_num_features device instance_norm_cls no_batch_dim affine inst_norm = instance_norm_cls affine=affine size = inst_norm _get_no_batch_dim no_batch_dim size = + size t = torch randn size affine assertRaisesRegex ValueError expected input s size dim= inst_norm t warnings catch_warnings record=True w inst_norm t assertIn which used because affine=False str w message test_instancenorm_raises_error_if_less_than_one_value_per_channel device x = torch rand None None assertRaises ValueError torch nn InstanceNorm d x device test_instancenorm_raises_error_for_single_spatial_element_during_training device BATCH_SIZE = NUM_CHANNELS = norms = torch nn InstanceNorm d torch nn InstanceNorm d torch nn InstanceNorm d i norm enumerate norms m = norm NUM_CHANNELS track_running_stats=True m device Create appropriately-sized input single spatial element input = torch randn BATCH_SIZE NUM_CHANNELS _ range i + device=device assertRaises ValueError m input Single spatial element should fine eval m eval m input test_LayerNorm_general device _test_LayerNorm_general device device_type == cuda device_type == cpu dtype torch half torch bfloat _test_LayerNorm_general device dtype=dtype device_type == cuda _test_LayerNorm_cuda_half device device_type == cpu dtype torch half torch bfloat _test_LayerNorm_cpu_mixed_dtype device dtype=dtype onlyNativeDeviceTypes test_LayerNorm_numeric device layer_norm_ref X gamma beta normalized_shape eps feature_size = np prod normalized_shape X_view = X view - feature_size mean = X_view mean dim=- keepdim=True var = X_view var dim=- unbiased=False keepdim=True Y = X_view - mean torch sqrt var + eps Y = Y gamma view - + beta view - Y view X size normalized_shape = layer_norm = nn LayerNorm normalized_shape float device X = torch rand normalized_shape dtype=torch float device=device Y = layer_norm X Y_ref = layer_norm_ref X layer_norm weight data layer_norm bias data normalized_shape layer_norm eps assertEqual Y Y_ref rtol= atol= e- device_type == cuda layer_norm cpu Y_cpu = layer_norm X cpu assertEqual Y_cpu Y rtol= atol= e- onlyNativeDeviceTypes dtypes torch float torch bfloat test_rmsnorm_numeric device dtype rms_norm_reference_fn i normalized_shape weight eps=None eps None eps = torch finfo i dtype eps ndim = i ndim dims = ndim - i - i range len normalized_shape upcasted_i = i float result = upcasted_i torch rsqrt upcasted_i pow mean dim=dims keepdim=True + eps weight None result = weight result type_as i shape = X = torch rand shape dtype=dtype device=device w = torch rand shape dtype=dtype device=device Y = torch nn functional rms_norm X shape w Y_ref = rms_norm_reference_fn X shape w assertEqual Y_ref Y onlyNativeDeviceTypes dtypes torch float torch bfloat torch float torch float dtypesIfMPS torch float torch bfloat torch float test_rmsnorm_epsilon device dtype rms_norm_reference_fn i normalized_shape eps = torch finfo i dtype eps ndim = i ndim dims = ndim - i - i range len normalized_shape i dtype torch float upcasted_i = i float upcasted_i = i result = upcasted_i torch rsqrt upcasted_i pow mean dim=dims keepdim=True + eps result type_as i shape = X = torch tensor e- - e- e- - e- dtype=dtype device=device Y = torch nn functional rms_norm X shape Y_ref = rms_norm_reference_fn X shape assertEqual Y_ref Y onlyCPU test_glu_bfloat device test_dtype fn input dtype input = input detach clone dtype=dtype requires_grad_ True input = input detach clone float requires_grad_ True out = fn input out sum backward out = fn input out sum backward assertEqual out dtype dtype assertEqual input grad dtype dtype assertEqual out out exact_dtype=False assertEqual input grad input grad atol= e- rtol= exact_dtype=False func device torch nn GLU dim=- device shapes = shape shapes x = torch randn shape device=device test_dtype func device x torch bfloat onlyNativeDeviceTypes test_GroupNorm_general device _test_GroupNorm_general device device_type == cuda _test_GroupNorm_cuda_half device_type == cpu _test_GroupNorm_cpu_mixed_dtype test_GroupNorm_raises_error_if_one_value_per_group device x = torch rand None None assertRaises ValueError torch nn GroupNorm x device test_GroupNorm_empty device mod = torch nn GroupNorm device inp = torch randn device=device _test_module_empty_input mod inp device_type == cuda has_cudnn torch backends cudnn flags enabled=False _test_module_empty_input mod inp onlyCPU dtypes torch float torch double torch bfloat torch half test_groupnorm_nhwc device dtype helper size groups memory_format is_mixed channels = size input = torch randn size dtype=dtype device=device requires_grad=True input = input contiguous memory_format=memory_format input retain_grad grad = torch randn size dtype=dtype device=device grad = grad contiguous memory_format=memory_format dtype == torch bfloat is_mixed gn = nn GroupNorm groups channels device torch float gn = nn GroupNorm groups channels device dtype gn weight data uniform_ gn bias data uniform_ ref_input = input detach clone contiguous memory_format=torch contiguous_format requires_grad_ True ref_grad = grad detach clone contiguous memory_format=torch contiguous_format dtype == torch bfloat is_mixed ref_gn = nn GroupNorm groups channels device torch float ref_gn = nn GroupNorm groups channels device dtype ref_gn load_state_dict gn state_dict out = gn input out backward grad ref_out = ref_gn ref_input ref_out backward ref_grad assertTrue out is_contiguous memory_format=memory_format assertTrue ref_out is_contiguous memory_format=torch contiguous_format assertEqual out ref_out parameters bfloat Half recommended atol = e- rtol = e- assertEqual gn weight grad ref_gn weight grad atol=atol rtol=rtol assertEqual gn bias grad ref_gn bias grad atol=atol rtol=rtol assertEqual input grad ref_input grad atol=atol rtol=rtol is_mixed True False helper torch channels_last is_mixed helper torch channels_last is_mixed helper torch channels_last is_mixed helper torch channels_last is_mixed helper torch channels_last is_mixed helper torch channels_last is_mixed helper torch channels_last_ d is_mixed helper torch channels_last_ d is_mixed helper torch channels_last_ d is_mixed onlyNativeDeviceTypes test_GroupNorm_memory_format device Tests regression reported https github com pytorch pytorch issues helper input_format grad_format B= C= W= H= copy net_orig = torch nn GroupNorm B C device=device net = copy deepcopy net_orig x_orig = torch rand B C W H device=device requires_grad=True grad_orig = torch rand B C W H device=device x = x_orig detach clone memory_format=input_format requires_grad_ True grad = grad_orig detach memory_format=grad_format y = net x y backward grad y_orig = net_orig x_orig y_orig backward grad_orig assertEqual y y_orig assertEqual x grad x_orig grad input_format torch contiguous_format torch channels_last grad_format torch contiguous_format torch channels_last helper input_format grad_format onlyNativeDeviceTypes test_GroupNorm_numeric device group_norm_ref X gamma beta groups channels eps batch_size = X size X_view = X view batch_size groups - mean = X_view mean dim=- keepdim=True var = X_view var dim=- unbiased=False keepdim=True Y = X_view - mean torch sqrt var + eps view batch_size channels - Y = Y gamma view channels + beta view channels Y view X size batch_size = groups = channels = group_norm = nn GroupNorm groups channels float device X = torch rand batch_size channels dtype=torch float device=device Y = group_norm X Y_ref = group_norm_ref X group_norm weight data group_norm bias data groups channels group_norm eps assertEqual Y Y_ref rtol= atol= e- device_type == cuda group_norm cpu Y_cpu = group_norm X cpu assertEqual Y_cpu Y rtol= atol= e- expectedFailureMPS Double supported MPS onlyNativeDeviceTypes dtypes torch float torch complex test_pad device dtype Assert assertion errors raised invalid circular padding values inputs = torch randn device=device dtype=dtype requires_grad=True Should raise error when trying wrap around more than once assertRaises RuntimeError lambda F pad inputs mode= circular assertRaises RuntimeError lambda F pad inputs mode= circular Should raise error when negative padding results negative output shape assertRaises RuntimeError lambda F pad inputs - - mode= circular assert reflection padding errors when pad = input size expected_err_msg = r Padding size should less than corresponding input dimension inputs = torch randn device=device dtype=dtype assertRaisesRegex RuntimeError expected_err_msg lambda F pad inputs mode= reflect inputs = torch randn device=device dtype=dtype assertRaisesRegex RuntimeError expected_err_msg lambda F pad inputs mode= reflect inputs = torch rand device=device dtype=dtype assert pad doesn t view into input tensor mode constant reflect replicate circular out = F pad inputs mode=mode out fill_ assertTrue torch all torch abs inputs out = F pad inputs - - mode=mode out fill_ assertTrue torch all torch abs inputs expectedFailureMPS Unsupported float complex onlyNativeDeviceTypes dtypes torch float torch complex test_ReplicationPad_empty device dtype mod inp torch nn ReplicationPad d torch randn device=device dtype=dtype torch nn ReplicationPad d torch randn device=device dtype=dtype torch nn ReplicationPad d torch randn device=device dtype=dtype _test_module_empty_input mod inp check_size=False assertRaisesRegex RuntimeError Expected D D mod = torch nn ReplicationPad d inp = torch randn device=device dtype=dtype mod inp assertRaisesRegex RuntimeError Expected D D mod = torch nn ReplicationPad d inp = torch randn device=device dtype=dtype mod inp assertRaisesRegex RuntimeError Expected D D mod = torch nn ReplicationPad d inp = torch randn device=device dtype=dtype mod inp assertRaisesRegex RuntimeError padding size expected torch _C _nn replication_pad d torch randn padding= assertRaisesRegex RuntimeError padding size expected torch _C _nn replication_pad d torch randn padding= assertRaisesRegex RuntimeError padding size expected torch _C _nn replication_pad d torch randn padding= expectedFailureMPS Correctness issue https github com pytorch pytorch issues test_ReplicationPad d_large device shapes = pl pr = shape shapes x = torch randn shape device=device requires_grad=True model = torch nn ReplicationPad d pl pr forward out = model x assertEqual out pl -pr x left_padding = out pl assertEqual left_padding x expand_as left_padding right_padding = out -pr assertEqual right_padding x - expand_as right_padding backward g = torch randn_like out out backward g assertEqual x grad - g pl + -pr - assertEqual x grad g pl + sum - assertEqual x grad - g -pr - sum - expectedFailureMPS Correctness issue https github com pytorch pytorch issues test_ReplicationPad d_large device shapes = pl pr pt pb = shape shapes x = torch randn shape device=device requires_grad=True model = torch nn ReplicationPad d pl pr pt pb forward center edge out = model x assertEqual out pt -pb pl -pr x left_padding = out pt -pb pl assertEqual left_padding x expand_as left_padding right_padding = out pt -pb -pr assertEqual right_padding x - expand_as right_padding top_padding = out pt pl -pr assertEqual top_padding x expand_as top_padding bottom_padding = out -pb pl -pr assertEqual bottom_padding x - expand_as bottom_padding forward corner tl_padding = out pt + pl + assertEqual tl_padding x expand_as tl_padding tr_padding = out pt + -pr - assertEqual tr_padding x - expand_as tr_padding bl_padding = out -pb - pl + assertEqual bl_padding x - expand_as bl_padding br_padding = out -pb - -pr - assertEqual br_padding x - - expand_as br_padding backward center edge g = torch randn_like out out backward g assertEqual x grad - - g pt + -pb - pl + -pr - assertEqual x grad - g pt + -pb - pl + sum - assertEqual x grad - - g pt + -pb - -pr - sum - assertEqual x grad - g pt + pl + -pr - sum - assertEqual x grad - - g -pb - pl + -pr - sum - backward corner assertEqual x grad g pt + pl + sum - - assertEqual x grad - g pt + -pr - sum - - assertEqual x grad - g -pb - pl + sum - - assertEqual x grad - - g -pb - -pr - sum - - largeTensorTest GB test_ReplicationPad d_large device shapes = pl pr pt pbt pf pbk = shape shapes x = torch randn shape device=device requires_grad=True model = torch nn ReplicationPad d pl pr pt pbt pf pbk forward center out = model x assertEqual out pf -pbk pt -pbt pl -pr x backward center g = torch randn_like out out backward g assertEqual x grad - - - g pf + -pbk - pt + -pbt - pl + -pr - onlyNativeDeviceTypes test_Bilinear_empty device mod = torch nn Bilinear device inp = torch randn requires_grad=True device=device inp = torch randn requires_grad=True device=device output = mod inp inp output sum backward assertEqual inp torch zeros_like inp assertEqual inp torch zeros_like inp assertEqual inp grad torch zeros_like inp assertEqual inp grad torch zeros_like inp expectedFailureMPS Double supported expectedFailureMeta RuntimeError cannot reshape tensor elements into shape - onlyNativeDeviceTypes test_TransformerEncoderLayer_empty device training True False batch_first input_shape True False input = torch rand input_shape device=device dtype=torch double encoder_layer = nn TransformerEncoderLayer d_model= nhead= batch_first=batch_first dtype=torch double device training encoder_layer = encoder_layer eval torch no_grad _test_module_empty_input encoder_layer input check_size=False inference=True batch_first TEST_WITH_CROSSREF torch no_grad A NestedTensor no tensors inside doesn t have dim dim matter so can t hit fast path nor can we give result assertRaisesRegex AssertionError MultiheadAttention does support NestedTensor outside nt = torch nested nested_tensor device=device _test_module_empty_input encoder_layer nt check_size=False inference=True nt = torch nested nested_tensor torch rand device=device dtype=torch double device=device _test_module_empty_input encoder_layer nt check_size=False inference=True _test_module_empty_input encoder_layer input check_size=False expectedFailureMeta RuntimeError cannot reshape tensor elements into shape - expectedFailureMPS Float supported onlyNativeDeviceTypes test_TransformerEncoder_empty device batch_first input_shape True False input = torch rand input_shape device=device dtype=torch double encoder_layer = nn TransformerEncoderLayer d_model= nhead= batch_first=batch_first dtype=torch double device transformer_encoder = nn TransformerEncoder encoder_layer num_layers= device _test_module_empty_input transformer_encoder input check_size=False expectedFailureMeta RuntimeError cannot reshape tensor elements into shape - expectedFailureMPS Float supported onlyNativeDeviceTypes test_TransformerDecoderLayer_empty device batch_first memory_shape tgt_shape True False memory = torch rand memory_shape device=device dtype=torch double tgt = torch rand tgt_shape requires_grad=True device=device dtype=torch double decoder_layer = nn TransformerDecoderLayer d_model= nhead= batch_first=batch_first dtype=torch double device _test_module_empty_inputs decoder_layer tgt memory expectedFailureMeta RuntimeError cannot reshape tensor elements into shape - expectedFailureMPS Float supported onlyNativeDeviceTypes test_TransformerDecoder_empty device batch_first memory_shape tgt_shape True False memory = torch rand memory_shape device=device dtype=torch double tgt = torch rand tgt_shape requires_grad=True device=device dtype=torch double decoder_layer = nn TransformerDecoderLayer d_model= nhead= batch_first=batch_first dtype=torch double device transformer_decoder = nn TransformerDecoder decoder_layer num_layers= device _test_module_empty_inputs transformer_decoder tgt memory expectedFailureMeta RuntimeError cannot reshape tensor elements into shape - expectedFailureMPS Float supported onlyNativeDeviceTypes test_Transformer_empty device batch_first src_shape tgt_shape True transformer_model = nn Transformer nhead= num_encoder_layers= dtype=torch double device src = torch rand src_shape requires_grad=True device=device dtype=torch double tgt = torch rand tgt_shape requires_grad=True device=device dtype=torch double _test_module_empty_inputs transformer_model src tgt onlyNativeDeviceTypes dtypes torch float torch complex test_ReflectionPad_empty device dtype mod inp torch nn ReflectionPad d torch randn device=device dtype=dtype torch nn ReflectionPad d torch randn device=device dtype=dtype torch nn ReflectionPad d torch randn device=device dtype=dtype _test_module_empty_input mod inp check_size=False assertRaisesRegex RuntimeError D D mod = torch nn ReflectionPad d inp = torch randn device=device dtype=dtype mod inp assertRaisesRegex RuntimeError D D mod = torch nn ReflectionPad d inp = torch randn device=device dtype=dtype mod inp assertRaisesRegex RuntimeError D D mod = torch nn ReflectionPad d inp = torch randn device=device dtype=dtype mod inp onlyNativeDeviceTypes test_ReflectionPad_fails device assertRaisesRegex RuntimeError r Padding size supported D input tensor mod = torch nn ReflectionPad d inp = torch randn device=device mod inp assertRaisesRegex RuntimeError D D inp = torch randn device=device torch ops aten reflection_pad d inp assertRaisesRegex RuntimeError r Padding size supported D input tensor mod = torch nn ReflectionPad d inp = torch randn device=device mod inp assertRaisesRegex RuntimeError D D inp = torch randn device=device torch ops aten reflection_pad d inp assertRaisesRegex RuntimeError r Padding size supported D input tensor mod = torch nn ReflectionPad d inp = torch randn device=device mod inp assertRaisesRegex RuntimeError D D inp = torch randn device=device torch ops aten reflection_pad d inp onlyCUDA Test CPU GPU results match test_ReflectionPad d_large device shapes = pad = shape shapes x = torch randn shape device=device requires_grad=True ref_x = x detach cpu requires_grad_ out = F pad x pad mode= reflect ref_out = F pad ref_x pad mode= reflect assertEqual out ref_out g = torch randn_like out ref_g = g cpu out backward g ref_out backward ref_g assertEqual x grad ref_x grad onlyCUDA Test CPU GPU results match deterministic mode test_ReflectionPad d_large_deterministic device original_deterministic = torch are_deterministic_algorithms_enabled try torch use_deterministic_algorithms True shape = pad = x = torch randn shape device=device requires_grad=True ref_x = x detach cpu requires_grad_ out = F pad x pad mode= reflect ref_out = F pad ref_x pad mode= reflect assertEqual out ref_out g = torch randn_like out ref_g = g cpu out backward g ref_out backward ref_g assertEqual x grad ref_x grad finally avoid state leaking outside test torch use_deterministic_algorithms original_deterministic onlyNativeDeviceTypes test_LocalResponseNorm_empty device mod = torch nn LocalResponseNorm device inp = torch ones device=device _test_module_empty_input mod inp check_size=False onlyCUDA Test CPU GPU results match test_ReflectionPad d_large device shapes = pad = shape shapes x = torch randn shape device=device requires_grad=True ref_x = x detach cpu requires_grad_ out = F pad x pad mode= reflect ref_out = F pad ref_x pad mode= reflect assertEqual out ref_out g = torch randn_like out ref_g = g cpu out backward g ref_out backward ref_g assertEqual x grad ref_x grad expectedFailureMPS Unimplemented margin_loss onlyNativeDeviceTypes dtypes torch float torch double test_MarginLoss_empty device dtype mod x y torch nn MultiMarginLoss device torch randn requires_grad=True device=device dtype=dtype torch ones device=device type torch long torch nn MultiLabelMarginLoss device torch randn requires_grad=True device=device dtype=dtype torch ones device=device type torch long out = mod x y out sum backward assertEqual x torch zeros_like x assertEqual x grad torch zeros_like x assertRaisesRegex RuntimeError Expected x = torch randn requires_grad=True device=device dtype=dtype y = torch ones device=device type torch long mod x y assertRaisesRegex RuntimeError Expected x = torch randn requires_grad=True device=device dtype=dtype y = torch ones device=device type torch long mod x y onlyCUDA dtypes torch float torch double test_MarginLoss_race device dtype loss = torch nn MultiMarginLoss device batch = classes = x = torch randn batch classes requires_grad=True device=device dtype=dtype y = torch randint low= high=classes size= batch device=device dtype=torch long x_cpu = x detach clone cpu y_cpu = y detach clone cpu out = loss x y out backward x_cpu = x detach clone cpu x_cpu requires_grad = True y_cpu = y detach clone cpu out_cpu = loss cpu x_cpu y_cpu out_cpu backward assertEqual x_cpu grad x grad cpu onlyCUDA test_MarginLoss_warnings device model = torch nn Linear device=device loss = torch nn MultiMarginLoss x = torch rand device=device targets = torch randint device=device f = io StringIO contextlib redirect_stderr f out = model x l = loss out targets l backward assertTrue len f getvalue == onlyCUDA test_mse_loss_error device i = torch randn device=device t = torch randn assertRaisesRegex RuntimeError Expected all tensors same device F mse_loss i t expectedFailureMPS TODO Fixme raise assert empty tensor onlyNativeDeviceTypes test_Unfold_empty device inp = torch randn device=device unfold = torch nn Unfold kernel_size= device _test_module_empty_input unfold inp check_size=False assertRaisesRegex RuntimeError Expected D D inp = torch randn device=device unfold = torch nn Unfold kernel_size= device unfold inp onlyCUDA skipIfRocmArch MI _ARCH dtypes torch float torch double tf _on_and_off test_rnn_fused device dtype copy_rnn rnn rnn x_layer y_layer zip rnn all_weights rnn all_weights x y zip x_layer y_layer x data copy_ y data check_rnn_grads rnn rnn x_layer y_layer zip rnn all_weights rnn all_weights x y zip x_layer y_layer assertEqual x grad y grad atol= e- rtol= input_size = hidden_size = num_layers = seq_length = batch = input_val = torch randn seq_length batch input_size dtype=dtype grad_output = torch randn seq_length batch hidden_size dtype=dtype hx_val = torch randn num_layers batch hidden_size dtype=dtype grad_hy = torch randn num_layers batch hidden_size dtype=dtype torch backends cudnn flags enabled=False allow_tf =None module nn GRU nn LSTM bias True False rnn = module input_size hidden_size num_layers bias=bias dtype rnn_device = module input_size hidden_size num_layers bias=bias device dtype copy_rnn rnn rnn_device is_lstm = isinstance rnn nn LSTM is_lstm hx = hx_val clone requires_grad_ True hx_val clone add requires_grad_ True hx_device = hx_val clone device requires_grad_ True hx_val clone device add requires_grad_ True hx = hx_val clone requires_grad_ True hx_device = hx_val clone device requires_grad_ True inp = input_val clone requires_grad_ True inp_cu = input_val clone device requires_grad_ True output hy = rnn inp hx output hy = rnn_device inp_cu hx_device is_lstm torch autograd backward output hy hy grad_output grad_hy grad_hy + torch autograd backward output hy hy grad_output device grad_hy device grad_hy + device torch autograd backward output hy grad_output grad_hy torch autograd backward output hy grad_output device grad_hy device assertEqual output output assertEqual hy hy check_rnn_grads rnn rnn_device assertEqual inp grad inp_cu grad is_lstm assertEqual hx grad hx_device grad assertEqual hx grad hx_device grad assertEqual hx grad hx_device grad dtypes torch double dtypesIfMPS torch float test_BatchNorm_empty device dtype mod = torch nn BatchNorm d device inp = torch randn device=device dtype=dtype _test_module_empty_input mod inp device_type == cuda has_cudnn torch backends cudnn flags enabled=False _test_module_empty_input mod inp assertEqual mod running_mean torch tensor device=device assertEqual mod running_var torch tensor device=device assertEqual mod weight grad torch tensor device=device assertEqual mod bias grad torch tensor device=device onlyCUDA largeTensorTest GB test_prelu_backward_ bit_indexing device m = torch nn PReLU cuda half input_ = torch ones dtype=torch half device=device output = m input_ output backward input_ test_linear_empty device mod = torch nn Linear device inp = torch randn device=device _test_module_empty_input mod inp test_one_hot device cuda throws device assert invalid data xla mps ignore out bound indices device_type = cuda device_type = xla device_type = mps assertRaises RuntimeError torch nn functional one_hot torch tensor - device=device - assertRaises RuntimeError torch nn functional one_hot torch tensor device=device t = torch nn functional one_hot torch tensor device=device expected = torch tensor device=device assertEqual t expected t = torch nn functional one_hot torch tensor device=device - expected = torch tensor device=device assertEqual t expected t = torch nn functional one_hot torch tensor device=device expected = torch tensor device=device assertEqual t expected t = torch nn functional one_hot torch tensor device=device expected = torch tensor device=device assertEqual t expected t = torch nn functional one_hot torch tensor device=device expected = torch tensor device=device assertEqual t expected t = torch nn functional one_hot torch empty dtype=torch long device=device expected = torch empty dtype=torch long assertEqual t expected assertRaises RuntimeError torch nn functional one_hot torch empty dtype=torch long device=device assertRaises RuntimeError torch nn functional one_hot torch tensor device=device - expectedFailureMPS NotImplementedError aten rrelu_with_noise https github com pytorch pytorch issues test_nn_empty device One off tests ensure scalars nn yaml properly applied verify_scalars input output assertEqual input shape output shape assertEqual output numel input_shape module torch nn ELU torch nn Hardtanh torch nn LeakyReLU torch nn LogSigmoid torch nn RReLU torch nn Softshrink torch nn Softplus torch nn Sigmoid torch nn Tanh input = torch randn input_shape device=device requires_grad=True m = module output = m input verify_scalars input output expectedFailureMPS NotImplementedError aten rrelu_with_noise https github com pytorch pytorch issues test_nn_scalars device One off tests ensure scalars nn yaml properly applied verify_scalars input output input dim == assertEqual output shape assertNotEqual output shape output sum backward assertEqual input shape input grad shape input_shape module torch nn ELU torch nn Hardtanh torch nn LeakyReLU torch nn LogSigmoid torch nn RReLU torch nn Softshrink torch nn Softplus torch nn Sigmoid torch nn Tanh input = torch randn input_shape device=device requires_grad=True m = module output = m input verify_scalars input output test_nn_scalars_reductions device One off tests ensure scalars nn yaml properly applied verify_reduction_scalars input reduction output reduction = none input dim == assertEqual output shape assertNotEqual output shape output sum backward assertEqual input shape input grad shape input_shape reduction none mean sum module torch nn BCELoss torch nn L Loss torch nn MSELoss torch nn SmoothL Loss torch nn SoftMarginLoss input = torch randn input_shape device=device requires_grad=True target = torch empty input_shape device=device random_ sigmoid = nn Sigmoid input = torch randn input_shape device=device requires_grad=True m = module reduction=reduction output = m sigmoid input target verify_reduction_scalars input reduction output verify bogus reduction strings errors expectedFailureMPS CTCLoss unimplemented onlyNativeDeviceTypes test_invalid_reduction_strings device input = torch randn requires_grad=True device=device cinput = torch randn requires_grad=True device=device dtype=torch cfloat target = torch tensor device=device var = torch ones size=input size requires_grad=True device=device reduction none invalid v fn reduction == invalid assertRaises ValueError lambda fn fn v lambda F nll_loss input target reduction=reduction v lambda F cross_entropy input target reduction=reduction v lambda F kl_div input input reduction=reduction v lambda F huber_loss input input reduction=reduction v lambda F smooth_l _loss input input reduction=reduction v lambda F l _loss input input reduction=reduction v lambda F l _loss cinput cinput reduction=reduction v lambda F mse_loss input input reduction=reduction v lambda F hinge_embedding_loss input input reduction=reduction v lambda F poisson_nll_loss input input reduction=reduction v lambda F gaussian_nll_loss input input var reduction=reduction v lambda F binary_cross_entropy torch sigmoid input input gt torch get_default_dtype reduction=reduction v lambda F binary_cross_entropy_with_logits input input reduction=reduction zeros = torch zeros_like input torch int v lambda F multilabel_soft_margin_loss input zeros reduction=reduction v lambda F triplet_margin_loss input input input reduction=reduction v lambda F triplet_margin_with_distance_loss input input input reduction=reduction v lambda F margin_ranking_loss input input input sign reduction=reduction v lambda F cosine_embedding_loss input input input sign reduction=reduction log_probs = torch randn requires_grad=True device=device log_softmax targets = torch randint dtype=torch long device=device input_lengths = torch full dtype=torch long device=device target_lengths = torch randint dtype=torch long device=device v lambda F ctc_loss log_probs targets input_lengths target_lengths reduction=reduction FIXME should we allow derivatives these v lambda F soft_margin_loss input input sign detach reduction=reduction onlyNativeDeviceTypes test_smooth_l _loss_vs_huber_loss device _make_test_tensor shape contiguous=True contiguous test_tensor = torch randn shape device=device Select every other element innermost dimension make non-contiguous doubled_shape = list shape doubled_shape - = test_tensor = torch randn doubled_shape device=device test_tensor = test_tensor test_tensor _test_smooth_l _loss_vs_huber_loss_helper input target beta require_equal reduction mean sum none smooth_l = torch nn SmoothL Loss beta=beta reduction=reduction beta hyper-parameter called delta Huber huber = torch nn HuberLoss delta=beta reduction=reduction smooth_l _loss = smooth_l input target huber_loss = huber input target require_equal assertEqual smooth_l _loss huber_loss Huber loss should larger than smooth L loss factor beta assertEqual smooth_l _loss beta huber_loss _test_smooth_l _loss_vs_huber_loss_multi_input_helper beta require_equal Test non-vectorized case shape = _test_smooth_l _loss_vs_huber_loss_helper input=_make_test_tensor shape target=_make_test_tensor shape beta=beta require_equal=require_equal Test vectorized case innermost dim shape = _test_smooth_l _loss_vs_huber_loss_helper input=_make_test_tensor shape target=_make_test_tensor shape beta=beta require_equal=require_equal Test non-contiguous case _test_smooth_l _loss_vs_huber_loss_helper input=_make_test_tensor shape contiguous=False target=_make_test_tensor shape contiguous=False beta=beta require_equal=require_equal test_equal_when_beta_is_one _test_smooth_l _loss_vs_huber_loss_multi_input_helper beta= require_equal=True test_unequal_when_beta_is_less_than_one _test_smooth_l _loss_vs_huber_loss_multi_input_helper beta= require_equal=False test_unequal_when_beta_is_greater_than_one _test_smooth_l _loss_vs_huber_loss_multi_input_helper beta= require_equal=False test_equal_when_beta_is_one test_unequal_when_beta_is_less_than_one test_unequal_when_beta_is_greater_than_one onlyCPU test_smooth_l _loss_bfloat device test_dtype fn input target dtype input = input detach clone dtype=dtype requires_grad_ True input = input detach clone float requires_grad_ True target = target detach clone dtype=dtype target = target detach clone float out = fn input target out sum backward out = fn input target out sum backward assertEqual out dtype dtype assertEqual input grad dtype dtype assertEqual out out exact_dtype=False assertEqual input grad input grad exact_dtype=False func device nn SmoothL Loss device=device shapes = shape shapes x = torch randn shape device=device requires_grad=True t = torch randn shape device=device test_dtype func device x t torch bfloat We don t want make propagating NaN hard requirement ops these easy ones we should make them do so MPS NotImplementedError aten rrelu_with_noise_ https github com pytorch pytorch issues MPS NotImplementedError aten hardshrink out https github com pytorch pytorch issues expectedFailureMPS test_nonlinearity_propagate_nan device test nonlinearity args kwargs x = torch tensor nan device=device fn = getattr F nonlinearity try assertTrue math isnan fn x args kwargs item except Exception e implemented str e raise test relu test relu inplace=True test relu test elu test selu test celu test rrelu test rrelu inplace=True test hardtanh test tanh test sigmoid test logsigmoid test hardshrink test tanhshrink test softsign test softmin test softmax test log_softmax test leaky_relu test threshold test threshold inplace=True expectedFailureMPS TypeError float MPS framework doesn t support float parametrize_test mode nearest-exact nearest test_upsamplingNearest d device mode Forward AD does support XLA because XLA tensors don t have storage check_forward_ad = torch device device type = xla m = nn Upsample size= mode=mode in_t = torch ones device=device dtype=torch double in_uint _t = torch ones dtype=torch uint device=device warnings catch_warnings record=True w out_t = m in_t out_uint _t = m in_uint _t assertEqual torch ones device=device dtype=torch double out_t data assertEqual torch ones dtype=torch uint device=device out_uint _t data Checks upsampling input = torch randn requires_grad=True device=device dtype=torch double gradcheck lambda x F interpolate x mode=mode input check_forward_ad=check_forward_ad gradgradcheck lambda x F interpolate x mode=mode input check_fwd_over_rev=check_forward_ad Checks downsampling input = torch randn requires_grad=True device=device dtype=torch double gradcheck lambda x F interpolate x mode=mode input check_forward_ad=check_forward_ad gradgradcheck lambda x F interpolate x mode=mode input check_fwd_over_rev=check_forward_ad consistency CUDA CPU check torch device device type == cuda input_cuda = torch randn device=device dtype=torch double input_cpu = input_cuda cpu output_cuda = F interpolate input_cuda mode=mode output_cpu = F interpolate input_cpu mode=mode assertEqual output_cuda cpu output_cpu output_cuda = F interpolate input_cuda mode=mode output_cpu = F interpolate input_cpu mode=mode assertEqual output_cuda cpu output_cpu parametrize_test isize osize test_upsamplingNearest d_correctness device isize osize Here we check output matches OpenCV s INTER_NEAREST-like result in_t = torch arange isize dtype=torch float device=device unsqueeze unsqueeze out_t = F interpolate in_t size= osize recompute_scale_factor=False mode= nearest compute expected output OpenCV expected_out = torch zeros osize dtype=torch float unsqueeze unsqueeze scale = isize osize o range osize i_f = o scale i = int i_f expected_out o = in_t i expected_out = expected_out device=device assertEqual out_t expected_out test_upsamplingNearestExact d_rescale device Checks https github com pytorch pytorch issues isize = in_t = torch arange isize dtype=torch float device=device unsqueeze unsqueeze s case broken See issue https github com pytorch pytorch issues s out_t = F interpolate in_t scale_factor=s recompute_scale_factor=False mode= nearest-exact expected_out = in_t assertEqual out_t expected_out msg=f scale s checks data duplication output_size == input_size s case broken See issue https github com pytorch pytorch issues s out_t = F interpolate in_t scale_factor=s recompute_scale_factor=False mode= nearest-exact input expected out expected_out = in_t repeat_interleave dim=- assertEqual out_t expected_out skipIfMPS Partially passes https github com pytorch pytorch issues parametrize_test isize osize test_upsamplingNearestExact d_correctness device isize osize Here we check output matches Scikit-Image Scipy-like result Checks https github com pytorch pytorch issues in_t = torch arange isize dtype=torch float device=device unsqueeze unsqueeze out_t = F interpolate in_t size= osize recompute_scale_factor=False mode= nearest-exact compute expected output scikit-image scipy expected_out = torch zeros osize dtype=torch float unsqueeze unsqueeze scale = isize osize o range osize i_f = o + scale i = int i_f expected_out o = in_t i expected_out = expected_out device=device assertEqual out_t expected_out expectedFailureMPS TypeError MPS framework doesn t support float parametrize_test memory_format torch contiguous_format torch channels_last parametrize_test mode nearest nearest-exact test_upsamplingNearest d device memory_format mode Forward AD does support XLA because XLA tensors don t have storage check_forward_ad = torch device device type = xla in_t = torch ones device=device dtype=torch double contiguous memory_format=memory_format in_uint _t = torch ones dtype=torch uint device=device contiguous memory_format=memory_format warnings catch_warnings record=True w out_t = F interpolate in_t size= mode=mode out_uint _t = F interpolate in_uint _t size= mode=mode assertEqual len w assertEqual torch ones device=device dtype=torch double out_t assertEqual torch ones dtype=torch uint device=device out_uint _t Assert memory format carried through output assertTrue out_t is_contiguous memory_format=memory_format test forward when input s height same width in_t = torch ones device=device dtype=torch double contiguous memory_format=memory_format requires_grad_ out_t = F interpolate in_t size= mode=mode assertEqual torch ones device=device dtype=torch double out_t assertTrue out_t is_contiguous memory_format=memory_format out_t backward torch randn_like out_t assertTrue in_t grad is_contiguous memory_format=memory_format test backward when input s height same width input = torch ones requires_grad=True device=device dtype=torch double contiguous memory_format=memory_format gradcheck lambda x F interpolate x size= mode=mode input check_forward_ad=check_forward_ad gradgradcheck lambda x F interpolate x size= mode=mode input check_fwd_over_rev=check_forward_ad input = torch randn requires_grad=True device=device dtype=torch double contiguous memory_format=memory_format assertEqual F interpolate input mode=mode F interpolate input scale_factor= mode=mode gradcheck lambda x F interpolate x mode=mode input check_forward_ad=check_forward_ad gradgradcheck lambda x F interpolate x mode=mode input check_fwd_over_rev=check_forward_ad Assert cpu cuda handle channels_last memory format same way https github com pytorch pytorch issues torch device device type == cuda shapes scale_factor product a_cuda = torch randn shapes device=device dtype=torch double contiguous memory_format=memory_format requires_grad_ a_cpu = a_cuda detach cpu requires_grad_ out_cuda = F interpolate a_cuda scale_factor=scale_factor mode=mode out_cpu = F interpolate a_cpu scale_factor=scale_factor mode=mode assertEqual out_cpu cuda out_cuda g_cuda = torch randn_like out_cuda g_cpu = g_cuda cpu out_cuda backward g_cuda out_cpu backward g_cpu assertEqual a_cuda grad a_cpu grad parametrize_test memory_format torch contiguous_format torch channels_last parametrize_test isize osize test_upsamplingNearest d_correctness device memory_format isize osize Here we check output matches OpenCV s INTER_NEAREST-like result in_t = torch arange isize isize dtype=torch float device=device reshape isize isize in_t = in_t contiguous memory_format=memory_format out_t = F interpolate in_t size= osize osize recompute_scale_factor=False mode= nearest compute expected output OpenCV expected_out = torch zeros osize osize dtype=torch float scale = isize osize o range osize i _f = o scale i = int i _f o range osize i _f = o scale i = int i _f expected_out o o = in_t i i expected_out = expected_out device=device assertEqual out_t expected_out skipIfMPS Partially passes https github com pytorch pytorch issues parametrize_test memory_format torch contiguous_format torch channels_last parametrize_test isize osize test_upsamplingNearestExact d_correctness device memory_format isize osize Here we check output matches Scikit-Image Scipy-like result Checks https github com pytorch pytorch issues in_t = torch arange isize isize dtype=torch float device=device reshape isize isize in_t = in_t contiguous memory_format=memory_format out_t = F interpolate in_t size= osize osize recompute_scale_factor=False mode= nearest-exact compute expected output Scikit-Image Scipy expected_out = torch zeros osize osize dtype=torch float scale = isize osize o range osize i _f = o + scale i = int i _f o range osize i _f = o + scale i = int i _f expected_out o o = in_t i i expected_out = expected_out device=device assertEqual out_t expected_out expectedFailureMPS TypeError MPS framework doesn t support float parametrize_test memory_format torch contiguous_format torch channels_last_ d parametrize_test mode nearest nearest-exact test_upsamplingNearest d device memory_format mode Forward AD does support XLA because XLA tensors don t have storage check_forward_ad = torch device device type = xla m = nn Upsample size= mode=mode in_t = torch ones device=device dtype=torch double contiguous memory_format=memory_format requires_grad_ in_uint _t = torch ones dtype=torch uint device=device contiguous memory_format=memory_format warnings catch_warnings record=True w out_t = m in_t out_uint _t = m in_uint _t expected_output = torch ones device=device dtype=torch double assertEqual expected_output out_t assertEqual expected_output torch uint out_uint _t Assert memory format carried through output assertTrue out_t is_contiguous memory_format=memory_format out_t backward torch randn_like out_t assertTrue in_t grad is_contiguous memory_format=memory_format input = torch randn requires_grad=True device=device dtype=torch double contiguous memory_format=memory_format gradcheck lambda x F interpolate x mode=mode input check_forward_ad=check_forward_ad gradgradcheck lambda x F interpolate x mode=mode input check_fwd_over_rev=check_forward_ad Assert cpu cuda handle channels_last memory format same way https github com pytorch pytorch issues torch device device type == cuda = torch ones device=device requires_grad=True dtype=torch double contiguous memory_format=torch channels_last_ d make data asymmetric ensure cuda cpu handle channels_last appropriately = = out_cuda = torch nn functional interpolate scale_factor= mode=mode out_cpu = torch nn functional interpolate cpu scale_factor= mode=mode assertEqual out_cpu out_cuda cpu gradcheck lambda x F interpolate x mode=mode check_forward_ad=check_forward_ad gradgradcheck lambda x F interpolate x mode=mode check_fwd_over_rev=check_forward_ad gradcheck lambda x F interpolate x mode=mode cuda check_forward_ad=check_forward_ad gradgradcheck lambda x F interpolate x mode=mode cuda check_fwd_over_rev=check_forward_ad parametrize_test memory_format torch contiguous_format torch channels_last_ d parametrize_test isize osize test_upsamplingNearest d_correctness device memory_format isize osize Here we check output matches OpenCV s INTER_NEAREST-like result in_t = torch arange isize isize isize dtype=torch float device=device in_t = in_t reshape isize isize isize in_t = in_t contiguous memory_format=memory_format out_t = F interpolate in_t size= osize osize osize recompute_scale_factor=False mode= nearest compute expected output OpenCV expected_out = torch zeros osize osize osize dtype=torch float scale = isize osize o range osize i _f = o scale i = int i _f o range osize i _f = o scale i = int i _f o range osize i _f = o scale i = int i _f expected_out o o o = in_t i i i expected_out = expected_out device=device assertEqual out_t expected_out parametrize_test memory_format torch contiguous_format torch channels_last_ d parametrize_test isize osize test_upsamplingNearestExact d_correctness device memory_format isize osize Here we check output matches Scikit-Image Scipy-like result Checks https github com pytorch pytorch issues in_t = torch arange isize isize isize dtype=torch float device=device in_t = in_t reshape isize isize isize in_t = in_t contiguous memory_format=memory_format out_t = F interpolate in_t size= osize osize osize recompute_scale_factor=False mode= nearest-exact compute expected output Scikit-Image Scipy expected_out = torch zeros osize osize osize dtype=torch float scale = isize osize o range osize i _f = o + scale i = int i _f o range osize i _f = o + scale i = int i _f o range osize i _f = o + scale i = int i _f expected_out o o o = in_t i i i expected_out = expected_out device=device assertEqual out_t expected_out parametrize_test antialias True False parametrize_test align_corners True False parametrize_test mode bilinear bicubic parametrize_test memory_format torch contiguous_format torch channels_last expectedFailureMPS double device type onlyNativeDeviceTypes test_upsamplingBiMode d device antialias align_corners mode memory_format Forward AD does support XLA because XLA tensors don t have storage check_forward_ad = torch device device type = xla kwargs = dict mode=mode align_corners=align_corners antialias=antialias test float scale factor up downsampling scale_factor in_t = torch ones device=device dtype=torch double contiguous memory_format=memory_format requires_grad_ out_size = int math floor in_t shape - scale_factor warnings catch_warnings record=True w out_t = F interpolate in_t scale_factor=scale_factor kwargs expected_out = torch ones out_size out_size device=device dtype=torch double assertEqual expected_out out_t Assert memory format carried through output assertTrue out_t is_contiguous memory_format=memory_format out_t backward torch randn_like out_t assertTrue in_t grad is_contiguous memory_format=memory_format torch device device type == cuda Bilinear backward nondeterministic because atomicAdd usage nondet_tol = e- nondet_tol = input = torch randn device=device dtype=torch double contiguous memory_format=memory_format requires_grad_ gradcheck lambda x F interpolate x out_size kwargs input check_forward_ad=check_forward_ad nondet_tol=nondet_tol gradgradcheck lambda x F interpolate x out_size kwargs input check_fwd_over_rev=check_forward_ad nondet_tol=nondet_tol Assert cpu cuda give same results torch device device type == cuda shapes a_cuda = torch randn shapes device=device dtype=torch double contiguous memory_format=memory_format requires_grad_ a_cpu = a_cuda detach cpu requires_grad_ warnings catch_warnings record=True out_cuda = F interpolate a_cuda scale_factor=scale_factor kwargs out_cpu = F interpolate a_cpu scale_factor=scale_factor kwargs assertEqual out_cpu out_cuda cpu g_cuda = torch randn_like out_cuda g_cpu = g_cuda cpu out_cuda backward g_cuda out_cpu backward g_cpu assertEqual a_cuda grad a_cpu grad parametrize_test antialias True False parametrize_test num_channels parametrize_test mode nearest nearest-exact bilinear bicubic parametrize_test dtype integral_types + floating_types skipIfMPS Error message wrong some dtypes onlyNativeDeviceTypes test_upsamplingBiMode d_nonsupported_dtypes device antialias num_channels mode dtype x = torch ones num_channels dtype=dtype device=device should_raise_runtime_error = True nearest mode antialias raise SkipTest Nearest mode does have antialiasing dtype torch uint + floating_types should_raise_runtime_error = False mode bilinear bicubic dtype floating_types device == cpu dtype == torch uint should_raise_runtime_error = False should_raise_runtime_error assertRaisesRegex RuntimeError implemented F interpolate x mode=mode antialias=antialias _ = F interpolate x mode=mode antialias=antialias parametrize_test memory_format torch contiguous_format torch channels_last test_upsamplingBilinear d_aa_correctness device memory_format NOTE We expand batch dim such ` b c ` above maximum size CUDA grid z-dimension shape = t_in = torch arange dtype=torch float device=device reshape shape t_in = t_in expand shape t_in = t_in contiguous memory_format=memory_format This expected result obtain using PIL Image resize c range a_in = t_in numpy c pil_in = Image fromarray a_in pil_out = pil_in resize resample=Image LINEAR expected_out = torch tensor device=device dtype=t_in dtype reshape t_out = F interpolate t_in size= mode= bilinear align_corners=False antialias=True assertEqual expected_out expand shape t_out Partially passes NotImplementedError aten upsample_bicubic d out https github com pytorch pytorch issues skipIfMPS parametrize_test memory_format torch contiguous_format torch channels_last parametrize_test mode bilinear bicubic parametrize_test antialias True False parametrize_test align_corners True False parametrize_test num_channels parametrize_test output_size parametrize_test check_as_unsqueezed_ d_tensor True False parametrize_test non_contig False sliced restrided parametrize_test batch_size test_upsamplingBiMode d_consistency device memory_format mode antialias align_corners num_channels output_size check_as_unsqueezed_ d_tensor non_contig batch_size Check output value consistency between resized_input_uint resized input_float torch device device type == cuda raise SkipTest CUDA implementation yet supporting uint torch manual_seed - input range set bicubic mode because bicubic kernel may create intermediate values outside range which need clipped uint path float path This isn t issue bilinear kernel input_range = mode == bicubic input_ui = torch randint input_range size= batch_size num_channels dtype=torch uint device=device input_ui = input_ui contiguous memory_format=memory_format non_contig == sliced input_ui = input_ui - - non_contig == restrided input_ui = input_ui batch_size == check_as_unsqueezed_ d_tensor input_ui = input_ui input_ui = input_ui None input_f = input_ui float output_f = F interpolate input_f size= output_size output_size mode=mode align_corners=align_corners antialias=antialias round clip output_ui = F interpolate input_ui size= output_size output_size mode=mode align_corners=align_corners antialias=antialias non_contig False assertTrue input_ui is_contiguous memory_format=memory_format FIXME if-clause shows current behaviour which definitely unexpected Ideally we want fix such both ui f outputs also channels_last See more details https github com pytorch pytorch pull batch_size == check_as_unsqueezed_ d_tensor memory_format == torch channels_last assertTrue output_ui is_contiguous assertTrue output_f is_contiguous assertTrue output_ui is_contiguous memory_format=memory_format assertTrue output_f is_contiguous memory_format=memory_format mode == bilinear torch testing assert_close output_f output_ui float rtol= atol= diff = output_f - output_ui float abs assertLess diff max threshold = percent = assertLess diff threshold float mean percent threshold = percent = assertLess diff threshold float mean percent assertLess diff mean parametrize_test memory_format torch contiguous_format torch channels_last parametrize_test align_corners True False parametrize_test input_size output_size test_upsamplingBiLinear d_consistency_interp_size_bug device memory_format align_corners input_size output_size Non-regression test https github com pytorch pytorch pull torch device device type == cuda raise SkipTest CUDA implementation yet supporting uint mode = bilinear input_ui = torch randint size= input_size input_size dtype=torch uint device=device input_ui = input_ui contiguous memory_format=memory_format input_f = input_ui float output_f = F interpolate input_f size= output_size output_size mode=mode align_corners=align_corners antialias=False round torch uint output_ui = F interpolate input_ui size= output_size output_size mode=mode align_corners=align_corners antialias=False torch testing assert_close output_f output_ui atol= rtol= test_upsamplingBicubic d_correctness device test output against known input align_corners=False result must match opencv in_t = torch arange device=device view expected_out_t = torch tensor - device=device out_t = F interpolate in_t scale_factor= mode= bicubic align_corners=False torch set_printoptions precision= assertEqual out_t expected_out_t atol= e- rtol= parametrize_test memory_format torch contiguous_format torch channels_last test_upsamplingBicubic d_aa_correctness device memory_format t_in = torch arange dtype=torch float device=device reshape t_in = t_in contiguous memory_format=memory_format This expected result obtain using PIL Image resize c range a_in = t_in numpy c pil_in = Image fromarray a_in pil_out = pil_in resize resample=Image BICUBIC expected_out = torch tensor device=device dtype=t_in dtype reshape t_out = F interpolate t_in size= mode= bicubic align_corners=False antialias=True assertEqual expected_out t_out expectedFailureMPS NotImplementedError aten upsample_trilinear d out https github com pytorch pytorch issues parametrize_test align_corners True False parametrize_test memory_format torch contiguous_format torch channels_last_ d test_upsamplingTrilinear d device align_corners memory_format kwargs = dict mode= trilinear align_corners=align_corners test float scale factor up downsampling scale_factor m = nn Upsample scale_factor=scale_factor kwargs in_t = torch ones device=device dtype=torch double in_t = in_t contiguous memory_format=memory_format requires_grad_ out_size = int math floor in_t shape - scale_factor warnings catch_warnings record=True w out_t = m in_t expected_out = torch ones out_size out_size out_size device=device dtype=torch double assertEqual expected_out out_t Assert memory format carried through output assertTrue out_t is_contiguous memory_format=memory_format grad_out = torch randn_like out_t contiguous memory_format=memory_format in_t grad = None out_t backward grad_out grad_in = in_t grad assertTrue grad_in is_contiguous memory_format=memory_format memory_format == torch channels_last_ d check grad inputs CF CL match in_t grad = None out_t backward grad_out contiguous assertEqual in_t grad grad_in input = torch randn requires_grad=True dtype=torch double assertEqual F interpolate input out_size out_size out_size kwargs F interpolate input scale_factor=scale_factor kwargs gradcheck lambda x F interpolate x out_size kwargs input gradgradcheck lambda x F interpolate x out_size kwargs input onlyCUDA skipCUDAIfRocm msg= launch bounds error out ROCM dtypes torch half torch bfloat largeTensorTest GB test_upsampling_ bit_indexing_channels_last device dtype x = torch rand dtype=dtype device=device out = torch nn functional interpolate x memory_format=torch channels_last scale_factor= mode= nearest out_ref = torch nn functional interpolate x scale_factor= mode= nearest del x assertTrue torch allclose out out_ref x = torch ones dtype=dtype cuda memory_format=torch channels_last out = torch nn functional interpolate x scale_factor= mode= nearest assertEqual out out - onlyCUDA dtypes torch half largeTensorTest GB test_replicatepad_ bit_indexing device dtype conv = torch nn Conv d padding_mode= replicate device=device dtype=dtype x = torch randn size= dtype=dtype device=device y = conv x torch mean y backward onlyCUDA dtypes torch half largeTensorTest GB test_upsamplingnearest d_backward_ bit_indexing device dtype x = torch randn size= device=device dtype=dtype requires_grad_ y = F interpolate x scale_factor= mode= nearest y backward torch randn_like y _slow_masked_softmax input mask exp = torch exp input exp = exp mask s = exp sum dim= keepdim=True expand exp size exp s test_masked_softmax_mask_types device Test mask type LxL attention mask mask type BxL padding mask mask type generic BxHxLxL mask processed correctly fast path results match explicit slow calculation sizes = B num_heads L sizes mask_type == = attention mask shape LxL src_mask_orig = torch randint L L bool src_mask = src_mask_orig reshape L L expand B num_heads L L bool mask_type == = padding mask shape BxL src_key_padding_mask_orig = torch randint B L bool src_key_padding_mask = src_key_padding_mask_orig reshape B L expand B num_heads L L bool mask_type == = shape BxHxLxL generic_mask = torch randint B num_heads L L bool masks = src_mask_orig src_mask src_key_padding_mask_orig src_key_padding_mask generic_mask generic_mask dim mask_orig mask mask_type masks device_type == cuda num_heads mask_type == CUDA path doesn t support padding mask when number heads odd continue input = torch randn B num_heads L L device_type == cuda input = input cuda mask = mask cuda mask_orig = mask_orig cuda native_res = torch _masked_softmax input mask_orig dim mask_type mask = ~mask slow_masked_softmax input mask exp = torch exp input exp = exp mask s = exp sum dim=dim keepdim=True expand exp size exp s pt_res = slow_masked_softmax input mask pt_res = torch nan_to_num pt_res mask_not = mask logical_not In result should only fill entirely masked out rows since those non-deterministic may Converts rows all True s False mask_out = mask_not all dim keepdim=True expand mask_not shape assertEqual pt_res masked_fill mask_out native_res masked_fill mask_out exact_dtype=True onlyCUDA gcIfJetson test_masked_softmax_devices_parity Test softmax mask type LxL attention mask mask type BxL padding mask mask type BxHxLxL generic mask gives same result CPU CUDA sizes = B num_heads L sizes mask_type == = attention mask shape LxL src_mask = torch randint L L bool mask_type == = padding mask shape BxL src_key_padding_mask = torch randint B L bool mask_type == = generic mask shape BxHxLxL generic_mask = torch randint B num_heads L L bool masks = src_mask src_key_padding_mask generic_mask input = torch randn B num_heads L L dim mask mask_type masks num_heads mask_type == CUDA path doesn t support padding mask when number heads odd continue softmax_on_device mask input device Compute softmax given device input_device = input device mask_device = mask device softmax_res = torch _masked_softmax input_device mask_device dim mask_type mask_type == mask_expanded = mask_device reshape L L expand B num_heads L L bool mask_type == mask_expanded = mask_device reshape B L expand B num_heads L L bool mask_expanded = mask_device In result should only fill entirely masked out rows since those non-deterministic may Fill rows all True s mask_out = mask_expanded all dim keepdim=True expand mask_expanded shape softmax_res = softmax_res masked_fill mask_out softmax_res cpu_res = softmax_on_device mask input cpu cuda_res = softmax_on_device mask input cuda assertEqual cpu_res cuda_res exact_dtype=True test_masked_softmax device sizes = B num_heads L sizes dim input = torch randn B num_heads L L mask = torch randint B L mask = mask reshape B L expand B num_heads L L bool mask_type = BxL = src_key_padding_mask device_type == cuda input = input cuda mask = mask cuda native_res = torch _masked_softmax input mask dim mask_type mask = ~mask slow_masked_softmax input mask exp = torch exp input exp = exp mask s = exp sum dim=dim keepdim=True expand exp size exp s pt_res = slow_masked_softmax input mask pt_res = torch nan_to_num pt_res mask_not = mask logical_not In result should only fill entirely masked out rows since those non-deterministic may Converts rows all True s False mask_out = mask_not all dim keepdim=True expand mask_not shape assertEqual pt_res masked_fill mask_out native_res masked_fill mask_out exact_dtype=True dtypes torch bfloat torch half precisionOverride torch bfloat e- torch half e- test_masked_softmax_lowp dtype sizes = B num_heads L sizes dim input_lowp = torch randn B num_heads L L dtype=dtype requires_grad_ input_ref = input_lowp float detach requires_grad_ mask = torch randint B L mask = mask reshape B L expand B num_heads L L bool mask_type res_ref = torch _masked_softmax input_ref mask dim mask_type res = torch _masked_softmax input_lowp mask dim mask_type assertEqual res_ref dtype res grad_lowp = torch randn_like res_ref dtype=dtype grad_ref = grad_lowp float res_ref backward grad_ref res backward grad_lowp assertEqual input_ref grad dtype input_lowp grad _test_masked_softmax_helper input dim mask mask_type input_ref = input detach clone requires_grad_ result = torch _masked_softmax input mask dim mask_type expected = torch _softmax input_ref masked_fill mask float -inf dim False grad = torch randn_like expected dtype=expected dtype result backward grad expected backward grad Make sure optional argument works well dim == input dim - input_ref_default = input detach clone requires_grad_ result_default = torch _masked_softmax input_ref_default mask None mask_type result_default backward grad assertEqual result result_default assertEqual input grad input_ref_default grad In result should only fill entirely masked out rows since those non-deterministic may Converts rows all True s False mask_out = mask all dim keepdim=True expand mask shape assertEqual result masked_fill mask_out expected masked_fill mask_out assertEqual input grad torch nan_to_num input_ref grad assertEqual input grad input grad masked_fill mask test_masked_softmax_grad device shapes = shape shapes dims = len shape - len shape dim dims mask_type = BxL = src_key_padding_mask input = torch randn shape requires_grad=True mask = torch randint shape bool device_type == cuda input = input cuda detach requires_grad_ mask = mask cuda _test_masked_softmax_helper input dim mask mask_type In test forward pass expected produce nan s because when dim= we only have unspecified values test_masked_softmax_forward_with_nans device dim = shapes = x y shapes mask_type = BxL = src_key_padding_mask input = torch randn x y requires_grad=True mask = torch tensor i i range y expand x y bool device_type == cuda input = input cuda detach requires_grad_ mask = mask cuda _test_masked_softmax_helper input dim mask mask_type onlyCUDA test_masked_softmax_transformer_layout device B = num_heads = L = input = torch randn B num_heads L L dim = input dim - mask = torch randint B L mask_type = BxL = src_key_padding_mask device_type == cuda input = input cuda mask = mask cuda mask = mask bool native_res = torch _masked_softmax input mask dim mask_type mask = mask reshape B L expand B num_heads L L mask = ~mask mask = mask float pt_res = _slow_masked_softmax input mask assertEqual pt_res native_res exact_dtype=True onlyCUDA test_masked_softmax_TxT_layout device B = num_heads = L = input = torch randn B num_heads L L dim = input dim - mask = torch randint L L mask_type = LxL = src_mask device_type == cuda input = input cuda mask = mask cuda mask = mask bool native_res = torch _masked_softmax input mask dim mask_type mask = mask expand B num_heads L L mask = ~mask mask = mask float pt_res = _slow_masked_softmax input mask assertEqual pt_res native_res exact_dtype=True onlyCPU dtypes torch bfloat torch half test_log_softmax_cpu device dtype dim inputf = torch rand device=device dtype=torch float requires_grad=True input = inputf dtype detach requires_grad_ True outf = F log_softmax inputf dim=dim out = F log_softmax input dim=dim assertEqual out outf dtype=dtype atol= rtol= out sum backward outf sum backward assertEqual input grad inputf grad dtype atol= rtol= onlyCPU dtypes torch bfloat torch half test_softmax_cpu device dtype dim inputf = torch rand device=device dtype=torch float requires_grad=True input = inputf dtype detach requires_grad_ True outf = F softmax inputf dim=dim out = F softmax input dim=dim assertEqual out outf dtype atol= e- rtol= out sum backward outf sum backward assertEqual input grad inputf grad dtype atol= e- rtol= dtypesIfCUDA torch half torch float dtypes torch float test_softmax_results device dtype Non-even sizes non-zero shifts test fallback paths vectorized kernel Note dim needed exercise vectorized non-persistent path BERT-esque sizes = shifts = fn F softmax F log_softmax size sizes shift shifts input = torch rand size device=device dtype=dtype Note With largest tests we can hit upper limit fp when we sum so scale input down stay nicer range dtype == torch float input = input input = input shift shift Note Don t want bprop back through slice op input = input detach requires_grad_ True ref_input = input clone cpu detach requires_grad_ True dim ref_output = fn ref_input dtype=torch float dim=dim output = fn input dtype=torch float dim=dim grad_output = torch rand size device=device dtype=dtype grad_output = grad_output shift shift ref_grad_output = grad_output clone cpu detach grad_input = torch autograd grad output input grad_outputs= grad_output create_graph=True ref_grad_input = torch autograd grad ref_output ref_input grad_outputs= ref_grad_output create_graph=True grad_input sum backward ref_grad_input sum backward assertEqual output ref_output assertEqual grad_input ref_grad_input assertEqual input grad ref_input grad onlyCUDA dtypes torch float torch half largeTensorTest GB largeTensorTest GB cpu test_warp_softmax_ bit_indexing device dtype run_test shape x = torch randn shape device= cuda dtype=torch float requires_grad=True y = F log_softmax x dim=- dtype=dtype y backward y torch no_grad xx = x cpu requires_grad_ yy = F log_softmax xx float dim=- dtype yy backward yy workaround reduce memory usage vs assertEqual see rtol atol = torch testing _comparison get_tolerances dtype rtol=None atol=None assertTrue torch allclose y cpu yy rtol=rtol atol=atol x half rtol _ = torch testing _comparison get_tolerances torch half rtol=None atol=None assertTrue torch allclose x grad cpu xx grad rtol=rtol atol= e- run_test Illegal memory access https github com pytorch pytorch issues run_test invalid configuration argument https github com pytorch pytorch issues onlyCUDA dtypes torch double test_softmax_double device dtype logits = torch randn dtype=dtype device=device expected_ones = F log_softmax logits dim= exp sum dim= assertEqual expected_ones torch ones_like expected_ones backward logits = torch randn dtype=dtype device=device requires_grad=True out = F log_softmax logits dim= grad = torch randn_like out out backward grad logits_cpu = logits detach cpu logits_cpu requires_grad = True out_cpu = F log_softmax logits_cpu dim= out_cpu backward grad detach cpu assertEqual logits grad logits_cpu grad onlyCUDA dtypes torch half largeTensorTest GB largeTensorTest GB cpu precisionOverride torch half test_softmax_ bit_indexing device dtype run_test shape x = torch ones shape device=device dtype=dtype requires_grad=True y = F log_softmax x dim=- dtype=dtype y backward y assertEqual y y - assertEqual x grad x grad - run_test + https github com pytorch pytorch issues dtypes torch float dtypesIfCUDA torch float torch half test_log_softmax_big device dtype _test_helper shape generate tensor big numbers exactly representable dtype constant offset tensor small numbers logsoftmax small big tensors should equal x_small = torch randint shape dtype=dtype device=device offset = e dtype == torch half e x_big = x_small + offset assertEqual F log_softmax x_small - F log_softmax x_big - _test_helper device_type == cuda test non-persistent softmax kernel _test_helper test_save_lstm_compatibility device Test saving LSTM PyTorch older can still loaded newer versions PyTorch model = nn LSTM x = torch randn expected = model x Get state dict PyTorch LSTM Before PyTorch proj_size didn t exist assert model proj_size == state_dict = model __dict__ del state_dict proj_size load model loaded_model = nn LSTM loaded_model __setstate__ state_dict result = loaded_model x assertEqual result expected onlyCUDA tf _on_and_off test_grid_sample_large device issue_ input_tensor = torch rand dtype=torch float device=device requires_grad=True coords = torch tensor - dtype=torch float device=device coords = coords unsqueeze unsqueeze repeat result = torch nn functional grid_sample input_tensor coords assertEqual result torch tensor dtype=torch float device=device result backward torch ones_like result torch cuda synchronize issue_ issue_ _ dtype image = torch arange - dtype=dtype device=device view image requires_grad_ grid = torch nn functional affine_grid torch tensor dtype=dtype device=device grid = float inf result = torch nn functional grid_sample image grid padding_mode= zeros tol_override = atol rtol dtype == torch half assertEqual result torch tensor device=device dtype=dtype tol_override result backward torch ones_like result expected_grad = torch ones_like image expected_grad = assertEqual image grad expected_grad atol= rtol= issue_ _ torch half issue_ _ torch float issue_ _ torch double issue_ _ param = torch tensor - e+ - e+ dtype=torch float device=device img = torch zeros dtype=torch float device=device requires_grad=True grid = torch nn functional affine_grid param img size result = torch nn functional grid_sample img grid assertEqual result torch zeros device=device dtype=torch float result backward torch ones_like result torch cuda synchronize issue_ _ dtypes torch float torch double largeTensorTest lambda device dtype Compute sum large tensor sizes im numel + small_image numel + small_image grad numel + large_view grad numel sizeof dtype + torch tensor dtype=dtype element_size test_grid_sample_large_index_ d device dtype Test -bit indexing grid_sample gh- Try accessing corners there should no segfault coords = torch tensor - - + - - + + + device=device dtype=dtype coords = coords expand im = torch zeros device=device dtype=dtype Compare sampling large strides same op contiguous tensor coords = torch rand device=device dtype=dtype large_view = im small_image = torch rand_like large_view large_view = small_image large_view requires_grad small_image requires_grad = True True assertTrue sum i s i s zip large_view size large_view stride = msg= View must use -bit indexing mode padding_mode align_corners itertools product nearest bilinear bicubic zeros border reflection True False = F grid_sample small_image coords mode=mode padding_mode=padding_mode align_corners=align_corners sum backward b = F grid_sample large_view coords mode=mode padding_mode=padding_mode align_corners=align_corners b sum backward assertEqual b assertEqual small_image grad large_view grad small_image grad zero_ large_view grad zero_ dtypes torch float torch double largeTensorTest lambda device dtype Compute sum large tensor sizes im numel + small_image numel + small_image grad numel + large_view grad numel sizeof dtype + torch tensor dtype=dtype element_size test_grid_sample_large_index_ d device dtype Test -bit indexing grid_sample gh- Try accessing corners there should no segfault coords = torch full device=device dtype=dtype im = torch zeros device=device dtype=dtype result = F grid_sample im coords align_corners=False assertEqual result torch zeros device=device dtype=dtype Compare sampling large strides same op contiguous tensor coords = torch rand device=device dtype=dtype large_view = im small_image = torch rand_like large_view large_view = small_image small_image requires_grad large_view requires_grad = True True assertTrue sum i s i s zip large_view size large_view stride = msg= View must use -bit indexing mode padding_mode align_corners itertools product nearest bilinear zeros border reflection True False = F grid_sample small_image coords mode=mode padding_mode=padding_mode align_corners=align_corners sum backward b = F grid_sample large_view coords mode=mode padding_mode=padding_mode align_corners=align_corners b sum backward assertEqual b assertEqual small_image grad large_view grad small_image grad zero_ large_view grad zero_ onlyCUDA test_grid_sample_half_precision helper shape_in shape_out align_corners mode bilinear nearest bicubic len shape_in = mode == bicubic continue data = torch randn shape_in device= cuda dtype=torch half grid = torch rand shape_out device= cuda dtype=torch half - out_half = F grid_sample data grid mode=mode padding_mode= zeros align_corners=align_corners out_double = F grid_sample data double grid double mode=mode padding_mode= zeros align_corners=align_corners assertEqual out_half out_double half msg=f grid_sample mode = mode doesn t match helper True helper True helper False helper False onlyCUDA test_grid_sample_bfloat _precision helper shape_in shape_out align_corners mode bilinear nearest bicubic len shape_in = mode == bicubic continue data = torch randn shape_in device= cuda dtype=torch bfloat grid = torch rand shape_out device= cuda dtype=torch bfloat - out_half = F grid_sample data grid mode=mode padding_mode= zeros align_corners=align_corners out_double = F grid_sample data double grid double mode=mode padding_mode= zeros align_corners=align_corners assertEqual out_half out_double bfloat msg=f grid_sample mode = mode doesn t match helper True helper True helper False helper False _test_gumbel_softmax_st_shapes device dtype shape dim count_expected logits = torch randn shape dtype=torch float device=device logits = logits dtype y_draw = F gumbel_softmax logits hard=True dim=dim All values positive assertGreaterEqual y_draw min Shape unchanged assertTrue y_draw shape == logits shape One choice per draw assertEqual y_draw sum count_expected atol=torch finfo y_draw dtype eps rtol= _test_gumbel_softmax_straight_through device dtype num_draws = logits = torch tensor device=device logits = logits reshape logits = logits dtype requires_grad_ probs = logits softmax dim=- counts = torch zeros_like logits _ range num_draws y_draw = F gumbel_softmax logits hard=True counts = counts + y_draw All values positive assertGreaterEqual y_draw min Each experiment should result draw assertEqual counts sum num_draws atol=torch finfo counts dtype eps rtol= check results asymptotically expected expected = probs num_draws ~z approximately N unbiased count z = counts - expected expected - probs sqrt A lazy approximate two-sided test occurs prob alpha~ = unbiased assertLess z abs max item _test_gumbel_softmax_grad device dtype hard hard should propagate same gradient logits_soft = torch zeros dtype=dtype device=device requires_grad=True logits_hard = torch zeros dtype=dtype device=device requires_grad=True seed = torch random get_rng_state y_soft = F gumbel_softmax logits_soft hard=False torch random set_rng_state seed y_hard = F gumbel_softmax logits_hard hard=True y_soft sum backward y_hard sum backward eps = x addition + x subtraction tol = torch finfo dtype eps assertEqual logits_soft grad logits_hard grad atol=tol rtol= dtypesIfCUDA torch half torch float torch double dtypesIfMPS torch float dtypes torch float torch double test_gumbel_softmax device dtype _test_gumbel_softmax_st_shapes device dtype shape= dim= count_expected= _test_gumbel_softmax_st_shapes device dtype shape= dim=- count_expected= _test_gumbel_softmax_st_shapes device dtype shape= dim= count_expected= _test_gumbel_softmax_st_shapes device dtype shape= dim= count_expected= _test_gumbel_softmax_st_shapes device dtype shape= dim=- count_expected= _test_gumbel_softmax_straight_through device dtype _test_gumbel_softmax_grad device dtype _test_rnn_retain_variables device dtype rnns = nn LSTM num_layers= device dtype nn GRU num_layers= device dtype nn RNN num_layers= device dtype rnn rnns input = torch randn device=device dtype=dtype requires_grad=True output = rnn input output sum backward retain_graph=True grads = input grad data clone + p grad data clone p rnn parameters _ range rnn zero_grad input grad data zero_ output sum backward retain_graph=True grads = input grad data + p grad data p rnn parameters assertEqual grads grads dtypesIfCUDA torch half torch float torch double dtypesIfMPS torch half torch float dtypes torch double test_rnn_retain_variables device dtype _test_rnn_retain_variables device dtype device_type == cuda has_cudnn torch backends cudnn flags enabled=False _test_rnn_retain_variables device dtype onlyCUDA dtypes torch double test_lstmcell_backward_only_one_output_grad device dtype checks undefined gradients doesn t hamper backward see l = torch nn LSTMCell device dtype=dtype s = torch randn device=device dtype=dtype requires_grad=True i range out = l s i out sum backward assertFalse s grad None s grad abs sum item == _test_rnn_mod mod inp flatten_out mod inp out = mod inp tuple t isinstance t torch Tensor tt t out tt t gradcheckfunc = partial flatten_out mod torch backends cudnn flags enabled=False gradcheck gradcheckfunc inp check_batched_grad=False gradgradcheck gradcheckfunc inp check_batched_grad=False inp is_cuda TEST_WITH_ROCM Assert we have good error message around unsupported CuDNN double backward NB we trigger double backward using backward instead autograd grad due https github com pytorch pytorch issues torch backends cudnn flags enabled=True result = gradcheckfunc inp result sum backward create_graph=True grad = next mod parameters grad assertRaisesRegex RuntimeError please disable CuDNN backend temporarily grad sum backward Here we avoid backward create_graph=True memory leak described https github com pytorch pytorch issues param mod parameters param grad = None inp grad = None Merge into OpInfo skipMeta LSTM cell reuses output which resized expectedFailureMPS TypeError MPS framework doesn t support float dtypes torch double test_LSTM_grad_and_gradgrad device dtype hsize = inp = torch rand hsize device=device dtype=dtype requires_grad=True bias True False mod = torch nn LSTM hsize hsize bias=bias device dtype _test_rnn_mod mod inp skipMeta GRU cell reuses output which resized expectedFailureMPS TypeError MPS framework doesn t support float dtypes torch double test_GRU_grad_and_gradgrad device dtype hsize = inp = torch rand hsize device=device dtype=dtype requires_grad=True bias True False mod = torch nn GRU hsize hsize bias=bias device dtype _test_rnn_mod mod inp skipMeta dtypes torch float torch bfloat onlyCPU test_LSTM_differentiable_backward_using_oneDNN dtype batch = seq_len = input = Net = nn LSTM input batch_first=True copy Net_clone = copy deepcopy Net x = torch rand batch seq_len input x = x clone requires_grad_ True x = x clone requires_grad_ True torch _C _set_mkldnn_enabled False out _ = Net x der_out = torch autograd grad out x grad_outputs=torch ones_like out retain_graph=True create_graph=True loss = der_out sum loss backward retain_graph=True torch _C _set_mkldnn_enabled True out _ = Net x der_out = torch autograd grad out x grad_outputs=torch ones_like out retain_graph=True create_graph=True loss = der_out sum loss backward retain_graph=True assert torch allclose der_out der_out assert torch allclose x grad x grad onlyCUDA test_upsamplingNearest d_launch_config device m = nn Upsample scale_factor= inp = torch rand device=device out = m inp inp_ref = inp cpu out_ref = m inp_ref assertEqual out_ref out onlyCUDA test_upsamplingNearest d_launch_config device m = nn Upsample scale_factor= inp = torch rand device=device out = m inp inp_ref = inp cpu out_ref = m inp_ref assertEqual out_ref out onlyCUDA dtypes torch half torch bfloat test_cudnn_rnn dtype rnn = nn RNN num_layers= device= cuda dtype=dtype input = torch randn device= cuda dtype=dtype hx = torch randn device= cuda dtype=dtype output = rnn input hx output_ref = rnn cpu input cpu hx cpu assertEqual tuple i cuda i output_ref output atol= e- rtol= e- onlyCUDA gcIfJetson test_upsamplingNearest d_launch_config device m = nn Upsample scale_factor= inp = torch rand device=device out = m inp inp_ref = inp cpu out_ref = m inp_ref assertEqual out_ref out unittest expectedFailure skipIfRocm onlyCUDA test_upsamplingNearest d_launch_fail device m = nn Upsample scale_factor= launch grid_y == larger than maximum y-dimension limit inp = torch rand device=device out = m inp onlyCUDA skipCUDAIfNotRocm test_upsamplingNearest d_launch_rocm device test_upsamplingNearest d_launch_fail should run OK ROCm m = nn Upsample scale_factor= inp = torch rand device=device out = m inp onlyCUDA test_CTCLoss_cudnn device _helper zero_infinity target_lengths = input_lengths = targets = torch randint sum target_lengths dtype=torch int log_probs = torch randn dtype=torch float device=device log_softmax requires_grad_ log_probs_ref = log_probs detach clone requires_grad_ torch backends cudnn flags enabled=True res = torch nn functional ctc_loss log_probs targets input_lengths target_lengths zero_infinity=zero_infinity res backward expected = ctcloss_reference log_probs targets cuda input_lengths target_lengths float torch backends cudnn flags enabled=False res = torch nn functional ctc_loss log_probs_ref targets cuda long input_lengths target_lengths zero_infinity=zero_infinity res backward assertEqual res expected assertEqual res res assertEqual log_probs grad log_probs_ref grad _helper zero_infinity=True _helper zero_infinity=False _CTCLoss_gen_losses device input_length vocab_size target_length reduction use_module_form batch_size = log_probs = torch randn input_length batch_size vocab_size dtype=torch float device=device \ log_softmax requires_grad_ targets = torch randint low= high=vocab_size - size= batch_size target_length dtype=torch int device=device input_lengths = batch_size input_length target_lengths = batch_size target_length log_probs_no_bd = log_probs squeeze detach clone requires_grad_ targets_no_bd = targets squeeze detach clone input_lengths_no_bd = torch tensor input_length target_lengths_no_bd = torch tensor target_length currently only length right now left flexible additional potential cases log_probs_refs = log_probs detach clone requires_grad_ _ range log_probs_no_bd_refs = log_probs_no_bd detach clone requires_grad_ _ range losses = losses_no_bd = has_cuda = torch cuda is_available has_cudnn = has_cuda cuda device has_cudnn cudnn requires cpu target has_cuda has_cudnn targets = targets cpu targets_no_bd = targets_no_bd cpu ctc_loss = nn CTCLoss reduction=reduction zero_infinity=True use_module_form partial torch nn functional ctc_loss reduction=reduction zero_infinity=True torch backends cudnn flags enabled=has_cudnn batched case log_probs shape = T N C targets = N S input_lengths target_lengths = N losses append ctc_loss log_probs_refs targets input_lengths target_lengths batched case input shape = T N C targets = S input_lengths target_lengths = N losses append ctc_loss log_probs_refs targets_no_bd input_lengths target_lengths unbatched case input shape = T C targets = S input_lengths target_lengths = N losses_no_bd append ctc_loss log_probs_no_bd_refs targets_no_bd input_lengths_no_bd target_lengths_no_bd loss losses + losses_no_bd loss backward losses losses_no_bd log_probs_refs log_probs_no_bd_refs _assertEqual_list expected list_to_compare atol=None rtol=None ele list_to_compare assertEqual expected ele atol=atol rtol=rtol expectedFailureMPS NotImplementedError aten _ctc_loss https github com pytorch pytorch issues parametrize_test reduction none mean sum parametrize_test use_module_form True False test_CTCLoss_no_batch_dim device reduction use_module_form input_length = vocab_size = target_length = args = _CTCLoss_gen_losses device input_length vocab_size target_length reduction use_module_form losses losses_no_bd log_probs_refs log_probs_no_bd_refs = args test output values _assertEqual_list losses losses atol= e- rtol= _assertEqual_list losses squeeze losses_no_bd atol= e- rtol= test gradient values _assertEqual_list log_probs_refs grad t grad t log_probs_refs atol= e- rtol= _assertEqual_list log_probs_refs grad squeeze t grad t log_probs_no_bd_refs atol= e- rtol= checking output s shape batch dim case should N no batch dim case should _assertEqual_list reduction == none loss shape loss losses _assertEqual_list loss shape loss losses_no_bd checking gradient s shape batch dim case should have shape T N C no batch dim case should have shape T C _assertEqual_list input_length vocab_size t grad shape t log_probs_refs _assertEqual_list input_length vocab_size t grad shape t log_probs_no_bd_refs _ordered_sequence device dtype Create ordered list random sequences seqs = torch empty random randint device=device dtype=dtype _ range seqs = s random_ - s seqs ordered = sorted seqs key=len reverse=True ordered _padded_sequence device dtype Create Tensor random padded sequences ordered = _ordered_sequence device dtype lengths = len i i ordered padded_tensor = rnn_utils pad_sequence ordered padded_tensor lengths onlyCUDA test_device_mask device enforce_sorted True False padded lengths = _padded_sequence cpu torch float packed = rnn_utils pack_padded_sequence padded lengths enforce_sorted=enforce_sorted assertFalse packed is_cuda packed = packed device assertTrue packed is_cuda unpacked _ = rnn_utils pad_packed_sequence packed assertTrue unpacked is_cuda assertEqual unpacked dtype torch float onlyCUDA test_overwrite_module_params_on_conversion_cpu_device device Test under current default settings ` torch __future__ get_overwrite_module_params_on_conversion == False ` view module s parameters pointing same storage its base variable after converting module different device m = nn Linear mw = m weight m device torch no_grad Without using ` torch no_grad ` will leak CUDA memory Issue filed https github com pytorch pytorch issues mw = assertTrue mw device type == cpu assertTrue mw _base device type == cuda try torch __future__ set_overwrite_module_params_on_conversion True Test ` torch __future__ get_overwrite_module_params_on_conversion == True ` view module s parameters still pointing same storage its base variable after converting module different device m = nn Linear mw = m weight m device torch no_grad mw = assertTrue mw == mw _base Test ` torch __future__ get_overwrite_module_params_on_conversion == True ` ` cpu_module cuda ` doesn t preserve previous references ` cpu_module ` s parameters gradients m = nn Linear m weight grad = torch randn weight_ref = m weight weight_grad_ref = m weight grad m device assertNotEqual weight_ref device m weight device assertNotEqual weight_grad_ref device m weight grad device finally torch __future__ set_overwrite_module_params_on_conversion False onlyCUDA dtypes torch half torch float test_softmax device dtype input = torch rand device=device dtype=dtype requires_grad=True inputf = input torch float detach requires_grad_ True out = F softmax input dim=- dtype=torch float outf = F softmax inputf dim=- should bitwise equal assertEqual out outf atol= rtol= gO = torch empty_like outf uniform_ out backward gO outf backward gO should bitwise equal assertEqual input grad inputf grad dtype atol= rtol= _test_batchnorm_grad device dtype=torch double bs n_feat size_feat = input = torch arange bs n_feat size_feat device=device requires_grad=True dtype=dtype view bs n_feat size_feat weight = torch arange n_feat + device=device requires_grad=True dtype=dtype bias = torch arange n_feat device=device requires_grad=True dtype=dtype running_mean = - torch arange n_feat device=device dtype=dtype running_var = torch arange n_feat device=device dtype=dtype training False True _assertGradAndGradgradChecks F batch_norm input running_mean running_var weight bias training expectedFailureMPS TypeError MPS framework doesn t support float test_batchnorm_grad device _test_batchnorm_grad device device_type == cuda has_cudnn torch backends cudnn flags enabled=False _test_batchnorm_grad device onlyCUDA test_layernorm_half_precision width = input = torch rand width device= cuda dtype=torch half normalized_shape = width weight = torch ones width device= cuda dtype=torch half bias = torch zeros width device= cuda dtype=torch half eps = e- output_fp = torch layer_norm input normalized_shape weight bias eps output_fp = torch layer_norm input float normalized_shape weight float bias float eps half assertEqual output_fp output_fp atol= rtol= onlyCUDA test_layernorm_weight_bias width = input = torch rand width device= cuda dtype=torch float normalized_shape = width data = torch randn width device= cuda dtype=torch float weight = torch ones width device= cuda dtype=torch float bias = torch zeros width device= cuda dtype=torch float eps = e- out_none_weight = torch layer_norm input normalized_shape None data eps out_one_weight = torch layer_norm input normalized_shape weight data eps assertEqual out_none_weight out_one_weight out_none_bias = torch layer_norm input normalized_shape data None eps out_zero_bias = torch layer_norm input normalized_shape data bias eps assertEqual out_none_bias out_zero_bias expectedFailureMPS TypeError MPS framework doesn t support float test_hardsigmoid_grad device inputs = torch randn device=device dtype=torch double - inputs requires_grad = True assertTrue gradcheck F hardsigmoid inputs currently fails XLA expectedFailureMPS TypeError MPS framework doesn t support float onlyNativeDeviceTypes test_hardswish_grad device inputs = torch randn device=device dtype=torch double - inputs requires_grad = True assertTrue gradcheck F hardswish inputs _test_hardswish_grad_corner device dtype scalar ref_fn m = nn Hardswish shape = inputs = torch ones shape device=device dtype=dtype inputs = inputs scalar inputs requires_grad = True fwd_result = m inputs fwd_result backward torch ones_like fwd_result ref = ref_fn shape device=device dtype=dtype assertEqual inputs grad ref onlyNativeDeviceTypes dtypes torch half torch bfloat torch float test_hardswish_grad_corner device dtype _test_hardswish_grad_corner device dtype torch ones _test_hardswish_grad_corner device dtype - torch zeros _test_batchnorm_eval ndim device dtype module_dtype=None module_dtype = module_dtype dtype module = nn BatchNorm d device module_dtype module eval data = torch rand ndim device=device dtype=dtype requires_grad=True grad = torch rand ndim device=device dtype=dtype st pass res = module data res backward grad grad = data grad clone nd pass data grad None data grad data zero_ res = module data res backward grad grad = data grad clone assertEqual res res assertEqual grad grad track_running_stats=False module = nn BatchNorm d track_running_stats=False device module_dtype data = torch rand device=device dtype=dtype requires_grad=True grad = torch rand device=device dtype=dtype st pass res = module data res backward grad grad = data grad clone set eval module eval nd pass data grad None data grad data zero_ res = module data res backward grad grad = data grad clone assertEqual res res assertEqual grad grad dtypes torch float dtypesIfCUDA torch float torch bfloat test_batchnorm_eval device dtype _test_batchnorm_eval device dtype _test_batchnorm_eval device dtype device_type == cuda has_cudnn torch backends cudnn flags enabled=False _test_batchnorm_eval device dtype _test_batchnorm_eval device dtype onlyCUDA dtypes torch bfloat torch half test_batchnorm_eval_mixed device dtype Test bfloat input float module _test_batchnorm_eval device dtype torch float _test_batchnorm_eval device dtype torch float device_type == cuda has_cudnn torch backends cudnn flags enabled=False _test_batchnorm_eval device dtype torch float _test_batchnorm_eval device dtype torch float _test_batchnorm_affine ndim device dtype module_dtype=None Compare affine against no-op weights bias module_dtype = module_dtype dtype module = nn BatchNorm d affine=False device module_dtype module_affine = nn BatchNorm d affine=True device module_dtype torch no_grad module_affine weight fill_ module_affine bias zero_ data = torch rand ndim device=device dtype=dtype requires_grad=True grad = torch ones_like data requires_grad=False With weights all ones bias all zeros res = module_affine data res backward grad grad = data grad clone data grad zero_ Without any weights bias res = module data res backward grad grad = data grad assertEqual res res assertEqual grad grad dtypes torch float dtypesIfCUDA torch float torch bfloat test_batchnorm_affine device dtype _test_batchnorm_affine device dtype _test_batchnorm_affine device dtype device_type == cuda has_cudnn torch backends cudnn flags enabled=False _test_batchnorm_affine device dtype _test_batchnorm_affine device dtype onlyCUDA dtypes torch bfloat torch half test_batchnorm_affine_mixed device dtype cudnn_enabled = False device_type == cuda has_cudnn TODO Test fails cudnn see gh- cudnn_enabled = False True pass Test bfloat input float module enabled cudnn_enabled torch backends cudnn flags enabled=enabled _test_batchnorm_affine device dtype torch float _test_batchnorm_affine device dtype torch float _test_batchnorm_simple_average device dtype module_dtype=None module_dtype = module_dtype dtype module = nn BatchNorm d momentum=None dtype=module_dtype device=device zeros = torch zeros dtype=module_dtype device=device ones = torch ones dtype=module_dtype device=device assertEqual module running_mean zeros assertEqual module running_var ones data = torch rand dtype=dtype device=device data = torch rand dtype=dtype device=device st pass res = module data running_mean = module running_mean clone running_var = module running_var clone assertNotEqual running_mean zeros assertNotEqual running_var ones reset stats module reset_running_stats assertEqual module running_mean zeros assertEqual module running_var ones nd pass res = module data running_mean = module running_mean clone running_var = module running_var clone assertNotEqual running_mean zeros assertNotEqual running_var ones reset stats module reset_running_stats assertEqual module running_mean zeros assertEqual module running_var ones rd combined pass res = module data res = module data assertEqual res res assertEqual res res assertEqual module running_mean running_mean + running_mean assertEqual module running_var running_var + running_var dtypes torch float dtypesIfCUDA torch float torch bfloat test_batchnorm_simple_average device dtype _test_batchnorm_simple_average device dtype device_type == cuda has_cudnn torch backends cudnn flags enabled=False _test_batchnorm_simple_average device dtype onlyCUDA dtypes torch bfloat torch half test_batchnorm_simple_average_mixed device dtype _test_batchnorm_simple_average device dtype torch float device_type == cuda has_cudnn torch backends cudnn flags enabled=False _test_batchnorm_simple_average device dtype torch float onlyNativeDeviceTypes expectedFailureMPS Unsupported Border padding mode dtypes torch float torch double test_grid_sample_nan_inf device dtype input = torch zeros device=device dtype=dtype grid = torch tensor nan inf device=device dtype=dtype padding_mode reflection border zeros sample = torch nn functional grid_sample input=input grid=grid mode= nearest padding_mode=padding_mode align_corners=False assertEqual sample torch zeros device=device dtype=dtype expectedFailureMPS NotImplementedError aten _ctc_loss https github com pytorch pytorch issues test_CTCLoss_empty_target device target_lengths = input_lengths = targets = torch randint dtype=torch long device=device log_probs = torch randn dtype=torch double device=device log_softmax loss = torch nn functional ctc_loss log_probs targets input_lengths target_lengths reduction= none assertTrue loss = all item assertEqual -log_probs sum loss target_lengths = input_lengths = targets = torch randint dtype=torch long device=device log_probs = torch randn dtype=torch double device=device log_softmax loss = torch nn functional ctc_loss log_probs targets input_lengths target_lengths reduction= none assertTrue loss = all item assertEqual -log_probs sum loss Merge into OpInfo skipCUDAIf True Test flaky Linux Windows typical error message https github com pytorch pytorch issues expectedFailureMPS NotImplementedError aten _ctc_loss https github com pytorch pytorch issues test_ctc_loss device batch_size = num_labels = target_length = gradcheck_input_size = ZERO_NONE = ZERO_SOME = ZERO_ALL = input_length vary_lengths zero_lengths tests = False ZERO_NONE True ZERO_NONE True ZERO_SOME True ZERO_ALL cuda device tests += False ZERO_NONE True ZERO_NONE True ZERO_SOME True ZERO_ALL input_length vary_lengths zero_mode tests targets = torch randint num_labels batch_size target_length device=device dtype=torch long x = torch randn gradcheck_input_size dtype=torch double device=device requires_grad=True tile_factors = torch randn input_length batch_size num_labels gradcheck_input_size + device=device input_lengths = torch randint input_length input_length + item vary_lengths i == input_length i range batch_size zero_mode == ZERO_ALL target_lengths = _ range batch_size target_lengths = torch randint target_length target_length + item vary_lengths target_length _ range batch_size zero_mode == ZERO_SOME idxes = torch randint batch_size i idxes target_lengths i = ctc_after_softmax x x_full = x None tile_factors None view - input_length batch_size num_labels view input_length batch_size num_labels log_probs = torch log_softmax x_full torch nn functional ctc_loss log_probs targets input_lengths target_lengths gradcheck ctc_after_softmax x onlyCUDA skipCUDAIfRocm msg= skipped Cudnn test ROCm test_ctc_loss_cudnn device batch_size = input_length = num_labels = target_length = targets = torch randint num_labels batch_size target_length device= cuda dtype=torch long log_probs = torch log_softmax torch randn input_length batch_size num_labels device= cuda dtype=torch float log_probs requires_grad_ input_lengths = batch_size input_length target_lengths = batch_size target_length grad_out = torch randn batch_size device= cuda dtype=torch float torch backends cudnn flags enabled=False loss_native = torch nn functional ctc_loss log_probs targets input_lengths target_lengths reduction= none grad_native = torch autograd grad loss_native log_probs grad_out loss_cudnn = torch nn functional ctc_loss log_probs targets cpu torch int input_lengths target_lengths reduction= none assertTrue Cudnn str loss_cudnn grad_fn grad_cudnn = torch autograd grad loss_cudnn log_probs grad_out assertEqual grad_cudnn grad_native atol= e- rtol= onlyCUDA skipCUDAIfRocm msg= skipped Cudnn test ROCm test_ctc_loss_cudnn_tensor_cuda batch_size = input_length = num_labels = target_length = targets = torch randint num_labels batch_size target_length device= cuda dtype=torch long log_probs = torch log_softmax torch randn input_length batch_size num_labels device= cuda dtype=torch float log_probs requires_grad_ input_lengths = batch_size input_length input_lengths = torch linspace start= end=input_length steps=batch_size dtype=torch long device= cuda target_lengths = torch tensor batch_size target_length dtype=torch long device= cuda grad_out = torch randn batch_size device= cuda dtype=torch float torch backends cudnn flags enabled=False loss_native = torch nn functional ctc_loss log_probs targets input_lengths target_lengths reduction= none grad_native = torch autograd grad loss_native log_probs grad_out loss_cudnn = torch nn functional ctc_loss log_probs targets cuda torch int input_lengths cuda torch int target_lengths cuda torch int reduction= none assertTrue Cudnn str loss_cudnn grad_fn grad_cudnn = torch autograd grad loss_cudnn log_probs grad_out assertEqual grad_cudnn grad_native atol= e- rtol= onlyCUDA skipCUDAIfRocm msg= skipped Cudnn test ROCm test_ctc_loss_cudnn_tensor_cpu_length_cuda batch size N = audio length T = text dimension C = max text length S = prob_device = torch device cuda other_device = torch device cpu other_dtype = torch int log_probs = torch randn T N C log_softmax prob_device input_lengths = torch full N T dtype=other_dtype other_device target_lengths = torch randint low= high=S size= N dtype=other_dtype other_device targets = torch randint low= high=C size= sum target_lengths dtype=other_dtype other_device ctc_loss = torch nn functional ctc_loss log_probs=log_probs targets=targets input_lengths=input_lengths target_lengths=target_lengths reduction= sum expectedFailureMPS test_ctc_loss_error device log_probs = torch rand device=device targets = torch tensor device=device dtype=torch long input_lengths = torch tensor device=device dtype=torch long target_lengths = torch tensor device=device dtype=torch long assertRaisesRegex RuntimeError log_probs tensor must empty F ctc_loss log_probs targets input_lengths target_lengths reduction= none skipIfRocmArch MI _ARCH expectedFailureMPS RuntimeError LSTM projections currently supported MPS dtypesIfCUDA torch half torch float torch double dtypes torch float tf _on_and_off skipIfTorchDynamo TorchDynamo fails here unknown reasons test_variable_sequence device dtype pad var length var size == length var torch cat var var new_zeros length - var size var size maybe_index_tuple maybe_tuple_of_tensors index maybe_tuple_of_tensors None None tuple maybe_tuple_of_tensors j index index + contiguous j range check_lengths lengths enforce_sorted use_default_hiddens proj_size input_size = hidden_size = num_layers = bidirectional = True max_length = max lengths x_leaf = torch randn max_length len lengths input_size device=device dtype=dtype requires_grad=True num_directions = bidirectional lstm = nn LSTM input_size hidden_size bidirectional=bidirectional num_layers=num_layers proj_size=proj_size device dtype lstm = deepcopy lstm device dtype x = x_leaf hidden = None use_default_hiddens real_hidden_size = hidden_size proj_size == proj_size hidden = torch randn num_directions num_layers len lengths real_hidden_size device=device dtype=dtype torch randn num_directions num_layers len lengths hidden_size device=device dtype=dtype Compute sequences separately seq_outs = seq_hiddens = i l enumerate lengths hidden_i = maybe_index_tuple hidden i out hid = lstm x l i i + hidden_i out_pad = pad out max_length seq_outs append out_pad seq_hiddens append hid seq_out = torch cat seq_outs seq_hidden = tuple torch cat hids hids zip seq_hiddens Use packed format packed = rnn_utils pack_padded_sequence x lengths enforce_sorted=enforce_sorted packed_out packed_hidden = lstm packed hidden unpacked unpacked_len = rnn_utils pad_packed_sequence packed_out Check forward prec = dtype prec_DONTUSE dtype assertEqual packed_hidden seq_hidden atol=prec rtol= assertEqual unpacked seq_out atol=prec rtol= assertEqual unpacked_len lengths atol=prec rtol= Check backward seq_out sum backward grad_x = x_leaf grad data clone x_leaf grad data zero_ unpacked sum backward assertEqual x_leaf grad grad_x atol=dtype prec_DONTUSE dtype rtol= p p zip lstm parameters lstm parameters prec = dtype prec_DONTUSE dtype dtype == torch float prec = e- dtype == torch float prec = e- assertEqual p grad p grad atol=prec rtol= tests = enforce_sorted lengths True False True False False enforce_sorted seq_lens tests use_default_hiddens True False proj_size check_lengths seq_lens enforce_sorted use_default_hiddens proj_size _test_batchnorm_update_stats device dtype=torch float module = nn BatchNorm d device dtype data = torch rand device=device dtype=dtype training pass old_running_mean = module running_mean clone old_running_var = module running_var clone old_num_batches_tracked = module num_batches_tracked clone module data assertNotEqual old_running_mean module running_mean assertNotEqual old_running_var module running_var assertEqual old_num_batches_tracked + module num_batches_tracked eval pass module eval old_running_mean = module running_mean clone old_running_var = module running_var clone old_num_batches_tracked = module num_batches_tracked clone module data assertEqual old_running_mean module running_mean assertEqual old_running_var module running_var assertEqual old_num_batches_tracked module num_batches_tracked test_batchnorm_update_stats device _test_batchnorm_update_stats device device_type == cuda has_cudnn torch backends cudnn flags enabled=False _test_batchnorm_update_stats device onlyCPU dtypes torch bfloat torch float test_activations_bfloat _half_cpu device dtype test_helper fn device inp_dims prec=None torch manual_seed bfloat half compute fn = fn dtype=dtype input = torch randn inp_dims dtype=dtype device=device requires_grad=True out = fn input grad_input = torch randn_like out dtype=dtype device=device out backward grad_input fp compute input = input detach clone float requires_grad_ True out = fn float input grad_input = grad_input detach clone float out backward grad_input assertEqual out dtype dtype assertEqual input grad dtype dtype assertEqual out out dtype=dtype atol=prec rtol=prec assertEqual input grad data input grad data dtype=dtype atol=prec rtol=prec shapes = shape shapes test_helper torch nn LogSigmoid device shape test_helper torch nn Hardsigmoid device shape test_helper torch nn Hardshrink device shape test_helper torch nn Softshrink device shape test_helper torch nn Hardswish device shape test_helper torch nn Softplus device shape test_helper torch nn SiLU device shape test_helper torch nn Hardtanh device shape test_helper torch nn Mish device shape test_helper torch nn ELU device shape test_helper torch nn PReLU device shape test_helper torch nn GLU device shape prec= e- test_helper torch nn Threshold device shape test_helper torch nn GELU device shape test_helper torch nn Hardtanh device shape test_helper torch nn LeakyReLU device shape onlyCUDA test_activations_bfloat device _test_bfloat _ops torch nn ReLU device inp_dims= prec= e- _test_bfloat _ops torch nn Threshold device inp_dims= prec= e- _test_bfloat _ops torch nn ELU device inp_dims= prec= e- _test_bfloat _ops torch nn Softplus device inp_dims= prec= e- _test_bfloat _ops torch nn Hardshrink device inp_dims= prec= e- _test_bfloat _ops torch nn Softshrink device inp_dims= prec= e- _test_bfloat _ops torch nn LeakyReLU device inp_dims= prec= e- onlyNativeDeviceTypes test_softmax_bfloat device dim _test_bfloat _ops torch nn Softmax dim=dim device inp_dims= prec= e- test softmax large input value which causes exp overflow _test_bfloat _ops torch nn Softmax dim=dim device inp_dims= prec= scale_factor= test_nll_loss_ d_input_ d_target_invalid_size device x = torch randn device=device t = torch randint dtype=torch int device=device assertRaisesRegex ValueError For D input D target must have size F nll_loss x t test_nll_loss_mismatched_batch device x = torch randn requires_grad=True device=device t should have size t = torch zeros dtype=torch int device=device assertRaisesRegex ValueError Expected batch_size F nll_loss x t test_nll_loss_out_of_bounds_ignore_index device x = torch randn requires_grad=True device=device t = torch tensor dtype=torch int device=device reduction mean none F nll_loss x t ignore_index= reduction=reduction sum backward test_nll_loss_invalid_target_dim device x = torch randn device=device t = torch zeros dtype=torch int device=device assertRaisesRegex RuntimeError D target tensor expected F nll_loss x t test_nll_loss_invalid_weights device x = torch randn device=device t = torch empty dtype=torch int device=device random_ invalid_weights = torch randn device=device torch randn device=device msg = weight tensor should defined either all classes no classes weight invalid_weights assertRaisesRegex RuntimeError msg F nll_loss x t weight=weight Ref https github com pytorch pytorch issues onlyCUDA largeTensorTest GB cpu largeTensorTest GB cuda parametrize_test reduction none mean sum test_nll_loss_large_tensor device reduction shape = int int + input = torch randn shape device=device dtype=torch float requires_grad=True labels = torch randint shape shape dtype=torch long device=device out = F nll_loss input labels reduction=reduction torch no_grad input_cpu = input cpu float requires_grad_ labels_cpu = labels cpu out_cpu = F nll_loss input_cpu labels_cpu reduction=reduction workaround reduce memory usage vs assertEqual see rtol atol = torch testing _comparison get_tolerances torch float rtol=None atol=None reduction == sum orig_rtol orig_atol = rtol atol rtol atol = rtol atol torch no_grad assertTrue torch allclose out cpu out_cpu rtol=rtol atol=atol reduction == sum rtol atol = orig_rtol orig_atol reduction = none out backward out_cpu backward torch no_grad assertTrue torch allclose input grad cpu input_cpu grad rtol=rtol atol=atol Ref https github com pytorch pytorch issues onlyCUDA largeTensorTest GB cpu largeTensorTest GB cuda parametrize_test reduction none mean sum test_cross_entropy_ bit device reduction labels = torch zeros dtype=torch long device=device logits = torch ones dtype=torch float device=device loss = torch nn functional cross_entropy logits labels loss_cpu = torch nn functional cross_entropy logits cpu labels cpu print logits numel labels numel loss numel assertTrue torch allclose loss_cpu loss cpu rtol= e- atol= e- _nll_loss_helper input_size reduction expected device dtype input = torch rand input_size requires_grad=True device=device dtype=dtype num_channels = input_size target_size = input_size + tuple input_size target = torch randint num_channels target_size device=device output = F nll_loss input target reduction=reduction assertEqual output expected exact_dtype=False output sum backward assertEqual input grad size input size dtypesIfMPS torch half torch float dtypes torch float test_nll_loss_empty_tensor_reduction_none device dtype _nll_loss_helper none torch empty device=device device dtype _nll_loss_helper none torch empty device=device device dtype _nll_loss_helper none torch empty device=device device dtype _nll_loss_helper none torch empty device=device device dtype _nll_loss_helper none torch empty device=device device dtype dtypesIfMPS torch half torch float dtypes torch float test_nll_loss_empty_tensor_reduction_mean device dtype nan = torch tensor float nan device=device _nll_loss_helper mean nan device dtype _nll_loss_helper mean nan device dtype _nll_loss_helper mean nan device dtype _nll_loss_helper mean nan device dtype _nll_loss_helper mean nan device dtype dtypesIfMPS torch half torch float dtypes torch float test_nll_loss_empty_tensor_reduction_sum device dtype zero = torch tensor device=device _nll_loss_helper sum zero device dtype _nll_loss_helper sum zero device dtype _nll_loss_helper sum zero device dtype _nll_loss_helper sum zero device dtype _nll_loss_helper sum zero device dtype test_nll_loss_total_weight_is_zero device helper input_size input = torch ones input_size requires_grad=True device=device num_channels = input_size target_size = input_size + tuple input_size target = torch zeros target_size dtype=torch long device=device weight = torch zeros num_channels device=device assertEqual F nll_loss input target weight reduction= sum item assertEqual F nll_loss input target weight reduction= mean item float nan assertEqual F nll_loss input target weight reduction= none torch zeros target shape device=device helper helper helper test_nll_loss_all_ignored device helper input_size input = torch ones input_size device=device num_channels = input_size target_size = input_size + tuple input_size target = torch zeros target_size dtype=torch long device=device assertEqual F nll_loss input target ignore_index= reduction= sum item assertEqual F nll_loss input target ignore_index= reduction= mean item float nan assertEqual F nll_loss input target ignore_index= reduction= none torch zeros target shape device=device helper helper helper test_nll_loss_byte_target_matches_long device N C = input = torch randn N C device=device requires_grad=True target = torch empty N dtype=torch long device=device random_ C compute_result_and_gradient reduction target_dtype input_ = input detach input_ requires_grad_ prob = F log_softmax input_ dim=- loss = nn NLLLoss reduction=reduction result = loss prob target target_dtype result sum backward result input_ grad reduction none mean sum result_long grad_long = compute_result_and_gradient reduction torch long result_byte grad_byte = compute_result_and_gradient reduction torch uint assertEqual result_long result_byte assertEqual grad_long grad_byte onlyCUDA skipIfRocm dtypes torch float torch float test_cross_entropy_loss_ d_out_of_bounds_class_index device dtype Test issue Run different process prevent device-side assert affecting other tests stderr = TestCase runWithPytorchAPIUsageStderr f \ usr bin env python torch torch nn functional F torch testing _internal common_utils run_tests TestCase TestThatContainsCUDAAssert TestCase test_cross_entropy_loss_ d_out_of_bounds_class_index device = str device dtype = str dtype strip ignore_index = b = n_classes = w = h = pred = torch randn b n_classes w h dtype=dtype device=device labels = torch zeros b w h dtype=torch int device=device labels = ignore_index Set invalid index labels = x = F cross_entropy pred labels reduction= none ignore_index=ignore_index torch cuda synchronize __name__ == __main__ run_tests assertIn CUDA error device-side assert triggered stderr test_cross_entropy_loss_prob_target_all_reductions device Test k-dimensional loss k range N C = other_dims = torch randint size= item _ range k input = torch randn N C other_dims device=device requires_grad=True target = torch randn N C other_dims device=device requires_grad=True weight = torch randn C device=device abs reduction w product none mean sum None weight m = torch nn CrossEntropyLoss weight=w reduction=reduction output = m input target output_ref = loss_reference_fns CrossEntropyLoss input target reduction=reduction weight=w assertEqual output output_ref test_cross_entropy_loss_prob_target_unit_weights device Test k-dimensional loss k range N C = other_dims = torch randint size= item _ range k input = torch randn N C other_dims device=device requires_grad=True target = torch randn N C other_dims device=device requires_grad=True reduction none mean sum Ensure result unit weights equivalent result without weights m = torch nn CrossEntropyLoss reduction=reduction unit_weight = torch ones C device=device dtype=target dtype m_unit = torch nn CrossEntropyLoss weight=unit_weight reduction=reduction output = m input target output_unit = m_unit input target assertEqual output output_unit parametrize_test reduction none mean sum parametrize_test weighted False True test_cross_entropy_loss_prob_target_no_batch_dim device reduction weighted C = input = torch randn C device=device log_softmax dim=- target = torch randn C device=device softmax dim=- weight = torch randn C device=device weighted None m = nn CrossEntropyLoss reduction=reduction weight=weight loss_no_batch = m input target loss_batch = m input unsqueeze target unsqueeze reduction == none loss_batch = loss_batch squeeze assertEqual loss_no_batch loss_batch test_cross_entropy_loss_index_target_unit_weights device Test k-dimensional loss k range N C = other_dims = torch randint size= item _ range k input = torch randn N C other_dims device=device requires_grad=True target = torch empty N other_dims dtype=torch long device=device random_ C reduction none mean sum Ensure result unit weights equivalent result without weights m = torch nn CrossEntropyLoss reduction=reduction unit_weight = torch ones C device=device dtype=input dtype m_unit = torch nn CrossEntropyLoss weight=unit_weight reduction=reduction output = m input target output_unit = m_unit input target assertEqual output output_unit test_cross_entropy_loss_one_hot_target device Test k-dimensional loss k range N C = other_dims = torch randint size= item _ range k input = torch randn N C other_dims device=device requires_grad=True target = torch empty N other_dims dtype=torch long device=device random_ C weight = torch randn C device=device abs Get one-hot representation target target_one_hot = F one_hot target num_classes=C input dtype Need put C dim index target_one_hot = target_one_hot permute - range target_one_hot dim - reduction w product none mean sum None weight Skip case now because soft hard label CE consistent way they apply weights see issue reduction == mean weight None continue Ensure loss computed indices matches loss computed one-hot probs m = torch nn CrossEntropyLoss weight=w reduction=reduction output = m input target output_one_hot = m input target_one_hot assertEqual output output_one_hot test_cross_entropy_label_smoothing_errors device N C = input_args = torch randn N C device=device torch arange C device=device torch randn N C device=device torch randn N C device=device input_arg input_args loss = nn CrossEntropyLoss label_smoothing= assertRaisesRegex RuntimeError r label_smoothing must between \ loss input_arg expectedFailureMPS TypeError MPS framework doesn t support float set_default_dtype torch double test_cross_entropy_label_smoothing_consistent_index_target_and_probs device N C = ks = range reductions = none mean sum label_smoothings = k reduction label_smoothing product ks reductions label_smoothings other_dims = torch randint size= item _ range k input = torch randn N C other_dims device=device requires_grad=True target = torch empty N other_dims dtype=torch long device=device random_ C construct target probability should have same result label_smoothing target_proba = F one_hot target num_classes=C Need put C dim index target_proba = target_proba permute - range target_proba dim - target_mask = target_proba == target_proba = target_proba dtype=input dtype y_k^ls = y_k - label_smoothing + label_smoothing n_classes Get one-hot representation target target_proba masked_fill_ target_mask - label_smoothing + label_smoothing C target_proba masked_fill_ ~target_mask label_smoothing C loss = nn CrossEntropyLoss reduction=reduction output_with_prob = loss input target_proba loss = nn CrossEntropyLoss reduction=reduction label_smoothing=label_smoothing output_with_index = loss input target assertEqual output_with_prob output_with_index rtol= e- atol= e- test_cross_entropy_label_smoothing_with_probs device N C = ks = range reductions = none mean sum label_smoothings = Test k-dimensional loss k label_smoothing product ks label_smoothings other_dims = torch randint size= item _ range k input = torch randn N C other_dims device=device requires_grad=True target = F log_softmax torch randn N C other_dims device=device dim= reduction reductions use label_smoothing loss = nn CrossEntropyLoss reduction=reduction label_smoothing=label_smoothing output_with_smoothing = loss input target manually smoothing target class_proba^ls = class_proba - label_smoothing + label_smoothing n_classes target_with_smoothing = target - label_smoothing + label_smoothing C loss = nn CrossEntropyLoss reduction=reduction output_with_manual_smoothing = loss input target_with_smoothing assertEqual output_with_smoothing output_with_manual_smoothing test_cross_entropy_label_smoothing_weight_ignore_indices device reductions = none sum mean label_smoothings = wgt = torch tensor device=device inp = torch tensor device=device inp = torch tensor device=device targ_default_ignore_index = torch tensor - device=device targ_negative_ignore_index = torch tensor - device=device targ_positive_ignore_index = torch tensor device=device reduction label_smoothing weight product reductions label_smoothings None wgt check_equal loss inp_targ_ inp_targ_ inp targ = inp_targ_ inp targ = inp_targ_ l = loss inp targ l = loss inp targ assertEqual l l Default ignore_index loss = nn CrossEntropyLoss reduction=reduction label_smoothing=label_smoothing weight=weight check_equal loss inp targ_default_ignore_index inp targ_default_ignore_index reduction = none Check we correctly tally denominator ` mean ` i e we don t count ignored_idx all check_equal loss inp targ_default_ignore_index inp targ_default_ignore_index negative ignore_index loss = nn CrossEntropyLoss reduction=reduction label_smoothing=label_smoothing ignore_index=- weight=weight check_equal loss inp targ_negative_ignore_index inp targ_negative_ignore_index reduction = none Check we correctly tally denominator ` mean ` i e we don t count ignored_idx all check_equal loss inp targ_negative_ignore_index inp targ_negative_ignore_index positive ignore_index loss = nn CrossEntropyLoss reduction=reduction label_smoothing=label_smoothing ignore_index= weight=weight check_equal loss inp targ_positive_ignore_index inp targ_positive_ignore_index reduction = none Check we correctly tally denominator ` mean ` i e we don t count ignored_idx all check_equal loss inp targ_positive_ignore_index inp targ_positive_ignore_index Ref https github com pytorch pytorch issues onlyCUDA largeTensorTest GB cpu largeTensorTest GB cuda parametrize_test reduction none mean sum test_cross_entropy_large_tensor device reduction logits = torch randn int int + dtype=torch float device= cuda requires_grad=True labels = torch zeros logits size dtype=torch long device= cuda loss = F cross_entropy logits labels reduction=reduction reduction = none loss backward torch no_grad logits_cpu = logits cpu detach requires_grad_ labels_cpu = labels cpu detach loss_cpu = F cross_entropy logits_cpu labels_cpu reduction=reduction reduction = none loss_cpu backward workaround reduce memory usage vs assertEqual see rtol atol = torch testing _comparison get_tolerances torch float rtol=None atol=None assertTrue torch allclose loss cpu loss_cpu rtol=rtol atol=atol reduction = none assertTrue torch allclose logits grad cpu logits_cpu grad rtol=rtol atol=atol test_smoothl loss_backward_zero_beta device input = torch randn requires_grad=True device=device target = input detach loss = F smooth_l _loss input target beta= reduction= sum loss backward grad_max_abs = input grad abs max item assertLessEqual grad_max_abs test_softshrink_negative device input = torch randn device=device requires_grad=True m = torch nn Softshrink - assertRaisesRegex RuntimeError r lambda must greater equal found - \ m input expectedFailureMPS TypeError MPS framework doesn t support float test_fold device test_dtype fn input dtype input = input detach clone dtype=dtype requires_grad_ True input = input detach clone float requires_grad_ True out = fn input out sum backward out = fn input out sum backward assertEqual out dtype dtype assertEqual input grad dtype dtype assertEqual out out dtype=dtype atol= rtol= assertEqual input grad input grad dtype=dtype func x F fold x output_size= kernel_size= seeds = sd seeds torch manual_seed sd x = torch randn device=device requires_grad=True dtype=torch double gradcheck func x check_forward_ad=True gradgradcheck func x check_fwd_over_rev=True device == cpu test_dtype func x torch bfloat test_logsigmoid_out device isn t actually documented broken previously https github com pytorch pytorch issues x = torch randn device=device t empty_out = torch randn device=device assertEqual F logsigmoid x F logsigmoid x out=empty_out noncontig_out = torch randn device=device t assertEqual F logsigmoid x F logsigmoid x out=noncontig_out Check clip_grad_norm_ raises error total norm parameters gradients non-finite expectedFailureMPS TypeError MPS framework doesn t support float test_clip_grad_norm_error_if_nonfinite device norms_pos = inf norms_neg = - - - - norms_except_ = norms_pos + norms_neg norms_all = norms_except_ + Each entry test_cases has following values order grad_only_one_elem If True only one element parameter s gradient set scalar grad rest elements If False all grad elements equal scalar prefix_finite_grad_param If True prefix parameter has grad scalars Scalars use parameter s grad through multiplication norms_nonfinite Norm types should produce nonfinite total norm norms_finite Norm types should produce finite total norm test_cases = Test errors infinite grad False False inf -inf norms_except_ False True inf -inf norms_pos norms_neg + True False inf -inf norms_pos norms_neg + True True inf -inf norms_pos norms_neg + Test errors NaN grad False False nan norms_except_ False True nan norms_except_ True False nan norms_except_ True True nan norms_except_ Test grad should never error False False e - e norms_all False True e - e norms_all True False e - e norms_all True True e - e norms_all Test grad will overflow inf only some norm orders False False e - e - - inf - - False True e - e norms_neg + inf True False e - e norms_neg + inf True True e - e norms_neg + inf gen_parameters scalar grad_only_one_elem prefix_finite_grad_param param = torch ones dtype=torch float device=device requires_grad=True grad_only_one_elem param mul scalar sum backward param mul scalar sum backward prefix_finite_grad_param prefix_param = torch ones dtype=torch float device=device requires_grad=True prefix_param mul sum backward parameters = prefix_param param parameters = param parameters run_test_case norm_type error_if_nonfinite scalar grad_only_one_elem prefix_finite_grad_param is_norm_nonfinite msg = f norm_type norm_type f error_if_nonfinite error_if_nonfinite f scalar scalar f grad_only_one_elem grad_only_one_elem f prefix_finite_grad_param prefix_finite_grad_param f is_norm_nonfinite is_norm_nonfinite parameters = gen_parameters scalar grad_only_one_elem prefix_finite_grad_param Should only throw error total norm expected nonfinite ` error_if_nonfinite=True ` is_norm_nonfinite error_if_nonfinite error_msg = f The total norm order float norm_type gradients grads_before = p grad clone p parameters assertRaisesRegex RuntimeError error_msg msg=msg clip_grad_norm_ parameters norm_type=norm_type error_if_nonfinite=True Grad should change error thrown grads_after = p grad p parameters assertEqual grads_before grads_after msg=msg clip_grad_norm_ parameters norm_type=norm_type error_if_nonfinite=error_if_nonfinite grad_only_one_elem prefix_finite_grad_param scalars norms_nonfinite norms_finite test_cases error_if_nonfinite False True norm_type scalar product norms_nonfinite scalars run_test_case norm_type error_if_nonfinite scalar grad_only_one_elem prefix_finite_grad_param True norm_type scalar product norms_finite scalars run_test_case norm_type error_if_nonfinite scalar grad_only_one_elem prefix_finite_grad_param False onlyCUDA deviceCountAtLeast parametrize_test foreach False True test_clip_grad_norm_multi_device devices foreach TestModel nn Module __init__ - None super __init__ layer = nn Linear layer = nn Linear test_model = TestModel test_model layer devices test_model layer devices ref_model = TestModel devices norm_type math inf p test_model parameters p grad = torch ones_like p p ref_model parameters p grad = torch ones_like p norm = clip_grad_norm_ test_model parameters norm_type=norm_type foreach=foreach expected = clip_grad_norm_ ref_model parameters norm_type=norm_type foreach=foreach assertEqual norm expected p pe zip test_model parameters ref_model parameters assertEqual p grad devices pe grad test_elu_inplace_overlap device dtype = torch bfloat device = mps torch float x = torch randn dtype=dtype device=device expand assertRaisesRegex RuntimeError unsupported operation F elu x inplace=True assertRaisesRegex RuntimeError unsupported operation F elu_ x Merge into OpInfo onlyNativeDeviceTypes test_elu_inplace_with_neg_alpha device = torch tensor - device=device requires_grad=True b = torch nn functional elu_ clone alpha=- assertRaisesRegex RuntimeError call out-of-place version b backward torch ones device=device = torch tensor - device=device requires_grad=True b = torch nn functional celu_ clone alpha=- assertRaisesRegex RuntimeError call out-of-place version b backward torch ones device=device expectedFailureMeta https github com pytorch pytorch issues test_hardswish_inplace_overlap device x = torch randn device=device expand assertRaisesRegex RuntimeError unsupported operation F hardswish x inplace=True test_silu_inplace_overlap device x = torch randn device=device expand assertRaisesRegex RuntimeError unsupported operation F silu x inplace=True onlyNativeDeviceTypes test_mish_inplace_overlap device x = torch randn device=device expand assertRaisesRegex RuntimeError unsupported operation F mish x inplace=True test_softplus_inplace_overlap device x = torch randn device=device expand assertRaisesRegex RuntimeError unsupported operation F softplus x out=x expectedFailureMPS TypeError MPS framework doesn t support float test_softplus_low_threshold device Ensure gradients computed correctly low threshold model = torch nn Softplus threshold= double input = torch tensor device=device dtype=torch double requires_grad=True output = model input torch autograd gradcheck model input test_softshrink_inplace_overlap device x = torch randn device=device expand assertRaisesRegex RuntimeError unsupported operation F softshrink x out=x test_leaky_relu_inplace_overlap device x = torch randn device=device expand assertRaisesRegex RuntimeError unsupported operation F leaky_relu x inplace=True assertRaisesRegex RuntimeError unsupported operation F leaky_relu_ x Merge into OpInfo expectedFailureMPS NotImplementedError aten rrelu_with_noise_ https github com pytorch pytorch issues test_leaky_relu_inplace_with_neg_slope device = torch tensor - device=device requires_grad=True b = torch nn functional leaky_relu_ clone - assertRaisesRegex RuntimeError call out-of-place version b backward torch ones device=device = torch tensor - device=device requires_grad=True b = torch nn functional rrelu_ clone - assertRaisesRegex RuntimeError call out-of-place version b backward torch ones device=device Merge into OpInfo test_leaky_relu_inplace_with_zero_slope device = torch tensor - device=device requires_grad=True b = torch nn functional leaky_relu_ clone b backward torch ones device=device expected = torch tensor device=device assertEqual grad expected dtype = torch bfloat device = mps torch float a_bf = torch tensor - device=device dtype=dtype requires_grad=True b_bf = torch nn functional leaky_relu_ a_bf clone b_bf backward torch ones device=device expected_bf = torch tensor device=device dtype=dtype assertEqual a_bf grad expected_bf onlyCPU test_rrelu_bounds_validation device Test RReLU bounds validation finite infinite values x = torch randn device=device Test finite bounds result = F rrelu x lower= upper= assertEqual result shape x shape Test infinite lower bound assertRaisesRegex RuntimeError rrelu lower bound must finite got inf F rrelu x lower=float inf upper= Test infinite upper bound assertRaisesRegex RuntimeError rrelu upper bound must finite got inf F rrelu x lower= upper=float inf Test NaN lower bound assertRaisesRegex RuntimeError rrelu lower bound must finite got nan F rrelu x lower=float nan upper= Test NaN upper bound assertRaisesRegex RuntimeError rrelu upper bound must finite got nan F rrelu x lower= upper=float nan Test negative infinity lower bound assertRaisesRegex RuntimeError rrelu lower bound must finite got -inf F rrelu x lower=float -inf upper= Test negative infinity upper bound assertRaisesRegex RuntimeError rrelu upper bound must finite got -inf F rrelu x lower= upper=float -inf Test lower bound greater than upper bound assertRaisesRegex RuntimeError Lower bound should less than equal upper bound F rrelu x lower= upper= onlyCPU test_softshrink device x = torch tensor - - - - - - - - - - - - - - - - float nan - - - - - - - - - - - - - - - - - float nan device=device expected = torch tensor - - - - - - - float nan - - - - - - - - - - - float nan softshrink = torch nn Softshrink out = softshrink x assertEqual out expected atol= e- rtol= test_threshold_inplace_overlap device Inplace threshold okay because idempotent x = torch randn device=device expand F threshold x inplace=True F threshold_ x expectedFailureMPS Double unsupported onlyNativeDeviceTypes test_triplet_margin_with_distance_loss_default_parity device Test ` nn TripletMarginWithDistanceLoss ` ` F triplet_margin_with_distance_loss ` Checks parity against respective non-distance-agnostic implementations triplet margin loss ` ` nn TripletMarginLoss ` ` F triplet_margin_loss ` under default args extra_args \ itertools product True False none mean sum kwargs = margin extra_args swap extra_args reduction extra_args anchor = torch randn device=device requires_grad=True dtype=torch double positive = torch randn device=device requires_grad=True dtype=torch double negative = torch randn device=device requires_grad=True dtype=torch double Test forward functional expected = F triplet_margin_loss anchor positive negative kwargs actual = F triplet_margin_with_distance_loss anchor positive negative kwargs assertEqual actual expected rtol= e- atol= e- Test forward module loss_ref = nn TripletMarginLoss kwargs loss_op = nn TripletMarginWithDistanceLoss kwargs assertEqual loss_op anchor positive negative loss_ref anchor positive negative rtol= e- atol= e- Test backward assertTrue gradcheck lambda p n F triplet_margin_with_distance_loss p n kwargs anchor positive negative assertTrue gradcheck lambda p n loss_op p n anchor positive negative expectedFailureMPS Double unsupported onlyNativeDeviceTypes test_triplet_margin_with_distance_loss device Test parity between ` nn TripletMarginWithDistanceLoss ` ` F triplet_margin_with_distance_loss ` pairwise_distance = nn PairwiseDistance cosine_distance x y - F cosine_similarity x y distance_functions = pairwise_distance cosine_distance lambda x y - F cosine_similarity x y reductions = mean none sum margins = swaps = True False distance_fn reduction margin swap \ itertools product distance_functions reductions margins swaps anchor = torch randn device=device requires_grad=True dtype=torch double positive = torch randn device=device requires_grad=True dtype=torch double negative = torch randn device=device requires_grad=True dtype=torch double Test backward assertTrue gradcheck lambda p n F triplet_margin_with_distance_loss p n distance_function=distance_fn reduction=reduction margin=margin swap=swap anchor positive negative loss_op = nn TripletMarginWithDistanceLoss distance_function=distance_fn reduction=reduction margin=margin swap=swap assertTrue gradcheck lambda p n loss_op p n anchor positive negative traced_loss_op = torch jit trace loss_op anchor positive negative assertTrue gradcheck lambda p n traced_loss_op p n anchor positive negative Test forward parity functional = F triplet_margin_with_distance_loss anchor positive negative distance_function=distance_fn reduction=reduction margin=margin swap=swap modular = loss_op anchor positive negative traced = traced_loss_op anchor positive negative assertEqual functional modular atol= e- rtol= e- assertEqual traced modular atol= e- rtol= e- dtypesIfMPS torch cfloat torch float dtypes torch cfloat torch cdouble torch float test_to_complex device dtype m = nn Linear device assertIs m m device m dtype assertIs m weight dtype dtype warnings catch_warnings record=True w Trigger warning m torch cfloat Check warning occurs assertEqual len w assertTrue Complex modules new feature str w - message skipMeta dtypesIfMPS torch float dtypes torch float torch float test_module_to_empty device dtype MyModule nn Module __init__ in_features out_features device=None dtype=None super __init__ factory_kwargs = device device dtype dtype weight = nn Parameter torch randn in_features out_features factory_kwargs forward x x weight Test meta module instantiation input = torch randn device=device dtype=dtype m = MyModule device= meta dtype=dtype m input Test empty meta module error torch nn Module assertRaisesRegex NotImplementedError re escape Cannot copy out meta tensor no data Please use torch nn Module to_empty instead torch nn Module when moving module meta different device m device Test materializing meta module real device m to_empty device=device m input torch no_grad torch nn init kaiming_uniform_ m weight m input Test creating meta module materialized module m to_empty device= meta m input test_module_to_empty_non_recursive device Layer nn Module __init__ in_features out_features super __init__ weight = nn Parameter torch randn in_features out_features register_buffer buf torch randn out_features forward x x weight + buf MyModule nn Module __init__ in_features out_features super __init__ weight = nn Parameter torch randn in_features out_features register_buffer buf torch randn out_features layer = Layer out_features out_features forward x layer x weight + buf torch device meta m = MyModule m to_empty device=device recurse=False params buffers parent should have been materialized device assertTrue m weight is_meta assertTrue m buf is_meta parameters buffers children submodules should still meta p m layer parameters m layer buffers assertTrue p is_meta skipMeta test_skip_init device torch manual_seed m_initialized = torch nn Linear m_initialized device torch manual_seed m_uninitialized = torch nn utils skip_init torch nn Linear device=device assertEqual m_initialized weight device m_uninitialized weight device assertFalse torch allclose m_initialized weight m_uninitialized weight skipIfMPS TODO hvaara Investigate possible bug macOS passes while fails dtypes torch float dtypesIfCUDA torch double torch float torch half test_transformerencoderlayer device dtype deterministic test TransformerEncoderLayer d_model = nhead = dim_feedforward = dropout = bsz = atol = e- rtol = e- cuda device atol = e- rtol = e- _test training batch_first atol rtol perm_fn x x transpose batch_first x model = nn TransformerEncoderLayer d_model nhead dim_feedforward dropout batch_first=batch_first device=device dtype=dtype training assert dropout == model = model eval set constant weights model p model parameters x = p data sz = x view - size shape = x shape x = torch cos torch arange sz float view shape p data copy_ x deterministic input encoder_input = torch tensor device=device dtype=dtype result = model encoder_input ref_output = torch tensor - device=device dtype=dtype assertEqual result shape ref_output shape torch testing assert_close result ref_output atol=atol rtol=rtol values NOT masked This shouldn t mask anything mask = torch tensor device=device == TODO enable fast path calls mask result = model encoder_input src_key_padding_mask=mask assertEqual result shape ref_output shape torch testing assert_close result ref_output atol=atol rtol=rtol mask = torch tensor device=device == result = model encoder_input src_key_padding_mask=mask fast_path_device = result is_cuda result is_cpu result = result cpu detach numpy Non Fast Paths training batch_first TEST_WITH_CROSSREF fast_path_device We changed semenatic non fast path so fully masked out rows attention thus NaNs should no longer present output should nonzero due skip connections assertTrue np isnan result any Fast Paths assertTrue np isnan result all deterministic input encoder_input = perm_fn torch tensor device=device dtype=dtype result = model encoder_input ref_output = perm_fn torch tensor - - device=device dtype=dtype assertEqual result shape ref_output shape torch testing assert_close result ref_output atol=atol rtol=rtol all which no masking mask = torch tensor device=device == result = model encoder_input src_key_padding_mask=mask assertEqual result shape ref_output shape torch testing assert_close result ref_output atol=atol rtol=rtol mask = torch tensor device=device == result = model encoder_input src_key_padding_mask=mask ref_output = perm_fn torch tensor - - device=device dtype=dtype assertEqual result shape ref_output shape torch testing assert_close result ref_output atol=atol rtol=rtol deterministic input encoder_input = perm_fn torch tensor device=device dtype=dtype result = model encoder_input ref_output = perm_fn torch tensor - - - - - - - - - - - - - - - - - - - - device=device dtype=dtype assertEqual result shape ref_output shape torch testing assert_close result ref_output atol=atol rtol=rtol all mask = torch zeros device=device == result = model encoder_input src_key_padding_mask=mask assertEqual result shape ref_output shape torch testing assert_close result ref_output atol=atol rtol=rtol mask = mask = mask = result = model encoder_input src_key_padding_mask=mask ref_output = perm_fn torch tensor - - - - - - - - - - - - - - - - - - - - device=device dtype=dtype assertEqual result shape ref_output shape torch testing assert_close result ref_output atol=atol rtol=rtol NestedTensor only supported fast path currently which won t used training batch_first training cuda str device cpu str device TEST_WITH_CROSSREF encoder_input - = torch zeros_like encoder_input mask = torch zeros encoder_input shape - device=device dtype=torch bool mask - = True nt = torch nested nested_tensor encoder_input - encoder_input device=device result = model nt ref_output = torch tensor - - - - - - - - - - - - - - - - - - - - device=device dtype=dtype result = result to_padded_tensor ref_output - = torch zeros_like ref_output - device=device dtype=dtype result - = torch zeros_like result - device=device dtype=dtype assertEqual tuple result shape tuple ref_output shape cuda device dtype == torch float atol = e- rtol = e- atol = e- rtol = e- torch testing assert_close result ref_output atol=atol rtol=rtol torch testing assert_close result ref_output batch_first True False training True False training cm = contextlib nullcontext Fast path requires inference mode cm = torch no_grad cm _test batch_first=batch_first training=training atol=atol rtol=rtol onlyCPU dtypes torch double test_transformerencoderlayer_fast_path device dtype Test transformer fast path CPU different valid mask types shapes d_model = nhead = batch_size = src_len = model = torch nn TransformerEncoderLayer d_model=d_model nhead=nhead batch_first=True device=device dtype=dtype dropout= model eval Batched inputs src = torch rand batch_size src_len dtype=dtype Attention mask shape src_len src_len src_mask = torch zeros src_len src_len torch bool torch no_grad model src src_mask=src_mask Padding mask shape batch_size src_len src_key_padding_mask = torch zeros batch_size src_len torch bool torch no_grad model src src_key_padding_mask=src_key_padding_mask Provide both masks torch no_grad model src src_mask=src_mask src_key_padding_mask=src_key_padding_mask dtypes torch float dtypesIfCUDA torch half torch float test_transformerencoderlayer_gelu device dtype deterministic test TransformerEncoderLayer gelu activation d_model = nhead = dim_feedforward = dropout = bsz = atol = rtol = e- cuda device atol = e- rtol = e- _test activation batch_first training perm_fn x x transpose batch_first x model = nn TransformerEncoderLayer d_model nhead dim_feedforward dropout activation batch_first=batch_first device=device dtype=dtype training assert dropout == model = model eval set constant weights model p model parameters x = p data sz = x view - size shape = x shape x = torch cos torch arange sz float view shape p data copy_ x deterministic input encoder_input = torch tensor device=device dtype=dtype result = model encoder_input ref_output = torch tensor - device=device dtype=dtype torch testing assert_close result ref_output rtol=rtol atol=atol deterministic input encoder_input = perm_fn torch tensor device=device dtype=dtype result = model encoder_input ref_output = perm_fn torch tensor - - device=device dtype=dtype torch testing assert_close result ref_output rtol=rtol atol=atol deterministic input encoder_input = perm_fn torch tensor device=device dtype=dtype result = model encoder_input ref_output = perm_fn torch tensor - - - - - - - - - - - - - - - - - - - - device=device dtype=dtype torch testing assert_close result ref_output rtol=rtol atol=atol activation batch_first training product gelu F gelu nn GELU True False True False Fast path requires inference mode training cm = contextlib nullcontext cm = torch no_grad cm _test activation=activation batch_first=batch_first training=training skipIfMPS RuntimeError foreach=True passed can t use foreach API mps tensors parametrize_test foreach False True test_clip_grad_value foreach device torch device device type == xla foreach raise SkipTest foreach supported XLA torch device device type == mps foreach raise SkipTest foreach supported MPS l = nn Linear device clip_value = grad_w grad_b = torch arange - device=device view div_ torch ones device=device mul_ grad_list grad_w grad_b grad_w None p g zip l parameters grad_list p _grad = g clone view_as p data g None g clip_grad_value_ l parameters clip_value foreach=foreach p filter lambda p p grad None l parameters assertLessEqual p grad data max clip_value assertGreaterEqual p grad data min -clip_value Should accept single Tensor input p p = torch randn device=device torch randn device=device g = torch arange - device=device view div_ p _grad = g clone p _grad = g clone clip_grad_value_ p clip_value foreach=foreach clip_grad_value_ p clip_value foreach=foreach assertEqual p grad p grad skipIfMPS TypeError MPS framework doesn t support float parametrize_test foreach False True parametrize_test norm_type inf test_clip_grad_norm norm_type foreach device torch device device type == xla foreach raise SkipTest foreach supported XLA torch device device type == mps foreach raise SkipTest foreach supported MPS l = nn Linear device max_norm = compute_norm norm_type norm_type = float norm_type norm_type = inf total_norm = p l parameters total_norm += p grad data abs pow norm_type sum pow total_norm norm_type max p grad data abs max p l parameters compare_scaling grads p_scale = p grad data div g view - p g zip l parameters grads scale = torch cat p_scale assertEqual scale std scale grads = torch arange device=device view torch ones device=device div p g zip l parameters grads p _grad = g clone view_as p data norm_before = compute_norm norm_type norm = clip_grad_norm_ l parameters max_norm norm_type=norm_type foreach=foreach norm_after = compute_norm norm_type assertEqual norm norm_before assertEqual norm_after max_norm assertLessEqual norm_after norm_before compare_scaling grads decomposed APIs should behave expected grads = torch arange device=device view torch ones device=device div p g zip l parameters grads p _grad = g clone view_as p norm_before = compute_norm norm_type grads = p grad p l parameters total_norm = get_total_norm grads norm_type=norm_type foreach=foreach clip_grads_with_norm_ l parameters max_norm total_norm foreach=foreach norm_after = compute_norm norm_type assertEqual total_norm norm_before assertEqual norm_after max_norm assertLessEqual norm_after norm_before compare_scaling grads Small gradients should left unchanged grads = torch rand device=device div torch ones device=device div p g zip l parameters grads p grad data copy_ g norm_before = compute_norm norm_type norm = clip_grad_norm_ l parameters max_norm norm_type=norm_type foreach=foreach norm_after = compute_norm norm_type assertEqual norm norm_before assertEqual norm_before norm_after assertLessEqual norm_after max_norm scale = compare_scaling grads assertEqual scale Should accept single Tensor input p p = torch randn device=device torch randn device=device g = torch arange device=device view p _grad = g clone p _grad = g clone clip_grad_norm_ p max_norm norm_type=norm_type foreach=foreach clip_grad_norm_ p max_norm norm_type=norm_type foreach=foreach assertEqual p grad p grad Should warning when parameters generator exhausted params = l parameters p params pass warnings catch_warnings record=True w warnings simplefilter always clip_grad_norm_ params max_norm norm_type=norm_type foreach=foreach assertEqual len w assertEqual str w message ` parameters ` empty generator no gradient clipping will occur reference issue https github com pytorch pytorch issues onlyCUDA largeTensorTest GB cuda test_softmax_forward_ bit_indexing device batch_size = seq_len = vocab_size = shift_labels = torch zeros batch_size seq_len - dtype=torch long device=device logits = torch ones batch_size seq_len - vocab_size dtype=torch float device=device loss_fct = torch nn CrossEntropyLoss reduction= none nll = loss_fct logits permute shift_labels float rtol atol = torch testing _comparison get_tolerances torch float rtol=None atol=None assertEqual nll torch ones_like nll torch log torch tensor vocab_size rtol=rtol atol=atol onlyCUDA largeTensorTest GB cuda test_softmax_backward_ bit_indexing device numel + x = torch ones numel device=device dtype=torch float x fill_ numel out = torch _softmax_backward_data x x x dtype assertEqual out numel onlyCUDA test_softmax_backward_smem device torch manual_seed We use smem tensors have elements size = bytes numel = dtype torch half torch float output = torch rand numel device=device dtype=dtype grad_output = torch rand numel device=device dtype=dtype result = torch _softmax_backward_data grad_output output output dtype expected_result = torch _softmax_backward_data grad_output cpu output cpu dtype assertEqual expected_result result onlyCUDA test_softmax_backward_without_fully_vectorized device torch manual_seed We don t use smem here because size elements does divide ILP cleanly ILP defined sizeof float sizeof dtype Since ILP numel divisible we don t use shared memory here numel = + dtype torch half torch float output = torch rand numel device=device dtype=dtype grad_output = torch rand numel device=device dtype=dtype numel result = torch _softmax_backward_data grad_output output output dtype expected_result = torch _softmax_backward_data grad_output cpu output cpu dtype assertEqual expected_result result make_unaligned_ d_tensor_of_rand numel device dtype It s hard get pytorch us tensor aligned bytes To work around we create aligned tensor create slice aligned output = torch ones numel + device=device dtype=dtype unaligned_output = output assertNotEqual unaligned_output data_ptr unaligned_output onlyCUDA test_softmax_backward_unaligned_output device torch manual_seed We don t use smem here because output aligned bytes numel = dtype torch half torch float unaligned_output = make_unaligned_ d_tensor_of_rand numel device dtype grad_output = torch rand numel device=device dtype=dtype numel result = torch _softmax_backward_data grad_output unaligned_output unaligned_output dtype expected_result = torch _softmax_backward_data grad_output cpu unaligned_output cpu dtype assertEqual expected_result result onlyCUDA test_softmax_backward_unaligned_grad_output device torch manual_seed numel = dtype torch half torch float output = torch rand numel device=device dtype=dtype unaligned_grad_output = make_unaligned_ d_tensor_of_rand numel device dtype numel result = torch _softmax_backward_data unaligned_grad_output output output dtype expected_result = torch _softmax_backward_data unaligned_grad_output cpu output cpu dtype assertEqual expected_result result reference issue https github com pytorch pytorch issues onlyCUDA test_adaptiveavg_pool d_shmem device x = torch randn device=device memory_format=torch channels_last x_cpu = x cpu x_cpu requires_grad_ x requires_grad_ y = torch nn functional adaptive_avg_pool d x y_cpu = torch nn functional adaptive_avg_pool d x_cpu grad = torch randn_like y grad_cpu = grad cpu y backward grad y_cpu backward grad_cpu assertEqual x grad x_cpu grad skipMeta expectedFailureMPS NotImplementedError aten channel_shuffle https github com pytorch pytorch issues test_channel_shuffle device D tensor x = torch tensor device=device y_ref = torch tensor device=device ChannelsFirst warnings catch_warnings record=True w y = F channel_shuffle x device assertEqual len w assertEqual y y_ref ChannelsLast supported dim D tensor x = torch tensor device=device y_ref = torch tensor device=device ChannelsFirst NCHW warnings catch_warnings record=True w y = F channel_shuffle x device assertEqual len w assertEqual y y_ref ChannelsLast NHWC warnings catch_warnings record=True w y = F channel_shuffle x contiguous memory_format=torch channels_last device assertEqual len w y = y contiguous memory_format=torch contiguous_format assertEqual y y_ref D tensor x = torch tensor device=device y_ref = torch tensor device=device ChannelsFirst NCHW warnings catch_warnings record=True w y = F channel_shuffle x device assertEqual len w assertEqual y y_ref ChannelsLast NHWC warnings catch_warnings record=True w y = F channel_shuffle x contiguous memory_format=torch channels_last_ d device assertEqual len w y = y contiguous memory_format=torch contiguous_format assertEqual y y_ref TestFunctionalPickle TestCase issue gh- test_pickle_softsign Make sure does throw exception s = pickle dumps F softsign TestFusionUtils TestCase test_fuse_conv_bn_requires_grad conv = torch nn Conv d bn = torch nn BatchNorm d cases = itertools product True False True False w_rg b_rg cases conv weight requires_grad = w_rg conv bias requires_grad = b_rg weight bias = \ fuse_conv_bn_weights conv weight conv bias bn running_mean bn running_var bn eps bn weight bn bias assertEqual weight requires_grad w_rg assertEqual bias requires_grad b_rg test_fuse_linear_bn_requires_grad linear = torch nn Linear bn = torch nn BatchNorm d cases = itertools product True False True False w_rg b_rg cases linear weight requires_grad = w_rg linear bias requires_grad = b_rg weight bias = \ fuse_linear_bn_weights linear weight linear bias bn running_mean bn running_var bn eps bn weight bn bias assertEqual weight requires_grad w_rg assertEqual bias requires_grad b_rg TestUtils TestCase test_consume_prefix_in_state_dict_if_present Block nn Module __init__ - None super __init__ conv = nn Conv d bias=True conv = nn Conv d bias=False Net nn Module __init__ - None super __init__ linear = nn Linear linear = nn Linear net bn = nn BatchNorm d block = Block Case non-DDP model empty state_dict net = nn Module state_dict = net state_dict nn modules utils consume_prefix_in_state_dict_if_present state_dict module check they same preserving order assertEqual list state_dict keys list net state_dict keys assertEqual list state_dict _metadata keys list net state_dict _metadata keys Case non-DDP model test example state_dict net = Net state_dict = net state_dict nn modules utils consume_prefix_in_state_dict_if_present state_dict module Check they same preserving order assertEqual list state_dict keys list net state_dict keys assertEqual list state_dict _metadata keys list net state_dict _metadata keys Case DDP model test example state_dict state_dict = net state_dict metadata = state_dict _metadata ddp_state_dict = OrderedDict f module k v k v state_dict items ddp_state_dict _metadata = OrderedDict metadata ddp_state_dict _metadata update module k == f module k v k v metadata items nn modules utils consume_prefix_in_state_dict_if_present ddp_state_dict module Check they same preserving order assertEqual list state_dict keys list ddp_state_dict keys assertEqual list state_dict _metadata keys list ddp_state_dict _metadata keys instantiate_device_type_tests TestNNDeviceType globals allow_mps=True instantiate_parametrized_tests TestNN __name__ == __main__ TestCase _default_dtype_check_enabled = True run_tests