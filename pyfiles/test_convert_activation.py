Owner s oncall jit os sys unittest itertools product torch torch nn nn torch nn functional F torch testing FileCheck try torchvision HAS_TORCHVISION = True except ImportError HAS_TORCHVISION = False skipIfNoTorchVision = unittest skipIf HAS_TORCHVISION no torchvision Make helper files test importable pytorch_test_dir = os path dirname os path dirname os path realpath __file__ sys path append pytorch_test_dir torch testing _internal common_utils raise_on_run_directly torch testing _internal jit_utils JitTestCase activations = F celu F elu F hardsigmoid F hardswish F hardtanh F leaky_relu F relu F relu F rrelu F selu F silu TestFunctionalToInplaceActivation JitTestCase test_check_no_type_promotion dtypes = torch bool torch int torch int torch int torch int torch float torch float restore_mutation h contains mapping activation operators whether they allow type conversion Use checking guard mapping any later change breaks assumption we need update mapping correspondingly activation dtype product activations dtypes inp = torch normal size= dtype try out = activation inp assertEqual dtype out dtype except RuntimeError Skip implemented error pass test_functional_to_inplace_activation activation activations test_basic x y = x + z = activation y z fn = torch jit script test_basic run_pass inline fn graph run_pass constant_propagation fn graph FileCheck check f aten activation __name__ run fn graph run_pass functional_to_inplace_activation fn graph FileCheck check_not f aten activation __name__ run fn graph FileCheck check f aten activation __name__ _ run fn graph inp = torch rand assertEqual fn inp test_basic inp test_no_functional_to_inplace inplace conversion should happen because sigmoid may perform type conversion test y = torch ones z = torch sigmoid y z fn = torch jit script test run_pass functional_to_inplace_activation fn graph FileCheck check_not aten sigmoid_ run fn graph inplace conversion should happen because y alias input x test x y = x z = torch relu y z fn = torch jit script test run_pass functional_to_inplace_activation fn graph FileCheck check_not aten relu_ run fn graph inplace conversion should happen because x global scope Test nn Module __init__ x super __init__ x = x forward y = torch relu x y fn = torch jit script Test torch rand eval run_pass functional_to_inplace_activation fn graph FileCheck check_not aten relu_ run fn graph skipIfNoTorchVision test_resnet _correctness model = torchvision models resnet frozen_model = torch jit freeze torch jit script model eval N C H W = inp = torch randn N C H W run_pass functional_to_inplace_activation frozen_model graph assertEqual model inp frozen_model inp TestInplaceToFunctionalActivation JitTestCase test_inplace_to_functional_activation activation activations test_basic x y = x + activation y inplace=True y fn = torch jit script test_basic run_pass inline fn graph run_pass constant_propagation fn graph FileCheck check f aten activation __name__ _ run fn graph run_pass inplace_to_functional_activation fn graph FileCheck check_not f aten activation __name__ _ run fn graph FileCheck check f aten activation __name__ run fn graph activation torch relu_ torch sigmoid_ torch tanh_ test_basic x y = x + activation y y fn = torch jit script test_basic run_pass inline fn graph run_pass constant_propagation fn graph FileCheck check f aten activation __name__ run fn graph run_pass inplace_to_functional_activation fn graph FileCheck check_not f aten activation __name__ run fn graph FileCheck check f aten activation __name__ - run fn graph inp = torch rand assertEqual fn inp test_basic inp skipIfNoTorchVision test_resnet _correctness model = torchvision models resnet frozen_model = torch jit freeze torch jit script model eval N C H W = inp = torch randn N C H W run_pass inplace_to_functional_activation frozen_model graph assertEqual model inp frozen_model inp __name__ == __main__ raise_on_run_directly test test_jit py