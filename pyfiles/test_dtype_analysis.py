Owner s oncall jit itertools product typing Tuple unittest case expectedFailure torch torch complex float float int int torch jit _passes _property_propagation torch testing _internal common_device_type instantiate_device_type_tests ops torch testing _internal common_methods_invocations op_db sample_inputs_adaptive_avg_pool d sample_inputs_conv d SampleInput torch testing _internal common_utils first_sample raise_on_run_directly set_default_dtype torch testing _internal jit_metaprogramming_utils create_traced_fn torch testing _internal jit_utils JitTestCase Dtype Analysis relies symbolic shape analysis which still beta custom_rules_works_list = nn functional adaptive_avg_pool d nn functional adaptive_avg_pool d nn functional adaptive_avg_pool d nn functional adaptive_max_pool d nn functional adaptive_max_pool d avg_pool d avg_pool d conv_transpose d conv d conv d hardswish avg_pool d max_pool d max_pool d max_pool d nn functional prelu batch_norm custom_rules_expected_failure_list = create_traced_fn generates prim NumToTensor nodes graph supported yet nn functional adaptive_max_pool d These ops seem opinfos custom_rules_not_tested_list = conv d conv_tbc conv_transpose d conv_transpose d convolution _convolution max_unpool d max_unpool d reflection_pad d reflection_pad d reflection_pad d replication_pad d replication_pad d replication_pad d upsample_bilinear d upsample_linear d upsample_nearest d upsample_nearest d upsample_nearest d upsample_trilinear d flatten TestDtypeBase JitTestCase SCALAR = SCALAR To mark unary vs dim tensor setUp prev_symbolic_shapes_test_enabled = torch _C _jit_symbolic_shapes_test_mode_enabled torch _C _jit_set_symbolic_shapes_test_mode True tearDown torch _C _jit_set_symbolic_shapes_test_mode prev_symbolic_shapes_test_enabled staticmethod node_output_dtypes graph dtypes = out graph outputs isinstance out type torch _C TensorType dtypes append out type dtype dtypes append None dtypes staticmethod node_output_dtype_single graph dtypes = TestDtypeBase node_output_dtypes graph assert len dtypes == dtypes prop_dtype_on_graph graph example_inputs We need clear shape information because torch jit script will cached graph function scripted twice torch _C _jit_pass_erase_shape_information graph _property_propagation apply_input_props_using_example graph example_inputs torch _C _jit_pass_propagate_shapes_on_graph graph torch _C _jit_pass_propagate_dtype graph assert_dtype_equal fn in_shapes in_dtypes inputs = get_rand_tensor s d s d zip in_shapes in_dtypes try assert_dtype_equal_custom_args fn inputs except Exception e fail_text = f Failed shapes in_shapes dtypes in_dtypes raise AssertionError fail_text e assert_dtype_equal_custom_args fn args try Eager execution expected_res = fn args except RuntimeError expected_dtype = expected_res dtype Run Dtype Analysis graph = torch jit script fn graph Note cached graph prop_dtype_on_graph graph args actual_dtype = node_output_dtype_single graph assertEqual actual_dtype expected_dtype Failed Verification get_rand_tensor shape dtype shape SCALAR dtype float dtype int raise RuntimeError Testing scalars only supported fp int dtype int int rand_tensor = torch randint shape dtype=dtype rand_tensor = torch rand shape dtype=dtype Sanity check assertEqual rand_tensor dtype dtype rand_tensor TestDtypeAnalysis TestDtypeBase test_unary Testing Unary Implementation uses metatensors relu_inplace x x relu_ log x torch log x functions = relu_inplace log input_shapes = Simple Case Size Tensor zerodim input_dtypes = float Simple Case int Test how some unary ops implicitly convert float complex Show we can handle complex vals well fn in_shapes in_dtypes product functions input_shapes input_dtypes assert_dtype_equal fn in_shapes in_dtypes test_binary_tensors Testing using Metatensors add x y x + y div x y x y functions = add div input_shapes = Different Dim non-zerodim One zerodim Other zerodim Test tensor dim both zerodim input_dtypes = float float Simple Case int int Size Promotion compliated case dim tensors float int type Promotion int float Type promotion size change float complex Show we can handle complex vals well fn in_shapes in_dtypes product functions input_shapes input_dtypes assert_dtype_equal fn in_shapes in_dtypes test_binary_scalar Test mixing scalar non-scalar args input_shapes = SCALAR Non-Zerodim vs scalar SCALAR Zerodim vs scalar Scalar vs Scalar automatically inferred input_dtypes = float float Simple Case int int Size Promotion compliated case dim tensors int float type Promotion set_default_dtype float in_shapes in_dtypes product input_shapes input_dtypes scalar_type = in_dtypes scalar_type == float add x y float x + y add x y int x + y assert_dtype_equal add in_shapes in_dtypes test_custom_rules Test some ops covered Metatensors Note unlike Conv d module function conv d does take dtype device arguments conv d_fn input weight bias torch nn functional conv d input weight bias adaptive_avg_pool d_fn input output_size Tuple int torch _C _nn adaptive_avg_pool d input output_size fn inputs_fn conv d_fn sample_inputs_conv d adaptive_avg_pool d_fn sample_inputs_adaptive_avg_pool d dtype torch int torch float Gets default version conv d sample_input SampleInput = list inputs_fn None cpu dtype False - input_args = sample_input input sample_input args assert_dtype_equal_custom_args fn input_args test_conv_no_mixed_args conv d_fn input weight bias torch nn functional conv d input weight bias Now make sure conv d doesn t support mixed args conv_ins = sample_inputs_conv d None cpu torch float False conv_in = list conv_ins - weight bias = conv_in args weight = weight type torch long assertRaises RuntimeError conv d_fn conv_in input weight bias Check we also don t propagate graph = torch jit script conv d_fn graph Note cached graph prop_dtype_on_graph graph conv_in input weight bias actual_dtype = node_output_dtype_single graph assertEqual actual_dtype None test_combined Test case both custom rules metatensors func input weight bias y conv_out = torch nn functional conv d input weight bias conv_ = conv_out + y flattened = torch flatten conv_ start_dim= add_res = flattened + y add_res conv_ins = sample_inputs_conv d None cpu torch int False conv_in = list conv_ins - y_val = torch rand dtype=torch float input_args = conv_in input conv_in args y_val assert_dtype_equal_custom_args func input_args TestDtypeCustomRules TestDtypeBase assert_output_dtype_equal expected_res prop_graph actual_dtype = node_output_dtypes prop_graph len actual_dtype == For len= there no tuple packing expected_res assert_tensor_dtype_equal expected_res actual_dtype assertEqual len expected_res len actual_dtype expected actual zip expected_res actual_dtype assert_tensor_dtype_equal expected actual assert_tensor_dtype_equal tensor_output graph_dtype isinstance tensor_output torch Tensor assertEqual tensor_output dtype graph_dtype custom_rules_test_base device dtype op allow_eager_fail=False try samples = op sample_inputs device dtype requires_grad=False sample_input = first_sample samples input_args = sample_input input sample_input args expected_res = op input_args sample_input kwargs except Exception e allow_eager_fail raise e func = op get_op traced_fn = create_traced_fn func Have run traced function actually generate trace traced_fn sample_input input sample_input args sample_input kwargs Run Dtype Analysis graph = traced_fn graph Note cached graph input_tensors = t t input_args isinstance t torch Tensor input_tensors += v v sample_input kwargs values isinstance v torch Tensor prop_dtype_on_graph graph input_tensors assert_output_dtype_equal expected_res graph ops op op op_db op aten_name custom_rules_works_list test_custom_rules device dtype op custom_rules_test_base device dtype op ops op op op_db op aten_name custom_rules_works_list test_custom_rules_ints device dtype op This done because opinfos currently only runs floats Return fn inputs_fn all dtype == torch float dtype = torch int dtype = torch int Because ints always implemented we need allow eager fail custom_rules_test_base device dtype op allow_eager_fail=True expectedFailure ops op op op_db op aten_name custom_rules_expected_failure_list test_custom_rules_expected_failure device dtype op custom_rules_test_base device dtype op TestDtypeCustomRulesCPU = None This creates TestDtypeCustomRulesCPU instantiate_device_type_tests TestDtypeCustomRules globals only_for= cpu __name__ == __main__ raise_on_run_directly test test_jit py