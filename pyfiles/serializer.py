mypy allow-untyped-defs array enum functools logging operator struct sys typing NamedTuple Optional torch TODO Add type annotations TODO Check tensor types ops LOG = logging getLogger nnapi_serialize NNAPI_OperandCode FLOAT = INT = UINT = TENSOR_FLOAT = TENSOR_INT = TENSOR_QUANT _ASYMM = BOOL = TENSOR_QUANT _SYMM = TENSOR_FLOAT = TENSOR_BOOL = FLOAT = TENSOR_QUANT _SYMM_PER_CHANNEL = TENSOR_QUANT _ASYMM = NNAPI_OperationCode ADD = AVERAGE_POOL_ D = CONCATENATION = CONV_ D = DEPTHWISE_CONV_ D = DEPTH_TO_SPACE = DEQUANTIZE = EMBEDDING_LOOKUP = FLOOR = FULLY_CONNECTED = HASHTABLE_LOOKUP = L _NORMALIZATION = L _POOL_ D = LOCAL_RESPONSE_NORMALIZATION = LOGISTIC = LSH_PROJECTION = LSTM = MAX_POOL_ D = MUL = RELU = RELU = RELU = RESHAPE = RESIZE_BILINEAR = RNN = SOFTMAX = SPACE_TO_DEPTH = SVDF = TANH = BATCH_TO_SPACE_ND = DIV = MEAN = PAD = SPACE_TO_BATCH_ND = SQUEEZE = STRIDED_SLICE = SUB = TRANSPOSE = ABS = ARGMAX = ARGMIN = AXIS_ALIGNED_BBOX_TRANSFORM = BIDIRECTIONAL_SEQUENCE_LSTM = BIDIRECTIONAL_SEQUENCE_RNN = BOX_WITH_NMS_LIMIT = CAST = CHANNEL_SHUFFLE = DETECTION_POSTPROCESSING = EQUAL = EXP = EXPAND_DIMS = GATHER = GENERATE_PROPOSALS = GREATER = GREATER_EQUAL = GROUPED_CONV_ D = HEATMAP_MAX_KEYPOINT = INSTANCE_NORMALIZATION = LESS = LESS_EQUAL = LOG = LOGICAL_AND = LOGICAL_NOT = LOGICAL_OR = LOG_SOFTMAX = MAXIMUM = MINIMUM = NEG = NOT_EQUAL = PAD_V = POW = PRELU = QUANTIZE = QUANTIZED_ BIT_LSTM = RANDOM_MULTINOMIAL = REDUCE_ALL = REDUCE_ANY = REDUCE_MAX = REDUCE_MIN = REDUCE_PROD = REDUCE_SUM = ROI_ALIGN = ROI_POOLING = RSQRT = SELECT = SIN = SLICE = SPLIT = SQRT = TILE = TOPK_V = TRANSPOSE_CONV_ D = UNIDIRECTIONAL_SEQUENCE_LSTM = UNIDIRECTIONAL_SEQUENCE_RNN = RESIZE_NEAREST_NEIGHBOR = NNAPI_FuseCode FUSED_NONE = FUSED_RELU = FUSED_RELU = FUSED_RELU = OperandValueSourceType IMMEDIATE = NUMBERED_BUFFER = NUMBERED_MEMORY = Scalar types appear explicitly models These must kept sync AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS TODO Expose these directly Python avoid maintaining list TorchScalarTypes enum Enum QUINT = approx_equal lhs rhs tolerance= e- abs lhs - rhs = tolerance min lhs rhs tensor_size op_type dims ITEM_SIZES = NNAPI_OperandCode TENSOR_FLOAT NNAPI_OperandCode TENSOR_INT NNAPI_OperandCode TENSOR_QUANT _ASYMM NNAPI_OperandCode TENSOR_QUANT _SYMM NNAPI_OperandCode TENSOR_QUANT _ASYMM size = ITEM_SIZES op_type d dims size = d size change_element tup index value ls = list tup ls index = value tuple ls ConvPoolArgs d NamedTuple Configuration arguments convolution kernel_h int kernel_w int stride_h int stride_w int pad_t int pad_b int pad_l int pad_r int dilation_h int dilation_w int group int DimOrder enum Enum PRESUMED_CONTIGUOUS = CHANNELS_LAST = SCALAR_OR_VECTOR = UNKNOWN_CONSTANT = Operand NamedTuple Representation NNAPI operand NNAPI operand type One NNAPI_OperandCode TODO Make enum op_type int This always PyTorch shape which NCHW feature maps The actual NNAPI operand might have transposed shape we use load time dynamic shapes - runtime dynamic shapes shape tuple int Specifies how shape operand we define NNAPI relates shape we track above - PRESUMED_CONTIGUOUS physical NNAPI operand will exactly match shape PyTorch tensor - CHANNELS_LAST The PyTorch tensor expected NCHW NNAPI operand will represented explicitly NHWC dim_order DimOrder Quantization params scale float zero_point int use_nchw dim_order DimOrder PRESUMED_CONTIGUOUS True dim_order DimOrder CHANNELS_LAST False raise Exception Unknown dim order noqa TRY broadcast_shapes shape shape assert len shape assert len shape s = list shape s = list shape TODO Support non-equal-rank broadcast where semantics match This can tricky NHWC tensors because dimension orders don t match between PT NNAPI even though semantics match len s len s s = len s - len s + s raise Exception noqa TRY Non-equal-rank broadcast supported yet noqa TRY len s len s s = len s - len s + s raise Exception noqa TRY Non-equal-rank broadcast supported yet noqa TRY ret = d d zip s s d == ret append d d == ret append d d == d ret append d raise Exception noqa TRY f Cannot broadcast shapes shape shape noqa TRY tuple ret get_conv_pool_shape image_shape args out_ch transpose batch _in_c in_h in_w = image_shape TODO Handle dilation args dilation_h = args dilation_w = raise Exception Dilation supported yet noqa TRY transpose out_h = in_h - args stride_h + args kernel_h - args pad_t - args pad_b out_w = in_w - args stride_w + args kernel_w - args pad_l - args pad_l out_h = in_h - args kernel_h + args pad_t + args pad_b args stride_h + out_w = in_w - args kernel_w + args pad_l + args pad_r args stride_w + Handle variable-sized tensors in_h == out_h = in_w == out_w = out_shape = batch out_ch out_h out_w out_shape fix_shape shape dim_order Return actual shape operand should have NNAPI given PyTorch shape dimension order This where we convert PyTorch s always NCHW shape explicit NHWC dim_order DimOrder PRESUMED_CONTIGUOUS shape dim_order DimOrder CHANNELS_LAST tuple shape + list shape + shape dim_order DimOrder SCALAR_OR_VECTOR assert len shape == len shape == shape dim_order DimOrder UNKNOWN_CONSTANT XXX think through shape raise Exception f Bad dim_order dim_order r noqa TRY reverse_map_dim dim_order d Return original PyTorch dimension position given dimension d should dimension NNAPI will see reverse_map_dim PRESUMED_CONTIGUOUS x == x reverse_map_dim CHANNELS_LAST == dim_order DimOrder PRESUMED_CONTIGUOUS DimOrder SCALAR_OR_VECTOR d assert dim_order DimOrder CHANNELS_LAST d flex_name op_id dim Return local variable name computed flexible size given op dimension f s_ op_id _ dim _NnapiSerializer __init__ config use_int _for_qint =False operands = values = operations = value_data = operation_args = inputs = outputs = flexible_shape_computation_lines = modules = constants = tensor_sequences = jitval_operand_map = cached_immediates = used_weights = weight_offset = use_int _for_qint = use_int _for_qint config None config = get_next_operand_id len operands Add tensor operand corresponding JIT Value Returns NNAPI operand ID Can looked up later get_tensor_operand_by_jitval add_tensor_operand jitval oper assert isinstance oper Operand jitval jitval_operand_map raise Exception f Duplicate tensor jitval r noqa TRY operand_id = get_next_operand_id operands append oper jitval_operand_map jitval = operand_id operand_id Add tensor operand does correspond JIT Value Useful cases where multiple NNAPI operands required implement one JIT IR node Returns NNAPI operand ID add_anonymous_tensor_operand oper assert isinstance oper Operand operand_id = get_next_operand_id operands append oper operand_id torch_tensor_to_operand tensor dim_order dtype = str tensor dtype replace torch scale = zero_point = dtype == float op_type = NNAPI_OperandCode TENSOR_FLOAT dtype == int op_type = NNAPI_OperandCode TENSOR_INT dtype == quint op_type = NNAPI_OperandCode TENSOR_QUANT _ASYMM scale = tensor q_scale zero_point = tensor q_zero_point dtype == qint op_type = NNAPI_OperandCode TENSOR_INT scale = tensor q_scale zero_point = tensor q_zero_point assert zero_point == dtype == int use_int _for_qint nnapi_dtype = getattr tensor nnapi_dtype None op_codes = NNAPI_OperandCode TENSOR_QUANT _SYMM NNAPI_OperandCode TENSOR_QUANT _ASYMM nnapi_dtype op_codes op_type = nnapi_dtype scale = tensor nnapi_scale zero_point = tensor nnapi_zero_point raise Exception noqa TRY f ` nnapi_type ` needs one op_codes ` int ` raise Exception noqa TRY ` int ` isn t supported If you re trying represent NNAPI qint Pytorch int set ` use_int _for_qint = True ` raise Exception noqa TRY f Can t handle input dtype tensor dtype noqa TRY Operand shape=tuple tensor shape pyrefly ignore bad-argument-type op_type=op_type dim_order=dim_order scale=scale zero_point=zero_point add_tensor_operand_for_input arg_idx jitval tensor dim_order = DimOrder CHANNELS_LAST getattr tensor nnapi_nhwc False DimOrder PRESUMED_CONTIGUOUS toper = torch_tensor_to_operand tensor dim_order operand_id = add_tensor_operand jitval toper inputs append operand_id dim size enumerate tensor shape size == compute_operand_shape operand_id dim f args arg_idx shape dim operand_id add_tensor_operand_for_weight tensor dim_order=DimOrder UNKNOWN_CONSTANT toper = torch_tensor_to_operand tensor dim_order operand_id = len operands operands append toper tsize = tensor_size toper op_type toper shape values append operand_id OperandValueSourceType NUMBERED_BUFFER buf_num = len used_weights offset = value_data append struct pack iii buf_num offset tsize For NHWC NNAPI op lay out data same dim order permuting torch tensor dim_order == DimOrder CHANNELS_LAST tensor = tensor permute used_weights append tensor operand_id add_immediate_operand code value dims assert isinstance dims tuple cache_key = code value cache_key cached_immediates operand_id = len operands operands append Operand code dims DimOrder SCALAR_OR_VECTOR values append operand_id OperandValueSourceType IMMEDIATE value_data append value cached_immediates cache_key = operand_id cached_immediates cache_key add_immediate_int_scalar value add_immediate_operand NNAPI_OperandCode INT struct pack i value add_immediate_float_scalar value add_immediate_operand NNAPI_OperandCode FLOAT struct pack f value add_immediate_bool_scalar value add_immediate_operand NNAPI_OperandCode BOOL b \x value b \x add_immediate_int_vector value add_immediate_operand NNAPI_OperandCode TENSOR_INT array array i value tobytes len value has_operand_for_jitval jitval jitval jitval_operand_map get_tensor_operand_by_jitval jitval operand_id = jitval_operand_map jitval operand_id operands operand_id get_tensor_operand_by_jitval_fixed_size jitval op_id oper = get_tensor_operand_by_jitval jitval s oper shape s == TODO Improve error message possibly after converting many callsites support flexible size raise Exception noqa TRY Flexible size supported operand noqa TRY s runtime flex LOG warning Operand s has runtime flex shape oper op_id oper get_tensor_operand_or_constant jitval dim_order=DimOrder PRESUMED_CONTIGUOUS operand_id = jitval_operand_map get jitval operand_id None _ value = get_constant_value jitval TensorType operand_id = add_tensor_operand_for_weight value dim_order operand_id operands operand_id get_tensor_operand_for_weight jitval _ value = get_constant_value jitval TensorType operand_id = add_tensor_operand_for_weight value operand_id operands operand_id add_operation opcode inputs outputs operations append opcode len inputs len outputs operation_args extend inputs + outputs add_tensor_sequence jitval values assert jitval tensor_sequences tensor_sequences jitval = values add_constant_value jitval ctype value assert jitval constants constants jitval = ctype value get_constant_value jitval typekind=None record = constants get jitval record None raise Exception noqa TRY f Could find constant value jitval r noqa TRY ctype _ = record typekind None ctype kind = typekind raise Exception noqa TRY f Expected constant value type typekind got ctype kind value jitval r record operand_to_template_torchscript op_id oper shape=None Return TorchScript expression build template given operand shape None shape = oper shape assert len shape == len oper shape shape_parts = d s enumerate shape s Fixed shape dimension just add value shape_parts append str s s == Load time flexible shape dimension should have been computed variable shape_parts append flex_name op_id d s == - Runtime flexible shape shape_parts append raise Exception noqa TRY Unknown dim value dimensions should = - noqa TRY shape_parts append shape_parts append shape_code = join shape_parts oper op_type == NNAPI_OperandCode TENSOR_FLOAT f torch zeros shape_code dtype=torch float oper op_type == NNAPI_OperandCode TENSOR_INT f torch zeros shape_code dtype=torch int oper op_type == NNAPI_OperandCode TENSOR_QUANT _ASYMM f torch quantize_per_tensor f torch zeros scale= oper scale zero_point= oper zero_point dtype=torch quint f expand shape_code contiguous oper op_type NNAPI_OperandCode TENSOR_QUANT _ASYMM NNAPI_OperandCode TENSOR_QUANT _SYMM use_int _for_qint f torch zeros shape_code dtype=torch int raise Exception noqa TRY ` int ` isn t supported If you re trying represent NNAPI qint Pytorch int set ` use_int _for_qint = True ` raise Exception noqa TRY f Unsupported output operand type oper op_type noqa TRY forward_operand_shape out_op_id out_dim in_op_id in_dim compute_operand_shape out_op_id out_dim flex_name in_op_id in_dim compute_operand_shape op_id dim expr flexible_shape_computation_lines append f flex_name op_id dim = expr transpose_to_nhwc in_id oper oper shape = raise Exception noqa TRY Automatic transpose only supported H W == noqa TRY out_oper = oper _replace dim_order=DimOrder CHANNELS_LAST inputs = None inputs = in_id inputs = add_immediate_int_vector outputs = None outputs = add_anonymous_tensor_operand out_oper add_operation NNAPI_OperationCode TRANSPOSE inputs outputs outputs out_oper Transpose inputs necessary allow broadcasting transpose_for_broadcast _id _oper _id _oper _oper dim_order == _oper dim_order _id _oper _id _oper Assume NHWC preferred there mismatch orders = _oper dim_order _oper dim_order orders == DimOrder PRESUMED_CONTIGUOUS DimOrder CHANNELS_LAST transpose_to_nhwc _id _oper + _id _oper orders == DimOrder CHANNELS_LAST DimOrder PRESUMED_CONTIGUOUS _id _oper + transpose_to_nhwc _id _oper raise Exception noqa TRY f Automatic transpose supported dim_orders _oper dim_order r _oper dim_order r get_size_arg jitval ctype value = get_constant_value jitval ctype kind == ListType assert ctype getElementType kind == IntType value raise Exception noqa TRY f Can t handle size arg type ctype r jitval r noqa TRY get_conv_pool_args_ d_from_pack kernel_size packed_config pc = i item i packed_config assert pc == strides = pc pc paddings = pc pc dilations = pc pc output_padding = pc pc group_num = pc assert len pc == assert output_padding == get_conv_pool_args_ d_common kernel_size strides paddings dilations group_num get_conv_pool_args_ d_from_jit kernel_size stride padding dilation=None group=None strides = get_size_arg stride paddings = get_size_arg padding dilation None dilations = dilations = get_size_arg dilation group None _ group_num = get_constant_value group IntType group_num = None get_conv_pool_args_ d_common kernel_size strides paddings dilations group_num get_conv_pool_args_ d_common kernel_size strides paddings dilations group_num kernels = list kernel_size assert len kernels == assert len strides == assert len paddings == assert len dilations == NNAPI uses values padding ph pw = paddings real_paddings = ph ph pw pw ConvPoolArgs d kernels + strides + real_paddings + dilations + group_num serialize_model model inputs return_shapes=None add_immediate_bool_scalar False add_immediate_bool_scalar True inp_dim_orders = out_dim_orders = self_jitval = next model graph inputs add_constant_value self_jitval self_jitval type model arg_idx input_value input_tensor enumerate zip list model graph inputs inputs op_id = add_tensor_operand_for_input arg_idx input_value input_tensor inp_dim_orders append operands op_id dim_order value idx node enumerate model graph nodes LOG debug Processing node d r idx node add_node node retn = model graph return_node assert retn inputsSize == assert retn outputsSize == retn_input = retn inputsAt template_return_lines = retn_input type kind == TensorType return_values = retn_input retval_count = - retn_input type kind == TupleType return_values = tensor_sequences retn_input retval_count = len return_values raise Exception noqa TRY f Unsupported type retn_input type noqa TRY return_shapes None assert len return_shapes == len return_values i v enumerate return_values op_id = jitval_operand_map v outputs append op_id out_dim_orders append operands op_id dim_order value shape = return_shapes i return_shapes None template_return_lines append operand_to_template_torchscript op_id operands op_id shape + template_return_lines append model = version = header = struct pack iiiiii version len operands len values len operations len inputs len outputs model append header serialized_values serialized_value_data = serialize_values model extend struct pack iifi t len d s z t d _m s z operands model extend serialized_values model extend struct pack iii x x operations Compact model so we can get its length so far model = b join model model_offset = len model Model offset index into model -bit words bytes next dimension we re about serialize If s generate code mutate before passing NNAPI assert model_offset == model_offset = int model_offset op_id _ dims dim_order _ _ enumerate operands shape = fix_shape dims dim_order d s enumerate shape s == pt_d = reverse_map_dim dim_order d flexible_shape_computation_lines append f ser_model model_offset = flex_name op_id pt_d model_offset += convert runtime flex shape - shape = tuple d d = - d shape model append serialize_ints shape model extend serialized_value_data model append serialize_ints operation_args model append serialize_ints inputs model append serialize_ints outputs flexible_shape_computation_lines extend template_return_lines array array i b join model used_weights inp_dim_orders out_dim_orders flexible_shape_computation_lines retval_count serialize_values serialized_values = serialized_value_data = assert len values == len value_data op_index source_type data zip values value_data source_length = len data Pad bytes out multiple alignment physical_length = source_length - &#124; x + padded_data = data + b \ physical_length - source_length serialized_values append struct pack iii op_index source_type source_length serialized_value_data append padded_data serialized_values serialized_value_data staticmethod serialize_ints ints array array i ints tobytes ADDER_MAP = prim GetAttr lambda node add_getattr node prim Constant lambda node add_constant_node node prim ListConstruct lambda node add_list_construct node prim TupleConstruct lambda node add_tuple_construct node aten unsqueeze lambda node add_unsqueeze node aten lambda node add_to node aten detach lambda node _identity node aten reshape lambda node add_reshape node aten flatten lambda node add_flatten node aten slice lambda node add_slice node aten size lambda node add_size node aten cat lambda node add_cat node aten mean lambda node add_mean node aten quantize_per_tensor lambda node add_quantize node aten dequantize lambda node add_dequantize node aten add lambda node add_add_sub_op node NNAPI_OperationCode ADD NNAPI_FuseCode FUSED_NONE aten sub lambda node add_add_sub_op node NNAPI_OperationCode SUB NNAPI_FuseCode FUSED_NONE aten mul lambda node add_pointwise_simple_binary_broadcast_op node NNAPI_OperationCode MUL NNAPI_FuseCode FUSED_NONE aten div lambda node add_pointwise_simple_binary_broadcast_op node NNAPI_OperationCode DIV NNAPI_FuseCode FUSED_NONE aten relu lambda node add_pointwise_simple_unary_op node NNAPI_OperationCode RELU aten sigmoid lambda node add_pointwise_simple_unary_op node NNAPI_OperationCode LOGISTIC aten softmax lambda node add_softmax node aten hardtanh lambda node add_hardtanh node aten avg_pool d lambda node add_avg_pool d node aten max_pool d lambda node add_pool d_node node NNAPI_OperationCode MAX_POOL_ D aten adaptive_avg_pool d lambda node add_adaptive_avg_pool d node aten upsample_nearest d lambda node add_upsample_nearest d node aten prelu lambda node add_prelu_op node aten addmm lambda node add_addmm node aten linear lambda node add_linear node aten _convolution lambda node add_conv_underscore node aten conv d lambda node add_conv d node aten log_softmax lambda node add_log_softmax node quantized linear lambda node add_qlinear node quantized conv d lambda node add_qconv d node NNAPI_FuseCode FUSED_NONE quantized conv d_relu lambda node add_qconv d node NNAPI_FuseCode FUSED_RELU quantized conv_transpose d lambda node add_qconv d node NNAPI_FuseCode FUSED_NONE transpose=True quantized add lambda node add_qadd node NNAPI_OperationCode ADD NNAPI_FuseCode FUSED_NONE quantized add_relu lambda node add_qadd node NNAPI_OperationCode ADD NNAPI_FuseCode FUSED_RELU quantized mul lambda node add_qadd node NNAPI_OperationCode MUL NNAPI_FuseCode FUSED_NONE add_node node adder = ADDER_MAP get node kind adder raise Exception noqa TRY f Unsupported node kind node kind r node node r noqa TRY adder node _identity node in_id _in_oper = get_tensor_operand_by_jitval node inputsAt jitval = node outputsAt jitval_operand_map jitval = in_id add_getattr node assert node inputsSize == assert node outputsSize == obj_ctype obj = get_constant_value node inputsAt assert str obj_ctype startswith __torch__ name = node s name value = getattr obj name output = node outputsAt ctype = output type add_constant_value output ctype value add_constant_node node assert node inputsSize == assert node outputsSize == output = node outputsAt ctype = output type value = output toIValue add_constant_value output ctype value add_list_construct node assert node outputsSize == output = node outputsAt ctype = output type const_vals Optional list = tensors Optional list = inp node inputs const_vals None inp constants _ val = get_constant_value inp const_vals append val const_vals = None tensors None inp type kind == TensorType tensors append inp tensors = None const_vals None NOTE Now TorchScript supports list constants code path might used anymore add_constant_value output ctype const_vals tensors None add_tensor_sequence output tensors const_vals None tensors None raise Exception noqa TRY f Unable handle ListConstruct node Neither all constants nor all tensors node r add_tuple_construct node assert node outputsSize == output = node outputsAt values = list node inputs add_tensor_sequence output values add_unsqueeze node assert node inputsSize == assert node outputsSize == in_id in_oper = get_tensor_operand_by_jitval_fixed_size node inputsAt _ dim = get_constant_value node inputsAt IntType assert in_oper dim_order == DimOrder PRESUMED_CONTIGUOUS real_dim = dim dim = dim + len in_oper shape + out_shape_list = list in_oper shape out_shape_list insert real_dim out_shape = tuple out_shape_list out_oper = in_oper _replace shape=out_shape inputs = None inputs = in_id inputs = add_immediate_int_scalar dim outputs = None outputs = add_tensor_operand node outputsAt out_oper add_operation NNAPI_OperationCode EXPAND_DIMS inputs outputs add_to node Handle cpu gpu case _identity node add_reshape node assert node inputsSize == assert node outputsSize == in_id in_oper = get_tensor_operand_by_jitval_fixed_size node inputsAt shape_ctype shape = get_constant_value node inputsAt assert shape_ctype kind == ListType assert shape_ctype getElementType kind == IntType is_trivial_reshape = len shape == shape == - in_oper dim_order = DimOrder PRESUMED_CONTIGUOUS is_trivial_reshape raise Exception noqa TRY Currently reshape only supported NHWC tensors target size X - Bit hack here Use real tensor infer output shape out_shape = torch zeros expand in_oper shape reshape shape shape out_oper = in_oper _replace shape=out_shape dim_order=DimOrder PRESUMED_CONTIGUOUS inputs = None inputs = in_id inputs = add_immediate_int_vector shape outputs = None outputs = add_tensor_operand node outputsAt out_oper add_operation NNAPI_OperationCode RESHAPE inputs outputs add_flatten node assert node inputsSize == assert node outputsSize == in_id in_oper = get_tensor_operand_by_jitval node inputsAt _start_ctype start_dim = get_constant_value node inputsAt IntType _end_ctype end_dim = get_constant_value node inputsAt IntType channels last channels == height width both is_trivial_flatten = len in_oper shape == in_oper shape == in_oper shape == in_oper shape == in_oper dim_order = DimOrder PRESUMED_CONTIGUOUS is_trivial_flatten raise Exception noqa TRY Currently flatten supported NHWC tensors unless C= H=W= start_dim start_dim += len in_oper shape end_dim end_dim += len in_oper shape out_shape = in_oper shape start_dim + functools reduce operator mul in_oper shape start_dim end_dim + + in_oper shape end_dim + any dim == dim in_oper shape start_dim end_dim + raise Exception noqa TRY Flattening flexible dims supported yet noqa TRY non_flattened_dims = in_oper shape start_dim + in_oper shape end_dim + non_flattened_dims count raise Exception Only dim can flexible noqa TRY out_oper = in_oper _replace shape=out_shape dim_order=DimOrder PRESUMED_CONTIGUOUS out_id = add_tensor_operand node outputsAt out_oper idx dim enumerate out_shape dim == forward_operand_shape out_id idx in_id in_oper shape index inputs_ = tuple dim dim = - dim out_shape inputs = None inputs = in_id inputs = add_immediate_int_vector inputs_ outputs = None outputs = out_id add_operation NNAPI_OperationCode RESHAPE inputs outputs add_slice node assert node inputsSize == assert node outputsSize == in_id in_oper = get_tensor_operand_by_jitval node inputsAt _ dim_value = get_constant_value node inputsAt _ start_value = get_constant_value node inputsAt _ stop_value = get_constant_value node inputsAt _ step_value = get_constant_value node inputsAt start_value None start_value = stop_value None stop_value = sys maxsize start_value start_value += in_oper shape dim_value start_value == sys maxsize start_value = start_value == stop_value == sys maxsize _identity node in_oper shape dim_value == raise Exception Unable slice flexible shape noqa TRY stop_value stop_value += in_oper shape dim_value stop_value == sys maxsize stop_value = in_oper shape dim_value start_value = stop_value raise Exception noqa TRY Slice start value should less than stop value noqa TRY out_len = stop_value - start_value step_value out_shape = tuple out_len i == dim_value dim i dim enumerate in_oper shape out_id = add_tensor_operand node outputsAt in_oper _replace shape=out_shape flex inputs end_mask = idx dim enumerate out_shape dim == forward_operand_shape out_id idx in_id idx end_mask &#124; = idx inputs = None inputs = in_id inputs = add_immediate_int_vector start_value i == dim_value i range len in_oper shape inputs = add_immediate_int_vector stop_value i == dim_value dim i dim enumerate in_oper shape inputs = add_immediate_int_vector step_value i == dim_value i range len in_oper shape inputs = add_immediate_int_scalar begin mask inputs = add_immediate_int_scalar end_mask inputs = add_immediate_int_scalar shrink axis mas outputs = None outputs = out_id add_operation NNAPI_OperationCode STRIDED_SLICE inputs outputs add_size node assert node inputsSize == assert node outputsSize == _ in_oper = get_tensor_operand_by_jitval_fixed_size node inputsAt _ value = constants node inputsAt res = in_oper shape value output = node outputsAt add_constant_value output output type res add_cat node assert node inputsSize == assert node outputsSize == tensors = tensor_sequences node inputsAt _ dim = get_constant_value node inputsAt IntType assert len tensors in_ids = out_oper = None out_dim_size = inp tensors in_id in_oper = get_tensor_operand_by_jitval inp out_oper None out_shape = change_element in_oper shape dim - out_oper = in_oper _replace shape=out_shape assert in_oper op_type == out_oper op_type assert in_oper dim_order == out_oper dim_order assert change_element in_oper shape dim - == change_element out_oper shape dim - TODO Possibly check scale zero point in_ids append in_id TODO Possibly support variable-sized inputs out_dim_size += in_oper shape dim assert out_oper None out_oper = out_oper _replace shape=change_element out_oper shape dim out_dim_size in_oper dim_order == DimOrder CHANNELS_LAST type ignore possibly-undefined assert len out_oper shape == nnapi_dim = dim nnapi_dim = dim out_id = add_tensor_operand node outputsAt out_oper idx d enumerate out_oper shape d == idx == dim shape = + join flex_name ip_id dim ip_id in_ids compute_operand_shape out_id idx shape forward_operand_shape out_id idx in_ids idx inputs = in_ids + add_immediate_int_scalar nnapi_dim outputs = None outputs = out_id add_operation NNAPI_OperationCode CONCATENATION inputs outputs add_mean node assert node inputsSize == assert node outputsSize == in_id in_oper = get_tensor_operand_by_jitval_fixed_size node inputsAt dim_ctype dim = get_constant_value node inputsAt assert dim_ctype kind == ListType assert dim_ctype getElementType kind == IntType _ keep_dim = get_constant_value node inputsAt BoolType Expect None dtype get_constant_value node inputsAt NoneType in_oper dim_order == DimOrder CHANNELS_LAST assert len in_oper shape == nnapi_dim = d d dim nnapi_dim = dim collapsed_dims = set d dim d d += len in_oper shape collapsed_dims add d in_oper dim_order == DimOrder CHANNELS_LAST keep_dim assert collapsed_dims issuperset out_dim_order = DimOrder PRESUMED_CONTIGUOUS out_dim_order = in_oper dim_order out_shape = i s enumerate in_oper shape i collapsed_dims out_shape append s keep_dim out_shape append out_oper = in_oper _replace shape=out_shape dim_order=out_dim_order inputs = None inputs = in_id inputs = add_immediate_int_vector nnapi_dim inputs = add_immediate_int_scalar keep_dim outputs = None outputs = add_tensor_operand node outputsAt out_oper add_operation NNAPI_OperationCode MEAN inputs outputs add_quantize node assert node inputsSize == assert node outputsSize == in_id in_oper = get_tensor_operand_by_jitval_fixed_size node inputsAt in_oper dim_order = DimOrder CHANNELS_LAST raise Exception noqa TRY Most hardware backends prefer NHWC quantized tensors Try setting ` t nnapi_nhwc = True ` your tensor inputs _ scale = get_constant_value node inputsAt FloatType _ zero_point = get_constant_value node inputsAt IntType _ scalar_type = get_constant_value node inputsAt IntType scalar_type = TorchScalarTypes QUINT value raise Exception noqa TRY PyTorch NNAPI export only supports quantized tensors quint dtype op_type = NNAPI_OperandCode TENSOR_QUANT _ASYMM out_oper = in_oper _replace op_type=op_type scale=scale zero_point=zero_point inputs = None inputs = in_id outputs = None outputs = add_tensor_operand node outputsAt out_oper add_operation NNAPI_OperationCode QUANTIZE inputs outputs add_dequantize node assert node inputsSize == assert node outputsSize == in_id in_oper = get_tensor_operand_by_jitval_fixed_size node inputsAt out_oper = in_oper _replace op_type=NNAPI_OperandCode TENSOR_FLOAT scale= zero_point= inputs = None inputs = in_id outputs = None outputs = add_tensor_operand node outputsAt out_oper add_operation NNAPI_OperationCode DEQUANTIZE inputs outputs add_pointwise_simple_unary_op node opcode assert node inputsSize == assert node outputsSize == in_id in_oper = get_tensor_operand_by_jitval node inputsAt out_oper = in_oper opcode == NNAPI_OperationCode LOGISTIC NNAPI docs For ANEURALNETWORKS_TENSOR_QUANT _ASYMM scale must f zeroPoint must https fburl com h stoog in_oper op_type == NNAPI_OperandCode TENSOR_QUANT _ASYMM out_oper = in_oper _replace zero_point= scale= out_id = add_tensor_operand node outputsAt out_oper idx dim enumerate in_oper shape dim == forward_operand_shape out_id idx in_id idx inputs = None inputs = in_id outputs = None outputs = out_id add_operation opcode inputs outputs _do_add_binary node opcode fuse_code qparams=None noqa D Helper pointwise binary broadcast ops superfluous extra args assert node outputsSize == assert node inputsAt type kind == TensorType assert node inputsAt type kind == TensorType has_operand_for_jitval node inputsAt _id _oper = get_tensor_operand_by_jitval node inputsAt _id _oper = get_tensor_operand_or_constant node inputsAt _oper dim_order has_operand_for_jitval node inputsAt _id _oper = get_tensor_operand_by_jitval node inputsAt _id _oper = get_tensor_operand_or_constant node inputsAt _oper dim_order raise Exception noqa TRY f Can t do NNAPI binary op opcode two constants noqa TRY assert _oper op_type == _oper op_type _id _oper _id _oper = transpose_for_broadcast _id _oper _id _oper NOTE PyTorch NNAPI have same broadcast semantics out_shape = broadcast_shapes _oper shape _oper shape out_oper = _oper _replace shape=out_shape qparams None scale zp = qparams out_oper = out_oper _replace scale=scale zero_point=zp out_id = add_tensor_operand node outputsAt out_oper idx d d enumerate zip _oper shape _oper shape d == d == forward_operand_shape out_id idx _id idx d == d == forward_operand_shape out_id idx _id idx d == d == flexible_shape_computation_lines append f assert flex_name _id idx == flex_name _id idx forward_operand_shape out_id idx _id idx inputs = None inputs = _id inputs = _id inputs = add_immediate_int_scalar fuse_code outputs = None outputs = out_id add_operation opcode inputs outputs add_pointwise_simple_binary_broadcast_op node opcode fuse_code assert node inputsSize == _do_add_binary node opcode fuse_code add_add_sub_op node opcode fuse_code assert node inputsSize == _ alpha = get_constant_value node inputsAt IntType alpha = raise Exception noqa TRY NNAPI does support add sub alpha noqa TRY _do_add_binary node opcode fuse_code add_qadd node opcode fuse_code assert node inputsSize == _ scale = get_constant_value node inputsAt FloatType _ zero_point = get_constant_value node inputsAt IntType _do_add_binary node opcode fuse_code qparams= scale zero_point add_softmax node assert node inputsSize == in_id in_oper = get_tensor_operand_by_jitval node inputsAt _ softmax_dim = get_constant_value node inputsAt IntType out_id = add_tensor_operand node outputsAt in_oper dim size enumerate in_oper shape size == forward_operand_shape out_id dim in_id dim inputs = None inputs = in_id inputs = add_immediate_float_scalar positive scaling factor exponent beta inputs = add_immediate_int_scalar softmax_dim outputs = None outputs = out_id add_operation NNAPI_OperationCode SOFTMAX inputs outputs add_hardtanh node assert node inputsSize == assert node outputsSize == in_id in_oper = get_tensor_operand_by_jitval_fixed_size node inputsAt _ min_val = get_constant_value node inputsAt FloatType _ max_val = get_constant_value node inputsAt FloatType op_map = - NNAPI_OperationCode RELU NNAPI_OperationCode RELU noqa E opcode = op_map get min_val max_val opcode None raise Exception noqa TRY NNAPI only supports hardtanh args - noqa TRY inputs = None inputs = in_id outputs = None outputs = add_tensor_operand node outputsAt in_oper add_operation opcode inputs outputs add_prelu_op node assert node inputsSize == assert node outputsSize == assert node inputsAt type kind == TensorType assert node inputsAt type kind == TensorType in_id in_oper = get_tensor_operand_by_jitval node inputsAt w_id w_oper = get_tensor_operand_for_weight node inputsAt assert len w_oper shape == assert w_oper shape w_oper shape in_oper use_nchw TODO Support adding trailing dims raise Exception noqa TRY Per-channel PReLU only supports channels_last right now out_id = add_tensor_operand node outputsAt in_oper dim size enumerate in_oper shape size pass dim = raise Exception noqa TRY PReLU requires fixed size dim dim noqa TRY forward_operand_shape out_id dim in_id dim inputs = None inputs = in_id inputs = w_id outputs = None outputs = out_id add_operation NNAPI_OperationCode PRELU inputs outputs add_pool d_node node opcode assert node inputsSize == assert node outputsSize == image kernel stride padding dilation _ceil_mode = node inputs stride = stride kernel TODO Validate ceil_mode semantics args = get_conv_pool_args_ d_from_jit get_size_arg kernel stride padding dilation args dilation_h = args dilation_w = raise Exception NNAPI does support dilated pooling noqa TRY image_id image_oper = get_tensor_operand_by_jitval_fixed_size image assert len image_oper shape == out_shape = get_conv_pool_shape image_oper shape args image_oper shape False use_nchw = image_oper use_nchw inputs = None inputs = image_id inputs = add_immediate_int_scalar args pad_l inputs = add_immediate_int_scalar args pad_r inputs = add_immediate_int_scalar args pad_t inputs = add_immediate_int_scalar args pad_b inputs = add_immediate_int_scalar args stride_w inputs = add_immediate_int_scalar args stride_h inputs = add_immediate_int_scalar args kernel_w inputs = add_immediate_int_scalar args kernel_h inputs = add_immediate_int_scalar NNAPI_FuseCode FUSED_NONE inputs = add_immediate_bool_scalar use_nchw outputs = None outputs = add_tensor_operand node outputsAt image_oper _replace shape=out_shape add_operation opcode inputs outputs add_avg_pool d node assert node inputsSize == assert node outputsSize == image kernel stride padding _ceil_mode count_include_pad divisor_override = node inputs _ count_include_pad_value = get_constant_value count_include_pad _ divisor_override_value = get_constant_value divisor_override count_include_pad_value divisor_override_value raise Exception noqa TRY NNAPI doesn t support count_include_pad=False divisor_override args = get_conv_pool_args_ d_from_jit get_size_arg kernel stride padding image_id image_oper = get_tensor_operand_by_jitval image assert len image_oper shape == out_shape = get_conv_pool_shape image_oper shape args image_oper shape False use_nchw = image_oper use_nchw inputs = None inputs = image_id inputs = add_immediate_int_scalar args pad_l inputs = add_immediate_int_scalar args pad_r inputs = add_immediate_int_scalar args pad_t inputs = add_immediate_int_scalar args pad_b inputs = add_immediate_int_scalar args stride_w inputs = add_immediate_int_scalar args stride_h inputs = add_immediate_int_scalar args kernel_w inputs = add_immediate_int_scalar args kernel_h inputs = add_immediate_int_scalar NNAPI_FuseCode FUSED_NONE inputs = add_immediate_bool_scalar use_nchw outputs = None out_id = add_tensor_operand node outputsAt image_oper _replace shape=out_shape _handle_conv_pool_flexible_input out_id image args False outputs = out_id add_operation NNAPI_OperationCode AVERAGE_POOL_ D inputs outputs add_adaptive_avg_pool d node assert node inputsSize == assert node outputsSize == image_id image_oper = get_tensor_operand_by_jitval_fixed_size node inputsAt assert len image_oper shape == size_ctype size_arg = get_constant_value node inputsAt assert size_ctype kind == ListType assert size_ctype getElementType kind == IntType size_arg = raise Exception noqa TRY NNAPI only supports adaptive_avg_pool d output size out_shape = image_oper shape + tuple size_arg use_nchw = image_oper use_nchw inputs = None inputs = image_id inputs = add_immediate_int_scalar inputs = add_immediate_int_scalar inputs = add_immediate_int_scalar inputs = add_immediate_int_scalar inputs = add_immediate_int_scalar inputs = add_immediate_int_scalar inputs = add_immediate_int_scalar image_oper shape inputs = add_immediate_int_scalar image_oper shape inputs = add_immediate_int_scalar NNAPI_FuseCode FUSED_NONE inputs = add_immediate_bool_scalar use_nchw outputs = None outputs = add_tensor_operand node outputsAt image_oper _replace shape=out_shape add_operation NNAPI_OperationCode AVERAGE_POOL_ D inputs outputs add_upsample_nearest d node assert node inputsSize == node inputsSize == assert node outputsSize == node inputsSize == image size_jit scale_jit = node inputs image size_jit scale_h_jit scale_w_jit = node inputs size_ctype size_arg = get_constant_value size_jit node inputsSize == scale_ctype scale_arg = get_constant_value scale_jit type ignore possibly-undefined scale_h_ctype scale_h_arg = get_constant_value scale_h_jit type ignore possibly-undefined scale_w_ctype _scale_w_arg = get_constant_value scale_w_jit type ignore possibly-undefined The only way -argument overload upsample_nearest d have been added graph without error scale_h scale_w arguments None assert scale_h_ctype kind == NoneType assert scale_w_ctype kind == NoneType scale_ctype = scale_h_ctype scale_arg = scale_h_arg image_id image_oper = get_tensor_operand_by_jitval image assert len image_oper shape == size_ctype kind = NoneType scale_ctype kind = NoneType raise Exception Size scale cannot both non-None noqa TRY size_ctype kind = NoneType assert size_ctype kind == ListType assert size_ctype getElementType kind == IntType assert scale_ctype kind == NoneType assert scale_arg None assert isinstance size_arg list assert size_arg assert all isinstance val int val size_arg len size_arg == size_arg = size_arg assert len size_arg == out_h = size_arg out_w = size_arg arg_h = add_immediate_int_scalar out_h arg_w = add_immediate_int_scalar out_w scale_ctype kind = NoneType assert scale_ctype kind == ListType assert scale_ctype getElementType kind == FloatType assert size_ctype kind == NoneType assert size_arg None assert isinstance scale_arg list assert scale_arg assert all isinstance val float val scale_arg len scale_arg == scale_arg = scale_arg assert len scale_arg == out_h = int scale_arg image_oper shape out_w = int scale_arg image_oper shape arg_h = add_immediate_float_scalar scale_arg arg_w = add_immediate_float_scalar scale_arg raise Exception Size scale cannot both None noqa TRY out_shape = image_oper shape image_oper shape out_h out_w use_nchw = image_oper use_nchw out_id = add_tensor_operand node outputsAt image_oper _replace shape=out_shape image_oper shape == image_oper shape == raise Exception Flexible batch channels supported noqa TRY Handle variable input size dim h w indices image_oper shape dim == size_ctype kind = NoneType pyrefly ignore unsupported-operation compute_operand_shape out_id dim size_arg dim - scale_ctype kind = NoneType compute_operand_shape out_id dim pyrefly ignore unsupported-operation f int scale_arg dim - flex_name image_id dim raise Exception noqa TRY Size scale cannot both None noqa TRY inputs = None inputs = image_id inputs = arg_w inputs = arg_h inputs = add_immediate_bool_scalar use_nchw outputs = None outputs = out_id add_operation NNAPI_OperationCode RESIZE_NEAREST_NEIGHBOR inputs outputs add_addmm node assert node inputsSize == assert node outputsSize == jit_bias jit_input jit_weight jit_beta jit_alpha = node inputs jitval jit_beta jit_alpha scale_ctype scale_value = get_constant_value jitval assert scale_ctype kind IntType FloatType scale_value = raise Exception noqa TRY NNAPI Fully-Connected does support alpha beta add_addmm_or_linear node True jit_input jit_weight jit_bias add_linear node assert node inputsSize == assert node outputsSize == jit_input jit_weight jit_bias = node inputs add_addmm_or_linear node False jit_input jit_weight jit_bias add_addmm_or_linear node transpose_weight jit_input jit_weight jit_bias input_id input_oper = get_tensor_operand_by_jitval jit_input bias_id bias_oper = get_tensor_operand_for_weight jit_bias assert len input_oper shape == assert len bias_oper shape == TODO Transform load time share weights CPU model _ weight_tensor = get_constant_value jit_weight TensorType assert len weight_tensor shape == transpose_weight nnapi_weight_tensor = weight_tensor t contiguous nnapi_weight_tensor = weight_tensor contiguous weight_id = add_tensor_operand_for_weight nnapi_weight_tensor weight_oper = operands weight_id out_shape = input_oper shape weight_oper shape out_id = add_tensor_operand node outputsAt input_oper _replace shape=out_shape input_oper shape == forward_operand_shape out_id input_id inputs = None inputs = input_id inputs = weight_id inputs = bias_id inputs = add_immediate_int_scalar NNAPI_FuseCode FUSED_NONE outputs = None outputs = out_id add_operation NNAPI_OperationCode FULLY_CONNECTED inputs outputs add_qlinear node assert node inputsSize == assert node outputsSize == jit_input jit_packed_weight jit_scale jit_zero_point = node inputs input_id input_oper = get_tensor_operand_by_jitval_fixed_size jit_input TODO Support automatic reshape assert len input_oper shape == _ out_scale = get_constant_value jit_scale FloatType _ out_zero_point = get_constant_value jit_zero_point IntType weight_ctype packed_weight = get_constant_value jit_packed_weight assert weight_ctype name == LinearPackedParamsBase raw_weight raw_bias = packed_weight __getstate__ assert raw_bias None assert len raw_weight shape == assert len raw_bias shape == assert raw_bias shape == raw_weight shape assert raw_weight shape == input_oper shape assert raw_weight qscheme == torch per_tensor_affine raw_weight dtype == torch quint unsigned_weight = raw_weight assert raw_weight dtype == torch qint unsigned_weight = torch _make_per_tensor_quantized_tensor raw_weight int_repr int + torch uint scale=raw_weight q_scale zero_point=raw_weight q_zero_point + weight_scale = unsigned_weight q_scale bias_scale = input_oper scale weight_scale int_bias = torch quantize_per_tensor raw_bias bias_scale torch qint bias_id = add_tensor_operand_for_weight int_bias multiplier = input_oper scale weight_scale out_scale assert multiplier multiplier = raise Exception noqa TRY Quantized convolution multiplier greater than This supported NNAPI most hardware backends Try training model without quantization-aware training TODO Transform load time share weights CPU model nnapi_weight_tensor = unsigned_weight contiguous weight_id = add_tensor_operand_for_weight nnapi_weight_tensor weight_oper = operands weight_id out_shape = input_oper shape weight_oper shape out_oper = input_oper _replace shape=out_shape scale=out_scale zero_point=out_zero_point inputs = None inputs = input_id inputs = weight_id inputs = bias_id inputs = add_immediate_int_scalar NNAPI_FuseCode FUSED_NONE outputs = None outputs = add_tensor_operand node outputsAt out_oper add_operation NNAPI_OperationCode FULLY_CONNECTED inputs outputs get_optional_bias jit_bias weight_tensor transpose=False ctype _value = get_constant_value jit_bias ctype kind == NoneType bias_idx = transpose nnapi_bias_tensor = torch zeros weight_tensor size bias_idx dtype=weight_tensor dtype bias_id = add_tensor_operand_for_weight nnapi_bias_tensor bias_oper = operands bias_id bias_id bias_oper get_tensor_operand_for_weight jit_bias add_conv d node assert node inputsSize == assert node outputsSize == jit_image jit_weight jit_bias jit_stride jit_pad jit_dilation jit_groups = node inputs _ weight_tensor = get_constant_value jit_weight TensorType bias_id _bias_oper = get_optional_bias jit_bias weight_tensor args = get_conv_pool_args_ d_from_jit weight_tensor shape jit_stride jit_pad jit_dilation jit_groups add_conv d_common node outputsAt jit_image weight_tensor bias_id args False transpose NNAPI_FuseCode FUSED_NONE add_conv_underscore node assert node inputsSize == assert node outputsSize == jit_image jit_weight jit_bias jit_stride jit_pad jit_dilation jit_transpose _ jit_groups _ _ _ _ = node inputs _ weight_tensor = get_constant_value jit_weight TensorType _ transpose = get_constant_value jit_transpose bias_id _bias_oper = get_optional_bias jit_bias weight_tensor transpose args = get_conv_pool_args_ d_from_jit weight_tensor shape jit_stride jit_pad jit_dilation jit_groups add_conv d_common node outputsAt jit_image weight_tensor bias_id args transpose NNAPI_FuseCode FUSED_NONE add_log_softmax node assert node inputsSize == assert node outputsSize == jit_input jit_dim _jit_half_to_float = node inputs input_id input_oper = get_tensor_operand_by_jitval_fixed_size jit_input _ dim = get_constant_value jit_dim IntType out_shape = input_oper shape inputs = None inputs = input_id specifying scaling factor exponent beta inputs = add_immediate_float_scalar inputs = add_immediate_int_scalar dim outputs = None outputs = add_tensor_operand node outputsAt input_oper _replace shape=out_shape add_operation NNAPI_OperationCode LOG_SOFTMAX inputs outputs add_qconv d node fuse_code transpose=False assert node inputsSize == assert node outputsSize == jit_image jit_packed_weight jit_scale jit_zero_point = node inputs _ out_scale = get_constant_value jit_scale FloatType _ out_zero_point = get_constant_value jit_zero_point IntType weight_ctype packed_weight = get_constant_value jit_packed_weight assert weight_ctype name == Conv dPackedParamsBase pack_version tensors opt_tensors = packed_weight __getstate__ assert pack_version == packed_config raw_weight = tensors raw_bias = opt_tensors assert raw_bias None args = get_conv_pool_args_ d_from_pack raw_weight shape packed_config assert raw_weight qscheme == torch per_tensor_affine raw_weight dtype == torch quint unsigned_weight = raw_weight assert raw_weight dtype == torch qint unsigned_weight = torch _make_per_tensor_quantized_tensor raw_weight int_repr int + torch uint scale=raw_weight q_scale zero_point=raw_weight q_zero_point + weight_scale = unsigned_weight q_scale _ image_oper = get_tensor_operand_by_jitval jit_image bias_scale = image_oper scale weight_scale int_bias = torch quantize_per_tensor raw_bias bias_scale torch qint bias_id = add_tensor_operand_for_weight int_bias multiplier = image_oper scale weight_scale out_scale assert multiplier multiplier = raise Exception noqa TRY Quantized convolution multiplier greater than This supported NNAPI most hardware backends Try training model without quantization-aware training add_conv d_common node outputsAt out_scale out_zero_point jit_image unsigned_weight bias_id args transpose fuse_code add_conv d_common jit_out out_scale out_zero_point jit_image weight_tensor bias_id args transpose fuse_code image_id image_oper = get_tensor_operand_by_jitval jit_image in_c = image_oper shape args group == Full convolution depthwise = False transpose weight_permutation = weight_permutation = args group == in_c Depthwise convolution depthwise = True weight_permutation = raise Exception Group convolution supported yet noqa TRY TODO Transform load time share weights CPU model nnapi_weight_tensor = weight_tensor permute weight_permutation contiguous weight_id = add_tensor_operand_for_weight nnapi_weight_tensor weight_oper = operands weight_id bias_oper = operands bias_id image_oper op_type == NNAPI_OperandCode TENSOR_FLOAT assert weight_oper op_type == NNAPI_OperandCode TENSOR_FLOAT assert bias_oper op_type == NNAPI_OperandCode TENSOR_FLOAT image_oper op_type == NNAPI_OperandCode TENSOR_QUANT _ASYMM assert weight_oper op_type == NNAPI_OperandCode TENSOR_QUANT _ASYMM assert bias_oper op_type == NNAPI_OperandCode TENSOR_INT assert approx_equal image_oper scale weight_oper scale bias_oper scale assert bias_oper zero_point == raise Exception noqa TRY f Unsupported input type conv d image_oper op_type noqa TRY assert len image_oper shape == assert len weight_oper shape == assert len bias_oper shape == depthwise Depthwise convolution one _kern_h _kern_w out_c = weight_oper shape assert one == assert out_c in_c == channel_multiplier = out_c in_c assert channel_multiplier == Don t support multiplier assert out_c == in_c Full convolution out_c _kern_h _kern_w kern_d = weight_oper shape assert kern_d == in_c assert out_c == bias_oper shape use_nchw = image_oper use_nchw depthwise num_args = opcode = NNAPI_OperationCode DEPTHWISE_CONV_ D num_args = transpose opcode = NNAPI_OperationCode TRANSPOSE_CONV_ D opcode = NNAPI_OperationCode CONV_ D inputs = None num_args inputs = image_id inputs = weight_id inputs = bias_id inputs = add_immediate_int_scalar args pad_l inputs = add_immediate_int_scalar args pad_r inputs = add_immediate_int_scalar args pad_t inputs = add_immediate_int_scalar args pad_b inputs = add_immediate_int_scalar args stride_w inputs = add_immediate_int_scalar args stride_h depthwise inputs = add_immediate_int_scalar inputs = add_immediate_int_scalar fuse_code inputs = add_immediate_bool_scalar use_nchw inputs = add_immediate_int_scalar fuse_code inputs = add_immediate_bool_scalar use_nchw outputs = None out_shape = get_conv_pool_shape image_oper shape args out_c transpose out_oper = image_oper _replace shape=out_shape scale=out_scale zero_point=out_zero_point out_id = add_tensor_operand jit_out out_oper _handle_conv_pool_flexible_input out_id jit_image args transpose outputs = out_id add_operation opcode inputs outputs _handle_conv_pool_flexible_input out_id jit_image args transpose image_id image_oper = get_tensor_operand_by_jitval jit_image batch in_ch in_h in_w = image_oper shape batch == forward_operand_shape out_id image_id in_ch == raise Exception Input channels can t flexible noqa TRY H W transpose in_h == compute_operand_shape out_id f flex_name image_id - args stride_h + args kernel_h - args pad_t - args pad_b in_w == compute_operand_shape out_id f flex_name image_id - args stride_w + args kernel_w - args pad_l - args pad_r in_h == compute_operand_shape out_id f flex_name image_id - args kernel_h + args pad_t + args pad_b args stride_h + in_w == compute_operand_shape out_id f flex_name image_id - args kernel_w + args pad_l + args pad_r args stride_w + serialize_model module inputs config=None return_shapes=None use_int _for_qint =False Convert NNAPI serialize torchscript module Parameters module Torchscript module convert inputs Tensors used specify input details NNAPI config optional Optional config attach module return_shapes optional Specify shape outputs your module uses runtime flexible shapes set output buffer size NNAPI use_int _for_qint optional Use Pytorch int represent NNAPI qint values _NnapiSerializer config use_int _for_qint serialize_model module inputs return_shapes