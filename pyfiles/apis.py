mypy allow-untyped-defs NOTE We allow Dynamo see file via torch _dynamo trace_rules py so can trace through functorch transforms Currently we can t allow Dynamo see ` eager_transforms py ` ` vmap py ` break lot thing there isn t mechanism selectively expose only some functions eg grad file Dynamo functools torch _functorch utils argnums_t exposed_in torch _functorch vmap _check_out_dims_is_int_or_int_pytree _check_randomness_arg _chunked_vmap _process_batched_inputs Callable in_dims_t out_dims_t vmap_impl vmap func inputs wraps all Tensor inputs batched BatchedTensors sends those into func then unwraps output BatchedTensors Operations BatchedTensors perform batched operations user asking vmap s randomness behavior differs JAX s which would require PRNG key passed everywhere exposed_in torch func vmap func Callable in_dims in_dims_t = out_dims out_dims_t = randomness str = error chunk_size=None - Callable vmap vectorizing map ` ` vmap func ` ` returns new function maps ` ` func ` ` over some dimension inputs Semantically vmap pushes map into PyTorch operations called ` ` func ` ` effectively vectorizing those operations vmap useful handling batch dimensions one can write function ` ` func ` ` runs examples then lift function can take batches examples ` ` vmap func ` ` vmap can also used compute batched gradients when composed autograd note func ` torch vmap ` aliased func ` torch func vmap ` convenience Use whichever one you d like Args func function A Python function takes one more arguments Must one more Tensors in_dims int nested structure Specifies which dimension inputs should mapped over ` ` in_dims ` ` should have structure like inputs If ` ` in_dim ` ` particular input None then indicates there no map dimension Default out_dims int Tuple int Specifies where mapped dimension should appear outputs If ` ` out_dims ` ` Tuple then should have one element per output Default randomness str Specifies whether randomness vmap should same different across batches If different randomness each batch will different If same randomness will same across batches If error any calls random functions will error Default error WARNING flag only applies random PyTorch operations does apply Python s random module numpy randomness chunk_size None int If None default apply single vmap over inputs If None then compute vmap attr ` chunk_size ` samples time Note attr ` chunk_size= ` equivalent computing vmap for-loop If you run into memory issues computing vmap please try non-None chunk_size Returns Returns new batched function It takes same inputs ` ` func ` ` except each input has extra dimension index specified ` ` in_dims ` ` It takes returns same outputs ` ` func ` ` except each output has extra dimension index specified ` ` out_dims ` ` warning func ` vmap ` works best functional-style code Please do perform any side-effects ` ` func ` ` exception in-place PyTorch operations Examples side-effects include mutating Python data structures assigning values variables captured ` ` func ` ` One example using func ` vmap ` compute batched dot products PyTorch doesn t provide batched ` ` torch dot ` ` API instead unsuccessfully rummaging through docs use func ` vmap ` construct new function torch dot D D - batched_dot = torch func vmap torch dot N D N D - N x y = torch randn torch randn batched_dot x y func ` vmap ` can helpful hiding batch dimensions leading simpler model authoring experience batch_size feature_size = weights = torch randn feature_size requires_grad=True model feature_vec Very simple linear model activation feature_vec dot weights relu examples = torch randn batch_size feature_size result = torch vmap model examples func ` vmap ` can also help vectorize computations previously difficult impossible batch One example higher-order gradient computation The PyTorch autograd engine computes vjps vector-Jacobian products Computing full Jacobian matrix some function f R^N - R^N usually requires N calls ` ` autograd grad ` ` one per Jacobian row Using func ` vmap ` we can vectorize whole computation computing Jacobian single call ` ` autograd grad ` ` Setup N = f = lambda x x x = torch randn N requires_grad=True y = f x I_N = torch eye N Sequential approach jacobian_rows = torch autograd grad y x v retain_graph=True v I_N unbind jacobian = torch stack jacobian_rows vectorized gradient computation get_vjp v torch autograd grad y x v jacobian = torch vmap get_vjp I_N func ` vmap ` can also nested producing output multiple batched dimensions torch dot D D - batched_dot = torch vmap torch vmap torch dot N N D N N D - N N x y = torch randn torch randn batched_dot x y tensor size If inputs batched along first dimension ` ` in_dims ` ` specifies dimension each inputs batched along torch dot N N - batched_dot = torch vmap torch dot in_dims= N D N D - D x y = torch randn torch randn batched_dot x y output instead batched along th dimension If there multiple inputs each which batched along different dimensions ` ` in_dims ` ` must tuple batch dimension each input torch dot D D - batched_dot = torch vmap torch dot in_dims= None N D D - N x y = torch randn torch randn batched_dot x y second arg doesn t have batch dim because in_dim None If input Python struct ` ` in_dims ` ` must tuple containing struct matching shape input f = lambda dict torch dot dict x dict y x y = torch randn torch randn input = x x y y batched_dot = torch vmap f in_dims= x y None batched_dot input By default output batched along first dimension However can batched along any dimension using ` ` out_dims ` ` f = lambda x x x = torch randn batched_pow = torch vmap f out_dims= batched_pow x For any function uses kwargs returned function will batch kwargs will accept kwargs x = torch randn fn x scale= x scale batched_pow = torch vmap fn assert torch allclose batched_pow x x batched_pow x scale=x scale batched output has shape note vmap does provide general autobatching handle variable-length sequences out box torch compiler is_compiling _check_randomness_arg randomness chunk_size None chunk_size raise ValueError f vmap chunk_size should None greater than got chunk_size wrapped args kwargs vmap_impl func in_dims out_dims randomness chunk_size args kwargs is_compiling wrapped = functools wraps func wrapped wrapped chunk_vmap func Callable in_dims in_dims_t = out_dims out_dims_t = randomness str = error chunks= - Callable chunk_vmap vectorizing map vmap using chunks input data It mix vmap which vectorizes everything map which executes things sequentially ` ` chunk_vmap ` ` vectorizes input number chunks time For more details about vectorizing map see func ` vmap ` note Please use func ` vmap ` ` ` chunk_size ` ` argument instead API Args func function A Python function takes one more arguments Must one more Tensors in_dims int nested structure Specifies which dimension inputs should mapped over ` ` in_dims ` ` should have structure like inputs If ` ` in_dim ` ` particular input None then indicates there no map dimension Default out_dims int Tuple int Specifies where mapped dimension should appear outputs If ` ` out_dims ` ` Tuple then should have one element per output Default randomness str Specifies whether randomness vmap should same different across batches If different randomness each batch will different If same randomness will same across batches If error any calls random functions will error Default error WARNING flag only applies random PyTorch operations does apply Python s random module numpy randomness chunks int Number chunks use split input data Default If equals then func ` vmap ` called Returns Returns new batched function It takes same inputs ` ` func ` ` except each input has extra dimension index specified ` ` in_dims ` ` It takes returns same outputs ` ` func ` ` except each output has extra dimension index specified ` ` out_dims ` ` _check_randomness_arg randomness chunks == vmap func in_dims=in_dims out_dims=out_dims randomness=randomness _get_chunk_flat_args flat_args_ flat_in_dims_ chunks_ flat_args_chunks = tuple t chunk chunks_ dim=in_dim in_dim None t chunks_ t in_dim zip flat_args_ flat_in_dims_ transpose chunk dim flatten structure chunks_flat_args list flatten args chunks_flat_args = zip flat_args_chunks chunks_flat_args functools wraps func wrapped_with_chunks args kwargs _check_out_dims_is_int_or_int_pytree out_dims func _ flat_in_dims flat_args args_spec = _process_batched_inputs in_dims args func Chunk flat arguments chunks_flat_args = _get_chunk_flat_args flat_args flat_in_dims chunks Apply vmap chunks _chunked_vmap func flat_in_dims chunks_flat_args args_spec out_dims randomness kwargs wrapped_with_chunks exposed_in torch func grad func Callable argnums argnums_t = has_aux bool = False - Callable ` ` grad ` ` operator helps computing gradients ` ` func ` ` respect input s specified ` ` argnums ` ` This operator can nested compute higher-order gradients Args func Callable A Python function takes one more arguments Must single-element Tensor If specified ` ` has_aux ` ` equals ` ` True ` ` function can tuple single-element Tensor other auxiliary objects ` ` output aux ` ` argnums int Tuple int Specifies arguments compute gradients respect ` ` argnums ` ` can single integer tuple integers Default has_aux bool Flag indicating ` ` func ` ` returns tensor other auxiliary objects ` ` output aux ` ` Default False Returns Function compute gradients respect its inputs By default output function gradient tensor s respect first argument If specified ` ` has_aux ` ` equals ` ` True ` ` tuple gradients output auxiliary objects returned If ` ` argnums ` ` tuple integers tuple output gradients respect each ` ` argnums ` ` value returned Example using ` ` grad ` ` xdoctest +SKIP torch func grad x = torch randn cos_x = grad lambda x torch sin x x assert torch allclose cos_x x cos Second-order gradients neg_sin_x = grad grad lambda x torch sin x x assert torch allclose neg_sin_x -x sin When composed ` ` vmap ` ` ` ` grad ` ` can used compute per-sample-gradients xdoctest +SKIP torch func grad vmap batch_size feature_size = model weights feature_vec Very simple linear model activation assert feature_vec dim == feature_vec dot weights relu compute_loss weights example target y = model weights example y - target mean MSELoss weights = torch randn feature_size requires_grad=True examples = torch randn batch_size feature_size targets = torch randn batch_size inputs = weights examples targets grad_weight_per_example = vmap grad compute_loss in_dims= None inputs Example using ` ` grad ` ` ` ` has_aux ` ` ` ` argnums ` ` xdoctest +SKIP torch func grad my_loss_func y y_pred loss_per_sample = y_pred - y loss = loss_per_sample mean loss y_pred loss_per_sample fn = grad my_loss_func argnums= has_aux=True y_true = torch rand y_preds = torch rand requires_grad=True out = fn y_true y_preds output grads w r t y_true grads w r t y_preds y_pred loss_per_sample note Using PyTorch ` ` torch no_grad ` ` together ` ` grad ` ` Case Using ` ` torch no_grad ` ` inside function xdoctest +SKIP f x torch no_grad c = x x - c In case ` ` grad f x ` ` will respect inner ` ` torch no_grad ` ` Case Using ` ` grad ` ` inside ` ` torch no_grad ` ` context manager xdoctest +SKIP torch no_grad grad f x In case ` ` grad ` ` will respect inner ` ` torch no_grad ` ` outer one This because ` ` grad ` ` function transform its result should depend result context manager outside ` ` f ` ` To avoid cyclical dependency torch _functorch eager_transforms eager_transforms torch compiler is_compiling wrapper args kwargs eager_transforms grad_impl func argnums has_aux args kwargs is_compiling wrapper = functools wraps func wrapper wrapper exposed_in torch func grad_and_value func Callable argnums argnums_t = has_aux bool = False - Callable Returns function compute tuple gradient primal forward computation Args func Callable A Python function takes one more arguments Must single-element Tensor If specified ` ` has_aux ` ` equals ` ` True ` ` function can tuple single-element Tensor other auxiliary objects ` ` output aux ` ` argnums int Tuple int Specifies arguments compute gradients respect ` ` argnums ` ` can single integer tuple integers Default has_aux bool Flag indicating ` ` func ` ` returns tensor other auxiliary objects ` ` output aux ` ` Default False Returns Function compute tuple gradients respect its inputs forward computation By default output function tuple gradient tensor s respect first argument primal computation If specified ` ` has_aux ` ` equals ` ` True ` ` tuple gradients tuple forward computation output auxiliary objects returned If ` ` argnums ` ` tuple integers tuple tuple output gradients respect each ` ` argnums ` ` value forward computation returned See func ` grad ` examples torch _functorch eager_transforms torch compiler is_compiling wrapper args kwargs eager_transforms grad_and_value_impl func argnums has_aux args kwargs is_compiling wrapper = functools wraps func wrapper wrapper