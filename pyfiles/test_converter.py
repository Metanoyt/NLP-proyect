Owner s oncall export unittest collections OrderedDict typing Any Optional torch torch utils _pytree pytree torch _dynamo test_case TestCase torch _export converter TS EPConverter torch export ExportedProgram torch testing _internal common_quantized override_quantized_engine torch testing _internal common_utils IS_WINDOWS run_tests xfailIfS X torch testing _internal torchbind_impls _empty_tensor_queue init_torchbind_implementations requires_cuda = unittest skipUnless torch cuda is_available requires cuda TestConverter TestCase setUp init_torchbind_implementations torch_bind_ops = torch ops _TorchScriptTesting queue_pop torch ops _TorchScriptTesting queue_push torch ops _TorchScriptTesting queue_size tearDown _check_equal_ts_ep_converter M tracing_inputs option Optional list str = None check_persistent=False lifted_tensor_constants=None runtime_inputs Optional list Any = None - list ExportedProgram By default tests both jit trace jit script option None option = trace script check_persistent num_iterations = num_iterations = ep_list = opt option opt == script Separate two models testing non-functional effects check_persistent original_ts_model = torch jit script M ts_model = torch jit script M eager_model = M original_ts_model = torch jit script M ts_model = torch jit script M eager_model = M opt == trace check_persistent original_ts_model = torch jit trace M tracing_inputs ts_model = torch jit trace M tracing_inputs eager_model = M original_ts_model = torch jit trace M tracing_inputs ts_model = torch jit trace M tracing_inputs eager_model = M raise RuntimeError f Unrecognized mode torch jit opt converter = TS EPConverter ts_model tracing_inputs ep = converter convert ep_list append ep runtime_inputs None runtime_inputs = inp tracing_inputs + runtime_inputs _ range num_iterations orig_out _ = pytree tree_flatten original_ts_model inp ep_out _ = pytree tree_flatten ep module inp Check module isinstance eager_model torch nn Module expected_state_dict = OrderedDict expected_state_dict update ts_model state_dict lifted_tensor_constants expected_state_dict update lifted_tensor_constants assertEqual ep state_dict keys expected_state_dict keys Check results _check_tensor_list_equal ep_out orig_out ep_list _check_tensor_list_equal xs list torch Tensor ys list torch Tensor assertEqual len xs len ys x y zip xs ys isinstance x torch Tensor isinstance y torch Tensor assertEqual x shape y shape assertTrue torch allclose x y assertEqual type x type y assertEqual x y test_ts ep_converter_basic MSingle torch nn Module forward x y x + y MMulti torch nn Module forward x y x = x cos + y = y sin - x y inp = torch ones torch ones runtime_inps = torch ones torch ones torch ones torch ones _check_equal_ts_ep_converter MSingle inp runtime_inputs=runtime_inps _check_equal_ts_ep_converter MMulti inp runtime_inputs=runtime_inps test_ts ep_converter_container_output Output List MOutputList torch nn Module forward x torch Tensor y torch Tensor = x x b = y + y b Output Tuple MOutputTuple torch nn Module forward x torch Tensor y torch Tensor = x x b = y + y b Output Dict MOutputDict torch nn Module forward x torch Tensor y torch Tensor = x x b = y + y data mul add b inp = torch tensor torch tensor runtime_inputs = torch tensor torch tensor torch tensor torch tensor Traced function must use immutable structure output _check_equal_ts_ep_converter MOutputList inp script runtime_inputs=runtime_inputs _check_equal_ts_ep_converter MOutputTuple inp runtime_inputs=runtime_inputs _check_equal_ts_ep_converter MOutputDict inp script runtime_inputs=runtime_inputs test_aten_dim Module torch nn Module forward x num_dim = x dim torch ones num_dim inp = torch ones _check_equal_ts_ep_converter Module inp runtime_inputs= torch ones test_aten_len Module torch nn Module forward x torch Tensor length = len x torch ones length aten len Tensor inp = torch ones _check_equal_ts_ep_converter Module inp Module torch nn Module forward x list int length = len x torch ones length aten len t inp = _check_equal_ts_ep_converter Module inp script Module torch nn Module forward x dict int str length = len x torch ones length aten len Dict_int inp = b c _check_equal_ts_ep_converter Module inp script Module torch nn Module forward x dict bool str length = len x torch ones length aten len Dict_bool inp = True False b _check_equal_ts_ep_converter Module inp script Module torch nn Module forward x dict float str length = len x torch ones length aten len Dict_float inp = b _check_equal_ts_ep_converter Module inp script Module torch nn Module forward x dict torch Tensor str length = len x torch ones length aten len Dict_Tensor inp = torch zeros torch ones b _check_equal_ts_ep_converter Module inp script aten len str aten len Dict_str supported since torch _C _jit_flatten does support str inp = abcdefg _check_equal_ts_ep_converter Module inp inp = b _check_equal_ts_ep_converter Module inp test_aten_add_t python list append Module torch nn Module forward x list torch Tensor out = out = out + x = torch cat out out = out + x b = torch cat out b inp = torch ones torch ones runtime_inputs = torch ones torch ones torch ones torch ones _check_equal_ts_ep_converter Module inp script runtime_inputs=runtime_inputs test_aten_to_dtype_with_mutating_storage Module torch nn Module forward x torch Tensor y torch Tensor x = x y dtype torch ops aten index_put_ x torch tensor y x inp = torch ones torch tensor _check_equal_ts_ep_converter Module inp test_prim_min Module torch nn Module forward x torch Tensor y torch Tensor - torch Tensor x_len = len x y_len = len y prim min int len_int = min x_len y_len prim min float len_float = int min x_len y_len prim min self_int len_self_int = min x_len y_len prim min self_float len_self_float = int min x_len y_len prim min float_int len_float_int = int min x_len y_len prim min int_float len_int_float = int min x_len y_len torch ones len_int + len_float + len_self_int + len_self_float + len_float_int + len_int_float inp = torch randn torch randn _check_equal_ts_ep_converter Module inp test_prim_max Module torch nn Module forward x torch Tensor y torch Tensor - torch Tensor x_len = len x y_len = len y prim max int len_int = max x_len y_len prim max float len_float = int max x_len y_len prim max self_int len_self_int = max x_len y_len prim max self_float len_self_float = int max x_len y_len prim max float_int len_float_int = int max x_len y_len prim max int_float len_int_float = int max x_len y_len torch ones len_int + len_float + len_self_int + len_self_float + len_float_int + len_int_float inp = torch randn torch randn _check_equal_ts_ep_converter Module inp test_aten___getitem___list Module torch nn Module forward x y = torch split x y inp = torch rand runtime_inps = torch rand _check_equal_ts_ep_converter Module inp runtime_inputs=runtime_inps test_aten___getitem___dict Module torch nn Module forward x y = torch split x d_int = y y d_str = y y d_bool = True y False y d_float = y y d_int d_str d_bool True d_float inp = torch rand _check_equal_ts_ep_converter Module inp test_prim_device Module torch nn Module forward x device = x device torch ones device=device inp = torch rand _check_equal_ts_ep_converter Module inp requires_cuda test_prim_device_cuda Module torch nn Module forward x device = x device torch ones device=device inp = torch rand device= cuda _check_equal_ts_ep_converter Module inp test_prim_dtype Module torch nn Module forward x dtype = x dtype torch ones dtype=dtype dtype torch float torch double inp = torch rand dtype=dtype _check_equal_ts_ep_converter Module inp dtype torch uint torch int torch int inp = torch randint high= size= dtype=dtype _check_equal_ts_ep_converter Module inp test_convert_if_basic M torch nn Module forward x torch Tensor y torch Tensor x y y y + y inp = torch tensor True torch tensor ep_list = _check_equal_ts_ep_converter M inp ep ep_list torch testing assert_close ep module torch tensor False torch tensor M torch tensor False torch tensor test_convert_if_tuple_out M torch nn Module true_fn y z z z z + z false_fn y z y y y y + y forward x torch Tensor y torch Tensor z = y y x res = true_fn y z res = false_fn y z res + res inp = torch tensor True torch tensor ep_list = _check_equal_ts_ep_converter M inp ep ep_list torch testing assert_close ep module torch tensor False torch tensor M torch tensor False torch tensor test_convert_if_multiple_out M torch nn Module true_fn y z z z false_fn y z y y y forward x torch Tensor y torch Tensor z = y y x res = true_fn y z res = y res = z res = false_fn y z res + res inp = torch tensor True torch tensor ep_list = _check_equal_ts_ep_converter M inp ep ep_list torch testing assert_close ep module torch tensor False torch tensor M torch tensor False torch tensor test_profiler__record_function Module torch nn Module forward x torch Tensor - torch Tensor handle = torch ops profiler _record_function_enter_new foo None y = x + torch ops profiler _record_function_exit handle y x = torch randn _check_equal_ts_ep_converter Module x test_aten_floordiv Module torch nn Module forward x torch Tensor - torch Tensor x x = torch randn _check_equal_ts_ep_converter Module x test_aten___is__ Module torch nn Module forward x torch Tensor y torch Tensor - tuple bool torch Tensor z = x + x y z Traced function must output has tensors inp = torch randn torch rand runtime_inps = torch randn torch rand _check_equal_ts_ep_converter Module inp script runtime_inputs=runtime_inps test_aten___isnot__ Module torch nn Module forward x torch Tensor y torch Tensor - tuple bool torch Tensor z = x + x y z Traced function must output has tensors inp = torch randn torch rand _check_equal_ts_ep_converter Module inp script test_aten___not__ Module torch nn Module forward x torch Tensor y torch Tensor - tuple bool torch Tensor z = x + x y z Traced function must output has tensors inp = torch randn torch rand _check_equal_ts_ep_converter Module inp script test_ts ep_converter_unpack MUnpackList torch nn Module forward x x y = torch split x x + y MUnpackTuple torch nn Module forward x_tuple tuple torch Tensor torch Tensor x y = x_tuple x = x cos x + y inp = torch ones _check_equal_ts_ep_converter MUnpackList inp inp = torch zeros torch ones _check_equal_ts_ep_converter MUnpackTuple inp unittest skipIf IS_WINDOWS torch cond doesn t go through torch compile windows causing output normalized list test_convert_retrace_nested_scripted_modules Wrapper torch nn Module __init__ mod - None super __init__ mod = mod forward x y mod x y LinearM torch nn Module __init__ dim int - None super __init__ linear = torch nn Linear dim dim forward x y linear y M torch nn Module __init__ dim int - None super __init__ m = LinearM dim m = torch jit script m mod = m mod = Wrapper m forward x torch Tensor y torch Tensor x -self mod x y - mod x y -self mod x y + mod x y NestedM torch nn Module __init__ dim int - None super __init__ m = M dim m = torch jit script m mod = m mod = Wrapper m forward x torch Tensor y torch Tensor x mod x y + mod x y mod x y - mod x y inp = torch tensor True torch randn _check_equal_ts_ep_converter NestedM inp test_convert_nn_module_with_nested_param M torch nn Module __init__ dim int - None super __init__ linear = torch nn Linear dim dim forward x torch Tensor linear x NestedM torch nn Module __init__ dim int - None super __init__ linear = torch nn Linear dim dim m = M dim forward x torch Tensor linear m x SuperNestedM torch nn Module __init__ dim int - None super __init__ linear = torch nn Linear dim dim m = NestedM dim forward x torch Tensor linear m x inp = torch ones orig_m = NestedM _check_equal_ts_ep_converter orig_m inp orig_m = SuperNestedM _check_equal_ts_ep_converter orig_m inp test_convert_nn_module_with_nested_buffer M torch nn Module __init__ - None super __init__ w = torch nn Buffer torch randn forward x torch Tensor w + x NestedM torch nn Module __init__ - None super __init__ m = M w = torch nn Buffer torch randn forward x torch Tensor w + m x SuperNestedM torch nn Module __init__ - None super __init__ m = NestedM w = torch nn Buffer torch randn forward x torch Tensor w + m x inp = torch ones orig_m = NestedM _check_equal_ts_ep_converter orig_m inp orig_m = SuperNestedM _check_equal_ts_ep_converter orig_m inp test_convert_nn_module_with_nested_if_and_buffer M torch nn Module __init__ - None super __init__ w = torch nn Buffer torch randn count = forward x torch Tensor w + x + count NestedM torch nn Module __init__ - None super __init__ m = M m = M w = torch nn Buffer torch randn forward x torch Tensor torch sum x w + m x w + m x Super nested parameters need lifted multiple times SuperNestedM torch nn Module __init__ - None super __init__ m = NestedM m = NestedM w = torch nn Buffer torch randn forward x torch Tensor torch max x w + m x w + m x Super nested module testing inp = torch ones orig_m = SuperNestedM ep_list = _check_equal_ts_ep_converter orig_m inp t = inp t -= ep ep_list torch testing assert_close ep module inp orig_m inp unittest skipIf IS_WINDOWS torch cond doesn t go through torch compile windows causing output normalized list test_convert_nn_module_with_nested_if_and_param M torch nn Module __init__ dim int - None super __init__ linear = torch nn Linear dim dim forward x torch Tensor linear x NestedM torch nn Module __init__ dim int - None super __init__ m = M dim m = M dim linear = torch nn Linear dim dim forward x torch Tensor torch sum x linear m x linear m x Super nested parameters need lifted multiple times SuperNestedM torch nn Module __init__ dim int - None super __init__ m = NestedM dim m = NestedM dim linear = torch nn Linear dim dim forward x torch Tensor torch max x linear m x linear m x Super nested even input needs lifted recursively due value propagation optimization SuperNestedM torch nn Module __init__ dim int - None super __init__ m = NestedM dim m = NestedM dim linear = torch nn Linear dim dim forward x torch Tensor torch sum x linear m x linear m x Basic module testing inp = torch ones orig_m = M ep_list = _check_equal_ts_ep_converter orig_m inp t = inp t -= ep ep_list torch testing assert_close ep module inp orig_m inp Nested module testing inp = torch ones orig_m = NestedM ep_list = _check_equal_ts_ep_converter orig_m inp t = inp t -= Skip jit traced because specializes one path ep ep_list torch testing assert_close ep module inp orig_m inp Super nested module testing inp = torch ones orig_m = SuperNestedM ep_list = _check_equal_ts_ep_converter orig_m inp t = inp t -= Skip jit traced because specializes one path ep ep_list torch testing assert_close ep module inp orig_m inp Super nested module testing inp = torch ones orig_m = SuperNestedM ep_list = _check_equal_ts_ep_converter orig_m inp t = inp t -= Skip jit traced because specializes one path ep ep_list torch testing assert_close ep module inp orig_m inp test_convert_if_duplicate_attr_names M torch nn Module __init__ - None super __init__ w = h = forward x torch Tensor y int w = w h = h y res = w + x res = h + x y res = w + res res = h + res res inp = torch ones _check_equal_ts_ep_converter M inp option= script test_ts ep_converter_contains MIn torch nn Module forward x torch Tensor x dtype torch float torch float MNotIn torch nn Module forward x torch Tensor x dtype torch int MTensorIn torch nn Module forward x torch Tensor x_dict dict torch Tensor str x x_dict Traced function must output has tensors inp = torch tensor _check_equal_ts_ep_converter MIn inp script _check_equal_ts_ep_converter MNotIn inp script TODO update test use reference inp = torch tensor torch tensor foo _check_equal_ts_ep_converter MTensorIn inp script inp = torch tensor torch tensor foo _check_equal_ts_ep_converter MTensorIn inp script test_ts ep_converter_custom_op torch library _scoped_library mylib FRAGMENT lib torch _dynamo config capture_scalar_outputs = True torch _dynamo config capture_dynamic_output_shape_ops = True torch library define mylib foo Tensor x - Tensor lib=lib PyTorch custorm op implementation torch library impl mylib foo CompositeExplicitAutograd lib=lib foo_impl x x + x Meta function custom op torch library register_fake mylib foo lib=lib foo_meta x x + x M torch nn Module forward x torch ops mylib foo x inp = torch randn m = M _check_equal_ts_ep_converter m inp test_convert_func_without_param func x y x + y func x y x sum x + y x - y inp = torch tensor torch tensor _check_equal_ts_ep_converter func inp ep_list = _check_equal_ts_ep_converter func inp t = inp t -= ep ep_list torch testing assert_close ep module inp func inp test_implicit_constant_to_tensor_handling func x x + func x y x y x - y + y func x x + torch tensor func val = torch tensor float inf torch full val func x = - x torch ones dtype=torch float torch zeros dtype=torch float func x x x x x numel x size x numel x size x numel x size x numel x size torch ones x numel Just make sure downstream ops still work torch ones x size Just make sure downstream ops still work M torch nn Module __init__ value super __init__ x = torch tensor value forward x clone M torch nn Module forward x torch tensor + x inp = torch randn _check_equal_ts_ep_converter func inp inp = torch randn torch randn _check_equal_ts_ep_converter func inp inp = torch randn _check_equal_ts_ep_converter func inp _check_equal_ts_ep_converter func _check_equal_ts_ep_converter M inp = torch randn _check_equal_ts_ep_converter M inp _check_equal_ts_ep_converter func inp = torch randn torch int torch randn torch int torch randn torch float torch randn torch float _check_equal_ts_ep_converter func inp TODO Additional check once dynamic shape supported ep ep_list assertEqual ep module torch randn torch int torch randn torch int torch randn torch float torch randn torch float test_aten_tensor_dtype_int M torch nn Module forward x y = torch tensor dtype=torch int y + x ep_list = _check_equal_ts_ep_converter M torch tensor ep ep_list assertEqual len ep constants test_aten_tensor_prim_dtype M torch nn Module forward x y = torch tensor dtype=x dtype y + x ep_list = _check_equal_ts_ep_converter M torch tensor ep ep_list assertEqual len ep constants test_aten_tensor_dynamic M torch nn Module forward x s = x shape y = torch tensor s y ep_list = _check_equal_ts_ep_converter M torch ones ep ep_list assertEqual len ep constants TODO Additional check once dynamic shape supported ep ep_list torch testing assert_close ep module torch ones M torch ones M torch nn Module forward x s = x shape y = torch tensor s s y ep_list = _check_equal_ts_ep_converter M torch ones Trace directly inline tensor constant ep ep_list assertEqual len ep constants TODO Additional check once dynamic shape supported ep ep_list torch testing assert_close ep module torch ones M torch ones test_prim_tolist Module torch nn Module forward x torch Tensor - list int x tolist inp = torch tensor _check_equal_ts_ep_converter Module inp script Module torch nn Module forward x torch Tensor - list list int x tolist inp = torch tensor _check_equal_ts_ep_converter Module inp script test_get_tensor_constants Since data only read written lifted constant tensors Foo torch nn Module __init__ - None super __init__ data = torch randn forward x torch Tensor - torch Tensor x + data Goo torch nn Module __init__ - None super __init__ data = torch randn foo = Foo forward x torch Tensor - torch Tensor x + data + foo data + foo x inp = torch randn goo = Goo _check_equal_ts_ep_converter goo inp test_prim_SetAttr Module torch nn Module __init__ - None super __init__ data = torch nn Buffer torch ones forward x torch Tensor - torch Tensor data = data + x x + x inp = torch ones _check_equal_ts_ep_converter Module inp script check_persistent=True Module torch nn Module __init__ - None super __init__ data = torch nn Buffer torch ones forward x torch Tensor - torch Tensor data = data + x x + data inp = torch ones _check_equal_ts_ep_converter Module inp script check_persistent=True export lifts tensor constant data input assigned If assigned export will error ask users register buffer In converter we change tensor constants assigned buffer automatically since might hard manually register them buffers Module torch nn Module __init__ - None super __init__ data = torch ones forward x torch Tensor - torch Tensor data = data + x x + data inp = torch ones _check_equal_ts_ep_converter Module inp script check_persistent=True lifted_tensor_constants=OrderedDict data torch ones Module torch nn Module __init__ - None super __init__ count = forward x torch Tensor - torch Tensor count += x + count check_persistent False since export specializes non-tensor constants inp = torch ones _check_equal_ts_ep_converter Module inp script check_persistent=False M torch nn Module __init__ - None super __init__ count = forward x count = count count += count = count count += count = count x + count + count + count inp = torch ones _check_equal_ts_ep_converter M inp script check_persistent=False M torch nn Module __init__ - None super __init__ w = torch nn Buffer torch ones forward x torch Tensor w += w inp = torch ones _check_equal_ts_ep_converter M inp script check_persistent=True test_raise_exception Module torch nn Module forward x torch Tensor y int - torch Tensor y raise RuntimeError test x + y match non-strict export behavior errors when given input leads RaiseException assertRaisesRegex torch jit Error builtins RuntimeError inp = torch randn _check_equal_ts_ep_converter Module inp script Matching non-strict export behavior only executes if-branch according given input inp = torch randn _check_equal_ts_ep_converter Module inp script Module torch nn Module forward x torch Tensor y int - torch Tensor z = x y raise RuntimeError test z = x z = x + y x + y + z match non-strict export behavior errors when given input leads RaiseException assertRaisesRegex torch jit Error builtins RuntimeError inp = torch randn _check_equal_ts_ep_converter Module inp script Matching non-strict export behavior only executes if-branch according given input inp = torch randn _check_equal_ts_ep_converter Module inp script test_context_manager ContextManager __init__ - None count = __enter__ count += __exit__ exc_type Any exc_value Any traceback Any - None count -= M torch nn Module forward x y ContextManager res = x + y res inp = torch ones torch ones _check_equal_ts_ep_converter M inp test_hidden_input_name torch jit script func x x + func args v = torch cat args dim= v v inp = torch randn _check_equal_ts_ep_converter func inp inp = torch ones Cannot script again _check_equal_ts_ep_converter torch ops aten relu inp trace M = Ns = empty = torch tensor dtype=torch double values = empty + torch randn M N N Ns Cannot script variable length inputs _check_equal_ts_ep_converter func tuple values trace test_ts ep_multi_outputs_on_call_ops M torch nn Module __init__ - None super __init__ pool = torch nn AdaptiveMaxPool d return_indices=True forward x torch Tensor y torch Tensor torch max x dim= torch topk x torch sort x dim= pool y inp = torch randn torch randn _check_equal_ts_ep_converter M inp test_aten_append_t M torch nn Module forward x list torch Tensor out = out append x + x out append x - x out = torch cat out out append x x out = torch cat out out out out inp = torch ones torch ones Trace already unrolls list _check_equal_ts_ep_converter M inp script test_convert_script_object M torch nn Module __init__ super __init__ tq = _empty_tensor_queue forward x torch Tensor tq push x torch ops _TorchScriptTesting queue_push tq x cos torch ops _TorchScriptTesting queue_pop tq tq pop inp = torch randn _check_equal_ts_ep_converter M inp script test_ts ep_with_loop func x x_list list torch Tensor b c = x x x _ range k range = + + k b = b + b - k x_list append x_list k + x_list k + k range b = b + b - k c = c + c k x_list append x_list k + x_list k + - x_list k + x x_list func x noqa F i range x size x = x x i x func x noqa F while x sum x += x sin x inp = torch tensor torch ones torch ones runtime_inps = torch tensor torch ones torch ones Trace unrolls loop _check_equal_ts_ep_converter func inp script runtime_inputs=runtime_inps TODO N Trace unrolls loop _check_equal_ts_ep_converter func inp script TODO N Trace unrolls loop _check_equal_ts_ep_converter func inp script unittest skipIf IS_WINDOWS Windows does support qnnpack qnnpack supported s x xfailIfS X test_ts ep_convert_quantized_model Standalone torch nn Module __init__ super __init__ quant = torch ao quantization QuantStub conv = torch nn Conv d conv = torch nn Conv d relu = torch nn ReLU dequant = torch ao quantization DeQuantStub forward x x = quant x x = conv x x = conv x x = relu x x = dequant x x fuse_model torch ao quantization fuse_modules conv relu inplace=True override_quantized_engine qnnpack model = Standalone model qconfig = torch ao quantization get_default_qconfig qnnpack model fuse_model torch ao quantization prepare model inplace=True model torch randn torch ao quantization convert model inplace=True Use customized checking here because state_dict quantization will modified quantization pass inp = torch randn original_ts_model = torch jit script model ts_model = torch jit script model converter = TS EPConverter ts_model inp ep = converter convert orig_out _ = pytree tree_flatten original_ts_model inp ep_out _ = pytree tree_flatten ep module inp _check_tensor_list_equal orig_out ep_out qnnpack xnnpack supported s x required torch ops prepacked linear_clamp_prepack torch ops prepacked linear_clamp_run xfailIfS X test_ts ep_convert_quantized_model_with_opcontext M torch nn Module __init__ linear_op super __init__ linear_op = linear_op forward x x = torch ops prepacked linear_clamp_run x linear_op x linear_op = torch ops prepacked linear_clamp_prepack torch randn torch randn m = M linear_op inp = torch randn _check_equal_ts_ep_converter m inp script qnnpack xnnpack supported s x required torch ops prepacked linear_clamp_prepack torch ops prepacked linear_clamp_run xfailIfS X test_ts ep_convert_quantized_model_with_opcontext_and_constant M torch nn Module __init__ linear_op super __init__ linear_op = linear_op forward x x = torch ops prepacked linear_clamp_run x + torch ones linear_op x linear_op = torch ops prepacked linear_clamp_prepack torch randn torch randn m = M linear_op inp = torch randn _check_equal_ts_ep_converter m inp script __name__ == __main__ run_tests