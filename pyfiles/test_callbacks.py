mypy allow-untyped-defs importlib math unittest warnings torch torch nn nn torch ao pruning _experimental data_scheduler base_data_scheduler BaseDataScheduler torch ao pruning _experimental data_sparsifier base_data_sparsifier SUPPORTED_TYPES torch ao pruning _experimental data_sparsifier data_norm_sparsifier DataNormSparsifier torch ao pruning _experimental data_sparsifier lightning callbacks _data_sparstity_utils _get_valid_name torch ao pruning _experimental data_sparsifier lightning callbacks data_sparsity PostTrainingDataSparsity TrainingAwareDataSparsity torch nn utils parametrize is_parametrized torch testing _internal common_utils run_tests TestCase DummyModel nn Module __init__ iC int oC list int super __init__ linears = nn Sequential i = iC idx c enumerate oC linears append nn Linear i c bias=False idx len oC - linears append nn ReLU i = c _make_lightning_module iC int oC list int pytorch_lightning pl type ignore DummyLightningModule pl LightningModule __init__ ic int oC list int super __init__ model = DummyModel iC oC forward pass DummyLightningModule iC oC StepSLScheduler BaseDataScheduler The sparsity param each data group multiplied gamma every step_size epochs __init__ data_sparsifier schedule_param= sparsity_level step_size= gamma= last_epoch=- verbose=False gamma = gamma step_size = step_size super __init__ data_sparsifier schedule_param last_epoch verbose get_schedule_param _get_sp_called_within_step warnings warn To get last learning rate computed scheduler please use ` get_last_lr ` UserWarning stacklevel= data_groups = data_sparsifier data_groups last_epoch == last_epoch step_size = name config schedule_param name config data_groups items name config schedule_param gamma name config data_groups items TestPostTrainingCallback TestCase _check_on_fit_end pl_module callback sparsifier_args Makes sure each component working expected while calling post-training callback Specifically check following - sparsifier config same input config data sparsifier correctly attached model sparsity achieved after step non-sparsified values same original values callback on_fit_end pl_module dummy value check sparsifier config key value sparsifier_args items assert callback data_sparsifier defaults key == value assert model correctly attached sparsifier name param pl_module model named_parameters valid_name = _get_valid_name name type param SUPPORTED_TYPES assert valid_name callback data_sparsifier state assert valid_name callback data_sparsifier data_groups continue assert valid_name callback data_sparsifier data_groups assert valid_name callback data_sparsifier state mask = callback data_sparsifier get_mask name=valid_name assert some level sparsity achieved assert - mask float mean make sure non-zero values data after squash mask equal original values sparsified_data = callback data_sparsifier get_data name=valid_name return_original=False assert torch all sparsified_data sparsified_data = == param sparsified_data = unittest skipIf importlib util find_spec pytorch_lightning No pytorch_lightning test_post_training_callback sparsifier_args = sparsity_level sparse_block_shape zeros_per_block callback = PostTrainingDataSparsity DataNormSparsifier sparsifier_args pl_module = _make_lightning_module _check_on_fit_end pl_module callback sparsifier_args TestTrainingAwareCallback TestCase Class test in-training version lightning callback Simulates model training makes sure each hook doing what expected _check_on_train_start pl_module callback sparsifier_args scheduler_args Makes sure data_sparsifier data_scheduler objects being created correctly Basically confirms input args sparsifier scheduler args in-line callback on_train_start pl_module dummy value sparsifier scheduler instantiated assert callback data_scheduler None callback data_sparsifier None data sparsifier args correct key value sparsifier_args items assert callback data_sparsifier defaults key == value data scheduler args correct key value scheduler_args items assert getattr callback data_scheduler key == value _simulate_update_param_model pl_module This function might needed model being copied during train_epoch_end good have things change future _ param pl_module model named_parameters param data = param + _check_on_train_epoch_start pl_module callback Basically ensures sparsifier s state correctly being restored The state_dict comparison needed Consider flow - Epoch on_train_epoch_start Nothing happens now on_train_epoch_end model copied into data_sparsifier b step called c internally state each layer model inside data sparsifier changes Epoch on_train_epoch_start Assume nothing happens on_train_epoch_end model copied into data_sparsifier But wait you need config attach layer module sparsifier If config None data_sparsifier uses default config which we do want config each layer changes after step Hence we need dump restore state_dict every time because we re copying model after each epoch Hence essential make sure sparsifier s state_dict being correctly dumped restored check each component state dict being loaded correctly callback on_train_epoch_start pl_module callback data_sparsifier_state_dict None data_sparsifier_state_dict = callback data_sparsifier state_dict compare container objects container_obj = data_sparsifier_state_dict _container container_obj = callback data_sparsifier_state_dict _container assert len container_obj == len container_obj key value container_obj items assert key container_obj assert torch all value == container_obj key compare state objects state_obj = data_sparsifier_state_dict state state_obj = callback data_sparsifier_state_dict state assert len state_obj == len state_obj key value state_obj items assert key state_obj assert mask value mask state_obj key assert torch all value mask == state_obj key mask compare data_groups dict data_grp = data_sparsifier_state_dict data_groups data_grp = callback data_sparsifier_state_dict data_groups assert len data_grp == len data_grp key value data_grp items assert key data_grp assert value == data_grp key _check_on_train_epoch_end pl_module callback Checks following - sparsity correctly being achieved after step scheduler data_sparsifier sparsity levels in-line callback on_train_epoch_end pl_module data_scheduler = callback data_scheduler base_sl = data_scheduler base_param name _ pl_module model named_parameters valid_name = _get_valid_name name mask = callback data_sparsifier get_mask name=valid_name check sparsity levels assert - mask float mean some sparsity level achieved last_sl = data_scheduler get_last_param last_epoch = data_scheduler last_epoch check sparsity levels scheduler log_last_sl = math log last_sl valid_name log_actual_sl = math log base_sl valid_name data_scheduler gamma last_epoch assert log_last_sl == log_actual_sl _check_on_train_end pl_module callback Confirms mask squashed after training ends This achieved making sure each parameter internal container parametrized callback on_train_end pl_module check masks have been squashed name _ pl_module model named_parameters valid_name = _get_valid_name name assert is_parametrized callback data_sparsifier _continer valid_name unittest skipIf importlib util find_spec pytorch_lightning No pytorch_lightning test_train_aware_callback sparsifier_args = sparsity_level sparse_block_shape zeros_per_block scheduler_args = gamma step_size callback = TrainingAwareDataSparsity data_sparsifier_class=DataNormSparsifier data_sparsifier_args=sparsifier_args data_scheduler_class=StepSLScheduler data_scheduler_args=scheduler_args pl_module = _make_lightning_module simulate training process check all steps _check_on_train_start pl_module callback sparsifier_args scheduler_args num_epochs = _ range num_epochs _check_on_train_epoch_start pl_module callback _simulate_update_param_model pl_module _check_on_train_epoch_end pl_module callback __name__ == __main__ run_tests