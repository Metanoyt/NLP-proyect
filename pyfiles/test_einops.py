Owner s module dynamo importlib subprocess sys unittest torch torch _dynamo config torch _dynamo test_case torch nn torch _dynamo test_case TestCase torch testing _internal common_utils instantiate_parametrized_tests parametrize HAS_EINOPS = importlib util find_spec einops HAS_EINOPS einops einops_version = einops __version__ einops_version = none einops_version_sanitized = einops_version replace _ unittest skipIf HAS_EINOPS these tests require einops TestEinops TestCase These tests adapted similar tests einops repo https github com arogozhnikov einops blob main einops tests test_other py#L The goal test suite test torch compile x einops multiple versions einops Our goal prevent regressions einops changes PyTorch unittest skipIf einops_version == https github com pytorch pytorch issues parametrize version einops_version_sanitized test_functions version einops einsum pack rearrange reduce repeat unpack TorchModuleWithOperations nn Module __init__ - None super __init__ forward x_abc suffix= b c = x_abc shape suf pattern parts = pattern split join p p - acd p + suffix p parts patterns look bit strange because names c d will modified every run suf function x_abcd = repeat x_abc suf b c - b c x_abc = reduce x_abcd suf b c d - b c min x_abdc ps = pack x_abc + len suffix suf b c x_array = unpack rearrange x_abdc suf b d c - b c d ps ab one c x = x_array + len x_array x = rearrange x suf b c - b c b=b addition = einsum x_abc x_abcd suf b c b c d - d x + addition original = TorchModuleWithOperations Einops only interacts Dynamo we test backend= inductor just case compiled = torch compile original backend= inductor fullgraph=True size x = torch rand size size + size + suffix suf other_suffix result = compiled x suffix result = original x double suffix float assertEqual result result parametrize version einops_version_sanitized test_layers version einops layers torch EinMix Rearrange Reduce original = nn Sequential Rearrange b t c - b t c c= EinMix b t c - qkv b t cout weight_shape= qkv c cout bias_shape= qkv cout qkv= c= cout= Reduce qkv b t cout - b t qkv min cout= Einops only interacts Dynamo we test backend= inductor just case compiled = torch compile original backend= inductor fullgraph=True size x = torch rand size size result = original x result = compiled x double float assertEqual result result parametrize version einops_version_sanitized test_no_recompile_on_lazy_state version einops has some lazy state gets initialized first time API called This should trigger recompile script = \ torch torch nn nn einops einsum pack reduce repeat unpack rearrange TorchModuleWithOperations nn Module __init__ - None super __init__ forward x_abc suffix= b c = x_abc shape suf pattern parts = pattern split join p p - acd p + suffix p parts patterns look bit strange because names c d will modified every run suf function x_abcd = repeat x_abc suf b c - b c x_abc = reduce x_abcd suf b c d - b c min x_abdc ps = pack x_abc + len suffix suf b c x_array = unpack rearrange x_abdc suf b d c - b c d ps ab one c x = x_array + len x_array x = rearrange x suf b c - b c b=b addition = einsum x_abc x_abcd suf b c b c d - d x + addition compiled_fn = torch compile TorchModuleWithOperations fullgraph=True x = torch arange view y = compiled_fn x Should recompile torch compiler set_stance fail_on_recompile z = compiled_fn x subprocess check_output sys executable -c script instantiate_parametrized_tests TestEinops __name__ == __main__ torch _dynamo test_case run_tests run_tests