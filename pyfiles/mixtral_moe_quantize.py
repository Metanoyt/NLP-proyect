flake noqa E C B mixtral_moe_model ConditionalFeedForward torch torch nn nn torch nn functional F ##### Quantization Primitives ###### dynamically_quantize_per_channel x quant_min quant_max target_dtype assumes symmetric quantization assumes axis == assumes dense memory format TODO future relax ^ needed default setup affine quantization activations eps = torch finfo torch float eps get min max min_val max_val = torch aminmax x dim= calculate scales zero_points based min max reference https fburl com code srbiybme min_val_neg = torch min min_val torch zeros_like min_val max_val_pos = torch max max_val torch zeros_like max_val device = min_val_neg device reference https fburl com code wll rk max_val_pos = torch max -min_val_neg max_val_pos scales = max_val_pos float quant_max - quant_min ensure scales same dtype original tensor scales = torch clamp scales min=eps x dtype zero_points = torch zeros min_val_neg size dtype=torch int device=device quantize based qmin qmax scales zp reference https www internalfb com code fbsource edc b fbcode caffe torch ao quantization fx _decomposed py lines= x_div = x scales unsqueeze - x_round = torch round x_div x_zp = x_round + zero_points unsqueeze - quant = torch clamp x_zp quant_min quant_max target_dtype quant scales zero_points ##### Weight-only int per-channel quantized code ###### replace_linear_weight_only_int _per_channel module name child module named_children isinstance child nn Linear name = gate setattr module name WeightOnlyInt Linear child in_features child out_features target_dtype=torch int isinstance child ConditionalFeedForward num_experts intermediate_size dim = child w shape setattr module name ConditionalFeedForwardInt num_experts intermediate_size dim target_dtype=torch int replace_linear_weight_only_int _per_channel child WeightOnlyInt QuantHandler __init__ mod mod = mod torch no_grad create_quantized_state_dict cur_state_dict = mod state_dict fqn mod mod named_modules isinstance mod torch nn Linear fqn endswith gate int _weight scales _ = dynamically_quantize_per_channel mod weight float - torch int cur_state_dict f fqn weight = int _weight cur_state_dict f fqn scales = scales mod weight dtype isinstance mod ConditionalFeedForward weight_idx range weight_name = f w weight_idx + scales_name = f scales weight_idx + weight = getattr mod weight_name num_experts intermediate_size dim = weight shape bit _weight_list = scales_list = expert_idx range num_experts bit _weight scales _ = dynamically_quantize_per_channel weight expert_idx float - torch int bit _weight_list append bit _weight reshape intermediate_size dim scales_list append scales reshape intermediate_size cur_state_dict f fqn weight_name = torch cat bit _weight_list dim= cur_state_dict f fqn scales_name = torch cat scales_list dim= cur_state_dict convert_for_runtime replace_linear_weight_only_int _per_channel mod mod WeightOnlyInt Linear torch nn Module __constants__ = in_features out_features in_features int out_features int weight torch Tensor __init__ in_features int out_features int bias bool = True device=None dtype=None target_dtype=None - None assert target_dtype None super __init__ in_features = in_features out_features = out_features register_buffer weight torch empty out_features in_features dtype=target_dtype register_buffer scales torch ones out_features dtype=torch bfloat forward input torch Tensor - torch Tensor F linear input weight dtype=input dtype scales ConditionalFeedForwardInt nn Module __init__ num_experts intermediate_size dim target_dtype super __init__ target_dtype = target_dtype register_buffer w torch empty num_experts intermediate_size dim dtype=target_dtype register_buffer w torch empty num_experts dim intermediate_size dtype=target_dtype register_buffer w torch empty num_experts intermediate_size dim dtype=target_dtype register_buffer scales torch empty num_experts intermediate_size dtype=torch bfloat register_buffer scales torch empty num_experts dim dtype=torch bfloat register_buffer scales torch empty num_experts intermediate_size dtype=torch bfloat forward x expert_indices w _weights = w x dtype expert_indices T A D D w _weights = w x dtype expert_indices T A D D w _weights = w x dtype expert_indices x = F silu torch einsum ti taoi - tao x w _weights scales expert_indices x dtype x = torch einsum ti taoi - tao x w _weights scales expert_indices x dtype expert_outs = torch einsum tao taio - tai x x w _weights scales expert_indices x dtype T A D D expert_outs