mypy allow-untyped-defs warnings torch nn functional F torch Tensor batchnorm _LazyNormBase _NormBase __all__ = InstanceNorm d InstanceNorm d InstanceNorm d LazyInstanceNorm d LazyInstanceNorm d LazyInstanceNorm d _InstanceNorm _NormBase __init__ num_features int eps float = e- momentum float = affine bool = False track_running_stats bool = False device=None dtype=None - None factory_kwargs = device device dtype dtype super __init__ num_features eps momentum affine track_running_stats factory_kwargs _check_input_dim input raise NotImplementedError _get_no_batch_dim raise NotImplementedError _handle_no_batch_input input _apply_instance_norm input unsqueeze squeeze _apply_instance_norm input F instance_norm input running_mean running_var weight bias training track_running_stats momentum momentum None eps _load_from_state_dict state_dict prefix local_metadata strict missing_keys unexpected_keys error_msgs - None version = local_metadata get version None version removed running_mean running_var when track_running_stats=False default version None track_running_stats running_stats_keys = name running_mean running_var key = prefix + name key state_dict running_stats_keys append key len running_stats_keys error_msgs append Unexpected running stats buffer s names klass track_running_stats=False If state_dict checkpoint saved before may expected because klass does track running stats default since Please remove these keys state_dict If running stats actually needed instead set track_running_stats=True klass enable them See documentation klass details format names= join f k k running_stats_keys klass=self __class__ __name__ key running_stats_keys state_dict pop key super _load_from_state_dict state_dict prefix local_metadata strict missing_keys unexpected_keys error_msgs forward input Tensor - Tensor _check_input_dim input feature_dim = input dim - _get_no_batch_dim input size feature_dim = num_features affine raise ValueError f expected input s size dim= feature_dim match num_features f num_features got input size feature_dim warnings warn f input s size dim= feature_dim does match num_features You can silence warning passing num_features which used because affine=False stacklevel= input dim == _get_no_batch_dim _handle_no_batch_input input _apply_instance_norm input InstanceNorm d _InstanceNorm r Applies Instance Normalization This operation applies Instance Normalization over D unbatched D batched input described paper ` Instance Normalization The Missing Ingredient Fast Stylization https arxiv org abs ` __ math y = \frac x - \mathrm E x \sqrt \mathrm Var x + \epsilon \gamma + \beta The mean standard-deviation calculated per-dimension separately each object mini-batch math ` \gamma ` math ` \beta ` learnable parameter vectors size ` C ` where ` C ` number features channels input attr ` affine ` ` ` True ` ` The variance calculated via biased estimator equivalent ` torch var input unbiased=False ` By default layer uses instance statistics computed input data both training evaluation modes If attr ` track_running_stats ` set ` ` True ` ` during training layer keeps running estimates its computed mean variance which then used normalization during evaluation The running estimates kept default attr ` momentum ` note This attr ` momentum ` argument different one used optimizer classes conventional notion momentum Mathematically update rule running statistics here math ` \hat x _\text new = - \text momentum \times \hat x + \text momentum \times x_t ` where math ` \hat x ` estimated statistic math ` x_t ` new observed value note ` InstanceNorm d ` ` LayerNorm ` very similar have some subtle differences ` InstanceNorm d ` applied each channel channeled data like multidimensional time series ` LayerNorm ` usually applied entire sample often NLP tasks Additionally ` LayerNorm ` applies elementwise affine transform while ` InstanceNorm d ` usually don t apply affine transform Args num_features number features channels math ` C ` input eps value added denominator numerical stability Default e- momentum value used running_mean running_var computation Default affine boolean value when set ` ` True ` ` module has learnable affine parameters initialized same way done batch normalization Default ` ` False ` ` track_running_stats boolean value when set ` ` True ` ` module tracks running mean variance when set ` ` False ` ` module does track such statistics always uses batch statistics both training eval modes Default ` ` False ` ` Shape - Input math ` N C L ` math ` C L ` - Output math ` N C L ` math ` C L ` same shape input Examples Without Learnable Parameters m = nn InstanceNorm d With Learnable Parameters m = nn InstanceNorm d affine=True input = torch randn output = m input _get_no_batch_dim - int _check_input_dim input - None input dim raise ValueError f expected D D input got input dim D input LazyInstanceNorm d _LazyNormBase _InstanceNorm r A ` torch nn InstanceNorm d ` module lazy initialization ` ` num_features ` ` argument The ` ` num_features ` ` argument ` InstanceNorm d ` inferred ` ` input size ` ` The attributes will lazily initialized ` weight ` ` bias ` ` running_mean ` ` running_var ` Check ` torch nn modules lazy LazyModuleMixin ` further documentation lazy modules their limitations Args num_features math ` C ` expected input size math ` N C L ` math ` C L ` eps value added denominator numerical stability Default e- momentum value used running_mean running_var computation Default affine boolean value when set ` ` True ` ` module has learnable affine parameters initialized same way done batch normalization Default ` ` False ` ` track_running_stats boolean value when set ` ` True ` ` module tracks running mean variance when set ` ` False ` ` module does track such statistics always uses batch statistics both training eval modes Default ` ` False ` ` Shape - Input math ` N C L ` math ` C L ` - Output math ` N C L ` math ` C L ` same shape input cls_to_become = InstanceNorm d type ignore assignment _get_no_batch_dim - int _check_input_dim input - None input dim raise ValueError f expected D D input got input dim D input InstanceNorm d _InstanceNorm r Applies Instance Normalization This operation applies Instance Normalization over D input mini-batch D inputs additional channel dimension described paper ` Instance Normalization The Missing Ingredient Fast Stylization https arxiv org abs ` __ math y = \frac x - \mathrm E x \sqrt \mathrm Var x + \epsilon \gamma + \beta The mean standard-deviation calculated per-dimension separately each object mini-batch math ` \gamma ` math ` \beta ` learnable parameter vectors size ` C ` where ` C ` input size attr ` affine ` ` ` True ` ` The standard-deviation calculated via biased estimator equivalent ` torch var input unbiased=False ` By default layer uses instance statistics computed input data both training evaluation modes If attr ` track_running_stats ` set ` ` True ` ` during training layer keeps running estimates its computed mean variance which then used normalization during evaluation The running estimates kept default attr ` momentum ` note This attr ` momentum ` argument different one used optimizer classes conventional notion momentum Mathematically update rule running statistics here math ` \hat x _\text new = - \text momentum \times \hat x + \text momentum \times x_t ` where math ` \hat x ` estimated statistic math ` x_t ` new observed value note ` InstanceNorm d ` ` LayerNorm ` very similar have some subtle differences ` InstanceNorm d ` applied each channel channeled data like RGB images ` LayerNorm ` usually applied entire sample often NLP tasks Additionally ` LayerNorm ` applies elementwise affine transform while ` InstanceNorm d ` usually don t apply affine transform Args num_features math ` C ` expected input size math ` N C H W ` math ` C H W ` eps value added denominator numerical stability Default e- momentum value used running_mean running_var computation Default affine boolean value when set ` ` True ` ` module has learnable affine parameters initialized same way done batch normalization Default ` ` False ` ` track_running_stats boolean value when set ` ` True ` ` module tracks running mean variance when set ` ` False ` ` module does track such statistics always uses batch statistics both training eval modes Default ` ` False ` ` Shape - Input math ` N C H W ` math ` C H W ` - Output math ` N C H W ` math ` C H W ` same shape input Examples Without Learnable Parameters m = nn InstanceNorm d With Learnable Parameters m = nn InstanceNorm d affine=True input = torch randn output = m input _get_no_batch_dim - int _check_input_dim input - None input dim raise ValueError f expected D D input got input dim D input LazyInstanceNorm d _LazyNormBase _InstanceNorm r A ` torch nn InstanceNorm d ` module lazy initialization ` ` num_features ` ` argument The ` ` num_features ` ` argument ` InstanceNorm d ` inferred ` ` input size ` ` The attributes will lazily initialized ` weight ` ` bias ` ` running_mean ` ` running_var ` Check ` torch nn modules lazy LazyModuleMixin ` further documentation lazy modules their limitations Args num_features math ` C ` expected input size math ` N C H W ` math ` C H W ` eps value added denominator numerical stability Default e- momentum value used running_mean running_var computation Default affine boolean value when set ` ` True ` ` module has learnable affine parameters initialized same way done batch normalization Default ` ` False ` ` track_running_stats boolean value when set ` ` True ` ` module tracks running mean variance when set ` ` False ` ` module does track such statistics always uses batch statistics both training eval modes Default ` ` False ` ` Shape - Input math ` N C H W ` math ` C H W ` - Output math ` N C H W ` math ` C H W ` same shape input cls_to_become = InstanceNorm d type ignore assignment _get_no_batch_dim - int _check_input_dim input - None input dim raise ValueError f expected D D input got input dim D input InstanceNorm d _InstanceNorm r Applies Instance Normalization This operation applies Instance Normalization over D input mini-batch D inputs additional channel dimension described paper ` Instance Normalization The Missing Ingredient Fast Stylization https arxiv org abs ` __ math y = \frac x - \mathrm E x \sqrt \mathrm Var x + \epsilon \gamma + \beta The mean standard-deviation calculated per-dimension separately each object mini-batch math ` \gamma ` math ` \beta ` learnable parameter vectors size C where C input size attr ` affine ` ` ` True ` ` The standard-deviation calculated via biased estimator equivalent ` torch var input unbiased=False ` By default layer uses instance statistics computed input data both training evaluation modes If attr ` track_running_stats ` set ` ` True ` ` during training layer keeps running estimates its computed mean variance which then used normalization during evaluation The running estimates kept default attr ` momentum ` note This attr ` momentum ` argument different one used optimizer classes conventional notion momentum Mathematically update rule running statistics here math ` \hat x _\text new = - \text momentum \times \hat x + \text momentum \times x_t ` where math ` \hat x ` estimated statistic math ` x_t ` new observed value note ` InstanceNorm d ` ` LayerNorm ` very similar have some subtle differences ` InstanceNorm d ` applied each channel channeled data like D models RGB color ` LayerNorm ` usually applied entire sample often NLP tasks Additionally ` LayerNorm ` applies elementwise affine transform while ` InstanceNorm d ` usually don t apply affine transform Args num_features math ` C ` expected input size math ` N C D H W ` math ` C D H W ` eps value added denominator numerical stability Default e- momentum value used running_mean running_var computation Default affine boolean value when set ` ` True ` ` module has learnable affine parameters initialized same way done batch normalization Default ` ` False ` ` track_running_stats boolean value when set ` ` True ` ` module tracks running mean variance when set ` ` False ` ` module does track such statistics always uses batch statistics both training eval modes Default ` ` False ` ` Shape - Input math ` N C D H W ` math ` C D H W ` - Output math ` N C D H W ` math ` C D H W ` same shape input Examples Without Learnable Parameters m = nn InstanceNorm d With Learnable Parameters m = nn InstanceNorm d affine=True input = torch randn output = m input _get_no_batch_dim - int _check_input_dim input - None input dim raise ValueError f expected D D input got input dim D input LazyInstanceNorm d _LazyNormBase _InstanceNorm r A ` torch nn InstanceNorm d ` module lazy initialization ` ` num_features ` ` argument The ` ` num_features ` ` argument ` InstanceNorm d ` inferred ` ` input size ` ` The attributes will lazily initialized ` weight ` ` bias ` ` running_mean ` ` running_var ` Check ` torch nn modules lazy LazyModuleMixin ` further documentation lazy modules their limitations Args num_features math ` C ` expected input size math ` N C D H W ` math ` C D H W ` eps value added denominator numerical stability Default e- momentum value used running_mean running_var computation Default affine boolean value when set ` ` True ` ` module has learnable affine parameters initialized same way done batch normalization Default ` ` False ` ` track_running_stats boolean value when set ` ` True ` ` module tracks running mean variance when set ` ` False ` ` module does track such statistics always uses batch statistics both training eval modes Default ` ` False ` ` Shape - Input math ` N C D H W ` math ` C D H W ` - Output math ` N C D H W ` math ` C D H W ` same shape input cls_to_become = InstanceNorm d type ignore assignment _get_no_batch_dim - int _check_input_dim input - None input dim raise ValueError f expected D D input got input dim D input