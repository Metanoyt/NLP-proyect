mypy allow-untyped-defs dataclasses json logging queue threading typing Any Optional torch torch distributed checkpoint FileSystemReader FileSystemWriter torch distributed checkpoint _consolidate_hf_safetensors consolidate_safetensors_files torch distributed checkpoint _hf_utils _gen_file_name _HFStorageInfo _metadata_fn CUSTOM_METADATA_KEY SAVED_OFFSETS_KEY SHARDED_DIR_NAME SUFFIX torch distributed checkpoint filesystem SerializationFormat torch distributed checkpoint metadata ChunkStorageMetadata Metadata MetadataIndex StorageMeta TensorProperties TensorStorageMetadata torch distributed checkpoint planner LoadPlan LoadPlanner ReadItem SavePlan SavePlanner WriteItem torch distributed checkpoint storage WriteResult torch futures Future logger logging Logger = logging getLogger __name__ __all__ = HuggingFaceStorageWriter HuggingFaceStorageReader HuggingFaceStorageWriter FileSystemWriter A writer writes storage huggingface safetensors format __init__ path str fqn_to_index_mapping Optional dict str int = None thread_count int = save_distributed bool = False enable_consolidation bool = False thread_count_consolidation int = - None Initialize huggingface writer pointing path Args path directory where checkpoint will read fqn_to_index_mapping A mapping tensor FQN index file tensor should written Indices N where N number files If provided tensors will written single file If none then all tensors same rank will written same file thread_count Number threads use write distributed checkpoint Default save_distributed If True save checkpoint using distributed APIs where every rank saves its own shard Default False which assumes rank- checkpointing full state_dict enable_consolidation If True consolidate sharded checkpoint after saving The sharded tensors will saved path sharded full tensors will saved path Default False thread_count_consolidation Number threads use parallel processing saving data consolidated output files Default super __init__ path=path serialization_format=SerializationFormat SAFETENSORS thread_count=thread_count fqn_to_index_mapping Optional dict str int = fqn_to_index_mapping save_distributed bool = save_distributed enable_consolidation bool = enable_consolidation consolidated_output_path Optional str = None enable_consolidation consolidated_output_path = str path path = fs concat_path path SHARDED_DIR_NAME thread_count_consolidation = thread_count_consolidation prepare_global_plan plans list SavePlan - list SavePlan new_plans = i plan enumerate plans start= storage_data dict str Any = fqn_to_index_mapping None storage_data fqn_to_index_mapping = fqn_to_index_mapping save_distributed storage_data shard_index = i new_plans append dataclasses replace plan storage_data=storage_data new_plans write_data plan SavePlan planner SavePlanner - Future list WriteResult len plan items == fut Future = Future fut set_result fut storage_plan map key file index storage_data dict str Any = plan storage_data storage_plan Optional dict str int = None shard_index Optional int = None fqn_to_index_mapping storage_data storage_plan = storage_data fqn_to_index_mapping shard_index storage_data shard_index = storage_data shard_index buckets = _split_by_storage_plan storage_plan plan items highest_index = max storage_plan values storage_plan None file_queue queue Queue = queue Queue file_index write_items buckets items file_name = _gen_file_name file_index highest_index shard_index file_queue put fs concat_path path file_name file_name write_items super _write_data planner file_queue finish metadata Metadata results list list WriteResult - None save_distributed enable_consolidation we saving distributed without consolidating then we have no metadata write because metadata file fqn file mapping doesn t make sense case because fqns will multiple files logger info Not consolidating sharded checkpoint finish step save_distributed fqn_to_index_mapping dict str int = fqn_to_index_mapping fqn_to_index_mapping None dict fromkeys metadata state_dict_metadata keys consolidate_safetensors_files input_dir=str path output_dir=self consolidated_output_path type ignore arg-type num_threads=self thread_count_consolidation fqn_to_index_mapping=fqn_to_index_mapping writing model index safetensors json file fqn file mapping rank- checkpointing case metadata_to_write = storage_md = total_size = wr_list results storage_md update wr index fqn wr storage_data relative_path wr wr_list total_size += sum wr storage_data length wr wr_list metadata_to_write metadata = total_size total_size metadata_to_write weight_map = storage_md metadata_path = fs concat_path path f _metadata_fn fs create_stream metadata_path w metadata_file json dump metadata_to_write metadata_file indent= _split_by_storage_plan storage_plan Optional dict str int items list WriteItem - dict int list WriteItem storage_plan map key index storage_plan None items buckets = item items key = item index fqn idx = storage_plan key idx buckets buckets idx = item buckets idx append item buckets property metadata_path - str _metadata_fn HuggingFaceStorageReader FileSystemReader A reader reads checkpoint huggingface safetensors format __init__ path str thread_count int = - None Initialize huggingface reader pointing path Args path directory where checkpoint will read thread_count Number threads use read distributed checkpoint Default super __init__ path=path thread_count = thread_count _process_read_request f req ReadItem planner LoadPlanner - None Helper function process single read request Create slices each dimension based offsets lengths slices = tuple slice offset offset + length offset length zip req storage_offsets req lengths tensor = f get_slice req storage_index fqn slices target_tensor = planner resolve_tensor req detach target_tensor size = tensor size raise AssertionError f req req storage_index mismatch sizes target_tensor size vs tensor size target_tensor copy_ tensor planner commit_tensor req target_tensor _read_files_from_queue file_queue queue Queue result_queue queue Queue planner LoadPlanner - None safetensors safe_open type ignore try while True file_name reqs = file_queue get_nowait safe_open filename=file_name framework= pt f req reqs _process_read_request f req planner result_queue put True Signal file has been processed except queue Empty pass read_data plan LoadPlan planner LoadPlanner - Future None safetensors safe_open type ignore per_file dict str list ReadItem = read_item plan items item_md _HFStorageInfo = storage_data read_item storage_index file_name = item_md relative_path per_file setdefault file_name append read_item thread_count = len per_file = file_name reqs per_file items safe_open filename=file_name framework= pt f req reqs _process_read_request f req planner Use parallel implementation thread pool file_queue queue Queue = queue Queue result_queue queue Queue = queue Queue Fill queue files process file_name reqs per_file items file_queue put file_name reqs Create start worker threads threads = num_threads = min thread_count len per_file _ range num_threads t = threading Thread target=self _read_files_from_queue args= file_queue result_queue planner t start threads append t Wait all threads complete t threads t join Check all files processed processed_count = try while True result_queue get_nowait processed_count += except queue Empty pass processed_count = len per_file raise AssertionError f Not all files processed processed_count out len per_file fut Future = Future fut set_result None fut pyrefly ignore bad-override read_metadata - Metadata safetensors safe_open type ignore safetensors torch _getdtype type ignore state_dict_metadata dict str TensorStorageMetadata = storage_data dict MetadataIndex _HFStorageInfo = safetensors_files = file fs ls path file endswith SUFFIX safetensors_files append file safetensor_file safetensors_files safe_open safetensor_file framework= pt f keys = f keys extra_metadata = f metadata dcp_sharding_info = None extra_metadata extra_metadata get CUSTOM_METADATA_KEY dcp_sharding_info = json loads extra_metadata get CUSTOM_METADATA_KEY key keys shape = f get_slice key get_shape dtype = f get_slice key get_dtype construct state_dict_metadata dcp_sharding_info None offset = dcp_sharding_info key SAVED_OFFSETS_KEY offset = len shape key state_dict_metadata state_dict_metadata key = TensorStorageMetadata properties=TensorProperties dtype=_getdtype dtype size=torch Size saved + offset saved offset zip shape offset chunks= ChunkStorageMetadata offsets=torch Size offset sizes=torch Size shape state_dict_metadata key chunks append ChunkStorageMetadata torch Size offset sizes=torch Size shape size = list state_dict_metadata key size i range len size size i = max size i shape i + offset i state_dict_metadata key size = torch Size size construct storage data dcp_sharding_info None metadata_index = MetadataIndex fqn=key offset=dcp_sharding_info key SAVED_OFFSETS_KEY metadata_index = MetadataIndex fqn=key offset= len shape storage_data metadata_index = _HFStorageInfo relative_path=safetensor_file shape=torch Size shape dtype=_getdtype dtype metadata = Metadata state_dict_metadata=state_dict_metadata type ignore arg-type storage_data=storage_data getattr metadata storage_meta None None metadata storage_meta = StorageMeta metadata storage_meta load_id = load_id type ignore union-attr metadata