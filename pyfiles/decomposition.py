mypy allow-untyped-decorators functools logging math operator sys typing typing Any Callable Optional TypeVar Union typing_extensions ParamSpec TypeAlias torch torch _decomp decomp torch _prims_common utils torch ao quantization fx _decomposed torch _decomp core_aten_decompositions get_decompositions remove_decompositions torch _decomp decompositions _grid_sampler_ d decomp_grid_sampler_ d _index_add embedding_dense_backward decomp_embedding_dense_backward pw_cast_for_opmath pw_cast_for_opmath_non_tensor_args torch _decomp decompositions_for_rng extra_random_decomps torch _dynamo utils counters torch _environment is_fbcode torch _higher_order_ops out_dtype out_dtype torch _inductor utils pad_listlike torch _prims_common elementwise_dtypes ELEMENTWISE_TYPE_PROMOTION_KIND type_to_dtype torch fx experimental symbolic_shapes guard_or_false statically_known_true config inductor_prims utils is_gpu needs_fallback_due_to_atomic_add_limitations use_scatter_fallback _T = TypeVar _T _P = ParamSpec _P _GenericOperator TypeAlias = Union torch _ops OperatorBase torch _ops OpOverloadPacket log = logging getLogger __name__ aten = torch ops aten prims = torch ops prims quantized = torch ops quantized _quantized = torch ops _quantized quantized_decomposed = torch ops quantized_decomposed inductor_decompositions = get_decompositions aten _adaptive_avg_pool d_backward aten index_select aten addmv aten arange aten bitwise_and_ aten bitwise_or_ aten clamp_min_ aten dist aten elu aten empty_like aten flip aten gelu aten hardtanh aten lcm aten leaky_relu aten linalg_vector_norm aten _log_softmax aten max_pool d_with_indices_backward aten _native_batch_norm_legit aten _native_batch_norm_legit_functional aten _native_batch_norm_legit_no_training aten _batch_norm_with_update aten _batch_norm_with_update_functional aten _batch_norm_no_update aten batch_norm_backward aten native_batch_norm aten native_group_norm aten native_layer_norm aten nll_loss d_backward aten permute_copy aten rrelu_with_noise_backward aten _softmax aten sin_ aten sqrt_ out_dtype aten _to_copy aten tril_indices aten triu_indices aten unbind_copy int aten upsample_bilinear d vec quantized linear_dynamic_fp _unpacked_weight _quantized wrapped_quantized_linear decompositions = core_aten_decompositions inductor_decompositions Remove unwanted decompositions included via core ATen decompositions Inductor decomp table decomps_to_exclude list Union torch _ops OpOverload torch _ops OpOverloadPacket = aten _unsafe_index aten _unsafe_masked_index aten _unsafe_masked_index_put_accumulate aten _scaled_dot_product_flash_attention_for_cpu default See comments torch _decomp decompositions py aten _softmax_backward_data aten clamp_max aten clamp_min aten embedding_dense_backward we fall back xpu aten index_add we conditionally call decomp aten glu inductor lowers directly aten select_scatter need ATen graph order work re-inplacing pass aten slice_scatter need ATen graph order work re-inplacing pass aten split Tensor inductor lowers directly aten squeeze inductor lowers directly aten sum inductor lowers directly aten unbind inductor lowers directly aten baddbmm upcasts fp perf issue remove_decompositions decompositions decomps_to_exclude register_decomposition ops Union _GenericOperator list _GenericOperator - Callable Callable _P _T Callable _P _T op ops isinstance ops list ops op decompositions log warning duplicate decomp s ops decomp register_decomposition ops decompositions register_decomposition aten embedding_dense_backward _embedding_dense_backward grad_output torch Tensor indices torch Tensor num_weights int padding_idx int scale_grad_by_freq bool - torch Tensor TODO check XE still need fallback check torch xpu get_device_properties grad_output device architecture grad_output is_xpu NotImplemented We can write util function update decomp table we have more ops fallback decomp_embedding_dense_backward grad_output indices num_weights padding_idx scale_grad_by_freq register_decomposition aten sym_constrain_range_for_size default sym_constrain_range_for_size symbol torch SymInt min Optional torch types Number = None max Optional torch types Number = None - None register_decomposition aten clamp pw_cast_for_opmath_non_tensor_args clamp x torch Tensor min Optional torch types Number = None max Optional torch types Number = None - torch Tensor min None x = x clamp_min min max None x = x clamp_max max x register_decomposition aten full full size list Union int torch SymInt fill_value torch types Number kwargs Any - torch Tensor dtype = kwargs get dtype dtype None kwargs dtype = type_to_dtype type fill_value torch full size fill_value kwargs NotImplemented register_decomposition aten index_add index_add x torch Tensor dim int index torch Tensor tensor torch Tensor alpha torch types Number = - torch Tensor If we fbcode dtype bfloat fallback index_add kernel see https github com pytorch pytorch issues details is_fbcode x dtype == torch bfloat NotImplemented _index_add x dim index tensor inplace=False alpha=alpha Not really sure how put into main library PrimTorch wants empty_permuted go prim typically users don t really want decompose empty_strided inductor OK because we cool strides everything goes empty_strided register_decomposition aten empty_permuted default empty_permuted size list Union int torch SymInt physical_layout list int kwargs Any - torch Tensor perm = len size p l enumerate physical_layout perm l = p torch empty size l l physical_layout kwargs permute perm register_decomposition aten convolution_backward convolution_backward grad_output torch Tensor input torch Tensor weight torch Tensor bias_sizes list int stride Union int list int padding Union int list int dilation Union int list int transposed bool output_padding list int groups int output_mask list bool - tuple torch Tensor torch Tensor torch Tensor output_mask is_gpu grad_output device type NotImplemented grad_bias = aten sum grad_output + list range grad_output dim grad_inp grad_weight _ = aten convolution_backward grad_output input weight bias_sizes stride padding dilation transposed output_padding groups output_mask output_mask False grad_inp grad_weight grad_bias register_decomposition aten round decimals round_dec x torch Tensor decimals int = - torch Tensor ten_pow_decimals = decimals aten round x ten_pow_decimals ten_pow_decimals register_decomposition aten bmm pw_cast_for_opmath bmm torch Tensor batch torch Tensor out_dtype Optional torch dtype = None - torch Tensor TODO Re-enable mps once our reductions performant enough https github com pytorch pytorch issues config coordinate_descent_tuning device type cpu mps statically_known_true shape == statically_known_true batch shape == out = unsqueeze - batch unsqueeze sum dim= out device type == cpu statically_known_true size == statically_known_true batch size - == counters inductor decompose_bmm += torch sum squeeze batch squeeze - dim= keepdim=True unsqueeze NotImplemented register_decomposition aten addmm pw_cast_for_opmath addmm torch Tensor mat torch Tensor mat torch Tensor out_dtype Optional torch dtype = None beta torch types Number = alpha torch types Number = - torch Tensor device type == cpu statically_known_true mat size == statically_known_true mat size - == counters inductor decompose_addmm += out = torch sum mat squeeze mat squeeze - dim= keepdim=True unsqueeze alpha out + beta statically_known_true mat size == guard_or_false mat size = guard_or_false mat size = counters inductor decompose_addmm += out = mat T mat sum dim= keepdim=True alpha out + beta NotImplemented register_decomposition aten mm pw_cast_for_opmath mm torch Tensor input torch Tensor out_dtype Optional torch dtype = None - torch Tensor Our matrix vector multiplies only achieve peak bandwidth coordinate descent tuning todo Look into why fix hopefully TODO Re-enable mps once our reductions performant enough https github com pytorch pytorch issues config coordinate_descent_tuning device type cpu mps statically_known_true shape == statically_known_true input shape == unsqueeze input unsqueeze sum dim= device type == cpu statically_known_true size - == statically_known_true size statically_known_true input size == dtype == input dtype guard_or_false torch numel + torch numel input = counters inductor decompose_mm += input statically_known_true size == statically_known_true input size - == counters inductor decompose_mm += torch sum squeeze input squeeze - dim= keepdim=True unsqueeze NotImplemented This pass does two things - Eliminate cat when there only one tensor input - Normalize cat calls so legacy empty -D tensors removed NB we don t remove ALL empty tensors only naughty ones register_decomposition aten cat default cat tensors list torch Tensor dim int = - torch Tensor non_empty_tensor x torch Tensor - bool For better worse valid cat torch cat torch randn torch randn torch randn We d like eliminate naughtiness like downstream passes like split_cat The easiest way just drop such inputs guarding they non-zero Is permissible filtering size-oblivious A case where could matter cat u dim= u happened zero we would have liked have filtered out But actually ONLY way could have passed u == so time we get here we have already installed deferred runtime assert forcing u zero So hasn t happened we know unbacked SymInt has appropriate size there no problems len x shape == guard_or_false x shape == False dim len x shape guard_or_false x shape dim == False True filtered_tensors = list filter non_empty_tensor tensors len filtered_tensors == check dtype promotion promoted_dtype = elementwise_dtypes tensors type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND DEFAULT filtered_t = filtered_tensors filtered_t clone promoted_dtype == filtered_t dtype filtered_t dtype=promoted_dtype len filtered_tensors len tensors first call when we remove empty tensors we redispatch recursively aten cat default filtered_tensors dim optimization avoid concat single repeated input len filtered_tensors all t filtered_tensors t filtered_tensors inp = filtered_tensors shape = list inp shape dim = dim + len inp shape dim dim shape insert dim len filtered_tensors inp unsqueeze dim expand shape flatten dim dim + clone when no filtering has occurred we raise prevent infinite recursion no more decomposition needed NotImplemented register_decomposition aten angle angle x torch Tensor - torch Tensor x is_complex torch where torch isnan x real float nan torch atan x imag x real when x real number x = x pi x nan nan _ dtype = elementwise_dtypes x type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND INT_TO_FLOAT pi = torch scalar_tensor math pi dtype=dtype device=x device ret = torch where x pi torch where torch isnan x float nan ret register_decomposition aten add add x torch Tensor y torch Tensor alpha Optional torch types Number = None - torch Tensor Require both x y complex tensors x_is_complex_tensor = torch is_tensor x x is_complex y_is_complex_tensor = torch is_tensor y y is_complex x_is_complex_tensor y_is_complex_tensor NotImplemented _requires_fallback tensor torch Tensor - bool tensor ndim == False Viewing complex tensors their real dtype requires last stride tensor stride - = output_size_zero = False x ndim == y ndim == output_size_zero = True x ndim == x = x reshape y ndim == y = y reshape z = y alpha None z = alpha y complex_type = torch promote_types x dtype y dtype _requires_fallback x _requires_fallback z NotImplemented For complex typed ` x ` ` x view x real dtype ` doubles last dimension can cause problem when broadcasting add reshape_tensor_complex tensor torch Tensor - torch Tensor Reshape tensor initial_dims last_dim initial_dims last_dim Get current shape tensor initial_dims last_dim = tensor shape Check last dimension even We should never reach here since ` x view x real dtype ` doubles last dimension complex numbers last_dim = raise AssertionError The size last dimension must even reshape last_dim Reshape tensor new_shape = initial_dims last_dim reshaped_tensor = tensor view new_shape reshaped_tensor Manually resolve complex tensors is_conj unreliable after cloning during compilation x = x + z = z + x_reshaped = reshape_tensor_complex x view x real dtype z_reshaped = reshape_tensor_complex z view y real dtype result = torch flatten x_reshaped + z_reshaped start_dim=- view complex_type output_size_zero result result register_decomposition aten conj_physical conj_physical torch Tensor - torch Tensor is_complex NotImplemented register_decomposition aten lift aten detach_ lift torch Tensor - torch Tensor register_decomposition aten fmin prims fmin fmin torch Tensor other torch Tensor - torch Tensor torch where torch isnan other &#124; other other register_decomposition aten fmax prims fmax fmax torch Tensor other torch Tensor - torch Tensor torch where torch isnan other &#124; other other register_decomposition aten amax amax torch Tensor dim Optional int = None keepdim bool = False - torch Tensor dtype == torch bool torch any dim=dim keepdim=keepdim NotImplemented register_decomposition aten amin amin torch Tensor dim Optional int = None keepdim bool = False - torch Tensor dtype == torch bool torch all dim=dim keepdim=keepdim NotImplemented register_decomposition aten narrow_copy narrow_copy torch Tensor dim int start int length int - torch Tensor torch narrow dim start length clone register_decomposition aten view_copy default view_copy_default torch Tensor size list Union int torch SymInt - torch Tensor aten view size clone register_decomposition aten view_copy dtype view_copy_dtype torch Tensor dtype torch dtype - torch Tensor dtype clone _get_shape_permutation_like torch Tensor - tuple utils ShapeType utils StrideType physical_layout _ = utils compute_elementwise_output_logical_to_physical_perm shape = shape l l physical_layout permutation = len shape p l enumerate physical_layout permutation l = p shape permutation register_decomposition aten full_like full_like torch Tensor fill_value Union int float dtype Optional torch dtype = None layout Optional torch layout = None device Optional torch device = None pin_memory bool = False requires_grad bool = False memory_format torch memory_format = torch preserve_format - torch Tensor dtype = dtype dtype None dtype layout = layout layout None layout device = device device None device memory_format = torch preserve_format result = torch full shape fill_value dtype=dtype layout=layout device=device pin_memory=pin_memory requires_grad=requires_grad result memory_format=memory_format assert layout == torch strided shape permutation = _get_shape_permutation_like result = torch full shape fill_value dtype=dtype layout=layout device=device pin_memory=pin_memory requires_grad=requires_grad permutation == list range len permutation result result permute permutation clone _rand_like rand_fn Callable torch Tensor torch Tensor dtype Optional torch dtype = None device Optional torch device = None memory_format torch memory_format = torch preserve_format kwargs Any - torch Tensor dtype = dtype dtype None dtype device = device device None device memory_format = torch preserve_format rand_fn shape dtype=dtype device=device kwargs memory_format=memory_format shape permutation = _get_shape_permutation_like result = rand_fn shape dtype=dtype device=device kwargs permutation == list range len permutation result result permute permutation clone register_decomposition aten rand_like rand_like torch Tensor kwargs Any - torch Tensor _rand_like torch rand kwargs register_decomposition aten randn_like randn_like torch Tensor kwargs Any - torch Tensor _rand_like torch randn kwargs register_decomposition aten randint_like default randint_like torch Tensor high int kwargs Any - torch Tensor _rand_like functools partial aten randint low high kwargs register_decomposition aten randint_like low_dtype randint_like_low torch Tensor low int high int kwargs Any - torch Tensor _rand_like functools partial aten randint low low high kwargs register_decomposition aten randint default randint high int size list Union int torch SymInt kwargs Any - torch Tensor aten randint low high size kwargs register_decomposition quantized linear_dynamic_fp _unpacked_weight default linear_dynamic_fp _unpacked_weight input torch Tensor weight torch Tensor bias Optional torch Tensor = None - torch Tensor packed_weight = torch ops _quantized wrapped_fbgemm_pack_gemm_matrix_fp weight torch ops _quantized wrapped_fbgemm_linear_fp _weight input packed_weight bias weight size register_decomposition _quantized wrapped_quantized_linear default wrapped_quantized_linear input torch Tensor input_scale torch Tensor input_zero_point torch Tensor weight torch Tensor weight_scale torch Tensor weight_zero_point torch Tensor bias torch Tensor out_scale torch Tensor out_zero_point torch Tensor out_channel int - torch Tensor packed_weight = torch ops _quantized _wrapped_linear_prepack weight weight_scale weight_zero_point bias torch ops _quantized _wrapped_quantized_linear_prepacked input input_scale input_zero_point packed_weight out_scale out_zero_point out_channel register_decomposition torch ops quantized embedding_bag_byte_unpack q_embedding_bag_byte_unpack_decomp packed torch Tensor - torch Tensor bitcast_u _to_f u torch Tensor - torch Tensor x y z w = u n torch int n sys byteorder == little x + y + z + w view torch float None x + y + z + w view torch float None scales = bitcast_u _to_f packed - - offsets = bitcast_u _to_f packed - packed - torch float scales + offsets register_decomposition aten grid_sampler_ d pw_cast_for_opmath grid_sampler_ d torch Tensor grid torch Tensor interpolation_mode int = padding_mode int = align_corners bool = False - torch Tensor We do expand grid _expand_grid=False cpu performance reasons Experimenting locally found compiled CUDA code accelerated ~ x CPU code ~ x bicubic mode we expand grid N H W into N C H W However leads slowdown around ~ x CPU bilinear mode channels first Thus we apply hack expand grid case _expand_grid = device == torch device cpu interpolation_mode == is_contiguous memory_format=torch contiguous_format output = decomp_grid_sampler_ d grid=grid interpolation_mode=interpolation_mode padding_mode=padding_mode align_corners=align_corners _expand_grid=_expand_grid output register_decomposition aten _foreach_addcmul Scalar _foreach_addcmul_scalar list torch Tensor left_tensors list torch Tensor right_tensors list torch Tensor scalar float = - list torch Tensor aten _foreach_add List aten _foreach_mul List left_tensors right_tensors alpha=scalar register_decomposition aten _foreach_addcdiv Scalar _foreach_addcdiv_scalar list torch Tensor left_tensors list torch Tensor right_tensors list torch Tensor scalar float = - list torch Tensor aten _foreach_add List aten _foreach_div List left_tensors right_tensors alpha=scalar register_decomposition aten _foreach_lerp Scalar _foreach_lerp_scalar start_tensors list torch Tensor end_tensors list torch Tensor weight torch types Number - list torch Tensor aten _foreach_add List start_tensors aten _foreach_mul Scalar aten _foreach_sub List end_tensors start_tensors weight register_decomposition aten _foreach_lerp ScalarList _foreach_lerp_scalarlist start_tensors list torch Tensor end_tensors list torch Tensor scalars list torch types Number - list torch Tensor aten _foreach_add List start_tensors aten _foreach_mul ScalarList aten _foreach_sub List end_tensors start_tensors scalars aten miopen_batch_norm default py_impl torch _C DispatchKey Autograd register_decomposition aten miopen_batch_norm miopen_batch_norm input torch Tensor weight torch Tensor bias typing Optional torch Tensor running_mean typing Optional torch Tensor running_var typing Optional torch Tensor training bool exponential_average_factor float epsilon float - tuple torch Tensor torch Tensor torch Tensor b c = aten native_batch_norm input weight bias running_mean running_var training exponential_average_factor epsilon training b c weight new_zeros weight new_zeros functools cache fast_random_decomps - dict Any Callable Any decompositions extra_random_decomps TODO aakhundov replace above Any more specific type fix all cascading mypy errors select_decomp_table - dict Any Callable Any decomps can change based config config fallback_random decompositions config fallback_embedding_bag_byte_unpack remove q_embedding_bag_byte_unpack_decomp decompositions decompositions pop torch ops quantized embedding_bag_byte_unpack default None decompositions fast_random_decomps register_decomposition aten masked_scatter masked_scatter torch Tensor mask torch Tensor source torch Tensor - torch Tensor codegen common BackendFeature has_backend_feature has_backend_feature device BackendFeature MASKED_SCATTER_WITH_INDEX This two-step algorithm same eager CUDA eager CPU we use -shot serial iteration mask = aten broadcast_tensors mask source_idx = mask reshape - cumsum - self_flat mask_flat source_flat = x flatten x mask source result = aten _unsafe_masked_index source_flat mask_flat source_idx torch where mask_flat result self_flat view shape NotImplemented register_decomposition quantized_decomposed choose_qparams tensor choose_qparams_tensor input torch Tensor quant_min int quant_max int eps float dtype torch dtype - tuple torch Tensor torch Tensor min_val max_val = torch aminmax input scale = max_val - min_val float quant_max - quant_min scale = torch max scale torch Tensor eps zero_point = quant_min - torch round min_val scale torch int zero_point = torch clamp zero_point quant_min quant_max scale torch float zero_point torch int register_decomposition aten put put torch Tensor index torch Tensor source torch Tensor accumulate bool = False - torch Tensor flattened = flatten flattened = torch index_put flattened index source reshape index shape accumulate flattened reshape shape register_decomposition aten put_ put_ torch Tensor index torch Tensor source torch Tensor accumulate bool = False - torch Tensor out = aten put index source accumulate=accumulate copy_ out register_decomposition aten _softmax_backward_data default pw_cast_for_opmath _softmax_backward_data grad_output torch Tensor output torch Tensor dim int input_dtype torch dtype - torch Tensor new_grad_output = grad_output output sum_new_grad = torch sum new_grad_output dim=dim keepdim=True grad_input = new_grad_output - output sum_new_grad grad_input = inductor_prims fma -output sum_new_grad new_grad_output CPU kernel doesn t respect input_dtype following check doesn t work meta tensor grad_output device == torch device cpu grad_input contiguous grad_output dtype = input_dtype grad_input = grad_input input_dtype grad_input contiguous register_decomposition aten index_reduce index_reduce torch Tensor dim int index torch Tensor src torch Tensor reduction_type str include_self bool = True - torch Tensor reduction_type == mean needs_fallback_due_to_atomic_add_limitations dtype true_division = dtype is_floating_point dtype is_complex ones = torch ones_like src include_self out = counts = torch ones_like index_add dim index ones out = index_fill dim index counts = torch zeros_like index_add dim index ones counts = counts masked_fill counts out = out index_add dim index src out counts true_division out counts use_scatter_fallback aten scatter_reduce_ two reduction_type dtype src dtype src device type True NotImplemented repeats = shape dim + numel shape dim numel index_shape = index numel shape dim + shape dim perm = range ndim - dim ndim range ndim - dim scatter_index = index torch int repeat_interleave repeats reshape index_shape permute perm scatter_reduce dim scatter_index src reduction_type include_self=include_self _max_pool_with_indices x torch Tensor kernel_size list int stride Optional Union int list int padding Union int list int dilation Union int list int ceil_mode bool dim int - tuple torch Tensor torch Tensor dilation == dilation = dim padding == padding = dim stride stride = kernel_size pyrefly ignore bad-assignment kernel_size = pad_listlike kernel_size dim pyrefly ignore bad-assignment dilation = pad_listlike dilation dim pyrefly ignore bad-assignment padding = pad_listlike padding dim pyrefly ignore bad-assignment stride = pad_listlike stride dim window_size = functools reduce operator mul kernel_size We fallback when using non-default dilation when window size too large torch _inductor lowering should_fallback_max_pool_with_indices kernel_size n_dim=dim window_size torch iinfo torch int max NotImplemented vals offsets = prims _low_memory_max_pool_with_offsets x kernel_size stride padding dilation ceil_mode indices = prims _low_memory_max_pool_offsets_to_indices offsets kernel_size x shape -dim stride padding dilation vals indices register_decomposition aten max_pool d_with_indices max_pool d_with_indices x torch Tensor kernel_size list int stride Optional Union int list int = None padding Union int list int = dilation Union int list int = ceil_mode bool = False - tuple torch Tensor torch Tensor _max_pool_with_indices x kernel_size stride padding dilation ceil_mode dim= register_decomposition aten max_pool d_with_indices max_pool d_with_indices x torch Tensor kernel_size list int stride Optional Union int list int = None padding Union int list int = dilation Union int list int = ceil_mode bool = False - tuple torch Tensor torch Tensor _max_pool_with_indices x kernel_size stride padding dilation ceil_mode dim= register_decomposition aten adaptive_max_pool d adaptive_max_pool d x torch Tensor output_size list int - tuple torch Tensor torch Tensor batch h_in w_in = x shape h_out w_out = output_size h_out == w_out == o_size = batch h_out w_out x new_empty o_size x new_empty o_size dtype=torch int h_in h_out == w_in w_out == kernel_size = h_in h_out w_in w_out aten max_pool d_with_indices x kernel_size NotImplemented register_decomposition aten searchsorted Scalar searchsorted_scalar sorted_sequence torch Tensor torch types Number out_int bool = False right bool = False side Optional str = None sorter Optional torch Tensor = None - torch Tensor aten searchsorted sorted_sequence torch tensor device=sorted_sequence device out_int =out_int right=right side=side sorter=sorter register_decomposition aten rrelu_with_noise_functional rrelu_with_noise_functional torch Tensor noise torch Tensor lower float = upper float = training bool = False generator Optional torch Generator = None - tuple torch Tensor torch Tensor training not_positive = = r = aten uniform lower upper generator=generator output = torch where not_positive r noise_out = torch where not_positive r output noise_out negative_slope = lower + upper aten leaky_relu negative_slope torch Tensor register_decomposition aten repeat_interleave Tensor repeat_interleave_Tensor repeat torch Tensor output_size Optional int = None - torch Tensor config triton autotune_at_compile_time We can t compile-time auto-tune because expects specific data ` repeat ` NotImplemented output_size None type output_size int NotImplemented repeat device type == mps NotImplemented assert repeat dtype torch int torch int assert repeat ndim == cumsum = repeat cumsum pos = torch arange output_size device=repeat device indices = torch searchsorted cumsum pos out_int = repeat dtype == torch int right=True torch clamp indices max=repeat size - intentionally regiestered conv d_to_conv d input torch Tensor weight torch Tensor bias Optional torch Tensor = None stride tuple int = padding tuple int = dilation tuple int = groups int = - torch Tensor Shapes input N C_in L_in weight C_out C_in groups K bias C_out assert input dim == weight dim == Expect N C_in L C_out C_in groups K pyrefly ignore bad-assignment stride = stride pyrefly ignore bad-assignment padding = padding pyrefly ignore bad-assignment dilation = dilation Unsqueeze make input D N C L - N C L input_ d = input unsqueeze - Unsqueeze kernel C_out C_in groups K - C_out C_in groups K weight_ d = weight unsqueeze - Call conv d adjusted args out_ d = aten conv d default input_ d weight_ d bias stride= stride padding= padding dilation= dilation groups=groups Squeeze dummy dimension back out N C_out L_out - N C_out L_out out_ d squeeze -