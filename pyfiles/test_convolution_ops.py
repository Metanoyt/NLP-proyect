Copyright c Meta Platforms Inc affiliates Owner s oncall distributed copy torch torch nn nn torch distributed DeviceMesh torch distributed tensor distribute_module distribute_tensor DTensor Replicate Shard torch nn functional F torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor create_local_tensor_test_class DTensorTestBase skip_if_lt_x_gpu with_comms ITER_TIME = LR = _conv_fn name str module nn Module device_mesh DeviceMesh - None name param module named_parameters dist_spec = Replicate dist_param = torch nn Parameter distribute_tensor param device_mesh dist_spec name = _ join name split module register_parameter name dist_param DistConvolutionOpsTest DTensorTestBase property world_size - int hard code world size with_comms test_downsampling_convolution device_mesh = build_device_mesh shard_spec = Shard input_list = torch rand ITER_TIME grad_output_list = torch rand ITER_TIME e- model = nn Conv d kernel_size= stride= padding= device_type nn init ones_ model weight nn init zeros_ model bias model_gt = copy deepcopy model device_type training dtensor model = distribute_module model device_mesh _conv_fn input_fn=None output_fn=None optimizer = torch optim SGD model parameters lr=LR i range ITER_TIME optimizer zero_grad inp = input_list i device_type requires_grad_ inp_dtensor = distribute_tensor inp device_mesh shard_spec output = model inp_dtensor grad_output = grad_output_list i device_type grad_output_dtensor = distribute_tensor grad_output device_mesh shard_spec output backward grad_output_dtensor optimizer step training plain tensor optimizer_gt = torch optim SGD model_gt parameters lr=LR i range ITER_TIME optimizer_gt zero_grad inp = input_list i device_type requires_grad_ output = model_gt inp grad_output = grad_output_list i device_type output backward grad_output optimizer_gt step weight_diff_abs = model weight to_local - model_gt weight bias_diff_abs = model bias to_local - model_gt bias weight_diff_rel = weight_diff_abs torch abs model_gt weight + e- bias_diff_rel = bias_diff_abs torch abs model_gt bias + e- weight_mse_abs = torch mean weight_diff_abs weight_diff_abs item bias_mse_abs = torch mean bias_diff_abs bias_diff_abs item weight_mse_rel = torch mean weight_diff_rel weight_diff_rel item bias_mse_rel = torch mean bias_diff_rel bias_diff_rel item assertTrue weight_mse_abs = e- f Too large absolute mse weight tensor expected less equal e- got weight_mse_abs assertTrue bias_mse_abs = e- f Too large absolute mse bias tensor expected less equal e- got bias_mse_abs assertTrue weight_mse_rel = e- f Too large relative mse weight tensor expected less equal e- got weight_mse_rel assertTrue bias_mse_rel = e- f Too large relative mse bias tensor expected less equal e- got bias_mse_rel TODO test_depthwise_convolution broken CI gloo backend Temporarily disable unblock CI with_comms skip_if_lt_x_gpu test_depthwise_convolution device_mesh = build_device_mesh shard_spec = Shard input_list = torch rand ITER_TIME grad_output_list = torch rand ITER_TIME e- model = nn Conv d kernel_size= padding= groups= device_type nn init ones_ model weight nn init zeros_ model bias model_gt = copy deepcopy model device_type training dtensor model = distribute_module model device_mesh _conv_fn input_fn=None output_fn=None optimizer = torch optim SGD model parameters lr=LR i range ITER_TIME optimizer zero_grad inp = input_list i device_type requires_grad_ inp_dtensor = distribute_tensor inp device_mesh shard_spec output = model inp_dtensor grad_output = grad_output_list i device_type grad_output_dtensor = distribute_tensor grad_output device_mesh shard_spec output backward grad_output_dtensor optimizer step training plain tensor optimizer_gt = torch optim SGD model_gt parameters lr=LR i range ITER_TIME optimizer_gt zero_grad inp = input_list i device_type requires_grad_ output = model_gt inp grad_output = grad_output_list i device_type output backward grad_output optimizer_gt step weight_diff_abs = model weight to_local - model_gt weight bias_diff_abs = model bias to_local - model_gt bias weight_diff_rel = weight_diff_abs torch abs model_gt weight + e- bias_diff_rel = bias_diff_abs torch abs model_gt bias + e- weight_mse_abs = torch mean weight_diff_abs weight_diff_abs item bias_mse_abs = torch mean bias_diff_abs bias_diff_abs item weight_mse_rel = torch mean weight_diff_rel weight_diff_rel item bias_mse_rel = torch mean bias_diff_rel bias_diff_rel item assertTrue weight_mse_abs = e- f Too large absolute mse weight tensor expected less equal e- got weight_mse_abs assertTrue bias_mse_abs = e- f Too large absolute mse bias tensor expected less equal e- got bias_mse_abs assertTrue weight_mse_rel = e- f Too large relative mse weight tensor expected less equal e- got weight_mse_rel assertTrue bias_mse_rel = e- f Too large relative mse bias tensor expected less equal e- got bias_mse_rel with_comms skip_if_lt_x_gpu test_conv_backward_none_grad_inp device_mesh = build_device_mesh conv = nn Conv d padding= train x = torch randn x_dt = DTensor from_local x device_mesh Replicate w = conv weight w_dt = torch nn Parameter DTensor from_local w device_mesh Replicate b = conv bias b_dt = torch nn Parameter DTensor from_local b device_mesh Replicate res = F conv d x_dt w_dt b_dt padding= dres = torch rand_like res res backward dres assertTrue w_dt grad None assertTrue b_dt grad None assertTrue x_dt grad None _run_single_arg_fwd model arg - tuple torch Tensor torch Tensor Given model arg runs fwd model local distbuted given device_mesh device_mesh = build_device_mesh model_copy = copy deepcopy model device=self device_type dist_model = distribute_module model device_mesh _conv_fn arg_dt = DTensor from_local arg device_mesh Replicate out_dt = dist_model arg_dt device=self device_type out = model_copy arg out_dt full_tensor out with_comms test_conv d model = nn Conv d padding= x = torch randn device=self device_type out_dt out = _run_single_arg_fwd model x assertEqual out_dt shape out shape with_comms test_conv d model = nn Conv d padding= x = torch randn device=self device_type out_dt out = _run_single_arg_fwd model x assertEqual out_dt shape out shape DistConvolutionOpsTestWithLocalTensor = create_local_tensor_test_class DistConvolutionOpsTest Send recv ops supported skipped_tests= test_conv d test_conv d test_conv_backward_none_grad_inp test_depthwise_convolution test_downsampling_convolution __name__ == __main__ run_tests