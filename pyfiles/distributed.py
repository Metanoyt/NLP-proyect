Distributed computing variable tracking classes PyTorch Dynamo This module implements variable tracking distributed computing components - Process Groups collective communication - Device Meshes distributed tensor sharding - Placement Types specifying distribution strategies - Distributed Tensors their operations - Backward hooks distributed module operations These classes responsible tracking distributed operations during graph compilation while maintaining proper guards handling distributed-specific behaviors They ensure correct handling distributed components like process groups device meshes placement strategies while preserving proper semantics distributed tensor operations compiled code The implementation provides special handling distributed package availability checks proper tracking distributed state operations across processes functools inspect typing Any Sequence TYPE_CHECKING torch torch fx experimental _backward_state BackwardState compiled_autograd variables _trace_wrapped_higher_order_op trace_wrapped bytecode_transformation create_call_function exc unimplemented_v external_utils call_module_hooks_from_backward_state guards GuardBuilder install_guard source AttrSource utils istype base VariableTracker constant ConstantVariable EnumVariable TYPE_CHECKING torch _dynamo codegen PyCodegen torch _dynamo symbolic_convert InstructionTranslator DistributedVariable VariableTracker The base distributed variable encapsulates common methods distributed objects i e ProcessGroup DeviceMesh etc Concrete distributed objects could inherit add object specific logic i e It provides check distributed package existence hold tracking value corresponding distributed object __init__ value Any kwargs Any - None super __init__ kwargs DistributedVariable is_available unimplemented_v gb_type= torch distributed package available context= explanation= The PyTorch package doesn t include torch distributed when building source hints= Set USE_DISTRIBUTED= enable when building PyTorch source value = value python_type - type type value staticmethod is_available - bool check distributed package available torch distributed is_available is_from_local value object - bool DistributedVariable is_available False torch distributed tensor DTensor inspect isfunction value value DTensor from_local is_constant_pg_functions value object - bool DistributedVariable is_available False torch distributed distributed_c d _get_group_size_by_name _get_group_tag _rank_not_in_group _resolve_group_name_by_ranks_and_tag get_process_group_ranks constant_processgroup_functions = _get_group_size_by_name _get_group_tag _rank_not_in_group get_process_group_ranks _resolve_group_name_by_ranks_and_tag inspect isfunction value value constant_processgroup_functions WorldMetaClassVariable DistributedVariable Tracks torch distributed GroupMember torch distributed group which instances metaclass _WorldMeta classmethod is_group_member_type cls value object - bool cls is_available False torch distributed distributed_c d _WorldMeta type value _WorldMeta var_getattr tx InstructionTranslator name str - VariableTracker name == WORLD assert source source = AttrSource base=self source member= WORLD install_guard source make_guard GuardBuilder ID_MATCH ProcessGroupVariable value WORLD name == NON_GROUP_MEMBER assert source source = AttrSource base=self source member= NON_GROUP_MEMBER install_guard source make_guard GuardBuilder ID_MATCH EnumVariable value NON_GROUP_MEMBER super var_getattr tx name PlacementClassVariable DistributedVariable staticmethod is_placement_type value object - bool we can t rely importing accessing torch distributed always built DistributedVariable is_available False torch distributed tensor placement_types Placement isinstance value type issubclass value Placement as_python_constant - Any value call_function tx InstructionTranslator args Sequence VariableTracker kwargs dict str VariableTracker - VariableTracker source NOTE we don t need track mutations placement they supposed immutable new_obj = value __new__ value var = PlacementVariable new_obj inspect getattr_static value __init__ None var call_method tx __init__ args kwargs var super call_function tx args kwargs PlacementVariable DistributedVariable staticmethod is_placement value object - bool we can t rely importing accessing torch distributed always built DistributedVariable is_available False torch distributed tensor placement_types Placement isinstance value Placement as_python_constant - Any value var_getattr tx InstructionTranslator name str - VariableTracker name == dim ConstantVariable create value dim super var_getattr tx name call_method tx InstructionTranslator name str args Sequence VariableTracker kwargs dict str VariableTracker - VariableTracker ConstantVariable Placement types dynamo tracking only allows following methods __setattr__ case like ` Shard dim ` methods Methods list must satisfy Input arguments constants do need guarded Output constant respect their inputs constant_fold_functions = __init__ __setattr__ is_shard is_partial is_replicate name constant_fold_functions try value_type = type value inspect getattr_static value_type __getattr__ None None unimplemented_v gb_type= Placement custom __getattr__ supported context=f value_type __name__ custom __getattr__ explanation= Dynamo does support Placement types custom __getattr__ methods hints= Use Placement types without custom __getattr__ methods Move Placement usage outside compiled region method = inspect getattr_static value_type name except AttributeError method = None method object __init__ ConstantVariable create None args = x as_python_constant x args kwargs = k v as_python_constant k v kwargs items assert method None name == __setattr__ method value args kwargs constant_val = method value args kwargs ConstantVariable create constant_val super call_method tx name args kwargs type ignore arg-type reconstruct codegen PyCodegen - None Reconstruct Placement object calling its constructor e g Shard Replicate Partial torch distributed tensor placement_types Partial Replicate Shard placement_type = type value Load placement codegen add_push_null lambda codegen load_import_from torch distributed tensor placement_types placement_type __name__ For Shard we need pass dim argument isinstance value Shard codegen ConstantVariable create value dim codegen extend_output create_call_function False Replicate Partial have no required args istype value Replicate Partial codegen extend_output create_call_function False super reconstruct codegen DeviceMeshVariable DistributedVariable staticmethod is_device_mesh value object - bool we can t rely importing accessing torch distributed always built DistributedVariable is_available False torch distributed device_mesh DeviceMesh istype value DeviceMesh as_python_constant - Any value var_getattr tx InstructionTranslator name str - VariableTracker name == ndim ConstantVariable create value ndim name == device_type ConstantVariable create value device_type name == mesh_dim_names source = source source source = AttrSource base=source member= mesh_dim_names VariableTracker build tx value mesh_dim_names source super var_getattr tx name call_method tx InstructionTranslator name str args list VariableTracker kwargs dict str VariableTracker - VariableTracker name == size const_args = x as_python_constant x args const_kwargs = k v as_python_constant k v kwargs items ConstantVariable create value size const_args const_kwargs name == get_coordinate ConstantVariable create value get_coordinate name == get_rank ConstantVariable create value get_rank name == get_local_rank const_args = x as_python_constant x args const_kwargs = k v as_python_constant k v kwargs items ConstantVariable create value get_local_rank const_args const_kwargs name == get_group const_args = x as_python_constant x args const_kwargs = k v as_python_constant k v kwargs items ProcessGroupVariable value get_group const_args const_kwargs name == _get_or_create_default_group ProcessGroupVariable value _get_or_create_default_group super call_method tx name args kwargs ProcessGroupVariable DistributedVariable We don t want ProcessGroup object end up our output graph But s common dynamo intercept PG then used get info like rank world_size well passed utility functions distributed_c d which desugar into plain types like ranklist tag For convenience proper guarding we construct variable type TODO make possible use ProcessGroupVariable input simple functions like _expand_group without dynamo complaining about making proxy It tensor-like type we don t want proxy- dynamo assumes torch library functions dealing tensor-like types would have proxies their args TODO should we make inherit VT instead UDOV Do we want any default behaviors just graph-break whenever one our special cases hit as_python_constant - Any value call_method tx InstructionTranslator name str args list VariableTracker kwargs dict str VariableTracker - VariableTracker name == rank variables ConstantVariable create value rank name == size variables ConstantVariable create value size name == _get_backend_name variables ConstantVariable create value _get_backend_name super call_method tx name args kwargs var_getattr tx InstructionTranslator name str - VariableTracker name == group_name variables ConstantVariable create value group_name name rank size variables LambdaVariable lambda args kwargs call_method tx name args kwargs TODO should just raise unimplemented super var_getattr tx name staticmethod is_process_group value object - bool we can t rely importing accessing torch distributed always built DistributedVariable is_available False torch _C _distributed_c d ProcessGroup torch testing _internal distributed fake_pg FakeProcessGroup istype value ProcessGroup FakeProcessGroup BackwardHookVariable VariableTracker Handles torch utils hooks BackwardHook module-level backward hooks staticmethod create tx InstructionTranslator module VariableTracker user_hooks VariableTracker user_pre_hooks VariableTracker - BackwardHookVariable compiled_autograd compiled_autograd_enabled unimplemented_v gb_type= Module-level backwards hooks require compiled autograd context= explanation= hints= Enable compiled autograd setting torch _dynamo config compiled_autograd = True _in_graph_bw_hooks bw_state BackwardState - torch utils hooks BackwardHook Rather than installing user hooks graph which don t survive AotAutograd we install hooks will call trace_wrapped backward pass CompiledAutograd can turn into actual hook calls torch utils hooks BackwardHook None functools partial trace_wrapped fn=call_module_hooks_from_backward_state bw_state=bw_state hooks_name=user_hooks_name module_name=module_name functools partial trace_wrapped fn=call_module_hooks_from_backward_state bw_state=bw_state hooks_name=user_pre_hooks_name module_name=module_name module_name bw_state_proxy = tx output add_backward_state_hook module mod user_pre_hooks_name _ = tx output add_backward_state_hook user_pre_hooks user_hooks_name _ = tx output add_backward_state_hook user_hooks proxy = tx output create_proxy call_function _in_graph_bw_hooks bw_state_proxy proxy node meta example_value = torch utils hooks BackwardHook None BackwardHookVariable proxy module user_hooks user_pre_hooks __init__ proxy torch fx Proxy module VariableTracker user_hooks VariableTracker user_pre_hooks VariableTracker options Any - None super __init__ options proxy = proxy module = module user_hooks = user_hooks user_pre_hooks = user_pre_hooks as_proxy - torch fx Proxy proxy call_method tx InstructionTranslator name str args list VariableTracker kwargs dict str VariableTracker - VariableTracker name setup_input_hook setup_output_hook _setup_hook tx name args kwargs super call_method tx name args kwargs _setup_hook tx InstructionTranslator hook_method_name str args VariableTracker - VariableTracker builder wrap_fx_proxy wrap_fx_proxy tx tx output create_proxy call_method hook_method_name as_proxy args as_proxy