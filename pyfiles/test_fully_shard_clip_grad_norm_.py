Owner s oncall distributed copy functools typing Optional Union torch torch nn nn torch distributed _composable replicate torch distributed device_mesh DeviceMesh init_device_mesh torch distributed fsdp fully_shard torch distributed tensor debug CommDebugMode torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest get_devtype MLPStack torch testing _internal common_utils run_tests TEST_XPU xfailIf torch testing _internal distributed _tensor common_dtensor ModelArgs Transformer TransformerBlock device_type = torch device get_devtype _TestClipGradNormBase FSDPTest _test_clip_grad_norm max_norm Union float int norm_type Union float int ref_model nn Module ref_optim torch optim Optimizer model nn Module optim torch optim Optimizer inp torch Tensor dp_mesh Optional DeviceMesh = None vector_norm_fn = functools partial torch linalg vector_norm ord=norm_type dp_mesh = dp_mesh init_device_mesh device_type type world_size torch manual_seed + dp_mesh get_local_rank + _ range ref_optim zero_grad ref_model inp sum backward optim zero_grad model inp sum backward ref_grads = p grad detach clone p ref_model parameters local_grads = p grad to_local detach clone p model parameters ref_grad param zip ref_grads model parameters assertEqual ref_grad param grad full_tensor Check least one gradient has norm greater than max norm before clipping ensure clipping vacuous assertTrue any vector_norm_fn g item max_norm g ref_grads assertTrue any vector_norm_fn g item max_norm g local_grads Check gradient norm clipping via total norm individual gradient norms post-clipping ref_total_norm = torch nn utils clip_grad_norm_ ref_model parameters max_norm=max_norm norm_type=norm_type comm_mode = CommDebugMode comm_mode foreach default turn so we don t need specify total_norm = torch nn utils clip_grad_norm_ model parameters max_norm=max_norm norm_type=norm_type assertEqual ref_total_norm total_norm full_tensor Expect one all-reduce per mesh dim partial - replicate expected_all_reduces = len total_norm placements assertEqual comm_mode get_comm_counts torch ops c d_functional all_reduce expected_all_reduces For zero gradients clipping has no effect param grad zip ref_model parameters ref_grads assertTrue vector_norm_fn param grad item = max_norm torch count_nonzero grad assertFalse torch equal param grad grad param grad zip model parameters local_grads assertTrue vector_norm_fn param grad to_local item = max_norm torch count_nonzero grad assertFalse torch equal param grad to_local grad TestClipGradNormWorldSize _TestClipGradNormBase property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_clip_grad_norm_ d norm_type float inf torch manual_seed model_args = ModelArgs dropout_p= model = Transformer model_args ref_model = replicate copy deepcopy model device_type ref_optim = torch optim Adam ref_model parameters lr= e- module model modules isinstance module TransformerBlock fully_shard module fully_shard model optim = torch optim Adam model parameters lr= e- inp = torch randint model model_args vocab_size device=device_type _test_clip_grad_norm norm_type ref_model ref_optim model optim inp TestClipGradNormWorldSize _TestClipGradNormBase property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu xfailIf TEST_XPU https github com intel torch-xpu-ops issues test_clip_grad_norm_ d norm_type float inf dp_size = global_mesh = init_device_mesh device_type type dp_size world_size dp_size mesh_dim_names= dp tp dp_mesh tp_mesh = global_mesh dp global_mesh tp torch manual_seed Test using MLP stack transformer since transformer has some more significant numeric differences TP model = MLPStack with_seq_parallel=True ref_model = replicate copy deepcopy model device_type process_group=dp_mesh get_group ref_optim = torch optim Adam ref_model parameters lr= e- model parallelize tp_mesh dp_mesh use_activation_checkpointing=False reshard_after_forward=True optim = torch optim Adam model parameters lr= e- inp = torch randn device=device_type _test_clip_grad_norm norm_type ref_model ref_optim model optim inp dp_mesh __name__ == __main__ run_tests