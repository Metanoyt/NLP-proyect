numbers warnings collections namedtuple torch torch jit jit torch nn nn torch Tensor torch nn Parameter Some helper classes writing custom TorchScript LSTMs Goals - Classes easy read use extend - Performance custom LSTMs approach fused-kernel-levels speed A few notes about features we could add clean up below code - Support enumerate nn ModuleList https github com pytorch pytorch issues - Support enumerate zip lists https github com pytorch pytorch issues - Support overriding methods https github com pytorch pytorch issues - Support passing around user-defined namedtuple types readability - Support slicing w range It enables reversing lists easily https github com pytorch pytorch issues - Multiline type annotations List List Tuple Tensor Tensor verbose https github com pytorch pytorch pull script_lstm input_size hidden_size num_layers bias=True batch_first=False dropout=False bidirectional=False Returns ScriptModule mimics PyTorch native LSTM The following implemented assert bias assert batch_first bidirectional stack_type = StackedLSTM layer_type = BidirLSTMLayer dirs = dropout stack_type = StackedLSTMWithDropout layer_type = LSTMLayer dirs = stack_type = StackedLSTM layer_type = LSTMLayer dirs = stack_type num_layers layer_type first_layer_args= LSTMCell input_size hidden_size other_layer_args= LSTMCell hidden_size dirs hidden_size script_lnlstm input_size hidden_size num_layers bias=True batch_first=False dropout=False bidirectional=False decompose_layernorm=False Returns ScriptModule mimics PyTorch native LSTM The following implemented assert bias assert batch_first assert dropout bidirectional stack_type = StackedLSTM layer_type = BidirLSTMLayer dirs = stack_type = StackedLSTM layer_type = LSTMLayer dirs = stack_type num_layers layer_type first_layer_args= LayerNormLSTMCell input_size hidden_size decompose_layernorm other_layer_args= LayerNormLSTMCell hidden_size dirs hidden_size decompose_layernorm LSTMState = namedtuple LSTMState hx cx reverse lst list Tensor - list Tensor lst - LSTMCell jit ScriptModule __init__ input_size hidden_size super __init__ input_size = input_size hidden_size = hidden_size weight_ih = Parameter torch randn hidden_size input_size weight_hh = Parameter torch randn hidden_size hidden_size bias_ih = Parameter torch randn hidden_size bias_hh = Parameter torch randn hidden_size jit script_method forward input Tensor state tuple Tensor Tensor - tuple Tensor tuple Tensor Tensor hx cx = state gates = torch mm input weight_ih t + bias_ih + torch mm hx weight_hh t + bias_hh ingate forgetgate cellgate outgate = gates chunk ingate = torch sigmoid ingate forgetgate = torch sigmoid forgetgate cellgate = torch tanh cellgate outgate = torch sigmoid outgate cy = forgetgate cx + ingate cellgate hy = outgate torch tanh cy hy hy cy LayerNorm jit ScriptModule __init__ normalized_shape super __init__ isinstance normalized_shape numbers Integral normalized_shape = normalized_shape normalized_shape = torch Size normalized_shape XXX This true our LSTM NLP use case helps simplify code assert len normalized_shape == weight = Parameter torch ones normalized_shape bias = Parameter torch zeros normalized_shape normalized_shape = normalized_shape jit script_method compute_layernorm_stats input mu = input mean - keepdim=True sigma = input std - keepdim=True unbiased=False mu sigma jit script_method forward input mu sigma = compute_layernorm_stats input input - mu sigma weight + bias LayerNormLSTMCell jit ScriptModule __init__ input_size hidden_size decompose_layernorm=False super __init__ input_size = input_size hidden_size = hidden_size weight_ih = Parameter torch randn hidden_size input_size weight_hh = Parameter torch randn hidden_size hidden_size The layernorms provide learnable biases decompose_layernorm ln = LayerNorm ln = nn LayerNorm layernorm_i = ln hidden_size layernorm_h = ln hidden_size layernorm_c = ln hidden_size jit script_method forward input Tensor state tuple Tensor Tensor - tuple Tensor tuple Tensor Tensor hx cx = state igates = layernorm_i torch mm input weight_ih t hgates = layernorm_h torch mm hx weight_hh t gates = igates + hgates ingate forgetgate cellgate outgate = gates chunk ingate = torch sigmoid ingate forgetgate = torch sigmoid forgetgate cellgate = torch tanh cellgate outgate = torch sigmoid outgate cy = layernorm_c forgetgate cx + ingate cellgate hy = outgate torch tanh cy hy hy cy LSTMLayer jit ScriptModule __init__ cell cell_args super __init__ cell = cell cell_args jit script_method forward input Tensor state tuple Tensor Tensor - tuple Tensor tuple Tensor Tensor inputs = input unbind outputs = torch jit annotate list Tensor i range len inputs out state = cell inputs i state outputs += out torch stack outputs state ReverseLSTMLayer jit ScriptModule __init__ cell cell_args super __init__ cell = cell cell_args jit script_method forward input Tensor state tuple Tensor Tensor - tuple Tensor tuple Tensor Tensor inputs = reverse input unbind outputs = jit annotate list Tensor i range len inputs out state = cell inputs i state outputs += out torch stack reverse outputs state BidirLSTMLayer jit ScriptModule __constants__ = directions __init__ cell cell_args super __init__ directions = nn ModuleList LSTMLayer cell cell_args ReverseLSTMLayer cell cell_args jit script_method forward input Tensor states list tuple Tensor Tensor - tuple Tensor list tuple Tensor Tensor List LSTMState forward LSTMState backward LSTMState outputs = jit annotate list Tensor output_states = jit annotate list tuple Tensor Tensor XXX enumerate https github com pytorch pytorch issues i = direction directions state = states i out out_state = direction input state outputs += out output_states += out_state i += noqa SIM torch cat outputs - output_states init_stacked_lstm num_layers layer first_layer_args other_layer_args layers = layer first_layer_args + layer other_layer_args _ range num_layers - nn ModuleList layers StackedLSTM jit ScriptModule __constants__ = layers Necessary iterating through layers __init__ num_layers layer first_layer_args other_layer_args super __init__ layers = init_stacked_lstm num_layers layer first_layer_args other_layer_args jit script_method forward input Tensor states list tuple Tensor Tensor - tuple Tensor list tuple Tensor Tensor List LSTMState One state per layer output_states = jit annotate list tuple Tensor Tensor output = input XXX enumerate https github com pytorch pytorch issues i = rnn_layer layers state = states i output out_state = rnn_layer output state output_states += out_state i += noqa SIM output output_states Differs StackedLSTM its forward method takes List List Tuple Tensor Tensor It would nice subclass StackedLSTM except we don t support overriding script methods https github com pytorch pytorch issues StackedLSTM jit ScriptModule __constants__ = layers Necessary iterating through layers __init__ num_layers layer first_layer_args other_layer_args super __init__ layers = init_stacked_lstm num_layers layer first_layer_args other_layer_args jit script_method forward input Tensor states list list tuple Tensor Tensor - tuple Tensor list list tuple Tensor Tensor List List LSTMState The outer list layers inner list directions output_states = jit annotate list list tuple Tensor Tensor output = input XXX enumerate https github com pytorch pytorch issues i = rnn_layer layers state = states i output out_state = rnn_layer output state output_states += out_state i += noqa SIM output output_states StackedLSTMWithDropout jit ScriptModule Necessary iterating through layers dropout support __constants__ = layers num_layers __init__ num_layers layer first_layer_args other_layer_args super __init__ layers = init_stacked_lstm num_layers layer first_layer_args other_layer_args Introduces Dropout layer outputs each LSTM layer except last layer dropout probability = num_layers = num_layers num_layers == warnings warn dropout lstm adds dropout layers after all last recurrent layer expects num_layers greater than got num_layers = dropout_layer = nn Dropout jit script_method forward input Tensor states list tuple Tensor Tensor - tuple Tensor list tuple Tensor Tensor List LSTMState One state per layer output_states = jit annotate list tuple Tensor Tensor output = input XXX enumerate https github com pytorch pytorch issues i = rnn_layer layers state = states i output out_state = rnn_layer output state Apply dropout layer except last layer i num_layers - output = dropout_layer output output_states += out_state i += noqa SIM output output_states flatten_states states states = list zip states assert len states == torch stack state state states double_flatten_states states XXX Can probably write nicer way states = flatten_states flatten_states inner inner states hidden view - + list hidden shape hidden states test_script_rnn_layer seq_len batch input_size hidden_size inp = torch randn seq_len batch input_size state = LSTMState torch randn batch hidden_size torch randn batch hidden_size rnn = LSTMLayer LSTMCell input_size hidden_size out out_state = rnn inp state Control pytorch native LSTM lstm = nn LSTM input_size hidden_size lstm_state = LSTMState state hx unsqueeze state cx unsqueeze lstm_param custom_param zip lstm all_weights rnn parameters assert lstm_param shape == custom_param shape torch no_grad lstm_param copy_ custom_param lstm_out lstm_out_state = lstm inp lstm_state assert out - lstm_out abs max e- assert out_state - lstm_out_state abs max e- assert out_state - lstm_out_state abs max e- test_script_stacked_rnn seq_len batch input_size hidden_size num_layers inp = torch randn seq_len batch input_size states = LSTMState torch randn batch hidden_size torch randn batch hidden_size _ range num_layers rnn = script_lstm input_size hidden_size num_layers out out_state = rnn inp states custom_state = flatten_states out_state Control pytorch native LSTM lstm = nn LSTM input_size hidden_size num_layers lstm_state = flatten_states states layer range num_layers custom_params = list rnn parameters layer layer + lstm_param custom_param zip lstm all_weights layer custom_params assert lstm_param shape == custom_param shape torch no_grad lstm_param copy_ custom_param lstm_out lstm_out_state = lstm inp lstm_state assert out - lstm_out abs max e- assert custom_state - lstm_out_state abs max e- assert custom_state - lstm_out_state abs max e- test_script_stacked_bidir_rnn seq_len batch input_size hidden_size num_layers inp = torch randn seq_len batch input_size states = LSTMState torch randn batch hidden_size torch randn batch hidden_size _ range _ range num_layers rnn = script_lstm input_size hidden_size num_layers bidirectional=True out out_state = rnn inp states custom_state = double_flatten_states out_state Control pytorch native LSTM lstm = nn LSTM input_size hidden_size num_layers bidirectional=True lstm_state = double_flatten_states states layer range num_layers direct range index = layer + direct custom_params = list rnn parameters index index + lstm_param custom_param zip lstm all_weights index custom_params assert lstm_param shape == custom_param shape torch no_grad lstm_param copy_ custom_param lstm_out lstm_out_state = lstm inp lstm_state assert out - lstm_out abs max e- assert custom_state - lstm_out_state abs max e- assert custom_state - lstm_out_state abs max e- test_script_stacked_lstm_dropout seq_len batch input_size hidden_size num_layers inp = torch randn seq_len batch input_size states = LSTMState torch randn batch hidden_size torch randn batch hidden_size _ range num_layers rnn = script_lstm input_size hidden_size num_layers dropout=True just smoke test out out_state = rnn inp states test_script_stacked_lnlstm seq_len batch input_size hidden_size num_layers inp = torch randn seq_len batch input_size states = LSTMState torch randn batch hidden_size torch randn batch hidden_size _ range num_layers rnn = script_lnlstm input_size hidden_size num_layers just smoke test out out_state = rnn inp states test_script_rnn_layer test_script_stacked_rnn test_script_stacked_bidir_rnn test_script_stacked_lstm_dropout test_script_stacked_lnlstm