Owner s module onnx copy io re warnings onnx parameterized pytorch_test_common torchvision autograd_helper CustomFunction CustomFunction pytorch_test_common skipIfNoCuda skipIfUnsupportedMaxOpsetVersion skipIfUnsupportedMinOpsetVersion torch torch onnx torch utils cpp_extension torch onnx _constants OperatorExportTypes TrainingMode utils torch onnx _internal torchscript_exporter _globals GLOBALS torch onnx symbolic_helper _unpack_list parse_args torch testing _internal common_utils torch testing _internal common_utils skipIfNoLapack _remove_test_environment_prefix_from_scope_name scope_name str - str Remove test environment prefix added module Remove prefix normalize scope names since different test environments add prefixes slight differences Example _remove_test_environment_prefix_from_scope_name test_utility_funs M M _remove_test_environment_prefix_from_scope_name test_utility_funs test_abc locals M M _remove_test_environment_prefix_from_scope_name __main__ M M prefixes_to_remove = test_utility_funs __main__ prefix prefixes_to_remove scope_name = re sub f prefix \\ locals \\ scope_name scope_name _BaseTestCase pytorch_test_common ExportTestCase _model_to_graph model input do_constant_folding=True training=TrainingMode EVAL operator_export_type=OperatorExportTypes ONNX input_names=None dynamic_axes=None torch onnx utils _setup_trace_module_map model False training == torch onnx TrainingMode TRAINING model train training == torch onnx TrainingMode EVAL model eval utils _validate_dynamic_axes dynamic_axes model None None graph params_dict torch_out = utils _model_to_graph model input do_constant_folding=do_constant_folding _disable_torch_constant_prop=True operator_export_type=operator_export_type training=training input_names=input_names dynamic_axes=dynamic_axes graph params_dict torch_out parameterized parameterized_class opset_version opset opset range _constants ONNX_BASE_OPSET _constants ONNX_TORCHSCRIPT_EXPORTER_MAX_OPSET + class_name_func=lambda cls num params_dict f cls __name__ _opset_ params_dict opset_version TestUtilityFuns _BaseTestCase opset_version = None test_is_in_onnx_export test_self = MyModule torch nn Module forward x test_self assertTrue torch onnx is_in_onnx_export raise ValueError x + x = torch randn f = io BytesIO try torch onnx export MyModule x f opset_version=self opset_version dynamo=False except ValueError assertFalse torch onnx is_in_onnx_export test_validate_dynamic_axes_invalid_input_output_name warnings catch_warnings record=True w warnings simplefilter always utils _validate_dynamic_axes input output invalid_name invalid_name None input input output messages = str warning message warning w assertIn Provided key invalid_name dynamic axes valid input output name messages assertIn Provided key invalid_name dynamic axes valid input output name messages assertEqual len messages skipIfUnsupportedMinOpsetVersion test_split_to_slice SplitModule torch nn Module forward x y t splits = x size y size out out = torch split t splits dim= out out GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX x = torch randn y = torch randn t = torch randn graph _ _ = _model_to_graph SplitModule x y t input_names= x y t dynamic_axes= x y t node graph nodes assertNotEqual node kind onnx SplitToSequence test_constant_fold_transpose TransposeModule torch nn Module forward x = torch tensor b = torch transpose b + x GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX x = torch ones graph _ __ = _model_to_graph TransposeModule x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Transpose assertNotEqual node kind onnx Cast assertEqual len list graph nodes skipIfUnsupportedMaxOpsetVersion test_constant_fold_reduceL ReduceModule torch nn Module forward x = torch tensor b = torch norm p= dim=- keepdim=False b + x GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX x = torch ones graph _ __ = _model_to_graph ReduceModule x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx ReduceL skipIfUnsupportedMaxOpsetVersion test_constant_fold_reduceL NormModule torch nn Module forward x = torch tensor b = torch norm p= dim=- b + x GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX x = torch ones graph _ __ = _model_to_graph NormModule x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx ReduceL test_constant_fold_slice NarrowModule torch nn Module forward x = torch tensor b = torch narrow b + x GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX x = torch ones graph _ __ = _model_to_graph NarrowModule x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Slice assertNotEqual node kind onnx Cast assertEqual len list graph nodes test_constant_fold_slice_index_exceeds_dim SliceIndexExceedsDimModule torch nn Module forward x = torch tensor b = index exceeds dimension b + x GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX x = torch ones graph _ __ = _model_to_graph SliceIndexExceedsDimModule x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Slice assertNotEqual node kind onnx Cast assertEqual len list graph nodes test_constant_fold_slice_negative_index SliceNegativeIndexModule torch nn Module forward x = torch tensor b = - index relative end c = torch select dim=- index=- d = torch select dim= index= b + x c + d GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX x = torch ones graph _ __ = _model_to_graph SliceNegativeIndexModule x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Slice assertNotEqual node kind onnx Cast test_constant_fold_gather GatherModule torch nn Module forward x = torch tensor b = torch select dim= index=- c = torch index_select dim=- index=torch tensor b + c + x GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX x = torch ones model = GatherModule model x graph _ __ = _model_to_graph GatherModule x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Gather test_constant_fold_unsqueeze UnsqueezeModule torch nn Module forward x = torch tensor b = torch unsqueeze - b + x GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX x = torch ones graph _ __ = _model_to_graph UnsqueezeModule x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Unsqueeze assertNotEqual node kind onnx Cast assertEqual len list graph nodes test_constant_fold_unsqueeze_multi_axies PReluModel torch nn Module __init__ - None super __init__ prelu = torch nn PReLU forward x = torch randn prelu x + GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX x = torch randn graph _ __ = _model_to_graph PReluModel x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Unsqueeze assertNotEqual node kind onnx Cast assertEqual len list graph nodes test_constant_fold_squeeze_without_axes SqueezeModule torch nn Module forward x = torch tensor torch squeeze + x + torch squeeze GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX x = torch ones graph _ __ = _model_to_graph SqueezeModule x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Squeeze assertNotEqual node kind onnx Cast assertEqual len list graph nodes test_constant_fold_squeeze_with_axes SqueezeAxesModule torch nn Module forward x = torch tensor torch squeeze dim=- + x GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX x = torch ones graph _ __ = _model_to_graph SqueezeAxesModule x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Squeeze assertNotEqual node kind onnx Cast assertEqual len list graph nodes test_constant_fold_concat ConcatModule torch nn Module forward x Why did I insert Cast here There appears intentional behavior ONNX constant folding where constant tensors which attached any known foldable onnx operations don t get extracted into initializer graph So without these casts we will actually fail pull out one constants thus failing constant folding I think test wrong I don t have time write more correct test I think right way go about test setup predicate what invariant graphs should hold after constant folding then verify predicate holds I think asserts below attempt predicate right More commentary https github com pytorch pytorch pull files#r = torch tensor torch float b = torch tensor torch float c = torch cat b d = b + c x + d GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX x = torch ones graph _ __ = _model_to_graph ConcatModule x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Concat assertNotEqual node kind onnx Cast assertEqual len list graph nodes test_constant_fold_lstm GruNet torch nn Module __init__ - None super __init__ mygru = torch nn GRU bidirectional=False forward input initial_state mygru input initial_state GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX input = torch randn h = torch randn graph _ __ = _model_to_graph GruNet input h input_names= input h dynamic_axes= input h node graph nodes assertNotEqual node kind onnx Slice assertNotEqual node kind onnx Concat assertNotEqual node kind onnx Unsqueeze opset_version = assertEqual len list graph nodes Unsqueeze op parameter axes input instead attribute when opset version = assertEqual len list graph nodes test_constant_fold_transpose_matmul MatMulNet torch nn Module __init__ - None super __init__ B = torch nn Parameter torch ones forward A torch matmul A torch transpose B - - GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX A = torch randn graph _ __ = _model_to_graph MatMulNet A input_names= A dynamic_axes= A node graph nodes assertNotEqual node kind onnx Transpose assertEqual len list graph nodes test_constant_fold_reshape ReshapeModule torch nn Module __init__ super __init__ weight = torch nn Buffer torch ones forward x b = weight reshape - x b GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX x = torch randn graph _ __ = _model_to_graph ReshapeModule x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Reshape assertEqual len list graph nodes test_constant_fold_div Module torch nn Module __init__ super __init__ weight = torch nn Buffer torch ones forward x div = weight div torch tensor div x x = torch randn GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX graph _ __ = _model_to_graph Module x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Div assertEqual len list graph nodes test_constant_fold_mul Module torch nn Module __init__ super __init__ weight = torch nn Buffer torch ones forward x mul = weight mul torch tensor mul x x = torch randn GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX graph _ __ = _model_to_graph Module x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Mul assertEqual len list graph nodes test_constant_fold_add Module torch nn Module __init__ super __init__ weight = torch nn Buffer torch ones forward x add = weight + torch tensor add - x x = torch randn GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX graph params_dict __ = _model_to_graph Module x do_constant_folding=True operator_export_type=OperatorExportTypes ONNX input_names= x dynamic_axes= x node graph nodes assertTrue node kind = onnx Add assertEqual len list graph nodes params = list params_dict values assertEqual len params weight = params assertEqual weight torch tensor test_constant_fold_sub Module torch nn Module __init__ super __init__ weight = torch nn Buffer torch ones forward x sub = weight - torch tensor sub + x x = torch randn GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX graph params_dict __ = _model_to_graph Module x do_constant_folding=True operator_export_type=OperatorExportTypes ONNX input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Sub assertEqual len list graph nodes params = list params_dict values assertEqual len params weight = params assertEqual weight torch tensor - - - - test_constant_fold_sqrt Module torch nn Module __init__ super __init__ weight = torch nn Buffer torch ones forward x sqrt = torch sqrt weight sqrt x x = torch randn GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX graph _ __ = _model_to_graph Module x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Sqrt assertEqual len list graph nodes test_constant_fold_shape ShapeModule torch nn Module __init__ - None super __init__ weight = torch nn Buffer torch ones forward x shape = weight shape x + shape x = torch randn GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX graph _ __ = _model_to_graph ShapeModule x input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx Shape assertEqual len list graph nodes test_constant_fold_upsample_scale_fold_as_constant upsample scale constant model parameter therefore should added initializer after constant folding model = torch nn Upsample scale_factor= mode= bilinear align_corners=True x = torch randn f = io BytesIO torch onnx export model x f dynamo=False onnx_model = onnx load io BytesIO f getvalue assertEqual len onnx_model graph initializer test_verbose MyModule torch nn Module forward input torch exp input x = torch randn is_model_stripped f verbose=None verbose None torch onnx export MyModule x f opset_version=self opset_version dynamo=False torch onnx export MyModule x f verbose=verbose opset_version=self opset_version dynamo=False model = onnx load io BytesIO f getvalue model_strip = copy copy model onnx helper strip_doc_string model_strip model == model_strip test verbose=False default assertTrue is_model_stripped io BytesIO test verbose=True assertFalse is_model_stripped io BytesIO True NB remove test once DataParallel can correctly handled test_error_on_data_parallel model = torch nn DataParallel torch nn ReflectionPad d x = torch randn f = io BytesIO assertRaisesRegex ValueError torch nn DataParallel supported ONNX exporter please use attribute module unwrap model torch nn DataParallel Try torch onnx export model x f opset_version=self opset_version dynamo=False skipIfUnsupportedMinOpsetVersion test_sequence_dim Module torch nn Module forward x y x y model = Module Export scripting keep output Sequence type Tracing unpacks list script_model = torch jit script model x = torch randn Case dynamic axis f = io BytesIO y = torch randn torch onnx export script_model x y f opset_version=self opset_version input_names= x y dynamic_axes= y dynamo=False onnx_model = onnx load io BytesIO f getvalue loop_output_value_info_proto = onnx_model graph output ref_value_info_proto = onnx helper make_tensor_sequence_value_info loop_output_value_info_proto name None assertEqual loop_output_value_info_proto ref_value_info_proto Case no dynamic axes f = io BytesIO y = torch randn torch onnx export script_model x y f opset_version=self opset_version dynamo=False onnx_model = onnx load io BytesIO f getvalue loop_output_value_info_proto = onnx_model graph output ref_value_info_proto = onnx helper make_tensor_sequence_value_info loop_output_value_info_proto name assertEqual loop_output_value_info_proto ref_value_info_proto test_export_mode MyModule torch nn Module forward x y = x + y model = MyModule x = torch randn f = io BytesIO set mode inference mode export training mode model eval old_state = model training torch onnx export model x f opset_version=self opset_version training=torch onnx TrainingMode TRAINING dynamo=False verify model state preserved assertEqual model training old_state set mode training mode export inference mode model train old_state = model training torch onnx export model x f opset_version=self opset_version training=torch onnx TrainingMode EVAL dynamo=False verify model state preserved assertEqual model training old_state test_export_does_not_fail_on_frozen_scripted_module Inner torch nn Module forward x x x x x Outer torch nn Module __init__ - None super __init__ inner = torch jit script Inner forward x inner x x = torch zeros Freezing only implemented eval mode So we need call eval outer_module = Outer eval module = torch jit trace_module outer_module forward x jit freeze removes training attribute module module = torch jit freeze module torch onnx export module x io BytesIO opset_version=self opset_version dynamo=False skipIfUnsupportedMinOpsetVersion test_local_function N torch nn Module __init__ prob super __init__ dropout = torch nn Dropout prob forward x dropout x M torch nn Module __init__ num_layers super __init__ num_layers = num_layers lns = torch nn ModuleList torch nn LayerNorm eps=i i range num_layers celu = torch nn CELU celu = torch nn CELU dropout = N forward x y z res = celu x res = celu y ln lns z = ln z res + res dropout z x = torch randn y = torch randn z = torch randn Export specified modules Test against specifying modules won t exist exported model Model export inference mode will remove dropout node thus dropout module no longer exist graph f = io BytesIO torch onnx export M x y z f opset_version=self opset_version export_modules_as_functions= torch nn CELU torch nn Dropout torch nn LayerNorm dynamo=False onnx_model = onnx load io BytesIO f getvalue Check function definition funcs = onnx_model functions celu_funcs = f f funcs f name == CELU assertEqual len celu_funcs assertEqual celu_funcs domain torch nn modules activation assertEqual len celu_funcs attribute ln_funcs = f f funcs f name == LayerNorm assertEqual len ln_funcs assertEqual ln_funcs domain torch nn modules normalization assertEqual len ln_funcs attribute Check local function nodes nodes = onnx_model graph node celu_ns = n n nodes n op_type == CELU ln_ns = n n nodes n op_type == LayerNorm assertEqual len celu_ns assertEqual celu_ns domain torch nn modules activation assertEqual len celu_ns attribute assertEqual len ln_ns assertEqual ln_ns domain torch nn modules normalization assertEqual len ln_ns attribute Export specified modules f = io BytesIO torch onnx export M x y z f opset_version=self opset_version export_modules_as_functions= torch nn CELU dynamo=False onnx_model = onnx load io BytesIO f getvalue funcs = onnx_model functions assertEqual len funcs assertEqual funcs name CELU Export empty specified modules Normal export f = io BytesIO torch onnx export M x y z f opset_version=self opset_version export_modules_as_functions=set dynamo=False onnx_model = onnx load io BytesIO f getvalue funcs = onnx_model functions assertEqual len funcs Export all modules Should contain M CELU LayerNorm f = io BytesIO torch onnx export M x y z f opset_version=self opset_version export_modules_as_functions=True dynamo=False onnx_model = onnx load io BytesIO f getvalue funcs = onnx_model functions assertEqual len funcs skipIfUnsupportedMinOpsetVersion test_local_function_overloads NWithOverloads torch nn Module forward x y=None z=None y None x + z None x + y x + y x + z M torch nn Module __init__ num_layers super __init__ n = NWithOverloads forward x y z n x n x y n x y z x = torch randn y = torch randn z = torch randn f = io BytesIO torch onnx export M x y z f opset_version=self opset_version export_modules_as_functions= NWithOverloads dynamo=False onnx_model = onnx load io BytesIO f getvalue funcs = onnx_model functions assertEqual len funcs func_names = f name f funcs assertIn NWithOverloads func_names assertIn NWithOverloads func_names assertIn NWithOverloads func_names Failing after ONNX skipIfUnsupportedMaxOpsetVersion test_local_function_infer_scopes M torch nn Module forward x Concatenation scalars inserts unscoped tensors IR graph new_tensor_shape = x size - + - tensor = x view new_tensor_shape tensor x = torch randn f = io BytesIO torch onnx export M x f export_modules_as_functions=True opset_version=self opset_version do_constant_folding=False dynamo=False onnx_model = onnx load io BytesIO f getvalue funcs = onnx_model functions assertIn M f name f funcs skipIfUnsupportedMinOpsetVersion test_local_function_predefined_attributes M torch nn Module num_layers int __init__ num_layers super __init__ num_layers = num_layers lns = torch nn ModuleList torch nn LayerNorm eps= e- _ range num_layers forward x ln lns x = ln x x x = torch randn f = io BytesIO model = M torch onnx export model x f export_modules_as_functions=True opset_version=self opset_version dynamo=False onnx_model = onnx load io BytesIO f getvalue funcs = onnx_model functions m_funcs = fn fn funcs fn name == M assertEqual m_funcs attribute num_layers ln_funcs = fn fn funcs fn name == LayerNorm assertEqual ln_funcs attribute eps elementwise_affine onnx helper m_node = n n onnx_model graph node n op_type == M assertEqual m_node attribute helper make_attribute num_layers model num_layers ln_nodes = n n m_funcs node n op_type == LayerNorm expected_ln_attrs = helper make_attribute elementwise_affine model lns elementwise_affine helper make_attribute eps model lns eps ln_node ln_nodes assertIn ln_node attribute expected_ln_attrs assertIn ln_node attribute expected_ln_attrs This test cases checks issue where object does have attribute When enabling ` export_modules_as_functions = True ` exporter could AttributeError With test case we check export passes successfully without any AttributeError exceptions See https github com pytorch pytorch pull example The exception test tries avoid ` AttributeError Embedding object has no attribute freeze ` skipIfUnsupportedMinOpsetVersion test_local_function_subset_of_predefined_attributes M torch nn Module num_layers int __init__ num_layers super __init__ embed_layer = torch nn Embedding from_pretrained torch FloatTensor num_layers = num_layers lns = torch nn ModuleList torch nn LayerNorm eps= e- _ range num_layers forward x e = embed_layer torch LongTensor ln lns x = ln x x e x = torch randn f = io BytesIO model = M torch onnx export model x f export_modules_as_functions=True opset_version=self opset_version verbose=True Allows test case print ` Skipping module attribute freeze ` dynamo=False test_node_scope N torch nn Module __init__ - None super __init__ relu = torch nn ReLU forward x relu x M torch nn Module __init__ num_layers super __init__ num_layers = num_layers lns = torch nn ModuleList torch nn LayerNorm eps=float i i range num_layers gelu = torch nn GELU gelu = torch nn GELU relu = N forward x y z res = gelu x res = gelu y ln lns z = ln z res + res relu z x = torch randn y = torch randn z = torch randn model = M expected_scope_names = M torch nn modules activation GELU gelu M torch nn modules activation GELU gelu M torch nn modules normalization LayerNorm lns M torch nn modules normalization LayerNorm lns M torch nn modules normalization LayerNorm lns M N relu torch nn modules activation ReLU relu M graph _ _ = _model_to_graph model x y z input_names= dynamic_axes= node graph nodes assertIn _remove_test_environment_prefix_from_scope_name node scopeName expected_scope_names graph _ _ = _model_to_graph torch jit script model x y z input_names= dynamic_axes= node graph nodes assertIn _remove_test_environment_prefix_from_scope_name node scopeName expected_scope_names test_scope_of_constants_when_combined_by_cse_pass layer_num = M torch nn Module __init__ constant super __init__ constant = constant forward x constant designed same all layers hence common sub expression x + constant N torch nn Module __init__ layers int = layer_num super __init__ layers = torch nn ModuleList M constant=torch tensor i range layers forward x layer layers x = layer x x graph _ _ = _model_to_graph N torch randn input_names= dynamic_axes= NOTE Duplicated constants populated due implicit casting scalar_type_analysis so we expect constants different scopes The constants layers If CSE exporter improved later test needs updated It should expect constant same scope root expected_root_scope_name = N expected_layer_scope_name = M layers expected_constant_scope_name = f expected_root_scope_name expected_layer_scope_name i i range layer_num constant_scope_names = node graph nodes node kind == onnx Constant constant_scope_names append _remove_test_environment_prefix_from_scope_name node scopeName assertEqual constant_scope_names expected_constant_scope_name test_scope_of_nodes_when_combined_by_cse_pass layer_num = M torch nn Module __init__ constant bias super __init__ constant = constant bias = bias forward x constant x designed same all layers hence ` x + constant ` common sub expression bias designed different all layers hence ` bias ` common sub expression x + constant bias N torch nn Module __init__ layers int = layer_num super __init__ layers = torch nn ModuleList M constant=torch tensor bias=torch randn i range layers forward x y = layer layers y append layer x y y y graph _ _ = _model_to_graph N torch randn input_names= dynamic_axes= expected_root_scope_name = N expected_layer_scope_name = M layers expected_add_scope_names = f expected_root_scope_name expected_layer_scope_name expected_mul_scope_names = f expected_root_scope_name expected_layer_scope_name i i range layer_num add_scope_names = mul_scope_names = node graph nodes node kind == onnx Add add_scope_names append _remove_test_environment_prefix_from_scope_name node scopeName node kind == onnx Mul mul_scope_names append _remove_test_environment_prefix_from_scope_name node scopeName assertEqual add_scope_names expected_add_scope_names assertEqual mul_scope_names expected_mul_scope_names test_aten_fallthrough Test aten export op no symbolic Module torch nn Module forward x torch erfc x x = torch randn GLOBALS export_onnx_opset_version = opset_version graph _ __ = _model_to_graph Module x operator_export_type=OperatorExportTypes ONNX_FALLTHROUGH input_names= x dynamic_axes= x iter = graph nodes assertEqual next iter kind aten erfc test_custom_op_fallthrough Test custom op op_source = #include torch script h torch Tensor custom_add torch Tensor torch Tensor other + other static auto registry = torch RegisterOperators custom_namespace custom_op custom_add torch utils cpp_extension load_inline name= custom_add cpp_sources=op_source is_python_module=False verbose=True FooModel torch nn Module forward input other Calling custom op torch ops custom_namespace custom_op input other x = torch randn requires_grad=False y = torch randn requires_grad=False model = FooModel graph _ __ = _model_to_graph model x y operator_export_type=torch onnx OperatorExportTypes ONNX_FALLTHROUGH input_names= x y dynamic_axes= x y iter = graph nodes assertEqual next iter kind custom_namespace custom_op gelu exported onnx Gelu opset = skipIfUnsupportedMaxOpsetVersion test_custom_opsets_gelu addCleanup torch onnx unregister_custom_op_symbolic gelu gelu g approximate g op com microsoft Gelu setType type torch onnx register_custom_op_symbolic gelu gelu model = torch nn GELU approximate= none x = torch randn f = io BytesIO torch onnx export model x f opset_version=self opset_version custom_opsets= com microsoft dynamo=False graph = onnx load io BytesIO f getvalue assertEqual graph graph node op_type Gelu assertEqual graph opset_import version opset_version assertEqual graph opset_import domain com microsoft assertEqual graph opset_import version gelu exported onnx Gelu opset = skipIfUnsupportedMaxOpsetVersion test_register_aten_custom_op_symbolic addCleanup torch onnx unregister_custom_op_symbolic aten gelu gelu g approximate g op com microsoft Gelu setType type torch onnx register_custom_op_symbolic aten gelu gelu model = torch nn GELU approximate= none x = torch randn f = io BytesIO torch onnx export model x f opset_version=self opset_version dynamo=False graph = onnx load io BytesIO f getvalue assertEqual graph graph node op_type Gelu assertEqual graph opset_import domain com microsoft skipIfNoLapack test_custom_opsets_inverse addCleanup torch onnx unregister_custom_op_symbolic linalg_inv CustomInverse torch nn Module forward x torch inverse x + x linalg_inv g g op com microsoft Inverse setType type torch onnx register_custom_op_symbolic linalg_inv linalg_inv model = CustomInverse x = torch randn f = io BytesIO torch onnx export model x f opset_version=self opset_version custom_opsets= com microsoft dynamo=False graph = onnx load io BytesIO f getvalue assertEqual graph graph node op_type Inverse assertEqual graph opset_import version opset_version assertEqual graph opset_import domain com microsoft assertEqual graph opset_import version test_onnx_fallthrough Test aten export op symbolic aten Module torch nn Module forward x torch digamma x x = torch randn graph _ __ = _model_to_graph Module x operator_export_type=OperatorExportTypes ONNX_FALLTHROUGH input_names= x dynamic_axes= x iter = graph nodes assertEqual next iter kind aten digamma prim ListConstruct exported onnx SequenceConstruct opset = skipIfUnsupportedMaxOpsetVersion test_prim_fallthrough Test prim op PrimModule torch jit ScriptModule torch jit script_method forward x isinstance x list y = x y = x y x = torch tensor model = PrimModule model eval graph _ __ = _model_to_graph model x operator_export_type=OperatorExportTypes ONNX_FALLTHROUGH input_names= x dynamic_axes= x iter = graph nodes assertEqual next iter kind prim ListConstruct test_custom_layer_tuple CustomFunction torch autograd Function staticmethod symbolic g input g op CustomNamespace Custom input outputs= staticmethod forward ctx input input input Custom torch nn Module forward input CustomFunction apply input model = Custom batch = torch FloatTensor graph _ _ = _model_to_graph model batch input_names= batch dynamic_axes= batch iter = graph nodes assertEqual next iter kind CustomNamespace Custom test_autograd_onnx_fallthrough CustomFunction torch autograd Function staticmethod forward ctx input ctx save_for_backward input input clamp min= staticmethod backward ctx grad_output input = ctx saved_tensors grad_input = grad_output clone grad_input input = grad_input Custom torch nn Module forward input CustomFunction apply input model = Custom batch = torch FloatTensor graph _ _ = _model_to_graph model batch operator_export_type=OperatorExportTypes ONNX_FALLTHROUGH input_names= batch dynamic_axes= batch iter = graph nodes assertEqual next iter kind prim PythonOp test_autograd_module_name CustomFunction torch autograd Function staticmethod forward ctx input ctx save_for_backward input input clamp min= staticmethod backward ctx grad_output input = ctx saved_tensors grad_input = grad_output clone grad_input input = grad_input Custom torch nn Module forward input CustomFunction apply input + CustomFunction apply input model = Custom batch = torch FloatTensor graph _ _ = _model_to_graph model batch operator_export_type=OperatorExportTypes ONNX_FALLTHROUGH input_names= batch dynamic_axes= batch iter = graph nodes autograd = next iter autograd = next iter assertEqual autograd kind prim PythonOp assertEqual autograd kind prim PythonOp assertNotEqual autograd s module autograd s module test_unused_initializers Model torch nn Module __init__ - None super __init__ conv = torch nn ConvTranspose d stride= padding= dilation= k_proj = torch nn Linear bias=True forward x x = conv x x x = torch randn GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX _ params_dict __ = _model_to_graph Model x do_constant_folding=False operator_export_type=OperatorExportTypes ONNX input_names= x dynamic_axes= x assertEqual len params_dict test_scripting_param MyModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d kernel_size= stride= padding= bias=True bn = torch nn BatchNorm d affine=True forward x x = conv x bn = bn x bn model = torch jit script MyModule x = torch randn GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX graph _ __ = _model_to_graph model x do_constant_folding=True operator_export_type=OperatorExportTypes ONNX training=torch onnx TrainingMode TRAINING input_names= x dynamic_axes= x graph_input_params = param debugName param graph inputs item dict model named_parameters assertIn item graph_input_params Graph parameter names does match model parameters test_fuse_conv_bn Fuse torch nn Module __init__ - None super __init__ conv = torch nn Conv d kernel_size= stride= padding= bias=True bn = torch nn BatchNorm d forward x out = conv x bn out x = torch randn requires_grad=True graph _ __ = _model_to_graph Fuse x training=TrainingMode EVAL input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx BatchNormalization assertEqual node kind onnx Conv assertEqual len list graph nodes test_fuse_resnet model = torchvision models resnet weights=None x = torch randn requires_grad=True graph _ __ = _model_to_graph model x training=TrainingMode EVAL input_names= x dynamic_axes= x node graph nodes assertNotEqual node kind onnx BatchNormalization test_onnx_function_substitution_pass torch jit script f x torch Tensor y torch Tensor z = x - y x + z MyModule torch nn Module forward x y f x y input_ = torch tensor input_ = torch tensor GLOBALS export_onnx_opset_version = opset_version GLOBALS operator_export_type = OperatorExportTypes ONNX graph _ __ = _model_to_graph MyModule input_ input_ do_constant_folding=True operator_export_type=OperatorExportTypes ONNX input_names= input_ input_ dynamic_axes= input_ input_ Check prim Constant node graph representing scripted function ` f ` removed following prim CallFunction replced inline graph onnx Sub onnx Add nodes node graph nodes assertNotEqual node kind prim Constant assertEqual len list graph nodes onnx Sub onnx Add nodes only test_onnx_value_name MyModule torch nn Module __init__ - None super __init__ in_weight = torch nn Parameter torch Tensor in_bias = torch nn Parameter torch Tensor forward x start = end = None weight = in_weight bias = in_bias weight = weight start end bias None bias = bias start end torch nn functional linear x weight bias model = MyModule x = torch randn f = io BytesIO model eval torch onnx export model x f opset_version=self opset_version keep_initializers_as_inputs=True dynamo=False graph = onnx load io BytesIO f getvalue assertEqual graph graph input name in_weight assertEqual graph graph input name in_bias test_onnx_node_naming MainModule torch nn Module __init__ - None super __init__ _module_ = torch nn Linear _module_ = torch nn Linear _module_ = torch nn Linear _module_ = torch nn Linear forward x y = _module_ x z = _module_ y z = _module_ y z z = _module_ y z z module = MainModule ref_node_names = _module_ Gemm _module_ Gemm _module_ Gemm _module_ Gemm Mul Mul_ f = io BytesIO torch onnx export module torch ones f output_names= y dynamo=False onnx_model = onnx load io BytesIO f getvalue n onnx_model graph node assertIn n name ref_node_names torch onnx export torch jit script module torch ones f output_names= y dynamo=False onnx_model = onnx load io BytesIO f getvalue n onnx_model graph node assertIn n name ref_node_names _test_deduplicate_initializers torchscript=False MyModule torch nn Module __init__ - None super __init__ layer = torch nn Linear layer = torch nn Linear Reusing layers layer = layer Reusing parameters layer weight = layer weight layer bias = layer bias Parameter different tensors equal value param = torch nn Parameter torch tensor param = torch nn Parameter torch tensor forward x layer layer layer x + param + param model = torch jit script MyModule torchscript MyModule x = torch randn param_name_set = k k _ model named_parameters Test training mode model train f = io BytesIO torch onnx export model x f training=TrainingMode TRAINING opset_version=self opset_version dynamo=False graph = onnx load io BytesIO f getvalue assertSetEqual i name i graph graph initializer param_name_set model train f = io BytesIO torch onnx export model x f training=TrainingMode PRESERVE opset_version=self opset_version dynamo=False graph = onnx load io BytesIO f getvalue assertSetEqual i name i graph graph initializer param_name_set Test eval mode model eval f = io BytesIO torch onnx export model x f opset_version=self opset_version dynamo=False graph = onnx load io BytesIO f getvalue param_name_set remove param assertSetEqual i name i graph graph initializer param_name_set test_deduplicate_initializers _test_deduplicate_initializers torchscript=False test_deduplicate_initializers_torchscript _test_deduplicate_initializers torchscript=True skipIfNoCuda test_deduplicate_initializers_diff_devices Model torch nn Module __init__ - None super __init__ w_cpu = torch nn Parameter torch ones device=torch device cpu w_cuda = torch nn Parameter torch ones device=torch device cuda forward x y x + w_cpu y + w_cuda x = torch randn device=torch device cpu y = torch randn device=torch device cuda f = io BytesIO torch onnx export Model x y f opset_version=self opset_version dynamo=False graph = onnx load io BytesIO f getvalue assertSetEqual i name i graph graph initializer w_cpu test_duplicated_output_node DuplicatedOutputNet torch nn Module __init__ input_size num_classes super __init__ fc = torch nn Linear input_size num_classes forward input input out = fc input out = fc input out out out out out N D_in D_out = pt_model = DuplicatedOutputNet D_in D_out f = io BytesIO x = torch randn N D_in dynamic_axes = input input _dim input _dim input input _dim input _dim output- output- _dim output- _dim output- output- _dim output- _dim output- output- _dim output- _dim output- output- _dim output- _dim output- output- _dim output- _dim torch onnx export pt_model x x f input_names= input input output_names= output- output- output- output- output- do_constant_folding=False training=torch onnx TrainingMode TRAINING dynamic_axes=dynamic_axes verbose=True keep_initializers_as_inputs=True dynamo=False graph = onnx load io BytesIO f getvalue assertEqual graph graph input name input assertEqual graph graph input name input i range assertEqual graph graph output i name f output- i assertEqual graph graph node op_type Gemm assertEqual graph graph node op_type Identity assertEqual graph graph node op_type Identity assertEqual graph graph node op_type Gemm assertEqual graph graph node op_type Identity test_deduplicate_ignore_upsample_scale upsample scale constant model parameter therefore should ignored shared weight deduplication Model torch nn Module __init__ - None super __init__ upsample_ = torch nn Upsample scale_factor= upsample_ = torch nn Upsample scale_factor= forward x upsample_ x upsample_ x f = io BytesIO x = torch randn torch onnx export Model x f dynamo=False onnx_model = onnx load io BytesIO f getvalue aten upsample converts onnx resize resize_nodes = n n onnx_model graph node n op_type == Resize assertEqual len resize_nodes resize_node resize_nodes scale_node = n n onnx_model graph node n output == resize_node input assertEqual len scale_node assertEqual scale_node op_type Constant test_bad_symbolic_registration _onnx_opset_version = parse_args v cat g tensor_list dim tensors = _unpack_list tensor_list g op Concat tensors axis_i=dim torch onnx register_custom_op_symbolic cat cat _onnx_opset_version CatModel torch nn Module forward x torch cat x x x model = CatModel x = torch randn f = io BytesIO assertExpectedRaisesInline AssertionError lambda torch onnx export model x f opset_version=_onnx_opset_version dynamo=False A mismatch between number arguments their descriptors found symbolic function cat If you believe due custom symbolic implementation within your code external library please file issue https github com pytorch pytorch issues new template=bug-report yml report bug torch onnx unregister_custom_op_symbolic cat _onnx_opset_version __name__ == __main__ common_utils run_tests