Owner s oncall jit re torch torch _lazy metrics metrics torch _lazy ts_backend torch testing _internal common_utils run_tests TestCase torch _lazy ts_backend init NODE_TYPE_PATTERN = re compile r NodeType= ^\n + LazyFuncionalizationTest TestCase test_lazy_init_with_view f device reset_storage=False torch manual_seed device == lazy metrics reset Model torch nn Module __init__ - None super __init__ fc = torch nn Linear bias=False forward x x fc weight transpose torch device device model = Model device == lazy reset_storage torch _C _unsafe_reset_storage model fc weight torch _lazy mark_step sync_tensors = metrics counter_value SyncedTensorsWithIR reset_storage assert sync_tensors == There extra tensor being unnecessarily synced functional storage reset assert sync_tensors == x = torch ones out = model x device == lazy torch _lazy mark_step out cpu_out = f cpu lazy_out_ = f lazy reset_storage=False lazy_out_ = f lazy reset_storage=True assertEqual cpu_out lazy_out_ cpu assertEqual cpu_out lazy_out_ cpu test_data_assign text lazyt raw = torch _C _lazy _get_tensors_text lazyt NODE_TYPE_PATTERN sub raw origin = torch rand dtype=torch float tensor = origin lazy assertExpectedInline text tensor \ IR = Float lazy_tensors device_data device=CPU ROOT= Modify data-type tensor assign data This should update inner tensor FunctionalTensorWrapper changing corresponding IR node modified_tensor = tensor torch bfloat tensor data = modified_tensor assertExpectedInline text tensor \ IR = Float lazy_tensors device_data device=CPU = BFloat aten _to_copy dtype=BFloat layout=null device=null pin_memory=null non_blocking= memory_format=null ROOT= noqa B __name__ == __main__ run_tests