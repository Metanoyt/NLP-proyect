Owner s oncall quantization copy typing Any Optional torch torch _higher_order_ops out_dtype out_dtype noqa F torch ao quantization quantize_pt e convert_pt e prepare_pt e torch ao quantization quantizer Quantizer torch ao quantization quantizer xnnpack_quantizer get_symmetric_quantization_config XNNPACKQuantizer torch export export torch testing _internal common_quantization NodeSpec ns QuantizationTestCase skipIfNoQNNPACK TestHelperModules torch testing _internal common_utils raise_on_run_directly skipIfNoQNNPACK TestPT ERepresentation QuantizationTestCase _test_representation model torch nn Module example_inputs tuple Any quantizer Quantizer ref_node_occurrence dict ns int non_ref_node_occurrence dict ns int fixed_output_tol Optional float = None output_scale_idx int = - torch nn Module resetting dynamo cache torch _dynamo reset model = export model example_inputs strict=True module model_copy = copy deepcopy model model = prepare_pt e model quantizer Calibrate model example_inputs model = convert_pt e model use_reference_representation=True checkGraphModuleNodes model expected_node_occurrence=ref_node_occurrence make sure runs pt e_quant_output = model example_inputs TODO torchdynamo times out when we do we can enable numerical checking after fixed model_copy = prepare_pt e model_copy quantizer Calibrate model_copy example_inputs model_copy = convert_pt e model_copy use_reference_representation=False checkGraphModuleNodes model_copy expected_node_occurrence=non_ref_node_occurrence pt e_quant_output_copy = model_copy example_inputs output_tol = None fixed_output_tol None output_tol = fixed_output_tol idx = n model_copy graph nodes n target == torch ops quantized_decomposed quantize_per_tensor default idx += idx == output_scale_idx output_tol = n args assert output_tol None make sure result off one most quantized integer representation assertTrue torch max torch abs pt e_quant_output_copy - pt e_quant_output = output_tol + e- test_static_linear M torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x quantizer = XNNPACKQuantizer operator_config = get_symmetric_quantization_config is_per_channel=False quantizer set_global operator_config example_inputs = torch randn _test_representation M eval example_inputs quantizer ref_node_occurrence= non_ref_node_occurrence= test_dynamic_linear M torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x quantizer = XNNPACKQuantizer operator_config = get_symmetric_quantization_config is_per_channel=False is_dynamic=True quantizer set_global operator_config example_inputs = torch randn _test_representation M eval example_inputs quantizer ref_node_occurrence= non_ref_node_occurrence= fixed_output_tol= e- test_conv d M torch nn Module __init__ - None super __init__ conv d = torch nn Conv d forward x conv d x quantizer = XNNPACKQuantizer operator_config = get_symmetric_quantization_config is_per_channel=False quantizer set_global operator_config example_inputs = torch randn _test_representation M eval example_inputs quantizer ref_node_occurrence= non_ref_node_occurrence= test_add M torch nn Module __init__ - None super __init__ forward x y x + y quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config M eval example_inputs = torch randn torch randn _test_representation M eval example_inputs quantizer ref_node_occurrence= non_ref_node_occurrence= test_add_relu M torch nn Module __init__ - None super __init__ forward x y out = x + y out = torch nn functional relu out out quantizer = XNNPACKQuantizer operator_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global operator_config example_inputs = torch randn torch randn ref_node_occurrence = ns call_function out_dtype _test_representation M eval example_inputs quantizer ref_node_occurrence=ref_node_occurrence non_ref_node_occurrence= test_maxpool d quantizer = XNNPACKQuantizer operator_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global operator_config m_eager = TestHelperModules ConvMaxPool d eval example_inputs = torch randn _test_representation m_eager example_inputs quantizer ref_node_occurrence= non_ref_node_occurrence= test_qdq_per_channel Test representation quantize_per_channel dequantize_per_channel op M torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x quantizer = XNNPACKQuantizer use per channel quantization weight operator_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global operator_config M eval inputs = torch randn torch randn torch randn torch randn example_inputs inputs ref_node_occurrence = ns call_function torch ops quantized_decomposed quantize_per_channel default ns call_function torch ops quantized_decomposed dequantize_per_channel default non_ref_node_occurrence = quantize_per_channel folded ns call_function torch ops quantized_decomposed quantize_per_channel default ns call_function torch ops quantized_decomposed dequantize_per_channel default _test_representation M eval example_inputs quantizer ref_node_occurrence non_ref_node_occurrence output_scale_idx= test_qdq Test representation quantize dequantize op M torch nn Module __init__ - None super __init__ forward x y x + y quantizer = XNNPACKQuantizer quantization_config = get_symmetric_quantization_config is_per_channel=True quantizer set_global quantization_config M eval example_inputs = torch randn torch randn ref_node_occurrence = ns call_function torch ops quantized_decomposed quantize_per_tensor ns call_function torch ops quantized_decomposed dequantize_per_tensor non_ref_node_occurrence = ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default _test_representation M eval example_inputs quantizer ref_node_occurrence non_ref_node_occurrence __name__ == __main__ raise_on_run_directly test test_quantization py