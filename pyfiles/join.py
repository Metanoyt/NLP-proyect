mypy allow-untyped-defs warnings abc ABC abstractmethod types TracebackType typing Any NamedTuple Optional torch torch distributed dist __all__ = JoinHook Joinable Join JoinHook r This defines join hook which provides two entry points join context manager Entry points main hook which called repeatedly while there exists non-joined process post-hook which called once all processes have joined To implement join hook generic join context manager define inherits ` JoinHook ` override ` ` main_hook ` ` ` ` post_hook ` ` appropriate main_hook - None r Call hook while there exists non-joined process shadow collective communications training iteration Training iteration i e one forward pass backward pass optimizer step post_hook is_last_joiner bool - None r Call hook after all processes have joined It passed additional ` ` bool ` ` argument ` ` is_last_joiner ` ` which indicates rank one last join Arguments is_last_joiner bool ` ` True ` ` rank one last join ` ` False ` ` otherwise Joinable ABC r This defines abstract base joinable classes A joinable inheriting ` Joinable ` should implement meth ` join_hook ` which returns ` JoinHook ` instance addition meth ` join_device ` meth ` join_process_group ` device process group information respectively abstractmethod __init__ - None super __init__ _join_config = _JoinConfig construct_disabled_join_config abstractmethod join_hook kwargs - JoinHook r Return ` JoinHook ` instance given ` Joinable ` Arguments kwargs dict ` dict ` containing any keyword arguments modify behavior join hook run time all ` Joinable ` instances sharing same join context manager forwarded same value ` ` kwargs ` ` property abstractmethod join_device - torch device r Return device which perform collective communications needed join context manager property abstractmethod join_process_group - Any r Returns process group collective communications needed join context manager itself _JoinConfig NamedTuple r This includes all fields needed ` Joinable ` instance join context manager side enable bool throw_on_early_termination bool is_first_joinable bool staticmethod construct_disabled_join_config r Return ` _JoinConfig ` instance indicating join-related logic should disabled e g caller join context manager _JoinConfig enable=False throw_on_early_termination=False is_first_joinable=False Join r This defines generic join context manager which allows custom hooks called after process joins These hooks should shadow collective communications non-joined processes prevent hanging erroring ensure algorithmic correctness Refer ` JoinHook ` details about hook definition warning The context manager requires each participating ` Joinable ` call method meth ` notify_join_context ` before its own per- iteration collective communications ensure correctness warning The context manager requires all ` ` process_group ` ` attributes ` JoinHook ` objects same If there multiple ` JoinHook ` objects then ` ` device ` ` first used The process group device information used checking non- joined processes notifying processes throw exception ` ` throw_on_early_termination ` ` enabled both which using all- reduce Arguments joinables List Joinable list participating ` Joinable ` s their hooks iterated over given order enable bool flag enabling uneven input detection setting ` ` False ` ` disables context manager s functionality should only set when user knows inputs will uneven default ` ` True ` ` throw_on_early_termination bool flag controlling whether throw exception upon detecting uneven inputs default ` ` False ` ` Example os torch torch distributed dist torch multiprocessing mp xdoctest +SKIP torch nn parallel DistributedDataParallel DDP torch distributed optim ZeroRedundancyOptimizer ZeRO torch distributed algorithms join Join On each spawned worker worker rank dist init_process_group nccl rank=rank world_size= model = DDP torch nn Linear rank device_ids= rank optim = ZeRO model parameters torch optim Adam lr= Rank gets one more input than rank inputs = torch tensor rank _ range + rank Join model optim input inputs loss = model input sum loss backward optim step All ranks reach here without hanging erroring __init__ joinables list Joinable enable bool = True throw_on_early_termination bool = False kwargs len joinables == raise ValueError The join context manager requires least one joinable _joinables = joinables _join_hooks = joinable join_hook kwargs joinable _joinables _enable = enable _throw_on_early_termination = throw_on_early_termination _set_joinable_configs _extract_dist_info _set_joinable_configs - None r Set ` _JoinConfig ` each participating ` Joinable ` assert len _joinables is_first_joinable = True joinable _joinables joinable _join_config = _JoinConfig enable=self _enable throw_on_early_termination=self _throw_on_early_termination is_first_joinable=is_first_joinable is_first_joinable = False _extract_dist_info - None r Extract process group device information joinables If there multiple joinables then context manager uses first specified device Preconditions ` ` _joinables ` ` ` ` None ` ` non-empty Raises ValueError If there multiple conflicting ` ` process_group ` ` attributes among ` ` Joinable ` ` objects process_group = None device = None pyrefly ignore bad-assignment joinable _joinables process_group None process_group = joinable join_process_group process_group = joinable join_process_group raise ValueError Using join context manager multiple process groups device None device = joinable join_device _process_group = process_group _rank = dist get_rank _process_group _device = device __enter__ __exit__ type Optional type BaseException value Optional BaseException traceback Optional TracebackType r Repeatedly runs main hooks until all processes join then runs post-hooks Raises RuntimeError If ` ` throw_on_early_termination=True ` ` _enable type propagate exception directly one raised all_procs_joined = False is_last_joiner = True i = WARN_THRESHOLD = warnings simplefilter once while all_procs_joined i WARN_THRESHOLD warnings warn Detected uneven input skew greater than f WARN_THRESHOLD This means rank f _rank has least WARN_THRESHOLD f fewer inputs than other currently-active ranks This level skew could lead performance degradation during training stacklevel= Shadow all-reduce non-joined processes num_nonjoined_procs = _get_num_nonjoined_procs num_nonjoined_procs == all_procs_joined = True _throw_on_early_termination _notify_procs_to_terminate Run main hooks join_hook _join_hooks join_hook main_hook is_last_joiner = False i += Run post-hooks join_hook _join_hooks join_hook post_hook is_last_joiner _get_num_nonjoined_procs r Return number non-joined processes shadowing all-reduce non-joined processes num_nonjoined_procs = torch zeros device=self _device dist all_reduce num_nonjoined_procs group=self _process_group num_nonjoined_procs item _notify_procs_to_terminate r Schedule all-reduce notify non-joined processes terminate Also raise ` ` RuntimeError ` ` indicating current process has exhausted its inputs ones = torch ones device=self _device dist all_reduce ones group=self _process_group raise RuntimeError f Rank _rank exhausted all inputs staticmethod notify_join_context joinable Joinable r Notifies join context manager calling process has yet joined Then ` ` throw_on_early_termination=True ` ` checks uneven inputs have been detected i e one process has already joined throws exception so This method should called ` Joinable ` object before its per-iteration collective communications For example should called beginning forward pass ` DistributedDataParallel ` Only first ` Joinable ` object passed into context manager performs collective communications method others method vacuous Arguments joinable Joinable ` Joinable ` object calling method Returns An async work handle all-reduce meant notify context manager process has yet joined ` ` joinable ` ` first one passed into context manager ` ` None ` ` otherwise assert hasattr joinable _join_config f Check type joinable constructor calls ` ` Joinable ` ` constructor join_config = joinable _join_config First joinable responsible collective communications join_config is_first_joinable join_config enable None device = joinable join_device process_group = joinable join_process_group Schedule all-reduce indicate caller has yet joined ones = torch ones device=device work = dist all_reduce ones group=process_group async_op=True join_config throw_on_early_termination Check uneven inputs have been detected zeros = torch zeros device=device dist all_reduce zeros group=process_group should_throw = zeros item should_throw raise RuntimeError Detected least one rank exhausted inputs Throwing across all ranks work