CUDA graph trees safety abstraction over CUDAGraphs similar make_graph_callables which share same memory pool Sharing memory pool extremely important optimization when chaining multiple CUDA graphs together prevents you needing copy intermediate tensors one graph next reduces overall memory usage allowing dead memory first pool reused second The standard graph make_graph_callables support sharing memory pool lot caveats CUDA graph trees remove these restrictions Previously you recorded graphs A B you had replay A B order With CUDA graph trees after replaying A you can change your mind record replay different graph B we will support efficient execution both A B A B using only max mem A B mem A B In other words we support arbitrary trees CUDA graph operations just sequences why feature called CUDA graph trees Previously you executed graph A some non-CUDA graph code then graph B after executing graph B safe retain any references intermediates produced A With CUDA graph trees we track any outputs graph A still live time graph B run make sure graph B doesn t clobber there memory when reusing CUDA graphs pool You ll get separate recording B depending what tensors stay live dead CUDA graph trees flexible enough used Dynamo across graph breaks which their primary use case The ability switch replay record fairly nontrivial remember when you replay CUDA graph you only replay CUDA operations no CPU side state updated In particular CPU-side book-keeping allocator reconstructed However record new child CUDA graph we must restore book-keeping This what checkpoint pool state used __future__ annotations contextlib dataclasses functools gc itertools operator sys threading traceback warnings weakref collections defaultdict contextlib AbstractContextManager enum auto Enum typing Any Callable cast Optional TYPE_CHECKING TypeVar Union torch fx torch Tensor torch _dynamo callback CallbackTrigger torch _dynamo mutation_guard GenerationTracker torch _dynamo utils counters dynamo_timed preserve_rng_state torch _inductor compile_fx align_inputs_from_check_idxs copy_misaligned_inputs get_expanded_dims get_input_idxs_to_check index_expanded_dims remove_unaligned_input_idxs static_input torch _inductor cudagraph_utils check_for_mutation CheckInvariantStatus FunctionID log_cudagraph_skip_and_bump_counter log_data_ptr_mismatch maybe_warning_due_to_dynamic_shape ModelType OutputType PlaceholderInfo WrappedFunction torch multiprocessing reductions StorageWeakRef torch storage UntypedStorage torch utils _pytree pytree torch utils _ordered_set OrderedSet torch utils weak TensorWeakRef TYPE_CHECKING collections abc Generator Iterator Sequence torch _guards CompileId torch _inductor utils InputType torch cuda _POOL_HANDLE torch types _bool StorageWeakRefPointer = int StorageDataPtr = int NBytes = int S = TypeVar S bound= StorageWeakRefWrapper torch backends cuda is_built torch _C _cuda_CUDAAllocator_AllocatorState AllocatorState _set_cached_tensors_enabled AllocatorState type ignore no-redef pass _set_cached_tensors_enabled enabled _bool - None pass log = torch _logging getArtifactLogger __name__ cudagraphs config dataclasses dataclass frozen=True GraphID Unique counter cuda graph recording id int clear_cublass_cache - None Cublas keeps persistent workspace allocation running matmuls This poses problem doing warmup within CUDAGraph private pool because we do want persistent allocations one one run next When we begin new run cudagraphs path generation all tensors previous generation freed This frees them memory pool elsewhere A tensor cublas workspace would continue use workspace would also get allocated next run The memory would use two places To solve we clear cublas caches before after warming up recording If workspace required will allocated cudagraph private pool accounted allocator duration program There no overhead replay since cudagraphs removes allocation overhead torch _C _cuda_clearCublasWorkspaces contextlib contextmanager clear_cublas_manager - Generator None None None Context manager around clearing cublas caches will clear enter exit clear_cublass_cache try yield finally clear_cublass_cache contextlib contextmanager disable_conv_cache_emptying - Generator None None None prev = torch _C _cuda_get_conv_benchmark_empty_cache torch _C _cudnn_set_conv_benchmark_empty_cache False try yield finally torch _C _cudnn_set_conv_benchmark_empty_cache prev contextlib contextmanager enable_history_recording - Generator None None None Turns history recording CUDA Caching Allocator enabled = torch _C _cuda_isHistoryEnabled try enabled torch cuda memory _record_memory_history yield finally enabled torch cuda memory _record_memory_history None get_history_recording - AbstractContextManager None TODO - remove prevents cleanup config triton cudagraph_trees_history_recording contextlib nullcontext enable_history_recording TreeManagerContainer Manages lifetime tree manager Like ` PrivatePool ` cuda caching allocator tree its corresponding memory pool should kept alive long any outstanding graph tensor which output graph remains alive There single tree manager container per device The lifecycle tree_manager - Is constructed no graph no fns no tensors - Tree manager fetched resulting tree manager being allocated - We generate bunch functions calling add_strong_reference - These functions die calling finalize_reference - When all functions die we finalize_tree_manager TODO future we would like do following once storage weak refs land - We look all live storages add references THOSE - We count storages die - All storages dead we deallocate tree manager __init__ device_index int - None This keeps strong reference tree_manager upon all other strong references tree_manager will reset None We need strong reference so we can still access its attributes upon cleanup tree_manager Optional CUDAGraphTreeManager = None Number outstanding references current tree manager live_cudagraphify_fns = device_index = device_index Following two objects only set case Tensor outputs outlive cudagraphify_fns Reference Graph needed keep private pool deallocation live_storages_count = graph Optional torch cuda CUDAGraph = None lock = threading Lock _finalize_tensor - None lock live_storages_count -= live_storages_count == graph = None manager used again after existing cleanup we shouldn t set None live_cudagraphify_fns == tree_manager = None finalize_cudagraphify_fn - None lock live_cudagraphify_fns -= live_cudagraphify_fns == _finalize_tree_manager _finalize_tree_manager - None assert lock locked tree_manager = None TODO - when issue landed we can set weakref storages trigger deallocation when all outputs cudagraph dead live_storages = list tree_manager live_cudagraph_pool_storages_in_curr_execution Maintain reference graph keep tensors alive assert len tree_manager roots expected least one use root = next tree_manager get_roots graph = root graph seen_storages = set stor live_storages stor seen_storages continue seen_storages add stor live_storages_count += weakref finalize stor _finalize_tensor add_strong_reference fn Callable Any - None lock live_cudagraphify_fns += weakref finalize fn finalize_cudagraphify_fn get_tree_manager - CUDAGraphTreeManager lock tree_manager None tree_manager = CUDAGraphTreeManager device_index tree_manager local = threading local one tree manager per device local tree_manager_containers = local tree_manager_locks = defaultdict threading Lock only incremented user call mark_step_begin MarkStepBox mark_step_counter = We need register object will copied over TLS when new threads created autograd torch _C _stash_obj_in_tls tree_manager_containers local tree_manager_containers torch _C _stash_obj_in_tls tree_manager_locks local tree_manager_locks mark_step_begin - None Indicates new iteration inference training about begin iterate down distinguish GenerationTracking counter MarkStepBox mark_step_counter -= reset_cudagraph_trees - None Clear all cudagraph trees see shutdown below why necessary container_dict = get_obj local tree_manager_containers locks_dict = get_obj local tree_manager_locks device lock locks_dict items lock container = container_dict get device container container tree_manager continue container tree_manager shutdown _set_cached_tensors_enabled False container_dict clear MarkStepBox mark_step_counter = get_obj local Any attr_name str - Any hasattr local attr_name getattr local attr_name assert torch _C _is_key_in_tls attr_name torch _C _get_obj_in_tls attr_name get_container device_index int - TreeManagerContainer container_dict = get_obj local tree_manager_containers lock = get_obj local tree_manager_locks device_index lock device_index container_dict container_dict device_index = TreeManagerContainer device_index container_dict device_index get_manager device_index int create_if_none_exists bool = True - Optional CUDAGraphTreeManager create_if_none_exists get_container device_index get_tree_manager get_container device_index tree_manager is_cudagraph_capture_sizes int_key Union int tuple int - bool Returns true all dynamic shapes should captured dynamic shape int_key should captured config triton cudagraph_capture_sizes None int_key config triton cudagraph_capture_sizes cudagraphify_impl model ModelType inputs list InputType static_input_idxs Sequence int args Any kwargs Any - ModelType fn_cache dict tuple int Callable Any = Detect int inputs we need index these int_key = i i v enumerate inputs isinstance v int get_ints Any = operator itemgetter int_key int_key lambda _ None has_warn = False del inputs deferred_cudagraphify inputs list InputType - OutputType nonlocal has_warn int_key = get_ints inputs is_cudagraph_capture_sizes int_key model inputs fn = fn_cache get int_key fn None fn inputs int_key None log info recording cudagraph tree graph without symints log info recording cudagraph tree symint key s int_key has_warn has_warn = maybe_warning_due_to_dynamic_shape fn_cache int_key first get indices we need check align then update our static inputs finally copy check_input_idxs = get_input_idxs_to_check inputs static_input_idxs new_static_input_idxs = remove_unaligned_input_idxs inputs static_input_idxs copy_misaligned_inputs inputs check_input_idxs fn out = cudagraphify model inputs new_static_input_idxs args kwargs cudagraph will already clones input locally no need copy back mutated_input_idxs OrderedSet int = OrderedSet fn = align_inputs_from_check_idxs fn inputs_to_check=check_input_idxs mutated_input_idxs=mutated_input_idxs pyrefly ignore unsupported-operation fn_cache int_key = fn out deferred_cudagraphify contextlib contextmanager dynamo_timed_cudagraph name str compile_id Optional CompileId mode Optional CompilationMode - Generator Any None None Makes usages dynamo_timed file less verbose NOTE This CM sums all durations into single column dynamo_compile table Use only you consider timed region part runtime overhead associated compiler dynamo_timed name log_pt _compile_event=True compile_id=compile_id is_backward=mode == CompilationMode BACKWARD dynamo_compile_column_us= runtime_cudagraphify_time_us yield cudagraphify model ModelType inputs list InputType static_input_idxs Sequence int = device_index int is_backward bool is_inference bool stack_traces Optional StackTraces = None constants tuple torch Tensor = placeholders tuple PlaceholderInfo = mutated_input_idxs tuple int = compile_id Optional CompileId = None - tuple ModelType OutputType assert is_backward is_inference mode = CompilationMode BACKWARD is_backward CompilationMode INFERENCE is_inference CompilationMode FORWARD dynamo_timed_cudagraph cudagraphify get_container compile_id mode manager = get_container device_index get_tree_manager manager add_function model inputs static_input_idxs stack_traces mode constants placeholders mutated_input_idxs compile_id StorageWeakRefWrapper Wrapper around storage weak ref Will deallocate upon expiration invoked __slots__ = ref _data_ptr extra_ref_check storage_ref Optional StorageWeakRef __init__ inp Union Tensor UntypedStorage extra_ref_check Optional Callable bool = None - None extra_ref_check additional check we need run check weak ref has expired checking storage use count we assume extra_ref_check will hold additional reference storage isinstance inp Tensor stor = inp untyped_storage assert isinstance inp UntypedStorage stor = inp ref = StorageWeakRef stor _data_ptr = stor data_ptr extra_ref_check = extra_ref_check classmethod from_weakref_and_data_ptr cls type StorageWeakRefWrapper cdata Any data_ptr int extra_ref_check Optional Callable bool = None - StorageWeakRefWrapper instance = cls __new__ cls instance _data_ptr = data_ptr instance ref = StorageWeakRef from_weakref cdata instance extra_ref_check = extra_ref_check instance __call__ - Optional StorageWeakRefPointer expired None ref cdata swap_weakref cdata Any - None ref __del__ ref cdata = cdata data_ptr - int NB returns data ptr even storage has expired _data_ptr remove_extra_reference - None extra_ref_check = None expired - bool extra_ref_check None extra_ref_check False extra_ref_check None we expect additional reference stor_count = torch _C _storage_Use_Count ref cdata stor_count - extra_ref_check None == __repr__ - str ref None ref expired f StorageWeakRefWrapper data_ptr dead f StorageWeakRefWrapper data_ptr alive is_live weak_ref Optional StorageWeakRefWrapper - bool maybe_deref weak_ref None maybe_deref weak_ref Optional StorageWeakRefWrapper - Optional tuple StorageWeakRefPointer int weak_ref None None r = weak_ref r None None NB r data_ptr does necessarily equal weak_ref data_ptr r weak_ref data_ptr contextlib contextmanager _use_cuda_memory_pool_manager device int mem_pool tuple int int stream torch cuda Stream - Generator None None None Context manager use cuda graph pool new allocations If you use manager all cudagraph tensors use should reflected allocator they will overwritten existing_graph should already have been used capture mem_pool must already exist because manager will preserve reference pool which keeps alive torch cuda synchronize stream wait_stream torch cuda current_stream torch cuda stream stream torch device device Begin allocate mem pool all memory allocation current thread This thread safe since thread can only warmup record cudagraph same time torch _C _cuda_beginAllocateCurrentThreadToPool device mem_pool try yield finally torch _C _cuda_endAllocateToPool device mem_pool torch _C _cuda_releasePool device mem_pool torch cuda current_stream wait_stream stream map_to_ref t Optional Tensor - Optional StorageWeakRefWrapper isinstance t torch Tensor assert t None None StorageWeakRefWrapper t A path index depth offset indices into graph ` depth ` ` number nodes root graph output offset PathOutputIndex = tuple int int For each node path each output output alive PathLiveness = list list bool StackTraces = list Optional str CUDAWarmupNode Simplified Wrapper around A CUDA Model wraps outputs storage refs exposes apis get live storages current chain warmup A CUDAWarmupNode may have either CUDAGraphNode CUDAWarmupNode parent may only have CUDAWarmupNode children because we cannot record execute tensors which do have stable memory addresses CUDAWarmupNode CUDAGraphNode have number differences make easier use separate classes - Much CUDAGraphNode logic initialization based tensor properties first recording In first instance warmup these finalized yet - All Inputs RecordedFunction must copied over cuda graph memory pool unnecessary warmup - CUDAWarmup only used once so does need optimize much bookkeeping It much simpler NB CUDAGraphNode need expose ` path_live_weakrefs ` ` all_outputs_are_dead ` ` outputs_weakrefs ` ` stack_traces ` ` tensor_weakrefs ` compatibility __init__ wrapped_function WrappedFunction parent Optional Union CUDAGraphNode CUDAWarmupNode cuda_graphs_pool tuple int int existing_cuda_graph Optional torch cuda CUDAGraph device_index int stack_traces Optional StackTraces stream torch cuda Stream already_warm bool id GraphID - None wrapped_function = wrapped_function parent Optional Union CUDAGraphNode CUDAWarmupNode = parent cuda_graphs_pool = cuda_graphs_pool outputs_weakrefs list Optional StorageWeakRefWrapper = tensor_weakrefs list Optional TensorWeakRef = existing_cuda_graph = existing_cuda_graph has_run = False device_index = device_index stack_traces = stack_traces stream = stream already_warm = already_warm id = id run new_inputs Any - OutputType assert has_run Wrapped function should never run twice See output_is_alias_of_persistent_static_inputs below We should only returning freshly created storages path_live_weakrefs existing_path_data_ptrs = OrderedSet t data_ptr t path_live_weakrefs t get_non_cudagraph_inps - list weakref ReferenceType UntypedStorage non_cudagraph_inps = weakref ref t untyped_storage t itertools chain new_inputs wrapped_function constants isinstance t torch Tensor t untyped_storage data_ptr existing_path_data_ptrs non_cudagraph_inps non_cudagraph_inps_storages = get_non_cudagraph_inps config triton slow_path_cudagraph_asserts already_warm refs = list path_live_weakrefs check_memory_pool device_index cuda_graphs_pool refs torch cuda device device_index disable_conv_cache_emptying clear_cublas_manager _use_cuda_memory_pool_manager device_index cuda_graphs_pool stream get_history_recording out = wrapped_function model new_inputs We need know which outputs allocated within cudagraph pool so we can deallocate them beginning next cudagraph step set their access error We use weakref inputs storage case block which previously allocated general caching allocator pool gets reallocated private pool non_cudagraph_inps_storage_ptrs = OrderedSet Any storage non_cudagraph_inps_storages s = storage s None non_cudagraph_inps_storage_ptrs add s _cdata assert len new_inputs == sdpa returns cpu tensors when recording cuda graph add_ref o Any - bool isinstance o torch Tensor o is_cuda o untyped_storage _cdata non_cudagraph_inps_storage_ptrs o untyped_storage data_ptr = outputs_weakrefs extend map_to_ref o add_ref o None o out tensor_weakrefs extend TensorWeakRef o add_ref o None o out config triton slow_path_cudagraph_asserts already_warm out_refs = list path_live_weakrefs check_memory_pool device_index cuda_graphs_pool out_refs out property _path_from_root - Generator Union CUDAGraphNode CUDAWarmupNode None None nodes = node Union CUDAGraphNode CUDAWarmupNode = while node nodes append node node = node parent type ignore assignment yield reversed nodes path_live_weakrefs - Iterator StorageWeakRefWrapper Returns all live storages weakrefs created nodes path node _path_from_root output node outputs_weakrefs is_live output yield output type ignore misc all_outputs_are_dead - bool list path_live_weakrefs _is_cuda_graph_recorded_tensor t torch Tensor - bool storage_weak_ref path_live_weakrefs t untyped_storage data_ptr == storage_weak_ref data_ptr True False Aliases List say what indices denote InputList = list input indexes OutputList = list output indexes LevelList = list levels distance root tree OutputAliasInfo pass _UnaliasedStorage OutputAliasInfo Singleton mark graph output constructs new alias None UnaliasedStorage = _UnaliasedStorage AliasesPriorGraphOutput OutputAliasInfo Marks graph output aliases output prior graph __slots__ = index index PathOutputIndex __init__ index PathOutputIndex - None assert isinstance index tuple index = index AliasesNewOutput OutputAliasInfo Marks graph output aliases index new returned outputs __slots__ = index index int __init__ index int - None assert isinstance index int index = index CUDAGraphNode A single recording function into CUDA Graph Recordings CUDA Graphs share single memory pool structured into tree where there single recording can precede parent multiple subsequent recordings may follow children A node will have no parent first recording tree i e when first recorded there no live tensors previous recording which would force dependency On first recording all live tensors current CUDA Graph Node path will reflected corresponding private pool On subsequent executions caching allocator unaffected when graph replayed In order support recording subsequent cuda graph recording after execution graph we checkpoint state memory pool so may later resumed WrappedFunction should have already been warmed up prior invocation See setCheckpointPoolState further explanation well https user-images githubusercontent com - f -f d- f d- fa - b bb png __init__ wrapped_function WrappedFunction id GraphID parent Optional CUDAGraphNode inputs list InputType cuda_graphs_pool _POOL_HANDLE device_index int stack_traces Optional StackTraces stream torch cuda Stream mode Optional CompilationMode compile_id Optional CompileId - None assert isinstance inputs list tuple wrapped_function = wrapped_function id = id device = device_index stack_traces = stack_traces stream = stream Enable re-record cudagraph when static tensor address changed we should error when changed rerecord_if_static_inputs_change = torch _dynamo config inline_inbuilt_nn_modules torch _inductor config triton cudagraph_support_input_mutation root parent will None use weakref prevent reference cycle _parent = weakref ref parent parent None None reference shared memory pool entire cuda graphs tree cuda_graphs_pool = cuda_graphs_pool A single wrapped function may recorded multiple times memory patterns invariants change one execution next children dict FunctionID list CUDAGraphNode = defaultdict list StorageWeakRef maintains whether Storage C++ object remains allocated whether corresponding memory has been deallocated In order use them track memory deallocations we must maintain single StorageWeakRef all Storages reference memory even we constructing Storages do have deallocator function We maintain one single storage_cache we execute any tree path When we retrieve storage cache we check still alive we hash based observed recording data ptr storage cdata we preserve single reference executed outputs then referenced children avoid children having chase parent pointers hot path DO NOT reassign output_weakrefs only call ` clear ` Path series nodes root current node outputs_weakrefs OutputList Optional StorageWeakRefWrapper = path_weakrefs LevelList OutputList Optional StorageWeakRefWrapper = node outputs_weakrefs node _path_from_root path_stacktraces LevelList Optional StackTraces = node stack_traces node _path_from_root tensor_weakrefs OutputList Optional TensorWeakRef = tensors which outputs previous graphs tree cudagraph_managed_idxs list int = idx idx t enumerate inputs isinstance t torch Tensor _is_cuda_graph_recorded_tensor t depth offset live tensors which alias previous graph outputs live_cudagraph_managed_path_refs InputList Optional PathOutputIndex = _is_alias_of_live_recorded_tensor t isinstance t torch Tensor None t inputs when replay preserve liveness input AliasesPriorGraphOutput also aliases output current CUDAGraphNode preserved_aliased_inputs InputList bool = False len inputs static_input_idxs list int = list OrderedSet wrapped_function static_input_idxs &#124; OrderedSet cudagraph_managed_idxs non_static_input_idx LevelList int = i i range len inputs i static_input_idxs counters inductor cudagraph_recorded_non_static_inputs += len non_static_input_idx non_managed_static_input_idxs LevelList int = i i wrapped_function static_input_idxs i cudagraph_managed_idxs maybe_get_static_data_ptr idx int inputs list InputType static_input_idxs list int - Optional int inp = inputs idx isinstance inp torch Tensor idx static_input_idxs inp data_ptr None static_input_data_ptrs InputList Optional int = pyrefly ignore bad-argument-type maybe_get_static_data_ptr i inputs static_input_idxs i range len inputs When we checkpoint free generations we will manually freeing outputs CUDAGraphNodes We should freeing parameters do we need account their liveness they static so we need compute which outputs aliases parameters Some static inputs saved tensors forward die backward Their locations static lifetimes We only include persistent static data ptrs below because non persistent data ptrs may outputs record fresh allocations precompute expanded dims avoid computing hot path expanded_dims list list int = get_expanded_dims x isinstance x torch Tensor idx static_input_idxs idx x enumerate inputs For each node path which outputs observed live before invoking graph recording after graph recording recorded_liveness_before_graph LevelList OutputList bool = recorded_liveness_after_graph LevelList OutputList bool = List tuples depth output_index index into node depth number nodes root output_index outputs Will index into path_weakrefs expected_dead_indices_before_graph list PathOutputIndex = expected_dead_indices_after_graph list PathOutputIndex = all live indices after graph recording live_indices_after_graph list PathOutputIndex = parent None previous_liveness = parent recorded_liveness_after_graph curr_liveness = _get_liveness path_weakrefs different_indices = _get_different_indices previous_liveness curr_liveness recorded_liveness_before_graph = curr_liveness expected_dead_indices_before_graph = different_indices rng_states = inp inp inputs isinstance inp torch Generator pyrefly ignore bad-argument-type recording_inputs = _allocate_and_copy_recording_inputs inputs recording inputs will copy over memory so we can free non recording inputs pyrefly ignore missing-attribute inputs clear del inputs graph used recording model invocation graph Optional torch cuda CUDAGraph = torch cuda CUDAGraph TODO register_generator_state should potentially take explicit device torch cuda device device rng_state rng_states graph register_generator_state rng_state we allocate non-static inputs within same memory pool CUDAGraph which we will record model For memory efficiency important reclaim input memory when inputs no longer live To accomplish we reconstruct tensors correct data pointers our inputs which non owning do prevent deallocation On subsequent executions input values will copied over these tensors reconstructed_inputs list InputType = _reconstruct_from_tensor_metadata _tensor_metadata x isinstance x torch Tensor x x recording_inputs DO THE RECORDING We record CUDA graph constructor CUDAGraphNode which gives you what CPU side compute function would do We don t throw recording outputs away their memory correctly accounted CUDAGraphs caching allocator This means very FIRST run CUDA graph node we can directly do more recording because we have valid caching allocator state NB This relies run being called immediately after constructor otherwise optimization would valid initialized below _record checkpointed_caching_state Optional AllocatorState = None Output Storage Alias information can - A new unaliased storage output None - An alias output prior graph - An alias output already created reconstructed outputs This None output question int output_storage_alias OutputList Optional OutputAliasInfo = output Storage unaliased subsequent outputs all subsequent paths we cached output tensor adjust storage liveness tracking also check output tensor does have additional python reference If descendent node discovers has alias prior output then output will no longer cached ancestor The large majority tensors unaliased preserving aliased output tensors would add significant additional complexity marginal gains The cached tensor outputs added first execution cleared whenever we need do subsequent recording unaliased_in_all_paths OutputList bool = cached_tensor_outputs OutputList Optional Tensor = output aliases static persistent input then corresponding Tensor will set here These different than cached tensors because they tensors aliases parameters always live static_output_tensors OutputList Optional Tensor = Cleared after recording dynamo_timed_cudagraph CUDAGraphNode record compile_id mode recording_outputs Optional OutputType = _record wrapped_function model recording_inputs outputs_metadata OutputList Union dict str Any int None = As inputs we do want keep outputs permanently alive because would prevent their memory being reclaimed subsequent cuda graph recordings We record tensor metadata needed reconstruct instead assert recording_outputs None out recording_outputs isinstance out torch Tensor outputs_metadata append _tensor_metadata out ignore_storage_offset=False assert isinstance out int type None type out outputs_metadata append out graph replay _copy_inputs_and_remove_from_src dsts list InputType srcs list InputType - None dst_tensors = src_tensors = idx non_static_input_idx isinstance srcs idx torch Tensor continue expanded_dims = expanded_dims idx dst_tensors append index_expanded_dims dsts idx expanded_dims type ignore arg-type src_tensors append index_expanded_dims srcs idx expanded_dims type ignore arg-type srcs idx = None type ignore call-overload Fails empty lists dst_tensors torch _foreach_copy_ dst_tensors src_tensors check_static_inputs_are_stable new_inputs list InputType - None avoid checking managed tensor static points since we already checked those check_invariants rerecord_if_static_inputs_change torch _C _tensors_data_ptrs_at_indices_equal new_inputs type ignore arg-type static_input_data_ptrs non_managed_static_input_idxs should error error_msg = log_data_ptr_mismatch wrapped_function placeholders new_inputs static_input_data_ptrs non_managed_static_input_idxs CheckInvariantStatus StaticInputIdxMismatch torch _check False lambda error_msg run_first_inputs new_inputs list InputType - OutputType config triton fast_path_cudagraph_asserts debug_check_invariants_before_invocation graph already invoked __init__ inputs copied over _allocate_recording_inputs subsequently cleared assert len new_inputs == outputs = recording_outputs recording_outputs = None assert outputs None outputs run new_inputs list InputType - OutputType check_static_inputs_are_stable new_inputs _copy_inputs_and_remove_from_src reconstructed_inputs new_inputs run_graph outputs = reconstruct_outputs new_inputs clear config triton fast_path_cudagraph_asserts debug_check_invariants_after_invocation config triton force_cudagraph_sync torch cuda synchronize Reset run check future static_inputs_stable = False outputs reconstruct_outputs - OutputType Reconstruct output tensors according their saved metadata alias information Cached tensors will yet set first execution They also cleared checkpointing so we checkpoint node then execute again we will need repopulate cached tensors cached_tensor_outputs _initialize_cached_tensors outputs OutputType = i storage_info metadata enumerate zip output_storage_alias outputs_metadata isinstance metadata dict tensor metadata assert isinstance metadata int type None outputs append metadata continue cached_t = cached_tensor_outputs i cached_t None output represents fresh allocated tensor We same TensorImpl run run avoid overhead autograd Function will reset Autograd meta output tensors part aot_autograd _backward_hooks stored tensors separately so we need manually reset hooks cached_t _backward_hooks None cached_t _backward_hooks = None No need update weakrefs already correctly initialized outputs append cached_t continue static_t = static_output_tensors i static_t None assert outputs_weakrefs i None outputs append static_t continue storage = prepare_alias_info_for_tensor_construction storage_info metadata isinstance storage UntypedStorage storage None out = _reconstruct_from_tensor_metadata metadata storage assert isinstance storage int out = _reconstruct_from_tensor_metadata metadata cast torch Tensor outputs storage untyped_storage outputs append out w = outputs_weakrefs i assert w None w swap_weakref out untyped_storage _weak_ref outputs prepare_alias_info_for_tensor_construction out_alias_info Optional OutputAliasInfo metadata Union dict str Any int None - Union UntypedStorage None int isinstance metadata int type None out_alias_info UnaliasedStorage None isinstance out_alias_info AliasesPriorGraphOutput depth existing_output_index = out_alias_info index ref = path_weakrefs depth existing_output_index assert ref None torch UntypedStorage _new_with_weak_ptr ref assert isinstance out_alias_info AliasesNewOutput out_alias_info index prepare_storages_for_construction - list Union UntypedStorage None int output_storages = output_storage_alias metadata zip output_storage_alias outputs_metadata output_storages append prepare_alias_info_for_tensor_construction output_storage_alias metadata output_storages run_graph - None assert graph None graph replay all_outputs_are_dead - bool All outputs path node its root dead depth output_index live_indices_after_graph is_live path_weakrefs depth output_index False True _record model ModelType inputs list InputType - OutputType Record model assert graph None static_input_iter - Generator torch Tensor None None i wrapped_function static_input_idxs _inp = inputs i isinstance _inp torch Tensor _is_cuda_graph_recorded_tensor _inp yield _inp see output_is_alias_of_persistent_static_inputs above static_input_persistent_storage_ptrs dict int StorageWeakRefWrapper = inp untyped_storage data_ptr StorageWeakRefWrapper inp inp itertools chain static_input_iter wrapped_function constants config triton slow_path_cudagraph_asserts need use parent live weakrefs because live_indices isn t set yet memory = parent None list parent path_live_weakrefs memory += StorageWeakRefWrapper elem i elem enumerate inputs isinstance elem torch Tensor i wrapped_function static_input_idxs elem untyped_storage data_ptr = check_memory_pool device cuda_graphs_pool memory preserve_rng_state torch cuda device device clear_cublas_manager torch cuda graph graph stream=self stream pool=self cuda_graphs_pool capture_error_mode= thread_local get_history_recording static_outputs = model inputs running model should reclaim memory assert len inputs == isinstance static_outputs list tuple static_outputs = static_outputs pyrefly ignore bad-argument-type _add_first_outputs static_outputs static_input_persistent_storage_ptrs pyrefly ignore bad-return static_outputs _add_first_outputs outputs OutputType static_input_persistent_storage_ptrs dict int StorageWeakRefWrapper - None Add outputs first invocation node set up metadata getting liveness before we have added outputs path so length two lists equal prev_liveness = recorded_liveness_before_graph curr_liveness = _get_liveness path_weakrefs delta = _get_different_indices prev_liveness curr_liveness expected_dead_indices_after_graph = delta assert len outputs_weakrefs == index data pointer index outputs output_new_storages_index dict StorageDataPtr int = unaliased_in_all_paths = False _ range len outputs static_output_tensors = None _ range len outputs i o enumerate outputs o None isinstance o torch Tensor output_storage_alias append UnaliasedStorage continue torch _check o is_cuda o untyped_storage data_ptr == lambda Expected all cuda outputs cuda graph recording Non cuda output f stack_traces i stack_traces unknown ref = static_input_persistent_storage_ptrs get o untyped_storage data_ptr None also treat empty storages static outputs because we do need manage their lifetime they should participate checkpointing is_empty_storage = o untyped_storage data_ptr == ref ref None is_empty_storage output_storage_alias append None static_output_tensors i = o continue path_ref = _is_alias_of_live_recorded_tensor o path_ref None _mark_prior_graph_output_as_aliased path_ref idx inp_path_ref enumerate live_cudagraph_managed_path_refs path_ref == inp_path_ref preserved_aliased_inputs idx = True output_storage_alias append AliasesPriorGraphOutput path_ref continue o untyped_storage data_ptr output_new_storages_index index = output_new_storages_index o untyped_storage data_ptr unaliased_in_all_paths index = False output_storage_alias append AliasesNewOutput index continue output_new_storages_index o untyped_storage data_ptr = i output_storage_alias append UnaliasedStorage unaliased_in_all_paths i = True stack_traces None stack_traces = None _ range len outputs assert len stack_traces == len outputs Wrong number stack traces passed assert outputs_weakrefs out static_output_tensor zip outputs static_output_tensors isinstance out torch Tensor static_output_tensor None outputs_weakrefs append None tensor_weakrefs append None outputs_weakrefs append StorageWeakRefWrapper out tensor_weakrefs append TensorWeakRef out recorded_liveness_after_graph = _get_liveness path_weakrefs checkpointed_caching_state = torch _C _cuda_getCheckpointState device cuda_graphs_pool now get liveness outputs added depth range len path_weakrefs output_index range len path_weakrefs depth is_live path_weakrefs depth output_index live_indices_after_graph append depth output_index debug_check_invariants_after_invocation config triton slow_path_cudagraph_asserts check_memory_pool device cuda_graphs_pool list path_live_weakrefs _mark_prior_graph_output_as_aliased index PathOutputIndex - None Remove graph output unaliased cached tensors ancestor node depth output_index = index node = list _path_from_root depth node unaliased_in_all_paths output_index = False x = path_weakrefs depth output_index assert x None x remove_extra_reference _initialize_cached_tensors - None we should clearing output_weakrefs they should set first record run assert len outputs_weakrefs == len outputs_metadata i storage_info metadata make_cached enumerate zip output_storage_alias outputs_metadata unaliased_in_all_paths make_cached cached_tensor_outputs append None continue assert storage_info UnaliasedStorage assert isinstance metadata dict s = create_storage metadata out = _reconstruct_from_tensor_metadata metadata storage=s type ignore arg-type XXX let autograd know there will additional reference tensor can ignored when deciding whether do gradient buffer inplacing Otherwise inplacing could differ between tracing subsequent execution For some models we tested led inputs no longer being cudagraph pools leading spurious re-recordings It also tells AMP cache even though tensor impls cannot cached dtype conversions torch _C _add_cached_tensor out self_ref = weakref ref one reference our array calling sys getrefcount bumps refcount one check_refcount i int - bool self_loc = self_ref self_loc None False self_loc get_output_refcount i == check = functools partial check_refcount i=i outputs_weakrefs i = StorageWeakRefWrapper out extra_ref_check=check cached_tensor_outputs append out get_output_refcount index int - int sys getrefcount cached_tensor_outputs index property parent - Optional CUDAGraphNode unwraps weakref _parent _parent _parent None None property _path_to_root - Generator CUDAGraphNode None None Returns all nodes path starting ending root node = while node yield node node = node parent type ignore assignment property _path_from_root - Generator CUDAGraphNode None None Returns all nodes path starting root ending nodes = reversed list _path_to_root yield nodes _is_cuda_graph_recorded_tensor t torch Tensor - bool Is tensor output node path output_refs path_weakrefs storage_weak_ref output_refs storage_weak_ref None continue don t need check liveness storage since cuda graph managed memory never released data_ptr = storage_weak_ref data_ptr t untyped_storage data_ptr == data_ptr True False _is_alias_of_live_recorded_tensor t torch Tensor - Optional PathOutputIndex depth output_refs enumerate path_weakrefs output_index storage_ref enumerate output_refs storage_and_ptr = maybe_deref storage_ref None _storage ptr = storage_and_ptr ptr == t untyped_storage data_ptr depth output_index None staticmethod _check_liveness indices list PathOutputIndex output_refs list list Optional StorageWeakRefWrapper - bool Check all indices specified dead references depth output_index indices w = output_refs depth output_index assert w None w None False True add_child function_id FunctionID node CUDAGraphNode - None Adds node child children function_id append node staticmethod _get_different_indices prev list list bool curr list list bool - list PathOutputIndex Find indices where two lists differ dead_indices = assert len prev = len curr i outputs outputs enumerate zip prev curr assert len outputs == len outputs j output output enumerate zip outputs outputs output = output dead_indices append i j dead_indices staticmethod _get_liveness weakrefs list list Optional StorageWeakRefWrapper - list list bool Maps weakrefs true reference alive false otherwise len weakrefs == pytree tree_map is_live outputs outputs weakrefs debug_assert_invariants expected_liveness list list bool newly_dead list PathOutputIndex - None config triton fast_path_cudagraph_asserts i node enumerate _path_from_root assert path_weakrefs i node outputs_weakrefs nodes = list _path_from_root live_blocks = get_block_addrs cuda_graphs_pool live_storage_data_ptrs = OrderedSet Any live_storage_weak_ptrs = OrderedSet Any depth outputs_liveness enumerate expected_liveness output_idx output_liveness enumerate outputs_liveness tensor can die early can t alive when should dead w = path_weakrefs depth output_idx stor_weak_ptr_and_data_ptr = maybe_deref w None assert output_liveness stor_weak_ptr stor_data_ptr = stor_weak_ptr_and_data_ptr assert stor_data_ptr live_storage_data_ptrs == stor_weak_ptr live_storage_weak_ptrs live_storage_data_ptrs add stor_data_ptr live_storage_weak_ptrs add stor_weak_ptr is_persistent_alias = nodes depth static_output_tensors output_idx None is_persistent_alias assert stor_data_ptr live_blocks depth output_index newly_dead assert is_live path_weakrefs depth output_index debug_check_invariants_before_invocation - None debug_assert_invariants recorded_liveness_before_graph expected_dead_indices_before_graph debug_check_invariants_after_invocation - None debug_assert_invariants recorded_liveness_before_graph expected_dead_indices_after_graph data_ptrs_dead_since_invocation - list int Since node invoked data ptrs all tensor outputs have died current executing tree path curr_liveness = _get_liveness path_weakrefs _get_different_indices = _get_different_indices recorded_liveness_after_graph curr_liveness path = list _path_from_root ptrs_to_deallocate = depth output_index _get_different_indices ptrs_to_deallocate append path depth outputs_metadata output_index data_ptr type ignore index ptrs_to_deallocate path_live_weakrefs - Iterator StorageWeakRefWrapper i j live_indices_after_graph out = path_weakrefs i j out None is_live out yield out remove_node_cached_tensors - None t cached_tensor_outputs t None torch _C _remove_cached_tensor t cached_tensor_outputs clear i unaliased enumerate unaliased_in_all_paths unaliased n = outputs_weakrefs i assert n None n remove_extra_reference remove_path_cached_tensors - None node _path_from_root node remove_node_cached_tensors clear_path_state - None Clear path state current executing node doesn t actually do anything right now leaving placeholder staticmethod _tensor_metadata x torch Tensor ignore_storage_offset bool = True - dict str Any assert isinstance x torch Tensor We ignore storage offset inputs outputs TODO - should we make storage resizable nbytes x untyped_storage nbytes data_ptr x untyped_storage data_ptr size x shape stride x stride dtype x dtype device x device storage_offset x storage_offset ignore_storage_offset _reconstruct_from_tensor_metadata metadata dict str Any storage Optional UntypedStorage = None - Tensor s = create_storage metadata storage None storage torch _C _construct_CUDA_Tensor_From_Storage_And_Metadata metadata s type ignore arg-type create_storage metadata dict str Any - torch types Storage torch _C _construct_storage_from_data_pointer metadata data_ptr metadata device metadata nbytes _allocate_and_copy_recording_inputs inputs list InputType - list InputType Allocate inputs non static non cudagraph managed tensors memory pool copy over tensor values torch cuda synchronize stream wait_stream torch cuda current_stream recording_inputs list InputType = warnings catch_warnings record=True torch cuda device device _use_cuda_memory_pool_manager device mem_pool=self cuda_graphs_pool stream=self stream i inp enumerate inputs isinstance inp torch Tensor assert isinstance inp int torch Generator pyrefly ignore bad-argument-type recording_inputs append inp i static_input_idxs static_input does allocation recording_inputs append static_input inp recording_inputs append inp _copy_inputs_and_remove_from_src recording_inputs inputs recording_inputs check_invariants inputs list InputType - tuple CheckInvariantStatus Callable str Checks node can run The same pattern tensor liveness static inputs tensors managed cudagraph private pool must remain stable _logger = functools partial log_data_ptr_mismatch wrapped_function placeholders inputs static_input_data_ptrs previously managed data pointers remain stable hot path so moved C++ equivalent all t data_ptr == data_ptr t data_ptr zip tensors data_ptrs torch _C _tensors_data_ptrs_at_indices_equal inputs type ignore arg-type static_input_data_ptrs cudagraph_managed_idxs status = CheckInvariantStatus CudagraphManagedIdxMismatch _logger = functools partial _logger cudagraph_managed_idxs status status _logger _check_liveness expected_dead_indices_before_graph path_weakrefs status = CheckInvariantStatus ExpectedDeadIndicesBeforeGraphMismatch status lambda f status static input data pointers should remain stable we inlining builtin nn modules we re-record case we inlining builtin nn modules we check check_static_inputs_are_stable error they stable rerecord_if_static_inputs_change torch _C _tensors_data_ptrs_at_indices_equal inputs type ignore arg-type static_input_data_ptrs static_input_idxs status = CheckInvariantStatus StaticInputIdxMismatch _logger = functools partial _logger static_input_idxs status status _logger cudagraph managed tensors which died upon recording must also die upon invocation too late check after we ve replayed graph because we would have already written over their memory idx cudagraph_managed_idxs preserved_aliased_inputs idx inputs idx = None type ignore call-overload torch _check _check_liveness expected_dead_indices_after_graph path_weakrefs lambda TODO graph recording observed input tensor deallocate during graph recording did occur during replay Please file issue CheckInvariantStatus SUCCESS lambda f CheckInvariantStatus SUCCESS num_descendants - int Total number descendents node num_desc = children children values child children num_desc += num_desc += child num_descendants num_desc get_cudagraph_segments pool_id tuple int int - Any segments = torch cuda memory_snapshot segment segment segments segment segment_pool_id == pool_id get_block_addrs pool_id tuple int int live_only bool = True - list int blocks = segment get_cudagraph_segments pool_id addr = segment address block segment blocks block state == active_allocated live_only blocks append addr addr += block size blocks format_tb frames list Any - str formatted_traceback = traceback FrameSummary entry filename entry line entry name entry frames join traceback format_list formatted_traceback check_memory_pool device int pool_id tuple int int live_storages_ptrs list StorageWeakRefWrapper - None assert all isinstance elem StorageWeakRefWrapper elem live_storages_ptrs noqa C unique_storages = stor data_ptr stor live_storages_ptrs stor noqa set_linter check there divergence first then do expensive snapshot call after we know will error torch _C _cuda_checkPoolLiveAllocations device pool_id unique_storages point we past fast-path we have seen rare cases where dead tensor dead hasn t been gc d yet gives false positive allocated_not_in_live_storages gc collect torch cuda synchronize segments = get_cudagraph_segments pool_id allocated_not_in_live_storages = segment segments addr = segment address block segment blocks block state == active_allocated addr unique_storages allocated_not_in_live_storages addr = block unique_storages remove addr addr += block size torch _check len unique_storages == lambda f These storage data ptrs allocated pool pool_id should unique_storages len allocated_not_in_live_storages = formatted = dp block allocated_not_in_live_storages items trace = format_tb block get frames pyrefly ignore bad-argument-type formatted append f Data Pointer dp history \n trace formatted_s = \n join formatted msg = f These live storage data ptrs cudagraph pool f accounted output cudagraph trees \n\n formatted_s raise RuntimeError msg ExecutionState Enum Represents state CUDAGraph Tree Will None there no live current memory allocated cuda graph pool Otherwise will reflect state most recently executed node NONE = auto WARMUP = auto RECORDING = auto EXECUTION = auto CompilationMode Enum FORWARD = auto BACKWARD = auto INFERENCE = auto CUDAGraphTreeManager Groups individual recordings executions cuda graphs into tree recordings checks required invariants manages warmups graphs When graphs recorded same tree enforces subsequent execution follow same order have same output tensor livespans To remove unnecessary coupling cuda graphs additional imposed invariants tree manager will end currently recording tree whenever valid - when memory pool no longer has any live allocations We ignore outputs previous generation correspond prior model outputs Currently hardcoded ` GenerationTracker generation ` tracked torch dynamo TODO make generation increment configurable warn overwrite We run graph warmups cudagraph memory pool result first invocation function For many models important reclaim activations you run backward If we warm up model keep extra copy inputs around subsequently use recording we would incur memory penalty Additionally we part way through training your model need recompile memory will allocated cuda graph pool so we run warmup run cuda graph memory pool As recording warm up needs state live tensors accurately reflected so we checkpoint allocator state we need warm up following graph replay __init__ device_index int - None roots functions which have no dependencies other node I e when they first invoked none their inputs outputs outputs another node nor there any live outputs another node whose liveness would create dependency roots dict FunctionID list CUDAGraphNode = defaultdict list mapping function id wrapped function ids_to_funcs dict FunctionID WrappedFunction = ids_to_stack_traces dict FunctionID Optional StackTraces = warmed_up_functions OrderedSet FunctionID = OrderedSet we fail increment generation stuck warming up only warn each function once warned_functions OrderedSet FunctionID = OrderedSet torch _C _set_cached_tensors_enabled True warn only once function mutates inputs warned_mutation OrderedSet FunctionID = OrderedSet NB cuda caching allocator will remember stream segment allocated only allocate segment same stream we need use single stream all allocations memory pool otherwise allocations separate streams will reused separate recordings would have use same memory pool same memory torch cuda device device_index torch cuda synchronize stream = torch cuda Stream stream wait_stream torch cuda current_stream Keeps Memory Pool Alive graph Optional torch cuda CUDAGraph = torch cuda CUDAGraph cuda_graphs_thread_pool = torch cuda graph_pool_handle warnings catch_warnings record=True torch cuda graph graph pool=self cuda_graphs_thread_pool stream=self stream capture_error_mode= thread_local pass graph_counter = itertools count func_counter = itertools count mapping graph_id function id mutation type hint since we specializing particular combination Parent Node - Function ID non_cudagraph_managed_mutation_hint dict Optional GraphID dict FunctionID bool = defaultdict dict warmup_node_counter = itertools count start=- step=- mapping graph_id function id re-record count We fall back eager function function re-recorded frequently node num_rerecord dict Optional GraphID dict FunctionID int = defaultdict lambda defaultdict lambda whether we current node state warmup recording execution If there no current node state will ExecutionState None path_state = ExecutionState NONE device_index = device_index most recently invoked cudagraph wrapping function Will None when there no output previous recording execution whose memory we need respect cuda caching allocation If you incremented generation will also none ignore those allocations current_node Optional Union CUDAGraphNode CUDAWarmupNode = None current generation cudagraph invocations when torch compile run we increment current generation willing ignore live outputs previous generation checking liveness current_gen int = - number instances we execution failed match existing child debug_fail_counter = number instances we had checkpoint function debug_checkpointing_counter = id_to_mode dict FunctionID CompilationMode = id_to_compile_id dict FunctionID Optional CompileId = Note Backward Generation Handling We generally perform sequence forward executions followed backward executions If multiple torch compile wrapped forwards executed their backwards pending we should disregard outputs prior torch compile since entire training loop hasn t completed Occasionally backward pass corresponding forward pass may executed so we cannot wait all pending forward pass backward completions so we cannot wait all backwards have been invoked Instead we wait single backward invocation Triggering backward pass typically doesn t lead another torch compile invocation making less likely generation increase between multiple backward calls The following use case covered approach mod = torch compile mod = torch compile mod mod x sum backward running_forwards_with_pending_backwards = False mode Optional CompilationMode = None disable_invalidate_aliases = False torch _environment is_fbcode torch _utils_internal justknobs_check pytorch inductor disable_cudagraph_alias_invalidation run new_inputs list InputType function_id FunctionID - OutputType assert graph None Running CUDAGraph after shutdown mode = id_to_mode function_id compile_id = id_to_compile_id function_id out = _run new_inputs function_id The forwards only pending following invocation before mode == CompilationMode FORWARD running_forwards_with_pending_backwards = True mode == CompilationMode BACKWARD running_forwards_with_pending_backwards = False out set_to_running_backward - None running_forwards_with_pending_backwards = False mode = CompilationMode BACKWARD _get_cuda_graph_recorded_tensor_checker - Callable Tensor bool current_node _is_cuda_graph_recorded_tensor isinstance current_node CUDAGraphNode CUDAWarmupNode lambda _ False new_warmup_node_id - GraphID GraphID next warmup_node_counter _update_non_cudagraph_managed_mutation function_id FunctionID inputs list InputType - None node_id = _get_node_id maybe_mutation_str = check_for_mutation ids_to_funcs function_id inputs _get_cuda_graph_recorded_tensor_checker non_cudagraph_managed_mutation_hint node_id function_id = True warn once per function_id function_id warned_mutation warned_mutation add function_id log_cudagraph_skip_and_bump_counter maybe_mutation_str non_cudagraph_managed_mutation_hint node_id function_id = False _get_node_id - Optional GraphID current_node None None isinstance current_node CUDAGraphNode CUDAWarmupNode current_node id raise RuntimeError f Unknown node type type current_node exceed_rerecord_limit node_id Optional GraphID function_id FunctionID - bool torch _dynamo config inline_inbuilt_nn_modules False num_rerecord node_id function_id torch _inductor config triton cudagraph_unexpected_rerecord_limit _run new_inputs list InputType function_id FunctionID - OutputType we will try end current execution lazily since we dont want do unnecessary checking existing outputs hot path both recording warmup only happen once so we check up front in_recording try_end_curr_recording function_id in_warmup try_end_curr_warmup function_id node_id = _get_node_id function_id non_cudagraph_managed_mutation_hint node_id _update_non_cudagraph_managed_mutation function_id new_inputs Early exit function mutates inputs which neither parameters buffers nor cudagraph recorded tensors This check should happen after ` try_end_curr_recording ` ` try_end_curr_warmup ` which may change current_node non_cudagraph_managed_mutation_hint node_id function_id exceed_rerecord_limit node_id function_id ids_to_funcs function_id model new_inputs warming up function subsequentally recording may use different memory addresses because both depend state caching allocator we warm up graph A then warm up graph B make more allocations subsequent recording A will necessarily use same addresses warm up Thus any warm up node can only followed warm up runs function_id warmed_up_functions config triton skip_cudagraph_warmup in_warmup config triton force_cudagraphs_warmup If we middle executing cuda graphs then we need checkpoint memory state Both Recording Warmup will reflected allocator dont need changes path_state == ExecutionState EXECUTION apply_checkpoint_execution_state_in_allocator run_eager new_inputs function_id assert isinstance current_node CUDAWarmupNode child_nodes = roots current_node None current_node children in_recording unexpected_rerecord unexpected_rerecord_reason = False lambda child child_nodes function_id here we checking memory consistency between recording execution well things like stability tensor locations etc other status status_logger = child check_invariants new_inputs status == CheckInvariantStatus SUCCESS execute_node child new_inputs status == CheckInvariantStatus StaticInputIdxMismatch status == CheckInvariantStatus CudagraphManagedIdxMismatch unexpected_rerecord = True unexpected_rerecord_reason = status_logger now we know new function can t run child current node root try end current execution noted above we want do lazily avoid having check all existing outputs current_node None function_id roots try_end_curr_execution run again hit root matching case which must succeed current_node None run new_inputs function_id len ids_to_funcs function_id mutated_input_idxs _update_non_cudagraph_managed_mutation function_id new_inputs non_cudagraph_managed_mutation_hint _get_node_id function_id ids_to_funcs function_id model new_inputs nb run before checkpointing because checkpointing slow we will using eager caching allocator pool which does require live accounting tensors cudagraph allocator unexpected_rerecord curr_node_id = _get_node_id num_rerecord curr_node_id function_id += exceed_rerecord_limit curr_node_id function_id _id = curr_node_id id curr_node_id None log_cudagraph_skip_and_bump_counter f skipping cudagraph due function function_id id exceeding max f re-recording limit f = torch _inductor config triton cudagraph_unexpected_rerecord_limit f cudagraph node _id due unexpected_rerecord_reason ids_to_funcs function_id model new_inputs point we necessarily will do new recording debug_fail_counter += try_end_curr_execution current_node None apply_checkpoint_execution_state_in_allocator now we recording state record_function new_inputs function_id shutdown - None Remove all cached tensors all nodes Because cached tensors can hold gradients which turn might reference backward which invokes CUDA Graph Node we have manually clear them shutdown avoid reference cycle nodes = roots roots values nodes extend roots while nodes node = nodes pop children node children values nodes extend children node remove_node_cached_tensors node graph = None graph = None roots = None type ignore assignment current_node = None record_function new_inputs list InputType function_id FunctionID - OutputType assert isinstance current_node CUDAWarmupNode torch _dynamo callback_handler install_callbacks CallbackTrigger CUDAGRAPH_RECORDING str compile_id graph_id = new_graph_id log debug Recording function d graph recording id d function_id id graph_id id torch cuda synchronize node = CUDAGraphNode ids_to_funcs function_id graph_id current_node new_inputs cuda_graphs_thread_pool device_index ids_to_stack_traces function_id stream mode compile_id current_node None roots function_id append node current_node add_child function_id node current_node = node path_state = ExecutionState RECORDING update_generation torch cuda synchronize node run_first_inputs new_inputs execute_node node CUDAGraphNode new_inputs list InputType - OutputType current_node = node path_state = ExecutionState EXECUTION update_generation node run new_inputs run_eager new_inputs list InputType function_id FunctionID - OutputType only stored current node because when we start new path we will deallocate already_warm = function_id warmed_up_functions already_warm log debug Running warmup function d function_id id log debug Running eager function d because ancestor needed warm up function_id id warmed_up_functions add function_id node = CUDAWarmupNode ids_to_funcs function_id current_node cuda_graphs_thread_pool graph device_index ids_to_stack_traces function_id stream already_warm new_warmup_node_id current_node = node path_state = ExecutionState WARMUP update_generation node run new_inputs new_graph_id - GraphID GraphID next graph_counter new_func_id - FunctionID FunctionID next func_counter add_function model ModelType inputs list InputType static_input_idxs Sequence int stack_traces Optional StackTraces mode CompilationMode constants tuple torch Tensor placeholders tuple PlaceholderInfo mutated_input_idxs tuple int compile_id Optional CompileId - tuple ModelType OutputType id = new_func_id ids_to_stack_traces id = stack_traces ids_to_funcs id = WrappedFunction model list static_input_idxs id tuple t t constants isinstance t torch Tensor t is_cuda placeholders mutated_input_idxs id_to_mode id = mode id_to_compile_id id = compile_id fn = functools partial run function_id=id container needs set clean up when fn dies get_container device_index add_strong_reference fn fn fn inputs property in_recording - bool path_state == ExecutionState RECORDING property in_warmup - bool path_state == ExecutionState WARMUP get_roots - Iterator CUDAGraphNode nodes roots values yield nodes property current_node - Optional Union CUDAGraphNode CUDAWarmupNode _current_node current_node setter current_node value Optional Union CUDAGraphNode CUDAWarmupNode - None _current_node = value value None path_state = ExecutionState NONE update_generation - None current_gen = get_curr_generation staticmethod get_curr_generation - int MarkStepBox mark_step_counter = MarkStepBox mark_step_counter GenerationTracker generation staticmethod user_invoked_mark_step - bool MarkStepBox mark_step_counter = can_start_new_generation - bool in_new_torch_compile_invocation False user_invoked_mark_step True running_forwards_with_pending_backwards in_new_torch_compile_invocation - bool current_gen = get_curr_generation try_end_curr_recording function_id FunctionID - None Check current recording can terminated either because all outputs previously recorded node dead because executed different generation Will set current_node None in_recording False successful assert in_recording assert current_node None multiple invocations allow overwriting previous generation can_start_new_generation dealloc_current_path_weakrefs clear_current_path_state_and_set_to_none current_node all_outputs_are_dead clear_current_path_state_and_set_to_none check_warn_on_unable_to_start_executing function_id try_end_curr_execution - None Check current executing node can terminated either because all outputs previously executed node dead because executed different generation Will set current_node None successful assert in_recording current_node None can_start_new_generation clear_current_path_state_and_set_to_none current_node all_outputs_are_dead clear_current_path_state_and_set_to_none try_end_curr_warmup function_id FunctionID - None can_start_new_generation dealloc_current_path_weakrefs current_node = None assert current_node None current_node all_outputs_are_dead current_node = None check_warn_on_unable_to_start_executing function_id check_warn_on_unable_to_start_executing function_id FunctionID - None Warn we potential loop where we unable hit fast path function_id warned_functions in_new_torch_compile_invocation assert current_node None existing_nodes = node node current_node _path_from_root node wrapped_function id == function_id len existing_nodes = repeated same pattern parents = OrderedSet n parent wrapped_function id n itertools chain existing_nodes current_node n parent None len parents == len existing_nodes warned_functions add function_id warnings warn Unable hit fast path CUDAGraphs because pending uninvoked backwards Consider running torch no_grad using torch compiler cudagraph_mark_step_begin before each model invocation staticmethod format_dealloc_msg stack_trace Optional str - str stack_trace = stack_trace strip stack_trace Could find stack trace Error accessing tensor output CUDAGraphs has been overwritten subsequent run f Stack trace stack_trace To prevent overwriting clone tensor outside torch compile call torch compiler cudagraph_mark_step_begin before each model invocation dealloc_current_path_weakrefs - None assert current_node None TODO we could also allow these weak refs continue allocated adds some complications stor_stack_trace dict int Optional str = node current_node _path_from_root assert node stack_traces None assert len node tensor_weakrefs == len node stack_traces t stack_trace zip node tensor_weakrefs node stack_traces ten = None t None t ten None continue torch _C _set_storage_access_error_msg ten format_dealloc_msg stack_trace we would enable following assertion internal model failed command does repro len node outputs_weakrefs == len node stack_traces so pessimistically assume they might differ doing debug info loop separately dealloc loop disable_invalidate_aliases continue storage_ref stack_trace zip node outputs_weakrefs node stack_traces storage_ref continue stor_stack_trace storage_ref data_ptr = stack_trace deleted = OrderedSet Any storage_ref current_node path_live_weakrefs _storage_deref = storage_ref _storage_deref storage_ref data_ptr deleted deleted add storage_ref data_ptr msg = format_dealloc_msg stor_stack_trace get storage_ref data_ptr torch _C _free_And_Remove_DeleterFn _storage_deref disable_invalidate_aliases continue torch _C _set_storage_data_ptr_access_error_msg _storage_deref msg clear_current_path_state_and_set_to_none - None assert isinstance current_node CUDAGraphNode current_node clear_path_state current_node = None apply_checkpoint_execution_state_in_allocator - None Checkpoint current execution state caching allocator so additional cudagraph recordings can made respecting existent live storages assert isinstance current_node CUDAGraphNode debug_checkpointing_counter += log debug Checkpointing cuda caching allocator state Number checkpoints d debug_checkpointing_counter state = current_node checkpointed_caching_state device = current_node device assert state None device None currently we deallocate instead allowing stale recordings stale_storages list int = remove cached tensors otherwise they would prevent memory being reclaimed subsequent recordings current_node remove_path_cached_tensors live_storages_wrappers = list current_node path_live_weakrefs path_live_weakrefs guarantees t will None live_storages_weak_refs list int = t t live_storages_wrappers type ignore misc ptrs_to_deallocate = current_node data_ptrs_dead_since_invocation torch _C _cuda_setCheckpointPoolState device pyrefly ignore bad-argument-type state stale_storages live_storages_weak_refs NB deduplicate aliased outputs ptr OrderedSet ptrs_to_deallocate torch _C _cuda_cudaCachingAllocator_raw_delete ptr Now live blocks should exactly equal live storages private pool config triton slow_path_cudagraph_asserts check_memory_pool device_index cuda_graphs_thread_pool live_storages_wrappers wrapper live_storages_wrappers storage_ptr = wrapper assert storage_ptr None assert torch _C _has_Standard_Deleter storage_ptr assert wrapper data_ptr ptrs_to_deallocate live_cudagraph_pool_storages_in_curr_execution - list StorageWeakRefPointer current_node None explicitly ignoring previous recorded outputs past path path_live_weakrefs guarantees t will None t t current_node path_live_weakrefs type ignore misc