pyre-strict concurrent futures glob json logging math mmap os struct time dataclasses dataclass field typing Any Optional torch torch distributed dist torch distributed checkpoint _hf_utils _gen_file_name _get_dcp_custom_metadata _get_safetensors_file_metadata _metadata_fn DATA_OFFSETS_KEY DEFAULT_EXTRA_METADATA_KEY DTYPE_KEY SAVED_OFFSETS_KEY SHAPE_KEY SUFFIX logger logging Logger = logging getLogger __name__ dataclass _FqnData Dataclass store information about tensor identified its fully qualified name Attributes offset_in_file Byte offset where tensor s data begins output file shape_in_file Shape tensor output file dtype_size Size tensor s data type bytes dtype_str String representation tensor s data type offset_in_file int = shape_in_file list int = field default_factory=list dtype_size int = dtype_str str = dataclass _OutputFileData Dataclass store information about output safetensors file Attributes metadata_size Size metadata section bytes fqn_data Dictionary mapping tensor names their metadata metadata_size int = fqn_data dict str _FqnData = field default_factory=dict dataclass _InputFileData Dataclass store information about input safetensors file Attributes metadata_size Size metadata section bytes metadata Json metadata safetensors file metadata_size int = metadata Any = None _parse_input_metadata input_files_data dict str _InputFileData output_files_data dict str _OutputFileData - None Parse metadata input safetensors files determine full tensor shapes types This function analyzes metadata all input files determine complete shape each tensor after consolidation It updates output_files_data information Args input_files_data dict metadata input safetensors files output_files_data Dictionary mapping output file paths their metadata Raises ValueError If no DCP custom metadata found safetensors file safetensors torch _getdtype type ignore Dictionary track full size each tensor across all shards fqn_to_size_mapping dict str tuple list int str = file_data input_files_data values safetensors_metadata = file_data metadata dcp_sharding_info = _get_dcp_custom_metadata safetensors_metadata dcp_sharding_info raise ValueError No DCP custom metadata found safetensors file The file must saved DCP consolidated key val safetensors_metadata items key == DEFAULT_EXTRA_METADATA_KEY continue Get shape tensor shard its offset full tensor sizes = val SHAPE_KEY offsets = dcp_sharding_info key SAVED_OFFSETS_KEY key fqn_to_size_mapping First time seeing tensor - calculate its full size adding offsets dimensions cur_size = size + offset size offset zip sizes offsets fqn_to_size_mapping key = cur_size val DTYPE_KEY We ve seen tensor before - update its size shard extends beyond current known dimensions cur_size = fqn_to_size_mapping key i range len sizes cur_size i = max cur_size i sizes i + offsets i Now we know full size each tensor populate output file data fqn tensor_info fqn_to_size_mapping items tensor_size = tensor_info dtype_str = tensor_info output_data output_files_data values Add tensor output file s already assigned there fqn output_data fqn_data output_data fqn_data fqn = _FqnData shape_in_file=tensor_size dtype_size=torch finfo _getdtype dtype_str bits Convert bits bytes dtype_str=dtype_str _write_metadata output_files_data dict str _OutputFileData - None Write metadata beginning each output safetensors file This function writes metadata section each output file including information about tensor shapes data types offsets It also updates offset_in_file field each tensor output_files_data Args output_files_data Dictionary mapping output file paths their metadata Process each output file file_path output_data output_files_data items open file_path wb f metadata = curr_offset = Calculate offsets each tensor file fqn fqn_data output_data fqn_data items Calculate end offset multiplying all dimensions data type size end_offset = curr_offset + math prod fqn_data shape_in_file fqn_data dtype_size Store metadata tensor metadata fqn = SHAPE_KEY fqn_data shape_in_file DTYPE_KEY fqn_data dtype_str DATA_OFFSETS_KEY curr_offset end_offset Start end byte offsets Store offset later use when writing actual tensor data fqn_data offset_in_file = curr_offset Update current offset next tensor curr_offset = end_offset Convert metadata JSON encode bytes json_metadata = json dumps metadata json_bytes = json_metadata encode utf- Write metadata size -byte unsigned integer little-endian size_in_bytes = len json_bytes header_len = struct pack Q size_in_bytes Write header length metadata file f write header_len f write json_bytes Store total metadata size header + JSON later use output_data metadata_size = f tell _read_tensor_data_mmap file_path str start_offset int end_offset int metadata_size int - bytes Read tensor data safetensors file using memory mapping efficiency Args file_path Path safetensors file start_offset Start offset tensor data within data section end_offset End offset tensor data within data section metadata_size Size metadata header Returns Raw tensor data bytes Use mmap efficient access open file_path rb f mmap mmap f fileno access=mmap ACCESS_READ mm absolute_start = metadata_size + start_offset absolute_end = metadata_size + end_offset bytes mm absolute_start absolute_end _process_output_file output_file str output_data _OutputFileData input_files_data dict str _InputFileData - None Process single output file writing tensor data input files using memory mapping This function designed run parallel different output files Args output_file Path output file output_data Metadata output file input_files_data Dictionary mapping input file paths their metadata sorted_tensors = sorted output_data fqn_data items key=lambda x x offset_in_file open output_file r+b output_stream output_stream seek os SEEK_END Process each tensor sequential output order tensor_fqn tensor_fqn_data sorted_tensors full_tensor_mv = memoryview bytearray math prod tensor_fqn_data shape_in_file tensor_fqn_data dtype_size Process each input safetensors file safetensors_file input_files_data keys file_metadata = input_files_data safetensors_file metadata input_metadata_size = input_files_data safetensors_file metadata_size tensor_fqn file_metadata keys continue metadata = file_metadata tensor_fqn data_offsets = metadata DATA_OFFSETS_KEY Use memory mapping read tensor data efficiently data_to_write = _read_tensor_data_mmap safetensors_file data_offsets data_offsets input_metadata_size Get offsets tensor shard within full tensor fqn_custom_metadata = _get_dcp_custom_metadata file_metadata tensor_fqn type ignore index offsets_of_tensor_being_read = fqn_custom_metadata SAVED_OFFSETS_KEY type ignore index Write tensor shard appropriate position output file _write_sub_tensor_to_file_optimized full_tensor_mv data_to_write tensor_fqn_data dtype_size Size each element bytes tensor_fqn_data shape_in_file Full tensor shape offsets_of_tensor_being_read Where shard belongs full tensor metadata SHAPE_KEY Shape shard output_stream write full_tensor_mv _write_data input_files_data dict str _InputFileData output_files_data dict str _OutputFileData num_threads int = - None Write tensor data input files output files using memory mapping This function reads tensor data each input file writes appropriate position output files based tensor s offsets When num_threads work split across threads each thread handling different output file Args input_files_data Dictionary mapping input file paths their metadata output_files_data Dictionary mapping output file paths their metadata num_threads Number threads use parallel processing num_threads = len output_files_data = Sequential processing output_file output_data output_files_data items _process_output_file output_file output_data input_files_data Parallel processing ThreadPoolExecutor concurrent futures ThreadPoolExecutor max_workers=min num_threads len output_files_data executor futures = output_file output_data output_files_data items futures append executor submit _process_output_file output_file output_data input_files_data Wait all futures complete future concurrent futures as_completed futures Handle any exceptions might have occurred try future result except Exception e print f Error processing output file e raise _write_sub_tensor_to_file_optimized full_tensor_mv memoryview sub_tensor_bytes bytes element_size int tensor_shape list int sub_tensor_offsets list int sub_tensor_shape list int - None Optimized version writes maximum number contiguous bytes possible Uses unified algorithm calculates maximum contiguous bytes can written each iteration continues until entire subtensor written Handles all sharding patterns efficiently - Full sub-tensor once row-wise sharding - Row-by-row column-wise sharding - Optimized chunks other patterns Args full_tensor_mv Buffer write full tensor sub_tensor_bytes Raw tensor data bytes element_size Size each element bytes tensor_shape Shape full tensor sub_tensor_offsets Starting offsets sub-tensor within full tensor sub_tensor_shape Shape sub-tensor Handle empty tensors tensor_shape sub_tensor_shape Calculate tensor strides efficient indexing tensor_strides = i range len tensor_shape - - tensor_strides insert tensor_strides tensor_shape i sub_tensor_strides = i range len sub_tensor_shape - - sub_tensor_strides insert sub_tensor_strides sub_tensor_shape i total_elements = math prod sub_tensor_shape elements_written = while elements_written total_elements Convert linear index multi-dimensional indices temp_idx = elements_written indices = dim_size reversed sub_tensor_shape indices append temp_idx dim_size temp_idx = dim_size indices reverse Calculate maximum contiguous elements we can write position max_contiguous = _calculate_max_contiguous_elements indices sub_tensor_shape tensor_shape Calculate source position bytes src_pos = sum idx stride idx stride zip indices sub_tensor_strides src_byte_offset = src_pos element_size Calculate destination position bytes dest_indices = idx + offset idx offset zip indices sub_tensor_offsets dest_pos = sum idx stride idx stride zip dest_indices tensor_strides dest_byte_offset = dest_pos element_size Write contiguous chunk bytes_to_write = max_contiguous element_size chunk_data = sub_tensor_bytes src_byte_offset src_byte_offset + bytes_to_write full_tensor_mv dest_byte_offset dest_byte_offset + bytes_to_write = chunk_data elements_written += max_contiguous _calculate_max_contiguous_elements indices list int sub_tensor_shape list int tensor_shape list int - int Calculate maximum number contiguous elements can written current position This determines largest chunk checking how elements laid out memory finding natural boundaries where contiguity breaks Args indices Current position indices sub-tensor sub_tensor_shape Shape sub-tensor being written tensor_shape Shape full tensor Raises ValueError If input lists empty have mismatched lengths contain invalid values Validate input lists empty indices sub_tensor_shape tensor_shape raise ValueError Input lists cannot empty Validate all lists have same length same number dimensions len indices == len sub_tensor_shape == len tensor_shape raise ValueError f All input lists must have same length Got indices len indices f sub_tensor_shape len sub_tensor_shape tensor_shape len tensor_shape Validate indices within bounds sub_tensor_shape i idx sub_dim enumerate zip indices sub_tensor_shape idx = sub_dim raise ValueError f Index idx dimension i out bounds sub-tensor shape sub_tensor_shape Validate sub_tensor dimensions don t exceed tensor dimensions i sub_dim tensor_dim enumerate zip sub_tensor_shape tensor_shape sub_dim tensor_dim raise ValueError f Sub-tensor dimension sub_dim position i exceeds tensor dimension tensor_dim Start elements remaining last dimension max_contiguous = sub_tensor_shape - - indices - Check we can extend across multiple dimensions We can write across dimension boundaries we re writing complete rows layout destination tensor maintains contiguity For D case check we can write multiple complete rows len sub_tensor_shape = If we re start row can write complete rows indices - == At start last dimension column rows_remaining = sub_tensor_shape - - indices - Rows left write Check writing complete rows maintains contiguity destination This true row-wise sharding when sub-tensor spans full width sub_tensor_shape - == tensor_shape - Full width max_contiguous = rows_remaining sub_tensor_shape - For higher dimensions check we can extend further len sub_tensor_shape = indices - == Check we can write complete D slices remaining_in_dim = sub_tensor_shape - - indices - sub_tensor_shape - == tensor_shape - sub_tensor_shape - == tensor_shape - max_contiguous = remaining_in_dim sub_tensor_shape - sub_tensor_shape - max_contiguous _write_overall_metadata_file output_dir str output_files_data dict str _OutputFileData - None Write overall metadata file maps tensor names their file locations This creates model safetensors index json file HuggingFace models use locate tensors across multiple files Args output_dir Directory where metadata file will written output_files_data Dictionary mapping output file paths their metadata total_size = weight_map = output_path value output_files_data items fqn fqn_data value fqn_data items total_size += math prod fqn_data shape_in_file fqn_data dtype_size weight_map fqn = os path basename output_path metadata_to_write dict str Any = metadata_to_write metadata = total_size total_size metadata_to_write weight_map = weight_map metadata_path = os path join output_dir f _metadata_fn open metadata_path w metadata_file json dump metadata_to_write metadata_file indent= _consolidate_safetensors_files input_dir str output_dir str fqn_to_file_mapping dict str str num_threads int - dict str _OutputFileData output_files_data dict str _OutputFileData = Create multiple output files based provided mapping fqn filename fqn_to_file_mapping items output_path = os path join output_dir filename output_path output_files_data output_files_data output_path = _OutputFileData fqn_data= fqn _FqnData output_files_data output_path fqn_data fqn = _FqnData Find all safetensors files input directory safetensors_files = glob glob os path join input_dir f SUFFIX Read metadata all input files input_files_data dict str _InputFileData = safetensor_file safetensors_files open safetensor_file rb f metadata size = _get_safetensors_file_metadata f input_files_data safetensor_file = _InputFileData metadata_size=size metadata=metadata Step Parse metadata determine tensor shapes types _parse_input_metadata input_files_data output_files_data Step Write metadata headers output files _write_metadata output_files_data Step Write actual tensor data input files output files _write_data input_files_data output_files_data num_threads output_files_data consolidate_safetensors_files input_dir str output_dir str fqn_to_index_mapping dict str int num_threads int = - None Main function consolidate sharded safetensors files into one more output files This function orchestrates entire consolidation process Sets up output file structure based fqn_to_index_mapping Finds all safetensors files input directory Parses metadata all input files Writes metadata output files Writes tensor data input files output files Writes overall model index safetensors json file weight map Args input_dir Directory containing sharded safetensors files output_dir Directory where consolidated files will written fqn_to_index_mapping Optional mapping tensor names output file indices If None all tensors will consolidated into single file num_threads Number threads use parallel processing saving data output files start_time = time time logger info Consolidating safetensors files s s Beginning time f input_dir output_dir start_time max_index = max fqn_to_index_mapping values fqn_to_file_mapping = fqn _gen_file_name idx max_index fqn idx fqn_to_index_mapping items output_files_data = _consolidate_safetensors_files input_dir output_dir fqn_to_file_mapping num_threads Step Write overall model index safetensors json file weight map _write_overall_metadata_file output_dir output_files_data logger info Done consolidating Took f secs time time - start_time consolidate_safetensors_files_on_every_rank input_dir str output_dir str fqn_to_index_mapping dict str int num_threads int = process_group Optional dist ProcessGroup = None - None Consolidate sharded safetensors files across multiple ranks each rank handling subset output files This function distributes consolidation work assigning output files different ranks All tensors same index fqn_to_index_mapping processed same rank they belong same output file If process_group provided rank world_size will derived Otherwise they will automatically detected distributed environment available Args input_dir Directory containing sharded safetensors files output_dir Directory where consolidated files will written fqn_to_index_mapping Mapping tensor names output file indices num_threads Number threads use parallel processing each rank process_group PyTorch distributed process group default None will use default group start_time = time time Derive rank world_size process_group default distributed environment dist is_available dist is_initialized rank = dist get_rank group=process_group world_size = dist get_world_size group=process_group Default single process mode distributed initialized rank = world_size = logger warning Distributed environment initialized Running single process mode logger info Rank d d Consolidating safetensors files s s rank world_size input_dir output_dir Find all unique indices mapping unique_indices = set fqn_to_index_mapping values Distribute indices across ranks indices_for_this_rank = idx unique_indices Simple distribution index world_size == rank idx world_size == rank indices_for_this_rank append idx logger info Rank d Assigned d output files out d total files rank len indices_for_this_rank len unique_indices Filter fqn_to_index_mapping only include tensors rank filtered_mapping = fqn idx fqn idx fqn_to_index_mapping items idx indices_for_this_rank filtered_mapping Convert index mapping filename mapping max_index = max unique_indices filtered_filename_mapping = fqn idx filtered_mapping items filename = _gen_file_name idx max_index filtered_filename_mapping fqn = filename Call existing consolidation function filtered mapping _consolidate_safetensors_files input_dir=input_dir output_dir=output_dir fqn_to_file_mapping=filtered_filename_mapping num_threads=num_threads logger info Rank d Done consolidating Processed d unique indices f secs rank len indices_for_this_rank time time - start_time Wait all ranks complete dist is_available dist is_initialized logger info Rank d Waiting all ranks complete rank dist barrier logger info Rank d All ranks have completed rank rank == logger info Total time taken f secs time time - start_time