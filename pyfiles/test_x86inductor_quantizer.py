Owner s oncall quantization copy itertools enum Enum torch torch ao quantization quantizer x _inductor_quantizer xiq torch nn nn torch ao quantization ObserverBase torch ao quantization pt e lowering lower_pt e_quantized_to_x torch ao quantization quantize_pt e convert_pt e prepare_pt e prepare_qat_pt e torch ao quantization quantizer x _inductor_quantizer QUANT_ANNOTATION_KEY X InductorQuantizer torch export export torch testing _internal common_quantization NodeSpec ns QuantizationTestCase skipIfNoInductorSupport skipIfNoX torch testing _internal common_quantized override_quantized_engine torch testing _internal common_utils raise_on_run_directly skipIfTorchDynamo NodePosType Enum left = right = both = TestHelperModules SingleConv dModule torch nn Module __init__ with_bn=False - None super __init__ conv = nn Conv d stride= padding= bn = torch nn BatchNorm d with_bn = with_bn forward x x = conv x with_bn x = bn x x Conv dUnaryModule torch nn Module __init__ post_op use_bias bool = False with_bn=False - None super __init__ conv = nn Conv d stride= padding= bias=use_bias post_op = post_op bn = torch nn BatchNorm d with_bn = with_bn maxpool = torch nn MaxPool d forward x x = conv x with_bn x = bn x x = post_op x x = maxpool x x Conv dAddModule torch nn Module __init__ inplace_add bool = False conv d_type NodePosType = NodePosType left use_bias bool = False with_bn bool = False - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= bias=use_bias conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= bias=use_bias relu = nn ReLU inplace_add = inplace_add conv d_type = conv d_type bn = torch nn BatchNorm d with_bn = with_bn forward x conv d_type == NodePosType left inplace_add tmp = conv x with_bn tmp = bn tmp tmp += relu x tmp tmp = conv x with_bn tmp = bn tmp tmp + relu x conv d_type == NodePosType right inplace_add tmp = relu x tmp += conv x tmp relu x + conv x conv d_type == NodePosType both inplace_add tmp = conv x tmp += conv x tmp conv x + conv x Conv dAddReLUModule torch nn Module __init__ inplace_add bool = False conv d_type NodePosType = NodePosType left inplace_relu bool = False use_bias bool = False with_bn bool = False - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= bias=use_bias conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= bias=use_bias relu = nn ReLU inplace_add = inplace_add conv d_type = conv d_type relu = nn ReLU inplace=inplace_relu bn = torch nn BatchNorm d with_bn = with_bn forward x conv d_type == NodePosType left inplace_add tmp = conv x with_bn tmp = bn tmp tmp += relu x relu tmp tmp = conv x with_bn tmp = bn tmp relu tmp + relu x conv d_type == NodePosType right inplace_add tmp = relu x tmp += conv x relu tmp relu relu x + conv x conv d_type == NodePosType both inplace_add tmp = conv x tmp += conv x relu tmp relu conv x + conv x Conv dSingleOpPowModule nn Module __init__ single_op super __init__ conv = nn Conv d single_op = single_op forward x x = conv x x = single_op x torch pow x SerialsConv dAddReLUModule torch nn Module Serials Conv d - Add - ReLU Pattern __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= bias=True conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= bias=True conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= bias=True conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= bias=True relu = nn ReLU relu = nn ReLU forward x x = conv x res = relu conv x + conv x res = relu conv res + res res Conv dCatMaxpool d torch nn Module __init__ super __init__ conv = torch nn Conv d bias=True stride= padding= dilation= conv = torch nn Conv d bias=True stride= padding= dilation= relu = torch nn ReLU maxpool = torch nn MaxPool d stride= padding= conv = torch nn Conv d bias=True stride= padding= dilation= forward x temp = relu conv x temp = conv x + temp = torch cat temp temp temp = maxpool temp temp = conv temp temp Conv dAvgPool d torch nn Module __init__ super __init__ conv = torch nn Conv d bias=True stride= padding= dilation= avgpool = torch nn AvgPool d stride= padding= forward x temp = avgpool conv x temp Conv dCatSameInputs torch nn Module __init__ super __init__ conv = torch nn Conv d bias=True stride= padding= dilation= relu = torch nn ReLU forward x temp = relu conv x temp = torch cat temp temp temp Conv dCatSingleInput torch nn Module __init__ super __init__ conv = torch nn Conv d bias=True stride= padding= dilation= relu = torch nn ReLU forward x temp = relu conv x temp = torch cat temp temp SingleLinearModule torch nn Module __init__ use_bias - None super __init__ linear = nn Linear bias=use_bias forward x linear x LinearUnaryModule torch nn Module __init__ use_bias postop inplace_postop=False post_op_algo= none - None super __init__ linear = nn Linear bias=use_bias postop nn GELU postop = postop approximate=post_op_algo postop = postop inplace=inplace_postop forward x postop linear x LinearAddModule torch nn Module __init__ inplace_add bool = False linear_pos NodePosType = NodePosType left use_bias bool = False - None super __init__ linear = torch nn Linear in_features= out_features= bias=use_bias linear = torch nn Linear in_features= out_features= bias=use_bias relu = nn ReLU inplace_add = inplace_add linear_pos = linear_pos forward x linear_pos == NodePosType left inplace_add tmp = linear x tmp += relu x tmp tmp = linear x tmp + relu x linear_pos == NodePosType right inplace_add tmp = relu x tmp += linear x tmp relu x + linear x linear_pos == NodePosType both inplace_add tmp = linear x tmp += linear x tmp linear x + linear x LinearAddReLUModule torch nn Module __init__ inplace_add bool = False linear_pos NodePosType = NodePosType left inplace_relu bool = False use_bias bool = False - None super __init__ linear = torch nn Linear in_features= out_features= bias=use_bias linear = torch nn Linear in_features= out_features= bias=use_bias relu = nn ReLU inplace_add = inplace_add linear_pos = linear_pos relu = nn ReLU inplace=inplace_relu forward x linear_pos == NodePosType left inplace_add tmp = linear x tmp += relu x relu tmp tmp = linear x relu tmp + relu x linear_pos == NodePosType right inplace_add tmp = relu x tmp += linear x relu tmp relu relu x + linear x linear_pos == NodePosType both inplace_add tmp = linear x tmp += linear x relu tmp relu linear x + linear x SerialsLinearAddReLUModule torch nn Module Serials Linear - Add - ReLU Pattern __init__ - None super __init__ linear = torch nn Linear in_features= out_features= bias=True linear = torch nn Linear in_features= out_features= bias=True linear = torch nn Linear in_features= out_features= bias=True linear = torch nn Linear in_features= out_features= bias=True relu = nn ReLU relu = nn ReLU forward x x = linear x res = relu linear x + linear x res = relu linear res + res res LinearAddModule torch nn Module __init__ inplace_add bool = False - None super __init__ linear = torch nn Linear in_features= out_features= bias=True linear = torch nn Linear in_features= out_features= bias=True inplace_add = inplace_add forward x inplace_add tmp = linear x tmp += linear tmp tmp tmp = linear x tmp + linear tmp Conv dAddModule torch nn Module __init__ inplace_add bool = False - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= inplace_add = inplace_add bn = torch nn BatchNorm d bn = torch nn BatchNorm d forward x inplace_add tmp = bn conv x tmp += bn conv tmp tmp tmp = bn conv x tmp + bn conv tmp SelfAttnLikeModule torch nn Module __init__ input_dim transpose_for_score=False num_attention_heads=None attention_head_size=None - None super __init__ input_dim = input_dim q_proj = nn Linear input_dim input_dim bias=False k_proj = nn Linear input_dim input_dim bias=False v_proj = nn Linear input_dim input_dim bias=False softmax = nn Softmax dim=- transpose_for_score = transpose_for_score transpose_for_score assert num_attention_heads None assert attention_head_size None num_attention_heads = num_attention_heads attention_head_size = attention_head_size transpose_for_scores x torch Tensor - torch Tensor new_x_shape = x size - + num_attention_heads attention_head_size x = x view new_x_shape x permute forward x q = q_proj x k = k_proj x v = v_proj x transpose_for_score q = transpose_for_scores q k = transpose_for_scores k v = transpose_for_scores v scores = torch matmul q k transpose - - input_dim attention = softmax scores weighted = torch matmul attention v weighted Conv dFlattenTranspose nn Module __init__ super __init__ projection = torch nn Conv d kernel_size= stride= cls_token = torch rand forward pixel_values embeddings = projection pixel_values flatten transpose embeddings = torch cat cls_token embeddings dim= embeddings Conv dFlattenCatTranspose nn Module __init__ super __init__ conv = torch nn Conv d kernel_size= stride= forward x y = conv x flatten y = torch cat y y dim=- y transpose MiniResNet nn Module BasicBlock nn Module __init__ in_channels out_channels stride= downsample=None super __init__ conv = nn Conv d in_channels out_channels kernel_size= stride=stride padding= bias=False bn = nn BatchNorm d out_channels relu = nn ReLU conv = nn Conv d out_channels out_channels kernel_size= stride= padding= bias=False bn = nn BatchNorm d out_channels downsample = downsample forward x identity = x out = conv x out = bn out out = relu out out = conv out out = bn out downsample None identity = downsample x out += identity out = relu out out __init__ num_classes= super __init__ in_channels = conv = nn Conv d in_channels kernel_size= stride= padding= bias=False bn = nn BatchNorm d in_channels relu = nn ReLU layer = _make_layer layer = _make_layer stride= layer = _make_layer stride= avgpool = nn AdaptiveAvgPool d fc = nn Linear num_classes _make_layer out_channels blocks stride= downsample = None stride = in_channels = out_channels downsample = nn Sequential nn Conv d in_channels out_channels kernel_size= stride=stride bias=False nn BatchNorm d out_channels layers = layers append BasicBlock in_channels out_channels stride downsample in_channels = out_channels _ range blocks layers append BasicBlock in_channels out_channels nn Sequential layers forward x x = conv x x = bn x x = relu x x = layer x x = layer x x = layer x x = avgpool x x = torch flatten x x = fc x x X InductorQuantTestCase QuantizationTestCase _test_quantizer model example_inputs quantizer expected_node_occurrence expected_node_list=None is_qat=False debug=False lower=False m_eager = model train is_qat model eval program capture m = copy deepcopy m_eager m = export m example_inputs strict=True module QAT Model failed deepcopy export_model = m is_qat copy deepcopy m m = prepare_qat_pt e m quantizer is_qat prepare_pt e m quantizer Calibrate m example_inputs prepare_model = copy deepcopy m m = convert_pt e m convert_model = copy deepcopy m debug convert_model print_readable True lower m = lower_pt e_quantized_to_x m example_inputs m example_inputs node_occurrence = ns call_function k v k v expected_node_occurrence items expected_node_list None expected_node_list = node_list = ns call_function n n expected_node_list checkGraphModuleNodes m expected_node_occurrence=node_occurrence expected_node_list=node_list export_model prepare_model convert_model skipIfNoInductorSupport TestQuantizePT EX Inductor X InductorQuantTestCase skipIfNoX test_conv d Test pattern single conv d X InductorQuantizer override_quantized_engine x torch no_grad m = TestHelperModules SingleConv dModule eval example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config node_occurrence = one input weight conv torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfNoX test_conv d_unary Test pattern conv d unary post ops such relu hardtanh hardswish relu X InductorQuantizer unary_map = relu torch nn ReLU inplace=False torch ops aten relu default relu_inplace torch nn ReLU inplace=True torch ops aten relu_ default hardtanh torch nn Hardtanh min_val= max_val= inplace=False torch ops aten hardtanh default hardtanh_inplace torch nn Hardtanh min_val= max_val= inplace=True torch ops aten hardtanh_ default relu torch nn ReLU inplace=False torch ops aten hardtanh default relu _inplace torch nn ReLU inplace=True torch ops aten hardtanh_ default hardswish torch nn Hardswish inplace=False torch ops aten hardswish default hardswish_inplace torch nn Hardswish inplace=True torch ops aten hardswish_ default swish torch nn SiLU inplace=False torch ops aten silu default swish_inplace torch nn SiLU inplace=True torch ops aten silu_ default use_bias_list = True False override_quantized_engine x torch no_grad unary_op use_bias itertools product unary_map keys use_bias_list m = TestHelperModules Conv dUnaryModule unary_map unary_op use_bias=use_bias eval example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config node_occurrence = one input weight conv torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default unary_map unary_op _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfNoX test_conv d_binary Test pattern conv d binary post ops such add X InductorQuantizer Currently only add binary post op supported conv d_type_list = NodePosType left NodePosType both example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config override_quantized_engine x torch no_grad conv d_type conv d_type_list m = TestHelperModules Conv dAddModule conv d_type=conv d_type eval conv d_type = NodePosType both node_occurrence = one input weight conv one extra input node add torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_occurrence = one input conv one input another conv conv will share same input quant dequant one extra input node add torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops aten add Tensor _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfNoX test_conv d_binary Test Pattern tmp = conv d_ x tmp = conv d_ tmp tmp + tmp Since conv d_ has users we should annotate conv d_ binary fusion instead conv d_ example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config inplace_add_list = True False override_quantized_engine x torch no_grad inplace_add inplace_add_list m = TestHelperModules Conv dAddModule inplace_add=inplace_add eval node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default torch ops aten add_ Tensor inplace_add torch ops aten add Tensor _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfNoX test_conv d_binary_unary Test pattern conv d binary + unary post ops such add + relu X InductorQuantizer Currently only add binary post op relu unary post op supported conv d_type_list = NodePosType left NodePosType both example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config override_quantized_engine x torch no_grad conv d_type conv d_type_list m = TestHelperModules Conv dAddReLUModule conv d_type=conv d_type eval conv d_type = NodePosType both node_occurrence = one input conv one extra input node add torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_occurrence = one input conv one input another conv conv will share same input quant dequant one extra input node add torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops aten add Tensor _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfNoX test_conv d_serials_binary_unary Test pattern following up conv d add relu X InductorQuantizer override_quantized_engine x torch no_grad m = TestHelperModules SerialsConv dAddReLUModule eval example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops aten conv d default torch ops aten add Tensor torch ops aten relu default _test_quantizer m example_inputs quantizer node_occurrence node_list _single_op_share_observer_recipe_test_helper m x single_op quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config example_inputs = x node_occurrence = one input weight conv two input output maxpool d torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default single_op torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default _ prepare_model _ = _test_quantizer m example_inputs quantizer node_occurrence node_list Check Maxpool d has share observer input output node prepare_model graph nodes node op == call_function node target single_op single_op_node = node input_obs_of_single_op = getattr prepare_model single_op_node args target output_obs_of_single_op = getattr prepare_model next iter single_op_node users target node op == call_function node target torch ops aten conv d default conv_node = node input_obs_of_conv = getattr prepare_model conv_node args target assertTrue isinstance input_obs_of_single_op ObserverBase assertTrue isinstance output_obs_of_single_op ObserverBase assertTrue isinstance input_obs_of_conv ObserverBase assertTrue input_obs_of_single_op output_obs_of_single_op assertTrue input_obs_of_single_op input_obs_of_conv skipIfNoX test_maxpool d_recipe r Test pattern int _in_int _out_ops maxpool - non_quantizable op pow Since maxpool int _in_int _out_op there obs between maxpool pow _single_op_share_observer_recipe_test_helper TestHelperModules Conv dSingleOpPowModule nn MaxPool d eval torch rand torch ops aten max_pool d default skipIfNoX test_adaptive_avg_pool d_recipe r Test pattern int _in_int _out_ops adaptive_avg_pool d - non_quantizable op pow Since adaptive_avg_pool d int _in_int _out_op there obs between adaptive_avg_pool d pow _single_op_share_observer_recipe_test_helper TestHelperModules Conv dSingleOpPowModule nn AdaptiveAvgPool d eval torch rand torch ops aten adaptive_avg_pool d default skipIfNoX test_flatten_recipe r Test pattern conv - flatten - cat - transpose m = TestHelperModules Conv dFlattenCatTranspose eval x = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config example_inputs = x node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten flatten using_ints torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten cat default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default _ prepare_model _ = _test_quantizer m example_inputs quantizer node_occurrence node_list Check Flatten has share observer input output node prepare_model graph nodes node op == call_function node target torch ops aten flatten using_ints single_op_node = node input_obs_of_single_op = getattr prepare_model single_op_node args target output_obs_of_single_op = getattr prepare_model next iter single_op_node users target node op == call_function node target torch ops aten conv d default conv_node = node input_obs_of_conv = getattr prepare_model conv_node args target assertTrue isinstance input_obs_of_single_op ObserverBase assertTrue isinstance output_obs_of_single_op ObserverBase assertTrue isinstance input_obs_of_conv ObserverBase assertTrue input_obs_of_single_op output_obs_of_single_op assertTrue input_obs_of_single_op input_obs_of_conv skipIfNoX test_flatten_recipe r Test pattern conv - flatten - transpose m = TestHelperModules Conv dFlattenTranspose eval x = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config example_inputs = x node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops aten flatten using_ints torch ops aten transpose int _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfNoX test_cat_recipe r Test pattern conv - cat - maxpool d Since cat maxpool int _in_int _out_op inputs outputs should same observer m = TestHelperModules Conv dCatMaxpool d eval x = torch randn contiguous memory_format=torch channels_last quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config example_inputs = x node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten cat default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten max_pool d default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default _ prepare_model _ = _test_quantizer m example_inputs quantizer node_occurrence node_list Check Cat Maxpool d has share observer input output node prepare_model graph nodes node op == call_function node target == torch ops aten cat default cat_act_obs = getattr prepare_model node all_input_nodes target cat_act_obs = getattr prepare_model node all_input_nodes target cat_out_obs = getattr prepare_model next iter node users target node op == call_function node target torch ops aten max_pool d default maxpool_node = node input_obs_of_maxpool = getattr prepare_model maxpool_node args target output_obs_of_maxpool = getattr prepare_model next iter maxpool_node users target assertTrue isinstance cat_act_obs ObserverBase assertTrue isinstance cat_act_obs ObserverBase assertTrue isinstance cat_out_obs ObserverBase assertTrue isinstance input_obs_of_maxpool ObserverBase assertTrue isinstance output_obs_of_maxpool ObserverBase assertTrue cat_act_obs cat_act_obs assertTrue cat_act_obs cat_out_obs assertTrue cat_out_obs input_obs_of_maxpool assertTrue input_obs_of_maxpool output_obs_of_maxpool skipIfNoX test_cat_recipe_same_inputs r Test pattern conv - cat input input Since cat has input node same tensor they should also same observer m = TestHelperModules Conv dCatSameInputs eval x = torch randn contiguous memory_format=torch channels_last quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config example_inputs = x node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten cat default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default _ prepare_model _ = _test_quantizer m example_inputs quantizer node_occurrence node_list Check Cat has share observer input output node prepare_model graph nodes node op == call_function node target == torch ops aten cat default cat_act_obs = getattr prepare_model node args target cat_act_obs = getattr prepare_model node args target cat_out_obs = getattr prepare_model next iter node users target assertTrue isinstance cat_act_obs ObserverBase assertTrue isinstance cat_act_obs ObserverBase assertTrue isinstance cat_out_obs ObserverBase assertTrue cat_act_obs cat_act_obs assertTrue cat_act_obs cat_out_obs skipIfNoX test_cat_recipe_single_input r Test pattern conv - cat input Since cat has input node they should also same observer m = TestHelperModules Conv dCatSingleInput eval x = torch randn contiguous memory_format=torch channels_last quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config example_inputs = x node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten cat default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default _ prepare_model _ = _test_quantizer m example_inputs quantizer node_occurrence node_list Check Cat has share observer input output node prepare_model graph nodes node op == call_function node target == torch ops aten cat default cat_act_obs = getattr prepare_model node args target cat_out_obs = getattr prepare_model next iter node users target assertTrue isinstance cat_act_obs ObserverBase assertTrue isinstance cat_out_obs ObserverBase assertTrue cat_act_obs cat_out_obs skipIfNoX test_avg_pool d_recipe r Test pattern conv - AvgPool d Since AvgPool d int _in_int _out_op inputs outputs should same observer m = TestHelperModules Conv dAvgPool d eval x = torch randn contiguous memory_format=torch channels_last quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config example_inputs = x node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten avg_pool d default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default _ prepare_model _ = _test_quantizer m example_inputs quantizer node_occurrence node_list node prepare_model graph nodes node op == call_function node target torch ops aten avg_pool d default avgpool_node = node input_obs_of_avgpool = getattr prepare_model avgpool_node args target output_obs_of_avgpool = getattr prepare_model next iter avgpool_node users target node op == call_function node target torch ops aten conv d default conv_node = node output_obs_of_conv = getattr prepare_model next iter conv_node users target assertTrue isinstance input_obs_of_avgpool ObserverBase assertTrue isinstance output_obs_of_avgpool ObserverBase assertTrue isinstance output_obs_of_conv ObserverBase assertTrue input_obs_of_avgpool output_obs_of_avgpool assertTrue input_obs_of_avgpool output_obs_of_conv skipIfNoX test_linear Test pattern single linear X InductorQuantizer override_quantized_engine x torch no_grad use_bias True False m = TestHelperModules SingleLinearModule use_bias eval example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config node_occurrence = one input weight one output torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten linear default _test_quantizer m example_inputs quantizer node_occurrence node_list _test_linear_unary_helper post_op_module post_op_aten post_op_aten_inplace post_op_algo_list=None is_qat=False is_dynamic=False Test pattern linear unary post ops e g relu X InductorQuantizer use_bias_list = True False TODO test inplace add after refactoring export_for_training inplace_list = False post_op_algo_list None post_op_algo_list = None cases = itertools product use_bias_list inplace_list post_op_algo_list override_quantized_engine x torch no_grad use_bias inplace post_op_algo cases inplace post_op_aten_inplace None continue m = TestHelperModules LinearUnaryModule use_bias=use_bias postop=post_op_module inplace_postop=inplace post_op_algo=post_op_algo eval example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config is_qat=is_qat is_dynamic=is_dynamic quantize_per_tensor_op = torch ops quantized_decomposed quantize_per_tensor tensor is_dynamic torch ops quantized_decomposed quantize_per_tensor default dequantize_per_tensor_op = torch ops quantized_decomposed dequantize_per_tensor tensor is_dynamic torch ops quantized_decomposed dequantize_per_tensor default node_occurrence = one input linear quantize_per_tensor_op dequantize_per_tensor_op quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = quantize_per_tensor_op dequantize_per_tensor_op torch ops aten linear default post_op_aten_inplace inplace post_op_aten _test_quantizer m example_inputs quantizer node_occurrence node_list is_qat=is_qat skipIfNoX test_linear_unary aten = torch ops aten _test_linear_unary_helper nn ReLU aten relu default aten relu_ default _test_linear_unary_helper nn LeakyReLU aten leaky_relu default aten leaky_relu_ default _test_linear_unary_helper nn GELU aten gelu default None none tanh skipIfNoX test_linear_unary_qat aten = torch ops aten _test_linear_unary_helper nn ReLU aten relu default aten relu_ default is_qat=True _test_linear_unary_helper nn LeakyReLU aten leaky_relu default aten leaky_relu_ default is_qat=True _test_linear_unary_helper nn GELU aten gelu default None none tanh is_qat=True skipIfNoX test_linear_unary_dynamic aten = torch ops aten _test_linear_unary_helper nn ReLU aten relu default aten relu_ default is_dynamic=True _test_linear_unary_helper nn LeakyReLU aten leaky_relu default aten leaky_relu_ default is_dynamic=True _test_linear_unary_helper nn GELU aten gelu default None none tanh is_dynamic=True skipIfNoX test_linear_unary_dynamic_qat aten = torch ops aten _test_linear_unary_helper nn ReLU aten relu default aten relu_ default is_qat=True is_dynamic=True _test_linear_unary_helper nn LeakyReLU aten leaky_relu default aten leaky_relu_ default is_qat=True is_dynamic=True _test_linear_unary_helper nn GELU aten gelu default None none tanh is_qat=True is_dynamic=True _check_annotation_stat gm expected_stat_dict Check expected annotation statistics ensure annotation correct _check_annotation node annot = node meta get QUANT_ANNOTATION_KEY None annot None False False annot _annotated annot _is_output_of_quantized_pattern node gm graph nodes node target expected_stat_dict keys annotated is_quant_out = _check_annotation node expected_stat_dict node target annotated -= annotated expected_stat_dict node target is_quant_out -= is_quant_out op_stat expected_stat_dict values assert all v == v op_stat values _test_linear_binary_helper is_qat=False is_dynamic=False Test pattern linear binary post ops such add X InductorQuantizer Currently only add binary post op supported linear_pos_list = NodePosType left NodePosType right NodePosType both TODO test inplace add after refactoring export_for_training inplace_add_list = False example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config is_qat=is_qat is_dynamic=is_dynamic quantize_per_tensor_op = torch ops quantized_decomposed quantize_per_tensor tensor is_dynamic torch ops quantized_decomposed quantize_per_tensor default dequantize_per_tensor_op = torch ops quantized_decomposed dequantize_per_tensor tensor is_dynamic torch ops quantized_decomposed dequantize_per_tensor default cases = itertools product linear_pos_list inplace_add_list override_quantized_engine x torch no_grad linear_pos inplace_add cases m = TestHelperModules LinearAddModule inplace_add=inplace_add linear_pos=linear_pos eval linear_pos = NodePosType both node_occurrence = Only one q-dq input linear No q-dq extra input node add quantize_per_tensor_op dequantize_per_tensor_op quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default convert_pt e disables duplicate dequant dynamic quant num_dequant = is_dynamic node_occurrence = One quantize_per_tensor both linear nodes shared Two dequantize_per_tensor two linear nodes No q-dq extra input node add quantize_per_tensor_op dequantize_per_tensor_op num_dequant quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = quantize_per_tensor_op dequantize_per_tensor_op torch ops aten linear default torch ops aten add_ Tensor inplace_add torch ops aten add Tensor fq_m = _test_quantizer m example_inputs quantizer node_occurrence node_list is_qat=is_qat - One linear add fused The other linear quantized alone present aten = torch ops aten add_op = aten add_ Tensor inplace_add aten add Tensor expected_annotation_stat = aten linear default annotated linear_pos == NodePosType both is_quant_out linear_pos == NodePosType both add_op annotated is_quant_out _check_annotation_stat fq_m expected_annotation_stat skipIfNoX test_linear_binary _test_linear_binary_helper skipIfNoX test_linear_binary_qat _test_linear_binary_helper is_qat=True skipIfNoX test_linear_binary_dynamic _test_linear_binary_helper is_dynamic=True skipIfNoX test_linear_binary_dynamic_qat _test_linear_binary_helper is_qat=True is_dynamic=True skipIfNoX test_linear_binary Test Pattern tmp = linear_ x tmp = linear_ tmp tmp + tmp Since linear_ has users we should annotate linear_ binary fusion instead linear_ example_inputs = torch randn TODO test inplace add after refactoring export_for_training inplace_add_list = False is_qat_list = False True is_dynamic_list = False True cases = itertools product inplace_add_list is_qat_list is_dynamic_list override_quantized_engine x torch no_grad inplace_add is_qat is_dynamic cases quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config is_qat=is_qat is_dynamic=is_dynamic m = TestHelperModules LinearAddModule inplace_add=inplace_add eval quantize_per_tensor_op = torch ops quantized_decomposed quantize_per_tensor tensor is_dynamic torch ops quantized_decomposed quantize_per_tensor default dequantize_per_tensor_op = torch ops quantized_decomposed dequantize_per_tensor tensor is_dynamic torch ops quantized_decomposed dequantize_per_tensor default Two q-dq nodes inputs linear nodes No q-dq extra input node add node_occurrence = quantize_per_tensor_op dequantize_per_tensor_op quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed dequantize_per_channel default quantize_per_tensor_op dequantize_per_tensor_op torch ops aten linear default torch ops aten add_ Tensor inplace_add torch ops aten add Tensor fq_m = _test_quantizer m example_inputs quantizer node_occurrence node_list - One linear add fused The other linear quantized alone present aten = torch ops aten add_op = aten add_ Tensor inplace_add aten add Tensor expected_annotation_stat = aten linear default annotated is_quant_out add_op annotated is_quant_out _check_annotation_stat fq_m expected_annotation_stat skipIfNoX _test_linear_binary_unary_helper is_qat=False is_dynamic=False Test pattern linear binary + unary post ops such add + relu X InductorQuantizer Currently only add binary post op relu unary post op supported linear_pos_list = NodePosType left NodePosType right NodePosType both TODO test inplace add after refactoring export_for_training inplace_add_list = False TODO test inplace relu after refactoring export_for_training inplace_relu_list = False example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config is_qat=is_qat is_dynamic=is_dynamic quantize_per_tensor_op = torch ops quantized_decomposed quantize_per_tensor tensor is_dynamic torch ops quantized_decomposed quantize_per_tensor default dequantize_per_tensor_op = torch ops quantized_decomposed dequantize_per_tensor tensor is_dynamic torch ops quantized_decomposed dequantize_per_tensor default cases = itertools product linear_pos_list inplace_add_list inplace_relu_list override_quantized_engine x torch no_grad linear_pos inplace_add inplace_relu cases m = TestHelperModules LinearAddReLUModule inplace_add=inplace_add linear_pos=linear_pos inplace_relu=inplace_relu eval linear_pos = NodePosType both node_occurrence = Only one q-dq node input linear No q-dq node extra input node add quantize_per_tensor_op dequantize_per_tensor_op note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default convert_pt e disables duplicate dequant dynamic quant num_dequant = is_dynamic node_occurrence = One quantize_per_tensor both linear nodes shared Two dequantize_per_tensor two linear nodes No q-dq extra input node add quantize_per_tensor_op dequantize_per_tensor_op num_dequant note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = quantize_per_tensor_op dequantize_per_tensor_op torch ops aten linear default torch ops aten add_ Tensor inplace_add torch ops aten add Tensor fq_m = _test_quantizer m example_inputs quantizer node_occurrence node_list - linear add relu fused The other linear quantized alone present aten = torch ops aten add_op = aten add_ Tensor inplace_add aten add Tensor relu_op = aten relu_ default inplace_relu aten relu default expected_annotation_stat = aten linear default annotated linear_pos == NodePosType both is_quant_out linear_pos == NodePosType both add_op annotated is_quant_out relu_op annotated is_quant_out _check_annotation_stat fq_m expected_annotation_stat skipIfNoX test_linear_binary_unary _test_linear_binary_unary_helper skipIfNoX test_linear_binary_unary_qat _test_linear_binary_unary_helper is_qat=True skipIfNoX test_linear_binary_unary_dynamic _test_linear_binary_unary_helper is_dynamic=True skipIfNoX test_linear_binary_unary_dynamic_qat _test_linear_binary_unary_helper is_qat=True is_dynamic=True skipIfNoX test_linear_binary_unary_serials Test pattern following up linear add relu X InductorQuantizer is_qat_list = False True is_dynamic_list = False True cases = itertools product is_qat_list is_dynamic_list override_quantized_engine x torch no_grad is_qat is_dynamic cases m = TestHelperModules SerialsLinearAddReLUModule eval example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config is_qat=is_qat is_dynamic=is_dynamic quantize_per_tensor_op = torch ops quantized_decomposed quantize_per_tensor tensor is_dynamic torch ops quantized_decomposed quantize_per_tensor default dequantize_per_tensor_op = torch ops quantized_decomposed dequantize_per_tensor tensor is_dynamic torch ops quantized_decomposed dequantize_per_tensor default convert_pt e disables duplicate dequant dynamic quant num_dequant = is_dynamic node_occurrence = quantize_per_tensor linear_ linear_ shared linear_ dequantize_per_tensor each linear No q-dq extra input node add quantize_per_tensor_op dequantize_per_tensor_op num_dequant quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed dequantize_per_channel default quantize_per_tensor_op dequantize_per_tensor_op torch ops aten linear default torch ops aten linear default torch ops aten linear default torch ops aten add Tensor torch ops aten relu default fq_m = _test_quantizer m example_inputs quantizer node_occurrence node_list - Two linear nodes quantized alone The other two fused add relu aten = torch ops aten expected_annotation_stat = aten linear default annotated is_quant_out aten add Tensor annotated is_quant_out aten relu default annotated is_quant_out _check_annotation_stat fq_m expected_annotation_stat skipIfNoX test_linear_dynamic_fp Test pattern linear_dynamic_fp override_quantized_engine x torch no_grad use_bias True False m = TestHelperModules SingleLinearModule use_bias eval example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config quantizer set_module_type_qconfig torch nn Linear xiq get_x _inductor_linear_dynamic_fp _config node_occurrence = convert_element_type nodes inserted weight torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default torch ops quantized_decomposed convert_element_type no_fuse node_list = torch ops quantized_decomposed convert_element_type no_fuse torch ops aten linear default _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfTorchDynamo very slow skipIfNoX test_qat_conv d Test QAT pattern conv d_bn X InductorQuantizer override_quantized_engine x m = TestHelperModules SingleConv dModule with_bn=True example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config is_qat=True node_occurrence = one input weight conv one output conv torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default BN should folded into Conv torch ops aten _native_batch_norm_legit default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default _test_quantizer m example_inputs quantizer node_occurrence node_list is_qat=True skipIfTorchDynamo very slow skipIfNoX test_qat_conv d_unary Test QAT pattern conv d_bn unary post ops such relu sigmoid X InductorQuantizer Currently only relu unary post op supported unary_map = relu torch nn ReLU inplace=False torch ops aten relu default relu_inplace torch nn ReLU inplace=True torch ops aten relu_ default hardtanh torch nn Hardtanh min_val= max_val= inplace=False torch ops aten hardtanh default hardtanh_inplace torch nn Hardtanh min_val= max_val= inplace=True torch ops aten hardtanh_ default relu torch nn ReLU inplace=False torch ops aten hardtanh default relu _inplace torch nn ReLU inplace=True torch ops aten hardtanh_ default hardswish torch nn Hardswish inplace=False torch ops aten hardswish default hardswish_inplace torch nn Hardswish inplace=True torch ops aten hardswish_ default swish torch nn SiLU inplace=False torch ops aten silu default swish_inplace torch nn SiLU inplace=True torch ops aten silu_ default override_quantized_engine x unary_op unary_map keys m = TestHelperModules Conv dUnaryModule unary_map unary_op with_bn=True example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config is_qat=True node_occurrence = one input weight conv one output relu torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default BN should folded into Conv torch ops aten _native_batch_norm_legit default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default unary_map unary_op torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default _test_quantizer m example_inputs quantizer node_occurrence node_list is_qat=True skipIfTorchDynamo very slow skipIfNoX test_qat_conv d_binary Test qat pattern conv d_bn binary post ops such add X InductorQuantizer Currently only add binary post op supported example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config is_qat=True override_quantized_engine x inplace_add True False m = TestHelperModules Conv dAddModule inplace_add=inplace_add with_bn=True node_occurrence = one input weight conv one output add one extra input node add torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default BN should folded into Conv torch ops aten _native_batch_norm_legit default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops aten add_ Tensor inplace_add torch ops aten add Tensor torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default _test_quantizer m example_inputs quantizer node_occurrence node_list is_qat=True skipIfTorchDynamo very slow skipIfNoX test_qat_conv d_binary Test qat Pattern tmp = bn conv d_ x tmp = bn conv d_ tmp tmp + tmp Since conv d_ has users we should annotate conv d_ binary fusion instead conv d_ example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config is_qat=True inplace_add_list = True False override_quantized_engine x torch no_grad inplace_add inplace_add_list m = TestHelperModules Conv dAddModule inplace_add=inplace_add node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default BN should folded into Conv torch ops aten _native_batch_norm_legit default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops quantized_decomposed quantize_per_tensor default torch ops aten add_ Tensor inplace_add torch ops aten add Tensor _test_quantizer m example_inputs quantizer node_occurrence node_list is_qat=True skipIfTorchDynamo very slow skipIfNoX test_qat_conv d_binary_unary Test QAT pattern conv d_bn binary + unary post ops such add + relu X InductorQuantizer Currently only add binary post op relu unary post op supported example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config is_qat=True override_quantized_engine x m = TestHelperModules Conv dAddReLUModule with_bn=True node_occurrence = one input conv one output relu one extra input node add torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default BN should folded into Conv torch ops aten _native_batch_norm_legit default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops aten add Tensor torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default _test_quantizer m example_inputs quantizer node_occurrence node_list is_qat=True skipIfNoX test_dynamic_quant_linear Test pattern dynamic quantization linear X InductorQuantizer override_quantized_engine x torch no_grad m = TestHelperModules SelfAttnLikeModule input_dim= eval example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config is_dynamic=True node_occurrence = torch ops quantized_decomposed choose_qparams tensor torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_tensor tensor quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed choose_qparams tensor torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_tensor tensor torch ops aten linear default _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfNoX test_qat_dynamic_quant_linear Test pattern qat dynamic quantization linear X InductorQuantizer override_quantized_engine x torch no_grad m = TestHelperModules SelfAttnLikeModule input_dim= eval example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config is_qat=True is_dynamic=True node_occurrence = torch ops quantized_decomposed choose_qparams tensor torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_tensor tensor quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed choose_qparams tensor torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_tensor tensor torch ops aten linear default _test_quantizer m example_inputs quantizer node_occurrence node_list is_qat=True skipIfNoX test_set_module_name_qconfig Test case quantizing specific submodule configuring ` set_module_name_qconfig ` Expect all linear layers within submodule ` sub ` quantized Sub torch nn Module __init__ - None super __init__ linear = torch nn Linear relu = torch nn ReLU inplace=False linear = torch nn Linear forward x x = linear x x = relu x x = linear x x M torch nn Module __init__ - None super __init__ linear = torch nn Linear sub = Sub forward x x = linear x x = sub x x m = M eval example_inputs = torch randn Set global ` None ` then default config specific submodule quantizer = X InductorQuantizer quantizer set_module_name_qconfig sub xiq get_default_x _inductor_quantization_config node_occurrence = torch ops aten linear default quantize dequantize input two linear layers ` sub ` torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default dequantize weight two linear layers ` sub ` torch ops quantized_decomposed dequantize_per_channel default node_list = first linear quantized torch ops aten linear default two Q DQ pairs two linear layers ` sub ` torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten linear default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten linear default _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfNoX test_set_module_name_qconfig_with_underscores - None Test module name has underscore we can still quantize M torch nn Module __init__ - None super __init__ This module name has underscores which can part mangled name foo_bar = torch nn Linear baz = torch nn Linear forward x baz foo_bar x Set global no quantization then default config specific submodule whose name includes underscore quantizer = X InductorQuantizer quantizer set_module_name_qconfig foo_bar xiq get_default_x _inductor_quantization_config example_inputs = torch randn m = M eval m = export m example_inputs strict=True module m = prepare_pt e m quantizer Use linear count instead names because names might change order should same count = n m graph nodes n op == call_function n target == torch ops aten linear default Get weight observer see per-channel vs per-tensor weight_observer_node = n args count == foo_bar assertEqual weight_observer_node op call_module f The op linear count s weight_observer_node weight_observer_node op instead call_module observer_instance = getattr m weight_observer_node target assertEqual observer_instance qscheme torch per_channel_symmetric For baz should have no observer all assertNotEqual weight_observer_node op call_module f The op linear count s weight_observer_node weight_observer_node op instead call_module count += skipIfNoX test_set_module_name_and_module_type_case Test set ` module_name_qconfig ` ` module_type_qconfig ` same time Expect all linear layers quantized except last one M torch nn Module __init__ - None super __init__ linear = torch nn Linear linear = torch nn Linear sub = torch nn Linear forward x x = linear x x = linear x x = sub x x m = M eval example_inputs = torch randn Set ` sub ` default config then ` None ` all ` Linear ` The config set ` set_module_name_qconfig ` has higher priority than ` set_module_type_qconfig ` quantizer = X InductorQuantizer quantizer set_module_name_qconfig sub xiq get_default_x _inductor_quantization_config set_module_type_qconfig torch nn Linear None node_occurrence = torch ops aten linear default quantize dequantize input last linear torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default dequantize weight last linear torch ops quantized_decomposed dequantize_per_channel default node_list = first second linear quantized torch ops aten linear default torch ops aten linear default last linear quantized torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten linear default _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfNoX test_set_module_name_and_module_type_case Test set ` module_name_qconfig ` ` module_type_qconfig ` same time Expect all linear layers quantized except last one M torch nn Module __init__ - None super __init__ linear = torch nn Linear linear = torch nn Linear sub = torch nn Linear forward x x = linear x x = linear x x = sub x x m = M eval example_inputs = torch randn Set ` sub ` None then default config all ` Linear ` quantizer = X InductorQuantizer quantizer set_module_name_qconfig sub None set_module_type_qconfig torch nn Linear xiq get_default_x _inductor_quantization_config node_occurrence = torch ops aten linear default quantize dequantize input output first second linear torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default dequantize weight first second linear torch ops quantized_decomposed dequantize_per_channel default node_list = Q DQ first linear torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten linear default Q DQ second linear torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten linear default last linear quantized torch ops aten linear default _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfNoX test_set_module_name_qconfig_for_dynamic_quant Test quantize specific submodule dynamic quantization override_quantized_engine x torch no_grad is_qat False True m = TestHelperModules SelfAttnLikeModule input_dim= eval example_inputs = torch randn only quantize ` q_proj ` ` v_proj ` dynamic_config = xiq get_default_x _inductor_quantization_config is_dynamic=True is_qat=is_qat quantizer = X InductorQuantizer set_module_name_qconfig q_proj dynamic_config set_module_name_qconfig v_proj dynamic_config node_occurrence = quantize dequantize input torch ops quantized_decomposed choose_qparams tensor torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_tensor tensor dequantize weight q_proj v_proj torch ops quantized_decomposed dequantize_per_channel default node_list = quantize dequantize input torch ops quantized_decomposed choose_qparams tensor torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_tensor tensor q_proj torch ops aten linear default k_proj torch ops aten linear default v_proj torch ops aten linear default _test_quantizer m example_inputs quantizer node_occurrence node_list is_qat=is_qat skipIfNoX test_set_module_name_with_mixed_configs Test case setting module names mixed static dynamic QAT non-QAT configurations The config v_proj will always ignored raise warning override_quantized_engine x torch no_grad assertWarns UserWarning context q_is_dynamic v_is_dynamic q_is_qat v_is_qat itertools product False True repeat= q_is_dynamic == v_is_dynamic q_is_qat == v_is_qat continue m = TestHelperModules SelfAttnLikeModule input_dim= eval example_inputs = torch randn quantizer = X InductorQuantizer set_module_name_qconfig q_proj xiq get_default_x _inductor_quantization_config is_qat=q_is_qat is_dynamic=q_is_dynamic set_module_name_qconfig v_proj xiq get_default_x _inductor_quantization_config is_qat=v_is_qat is_dynamic=v_is_dynamic quant_op = torch ops quantized_decomposed quantize_per_tensor tensor q_is_dynamic torch ops quantized_decomposed quantize_per_tensor default dequant_op = torch ops quantized_decomposed dequantize_per_tensor tensor q_is_dynamic torch ops quantized_decomposed dequantize_per_tensor default node_occurrence = quantize dequantize input quant_op dequant_op only ` q_proj ` quantized dequantize its weight torch ops quantized_decomposed dequantize_per_channel default node_list = quantize dequantize input quant_op dequant_op q_proj torch ops aten linear default k_proj v_proj torch ops aten linear default torch ops aten linear default _test_quantizer m example_inputs quantizer node_occurrence node_list is_qat=q_is_qat warning_msg = Mixed QAT Non-QAT q_is_qat = v_is_qat Mixed dynamic static assertTrue any warning_msg msg msg str w message w context warnings skipIfNoX test_set_module_name_and_module_type_with_mixed_configs Test set ` module_name_qconfig ` ` module_type_qconfig ` same time mixed configs Expect only last linear ` sub ` quantized using static quantization M torch nn Module __init__ - None super __init__ linear = torch nn Linear linear = torch nn Linear sub = torch nn Linear forward x x = linear x x = linear x x = sub x x m = M eval example_inputs = torch randn Set ` sub ` static config then dynamic config all ` Linear ` ignored quantizer = X InductorQuantizer quantizer set_module_name_qconfig sub xiq get_default_x _inductor_quantization_config is_dynamic=False set_module_type_qconfig torch nn Linear xiq get_default_x _inductor_quantization_config is_dynamic=True node_occurrence = torch ops aten linear default quantize dequantize input last linear torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default dequantize weight last linear torch ops quantized_decomposed dequantize_per_channel default node_list = first second linear quantized torch ops aten linear default torch ops aten linear default Q DQ pairs last linear torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten linear default _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfNoX test_filter_conv d_recipe Test removing conv d default recipe X InductorQuantizer override_quantized_engine x torch no_grad m = TestHelperModules Conv dUnaryModule torch nn ReLU inplace=False eval example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config quantizer set_module_type_qconfig torch nn Conv d None node_occurrence = one input weight conv torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops aten conv d default torch ops aten relu default _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfNoX test_filter_linear_recipe Test removing linear default recipe X InductorQuantizer override_quantized_engine x torch no_grad m = TestHelperModules LinearUnaryModule use_bias=True postop=nn ReLU eval example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config quantizer set_function_type_qconfig torch nn functional linear None node_occurrence = one input weight conv torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops aten linear default torch ops aten relu default _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfNoX test_filter_maxpool d_recipe Test removing maxpool d default recipe X InductorQuantizer override_quantized_engine x torch no_grad m = TestHelperModules Conv dUnaryModule torch nn ReLU inplace=False eval example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config quantizer set_function_type_qconfig torch nn functional max_pool d None node_occurrence = one input weight conv torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default note quantize op weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten conv d default torch ops aten relu default torch ops aten max_pool d default _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfNoX test_attention_block Test pattern Attention like Block X InductorQuantizer annotate_matmul False True override_quantized_engine x torch no_grad m = TestHelperModules SelfAttnLikeModule input_dim= transpose_for_score=True num_attention_heads= attention_head_size= eval example_inputs = torch randn m example_inputs quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config annotate_matmul quantizer set_function_type_qconfig torch matmul quantizer get_global_quantization_config node_occurrence = torch ops quantized_decomposed quantize_per_tensor default annotate_matmul torch ops quantized_decomposed dequantize_per_tensor default annotate_matmul quantize_per_channel weights const propagated torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel default annotate_matmul node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten linear default torch ops aten view default torch ops aten permute default torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten matmul default torch ops aten div Tensor torch ops aten softmax int node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor default torch ops aten linear default torch ops aten view default torch ops aten permute default torch ops aten matmul default torch ops aten div Tensor torch ops aten softmax int _test_quantizer m example_inputs quantizer node_occurrence node_list skipIfNoX test_lowering_to_x override_quantized_engine x torch no_grad m = TestHelperModules MiniResNet eval example_inputs = torch randn quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config node_occurrence = torch ops quantized_decomposed quantize_per_tensor default torch ops onednn qconv_pointwise default torch ops onednn qconv d_pointwise binary torch ops onednn qlinear_pointwise default node_list = torch ops quantized_decomposed quantize_per_tensor default torch ops onednn qconv_pointwise default torch ops onednn qconv d_pointwise binary torch ops onednn qlinear_pointwise default _test_quantizer m example_inputs quantizer node_occurrence node_list lower=True __name__ == __main__ raise_on_run_directly test test_quantization py