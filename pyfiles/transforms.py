mypy allow-untyped-defs functools math operator weakref collections abc Sequence typing Optional Union torch torch nn functional F torch Tensor torch distributions constraints torch distributions distribution Distribution torch distributions utils _sum_rightmost broadcast_all lazy_property tril_matrix_to_vec vec_to_tril_matrix torch nn functional pad softplus torch types _Number __all__ = AbsTransform AffineTransform CatTransform ComposeTransform CorrCholeskyTransform CumulativeDistributionTransform ExpTransform IndependentTransform LowerCholeskyTransform PositiveDefiniteTransform PowerTransform ReshapeTransform SigmoidTransform SoftplusTransform TanhTransform SoftmaxTransform StackTransform StickBreakingTransform Transform identity_transform Transform Abstract invertable transformations computable log det jacobians They primarily used ` torch distributions TransformedDistribution ` Caching useful transforms whose inverses either expensive numerically unstable Note care must taken memoized values since autograd graph may reversed For example while following works without caching y = t x t log_abs_det_jacobian x y backward x will receive gradients However following will error when caching due dependency reversal y = t x z = t inv y grad z sum y error because z x Derived classes should implement one both meth ` _call ` meth ` _inverse ` Derived classes set ` bijective=True ` should also implement meth ` log_abs_det_jacobian ` Args cache_size int Size cache If zero no caching done If one latest single value cached Only supported Attributes domain ` ~torch distributions constraints Constraint ` The constraint representing valid inputs transform codomain ` ~torch distributions constraints Constraint ` The constraint representing valid outputs transform which inputs inverse transform bijective bool Whether transform bijective A transform ` ` t ` ` bijective iff ` ` t inv t x == x ` ` ` ` t t inv y == y ` ` every ` ` x ` ` domain ` ` y ` ` codomain Transforms bijective should least maintain weaker pseudoinverse properties ` ` t t inv t x == t x ` ` ` ` t inv t t inv y == t inv y ` ` sign int Tensor For bijective univariate transforms should + - depending whether transform monotone increasing decreasing bijective = False domain constraints Constraint codomain constraints Constraint __init__ cache_size int = - None _cache_size = cache_size _inv Optional weakref ReferenceType Transform = None cache_size == pass default behavior cache_size == _cached_x_y = None None raise ValueError cache_size must super __init__ __getstate__ state = __dict__ copy state _inv = None state property event_dim - int domain event_dim == codomain event_dim domain event_dim raise ValueError Please use either domain event_dim codomain event_dim property inv - Transform Returns inverse ` Transform ` transform This should satisfy ` ` t inv inv t ` ` inv = None _inv None inv = _inv inv None inv = _InverseTransform _inv = weakref ref inv inv property sign - int Returns sign determinant Jacobian applicable In general only makes sense bijective transforms raise NotImplementedError with_cache cache_size= _cache_size == cache_size type __init__ Transform __init__ type cache_size=cache_size raise NotImplementedError f type with_cache implemented __eq__ other other __ne__ other Necessary Python __eq__ other __call__ x Computes transform ` x = y ` _cache_size == _call x x_old y_old = _cached_x_y x x_old y_old y = _call x _cached_x_y = x y y _inv_call y Inverts transform ` y = x ` _cache_size == _inverse y x_old y_old = _cached_x_y y y_old x_old x = _inverse y _cached_x_y = x y x _call x Abstract method compute forward transformation raise NotImplementedError _inverse y Abstract method compute inverse transformation raise NotImplementedError log_abs_det_jacobian x y Computes log det jacobian ` log &#124; dy dx &#124; ` given input output raise NotImplementedError __repr__ __class__ __name__ + forward_shape shape Infers shape forward computation given input shape Defaults preserving shape shape inverse_shape shape Infers shapes inverse computation given output shape Defaults preserving shape shape _InverseTransform Transform Inverts single ` Transform ` This private please instead use ` ` Transform inv ` ` property __init__ transform Transform - None super __init__ cache_size=transform _cache_size _inv Transform = transform type ignore assignment constraints dependent_property is_discrete=False pyrefly ignore bad-override domain assert _inv None _inv codomain constraints dependent_property is_discrete=False pyrefly ignore bad-override codomain assert _inv None _inv domain property bijective - bool type ignore override assert _inv None _inv bijective property sign - int assert _inv None _inv sign property inv - Transform _inv with_cache cache_size= assert _inv None inv with_cache cache_size inv __eq__ other isinstance other _InverseTransform False assert _inv None _inv == other _inv __repr__ f __class__ __name__ repr _inv __call__ x assert _inv None _inv _inv_call x log_abs_det_jacobian x y assert _inv None -self _inv log_abs_det_jacobian y x forward_shape shape _inv inverse_shape shape inverse_shape shape _inv forward_shape shape ComposeTransform Transform Composes multiple transforms chain The transforms being composed responsible caching Args parts list ` Transform ` A list transforms compose cache_size int Size cache If zero no caching done If one latest single value cached Only supported __init__ parts list Transform cache_size int = - None cache_size parts = part with_cache cache_size part parts super __init__ cache_size=cache_size parts = parts __eq__ other isinstance other ComposeTransform False parts == other parts constraints dependent_property is_discrete=False pyrefly ignore bad-override domain parts constraints real domain = parts domain Adjust event_dim maximum among all parts event_dim = parts - codomain event_dim part reversed parts event_dim += part domain event_dim - part codomain event_dim event_dim = max event_dim part domain event_dim assert event_dim = domain event_dim event_dim domain event_dim domain = constraints independent domain event_dim - domain event_dim domain constraints dependent_property is_discrete=False pyrefly ignore bad-override codomain parts constraints real codomain = parts - codomain Adjust event_dim maximum among all parts event_dim = parts domain event_dim part parts event_dim += part codomain event_dim - part domain event_dim event_dim = max event_dim part codomain event_dim assert event_dim = codomain event_dim event_dim codomain event_dim codomain = constraints independent codomain event_dim - codomain event_dim codomain lazy_property bijective - bool type ignore override all p bijective p parts lazy_property sign - int type ignore override sign = p parts sign = sign p sign sign property inv - Transform inv = None _inv None inv = _inv inv None inv = ComposeTransform p inv p reversed parts _inv = weakref ref inv inv _inv = weakref ref inv with_cache cache_size= _cache_size == cache_size ComposeTransform parts cache_size=cache_size __call__ x part parts x = part x x log_abs_det_jacobian x y parts torch zeros_like x Compute intermediates This will free parts - all cached xs = x part parts - xs append part xs - xs append y terms = event_dim = domain event_dim part x y zip parts xs - xs terms append _sum_rightmost part log_abs_det_jacobian x y event_dim - part domain event_dim event_dim += part codomain event_dim - part domain event_dim functools reduce operator add terms forward_shape shape part parts shape = part forward_shape shape shape inverse_shape shape part reversed parts shape = part inverse_shape shape shape __repr__ fmt_string = __class__ __name__ + \n fmt_string += \n join p __repr__ p parts fmt_string += \n fmt_string identity_transform = ComposeTransform IndependentTransform Transform Wrapper around another transform treat ` ` reinterpreted_batch_ndims ` ` -many extra right most dimensions dependent This has no effect forward backward transforms does sum out ` ` reinterpreted_batch_ndims ` ` -many rightmost dimensions meth ` log_abs_det_jacobian ` Args base_transform ` Transform ` A base transform reinterpreted_batch_ndims int The number extra rightmost dimensions treat dependent __init__ base_transform Transform reinterpreted_batch_ndims int cache_size int = - None super __init__ cache_size=cache_size base_transform = base_transform with_cache cache_size reinterpreted_batch_ndims = reinterpreted_batch_ndims with_cache cache_size= _cache_size == cache_size IndependentTransform base_transform reinterpreted_batch_ndims cache_size=cache_size constraints dependent_property is_discrete=False pyrefly ignore bad-override domain constraints independent base_transform domain reinterpreted_batch_ndims constraints dependent_property is_discrete=False pyrefly ignore bad-override codomain constraints independent base_transform codomain reinterpreted_batch_ndims property bijective - bool type ignore override base_transform bijective property sign - int base_transform sign _call x x dim domain event_dim raise ValueError Too few dimensions input base_transform x _inverse y y dim codomain event_dim raise ValueError Too few dimensions input base_transform inv y log_abs_det_jacobian x y result = base_transform log_abs_det_jacobian x y result = _sum_rightmost result reinterpreted_batch_ndims result __repr__ f __class__ __name__ repr base_transform reinterpreted_batch_ndims forward_shape shape base_transform forward_shape shape inverse_shape shape base_transform inverse_shape shape ReshapeTransform Transform Unit Jacobian transform reshape rightmost part tensor Note ` ` in_shape ` ` ` ` out_shape ` ` must have same number elements just meth ` torch Tensor reshape ` Arguments in_shape torch Size The input event shape out_shape torch Size The output event shape cache_size int Size cache If zero no caching done If one latest single value cached Only supported Default bijective = True __init__ in_shape torch Size out_shape torch Size cache_size int = - None in_shape = torch Size in_shape out_shape = torch Size out_shape in_shape numel = out_shape numel raise ValueError in_shape out_shape have different numbers elements super __init__ cache_size=cache_size constraints dependent_property pyrefly ignore bad-override domain constraints independent constraints real len in_shape constraints dependent_property pyrefly ignore bad-override codomain constraints independent constraints real len out_shape with_cache cache_size= _cache_size == cache_size ReshapeTransform in_shape out_shape cache_size=cache_size _call x batch_shape = x shape x dim - len in_shape x reshape batch_shape + out_shape _inverse y batch_shape = y shape y dim - len out_shape y reshape batch_shape + in_shape log_abs_det_jacobian x y batch_shape = x shape x dim - len in_shape x new_zeros batch_shape forward_shape shape len shape len in_shape raise ValueError Too few dimensions input cut = len shape - len in_shape shape cut = in_shape raise ValueError f Shape mismatch expected shape cut got in_shape shape cut + out_shape inverse_shape shape len shape len out_shape raise ValueError Too few dimensions input cut = len shape - len out_shape shape cut = out_shape raise ValueError f Shape mismatch expected shape cut got out_shape shape cut + in_shape ExpTransform Transform r Transform via mapping math ` y = \exp x ` domain = constraints real codomain = constraints positive bijective = True sign = + __eq__ other isinstance other ExpTransform _call x x exp _inverse y y log log_abs_det_jacobian x y x PowerTransform Transform r Transform via mapping math ` y = x^ \text exponent ` domain = constraints positive codomain = constraints positive bijective = True __init__ exponent Tensor cache_size int = - None super __init__ cache_size=cache_size exponent = broadcast_all exponent with_cache cache_size= _cache_size == cache_size PowerTransform exponent cache_size=cache_size lazy_property sign - int type ignore override exponent sign type ignore return-value __eq__ other isinstance other PowerTransform False exponent eq other exponent all item _call x x pow exponent _inverse y y pow exponent log_abs_det_jacobian x y exponent y x abs log forward_shape shape torch broadcast_shapes shape getattr exponent shape inverse_shape shape torch broadcast_shapes shape getattr exponent shape _clipped_sigmoid x finfo = torch finfo x dtype torch clamp torch sigmoid x min=finfo tiny max= - finfo eps SigmoidTransform Transform r Transform via mapping math ` y = \frac + \exp -x ` math ` x = \text logit y ` domain = constraints real codomain = constraints unit_interval bijective = True sign = + __eq__ other isinstance other SigmoidTransform _call x _clipped_sigmoid x _inverse y finfo = torch finfo y dtype y = y clamp min=finfo tiny max= - finfo eps y log - -y log p log_abs_det_jacobian x y -F softplus -x - F softplus x SoftplusTransform Transform r Transform via mapping math ` \text Softplus x = \log + \exp x ` The implementation reverts linear function when math ` x ` domain = constraints real codomain = constraints positive bijective = True sign = + __eq__ other isinstance other SoftplusTransform _call x softplus x _inverse y -y expm neg log + y log_abs_det_jacobian x y -softplus -x TanhTransform Transform r Transform via mapping math ` y = \tanh x ` It equivalent code-block python ComposeTransform AffineTransform SigmoidTransform AffineTransform - However might numerically stable thus recommended use ` TanhTransform ` instead Note one should use ` cache_size= ` when comes ` NaN Inf ` values domain = constraints real codomain = constraints interval - bijective = True sign = + __eq__ other isinstance other TanhTransform _call x x tanh _inverse y We do clamp boundary here may degrade performance certain algorithms one should use ` cache_size= ` instead torch atanh y log_abs_det_jacobian x y We use formula more numerically stable see details following link https github com tensorflow probability blob master tensorflow_probability python bijectors tanh py#L -L math log - x - softplus - x AbsTransform Transform r Transform via mapping math ` y = &#124; x &#124; ` domain = constraints real codomain = constraints positive __eq__ other isinstance other AbsTransform _call x x abs _inverse y y AffineTransform Transform r Transform via pointwise affine mapping math ` y = \text loc + \text scale \times x ` Args loc Tensor float Location parameter scale Tensor float Scale parameter event_dim int Optional size ` event_shape ` This should zero univariate random variables distributions over vectors distributions over matrices etc bijective = True __init__ loc Union Tensor float scale Union Tensor float event_dim int = cache_size int = - None super __init__ cache_size=cache_size loc = loc scale = scale _event_dim = event_dim property event_dim - int _event_dim constraints dependent_property is_discrete=False pyrefly ignore bad-override domain event_dim == constraints real constraints independent constraints real event_dim constraints dependent_property is_discrete=False pyrefly ignore bad-override codomain event_dim == constraints real constraints independent constraints real event_dim with_cache cache_size= _cache_size == cache_size AffineTransform loc scale event_dim cache_size=cache_size __eq__ other isinstance other AffineTransform False isinstance loc _Number isinstance other loc _Number loc = other loc False loc == other loc all item type ignore union-attr False isinstance scale _Number isinstance other scale _Number scale = other scale False scale == other scale all item type ignore union-attr False True property sign - Union Tensor int type ignore override isinstance scale _Number float scale - float scale scale sign _call x loc + scale x _inverse y y - loc scale log_abs_det_jacobian x y shape = x shape scale = scale isinstance scale _Number result = torch full_like x math log abs scale result = torch abs scale log event_dim result_size = result size -self event_dim + - result = result view result_size sum - shape = shape -self event_dim result expand shape forward_shape shape torch broadcast_shapes shape getattr loc shape getattr scale shape inverse_shape shape torch broadcast_shapes shape getattr loc shape getattr scale shape CorrCholeskyTransform Transform r Transforms unconstrained real vector math ` x ` length math ` D D- ` into Cholesky factor D-dimension correlation matrix This Cholesky factor lower triangular matrix positive diagonals unit Euclidean norm each row The transform processed follows First we convert x into lower triangular matrix row order For each row math ` X_i ` lower triangular part we apply signed version ` StickBreakingTransform ` transform math ` X_i ` into unit Euclidean length vector using following steps - Scales into interval math ` - ` domain math ` r_i = \tanh X_i ` - Transforms into unsigned domain math ` z_i = r_i^ ` - Applies math ` s_i = StickBreakingTransform z_i ` - Transforms back into signed domain math ` y_i = sign r_i \sqrt s_i ` domain = constraints real_vector codomain = constraints corr_cholesky bijective = True _call x x = torch tanh x eps = torch finfo x dtype eps x = x clamp min=- + eps max= - eps r = vec_to_tril_matrix x diag=- apply stick-breaking squared values Note y = sign r sqrt z z m_cumprod = sign r sqrt z sqrt z m_cumprod = r sqrt z m_cumprod pyrefly ignore unsupported-operation z = r z m_cumprod_sqrt = - z sqrt cumprod - Diagonal elements must r = r + torch eye r shape - dtype=r dtype device=r device y = r pad z m_cumprod_sqrt - value= y _inverse y inverse stick-breaking See https mc-stan org docs _ reference-manual cholesky-factors-of-correlation-matrices- html y_cumsum = - torch cumsum y y dim=- y_cumsum_shifted = pad y_cumsum - value= y_vec = tril_matrix_to_vec y diag=- y_cumsum_vec = tril_matrix_to_vec y_cumsum_shifted diag=- t = y_vec y_cumsum_vec sqrt inverse tanh x = t log p - t neg log p x log_abs_det_jacobian x y intermediates=None Because domain codomain two spaces different dimensions determinant Jacobian well-defined We ` log_abs_det_jacobian ` ` x ` flattened lower triangular part ` y ` See https mc-stan org docs _ reference-manual cholesky-factors-of-correlation-matrices- html y m_cumsum = - y y cumsum dim=- taking diagonal=- we don t need shift z_cumprod right also works x matrix y m_cumsum_tril = tril_matrix_to_vec y m_cumsum diag=- stick_breaking_logdet = y m_cumsum_tril log sum - tanh_logdet = - x + softplus - x - math log sum dim=- stick_breaking_logdet + tanh_logdet forward_shape shape Reshape N D D len shape raise ValueError Too few dimensions input N = shape - D = round + N + D D - = N raise ValueError Input flattened lower-diagonal number shape - + D D inverse_shape shape Reshape D D N len shape raise ValueError Too few dimensions input shape - = shape - raise ValueError Input square D = shape - N = D D - shape - + N SoftmaxTransform Transform r Transform unconstrained space simplex via math ` y = \exp x ` then normalizing This bijective cannot used HMC However acts mostly coordinate-wise except final normalization thus appropriate coordinate-wise optimization algorithms domain = constraints real_vector codomain = constraints simplex __eq__ other isinstance other SoftmaxTransform _call x logprobs = x probs = logprobs - logprobs max - True exp probs probs sum - True _inverse y probs = y probs log forward_shape shape len shape raise ValueError Too few dimensions input shape inverse_shape shape len shape raise ValueError Too few dimensions input shape StickBreakingTransform Transform Transform unconstrained space simplex one additional dimension via stick-breaking process This transform arises iterated sigmoid transform stick-breaking construction ` Dirichlet ` distribution first logit transformed via sigmoid first probability probability everything then process recurses This bijective appropriate use HMC however mixes coordinates together less appropriate optimization domain = constraints real_vector codomain = constraints simplex bijective = True __eq__ other isinstance other StickBreakingTransform _call x offset = x shape - + - x new_ones x shape - cumsum - z = _clipped_sigmoid x - offset log z_cumprod = - z cumprod - y = pad z value= pad z_cumprod value= y _inverse y y_crop = y - offset = y shape - - y new_ones y_crop shape - cumsum - sf = - y_crop cumsum - we clamp make sure sf positive which sometimes does happen when y - ~ y - sum ~ sf = torch clamp sf min=torch finfo y dtype tiny x = y_crop log - sf log + offset log x log_abs_det_jacobian x y offset = x shape - + - x new_ones x shape - cumsum - x = x - offset log use identity - sigmoid x = exp -x sigmoid x detJ = -x + F logsigmoid x + y - log sum - detJ forward_shape shape len shape raise ValueError Too few dimensions input shape - + shape - + inverse_shape shape len shape raise ValueError Too few dimensions input shape - + shape - - LowerCholeskyTransform Transform Transform unconstrained matrices lower-triangular matrices nonnegative diagonal entries This useful parameterizing positive definite matrices terms their Cholesky factorization domain = constraints independent constraints real codomain = constraints lower_cholesky __eq__ other isinstance other LowerCholeskyTransform _call x x tril - + x diagonal dim =- dim =- exp diag_embed _inverse y y tril - + y diagonal dim =- dim =- log diag_embed PositiveDefiniteTransform Transform Transform unconstrained matrices positive-definite matrices domain = constraints independent constraints real codomain = constraints positive_definite __eq__ other isinstance other PositiveDefiniteTransform _call x x = LowerCholeskyTransform x x x mT _inverse y y = torch linalg cholesky y LowerCholeskyTransform inv y CatTransform Transform Transform functor applies sequence transforms ` tseq ` component-wise each submatrix ` dim ` length ` lengths dim ` way compatible func ` torch cat ` Example x = torch cat torch range torch range dim= x = torch cat x x dim= t = CatTransform ExpTransform identity_transform dim= lengths= t = CatTransform t t dim= lengths= y = t x transforms list Transform __init__ tseq Sequence Transform dim int = lengths Optional Sequence int = None cache_size int = - None assert all isinstance t Transform t tseq cache_size tseq = t with_cache cache_size t tseq super __init__ cache_size=cache_size transforms = list tseq lengths None lengths = len transforms lengths = list lengths assert len lengths == len transforms dim = dim lazy_property event_dim - int type ignore override max t event_dim t transforms lazy_property length - int sum lengths with_cache cache_size= _cache_size == cache_size CatTransform transforms dim lengths cache_size _call x assert -x dim = dim x dim assert x size dim == length yslices = start = trans length zip transforms lengths xslice = x narrow dim start length yslices append trans xslice start = start + length avoid += jit compat torch cat yslices dim=self dim _inverse y assert -y dim = dim y dim assert y size dim == length xslices = start = trans length zip transforms lengths yslice = y narrow dim start length xslices append trans inv yslice start = start + length avoid += jit compat torch cat xslices dim=self dim log_abs_det_jacobian x y assert -x dim = dim x dim assert x size dim == length assert -y dim = dim y dim assert y size dim == length logdetjacs = start = trans length zip transforms lengths xslice = x narrow dim start length yslice = y narrow dim start length logdetjac = trans log_abs_det_jacobian xslice yslice trans event_dim event_dim logdetjac = _sum_rightmost logdetjac event_dim - trans event_dim logdetjacs append logdetjac start = start + length avoid += jit compat Decide whether concatenate sum dim = dim dim = dim = dim - x dim dim = dim + event_dim dim torch cat logdetjacs dim=dim sum logdetjacs property bijective - bool type ignore override all t bijective t transforms constraints dependent_property pyrefly ignore bad-override domain constraints cat t domain t transforms dim lengths constraints dependent_property pyrefly ignore bad-override codomain constraints cat t codomain t transforms dim lengths StackTransform Transform Transform functor applies sequence transforms ` tseq ` component-wise each submatrix ` dim ` way compatible func ` torch stack ` Example x = torch stack torch range torch range dim= t = StackTransform ExpTransform identity_transform dim= y = t x transforms list Transform __init__ tseq Sequence Transform dim int = cache_size int = - None assert all isinstance t Transform t tseq cache_size tseq = t with_cache cache_size t tseq super __init__ cache_size=cache_size transforms = list tseq dim = dim with_cache cache_size= _cache_size == cache_size StackTransform transforms dim cache_size _slice z z select dim i i range z size dim _call x assert -x dim = dim x dim assert x size dim == len transforms yslices = xslice trans zip _slice x transforms yslices append trans xslice torch stack yslices dim=self dim _inverse y assert -y dim = dim y dim assert y size dim == len transforms xslices = yslice trans zip _slice y transforms xslices append trans inv yslice torch stack xslices dim=self dim log_abs_det_jacobian x y assert -x dim = dim x dim assert x size dim == len transforms assert -y dim = dim y dim assert y size dim == len transforms logdetjacs = yslices = _slice y xslices = _slice x xslice yslice trans zip xslices yslices transforms logdetjacs append trans log_abs_det_jacobian xslice yslice torch stack logdetjacs dim=self dim property bijective - bool type ignore override all t bijective t transforms constraints dependent_property pyrefly ignore bad-override domain constraints stack t domain t transforms dim constraints dependent_property pyrefly ignore bad-override codomain constraints stack t codomain t transforms dim CumulativeDistributionTransform Transform Transform via cumulative distribution function probability distribution Args distribution Distribution Distribution whose cumulative distribution function use transformation Example Construct Gaussian copula multivariate normal base_dist = MultivariateNormal loc=torch zeros scale_tril=LKJCholesky sample transform = CumulativeDistributionTransform Normal copula = TransformedDistribution base_dist transform bijective = True codomain = constraints unit_interval sign = + __init__ distribution Distribution cache_size int = - None super __init__ cache_size=cache_size distribution = distribution property domain - Optional constraints Constraint type ignore override distribution support _call x distribution cdf x _inverse y distribution icdf y log_abs_det_jacobian x y distribution log_prob x with_cache cache_size= _cache_size == cache_size CumulativeDistributionTransform distribution cache_size=cache_size