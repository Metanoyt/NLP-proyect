Owner s oncall distributed copy sys unittest mock torch torch distributed dist torch nn nn torch _utils _get_device_module torch distributed fsdp BackwardPrefetch CPUOffload MixedPrecision torch distributed fsdp fully_sharded_data_parallel FullyShardedDataParallel FSDP ShardingStrategy torch distributed fsdp wrap ModuleWrapPolicy torch nn parallel DistributedDataParallel DDP torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest get_devtype torch testing _internal common_utils run_tests TEST_CUDA TEST_WITH_DEV_DBG_ASAN device_type = torch device get_devtype dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit LinearUnusedInput nn Linear forward frozen_input learnable_input super forward frozen_input ModelUnusedInput nn Module __init__ freeze bool super __init__ layer = LinearUnusedInput layer _frozen = LinearUnusedInput freeze param layer _frozen parameters param requires_grad = False layer = LinearUnusedInput forward frozen_input learnable_input x = layer frozen_input learnable_input y = layer _frozen frozen_input learnable_input z = layer frozen_input learnable_input torch concat x y z learnable_input TestFSDPFineTune FSDPTest Tests fine-tuning cases where some parameters frozen NUM_LINEARS = property world_size - int min _get_device_module device_type device_count _init_seq_module device - nn Module torch manual_seed modules = _ range NUM_LINEARS modules += nn Linear device=device nn ReLU seq = nn Sequential modules _set_seq_module_requires_grad seq False seq _set_seq_module_requires_grad seq nn Module requires_grad bool Assume linears leaf modules meaning we can pass ` recurse=True ` have work both pre post FSDP wrapping i range NUM_LINEARS Only set every other linear test mixing frozen non-frozen i == param seq i parameters recurse=True param requires_grad = requires_grad skip_if_lt_x_gpu test_backward_reshard_hooks device Tests post-backward reshard happens even flat parameters do require gradients run_subtests device_id device sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP ShardingStrategy NO_SHARD use_orig_params False True inp_requires_grad False True unfreeze_params False True _test_backward_reshard_hooks _test_backward_reshard_hooks device_id sharding_strategy ShardingStrategy use_orig_params bool inp_requires_grad bool unfreeze_params bool seq = _init_seq_module device_type policy = ModuleWrapPolicy nn Linear fsdp_kwargs = device_id device_type seq = FSDP seq auto_wrap_policy=policy sharding_strategy=sharding_strategy use_orig_params=use_orig_params fsdp_kwargs orig_post_backward_reshard = torch distributed fsdp _runtime_utils _post_backward_reshard post_backward_reshard_count = _post_backward_reshard_with_count args kwargs nonlocal post_backward_reshard_count post_backward_reshard_count += orig_post_backward_reshard args kwargs _assert_post_backward_requires_grad seq step_idx == num_steps - unfreeze_params assertTrue all p requires_grad p seq parameters msg= Expected all parameters require grad some did _assert_post_backward_reshard_count step_idx num_steps step_idx num_steps - unfreeze_params If input does require gradient then th frozen linear gets resharded catch-all reshard since we cannot register autograd hook expected_post_backward_reshard_count = NUM_LINEARS inp_requires_grad NUM_LINEARS - This follows normal post-backward hook path expected_post_backward_reshard_count = NUM_LINEARS assertEqual post_backward_reshard_count expected_post_backward_reshard_count mock patch torch distributed fsdp _runtime_utils _post_backward_reshard _post_backward_reshard_with_count num_steps = interleave ` no_grad ` step validate post-backward hooks registered context ` requires_grad ` reset appropriately when unfreezing nograd_step_idx = step_idx range num_steps unfreeze_params step_idx == num_steps - Unfreeze parameters last step emulate some kinds fine-tuning _set_seq_module_requires_grad seq True inp = torch randn device=device_type requires_grad=inp_requires_grad step_idx == nograd_step_idx torch no_grad output = seq inp output = seq inp step_idx = nograd_step_idx output sum backward _assert_post_backward_requires_grad seq _assert_post_backward_reshard_count step_idx num_steps post_backward_reshard_count = _init_multi_traversal_module device - nn Module torch manual_seed TestModule nn Module __init__ - None super __init__ layer_ = nn Linear device=device layer_no_grad = nn Linear device=device layer_with_grad = nn Linear device=device layer_no_grad requires_grad_ False forward x Layer ` layer_no_grad ` ` layer_with_grad ` called multiple times IOW their parameters used multiple times during forward pass x = layer_ x _ range x = layer_no_grad layer_with_grad x Make sure calling same layer multiple times works regardless whether gradient enabled torch no_grad x += layer_with_grad x x TestModule skip_if_lt_x_gpu test_hooks_multi_traversal Tests hooks do reshard unshard correctly case same parameters being used multiple times during forward pass run_subtests sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP ShardingStrategy NO_SHARD use_orig_params False True inp_requires_grad False True forward_prefetch False True _test_hooks_multi_traversal _test_hooks_multi_traversal sharding_strategy ShardingStrategy use_orig_params bool inp_requires_grad bool forward_prefetch bool seq = _init_multi_traversal_module device_type type policy = ModuleWrapPolicy nn Linear fsdp_kwargs = device_id device_type fsdp_seq = FSDP copy deepcopy seq auto_wrap_policy=policy sharding_strategy=sharding_strategy use_orig_params=use_orig_params forward_prefetch=forward_prefetch fsdp_kwargs ddp_seq = DDP copy deepcopy seq device_ids= device_type fsdp_optim = torch optim Adam fsdp_seq parameters lr= e- ddp_optim = torch optim Adam ddp_seq parameters lr= e- torch manual_seed rank + losses = _ range inp = torch randn device=device_type requires_grad=inp_requires_grad seq optim fsdp_seq fsdp_optim ddp_seq ddp_optim loss = seq inp sum losses append loss loss backward optim step optim zero_grad torch testing assert_close losses losses losses clear skip_if_lt_x_gpu test_parity_with_ddp Tests parity DDP when mixing flat parameters require do require gradients run_subtests sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP ShardingStrategy NO_SHARD use_orig_params False True _test_parity_with_ddp _test_parity_with_ddp sharding_strategy ShardingStrategy use_orig_params bool seq = _init_seq_module device_type policy = ModuleWrapPolicy nn Linear fsdp_kwargs = device_id device_type fsdp_seq = FSDP copy deepcopy seq auto_wrap_policy=policy sharding_strategy=sharding_strategy use_orig_params=use_orig_params fsdp_kwargs ddp_seq = DDP copy deepcopy seq device_ids= device_type fsdp_optim = torch optim Adam fsdp_seq parameters lr= e- ddp_optim = torch optim Adam ddp_seq parameters lr= e- torch manual_seed rank + losses = _ range inp = torch randn device=device_type type seq optim fsdp_seq fsdp_optim ddp_seq ddp_optim loss = seq inp sum losses append loss loss backward optim step optim zero_grad TEST_CUDA torch testing assert_close losses losses torch testing assert_close losses losses atol= e- rtol= e- losses clear skip_if_lt_x_gpu test_parity_with_non_frozen_fsdp device For frozen modules unused input reshard could happen without unshard Verify numerical parity between ` _post_backward_reshard_only_hook ` ` _post_backward_hook ` path run_subtests device_id device sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP use_orig_params True False offload_params True False mixed_precision MixedPrecision MixedPrecision param_dtype=torch float buffer_dtype=torch float reduce_dtype=torch float backward_prefetch BackwardPrefetch BACKWARD_PRE BackwardPrefetch BACKWARD_POST _test_parity_with_non_frozen_fsdp _test_parity_with_non_frozen_fsdp device_id sharding_strategy ShardingStrategy use_orig_params bool offload_params bool mixed_precision MixedPrecision backward_prefetch BackwardPrefetch torch manual_seed model = ModelUnusedInput freeze=True device_type torch manual_seed ref_model = ModelUnusedInput freeze=False device_type fsdp_kwargs = device_id device_type auto_wrap_policy ModuleWrapPolicy LinearUnusedInput sharding_strategy sharding_strategy use_orig_params use_orig_params cpu_offload CPUOffload offload_params=offload_params mixed_precision mixed_precision backward_prefetch backward_prefetch model = FSDP model fsdp_kwargs ref_model = FSDP ref_model fsdp_kwargs model_optim = torch optim Adam model parameters lr= e- ref_model_optim = torch optim Adam param name param ref_model named_parameters name startswith _fsdp_wrapped_module layer _frozen lr= e- torch manual_seed rank + losses = _ range frozen_input = torch randn device=device_type requires_grad=False _model _optim model model_optim ref_model ref_model_optim loss = _model frozen_input frozen_input sum losses append loss loss backward _optim step _optim zero_grad assertEqual losses losses losses clear FSDP summon_full_params model FSDP summon_full_params ref_model param ref_param zip model parameters ref_model parameters assertEqual param ref_param devices = cuda hpu xpu instantiate_device_type_tests TestFSDPFineTune globals only_for=devices allow_xpu=True __name__ == __main__ run_tests