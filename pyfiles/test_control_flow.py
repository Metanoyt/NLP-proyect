Owner s module inductor itertools unittest torch torch _dynamo testing torch utils _pytree pytree torch _higher_order_ops associative_scan associative_scan torch _higher_order_ops map _fake_map torch _higher_order_ops scan _fake_scan scan torch _inductor test_case TestCase torch testing _internal common_utils decorateIf instantiate_parametrized_tests parametrize skipIfXpu torch testing _internal inductor_utils GPU_TYPE HAS_CPU HAS_GPU torch testing _internal triton_utils requires_gpu _prepend_product_of_values inputs possible_values num_to_prepend= result = device = inputs device iterate over cartesian product predicate values values itertools product possible_values num_to_prepend prepended = torch tensor v device=device v values result append prepended inputs result prepend_predicates inputs num_predicates= _prepend_product_of_values inputs False True num_predicates prepend_counters inputs num_counters= counter_values= _prepend_product_of_values inputs counter_values num_counters testing loss_fn loss_fn result - torch Tensor flat_results _ = pytree tree_flatten result total_loss = torch tensor device=flat_results device flat_results torch device cpu res flat_results Convert float integer tensor avoid numerical issues res dtype is_floating_point res = res float Simple robust loss abs values + small constant avoid inf nan total_loss = total_loss + torch abs res + torch abs res sum total_loss CondModels Simple torch nn Module forward p b true_fn x y x + y false_fn x y x - y torch cond p true_fn false_fn b SimpleWithIntClosure torch nn Module __init__ super __init__ num = forward p b torch cond pred=p true_fn=lambda b + b + num false_fn=lambda b - b - num operands= b Nested torch nn Module forward p p p b c true_fn x y z true_true_fn x y z x - y z true_false_fn x y z true_false_true_fn x y z x y z true_false_false_fn x y z x + y + z torch cond p true_false_true_fn true_false_false_fn x y z torch cond p true_true_fn true_false_fn x y z false_fn x y z false_true_fn x y z false_true_true_fn x y z x - y - z + false_true_false_fn x y z x y z - torch cond p false_true_true_fn false_true_false_fn x y z false_false_fn x y z x - y z torch cond p false_true_fn false_false_fn x y z torch cond p true_fn false_fn b c Parameters torch nn Module InnerModel torch nn Module __init__ device super __init__ layer = torch nn Linear device=device forward x layer x + InnerModel torch nn Module __init__ device super __init__ layer = torch nn Linear device=device layer = torch nn Linear device=device forward x layer layer x - __init__ device super __init__ true_fn = InnerModel device false_fn = InnerModel device forward p torch cond p true_fn false_fn ReinterpretView torch nn Module forward p b true_fn x y z = x + y z = x - y z z contiguous false_fn x y z = x - y z = x + y z z contiguous torch cond p true_fn false_fn - b - MultipleOutputs torch nn Module forward p b c true_fn x y z x y z y - x sum dim= false_fn x y z y x z x + y mean dim= torch cond p true_fn false_fn b c OuterCode torch nn Module forward p b c = b + d = b - true_fn x y x + y false_fn x y x - y e = torch cond p true_fn false_fn c d e e OuterBuffers torch nn Module forward p b c d = e = b true_fn x x + d false_fn x x - e torch cond p true_fn false_fn c WithNonTensorPredicate torch nn Module forward b true_fn x y x sum false_fn x y y sum torch cond size b size true_fn false_fn b UnbackedSymIntClosure torch nn Module forward p x y z = y shape b = z sum torch int item true_fn x x + false_fn x x + b z torch cond x shape true_fn false_fn x MismatchedOutputSize torch nn Module forward p x y z = y shape b = z shape true_fn x x + sin false_fn x x + b z cos y sum - torch cond x sum true_fn false_fn x FunctionalCall torch nn Module __init__ super __init__ linear = torch nn Linear forward p x true_new_weight = torch ones x size x size device=x device false_new_weight = torch zeros x size x size device=x device true_new_bias = torch ones x size device=x device false_new_bias = torch zeros x size device=x device x = x reshape - x size true_fn x torch func functional_call linear weight true_new_weight bias true_new_bias x false_fn x torch func functional_call linear weight false_new_weight bias false_new_bias x torch cond p true_fn false_fn x SelectWithInputIdx torch nn Module forward p x idx u = idx item x = x select u fn x sin torch cond x sum fn fn CondTests TestCase _run_test model inputs device dynamic=False num_predicates= cnt = torch _dynamo testing CompileCounterWithBackend inductor compiled_model = torch compile backend=cnt fullgraph=True model inputs = inp device=device inp inputs input_sets = inputs dynamic larger_inputs = inp inputs only tile non-scalar tensor inputs inp ndim tile every first dim x tiling = + inp ndim - larger_inputs append torch tile inp tiling larger_inputs append inp input_sets append larger_inputs inputs input_sets inp inputs mark every first dim dynamic torch _dynamo mark_dynamic inp inputs input_sets inputs_with_predicates prepend_predicates inputs num_predicates cloned_inputs = inp clone inp inputs_with_predicates result = model inputs_with_predicates result_compiled = compiled_model inputs_with_predicates inputs must mutated torch testing assert_close cloned_inputs inputs_with_predicates torch testing assert_close result result_compiled assertEqual cnt frame_count only one compilation expected requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic False True test_cond_simple_control_flow device dynamic cond control flow without nesting _run_test model=CondModels Simple inputs= torch randn torch randn device=device dynamic=dynamic requires_gpu parametrize device cpu GPU_TYPE test_cond_simple_with_int_closure device _run_test model=torch compile CondModels SimpleWithIntClosure dynamic=True inputs= torch randn torch randn device=device requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic False True torch _dynamo config patch capture_scalar_outputs True test_cond_unbacked_symint_closure device dynamic _run_test model=CondModels UnbackedSymIntClosure inputs= torch randn torch randn torch randn device=device dynamic=dynamic skipIfXpu msg= Remove skip after issue resolved requires_gpu test_cond_control_flow_with_precomputed_size TestModel torch nn Module __init__ super __init__ conv d = torch nn Conv d kernel_size= stride= padding= threshold = forward x torch Tensor index - torch Tensor true_fn x torch Tensor conv d x false_fn x torch Tensor conv d x torch cond index threshold index = true_fn false_fn x main_model = TestModel GPU_TYPE x = torch rand GPU_TYPE x = torch rand GPU_TYPE opt_model = torch compile main_model out = main_model x opt_out = opt_model x assertTrue torch allclose out opt_out atol= e- out = main_model x opt_out = opt_model x assertTrue torch allclose out opt_out atol= e- requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic False True test_cond_nested_control_flow device dynamic cond control flow nesting _run_test model=CondModels Nested inputs= torch randn torch randn torch randn device=device dynamic=dynamic num_predicates= requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic False True test_cond_outer_code_before_after device dynamic some code before after conditional _run_test model=CondModels OuterCode inputs= torch randn torch randn device=device dynamic=dynamic requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic False True test_cond_multiple_outputs device dynamic multiple outputs different shapes _run_test model=CondModels MultipleOutputs inputs= torch randn torch randn torch randn device=device dynamic=dynamic requires_gpu parametrize device cpu GPU_TYPE test_cond_advanced_dynamic_shapes device subgraphs input shapes include symbolic expressions Model torch nn Module forward p b true_fn x y torch cat x - y dim= false_fn x y torch cat x y - dim= c = torch cat b dim= d = c e = c torch cond p true_fn false_fn d e _run_test model=Model inputs= torch randn torch randn device=device dynamic=True requires_gpu parametrize device cpu GPU_TYPE test_cond_unbacked_symint_outer_to_inner device Model torch nn Module forward p true_fn x torch cos x false_fn x torch sin x nz = torch nonzero b = torch ones nz size device=nz device torch cond p true_fn false_fn b torch _dynamo config patch capture_dynamic_output_shape_ops True _run_test model=Model inputs= torch randn device=device dynamic=True requires_gpu parametrize device cpu GPU_TYPE torch _inductor config patch size_asserts=False TODO graph partition does support creating tensor dynamic shape conditional subgraph yet torch _inductor config patch graph_partition=False test_cond_unbacked_symint_inner device Model torch nn Module forward p true_fn x nz = torch nonzero x b = torch ones nz size device=nz device torch cos b false_fn x nz = torch nonzero x b = torch ones nz size device=nz device torch sin b b = torch sin torch cond p true_fn false_fn b torch _dynamo config patch capture_dynamic_output_shape_ops True _run_test model=Model inputs= torch randn device=device dynamic=True requires_gpu parametrize device cpu GPU_TYPE test_cond_unbacked_symint_inner_to_outer device Model torch nn Module forward p true_fn x nz = torch nonzero x b = torch ones nz size device=nz device torch cos b false_fn x nz = torch nonzero x b = torch ones nz size device=nz device torch sin b b = torch sin y = torch cond p true_fn false_fn b torch sin y torch _dynamo config patch capture_dynamic_output_shape_ops True _run_test model=Model inputs= torch randn device=device dynamic=True requires_gpu test_cond_use_buffers_from_outer_scope subgraphs input shapes include symbolic expressions _run_test model=CondModels OuterBuffers inputs= torch randn torch randn torch randn device=GPU_TYPE dynamic=False requires_gpu test_cond_reintepret_view_inputs_outputs ReinterpretView inputs outputs subgraphs _run_test model=CondModels ReinterpretView inputs= torch randn torch randn device=GPU_TYPE dynamic=True requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic False True test_cond_subgraphs_with_parameters device dynamic nested Modules parameters _run_test model=CondModels Parameters device inputs= torch randn device=device dynamic=dynamic requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic False True test_cond_non_tensor_predicates device dynamic model boolean predicate b_size_ torch _dynamo reset _run_test model=CondModels WithNonTensorPredicate inputs= torch randn torch randn b_size_ device=device dynamic=dynamic num_predicates= requires_gpu test_cond_aliasing_outputs output aliasing subgraphs supported Model torch nn Module forward p b true_fn x y z = x + y z z false_fn x y z = x - y z z torch cond p true_fn false_fn b AssertionError Output aliasing currently supported assertRaises torch _dynamo exc UncapturedHigherOrderOpError torch compile Model torch tensor True torch randn torch randn requires_gpu parametrize device cpu GPU_TYPE test_cond_decompose_ops_in_subgraph device Model torch nn Module forward p true_fn x torch zeros_like x false_fn x torch ones_like x b = torch ones_like c = torch cond p true_fn false_fn b c _run_test model=Model inputs= torch rand device=device requires_gpu parametrize device cpu GPU_TYPE test_cond_decompose_ops_in_subgraph_recursive device inner_fn x torch zeros_like x inner_fn x torch ones_like x Model torch nn Module forward p true_fn x torch cond p inner_fn inner_fn x false_fn x torch cond p inner_fn inner_fn x b = torch ones_like c = torch cond p true_fn false_fn b c _run_test model=Model inputs= torch rand device=device requires_gpu test_cond_inductor_fx_passes_recursively_applied counters = pre_grad post_grad pre_grad_pass_counter gm counters pre_grad += post_grad_pass_counter gm counters post_grad += torch _inductor config patch pre_grad_custom_pass pre_grad_pass_counter post_grad_custom_pre_pass post_grad_pass_counter The above patches don t pickle fx_graph_cache False _run_test model=CondModels Nested inputs= torch randn torch randn torch randn device=GPU_TYPE dynamic=True num_predicates= assertEqual counters pre_grad assertEqual counters post_grad requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False test_cond_mismatched_branch_output_size device dynamic _run_test model=CondModels MismatchedOutputSize inputs= torch randn torch randn torch randn device=device dynamic=dynamic requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False test_cond_functional_call device dynamic _run_test model=CondModels FunctionalCall inputs= torch randn device=device dynamic=dynamic requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False torch _dynamo config patch capture_scalar_outputs True test_cond_select_with_input_idx device dynamic _run_test model=CondModels SelectWithInputIdx inputs= torch randn torch tensor dtype=torch int device=device dynamic=dynamic WhileLoopModels Simple torch nn Module forward ci b cond_fn i x y i body_fn i x y i - x + y y - x torch _higher_order_ops while_loop cond_fn body_fn ci b Nested torch nn Module forward ci cj b cond_fn i j x y i body_fn i j x y cond_fn_nested i j x y j body_fn_nested i j x y i clone j - x + y - i j x y = torch _higher_order_ops while_loop cond_fn_nested body_fn_nested i j x y i - j clone x y torch _higher_order_ops while_loop cond_fn body_fn ci cj b Parameters torch nn Module InnerModel torch nn Module __init__ device super __init__ layer = torch nn Linear device=device dtype=torch float layer = torch nn Linear device=device dtype=torch float forward c x c - layer layer x - __init__ device super __init__ body_fn = InnerModel device cond_fn = lambda c x c forward c torch _higher_order_ops while_loop cond_fn body_fn c OuterCode torch nn Module forward c b d = b + e = b - cond_fn c x y c body_fn c x y c - y - x x + y _ f g = torch _higher_order_ops while_loop cond_fn body_fn c d e f g TODO aakhundov add while_loop test outer buffers dynamic=True once dynamo export allows while_loop closure capture mark_dynamic https github com pytorch pytorch issues OuterBuffers torch nn Module forward c b d = e = b cond_fn c x y c body_fn c x y c - x + d y - e torch _higher_order_ops while_loop cond_fn body_fn c b PytreeCarry torch nn Module forward pytree_input cond_fn pytree_input body_fn pytree_input x = pytree_input y = pytree_input x z = pytree_input y new_x = y sin new_y = z cos new_z = x + - new_x x new_y y new_z torch _higher_order_ops while_loop cond_fn body_fn pytree_input DataDependentOpInSubgraph torch nn Module forward c b cond_fn c reduced_carry c body_fn c reduced_carry k = torch masked_select b d = torch concat k k c - torch min d unsqueeze + reduced_carry torch _higher_order_ops while_loop cond_fn body_fn c torch zeros dtype=torch int device=c device DataDependentInOut torch nn Module forward c b inp = torch zeros sum torch int item device=a device dtype=torch int cond_fn c inp c body_fn c inp c - inp sin + torch int torch _higher_order_ops while_loop cond_fn body_fn c inp DataDependentInOutMismatch torch nn Module forward c b cond_fn c b c body_fn c b c - nonzero b nonzero torch _higher_order_ops while_loop cond_fn body_fn c b InfiniteLoop torch nn Module forward c a_view = view - cond_fn c a_view a_view size - body_fn c a_view c - a_view + torch _higher_order_ops while_loop cond_fn body_fn c a_view ZeroLoop torch nn Module forward c a_view = torch sin view - cond_fn c a_view a_view size - == body_fn c a_view c - a_view + out out = torch _higher_order_ops while_loop cond_fn body_fn c a_view out + out + ZeroLoop torch nn Module forward c a_view = torch sin view - cond_fn c a_view False body_fn c a_view c - a_view + out out = torch _higher_order_ops while_loop cond_fn body_fn c a_view out + out + ZeroLoop torch nn Module forward c a_view = torch sin view - cond_fn c a_view body_fn c a_view c - a_view + out out = torch _higher_order_ops while_loop cond_fn body_fn c a_view out + out + ZeroLoop torch nn Module forward c a_view = torch sin view - cond_fn c a_view torch clip a_view sum body_fn c a_view c - a_view + out out = torch _higher_order_ops while_loop cond_fn body_fn c a_view out sin_ a_view cos_ UnbackedSymIntClosure torch nn Module forward c b d = sum torch int item e = torch nonzero b size cond_fn c b c d + e + shape - b shape body_fn c b c - + e b + d torch _higher_order_ops while_loop cond_fn body_fn c b SymExprCond torch nn Module forward c b d = sum torch int item e = torch nonzero b size cond_fn c b c + d + e + shape - b shape body_fn c b c + + e b + d torch _higher_order_ops while_loop cond_fn body_fn c b MixedDevice torch nn Module forward c b Force loop idx cpu c = c torch device cpu cond_fn loop_idx b loop_idx shape body_fn loop_idx b loop_idx + + b - b torch _higher_order_ops while_loop cond_fn body_fn c b MixedDevice torch nn Module forward c b Force loop idx cpu c torch device cpu cond_fn loop_idx b loop_idx shape body_fn loop_idx b loop_idx + sum + b - b torch _higher_order_ops while_loop cond_fn body_fn c b Conv torch nn Module __init__ device super __init__ conv d = torch nn Conv d stride= padding= device=device dtype=torch float forward c x cond_fn loop_idx x loop_idx x size body_fn loop_idx x loop_idx + conv d x + torch _higher_order_ops while_loop cond_fn body_fn c x WhileLoopStackOutputSimple torch nn Module __init__ device super __init__ linear = torch nn Linear device=device forward c x cond_fn c x c x size body_fn c x c + linear x stacked_c stacked_x = torch ops higher_order while_loop_stack_output cond_fn body_fn c x tuple stacked_c stacked_x WhileLoopTests TestCase _run_test model inputs device dynamic=False num_counters= autograd=False torch utils _pytree pytree cnt = torch _dynamo testing CompileCounterWithBackend inductor copy autograd p model parameters p requires_grad_ False compiled_model = copy deepcopy model compiled_fn = torch compile backend=cnt fullgraph=True compiled_model inputs = pytree tree_map lambda t t device=device inputs input_sets = inputs mark_first_dim_dyn inp torch _dynamo mark_dynamic inp dynamic tile_fn inp tile every first dim x tiling = + inp ndim - t = torch tile inp tiling t larger_inputs = pytree tree_map tile_fn inputs input_sets append larger_inputs inputs input_sets flat_inputs inp_spec = pytree tree_flatten inputs flat_inputs_with_counters prepend_counters flat_inputs num_counters counters flat = flat_inputs_with_counters num_counters flat_inputs_with_counters num_counters unflat_inputs = pytree tree_unflatten flat inp_spec inputs_with_counters = counters + unflat_inputs process_inputs inp inp = inp clone dynamic mark_first_dim_dyn inp autograd inp dtype is_floating_point inp requires_grad_ True inp cloned_inputs = pytree tree_map process_inputs inputs_with_counters cloned_inputs = pytree tree_map process_inputs inputs_with_counters result = model cloned_inputs result_compiled = compiled_fn cloned_inputs inputs must mutated torch testing assert_close cloned_inputs inputs_with_counters torch testing assert_close result result_compiled atol= e- rtol= e- autograd any pytree tree_map_only torch Tensor lambda t t requires_grad cloned_inputs result_loss = loss_fn pytree tree_flatten result compiled_loss = loss_fn pytree tree_flatten result_compiled assertTrue torch isnan result_loss torch isinf compiled_loss assertTrue torch isnan compiled_loss torch isinf compiled_loss assertEqual result_loss compiled_loss result_loss backward compiled_loss backward model_parameters = dict model named_parameters compiled_parameters = dict compiled_model named_parameters name param model_parameters items assertEqual param compiled_parameters name assertEqual param grad compiled_parameters name grad atol= e- rtol= e- inp inp zip pytree tree_flatten cloned_inputs pytree tree_flatten cloned_inputs inp requires_grad assertEqual inp grad inp grad atol= e- rtol= e- assertEqual cnt frame_count only one compilation expected requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic False True parametrize autograd False True torch _dynamo config patch capture_scalar_outputs True test_while_loop_simple_control_flow device dynamic autograd while_loop control flow without nesting _run_test model=WhileLoopModels Simple inputs= torch randn torch randn device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic False True parametrize autograd False True torch _dynamo config patch capture_scalar_outputs True test_while_loop_nested_control_flow device dynamic autograd while_loop control flow nesting _run_test model=WhileLoopModels Nested inputs= torch randn torch randn device=device dynamic=dynamic num_counters= autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic False True parametrize autograd False True torch _dynamo config patch capture_scalar_outputs True test_while_loop_with_outer_code device dynamic autograd while_loop control flow outer code _run_test model=WhileLoopModels OuterCode inputs= torch randn torch randn device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic False True parametrize autograd False True torch _dynamo config patch capture_scalar_outputs True test_while_loop_with_parameters device dynamic autograd while_loop control flow parameters _run_test model=WhileLoopModels Parameters device inputs= torch randn dtype=torch float device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE dynamic=True doesn t work now due https github com pytorch pytorch issues parametrize dynamic False parametrize autograd False True torch _dynamo config patch capture_scalar_outputs True test_while_loop_with_outer_buffers device dynamic autograd while_loop control flow outer code _run_test model=WhileLoopModels OuterBuffers inputs= torch randn torch randn device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize autograd False True torch _dynamo config patch capture_scalar_outputs True test_while_loop_with_pytree_inputs device dynamic autograd _run_test model=WhileLoopModels PytreeCarry inputs= torch randn x torch randn y torch randn device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize autograd False True torch _dynamo config patch capture_scalar_outputs True test_while_loop_with_data_dependent_ops device dynamic autograd torch _dynamo config patch capture_dynamic_output_shape_ops True _run_test model=WhileLoopModels DataDependentOpInSubgraph inputs= torch tensor torch tensor True True True True True device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize autograd False True torch _dynamo config patch capture_scalar_outputs True test_while_loop_with_data_dependent_in_out device dynamic autograd torch _dynamo config patch capture_dynamic_output_shape_ops True capture_scalar_outputs True _run_test model=WhileLoopModels DataDependentInOut inputs= torch tensor torch tensor True True True True True device=device dynamic=dynamic autograd=autograd parametrize dynamic True False test_while_loop_with_data_dependent_in_out_mismatch dynamic assertRaisesRegex torch _dynamo exc UncapturedHigherOrderOpError Expected body_fn_output carried_inputs have same metadata found torch _dynamo config patch capture_dynamic_output_shape_ops True _run_test model=WhileLoopModels DataDependentInOutMismatch inputs= torch tensor torch tensor True True True True True device= cpu dynamic=dynamic test_while_loop_infinite_loop_error assertRaisesRegex torch _dynamo exc UncapturedHigherOrderOpError while_loop doesn t work unless captured completely _run_test model=WhileLoopModels InfiniteLoop inputs= torch tensor device= cpu dynamic=False requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False test_while_loop_zero_loop device dynamic model WhileLoopModels ZeroLoop WhileLoopModels ZeroLoop WhileLoopModels ZeroLoop WhileLoopModels ZeroLoop _run_test model=model inputs= torch tensor device=device dynamic=dynamic requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False torch _dynamo config patch capture_scalar_outputs True capture_dynamic_output_shape_ops True parametrize autograd False True test_while_loop_with_unbacked_symint_closure device dynamic autograd _run_test model=WhileLoopModels UnbackedSymIntClosure inputs= torch randn torch randn device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device GPU_TYPE test_while_loop_models_with_mixed_device device _run_test model=WhileLoopModels MixedDevice inputs= torch randn torch randn device=device dynamic=True assertRaisesRegex torch _dynamo exc UncapturedHigherOrderOpError Expected body_fn_output carried_inputs have same metadata found Error front end because device promoted different one after first iteration _run_test model=WhileLoopModels MixedDevice inputs= torch randn torch randn device=device dynamic=True requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize autograd False True torch _dynamo config patch capture_scalar_outputs True capture_dynamic_output_shape_ops True test_while_loop_with_sym_expr_cond device dynamic autograd _run_test model=WhileLoopModels SymExprCond inputs= torch randn torch randn device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize autograd False True torch _dynamo config patch capture_scalar_outputs True test_while_loop_with_conv device dynamic autograd _run_test model=WhileLoopModels Conv device inputs= torch randn dtype=torch float device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False torch _dynamo config patch capture_scalar_outputs True test_while_loop_stack_output_simple device dynamic _run_test model=WhileLoopModels WhileLoopStackOutputSimple device inputs= torch randn dtype=torch float device=device dynamic=dynamic AssociativeScanTests TestCase requires_gpu parametrize combine_mode pointwise generic parametrize backend inductor parametrize device torch device cpu GPU_TYPE This test will fail flip combination particular input lengths produces weird results This under investigations https github com pytorch pytorch issues decorateIf unittest skip lambda params params device == GPU_TYPE test_associative_scan_CUDA_flip combine_mode backend device fct x torch Tensor y torch Tensor x + y n range n x = torch arange n device=device torch compiler reset associative_scan = torch compile associative_scan backend=backend fullgraph=True associative_scan = associative_scan combine_mode == pointwise device == torch device cpu assertRaisesRegex Exception r associative_scan fct x reverse=False combine_mode=combine_mode Skipping test because combine_mode currently only supports CUDA tensors result = associative_scan fct x reverse=False combine_mode=combine_mode result = associative_scan fct x reverse=False combine_mode=combine_mode result = torch cumsum x assertEqual result result assertEqual result result Flip only non-compiled compare compiled reverse=True result = associative_scan fct x reverse=True combine_mode=combine_mode result = torch flip associative_scan fct torch flip x reverse=False combine_mode=combine_mode result = torch flip torch cumsum torch flip x assertEqual result result assertEqual result result Flip only compiled compare non-compiled reverse=True result = torch flip associative_scan fct torch flip x reverse=False combine_mode=combine_mode result = associative_scan fct x reverse=True combine_mode=combine_mode result = torch flip torch cumsum torch flip x assertEqual result result assertEqual result result Use reverse=False flip both results before after result = torch flip associative_scan fct torch flip x reverse=False combine_mode=combine_mode result = torch flip associative_scan fct torch flip x reverse=False combine_mode=combine_mode result = torch flip torch cumsum torch flip x assertEqual result result assertEqual result result Reverse=True result = associative_scan fct x reverse=True combine_mode=combine_mode result = associative_scan fct x reverse=True combine_mode=combine_mode result = torch flip torch cumsum torch flip x assertEqual result result assertEqual result result ScanModels SimpleScan torch nn Module __init__ reverse dim super __init__ reverse = reverse dim = dim forward _input weight bias combine_fn carry x torch utils _pytree pytree new_carry = param carry param x + carry bias bias carry bias sin new_carry pytree tree_map lambda x x clone new_carry dummy x sin scan combine_fn param weight bias bias _input reverse=self reverse dim=self dim ScanLinearWithView torch nn Module __init__ reverse dim super __init__ reverse = reverse dim = dim linear = torch nn Linear dtype=torch float forward scan_op init xs combine_fn carry x prev_sz = x size x = linear x view - x size - x_view = x view prev_sz x_view x_view clone scan_op combine_fn init xs dim=self dim reverse=self reverse ScanConv torch nn Module __init__ reverse dim super __init__ reverse = reverse dim = dim conv d = torch nn Conv d stride= padding= dtype=torch float init = torch randn xs = torch randn scan_dim forward scan_op init xs combine_fn carry x x = conv d x x x clone scan_op combine_fn init xs dim=self dim reverse=self reverse ScanInCond torch nn Module __init__ reverse dim super __init__ true_scan_linear = ScanModels ScanLinearWithView reverse dim false_scan_linear = ScanModels ScanLinearWithView reverse dim forward scan_op pred init xs true_fn last_carry y = true_scan_linear scan_op init xs last_carry sum y sin false_fn last_carry y = false_scan_linear scan_op init xs -last_carry sum y cos torch cond pred true_fn false_fn tuple CondInScan torch nn Module __init__ reverse dim super __init__ reverse = reverse dim = dim true_linear = torch nn Linear false_linear = torch nn Linear forward scan_op init xs combine_fn carry x old_sizes = carry size carry_view = carry view - carry size - new_carry_out = torch cond torch all carry_view lambda true_linear carry_view sin lambda false_linear carry_view cos tuple carry + new_carry_out view old_sizes new_carry_out scan_op combine_fn init xs dim=self dim reverse=self reverse SimpleWithPytreeInOuts torch nn Module __init__ reverse dim super __init__ reverse = reverse dim = dim forward scan_op _input weight bias combine_fn carry x new_carry = param carry param x + carry bias bias carry bias sin new_carry pytree tree_map lambda x x clone new_carry dummy x sin scan_op combine_fn param weight bias bias _input reverse=self reverse dim=self dim ChunkedCE torch nn Module __init__ chunk_size super __init__ chunk_size = chunk_size ce = lambda logits target torch abs target - logits sum forward scan_op _input weight target bias CHUNK_SIZE = chunk_size compute_loss input_chunk weight bias target logits = torch addmm bias input_chunk weight t logits = logits float loss = ce logits target loss grad_weight = torch zeros_like weight grad_bias = torch zeros_like bias loss_acc = torch zeros device=_input device chunks = _input shape CHUNK_SIZE _input_chunks = _input view CHUNK_SIZE chunks _input shape target_chunks = target view CHUNK_SIZE chunks target shape combine_fn carry xs grad_weight grad_bias loss_acc = carry input_chunk target_chunk = xs chunk_grad_input chunk_grad_weight chunk_grad_bias chunk_loss = torch func grad_and_value compute_loss argnums= input_chunk weight bias target_chunk grad_weight + chunk_grad_weight grad_bias + chunk_grad_bias loss_acc + chunk_loss chunk_grad_input grad_weight grad_bias loss_acc grad_inputs = scan_op combine_fn grad_weight grad_bias loss_acc _input_chunks target_chunks grad_weight chunks grad_bias chunks loss_acc chunks grad_inputs view - _input shape chunks ChunkedCENoScan torch nn Module __init__ chunk_size super __init__ chunk_size = chunk_size ce = lambda logits target torch abs target - logits sum forward scan_op _input weight target bias CHUNK_SIZE = chunk_size compute_loss input_chunk weight bias target logits = torch addmm bias input_chunk weight t logits = logits float loss = ce logits target loss grad_weight = torch zeros_like weight grad_inputs = grad_bias = torch zeros_like bias loss_acc = torch zeros device=_input device chunks = _input shape CHUNK_SIZE accumulate_chunk input_chunk target_chunk chunk_grad_input chunk_grad_weight chunk_grad_bias chunk_loss = torch func grad_and_value compute_loss argnums= input_chunk weight bias target_chunk grad_weight add_ chunk_grad_weight grad_bias add_ chunk_grad_bias loss_acc add_ chunk_loss chunk_grad_input accumulate_chunk = torch compile accumulate_chunk input_chunks = torch chunk _input chunks=chunks dim= target_chunks = torch chunk target chunks=chunks dim= input_chunk target_chunk zip input_chunks target_chunks grad_inputs append accumulate_chunk input_chunk target_chunk grad_weight chunks grad_bias chunks loss_acc chunks torch cat grad_inputs dim= chunks ScanWithClamp torch nn Module __init__ super __init__ forward scan_op initial xs step h_prev x_t h_next = h_prev + x_t clamp min= h_next h_next clone final ys = scan_op step initial xs final ys ScanTests TestCase _run_test model inputs device dynamic autograd=False copy inputs = inp requires_grad_ autograd inp dtype is_floating_point inp inp inputs inputs = inp device=device inp inputs model = model device=device p model parameters p requires_grad_ autograd model = copy deepcopy model model = copy deepcopy model model = copy deepcopy model model = copy deepcopy model model compile fullgraph=True dynamic=dynamic model compile fullgraph=True dynamic=dynamic _run_model model inputs cloned_inputs = inp clone isinstance inp torch Tensor inp inp inputs fw_result = model cloned_inputs loss = loss_fn fw_result autograd loss backward fw_result loss inp grad inp cloned_inputs isinstance inp torch Tensor n p grad n p model named_parameters fw_result loss result_exp = _run_model model _fake_scan + inputs result_eager = _run_model model scan + inputs result_compiled = _run_model model scan + inputs result_compiled_exp = _run_model model _fake_scan + inputs assertEqual result_exp result_eager assertEqual result_exp result_compiled assertEqual result_exp result_compiled_exp _compare_result model model inputs device inp_on_device = elem device=device elem inputs cloned_inputs = arg clone arg inp_on_device model _out = model scan cloned_inputs model _out = model scan cloned_inputs assertEqual model _out model _out requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize reverse True False parametrize dim parametrize autograd True False torch _dynamo config patch capture_scalar_outputs True test_scan_pytree_in_out device dynamic reverse dim autograd _run_test model=ScanModels SimpleWithPytreeInOuts reverse=reverse dim=dim inputs= torch ones torch ones torch ones device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize reverse True False parametrize dim parametrize scan_length parametrize autograd True False torch _dynamo config patch capture_scalar_outputs True test_scan_nn_modules device dynamic reverse dim scan_length autograd init = torch randn dtype=torch float xs = torch randn scan_length dtype=torch float xs = xs movedim dim _run_test model=ScanModels ScanLinearWithView reverse=reverse dim=dim inputs= init xs device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize reverse True False parametrize dim parametrize scan_length parametrize autograd True False torch _dynamo config patch capture_scalar_outputs True test_scan_conv device dynamic reverse dim scan_length autograd init = torch randn dtype=torch float xs = torch randn scan_length dtype=torch float xs = xs movedim dim _run_test model=ScanModels ScanConv reverse=reverse dim=dim inputs= init xs device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize reverse True False parametrize dim parametrize pred True False parametrize scan_length parametrize autograd True False torch _dynamo config patch capture_scalar_outputs True test_scan_in_cond device dynamic reverse dim pred scan_length autograd init = torch randn dtype=torch float xs = torch randn scan_length dtype=torch float xs = xs movedim dim _run_test model=ScanModels ScanInCond reverse=reverse dim=dim inputs= torch tensor pred init xs device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize reverse True False parametrize dim parametrize scan_length parametrize autograd True False torch _dynamo config patch capture_scalar_outputs True test_cond_in_scan device dynamic reverse dim scan_length autograd init = torch randn xs = torch randn scan_length xs = xs movedim dim _run_test model=ScanModels CondInScan reverse=reverse dim=dim inputs= init xs device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize autograd True False torch _dynamo config patch capture_scalar_outputs True test_scan_chunked_ce device dynamic autograd _run_test model=ScanModels ChunkedCE inputs= torch randn torch randn torch randn torch randn device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False torch _dynamo config patch capture_scalar_outputs True test_scan_compare_chunked_ce_with_no_scan device dynamic trunk_size B T zip _compare_result model =torch compile ScanModels ChunkedCE trunk_size dynamic=dynamic model =ScanModels ChunkedCENoScan trunk_size inputs= torch randn B T torch randn T T torch randn B T torch randn T device=device requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize autograd True False torch _dynamo config patch capture_scalar_outputs True test_scan_with_clamp device dynamic autograd B = T = H = _run_test model=ScanModels ScanWithClamp inputs= torch randn B H torch randn T B H device=device dynamic=dynamic autograd=autograd MapModels Simple torch nn Module forward map_op x = torch ones device=x device f x x sin + map_op f x SimpleWithLinearWithView torch nn Module __init__ super __init__ linear = torch nn Linear forward map_op x f x linear x sin map_op f x view PytreeInOut torch nn Module __init__ super __init__ linear = torch nn Linear forward map_op x y z f x_y_z x = x_y_z x y z = x_y_z y_z linear x sin linear y z cos map_op f x x y_z y z ReinterpretView torch nn Module forward map_op x y z f xyz x y z = xyz x sin y cos + z - clone map_op f x y z NestedWithCond torch nn Module forward map_op x y z true_fn x y z inner_f yz y z = yz y + z map_op inner_f y z false_fn x y z inner_f yz y z = yz y - z map_op inner_f y z torch _higher_order_ops cond x sum true_fn false_fn x y z MapTests TestCase _run_test model inputs device dynamic=False autograd=False copy inputs = inp device=device inp inputs model = model device=device model_eager = copy deepcopy model model_compiled = copy deepcopy model cnt = torch _dynamo testing CompileCounterWithBackend inductor compiled_model = torch compile backend=cnt fullgraph=True dynamic=dynamic model_compiled autograd pytree tree_map_only torch Tensor lambda t t requires_grad_ True inputs cloned_inputs = inp clone inp inputs result = model torch _higher_order_ops map cloned_inputs result_exp = model_eager _fake_map cloned_inputs result_compiled = compiled_model torch _higher_order_ops map cloned_inputs assertEqual result result_exp assertEqual result result_compiled autograd loss_fn result backward loss_fn result_exp backward loss_fn result_compiled backward model_params = dict model named_parameters model_eager_params = dict model_eager named_parameters model_compiled_params = dict model_compiled named_parameters name param model_eager_params items assertEqual param model_params name assertEqual param model_compiled_params name assertEqual param grad model_params name grad assertEqual param grad model_compiled_params name grad requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize autograd True False torch _dynamo config patch capture_scalar_outputs True test_map_simple device dynamic autograd _run_test model=MapModels Simple inputs= torch randn device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize autograd True False torch _dynamo config patch capture_scalar_outputs True test_map_simple_linear_with_view device dynamic autograd _run_test model=MapModels SimpleWithLinearWithView inputs= torch randn device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize autograd True False torch _dynamo config patch capture_scalar_outputs True test_map_pytree_in_out device dynamic autograd _run_test model=MapModels PytreeInOut inputs= torch randn torch randn torch randn device=device dynamic=dynamic autograd=autograd requires_gpu parametrize device cpu GPU_TYPE parametrize dynamic True False parametrize autograd True False torch _dynamo config patch capture_scalar_outputs True test_map_nested_with_cond device dynamic autograd _run_test model=MapModels NestedWithCond inputs= torch randn torch randn torch randn device=device dynamic=dynamic autograd=autograd instantiate_parametrized_tests CondTests instantiate_parametrized_tests WhileLoopTests instantiate_parametrized_tests AssociativeScanTests instantiate_parametrized_tests ScanTests instantiate_parametrized_tests MapTests __name__ == __main__ torch _inductor test_case run_tests HAS_CPU HAS_GPU run_tests needs= filelock