mypy ignore-errors torch torch Tensor itertools torch utils _python_dispatch TorchDispatchMode torch utils _pytree tree_map tree_flatten tree_unflatten torch utils _pytree pytree functools partial torch utils _mode_utils no_dispatch all_same_mode torch autograd forward_ad fwAD collections abc Callable re check_attr_consistency wrapper_tensor metadata_name metadata_accessor elem = wrapper_tensor elem metadata_wrapper_tensor = metadata_accessor wrapper_tensor metadata_elem = metadata_accessor elem metadata_wrapper_tensor == metadata_elem raise RuntimeError f This operator Composite Compliant f metadata_name tensor modified directly without f going through PyTorch dispatcher check_metadata_consistency wrapper_tensor CCT CCT CompositeCompliantTensor which generated using generate_cct isinstance wrapper_tensor CCT things_to_check = shape Tensor size dtype lambda x x dtype device lambda x x device numel Tensor numel stride Tensor stride storage_offset Tensor storage_offset metadata_name metadata_accessor things_to_check items check_attr_consistency wrapper_tensor metadata_name metadata_accessor is_view_fn func func overloadpacket __name__ as_strided detach diagonal expand expand_as movedim narrow permute select squeeze transpose t real imag view_as_real view_as_complex unflatten unfold unsqueeze view view_as unbind split split_with_sizes vsplit hsplit tensor_split chunk swapaxes slice _reshape_alias _unsafe_view _conj alias manually populated native_functions have inplace_view True In future we will probably able grab list directly is_inplace_view_fn func func overloadpacket __name__ as_strided_ detach_ squeeze_ swapaxes_ swapdims_ t_ transpose_ unsqueeze_ Introspection please save us is_inplace func name = func overloadpacket __name__ re match __i +__ name True re match __ +__ name False name - == _ generate_cct_and_mode autograd_view_consistency=True This function returns new CompositeCompliantTensor The two arguments control behaviour described below autograd_view_consistency If True alias result using ` set_ ` func returns view See Note Alias Result Since Forward AD doesn t work ` set_ ` we disable setting alias False CompositeCompliantTensor torch Tensor elem torch Tensor __slots__ = elem staticmethod __new__ cls elem mode args kwargs assert type elem cls \ Wrapping CompositeCompliantTensor CompositeCompliantTensor supported The storage CompositeCompliantTensor should never used directly Composite operation Composite operator attempts read storage without dispatching then ll raise RuntimeError due being meta storage r = torch Tensor _make_wrapper_subclass cls elem size dtype=elem dtype layout=elem layout device=elem device requires_grad=elem requires_grad strides=elem stride storage_offset=elem storage_offset elem requires_grad CompositeCompliantTensor steals requires_grad -ness Why new copy ` elem ` Because sometimes OpInfo shares inputs between tests tmp = torch empty dtype=elem dtype device=elem device layout=elem layout requires_grad=False Use set_ rather than empty_strided + copy_ so we can preserve things like storage_offset tmp set_ source=elem untyped_storage clone storage_offset=elem storage_offset size=elem size stride=elem stride r elem = tmp r elem = elem assert r stride == r elem stride Propagate conjugate bits wrapper tensor Ref https github com albanD subclass_zoo issues Ref https github com albanD subclass_zoo issues torch _C _set_conj r r elem is_conj torch _C _set_neg r r elem is_neg r mode = mode r __repr__ f CompositeCompliantTensor elem classmethod __torch_dispatch__ cls func types args= kwargs=None all_args = pytree arg_tree_leaves args kwargs modes = tuple e mode e all_args isinstance e CompositeCompliantTensor all_same_mode modes raise RuntimeError Multiple CompositeCompliantTensorModes NYI modes func args kwargs CompositeCompliantTensorMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None unwrap e e elem isinstance e CompositeCompliantTensor e wrap e CompositeCompliantTensor e isinstance e torch Tensor e func torch ops aten _local_scalar_dense default raise RuntimeError item allowed called inside composite functions PyTorch library because all backends Tensor subclasses e g vmap ProxyTensor support them func overloadpacket __name__ set_ resize_ raise RuntimeError f func __name__ allowed called inside f Composite operators is_inplace func NB We making assumption function in-place then first argument being written Introspection please save us mutated_argument = args isinstance mutated_argument CompositeCompliantTensor \ any isinstance CompositeCompliantTensor args raise RuntimeError Not composite compliant performing in-place operation f func __name__ where Tensor being written regular Tensor other tensors Tensor Subclasses Please try avoid in-place operation unwrapped_args = tree_map unwrap args unwrapped_kwargs = tree_map unwrap kwargs unwrapped_rs = func unwrapped_args unwrapped_kwargs rs = tree_map wrap unwrapped_rs is_view_fn func autograd_view_consistency Note Alias Result Autograd asserts B = A view_fn B A s storages same Here we try make B alias A avoid those asserts See https github com pytorch pytorch issues more information about issue no_dispatch Idea weird way getting storage aliases input This workaround under no_dispatch all wrapper tensors look like regular tensors special storage storage nullptr advertises CPU CUDA device we run func which ends up running view operation All view operations reuse input s storage result Tensor s new sizes strides offset alias input we set storage sizes strides offset wrapper tensor results tensors alias input result = func args kwargs isinstance result tuple list b zip rs result strict=True set_ b rs set_ result Some operations allowed in-place modify metadata inputs The only ones inplace view functions when we run into these we manually modify metadata input no_dispatch is_inplace_view_fn func func args kwargs For each CompositeCompliantTensor t we check t t elem have consistent metadata If they don t have consistent metadata means operator did something fishy check = partial check_metadata_consistency CCT=CompositeCompliantTensor pytree tree_map_ check args pytree tree_map_ check kwargs pytree tree_map_ check rs rs CompositeCompliantTensor CompositeCompliantTensorMode is_tensorlist lst isinstance lst list isinstance lst tuple False len lst == False all_tensors = all isinstance elt torch Tensor elt lst all_tensors True exists_one_tensor = all isinstance elt torch Tensor elt lst exists_one_tensor raise RuntimeError This test assumes PyTorch APIs cannot take mixed lists Tensor other things False maybe_map fn should_map arg fn arg should_map arg wrap arg CCT cct_mode CCT CompositeCompliantTensor which generated using generate_cct_and_mode isinstance arg torch Tensor CCT arg cct_mode is_tensorlist arg CCT cct_mode arg raise RuntimeError wrap assumes input can wrapped Given list flat arguments some which may Tensors all possible ways some arguments could CompositeCompliantTensors CCT For example given Tensors A B C flat_args = A B We would following options CCT A CCT B CCT A B A CCT B A B NB Yes exponential No we don t care too much because PyTorch ops don t accept many input Tensors generate_subclass_choices flat_args CCT cct_mode CCT CompositeCompliantTensor which generated using generate_cct_and_mode is_tensor_likes = isinstance arg torch Tensor is_tensorlist arg arg flat_args subclass_options = False True is_tensor_like False is_tensor_like is_tensor_likes which_args_are_wrapped itertools product subclass_options result = maybe_map partial wrap CCT=CCT cct_mode=cct_mode should_wrap_arg arg should_wrap_arg arg zip which_args_are_wrapped flat_args strict=True yield result which_args_are_wrapped For operation f args kwargs each Tensor argument may either regular Tensor Tensor Subclass This iterator iterates through all those options generate_subclass_choices_args_kwargs args kwargs CCT cct_mode CCT CompositeCompliantTensor which generated using generate_cct_and_mode flat_kwargs spec = tree_flatten kwargs flat_args_kwargs = list args + list flat_kwargs choice debug_metadata generate_subclass_choices flat_args_kwargs CCT cct_mode new_args = choice len args new_kwargs = tree_unflatten choice len args spec which_args_are_wrapped = debug_metadata len args which_kwargs_are_wrapped = tree_unflatten debug_metadata len args spec yield new_args new_kwargs which_args_are_wrapped which_kwargs_are_wrapped raise_composite_compliance_error err additional_info= raise RuntimeError Composite compliance check failed above error \n f additional_info If you adding OpInfo existing operator please feel free skip test because problem pre-existing file issue Otherwise you added new operator please read through Composite Compliance section aten src ATen native README md how resolve err This test checks ALL possible permutations calling ` op ` arguments individually either regular Tensor Tensor subclass The general strategy wrap some Tensor args kwargs CompositeCompliantTensor wrappers call operation If some composite operation does any non-compliant behavior CompositeCompliantTensor will raise error check_all_permutations op args kwargs assert_equal_fn CCT cct_mode = generate_cct_and_mode expected = op args kwargs choice generate_subclass_choices_args_kwargs args kwargs CCT cct_mode new_args new_kwargs which_args_are_wrapped which_kwargs_are_wrapped = choice try actual = op new_args new_kwargs NOTE What errors Composite Compliance trying catch There s two things we want catch - errors would raise within torch_dispatch impl - data_ptr accesses The first easy filter we could make error different error second always going RuntimeError due how implemented you try access data_ptr wrapper Tensor raises you some internal RuntimeError So most general thing catch here RuntimeError If you here debugging why your test failed s plausible operator itself broken there other tests failing except RuntimeError err raise_composite_compliance_error err f - wrapped_args which_args_are_wrapped \n f - wrapped_kwargs which_kwargs_are_wrapped \n unwrap e e elem isinstance e CCT e assert_equal_fn tree_map unwrap actual expected Checks via usage torch dispatch mode certain anti-patterns composite compliant In particular anti-pattern we trying prevent user creating empty tensor then resize_-ing Torch Dispatch Mode helps here because all factory functions will create tensors CompositeCompliantTensor The general strategy wrap all Tensor args kwargs CompositeCompliantTensor wrappers If operator Composite does any non-compliant behavior CompositeCompliantTensor will raise error check_with_mode op args kwargs assert_equal_fn CCT cct_mode = generate_cct_and_mode wrap e CCT e cct_mode isinstance e torch Tensor e expected = op args kwargs args = tree_map wrap args kwargs = tree_map wrap kwargs try cct_mode actual = op args kwargs see NOTE What errors Composite Compliance trying catch except RuntimeError err raise_composite_compliance_error err unwrap e e elem isinstance e CCT e assert_equal_fn tree_map unwrap actual expected gather_leaf_tensors args kwargs leaf_tensors = args _args_spec = tree_flatten args kwargs _kwargs_spec = tree_flatten kwargs args = args + kwargs arg args isinstance arg torch Tensor continue arg requires_grad leaf_tensors append arg leaf_tensors compute_expected_grads op args kwargs output_process_fn_grad=None gradcheck_wrapper=None gradcheck_wrapper None results = op args kwargs results = gradcheck_wrapper op args kwargs output_process_fn_grad None results = output_process_fn_grad results flat_results = pytree tree_leaves results flat_results = r r flat_results isinstance r torch Tensor flat_diff_results = r r flat_results r requires_grad assert len flat_diff_results grads = torch ones r shape device=r device dtype=r dtype r flat_diff_results leaf_tensors = gather_leaf_tensors args kwargs assert len leaf_tensors torch autograd grad flat_diff_results leaf_tensors grads allow_unused=True retain_graph=True Checks backward formula composite compliant testing all possible permutations inputs grad_outputs being CompositeCompliantTensor regular Tensors NB important op accepted Callable OpInfo means we can apply check_backward_formula things aren t OpInfos while debugging check_backward_formula op Callable args kwargs output_process_fn_grad=None gradcheck_wrapper=None assert_equal_fn=None CCT cct_mode = generate_cct_and_mode expected = compute_expected_grads op args kwargs output_process_fn_grad gradcheck_wrapper choice generate_subclass_choices_args_kwargs args kwargs CCT cct_mode new_args new_kwargs which_args_are_wrapped which_kwargs_are_wrapped = choice leaf_tensors = gather_leaf_tensors new_args new_kwargs assert len leaf_tensors try gradcheck_wrapper None results = op new_args new_kwargs results = gradcheck_wrapper op new_args new_kwargs output_process_fn_grad None results = output_process_fn_grad results see NOTE What errors Composite Compliance trying catch except RuntimeError err raise_composite_compliance_error err f - wrapped_args which_args_are_wrapped \n f - wrapped_kwargs which_kwargs_are_wrapped \n flat_results = pytree tree_leaves results flat_results = r r flat_results isinstance r torch Tensor flat_diff_results = r r flat_results r requires_grad assert len flat_diff_results NB ones ones_like so we get regular Tensor here grads = torch ones r shape device=r device dtype=r dtype r flat_diff_results flat_new_grads which_grad_is_batched generate_subclass_choices grads CCT cct_mode try actual = torch autograd grad flat_diff_results leaf_tensors flat_new_grads allow_unused=True retain_graph=True see NOTE What errors Composite Compliance trying catch except RuntimeError err raise_composite_compliance_error err f - wrapped_args which_args_are_wrapped \n f - wrapped_kwargs which_kwargs_are_wrapped \n f - wrapped_grads which_grad_is_batched \n unwrap e e elem isinstance e CCT e assert_equal_fn tuple map unwrap actual expected equal_nan=True Checks forward AD formula composite compliant testing all possible permutations primals tangents being CompositeCompliantTensor regular Tensors NB important op accepted Callable OpInfo means we can apply check_forward_ad_formula things aren t OpInfos while debugging check_forward_ad_formula op Callable args kwargs gradcheck_wrapper=None assert_equal_fn=None CCT cct_mode = generate_cct_and_mode autograd_view_consistency=False maybe_tangent t assert type t CCT Generate ` tangent ` tensor given object Tensor requires grad set isinstance t torch Tensor t requires_grad torch randn_like t is_tensorlist t torch randn_like e e requires_grad None e t None tangent_args = tuple maybe_tangent arg arg args flat_kwargs spec = tree_flatten kwargs flat_tangent_kwargs = tuple maybe_tangent arg arg flat_kwargs tangent_kwargs = tree_unflatten flat_tangent_kwargs spec fwAD dual_level maybe_make_dual dual Returns dual tensor primal tensor tensor subclass requires_grad set primal tangent = dual isinstance primal torch Tensor primal requires_grad fwAD make_dual primal detach tangent is_tensorlist primal tuple fwAD make_dual pri detach tang tang None pri pri tang zip primal tangent strict=True primal compute_expected_grad args tangent_args kwargs tangent_kwargs op_args = tuple map maybe_make_dual zip args tangent_args strict=True op_kwargs = k maybe_make_dual v tangent_kwargs k k v kwargs items gradcheck_wrapper None op op_args op_kwargs gradcheck_wrapper op op_args op_kwargs expected = compute_expected_grad args tangent_args kwargs tangent_kwargs expected = tree_map fwAD unpack_dual expected expected_primals = tree_map lambda x x primal expected is_leaf=lambda x type x fwAD UnpackedDualTensor expected_tangents = tree_map lambda x x tangent expected is_leaf=lambda x type x fwAD UnpackedDualTensor Permutations arg kwargs CCT choice generate_subclass_choices_args_kwargs args kwargs CCT cct_mode new_args new_kwargs which_args_are_wrapped which_kwargs_are_wrapped = choice Permutations tangent arg tangent kwargs CCT tang_choice generate_subclass_choices_args_kwargs tangent_args tangent_kwargs CCT cct_mode new_tang_args new_tang_kwargs \ which_tang_args_are_wrapped which_tang_kwargs_are_wrapped = tang_choice op_args = tuple map maybe_make_dual zip new_args new_tang_args strict=True op_kwargs = k maybe_make_dual v new_tang_kwargs k k v new_kwargs items try gradcheck_wrapper None actual = op op_args op_kwargs actual = gradcheck_wrapper op op_args op_kwargs see NOTE What errors Composite Compliance trying catch except RuntimeError err raise_composite_compliance_error err f - wrapped_args which_args_are_wrapped \n f - wrapped_kwargs which_kwargs_are_wrapped \n f - wrapped_tangent_args which_tang_args_are_wrapped \n f - wrapped_tangent_kwargs which_tang_kwargs_are_wrapped \n unwrap e e elem isinstance e CCT e actual = tree_map fwAD unpack_dual actual actual_primals = tree_map lambda x unwrap x primal actual is_leaf=lambda x type x fwAD UnpackedDualTensor actual_tangents = tree_map lambda x unwrap x tangent actual is_leaf=lambda x type x fwAD UnpackedDualTensor assert_equal_fn actual_primals expected_primals equal_nan=True assert_equal_fn actual_tangents expected_tangents equal_nan=True