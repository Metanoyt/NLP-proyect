Implementation higher-order operators __future__ annotations typing TYPE_CHECKING torch torch onnx _internal _lazy_import onnxscript_ir ir torch onnx _internal exporter _core torch onnx _internal exporter _torchlib _torchlib_registry onnx_impl TYPE_CHECKING collections abc Sequence call_op op_type str args ir Value _num_outputs int = _domain str = kwargs int &#124; float &#124; str &#124; bool &#124; ir Graph &#124; ir TensorProtocol &#124; Sequence int - Sequence ir Value Call operator given arguments keyword arguments Arguments always inputs while keyword arguments attributes This wrapper around IR node creation hooks into _builder OpRecorder tracer so all nodes created recorded same way we use onnxscript ops directly onnxscript ir convenience ir_convenience assert _core current_tracer None tracer = _core current_tracer inputs = list args If final inputs None strip them node inputs input reversed inputs input None break inputs pop Construct filter out None attributes attributes = attr attr ir_convenience convert_attributes kwargs attr value None type ignore union-attr tracer nodes append node = ir Node _domain op_type inputs=inputs attributes=attributes num_outputs=_num_outputs version=tracer opset version node outputs onnx_impl torch ops higher_order cond no_compile=True higher_order_cond cond ir Value true_func ir Function false_func ir Function inputs Sequence ir Value - Sequence ir Value then_node = ir Node true_func domain true_func name inputs num_outputs=len true_func outputs else_node = ir Node false_func domain false_func name inputs num_outputs=len false_func outputs ONNX Runtime complains about duplicate output names we don t rename them But doesn t seem actual violation SSA form without renaming func_out out zip true_func outputs then_node outputs out name = f func_out name _ true_func name func_out out zip false_func outputs else_node outputs out name = f func_out name _ false_func name call_op If cond _num_outputs=len true_func outputs then_branch=ir Graph then_node outputs nodes= then_node name=true_func name else_branch=ir Graph else_node outputs nodes= else_node name=false_func name onnx_impl torch ops higher_order scan no_compile=True higher_order_scan body_func ir Function scan_inits Sequence ir Value scan_inputs Sequence ir Value additional_inputs Sequence ir Value &#124; None reverse bool = False - Sequence ir Value https github com pytorch pytorch blob ac b e c f e ef f d f torch _higher_order_ops scan py#L subgraph_inputs = ir Value name=f inp name _ body_func name __subgraph_in shape=inp shape type=ir TensorType inp dtype type ignore arg-type inp scan_inits ir Value name=f inp name _ body_func name __subgraph_in The iterated element passed body subgraph does have sequence axis It will have rank one less than rank corresponding scan_input shape=ir Shape inp shape type ignore index type=ir TensorType inp dtype type ignore arg-type inp scan_inputs The one only node Scan subgraph calls body_func body_node = ir Node body_func domain body_func name subgraph_inputs additional_inputs num_outputs=len body_func outputs ONNX Runtime complains about duplicate output names we don t rename them But doesn t seem actual violation SSA form without renaming func_out out zip body_func outputs body_node outputs out name = f func_out name _ body_func name n_outputs = len body_func outputs - len scan_inits call_op Scan scan_inits scan_inputs _num_outputs=len body_func outputs body=ir Graph subgraph_inputs body_node outputs nodes= body_node name=body_func name num_scan_inputs=len scan_inputs scan_input_directions= reverse _ scan_inputs scan_output_directions= reverse _ range n_outputs