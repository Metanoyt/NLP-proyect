mypy allow-untyped-defs typing Union torch torch Tensor _functional F optimizer _maximize_doc _params_doc _to_scalar Optimizer ParamsT __all__ = SparseAdam SparseAdam Optimizer __init__ params ParamsT lr Union float Tensor = e- betas tuple float float = eps float = e- maximize bool = False isinstance lr Tensor lr numel = raise ValueError Tensor lr must -element lr raise ValueError f Invalid learning rate lr eps raise ValueError f Invalid epsilon value eps = betas raise ValueError f Invalid beta parameter index betas = betas raise ValueError f Invalid beta parameter index betas defaults = lr lr betas betas eps eps maximize maximize super __init__ params defaults sparse_params = complex_params = index param_group enumerate param_groups isinstance param_group dict raise AssertionError f param_groups must list dicts got type param_group given param group convert given params list first before iterating d_index d_param enumerate param_group params d_param is_sparse sparse_params append index d_index d_param is_complex complex_params append index d_index sparse_params raise ValueError f Sparse params indices sparse_params SparseAdam requires dense parameter tensors complex_params raise ValueError f Complex params indices complex_params SparseAdam does support complex parameters torch no_grad step closure=None Perform single optimization step Args closure Callable optional A closure reevaluates model returns loss loss = None closure None torch enable_grad loss = closure group param_groups params_with_grad list Tensor = grads list Tensor = exp_avgs list Tensor = exp_avg_sqs list Tensor = state_steps list int = beta beta = group betas maximize = group get maximize False p group params p grad None params_with_grad append p p grad is_sparse raise RuntimeError SparseAdam does support dense gradients please consider Adam instead grads append p grad state = state p State initialization len state == state step = Exponential moving average gradient values state exp_avg = torch zeros_like p memory_format=torch preserve_format Exponential moving average squared gradient values state exp_avg_sq = torch zeros_like p memory_format=torch preserve_format exp_avgs append state exp_avg exp_avg_sqs append state exp_avg_sq update steps each param group update state step += record step after step update state_steps append state step F sparse_adam params_with_grad grads exp_avgs exp_avg_sqs state_steps eps=group eps beta =beta beta =beta lr=_to_scalar group lr maximize=maximize loss SparseAdam __doc__ = rf SparseAdam implements masked version Adam algorithm suitable sparse gradients Currently due implementation constraints explained below SparseAdam only intended narrow subset use cases specifically parameters dense layout gradients sparse layout This occurs special case where module backwards produces grads already sparse layout One example NN module behaves such ` ` nn Embedding sparse=True ` ` SparseAdam approximates Adam algorithm masking out parameter moment updates corresponding zero values gradients Whereas Adam algorithm will update first moment second moment parameters based all values gradients SparseAdam only updates moments parameters corresponding non-zero values gradients A simplified way thinking about ` intended ` implementation such Create mask non-zero values sparse gradients For example your gradient looks like mask would Apply mask over running moments do computation only non-zero values Apply mask over parameters only apply update non-zero values In actuality we use sparse layout Tensors optimize approximation which means more gradients masked being materialized more performant optimization Since we rely using sparse layout tensors we infer any materialized value sparse layout non-zero we do NOT actually verify all values zero It important conflate semantically sparse tensor tensor where many its values zeros sparse layout tensor tensor where ` ` is_sparse ` ` returns ` ` True ` ` The SparseAdam approximation intended ` semantically ` sparse tensors sparse layout only implementation detail A clearer implementation would use MaskedTensors those experimental note If you suspect your gradients semantically sparse do have sparse layout variant may best you Ideally you want avoid materializing anything suspected sparse first place since needing convert all your grads dense layout sparse layout may outweigh performance gain Here using Adam may best alternative unless you can easily rig up your module output sparse grads similar ` ` nn Embedding sparse=True ` ` If you insist converting your grads you can do so manually overriding your parameters ` ` grad ` ` fields their sparse equivalents before calling ` ` step ` ` Args _params_doc lr float Tensor optional learning rate default e- betas Tuple float float optional coefficients used computing running averages gradient its square default eps float optional term added denominator improve numerical stability default e- _maximize_doc _Adam\ A Method Stochastic Optimization https arxiv org abs