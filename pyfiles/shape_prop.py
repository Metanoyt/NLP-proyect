mypy ignore-errors traceback typing Any NamedTuple Optional torch torch fx torch _dispatch python enable_python_dispatcher torch _guards detect_fake_mode torch _prims_common is_contiguous_for_memory_format_or_false torch _subclasses meta_utils is_sparse_any torch fx _compatibility compatibility torch fx node map_aggregate Node __all__ = TensorMetadata ShapeProp compatibility is_backward_compatible=True TensorMetadata NamedTuple TensorMetadata structure containing pertinent information about tensor within PyTorch program General Tensor metadata shape torch Size dtype torch dtype requires_grad bool stride tuple int memory_format Optional torch memory_format Quantization metadata is_quantized bool qparams dict str Any When include_contiguity True we will set contiguity when its always true tensor Some tensors can represent both contiguous non-contiguous tensors e g u u u u In such situation contiguity set We could also make tri-state i e def_contiguous def_not_contiguous unknown _extract_tensor_metadata result torch Tensor include_contiguity=True - TensorMetadata Extract TensorMetadata NamedTuple describing ` result ` shape = result shape dtype = result dtype requires_grad = result requires_grad stride = result stride is_sparse_any result memory_format = None include_contiguity is_sparse_any result memory_formats = torch contiguous_format torch channels_last torch channels_last_ d query_format memory_formats is_contiguous_for_memory_format_or_false result memory_format=query_format memory_format = query_format break is_quantized = result is_quantized qparams dict str Any = is_quantized qscheme = result qscheme qparams qscheme = qscheme qscheme torch per_tensor_affine torch per_tensor_symmetric qparams scale = result q_scale type ignore assignment qparams zero_point = result q_zero_point type ignore assignment qscheme torch per_channel_affine torch per_channel_affine_float_qparams torch per_channel_symmetric In branch scale zero_point expected tensors we store values immutable_list TensorMetadata easier serialization downstream qparams scale = result q_per_channel_scales tolist type ignore assignment qparams zero_point = result q_per_channel_zero_points tolist type ignore assignment qparams axis = result q_per_channel_axis type ignore assignment TensorMetadata shape dtype requires_grad stride memory_format is_quantized qparams compatibility is_backward_compatible=True ShapeProp torch fx Interpreter Execute FX graph Node-by-Node record shape type result into corresponding node Example In example we record shape data type module given example input ` ` torch randn D_in ` ` We print name shape dtype each node TwoLayerNet torch nn Module __init__ D_in H D_out super __init__ linear = torch nn Linear D_in H linear = torch nn Linear H D_out forward x h_relu = linear x clamp min= y_pred = linear h_relu y_pred N D_in H D_out = x = torch randn N D_in y = torch randn N D_out model = TwoLayerNet D_in H D_out gm = torch fx symbolic_trace model sample_input = torch randn D_in ShapeProp gm propagate sample_input node gm graph nodes print node name node meta tensor_meta dtype node meta tensor_meta shape The output code x torch float torch Size linear torch float torch Size clamp_ torch float torch Size linear torch float torch Size output torch float torch Size Args module GraphModule The module executed fake_mode FakeTensorMode A fake mode copying gm __init__ gm fake_mode=None super __init__ gm fake_mode None fake_mode = detect_fake_mode fake_mode None torch _dynamo utils deepcopy_to_fake_tensor Note We need fake execution cause inputs fake however we cannot fakify module - because we need write tensor_meta real module So we fakify produce result L below extract tensor meta then keep going If we fakify we would write wrong node then downstream fusion would missing tensor_meta See torch _inductor overrides py where called upstream fusion fake_module = deepcopy_to_fake_tensor module fake_mode fake_mode = fake_mode fake_module = None fake_mode = None real_module = module run_node n Node - Any torch fx experimental symbolic_shapes compute_unbacked_bindings rebind_unbacked try fake_module None Hacky swap Alternatively we could do overriding call_module get_attr module = fake_module try fake_mode None fake_mode enable_python_dispatcher result = super run_node n rebind_unbacked fake_mode shape_env n result result = super run_node n finally module = real_module except Exception e traceback print_exc raise RuntimeError f ShapeProp error node= n format_node meta= n meta e found_tensor = False extract_tensor_meta obj isinstance obj torch Tensor nonlocal found_tensor found_tensor = True _extract_tensor_metadata obj obj meta = map_aggregate result extract_tensor_meta found_tensor n meta tensor_meta = meta fake_mode shape_env = fake_mode shape_env symbol_to_path = compute_unbacked_bindings shape_env result n meta unbacked_bindings = symbol_to_path n meta type = type result result propagate args Run ` module ` via interpretation result record shape type each node Args args Tensor sample input Returns Any The value returned executing Module fake_mode None fake_args = fake_mode from_tensor t isinstance t torch Tensor t t args fake_args = args super run fake_args