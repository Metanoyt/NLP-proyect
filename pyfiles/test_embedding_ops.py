Owner s oncall distributed sys torch torch distributed tensor distribute_module distribute_tensor DTensor Replicate Shard torch distributed tensor debug CommDebugMode torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN torch testing _internal distributed _tensor common_dtensor create_local_tensor_test_class DTensorTestBase with_comms TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit funcol = torch ops c d_functional TestEmbeddingOp DTensorTestBase _apply_sharding embedding_mod shard_dim device_mesh shard_embedding_fn name module device_mesh name param module named_parameters dist_param = torch nn Parameter distribute_tensor param device_mesh Shard shard_dim module register_parameter name dist_param sharded_embedding = distribute_module embedding_mod device_mesh shard_embedding_fn sharded_embedding _run_embedding_op_test device_mesh shard_dim input_size num_embeddings embedding_dim kwargs Use same seed torch manual_seed local_embedding = torch nn Embedding num_embeddings embedding_dim device=self device_type kwargs sharded_embedding = torch nn Embedding num_embeddings embedding_dim device=self device_type kwargs Shard parameter local embedding set sharded embedding sharded_embedding weight = torch nn Parameter local_embedding weight detach clone sharded_embedding = _apply_sharding sharded_embedding shard_dim device_mesh Run sharded computation torch manual_seed inp = torch randint num_embeddings tuple input_size device=self device_type target = torch empty inp size embedding_dim dtype=torch float device=self device_type random_ dist_inp = distribute_tensor inp device_mesh Replicate fwd computation ensure no comm happened CommDebugMode fwd_mode dist_output = sharded_embedding dist_inp assertEqual fwd_mode get_total_counts output = dist_output full_tensor Run local computation local_output = local_embedding inp Verify assertEqual local_output output Use sample cross entry loss verify backward grad computation loss = torch nn CrossEntropyLoss emb_loss = loss output target emb_dup_loss = loss local_output target local embedding backward emb_dup_loss backward sharded embedding bwd computation ensure no comm happened CommDebugMode bwd_mode emb_loss backward assertEqual bwd_mode get_total_counts gradient = sharded_embedding weight grad full_tensor local_grad = local_embedding weight grad Verify gradient assertEqual gradient local_grad Validate torch nn functional embedding version local_output = torch nn functional embedding inp local_embedding weight kwargs sharded_output = torch nn functional embedding DTensor from_local inp device_mesh Replicate run_check=False sharded_embedding weight kwargs assertEqual local_output sharded_output full_tensor with_comms test_sharded_embedding_colwise mesh = build_device_mesh _run_embedding_op_test mesh _run_embedding_op_test mesh _run_embedding_op_test mesh _run_embedding_op_test mesh _run_embedding_op_test mesh _run_embedding_op_test mesh padding_idx= _run_embedding_op_test mesh padding_idx= with_comms test_sharded_embedding_colwise_max_norm_errors mesh = build_device_mesh assertRaisesRegex NotImplementedError aten embedding_renorm_ default does have sharding strategy registered _run_embedding_op_test mesh padding_idx= max_norm= with_comms test_sharded_embedding_rowwise mesh = build_device_mesh test correctness _run_embedding_op_test mesh _run_embedding_op_test mesh _run_embedding_op_test mesh padding_idx= torch distributed tensor placement_types MaskPartial test collectives embedding_mod = torch nn Embedding device=self device_type sharded_embedding = _apply_sharding embedding_mod mesh inp = torch randint device=self device_type replicated_inp = DTensor from_local inp mesh Replicate run_check=False output = sharded_embedding replicated_inp assertIsInstance output placements MaskPartial comm_mode = CommDebugMode comm_mode output full_tensor assertEqual comm_mode get_total_counts assertEqual comm_mode get_comm_counts funcol all_reduce with_comms test_multiple_embeddings_rowwise mesh = build_device_mesh inp = torch randint device=self device_type replicated_inp = DTensor from_local inp mesh Replicate run_check=False torch distributed tensor placement_types MaskPartial case two embeddings same shape thus sharing underlying MaskPartial MaskBuffer because cache hit sharding propagation emb = torch nn Embedding device=self device_type sharded_emb = _apply_sharding emb mesh output = sharded_emb replicated_inp emb = torch nn Embedding device=self device_type sharded_emb = _apply_sharding emb mesh output = sharded_emb replicated_inp partial_placement = output placements assertIsInstance partial_placement MaskPartial output full_tensor partial_placement = output placements assertIsInstance partial_placement MaskPartial output full_tensor assertTrue id partial_placement id partial_placement case two embeddings same logical_dim_size different logical_shape thus they will have different MaskPartial placements no cache hit emb = torch nn Embedding device=self device_type sharded_emb = _apply_sharding emb mesh output = sharded_emb replicated_inp partial_placement = output placements assertIsInstance partial_placement MaskPartial output full_tensor equal because different logical_shape despite same logical_dim_size assertNotEqual partial_placement partial_placement TestEmbeddingOpWithLocalTensor = create_local_tensor_test_class TestEmbeddingOp __name__ == __main__ run_tests