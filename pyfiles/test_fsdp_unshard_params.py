Owner s oncall distributed contextlib itertools math sys typing Any Optional Union torch torch distributed fsdp _traversal_utils traversal_utils torch nn nn torch distributed dist torch distributed fsdp CPUOffload FullyShardedDataParallel FSDP MixedPrecision ShardingStrategy torch distributed fsdp _common_utils clean_tensor_name torch distributed fsdp _flat_param FlatParameter torch distributed fsdp fully_sharded_data_parallel FLAT_PARAM torch distributed fsdp wrap ModuleWrapPolicy torch nn parallel distributed DistributedDataParallel DDP torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp DEVICEInitMode FSDPInitMode FSDPTest get_devtype NestedWrappedModule TransformerWithSharedParams torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN device_type = torch device get_devtype dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit TestUnshardParamsBase FSDPTest This contains any methods common both sharded non-sharded cases _test_unshard_params_writeback writeback bool check_outer bool fsdp_kwargs dict str Any model = nn Sequential nn Linear bias=False device=device_type type nn Linear bias=False device=device_type type model = FSDP model fsdp_kwargs model = FSDP model fsdp_kwargs uses_sharded_strategy = model sharding_strategy = ShardingStrategy NO_SHARD offloading_params = model cpu_offload offload_params Assumes depth-first ` parameters ` outer_param Union FlatParameter nn Parameter = next model parameters inner_param Union FlatParameter nn Parameter = next model parameters param_to_check = outer_param check_outer inner_param Write known value all elements sharded parameter ` FlatParameter ` check torch no_grad param_to_check zero_ param_to_check += rank + Zero unsharded parameters FSDP summon_full_params model writeback=writeback torch no_grad param model parameters param zero_ Check th singleton element sharded parameter see zeroing inside context persists param_elem_to_check = param_to_check param_elem_to_check numel For ` use_orig_params=True ` ` NO_SHARD ` parameter preserves original D shape so we must access one more time param_elem_to_check = param_elem_to_check writeback uses_sharded_strategy offloading_params When FSDP does use sharded strategy offloading parameters CPU directly exposes tensor storage serves unsharded source truth so write always reflected regardless ` writeback ` assertEqual param_elem_to_check assertEqual param_elem_to_check rank + offloading_params cpu_device = torch device cpu param model parameters assertEqual param device cpu_device _get_test_unshard_params_writeback_config - dict str list Any writeback True False check_outer True False mixed_precision MixedPrecision param_dtype=torch float None cpu_offload CPUOffload offload_params=False CPUOffload offload_params=True use_orig_params True False _test_unshard_params_param_data rank _only bool offload_to_cpu bool cpu_offload CPUOffload mixed_precision Optional MixedPrecision use_orig_params bool local_model = NestedWrappedModule init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE fsdp_kwargs= device_id device_type type deterministic=True Apply FSDP such root module does have FSDP applied while there multiple FSDP root submodules proven later fsdp_model = NestedWrappedModule init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE fsdp_kwargs= cpu_offload cpu_offload mixed_precision mixed_precision use_orig_params use_orig_params deterministic=True assertFalse isinstance fsdp_model FSDP Hard code following names because getting them non-trivial non_fsdp_managed_param_names = module weight module bias module weight module bias FSDP summon_full_params fsdp_model rank _only=rank _only writeback=not rank _only offload_to_cpu=offload_to_cpu rank _only rank == p n p zip local_model parameters fsdp_model named_parameters assertEqual p shape p shape offload_to_cpu clean_tensor_name n non_fsdp_managed_param_names assertEqual torch device cpu p device assertEqual p device p device assertEqual p dtype p dtype even FSDP uses mixed precision assertEqual p p assertTrue isinstance p nn Parameter Check each ` FlatParameter ` has sharded size proxy being resharded handle traversal_utils _get_fsdp_handles fsdp_model handle uses_sharded_strategy assertEqual handle flat_param shape handle flat_param _sharded_size assertEqual handle flat_param shape handle flat_param _unpadded_unsharded_size Prove number FSDP roots after lazy initialization num_fsdp_roots = fsdp_state traversal_utils _get_fsdp_states fsdp_model num_fsdp_roots += fsdp_state _is_root assertGreater num_fsdp_roots _get_test_unshard_params_param_data_config - dict str list Any rank _only False True offload_to_cpu False True cpu_offload CPUOffload offload_params=False CPUOffload offload_params=True mixed_precision MixedPrecision param_dtype=torch float None use_orig_params True False TestUnshardParams TestUnshardParamsBase property world_size - int skip_if_lt_x_gpu test_unshard_params_writeback Tests ` ` writeback ` ` argument using default all others run_subtests _get_test_unshard_params_writeback_config _test_unshard_params_writeback skip_if_lt_x_gpu test_unshard_params_param_data Tests parameters exposed correctly ` ` recurse=True ` ` all other argument configs non-FSDP root module run_subtests _get_test_unshard_params_param_data_config _test_unshard_params_param_data skip_if_lt_x_gpu test_unshard_singleton_param_writeback Tests ` ` writeback=True ` ` singleton parameter which includes testing writing padding does persist NOTE This method depends FSDP internals model = FSDP nn Linear bias=False device=device_type type flat_param = model _handle flat_param assertEqual flat_param numel Write known value sharded ` FlatParameter ` torch no_grad For nonzero ranks write padding flat_param = rank + FSDP summon_full_params model writeback=True assertEqual flat_param numel torch no_grad flat_param zero_ NOTE This checks writes padding did persist which strictly required correctness rank == did write padding assertEqual flat_param wrote padding assertEqual rank + flat_param skip_if_lt_x_gpu test_unshard_params_respects_reshard Tests unsharding parameters respects expected reshard behavior between forward backward well after backward For mixed precision we should respect reshard behavior because ` ` summon_full_params ` ` forces full precision which uses different all-gather tensor than one already memory will persist any modifications correctly run_subtests rank _only False True offload_to_cpu False True mixed_precision MixedPrecision param_dtype=torch float None use_orig_params False True _test_unshard_params_respects_reshard _test_unshard_params_respects_reshard rank _only bool offload_to_cpu bool mixed_precision Optional MixedPrecision use_orig_params bool NOTE This method depends FSDP internals fsdp_kwargs = mixed_precision mixed_precision use_orig_params use_orig_params model = FSDP nn Sequential FSDP nn Linear bias=False device=device_type type fsdp_kwargs nn Linear bias=False device=device_type type fsdp_kwargs outer_flat_param = model _handle flat_param inner_flat_param = model module _handle flat_param NOTE This assumes uniform sharding padding across ranks expected_outer_flat_param_unsharded_numel = outer_flat_param numel world_size _get_unsharded_storage_size flat_param FlatParameter flat_param _full_param_padded storage size Validate expected behavior root does reshard after forward non-root reshards after forward both reshard after backward output = model torch zeros device=device_type type assertEqual expected_outer_flat_param_unsharded_numel _get_unsharded_storage_size outer_flat_param assertEqual _get_unsharded_storage_size inner_flat_param output sum backward assertEqual _get_unsharded_storage_size outer_flat_param assertEqual _get_unsharded_storage_size inner_flat_param Check parameter unsharding between forward backward well after backward reshard behavior matches output = model torch zeros device=device_type type FSDP summon_full_params model rank _only=rank _only writeback=not rank _only offload_to_cpu=offload_to_cpu pass mixed_precision None After forcing full precision we must invalidate existing unsharded low-precision flat parameter since will persist changes ` summon_full_params ` context so we cannot respect reshard behavior expected_outer_flat_param_unsharded_numel = assertEqual expected_outer_flat_param_unsharded_numel _get_unsharded_storage_size outer_flat_param assertEqual _get_unsharded_storage_size inner_flat_param output sum backward FSDP summon_full_params model rank _only=rank _only writeback=not rank _only offload_to_cpu=offload_to_cpu pass assertEqual _get_unsharded_storage_size outer_flat_param assertEqual _get_unsharded_storage_size inner_flat_param skip_if_lt_x_gpu test_unshard_params_recurse Tests ` ` recurse ` ` argument using default all others run_subtests recurse False True unshard_outer False True mixed_precision MixedPrecision param_dtype=torch float None use_orig_params False True _test_unshard_params_recurse _test_unshard_params_recurse recurse bool unshard_outer bool mixed_precision Optional MixedPrecision use_orig_params bool NOTE This method depends FSDP internals fsdp_kwargs = mixed_precision mixed_precision use_orig_params use_orig_params model = FSDP nn Sequential FSDP nn Linear bias=False device=device_type type fsdp_kwargs nn Linear bias=False device=device_type type fsdp_kwargs Hard code numel values based model unsharded_inner_numel = unsharded_outer_numel = use_orig_params Account unsharded padding since each ` FlatParameter ` only has one original parameter we only need pad divisibility world size address alignment unsharded_inner_numel world_size unsharded_inner_numel += world_size - unsharded_inner_numel world_size unsharded_outer_numel world_size unsharded_outer_numel += world_size - unsharded_outer_numel world_size Round up sharded numel account padding sharded_inner_numel = int math ceil unsharded_inner_numel world_size sharded_outer_numel = int math ceil unsharded_outer_numel world_size inner_flat_param = model module _handle flat_param outer_flat_param = model _handle flat_param assertEqual sharded_inner_numel inner_flat_param numel assertEqual sharded_outer_numel outer_flat_param numel expected_outer_numel = unsharded_outer_numel unshard_outer sharded_outer_numel expected_inner_numel = unsharded_inner_numel recurse unshard_outer sharded_inner_numel module_to_unshard = model unshard_outer model FSDP summon_full_params module_to_unshard recurse=recurse assertEqual expected_outer_numel outer_flat_param numel assertEqual expected_inner_numel inner_flat_param numel skip_if_lt_x_gpu test_named_parameters_and_buffers Tests ` ` named_parameters ` ` ` ` named_buffers ` ` top-level FSDP-wrapped model matches their behavior equivalent non-wrapped module run_subtests prefix test_prefix recurse False True _test_named_parameters_and_buffers _test_named_parameters_and_buffers prefix str recurse bool model = NestedWrappedModule init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE deterministic=True model buffer = nn Buffer torch ones Wrap top-level FSDP since ` named_parameters ` ` named_buffers ` will contain FSDP prefixes called non-FSDP root module fsdp_model = FSDP NestedWrappedModule init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE deterministic=True process_group fsdp_model buffer = nn Buffer torch ones FSDP summon_full_params fsdp_model call named_parameters named_buffers n p n p itertools zip_longest getattr fsdp_model call prefix=prefix recurse=recurse getattr model call prefix=prefix recurse=recurse assertEqual n n assertEqual p p skip_if_lt_x_gpu test_with_grads_core Tests core usage ` ` with_grads=True ` ` comparing against DDP unsharded equivalent run_subtests writeback False True offload_to_cpu False True sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP ShardingStrategy NO_SHARD use_orig_params True _test_with_grads_core _test_with_grads_core writeback bool offload_to_cpu bool sharding_strategy ShardingStrategy use_orig_params bool _check_grads ddp_model DDP fsdp_model FSDP old_fsdp_grads Optional list torch Tensor Checks writes FSDP parameters gradients persist do persist depending ` ` writeback ` ` sharding strategy The DDP model used checking gradient parity ensure FDSP all-gathers correct gradient values WRITEBACK_FACTOR = FSDP summon_full_params fsdp_model writeback=writeback offload_to_cpu=offload_to_cpu with_grads=True n p n p zip ddp_model module named_parameters fsdp_model named_parameters assertEqual n clean_tensor_name n assert p grad None torch testing assert_close p grad p grad Ensure tensor all zeros which would mean multiplication vacuous assert torch count_nonzero p grad p grad = WRITEBACK_FACTOR new_fsdp_grads = param grad param fsdp_model parameters param grad None writeback_persists = writeback sharding_strategy == ShardingStrategy NO_SHARD offload_to_cpu old_grad new_grad zip old_fsdp_grads new_fsdp_grads writeback_persists torch testing assert_close old_grad WRITEBACK_FACTOR new_grad torch testing assert_close old_grad new_grad writeback_persists Modify DDP gradients same way parity param ddp_model parameters param grad = WRITEBACK_FACTOR _get_error_context is_supported bool contextlib nullcontext is_supported assertRaises NotImplementedError some configs implemented yet _get_fsdp_grads fsdp_model FSDP is_supported bool is_supported param grad clone param fsdp_model parameters param grad None None unused is_supported = use_orig_params offload_to_cpu model = TransformerWithSharedParams init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE deterministic=True ddp_model = DDP model device_ids= device_type fsdp_model = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE deterministic=True fsdp_kwargs= use_orig_params use_orig_params sharding_strategy sharding_strategy device_id device_type type FSDP summon_full_params fsdp_model p p zip ddp_model module parameters fsdp_model parameters assert torch all torch isclose p p Check calling after backward inp = fsdp_model get_input torch device device_type ddp_out = ddp_model inp fsdp_out = fsdp_model inp ddp_out sum backward fsdp_out sum backward old_fsdp_grads = _get_fsdp_grads fsdp_model is_supported _get_error_context is_supported _check_grads ddp_model fsdp_model old_fsdp_grads Check calling between forward backward inp = fsdp_model get_input torch device device_type ddp_out = ddp_model inp fsdp_out = fsdp_model inp old_fsdp_grads = _get_fsdp_grads fsdp_model is_supported _get_error_context is_supported _check_grads ddp_model fsdp_model old_fsdp_grads skip_if_lt_x_gpu test_with_grads_none_grads Tests all ranks ` ` FlatParameter ` ` has ` ` None ` ` gradient then each original parameter sees ` ` None ` ` gradient well run_subtests sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP ShardingStrategy NO_SHARD _test_with_grads_none_grads _test_with_grads_none_grads sharding_strategy ShardingStrategy fsdp_model = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE deterministic=True fsdp_kwargs= use_orig_params True sharding_strategy sharding_strategy device_id device_type type fsdp_module FSDP fsdp_modules fsdp_model fsdp_module _handle assert fsdp_module _handle flat_param grad None FSDP summon_full_params fsdp_model with_grads=True param fsdp_model parameters assertTrue param grad None skip_if_lt_x_gpu test_unshard_submodule model = nn Sequential nn Sequential nn Linear nn Linear nn Sequential nn Linear nn Linear device_type type model = FSDP model auto_wrap_policy=ModuleWrapPolicy nn Sequential FSDP summon_full_params model Check summoned module does have its flat parameter param_name _ model named_parameters assertFalse FLAT_PARAM param_name assertGreater len list model parameters TestUnshardParamsNoShard TestUnshardParamsBase property world_size - int skip_if_lt_x_gpu test_unshard_params_writeback_no_shard Tests ` ` writeback ` ` argument using default all others run_subtests _get_test_unshard_params_writeback_config _test_unshard_params_writeback skip_if_lt_x_gpu test_unshard_params_param_data_no_shard Tests parameters exposed correctly ` ` recurse=True ` ` all other argument configs non-FSDP root module config = _get_test_unshard_params_param_data_config TODO ` offload_to_cpu=True ` ` NO_SHARD ` supported yet See ` test_offload_to_cpu_no_shard_raises ` config offload_to_cpu = False run_subtests config _test_unshard_params_param_data TestUnshardParamsErrors TestUnshardParamsBase property world_size - int skip_if_lt_x_gpu test_unshard_params_from_forward_raises MyModule nn Module __init__ - None super __init__ = nn Parameter torch zeros forward fsdp_module fsdp_module summon_full_params fsdp_module pass model = FSDP MyModule device_type type assertRaisesRegex AssertionError Cannot manually unshard parameters during forward backward model model skip_if_lt_x_gpu test_unshard_params_from_backward_raises model = FSDP nn Linear device=device_type type output = model torch ones device=device_type type invalid_backward_hook args kwargs FSDP summon_full_params model pass assertTrue output requires_grad output register_hook invalid_backward_hook assertRaisesRegex AssertionError Cannot manually unshard parameters during forward backward output backward skip_if_lt_x_gpu test_rank _only_with_writeback_raises nested_wrapped_module = NestedWrappedModule init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE assertRaisesRegex NotImplementedError supported FSDP summon_full_params nested_wrapped_module rank _only=True writeback=True pass skip_if_lt_x_gpu test_offload_to_cpu_no_shard_raises nested_wrapped_module = NestedWrappedModule init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE sharding_strategy ShardingStrategy NO_SHARD assertRaisesRegex NotImplementedError supported FSDP summon_full_params nested_wrapped_module rank _only=True writeback=True pass __name__ == __main__ run_tests