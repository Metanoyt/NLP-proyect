mypy ignore-errors Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree contextlib functools itertools collections abc Callable functools partial typing Any Optional Union torch torch Tensor torch _C _functorch is_batchedtensor torch _functorch predispatch _add_batch_dim _remove_batch_dim _vmap_decrement_nesting _vmap_increment_nesting lazy_load_decompositions torch utils _pytree _broadcast_to_and_flatten tree_flatten tree_map_ tree_unflatten TreeSpec in_dims_t = Union int tuple out_dims_t = Union int tuple int doesnt_support_saved_tensors_hooks f message = torch func grad vjp jacrev hessian don t yet support saved tensor hooks Please open issue your use case functools wraps f fn args kwargs torch autograd graph disable_saved_tensors_hooks message f args kwargs fn Checks all args-to-be-batched have same batch dim size _validate_and_get_batch_size flat_in_dims list Optional int flat_args list - int batch_sizes = arg size in_dim in_dim arg zip flat_in_dims flat_args in_dim None len batch_sizes == raise ValueError vmap Expected least one Tensor vmap over batch_sizes any size = batch_sizes size batch_sizes raise ValueError f vmap Expected all tensors have same size mapped f dimension got sizes batch_sizes mapped dimension batch_sizes _num_outputs batched_outputs Union Tensor tuple Tensor - int isinstance batched_outputs tuple len batched_outputs If value tuple check has length ` num_elements ` If value tuple make tuple ` value ` repeated ` num_elements ` times _as_tuple value Any num_elements int error_message_lambda Callable str - tuple isinstance value tuple value num_elements len value = num_elements raise ValueError error_message_lambda value _process_batched_inputs in_dims in_dims_t args tuple func Callable - tuple int list Any list Any TreeSpec isinstance in_dims int isinstance in_dims tuple raise ValueError f vmap _get_name func in_dims= in_dims inputs f expected ` in_dims ` int potentially nested tuple f matching structure inputs got type in_dims len args == raise ValueError f vmap _get_name func inputs got no inputs Maybe you forgot add f inputs you trying vmap over function no inputs f The latter unsupported flat_args args_spec = tree_flatten args flat_in_dims = _broadcast_to_and_flatten in_dims args_spec flat_in_dims None raise ValueError f vmap _get_name func in_dims= in_dims inputs f in_dims compatible structure ` inputs ` f in_dims has structure tree_flatten in_dims inputs f has structure args_spec i arg in_dim enumerate zip flat_args flat_in_dims isinstance in_dim int in_dim None raise ValueError f vmap _get_name func in_dims= in_dims inputs f Got in_dim= in_dim input in_dim must either f integer dimension None isinstance in_dim int isinstance arg Tensor raise ValueError f vmap _get_name func in_dims= in_dims inputs f Got in_dim= in_dim input input type f type arg We cannot vmap over non-Tensor arguments f please use None respective in_dim in_dim None in_dim -arg dim in_dim = arg dim raise ValueError f vmap _get_name func in_dims= in_dims inputs f Got in_dim= in_dim some input input Tensor f dimensionality arg dim so expected in_dim satisfy f - arg dim = in_dim arg dim in_dim None in_dim flat_in_dims i = in_dim arg dim _validate_and_get_batch_size flat_in_dims flat_args flat_in_dims flat_args args_spec Creates BatchedTensors every Tensor arg should batched Returns potentially batched arguments batch_size _create_batched_inputs flat_in_dims list Any flat_args list Any vmap_level int args_spec - tuple See NOTE Ignored _remove_batch_dim _add_batch_dim batched_inputs = arg in_dim None _add_batch_dim arg in_dim vmap_level in_dim arg zip flat_in_dims flat_args tree_unflatten batched_inputs args_spec _maybe_remove_batch_dim name batched_output vmap_level batch_size out_dim out_dim None isinstance batched_output torch Tensor is_batchedtensor batched_output raise ValueError f vmap name ` name ` can f BatchedTensor when out_dim None batched_output out_dim non None isinstance batched_output torch Tensor raise ValueError f vmap name ` name ` must only f Tensors got type type batched_output Did you mean set out_dims= None output _remove_batch_dim batched_output vmap_level batch_size out_dim Undos batching any batch dimensions associated ` vmap_level ` _unwrap_batched batched_outputs Union Tensor tuple Tensor out_dims out_dims_t vmap_level int batch_size int func Callable - tuple flat_batched_outputs output_spec = tree_flatten batched_outputs incompatible_error raise ValueError f vmap _get_name func out_dims= out_dims inputs f out_dims compatible structure ` outputs ` f out_dims has structure tree_flatten out_dims outputs f has structure output_spec isinstance batched_outputs torch Tensor Some weird edge case requires us spell out following see test_out_dims_edge_case isinstance out_dims int flat_out_dims = out_dims isinstance out_dims tuple len out_dims == flat_out_dims = out_dims out_dims None flat_out_dims = out_dims incompatible_error flat_out_dims = _broadcast_to_and_flatten out_dims output_spec flat_out_dims None incompatible_error flat_outputs = _maybe_remove_batch_dim _get_name func batched_output vmap_level batch_size out_dim batched_output out_dim zip flat_batched_outputs flat_out_dims tree_unflatten flat_outputs output_spec _check_int_or_none x func out_dims isinstance x int x None raise ValueError f vmap _get_name func out_dims= out_dims ` out_dims ` must f int None python collection ints representing where outputs f vmapped dimension should appear _check_out_dims_is_int_or_int_pytree out_dims out_dims_t func Callable - None isinstance out_dims int tree_map_ partial _check_int_or_none func=func out_dims=out_dims out_dims _get_name func Callable hasattr func __name__ func __name__ isinstance func functools partial f functools partial _get_name func func Not all callables have __name__ fact only static functions methods do A callable created via nn Module name one example doesn t have __name__ repr func vmap_impl func in_dims out_dims randomness chunk_size args kwargs lazy_load_decompositions _check_out_dims_is_int_or_int_pytree out_dims func batch_size flat_in_dims flat_args args_spec = _process_batched_inputs in_dims args func chunk_size None chunks_flat_args = _get_chunked_inputs flat_args flat_in_dims batch_size chunk_size _chunked_vmap func flat_in_dims chunks_flat_args args_spec out_dims randomness kwargs If chunk_size specified _flat_vmap func batch_size flat_in_dims flat_args args_spec out_dims randomness kwargs get_chunk_sizes total_elems chunk_size n_chunks = total_elems chunk_size chunk_sizes = chunk_size n_chunks remainder chunk remainder = total_elems chunk_size remainder = chunk_sizes append remainder chunk_sizes _get_chunked_inputs flat_args flat_in_dims batch_size chunk_size split_idxs = batch_size chunk_size None chunk_sizes = get_chunk_sizes batch_size chunk_size split_idxs = tuple itertools accumulate chunk_sizes flat_args_chunks = tuple t tensor_split split_idxs dim=in_dim in_dim None t len split_idxs t in_dim zip flat_args flat_in_dims transpose chunk dim flatten structure chunks_flat_args list flatten args chunks_flat_args = zip flat_args_chunks chunks_flat_args _flatten_chunks_output chunks_output_ chunks_output list chunked outputs flatten chunked outputs flat_chunks_output = arg_spec = None output chunks_output_ flat_output arg_specs = tree_flatten output flat_chunks_output append flat_output arg_spec None arg_spec = arg_specs transpose chunk dim flatten structure flat_output_chunks flat list chunks flat_output_chunks = list zip flat_chunks_output flat_output_chunks arg_spec _concat_chunked_outputs out_dims arg_spec flat_output_chunks concat chunks out_dim flat_out_dims = _broadcast_to_and_flatten out_dims arg_spec assert len flat_out_dims == len flat_output_chunks flat_output = idx out_dim enumerate flat_out_dims flat_output append torch cat flat_output_chunks idx dim=out_dim release tensors flat_output_chunks idx = None flat_output Applies vmap chunked_input returns concatenated output over chunks _chunked_vmap func flat_in_dims chunks_flat_args args_spec out_dims randomness kwargs chunks_output = rs = torch get_rng_state randomness == same None flat_args chunks_flat_args batch_size = _validate_and_get_batch_size flat_in_dims flat_args The way we compute split input ` _get_chunked_inputs ` we may get tensor ` ` batch-size We skip any computation case Eg chunk_size = batch_size = t = torch zeros batch_size t tensor_split tensor tensor tensor tensor tensor tensor tensor size= batch_size == continue rs None torch set_rng_state rs chunks_output append _flat_vmap func batch_size flat_in_dims flat_args args_spec out_dims randomness kwargs flat_output_chunks arg_spec = _flatten_chunks_output chunks_output chunked output tensors held both ` flat_output_chunks ` ` chunks_output ` eagerly remove reference ` chunks_output ` del chunks_output concat chunks out_dim flat_output = _concat_chunked_outputs out_dims arg_spec flat_output_chunks finally unflatten output tree_unflatten flat_output arg_spec Vmap refactored helper functions _check_randomness_arg randomness randomness error different same raise RuntimeError f Only allowed values randomness error different same Got randomness contextlib contextmanager vmap_increment_nesting batch_size randomness try vmap_level = _vmap_increment_nesting batch_size randomness yield vmap_level finally _vmap_decrement_nesting _flat_vmap func batch_size flat_in_dims flat_args args_spec out_dims randomness kwargs vmap_increment_nesting batch_size randomness vmap_level batched_inputs = _create_batched_inputs flat_in_dims flat_args vmap_level args_spec batched_outputs = func batched_inputs kwargs _unwrap_batched batched_outputs out_dims vmap_level batch_size func ` restore_vmap ` private helper function It vmap has following differences - instead returning outputs returns outputs out_dims tuple out_dims pytree same shape outputs contains Optional int specifying where vmapped dimension exists corresponding output - does no validation in_dims inputs vmap expects least one Tensor vmapped restore_vmap allows no inputs have vmap dimension - does no validation outputs vmap expects only Tensor outputs restore_vmap allows arbitrary outputs just Tensors The TL DR restore_vmap more general than vmap has slightly different API The relaxations so we can pause vmap middle its execution then restore later what we do generate_vmap_rule=True implementation autograd Function restore_vmap can technically used implementation vmap doing refactor bit technically challenging because - vmap couples tensor-wrapping code error checking - vmap s tensor unwrapping code C++ we would need rewrite part python because overlaps unwrap_batched restore_vmap func in_dims batch_size randomness inner args kwargs vmap_increment_nesting batch_size randomness vmap_level batched_inputs = wrap_batched args in_dims vmap_level batched_outputs = func batched_inputs kwargs unwrap_batched batched_outputs vmap_level inner wrap_batched args bdims level flat_args spec = tree_flatten args flat_bdims = _broadcast_to_and_flatten bdims spec assert flat_bdims None result = _create_batched_inputs flat_bdims flat_args level spec result unwrap_batched args level flat_args spec = tree_flatten args len flat_args == args result = torch _C _functorch _unwrap_batched arg level isinstance arg torch Tensor arg None arg flat_args output bdims = zip result tree_unflatten output spec tree_unflatten bdims spec