Owner s module inductor Test FX IR backend itertools operator unittest collections abc Callable typing Optional sympy torch torch _inductor codegen common common torch utils _pytree pytree torch _dynamo exc BackendCompilerFailed torch _dynamo utils same torch _higher_order_ops triton_kernel_wrap triton_kernel_wrapper_mutation torch _inductor config torch _inductor codegen cpp CppScheduling torch _inductor codegen triton TritonScheduling torch _inductor codegen wrapper PythonWrapperCodegen torch _inductor codegen wrapper_fxir FxConverter replace_floor_div WrapperFxCodegen torch _inductor test_case TestCase InductorTestCase torch export Dim torch testing _internal common_utils DeterministicGuard instantiate_parametrized_tests parametrize torch testing _internal inductor_utils GPU_TYPE HAS_GPU requires_gpu TRITON_HAS_CPU torch utils _sympy functions FloorDiv try test_control_flow CondModels except ImportError test_control_flow CondModels manual=fbcode caffe test inductor control_flow-library HAS_GPU triton triton language tl torch testing _internal triton_utils add_kernel_ d_autotuned test_config = compile_threads alignment_asserts False size_asserts False scalar_asserts False nan_asserts False requires_gpu config patch test_config instantiate_parametrized_tests FxirTestCase InductorTestCase device = GPU_TYPE _count_ops gm torch fx GraphModule target Callable - int len gm graph find_nodes op= call_function target=target _run_and_capture_graphs opt args - torch fx GraphModule gms = orig_generate = FxConverter generate generate - torch fx GraphModule nonlocal gms gm = orig_generate gms append gm gm unittest mock patch object torch _inductor codegen wrapper_fxir FxConverter generate generate opt args gms _compile_and_check func args expected_num_triton_kernels int = metadata_only bool = False compile_kwargs Optional dict = None compile_kwargs None compile_kwargs = opt = torch compile func compile_kwargs Get FX graph backend gms = _run_and_capture_graphs opt args Check code triton kernels num_kernels = sum _count_ops gm triton_kernel_wrapper_mutation gm gms assertEqual num_kernels expected_num_triton_kernels Check accuracy result = opt args ref = func args metadata_only When we only want check metadata fill zeros tensor data ref result = tuple pytree tree_map torch zeros_like x x ref result assertTrue same ref result gms classmethod setUpClass cls super setUpClass Register FX backend storing default later common init_backend_registration cls _default_backend = common device_codegens cls device common register_backend_for_device cls device TritonScheduling WrapperFxCodegen classmethod tearDownClass cls super tearDownClass Restore default backend common device_codegens cls device = cls _default_backend test_basic args = torch randn device=self device _ range _compile_and_check torch add args test_multiple_kernels foo x y x sum + y sum args = torch randn length device=self device length _compile_and_check foo args expected_num_triton_kernels= test_free Test program frees buffer which no longer use foo x y z w = x sum + y z sum + w sum args = torch randn length device=self device length gm = _compile_and_check foo args expected_num_triton_kernels= Check generated code frees num_frees = gm code count = None assertGreater num_frees test_extern Test program calls extern kernel foo x y x y + y sum args = torch randn size device=self device size gm = _compile_and_check foo args expected_num_triton_kernels= Check extern kernel num_extern = _count_ops gm torch ops aten addmm out assertEqual num_extern test_fallback Test program calls aten fallbacks foo x batch = torch randn device=self device batch = torch randn device=self device torch addbmm x batch batch args = torch randn device=self device Since program has random output just check metadata Don t check exact value gm = _compile_and_check foo args expected_num_triton_kernels= metadata_only=True Check fallback kernel num_fallback = _count_ops gm torch ops aten randint low_out + _count_ops gm torch ops aten addbmm default assertEqual num_fallback test_cat_inputs Test concatenation graph inputs foo x y torch cat x y + args = torch randn device=self device _ range _compile_and_check foo args expected_num_triton_kernels= test_cat_views Test concatenation multiple kernels writing same buffer foo x y = x - b = y sum keepdim=True c = torch cat b clone b c args = torch randn device=self device _ range gm = _compile_and_check foo args expected_num_triton_kernels= get_offset node torch fx Node - int input_ shape stride offset = node args assert isinstance offset int offset Check views one which offset as_strided_nodes = list gm graph find_nodes op= call_function target=torch as_strided assertEqual len as_strided_nodes num_offset_views = sum get_offset node node as_strided_nodes assertEqual num_offset_views test_cat_to_alloc Test concatenation s optimized out allocation length = foo x y z = tuple torch arange length device=self device _ range x + torch cat y z args = torch randn length device=self device gm = _compile_and_check foo args expected_num_triton_kernels= Expect single allocation even though eager mode would use num_allocs = _count_ops gm torch empty_strided assertEqual num_allocs test_cat_reinterpret_view Test torch cat using ReinterpretView length = foo x y z = tuple torch randn length device=self device _ range x + torch cat y z args = torch randn length device=self device Since test generates random numbers check metadata only gm = _compile_and_check foo args expected_num_triton_kernels= metadata_only=True Check as_strided We map ReinterpretView num_as_strided = _count_ops gm torch as_strided assertEqual num_as_strided test_reshape_output Test reshaping output which maps ReinterpretView foo x y torch reshape x + y args = torch randn device=self device _ range gm = _compile_and_check foo args expected_num_triton_kernels= Check as_strided We map ReinterpretView num_as_strided = _count_ops gm torch as_strided assertEqual num_as_strided test_extern_multi_output Test extern kernel multiple outputs Also test graph multiple outputs foo x top idx = torch topk x top + idx args = torch randn device=self device gm = _compile_and_check foo args expected_num_triton_kernels= Check multiple kernel outputs via getitems num_getitems = _count_ops gm operator getitem assertEqual num_getitems Check multiple graph outputs output_node = gm graph find_nodes op= output assertEqual len output_node args test_duplicate_input Test duplicated inputs This will collapse into single input GM args = torch randn device=self device gm = _compile_and_check torch add args expected_num_triton_kernels= num_placeholders = len gm graph find_nodes op= placeholder assertEqual num_placeholders test_backward Test program backward pass x = torch ones device=self device input tensor y = torch zeros device=self device expected output w = torch randn requires_grad=True device=self device b = torch randn requires_grad=True device=self device foo x y z = torch matmul x w + b loss = torch nn functional binary_cross_entropy_with_logits z y loss backward w grad b grad Expect separate forward backward graphs forward_gm backward_gm = _compile_and_check foo x y expected_num_triton_kernels= test_custom_compiler Test derived backend custom compiler offset = CustomWrapperCodegen WrapperFxCodegen compile_graph gm compiled_fn args Adds offset program s outputs outputs = gm args pytree tree_map lambda x x + outputs compiled_fn args = torch randn device=self device _ range custom_backend = common DeviceCodegen TritonScheduling CustomWrapperCodegen None unittest mock patch dict common device_codegens device custom_backend func = torch add opt = torch compile func result = opt args Check output offset eager mode ref = func args assertFalse same result ref assertNotEqual offset assertTrue same result - offset ref test_dynamic_shapes_and_strides Test graph dynamic shapes strides static_dims = get_input full_size = full = torch randn full_size device=self device view = torch as_strided full static_dims full stride view func = torch add args = get_input _ range gm = _compile_and_check func args compile_kwargs= dynamic True Check symbolic output shape empty_strided = gm graph find_nodes op= call_function target=torch empty_strided example_tensor = empty_strided meta val symbolic_dims = example_tensor shape assertEqual len symbolic_dims len static_dims Check symbolic output strides stride one = example_tensor stride assertEqual one sympy S One Find size symbols check corresponding placeholders defining them symbol itertools chain symbolic_dims stride assertTrue isinstance symbol torch SymInt placeholder = node node gm graph find_nodes op= placeholder node name == str symbol assertEqual placeholder meta val symbol parametrize shape torch _inductor config patch pad_dynamic_shapes True comprehensive_padding True padding_alignment_bytes pad_outputs True test_dynamic_shapes_with_padding shape Test graph dynamic shapes padding get_input shape pad_size = list shape pad_size - = shape - + pad = torch randn pad_size dtype=torch float device=self device view = torch as_strided pad shape pad stride view args = get_input shape _ range gm = _compile_and_check torch add args compile_kwargs= dynamic True Check symbolic output shape empty_strided = gm graph find_nodes op= call_function target=torch empty_strided example_tensor = empty_strided meta val symbolic_dims = example_tensor shape symbolic_strides = example_tensor stride align_elems = args dtype itemsize expected_strides = _ range len shape i range len shape - - expected_strides i - = align_elems expected_strides i symbolic_dims i + align_elems - align_elems i j zip symbolic_strides expected_strides assertEqual i j test_dynamic_shapes_precomputed_size Test dynamic shapes where kernel s size arg precomputed func = torch add args = torch randn shape device=self device shape gm = _compile_and_check func args compile_kwargs= dynamic True Check precomputed size arg triton_node = gm graph find_nodes op= call_function target=triton_kernel_wrapper_mutation assertIn ks triton_node kwargs kwargs test_dynamic_launch_grid_calc Test dyanmic launch grid calculation func = torch add args = torch randn shape device=self device shape gm = _compile_and_check func args compile_kwargs= dynamic True Check precomputed size arg triton_node = gm graph find_nodes op= call_function target=triton_kernel_wrapper_mutation assertIn grid triton_node kwargs assertIn xnumel triton_node kwargs kwargs assertIn XBLOCK triton_node kwargs kwargs grid = triton_node kwargs grid xnumel = triton_node kwargs kwargs xnumel meta val xblock = triton_node kwargs kwargs XBLOCK assertEqual grid meta val - -xnumel xblock assertEqual grid assertEqual grid config patch trace enabled True unittest mock patch torch _inductor debug DebugFormatter output_code test_debug mock_output_code Compile debug mode args = torch randn device=self device _ range _compile_and_check torch sub args Check output code Triton kernel call mock_output_code assert_called_once output_filename = mock_output_code call_args args open output_filename f output_code = f read assertIn triton_kernel_wrapper_mutation output_code parametrize const test_export_const_placeholder const Test we can compile graph coming torch export constant input TestModule torch nn Module forward x y x - y args = torch randn device=self device const mod = TestModule export_gm = torch export export mod args module compile_module inps torch _inductor compile export_gm inps inductor_gm = _run_and_capture_graphs compile_module args result = inductor_gm args ref = mod args assertTrue same ref result test_scatter_fallback_scalar_src Test special case where ScatterFallback takes scalar src argument foo input_ dim = src = torch ops aten scatter input_ dim index src length = index = torch randint length length device=self device input_ = torch randn length device=self device DeterministicGuard True gm = _compile_and_check foo input_ Check fallback op num_fallback = _count_ops gm torch ops aten scatter_ value assertEqual num_fallback test_index_put_fallback Test deterministic fallback index_put length = out values = torch randn length device=self device _ range indices = torch randint length length device=self device accumulate = True DeterministicGuard True gm = _compile_and_check torch index_put out indices values accumulate expected_num_triton_kernels= Check fallback op assertEqual _count_ops gm torch ops aten index_put_ default test_scatter_reduce_fallback Test customized wrapper codegen ScatterFallback ops fallback_op = torch ops aten scatter_reduce_ two foo out index src dim = out = fallback_op out dim index src reduce= amax include_self=False out + length = out src = torch randn length device=self device _ range index = torch randint length length device=self device gm = _compile_and_check foo out index src expected_num_triton_kernels= Check fallback assertEqual _count_ops gm fallback_op parametrize pred False True test_cond_subgraph pred bool Test model subgraphs foo pred x torch cond pred torch cos torch sin x + x = torch randn device=self device pred_tensor = torch tensor pred device=self device gm = _compile_and_check foo pred_tensor x expected_num_triton_kernels= - Check subgraphs subgm_getattrs = list gm graph find_nodes op= get_attr assertEqual len subgm_getattrs subgm_getattr subgm_getattrs target = subgm_getattr name assertTrue isinstance getattr gm target torch fx GraphModule parametrize pred False True test_cond_no_operands pred bool Test torch cond when subgraphs take no inputs length = true_fn torch zeros length device=self device false_fn true_fn + foo pred torch cond pred true_fn false_fn pred_tensor = torch tensor pred device=self device _compile_and_check foo pred_tensor expected_num_triton_kernels= test_cpp_raises Test C++ CPU backend C++ kernels yet supported so now check we get expected exception foo x y x + y device = torch device cpu args = torch randn device=device _ range cpp_backend = common DeviceCodegen CppScheduling WrapperFxCodegen None unittest mock patch dict common device_codegens device type cpp_backend assertRaisesRegex BackendCompilerFailed Triton _compile_and_check foo args parametrize enable_tuning False True parametrize use_dynamic_shapes False True test_autotune use_dynamic_shapes bool enable_tuning bool orig_run = torch _inductor runtime triton_heuristics CachingAutotuner run called = False run args kwargs nonlocal called called = True orig_run args kwargs args = torch randn device=self device _ range config patch triton autotune_at_compile_time enable_tuning unittest mock patch object torch _inductor runtime triton_heuristics CachingAutotuner run run Compile check tuner called assertFalse called gm = _compile_and_check torch mul args compile_kwargs= dynamic use_dynamic_shapes assertEqual called enable_tuning Check symbolic output shape empty_strided = gm graph find_nodes op= call_function target=torch empty_strided shape stride = empty_strided args use_dynamic_shapes assertEqual type shape torch fx Node test_custom_triton triton jit add_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask add x torch Tensor y torch Tensor - torch Tensor output = torch empty_like x n_elements = output numel grid meta triton cdiv n_elements meta BLOCK_SIZE add_kernel grid x y output n_elements BLOCK_SIZE= output args = torch randn device=self device _ range _compile_and_check add args test_output_slice_view Test when output view input The sliced strides create TensorBox output IR foo x x T squeeze args = torch rand device=self device _compile_and_check foo args expected_num_triton_kernels= test_fallback_tuple_constant_arg Test fallback op tuple constant argument Check tuple arguments flattened during codegen foo x permute tuple argument torch permute x Use complex force permute become fallback op args = torch randn dtype=torch complex device=self device gm = _compile_and_check foo args expected_num_triton_kernels= Check fallback kernel permute num_fallback = _count_ops gm torch ops aten permute default assertEqual num_fallback Verify permute node has correct tuple argument permute_node = next iter gm graph find_nodes op= call_function target=torch ops aten permute default The second argument should permutation Check s flattened perm_arg = permute_node args assertIsInstance perm_arg list Permutation argument should flattened assertEqual len perm_arg assertEqual tuple perm_arg instantiate_parametrized_tests AOTFxirTestCase InductorTestCase device = GPU_TYPE check model inp dynamic_shapes=None strict=False - torch fx GraphModule device == xpu raise unittest SkipTest The feature AOTFxir currently ready XPU torch no_grad ep = torch export export model inp dynamic_shapes=dynamic_shapes strict=strict gm = torch _inductor aot_compile ep module inp options= fx_wrapper True test_config assertTrue same model inp gm inp node gm graph nodes node op == call_function node target = triton_kernel_wrapper_mutation assertTrue node meta get val None None gm test_aoti_fx_add M torch nn Module forward x y x + y inp = torch ones device=self device torch ones device=self device check M inp test_aoti_fx_const M torch nn Module __init__ device super __init__ device = device = torch nn Parameter torch ones device=self device b = torch ones device=self device forward x y x + y + + b + torch tensor device=self device inp = torch ones device=self device torch ones device=self device check M device inp test_aoti_fx_linear M torch nn Module __init__ super __init__ linear = torch nn Linear forward x linear x inp = torch ones device=self device check M device inp test_aoti_fx_dynamic M torch nn Module __init__ super __init__ forward x y x + y inp = torch ones device=self device torch ones device=self device check M device=self device inp dynamic_shapes= Dim DYNAMIC Dim DYNAMIC test_custom_triton_autotune_dynamic Model torch nn Module forward x y output = torch zeros_like x x_elements = output size y_elements = output size grid meta triton cdiv x_elements meta BLOCK_SIZE_X triton cdiv y_elements meta BLOCK_SIZE_Y add_kernel_ d_autotuned grid x y output x_elements y_elements output num_dims = dims = num_dims x = torch randn dims device=self device y = torch randn dims device=self device dim _x = Dim dim _x min= max= dim _y = Dim dim _y min= max= dynamic_shapes = x dim _x y dim _y check Model device=self device x y dynamic_shapes=dynamic_shapes strict=True test_custom_backend Test registering custom FX backend called = False CustomWrapperCodegen WrapperFxCodegen compile_graph gm Simply records whether override called nonlocal called called = True super compile_graph gm M torch nn Module forward x x + Register custom FX backend custom_backend = common DeviceCodegen TritonScheduling PythonWrapperCodegen fx_wrapper_codegen=CustomWrapperCodegen unittest mock patch dict common device_codegens device custom_backend The backend should have been called yet assertFalse called inp = torch randn device=self device check M device inp Now backend should have been called assertTrue called parametrize expr Dim x + Dim x min= - test_dynamic_input_expr expr sympy Expr Test dynamic shapes nontrivial input expression M torch nn Module forward x x reshape x shape x shape + x shape dynamic_shapes = x expr inp = torch randn device=self device gm = check M device inp dynamic_shapes=dynamic_shapes Check dynamic size ops assertEqual len gm graph find_nodes op= call_function target=torch ops aten sym_size int parametrize pred False True test_cond_multi_inputs_and_outputs pred Test torch cond check output graphs M torch nn Module forward pred x y true_fn x y torch tanh x torch relu y false_fn x y tuple t t true_fn x y torch cond pred true_fn false_fn x y pred = torch tensor True device=self device x y = torch randn device=self device _ range gm = check M pred x y Check graph assertExpectedInline gm code strip \ forward arg _ arg _ arg _ true_graph_ = true_graph_ false_graph_ = false_graph_ cond = torch ops higher_order cond arg _ true_graph_ false_graph_ arg _ arg _ arg _ = true_graph_ = false_graph_ = arg _ = arg _ = None buf = cond buf = cond cond = None buf buf noqa B test_dims_dynamic_outer_static_padded_inner Test padding inner dimensions dynamic outer dimensions M torch nn Module forward x y x + y get_input_padded_inner shape full_shape = shape - + shape - full = torch randn full_shape dtype=torch float device=self device view = torch as_strided full shape full stride view shape = args = tuple get_input_padded_inner shape _ range check M args dynamic_shapes= Dim DYNAMIC Dim DYNAMIC Dim STATIC parametrize length test_cond_dynamic_shape_pred_scalar_closure length int Test cond using predicate computed dynamic shapes Also test dynamic scalar computed outside branches M torch nn Module forward x y z = x reshape - = y shape true_fn x x + false_fn x true_fn x torch cond x shape true_fn false_fn z x y = torch randn shape device=self device shape length length dynamic_shapes = x Dim DYNAMIC y Dim DYNAMIC check M x y dynamic_shapes=dynamic_shapes test_dynamic_scalar_output Test output scalar dynamic shapes M torch nn Module forward x x shape x = torch randn device=self device check M x dynamic_shapes= Dim DYNAMIC parametrize dynamic False True parametrize input_ False test_item input_ dynamic bool Test calling Tensor item M torch nn Module forward x x item x = torch tensor input_ d = Dim s min= dynamic_shapes = d dynamic None check M x dynamic_shapes=dynamic_shapes parametrize pred False True test_mismatched_branch_dynamic pred bool Test cond branches mismatched dynamic shapes Apply offset guarantee truith predicate pred_offset = pred - inputs = torch tensor pred device=self device + torch randn device=self device + pred_offset _ range dim _a = Dim s min= max= dim _b = Dim s min= max= dynamic_shapes = p x dim _a None y dim _b None z dim _a None check CondModels MismatchedOutputSize tuple inputs dynamic_shapes=dynamic_shapes test_const_folded_subgraph If graph only contains call_module node subgraph where subgraph can const-folded away validate fake mode used FXConverter generation None device = device shape = Submodule torch nn Module forward torch randn shape device=device + Create parent graph module subgraph output ep = torch export export Submodule parent_graph = torch fx Graph call_mod = parent_graph call_module sub args= get_item = parent_graph call_function operator getitem args= call_mod slice None parent_graph output get_item parent = torch fx GraphModule sub ep module parent_graph Verify FXConverter generate uses non-null fake mode Intercept _set_node_metadata_hook ensure fake_mode None orig_set_hook = torch _inductor codegen wrapper_fxir _set_node_metadata_hook called = False mock_set_hook gm torch fx GraphModule fn nonlocal called called = True Please update check ` fake_mode ` no longer used FXConverter call _node_metadata_hook assertTrue fake_mode fn keywords assertIsNotNone fn keywords fake_mode orig_set_hook gm fn assertFalse called unittest mock patch object torch _inductor codegen wrapper_fxir _set_node_metadata_hook mock_set_hook args = compiled = torch _inductor aot_compile parent args options= fx_wrapper True assertTrue called compiled_out = compiled args assertEqual compiled_out shape shape TestReplaceFloorDiv InductorTestCase Tests floor - FloorDiv conversion _check expr sympy Expr - sympy Expr Check we started floor s num_floors = expr count sympy floor assertGreater num_floors replaced = replace_floor_div expr Check all floor s replaced We should have no more new FloorDiv s than floor s original expression although we can have less due simplification assertEqual replaced count sympy floor assertLessEqual replaced count FloorDiv - expr count FloorDiv num_floors expand_floor_div numerator sympy Expr denominator sympy Expr - sympy Expr sympy floor numerator denominator Expand FloorDiv back into floor check equality assertEqual sympy simplify e replace FloorDiv expand_floor_div e replaced expr replaced test_rewrite_floor_div_mul_pow x y = sympy symbols x y expr = sympy floor x y assertEqual expr count FloorDiv assertEqual expr count sympy core mul Mul assertEqual expr count sympy Pow rewritten = _check expr assertTrue isinstance rewritten FloorDiv assertEqual rewritten args x y test_rewrite_floor_div_mul_rational x = sympy Symbol x expr = sympy floor x assertEqual expr count FloorDiv assertEqual expr count sympy core mul Mul assertEqual expr count sympy Rational rewritten = _check expr assertTrue isinstance rewritten FloorDiv assertEqual rewritten args x test_no_rewrite_div x y = sympy symbols x y expr = x y assertEqual expr count FloorDiv rewritten = replace_floor_div expr assertEqual rewritten expr test_rewrite_floor_div_nested x y = sympy symbols x y expr = sympy floor sympy floor x + y assertEqual expr count FloorDiv rewritten = _check expr assertEqual rewritten count FloorDiv test_rewrite_floor_div_rational_const expr = sympy floor sympy S One evaluate=False assertEqual expr count FloorDiv assertEqual expr count sympy Mul assertEqual expr count sympy Rational Expression evaluates compile time constant rewritten = _check expr assertEqual rewritten sympy S Zero test_no_distribute_mul_floordiv Test multiplication doesn t distribute floor division x = sympy Symbol x expr = sympy floor x rewritten = _check expr assertEqual rewritten count sympy Mul assertEqual rewritten count FloorDiv test_rational_multi_pows Test expression rational multiple pows x y z = sympy symbols x y z expr = sympy floor x y z mul = expr args assertTrue isinstance mul sympy Mul assertTrue isinstance mul args sympy Rational assertEqual expr count sympy Pow rewritten = _check expr assertEqual rewritten count FloorDiv test_variable_exp Test pow when exponent variable x = sympy Symbol x positive=True expr = sympy floor -x replaced = _check expr Check x went denominator assertEqual replaced args x test_launch_grid_dynamic_padding Test complex launch grid expression arising padding dynamic shapes x y = sympy symbols x y expr = sympy floor -FloorDiv x y FloorDiv -x y _check expr __name__ == __main__ torch _inductor test_case run_tests HAS_GPU TRITON_HAS_CPU run_tests needs= filelock