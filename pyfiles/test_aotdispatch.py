Owner s oncall pt Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree copy itertools unittest warnings collections abc Callable contextlib ContextDecorator ExitStack nullcontext functools partial wraps typing Any Optional Union unittest mock patch common_utils decorate decorateForModules saved_tensors_hooks_to_gm skip skipOps xfail torch torch _dynamo torchdynamo torch nn nn torch nn functional F torch utils _pytree pytree functorch grad jacrev make_fx vjp vmap functorch compile aot_function aot_module aot_module_simplified compiled_function compiled_module default_decompositions default_partition get_aot_compilation_context make_boxed_compiler make_boxed_func memory_efficient_fusion min_cut_rematerialization_partition nnc_jit nop functorch experimental control_flow torch _decomp decomposition_table torch _dynamo testing normalize_gm torch _dynamo utils counters torch _functorch _aot_autograd autograd_cache AOTAutogradCache torch _functorch aot_autograd _aot_export_function aot_export_joint_simple aot_export_module SerializableAOTDispatchCompiler torch _higher_order_ops out_dtype out_dtype torch _inductor codecache compiled_fx_graph_hash torch _inductor custom_graph_pass CustomPartitionerFn torch _inductor output_code MockFXGraphCacheOutput torch _subclasses fake_tensor DynamicOutputShapeException FakeTensorMode torch fx experimental proxy_tensor is_sym_node torch fx experimental symbolic_shapes GuardOnDataDependentSymNode ShapeEnv torch nn attention flex_attention flex_attention torch nn utils rnn PackedSequence torch testing FileCheck torch testing _internal common_cuda SM OrLater torch testing _internal common_device_type instantiate_device_type_tests ops tol toleranceOverride torch testing _internal common_methods_invocations op_db torch testing _internal common_modules module_db modules torch testing _internal common_utils compare_equal_outs_and_grads instantiate_parametrized_tests IS_ARM IS_MACOS IS_WINDOWS IS_X outs_and_grads parametrize run_tests skipIfRocm TEST_MKL TestCase xfail_inherited_tests xfailIfTorchDynamo torch testing _internal custom_tensor ConstantExtraMetadataTensor torch testing _internal hop_db hop_db torch testing _internal optests _test_aot_autograd_forwards_backwards_helper aot_autograd_check torch testing _internal subclasses WrapperSubclass torch testing _internal two_tensor TwoTensor TwoTensorMode torch utils _python_dispatch TorchDispatchMode USE_TORCHVISION = False try torchvision USE_TORCHVISION = True except ImportError warnings warn Couldn t torchvision Some our tests use try install commands pytorch org post-fixed ` -- no-deps ` avoid overwriting pytorch installation UserWarning USE_NETWORKX = False try networkx noqa F USE_NETWORKX = True except ImportError warnings warn Some tests use networkx installed UserWarning NB numpy testing dependency amax_to_scale amax torch Tensor float _dtype torch dtype round_scales_to_power_of_ bool = False amax = amax torch float res = torch finfo float _dtype max torch clamp amax min= e- res = res torch float res Must module level use fx wrap torch fx wrap _pack_fp _with_scale_wrap x x dtype is_floating_point x amax = torch max torch abs x scale = amax_to_scale amax torch float _e m x_scaled = x torch float scale x_fp = x_scaled torch float _e m x dtype scale x_fp torch fx wrap _unpack_fp _with_scale_wrap x isinstance x torch Tensor x dtype scale x_fp = x y = x_fp torch float scale y dtype torch fx wrap _pack_fp _wrap x x dtype is_floating_point x x dtype x torch float _e m torch fx wrap _unpack_fp _wrap x isinstance x torch Tensor x dtype tensor = x tensor dtype pack_fp x _pack_fp _wrap x unpack_fp packed _unpack_fp _wrap packed pack_fp _with_scale x _pack_fp _with_scale_wrap x unpack_fp _with_scale packed _unpack_fp _with_scale_wrap packed AOTTestCase TestCase pass TestPythonKey AOTTestCase test_make_fx device f x torch sin x inp = torch randn fx_f = make_fx f inp new_inp = torch randn assertEqual fx_f new_inp f new_inp test_make_fx_grad device f x torch sin x sum inp = torch randn f = grad f fx_f = make_fx f inp new_inp = torch randn assertEqual fx_f new_inp f new_inp test_scalar_device device f b + b inps = torch randn device=device torch tensor fx_f = make_fx f inps assertEqual fx_f inps f inps test_make_fx_vmap device f x torch sin x inp = torch randn f = vmap f fx_f = make_fx f inp new_inp = torch randn assertEqual fx_f new_inp f new_inp test_make_fx_jacrev device f x x sin sum inp = torch randn f = jacrev jacrev f fx_f = make_fx f inp new_inp = torch randn assertEqual fx_f new_inp f new_inp test_make_fx_vjp device f x torch sin x sum primals = torch randn _ vjp_fn = vjp f primals cotangent = torch randn fx_f = make_fx vjp_fn cotangent True True new_cotangent = torch randn assertEqual fx_f new_cotangent True True vjp_fn new_cotangent test_make_fx_functionalize device functorch experimental functionalize fn = relu_ = torch randn device=device symbolic_gm = torch fx symbolic_trace fn includes_method_relu_ = any str n target == relu_ n symbolic_gm graph nodes assertTrue includes_method_relu_ Also verifies fix https github com pytorch pytorch issues gm = make_fx functionalize symbolic_gm includes_aten_relu = any n target == torch ops aten relu default n gm graph nodes assertTrue includes_aten_relu test_make_fx_no_decompose device FIXME skipTest error maximum recursion reached f x torch tanh x sum fx_f = make_fx grad f torch randn ops = i target i fx_f graph nodes assertEqual torch ops aten tanh_backward ops True fx_f = make_fx grad f decomposition_table torch randn ops = i target i fx_f graph nodes assertEqual torch ops aten tanh_backward ops False test_nnc_jit device f x torch sin x jit_f = nnc_jit f inp = torch randn assertEqual jit_f inp f inp test_nnc_scalar device f x torch sin x jit_f = nnc_jit f inp = torch randn assertEqual jit_f inp f inp test_nnc_pytrees device f x torch sin x jit_f = nnc_jit f inp = torch randn assertEqual jit_f inp f inp test_external_calls device f b torch mv b jit_f = nnc_jit f inp = torch randn torch randn assertEqual jit_f inp f inp test_nnc_passthrough device f x y x + y y inp = torch randn torch randn jit_f = nnc_jit f assertEqual jit_f inp f inp f x x = x x inp = torch randn b torch randn jit_f = nnc_jit f assertEqual jit_f inp f inp unittest skipIf USE_TORCHVISION test requires torchvision test_resnet _backward_trace device mod = torchvision models resnet f x out = mod x out sum backward grad mod parameters inp = torch randn requires_grad=True grads = f inp mod zero_grad mod inp sum backward grads = grad mod parameters assertEqual grads grads get_base t t _base t _is_view t is_in_base t maybe_tensors t_base = get_base t maybe_tensor maybe_tensors isinstance maybe_tensor torch Tensor t_base get_base maybe_tensor True False skipIfDynamoInput reason Skip TestAOTAutograd running dynamo input decorator func wraps func wrapper args kwargs isinstance TestAOTAutogradWithDynamo skipTest f Skipping _testMethodName TestAOTAutogradWithDynamo because reason func args kwargs wrapper decorator TestAOTAutograd AOTTestCase run_autograd f Callable fw_graph_cell list Optional Callable decompositions Optional dict keep_input_mutations bool dynamic bool Runs aot_autograd specified settings f isinstance f nn Module compiled_f = aot_module f fw_compiler=make_boxed_compiler partial extract_graph graph_cell=fw_graph_cell bw_compiler=nop decompositions=decompositions keep_inference_input_mutations=keep_input_mutations dynamic=dynamic compiled_f = aot_function f fw_compiler=make_boxed_compiler partial extract_graph graph_cell=fw_graph_cell bw_compiler=nop decompositions=decompositions keep_inference_input_mutations=keep_input_mutations dynamic=dynamic compiled_f test_mutation will - Ensure inputs non-leaves so our graphs can mutate them - try mutate outputs graph ensure autograd meta set properly outputs patch functorch compile config debug_assert True verify_aot_autograd f inp_ Union Callable list Any test_mutation bool = False keep_inp_mutations bool = False decompositions Optional dict = None dynamic bool = False Only active when inp_ Callable TODO probably consolidate all tests make inp Callable make_inputs_subclasses bool = False make_inputs inp_ Some tests pass callable inp generate inputs useful we want generate complicated aliasing inputs isinstance inp_ Callable inp_callable = inp_ The callable should tuple f_inputs f_graph_inputs The idea we might want compile function graph inputs test autograd backprop all way through actual inputs TwoTensorMode make_inputs_subclasses nullcontext inp graph_inps = inp_callable inp = Our input clones need mimic when inputs duplicates one another dupes_map = i x enumerate inp_ x dupes_map x_dupe_idx = dupes_map x inp append inp x_dupe_idx dupes_map x = i isinstance x torch Tensor x_copy = x x_copy = x detach clone requires_grad_ x requires_grad x requires_grad x is_leaf x_copy = x_copy clone inp append x_copy test_mutation For graphs where we mutate inputs need our test make sure inputs aren t leaves graph_inps = x add x inp graph_inps = inp inp graph_inps check_results ref_results test_results ref_graph_inps test_graph_inps ref_inp test_inp ref_out ref_grad = ref_results test_out test_grad = test_results assertEqual ref_grad test_grad isinstance ref_out torch Tensor assertTrue isinstance test_out torch Tensor ref_out test_out = ref_out test_out ref_o test_o zip ref_out test_out isinstance ref_o torch Tensor assertEqual ref_o requires_grad test_o requires_grad assertEqual ref_o is_leaf test_o is_leaf ref_is_view_of_non_interm = is_in_base ref_o ref_graph_inps is_in_base ref_o ref_out test_is_view_of_non_interm = is_in_base test_o test_graph_inps is_in_base test_o test_out assertEqual ref_is_view_of_non_interm test_is_view_of_non_interm assertEqual ref_o test_o test_mutation This tests autograd meta set properly output we can mutate ref_o add_ test_o add_ assertEqual ref_o test_o Reverse modification ref_o sub_ test_o sub_ assertEqual ref_o test_o ref_i test_i zip ref_inp test_inp isinstance ref_i torch Tensor assertEqual ref_i requires_grad test_i requires_grad assertEqual ref_i test_i keep_input_mutations True keep_inp_mutations True False inp graph_inps = make_inputs inp_ test_inp test_graph_inps = make_inputs inp_ fw_graph_cell = None compiled_f = run_autograd f fw_graph_cell decompositions keep_input_mutations dynamic ref_results = outs_and_grads f graph_inps inp test_results = outs_and_grads compiled_f test_graph_inps test_inp check_results ref_results test_results graph_inps test_graph_inps inp test_inp isinstance TestAOTAutogradWithCache When testing cache run compiled_f second time cached_inp cached_graph_inps = make_inputs inp_ cached_results = outs_and_grads compiled_f cached_graph_inps cached_inp check_results ref_results cached_results graph_inps cached_graph_inps inp cached_inp fw_graph_cell test_non_tensor_and_none_inputs int None Tensor f b c c inp = None torch ones dtype=torch float requires_grad=True verify_aot_autograd f inp inp = None torch ones dtype=torch float requires_grad=False verify_aot_autograd f inp test_single_output f b + b inp = torch randn requires_grad=True torch randn verify_aot_autograd f inp inp = torch randn requires_grad=False torch randn verify_aot_autograd f inp test_multi_output f b + b - b inp = torch randn requires_grad=True torch randn verify_aot_autograd f inp inp = torch randn requires_grad=False torch randn verify_aot_autograd f inp test_multi_output_list f b + b - b inp = torch randn requires_grad=True torch randn verify_aot_autograd f inp inp = torch randn requires_grad=False torch randn verify_aot_autograd f inp Test bug occurring intersection fake tensors functionalization test_squeeze_mutation f b = clone squeeze - b add_ + b inp = torch randn requires_grad=True verify_aot_autograd f inp dynamic=True inp = torch randn requires_grad=False verify_aot_autograd f inp dynamic=True test_complex_linear https github com pytorch pytorch issues inp = torch randn dtype=torch complex F torch nn Module __init__ - None super __init__ linear = nn Linear dtype=torch complex forward x linear x sum abs verify_aot_autograd F inp test_embedding_bag_view_dynamic Backwards pass tries wrap sparse tensor FunctionalTensorWrapper test works even though sparse tensor has no storage F torch nn Module __init__ - None super __init__ emb = torch nn EmbeddingBag sparse=True forward x y emb x y view - x = torch arange y = torch arange verify_aot_autograd F x y dynamic=False verify_aot_autograd F x y dynamic=True test_input_mutation_simple f mul_ inp = torch ones requires_grad=True fw_graph = verify_aot_autograd f inp test_mutation=True inp = torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True Things note - extra clone because we need pass pre-mutated input grad autograd operates above functionalization so we need manually clone Hopefully backends can optimize easily - The extra arg because compiled forward returns mutated inputs + outputs assertExpectedInline fw_graph code strip \ forward primals_ clone = torch ops aten clone default primals_ primals_ = None mul = torch ops aten mul Tensor clone clone = None mul_ = torch ops aten mul Tensor mul mul mul_ test_input_mutation_set__input_mutation f b = torch arange dtype=a dtype reshape torch no_grad set_ b b inp = torch ones requires_grad=True verify_aot_autograd f inp test_mutation=True keep_inp_mutations=True inp = torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True keep_inp_mutations=True test_set__steals_view_chain f b a_ = mul b_ = b mul b_slice = b_ view a_clone should inherit view chain b_slice a_ set_ b_slice Also mutates b_ a_ view - mul_ a_ b_slice inp = torch ones requires_grad=False torch zeros requires_grad=False verify_aot_autograd f inp keep_inp_mutations=True _compile_autocast device forward_autocast torch library _scoped_library mylib FRAGMENT m m define foo Tensor x - Tensor m impl foo torch clone CompositeExplicitAutograd autocast x x + m impl foo autocast AutocastCPU m impl foo autocast AutocastCUDA foo = torch ops mylib foo default Foo torch autograd Function staticmethod forward ctx x ctx save_for_backward x foo x staticmethod backward ctx grad x = ctx saved_tensors grad foo x fn x torch amp autocast device enabled=False Foo apply x x = torch tensor device=device requires_grad=True forward_autocast torch amp autocast device torch _dynamo config patch recompile_limit= out = torch compile fn fullgraph=True backend= aot_eager x torch _dynamo config patch recompile_limit= out = torch compile fn fullgraph=True backend= aot_eager x grad = torch autograd grad out x out grad torch _functorch config patch backward_pass_autocast= same_as_forward test_backward_pass_autocast_on devices = cpu torch cuda is_available devices append cuda device devices out grad = _compile_autocast device forward_autocast=True assertEqual out torch zeros_like out assertEqual grad torch ones_like grad torch _functorch config patch backward_pass_autocast= off test_backward_pass_autocast_off devices = cpu torch cuda is_available devices append cuda device devices out grad = _compile_autocast device forward_autocast=True assertEqual out torch zeros_like out assertEqual grad torch zeros_like grad torch _functorch config patch backward_pass_autocast= off test_backward_pass_autocast_custom devices = cpu torch cuda is_available devices append cuda device devices torch _functorch config patch backward_pass_autocast= device_type device out grad = _compile_autocast device forward_autocast=False assertEqual out torch zeros_like out assertEqual grad torch ones_like grad skipIfDynamoInput Test doesn t make sense dynamo which changes order mutations test_set__and_data_mutation_good f b The data mutation happens after set_ This ok see graph below torch no_grad set_ b b mul_ + b inp = torch ones requires_grad=True torch ones requires_grad=True fw_graph = verify_aot_autograd f inp test_mutation=True keep_inp_mutations=True inp = torch ones requires_grad=False torch zeros requires_grad=False verify_aot_autograd f inp test_mutation=True keep_inp_mutations=True Important things note - set_ b desugars into b - Both b recorded experiencing mutations which why we see b_updated output mul twice graph outputs recorded both data mutation metadata mutation due set_ swapping its storage - runtime epilogue set_ mul - runtime epilogue b b copy_ mul assertExpectedInline fw_graph code strip \ forward primals_ primals_ mul = torch ops aten mul Tensor primals_ add = torch ops aten add Tensor mul mul set_ = torch ops aten set_ source_Tensor primals_ mul primals_ = set_ = None copy_ = torch ops aten copy_ default primals_ mul primals_ = mul = copy_ = None add This hopefully extremely rare case difficult handle so we ban https github com pytorch pytorch issues https github com pytorch pytorch pull xfailIfTorchDynamo test_set__and_data_mutation_bad f a_view = view - tmp = torch ones requires_grad=True Now any mutations either tmp will tracked graph input mutations torch no_grad set_ tmp BAD a_view now detached every graph input so we won t recognize caused input mutation a_view mul_ + tmp inp = torch ones requires_grad=True assertRaisesRegex RuntimeError cannot mutate tensors frozen storage verify_aot_autograd f inp test_mutation=True keep_inp_mutations=True skipIfDynamoInput Test doesn t make sense dynamo which changes order mutations test_set__not_allowed f b torch no_grad set_ b Mutating will change s grad_fn which requires us replay mutation outside graph We currently ban today when input also received set_ input mutation mul_ + b inp = torch ones requires_grad=True torch ones requires_grad=True assertRaisesRegex AssertionError input has other mutations we cannot verify_aot_autograd f inp test_mutation=True keep_inp_mutations=True test_input_mutation_set__nop f b = torch arange dtype=a dtype a_old = torch ops aten alias default torch no_grad set_ b set_ a_old + b reshape inp = torch ones requires_grad=True fw_graph = verify_aot_autograd f inp test_mutation=True keep_inp_mutations=True inp = torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True keep_inp_mutations=True Things note - There no set_ calls graph we functionalize set_ b into b - There only graph output We properly realized two set_ calls undo each other so effectively no inputs mutated assertExpectedInline fw_graph code strip \ forward primals_ arange = torch ops aten arange default dtype = torch float device = device type= cpu pin_memory = False alias = torch ops aten alias default primals_ primals_ = None view = torch ops aten view default arange arange = None add = torch ops aten add Tensor alias view alias = view = None add test_input_mutation_simple_with_none_and_nontensor Tensor None int f b c c f_compiled = aot_function f nop req_grad True False inp = torch ones requires_grad=req_grad None out_ref = f inp out_test = f_compiled inp assertEqual out_ref out_test https github com pytorch pytorch issues test_mutates_input_noncontiguous f add_ f_compiled = aot_function f nop ref = torch ones requires_grad=True + ref_view = ref test = torch ones requires_grad=True + test_view = test out_ref = f ref_view noqa F out_test = f_compiled test_view noqa F assertEqual ref test test_input_mutation_modifies_autograd_meta_of_aliases f mul_ out = + out detach x_ref = torch ones requires_grad=True clone x_ref_view = x_ref view x_test = torch ones requires_grad=True clone x_test_view = x_test view f_compiled = aot_function f nop keep_inference_input_mutations=True f x_ref f_compiled x_test f will mutate aliases input including its autograd metadata y grad_fn AsStridedBackward assertEqual x_ref_view x_test_view assertEqual x_ref_view _version x_test_view _version assertEqual x_ref_view grad_fn __class__ x_test_view grad_fn __class__ Test actual gradients correct x_ref x_ref_view sum backward x_test x_test_view sum backward assertEqual x_ref grad x_test grad assertEqual x_ref_view grad x_test_view grad test_nested_subclasses torch compile backend= aot_eager f x x sin cos = torch ones requires_grad=True = detach clone requires_grad_ = detach clone requires_grad_ = detach clone requires_grad_ aa = TwoTensor aa = TwoTensor aaaa = TwoTensor aa aa out = f aaaa assertTrue isinstance out TwoTensor assertTrue isinstance out TwoTensor assertTrue isinstance out b TwoTensor assertTrue isinstance out torch Tensor assertTrue isinstance out b torch Tensor assertTrue isinstance out b torch Tensor assertTrue isinstance out b b torch Tensor out sum backward assertTrue isinstance aaaa grad TwoTensor assertTrue isinstance aaaa grad TwoTensor assertTrue isinstance aaaa grad b TwoTensor test_nested_subclasses_non_nested_grad torch compile backend= aot_eager f x x sin cos = torch ones requires_grad=True = detach clone requires_grad_ = detach clone requires_grad_ = detach clone requires_grad_ new_aa = TwoTensor aa = TwoTensor aa = aa detach clone requires_grad_ aaaa = TwoTensor aa aa out = f new_aa new_out = out + aaaa assertRaisesRegex RuntimeError During backward we encountered tensor subclass where we guessed its metadata incorrectly noqa F new_out sum backward test_nested_subclasses_non_homogenous f x x_elem = x elem x_metadata = x constant_attribute x_metadata x_elem x sin cos = torch ones requires_grad=True = detach clone requires_grad_ = detach clone requires_grad_ = detach clone requires_grad_ aa = TwoTensor aa = TwoTensor custom_aa = ConstantExtraMetadataTensor aa custom_aa constant_attribute = custom_aa = ConstantExtraMetadataTensor aa custom_aa constant_attribute = out_eager = f custom_aa compiled_f = torch compile f backend= aot_eager out = compiled_f custom_aa assertTrue isinstance out TwoTensor assertTrue isinstance out ConstantExtraMetadataTensor assertTrue isinstance out b ConstantExtraMetadataTensor assertTrue torch allclose out_eager out out_eager sum backward out sum backward assertTrue torch allclose custom_aa grad custom_aa grad assertTrue isinstance custom_aa grad TwoTensor assertTrue isinstance custom_aa grad ConstantExtraMetadataTensor assertTrue isinstance custom_aa grad b ConstantExtraMetadataTensor test_subclasses_mixed f x y x_metadata = x constant_attribute out_a = x_metadata x y out_b = x y y b TwoTensor out_a out_b = torch ones requires_grad=False = clone custom_a = ConstantExtraMetadataTensor custom_a constant_attribute = custom_a = ConstantExtraMetadataTensor custom_a constant_attribute = b = torch ones requires_grad=False b = b clone b = b clone b = b clone bb = TwoTensor b b bb = TwoTensor b b out_eager = f custom_a bb compiled_f = torch compile f backend= aot_eager out = compiled_f custom_a bb assertTrue torch allclose out_eager out assertTrue isinstance out TwoTensor assertTrue isinstance out ConstantExtraMetadataTensor assertTrue isinstance out b ConstantExtraMetadataTensor test_subclasses_mixed_mode f x x sin cos AddConstantMetadataMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None out = func args kwargs ConstantExtraMetadataTensor types out = ConstantExtraMetadataTensor out out constant_attribute = out = torch ones requires_grad=True = detach clone requires_grad_ = detach clone requires_grad_ = detach clone requires_grad_ aa = TwoTensor aa = TwoTensor AddConstantMetadataMode out_eager = f aa compiled_f = torch compile f backend= aot_eager AddConstantMetadataMode out = compiled_f aa assertTrue isinstance out ConstantExtraMetadataTensor assertTrue isinstance out elem TwoTensor assertTrue torch allclose out_eager out out_eager sum backward out sum backward assertTrue torch allclose aa grad aa grad assertTrue isinstance aa grad ConstantExtraMetadataTensor assertTrue isinstance aa grad elem TwoTensor unittest skipIf IS_WINDOWS Windows isn t supported case test_custom_tensor_metadata f x x_elem = x elem x_elem_elem = x_elem elem x_elem_metadata = x_elem constant_attribute x x_elem x_elem_elem x_elem_metadata = torch ones requires_grad=True custom_a = ConstantExtraMetadataTensor custom_a constant_attribute = custom_aa = ConstantExtraMetadataTensor custom_a custom_aa constant_attribute = custom_aa_compile = custom_aa detach clone requires_grad_ custom_aa_compile elem constant_attribute = out_eager = f custom_aa compiled_f = torch compile f backend= aot_eager out = compiled_f custom_aa_compile assertTrue torch allclose out_eager out out sum backward assertTrue isinstance custom_aa_compile grad ConstantExtraMetadataTensor assertTrue isinstance custom_aa_compile grad elem ConstantExtraMetadataTensor test_nested_subclasses_complicated_inps f x y z temp = x + y temp_plain = x + y b res = temp sum + temp_plain sum x sin cos + res x = torch ones requires_grad=True x = x detach clone requires_grad_ xx = TwoTensor x x xx = xx detach clone requires_grad_ x_nested = TwoTensor xx xx x_nested_compile = x_nested detach clone requires_grad_ y_nested = x_nested detach clone requires_grad_ y_nested_compile = y_nested detach clone requires_grad_ z = x detach clone requires_grad_ z_compile = z detach clone requires_grad_ out_eager = f x_nested y_nested z compiled_f = torch compile f backend= aot_eager out = compiled_f x_nested_compile y_nested_compile z_compile assertTrue torch allclose out_eager out assertTrue isinstance out TwoTensor assertTrue isinstance out TwoTensor assertTrue isinstance out b TwoTensor assertTrue isinstance out torch Tensor assertTrue isinstance out b torch Tensor assertTrue isinstance out b torch Tensor assertTrue isinstance out b b torch Tensor out sum backward out_eager sum backward assertTrue isinstance x_nested_compile grad TwoTensor assertTrue isinstance x_nested_compile grad TwoTensor assertTrue isinstance x_nested_compile grad b TwoTensor assertTrue isinstance y_nested_compile grad TwoTensor assertTrue isinstance y_nested_compile grad TwoTensor assertTrue isinstance y_nested_compile grad b TwoTensor assertTrue torch allclose x_nested_compile grad x_nested grad assertTrue torch allclose x_nested_compile grad b x_nested grad b assertTrue torch allclose y_nested_compile grad y_nested grad assertTrue torch allclose y_nested_compile grad b y_nested grad b unittest skipIf IS_WINDOWS Windows isn t supported case test_nested_subclasses_complicated_inps_mixed f x y y_elem = y elem y_elem_elem = y_elem elem y_elem_metadata = y_elem constant_attribute y y_elem y_elem_elem y_elem_metadata + x x = torch ones requires_grad=True x = x detach clone requires_grad_ xx = TwoTensor x x xx = xx detach clone requires_grad_ x_nested = TwoTensor xx xx x_nested_compile = x_nested detach clone requires_grad_ = torch ones requires_grad=True custom_a = ConstantExtraMetadataTensor custom_a constant_attribute = custom_aa = ConstantExtraMetadataTensor custom_a custom_aa constant_attribute = custom_aa_compile = custom_aa detach clone requires_grad_ custom_aa_compile constant_attribute = custom_aa_compile elem constant_attribute = compiled_f = torch compile f backend= aot_eager out_eager = f x_nested custom_aa out = compiled_f x_nested_compile custom_aa_compile assertTrue torch allclose out_eager out out sum backward out_eager sum backward assertTrue torch allclose x_nested_compile grad x_nested grad assertTrue torch allclose custom_aa_compile grad custom_aa grad test_composite_impl_compile Foo torch nn Module __init__ - None super __init__ linear = torch nn Linear forward linear inp = torch ones requires_grad=True fw_graph = verify_aot_autograd Foo inp test_mutation=True inp = torch ones requires_grad=False assertExpectedInline fw_graph code strip \ forward primals_ primals_ primals_ t = torch ops aten t default primals_ primals_ = None addmm = torch ops aten addmm default primals_ primals_ t primals_ = None addmm primals_ t torch inference_mode fw_graph = verify_aot_autograd Foo inp test_mutation=True inp = torch ones requires_grad=False assertExpectedInline fw_graph code strip \ forward arg _ arg _ arg _ t = torch ops aten t default arg _ arg _ = None addmm = torch ops aten addmm default arg _ arg _ t arg _ = arg _ = t = None addmm test_outputs_are_aliased Tensor None int f b = mul c = b view - b c f_compiled = aot_function f nop req_grad True False inp = torch ones requires_grad=req_grad out_ref = f inp out_test = f_compiled inp assertEqual out_ref out_test assertEqual out_ref out_test Try mutating one outputs which aliased out_ref mul_ out_test mul_ Assert aliasing relationship preserved assertEqual out_ref out_test assertEqual out_ref out_test test_input_mutation_is_output f mul_ inp = torch ones requires_grad=True fw_graph = verify_aot_autograd f inp test_mutation=True inp = torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True assertExpectedInline fw_graph code strip \ forward primals_ clone = torch ops aten clone default primals_ primals_ = None mul = torch ops aten mul Tensor clone clone = None mul mul test_input_mutation_multiple f b c mul_ c mul_ + b + c create_inp req_grad torch ones requires_grad=req_grad torch ones requires_grad=req_grad torch ones requires_grad=req_grad verify_aot_autograd f create_inp False test_mutation=True fw_graph = verify_aot_autograd f create_inp True test_mutation=True assertExpectedInline fw_graph code strip \ forward primals_ primals_ primals_ clone = torch ops aten clone default primals_ primals_ = None clone_ = torch ops aten clone default primals_ primals_ = None mul = torch ops aten mul Tensor clone clone = None mul_ = torch ops aten mul Tensor clone_ clone_ = None add = torch ops aten add Tensor mul primals_ primals_ = None add_ = torch ops aten add Tensor add mul_ add = None mul mul_ add_ test_input_mutation_return f b torch sin out=b inp = torch randn torch ones fw_graph = verify_aot_autograd f inp test_mutation=True keep_inp_mutations=True assertExpectedInline fw_graph code strip \ forward arg _ arg _ sin = torch ops aten sin default arg _ arg _ = None copy_ = torch ops aten copy_ default arg _ sin arg _ = sin = None copy_ test_input_mutation_metadata f b transpose_ + b create_inp req_grad torch ones requires_grad=req_grad torch ones requires_grad=req_grad verify_aot_autograd f create_inp True test_mutation=True verify_aot_autograd f create_inp False test_mutation=True parametrize backend aot_eager inductor parametrize view_replay_for_aliased_outputs False True parametrize dynamic_shapes False True test_alias_of_intermediate_detach backend view_replay_for_aliased_outputs dynamic_shapes patch torch _functorch config view_replay_for_aliased_outputs view_replay_for_aliased_outputs fn x x = x + = x transpose detach inp_fn t = torch ones requires_grad=True dynamic_shapes torch _dynamo mark_dynamic t torch _dynamo mark_dynamic t t x_ref = inp_fn y_ref = fn x_ref x = inp_fn y = torch compile fn backend=backend fullgraph=True x assertEqual y_ref y y y = y assertFalse y requires_grad assertTrue y requires_grad Check detach diff view points same intermediate tensor storage assertEqual y data_ptr y data_ptr assertTrue y _is_view sum y_ref sum backward sum y sum backward assertEqual x_ref grad x grad test_input_mutation_storage_resize_up f torch ops inductor resize_storage_bytes_ float bytes per element bytes == elements torch no_grad copy_ torch ones + inp = torch zeros requires_grad=True Input starts zero-size-storage inp untyped_storage resize_ fw_graph_cell = None compiled_f = aot_function f fw_compiler=make_boxed_compiler partial extract_graph graph_cell=fw_graph_cell bw_compiler=nop decompositions= keep_inference_input_mutations=True dynamic=False compiled_f inp Final functionalized graph has two mutation ops resize_ resize input tensor up copy_ fill resized input valid data assertExpectedInline fw_graph_cell code strip \ forward primals_ resize_storage_bytes_ = torch ops inductor resize_storage_bytes_ default primals_ resize_storage_bytes_ = None ones = torch ops aten ones default device = device type= cpu pin_memory = False copy = torch ops aten copy default primals_ ones ones = None add = torch ops aten add Tensor copy copy_ = torch ops aten copy_ default primals_ copy primals_ = copy = copy_ = None add test_input_mutation_storage_resize_down f out = sin torch ops inductor resize_storage_bytes_ out inp = torch zeros requires_grad=True fw_graph_cell = None compiled_f = aot_function f fw_compiler=make_boxed_compiler partial extract_graph graph_cell=fw_graph_cell bw_compiler=nop decompositions= keep_inference_input_mutations=True dynamic=False compiled_f inp Final functionalized graph has one mutation ops resize_ resize input tensor down Even though there technically data mutation input copy_ We don t include graph since final input size has zero storage assertExpectedInline fw_graph_cell code strip \ forward primals_ sin = torch ops aten sin default primals_ resize_storage_bytes_ = torch ops inductor resize_storage_bytes_ default primals_ resize_storage_bytes_ = None sin primals_ test_input_mutation_storage_resize_up_down f torch ops inductor resize_storage_bytes_ float bytes per element bytes == elements torch no_grad copy_ torch ones out = sin torch ops inductor resize_storage_bytes_ out inp = torch zeros requires_grad=True Input starts zero-size-storage inp untyped_storage resize_ fw_graph_cell = None compiled_f = aot_function f fw_compiler=make_boxed_compiler partial extract_graph graph_cell=fw_graph_cell bw_compiler=nop decompositions= keep_inference_input_mutations=True dynamic=False out = compiled_f inp Final graph has two interesting properties no resizes functional graph since two resizes cancel out final size zero no copy_ functional graph even though we copied data into input because input has no storage end graph execution so no data copy assertExpectedInline fw_graph_cell code strip \ forward primals_ ones = torch ops aten ones default device = device type= cpu pin_memory = False copy = torch ops aten copy default primals_ ones primals_ = ones = None sin = torch ops aten sin default copy sin copy skipped after confirming yf bdhirsh unittest skipIf True using set_ unsafely PT FSDP no longer uses set_ used test test_input_mutation_storage_resize_down_and_set_ Meant mimic ppFSDP TracableCreateParameter torch autograd Function staticmethod forward ctx tensor placeholder assert tensor requires_grad placeholder set_ tensor staticmethod backward ctx grad None grad grad flows placeholder f dummy_param param_shard simulate allgather torch no_grad allgather_param = torch cat param_shard param_shard simulate propagating grad state through dummy param using data allgather param dummy_param_with_grad_state = TracableCreateParameter apply noqa F allgather_param dummy_param out = dummy_param sin Resize out dummy param which now has allgather data torch ops inductor resize_storage_bytes_ dummy_param out Simulates local shard our param param_shard = torch zeros requires_grad=True The dummy zero-sized allgathered param autograd will actually compute gradients dummy_param = torch zeros requires_grad=True dummy_param untyped_storage resize_ fw_graph_cell = None compiled_f = aot_function f fw_compiler=make_boxed_compiler partial extract_graph graph_cell=fw_graph_cell bw_compiler=nop decompositions= keep_inference_input_mutations=True dynamic=False compiled_f dummy_param param_shard Important stuff point out We save cat backward input sin While original code dummy_param sin dummy_param actually contains ` cat ` tensor due set_ call We emit cat resize_storage_ graph After set_ cat actually data dummy_param which what we call resize_ assertExpectedInline fw_graph_cell code strip \ forward primals_ primals_ cat = torch ops aten cat default primals_ primals_ primals_ = None sin = torch ops aten sin default cat resize_storage_bytes_ = torch ops inductor resize_storage_bytes_ default cat resize_storage_bytes_ = None set_ = torch ops aten set_ source_Tensor primals_ cat primals_ = set_ = None sin cat test_input_mutation_storage_resize_before_set_ f torch no_grad torch ops inductor resize_storage_bytes_ set_ torch ones inp = torch zeros requires_grad=True compiled_f = aot_function f fw_compiler=nop bw_compiler=nop decompositions= keep_inference_input_mutations=True dynamic=False compiled_f inp test_input_mutation_storage_resize_not_supported f mul_ torch ops inductor resize_storage_bytes_ inp = torch zeros requires_grad=True assertRaisesRegex AssertionError input has other mutations we cannot compiled_f = aot_function f fw_compiler=nop bw_compiler=nop decompositions= keep_inference_input_mutations=True dynamic=False out = compiled_f inp test_input_output_aliase_custom_autograd_function Foo torch autograd Function staticmethod forward ctx x x staticmethod backward ctx gx gx f x Foo apply x inp = torch ones requires_grad=True verify_aot_autograd f inp test_mutation=False test_input_mutation_requires_grad_detach Here requires grad gets mutated so we append copy_ end graph Its mutation doesn t take part autograd though because we mutated detach d view Need make sure copy_ doesn t error doesn t participate autograd either f detach mul_ + inp = torch ones requires_grad=True verify_aot_autograd f inp test_mutation=False inp = torch ones requires_grad=True test_mutation=True will first do some compute inp so no longer autograd leaf time becomes graph input Good test both cases verify_aot_autograd f inp test_mutation=True test_input_mutation_hidden_from_autograd_aliasing f a_alias = view - torch no_grad a_alias mul_ + inp = torch ones requires_grad=True The important bit we detected input mutation safe include inside graph since under no_grad so all we need do use mark_dirty input bump VC fw_graph = verify_aot_autograd f inp test_mutation=True keep_inp_mutations=True assertExpectedInline fw_graph code strip \ forward primals_ view = torch ops aten view default primals_ - mul = torch ops aten mul Tensor view view = None view_ = torch ops aten view default mul mul = None add = torch ops aten add Tensor view_ copy_ = torch ops aten copy_ default primals_ view_ primals_ = view_ = copy_ = None add test_input_mutation_requires_grad_no_grad f torch no_grad mul_ + inp = torch ones requires_grad=True fw_graph = verify_aot_autograd f inp test_mutation=True keep_inp_mutations=True Even though input requires_grad we expect keep input mutation graph Even though training graph assertExpectedInline fw_graph code strip \ forward primals_ mul = torch ops aten mul Tensor primals_ add = torch ops aten add Tensor mul copy_ = torch ops aten copy_ default primals_ mul primals_ = mul = copy_ = None add test_input_mutation_requires_grad_no_grad_inference_graph f torch no_grad mul_ + inp = torch ones requires_grad=True Even though input requires_grad we expect keep input mutation graph fw_graph = verify_aot_autograd f inp test_mutation=True keep_inp_mutations=True assertExpectedInline fw_graph code strip \ forward arg _ mul = torch ops aten mul Tensor arg _ add = torch ops aten add Tensor mul copy_ = torch ops aten copy_ default arg _ mul arg _ = mul = copy_ = None add test_input_mutation_requires_grad_no_grad_detach_mixed Perform mix mutations normal no_grad detach d tensor Only first should participate gradient computation f detach mul_ mul_ torch no_grad mul_ + inp = torch ones requires_grad=True verify_aot_autograd f inp test_mutation=True test_input_mutation_metadata f transpose_ mul_ + inp = torch ones requires_grad=True verify_aot_autograd f inp test_mutation=True inp = torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True test_input_mutation_batchnorm f inpt weight bias running_mean running_var This additionally good test because input tensors we mutate also saved backwards This tests what we save backward actually cloned inputs original inputs got mutated torch _native_batch_norm_legit inpt weight bias running_mean running_var True e- create_inp req_grad torch ones requires_grad=req_grad torch ones requires_grad=req_grad torch ones requires_grad=req_grad torch ones torch ones torch _decomp get_decompositions This simulates what inductor does running fw + bw decompositions decompositions = get_decompositions torch ops aten _native_batch_norm_legit_functional torch ops aten native_batch_norm_backward verify_aot_autograd f create_inp True test_mutation=True decompositions=decompositions verify_aot_autograd f create_inp False test_mutation=True decompositions=decompositions test_batchnorm_inference inp = torch ones requires_grad=True torch ones requires_grad=True torch ones requires_grad=True torch ones torch ones m = torch nn BatchNorm d m eval fw_graph_cell = None inp = torch ones fw_graph_cell = None compiled_m = aot_module m fw_compiler=partial extract_graph graph_cell=fw_graph_cell bw_compiler=nop keep_inference_input_mutations=True inp = torch ones torch no_grad compiled_m inp expectation there no copy_ calls decomposed batch norm when running under training=False eval mode code = fw_graph_cell code strip assertTrue copy_ str code test_input_output_view_simple f view - inp = torch ones requires_grad=False add verify_aot_autograd f inp test_mutation=True inp = torch ones requires_grad=True add fw_graph = verify_aot_autograd f inp test_mutation=True Outputs alias inputs pulled out graph entirely so we don t compile anything here assertExpectedInline fw_graph code strip \ forward arg _ view = torch ops aten view default arg _ - arg _ = None view test_input_output_view_mutate_multiple f b c mul_ c mul_ b view c view create_inp req_grad torch ones requires_grad=req_grad add torch ones requires_grad=req_grad add torch ones requires_grad=req_grad add verify_aot_autograd f create_inp False test_mutation=True fw_graph = verify_aot_autograd f create_inp True test_mutation=True The original function returned two outputs both which aliased inputs We expect two outputs functional graph a_updated c_updated The actual aliased outputs themselves aren t compiled forward graph Instead they re generated outside graph assertExpectedInline fw_graph code strip \ forward primals_ primals_ primals_ clone = torch ops aten clone default primals_ primals_ = None clone_ = torch ops aten clone default primals_ primals_ = None mul = torch ops aten mul Tensor clone clone = None mul_ = torch ops aten mul Tensor clone_ clone_ = None view = torch ops aten view default primals_ primals_ = None view_ = torch ops aten view default mul_ mul mul_ view view_ test_input_output_view_metadata_mutate_multiple f b c b mul_ c t_ view b view c view create_inp req_grad torch ones requires_grad=req_grad add torch ones requires_grad=req_grad add torch ones requires_grad=req_grad add verify_aot_autograd f create_inp False test_mutation=True fw_graph = verify_aot_autograd f create_inp True test_mutation=True Important thing check here three inputs Only b mul_ should show up graph we functionalize Everything does show up graph includes - The metadata mutation c we do outside graph - All original fw outputs which aliases inputs we regenerate them outside graph assertExpectedInline fw_graph code strip \ forward primals_ primals_ primals_ clone = torch ops aten clone default primals_ primals_ = None view = torch ops aten view default primals_ primals_ = None mul = torch ops aten mul Tensor clone clone = None t = torch ops aten t default view view = None view_ = torch ops aten view default primals_ primals_ = None view_ = torch ops aten view default t view_ = torch ops aten view default mul mul t view_ view_ view_ test_input_mutation_and_output_view f add_ view - inp = torch ones requires_grad=False add verify_aot_autograd f inp test_mutation=True inp = torch ones requires_grad=True add fw_graph = verify_aot_autograd f inp test_mutation=True Here total outputs because - num_mutated_inps = a_updated - num_fw_outputs = output alias input so we move outside compiled fw assertExpectedInline fw_graph code strip \ forward primals_ clone = torch ops aten clone default primals_ primals_ = None add = torch ops aten add Tensor clone clone = None view_ = torch ops aten view default add - add view_ test_input_mutation_output_view_multiple f b c d b transpose_ c add_ d + b diagonal + c create_inp req_grad torch arange requires_grad=req_grad dtype=torch float view add torch arange requires_grad=req_grad dtype=torch float view add torch ones requires_grad=req_grad add torch ones requires_grad=req_grad add verify_aot_autograd f create_inp False test_mutation=True fw_graph = verify_aot_autograd f create_inp True test_mutation=True assertExpectedInline fw_graph code strip \ forward primals_ primals_ primals_ primals_ view = torch ops aten view default primals_ primals_ = None clone = torch ops aten clone default primals_ primals_ = None transpose = torch ops aten transpose int view view = None add = torch ops aten add Tensor clone clone = None add_ = torch ops aten add Tensor primals_ primals_ = None diagonal = torch ops aten diagonal default transpose add_ = torch ops aten add Tensor primals_ add primals_ = None transpose add add_ diagonal add_ test_output_aliases_intermediate_single f out = torch mul out view - inp = torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True inp = torch ones requires_grad=True fw_graph = verify_aot_autograd f inp test_mutation=True In AOTAutograd we obligated make compiled forward directly ` out ` reconstruct ` out view - ` fresh output assertExpectedInline fw_graph code strip \ forward primals_ mul = torch ops aten mul Tensor primals_ primals_ = None view = torch ops aten view default mul - mul = None view test_output_aliases_input_multi_output_view_should_raise_autograd_error f list unbind f _compiled = aot_function f nop inp = torch ones requires_grad=True clone inp = torch ones requires_grad=True clone inp = torch ones requires_grad=True clone assertRaisesRegex RuntimeError Such functions do allow output views out_test = f _compiled inp This raises runtime error autograd eager mode out_test mul_ assertRaisesRegex RuntimeError Such functions do allow output views out_test = f _compiled inp inp mul_ In eager mode we mutate tensor any multi-output-view aliases get their grad_fn replaced error nodes so accessing grad_fn should error out_test grad_fn assertRaisesRegex RuntimeError Such functions do allow output views f _compiled inp out_test detach mul_ The above case also applies detached aliases they turn multi-output-view alias s grad_fns into error nodes out_test grad_fn test_output_aliases_input_multi_output_view All aliased outs multi-output views so AOTAutograd will hide aliasing autograd f list unbind inp = torch ones requires_grad=True inp_ref = torch ones requires_grad=True f _compiled = aot_function f nop out_ref = f inp_ref out_test = f _compiled inp Assert we get CompiledFunctionBackward backward graph AsStridedBackward No view-regeneration necessary mult-output view case See Note AOTAutograd differentiable outputs alias each other multi-output view call assertTrue all CompiledFunctionBackward str o grad_fn o out_test sum out_ref sum backward sum out_test sum backward assertEqual inp_ref grad inp grad Several outputs multi-output views However they part same alias set view out shape which both user-visible AOTAutograd will try smart here hide aliasing relationships autograd Instead will perform its output aliases input logic regenerate all aliases f list unbind view shape inp = torch ones requires_grad=True inp_ref = torch ones requires_grad=True f _compiled = aot_function f nop inp_ref_clone = inp_ref clone inp_clone = inp clone out_ref = f inp_ref_clone out_test = f _compiled inp_clone assertTrue all UnbindBackward str o grad_fn o out_test The last output multi-output view so autograd will let us mutate out_ref - mul_ out_test - mul_ Also mutate input which should affect aliased output inp_ref_clone view - mul_ inp_clone view - mul_ Do backward inp_ref + out_ref - sum backward inp + out_test - sum backward assertEqual inp_ref grad inp grad test_output_aliases_intermediate_multi_output_view All aliased outs multi-output views so AOTAutograd will hide aliasing autograd f out = torch mul list out unbind inp = torch ones requires_grad=True inp_ref = torch ones requires_grad=True f _compiled = aot_function f nop out_ref = f inp_ref out_test = f _compiled inp Assert we get CompiledFunctionBackward backward graph AsStridedBackward No view-regeneration necessary mult-output view case See Note AOTAutograd differentiable outputs alias each other multi-output view call assertTrue all CompiledFunctionBackward str o grad_fn o out_test sum out_ref sum backward sum out_test sum backward assertEqual inp_ref grad inp grad All aliased outs one multi-output views so AOTAutograd will hide aliasing autograd f out = torch mul list out unbind out inp = torch ones requires_grad=True inp_ref = torch ones requires_grad=True f _compiled = aot_function f nop out_ref = f inp_ref out_test = f _compiled inp Assert we get CompiledFunctionBackward backward graph AsStridedBackward No view-regeneration necessary mult-output view case See Note AOTAutograd differentiable outputs alias each other multi-output view call assertTrue all CompiledFunctionBackward str o grad_fn o out_test The last output multi-output view so autograd will let us mutate out_ref - mul_ out_test - mul_ out_ref - sum backward out_test - sum backward assertEqual inp_ref grad inp grad All aliased outs one multi-output views so AOTAutograd will hide aliasing autograd f out = torch mul list out unbind out view out shape inp = torch ones requires_grad=True inp_ref = torch ones requires_grad=True f _compiled = aot_function f nop out_ref = f inp_ref out_test = f _compiled inp Assert we get CompiledFunctionBackward backward graph AsStridedBackward No view-regeneration necessary mult-output view case See Note AOTAutograd differentiable outputs alias each other multi-output view call assertTrue all CompiledFunctionBackward str o grad_fn o out_test The last output multi-output view so autograd will let us mutate out_ref - mul_ out_test - mul_ out_ref - sum backward out_test - sum backward assertEqual inp_ref grad inp grad There outputs all alias each other them come multi-output views other ordinary aliases Therefore AOTAutograd will attempt multi-output-view optimization apply intermediate_base logic all aliases In theory we could probably get AOTAutograd only apply intermediate base logic last outputs first We should probably just do graph partitioning defined doc instead though https docs google com document d DlfFq TKbuAn zyJxLfoW-X qkkm PLdHFtySo QAk edit f out = torch mul also graph intermediate directly which will force AOTAutograd do intermediate base logic Why The user can mutate out which should change autograd metadata other aliased outputs list out unbind out out view out shape inp = torch ones requires_grad=True inp_ref = torch ones requires_grad=True f _compiled = aot_function f nop out_ref = f inp_ref out_test = f _compiled inp Mutate last output f autograd will allow since multi-output view long only non-multi-output views participate backward Note We could probably try hide only multi-output views autograd here only do intermediate base logic last two aliases Longer term solution graph partitioning probably cleaner though see note out_ref - mul_ out_test - mul_ out_ref_sum = out_ref - + out_ref - out_test_sum = out_test - + out_test - out_ref_sum sum backward out_test_sum sum backward assertEqual inp_ref grad inp grad test_output_aliases_intermediate_mutation_linear f x x + view - inp = torch ones requires_grad=True use inductor s decomps which will e g turn _unsafe_view into view torch _inductor decomposition decompositions f_compiled = aot_function f nop decompositions=decompositions out_ref = f inp out_test = f_compiled inp out_ref mul_ out_test mul_ assertEqual out_ref out_test test_output_aliases_intermediate_no_grad f b out = torch mul First output alias intermediate doesn t require grad out view - b add inp = torch ones torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True inp = torch ones torch ones requires_grad=True fw_graph = verify_aot_autograd f inp test_mutation=True important bit we don t bother generating intermediate base output graph because intermediate base itself didn t require gradients only problematic case when both base aliasesed output require gradients assertExpectedInline fw_graph code strip \ forward primals_ primals_ mul = torch ops aten mul Tensor primals_ primals_ = None view = torch ops aten view default mul - mul = None add = torch ops aten add Tensor primals_ primals_ = None view add test_output_aliases_intermediate_returned_multiple_times f out = torch mul out_view = out view - out out_view out inp = torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True inp = torch ones requires_grad=True verify_aot_autograd f inp test_mutation=True test_output_aliases_intermediate_multiple f out = torch mul AOTAutograd should manually generate these two output views epilogue out view - out view - inp = torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True inp = torch ones requires_grad=True fw_graph = verify_aot_autograd f inp test_mutation=True assertExpectedInline fw_graph code strip \ forward primals_ mul = torch ops aten mul Tensor primals_ primals_ = None view = torch ops aten view default mul - view_ = torch ops aten view default mul - view view_ mul test_output_aliases_intermediate_and_returned f out = torch mul AOTAutograd should manually generate first output view intermediate second which itself intermediate first out view - out inp = torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True inp = torch ones requires_grad=True fw_graph = verify_aot_autograd f inp test_mutation=True assertExpectedInline fw_graph code strip \ forward primals_ mul = torch ops aten mul Tensor primals_ primals_ = None view = torch ops aten view default mul - view mul test_output_aliases_intermediate_and_returned_flipped f out = torch mul AOTAutograd should manually generate first output view intermediate second which itself intermediate first out out view - inp = torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True inp = torch ones requires_grad=True fw_graph = verify_aot_autograd f inp test_mutation=True assertExpectedInline fw_graph code strip \ forward primals_ mul = torch ops aten mul Tensor primals_ primals_ = None view = torch ops aten view default mul - mul view test_output_aliases_intermediate_and_returned_different_grad f out = torch mul AOTAutograd should manually generate first output view intermediate second which itself intermediate first out view - out out detach inp = torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True inp = torch ones requires_grad=True fw_graph = verify_aot_autograd f inp test_mutation=True assertExpectedInline fw_graph code strip \ forward primals_ mul = torch ops aten mul Tensor primals_ primals_ = None view = torch ops aten view default mul - select = torch ops aten select int mul detach = torch ops aten detach default select select = None view mul detach test_output_aliases_intermediate_inplace_view f out = torch mul out t_ out TODO fix test See https github com pytorch pytorch issues verify_aot_autograd f inp test_mutation=True test_output_aliases_intermediate_inplace_view_with_detach f out = torch mul out t_ out detach_ Thanks detach_ AOT Autograd doesn t need do anything ` out ` will show up having OutputType non_alias _is_view == False out + inp = torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True inp = torch ones requires_grad=True fw_graph = verify_aot_autograd f inp test_mutation=True assertExpectedInline fw_graph code strip \ forward primals_ mul = torch ops aten mul Tensor primals_ t = torch ops aten t default mul mul = None add = torch ops aten add Tensor primals_ primals_ = None t add test_output_aliases_intermediate_inplace_view_and_view f out = torch mul out_view = out unsqueeze out t_ out_view = out unsqueeze out_view out out_view inp = torch ones requires_grad=True noqa F TODO fix test See github issue link verify_aot_autograd f inp test_mutation=True test_output_aliases_intermediate_multiple_mixed f out = torch mul out = torch mul AOTAutograd should manually generate these two output views epilogue out view - out transpose out transpose inp = torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True inp = torch ones requires_grad=True fw_graph = verify_aot_autograd f inp test_mutation=True assertExpectedInline fw_graph code strip \ forward primals_ mul = torch ops aten mul Tensor primals_ mul_ = torch ops aten mul Tensor primals_ primals_ = None view = torch ops aten view default mul - transpose = torch ops aten transpose int mul_ mul_ = None transpose_ = torch ops aten transpose int mul view transpose transpose_ mul test_output_all_alias_types There types aliasing require us metadata compiled fw outputs views inputs outputs views intermediates inputs get metadata mutations test all them here f transpose_ tmp = mul tmp squeeze tmp transpose unsqueeze inp_callable req_grad x = torch ones requires_grad=req_grad clone x x verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True fw_graph = verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True TODO make test run dynamic shapes so more meaningful metadata output order a_updated_meta out _meta out _meta out _meta assertExpectedInline fw_graph code strip \ forward primals_ view = torch ops aten view default primals_ primals_ = None transpose = torch ops aten transpose int view view = None mul = torch ops aten mul Tensor transpose squeeze = torch ops aten squeeze default mul transpose_ = torch ops aten transpose int mul unsqueeze = torch ops aten unsqueeze default transpose transpose squeeze transpose_ unsqueeze mul parametrize req_grad False True test_subclass_metadata_mutation req_grad f transpose_ tmp = mul tmp transpose inp_callable req_grad x = torch ones requires_grad=req_grad clone x x See https github com pytorch pytorch issues assertRaisesRegex RuntimeError Metadata mutations currently allowed tensor subclasses verify_aot_autograd f partial inp_callable req_grad=req_grad test_mutation=True make_inputs_subclasses=True test_input_data_and_metadata_mutation f t_ mul_ view shape inp = torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True inp = torch ones requires_grad=True fw_graph = verify_aot_autograd f inp test_mutation=True assertExpectedInline fw_graph code strip \ forward primals_ clone = torch ops aten clone default primals_ primals_ = None t = torch ops aten t default clone select = torch ops aten select int t t = None mul = torch ops aten mul Tensor select select = None t_ = torch ops aten t default clone clone = None select_scatter = torch ops aten select_scatter default t_ mul t_ = mul = None t_ = torch ops aten t default select_scatter select_scatter = None t_ = torch ops aten t default t_ t_ = torch ops aten t default t_ t_ = None view_ = torch ops aten view default t_ t_ = None t_ view_ test_view_and_inplace_view f b t_ b view b shape view shape create_inp req_grad torch ones requires_grad=req_grad torch ones requires_grad=req_grad verify_aot_autograd f create_inp False test_mutation=True fw_graph = verify_aot_autograd f create_inp True test_mutation=True assertExpectedInline fw_graph code strip \ forward arg _ arg _ t = torch ops aten t default arg _ arg _ = None view = torch ops aten view default arg _ arg _ = None view_ = torch ops aten view default t t view view_ test_view_detach f tmp = detach mul_ tmp inp = torch ones requires_grad=True verify_aot_autograd f inp test_mutation=True inp = torch ones requires_grad=False verify_aot_autograd f inp test_mutation=True test_input_inplace_requires_grad_true f b requires_grad_ True mul b mul inp = First inp doesn t require grad we switch torch ones requires_grad=False torch ones requires_grad=True fw_graph = verify_aot_autograd f inp test_mutation=True assertExpectedInline fw_graph code strip \ forward primals_ primals_ mul = torch ops aten mul Tensor primals_ primals_ = None mul_ = torch ops aten mul Tensor primals_ primals_ = None mul mul_ This torture test b get turned into synthetic base compiled graph One gets data mutation other gets metadata mutation We need make sure metadata mutation gets propagated back original input skipIfDynamoInput Dynamo removes runtime error test_input_data_and_metadata_mutation_aliases_other_input b aliased f b mul_ b t_ mul b inp_callable req_grad base = torch ones requires_grad=req_grad Note our test add important because we need graph inputs non-leaves so we can mutate them x = base add inp = x inp = x base inp inp verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True assertRaisesRegex RuntimeError Encountered aliased inputs mutated graph verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True make_inputs_subclasses=True assertRaisesRegex RuntimeError Encountered aliased inputs mutated graph verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True make_inputs_subclasses=True https github com pytorch pytorch issues test_input_mutation_noncontiguous f mul_ + inp_callable req_grad base = torch ones requires_grad=req_grad x = base add create non-contiguous view pass input compiler inp = x base inp verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True make_inputs_subclasses=True verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True make_inputs_subclasses=True test_backward_mutation_data BwMutation torch autograd Function staticmethod forward ctx x ctx save_for_backward x x clone staticmethod backward ctx grad_output x = ctx saved_tensors bw mutation x mul_ grad_output clone f b out = BwMutation apply b out inp_no_grad = torch ones requires_grad=True torch ones requires_grad=False Mutation buffer does require grad during backward allowed verify_aot_autograd f inp_no_grad test_mutation=True inp_grad = torch ones requires_grad=True torch ones requires_grad=True verify_aot_autograd f inp_grad test_mutation=True test_fw_bw_mutation_no_functionalization FwBwMutation torch autograd Function staticmethod forward ctx b input mutation torch _foreach_mul_ b x = b + intermediate mutation torch _foreach_mul_ x ctx save_for_backward x x staticmethod backward ctx grad_output x = ctx saved_tensors bw mutation torch _foreach_mul_ x grad_output x grad_output x f b FwBwMutation apply b inps = torch ones requires_grad=True torch ones requires_grad=False inps_ref = torch ones requires_grad=True torch ones requires_grad=False fw_graph = None bw_graph = None fw_compiler gm example_inputs fw_graph = gm gm bw_compiler gm example_inputs bw_graph = gm gm compiled_f = compiled_function f fw_compiler bw_compiler dynamic=False partition_fn=default_partition keep_inference_input_mutations=True disable_functionalization=True out_ref = f inps_ref out = compiled_f inps assertEqual out out_ref out_ref sum backward out sum backward assertEqual inps_ref grad inps grad important bit there mutations fw assertExpectedInline fw_graph code strip \ forward primals_ primals_ _foreach_mul_ = torch ops aten _foreach_mul_ ScalarList primals_ _foreach_mul_ = None add = torch ops aten add Tensor primals_ primals_ = None _foreach_mul__ = torch ops aten _foreach_mul_ ScalarList add _foreach_mul__ = None mul = torch ops aten mul Tensor add primals_ primals_ = None mul add important bit there mutation bw assertExpectedInline bw_graph code strip \ forward add tangents_ _foreach_mul__ = torch ops aten _foreach_mul_ ScalarList add _foreach_mul__ = None mul_ = torch ops aten mul Tensor tangents_ add tangents_ = add = None mul_ None test_fw_bw_mutation_no_functionalization FwBwMutation torch autograd Function staticmethod forward ctx x input mutation torch _foreach_mul_ x x = x + intermediate mutation torch _foreach_mul_ x ctx save_for_backward x x staticmethod backward ctx grad_output x = ctx saved_tensors bw mutation torch _foreach_mul_ x grad_output x f b out = FwBwMutation apply b out inps = torch ones requires_grad=True torch ones requires_grad=False inps_ref = torch ones requires_grad=True torch ones requires_grad=False fw_graph = None bw_graph = None fw_compiler gm example_inputs fw_graph = gm gm bw_compiler gm example_inputs bw_graph = gm gm compiled_f = compiled_function f fw_compiler bw_compiler dynamic=False partition_fn=default_partition keep_inference_input_mutations=True disable_functionalization=True out_ref = f inps_ref out = compiled_f inps assertEqual out out_ref out_ref sum backward out sum backward assertEqual inps_ref grad inps grad important bit there mutations fw mutation activation doesn t get moved bw assertExpectedInline fw_graph code strip \ forward primals_ primals_ _foreach_mul_ = torch ops aten _foreach_mul_ ScalarList primals_ _foreach_mul_ = None add = torch ops aten add Tensor primals_ primals_ = None _foreach_mul__ = torch ops aten _foreach_mul_ ScalarList add _foreach_mul__ = None mul = torch ops aten mul Tensor add primals_ primals_ = None mul add assertExpectedInline bw_graph code strip \ forward add tangents_ mul_ = torch ops aten mul Tensor tangents_ add tangents_ = add = None mul_ None test_backward_mutation_metadata BwMutation torch autograd Function staticmethod forward ctx b ctx save_for_backward b clone b clone staticmethod backward ctx grad_a grad_b b = ctx saved_tensors bw metadata mutation b transpose_ grad_a clone grad_b clone f b a_ b_ = BwMutation apply b out = a_ b_ out inp_no_grad = torch ones requires_grad=True torch ones requires_grad=False assertRaisesRegex AssertionError input had its metadata mutated backward verify_aot_autograd f inp_no_grad test_mutation=True test_backward_mutation_on_grad_out BwMutation torch autograd Function staticmethod forward ctx x x clone staticmethod backward ctx grad_output grad_output mul_ grad_output clone f b tmp = b out = BwMutation apply tmp out inp_grad = torch ones requires_grad=True torch ones requires_grad=True inp_grad_ref = torch ones requires_grad=True torch ones requires_grad=True f_compiled = aot_function f nop out = f_compiled inp_grad out mul sum backward out_ref = f inp_grad_ref out_ref mul sum backward assertEqual inp_grad grad inp_grad_ref grad assertEqual inp_grad grad inp_grad_ref grad test_backward_mutation_forward_inputs torch library custom_op _test _clone mutates_args= f x torch Tensor x torch Tensor - torch Tensor x clone f_fake x x torch empty_like x backward ctx grad torch no_grad ctx x zero_ grad None setup_context ctx inputs output x x = inputs ctx x = x ctx x = x f register_fake f_fake f register_autograd backward setup_context=setup_context fn x torch Tensor x torch Tensor x torch Tensor - torch Tensor x mul_ torch ops _test _clone x x + x inp_x inp_x inp_x = torch randn requires_grad=True torch randn requires_grad=False torch randn requires_grad=False ref_x ref_x ref_x = inp_x clone inp_x clone inp_x clone ref_y = fn ref_x ref_x ref_x compiled_f = aot_function fn nop keep_inference_input_mutations=True x x x = inp_x clone inp_x clone inp_x clone y = compiled_f x x x Verify mutation forward applied mutation backward forward assertEqual ref_x x assertEqual ref_x x assertEqual ref_x x assertEqual ref_y y ref_y sum backward y sum backward Verify mutations backward applied assertEqual ref_x x assertEqual ref_x x assertEqual ref_x x assertEqual ref_y y assertEqual ref_x grad x grad assertEqual ref_x grad x grad assertEqual ref_x grad x grad test_backward_mutation_forward_inputs_create_graph torch library custom_op _test _clone_create_graph mutates_args= f x torch Tensor x torch Tensor - torch Tensor x clone f_fake x x torch empty_like x backward ctx grad torch no_grad ctx x zero_ grad None setup_context ctx inputs output x x = inputs ctx x = x ctx x = x f register_fake f_fake f register_autograd backward setup_context=setup_context fn x torch Tensor x torch Tensor - torch Tensor torch ops _test _clone_create_graph x x inp_x inp_x = torch randn requires_grad=True torch randn requires_grad=True ref_x ref_x = inp_x clone inp_x clone ref_y = f ref_x ref_x ref_y sum backward x x = inp_x clone inp_x clone compiled_f = aot_function fn nop y = compiled_f x x loss = y sum assertRaisesRegex RuntimeError aot_autograd does support input mutations requires_grad backward create_graph=True torch autograd grad loss inp_x create_graph=True Not checking equality ref x Exception expected Partially addresses https github com pytorch pytorch issues test_input_mutation_false_aliasing f b mul_ b mul_ clone view - + b clone view - No overlap contiguous inp_callable req_grad base = torch ones requires_grad=req_grad x = base add create two views share storage actually non-overlapping = x b = x base b fw_graph = verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True make_inputs_subclasses=True verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True make_inputs_subclasses=True Important characteristic graph takes inputs That shows we didn t try run our complicated synthetic base logic because we successfully detected false aliasing across two inputs assertExpectedInline fw_graph code strip \ forward arg _ arg _ mul = torch ops aten mul Tensor arg _ arg _ = None mul_ = torch ops aten mul Tensor arg _ arg _ = None clone = torch ops aten clone default mul view = torch ops aten view default clone - clone = None clone_ = torch ops aten clone default mul_ view_ = torch ops aten view default clone_ - clone_ = None add = torch ops aten add Tensor view view_ view = view_ = None mul mul_ add No overlap non-contiguous first tensor ends before second tensor start inp_callable req_grad base = torch ones requires_grad=req_grad x = base add = x as_strided storage_offset= b = x as_strided storage_offset= base b No overlap non-contiguous tensors perfectly interleaved inp_callable req_grad base = torch ones requires_grad=req_grad x = base add = x b = x base b No overlap non-contiguous inp_callable req_grad base = torch ones requires_grad=req_grad x = base add = x as_strided storage_offset= b = x as_strided storage_offset= base b No overlap non-contiguous inp_callable req_grad base = torch ones requires_grad=req_grad x = base add = x as_strided storage_offset= b = x as_strided storage_offset= base b No overlap non-contiguous inp_callable req_grad base = torch ones requires_grad=req_grad x = base add s last element offset total elements = x as_strided storage_offset= b s first element offset no overlap b = x + numel base b overlap non-contiguous inp_callable_overlap req_grad base = torch ones requires_grad=req_grad x = base add = x as_strided storage_offset= b = x as_strided storage_offset= base b overlap non-contiguous inp_callable_overlap req_grad base = torch ones requires_grad=req_grad x = base add = x as_strided storage_offset= b = x as_strided storage_offset= base b overlap non-contiguous inp_callable_overlap req_grad base = torch ones requires_grad=req_grad x = base add s last element offset total elements = x as_strided storage_offset= b s first element offset overlap b = x + numel base b fw_graph = verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True fw_graph = verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True fw_graph = verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True fw_graph = verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True fw_graph = verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True fw_graph_overlap = verify_aot_autograd f partial inp_callable_overlap req_grad=False test_mutation=True fw_graph_overlap = verify_aot_autograd f partial inp_callable_overlap req_grad=False test_mutation=True All non-overlap graphs should same since we detected false aliasing assertEqual str fw_graph code str fw_graph code assertEqual str fw_graph code str fw_graph code assertEqual str fw_graph code str fw_graph code assertEqual str fw_graph code str fw_graph code assertEqual str fw_graph code str fw_graph code All overlap graphs should same since we detected real aliasing assertNotEqual str fw_graph code str fw_graph_overlap code assertNotEqual str fw_graph code str fw_graph_overlap code assertTrue as_strided_scatter str fw_graph_overlap code assertTrue as_strided_scatter str fw_graph_overlap code unittest skipIf torch cuda is_available CUDA unavailable test_mem_leak_from_save_for_bw See full diagnosis issue https github com pytorch pytorch issues Note Detaching saved tensors AOTAutograd This program creates ref-cycle Long term we should fix ref cycle since can arise naturally albeit rarely uses autograd Function But AOTAutograd makes more likely show up tracing user programs so we deal manually detaching tensors we save backward This completely wrong would give wrong results we do double backward Fortunately today double backward explicitly banned AOTAutograd f b add = + split = torch functional split add dim= getitem_ = split unsqueeze = getitem_ unsqueeze - mul = unsqueeze b getitem_ mul f_compiled = aot_function f nop inps = torch ones device= cuda requires_grad=True torch ones device= cuda requires_grad=True mem_before = torch cuda memory_allocated f_compiled inps mem_after = torch cuda memory_allocated assertTrue mem_after == mem_before test_output_aliases_multiple_inputs_get_correct_one b aliased have different shapes The first output should view off first input nd output should view off nd input f b view shape b view b shape inp_callable req_grad base = torch ones requires_grad=req_grad Note our test add important because we need graph inputs non-leaves so we can mutate them x = base mul inp = x view - inp = x base inp inp verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True make_inputs_subclasses=True verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True make_inputs_subclasses=True test_input_mutation_aliases_other_input f b add_ + b inp_callable req_grad base = torch ones requires_grad=req_grad Note our test add important because we need graph inputs non-leaves so we can mutate them x = base add inp = x inp = x base inp inp verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True fw_graph = verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True Important parts graph - compiled graph takes base we generate b views off base - clone still graph because we need call grad original non-mutated inputs - We re-generate views after clone preserve view relationships assertExpectedInline fw_graph code strip \ forward primals_ clone = torch ops aten clone default primals_ primals_ = None as_strided = torch ops aten as_strided default clone add = torch ops aten add Tensor as_strided as_strided = None as_strided_scatter = torch ops aten as_strided_scatter default clone add clone = add = None as_strided_ = torch ops aten as_strided default as_strided_scatter as_strided_ = torch ops aten as_strided default as_strided_scatter add_ = torch ops aten add Tensor as_strided_ as_strided_ as_strided_ = as_strided_ = None as_strided_scatter add_ noqa B test_input_mutation_aliases_other_input f b add_ + b inp_callable req_grad base = torch ones requires_grad=req_grad x = base add inp = x Here one aliased inputs base itself inp = x base inp inp verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True fw_graph = verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True assertExpectedInline fw_graph code strip \ forward primals_ clone = torch ops aten clone default primals_ primals_ = None as_strided = torch ops aten as_strided default clone add = torch ops aten add Tensor as_strided as_strided = None as_strided_scatter = torch ops aten as_strided_scatter default clone add clone = add = None as_strided_ = torch ops aten as_strided default as_strided_scatter as_strided_ = torch ops aten as_strided default as_strided_scatter add_ = torch ops aten add Tensor as_strided_ as_strided_ as_strided_ = as_strided_ = None as_strided_scatter add_ noqa B test_input_mutation_aliases_and_output_alias f b Here we need take care because b aliased since b aliased we generate view off updated b add_ b view b shape inp_callable req_grad base = torch ones requires_grad=req_grad x = base add base x view - x view - verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True fw_graph = verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True assertExpectedInline fw_graph code strip \ forward primals_ clone = torch ops aten clone default primals_ primals_ = None as_strided = torch ops aten as_strided default clone add = torch ops aten add Tensor as_strided as_strided = None as_strided_scatter = torch ops aten as_strided_scatter default clone add clone = add = None as_strided_ = torch ops aten as_strided default as_strided_scatter view_ = torch ops aten view default as_strided_ as_strided_ = None as_strided_scatter view_ noqa B test_input_aliased_with_mutation_output_alias f b c c alias c mul_ The main thing we re testing here We need reconstruct c view - rd input forward But we need careful do before converting aliased inputs into synthetic bases The original fw takes args compiled fw takes only args b add c view - inp_callable req_grad base = torch ones requires_grad=req_grad base = torch ones requires_grad=req_grad x = base add y = base add base base x view - y x view - verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True fw_graph = verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True assertExpectedInline fw_graph code strip \ forward primals_ primals_ clone = torch ops aten clone default primals_ primals_ = None as_strided_ = torch ops aten as_strided default clone mul = torch ops aten mul Tensor as_strided_ as_strided_ = None as_strided_scatter = torch ops aten as_strided_scatter default clone mul clone = mul = None add = torch ops aten add Tensor primals_ primals_ = None as_strided_ = torch ops aten as_strided default as_strided_scatter view_ = torch ops aten view default as_strided_ - as_strided_ = None as_strided_scatter add view_ noqa B test_input_metadata_mutation_aliases f b b alias we do metadata mutation Since we re mutating data then b isn t affected all We expect aot autograd bother constructing synthetic base t_ + b inp_callable req_grad base = torch ones requires_grad=req_grad x = base add base x view - x view - verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True fw_graph = verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True Expectation fwd takes args we don t construct synthetic base assertExpectedInline fw_graph code strip \ forward primals_ primals_ t = torch ops aten t default primals_ primals_ = None add = torch ops aten add Tensor t primals_ t = primals_ = None add test_input_mutation_aliases_and_none_require_gradients f b c b alias neither require gradients so they don t have _base aot autograd should construct synthetic base ` torch Tensor storage ` mul_ b + c + inp_callable req_grad base = torch ones c_arg = torch ones requires_grad=req_grad x = base add base c_arg x view - x view - c_arg verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True assertRaisesRegex RuntimeError tensor subclass This supported today verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True make_inputs_subclasses=True fw_graph = verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True assertExpectedInline fw_graph code strip \ forward primals_ primals_ as_strided = torch ops aten as_strided default primals_ mul = torch ops aten mul Tensor as_strided as_strided = None as_strided_scatter = torch ops aten as_strided_scatter default primals_ mul primals_ = mul = None as_strided_ = torch ops aten as_strided default as_strided_scatter add = torch ops aten add Tensor as_strided_ as_strided_ = None add_ = torch ops aten add Tensor primals_ primals_ = None as_strided_scatter add add_ noqa B skipIfDynamoInput Fails dynamo test_input_mutation_aliases_bases_out_of_order This tests our calling convention b d aliased then outer calling convention we send compiled forward becomes b_d_base c Importantly even though c alias our test neither inputs mutated So we don t need do base construction deconstruction f b c d b add_ d unsqueeze_ + c + d b view - inp_callable req_grad base = torch ones requires_grad=req_grad base = torch ones requires_grad=req_grad x = base add x = base add c alias b d alias base base x view - x view - x view - x view - verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True assertRaisesRegex RuntimeError Metadata mutations currently allowed tensor subclasses verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True make_inputs_subclasses=True fw_graph = verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True graph inputs b_d_base c returns b_updated a+c+d there original fw outs one view b so s part graph there also input mutations one metadata-only mutation so compiled forward doesn t assertExpectedInline fw_graph code strip \ forward primals_ primals_ primals_ clone = torch ops aten clone default primals_ primals_ = None as_strided = torch ops aten as_strided default clone add = torch ops aten add Tensor as_strided as_strided = None as_strided_scatter = torch ops aten as_strided_scatter default clone add clone = add = None as_strided_ = torch ops aten as_strided default as_strided_scatter unsqueeze = torch ops aten unsqueeze default as_strided_ as_strided_ = None add_ = torch ops aten add Tensor primals_ primals_ primals_ = primals_ = None add_ = torch ops aten add Tensor add_ unsqueeze add_ = None as_strided_ = torch ops aten as_strided default as_strided_scatter view_ = torch ops aten view default as_strided_ - as_strided_ = None as_strided_scatter add_ view_ unsqueeze noqa B unittest skipIf torch cuda is_available CUDA unavailable test_synthetic_base_base_attribute_is_none f b add_ + b inp_callable base = torch ones device= cuda detach so none inputs have _base attribute = base detach b = base detach base = torch ones requires_grad=True noqa F base b verify_aot_autograd f inp_callable test_mutation=True test_input_mutation_alias_everything Mondo test tests combination input mutated aliases another input so we make synthetic base output alias another output output alias intermediate c aliased f b c c mul_ mutates c b t_ metadata mutate b tmp = + c out = tmp view - out = b t out = out unsqueeze out out aliases intermediate alias each other out aliases input so we don t out out out inp_callable req_grad base = torch ones requires_grad=req_grad base = torch ones requires_grad=req_grad Note our test add important because we need graph inputs non-leaves so we can mutate them base _ = base add base _ = base add = base _ view - b = base _ c = base _ view - base base b c verify_aot_autograd f partial inp_callable req_grad=False test_mutation=True fw_graph = verify_aot_autograd f partial inp_callable req_grad=True test_mutation=True Expected - inputs forward synthetic_base_a_c b - output forward tmp out alias input will generated off b outside compiled fn out out aliases tmp we generate outside compiled function assertExpectedInline fw_graph code strip \ forward primals_ primals_ clone = torch ops aten clone default primals_ primals_ = None view = torch ops aten view default primals_ primals_ = None as_strided_ = torch ops aten as_strided default clone mul = torch ops aten mul Tensor as_strided_ as_strided_ = None as_strided_scatter = torch ops aten as_strided_scatter default clone mul clone = mul = None as_strided_ = torch ops aten as_strided default as_strided_scatter t = torch ops aten t default view view = None as_strided_ = torch ops aten as_strided default as_strided_scatter add = torch ops aten add Tensor as_strided_ as_strided_ as_strided_ = as_strided_ = None view_ = torch ops aten view default add - t_ = torch ops aten t default t unsqueeze = torch ops aten unsqueeze default view_ as_strided_scatter t view_ t_ unsqueeze add noqa B test_dynamic_shape_output_not_in_bw_graph f x x + x shape inp = torch ones requires_grad=True bw_graph_cell = None compiled_f = aot_function f fw_compiler=nop bw_compiler=partial extract_graph graph_cell=bw_graph_cell decompositions= keep_inference_input_mutations=False dynamic=True out = compiled_f inp out sum backward The important bit forward fn returns outputs one them symint so we should only see grad_output input backward graph Otherwise autograd will plumb None value grad_output which causes inductor complain assertExpectedInline bw_graph_cell code strip \ forward tangents_ tangents_ test_no_grad_input_output f b cos b cos b inp_thunks = lambda torch randn requires_grad=True lambda torch randn requires_grad=False inps itertools product inp_thunks repeat= inps = i i inps verify_aot_autograd f inps test_some_output_requires_grad_input_doesnt f b a_view = view - a_view requires_grad_ True a_view inp = torch randn torch randn requires_grad=True verify_aot_autograd f inp test_some_outputs_dont_require_grad_view f b detach b inp = torch randn requires_grad=True torch randn requires_grad=True verify_aot_autograd f inp test_some_outputs_dont_require_grad_non_view f b add detach b inp = torch randn requires_grad=True torch randn requires_grad=True verify_aot_autograd f inp test_inner_grad foo x y = torch exp x z = torch autograd grad y x z inps = torch randn requires_grad=True verify_aot_autograd foo inps test_grad_context foo x x inps = torch randn requires_grad=True graph_size = None get_graph_size fx_g _ nonlocal graph_size graph_size = len fx_g graph nodes fx_g f = aot_function foo nop get_graph_size torch set_grad_enabled False f inps assertIsNone graph_size f = aot_function foo nop get_graph_size torch set_grad_enabled True out = f inps assertIsNone graph_size out sum backward assertTrue graph_size test_output_dict f x x b x inp = torch randn requires_grad=True verify_aot_autograd f inp f x y x b y + x inp = torch randn requires_grad=True torch randn verify_aot_autograd f inp f x new_d = k x new_d k = x k new_d = torch randn requires_grad=True b = torch randn requires_grad=True inp_callable inps = b b inps inps verify_aot_autograd f inp_callable test_module mod = nn Sequential nn Linear nn ReLU compiled_mod = compiled_module mod nop nop inp = torch randn ref_out = mod inp ref_out sum backward ref_grads = sorted name p grad name p mod named_parameters out = compiled_mod inp out sum backward grads = sorted name p grad name p mod named_parameters assertEqual out grads ref_out ref_grads test_batchnorm mod = compiled_module nn BatchNorm d nop nop x = torch ones mod x sum backward test_list_codegen list_nop f _ g inps f inps g _boxed_call = True g f b c sin b cos c sin f = aot_function f list_nop inp = torch randn requires_grad=True _ range f inp sum backward patch torch _functorch aot_autograd AOT_COUNTER new_callable=itertools count test_compilation_context counter f x x sin sin count = compiler fx_g _ context = get_aot_compilation_context count append context len fx_g graph nodes fx_g f = aot_function f compiler out = f torch randn requires_grad=True f = aot_function f compiler f torch randn out sum backward assertExpectedInline str count _forward _inference _backward test_dupe_arg f x y x + y x = torch randn requires_grad=True verify_aot_autograd f x x test_dupe_arg_torture f x y x t_ y unsqueeze_ x + y x = torch randn requires_grad=True clone verify_aot_autograd f x x See https github com pytorch pytorch issues test_dupe_arg_returned_as_output f b a_ add_ a_ f_compiled = aot_function f nop = torch ones b = torch ones out_ref = f b = torch ones b = torch ones out_test = f_compiled b assertEqual out_ref out_test assertEqual patch torch _functorch aot_autograd AOT_COUNTER new_callable=itertools count patch torch _functorch config debug_assert True test_invalid_dupe_left_bias counter This test checks just because only first argument did metadata mutation we still correctly switch strategy deduplicate See https github com pytorch pytorch pull #discussion_r F torch nn Module forward x y x t_ x + y x = torch randn requires_grad=True clone y = torch randn requires_grad=True verify_aot_autograd F x x fxx = aot_module_simplified F x x nop assertExpectedRaisesInline AssertionError lambda fxx x y At compilation time graph compiled under assumption input would duplicate input runtime case This indicates guard bug AOTAutograd Dynamo please file bug PyTorch noqa B patch torch _functorch aot_autograd AOT_COUNTER new_callable=itertools count patch torch _functorch config debug_assert True test_invalid_dupe counter _test_invalid_dupe counter fake=False See Note Dynamo recompilation guarding invalid grad why test exists patch torch _functorch aot_autograd AOT_COUNTER new_callable=itertools count patch torch _functorch config debug_assert True test_invalid_dupe_fake counter _test_invalid_dupe counter fake=True _test_invalid_dupe counter fake F torch nn Module forward x y x unsqueeze_ y unsqueeze_ x + y x = torch randn requires_grad=True clone y = torch randn requires_grad=True clone fake shape_env = ShapeEnv fake_mode = FakeTensorMode shape_env=shape_env fake_x = fake_mode from_tensor x fake_y = fake_mode from_tensor y fake fxy = aot_module_simplified F fake_x fake_y nop fxy = aot_module_simplified F x y nop fxy x y x = torch randn requires_grad=True clone y = torch randn requires_grad=True clone fxy x x ok fake fxx = aot_module_simplified F fake_x fake_x nop fxx = aot_module_simplified F x x nop x = torch randn requires_grad=True clone y = torch randn requires_grad=True clone fxx x x Note This should raise Once we have guards place here we will have working correctly should recompile x = torch randn requires_grad=True clone y = torch randn requires_grad=True clone assertExpectedRaisesInline AssertionError lambda fxx x y At compilation time graph compiled under assumption input would duplicate input runtime case This indicates guard bug AOTAutograd Dynamo please file bug PyTorch noqa B patch torch _functorch aot_autograd AOT_COUNTER new_callable=itertools count patch torch _functorch config debug_assert True test_invalid_requires_grad counter _test_invalid_requires_grad counter fake=False See Note Dynamo recompilation guarding invalid grad why test exists patch torch _functorch aot_autograd AOT_COUNTER new_callable=itertools count patch torch _functorch config debug_assert True test_invalid_requires_grad_fake counter _test_invalid_requires_grad counter fake=True _test_invalid_requires_grad counter fake F torch nn Module forward x y x + y x = torch randn requires_grad=True y = torch randn requires_grad=True z = torch randn requires_grad=False fake shape_env = ShapeEnv fake_mode = FakeTensorMode shape_env=shape_env fake_x = fake_mode from_tensor x fake_y = fake_mode from_tensor y fake_z = fake_mode from_tensor z fake fxy = aot_module_simplified F fake_x fake_y nop fxy = aot_module_simplified F x y nop compare_equal_outs_and_grads F fxy x y compare_equal_outs_and_grads F fxy x z fake fxz = aot_module_simplified F fake_x fake_z nop fxz = aot_module_simplified F x z nop compare_equal_outs_and_grads F fxz x z assertExpectedRaisesInline AssertionError lambda fxz x y At compilation time graph compiled under assumption input would require grad runtime case This indicates guard bug AOTAutograd Dynamo please file bug PyTorch noqa B test_custom_autograd CustomFn torch autograd Function staticmethod forward ctx x x clone staticmethod backward ctx grad_output grad_output + f x CustomFn apply x verify_aot_autograd f torch randn unittest skipIf torch cuda is_available CUDA unavailable test_autocast_disable_guard torch _C _DisableAutocast x = torch rand cuda y = x x assertEqual y dtype torch float unittest skipIf torch cuda is_available CUDA unavailable test_nonidempotent_amp f self_s_emb add_ einsum_ = torch functional einsum ah th- t self_s_emb add_ log_softmax_ = einsum_ log_softmax - log_softmax_ args = torch rand dtype=torch float device= cuda torch rand dtype=torch float device= cuda torch cuda amp autocast enabled=True verify_aot_autograd f args args = e requires_grad_ True e args torch cuda amp autocast enabled=True verify_aot_autograd f args unittest skipIf torch cuda is_available CUDA unavailable unittest skipIf torch backends cudnn is_available CUDNN unavailable skipIfRocm https github com pytorch pytorch issues test_batch_norm_amp device = cuda input_dtype = torch float param_dtype = torch float weight bias = torch ones device=device dtype=param_dtype requires_grad=True _ range running_mean running_var = torch ones device=device dtype=param_dtype _ range bn x torch ops aten cudnn_batch_norm x weight bias running_mean running_var False e- inp = torch ones torch Size dtype=input_dtype device=device ref = bn inp cudnn_batch_norm_decomp = torch _decomp get_decompositions torch ops aten cudnn_batch_norm aot_fn = make_fx bn decomposition_table=cudnn_batch_norm_decomp inp res = aot_fn inp b zip ref res assert torch allclose b test_output_op_depending_on_symint It won t obvious reading test what s testing We should probably make into more focused unit test An issue following program expand op would end up depending symint whose proxy incorrectly associated one grad tensors rather than input tensors It broke partitioner logic net result aot_function failed produce function threw exception instead inp = torch randn requires_grad=True f x x expand x shape TODO whc make work test setup wrong somehow joint_forward_backward = create_joint_forward_backward f out = f inp joint_inputs = inp out detach contiguous fx_g = make_fx joint_forward_backward joint_inputs TODO assert outputs fwd graph trace correct symint e e test fails without symint clone fix af = aot_function f nop partition_fn=partial min_cut_rematerialization_partition compiler= inductor dynamic=True out = af inp assertEqual out f inp test_inference_mode m = torch nn Linear inp = torch randn aot_mod = aot_module m fw_compiler=nop torch inference_mode out_ref = m inp out_test = aot_mod inp assertEqual out_ref out_test test_default_partitioner_saves_symints_not_tensors_for_bw In test important thing primals_ only needed backward order grab its sizes We need assert what we save backward tensor s sizes tensor itself The way test set up will actually fail we try save input tensor backward Why b masked_fill_ c has backward requires knowing s sizes b masked_fill_ c also mutates because b aliased The autograd engine yells us we save backward then try mutate f b = c = torch ones_like b dtype=torch bool d = b masked_fill_ c d compiled_f = aot_function f nop dynamic=True inp_ref = torch ones requires_grad=True inp_test = torch ones requires_grad=True out_ref = f inp_ref clone out_test = compiled_f inp_test clone assertEqual out_ref out_test out_ref sum backward out_test sum backward assertEqual inp_ref grad inp_test grad test_buffer_copied_in_graph MyModel torch nn Module __init__ - None super __init__ buf = torch nn Buffer torch zeros w = torch nn Parameter torch zeros w = torch nn Parameter torch zeros forward x buf add_ w x w sum + buf sum model_for_eager = MyModel model_for_compile = copy deepcopy model_for_eager fw_graph_cell = None compiled_f = aot_module model_for_compile fw_compiler=make_boxed_compiler partial extract_graph graph_cell=fw_graph_cell bw_compiler=nop keep_inference_input_mutations=True inp_ref = torch ones requires_grad=True inp_test = torch ones requires_grad=True out_ref = model_for_eager inp_ref clone out_test = compiled_f inp_test clone assertExpectedInline fw_graph_cell code strip \ forward primals_ primals_ primals_ primals_ add = torch ops aten add Tensor primals_ mul = torch ops aten mul Tensor primals_ primals_ mul_ = torch ops aten mul Tensor mul primals_ sum_ = torch ops aten sum default mul_ mul_ = None sum_ = torch ops aten sum default add add_ = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None copy_ = torch ops aten copy_ default primals_ add primals_ = add = copy_ = None add_ primals_ primals_ primals_ mul assertEqual out_ref out_test out_ref sum backward out_test sum backward eager_grads = p grad _ p model_for_eager named_parameters compile_grads = p grad _ p model_for_compile named_parameters assertEqual eager_grads compile_grads assertEqual inp_ref grad inp_test grad test_buffer_copied_in_graph_with_different_shapes MyModel torch nn Module __init__ - None super __init__ buf = torch nn Buffer torch ones w = torch nn Parameter torch Tensor forward x buf add_ w x sum + buf sum model_for_eager = MyModel model_for_compile = copy deepcopy model_for_eager fw_graph_cell = None compiled_f = aot_module model_for_compile fw_compiler=make_boxed_compiler partial extract_graph graph_cell=fw_graph_cell bw_compiler=nop keep_inference_input_mutations=True inp_ref = torch ones requires_grad=True inp_test = torch ones requires_grad=True out_ref = model_for_eager inp_ref clone out_test = compiled_f inp_test clone assertExpectedInline fw_graph_cell code strip \ forward primals_ primals_ primals_ add = torch ops aten add Tensor primals_ mm = torch ops aten mm default primals_ primals_ sum_ = torch ops aten sum default mm mm = None sum_ = torch ops aten sum default add add_ = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None copy_ = torch ops aten copy_ default primals_ add primals_ = add = copy_ = None add_ primals_ primals_ assertEqual out_ref out_test out_ref sum backward out_test sum backward eager_grads = p grad _ p model_for_eager named_parameters compile_grads = p grad _ p model_for_compile named_parameters assertEqual eager_grads compile_grads assertEqual inp_ref grad inp_test grad test_buffer_batch_norm MyModel torch nn Module __init__ - None super __init__ m = torch nn BatchNorm d forward x m x model_for_eager = MyModel model_for_compile = copy deepcopy model_for_eager fw_graph_cell = None bw_graph_cell = None compiled_f = aot_module model_for_compile fw_compiler=make_boxed_compiler partial extract_graph graph_cell=fw_graph_cell bw_compiler=make_boxed_compiler partial extract_graph graph_cell=bw_graph_cell keep_inference_input_mutations=True inp_ref = torch ones requires_grad=True inp_test = torch ones requires_grad=True out_ref = model_for_eager inp_ref clone out_test = compiled_f inp_test clone assertExpectedInline fw_graph_cell code strip \ forward primals_ primals_ primals_ primals_ primals_ primals_ add = torch ops aten add Tensor primals_ _native_batch_norm_legit_functional = torch ops aten _native_batch_norm_legit_functional default primals_ primals_ primals_ primals_ primals_ True e- primals_ = None getitem = _native_batch_norm_legit_functional getitem_ = _native_batch_norm_legit_functional getitem_ = _native_batch_norm_legit_functional getitem_ = _native_batch_norm_legit_functional getitem_ = _native_batch_norm_legit_functional _native_batch_norm_legit_functional = None copy_ = torch ops aten copy_ default primals_ getitem_ primals_ = copy_ = None copy__ = torch ops aten copy_ default primals_ getitem_ primals_ = copy__ = None copy__ = torch ops aten copy_ default primals_ add primals_ = add = copy__ = None getitem primals_ primals_ getitem_ getitem_ getitem_ getitem_ noqa B assertEqual out_ref out_test out_ref sum backward out_test sum backward eager_grads = p grad _ p model_for_eager named_parameters compile_grads = p grad _ p model_for_compile named_parameters assertEqual eager_grads compile_grads assertExpectedInline bw_graph_cell code strip \ forward primals_ primals_ getitem_ getitem_ getitem_ getitem_ tangents_ native_batch_norm_backward = torch ops aten native_batch_norm_backward default tangents_ primals_ primals_ getitem_ getitem_ getitem_ getitem_ True e- True True True tangents_ = primals_ = primals_ = getitem_ = getitem_ = getitem_ = getitem_ = None getitem_ = native_batch_norm_backward getitem_ = native_batch_norm_backward getitem_ = native_batch_norm_backward native_batch_norm_backward = None getitem_ getitem_ None None None getitem_ noqa B assertEqual inp_ref grad inp_test grad test_new_inp_requires_grad_now f x y x add_ y fw_graph_cell = None bw_graph_cell = None compiled_f = aot_function f fw_compiler=make_boxed_compiler partial extract_graph graph_cell=fw_graph_cell bw_compiler=make_boxed_compiler partial extract_graph graph_cell=bw_graph_cell keep_inference_input_mutations=True inp_ref = torch ones requires_grad=False torch ones requires_grad=True inp_test = torch ones requires_grad=False torch ones requires_grad=True out_ref = f inp_ref out_test = compiled_f inp_test There no copy_ method assertExpectedInline fw_graph_cell code strip \ forward primals_ primals_ clone = torch ops aten clone default primals_ primals_ = None add = torch ops aten add Tensor clone primals_ clone = primals_ = None add add noqa B assertEqual out_ref out_test out_ref sum backward out_test sum backward assertExpectedInline bw_graph_cell code strip \ forward tangents_ None tangents_ noqa B test_real_weights_in_symbolic_mode functorch experimental functionalize M torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x x = linear x x m = M eval inp = torch randn gm = make_fx m tracing_mode= symbolic _allow_non_fake_inputs=True inp assertEqual gm torch ones m torch ones gm_functionalized = make_fx functionalize gm tracing_mode= symbolic _allow_non_fake_inputs=True inp assertEqual gm_functionalized torch ones m torch ones inp_count = node gm graph nodes node op == placeholder inp_count += No more param lifting assertEqual inp_count inp_count = node gm_functionalized graph nodes node op == placeholder inp_count += No more param lifting assertEqual inp_count assertRaisesRegex Exception Please convert all Tensors FakeTensors make_fx m tracing_mode= symbolic _allow_non_fake_inputs=False torch randn test_real_weights_in_symbolic_mode_with_inplace_ops M torch nn Module __init__ - None super __init__ buffer = torch nn Buffer torch ones forward x y = buffer add_ y resize_ assert y shape == buffer shape x sum + buffer sum m = M eval inp = torch randn inplace mutation attr allowed assertRaisesRegex Exception Can t call metadata make_fx m tracing_mode= symbolic _allow_non_fake_inputs=True inp _compile_and_erase_bases output_view_indices Overrides _base _view_func tensor attributes so avoid view-replay execution path when reconstructing views NoViewReplayTensor torch Tensor property _base None property _view_func None Wraps outputs views FX graph g NoViewReplayTensor since they only ones will get reconstructed wrapper g args kwargs outs = list g args kwargs i output_view_indices outs i = NoViewReplayTensor outs i tuple outs lambda f aot_function f fw_compiler=lambda g _ partial wrapper g test_output_aliases_input_view_meta_replay _compile_and_erase_bases f view - inp = torch ones requires_grad=True out = f inp assertIsNotNone out grad_fn assertExpectedInline str out grad_fn __class__ ViewBackward test_output_aliases_intermediate_view_meta_replay _compile_and_erase_bases f b = clone b view - b view - inp = torch ones requires_grad=True out out = f inp assertIsNotNone out grad_fn assertExpectedInline str out grad_fn __class__ ViewBackward assertIsNotNone out grad_fn assertExpectedInline str out grad_fn __class__ ViewBackward test_output_aliases_output_view_meta_replay _compile_and_erase_bases f b = add b b view - inp = torch ones requires_grad=True out out = f inp assertEqual out untyped_storage out untyped_storage assertIsNotNone out grad_fn assertExpectedInline str out grad_fn __class__ ViewBackward patch torch _dynamo config assume_static_by_default False test_dynamic_output_aliases_input_view_meta_replay - torch compile using so we can have SymInt FX graph - Compiling inductor so tensor _base isn t tracked This should force use as_strided view reconstruction path The first view-replay paths won t taken because - target_functional_tensor will symbolic _functionalize_is_symbolic call - tensor _base will None torch compile backend= inductor f sz view sz view - inp = torch ones requires_grad=True out out = f inp assertIsNotNone out grad_fn assertExpectedInline str out grad_fn __class__ AsStridedBackward assertIsNotNone out grad_fn assertExpectedInline str out grad_fn __class__ ViewBackward test_duplicated_arguments_on_tensor_overlap Test whether we correctly handle duplicated arguments when changing parameters so we take base tensor argument - t t must have storage overlap triggers target execution flow - s s must equal triggers error target execution flow torch compile dynamic=True foo t t s s t add_ s t add_ s tensor = torch rand foo tensor tensor - parametrize use_autograd False True test_mark_outputs_dynamic use_autograd bool counters clear torch _dynamo reset torch compile backend= aot_eager fullgraph=True fn x y torch matmul x y torch compile backend= aot_eager fullgraph=True fn z z static x = torch randn requires_grad=use_autograd y = torch randn requires_grad=use_autograd out = fn x y assertFalse hasattr out _dynamo_weak_dynamic_indices out = fn out assertFalse hasattr out _dynamo_weak_dynamic_indices assertEqual counters aot_autograd total counters clear dynamic x = torch randn y = torch randn out = fn x y assertTrue hasattr out _dynamo_weak_dynamic_indices out = fn out assertTrue hasattr out _dynamo_weak_dynamic_indices assertEqual counters aot_autograd total counters clear torch _dynamo reset test_mark_activations_dynamic counters clear torch _dynamo reset torch compile backend= aot_eager fullgraph=True fn x y out = torch matmul x y out = torch matmul out y out = torch matmul out y torch matmul out y make_assert_pack dynamic pack activation assert hasattr activation _dynamo_weak_dynamic_indices == dynamic activation pack make_assert_unpack dynamic unpack activation assert hasattr activation _dynamo_weak_dynamic_indices == dynamic activation unpack static x = torch randn requires_grad=True y = torch randn requires_grad=True torch autograd graph saved_tensors_hooks make_assert_pack False make_assert_unpack False fn x y assertEqual counters aot_autograd total counters clear dynamic x = torch randn requires_grad=True y = torch randn requires_grad=True torch autograd graph saved_tensors_hooks make_assert_pack True make_assert_unpack True fn x y assertEqual counters aot_autograd total counters clear torch _dynamo reset unittest skipIf torch cuda is_available CUDA unavailable torch _functorch config patch saved_tensors_hooks_filtering_mode= no_static torch _functorch config patch recompute_views=True test_saved_tensors_hooks_mutations_raise ctx = torch autograd graph saved_tensors_hooks device = cuda SAF torch autograd Function staticmethod forward ctx x ctx save_for_backward x x staticmethod backward ctx gx saved_x = ctx saved_tensors gx + saved_x mutate x x mul_ fn x x = x x = SAF apply x x inp_fn x = torch ones device=device requires_grad=True torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x x assertRaisesRegex AssertionError Saved tensors hooks inputs mutations allowed try ctx saved_tensors_hooks_to_gm mutate mutate None None x = inp_fn y = torch compile fn backend= aot_eager fullgraph=True x y sum backward except torch _dynamo exc BackendCompilerFailed e raise e inner_exception e test_mark_activations_dynamic_with_nested The flattened tensors nested tensor aren t marked activations they add some offset fw_outs This test ensures we handle offset properly counters clear torch _dynamo reset make_assert_pack dynamic pack activation assert hasattr activation _dynamo_weak_dynamic_indices == dynamic activation pack make_assert_unpack dynamic unpack activation assert hasattr activation _dynamo_weak_dynamic_indices == dynamic activation unpack static torch compile backend= aot_eager fullgraph=True fn x y nt out = torch matmul x y out sum + nt clone x = torch randn requires_grad=True y = torch randn requires_grad=True = torch randn requires_grad=True dtype=torch float b = torch randn requires_grad=True dtype=torch float c = torch randn requires_grad=True dtype=torch float nt = torch nested as_nested_tensor b c layout=torch jagged torch autograd graph saved_tensors_hooks make_assert_pack False make_assert_unpack False fn x y nt assertEqual counters aot_autograd total counters clear dynamic x = torch randn requires_grad=True y = torch randn requires_grad=True = torch randn requires_grad=True dtype=torch float b = torch randn requires_grad=True dtype=torch float c = torch randn requires_grad=True dtype=torch float nt = torch nested as_nested_tensor b c layout=torch jagged torch autograd graph saved_tensors_hooks make_assert_pack True make_assert_unpack True fn x y nt assertEqual counters aot_autograd total counters clear torch _dynamo reset extract_graph fx_g _ graph_cell graph_cell = fx_g fx_g get_ins_outs fx_g ins = outs = n fx_g graph nodes n op == placeholder ins append n n op == output outs = tuple n args ins outs get_num_ins_outs fx_g tuple len i i get_ins_outs fx_g get_fw_bw_graph f inps partitioner=min_cut_rematerialization_partition dynamic=False fw_graph_cell = None bw_graph_cell = None aot_function f fw_compiler=partial extract_graph graph_cell=fw_graph_cell bw_compiler=partial extract_graph graph_cell=bw_graph_cell partition_fn=partitioner decompositions=default_decompositions dynamic=dynamic inps sum backward fw_graph_cell bw_graph_cell TestMod torch nn Module __init__ fn super __init__ p = torch nn Parameter torch ones requires_grad=True fn = fn forward args fn p args TestAOTExport AOTTestCase setUp super setUp torch _dynamo reset test_aot_export_ban_dropout_mut_pre_dispatch fn p x y = torch ops aten dropout default x train=False y add_ y mod = TestMod fn inp = torch randn assertRaisesRegex RuntimeError cannot mutate tensors frozen storage aot_export_module mod inp trace_joint=False pre_dispatch=True gm _ = aot_export_module mod inp trace_joint=False pre_dispatch=False assertExpectedInline str gm code strip \ forward arg _ arg _ clone = torch ops aten clone default arg _ arg _ = None add = torch ops aten add Tensor clone clone = None add fw_graph_cell = None bw_graph_cell = None aot_function fn fw_compiler=partial extract_graph graph_cell=fw_graph_cell bw_compiler=partial extract_graph graph_cell=bw_graph_cell partition_fn=default_partition decompositions=default_decompositions dynamic=True inp fw_graph = fw_graph_cell assertExpectedInline str fw_graph code strip \ forward arg _ arg _ clone = torch ops aten clone default arg _ arg _ = None add = torch ops aten add Tensor clone clone = None add test_aot_export_predispatch_func_simple fn p x y = x + torch no_grad y add_ x + y mod = TestMod fn inp = torch randn torch no_grad gm _ = aot_export_module mod inp trace_joint=False pre_dispatch=True assertExpectedInline str gm code strip \ forward arg _ arg _ add = torch ops aten add Tensor arg _ _set_grad_enabled = torch _C _set_grad_enabled False _set_grad_enabled = None add_ = torch ops aten add Tensor add add = None _set_grad_enabled_ = torch _C _set_grad_enabled False _set_grad_enabled_ = None mul = torch ops aten mul Tensor arg _ arg _ = None add_ = torch ops aten add Tensor mul add_ mul = add_ = None add_ test_aot_export_predispatch_func_composite_implicit fn p x torch enable_grad y = x x y add_ x sum + y sum mod = TestMod fn inp = torch randn torch no_grad gm _ = aot_export_module mod inp trace_joint=False pre_dispatch=True assertExpectedInline str gm code strip \ forward arg _ arg _ _set_grad_enabled = torch _C _set_grad_enabled True _set_grad_enabled = None matmul = torch ops aten matmul default arg _ arg _ _set_grad_enabled_ = torch _C _set_grad_enabled False _set_grad_enabled_ = None add = torch ops aten add Tensor matmul matmul = None sum_ = torch ops aten sum default arg _ arg _ = None sum_ = torch ops aten sum default add add = None add_ = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None add_ test_aot_export_predispatch_composite_implicit_inplace fn x p torch ops aten absolute_ default x clone mod = TestMod fn inp = torch randn gm _ = aot_export_module mod inp trace_joint=False pre_dispatch=True assertExpectedInline str gm code strip \ forward arg _ arg _ clone = torch ops aten clone default arg _ arg _ = None abs_ = torch ops aten abs default clone clone = None abs_ test_aot_export_predispatch_composite_implicit_linear MM torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x mod = MM inp = torch randn gm _ = aot_export_module mod inp trace_joint=False pre_dispatch=True assertExpectedInline str gm code strip \ forward arg _ arg _ arg _ linear = torch ops aten linear default arg _ arg _ arg _ arg _ = arg _ = arg _ = None linear unittest expectedFailure test_aot_export_predispatch_outdtype M torch nn Module __init__ weight super __init__ weight = weight forward x y = x + y add_ out_dtype torch ops aten mm default torch int y weight weight = torch randint - dtype=torch int mod = M weight inp = torch randint - dtype=torch int gm _ = aot_export_module mod inp trace_joint=False pre_dispatch=True assertExpectedInline str gm code strip \ forward arg _ arg _ _set_grad_enabled = torch _C _set_grad_enabled True _set_grad_enabled = None mm = torch ops aten mm default arg _ arg _ _set_grad_enabled_ = torch _C _set_grad_enabled False _set_grad_enabled_ = None add = torch ops aten add Tensor mm mm = None sum_ = torch ops aten sum default arg _ arg _ = None sum_ = torch ops aten sum default add add = None add_ = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None add_ test_aot_export_predispatch_func_view fn p x y = x x y add_ x sum + y view sum mod = TestMod fn inp = torch randn gm _ = aot_export_module mod inp trace_joint=False pre_dispatch=True assertExpectedInline str gm code strip \ forward arg _ arg _ matmul = torch ops aten matmul default arg _ arg _ add = torch ops aten add Tensor matmul matmul = None sum_ = torch ops aten sum default arg _ arg _ = None view_ = torch ops aten view default add add = None sum_ = torch ops aten sum default view_ view_ = None add_ = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None add_ test_aot_export_predispatch_buffer_mutation_metadata Foo torch nn Module __init__ - None super __init__ foo = torch nn Buffer torch zeros forward x foo add_ x sum + foo sum inp = torch randn gm graph_sig = aot_export_module Foo inp trace_joint=False pre_dispatch=True assertExpectedInline str gm code strip \ forward arg _ arg _ add = torch ops aten add Tensor arg _ arg _ = None sum_ = torch ops aten sum default arg _ arg _ = None sum_ = torch ops aten sum default add add_ = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None add add_ eager_mod = Foo output_ output_ = gm torch zeros inp eager_output = eager_mod inp assertTrue torch allclose output_ eager_output _ output_ = gm output_ inp eager_output = eager_mod inp assertTrue torch allclose output_ eager_output assertTrue foo graph_sig buffers assertTrue graph_sig inputs_to_buffers arg _ == foo test_aot_export_predispatch_with_autograd_op foo p x torch enable_grad y = x + y add_ y add_ x cos + y sin inp = torch randn mod = TestMod foo torch no_grad gm _ = aot_export_module mod inp trace_joint=False pre_dispatch=True assertExpectedInline str gm code strip \ forward arg _ arg _ _set_grad_enabled = torch _C _set_grad_enabled True _set_grad_enabled = None add = torch ops aten add Tensor arg _ add_ = torch ops aten add Tensor add add = None add_ = torch ops aten add Tensor add_ add_ = None cos = torch ops aten cos default arg _ arg _ = None sin = torch ops aten sin default add_ add_ = None add_ = torch ops aten add Tensor cos sin cos = sin = None _set_grad_enabled_ = torch _C _set_grad_enabled False _set_grad_enabled_ = None add_ unittest skipIf IS_WINDOWS Windows isn t supported case unittest skipIf torchdynamo is_dynamo_supported TorchDynamo supported test_aot_export_predispatch_with_cond_nested M torch nn Module __init__ - None super __init__ forward x true_fn x y = x sin y add_ true_true_fn x y = x sin y add_ y sin true_false_fn x x cos torch cond y cos sum true_true_fn true_false_fn y cos false_fn x z = x cos z add_ z sin = torch cond x sum true_fn false_fn x + + inp = torch randn gm _ = aot_export_module M inp trace_joint=False pre_dispatch=True assertExpectedInline str gm code strip \ forward arg _ sum_ = torch ops aten sum default arg _ gt = torch ops aten gt Scalar sum_ sum_ = None true_graph_ = true_graph_ false_graph_ = false_graph_ cond = torch ops higher_order cond gt true_graph_ false_graph_ arg _ gt = true_graph_ = false_graph_ = arg _ = None getitem = cond cond = None add = torch ops aten add Tensor getitem add_ = torch ops aten add Tensor getitem getitem = None add add_ noqa B assertExpectedInline str gm true_graph_ code strip \ forward arg _ sin = torch ops aten sin default arg _ arg _ = None add = torch ops aten add Tensor sin sin = None cos = torch ops aten cos default add sum_ = torch ops aten sum default cos cos = None gt = torch ops aten gt Scalar sum_ sum_ = None cos_ = torch ops aten cos default add add = None true_graph_ = true_graph_ false_graph_ = false_graph_ cond = torch ops higher_order cond gt true_graph_ false_graph_ cos_ gt = true_graph_ = false_graph_ = cos_ = None getitem = cond cond = None getitem noqa B assertExpectedInline str gm true_graph_ true_graph_ code strip \ forward arg _ sin = torch ops aten sin default arg _ arg _ = None add = torch ops aten add Tensor sin sin = None sin_ = torch ops aten sin default add add = None sin_ unittest skipIf IS_WINDOWS Windows isn t supported case unittest skipIf torchdynamo is_dynamo_supported TorchDynamo supported test_aot_export_predispatch_map_ M torch nn Module __init__ - None super __init__ forward x y true_fn x r y = x sin y add_ y cos + r sum false_fn x r z = x cos f x y = x cos add_ + y z + control_flow map f z r sum + control_flow map f z r sum = torch cond x sum true_fn false_fn x y + + inps = torch randn torch ones gm _ = aot_export_module M inps trace_joint=False pre_dispatch=True assertExpectedInline normalize_gm gm print_readable False expanded_def=True \ lambda torch nn Module forward arg _ f PlainAOTInput idx= arg _ f PlainAOTInput idx= sum_ f = torch ops aten sum default arg _ gt b = torch ops aten gt Scalar sum_ sum_ = None true_graph_ = true_graph_ false_graph_ = false_graph_ cond = torch ops higher_order cond gt true_graph_ false_graph_ arg _ arg _ gt = true_graph_ = false_graph_ = arg _ = arg _ = None getitem f = cond cond = None add f = torch ops aten add Tensor getitem add_ f = torch ops aten add Tensor getitem getitem = None add PlainAOTOutput idx= add_ PlainAOTOutput idx= true_graph_ torch nn Module forward arg _ f arg _ f sin f = torch ops aten sin default arg _ arg _ = None add f = torch ops aten add Tensor sin sin = None cos f = torch ops aten cos default add add = None sum_ f = torch ops aten sum default arg _ arg _ = None add_ f = torch ops aten add Tensor cos sum_ cos = sum_ = None add_ false_graph_ torch nn Module forward arg _ f arg _ f cos f = torch ops aten cos default arg _ arg _ = None body_graph_ = body_graph_ map_impl = torch ops higher_order map_impl body_graph_ cos arg _ body_graph_ = None getitem_ f = map_impl map_impl = None sum_ f = torch ops aten sum default getitem_ getitem_ = None add f = torch ops aten add Tensor cos sum_ sum_ = None body_graph_ = body_graph_ map_impl_ = torch ops higher_order map_impl body_graph_ cos arg _ body_graph_ = cos = arg _ = None getitem_ f = map_impl_ map_impl_ = None sum_ f = torch ops aten sum default getitem_ getitem_ = None add_ f = torch ops aten add Tensor add sum_ add = sum_ = None add_ body_graph_ torch nn Module forward arg _ f arg _ f cos f = torch ops aten cos default arg _ arg _ = None add f = torch ops aten add Tensor cos cos = None add_ f = torch ops aten add Tensor add arg _ add = arg _ = None add_ body_graph_ torch nn Module forward arg _ f arg _ f cos f = torch ops aten cos default arg _ arg _ = None add f = torch ops aten add Tensor cos cos = None add_ f = torch ops aten add Tensor add arg _ add = arg _ = None add_ noqa B test_aot_export_predispatch_map_ M torch nn Module __init__ - None super __init__ forward x y z = x cos f x y = x cos add_ + y z + control_flow map f z y sum inps = torch randn torch ones gm _ = aot_export_module M inps trace_joint=False pre_dispatch=True assertExpectedInline normalize_gm gm print_readable False expanded_def=True \ lambda torch nn Module forward arg _ f PlainAOTInput idx= arg _ f PlainAOTInput idx= cos f = torch ops aten cos default arg _ arg _ = None body_graph_ = body_graph_ map_impl = torch ops higher_order map_impl body_graph_ cos arg _ body_graph_ = arg _ = None getitem_ f = map_impl map_impl = None sum_ f = torch ops aten sum default getitem_ getitem_ = None add f = torch ops aten add Tensor cos sum_ cos = sum_ = None add PlainAOTOutput idx= body_graph_ torch nn Module forward arg _ f arg _ f cos f = torch ops aten cos default arg _ arg _ = None add f = torch ops aten add Tensor cos cos = None add_ f = torch ops aten add Tensor add arg _ add = arg _ = None add_ unittest skipIf IS_WINDOWS Windows isn t supported case unittest skipIf torchdynamo is_dynamo_supported TorchDynamo supported test_aot_export_predispatch_with_cond M torch nn Module __init__ - None super __init__ forward x true_fn x y = x sin z = torch ops aten linear default y torch randn z add_ z cos false_fn x z = x cos z add_ z sin = torch cond x sum true_fn false_fn x + + inp = torch randn gm _ = aot_export_module M inp trace_joint=False pre_dispatch=True assertExpectedInline str gm code strip \ forward arg _ sum_ = torch ops aten sum default arg _ gt = torch ops aten gt Scalar sum_ sum_ = None true_graph_ = true_graph_ false_graph_ = false_graph_ cond = torch ops higher_order cond gt true_graph_ false_graph_ arg _ gt = true_graph_ = false_graph_ = arg _ = None getitem = cond cond = None add = torch ops aten add Tensor getitem add_ = torch ops aten add Tensor getitem getitem = None add add_ noqa B assertExpectedInline str gm true_graph_ code strip \ forward arg _ sin = torch ops aten sin default arg _ arg _ = None randn = torch ops aten randn default device = device type= cpu pin_memory = False linear = torch ops aten linear default sin randn sin = randn = None add = torch ops aten add Tensor linear linear = None cos = torch ops aten cos default add add = None cos test_aot_export_predispatch_conv_and_bn ConvBatchnorm torch nn Module __init__ - None super __init__ conv = torch nn Conv d bn = torch nn BatchNorm d forward x x = conv x x = bn x x mod = ConvBatchnorm mod train inp = torch randn gm _ = aot_export_module mod inp trace_joint=False pre_dispatch=True assertExpectedInline str gm code strip \ forward arg _ arg _ arg _ arg _ arg _ arg _ arg _ arg _ conv d = torch ops aten conv d default arg _ arg _ arg _ arg _ = arg _ = arg _ = None add = torch ops aten add Tensor arg _ arg _ = None _native_batch_norm_legit_functional = torch ops aten _native_batch_norm_legit_functional default conv d arg _ arg _ arg _ arg _ True e- conv d = arg _ = arg _ = arg _ = arg _ = None getitem = _native_batch_norm_legit_functional getitem_ = _native_batch_norm_legit_functional getitem_ = _native_batch_norm_legit_functional _native_batch_norm_legit_functional = None getitem_ getitem_ add getitem noqa B test_aot_export_predispatch_reshape Reshape torch nn Module forward x y = x reshape y sum mod = Reshape inp = torch randn gm _ = aot_export_module mod inp trace_joint=False pre_dispatch=True assertExpectedInline str gm code strip \ forward arg _ view = torch ops aten view default arg _ arg _ = None sum_ = torch ops aten sum default view view = None sum_ noqa B test_aot_export_predispatch_contiguous Cont torch nn Module forward x y = torch ops aten contiguous default x y sum mod = Cont inp = torch randn gm _ = aot_export_module mod inp trace_joint=False pre_dispatch=True assertExpectedInline str gm code strip \ forward arg _ sum_ = torch ops aten sum default arg _ arg _ = None sum_ noqa B test_aot_export_module_joint ConvBatchnormRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d bn = torch nn BatchNorm d forward x x = conv x x = bn x user_out = torch nn functional relu x loss = user_out sum loss user_out detach mod = ConvBatchnormRelu mod train inp = torch randn mod inp fx_g signature = aot_export_module mod inp trace_joint=True output_loss_index= Some important characteristics exported graph below arguments params conv params batchnorm buffers batchnorm user input outputs mutated buffers batchnorm user outputs gradients since there parameters node fx_g graph nodes node meta pop stack_trace None assertExpectedInline fx_g print_readable print_output=False expanded_def=True \ lambda torch nn Module forward arg _ f arg _ f arg _ f arg _ f arg _ f arg _ f arg _ i arg _ f No stacktrace found following nodes convolution f = torch ops aten convolution default arg _ arg _ arg _ False arg _ = None add i = torch ops aten add Tensor arg _ arg _ = None _native_batch_norm_legit_functional = torch ops aten _native_batch_norm_legit_functional default convolution arg _ arg _ arg _ arg _ True e- arg _ = arg _ = arg _ = None getitem f = _native_batch_norm_legit_functional getitem_ f = _native_batch_norm_legit_functional getitem_ f = _native_batch_norm_legit_functional getitem_ f = _native_batch_norm_legit_functional getitem_ f = _native_batch_norm_legit_functional _native_batch_norm_legit_functional = None relu f = torch ops aten relu default getitem getitem = None detach f = torch ops aten detach default relu detach = None detach_ f = torch ops aten detach default relu sum_ f = torch ops aten sum default relu detach_ f = torch ops aten detach default relu relu = None ones_like f = torch ops aten ones_like default sum_ pin_memory = False memory_format = torch preserve_format expand f = torch ops aten expand default ones_like ones_like = None detach_ f = torch ops aten detach default detach_ detach_ = None threshold_backward f = torch ops aten threshold_backward default expand detach_ expand = detach_ = None native_batch_norm_backward = torch ops aten native_batch_norm_backward default threshold_backward convolution arg _ getitem_ getitem_ getitem_ getitem_ True e- True True True threshold_backward = convolution = arg _ = getitem_ = getitem_ = None getitem_ f = native_batch_norm_backward getitem_ f = native_batch_norm_backward getitem_ f = native_batch_norm_backward native_batch_norm_backward = None convolution_backward = torch ops aten convolution_backward default getitem_ arg _ arg _ False False True True getitem_ = arg _ = arg _ = None getitem_ = convolution_backward getitem_ = None getitem_ f = convolution_backward getitem_ f = convolution_backward convolution_backward = None getitem_ getitem_ add sum_ detach_ getitem_ getitem_ getitem_ getitem_ noqa B assertExpectedInline str signature parameters conv weight conv bias bn weight bn bias assertExpectedInline str signature buffers bn running_mean bn running_var bn num_batches_tracked assertExpectedInline str signature user_inputs arg _ assertExpectedInline str signature inputs_to_parameters arg _ conv weight arg _ conv bias arg _ bn weight arg _ bn bias noqa B assertExpectedInline str signature inputs_to_buffers arg _ bn running_mean arg _ bn running_var arg _ bn num_batches_tracked noqa B assertExpectedInline str signature buffers_to_mutate getitem_ bn running_mean getitem_ bn running_var add bn num_batches_tracked noqa B assertExpectedInline str signature backward_signature gradients_to_parameters getitem_ conv weight getitem_ conv bias getitem_ bn weight getitem_ bn bias noqa B assertExpectedInline str signature backward_signature gradients_to_user_inputs assertExpectedInline str signature backward_signature loss_output getitem_ Also check inference graph Main important thing here there total outputs total mutated buffers batchnorm user outputs fx_g_inference signature_inference = aot_export_module mod inp trace_joint=False node fx_g_inference graph nodes node meta pop stack_trace None assertExpectedInline fx_g_inference print_readable print_output=False expanded_def=True \ lambda torch nn Module forward arg _ f PlainAOTInput idx= arg _ f PlainAOTInput idx= arg _ f PlainAOTInput idx= arg _ f PlainAOTInput idx= arg _ f PlainAOTInput idx= arg _ f PlainAOTInput idx= arg _ i PlainAOTInput idx= arg _ f PlainAOTInput idx= No stacktrace found following nodes convolution f = torch ops aten convolution default arg _ arg _ arg _ False arg _ = arg _ = arg _ = None add i = torch ops aten add Tensor arg _ arg _ = None _native_batch_norm_legit_functional = torch ops aten _native_batch_norm_legit_functional default convolution arg _ arg _ arg _ arg _ True e- convolution = arg _ = arg _ = arg _ = arg _ = None getitem f = _native_batch_norm_legit_functional getitem_ f = _native_batch_norm_legit_functional getitem_ f = _native_batch_norm_legit_functional _native_batch_norm_legit_functional = None relu f = torch ops aten relu default getitem getitem = None sum_ f = torch ops aten sum default relu detach f = torch ops aten detach default relu relu = None getitem_ InputMutationAOTOutput mutated_input=PlainAOTInput idx= getitem_ InputMutationAOTOutput mutated_input=PlainAOTInput idx= add InputMutationAOTOutput mutated_input=PlainAOTInput idx= sum_ PlainAOTOutput idx= detach PlainAOTOutput idx= noqa B Some important characteristics exported graph below arguments params conv params batchnorm buffers batchnorm user input outputs mutated buffers batchnorm user outputs gradients since there parameters test_aot_export_simplified_basic f x y x y y y detach x = torch randn requires_grad=True y = torch randn requires_grad=True f_graph_fw = aot_export_joint_simple f x y trace_joint=False out_ref = f x y No calling convention changes necessary invoke traced graph out_test = f_graph_fw x y assertEqual out_ref out_test Now test backward x = torch randn requires_grad=True y = torch randn requires_grad=True x = x detach clone requires_grad_ True y = y detach clone requires_grad_ True x = x detach clone requires_grad_ True y = y detach clone requires_grad_ True f_graph_joint = aot_export_joint_simple f x y trace_joint=True num_fw_outputs = fw_g bw_g = default_partition f_graph_joint x y num_fwd_outputs=num_fw_outputs out_ref = f x y fw_outs = fw_g x y out_test activations = fw_outs num_fw_outputs fw_outs num_fw_outputs assertEqual out_ref out_test Test running traced backward graph mocked-up grad_output grad_outs = torch ones_like x x out_ref grads_ref = torch autograd grad out_ref x y grad_outputs=grad_outs grads_test = bw_g activations grad_outs g_ref g_test zip grads_ref grads_test assertEqual g_ref g_test test_aot_export_metadata_mutation_banned fn p x x t_ x mod = TestMod fn inp = torch randn assertRaisesRegex RuntimeError Found input received metadata mutation aot_export_joint_simple fn mod p inp trace_joint=False aot_export_joint_simple fn mod p inp trace_joint=True aot_export_module mod inp trace_joint=False test_aot_export_forward_mutation_no_buffer_mut M torch nn Module __init__ - None super __init__ buffer = torch nn Buffer torch ones forward x x add_ x cos sum + buffer sum mod = M inp = torch ones gm sig = aot_export_module mod inp trace_joint=False assertExpectedInline str gm code strip \ forward arg _ arg _ add = torch ops aten add Tensor arg _ arg _ = None cos = torch ops aten cos default add sum_ = torch ops aten sum default cos cos = None sum_ = torch ops aten sum default arg _ arg _ = None add_ = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None add add_ noqa B assertEqual sig user_inputs_to_mutate add arg _ test_aot_export_forward_mutation_multiple_mut M torch nn Module __init__ - None super __init__ buffer = torch nn Buffer torch ones forward x y y add_ buffer add_ x cos sum + y sin sum buffer sum mod = M inp = torch ones torch zeros gm sig = aot_export_module mod inp trace_joint=False assertExpectedInline str gm code strip \ forward arg _ arg _ arg _ add = torch ops aten add Tensor arg _ arg _ = None add_ = torch ops aten add Tensor arg _ arg _ = None cos = torch ops aten cos default arg _ arg _ = None sum_ = torch ops aten sum default cos cos = None sin = torch ops aten sin default add sum_ = torch ops aten sum default sin sin = None add_ = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None sum_ = torch ops aten sum default add_ add_ add add_ sum_ noqa B assertEqual sig user_inputs_to_mutate add arg _ assertEqual sig buffers_to_mutate add_ buffer test_aot_export_input_mutation_on_input_requiring_grad_banned M torch nn Module forward x x add_ x mod = M inp = torch randn requires_grad=True gm _ = aot_export_module mod inp trace_joint=False assertExpectedInline str gm graph strip \ graph arg _ num_users= = placeholder target=arg _ add num_users= = call_function target=torch ops aten add Tensor args = arg _ kwargs = add add test_aot_export_input_mutation_on_parameter_banned fn p x p mul_ p + x mod = TestMod fn inp = torch randn assertRaisesRegex RuntimeError aot_export_joint_simple does support input mutations ViewAndMutationMeta aot_export_joint_simple fn mod p inp trace_joint=False assertRaisesRegex RuntimeError Found graph input requires gradients received mutation aot_export_joint_simple fn mod p inp trace_joint=True gm _ = aot_export_module mod inp trace_joint=False assertExpectedInline str gm graph strip \ graph arg _ num_users= = placeholder target=arg _ arg _ num_users= = placeholder target=arg _ mul num_users= = call_function target=torch ops aten mul Tensor args = arg _ kwargs = add num_users= = call_function target=torch ops aten add Tensor args = mul arg _ kwargs = mul add test_aot_export_synthetic_bases_banned fn p x y x mul_ x + y mod = TestMod fn inp = torch randn inp = inp view - assertRaisesRegex RuntimeError Encountered aliased inputs mutated aot_export_joint_simple fn mod p inp inp trace_joint=False aot_export_joint_simple fn mod p inp inp trace_joint=True aot_export_module mod inp inp trace_joint=False test_aot_export_input_dupes_banned fn p x y x mul_ x + y mod = TestMod fn inp = torch randn assertRaisesRegex RuntimeError Encountered duplicated inputs mutated graph aot_export_joint_simple fn mod p inp inp trace_joint=False aot_export_joint_simple fn mod p inp inp trace_joint=True aot_export_module mod inp inp trace_joint=False test_aot_export_multiple_outputs_require_grad_banned fn p x out = p x out out sum mod = TestMod fn inp = torch randn assertRaisesRegex RuntimeError Found output forward requires gradients aot_export_module mod inp trace_joint=True output_loss_index= unittest skipIf IS_WINDOWS Windows isn t supported case unittest skipIf torch _dynamo is_dynamo_supported Cond needs dynamo run test_aot_export_with_torch_cond M torch nn Module __init__ - None super __init__ forward x true_fn x y = x + y add_ x cos false_fn x y = x + y add_ x sin = torch cond x sum true_fn false_fn x + + inp = torch randn gm _ = aot_export_module M inp trace_joint=False assertExpectedInline gm code strip \ forward arg _ sum_ = torch ops aten sum default arg _ gt = torch ops aten gt Scalar sum_ sum_ = None true_graph_ = true_graph_ false_graph_ = false_graph_ cond = torch ops higher_order cond gt true_graph_ false_graph_ arg _ gt = true_graph_ = false_graph_ = arg _ = None getitem = cond cond = None add = torch ops aten add Tensor getitem add_ = torch ops aten add Tensor getitem getitem = None add add_ noqa B assertExpectedInline gm true_graph_ code strip \ forward arg _ cos = torch ops aten cos default arg _ arg _ = None cos assertExpectedInline gm false_graph_ code strip \ forward arg _ sin = torch ops aten sin default arg _ arg _ = None sin test_aot_export_simplified_pytrees_banned fn inps inps + inps inp = torch randn inp = torch randn inps = inp inp assertRaisesRegex RuntimeError aot_export_joint_simple requires individual inputs pytrees aot_export_joint_simple fn inps trace_joint=False aot_export_joint_simple fn inps trace_joint=True test_aot_export_functionalized_rng_banned fn p x p + x mod = TestMod fn inp = torch randn patch functorch compile config functionalize_rng_ops True assertRaisesRegex RuntimeError Functionalized RNG currently supported aot_export aot_export_joint_simple fn mod p inp trace_joint=False aot_export_joint_simple fn mod p inp trace_joint=True aot_export_module mod inp trace_joint=False test_aot_export_unbacked_arg M torch nn Module forward full = torch full i = full item torch full i gm _ = aot_export_module mod=M args= trace_joint=False dynamic_shapes=True assertExpectedInline gm code strip \ forward full = torch ops aten full default device = device type= cpu pin_memory = False _local_scalar_dense = torch ops aten _local_scalar_dense default full full = None full_ = torch ops aten full default _local_scalar_dense device = device type= cpu pin_memory = False _local_scalar_dense = None full_ noqa B test_aot_export_input_mutation f x buf buf add_ buf x x = torch randn requires_grad=True buf = torch zeros requires_grad=False gm _ _ _ = _aot_export_function f x buf decompositions= num_params_buffers= no_tangents=False pre_dispatch=False dynamic_shapes=False keep_input_mutations=True kwargs= assertExpectedInline gm code strip \ forward primals tangents primals_ primals_ tangents_ = fx_pytree tree_flatten_spec primals tangents _in_spec add = torch ops aten add Tensor primals_ mul = torch ops aten mul Tensor add primals_ primals_ = None mul_ = torch ops aten mul Tensor tangents_ add tangents_ = None copy_ = torch ops aten copy_ default primals_ add primals_ = add = copy_ = None pytree tree_unflatten mul mul_ None _out_spec TestPartitioning AOTTestCase unittest skipIf USE_NETWORKX networkx available test_recompute_partitioning fn b torch sin torch sin + b Reference calculation ref_a = torch rand requires_grad=True ref_b = torch rand requires_grad=True ref = fn ref_a ref_b ref sum backward Compiled function calculation res_a = ref_a detach clone requires_grad_ True res_b = ref_b detach clone requires_grad_ True compile_fn x _ x compiled_fn = compiled_function fn compile_fn compile_fn min_cut_rematerialization_partition res = compiled_fn res_a res_b res sum backward assert torch allclose ref res atol= e- rtol= e- assert torch allclose ref_a grad res_a grad atol= e- rtol= e- assert torch allclose ref_b grad res_b grad atol= e- rtol= e- test_meta_tensor_inplace_op Following module results inplace ops while tracing The test checks meta tensor information stored inplace ops MockModule torch nn Module __init__ - None super __init__ weight = torch nn Parameter torch randn requires_grad=True bias = torch nn Parameter torch randn requires_grad=True forward add_ linear_ = torch nn functional linear add_ weight bias=self bias gelu = torch nn functional gelu linear_ gelu check_meta_tensor fx_g _ node fx_g graph nodes node op = output assert tensor_meta node meta fx_g inp = torch randn requires_grad=True inputs = inp mod = MockModule device= cpu aot_mod = aot_module mod fw_compiler=check_meta_tensor aot_mod inputs test_default_partitioner_getitem mod = nn LayerNorm f x mod_weight mod_bias torch nn functional layer_norm x mod_weight mod_bias eps= e- fw_graph bw_graph = get_fw_bw_graph f torch randn requires_grad=True mod weight mod bias partitioner=default_partition assertEqual get_num_ins_outs fw_graph assertEqual get_num_ins_outs bw_graph unittest skipIf USE_NETWORKX networkx available test_min_cut_partitioner_raise_getitems f x y = torch split x x size dim= = y sin b = y cos + b _ bw_graph = get_fw_bw_graph f torch randn requires_grad=True assertExpectedInline bw_graph code strip \ forward primals_ tangents_ split = torch ops aten split Tensor primals_ primals_ = None getitem_ = split getitem = split split = None sin_ = torch ops aten sin default getitem_ getitem_ = None neg = torch ops aten neg default sin_ sin_ = None mul = torch ops aten mul Tensor tangents_ neg neg = None cos_ = torch ops aten cos default getitem getitem = None mul_ = torch ops aten mul Tensor tangents_ cos_ tangents_ = cos_ = None cat = torch ops aten cat default mul_ mul mul_ = mul = None cat unittest skipIf USE_NETWORKX networkx available test_custom_partitioner_fn MyCustomPartitionerFn CustomPartitionerFn __init__ super __init__ called = False __call__ gm joint_inputs kwargs called = True min_cut_rematerialization_partition gm joint_inputs kwargs uuid None f x x cos cos inp = torch randn requires_grad=True custom_partitioner_fn = MyCustomPartitionerFn fw_graph bw_graph = get_fw_bw_graph f inp partitioner=custom_partitioner_fn assertTrue custom_partitioner_fn called assertExpectedInline fw_graph code strip \ forward primals_ cos = torch ops aten cos default primals_ cos_ = torch ops aten cos default cos cos = None cos_ primals_ assertExpectedInline bw_graph code strip \ forward primals_ tangents_ cos = torch ops aten cos default primals_ sin = torch ops aten sin default cos cos = None neg = torch ops aten neg default sin sin = None mul = torch ops aten mul Tensor tangents_ neg tangents_ = neg = None sin_ = torch ops aten sin default primals_ primals_ = None neg_ = torch ops aten neg default sin_ sin_ = None mul_ = torch ops aten mul Tensor mul neg_ mul = neg_ = None mul_ unittest skipIf USE_NETWORKX networkx available test_min_cut_partitioner_save_shape f x s = x sum dim= s inp = torch ones requires_grad=True fw_graph bw_graph = get_fw_bw_graph f inp dynamic=True _ fw_output = get_ins_outs fw_graph assertEqual get_num_ins_outs fw_graph assertEqual get_num_ins_outs bw_graph assertEqual str fw_output sum_ make sure we don t do suboptimal thing saving bigger primals input sum rather than saving sizes primals input use backward expand assertEqual str fw_output sym_size_int assertEqual str fw_output sym_size_int_ inp = torch randn requires_grad=True torch randn requires_grad=True torch randn requires_grad=True f b c tried test what happens we save size tuple graph turns out we never will due how we trace probably still good test case various size manipulations sb = torch ops aten sym_size b sc = c size x = sb + sc a_sz = x size torch cat expand a_sz b c fw_graph bw_graph = get_fw_bw_graph f inp dynamic=True assertEqual get_num_ins_outs fw_graph assertEqual get_num_ins_outs bw_graph _ outs = get_ins_outs fw_graph assertTrue all is_sym_node n n outs test_default_partitioner_output_tensor_shape_tensor inp = torch randn requires_grad=True torch randn requires_grad=True torch randn requires_grad=True torch randn requires_grad=True f b c d Try force symints intermixed outputs function s returns sb = b size sc = c size x = sb + sc a_sz = x size cat = torch cat expand a_sz b c mm = torch mm cat d mm = torch mm mm view mm size size saves new ints backward why what do i have do make save tensor backward cat sb c mm fw_graph_cell = None bw_graph_cell = None compiled_outs = aot_function f fw_compiler=partial extract_graph graph_cell=fw_graph_cell bw_compiler=partial extract_graph graph_cell=bw_graph_cell partition_fn=default_partition decompositions=default_decompositions dynamic=True inp fw_graph = fw_graph_cell compiled_outs sum + compiled_outs sum backward bw_graph = bw_graph_cell fwd graph outs because - original outputs sb tuple gets expanded symints - saved outputs backward tensors symints assertEqual get_num_ins_outs fw_graph bwd graph inputs grad outs because - The fwd graph had outputs - view input which gets regenerated outside graph doesn t participate backward - user outs symints b size which don t get tangents backward assertEqual get_num_ins_outs bw_graph _ fw_graph_out_nodes = get_ins_outs fw_graph assertEqual fw outputs include b size which expands symints TODO whc - saved-tensors saved-symints correct here i just made test pass based what default partition did Of original forward outputs th c input which won t show up compiled forward graph False True True False False + False + True is_sym_node n n fw_graph_out_nodes real_outs = f inp assertEqual compiled_outs real_outs assertTrue isinstance real_outs torch Size TODO whc we should learn torch Sizes assertFalse isinstance compiled_outs torch Size unittest skipIf USE_NETWORKX networkx available test_min_cut_partitioner_output_tensor_shape_tensor inp = torch randn requires_grad=True torch randn requires_grad=True torch randn requires_grad=True torch randn requires_grad=True f b c d Try force symints intermixed outputs function s returns sb = b size sc = c size x = sb + sc a_sz = x size cat = torch cat expand a_sz b c mm = torch mm cat d mm = torch mm mm view mm size size saves new ints backward why what do i have do make save tensor backward cat sb c mm fw_graph_cell = None bw_graph_cell = None compiled_outs = aot_function f fw_compiler=partial extract_graph graph_cell=fw_graph_cell bw_compiler=partial extract_graph graph_cell=bw_graph_cell partition_fn=min_cut_rematerialization_partition decompositions=default_decompositions dynamic=True inp fw_graph = fw_graph_cell compiled_outs sum + compiled_outs sum backward bw_graph = bw_graph_cell assertEqual get_num_ins_outs fw_graph assertEqual get_num_ins_outs bw_graph _ fw_graph_out_nodes = get_ins_outs fw_graph assertEqual fw outputs include b size which expands symints then tensors transposes matrices used mm saved finally symints saved False True True False False + False + True is_sym_node n n fw_graph_out_nodes real_outs = f inp assertEqual compiled_outs real_outs assertTrue isinstance real_outs torch Size TODO whc we should learn torch Sizes assertFalse isinstance compiled_outs torch Size unittest skipIf USE_NETWORKX networkx available test_min_cut_partitioner f x x cos cos cos fw_graph bw_graph = get_fw_bw_graph f torch randn requires_grad=True assertEqual get_num_ins_outs fw_graph assertEqual get_num_ins_outs bw_graph f b c d x = + b + c + d x cos cos fw_graph bw_graph = get_fw_bw_graph f torch randn requires_grad=True _ range assertEqual get_num_ins_outs fw_graph assertEqual get_num_ins_outs bw_graph test_contiguous The test simulates condition where transpose followed view happens backward pass https discuss pytorch org t error-on-transpose-and-view f x x view t inp = torch randn requires_grad=True out = aot_function f nop inp torch autograd grad out inp torch randn test_preserve_random fn x torch nn functional dropout x + x x = torch randn torch manual_seed ref = fn x torch manual_seed aot_fn = aot_function fn nop res = aot_fn x assert torch allclose ref res https github com pytorch pytorch issues test_generate_gives_inference_graph We expect give inference graph generate x torch no_grad torch mul x x inference_graph_cell = None inference_compiler = make_boxed_compiler partial extract_graph graph_cell=inference_graph_cell aot_fn = aot_function generate nop inference_compiler=inference_compiler Even though x requires grad we should still get inference graph x = torch randn requires_grad=True aot_fn x assertTrue inference_graph_cell None unittest skipIf torch cuda is_available CUDA unavailable unittest skipIf USE_TORCHVISION test requires torchvision test_autocast mod = torchvision models resnet cuda mod train x = torch randn device= cuda aot_mod = memory_efficient_fusion mod Ensure AOT Autograd works AMP torch cuda amp autocast True res = aot_mod x res sum backward test_quantize_activation_duplicate_nodes Test both quantize_activation_fw quantize_activation_bw handle duplicate nodes correctly torch fx fx torch _functorch partitioners quantize_activation_bw quantize_activation_fw torch _subclasses fake_tensor extract_tensor_metadata Mock inductor config patch dict torch _inductor config post_grad_fusion_options activation_quantization_aten_pass allowed_dtypes torch bfloat size_in_mb use_scaling True exclude_primals False skip_dynamo_guards True quantize_dynamic_shape False quant_type torch float float _e m must GPU Test Forward Graph duplicate nodes fwd_graph = fx Graph Create input nodes x = fwd_graph placeholder x x meta val = torch randn dtype=torch bfloat x meta tensor_meta = extract_tensor_metadata x meta val y = fwd_graph placeholder y y meta val = torch randn dtype=torch bfloat y meta tensor_meta = extract_tensor_metadata y meta val Create computation node will duplicated outputs mul_node = fwd_graph call_function torch ops aten mul Tensor x y mul_node meta val = torch randn dtype=torch bfloat mul_node meta tensor_meta = extract_tensor_metadata mul_node meta val mul_node meta saved_for_quantization = True Create another node add_node = fwd_graph call_function torch ops aten add Tensor x y add_node meta val = torch randn dtype=torch bfloat add_node meta tensor_meta = extract_tensor_metadata add_node meta val Create output DUPLICATE nodes - mul_node appears positions fwd_graph output mul_node add_node mul_node Test forward quantization function quantize_activation_fw fwd_graph Get forward output node fwd_output_node = fwd_graph find_nodes op= output fwd_output_args = fwd_output_node args Verify forward graph has correct structure assertGreaterEqual len fwd_output_args Should have least original outputs Check positions reuse same quantized node pos_ _node = fwd_output_args pos_ _node = fwd_output_args Both should quantized nodes assertTrue pos_ _node name startswith fp _quant_ f Position should quantized node got pos_ _node name assertTrue pos_ _node name startswith fp _quant_ f Position should quantized node got pos_ _node name The shared quantized node should have first occurrence position its name assertIn _pos_ pos_ _node name f Shared quantized node should have _pos_ name pos_ _node name assertIn _pos_ pos_ _node name f Shared quantized node should have _pos_ name pos_ _node name Find scale nodes forward output fwd_scale_nodes = node node fwd_output_args fp _scale_ node name assertEqual len fwd_scale_nodes Should have exactly scale node shared both quantized instances Test Backward Graph duplicate nodes bwd_graph = fx Graph Create backward placeholders corresponding forward outputs quant_input = bwd_graph placeholder fp _quant_pos_ _mul_tensor quant_input meta val = torch randn dtype=torch float quant_input meta tensor_meta = extract_tensor_metadata quant_input meta val quant_input meta saved_for_quantization = True quant_input meta dequant_type = torch bfloat add_input = bwd_graph placeholder add add_input meta val = torch randn dtype=torch bfloat add_input meta tensor_meta = extract_tensor_metadata add_input meta val quant_input = bwd_graph placeholder fp _quant_pos_ _mul_tensor quant_input meta val = torch randn dtype=torch float quant_input meta tensor_meta = extract_tensor_metadata quant_input meta val quant_input meta saved_for_quantization = True quant_input meta dequant_type = torch bfloat Add scale node would come forward scale_input = bwd_graph placeholder fp _scale_pos_ _mul_tensor scale_input meta val = torch randn dtype=torch float scale_input meta tensor_meta = extract_tensor_metadata scale_input meta val scale_input = bwd_graph placeholder fp _scale_pos_ _mul_tensor scale_input meta val = torch randn dtype=torch float scale_input meta tensor_meta = extract_tensor_metadata scale_input meta val Create some backward computation using both quantized inputs grad_output = bwd_graph placeholder tangents_ grad_output meta val = torch randn dtype=torch bfloat grad_output meta tensor_meta = extract_tensor_metadata grad_output meta val grad_output = bwd_graph placeholder tangents_ grad_output meta val = torch randn dtype=torch bfloat grad_output meta tensor_meta = extract_tensor_metadata grad_output meta val Create backward operations using quantized inputs mul_bwd = bwd_graph call_function torch ops aten mul Tensor quant_input grad_output mul_bwd meta val = torch randn dtype=torch bfloat mul_bwd meta tensor_meta = extract_tensor_metadata mul_bwd meta val mul_bwd = bwd_graph call_function torch ops aten mul Tensor quant_input grad_output mul_bwd meta val = torch randn dtype=torch bfloat mul_bwd meta tensor_meta = extract_tensor_metadata mul_bwd meta val Create output bwd_graph output mul_bwd mul_bwd Test backward quantization function quantize_activation_bw bwd_graph Verify backward graph processing bwd_placeholders = list bwd_graph find_nodes op= placeholder quantized_placeholders = p p bwd_placeholders fp _quant_ p name scale_placeholders = p p bwd_placeholders fp _scale_ p name Should have processed quantized placeholders assertGreater len quantized_placeholders Should have quantized placeholders assertGreater len scale_placeholders Should have scale placeholders Check dequantization operations added dequant_operations = node node bwd_graph nodes node op == call_function convert_element_type str node target Should have dequantization operations each quantized input processed assertGreater len dequant_operations Should have dequantization operations backward graph Verify backward graph users properly updated quant_placeholder quantized_placeholders The quantized placeholder should directly used final operations should replaced dequantized versions direct_users = user user quant_placeholder users user op == call_function mul str user target Direct usage should minimal only dequantization chain assertLessEqual len direct_users f Quantized placeholder quant_placeholder name should have minimal direct users TestAOTDispatch AOTTestCase Tests add cases non-exhaustive list mostly my notes - subclass mode introduced middle compiled fn - various input mutation intermediate base tests - input mutation changes tensor into subclass - metadata mutation TBD - guard tests fw guards bw guards - subclass test involving _indices_of_inps_to_detach test_aot_dispatch_simple subclass b f b aa = torch mul bb = torch div b aa + bb _ref = torch ones requires_grad=True _ref = torch ones requires_grad=True a_ref = TwoTensor _ref _ref b_ref = torch ones requires_grad=True _test = _ref detach clone requires_grad_ True _test = _ref detach clone requires_grad_ True a_test = TwoTensor _test _test b_test = b_ref detach clone requires_grad_ True fw_graph_cell = None bw_graph_cell = None compiled_f = aot_function f fw_compiler=partial extract_graph graph_cell=fw_graph_cell bw_compiler=partial extract_graph graph_cell=bw_graph_cell partition_fn=min_cut_rematerialization_partition out_ref = f a_ref b_ref out_test = compiled_f a_test b_test Output TwoTensor check both inner tensors assertEqual out_ref out_test assertEqual out_ref b out_test b out_ref sum backward out_test sum backward Both grad_inputs TwoTensor assertEqual a_ref grad a_test grad assertEqual a_ref grad b a_test grad b assertEqual b_ref grad b_test grad assertEqual b_ref grad b b_test grad b Important pieces graph - mul div show up twice because we called them TwoTensor - add shows up once because we called plain Tensor - The user forward fn returns output result add while graph itself returns two outputs add add_ - add add_ correspond two inner dense tensors will wrapped - into single TwoTensor output assertExpectedInline fw_graph_cell code strip \ forward primals_ primals_ primals_ mul = torch ops aten mul Tensor primals_ primals_ = None mul_ = torch ops aten mul Tensor primals_ primals_ = None div = torch ops aten div Tensor primals_ primals_ = None add = torch ops aten add Tensor mul div mul = None add_ = torch ops aten add Tensor mul_ div mul_ = div = None add add_ Important pieces graph - total dense outputs This corresponds fact each user fwd input b will get gradient TwoTensor subclass so mul_ mul_ will wrapped into grad div_ div_ will wrapped into b grad - total dense outputs assertExpectedInline bw_graph_cell code strip \ forward tangents_ tangents_ div_ = torch ops aten div Tensor tangents_ div_ = torch ops aten div Tensor tangents_ mul_ = torch ops aten mul Tensor tangents_ tangents_ = None mul_ = torch ops aten mul Tensor tangents_ tangents_ = None mul_ mul_ div_ div_ test_aot_dispatch_inference subclass b f b aa = torch mul bb = torch div b aa + bb _ref = torch ones _ref = torch ones a_ref = TwoTensor _ref _ref b_ref = torch ones _test = _ref clone _test = _ref clone a_test = TwoTensor _test _test b_test = b_ref clone compiled_f = aot_function f fw_compiler=nop bw_compiler=nop partition_fn=min_cut_rematerialization_partition out_ref = f a_ref b_ref out_test = compiled_f a_test b_test Output TwoTensor check both inner tensors assertEqual out_ref out_test assertEqual out_ref b out_test b test_aot_dispatch_incorrect_backward subclass b f b aa = torch mul bb = torch add b out_subclass = torch div aa bb out_reg = torch add b b When creating joint we assume second grad_out subclass In below test case though we end up being wrong This would require re-tracing recompiling backward out_subclass out_reg _ref = torch ones requires_grad=True _ref = torch ones requires_grad=True a_ref = TwoTensor _ref _ref b_ref = torch ones requires_grad=True _test = _ref detach clone requires_grad_ True _test = _ref detach clone requires_grad_ True a_test = TwoTensor _test _test b_test = b_ref detach clone requires_grad_ True compiled_f = aot_function f fw_compiler=nop bw_compiler=nop partition_fn=min_cut_rematerialization_partition out_ref = f a_ref b_ref out_test = compiled_f a_test b_test First out TwoTensor second ordinary tensor assertEqual out_ref out_test assertEqual out_ref b out_test b assertEqual out_ref out_test We compiled our graph assuming type grad_out torch Tensor we wrong below tests subclass This will eventually require repartition + recompile assertRaisesRegex RuntimeError During backward we encountered tensor subclass where we guessed its metadata incorrectly noqa F out_test + out_test sum backward test_aot_dispatch_output_alias tensor b TwoTensor f b b view b shape b b _ref = torch ones requires_grad=True b _ref = torch ones requires_grad=True b_ref = TwoTensor b _ref b _ref a_ref = torch ones requires_grad=True b _test = b _ref detach clone requires_grad_ True b _test = b _ref detach clone requires_grad_ True b_test = TwoTensor b _test b _test a_test = a_ref detach clone requires_grad_ True compiled_f = aot_function f fw_compiler=nop bw_compiler=nop partition_fn=min_cut_rematerialization_partition out_ref out_ref = f a_ref b_ref out_test out_test = compiled_f a_test b_test assertEqual out_ref out_test assertEqual out_ref out_test assertEqual out_ref b out_test b out_ref + out_ref sum backward out_test + out_test sum backward Both grad_inputs TwoTensor assertEqual a_ref grad a_test grad assertEqual a_ref grad b a_test grad b assertEqual b_ref grad b_test grad assertEqual b_ref grad b b_test grad b torch _functorch config patch disable_guess_zero_tangent_for_mutated_input_subclass True test_aot_dispatch_input_mutation f b mul_ b mul_ + b b _ref = torch ones requires_grad=True b _ref = torch ones requires_grad=True b_ref_base = TwoTensor b _ref b _ref a_ref_base = torch ones requires_grad=True b_ref = b_ref_base + a_ref = a_ref_base + b _test = b _ref detach clone requires_grad_ True b _test = b _ref detach clone requires_grad_ True b_test_base = TwoTensor b _test b _test a_test_base = a_ref_base detach clone requires_grad_ True b_test = b_test_base + a_test = a_test_base + compiled_f = aot_function f fw_compiler=nop bw_compiler=nop partition_fn=min_cut_rematerialization_partition out_ref = f a_ref b_ref out_test = compiled_f a_test b_test assertEqual out_ref out_test assertEqual out_ref b out_test b confirm input mutations worked assertEqual a_test a_ref assertEqual b_test b_ref assertEqual b_test b b_ref b NOTE we need use b our gradient compute Otherwise we will need recompile backward b_ref out_ref sum backward b_test out_test sum backward Both grad_inputs TwoTensor assertEqual a_ref_base grad a_test_base grad assertEqual a_ref_base grad b a_test_base grad b assertEqual b_ref_base grad b_test_base grad assertEqual b_ref_base grad b b_test_base grad b NB Metadata mutation subclasses currently broken disabled See https github com pytorch pytorch issues unittest expectedFailure test_aot_dispatch_input_metadata_mutation f b t_ b unsqueeze_ + b b _ref = torch arange requires_grad=True dtype=torch float reshape b _ref = torch arange requires_grad=True dtype=torch float reshape b_ref_base = TwoTensor b _ref b _ref a_ref_base = torch arange dtype=torch float reshape detach requires_grad_ True b_ref = b_ref_base + a_ref = a_ref_base + b _test = b _ref detach clone requires_grad_ True b _test = b _ref detach clone requires_grad_ True b_test_base = TwoTensor b _test b _test a_test_base = a_ref_base detach clone requires_grad_ True b_test = b_test_base + a_test = a_test_base + compiled_f = aot_function f fw_compiler=nop bw_compiler=nop partition_fn=min_cut_rematerialization_partition out_ref = f a_ref b_ref out_test = compiled_f a_test b_test assertEqual out_ref out_test assertEqual out_ref b out_test b confirm input mutations worked assertEqual a_test a_ref assertEqual b_test b_ref assertEqual b_test b b_ref b NOTE we need use b our gradient compute Otherwise we will need recompile backward b_ref out_ref sum backward b_test out_test sum backward Both grad_inputs TwoTensor assertEqual a_ref_base grad a_test_base grad assertEqual a_ref_base grad b a_test_base grad b assertEqual b_ref_base grad b_test_base grad assertEqual b_ref_base grad b b_test_base grad b NB Metadata mutation subclasses currently broken disabled See https github com pytorch pytorch issues unittest expectedFailure test_aot_dispatch_input_data_and_metadata_mutation f b t_ b unsqueeze_ mul_ b mul_ + b b _ref = torch arange requires_grad=True dtype=torch float reshape b _ref = torch arange requires_grad=True dtype=torch float reshape b_ref_base = TwoTensor b _ref b _ref a_ref_base = torch arange dtype=torch float reshape detach requires_grad_ True b_ref = b_ref_base + a_ref = a_ref_base + b _test = b _ref detach clone requires_grad_ True b _test = b _ref detach clone requires_grad_ True b_test_base = TwoTensor b _test b _test a_test_base = a_ref_base detach clone requires_grad_ True b_test = b_test_base + a_test = a_test_base + compiled_f = aot_function f fw_compiler=nop bw_compiler=nop partition_fn=min_cut_rematerialization_partition out_ref = f a_ref b_ref out_test = compiled_f a_test b_test assertEqual out_ref out_test assertEqual out_ref b out_test b confirm input mutations worked assertEqual a_test a_ref assertEqual b_test b_ref assertEqual b_test b b_ref b NOTE we need use b our gradient compute Otherwise we will need recompile backward b_ref out_ref sum backward b_test out_test sum backward Both grad_inputs TwoTensor assertEqual a_ref_base grad a_test_base grad assertEqual a_ref_base grad b a_test_base grad b assertEqual b_ref_base grad b_test_base grad assertEqual b_ref_base grad b b_test_base grad b torch _functorch config patch disable_guess_zero_tangent_for_mutated_input_subclass True test_aot_dispatch_input_mutation_and_output_alias f b mul_ b mul_ b view b shape + b b _ref = torch arange requires_grad=True dtype=torch float reshape b _ref = torch arange requires_grad=True dtype=torch float reshape b_ref_base = TwoTensor b _ref b _ref a_ref_base = torch arange dtype=torch float reshape detach requires_grad_ True b_ref = b_ref_base + a_ref = a_ref_base + b _test = b _ref detach clone requires_grad_ True b _test = b _ref detach clone requires_grad_ True b_test_base = TwoTensor b _test b _test a_test_base = a_ref_base detach clone requires_grad_ True b_test = b_test_base + a_test = a_test_base + compiled_f = aot_function f fw_compiler=nop bw_compiler=nop partition_fn=min_cut_rematerialization_partition out_ref out_ref = f a_ref b_ref out_test out_test = compiled_f a_test b_test assertEqual out_ref out_test assertEqual out_ref b out_test b assertEqual out_ref out_test assertEqual out_ref b out_test b confirm input mutations worked assertEqual a_test a_ref assertEqual b_test b_ref assertEqual b_test b b_ref b out_ref out_ref sum backward out_test out_test sum backward Both grad_inputs TwoTensors assertEqual a_ref_base grad a_test_base grad assertEqual a_ref_base grad b a_test_base grad b test_aot_dispatch_output_requires_grad_in_no_grad fn x out = x sin torch enable_grad out = x cos out out inp_fns = lambda torch ones requires_grad=True lambda torch ones requires_grad=False compiled_f = aot_function fn nop inp_fn inp_fns torch no_grad ref_x = inp_fn ref_out = fn ref_x x = inp_fn out = compiled_f x r o zip ref_out out assertEqual r requires_grad o requires_grad ref_x requires_grad torch enable_grad ref_out + ref_out sum backward out + out sum backward assertEqual ref_x grad x grad assert torch allclose ref_x grad x grad atol= e- rtol= e- test_aot_dispatch_output_requires_grad_in_no_grad_views view-type ops preserve requires_grad even no_grad fn x x view - x sin inference_graph_cell = None inference_compiler = make_boxed_compiler partial extract_graph graph_cell=inference_graph_cell compiled_fn = aot_function fn nop inference_compiler=inference_compiler inp_x = torch ones requires_grad=True Clone no_grad will make requires_grad=False tensors keep clone outside no_grad ref_x = inp_x clone x = inp_x clone torch no_grad ref_out ref_out = fn ref_x out out = compiled_fn x Assert we executed inference graph assertTrue inference_graph_cell None assertEqual ref_out requires_grad out requires_grad assertEqual ref_out requires_grad out requires_grad GradsNoForceContiguousContextManager ContextDecorator __enter__ flake noqa TOR lib = torch library Library _test_aotdispatch_lib FRAGMENT d = torch channels_last torch contiguous_format tangent_strides = lib define log_tangents_memory_format Tensor x - Tensor lib define log_tangents_memory_format_log Tensor x - Tensor log_tangents_memory_format_impl clone log_tangents_memory_format_meta clone log_tangents_memory_format_log_impl x d torch _prims_common suggest_memory_format x += tangent_strides append x stride x clone log_tangents_memory_format_log_meta clone backend CPU CUDA lib impl log_tangents_memory_format log_tangents_memory_format_impl backend lib impl log_tangents_memory_format_log log_tangents_memory_format_log_impl backend lib impl log_tangents_memory_format log_tangents_memory_format_meta Meta lib impl log_tangents_memory_format_log log_tangents_memory_format_log_meta Meta log_tangents_memory_format_bwd ctx grad torch ops _test_aotdispatch_lib log_tangents_memory_format_log grad grad clone torch library register_autograd _test_aotdispatch_lib log_tangents_memory_format log_tangents_memory_format_bwd lib=self lib torch _higher_order_ops effects _EffectType _register_effectful_op _register_effectful_op torch ops _test_aotdispatch_lib log_tangents_memory_format default _EffectType ORDERED _register_effectful_op torch ops _test_aotdispatch_lib log_tangents_memory_format_log default _EffectType ORDERED __exit__ type value tb lib _destroy False reset_counters d = torch channels_last torch contiguous_format TestAOTModuleSimplified AOTTestCase test_aot_module_simplified MockModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y linear x + y mod = MockModule mod zero_grad x = torch randn requires_grad=True y = torch randn requires_grad=True inputs = x y cloned_inputs = x detach clone requires_grad_ True x inputs ref = mod inputs ref sum backward compiled_f = aot_module_simplified mod cloned_inputs nop mod zero_grad res = compiled_f cloned_inputs res sum backward assert torch allclose ref res assert torch allclose inputs grad cloned_inputs grad assert torch allclose inputs grad cloned_inputs grad test_aot_module_simplified_dynamic MockModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y linear x + y mod = MockModule shape_env = ShapeEnv fake_mode = FakeTensorMode shape_env=shape_env x = torch randn requires_grad=True y = torch randn requires_grad=True inputs = x y fake_inputs = fake_mode from_tensor x x inputs compiled_f = aot_module_simplified mod fake_inputs nop ref = mod inputs ref sum backward cloned_inputs = x detach clone requires_grad_ True x inputs res = compiled_f cloned_inputs res sum backward assertExpectedInline shape_env format_guards \ - Eq s - Eq s assert torch allclose ref res assert torch allclose inputs grad cloned_inputs grad assert torch allclose inputs grad cloned_inputs grad https github com pytorch pytorch issues test_lift_fresh_copy_in_graph MyMod torch nn Module forward x _tensor_constant = torch tensor lift_fresh_copy = torch ops aten lift_fresh_copy default _tensor_constant y = x mul lift_fresh_copy y mod = MyMod shape_env = ShapeEnv fake_mode = FakeTensorMode shape_env=shape_env x = torch ones requires_grad=True inputs = x fake_inputs = fake_mode from_tensor x x inputs compiled_f = aot_module_simplified mod fake_inputs nop out_ref = mod x out_test = compiled_f x assertEqual out_ref detach out_test detach test_inference_python_dispatcher Extracted unet MockModule torch nn Module __init__ - None super __init__ upsample = torch nn Upsample scale_factor= mode= bilinear align_corners=True forward x upsample x mod = MockModule shape_env = ShapeEnv fake_mode = FakeTensorMode shape_env=shape_env x = torch randn NB must require grad inputs = x fake_inputs = fake_mode from_tensor x x inputs aot_module_simplified mod fake_inputs nop test_aot_module_simplified_preserves_stack_trace MockModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x y z = linear x z = z + y z = z relu z tracer = torch fx Tracer tracer record_stack_traces = True graph = tracer trace MockModule mod = torch fx GraphModule tracer root graph node mod graph nodes node op = call_function continue assertTrue node stack_trace None assert test_aotdispatch py node stack_trace assert_compiler gm torch fx GraphModule _ node gm graph nodes node op == output node op == placeholder continue assertTrue node stack_trace None assert test_aotdispatch py node stack_trace gm forward python callable x = torch randn requires_grad=True y = torch randn requires_grad=True inputs = x y compiled_f = aot_module_simplified mod inputs fw_compiler=assert_compiler bw_compiler=assert_compiler res = compiled_f inputs res sum backward test_aot_module_simplified_preserves_stack_trace_from_mutation MockModule torch nn Module __init__ - None super __init__ forward x x_view = x x_view mul_ x + x tracer = torch fx Tracer tracer record_stack_traces = True graph = tracer trace MockModule mod = torch fx GraphModule tracer root graph node mod graph nodes node op = call_function continue assertTrue node stack_trace None assert test_aotdispatch py node stack_trace assert_compiler gm torch fx GraphModule _ assert torch ops aten copy_ default x target x gm graph nodes node gm graph nodes node target == torch ops aten copy_ default assert stack_trace node meta assert x_view mul_ node meta stack_trace gm forward python callable x = torch randn inputs = x aot_module_simplified mod inputs fw_compiler=assert_compiler bw_compiler=assert_compiler keep_inference_input_mutations=True test_aot_module_simplified_fake_tensor_gm_raises fake_mode = torch _subclasses fake_tensor FakeTensorMode real_x = torch randn requires_grad=True fake_x = fake_mode from_tensor real_x real_z = torch randn fake_z = fake_mode from_tensor real_z MockModule torch nn Module forward x Accessing free variable fake tensor will look like constant make_fx result tensor being traced into graph which error condition Make sure we report adequately case x + fake_z assertRaisesRegex AssertionError Unexpected fake aot_module_simplified MockModule fake_x nop test_aot_test_subclasses_with_tensor_factories torch testing _internal common_subclass SubclassWithTensorFactory inp = SubclassWithTensorFactory torch zeros fn x x ref_out = fn inp out = torch compile fn backend= aot_eager fullgraph=True inp assertEqual ref_out out Next several tests related issue https github com pytorch pytorch issues AOTD tries predict tangents tracing ahead time The first strategy coerce traced_tangents runtime_tangents contiguous But models working channels_last memory format will add additional contiguous calls The fix predicting tangents memory format similar outputs memory format And coerce runtime tangents traced memory format test_grads_no_force_contiguous_dense GradsNoForceContiguousContextManager ctx M torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x y cont_inp z = y + y mul_ r = conv x r = torch ops _test_aotdispatch_lib log_tangents_memory_format r r r transpose z view - z transpose cont_inp m = M m memory_format=torch channels_last m train dense_inps torch randn requires_grad=True memory_format=torch channels_last torch randn requires_grad=True memory_format=torch channels_last torch randn requires_grad=True ref_inps = dense_inps ref_outs = m ref_inps ref_outs sum backward ctx reset_counters inps = dense_inps outs = torch compile m backend= inductor fullgraph=True inps outs sum backward assertEqual ctx d torch channels_last assertEqual ctx d torch contiguous_format test_grads_no_force_contiguous_subclass GradsNoForceContiguousContextManager ctx M torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x y r = conv x r = torch ops _test_aotdispatch_lib log_tangents_memory_format r r y + m = M m memory_format=torch channels_last m train inps_fn TwoTensor torch randn requires_grad=True memory_format=torch channels_last torch randn requires_grad=True memory_format=torch channels_last torch randn requires_grad=True clone ref_outs = m inps_fn ref_outs sum backward ctx reset_counters mc = M mc memory_format=torch channels_last mc train outs = torch compile mc backend= aot_eager fullgraph=True inps_fn outs sum backward assertEqual ctx d torch channels_last assertEqual ctx d torch contiguous_format test_grads_no_force_contiguous_nested_subclass GradsNoForceContiguousContextManager ctx M torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x r = conv x r = torch ops _test_aotdispatch_lib log_tangents_memory_format r r m = M m memory_format=torch channels_last m train inps_fn x TwoTensor TwoTensor x clone x clone TwoTensor x clone x clone x = torch randn requires_grad=True memory_format=torch channels_last ref_inps = inps_fn x ref_outs = m ref_inps ref_outs sum backward ctx reset_counters mc = M mc memory_format=torch channels_last mc train x = torch randn requires_grad=True memory_format=torch channels_last inps = inps_fn x outs = torch compile mc backend= aot_eager fullgraph=True inps outs sum backward assertEqual ctx d torch channels_last assertEqual ctx d torch contiguous_format test_grads_no_force_contiguous_nested_tensor_tangent NestedTensor setattr could fails AttributeError attr _min_seqlen_tensor Adding test verify handled fn x x clone = torch randn requires_grad=True dtype=torch float b = torch randn requires_grad=True dtype=torch float c = torch randn requires_grad=True dtype=torch float nt = torch nested as_nested_tensor b c layout=torch jagged out = torch compile fn backend= aot_eager fullgraph=True nt out_buffer = out values ga gb gc = torch autograd grad out_buffer sum b c test_wrong_guess_tangent_type fn x x clone ref_x = TwoTensor torch randn requires_grad=True torch randn requires_grad=True ref_y = fn ref_x ref_y backward gradient=TwoTensor torch randn torch randn fn_comp = torch compile fn fullgraph=True x = TwoTensor torch randn requires_grad=True torch randn requires_grad=True y = fn_comp x y backward gradient=TwoTensor torch randn torch randn x = TwoTensor torch randn requires_grad=True torch randn requires_grad=True y = fn_comp x assertRaisesRegex RuntimeError During backward we encountered tensor subclass where we guessed its metadata incorrectly noqa F y backward gradient=torch randn test_tangent_type_coercion fn x x clone ref_y = fn WrapperSubclass torch randn requires_grad=True ref_y sum backward fn_comp = torch compile fn fullgraph=True x = TwoTensor torch randn requires_grad=True torch randn requires_grad=True y = fn_comp x y backward gradient=TwoTensor torch randn torch randn x = TwoTensor torch randn requires_grad=True torch randn requires_grad=True y = fn_comp x Test coercion WrapperSubclass - TwoTensor y backward gradient=WrapperSubclass torch randn y = torch compile fn fullgraph=True torch randn requires_grad=True Test coercion WrapperSubclass - Tensor y backward gradient=WrapperSubclass torch randn torch _inductor config patch freezing True test_inductor_freezing_with_subclasses M torch nn Module __init__ super __init__ w = TwoTensor torch randn torch randn wt = torch randn forward x x index_select dim= index=torch tensor dtype=torch int + w + wt m = M inp = torch randn torch no_grad torch compile m fullgraph=True inp test_rrelu fn x torch rrelu x training=True fn_ x torch rrelu_ x training=True x x = torch randn torch compile fn backend= inductor fullgraph=True x torch compile fn_ backend= inductor fullgraph=True x test_layer_norm fn x F layer_norm x normalized_shape= x = torch randn eager = fn x aot_eager = torch compile backend= aot_eager fn x assertEqual eager aot_eager atol= rtol= unittest expectedFailure unittest skipIf torch cuda is_available CUDA unavailable test_rms_norm Only CUDA rms norm fails decomposed fn x F rms_norm x normalized_shape= x = torch randn device= cuda eager = fn x aot_eager = torch compile backend= aot_eager fn x assertEqual eager aot_eager atol= rtol= test_subclass_parameters _M torch nn Module __init__ super __init__ p = torch nn Parameter TwoTensor TwoTensor torch zeros torch randn torch ones forward x x + p M torch nn Module __init__ super __init__ p = torch nn Parameter torch ones p = torch nn Parameter TwoTensor torch ones TwoTensor torch randn torch randn _m = _M forward x _m x + x + p + p m = M ref_x = torch randn ref_out = m ref_x ref_out sum backward m zero_grad torch _functorch _aot_autograd subclass_parametrization unwrap_tensor_subclass_parameters unwrap_tensor_subclass_parameters m ref_x = ref_x detach clone ref_out = m ref_x assertEqual ref_out ref_out ref_out sum backward assertEqual ref_x grad ref_x grad m zero_grad x = ref_x detach clone comp_fn = torch compile m backend= aot_eager fullgraph=True out = comp_fn x assertEqual ref_out out out sum backward assertEqual ref_x grad x grad test_subclass_parameters_torture_case M torch nn Module __init__ super __init__ p = torch nn Parameter torch ones p = torch nn Parameter TwoTensor TwoTensor torch ones TwoTensor torch randn torch randn TwoTensor TwoTensor torch randn torch randn TwoTensor torch ones torch randn forward x x + p + p b m = M ref_x = torch randn ref_out = m ref_x ref_out sum backward m zero_grad torch _functorch _aot_autograd subclass_parametrization unwrap_tensor_subclass_parameters unwrap_tensor_subclass_parameters m ref_x = ref_x detach clone ref_out = m ref_x assertEqual ref_out ref_out ref_out sum backward assertEqual ref_x grad ref_x grad m zero_grad x = ref_x detach clone comp_fn = torch compile m backend= aot_eager fullgraph=True out = comp_fn x assertEqual ref_out out out sum backward assertEqual ref_x grad x grad test_rrelu_with_noise_mutation fn_functional x noise = torch ones_like x result noise_out = torch ops aten rrelu_with_noise_functional x noise True result noise_out fn_mutation x noise = torch ones_like x result = torch ops aten rrelu_with_noise x noise True result noise fn_inplace x noise = torch ones_like x requires_grad=False torch ops aten rrelu_with_noise_ x noise True x noise _test_fn fn check_backward=True x = -torch abs torch randn dtype=torch bfloat requires_grad=True ref_y ref_noise = fn x assertTrue torch all ref_noise torch ones_like ref_noise item comp_y comp_noise = torch compile fn backend= inductor fullgraph=True x check_backward comp_y sum backward assertTrue torch all comp_noise torch ones_like comp_noise item _test_fn fn_functional _test_fn fn_mutation _test_fn fn_inplace check_backward=False unittest skipIf torch cuda is_available CUDA unavailable parametrize dynamic_shapes True False parametrize test_subclasses True False parametrize device cuda cpu patch torch _functorch config guess_tangent_strides_as_outputs True test_noncontig_nonmemformat_tangents dynamic_shapes test_subclasses device B = T = E = fn x x = x + x transpose _inp_dense t = torch randn B T E device=device requires_grad=True dynamic_shapes i range t ndim torch _dynamo mark_dynamic t i t _inp_sc TwoTensor _inp_dense _inp_dense _inp = _inp_dense test_subclasses _inp_sc comp_fn = torch compile fn backend= aot_eager fullgraph=True _tg y t = torch randn y shape dtype=y dtype layout=y layout device=y device t as_strided y shape tuple s s y stride TEST_CASES = _inp lambda y torch ones y shape dtype=y dtype device=y device Memory overlap dense tangent _inp lambda y torch tensor dtype=y dtype device=y device as_strided y shape y ndim No memory overlap not-dense tangent _inp _tg inp_fn tg_fn TEST_CASES ref_x = inp_fn x = ref_x detach clone requires_grad_ ref_y = fn ref_x y = comp_fn x assertEqual ref_y y ref_tg = tg_fn ref_y test_subclasses TwoTensor tg_fn ref_y tg_fn ref_y tg = ref_tg clone ref_y backward ref_tg y backward tg assertEqual ref_x grad x grad patch torch _functorch config guess_tangent_strides_as_outputs True test_flex_attn_noncontiguous_tangents GradsNoForceContiguousContextManager ctx E = embedding dim H = number heads torch compile backend= aot_eager fullgraph=True attn_fn q k v y = flex_attention query=q key=k value=v y = torch ops _test_aotdispatch_lib log_tangents_memory_format y y M torch nn Module __init__ super __init__ c_attn = torch nn Linear E E forward x B T E = x size q k v = c_attn x split E dim= k = k view B T H E H transpose B nh T hs q = q view B T H E H transpose B nh T hs v = v view B T H E H transpose B nh T hs y = attn_fn q k v y transpose contiguous view B T E m = M B = T = _inp torch randn B T E requires_grad=True x = _inp y = m x y backward torch ones_like y contiguous assertEqual len ctx tangent_strides assertEqual ctx tangent_strides _test_pack_hooks fn inp_fn hooks symbolic_tracing=True pre_compile_fn=None backend= inductor ctx = torch autograd graph saved_tensors_hooks torch _dynamo reset ExitStack stack All hooks eager get ref hook _ hooks pack unpack = hook stack enter_context ctx pack unpack ref_x = inp_fn _f t t dtype is_floating_point t detach clone requires_grad_ t x = pytree tree_map_only torch Tensor _f ref_x ref_y = fn ref_x ref_y sum backward pre_compile_fn pre_compile_fn ExitStack stack hook inline hooks pack unpack = hook inline symbolic_tracing stack enter_context ctx saved_tensors_hooks_to_gm pack unpack pack_hash unpack_hash stack enter_context ctx saved_tensors_hooks_to_gm pack unpack pack_hash unpack_hash stack enter_context ctx pack unpack y = torch compile fn backend=backend fullgraph=True x y sum backward assertEqual ref_y y atol= e- rtol= e- ref_x_grad = pytree tree_map_only torch Tensor lambda t t grad ref_x x_grad = pytree tree_map_only torch Tensor lambda t t grad x assertEqual ref_x_grad x_grad atol= e- rtol= e- unittest skipIf torch cuda is_available CUDA unavailable unittest skipIf SM OrLater bfloat float parametrize saved_tensors_hooks_filtering_mode donated no_static all test_saved_tensors_hooks_base saved_tensors_hooks_filtering_mode patch torch _functorch config saved_tensors_hooks_filtering_mode saved_tensors_hooks_filtering_mode y argument expected test saving int tensor check filtering functionality apply hooks e g is_floating_point SAF torch autograd Function staticmethod forward ctx x y ctx save_for_backward x y x staticmethod backward ctx gx saved_x saved_y = ctx saved_tensors gx + saved_x + saved_y None AF torch autograd Function staticmethod forward ctx x ctx save_for_backward x ctx d = x size x staticmethod backward ctx gx saved_x = ctx saved_tensors d = ctx d gx + saved_x d fn x y x = x relu x = x + x = x relu x = x x = AF apply x x simple_fn x y x = x + x = x t x = x relu x = x t x = SAF apply x y x device = torch device cuda inp_fn x = torch ones device=device requires_grad=True torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x y = torch zeros device=device dtype=torch int x y pack_dev_sym_cpu x x dtype x device x size x cpu unpack_dev_sym_cpu packed dtype device dim x = packed x = x device=device x dtype pack_tensor x x device x cpu unpack_tensor packed device t_cpu = packed t_cpu device pack_bf x x dtype x dtype=torch bfloat unpack_bf packed dtype x = packed x dtype pack_mul x x dtype x unpack_mul x dtype x = x x = x x dtype pack_wrapper_sc x WrapperSubclass x unpack_wrapper_sc x x pack_wrapper_two_tensor x TwoTensor x x unpack_wrapper_two_tensor x x + x b pack_mul _eager x x unpack_mul _eager x x pack_cpu x x device= cpu unpack_cpu x x device=device test_fn simple_fn fn _test_pack_hooks test_fn inp_fn pack_cpu unpack_cpu True symbolic_tracing=False _test_pack_hooks test_fn inp_fn pack_bf unpack_bf True _test_pack_hooks test_fn inp_fn pack_mul unpack_mul True _test_pack_hooks test_fn inp_fn pack_tensor unpack_tensor True _test_pack_hooks test_fn inp_fn pack_dev_sym_cpu unpack_dev_sym_cpu True _test_pack_hooks test_fn inp_fn pack_mul _eager unpack_mul _eager False _test_pack_hooks test_fn inp_fn pack_fp unpack_fp True _test_pack_hooks test_fn inp_fn pack_fp _with_scale unpack_fp _with_scale True Disable testing Subclasses now _test_pack_hooks test_fn inp_fn pack_wrapper_sc unpack_wrapper_sc _test_pack_hooks test_fn inp_fn pack_wrapper_two_tensor unpack_wrapper_two_tensor unittest skipIf torch cuda is_available CUDA unavailable unittest skipIf SM OrLater bfloat float test_saved_tensors_hooks_params lib = torch library Library _test_aotdispatch_lib FRAGMENT logged_shapes = logged_dtypes = lib define log Tensor x - Tensor log_impl x logged_shapes append list x shape logged_dtypes append x dtype x clone log_meta x x clone backend CPU CUDA lib impl log log_impl backend lib impl log log_meta Meta pack_fp _with_scale_and_log x torch ops _test_aotdispatch_lib log x _pack_fp _with_scale_wrap x unpack_fp _with_scale_and_log packed _unpack_fp _with_scale_wrap packed m_inp_fn x = torch ones device=device dtype=torch float requires_grad=True torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x x SAF torch autograd Function staticmethod forward ctx x ctx save_for_backward x x staticmethod backward ctx gx saved_x = ctx saved_tensors gx + saved_x M torch nn Module __init__ super __init__ fc = nn Linear relu = nn ReLU fc = nn Linear forward x x = SAF apply x x = x dtype=torch float x = fc x x = relu x x = fc x x _reset_logged logged_shapes clear logged_dtypes clear device = torch device cuda m = M device=device _test_m _test_pack_hooks m m_inp_fn pack_fp _with_scale_and_log unpack_fp _with_scale_and_log True pre_compile_fn=_reset_logged backend= aot_eager patch torch _functorch config saved_tensors_hooks_filtering_mode donated _reset_logged _test_m Check hooks applied Parameters parameters excluded assertFalse logged_shapes assertTrue logged_shapes input excluded assertFalse torch float logged_dtypes patch torch _functorch config saved_tensors_hooks_filtering_mode no_static _reset_logged _test_m Check hooks applied Parameters parameters excluded assertFalse logged_shapes assertTrue logged_shapes assertTrue torch float logged_dtypes patch torch _functorch config saved_tensors_hooks_filtering_mode all _reset_logged _test_m Check hooks applied all saved tensors assertTrue logged_shapes assertTrue logged_shapes assertTrue torch float logged_dtypes unittest skipIf torch cuda is_available CUDA unavailable unittest skipIf SM OrLater bfloat float torch _functorch config patch saved_tensors_hooks_filtering_mode= all test_saved_tensors_hooks_recompile ctx = torch autograd graph saved_tensors_hooks pack_bf x x dtype=torch bfloat unpack_bf x x dtype=torch float pack_mul x x unpack_mul x x _test hooks inline expected_compile_count SAF torch autograd Function staticmethod forward ctx x ctx save_for_backward x x staticmethod backward ctx gx saved_x = ctx saved_tensors gx + saved_x AF torch autograd Function staticmethod forward ctx x ctx save_for_backward x ctx d = x size x staticmethod backward ctx gx saved_x = ctx saved_tensors d = ctx d gx + saved_x d fn x x = x relu x = x + x = x x = AF apply x x device = torch device cuda inp_fn x = torch ones device=device requires_grad=True torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x x torch _dynamo testing CompileCounter cnt = CompileCounter x = inp_fn y = torch compile fn backend=cnt fullgraph=True x y sum backward _test_with_hooks hooks ExitStack stack pack unpack = hooks inline stack enter_context ctx saved_tensors_hooks_to_gm pack unpack pack_hash unpack_hash stack enter_context ctx pack unpack x = inp_fn y = torch compile fn backend=cnt fullgraph=True x y sum backward _test_with_hooks hooks _test_with_hooks hooks assertEqual cnt frame_count expected_compile_count _test pack_bf unpack_bf pack_mul unpack_mul inline=False expected_compile_count= _test pack_bf unpack_bf pack_mul unpack_mul inline=True expected_compile_count= torch _functorch config patch donated_buffer=True torch _functorch config patch saved_tensors_hooks_filtering_mode= no_static test_saved_tensors_hooks_donated_buffers pack_gm unpack_gm = saved_tensors_hooks_to_gm pack_fp unpack_fp pack_hash unpack_hash logger_name = torch _functorch _aot_autograd graph_compile SAF torch autograd Function staticmethod forward ctx x ctx save_for_backward x x staticmethod backward ctx gx saved_x = ctx saved_tensors gx + saved_x fn x x = x x = SAF apply x x torch nn functional relu x inp = torch rand requires_grad=True No donated buffers without hooks relu saves input which also user output assertLogs logger_name level= INFO captured out = torch compile fn backend= aot_eager fullgraph=True dynamic=False inp out sum backward expected_msg = bw_donated_idxs= FileCheck check expected_msg run \n join captured output Hooks applied all saved we set saved_tensors_hooks_no_filtering=True Results hooks become donated buffers inp = torch rand requires_grad=True torch autograd graph saved_tensors_hooks pack_gm unpack_gm assertLogs logger_name level= INFO captured out = torch compile fn backend= aot_eager fullgraph=True dynamic=False inp out sum backward expected_msg = bw_donated_idxs= FileCheck check expected_msg run \n join captured output entries here don t work need fixed Each one these bug needs investigated aot_autograd_failures = data-dependent control flow xfail cov xfail nn functional gaussian_nll_loss xfail tensor_split xfail corrcoef xfail quantile xfail nanquantile xfail narrow xfail istft xfail linalg eig skip as_strided_scatter skip as_strided partial_views flaky Given input size s xs x Calculated output size skip max_pool d_with_indices_backward Misc xfail to_sparse xfail corrcoef xfail cov xfail chalf RuntimeError sum_cpu implemented ComplexHalf xfail sparse sampled_addmm xfail sparse mm reduce skip nn functional binary_cross_entropy_with_logits seems fail sometimes skip nn functional margin_ranking_loss seems flaky skip linalg lu_solve flaky decorate matmul decorator=unittest skipIf IS_ARM flaky decorate __rmatmul__ decorator=unittest skipIf IS_ARM flaky overrides atol= e- rtol= e- would do well decorate svd_lowrank decorator=toleranceOverride torch float tol atol= e- rtol= e- decorate linalg householder_product decorator=unittest skipIf IS_MACOS IS_X flaky decorate linalg pinv singular This delta coming entirely clone tangents AOTDispatcher make them contiguous decorator=toleranceOverride torch float tol atol= e- rtol= e- decorate nn functional interpolate bicubic decorator=toleranceOverride torch float tol atol= e- rtol= e- conv d sometimes nondeterministic config decorate nn functional conv d decorator=unittest skipIf IS_ARM flaky TEST_MKL aot_autograd_failures update decorate matmul decorator=toleranceOverride torch float tol atol= e- rtol= e- decorate __rmatmul__ decorator=toleranceOverride torch float tol atol= e- rtol= e- symbolic_aot_autograd_failures = xfail combinations aten masked_select default xfail index_fill Cannot call sizes tensor symbolic sizes strides xfail linalg lstsq aten linalg_lstsq default - couldn t find symbolic meta function decomposition xfail linalg lstsq grad_oriented aten linalg_lstsq default - couldn t find symbolic meta funct xfail linalg lu_solve aten linalg_lu_solve default - couldn t find symbolic meta function deco skip nn functional batch_norm tracked proxy torch fx experimental proxy_te xfail nn functional binary_cross_entropy aten fill_ Scalar - couldn t find symbolic meta funct xfail nn functional cross_entropy Cannot call sizes tensor symbolic sizes strides xfail nn functional ctc_loss aten _ctc_loss Tensor - couldn t find symbolic meta function deco xfail nn functional fractional_max_pool d rand received invalid combination arguments - g xfail trace Cannot call sizes tensor symbolic sizes strides decorate linalg householder_product decorator=unittest skipIf IS_MACOS IS_X flaky _test_aot_autograd_helper device dtype op dynamic=False disable_functionalization=False op supports_autograd skipTest Op does support autograd aot_autograd_check able check data specialization randomizing inputs Here s list ops really do like random inputs which we want disable cant_check_data_specialization = set nn functional max_unpool d nn functional max_unpool d nn functional max_unpool d try_check_data_specialization = op name cant_check_data_specialization sample_inputs_itr = op sample_inputs device dtype requires_grad=True sample_input sample_inputs_itr t_args = sample_input input + list sample_input args t_kwargs = sample_input kwargs try aot_autograd_check op op t_args t_kwargs dynamic assertRaisesRegex assertEqual check_gradients=True try_check_data_specialization=try_check_data_specialization skip_correctness_check=op skip_correctness_check_compile_vs_eager disable_functionalization=disable_functionalization except DynamicOutputShapeException skipTest Dynamic output shape operation trace except GuardOnDataDependentSymNode Carveout getitem I don t want xfail entire test because will reject known good tests see https github com pytorch pytorch issues op name == __getitem__ skipTest Dynamic output shape operation trace raise _test_aot_autograd_module_helper device dtype training module_info dynamic=False module_cls = module_info module_cls module_inputs = module_info module_inputs_func module_info device=device dtype=dtype requires_grad=True training=training module_input module_inputs module_input forward_input None continue args kwargs = module_input constructor_input args module_input constructor_input kwargs m = module_cls args kwargs m device dtype m train training Lazy modules need see input first initialize params args kwargs = module_input forward_input args module_input forward_input kwargs flat_args args_spec = pytree tree_flatten args kwargs PackedSequence only used RNNs It might possible fake-ify they re pytrees torchdynamo already doesn t support RNNs any tuple isinstance flat_arg PackedSequence flat_arg flat_args continue issubclass module_info module_cls torch nn modules lazy LazyModuleMixin torch no_grad m args kwargs sentinel_val = - is_tensor_spec = sentinel_val isinstance arg torch Tensor arg arg flat_args args = arg arg flat_args isinstance arg torch Tensor f params_buffers_args named_params named_buffers args = params_buffers_args cur_flat_args = list is_tensor_spec args = iter args idx v enumerate cur_flat_args v == sentinel_val cur_flat_args idx = next args c_args c_kwargs = pytree tree_unflatten cur_flat_args args_spec params_and_buffers = named_params named_buffers torch func functional_call m params_and_buffers c_args c_kwargs named_params = dict m named_parameters remove_duplicate=False named_buffers = dict m named_buffers remove_duplicate=False num_params_buffers = len named_params + len named_buffers compiled_f = aot_function f nop num_params_buffers=num_params_buffers dynamic=dynamic params_buffers_args = named_params named_buffers args _test_aot_autograd_forwards_backwards_helper f compiled_f params_buffers_args assertRaisesRegex assertEqual True TestEagerFusionOpInfo AOTTestCase ops op_db + hop_db allowed_dtypes= torch float skipOps TestEagerFusionOpInfo test_aot_autograd_exhaustive aot_autograd_failures test_aot_autograd_exhaustive device dtype op _test_aot_autograd_helper device dtype op ops op_db + hop_db allowed_dtypes= torch float patch functorch compile config debug_assert True skipOps TestEagerFusionOpInfo test_aot_autograd_symbolic_exhaustive aot_autograd_failures &#124; symbolic_aot_autograd_failures test_aot_autograd_symbolic_exhaustive device dtype op _test_aot_autograd_helper device dtype op dynamic=True ops op_db + hop_db allowed_dtypes= torch float skipOps TestEagerFusionOpInfo test_aot_autograd_disable_functionalization_exhaustive aot_autograd_failures test_aot_autograd_disable_functionalization_exhaustive device dtype op _test_aot_autograd_helper device dtype op disable_functionalization=True ops op_db + hop_db allowed_dtypes= torch float patch functorch compile config debug_assert True skipOps TestEagerFusionOpInfo test_aot_autograd_disable_functionalization_symbolic_exhaustive aot_autograd_failures &#124; symbolic_aot_autograd_failures test_aot_autograd_disable_functionalization_symbolic_exhaustive device dtype op _test_aot_autograd_helper device dtype op dynamic=True disable_functionalization=True aot_autograd_module_failures = set torch nn CTCLoss torch _subclasses fake_tensor DynamicOutputShapeException aten _ctc_loss default torch nn GaussianNLLLoss RuntimeError It appears you re trying get value out tracing tensor aten _local_scalar_dense default - erroring out It s likely caused data-dependent control flow similar torch nn MultiLabelMarginLoss AssertionError The values attribute shape do match torch Size = torch Size Outputs operator different eager-mode PyTorch vs AOTAutograd This means operator will have incorrect output underneath torch compile This could because operator s implementation traceable there bug AOTAutograd torch nn TransformerEncoder DataDependentOutputException aten eq compares mask input causal mask tensor see Boolean is_causal should set TransformerEncoder layers MHA sdp custom kernels torch nn Transformer DataDependentOutputException aten equal compares mask input causal mask tensor see Boolean is_causal should set TransformerEncoder layers MHA sdp custom kernels bubbles up Transformer symbolic_aot_autograd_module_failures = torch nn Transformer DataDependentOutputException aten equal compares mask input mask producing bool torch nn TransformerEncoder DataDependentOutputException aten equal compares mask input mask producing bool torch nn GaussianNLLLoss NotImplementedError local_scalar_dense item NYI torch bool torch nn FractionalMaxPool d int argument must string bytes-like object number SymFloat torch nn BCELoss new_size = _infer_size target size weight size RuntimeError expected int position got SymInt TestEagerFusionModuleInfo AOTTestCase modules module_db allowed_dtypes= torch float decorateForModules unittest expectedFailure aot_autograd_module_failures test_aot_autograd_module_exhaustive device dtype training module_info _test_aot_autograd_module_helper device dtype training module_info modules module_db allowed_dtypes= torch float decorateForModules unittest expectedFailure aot_autograd_module_failures &#124; symbolic_aot_autograd_module_failures test_aot_autograd_symbolic_module_exhaustive device dtype training module_info _test_aot_autograd_module_helper device dtype training module_info dynamic=True instantiate_parametrized_tests TestAOTAutograd instantiate_parametrized_tests TestAOTModuleSimplified only_for = cpu instantiate_device_type_tests TestPythonKey globals only_for=only_for instantiate_device_type_tests TestEagerFusionOpInfo globals only_for=only_for instantiate_device_type_tests TestEagerFusionModuleInfo globals only_for=only_for xfail_inherited_tests test_set__and_data_mutation_bad test_subclass_metadata_mutation_req_grad_True test_subclass_metadata_mutation_req_grad_False TestAOTAutogradWithDynamo TestAOTAutograd These same TestAOTAutograd tests we run dynamo first get graph module assertExpectedInline args kwargs These will have different outputs because dynamo returns different graph module But we don t really care about assertion when testing dynamo only outputs match etc pass make_compiler graph_cell make_boxed_compiler partial extract_graph graph_cell=graph_cell Compiler passes dynamo run_autograd f Callable fw_graph_cell list Optional Callable decompositions Optional dict keep_input_mutations bool dynamic bool Runs dynamo aot_autograd specified settings dynamo_compiler gm inputs kwargs result = aot_module_simplified gm inputs fw_compiler=self make_compiler fw_graph_cell bw_compiler=self make_compiler None decompositions=decompositions keep_inference_input_mutations=keep_input_mutations Dynamic calculated whether inputs have fake tensors result torch_compile_wrapper args kwargs torch _dynamo reset fn = torch compile f backend=dynamo_compiler try result = fn args kwargs except torch _dynamo exc BackendCompilerFailed e So assertRaises works properly raise e inner_exception e result torch_compile_wrapper test_inputs_overlapping_unsqueeze_with_mutation f x y x add_ y add_ x run f base = torch ones inputs = base unsqueeze base unsqueeze f inputs optf = torch compile backend= aot_eager dynamic=True f out = run f optout = run optf assertEqual out optout test_inputs_overlapping_with_mutation_guard_base f x y x add_ y add_ x run f base = torch ones inputs = base base f inputs optf = torch compile backend= aot_eager dynamic=True f out = run f optout = run optf assertEqual out optout test_mutations_in_bw_detached_from_tangent AF torch autograd Function staticmethod forward ctx dummy inplace_tensor ctx inplace_tensor = inplace_tensor dummy clone staticmethod backward ctx grad_output inplace_tensor = ctx inplace_tensor gradient_attachment = grad_output + inplace_tensor add_ gradient_attachment grad_output None None fn dummy inplace_tensor AF apply dummy inplace_tensor _inps dummy = torch zeros requires_grad=True inplace_tensor = torch zeros requires_grad=False dummy inplace_tensor inps = _inps out = fn inps ref_inps_after_fw = x clone detach x inps out sum backward ref_inps_after_bw = x clone detach x inps inps = _inps out = torch compile fn backend= aot_eager fullgraph=True inps inps_after_fw = x clone detach x inps out sum backward inps_after_bw = x clone detach x inps assertEqual ref_inps_after_fw inps_after_fw assertEqual ref_inps_after_bw inps_after_bw test_mutation_of_input_in_fw_and_bw AF torch autograd Function staticmethod forward ctx dummy inplace_tensor inplace_tensor add_ ctx inplace_tensor = inplace_tensor dummy clone staticmethod backward ctx grad_output inplace_tensor = ctx inplace_tensor inplace_tensor add_ grad_output None None fn dummy inplace_tensor AF apply dummy inplace_tensor inps dummy = torch randn requires_grad=True inplace_tensor = torch zeros requires_grad=False dummy inplace_tensor sc_inps dummy = TwoTensor torch randn requires_grad=True torch randn requires_grad=True inplace_tensor = TwoTensor torch zeros requires_grad=False torch zeros requires_grad=False dummy inplace_tensor _inps inps sc_inps dummy inplace = _inps y = fn dummy inplace ref = inplace clone detach y sum backward ref = inplace clone detach dummy inplace = _inps y = torch compile fn backend= aot_eager fullgraph=True dummy inplace assertEqual ref inplace y sum backward assertEqual ref inplace MockFXGraphCache In memory version FXGraphCache so we can isolate testing FXGraphCache __init__ - None cache = save key gm cache key = gm load gm inputs key _ = compiled_fx_graph_hash gm inputs key cache cache key = gm gm _ = load_with_key key inputs None None None None None gm load_with_key key debug_lines inputs local remote_cache is_backward constants evaluate_guards gm = cache get key gm None gm = make_boxed_func gm gm = MockFXGraphCacheOutput gm gm _fx_graph_cache_key = key cache_key debug lines gm _fx_graph_cache_debug_lines = gm _time_taken_ns = gm The following tests fail strict caching mode i e they bypass cache miss instead cache hitting They will fixed PRs above FAILING_CACHE_TESTS = BypassAOTAutogradCache unsupported nodes test_backward_mutation_data Custom Autograd Function test_backward_mutation_metadata Custom Autograd Function test_input_output_aliase_custom_autograd_function xfail_inherited_tests FAILING_CACHE_TESTS TestAOTAutogradWithCache TestAOTAutogradWithDynamo In memory version FXGraphCache so we can isolate testing FXGraphCache make_compiler fw_graph_cell mock_inductor_cache = inductor_cache compiler gm example_inputs nonlocal mock_inductor_cache fw_graph_cell result = mock_inductor_cache load gm example_inputs fw_graph_cell = gm result compiler = SerializableAOTDispatchCompiler MockFXGraphCacheOutput compiler compiler run_autograd f Callable fw_graph_cell list Optional Callable decompositions Optional dict keep_input_mutations bool dynamic bool super run_autograd f fw_graph_cell decompositions keep_input_mutations dynamic torch _functorch config patch enable_autograd_cache True strict_autograd_cache True torch _inductor config patch fx_graph_cache True verify_aot_autograd f inp_ Union Callable list Any test_mutation bool = False keep_inp_mutations bool = False decompositions Optional dict = None dynamic bool = False Only active when inp_ Callable TODO probably consolidate all tests make inp Callable make_inputs_subclasses bool = False inductor_cache = MockFXGraphCache AOTAutogradCache clear patch torch _inductor codecache FxGraphCache load_with_key new=self inductor_cache load_with_key super verify_aot_autograd f inp_ test_mutation=test_mutation keep_inp_mutations=keep_inp_mutations decompositions=decompositions dynamic=dynamic make_inputs_subclasses=make_inputs_subclasses test_input_mutation_false_aliasing This test disabled because fails strict cache mode But also can t xfailed because causes undefined behavior ASAN skipTest Skipping because fails strict cache mode __name__ == __main__ run_tests