mypy allow-untyped-decorators __future__ annotations typing Any torch torch ao quantization fake_quantize _is_symmetric_quant torch ao quantization utils is_per_tensor torch quantization FakeQuantize torch quantization observer MinMaxObserver AdaroundFakeQuantizer FakeQuantize This FakeQuantizer enables adaptive rounding fake quantizer Adaround technique adaptively round weights derived paper https arxiv org pdf pdf For HTP compatibility we targeting use symmetric quantization scale torch Tensor zero_point torch Tensor V torch nn Parameter __init__ observer type = MinMaxObserver qscheme torch qscheme = torch per_tensor_symmetric used needed fakequant quant_min int = - quant_max int = ch_axis int = observer_kwargs Any - None super __init__ observer=observer qscheme=qscheme quant_min=quant_min quant_max=quant_max is_dynamic=False observer_kwargs Populate quant_min quant_max observer_kwargs valid quant_min None quant_max None quant_min quant_max raise AssertionError quant_min must less than equal quant_max f got quant_min quant_min quant_max quant_max qscheme torch qscheme = qscheme is_per_tensor bool = is_per_tensor qscheme is_symmetric bool = _is_symmetric_quant qscheme is_symmetric raise AssertionError Only symmetric quantization supported ch_axis int = ch_axis scale = torch tensor requires_grad=False zero_point = torch tensor requires_grad=False V = torch nn Parameter torch tensor requires_grad=True Fixed Stretch parameters zeta torch Tensor = torch tensor requires_grad=False gamma torch Tensor = torch tensor - requires_grad=False sigmoid = torch nn Sigmoid use_soft_rounding = True torch jit export calculate_qparams - tuple torch Tensor torch Tensor type ignore override scale zero_point torch jit export extra_repr - str f fake_quant_enabled= fake_quant_enabled observer_enabled= observer_enabled f quant_min= activation_post_process quant_min quant_max= activation_post_process quant_max f dtype= dtype qscheme= qscheme ch_axis= ch_axis f scale= scale zero_point= zero_point V = int sum = V = int sum enable_weight_fake_quant - None fake_quant_enabled = get_rectified_sigmoid_func - torch Tensor use_soft_rounding torch clamp sigmoid V zeta - gamma + gamma min= max= This will dump binary solution V = int torch jit ignore update_scale X torch Tensor _scale torch Tensor _zero_point torch Tensor - None scale numel == scale data = _scale X device zero_point = _zero_point X device scale data = _scale is_symmetric zero_point = _zero_point zero_point = torch zeros_like _zero_point i range X dim i == ch_axis continue zero_point = zero_point unsqueeze i X_q = X scale X_q_floor = torch floor X_q residual = X_q - X_q_floor torch all torch ge residual raise AssertionError residual should non-negative V_init = -torch log zeta - gamma residual - gamma - V data = V_init forward X torch Tensor - torch Tensor observer_enabled == X_detached = X detach activation_post_process X_detached _scale _zero_point = activation_post_process calculate_qparams _scale _zero_point = _scale scale device _zero_point zero_point device dims = list range X dim is_per_tensor dims remove ch_axis is_per_tensor i range X dim i == ch_axis continue _scale = _scale unsqueeze i _zero_point = _zero_point unsqueeze i update_scale X_detached _scale _zero_point fake_quant_enabled == Perform soft quantization See equation Adaround paper h_v = get_rectified_sigmoid_func X_q = X scale Straight-Through Estimator floor function X_q_floor = torch floor X_q + zero_point Regardless rounding gradient should able flow back V X_q_dq With adaround we don t train weight train V only X_q_dq = torch clamp X_q_floor + h_v min=self quant_min max=self quant_max - zero_point scale X_q_dq X