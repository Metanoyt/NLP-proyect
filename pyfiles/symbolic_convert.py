Core module responsible converting Python bytecode into TorchDynamo s symbolic execution format This module implements bytecode-level tracing system allows TorchDynamo analyze transform Python code It converts Python bytecode instructions into symbolic format tracks flow tensors other values through program Key components - InstructionTranslatorBase Base converting bytecode symbolic execution - InstructionTranslator Main translator function bytecode - InliningInstructionTranslator Handles inlining called functions - SpeculationLog Manages state speculative execution rollback The symbolic conversion process handles - Control flow loops conditionals etc - Function inlining call stack management - Tracking program values side effects - Graph breaks resumption points - Exception handling stack frame management This core part TorchDynamo s tracing system enables ahead-of-time optimization PyTorch programs __future__ annotations collections collections abc contextlib copy dataclasses dis functools importlib inspect itertools linecache logging operator re sys threading traceback types weakref collections deque traceback StackSummary typing Any cast NoReturn Optional TYPE_CHECKING TypeAlias Union typing_extensions TypeIs torch torch _logging torch _dynamo exc ObservedException TensorifyScalarRestartAnalysis torch _guards tracing TracingContext torch _logging structured dump_file torch fx experimental symbolic_shapes guard_bool torch utils _functools cache_method config exc graph_break_hints logging torchdynamo_logging trace_rules variables bytecode_analysis get_indexof JUMP_OPNAMES livevars_analysis propagate_line_nums bytecode_transformation cleaned_instructions create_binary_slice create_call_function create_call_function_ex create_copy create_dup_top create_instruction create_jump_absolute create_rot_n create_swap get_code_keys Instruction is_generator is_jump_absolute unique_id code_context code_context codegen PyCodegen exc ArgsMismatchError BackendCompilerFailed collapse_resume_frames format_graph_break_message get_stack_above_dynamo ResumePrologueTracingError StepUnsupported unimplemented_v Unsupported funcname_cache get_funcname guards GuardBuilder install_guard output_graph GraphCompileReason OutputGraph StackLocalsMetadata polyfills impl_CONTAINS_OP_fallback replay_record DummyModule ExecutionRecorder resume_execution ContinueExecutionCache IS_TRACING_RESUME_PROLOGUE_VARNAME ReenterWith source AttrSource DictGetItemSource GlobalSource GlobalWeakRefSource LocalCellSource LocalSource SkipGuardSource Source trace_rules is_builtin_constant is_forbidden utils _get_error_on_graph_break counters get_fake_value get_instruction_source_ get_metrics_context graph_break_dup_warning_checker istype LazyString proxy_args_kwargs variables base typestr ValueMutationNew VariableTracker variables builder FrameStateSizeEntry VariableBuilder wrap_fx_proxy variables builtin BuiltinVariable variables constant ConstantVariable variables ctx_manager ContextWrappingVariable GenericContextWrappingVariable WithEnterFunctionVariable WithExitFunctionVariable variables dicts ConstDictVariable SetVariable variables functions BaseUserFunctionVariable LocalGeneratorFunctionVariable LocalGeneratorObjectVariable NestedUserFunctionVariable SkipFunctionVariable UserFunctionVariable UserMethodVariable variables iter MAX_ITERATOR_LIMIT variables lazy LazyVariableTracker variables lists BaseListVariable IteratorVariable ListIteratorVariable ListVariable SliceVariable TupleVariable variables misc CellVariable ExceptionVariable GetAttrVariable NullVariable PythonModuleVariable UnknownVariable variables nn_module NNModuleVariable UnspecializedNNModuleVariable variables streams SymbolicStreamState variables tensor supported_comparison_ops SymNodeVariable TensorVariable variables torch_function SymbolicTorchFunctionState TorchFunctionModeVariable variables user_defined RemovableHandleVariable UserDefinedClassVariable UserDefinedExceptionClassVariable UserDefinedExceptionObjectVariable UserDefinedObjectVariable TYPE_CHECKING collections abc Callable Generator Sequence torch _subclasses fake_tensor FakeTensorMode package CompilePackage log = logging getLogger __name__ graph_break_log = torch _logging getArtifactLogger __name__ graph_breaks trace_call_log = torch _logging getArtifactLogger __name__ trace_call trace_source_log = torch _logging getArtifactLogger __name__ trace_source trace_bytecode_log = torch _logging getArtifactLogger __name__ trace_bytecode tls = threading local compare_op_handlers dict str Any = k BuiltinVariable v call_function k v supported_comparison_ops items handle_contains = BuiltinVariable operator contains call_function handle_not = BuiltinVariable operator not_ call_function compare_op_handlers = lambda tx args _ handle_contains tx reversed args compare_op_handlers = lambda tx args _ handle_not tx handle_contains tx reversed args PT _ISSUE_TRACKER_URL = https github com pytorch pytorch issues new labels=oncall A+pt projects= template=pt -bug-report yml ExceptionVals TypeAlias = Union variables ExceptionVariable UserDefinedExceptionClassVariable UserDefinedExceptionObjectVariable functools cache _import_module name str - types ModuleType Import named module cache result importlib import_module seems do some filesystem checking validate name so caching can slow importlib import_module name dataclasses dataclass SpeculationEntry filename str lineno int instruction_pointer int inst Instruction debugging only _failed bool = False error_on_graph_break Optional bool = None reason Optional GraphCompileReason = None fail_and_restart_analysis error_on_graph_break bool - None Start tracing current frame over again don t take branch _failed = True error_on_graph_break = error_on_graph_break reason None restart_reason = reason reason restart_reason = Unknown fail_and_restart_analysis raise exc SpeculationRestartAnalysis restart_reason=restart_reason failed tx InstructionTranslatorBase - bool _failed assert error_on_graph_break None tx error_on_graph_break = error_on_graph_break True False dataclasses dataclass SpeculationLog SpeculationLog replaces prior copy_graphstate restore_graphstate checkpointing Rather than saving restoring state we restart dynamo conversion process over beginning -- when we hit start speculation failed we instead generate graph break entries list SpeculationEntry = dataclasses field default_factory=list index int = restart - None index = clear - None entries clear index = next filename str lineno int instruction_pointer int inst Instruction - SpeculationEntry Lookup create SpeculationEntry shared across RestartAnalysis calls Args used only debug checks len entries == index entries append SpeculationEntry filename lineno instruction_pointer inst entry = entries index prev_entry_msg = index = prev_entry = entries index - prev_entry_msg = f Previous instruction prev_entry filename prev_entry lineno f prev_entry inst opname prev_entry instruction_pointer \n entry instruction_pointer == instruction_pointer entry filename == filename entry lineno == lineno raise SpeculationLogDivergence f SpeculationLog diverged index index log had len entries entries - Expected entry filename entry lineno entry inst opname ip= entry instruction_pointer - Actual filename lineno inst opname ip= instruction_pointer prev_entry_msg There two usual reasons why may have occurred - When Dynamo analysis restarted second run took different path than first If occurred previous instruction critical instruction behaved differently - Speculation entries only added under certain conditions seen step e g there must exist operators graph those conditions may have changed restart If divergence intentional clear speculation log before restarting do NOT do graph breaks you will infinite loop Otherwise please submit bug report ideally including contents TORCH_LOGS=+dynamo index += entry dataclasses dataclass LocalState automatic_dynamic dict str FrameStateSizeEntry = dataclasses field default_factory=dict render - str \n join f k v render k v automatic_dynamic items Mutable box shared across restarts dataclasses dataclass DistributedState compile_pg Any local_state LocalState all_states Optional list LocalState = None TensorifyState These set string symfloats names eg zf we collect tensorify_python_scalars py joint fx pass inform us about which float inputs we should specialize when we restart analysis force_specializations set str = set classmethod specialize cls index str - None cls force_specializations add index classmethod should_specialize cls index str - bool index cls force_specializations classmethod clear cls - None cls force_specializations clear classmethod empty cls - bool len cls force_specializations == functools cache _step_logger - Callable None torchdynamo_logging get_step_logger log contextlib contextmanager save_and_restart_speculation_log tx InstructionTranslatorBase - Generator None None None When reconstructing generator after graph break we advance until fully exhausted This process adds new entries speculation log previously observed Without temporarily clearing speculation log could lead divergence error entries = tx speculation_log entries index = tx speculation_log index try tx speculation_log entries = tx speculation_log index = yield finally tx speculation_log entries = entries tx speculation_log index = index contextlib contextmanager temporarely_allow_writes_to_output_graph tx InstructionTranslatorBase - Generator None None None try tmp = tx output should_exit tx output should_exit = False yield finally tx output should_exit = tmp dataclasses dataclass BlockStackEntry Current instruction pushes something block_stack inst Instruction target Instruction stack_index int with_context Optional Union ContextWrappingVariable GenericContextWrappingVariable = None can_restore - bool with_context None resume_fn - ReenterWith assert stack_index None with_context hasattr with_context target_values with_context target_values ReenterWith stack_index - tuple with_context target_values ReenterWith stack_index - exit tx InstructionTranslatorBase is_graph_break bool - None assert with_context None is_graph_break with_context exit_on_graph_break is_graph_break with_context exit tx type ignore arg-type SpeculationLogDivergence AssertionError pass ReturnValueOp Exception pass YieldValueOp Exception Signal symbolic tracer stop control flow caller stack_op fn Callable object - Callable Any nargs = len inspect signature fn parameters fn_var = BuiltinVariable fn functools wraps fn impl InstructionTranslator inst Instruction - None push fn_var call_function popn nargs impl is_stdlib mod object - bool isinstance mod types ModuleType False mod __name__ split sys stdlib_module_names functools cache get_assert_bytecode_sequence with_msg bool - list str with_msg fn x Any - None assert x msg fn x Any - None assert x insts = inst opname inst dis get_instructions fn expect find POP_JUMP_ FORWARD_ IF_TRUE begin_idx = next i i inst enumerate insts inst startswith POP_JUMP end_idx = insts index RAISE_VARARGS insts begin_idx + end_idx + _detect_and_normalize_assert_statement InstructionTranslatorBase truth_fn Callable object bool push bool - bool Detect jump instruction assert normalize assert pushing dummy error message when nothing given Python - assertion following format minus small differences POP_JUMP_IF_TRUE LOAD_ASSERTION_ERROR LOAD_CONST Assert message - optional instruction CALL_FUNCTION - optional instruction RAISE_VARARGS truth_fn operator truth push False assert isinstance instruction_pointer int current_instruction_pointer = instruction_pointer with_msg False True assert_insts = get_assert_bytecode_sequence with_msg cur_insts = instructions current_instruction_pointer current_instruction_pointer + len assert_insts cur_insts = inst opname inst cur_insts cur_insts == assert_insts with_msg load_const_idx = assert_insts index LOAD_CONST error_msg = instructions current_instruction_pointer + load_const_idx argval error_msg = assertion error push ConstantVariable create error_msg True False explain = False NOTE graph break handling symbolic_convert There possible graph break cases InstructionTranslatorBase handles Regular graph breaks CALL BINARY_SUBSCR etc implemented break_graph_if_unsupported Data-dependent condition graph breaks implemented generic_jump STORE_ATTR graph breaks implemented InstructionTranslatorBase STORE_ATTR All other unhandled graph breaks - unsupported step graph breaks implemented InstructionTranslatorBase step Graph breaks handled following manner The Unsupported exception caught If we cannot compile partial graph should_compile_partial_graph False then propagate exception upward For unsupported step graph breaks condition abort partial compilation more restrictive see InstructionTranslatorBase step If Unsupported exception escapes symbolic_convert py then we done Otherwise we want attempt partial compilation Log graph break via log_graph_break If we re handling data-dependent graph break type then we can immediately codegen compiled graph resume function we re done This because jump instruction we graph break limited how can manipulate Python state say comparison CALL which can modify Python state arbitrarily Otherwise we need restart compilation We need restart because processing unsupported instruction we may have modified VariableTrackers we need all our VariableTrackers state BEFORE tracing unsupported instruction During first compilation we updated speculation log indicating points code we can resume On second compilation we will stop tracing first speculation log fails Then we compile partial graph resume function Logging invariants No logs need made Unsupported escapes symbolic_convert py Python s default exception printing will print out all necessary information no partial compilation will attempted log_graph_break should called soon Unsupported caught we determined we want partial compile This always happens first compilation NOT restart handling graph Any compile_subgraph call should preceded immediately log form triggered compile generic_jump truth_fn Callable object bool push bool - Callable InstructionTranslatorBase Instruction None graph break message fields data dependent branching _gb_type = Data-dependent branching _explanation = Detected data-dependent branching e g ` my_tensor sum ` Dynamo does support tracing dynamic control flow _hints = graph_break_hints FUNDAMENTAL Use ` torch cond ` express dynamic control flow jump_graph_break InstructionTranslatorBase inst Instruction value VariableTracker extra_msg str = - None assert should_compile_partial_graph log_graph_break code_options reason=format_graph_break_message gb_type=_gb_type context=f attempted jump value explanation=_explanation hints=_hints compile partial subgraph prefix then jump into user code maybe_has_backedge msg = Skipping frame because there graph break while loop\n f frame_summary log info msg raise exc SkipFrame msg push value log debug generic_jump triggered compile all_stack_locals_metadata = output compile_subgraph reason=GraphCompileReason f generic_jump typestr value extra_msg frame_summary stack_pops= pop if_next = create_call_resume_at next_instruction all_stack_locals_metadata push push value assert inst target None if_jump = create_call_resume_at inst target all_stack_locals_metadata sys version_info = requires stack - bool type output add_output_instructions create_instruction TO_BOOL jump_inst = create_instruction inst opname target=if_jump jump_inst copy_positions inst output add_output_instructions jump_inst + if_next + if_jump inner InstructionTranslatorBase inst Instruction - None value VariableTracker = pop config rewrite_assert_with_torch_assert _detect_and_normalize_assert_statement truth_fn push error_msg VariableTracker = pop Skip over things like ` assert True ` value is_python_constant bool value as_python_constant jump inst should_compile_partial_graph jump_graph_break inst value unimplemented_v gb_type= Data-dependent assertion failed cannot compile partial graph context=f value value explanation= Dynamo has determined when encountering data-dependent assert failure should compile partial graph hints= graph_break_hints FUNDAMENTAL Use ` torch _assert ` raise hard AssertionError when check fails This error will propagate back user code called compiled function i e Dynamo will trace any exception handling Remove assert statement Move assert statement outside any context managers order graph break partial graph compilation fullgraph=False TODO maybe should respect DtoH sync intention users later Manually insert torch _assert_async instead python assert jump over assert related instructions we don t need them anymore we see Tensor assert statement no need call scalar_tensor isinstance value TensorVariable output create_proxy call_function torch _assert_async proxy_args_kwargs value error_msg jump inst isinstance value SymNodeVariable assertion normal shape expression just install guard bail out sym_expr = value sym_num isinstance sym_expr torch SymBool sym_expr = sym_expr = result = torch fx experimental symbolic_shapes expect_true sym_expr result unimplemented_v gb_type= Assertion failed symbolic shapes context=str sym_expr explanation= hints= graph_break_hints USER_ERROR jump inst scalar_to_tensor_proxy = output create_proxy call_function torch scalar_tensor proxy_args_kwargs value scalar_to_tensor = wrap_fx_proxy scalar_to_tensor_proxy example_value=get_fake_value scalar_to_tensor_proxy node output create_proxy call_function torch _assert_async proxy_args_kwargs scalar_to_tensor error_msg jump inst value is_python_constant ConstDictVariable optimized very lazy about insertion guards so we have manually insert SEQUENCE_LENGTH guard here isinstance value ConstDictVariable value source install_guard value source make_guard GuardBuilder SEQUENCE_LENGTH truth_fn value as_python_constant push push value jump inst isinstance value TensorVariable should_compile_partial_graph jump_graph_break inst value isinstance value NNModuleVariable Equivalent nn_module None mod = output get_submodule value module_key truth_fn mod push push value jump inst isinstance value UserDefinedObjectVariable try x = value var_getattr __bool__ type ignore arg-type except exc ObservedAttributeError exc handle_observed_exception __bool__ missing trying __len__ infer truth value try x = value var_getattr __len__ type ignore arg-type except exc ObservedAttributeError exc handle_observed_exception x = None __bool__ __len__ function isinstance x UserMethodVariable result = x call_function type ignore arg-type assignment isinstance result ConstantVariable isinstance result value bool int truth_fn result value push push value jump inst isinstance result SymNodeVariable result evaluate_expr push push value jump inst unimplemented_v gb_type= Data-dependent branching non-constant __bool__ context=f method x result result explanation= Attempted perform data-dependent branching user-defined object __bool__ method did constant hints= __bool__ __len__ non-function existed user defined object truth_fn True push push value jump inst isinstance value TensorVariable value has_unpack_var_sequence truth_fn len value unpack_var_sequence push push value jump inst isinstance value SymNodeVariable try user branching SymBool guard user has code like size then they just testing truthiness guard expr = isinstance value sym_num torch SymBool eval_result = value evaluate_expr output eval_result = guard_bool value sym_num = except exc UserError e should_compile_partial_graph jump_graph_break inst value extra_msg=f \n e raise truth_fn eval_result push push value jump inst isinstance value variables BackwardHookVariable truth_fn True push push value jump inst source is_constant_source value source None is_constant_source value source truth_fn value get_real_value type ignore attr-defined push push value jump inst unimplemented_v gb_type= Data-dependent branching context=f attempted jump value explanation=_explanation hints= graph_break_hints FUNDAMENTAL Use ` torch cond ` express dynamic control flow inner break_graph_if_unsupported push int - Callable Callable None Callable InstructionTranslatorBase Instruction None decorator inner_fn Callable None - Callable InstructionTranslatorBase Instruction None functools wraps inner_fn wrapper InstructionTranslatorBase inst Instruction - None speculation = speculate speculation failed assert speculation reason None handle_graph_break inst speculation reason try inner_fn inst except Unsupported excp active_generic_context_managers We don t support graph break under GenericContextWrappingVariable If there we roll back checkpoint fall back excp remove_from_stats unimplemented_v gb_type= Graph break under GenericContextWrappingVariable context=f Active generic context managers active_generic_context_managers explanation= Attempted graph break active context manager s doesn t support graph breaking hints= Move offending context manager s outside compiled region graph_break_hints CAUSED_BY_EARLIER_GRAPH_BREAK from_exc=excp isinstance excp exc UncapturedHigherOrderOpError raise should_compile_partial_graph raise log_graph_break code_options reason=str excp user_stack=excp real_stack maybe_has_backedge msg = Skipping frame because there graph break while loop\n f frame_summary log info msg raise exc SkipFrame msg excp excp remove_from_stats excp add_to_stats graph_break speculation reason = GraphCompileReason excp msg excp real_stack speculation fail_and_restart_analysis error_on_graph_break handle_graph_break InstructionTranslatorBase inst Instruction reason GraphCompileReason - None sys version_info = sys version_info inst opname == CALL stack effect PRECALL + CALL split between two instructions stack_effect = dis stack_effect dis opmap PRECALL inst arg + dis stack_effect dis opmap CALL inst arg stack_effect = dis stack_effect inst opcode inst arg log debug s triggered compile inst opname all_stack_locals_metadata = output compile_subgraph reason=reason stack_pops=push - stack_effect cg = PyCodegen output root_tx cleanup list Instruction = Reconstruct context variable CLASS block stack b block_stack Don t exit any modes we have entered output bytecode will mutate tf mode stack accordingly isinstance b with_context TorchFunctionModeVariable cg extend_output b resume_fn try_except_torch_function_mode cg code_options cleanup continue assert b with_context None assert isinstance b with_context ContextWrappingVariable b with_context reconstruct_type cg cg extend_output b resume_fn try_finally cg code_options cleanup output add_output_instructions cg get_instructions del cg sys version_info = inst opname == CALL kw_names = kw_names as_python_constant kw_names None len kw_names KW_NAMES no longer used assert sys version_info output add_output_instructions create_instruction KW_NAMES argval=kw_names assert inst arg None call_insts = create_call_function inst arg False call_insts - copy_positions inst output add_output_instructions call_insts copy instruction without exception table data assert inst target None inst_copy = copy copy inst inst_copy exn_tab_entry = None output add_output_instructions inst_copy output add_output_instructions cleanup popn push - stack_effect _ range push push UnknownVariable output add_output_instructions create_call_resume_at next_instruction all_stack_locals_metadata wrapper decorator BytecodeDispatchTableMeta type Installs ` cls dispatch_table ` every subclass speed up calls OPCODE __init__ cls type name str bases Any dct Any - None super __init__ name bases dct type ignore misc _missing opname str args Any - None unimplemented_v gb_type= Missing bytecode handler context=f opname args args explanation=f Dynamo does know how handle bytecode instruction ` opname ` hints= f Do trace code produces ` opname ` bytecode instruction see https docs python org library dis html bytecode semantics graph_break_hints SUPPORTABLE dispatch_table = op getattr cls opname functools partial _missing opname opname op dis opmap items pyrefly ignore missing-attribute cls dispatch_table = dispatch_table get i i range dataclasses dataclass ExceptionStack Exception stack shared among all InstructionTranslator instances Exception handling CPython bit confusing some bytecode have slightly different behavior than what documented While reading documentation important notice terms current exception stack sometimes refers C variable same name exception stack respectively The lifetime exception Python + + tx _raise_exception_variable = sets current_exception variable + PUSH_EXC_INFO = pushes current_exception exception stack + POP_EXCEPT = pops TOS exception stack _exc_stack list ExceptionVals = dataclasses field default_factory=list _current_exception Optional ExceptionVals = dataclasses field default=None clear_current_exception - None _current_exception = None set_current_exception val ExceptionVals - None _set_context_and_break_context_reference_cycle val _current_exception = val move_current_exception_to_stack - None assert _current_exception None append _current_exception clear_current_exception get_current_exception - ExceptionVals assert _current_exception None _current_exception _set_context_recursive val ExceptionVals prev_idx int - ExceptionVals ctx = val __context__ type ctx ConstantVariable type ignore union-attr val len _exc_stack + prev_idx prev = _exc_stack prev_idx _set_context_recursive prev prev_idx - val set_context prev type ignore union-attr arg-type val _break_context_reference_cycle val ExceptionVals - None See test_exceptions test_raise_does_not_create_context_chain_cycle Based https github com python cpython blob e bf e ecb ce fce ca Python errors c#L -L As noted CPython O chain length context chains usually very small o = slow_o = val slow_update_toggle = False floyd s algorithm detecting cycle while True context = o __context__ type ignore union-attr type context ConstantVariable context set break context val o set_context ConstantVariable None type ignore union-attr arg-type break o = context type ignore assignment o slow_o pre-existing cycle - all exceptions path visited checked break slow_update_toggle visited all exceptions slow_o = slow_o __context__ type ignore union-attr assignment slow_update_toggle = slow_update_toggle _set_context_and_break_context_reference_cycle val ExceptionVals - None set Exception __context__ _set_context_recursive val len _exc_stack - _break_context_reference_cycle val pop - ExceptionVals _exc_stack pop append val ExceptionVals - None _exc_stack append val __len__ - int len _exc_stack __getitem__ index int - ExceptionVals _exc_stack index __str__ - str f _exc_stack= - _current_exception= __repr__ = __str__ InstructionTranslatorBase metaclass=BytecodeDispatchTableMeta output OutputGraph symbolic_locals dict str VariableTracker symbolic_globals dict str VariableTracker symbolic_torch_function_state SymbolicTorchFunctionState symbolic_stream_state SymbolicStreamState post_prune_cell_and_freevars Optional dict str VariableTracker stack list VariableTracker instruction_pointer Optional int current_instruction Instruction block_stack list BlockStackEntry lineno int kw_names Optional ConstantVariable accept_prefix_inst bool prefix_insts list Instruction inline_depth int inconsistent_side_effects bool current_speculation Optional SpeculationEntry dispatch_table list Any exn_vt_stack ExceptionStack exec_recorder Optional ExecutionRecorder strict_checks_fn Optional Callable VariableTracker bool start_point Optional int is_leaf_tracer bool parent Optional InstructionTranslatorBase debug_locals list tuple VariableTracker list VariableTracker package Optional CompilePackage latest_bytecode_queue deque str Store latest bytecode before graph_break call user mark_inconsistent_side_effects - None InstructionTranslator has encountered instructions which may cause dynamo see different version history eager See https github com pytorch pytorch issues inconsistent_side_effects = True maybe_has_backedge - bool This function employs heuristic It does reliably detect backedge The heuristic straightforward starting current instruction continuing end any jump instruction targets instruction before current one there might backedge Python introduced changes bytecode group common paths blockstacks try allow early returns Consequently there can multiple RETURN_VALUE instructions Another heuristic halt detection upon encountering first RETURN_VALUE RETURN_CONST These heuristics can result both false positives negatives either case Dynamo code remains valid For false positives where edge incorrectly marked backedge Dynamo will perform SkipFrame instead potentially applying optimizations For false negatives where edge should marked backedge isn t multiple graphs may generated there s break graph during loop In general its better have fewer false negatives so Dynamo does skip whole frame If any parent tx has backedge then True cur_tx Optional InstructionTranslatorBase = while cur_tx None cur_offset = cur_tx current_instruction offset assert cur_tx instruction_pointer None inst cur_tx instructions cur_tx instruction_pointer inst opname RETURN_VALUE RETURN_CONST break inst opname JUMP_OPNAMES jump_offset = inst argval jump_offset cur_offset True cur_tx = cur_tx parent False cellvars - list str code_options co_cellvars freevars - list str code_options co_freevars cell_and_freevars - list str hasattr _cell_and_freevars _cell_and_freevars = cellvars + freevars _cell_and_freevars prune_dead_locals - None keep cell freevar references alive post_prune_cell_and_freevars = k v k v symbolic_locals items k cell_and_freevars Only keep locals must remain stack reads = livevars_analysis instructions current_instruction symbolic_locals = k v k v symbolic_locals items k reads call_function fn VariableTracker args list VariableTracker kwargs dict str VariableTracker - None assert isinstance fn VariableTracker assert isinstance args list assert isinstance kwargs dict assert all isinstance x VariableTracker x itertools chain args kwargs values inner_fn = None hasattr fn value inner_fn = fn value hasattr fn fn inner_fn = fn fn inner_fn callable inner_fn is_forbidden inner_fn raise AssertionError f Attempt trace forbidden callable inner_fn push fn call_function args kwargs type ignore arg-type inline_generator_function fn VariableTracker args Sequence Any kwargs dict str Any - Any Redirect call generator call_function isinstance fn LocalGeneratorFunctionVariable fn = LocalGeneratorFunctionVariable fn type ignore arg-type fn call_function args kwargs type ignore arg-type inline_user_function_return fn VariableTracker args Sequence Any kwargs dict str Any - Any A call some user defined function inlining is_leaf_tracer = False config enable_faithful_generator_behavior is_generator fn get_code type ignore attr-defined inline_generator_function fn args kwargs InliningInstructionTranslator inline_call fn args kwargs get_line_of_code_header lineno Optional int = None - str lineno None lineno = lineno inline_depth_str = f inline depth inline_depth inline_depth funcname = get_funcname f_code co_filename lineno funcname_str = funcname None f funcname f f_code co_filename lineno f_code co_name funcname_str inline_depth_str get_log_starts_line_log_str - str log_str = f TRACE starts_line get_line_of_code_header \n line = linecache getline f_code co_filename lineno rstrip log_str += f line log_str starts_line lineno int - None lineno == lineno lineno = lineno TracingContext set_current_loc f_code co_filename lineno f_code co_name is_trace_source_log_enabled trace_source_log debug s LazyString get_log_starts_line_log_str step - bool Process exactly one instruction False we should exit error_on_graph_break = _get_error_on_graph_break ip = instruction_pointer ip None False current_instruction = inst = instructions ip instruction_pointer = ip + inst starts_line starts_line inst starts_line stack should_compile_partial_graph is_non_empty_graph current_speculation = speculate current_speculation failed step_graph_break inst False is_trace_bytecode_log_enabled trace_bytecode_log debug TRACE s s s inst opname inst argval repr stack Store latest bytecode execution process Used repr byte processing limiting length config verbose try stack_repr = repr stack except ValueError Handle large integers exceed sys int_info str_digits_check_threshold stack_repr = stack repr truncated due large integer latest_bytecode_queue append f TRACE inst opname repr inst argval stack_repr update_block_stack inst try dispatch_table inst opcode inst output should_exit except TensorifyScalarRestartAnalysis raise except exc ObservedException e exception_handler e True except ReturnValueOp YieldValueOp False except Unsupported StepUnsupported e More restrictive condition than should_compile_partial_graph condition true then we SHOULD NOT attempt find previous checkpoint resume try resume - we should immediately error out The condition more restrictive because may possible resume significantly earlier code most recent speculation point This happens example case graph break try block one_graph error_on_graph_break is_tracing_resume_prologue isinstance e StepUnsupported unimplemented_v gb_type= cannot resume torch _dynamo step_unsupported context= explanation= traced torch _dynamo step_unsupported Dynamo instructed error graph break This graph break used debugging only hints= Remove torch _dynamo step_unsupported call Make sure fullgraph=False error_on_graph_break=False graph_break_hints DYNAMO_BUG raise current_speculation None log debug empty checkpoint - cannot resume graph break isinstance e StepUnsupported unimplemented_v gb_type= torch _dynamo step_unsupported empty checkpoint context= explanation= traced torch _dynamo step_unsupported there no checkpoint step_graph_break This graph break used debugging only hints= Remove torch _dynamo step_unsupported call Include least one checkpoint include least ops make sure there some line code try block has empty Python stack graph_break_hints DYNAMO_BUG raise reason = Encountered graph break we cannot resume Compiling up previous resumable state then skipping rest function f Graph break encountered \n str e log_graph_break code_options reason=reason user_stack=e real_stack current_speculation fail_and_restart_analysis error_on_graph_break False sys version_info = update_block_stack inst Instruction - None + no longer uses block stack we still keep track one so we know which contexts currently active For our purposes all exception table entries same target considered part same block NOTE we only keep track blocks contained try blocks This because we will create continuation functions graph breaks try blocks we may blocks We do push blocks here since blocks pushed when handling BEFORE_WITH entry = inst exn_tab_entry entry Detect when we have exited top block The blocks block stack enclosed try blocks so block s cleanup code should previous block any len block_stack = entry target block_stack - target entry target block_stack - target exit current block block_stack pop no longer any block It possible NOPs between two instructions same block NOPs covered exception table entry In case assume we still same block In + JUMP_BACKWARD might also covered exception table entry so we also assume we still same block It probably safe do even though we haven t encountered case before In + NOT_TAKEN might also covered exn table entry block_stack inst opname NOP JUMP_BACKWARD NOT_TAKEN If we really escape block current instruction another block then there should no other nested blocks we assert len block_stack == block_stack pop update_block_stack inst Instruction - None pass property next_instruction - Instruction assert instruction_pointer None instructions instruction_pointer step_graph_break continue_inst Instruction - None generate code checkpoint assert output output_instructions assert current_speculation None NOTE adding assert here since seems like only place where we call step_graph_break right now when stack empty so let s enforce now assert stack NOTE we support non-empty stack future ` stack_pops ` argument below should set stack length ensure stack codegen d rest function log debug step triggered compile all_stack_locals_metadata = output compile_subgraph partial_convert=True reason=GraphCompileReason step_unsupported frame_summary current frame state cells frame N locals frame N- stack + locals frame stack + locals parent eval_frame skip_code nested graph break assert config nested_graph_breaks cg = PyCodegen output root_tx codegen cells frame values only frame N cg extend_output create_copy cg create_load_const cg create_binary_subscr create_instruction BUILD_LIST arg= create_copy cg create_load_const cg create_binary_subscr create_instruction BUILD_LIST arg= No need fix stack since stack assumed empty here Do NOT handle_inactive_ctx because we will skipping resume code leaf_resume_code leaf_resume_name = create_resume continue_inst all_stack_locals_metadata cg True False skip_code leaf_resume_code current frame state cells frame N locals frame N- stack + locals frame stack + locals frame N cells frame N locals codegen_call_resume leaf_resume_code leaf_resume_name cg current frame state cells frame N locals frame N- stack + locals frame stack + locals leaf_resume result pop frame N cells locals cg extend_output create_copy cg create_load_const create_instruction DELETE_SUBSCR create_copy cg create_load_const create_instruction DELETE_SUBSCR add leaf_resume result frame N- stack num_stack = all_stack_locals_metadata num_stack cg extend_output create_instruction BUILD_LIST arg= create_copy cg create_load_const cg create_binary_subscr create_binary_slice num_stack num_stack True parent push UnknownVariable all_stack_locals_metadata num_stack += current frame state cells frame_values extract frame N- stack stack cg extend_output create_dup_top cg create_load_const cg create_binary_subscr create_binary_slice num_stack + current frame state cells frame_values frame N- stack + leaf_resume result remove frame N- stack frame_values cg extend_output frame_values = frame_values num_stack + create_copy cg create_load_const cg create_binary_subscr create_dup_top create_binary_slice num_stack + None create_swap cg create_load_const create_instruction STORE_SUBSCR current frame state cells frame_values frame N- stack + leaf_resume result unpack stack need unpack twice since UNPACK_SEQUENCE unpacks reverse order cg extend_output create_instruction UNPACK_SEQUENCE arg=num_stack + create_instruction BUILD_LIST arg=num_stack + create_instruction UNPACK_SEQUENCE arg=num_stack + call remaining resume functions current frame state frame N- cells frame cells frame N- locals frame N- stack + locals frame stack + locals frame N- stack leaf_resume result output add_output_instructions cg get_instructions + parent create_call_resume_at parent next_instruction all_stack_locals_metadata pop cells output add_output_instructions create_swap create_instruction POP_TOP load locals frame values cg = PyCodegen output root_tx output add_output_instructions cg create_load_const - cg create_binary_subscr local idx all_stack_locals_metadata - locals_names items output add_output_instructions create_dup_top cg create_load_const idx cg create_binary_subscr cg create_store local output add_output_instructions create_instruction POP_TOP create_jump_absolute continue_inst instructions run_ctx_mgr - Any NB Don t push top level frame summary set_current_loc will take care However DO make sure we attach real_stack exceptions TracingContext current_frame None run - None run_ctx_mgr dump_file f_code co_filename try output push_tx start_point = instruction_pointer try while step pass except Exception e is_tracing_resume_prologue raise ResumePrologueTracingError Error while tracing through Dynamo-generated resume function prologue Errors allowed when tracing resume function prologues \n f type e __qualname__ str e with_traceback e __traceback__ None raise except TensorifyScalarRestartAnalysis raise except BackendCompilerFailed raise except RuntimeError e hasattr e msg Data-dependent e msg readable_graph = torch fx GraphModule output nn_modules output graph print_readable print_output=False include_stride=True include_device=True e partial_fx_graph = readable_graph type ignore attr-defined raise raise except Exception e exec_recorder e exec_record = exec_recorder get_record type ignore attr-defined raise finally output pop_tx Cleanup outputGraph delete held tensors We perform cleanup only InstructionTranslator InliningInstructionTranslator The InliningInstructionTranslator mutates output object restored original state there exception isinstance InstructionTranslator output cleanup Note call maybe redundant compile_subgraph called This ok because calling exit stack close twice issue second stop no op output mark_bytecode_tracing_stop push val Optional VariableTracker - None assert val None isinstance val VariableTracker f push expects VariableTracker got typestr val stack append val type ignore arg-type push_many vals list VariableTracker - None val vals push val pop - VariableTracker stack pop popn n int - list VariableTracker reversed pop _ range n LOAD_FAST inst Instruction - None name = inst argval exec_recorder name f_locals exec_recorder add_local_var name f_locals name try push symbolic_locals name unwrap except KeyError name startswith try This happens dict list comprehensions new_name = name replace implicit push symbolic_locals new_name except KeyError unimplemented_v gb_type= Attempted read undefined local variable implicit context=f LOAD_FAST name explanation=f Could find implicit local variable name ` name ` hints= This happens dict list comprehensions graph_break_hints USER_ERROR unimplemented_v gb_type= Attempted read undefined local variable context=f LOAD_FAST name explanation=f Could find local variable name ` name ` hints= graph_break_hints USER_ERROR continuation functions name startswith __stack symbolic_locals pop name LOAD_DEREF inst Instruction - None assert inst argval cell_and_freevars cell = symbolic_locals inst argval contents_var = output side_effects load_cell cell push contents_var exec_recorder inst argval f_locals exec_recorder add_local_var inst argval f_locals inst argval STORE_FAST inst Instruction - None name = inst argval loaded_vt = pop loaded_vt set_name_hint name symbolic_locals name = loaded_vt name == IS_TRACING_RESUME_PROLOGUE_VARNAME val = loaded_vt as_python_constant assert type val bool is_tracing_resume_prologue = val DELETE_FAST inst Instruction - None del symbolic_locals inst argval STORE_DEREF inst Instruction - None type ignore override assert inst argval cell_and_freevars cell = symbolic_locals inst argval val = pop output side_effects store_cell cell val assert isinstance cell CellVariable tame mypy cell local_name None val set_name_hint cell local_name type ignore attr-defined LOAD_CLOSURE = LOAD_FAST _load_const inst Instruction - VariableTracker i = inst arg i None ConstantVariable create value=inst argval type ignore return-value val = _constants_cache i val _constants_cache i = ConstantVariable create value=inst argval type ignore call-overload val = _constants_cache i assert val None val LOAD_CONST inst Instruction - None push _load_const inst _load_global inst Instruction - None name = inst argval exec_recorder name f_globals exec_recorder add_global_var name f_globals name assert name f_builtins exec_recorder builtins name = f_builtins name name f_globals load_builtin inst name symbolic_globals variable = output side_effects symbolic_globals name push output side_effects load_global variable name value = f_globals name push VariableTracker build value GlobalSource name functools cached_property nn_modules_globals_vt - VariableTracker module_name = torch nn modules module module_source = import_source module_name fglobals_value = _import_module module_name VariableTracker build fglobals_value module_source LOAD_GLOBAL inst Instruction - None assert inst arg None sys version_info = sys version_info inst arg PUSH_NULL inst _load_global inst sys version_info = inst arg PUSH_NULL inst STORE_GLOBAL inst Instruction - None value = pop name = inst argval source = GlobalSource name name symbolic_globals symbolic_globals name = object type ignore assignment sentinel object variable = output side_effects track_global_existing source symbolic_globals name isinstance value RemovableHandleVariable unimplemented_v gb_type= Storing Tensor hook handle globals context=name explanation= This supported hints= output side_effects store_global variable name value Cache note This cache only exists duration InstructionTranslator - so should safe do cache_method import_source module_name str - GlobalSource Create alias module use guards torch_package module_name value = torch package package_importer _package_imported_modules module_name alias = module_name replace _ replace _ replace _dot_ value = _import_module module_name alias = f __import_ module_name replace _dot_ package None package add_import_source alias module_name output import_sources alias = module_name f_globals = output global_scope assert alias f_globals f_globals alias value f_globals alias = value output update_co_names alias GlobalSource alias resolve_name name str package str level int - str Copied Cpython implementation __import__ Resolve relative module name absolute one https github com python cpython blob f eea db fb cf c e ec e Lib importlib _bootstrap py#L bits = package rsplit level - len bits level raise ImportError attempted relative beyond top-level package base = bits f base name name base calc_package - str Copied Cpython implementation __import__ https github com python cpython blob f eea db fb cf c e ec e Lib importlib _bootstrap py#L package = f_globals get __package__ spec = f_globals get __spec__ package None spec None package = spec parent log warning __package__ = __spec__ parent r = r package spec parent stacklevel= package spec None spec parent log warning can t resolve package __spec__ __package__ falling back __name__ __path__ stacklevel= package = f_globals __name__ __path__ f_globals package = package rpartition package IMPORT_NAME inst Instruction - None level fromlist = popn level = level as_python_constant fromlist = fromlist as_python_constant module_name = inst argval Are we replaying so load recorded module recorded_name = f ExecutionRecorder LOCAL_MOD_PREFIX _ level _ fromlist _ module_name recorded_name f_globals value = f_globals recorded_name source = GlobalSource recorded_name try value = __import__ module_name fromlist=fromlist level=level globals=self f_globals except ImportError unimplemented_v gb_type= Import failure context=f module_name module_name fromlist fromlist level= level explanation= Failure when attempting hints= graph_break_hints USER_ERROR level = pkg = calc_package module_name = resolve_name module_name pkg level For __import__ when name variable form package module normally top-level package name up till first dot returned module named module_name However when non-empty fromlist argument given module named name returned Therefore we set source correctly here fromlist top_level_module_name = module_name partition source = import_source top_level_module_name source = import_source module_name exec_recorder pyrefly ignore unbound-name exec_recorder add_local_mod recorded_name value pyrefly ignore unbound-name istype value types ModuleType DummyModule pyrefly ignore unbound-name push PythonModuleVariable value source=source unimplemented_v gb_type= Bad result pyrefly ignore unbound-name context=typestr value explanation= Import result Python module hints= fb internal opcode EAGER_IMPORT_NAME = IMPORT_NAME IMPORT_FROM inst Instruction - None DUP_TOP inst _load_attr inst argval Cache note This cache only exists duration InstructionTranslator - so should safe do cache_method load_builtin_from_argval argval Any - VariableTracker argval f_builtins raise Unsupported f name argval defined val = f_builtins argval callable val builtins_source = GlobalSource output name_of_builtins_dict_key_in_fglobals var_source = DictGetItemSource builtins_source argval VariableTracker build val var_source assert is_builtin_constant val ConstantVariable create value=val load_builtin inst Instruction - None push load_builtin_from_argval inst argval jump inst Instruction - None assert instruction_pointer None assert start_point None assert inst target None get_metrics_context increment ir_count instruction_pointer - start_point instruction_pointer = indexof inst target start_point = instruction_pointer JUMP_FORWARD = jump JUMP_ABSOLUTE = jump POP_JUMP_IF_FALSE = generic_jump operator not_ False POP_JUMP_IF_TRUE = generic_jump operator truth False JUMP_IF_FALSE_OR_POP = generic_jump operator not_ True JUMP_IF_TRUE_OR_POP = generic_jump operator truth True SETUP_LOOP inst Instruction - None only exists python = assert inst target None block_stack append BlockStackEntry inst inst target len stack SETUP_EXCEPT inst Instruction - None only exists python = assert inst target None block_stack append BlockStackEntry inst inst target len stack POP_BLOCK inst Instruction - None block_stack pop SETUP_WITH inst Instruction - None setup_or_before_with inst SETUP_FINALLY inst Instruction - None assert inst target None block_stack append BlockStackEntry inst inst target len stack BEGIN_FINALLY inst Instruction - None push None WITH_CLEANUP_START inst Instruction - None exit exc = popn assert exc None push exc pyrefly ignore bad-argument-type push exit call_function ConstantVariable create None WITH_CLEANUP_FINISH inst Instruction - None popn push None FOR_ITER inst Instruction - None = pop realize try val = next_variable push push val except StopIteration exc ObservedUserStopIteration e isinstance e exc ObservedUserStopIteration exc handle_observed_exception leave iterator upon exhaustion sys version_info = CPython actually jumps instruction after END_FOR performs action END_FOR part FOR_ITER We jump END_FOR run so we need make sure values stack pop push push ConstantVariable create None jump inst _create_exception_type val VariableTracker - VariableTracker isinstance val variables BuiltinVariable UserDefinedExceptionClassVariable Create instance exception type https github com python cpython blob Python ceval c#L -L val = val call_function type ignore arg-type val _raise_exception_variable val VariableTracker - NoReturn User can raise exception ways raise exception type - raise NotImplementedError raise exception instance - raise NotImplementedError foo when user raises exception type val = _create_exception_type val Handle https peps python org pep- CPython + has specific bytecode instruction CALL_INTRINSIC_ is_generator f_code isinstance val variables ExceptionVariable val exc_type StopIteration val = variables BuiltinVariable RuntimeError call_function type ignore arg-type Save exception global data structure exn_vt_stack set_current_exception val type ignore arg-type when user raises exception instance _isinstance_exception val observed_exception_type = exc get_dynamo_observed_exception val exc_type type ignore attr-defined union-attr raise observed_exception_type f raised exception val unimplemented_v gb_type= Failed raise exception context=str exc explanation= Attempted raise non-Exception type value hints= graph_break_hints USER_ERROR RAISE_VARARGS inst Instruction - None inst arg == len exn_vt_stack msg = ConstantVariable No active exception reraise exc raise_observed_exception RuntimeError args= msg re-raise previous exception Here CPython refers exception top exception stack assert len exn_vt_stack val = exn_vt_stack - assert _isinstance_exception val val _raise_exception_variable val inst arg == raise TOS val = stack - type ignore assignment _raise_exception_variable val raise from_vt = pop val = pop type ignore assignment try _raise_exception_variable val finally Update __cause__ __suppress_context__ raised exception curr_exc = exn_vt_stack get_current_exception cause = _create_exception_type from_vt curr_exc call_setattr ConstantVariable __cause__ cause type ignore arg-type union-attr assignment CLEANUP_THROW inst Instruction - None https github com python cpython pull tos = stack - assert isinstance tos ExceptionVariable tos exc_type StopIteration unimplemented_v gb_type= CLEANUP_THROW StopIteration context= explanation= Received StopIteration when handling generator throw close This supported hints= RERAISE inst RERAISE inst Instruction - None https docs python org library dis html#opcode-RERAISE Re-raises exception currently top stack If oparg non-zero pops additional value stack which used set f_lasti current frame sys version_info = RERAISE currently supported narrow case ` raise None ` val = pop inst argval RERAISE _ = pop _raise_exception_variable val RERAISE push val _raise_exception_variable val _exc = pop val = pop _tb = pop _raise_exception_variable val _isinstance_exception val VariableTracker - TypeIs ExceptionVals isinstance val variables ExceptionVariable UserDefinedExceptionClassVariable UserDefinedExceptionObjectVariable WITH_EXCEPT_START inst Instruction - None args list VariableTracker = sys version_info = fn_loc = sys version_info At top stack values - TOP = exc_info - SECOND = previous exception - THIRD lasti exception exc_info - FOURTH context __exit__ bound method We call FOURTH type TOP TOP GetTraceback TOP Then we push __exit__ value In Python + there NULL placed between context __exit__ bound method lasti fn now th TOS assert len stack = fn_loc fn = stack -fn_loc val = stack - assert _isinstance_exception val typ = BuiltinVariable val exc_type type ignore attr-defined union-attr tb = ConstantVariable None sys version_info = isinstance stack - NullVariable args append stack - assert len stack = fn = stack - val = stack - assert _isinstance_exception val typ = BuiltinVariable val exc_type type ignore attr-defined tb = ConstantVariable None args += typ val tb call_function fn args exception_handler raised_exception ObservedException - None observed_exn_gb_explanation = Dynamo found no exception handler top-level compiled function when encountering exception Exception will propagate outside compiled region bubble_exception_to_interpreter - None Bubble exception interpreter curr_exc = exn_vt_stack get_current_exception dynamo_exc = exc get_dynamo_observed_exception curr_exc python_type assert isinstance raised_exception dynamo_exc sanity check unimplemented_v gb_type= Observed exception context=f raised exception curr_exc python_type_name curr_exc args type ignore union-attr explanation=observed_exn_gb_explanation hints= graph_break_hints USER_ERROR graph_break_hints SUPPORTABLE from_exc=raised_exception sys version_info = exn_tab_entry = current_instruction exn_tab_entry exn_tab_entry Implementation based https github com python cpython blob Objects exception_handling_notes txt pop values stack until matches stack depth handler while len stack exn_tab_entry depth pop lasti true then push offset exception raised exn_tab_entry lasti push variables ConstantVariable current_instruction offset push exception stack push exn_vt_stack get_current_exception jump handler jump exn_tab_entry type ignore arg-type No handler found Bubble exception parent instruction translator We use special exception stack clear type InstructionTranslator bubble_exception_to_interpreter raise raised_exception len block_stack base implementation - https github com python cpython blob Python ceval c#L block_stack_entry = block_stack pop while block_stack_entry inst opname == EXCEPT_HANDLER TODO anijain - This tested unable create testcase https github com python cpython blob Python ceval c#L popn exn_vt_stack pop len block_stack == No handler found frame Bubble exception parent instruction translator stack clear type InstructionTranslator unimplemented_v gb_type= Observed exception EXCEPT_HANDLER context=str raised_exception explanation=observed_exn_gb_explanation + This graph break unexpected hints= graph_break_hints DYNAMO_BUG raise raised_exception block_stack_entry = block_stack pop exception_var = exn_vt_stack get_current_exception exn_vt_stack move_current_exception_to_stack pop values stack until matches stack depth handler while len stack block_stack_entry stack_index pop Push dummy block stack entry EXCEPT_HANDLER https github com python cpython blob Python ceval c#L except_handler_inst = Instruction e EXCEPT_HANDLER None block_stack append BlockStackEntry except_handler_inst None len stack Push old exception len exn_vt_stack = old_exception = exn_vt_stack - Push old exception stack - tb value type Traceback currently mapped UnknownVariable push variables UnknownVariable push old_exception push variables BuiltinVariable old_exception exc_type Push empty exception tb value type push variables ConstantVariable None push variables ConstantVariable None push variables ConstantVariable None Push new exception - tb val type Traceback currently mapped UnknownVariable push variables UnknownVariable push exception_var push variables BuiltinVariable exception_var exc_type Jump target jump block_stack_entry No handler found Bubble exception parent instruction translator We use special exception stack clear type InstructionTranslator bubble_exception_to_interpreter raise raised_exception PUSH_EXC_INFO inst Instruction - None https docs python org library dis html#opcode-PUSH_EXC_INFO Pops value stack Pushes current exception top stack Pushes value originally popped back stack The behavior opcode CPython bit different than what described It pops value stack pushes top exception stack interpreter stack moves current exception exception stack As example suppose stack following state + stack = ConstantVariable ConstantVariable + current_exception = TypeError + exception_stack = ValueError After PUSH_EXC_INFO executed + stack = ConstantVariable ValueError ConstantVariable + current_exception = None + exception_stack = ValueError TypeError val = pop len exn_vt_stack == prev_exc VariableTracker = ConstantVariable None prev_exc = exn_vt_stack - push prev_exc push val exn_vt_stack move_current_exception_to_stack POP_EXCEPT inst Instruction - None sys version_info = _ = pop This exception handled therefore we can clear error indicator assert len exn_vt_stack exn_vt_stack pop assert len block_stack block_stack - inst opname = EXCEPT_HANDLER raise AssertionError Bug Dynamo tracing exception handling Top block stack EXCEPT_HANDLER block_stack pop popn This exception handled therefore we can clear error indicator assert len exn_vt_stack exn_vt_stack pop check_if_exc_matches - bool assert len stack = expected_exc_types = pop sys version_info = CHECK_EXC_MATCH which used onwards does pop This description disassembly doc Performs exception matching ` ` except ` ` Tests whether ` ` STACK - ` ` exception matching ` ` STACK - ` ` Pops ` ` STACK - ` ` pushes boolean result test exc_instance = stack - This used prior via opcode JUMP_IF_NOT_EXC_MATCH There no documentation here code pointer does pops https github com python cpython blob Python ceval c#L -L exc_instance = stack pop Users can check exception ways except NotImplementedError -- BuiltinVariable except CustomException -- UserDefinedExceptionClassVariable except NotImplementedError AttributeError - TupleVariable isinstance expected_exc_types BuiltinVariable TupleVariable UserDefinedExceptionClassVariable UserDefinedExceptionObjectVariable unimplemented_v gb_type= Exception bad expected type context=str expected_exc_types explanation=f ` except ` has unsupported type expected_exc_types hints= graph_break_hints USER_ERROR sys version_info = _isinstance_exception exc_instance unimplemented_v gb_type= Caught non-Exception value context=str exc_instance explanation=f Except expects receive object Exception type received exc_instance hints= graph_break_hints USER_ERROR isinstance expected_exc_types TupleVariable expected_types = expected_exc_types items expected_types = expected_exc_types expected_type expected_types isinstance expected_type BuiltinVariable UserDefinedExceptionObjectVariable UserDefinedExceptionClassVariable unimplemented_v gb_type= Exception non-type expectation context=str expected_type explanation=f ` except ` expects non-type expected_type hints= graph_break_hints USER_ERROR _isinstance_exception exc_instance issubclass exc_instance exc_type type ignore union-attr expected_type fn type ignore attr-defined True isinstance exc_instance variables BuiltinVariable issubclass exc_instance fn pyrefly ignore missing-attribute expected_type fn True False CHECK_EXC_MATCH inst Instruction - None push variables ConstantVariable check_if_exc_matches JUMP_IF_NOT_EXC_MATCH inst Instruction - None check_if_exc_matches jump inst COMPARE_OP inst Instruction - None inst argval == exception match CHECK_EXC_MATCH inst push compare_op_handlers inst argval popn GET_ITER inst Instruction - None call_function BuiltinVariable iter pop break_graph_if_unsupported push= CALL_FUNCTION inst Instruction - None args = popn inst argval fn = pop call_function fn args break_graph_if_unsupported push= CALL_FUNCTION_EX inst Instruction - None kwargsvars VariableTracker inst argval == kwargsvars = ConstDictVariable argsvars = pop inst argval == sys version_info = Python + removed argval replaced possibly NULL kwargs kwargsvars = pop isinstance kwargsvars NullVariable kwargsvars = ConstDictVariable argsvars = pop unimplemented_v gb_type= Variadic function call bad flags context=f flags inst argval explanation=f Attempted call variadic function CALL_FUNCTION_EX bad flags inst argval hints= graph_break_hints DYNAMO_BUG sys version_info = swapped null callable null = pop assert isinstance null NullVariable fn = pop sys version_info = sys version_info null = pop assert isinstance null NullVariable isinstance pyrefly ignore unbound-name argsvars BaseListVariable pyrefly ignore unbound-name argsvars has_force_unpack_var_sequence pyrefly ignore unbound-name argsvars = TupleVariable argsvars force_unpack_var_sequence Unpack cases like fn obj where obj map pyrefly ignore unbound-name isinstance kwargsvars UserDefinedObjectVariable kwargsvars = BuiltinVariable call_custom_dict dict kwargsvars type ignore arg-type pyrefly ignore unbound-name isinstance argsvars BaseListVariable isinstance pyrefly ignore unbound-name kwargsvars ConstDictVariable unimplemented_v gb_type= Variadic function call bad args kwargs type pyrefly ignore unbound-name context=f args type typestr argsvars kwargs type typestr kwargsvars explanation= Expected args list kwargs dict hints= graph_break_hints USER_ERROR Map dictionary str - VariableTracker pyrefly ignore unbound-name missing-attribute kwargsvars = kwargsvars keys_as_python_constant pyrefly ignore unbound-name missing-attribute call_function fn argsvars items kwargsvars break_graph_if_unsupported push= CALL_FUNCTION_KW inst Instruction - None argnames = pop args = popn inst argval fn = pop assert isinstance argnames TupleVariable argnames is_python_constant argnames = argnames as_python_constant args kwargs_list = args -len argnames args -len argnames kwargs = dict zip argnames kwargs_list assert len kwargs == len argnames call_function fn args kwargs LOAD_METHOD_SUPER inst Instruction - None CALL_FUNCTION dataclasses replace inst argval= arg = inst argval argval = code_options co_names arg sys version_info _load_attr argval LOAD_METHOD dataclasses replace inst argval=argval LOAD_ATTR_SUPER inst Instruction - None CALL_FUNCTION dataclasses replace inst argval= arg = inst argval argval = code_options co_names arg _load_attr argval LOAD_METHOD inst Instruction - None _load_attr inst argval obj = pop sys version_info = push obj PUSH_NULL inst sys version_info = always follow NULL + fn convention since obj actually method already bound so doesn t need passed arg PUSH_NULL inst push obj push obj push None CALL_METHOD inst Instruction - None args = popn inst argval dummy = pop assert dummy None fn = pop call_function fn args _load_attr attr Any - None obj = pop result = BuiltinVariable getattr call_function type ignore arg-type obj ConstantVariable create attr push result LOAD_ATTR inst Instruction - None sys version_info = pyrefly ignore unsupported-operation inst arg LOAD_METHOD inst _load_attr inst argval STORE_ATTR inst Instruction - None speculation = speculate speculation failed store_attr_graph_break inst val obj = popn isinstance obj NNModuleVariable isinstance val ConstantVariable We don t allow side effects during export non-constant values https github com pytorch torchdynamo issues assert export f Mutating module attribute inst argval during export try BuiltinVariable setattr call_function type ignore arg-type obj ConstantVariable create inst argval val except Unsupported e should_compile_partial_graph raise reason = f Encountered graph break when attempting store object s attribute STORE_ATTR \n\n str e log_graph_break code_options reason=reason user_stack=e real_stack e remove_from_stats e add_to_stats graph_break speculation fail_and_restart_analysis error_on_graph_break store_attr_graph_break inst Instruction - None should_compile_partial_graph unimplemented_v gb_type= Should compile partial graph STORE_ATTR context= explanation= Dynamo has determined when encountering unsupported STORE_ATTR instruction i e ` obj attr = val ` should compile partial graph hints= log debug STORE_ATTR triggered compile all_stack_locals_metadata = output compile_subgraph reason=GraphCompileReason store_attr frame_summary stack_pops= inst_copy = copy copy inst inst_copy exn_tab_entry = None output add_output_instructions inst_copy popn output add_output_instructions create_call_resume_at next_instruction all_stack_locals_metadata DELETE_ATTR inst Instruction - None obj = pop BuiltinVariable delattr call_function type ignore arg-type obj ConstantVariable create inst argval staticmethod codegen_return_with_pops inst Instruction num_stack int - list Instruction Debug CPython expects stack empty after Calling compile_subgraph will push cells frame values TOS This function will pop those values stack before actually returning Expects stack cells frame values current frame stack values Pops cells frame values leaving current frame stack TOS A instruction included insts = NOTE Debug CPython expects stack empty after Expect current stack state cells frame values current frame stack values assert num_stack = num_stack == insts extend create_swap return_inst = create_instruction RETURN_VALUE inst opname == RETURN_VALUE create_instruction RETURN_CONST argval=inst argval insts extend create_instruction POP_TOP create_instruction POP_TOP return_inst insts create_resume idx int resume_inst Instruction meta StackLocalsMetadata resume_codes list types CodeType cg PyCodegen is_leaf bool handle_inactive_ctx bool - tuple types CodeType str Creates resume function frame corresponding ` ` Expects TOS frame N cells frame cells frame N stack + locals frame stack + locals Some additional codegen may happen prepare frame stack + locals values generated resume function - inactive context variables stack locals will replaced their types - frame leaf frame prune dead locals Regardless codegen stack will left same state before Args - idx depth frame corresponds leaf frame frame N N- root frame frame - resume_inst instruction frame should resume - meta metadata frame returned OutputGraph compile_subgraph - resume_codes nested resume code objects generated previous create_resume calls - cg codegen object output - is_leaf True ` ` corresponds leaf frame - handle_inactive_ctx If True handles inactive context variables described above This necessary iff resume function traced Handle inactive context variables The resume function assumes context variables NOT object e g torch set_grad_enabled True will reconstructed torch set_grad_enabled NOTE unsupported instruction modifies inactive context variable may result silent incorrectness handle_inactive_ctx j _ j_orig zip meta stack_ctx_args meta stack_ctx_idxes_orig Replace stack var context ctx = cast ContextWrappingVariable stack j_orig frames idx j = reconstructed_ctx cg append_output create_dup_top ctx reconstruct_type cg cg extend_output create_swap cg create_load_const idx cg create_binary_subscr cg create_load_const j create_instruction STORE_SUBSCR name _ meta locals_ctx_args Replace local context ctx = cast ContextWrappingVariable symbolic_locals name frames idx meta num_stack +meta locals_names name = reconstructed_ctx cg append_output create_dup_top ctx reconstruct_type cg cg extend_output create_swap cg create_load_const idx cg create_binary_subscr cg create_load_const meta num_stack + meta locals_names name create_instruction STORE_SUBSCR If resume instruction jump absolute then resume target instead This handles case where we graph break again nested function before jump-resuming frame is_jump_absolute resume_inst assert resume_inst target resume_inst = resume_inst target resume_name = unique_id f __resume_at_ resume_inst offset More locals may have been pruned current leaf frame after unsupported instruction e g branch There should any pruning other frames since current instruction there should CALL is_leaf reads = livevars_analysis instructions resume_inst all_argnames = tuple k k symbolic_locals keys k reads k cell_and_freevars argnames_null_set = set meta locals_null_keys argnames = tuple k k all_argnames k argnames_null_set argnames_null = tuple k k all_argnames k argnames_null_set codegen filter current frame s locals current stack state frames cg extend_output create_dup_top cg create_load_const idx cg create_binary_subscr create_dup_top arg argnames current stack state frames frames i prev locals frames i cg extend_output create_dup_top cg create_load_const meta num_stack + meta locals_names arg cg create_binary_subscr create_swap current stack state frames frames i frame i live locals frames i cg extend_output create_instruction POP_TOP create_instruction BUILD_LIST arg=len argnames create_swap frames frames i live locals frames i create_binary_slice meta num_stack None True frames i num_stack = frame i live locals current stack state frames argnames = tuple meta locals_names keys argnames_null = tuple meta locals_null_keys sys version_info assert len argnames_null == variables should NULL compile_subgraph did codegen any NULLs so we should count NullVariables stack_len = len stack - len meta stack_null_idxes assert current_instruction offset None new_code types CodeType = ContinueExecutionCache lookup f_code lineno current_instruction offset resume_inst offset tuple b target offset b block_stack stack_len argnames argnames_null tuple b resume_fn b block_stack handle_inactive_ctx tuple meta stack_ctx_args tuple meta locals_ctx_args tuple meta stack_null_idxes tuple resume_codes Add original GraphModule context resume function handle case graph break while tracing GraphModule orig_graphmodule_maybe = code_context get_context f_code get orig_graphmodule lambda None orig_graphmodule_maybe None code_context get_context new_code orig_graphmodule = weakref ref orig_graphmodule_maybe add resume function global scope new_code co_freevars expose code object debugging purposes output install_global_unsafe resume_name new_code package_name = None This safe we pre-generate unique name output install_global_unsafe resume_name types FunctionType new_code f_globals resume_name package_name = resume_name package None package add_resume_function new_code f_globals __name__ package_name counters resumes new_code co_name += new_code resume_name create_call_resume_at inst Instruction all_stack_locals_metadata list StackLocalsMetadata - list Instruction Codegen all resume function s frame stack starting ` ` call them result Assumes unsupported instruction has already been run Expects TOS frame N locals frame N- stack + locals frame stack + locals frame N stack post-unsupported instruction Leaves result calling resume functions stack returns empty stack after Args - inst instruction current deepest frame resume - all_stack_locals_metadata metadata returned OutputGraph compile_subgraph - contains metadata such local names NULL positions stack length etc instruction_pointer = None cg = PyCodegen output root_tx NOTE We do need codegen frames whose resume instruction RETURN_VALUE We could also do something similar RETURN_CONST lot more code necessary since we would need track RETURN_CONST values inject constant right places Filter out tx es resuming RETURN_ txes list InstructionTranslatorBase = idxes list int = resume_insts list Instruction = cur_tx Optional InstructionTranslatorBase = idx = while cur_tx None cur_tx resume_inst = inst resume_inst = cur_tx next_instruction resume_inst opname = RETURN_VALUE txes append cur_tx idxes append idx resume_insts append resume_inst cur_tx = cur_tx parent idx += current_num_stack = len stack - len all_stack_locals_metadata stack_null_idxes Every tx returning - no need call resume function txes Pop everything TOS then TOS Frame N s stack must have length = since s about RETURN_VALUE Frame N actually should have stack length == because debug CPython expects empty stacks after there no guarantee written down anywhere assert current_num_stack = cg extend_output create_swap current_num_stack + _ range current_num_stack + cg append_output create_instruction POP_TOP cg append_output create_instruction RETURN_VALUE cg get_instructions Let frame k deepest frame where resume function RETURN_VALUE - If k == N then frame N stack prepended frame N locals - If k = N then frame N s TOS added frame k s stack Rearrange TOS compatible create_resume codegen_call_resume frame N stack + locals frame stack + locals create stack values should moved txes Frame N non-returning pack all frame N s stack moved frame N s frame values cg append_output create_instruction BUILD_LIST arg=current_num_stack frame N stack yet frame N s frame values stack_insert_idx = all_stack_locals_metadata num_stack = current_num_stack Frame N returning Let frame k deepest non-returning frame Add frame N s TOS frame k s stack pop frame N stack except TOS cg extend_output create_swap current_num_stack _ range current_num_stack - cg append_output create_instruction POP_TOP cg append_output create_instruction BUILD_LIST arg= frame k stack already frame k s frame values stack_insert_idx = all_stack_locals_metadata idxes num_stack all_stack_locals_metadata idxes num_stack += txes push UnknownVariable move predetermined stack value s deepest non-returning frame cg extend_output create_copy frame_values return_const frame_values cg create_load_const idxes cg create_binary_subscr create_binary_slice stack_insert_idx stack_insert_idx True frame_values idxes stack_insert_idx stack_insert_idx = frame N stack return_const TOS frame_values left top stack filter out frame values skipped tx es filter_insts = idx idxes filter_insts extend create_dup_top cg create_load_const idx cg create_binary_subscr create_swap TOS cells frame_values idxes frame_values idxes frame_values filter_insts extend create_instruction POP_TOP create_instruction BUILD_LIST arg=len idxes TOS cells filtered frame_values cg extend_output filter_insts filter out cells skipped tx es using same instructions filter_insts cells TOS instead frame values cg extend_output create_swap copy deepcopy filter_insts create_swap TOS filtered cells filtered frame_values resume_codes list types CodeType = resume_names = i cur_tx enumerate txes resume_code resume_name = cur_tx create_resume i resume_insts i all_stack_locals_metadata idxes i resume_codes cg cur_tx True resume_codes append resume_code resume_names append resume_name codegen_call_resume resume_codes resume_names cg cg append_output create_instruction RETURN_VALUE cg get_instructions staticmethod codegen_call_resume resume_codes list types CodeType resume_names list str cg PyCodegen - None Calls provided resume functions Expects TOS state frame N cells frame cells frame N stack + locals frame N- stack + locals frame stack + locals Pops cells frame values leaving result calling resume functions TOS Args - resume_codes list resume function code objects call - resume_names list corresponding names resume functions - cg PyCodegen object output instructions NOTE We will load cells we load resume functions load resume functions except root s cg extend_output create_copy i name code enumerate zip resume_names resume_codes i == len resume_names - break stack cells frames resume cells code co_freevars cg extend_output create_dup_top cg create_load_const i cg create_binary_subscr cg make_function_with_closure name code cg extend_output cg load_function_name name False cg extend_output create_swap cg extend_output create_instruction POP_TOP create_instruction BUILD_LIST arg=len resume_codes - stack cells frames resume resume N - load root resume function cg extend_output create_swap resume_codes - co_freevars cg extend_output cg create_load_const - cg create_binary_subscr cg make_function_with_closure resume_names - resume_codes - cg extend_output create_rot_n cg extend_output create_instruction POP_TOP cg load_function_name resume_names - False create_rot_n resume resume N resume frames load top level-frame final stack state should first resume function + NULL resume N resume frame N stack + locals frame stack + locals frame stack + locals cg extend_output create_dup_top create_dup_top frames frames frames cg create_load_const - cg create_binary_subscr frames frames frames - create_swap frames frames - frames cg create_load_const - create_instruction DELETE_SUBSCR TOS resume remaining resumes frames popped frame stack + locals cg extend_output create_rot_n create_instruction BUILD_LIST arg= create_swap resumes frames popped frame stack + locals create_instruction LIST_EXTEND arg= TOS resume remaining resumes frames frame stack + locals cg extend_output create_call_function_ex False True should_compile_partial_graph - bool sys version_info = Do compile current instruction s block top block entry = current_instruction exn_tab_entry entry block_stack entry target block_stack - target False all b can_restore b block_stack one_graph error_on_graph_break is_tracing_resume_prologue active_generic_context_managers Do allow nested graph breaks HOPs output current_tracer parent None break_graph_if_unsupported push= STORE_SUBSCR inst Instruction - None val obj key = popn obj call_method __setitem__ key val DELETE_SUBSCR inst Instruction - None obj key = popn obj call_method __delitem__ key BUILD_TUPLE inst Instruction - None items = popn inst argval push TupleVariable items BUILD_SLICE inst Instruction - None items = popn inst argval push SliceVariable items tx=self BUILD_LIST inst Instruction - None items = popn inst argval push ListVariable items mutation_type=ValueMutationNew BUILD_SET inst Instruction - None config inject_BUILD_SET_unimplemented_TESTING_ONLY unimplemented_v gb_type= missing BUILD_SET handler context= explanation= Missing BUILD_SET bytecode handler testing purposes hints= items = popn inst argval new_set = SetVariable items mutation_type=ValueMutationNew push new_set BUILD_LIST_UNPACK inst Instruction cls type = ListVariable - None seqs = popn inst argval items = seq seqs try items extend seq force_unpack_var_sequence except NotImplementedError unimplemented_v gb_type= Failed unpack object BUILD_LIST_UNPACK context=str seq explanation=f seq cannot unpacked into list BUILD_LIST_UNPACK bytecode ` x y ` hints= graph_break_hints USER_ERROR push cls items mutation_type=ValueMutationNew BUILD_TUPLE_UNPACK inst Instruction - None BUILD_LIST_UNPACK inst cls=TupleVariable BUILD_TUPLE_UNPACK_WITH_CALL = BUILD_TUPLE_UNPACK BUILD_MAP inst Instruction - None items = popn inst argval d = dict zip items items push ConstDictVariable d mutation_type=ValueMutationNew BUILD_MAP_UNPACK inst Instruction - None items = popn inst argval ensure everything dict items = BuiltinVariable dict call_function x x items type ignore arg-type result dict Any Any = x items assert isinstance x ConstDictVariable result update x items push ConstDictVariable result mutation_type=ValueMutationNew BUILD_MAP_UNPACK_WITH_CALL = BUILD_MAP_UNPACK BUILD_CONST_KEY_MAP inst Instruction - None keys = pop values = popn inst argval assert isinstance keys TupleVariable assert keys is_python_constant keys = keys force_unpack_var_sequence assert len keys == len values push ConstDictVariable dict zip keys values mutation_type=ValueMutationNew MAP_ADD inst Instruction - None k v = popn assert inst argval assert inst arg None obj = stack -inst arg realize assert isinstance obj ConstDictVariable obj call_method __setitem__ k v type ignore arg-type SET_ADD inst Instruction - None v = pop assert inst argval assert inst arg None obj = stack -inst arg assert isinstance obj SetVariable assert obj is_mutable obj call_method add v SET_UPDATE inst Instruction - None v = pop assert inst argval assert inst arg None obj = stack -inst arg assert isinstance obj SetVariable assert obj is_mutable obj call_method update v LIST_APPEND inst Instruction - None v = pop assert inst argval assert inst arg None obj = stack -inst arg realize assert isinstance obj ListVariable assert obj is_mutable output side_effects mutation obj obj items append v MAKE_FUNCTION inst Instruction - None flags = inst arg sys version_info fn_name = pop code = pop sys version_info = MAKE_FUNCTION behavior actually changed see https github com python cpython pull assert hasattr code value co_qualname type ignore attr-defined fn_name = ConstantVariable create value=code value co_qualname type ignore attr-defined defaults = None closure = None annotations = None kwdefaults = None sys version_info handled SET_FUNCTION_ATTRIBUTE flags None flags x closure = pop flags x annotations = pop flags x kwdefaults = pop flags x defaults = pop push NestedUserFunctionVariable fn_name code f_globals defaults kwdefaults annotations closure UNPACK_SEQUENCE inst Instruction - None seq = pop isinstance seq TensorVariable val = seq unpack_var_sequence idxes=range inst argval type ignore arg-type isinstance seq GetAttrVariable isinstance seq obj TensorVariable x y = shape proxy = getattr seq obj as_proxy seq name val = wrap_fx_proxy proxy i i range inst argval seq has_force_unpack_var_sequence val = seq force_unpack_var_sequence unimplemented_v gb_type= Failed unpack object UNPACK_SEQUENCE context=str seq explanation=f seq cannot unpacked into list UNPACK_SEQUENCE bytecode i e ` b c = d ` hints= graph_break_hints USER_ERROR pyrefly ignore unbound-name len val = inst argval unimplemented_v gb_type= Length mismatch when unpacking object UNPACK_SEQUENCE pyrefly ignore unbound-name context=f expected length inst argval actual len val explanation=f seq unpacked list UNPACK_SEQUENCE bytecode i e ` b c = d ` unexpected length hints= graph_break_hints DYNAMO_BUG pyrefly ignore unbound-name i reversed val push i UNPACK_EX inst Instruction - None assert = inst argval = xFFFF prefix = inst argval xFF low byte suffix = inst argval high byte seq = pop seq has_force_unpack_var_sequence vals = list seq force_unpack_var_sequence assert len vals = prefix + suffix vals_prefix = vals prefix vals_list = vals prefix len vals - suffix vals_suffix = vals len vals - suffix item reversed vals_suffix push item push TupleVariable vals_list item reversed vals_prefix push item unimplemented_v gb_type= Failed unpack object UNPACK_EX context=str seq explanation=f seq cannot unpacked into list UNPACK_EX bytecode hints= graph_break_hints USER_ERROR break_graph_if_unsupported push= graph_break_on_leaf_function inst Instruction - None is_leaf_tracer unimplemented_v gb_type= Forced graph break leaf function context= explanation= Forced graph break nested graph break testing purposes hints= Set torch _dynamo config debug_force_graph_break_on_leaf_return = False NOP inst Instruction - None Dynamo-specific testing behavior inst argval == GRAPH_BREAK_IF_LEAF graph_break_on_leaf_function inst POP_TOP inst Instruction - None pop ROT_TWO inst Instruction - None = pop b = pop push push b ROT_THREE inst Instruction - None = pop b = pop c = pop push push c push b ROT_FOUR inst Instruction - None = pop b = pop c = pop d = pop push push d push c push b DUP_TOP inst Instruction - None = pop push push DUP_TOP_TWO inst Instruction - None = pop b = pop push b push push b push _convert_value value VariableTracker flag int - VariableTracker flag == BuiltinVariable str call_function value type ignore arg-type flag == BuiltinVariable repr call_function value type ignore arg-type flag == BuiltinVariable ascii call_function value type ignore arg-type value _format_value fmt_spec VariableTracker flags int - None value = pop isinstance value SymNodeVariable torch _dynamo variables lazy LazySymNodeFormatString LazyVariableTracker value = LazyVariableTracker create LazySymNodeFormatString value fmt_spec source=value source push value value = _convert_value value flags x fmt_var = ConstantVariable create + fmt_spec as_python_constant + call_function BuiltinVariable str format fmt_var value FORMAT_VALUE inst Instruction - None flags = inst arg assert flags None flags x == x fmt_spec = pop fmt_spec = ConstantVariable create _format_value fmt_spec flags BUILD_STRING inst Instruction - None format_string_parts list str = args list VariableTracker = kwargs dict str VariableTracker = assert inst arg None part popn inst arg isinstance part ConstantVariable format_string_parts append args append part isinstance part variables StringFormatVariable format_string_parts append part format_string args extend part sym_args set kwargs keys set part sym_kwargs keys unimplemented_v gb_type= BUILD_STRING key conflict context=f format_string_parts format_string_parts kwargs kwargs part sym_kwargs part sym_kwargs explanation= Failed build format string due key conflict hints= graph_break_hints USER_ERROR kwargs update part sym_kwargs unimplemented_v gb_type= BUILD_STRING type error context=str part explanation= Format string part type correct - expected constant format string hints= graph_break_hints USER_ERROR push variables StringFormatVariable create join format_string_parts args kwargs IS_OP inst Instruction - None assert inst argval == inst argval == inst argval == new_argval = new_argval = new_inst = create_instruction COMPARE_OP argval=new_argval COMPARE_OP new_inst CONTAINS_OP inst Instruction - None assert inst argval == inst argval == left right = popn op = inst argval try push right call_method __contains__ left except right __contains__ can raise TypeError exc ObservedTypeError Ideally we should only capture TypeError here some VTs don t implement hasattr vt __contains__ entirely Unsupported excp object doesn t support __contains__ Use __iter__ fallback isinstance excp Unsupported excp remove_from_stats push inline_user_function_return VariableTracker build impl_CONTAINS_OP_fallback left right op == UNARY_NOT inst LIST_EXTEND inst Instruction - None v = pop assert inst argval assert inst arg None obj = stack -inst arg assert isinstance obj ListVariable assert obj is_mutable obj call_method extend v LIST_TO_TUPLE inst Instruction - None push BuiltinVariable tuple call_function pop type ignore arg-type STOPITERATION_ERROR inst Instruction - None wrap generator body try except StopIteration which converts StopIteration into RuntimeError https peps python org pep- https github com python cpython pull https github com python cpython commit cc ef ddbca ba c val = stack - assert _isinstance_exception val val exc_type StopIteration type ignore union-attr new_val = variables BuiltinVariable RuntimeError call_function type ignore arg-type ConstantVariable generator raised StopIteration new_val call_setattr ConstantVariable __context__ val type ignore attr-defined new_val call_setattr ConstantVariable __cause__ val type ignore attr-defined stack - = new_val DICT_MERGE inst Instruction - None v = pop assert inst argval assert inst arg None obj = stack -inst arg realize assert isinstance obj ConstDictVariable assert obj is_mutable obj call_method update v DICT_UPDATE = DICT_MERGE GEN_START inst Instruction - None pop GET_LEN inst Instruction - None tos = stack - tos is_python_constant push ConstantVariable create len tos as_python_constant push tos call_method __len__ MATCH_MAPPING inst Instruction - None tos = stack - assert isinstance tos ConstDictVariable isinstance tos items collections abc Mapping push ConstantVariable create True push ConstantVariable create False MATCH_SEQUENCE inst Instruction - None tos = stack - assert tos is_python_constant tos_value = tos as_python_constant isinstance tos_value collections abc Sequence isinstance tos_value str bytes bytearray push ConstantVariable create True push ConstantVariable create False MATCH_KEYS inst Instruction - None tos = stack - assert isinstance tos TupleVariable keys = tos unpack_var_sequence tos = stack - assert isinstance tos ConstDictVariable all k tos k keys type ignore attr-defined push TupleVariable tos getitem_const k k keys type ignore attr-defined arg-type sys version_info push ConstantVariable create True push ConstantVariable create None sys version_info push ConstantVariable create False LOAD_ASSERTION_ERROR inst Instruction - None push load_builtin_from_argval AssertionError LOAD_BUILD_CLASS inst Instruction - None push load_builtin_from_argval __build_class__ UNARY_POSITIVE = stack_op operator pos UNARY_NEGATIVE = stack_op operator neg UNARY_NOT = stack_op operator not_ UNARY_INVERT = stack_op operator invert BINARY_POWER = stack_op operator pow BINARY_MULTIPLY = stack_op operator mul BINARY_MATRIX_MULTIPLY = stack_op operator matmul BINARY_FLOOR_DIVIDE = stack_op operator floordiv BINARY_TRUE_DIVIDE = stack_op operator truediv BINARY_MODULO = stack_op operator mod BINARY_REMAINDER = stack_op operator mod BINARY_ADD = stack_op operator add BINARY_SUBTRACT = stack_op operator sub BINARY_SUBSCR = break_graph_if_unsupported push= stack_op operator getitem BINARY_LSHIFT = stack_op operator lshift BINARY_RSHIFT = stack_op operator rshift BINARY_AND = stack_op operator and_ BINARY_OR = stack_op operator or_ BINARY_XOR = stack_op operator xor INPLACE_POWER = stack_op operator ipow INPLACE_MULTIPLY = stack_op operator imul INPLACE_MATRIX_MULTIPLY = stack_op operator imatmul INPLACE_FLOOR_DIVIDE = stack_op operator ifloordiv INPLACE_TRUE_DIVIDE = stack_op operator itruediv INPLACE_MODULO = stack_op operator imod INPLACE_REMAINDER = stack_op operator imod INPLACE_ADD = stack_op operator iadd INPLACE_SUBTRACT = stack_op operator isub INPLACE_LSHIFT = stack_op operator ilshift INPLACE_RSHIFT = stack_op operator irshift INPLACE_AND = stack_op operator iand INPLACE_XOR = stack_op operator ixor INPLACE_OR = stack_op operator ior opcodes RESUME inst Instruction - None inst arg == append_prefix_inst inst accept_prefix_inst = False assert accept_prefix_inst sys version_info = BINARY_OP inst Instruction - None assert inst arg None _binary_op_lookup inst arg inst PRECALL inst Instruction - None pass KW_NAMES inst Instruction - None kw_names = code_options co_consts inst arg assert isinstance kw_names tuple name kw_names assert isinstance name str assert kw_names None kw_names = ConstantVariable create value=kw_names type ignore assignment PUSH_NULL inst Instruction - None push NullVariable _call inst Instruction call_kw bool = False - None see https docs python org library dis html#opcode-CALL convention call_kw TOS kw_names CALL_KW instruction assert sys version_info = kw_names = pop assert isinstance kw_names TupleVariable kw_names is_python_constant kw_names = kw_names as_python_constant kw_names = kw_names value kw_names assert inst arg None contents = popn inst arg + sys version_info = NULL callable swapped fn = contents args = isinstance contents NullVariable contents isinstance contents NullVariable fn = contents args = fn = contents args = contents kw_names pyrefly ignore bad-argument-type args = args + contents -len kw_names pyrefly ignore bad-argument-type kwargs_list = contents -len kw_names pyrefly ignore no-matching-overload kwargs = dict zip kw_names kwargs_list pyrefly ignore bad-argument-type assert len kwargs == len kw_names args = args + contents kwargs = try call_function fails need set kw_names None otherwise subsequent call may have kw_names set old value call_function fn args kwargs finally kw_names = None break_graph_if_unsupported push= CALL inst Instruction - None _call inst COPY inst Instruction - None assert inst arg None push stack -inst arg SWAP inst Instruction - None assert inst arg None stack - stack -inst arg = stack -inst arg stack - JUMP_BACKWARD = jump JUMP_BACKWARD_NO_INTERRUPT = jump POP_JUMP_FORWARD_IF_TRUE = generic_jump operator truth False POP_JUMP_BACKWARD_IF_TRUE = generic_jump operator truth False POP_JUMP_FORWARD_IF_FALSE = generic_jump operator not_ False POP_JUMP_BACKWARD_IF_FALSE = generic_jump operator not_ False CACHE inst Instruction - None pass BEFORE_WITH inst Instruction - None setup_or_before_with inst enter_ctx ctx Union ContextWrappingVariable GenericContextWrappingVariable inst Instruction - VariableTracker isinstance ctx GenericContextWrappingVariable ctx supports_graph_breaks active_generic_context_managers append ctx sys version_info = See update_block_stack create_resume block stack details Only push block current instruction s block block nested try block - current instruction s block target same top block s target inst exn_tab_entry block_stack inst exn_tab_entry target block_stack - target target = None assert next_instruction exn_tab_entry None target = next_instruction exn_tab_entry target target = inst target target isinstance InstructionTranslator config nested_graph_breaks block_stack append BlockStackEntry inst target len stack ctx block_stack append BlockStackEntry inst target len stack ctx enter staticmethod unsupported_ctx_graph_break ctx VariableTracker - NoReturn unimplemented_v gb_type= Unsupported context manager context=f Attempted SETUP_WITH BEFORE_WITH LOAD_SPECIAL ctx explanation=f Dynamo does know how enter ` ctx python_type_name ` context manager hints= Avoid using unsupported context manager If context manager seems like should supported e g torch set_grad_enabled then may case created outside compiled region which Dynamo does support Supported context managers can cross graph break boundaries only they local non-closure variables intermediate values File issue PyTorch Simple context managers can potentially supported note context managers can t supported general setup_or_before_with inst Instruction - None ctx = pop isinstance ctx ContextWrappingVariable GenericContextWrappingVariable unsupported_ctx_graph_break ctx Need redundant check mypy assert isinstance ctx ContextWrappingVariable GenericContextWrappingVariable push WithExitFunctionVariable ctx inst target push enter_ctx ctx inst append_prefix_inst inst Instruction - None assert accept_prefix_inst prefix_insts append inst MAKE_CELL inst Instruction - None sys version_info = accept_prefix_inst In + MAKE_CELL longer necessarily prefix instruction It can generated inlined comprehensions assert isinstance symbolic_locals inst argval NullVariable symbolic_locals inst argval = output side_effects track_cell_new append_prefix_inst inst COPY_FREE_VARS inst Instruction - None append_prefix_inst inst RETURN_GENERATOR inst Instruction - None append_prefix_inst inst opcodes BINARY STORE_SLICE opcodes broken down into BUILD_SLICE BINARY STORE_SUBSCR END_FOR inst Instruction - None sys version_info = pop popn LOAD_FAST_CHECK inst Instruction - None istype symbolic_locals get inst argval None NullVariable unimplemented_v gb_type= LOAD_FAST_CHECK uninitialized variable context=inst argval explanation=f Attempted load uninitialized local variable inst argval hints= graph_break_hints USER_ERROR LOAD_FAST inst LOAD_FAST_AND_CLEAR inst Instruction - None inst argval symbolic_locals push NullVariable LOAD_FAST inst symbolic_locals inst argval = NullVariable LOAD_SUPER_ATTR inst Instruction - None CALL_FUNCTION dataclasses replace inst argval= assert inst arg None inst arg LOAD_METHOD inst _load_attr inst argval CALL_INTRINSIC_ inst Instruction - None inst argval == INTRINSIC_STOPITERATION_ERROR STOPITERATION_ERROR inst inst argval == INTRINSIC_UNARY_POSITIVE UNARY_POSITIVE inst inst argval == INTRINSIC_LIST_TO_TUPLE push TupleVariable pop force_unpack_var_sequence unimplemented_v gb_type= Missing CALL_INTRINSIC_ handler context=f CALL_INTRINSIC_ operand inst argval explanation=f No handler implemented CALL_INTRINSIC_ inst argval instruction hints= graph_break_hints SUPPORTABLE END_SEND inst Instruction - None tos = pop pop push tos opcodes fused instructions LOAD_FAST_LOAD_FAST STORE_FAST_STORE_FAST STORE_FAST_LOAD_FAST broken down break_graph_if_unsupported push= CALL_KW inst Instruction - None _call inst call_kw=True TO_BOOL inst Instruction - None TO_BOOL only precedes conditional jump UNARY_NOT see compile c CPython So we can skip instruction long we remember codegen TO_BOOL before conditional jumps UNARY_NOT assert next_instruction opname POP_JUMP_IF_TRUE POP_JUMP_IF_FALSE UNARY_NOT SET_FUNCTION_ATTRIBUTE inst Instruction - None flags = inst arg assert flags None fn = pop assert isinstance fn NestedUserFunctionVariable attr = pop flags x fn closure = attr flags x fn annotations = attr flags x fn kwdefaults = attr flags x fn defaults = attr push fn CONVERT_VALUE inst Instruction - None push _convert_value pop inst argval FORMAT_SIMPLE inst Instruction - None _format_value ConstantVariable create FORMAT_WITH_SPEC inst Instruction - None _format_value pop opcodes LOAD_FAST_BORROW = LOAD_FAST NOT_TAKEN = NOP POP_ITER = POP_TOP See https github com python cpython blob e d d e d fdf Python ceval c#L LOAD_SPECIAL table - make sure matches Python + _load_special_names = __enter__ __exit__ __aenter__ __aexit__ LOAD_SPECIAL inst Instruction - None assert isinstance inst arg int expected LOAD_SPECIAL arg set int attr = _load_special_names inst arg attr __enter__ __exit__ ctx = pop isinstance ctx ContextWrappingVariable GenericContextWrappingVariable unsupported_ctx_graph_break ctx Need redundant check mypy assert isinstance ctx ContextWrappingVariable GenericContextWrappingVariable attr == __enter__ push WithEnterFunctionVariable ctx PUSH_NULL inst WithExitFunctionVariable doesn t really do anything target + push WithExitFunctionVariable ctx None PUSH_NULL inst Implementation similar LOAD_METHOD + _load_attr attr obj = pop push obj PUSH_NULL inst LOAD_SMALL_INT inst Instruction - None push ConstantVariable create inst argval See https github com python cpython blob ac fc c fd fb cb dc edc cf Python pylifecycle c#L common constants - make sure matches Python + The common constants all attributes ` builtins ` _common_constants = AssertionError NotImplementedError tuple all any LOAD_COMMON_CONSTANT inst Instruction - None assert isinstance inst arg int expected LOAD_COMMON_CONSTANT arg set int push load_builtin_from_argval _common_constants inst arg is_non_empty_graph - bool output count_calls perf optimization only is_non_empty_graph = lambda True type ignore method-assign True False format_frame_summary additional_stack_frames Optional list Any = None - str additional_stack_frames None additional_stack_frames = join traceback format_list frame_summary + list reversed additional_stack_frames frame_summary - traceback FrameSummary traceback FrameSummary getattr f_code co_filename unknown lineno getattr f_code co_name unknown lookup_line=False is_co_filename_from_nn_modules - bool filename = getattr f_code co_filename unknown nn_modules_pattern = re compile r torch nn modules nn_modules_pattern match filename None store_global_weakref_by_id prefix str value Any - str global_name = output install_global_by_id prefix weakref ref value install_guard GlobalWeakRefSource global_name make_guard GuardBuilder WEAKREF_ALIVE global_name property fake_mode - Optional FakeTensorMode output tracing_context fake_mode contextlib contextmanager strict_translation_mode check_fn Callable VariableTracker bool - Any Strict mode enabled per-VariableTracker level depending value check_fn node prior = strict_checks_fn strict_checks_fn = check_fn try yield finally strict_checks_fn = prior speculate - SpeculationEntry assert instruction_pointer None assert instruction_pointer speculation_log next f_code co_filename lineno instruction_pointer - instructions instruction_pointer - log_graph_break code_options dict str Any reason str = user_stack Optional StackSummary = None - None user_stack None user_stack = torch _guards TracingContext extract_stack try frame_loc = user_stack - filename user_stack - lineno except IndexError first instruction frame_loc = code_options co_filename code_options co_firstlineno stack_above_dynamo_formatted = config verbose stack_above_dynamo = get_stack_above_dynamo stack_above_dynamo_formatted = join traceback format_list stack_above_dynamo user_stack = get_stack_above_dynamo + user_stack type ignore assignment pyrefly ignore bad-argument-type user_stack = collapse_resume_frames user_stack user_stack_formatted = join traceback format_list user_stack user_stack_trace = f Graph break user code frame_loc frame_loc \n f Graph Break Reason reason \n User code traceback \n config verbose user_stack_trace += f stack_above_dynamo_formatted \n ========== most recent ` torch compile ` tracing attempt started here ==========\n\n f user_stack_formatted \n NOTE most recent ` torch compile ` tracing attempt might where you applied ` torch compile ` This due how graph breaks implemented - optimized code object returned Dynamo will call another Dynamo-generated resume function tracing re-enabled calling resume function normal Python function which Dynamo intercepts top-level frame \n user_stack_trace += str user_stack_formatted torch _logging trace_structured artifact metadata_fn=lambda name dynamo_graph_break_reason encoding string payload_fn=lambda f user_stack_trace \n traceback format_exc torch _dynamo explain formats little nicer presents slightly more actionable user code pointer graph_break_log isEnabledFor logging DEBUG explain graph_break_dup_warning_checker add frame_loc This log line MUST contain string Graph break user code This log line exercised python test dynamo test_exc py -k test_graph_break_log config verbose user_stack_trace += \nMost recent bytecode instructions traced max \n user_stack_trace += \n join latest_bytecode_queue + \n graph_break_log debug user_stack_trace This log line MUST contain string Graph break user code exercised python test dynamo test_misc py -k test_duplicate_graph_break_log graph_break_log debug Graph break user stack suppressed due duplicate graph break user code s s\nGraph Break Reason s frame_loc frame_loc reason __init__ output OutputGraph instructions list Instruction f_locals dict str Any f_globals dict str Any f_builtins dict str Any code_options dict str Any symbolic_locals dict str VariableTracker symbolic_globals dict str VariableTracker symbolic_torch_function_state SymbolicTorchFunctionState symbolic_stream_state SymbolicStreamState f_code types CodeType export bool inline_depth int speculation_log SpeculationLog exn_vt_stack ExceptionStack distributed_state Optional DistributedState This determines whether use execution recorder closure Optional tuple types CellType = None package Optional CompilePackage = None - None super __init__ speculation_log = speculation_log distributed_state = distributed_state Mutable state checkpointed copy_graphstate output = output symbolic_locals = symbolic_locals symbolic_globals = symbolic_globals symbolic_torch_function_state = symbolic_torch_function_state symbolic_stream_state = symbolic_stream_state used keep cell freevars alive after pruning symbolic_locals prune_dead_locals order generate any nested closures post_prune_cell_and_freevars = None stack list VariableTracker = instruction_pointer = start_point = None current_instruction = create_instruction NOP block_stack = states before SETUP_WITH checkpointing fallback active_generic_context_managers list GenericContextWrappingVariable = lineno = - kw_names = None accept_prefix_inst = True prefix_insts = exn_vt_stack = exn_vt_stack latest_bytecode_queue = deque maxlen= Properties input output code instructions list Instruction = instructions indexof dict Instruction int = get_indexof instructions f_locals dict str Any = f_locals needed recording accessed locals replay f_globals dict str Any = f_globals f_builtins dict str Any = f_builtins code_options dict str Any = code_options f_code types CodeType = f_code closure = closure Execution record replaying errors closure None config replay_record_enabled exec_recorder = ExecutionRecorder code=f_code closure=closure code_options=code_options exec_recorder = None Stack module being parsed current nn module end ordered dict The first field tuple fully qualified name current module original hierarchy The second field type current nn module nn_module_stack dict str tuple str type Any = num_calls dict str int = Flag indicate whether tracing used export export = export NOTE one_graph used export fullgraph=True always force errors graph breaks To toggle erroring resuming graph breaks during fullgraph=False compile error_on_graph_break used instead Every step its value updated global tls error_on_graph_break We mirror value since cleanup may correctly inadvertently change tls error_on_graph_break This assumes we cannot both trace change tls error_on_graph_break graph break same instruction one_graph = False error_on_graph_break = False Also do graph break when tracing resume function prologues is_tracing_resume_prologue = False current_speculation = None strict_checks_fn = None is_leaf_tracer = True parent = None debug_locals = package = package resume_execution CO_ASYNC_GENERATOR CO_COROUTINE CO_GENERATOR CO_ITERABLE_COROUTINE f_code co_flags CO_GENERATOR &#124; CO_COROUTINE &#124; CO_ITERABLE_COROUTINE &#124; CO_ASYNC_GENERATOR push BuiltinVariable None inline_depth = inline_depth inconsistent_side_effects = False _constants_cache list Optional Union ConstantVariable SliceVariable = None len f_code co_consts is_trace_bytecode_log_enabled Optional bool = trace_bytecode_log isEnabledFor logging DEBUG is_trace_source_log_enabled Optional bool = trace_source_log isEnabledFor logging DEBUG linecache lazycache f_code co_filename f_globals InstructionTranslator InstructionTranslatorBase staticmethod current_tx - InstructionTranslator tls current_tx contextlib contextmanager set_current_tx - Any prior = getattr tls current_tx None tls current_tx = try yield finally tls current_tx = prior __init__ instructions list Instruction f_code types CodeType f_locals dict str Any f_globals dict str Any f_builtins dict str Any closure Optional tuple Any torch_function_mode_stack Any code_options dict str Any compiler_fn Any one_graph bool export bool export_constraints Any frame_state Any speculation_log SpeculationLog exn_vt_stack ExceptionStack distributed_state Optional DistributedState package Optional CompilePackage - None _step_logger logging INFO f torchdynamo start tracing f_code co_name code_options co_filename code_options co_firstlineno super __init__ output=OutputGraph code_options compiler_fn export export_constraints frame_state local_scope=f_locals global_scope=f_globals f_code=f_code torch_function_mode_stack=torch_function_mode_stack one_graph=one_graph package=package instructions=instructions f_locals=f_locals f_globals=f_globals f_builtins=f_builtins closure=closure code_options=code_options symbolic_locals= set below A global var inserted only after STORE_GLOBAL happens symbolic_globals= symbolic_torch_function_state=None type ignore arg-type set below symbolic_stream_state=None type ignore arg-type set below f_code=f_code export=export inline_depth= speculation_log=speculation_log exn_vt_stack=exn_vt_stack distributed_state=distributed_state package=package _throw_if_in_functorch soon we create tracing context we should keep active so any calls into dynamo apis can rely finding tracing output tracing_context set_current_tx one_graph bool = one_graph export = export export assert one_graph Export without one graph - something has gone wrong symbolic_locals = Populate ` symbolic_locals ` non-cell variables cell_and_freevars set str = set cell_and_freevars dynamism = code_context get_context f_code get dynamism None name value f_locals items name cell_and_freevars local_dynamism = None dynamism local_dynamism = frozenset dynamism get name items var = LazyVariableTracker create value LocalSource name is_input=True dynamism=local_dynamism symbolic_locals name = var Populate ` symbolic_locals ` cells created frame effectively implementing ` MAKE_CELL ` instructions side_effects = output side_effects name cellvars name f_locals This models cells also function inputs value = f_locals name NOTE root frame inputs captured nested function become special cell objects -- they exist ` f_locals ` contents cells rather than cells objects themselves In Dynamo we choose represent such input cell objects newly created rather than pre-existing cell objects because The reason representing pre-existing cell object emit guard codegen mutations However local cells should never used guards Moreover point these input cell objects should ve never been accessed anyone since Dynamo intercepts frame right after its evaluation starts i e right after these cell objects created So they should have no external reference meaning no mutation needs propagated This conveniently allows codegen prune away mutations these cells unless they escape frame contents_source = LocalSource name is_input=True is_derefed_cell_contents=True contents_var VariableTracker = LazyVariableTracker create value contents_source cell_var = side_effects track_cell_new side_effects store_cell cell_var contents_var cell_var = side_effects track_cell_new cell_var local_name = name type ignore attr-defined symbolic_locals name = cell_var Populate ` symbolic_locals ` cells captured frame effectively implementing ` COPY_FREE_VARS ` instruction assert closure None name cell zip freevars closure cell_source = LocalCellSource name contents_source = LocalSource name is_derefed_cell_contents=True try contents_var = LazyVariableTracker create cell cell_contents contents_source except ValueError Cell has yet been assigned contents_var = variables DeletedVariable cell_var = side_effects track_cell_existing cell_source cell contents_var cell_var local_name = name type ignore attr-defined symbolic_locals name = cell_var symbolic_torch_function_state = SymbolicTorchFunctionState torch_function_mode_stack symbolic_stream_state = SymbolicStreamState export export gets confused we never realize unused inputs export mode just eagerly realize everything symbolic_locals = variables LazyVariableTracker realize_all symbolic_locals _throw_if_in_functorch - None Fallback eager case graph break inside vmap eager = torch _dynamo lookup_backend eager compiler_fn = inspect getattr_static output compiler_fn compiler_fn output compiler_fn ci = torch _C _functorch peek_interpreter_stack forbidden_keys = torch _C _functorch TransformType Vmap torch _C _functorch TransformType Grad torch _C _functorch TransformType Jvp ci None ci key forbidden_keys compiler_fn eager name = ci key name lower msg = If you reaching here means dynamo failed one following reasons \n Calling torch compiled function f - Calling torch func name compiled_fn function eager mode supported f Ensure torch func name also wrapped within torch compile function For more information see PyTorch issue \n reaches here means Dynamo failed inline functorch function f - torch func name fn requires function inlined dynamo unimplemented_v gb_type= Unsupported functorch tracing attempt context= explanation=msg hints= get_example_value source Source - Any isinstance source LocalSource f_locals source local_name isinstance source GlobalSource f_globals source global_name raise KeyError symbolic_locals_contain_module_class - bool v symbolic_locals values isinstance v UserDefinedClassVariable issubclass v as_python_constant torch nn Module True False replace_tos_if_return_is_generator - None len stack tos = stack - isinstance tos LocalGeneratorObjectVariable stack - = ListIteratorVariable tos force_unpack_var_sequence mutation_type=ValueMutationNew _return inst Instruction - None replace_tos_if_return_is_generator assert instruction_pointer None assert start_point None get_metrics_context increment ir_count instruction_pointer - start_point config allow_empty_graphs output count_calls == inconsistent_side_effects symbolic_locals_contain_module_class export one_graph error_on_graph_break is_tracing_resume_prologue raise exc SkipFrame because no content function call instruction_pointer = None _step_logger logging INFO f torchdynamo done tracing f_code co_name inst opname log debug triggered compile all_stack_locals_metadata = output compile_subgraph reason=GraphCompileReason return_value frame_summary graph_break=False value returned stack_pops= inst opname == RETURN_VALUE check our stack locals meta correct we should only tracing frame there should any NULLs stack assert len all_stack_locals_metadata == assert all_stack_locals_metadata stack_null_idxes output add_output_instructions codegen_return_with_pops inst all_stack_locals_metadata num_stack raise ReturnValueOp RETURN_VALUE inst Instruction - None _return inst RETURN_CONST inst Instruction - None _return inst sys version_info = _binary_op_lookup = getattr InstructionTranslator opname INPLACE opname f BINARY_ opname opname _ dis _nb_ops type ignore attr-defined InliningInstructionTranslator InstructionTranslatorBase Trace inline called method symbolic_result Optional VariableTracker pyrefly ignore bad-override parent InstructionTranslatorBase classmethod inline_call cls parent Any func Any args Any kwargs Any - Any tracer = cls build_inline_tracer parent func args kwargs tracer inline_call_ staticmethod check_inlineable func Any - trace_rules SkipResult func has_self unimplemented_v gb_type= Inline attempt __self__ context=str func explanation= Attempted inline function ` __self__ ` attribute Dynamo expected decompose method calls into function calls ` ` argument hints= isinstance func UserFunctionVariable inspect getattr_static func get_function _torchdynamo_disable False msg = inspect getattr_static func get_function _torchdynamo_disable_msg None unimplemented_v gb_type= Skip inlining ` torch compiler disable ` d function context=str func get_function explanation=f Skip inlining function func get_function since wrapped f ` torch compiler disable ` reason msg hints= Remove ` torch compiler disable ` call result = trace_rules check_verbose func is_inlined_call=True result skipped torch _dynamo variables misc produce_trampoline_autograd_apply _origin marks coming internal dynamo known function safe trace through hasattr getattr func fn None _origin pyrefly ignore missing-attribute func fn _origin produce_trampoline_autograd_apply Known sound trace_rules SkipResult False allowlist dynamo known function fn_qualname = func fn __qualname__ hasattr func fn hints = f Avoid calling function ` fn_qualname ` _dynamo func get_filename hints += f Apply ` torch _dynamo dont_skip_tracing ` function ` fn_qualname ` force tracing into function More graph breaks may occur result attempting trace into function Please file issue PyTorch unimplemented_v gb_type= Attempted inline function marked skipped context=f qualname fn_qualname name func get_name f filename ` func get_filename ` skip reason result reason explanation=f Dynamo developers have intentionally marked function ` fn_qualname ` should traced hints=hints result staticmethod build_inline_tracer parent Any func VariableTracker args list VariableTracker kwargs Any - InliningInstructionTranslator assert isinstance func UserFunctionVariable NestedUserFunctionVariable LocalGeneratorFunctionVariable LocalGeneratorObjectVariable code types CodeType = func get_code result = None tracing_ctx = parent output tracing_context Check we have already identified function inline-able The exception dont_skip_tracing flag which affects inline behavior If flag True don t rely previous results config dont_skip_tracing tracing_ctx previous_result = tracing_ctx previously_inlined_functions get code None result = previous_result result None isinstance func SkipFunctionVariable unimplemented_v gb_type= Attempted inline function marked skipped SkipFunctionVariable context=f Attempted inline SkipFunctionVariable func explanation= Attempted inline function previously determined marked intentionally skipped hints= result = InliningInstructionTranslator check_inlineable func assert result skipped False config dont_skip_tracing tracing_ctx tracing_ctx previously_inlined_functions code = result try pyrefly ignore missing-attribute sub_locals = func bind_args parent args kwargs except TypeError e Wrap general TypeError during bind_args internal ArgsMismatchError detailed info raise ArgsMismatchError noqa B reason \n func = func args = args kwargs = kwargs format reason=str e pyrefly ignore missing-attribute func=f func get_name func get_filename func get_code co_firstlineno args= arg python_type arg args kwargs=kwargs v itertools chain sub_locals values isinstance v VariableTracker unimplemented_v gb_type= Encountered unconverted argument when attempting inline context=f func func arg v explanation= An argument inlined function successfully converted VariableTracker hints= graph_break_hints DYNAMO_BUG code co_name __setitem__ __setattr__ args isinstance args variables UserDefinedObjectVariable unimplemented_v gb_type= Unsupported __setitem__ __setattr__ inline attempt context=f code name code co_name args args explanation=f Attempted inline code co_name where first argument user-defined object hints= suffix = TODO mlazos add support enabling multiple artifact logs single alias torch _logging _internal log_state is_artifact_enabled bytecode suffix = f \n dis Bytecode code dis sys version_info = cur_inst = parent current_instruction parent_code = parent f_code get_trace_call_log_str - str header = parent get_line_of_code_header lineno=cur_inst positions lineno line = get_instruction_source_ parent_code cur_inst rstrip f TRACE inlined call code co_name header \n line trace_call_log debug s LazyString get_trace_call_log_str log debug INLINING s s s code suffix result reason Detect inline GraphModule calls order propagate node metadata checking first argument variable tracking GraphModule args isinstance args NNModuleVariable module = parent output get_submodule args module_key isinstance module torch fx GraphModule The inline call might actually call ` forward ` enough add context ` forward ` case called code_context get_context module forward __code__ orig_graphmodule = weakref ref module When we have inline_nn_module turned modules resolve UnspecializedNNModuleVariable args isinstance args UnspecializedNNModuleVariable module = args value isinstance module torch fx GraphModule The inline call might actually call ` forward ` enough add context ` forward ` case called code_context get_context module forward __code__ orig_graphmodule = weakref ref module tracer InliningInstructionTranslator is_generator code tracer = InliningGeneratorInstructionTranslator parent code sub_locals parent symbolic_globals parent symbolic_torch_function_state parent symbolic_stream_state func need line below make MyPy happy assert isinstance func LocalGeneratorObjectVariable tracer = InliningInstructionTranslator parent code sub_locals parent symbolic_globals parent symbolic_torch_function_state parent symbolic_stream_state pyrefly ignore bad-argument-type func tracer inline_call_ - VariableTracker parent = parent code = f_code strict_ctx Any = contextlib nullcontext parent strict_checks_fn strict_ctx = strict_translation_mode parent strict_checks_fn try strict_ctx run except exc ObservedException e msg = f Observed exception DURING INLING code e log debug msg bubble up exception parent frame raise except exc SkipFrame e msg = f SKIPPED INLINING code e log debug msg raise Unsupported msg e except Exception log debug FAILED INLINING s code raise finally parent error_on_graph_break = error_on_graph_break output should_exit graph break ConstantVariable create None dummy variable assert symbolic_result None f_globals parent f_globals Merge symbolic_globals back parent child same namespace parent symbolic_globals update symbolic_globals parent inconsistent_side_effects &#124; = inconsistent_side_effects log debug DONE INLINING s code output tracing_context traced_code append code config enable_faithful_generator_behavior isinstance InliningGeneratorInstructionTranslator is_generator_from_ctx_manager is_generator code isinstance InliningGeneratorInstructionTranslator generator_exhausted assert isinstance InliningGeneratorInstructionTranslator When generator returns None we raise StopIteration args = isinstance symbolic_result ConstantVariable symbolic_result value None args = symbolic_result exc raise_observed_exception StopIteration args=args symbolic_result is_generator code assert isinstance InliningGeneratorInstructionTranslator assert symbolic_result as_python_constant None ListIteratorVariable generated_items mutation_type=ValueMutationNew symbolic_result __init__ parent InstructionTranslatorBase code types CodeType symbolic_locals dict str VariableTracker symbolic_globals dict str VariableTracker symbolic_torch_function_state SymbolicTorchFunctionState symbolic_stream_state SymbolicStreamState funcvar BaseUserFunctionVariable - None f_globals = funcvar get_globals type ignore attr-defined f_builtins = f_globals __builtins__ isinstance f_builtins dict f_builtins = f_builtins __dict__ Get cached instructions These instructions safe cache because we dont mutate them transform_code_object those instructions top most Instruction translator Also we have careful about using _cached_cleaned_instructions here because function global while we want cache alive only during compmilation tracing_ctx = parent output tracing_context instructions = None tracing_ctx tracing_ctx previously_cleaned_instructions get code instructions = tracing_ctx previously_cleaned_instructions code instructions None instructions = cleaned_instructions code propagate_line_nums instructions tracing_ctx tracing_ctx previously_cleaned_instructions code = instructions super __init__ output=parent output f_locals= f_globals=f_globals f_builtins=f_builtins symbolic_locals=symbolic_locals symbolic_globals=symbolic_globals symbolic_torch_function_state=symbolic_torch_function_state symbolic_stream_state=symbolic_stream_state instructions=instructions code_options= k getattr code k k get_code_keys f_code=code export=parent export inline_depth=parent inline_depth + speculation_log=parent speculation_log exn_vt_stack=parent exn_vt_stack distributed_state=parent distributed_state package=parent package funcvar = funcvar parent = parent num_calls = parent num_calls symbolic_result = None nn_module_stack = parent nn_module_stack copy one_graph = parent one_graph property fake_mode - Optional FakeTensorMode parent fake_mode run_ctx_mgr - Any TracingContext current_frame parent frame_summary should_compile_partial_graph - bool config nested_graph_breaks parent should_compile_partial_graph False super should_compile_partial_graph False inlining functions all-or-nothing create_call_resume_at inst Instruction all_stack_locals_metadata list StackLocalsMetadata - list Instruction config nested_graph_breaks super create_call_resume_at inst all_stack_locals_metadata unimplemented_v gb_type= Graph break inlined function context= explanation= Graph breaks inlined call supported hints= RETURN_VALUE inst Instruction - None symbolic_result = pop type ignore assignment instruction_pointer = None raise ReturnValueOp RETURN_CONST inst Instruction - None symbolic_result = _load_const inst instruction_pointer = None raise ReturnValueOp get_globals_source_and_value name str - tuple Any VariableTracker Source NamedTuple s ` __new__ ` has fake global scope s actual module TODO generalize check other non-importable cases https github com python cpython blob b b cb cdce ab d Lib collections __init__ py#L -L __name__ f_globals f_globals __name__ startswith namedtuple_ module_name = f_globals __name__ module_source = import_source module_name torch_package module_name fglobals_value = torch package package_importer _package_imported_modules module_name type ignore assignment fglobals_value = _import_module module_name Dont use lazy vt because we will do setattr afterwards fglobals_vt = VariableBuilder module_source fglobals_value global_source = AttrSource module_source name globals_name = output install_global_by_id ___unnamed_scope f_globals globals_source = GlobalSource globals_name fglobals_value = f_globals type ignore assignment Dont use lazy vt because we will do setattr afterwards fglobals_vt = VariableBuilder globals_source fglobals_value global_source = DictGetItemSource globals_source name type ignore assignment is_stdlib fglobals_value Users don t inplace mutate stdlib attribute like inspect collections skip guards originate stdlib modules global_source = SkipGuardSource global_source type ignore assignment fglobals_value fglobals_vt global_source _load_global inst Instruction - None name = inst argval name f_globals load_builtin inst output global_scope f_globals If global scope matches root frame use handler root frame instruction translator enforce consistency super _load_global inst _ fglobals_vt global_source = get_globals_source_and_value name output side_effects has_pending_mutation_of_attr fglobals_vt name push output side_effects load_attr fglobals_vt name value = f_globals name push VariableTracker build value global_source STORE_GLOBAL inst Instruction - None output global_scope f_globals If global scope matches root frame use handler root frame instruction translator enforce consistency super STORE_GLOBAL inst value = pop isinstance value RemovableHandleVariable unimplemented_v gb_type= Storing Tensor hook handle globals inline call context=inst argval explanation= This supported hints= name = inst argval _fglobals_value fglobals_vt _ = get_globals_source_and_value name output side_effects store_attr fglobals_vt name value InliningGeneratorInstructionTranslator InliningInstructionTranslator generated_items list VariableTracker Flag whether InlineGenerator should consume entire iterator __init__ args Any kwargs Any - None super __init__ args kwargs generated_items = generator_exhausted = False is_generator_from_ctx_manager = False should_compile_partial_graph - bool resuming graph break inlined generator supported False YIELD_VALUE inst Instruction - None top = pop generated_items append top len generated_items MAX_ITERATOR_LIMIT raise exc InfiniteGeneratorError Too many yield values generator Maybe you inlining infinite generator f If please report bug PT _ISSUE_TRACKER_URL push ConstantVariable create None config enable_faithful_generator_behavior is_generator_from_ctx_manager symbolic_result = top Stop tracing raise YieldValueOp GET_YIELD_FROM_ITER inst Instruction - None tos = stack - isinstance tos ListIteratorVariable pop res = BuiltinVariable iter call_function tos type ignore arg-type push res RETURN_VALUE inst Instruction - None generator_exhausted = True super RETURN_VALUE inst RETURN_CONST inst Instruction - None generator_exhausted = True super RETURN_CONST inst YIELD_FROM inst Instruction - None assert len stack = val = pop tos = stack - isinstance val ConstantVariable val value None invoke send Unreachable code - you hit you implementing generator support have lifted ` unimplemented generator ` frame conversion This codepath handles subgenerator lines up line Python https github com python cpython blob Python ceval c#L unimplemented_v gb_type= Unreachable sub-generator code context= explanation= Should only encountered while implementing generator support hints= try val = tos next_variable except StopIteration exc ObservedUserStopIteration ex isinstance ex exc ObservedUserStopIteration exc handle_observed_exception The iterator exhausted Stop loop pop push ConstantVariable create ex value Repeat YIELD_FROM instruction next eval loop assert isinstance instruction_pointer int instruction_pointer instruction_pointer -= push val Add value yield into generated_items replace top stack None YIELD_VALUE inst SEND inst Instruction - None assert len stack = val = pop tos = stack - isinstance tos IteratorVariable LocalGeneratorObjectVariable isinstance tos UserDefinedObjectVariable isinstance tos value collections abc Iterator isinstance val ConstantVariable val value None try val = tos next_variable type ignore arg-type except StopIteration exc ObservedUserStopIteration ex To implement SEND we have look implementation when iterator returns StopIteration This translates code https github com python cpython blob Python ceval c#L -L https github com python cpython blob Python bytecodes c#L -L The implementation different In we rely END_SEND clean up In SEND does cleanup well sys version_info pop Python uses new opcode END_SEND push ConstantVariable create ex value jump inst push val invoke send Unreachable code - you hit you implementing generator support have lifted ` unimplemented generator ` frame conversion This codepath handles subgenerator lines up line Python https github com python cpython blob Python ceval c#L unimplemented_v gb_type= Unreachable sub-generator code context= explanation= Should only encountered while implementing generator support hints= unimplemented_v gb_type= SEND bad type context=f TOS type typestr tos explanation=f Attempted SEND unsupported type typestr tos hints=