torch TorchTensorEngine rand shape device=None dtype=None requires_grad=False torch rand shape device=device dtype=dtype requires_grad=requires_grad randn shape device=None dtype=None requires_grad=False torch randn shape device=device dtype=dtype requires_grad=requires_grad nchw_rand shape device=None requires_grad=False rand shape device=device requires_grad=requires_grad reset _ pass rand_like v torch rand_like v numpy t t cpu numpy mul t t t t add t t t + t batch_norm data mean var training torch nn functional batch_norm data mean var training=training instance_norm data torch nn functional instance_norm data layer_norm data shape torch nn functional layer_norm data shape sync_cuda torch cuda synchronize backward tensors grad_tensors _ torch autograd backward tensors grad_tensors=grad_tensors sum data dims torch sum data dims softmax data dim=None dtype=None torch nn functional softmax data dim dtype cat inputs dim= torch cat inputs dim=dim clamp data min max torch clamp data min=min max=max relu data torch nn functional relu data tanh data torch tanh data max_pool d data kernel_size stride= torch nn functional max_pool d data kernel_size stride=stride avg_pool d data kernel_size stride= torch nn functional avg_pool d data kernel_size stride=stride conv d_layer ic oc kernel_size groups= torch nn Conv d ic oc kernel_size groups=groups matmul t t torch matmul t t to_device module device module device