mypy allow-untyped-defs Spectral Normalization https arxiv org abs typing Any Optional TypeVar torch torch nn functional F torch nn modules Module __all__ = SpectralNorm SpectralNormLoadStateDictPreHook SpectralNormStateDictHook spectral_norm remove_spectral_norm SpectralNorm Invariant before after each forward call u = F normalize W v NB At initialization invariant enforced _version int = At version made ` W ` buffer added ` v ` buffer made eval mode use ` W = u W_orig v ` rather than stored ` W ` name str dim int n_power_iterations int eps float __init__ name str = weight n_power_iterations int = dim int = eps float = e- - None name = name dim = dim n_power_iterations = raise ValueError Expected n_power_iterations positive f got n_power_iterations= n_power_iterations n_power_iterations = n_power_iterations eps = eps reshape_weight_to_matrix weight torch Tensor - torch Tensor weight_mat = weight dim = permute dim front weight_mat = weight_mat permute dim d d range weight_mat dim d = dim height = weight_mat size weight_mat reshape height - compute_weight module Module do_power_iteration bool - torch Tensor NB If ` do_power_iteration ` set ` u ` ` v ` vectors updated power iteration in-place This very important because ` DataParallel ` forward vectors being buffers broadcast parallelized module each module replica which new module object created fly And each replica runs its own spectral norm power iteration So simply assigning updated vectors module function runs will cause update lost forever And next time parallelized module replicated same randomly initialized vectors broadcast used Therefore make change propagate back we rely two important behaviors also enforced via tests ` DataParallel ` doesn t clone storage broadcast tensor already correct device makes sure parallelized module already ` device ` If out tensor ` out= ` kwarg has correct shape will just fill values Therefore since same power iteration performed all devices simply updating tensors in-place will make sure module replica ` device ` will update _u vector parallelized module shared storage However after we update ` u ` ` v ` in-place we need clone them before using them normalize weight This support backproping through two forward passes e g common pattern GAN training loss = D real - D fake Otherwise engine will complain variables needed do backward first forward i e ` u ` ` v ` vectors changed second forward weight = getattr module name + _orig u = getattr module name + _u v = getattr module name + _v weight_mat = reshape_weight_to_matrix weight do_power_iteration torch no_grad _ range n_power_iterations Spectral norm weight equals ` u^T W v ` where ` u ` ` v ` first left right singular vectors This power iteration produces approximations ` u ` ` v ` v = F normalize torch mv weight_mat t u dim= eps=self eps out=v u = F normalize torch mv weight_mat v dim= eps=self eps out=u n_power_iterations See above why we need clone u = u clone memory_format=torch contiguous_format v = v clone memory_format=torch contiguous_format sigma = torch dot u torch mv weight_mat v weight = weight sigma weight remove module Module - None torch no_grad weight = compute_weight module do_power_iteration=False delattr module name delattr module name + _u delattr module name + _v delattr module name + _orig module register_parameter name torch nn Parameter weight detach __call__ module Module inputs Any - None setattr module name compute_weight module do_power_iteration=module training _solve_v_and_rescale weight_mat u target_sigma Tries returns vector ` v ` s t ` u = F normalize W v ` invariant top ` u W v = sigma ` This uses pinverse case W^T W invertible v = torch linalg multi_dot weight_mat t mm weight_mat pinverse weight_mat t u unsqueeze squeeze v mul_ target_sigma torch dot u torch mv weight_mat v staticmethod apply module Module name str n_power_iterations int dim int eps float - SpectralNorm hook module _forward_pre_hooks values isinstance hook SpectralNorm hook name == name raise RuntimeError f Cannot register two spectral_norm hooks same parameter name fn = SpectralNorm name n_power_iterations dim eps weight = module _parameters name weight None raise ValueError f ` SpectralNorm ` cannot applied parameter ` name ` None isinstance weight torch nn parameter UninitializedParameter raise ValueError The module passed ` SpectralNorm ` can t have uninitialized parameters Make sure run dummy forward before applying spectral normalization torch no_grad weight_mat = fn reshape_weight_to_matrix weight h w = weight_mat size randomly initialize ` u ` ` v ` u = F normalize weight new_empty h normal_ dim= eps=fn eps v = F normalize weight new_empty w normal_ dim= eps=fn eps delattr module fn name module register_parameter fn name + _orig weight We still need assign weight back fn name because all sorts things may assume exists e g when initializing weights However we can t directly assign could nn Parameter gets added parameter Instead we register weight data plain attribute setattr module fn name weight data module register_buffer fn name + _u u module register_buffer fn name + _v v module register_forward_pre_hook fn module _register_state_dict_hook SpectralNormStateDictHook fn module _register_load_state_dict_pre_hook SpectralNormLoadStateDictPreHook fn fn This top level because Py pickle doesn t like inner nor instancemethod SpectralNormLoadStateDictPreHook See docstring SpectralNorm _version changes spectral_norm __init__ fn - None fn = fn For state_dict version None assuming has gone through least one training forward we have u = F normalize W_orig v W = W_orig sigma where sigma = u W_orig v To compute ` v ` we solve ` W_orig x = u ` let v = x u W_orig x W W_orig __call__ state_dict prefix local_metadata strict missing_keys unexpected_keys error_msgs - None fn = fn version = local_metadata get spectral_norm get fn name + version None version None version weight_key = prefix + fn name version None all weight_key + s state_dict s _orig _u _v weight_key state_dict Detect updated state dict just missing metadata This could happen users crafting state dict themselves so we just pretend newest has_missing_keys = False suffix _orig _u key = weight_key + suffix key state_dict has_missing_keys = True strict missing_keys append key has_missing_keys torch no_grad weight_orig = state_dict weight_key + _orig weight = state_dict pop weight_key sigma = weight_orig weight mean weight_mat = fn reshape_weight_to_matrix weight_orig u = state_dict weight_key + _u v = fn _solve_v_and_rescale weight_mat u sigma state_dict weight_key + _v = v This top level because Py pickle doesn t like inner nor instancemethod SpectralNormStateDictHook See docstring SpectralNorm _version changes spectral_norm __init__ fn - None fn = fn __call__ module state_dict prefix local_metadata - None spectral_norm local_metadata local_metadata spectral_norm = key = fn name + version key local_metadata spectral_norm raise RuntimeError f Unexpected key metadata spectral_norm key local_metadata spectral_norm key = fn _version T_module = TypeVar T_module bound=Module spectral_norm module T_module name str = weight n_power_iterations int = eps float = e- dim Optional int = None - T_module r Apply spectral normalization parameter given module math \mathbf W _ SN = \dfrac \mathbf W \sigma \mathbf W \sigma \mathbf W = \max_ \mathbf h \mathbf h \ne \dfrac \ &#124; \mathbf W \mathbf h \ &#124; _ \ &#124; \mathbf h \ &#124; _ Spectral normalization stabilizes training discriminators critics Generative Adversarial Networks GANs rescaling weight tensor spectral norm math ` \sigma ` weight matrix calculated using power iteration method If dimension weight tensor greater than reshaped D power iteration method get spectral norm This implemented via hook calculates spectral norm rescales weight before every meth ` ~Module forward ` call See ` Spectral Normalization Generative Adversarial Networks ` _ _ ` Spectral Normalization Generative Adversarial Networks ` https arxiv org abs Args module nn Module containing module name str optional name weight parameter n_power_iterations int optional number power iterations calculate spectral norm eps float optional epsilon numerical stability calculating norms dim int optional dimension corresponding number outputs default ` ` ` ` except modules instances ConvTranspose d when ` ` ` ` Returns The original module spectral norm hook note This function has been reimplemented func ` torch nn utils parametrizations spectral_norm ` using new parametrization functionality func ` torch nn utils parametrize register_parametrization ` Please use newer version This function will deprecated future version PyTorch Example m = spectral_norm nn Linear m Linear in_features= out_features= bias=True m weight_u size torch Size dim None isinstance module torch nn ConvTranspose d torch nn ConvTranspose d torch nn ConvTranspose d dim = dim = SpectralNorm apply module name n_power_iterations dim eps pyrefly ignore bad-return module remove_spectral_norm module T_module name str = weight - T_module r Remove spectral normalization reparameterization module Args module Module containing module name str optional name weight parameter Example m = spectral_norm nn Linear remove_spectral_norm m k hook module _forward_pre_hooks items isinstance hook SpectralNorm hook name == name hook remove module del module _forward_pre_hooks k break raise ValueError f spectral_norm name found module k hook module _state_dict_hooks items isinstance hook SpectralNormStateDictHook hook fn name == name del module _state_dict_hooks k break k hook module _load_state_dict_pre_hooks items isinstance hook SpectralNormLoadStateDictPreHook hook fn name == name del module _load_state_dict_pre_hooks k break module