mypy allow-untyped-defs logging multiprocessing multiprocessing connection os pickle signal sys tempfile time warnings concurrent futures as_completed ThreadPoolExecutor typing Optional _prctl_pr_set_pdeathsig type ignore attr-defined ENV_VAR_PARALLEL_START = TORCH_MP_PARALLEL_START log = logging getLogger __name__ __all__ = ProcessContext ProcessException ProcessExitedException ProcessRaisedException spawn SpawnContext start_processes ProcessException Exception __slots__ = error_index error_pid __init__ msg str error_index int pid int super __init__ msg msg = msg error_index = error_index pid = pid __reduce__ type msg error_index pid ProcessRaisedException ProcessException Exception raised when process failed due exception raised code __init__ msg str error_index int error_pid int super __init__ msg error_index error_pid ProcessExitedException ProcessException Exception raised when process failed due signal exited specific code __slots__ = exit_code __init__ msg str error_index int error_pid int exit_code int signal_name Optional str = None super __init__ msg error_index error_pid exit_code = exit_code signal_name = signal_name __reduce__ type msg error_index pid exit_code signal_name _wrap fn i args error_file prctl Linux specific system call On other systems following function call has no effect This set ensure non-daemonic child processes can terminate their parent terminates before they do _prctl_pr_set_pdeathsig signal SIGINT try fn i args except KeyboardInterrupt pass SIGINT Killed parent do nothing except Exception Propagate exception parent process keeping original traceback traceback open error_file wb fh pickle dump traceback format_exc fh sys exit ProcessContext __init__ processes error_files error_files = error_files processes = processes sentinels = process sentinel index index process enumerate processes pids int process pid process processes _join_procs_with_timeout timeout float Attempt join all processes shared timeout end = time monotonic + timeout process processes pyrefly ignore no-matching-overload time_to_wait = max end - time monotonic process join time_to_wait join timeout Optional float = None grace_period Optional float = None r Join one more processes within spawn context Attempt join one more processes spawn context If one them exited non-zero exit status function kills remaining processes optionally grace period raises exception cause first process exiting Returns ` ` True ` ` all processes have been joined successfully ` ` False ` ` there more processes need joined Args timeout float Wait long seconds before giving up waiting grace_period float When any processes fail wait long seconds others shutdown gracefully before terminating them If they still don t exit wait another grace period before killing them Ensure function can called even when we re done len sentinels == True Wait any process fail all them succeed ready = multiprocessing connection wait sentinels keys timeout=timeout error_index = None sentinel ready index = sentinels pop sentinel process = processes index process join process exitcode = error_index = index break Return there no error error_index None Return whether all processes have been joined len sentinels == An error occurred Clean-up all processes before returning First allow grace period processes shutdown themselves grace_period None _join_procs_with_timeout grace_period Then terminate processes still alive Try SIGTERM first process processes process is_alive log warning Terminating process s via signal SIGTERM process pid process terminate Try SIGKILL process isn t going down after another grace_period The reason related python signal handling limited main thread c c++ land stuck won t handle We have seen processes getting stuck handling SIGTERM above reason _join_procs_with_timeout grace_period None grace_period process processes process is_alive log warning Unable shutdown process s via SIGTERM forcefully exiting via SIGKILL process pid process kill process join The file will only created process crashed failed_process = processes error_index os access error_files error_index os R_OK exitcode = processes error_index exitcode exitcode try name = signal Signals -exitcode name except ValueError name = f Unknown signal -exitcode raise ProcessExitedException f process error_index d terminated signal name error_index=error_index error_pid=failed_process pid exit_code=exitcode signal_name=name raise ProcessExitedException f process error_index d terminated exit code exitcode d error_index=error_index error_pid=failed_process pid exit_code=exitcode open error_files error_index rb fh original_trace = pickle load fh msg = f \n\n -- Process error_index d terminated following error \n msg += original_trace raise ProcessRaisedException msg error_index failed_process pid SpawnContext ProcessContext __init__ processes error_files warnings warn SpawnContext renamed ProcessContext since release stacklevel= super __init__ processes error_files Note start_processes mp start_processes handles both start_method= spawn fork It s supposed more generalized API than mp spawn Currently we only document mp spawn s CUDA compatible start_method However environments like Ipython notebooks fork works better than spawn Every helper function we created mp spawn indeed general enough backends like XLA can reuse them Colab notebooks well Currently we only add API first we can consider adding documentation needed future start_processes fn args= nprocs= join=True daemon=False start_method= spawn To speed up performance certain cases see https github com pytorch pytorch issues func will start processes parallel start_method forkserver Please opt perf optimization setting env var TORCH_MP_PARALLEL_START todo investigate why spawn does work threadpool raises SIGINT start_method == forkserver os environ get ENV_VAR_PARALLEL_START == log info Starting processes parallel start_parallel = True Set env var TORCH_MP_PARALLEL_START disable parallel start start_parallel = False mp = multiprocessing get_context start_method error_files = None nprocs processes = None nprocs start_process i Each process assigned file write tracebacks We use file being non-empty indicate exception occurred vs expected shutdown Note previously used multiprocessing Queue can prone deadlocks so we went simpler solution one-shot message between processes tf = tempfile NamedTemporaryFile prefix= pytorch-errorfile- suffix= pickle delete=False tf close os unlink tf name process = mp Process pyrefly ignore missing-attribute target=_wrap args= fn i args tf name daemon=daemon process start i process tf name start_parallel i range nprocs idx process tf_name = start_process i error_files idx = tf_name processes idx = process ThreadPoolExecutor max_workers=nprocs executor futures = executor submit start_process i i range nprocs fut as_completed futures idx process tf_name = fut result idx process rank needs same error_files idx = tf_name processes idx = process context = ProcessContext processes error_files join context Loop join until returns True raises exception while context join pass spawn fn args= nprocs= join=True daemon=False start_method= spawn r Spawns ` ` nprocs ` ` processes run ` ` fn ` ` ` ` args ` ` If one processes exits non-zero exit status remaining processes killed exception raised cause termination In case exception caught child process forwarded its traceback included exception raised parent process Args fn function Function called entrypoint spawned process This function must defined top level module so can pickled spawned This requirement imposed multiprocessing The function called ` ` fn i args ` ` where ` ` i ` ` process index ` ` args ` ` passed through tuple arguments args tuple Arguments passed ` ` fn ` ` nprocs int Number processes spawn join bool Perform blocking join all processes daemon bool The spawned processes daemon flag If set True daemonic processes will created start_method str deprecated method will always use ` ` spawn ` ` start method To use different start method use ` ` start_processes ` ` Returns None ` ` join ` ` ` ` True ` ` ` ~ProcessContext ` ` ` join ` ` ` ` False ` ` start_method = spawn msg = f This method only supports start_method=spawn got start_method \n To use different start_method use \n\t\t torch multiprocessing start_processes warnings warn msg FutureWarning stacklevel= start_processes fn args nprocs join daemon start_method= spawn