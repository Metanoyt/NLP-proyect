Core graph building functionality PyTorch s Dynamo system This module contains essential components constructing managing FX graphs during compilation - OutputGraph Manages overall graph construction compilation process It owns SubgraphTracer handles graph compilation execution state management OutputGraph also manages features like graph deduplication symbolic shape handling tracking side effects - SubgraphTracer Handles actual FX graph construction tracing Python code It supports advanced features like higher-order operators through nested tracers lifting free variables handling symbolic shapes The module supports key Dynamo features including - Higher-order operators through nested SubgraphTracers - Graph deduplication optimization - Symbolic shape handling propagation - Side effect tracking management - Guard insertion management collections contextlib copy functools inspect itertools logging operator re sys traceback warnings weakref collections abc Callable Generator Sequence dataclasses dataclass field dc_field types CodeType typing Any cast Optional TYPE_CHECKING Union typing_extensions ParamSpec TypeVar sympy torch _guards torch _logging torch distributed dist torch nn torch utils _pytree pytree torch fx Tensor torch _C _dynamo guards torch _dynamo exc ShortenTraceback TensorifyScalarRestartAnalysis torch _guards CompileContext CompileId GlobalContextCheckpointState Source tracing TracingContext torch _subclasses fake_tensor FakeTensor torch _utils_internal signpost_event torch export dynamic_shapes _ConstraintTarget torch fx _lazy_graph_module _make_graph_module type ignore attr-defined torch fx experimental _backward_state BackwardState torch fx experimental symbolic_shapes free_symbols guard_scalar is_symbolic ShapeEnv Specialization uninteresting_files torch fx node Target torch fx passes runtime_assert insert_deferred_runtime_asserts torch multiprocessing reductions StorageWeakRef torch utils _ordered_set OrderedSet torch utils _python_dispatch is_traceable_wrapper_subclass config exc logging torchdynamo_logging variables backends registry CompiledFn CompilerFn bytecode_transformation create_binary_slice create_binary_subscr create_build_tuple create_call_function create_dup_top create_instruction create_load_const create_rot_n create_swap Instruction unique_id code_context code_context codegen PyCodegen current_scope_id enter_new_scope device_interface get_interface_for_device exc BackendCompilerFailed exceptions_allowed_to_be_fallback SkipFrame unimplemented_v unimplemented_v _with_warning graph_bytecode_inputs has_user_objects index_to_bytecode_constructor graph_deduplication apply_graph_deduplication graph_region_tracker GraphRegionTracker guards GuardBuilder install_guard mutation_guard is_dynamic_nn_module side_effects AttributeMutationExisting SideEffects ValueMutationExisting source _get_source_debug_name AttrSource BackwardStateSource ConstantSource GetItemSource GlobalStateSource is_constant_source is_from_local_source LocalSource NumpyTensorSource ParamBufferSource ShapeEnvSource SyntheticLocalSource TensorProperty TensorPropertySource utils _extract_tensor_dict checkpoint_params CleanupHook clone_inputs count_calls counters dynamo_timed get_instruction_source_ get_locals_to_steal get_static_address_type get_unique_name_wrt graph_break_reasons increment_op_count istype lazy_format_graph_code LazyString nn_module_proxy same set_example_value variables base VariableTracker variables builder BackwardStateGraphArg GraphArg TrackedFake wrap_fx_proxy variables ctx_manager ContextWrappingVariable variables lists BaseListVariable variables misc NullVariable variables nn_module NNModuleVariable variables tensor NumpyNdarrayVariable SymNodeVariable TensorVariable UnspecializedPythonVariable variables torch_function TensorWithTFOverrideVariable variables user_defined UserDefinedDictVariable TYPE_CHECKING torch _dynamo package CompilePackage torch _dynamo symbolic_convert InstructionTranslatorBase log = logging getLogger __name__ graph_tabular_log = torch _logging getArtifactLogger __name__ graph graph_code_log = torch _logging getArtifactLogger __name__ graph_code graph_sizes_log = torch _logging getArtifactLogger __name__ graph_sizes trace_call_log = torch _logging getArtifactLogger __name__ trace_call RootGuardManager = guards RootGuardManager Capture fn pointer time This guard against trying mark iterated tensors static case user overrides fn ptr og_module_named_buffers_fn_ptr = torch nn Module named_buffers og_module_named_parameters_fn_ptr = torch nn Module named_parameters dataclass frozen=True VariableTrackerCacheKey vt_id int Two different source can point same object However Dynamo handles globals local source differently when comes guards possibly some other parts well So cache also relies source source Source dataclass frozen=True AliasingInfo has_aliasing bool msg str dataclass frozen=True MutationInfo has_mutation bool msg str VariableTrackerCache __init__ - None cache dict VariableTrackerCacheKey VariableTracker = lookup value Any source Source - Optional VariableTracker key = VariableTrackerCacheKey id value source key cache None cache key add value Any source Source vt VariableTracker - None key = VariableTrackerCacheKey id value source cache key = vt clone - VariableTrackerCache Needed copy restore graph state new_cache = VariableTrackerCache new_cache cache update cache new_cache clear - None cache clear functools cache _step_logger - Any torchdynamo_logging get_step_logger log dataclass GraphCompileReason Stores why given output graph compiled i e what caused graph break reason str user_stack list traceback FrameSummary Indicates graph break reason due graph break graph_break bool = True __post_init__ - None graph_break graph_break_reasons append _get_gen_rand_values_fn random_calls Any - Callable list Any _gen_rand_values - list Any fn args kwargs fn args kwargs random_calls _gen_rand_values FakeRootModule torch nn Module Trick constructor fx GraphModule __init__ nn_modules dict str torch nn Module super __init__ k v nn_modules items setattr k v __repr__ - str FakeRootModule add_nn_modules nn_modules dict str torch nn Module - None k v nn_modules items setattr k v WrapperBackend __init__ backend CompilerFn - None backend CompilerFn = backend __call__ gm torch fx GraphModule example_inputs list torch Tensor - CompiledFn restore = checkpoint_params gm gm = gm copy_gm = copy deepcopy gm candidate = backend copy_gm example_inputs candidate None candidate gm forward gm forward config verify_correctness candidate verify_correctness=True try correct = gm forward clone_inputs example_inputs result = candidate clone_inputs example_inputs TODO replace ` same ` function one testing same correct result candidate raise RuntimeError f incorrect results backend except Exception log exception error verify_correctness raise finally restore Scope = dict str object dataclass OutputGraphGuardsState A base containing fields considered persistent when we want save all important state reconstrucing guards different process Normally we don t need add states here we may have when information needed serialize guards so fields here supposed serializable requirement local_scope Scope global_scope Scope This records initial torch function mode stack guarding torch_function_mode_stack list torch overrides TorchFunctionMode guard_on_key_order set Source Map graph input s ` Source ` sizes strides metadata input_source_to_sizes_strides dict Source dict str Any dual_level int functorch_layers list torch _functorch pyfunctorch FuncTorchInterpreter current_device Optional torch device global_state_guard torch _C _dynamo guards GlobalStateGuard _guards torch _guards GuardsSet _aotautograd_guards list torch _guards GuardEnvExpr Whether guards should checked correctness export bool = False skip_guards_check bool = False export_constraints bool = False name_of_builtins_dict_key_in_fglobals Optional str = None property shape_env - ShapeEnv raise AssertionError f shape_env shouldn t accessed type property guards - torch _guards GuardsSet _guards property aotautograd_guards - list torch _guards GuardEnvExpr _aotautograd_guards dump_guards_state - OutputGraphGuardsState Dump serializable version without extras OutputGraphGuardsState local_scope=self local_scope global_scope=self global_scope torch_function_mode_stack=self torch_function_mode_stack guard_on_key_order=self guard_on_key_order input_source_to_sizes_strides=self input_source_to_sizes_strides dual_level=self dual_level functorch_layers=self functorch_layers current_device=self current_device global_state_guard=self global_state_guard name_of_builtins_dict_key_in_fglobals=self name_of_builtins_dict_key_in_fglobals export=self export export_constraints=self export_constraints _guards=self guards _aotautograd_guards=self aotautograd_guards skip_guards_check=self skip_guards_check dataclass StackLocalsMetadata Stores metadata frame s stack locals purposes building resume functions num_stack int = number stack elements minus removed NULLs locals_names dict str int = dc_field default_factory=dict order locals codegen d stack stack_null_idxes list int = dc_field default_factory=list locals_null_keys list str = dc_field default_factory=list stack_ctx_args list tuple int tuple Any = dc_field default_factory=list stack_ctx_idxes_orig list int = dc_field default_factory=list locals_ctx_args list tuple str tuple Any = dc_field default_factory=list TODO we should expand make work atribtrary out dataclass ExportMetaData maps graph input index its source which later used export map correct user input In its flat form just looks like GetItem base=LocalSource foo idx= graph_input_idx_to_local_source dict int Source = dc_field default_factory=dict maps user output idx what type output There options graph out user input constants output_return_type dict int tuple str Any = dc_field default_factory=dict output spec traced function out_spec Union torch utils _pytree TreeSpec torch utils _pytree LeafSpec = torch utils _pytree _LEAF_SPEC module_call_spec dict str dict str Union torch utils _pytree TreeSpec torch utils _pytree LeafSpec = dc_field default_factory=dict get_builtins_dict global_scope Scope - dict str Any f_globals __builtins__ can dict module This implementation detail - https docs python org library builtins html This makes guarding any builtin messy because guard check_fn has check __builtins__ module dict then access either using getattr getitem respectively To solve problem we insert new entry f_globals which points builtins __dict__ then we guard any builtin dict To avoid any collision pre-existing keys we use install_global give us unique dict key f_builtins = global_scope __builtins__ isinstance f_builtins dict f_builtins = f_builtins __dict__ f_builtins OutputGraphCommon OutputGraphGuardsState A minimal interface full graph capture It intended target any tracer feeds into backends Currently dynamo s OutputGraph only known implementation interface used aot precompile strict export Importantly implementation also contains many other fields using during tracing included interface because they used once tracing complete It should safe assume caching precompile also uses interface In future we want make_fx used non-strict export also implement interface The serializable part interface OutputGraphGuardsState We do need serialize other parts however will pay disciplined about what those other parts especially since we want other tracers able meaningfully implement them we should generally try cut them down when possible __init__ output_graph_guards_state OutputGraphGuardsState import_sources Optional dict str str = None shape_env Optional ShapeEnv = None export_metadata Optional ExportMetaData = None tracked_fakes_id_to_source Optional dict int list Source = None super __init__ output_graph_guards_state local_scope output_graph_guards_state global_scope output_graph_guards_state torch_function_mode_stack output_graph_guards_state guard_on_key_order output_graph_guards_state input_source_to_sizes_strides output_graph_guards_state dual_level output_graph_guards_state functorch_layers output_graph_guards_state current_device output_graph_guards_state global_state_guard output_graph_guards_state _guards output_graph_guards_state _aotautograd_guards output_graph_guards_state export output_graph_guards_state skip_guards_check output_graph_guards_state export_constraints output_graph_guards_state name_of_builtins_dict_key_in_fglobals import_sources = import_sources The following fields currently known used clients In particular we need - shape_env building guards - export_metadata un flattening inputs outputs - tracked_fakes_id_to_source processing tensor dim constraints _shape_env = shape_env ShapeEnv private inheritance export_metadata = export_metadata ExportMetaData tracked_fakes_id_to_source dict int list Source = tracked_fakes_id_to_source property shape_env - ShapeEnv _shape_env bypass_package reason str = kwargs Any - None NOTE currently there no tests reachable when building guards so technically necessary include here It unclear whether we should include packaging altogether raise NotImplementedError OutputGraph OutputGraphCommon Wrapper hold outputs InstructionTranslator Mainly generated fx Graph OutputGraph frame being processed Each frame associated some root InstructionTranslator When user code calls function we construct InliningInstructionTranslator continues write into root InstructionTranslator s OutputGraph side_effects SideEffects __init__ code_options dict str Any compiler_fn Optional CompilerFn root_tx InstructionTranslatorBase export bool export_constraints Sequence _ConstraintTarget frame_state Any local_scope Scope global_scope Scope f_code CodeType torch_function_mode_stack list torch overrides TorchFunctionMode package Optional CompilePackage one_graph bool = False - None OutputGraphGuardsState __init__ local_scope global_scope torch_function_mode_stack guard_on_key_order=set input_source_to_sizes_strides= dual_level=torch autograd forward_ad _current_level functorch_layers=torch _functorch pyfunctorch retrieve_all_functorch_interpreters current_device=torch utils _device CURRENT_DEVICE initial_global_state only None during NopTest global_state_guard=torch _dynamo convert_frame initial_global_state torch _C _dynamo guards GlobalStateGuard These set property instead just initialize them blank _guards=torch _guards GuardsSet _aotautograd_guards= tracers = SubgraphTracer is_export=export Map graph input s ` Source ` its ` VariableTracker ` de-duplicate graph inputs source reuse tracker input_source_to_var dict Source VariableTracker = export = export export_constraints = export_constraints type ignore assignment frame_state = frame_state cleanup_hooks list Callable Any = compile_id id number current torch compile compile_id int = next _compile_id_counter Set globals installed via install_global APIs installed_globals set str = set TODO maybe should just pass entire f_code here Not sure co_fields = co_name f_code co_name co_filename f_code co_filename co_firstlineno f_code co_firstlineno region_tracker = GraphRegionTracker tracked_fakes says where any tensor wrapped fake came It similar GraphArg all GraphArgs will get will get added TrackedFakes TrackedFakes also contains GraphArgs got pruned things like Tensor attributes which aren t explicit graph inputs Used shape guard tracked_fakes list TrackedFake = shape_env = ShapeEnv Reference Cycle Share reference list TrackedFake ShapeEnv needs order able reproduce call produce_guards arbitrary time point That because TrackedFake instances may have its metadata changed throughout program execution tracked_fakes=self tracked_fakes We want allow capture scalar outputs allow_dynamic_output_shape_ops when fullgraph=True allow_scalar_outputs=one_graph config capture_scalar_outputs allow_dynamic_output_shape_ops=one_graph config capture_dynamic_output_shape_ops prefer_deferred_runtime_asserts_over_guards=config prefer_deferred_runtime_asserts_over_guards co_fields=self co_fields In export mode we force shape_env strictly disallow any constraining user marked dynamic dims torch _functorch config _config _config patch fake_tensor_allow_unsafe_data_ptr_access=False fake_mode = torch _subclasses FakeTensorMode shape_env=shape_env TODO tmanlaibaatar Remove once we always lift params buffers allow_non_fake_inputs=bool export export=self export tracing_context TracingContext = TracingContext fake_mode tracing_context traced_code append f_code traced_code = tracing_context traced_code dynamo_compile_id Optional CompileId = CompileContext current_compile_id init_ambient_guards Map each tensor id list sources This necessary because tensor ids cannot recovered tracked fakes general We use map interpret i e check violations constraints specifically equality constraints which have shared tensor ids them This map should also generally useful e g de serialization tracked_fakes_id_to_source dict int list Source = collections defaultdict list Stores full fqn param buffer relevant source param_name_to_source Optional dict str Source = side_effects = SideEffects Cached variable trackers This makes symbolic analysis LOAD_GLOBAL LOAD_ATTR same python objects free variable_tracker_cache = VariableTrackerCache unique_var_id = itertools count code_options dict str Any = dict code_options output_instructions list Instruction = used track nodes added between calls copy_graphstate restore_graphstate timestamp = A list register_finalizer_fns apply output graph module register_finalizer_fns list Callable fx GraphModule None = Not checkpointed compiler_fn Optional CompilerFn = compiler_fn root_tx = root_tx package = package Given source what user stacks all locations accessed For efficiency we only populate - During export - If source could potentially lead spurious export input Feel free populate more frequently other use-cases arise aware we have generate full stacks each recording source_to_user_stacks dict Source list traceback StackSummary = _current_tx list InstructionTranslatorBase = cleanups list CleanupHook = should_exit = False unspec_variable_map dict str UnspecializedPythonVariable = This returns false TF Overall both mode subclass disabled OR TF Mode stack empty torch_function_mode_enabled = torch _C _is_torch_function_mode_enabled Tracks output graph has user defined allowed function graph This used later determine we should fallback eager certain exceptions THe idea user has applied allow_in_graph they would like see error instead falling back backend errors has_user_defined_allowed_in_graph = False Tracks list called ops tagged pt _compliant_tag This information useful logging non_compliant_ops set torch _ops OpOverload = set Tracks list called custom ops tagged pt _compliant_tag This information useful logging compliant_custom_ops set torch _ops OpOverload = set We save global torch state here restored case graph breaks The relevant issue seen here https github com pytorch pytorch pull #issuecomment- where inlining function changes global state because presence torch no_grad there graph break save_global_state Tracks original FQNs constant tensors original graph i e buffers parameters dynamo_flat_name_to_original_fqn dict str str = All calls random replaced single call __gen_rand_values functions returns tuple random values each original call random_calls tracks calls random random_values_var stores name variable stores __gen_rand_values results random_calls list tuple Callable object tuple object dict str object = random_values_var Any = None Bytecode insert right before we call graph pregraph_bytecode list Instruction = Use pass values backward hooks when using compiled autograd backward_state dict str VariableTracker = backward_state_proxy Optional torch fx Proxy = None backward_state_var Optional str = None pyrefly ignore bad-override name_of_builtins_dict_key_in_fglobals str = install_builtins_dict_in_fglobals compiler_trace_stack = contextlib ExitStack These ambient currently-global saved_tensor_hooks stashed autograd set entire duration compiled region This invariant today because we graph break saved_tensor_hook context manager inside compiled region saved_tensors_hooks_subgraph_names Optional list str = maybe_install_saved_tensors_hooks_subgraphs mangled alias - module fqn name import_sources dict str str = export_metadata = ExportMetaData Set inlined unspecialized modules names generate dynamo_flat_name_to_original_fqn mapping used_inlined_inbuilt_modules_names OrderedSet str = OrderedSet mark_bytecode_tracing_start - None compiler_trace_stack enter_context dynamo_timed bytecode_tracing log_pt _compile_event=True mark_bytecode_tracing_stop - None compiler_trace_stack close install_builtins_dict_in_fglobals - str f_builtins = get_builtins_dict global_scope install_global __builtins_dict__ f_builtins add_backward_state_hook hook VariableTracker prefix str = hook - tuple str torch fx Proxy name = f prefix len backward_state assert name backward_state backward_state name = hook name get_backward_state_proxy get_backward_state_proxy - torch fx Proxy backward_state_proxy None export unimplemented_v gb_type= backward_state does support export context= explanation= Compiled autograd doesn t work ` torch export ` hints= example_value = BackwardState backward_state_proxy = root_tracer create_graph_input dynamo_backward_state type example_value example_value source=BackwardStateSource backward_state_proxy node meta grapharg = BackwardStateGraphArg backward_state_var = new_var backward_state_proxy This gets its own helper function so guards DEBUG logs more informative init_ambient_guards - None Register SHAPE_ENV guard make sure we setup shape guards show up ShapeEnv guards add ShapeEnvSource make_guard GuardBuilder SHAPE_ENV guards add GlobalStateSource make_guard GuardBuilder DETERMINISTIC_ALGORITHMS guards add GlobalStateSource make_guard GuardBuilder GRAD_MODE guards add GlobalStateSource make_guard GuardBuilder DEFAULT_DEVICE guards add GlobalStateSource make_guard GuardBuilder TORCH_FUNCTION_STATE ci = torch _C _functorch peek_interpreter_stack ci None guards add GlobalStateSource make_guard GuardBuilder FUNCTORCH_STACK_MATCH torch _dynamo compiled_autograd in_compiled_autograd_region guards add GlobalStateSource make_guard GuardBuilder AUTOGRAD_SAVED_TENSORS_HOOKS maybe_install_saved_tensors_hooks_subgraphs - Optional list str torch _dynamo compiled_autograd in_compiled_autograd_region None get_hooks = torch _functorch _aot_autograd utils top_saved_tensors_hooks are_inline_hooks = torch _functorch _aot_autograd utils saved_tensors_hooks_are_inlineable hooks = get_hooks are_inline_hooks hooks None If GraphModule provided user contains fx wrap We can only rely user provided cache hash case If user did provide cache hash - then we always bypass cache pack_gm unpack_gm = hooks pack_subgraph_name = install_subgraph saved_tensors_hooks_pack torch fx GraphModule nn_modules pack_gm graph unpack_subgraph_name = install_subgraph saved_tensors_hooks_unpack torch fx GraphModule nn_modules unpack_gm graph assert pack_subgraph_name == saved_tensors_hooks_pack_ assert unpack_subgraph_name == saved_tensors_hooks_unpack_ pack_subgraph_name unpack_subgraph_name synthetic_graph_input fn Callable Any args tuple Any - VariableTracker call fn args before graph runs turn result into fake input example_value = fn args varname = new_var cg = PyCodegen root_tx cg add_push_null lambda cg load_import_from fn __module__ fn __name__ cg foreach map variables ConstantVariable create args cg call_function len args False cg store varname pregraph_bytecode extend cg get_instructions source = SyntheticLocalSource varname result = VariableTracker build root_tx example_value source Realize VT because we will delete guards next line result = result realize TracingContext get guards_context dynamo_guards remove_guards_with_source source result add_cleanup_hook fn Callable Any - None cleanup_hooks append fn call_cleanup_hooks - None hook reversed cleanup_hooks hook cleanup_hooks clear property root_tracer - SubgraphTracer tracers property current_tracer - SubgraphTracer tracers - is_root_tracer - bool Helper tell we inside higher order operator tracing len tracers == property graph - torch fx Graph current_tracer graph TODO rzou can delete after we refactor speculate_subgraph use nested GraphTracer graph setter graph value torch fx Graph - None current_tracer graph = value property input_name_to_proxy - dict str fx Proxy current_tracer input_name_to_proxy property real_value_cache - dict fx Node torch Tensor current_tracer real_value_cache property bound_symbols - dict sympy Symbol Union torch fx Proxy LazyProxy current_tracer bound_symbols If you here you re looking create_graph_input avoid ambiguity please call one following - current_tracer create_graph_input - root_tracer create_graph_input See NOTE HigherOrderOperator tracing design more context create_proxy args Any kwargs Any - torch fx Proxy current_tracer create_proxy args kwargs create_node args Any kwargs Any - torch fx Node current_tracer create_node args kwargs remove_node args Any kwargs Any - None current_tracer remove_node args kwargs contextlib contextmanager subtracer source_target Optional Target prior_tracer SubgraphTracer - Generator fx Tracer None None new_scope_ctx = enter_new_scope try prior_tracer Lineage MUST stay preserved assert prior_tracer parent current_tracer new_scope_ctx __enter__ tracer = prior_tracer prior_tracer SubgraphTracer parent=self current_tracer source_target=source_target is_export=self current_tracer is_export tracers append tracer yield tracer finally new_scope_ctx __exit__ None None None tracers pop property output - OutputGraph property fake_mode - torch _subclasses FakeTensorMode assert tracing_context fake_mode None tracing_context fake_mode property shape_env - ShapeEnv assert tracing_context fake_mode None assert tracing_context fake_mode shape_env None tracing_context fake_mode shape_env property guards - torch _guards GuardsSet tracing_context guards_context dynamo_guards property nn_modules - dict str Any tracing_context module_context nn_modules property aotautograd_guards - list torch _guards GuardEnvExpr tracing_context guards_context aotautograd_guards save_global_state out Optional dict str tuple Callable Any bool = None - None Saves out provided Else saves tracing context s global_state global_state = cast dict str tuple Callable Any bool out out None tracing_context global_context global_state global_state grad_enabled = torch set_grad_enabled torch is_grad_enabled global_state autocast_enabled = functools partial torch set_autocast_enabled cuda torch is_autocast_enabled cuda global_state autocast_cpu_enabled = functools partial torch set_autocast_enabled cpu torch is_autocast_enabled cpu global_state autocast_gpu_dtype = type ignore assignment functools partial torch set_autocast_dtype cuda torch get_autocast_dtype cuda global_state autocast_cpu_dtype = type ignore assignment functools partial torch set_autocast_dtype cpu torch get_autocast_dtype cpu global_state autocast_cache_enabled = torch set_autocast_cache_enabled torch is_autocast_cache_enabled push_tx tx InstructionTranslatorBase - None _current_tx append tx pop_tx - InstructionTranslatorBase _current_tx pop property current_tx - InstructionTranslatorBase root_tx _current_tx _current_tx - count_calls - int count_calls graph is_empty_graph - bool len list graph nodes == has_outputs - bool len x x graph nodes x op == output get_submodule keys str - Union torch nn Module Any assert keys obj Union torch nn Module dict str torch nn Module = nn_modules k keys split isinstance obj dict obj = obj k obj = getattr obj k obj new_var name str = tmp - str existing = set code_options co_varnames In common case will O while True var = f name _ next unique_var_id var existing code_options co_varnames += var var update_co_names name str - None Ensure code_options co_names contains name name code_options co_names code_options co_names += name staticmethod module_key_name names Any - str create new unique name name = _ join map str names Strip _buffers _parameters _modules names name = re sub r \ _ modules &#124; parameters &#124; buffers \ \ ^ \ \ + \ \ r \ name Replace getattr b b name = re sub r getattr\ \s ^ + \s \s \ ^ \ + \ \s \ r \ \ name Strip guard lookup L G access name = re sub r ^ GL \ \ $ r \ name e g replace abc xyz qkv abc xyz_ qkv name = re sub r \ \d+ \ r _\g name e g replace abc xyz_ qkv abc_xyz_ _qkv name = re sub r ^a-zA-Z - _ name name name isalpha name = sub + name name register_static_attr_and_return_proxy attr_prefix str attr_value Any - fx Proxy Check module already exists does already added proxy This important executorch tests isinstance attr_value torch nn Module name mod nn_modules items mod attr_value proxy = create_proxy get_attr name proxy attr_name = get_unique_name_wrt attr_prefix nn_modules TODO ` nn_modules ` has been historically overloaded store lot more than just nn module objects fix nn_modules attr_name = attr_value proxy = create_proxy get_attr attr_name set_example_value proxy node attr_value proxy register_attr_or_module target Union torch nn Module torch Tensor Any names Any options Any - VariableTracker is_dynamic_nn_module target export Instead returning UnspecializedNNModuleVariable call VariableTracker build so tracked mutation VariableTracker build current_tx target options options = dict options assert source options source = options source assert isinstance source ParamBufferSource isinstance target torch Tensor tracer = current_tracer is_root_tracer For higher order ops we don t want insert get_attr innermost graph Instead we want raise params buffers inputs higher-order graph register them get_attrs root tracer Note Dynamo will still call lift_tracked_freevar_to_input when these inputs encountered inner graph The only difference what happens root tracer nn Parameters vs free inputs The free inputs registered placeholders root graph whereas nn Parameters registered get_attr nodes root graph tracer = root_tracer wrap_name module_key str - VariableTracker assert param_name_to_source None param_name_to_source module_key = source Check attr has already been registered This can happen when two different sources point same tensor assert root_tx None target root_tx output side_effects root_tx output side_effects target get_static_address_type target == guarded isinstance source NumpyTensorSource install_guard source make_guard GuardBuilder ID_MATCH is_constant_source source install_guard source make_guard GuardBuilder TENSOR_MATCH vt = wrap_fx_proxy root_tx tracer create_proxy get_attr module_key example_value=target options Track object so avoid duplicate registration case different sources pointing same tensor object vt = root_tx output side_effects track_object_existing target vt assert tensor_dict vt as_proxy node meta pyrefly ignore bad-argument-type vt as_proxy node meta tensor_dict = _extract_tensor_dict target vt isinstance target torch nn Module assert isinstance target torch nn Module source install_guard source make_guard GuardBuilder NN_MODULE wrap_name module_key str - VariableTracker pyrefly ignore bad-argument-type NNModuleVariable type target module_key target options This Dynamo created graph module e g graph module coming higher order ops NNModuleVariable tracker can t sourceless so let s unspecializedNNModule variable tracker wrap_name module_key str - VariableTracker variables UnspecializedNNModuleVariable target options isinstance target torch SymInt torch SymFloat HACKY CODE REGION BEGIN WE ARE PIGGYBACKING ON EXISTING INFRA TO REGISTER ATTRS This ultimately gets written nn_modules which unfortunate Attrs tenors symints such need migrated have their own storage alas like now wrap_name module_key str - VariableTracker SymNodeVariable create create_proxy get_attr module_key sym_num=target options HACKY CODE REGION END wrap_name module_key str - VariableTracker output update_co_names module_key global_scope module_key = target VariableTracker build type ignore arg-type target ConstantSource source_name=module_key k v nn_modules items v target already exists wrap_name k name = OutputGraph module_key_name names name = get_unique_name_wrt name nn_modules global_scope nn_modules name = target isinstance target torch nn Module register_leaf_name leaf_name str - None assert param_name_to_source None new_source = ParamBufferSource source leaf_name new_name = f name leaf_name param_name_to_source new_name = new_source isinstance source LocalSource dynamo_flat_name_to_original_fqn OutputGraph module_key_name new_source name = leaf_name annoying there cases when we do have parameters see test_nn_moduledict_contains hasattr target _parameters leaf_name _ target named_parameters register_leaf_name leaf_name hasattr target _buffers leaf_name _ target named_buffers register_leaf_name leaf_name wrap_name name handle_aliases_for_stolen_lists tx InstructionTranslatorBase - tuple list Instruction dict Source Source If list inputs stolen still needed after function call create aliases keep them alive maybe_gm = local_scope get stolen_list_names = get_locals_to_steal maybe_gm stolen_list_names alias_insts = needs_alias dict str list VariableTracker = queue = tx stack tx symbolic_locals values side_effects store_attr_mutations keys while queue x = queue pop isinstance x BaseListVariable assert isinstance x items list queue += x items continue x side_effects store_attr_mutations isinstance x mutation_type AttributeMutationExisting isinstance x source GetItemSource isinstance x source base LocalSource x source base local_name stolen_list_names continue stolen_name = x source base local_name stolen_name needs_alias needs_alias stolen_name = needs_alias stolen_name append x visited = overridden_sources dict Source Source = arg graphargs isinstance arg _example list isinstance arg source LocalSource arg source local_name needs_alias continue arg list will cleared compiled function list_name = arg source local_name assert list_name code_options co_varnames x needs_alias list_name Skip already handled x source overridden_sources continue A small codegen optimization because we might have different VariableTrackers share same source assert x source None list_idx = x source index type ignore attr-defined list_idx visited alias_name = new_var f list_name _ref new_var already adds unique id suffix visited list_idx = alias_name bytecode ` alias_name = list_name list_idx ` alias_insts extend create_instruction LOAD_FAST argval=list_name create_load_const list_idx create_binary_subscr create_instruction STORE_FAST argval=alias_name operate alias handled suffix codegen assert x source None old_source = x source overridden_sources old_source = LocalSource visited list_idx NOTE we need ` overridden_sources ` because we want codegen these list items use new local source we want avoid updating ` source ` place because might break invariants other parts Dynamo like guards alias_insts overridden_sources _get_stack_values_to_restore tx InstructionTranslatorBase stack_pops int - tuple list VariableTracker StackLocalsMetadata Gets stack + locals values belonging tx need restored Also prunes dead tx locals realizes all VTs tx s stack NullVariables stack locals will NOT restored unless they top ` stack_pops ` elements stack - expected next instruction run will pop top ` stack_pops ` elements stack so we should codegen NULLs Returns - stack_values stack locals values need restored - meta locations NULLs ContextWrappingVariables stack locals ignores top ` stack_pops ` values stack tx prune_dead_locals stack_values = meta = StackLocalsMetadata realize any unrealized tensor VTs case they need added nn_modules attributes i value enumerate tx stack variables LazyVariableTracker realize_all value ignore top ` stack_pops ` values stack len tx stack - i = stack_pops stack_values append value continue isinstance value NullVariable meta stack_null_idxes append i stack_values append value isinstance value ContextWrappingVariable target_values = value target_values None tuple value target_values NOTE track index stack after NULLs have been removed meta stack_ctx_args append len stack_values - target_values meta stack_ctx_idxes_orig append i meta num_stack = len stack_values cell_and_freevars = set tx cellvars + tx freevars NB Typically i e graph compile RETURN_VALUE symbolic_locals will empty point prune_dead_locals will clear out all symbolic_locals because RETURN_VALUE last instruction no more locals used The fanciness here only needed partial graphs NOTE All cell free variables represented CellVariable so checks NULLs context managers case codegen ing resume functions will performed them This expected behavior k v tx symbolic_locals items Note explicitly uses local_name matching Failure do so will cause spurious registrations val_to_names This will turn result spurious variables showing up graph This very tricky debug For example dump graph call_user_compiler while running test_subgraphs py Do include top-frame unmodified locals here - otherwise compiled graph may erroneously include them part We manually codegen them afterward isinstance v source LocalSource v source local_name == k tx root_tx continue Do load cell free vars k cell_and_freevars continue Do load variable NULL sys version_info = NOTE do use isinstance since realizes lazy VT s Continuation function will load NULL v type __instancecheck__ NullVariable v meta locals_null_keys append k continue A variable should never NULL assert type __instancecheck__ NullVariable v meta locals_names k = len meta locals_names isinstance v ContextWrappingVariable target_values = v target_values None tuple v target_values meta locals_ctx_args append k target_values stack_values append v stack_values meta compile_subgraph tx InstructionTranslatorBase reason GraphCompileReason partial_convert bool = False stack_pops int = - list StackLocalsMetadata Compiles current subgraph inputs w r t root_tx codegens - Call compiled subgraph - Apply side effects - Codegen stack locals - Store locals Python does allow NULL arg function so we do codegen NULLs stack unless value one top ` stack_pops ` values stack these values expected popped immediately after generated code The prologue resume function expected restore any dropped NULLs Returns stack indices locals keys where we dropped NULLs where we found inactive context manager objects assert root_tx None config nested_graph_breaks expect only compile frame assert root_tx tx bytecode tracing has finished Pop context manager dynamo_timed mark_bytecode_tracing_stop partial_convert = partial_convert compile_subgraph_reason = reason should_exit = True log debug COMPILING GRAPH due s reason prefix instructions Python + prefix_insts list Instruction = sys version_info = inst root_tx prefix_insts inst opname == COPY_FREE_VARS prefix_insts append create_instruction COPY_FREE_VARS arg=len root_tx code_options co_freevars prefix_insts append copy copy inst stack values restore vars each frame pushed reverse order i e last element corresponds root frame first element corresponds current frame N all_stack_values = all_stack_locals_metas = cur_tx Optional InstructionTranslatorBase = tx while cur_tx None should have been checked caller assert all block can_restore block cur_tx block_stack stack_values meta = _get_stack_values_to_restore cur_tx stack_pops cur_tx tx all_stack_values append stack_values all_stack_locals_metas append meta Exit all context manager variables make sure global state restored block reversed cur_tx block_stack block exit cur_tx is_graph_break=reason graph_break cur_tx = cur_tx parent Garbage collect heap side_effects prune_dead_object_new tx add_output_instructions prefix_insts assert pregraph_bytecode export export does support pregraph_bytecode add_output_instructions pregraph_bytecode alias_insts overridden_sources = handle_aliases_for_stolen_lists root_tx add_output_instructions alias_insts cleanup_graph Use nn Module proxies constructed GraphModule so resulting GM does hold additional strong references original modules This prevents strong ref cycle where Dynamo created code holds references modules also have Dynamo code cache invalidation checks When cache invalidation runs generated GM will invalidated which also deletes proxies nn_modules_proxies = name nn_module_proxy mod name mod nn_modules items root = FakeRootModule nn_modules_proxies decorators disable has_user_objects NB This where we store possible user objects before running graph index_to_user_object_weakref function used graph translate dynamo-generated index into actual object passed compiled function We generate bytecode store all user objects proper index below call codegen = PyCodegen root_tx root overridden_sources=overridden_sources codegen add_push_null lambda codegen load_import_from torch _dynamo graph_bytecode_inputs __name__ store_user_object_weakrefs tmp_vars = constructor reversed index_to_bytecode_constructor values constructor codegen var_name = new_var keep alive any temp objects rest frame codegen store var_name tmp_vars append var_name var_name tmp_vars codegen append_output codegen create_load var_name codegen call_function len index_to_bytecode_constructor False codegen pop_top add_output_instructions codegen get_instructions handle random calls len random_calls random_calls_instructions = random_values_var = new_var random_values rand_fn = disable _get_gen_rand_values_fn random_calls reason= do trace into Dynamo rng recovery function rand_fn_name = install_global __gen_rand_values rand_fn codegen = PyCodegen root_tx root overridden_sources=overridden_sources random_calls_instructions extend codegen load_function_name rand_fn_name True random_calls_instructions extend create_call_function False random_calls_instructions append codegen create_store random_values_var add_output_instructions random_calls_instructions Codegen stack convention before unsupported instruction NOTE these comment blocks locals EXCLUDE free cell vars NOTE stack locals cells must codegen d BEFORE unsupported instruction since latter can arbitrarily mutate former frame N cells frame cells frame N locals frame N- stack + locals frame stack + locals frame N stack see symbolic_convert py codegen stack convention after unsupported instruction NOTE cells will loaded into continuation functions directly symbolic_convert determines order values codegen d stack stack_values_flat = val vals all_stack_values val vals stored_graph_output_var = False graph_output_var = None call compiled fx graph codegen all values - stack locals root_tx tx single frame stack_values_flat all isinstance v UnspecializedPythonVariable NumpyNdarrayVariable TensorWithTFOverrideVariable isinstance v SymNodeVariable v python_type float v stack_values_flat all isinstance x TensorVariable x stack_values_flat len set stack_values_flat == len stack_values_flat side_effects is_empty tx debug_locals backward_state all_stack_locals_metas - stack_null_idxes all_stack_locals_metas - locals_null_keys optimization generate better code common case codegen cells no side effects so no new cells created - no need call side_effects codegen_save_tempvars cell_cg = PyCodegen root_tx codegen_cells tx cell_cg add_output_instructions load reverse since UNPACK_SEQUENCE will reverse compile_and_call_fx_graph tx list reversed stack_values_flat root cell_cg get_instructions create_swap create_instruction UNPACK_SEQUENCE arg=len stack_values_flat function output will moved correct places below graph_output_var = new_var graph_out load stack values flat manner - we will codegen bytecode place them correctly according our convention above pass = PyCodegen root_tx root graph_output_var overridden_sources=overridden_sources codegen_suffix tx stack_values_flat pass Use ` pass uses ` selectively cache multi-user variables into temporary local source This speeds up loading VTs long chained source b avoids redundantly saving single-user VT into temporary local tempvars = type ignore var-annotated val count pass uses items If s already local source no need cache count istype val SyntheticLocalSource LocalSource tempvars val = None pass = PyCodegen root_tx root graph_output_var tempvars=tempvars overridden_sources=overridden_sources codegen_suffix tx stack_values_flat pass torch _dynamo config log_graph_in_out_metadata stack_values_flat len stack_values_flat == vt = stack_values_flat isinstance vt torch _dynamo variables NamedTupleVariable vt tuple_cls torch _dynamo functional_export ExportTracerOutput flat_returns = vt items out_spec = vt items assert isinstance flat_returns torch _dynamo variables ListVariable vt_to_graph_out_idx dict VariableTracker int = value pass graph_outputs values assert isinstance value torch _dynamo codegen GraphOutputEntry variable VariableTracker = value variable vt_to_graph_out_idx variable = value index idx vt enumerate flat_returns items vt vt_to_graph_out_idx export_metadata output_return_type idx = graph_out vt_to_graph_out_idx vt vt source None source = getattr vt source base None type ignore assignment source is_input export_metadata output_return_type idx = input vt source isinstance vt torch _dynamo variables ConstantVariable export_metadata output_return_type idx = constant vt as_python_constant assert f Encountered unrecognized type vt output idx noqa PLW export_metadata out_spec = out_spec as_python_constant output = count_calls graph = len pass graph_outputs = output extend compile_and_call_fx_graph tx pass graph_output_vars root len pass graph_outputs = output append pass create_store graph_output_var stored_graph_output_var = True output append create_instruction POP_TOP NB Important run compiler collective even when there graph break run_compiler_collective add_output_instructions output + pass get_instructions store all stack locals each frame current state stack all cells frame N stack frame N locals frame stack frame locals add_output_instructions create_instruction BUILD_LIST arg=len stack_values_flat - all_stack_locals_metas num_stack current state stack all cells frame N stack frame N locals frame N- stack frame N- locals frame stack frame locals iterate current frame N root frame sliding window over frame stack locals start_idx = end_idx = i meta enumerate all_stack_locals_metas do pack frame N s stack into value list n_vals = len meta locals_names i = n_vals += meta num_stack n_vals == add_output_instructions create_instruction BUILD_LIST arg= create_swap stack_values_flat end_idx += n_vals add_output_instructions create_dup_top create_binary_slice start_idx end_idx create_swap start_idx += n_vals stack_values_flat x y stack_values_flat add root frame s unmodified locals here i == len all_stack_locals_metas - root_cg = PyCodegen root_tx unmodified_locals_names dict str int = k v root_tx symbolic_locals items isinstance v source LocalSource v source local_name == k root_cg append_output root_cg create_load k unmodified_locals_names k = len meta locals_names + len unmodified_locals_names add_output_instructions root_cg get_instructions + create_instruction BUILD_LIST arg=len unmodified_locals_names arg= because we already swapped locals list back create_instruction LIST_EXTEND arg= meta locals_names update unmodified_locals_names frame N stack metas stack + locals metas i stack + locals stack_values_flat current state stack all cells frame N stack frame N locals frame N- stack frame N- locals frame stack frame locals stack_values_flat add_output_instructions create_instruction POP_TOP create_instruction BUILD_LIST arg=len all_stack_locals_metas create_rot_n all_stack_locals_metas num_stack + final state stack before running unsupported bytecode all cells frame N locals frame N- stack + locals frame stack + locals frame N stack graph_output_var stored_graph_output_var add_output_instructions create_instruction DELETE_FAST argval=graph_output_var export torch export _trace _ExportModuleSpecTrackerDict potential_side_effects = var side_effects _get_modified_vars hasattr var mutation_type mut_type = var mutation_type Make sure skip codegen specific mutations isinstance mut_type AttributeMutationExisting ValueMutationExisting isinstance var UserDefinedDictVariable isinstance var value _ExportModuleSpecTrackerDict k v var items items specs = k_spec val v items items specs k_spec vt as_python_constant = val as_python_constant assert in_spec out_spec == list specs keys export_metadata module_call_spec k vt as_python_constant = specs export uses tracepoint pass dump submodule inp out spec into global state so we filter here isinstance var UserDefinedDictVariable isinstance var value _ExportModuleSpecTrackerDict potential_side_effects append var side_effect_refs = _get_source_debug_name var source var potential_side_effects side_effect_refs warnings warn f While exporting we found certain side effects happened model forward f Here list potential sources you can double check side_effect_refs all_stack_locals_metas codegen_cells tx InstructionTranslatorBase cg PyCodegen - None no need codegen reason graph_break False since we won t resume compile_subgraph_reason graph_break tx_cnt = cur_tx Optional InstructionTranslatorBase = tx while cur_tx None NOTE we generate cells same order resume_execution py sorted freevars + cellvars Emitting ` LOAD_FAST LOAD_CLOSURE ` names ` co_freevars ` requires generated bytecode these cells would keep their original local names which we ensure via ` CellVariable local_name ` freevars = tuple sorted cur_tx cell_and_freevars cell freevars cur_tx root_tx root frame cg append_output cg create_load_closure cell nested frame assert cur_tx post_prune_cell_and_freevars cg cur_tx post_prune_cell_and_freevars cell cg append_output create_build_tuple len freevars cur_tx = cur_tx parent tx_cnt += cg append_output create_instruction BUILD_LIST arg=tx_cnt cg append_output create_instruction BUILD_LIST arg= codegen_suffix tx InstructionTranslatorBase stack_values list VariableTracker cg PyCodegen - None NOTE ` codegen_save_tempvars ` must run first update ` source ` fields variables ` AttributeMutationNew ` they don t implement ` reconstruct ` themselves side_effects codegen_save_tempvars cg backward_state assert export name val backward_state items cg val assert backward_state_var None cg append_output cg create_load backward_state_var cg store_attr name side_effects codegen_hooks cg TODO get debug_locals working nested graph breaks Return variables used logging end debug_var args tx debug_locals cg add_push_null lambda cg debug_var arg args cg arg cg extend_output create_call_function len args False cg extend_output create_instruction POP_TOP codegen cells before we apply side effects codegen_cells tx cg cg restore_stack stack_values value_from_source=not tx export side_effects codegen_update_mutated cg cleanup_graph - None Remove creation_timestamp node meta Remove pattern graph torch _C _set_grad_enabled False torch _C _set_grad_enabled True assert should_exit nodes = list graph nodes node nodes node meta pop creation_timestamp None grad_enabled = torch is_grad_enabled node node itertools pairwise nodes node target torch _C _set_grad_enabled tuple node args == grad_enabled node _erased grad_enabled = node args node target torch _C _set_grad_enabled tuple node args == grad_enabled node _erased grad_enabled = node args graph erase_node node graph erase_node node bypass_package reason str = kwargs Any - None Do save output graph CompilePackage package torch _dynamo config strict_precompile raise torch _dynamo exc PackageError Detected package bypass s reason log warning Detected package bypass s reason torch _logging trace_structured artifact metadata_fn=lambda name precompile_cache_bypass encoding json payload_fn=lambda precede underscore so always appear first JSON tlparse _reason reason kwargs package bypass_current_entry package = None get_graph_sizes_structured - dict str list Union int str ret dict str list Union int str = node graph nodes example_value = node meta get example_value None isinstance example_value torch _subclasses FakeTensor size = example_value size ret node name = s isinstance s int repr s s size ret get_graph_sizes name str - str graph_sizes_str = TRACED GRAPH TENSOR SIZES\n graph_sizes_str += f ===== name =====\n node graph nodes example_value = node meta get example_value None isinstance example_value torch _subclasses FakeTensor size = example_value size graph_sizes_str += f node name tuple size \n concrete_size = has_symint = False sz size isinstance sz int concrete_size append sz isinstance sz torch SymInt has_symint = True concrete_size append sz node hint break has_symint graph_sizes_str += f node name concrete tuple concrete_size \n graph_sizes_str contextlib contextmanager restore_global_state - Any Momentarily restores global state what prior tracing current output prior_global_state = tracing_context global_context copy_graphstate current_global_state dict str tuple Any bool = save_global_state out=current_global_state try Set state prior tracing graph tracing_context global_context restore_graphstate prior_global_state yield finally Reset state current time e g before calling user compiler tracing_context global_context restore_graphstate GlobalContextCheckpointState current_global_state run_compiler_collective - None tx = root_tx assert tx None ds = tx distributed_state None ds all_states None compile_pg = ds compile_pg log info compiler_collective s ds local_state torch _logging trace_structured artifact metadata_fn=lambda name compiler_collective encoding string payload_fn=lambda ds local_state render device_types = compile_pg _device_types assert len device_types == Expect only one device type got format + join device_types get_interface_for_device device_types pop device type ignore attr-defined compile_pg rank torch accelerator device_count dynamo_timed compiler_collective log_pt _compile_event=True all_states list Any = None compile_pg size dist all_gather_object all_states ds local_state group=compile_pg ds all_states = all_states Clear speculation log because tracing may diverge due information compiler collective tx speculation_log clear raise exc CompileCollectiveRestartAnalysis compile_and_call_fx_graph tx InstructionTranslatorBase rv list VariableTracker root FakeRootModule - list Instruction Generate code graph Instruction s call generated code Code generated w r t root_tx tx only used preserving GraphModule metadata torch _guards TracingContext clear_frame decorators disable assert should_exit run_compiler_collective count_calls graph == len rv == name = unique_id __compiled_fn with_uuid=True assert isinstance rv list assert isinstance root FakeRootModule output_node = create_node output output current_tracer create_arg tuple x as_proxy x rv sub_gms = dedup_pass root add_nn_modules sub_gms type ignore arg-type current_tracer _maybe_preserve_original_meta tx output_node config do_not_emit_runtime_asserts There rare scenario where codegen_suffix adds new entry nn_modules while ` root ` knows only about nn_modules time its creation This causes failures while creating graph module because graph root out sync This only happens ` get_attr ` nodes so here we clean up get_attr nodes unused attr dir root subgraph = getattr root attr isinstance subgraph fx GraphModule insert_deferred_runtime_asserts subgraph shape_env name export=self export remove_unused_get_attr_nodes insert_deferred_runtime_asserts fx GraphModule root graph shape_env name export=self export NB deferred runtime asserts can keep graphargs live so make sure those inserted before pruning remove_unused_graphargs ncalls = count_calls graph counters stats calls_captured += ncalls remove_tensorify_specialized_graphargs free bit memory real_value_cache clear gm = _make_graph_module root graph Saved tensors hooks used graph GraphModule default only copies used graph submodules Copying them into result graph manually saved_tensors_hooks_subgraph_names subgraph_name saved_tensors_hooks_subgraph_names setattr gm subgraph_name getattr root subgraph_name register_finalizer register_finalizer_fns register_finalizer gm next gm parameters None None If dynamo produces graph parameters skip package stuff Bypass output graph bypass_package Graph contains named parameters either inline_inbuilt_nn_modules=False there static addresses inline_builtin_nn_modules=torch _dynamo config inline_inbuilt_nn_modules gm=gm print_readable print_output=False include_stride=True include_device=True package None gm _backend_id = name gm compile_subgraph_reason = compile_subgraph_reason gm meta dynamo_flat_name_to_original_fqn = dynamo_flat_name_to_original_fqn copy gm meta dynamo_compile_id = dynamo_compile_id gm meta backend_id = name graph_code_log debug s lazy_format_graph_code name gm include_stride=True include_device=True colored=True torch _logging trace_structured dynamo_output_graph lambda sizes get_graph_sizes_structured payload_fn=lambda gm print_readable print_output=False include_stride=True include_device=True call_cleanup_hooks old_fake_mode = tracing_context fake_mode assert old_fake_mode None export torch _functorch config _config _config patch fake_tensor_allow_unsafe_data_ptr_access=False TODO voz The way export uses gm fake tensors supported us resetting Why create new FakeTensorMode The reason needs done because when we do Dynamo tracing fake tensors can have their metadata mutated Thus fake tensor we allocated any given tensor may no longer valid beginning trace graph Nor convenient clone input tensors before mutating them since you have preserve aliasing So we just reconstruct FakeTensorMode scratch when we go AOTAutograd But ShapeEnv must preserved Dynamo made decisions about what dynamic guards user code graph backend_fake_mode = torch _subclasses FakeTensorMode shape_env=old_fake_mode shape_env TODO voz Ostensibly should scoped restore back old_fake_mode doing so currently violates lot fake_tensor ownership assumptions runs afoul detect_fake_mode tracing_context fake_mode = backend_fake_mode restore_global_state compiled_fn = call_user_compiler gm example_inputs torch fx _lazy_graph_module _LazyGraphModule isinstance compiled_fn _LazyGraphModule isinstance getattr compiled_fn __self__ None _LazyGraphModule compiled_fn __name__ == _lazy_forward type ignore attr-defined Since dynamo will run forward method GraphModule shortly anyways does hurt do real recompilation here _LazyGraphModule This makes easier dynamo optimize _LazyGraphModule lazy_gm = compiled_fn isinstance compiled_fn _LazyGraphModule compiled_fn __self__ type ignore attr-defined _LazyGraphModule force_recompile lazy_gm isinstance compiled_fn _LazyGraphModule replace compiled_fn real forward method compiled_fn = lazy_gm forward package None package add_backend_id name compiled_fn compiled_fn = disable compiled_fn reason= do trace Dynamo-compiled graph counters stats unique_graphs += assert old_fake_mode shape_env None specializations = old_fake_mode shape_env specializations specialization_guards = specialization_cache dict Specialization Callable Any Any = sources = source graphargs specialization specializations source_index = sources index specialization source check_fn_source = inspect getsource specialization check_fn strip Required because LABDA_GUARD API requires root guard manager unused_root_guard_manager = RootGuardManager check_fn = guards LAMBDA_GUARD type ignore attr-defined unused_root_guard_manager specialization check_fn check_fn_source log debug Compiling backend specialized graph specialization= s check_fn_source specialization_guards append functools partial lambda idx args check_fn=check_fn check_fn args idx source_index specialization torch _dynamo disable reason= do trace Dynamo-compiled graph type ignore misc specialized_dispatch args Any kwargs Any - Any check_fn specialization specialization_guards check_fn args specialization specialization_cache specialization_cache specialization args kwargs shape_env patch_source_specialization specialization source specialization check_fn Modify gm so AOTAutogradCache key changes per specialization gm meta specialization = specialization example_inputs list Tensor = list args tracing tracing_context specialization_cache specialization = call_user_compiler gm example_inputs specialization_cache specialization args kwargs compiled_fn args kwargs This safe because we pre-process name unique install_global_unsafe name specialized_dispatch This safe because we pre-process name unique install_global_unsafe name compiled_fn assert root_tx None cg = PyCodegen root_tx idx arg enumerate graphargs export_metadata graph_input_idx_to_local_source idx = arg source cg make_call_generated_code name cg get_instructions property placeholders - list fx Node graph find_nodes op= placeholder property graphargs - list GraphArg node meta grapharg node placeholders call_user_compiler gm fx GraphModule example_inputs list Tensor - CompiledFn dynamo_timed OutputGraph call_user_compiler phase_name= backend_compile log_pt _compile_event=True log_waitcounter=True waitcounter_name_override= compile_aot_autograd dynamo_compile_column_us= aot_autograd_cumulative_compile_time_us _call_user_compiler gm example_inputs _call_user_compiler gm fx GraphModule example_inputs list Tensor - CompiledFn assert compiler_fn None tot = placeholders = node gm graph nodes node op call_function call_method call_module tot += node op == placeholder placeholders append node increment_op_count tot pl placeholders hasattr pl _dynamo_source arg = pl meta grapharg TODO Why isn t stored meta think NOTE can t move these into meta https github com pytorch pytorch issues pl _dynamo_source = arg source NOTE can t move these into meta https github com pytorch pytorch issues gm _param_name_to_source = param_name_to_source type ignore assignment gm _source_to_user_stacks = source_to_user_stacks type ignore assignment name = compiler_fn __name__ hasattr compiler_fn __name__ unknown compiler_fn try _step_logger logging INFO f calling compiler function name compiler_fn = compiler_fn config verify_correctness compiler_fn = WrapperBackend compiler_fn compiled_fn = compiler_fn gm example_inputs _step_logger logging INFO f done compiler function name assert callable compiled_fn compiler_fn did callable except TensorifyScalarRestartAnalysis ShortenTraceback raise except exceptions_allowed_to_be_fallback e has_user_defined_allowed_in_graph raise BackendCompilerFailed compiler_fn e inspect currentframe with_traceback e __traceback__ None unimplemented_v _with_warning e root_tx f_code gb_type= Backend compiler exception context=f Backend name \nException str e \nTraceback \n root_tx format_frame_summary explanation=f Backend compiler ` name ` failed str e Adding graph break hints= Report issue backend compiler repo except SkipFrame e The backend compiler has requested we skip frame instead aborting execution raise e except Exception e raise BackendCompilerFailed compiler_fn e inspect currentframe with_traceback e __traceback__ None signpost_event dynamo OutputGraph call_user_compiler co_fields op_count tot node_count len gm graph nodes input_count len placeholders pyrefly ignore unbound-name compiled_fn dedup_pass - dict str torch fx GraphModule torch _dynamo config use_graph_deduplication apply_graph_deduplication install_subgraph name str sub_gm torch fx GraphModule - str next_name = get_unique_name_wrt name nn_modules requires_suffix=True sub_gm __name__ = next_name type ignore assignment sub_gm torchdynamo_force_dynamic = False type ignore assignment This graph module present user space so can t accessed source Set source=None register_attr_or_module sub_gm next_name source=None next_name example_inputs - list torch Tensor result = arg example arg graphargs result remove_unused_get_attr_nodes - None node sorted graph find_nodes op= get_attr reverse=True len list node users == remove_node node remove_unused_graphargs - None NB It s OK drop GraphArg symbols ended up being specialized iff they used runtime assertions You don t even have make guard because ShapeEnv produce_guards operates tracked_fakes which never gets pruned That being said you ll get marginally better generated guard code you promote guard into Dynamo guard since allows guard done using C++ guards If we get ShapeEnv guards go into C++ guards will stop being thing though assert should_exit Miniature DCE pass only obviously trivial operations is_static_true b_node fx node Argument - bool b_node True True isinstance b_node fx Node False b = b_node meta get example_value b None False b True True isinstance b torch SymBool r = b node maybe_as_bool None r TODO We can also technically remove all cases when input doesn t have unbacked inputs since s all ShapeEnv False is_symnode_arg fx node Argument - bool torch fx experimental sym_node SymTypes isinstance int float bool True isinstance fx Node isinstance meta get example_value SymTypes False NB We assume you cannot do mutations int float bool because they immutable types therefore always safe DCE is_symnode_compute_node node fx Node - bool torch fx experimental sym_node SymTypes node op = call_function False TODO I don t think s possible have bare int float here isinstance node meta get example_value SymTypes False TODO This will bail here you ever end up more complicated computation function like sum list_of_ints even though should DCE able all is_symnode_arg node args False all is_symnode_arg node kwargs values False True torch fx experimental symbolic_shapes is_accessor_node node reversed list graph nodes len list node users == node op == get_attr node op == call_function node target operator getitem node op == call_function node target torch _check is_static_true node args is_symnode_compute_node node is_accessor_node node remove_node node placeholder_binds_symbol node fx Node - Optional sympy Symbol arg = node meta grapharg example = arg example isinstance example torch SymInt isinstance example node expr sympy Symbol example node expr None remove_unused node fx Node - None log debug REMOVE UNUSED GRAPHARG s node meta grapharg source name I m really sure why you need delete these node since node going get removed del node meta grapharg remove_node node real_value_cache pop node None used_symbols set sympy Symbol = set update_used_symbols used_symbols set sympy Symbol fake Union torch SymInt torch Tensor - None used_symbols &#124; = free_symbols fake recheck_placeholders = node placeholders binds_symbol = placeholder_binds_symbol node None Don t delete symbol bindings yet binds_symbol node users recheck_placeholders append node node users isinstance node meta grapharg BackwardStateGraphArg remove_unused node Register free symbols uses arg = node meta grapharg isinstance arg BackwardStateGraphArg continue isinstance node meta grapharg example torch ScriptObject real_script_obj = node meta grapharg example fake_script_obj = node meta grapharg example_strong_ref torch _library fake_class_registry tracing_with_real real_script_obj flat_dict = dict real_script_obj __obj_flatten__ type ignore attr-defined attr flat_dict keys fake_attr_val = getattr fake_script_obj wrapped_obj attr pytree tree_map_only torch SymInt torch Tensor lambda t update_used_symbols used_symbols t fake_attr_val continue fake = arg fake_tensor arg fake_tensor None arg example update_used_symbols used_symbols fake After removing unused graphargs prune unused binds_symbol node recheck_placeholders symbol = placeholder_binds_symbol node symbol None symbol used_symbols remove_unused node Make sure we delete later occurrences same symbol used_symbols remove symbol remove_tensorify_specialized_graphargs - None This pretty interesting function Basically we have problem where our compiler tends choke when we have unused inputs The way we support dynamic float arguments doing joint fx pass tensorifying away many symfloats we can For remaining symfloats we have no choice specialize HOWEVER point time we can no longer remove graph inputs So our sledgehammer solution save state what inputs we should have specialized dynamo restart analysis This function incorporates view future state specializes inputs we know we won t able tensorify away joint pass In principle we shouldn t choke unused inputs so shouldn t necessary In practice CUDA graphs choke unused inputs so we need now Import here prevent circular torch _dynamo symbolic_convert TensorifyState node graph nodes example_value = node meta get example_value isinstance example_value FakeTensor example_value item_memo None hasattr example_value item_memo node _expr name all u target == item u node users TensorifyState should_specialize We use _expr instead expr b c we want symbol replacement example_value item_memo node _expr name u list node users u replace_all_uses_with guard_scalar example_value item_memo remove_node u remove_node node add_output_instructions prefix list Instruction - None We call creation new compiled subgraph inserted before user code output_instructions extend prefix should_exit = True install_global_unsafe name str value Any - None WARNING prefer safer ` install_global_by_id install_global ` torch compile instances should independent each other one footgun have one instance depend existence global installed another instance This can happen we mangle global same way across both instances assert name installed_globals installed_globals add name cleanups append CleanupHook create global_scope name value install_global_by_id prefix str value Any - str Installs global hasn t been installed already This determined prefix id value pair Returns name newly installed global NB need compile_id distinguish global another global created different torch compile instance name = f prefix _ id value _c compile_id name installed_globals name install_global_unsafe name value name install_global prefix str value Any - str Installs global generating unique name Returns name newly installed global NB unique_id unique even across torch compile instances name = unique_id prefix install_global_unsafe name value name cleanup - None There reference cycle between tracer OutputGraph causing some tensor objects held alive longer than necessary root_tx = None type ignore assignment nn_modules clear used_inlined_inbuilt_modules_names clear param_name_to_source = None node graph nodes grapharg node meta del node meta grapharg real_value_cache clear input_name_to_proxy clear side_effects clear variable_tracker_cache clear register_finalizer_fns clear dynamo_flat_name_to_original_fqn clear tracing_context clear input_source_to_var clear unspec_variable_map clear backward_state clear add_graph_finalizer register_finalizer Callable fx GraphModule None - None register_finalizer_fns append register_finalizer example_value_from_input_node node torch fx Node - Any Extract non-fake example tensor node op == placeholder node meta grapharg example assert node op == get_attr nn_modules node target type ignore index add_fqn_info_for_inlined_modules inlined_module torch nn Module source Source - None name = OutputGraph module_key_name source name name = get_unique_name_wrt name used_inlined_inbuilt_modules_names global_scope used_inlined_inbuilt_modules_names add name register_leaf_name leaf_name str - None assert param_name_to_source None new_source = ParamBufferSource source leaf_name new_name = f name leaf_name param_name_to_source new_name = new_source isinstance source LocalSource dynamo_flat_name_to_original_fqn OutputGraph module_key_name new_source name = leaf_name annoying there cases when we do have parameters see test_nn_moduledict_contains hasattr inlined_module _parameters callable inlined_module named_parameters inlined_module named_parameters __func__ type ignore attr-defined og_module_named_parameters_fn_ptr leaf_name _ inlined_module named_parameters register_leaf_name leaf_name hasattr inlined_module _buffers callable inlined_module named_buffers inlined_module named_buffers __func__ type ignore attr-defined og_module_named_buffers_fn_ptr leaf_name _ inlined_module named_buffers register_leaf_name leaf_name DynamoTracerOutput error_on_graph_break bool is_tracing_resume_prologue bool output_graph Optional OutputGraph closure Optional tuple Any __init__ tracer InstructionTranslatorBase error Optional Any = None - None error_on_graph_break = tracer error_on_graph_break is_tracing_resume_prologue = tracer is_tracing_resume_prologue closure = tracer closure error output_graph = None output_graph = tracer output err_epilogue = With current config we will graph break fall back eager-mode PyTorch all ops have do have pt _compliant_tag Please see following doc how mark op PT compliant https pytorch org tutorials advanced custom_ops_landing_page html check_pt _compliant_op output_graph OutputGraph kind str target Any args Any kwargs Any - None kind = call_function encountered_compliant_op target torch _ops OpOverload - None target namespace prim prims aten output_graph compliant_custom_ops add target encountered_non_compliant_op target torch _ops OpOverload msg str - None output_graph non_compliant_ops add target config only_allow_pt _compliant_ops unimplemented_v gb_type= Encountered non-PT -compliant op context= explanation=msg + + err_epilogue hints= isinstance target torch _ops OpOverload torch Tag pt _compliant_tag target tags encountered_compliant_op target encountered_non_compliant_op target f Encountered torch ops OpOverload target PT compliant isinstance target torch _ops OpOverloadPacket overloads = tuple target overloads Optimization Overload resolution expensive If there s only one overload we know what will resolve len overloads == op = getattr target overloads torch Tag pt _compliant_tag op tags encountered_compliant_op op encountered_non_compliant_op op f Encountered non-overloaded f torch ops OpOverloadPacket target f PT compliant args kwargs = torch _dynamo utils get_fake_values_from_nodes output_graph current_tx args kwargs False try overload = torch _C _jit_resolve_packet target _qualified_op_name args kwargs except RuntimeError e unimplemented_v gb_type= Error when attempting resolve op packet context= explanation=str e hints= pyrefly ignore unbound-name op = getattr target overload torch Tag pt _compliant_tag op tags encountered_compliant_op op encountered_non_compliant_op op f Encountered torch ops OpOverloadPacket target pyrefly ignore unbound-name f which resolves overload overload f PT compliant _compile_id_counter = itertools count P = ParamSpec P R = TypeVar R LazyProxy __init__ tracer SubgraphTracer fn Callable P R args P args kwargs P kwargs - None tracer = tracer pyrefly ignore invalid-type-var fn = fn args = args kwargs = kwargs __call__ - Any fn args kwargs SubgraphTracer fx Tracer Holds FX graph being traced OutputGraph owns SubgraphTracer separation responsibilities SubgraphTracer responsible building graph while OutputGraph responsible compiling executing graph __init__ output_graph OutputGraph parent Optional SubgraphTracer = None is_export bool = False source_target Optional Target = None - None super __init__ output_graph = weakref proxy output_graph graph = torch fx Graph See note Export inputs must explicitly passed is_export = is_export Map graph input name its placeholder proxy object where map s keys give all current placeholder node names can used create unique node names input_name_to_proxy dict str fx Proxy = Node = computed real value see utils get_real_value real_value_cache dict fx Node torch Tensor = SubgraphTracers can nested See NOTE HigherOrderOperator tracing design parent = parent source_target = source_target A dict mapping previously free variables Proxy objects new Proxy objects wrap inputs subgraph This dict maps proxies outer graphs placeholders current graph It serves two purposes - Proxies associated VariableTrackers If we see same VariableTracker twice free variable then we want use same Proxy current subgraph record tracing - If we tracing HigherOrderOperator s body_fn then we need keep track what free variables lifted so we can rewrite HigherOrderOperator call using traced body_fn Dicts maintain order args HigherOrderOperator call lifted_freevars dict fx Proxy fx Proxy = map basic symbols unbacked unbacked their bound proxies There only two cases where bound_symbols will recorded when we create_graph_input backed SymInt s basic symbol when we track_produced_symints intermediate results bound_symbols always map symbol proxy whose tracer current tracer s readily accessible current tracer s graph bound_symbols dict sympy Symbol Union torch fx Proxy LazyProxy = Maps _DynamicScalar object ids allocated SymInt nodes symbol reuse dynamic_scalar_nodes dict int torch SymInt = prev_inst = None True tracer currently tracing into torch utils checkpoint part speculate_subgraph under_activation_checkpoint = False True we want allow externally visible side-effects doesn t throw error their existence during tracer s tracing torch utils checkpoint via speculate_subgraph Only safe we know sure NOT replaying these side-effects during backward recomputation checkpoint region doesn t affect its correctness allow_side_effects_under_checkpoint = False True we want allow externally visible side-effects doesn t throw error their existence during tracer s tracing This currently only used experimental AC out-of-tree via torch _dynamo utils _disable_side_effect_safety_checks_for_current_subtracer Note Externally visible side-effects allowed flag OR above flag True unsafe_allow_externally_visible_side_effects = False True tracer currently tracing reconstructing into Python generator is_reconstructing_generator = False debug_level int = parent debug_level + parent None _cur_code = None _orig_gm_meta Optional list Any = None _orig_gm_lineno_map Optional dict int Optional int = None _orig_gm_firstlineno Optional int = None Each SubgraphTracer associated source target which indicates which operator subgraph attached We compute source_fn_stack based source target For root tracer s set This useful debugging transforming exported graph parent None source_fn_stack list Any = source_fn_stack = parent source_fn_stack + graph _target_to_str source_target source_target This used create unique name placeholder _used_names OrderedSet str = OrderedSet Stores versions input tensors time they inserted placeholders graph This used track input mutation _input_versions_at_beginning list int = torch is_inference_mode_enabled raise RuntimeError Inference mode supposed disabled during compilation Please open issue preserve original meta available _maybe_preserve_original_meta tx InstructionTranslatorBase node fx Node - None _orig_gm_meta _orig_gm_lineno_map _orig_gm_firstlineno lineno = tx current_instruction starts_line node_idx = None lineno None node_idx = _orig_gm_lineno_map get lineno - _orig_gm_firstlineno None node_idx None meta = _orig_gm_meta node_idx field fx proxy _COPY_META_FIELDS field meta node meta field = meta field stack_trace meta node meta stack_trace = meta stack_trace create_proxy kind str target Any args Any kwargs Any name Optional str = None type_expr Optional Any = None proxy_factory_fn Optional Callable fx Node fx Proxy = None - fx Proxy NOTE Nested SubgraphTracer free_variable handling -------------------------------------------------------- Read NOTE HigherOrderOperator tracing design first Let s say we re middle introspecting body possibly nested HigherOrderOperator we see free variable There two cases We see free variable already tracked Dynamo We see free variable has been tracked Dynamo In case we call ` maybe_lift_tracked_freevar_to_input ` below which will lift freevar input subgraph also recursively lift input parent s In case before call ` create_proxy ` InstructionTranslator will see freevar when gets loaded Python bytecode E g Python bytecodes may do LOAD_DEREF LOAD_GLOBAL There InstructionTranslator asks Dynamo begin tracking freevar building new Variable Building new Variable automatically lifts freevar input root SubgraphTracer The implications code below - We will always Case when we get code - Any free variable we encounter here guaranteed already bound either graph input root graph some local variable root graph subgraph - The additional work we need do here only we need lift free variable into inputs recursively each nested higher-order-op subgraph until we hit subgraph where free variable bound parent None flat_args tree_spec = pytree tree_flatten args kwargs new_flat_args = arg flat_args maybe_new_arg = maybe_lift_tracked_freevar_to_input arg new_flat_args append maybe_new_arg args kwargs = pytree tree_unflatten new_flat_args tree_spec rv = super create_proxy kind target args kwargs name type_expr proxy_factory_fn type ignore arg-type append stack trace fx node tx = output_graph current_tx log detailed location line code sys version_info = kind call_function call_method call_module cur_inst = tx current_instruction cur_inst prev_inst cur_inst positions None cur_inst positions lineno None tx_code = tx f_code header = tx get_line_of_code_header lineno=cur_inst positions lineno get_trace_call_log_str - str line = get_instruction_source_ tx_code cur_inst rstrip f TRACE FX call rv node name header \n line trace_call_log debug s LazyString get_trace_call_log_str prev_inst = cur_inst update reference original meta we re tracing new code object is_retracing = False tx f_code _cur_code orig_graphmodule_maybe = code_context get_context tx f_code get orig_graphmodule lambda None isinstance orig_graphmodule_maybe torch fx GraphModule is_retracing = True _orig_gm_meta = nd meta nd orig_graphmodule_maybe graph nodes _orig_gm_lineno_map = orig_graphmodule_maybe _lineno_map _orig_gm_firstlineno = orig_graphmodule_maybe forward __code__ co_firstlineno _orig_gm_meta = None _orig_gm_lineno_map = None _orig_gm_firstlineno = None nn_module_stack = tx nn_module_stack nn_module_stack rv node meta nn_module_stack = nn_module_stack copy kind call_function call_method stack = rv node name target nn_module_stack Current codebase assumes nn_module_stack has builtin modules stack current_nn_module = list rv node meta nn_module_stack values - current_nn_module __module__ startswith torch nn modules torch ao current_nn_module __module__ startswith torch nn modules container stack = rv node name current_nn_module rv node meta source_fn_stack = source_fn_stack + stack kind == call_module parent None TODO can remove once inline_inbuilt_nn_modules always True unimplemented_v gb_type= Invoking nn Module inside higher order operator context=f Higher order op name source_target explanation= This supported hints= For modules we store rv node meta source_fn_stack = source_fn_stack + rv node name next ty k _ ty rv node meta nn_module_stack items k split == target _maybe_preserve_original_meta tx rv node is_retracing nn_module_stack rv node meta nn_module_stack = tx nn_module_stack nn_module_stack rv node meta nn_module_stack = nn_module_stack copy source_fn_stack rv node meta kind call_function call_method rv node meta source_fn_stack = source_fn_stack + rv node name target kind == call_module parent None TODO can remove once inline_inbuilt_nn_modules always True unimplemented_v gb_type= Invoking nn Module inside HigherOrderOperator context= explanation= This supported hints= For modules we store rv node meta source_fn_stack = source_fn_stack + rv node name rv node meta nn_module_stack target stack_trace rv node meta frame_summaries list traceback FrameSummary = while tx Avoid frame summaries inside torch nn modules This ensures we keep stack trace user code tx is_co_filename_from_nn_modules frame_summaries append tx frame_summary tx = getattr tx parent None filtered_frame_summaries = frame frame frame_summaries frame filename uninteresting_files Reverse frame_summaries such innermost frame last filtered_frame_summaries reverse official from_list stub doesn t have new-style type msgs = traceback StackSummary from_list filtered_frame_summaries format rv node stack_trace = join msgs torch _dynamo config use_graph_deduplication torch _dynamo config track_nodes_for_deduplication output_graph region_tracker track_node output_graph current_tx rv node rv create_node op str target Target args Any = None kwargs Any = None name Optional str = None type_expr Optional Any = None - fx Node check_pt _compliant_op output_graph op target args kwargs parent None flat_args = pytree arg_tree_leaves args kwargs arg flat_args isinstance arg torch fx Node continue assert arg graph == graph create_node using arg SubgraphTracer node = super create_node op target args kwargs name type_expr node meta creation_timestamp = output_graph timestamp _used_names add node name node Note we did override erase_node since we call graph erase_node elsewhere remove_node node fx Node - None len node users user_graph_nodes list torch fx Node = user node users keys For case where user graph == graph real bug will raise properly user graph = graph This nested graph which needs deleted If we do do we will raise attempting remove As we only get here during restoration cleanup sound user_graph_nodes extend reversed list user graph nodes other_graph_node user_graph_nodes other_graph_node graph erase_node other_graph_node graph erase_node node input_name_to_proxy pop node name None when before=True we will insert input before most recent inserted proxy This hack get around ordering problem where we first insert tensor argument then insert bindings SymInts may occur tensor argument Remove https github com pytorch pytorch issues gets fixed create_graph_input name str type_expr Any example_value Any before bool = False source Optional Source = None - fx Proxy isinstance example_value torch Tensor _input_versions_at_beginning append example_value _version log debug create_graph_input s s s debug_level s before= s name source name source None none example_value debug_level before source None assert parent None f you required provide source inputs name example_val example_value root tracer Note Export inputs must explicitly passed In eager we generally OK adding graph inputs whenever we want because we take care writing bytecode knows how source all inputs In export bad because you want self-contained export object which only depends inputs you explicitly passed So we bit more strict about what sources can become inputs export is_export parent None assert source None is_from_local_source source only_allow_input=True output_graph source_to_user_stacks setdefault source append TracingContext extract_stack _used_names contains names all nodes graph including intermediates This ensures we do have name collision name = get_unique_name_wrt name _used_names input_name_to_proxy prev_name = next reversed input_name_to_proxy node = input_name_to_proxy prev_name node before ctx = graph inserting_before node ctx = graph inserting_after node ctx = graph inserting_before None ctx proxy = create_proxy placeholder name type_expr=type_expr set_example_value proxy node example_value input_name_to_proxy before k v = input_name_to_proxy popitem input_name_to_proxy name = proxy input_name_to_proxy k = v input_name_to_proxy name = proxy For placeholder nodes ` name ` passed str target then torch fx decides node name So record ` target ` name well _used_names prevent any collision _used_names add name NOTE Auto lift basic free symbols when create_graph_input There two sources basic symbols - They can come inputs e g when input tensor specified dynamic We handle case intercepting create_graph_input Whenever we call create_graph_input we try also lift basic symbols example values graph input When create_graph_input tensor has symbolic shapes we look basic symbols its size stride we check symbol bound current graph i e bound_symbols s bound we ll create placeholder then recursively check its parent creates ph bound parent until reachting top-level where we require source attached proxy When create_graph_input tensor contains compound exprs example input subgraph takes size s +s we ll look free basic symbols sizes lift all them following When create_graph_input symint The following invariants hold symint s expr basic symbol we only lift once b symint s expr compuned we lift expr single input We won t lift The basic symbols compuned expr NOT lifted Because basic symbols used inside subgraph they will lifted according - They can come intermediate results For example data-dependent operators such t item t nonzero where basic symbols might created For purpose we track basic symbols intermediate results immediately after they re created wrap_fx_proxy track_produced_symints Notice basic symbols re already tracked create_graph_input we won t track again Also see NOTE Export inputs must explicitly passed is_strict_export = is_export is_non_strict_export = torch compiler is_compiling is_strict_export is_non_strict_export isinstance example_value torch Tensor _lift_basic_symbols example_value source isinstance example_value list tuple i e enumerate example_value isinstance e torch Tensor continue e_source = None source e_source = GetItemSource base=source index=i index_is_slice=False _lift_basic_symbols e e_source Bound symbol ph example_value SymInt basic symbol isinstance example_value torch SymInt isinstance example_value node expr sympy Symbol bound_symbols example_value node expr = proxy proxy See NOTE Nested SubgraphTracer free_variable handling more details lift_tracked_freevar_to_input proxy fx Proxy - Union LazyProxy fx Proxy You re doing something wrong we root SubgraphTracer because Dynamo adds tensors graph inputs before creating proxy them assert parent None lift_tracked_freevar_to_input should called root SubgraphTracer example_value = proxy node meta example_value To avoid lifting same symbol twice we check whether basic symbols has been tracked For example basic symbols may have already been lifted current subgraph when we automatically lift basic symbols sizes strides tensor t Suppose parent graph calls sz = t size creates proxy parent subgraph accesses sz via closure sz s proxy tracked current sub-tracer so we may lift same symbol twice isinstance example_value torch SymInt example_value node expr bound_symbols bound_symbols example_value node expr Proxies associated VariableTracker It possible we ve already lifted Proxy input If case just already lifted Proxy proxy lifted_freevars lifted_freevars proxy We first lift proxy parent s graph then lift current graph s input so when we bind symints sizes current graph those symints would already lifted inputs parent graph proxy tracer = parent parent lift_tracked_freevar_to_input proxy example_value = proxy node meta example_value new_proxy = create_graph_input proxy node name type example_value example_value lifted_freevars proxy = new_proxy new_proxy maybe_lift_tracked_freevar_to_input arg Any - Any If arg free variable then lift input Returns new lifted arg arg freevar original arg isinstance arg torch fx Proxy Note arg can python built-in slice type e g x max_seq represented get_item t slice None max_seq None we need also look into slice variable itself lift proxies there isinstance arg slice slice maybe_lift_tracked_freevar_to_input sub_arg sub_arg arg start arg stop arg step arg arg tracer == arg lift_tracked_freevar_to_input arg See NOTE Auto lift basic free symbols when create_graph_input overall design You MUST call API every time when creating proxy wrap_fx_proxy call produced symints tensors unbacked symint shapes This function used track symints its proxies created during dynamo tracing so subgraph knows how bind symbol input parent s proxy LazyProxy created tensor shapes re unbacked so we don t create proxies symbols re going used LazyProxy will turned into proxy when s lifted input subgraph track_produced_symints example_value Any e_proxy Union LazyProxy torch fx Proxy - None When binding symbols example_value we bind symbols proxy s associated Tracer instead current tracer This because We may calling wrap_tensors during speculate_subgraph because variables lazily realized The proxy top-level phs current tracer subtracer For autograd Function we trace backward graph new tracer whose parent forward tracer we re using all proxies created forward tracer trace backward For example forward calls save_for_backward input tensor t Backward calls t tolist In case all proxies backward tracer sees parent tracer i e forward tracer e g t item See test_validate_outputs_unbacked repro tracer = e_proxy tracer assert isinstance tracer SubgraphTracer need_bind s Any - bool torch fx experimental symbolic_shapes is_symbolic is_symbolic s isinstance s node expr sympy Symbol s node expr bound_symbols _proxy_with_example_value example_value Any args Any kwargs Any - fx Proxy We need insert proxy creating sym_size sym_stride sym_storage right after e_proxy nonlocal e_proxy e_proxy = e_proxy isinstance e_proxy LazyProxy e_proxy assert isinstance e_proxy torch fx Proxy tracer graph inserting_after e_proxy node proxy = tracer create_proxy args kwargs set_example_value proxy node example_value proxy isinstance example_value torch Tensor i s enumerate example_value size need_bind s log debug track_produced_symints s s size s debug_level s s e_proxy i tracer debug_level lazy_proxy = LazyProxy tracer _proxy_with_example_value s call_function torch ops aten sym_size int e_proxy i type_expr=type s track_produced_symints s lazy_proxy storage_offset = example_value storage_offset need_bind storage_offset log debug track_produced_symints s s storage_offset debug_level s storage_offset e_proxy tracer debug_level lazy_proxy = LazyProxy tracer _proxy_with_example_value storage_offset call_function torch ops aten sym_storage_offset e_proxy type_expr=type storage_offset track_produced_symints storage_offset lazy_proxy example_value layout torch strided i s enumerate example_value stride need_bind s log debug track_produced_symints s s stride s debug_level s s e_proxy i tracer debug_level lazy_proxy = LazyProxy tracer _proxy_with_example_value s call_function torch ops aten sym_stride int e_proxy i type_expr=type s track_produced_symints s lazy_proxy example_value layout torch sparse_coo track_produced_symints example_value _indices e_proxy track_produced_symints example_value _values e_proxy example_value layout torch sparse_csr torch sparse_bsr track_produced_symints example_value crow_indices e_proxy track_produced_symints example_value col_indices e_proxy example_value layout torch sparse_csc torch sparse_bsc track_produced_symints example_value ccol_indices e_proxy track_produced_symints example_value row_indices e_proxy is_traceable_wrapper_subclass example_value attrs ctx = example_value __tensor_flatten__ attr attrs inner_t = getattr example_value attr track_produced_symints inner_t getattr e_proxy attr isinstance example_value torch SymInt need_bind example_value expr = example_value node expr tracer bound_symbols expr = e_proxy See Note Auto lift basic free symbols when create_graph_input _lift_basic_symbols example_value Union torch SymInt torch Tensor src Optional Source - None The before arg inserting symints sizes strides tensor before tensor This ordering ensures when we look tensor s symbols they re already lifted tracked E g assumption used insert_deferred_runtime_asserts _lift_symbols_in_symint s Union int torch SymInt source Optional Source before bool = False - None is_symbolic s assert isinstance s torch SymInt self_to_be_bound = lookup_unbound_symbols s len self_to_be_bound == For subgraph parent None Recursively lift symbols symint until top-level parent _lift_basic_symbols s source s self_to_be_bound parent_proxy = parent bound_symbols s example_val = parent_proxy node meta example_value type ignore union-attr assert isinstance example_val torch SymInt ph = create_graph_input str s type example_val example_val before=before source=source log debug _lift_symbols_in_symint s s debug_level s s source name source None subgraph inputs debug_level lifted_freevars parent_proxy = ph type ignore index For root_tracer assert len self_to_be_bound == f For root tracer we only expect bind basic symbols compound symbols f should cached before got unbound symbols self_to_be_bound s assert source None f Source s None when lifting input top-level If s unbacked symbol could because s tracked lazy_bind_unbacked_symbols f Otherwise should provide source when create_graph_input ` s ` root tracer s = next iter self_to_be_bound ph = create_graph_input str s type s s before=before source=source log debug _lift_symbols_in_symint s s debug_level s s source name source None subgraph inputs debug_level ph node meta grapharg = GraphArg source s pass_arg_as_tensor=False fake_tensor=None is_tensor=False isinstance example_value torch Tensor i s enumerate example_value size _lift_symbols_in_symint s TensorPropertySource src TensorProperty SIZE i src None None before=True example_value layout torch strided i s enumerate example_value stride _lift_symbols_in_symint s TensorPropertySource src TensorProperty STRIDE i src None None before=True _lift_symbols_in_symint example_value storage_offset TensorPropertySource src TensorProperty STORAGE_OFFSET src None None before=True example_value layout torch sparse_coo _lift_basic_symbols example_value _indices src _lift_basic_symbols example_value _values src example_value layout torch sparse_csr torch sparse_bsr _lift_basic_symbols example_value crow_indices src _lift_basic_symbols example_value col_indices src example_value layout torch sparse_csc torch sparse_bsc _lift_basic_symbols example_value ccol_indices src _lift_basic_symbols example_value row_indices src is_traceable_wrapper_subclass example_value attrs ctx = example_value __tensor_flatten__ attr attrs inner_t = getattr example_value attr _lift_basic_symbols inner_t AttrSource src attr src None None isinstance example_value torch SymInt _lift_symbols_in_symint example_value src Lookup proxy current tracer each symbol expressions s See Note Auto lift basic free symbols when create_graph_input lookup_unbound_symbols s torch SymInt - list sympy Symbol free_symbols = s node expr free_symbols len free_symbols == to_be_bound = s free_symbols s bound_symbols to_be_bound append s continue proxy = bound_symbols s isinstance proxy LazyProxy proxy = proxy bound_symbols s = proxy assert isinstance proxy torch fx Proxy proxy tracer f The proxy symbol s doesn t belong current tracer Sort symbols so we can have deterministic lifting order sorted to_be_bound key=lambda s s name has_input_mutation - MutationInfo input_versions_at_beginning = _input_versions_at_beginning input_nodes = input_versions_at_end = node graph nodes node op == placeholder example_value = node meta example_value isinstance example_value torch Tensor input_versions_at_end append example_value _version input_nodes append node break mutated_inputs = i i v v enumerate zip input_versions_at_beginning input_versions_at_end v = v mutated_inputs mutated_nodes = input_nodes i i mutated_inputs msg = f Input mutation detected mutated_nodes MutationInfo True msg MutationInfo False has_aliasing - AliasingInfo torch _higher_order_ops utils _collect_fake_inputs input_storages dict StorageWeakRef torch fx Node = dict node graph nodes node op == placeholder example_value = _collect_fake_inputs node isinstance example_value torch Tensor storage = StorageWeakRef example_value _typed_storage storage input_storages input-input aliasing msg = f Input-to-input aliasing detected nodes input_storages storage node AliasingInfo True msg input_storages storage = node break output_storages dict StorageWeakRef torch fx Node = dict out_nodes = graph find_nodes op= output out_node pytree tree_leaves out_nodes args out_node example_value = _collect_fake_inputs out_node assert isinstance example_value list isinstance example_value torch Tensor storage = StorageWeakRef example_value _typed_storage storage output_storages output-output aliasing msg = f Output-to-output aliasing detected nodes output_storages storage out_node AliasingInfo True msg output_storages storage = out_node intersected_storages = input_storages keys output_storages keys len intersected_storages input-output aliasing aliased = input_storages s output_storages s s intersected_storages aliased = join f i o i o aliased msg = f Input-to-output aliasing detected nodes aliased AliasingInfo True msg AliasingInfo False NOTE HigherOrderOperator tracing design Ignoring HigherOrderOperators moment OutputGraph represents graph being built Dynamo may compiled executed It holds root SubgraphTracer where FX graph built HigherOrderOperators operators take functions their arguments When Dynamo encounters HigherOrderOperator then attempts introspect function passed call body function capture into GraphModule rewrite call HigherOrderOperator use GraphModule The way we handle capture body functions through having possibly nested SubgraphTracers one per body function Mechanically we do introspection - Creating new SubgraphTracer via OutputGraph subtracer - Executing body function This constructs graph body function new SubgraphTracer while modifying state OutputGraph For example - OutputGraph can receive new GraphArgs we discover any new untracked Tensors - side effects body function get accumulated into OutputGraph side_effects - guards produced body function get accumulated into OutputGraph guards The traced function has some special properties make easier us transform later down line - we lift all free variables being inputs If introspection fails due existence graph breaks then we roll back current OutputGraph state graph break HigherOrderOperator