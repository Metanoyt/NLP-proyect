Owner s oncall cpu inductor contextlib copy itertools unittest torch torch ao quantization quantizer x _inductor_quantizer xiq torch _dynamo config dynamo_config torch _dynamo utils counters torch _inductor config metrics torch _inductor test_case run_tests TestCase torch _inductor utils is_mkldnn_bf _supported is_mkldnn_fp _supported run_and_get_code torch ao quantization quantizer x _inductor_quantizer X InductorQuantizer torch nn functional F torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_mkldnn reduced_f _on_and_off torch testing _internal common_quantization _generate_qdq_quantized_model skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoONEDNNBF torch testing _internal common_utils instantiate_parametrized_tests IS_FBCODE IS_LINUX IS_X MI _ARCH MI _ARCH parametrize skipIfNoXPU skipIfRocm skipIfRocmArch skipIfXpu TEST_ACL TEST_MKL xfailIfACL torch testing _internal inductor_utils _check_has_dynamic_shape clone_preserve_strides_offset HAS_CPU The dict value match_nodes computation_op+unary_op unary_list = torch nn ReLU torch nn Sigmoid torch nn Tanh torch nn Hardswish torch nn LeakyReLU inplace=False Use floats min max otherwise they can get converted symints torch nn Hardtanh min_val=- max_val= inplace=False torch nn Hardtanh min_val=- max_val=float inf inplace=False torch nn GELU approximate= none torch nn GELU approximate= tanh torch nn ReLU torch nn SiLU torch nn Hardsigmoid non_decomposed_unary_list = torch nn ReLU torch nn Sigmoid torch nn Tanh The dict value match_count match_nodes inplace binary_list = lambda x y torch add x y False call_function lambda x y torch add y x False call_function lambda x y x add y False call_method lambda x y x add_ y True call_method lambda x y torch sub x y False call_function lambda x y x sub y False call_method lambda x y x sub_ y True call_method quantization_add_fn_list = lambda x y torch add x y lambda x y x add y quantization_inplace_add_fn_list = lambda x y x add_ y get_default_quantizer is_qat is_dynamic quantizer = X InductorQuantizer quantizer set_global xiq get_default_x _inductor_quantization_config is_qat=is_qat is_dynamic=is_dynamic quantizer cal_conv_generated_kernel_number mod input dtype dim= device= cpu function decide how many kernels generated while testing conv d d deconv d assumption There will to_dtype kernel input lp inductor always use channel_last format there will to_channel_last format input to_dtype to_channel_last input can fused inductor always get channel last format mkldnn_conv_pointwise binary force output have same stride eager So there will to_contiguous output eager output contiguouse mod = copy deepcopy mod mod = mod device=device input = input clone input = input device dtype == torch float maybe_autocast = contextlib nullcontext maybe_autocast = torch amp autocast device_type=device dtype=dtype torch no_grad maybe_autocast output = mod input input_kernel output_kernel = input is_contiguous memory_format=torch contiguous_format dtype = torch float TEST_ACL dim == input_kernel = output is_contiguous memory_format=torch contiguous_format TEST_ACL dtype == torch bfloat output_kernel = input_kernel + output_kernel TestPatternMatcherBase TestCase setUp TestCase setUp ctx_stack = contextlib ExitStack ctx_stack enter_context config patch freezing True tearDown TestCase tearDown ctx_stack close _check_unary_is_decomposed unary_fn any isinstance unary_fn fn fn torch nn ReLU torch nn Sigmoid torch nn Tanh _clone_inputs inputs clone x isinstance x torch Tensor x x clone tuple clone x x inputs _test_common mod inputs matcher_check_fn atol= e- rtol= e- check_autocast=torch float check_quantization=False is_qat=False dtype=None is_dynamic=False quantizer=None compile_options= noqa B quantization_with_autocast=False hasattr device has_xpu = any isinstance input torch Tensor input device type == xpu input inputs device = xpu has_xpu cpu device = device mod = mod device=device device = cpu inputs = tuple clone_preserve_strides_offset x device=device x inputs counters clear torch _dynamo reset check_autocast == torch bfloat is_mkldnn_bf _supported device maybe_autocast = torch amp autocast device_type=device dtype=torch bfloat atol rtol = e- e- check_autocast == torch float is_mkldnn_fp _supported device maybe_autocast = torch amp autocast device_type=device dtype=torch float atol rtol = e- e- assert check_autocast == torch float maybe_autocast = contextlib nullcontext check_quantization quantization_with_autocast maybe_autocast convert_model = _generate_qdq_quantized_model mod inputs is_qat is_dynamic quantizer convert_model = _generate_qdq_quantized_model mod inputs is_qat is_dynamic quantizer torch no_grad maybe_autocast _ = torch compile convert_model inputs matcher_check_fn torch no_grad maybe_autocast clone_inputs = _clone_inputs inputs expected = mod inputs actual = torch compile mod compile_options clone_inputs precision = torch testing assert_close actual expected atol=self precision rtol=self precision torch testing assert_close actual expected atol=atol rtol=rtol matcher_check_fn _test_code_common mod inputs include_ops exclude_ops atol= e- rtol= e- check_quantization=False check_dynamic=None num_include_ops=None quantizer=None torch no_grad clone_inputs = _clone_inputs inputs check_quantization mod = _generate_qdq_quantized_model mod inputs quantizer=quantizer expected = mod inputs actual source_code = run_and_get_code torch compile mod fullgraph=True dynamic=check_dynamic clone_inputs assert_keywords = assert_size_stride assert_alignment filtered_lines = line line source_code splitlines any assert_key line assert_key assert_keywords source_code = \n join filtered_lines op include_ops assertIn op source_code num_include_ops None assert len include_ops == len num_include_ops i range len include_ops assertEqual source_code count include_ops i num_include_ops i op exclude_ops assertNotIn op source_code check_dynamic None _check_has_dynamic_shape source_code check_quantization Skip due reduce range setting Quantization preCI system torch testing assert_close actual expected atol=atol rtol=rtol TestPatternMatcherGeneric TestPatternMatcherBase _test_conv_unary_base dim= assert dim == dim == M torch nn Module __init__ unary_fn kwargs super __init__ dim == conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= unary_fn = unary_fn forward x x = conv x unary_fn x dtypes = torch float is_mkldnn_bf _supported device dtypes append torch bfloat is_mkldnn_fp _supported device dtypes append torch float cl_format = torch channels_last dim == torch channels_last_ d options = itertools product unary_list keys torch contiguous_format cl_format dtypes unary_fn memory_format dtype options dtype = torch float torch backends mkldnn matmul fp _precision == tf continue metrics reset dim == x_shape = x_shape = mod = M unary_fn memory_format=memory_format eval v = torch randn x_shape dtype=torch float add memory_format=memory_format matcher_check_fn match_nodes = unary_list unary_fn dtype torch float torch bfloat _check_unary_is_decomposed unary_fn Has extra dtype conversion nodes autocast match_nodes += assertEqual counters inductor mkldnn_unary_fusion_matcher_nodes TEST_ACL match_nodes assertEqual counters inductor mkldnn_conv_weight_pack_matcher_count _test_common mod v matcher_check_fn check_autocast=dtype generated_kernel_count = cal_conv_generated_kernel_number mod v dtype dim device assertEqual metrics generated_kernel_count generated_kernel_count skipIfNoDynamoSupport skipIfNoONEDNN skipIfRocm reduced_f _on_and_off test_conv d_unary device device = device _test_conv_unary_base dim= skipIfNoDynamoSupport skipIfNoONEDNN skipIfRocm reduced_f _on_and_off test_conv d_unary device device = device _test_conv_unary_base dim= _test_conv_transpose_unary_base dim= assert dim == dim == M torch nn Module __init__ unary_fn kwargs super __init__ dim == conv_transpose = torch nn ConvTranspose d stride= padding= conv_transpose = torch nn ConvTranspose d stride= padding= unary_fn = unary_fn forward x x = conv_transpose x unary_fn x dtypes = torch float is_mkldnn_bf _supported device dtypes append torch bfloat is_mkldnn_fp _supported device dtypes append torch float cl_format = torch channels_last dim == torch channels_last_ d options = itertools product unary_list torch contiguous_format cl_format dtypes unary_fn memory_format dtype options metrics reset dim == x_shape = x_shape = mod = M unary_fn eval v = torch randn x_shape dtype=torch float memory_format=memory_format matcher_check_fn match_nodes = unary_list unary_fn dtype torch float torch bfloat _check_unary_is_decomposed unary_fn Has extra dtype conversion nodes autocast match_nodes += assertEqual counters inductor mkldnn_unary_fusion_matcher_nodes TEST_ACL match_nodes assertEqual counters inductor mkldnn_conv_weight_pack_matcher_count _test_common mod v matcher_check_fn check_autocast=dtype generated_kernel_count = cal_conv_generated_kernel_number mod v dtype dim device assertEqual metrics generated_kernel_count generated_kernel_count skipIfNoDynamoSupport skipIfNoONEDNN skipIfRocm skipIfXpu msg= The operator mkldnn _convolution_transpose_pointwise currently implemented XPU device reduced_f _on_and_off test_conv_transpose d_unary device device = device _test_conv_transpose_unary_base dim= skipIfNoDynamoSupport skipIfNoONEDNN skipIfRocm skipIfXpu msg= The operator mkldnn _convolution_transpose_pointwise currently implemented XPU device reduced_f _on_and_off test_conv_transpose d_unary device device = device _test_conv_transpose_unary_base dim= _test_conv_binary_base dim= assert dim == dim == M torch nn Module __init__ binary_fn has_relu kwargs super __init__ dim == conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= binary_fn = binary_fn has_relu = has_relu forward x x = conv x x = conv x has_relu binary_fn x x relu binary_fn x x dtypes = torch float is_mkldnn_bf _supported device dtypes append torch bfloat is_mkldnn_fp _supported device dtypes append torch float cl_format = torch channels_last dim == torch channels_last_ d test_memory_format = torch contiguous_format cl_format options = itertools product binary_list True False test_memory_format dtypes binary_fn has_relu memory_format dtype options dtype = torch float torch backends mkldnn matmul fp _precision == tf continue metrics reset dim == x_shape = x_shape = mod = M binary_fn has_relu eval v = torch randn x_shape dtype=torch float requires_grad=True add memory_format=memory_format matcher_check_fn match_nodes = binary_list binary_fn has_relu match_nodes += assertEqual counters inductor mkldnn_conv_binary_unary_fusion_matcher_nodes TEST_ACL match_nodes assertEqual counters inductor mkldnn_conv_weight_pack_matcher_count _test_common mod v matcher_check_fn check_autocast=dtype generated_kernel_count = cal_conv_generated_kernel_number mod v dtype dim device assertEqual metrics generated_kernel_count generated_kernel_count skipIfNoDynamoSupport skipIfNoONEDNN skipIfRocm reduced_f _on_and_off test_conv d_binary device device = device _test_conv_binary_base dim= skipIfNoDynamoSupport skipIfNoONEDNN skipIfRocm reduced_f _on_and_off test_conv d_binary device device = device _test_conv_binary_base dim= _test_conv_binary_broadcast_shapes_base dim= assert dim == dim == M torch nn Module __init__ binary_fn has_relu kwargs super __init__ dim == conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= binary_fn = binary_fn has_relu = has_relu forward x x x = conv x has_relu binary_fn x x relu binary_fn x x dtypes = torch float is_mkldnn_bf _supported device dtypes append torch bfloat is_mkldnn_fp _supported device dtypes append torch float cl_format = torch channels_last dim == torch channels_last_ d test_memory_format = torch contiguous_format cl_format dim == input_shapes = other_shapes = input_shapes = other_shapes = options = itertools product binary_list input_shapes other_shapes True False test_memory_format dtypes binary_fn x_shape other_shape has_relu memory_format dtype options metrics reset mod = M binary_fn has_relu eval x = torch randn x_shape dtype=torch float requires_grad=True add memory_format=memory_format other = torch randn other_shape dtype=torch float requires_grad=True add memory_format=memory_format dtype matcher_check_fn match_nodes = binary_list binary_fn has_relu match_nodes += assertEqual counters inductor mkldnn_conv_binary_unary_fusion_matcher_nodes TEST_ACL match_nodes assertEqual counters inductor mkldnn_conv_weight_pack_matcher_nodes _test_common mod x other matcher_check_fn check_autocast=dtype skipIfNoDynamoSupport skipIfNoONEDNN skipIfRocm reduced_f _on_and_off test_conv d_binary_broadcast_shapes device device = device _test_conv_binary_broadcast_shapes_base dim= skipIfNoDynamoSupport skipIfNoONEDNN skipIfRocm reduced_f _on_and_off test_conv d_binary_broadcast_shapes device device = device _test_conv_binary_broadcast_shapes_base dim= skipIfNoDynamoSupport skipIfNoONEDNN skipIfRocm unittest skipIf IS_FBCODE Failing fbcode reduced_f _on_and_off test_conv d_linear_add_broadcast_shapes device device = device M torch nn Module __init__ super __init__ conv = torch nn Conv d kernel_size= stride= linear = torch nn Linear forward x x conv x + linear x None None metrics reset mod = M eval x = torch randn x = torch randn matcher_check_fn match_nodes = TEST_ACL assertEqual counters inductor mkldnn_conv_binary_unary_fusion_matcher_nodes match_nodes assertEqual counters inductor mkldnn_conv_weight_pack_matcher_nodes _test_common mod x x matcher_check_fn TestPatternMatcher TestPatternMatcherBase reduced_f _on_and_off test_linear_unary device= cpu device = device M torch nn Module __init__ unary_fn in_features out_features bias kwargs super __init__ linear = torch nn Linear in_features out_features bias kwargs unary_fn = unary_fn forward x x = linear x unary_fn x dtypes = is_mkldnn_bf _supported device dtypes append torch bfloat is_mkldnn_fp _supported device dtypes append torch float torch backends mkldnn matmul fp _precision bf tf dtypes append torch float options = itertools product unary_list True False dtypes unary_fn bias dtype options dtype = torch float torch backends mkldnn matmul fp _precision == tf continue metrics reset mod = M unary_fn bias=bias eval only fuse linear when dtype bf v = torch randn matcher_check_fn match_nodes = unary_list unary_fn dtype = torch float _check_unary_is_decomposed unary_fn Has extra dtype conversion nodes autocast match_nodes += assertEqual counters inductor mkldnn_unary_fusion_matcher_nodes TEST_ACL match_nodes assertEqual counters inductor mkldnn_linear_weight_pack_matcher_count _test_common mod v matcher_check_fn check_autocast=dtype only generated kernel to_dtype expected_kernel_count = TEST_ACL dtype == torch float In BF input float will generate kernel to_dtype expected_kernel_count -= assertEqual metrics generated_kernel_count expected_kernel_count reduced_f _on_and_off unittest skipIf TEST_MKL Test requires MKL test_linear_fp device= cpu device = device M torch nn Module __init__ bias super __init__ linear = torch nn Linear bias forward x linear x bias True False mod = M bias=bias eval v = torch randn packing pass matcher_check_fn assertEqual counters inductor mkldnn_linear_weight_pack_matcher_count _test_common mod v matcher_check_fn unittest skipIf TEST_MKL Test requires MKL test_linear_input_non_contiguous_ D_wo_bias device= cpu device = device Activation D non-contiguous without Bias M torch nn Module __init__ super __init__ linear = torch nn Linear bias=False forward x x = torch ops aten permute default x x = torch ops aten reshape default x linear x mod = M eval v = torch randn dtypes = torch float is_mkldnn_bf _supported device dtypes append torch bfloat is_mkldnn_fp _supported device dtypes append torch float dtype dtypes torch _dynamo reset autocast_enabled = dtype torch bfloat torch float torch no_grad torch autocast device_type= cpu enabled=autocast_enabled dtype=dtype expected = mod v actual source_code = run_and_get_code torch compile mod fullgraph=True v assertIn torch ops mkldnn _linear_pointwise default autocast_enabled torch ops mkl _mkl_linear default source_code torch testing assert_close actual expected atol= e- rtol= e- skipIfXpu msg= Different CPU two linears will concat XPU better performance test_linear_add_bias device= cpu device = device M torch nn Module __init__ device dtype unary_fn cast_bias super __init__ linear = torch nn Linear bias=False bias = torch randn device=device linear = torch nn Linear bias=False bias = torch randn device=device cast_bias bias = bias dtype=dtype device=device bias = bias dtype=dtype device=device unary_fn = unary_fn forward x = linear x + bias b = linear x + bias unary_fn unary_fn b dtypes = is_mkldnn_bf _supported device dtypes append torch bfloat is_mkldnn_fp _supported device dtypes append torch float options = itertools product unary_list dtypes unary_fn dtype options metrics reset fold_mod = M device dtype unary_fn cast_bias=True eval v = torch randn folder_matcher_check_fn match_nodes = unary_list unary_fn _check_unary_is_decomposed unary_fn Has extra dtype conversion nodes autocast match_nodes += we have linears so we double matcher_count nodes assertEqual counters inductor mkldnn_unary_fusion_matcher_count TEST_ACL assertEqual counters inductor mkldnn_unary_fusion_matcher_nodes TEST_ACL match_nodes assertEqual counters inductor mkldnn_linear_weight_pack_matcher_count _test_common fold_mod v folder_matcher_check_fn check_autocast=dtype assertEqual metrics generated_kernel_count TEST_ACL we won t fold bias bias same dtype weight https github com pytorch pytorch pull metrics reset mod = M device dtype unary_fn cast_bias=False eval matcher_check_fn assertEqual counters inductor mkldnn_linear_weight_pack_matcher_count _test_common mod v matcher_check_fn check_autocast=dtype kernel to_lowp kernels unary ops assertEqual metrics generated_kernel_count reduced_f _on_and_off test_linear_binary device= cpu device = device M torch nn Module __init__ binary_fn in_channels out_channels bias kwargs super __init__ linear = torch nn Linear in_channels out_channels bias=bias kwargs binary_fn = binary_fn forward x y x = linear x x = binary_fn x y clone x dtypes = is_mkldnn_bf _supported device dtypes append torch bfloat is_mkldnn_fp _supported device dtypes append torch float torch backends mkldnn matmul fp _precision bf tf dtypes append torch float options = itertools product binary_list True False dtypes out_feature = binary_fn input_shape bias dtype options metrics reset dtype = torch float torch backends mkldnn matmul fp _precision == tf continue matcher_check_fn assertEqual counters inductor mkldnn_conv_binary_unary_fusion_matcher_nodes TEST_ACL reshape_linear_reshape_match_nodes = len input_shape == assertEqual counters inductor mkldnn_reshape_linear_reshape_matcher_nodes reshape_linear_reshape_match_nodes assertEqual counters inductor mkldnn_linear_weight_pack_matcher_count mod = M binary_fn input_shape - out_feature bias eval v = torch randn input_shape other = torch randn input_shape - + out_feature dtype _test_common mod v other matcher_check_fn check_autocast=dtype only generated kernel to_dtype expected_kernel_count = TEST_ACL dtype == torch float In BF input float will generate kernel to_dtype expected_kernel_count -= assertEqual metrics generated_kernel_count expected_kernel_count test_linear_binary_broadcast_shapes device= cpu device = device M torch nn Module __init__ binary_fn in_channels out_channels bias kwargs super __init__ linear = torch nn Linear in_channels out_channels bias=bias kwargs binary_fn = binary_fn forward x y x = linear x x = binary_fn x y clone x dtypes = is_mkldnn_bf _supported device dtypes append torch bfloat is_mkldnn_fp _supported device dtypes append torch float options = itertools product binary_list True False dtypes out_feature = binary_fn input_shape other_shape bias dtype options metrics reset mod = M binary_fn input_shape - out_feature bias eval v = torch randn input_shape other = torch randn other_shape dtype matcher_check_fn reshape_linear_reshape_match_nodes = len input_shape == assertEqual counters inductor mkldnn_reshape_linear_reshape_matcher_nodes reshape_linear_reshape_match_nodes assertEqual counters inductor mkldnn_conv_binary_unary_fusion_matcher_nodes TEST_ACL assertEqual counters inductor mkldnn_linear_weight_pack_matcher_nodes _test_common mod v other matcher_check_fn check_autocast=dtype assertEqual metrics generated_kernel_count TEST_ACL skipIfXpu msg= Different CPU two linears will concat XPU better performance test_multi_linear_share_same_input device= cpu device = device llama pattern M torch nn Module __init__ super __init__ w = torch nn Linear bias=False w = torch nn Linear bias=False forward x F silu w x F relu w x dtypes = is_mkldnn_bf _supported device dtypes append torch bfloat is_mkldnn_fp _supported device dtypes append torch float matcher_check_fn assertEqual counters inductor mkldnn_unary_fusion_matcher_nodes TEST_ACL assertEqual counters inductor mkldnn_unary_fusion_matcher_count TEST_ACL assertEqual counters inductor mkldnn_reshape_linear_reshape_matcher_nodes assertEqual counters inductor mkldnn_linear_weight_pack_matcher_count dtype dtypes mod = M dtype eval v = torch randn dtype _test_common mod v matcher_check_fn rtol= e- atol= e- _qconv d_test_helper device= cpu int _mixed_bf =False quantization_with_autocast=False M torch nn Module __init__ kwargs super __init__ conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= groups= forward x conv conv conv x mod = M eval device=device v = torch randn dtype=torch float requires_grad=False add device=device matcher_check_fn Dequant-Conv D pattern matched QConv D weight prepack int _mixed_fp dequant_node dequantize_per_channel clone convolution int _mixed_bf dequant_node optional convert_element_type_ dequantize_per_channel optional convert_element_type_ clone convolution assertEqual counters inductor qconv_weight_prepack_matcher_count assertEqual counters inductor qconv_weight_prepack_matcher_nodes quantization_with_autocast int _mixed_bf assertEqual counters inductor qconv_unary_lower_count TEST_ACL _test_common mod v matcher_check_fn check_quantization=True check_autocast=torch bfloat int _mixed_bf torch float quantization_with_autocast=quantization_with_autocast skipIfNoDynamoSupport skipIfNoONEDNN skipIfRocm test_qconv d_cpu r This testcase will quantize single Conv d module _qconv d_test_helper cpu skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qconv d_xpu r This testcase will quantize single Conv d module _qconv d_test_helper xpu skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN skipIfRocmArch MI _ARCH + MI _ARCH test_qconv d_int _mixed_bf r This testcase will quantize single Conv d module int _mixed_bf quantization _qconv d_test_helper int _mixed_bf =True skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN skipIfRocmArch MI _ARCH + MI _ARCH test_qconv d_int _mixed_bf _use_autocast r This testcase will quantize single Conv d module int _mixed_bf quantization _qconv d_test_helper int _mixed_bf =True quantization_with_autocast=True skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN skipIfNoXPU test_qconv d_int _mixed_bf _xpu r This testcase will quantize single Conv d module int _mixed_bf quantization _qconv d_test_helper device= xpu int _mixed_bf =True _qconv d_unary_test_helper device= cpu int _mixed_bf =False unary_op=torch nn ReLU qconv_unary_matcher_nodes=None M torch nn Module __init__ kwargs super __init__ conv = torch nn Conv d kernel_size= stride= unary_fn = copy deepcopy unary_op conv = torch nn Conv d kernel_size= stride= bias=False unary_fn = copy deepcopy unary_op forward x tmp = unary_fn conv x unary_fn conv tmp mod = M eval device=device v = torch randn dtype=torch float requires_grad=False add device=device matcher_check_fn Dequant-Conv D pattern matched quantization weight prepack assertEqual counters inductor qconv_weight_prepack_matcher_count QConv D Unary fusion post-grad fusion pass assertEqual counters inductor qconv_unary_matcher_count TEST_ACL assertEqual counters inductor qconv_unary_lower_count TEST_ACL qconv_unary_matcher_nodes assertEqual counters inductor qconv_unary_matcher_nodes TEST_ACL qconv_unary_matcher_nodes _test_common mod v check_quantization=True check_autocast=torch bfloat int _mixed_bf torch float matcher_check_fn=matcher_check_fn skipIfNoDynamoSupport skipIfNoONEDNN test_qconv d_relu_cpu r This testcase will quantize Conv d- ReLU pattern _qconv d_unary_test_helper device= cpu skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qconv d_relu_xpu r This testcase will quantize Conv d- ReLU pattern _qconv d_unary_test_helper device= xpu skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qconv d_relu_int _mixed_bf _xpu r This testcase will quantize Conv d- ReLU pattern int _mixed_bf quantization _qconv d_unary_test_helper int _mixed_bf =True skipIfNoDynamoSupport skipIfNoONEDNN test_qconv d_relu _cpu r This testcase will quantize Conv d- ReLU pattern _qconv d_unary_test_helper device= cpu unary_op=torch nn ReLU skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qconv d_relu _xpu r This testcase will quantize Conv d- ReLU pattern _qconv d_unary_test_helper device= xpu unary_op=torch nn ReLU skipIfNoDynamoSupport skipIfNoONEDNN test_qconv d_hardtanh_cpu r This testcase will quantize Conv d- Hardtanh pattern _qconv d_unary_test_helper device= cpu unary_op=torch nn Hardtanh skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qconv d_hardtanh_xpu r This testcase will quantize Conv d- Hardtanh pattern _qconv d_unary_test_helper device= xpu unary_op=torch nn Hardtanh skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qconv d_hardtanh_int _mixed_bf _cpu r This testcase will quantize Conv d- Hardtanh pattern Match nodes qconv d_pointwise_default convert_element_type clamp_min clamp_max convert_element_type quantize_per_tensor qconv d_pointwise_default convert_element_type clamp_min clamp_max convert_element_type _qconv d_unary_test_helper unary_op=torch nn Hardtanh int _mixed_bf =True qconv_unary_matcher_nodes= skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN skipIfNoXPU test_qconv d_hardtanh_int _mixed_bf _xpu r This testcase will quantize Conv d- Hardtanh pattern Match nodes qconv d_pointwise_default convert_element_type clamp_min clamp_max convert_element_type quantize_per_tensor qconv d_pointwise_default convert_element_type clamp_min clamp_max convert_element_type _qconv d_unary_test_helper device= xpu unary_op=torch nn Hardtanh int _mixed_bf =True qconv_unary_matcher_nodes= skipIfNoDynamoSupport skipIfNoONEDNN test_qconv d_hardswish_cpu r This testcase will quantize Conv d- Hardswish pattern _qconv d_unary_test_helper device= cpu unary_op=torch nn Hardswish skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qconv d_hardswish_xpu r This testcase will quantize Conv d- Hardswish pattern _qconv d_unary_test_helper device= xpu unary_op=torch nn Hardswish skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qconv d_hardswish_int _mixed_bf _cpu r This testcase will quantize Conv d- Hardswish pattern Match nodes qconv d_pointwise_default convert_element_type add clamp_min clamp_max mul div convert_element_type quantize_per_tensor qconv d_pointwise_default convert_element_type add clamp_min clamp_max mul div convert_element_type _qconv d_unary_test_helper unary_op=torch nn Hardswish int _mixed_bf =True qconv_unary_matcher_nodes= skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN skipIfNoXPU test_qconv d_hardswish_int _mixed_bf _xpu r This testcase will quantize Conv d- Hardswish pattern Match nodes qconv d_pointwise_default convert_element_type add clamp_min clamp_max mul div convert_element_type quantize_per_tensor qconv d_pointwise_default convert_element_type add clamp_min clamp_max mul div convert_element_type _qconv d_unary_test_helper device= xpu unary_op=torch nn Hardswish int _mixed_bf =True qconv_unary_matcher_nodes= skipIfNoDynamoSupport skipIfNoONEDNN test_qconv d_silu_cpu r This testcase will quantize Conv d- SiLU pattern _qconv d_unary_test_helper device= cpu unary_op=torch nn SiLU skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qconv d_silu_xpu r This testcase will quantize Conv d- SiLU pattern _qconv d_unary_test_helper device= xpu unary_op=torch nn SiLU skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qconv d_silu_int _mixed_bf _cpu r This testcase will quantize Conv d- SiLU pattern Match nodes qconv d_pointwise_default convert_element_type sigmoid mul convert_element_type quantize_per_tensor qconv d_pointwise_default convert_element_type sigmoid mul convert_element_type _qconv d_unary_test_helper unary_op=torch nn SiLU int _mixed_bf =True qconv_unary_matcher_nodes= skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN skipIfNoXPU test_qconv d_silu_int _mixed_bf _xpu r This testcase will quantize Conv d- SiLU pattern Match nodes qconv d_pointwise_default convert_element_type sigmoid mul convert_element_type quantize_per_tensor qconv d_pointwise_default convert_element_type sigmoid mul convert_element_type _qconv d_unary_test_helper device= xpu unary_op=torch nn SiLU int _mixed_bf =True qconv_unary_matcher_nodes= _qconv d_add_test_helper device= cpu use_relu=False int _mixed_bf =False r This testcase will quantize Conv d- Add pattern X \ Conv X Conv X \ Add &#124; Optional relu &#124; Y M torch nn Module __init__ add_fn use_relu kwargs super __init__ conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= add_fn = add_fn relu = torch nn ReLU conv = torch nn Conv d kernel_size= stride= bias=False conv = torch nn Conv d kernel_size= stride= bias=False add_fn = add_fn relu = torch nn ReLU use_relu = use_relu forward x x = conv x x = conv x tmp = add_fn x x use_relu tmp = relu tmp tmp = conv tmp tmp = conv tmp res = add_fn tmp tmp use_relu res = relu res res add_fn quantization_add_fn_list + quantization_inplace_add_fn_list mod = M add_fn use_relu eval device=device v = torch randn dtype=torch float requires_grad=False add device=device matcher_check_fn Dequant-Conv D pattern matched quantization weight prepack assertEqual counters inductor qconv_weight_prepack_matcher_count Qconv d Binary Unary fusion post-grad fusion pass assertEqual counters inductor qconv d_binary_matcher_count TEST_ACL assertEqual counters inductor qconv d_binary_lower_count TEST_ACL _test_common mod v matcher_check_fn check_quantization=True check_autocast=torch bfloat int _mixed_bf torch float _qconv d_add_test_helper device= cpu use_relu=False int _mixed_bf =False r This testcase will quantize two Conv d- Add patterns Conv X extra input \ Add &#124; Optional relu &#124; Y extra input Conv X \ Add &#124; Optional relu &#124; Y M torch nn Module __init__ add_fn use_relu swap_inputs kwargs super __init__ conv = torch nn Conv d kernel_size= stride= add_fn = add_fn relu = torch nn ReLU conv = torch nn Conv d kernel_size= stride= bias=False add_fn = add_fn relu = torch nn ReLU use_relu = use_relu swap_inputs = swap_inputs forward x x x x = conv x swap_inputs tmp = add_fn x x tmp = add_fn x x use_relu tmp = relu tmp tmp = conv tmp swap_inputs res = add_fn x tmp res = add_fn tmp x use_relu res = relu res res add_fn swap_inputs itertools product quantization_add_fn_list + quantization_inplace_add_fn_list False True mod = M add_fn use_relu swap_inputs eval device=device x = torch randn dtype=torch float requires_grad=False device=device x = torch randn dtype=torch float requires_grad=False device=device x = torch randn dtype=torch float requires_grad=False device=device matcher_check_fn Dequant-Conv D pattern matched quantization weight prepack assertEqual counters inductor qconv_weight_prepack_matcher_count Qconv d Binary Unary fusion post-grad fusion pass assertEqual counters inductor qconv d_binary_matcher_count TEST_ACL assertEqual counters inductor qconv d_binary_lower_count TEST_ACL _test_common mod x x x matcher_check_fn check_quantization=True check_autocast=torch bfloat int _mixed_bf torch float skipIfNoDynamoSupport skipIfNoONEDNN test_qconv d_add_cpu _qconv d_add_test_helper _qconv d_add_test_helper skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qconv d_add_xpu _qconv d_add_test_helper device= xpu _qconv d_add_test_helper device= xpu skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qconv d_add_int _mixed_bf _qconv d_add_test_helper int _mixed_bf =True _qconv d_add_test_helper int _mixed_bf =True skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN skipIfNoXPU test_qconv d_add_int _mixed_bf _xpu _qconv d_add_test_helper device= xpu int _mixed_bf =True skipIfNoDynamoSupport skipIfNoONEDNN test_qconv d_add_relu_cpu _qconv d_add_test_helper use_relu=True _qconv d_add_test_helper use_relu=True skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qconv d_add_relu_xpu _qconv d_add_test_helper device= xpu use_relu=True _qconv d_add_test_helper device= xpu use_relu=True skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qconv d_add_relu_int _mixed_bf _qconv d_add_test_helper use_relu=True int _mixed_bf =True _qconv d_add_test_helper use_relu=True int _mixed_bf =True skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN skipIfNoXPU test_qconv d_add_relu_int _mixed_bf _xpu _qconv d_add_test_helper device= xpu use_relu=True int _mixed_bf =True skipIfNoDynamoSupport skipIfNoONEDNN test_qconv d_add_broadcast_shapes_cpu r This testcase will quantize Conv d- add pattern using broadcast shape inputs Conv d- Add fusion will fail broadcast shape inputs case M torch nn Module __init__ use_bias super __init__ conv = torch nn Conv d kernel_size= stride= forward x x torch add conv x x bias_list = True False bias bias_list mod = M bias eval x = torch randn x = torch randn matcher_check_fn Dequant-Conv D pattern matched quantization weight prepack assertEqual counters inductor qconv_weight_prepack_matcher_count Qconv d Binary Unary fusion post-grad fusion pass assertEqual counters inductor qconv d_binary_matcher_count _test_common mod x x matcher_check_fn check_quantization=True skipIfNoDynamoSupport skipIfNoONEDNN test_qconv d_with_concat_cpu channel_ = channel_ = channel_ = channel_ = int channel_ + channel_ Model torch nn Module __init__ super __init__ conv = torch nn Conv d channel_ channel_ stride= dilation= padding= conv = torch nn Conv d channel_ channel_ stride= dilation= padding= conv = torch nn Conv d channel_ channel_ stride= dilation= padding= conv = torch nn Conv d channel_ channel_ stride= dilation= padding= forward x torch Tensor x = conv x x = conv x x = conv x res = torch cat x x x dim= res = conv res res mod = Model eval v = torch randn channel_ dtype=torch float requires_grad=False matcher_check_fn assertEqual counters inductor qconv_weight_prepack_matcher_count assertEqual counters inductor qconv_unary_matcher_count TEST_ACL assertEqual counters inductor qconv_unary_lower_count TEST_ACL _test_common mod v matcher_check_fn check_quantization=True skipIfNoDynamoSupport skipIfNoONEDNN test_qconv d_add_ r This testcase prevents pattern matched conv_binary fusion mistake Conv X \ Add We see pattern Mobilenet v large which add decomposed torch nn Hardswish torch nn Hardsigmoid M torch nn Module __init__ post_op super __init__ conv = torch nn Conv d kernel_size= stride= post_op = post_op forward x post_op conv x post_op torch nn Hardswish inplace=True torch nn Hardsigmoid inplace=True mod = M post_op eval v = torch randn dtype=torch float requires_grad=False add matcher_check_fn Shouldn t hit conv binary fusion assertEqual counters inductor qconv d_binary_matcher_count _test_common mod v matcher_check_fn check_quantization=True skipIfNoDynamoSupport skipIfNoONEDNN test_qconv d_add_ r This testcase will test below model x \ conv maxpool \ \ add conv \ cat Based default recipe x InductorQuantizer we will see pattern after convert qconv maxpool \ &#124; \ q \ \ \ dq qconv \ add &#124; q Since q has users qconv ancestor node qconv we shouldn t fuse int qconv dq \ add &#124; q &#124; int Instead we can match fuse pattern into qconv_binary qconv fp \ add &#124; fp M torch nn Module __init__ super __init__ conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= maxpool = torch nn MaxPool d kernel_size= stride= padding= dilation= forward x tmp = conv x tmp = maxpool x add = torch add tmp tmp tmp = conv tmp torch cat add tmp dim= mod = M eval v = torch randn dtype=torch float requires_grad=False add matcher_check_fn assertEqual counters inductor qconv d_binary_matcher_count TEST_ACL The matched qconv binary pattern should have nodes qconv add instead which has dequant binary input output quant assertEqual counters inductor qconv d_binary_matcher_nodes TEST_ACL assertEqual counters inductor qconv d_binary_lower_count TEST_ACL _test_common mod v matcher_check_fn check_quantization=True skipIfNoDynamoSupport skipIfNoONEDNN skipIfRocm test_qat_qconv d r This testcase will quantize single Conv d module qat flow M torch nn Module __init__ kwargs super __init__ conv = torch nn Conv d kernel_size= stride= bn = torch nn BatchNorm d forward x bn conv x mod = M train v = torch randn dtype=torch float requires_grad=True add matcher_check_fn Dequant-conv pattern matched quantization weight prepack dequantize_per_tensor dequantize_per_channel clone convolution assertEqual counters inductor qconv_weight_prepack_matcher_count assertEqual counters inductor qconv_weight_prepack_matcher_nodes QConv D Unary fusion post-grad fusion pass qconv d_pointwise_default quantize_per_tensor assertEqual counters inductor qconv_unary_matcher_count TEST_ACL assertEqual counters inductor qconv_unary_matcher_nodes TEST_ACL assertEqual counters inductor qconv_unary_lower_count TEST_ACL _test_common mod v matcher_check_fn check_quantization=True is_qat=True _qat_qconv d_unary_cpu_test_helper unary_op=torch nn ReLU M torch nn Module __init__ kwargs super __init__ conv = torch nn Conv d kernel_size= stride= unary_fn = copy deepcopy unary_op bn = torch nn BatchNorm d conv = torch nn Conv d kernel_size= stride= unary_fn = copy deepcopy unary_op bn = torch nn BatchNorm d forward x tmp = unary_fn bn conv x unary_fn bn conv tmp mod = M v = torch randn dtype=torch float requires_grad=True add matcher_check_fn Dequant-conv pattern matched quantization weight prepack convert_element_type_ sub mul_ dequantize_per_channel clone convolution assertEqual counters inductor qconv_weight_prepack_matcher_count QConv D Unary fusion post-grad fusion pass qconv d_pointwise_default relu div_ round_ add_ clamp_min_ clamp_max_ convert_element_type_ assertEqual counters inductor qconv_unary_matcher_count TEST_ACL assertEqual counters inductor qconv_unary_lower_count TEST_ACL _test_common mod v matcher_check_fn check_quantization=True is_qat=True skipIfNoDynamoSupport skipIfNoONEDNN test_qat_qconv d_relu r This testcase will quantize Conv d- ReLU pattern qat flow _qat_qconv d_unary_cpu_test_helper skipIfNoDynamoSupport skipIfNoONEDNN test_qat_qconv d_relu r This testcase will quantize Conv d- ReLU pattern qat flow _qat_qconv d_unary_cpu_test_helper unary_op=torch nn ReLU skipIfNoDynamoSupport skipIfNoONEDNN test_qat_qconv d_hardtanh r This testcase will quantize Conv d- Hardtanh pattern qat flow _qat_qconv d_unary_cpu_test_helper unary_op=torch nn Hardtanh skipIfNoDynamoSupport skipIfNoONEDNN test_qat_qconv d_silu r This testcase will quantize Conv d- SiLU pattern qat flow _qat_qconv d_unary_cpu_test_helper unary_op=torch nn SiLU skipIfNoDynamoSupport skipIfNoONEDNN test_qat_qconv d_hardswish r This testcase will quantize Conv d- Hardswish pattern qat flow _qat_qconv d_unary_cpu_test_helper unary_op=torch nn Hardswish skipIfNoDynamoSupport skipIfNoONEDNN skipIfRocm test_qat_qconv d_add r This testcase will quantize Conv d- Add pattern X \ Conv X Conv X \ Add &#124; Y M torch nn Module __init__ kwargs super __init__ conv = torch nn Conv d kernel_size= stride= bn = torch nn BatchNorm d conv = torch nn Conv d kernel_size= stride= bn = torch nn BatchNorm d forward x x = bn conv x x = bn conv x x + x mod = M train v = torch randn dtype=torch float requires_grad=True add matcher_check_fn Dequant-conv pattern matched quantization weight prepack dequantize_per_tensor dequantize_per_channel clone convolution assertEqual counters inductor qconv_weight_prepack_matcher_count assertEqual counters inductor qconv_weight_prepack_matcher_nodes Qconv d Binary fusion post-grad fusion pass qconv d_pointwise_default_ dequantize_per_tensor add_ quantize_per_tensor assertEqual counters inductor qconv d_binary_matcher_count TEST_ACL assertEqual counters inductor qconv d_binary_matcher_nodes TEST_ACL assertEqual counters inductor qconv d_binary_lower_count TEST_ACL _test_common mod v matcher_check_fn check_quantization=True is_qat=True skipIfNoDynamoSupport skipIfNoONEDNN skipIfRocm test_qat_qconv d_add_relu r This testcase will quantize Conv d- Add- ReLU pattern X \ Conv X Conv X \ Add &#124; ReLU &#124; Y M torch nn Module __init__ kwargs super __init__ conv = torch nn Conv d kernel_size= stride= bn = torch nn BatchNorm d conv = torch nn Conv d kernel_size= stride= bn = torch nn BatchNorm d relu = torch nn ReLU forward x x = bn conv x x = bn conv x relu x + x mod = M train v = torch randn dtype=torch float requires_grad=True add matcher_check_fn Dequant-conv pattern matched quantization weight prepack dequantize_per_tensor dequantize_per_channel clone convolution assertEqual counters inductor qconv_weight_prepack_matcher_count assertEqual counters inductor qconv_weight_prepack_matcher_nodes Qconv d Binary fusion post-grad fusion pass qconv d_pointwise_default_ dequantize_per_tensor add_ relu quantize_per_tensor assertEqual counters inductor qconv d_binary_matcher_count TEST_ACL assertEqual counters inductor qconv d_binary_matcher_nodes TEST_ACL assertEqual counters inductor qconv d_binary_lower_count TEST_ACL _test_common mod v matcher_check_fn check_quantization=True is_qat=True _test_qconv d_dequant_promotion_helper device= cpu r This testcase tests dequant node before conv d promoted correctly X &#124; Conv X \ Conv X Conv X \ Add &#124; Y M torch nn Module __init__ kwargs super __init__ conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= forward x temp = conv x temp = conv temp + conv temp temp mod = M eval device=device v = torch randn dtype=torch float requires_grad=False add device=device matcher_check_fn Dequant pattern matcher dequant promotion dequantize_per_tensor assertEqual counters inductor dequant_promotion_matcher_count assertEqual counters inductor dequant_promotion_matcher_nodes Dequant-conv pattern matched quantization weight prepack dequantize_per_tensor dequantize_per_channel clone convolution assertEqual counters inductor qconv_weight_prepack_matcher_count assertEqual counters inductor qconv_weight_prepack_matcher_nodes Qconv d Binary fusion post-grad fusion pass qconv d_pointwise_default_ add_ assertEqual counters inductor qconv d_binary_matcher_count TEST_ACL assertEqual counters inductor qconv d_binary_matcher_nodes TEST_ACL assertEqual counters inductor qconv d_binary_lower_count TEST_ACL _test_common mod v matcher_check_fn check_quantization=True skipIfNoDynamoSupport skipIfNoONEDNN skipIfRocm test_qconv d_dequant_promotion_cpu _test_qconv d_dequant_promotion_helper skipIfNoDynamoSupport skipIfNoONEDNN skipIfRocm skipIfNoXPU test_qconv d_dequant_promotion_xpu _test_qconv d_dequant_promotion_helper device= xpu skipIfNoDynamoSupport skipIfNoONEDNN test_qconv d_relu_cpu r This testcase will quantize Conv d- ReLU pattern device = cpu unary_op = torch nn ReLU M torch nn Module __init__ super __init__ conv = torch nn Conv d kernel_size= stride= unary_fn = copy deepcopy unary_op conv = torch nn Conv d kernel_size= stride= bias=False unary_fn = copy deepcopy unary_op forward x tmp = unary_fn conv x unary_fn conv tmp mod = M eval device=device v = torch randn dtype=torch float requires_grad=False add device=device matcher_check_fn Dequant-Conv D pattern matched quantization weight prepack assertEqual counters inductor qconv_weight_prepack_matcher_count QConv D Unary fusion post-grad fusion pass assertEqual counters inductor qconv_unary_matcher_count TEST_ACL assertEqual counters inductor qconv_unary_lower_count TEST_ACL _test_common mod v check_quantization=True matcher_check_fn=matcher_check_fn _qlinear_test_helper inputs device= cpu int _mixed_bf =False do_permute=False matcher_check_fn=None bias=True is_dynamic=False is_qat=False quantization_with_autocast=False M torch nn Module __init__ use_bias do_permute=False super __init__ linear = torch nn Linear use_bias linear = torch nn Linear use_bias do_permute = do_permute forward x do_permute x = torch reshape torch permute x linear linear x mod = M bias do_permute=do_permute eval device=device assert isinstance inputs tuple __convert_tensor_to_device input device input device=device isinstance input torch Tensor input inputs = tuple __convert_tensor_to_device input device input inputs _default_matcher_check_fn assertEqual counters inductor qlinear_weight_prepack_matcher_count _test_common mod inputs matcher_check_fn= matcher_check_fn matcher_check_fn None _default_matcher_check_fn check_autocast=torch bfloat int _mixed_bf torch float check_quantization=True is_qat=is_qat is_dynamic=is_dynamic quantization_with_autocast=quantization_with_autocast skipIfNoDynamoSupport skipIfNoONEDNN test_qlinear_cpu r This testcase will quantize single Linear Module bias True False _qlinear_test_helper torch randn bias=bias skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qlinear_xpu r This testcase will quantize single Linear Module bias True False _qlinear_test_helper torch randn device= xpu device= xpu bias=bias skipIfNoDynamoSupport skipIfNoONEDNN test_dynamic_qlinear_cpu r This testcase will quantize single Linear Module bias True False _qlinear_test_helper torch randn bias=bias is_dynamic=True skipIfNoDynamoSupport skipIfNoONEDNN test_dynamic_qlinear_qat_cpu r This testcase will quantize single Linear Module bias True False _qlinear_test_helper torch randn bias=bias is_dynamic=True is_qat=True skipIfNoDynamoSupport skipIfNoONEDNN test_dynamic_qlinear_input_dim_exceeds_ r This testcase will quantize single Linear Module bias True False _qlinear_test_helper torch randn bias=bias is_dynamic=True skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qlinear_int _mixed_bf r This testcase will quantize single Linear Module int _mixed_bf quantization bias True False _qlinear_test_helper torch randn int _mixed_bf =True bias=bias skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qlinear_int _mixed_bf _use_autocast r This testcase will quantize single Linear Module int _mixed_bf quantization bias True False _qlinear_test_helper torch randn int _mixed_bf =True bias=bias quantization_with_autocast=True skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoXPU test_qlinear_int _mixed_bf _xpu r This testcase will quantize single Linear Module int _mixed_bf quantization bias True False _qlinear_test_helper torch randn device= xpu device= xpu int _mixed_bf =True bias=bias skipIfNoDynamoSupport skipIfNoONEDNN test_qlinear_input_dim_exceeds_ r This testcase will quantize single Linear Module bias True False _qlinear_test_helper torch randn bias=bias skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qlinear_input_dim_exceeds_ _xpu r This testcase will quantize single Linear Module bias True False _qlinear_test_helper torch randn device= xpu device= xpu bias=bias skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qlinear_int _mixed_bf _input_dim_exceeds_ r This testcase will quantize single Linear Module int _mixed_bf quantization bias True False _qlinear_test_helper torch randn int _mixed_bf =True bias=bias skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qlinear_int _mixed_bf _input_dim_exceeds_ _use_autocast r This testcase will quantize single Linear Module int _mixed_bf quantization bias True False _qlinear_test_helper torch randn int _mixed_bf =True bias=bias quantization_with_autocast=True skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN skipIfNoXPU test_qlinear_int _mixed_bf _input_dim_exceeds_ _xpu r This testcase will quantize single Linear Module int _mixed_bf quantization bias True False _qlinear_test_helper torch randn device= xpu device= xpu int _mixed_bf =True bias=bias skipIfNoDynamoSupport skipIfNoONEDNN test_qlinear_input_dim_exceeds_ _and_not_contiguous r This testcase will quantize single Linear Module Input dim exceeds Input contiguous bias True False matcher_check_fn assertEqual counters inductor qlinear_weight_prepack_matcher_count assertEqual counters inductor qlinear_weight_prepack_matcher_nodes bias _qlinear_test_helper torch randn do_permute=True matcher_check_fn=matcher_check_fn bias=bias skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qlinear_int _mixed_bf _input_dim_exceeds_ _and_not_contiguous r This testcase will quantize single Linear Module int _bf Input dim exceeds Input contiguous bias True False matcher_check_fn assertEqual counters inductor qlinear_weight_prepack_matcher_count assertEqual counters inductor qlinear_weight_prepack_matcher_nodes bias _qlinear_test_helper torch randn int _mixed_bf =True do_permute=True matcher_check_fn=matcher_check_fn bias=bias skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qlinear_int _mixed_bf _input_dim_exceeds_ _and_not_contiguous_use_autocast r This testcase will quantize single Linear Module int _bf Input dim exceeds Input contiguous bias True False matcher_check_fn assertEqual counters inductor qlinear_weight_prepack_matcher_count assertEqual counters inductor qlinear_weight_prepack_matcher_nodes bias _qlinear_test_helper torch randn int _mixed_bf =True do_permute=True matcher_check_fn=matcher_check_fn bias=bias quantization_with_autocast=True skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN skipIfNoXPU test_qlinear_int _mixed_bf _input_dim_exceeds_ _and_not_contiguous_xpu r This testcase will quantize single Linear Module int _bf Input dim exceeds Input contiguous bias True False matcher_check_fn assertEqual counters inductor qlinear_weight_prepack_matcher_count assertEqual counters inductor qlinear_weight_prepack_matcher_nodes bias _qlinear_test_helper torch randn device= xpu device= xpu int _mixed_bf =True do_permute=True matcher_check_fn=matcher_check_fn bias=bias _qlinear_unary_test_helper inputs unary_op=torch nn ReLU device= cpu int _mixed_bf =False M torch nn Module __init__ use_bias super __init__ linear = torch nn Linear use_bias unary_fn = copy deepcopy unary_op linear = torch nn Linear use_bias unary_fn = copy deepcopy unary_op forward x tmp = unary_fn linear x unary_fn linear tmp bias_list = True False bias bias_list mod = M bias eval device=device matcher_check_fn dequant-linear pattern matched quantization weight prepack assertEqual counters inductor qlinear_weight_prepack_matcher_count QLinear Unary fusion post-grad fusion pass assertEqual counters inductor qlinear_unary_matcher_count TEST_ACL assertEqual counters inductor qlinear_unary_lower_count TEST_ACL _test_common mod inputs matcher_check_fn check_autocast=torch bfloat int _mixed_bf torch float check_quantization=True skipIfNoDynamoSupport skipIfNoONEDNN test_qlinear_relu_cpu r This testcase will quantize Linear- ReLU pattern _qlinear_unary_test_helper torch randn skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qlinear_relu_xpu r This testcase will quantize Linear- ReLU pattern _qlinear_unary_test_helper torch randn device= xpu device= xpu skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qlinear_relu_int _mixed_bf r This testcase will quantize Linear- ReLU pattern int _mixed_bf quantization _qlinear_unary_test_helper torch randn int _mixed_bf =True skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN skipIfNoXPU test_qlinear_relu_int _mixed_bf _xpu r This testcase will quantize Linear- ReLU pattern int _mixed_bf quantization _qlinear_unary_test_helper torch randn device= xpu device= xpu int _mixed_bf =True skipIfNoDynamoSupport skipIfNoONEDNN test_qlinear_relu_input_dim_exceeds_ r This testcase will quantize Linear- ReLU pattern _qlinear_unary_test_helper torch randn skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qlinear_relu_input_dim_exceeds_ _xpu r This testcase will quantize Linear- ReLU pattern _qlinear_unary_test_helper torch randn device= xpu device= xpu skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qlinear_relu_int _mixed_bf _input_dim_exceeds_ r This testcase will quantize Linear- ReLU pattern int _mixed_bf quantization _qlinear_unary_test_helper torch randn int _mixed_bf =True skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN skipIfNoXPU test_qlinear_relu_int _mixed_bf _input_dim_exceeds_ _xpu r This testcase will quantize Linear- ReLU pattern int _mixed_bf quantization _qlinear_unary_test_helper torch randn device= xpu device= xpu int _mixed_bf =True skipIfNoDynamoSupport skipIfNoONEDNN test_qlinear_gelu_cpu r This testcase will quantize Linear- GELU pattern gelu torch nn GELU none torch nn GELU tanh _qlinear_unary_test_helper torch randn gelu skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qlinear_gelu_xpu r This testcase will quantize Linear- GELU pattern gelu torch nn GELU none torch nn GELU tanh _qlinear_unary_test_helper torch randn device= xpu gelu device= xpu skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qlinear_gelu_int _mixed_bf r This testcase will quantize Linear- GELU pattern int _mixed_bf quantization gelu torch nn GELU none torch nn GELU tanh _qlinear_unary_test_helper torch randn gelu int _mixed_bf =True skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN skipIfNoXPU test_qlinear_gelu_int _mixed_bf _xpu r This testcase will quantize Linear- GELU pattern int _mixed_bf quantization gelu torch nn GELU none torch nn GELU tanh _qlinear_unary_test_helper torch randn device= xpu gelu device= xpu int _mixed_bf =True _qlinear_add_test_helper device= cpu use_relu=False int _mixed_bf =False is_qat=True is_dynamic=True r This testcase will quantize two consecutive Linear- Add - relu patterns X \ linear X linear X \ Add &#124; Optional relu \ linear X linear X \ Add &#124; Optional relu &#124; Y fake_quant x produce float result extra input qlib = torch ops quantized_decomposed device == cpu qmin qmax dtype = torch uint qmin qmax dtype = - torch int x = qlib quantize_per_tensor default x qmin qmax dtype x = qlib dequantize_per_tensor default x qmin qmax dtype x M torch nn Module __init__ add_fn use_relu fake_quant_before_extra_input super __init__ linear = torch nn Linear linear = torch nn Linear add_fn = add_fn relu = torch nn ReLU linear = torch nn Linear linear = torch nn Linear add_fn = add_fn relu = torch nn ReLU use_relu = use_relu fake_quant_before_extra_input = fake_quant_before_extra_input forward x x = linear x x = linear x fake_quant_before_extra_input x = fake_quant x tmp = add_fn x x use_relu tmp = relu tmp tmp = linear tmp tmp = linear tmp fake_quant_before_extra_input tmp = fake_quant tmp res = add_fn tmp tmp use_relu res = relu res res add_fn_list = lambda x y x + y lambda x y y + x lambda x y x add_ y lambda x y y add_ x fake_quant_x _list = False True int _mixed_bf False shape_list = cases = itertools product add_fn_list fake_quant_x _list shape_list add_fn fq_x shape cases mod = M add_fn use_relu fq_x eval device=device v = torch randn shape dtype=torch float requires_grad=False device=device add matcher_check_fn Dequant-linear pattern matched quantization weight prepack assertEqual counters inductor qlinear_weight_prepack_matcher_count pattern = dequant_per_tensor convert_dtype dequant_per_channel convert_dtype permute addmm nodes_per_match = int _mixed_bf len shape == pattern = dequant_per_tensor convert_dtype view \ dequant_per_channel convert_dtype view permute addmm nodes_per_match += assertEqual counters inductor qlinear_weight_prepack_matcher_nodes nodes_per_match Qlinear Binary Unary fusion post-grad fusion pass assertEqual counters inductor qlinear_binary_matcher_count TEST_ACL Two linear-binary patterns matched matched patter = qlinear add convert dtype relu quantize_per_tensor matched patter = qlinear add convert dtype relu If add_fn x add_ y x bf y fp there to_bf node after binary to_bf _after_binary = add_fn == add_fn_list fq_x expected_matcher_nodes = is_dynamic + use_relu + to_bf _after_binary assertEqual counters inductor qlinear_binary_matcher_nodes TEST_ACL expected_matcher_nodes assertEqual counters inductor qlinear_binary_lower_count TEST_ACL _test_common mod v matcher_check_fn check_quantization=True check_autocast=torch bfloat int _mixed_bf torch float is_qat=is_qat is_dynamic=is_dynamic TEST_ACL continue torch _inductor config cpp_wrapper For CPP wrapper _test_code_common mod v f aoti_torch_ device __qlinear_pointwise_tensor f aoti_torch_ device __qlinear_pointwise_binary_tensor check_quantization=True num_include_ops= For python wrapper _test_code_common mod v torch ops onednn qlinear_pointwise tensor torch ops onednn qlinear_pointwise binary check_quantization=True num_include_ops= skipIfNoDynamoSupport skipIfNoONEDNN parametrize use_relu True False parametrize is_qat True False parametrize is_dynamic True False test_qlinear_add_cpu use_relu is_qat is_dynamic _qlinear_add_test_helper use_relu=use_relu is_qat=is_qat is_dynamic=is_dynamic skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU config patch fx_graph_cache False parametrize use_relu True parametrize is_qat False parametrize is_dynamic False test_qlinear_add_xpu use_relu is_qat is_dynamic _qlinear_add_test_helper device= xpu use_relu=use_relu is_qat=is_qat is_dynamic=is_dynamic skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN parametrize use_relu True False parametrize is_qat True False parametrize is_dynamic True False test_qlinear_add_int _mixed_bf use_relu is_qat is_dynamic _qlinear_add_test_helper int _mixed_bf =True use_relu=use_relu is_qat=is_qat is_dynamic=is_dynamic skipIfNoXPU parametrize use_relu True False parametrize is_qat False parametrize is_dynamic False test_qlinear_add_int _mixed_bf _xpu use_relu is_qat is_dynamic _qlinear_add_test_helper device= xpu int _mixed_bf =True use_relu=use_relu is_qat=is_qat is_dynamic=is_dynamic _test_qlinear_fp _inductor_cpu_helper qlinear_op post_op= none dtype = torch float _e m fn qlinear_prepack = torch ops onednn qlinear_prepack post_op_algo = none unary_post_op_args = batch_size = output_dtype = torch float _e m fn y_scale y_zp = ic = oc = torch _dynamo reset used_y_scale = y_scale used_y_zp = y_zp x = torch rand batch_size ic w = torch rand oc ic qx = x dtype qw = w dtype x_scale = w_scales = torch randn oc b = torch rand oc x_zp = w_zps = torch zeros_like w_scales dtype=torch int post_op == none Mod torch nn Module __init__ super __init__ qw_packed = qlinear_prepack qw x shape forward qx qy = qlinear_op qx x_scale x_zp qw_packed w_scales w_zps b used_y_scale used_y_zp output_dtype post_op unary_post_op_args post_op_algo qy post_op == add x = torch rand batch_size oc binary_alpha = we only support alpha= now Mod torch nn Module __init__ super __init__ qw_packed = qlinear_prepack qw x shape forward qx qy = qlinear_op qx x_scale x_zp qw_packed w_scales w_zps x b used_y_scale used_y_zp output_dtype add binary_alpha none unary_post_op_args post_op_algo qy torch no_grad model = Mod y_refe = model qx y_test = torch compile model qx assertEqual y_refe float y_test float skipIfNoONEDNN test_qlinear_fp _inductor_cpu qlinear_op = torch ops onednn qlinear_pointwise default _test_qlinear_fp _inductor_cpu_helper qlinear_op none skipIfNoONEDNN test_qlinear_add_fp _inductor_cpu qlinear_op = torch ops onednn qlinear_pointwise binary _test_qlinear_fp _inductor_cpu_helper qlinear_op add _qlinear_dequant_promotion_test_helper inputs device= cpu int _mixed_bf =False is_dynamic=False matcher_check_fn=None M torch nn Module __init__ kwargs super __init__ linear = torch nn Linear linear = torch nn Linear linear = torch nn Linear forward x temp = linear x temp = linear temp + linear temp temp mod = M eval device=device default_matcher_check_fn Dequant pattern matcher dequant promotion assertEqual counters inductor dequant_promotion_matcher_count dequant-linear pattern matched quantization weight prepack assertEqual counters inductor qlinear_weight_prepack_matcher_count QLinear Unary fusion post-grad fusion pass assertEqual counters inductor qlinear_unary_matcher_count TEST_ACL _test_common mod inputs matcher_check_fn= matcher_check_fn matcher_check_fn None default_matcher_check_fn check_autocast=torch bfloat int _mixed_bf torch float check_quantization=True is_dynamic=is_dynamic skipIfNoDynamoSupport skipIfNoONEDNN test_qlinear_dequant_promotion_cpu r This testcase test dequant node before linear promoted correctly X &#124; Linear X \ Linear X Linear X \ Add &#124; Y _qlinear_dequant_promotion_test_helper torch randn skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qlinear_dequant_promotion_xpu r This testcase test dequant node before linear promoted correctly X &#124; Linear X \ Linear X Linear X \ Add &#124; Y _qlinear_dequant_promotion_test_helper torch randn device= xpu device= xpu skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qlinear_dequant_promotion_int _mixed_bf r Test int _mixed_bf quantization This testcase test dequant node before linear promoted correctly X &#124; Linear X \ Linear X Linear X \ Add &#124; Y _qlinear_dequant_promotion_test_helper torch randn int _mixed_bf =True skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN skipIfNoXPU test_qlinear_dequant_promotion_int _mixed_bf _xpu r Test int _mixed_bf quantization This testcase test dequant node before linear promoted correctly X &#124; Linear X \ Linear X Linear X \ Add &#124; Y _qlinear_dequant_promotion_test_helper torch randn device= xpu device= xpu int _mixed_bf =True skipIfNoDynamoSupport skipIfNoONEDNN test_qlinear_dequant_promotion_cpu_input_dim_exceeds_ r This testcase test dequant node before linear promoted correctly X &#124; Linear X \ Linear X Linear X \ Add &#124; Y _qlinear_dequant_promotion_test_helper torch randn skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qlinear_dequant_promotion_input_dim_exceeds_ _xpu r This testcase test dequant node before linear promoted correctly X &#124; Linear X \ Linear X Linear X \ Add &#124; Y _qlinear_dequant_promotion_test_helper torch randn device= xpu device= xpu skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN test_qlinear_dequant_promotion_int _mixed_bf _input_dim_exceeds_ r Test int _mixed_bf quantization This testcase test dequant node before linear promoted correctly X &#124; Linear X \ Linear X Linear X \ Add &#124; Y _qlinear_dequant_promotion_test_helper torch randn int _mixed_bf =True skipIfNoDynamoSupport skipIfNoONEDNNBF skipIfNoONEDNN skipIfNoXPU test_qlinear_dequant_promotion_int _mixed_bf _input_dim_exceeds_ _xpu r Test int _mixed_bf quantization This testcase test dequant node before linear promoted correctly X &#124; Linear X \ Linear X Linear X \ Add &#124; Y _qlinear_dequant_promotion_test_helper torch randn device= xpu device= xpu int _mixed_bf =True skipIfNoDynamoSupport skipIfNoONEDNN test_qlinear_dequant_promotion_dynamic_cpu r This testcase test dequant node before linear promoted correctly X &#124; Linear X \ Linear X Linear X \ Add &#124; Y matcher_check_fn Dequant pattern matcher dequant promotion assertEqual counters inductor dequant_promotion_matcher_count dequant-linear pattern matched quantization weight prepack assertEqual counters inductor qlinear_weight_prepack_matcher_count _qlinear_dequant_promotion_test_helper torch randn matcher_check_fn=matcher_check_fn is_dynamic=True skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU config patch fx_graph_cache False test_qlinear_mul_xpu r This testcase will quantize Linear- Mul pattern M torch nn Module __init__ use_bias super __init__ linear = torch nn Linear use_bias forward x x torch mul linear x x bias_list = True False bias bias_list mod = M bias eval device= xpu x = torch randn device= xpu x = torch randn device= xpu matcher_check_fn assertEqual counters inductor qlinear_weight_prepack_matcher_count _test_common mod x x check_quantization=True matcher_check_fn=matcher_check_fn skipIfNoDynamoSupport skipIfNoONEDNN test_qlinear_mul_cpu r This testcase will quantize Linear- Mul pattern M torch nn Module __init__ use_bias super __init__ linear = torch nn Linear use_bias forward x x torch mul linear x x bias_list = True False bias bias_list mod = M bias eval x = torch randn x = torch randn matcher_check_fn assertEqual counters inductor qlinear_weight_prepack_matcher_count _test_common mod x x matcher_check_fn check_quantization=True skipIfNoDynamoSupport skipIfNoONEDNN skipIfNoXPU test_qlinear_mul r This testcase will quantize Linear- Mul pattern M torch nn Module __init__ use_bias super __init__ linear = torch nn Linear use_bias forward x x torch mul linear x x bias_list = True False bias bias_list mod = M bias eval device= xpu x = torch randn device= xpu x = torch randn device= xpu matcher_check_fn assertEqual counters inductor qlinear_weight_prepack_matcher_count _test_common mod x x check_quantization=True matcher_check_fn=matcher_check_fn skipIfNoDynamoSupport test_qmaxpool d r This testcase will quantize Conv d- ReLU- MaxPool d pattern M torch nn Module __init__ kwargs super __init__ conv = torch nn Conv d bias=True stride= padding= dilation= relu = torch nn ReLU maxpool = torch nn MaxPool d kwargs forward x maxpool relu conv x kwargs_list = stride stride padding stride padding dilation stride padding dilation ceil_mode False kwargs kwargs_list mod = M kwargs eval v = torch randn dtype=torch float requires_grad=False add matcher_check_fn assertEqual counters inductor qmaxpool d_matcher_count TEST_ACL assertEqual counters inductor qconv_weight_prepack_matcher_count assertEqual counters inductor qconv_unary_matcher_count TEST_ACL assertEqual counters inductor qconv_unary_lower_count TEST_ACL _test_common mod v matcher_check_fn check_quantization=True skipIfNoDynamoSupport test_qflatten r This testcase will quantize Conv d- AdaptiveAvgPool d- flatten- cat pattern M torch nn Module __init__ super __init__ conv = torch nn Conv d bias=True stride= padding= dilation= relu = torch nn ReLU adaptive_avg_pool d = torch nn AdaptiveAvgPool d forward x torch cat torch flatten adaptive_avg_pool d relu conv x mod = M eval v = torch randn dtype=torch float requires_grad=False add matcher_check_fn assertEqual counters inductor qreshape_matcher_count TEST_ACL _test_common mod v matcher_check_fn check_quantization=True skipIfNoDynamoSupport test_qcat r This testcase will quantize cat based pattern X \ Conv X Pow x \ \ \ Conv X \ Cat &#124; Y M torch nn Module __init__ super __init__ conv = torch nn Conv d bias=True stride= padding= dilation= conv = torch nn Conv d bias=True stride= padding= dilation= forward x temp = conv x temp = conv torch pow x torch cat temp temp mod = M eval v = torch randn dtype=torch float requires_grad=False add matcher_check_fn assertEqual counters inductor qcat_matcher_count TEST_ACL assertEqual counters inductor qconv_weight_prepack_matcher_count assertEqual counters inductor qconv_unary_matcher_count TEST_ACL assertEqual counters inductor qconv_unary_lower_count TEST_ACL _test_common mod v matcher_check_fn check_quantization=True https github com pytorch pytorch issues test_hardtanh_pattern_fallback Model torch nn Module __init__ - None super __init__ conv_transpose = torch nn ConvTranspose d in_channels= out_channels= kernel_size= stride= padding= forward x min_value max_value conv_transpose_output = conv_transpose x clamp_min_output = torch clamp_min conv_transpose_output min_value clamp_max_output = torch clamp_max clamp_min_output max_value clamp_max_output check works min_value max_value min_values = torch randn max_values = torch randn v = torch randn matcher_check_fn assertEqual counters inductor mkldnn_unary_fusion_matcher_nodes TEST_ACL assertEqual counters inductor mkldnn_conv_weight_pack_matcher_count min_value max_value zip min_values max_values mod = Model eval _test_common mod v min_value max_value matcher_check_fn test_leaky_relu_pattern_fallback Model torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= forward x negative_slope conv_out = conv x torch where conv_out conv_out conv_out negative_slope negative_slopes = torch randn matcher_check_fn assertEqual counters inductor mkldnn_unary_fusion_matcher_nodes TEST_ACL assertEqual counters inductor mkldnn_conv_weight_pack_matcher_count torch no_grad v = torch randn negative_slope negative_slopes mod = Model eval _test_common mod v negative_slope matcher_check_fn https github com pytorch pytorch issues test_conv d_add_scalar Model torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= forward x out_conv = conv x out = torch add out_conv out matcher_check_fn assertEqual counters inductor binary_folding assertEqual counters inductor mkldnn_conv_weight_pack_matcher_count torch no_grad mod = Model eval v = torch randn _test_common mod v matcher_check_fn xfailIfACL test_conv d_binary_inplace_fusion_pass_cpu include_ops=None exclude_ops=None Model_v torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= forward x other conv_out = conv x torch add conv_out other relu Model_v torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= forward x _ conv_out = conv x pow_out = torch pow conv_out conv_out = conv pow_out conv_out = conv conv_out res = torch add conv_out pow_out res input = torch randn memory_format=torch channels_last others = torch randn memory_format=torch channels_last torch randn memory_format=torch channels_last mod_v = Model_v memory_format=torch channels_last eval mod_v = Model_v memory_format=torch channels_last eval include_ops None include_ops = mkldnn _convolution_pointwise_ binary exclude_ops None exclude_ops = mkldnn _convolution_pointwise binary other mod zip others mod_v mod_v _test_code_common mod input other include_ops exclude_ops xfailIfACL test_conv d_binary_inplace_fusion_failed_cpu include_ops=None exclude_ops=None Written buffer graph input we can t fuse inplace Model_v torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= forward x other conv_out = conv x torch add conv_out other Written buffer alias tensor we can t fuse inplace Model_v torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= forward x other conv_out = conv x torch add conv_out other other Model_v torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= forward x _ pow_out = torch pow conv x other = F relu pow_out conv_out = conv pow_out res = torch add conv_out pow_out res = res + other res Written buffer ReinterpretView we can t fuse inplace Model_v torch nn Module __init__ - None super __init__ conv = torch nn Conv d padding= bias=True linear = torch nn Linear relu = torch nn ReLU forward x y x = conv relu x y = linear y y = torch cat y y + y = torch ops aten permute default y reshape x + y Model_v torch nn Module __init__ - None super __init__ conv = torch nn Conv d padding= bias=True relu = torch nn ReLU forward _ x x = relu x conv x + x input = torch randn memory_format=torch channels_last others = torch randn memory_format=torch channels_last torch randn memory_format=torch channels_last torch randn memory_format=torch channels_last torch randn torch randn memory_format=torch channels_last mod_v = Model_v memory_format=torch channels_last eval mod_v = Model_v memory_format=torch channels_last eval mod_v = Model_v memory_format=torch channels_last eval mod_v = Model_v memory_format=torch channels_last eval mod_v = Model_v memory_format=torch channels_last eval include_ops None include_ops = mkldnn _convolution_pointwise binary exclude_ops None exclude_ops = mkldnn _convolution_pointwise_ binary other mod zip others mod_v mod_v mod_v mod_v mod_v _test_code_common mod input other include_ops exclude_ops test_conv d_binary_fusion_failed we don t support alpha = case other has different size conv s output Model torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= forward x other alpha conv_out = conv x torch add conv_out other alpha=alpha https github com pytorch pytorch issues we can t do fusion when add s inputs same tensor Model torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= forward x out = conv x out = torch add out out out https github com pytorch pytorch issues we can t do fusion when add s inputs mixed dtype Model torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= forward x temp = conv x other = torch ones temp shape dtype=torch double out = torch add temp other out input = torch randn memory_format=torch channels_last others = torch randn memory_format=torch channels_last torch randn include_ops = mkldnn _convolution_pointwise exclude_ops = mkldnn _convolution_pointwise binary mkldnn _convolution_pointwise_ binary case other alpha zip others mod = Model memory_format=torch channels_last eval _test_code_common mod input other alpha include_ops exclude_ops case mod = Model memory_format=torch channels_last eval _test_code_common mod input include_ops exclude_ops case mod = Model memory_format=torch channels_last eval _test_code_common mod input include_ops exclude_ops xfailIfACL test_reproduce_ _issue Model torch nn Module __init__ - None super __init__ conv = torch nn Conv d kernel_size= stride= padding= forward input_tensor x = conv input_tensor x = F relu x + torch ones x size x input = torch randn mod = Model eval include_ops = mkldnn _convolution_pointwise_ binary _test_code_common mod input include_ops test_reproduce_ _issue_ Mod torch nn Module __init__ add_fn kwargs super __init__ conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= add_fn = add_fn relu = torch nn ReLU inplace=True conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= add_fn = add_fn relu = torch nn ReLU inplace=True use_relu = True forward x x = conv x x = conv x tmp = add_fn x x use_relu tmp = relu tmp tmp = conv tmp tmp = conv tmp res = add_fn tmp tmp use_relu res = relu res res torch no_grad example_inputs = torch randn dtype=torch float requires_grad=False add example_inputs get_device m = Mod lambda x y x add_ y eval om = torch compile m om example_inputs om example_inputs test_reproduce_ _issue_ Mod torch nn Module __init__ add_fn kwargs super __init__ conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= add_fn = add_fn relu = torch nn ReLU inplace=True conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= add_fn = add_fn relu = torch nn ReLU inplace=True conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= conv = torch nn Conv d kernel_size= stride= add_fn = add_fn relu = torch nn ReLU inplace=True use_relu = True forward x x = conv x x = conv x tmp = add_fn x x use_relu tmp = relu tmp tmp = conv tmp res = relu tmp res torch no_grad example_inputs = torch randn dtype=torch float requires_grad=False add m = Mod lambda x y x add_ y eval om = torch compile m om example_inputs om example_inputs unittest skipIf TEST_MKL Test requires MKL xfailIfACL torch _dynamo config patch inline_inbuilt_nn_modules True test_reproduce_ _issue_addmm_fusion_check Mod torch nn Module __init__ weight bias beta alpha super __init__ weight = weight bias = bias beta = beta alpha = alpha forward x torch addmm bias x weight beta=self beta alpha=self alpha dtypes = torch float torch ops mkldnn _is_mkldnn_bf _supported dtypes append torch bfloat dtype dtypes linear_op = mkl _mkl_linear dtype == torch float mkldnn _linear_pointwise beta alpha zip weight = torch nn Parameter torch randn dtype=dtype bias = torch nn Parameter torch randn dtype=dtype mod = Mod weight bias beta alpha dtype eval torch no_grad x = torch randn dtype=dtype include_ops = exclude_ops = beta = beta = alpha = exclude_ops = linear_op include_ops = linear_op _test_code_common mod x include_ops exclude_ops skipIfNoDynamoSupport test_woq_int M torch nn Module __init__ is_permute super __init__ is_permute = is_permute forward x weight scales is_permute weight = weight t m = torch mm x reshape - x shape - weight x dtype y = m scales m dtype y = y reshape x shape - y shape - y torch nn functional linear x weight dtype=x dtype scales x_shape = s_shape = x_strides = linear dispatching mm linear dispatching bmm is_permutes = False True x_stride is_permute itertools product x_strides is_permutes mod = M is_permute=is_permute eval x = torch randn x_shape dtype=torch bfloat as_strided x_shape x_stride w_shape = w = torch randint - w_shape dtype=torch int s = torch randn s_shape dtype=torch bfloat matcher_check_fn assertEqual counters inductor woq_matcher_count TEST_ACL _test_common mod x w s matcher_check_fn check_quantization=False atol= rtol= skipIfNoDynamoSupport test_woq_int _cpu M torch nn Module __init__ in_feature out_feature group_size super __init__ weight = torch randint out_feature in_feature dtype=torch uint group_size = group_size qScaleAndZeros = torch rand in_feature group_size out_feature dtype=torch bfloat forward x x ndim x = x reshape - x shape - y = torch ops aten _weight_int pack_mm_for_cpu default x weight group_size qScaleAndZeros y reshape x shape - y shape - torch ops aten _weight_int pack_mm_for_cpu default x weight group_size qScaleAndZeros bs = seq = x_dim_list = in_feature_list = out_feature_list = group_size_list = cases = itertools product x_dim_list in_feature_list out_feature_list group_size_list x_dim in_feature out_feature group_size cases x_shape = seq in_feature x_dim == bs seq in_feature x = torch randn x_shape dtype=torch bfloat m = M in_feature out_feature group_size eval matcher_check_fn assertEqual counters inductor woq_matcher_count TEST_ACL include_ops = aoti_torch_cpu__weight_int pack_mm_cpu_tensor torch _inductor config cpp_wrapper torch ops quantized int mm_packed_weight_cpu default _test_code_common m x include_ops torch ops aten _weight_int pack_mm_for_cpu default _test_linear_dynamic_fp _helper use_relu bool M torch nn Module __init__ bias bool use_relu bool super __init__ linear = torch nn Linear bias=bias relu = torch nn ReLU use_relu = use_relu forward x use_relu relu linear x linear x quantizer = X InductorQuantizer set_global xiq get_default_x _inductor_quantization_config quantizer set_module_type_qconfig torch nn Linear xiq get_x _inductor_linear_dynamic_fp _config bias_list = True False input_ndim_list = x_contig_list = True False cases = itertools product bias_list input_ndim_list x_contig_list bias input_ndim x_contig cases x_shape = input_ndim == x = torch randn x_shape x_contig x = x mod = M bias use_relu eval matcher_check_fn assertEqual counters inductor qlinear_weight_prepack_matcher_count Matched nodes w fp w fp permute w mm addmm bmm If x ndim == x contiguous two view nodes added If x ndim == x contiguous two expand nodes one add node added nodes_count = input_ndim x_contig nodes_count += nodes_count += bias use_relu nodes_count += assertEqual counters inductor qlinear_weight_prepack_matcher_nodes nodes_count _test_common mod x atol= e- rtol= e- matcher_check_fn=matcher_check_fn check_quantization=True quantizer=quantizer linear_op_str = torch ops onednn linear_relu_dynamic_fp default use_relu torch ops onednn linear_dynamic_fp default _test_code_common mod x linear_op_str torch ops aten addmm default torch ops aten mm default check_quantization=True quantizer=quantizer skipIfNoDynamoSupport skipIfNoONEDNN test_linear_dynamic_fp _test_linear_dynamic_fp _helper use_relu=False skipIfNoDynamoSupport skipIfNoONEDNN test_linear_relu_dynamic_fp _test_linear_dynamic_fp _helper use_relu=True skipIfNoDynamoSupport skipIfNoONEDNN TODO investigate options torch compile fbcode unittest skipIf IS_FBCODE Failing fbcode parametrize has_bias True False parametrize dtype torch float torch bfloat parametrize per_channel_quant True False parametrize dynamic True False test_smooth_quant_with_int_mm has_bias dtype per_channel_quant dynamic r This testcase check we can match SmoothQuant int linear pattern Torchao The pattern no bias reshape - _int_mm - convert_element_type - expand - mul - mul - reshape bias pattern_no_bias - add - reshape - reshape dtype == torch bfloat torch ops mkldnn _is_mkldnn_bf _supported M = in_feature = out_feature = q_min q_max = - Mod torch nn Module __init__ dtype torch dtype has_bias bool per_channel_quant bool super __init__ dtype = dtype has_bias = has_bias b = torch randint q_min q_max in_feature out_feature dtype=torch int per_channel_quant = per_channel_quant a_scale_per_tensor = torch rand dtype=dtype + a_scale_per_channel = torch rand M dtype=dtype + a_scale = a_scale_per_channel per_channel_quant a_scale_per_tensor b_scale = torch rand out_feature + b_scale = b_scale dtype bias = torch rand out_feature dtype=dtype has_bias None forward out_shape = shape - + b size - a_reshaped = reshape - size - c = torch _int_mm a_reshaped b c = c dtype c_shape = c shape a_scale = a_scale expand c shape c = c a_scale c = c b_scale has_bias c = c reshape list c_shape c = c + bias c = c reshape c_shape c = c reshape out_shape c mod = Mod dtype has_bias per_channel_quant eval = torch randint q_min q_max M in_feature dtype=torch int matcher_check_fn assertEqual counters inductor qlinear_weight_prepack_matcher_count dynamic nodes_count = has_bias nodes_count = has_bias counters inductor removed_pointless_view_pair == Removing pointless view pairs affect how pattern test matched assertEqual counters inductor qlinear_weight_prepack_matcher_nodes nodes_count _test_common mod matcher_check_fn=matcher_check_fn check_autocast=dtype compile_options= dynamic dynamic skipIfNoDynamoSupport skipIfNoONEDNN TODO investigate options torch compile fbcode unittest skipIf IS_FBCODE Failing fbcode parametrize has_bias True False parametrize dtype torch float torch bfloat parametrize dynamic True False parametrize reshape_a True False parametrize M parametrize inplace_add True False parametrize expand_a_scale True False test_da w _sym_act_sym_wgt_with_int_mm has_bias dtype dynamic reshape_a M inplace_add expand_a_scale r This testcase check we can match int _dynamic_activation_int _weight int linear pattern torchao when activation symmetrically quantized dynamically weights symmetrically quantized statically The pattern no bias _int_mm - convert_element_type - expand_a - mul - mul bias pattern_no_bias - add Expansion scale activation optional The pattern depiction doesn t mean convert_element_type output fed into expand_a input simply activation scale may applied after expand operation dtype == torch bfloat torch ops mkldnn _is_mkldnn_bf _supported in_feature = out_feature = q_min q_max = - we only test qlinear_binary case test_for_pointwise_binary = bool M == inplace_add expand_a_scale dynamic has_bias test_for_pointwise_binary IS_X skipTest Some UTs only supported x _ CPUs Mod torch nn Module __init__ dtype torch dtype has_bias bool super __init__ dtype = dtype has_bias = has_bias b = torch randint q_min q_max in_feature out_feature dtype=torch int a_scale = torch rand M dtype=dtype + b_scale = torch rand out_feature + b_scale = b_scale dtype bias = torch rand out_feature dtype=dtype has_bias None additive = torch rand M out_feature dtype=dtype forward reshape_a a_reshaped = reshape - size - a_reshaped = c = torch _int_mm a_reshaped b c = c dtype expand_a_scale a_scale = a_scale expand c shape a_scale = a_scale c = c a_scale c = c b_scale has_bias c = c + bias inplace_add test_for_pointwise_binary When M dynamic shapes enabled torch compile has_bias False expand_a_scale False inplace_add true output s outermost dim s stride can t determined due some Inductor bug c add_ additive c mod = Mod dtype has_bias eval = torch randint q_min q_max M in_feature dtype=torch int matcher_check_fn assertEqual counters inductor qlinear_weight_prepack_matcher_count _test_common mod matcher_check_fn check_autocast=dtype compile_options= dynamic dynamic test_for_pointwise_binary assertEqual counters inductor qlinear_binary_matcher_count TestDynamicPatternMatcherGeneric TestPatternMatcherBase setUp super setUp ctx_stack enter_context When testing kernel counts unspecializing float causes wobbling our tests because we end up reusing same compiled region across tests Thus we purposely specialize floats here since we primarily care about number kernels generated absence compile caching dynamo_config patch dynamic_shapes True assume_static_by_default False specialize_float True _test_conv_unary_base = TestPatternMatcherGeneric _test_conv_unary_base test_conv d_unary_dynamic_shapes = TestPatternMatcherGeneric test_conv d_unary test_conv d_unary_dynamic_shapes = TestPatternMatcherGeneric test_conv d_unary _test_conv_binary_base = TestPatternMatcherGeneric _test_conv_binary_base test_conv d_binary_dynamic_shapes = TestPatternMatcherGeneric test_conv d_binary test_conv d_binary_dynamic_shapes = TestPatternMatcherGeneric test_conv d_binary test_conv_transpose d_dynamic_shapes device device = device We don t support conv_transpose d now M torch nn Module __init__ - None super __init__ conv_transpose d = torch nn ConvTranspose d stride= padding= forward x conv_transpose d x x_shape = mod = M eval v = torch randn x_shape dtype=torch float matcher_check_fn _test_common mod v matcher_check_fn skipIfXpu msg= Different CPU two linears will concat XPU better performance test_multi_linear_share_same_input_dynamic device device = device llama pattern M torch nn Module __init__ super __init__ w = torch nn Linear bias=False w = torch nn Linear bias=False forward x F silu w x F relu w x dtypes = is_mkldnn_bf _supported device dtypes append torch bfloat is_mkldnn_fp _supported device dtypes append torch float matcher_check_fn assertEqual counters inductor mkldnn_unary_fusion_matcher_nodes TEST_ACL assertEqual counters inductor mkldnn_unary_fusion_matcher_count TEST_ACL assertEqual counters inductor mkldnn_reshape_linear_reshape_matcher_nodes assertEqual counters inductor mkldnn_reshape_linear_reshape_matcher_count assertEqual counters inductor mkldnn_linear_weight_pack_matcher_count dtype dtypes mod = M dtype eval v = torch randn dtype _test_common mod v matcher_check_fn rtol= e- atol= e- TestDynamicPatternMatcher TestPatternMatcherBase test_linear_unary_dynamic_shapes = TestPatternMatcher test_linear_unary test_linear_input_non_contiguous_ D_wo_bias_dynamic_shapes = TestPatternMatcher test_linear_input_non_contiguous_ D_wo_bias setUp super setUp ctx_stack enter_context When testing kernel counts unspecializing float causes wobbling our tests because we end up reusing same compiled region across tests Thus we purposely specialize floats here since we primarily care about number kernels generated absence compile caching dynamo_config patch dynamic_shapes True assume_static_by_default False specialize_float True xfailIfACL test_qconv d_maxpool d_linear_dynamic_cpu include_ops=None r This testcase will quantize single Conv d- Maxpool d- Linear module dynamic batch size input M torch nn Module __init__ kwargs super __init__ conv = torch nn Conv d stride= padding= relu = torch nn ReLU maxpool d = torch nn MaxPool d kernel_size= stride= padding= avgpool = torch nn AdaptiveAvgPool d linear = torch nn Linear forward x temp = relu conv x temp = maxpool d temp temp = avgpool temp temp = torch flatten temp linear temp mod = M eval v = torch randn dtype=torch float requires_grad=False add include_ops None include_ops = torch ops onednn qconv_pointwise torch ops quantized max_pool d torch ops onednn qlinear_pointwise exclude_ops = _test_code_common mod v include_ops exclude_ops check_quantization=True check_dynamic=True skipIfNoDynamoSupport skipIfNoONEDNN test_qat_bn_conv d r This testcase will quantize single BN Conv d module qat flow M torch nn Module __init__ super __init__ conv = torch nn Conv d bn = torch nn BatchNorm d bn = torch nn BatchNorm d forward x x = conv bn x bn x mod = M train v = torch randn dtype=torch float requires_grad=True add matcher_check_fn assertEqual counters inductor qconv_weight_prepack_matcher_count _test_common mod v matcher_check_fn check_quantization=True is_qat=True skipIfNoDynamoSupport skipIfNoONEDNN test_q_attention_block SelfAttnLikeModule torch nn Module __init__ input_dim num_attention_heads=None attention_head_size=None - None super __init__ input_dim = input_dim q_proj = torch nn Linear input_dim input_dim bias=False k_proj = torch nn Linear input_dim input_dim bias=False v_proj = torch nn Linear input_dim input_dim bias=False softmax = torch nn Softmax dim=- num_attention_heads = num_attention_heads attention_head_size = attention_head_size all_head_size = num_attention_heads attention_head_size dense = torch nn Linear all_head_size all_head_size transpose_for_scores x torch Tensor - torch Tensor new_x_shape = x size - + num_attention_heads attention_head_size x = x view new_x_shape x permute forward x q = q_proj x k = k_proj x v = v_proj x q = transpose_for_scores q k = transpose_for_scores k v = transpose_for_scores v scores = torch matmul q k transpose - - input_dim attention = softmax scores weighted = torch matmul attention v weighted = weighted permute contiguous weighted = weighted reshape weighted size - + all_head_size dense weighted annotate_matmul True False mod = SelfAttnLikeModule input_dim= num_attention_heads= attention_head_size= eval v = torch randn matcher_check_fn assertEqual counters inductor qlinear_weight_prepack_matcher_count assertEqual counters inductor qlinear_unary_matcher_count annotate_matmul TEST_ACL IS_X Some issues ARM assertEqual counters inductor quant_lift_up_count annotate_matmul TEST_ACL quantizer = X InductorQuantizer quantizer set_global xiq get_default_x _inductor_quantization_config annotate_matmul quantizer set_function_type_qconfig torch matmul quantizer get_global_quantization_config _test_common mod v matcher_check_fn check_quantization=True quantizer=quantizer instantiate_device_type_tests TestPatternMatcherGeneric globals allow_xpu=True only_for= cpu xpu instantiate_device_type_tests TestDynamicPatternMatcherGeneric globals allow_xpu=True only_for= cpu xpu instantiate_parametrized_tests TestPatternMatcher __name__ == __main__ IS_LINUX HAS_CPU torch backends mkldnn is_available run_tests