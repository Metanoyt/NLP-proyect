Owner s oncall distributed contextlib itertools sys dataclasses dataclass typing Any Optional torch torch distributed dist torch distributed fsdp CPUOffload FullyShardedDataParallel FSDP torch distributed fsdp fully_sharded_data_parallel BackwardPrefetch ShardingStrategy torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp DEVICEInitMode FSDPInitMode FSDPTest TransformerWithSharedParams torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests TEST_WITH_DEV_DBG_ASAN dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit dataclass _GradAccConfig This configures how gradients accumulated meth ` _test_grad_acc ` Each instance represents ` ` num_iters ` ` -many consecutive iterations where ` ` no_sync ` ` context manager used given ` ` use_no_sync ` ` Attributes use_no_sync bool Indicates whether use ` ` no_sync ` ` context manager way accumulate gradients num_iters int Number iterations accumulate gradients use_no_sync bool num_iters int __repr__ - str Override remove any spaces string appease internal build s test name parser f use_no_sync= use_no_sync num_iters= num_iters dataclass _GradAccConfigs This wraps ` list ` ` _GradAccConfig ` instances sole purpose overriding meth ` __repr__ ` remove spaces configs list _GradAccConfig __repr__ - str Override remove any spaces string appease internal build s test name parser + join config __repr__ config configs + TestGradAcc FSDPTest Tests ` ` FullyShardedDataParallel ` ` s gradient accumulation via both its ` ` no_sync ` ` context manager without context manager property world_size - int _test_grad_acc batch_dim int configs list _GradAccConfig cpu_offload CPUOffload backward_prefetch Optional BackwardPrefetch sharding_strategy ShardingStrategy use_orig_params bool Tests gradient accumulation comparing run trains sequentially through some batches while accumulating gradients run trains concatenation those batches single iteration The last iteration always synchronizes gradients regardless what specified last element ` ` configs ` ` Arguments batch_dim int Batch dimension input tensor passed into model forward pass configs List _GradAccConfig ` list ` configurations specifying how gradients accumulated example list corresponding False True False indicates accumulate over + + = total iterations where first two do use ` ` no_sync ` ` middle two do use ` ` no_sync ` ` final two again do use ` ` no_sync ` ` cpu_offload CPUOffload Configures CPU offloading backward_prefetch Optional BackwardPrefetch Specifies which point prefetch next layer s full parameters during backward pass all Initialize FSDP model optimizer fsdp_kwargs = cpu_offload cpu_offload backward_prefetch backward_prefetch sharding_strategy sharding_strategy use_orig_params use_orig_params fsdp_model FSDP = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE fsdp_kwargs deterministic=True add_bn=False disable BN since test uses varying batch sizes device = torch device cuda optim = torch optim SGD fsdp_model parameters lr= momentum= Generate sequence batches each containing same data permuted permute_tensor x torch Tensor x view - torch randperm x numel view_as x batch tuple torch Tensor = fsdp_model module get_input device batches list tuple torch Tensor = batch num_iters_to_acc = sum config num_iters config configs _ range num_iters_to_acc - batches append tuple permute_tensor t t batch batch batch itertools combinations batches r= t t zip batch batch assert torch all t == t Check test make sure batches distinct Concatenate batches along given batch dimension concat_batch tuple torch Tensor = tuple torch cat ts dim=batch_dim ts zip batches Establish reference gradients using concatenated batch fsdp_model zero_grad output = fsdp_model concat_batch ref_loss = fsdp_model module get_loss concat_batch output ref_loss backward ref_grads = p grad detach clone p fsdp_model parameters p grad None Compute accumulate gradients fsdp_model zero_grad losses = batch_idx = config configs sync_context = fsdp_model no_sync config use_no_sync contextlib nullcontext sync_context _ range config num_iters batch_idx == num_iters_to_acc - break always sync last iteration batch = batches batch_idx batch_idx += output = fsdp_model batch loss = fsdp_model module get_loss batch output loss backward losses append loss output = fsdp_model batches - loss = fsdp_model module get_loss batches - output loss backward losses append loss acc_loss = sum losses acc_grads = p grad detach clone p fsdp_model parameters p grad None Compare losses gradients torch testing assert_close ref_loss acc_loss assertEqual len ref_grads len acc_grads ref_grad acc_grad zip ref_grads acc_grads assertEqual ref_grad device acc_grad device assertEqual ref_grad size acc_grad size assertEqual ref_grad dtype acc_grad dtype torch testing assert_close ref_grad acc_grad Check optimizer step does error optim step _get_subtest_config - dict str list Any Returns subtest configuration subtests prefetching backward_prefetch None BackwardPrefetch BACKWARD_PRE BackwardPrefetch BACKWARD_POST sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP ShardingStrategy NO_SHARD skip_if_lt_x_gpu parametrize configs _GradAccConfigs _GradAccConfig use_no_sync=True num_iters= _GradAccConfig use_no_sync=False num_iters= _GradAccConfig use_no_sync=True num_iters= _GradAccConfigs _GradAccConfig use_no_sync=False num_iters= _GradAccConfig use_no_sync=True num_iters= _GradAccConfig use_no_sync=False num_iters= parametrize use_orig_params False True test_grad_acc configs _GradAccConfigs use_orig_params bool Tests gradient accumulation without parameter CPU offloading This exercises gradient accumulation inside outside ` ` no_sync ` ` context manager particular interleaving two It tests both interleaving starting ending resp inside versus outside ` ` no_sync ` ` ensure initial conditions final conditions resp do affect correctness subtest_config = _get_subtest_config subtest_config cpu_offload = CPUOffload offload_params=False run_subtests subtest_config _test_grad_acc batch_dim= configs=configs configs use_orig_params=use_orig_params skip_if_lt_x_gpu parametrize use_orig_params False True test_grad_acc_cpu_offload use_orig_params bool Tests gradient accumulation parameter CPU offloading NOTE Gradient accumulation without using ` ` no_sync ` ` context manager currently compatible CPU offloading Only test ` no_sync ` since outside ` no_sync ` supported parameter CPU offloading configs = _GradAccConfigs _GradAccConfig use_no_sync=True num_iters= subtest_config = _get_subtest_config subtest_config cpu_offload = CPUOffload offload_params=True run_subtests subtest_config _test_grad_acc batch_dim= configs=configs configs use_orig_params=use_orig_params instantiate_parametrized_tests TestGradAcc __name__ == __main__ run_tests