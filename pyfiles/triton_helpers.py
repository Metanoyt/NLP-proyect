mypy allow-untyped-decorators mypy allow-untyped-defs math pymath warnings typing Any Callable TypeVar triton_compat noqa F _log builtins_use_semantic_kwarg JITFunction libdevice math tl triton _T = TypeVar _T _LOG_ _E tl constexpr = tl constexpr pymath log pymath e set_driver_to_cpu driver = triton runtime driver backend = triton backends backends get cpu None isinstance driver active backend driver Don t re-initialize backend already active driver set_active backend driver This can hard error once triton-cpu merged into fbcode warnings warn Could find active CPU backend Generated kernels will executable set_driver_to_gpu driver = triton runtime driver name backend triton backends backends items backend driver is_active name = cpu After https github com triton-lang triton commit b d bc e edf fe b c c d badcd ` driver active ` can ` LazyProxy ` type sign - ` _obj ` attribute isinstance driver active backend driver hasattr driver active _obj isinstance driver active _obj backend driver Don t re-initialize backend already active driver set_active backend driver raise RuntimeError Could find active GPU backend get_backend_options triton runtime driver target = driver active get_current_target backend = triton compiler compiler make_backend target options = backend parse_options dict options __dict__ get_constexprs kernel JITFunction - list int p num p kernel params p is_constexpr triton jit promote_to_tensor x Addition promotes tensor us x + tl zeros tl int triton jit div_floor_integer b NOTE b C division we want floor division Based c div_floor_integer quot = b remainder = b fixed = tl where remainder = quot - quot tl where = b fixed quot triton jit remainder_integer b NOTE b matches C division floor division remainder = b tl where remainder = = b remainder + b remainder triton jit is_floating x promote_to_tensor x dtype is_floating triton jit _prod_accumulate b b triton jit prod input axis tl reduce input axis _prod_accumulate triton jit minimum b mask = b is_floating mask &#124; = = tl where mask b triton jit maximum b mask = b is_floating mask &#124; = = tl where mask b triton jit min dim tl reduce dim minimum triton jit max dim tl reduce dim maximum triton jit minimum_with_index a_value a_index b_value b_index mask = a_value b_value equal = a_value == b_value is_floating a_value a_isnan = a_value = a_value b_isnan = b_value = b_value mask &#124; = a_isnan b_isnan Consider NaNs equal equal &#124; = a_isnan b_isnan Prefer lowest index values equal mask &#124; = equal a_index b_index tl where mask a_value b_value tl where mask a_index b_index triton jit maximum_with_index a_value a_index b_value b_index mask = a_value b_value equal = a_value == b_value is_floating a_value a_isnan = a_value = a_value b_isnan = b_value = b_value mask &#124; = a_isnan b_isnan Consider NaNs equal equal &#124; = a_isnan b_isnan Prefer lowest index values equal mask &#124; = equal a_index b_index tl where mask a_value b_value tl where mask a_index b_index triton jit min_with_index value index dim tl reduce value index dim minimum_with_index triton jit max_with_index value index dim tl reduce value index dim maximum_with_index triton jit exp x use_fast_math tl constexpr use_fast_math math exp x libdevice exp x triton jit online_softmax_reduce lhs_max lhs_sum dim use_fast_math tl constexpr out_max = max lhs_max dim out_max_keepdim = tl expand_dims out_max dim delta = tl where out_max_keepdim == float -inf lhs_max - out_max_keepdim out_sum = tl sum lhs_sum exp delta use_fast_math dim out_max out_sum triton jit online_softmax_combine lhs_max lhs_sum rhs_max use_fast_math tl constexpr When we do combine we assume lhs accumulator rhs next block data Then rhs_sum always With assumption we can save some registers computation out_max = maximum lhs_max rhs_max lhs_scale = tl where out_max == float -inf exp lhs_max - out_max use_fast_math rhs_scale = tl where out_max == float -inf exp rhs_max - out_max use_fast_math Should out_sum = lhs_sum lhs_scale + rhs_sum rhs_scale since rhs_sum all we can simplify out_sum = lhs_sum lhs_scale + rhs_scale out_max out_sum triton jit welford_reduce value mean m weight first_iteration first_iteration new_weight = tl full weight shape weight dtype new_mean = value new_m = tl zeros_like m delta = value - mean new_weight = weight + new_mean = mean + delta new_weight new_m = m + delta value - new_mean new_mean new_m new_weight triton jit welford_combine mean_ m _ weight_ mean_ m _ weight_ delta = mean_ - mean_ new_weight = weight_ + weight_ w _over_w = tl where new_weight == weight_ new_weight mean_ + delta w _over_w m _ + m _ + delta delta weight_ w _over_w new_weight triton jit welford mean m weight dim tl reduce mean m weight dim welford_combine triton jit device_assert_then cond msg r tl device_assert cond msg r triton jit randint seed offset low high r r _r _r = tl randint x seed offset r = r tl uint r = r tl uint result = r &#124; r size = high - low result = result size tl uint result = result tl int + low result triton jit _any_combine b &#124; b triton jit any dim tl reduce dim _any_combine triton jit bucketize_binary_search values tl tensor boundaries_ptr tl tensor BOUNDARIES_SIZE int BOUNDARIES_UNDERLYING_NUMEL int BOUNDARIES_STRIDE int boundary_indices tl tensor indexing_dtype tl dtype right bool triton can t handle unquoted bool annotation sorter_ptr tl tensor SORTER_STRIDE int sorter_indices tl tensor See Note Inductor bucketize op Inputs ------- values values bucketize boundaries_ptr pointer beginning boundaries tensor -D BOUNDARIES_SIZE length last dimension boundaries tensor i e one individual set boundaries BOUNDARIES_UNDERLYING_NUMEL length boundaries tensor -D ignoring any striding BOUNDARIES_STRIDE stride last dimension boundaries tensor boundary_indices tensor same size values each element index into -D un-strided boundaries tensor pointing first element set boundaries used value indexing_dtype dtype used indexing into boundaries tensor dtype right true use boundary intervals closed left otherwise use intervals closed right sorter_ptr optional pointer sorter tensor same shape boundaries potentially different striding If present allows us treat boundaries sorted even elements boundaries unsorted SORTER_STRIDE must present sorter_ptr non-None stride last dimension sorter tensor sorter_indices must present sorter_ptr non-None see boundary_indices BLOCK_SHAPE shape data block being processed low = tl zeros values shape dtype=indexing_dtype high = tl full values shape BOUNDARIES_SIZE dtype=indexing_dtype full_range = BOUNDARIES_SIZE + while full_range mid = high + low mask = mid BOUNDARIES_STRIDE + boundary_indices BOUNDARIES_UNDERLYING_NUMEL logical_and mid BOUNDARIES_SIZE mid_indices = mid sorter_ptr None SORTER_STRIDE None tl load sorter_ptr + sorter_indices + SORTER_STRIDE mid mask=mask other= bucket_upper_bound = tl load boundaries_ptr + boundary_indices + BOUNDARIES_STRIDE mid_indices mask=mask other= right is_above = values = bucket_upper_bound is_above = values bucket_upper_bound low = tl where is_above mask mid + low high = tl where is_above high mid full_range = full_range + low triton jit pack_value_flag value flag DTYPE_VALUE_AS_UINT tl constexpr DTYPE_PACK tl constexpr Workaround triton bug tensor doesn t unwrap constexpr values DTYPE_VALUE_AS_UINT = tl core _unwrap_if_constexpr DTYPE_VALUE_AS_UINT bitwidth = DTYPE_VALUE_AS_UINT primitive_bitwidth uv = value DTYPE_VALUE_AS_UINT bitcast=True DTYPE_PACK flag DTYPE_PACK &#124; uv bitwidth triton jit unpack_value pack DTYPE_VALUE DTYPE_VALUE_AS_UINT Workaround triton bug tensor doesn t unwrap constexpr values DTYPE_VALUE = tl core _unwrap_if_constexpr DTYPE_VALUE DTYPE_VALUE_AS_UINT = tl core _unwrap_if_constexpr DTYPE_VALUE_AS_UINT bitwidth = DTYPE_VALUE_AS_UINT primitive_bitwidth value_uint = pack bitwidth DTYPE_VALUE_AS_UINT value_uint DTYPE_VALUE bitcast=True triton jit unpack_flag pack DTYPE_FLAG pack DTYPE_FLAG triton jit exclusive_scan_decoupled_lookback scratch_base block_value index combine_fn DTYPE_VALUE_AS_UINT tl constexpr DTYPE_PACK tl constexpr Compute exclusive scan scalar value between blocks Ref https research nvidia com publication - _single-pass-parallel-prefix-scan-decoupled-look-back scratch_base Pointer scratch space global memory block_value Scalar value block index Scalar index block relative current scan combine_fn Function ` ` value value - value ` ` which scanned over DTYPE_VALUE_AS_UINT A tl uint n type equal size ` ` block_value ` ` DTYPE_PACK Unsigned type twice width block_value NOTE This function limited values which -bits less because we need pack value flag into single unsigned int Publish block sum so subsequent blocks don t get stuck waiting us DTYPE_VALUE = block_value dtype pack = pack_value_flag block_value tl full block_value shape DTYPE_VALUE_AS_UINT DTYPE_VALUE_AS_UINT DTYPE_PACK index tl atomic_xchg scratch_base + index pack sem= relaxed Calculate exclusive prefix scan exclusive_prefix = tl zeros DTYPE_VALUE prefix_valid = False test_target = index - while test_target = tl atomic_load flag = tl full DTYPE_VALUE_AS_UINT while flag == pack = tl atomic_add scratch_base + test_target sem= relaxed flag = unpack_flag pack DTYPE_VALUE_AS_UINT value = unpack_value pack DTYPE_VALUE DTYPE_VALUE_AS_UINT prefix_valid exclusive_prefix = combine_fn value exclusive_prefix exclusive_prefix = value prefix_valid = True flag == test_target = - test_target = test_target - Make inclusive block sum visible other blocks prefix_valid inclusive_prefix = combine_fn exclusive_prefix block_value inclusive_prefix = block_value pack = pack_value_flag inclusive_prefix tl full DTYPE_VALUE_AS_UINT DTYPE_VALUE_AS_UINT DTYPE_PACK tl atomic_xchg scratch_base + index pack sem= relaxed exclusive_prefix triton jit exclusive_scan_decoupled_lookback_ scratch_base block_value index combine_fn Compute exclusive scan scalar value between blocks Ref https research nvidia com publication - _single-pass-parallel-prefix-scan-decoupled-look-back scratch_base Pointer scratch space global memory block_value Scalar value block must -bits wide index Scalar index block relative current scan combine_fn Function ` ` value value - value ` ` which scanned over init Scalar value equal identity combine_fn Publish block sum so subsequent blocks don t get stuck waiting us index block_value_u = block_value tl uint bitcast=True tl store scratch_base + index + block_value_u tl debug_barrier flag_one = tl full tl uint tl atomic_xchg scratch_base + index + flag_one sem= release Calculate exclusive prefix scan exclusive_prefix = tl zeros block_value dtype prefix_valid = False test_target = index - while test_target = flag = tl full tl uint while flag == flag = tl atomic_add scratch_base + test_target + sem= acquire value_u = tl load scratch_base + test_target + flag tl int value = value_u block_value dtype bitcast=True prefix_valid exclusive_prefix = combine_fn value exclusive_prefix exclusive_prefix = value prefix_valid = True flag == test_target = - test_target = test_target - Make inclusive block sum visible other blocks prefix_valid inclusive_prefix = combine_fn exclusive_prefix block_value inclusive_prefix = block_value inclusive_prefix_u = inclusive_prefix tl uint bitcast=True tl store scratch_base + index + inclusive_prefix_u tl debug_barrier flag_two = tl full tl uint tl atomic_xchg scratch_base + index + flag_two sem= release exclusive_prefix triton jit frexp x TODO isuruf use inline_asm_elementwise here y = libdevice ilogb x + exponent = tl where x == y mantissa = tl where x == libdevice ldexp x -y mantissa exponent triton jit _compare_and_swap_with_index x idxs rnumel flip i tl constexpr n_dims tl constexpr stable tl constexpr descending tl constexpr n_outer tl constexpr = x numel n_dims shape tl constexpr = n_outer i n_dims - i - idtype = tl core get_int_dtype bitwidth=x dtype primitive_bitwidth signed=True y = tl reshape x shape iy = y idtype bitcast=True slice left right stride n_dims - i - right_mask = tl arange None None idtype left_mask = - right_mask idtype ileft = tl broadcast_to tl sum iy left_mask idtype None shape iright = tl broadcast_to tl sum iy right_mask idtype None shape ileft = tl reshape ileft x shape iright = tl reshape iright x shape left = ileft x dtype bitcast=True right = iright x dtype bitcast=True idx y_idx = tl reshape idxs shape left_idx = tl broadcast_to tl sum y_idx left_mask y_idx dtype None shape right_idx = tl broadcast_to tl sum y_idx right_mask y_idx dtype None shape left_idx = tl reshape left_idx x shape right_idx = tl reshape right_idx x shape valid rnumel None left_valid_mask = tl full x shape True tl int right_valid_mask = tl full x shape True tl int left_valid_mask = left_idx rnumel right_valid_mask = right_idx rnumel actual compare-and-swap ix = x idtype bitcast=True sort treats nan having higher value comparisons nan always False align sort semantics we need update descending check right_isnan ascending check left_isnan left_isnan = left = left right_isnan = right = right descending cond = left right is_floating left stable cond = cond &#124; right_isnan cond = cond &#124; right_isnan ~left_isnan cond = left right is_floating left stable cond = cond &#124; left_isnan cond = cond &#124; left_isnan ~right_isnan stable When stable sorting tie break index eq = left == right is_floating left eq = eq &#124; left_isnan right_isnan cond = cond &#124; eq left_idx right_idx cond = right_valid_mask left_valid_mask &#124; right_valid_mask == left_valid_mask cond cond = cond ^ flip tl int ret = ix ^ tl where cond ileft ^ iright tl zeros_like ix new_idxs = idxs ^ tl where cond left_idx ^ right_idx tl zeros_like idxs ret x dtype bitcast=True new_idxs triton jit _bitonic_merge_with_index x idxs rnumel stage tl constexpr alternating tl constexpr n_dims tl constexpr stable tl constexpr descending tl constexpr n_outer tl constexpr = x numel n_dims tl static_assert stage = n_dims flip denotes whether re-arrange sub-sequences elements ascending descending order flip = then all elements will re-arranged ascendingly stage flip = then all elements will re-arranged alternatingly stride stage alternating shape tl constexpr = n_outer n_dims - - stage stage flip = tl reshape tl broadcast_to tl arange None None shape x shape flip = False perform ` stage ` rounds ` compare-and-swap ` i tl static_range stage x idxs = _compare_and_swap_with_index x idxs rnumel flip i + n_dims - stage n_dims stable descending x idxs triton jit sort_with_index x value idxs index rnumel number elements dim tl constexpr = None stable tl constexpr = tl constexpr False descending tl constexpr = tl constexpr False x idxs = tl broadcast x idxs handle default dimension check most minor dim _dim tl constexpr = len x shape - dim None dim tl static_assert _dim == len x shape - only minor dimension currently supported iteratively run bitonic merge-sort steps n_dims tl constexpr = _log x shape _dim i tl static_range n_dims + x idxs = _bitonic_merge_with_index x idxs rnumel i alternating=i n_dims n_dims=n_dims stable=stable descending=descending x idxs triton jit select_one x mask dim keep_dims=False idtype = tl core get_int_dtype x dtype primitive_bitwidth signed=False ix = x idtype bitcast=True iy = tl sum ix mask dim keep_dims=keep_dims iy x dtype bitcast=True triton jit x_grid_barrier sem Wait all other thread blocks grid sharing same y z program_id reach barrier before returning Args sem uint semaphores zero x initialized Must unique each y z program ID ensure stores before visible tl debug_barrier one_i = one_u = one_i tl uint type ignore attr-defined expected = tl num_programs tl uint tl program_id == nb = x - expected - one_u nb = one_u old_arrive = tl atomic_add sem nb sem= release bar_flipped = False while bar_flipped want ` ld acquire gpu u $ $ ` Triton doesn t have current_arrive = tl atomic_add sem sem= acquire current_arrive = tl load sem volatile=True bar_flipped = old_arrive ^ current_arrive x = TODO jansel needed tl debug_barrier triton_builtin f Callable _T - Callable _T Decorator mark function Triton built-in function These functions evaluated compile time Args f function The function marked Triton built-in Returns function The same function marked Triton built-in builtins_use_semantic_kwarg support Triton before after https github com triton-lang triton pull after https github com triton-lang triton pull wrapper args _semantic kwargs kwargs _builder = _semantic f args kwargs wrapper = f type ignore assignment wrapper __triton_builtin__ = True type ignore attr-defined wrapper triton_builtin constexpr_next_power_of_ n tl constexpr _builder object = None - tl constexpr A version triton next_power_of_two can used within kernel constants assert isinstance n tl constexpr tl constexpr triton next_power_of_ n value triton_builtin if_mask mask Any val _builder object = None - tl constexpr Work around triton compile error ` ValueError ` other ` cannot provided without ` mask ` ` A compile-time check either ` val ` ` None ` depending value mask isinstance mask tl constexpr mask value None tl constexpr None val