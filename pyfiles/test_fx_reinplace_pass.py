Owner s module functionalization torch torch testing _internal common_utils TestCase run_tests torch fx passes reinplace reinplace torch fx experimental proxy_tensor make_fx torch fx experimental symbolic_shapes ShapeEnv torch _dynamo source ConstantSource torch fx experimental sym_node SymNode try functorch experimental functionalize HAS_FUNCTIONALIZATION = True except Exception HAS_FUNCTIONALIZATION = False TestReinplacePass TestCase test_reinplace_basic Basic test out-of-place add call should converted into add_ f x = x clone b = add b inpt = torch ones f = reinplace make_fx f inpt inpt expected_out = f inpt actual_out = f inpt assertEqual actual_out expected_out assertExpectedInline f code \ forward x_ clone = torch ops aten clone default x_ x_ = None add = torch ops aten add_ Tensor clone add = None clone test_reinplace_with_view f x = x clone a_view = view - We shouldn t re-inplace first add because alias reused later program b = add noqa F Second add fine re-inplace c = a_view add c inpt = torch ones f = reinplace make_fx f inpt inpt expected_out = f inpt actual_out = f inpt assertEqual actual_out expected_out assertExpectedInline f code \ forward x_ clone = torch ops aten clone default x_ x_ = None view = torch ops aten view default clone - add = torch ops aten add Tensor clone clone = add = None add_ = torch ops aten add_ Tensor view add_ = None view test_reinplace_different_metadata f a_ = a_ clone b = + Naively we shouldn t try inplace ge call because would require resizing b float bool tensor c = torch ge b c inpt = torch ones f = reinplace make_fx f inpt inpt expected_out = f inpt actual_out = f inpt assertEqual actual_out expected_out The ge should reinplaced assertExpectedInline f code \ forward a__ clone = torch ops aten clone default a__ a__ = None add = torch ops aten add Tensor clone ge = torch ops aten ge Tensor add clone add = clone = None ge test_reinplace_overlapping_memory f a_ = a_ clone b = expand Can t reinplace because b has overlapping memory c = b add c inpt = torch ones f = reinplace make_fx f inpt inpt expected_out = f inpt actual_out = f inpt assertEqual actual_out expected_out assertExpectedInline f code \ forward a__ clone = torch ops aten clone default a__ a__ = None expand = torch ops aten expand default clone clone = None add = torch ops aten add Tensor expand expand = None add This test won t actually run CI because requires functionalize functorch I m planning testing more comprehensively torchbench models we can make testing better once functorch moves into pytorch pytorch test_reinplace_scatter_op f a_ now don t test mutations inputs = a_ clone e = view - b = view - c = b d = c view - d add_ + e HAS_FUNCTIONALIZATION inpt = torch ones f = reinplace make_fx functionalize f inpt inpt expected_out = f inpt actual_out = f inpt assertEqual actual_out expected_out NOTE one slight pessimization here fact there bunch redundant views graph Technically half these views duplicates we could de-dup This shouldn t really hurt performance though since creating extra view effectively just moving some metadata around allocating new TensorImpl We can should update pass future clean up assertExpectedInline f code \ forward a__ clone = torch ops aten clone default a__ a__ = None view = torch ops aten view default clone - view = None view_ = torch ops aten view default clone - select = torch ops aten select int view_ view_ = None view_ = torch ops aten view default select - select = None add = torch ops aten add_ Tensor view_ add = None view_ = torch ops aten view default clone - clone = None select_ = torch ops aten select int view_ select_ = None view_ = torch ops aten view default view_ view_ = view_ = None view_ = torch ops aten view default view_ view_ = None view_ = torch ops aten view default view_ - select_ = torch ops aten select int view_ view_ = None view_ = torch ops aten view default select_ - select_ = view_ = None view_ = torch ops aten view default view_ - add_ = torch ops aten add_ Tensor view_ view_ view_ = add_ = None view_ test_reinplace_scatter_twice f a_ now don t test mutations inputs = a_ clone b = c = b c add_ HAS_FUNCTIONALIZATION inpt = torch ones f = reinplace make_fx functionalize f inpt inpt expected_out = f inpt actual_out = f inpt assertEqual actual_out expected_out assertExpectedInline f code \ forward a__ clone = torch ops aten clone default a__ a__ = None select = torch ops aten select int clone select_ = torch ops aten select int select select = None add = torch ops aten add_ Tensor select_ select_ = add = None select_ = torch ops aten select int clone select_ = None select_ = torch ops aten select int clone select_ = torch ops aten select int select_ select_ = select_ = None clone test_reinplace_scatter_twice_with_different_view_op_valid f a_ = a_ clone b = c = b c_updated = c add good_mirror_of_b = as_strided good_mirror_of_b points same region memory b scatter op below tries scatter c_updated into same region c currently takes up reinplacing logic checks confirming c_updated good_mirror_of_b select have same size stride storage_offset b_updated = torch select_scatter good_mirror_of_b c_updated b_updated inpt = torch ones f = reinplace make_fx f inpt inpt expected_out = f inpt actual_out = f inpt assertEqual actual_out expected_out assertExpectedInline f code \ forward a__ clone = torch ops aten clone default a__ a__ = None select = torch ops aten select int clone select_ = torch ops aten select int select select = None add = torch ops aten add_ Tensor select_ select_ = add = None as_strided = torch ops aten as_strided default clone clone = None as_strided Test example where we have scatter op where base tensor has same size stride storage offset even though different view making valid re-inplace test_reinplace_scatter_twice_with_different_view_op_invalid f a_ = a_ clone b = c = b c_updated = c add good_mirror_of_b = as_strided The first arg select_scatter equivalent view b However select_scatter call below tries put c_updated into different slice b than what c currently occupies b_updated = torch select_scatter good_mirror_of_b c_updated b_updated inpt = torch ones f = reinplace make_fx f inpt inpt expected_out = f inpt actual_out = f inpt assertEqual actual_out expected_out assertExpectedInline f code \ forward a__ clone = torch ops aten clone default a__ a__ = None select = torch ops aten select int clone select_ = torch ops aten select int select select = None add = torch ops aten add Tensor select_ select_ = None as_strided = torch ops aten as_strided default clone clone = None select_int = torch ops aten select int as_strided copy__default = torch ops aten copy_ default select_int add select_int = add = copy__default = None as_strided noqa B test_reinplace_scatter_twice_with_different_view_op_invalid f a_ = a_ clone b = c = b c_updated = c add bad_mirror_of_b = as_strided The first arg select_scatter points different than c s base This makes invalid re-inplace b_updated = torch select_scatter bad_mirror_of_b c_updated b_updated inpt = torch ones f = reinplace make_fx f inpt inpt expected_out = f inpt noqa F actual_out = f inpt noqa F assertEqual actual_out expected_out assertExpectedInline f code \ forward a__ clone = torch ops aten clone default a__ a__ = None select = torch ops aten select int clone select_ = torch ops aten select int select select = None add = torch ops aten add Tensor select_ select_ = None as_strided = torch ops aten as_strided default clone clone = None select_int = torch ops aten select int as_strided copy__default = torch ops aten copy_ default select_int add select_int = add = copy__default = None as_strided noqa B test_out_node_updated f x = torch zeros y = x diagonal y_updated = y add z = torch diagonal_scatter x y_updated reinplace needs know replace output z x z HAS_FUNCTIONALIZATION f = reinplace make_fx functionalize f expected_out = f actual_out = f assertEqual actual_out expected_out assertExpectedInline f code \ forward zeros = torch ops aten zeros default device = device type= cpu pin_memory = False diagonal = torch ops aten diagonal default zeros add = torch ops aten add_ Tensor diagonal diagonal = add = None zeros test_reinplace_index_mutation f = torch zeros = torch ones HAS_FUNCTIONALIZATION f = reinplace make_fx functionalize f expected_out = f actual_out = f assertEqual actual_out expected_out assertExpectedInline f code \ forward zeros = torch ops aten zeros default device = device type= cpu pin_memory = False ones = torch ops aten ones default device = device type= cpu pin_memory = False slice_ = torch ops aten slice Tensor zeros copy = torch ops aten copy_ default slice_ ones slice_ = ones = copy = None slice_ = torch ops aten slice Tensor zeros slice_ = None zeros test_reinplace_sym_input Symbolic input test out-of-place add call should converted into add_ symbolic input won t cause any error f x index = torch select x index clone = clone b = clone add b x = torch randn requires_grad=False index = shape_env = ShapeEnv symbol = shape_env create_symbol index source=ConstantSource f __testing_only len shape_env var_to_val sym_index = torch SymInt SymNode symbol shape_env int hint=index inpt = x sym_index f = reinplace make_fx f inpt inpt real_inpt = x index expected_out = f real_inpt actual_out = f real_inpt assertEqual actual_out expected_out print f code assertExpectedInline f code \ forward x_ index_ select = torch ops aten select int x_ index_ x_ = index_ = None clone = torch ops aten clone default select select = None add = torch ops aten add_ Tensor clone add = None clone __name__ == __main__ run_tests