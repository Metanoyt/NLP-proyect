mypy ignore-errors functools unittest torch functorch experimental control_flow map torch nn attention flex_attention _create_empty_block_mask flex_attention torch testing make_tensor torch testing _internal common_device_type onlyCUDA torch testing _internal common_dtype all_types_and custom_types torch testing _internal opinfo core DecorateInfo OpInfo SampleInput torch _higher_order_ops invoke_subgraph mark_compile_region torch _higher_order_ops InvokeQuant invoke_quant_packed sample_inputs_map opinfo device dtype requires_grad kwargs make_arg = functools partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg low= high= make_arg low= high= args= make_arg low= high= make_arg low= high= inner_f x y y x cos add_ y x + y sin cos_ view x size simple_map xs y y f x y y inner_f x y y map f xs y y nested_map xs y y f xx y y f x y y inner_f x y y map f xx y y map f xs y y triple_nested_map xs y y f xs y y f xx y y f x y y inner_f x y y map f xx y y map f xs y y map f xs y y PLEASE DON T ADD ANYTHING NEW TO THIS LIST do add OpInfo your HOP The OpInfo lets us do automated testing HOP check your HOP will work correctly PyTorch Your new HOP may fail some automated testing That s OK If you don t care about certain features like torch export s fine xfail those failing tests It less fine xfail more critical check like checking torch compile works your HOP your HOP has docstring If you don t know test fine xfail please ask There legitimate reasons why something cannot added list e g uses executorch which PyTorch If s case then please leave comment FIXME_hop_that_doesnt_have_opinfo_test_allowlist = custom_function_call autograd_function_apply run_and_save_rng_state run_with_rng_state graphsafe_run_with_rng_state out_dtype trace_wrapped tag_activation_checkpoint executorch_call_delegate wrap wrap_with_set_grad_enabled auto_functionalized_v associative_scan flat_apply WIP doesn t pass any tests yet wrap_with_autocast wrap_activation_checkpoint run_const_graph auto_functionalized map T map_impl with_effects strict_mode _export_tracepoint call_torchbind triton_kernel_wrapper_mutation triton_kernel_wrapper_functional hints_wrapper dynamo_bypassing_wrapper TODO soulitzer foreach_map aoti_call_delegate torch library define testlib mutating_custom_op Tensor x Tensor b z - Tensor Tensor Tensor tags=torch Tag pt _compliant_tag torch library impl testlib mutating_custom_op cpu foo_impl_cpu x z x add_ z add_ x z x + z torch library impl testlib mutating_custom_op cuda foo_impl_cuda x z x add_ z add_ x z x + z torch library register_fake testlib mutating_custom_op foo_impl_abstract x z x z x + z sample_inputs_cond opinfo device dtype requires_grad kwargs make_arg = functools partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg low= high= simple_cond x torch cond x sum lambda x x cos lambda x x sin x sample_inputs_invoke_subgraph opinfo device dtype requires_grad kwargs make_arg = functools partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg low= high= mark_compile_region fn_for_invoke_subgraph x torch sin x simple_invoke_subgraph x fn_for_invoke_subgraph x sample_inputs_auto_functionalize opinfo device dtype requires_grad kwargs make_arg = functools partial make_tensor device=device dtype=dtype requires_grad=False yield SampleInput make_arg low= high= make_arg low= high= simple_auto_functionalize x z torch ops testlib mutating_custom_op x z sample_inputs_flex_attention opinfo device dtype requires_grad kwargs make_arg = functools partial make_tensor device=device dtype=dtype requires_grad=requires_grad score_mod score b h m n score + h q k v = make_arg low= high= _ range block_mask = _create_empty_block_mask q k yield SampleInput q k v score_mod block_mask sample_inputs_while_loop opinfo device dtype requires_grad kwargs make_arg = functools partial make_tensor device=device dtype=dtype requires_grad=False yield SampleInput torch tensor make_arg low= high= simple_while_loop iter_t x cond_fn iter_t x iter_t body_fn iter_t x iter_t - x cos torch _higher_order_ops while_loop cond_fn body_fn iter_t x simple_while_loop_stack_output iter_t x cond_fn iter_t x iter_t body_fn iter_t x iter_t - x cos torch _higher_order_ops while_loop_stack_output cond_fn body_fn iter_t x tuple sample_inputs_local_map_hop opinfo device dtype requires_grad kwargs TODO once HOPs support DTensor inputs we should also test DTensors make_arg = functools partial make_tensor device=device dtype=dtype requires_grad=False yield SampleInput make_arg low= high= make_arg low= high= simple_local_map_hop inp inp body_gm inp inp inp cos + inp sin gm = torch fx symbolic_trace body_gm assert torch distributed is_available torch distributed tensor placement_types Replicate gm meta local_map_kwargs = in_placements Replicate Replicate Replicate out_placements Replicate Replicate Replicate TODO Dynamo would rewrite op differently torch _higher_order_ops local_map_hop gm inp inp sample_inputs_scan opinfo device dtype requires_grad kwargs make_arg = functools partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg low= high= make_arg low= high= simple_scan init xs combine_fn carry x result = carry x + x result carry clone torch _higher_order_ops scan combine_fn init xs quant_tracer = InvokeQuant simple_invoke_quant x fn x y torch sin x y quant_tracer fn x x simple_invoke_quant_packed x fn x torch sin x invoke_quant_packed fn x hop_db = OpInfo name= scan variant_test_name= simple op=simple_scan sample_inputs_func=sample_inputs_scan dtypes=all_types_and torch bool torch half supports_out=False check_batched_grad=False check_batched_gradgrad=False check_batched_forward_grad=False check_inplace_batched_forward_grad=False supports_autograd=False torch compile aot_autograd does currently support double backward supports_gradgrad=False OpInfo name= invoke_subgraph variant_test_name= simple op=simple_invoke_subgraph sample_inputs_func=sample_inputs_invoke_subgraph dtypes=all_types_and torch bool torch half supports_out=False check_batched_grad=False check_batched_gradgrad=False check_batched_forward_grad=False check_inplace_batched_forward_grad=False supports_autograd=True torch compile aot_autograd does currently support double backward supports_gradgrad=False OpInfo name= map variant_test_name= simple op=simple_map sample_inputs_func=sample_inputs_map dtypes=all_types_and torch bool torch half supports_out=False check_batched_grad=False check_batched_gradgrad=False check_batched_forward_grad=False check_inplace_batched_forward_grad=False OpInfo name= map variant_test_name= nested op=nested_map sample_inputs_func=sample_inputs_map dtypes=all_types_and torch bool torch half supports_out=False check_batched_grad=False check_batched_gradgrad=False check_batched_forward_grad=False check_inplace_batched_forward_grad=False OpInfo name= map variant_test_name= triple_nested op=triple_nested_map sample_inputs_func=sample_inputs_map dtypes=all_types_and torch bool torch half supports_out=False check_batched_grad=False check_batched_gradgrad=False check_batched_forward_grad=False check_inplace_batched_forward_grad=False OpInfo name= cond variant_test_name= simple op=simple_cond sample_inputs_func=sample_inputs_cond dtypes=all_types_and torch bool torch half supports_out=False check_batched_grad=False check_batched_gradgrad=False check_batched_forward_grad=False check_inplace_batched_forward_grad=False supports_autograd=True torch compile aot_autograd does currently support double backward supports_gradgrad=False OpInfo name= invoke_quant variant_test_name= simple op=simple_invoke_quant sample_inputs_func=sample_inputs_invoke_subgraph dtypes=all_types_and torch bool torch half supports_out=False check_batched_grad=False check_batched_gradgrad=False check_batched_forward_grad=False check_inplace_batched_forward_grad=False supports_autograd=True torch compile aot_autograd does currently support double backward skips= DecorateInfo unittest expectedFailure TestHOP test_aot_export DecorateInfo unittest expectedFailure TestHOP test_pre_dispatch_export DecorateInfo unittest expectedFailure TestHOP test_serialize_export DecorateInfo unittest expectedFailure TestHOP test_retrace_export torch compile aot_autograd does currently support double backward supports_gradgrad=False OpInfo name= invoke_quant_packed variant_test_name= simple op=simple_invoke_quant_packed sample_inputs_func=sample_inputs_invoke_subgraph dtypes=all_types_and torch bool torch half supports_out=False check_batched_grad=False check_batched_gradgrad=False check_batched_forward_grad=False check_inplace_batched_forward_grad=False supports_autograd=True torch compile aot_autograd does currently support double backward supports_gradgrad=False OpInfo name= while_loop variant_test_name= simple op=simple_while_loop sample_inputs_func=sample_inputs_while_loop dtypes=all_types_and torch bool torch half supports_out=False check_batched_grad=False check_batched_gradgrad=False check_batched_forward_grad=False check_inplace_batched_forward_grad=False supports_autograd=False OpInfo name= while_loop_stack_output variant_test_name= simple op=simple_while_loop_stack_output sample_inputs_func=sample_inputs_while_loop dtypes=all_types_and torch bool torch half supports_out=False check_batched_grad=False check_batched_gradgrad=False check_batched_forward_grad=False check_inplace_batched_forward_grad=False supports_autograd=False OpInfo name= auto_functionalize variant_test_name= simple op=simple_auto_functionalize sample_inputs_func=sample_inputs_auto_functionalize dtypes=all_types_and torch bool torch half supports_out=False check_batched_grad=False check_batched_gradgrad=False check_batched_forward_grad=False check_inplace_batched_forward_grad=False supports_autograd=False OpInfo name= flex_attention variant_test_name= simple op=flex_attention sample_inputs_func=sample_inputs_flex_attention dtypes=custom_types torch float torch float supports_out=False check_batched_grad=False check_batched_gradgrad=False check_batched_forward_grad=False check_inplace_batched_forward_grad=False skips= DecorateInfo unittest expectedFailure TestHOP test_aot_export DecorateInfo unittest expectedFailure TestHOP test_pre_dispatch_export DecorateInfo unittest expectedFailure TestHOP test_serialize_export DecorateInfo unittest expectedFailure TestHOP test_retrace_export decorators= onlyCUDA OpInfo name= flex_attention_backward variant_test_name= simple op=flex_attention sample_inputs_func=sample_inputs_flex_attention dtypes=custom_types torch float torch float supports_out=False check_batched_grad=False check_batched_gradgrad=False check_batched_forward_grad=False check_inplace_batched_forward_grad=False skips= DecorateInfo unittest expectedFailure TestHOP test_aot_export DecorateInfo unittest expectedFailure TestHOP test_pre_dispatch_export DecorateInfo unittest expectedFailure TestHOP test_serialize_export DecorateInfo unittest expectedFailure TestHOP test_retrace_export decorators= onlyCUDA OpInfo name= local_map_hop variant_test_name= simple op=simple_local_map_hop sample_inputs_func=sample_inputs_local_map_hop dtypes=custom_types torch float torch float supports_out=False check_batched_grad=False check_batched_gradgrad=False check_batched_forward_grad=False check_inplace_batched_forward_grad=False skips= DecorateInfo unittest expectedFailure TestHOP test_aot_export DecorateInfo unittest expectedFailure TestHOP test_pre_dispatch_export DecorateInfo unittest expectedFailure TestHOP test_serialize_export DecorateInfo unittest expectedFailure TestHOP test_retrace_export decorators= onlyCUDA unittest skipIf torch distributed is_available requires distributed build