mypy allow-untyped-defs inspect textwrap torch jit torch jit _builtins _find_builtin file generating documentation using sphinx autodoc help torch jit supported_ops will also give nice listed supported ops programmatically _hidden name name startswith _ name startswith __ _emit_type type str type _emit_arg indent i arg v = f arg name _emit_type arg type default = arg default_value default None v = f v = str default i v = f \n indent v v _emit_args indent arguments join _emit_arg indent i arg i arg enumerate arguments _emit_ret ret _emit_type ret type _emit_rets returns len returns == _emit_ret returns f Tuple join _emit_ret r r returns _emit_schema mod name schema arg_start= padding= mod None qualified_name = name qualified_name = f mod name schema_str = f qualified_name f _emit_args len qualified_name + + padding schema arguments arg_start f - _emit_rets schema returns schema_str _get_tensor_ops is_tensor_method schema len schema arguments == False = schema arguments name = False type isSubtypeOf torch _C TensorType get False True methods = discover methods elem dir torch Tensor _hidden elem schemas = torch _C _jit_get_schemas_for_operator aten + elem schema schemas is_tensor_method schema methods append _emit_schema Tensor elem schema arg_start= Supported Tensor Methods methods _get_nn_functional_ops functions = Iterate over torch nn functional mod = torch nn functional name = mod __name__ elem dir torch nn functional attr = getattr mod elem inspect isfunction attr _hidden elem Ignore non-functions internal methods continue attr_module = inspect getmodule attr attr_module raise RuntimeError f Module attr found torch nn functional attr_module __name__ Ignore functions outside torch nn functional continue try compile fn get schema scripted = torch jit script attr scripted_schema = scripted schema functions append _emit_schema name elem scripted_schema except noqa B E Skip interpolate boolean dispatched things pass Iterate over modules we know contain lot builtins mod torch jit _builtins _modules_containing_builtins name = mod __name__ elem dir mod builtin = _find_builtin getattr mod elem builtin None schemas = torch _C _jit_get_schemas_for_operator builtin schema schemas remove _tan __and__ _hidden elem functions append _emit_schema name elem schema Supported PyTorch Functions functions _get_builtins_helper builtins = fn _builtin_name torch jit _builtins _builtin_ops mod = inspect getmodule fn hasattr fn __name__ typing classes continue mod continue _hidden fn __name__ _hidden fn __qualname__ _hidden mod __name__ skip internal-only methods continue torch _C mod __name__ continue builtins append fn _builtin_name builtins _is_math_fn fn mod = inspect getmodule fn mod raise RuntimeError f Module fn found mod __name__ == math _get_torchscript_builtins functions = builtins = filter lambda fn _is_math_fn fn _get_builtins_helper builtins_list = list builtins Iterate over specially added builtins fn _builtin_name builtins_list mod = inspect getmodule fn mod raise RuntimeError f Module fn found builtin = _find_builtin fn builtin None schemas = torch _C _jit_get_schemas_for_operator builtin schema schemas functions append _emit_schema mod __name__ fn __name__ schema TorchScript Builtin Functions functions _get_math_builtins functions = builtins = filter lambda fn _is_math_fn fn _get_builtins_helper builtins_list = list builtins Iterate over specially added builtins fn _builtin_name builtins_list mod = inspect getmodule fn mod raise RuntimeError f Module fn found builtin = _find_builtin fn builtin None schemas = torch _C _jit_get_schemas_for_operator builtin schema schemas schema_str = _emit_schema mod __name__ fn __name__ schema Tensor schema_str Skip Tensor ops have same name math functions they will show up tensor methods section continue functions append schema ` ` math ` ` Module functions _get_global_builtins Taken globals map torch csrc jit frontend ir_emitter cpp supported_builtins = print tuple float complex int bool str getattr hasattr isinstance len hex oct round hash min max abs all divmod list ord chr bin range zip enumerate sorted op_renames = bool aten Bool int aten Int float aten Float complex aten Complex abs prim abs max prim max min prim min range fake does_not_exist schemaless_op_explanations = print Print any value tuple Lists cannot converted tuples method since their size statically known getattr Attribute name must literal string hasattr Attribute name must literal string isinstance Result static zip Arguments must iterable enumerate Arguments must iterable range Can only used iterator loop magic_methods = complex __complex__ float __float__ int __int__ bool __bool__ str __str__ len __len__ hex __hex__ oct __oct__ magic_methods_rows = fn magic_method magic_methods pyrefly ignore bad-argument-type magic_methods_rows append f fn ` ` magic_method ` ` schematized_ops = schemaless_ops = fn supported_builtins op_name = f aten fn fn op_renames op_name = op_renames fn schemas = torch _C _jit_get_schemas_for_operator op_name s schemas schematized_ops append _emit_schema None fn s padding= len schemas schematized_ops append table_row = f external+python py obj ` fn ` schemaless_op_explanations fn pyrefly ignore bad-argument-type schemaless_ops append table_row schematized_ops_str = \n join schematized_ops schemaless_ops_str = \n join schemaless_ops magic_methods_rows_str = \n join magic_methods_rows schematized_ops_str = textwrap indent schematized_ops_str \t schemaless_ops_str = textwrap indent schemaless_ops_str \t magic_methods_rows_str = textwrap indent magic_methods_rows_str \t section = f The functions following table supported do have static schema csv-table header Function Note schemaless_ops_str The following functions will use corresponding magic method TorchScript classes csv-table header Function Magic Method magic_methods_rows_str These built-in functions use schema rst-class codeblock-height-limiter schematized_ops_str Python Built-in Functions section _list_supported_ops emit_block decls \n rst-class codeblock-height-limiter\n\n \n\n \n format join f d \n\n d decls body = op_gathering_fns = _get_tensor_ops _get_nn_functional_ops _get_torchscript_builtins _get_global_builtins _get_math_builtins fn op_gathering_fns header items = fn link_target = header replace ` replace - lower replace - isinstance items str section = f header \n ~ len header \n items \n section = f header \n ~ len header \n emit_block items section = f _ link_target + \n\n + section body += section body __doc__ = _list_supported_ops