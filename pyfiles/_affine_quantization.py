copied https github com pytorch ao blob main torchao quantization observer py https github com pytorch ao blob main torchao quantization quant_primitives py PLEASE DON T MODIFY THIS FILE SO THAT WE DON T GET OUT OF SYNC logging abc ABCMeta typing Any Optional Union torch torch ao quantization observer AffineQuantizedObserverBase get_block_size Granularity MappingType TorchAODType ZeroPointDomain ABC Any = ABCMeta ABC object compatible Python logger = logging getLogger __name__ FP _TYPES = torch float _e m fn torch float _e m torch float _e m fnuz torch float _e m fnuz _SUB_BYTE_UINT_BOUNDS = torch uint - torch uint - torch uint - torch uint - torch uint - torch uint - torch uint - Map dtype bound value integers TODO maybe can replace call torch iinfo _DTYPE_TO_QVALUE_BOUNDS dict Union torch dtype TorchAODType tuple int int = torch uint torch int - torch int - - torch int - - _DTYPE_TO_QVALUE_BOUNDS update _SUB_BYTE_UINT_BOUNDS _is_float _type dtype torch dtype - bool fp _types = torch float _e m fn torch float _e m fnuz torch float _e m torch float _e m fnuz dtype fp _types TODO decide we want allow custom quant_min quant_max here _get_and_check_qmin_qmax dtype quant_min quant_max Get quant_min quant_max args based dtype also verify they within range possible quant_min quant_max dtype dtype FP _TYPES quant_min_lower_bound quant_max_upper_bound = torch finfo dtype min torch finfo dtype max dtype _DTYPE_TO_QVALUE_BOUNDS raise ValueError f Unsupported dtype dtype quant_min_lower_bound quant_max_upper_bound = _DTYPE_TO_QVALUE_BOUNDS dtype quant_min None quant_min = quant_min_lower_bound quant_max None quant_max = quant_max_upper_bound quant_min quant_min_lower_bound raise AssertionError quant_min out bound dtype f quant_min_lower_bound quant_min_lower_bound quant_min quant_min quant_max quant_max_upper_bound raise AssertionError quant_max out bound dtype f quant_max_upper_bound quant_max_upper_bound quant_max quant_max quant_min quant_max _get_reduction_params block_size input_size Given block_size input size find parameters reduction Output shape_for_reduction shape we use ` view ` input prepare reduction reduction_dims dims we ll do reduction over Example Input block_size input_size Output shape_for_reduction reduction_dim len block_size = len input_size raise AssertionError block_size length must equal input_size length got f block_size= block_size input_size= input_size shape_for_reduction = reduction_dims = cur_dim = i range len block_size block_size i = input_size i block_size i input_size i block_size i = raise AssertionError f Expecting input size i dimension input_size i divisible f block_size i dimension block_size i shape_for_reduction append input_size i block_size i shape_for_reduction append block_size i reduce over block_size i dim reduction_dims append cur_dim + cur_dim += block_size i == input_size i block_size i == shape_for_reduction append input_size i we only need reduce over dimension block_size greater than otherwise s already same reduced dimension block_size i = reduction_dims append cur_dim cur_dim += shape_for_reduction reduction_dims _register_custom_op lib This decorator used preserve some high level operators torch export export while still allow them decomposed inductor path requirement make sure ` fn __name__ ` operator name you want register NOTE This should applied top after all other decorators have been applied NOTE We haven t tested case when ` fn ` accepts tensor subclass instance input e g uint tensor subclass instance we ll probably need figure out what would make sense downstream system like executorch accept well Example lib = torch library Library my_namespace FRAGMENT register_custom_op = _register_custom_op lib register_custom_op _the_op_that_needs_to_be_preserved after ` _the_op_that_needs_to_be_preserved ` will preserved torch ops my_namespace the_op_that_needs_to_be_preserved operator after torch export export torch _export export_for_training torch _inductor decomposition register_decomposition decorator fn torch _library infer_schema infer_schema expecting fn __name__ starts ` _ ` we want take rest name custom op fn __name__ = _ raise AssertionError f Expecting function name starts ` _ ` got fn __name__ any c fn __name__ c raise AssertionError f Expecting op defined normal functions lambda local fn __name__ op_name = fn __name__ schema = op_name + infer_schema fn mutates_args= lib define schema lib impl op_name fn CompositeImplicitAutograd lib_namespace = lib ns op = getattr getattr torch ops lib_namespace op_name register_decomposition op fn op decorator quant_lib = torch library Library pt e_quant FRAGMENT noqa TOR register_custom_op = _register_custom_op quant_lib choose_qparams_affine_with_min_max min_val torch Tensor max_val torch Tensor mapping_type MappingType block_size tuple int target_dtype torch dtype quant_min Optional int = None quant_max Optional int = None eps Optional float = None scale_dtype Optional torch dtype = None zero_point_dtype Optional torch dtype = None preserve_zero bool = True zero_point_domain Optional ZeroPointDomain = ZeroPointDomain INT - tuple torch Tensor torch Tensor A variant func ` ~torchao quantization quant_primitives choose_qparams_affine ` operator pass min_val max_val directly instead deriving these single input This used observers static quantization where min_val max_val may obtained through tracking all data calibration data set Args Mostly same func ` ~torchao quantization quant_primitives choose_qparams_affine ` one difference instead passing ` input ` Tensor use calculate min_val max_val then scale zero_point we pass min_val max_val directly _choose_qparams_affine None mapping_type name block_size target_dtype quant_min quant_max eps scale_dtype zero_point_dtype preserve_zero zero_point_domain name zero_point_domain None None min_val max_val register_custom_op _choose_qparams_affine input Optional torch Tensor mapping_type str block_size list int target_dtype torch dtype quant_min Optional Union int float bool = None quant_max Optional Union int float bool = None eps Optional float = None scale_dtype Optional torch dtype = None zero_point_dtype Optional torch dtype = None preserve_zero bool = True zero_point_domain Optional str = INT min_val Optional torch Tensor = None max_val Optional torch Tensor = None - tuple torch Tensor torch Tensor op definition has compatible signatures custom op library The op does following figure out dimension reduction based block_size find min_val max_val based dimension reduction calculate quantization parameters based min_val max_val based args like ` preserve_zero ` ` zero_point_domain ` quant_min quant_max = _get_and_check_qmin_qmax target_dtype quant_min quant_max mapping_type MappingType SYMMETRIC name MappingType SYMMETRIC_NO_CLIPPING_ERR name MappingType ASYMMETRIC name raise AssertionError f Unsupported mapping type mapping_type target_dtype FP _TYPES mapping_type = MappingType SYMMETRIC name raise AssertionError f Only symmetric quantization supported FP types got mapping_type input None scale_dtype None scale_dtype = input dtype zero_point_dtype None zero_point_dtype = input dtype eps None eps = torch finfo input dtype eps len block_size = input dim raise AssertionError f Got input dim input dim block_size block_size shape_for_reduction reduction_dims = _get_reduction_params block_size input size input = input view shape_for_reduction min_val = torch amin input dim=reduction_dims keepdim=False max_val = torch amax input dim=reduction_dims keepdim=False min_val None max_val None raise AssertionError f Need provide ` min_val ` ` max_val ` when ` input ` None got min_val max_val min_val dtype = max_val dtype raise AssertionError f Expecting ` min_val ` ` max_val ` have same dtype got min_val dtype max_val dtype scale_dtype None scale_dtype = min_val dtype zero_point_dtype None zero_point_dtype = min_val dtype eps None eps = torch finfo min_val dtype eps preserve_zero min_val_neg = torch min min_val torch zeros_like min_val max_val_pos = torch max max_val torch zeros_like max_val min_val_neg = min_val max_val_pos = max_val mapping_type == MappingType SYMMETRIC name mapping_type == MappingType SYMMETRIC_NO_CLIPPING_ERR name scales mapping_type == MappingType SYMMETRIC name max_val_pos = torch max -min_val_neg max_val_pos scale = max_val_pos float quant_max - quant_min mapping_type = MappingType SYMMETRIC_NO_CLIPPING_ERR name raise AssertionError f Expected mapping_type SYMMETRIC_NO_CLIPPING_ERR got mapping_type calculate smin smax individually choose larger one For example quant_min = - quant_max = - If smin bigger There would coverage negative values down - less rounding error than existing SYMMETRIC case - If smax bigger covers positive values up The round error may bigger than existing SYMMETRIC case Either way there s no out-of-range fp values after quantization smin = min_val_neg float quant_min smax = max_val_pos float quant_max mask = smin smax scale = torch where mask smin smax zeros preserve_zero raise ValueError preserve_zero == False supported symmetric quantization zero_point_domain None zero_point_domain = ZeroPointDomain INT name raise ValueError zero_point_domain = ZeroPointDomain INT supported symmetric quantization scale = torch clamp scale min=eps zero_point = torch full_like scale int quant_max + quant_min + mapping_type = MappingType ASYMMETRIC name raise AssertionError f Expected mapping_type ASYMMETRIC got mapping_type scale = max_val_pos - min_val_neg float quant_max - quant_min scale = torch clamp scale min=eps zero_point_domain == ZeroPointDomain NONE name zero_point = None preserve_zero zero_point = quant_min - torch round min_val_neg scale zero_point = torch clamp zero_point quant_min quant_max zero_point_domain = ZeroPointDomain FLOAT name raise AssertionError preserve_zero zero_point must FLOAT domain mid_point = quant_max + quant_min + zero_point = min_val_neg + scale mid_point zero_point None zero_point = zero_point dtype=zero_point_dtype scale dtype=scale_dtype zero_point torch no_grad quantize_affine input torch Tensor block_size tuple int scale torch Tensor zero_point Optional torch Tensor output_dtype torch dtype quant_min Optional Union int float = None quant_max Optional Union int float = None zero_point_domain Optional ZeroPointDomain = ZeroPointDomain INT - torch Tensor Args input torch Tensor original float float bfloat Tensor block_size Tuple int granularity quantization means size tensor elements s sharing same qparam e g when size same input tensor dimension we using per tensor quantization scale float quantization parameter affine quantization zero_point int quantization parameter affine quantization output_dtype torch dtype requested dtype e g torch uint output Tensor quant_min Optional int minimum quantized value output Tensor specified will derived dtype quant_max Optional int maximum quantized value output Tensor specified will derived dtype zero_point_domain ZeroPointDomain domain zero_point should either integer float zero_point integer domain zero point added quantized integer value during quantization zero_point floating point domain zero point subtracted floating point unquantized value during quantization default ZeroPointDomain INT Note How can block_size represent different granularities let s say we have Tensor size here table showing how block_size represents different granularities granularity type &#124; block_size per_tensor &#124; per_axis axis= &#124; per_axis axis= &#124; per_group groupsize= &#124; per_group groupsize= axis = &#124; Output quantized tensor requested dtype _quantize_affine input block_size scale zero_point output_dtype quant_min quant_max zero_point_domain name zero_point_domain None None register_custom_op _quantize_affine input torch Tensor block_size list int scale torch Tensor zero_point Optional torch Tensor output_dtype torch dtype quant_min Optional Union int float bool = None quant_max Optional Union int float bool = None zero_point_domain Optional str = ZeroPointDomain INT name - torch Tensor op definition has compatible signatures custom op library Note zero_point_domain optional specifies how we quantize floating point quantized data INT quantized_val = float_val scale integer + zero_point integer FLOAT quantized_val = float_val - zero_point float - scale mid_point scale None quantized_val = float_val scale &#124; primarily used floatx quantization Where we do want round values nearest integer instead scale cast quant_min quant_max = _get_and_check_qmin_qmax output_dtype quant_min quant_max workaround uintx dtypes since we don t have native Uintx dtype connected torch uintx dtypes yet output_dtype _SUB_BYTE_UINT_BOUNDS output_dtype = torch uint _quantize_affine_no_dtype_cast input block_size scale zero_point quant_min quant_max zero_point_domain output_dtype _quantize_affine_no_dtype_cast input torch Tensor block_size list int scale torch Tensor zero_point Optional torch Tensor quant_min Union int float quant_max Union int float zero_point_domain Optional str = ZeroPointDomain INT name - torch Tensor The op does following figure out dimension reduction based block_size also reshape input align shape after reduction quantize input based quantization parameters scale zero_point args like zero_point_domain reshape quantized result original shape TODO validations TODO validate scale zero_point dimensions compatible block_size input dtype torch float torch float torch bfloat raise AssertionError f Unsupported input dtype input dtype len block_size = input dim raise AssertionError f Got input dim input dim block_size block_size shape_for_reduction reduction_dims = _get_reduction_params block_size input size original_shape = input shape input = input view shape_for_reduction shape_after_reduction = shape_for_reduction i reduction_dims shape_after_reduction i = scale = scale view shape_after_reduction zero_point None zero_point = zero_point view shape_after_reduction zero_point_domain == ZeroPointDomain INT name quant = torch clamp torch round input scale + zero_point quant_min quant_max zero_point_domain == ZeroPointDomain NONE name zero_point None raise AssertionError zero_point should None when zero_point_domain NONE quant = torch clamp torch round input scale quant_min quant_max zero_point_domain None This case handles quantization float we expect no zero point no zero point domain zero_point None raise AssertionError zero_point should None when zero_point_domain None quant = torch clamp input scale reciprocal quant_min quant_max zero_point_domain = ZeroPointDomain FLOAT name raise AssertionError f Unexpected zero_point_domain zero_point_domain mid_point = quant_max + quant_min + min_val = zero_point - scale mid_point quant = torch clamp torch round input - min_val scale quant_min quant_max quant = quant view original_shape quant dequantize_affine input torch Tensor block_size tuple int scale torch Tensor zero_point Optional torch Tensor input_dtype torch dtype quant_min Optional Union int float = None quant_max Optional Union int float = None zero_point_domain ZeroPointDomain = ZeroPointDomain INT output_dtype torch dtype = torch float - torch Tensor Args input torch Tensor quantized tensor should match dtype ` dtype ` argument block_size List int granularity quantization means size tensor elements s sharing same qparam e g when size same input tensor dimension we using per tensor quantization scale Tensor quantization parameter affine quantization zero_point Tensor quantization parameter affine quantization input_dtype torch dtype requested dtype e g torch uint output Tensor quant_min Optional int minimum quantized value input Tensor quant_max Optional int maximum quantized value input Tensor output_dtype torch dtype dtype output Tensor default fp zero_point_domain ZeroPointDomain domain zero_point should either integer float zero_point integer domain zero point added quantized integer value during quantization zero_point floating point domain zero point subtracted floating point unquantized value during quantization default ZeroPointDomain INT Output dequantized Tensor requested dtype fp _dequantize_affine input block_size scale zero_point input_dtype quant_min quant_max zero_point_domain name zero_point_domain None None output_dtype=output_dtype register_custom_op _dequantize_affine input torch Tensor block_size list int scale torch Tensor zero_point Optional torch Tensor input_dtype torch dtype quant_min Optional Union int float bool = None quant_max Optional Union int float bool = None zero_point_domain Optional str = ZeroPointDomain INT name output_dtype torch dtype = torch float - torch Tensor op definition has compatible signatures custom op library TODO validate scale zero_point dimensions compatible block_size input_dtype _SUB_BYTE_UINT_BOUNDS input dtype = input_dtype raise AssertionError f Expected input_dtype got input dtype output_dtype torch float torch float torch bfloat raise AssertionError f Unsupported output dtype output_dtype quant_min quant_max = _get_and_check_qmin_qmax input_dtype quant_min quant_max _dequantize_affine_no_dtype_check input block_size scale zero_point quant_min quant_max zero_point_domain output_dtype _dequantize_affine_no_dtype_check input torch Tensor block_size list int scale torch Tensor zero_point Optional torch Tensor quant_min Union int float quant_max Union int float zero_point_domain Optional str = ZeroPointDomain INT name output_dtype torch dtype = torch float - torch Tensor This function converts AQT tensors their high precision floating point representation The op does following figure out dimension reduction based block_size also reshape input align shape after reduction dequantize input based quantization parameters scale zero_point args like zero_point_domain reshape quantized result original shape change dtype output_dtype len block_size = input dim raise AssertionError f Got input dim input dim block_size block_size shape_for_reduction reduction_dims = _get_reduction_params block_size input size original_shape = input shape input = input view shape_for_reduction shape_after_reduction = shape_for_reduction i reduction_dims shape_after_reduction i = scale = scale view shape_after_reduction zero_point None zero_point = zero_point view shape_after_reduction zero_point_domain == ZeroPointDomain INT name Force copy avoid input modification due upcoming in-place operations dequant = input torch int copy=True zero_point None dequant = dequant - zero_point torch int dequant = dequant output_dtype dequant = dequant scale zero_point_domain == ZeroPointDomain NONE name zero_point None raise AssertionError zero_point should None when zero_point_domain NONE dequant = input output_dtype dequant = dequant scale zero_point_domain None This case handles dequantization float we expect no zero point no zero point domain zero_point None raise AssertionError zero_point should None when zero_point_domain None _is_float _type input dtype raise AssertionError f dequantiztion no zero point domain only supported FP types got input dtype dequant = input output_dtype dequant = dequant scale zero_point_domain = ZeroPointDomain FLOAT name raise AssertionError f Unexpected zero point domain zero_point_domain TODO seems detail tinygemm converting uint int probably need refactor mid_point = quant_max + quant_min + This should allocate new memory avoid input modification dequant = input - mid_point dequant = dequant output_dtype dequant = scale zero_point None dequant += zero_point dequant view original_shape output_dtype AffineQuantizedMinMaxObserver AffineQuantizedObserverBase forward input torch Tensor input numel == input input_detached = input detach original_dtype = input_detached dtype granularity None raise AssertionError granularity None block_size = get_block_size input_detached shape granularity shape_for_reduction reduction_dims = _get_reduction_params block_size input_detached size input_detached = input_detached view shape_for_reduction min_val = torch amin input_detached dim=reduction_dims keepdim=False max_val = torch amax input_detached dim=reduction_dims keepdim=False hasattr min_val hasattr max_val min_val = min_val max_val = max_val min_val shape = min_val shape raise AssertionError f Can t update existing min_val - shape mismatch min_val min_val shape = min_val min_val shape max_val shape = max_val shape raise AssertionError f Can t update existing max_val - shape mismatch max_val max_val shape = max_val max_val shape min_val = torch min min_val min_val max_val = torch max max_val max_val min_val copy_ min_val max_val copy_ max_val returning original input input calculate_qparams - tuple torch Tensor torch Tensor hasattr min_val hasattr max_val raise AssertionError Expecting observer has min_val max_val please run observer before calling calculate_qparams choose_qparams_affine_with_min_max min_val max_val mapping_type BlockSize needed because min max already reduced target_dtype quant_min quant_max eps scale_dtype zero_point_dtype preserve_zero zero_point_domain AffineQuantizedMovingAverageMinMaxObserver AffineQuantizedObserverBase __init__ mapping_type MappingType target_dtype torch dtype granularity Granularity averaging_constant= quant_min Optional int = None quant_max Optional int = None eps Optional float = None is_dynamic=False scale_dtype Optional torch dtype = None zero_point_dtype Optional torch dtype = None preserve_zero bool = True zero_point_domain Optional ZeroPointDomain = ZeroPointDomain INT there could some extra args s ignored kwargs is_dynamic = is_dynamic averaging_constant = averaging_constant is_dynamic averaging_constant = raise NotImplementedError MovingAverageMinMaxObserver doesn t support dynamic quantization f averaging constant averaging_constant super __init__ mapping_type=mapping_type target_dtype=target_dtype granularity=granularity quant_min=quant_min quant_max=quant_max eps=eps scale_dtype=scale_dtype zero_point_dtype=zero_point_dtype preserve_zero=preserve_zero zero_point_domain=zero_point_domain forward input torch Tensor input numel == input input_detached = input detach original_dtype = input_detached dtype granularity None raise AssertionError granularity None block_size = get_block_size input_detached shape granularity shape_for_reduction reduction_dims = _get_reduction_params block_size input_detached size input_detached = input_detached view shape_for_reduction min_val = torch amin input_detached dim=reduction_dims keepdim=False max_val = torch amax input_detached dim=reduction_dims keepdim=False hasattr min_val hasattr max_val min_val = min_val max_val = max_val min_val shape = min_val shape raise AssertionError f Can t update existing min_val - shape mismatch min_val min_val shape = min_val min_val shape max_val shape = max_val shape raise AssertionError f Can t update existing max_val - shape mismatch max_val max_val shape = max_val max_val shape min_val = min_val + averaging_constant min_val - min_val max_val = max_val + averaging_constant max_val - max_val min_val copy_ min_val max_val copy_ max_val returning original input input calculate_qparams - tuple torch Tensor torch Tensor hasattr min_val hasattr max_val raise AssertionError Expecting observer has min_val max_val please run observer before calling calculate_qparams choose_qparams_affine_with_min_max min_val max_val mapping_type BlockSize needed because min max already reduced target_dtype quant_min quant_max eps scale_dtype zero_point_dtype preserve_zero zero_point_domain AffineQuantizedPlaceholderObserver AffineQuantizedObserverBase __init__ mapping_type MappingType target_dtype torch dtype granularity Granularity quant_min Optional int = None quant_max Optional int = None eps Optional float = None is_dynamic=False scale_dtype Optional torch dtype = None zero_point_dtype Optional torch dtype = None preserve_zero bool = True zero_point_domain Optional ZeroPointDomain = ZeroPointDomain INT there could some extra args s ignored kwargs is_dynamic = is_dynamic super __init__ mapping_type=mapping_type target_dtype=target_dtype granularity=granularity quant_min=quant_min quant_max=quant_max eps=eps scale_dtype=scale_dtype zero_point_dtype=zero_point_dtype preserve_zero=preserve_zero zero_point_domain=zero_point_domain forward input block_size = get_block_size input shape granularity original_dtype = input dtype input calculate_qparams raise Exception noqa TRY calculate_qparams should called PlaceholderObserver