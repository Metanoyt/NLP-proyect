mypy ignore-errors This module contains classes utilities handling higher-order operators Dynamo It provides functionality tracing transforming control flow constructs like conditions torch cond loops torch while_loop maps torch ops higher_order map other higher-order operations The module includes specialized VariableTracker classes different types higher-order operations along utilities - Speculating capturing subgraphs - Managing control flow - Handling autograd function applications - Supporting function transformations - Processing activation checkpoints These classes work together enable Dynamo correctly trace compile code containing complex control flow patterns higher-order functions while preserving their semantic behavior contextlib functools inspect itertools logging types warnings collections abc Sequence dataclasses dataclass typing Any Optional TYPE_CHECKING torch _C torch fx torch nn torch _dispatch python enable_python_dispatcher torch _dynamo utils get_fake_value torch _dynamo variables builtin BuiltinVariable torch _dynamo variables constant ConstantVariable torch _dynamo variables ctx_manager RepararametrizeModuleContextVariable torch _dynamo variables functions UserFunctionVariable torch _dynamo variables nn_module UnspecializedNNModuleVariable torch _dynamo variables tensor SymNodeVariable torch _guards Source torch _ops HigherOrderOperator torch fx passes shape_prop _extract_tensor_metadata torch utils _pytree pytree graph_break_hints variables exc IncorrectUsage ObservedException UncapturedHigherOrderOpError unimplemented unimplemented_v Unsupported source AttrSource DictGetItemSource utils proxy_args_kwargs set_example_value base VariableTracker dicts ConstDictVariable lazy LazyVariableTracker lists ListVariable TupleVariable TYPE_CHECKING torch _dynamo symbolic_convert InstructionTranslator log = logging getLogger __name__ hc_log = torch _logging getArtifactLogger __name__ hierarchical_compile dataclass OutputSpec Contains treespec output speculated subgraph information mask out constant values output during flattening inserting them back during unflattening Cleaning up constants graph makes graph simpler AOTDispatcher Inductor treespec pytree TreeSpec list True False identify locations const values subgraph output True means value index constant masks_to_filter_const_values Optional list bool = None The actual constant values present subgraph output Note same length mask we just look indices where mask True const_values Optional list Any = None __post_init__ masks_to_filter_const_values None const_values None assert len masks_to_filter_const_values == len const_values raise_hard_error_if_graph_break reason deco fn functools wraps fn graph_break_as_hard_error args kwargs try fn args kwargs except Unsupported ObservedException e sys isinstance e Unsupported exc = UncapturedHigherOrderOpError f reason Got e msg e real_stack msg = e msg hasattr e msg type e real_stack = e real_stack hasattr e real_stack None exc = UncapturedHigherOrderOpError f reason Got msg real_stack raise exc with_traceback sys exc_info None graph_break_as_hard_error deco This function syntax sugar creating dummy new subtracer so newly added nodes added separate subgraph subtracer instead affecting main graph This useful creating sample inputs tracing subgraph For example FlexAttentionHigherOrderVariable we want create several scalars trace score_mod function we don t want operators creates scalar show up graph we could function discard graph changes Example usage discard_graph_changes sample_input= create_sample_inputs speculate_subgraph tx f sample_inputs contextlib contextmanager discard_graph_changes tx ctx = tx output subtracer subgraph_wrapper None try ctx __enter__ yield finally ctx __exit__ None None None check_meta_consistency_vt vars list VariableTracker vars list VariableTracker lhs_name str rhs_name str include_contiguity bool = True - None torch _higher_order_ops utils check_meta_consistency TensorVariable _unwrap_var var isinstance var TensorVariable var proxy node meta example_value isinstance var SymNodeVariable var sym_num isinstance var ConstantVariable var as_python_constant unimplemented f Cannot unwrap var var unwrapped = _unwrap_var var var vars unwrapped = _unwrap_var var var vars check_meta_consistency unwrapped unwrapped lhs_name rhs_name include_contiguity=include_contiguity contextlib contextmanager dynamo_enable_grad tx InstructionTranslator enable=True GradModeVariable org_value = torch is_grad_enabled try GradModeVariable create tx enable initialized=True yield finally GradModeVariable create tx org_value initialized=True contextlib contextmanager dynamo_under_activation_checkpoint tx InstructionTranslator orig_val = tx output current_tracer under_activation_checkpoint try tx output current_tracer under_activation_checkpoint = True yield finally tx output current_tracer under_activation_checkpoint = orig_val find_mismatched_vars var types allow_none=False Recursively finds variables whose type instance specified types Args var The variable check types A tuple allowed types allow_none bool Whether allow None values Defaults False Returns A set variables whose type instance specified types mismatched_vars = set isinstance var TupleVariable ListVariable item var items mismatched_vars update find_mismatched_vars item types allow_none isinstance var ConstDictVariable value var items values mismatched_vars update find_mismatched_vars value types allow_none _is_none var var is_python_constant var as_python_constant None isinstance var types allow_none _is_none var mismatched_vars add var mismatched_vars only_consist_of var types allow_none=False mismatch_vars = find_mismatched_vars var types allow_none=allow_none len mismatch_vars == A more read-able syntax sugar creating UserFunctionVariable f run call_function Make function preserve calling convention original f _make_inlined tx InstructionTranslator f assert callable f Expect f python callable inline_call args kwargs UserFunctionVariable f call_function tx args kwargs inline_call _call_function_and_unflatten_output tx fn args kwargs flat_example_value ret_spec builder wrap_fx_proxy Store invocation call flat_variable = wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function fn args=args kwargs=kwargs example_value=flat_example_value ret_spec masks_to_filter_const_values torch _dynamo external_utils insert_const_values_with_mask During flattening we removed constant values To ensure Dynamo can trace correctly insert back constant values output flat_variable = _make_inlined tx insert_const_values_with_mask flat_variable ret_spec masks_to_filter_const_values ret_spec const_values Transform variable back into list previously made into tuple speculate_subgraph function so respect pytree API typing flat_list_variable = BuiltinVariable list call_function tx flat_variable _make_inlined tx pytree tree_unflatten flat_list_variable ret_spec treespec ret_spec treespec flat_variable _assert_tensors_nonaliasing inputs outputs input_tensor_ids = id t t pytree tree_leaves inputs isinstance t torch Tensor output_tensor_ids = id t t pytree tree_leaves outputs isinstance t torch Tensor assert input_tensor_ids isdisjoint output_tensor_ids inputs function body cannot alias outputs _check_all_tensorvariable args TensorVariable all type realize TensorVariable args unimplemented f Expected all leaves torch Tensor type got type realize args _check_supported_callable_arg tx InstructionTranslator func_var VariableTracker arg_name is_callable = BuiltinVariable callable call_function tx func_var as_python_constant is_callable unimplemented f arg_name should Callable type str func_var _call_while_loop VariableTracker tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker stack_output bool - VariableTracker torch _higher_order_ops while_loop _create_unbacked_symint TensorVariable args kwargs = LazyVariableTracker realize_all args kwargs cond_fn body_fn operands additional_inputs = args Input checks i k enumerate cond_fn body_fn operands v = kwargs pop k None assert i == len args did provide right number non-keyword args args append v kwargs unimplemented f torch while_loop Got unexpected kwargs list kwargs keys len args = unimplemented f Expected arguments got len args \n f Usage while_loop cond_fn body_fn operands cond_fn body_fn input check _check_supported_callable_arg tx cond_fn cond_fn _check_supported_callable_arg tx body_fn body_fn operands input check operands_seq = operands unpack_var_sequence tx additional_inputs input check isinstance additional_inputs ListVariable TupleVariable unimplemented f Expected additional_inputs list tuple got f additional_inputs python_type It seems f internal error please report issue PyTorch additional_inputs_seq = additional_inputs unpack_var_sequence tx discard_graph_changes tx Note must run under discard graph changes unspecialize_carried_inputs tx carry - VariableTracker See NOTE unspecialize int carry unbacked symints isinstance carry ConstantVariable carry python_type int isinstance carry SymNodeVariable example_value = _create_unbacked_symint tx output fake_mode ignore_fresh_unbacked_symbols=True proxy = tx output current_tracer create_graph_input unbacked_symint type example_value example_value SymNodeVariable create tx proxy example_value See NOTE unspecialize constant tensor carry assert isinstance carry TensorVariable cloned_carry = carry clone cloned_carry proxy node meta example_value constant = None cloned_carry clone inputs across subgraphs avoid unbacked memoization fake prop cond_operands_seq = unspecialize_carried_inputs tx carry call_method tx clone args= kwargs= isinstance carry TensorVariable carry carry operands_seq body_operands_seq = unspecialize_carried_inputs tx carry call_method tx clone args= kwargs= isinstance carry TensorVariable carry carry operands_seq create cond subgrpahs cond_r _cond_treespec cond_graph cond_lifted_freevars = speculate_subgraph tx cond_fn cond_operands_seq + additional_inputs_seq while_loop source_target=self value NOTE why we cannot use automatic while_loop The reason we want enforce ordering inputs outputs consistent ordering cond_fn body_fn consistent e g suppose we use automatic we have body_fn ph ph new_a new_b = ph cos ph sin new_a new_b b = torch randn torch randn new_a new_b = body_fn b Using automatic ordering arguments will order they re used In example capture graph looks like captured_body ph ph new_a new_b = ph cos ph add_ new_a new_b This fine when we change calling convention captured_body new_a new_b = captured_body b But while_loop next iteration s input previous iteration output we ll end up feeding captured_body new_a new_b instead So s best we always enforce ordering carried_inputs same outputs flatten_manual set_subgraph_inputs= flatten_manual supports_input_mutation=self supports_input_mutation supports_aliasing=self supports_aliasing remove_consts_from_outputs=False cond_nn_modules = dict tx output nn_modules validate_subgraph_output_types cond_r isinstance cond_r TensorVariable cond_r_meta = _extract_tensor_metadata cond_r proxy node meta example_value include_contiguity=False cond_r_meta dtype = torch bool cond_r_meta shape = torch Size unimplemented f Expected cond_fn scalar tensor bool got cond_r_meta shape isinstance cond_r ConstantVariable short-circuiting while_loop when cond_fn returns constant such True False pred = cond_r as_python_constant pred unimplemented f Infinite loop detected because while_loop s cond_fn always returns same value pred operands create body subgraph body_r body_treespec body_graph body_lifted_freevars = speculate_subgraph tx body_fn body_operands_seq + additional_inputs_seq while_loop source_target=self value set_subgraph_inputs= flatten_manual should_flatten_outputs=True supports_input_mutation=False supports_aliasing=False remove_consts_from_outputs=False validate_subgraph_output_types body_r We set include contiguity=False because we have vmap x HOP tests where include_contiguity=True will call t is_contiguous inside vmap get error querying is_contiguous inside vmap memory_format other than torch contiguous_format yet implemented This okay because stride still checked check_meta_consistency_vt body_r unpack_var_sequence tx operands_seq body_fn_output carried_inputs include_contiguity=False cond_graph body_graph cond_shared _body_shared cond_unique body_unique = _merge_graph_inputs cond_graph cond_lifted_freevars cond_fn body_graph body_lifted_freevars body_fn Note cond_shared body_shared refer same proxy parent graph so using either them OK Use cond_shared doesn t matter additional_lifted_inputs = cond_shared + cond_unique + body_unique body_nn_modules = dict tx output nn_modules cond_gm = torch fx GraphModule cond_nn_modules cond_graph body_gm = torch fx GraphModule body_nn_modules body_graph cond_name = tx output install_subgraph cond_fn cond_gm body_name = tx output install_subgraph body_fn body_gm cond_node = make_attr tx cond_name body_node = make_attr tx body_name operands_proxy = tuple operand as_proxy operand operands_seq additional_inputs_proxy = tuple inp as_proxy inp additional_inputs_seq + additional_lifted_inputs p_args = cond_node body_node operands_proxy additional_inputs_proxy _call_function_and_unflatten_output tx value p_args None body_treespec are_same_graph_modules fn_name a_mod b_mod fake_mode torch _subclasses _fake_tensor_utils _CacheKeyState torch _subclasses fake_tensor extract_tensor_metadata Maps equivalent nodes b node_map = check_all_args a_nodes b_nodes arg_a arg_b zip a_nodes b_nodes isinstance arg_a torch fx Node node_map arg_a = arg_b False isinstance arg_a slice isinstance arg_b slice False check_all_args arg_a start arg_a stop arg_a step arg_b start arg_b stop arg_b step False arg_a = arg_b This catch-all everything ` slice ` surprise can there other data structures can contain fx Nodes them False True a_node b_node zip a_mod graph nodes b_mod graph nodes a_node op = b_node op False a_node op == placeholder a_value = a_node meta example_value b_value = b_node meta example_value isinstance a_value torch Tensor isinstance b_value torch Tensor False Extract fake tensor metadata b then compare a_result = state = _CacheKeyState fake_mode shape_env a_metadata = extract_tensor_metadata a_value a_metadata _flatten_into a_result fake_mode state b_result = state = _CacheKeyState fake_mode shape_env b_metadata = extract_tensor_metadata b_value b_metadata _flatten_into b_result fake_mode state a_result = b_result False isinstance a_value torch SymInt isinstance b_value torch SymInt False a_value b_value False a_node op == call_function a_node target b_node target False a_flat _ = pytree tree_flatten a_node args a_node kwargs b_flat _ = pytree tree_flatten b_node args b_node kwargs check_all_args a_flat b_flat hc_log debug s Graph comparison failed node call_function s fn_name a_node False a_node op == call_method a_node target = b_node target False a_flat _ = pytree tree_flatten a_node args a_node kwargs b_flat _ = pytree tree_flatten b_node args b_node kwargs check_all_args a_flat b_flat hc_log debug s Graph comparison failed node call_method s fn_name a_node False a_node op == output a_flat _ = pytree tree_flatten a_node args a_node kwargs b_flat _ = pytree tree_flatten b_node args b_node kwargs check_all_args a_flat b_flat hc_log debug s Graph comparison failed output node fn_name False a_node op == get_attr a_attr = getattr a_mod a_node target b_attr = getattr b_mod b_node target isinstance a_attr torch fx GraphModule isinstance b_attr torch fx GraphModule False This example HOP inside HOP are_same_graph_modules fn_name a_attr b_attr fake_mode False TODO - write example tensor graph attribute Fx graph raise NotImplementedError f get_attr type a_attr TODO - call_module supported because Dynamo Fx graph does install call_module raise NotImplementedError f Graph equivalence check saw a_node op Two nodes equal - add them them map node_map a_node = b_node True validate_args_and_maybe_create_graph_inputs sub_args tracer tx set_subgraph_inputs description sub_args_names=None AutogradFunctionContextVariable builder wrap_fx_proxy_cls assert tracer parent None set_subgraph_inputs == flatten_manual flat_args tree_spec = _make_inlined tx pytree tree_flatten ListVariable sub_args unpack_var_sequence tx flat_inputs = validate_args_and_maybe_create_graph_inputs flat_args unpack_var_sequence tx tracer tx set_subgraph_inputs= manual description=description _make_inlined tx pytree tree_unflatten ListVariable flat_inputs tree_spec unpack_var_sequence tx sub_args_names None Can greater user passes some args kwargs assert len sub_args_names = len sub_args args = idx enumerate sub_args assert isinstance VariableTracker set_subgraph_inputs == automatic args append continue set_subgraph_inputs == semi_automatic isinstance AutogradFunctionContextVariable example_value = as_proxy node meta example_value arg_name = as_proxy node name sub_args_names None sub_args_names idx tracer create_graph_input arg_name python_type example_value maybe_fx_node None node = maybe_fx_node example_value = node meta example_value arg_name = as_proxy node name sub_args_names None sub_args_names idx new_proxy = tracer create_graph_input arg_name python_type example_value example_value = node meta get example_value None = wrap_fx_proxy_cls target_cls=type tx=tx proxy=new_proxy example_value=example_value args append continue is_python_constant This arg used body higher order op Currently new input added make calls happy which expect fixed number arguments In future we can clean up arg_name = const_unused sub_args_names None f const_unused_ sub_args_names idx tracer create_graph_input arg_name python_type as_python_constant new_arg = Weird special case we probably want delete fold into next case ` ` being placeable into graph isinstance AutogradFunctionContextVariable example_value = as_proxy node meta example_value arg_name = as_proxy node name sub_args_names None sub_args_names idx tracer create_graph_input arg_name python_type example_value new_arg = If ` ` can put into graph maybe_fx_node None node = maybe_fx_node example_value = node meta get example_value None arg_name = node name sub_args_names None sub_args_names idx new_proxy = tracer create_graph_input arg_name python_type example_value new_arg = wrap_fx_proxy_cls target_cls=type tx=tx proxy=new_proxy example_value=example_value If ` ` cannot put into graph HOPs work much better they use speculate_subgraph set_subgraph_inputs= automatic unimplemented f description body accepts non-Tensors input f Got python_type args append new_arg args This helper function used make sure two graphs share same input signature For example torch cond two branches might lift different set tensors inputs This function helps dedup inputs modify graphs take same set inputs _merge_graph_inputs l_graph l_lifted_freevars l_name r_graph r_lifted_freevars r_name dedup_and_sort_lifted_freevars l_lifted_freevars r_lifted_freevars The nn module attributes guaranteed registered into top-level graph module during higher order op speculation Therefore get_attr nodes two branches same target refer same attribute we can safely deduplicate them their target Note ideally dynamo should just create single proxy same attribute nn module But true_branch false_branch belong two separate tracing contexts they may register same attribute top level separately This creates two get_attr proxies same attribute have different meta data such stack_trace one stack trace true_branch other false_branch It seems better discard proxy explicitly cond than make dynamo create single proxy same get_attr target shared_getattrs l_lifted_proxies r_lifted_proxies true_targets = proxy node target proxy proxy l_lifted_proxies proxy node op == get_attr l_shared_getattrs = r_shared_getattrs = false_proxy r_lifted_proxies false_proxy node op == get_attr false_proxy node target true_targets true_proxy = true_targets false_proxy node target l_shared_getattrs true_proxy = true_proxy r_shared_getattrs false_proxy = true_proxy l_shared_getattrs r_shared_getattrs l_shared_getattrs r_shared_getattrs = shared_getattrs l_lifted_freevars keys r_lifted_freevars keys l_shared_freevars = l_lifted_freevars keys r_lifted_freevars keys union l_shared_getattrs keys r_shared_freevars = l_lifted_freevars keys r_lifted_freevars keys union r_shared_getattrs keys unique_l_freevars = l_lifted_freevars keys - l_shared_freevars unique_r_freevars = r_lifted_freevars keys - r_shared_freevars _sort_by_name vars sorted vars key=lambda var var node name list _sort_by_name list l_shared_freevars list _sort_by_name list r_shared_freevars list _sort_by_name list unique_l_freevars list _sort_by_name list unique_r_freevars l_shared r_shared unique_l unique_r = dedup_and_sort_lifted_freevars l_lifted_freevars r_lifted_freevars Let s say we capture cond pred true_fn false_fn x With set_graph_input set automatic true_fn has lifted variables x b c false_fn has lifted variables x b d Then fixup_branch_inps make sure both branches have same signature i e - true_fn x b c_true_branch d_false_branch - false_fn x b c_true_branch d_false_branch More formally signature has three parts following order used both branches x b only used true branches c suffixed _true_branch only used false branches d suffixed _false_branch Within each part we re-order nodes name have derterministic ordering testing fixup_branch_inps graph lifted_freevars shared unique_l unique_r _insert_or_replace_phs new_args name_suffix arg new_args new_ph = graph placeholder arg node name + name_suffix new_ph meta = arg node meta Override new_ph there exists old placeholder arg lifted_freevars old_ph = lifted_freevars arg node old_ph replace_all_uses_with new_ph replace_all_uses_with doesn t clean users Clean manually so we could erase old_ph users = graph erase_node old_ph first_not_ph_node = next node node graph nodes node op = placeholder graph inserting_before first_not_ph_node _insert_or_replace_phs shared _insert_or_replace_phs unique_l _ + l_name _insert_or_replace_phs unique_r _ + r_name fixup_branch_inps l_graph l_lifted_freevars l_shared unique_l unique_r fixup_branch_inps r_graph r_lifted_freevars r_shared unique_l unique_r l_graph r_graph l_shared r_shared unique_l unique_r See NOTE HigherOrderOperator tracing design details design speculate_subgraph tx f sub_args sub_kwargs description source_target value HigherOrderOpVariable target proxy we created higherOrderOperator source_target=None always_restore=False enable_grad=None NOTE argument ` set_subgraph_inputs ` set_subgraph_inputs controls what how construct subgraphs placeholders sub_args your HOP supports arbitrary inputs use set_subgraph_inputs= automatic most recommended your HOP supports only Tensor symnode inputs use set_subgraph_inputs= flatten_manual recommended If sub_args contain Pytree structure e g dict list tuple set sub_args will flattened first Then flattened args manually set subgraph s placeholders your HOP must preserve inputs tensor symnode placeholders e g AutogradFunctionContextVariable use set_subgraph_inputs= manual recommended We do recommend general because has restriction user need manually control how create placeholders VariableTrackers args set_subgraph_inputs= automatic restore_side_effects=True should_flatten_outputs=False should_flatten_outputs True ` remove_consts_from_outputs ` remove const outputs subgraph output remove_consts_from_outputs=True under_activation_checkpoint=False TODO - supports input_mutation aliasing should False default strictness supports_input_mutation=True supports_aliasing=True Pass originating tracer - needed preserving context across fwd-bwd autograd Function tracer=None sub_kwargs None sub_kwargs = assert set_subgraph_inputs automatic semi_automatic flatten_manual manual Please use one supported set_subgraph_inputs options See NOTE Temporary argument ` set_subgraph_inputs ` sub_kwargs set_subgraph_inputs = automatic unimplemented Use ` set_subgraph_inputs=automatic ` when passing ` sub_kwargs ` try ensure guards args get installed parent subgraph f sub_args sub_kwargs = LazyVariableTracker realize_all f sub_args sub_kwargs tx output subtracer source_target tracer subtracer sub_args_names = maybe_positional_arg_names f User mismatch number args Will eventually lead error sub_args_names None len sub_args_names len sub_args sub_args_names = None args = validate_args_and_maybe_create_graph_inputs sub_args subtracer tx set_subgraph_inputs description sub_args_names validate_args_and_maybe_create_graph_inputs sub_kwargs values subtracer tx set_subgraph_inputs= automatic description=description autograd_ctx = dynamo_enable_grad tx enable_grad enable_grad None contextlib nullcontext checkpoint_ctx = dynamo_under_activation_checkpoint tx under_activation_checkpoint contextlib nullcontext For handling side effects we can make argument we don t have do anything here The side effects infra does good job graph breaking we mutate any nonlocal global variable while subtracing As result tracing succeeds side effects data structure will only contain read-only data structures put there tracking purposes But other hand there argument we ever write new side effect Dynamo which does go through side effect infra we can end up bad state Therefore we restore side effects after tracing The catch we have special handle tensor variables If we have seen nonlocal variable tensor during subtracing we want keep track tensor so later subtracing root tracer itself does create new proxy already observed tensor variable restore_side_effects prev_side_effects = tx output side_effects clone autograd_ctx checkpoint_ctx output = f call_function tx args sub_kwargs restore_side_effects new_side_effects = tx output side_effects clone prev_side_effects track_runahead_tensor_and_symvar_side_effects new_side_effects tx output side_effects = prev_side_effects treespec = None masks_to_filter_const_values = None const_values = None should_flatten_outputs torch _dynamo external_utils filter_out_const_values Flatten speculated subgraph output output treespec = _make_inlined tx pytree tree_flatten output unpack_var_sequence tx Actually transform list returned flatten into tuple dynamo consistency output = BuiltinVariable tuple call_function tx output remove_consts_from_outputs Filter out constants save them into spec Filtering out constants makes graph simpler backends We need ensure after unflattening constants inserted back right positions Dynamo tracing continue This done filter_const_spec output_proxies = output as_proxy masks_to_filter_const_values = pytree tree_map lambda x isinstance x torch fx Proxy output_proxies const_values = pytree tree_map lambda x None isinstance x torch fx Proxy x output_proxies output = _make_inlined tx filter_out_const_values output masks_to_filter_const_values Register output graph Modeled off compile_and_call_fx_graph TODO support pytree output We check always_restore because we dont use output side effects always_restore code like bwd always_restore Nothing left do here output OutputSpec treespec masks_to_filter_const_values const_values tx output graph subtracer lifted_freevars validate_subgraph_output_types output The output proxies might belong SubgraphTracer they free variables never lifted so lift them here output_proxies = output as_proxy output_proxies = pytree tree_map subtracer maybe_lift_tracked_freevar_to_input output_proxies tx output create_node output output subtracer create_arg output_proxies graph = tx output graph graph lint lifted_freevars = subtracer lifted_freevars NOTE HigherOrderOperator subgraph input ordering The input ordering higher order ops determined order creation placeholder Manually created inputs created validate_args_and_maybe_create_graph_inputs before speculating subgraph During subgraph speculation we may lift closured tensors free symbols inputs their ordering determined time they lifted earlier lifted ones precede later lifted ones Suppose placeholders O O X O O X X O where Xs lifted phs The following code re-order placeholders O O O O O X X X move_lifted_freevars_phs_to_end graph torch fx Graph lifted_freevars tuple torch fx Node lifted_ph_set = child_p node child_p lifted_freevars values prev_phs = n n graph nodes n op == placeholder No need reorder when graph doesn t have args doesn t have lifted freevars all inputs lifted freevars len prev_phs == len lifted_ph_set == len prev_phs == len lifted_ph_set Step find first X x prev_phs x lifted_ph_set break assert x None x op == placeholder Step starting X skip Xs prepend Os before X cand_x = x next while cand_x None cand_x op == placeholder cand_x lifted_ph_set cand_x = cand_x next nxt = cand_x next cand_x _remove_from_list x prepend cand_x cand_x = nxt Step assert all placeholders correct order lifted_freevars after_phs = node node graph nodes node op == placeholder -len lifted_freevars assert len after_phs == len lifted_freevars child_proxy ph zip lifted_freevars values after_phs assert child_proxy node ph The order placeholders different order lifted_freevars graph lint len lifted_freevars move_lifted_freevars_phs_to_end graph lifted_freevars supports_input_mutation mutation_info = subtracer has_input_mutation mutation_info has_mutation context = f mutation_info msg in\n graph unimplemented_v gb_type= Encountered input mutation during higher order op tracing context=context explanation=f Higher order ops do support input mutation Found source_target name hints= Consider using debug context change user code avoid mutation Please open issue supports_aliasing aliasing_info = subtracer has_aliasing aliasing_info has_aliasing context = f aliasing_info msg in\n graph unimplemented_v gb_type= Encountered aliasing during higher order op tracing context=context explanation=f Higher order ops do support aliasing Found source_target name hints= Replace ` input ` ` input clone ` avoid aliasing Consider using debug context change user code avoid aliasing Please open issue output OutputSpec treespec masks_to_filter_const_values const_values graph lifted_freevars except Unsupported ex f_name = f type f __name__ isinstance f UserFunctionVariable f_name = f get_name msg = f speculate_subgraph while introspecting description we unable f trace function ` f_name ` into single graph This means f Dynamo unable prove safety API will f fall back eager-mode PyTorch which could lead slowdown log info msg log info ex noqa G raise ex make_attr tx InstructionTranslator name node = tx output create_proxy get_attr name node TorchHigherOrderOperatorVariable VariableTracker __init__ value HigherOrderOperator source Optional Source = None kwargs - None super __init__ kwargs value = value source = source staticmethod make value source=None kwargs variable_class = _hop_name_to_variable_class get value __name__ variable_class None variable_class value source kwargs torch _higher_order_ops BaseHOP isinstance value BaseHOP BaseHOPVariable value source kwargs unimplemented f HigherOrderOperator value __name__ call_function tx InstructionTranslator args Sequence VariableTracker kwargs dict str VariableTracker - VariableTracker torch_function can_dispatch_torch_function dispatch_torch_function can_dispatch_torch_function tx args kwargs dispatch_torch_function tx args kwargs _call_function tx args kwargs _call_function tx InstructionTranslator args Sequence VariableTracker kwargs dict str VariableTracker - VariableTracker unimplemented f HigherOrderOperator value __name__ as_python_constant value CustomFunctionHigherOrderOperatorVariable TorchHigherOrderOperatorVariable Wraps torch _functorch autograd_function custom_function_call _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker torch _dynamo variables UserMethodVariable value __call__ __func__ torch _dynamo variables UserDefinedObjectVariable value source=self source source=AttrSource source __call__ call_function tx args kwargs CondHigherOrderVariable TorchHigherOrderOperatorVariable supports_input_mutation = False supports_aliasing = False raise_hard_error_if_graph_break reason= Cond doesn t work unless captured completely torch compile _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker ListVariable TensorVariable args kwargs = LazyVariableTracker realize_all args kwargs i k enumerate pred true_fn false_fn operands v = kwargs pop k None assert i == len args did provide right number non-keyword args args append v kwargs unimplemented f torch cond Got unexpected kwargs list kwargs keys TODO voz Support fake tensor dispatch recursive ops - see torch dispatch _dispatcher py len args = unimplemented f Expected arguments got len args \n f Usage cond pred true_fn false_fn operands Specialize into one branches since pred constant pred true_fn false_fn operands = args type args ConstantVariable warnings warn Pred Python constant When used torch cond specializes one branches If you want torch cond preserve two branches please make predicate boolean tensor SymBool UserWarning pred as_python_constant true_fn call_function tx operands unpack_var_sequence tx false_fn call_function tx operands unpack_var_sequence tx predicate type pred ConstantVariable TensorVariable SymNodeVariable unimplemented f Expected pred bool boolean tensor single f item got str type pred f original python type str pred python_type operands isinstance operands ListVariable TupleVariable unimplemented f Expected operands list tuple got f operands python_type operands_seq = operands unpack_var_sequence tx only_consist_of operands TensorVariable ConstantVariable SymNodeVariable unimplemented Expect operands tuple pytrees only consists tensor leaves branches _check_supported_callable_arg tx true_fn true_fn _check_supported_callable_arg tx false_fn false_fn Our strategy tracing true false branches cond checkpoint our graphstate run true branch roll back checkpoint run false branch then merge graphstates Well perhaps merge too strong word we mostly assert resulting graphstates have same We only permit guards diverge we union guards both branches In particular means side effects NOT permitted inside true false branches would difficult implement because path explosion problem speculate_branch branch NB predicate ix = branch TODO Support kwargs ret_val ret_spec ret_graph ret_lifted_freevars = speculate_subgraph tx args ix operands_seq cond source_target=self value should_flatten_outputs=True TODO - removing consts control flow ops need more work remove_consts_from_outputs=False supports_input_mutation=self supports_input_mutation supports_aliasing=self supports_aliasing need ensure we increase epoch so we don t memoize unbacked bindings across different subgraphs which can interfere runtime assertion generation tx fake_mode epoch += only_consist_of ret_val TensorVariable ConstantVariable unimplemented Expected branches possibly nested pytree tensors constant ints consists others ret ret_val unpack_var_sequence tx isinstance ret ConstantVariable ret python_type int unimplemented Expected branches possibly nested pytree tensors f constant ints consists others ret python_type ret_val ret_spec ret_graph ret_lifted_freevars true_r true_spec true_graph true_lifted_freevars = speculate_branch True true_nn_modules = dict tx output nn_modules false_r false_spec false_graph false_lifted_freevars = speculate_branch False false_nn_modules = dict tx output nn_modules same_spec = _make_inlined tx pytree TreeSpec __eq__ true_spec treespec false_spec treespec as_python_constant NotImplemented cannot converted bool same_spec NotImplemented same_spec unimplemented Expected branches same pytree structure true_graph false_graph true_shared _false_shared unique_true unique_false = _merge_graph_inputs true_graph true_lifted_freevars true_branch false_graph false_lifted_freevars false_branch true_name = tx output install_subgraph cond_true torch fx GraphModule true_nn_modules true_graph false_name = tx output install_subgraph cond_false torch fx GraphModule false_nn_modules false_graph true_node = make_attr tx true_name false_node = make_attr tx false_name p_args = pred as_proxy true_node false_node We pick true_shared shouldn t matter tuple true_shared + unique_true + unique_false _call_function_and_unflatten_output tx torch ops higher_order cond p_args None true_spec CallTorchbindHigherOrderVariable TorchHigherOrderOperatorVariable __init__ hop source script_obj_var method_name - None super __init__ hop source script_obj_var = script_obj_var method_name = method_name _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker builder wrap_fx_proxy args kwargs = LazyVariableTracker realize_all args kwargs args_proxy = arg as_proxy arg args kwargs_proxy = k v as_proxy k v kwargs items wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function value args=tuple script_obj_var as_proxy method_name + args_proxy kwargs=kwargs_proxy validate_subgraph_output_types output VariableTracker Verify output subgraph tensor int bool SymBool SymInt TensorVariable non_tensor_output = find_mismatched_vars output TensorVariable allow_none=True out non_tensor_output isinstance out SymNodeVariable out python_type int bool isinstance out ConstantVariable out python_type int bool continue unimplemented f HigherOrderOperator body s output must consist tensors ints only got out python_type WhileLoopHigherOrderVariable TorchHigherOrderOperatorVariable supports_input_mutation = False supports_aliasing = False raise_hard_error_if_graph_break reason= while_loop doesn t work unless captured completely torch compile _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker _call_while_loop tx args kwargs stack_output=False WhileLoopStackOutputHigherOrderVariable TorchHigherOrderOperatorVariable supports_input_mutation = False supports_aliasing = False raise_hard_error_if_graph_break reason= while_loop_stack_output doesn t work unless captured completely torch compile _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker _call_while_loop tx args kwargs stack_output=True AssociativeScanHigherOrderVariable TorchHigherOrderOperatorVariable supports_input_mutation = False supports_aliasing = False raise_hard_error_if_graph_break reason= associative_scan must captured completely torch compile _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker torch _higher_order_ops utils first_slice_copy args kwargs = LazyVariableTracker realize_all args kwargs arg_extractor combine_fn xs additional_inputs combine_fn xs additional_inputs combine_fn xs additional_inputs = arg_extractor args kwargs args python_type functools partial This standard case when user calls frontend frontend invokes dynamo len args = unimplemented f Expected positional arguments got len args \n f Usage associative_scan combine_fn xs xs_treespec = args keywords spec combine_fn input check We need get pure combine_fn functools partial _check_supported_callable_arg tx combine_fn keywords combine_fn combine_fn This case hit during re-tracing example export tests In case combine_fn callable functools partial xs_treespec = _make_inlined tx pytree tree_structure xs _check_supported_callable_arg tx combine_fn combine_fn xs input check isinstance xs ListVariable TupleVariable unimplemented f Expected xs list tuple got f xs python_type It seems f internal error please report issue PyTorch xs_vars = xs unpack_var_sequence tx _check_all_tensorvariable xs_vars additional_inputs input check isinstance additional_inputs ListVariable TupleVariable unimplemented f Expected additional_inputs list tuple got f additional_inputs python_type It seems f internal error please report issue PyTorch additional_inputs_vars = additional_inputs unpack_var_sequence tx _check_all_tensorvariable additional_inputs_vars scan_length = get_fake_value xs_vars as_proxy node tx size scan_length == unimplemented associative_scan operator doesn t support zero-sized tensors during tracing Trace subgraph The sub_args slice original input e g input size scan dim= sub_args shape will discard_graph_changes tx sub_args = _make_inlined tx first_slice_copy leaf leaf itertools chain xs_vars xs_vars sub_args_additional_inputs = t call_method tx clone args= kwargs= t additional_inputs_vars sub_args = sub_args + sub_args_additional_inputs combine_result _combine_spec combine_graph combine_lifted_freevars = speculate_subgraph tx combine_fn sub_args sub_kwargs= description= associative_scan_combine_fn source_target=self value set_subgraph_inputs= flatten_manual supports_input_mutation=self supports_input_mutation supports_aliasing=self supports_aliasing Ensure output scan flattened list elements because downstream operations assume output HOPs flattened output_node = combine_graph find_nodes op= output output_node args = pytree tree_leaves output_node args combine_graph lint Collect results combine_fn results _combine_treespec = _make_inlined tx pytree tree_flatten combine_result unpack_var_sequence tx Check whether combine_fn returns one child tree output _combine_treespec as_python_constant num_leaves unimplemented f combine_fn needs produce one pytree output f combine_fn produces pytree _combine_treespec as_python_constant Check whether outs produced combine_fn has same treespec xs We need have check way because case init TreeSpec carry carry only LeafSpec these two cannot compared correctly xs_treespec as_python_constant is_leaf = _combine_treespec as_python_constant is_leaf _make_inlined tx pytree TreeSpec __eq__ xs_treespec _combine_treespec as_python_constant unimplemented f The tree structure xs outs combine_fn expected identical got f xs xs_treespec as_python_constant vs output _combine_treespec as_python_constant We set include contiguity=False because we have vmap x HOP tests where include_contiguity=True will call t is_contiguous inside vmap get error querying is_contiguous inside vmap memory_format other than torch contiguous_format yet implemented This okay because stride still checked check_meta_consistency_vt _make_inlined tx first_slice_copy t t xs_vars results items initial_xs combine_fn_output include_contiguity=False combine_gm = torch fx GraphModule dict tx output nn_modules combine_graph combine_freevars_proxy = tuple combine_lifted_freevars keys Compute proxies input check proxy_vars_inputcheck = tuple sarg as_proxy sarg sub_args + combine_freevars_proxy torch _higher_order_ops utils _maybe_fake_tracing torch _inductor utils is_pointwise_use tx fake_mode sub_args_fake = leaf node meta example_value clone hasattr leaf node meta example_value clone leaf node meta example_value leaf pytree tree_leaves proxy_vars_inputcheck pre_dispatch = False fx = _maybe_fake_tracing combine_gm sub_args_fake pre_dispatch=pre_dispatch node fx graph nodes Check combine_fn pointwise combine_mode= pointwise all is_pointwise_use use use op == output use node users raise RuntimeError For combine_mode= pointwise combine_fn needs pointwise combine_fn_name = tx output install_subgraph associative_scan_combine_fn combine_gm Compute proxies xs_proxy = xs as_proxy combine_freevars_proxy = tuple combine_lifted_freevars keys additional_inputs_proxy = additional_inputs as_proxy + combine_freevars_proxy p_args = make_attr tx combine_fn_name xs_proxy additional_inputs_proxy _call_function_and_unflatten_output tx torch ops higher_order associative_scan p_args None OutputSpec xs_treespec ScanHigherOrderVariable TorchHigherOrderOperatorVariable supports_input_mutation = False supports_aliasing = False raise_hard_error_if_graph_break reason= scan must captured completely torch compile _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker torch _higher_order_ops scan _extract_carry_and_out torch _higher_order_ops utils first_slice_copy args kwargs = LazyVariableTracker realize_all args kwargs combine_fn input check _check_combine_fn_is_normalized combine_fn_var isinstance combine_fn_var variables nn_module NNModuleVariable variables nn_module UnspecializedNNModuleVariable variables FunctoolsPartialVariable unimplemented f Expected combine_fn wrapped functools partial scan user-facing api f graph module we re re-exporting got f combine_fn python_type Please report issue PyTorch you re seeing isinstance combine_fn_var variables nn_module NNModuleVariable variables nn_module UnspecializedNNModuleVariable arg_extractor combine_fn init xs additional_inputs combine_fn init xs additional_inputs combine_fn init xs additional_inputs = arg_extractor args kwargs init_vars = init unpack_var_sequence tx xs_vars = xs unpack_var_sequence tx additional_inputs_vars = additional_inputs unpack_var_sequence tx combine_fn input check combine_fn_is_normalized = _check_combine_fn_is_normalized combine_fn combine_fn_is_normalized combine_gm = combine_fn value assert isinstance combine_gm torch fx GraphModule combine_fn combine_gm combine_fn input check We need get pure combine_fn functools partial _check_supported_callable_arg tx combine_fn keywords combine_fn combine_fn xs input check isinstance xs ListVariable TupleVariable unimplemented f Expected xs list tuple got f xs python_type It seems f internal error please report issue PyTorch init input check isinstance init ListVariable TupleVariable unimplemented f Expected init list tuple least one element got f init python_type It seems f internal error please report issue PyTorch len init_vars == unimplemented scan operator requires init leaves It seems internal error please report issue PyTorch additional_inputs input check isinstance additional_inputs ListVariable TupleVariable unimplemented f Expected additional_inputs list tuple got f additional_inputs python_type It seems f internal error please report issue PyTorch scan_length check scan_length = get_fake_value xs_vars as_proxy node tx size scan_length == unimplemented NYI scan operator doesn t support zero scan_length _check_all_tensorvariable init_vars _check_all_tensorvariable xs_vars _check_all_tensorvariable additional_inputs_vars discard_graph_changes tx sub_args_init = ini call_method tx clone args= kwargs= ini init_vars The sub_args_inp slice original input e g input size scan dim= sub_args_inp shape will sub_args_inp = _make_inlined tx first_slice_copy inp inp xs_vars sub_args_additional_inputs = t call_method tx clone args= kwargs= t additional_inputs_vars sub_args = sub_args_init + sub_args_inp + sub_args_additional_inputs combine_result _combine_spec combine_graph combine_lifted_freevars = speculate_subgraph tx combine_fn sub_args sub_kwargs= description= scan_combine_fn source_target=self value set_subgraph_inputs= flatten_manual supports_input_mutation=self supports_input_mutation supports_aliasing=self supports_aliasing Ensure output scan flattened list elements because downstream operations assume output HOPs flattened output_node = combine_graph find_nodes op= output output_node args = pytree tree_leaves output_node args combine_graph lint combine_freevars_proxy = list combine_lifted_freevars keys combine_result_vars = combine_result unpack_var_sequence tx combine_fn_is_normalized carry_vars out_vars = _extract_carry_and_out combine_result_vars len init_vars len combine_result_vars = unimplemented f Expect combine_fn tuple next_carry y got combine_result_vars carry_tree out_vars = combine_result_vars carry_vars _ = _make_inlined tx pytree tree_flatten carry_tree unpack_var_sequence tx carry_vars = carry_vars unpack_var_sequence tx out_vars = _make_inlined tx pytree tree_leaves out_vars unpack_var_sequence tx additional output checking _combine_spec = OutputSpec _make_inlined tx pytree tree_structure combine_result check_meta_consistency_vt init_vars carry_vars init carry Check meta data carries inits If we pass stage we sure init carries have same tree structure We set include contiguity=False because we have vmap x HOP tests where include_contiguity=True will call t is_contiguous inside vmap get error querying is_contiguous inside vmap memory_format other than torch contiguous_format yet implemented This okay because stride still checked check_meta_consistency_vt init_vars carry_vars init carry include_contiguity=False xs_proxy = xs as_proxy init_proxy = init as_proxy additional_inputs_proxy = list additional_inputs as_proxy + list combine_freevars_proxy combine_gm = torch fx GraphModule dict tx output nn_modules combine_graph combine_fn_name = tx output install_subgraph scan_combine_fn combine_gm p_args = make_attr tx combine_fn_name init_proxy xs_proxy additional_inputs_proxy _call_function_and_unflatten_output tx torch ops higher_order scan p_args None _combine_spec non_single_tensor_return_unsupported api ret TensorVariable isinstance ret TensorVariable raise Unsupported f api over function returns something other than one Tensor MapHigherOrderVariable TorchHigherOrderOperatorVariable supports_input_mutation = False supports_aliasing = False raise_hard_error_if_graph_break reason= map doesn t work unless captured completely torch compile _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker args kwargs = LazyVariableTracker realize_all args kwargs len kwargs unimplemented torch ops higher_order map kwargs supported map operator _check_supported_callable_arg tx args map_fn args = f flat_xs flat_args assert isinstance args ListVariable TupleVariable args assert isinstance args ListVariable TupleVariable args unpacked_xs = args unpack_var_sequence tx unpacked_args = args unpack_var_sequence tx sample_shape = get_fake_value unpacked_xs as_proxy node tx size len sample_shape sample_shape == unimplemented map operator doesn t support scalar zero-sized tensors during tracing To get example output map we will need provide least one sample loop body In our case we will always use xs our map won t support zero sized tensor during tracing discard_graph_changes tx sliced_xs = xs call_method tx select args= VariableTracker build tx VariableTracker build tx kwargs= xs unpacked_xs TODO Support kwargs body_r body_spec body_graph body_lifted_freevars = speculate_subgraph tx args sliced_xs unpacked_args torch ops higher_order map source_target=self value set_subgraph_inputs= flatten_manual should_flatten_outputs=True TODO - removing consts control flow ops need more work remove_consts_from_outputs=False supports_input_mutation=self supports_input_mutation supports_aliasing=self supports_aliasing Check all outputs map tensors For map outputting None OK thus ignore None values check body_r_vars = body_r unpack_var_sequence tx none_mask = type x realize ConstantVariable x as_python_constant None x body_r_vars _check_all_tensorvariable br bm br zip none_mask body_r_vars bm body_nn_modules = dict tx output nn_modules body_name = tx output install_subgraph map_body torch fx GraphModule body_nn_modules body_graph body_node = make_attr tx body_name p_args = body_node xs as_proxy xs unpacked_xs arg as_proxy arg unpacked_args + list body_lifted_freevars keys _call_function_and_unflatten_output tx torch ops higher_order map_impl p_args None body_spec ExecutorchCallDelegateHigherOrderVariable TorchHigherOrderOperatorVariable _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker builder wrap_fx_proxy This operator delegation within Executorch which calls specific function given lowered module given operators The actual operator defined Executorch codebase This bad hierarchical violation since executorch_call_delegate sits higher level than dynamo there s no real solution issue yet len kwargs unimplemented executorch_call_delegate kwargs arguments enabled isinstance args variables NNModuleVariable lowered_module = tx output get_submodule args module_key lowered_node = make_attr tx args module_key isinstance args variables UnspecializedNNModuleVariable This nn module special sa delegated executorch Just install attr graph lowered_module = args value lowered_node = tx output register_static_attr_and_return_proxy delegate lowered_module p_args = tuple arg as_proxy arg args real_sub_args = pytree tree_map_only torch fx Proxy lambda get_fake_value node tx p_args tx fake_mode example_value = lowered_module original_module module real_sub_args NOTE Guaranteeing - correspondence FakeTensors real tensors executorch modules promise alias inputs outputs Thus output FakeTensors will correctly alias input FakeTensors _assert_tensors_nonaliasing real_sub_args example_value p_args = lowered_node + p_args Store invocation call wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function value args=tuple p_args kwargs= example_value=example_value FunctorchHigherOrderVariable UserFunctionVariable call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker super call_function tx args kwargs FunctionalCallVariable FunctorchHigherOrderVariable call_function tx args list VariableTracker kwargs dict str VariableTracker - VariableTracker torch _dynamo config inline_inbuilt_nn_modules unimplemented torch func functional_call capture disabled can turned setting ` torch _dynamo config inline_inbuilt_nn_modules=True ` super call_function tx args kwargs ReparametrizeModuleCallVariable FunctorchHigherOrderVariable __init__ args kwargs super __init__ args kwargs call_function tx args list VariableTracker kwargs dict str VariableTracker - VariableTracker ctx_manager_vt = super call_function tx args kwargs RepararametrizeModuleContextVariable ctx_manager_vt args WrapHigherOrderVariable TorchHigherOrderOperatorVariable supports_input_mutation = True supports_aliasing = True TODO - Go through all subclasses WrapHigherOrderVariable see restore_side_effects can ignored For now conservative restore_side_effects = True install_subgraph_in_output_graph tx fn_vt fn_args_vt kwargs body_gmod attr_name= wrap_body tx output install_subgraph f attr_name body_gmod create_wrapped_node tx InstructionTranslator fn_vt fn_args_vt kwargs description under_activation_checkpoint=False subgraph_name= wrap_body See NOTE HigherOrderOperator tracing design more details body_r treespec body_graph body_lifted_freevars = speculate_subgraph tx fn_vt fn_args_vt kwargs description source_target=self value restore_side_effects=self restore_side_effects should_flatten_outputs=True under_activation_checkpoint=under_activation_checkpoint supports_input_mutation=self supports_input_mutation supports_aliasing=self supports_aliasing body_gmod = torch fx GraphModule tx output nn_modules body_graph body_name = install_subgraph_in_output_graph tx fn_vt fn_args_vt kwargs body_gmod attr_name=subgraph_name body_node = make_attr tx body_name Since we call ` speculate_subgraph ` ` set_subgraph_inputs= automatic ` all arguments lifted lifted_args = tuple arg arg body_lifted_freevars keys proxy_args = body_node + lifted_args example_value = pytree tree_map_only torch fx Proxy lambda node meta example_value body_r as_proxy proxy_args example_value body_r treespec body_gmod body_name _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker This flattens kwargs into lifted args p_args p_kwargs _example_value body_r treespec _ _ = create_wrapped_node tx args args kwargs wrap len p_kwargs unimplemented kwargs should have been flattened into lifted args flat_example_value = pytree tree_map_only torch fx Proxy lambda node meta example_value body_r as_proxy _call_function_and_unflatten_output tx value tuple p_args p_kwargs flat_example_value treespec WrapWithSetGradEnabledHigherOrderVariable TorchHigherOrderOperatorVariable This hop exposed users inserted into graph after export post-processing step call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker args kwargs = LazyVariableTracker realize_all args kwargs kwargs unimplemented f wrap_with_set_grad_enabled Got unexpected kwargs list kwargs keys grad_enabled fn_var rest_args = args isinstance grad_enabled ConstantVariable unimplemented grad_enabled must constant _check_supported_callable_arg tx fn_var enable_grad_fn torch set_grad_enabled grad_enabled as_python_constant body_r treespec body_graph body_lifted_freevars = speculate_subgraph tx fn_var rest_args torch ops higher_order wrap_with_set_grad_enabled source_target=self value set_subgraph_inputs= manual should_flatten_outputs=True len body_lifted_freevars unimplemented f wrap_with_set_grad_enabled Got unexpected freevars body_lifted_freevars body_gmod = torch fx GraphModule tx output nn_modules body_graph body_name = tx output install_subgraph wrap_body body_gmod body_node = make_attr tx body_name proxy_args = tuple grad_enabled as_python_constant body_node + operand as_proxy operand rest_args example_value = pytree tree_map_only torch fx Proxy lambda node meta example_value body_r as_proxy _call_function_and_unflatten_output tx value proxy_args example_value treespec WrapWithAutocastHigherOrderVariable TorchHigherOrderOperatorVariable This hop exposed users inserted into graph after export post-processing step call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker args kwargs = LazyVariableTracker realize_all args kwargs kwargs unimplemented f wrap_with_autocast Got unexpected kwargs list kwargs keys device_type dtype enabled cache_enabled fn_var rest_args = args arg device_type dtype enabled cache_enabled isinstance arg ConstantVariable unimplemented device_type dtype enabled cache_enabled must constants _check_supported_callable_arg tx fn_var autocast python_constants = arg as_python_constant arg device_type dtype enabled cache_enabled torch autocast python_constants body_r treespec body_graph body_lifted_freevars = speculate_subgraph tx fn_var rest_args torch ops higher_order wrap_with_autocast source_target=self value set_subgraph_inputs= manual should_flatten_outputs=True len body_lifted_freevars unimplemented f wrap_with_autocast Got unexpected freevars body_lifted_freevars body_gmod = torch fx GraphModule tx output nn_modules body_graph body_name = tx output install_subgraph wrap_body body_gmod body_node = make_attr tx body_name proxy_args = tuple python_constants body_node + operand as_proxy operand rest_args example_value = pytree tree_map_only torch fx Proxy lambda node meta example_value body_r as_proxy _call_function_and_unflatten_output tx value proxy_args example_value treespec HintsWrapperHigherOrderVariable TorchHigherOrderOperatorVariable raise_hard_error_if_graph_break reason= Hints_wrapper doesn t work unless captured completely torch compile _call_function tx args list VariableTracker kwargs dict str VariableTracker - VariableTracker _check_supported_callable_arg tx args body_fn inputs len args = unimplemented f Expected arguments got len args \n f Usage hints_wrapper body_fn args kwargs hints \n f kwargs required provided explicitly isinstance args ListVariable TupleVariable unimplemented f Expected tuple got args python_type operands = args unpack_var_sequence tx isinstance args ConstDictVariable unimplemented f Expected dict got args python_type hints kwargs raise IncorrectUsage hints_wrapper - key hints provided body_r treespec body_graph body_lifted_freevars = speculate_subgraph tx args function operands args as_python_constant hints_wrapper source_target=self value should_flatten_outputs=True body_gmod = torch fx GraphModule tx output nn_modules body_graph body_name = tx output install_subgraph hints_wrapper_body body_gmod body_node = make_attr tx body_name Since we call ` speculate_subgraph ` ` set_subgraph_inputs= automatic ` all arguments lifted lifted_args = tuple arg arg body_lifted_freevars keys p_args = body_node lifted_args p_kwargs = add hints into p_kwargs p_kwargs hints = kwargs hints as_python_constant flat_example_value = pytree tree_map_only torch fx Proxy lambda node meta example_value body_r as_proxy _call_function_and_unflatten_output tx value p_args p_kwargs flat_example_value treespec OutDtypeHigherOrderVariable TorchHigherOrderOperatorVariable _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker builder wrap_fx_proxy len kwargs unimplemented out_dtype does handle kwargs p_args = tuple arg as_proxy arg args op = p_args output_dtype = p_args fake_sub_args = pytree tree_map_only torch fx Proxy lambda node meta example_value p_args This simplified implementation operator just tracing Actual implementation may also first promote arguments example_value = op fake_sub_args dtype=output_dtype Store invocation call wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function value args=tuple p_args kwargs= example_value=example_value StrictModeHigherOrderVariable TorchHigherOrderOperatorVariable raise_hard_error_if_graph_break reason= strict_mode HOO doesn t work unless captured completely torch compile _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker unpacked_sequence = args unpack_var_sequence tx TODO tmanlaibaatar support pytree here arg unpacked_sequence isinstance arg ListVariable TupleVariable ConstDictVariable unimplemented strict_mode HOO only works flat inputs now kwargs unimplemented f strict_mode HOO received unexpected kwargs list kwargs keys ret_val ret_spec ret_graph ret_lifted_freevars = speculate_subgraph tx args unpacked_sequence strict_mode source_target=self value should_flatten_outputs=True strict_mode_nn_modules = dict tx output nn_modules strict_mode_name = tx output install_subgraph strict_mode_body torch fx GraphModule strict_mode_nn_modules ret_graph strict_mode_node = make_attr tx strict_mode_name p_args = strict_mode_node tuple arg arg ret_lifted_freevars keys flat_example_value = pytree tree_map_only torch fx Proxy lambda node meta example_value ret_val as_proxy _call_function_and_unflatten_output tx torch ops higher_order strict_mode p_args flat_example_value ret_spec CheckpointHigherOrderVariable WrapHigherOrderVariable __init__ args kwargs - None super __init__ args kwargs If side effects allowed under checkpoint we should restore side effects after speculate subgraph restore_side_effects = torch _dynamo config skip_fwd_side_effects_in_bwd_under_checkpoint _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker torch _higher_order_ops wrap TagActivationCheckpoint torch utils checkpoint noop_context_fn context_fn = None context_fn kwargs kwargs context_fn noop_context_fn ctx = kwargs pop context_fn isinstance ctx torch _dynamo variables UserFunctionVariable context_fn = ctx fn isinstance ctx torch _dynamo variables functions FunctoolsPartialVariable context_fn = ctx guard_as_python_constant raise NotImplementedError f checkpoint implemented type ctx context_fn checkpoint_kwargs gmod_kwargs = TagActivationCheckpoint divide_kwargs kwargs Here we use checkpoint_kwargs gmod kwargs gmod_kwargs already flattened above managed inside fx graph p_args _ example_value _body_r out_spec checkpointed_gmod _ = create_wrapped_node tx args args gmod_kwargs torch utils checkpoint checkpoint under_activation_checkpoint=True context_fn None checkpointed_gmod meta _checkpoint_context_fn = context_fn _ checkpoint_kwargs = proxy_args_kwargs checkpoint_kwargs _call_function_and_unflatten_output tx value p_args checkpoint_kwargs example_value out_spec DynamoBypassingWrapperHigherOrderVariable WrapHigherOrderVariable __init__ hop source - None super __init__ hop source _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker func_var = args isinstance func_var torch _dynamo variables UserFunctionVariable func = func_var fn isinstance func_var torch _dynamo variables functions FunctoolsPartialVariable func = func_var as_python_constant raise RuntimeError f DynamoBypassingWrapperHigherOrderVariable Unsupported function type func_var p_args _ example_value _body_r out_spec gmod _ = create_wrapped_node tx args args kwargs str func Alternatively we could ve stored only function s fqn reconstructed requires function global gmod_meta_key = _dynamo_bypassing_wrapper_fn gmod meta gmod_meta_key = func _call_function_and_unflatten_output tx value gmod_meta_key + tuple p_args example_value out_spec ExportTracepointHigherOrderVariable TorchHigherOrderOperatorVariable call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker builder wrap_fx_proxy p_args = tuple arg as_proxy arg args p_kwargs = key arg as_proxy key arg kwargs items wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function value args=p_args kwargs=p_kwargs example_value=None RunWithRNGStateHigherOrderVariable TorchHigherOrderOperatorVariable _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker builder wrap_fx_proxy p_args = tuple arg as_proxy arg args p_kwargs = key arg as_proxy key arg kwargs items wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function value args=p_args kwargs=p_kwargs example_value=None AutoFunctionalizeHigherOrderVariable TorchHigherOrderOperatorVariable _call_function tx args list VariableTracker kwargs dict str VariableTracker - VariableTracker builder wrap_fx_proxy p_args = tuple arg as_proxy arg args p_kwargs = key arg as_proxy key arg kwargs items wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function value args=p_args kwargs=p_kwargs example_value=None FlexAttentionBackwardHighOrderVariable TorchHigherOrderOperatorVariable proxy_submod tx arg assert isinstance arg source base DictGetItemSource submod_name = tx output install_subgraph arg source base index arg value p_submod = make_attr tx submod_name set_example_value p_submod node arg value p_submod to_proxy tx arg isinstance arg UnspecializedNNModuleVariable proxy_submod tx arg isinstance arg ListVariable TupleVariable arg python_type to_proxy tx nested_arg nested_arg arg items arg as_proxy _call_function tx args list VariableTracker kwargs dict str VariableTracker - VariableTracker builder wrap_fx_proxy try p_args = tuple to_proxy tx arg arg args p_kwargs = key to_proxy tx arg key arg kwargs items except NotImplementedError Unsupported err raise Unsupported Missing Dynamo support FlexAttentionBackward HOP argument Please file issue err wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function value args=p_args kwargs=p_kwargs example_value=None TraceWrappedHigherOrderOperatorVariable TorchHigherOrderOperatorVariable Handles torch _dynamo _trace_wrapped_higher_order_op inner_trace unwrapping higher order op inlining through This op created dynamo survive through AotAutograd then unwrapped here call dynamo compiled autograd _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker kwargs = dict kwargs fn = kwargs pop fn fn call_function tx args kwargs FlexAttentionHigherOrderVariable TorchHigherOrderOperatorVariable staticmethod normalize_to_args args kwargs input signature query key value score_mod block_mask other_buffers block_mask tuple we don t want flatten only flatten kwargs into lists flat_kwargs = pytree tree_flatten kwargs Combine flattened lists all_args = args + flat_kwargs all_args create_wrapped_node tx InstructionTranslator query VariableTracker fn VariableTracker fn_name str _trace_wrapped_higher_order_op TransformGetItemToIndex create_scalar query call_method tx new_empty VariableTracker build tx dtype VariableTracker build tx torch int discard_graph_changes tx bhmn = create_scalar _ range fn_name == score_mod scores_require_grad bool = query requires_grad score = query call_method tx new_empty VariableTracker build tx requires_grad VariableTracker build tx scores_require_grad new_args = score bhmn assert fn_name == mask_fn Illegal function name + fn_name new_args = bhmn TransformGetItemToIndex _body_output _body_spec body_graph body_lifted_freevars = speculate_subgraph tx fn new_args expect only args no kwargs now description=fn_name source_target=self value set_subgraph_inputs= flatten_manual body_name = tx output install_subgraph fn_name torch fx GraphModule tx output nn_modules body_graph body_node = make_attr tx body_name It possible score-mod function captures some free variables passed arguments In case we need lift them which handled speculate_subgraph We then need create proxies + inputs lifted_args = tuple arg arg body_lifted_freevars keys proxy_args = body_node lifted_args proxy_args _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker builder wrap_fx_proxy query key value score_mod block_mask scale kernel_options = normalize_to_args args kwargs score_mod_node score_mod_lifted_args = create_wrapped_node tx query score_mod score_mod mask_fn = block_mask items - isinstance mask_fn ConstantVariable mask_fn = UserFunctionVariable torch nn attention _flex_attention _no_mask mask_fn_node mask_fn_lifted_args = create_wrapped_node tx query mask_fn mask_fn proxied_args = query key value TupleVariable block_mask items - source=block_mask source scale kernel_options Store invocation call Norm_kwargs contains score_function we dont want proxy because Proxying user defined functions supported inp_args _ = proxy_args_kwargs proxied_args Compose ordered HOO args - inp_args query key value block_mask scale kernel_options - subgraph node score_mod mask_fn_node - lifted args tracing subgraph score_mod_other_buffers mask_fn_other_buffers _ _ _ inp_arg_block_mask inp_arg_scale inp_arg_kernel_options = inp_args block_mask = tuple inp_arg_block_mask + mask_fn_node wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function value args=inp_args + score_mod_node block_mask inp_arg_scale inp_arg_kernel_options score_mod_lifted_args mask_fn_lifted_args kwargs= example_value=None AutogradFunctionApplyVariable VariableTracker __init__ fwd_graph bwd_graph parent_source kwargs - None super __init__ kwargs fwd_graph = fwd_graph bwd_graph = bwd_graph parent_source = parent_source call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker AutogradFunctionContextVariable UserDefinedClassVariable UserFunctionVariable UserMethodVariable builder wrap_fx_proxy Consider following MySin torch autograd Function staticmethod forward ctx x ctx save_for_backward x x sin staticmethod backward ctx grad x = ctx saved_tensors grad x cos We want resulting graphs look like fwd ctx x output saved tensors attrs x sin x bwd ctx grad grad gradn saved_tensors_or_attrs bwd ctx grad x grad x cos To accomplish we re going Construct ctx object fwd_out _ fwd_graph fwd_freevars = speculate_subgraph MySin forward manually_set_inputs=True bwd_out _ bwd_graph bwd_freevars = speculate_subgraph MySin backward while manually setting ctx grad inputs Manually rewriting fwd graph s output output stuff_that_gets_used bwd_graph Getting pretty elegant stuff_that_gets_used bwd graph just bwd_freevars returned speculate_subgraph assuming MySin backward doesn t capture any arguments All these steps work MySin backward doesn t capture any values This limitation general we should check prev_side_effects = tx output side_effects clone fwd_tracer = torch _dynamo output_graph SubgraphTracer tx output parent=tx output current_tracer source_target= autograd Function ctx = AutogradFunctionContextVariable create tx args kwargs discard_graph_changes tx A little hacky we need dummy ctx proxy speculate_subgraph We should clean up some point proxy = tx output create_proxy call_function torch autograd function FunctionCtx set_example_value proxy node ctx value ctx proxy = proxy isinstance fwd_graph types FunctionType fwd_fn = UserFunctionVariable fwd_graph fwd_args = ctx args isinstance fwd_graph types MethodType fwd_fn = UserMethodVariable fwd_graph __func__ UserDefinedClassVariable fwd_graph __class__ fwd_args = fwd_fn obj ctx args unimplemented non-function method Speculate subgraph fwd fwd_out _ fwd_graph fwd_freevars = speculate_subgraph tx fwd_fn fwd_args kwargs autograd Function enable_grad=False set_subgraph_inputs= semi_automatic restore_side_effects=False tracer=fwd_tracer ctx tx output side_effects store_attr_mutations _materialize_non_diff_grads tx output side_effects store_attr_mutations ctx unimplemented NYI bwd_tracer = torch _dynamo output_graph SubgraphTracer tx output parent=fwd_tracer source_target= autograd Function Speculate subgraph backward We make bwd tracer child fwd tracer because backward may rely tensors attrs created fwd tracer isinstance fwd_out variables BaseListVariable bwd_args = ctx fwd_out items bwd_args = ctx fwd_out bwd_src = AttrSource parent_source member= backward isinstance bwd_graph types FunctionType bwd_fn = UserFunctionVariable bwd_graph source=bwd_src isinstance bwd_graph types MethodType bwd_fn = UserMethodVariable bwd_graph __func__ UserDefinedClassVariable bwd_graph __class__ source=bwd_src bwd_args = bwd_fn obj bwd_args unimplemented non-function method is_strict_for v VariableTracker isinstance v variables TensorVariable we can more lax stuff forward v proxy tracer fwd_tracer True tx output subtracer fwd_fn fwd_tracer tx strict_translation_mode is_strict_for try bwd_out _ bwd_graph bwd_freevars = speculate_subgraph tx bwd_fn bwd_args kwargs autograd Function enable_grad=False set_subgraph_inputs= manual restore_side_effects=False tracer=bwd_tracer except torch _dynamo exc Unsupported e isinstance e torch _dynamo exc UnknownPropertiesDuringBackwardTrace unittest mock bwd_tracer = torch _dynamo output_graph SubgraphTracer tx output parent=fwd_tracer source_target= autograd Function _trace_wrapped_higher_order_op autograd_function_backward_rewritten isinstance bwd_graph types FunctionType bwd_fn = UserFunctionVariable autograd_function_backward_rewritten bwd_graph isinstance bwd_graph types MethodType bwd_fn = UserMethodVariable autograd_function_backward_rewritten bwd_graph __func__ UserDefinedClassVariable bwd_graph __class__ unimplemented non-function method mock patch torch _dynamo config _autograd_backward_strict_mode_conditional_banned_ops bwd_out _ bwd_graph bwd_freevars = speculate_subgraph tx bwd_fn bwd_args kwargs autograd Function enable_grad=False set_subgraph_inputs= manual restore_side_effects=False tracer=bwd_tracer raise e TODO assert bwd_graph didn t capture values created inside fwd_graph TODO oulgen Ideally we would do linear search output node things currently there could nodes after output node This bug prone there s code after output node then graph output will append output very end This might behavior difference If users call ctx mark_non_differentiable we should capture these output tensors who marked non-differentiable pass them ApplyTemplate torch _functorch autograd_function AutogradFunctionApply reconstruction non_differentiable_idx = ctx non_differentiable None non_differentiable_set = set ctx non_differentiable assert isinstance fwd_out variables BaseListVariable i x enumerate fwd_out items isinstance x variables TensorVariable x as_proxy non_differentiable_set non_differentiable_idx append i Rewrite output fwd_graph output stuff_necessary_for_bwd node fwd_graph find_nodes op= output fwd_graph erase_node node break Because we lift bwd_freevars inputs bwd_graph we have manually add bwd_freevars output fwd_graph However bwd_freevars got speculate_subgraph use Proxies bwd_graph we need convert them Proxies fwd_graph then generate new fwd_graph output fwd_proxy_of_bwd_freevars = k bwd_freevars keys k fwd_freevars fwd_proxy_of_bwd_freevars append fwd_freevars k fwd_proxy_of_bwd_freevars append k unwrap_proxy x isinstance x torch fx Proxy x node assert variables ConstantVariable is_literal x f Only constant allowed Got x x new_fwd_graph_outputs = fwd_out as_proxy fwd_proxy_of_bwd_freevars new_fwd_graph_outputs = pytree tree_map unwrap_proxy new_fwd_graph_outputs fwd_graph output new_fwd_graph_outputs fwd_graph lint Store fwd_body fwd_nn_modules = tx output tracing_context module_context copy_graphstate fwd_name = tx output install_subgraph fwd_body torch fx GraphModule fwd_nn_modules nn_modules fwd_graph fwd_node = make_attr tx fwd_name The type original args can arbitrary we only support basic type FX graph So speculated subgraph input includes original tensor args lifted freevars We need filter out original tensor args concat them lifted freevars generate proxy args FX call_function node filtered_args = A boolean list mark type corresponding argument tensor This used determine FX node s argument should argument ApplyTemplate forward we should skip output ApplyTemplate backward torch _functorch autograd_function AutogradFunctionApply args_tensor_mask = False len args i arg enumerate args isinstance arg variables TensorVariable variables SymNodeVariable filtered_args append arg args_tensor_mask i = True Rewrite output bwd_graph remove grad output non-Tensor args new_bwd_graph_outputs = None node bwd_graph find_nodes op= output bwd_graph erase_node node break The same above fwd proxies we need use bwd proxies bwd_graph some output fwd_freevars bwd_out_proxy = bwd_out as_proxy bwd_proxy_of_fwd_freevars = isinstance bwd_out_proxy tuple list k bwd_out_proxy k bwd_freevars bwd_proxy_of_fwd_freevars append bwd_freevars k bwd_proxy_of_fwd_freevars append k bwd_out_proxy bwd_freevars bwd_proxy_of_fwd_freevars = bwd_freevars bwd_out_proxy bwd_proxy_of_fwd_freevars = bwd_out_proxy Remove bwd output non-Tensor args output_proxy = bwd_proxy_of_fwd_freevars isinstance output_proxy tuple list new_bwd_graph_outputs = x mask zip output_proxy args_tensor_mask mask new_bwd_graph_outputs = new_bwd_graph_outputs + x assert x None f Grad non-Tensor arg x None new_bwd_graph_outputs = output_proxy Update bwd graph output new_bwd_graph_outputs = pytree tree_map lambda x None x None x node new_bwd_graph_outputs bwd_graph output new_bwd_graph_outputs bwd_graph lint Store bwd_body bwd_nn_modules = tx output tracing_context module_context copy_graphstate bwd_name = tx output install_subgraph bwd_body torch fx GraphModule bwd_nn_modules nn_modules bwd_graph bwd_node = make_attr tx bwd_name tx output side_effects = prev_side_effects p_args = fwd_node bwd_node arg as_proxy arg filtered_args + list fwd_freevars keys kwargs = args_tensor_mask args_tensor_mask non_differentiable_idx non_differentiable_idx Store invocation call torch _functorch autograd_function autograd_function_apply We use speculate_subgraph get fwd graph s always under no grad mode like what eager mode does The fwd outputs tensor s example_value need inferred fake tensor prop get correct attributes e g tensor requires_grad which would used downstream Dynamo tracing Since there can other ops like Triton kernels which depends python dispatcher we have enable enable_python_dispatcher tx output fake_mode fake_args = tx output nn_modules fwd_node node name tx output nn_modules bwd_node node name _get_fake_value arg arg filtered_args + list fwd_freevars keys example_value = autograd_function_apply fake_args kwargs wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function autograd_function_apply args=p_args kwargs=kwargs example_value=example_value _get_fake_value x isinstance x variables VariableTracker x as_proxy node meta example_value isinstance x torch fx Proxy x node meta example_value x maybe_positional_arg_names func result = hasattr func get_function None try fn = func get_function except Unsupported NotImplementedError None try sig = inspect signature fn except ValueError None name param sig parameters items param kind inspect Parameter VAR_POSITIONAL None param kind inspect Parameter POSITIONAL_ONLY param kind inspect Parameter POSITIONAL_OR_KEYWORD name == FX graphs can t have placeholder named result append self_ result append name result BaseHOPVariable WrapHigherOrderVariable supports_input_mutation = False supports_aliasing = False python_type type value _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker p_args p_kwargs example_value body_r treespec body_gmod body_name = create_wrapped_node tx args args value _name subgraph_name= subgraph assert len p_kwargs == flat_example_value = pytree tree_map_only torch fx Proxy lambda node meta example_value body_r as_proxy p_kwargs = key value as_proxy key value kwargs items _call_function_and_unflatten_output tx value p_args p_kwargs flat_example_value treespec InvokeSubgraphHigherOrderVariable WrapHigherOrderVariable supports_input_mutation = True supports_aliasing = False install_subgraph_in_output_graph tx fn_vt fn_args_vt kwargs body_gmod attr_name Check subgraph speculate_subgraph body_gmod fake inputs have already been seen before If yes subgraph already installed output graph we can just access subgraph using saved attr name isinstance fn_vt UnspecializedNNModuleVariable UserFunctionVariable unimplemented_v gb_type= Encountered non user function variable during invoke_subgraph HOP tracing context=str fn_vt explanation= invoke_subgraph does support non user function variable hints= graph_break_hints SUPPORTABLE invoke_subgraph_cache = tx output tracing_context hop_dispatch_set_cache get_cache torch _higher_order_ops invoke_subgraph isinstance fn_vt UserFunctionVariable fn_id = id fn_vt get_function fn_name = fn_vt get_function __name__ assert isinstance fn_vt UnspecializedNNModuleVariable fn_id = id fn_vt value forward __func__ fn_name = fn_vt value forward __name__ previously_installed_submodules = invoke_subgraph_cache previously_installed_submodules = invoke_subgraph_cache get_dynamo_installed_submodules fn_id current_mod = body_gmod NB - reverse more likely cause hit sooner because first graph can have requires_grad=False few inputs submodule_name reversed previously_installed_submodules assert submodule_name tx output nn_modules previous_mod = tx output nn_modules submodule_name are_same_graph_modules fn_name previous_mod current_mod tx fake_mode submodule_name body_name = super install_subgraph_in_output_graph tx fn_vt fn_args_vt kwargs body_gmod subgraph hc_log debug s Installing subgraph identifier s bringing total count s function s fn_name body_name fn_name len previously_installed_submodules + invoke_subgraph_cache invoke_subgraph_cache add_dynamo_installed_submodule fn_id body_name body_name raise_hard_error_if_graph_break reason= torch compile requires ` nested_compile_region ` decorated function capturable into single graph _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker This flattens kwargs into lifted args p_args p_kwargs example_value body_r treespec body_gmod body_name = create_wrapped_node tx args args kwargs invoke_subgraph len p_kwargs unimplemented kwargs should have been flattened into lifted args flat_example_value = pytree tree_map_only torch fx Proxy lambda node meta example_value body_r as_proxy p_args = p_args body_name p_args _call_function_and_unflatten_output tx torch _higher_order_ops invoke_subgraph tuple p_args p_kwargs flat_example_value treespec LocalMapWrappedHigherOrderVariable WrapHigherOrderVariable supports_input_mutation = False supports_aliasing = False Subclasses aren t supported speculate_subgraph yet So HOP only usable plain tensors _enabled = False classmethod contextlib contextmanager enable cls Context manager temporarily enable local map wrapping Will removed when speculate_subgraph supports subclass inputs https github com pytorch pytorch issues Usage LocalMapWrappedHigherOrderVariable enable_wrapping Code where should_wrap_in_hop will True pass old_value = cls _enabled cls _enabled = True try yield finally cls _enabled = old_value classmethod should_wrap_in_hop cls value torch distributed is_available False torch distributed tensor experimental _func_map _local_map_wrapped check important avoid subclass dispatch type value type _local_map_wrapped False value _local_map_wrapped cls _enabled staticmethod build options TorchHigherOrderOperatorVariable make torch _higher_order_ops local_map_hop options python_type type value _call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker Goal function rewrite local_map usage HOP local_map func - local_map_hop gm user_func out_placements in_placements in_grad_placements device_mesh redistribute_inputs user_args = args None placements used pass non-Tensors into local_map function Containers passed way can hold tensors Thus Dynamo would have inlined into them we handle None placements assuming they will desugared away This will need adjusted dynamic shapes support check_none_last placements seen_none = p placements p None seen_none += assert seen_none == Tracing local_map only currently supported None placements last seen_none inputs_none_placements = check_none_last in_placements value output_none_placements = check_none_last out_placements value local_map_kwargs = out_placements out_placements value in_placements in_placements value redistribute_inputs redistribute_inputs value in_grad_placements in_grad_placements value device_mesh device_mesh value assert local_map_kwargs device_mesh None Not yet implemented please manually provide device_mesh local_map mesh = local_map_kwargs device_mesh For Autoparallel initial trace done global shapes then we decide model weights sharding reuse graph Since sharding decision after initial trace we can t trace local shapes For local_map however since we specify all placements we can trace local shapes Step Validate annotated function matches input_placements i e can run eager template = Expecting expected inputs_or_outputs local_map function based placements found actual Please ensure count matches eager assert len in_placements value == len user_args template format expected=len in_placements value inputs_or_outputs= inputs actual=len user_args torch _higher_order_ops local_map redistribute_fw_inputs redistribute_fw_outputs Step Convert inputs local shapes priors = placements vt zip in_placements value user_args isinstance vt variables lazy LazyVariableTracker vt = variables lazy LazyVariableTracker realize_all vt isinstance vt variables TensorVariable assert placements None continue global_tensor = vt as_proxy node meta example_value NOTE We don t support local_map region relying exact grad_fn information This okay since accessing grad_fn graph break local_tensor = redistribute_fw_inputs global_tensor placements mesh local_tensor = local_tensor priors vt = global_tensor vt as_proxy node meta example_value = local_tensor vt synchronize_attributes tx Step Trace local_map subgraph local tensors p_args p_kwargs example_value body_r treespec body_gmod body_name = create_wrapped_node tx user_func user_args kwargs value _name subgraph_name= subgraph Step Validate traced graph signature still matches placement information expected_num_inputs = len in_placements value - inputs_none_placements actual_num_inputs = len body_gmod graph find_nodes op= placeholder expected_num_outputs = len out_placements value - output_none_placements assert len body_gmod graph find_nodes op= output == actual_num_outputs = len body_gmod graph find_nodes op= output args template = Expecting expected inputs_or_outputs local_map function based placements found actual If count matches eager Dynamo may have flattened inputs_or_outputs function found additional tensors used via closures Please adjust input placements match what traced graph sees \n gm_str make_error_msg args expected_num actual_num inputs_or_outputs = args gm_str = body_gmod print_readable print_output=False template format expected=expected_num inputs_or_outputs=inputs_or_outputs actual=actual_num gm_str=gm_str expected_num_inputs = actual_num_inputs raise AssertionError make_error_msg expected_num_inputs actual_num_inputs inputs expected_num_outputs = actual_num_outputs raise AssertionError make_error_msg expected_num_outputs actual_num_outputs outputs inputs_none_placements expected_input_nodes = arg as_proxy node arg user_args -inputs_none_placements expected_input_nodes = arg as_proxy node arg user_args actual_input_nodes = proxy node proxy p_args assert actual_input_nodes op == get_attr assert subgraph actual_input_nodes target assert len expected_input_nodes == len actual_input_nodes - expected_order actual_order zip expected_input_nodes actual_input_nodes assert expected_order == actual_order Dynamo changed order inputs local_map function please adjust f order inputs input_placements expected_input_nodes actual_input_nodes assert len p_kwargs == flat_example_value = pytree tree_map_only torch fx Proxy lambda node meta example_value body_r as_proxy Step Install local_map subgraph p_kwargs = key value as_proxy key value kwargs items out = _call_function_and_unflatten_output tx value p_args p_kwargs flat_example_value treespec Step Restore inputs outputs global shapes vt global_tensor priors items vt as_proxy node meta example_value = global_tensor vt synchronize_attributes tx outs = out items isinstance out TupleVariable out assert len outs == len out_placements value placements vt zip out_placements value outs isinstance vt variables TensorVariable assert placements None continue local_tensor = vt as_proxy node meta example_value NOTE We don t support code after local_map region relying exact grad_fn information This okay since accessing grad_fn graph break global_tensor = redistribute_fw_outputs local_tensor placements mesh num_activations= joint global_tensor = global_tensor vt as_proxy node meta example_value = global_tensor vt synchronize_attributes tx TODO Figure out how handle output order diverging eager Treat const so we don t have deal Placement types fx IR Guarded EQUALS_MATCH local_map call s arguments body_gmod meta local_map_kwargs = out_placements out_placements value expected_num_outputs in_placements in_placements value expected_num_inputs redistribute_inputs redistribute_inputs value in_grad_placements in_grad_placements value device_mesh device_mesh value out Map operator names their corresponding variable fast TorchHigherOrderOperatorVariable make _hop_name_to_variable_class = cond CondHigherOrderVariable while_loop WhileLoopHigherOrderVariable while_loop_stack_output WhileLoopStackOutputHigherOrderVariable map_impl MapHigherOrderVariable executorch_call_delegate ExecutorchCallDelegateHigherOrderVariable out_dtype OutDtypeHigherOrderVariable wrap WrapHigherOrderVariable hints_wrapper HintsWrapperHigherOrderVariable flex_attention FlexAttentionHigherOrderVariable flex_attention_backward FlexAttentionBackwardHighOrderVariable wrap_activation_checkpoint CheckpointHigherOrderVariable tag_activation_checkpoint CheckpointHigherOrderVariable _export_tracepoint ExportTracepointHigherOrderVariable trace_wrapped TraceWrappedHigherOrderOperatorVariable strict_mode StrictModeHigherOrderVariable run_with_rng_state RunWithRNGStateHigherOrderVariable associative_scan AssociativeScanHigherOrderVariable scan ScanHigherOrderVariable call_torchbind CallTorchbindHigherOrderVariable wrap_with_set_grad_enabled WrapWithSetGradEnabledHigherOrderVariable wrap_with_autocast WrapWithAutocastHigherOrderVariable dynamo_bypassing_wrapper DynamoBypassingWrapperHigherOrderVariable auto_functionalized AutoFunctionalizeHigherOrderVariable auto_functionalized_v AutoFunctionalizeHigherOrderVariable invoke_subgraph InvokeSubgraphHigherOrderVariable custom_function_call CustomFunctionHigherOrderOperatorVariable local_map_hop LocalMapWrappedHigherOrderVariable