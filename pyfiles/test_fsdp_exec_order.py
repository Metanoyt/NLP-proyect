Owner s oncall distributed sys warnings contextlib nullcontext torch torch distributed dist torch distributed fsdp FullyShardedDataParallel FSDP torch distributed fsdp fully_sharded_data_parallel ShardingStrategy torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest get_devtype torch testing _internal common_utils parametrize run_tests TEST_WITH_DEV_DBG_ASAN device_type = torch device get_devtype dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit Model torch nn Module Model supports two computation paths ` layer ` - ` layer ` ` layer ` - ` layer ` Notably both ` layer ` ` layer ` have elements when flattened which means their corresponding all-gathers reduce-scatters may silently matched we do perform any checks __init__ - None super __init__ layer = torch nn Linear layer = torch nn Linear bias=False layer = torch nn Sequential torch nn Linear bias=False torch nn ReLU torch nn Linear bias=False relu = torch nn ReLU use_alt_path = False param layer parameters param requires_grad = False forward x ` layer ` - ` layer ` normal ` layer ` - ` layer ` alternate z = relu layer x z = relu layer z use_alt_path relu layer z z get_input device torch randn device get_loss input output output sum run_backward loss loss backward flip_path params_to_freeze = layer parameters use_alt_path layer parameters params_to_unfreeze = layer parameters use_alt_path layer parameters param params_to_freeze param requires_grad = False param params_to_unfreeze param requires_grad = True use_alt_path = use_alt_path staticmethod wrap sharding_strategy ShardingStrategy device model = Model model layer = FSDP model layer sharding_strategy=sharding_strategy device_id=device model layer = FSDP model layer sharding_strategy=sharding_strategy device_id=device fsdp_model = FSDP model sharding_strategy=sharding_strategy device_id=device fsdp_model device TestFSDPExecOrder FSDPTest skip_if_lt_x_gpu parametrize sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP test_invalid_first_iter_order device sharding_strategy ShardingStrategy Tests FSDP errors all-gather order differs across ranks first iteration Rank runs forward pass one order all other ranks run different order dist set_debug_level dist DebugLevel DETAIL fsdp_model = Model wrap sharding_strategy device_type rank = fsdp_model flip_path inp = fsdp_model module get_input device_type Match error message following prefix error_regex = ^ Forward order differs across ranks assertRaisesRegex RuntimeError error_regex fsdp_model inp skip_if_lt_x_gpu parametrize sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP parametrize iters_before_path_change test_invalid_later_iter_order device sharding_strategy ShardingStrategy iters_before_path_change int Tests FSDP warns user all-gather order changes after first iteration dist set_debug_level dist DebugLevel DETAIL On first iteration all ranks run same order next iteration all rank run different order fsdp_model = Model wrap sharding_strategy device_type _ range iters_before_path_change inp = fsdp_model module get_input device_type output = fsdp_model inp loss = fsdp_model module get_loss inp output device_type fsdp_model module run_backward loss Match warning message following prefix regex = ^ Forward order differs first iteration f rank rank Collectives unchecked may give incorrect results hang context = assertWarnsRegex expected_warning=UserWarning expected_regex=regex rank = nullcontext rank = fsdp_model flip_path inp = fsdp_model module get_input device_type Expect warning forward pass all-gather context warning forward pass all-gather output = fsdp_model inp loss = fsdp_model module get_loss inp output device_type fsdp_model module run_backward loss Run additional iteration check there no more warnings inp = fsdp_model module get_input device_type output = fsdp_model inp loss = fsdp_model module get_loss inp output device_type fsdp_model module run_backward loss skip_if_lt_x_gpu parametrize sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP test_train_eval device sharding_strategy ShardingStrategy dist set_debug_level dist DebugLevel DETAIL fsdp_model = Model wrap sharding_strategy device_type NUM_ITERS = NUM_EPOCHS = warnings catch_warnings record=True w records warnings ` w ` _ range NUM_EPOCHS fsdp_model train _ range NUM_ITERS inp = fsdp_model module get_input device_type output = fsdp_model inp loss = fsdp_model module get_loss inp output device_type fsdp_model module run_backward loss fsdp_model eval _ range NUM_ITERS inp = fsdp_model module get_input device_type output = fsdp_model inp fsdp_model module get_loss inp output device_type Check order validation warning issued errors do need checked since they will directly reported warning_prefix = Forward order differs warning w str warning message startswith warning_prefix raise AssertionError f Warning incorrectly issued warning message If we still validate forward execution order eval mode then ` AssertionError ` will raised above both sharding strategies devices = cuda hpu xpu instantiate_device_type_tests TestFSDPExecOrder globals only_for=devices allow_xpu=True __name__ == __main__ run_tests