Owner s module sdpa unittest collections namedtuple torch torch nn nn torch nn functional F torch nn attention varlen varlen_attn torch testing _internal common_cuda PLATFORM_SUPPORTS_FLASH_ATTENTION torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_nn NNTestCase torch testing _internal common_utils parametrize run_tests skipIfRocm torch utils _python_dispatch TorchDispatchMode VarlenShape = namedtuple VarlenShape batch_size max_seq_len embed_dim num_heads OpLoggingMode TorchDispatchMode Logging mode captures all dispatched operations __init__ called_ops = __torch_dispatch__ func types args= kwargs=None op_name = str func called_ops append op_name func args kwargs AttentionBlock nn Module __init__ embed_dim int num_heads int device torch device dtype torch dtype super __init__ embed_dim = embed_dim num_heads = num_heads head_dim = embed_dim num_heads qkv_proj = nn Linear embed_dim embed_dim bias=False device=device dtype=dtype out_proj = nn Linear embed_dim embed_dim bias=False device=device dtype=dtype get_varlen_qkv x_packed torch Tensor qkv = qkv_proj x_packed q k v = qkv chunk dim=- q = q view - num_heads head_dim k = k view - num_heads head_dim v = v view - num_heads head_dim q k v forward_varlen x_packed torch Tensor cu_seq torch Tensor max_len int is_causal bool = False q k v = get_varlen_qkv x_packed attn_out = varlen_attn q k v cu_seq cu_seq max_len max_len is_causal attn_out = attn_out view - embed_dim out_proj attn_out forward_sdpa x_padded torch Tensor seq_lengths torch Tensor is_causal bool = False batch_size seq_len _ = x_padded shape qkv = qkv_proj x_padded q k v = qkv chunk dim=- mask = torch arange seq_len device=x_padded device None seq_lengths None attn_mask = mask None None expand batch_size num_heads seq_len seq_len q = q view batch_size seq_len num_heads head_dim transpose k = k view batch_size seq_len num_heads head_dim transpose v = v view batch_size seq_len num_heads head_dim transpose attn_out = F scaled_dot_product_attention q k v attn_mask=attn_mask is_causal=is_causal attn_out = attn_out transpose contiguous view batch_size seq_len embed_dim out_proj attn_out create_variable_length_batch shape VarlenShape device torch device dtype torch dtype seq_lengths = _ range shape batch_size length = torch randint shape max_seq_len + item seq_lengths append min length shape max_seq_len seq_lengths = torch tensor seq_lengths device=device total_tokens = seq_lengths sum item x_packed = torch randn total_tokens shape embed_dim device=device dtype=dtype requires_grad=True cu_seq = torch zeros shape batch_size + device=device dtype=torch int cu_seq = seq_lengths cumsum max_len = seq_lengths max item x_padded = torch zeros shape batch_size max_len shape embed_dim device=device dtype=dtype start_idx = i seq_len enumerate seq_lengths end_idx = start_idx + seq_len x_padded i seq_len = x_packed start_idx end_idx start_idx = end_idx x_padded = x_padded clone detach requires_grad_ seq_lengths seq_lengths cu_seq cu_seq x_packed x_packed x_padded x_padded max_len max_len total_tokens total_tokens TestVarlenAttention NNTestCase skipIfRocm msg= ROCM does support variable length attention unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Flash Attention supported parametrize dtype torch bfloat torch float test_basic_functionality device dtype torch manual_seed shape = VarlenShape batch_size= max_seq_len= embed_dim= num_heads= attention_block = AttentionBlock shape embed_dim shape num_heads device dtype total_tokens = shape batch_size shape max_seq_len x_packed = torch randn total_tokens shape embed_dim device=device dtype=dtype requires_grad=True cu_seq = torch tensor shape max_seq_len total_tokens device=device dtype=torch int output = attention_block forward_varlen x_packed cu_seq shape max_seq_len is_causal=False assertEqual output shape total_tokens shape embed_dim assertEqual output device torch device device assertEqual output dtype dtype varlen_grad_out = torch ones_like output varlen_grad = torch autograd grad outputs=output inputs=x_packed grad_outputs=varlen_grad_out retain_graph=True create_graph=False allow_unused=False assertIsNotNone varlen_grad assertEqual varlen_grad shape x_packed shape assertEqual varlen_grad dtype x_packed dtype skipIfRocm msg= ROCM does support variable length attention unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Flash Attention supported parametrize dtype torch bfloat torch float test_custom_op_compliance device dtype torch manual_seed shape = VarlenShape batch_size= max_seq_len= embed_dim= num_heads= attention_block = AttentionBlock shape embed_dim shape num_heads device dtype total_tokens = shape batch_size shape max_seq_len x_packed = torch randn total_tokens shape embed_dim device=device dtype=dtype cu_seq = torch tensor shape max_seq_len total_tokens device=device dtype=torch int q k v = attention_block get_varlen_qkv x_packed torch library opcheck torch ops torch_attn _varlen_attn q k v cu_seq cu_seq shape max_seq_len shape max_seq_len False out lse rng_state = torch ops torch_attn _varlen_attn q k v cu_seq cu_seq shape max_seq_len shape max_seq_len False grad_out = torch randn_like out we don t support double backward skipping test_autograd_registration test_aot_dispatch_dynamic test_aot_dispatch_static torch library opcheck torch ops torch_attn _varlen_attn_backward grad_out q k v out lse cu_seq cu_seq shape max_seq_len shape max_seq_len False rng_state test_utils= test_schema test_faketensor skipIfRocm msg= ROCM does support variable length attention unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Flash Attention supported parametrize dtype torch bfloat torch float test_custom_op_registration device dtype torch manual_seed shape = VarlenShape batch_size= max_seq_len= embed_dim= num_heads= attention_block = AttentionBlock shape embed_dim shape num_heads device dtype total_tokens = shape batch_size shape max_seq_len x_packed = torch randn total_tokens shape embed_dim device=device dtype=dtype requires_grad=True cu_seq = torch tensor shape max_seq_len total_tokens device=device dtype=torch int compiled_forward = torch compile attention_block forward_varlen backend= eager fullgraph=True OpLoggingMode mode output = compiled_forward x_packed cu_seq shape max_seq_len is_causal=False varlen_grad_out = torch ones_like output _ = torch autograd grad outputs=output inputs=x_packed grad_outputs=varlen_grad_out retain_graph=True create_graph=False allow_unused=False called_ops = mode called_ops custom_ops_called = any torch_attn _varlen_attn op op called_ops any torch_attn _varlen_attn_backward op op called_ops assert custom_ops_called skipIfRocm msg= ROCM does support variable length attention unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Flash Attention supported parametrize dtype torch bfloat torch float parametrize is_causal False True test_varlen_vs_sdpa device dtype is_causal torch manual_seed shape = VarlenShape batch_size= max_seq_len= embed_dim= num_heads= attention_block = AttentionBlock shape embed_dim shape num_heads device dtype golden_attention_block = AttentionBlock shape embed_dim shape num_heads device torch float variable_length_batch_data = create_variable_length_batch shape device dtype golden_variable_length_batch_data = create_variable_length_batch shape device torch float varlen_output = attention_block forward_varlen variable_length_batch_data x_packed variable_length_batch_data cu_seq variable_length_batch_data max_len is_causal=is_causal sdpa_output = attention_block forward_sdpa variable_length_batch_data x_padded variable_length_batch_data seq_lengths is_causal=is_causal golden_sdpa_output = golden_attention_block forward_sdpa golden_variable_length_batch_data x_padded golden_variable_length_batch_data seq_lengths is_causal=is_causal start_idx = i seq_len enumerate variable_length_batch_data seq_lengths end_idx = start_idx + seq_len varlen_seq = varlen_output start_idx end_idx sdpa_seq = sdpa_output i seq_len golden_sdpa_seq = golden_sdpa_output i seq_len fwd_atol = golden_sdpa_seq + - - golden_sdpa_seq abs max item varlen_error = varlen_seq - fwd_atol abs max item sdpa_error = sdpa_seq - fwd_atol abs max item assert varlen_error = sdpa_error + fwd_atol start_idx = end_idx varlen_grad_out = torch ones_like varlen_output sdpa_grad_out = torch ones_like sdpa_output golden_sdpa_grad_out = torch ones_like golden_sdpa_output start_idx = i seq_len enumerate variable_length_batch_data seq_lengths end_idx = start_idx + seq_len sdpa_grad_out i seq_len = varlen_grad_out start_idx end_idx start_idx = end_idx varlen_grad = torch autograd grad outputs=varlen_output inputs=variable_length_batch_data x_packed grad_outputs=varlen_grad_out retain_graph=True create_graph=False allow_unused=False sdpa_grad = torch autograd grad outputs=sdpa_output inputs=variable_length_batch_data x_padded grad_outputs=sdpa_grad_out retain_graph=True create_graph=False allow_unused=False golden_sdpa_grad = torch autograd grad outputs=golden_sdpa_output inputs=golden_variable_length_batch_data x_padded grad_outputs=golden_sdpa_grad_out retain_graph=True create_graph=False allow_unused=False start_idx = i seq_len enumerate variable_length_batch_data seq_lengths end_idx = start_idx + seq_len varlen_grad_seq = varlen_grad start_idx end_idx sdpa_grad_seq = sdpa_grad i seq_len golden_sdpa_seq = golden_sdpa_grad i seq_len fwd_atol = golden_sdpa_seq + - - golden_sdpa_seq abs max item varlen_error = varlen_grad_seq - fwd_atol abs max item sdpa_error = sdpa_grad_seq - fwd_atol abs max item assert varlen_error = sdpa_error + fwd_atol start_idx = end_idx device_types = cuda instantiate_device_type_tests TestVarlenAttention globals only_for=device_types __name__ == __main__ run_tests