Owner s oncall distributed contextlib functools itertools sys unittest collections abc Callable typing Any Optional unittest mock torch torch distributed dist torch nn nn torch distributed fsdp CPUOffload MixedPrecision torch distributed fsdp _flat_param FlatParamHandle torch distributed fsdp fully_sharded_data_parallel BackwardPrefetch FullyShardedDataParallel FSDP ShardingStrategy torch distributed fsdp wrap ModuleWrapPolicy torch distributed utils _p_assert torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp AlwaysWrapNestedWrappedModule DEVICEInitMode DummyDDP FSDPInitMode FSDPTest get_devtype MixtureOfExperts NestedWrappedModule NestedWrappedModuleWithDelay subtest_name TransformerWithSharedParams torch testing _internal common_utils parametrize run_tests TEST_HPU TEST_WITH_DEV_DBG_ASAN dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit device_type = torch device get_devtype params = cpu_offload sharding_strategy cpu_offload_config = CPUOffload offload_params=True CPUOffload offload_params=False sharding_strategy_config = None ShardingStrategy SHARD_GRAD_OP ShardingStrategy NO_SHARD configs = list itertools product cpu_offload_config sharding_strategy_config test_name_mapping = str CPUOffload offload_params=True offload_true str CPUOffload offload_params=False offload_false str ShardingStrategy SHARD_GRAD_OP shard_grad_op str ShardingStrategy NO_SHARD no_shard subtest_name = functools partial subtest_name test_name_mapping TestParityWithDDP FSDPTest Compare losses parameter values after several updates when using PyTorch DDP vs FullyShardedDataParallel _get_device_init_modes cpu_offload CPUOffload - list DEVICEInitMode modes = DEVICEInitMode DEVICE_AFTER DEVICEInitMode DEVICE_BEFORE Note DEVICEInitMode DEVICE_NEVER works currently only CPU offload we explicitly bring param back CUDA device In general will work since we try all_gather p data which CPU NCCL only supports GPU cpu_offload offload_params modes append DEVICEInitMode DEVICE_NEVER modes _get_subtest_config cpu_offload CPUOffload - dict str list Any Returns subtest configuration subtests CUDA initialization modes prefetching settings together device_init_mode _get_device_init_modes cpu_offload backward_prefetch None BackwardPrefetch BACKWARD_PRE BackwardPrefetch BACKWARD_POST forward_prefetch False True use_orig_params False True skip_if_lt_x_gpu parametrize params configs subtest_name test_nested_wrapped_model cpu_offload CPUOffload sharding_strategy Optional ShardingStrategy run_subtests _get_subtest_config cpu_offload _test_fsdp_parity NestedWrappedModule FSDPInitMode RECURSIVE cpu_offload=cpu_offload sharding_strategy=sharding_strategy skip_if_lt_x_gpu parametrize params configs subtest_name test_nested_wrapped_model_single_iteration_mixed_precision cpu_offload CPUOffload sharding_strategy Optional ShardingStrategy mixed_precision = MixedPrecision param_dtype=torch float buffer_dtype=torch float reduce_dtype=torch float run_subtests _get_subtest_config cpu_offload _test_fsdp_parity NestedWrappedModule FSDPInitMode RECURSIVE cpu_offload=cpu_offload sharding_strategy=sharding_strategy num_iters= mixed_precision=mixed_precision skip_if_lt_x_gpu parametrize params configs subtest_name test_nested_always_wrap_model cpu_offload CPUOffload sharding_strategy Optional ShardingStrategy run_subtests _get_subtest_config cpu_offload _test_fsdp_parity AlwaysWrapNestedWrappedModule FSDPInitMode RECURSIVE cpu_offload=cpu_offload sharding_strategy=sharding_strategy skip_if_lt_x_gpu parametrize params configs subtest_name test_transformer cpu_offload CPUOffload sharding_strategy Optional ShardingStrategy run_subtests _get_subtest_config cpu_offload _test_fsdp_parity TransformerWithSharedParams FSDPInitMode RECURSIVE cpu_offload=cpu_offload sharding_strategy=sharding_strategy skip_if_lt_x_gpu parametrize params configs subtest_name test_delayed_optim_step cpu_offload CPUOffload sharding_strategy Optional ShardingStrategy Tests FSDP forward backward optimizer step runtime using model long CUDA delay after loss computation before optimizer step exercise internal CUDA stream usage forward pass all-gathers do start until after optimizer step completes run_subtests _get_subtest_config cpu_offload _test_fsdp_parity NestedWrappedModuleWithDelay FSDPInitMode RECURSIVE cpu_offload=cpu_offload sharding_strategy=sharding_strategy init_kwargs= delay_after_loss_ms skip_if_lt_x_gpu parametrize params configs subtest_name test_delayed_reduce_scatter cpu_offload CPUOffload sharding_strategy Optional ShardingStrategy Tests FSDP forward backward optimizer step runtime using model long CUDA delay before gradient reduce-scatter exercise internal CUDA stream usage backward pass waits those reductions finish run_subtests _get_subtest_config cpu_offload _test_fsdp_parity NestedWrappedModuleWithDelay FSDPInitMode RECURSIVE cpu_offload=cpu_offload sharding_strategy=sharding_strategy init_kwargs= delay_before_reduction_ms _dummy_ddp_fn model ` MixtureOfExperts ` ` implements custom gradient reduction logic so reference behavior should follow logic instead DDP DummyDDP model skip_if_lt_x_gpu parametrize params configs subtest_name test_mixture_of_experts cpu_offload CPUOffload sharding_strategy Optional ShardingStrategy fsdp_kwargs = device_id device_type type run_subtests _get_subtest_config cpu_offload _test_fsdp_parity MixtureOfExperts FSDPInitMode RECURSIVE ref_init_fn=self _dummy_ddp_fn cpu_offload=cpu_offload sharding_strategy=sharding_strategy fsdp_kwargs unittest skipIf TEST_HPU HPU doesn t has HW sleep API support like CUDA skipping skip_if_lt_x_gpu parametrize params configs subtest_name test_mixture_of_experts_with_delay_before_free cpu_offload CPUOffload sharding_strategy Optional ShardingStrategy fsdp_kwargs = device_id device_type type run_subtests _get_subtest_config cpu_offload _test_fsdp_parity MixtureOfExperts FSDPInitMode RECURSIVE ref_init_fn=self _dummy_ddp_fn cpu_offload=cpu_offload sharding_strategy=sharding_strategy init_kwargs= delay_before_free_ms fsdp_kwargs TestParamInit FSDPTest skip_if_lt_x_gpu parametrize mixed_precision True False test_param_change_after_init mixed_precision Tests changing FSDP model parameter values in-place after FSDP initialization persist Establish reference behavior fsdp_kwargs = device_id device_type mixed_precision fsdp_kwargs mixed_precision = MixedPrecision fsdp_model = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_AFTER fsdp_kwargs deterministic=True input = fsdp_model module get_input device_type ref_output = fsdp_model input Initialize same model change its first parameter value in-place after FSDP initialization new_fsdp_model = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_AFTER fsdp_kwargs deterministic=True first_param = next new_fsdp_model parameters nn init normal_ first_param data new_output = new_fsdp_model input assertNotEqual ref_output new_output msg= new_output did reflect change param after init TestHooks FSDPTest skip_if_lt_x_gpu parametrize cuda_first False True test_pre_backward_hook_registration cuda_first bool Tests FSDP pre-backward hooks registered forward pass outputs fsdp_kwargs = device_id device_type type fsdp_model = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE cuda_first DEVICEInitMode DEVICE_AFTER fsdp_kwargs _test_pre_backward_hook_registration fsdp_model skip_if_lt_x_gpu test_pre_backward_hook_registration_after_state_dict Tests FSDP pre-backward hooks registered forward pass outputs after saving loading model checkpoint fsdp_kwargs = device_id device_type type fsdp_model = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_AFTER fsdp_kwargs _train_for_several_steps fsdp_model num_steps= autocast=False state_dict = fsdp_model state_dict fsdp_model load_state_dict state_dict _test_pre_backward_hook_registration fsdp_model _test_pre_backward_hook_registration model optim = torch optim SGD model parameters lr= momentum= optim zero_grad Inputs always cuda computation happens CUDA device only input = model module get_input device_type output = model input pre-bwd hook assertEqual len output _backward_hooks loss = model module get_loss input output device_type type loss backward It doesn t get removed assertEqual len output _backward_hooks optim step assertEqual len output _backward_hooks skip_if_lt_x_gpu parametrize cuda_first False True parametrize mixed_precision True False test_register_functions_called cuda_first bool mixed_precision bool Tests ` ` _register_ pre &#124; post _backward_hooks ` ` called during FSDP forward fsdp_kwargs = device_id device_type type mixed_precision fsdp_kwargs mixed_precision = MixedPrecision fsdp_model = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE cuda_first DEVICEInitMode DEVICE_AFTER fsdp_kwargs input = fsdp_model module get_input device_type Since ` _register_pre_backward_hooks ` modifies forward output we cannot directly mock We implement our own counter instead orig_register_pre_backward_hooks = torch distributed fsdp _runtime_utils _register_pre_backward_hooks register_pre_backward_hooks_call_count = _register_pre_backward_hooks_with_count args kwargs nonlocal register_pre_backward_hooks_call_count register_pre_backward_hooks_call_count += orig_register_pre_backward_hooks args kwargs mock patch torch distributed fsdp _runtime_utils _register_pre_backward_hooks _register_pre_backward_hooks_with_count mock patch torch distributed fsdp _runtime_utils _register_post_backward_hook register_post_bwd_mock assertEqual register_pre_backward_hooks_call_count assertFalse register_post_bwd_mock called fsdp_model input assertTrue register_pre_backward_hooks_call_count assertTrue register_post_bwd_mock called TestNoGrad FSDPTest skip_if_lt_x_gpu parametrize mixed_precision True False test_transformer_no_grad mixed_precision Tests FSDP-wrapped transformer model shared parameters after training one iteration running forward pass ` ` eval ` ` mode gives same output running forward pass ` ` torch no_grad ` ` fsdp_kwargs = device_id device_type type mixed_precision fsdp_kwargs mixed_precision = MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float fsdp_kwargs mixed_precision = None fsdp_model = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_AFTER fsdp_kwargs _train_for_several_steps fsdp_model num_steps= autocast=False mixed_precision=fsdp_kwargs mixed_precision input = fsdp_model module get_input device_type Run forward eval mode fsdp_model eval ref_output = fsdp_model input Run forward ` no_grad ` compare torch no_grad no_grad_output = fsdp_model input assertEqual ref_output no_grad_output TestAutograd FSDPTest skip_if_lt_x_gpu test_unshard_params_as_tensors Tests FSDP always unshards logical parameters ` ` Tensor ` ` views during forward backward computation even when forward backward prefetching run_subtests sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP Skip testing ` NO_SHARD ` since doubly uses ` _use_unsharded_views ` sharded views Testing ` FULL_SHARD ` ` SHARD_GRAD_OP ` provides good confidence ` as_params ` logic correct use_orig_params False True forward_prefetch False True backward_prefetch BackwardPrefetch BACKWARD_PRE BackwardPrefetch BACKWARD_POST None _test_unshard_params_as_tensors _test_unshard_params_as_tensors sharding_strategy ShardingStrategy use_orig_params bool forward_prefetch bool backward_prefetch Optional BackwardPrefetch orig_use_unsharded_views = FlatParamHandle _use_unsharded_views _use_unsharded_views_assert_as_tensors FlatParamHandle as_params bool - None _p_assert as_params Expects use Tensor views using parameter views orig_use_unsharded_views as_params fsdp_kwargs = sharding_strategy sharding_strategy use_orig_params use_orig_params forward_prefetch forward_prefetch backward_prefetch backward_prefetch auto_wrap_policy ModuleWrapPolicy nn Linear device_id device_type Define model enough FSDP instances exercise prefetching NUM_LINEARS = model = nn Sequential nn Linear device=device_type _ range NUM_LINEARS fsdp_model = FSDP model fsdp_kwargs assertEqual len list FSDP fsdp_modules fsdp_model NUM_LINEARS + _ range inp = torch randn device=device_type _patch_use_unsharded_views _use_unsharded_views_assert_as_tensors loss = fsdp_model inp sum loss backward contextlib contextmanager _patch_use_unsharded_views new_use_unsharded_views Callable orig_use_unsharded_views = FlatParamHandle _use_unsharded_views FlatParamHandle _use_unsharded_views = new_use_unsharded_views try yield finally FlatParamHandle _use_unsharded_views = orig_use_unsharded_views devices = cuda hpu xpu instantiate_device_type_tests TestHooks globals only_for=devices allow_xpu=True instantiate_device_type_tests TestParityWithDDP globals only_for=devices allow_xpu=True instantiate_device_type_tests TestNoGrad globals only_for=devices allow_xpu=True instantiate_device_type_tests TestParamInit globals only_for=devices allow_xpu=True instantiate_device_type_tests TestAutograd globals only_for=devices allow_xpu=True __name__ == __main__ run_tests