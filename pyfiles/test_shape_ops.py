Owner s module tests random unittest warnings functools partial itertools chain combinations permutations product numpy np torch torch nan torch testing make_tensor torch testing _internal common_device_type dtypes dtypesIfCUDA dtypesIfXPU instantiate_device_type_tests largeTensorTest onlyCPU onlyNativeDeviceTypes onlyOn torch testing _internal common_dtype all_types all_types_and all_types_and_complex_and torch testing _internal common_utils IS_JETSON run_tests skipIfTorchDynamo TEST_PRIVATEUSE _DEVICE_TYPE TestCase torch_to_numpy_dtype_dict TODO replace make_tensor _generate_input shape dtype device with_extremal shape == x = torch tensor dtype=dtype device=device dtype is_floating_point dtype is_complex work around torch randn being implemented bfloat dtype == torch bfloat x = torch randn shape device=device random randint x = x torch bfloat x = torch randn shape dtype=dtype device=device random randint x torch randn shape = with_extremal dtype is_floating_point Use extremal values x torch randn shape = float nan x torch randn shape = float inf x torch randn shape = float -inf with_extremal dtype is_complex x torch randn shape = complex nan x torch randn shape = complex inf x torch randn shape = complex -inf dtype == torch bool x = torch zeros shape dtype=dtype device=device x torch randn shape = True x = torch randint shape dtype=dtype device=device x TestShapeOps TestCase TODO update work CUDA too onlyCPU test_unbind device x = torch rand dim range res = torch unbind x dim res = x unbind dim assertEqual x size dim len res assertEqual x size dim len res i range dim assertEqual x select dim i res i assertEqual x select dim i res i TODO update work CUDA too skipIfTorchDynamo TorchDynamo fails unknown error onlyCPU test_tolist device list D = tensor D = torch tensor list D assertEqual tensor D tolist list D table D = tensor D = torch tensor table D storage = torch Storage table D assertEqual tensor D tolist table D assertEqual storage tolist table D assertEqual tensor D tolist table D assertEqual storage tolist table D table D = tensor D = torch tensor table D assertEqual tensor D tolist table D tensor D = torch tensor tensorNonContig = tensor D select assertFalse tensorNonContig is_contiguous assertEqual tensorNonContig tolist dtypes torch int torch float torch complex test_movedim_invalid device dtype shape = _rand_shape min_size= max_size= x = _generate_input shape dtype device False fn torch movedim torch moveaxis Invalid ` source ` ` destination ` dimension assertRaisesRegex IndexError Dimension out range fn x assertRaisesRegex IndexError Dimension out range fn x Mismatch size ` source ` ` destination ` assertRaisesRegex RuntimeError movedim Invalid source destination dims fn x assertRaisesRegex RuntimeError movedim repeated dim ` source ` fn x assertRaisesRegex RuntimeError movedim repeated dim ` source ` fn x assertRaisesRegex RuntimeError movedim repeated dim ` destination ` fn x assertRaisesRegex RuntimeError movedim repeated dim ` destination ` fn x dtypes torch int torch float torch complex test_movedim device dtype fn torch moveaxis torch movedim nd range shape = _rand_shape nd min_size= max_size= x = _generate_input shape dtype device with_extremal=False random_negative True False src_dim dst_dim permutations range nd r= random_prob = random random random_negative random_prob src_dim = src_dim - nd random_negative random_prob dst_dim = dst_dim - nd random_negative src_dim = src_dim - nd dst_dim = dst_dim - nd Integer ` source ` ` destination ` torch_fn = partial fn source=src_dim destination=dst_dim np_fn = partial np moveaxis source=src_dim destination=dst_dim compare_with_numpy torch_fn np_fn x device=None dtype=None nd == continue make_index_negative sequence idx sequence = list sequence sequence random_idx = sequence random_idx - nd tuple src_sequence src_sequence permutations range nd r=random randint nd Sequence ` source ` ` destination ` dst_sequence = tuple random sample range nd len src_sequence Randomly change dim negative dim representation itself random_prob = random random random_negative random_prob random_idx = random randint len src_sequence - src_sequence = make_index_negative src_sequence random_idx random_negative random_prob random_idx = random randint len src_sequence - dst_sequence = make_index_negative dst_sequence random_idx random_negative random_idx = random randint len src_sequence - dst_sequence = make_index_negative dst_sequence random_idx random_idx = random randint len src_sequence - src_sequence = make_index_negative src_sequence random_idx torch_fn = partial fn source=src_sequence destination=dst_sequence np_fn = partial np moveaxis source=src_sequence destination=dst_sequence compare_with_numpy torch_fn np_fn x device=None dtype=None Move dim same position x = torch randn torch_fn = partial fn source= destination= np_fn = partial np moveaxis source= destination= compare_with_numpy torch_fn np_fn x device=None dtype=None torch_fn = partial fn source= destination= np_fn = partial np moveaxis source= destination= compare_with_numpy torch_fn np_fn x device=None dtype=None Empty Sequence torch_fn = partial fn source= destination= np_fn = partial np moveaxis source= destination= compare_with_numpy torch_fn np_fn x device=None dtype=None dtypes torch float torch bool test_diag device dtype dtype torch bool x = torch rand device=device = x = torch rand dtype=dtype device=device res = torch diag x res = torch tensor dtype=dtype device=device torch diag x out=res assertEqual res res test_diagonal device x = torch randn device=device result = torch diagonal x expected = torch diag x assertEqual result expected x = torch randn device=device result = torch diagonal x expected = torch diag x assertEqual result expected onlyCPU dtypes torch float test_diagonal_multidim device dtype x = torch randn dtype=dtype device=device xn = x numpy args - - - result = torch diagonal x args expected = xn diagonal args assertEqual expected shape result shape assertEqual expected result test non-contiguous xp = x permute result = torch diagonal xp - - expected = xp numpy diagonal - - assertEqual expected shape result shape assertEqual expected result onlyNativeDeviceTypes dtypes all_types dtypesIfCUDA all_types_and torch half dtypesIfXPU all_types_and torch half test_trace device dtype test shape tensor = make_tensor shape dtype=dtype device=device low=- high= expected_dtype = tensor sum dtype expected_dtype = torch_to_numpy_dtype_dict expected_dtype result = np trace tensor cpu numpy dtype=expected_dtype expected = torch tensor result device=device assertEqual tensor trace expected shapes = shape shapes test shape generate_clamp_baseline device dtype min_vals max_vals with_nans Creates random tensor given device dtype computes expected clamped values given min_vals max_vals If with_nans provided then some values randomly set nan X = torch rand device=device mul add - uniform - X = X dtype with_nans mask = torch randint X shape dtype=torch bool device=device X mask = nan isinstance min_vals torch Tensor min_vals = min_vals cpu numpy isinstance max_vals torch Tensor max_vals = max_vals cpu numpy Use NumPy implementation reference X_clamped = torch tensor np clip X cpu numpy a_min=min_vals a_max=max_vals device=device X X_clamped Tests clamp its alias clip dtypes torch int torch float test_clamp device dtype op_list = torch clamp torch Tensor clamp torch Tensor clamp_ torch clip torch Tensor clip torch Tensor clip_ min max argument product args = product - None None op op_list min_val max_val args min_val None max_val None continue X Y_expected = generate_clamp_baseline device dtype min_vals=min_val max_vals=max_val with_nans=False Test op X = X clone So in-place ops do change X Y_actual = op X min_val max_val assertEqual Y_expected Y_actual Test op-out behavior out does exist method versions op torch clamp torch clip Y_out = torch empty_like X op X min=min_val max=max_val out=Y_out assertEqual Y_expected Y_out test_clamp_propagates_nans device op_list = torch clamp torch Tensor clamp torch Tensor clamp_ torch clip torch Tensor clip torch Tensor clip_ min max argument product args = product - None None op op_list min_val max_val args min_val None max_val None continue X Y_expected = generate_clamp_baseline device torch float min_vals=min_val max_vals=max_val with_nans=True Y_expected = torch isnan Y_expected Test op X = X clone So in-place ops do change X Y_actual = op X min_val max_val assertEqual Y_expected torch isnan Y_actual Test op-out behavior out does exist method versions op torch clamp torch clip Y_out = torch empty_like X op X min_val max_val out=Y_out assertEqual Y_expected torch isnan Y_out test_clamp_raises_arg_errors device X = torch randn dtype=torch float device=device error_msg = At least one min max must None assertRaisesRegex RuntimeError error_msg X clamp assertRaisesRegex RuntimeError error_msg X clamp_ assertRaisesRegex RuntimeError error_msg torch clamp X dtypes all_types_and_complex_and torch half torch bool torch bfloat test_flip device dtype make_from_data = partial torch tensor device=device dtype=dtype make_from_size = partial make_tensor device=device dtype=dtype test_flip_impl input_t dims output_t all_t yield input_t output_t dtype torch float We generate quantized versions well qdtype torch quint torch qint torch qint qinput_t = torch quantize_per_tensor input_t qdtype qoutput_t = torch quantize_per_tensor output_t qdtype yield qinput_t qoutput_t in_t out_t all_t assertEqual in_t flip dims out_t n = in_t ndim isinstance dims tuple Wrap dim assertEqual in_t flip -n + dims out_t Permute dimensions p_dims permutations dims assertEqual in_t flip p_dims out_t len p_dims Wrap st dim assertEqual in_t flip -n + p_dims + p_dims out_t gen_data Basic tests data = make_from_data view nonctg = make_from_size noncontiguous=True copy_ data dims_result = make_from_data view make_from_data view make_from_data view make_from_data view make_from_data view in_tensor dims out_tensor product data nonctg dims_result yield in_tensor dims out_tensor Expanded in_t = make_from_data view expand dims = out_t = make_from_data view yield in_t dims out_t Noop expanded dimension yield in_t in_t Transposed in_t = make_from_data view transpose dims = out_t = make_from_data view yield in_t dims out_t Rectangular case in_t = make_from_data view dims = out_t = make_from_data yield in_t dims out_t dims = out_t = make_from_data yield in_t dims out_t vectorized NCHW cases images device == cpu dtype = torch bfloat mf torch contiguous_format torch channels_last c in_t = make_from_size c contiguous memory_format=mf np_in_t = in_t numpy np_out_t = np_in_t - copy out_t = torch from_numpy np_out_t yield in_t out_t np_out_t = np_in_t - copy out_t = torch from_numpy np_out_t yield in_t out_t non-contig cases in_tt = in_t np_in_t = in_tt numpy np_out_t = np_in_t - copy out_t = torch from_numpy np_out_t yield in_tt out_t in_tt = in_t np_in_t = in_tt numpy np_out_t = np_in_t - copy out_t = torch from_numpy np_out_t yield in_tt out_t Noops edge cases Size in_t = make_from_data yield in_t in_t yield in_t in_t dims = in_t = make_from_size yield in_t in_t Zero elements non-zero size in_t = make_from_size i range in_t ndim yield in_t i in_t Size in_t = make_from_size yield in_t in_t in_t = make_from_size yield in_t in_t in_tensor dims out_tensor gen_data test_flip_impl in_tensor dims out_tensor test shape size = data = make_from_size size possible_dims = range len size test_dims = chain combinations possible_dims combinations possible_dims dims test_dims assertEqual size list data flip dims size dtypes all_types_and_complex_and torch half torch bool torch bfloat test_flip_errors device dtype make_arg = partial make_tensor dtype=dtype device=device data = make_arg allow flip same dim more than once assertRaises RuntimeError lambda data flip allow empty list input assertRaises TypeError lambda data flip allow dim max dim assertRaises IndexError lambda data flip assertRaises IndexError lambda data flip _rand_shape dim min_size max_size tuple torch randint min_size max_size + dim dtypes all_types_and_complex_and torch half torch bool torch bfloat test_flip_numpy device dtype make_arg = partial make_tensor dtype=dtype device=device ndim shape = _rand_shape ndim data = make_arg shape Axis sample given shape i range ndim + Check all combinations ` i ` axis flip_dim combinations range ndim i torch_fn = partial torch flip dims=flip_dim np_fn = partial np flip axis=flip_dim compare_with_numpy torch_fn np_fn data onlyOn cuda xpu CPU too slow largeTensorTest GB tensors GB out x torch numpy + GB largeTensorTest GB cpu even CUDA test sufficient system memory required unittest skipIf IS_JETSON Too large Jetson test_flip_large_tensor device t_in = torch empty + dtype=torch uint random_ torch_fn = partial torch flip dims= np_fn = partial np flip axis= compare_with_numpy torch_fn np_fn t_in del t_in onlyCPU unittest expectedFailure dtypes torch quint x torch quint x test_flip_unsupported_dtype dtype scale zero_point = qt = torch quantize_per_tensor torch randn scale=scale zero_point=zero_point dtype=dtype torch flip qt dims= _test_fliplr_flipud torch_fn np_fn min_dim max_dim device dtype dim range min_dim max_dim + shape = _rand_shape dim Randomly scale input dtype is_floating_point dtype is_complex data = torch randn shape device=device dtype=dtype data = torch randint shape device=device dtype=dtype compare_with_numpy torch_fn np_fn data dtypes torch int torch double torch cdouble test_fliplr device dtype _test_fliplr_flipud torch fliplr np fliplr device dtype dtypes torch int torch double torch cdouble test_fliplr_invalid device dtype x = torch randn dtype assertRaisesRegex RuntimeError Input must = -d torch fliplr x assertRaisesRegex RuntimeError Input must = -d torch fliplr torch tensor device=device dtype=dtype dtypes torch int torch double torch cdouble test_flipud device dtype _test_fliplr_flipud torch flipud np flipud device dtype dtypes torch int torch double torch cdouble test_flipud_invalid device dtype assertRaisesRegex RuntimeError Input must = -d torch flipud torch tensor device=device dtype=dtype test_rot device data = torch arange device=device view assertEqual torch tensor view data rot assertEqual torch tensor view data rot assertEqual torch tensor view data rot assertEqual torch tensor view data rot test default args k= dims= assertEqual data rot data rot test reversed order dims assertEqual data rot data rot test modulo k assertEqual data rot data rot assertEqual data rot data rot - assertEqual data rot - data rot - test dims out-of-range error assertRaises RuntimeError lambda data rot - assertRaises RuntimeError lambda data rot test tensor more than D data = torch arange device=device view assertEqual torch tensor view data rot assertEqual data rot - data rot test errors assertRaises RuntimeError lambda data rot assertRaises RuntimeError lambda data rot assertRaises RuntimeError lambda data rot assertRaises RuntimeError lambda data rot skipIfTorchDynamo TorchDynamo fails unknown error dtypes torch cfloat torch cdouble test_complex_rot device dtype shape = _rand_shape random randint rot_times range data = torch randn shape device=device dtype=dtype torch_fn = partial torch rot k=rot_times dims= np_fn = partial np rot k=rot_times axes= compare_with_numpy torch_fn np_fn data TODO update once warning flag available always trigger ONCE warnings Ensures nonzero does throw warning even when as_tuple argument provided test_nonzero_no_warning device t = torch randn device=device warnings catch_warnings record=True w warnings simplefilter always torch nonzero t t nonzero assertEqual len w dtypes all_types_and torch half torch bool torch bfloat test_nonzero device dtype shapes = torch Size torch Size torch Size torch Size torch Size torch Size gen_nontrivial_input shape dtype device dtype = torch bfloat torch randint shape device=device dtype=dtype windows does work bfloat randing torch randint shape device=device dtype=torch float dtype shape shapes tensor = gen_nontrivial_input shape dtype device dst = torch nonzero tensor as_tuple=False dst = tensor nonzero as_tuple=False dst = torch empty dtype=torch long device=device torch nonzero tensor out=dst device_type = xla xla does raise runtime error assertRaisesRegex RuntimeError scalar type Long lambda torch nonzero tensor out=torch empty dtype=torch float device=device device_type == cuda device_type == xpu device_type == TEST_PRIVATEUSE _DEVICE_TYPE assertRaisesRegex RuntimeError same device lambda torch nonzero tensor out=torch empty dtype=torch long np_array = tensor cpu numpy dtype = torch bfloat tensor float cpu numpy np_result = torch from_numpy np stack np_array nonzero t assertEqual dst cpu np_result atol= rtol= assertEqual dst cpu np_result atol= rtol= assertEqual dst cpu np_result atol= rtol= tup = torch nonzero tensor as_tuple=True tup = tensor nonzero as_tuple=True tup = torch stack tup t cpu tup = torch stack tup t cpu assertEqual tup np_result atol= rtol= assertEqual tup np_result atol= rtol= test_nonzero_astuple_out device t = torch randn device=device out = torch empty_like t dtype=torch long assertRaises RuntimeError torch nonzero t as_tuple=True out=out assertEqual torch nonzero t as_tuple=False out=out torch nonzero t out=out Verifies JIT script cannot handle as_tuple kwarg See Issue https github com pytorch pytorch issues _foo t tuple_result = torch nonzero t as_tuple=True nontuple_result = torch nonzero t as_tuple=False out = torch empty_like nontuple_result torch nonzero t as_tuple=False out=out tuple_result nontuple_result out assertRaises RuntimeError torch jit script _foo Verifies JIT tracing works fine traced_foo = torch jit trace _foo t traced_tuple traced_nontuple traced_out = traced_foo t expected_tuple = torch nonzero t as_tuple=True expected_nontuple = torch nonzero t assertEqual traced_tuple expected_tuple assertEqual traced_nontuple expected_nontuple assertEqual traced_out expected_nontuple onlyNativeDeviceTypes test_nonzero_discontiguous device shape = tensor = torch randint shape device=device tensor_nc = torch empty shape shape device=device copy_ tensor dst = tensor nonzero as_tuple=False dst = tensor_nc nonzero as_tuple=False assertEqual dst dst atol= rtol= dst = torch empty_like dst data_ptr = dst data_ptr expect dst storage reused torch nonzero tensor out=dst assertEqual data_ptr dst data_ptr assertEqual dst dst atol= rtol= discontiguous out dst = torch empty dst size dst size dtype=torch long device=device data_ptr = dst data_ptr strides = dst stride torch nonzero tensor out=dst assertEqual data_ptr dst data_ptr assertEqual dst dst atol= rtol= assertEqual strides dst stride test_nonzero_non_diff device x = torch randn requires_grad=True nz = x nonzero assertFalse nz requires_grad dtypes torch int torch float torch complex test_sparse_dense_dim device dtype shape dtype is_complex dtype is_floating_point x = torch rand shape device=device dtype=dtype x = torch randint - shape device=device dtype=dtype assertEqual x sparse_dim assertEqual x dense_dim len shape test_unfold_all_devices_and_dtypes device dt all_types_and_complex_and torch half torch bool torch bfloat dt == torch bool x = torch empty dtype=dt device=device assertEqual x unfold shape x = torch empty dtype=dt device=device assertEqual x unfold shape test_unfold_scalars device x = torch tensor device=device unfold -dimensional tensor should always -d dimensional tensor shape size i e second parameter unfold assertEqual torch empty device=device x unfold assertEqual torch empty device=device x unfold assertEqual torch tensor device=device x unfold test_unfold_errors device x = torch arange device=device assertRaisesRegex RuntimeError size - must = x unfold - assertRaisesRegex RuntimeError step - must x unfold - instantiate_device_type_tests TestShapeOps globals __name__ == __main__ run_tests