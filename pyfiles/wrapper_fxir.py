dataclasses functools logging operator textwrap collections Counter collections abc Sequence typing Any Callable Optional Union sympy torch torch _export passes _node_metadata_hook _node_metadata_hook _set_node_metadata_hook torch _higher_order_ops triton_kernel_wrap TraceableTritonKernelWrapper tracing_triton_hopifier_singleton triton_kernel_wrapper_mutation torch _inductor codecache LambdaFuture PyCodeCache torch _inductor runtime triton_heuristics CachingAutotuner torch _inductor select_algorithm extern_kernels noqa F torch _inductor utils convert_to_symint torch _inductor virtualized V torch _library triton wrap_triton torch fx GraphModule torch fx experimental symbolic_shapes CallMethodKey ConvertIntKey DivideByKey free_unbacked_symbols torch utils _pytree pytree torch utils _sympy functions FloorDiv torch utils _sympy interp _run_sympy_handler sympy_interp torch utils _sympy reference OptimizedPythonReferenceAnalysis torch utils _sympy solve try_solve config ir runtime triton_compat Config utils cache_property_on_self LineContext ValueWithLineMap common CodegenSymbol FileBackedGraphModule WorkspaceArg WorkspaceZeroMode wrapper AllocateLine BufferLike CommBufferAllocateLine CommBufferFreeLine CommentLine ConditionalLine DynamicScalarLine EnterDeviceContextManagerLine EnterSubgraphLine ExitDeviceContextManagerLine ExitSubgraphLine ExternKernelAllocLine ExternKernelOutLine FreeIfNotReusedLine FreeLine IndexPutFallbackLine KernelCallLine KernelDefinitionLine Line MultiOutputLine NullLine PythonWrapperCodegen ReinterpretLine ReuseLine ScatterFallbackLine SubgraphPythonWrapperCodegen SymbolicCallArg SymbolicCallArgLine UnbackedSymbolDefsLine WrapperLine aten = torch ops aten log = logging getLogger __name__ dataclasses dataclass SymbolBuffer CodegenSymbol Represents sympy Symbol graph input symbol sympy Symbol get_name - str str symbol get_example - Union torch Tensor torch SymInt sym_int = convert_to_symint symbol assert isinstance sym_int torch SymInt sym_int CodegenBuffer = Union BufferLike SymbolBuffer dataclasses dataclass TritonKernel Stores metadata about Triton kernels use FX tuner CachingAutotuner wrapped TraceableTritonKernelWrapper replace_floor_div expr sympy Expr - sympy Expr Replace sympy floor FloorDiv replace expr sympy Expr - sympy Expr expr = sympy together expr Division represented Mul Rational factor Pow negative exponent We convert floor Mul FloorDiv numerator denominator partitioning factors into numerator denominator numerator denominator = sympy S One arg sympy Mul make_args expr isinstance arg sympy Rational numerator = arg numerator denominator = arg denominator isinstance arg sympy Pow arg exp is_negative denominator = arg base -arg exp numerator = arg FloorDiv numerator denominator expr replace sympy floor replace WrapperFxCodegen PythonWrapperCodegen Backend generate wrapper code FX IR graph supports_caching = False __init__ args Any kwargs Any super __init__ args kwargs subgms dict str torch fx GraphModule = codegen_inputs - None This would generate code symbolic input shapes strides etc Since FX converter handles do nothing here codegen_conditional conditional ir Conditional - None Conditional codegen normally emits number different wrapper lines Instead FX conversion uses dedicated line whole conditional writeline ConditionalLine conditional subgraph conditional true_subgraph conditional false_subgraph codegen_subgraph_common subgraph define_subgraph_launcher_fn name str subgraph_code Union ValueWithLineMap FileBackedGraphModule - None Record subgms they re generated assert isinstance subgraph_code FileBackedGraphModule subgms name = subgraph_code gm property cache_property_on_self is_subgraph - bool isinstance SubgraphPythonWrapperCodegen get_fx_graph_inputs - dict str Union ir TensorBox ir TorchBindObject sympy Expr None Get input nodes corresponding FX graph placeholders pyrefly ignore missing-argument V aot_compilation is_subgraph AOT graphs must match signature input module node name V graph graph_inputs get node name node V graph module graph find_nodes op= placeholder type ignore operator union-attr get_graph_inputs _generate is_inference bool - tuple FileBackedGraphModule None run_wrapper_ir_passes is_inference prologue = \n join imports getvalue header getvalue gm = FxConverter lines=self lines prologue=prologue graph_inputs=self get_fx_graph_inputs graph_outputs=self get_graph_outputs subgms=self subgms pyrefly ignore missing-argument is_subgraph=self is_subgraph generate compiled_fn = compile_graph gm FileBackedGraphModule gm compiled_fn None compile_graph gm GraphModule - Callable Any Converts graph module into runnable function The default implementation simply interpreter calling kernels eager mode Derived backends can override do further compilation gm forward write_header - None Python subgraphs normally lack headers Override behavior generate prologues FX subgraphs PythonWrapperCodegen write_header classmethod create cls type WrapperFxCodegen is_subgraph bool subgraph_name Optional str parent_wrapper Optional PythonWrapperCodegen partition_signatures Optional ir GraphPartitionSignature = None - WrapperFxCodegen is_subgraph assert subgraph_name None assert parent_wrapper None Subgraphs override some methods PythonWrapperCodegen Apply these overrides user-provided priority given user-provided methods SubgraphFxWrapperCodegen cls SubgraphPythonWrapperCodegen type ignore misc valid-type compile_graph gm GraphModule - Callable Any Skip graph compilation subgraphs crash_if_run args Any - None raise NotImplementedError Cannot run subgraph isolation crash_if_run SubgraphFxWrapperCodegen subgraph_name parent_wrapper partition_signatures cls dataclasses dataclass FxConverter Generates FX IR Wrapper IR As each instance only meant used once input output code stored attributes lines list Line prologue str graph_inputs dict str Union ir TensorBox ir TorchBindObject sympy Expr None graph_outputs list ir IRNode subgms dict str torch fx GraphModule is_subgraph bool __post_init__ - None graph = torch fx Graph gm = GraphModule graph Wrapper FX IR buffer_to_node dict Optional str torch fx Node = Symbol table codegen kernels dict str TritonKernel = Table store Triton kernels _unique_symbol_ids Counter str = Counter tracer = torch fx proxy GraphAppendingTracer graph expr_to_proxy dict sympy Expr torch fx Proxy = _import_kernel code str kernel_name str - CachingAutotuner Imports kernel source possibly autotuning block parameters module_code = \n join prologue code mod = PyCodeCache load module_code kernel = getattr mod kernel_name isinstance kernel LambdaFuture kernel = kernel result isinstance kernel CachingAutotuner raise NotImplementedError textwrap dedent f Unsupported type kernel kernel_name type kernel FX conversion only supports Triton kernels kernel _create_as_strided input_node torch fx Node size tuple Any stride tuple Any offset Union int sympy Expr - torch fx Node gm graph call_function torch as_strided args= input_node _generate_sym_nodes size _generate_sym_nodes stride _generate_sym_node offset _record_allocation buffer CodegenBuffer node torch fx Node - None Updates symbol table record Inductor buffer maps result FX node assert node buffer_to_node buffer_to_node buffer get_name = node _free buffer Union CodegenBuffer ir TorchBindObject - None Removes buffer symbol table name = buffer get_name del buffer_to_node name _lookup_args args tuple Any - tuple Any Maps call args back FX nodes tuple buffer_to_node arg isinstance arg str arg inner_expr isinstance arg SymbolicCallArg arg arg args _get_buffer node ir IRNode - CodegenBuffer Extract buffer data IR node isinstance node ir Buffer WorkspaceArg node isinstance node ir BaseView ir MutableBox _get_buffer node data isinstance node sympy Symbol SymbolBuffer node raise NotImplementedError f Unable extract buffer node node _generate_size_proxy node torch fx Node expr sympy Expr - torch fx Proxy proxy = torch fx Proxy node tracer=self tracer expr_to_proxy expr = proxy proxy _generate_graph_inputs - None Converts graph inputs FX placeholders name ir_node graph_inputs items ir_node None Create dummy input nodes match input signature gm graph placeholder name continue Introduce new symbol constant inputs is_constant = isinstance ir_node int float sympy Integer sympy Float buffer = SymbolBuffer sympy Symbol name is_integer=True is_constant _get_buffer ir_node placeholder_node = gm graph placeholder buffer get_name placeholder_node meta val = ir_node is_constant buffer get_example _record_allocation buffer placeholder_node Record symbol definitions dynamic shapes isinstance ir_node sympy Symbol _generate_size_proxy placeholder_node ir_node _generate_graph_input_shapes - None Generate nodes creating symints part graph input shape strides _codegen_symbol sym_or_exp Union sympy Symbol sympy Expr base_node torch fx Node target torch _ops OpOverload dim int - None codegen_proxy - torch fx Proxy size_node = gm graph call_function target base_node dim size_proxy = _generate_size_proxy size_node sym_or_exp size_proxy isinstance sym_or_exp sympy Symbol sym_or_exp expr_to_proxy codegen_proxy isinstance sym_or_exp sympy Integer isinstance sym_or_exp sympy Expr Check we need solve undefined symbol undefined_symbols = sym sym sym_or_exp free_symbols sym expr_to_proxy len undefined_symbols == _sympy_interp sym_or_exp len undefined_symbols raise ValueError f Underdetermined input expression sym_or_exp Define new symbol input size size_proxy = codegen_proxy size_symbol = sympy Symbol size_proxy node name integer=True nonnegative=True expr_to_proxy size_symbol = size_proxy Solve undefined symbol undefined_symbol = undefined_symbols solution = try_solve sympy Eq sym_or_exp size_symbol undefined_symbol solution None raise ValueError f Cannot solve input expression sym_or_exp Since symbol size must integer Therefore we can convert division FloorDiv undefined_symbol_expr = solution undefined_symbol is_integer undefined_symbol_expr = replace_floor_div sympy floor undefined_symbol_expr Generate FX symbol _sympy_interp undefined_symbol_expr expr_to_proxy undefined_symbol = expr_to_proxy undefined_symbol_expr ir_node graph_inputs values isinstance ir_node ir TensorBox buffer = _get_buffer ir_node placeholder_node = buffer_to_node buffer get_name dim size enumerate ir_node get_size _codegen_symbol size placeholder_node torch ops aten sym_size int dim dim stride enumerate ir_node get_stride _codegen_symbol stride placeholder_node torch ops aten sym_stride int dim _generate_graph_constants - None name value V graph constants items node = gm graph get_attr name node meta val = value setattr gm name value buffer_to_node name = node _generate_buffer node ir IRNode - Optional torch fx Node Generates FX IR transformations buffer such ReinterpretView Does nothing no such transformations present isinstance node ir ShapeAsConstantBuffer Generate FX nodes compute shape expression _sympy_interp node expr node generate_to_buffer node ir IRNode - Optional BufferLike isinstance node ir Buffer WorkspaceArg node isinstance node ir NoneAsConstantBuffer None isinstance node ir MutableBox generate_to_buffer node data isinstance node ir ReinterpretView We need introduce new symbol output ReinterpretView Use WorkspaceArg buffer = _get_buffer node data assert isinstance buffer ir Buffer WorkspaceArg unique_name = gm graph _graph_namespace create_name f buffer get_name _view None device = buffer get_device assert device reused_as = WorkspaceArg count=buffer get_size zero_mode=WorkspaceZeroMode UNINITIALIZED device=device outer_name=unique_name dtype=buffer get_dtype Generate FX IR view _generate_reinterpret_helper buffer reused_as node layout reused_as raise NotImplementedError f Unrecognized buffer view node node buffer = generate_to_buffer node buffer_to_node buffer get_name buffer None None _generate_outputs - Union Optional torch fx Node list Optional torch fx Node Generate FX IR graph outputs output_nodes = _generate_buffer node idx node enumerate graph_outputs Parent graphs single elements don t use tuple output_value = output_nodes len output_nodes == is_subgraph output_nodes output_value _generate_subgm_getattrs - None Generate getattr nodes subgms generate_getattr name str subgm torch fx GraphModule - torch fx Node gm add_submodule name subgm node = gm graph get_attr name node meta val = subgm node subgm_getattrs = name generate_getattr name subgm name subgm subgms items _get_subgm_attr subgraph ir Subgraph - torch fx Node Look up getattr node subgraph graph = subgraph graph assert graph None subgm_getattrs graph name generate - torch fx GraphModule Main entrypoint FX codegen _generate_graph_inputs _generate_graph_constants _generate_subgm_getattrs _set_node_metadata_hook gm functools partial _node_metadata_hook fake_mode=V fake_mode _generate_graph_input_shapes Generate FX IR Wrapper IR lines line lines isinstance line WrapperLine line codegen_fx line isinstance line LineContext Ignore line context FX IR pass raise NotImplementedError textwrap dedent f Found line unrecognized type type line line FX conversion only supports Wrapper IR lines output = _generate_outputs gm graph output output gm recompile gm _sympy_interp expr sympy Expr - torch fx Proxy hash cons expr expr_to_proxy expr_to_proxy expr base cases don t cache isinstance expr sympy Integer sympy Number sympy Symbol sympy logic boolalg BooleanAtom sympy_interp OptimizedPythonReferenceAnalysis expr_to_proxy expr hash cons arguments run expr handler expr_to_proxy expr = _run_sympy_handler OptimizedPythonReferenceAnalysis _sympy_interp arg arg expr args expr expr_to_proxy expr _generate_sym_node s Union int sympy Expr - Union int torch fx Node isinstance s int sympy Integer int s isinstance s sympy Symbol assert s expr_to_proxy f Could find node corresponding symbol s expr_to_proxy s node isinstance s sympy Expr _sympy_interp s node isinstance s torch fx Node s raise ValueError f s type type s valid input _generate_sym_nodes shape Sequence sympy Expr - list Union int torch fx Node _generate_sym_node s s shape _generate_allocate line WrapperLine - None assert isinstance line AllocateLine buffer = line node name = buffer get_name assert name V graph removed_buffers device = buffer get_device dtype = buffer get_dtype shape = _generate_sym_nodes buffer get_size stride = _generate_sym_nodes buffer get_stride node = gm graph call_function torch empty_strided args= shape stride kwargs= dtype dtype device device assert name node name = name _record_allocation buffer node _generate_conditional line WrapperLine - None assert isinstance line ConditionalLine get_subgm_attr subgraph Optional ir Subgraph - torch fx Node assert subgraph None _get_subgm_attr subgraph Access subgraphs getattrs ir_node = line node true_subgm false_subgm = get_subgm_attr subgraph subgraph ir_node true_subgraph ir_node false_subgraph generate_buffer node Optional ir IRNode - Optional torch fx Node assert node None _generate_buffer node predicate = generate_buffer ir_node predicate assert ir_node operands None operands = tuple generate_buffer arg arg ir_node operands fx_node = gm graph call_function torch ops higher_order cond args= predicate true_subgm false_subgm operands _record_allocation ir_node fx_node _generate_comment line WrapperLine - None assert isinstance line CommentLine We ignore comments FX IR _generate_dynamic_scalar line WrapperLine - None assert isinstance line DynamicScalarLine ir_node = line node input_ir_node = ir_node inputs assert isinstance input_ir_node ir IRNode input_fx_node = _generate_buffer input_ir_node keypath = ir_node keypath graph = gm graph generate_item x Optional torch fx Node - torch fx Node assert x None graph call_function aten item default args= x len keypath == result_fx_node = generate_item input_fx_node len keypath == isinstance keypath ConvertIntKey where_fx_node = graph call_function aten where Scalar args= input_fx_node result_fx_node = generate_item where_fx_node raise NotImplementedError f Unsupported keypath keypath result_symbol = ir_node sym result_buffer = SymbolBuffer result_symbol _record_allocation result_buffer result_fx_node _generate_size_proxy result_fx_node result_symbol _generate_enter_device_context_manager line WrapperLine - None assert isinstance line EnterDeviceContextManagerLine We ignore device context FX IR _generate_exit_device_context_manager line WrapperLine - None assert isinstance line ExitDeviceContextManagerLine We ignore device context FX IR _generate_enter_subgraph line WrapperLine - None assert isinstance line EnterSubgraphLine We ignore memory planning lines FX IR _generate_exit_subgraph line WrapperLine - None assert isinstance line ExitSubgraphLine We ignore memory planning lines FX IR _generate_free line WrapperLine - None assert isinstance line FreeLine buf = line node No need free placeholders buffer_to_node buf get_name op == placeholder _free buf _generate_free_if_not_reused line WrapperLine - None assert isinstance line FreeIfNotReusedLine buf = line node assert buf get_name V graph removed_buffers line is_reused _free buf _generate_line_context line WrapperLine - None assert isinstance line LineContext We ignore line context FX IR _generate_reinterpret line WrapperLine - None assert isinstance line ReinterpretLine _generate_reinterpret_helper line node line reused_as line layout _generate_reinterpret_helper input_buffer BufferLike result_buffer BufferLike layout ir Layout - None input_node = buffer_to_node input_buffer get_name Look up output metadata name = result_buffer get_name assert name size = tuple layout size stride = tuple layout stride isinstance layout ir NonOwningLayout Look up view s layout view = layout view assert isinstance view ir ReinterpretView f unexpected type type view layout = view layout offset = input_buffer get_offset + layout offset Map ReinterpretView as_strided result_node = _create_as_strided input_node size stride offset result_node name = name _record_allocation result_buffer result_node _generate_reuse line WrapperLine - None assert isinstance line ReuseLine old = line node new = line reused_as assert any buf get_name V graph removed_buffers buf old new assert old get_dtype == new get_dtype old_node = buffer_to_node old get_name result_node = old_node Change shape stride size = tuple new get_size stride = tuple new get_stride offset = new get_offset tuple old get_size = size tuple old get_stride = stride old get_offset = offset result_node = _create_as_strided old_node size stride offset _record_allocation new result_node Free old buffer we allocated new tensor old get_name V graph get_output_names line delete_old result_node old_node _free old _generate_multi_output line WrapperLine - None assert isinstance line MultiOutputLine arg_node = buffer_to_node line arg_name For non-tuple non-list outputs map output same node input len line indices == buffer_to_node line result_name = arg_node Extract index tuple access inds = line indices assert len inds == f Cannot convert inds index idx = inds node = gm graph call_function operator getitem args= arg_node idx node name = line result_name buffer_to_node line result_name = node _generate_fallback_call ir_node ir ExternKernel args Optional tuple Any = None kwargs Optional dict str Any = None - None fx_node = gm graph call_function ir_node op_overload type ignore arg-type args=args kwargs=kwargs result_buffer = ir_node codegen_reference buffer_to_node result_buffer = fx_node _generate_index_put_fallback line WrapperLine - None assert isinstance line IndexPutFallbackLine ir_node = line node generate_buffer_or_none x Union ir IRNode Sequence ir IRNode None - Optional torch fx Node Handles None before calling _generate_buffer x None None assert isinstance x ir IRNode _generate_buffer x x values = generate_buffer_or_none t t ir_node inputs indices = tuple generate_buffer_or_none t t line indices accumulate = ir_node constant_args args = x indices values accumulate _generate_fallback_call ir_node args _generate_scatter_fallback line WrapperLine - None assert isinstance line ScatterFallbackLine ir_node = line node assert ir is_node_sequence ir_node inputs x index src = _generate_buffer t t ir_node inputs + ir_node src_is_tensor ir_node constant_args args = x ir_node constant_args index src kwargs = reduce = ir_node kwargs get reduce kwargs reduce = reduce _generate_fallback_call ir_node args kwargs _generate_null line WrapperLine - None assert isinstance line NullLine Does nothing _generate_comm_buffer_allocate line WrapperLine - None assert isinstance line CommBufferAllocateLine raise NotImplementedError Comm buffer allocation yet supported _generate_comm_buffer_free line WrapperLine - None assert isinstance line CommBufferFreeLine _free line node _generate_triton_call line WrapperLine - None assert isinstance line KernelCallLine Collect all kwargs including autotuned block sizes call_args = _lookup_args line call_args kernel = kernels line kernel_name tuner = kernel tuner UnbackedSymintsError Exception pass tune_kernel tuner CachingAutotuner call_args Sequence Any - None triton runtime driver log info Autotuning Triton kernel s compile time kernel_name device = driver active get_current_device stream = driver active get_current_stream device node_to_tuning_arg arg Any - Any Create real tensors autotuning arguments substituting size hints dynamic shapes to_size_hint arg Any - Any len free_unbacked_symbols arg NYI tuning args require backed symints raise UnbackedSymintsError pytree tree_map V graph sizevars size_hint arg isinstance arg torch fx Node to_size_hint arg fake = arg meta val torch empty_strided to_size_hint fake shape to_size_hint fake stride dtype=fake dtype device=device zero_ arg_values = node_to_tuning_arg arg arg call_args tuner run arg_values stream=stream Optionally autotune kernels The FX backend currently only supports compile-time tuning kernel_name = tuner fn __name__ config triton autotune_at_compile_time try tune_kernel tuner call_args except UnbackedSymintsError log info Detected unbacked symints Skipping autotuning kernel s kernel_name log info Skipping autotuning kernel s Set config triton autotune_at_compile_time = True enable kernel_name triton_meta = tuner triton_meta signature = triton_meta signature add_constants_to_call_args call_args Sequence Any cfg Config - tuple Any Add constant kwargs arg list Add args proper Triton signature Exclude constants config kwargs those tracked separately new_call_args = constants = triton_meta constants call_kwargs = key val key val zip signature call_args pyrefly ignore missing-attribute key constants key cfg kwargs Add constants stored Triton metadata signature order call_kwargs &#124; = constants new_call_args = call_kwargs key key signature pyrefly ignore missing-attribute key cfg kwargs Add Inductor s extra launcher args end extra_launcher_args = tuner inductor_meta get extra_launcher_args new_call_args extend call_args len call_args - len extra_launcher_args tuple new_call_args kernel_config = tuner compile_results config extra_options = getattr kernel_config extra_options None call_args = add_constants_to_call_args call_args kernel_config call_args grid = tuner _interpret_args_grid call_args kernel_config call_kwargs = dict zip signature call_args pyrefly ignore missing-attribute assert any kwarg kernel_config kwargs kwarg call_kwargs f kwargs overlap config call_kwargs pyrefly ignore missing-attribute call_kwargs update kernel_config kwargs Replace sympy floor FloorDiv make expression traceable grid = replace_floor_div x isinstance x sympy Expr x x grid wrapper_grid = tuple _generate_sym_nodes grid call_kwargs = name _generate_sym_node val name val call_kwargs items Store non-graphable kwargs side table call_kwargs constant_args_idx = tracing_triton_hopifier_singleton store_non_graphable_args call_kwargs triton_node = gm graph call_function triton_kernel_wrapper_mutation kwargs= kernel_idx kernel wrapped kernel_idx constant_args_idx constant_args_idx grid wrapper_grid tma_descriptor_metadata kwargs call_kwargs extra_options triton_node meta extra_options = extra_options _generate_extern_kernel_alloc line WrapperLine - None assert isinstance line ExternKernelAllocLine node = line node _generate_extern_kernel_common node node _generate_extern_kernel_out line WrapperLine - None assert isinstance line ExternKernelOutLine node = line node out_node = node output_view node output_view node _generate_extern_kernel_common node out_node _generate_extern_kernel_common kernel ir ExternKernel out_ir_node ir IRNode - None Generates FX IR either ExternKernelAlloc ExternKernelOut Get FX nodes corresponding call args assert ir is_node_sequence kernel inputs tensor_nodes = tuple _generate_buffer arg arg kernel inputs hasattr kernel unflatten_args args _ = kernel unflatten_args tensor_nodes kernel constant_args args = tensor_nodes + tuple kernel constant_args Get result buffer Some kernels write pre-existing output tensor via out kwarg kwargs = kernel kwargs copy result_buffer Optional str = None isinstance kernel ir ExternKernelOut kwargs out = buffer_to_node out_ir_node codegen_reference isinstance kernel layout ir Layout ir MultiOutputLayout result_buffer = kernel get_name isinstance kernel layout ir NoneLayout pass raise NotImplementedError f Unrecognized output layout kernel layout fx_node = gm graph call_function kernel op_overload type ignore arg-type args=args kwargs=kwargs Assign result given name result_buffer assert out kwargs f Extern kernel kernel has both result out kwarg Expected only one fx_node name = result_buffer buffer_to_node result_buffer = fx_node _generate_kernel_call line WrapperLine - None assert isinstance line KernelCallLine line triton raise NotImplementedError FX conversion only supports Triton kernels _generate_triton_call line _generate_kernel_definition line WrapperLine - None assert isinstance line KernelDefinitionLine Generate code kernel kernel_code = PythonWrapperCodegen _format_kernel_definition line kernel_name line kernel_body metadata=line metadata Import module store JIT kernel tuner = _import_kernel kernel_code line kernel_name wrapped = wrap_triton tuner fn kernels line kernel_name = TritonKernel tuner wrapped _generate_symbolic_call_arg line WrapperLine - None assert isinstance line SymbolicCallArgLine Store arg expr mapping later use arg = line arg inner_expr_proxy = _sympy_interp arg inner_expr expr_to_proxy arg inner = inner_expr_proxy _generate_unbacked_symbol_defs line WrapperLine - None assert isinstance line UnbackedSymbolDefsLine graph = gm graph convert_key node torch fx Node path pytree KeyPath - torch fx Node Generate FX IR each key entry Base case len path == node Process first entry recurse entry = path isinstance entry CallMethodKey target = size aten sym_size int stride aten sym_stride int storage_offset aten sym_storage_offset entry name assert callable target node = graph call_function target args= node path idx len path isinstance path pytree SequenceKey node convert_key node path + len node args isinstance entry pytree SequenceKey node = graph call_function operator getitem args= node entry idx convert_key node path isinstance entry DivideByKey node = graph call_function operator floordiv args= node entry divisor convert_key node path raise NotImplementedError f Unrecognized entry type type entry root_node = buffer_to_node line output_name unbacked_bindings = line unbacked_bindings assert unbacked_bindings None s keypath unbacked_bindings items Check we already generated symbol s name buffer_to_node continue node = convert_key root_node keypath out_buffer = SymbolBuffer s _record_allocation out_buffer node _generate_size_proxy node s