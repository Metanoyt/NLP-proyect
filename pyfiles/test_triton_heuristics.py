Owner s module inductor functools sys unittest unittest skipUnless unittest mock MagicMock patch torch torch _dynamo testing rand_strided torch _inductor runtime triton_compat HAS_WARP_SPEC torch _inductor utils clone_preserve_strides torch testing _internal common_utils instantiate_parametrized_tests IS_LINUX parametrize runOnRocm skipIfRocm skipIfXpu torch testing _internal inductor_utils GPU_TYPE HAS_CUDA_AND_TRITON HAS_GPU requires_cuda_with_enough_memory try triton noqa F manual triton language tl manual except ImportError __name__ == __main__ sys exit raise unittest SkipTest requires triton noqa B torch _inductor config torch _inductor runtime hints AttrsDescriptorWrapper AutotuneHint DeviceProperties HeuristicType TRITON_MAX_BLOCK torch _inductor runtime triton_helpers math tl_math torch _inductor runtime triton_heuristics autotune_hints_to_configs CachingAutotuner template triton_config torch _inductor test_case run_tests TestCase triton jit amd_sqr_kernel in_ptr out_ptr numel BLOCK_SIZE tl constexpr pid = tl program_id offsets = pid BLOCK_SIZE + tl arange BLOCK_SIZE data = tl load in_ptr + offsets mask=offsets numel sqr = data data tl store out_ptr + offsets sqr mask=offsets numel functools lru_cache get_autotuned_amd_sqr_kernel triton autotune configs= triton Config BLOCK_SIZE waves_per_eu key= amd_sqr_kernel instantiate_parametrized_tests TestTritonHeuristics TestCase device_type = GPU_TYPE test_triton_config Make sure block size does exceed maximum defined inductor config cfg = triton_config x y label XYZ key = f label BLOCK key cfg kwargs continue assertTrue cfg kwargs key = TRITON_MAX_BLOCK label _test_artificial_zgrid forward primals_ primals_ primals_ view = torch ops aten reshape default primals_ - primals_ = None permute = torch ops aten permute default view clone = torch ops aten clone default permute memory_format=torch contiguous_format permute = None view_ = torch ops aten reshape default clone - clone = None permute_ = torch ops aten permute default primals_ primals_ = None addmm = torch ops aten addmm default primals_ view_ permute_ primals_ = None addmm s = s = args = torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand s s device=GPU_TYPE torch _dynamo mark_dynamic args - foo_c = torch compile forward assertEqual forward args foo_c args args = torch rand device=GPU_TYPE torch rand device=GPU_TYPE torch rand s s device=GPU_TYPE assertEqual forward args foo_c args skipIfXpu test_artificial_zgrid _test_artificial_zgrid skipIfXpu config patch cpp_wrapper True test_artificial_grid_cpp_wrapper _test_artificial_zgrid staticmethod _get_cos_kernel_caching_autotuner_args triton jit triton_ in_ptr out_ptr xnumel XBLOCK tl constexpr xnumel = xoffset = tl program_id XBLOCK xindex = xoffset + tl arange XBLOCK xmask = xindex xnumel x = xindex tmp = tl load in_ptr + x xmask tmp = tl_math cos tmp tl store out_ptr + x tmp xmask triton_meta = signature in_ptr fp out_ptr fp xnumel i device DeviceProperties create torch device cuda constants configs AttrsDescriptorWrapper divisible_by_ = equal_to_ = configs = triton_config x triton_config x inductor_meta = fn triton_ triton_meta triton_meta configs configs save_cache_hook False mutated_arg_names reset_to_zero_arg_names optimize_mem True heuristic_type HeuristicType POINTWISE inductor_meta inductor_meta skipIfXpu test_pre_hook_assert assert any configs passed CachingAutotuner have pre-hooks args = _get_cos_kernel_caching_autotuner_args pre_hook kwargs in_ptr kwargs kwargs in_ptr zero_ cfg args configs cfg pre_hook = pre_hook assertRaisesRegex AssertionError pre_hook CachingAutotuner args test_autotune_hints_to_configs device_props = DeviceProperties create torch device GPU_TYPE device_props = device_props _replace warp_size= hints = AutotuneHint ONE_ELEMENT_PER_THREAD size_hints = block_size = seen_num_elements_per_warp = set mock_triton_config size_hints x y=None z=None num_stages=None num_elements_per_warp=None min_elem_per_thread=None seen_num_elements_per_warp add num_elements_per_warp None unittest mock patch torch _inductor runtime triton_heuristics triton_config mock_triton_config _ = autotune_hints_to_configs hints size_hints block_size device_props assertTrue seen_num_elements_per_warp unittest skipIf HAS_WARP_SPEC FBCODE Triton required test test_template_function_ws triton_meta = device MagicMock num_stages = num_warps = num_consumer_groups = num_buffers_warp_spec = patch torch _inductor runtime triton_heuristics cached_autotune mock_cached_autotune template num_stages=num_stages num_warps=num_warps triton_meta=triton_meta num_consumer_groups=num_consumer_groups num_buffers_warp_spec=num_buffers_warp_spec mock_cached_autotune assert_called_once configs = mock_cached_autotune call_args assertEqual configs num_consumer_groups num_consumer_groups assertEqual configs num_buffers_warp_spec num_buffers_warp_spec runOnRocm test_amd_special_config_args waves_per_eu example special config arg AMD explicitly specified config kwarg will exist kwargs function signature torch library triton_op test_triton_heuristics triton_sqr mutates_args= triton_sqr x torch Tensor - torch Tensor y = torch empty_like x grid meta triton cdiv x numel meta BLOCK_SIZE torch library wrap_triton get_autotuned_amd_sqr_kernel grid x y x numel fn x triton_sqr x x = torch randn device=GPU_TYPE ref = fn x res = torch compile fn x assertEqual ref res skipIfXpu skipIfRocm skipUnless HAS_CUDA_AND_TRITON requires CUDA parametrize do_pruning False True test_prune_configs_over_shared_memory_limit do_pruning torch _inductor template_heuristics triton CUDAConfigHeuristic GemmConfig expected_count = do_pruning mm_configs = GemmConfig GemmConfig intentionally large exceed shared memory limit config patch max_autotune_prune_choices_based_on_shared_mem do_pruning config_heuristic = CUDAConfigHeuristic config_heuristic should_scale_configs = False config_heuristic mm_configs = mm_configs configs = list config_heuristic get_mm_configs dtype_size= op_name= mm assertEqual len configs expected_count TestArgumentCloneAndRestore TestCase Our tensor large enough If unexpected copy happens peak memory increase should larger than tolerance test will fail MEM_TOLERANCE = int e _create_caching_autotuner args = TestTritonHeuristics _get_cos_kernel_caching_autotuner_args args optimize_mem = True args mutated_arg_names = in_ptr autotuner = CachingAutotuner args autotuner _create_tensor pad= with_offset=False Create GPU tensor about GB size M = N = out = rand_strided M N N + pad device=GPU_TYPE with_offset out = out out _do_test gpu_tensor torch cuda reset_peak_memory_stats autotuner = _create_caching_autotuner old_storage_offset = gpu_tensor storage_offset gpu_tensor_clone = clone_preserve_strides gpu_tensor peak_mem_before = torch cuda max_memory_allocated cpu_copies = autotuner copy_args_to_cpu_if_needed gpu_tensor assertTrue len cpu_copies == Mutate arg gpu_tensor add_ will restore gpu_tensor autotuner restore_args_from_cpu cpu_copies assertTrue gpu_tensor gpu_tensor_clone assertEqual gpu_tensor size gpu_tensor_clone size assertEqual gpu_tensor stride gpu_tensor_clone stride assertEqual gpu_tensor storage_offset old_storage_offset Note torch allclose somehow allocates large amount extra memory Record peak memory before peak_mem_after = torch cuda max_memory_allocated assertTrue torch allclose gpu_tensor gpu_tensor_clone assertTrue peak_mem_after = peak_mem_before + MEM_TOLERANCE f peak_mem_before= v s peak_mem_after= Avoid OOM CI assertTrue peak_mem_after e requires_cuda_with_enough_memory e test_clone_contiguous_args arg = _create_tensor pad= assertTrue arg is_contiguous assertTrue arg storage_offset == _do_test arg requires_cuda_with_enough_memory e test_clone_non_contiguous_args arg = _create_tensor pad= assertFalse arg is_contiguous assertTrue arg storage_offset == _do_test arg requires_cuda_with_enough_memory e test_clone_args_with_non_zero_offset arg = _create_tensor pad= with_offset=True assertFalse arg is_contiguous assertTrue arg storage_offset _do_test arg __name__ == __main__ IS_LINUX HAS_GPU run_tests