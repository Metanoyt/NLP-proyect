Owner s module inductor json tempfile unittest uuid io StringIO unittest mock patch torch torch nn functional F torch _inductor analysis profile_analysis _augment_trace_helper _create_extern_mapping main torch _inductor utils fresh_inductor_cache tabulate_ d zip_dicts torch testing _internal common_cuda SM OrLater torch testing _internal common_device_type dtypes instantiate_device_type_tests skipIf torch testing _internal common_utils parametrize run_tests TestCase torch testing _internal inductor_utils IS_BIG_GPU example_profile = schemaVersion deviceProperties id name NVIDIA H totalGlobalMem computeMajor computeMinor maxThreadsPerBlock maxThreadsPerMultiprocessor regsPerBlock warpSize sharedMemPerBlock numSms regsPerMultiprocessor sharedMemPerBlockOptin sharedMemPerMultiprocessor cupti_version cuda_runtime_version with_flops record_shapes cuda_driver_version profile_memory trace_id E ED FBD E E DC displayTimeUnit ms baseTimeNanoseconds traceEvents ph X cat cpu_op name aten convolution pid tid ts dur args External id Sequence number Fwd thread id Record function id Concrete Inputs \ False Input type float float \ ScalarList ScalarList ScalarList Scalar ScalarList Scalar Input Strides \ Input Dims \ Ev Idx ph X cat cpu_op name aten _convolution pid tid ts dur args External id Record function id Concrete Inputs \ False False False True True Input type float float ScalarList \ ScalarList ScalarList Scalar ScalarList Scalar Scalar Scalar Scalar Scalar Input Strides \ Input Dims \ Ev Idx ph X cat cpu_op name aten addmm pid tid ts dur args External id Sequence number Fwd thread id Record function id Concrete Inputs \ Input type float float float Scalar Scalar float Input Strides \ Input Dims \ Ev Idx ph X cat kernel name void cutlass_addmm pid tid ts dur args External id Sequence number Fwd thread id Record function id Ev Idx ph X cat kernel name void convolution_kernel pid tid ts dur args External id Sequence number Fwd thread id Record function id Ev Idx ph X cat cpu_op name aten convolution pid tid ts dur args External id Record function id Concrete Inputs \ False False False True True Input type float float ScalarList \ ScalarList ScalarList Scalar ScalarList Scalar Scalar Scalar Scalar Scalar Input \ Strides Input Dims \ Ev Idx traceName tmp compiled_module_profile json verify_flops expected_flops out_profile j = i range len out_profile traceEvents kernel_flop out_profile traceEvents i args assertEqual out_profile traceEvents i args kernel_flop expected_flops j j += random_tensor size dtype kwargs dtype torch half torch bfloat torch float torch double torch randn size dtype=dtype kwargs dtype torch uint torch int torch short torch int torch long torch randint size dtype=dtype kwargs raise ValueError Unsupported data type cT device dtype T shape requires_grad=False random_tensor shape requires_grad=requires_grad device=device dtype=dtype T FlopCounterMode args kwargs torch utils flop_counter FlopCounterMode args kwargs display=False TMP_DIR = tempfile mkdtemp trace_files TRACE = f TMP_DIR trace - uuid uuid json TRACE = f TMP_DIR trace - uuid uuid json TRACE TRACE _test_model device dtype compile=True addmm=True bmm=True T = cT device dtype model input_conv = T conv_weight = T Increased matrix sizes B = M = N = K = mat = T M N mat = T N K batch_mat = T B M N batch_mat = T B N K conv_output = F conv d input_conv conv_weight conv_output = conv_output mm_output = torch mm mat mat ret = conv_output flatten mm_output flatten addmm addmm_output = torch addmm torch zeros mm_output shape device=mat device dtype=mat dtype mat mat ret append addmm_output flatten bmm bmm_output = torch bmm batch_mat batch_mat ret append bmm_output flatten bmm addmm baddbmm_output = torch baddbmm torch zeros mm_output shape device=batch_mat device dtype=batch_mat dtype batch_mat batch_mat ret append baddbmm_output flatten torch cat ret compile torch compile model options= benchmark_kernel True profile_bandwidth True model _pointwise_test_model device dtype compile=True T = cT device dtype model M = N = mat = T M N mat = T M N pointwise_output = torch add mat mat sin pointwise_output compile torch compile model options= benchmark_kernel True profile_bandwidth True model prefix = profile py TestUtils TestCase test_tabulate d headers = Kernel Self H TIME ms Count Percent rows = aten mm aten bmm aten baddbmm aten convolution aten cudnn_convolution table = Kernel &#124; Self H TIME ms &#124; Count &#124; Percent ----------------------------------------------------------------- aten mm &#124; &#124; &#124; aten bmm &#124; &#124; &#124; aten baddbmm &#124; &#124; &#124; aten convolution &#124; &#124; &#124; aten cudnn_convolution &#124; &#124; &#124; res = tabulate_ d rows headers r t zip res split \n table assertEqual r t test_zip_dicts d = b d = c res = zip_dicts d d d _default= d _default= assertEqual set res b c res = zip_dicts d d assertEqual set res b None c None TestAnalysis TestCase skipIf SM OrLater Requires SM test_noop patch sys stdout new_callable=StringIO mock_stdout patch sys argv prefix main assertEqual mock_stdout getvalue skipIf SM OrLater Requires SM dtypes torch float torch double torch float test_diff device dtype diff testing out nruns feature too device == cpu torch version hip None TODO cpu support om = _test_model device dtype REPEAT = trace trace = trace_files print f first trace trace torch _dynamo reset reset cache fresh_inductor_cache torch profiler profile record_shapes=True p om p export_chrome_trace trace print f second trace trace torch _dynamo reset reset cache fresh_inductor_cache torch profiler profile record_shapes=True p _ range REPEAT om p export_chrome_trace trace print diffing patch sys argv prefix -- diff trace foo trace bar str dtype split - -- name_limit main skipIf SM OrLater Requires SM test_augment_trace_helper_unit js = json loads example_profile out_profile = _augment_trace_helper js expected_flops = verify_flops expected_flops out_profile skipIf SM OrLater Requires SM dtypes torch float torch double torch float parametrize maxat True TRITON skipIf IS_BIG_GPU we can t use Triton only backend max autotune torch _inductor config patch force_disable_caches=True test_triton_has_metadata device dtype maxat make sure chrome trace triton kernels contains certain values device == cpu torch version hip None T = cT device dtype input_conv = T conv_weight = T om i w Convolution operation conv_output = F conv d i w conv_output max_autotune backends = maxat comp_omni = torch compile om options= benchmark_kernel True max_autotune_gemm_backends backends max_autotune max_autotune verify_triton comp torch _dynamo reset reset cache fresh_inductor_cache torch profiler profile record_shapes=True profile comp input_conv conv_weight trace _ = trace_files profile export_chrome_trace trace open trace f out_profile = json load f seen = False event out_profile traceEvents triton event name conv event name seen = True assertTrue seen no triton conv found verify_triton comp_omni skipIf SM OrLater Requires SM dtypes torch float torch float parametrize maxat False ATEN TRITON True ATEN TRITON True ATEN True TRITON unittest skipIf IS_BIG_GPU we can t use Triton only backend max autotune torch _inductor config patch force_disable_caches=True test_augment_trace_against_flop_counter device dtype maxat tests see we can only use Triton backend max autotune max_autotune backends = maxat device == cpu torch version hip None om = _test_model device dtype compile=False comp_omni = torch compile om options= benchmark_kernel True max_autotune_gemm_backends backends max_autotune max_autotune comp_omni torch _dynamo reset reset cache fresh_inductor_cache torch profiler profile record_shapes=True profile comp_omni torch _dynamo reset reset cache fresh_inductor_cache FlopCounterMode mode comp_omni trace trace = trace_files profile export_chrome_trace trace patch sys argv prefix -- augment_trace trace trace str dtype split - main open trace f out_profile = json load f flop_counts = mode flop_counts extern_mapping = _create_extern_mapping out_profile seen_mm = False seen_bmm = False seen_baddbmm = False seen_conv = False event out_profile traceEvents cat event event cat = kernel args event External id event args continue external_op = extern_mapping event args External id name str = external_op name assertNotEqual name None assertEqual type name str name startswith aten mm _mm_ name seen_mm = True assertEqual event args kernel_flop flop_counts Global torch ops aten mm name startswith aten cudnn_convolution aten convolution aten _convolution conv name seen_conv = True assertEqual event args kernel_flop flop_counts Global torch ops aten convolution name startswith aten baddbmm _baddbmm_ name seen_baddbmm = True assertEqual event args kernel_flop flop_counts Global torch ops aten baddbmm name startswith aten bmm _bmm_ name seen_bmm = True assertEqual event args kernel_flop flop_counts Global torch ops aten bmm assertTrue seen_mm assertTrue seen_bmm assertTrue seen_baddbmm assertTrue seen_conv skipIf SM OrLater Requires SM dtypes torch float torch float parametrize maxat False ATEN TRITON True ATEN TRITON True ATEN True TRITON unittest skipIf IS_BIG_GPU we can t use Triton only backend max autotune torch _inductor config patch force_disable_caches=True test_pointwise_bandwidth device dtype maxat tests see we can only use Triton backend max autotune max_autotune backends = maxat device == cpu torch version hip None om = _pointwise_test_model device dtype compile=False comp_omni = torch compile om options= benchmark_kernel True max_autotune_gemm_backends backends max_autotune max_autotune comp_omni fresh_inductor_cache torch profiler profile record_shapes=True profile comp_omni trace _ = trace_files profile export_chrome_trace trace patch sys argv prefix -- analysis trace str dtype split - main open trace f out_profile = json load f event out_profile traceEvents event name == triton_poi_fused_add_randn_sin_ event args kernel_num_gb = skipIf SM OrLater Requires SM dtypes torch float torch float test_combine_profiles device dtype Test combining multiple profiles into single profile device == cpu torch version hip None Create three different models generate different traces om = _test_model device dtype addmm=True bmm=False om = _test_model device dtype addmm=False bmm=True om = _pointwise_test_model device dtype Generate three separate traces trace trace = trace_files trace = f TMP_DIR trace - uuid uuid json combined_trace = f TMP_DIR combined- uuid uuid json Generate first trace torch _dynamo reset fresh_inductor_cache torch profiler profile record_shapes=True p om p export_chrome_trace trace Generate second trace torch _dynamo reset fresh_inductor_cache torch profiler profile record_shapes=True p om p export_chrome_trace trace Generate third trace torch _dynamo reset fresh_inductor_cache torch profiler profile record_shapes=True p om p export_chrome_trace trace Combine three traces patch sys argv prefix -- combine trace trace trace combined_trace main Verify combined trace exists contains expected data open combined_trace f combined_profile = json load f Load original traces comparison open trace f profile = json load f open trace f profile = json load f open trace f profile = json load f Verify trace events combined expected_event_count = len profile traceEvents + len profile traceEvents + len profile traceEvents assertEqual len combined_profile traceEvents expected_event_count Verify device properties present assertIn deviceProperties combined_profile assertGreater len combined_profile deviceProperties Verify some trace events each original profile present combined_event_names = event name event combined_profile traceEvents Check we have events each original profile profile _event_names = event name event profile traceEvents profile _event_names = event name event profile traceEvents profile _event_names = event name event profile traceEvents At least some events each profile should combined profile assertTrue profile _event_names intersection combined_event_names assertTrue profile _event_names intersection combined_event_names assertTrue profile _event_names intersection combined_event_names instantiate_device_type_tests TestAnalysis globals __name__ == __main__ run_tests