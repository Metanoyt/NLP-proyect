mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates The following example demonstrates how use Pytorch Distributed Checkpoint save FSDP model This current recommended way checkpoint FSDP torch save torch load recommended when checkpointing sharded models os shutil torch torch distributed dist torch distributed checkpoint dist_cp torch multiprocessing mp torch distributed checkpoint optimizer load_sharded_optimizer_state_dict torch distributed fsdp FullyShardedDataParallel FSDP torch distributed fsdp fully_sharded_data_parallel StateDictType CHECKPOINT_DIR = f scratch os environ get LOGNAME checkpoint opt_at opt idx list opt state values idx init_model model = FSDP torch nn Linear cuda dist get_rank optim = torch optim Adam model parameters lr= model torch rand sum backward optim step model optim print_params stage model_ model_ optim_ optim_ FSDP summon_full_params model_ FSDP summon_full_params model_ print f stage --- rank dist get_rank \n f model weight model_ weight \n f model_ weight model_ weight \n f model bias model_ bias \n f model_ bias model_ bias \n print f stage --- rank dist get_rank \n f optim exp_avg opt_at optim_ exp_avg \n f optim_ exp_avg opt_at optim_ exp_avg \n f optim exp_avg_sq opt_at optim_ exp_avg_sq \n f optim_ exp_avg_sq opt_at optim_ exp_avg_sq \n run_fsdp_checkpoint_example rank world_size Set up world pg os environ MASTER_ADDR = localhost os environ MASTER_PORT = Initialize process group dist init_process_group cpu gloo cuda nccl rank=rank world_size=world_size torch cuda set_device rank Create model model_ optim_ = init_model Save model CHECKPOINT_DIR FSDP state_dict_type model_ StateDictType SHARDED_STATE_DICT state_dict = model model_ state_dict optim FSDP optim_state_dict model_ optim_ dist_cp save_state_dict state_dict=state_dict storage_writer=dist_cp FileSystemWriter CHECKPOINT_DIR Create second model model_ optim_ = init_model Print model parameters both models Before loading parameters should different print_params Before loading model_ model_ optim_ optim_ Load model_ parameters saved CHECKPOINT_DIR FSDP state_dict_type model_ StateDictType SHARDED_STATE_DICT state_dict = model model_ state_dict cannot load optimizer state_dict together model state_dict dist_cp load_state_dict state_dict=state_dict storage_reader=dist_cp FileSystemReader CHECKPOINT_DIR model_ load_state_dict state_dict model optim_state = load_sharded_optimizer_state_dict model_state_dict=state_dict model optimizer_key= optim storage_reader=dist_cp FileSystemReader CHECKPOINT_DIR flattened_osd = FSDP optim_state_dict_to_load model_ optim_ optim_state optim optim_ load_state_dict flattened_osd Print model parameters both models After loading parameters should same print_params After loading model_ model_ optim_ optim_ Shut down world pg dist destroy_process_group __name__ == __main__ world_size = torch cuda device_count print f Running fsdp checkpoint example world_size devices shutil rmtree CHECKPOINT_DIR ignore_errors=True mp spawn run_fsdp_checkpoint_example args= world_size nprocs=world_size join=True