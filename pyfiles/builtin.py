mypy allow-untyped-defs Built-in function type variable tracking TorchDynamo s symbolic execution This module contains variable tracker classes Python built-in functions types operations during graph compilation It handles symbolic execution - Built-in functions len getattr isinstance etc - Type constructors int float str list dict etc - Built-in operators methods - Special Python constructs super hasattr etc Key classes - BuiltinVariable Tracks built-in functions handles their execution - TypeVariable Manages type constructor calls type checking - SuperVariable Handles super calls hierarchies These variable trackers ensure built-in Python operations correctly handled during symbolic execution either executing them directly when safe creating appropriate graph nodes when needed contextlib functools inspect itertools logging math operator types typing unittest collections defaultdict OrderedDict collections abc Callable Iterable KeysView Sequence typing Any TYPE_CHECKING Union torch torch sym_float sym_int torch _subclasses meta_utils is_sparse_any torch overrides BaseTorchFunctionMode torch utils _python_dispatch is_traceable_wrapper_subclass config graph_break_hints polyfills variables exc AttributeMutationError ObservedAttributeError ObservedUserStopIteration raise_observed_exception unimplemented_v Unsupported UserError UserErrorType guards GuardBuilder install_guard replay_record DummyModule source AttrSource GetItemSource GlobalSource is_constant_source TypeSource utils check_constant_args check_numpy_ndarray_args check_unspec_or_constant_args check_unspec_python_args cmp_name_to_op_mapping dict_methods extract_fake_example_value frozenset_methods get_fake_value guard_if_dyn is_tensor_getset_descriptor is_wrapper_or_member_descriptor istype numpy_operator_wrapper proxy_args_kwargs raise_args_mismatch set_methods str_methods tensortype_to_dtype base AsPythonConstantNotImplementedError ValueMutationNew VariableTracker constant ConstantVariable dicts ConstDictVariable DefaultDictVariable DictKeysVariable DictViewVariable FrozensetVariable is_hashable SetVariable lists BaseListVariable ListIteratorVariable ListVariable SizeVariable TupleIteratorVariable TupleVariable streams EventVariable StreamVariable tensor FakeItemVariable supported_comparison_ops SymNodeVariable TensorVariable UnspecializedPythonVariable user_defined MutableMappingVariable UserDefinedDictVariable UserDefinedObjectVariable UserDefinedVariable TYPE_CHECKING Cyclic dependency torch _dynamo codegen PyCodegen torch _dynamo symbolic_convert InstructionTranslator log = logging getLogger __name__ IN_PLACE_DESUGARING_MAP = operator iadd operator add operator isub operator sub operator imul operator mul operator ifloordiv operator floordiv operator itruediv operator truediv operator imod operator mod operator imatmul operator imatmul operator ilshift operator lshift operator irshift operator rshift operator ipow operator pow operator iand operator and_ operator ior operator or_ operator ixor operator xor _HandlerCallback = Callable InstructionTranslator typing Any typing Any VariableTracker _TrackersType = Union type VariableTracker tuple type VariableTracker polyfill_fn_mapping = operator eq polyfills cmp_eq operator ne polyfills cmp_ne operator lt polyfills cmp_lt operator le polyfills cmp_le operator gt polyfills cmp_gt operator ge polyfills cmp_ge bin_ops = operator pow operator mul operator matmul operator floordiv operator truediv operator mod operator add operator lt operator gt operator ge operator le operator ne operator eq operator sub operator ipow operator imul operator imatmul operator ifloordiv operator itruediv operator imod operator iadd operator isub bin_int_ops = operator and_ operator or_ operator xor operator iand operator ixor operator ior un_int_ops = operator invert tensor_and_int_ops = operator lshift operator rshift operator ilshift operator irshift operator getitem un_ops = operator abs operator pos operator neg operator not_ Note has local scalar dense call operator length_hint BUILTIN_TO_TENSOR_FN_MAP dict Callable Any Callable Any = These functions represent r versions above ops Basically __add__ Tensor called translated __radd__ Tensor In builtin var we check there tensor first args position we swap args use r version op BUILTIN_TO_TENSOR_RFN_MAP dict Callable Any Callable Any = populate_builtin_to_tensor_fn_map global BUILTIN_TO_TENSOR_FN_MAP len BUILTIN_TO_TENSOR_FN_MAP Only populate once after there elements present no need repopulate most_recent_func = None GetMethodMode BaseTorchFunctionMode Mode extract correct methods torch function invocations Used get correct torch Tensor methods builtins __torch_function__ func types args= kwargs=None kwargs = kwargs nonlocal most_recent_func most_recent_func = func func args kwargs inp = torch ones inp = torch ones inp _int = torch ones dtype=torch int inp _int = torch ones dtype=torch int GetMethodMode setups_and_oplists list tuple Callable Any Iterable Any = lambda o o inp un_ops lambda o o inp _int un_int_ops lambda o o inp inp bin_ops lambda o o inp _int inp _int bin_int_ops lambda o o inp _int tensor_and_int_ops setup_fn op_list setups_and_oplists op op_list setup_fn op assert most_recent_func None BUILTIN_TO_TENSOR_FN_MAP op = most_recent_func gather reverse functions rsetups_and_oplists list tuple Callable Any Iterable Any = lambda o o inp bin_ops Get r ops ex __sub__ int Tensor - __rsub__ Tensor int lambda o o inp _int bin_int_ops lambda o o inp _int tensor_and_int_ops rskips = operator matmul operator imatmul operator getitem setup_fn op_list rsetups_and_oplists op op_list op rskips continue setup_fn op assert most_recent_func None most_recent_func = BUILTIN_TO_TENSOR_FN_MAP op BUILTIN_TO_TENSOR_RFN_MAP op = most_recent_func BuiltinVariable VariableTracker A VariableTracker represents built-in value functions operators A lot code here assumes will function object The BuiltinVariable wraps Python built-in functions like len isinstance etc operators like + - etc enable symbolic execution during tracing This allows Dynamo properly handle these operations when converting Python code FX graphs while maintaining correct semantics enabling optimizations _SENTINEL = object _nonvar_fields = fn VariableTracker _nonvar_fields classmethod create_with_source cls value source install_guard source make_guard GuardBuilder BUILTIN_MATCH cls value source=source staticmethod functools cache _constant_fold_functions fns = abs all any bool callable chr complex divmod float getattr int len max min ord pow repr round str str format sum type operator abs operator pos operator neg operator not_ operator truth operator invert operator pow operator mul operator matmul operator floordiv operator truediv operator mod operator add operator sub operator getitem operator length_hint operator lshift operator rshift operator and_ operator or_ operator xor operator ipow operator imul operator imatmul operator ifloordiv operator itruediv operator imod operator iadd operator isub operator ilshift operator irshift operator iand operator ixor operator ior operator index tensor supported_comparison_ops fns update supported_comparison_ops values fns update x x math __dict__ values isinstance x type math sqrt fns can_constant_fold_through fn _constant_fold_functions staticmethod functools cache _fx_graph_functions fns = operator abs operator pos operator neg operator not_ operator invert operator pow operator mul operator matmul operator floordiv operator truediv operator mod operator add operator lt operator gt operator ge operator le operator ne operator eq operator sub operator length_hint operator lshift operator rshift operator and_ operator or_ operator xor operator ipow operator imul operator imatmul operator ifloordiv operator itruediv operator getitem operator imod operator iadd operator isub operator ilshift operator irshift operator iand operator ixor operator ior fns staticmethod functools cache _binops - dict Callable object tuple list str Callable object function - forward name reverse name in-place name in-place op fns dict Callable object tuple list str Callable object = operator add __add__ __radd__ __iadd__ operator iadd operator sub __sub__ __rsub__ __isub__ operator isub operator mul __mul__ __rmul__ __imul__ operator imul operator truediv __truediv__ __rtruediv__ __itruediv__ operator itruediv operator floordiv __floordiv__ __rfloordiv__ __ifloordiv__ operator ifloordiv operator mod __mod__ __rmod__ __imod__ operator imod pow __pow__ __rpow__ __ipow__ operator ipow operator pow __pow__ __rpow__ __ipow__ operator ipow operator lshift __lshift__ __rlshift__ __ilshift__ operator ilshift operator rshift __rshift__ __rrshift__ __irshift__ operator irshift NB The follow binary operators supported now since corresponding magic methods aren t defined SymInt SymFloat operator matmul divmod operator and_ operator or_ operator xor fns staticmethod functools cache _binop_handlers Multiple dispatch mechanism defining custom binop behavior certain type combinations Handlers attempted order will used type checks match They expected have signature fn tx arg VariableTracker arg VariableTracker - VariableTracker functions BaseUserFunctionVariable UserFunctionVariable nn_module NNModuleVariable tensor supported_const_comparison_ops torch BaseTorchVariable user_defined UserDefinedClassVariable UserDefinedObjectVariable UserDefinedVariable Override table contains op_fn - list handlers op_handlers dict Callable object list tuple tuple type VariableTracker _TrackersType _HandlerCallback = op magic_method_names in_place_op BuiltinVariable _binops items op_handlers op = op_handlers in_place_op = forward_name reverse_name inplace_name = magic_method_names User-defined args highest precedence user_defined_handler tx b forward_name=forward_name reverse_name=reverse_name Manually handle reversing logic needed e g call __radd__ TODO If we expand handle tensor args we need manually handle cases like A int __radd__ other print woof torch randn + A In example A __radd__ called - nothing printed because Tensor __add__ only does subtype test against int ignoring subclass To fully correct we should call A __radd__ here there may other cases reason about add exceptions isinstance UserDefinedVariable call_method tx forward_name b b call_method tx reverse_name op_handlers op append UserDefinedVariable VariableTracker user_defined_handler op_handlers op append VariableTracker UserDefinedVariable user_defined_handler user_defined_inplace_handler tx InstructionTranslator b forward_name=inplace_name call_method tx forward_name b op_handlers in_place_op append UserDefinedVariable VariableTracker user_defined_inplace_handler op_handlers in_place_op append VariableTracker UserDefinedVariable user_defined_inplace_handler Dynamic shape args dynamic_handler tx InstructionTranslator b fn=op builder wrap_fx_proxy wrap_fx_proxy tx tx output create_proxy call_function fn proxy_args_kwargs b op_handlers op append SymNodeVariable VariableTracker dynamic_handler op_handlers op append VariableTracker SymNodeVariable dynamic_handler NB Prefer out-of-place op when calling in-place op generate valid graph op_handlers in_place_op append SymNodeVariable VariableTracker dynamic_handler op_handlers in_place_op append VariableTracker SymNodeVariable dynamic_handler Special cases - lower precedence still prefer these over constant folding List-like addition e g + tuple_add_handler tx InstructionTranslator b TupleVariable items b unpack_var_sequence tx size_add_handler tx InstructionTranslator b SizeVariable items b unpack_var_sequence tx list_like_addition_handlers list tuple tuple type VariableTracker _TrackersType _HandlerCallback = NB Prefer tuple-specific logic over base logic because some SizeVariable weirdness Specifically tuple-specific logic drops subclass type e g SizeVariable returns TupleVariables SizeVariable SizeVariable size_add_handler SizeVariable TupleVariable size_add_handler TupleVariable SizeVariable size_add_handler TupleVariable TupleVariable tuple_add_handler TupleVariable ConstantVariable tuple_add_handler ConstantVariable TupleVariable lambda tx b TupleVariable unpack_var_sequence tx b items ListVariable BaseListVariable ConstantVariable ListIteratorVariable lambda tx b ListVariable items b unpack_var_sequence tx mutation_type=ValueMutationNew BaseListVariable BaseListVariable lambda tx b type items b items op_handlers operator add extend list_like_addition_handlers list_iadd_handler tx InstructionTranslator b is_immutable b has_unpack_var_sequence tx Handler doesn t apply None seq = b unpack_var_sequence tx tx output side_effects mutation items extend seq list_like_iadd_handlers list tuple tuple type VariableTracker type VariableTracker _HandlerCallback = ListVariable VariableTracker list_iadd_handler TupleVariable TupleVariable tuple_add_handler TupleVariable ConstantVariable tuple_add_handler op_handlers operator iadd extend list_like_iadd_handlers List-like expansion e g expand_list_like tx InstructionTranslator lst const isinstance lst ConstantVariable lst const = const lst try lst __class__ items=lst items const as_python_constant mutation_type=ValueMutationNew except MemoryError exc raise_observed_exception type exc tx args=list map ConstantVariable create exc args list_like_expansion_handlers list tuple tuple type VariableTracker type VariableTracker _HandlerCallback = ListVariable ConstantVariable expand_list_like TupleVariable ConstantVariable expand_list_like ConstantVariable ListVariable expand_list_like ConstantVariable TupleVariable expand_list_like op_handlers operator mul extend list_like_expansion_handlers create_cmp_op_handlers op compare_by_value tx InstructionTranslator b try ConstantVariable op value b value except TypeError exc raise_observed_exception type exc tx args=list map ConstantVariable create exc args result list tuple tuple _TrackersType _TrackersType _HandlerCallback = ConstantVariable ConstantVariable compare_by_value op polyfill_fn_mapping For constants speedup comparison instead using polyfill Removing line causes major regression pr time benchmark - add_loop_eager result = ConstantVariable ConstantVariable compare_by_value op_var = BuiltinVariable op Special handling SymNode variable result extend SymNodeVariable VariableTracker op_var _comparison_with_symnode VariableTracker SymNodeVariable op_var _comparison_with_symnode handler tx b tx inline_user_function_return VariableTracker build tx polyfill_fn_mapping op b result append VariableTracker VariableTracker handler result result = ConstantVariable ConstantVariable compare_by_value op supported_const_comparison_ops values op __name__ startswith is_ Tensor None List None etc none_result = op object None never tx InstructionTranslator b ConstantVariable none_result obj_op_none = never none_op_obj = never types_that_are_never_none = TensorVariable SymNodeVariable NNModuleVariable BaseListVariable UserDefinedVariable BaseUserFunctionVariable ConstDictVariable BaseTorchVariable result extend types_that_are_never_none ConstantVariable obj_op_none ConstantVariable types_that_are_never_none none_op_obj op_var = BuiltinVariable op result extend UserFunctionVariable BuiltinVariable UserFunctionVariable BuiltinVariable lambda tx b ConstantVariable op fn b fn NNModuleVariable NNModuleVariable lambda tx b ConstantVariable op tx output get_submodule module_key tx output get_submodule b module_key UserDefinedObjectVariable UserDefinedObjectVariable compare_by_value UserDefinedClassVariable UserDefinedClassVariable compare_by_value StreamVariable EventVariable ConstantVariable StreamVariable EventVariable ConstantVariable compare_by_value TensorVariable VariableTracker op_var _comparison_with_tensor VariableTracker TensorVariable op_var _comparison_with_tensor SymNodeVariable VariableTracker op_var _comparison_with_symnode VariableTracker SymNodeVariable op_var _comparison_with_symnode handle_is tx InstructionTranslator left right If two objects different type we can safely False True ` ` ` ` respectively type left type right ConstantVariable create op __name__ = is_ left right ConstantVariable create op left right istype left variables ExceptionVariable istype right variables ExceptionVariable left exc_type right exc_type ConstantVariable create op left right result append VariableTracker VariableTracker handle_is result op supported_comparison_ops values assert callable op assert op op_handlers op_handlers op = create_cmp_op_handlers op op_handlers staticmethod _find_binop_handler op a_type b_type handlers = BuiltinVariable _binop_handlers get op handlers None None matches = type type handler handlers issubclass a_type type issubclass b_type type matches append handler matches can_insert_in_graph fn _fx_graph_functions __init__ fn kwargs - None super __init__ kwargs fn = fn __repr__ - str fn None name = None name = fn __name__ f __class__ __name__ name as_python_constant fn as_proxy DTYPE = bool torch bool int torch int float torch float fn DTYPE DTYPE fn super as_proxy reconstruct codegen PyCodegen name = fn __name__ assert fn __module__ == builtins assert name codegen tx f_globals shadowed global codegen append_output codegen create_load_global name add=True constant_args args kwargs check_constant_args args kwargs tensor_args args any_tensor = False arg args isinstance arg variables GetAttrVariable False any_tensor = any_tensor isinstance arg variables TensorVariable any_tensor tensor_args_type arg_types any_tensor = False arg_type arg_types issubclass arg_type variables GetAttrVariable False any_tensor = any_tensor issubclass arg_type variables TensorVariable any_tensor python_and_tensor_constant_only args kwargs tensor_args = non_tensor_args = i itertools chain args kwargs values isinstance i variables TensorVariable tensor_args append i non_tensor_args append i all is_constant_source t source t source None False t tensor_args constant_args non_tensor_args staticmethod unwrap_unspec_args_kwargs args kwargs x as_python_constant x args k v as_python_constant k v kwargs items has_constant_handler args kwargs can_constant_fold_through check_unspec_or_constant_args args kwargs staticmethod _make_handler fn arg_types list type has_kwargs bool lazy LazyVariableTracker obj = BuiltinVariable fn handlers list _HandlerCallback = any issubclass t LazyVariableTracker t arg_types lambda tx args kwargs obj call_function tx v realize v args kwargs inspect isclass fn issubclass fn Exception GeneratorExit doesn t inherit Exception issubclass GeneratorExit Exception False fn GeneratorExit create_exception_class_object tx InstructionTranslator args kwargs fn AssertionError all isinstance x variables ConstantVariable isinstance x value str x args unimplemented_v gb_type= assert non-string message context=str args explanation= Dynamo only supports asserts string messages hints= graph_break_hints SUPPORTABLE variables ExceptionVariable fn args kwargs create_exception_class_object obj can_insert_in_graph fn operator getitem issubclass arg_types variables TensorVariable obj tensor_args_type arg_types obj _handle_insert_op_in_graph has_kwargs need runtime check kwargs handlers append obj _handle_insert_op_in_graph Handle binary ops e g __add__ __radd__ __iadd__ etc NB Tensor args handled above here len arg_types == has_kwargs Try find handler arg types otherwise fall through constant handler binop_handlers = BuiltinVariable _find_binop_handler fn arg_types binop_handlers pass len binop_handlers == binop_handler = binop_handlers handlers append lambda tx args _ binop_handler tx args call_binop_handlers tx InstructionTranslator args _ fn binop_handlers rv = fn tx args rv rv handlers append call_binop_handlers self_handler = getattr obj f call_ fn __name__ None self_handler call_self_handler tx InstructionTranslator args kwargs try pyrefly ignore not-callable result = self_handler tx args kwargs result None result except TypeError Check binding bad inspect signature bind expensive So check only when handler call fails try pyrefly ignore bad-argument-type inspect signature self_handler bind tx args kwargs except TypeError e has_constant_handler = obj has_constant_handler args kwargs has_constant_handler log warning noqa G incorrect arg count s s no constant handler self_handler e unimplemented_v gb_type= invalid call builtin op handler context=f invalid args self_handler args kwargs explanation=f Encountered TypeError when trying handle op fn __name__ hints= graph_break_hints DIFFICULT raise except Unsupported exc has_constant_handler = obj has_constant_handler args kwargs has_constant_handler raise Actually we will handle just fine exc remove_from_stats handlers append call_self_handler obj can_constant_fold_through all issubclass x ConstantVariable x arg_types has_kwargs constant_fold_handler tx InstructionTranslator args kwargs fast path try res = fn x as_python_constant x args except Exception exc raise_observed_exception type exc tx args=list map ConstantVariable create exc args except AsPythonConstantNotImplementedError exc unimplemented_v gb_type= constant fold exception context=f attempted run function fn arguments args explanation= Encountered exception when attempting constant fold hints= graph_break_hints DYNAMO_BUG from_exc=exc pyrefly ignore unbound-name VariableTracker build tx res constant_fold_handler tx InstructionTranslator args kwargs path runtime check check_unspec_or_constant_args args kwargs try res = fn x as_python_constant x args k v as_python_constant k v kwargs items except AsPythonConstantNotImplementedError exc unimplemented_v gb_type= constant fold exception context=f attempted run function fn arguments args explanation= Encountered exception when attempting constant fold hints= graph_break_hints DYNAMO_BUG from_exc=exc except Exception exc raise_observed_exception type exc tx args=list map ConstantVariable create exc args pyrefly ignore unbound-name VariableTracker build tx res handlers append constant_fold_handler call_unimplemented_v args real_arg_types = arg python_type_name arg args unimplemented_v gb_type= Failed trace builtin operator context=f builtin fn __name__ arg_types has_kwargs explanation=f Dynamo does know how trace builtin operator ` fn __name__ ` f argument types real_arg_types has_kwargs has_kwargs hints= f Avoid calling builtin ` fn __name__ ` argument types real_arg_types f Consider using equivalent alternative function method ` fn __name__ ` If you attempting call logging function e g ` print ` you can try adding ` torch _dynamo config reorderable_logging_functions ` Please report issue PyTorch len handlers == lambda tx args kwargs call_unimplemented_v args len handlers == handler = handlers builtin_dispatch tx InstructionTranslator args kwargs rv = handler tx args kwargs rv rv call_unimplemented_v args builtin_dispatch tx InstructionTranslator args kwargs fn handlers rv = fn tx args kwargs rv rv call_unimplemented_v args builtin_dispatch call_vars tx InstructionTranslator args len args == unimplemented_v gb_type= unimplemented builtin op vars no arguments context=f vars args explanation=f Dynamo does know how trace builtin operator fn no arguments hints= graph_break_hints SUPPORTABLE assert len args == vars obj obj __dict__ __dict__ present TypeError try args var_getattr tx __dict__ except ObservedAttributeError raise_observed_exception TypeError tx _handle_insert_op_in_graph tx InstructionTranslator args kwargs builder wrap_fx_proxy wrap_fx_proxy_cls kwargs tensor_args args kwargs values insert handling torch function here builder SourcelessBuilder torch_function can_dispatch_torch_function dispatch_torch_function global BUILTIN_TO_TENSOR_RFN_MAP BUILTIN_TO_TENSOR_FN_MAP can_dispatch_torch_function tx args kwargs Only remap fn tensor methods we aren t exporting export serde does handle method descriptors today tx export Ensure builtin maps populated before accessing them populate_builtin_to_tensor_fn_map Use sourceless builder we built map ourselves isinstance args TensorVariable fn BUILTIN_TO_TENSOR_RFN_MAP func = BUILTIN_TO_TENSOR_RFN_MAP fn func = BUILTIN_TO_TENSOR_FN_MAP fn tmp = args swap args call reverse version func args = args args = tmp func = BUILTIN_TO_TENSOR_FN_MAP fn func = fn fn_var = SourcelessBuilder create tx func dispatch_torch_function tx fn_var args kwargs fn = fn try Constant fold constant tensor python constants python_and_tensor_constant_only args kwargs bytecode_transformation unique_id functions invoke_and_store_as_constant invoke_and_store_as_constant tx fn unique_id fn __name__ args kwargs fn IN_PLACE_DESUGARING_MAP isinstance args variables ConstantVariable In-place operators like += usually mustate tensor values edge case immutable values they re-bind variable The easiest way keep graph consistent scenario de-sugar eagerly fn args = IN_PLACE_DESUGARING_MAP fn args args fn operator getitem isinstance args SymNodeVariable Standard indexing will force specialization due __index__ Rewrite regular torch op which will trace fine fn args = torch select args variables ConstantVariable create args Interaction between ndarray tensors We prefer tensor op whenever there tensors involved check_numpy_ndarray_args args kwargs any type arg variables TensorVariable arg args proxy = tx output create_proxy call_function numpy_operator_wrapper fn proxy_args_kwargs args kwargs wrap_fx_proxy_cls variables NumpyNdarrayVariable tx proxy fn operator eq len args == isinstance args variables TensorVariable Dynamo expects ` __eq__ ` str while operator eq gives just ` eq ` TODO - supporting all comparison operators could also work fails lots tests because graph str changes args call_method tx __eq__ args kwargs proxy = tx output create_proxy call_function fn proxy_args_kwargs args kwargs any isinstance arg FakeItemVariable arg args wrap_fx_proxy_cls FakeItemVariable tx proxy check_unspec_python_args args kwargs _args _kwargs = unwrap_unspec_args_kwargs args kwargs raw_value = fn _args _kwargs need_unwrap = any x need_unwrap x itertools chain args kwargs values isinstance x variables UnspecializedPythonVariable wrap_fx_proxy_cls UnspecializedPythonVariable tx proxy raw_value=raw_value need_unwrap=need_unwrap all isinstance x SymNodeVariable x args SymNodeVariable create tx proxy None Work around vision_maskrcnn due precision difference specialize dividend when float divide tensor fn operator truediv isinstance args variables UnspecializedPythonVariable args = args as_python_constant wrap_fx_proxy tx proxy except NotImplementedError unimplemented_v gb_type= unimplemented builtin op tensor arguments context=f partial tensor op args kwargs explanation=f Dynamo does know how trace builtin operator fn tensor arguments hints= graph_break_hints SUPPORTABLE call_function_handler_cache dict tuple object Callable InstructionTranslator Sequence VariableTracker dict str VariableTracker VariableTracker = call_function tx InstructionTranslator args Sequence VariableTracker kwargs dict str VariableTracker - VariableTracker key tuple object kwargs kwargs = k v realize k v kwargs items key = fn type x x args True key = fn type x x args handler = call_function_handler_cache get key handler call_function_handler_cache key = handler = _make_handler fn type x x args bool kwargs handler tx args kwargs call_method tx name args list VariableTracker kwargs dict str VariableTracker - VariableTracker fn object name == __setattr__ assert len args == assert len kwargs == obj name_var val = args obj = obj realize isinstance obj UserDefinedObjectVariable tx output side_effects is_attribute_mutation obj name_var is_python_constant obj method_setattr_standard tx name_var val name == __new__ Supported __new__ methods fn object len args == assert len kwargs == tx output side_effects track_new_user_defined_object args args fn dict len args == kwargs dict_vt = ConstDictVariable dict mutation_type=ValueMutationNew isinstance args BuiltinVariable args fn dict dict_vt We don t have set underlying dict_vt UserDefinedDictVariable because will set empty ConstDictVariableTracker constructor tx output side_effects track_new_user_defined_object args args fn tuple len args == args has_force_unpack_var_sequence tx kwargs isinstance args BuiltinVariable args fn tuple init_args = args force_unpack_var_sequence tx variables TupleVariable init_args mutation_type=ValueMutationNew tx output side_effects track_new_user_defined_object args args fn list list_vt = ListVariable mutation_type=ValueMutationNew isinstance args BuiltinVariable args fn list list_vt tx output side_effects track_new_user_defined_object args args fn float len args == name fromhex hex isinstance args ConstantVariable try fn = getattr float name res = fn args as_python_constant variables ConstantVariable create res except OverflowError ValueError e raise_observed_exception type e tx args=list map ConstantVariable create e args fn object name == __init__ object __init__ no-op variables ConstantVariable None fn dict name == fromkeys BuiltinVariable call_custom_dict_fromkeys tx dict args kwargs fn dict resolved_fn = getattr fn name resolved_fn dict_methods isinstance args variables UserDefinedDictVariable pyrefly ignore missing-attribute args _dict_vt call_method tx name args kwargs isinstance args variables ConstDictVariable args call_method tx name args kwargs fn set resolved_fn = getattr fn name resolved_fn set_methods isinstance args variables UserDefinedSetVariable pyrefly ignore missing-attribute args _set_vt call_method tx name args kwargs isinstance args variables SetVariable args call_method tx name args kwargs fn frozenset resolved_fn = getattr fn name resolved_fn frozenset_methods isinstance args variables FrozensetVariable args call_method tx name args kwargs fn str len args = resolved_fn = getattr fn name resolved_fn str_methods isinstance args ConstantVariable args call_method tx name args kwargs fn float len args = isinstance args ConstantVariable ConstantVariable create getattr float name args as_python_constant super call_method tx name args kwargs _call_int_float tx InstructionTranslator arg Handle cases like int torch seed Also handle sym_float sym_int cases isinstance arg SymNodeVariable variables TensorVariable isinstance arg variables TensorVariable item = arg call_method tx item item = arg fn_ = sym_int fn int sym_float torch _dynamo variables builder wrap_fx_proxy wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function fn_ item as_proxy call_int = _call_int_float call_float = _call_int_float call_bool tx InstructionTranslator arg Emulate ` PyBool_Type tp_vectorcall ` which boils down ` PyObject_IsTrue ` https github com python cpython blob Objects object c#L -L isinstance arg SymNodeVariable Note we delay specializing symbolic values avoid unnecessary guards Specialization will happen later e g resulting boolean used branching isinstance arg sym_num torch SymBool arg Emulate ` nb_bool ` int float objects - https github com python cpython blob Objects longobject c#L -L - https github com python cpython blob Objects floatobject c#L -L assert istype arg sym_num torch SymInt torch SymFloat SymNodeVariable create tx arg as_proxy = TODO handle more cases merge ` generic_jump ` call_str tx InstructionTranslator arg Handle ` str ` user defined function object isinstance arg variables UserFunctionVariable variables ConstantVariable create value=str arg fn isinstance arg variables UserDefinedObjectVariable Check object has __str__ method hasattr arg value __str__ str_method = arg value __str__ hasattr arg value __repr__ account __repr__ functions when __str__ absent str_method = arg value __repr__ unimplemented_v gb_type= failed call str user defined object context=str arg explanation= User defined object has no __str__ __repr__ method hints= graph_break_hints USER_ERROR type arg value __str__ object __str__ Rely object str method try pyrefly ignore unbound-name variables ConstantVariable create value=str_method except AttributeError Graph break pyrefly ignore unbound-name is_wrapper_or_member_descriptor str_method unimplemented_v gb_type= Attempted str method implemented C C++ context= explanation=f type arg value has C C++ based str method This supported hints= Write str method Python Overrides custom str method Pass method function call tx inline_user_function_return bound_method = str_method __func__ type ignore attr-defined try Only supports certain function types user_func_variable = VariableTracker build tx bound_method except AssertionError Won t able do inline str method avoid graph break log warning Failed create UserFunctionVariable exc_info=True Inline user function user_func_variable call_function tx arg isinstance arg variables ExceptionVariable len arg args == value = f arg exc_type value = join as_python_constant arg args variables ConstantVariable create value=value _call_min_max tx InstructionTranslator args len args == args has_force_unpack_var_sequence tx items = args force_unpack_var_sequence tx _call_min_max_seq tx items len args == _call_min_max_binary tx args args len args _call_min_max_seq tx args _call_min_max_seq tx InstructionTranslator items assert len items len items == items functools reduce functools partial _call_min_max_binary tx items _call_min_max_binary tx InstructionTranslator b None b None b could none we reduce _call_min_max_binary failed something tensor_args b isinstance variables TensorVariable b = b assert isinstance variables TensorVariable result item call scalar convert tensor isinstance FakeItemVariable = variables TorchInGraphFunctionVariable torch tensor call_function tx Dynamic input does get resolved rather gets stored call_function isinstance SymNodeVariable isinstance b SymNodeVariable builder wrap_fx_proxy_cls wrap_fx_proxy_cls type tx=tx proxy=tx output create_proxy call_function fn proxy_args_kwargs b convert min max torch ops b is_python_constant fn VariableTracker isinstance variables NumpyNdarrayVariable numpy np fn = variables NumpyVariable np clip fn = variables TorchInGraphFunctionVariable torch clamp kwargs = min b fn max max b result = fn call_function tx kwargs isinstance variables NumpyNdarrayVariable numpy np np_fn = max np maximum min np minimum fn fn = variables NumpyVariable np_fn torch_fn = max torch maximum min torch minimum fn fn = variables TorchInGraphFunctionVariable torch_fn result = fn call_function tx b unspec both b unspec const all isinstance i variables UnspecializedPythonVariable variables ConstantVariable i b any isinstance val FakeItemVariable val b variables FakeItemVariable from_tensor_variable result b is_python_constant raw_b = b as_python_constant raw_b = b raw_value fn max pyrefly ignore missing-attribute raw_res = max raw_value raw_b pyrefly ignore missing-attribute raw_res = min raw_value raw_b need_unwrap = any x need_unwrap x b isinstance x variables UnspecializedPythonVariable variables UnspecializedPythonVariable from_tensor_variable result raw_res need_unwrap otherwise tensor result isinstance SymNodeVariable isinstance b SymNodeVariable py_fn = torch sym_max fn max torch sym_min proxy = tx output create_proxy call_function py_fn proxy_args_kwargs b SymNodeVariable create tx proxy None isinstance ConstantVariable isinstance b ConstantVariable value = fn as_python_constant b as_python_constant ConstantVariable value call_min = _call_min_max call_max = _call_min_max call_abs tx InstructionTranslator arg VariableTracker Call arg __abs__ abs_method = BuiltinVariable getattr call_function tx arg ConstantVariable create __abs__ abs_method call_function tx call_pos tx InstructionTranslator arg VariableTracker Call arg __pos__ pos_method = BuiltinVariable getattr call_function tx arg ConstantVariable create __pos__ pos_method call_function tx call_index tx InstructionTranslator arg VariableTracker isinstance arg variables TensorVariable unimplemented_v gb_type= unsupported index Tensor context= explanation= Dynamo does support tracing builtin index Tensor hints= arg = guard_if_dyn arg constant_value = operator index arg variables ConstantVariable create constant_value call_round tx InstructionTranslator arg args kwargs Call arg __round__ round_method = BuiltinVariable getattr call_function tx arg ConstantVariable create __round__ round_method call_function tx args kwargs call_range tx InstructionTranslator args check_unspec_or_constant_args args variables RangeVariable args _dynamic_args args args = tuple variables ConstantVariable create guard_if_dyn arg arg args variables RangeVariable args None no-ops handler lets driving function proceed None _dynamic_args args kwargs any isinstance x SymNodeVariable x args any isinstance x SymNodeVariable x kwargs values call_slice tx InstructionTranslator args variables SliceVariable args tx _dyn_proxy tx InstructionTranslator args kwargs builder wrap_fx_proxy wrap_fx_proxy tx tx output create_proxy call_function fn proxy_args_kwargs args kwargs NOTE must handle IteratorVariable separately _call_iter_tuple_list tx InstructionTranslator obj=None args kwargs assert isinstance obj variables IteratorVariable _dynamic_args args kwargs _dyn_proxy tx args kwargs cls = variables BaseListVariable cls_for fn obj None cls mutation_type=ValueMutationNew obj has_unpack_var_sequence tx obj source is_constant_source obj source isinstance obj TupleIteratorVariable install_guard obj source make_guard GuardBuilder TUPLE_ITERATOR_LEN getattr obj source False isinstance obj ConstDictVariable istype obj SetVariable FrozensetVariable tx output guard_on_key_order add obj source isinstance obj variables MappingProxyVariable This could overguarding its rare iterate through mapping proxy use keys install_guard obj source make_guard GuardBuilder MAPPING_KEYS_CHECK isinstance obj variables UnspecializedNNModuleVariable Prevent calling __len__ method guards tracing __iter__ will insert right guards later install_guard obj source make_guard GuardBuilder SEQUENCE_LENGTH cls list obj unpack_var_sequence tx mutation_type=ValueMutationNew _call_iter_tuple_generator tx obj args kwargs cls = variables BaseListVariable cls_for fn cls list obj force_unpack_var_sequence tx exhaust generator mutation_type=ValueMutationNew _call_tuple_list tx obj=None args kwargs isinstance obj variables IteratorVariable cls = variables BaseListVariable cls_for fn cls list obj force_unpack_var_sequence tx mutation_type=ValueMutationNew isinstance obj variables LocalGeneratorObjectVariable isinstance obj UserDefinedObjectVariable obj has_force_unpack_var_sequence tx _call_iter_tuple_generator tx obj args kwargs _call_iter_tuple_list tx obj args kwargs call_iter tx InstructionTranslator obj args kwargs avoid overhead tracing polyfill we already know implemented __iter__ isinstance obj variables ListVariable variables RangeVariable variables IteratorVariable variables ConstDictVariable variables NNModuleVariable variables TensorVariable obj call_method tx __iter__ If object doesn t implement __iter__ method will error eager mode when calling iter anyway If object implements __iter__ method inlining effectively forwards call another iter call e g when __iter__ just returns iter list user-defined iterator If object implements __getitem__ method iter will call obj __getitem__ integer argument starting until __getitem__ raises IndexError ret = variables UserFunctionVariable polyfills builtins iter_ call_function tx obj args args iter obj sentinel returns object implements __iter__ __next__ methods UserDefinedObjectVariable Wrap value IteratorVariable subclass LazyObjectIteratorVariable forwards next_variable call object ret = variables ObjectIteratorVariable ret ret call_tuple = _call_tuple_list call_list = _call_tuple_list call_callable tx InstructionTranslator arg functions BaseUserFunctionVariable FunctoolsPartialVariable nn_module NNModuleVariable isinstance arg variables UserDefinedClassVariable BaseUserFunctionVariable FunctoolsPartialVariable NNModuleVariable variables ConstantVariable create True isinstance arg UserDefinedVariable variables ConstantVariable create callable arg value isinstance arg ConstantVariable SymNodeVariable TensorVariable ListVariable TupleVariable ListIteratorVariable variables ConstantVariable create False call_cast _ args kwargs len args == args unimplemented_v gb_type= bad args builtin cast context=f got args args kwargs explanation= Dynamo expects exactly args builtin cast hints= Ensure your call cast has exactly arguments call_dir tx InstructionTranslator arg isinstance arg variables UserDefinedClassVariable VariableTracker build tx dir arg value isinstance arg BuiltinVariable VariableTracker build tx dir arg fn call_dict tx InstructionTranslator args kwargs BuiltinVariable call_custom_dict tx dict args kwargs staticmethod call_custom_dict tx InstructionTranslator user_cls args kwargs args = list args len args == isinstance args variables GetAttrVariable isinstance args obj variables UserDefinedClassVariable tx output side_effects has_pending_mutation args obj Forward GetAttrVariable foo __dict__ realized vt VT foo __dict__ This simplifies construction new dict args = args get_forwarded_dict tx tx inline_user_function_return VariableTracker build tx polyfills construct_dict VariableTracker build tx user_cls args kwargs staticmethod call_custom_dict_fromkeys tx InstructionTranslator user_cls args kwargs user_cls dict OrderedDict defaultdict unimplemented_v gb_type= Unsupported dict type fromkeys context=f user_cls __name__ fromkeys args kwargs explanation=f Failed call user_cls __name__ fromkeys because f user_cls __name__ any type dict OrderedDict defaultdict hints= f Ensure user_cls __name__ type dict OrderedDict defaultdict kwargs Only ` OrderedDict fromkeys ` accepts ` value ` passed keyword user_cls OrderedDict len args = len kwargs = value kwargs raise_args_mismatch tx f user_cls __name__ fromkeys args kwargs ` value ` f len args args len kwargs kwargs args = args kwargs pop value len args == raise_args_mismatch tx f user_cls __name__ fromkeys least args f len args args len args == args = args ConstantVariable create None len args = raise_args_mismatch tx f user_cls __name__ fromkeys args f len args args arg value = args DictVariableType = ConstDictVariable user_cls defaultdict DefaultDictVariable isinstance arg dict arg = ConstantVariable create k k arg keys DictVariableType pyrefly ignore bad-argument-type dict fromkeys arg value user_cls mutation_type=ValueMutationNew arg has_force_unpack_var_sequence tx keys = arg force_unpack_var_sequence tx all is_hashable v v keys DictVariableType pyrefly ignore bad-argument-type dict fromkeys keys value user_cls mutation_type=ValueMutationNew unimplemented_v gb_type= failed call dict fromkeys context=f user_cls __name__ fromkeys args kwargs explanation=f Failed call user_cls __name__ fromkeys because arguments could automatically converted list some dict key hashable hints= Manually convert argument list Ensure all keys hashable call_set tx InstructionTranslator args kwargs Can we merge implementation call_dict s one assert kwargs args SetVariable mutation_type=ValueMutationNew len args = raise_observed_exception TypeError tx args= ConstantVariable create f set takes positional argument len args given arg = args istype arg variables SetVariable arg clone mutation_type=ValueMutationNew arg has_force_unpack_var_sequence tx items = arg force_unpack_var_sequence tx SetVariable items mutation_type=ValueMutationNew isinstance arg variables UserDefinedObjectVariable isinstance arg value KeysView iter_fn = arg var_getattr tx __iter__ isinstance iter_fn variables UserMethodVariable out = tx inline_user_function_return iter_fn args kwargs isinstance out SetVariable out BuiltinVariable set call_set tx out raise_observed_exception TypeError tx args= ConstantVariable create failed construct builtin set call_frozenset tx InstructionTranslator args kwargs assert kwargs args FrozensetVariable len args = raise_observed_exception TypeError tx args= ConstantVariable create f frozenset takes positional argument len args given arg = args istype arg variables FrozensetVariable FrozensetVariable x vt x arg set_items arg has_force_unpack_var_sequence tx items = arg force_unpack_var_sequence tx FrozensetVariable items raise_observed_exception TypeError tx args= ConstantVariable create failed construct builtin frozenset call_zip tx InstructionTranslator args kwargs kwargs len kwargs == strict kwargs raise_args_mismatch tx zip kwargs ` strict ` f len kwargs kwargs strict = kwargs pop strict False args = BuiltinVariable iter call_function tx arg arg args variables ZipVariable args strict=strict mutation_type=ValueMutationNew call_len tx InstructionTranslator args kwargs try args call_method tx __len__ args kwargs except AttributeError e raise_observed_exception type e tx args=list e args call_getitem tx InstructionTranslator args kwargs args call_method tx __getitem__ args kwargs call_isinstance tx InstructionTranslator arg isinstance_type try arg_type = arg python_type except NotImplementedError unimplemented_v gb_type= builtin isinstance cannot determine type argument context=f isinstance arg isinstance_type explanation=f Dynamo doesn t have rule determine type argument arg hints= graph_break_hints DYNAMO_BUG isinstance_type = isinstance_type as_python_constant isinstance arg variables TensorVariable arg dtype None _tensor_isinstance tensor_var tensor_type check_type ty ty tensortype_to_dtype example_val = arg as_proxy node meta example_value is_traceable_wrapper_subclass example_val ty torch nn parameter Parameter N B we calling isinstance directly example value torch nn Parameter has meta-class overrides __isinstance__ isinstance check here allows us invoke logic isinstance example_val ty issubclass arg python_type ty dtypes = tensortype_to_dtype ty arg dtype dtypes type tensor_type tuple any check_type ty ty tensor_type check_type tensor_type variables ConstantVariable create _tensor_isinstance arg isinstance_type UserDefinedObject C extensions can have torch Tensor attributes so break graph isinstance arg variables UserDefinedObjectVariable isinstance arg value types MemberDescriptorType unimplemented_v gb_type= isinstance called user defined object C extensions context=f isinstance arg isinstance_type explanation= User-defined object C extensions can have torch Tensor attributes intentionally graph breaking hints= graph_break_hints SUPPORTABLE handle __instancecheck__ defined user isinstance arg variables UserDefinedObjectVariable __instancecheck__ isinstance_type __class__ __dict__ variables ConstantVariable create isinstance_type __class__ __instancecheck__ isinstance_type arg value isinstance arg variables UserDefinedExceptionClassVariable pyrefly ignore unbound-name ConstantVariable create isinstance arg_type isinstance_type isinstance_type_tuple tuple type isinstance isinstance_type type callable E g isinstance obj typing Sequence getattr isinstance_type __instancecheck__ None isinstance_type_tuple = isinstance_type isinstance isinstance_type types UnionType isinstance_type_tuple = isinstance_type __args__ isinstance isinstance_type tuple all isinstance tp type callable getattr tp __instancecheck__ None tp isinstance_type isinstance_type_tuple = isinstance_type raise_observed_exception TypeError tx args= isinstance arg must type tuple types union try NB ` isinstance ` does call ` __subclasscheck__ ` use ` __instancecheck__ ` But usually ` isinstance obj type_info ` ` issubclass type obj type_info ` gives same result WARNING This might run arbitrary user code ` __subclasscheck__ ` we did trace through This limitation current implementation Usually ` __subclasscheck__ ` ` __instancecheck__ ` can constant fold through might big issue we trade off performance pyrefly ignore unbound-name val = issubclass arg_type isinstance_type_tuple except TypeError pyrefly ignore unbound-name val = arg_type isinstance_type_tuple variables ConstantVariable create val call_issubclass tx InstructionTranslator left_ty right_ty Checks first arg subclass right arg try left_ty_py = left_ty as_python_constant right_ty_py = right_ty as_python_constant except NotImplementedError unimplemented_v gb_type= issubclass non-constant arguments context=f issubclass left_ty right_ty explanation= issubclass non-constant arguments supported hints= Make sure your arguments types graph_break_hints USER_ERROR WARNING This might run arbitrary user code ` __subclasscheck__ ` See comment call_isinstance above pyrefly ignore unbound-name variables ConstantVariable issubclass left_ty_py right_ty_py call_super tx InstructionTranslator b variables SuperVariable b call_next tx InstructionTranslator args arg = args try arg next_variable tx except ObservedUserStopIteration len args == args raise except Unsupported ex isinstance arg variables BaseListVariable ex remove_from_stats arg items raise call_hasattr tx InstructionTranslator obj attr attr is_python_constant name = attr as_python_constant isinstance obj variables BuiltinVariable variables ConstantVariable hasattr obj fn name obj call_obj_hasattr tx name call_map tx InstructionTranslator fn seqs seqs = seq unpack_var_sequence tx seq has_unpack_var_sequence tx seq seq seqs variables MapVariable fn seqs mutation_type=ValueMutationNew call_filter tx InstructionTranslator fn seq seq = seq unpack_var_sequence tx seq has_unpack_var_sequence tx seq variables FilterVariable fn seq mutation_type=ValueMutationNew var_getattr tx InstructionTranslator name source = source AttrSource source name fn object object we can just directly read attribute try value = getattr fn name except AttributeError raise_observed_exception AttributeError tx pyrefly ignore unbound-name callable value pyrefly ignore unbound-name VariableTracker build tx value source variables GetAttrVariable name source=source call_getattr tx InstructionTranslator obj VariableTracker name_var VariableTracker default=None name_var is_python_constant unimplemented_v gb_type= getattr non-constant name argument context=f getattr obj name_var default explanation= getattr non-constant name argument supported hints= Ensure name argument getattr string name = name_var as_python_constant See NOTE Tensor grad _grad attr isinstance obj TensorVariable name == _grad name = grad tx output side_effects is_attribute_mutation obj isinstance obj variables UnspecializedNNModuleVariable name named_parameters parameters named_buffers buffers named_modules modules obj is_state_mutated tx output side_effects has_pending_mutation obj unimplemented_v gb_type= getattr nn Module pending mutation context=f getattr obj name default explanation= Intentionally graph breaking getattr nn Module pending mutation hints= tx output side_effects has_pending_mutation_of_attr obj name tx output side_effects load_attr obj name default None hasattr_var = call_hasattr tx obj name_var assert hasattr_var as_python_constant True False hasattr_var as_python_constant default source = obj source AttrSource obj source name name __bases__ __base__ __flags__ try value = obj as_python_constant isinstance value type name == __bases__ tuple_args = VariableTracker build tx b source GetItemSource source i i b enumerate value __bases__ variables TupleVariable tuple_args source=source name == __base__ VariableTracker build tx value __base__ source name == __flags__ ConstantVariable create value __flags__ except NotImplementedError pass isinstance obj variables NNModuleVariable obj var_getattr tx name isinstance obj variables TensorVariable variables NamedTupleVariable variables ConstantVariable variables DistributedVariable variables UserDefinedClassVariable variables UserDefinedObjectVariable isinstance obj variables UserDefinedObjectVariable issubclass obj value __class__ unittest TestCase config enable_trace_unittest name assertRaisesRegex assertNotWarns assertWarnsRegex assertWarns unimplemented_v gb_type= Failed trace unittest method context=f function unittest TestCase name explanation=f Dynamo does know how trace unittest method ` name ` hints= f Avoid calling ` TestCase name ` Please report issue PyTorch isinstance obj TensorVariable fake_val = obj proxy node meta example_value isinstance fake_val torch Tensor is_sparse_any fake_val tx export config capture_sparse_compute unimplemented_v gb_type= Attempted wrap sparse Tensor context= explanation= torch compile does support sparse Tensors hints= graph_break_hints SUPPORTABLE try obj var_getattr tx name except NotImplementedError variables GetAttrVariable obj name source=source isinstance obj variables TorchInGraphFunctionVariable Get OpOverload OpOverloadPacket e g torch ops aten add default member = getattr obj value name isinstance member torch _ops OpOverloadPacket torch _ops OpOverload torch _dynamo trace_rules is_aten_op_or_tensor_method member variables TorchInGraphFunctionVariable member source=source name cmp_name_to_op_mapping variables GetAttrVariable obj name source=source isinstance obj DummyModule TODO mlazos - Do we need obj is_torch name obj value __dict__ member = getattr obj value name member = obj value __dict__ name config replay_record_enabled tx exec_recorder record_module_access obj value name member type ignore arg-type union-attr VariableTracker build tx member source istype obj variables UserFunctionVariable name __name__ __module__ ConstantVariable create getattr obj fn name try obj var_getattr tx name except NotImplementedError variables GetAttrVariable obj name source=source call_setattr tx InstructionTranslator obj VariableTracker name_var VariableTracker val VariableTracker isinstance obj variables PlacementVariable variables NamedTupleVariable variables UserDefinedObjectVariable variables NestedUserFunctionVariable variables ExceptionVariable obj call_method tx __setattr__ name_var val tx output side_effects is_attribute_mutation obj name_var is_python_constant name = name_var as_python_constant isinstance obj variables TensorVariable builder wrap_fx_proxy Some special handling tensor attributes name == requires_grad TODO voz Make work properly unimplemented_v gb_type= setattr Tensor requires_grad context=f setattr obj name val explanation= setattr Tensor requires_grad supported Mutating requires_grad can introduce new leaf non-leaf vice versa middle graph which AOTAutograd does currently know how handle hints= graph_break_hints SUPPORTABLE name == data See comments ` test_set_data_on_scoped_tensor ` plans support obj source None unimplemented_v gb_type= Failed mutate tensor data attribute context=f setattr obj name val explanation= Dyanmo only supports mutating ` data ` tensor created outside ` torch compile ` region hints= Don t mutate ` data ` tensor move mutation out ` torch compile ` region obj dtype = val dtype type ignore attr-defined unimplemented_v gb_type= Failed mutate tensor data attribute different dtype context=f setattr obj name val explanation= Dyanmo only supports mutating ` data ` tensor new one same dtype hints= Don t mutate ` data ` tensor move mutation out ` torch compile ` region Remove old reference tracked fakes - we don t do new data value size shape differences will cause tracked fakes produce incorrect guards This sound because TensorVariable coming out set_ below will new one get installed tracked fakes to_remove = tf tf tx output tracked_fakes tf source == obj source tf to_remove tx output tracked_fakes remove tf Step - disable grads dynamo_disable_grad tx torch no_grad Step - call ` set_ ` out = wrap_fx_proxy tx tx output create_proxy call_function torch Tensor set_ proxy_args_kwargs obj val Step - drop version counter - step required get data setting play correctly autograd engine Essentially dynamo trying faithfully preserve absurd behavior data= eager mode _lower_version_count_by_ x version = x _version version version = version - torch _C _autograd _unsafe_set_version_counter x version x tx output create_proxy call_function _lower_version_count_by_ out as_proxy _lower_version_count_by_ obj as_proxy node meta example_value This handles options prop guards ends clone Step - replace all reference current object new one out name _grad grad NOTE Tensor grad _grad attr _grad grad share same setter getter see THPVariable_properties here we make sure setting one enables reading ` val ` other routing all read write ` grad ` name = grad is_tensor_getset_descriptor name Attribute like ` torch Tensor real ` has special setters we don t yet support s simple adding entry side effect mapping unimplemented_v gb_type= Failed set tensor attribute context=f setattr obj name val explanation= Dyanmo doesn t support setting these tensor attributes hints= f Don t mutate attribute name tensors move mutation out ` torch compile ` region tx output side_effects store_attr obj name val val isinstance obj variables NNModuleVariable tx output is_root_tracer raise AttributeMutationError Can t inplace modify module params buffers inside HigherOrderOp name_var is_python_constant isinstance val variables TensorVariable assigning_fake_val = get_fake_value val as_proxy node tx try getattr_var = obj var_getattr tx name_var as_python_constant except AttributeError ObservedAttributeError getattr_var = None isinstance getattr_var variables TensorVariable get_fake_val will get same fake tensor existing_fake_attr = get_fake_value getattr_var as_proxy node tx same tensor identity setattr no-op mod_setattr = inspect getattr_static obj module_type __setattr__ existing_fake_attr assigning_fake_val mod_setattr torch nn Module __setattr__ getattr_var obj convert_to_unspecialized tx call_delattr tx InstructionTranslator obj VariableTracker name_var VariableTracker obj call_method tx __delattr__ name_var call_type tx InstructionTranslator obj VariableTracker try py_type = obj python_type except NotImplementedError error raise UserError UserErrorType INVALID_INPUT str error case_name= unknown_python_type None source = obj source TypeSource obj source source None isinstance obj variables UserDefinedObjectVariable obj cls_source source = obj cls_source py_type torch Tensor In some cases torch isn t available globals name = tx output install_global_by_id torch source = AttrSource GlobalSource name Tensor VariableTracker build tx py_type source call_reversed tx InstructionTranslator obj VariableTracker obj has_unpack_var_sequence tx items = list reversed obj unpack_var_sequence tx variables TupleVariable items call_sorted tx InstructionTranslator obj VariableTracker kwargs VariableTracker obj has_force_unpack_var_sequence tx isinstance obj variables TensorVariable list_var = variables ListVariable obj force_unpack_var_sequence tx mutation_type=ValueMutationNew list_var call_method tx sort kwargs list_var neg constant fold function so we only get here constant fold valid call_neg tx InstructionTranslator isinstance SymNodeVariable SymNodeVariable create tx operator neg as_proxy sym_num=None isinstance UserDefinedObjectVariable call_obj_hasattr tx __neg__ value type ignore attr-defined call_method tx __neg__ None no-ops handler lets driving function proceed None call_format tx InstructionTranslator _format_string args kwargs format_string = _format_string as_python_constant format_string = str format_string variables StringFormatVariable create format_string args kwargs call_id tx InstructionTranslator args len args isinstance args variables NNModuleVariable nn_mod_variable = args mod = tx output get_submodule nn_mod_variable module_key variables ConstantVariable create id mod len args == isinstance args variables UserDefinedClassVariable variables UserDefinedObjectVariable args source isinstance args variables UserDefinedClassVariable install_guard args source make_guard GuardBuilder CLASS_MATCH install_guard args source make_guard GuardBuilder ID_MATCH constant_result = id args value variables ConstantVariable create constant_result len args == isinstance args TensorVariable tensor_variable = args tensor_variable call_id tx istype args variables UserFunctionVariable variables ConstantVariable create id args fn istype args variables SkipFunctionVariable variables ConstantVariable create id args value istype args variables FunctoolsPartialVariable variables ConstantVariable create id args fake_value unimplemented_v gb_type= id unsupported args context=str args explanation=f Dynamo doesn t know how trace id call args args hints= Supported args Tensors functions nn Modules user-defined objects outside compiled region graph_break_hints SUPPORTABLE call_deepcopy tx InstructionTranslator x unimplemented_v gb_type= copy deepcopy context=f copy deepcopy x explanation= Dynamo does support copy deepcopy hints= Avoid calling copy deepcopy graph_break_hints SUPPORTABLE _comparison_with_tensor tx InstructionTranslator left right builder wrap_fx_proxy_cls tensor supported_tensor_comparison_op_values op = fn op operator is_ operator is_not is_result = isinstance left TensorVariable isinstance right TensorVariable id extract_fake_example_value left as_proxy node == id extract_fake_example_value right as_proxy node op operator is_ ConstantVariable create is_result ConstantVariable create is_result op supported_tensor_comparison_op_values unimplemented_v gb_type= unsupported Tensor comparison op context=f op __name__ left right explanation=f Dynamo does support comparison op op __name__ f Tensor arguments left right hints= graph_break_hints SUPPORTABLE isinstance left TensorVariable isinstance right TensorVariable left size right size None left size = right size try torch broadcast_shapes left size right size except RuntimeError broadcastable can t compared unimplemented_v gb_type= failed broadcast when attempting Tensor comparison op context=f op __name__ left right explanation=f Dynamo unable broad cast arguments left right f when attempting trace comparison op op __name__ hints= graph_break_hints USER_ERROR tensor_cls = left isinstance left TensorVariable right proxy = tx output create_proxy call_function op left as_proxy right as_proxy wrap_fx_proxy_cls type tensor_cls handle Ndarrays Tensors tx proxy _comparison_with_symnode tx InstructionTranslator left right tensor supported_tensor_comparison_op_values op = fn op supported_tensor_comparison_op_values unimplemented_v gb_type= unsupported SymNode comparison op context=f op __name__ left right explanation=f Dynamo does support comparison op op __name__ f SymNode arguments left right hints= graph_break_hints SUPPORTABLE This seen inspect signature where we check value default value isinstance right variables UserDefinedClassVariable variables ConstantVariable op object None proxy = tx output create_proxy call_function op left as_proxy right as_proxy SymNodeVariable create tx proxy sym_num=None call_xor tx InstructionTranslator b isinstance DictKeysVariable SetVariable UserDefinedObjectVariable call_method tx __xor__ b call_ixor tx InstructionTranslator b isinstance DictKeysVariable SetVariable UserDefinedObjectVariable call_method tx __ixor__ b call_sub tx InstructionTranslator b isinstance DictKeysVariable SetVariable UserDefinedObjectVariable call_method tx __sub__ b call_isub tx InstructionTranslator b isinstance DictKeysVariable SetVariable UserDefinedObjectVariable call_method tx __isub__ b call_and_ tx InstructionTranslator b Rely constant_handler isinstance ConstantVariable isinstance b ConstantVariable None isinstance SymNodeVariable ConstantVariable isinstance b SymNodeVariable ConstantVariable SymNodeVariable create tx tx output create_proxy call_function operator and_ proxy_args_kwargs b sym_num=None isinstance DictKeysVariable SetVariable UserDefinedObjectVariable call_method tx __and__ b None no-ops handler lets driving function proceed call_iand tx InstructionTranslator b Rely constant_handler isinstance ConstantVariable isinstance b ConstantVariable None isinstance SymNodeVariable ConstantVariable isinstance b SymNodeVariable ConstantVariable SymNodeVariable create tx tx output create_proxy call_function operator iand proxy_args_kwargs b sym_num=None isinstance DictKeysVariable SetVariable UserDefinedObjectVariable call_method tx __iand__ b call_or_ tx InstructionTranslator b Rely constant_handler isinstance ConstantVariable isinstance b ConstantVariable None isinstance SymNodeVariable ConstantVariable isinstance b SymNodeVariable ConstantVariable SymNodeVariable create tx tx output create_proxy call_function operator or_ proxy_args_kwargs b sym_num=None This call looks like ` one torch ones &#124; two torch ones ` isinstance ConstDictVariable DictKeysVariable MutableMappingVariable SetVariable UserDefinedDictVariable UserDefinedObjectVariable TODO guilhermeleobas forward call b __ror__ __ror__ b returns NotImplemented call_method tx __or__ b None no-ops handler lets driving function proceed None call_ior tx InstructionTranslator b Rely constant_handler isinstance ConstantVariable isinstance b ConstantVariable None isinstance SymNodeVariable ConstantVariable isinstance b SymNodeVariable ConstantVariable SymNodeVariable create tx tx output create_proxy call_function operator ior proxy_args_kwargs b sym_num=None This call looks like ` one torch ones &#124; = two torch ones ` isinstance ConstDictVariable DictKeysVariable MutableMappingVariable SetVariable UserDefinedObjectVariable call_method tx __ior__ b None no-ops handler lets driving function proceed None call_not_ tx InstructionTranslator isinstance SymNodeVariable SymNodeVariable create tx tx output create_proxy call_function operator not_ proxy_args_kwargs sym_num=None Unwrap underlying ConstDictVariable isinstance DictViewVariable = dv_dict isinstance ListVariable ConstDictVariable ConstantVariable create len items == None call_contains tx InstructionTranslator VariableTracker b VariableTracker call_method tx __contains__ b contextlib contextmanager dynamo_disable_grad tx GradModeVariable gmv = GradModeVariable create tx False try gmv enter tx yield finally gmv exit tx