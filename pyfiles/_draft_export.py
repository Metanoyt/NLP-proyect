getpass json logging os re tempfile time collections abc Callable Mapping dataclasses dataclass enum IntEnum typing Any Optional Union torch torch _logging _internal torch utils _pytree pytree torch _dynamo exc UserError UserErrorType torch _export passes insert_custom_op_guards get_op_profiles insert_custom_op_guards OpProfile torch _utils_internal log_draft_export_usage _trace _export get_ep_stats dynamic_shapes _DimHint _DimHintType Dim exported_program ExportedProgram log = logging getLogger __name__ FailureType IntEnum MISSING_FAKE_KERNEL = DATA_DEPENDENT_ERROR = GUARD_ADDED = MISMATCHED_FAKE_KERNEL = __str__ - str name prettify_stack stack list dict str str str_to_filename dict int str - str res = frame stack frame filename str_to_filename continue res += f File str_to_filename frame filename lineno frame line frame name type ignore index res += f \n stack - loc res prettify_frame_locals loc str locals dict str Any symbols dict str Any - str local_str = \n join f k v k v locals items res = f Locals local_str any v None v symbols values symbol_str = \n join f k v k v symbols items v None res += f Symbols symbol_str res get_loc filename str lineno int - Optional str try open filename f i line enumerate f i == lineno - line strip except FileNotFoundError pass None FailureReport __init__ failure_type FailureType data dict str Any xfail bool = False - None failure_type FailureType = failure_type data dict str Any = data xfail bool = xfail __repr__ - str f FailureReport failure_type= failure_type xfail= xfail data= data print str_to_filename dict int str - str failure_type == FailureType MISSING_FAKE_KERNEL op = data op f Missing fake kernel torch ops op missing fake kernel implementation Please refer https docs google com document d _W p WJOQQUzPsJYa s JXt qf OfLub sbkHOaU edit#heading=h ahugy p jmz more detailed instructions how write meta implementation noqa B failure_type == FailureType GUARD_ADDED locals_info = prettify_frame_locals data frame_locals data frame_locals f Guard Added A guard added during tracing which might ve resulted some incorrect tracing constraint violation error Specifically guard added data expr where data symbol_to_sources This occurred following stacktrace prettify_stack data user_stack str_to_filename locals_info And following framework stacktrace prettify_stack data stack str_to_filename \n Because we have modified dynamic shapes structure following You can also use torch export Dim AUTO instead specify your dynamic shapes we will automatically infer dynamism you ` ` ` dynamic_shapes = data new_dynamic_shapes ` ` ` failure_type == FailureType DATA_DEPENDENT_ERROR locals_info = prettify_frame_locals data frame_locals data frame_locals f Data dependent error When exporting we unable evaluate value ` data expr ` This encountered data occurrences times This occurred following user stacktrace prettify_stack data user_stack str_to_filename locals_info And following framework stacktrace prettify_stack data stack str_to_filename \n As result specialized constant e g ` data result ` st occurrence asserts inserted into graph Please add ` torch _check ` original code assert data-dependent assumption Please refer https docs google com document d kZ_BbB JnoLbUZleDT dHs ZVYId jT-yTFgf A edit#heading=h boi xurpqa o more details noqa B failure_type == FailureType MISMATCHED_FAKE_KERNEL op = data op reason = data reason f Mismatched fake kernel torch ops op has fake kernel implementation has incorrect behavior based real kernel The reason mismatch reason Please refer https docs google com document d _W p WJOQQUzPsJYa s JXt qf OfLub sbkHOaU edit#heading=h ahugy p jmz more detailed instructions how write fake implementation noqa B raise ValueError f Unknown failure type failure_type DraftExportReport __init__ failures list FailureReport str_to_filename dict int str expressions_created dict int dict str Any op_profiles dict str set OpProfile failures list FailureReport = failures str_to_filename = str_to_filename expressions_created dict int dict str Any = expressions_created op_profiles = op_profiles successful - bool len failures == all failure xfail failure failures __repr__ - str f DraftExportReport failures __str__ - str WARNING_COLOR = \ m GREEN_COLOR = \ m END_COLOR = \ m successful f GREEN_COLOR ############################################################################################## Congratuations No issues found during export able soundly produce graph You can now change back torch export export ############################################################################################## END_COLOR error = f WARNING_COLOR ################################################################################################### WARNING len failures issue s found during export able soundly produce graph Please follow instructions fix errors ################################################################################################### i failure enumerate failures error += f i + failure print str_to_filename \n error += END_COLOR error apply_suggested_fixes - None raise NotImplementedError Not implemented yet dataclass ExpressionCreatedNode result_id int argument_ids list int record dict str object visited bool = False LogRecord __init__ - None log_count dict int int = logs list tuple str dict str Any = _hash element tuple str dict str Any - int key data = element key == missing_fake_kernel hash key data op key == mismatched_fake_kernel hash key data op data reason key == propagate_real_tensors_provenance hash key json dumps data user_stack key == guard_added hash key json dumps data user_stack key == create_unbacked_symbol hash key json dumps data user_stack hash key json dumps data try_add element tuple str dict str str - bool hash_value = _hash element hash_value log_count log_count hash_value += False log_count hash_value = logs append element True get_log_count element tuple str dict str Any - int log_count _hash element CaptureStructuredTrace torch _logging _internal LazyTraceHandler __init__ - None specific_log_keys = str exported_program propagate_real_tensors_provenance guard_added missing_fake_kernel mismatched_fake_kernel expression_created create_unbacked_symbol log_record LogRecord = LogRecord expression_created_logs dict int ExpressionCreatedNode = symbol_to_expressions dict str list dict str Any = logger = logging getLogger torch __trace prev_get_dtrace = False root_dir = os environ get torch _logging _internal DTRACE_ENV_VAR super __init__ root_dir sanitized_username = re sub r \\ &#124; _ getpass getuser root_dir = os path join tempfile gettempdir export_ + sanitized_username super __init__ root_dir setFormatter torch _logging _internal TorchLogsFormatter trace=True __enter__ - CaptureStructuredTrace log_record = LogRecord expression_created_logs = Remove lazy trace handler exists possible_lazy_trace_handlers = handler handler logger handlers isinstance handler torch _logging _internal LazyTraceHandler handler possible_lazy_trace_handlers logger removeHandler handler logger addHandler prev_get_dtrace = torch _logging _internal GET_DTRACE_STRUCTURED pyrefly ignore bad-assignment torch _logging _internal GET_DTRACE_STRUCTURED = True __exit__ exc_type exc_value traceback - None type ignore no-untyped-def log_record = LogRecord expression_created_logs = logger removeHandler pyrefly ignore bad-assignment torch _logging _internal GET_DTRACE_STRUCTURED = prev_get_dtrace prev_get_dtrace = False emit record Any - None _log_expression_created emit_func Callable Any None sym_node_id int - None Log all relevant expression_created logs sym_node_id None res = expression_created_logs get sym_node_id None Don t log expression we have already printed beforehand res visited res visited = True arg res argument_ids _log_expression_created emit_func arg emit_func res record metadata = record metadata key specific_log_keys key metadata log_record try_add key metadata key key == expression_created We don t want log all expression_created logs only ones relevant guards propagate_real_tensor expression_created_logs metadata key result_id = ExpressionCreatedNode metadata key result_id metadata key get argument_ids record key == propagate_real_tensors_provenance _log_expression_created super emit metadata key get expr_node_id key == guard_added len metadata key symbol_to_sources == We only want include guards added relevant symbolic shapes corresponding inputs which specified dynamic_shapes arg These have source metadata key prefix == runtime_assert This should ve been captured propagate_real_tensors log _log_expression_created super emit metadata key get expr_node_id super emit record draft_export mod torch nn Module args tuple Any kwargs Optional Mapping str Any = None dynamic_shapes Optional Union dict str Any tuple Any list Any = None preserve_module_call_signature tuple str = strict bool = False pre_dispatch bool = True prefer_deferred_runtime_asserts_over_guards bool = False - ExportedProgram start_time = time time kwargs = kwargs dynamic_shapes = dynamic_shapes constraint_violation_msg = None capture_structured_log = CaptureStructuredTrace torch _functorch config patch fake_tensor_propagate_real_tensors=True generate_fake_kernels_from_real_mismatches=True capture_structured_log try new_shapes = None ep = _export mod args kwargs dynamic_shapes=dynamic_shapes strict=strict pre_dispatch=pre_dispatch preserve_module_call_signature=preserve_module_call_signature prefer_deferred_runtime_asserts_over_guards=prefer_deferred_runtime_asserts_over_guards except Exception exc isinstance exc UserError exc error_type == UserErrorType CONSTRAINT_VIOLATION constraint_violation_msg = exc msg convert_dim_to_auto dim Any - Any isinstance dim Dim Dim AUTO min=dim min max=dim max isinstance dim _DimHint dim type == _DimHintType DYNAMIC Dim AUTO min=dim min max=dim max dim new_shapes = pytree tree_map convert_dim_to_auto dynamic_shapes ep = _export mod args kwargs dynamic_shapes=new_shapes strict=strict pre_dispatch=pre_dispatch preserve_module_call_signature=preserve_module_call_signature prefer_deferred_runtime_asserts_over_guards=prefer_deferred_runtime_asserts_over_guards log_draft_export_usage error=True export_time=time time - start_time strict=strict message=str exc type=f type exc __name__ type exc __qualname__ raise exc torch _logging dtrace_structured exported_program payload_fn=lambda str ep str_to_filename dict int str = failures list FailureReport = incorrect_custom_ops set str = set expressions_created dict int dict str Any = log_name log_contents capture_structured_log log_record logs failure_type = None log_name == str str_to_filename log_contents = log_contents type ignore index continue log_name == propagate_real_tensors_provenance log_contents occurrences = capture_structured_log log_record get_log_count log_name log_contents failure_type = FailureType DATA_DEPENDENT_ERROR log_name == guard_added new_shapes None continue failure_type = FailureType GUARD_ADDED log_contents new_dynamic_shapes = new_shapes log_name == missing_fake_kernel failure_type = FailureType MISSING_FAKE_KERNEL incorrect_custom_ops add log_contents op log_name == mismatched_fake_kernel failure_type = FailureType MISMATCHED_FAKE_KERNEL incorrect_custom_ops add log_contents op continue assert failure_type None failures append FailureReport failure_type log_contents k v capture_structured_log expression_created_logs items v visited expressions_created k = v record op_profiles = get_op_profiles ep graph_module incorrect_custom_ops report = DraftExportReport failures str_to_filename expressions_created op_profiles Add asserts around custom ops insert_custom_op_guards ep graph_module incorrect_custom_ops ep _report = report report successful log_filename = capture_structured_log stream name warning_msg = f ################################################################################################### WARNING len report failures issue s found during export able soundly produce graph To view report failures html page please run command ` tlparse log_filename -- export ` Or you can view errors python inspecting ` print ep _report ` len report op_profiles warning_msg += f While tracing we found len report op_profiles operator s which do have fake kernel registered If you intend retrace exported graph run fake tensors please run under following context manager which will register fake kernel those operators ` ` ` torch _library fake_profile unsafe_generate_fake_kernels ep _report op_profiles run fake tensors ` ` ` warning_msg += ################################################################################################# log warning warning_msg log info ############################################################################################## Congratuations No issues found during export able soundly produce graph You can now change back torch export export ############################################################################################## log_draft_export_usage error=False export_time=time time - start_time strict=strict constraint_violations=constraint_violation_msg report=ep _report get_ep_stats ep ep