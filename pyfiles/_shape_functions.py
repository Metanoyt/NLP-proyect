mypy allow-untyped-defs math typing Any Callable Dict List Optional Tuple Union number = Union int float flake noqa ### There generated files depend file To re-generate please run root repo python torchgen shape_functions gen_jit_shape_functions py How test After regenerating files compile PyTorch Then run build bin test_jit -- gtest_filter=TestShapeGraphLinting Basic If you have enabled opinfo testing op also run python test test_ops_jit py TestJitCPU test_variant_consistency_jit_ FAILING_OP _cpu_float reproduce errors opinfo tests Example PR https github com pytorch pytorch pull files #### torch broadcast list int b list int dimsA = len dimsB = len b ndim = max dimsA dimsB expandedSizes list int = i range ndim offset = ndim - - i dimA = dimsA - - offset dimB = dimsB - - offset sizeA = dimA dimA = sizeB = b dimB dimB = sizeA = sizeB sizeA = sizeB = TODO only assertion error bound C++ compilation right now raise AssertionError f The size tensor sizeA must match size tensor b sizeB non-singleton dimension i expandedSizes append sizeB sizeA == sizeA expandedSizes broadcast_three list int b list int c list int broadcast broadcast b c broadcast_one_three list int b Any c list int broadcast c adaptive_avg_pool d list int out list int assert len out == assert len == len == i range len assert i = shape list int = i range len - shape append i elem out shape append elem shape _copy list int out list int = elem out append elem out unary list int _copy broadcast_inplace list int b list int dimsA = len dimsB = len b dimsB dimsA raise AssertionError f The dims tensor b dimsB must less than equal dims tensor dimsA dimA range dimsA dimB = dimsB - dimsA + dimA sizeA = dimA sizeB = b dimB dimB = sizeA = sizeB sizeB = TODO only assertion error bound C++ compilation right now raise AssertionError The size tensor must match size tensor b non-singleton dimension format sizeA sizeB dimA _copy expand list int sizes list int assert len sizes = len ndim = len sizes tensor_dim = len ndim == _copy sizes out list int = i range ndim offset = ndim - - i dim = tensor_dim - - offset size = dim dim = targetSize = sizes i targetSize == - assert dim = targetSize = size size = targetSize assert size == size = targetSize out append size out expand_one_unused list int sizes list int inp Any expand sizes infer_size_impl shape list int numel int - list int newsize = infer_dim Optional int = None dim range len shape shape dim == - infer_dim None raise AssertionError only one dimension can inferred infer_dim = dim shape dim = newsize = shape dim raise AssertionError invalid shape dimensions numel == newsize infer_dim None newsize numel newsize == raise AssertionError invalid shape out = _copy shape infer_dim None out infer_dim = numel newsize out numel sizes list int numel = elem sizes numel = elem numel view list int sizes list int infer_size_impl sizes numel view_one_unused list int sizes list int implicit bool = False view sizes sum_mean_dim list int opt_dims Optional list int keep_dim bool dt Any out list int = opt_dims None len opt_dims == dims list int = list range len dims = opt_dims idx range len is_mean_dim bool = False reduce_dim dims idx == maybe_wrap_dim reduce_dim len is_mean_dim = True is_mean_dim keep_dim out append out append idx out max_dim list int dim int keep_dim bool out = sum_mean_dim dim keep_dim None out out note python already rounds down towards negative infinity integer division special arithmetic needed div_rtn x int y int x y pooling_output_shape_pad_lr inputSize int kernelSize int pad_l int pad_r int stride int dilation int ceil_mode bool outputSize = div_rtn inputSize + pad_l + pad_r - dilation kernelSize - - + stride - ceil_mode stride + ceil_mode outputSize - stride = inputSize + pad_l outputSize = outputSize - outputSize pooling_output_shape inputSize int kernelSize int pad_l int stride int dilation int ceil_mode bool assert stride = stride should zeero pooling_output_shape_pad_lr inputSize kernelSize pad_l pad_l stride dilation ceil_mode pool d_shape_check input list int kH int kW int dH int dW int padH int padW int dilationH int dilationW int nInputPlane int inputHeight int inputWidth int outputHeight int outputWidth int ndim = len input assert kW kH assert dW dH assert dilationH dilationW valid_dims = input = input = assert ndim == input = valid_dims ndim == valid_dims input = assert kW = padW kH = padH assert outputWidth = outputHeight = max_pool d input list int kernel_size list int stride list int padding list int dilation list int ceil_mode bool assert len kernel_size == len kernel_size == max_pool d kernel_size must either single int tuple two ints kH = kernel_size kW = kH len kernel_size == kernel_size assert len stride == len stride == len stride == max_pool d stride must either omitted single int tuple two ints dH = kH len stride == stride len stride == dW = kW len stride == dW = dH dW = stride assert len padding == len padding == max_pool d padding must either single int tuple two ints padH = padding padW = padH len padding == padding assert len dilation == len dilation == max_pool d dilation must either single int tuple two ints dilationH = dilation dilationW = dilationH len dilation == dilation assert len input == len input == nbatch = input - len input == nInputPlane = input - inputHeight = input - inputWidth = input - outputHeight = pooling_output_shape inputHeight kH padH dH dilationH ceil_mode outputWidth = pooling_output_shape inputWidth kW padW dW dilationW ceil_mode pool d_shape_check input kH kW dH dW padH padW dilationH dilationW nInputPlane inputHeight inputWidth outputHeight outputWidth len input == nInputPlane outputHeight outputWidth nbatch nInputPlane outputHeight outputWidth max_pool d_with_indices input list int kernel_size list int stride list int padding list int dilation list int ceil_mode bool out = max_pool d input kernel_size stride padding dilation ceil_mode out out upsample_nearest d input list int output_size Optional list int scale_factors Optional list float out list int = out append input out append input scale_factors None output_size None assert Either output_size scale_factors must presented output_size None assert scale_factors None Must specify exactly one output_size scale_factors assert len output_size == out append output_size out append output_size scale_factors None assert output_size None Must specify exactly one output_size scale_factors assert len scale_factors == out append int input scale_factors out append int input scale_factors out mm list int mat list int assert len == must matrix assert len mat == mat must matrix assert == mat mat dot list int tensor list int assert len == len tensor == assert == tensor out list int = out mv list int vec list int assert len == len vec == assert == vec TODO unsqueeze li list int dim int dim = maybe_wrap_dim dim len li + out = _copy li out insert dim out squeeze_nodim li list int out list int = i range len li li i = out append li i out squeeze li list int dim int out list int = wrapped_dim = maybe_wrap_dim dim len li i range len li i == wrapped_dim li i = out append li i out append li i out squeeze_dims li list int dims list int len dims == li wrapped_dims = _copy dims i range len dims wrapped_dims i = maybe_wrap_dim wrapped_dims i len li result list int = i range len li li i == i wrapped_dims result append li i result append li i result index_select list int dim int index list int dim = maybe_wrap_dim dim len numel = multiply_integers index assert len index = assert dim == dim len result_size list int = i range len dim == i result_size append numel result_size append i result_size embedding weight list int indices list int padding_idx int = - scale_grad_by_freq bool = False sparse bool = False assert len weight == len indices == index_select weight indices size = _copy indices size append weight size max_int slice list int dim int start Optional int end Optional int step int ndim = len assert ndim = dim = maybe_wrap_dim dim ndim start_val = start start None end_val = end end None max_int assert step start_val == max_int start_val = start_val start_val += dim end_val end_val += dim start_val start_val = start_val dim start_val = dim end_val start_val end_val = start_val end_val = dim end_val = dim slice_len = end_val - start_val out = _copy out dim = slice_len + step - step out check_cat_no_zero_dim tensors list list int tensor tensors assert len tensor legacy_cat_wrap_dim dim int tensor_sizes list list int out_dim Optional int = None size tensor_sizes len size == size == out_dim None out_dim = maybe_wrap_dim dim len size out_dim None out_dim = dim out_dim should_skip tensor list int numel tensor == len tensor == check_cat_shape_except_dim first list int second list int dimension int index int first_dims = len first second_dims = len second assert first_dims == second_dims Tensors must have same number dimensions dim range first_dims dim = dimension assert first dim == second dim Sizes tensors must match except dimension cat tensors list list int dim int check_cat_no_zero_dim tensors dim = legacy_cat_wrap_dim dim tensors assert len tensors not_skipped_tensor Optional list int = None tensor tensors should_skip tensor not_skipped_tensor = tensor not_skipped_tensor None cat_dim_size = i range len tensors tensor = tensors i should_skip tensor check_cat_shape_except_dim not_skipped_tensor tensor dim i cat_dim_size = cat_dim_size + tensor dim result_size = _copy not_skipped_tensor result_size dim = cat_dim_size result_size stack tensors list list int dim int unsqueezed_tensors list list int = tensor tensors unsqueezed = unsqueeze tensor dim unsqueezed_tensors append unsqueezed cat unsqueezed_tensors dim select list int dim int index int ndim = len assert ndim = dim = maybe_wrap_dim dim ndim size = dim assert index -size index = size index index += size out list int = i range ndim i = dim out append i out matmul tensor list int tensor list int dim_tensor = len tensor dim_tensor = len tensor dim_tensor == dim_tensor == dot tensor tensor dim_tensor == dim_tensor == mv tensor tensor dim_tensor == dim_tensor == squeeze mm unsqueeze tensor tensor dim_tensor == dim_tensor == mm tensor tensor dim_tensor = dim_tensor = We multiplying b x n x m x x m x p where b can list we track m vs m separately even though they must match nicer error messages n = tensor - dim_tensor batch_tensor list int = TODO handling slice i range dim_tensor - batch_tensor append tensor i p = tensor - batch_tensor list int = TODO handling slice i range dim_tensor - batch_tensor append tensor i expand batch portion i e cut off matrix dimensions expand rest expand_batch_portion = broadcast batch_tensor batch_tensor todo copy output_shape = expand_batch_portion dim_tensor output_shape append n dim_tensor output_shape append p output_shape assert False both arguments matmul need least D t list int assert len = self_len = len self_len == out list int = out self_len == transpose list int dim int dim int ndims = len dim = maybe_wrap_dim dim ndims dim = maybe_wrap_dim dim ndims dim == dim _copy out list int = i range ndims i == dim out append dim i == dim out append dim out append i out linear input list int weight list int bias Optional list int out = matmul input t weight bias None assert broadcast bias out == out out addmm list int mat list int mat list int beta Any alpha Any broadcast mm mat mat check_non_negative array list int - bool TODO look into rewriting early getting loop unrolling fire non_negative = False val array val non_negative = True non_negative check_shape_forward input list int weight_sizes list int bias Optional list int stride list int padding list int dilation list int groups int k = len input weight_dim = len weight_sizes TODO assertions could expanded error messages assert check_non_negative padding assert check_non_negative stride assert weight_dim == k assert weight_sizes = groups assert weight_sizes groups == only handling transposed assert input == weight_sizes groups assert bias None len bias == bias == weight_sizes i range k assert input i + padding i - = dilation i - weight_sizes i - + handling transposed convolution yet conv_output_size input_size list int weight_size list int bias Optional list int stride list int padding list int dilation list int groups int check_shape_forward input_size weight_size bias stride padding dilation groups has_dilation = len dilation dim = len input_size output_size list int = input_batch_size_dim = weight_output_channels_dim = output_size append input_size input_batch_size_dim output_size append weight_size weight_output_channels_dim d range dim dilation_ = dilation d - has_dilation kernel = dilation_ weight_size d - + output_size append input_size d + padding d - - kernel stride d - + output_size conv d input list int weight list int bias Optional list int stride list int padding list int dilation list int groups int assert len weight == assert len input == conv_output_size input weight bias stride padding dilation groups conv d input list int weight list int bias Optional list int stride list int padding list int dilation list int groups int assert len weight == assert len input == conv_output_size input weight bias stride padding dilation groups conv_backwards grad_output list int input list int weight list int biases Optional list int Bias gradient always generated regardess biases supplied _copy input _copy weight grad_output conv_transpose d_input input list int weight list int bias Optional list int = None stride Optional list int = None padding Optional list int = None output_padding Optional list int = None groups int = dilation Optional list int = None - list int stride None stride = padding None padding = output_padding None output_padding = dilation None dilation = has_dilation = len dilation dim = len input output_size list int = input_batch_size_dim = weight_output_channels_dim = output_size append input input_batch_size_dim output_size append weight weight_output_channels_dim groups d range dim dilation_ = dilation d - has_dilation kernel = dilation_ weight d - output_size append input d - stride d - - padding d - + kernel + output_padding d - + output_size conv_forwards input list int weight list int bias Optional list int stride list int padding list int dilation list int transposed bool output_padding list int groups int - list int has_dilation = len dilation has_output_padding = len output_padding dim = len input output_size list int = input_batch_size_dim = weight_output_channels_dim = transposed output_size append input input_batch_size_dim transposed output_size append weight weight_output_channels_dim groups output_size append weight weight_output_channels_dim d range dim dilation_ = dilation d - has_dilation output_padding_ = output_padding d - has_output_padding transposed kernel = dilation_ weight d - output_size append input d - stride d - - padding d - + kernel + output_padding_ + kernel = dilation_ weight d - + output_size append input d + padding d - - kernel stride d - + output_size _conv_forwards input list int weight list int bias Optional list int stride list int padding list int dilation list int transposed bool output_padding list int groups int benchmark bool deterministic bool cudnn_enabled bool allow_tf bool - list int conv_forwards input weight bias stride padding dilation transposed output_padding groups batch_norm input list int weight Optional list int bias Optional list int running_mean Optional list int running_var Optional list int training bool momentum float eps float cudnn_enabled bool out list int = elem input out append elem out conv d input list int weight list int bias Optional list int stride list int padding list int dilation list int groups int assert len weight == assert len input == conv_output_size input weight bias stride padding dilation groups maybe_wrap_dim dim int dim_post_expr int wrap_scalar bool = True dim_post_expr = assert wrap_scalar dim_post_expr = min = -dim_post_expr max = dim_post_expr - assert dim min dim max dim dim += dim_post_expr dim zero_dim_tensor input Any out list int = out multiply_integers li list int out = elem li out = out elem out arange_end end number inp Any inp Any inp Any inp Any assert end = int math ceil end arange_start start number end number inp Any inp Any inp Any inp Any assert end = assert end = start int math ceil end - start arange_start_step start number end number step number inp Any inp Any inp Any inp Any assert step = step assert start = end assert end = start int math ceil end - start step permute input list int dims list int assert len input == len dims ndim = len dims seen_dims list int = newSizes list int = i range ndim dim = maybe_wrap_dim dims i ndim seen_dims append dim newSizes append input dim i range ndim j range i assert seen_dims i = seen_dims j newSizes movedim list int source list int destination list int - list int self_dim = len self_dim = normalized_src list int = normalized_dst list int = i range len source normalized_src append maybe_wrap_dim source i self_dim normalized_dst append maybe_wrap_dim destination i self_dim order = - i range self_dim src_dims = i i range self_dim dst_dims = i i range self_dim i range len source order normalized_dst i = normalized_src i src_dims normalized_src i = - dst_dims normalized_dst i = - source_dims list int = destination_dims list int = ele src_dims ele = - source_dims append ele ele dst_dims ele = - destination_dims append ele rest_dim = self_dim - len source i range rest_dim order destination_dims i = source_dims i permute order flatten input list int start_dim int end_dim int start_dim = maybe_wrap_dim start_dim len input end_dim = maybe_wrap_dim end_dim len input assert start_dim = end_dim len input == start_dim == end_dim TODO out list int = elem input out append elem out slice_numel = i range start_dim end_dim + slice_numel = input i TODO use slicing when slice optimization has landed slice_numel = multiply_integers input start_dim end_dim - start_dim + shape list int = i range start_dim shape append input i shape append slice_numel i range end_dim + len input shape append input i shape nonzero_lower_bound input list int len input nonzero_upper_bound input list int numel input len input _reduce_along_dim list int dim int keepdim bool dim = maybe_wrap_dim dim len out list int = i self_dim enumerate i == dim keepdim out append out append self_dim out argmax list int dim Optional int = None keepdim bool = False - list int dim None _reduce_along_dim dim keepdim bmm list int mat list int - list int assert len == bmm only supports D tensors assert len mat == bmm only supports D tensors assert == mat mismatching batch dimension assert == mat mismatching contracting dimension mat _shape_as_tensor list int - list int len topk list int k int dim int = - - tuple list int list int len == result list int = assert k = dim f k k too big dimension dim size dim result = _copy result dim = k result result nll_loss_forward list int target list int weight Optional list int reduction int - tuple list int list int This taken shamelessly meta function LossNLL cpp self_dim = len target_dim = len target assert self_dim = assert target_dim = no_batch_dim = self_dim == target_dim == assert no_batch_dim == target n_classes = - scalar_shape list int = assert weight None len weight == weight == n_classes reduction == self_dim == reduction_shape = reduction_shape = scalar_shape reduction_shape scalar_shape native_layer_norm input list int normalized_shape list int - tuple list int list int list int reduction_shape list int = num_unreduced_dimensions = len input - len normalized_shape assert num_unreduced_dimensions = i range num_unreduced_dimensions reduction_shape append input i i range num_unreduced_dimensions len input reduction_shape append _copy input reduction_shape reduction_shape native_batch_norm input list int weight Optional list int bias Optional list int running_mean Optional list int running_var Optional list int training bool - tuple list int list int list int training _size = input _size = _copy input _size _size _batch_norm_with_update input list int weight Optional list int bias Optional list int running_mean Optional list int running_var Optional list int - tuple list int list int list int list int _size = input _copy input _size _size cross_entropy_loss list int target list int weight Optional list int = None reduction int = ignore_index int = - label_smoothing float = - list int result_shape = nll_loss_forward target weight reduction result_shape Currently deferring enabling part propoasal suspend adding ops There currently cases test case where being called SSA opinfo tests unexpected values eg list two ints see first opinfo test The behavior index significantly dependent inputs This could error how we matching up shape functions function needs just implement everything index_Tensor List int indices List Optional List int - List int assert len indices = len More indices than dimensions index broadcasted_shape List int = index_tensor_shape indices index_tensor_shape None broadcasted_shape = broadcast broadcasted_shape index_tensor_shape broadcasted_shape ScriptFn = torch _C ScriptFunction shape_compute_graph_mapping dict str ScriptFn = bounded_compute_graph_mapping dict str tuple ScriptFn ScriptFn = script_func_map dict Callable ScriptFn = process_func func Callable func script_func_map scripted_func = torch jit script func torch _C _jit_pass_inline scripted_func graph _ range torch _C _jit_pass_peephole scripted_func graph torch _C _jit_pass_constant_propagation scripted_func graph script_func_map func = scripted_func script_func_map func add_shape_compute_mapping operator_schema str func Callable global shape_compute_graph_mapping shape_compute_graph_mapping operator_schema = process_func func add_bounded_compute_mapping operator_schema str lower_bound_func Callable upper_bound_func Callable Adds shape compute function both upper lower bounds fns = process_func lower_bound_func process_func upper_bound_func bounded_compute_graph_mapping operator_schema = fns add_shape_compute_mapping aten contiguous Tensor MemoryFormat memory_format=contiguous_format - Tensor unary add_shape_compute_mapping aten rsub Tensor Tensor Scalar other Scalar alpha= - Tensor unary add_shape_compute_mapping aten dropout Tensor input float p bool train - Tensor unary add_shape_compute_mapping aten adaptive_avg_pool d Tensor int output_size - Tensor adaptive_avg_pool d add_shape_compute_mapping prim NumToTensor Scalar Scalar - Tensor zero_dim_tensor add_shape_compute_mapping prim NumToTensor bool bool - Tensor zero_dim_tensor add_shape_compute_mapping aten zeros int size int dtype=None int layout=None Device device=None bool pin_memory=None - Tensor unary add_shape_compute_mapping aten dtype Tensor int dtype bool non_blocking=False bool copy=False int memory_format=None - Tensor unary add_shape_compute_mapping aten arange Scalar end int dtype=None int layout=None Device device=None bool pin_memory=None - Tensor arange_end add_shape_compute_mapping aten arange start Scalar start Scalar end ScalarType dtype=None Layout layout=None Device device=None bool pin_memory=None - Tensor arange_start add_shape_compute_mapping aten arange start_step Scalar start Scalar end Scalar step ScalarType dtype=None Layout layout=None Device device=None bool pin_memory=None - Tensor arange_start_step add_shape_compute_mapping aten squeeze Tensor - Tensor squeeze_nodim add_shape_compute_mapping aten squeeze dim Tensor int dim - Tensor squeeze add_shape_compute_mapping aten squeeze dims Tensor int dim - Tensor squeeze_dims add_shape_compute_mapping aten unsqueeze Tensor int dim - Tensor unsqueeze add_shape_compute_mapping aten slice Tensor Tensor int dim= int start=None int end=None int step= - Tensor slice add_shape_compute_mapping aten select int Tensor int dim int index - Tensor select add_shape_compute_mapping aten index_select Tensor int dim Tensor index - Tensor index_select add_shape_compute_mapping aten layer_norm Tensor input int normalized_shape Tensor weight=None Tensor bias=None float eps= e- bool cudnn_enable=True - Tensor unary add_shape_compute_mapping aten softmax int Tensor int dim ScalarType dtype=None - Tensor unary add_shape_compute_mapping aten _no_grad_embedding_renorm_ Tensor weight Tensor input float max_norm float norm_type - Tensor unary add_shape_compute_mapping aten embedding_renorm_ Tensor Tensor indices float max_norm float norm_type - Tensor unary add_shape_compute_mapping aten embedding Tensor weight Tensor indices int padding_idx=- bool scale_grad_by_freq=False bool sparse=False - Tensor embedding add_shape_compute_mapping aten mm Tensor Tensor mat - Tensor mm add_shape_compute_mapping aten dot Tensor Tensor tensor - Tensor dot add_shape_compute_mapping aten mv Tensor Tensor vec - Tensor mv add_shape_compute_mapping aten matmul Tensor Tensor other - Tensor matmul add_shape_compute_mapping aten linear Tensor input Tensor weight Tensor bias=None - Tensor linear add_shape_compute_mapping aten max_pool d Tensor int kernel_size int stride= int padding= int dilation= bool ceil_mode=False - Tensor max_pool d add_shape_compute_mapping aten max_pool d_with_indices Tensor int kernel_size int stride= int padding= int dilation= bool ceil_mode=False - Tensor Tensor max_pool d_with_indices add_shape_compute_mapping aten t Tensor - Tensor t add_shape_compute_mapping aten transpose int Tensor int dim int dim - Tensor transpose add_shape_compute_mapping aten conv d Tensor input Tensor weight Tensor bias=None int stride= int padding= int dilation= int groups= - Tensor conv d add_shape_compute_mapping aten conv d Tensor input Tensor weight Tensor bias=None int stride= int padding= int dilation= int groups= - Tensor conv d add_shape_compute_mapping aten batch_norm Tensor input Tensor weight Tensor bias Tensor running_mean Tensor running_var bool training float momentum float eps bool cudnn_enabled - Tensor batch_norm add_shape_compute_mapping aten conv d Tensor input Tensor weight Tensor bias=None int stride= int padding= int dilation= int groups= - Tensor conv d add_shape_compute_mapping aten convolution_backward Tensor grad_output Tensor input Tensor weight int bias_sizes int stride int padding int dilation bool transposed int output_padding int groups bool output_mask - Tensor Tensor Tensor conv_backwards add_shape_compute_mapping aten convolution Tensor input Tensor weight Tensor bias int stride int padding int dilation bool transposed int output_padding int groups - Tensor conv_forwards add_shape_compute_mapping aten _convolution Tensor input Tensor weight Tensor bias int stride int padding int dilation bool transposed int output_padding int groups bool benchmark bool deterministic bool cudnn_enabled bool allow_tf - Tensor _conv_forwards add_shape_compute_mapping aten conv_transpose d input Tensor input Tensor weight Tensor bias=None int stride= int padding= int output_padding= int groups= int dilation= - Tensor conv_transpose d_input add_shape_compute_mapping aten flatten using_ints Tensor int start_dim= int end_dim=- - Tensor flatten add_shape_compute_mapping aten cat Tensor tensors int dim= - Tensor cat add_shape_compute_mapping aten stack Tensor tensors int dim= - Tensor stack add_shape_compute_mapping aten permute Tensor int dims - Tensor permute add_shape_compute_mapping aten movedim intlist Tensor int source int destination - Tensor movedim add_shape_compute_mapping aten view Tensor int size - Tensor view add_shape_compute_mapping aten expand_as Tensor Tensor other - Tensor expand add_shape_compute_mapping aten expand Tensor int size bool implicit=False - Tensor expand_one_unused add_shape_compute_mapping aten mean dim Tensor int dim bool keepdim=False ScalarType dtype=None - Tensor sum_mean_dim add_shape_compute_mapping aten sum dim_IntList Tensor int dim bool keepdim=False ScalarType dtype=None - Tensor sum_mean_dim add_shape_compute_mapping aten max dim Tensor int dim bool keepdim=False - Tensor values Tensor indices max_dim add_shape_compute_mapping aten mean Tensor ScalarType dtype=None - Tensor zero_dim_tensor add_shape_compute_mapping aten sum Tensor ScalarType dtype=None - Tensor zero_dim_tensor add_shape_compute_mapping aten addmm Tensor Tensor mat Tensor mat Scalar beta= Scalar alpha= - Tensor addmm add_shape_compute_mapping aten upsample_nearest d vec Tensor input int output_size float scale_factors - Tensor upsample_nearest d add_shape_compute_mapping aten quantize_per_tensor Tensor float scale int zero_point ScalarType dtype - Tensor unary add_shape_compute_mapping aten quantize_per_tensor tensor_qparams Tensor Tensor scale Tensor zero_point ScalarType dtype - Tensor unary add_shape_compute_mapping aten dequantize Tensor - Tensor unary add_shape_compute_mapping quantized add Tensor qa Tensor qb float scale int zero_point - Tensor qc broadcast add_shape_compute_mapping aten argmax Tensor int dim=None bool keepdim=False - Tensor argmax add_shape_compute_mapping aten bmm Tensor Tensor mat - Tensor bmm add_shape_compute_mapping aten _shape_as_tensor Tensor - Tensor _shape_as_tensor add_shape_compute_mapping aten topk Tensor int k int dim=- bool largest=True bool sorted=True - Tensor values Tensor indices topk add_shape_compute_mapping aten nll_loss_forward Tensor Tensor target Tensor weight int reduction int ignore_index - Tensor output Tensor total_weight nll_loss_forward add_shape_compute_mapping aten native_layer_norm Tensor input int normalized_shape Tensor weight Tensor bias float eps - Tensor Tensor Tensor native_layer_norm add_shape_compute_mapping aten native_batch_norm Tensor input Tensor weight Tensor bias Tensor running_mean Tensor running_var bool training float momentum float eps - Tensor Tensor Tensor native_batch_norm add_shape_compute_mapping aten _native_batch_norm_legit Tensor input Tensor weight Tensor bias Tensor running_mean Tensor running_var bool training float momentum float eps - Tensor Tensor Tensor native_batch_norm add_shape_compute_mapping aten _native_batch_norm_legit no_stats Tensor input Tensor weight Tensor bias Tensor running_mean Tensor running_var bool training float momentum float eps - Tensor Tensor Tensor native_batch_norm add_shape_compute_mapping _batch_norm_with_update Tensor input Tensor weight Tensor bias Tensor running_mean Tensor b running_var float momentum float eps - Tensor Tensor Tensor Tensor _batch_norm_with_update add_shape_compute_mapping aten cross_entropy_loss Tensor Tensor target Tensor weight=None int reduction=Mean SymInt ignore_index=- float label_smoothing= - Tensor cross_entropy_loss add_shape_compute_mapping aten index Tensor Tensor Tensor indices - Tensor index_Tensor TODO migrate over all symbolic_shape_registry_util cpp These duplicated here so functions will serialized add_shape_compute_mapping aten lerp Tensor Tensor Tensor end Tensor weight - Tensor broadcast_three add_shape_compute_mapping aten where ScalarSelf Tensor condition Scalar Tensor other - Tensor broadcast_one_three add_shape_compute_mapping aten add_ Tensor Tensor Tensor other Scalar alpha= - Tensor broadcast_inplace quantized_conv_prepack TODO Shape Compute Fn upper lower bounds add_bounded_compute_mapping aten nonzero Tensor - Tensor nonzero_lower_bound nonzero_upper_bound