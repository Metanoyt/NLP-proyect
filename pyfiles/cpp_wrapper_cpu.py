mypy allow-untyped-defs __future__ annotations ctypes functools math os sys textwrap itertools chain count typing Any Callable Optional Protocol TYPE_CHECKING Union sympy torch torch _higher_order_ops torchbind torch _inductor async_compile noqa F required warm up AsyncCompile pools torch _ops torch _inductor runtime runtime_utils dynamo_timed torch fx experimental symbolic_shapes ConvertIntKey DivideByKey SymTypes torch utils _ordered_set OrderedSet torch utils _sympy symbol symbol_is_type SymT config cpp_builder ir ir ExternKernel utils _align DeferredLineBase LineContext normalize_name virtualized V aoti_hipify_utils maybe_hipify_code_wrapper common get_device_op_overrides IndentedBuffer Kernel cpp_utils cexpr DEVICE_TO_ATEN DEVICE_TO_INT DTYPE_TO_ATEN DTYPE_TO_CPP wrapper EnterSubgraphLine ExitSubgraphLine PythonWrapperCodegen SymbolicCallArg TYPE_CHECKING collections abc Sequence graph GraphLowering At most list nesting can go one layer deep _OUTPUT_ARGS_TYPE = list Union Optional str list Optional str scheduler BaseSchedulerNode HasWriteLine Protocol writeline line Union LineContext DeferredLineBase str - None CppWrapperCpu PythonWrapperCodegen Generates cpp wrapper running CPU calls cpp kernels __init__ hasattr device device = cpu must initialized prior calling super __init__ included_devices OrderedSet str = OrderedSet model_class_name_suffix = config aot_inductor dynamic_linkage config aot_inductor model_name_for_generated_files aoti_model_class_name = f AOTInductorModel model_class_name_suffix super __init__ declare = auto declare_maybe_reference = decltype auto ending = comment = none_str = nullptr supports_intermediate_hooks = False kernel_callsite_id = count int_array_id = count int array local variable declarations declared_int_array_vars OrderedSet str = OrderedSet tmp_tensor_id = count tmp tensor local variable declarations arg_var_id = count used_cached_devices OrderedSet str = OrderedSet used_cached_dtypes OrderedSet str = OrderedSet used_cached_layouts OrderedSet str = OrderedSet used_cached_memory_formats OrderedSet str = OrderedSet used_cond_predicate OrderedSet str = OrderedSet cached_output_id = count scalar_to_tensor_id = count custom_op_wrapper_loaded = False For GEMM kernels must initialized resolved linking initialized_kernels dict str Kernel = device_codegen = get_device_op_overrides device only need include each header once include_extra_header = functools lru_cache None type ignore method-assign _include_extra_header staticmethod create is_subgraph bool subgraph_name Optional str parent_wrapper Optional PythonWrapperCodegen partition_signatures Optional ir GraphPartitionSignature = None TODO - support subgraph codegen lifting functions Check comment CppWrapperCpu ` codegen_subgraph ` function CppWrapperCpu staticmethod _generate_temporary_array_pointer c_type str elements Sequence str force_mutable bool = False - str Get pointer array only exists duration C++ statement s used If c_type already pointer mutable pointer array Otherwise const pointer In C-shim API pointer types only const-qualified respect underlying value any nested pointers e g const double possible const double const This means array containing pointers must _already_ properly const-qualified c_type add additional const-ness MSVC does support implicitly converting const iterator const pointer ptr_call = data force_mutable c_type endswith cpp_builder is_msvc_cl cbegin f std array c_type len elements join elements ptr_call _generate_kernel_call_helper kernel_name str call_args device=None triton=True arg_types=None raw_keys=None raw_args=None triton_meta=None graph_name= original_fxnode_name=None Generates kernel call code triton Defines whether GPU backend uses Triton codegen Otherwise uses CUDA language codegen Only valid when cuda == True assert arg_types None len call_args == len arg_types Mismatch call_args arg_types generate_kernel_call \n f call_args call_args \n f arg_types arg_types new_args = idx arg enumerate call_args arg_types idx new_args append f arg_types idx arg data_ptr arg scalar new_args append arg debug printer related logic cpp kernel type debug_printer_manager = V graph wrapper_code debug_printer debug_printer_manager set_printer_args call_args kernel_name None None cpp debug_printer_manager writeline wrap_kernel_call kernel_name new_args write_constant name hashed include hash so our code cache gives different constants different files header writeline f name hashed staticmethod get_device_include_path device str - str V graph aot_mode f #include torch csrc inductor aoti_include device h f #include torch csrc inductor cpp_wrapper device h add_device_include device str - None device included_devices included_devices add device Add default header device plus any C-shim extensions present header splice get_device_include_path device extend_aoti_c_shim_include = f torch csrc inductor aoti_torch generated extend c_shim_ device h extend_aoti_c_shim_path = os path join os path dirname torch __file__ include extend_aoti_c_shim_include os path exists extend_aoti_c_shim_path header splice f #include extend_aoti_c_shim_include write_header V graph is_const_graph We do write header constant graph will written main module V graph aot_mode header splice torch torch _inductor codecache CppWrapperCodeCache cpp_wrapper_src = r add_device_include device V graph aot_mode config aot_inductor dynamic_linkage open os path join os path dirname __file__ aoti_runtime interface cpp f header splice f read we produce separate model header each model static linkage header splice f #include \ model_class_name_suffix h\ header splice \n config cpp enable_kernel_profile header splice #include torch csrc inductor aoti_runtime kernel_context_tls h header splice namespace torch aot_inductor thread_local KernelContext tls_kernel_context = nullptr _include_extra_header header str This needed cpp python dtype conversion header splice f #include header mark_output_type mark output type unwrap tensor back python scalar ir ShapeAsConstantBuffer output_is_tensor = idx x enumerate V graph graph_outputs isinstance x ShapeAsConstantBuffer output_is_tensor idx = False output_is_tensor idx = True output_is_tensor = output_is_tensor write_prefix V graph is_const_graph We do write prefix constant graph will written main module config aot_inductor custom_ops_to_c_shims custom_ops_to_c_shims contains declaration custom ops C shim TODO could auto-generated passed-in custom op schema custom_c_shims = list chain config aot_inductor custom_ops_to_c_shims values declarations = \n join f extern textwrap dedent shim shim custom_c_shims prefix splice f extern C declarations V graph aot_mode prefix writeline namespace torch aot_inductor write_input_output_info info_kind str idx int name str prefix writeline f info_kind idx name = name codegen_input_symbol_assignment name str value ir TensorBox bound_vars OrderedSet sympy Symbol code = prefix functools cache sizeof name codegen_input_size_var_decl code name f name _size functools cache strideof name codegen_input_stride_var_decl code name f name _stride codegen_symbol sym_or_exp Union sympy Symbol sympy Expr base_name str name_fn Callable str str dim int isinstance sym_or_exp sympy Symbol sym_or_exp bound_vars code writeline f int _t sym_or_exp = name_fn base_name dim bound_vars add sym_or_exp isinstance sym_or_exp sympy Expr undefined_symbols = sym sym sym_or_exp free_symbols sym bound_vars len undefined_symbols = Skip expression contains no symbols multiple symbols exists since we assume each base symbol defined other codegen_symbol calls torch utils _sympy solve try_solve free_symbol = undefined_symbols pop base_name = name_fn base_name Use size symbol solve free symbol size_symbol = sympy Symbol f base_name _ dim integer=True code writeline f int _t size_symbol = base_name dim solution = try_solve sympy Eq sym_or_exp size_symbol free_symbol solution None code writeline f int _t free_symbol = cexpr solution bound_vars add free_symbol raise AssertionError str sympy Eq sym_or_exp size_symbol + solvable isinstance value sympy Expr isinstance value sympy Symbol value bound_vars value is_integer decl = int _t value is_float decl = double raise AssertionError Unexpected symbol type code writeline f decl value = name bound_vars add value isinstance value ir TensorBox dim size enumerate value get_size codegen_symbol size name sizeof dim dim stride enumerate value get_stride codegen_symbol stride name strideof dim isinstance value ir TorchBindObject torchbind objects loaded proxy executor pass raise AssertionError f Unknown value type type value generate_input_output_runtime_checks In debug_compile mode we generate checks ensure dtype shape stride device each real input output tensor match ones provided compile time via sample input output gen_check handle_kind idx name tensor Wrap AtenTensorHandle ConstantHandle cleaner utility function access prefix writeline f ConstantHandle name = ConstantHandle handle_kind idx codegen_tensor_dtype_var_decl prefix name expected_dtype_name = DTYPE_TO_ATEN tensor dtype dtype_str = str tensor dtype split - prefix splice f int _t name _expected_dtype = aoti_torch_dtype_ dtype_str name _expected_dtype = name _dtype std stringstream ss ss handle_kind idx unmatched dtype expected name _expected_dtype expected_dtype_name got name _dtype \\n throw std runtime_error ss str codegen_input_size_var_decl prefix name dim_idx d enumerate tensor get_size isinstance d int sympy Integer prefix splice f d = name _size dim_idx std stringstream ss ss handle_kind idx unmatched dim value dim_idx expected d got name _size dim_idx \\n throw std runtime_error ss str torch utils _sympy value_ranges bound_sympy sym_range = bound_sympy d V graph sizevars shape_env var_to_range math isinf sym_range lower prefix splice f name _size dim_idx sym_range lower std stringstream ss ss handle_kind idx dim value too small dim_idx expected = sym_range lower got name _size dim_idx \\n throw std runtime_error ss str math isinf sym_range upper Limit upper bound max C long long value ^ - max_long_long = ctypes c_longlong - value upper_bound = min sym_range upper max_long_long prefix splice f name _size dim_idx upper_bound std stringstream ss ss handle_kind idx dim value too large dim_idx expected = upper_bound got name _size dim_idx \\n throw std runtime_error ss str codegen_input_stride_var_decl prefix name stride_idx s enumerate tensor get_stride isinstance s int sympy Integer continue prefix splice f s = name _stride stride_idx std stringstream ss ss handle_kind idx unmatched stride value stride_idx expected s got name _stride stride_idx \\n throw std runtime_error ss str check input device type isinstance tensor ir TensorBox tensor_device = tensor get_device tensor_device None expected_device_type = DEVICE_TO_INT get tensor_device type expected_device_type None codegen_input_device_type_var_decl prefix name device_type_str = str tensor_device type prefix splice f int _t name _expected_device_type = expected_device_type name _expected_device_type = name _device_type std stringstream ss ss handle_kind idx unmatched device type expected name _expected_device_type expected_device_type device_type_str got name _device_type \\n throw std runtime_error ss str Create separate function each input check avoid too big optimize error idx name tensor enumerate V graph graph_inputs items prefix splice f AOTI_NOINLINE static void check_input_ idx AtenTensorHandle input_handles prefix indent gen_check input_handles idx name tensor prefix writeline force noinline avoid any potential compilation slowdown due aggressive inline done host compiler prefix splice static bool _check_aoti_runtime_check_inputs_env const static char env_var_value = getenv AOTI_RUNTIME_CHECK_INPUTS const static bool result = env_var_value = nullptr env_var_value = result AOTI_NOINLINE static void __check_inputs_outputs AtenTensorHandle input_handles AtenTensorHandle output_handles _check_aoti_runtime_check_inputs_env prefix indent idx range len V graph graph_inputs prefix writeline f check_input_ idx input_handles prefix writeline write_wrapper_decl inputs_len = len V graph graph_inputs keys V graph aot_mode codegen_additional_funcs V graph const_module header splice V graph const_module wrapper_code header assert V graph const_wrapper_code None prefix splice V graph const_wrapper_code assert V graph const_kernel_code None kernel_declarations splice V graph const_kernel_code V graph is_const_graph prefix splice f void aoti_model_class_name _const_run_impl std vector AtenTensorHandle output_handles DeviceStreamType stream AOTIProxyExecutorHandle proxy_executor config aot_inductor use_runtime_constant_folding If we do split constant graph we ll just create empty implementation when wrapping main module prefix splice f void aoti_model_class_name _const_run_impl std vector AtenTensorHandle output_handles DeviceStreamType stream AOTIProxyExecutorHandle proxy_executor run_impl_proto = f void aoti_model_class_name run_impl AtenTensorHandle input_handles array input AtenTensorHandle handles stolen array itself borrowed AtenTensorHandle output_handles array writing output AtenTensorHandle handles will stolen caller array itself borrowed DeviceStreamType stream AOTIProxyExecutorHandle proxy_executor __check_inputs_outputs input_handles output_handles generate_input_output_runtime_checks prefix splice run_impl_proto cpp entry function JIT cpp wrapper prefix splice void inductor_entry_impl AtenTensorHandle input_handles array input AtenTensorHandle handles stolen array itself borrowed AtenTensorHandle output_handles array writing output AtenTensorHandle handles will stolen caller array itself borrowed prefix indent assign inputs outputs both cases so later codegen can simplified V graph is_const_graph V graph aot_mode num_args = len V graph graph_inputs Weights promoted JIT mode num_args = len V graph graph_inputs + len V graph constants release GIL support multiple instances inference different threads same process prefix splice py gil_scoped_release_simple release prefix splice f auto inputs = steal_from_raw_handles_to_raii_handles input_handles num_args inputs_len = idx input_key enumerate V graph graph_inputs keys unwrap input tensor back scalar isinstance V graph graph_inputs input_key sympy Expr graph may_get_constant_buffer_dtype dtype = may_get_constant_buffer_dtype V graph graph_inputs input_key type ignore arg-type assert dtype None Fails get dtype sympy Expr codegen_tensor_item dtype f inputs idx input_key prefix prefix writeline f auto input_key = std move inputs idx debug printing all input args AOTI model debug_printer_manager = V graph wrapper_code debug_printer debug_printer_manager codegen_model_inputs_value_print input_args_to_print= input_key input_key V graph graph_inputs keys input_key startswith arg assert all isinstance v torch Tensor v list V graph constants values Expect all constants Tensor idx constants_key enumerate V graph constants keys V graph aot_mode Weights stored constants_ owned ConstantHandle there Don t call std move here because will cause constants_ lose ownership prefix writeline f maybe_unused auto constants_key = constants_- idx Append constants inputs graph constants_idx = inputs_len + idx prefix writeline f maybe_unused auto constants_key = std move inputs constants_idx codegen_inputs V graph aot_mode V graph is_const_graph prefix writeline inputs clear prefix writeline maybe_unused auto kernels = static_cast AOTInductorModelKernels this- kernels_ get codegen_tensor_dtype_var_decl code IndentedBuffer name code writeline f int _t name _dtype code writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_get_dtype name name _dtype codegen_input_size_var_decl code IndentedBuffer name code writeline f auto name _size = name sizes codegen_input_stride_var_decl code IndentedBuffer name code writeline f auto name _stride = name strides codegen_input_device_type_var_decl code IndentedBuffer name code writeline f int _t name _device_type code writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_get_device_type name name _device_type codegen_additional_funcs pass codegen_model_kernels prefix writeline namespace Tell compiler we need link non-mangled symbols kernel initialized_kernels values assert hasattr kernel get_signature f kernel must have get_signature implemented signature = kernel get_signature prefix writeline f extern C signature prefix writeline AOTInductorModelKernels public AOTInductorModelKernelsBase prefix writeline public declare_kernel = OrderedSet src_to_kernel values - OrderedSet initialized_kernels keys declare_kernel update entry entry user_defined_kernel_cache values V graph const_module declare_kernel update V graph const_module wrapper_code src_to_kernel values kernel sorted declare_kernel prefix writeline maybe_hipify_code_wrapper f device_codegen cpp_kernel_type kernel nullptr name kernel initialized_kernels items assert hasattr kernel get_signature f kernel must have get_signature implemented kernel_ptr = f name signature = kernel get_signature replace name kernel_ptr prefix writeline f signature = torch aot_inductor name prefix writeline prefix writeline namespace\n\n config aot_inductor embed_kernel_binary prefix writeline extern C name sorted declare_kernel prefix writeline f extern const unsigned char __ name _start torch xpu is_available prefix writeline f extern const unsigned char __ name _end prefix writeline MSVC string longer than limit single-byte characters https learn microsoft com en-us cpp error-messages compiler-errors- compiler-error-c MSVC_C _MAX_STRING_LENGTH = codegen_write_arg_with_large_length_string arg_name str arg_str_val str max_truncate_length int = MSVC_C _MAX_STRING_LENGTH truncate_string s str length int - list str s i i + length i range len s length len arg_str_val max_truncate_length truncated_strs = truncate_string arg_str_val max_truncate_length prefix writeline f arg_name = truncate_str truncated_strs prefix writeline f R truncate_str prefix writeline prefix writeline f arg_name = R arg_str_val codegen_model_constructor Generated code example AOTInductorModel AOTInductorModel AOTInductorModelBase inputs_info_ name = input inputs_info_ dtype = torch float constants_info_ name = L__self___weight constants_info_ dtype = kFloat constants_info_ offset = constants_info_ data_size = constants_info_ shape = constants_info_ stride = outputs_info_ name = output outputs_info_ dtype = torch float num_inputs = len V graph graph_inputs num_outputs = len V graph graph_outputs num_constants = len V graph constants include_weights = true config aot_inductor package_constants_in_so config aot_inductor package_constants_on_disk_format = binary_blob false prefix splice f aoti_model_class_name aoti_model_class_name std shared_ptr ConstantMap constants_map std shared_ptr std vector ConstantHandle constants_array const std string device_str std optional std string cubin_dir AOTInductorModelBase num_inputs num_outputs num_constants device_str std move cubin_dir include_weights prefix indent idx name inp enumerate V graph graph_inputs items assert isinstance inp sympy Expr f input name= cannot symbolic write_input_output_info inputs_info_ idx name all_cuda = all V graph get_original_value_of_constant name is_cuda name V graph constants keys name V graph folded_constants idx name enumerate V graph constants keys tensor = V graph get_original_value_of_constant name assert isinstance tensor torch Tensor prefix writeline f constants_info_ idx name = name prefix writeline f constants_info_ idx dtype = static_cast int _t codegen_dtype tensor dtype prefix writeline f constants_info_ idx offset = tensor storage_offset If constants serialize contain cpu tensors we always align data_size When loading constants valid data will depends size data_size so there won t correctness issue data_size = torch ops mkldnn _nbytes tensor tensor is_mkldnn tensor untyped_storage nbytes prefix writeline f constants_info_ idx data_size = data_size all_cuda _align data_size from_folded = true name V graph folded_constants false prefix writeline f constants_info_ idx from_folded = from_folded name V graph folded_constants constant_type_str = FoldedConstant name startswith _tensor_constant constant_type_str = TensorConstant any name == normalize_name parameter_name parameter_name V graph named_parameters constant_type_str = Parameter any name == normalize_name buffer_name buffer_name V graph named_buffers constant_type_str = Buffer constant_type_str = Unknown prefix writeline f constants_info_ idx type = static_cast int _t torch aot_inductor ConstantType constant_type_str size_str = join str s s tensor size prefix writeline f constants_info_ idx shape = size_str stride_str = join str s s tensor stride prefix writeline f constants_info_ idx stride = stride_str prefix writeline f constants_info_ idx layout = static_cast int _t codegen_layout tensor layout tensor is_mkldnn opaque_metadata_tensor = torch ops mkldnn _get_mkldnn_serialized_md tensor assert opaque_metadata_tensor dim == Expect opaque_metadata_tensor -D opaque_metadata_list = opaque_metadata_tensor tolist opaque_metadata_str = codegen_shape_tuple opaque_metadata_list prefix writeline f constants_info_ idx opaque_metadata = opaque_metadata_str name V graph dynamo_flat_name_to_original_fqn original_fqn = V graph dynamo_flat_name_to_original_fqn get name name name V graph allocated_constant_name original_fqn = V graph allocated_constant_name name raise AssertionError original_fqn must set constant prefix writeline f constants_info_ idx original_fqn = original_fqn prefix writeline update_constants_map std move constants_map prefix writeline update_constants_array std move constants_array escape_string x x replace \\ \\\\ replace \\ replace \n \\n replace \t \\t Origin code prefix writeline f in_spec_ = R config aot_inductor serialized_in_spec Fix msvc C error via codegen_write_arg_with_large_length_string codegen_write_arg_with_large_length_string arg_name= in_spec_ arg_str_val=config aot_inductor serialized_in_spec Origin code prefix writeline f out_spec_ = R config aot_inductor serialized_out_spec Fix msvc C error via codegen_write_arg_with_large_length_string codegen_write_arg_with_large_length_string arg_name= out_spec_ arg_str_val=config aot_inductor serialized_out_spec idx output enumerate V graph graph_outputs assert isinstance output sympy Expr f output name= cannot symbolic name = f output idx write_input_output_info outputs_info_ idx name prefix writeline this- kernels_ = std make_unique AOTInductorModelKernels prefix writeline codegen_const_run_driver Generated code example std unordered_map std string AtenTensorHandle AOTInductorModel const_run_impl DeviceStreamType stream AOTIProxyExecutorHandle proxy_executor bool initialization std unordered_map std string AtenTensorHandle folded_constants_map std vector AtenTensorHandle output_handles build up output_handles over here _const_run_impl output_handles stream proxy_executor build up folded_constants_map folded_constants_map prefix splice f std unordered_map std string AtenTensorHandle aoti_model_class_name const_run_impl DeviceStreamType stream AOTIProxyExecutorHandle proxy_executor bool initialization config aot_inductor use_runtime_constant_folding prefix splice initialization std cerr WARNING Calling constant_folding model compiled config aot_inductor use_runtime_constant_folding=False\\n prefix indent This mapping index constant folding graph s output const_index_mapping list Optional tuple int str = None len V graph const_output_index idx name _ enumerate V graph constants items name V graph const_output_index const_index_mapping V graph const_output_index name = idx name type ignore call-overload assert None const_index_mapping Not all constant gets mapped constant folding graph prefix writeline f std unordered_map std string AtenTensorHandle folded_constants_map folded_constants_map reserve len const_index_mapping std vector AtenTensorHandle output_handles len const_index_mapping prefix splice The below assignment output_handles constants used directly It s only used memo correspondence handle constants output_idx const_idx _ enumerate const_index_mapping type ignore misc prefix writeline f output_handles output_idx = constants_- const_idx prefix writeline _const_run_impl output_handles stream proxy_executor output_idx _ const_name enumerate const_index_mapping type ignore misc prefix writeline f folded_constants_map const_name = output_handles output_idx prefix writeline folded_constants_map prefix writeline generate is_inference dynamo_timed CppWrapperCpu generate log_pt _compile_event=True write_wrapper_decl super generate is_inference finalize_prefix prior = prefix prefix = aot_mode_decls = IndentedBuffer V graph aot_mode V graph is_const_graph aot_mode_decls writeline namespace torch aot_inductor codegen_model_kernels codegen_model_constructor codegen_const_run_driver aot_mode_decls writeline namespace torch aot_inductor aot_mode_decls writeline using namespace torch aot_inductor prefix = cache_decls = IndentedBuffer dtype used_cached_dtypes cache_decls writeline f CACHE_TORCH_DTYPE dtype device used_cached_devices cache_decls writeline f CACHE_TORCH_DEVICE device layout used_cached_layouts cache_decls writeline f CACHE_TORCH_LAYOUT layout memory_format used_cached_memory_formats cache_decls writeline f CACHE_TORCH_MEMORY_FORMAT memory_format prefix splice aot_mode_decls prefix splice prior _define_kernel_helper kernel_name str kernel_body str metadata Optional str = None gpu bool = False cpp_definition Optional str = None cpp_definition None header splice cpp_definition kernel_declarations splice f \n kernel_body \n header splice f \n kernel_body \n codegen_scalar_to_tensor output str name = f scalar_to_tensor_ next scalar_to_tensor_id wrapper_call writeline f RAIIAtenTensorHandle name = scalar_to_tensor_handle output name codegen_tensor_item dtype torch dtype tensor str scalar str indented_buffer=None dtype_str = str dtype split - writer = indented_buffer dtype == torch float dtype == torch bfloat scalar_tmp = f scalar _tmp writer writeline f DTYPE_TO_CPP dtype scalar_tmp writer writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_item_ dtype_str tensor scalar_tmp writer writeline f float scalar = float scalar_tmp writer writeline f DTYPE_TO_CPP dtype scalar writer writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_item_ dtype_str tensor scalar generate_return output_refs list str cst_names = V graph constants keys output idx dict str int = If any output ref represents rvalue tensor materialize lvalue RAIIAtenTensorHandle first This prevents situations where code rvalue tensor references tensor handles whose contents modified below output_refs = create_tmp_raii_handle_var_if_needed o wrapper_call o output_refs idx output enumerate output_refs output == nullptr continue is_constant_buffer = output cst_names output_buffer = V graph graph_outputs idx isinstance output_buffer ir BaseView output_storage = output_buffer unwrap_view assert isinstance output_storage ir BaseView ir MutableBox isinstance output_storage data ir ConstantBuffer is_constant_buffer = True isinstance output_buffer ir ShapeAsConstantBuffer Need wrap scalar into tensor main function returns vector tensors output_tensor = codegen_scalar_to_tensor output wrapper_call writeline f output_handles idx = output_tensor release continue is_constant_buffer See NOTE return_constant above wrapper_call writeline f aoti_torch_clone output output_handles idx output output idx src_idx = output idx output wrapper_call writeline f output_handles idx = output_handles src_idx wrapper_call writeline f output_handles idx = output release output output idx output idx output = idx generate_before_suffix result V graph is_const_graph V graph aot_mode result writeline f aoti_model_class_name run_impl result writeline inductor_entry_impl generate_end result Generates end code block any code needed call V graph aot_mode V graph is_const_graph result writeline f aoti_model_class_name _const_run_impl result writeline namespace torch aot_inductor\n\n\n config cpp_wrapper_build_separate Close wrapper code block then write any kernel definitions result splice \n kernel_declarations result splice \nkernel_src = \nr result splice kernel_declarations getvalue result splice \n result splice kernel_src = Merge main code kernel code result splice kernel_declarations getvalue kernel_declarations clear Close wrapper code block result splice \n kernel_code = kernel_src config cpp_wrapper_build_separate None Cpp entry function JIT cpp wrapper result splice f inductor_entry = CppWrapperCodeCache load_pybinding argtypes= std vector AtenTensorHandle main_code=cpp_wrapper_src device_type= device num_outputs= len V graph graph_outputs kernel_code= kernel_code wrapper_body = input_tensors = arg isinstance arg torch Tensor torch tensor arg device= cpu arg args V graph constants Append constants input args cpp wrapper Python wrapper directly gets value inside wrapper call global variable passed when calling exec code mod __dict__ mod __dict__ For cpp wrapper we need pass python value inductor_entry_impl function explicitly assert all isinstance v torch Tensor v list V graph constants values Expect all constants Tensor constants_str = f join V graph constants keys wrapper_body += f constants_tensor = constants_str input_tensors extend constants_tensor Convert vector Tensor vector AtenTensorHandle If we pass Tensor compilation will too slow wrapper_body += input_handles = torch _C _aoti unsafe_alloc_void_ptrs_from_tensors input_tensors Release inputs memory reuse wrapper_body += args clear del input_tensors unwrap output tensor back python scalar all x x output_is_tensor values If no ShapeAsConstantBuffer output directly output tensors outputs_str = output_tensors outputs = f output_tensors i output_is_tensor i f output_tensors i item i range len V graph graph_outputs outputs_str = f join outputs wrapper_body += f output_handles = f input_handles output_tensors = torch _C _aoti alloc_tensors_by_stealing_from_void_ptrs output_handles outputs_str Wrap func support setting result _boxed_call = True result splice f _wrap_func f g args wrapper_body g call = _wrap_func inductor_entry staticmethod get_c_shim_func_name kernel str device str - str kernel startswith aoti_torch_ kernel assert kernel Cpp kernel name + kernel + does contain kernel_tokens = kernel split kernel_suffix = kernel_tokens - kernel_suffix == call kernel_suffix = kernel_tokens - shim_fn = f aoti_torch_ device _ kernel_suffix shim_fn generate_c_shim_extern_kernel_call kernel str args list str device str debug_args Optional list str = None stack_traces Optional OrderedSet str = None - None debug_args kwarg allows CppWrapperCpuArrayRef pass wrapped arguments place args while preserving debug printer output We can do unconditionally since we cache call add_device_include device debug_printer_manager = V graph wrapper_code debug_printer debug_printer_manager set_printer_args debug_args debug_args None args kernel None None extern enable_kernel_profile = config cpp enable_kernel_profile sys platform linux win debug_printer_manager shim_fn = get_c_shim_func_name kernel device shim_fn_codes = f AOTI_TORCH_ERROR_CODE_CHECK shim_fn join args enable_kernel_profile stack_trace_str = R stack_traces stack_trace stack_traces line stack_trace split \n stack_trace_str += f \n line stack_trace_str += \n stack_trace_str += shim_fn_codes = f KernelContextGuard _ctx shim_fn stack_trace_str f RAIIAtenRecordFunctionHandle record_ shim_fn _ shim_fn nullptr shim_fn_codes writelines shim_fn_codes generate_c_shim_extern_kernel_alloc extern_kernel ir ExternKernelAlloc args list str - None registered output buffer name name = extern_kernel name output_handle_name = f name _handle is_inplace = isinstance extern_kernel op_overload torch _ops OpOverload torch Tag inplace_view extern_kernel op_overload tags is_inplace writeline f AtenTensorHandle output_handle_name args = args f output_handle_name device = d type d = extern_kernel get_device device generate_c_shim_extern_kernel_call extern_kernel get_kernel_name args device extern_kernel python_kernel_name torch ops _c d_functional all_reduce_ default torch ops _c d_functional wait_tensor default all_reduce_ inplace op its returned tensor used anywhere wait_tensor returns its input without any modification returned tensor used anywhere In both cases we can immediately delete returned AtenTensorHandle reduce its lifetime writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_delete_tensor_object output_handle_name is_inplace writeline f RAIIAtenTensorHandle name output_handle_name _generate_extern_kernel_alloc_helper extern_kernel args getattr extern_kernel outputs None ir ExternKernelAlloc may have outputs returns tuple generate_c_shim_fallback_kernel extern_kernel args generate_c_shim_extern_kernel_alloc extern_kernel args generate_c_shim_fallback_kernel fallback_kernel ir FallbackKernel args list str - None output_args = output_raii_handles = output_name_base = fallback_kernel get_name idx output enumerate fallback_kernel outputs isinstance output ir MultiOutput TODO handle integer output e g attention name = f output get_name output_handle_name = f name _handle output indices assert output indices == idx f expected output indices = == idx= output_name_base= writeline f AtenTensorHandle output_handle_name output_args append f output_handle_name output_raii_handles append f RAIIAtenTensorHandle name output_handle_name isinstance output int output_name = f output_name_base _ idx writeline f int _t output_name = output output_args append f output_name isinstance output sympy Expr output_name = f output_name_base _ idx writeline f auto output_name = cexpr output output_args append f output_name output None output_args append nullptr raise NotImplementedError f unsupported type output= args = args + output_args device = d type d = fallback_kernel get_device device generate_c_shim_extern_kernel_call fallback_kernel cpp_kernel_name type ignore arg-type args device raii_handle output_raii_handles writeline raii_handle _generate_extern_kernel_out_helper kernel str out str out_view Optional str args list str device str stack_traces Optional OrderedSet str = None - None out_view out_name = f out _as_strided writeline f auto out_name = out_view args insert out_name args insert out generate_c_shim_extern_kernel_call kernel args device stack_traces=stack_traces _get_scatter_reduce_enum reduce Follow aten src ATen native ReductionType h get_operator_enum get_operator_enum = add sum multiply prod reduce get_operator_enum reduce = get_operator_enum reduce reduce _generate_scatter_fallback output inputs cpp_kernel_name python_kernel_name src_is_tensor reduce kwargs reduce = _get_scatter_reduce_enum reduce call ABI shim function instead ATen one cpp_kernel_name = get_c_shim_func_name cpp_kernel_name device TODO consider remove _out add missing inplace variants fallback_ops py cpp_kernel_name = cpp_kernel_name replace __ _ + _out inputs_wrapped = str x x inputs line = f cpp_kernel_name output join inputs_wrapped python_kernel_name startswith aten scatter_reduce line += f join kwargs src_is_tensor reduce line += f V graph wrapper_code val_to_arg_str reduce assert reduce None Expect reduce None aten scatter_ scalar src line += writeline line _generate_index_put_fallback kernel x indices values accumulate TODO update aoti_torch_index_put_out ir py use autogen out version See comment codegen_reinterpret_view about why having something like RAIIAtenTensorHandle tmp_tensor_handle_ tmp array can cause corresponding tensor prematurely deallocated thus temporary array trick here indices_str = _generate_temporary_array_pointer AtenTensorHandle indices args = x indices_str str len indices values accumulate args insert x set x output tensor fallback mutates x writeline wrap_kernel_call kernel args add_benchmark_harness output V graph aot_mode super add_benchmark_harness output codegen_cpp_sizevar x sympy Expr simplify bool = True - str cexpr V graph sizevars simplify x simplify x codegen_sizevar x sympy Expr - str codegen_cpp_sizevar x codegen_tuple_access basename str name str index str - str abi_compatible mode outputs returned via arguments name codegen_shape_tuple shape Sequence sympy Expr - str parts = map codegen_sizevar shape len parts == len parts == f parts f join parts ensure_size_computed sym sympy Symbol isinstance sym sympy Symbol symbol_is_type sym SymT PRECOMPUTED_SIZE sym computed_sizes computed_sizes add sym expr = V graph sizevars inv_precomputed_replacements sym writeline f int _t sym = cexpr expr _generate_symbolic_call_arg_helper arg SymbolicCallArg graph GraphLowering - None arg inner graph kernel_numel_expr declare expr once each graph scope kernel_numel_expr add arg inner graph writeline f int _t arg inner = cexpr arg inner_expr writeline f arg inner = cexpr arg inner_expr _codegen_dynamic_scalar node data = t codegen_reference t node inputs codegen_tensor_item node inputs get_dtype data f node sym _raw len node keypath == writeline f auto node sym = node sym _raw len node keypath == isinstance node keypath ConvertIntKey writeline f int _t node sym = node sym _raw len node keypath == isinstance node keypath DivideByKey TODO assert divisibility here writeline f int _t node sym = node sym _raw node keypath divisor raise AssertionError f unrecognized keypath node keypath record unbacked_symbol_decls so we won t generate declaration symbol again unbacked_symbol_decls add str node sym codegen_dynamic_select_index node clamp index_cpp_str = val_to_arg_str_for_prim_type node index int size_cpp_str = val_to_arg_str_for_prim_type node size int codegen index sym = node unbacked_offset_symbol index_str = f index_cpp_str index_cpp_str + f val_to_arg_str_for_prim_type node size int index_cpp_str writeline f auto sym _index = index_str index_str_clamped = f sym _index sym _index size_cpp_str size_cpp_str sym _index clamp f sym _index writeline f auto sym _index_clamped = index_str_clamped writeline f auto sym = val_to_arg_str_for_prim_type node base_offset int + f val_to_arg_str_for_prim_type node base_dim_stride int sym _index_clamped record unbacked_symbol_decls so we won t generate declaration symbol again unbacked_symbol_decls add str sym codegen_dynamic_slice_size node start_cpp_str = val_to_arg_str_for_prim_type node start int end_cpp_str = val_to_arg_str_for_prim_type node end int size_cpp_str = val_to_arg_str_for_prim_type node size int step_cpp_str = val_to_arg_str_for_prim_type node step int sym = node unbacked_size_symbol codegen_clamp index_str start=True suf = st start en index_ = f sym _ suf _index writeline f int _t index_ = index_str index_str + size_cpp_str index_str writeline f int _t sym _ suf _cl = index_ index_ size_cpp_str size_cpp_str index_ codegen_clamp start_cpp_str start=True codegen_clamp end_cpp_str start=False node step == step_str = f sym _en_cl - sym _st_cl step_str = f sym _en_cl - sym _st_cl + step_cpp_str - step_cpp_str writeline f int _t sym _with_step = step_str writeline f int _t sym = sym _with_step sym _with_step unbacked_symbol_decls add str sym make_buffer_free buffer isinstance buffer get_output_spec ir MultiOutputLayout isinstance buffer ir TMADescriptor f buffer get_name reset make_free_by_names names_to_del list str join f name reset name names_to_del codegen_exact_buffer_reuse old_name str new_name str del_line str f auto new_name = std move old_name reuse generate_profiler_mark_wrapper_call stack wrapper_call writeline RAIIAtenRecordFunctionHandle record_inductor_wrapper_call_ inductor_wrapper_call nullptr generate_start_graph pass generate_end_graph pass generate_inf_and_nan_checker nodes buf nodes get_names TODO Add buf name directly into check_inf_and_nan writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_check_inf_and_nan buf codegen_device device assert device type DEVICE_TO_ATEN device type + found DEVICE_TO_ATEN device_str = DEVICE_TO_ATEN device type lower remove k used_cached_devices add device_str f cached_torch_device_type_ device_str device index device index codegen_dtype dtype dtype_str = str dtype split - used_cached_dtypes add dtype_str f cached_torch_dtype_ dtype_str codegen_layout layout layout_str = str layout split - used_cached_layouts add layout_str f cached_torch_layout_ layout_str codegen_memory_format memory_format memory_format_str = str memory_format split - used_cached_memory_formats add memory_format_str f cached_torch_memory_format_ memory_format_str functools cache noqa B codegen_int_array_var int_array str writeline Callable None known_statically=False graph=None per-graph caching Used size stride declaration Because memory planning done two passes see implementation generate writeline behavior different two passes As result emitted int array declarations may appear later position generated code so second pass codegen should reuse int array declarations generated first pass This why writeline needs explicitly passed parameter var = f int_array_ next int_array_id ctype = int _t int_array == An array unknown bound cannot initialized known_statically config cpp use_constexpr_for_int_array writeline f static constexpr ctype var =nullptr writeline f static const ctype var =nullptr writeline f const ctype var =nullptr var declared_int_array_vars declared_int_array_vars add var known_statically config cpp use_constexpr_for_int_array writeline f static constexpr ctype var = int_array writeline f static const ctype var = int_array writeline f const ctype var = int_array var make_buffer_allocation buffer make_allocation buffer get_name buffer get_device buffer get_dtype buffer get_size buffer get_stride V graph get_allocation_size buffer buffer get_is_pinned make_allocation name device dtype shape stride allocation_shape=None is_pinned=False allocation_shape None allocation_shape = shape orig_stride = stride device_str = codegen_device device dtype_code = codegen_dtype dtype size = codegen_shape_tuple shape allocation_size = codegen_shape_tuple allocation_shape stride = codegen_shape_tuple orig_stride size_array_var = codegen_int_array_var size wrapper_call writeline known_statically=self is_statically_known_list_of_ints shape graph=self get_codegened_graph allocation_size = size allocation_size_array_var = codegen_int_array_var allocation_size wrapper_call writeline known_statically=self is_statically_known_list_of_ints allocation_shape graph=self get_codegened_graph allocation_size_array_var = size_array_var stride_array_var = codegen_int_array_var stride wrapper_call writeline known_statically=self is_statically_known_list_of_ints orig_stride graph=self get_codegened_graph device_type device_id = device_str split device_idx = this- device_idx_ V graph aot_mode device_id handle_name = f name _handle args = str len shape allocation_size_array_var stride_array_var dtype_code device_type device_idx f handle_name wrapper_call writeline f AtenTensorHandle handle_name pinned_str = _pinned is_pinned wrapper_call writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_empty_strided pinned_str join args allocation_size = size old_handle_name handle_name = handle_name f name _handle_restrided wrapper_call writeline f AtenTensorHandle handle_name args = old_handle_name size_array_var stride_array_var f handle_name wrapper_call writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_as_strided join args wrapper_call writeline f wrap_with_raii_handle_if_needed old_handle_name f RAIIAtenTensorHandle name handle_name codegen_alloc_from_pool name offset dtype shape stride - tuple str list str size = codegen_shape_tuple shape stride = codegen_shape_tuple stride tmp_name = f tmp_tensor_handle_ next tmp_tensor_id args = name cexpr offset bytes numel codegen_dtype dtype str len shape codegen_int_array_var size wrapper_call writeline graph=self get_codegened_graph codegen_int_array_var stride wrapper_call writeline graph=self get_codegened_graph f tmp_name We lines instead writing here because writing here bug prune If you write aoti_torch__alloc_from_pool lines you must write RAIIAtenTensorHandle well otherwise you get memory leaks allocations_to_write = f AtenTensorHandle tmp_name f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch__alloc_from_pool join args f RAIIAtenTensorHandle tmp_name allocations_to_write codegen_reinterpret_view data size stride offset writeline Callable None dtype=None - str Returns newly-created temporary RAII tensor handle containing reinterpreted tensor data Callers function responsible saving handle persistent access needed dim = str len size original_offset = offset offset = codegen_sizevar offset call_strs = final_tensor_str = None create_reinterpret_call - str args = f data get_name dim codegen_int_array_var codegen_shape_tuple size writeline known_statically=self is_statically_known_list_of_ints size graph=self get_codegened_graph codegen_int_array_var codegen_shape_tuple stride writeline known_statically=self is_statically_known_list_of_ints stride graph=self get_codegened_graph offset f wrap_with_raii_handle_if_needed reinterpret_tensor_wrapper join args create_dtypeview_call reinterpret_call str - tuple str list str tmp_AtenTensorHandle = f tmp_ data get_name _ next tmp_tensor_id tmp_call_strs = f AtenTensorHandle tmp_AtenTensorHandle device_name = data layout device type dtypeview_function = f aoti_torch_ device_name _view_dtype tmp_call_strs append f AOTI_TORCH_ERROR_CODE_CHECK dtypeview_function f reinterpret_call codegen_dtype dtype tmp_AtenTensorHandle f RAIIAtenTensorHandle tmp_AtenTensorHandle tmp_call_strs create_new_tensor_handle - tuple str list str tmp_AtenTensorHandle = f tmp_ data get_name _ next tmp_tensor_id tmp_call_strs = f AtenTensorHandle tmp_AtenTensorHandle f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_new_tensor_handle data get_name tmp_AtenTensorHandle f RAIIAtenTensorHandle tmp_AtenTensorHandle tmp_call_strs size == data layout size stride == data layout stride original_offset == data layout offset pure dtypeview dtype None dtype = data dtype final_tensor_str tmp_call_strs = create_dtypeview_call data get_name final_tensor_str tmp_call_strs = create_new_tensor_handle call_strs extend tmp_call_strs firstly create reinterpretview final_tensor_str = create_reinterpret_call dtype None dtype = data dtype wrap dtypeview final_tensor_str tmp_call_strs = create_dtypeview_call final_tensor_str call_strs extend tmp_call_strs line call_strs writeline line NB handle here represents temporary tensor which will automatically released Here s sample usage cpp wrapper code ` ` ` aoti_torch_addmm_out buf arg _ RAIIAtenTensorHandle tmp_tensor_handle_ buf L L ` ` ` RAIIAtenTensorHandle tmp_tensor_handle_ will released after call addmm_out This could problematic when s used different pattern example ` ` ` ` AtenTensorHandle tensor_args = RAIIAtenTensorHandle tmp_tensor_handle_ buf buf aoti_torch_proxy_executor_call_function tensor_args ` ` ` ` RAIIAtenTensorHandle tmp_tensor_handle_ will invalid when s used latter kernel call This solved updating proxy_executor invocation ` ` ` aoti_torch_proxy_executor_call_function std array AtenTensorHandle RAIIAtenTensorHandle tmp_tensor_handle_ buf buf cbegin ` ` ` final_tensor_str codegen_device_copy src dst non_blocking Union bool str This function overridden cpp_wrapper_cpu_array_ref so we don t need handle cases where dst AtenTensorHandle writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_copy_ dst src non_blocking codegen_multi_output node ir MultiOutput abi_compatible mode outputs retrieved passing output pointers so we skip its codegen here pass codegen_subgraph_prefix subgraph outer_inputs outer_outputs assert len subgraph graph graph_inputs == len outer_inputs inner_input inner_input_val outer_input zip subgraph graph graph_inputs items outer_inputs isinstance inner_input_val ir TensorBox continue ABI-compatible mode we copy underlying Tensor conditional input outer_input into another Tensor used subgraph input inner_input nested scope we can t std move here codegened outer input may expression rvalue e g reinterpret_view x so we can t necessarily std move back origin x writeline f AtenTensorHandle inner_input _handle writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_assign_tensors_out outer_input inner_input _handle writeline f RAIIAtenTensorHandle inner_input inner_input _handle codegen_subgraph_suffix subgraph outer_inputs outer_outputs inner_output outer_output zip subgraph graph graph_outputs outer_outputs src = inner_output codegen_reference isinstance inner_output ir ShapeAsConstantBuffer ABI-compatible mode we need std move subgraph output inner_output conditional output outer_output RAIIAtenTensorHandle s copy constructor deleted src = f std move src case outer_output carried value before e g while_loop codegen writeline f outer_output reset writeline f outer_output = src codegen_invoke_subgraph invoke_subgraph raise NotImplementedError codegen invoke_subgraph implemented cpp wrapper codegen_conditional conditional outer_inputs = f buf codegen_reference buf conditional operands outer_outputs = out conditional outputs ABI-compatible mode ir MultiOutput codegened hence pre-declare output variables directly separately writeline f RAIIAtenTensorHandle out get_name outer_outputs append out get_name isinstance conditional predicate ir ShapeAsConstantBuffer ABI-compatible mode we need use ABI shim function extract C++ bool underlying scalar bool Tensor predicate = f conditional predicate get_name _scalar predicate used_cond_predicate codegen_tensor_item torch bool conditional predicate codegen_reference predicate used_cond_predicate add predicate predicate Tensor SymBool Python bool predicate = conditional predicate codegen_reference writeline f predicate writeline EnterSubgraphLine conditional true_subgraph graph codegen_subgraph conditional true_subgraph outer_inputs outer_outputs writeline ExitSubgraphLine writeline writeline EnterSubgraphLine conditional false_subgraph graph codegen_subgraph conditional false_subgraph outer_inputs outer_outputs writeline ExitSubgraphLine writeline codegen_subgraph subgraph outer_inputs outer_outputs TODO desertfire - This function old way supporting subgraph codegen inlining subgraphs output code For python wrapper we have moved lifting subgraphs functions supported PythonWrapperCode ` codegen_subgraph ` function We should perhaps support lifting subgraphs functions cpp wrapper well try push_codegened_graph subgraph graph writeline f subgraph subgraph name codegen_subgraph_prefix subgraph outer_inputs outer_outputs parent_graph = V graph V set_graph_handler subgraph graph subgraph graph codegen_subgraph parent_graph=parent_graph codegen_subgraph_suffix subgraph outer_inputs outer_outputs finally pop_codegened_graph codegen_while_loop while_loop stack_output=False stack_output raise NotImplementedError NYI cpp wrapper while_loop_stack_output is_bool_pred = isinstance while_loop cond_subgraph graph graph_outputs ir ShapeAsConstantBuffer name = while_loop get_name outer_carried_inputs = buf codegen_reference buf while_loop carried_inputs outer_additional_inputs = buf codegen_reference buf while_loop additional_inputs cond_result_name = f name _cond_result is_bool_pred writeline f bool cond_result_name writeline f RAIIAtenTensorHandle cond_result_name cond_outer_inputs = inp out zip outer_carried_inputs while_loop outputs ABI-compatible mode carried inputs codegened buffers outside while loop set initial values end each while_loop iteration they will assigned carried values out_name = out get_name writeline f AtenTensorHandle out_name _handle writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_assign_tensors_out inp out_name _handle writeline f RAIIAtenTensorHandle out_name out_name _handle cond_outer_inputs append out_name additional inputs will assigned within while_loop iteration directly corresponding outer graph buffers cond_outer_inputs extend outer_additional_inputs cond_outer_outputs = cond_result_name body_outer_inputs = list cond_outer_inputs body_outer_outputs = body_outer_inputs len outer_carried_inputs writeline while writeline EnterSubgraphLine while_loop cond_subgraph graph codegen_subgraph while_loop cond_subgraph cond_outer_inputs cond_outer_outputs is_bool_pred cond_result = f cond_result_name cond_result = f cond_result_name _scalar codegen_tensor_item torch bool cond_result_name cond_result writeline f cond_result break writeline ExitSubgraphLine writeline EnterSubgraphLine while_loop body_subgraph graph codegen_subgraph while_loop body_subgraph body_outer_inputs body_outer_outputs writeline ExitSubgraphLine writeline generate_extern_kernel_args_decl_if_needed op_overload Union torch _ops OpOverload torch _ops HigherOrderOperator raw_args Sequence Any output_args _OUTPUT_ARGS_TYPE raw_outputs Sequence ir Buffer Generates declarations external kernel arguments needed based provided operator its arguments It processes both input output arguments categorizing them into tensor integer arguments further code generation schema = None isinstance op_overload torch _higher_order_ops torchbind CallTorchBind obj = raw_args method = raw_args schema = op_overload schema obj method assert isinstance op_overload torch _ops OpOverload type op_overload schema = op_overload _schema assert schema None arg_types = x real_type x schema arguments return_types = x type x schema returns new_tensor_args = new_int_args = fill_args arg arg_type static_arg_types = torch FloatType torch BoolType torch StringType torch Type torch DeviceObjType inductor_tensor_buffers = ir Buffer ir ReinterpretView isinstance arg_type torch TensorType assert isinstance arg inductor_tensor_buffers f got type arg new_tensor_args append f arg codegen_reference isinstance arg_type torch IntType int new_int_args append str arg isinstance arg_type torch SymIntType SymInt expr = arg node expr isinstance arg torch SymInt arg new_int_args append cexpr expr isinstance arg_type torch NumberType Scalar type int assert isinstance arg int float bool Only treat int Scalar dynamic isinstance arg int new_int_args append str arg isinstance arg ir TorchBindObject torchbind objects loaded proxy executor pass isinstance arg_type torch ListType assert isinstance arg list tuple List Tensor isinstance arg_type getElementType torch TensorType new_tensor_args extend f codegen_reference arg List Optional Tensor isinstance arg_type getElementType torch OptionalType isinstance arg_type getElementType getElementType torch TensorType new_tensor_args extend f codegen_reference arg None List int isinstance arg_type getElementType torch IntType new_int_args extend str arg List SymInt isinstance arg_type getElementType torch SymIntType expressions = node expr isinstance torch SymInt arg new_int_args extend cexpr expr expr expressions List Scalar isinstance arg_type getElementType torch NumberType Only treat int Scalar dynamic is_int_type = isinstance int arg any is_int_type assert all is_int_type AOTInductor only supports int scalars same type new_int_args extend str arg assert isinstance arg_type getElementType static_arg_types type ignore arg-type f Fall through arguments must one static_arg_types got type arg_type assert isinstance arg_type static_arg_types type ignore arg-type f Fall through arguments must one static_arg_types got type arg_type arg arg_type zip raw_args arg_types arg None isinstance arg_type torch OptionalType fill_args arg arg_type getElementType fill_args arg arg_type fill_output_arg arg str return_type torch JitType is_mutated_output bool - None isinstance return_type torch TensorType is_mutated_output writeline f AtenTensorHandle arg _handle output buffer writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_new_uninitialized_tensor arg _handle writeline f RAIIAtenTensorHandle arg arg _handle new_tensor_args append f arg isinstance return_type torch SymIntType raise NotImplementedError NYI support type SymInt isinstance return_type torch ListType isinstance return_type getElementType torch SymIntType raise NotImplementedError NYI support type List SymInt raise AssertionError f Unsupported type found return_type TODO Only support None tensor s returns now SymInt implemented yet return_type return_types isinstance return_type torch TensorType torch NoneType torch IntType pass isinstance return_type torch OptionalType assert isinstance return_type getElementType torch TensorType isinstance return_type torch ListType assert isinstance return_type getElementType torch TensorType raise NotImplementedError f type return_type yet supported output_arg raw_output_arg zip output_args raw_outputs type ignore arg-type None output supported Optional types yet supported output_arg None continue isinstance raw_output_arg int new_int_args append str raw_output_arg isinstance output_arg list out output_arg assert out None out fill_output_arg out torch TensorType get isinstance raw_output_arg ir MutationOutput fill_output_arg output_arg torch TensorType get isinstance raw_output_arg ir MutationOutput new_tensor_args new_int_args staticmethod _compatible_with_stableivalue op torch _ops OpOverload - bool Returns true op_overload _schema only utilizes types supported AOT C-shim internal function to_ivalue to_ivalue implementation detail so these types guaranteed supported long-term When generating code cpp_wrapper mode we don t have forward-compatible so changing function s implementation future fine supported_types = torch BoolType torch DeviceObjType torch FloatType ScalarTypeType LayoutType MemoryFormatType seen IntType when queried via torch JitType type torch IntType torch TensorType type_supported t torch JitType - bool isinstance t torch OptionalType type_supported t getElementType isinstance t supported_types all type_supported type chain op _schema arguments op _schema returns generate_fallback_kernel_with_runtime_lookup buf_name str python_kernel_name str get_args Callable Sequence str op_overload Union torch _ops OpOverload torch _ops HigherOrderOperator raw_args Sequence Any outputs Sequence ir Buffer - None Generate call kernel contained C-shim This results different code paths AOT Inductor vs cpp_wrapper Inductor mode extract_output_name out Optional Union ir Buffer Sequence ir Buffer - Union Optional str _OUTPUT_ARGS_TYPE out None None isinstance out ir MultiOutput ir _CollectiveKernel out get_name isinstance out ir MutationOutput mutated_buf_names = out get_mutation_names assert isinstance mutated_buf_names list len mutated_buf_names == Expect only one mutated buffer MutationOutput mutated_buf_names isinstance out list tuple extract_output_name o o out type ignore misc isinstance out int str out raise AssertionError f Unexpected output type out isinstance op_overload torch _ops HigherOrderOperator assert isinstance op_overload torch _higher_order_ops torchbind CallTorchBind type op_overload assert len raw_args obj = raw_args method = raw_args return_schema = op_overload schema obj method returns return_schema = op_overload _schema returns output_args has same pytree structure outputs return_schema kernel does value output_args _OUTPUT_ARGS_TYPE = isinstance output_name = extract_output_name outputs str output_args = output_name If schema indicates value we should have non-None value point assert isinstance output_name list type output_name output_args = output_name In AOT mode we use ProxyExecutor run fallback kernels V graph aot_mode generate_fallback_kernel_with_runtime_lookup_aot op_overload raw_args output_args outputs assert isinstance op_overload torch _ops OpOverload type op_overload output output_args assert output None isinstance output str fallback kernels runtime lookup currently only support tensor returns more complicated types such list-of-list-of-tensor In non-AOT mode we use aoti_torch_call_dispatcher all inputs outputs op can represented StableIValue This avoids overhead calling back into Python covers most remaining fallback ops _compatible_with_stableivalue op_overload generate_fallback_kernel_with_runtime_lookup_nopython get_args op_overload output_args type ignore arg-type outputs Otherwise we call back into Python which has some extra runtime overhead handles situations like list Tensor currently unrepresentable via StableIValue generate_fallback_kernel_with_runtime_lookup_python buf_name python_kernel_name op_overload raw_args output_args type ignore arg-type outputs generate_scoped_gil_acquire declarations_before_scope lines_in_scope scoped_lines = IndentedBuffer declaration declarations_before_scope scoped_lines writeline declaration scoped_lines writeline scoped_lines indent scoped_lines writeline py gil_scoped_acquire_simple acquire scoped_lines writelines lines_in_scope split \n scoped_lines writelines scoped_lines _lines load_custom_op_wrapper TODO need support control flow custom_op_wrapper_loaded lines = RAIIPyObject codecache_module PyImport_ImportModule torch _inductor codecache codecache_module throw std runtime_error Failed load torch _inductor codecache custom_op_wrapper = PyObject_GetAttrString codecache_module custom_op_wrapper custom_op_wrapper throw std runtime_error Failed load torch _inductor codecache custom_op_wrapper declarations_before_scope = RAIIPyObject custom_op_wrapper scope_gil_acquire = generate_scoped_gil_acquire declarations_before_scope lines writelines scope_gil_acquire custom_op_wrapper_loaded = True generate_float_value val assert isinstance val float val == float inf std numeric_limits double infinity val == float -inf -std numeric_limits double infinity math isnan val std numeric_limits double quiet_NaN f val generate_py_arg py_args_var idx raw_arg arg_type generate_py_arg_inner lines raw_arg arg_type handle_scalar scalar isinstance scalar int f PyLong_FromLongLong scalar isinstance scalar float f PyFloat_FromDouble generate_float_value scalar isinstance scalar bool f PyBool_FromLong scalar isinstance scalar complex real = generate_float_value scalar real imag = generate_float_value scalar imag f PyComplex_FromDoubles real imag isinstance scalar SymTypes scalar_var = cexpr scalar node expr isinstance scalar torch SymBool f PyBool_FromLong scalar_var isinstance scalar torch SymFloat f PyFloat_FromDouble scalar_var f PyLong_FromLongLong scalar_var raise NotImplementedError f scalar scalar type scalar cannot handled handle_scalar raw_arg None Py_None singleton so we have explicitly incref here lines append Py_INCREF Py_None \n Py_None isinstance arg_type torch TensorType In some cases scalar arguments may passed place tensors hasattr raw_arg codegen_reference handle_scalar raw_arg Store AtenTensorHandle void All Python args constructed nested scope so handle will self-destruct after function call base_handle = create_tmp_raii_handle_var_if_needed raw_arg codegen_reference lines f PyCapsule_New reinterpret_cast void base_handle get NULL NULL isinstance arg_type torch OptionalType generate_py_arg_inner lines raw_arg arg_type getElementType isinstance arg_type torch IntType int f PyLong_FromLongLong raw_arg isinstance arg_type torch SymIntType SymInt expr = raw_arg node expr isinstance raw_arg torch SymInt raw_arg f PyLong_FromLongLong cexpr expr isinstance arg_type torch FloatType f PyFloat_FromDouble generate_float_value raw_arg isinstance arg_type torch BoolType f PyBool_FromLong raw_arg isinstance arg_type torch StringType f PyUnicode_FromString raw_arg isinstance arg_type torch NumberType Union bool int float complex torch _prims_common __init__ py handle_scalar raw_arg isinstance raw_arg torch device device_str device_index = codegen_device raw_arg split f THPDevice_New c Device static_cast c DeviceType device_str device_index isinstance raw_arg torch dtype f Py_NewRef torch getTHPDtype static_cast c ScalarType codegen_dtype raw_arg isinstance raw_arg torch layout f Py_NewRef torch getTHPLayout static_cast c Layout codegen_layout raw_arg isinstance raw_arg torch memory_format Py_NewRef torch utils getTHPMemoryFormat static_cast c MemoryFormat f codegen_memory_format raw_arg raise NotImplementedError f arg type arg_type yet supported custom_op_wrapper lines = isinstance arg_type torch ListType assert isinstance raw_arg list tuple str raw_arg + list lines append f PyObject py_args_var _ idx = PyList_New len raw_arg \n i elem enumerate raw_arg lines append f PyList_SetItem py_args_var _ idx i generate_py_arg_inner lines elem arg_type getElementType \n lines append f PyTuple_SetItem py_args_var idx py_args_var _ idx \n lines append f PyTuple_SetItem py_args_var idx generate_py_arg_inner lines raw_arg arg_type \n join lines generate_fallback_kernel_with_runtime_lookup_nopython get_args Callable Sequence str op_overload torch _ops OpOverload output_args Sequence Optional str raw_outputs Sequence ir Buffer - None Generate fallback kernel calls runtime non-AOT dispatch This can only called cpp_wrapper mode assumes input non-None OpOverload In future we may switch over directly calling c Dispatcher we need support more datatypes raw_outputs declarations_before_scope = f RAIIAtenTensorHandle output_arg output_arg raw_output_arg zip output_args raw_outputs type ignore arg-type output_arg None isinstance raw_output_arg ir MutationOutput declarations_before_scope = f RAIIAtenTensorHandle output_arg output_arg output_args type ignore arg-type output_arg None dispatch_lines = IndentedBuffer dispatch_lines writelines declarations_before_scope dispatch_lines writeline dispatch_lines indent tmp_var_number = count parse_arg arg_type torch JitType codegen_arg str - str Strip off any temporary references we re indented context so any saved-off variables will auto-destroyed new_codegen_arg = codegen_arg removeprefix temporary_reference new_codegen_arg = codegen_arg If we removed temporary_reference there s good chance variable ends get which would retrieve ATenTensorHandle temporary RAII handle Strip off too since we re going save temporary RAII handle codegen_arg endswith get codegen_arg = new_codegen_arg removesuffix get codegen_arg = new_codegen_arg removesuffix isinstance arg_type torch OptionalType If we have pointer variable strip off let std optional handle any internal pointers codegen_arg = codegen_arg removeprefix codegen_arg == nullptr std nullopt var_name = f tmp_var_ next tmp_var_number dispatch_lines writeline f std optional var_name parse_arg arg_type getElementType codegen_arg f var_name raii_var = create_tmp_raii_handle_var_if_needed codegen_arg dispatch_lines temp_handle = raii_var = codegen_arg isinstance arg_type torch TensorType temp_handle If RAII tensor being referenced _isn t_ temporary scoped fallback call then create new handle referencing which AtenTensorHandle can steal var_name = f tmp_var_ next tmp_var_number dispatch_lines writeline f AtenTensorHandle var_name dispatch_lines writeline f aoti_torch_new_tensor_handle raii_var var_name f var_name If RAII tensor _is_ temporary scoped fallback call simply release steal handle f raii_var release f codegen_arg codegen_args = get_args ivalue_args = parse_arg type c c zip op_overload _schema arguments codegen_args array_len = max len codegen_args len output_args dispatch_lines writeline f std array StableIValue array_len dispatch_vars join ivalue_args dispatch_lines writeline AOTI_TORCH_ERROR_CODE_CHECK dispatch_lines indent dispatch_lines writeline f aoti_torch_call_dispatcher op_overload _schema name op_overload _schema overload_name dispatch_vars data noqa B dispatch_lines writeline len output_args == output = output_args None result single tensor dispatch_lines writeline f output = AtenTensorHandle dispatch_vars result tuple tensors idx output_arg enumerate output_args output_arg None continue dispatch_lines writeline f output_arg = AtenTensorHandle dispatch_vars idx dispatch_lines writeline writelines dispatch_lines getvalue splitlines generate_fallback_kernel_with_runtime_lookup_python buf_name str python_kernel_name str op_overload torch _ops OpOverload raw_args Sequence Any output_args Sequence Optional str raw_outputs Sequence ir Buffer - None Generate fallback kernel calls runtime non-AOT dispatch This can only called cpp_wrapper mode assumes input non-None OpOverload This function calls into Python dispatch which allows handle datatypes cannot contained StableIValue cost some performance load_custom_op_wrapper num_args = len raw_args py_args_var = f py_args_ next arg_var_id First arg always python op name lines = textwrap dedent f RAIIPyObject py_args_var PyTuple_New num_args + py_args_var throw std runtime_error PyTuple_New py_args_var failed PyTuple_SetItem py_args_var PyUnicode_FromString python_kernel_name idx raw_arg schema_arg enumerate zip raw_args op_overload _schema arguments lines += generate_py_arg py_args_var idx + raw_arg schema_arg real_type lines += textwrap dedent f Call custom op Python RAIIPyObject py_ buf_name PyObject_CallObject custom_op_wrapper py_args_var py_ buf_name PyErr_Occurred throw std runtime_error PyObject_CallObject python_kernel_name failed len output_args == output = output_args None result single tensor lines += f output = reinterpret_cast AtenTensorHandle PyCapsule_GetPointer py_ buf_name get NULL \n result tuple tensors idx output_arg enumerate output_args output_arg None continue lines += f output_arg = reinterpret_cast AtenTensorHandle PyCapsule_GetPointer PyList_GET_ITEM py_ buf_name get idx NULL \n noqa B raw_outputs declarations_before_scope = f RAIIAtenTensorHandle output_arg output_arg raw_output_arg zip output_args raw_outputs type ignore arg-type output_arg None isinstance raw_output_arg ir MutationOutput declarations_before_scope = f RAIIAtenTensorHandle output_arg output_arg output_args type ignore arg-type output_arg None scope_gil_acquire = generate_scoped_gil_acquire declarations_before_scope lines writelines scope_gil_acquire generate_fallback_kernel_with_runtime_lookup_aot op_overload Union torch _ops OpOverload torch _ops HigherOrderOperator raw_args Sequence Any output_args _OUTPUT_ARGS_TYPE raw_outputs Sequence ir Buffer - None tensor_call_args int_call_args = generate_extern_kernel_args_decl_if_needed op_overload raw_args output_args raw_outputs force both temporary arrays generate mutable data pointers since proxy executor signature requires datatype int_call_str = _generate_temporary_array_pointer int _t int_call_args force_mutable=True tensor_call_str = _generate_temporary_array_pointer AtenTensorHandle tensor_call_args force_mutable=True extern_kernel_node_index = len V extern_kernel_nodes - writeline f aoti_torch_proxy_executor_call_function proxy_executor f extern_kernel_node_index f len int_call_args f int_call_str f len tensor_call_args f tensor_call_str generate_reset_kernel_saved_flags pass generate_save_uncompiled_kernels pass c_type_for_prim_type val type_ - str isinstance type_ torch OptionalType f c_type_for_prim_type val type_ getElementType isinstance type_ torch TensorType AtenTensorHandle isinstance type_ torch IntType torch SymIntType int _t isinstance type_ torch BoolType torch SymBoolType torch EnumType repr type_ Layout MemoryFormat ScalarType int _t isinstance type_ torch FloatType double isinstance type_ torch NumberType isinstance val bool int _t isinstance val int float double val None This could happen when val optional value double raise AssertionError f Unexpected type c_type_for_prim_type type_= isinstance type_ torch StringType const char raise AssertionError f Unexpected type c_type_for_prim_type type_= val_to_arg_str_for_prim_type val type_ - str TODO using type_ first step refactoring Will update later isinstance val bool val isinstance val int uint _t long Linux long long MacOS Windows f val LL sys platform darwin win f val L isinstance val complex f c complex double generate_float_value val real generate_float_value val imag isinstance val str f val isinstance val ir Buffer ir ReinterpretView ir StorageBox ir TensorBox val codegen_reference isinstance val torch device codegen_device val isinstance val torch dtype codegen_dtype val isinstance val torch layout codegen_layout val isinstance val torch memory_format codegen_memory_format val isinstance val float generate_float_value val isinstance val list tuple FIXME This happens because type_ always properly set torch ListType f join val_to_arg_str x None x val isinstance val SymTypes cexpr val node expr isinstance val sympy Expr cexpr val repr val val_to_arg_str val type_=None - str val None None needs special care It either represent nullopt empty tensor type_ None isinstance type_ torch OptionalType type_ None isinstance type_ getElementType torch DeviceObjType torch ListType torch TupleType nullptr nullptr isinstance type_ torch TensorType create empty tensor equivalent Tensor var_name = f var_ next arg_var_id writeline f AtenTensorHandle var_name _handle writeline f AOTI_TORCH_ERROR_CODE_CHECK aoti_torch_new_uninitialized_tensor var_name _handle writeline f RAIIAtenTensorHandle var_name var_name _handle var_name raise AssertionError Can map None known data type isinstance type_ torch OptionalType element_type = type_ getElementType arg_str = val_to_arg_str val element_type Handle optional iterables special case Utilize temporary_reference function avoid saving them off increasing memory usage isinstance element_type torch ListType torch TupleType main_value aux = arg_str rsplit maxsplit= f temporary_reference main_value aux Handle optional tensors special case above isinstance element_type torch TensorType base_handle = val_to_arg_str val element_type f temporary_reference base_handle get var_name = f var_ next arg_var_id isinstance element_type torch DeviceObjType main_value aux = arg_str rsplit maxsplit= writeline f auto var_name = main_value f var_name aux writeline f c_type_for_prim_type val element_type var_name = arg_str f var_name isinstance type_ torch ListType torch TupleType assert isinstance val list tuple f val does match arg type type_ element_type = type_ getElementType len val == Zero-size array supported C C++ standard so nullptr nullptr result = val_to_arg_str x element_type x val isinstance element_type torch TensorType result = f t get t result c_type = c_type_for_prim_type val element_type see comment _generate_temporary_array_pointer explanation why c_type gets modified isinstance element_type torch OptionalType c_type startswith const c_type = f const c_type need pass array length because we can t use std array member function f _generate_temporary_array_pointer c_type result len val val_is_scalar = isinstance val bool complex float int SymTypes isinstance type_ torch TensorType val_is_scalar val_str = val_to_arg_str_for_prim_type val None codegen_scalar_to_tensor val_str val_to_arg_str_for_prim_type val type_ create_tmp_raii_handle_var_if_needed handle str writer Optional Union HasWriteLine list str = None - str If input handle rvalue RAII tensor creates lvalue variable writer Returns variable name can used access handle handle startswith borrow_arrayref_tensor_as_tensor copy_arrayref_tensor_to_tensor wrap_with_raii_handle_if_needed RAIIAtenTensorHandle handle tmp_var_name = f var_ next arg_var_id call_str = f auto tmp_var_name = handle writer = writer writer None isinstance writer list writer append call_str writer writeline call_str tmp_var_name write_kernel_context_guard_begin Beginning kernel context guarded block The block looks like KernelContextGuard _ctx kernel_name stack_trace_str operations writeline write_kernel_context_guard_end End kernel context guarded block writeline write_kernel_context_guard kernel_name str node_schedule Union Sequence BaseSchedulerNode ExternKernel aggregate_stack_traces node_schedule Union Sequence BaseSchedulerNode ExternKernel - OrderedSet str isinstance node_schedule list functools reduce lambda b &#124; b node node get_stack_traces node node_schedule hasattr node node node node OrderedSet isinstance node_schedule ExternKernel node_schedule get_stack_traces OrderedSet stack_trace_str = R stack_traces = aggregate_stack_traces node_schedule stack_trace stack_traces line stack_trace split \n stack_trace_str += f \n line stack_trace_str += \n stack_trace_str += writeline f KernelContextGuard _ctx kernel_name stack_trace_str