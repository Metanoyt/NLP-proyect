Owner s oncall profiler functools os re textwrap traceback unittest expecttest torch torch _C _profiler _ExtraFields_PyCall _ExtraFields_PyCCall torch testing _internal common_utils IS_ARM IS_WINDOWS run_tests skipIfTorchDynamo TEST_WITH_CROSSREF TestCase torch utils _pytree tree_map These functions can vary based platform build e g CUDA generally distract rather than adding test PRUNE_ALL = KEEP_ELLIPSES = KEEP_NAME_AND_ELLIPSES = PRUNE_FUNCTIONS = torch utils _pytree py tree_map KEEP_NAME_AND_ELLIPSES torch profiler profiler py start KEEP_ELLIPSES torch profiler profiler py stop_trace KEEP_ELLIPSES torch profiler profiler py _transit_action KEEP_ELLIPSES built-in method __exit__ torch _C DisableTorchFunctionSubclass object xXXXXXXXXXXXX PRUNE_ALL cudaStreamIsCapturing PRUNE_ALL These show up only CUDA prune them so CUDA CPU expected results can same cudaGetDeviceCount PRUNE_ALL cudaGetDeviceProperties_v PRUNE_ALL ROCTracer currently producing events profiler can extract We should bring up parity CUPTI Kineto profiler integration mean time there still utility running tests checking values match expected value We will still catch runtime errors assert failures We can diff output see how far we parity TODO We also fail capture events Windows some platforms ALLOW_CUDA_FAILURE = torch version hip None IS_WINDOWS TorchFunctionTensor torch Tensor classmethod __torch_function__ cls func types args= kwargs=None super __torch_function__ func types args kwargs TorchDispatchTensor torch Tensor staticmethod __new__ cls elem t = torch Tensor _make_subclass cls elem elem requires_grad t elem = elem t classmethod __torch_dispatch__ cls func types args= kwargs=None unwrap x x elem isinstance x TorchDispatchTensor x wrap x TorchDispatchTensor x isinstance x torch Tensor x args = tree_map unwrap args kwargs = tree_map unwrap kwargs tree_map wrap func args kwargs ProfilerTree staticmethod test f Mark unit test will using ProfilerTree test traces This decorator serves two purposes First provides method name ` format ` can use tell where test runner which environment specific ends unit test begins Second runs test replicates allows ` assertTreesMatch ` adjust based which replicate running functools wraps f begin_unit_test_marker replicates= try i range replicates tree_replicate = i out = f tree_replicate None break out finally delattr tree_replicate begin_unit_test_marker classmethod format cls profiler indent int = flatten nodes depth= out=None out None out = node nodes cls validate_node node name = cls fmt_name node name prune_level = PRUNE_FUNCTIONS get name strip None prune_level None out append depth name flatten node children depth + out prune_level == KEEP_NAME_AND_ELLIPSES out append depth name node children out append depth + prune_level == KEEP_ELLIPSES out append depth assert prune_level == PRUNE_ALL out flat_nodes = flatten profiler kineto_results experimental_event_tree Profiler inserts ` cudaDeviceSynchronize ` end profiling may also insert Context Sync CUDA synchronization event flat_nodes flat_nodes - == cudaDeviceSynchronize flat_nodes = flat_nodes - flat_nodes flat_nodes - == cudaDeviceSynchronize flat_nodes = flat_nodes - Profiler inserts ` hipDeviceSynchronize ` end profiling flat_nodes flat_nodes - == hipDeviceSynchronize flat_nodes = flat_nodes - min_depth = min d + d name flat_nodes begin_unit_test_marker name textwrap indent \n join f d - min_depth name rstrip d name flat_nodes d = min_depth indent staticmethod fmt_name name str - str match = re match r ^ \ py\ - + \ $ name match filename _ fn = match groups This test can appear ` test profiler test_profiler_tree py ` depending where run test_file = os path splitext os path split __file__ filename endswith test_file filename = test_file We test against string literal so all paths have look like POSIX paths filename = filename replace os sep We don t want have update test every time PyTorch changes At some point we should test some line numbers now s too brittle lineno = f filename py lineno fn kernel_pattern void native elementwise_kernel void native reduce_kernel void native vectorized_elementwise_kernel void native unrolled_elementwise_kernel r void a-zA-Z - +_kernel Nvidia kernels name = re sub rf kernel_pattern + \ +\ $ f kernel_pattern replace a-zA-Z - + name HACK patches around fact PyBind improperly sets __qualname__ attribute functions methods see https github com pybind pybind issues This should removed issue fixed name = re sub r pybind _builtins\ pybind _detail_function_record_v ^ + PyCapsule name re sub object x - a-fA-F + object xXXXXXXXXXXXX name classmethod validate_node cls node extra_fields = node extra_fields isinstance extra_fields _ExtraFields_PyCall _ExtraFields_PyCCall Check lineage established profiler matches caller recorded Python tracer parent = node parent while parent None isinstance parent extra_fields _ExtraFields_PyCall break parent = parent parent to_string frame_state f frame_state file_name frame_state function_name parent parent_name = to_string parent extra_fields callsite caller_name = to_string extra_fields caller assert parent_name == caller_name f parent_name vs caller_name unittest skipIf IS_ARM Not working ARM TestProfilerTree TestCase assertTreesMatch actual str expected str allow_failure bool = False Warning Here dragons Different platforms will have subtly different behavior Python tracing Observed differences include Windows symbolicates names differently posix The profile callback c_call does fire Tensor __pow__ certain platforms This caused function tracer cPython itself The purpose these unit tests ensure profiler doing reasonable things When these platform dependent variations occur simply coerce them into platform independent form If you made change codebase which changes trace produced simply use EXPECTTEST_ACCEPT= update tests reflect new structure expecttest will show diff view ` len actual len expected ` expecttest ACCEPT actual = actual ljust len expected maxDiff = None replicate = getattr tree_replicate None assertIsNotNone replicate Please annotate test ` ProfilerTree test ` The profiler should produce deterministic results should clean state after each run As result only first replicate allowed update ` expected ` If subsequent runs do match bug profiler replicate assertEqual actual expected try assertExpectedInline actual expected skip= except AssertionError e allow_failure tree_replicate = None msg = traceback format_exception_only type e e print msg split AssertionError - raise TODO Add logic CUDA version test ProfilerTree test unittest skipIf torch cuda is_available torch xpu is_available Test working CUDA XPU test_profiler_experimental_tree t t = torch ones requires_grad=True torch ones requires_grad=True torch profiler profile p z = torch add t t y = torch ones loss = y - z loss backward assertTreesMatch ProfilerTree format p profiler \ aten add aten ones aten empty aten fill_ aten sub aten pow aten result_type aten aten ones_like aten empty_like aten empty_strided aten fill_ autograd engine evaluate_function PowBackward PowBackward aten pow aten result_type aten aten copy_ aten mul aten mul aten aten _to_copy aten empty_strided aten copy_ aten mul autograd engine evaluate_function SubBackward SubBackward aten neg autograd engine evaluate_function AddBackward AddBackward autograd engine evaluate_function torch autograd AccumulateGrad torch autograd AccumulateGrad aten new_empty_strided aten empty_strided aten copy_ autograd engine evaluate_function torch autograd AccumulateGrad torch autograd AccumulateGrad aten detach detach TODO Add logic CUDA version test ProfilerTree test unittest skipIf torch cuda is_available torch xpu is_available Test working CUDA XPU test_profiler_experimental_tree_with_record_function torch profiler profile p torch autograd profiler record_function Top level Annotation torch autograd profiler record_function First Annotation x = torch ones requires_grad=True Check we correctly handle case when user annotation does call ` __exit__ ` _ = torch autograd profiler record_function Second Annotation __enter__ y = x + torch autograd profiler record_function Third Annotation y backward NB The ` aten zeros ` before record function annotations due ` cpp_custom_type_hack ` When we switch ` torch CustomClassHolder ` they will disappear assertTreesMatch ProfilerTree format p profiler \ Top level Annotation First Annotation aten ones aten empty aten fill_ Second Annotation aten add aten aten _to_copy aten empty_strided aten copy_ Third Annotation aten ones_like aten empty_like aten empty_strided aten fill_ autograd engine evaluate_function AddBackward AddBackward autograd engine evaluate_function torch autograd AccumulateGrad torch autograd AccumulateGrad aten new_empty_strided aten empty_strided aten copy_ TODO Add logic CUDA version test ProfilerTree test unittest skipIf torch cuda is_available torch xpu is_available Test working CUDA XPU test_profiler_experimental_tree_with_memory t t = torch ones requires_grad=True torch ones requires_grad=True torch profiler profile profile_memory=True p z = torch add t t y = torch ones loss = y - z loss backward assertTreesMatch ProfilerTree format p profiler \ aten add memory aten ones aten empty memory aten fill_ aten sub memory aten pow aten result_type aten memory aten ones_like aten empty_like aten empty_strided memory aten fill_ autograd engine evaluate_function PowBackward PowBackward aten pow aten result_type aten memory aten copy_ aten mul memory aten mul aten aten _to_copy aten empty_strided memory aten copy_ memory memory memory aten mul memory memory memory memory autograd engine evaluate_function SubBackward SubBackward aten neg memory memory autograd engine evaluate_function AddBackward AddBackward autograd engine evaluate_function torch autograd AccumulateGrad torch autograd AccumulateGrad aten new_empty_strided aten empty_strided memory aten copy_ autograd engine evaluate_function torch autograd AccumulateGrad torch autograd AccumulateGrad aten detach detach memory unittest skip https github com pytorch pytorch issues unittest skipIf TEST_WITH_CROSSREF crossref intercepts calls changes callsite ProfilerTree test test_profiler_experimental_tree_with_memory_and_stack t t = torch ones requires_grad=True torch ones requires_grad=True torch profiler profile with_stack=True profile_memory=True p z = torch add t t y = torch ones loss = torch pow y - z loss backward assertTreesMatch ProfilerTree format p profiler \ test_profiler_tree py test_profiler_experimental_tree_with_memory_and_stack torch profiler profiler py __enter__ built-in method add type object xXXXXXXXXXXXX aten add memory built-in method ones type object xXXXXXXXXXXXX aten ones aten empty memory aten fill_ aten sub memory built-in method pow type object xXXXXXXXXXXXX aten pow aten result_type aten memory torch _tensor py backward built-in function _has_torch_function_unary torch autograd __init__ py backward built-in method _are_functorch_transforms_active PyCapsule object xXXXXXXXXXXXX built-in function isinstance built-in function isinstance built-in function len torch autograd __init__ py _tensor_or_tensors_to_tuple torch autograd __init__ py _make_grads typing py inner typing py __hash__ built-in function hash typing py cast built-in function isinstance built-in function isinstance built-in function isinstance built-in function isinstance built-in function isinstance built-in function isinstance built-in method numel Tensor object xXXXXXXXXXXXX built-in function isinstance built-in function isinstance built-in method ones_like type object xXXXXXXXXXXXX aten ones_like aten empty_like aten empty_strided memory aten fill_ built-in method append list object xXXXXXXXXXXXX torch autograd graph py _engine_run_backward logging __init__ py getEffectiveLevel built-in method run_backward torch _C _EngineBase object xXXXXXXXXXXXX autograd engine evaluate_function PowBackward PowBackward aten pow aten result_type aten memory aten copy_ aten mul memory aten mul aten aten _to_copy aten empty_strided memory aten copy_ memory memory memory aten mul memory memory memory memory autograd engine evaluate_function SubBackward SubBackward aten neg memory memory autograd engine evaluate_function AddBackward AddBackward autograd engine evaluate_function torch autograd AccumulateGrad torch autograd AccumulateGrad aten new_empty_strided aten empty_strided memory aten copy_ autograd engine evaluate_function torch autograd AccumulateGrad torch autograd AccumulateGrad aten detach detach memory torch profiler profiler py __exit__ torch profiler profiler py stop skipIfTorchDynamo too slow unittest skipIf TEST_WITH_CROSSREF crossref intercepts calls changes callsite ProfilerTree test test_profiler_experimental_tree_with_stack_and_modules MyModule torch nn Module __init__ - None super __init__ layers = torch nn ReLU torch nn Linear torch nn ReLU forward x torch Tensor - torch Tensor l layers x = l x x model = MyModule torch profiler profile with_stack=True p _ range model torch ones maxDiff = None assertTreesMatch ProfilerTree format p profiler \ test_profiler_tree py test_profiler_experimental_tree_with_stack_and_modules torch profiler profiler py __enter__ built-in method ones type object xXXXXXXXXXXXX aten ones aten empty aten fill_ nn Module MyModule_ torch nn modules module py _call_impl built-in method _get_tracing_state PyCapsule object xXXXXXXXXXXXX test_profiler_tree py forward nn Module ReLU_ torch nn modules module py _call_impl built-in method _get_tracing_state PyCapsule object xXXXXXXXXXXXX torch nn modules activation py forward torch nn functional py relu built-in function _has_torch_function_unary built-in method relu type object xXXXXXXXXXXXX aten relu aten clamp_min nn Module Linear_ torch nn modules module py _call_impl built-in method _get_tracing_state PyCapsule object xXXXXXXXXXXXX torch nn modules linear py forward torch nn modules module py __getattr__ torch nn modules module py __getattr__ built-in function linear aten linear aten reshape aten view aten t aten transpose aten as_strided aten addmm aten expand aten as_strided aten copy_ aten resolve_conj aten resolve_conj aten resolve_conj aten view nn Module ReLU_ torch nn modules module py _call_impl built-in method _get_tracing_state PyCapsule object xXXXXXXXXXXXX torch nn modules activation py forward torch nn functional py relu built-in function _has_torch_function_unary built-in method relu type object xXXXXXXXXXXXX aten relu aten clamp_min built-in method ones type object xXXXXXXXXXXXX aten ones aten empty aten fill_ nn Module MyModule_ torch nn modules module py _call_impl built-in method _get_tracing_state PyCapsule object xXXXXXXXXXXXX test_profiler_tree py forward nn Module ReLU_ torch nn modules module py _call_impl built-in method _get_tracing_state PyCapsule object xXXXXXXXXXXXX torch nn modules activation py forward torch nn functional py relu built-in function _has_torch_function_unary built-in method relu type object xXXXXXXXXXXXX aten relu aten clamp_min nn Module Linear_ torch nn modules module py _call_impl built-in method _get_tracing_state PyCapsule object xXXXXXXXXXXXX torch nn modules linear py forward torch nn modules module py __getattr__ torch nn modules module py __getattr__ built-in function linear aten linear aten reshape aten view aten t aten transpose aten as_strided aten addmm aten expand aten as_strided aten copy_ aten resolve_conj aten resolve_conj aten resolve_conj aten view nn Module ReLU_ torch nn modules module py _call_impl built-in method _get_tracing_state PyCapsule object xXXXXXXXXXXXX torch nn modules activation py forward torch nn functional py relu built-in function _has_torch_function_unary built-in method relu type object xXXXXXXXXXXXX aten relu aten clamp_min torch profiler profiler py __exit__ torch profiler profiler py stop unittest skipIf TEST_WITH_CROSSREF crossref intercepts calls changes callsite ProfilerTree test test_profiler_experimental_tree_with_stack_and_torch_function x = TorchFunctionTensor torch ones y = torch ones There s some lazy initialization __torch_function__ If we don t run first run won t match replicates torch add x y torch profiler profile with_stack=True p torch add x y assertTreesMatch ProfilerTree format p profiler \ test_profiler_tree py test_profiler_experimental_tree_with_stack_and_torch_function torch profiler profiler py __enter__ built-in method add type object xXXXXXXXXXXXX test_profiler_tree py __torch_function__ torch _tensor py __torch_function__ built-in function all torch _tensor py genexpr built-in function issubclass torch _tensor py genexpr built-in method add type object xXXXXXXXXXXXX aten add torch _tensor py _convert built-in function isinstance built-in function isinstance built-in method as_subclass Tensor object xXXXXXXXXXXXX aten alias built-in function isinstance torch profiler profiler py __exit__ torch profiler profiler py stop skipIfTorchDynamo segfaults + unittest skipIf TEST_WITH_CROSSREF crossref intercepts calls changes callsite ProfilerTree test test_profiler_experimental_tree_with_stack_and_torch_dispatch x = TorchDispatchTensor torch ones y = torch ones warmup round torch profiler profile with_stack=True x + y torch profiler profile with_stack=True p x + y assertTreesMatch ProfilerTree format p profiler \ test_profiler_tree py test_profiler_experimental_tree_with_stack_and_torch_dispatch torch profiler profiler py __enter__ aten add PythonSubclass torch _library simple_registry py find_torch_dispatch_rule torch _library simple_registry py find built-in method get dict object xXXXXXXXXXXXX torch _library simple_registry py find built-in method get dict object xXXXXXXXXXXXX test_profiler_tree py __torch_dispatch__ torch utils _pytree py tree_map torch utils _pytree py tree_map torch _ops py __call__ built-in method PyCapsule object xXXXXXXXXXXXX aten add torch utils _pytree py tree_map torch profiler profiler py __exit__ torch profiler profiler py stop unittest skip https github com pytorch pytorch issues unittest skipIf torch cuda is_available CUDA required ProfilerTree test test_profiler_experimental_tree_cuda torch profiler profile profile_memory=True p weight = torch ones device= cuda requires_grad=True x = torch ones device= cuda y = torch add weight x loss = torch pow y loss backward torch optim SGD weight lr= momentum= step assertTreesMatch ProfilerTree format p profiler \ aten ones aten empty memory aten fill_ cudaLaunchKernel void native vectorized_elementwise_kernel aten ones aten empty memory aten fill_ cudaLaunchKernel void native vectorized_elementwise_kernel aten add cudaLaunchKernel void native vectorized_elementwise_kernel memory aten pow cudaLaunchKernel void native vectorized_elementwise_kernel aten result_type aten memory aten ones_like aten empty_like aten empty_strided memory aten fill_ cudaLaunchKernel void native vectorized_elementwise_kernel autograd engine evaluate_function PowBackward PowBackward aten pow aten result_type aten memory aten copy_ cudaMemcpyAsync Memcpy DtoD Device - Device aten mul memory aten mul cudaLaunchKernel void native vectorized_elementwise_kernel memory memory aten mul cudaLaunchKernel void native vectorized_elementwise_kernel memory memory memory autograd engine evaluate_function AddBackward AddBackward autograd engine evaluate_function torch autograd AccumulateGrad torch autograd AccumulateGrad aten detach detach memory aten zeros aten zeros aten empty memory aten zero_ Optimizer step#SGD step aten empty memory memory memory aten clone aten empty_strided memory aten copy_ cudaMemcpyAsync Memcpy DtoD Device - Device aten detach detach aten add_ cudaLaunchKernel void native vectorized_elementwise_kernel memory noqa B allow_failure=ALLOW_CUDA_FAILURE unittest skip https github com pytorch pytorch issues unittest skipIf torch cuda is_available CUDA required ProfilerTree test test_profiler_experimental_tree_cuda_with_stream streams = torch cuda Stream _ range results = torch profiler profile profile_memory=True p x = torch ones device= cuda stream streams torch cuda stream stream results append torch tanh x - x del results s streams torch cuda current_stream wait_stream s assertTreesMatch ProfilerTree format p profiler \ aten ones aten empty memory aten fill_ cudaLaunchKernel void native vectorized_elementwise_kernel aten tanh cudaMalloc cudaLaunchKernel void native vectorized_elementwise_kernel memory aten sub cudaLaunchKernel void native vectorized_elementwise_kernel memory memory aten tanh cudaMalloc cudaLaunchKernel void native vectorized_elementwise_kernel memory aten sub cudaLaunchKernel void native vectorized_elementwise_kernel memory memory aten tanh cudaMalloc cudaLaunchKernel void native vectorized_elementwise_kernel memory aten sub cudaLaunchKernel void native vectorized_elementwise_kernel memory memory allow_failure=ALLOW_CUDA_FAILURE unittest skip https github com pytorch pytorch issues unittest skipIf TEST_WITH_CROSSREF crossref intercepts calls changes callsite unittest skipIf torch cuda is_available CUDA required ProfilerTree test test_profiler_experimental_tree_cuda_detailed Do lazy imports ahead time avoid showing up tree torch nested _internal nested_tensor model = torch nn modules Linear device= cuda model train opt = torch optim SGD model parameters lr= momentum= step x = torch ones device= cuda loss = model x loss backward opt step Warmup _ range step torch profiler profile profile_memory=True with_stack=True p step assertTreesMatch ProfilerTree format p profiler \ test_profiler_tree py test_profiler_experimental_tree_cuda_detailed torch profiler profiler py __enter__ test_profiler_tree py step built-in method ones type object xXXXXXXXXXXXX aten ones aten empty memory aten fill_ cudaLaunchKernel void native vectorized_elementwise_kernel nn Module Linear_ built-in method _get_tracing_state PyCapsule object xXXXXXXXXXXXX torch nn modules linear py forward torch nn modules module py __getattr__ torch nn modules module py __getattr__ built-in function linear aten linear aten t aten transpose aten as_strided aten addmm cudaMemcpyAsync Memcpy DtoD Device - Device cudaLaunchKernel void _kernel memory aten expand aten as_strided torch _tensor py backward built-in function _has_torch_function_unary torch autograd __init__ py backward built-in function isinstance built-in function isinstance built-in function len torch autograd __init__ py _tensor_or_tensors_to_tuple torch autograd __init__ py _make_grads typing py inner typing py __hash__ built-in function hash typing py cast built-in function isinstance built-in function isinstance built-in function isinstance built-in function isinstance built-in function isinstance built-in function isinstance built-in method numel Tensor object xXXXXXXXXXXXX built-in function isinstance built-in function isinstance built-in method ones_like type object xXXXXXXXXXXXX aten ones_like aten empty_like aten empty_strided memory aten fill_ cudaLaunchKernel void native vectorized_elementwise_kernel built-in method append list object xXXXXXXXXXXXX built-in method run_backward torch _C _EngineBase object xXXXXXXXXXXXX autograd engine evaluate_function AddmmBackward AddmmBackward aten t aten transpose aten as_strided aten mm cudaLaunchKernel void _kernel memory aten t aten transpose aten as_strided aten sum aten sum cudaLaunchKernel void native reduce_kernel memory aten view aten view autograd engine evaluate_function torch autograd AccumulateGrad torch autograd AccumulateGrad aten add_ cudaLaunchKernel void native vectorized_elementwise_kernel memory autograd engine evaluate_function TBackward TBackward aten t aten transpose aten as_strided autograd engine evaluate_function torch autograd AccumulateGrad torch autograd AccumulateGrad aten add_ cudaLaunchKernel void native vectorized_elementwise_kernel memory memory torch optim optimizer py wrapper built-in method format str object xXXXXXXXXXXXX torch autograd profiler py __init__ built-in method zeros type object xXXXXXXXXXXXX aten zeros aten zeros aten empty memory aten zero_ torch autograd profiler py __enter__ torch _ops py __call__ built-in method _record_function_enter PyCapsule object xXXXXXXXXXXXX Optimizer step#SGD step aten empty memory memory memory torch optim optimizer py _use_grad built-in function is_grad_enabled torch autograd grad_mode py __init__ built-in function is_grad_enabled built-in function _set_grad_enabled torch optim sgd py step built-in method append list object xXXXXXXXXXXXX built-in method append list object xXXXXXXXXXXXX torch _tensor py __hash__ built-in function id built-in method append list object xXXXXXXXXXXXX built-in method append list object xXXXXXXXXXXXX built-in method append list object xXXXXXXXXXXXX torch _tensor py __hash__ built-in function id built-in method append list object xXXXXXXXXXXXX torch optim sgd py sgd torch optim sgd py _single_tensor_sgd built-in method mul_ Tensor object xXXXXXXXXXXXX memory aten mul_ cudaLaunchKernel void native vectorized_elementwise_kernel memory built-in method add_ Tensor object xXXXXXXXXXXXX aten add_ cudaLaunchKernel void native vectorized_elementwise_kernel built-in method add_ Tensor object xXXXXXXXXXXXX aten add_ cudaLaunchKernel void native vectorized_elementwise_kernel built-in method mul_ Tensor object xXXXXXXXXXXXX memory aten mul_ cudaLaunchKernel void native vectorized_elementwise_kernel memory built-in method add_ Tensor object xXXXXXXXXXXXX aten add_ cudaLaunchKernel void native vectorized_elementwise_kernel built-in method add_ Tensor object xXXXXXXXXXXXX aten add_ cudaLaunchKernel void native vectorized_elementwise_kernel torch _tensor py __hash__ built-in function id torch _tensor py __hash__ built-in function id torch autograd grad_mode py __init__ built-in function is_grad_enabled built-in function _set_grad_enabled torch autograd profiler py __exit__ torch _ops py __call__ built-in method _record_function_exit PyCapsule object xXXXXXXXXXXXX memory memory torch profiler profiler py __exit__ torch profiler profiler py stop torch profiler profiler py _transit_action built-in method get dict object xXXXXXXXXXXXX enum py __hash__ built-in function hash noqa B allow_failure=ALLOW_CUDA_FAILURE __name__ == __main__ run_tests