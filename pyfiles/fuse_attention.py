mypy allow-untyped-defs functools inspect logging math torch _dynamo utils counters pattern_matcher filter_nodes fwd_only gen_register_replacement joint_fwd_bwd log = logging getLogger __name__ aten = torch ops aten _scaled_dot_product_attention = aten scaled_dot_product_attention _sfdp_pattern_ query key value inv_scale torch matmul query key transpose - - div inv_scale softmax dim=- matmul value _sfdp_replacement_ query key value inv_scale counters inductor fuse_attention += _scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False scale= inv_scale _sfdp_pattern_ query key value scale_factor torch matmul query key transpose - - mul scale_factor softmax dim=- matmul value _sfdp_replacement_ query key value scale_factor counters inductor fuse_attention += _scaled_dot_product_attention query key value attn_mask=None dropout_p= is_causal=False scale=scale_factor _sfdp_pattern_ query key value inv_scale_factor dropout_p torch nn functional dropout torch matmul query key transpose - - div inv_scale_factor softmax dim=- p=dropout_p matmul value _sfdp_replacement_ query key value inv_scale_factor dropout_p counters inductor fuse_attention += _scaled_dot_product_attention query key value attn_mask=None dropout_p=dropout_p is_causal=False scale= inv_scale_factor _sfdp_pattern_ query key value scale_factor dropout_p torch nn functional dropout torch matmul query key transpose - - mul scale_factor softmax dim=- p=dropout_p matmul value _sfdp_replacement_ query key value scale_factor dropout_p counters inductor fuse_attention += _scaled_dot_product_attention query key value attn_mask=None dropout_p=dropout_p is_causal=False scale=scale_factor _sfdp_pattern_ query key value attn_mask attn_weight = torch softmax query key transpose - - math sqrt query size - + attn_mask dim=- attn_weight = torch dropout attn_weight dropout_p attn_weight value _sfdp_replacement_ query key value attn_mask counters inductor fuse_attention += _scaled_dot_product_attention query key value attn_mask=attn_mask dtype=query dtype dropout_p= is_causal=False _sfdp_pattern_ query key value attn_mask dropout_p attn_weight = torch softmax query key transpose - - math sqrt query size - + attn_mask dim=- attn_weight = torch dropout attn_weight dropout_p True attn_weight value _sfdp_replacement_ query key value attn_mask dropout_p counters inductor fuse_attention += _scaled_dot_product_attention query key value attn_mask=attn_mask dtype=query dtype dropout_p=dropout_p is_causal=False _sfdp_pattern_ query key value dropout_p real workloads inputs matmul permuted causing matmul expand series expand clone calls we want same happen during pattern tracing q = query permute k = key permute v = value permute div = q k transpose - - math sqrt q size - div = div torch float attn_weight = torch softmax div dim=- attn_weight = torch dropout attn_weight dropout_p True attn_weight = attn_weight torch float attn_weight v _sfdp_replacement_ query key value dropout_p sdpa prefers inputs permuted format makes copy put them format they aren t already make replacement efficient ensure inputs sdpa required order counters inductor fuse_attention += q = query permute k = key permute v = value permute _scaled_dot_product_attention q k v attn_mask=None attn_mask dropout_p=dropout_p is_causal=False _sfdp_pattern_ query key value no dropout version pattern q = query permute k = key permute v = value permute div = q k transpose - - math sqrt q size - div = div torch float attn_weight = torch softmax div dim=- attn_weight = attn_weight torch float attn_weight v _sfdp_replacement_ query key value counters inductor fuse_attention += q = query permute k = key permute v = value permute _scaled_dot_product_attention q k v attn_mask=None attn_mask dropout_p= is_causal=False _sfdp_pattern_ query key value dropout_p q = query permute k = key permute v = value permute q = q math sqrt q size - div = q k transpose - - div = div torch float attn_weight = torch softmax div dim=- attn_weight = torch dropout attn_weight dropout_p True attn_weight = attn_weight torch float attn_weight v _sfdp_replacement_ query key value dropout_p counters inductor fuse_attention += q = query permute k = key permute v = value permute _scaled_dot_product_attention q k v attn_mask=None attn_mask dropout_p=dropout_p is_causal=False _sfdp_pattern_ query key value no dropout version q = query permute k = key permute v = value permute q = q math sqrt q size - div = q k transpose - - div = div torch float attn_weight = torch softmax div dim=- attn_weight = attn_weight torch float attn_weight v _sfdp_replacement_ query key value counters inductor fuse_attention += q = query permute k = key permute v = value permute _scaled_dot_product_attention q k v attn_mask=None attn_mask dropout_p= is_causal=False _sfdp_pattern_ query key value inv_scale Mainly huggingface models q = query permute k = key permute v = value permute torch matmul q k transpose - - div inv_scale softmax dim=- matmul v _sfdp_replacement_ query key value inv_scale counters inductor fuse_attention += _scaled_dot_product_attention query transpose key transpose value transpose attn_mask=None dropout_p= is_causal=False scale= inv_scale _sfdp_pattern_ query key value inv_scale_factor dropout_p q = query permute k = key permute v = value permute torch nn functional dropout torch matmul q k transpose - - div inv_scale_factor softmax dim=- p=dropout_p matmul v _sfdp_replacement_ query key value inv_scale_factor dropout_p counters inductor fuse_attention += _scaled_dot_product_attention query transpose key transpose value transpose attn_mask=None dropout_p=dropout_p is_causal=False scale= inv_scale_factor _sfdp_pattern_ query key value dropout_p attn_weight = torch bmm query key transpose softmax dim=- attn_weight = torch nn functional dropout attn_weight p=dropout_p torch bmm attn_weight value _sfdp_replacement_ query key value dropout_p counters inductor fuse_attention += _scaled_dot_product_attention query unsqueeze key unsqueeze value unsqueeze dropout_p=dropout_p scale= squeeze _sfdp_pattern_ query key value attn_mask inv_scale BertLarge Permutations needed create clones graph q = query permute k = key permute v = value permute torch matmul q k transpose - - div inv_scale + attn_mask softmax dim=- matmul v _sfdp_replacement_ query key value attn_mask inv_scale counters inductor fuse_attention += _scaled_dot_product_attention query transpose key transpose value transpose attn_mask=attn_mask dtype=query dtype dropout_p= is_causal=False scale= inv_scale _sfdp_pattern_ query key value attn_mask inv_scale DistilBert Permutations needed create clones graph Ref https github com pytorch pytorch issues q = query permute k = key permute v = value permute bs = q size k_len = k size - scores = q k transpose - - scores = scores div inv_scale fill_value = torch full -float inf dtype=query dtype device=query device attn_mask = attn_mask == view bs k_len expand_as scores torch softmax scores masked_fill attn_mask fill_value dim=- v _sfdp_replacement_ query key value attn_mask inv_scale counters inductor fuse_attention += bs = query size n_head = query size q_len = query size k_len = key size do attn_mask- logical_not _scaled_dot_product_attention attn_mask = attn_mask == view bs k_len expand bs n_head q_len k_len _scaled_dot_product_attention query transpose key transpose value transpose attn_mask=attn_mask dtype=torch bool dropout_p= is_causal=False scale= inv_scale _sfdp_pattern_ query key value attn_mask inv_scale dropout_p BertLarge dropout q = query permute k = key permute v = value permute torch nn functional dropout torch matmul q k transpose - - div inv_scale + attn_mask softmax dim=- dropout_p dtype=query dtype matmul v _sfdp_replacement_ query key value attn_mask inv_scale dropout_p counters inductor fuse_attention += _scaled_dot_product_attention query transpose key transpose value transpose attn_mask=attn_mask dtype=query dtype dropout_p=dropout_p is_causal=False scale= inv_scale _sfdp_pattern_ query key value attn_mask inv_scale dropout_p DistilBert dropout q = query permute k = key permute v = value permute bs = q size k_len = k size - scores = q k transpose - - scores = scores div inv_scale fill_value = torch full -float inf dtype=query dtype device=query device attn_mask = attn_mask == view bs k_len expand_as scores torch nn functional dropout torch softmax scores masked_fill attn_mask fill_value dim=- dropout_p v _sfdp_replacement_ query key value attn_mask inv_scale dropout_p counters inductor fuse_attention += bs = query size n_head = query size q_len = query size k_len = key size do attn_mask- logical_not _scaled_dot_product_attention attn_mask = attn_mask == view bs k_len expand bs n_head q_len k_len _scaled_dot_product_attention query transpose key transpose value transpose attn_mask=attn_mask dtype=torch bool dropout_p=dropout_p is_causal=False scale= inv_scale _sfdp_pattern_ query key value causal_mask dropout_p hf_GPT dropout introduces clone node inference also returns permuted key value query = query permute key = key permute value = value permute attn_weights = torch matmul query key permute inv_scale = torch full value size - dtype=attn_weights dtype device=attn_weights device attn_weights = attn_weights div inv_scale causal_mask_value = torch full torch finfo query dtype min dtype=query dtype device=query device attn_weights = torch where causal_mask attn_weights causal_mask_value torch nn functional dropout attn_weights softmax dim=- dropout_p matmul value key value _sfdp_replacement_ query key value causal_mask dropout_p counters inductor fuse_attention += permuted_key = key transpose permuted_value = value transpose _scaled_dot_product_attention query transpose permuted_key permuted_value attn_mask=causal_mask dropout_p=dropout_p is_causal=False scale= math sqrt value size - permuted_key permuted_value _sfdp_pattern_ query key value causal_mask attn_mask dropout_p token-classification+gpt text-generation+gpt attn_weights = torch matmul query key permute inv_scale = torch full value size - dtype=attn_weights dtype device=attn_weights device attn_weights = attn_weights div inv_scale causal_mask_value = torch full torch finfo query dtype min dtype=query dtype device=query device attn_weights = torch where causal_mask attn_weights causal_mask_value attn_weights = attn_weights + attn_mask attn_weights = attn_weights softmax dim=- type value dtype torch nn functional dropout attn_weights dropout_p matmul value _sfdp_replacement_ query key value causal_mask attn_mask dropout_p counters inductor fuse_attention += fill_value = torch full -float inf dtype=query dtype device=query device attn_mask = torch where causal_mask attn_mask fill_value _scaled_dot_product_attention query key value attn_mask=attn_mask dropout_p=dropout_p is_causal=False scale= math sqrt value size - _sfdp_pattern_ query key value attn_mask dropout_p DistilBert dropout transformers== q = query permute k = key permute v = value permute bs = q size k_len = k size - q = q div math sqrt q size - scores = q k transpose - - fill_value = torch full -float inf dtype=query dtype device=query device attn_mask = attn_mask == view bs k_len expand_as scores torch nn functional dropout torch softmax scores masked_fill attn_mask fill_value dim=- dropout_p v _sfdp_replacement_ query key value attn_mask dropout_p counters inductor fuse_attention += bs = query size n_head = query size q_len = query size k_len = key size do attn_mask- logical_not _scaled_dot_product_attention attn_mask = attn_mask == view bs k_len expand bs n_head q_len k_len _scaled_dot_product_attention query transpose key transpose value transpose attn_mask=attn_mask dtype=torch bool dropout_p=dropout_p is_causal=False scale= math sqrt query size - _sfdp_pattern_ query key value attn_mask T inplace add query = query permute key = key permute value = value permute score = torch matmul query key permute masked_score = score + attn_mask score = masked_score type_as query viewd_score = score view score size score size score size score size viewd_score = viewd_score view score size score size score size score size viewd_score float softmax dim=- type_as query matmul value _sfdp_replacement_ query key value attn_mask counters inductor fuse_attention += query = query permute key = key permute value = value permute _scaled_dot_product_attention query key value attn_mask=attn_mask dtype=query dtype is_causal=False scale= _sfdp_pattern_ query key value attn_mask T inplace add key value query = query permute key = key permute value = value permute score = torch matmul query key permute masked_score = score + attn_mask score = masked_score type_as query viewd_score = score view score size score size score size score size viewd_score = viewd_score view score size score size score size score size viewd_score float softmax dim=- type_as query matmul value key value _sfdp_replacement_ query key value attn_mask counters inductor fuse_attention += query = query permute key = key permute value = value permute _scaled_dot_product_attention query key value attn_mask=attn_mask dtype=query dtype is_causal=False scale= key value _sfdp_pattern_ query key value T inplace add key value attn_mask generated atem full query = query permute key = key permute value = value permute score = torch matmul query key permute fp _score = score float score = fp _score type_as query viewd_score = score view score size score size score size score size viewd_score = viewd_score view score size score size score size score size viewd_score float softmax dim=- type_as query matmul value key value _sfdp_replacement_ query key value counters inductor fuse_attention += query = query permute key = key permute value = value permute _scaled_dot_product_attention query key value attn_mask=None is_causal=False scale= key value _sfdp_pattern_ query key value attention_mask pattern MBartForCausalLM PLBartForCausalLM attn_mask has different dtype QKV there no scale sdpa bs = query size n_head = query size seq_len = query size head_size = query size q = query view bs n_head - head_size k = key reshape bs n_head - head_size v = value reshape bs n_head - head_size attn_weights = torch bmm q k transpose attn_weights = attn_weights view bs n_head seq_len - + attention_mask attn_weights = attn_weights view bs n_head seq_len - attn_weights = torch nn functional softmax attn_weights dim=- query dtype == torch half attn_weights = attn_weights torch half attn_output = torch bmm attn_weights v attn_output = attn_output view bs n_head seq_len head_size attn_output _sfdp_replacement_ query key value attention_mask counters inductor fuse_attention += _scaled_dot_product_attention query key value attn_mask=attention_mask dtype=query dtype is_causal=False scale= _sfdp_params_check match assert all k match kwargs k query key value query = match kwargs query meta val key = match kwargs key meta val value = match kwargs value meta val query dtype == key dtype == value dtype query device == key device == value device False add_mask_node = filter_nodes match nodes aten add Tensor Has attn_mask add len add_mask_node attn_mask_node = add_mask_node args attn_mask_node may float int number hasattr attn_mask_node meta False attn_mask = attn_mask_node meta val type ignore union-attr Make sure attn_mask dtype == query dtype attn_mask dtype == torch bool attn_mask dtype == torch float models like albert isinstance attn_mask torch Tensor attn_mask dtype == query dtype attn_mask dtype == torch bool attn_mask dtype == torch float query device = attn_mask device When we tensorify floats we end up turning floats into d scalar tensors It doesn t make any sense have d scalar tensor attention mask so conveniently we can insert check get tests erroneously passing float attention mask fail expected attn_mask dim == False True _sfdp_extra_check scale_factor_op=None disable_cuda=False fn match disable_cuda query match kwargs cuda str match kwargs query meta val device False scale_factor_op None scale_factor_node = filter_nodes match nodes scale_factor_op Note args scale_factor_node always scale_factor current patterns scale_factor = scale_factor_node args make sure scale_factor float int SymInt isinstance scale_factor float int False _sfdp_params_check match fn partialize_and_update_signature func kwargs Equivalent functools partial also updates signature returned function original_sig = inspect signature func parameters = original_sig parameters new_parameters = key value key value parameters items key kwargs new_sig = inspect Signature parameters=list new_parameters values partial_func = functools partial func kwargs wrapper args kwargs partial_func args kwargs wrapper __signature__ = new_sig type ignore attr-defined wrapper __name__ = func __name__ wrapper _get_sfdp_patterns joint_graph patterns torch cuda is_available workaround https github com pytorch pytorch issues device = cuda device = cpu sizes values don t actually matter initial trace once we get possible match we re-trace actual values verify match still holds g_inp = functools partial torch empty device=device requires_grad=True attn_mask b_inp = functools partial torch empty device=device m_inp = functools partial torch empty device=device need d attn_mask generate patterns view op m_inp_ d = functools partial torch empty device=device inv_scale c_inp = functools partial torch tensor device=device workaround https github com pytorch pytorch issues magic value lets us recover lost input arg relationship d = dropout_p we could also generate all these patterns d TODO g_ d_inp = functools partial torch empty device=device requires_grad=True reshape matmul decomposition generates clone when batch_size due memory layout change however when batch_size= reshape does change memory layout so clone would generated here we need trace input batch_size= generate pattern graph without clone g_bs _inp = functools partial torch empty device=device requires_grad=True m_bs _inp = functools partial torch empty device=device softmax will generate dtype conversion inputs they half will float so we generate pattern both dtype torch float torch half g = functools partial g_inp dtype=dtype b = functools partial b_inp dtype=dtype b_float = functools partial b_inp dtype=torch float b_bool = functools partial b_inp dtype=torch bool m = functools partial m_inp dtype=dtype m_float = functools partial m_inp dtype=torch float m_bool = functools partial m_inp dtype=torch bool m_ d = functools partial m_inp_ d dtype=dtype c = functools partial c_inp dtype=dtype g_ d = functools partial g_ d_inp dtype=dtype g_bs = functools partial g_bs _inp dtype=dtype m_bs = functools partial m_bs _inp dtype=dtype m_bs _float = functools partial m_bs _inp dtype=torch float m_bs _bool = functools partial m_bs _inp dtype=torch bool candidates = _sfdp_pattern_ _sfdp_replacement_ g g g c _sfdp_extra_check aten div Tensor _sfdp_pattern_ _sfdp_replacement_ g g g c _sfdp_extra_check aten mul Tensor _sfdp_pattern_ _sfdp_replacement_ g g g c d _sfdp_extra_check aten div Tensor _sfdp_pattern_ _sfdp_replacement_ g g g c d _sfdp_extra_check aten mul Tensor _sfdp_pattern_ _sfdp_replacement_ g g g b _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g g g b d _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g g g d _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g g g _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g g g d _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g g g _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g g g c _sfdp_extra_check aten div Tensor _sfdp_pattern_ _sfdp_replacement_ g g g c d _sfdp_extra_check aten div Tensor _sfdp_pattern_ _sfdp_replacement_ g_ d g_ d g_ d d _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g g g m c _sfdp_extra_check aten div Tensor _sfdp_pattern_ _sfdp_replacement_ g g g m_ d c _sfdp_extra_check aten div Tensor TODO Enable CUDA after solving Bert accuracy issue calling efficient attention _sfdp_pattern_ _sfdp_replacement_ g g g m c d _sfdp_extra_check aten div Tensor disable_cuda=True _sfdp_pattern_ _sfdp_replacement_ g_bs g_bs g_bs m_bs c d _sfdp_extra_check aten div Tensor disable_cuda=True _sfdp_pattern_ _sfdp_replacement_ g g g m_ d c d _sfdp_extra_check aten div Tensor _sfdp_pattern_ _sfdp_replacement_ g g g m_bool d _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g_bs g_bs g_bs m_bs _bool d _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g g g b_bool b_float d _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g g g m_ d d _sfdp_extra_check aten div Tensor _sfdp_pattern_ _sfdp_replacement_ g g g m_float _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g_bs g_bs g_bs m_bs _float _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g g g m_float _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g_bs g_bs g_bs m_bs _float _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g g g _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g_bs g_bs g_bs _sfdp_params_check _sfdp_pattern_ _sfdp_replacement_ g g g b_float _sfdp_extra_check mask_fp _patterns = pattern_ dtype == torch half Add inputs bf q k v fp mask models like albert candidates append _sfdp_pattern_ _sfdp_replacement_ g g g m_float c d _sfdp_extra_check aten div Tensor disable_cuda=True candidates append _sfdp_pattern_ _sfdp_replacement_ g_bs g_bs g_bs m_bs _float c d _sfdp_extra_check aten div Tensor disable_cuda=True pattern replacement args workaround extra_check candidates XXX when adding new pattern re-run ` gen_attention_patterns ` so pattern gets serialized python file does require tracing runtime assert isinstance workaround dict name = pattern __name__ dtype = torch float name += _half any p name p mask_fp _patterns args dtype == torch float name += _mask_fp args size == name += _bs training_name = name + _training yield training_name search_fn pattern replace_fn replacement example_inputs args trace_fn joint_fwd_bwd pass_dicts patterns extra_check extra_check scalar_workaround workaround workaround assert len workaround == dropout_p workaround functools partial insufficient because we look signature downstream pattern = partialize_and_update_signature pattern dropout_p= replacement = partialize_and_update_signature replacement dropout_p= workaround = inference_name = name + _inference yield inference_name search_fn pattern replace_fn replacement example_inputs args trace_fn fwd_only pass_dicts patterns extra_check extra_check scalar_workaround workaround dropout turned into clone we end up number semantically identical graphs skip_duplicates True functools cache _sfdp_init key register_replacement_kwargs _get_sfdp_patterns gen_register_replacement key register_replacement_kwargs