Owner s module dynamo functools inspect os pickle contextlib contextmanager unittest mock patch torch torch _dynamo testing torch _inductor config torch _inductor test_case torch onnx operators torch utils cpp_extension torch _dynamo aot_compile ModelInput SerializableCallable torch _dynamo exc PackageError Unsupported torch _dynamo package DynamoCache torch _dynamo precompile_context PrecompileContext torch _inductor runtime runtime_utils cache_dir torch fx _graph_pickler GraphPickler torch testing _internal common_utils instantiate_parametrized_tests MY_LAMBDA = lambda x x + noqa E CustomCompiledFunction torch _dynamo aot_compile SerializableCallable __init__ gm torch fx GraphModule example_inputs list torch Tensor gm = gm example_inputs = example_inputs classmethod serialize_compile_artifacts cls fn - bytes sympy torch _subclasses FakeTensorMode torch fx _graph_pickler Options state = fn __dict__ copy graph_reducer_override = GraphPickler reducer_override _graph_reducer_override obj inspect isclass obj issubclass obj sympy Function hasattr obj _torch_unpickler obj _torch_unpickler obj _torch_handler_name isinstance obj FakeTensorMode type None graph_reducer_override obj patch object GraphPickler reducer_override _graph_reducer_override state gm = GraphPickler dumps state gm Options ops_filter=None pickle dumps state classmethod deserialize_compile_artifacts cls data bytes state = pickle loads data fake_mode = torch _subclasses FakeTensorMode state gm = GraphPickler loads state gm fake_mode state gm recompile cls state __call__ args kwargs gm args kwargs SimpleLinearModule torch nn Module __init__ super __init__ linear = torch nn Linear forward x linear x RepeatInterleaveModule torch nn Module forward x chunk = x chunk dim=- y = chunk y_repeat = y repeat_interleave dim=- y_repeat MultiModalMixin torch nn Module forward x super forward x TextModel torch nn Module forward x x + TestVLLMModel MultiModalMixin TextModel forward x super forward x torch _dynamo config patch enable_aot_compile True instantiate_parametrized_tests TestAOTCompile torch _inductor test_case TestCase path path = os path join cache_dir f package_ id os makedirs path exist_ok=True os path join path model pt setUp super setUp torch _dynamo reset torch _dynamo utils counters clear DynamoCache clear PrecompileContext clear test_aot_compile_basic_fn fn x y x + y backend gm example_inputs CustomCompiledFunction gm example_inputs compiled_fn = torch compile fn fullgraph=True backend=backend aot_compile torch randn torch randn inputs = torch randn torch randn expected = fn inputs actual = compiled_fn inputs assertEqual expected actual compiled_fn save_compiled_function path torch _dynamo reset torch compiler set_stance fail_on_recompile open path rb f compiled_fn = torch compiler load_compiled_function f actual = compiled_fn inputs assertEqual expected actual test_aot_compile_basic_forward mod = SimpleLinearModule backend gm example_inputs CustomCompiledFunction gm example_inputs compiled_fn = torch compile mod fullgraph=True backend=backend forward aot_compile torch randn inputs = torch randn expected = mod inputs actual = compiled_fn mod inputs assertEqual expected actual compiled_fn save_compiled_function path torch _dynamo reset torch compiler set_stance fail_on_recompile open path rb f compiled_fn = torch compiler load_compiled_function f actual = compiled_fn mod inputs assertEqual expected actual test_aot_compile_repeat_interleave mod = RepeatInterleaveModule backend gm example_inputs CustomCompiledFunction gm example_inputs inputs = torch randn The first dim should dynamic repro issue repeat_interleave torch _dynamo mark_dynamic inputs compiled_fn = torch compile mod fullgraph=True backend=backend forward aot_compile inputs expected = mod inputs actual = compiled_fn mod inputs assertEqual expected actual compiled_fn save_compiled_function path torch _dynamo reset torch compiler set_stance fail_on_recompile open path rb f compiled_fn = torch compiler load_compiled_function f actual = compiled_fn mod inputs assertEqual expected actual test_decorated_function_aot check_inputs fn _fn args kwargs arg args assert arg shape fn args kwargs _fn check_inputs foo x y = x + x b = y + y c = + b c example_inputs = torch ones torch ones expected = foo example_inputs backend gm example_inputs CustomCompiledFunction gm example_inputs torch compiler set_stance fail_on_recompile compiled_fn = torch compile foo fullgraph=True backend=backend aot_compile example_inputs actual = compiled_fn example_inputs assertEqual expected actual test_decorated_function_with_functools_wrap_aot check_inputs fn functools wraps fn _fn args kwargs arg args assert arg shape fn args kwargs _fn check_inputs foo x y = x + x b = y + y c = + b c example_inputs = torch ones torch ones expected = foo example_inputs backend gm example_inputs CustomCompiledFunction gm example_inputs torch compiler set_stance fail_on_recompile compiled_fn = torch compile foo fullgraph=True backend=backend aot_compile example_inputs actual = compiled_fn example_inputs assertEqual expected actual test_aot_compile_disable_guard_check fn x y x + y torch no_grad compiled_fn = torch compile fn fullgraph=True aot_compile torch randn torch randn inputs = torch randn torch randn expected = fn inputs assertRaisesRegex RuntimeError GuardManager check failed compiled_fn inputs compiled_fn disable_guard_check actual = compiled_fn inputs assertEqual expected actual test_aot_compile_source_info torch _dynamo package SourceInfo fn x y MY_LAMBDA x + y compiled_fn = torch compile fn fullgraph=True aot_compile torch randn torch randn source_info = compiled_fn source_info assertIsInstance source_info SourceInfo assertEqual len source_info inlined_sources assertEqual next iter source_info inlined_sources module __name__ compiled_fn save_compiled_function path open path rb f compiled_fn = torch compiler load_compiled_function f source_info = compiled_fn source_info assertIsInstance source_info SourceInfo assertEqual len source_info inlined_sources assertEqual next iter source_info inlined_sources module __name__ test_aot_compile_graph_break_error_fmt foo x y = x + x torch _dynamo graph_break b = y + y c = + b c assertExpectedInlineMunged Unsupported lambda torch compile foo fullgraph=True aot_compile torch ones torch ones \ Call ` torch _dynamo graph_break ` Explanation User-inserted graph break Message None Hint Remove ` torch _dynamo graph_break ` call Developer debug context Called ` torch _dynamo graph_break ` args ` ` kwargs ` ` For more details about graph break please visit https meta-pytorch github io compile-graph-break-site gb gb html user code File test_aot_compile py line N foo torch _dynamo graph_break test_guard_filter_override_aot check_inputs fn _fn args kwargs arg args assert arg shape fn args kwargs _fn check_inputs foo x y = x + x b = y + y c = + b c example_inputs = torch ones torch ones expected = foo example_inputs noqa F backend gm example_inputs CustomCompiledFunction gm example_inputs torch compiler set_stance fail_on_recompile assertRaisesRegex PackageError CLOSURE_MATCH guard cannot serialized compiled_fn = torch compile noqa F foo fullgraph=True backend=backend options= guard_filter_fn lambda guard_entries True g guard_entries aot_compile example_inputs test_aot_compile_basic_fn_inductor fn x y x + y compiled_fn = torch compile fn fullgraph=True backend= inductor aot_compile torch randn torch randn inputs = torch randn torch randn expected = fn inputs actual = compiled_fn inputs assertEqual expected actual compiled_fn save_compiled_function path torch _dynamo reset torch compiler set_stance fail_on_recompile open path rb f compiled_fn = torch compiler load_compiled_function f actual = compiled_fn inputs assertEqual expected actual test_aot_compile_module mod = SimpleLinearModule model = torch compile mod fullgraph=True backend= inductor options= guard_filter_fn torch compiler skip_guard_on_globals_unsafe contextmanager train_mode model Context manager sets model training mode before entering context model train yield contextmanager eval_mode model Context manager sets model evaluation mode before entering context model eval yield inputs = ModelInput args= torch randn kwargs= contexts= torch no_grad eval_mode model ModelInput args= torch randn kwargs= contexts= train_mode model assert isinstance model torch _dynamo eval_frame OptimizedModule model _aot_compile inputs torch compiler set_stance fail_on_recompile model eval inputs = torch randn expected = mod inputs actual = model inputs assertEqual expected actual Shouldn t recompile model train expected sum backward model _save_aot_compiled_module path torch _dynamo reset model = torch compile mod fullgraph=True backend= inductor options= guard_filter_fn torch compiler skip_guard_on_globals_unsafe assert isinstance model torch _dynamo eval_frame OptimizedModule open path rb f data = f read model _load_aot_compiled_module data torch compiler set_stance fail_on_recompile model eval inputs = torch randn expected = mod inputs actual = model inputs assertEqual expected actual Shouldn t recompile model train expected sum backward test_aot_module_simplified_serializable_autograd mod = SimpleLinearModule compiled_fn SerializableCallable = torch compile mod fullgraph=True backend= inductor forward aot_compile torch randn backend_result = compiled_fn _artifacts compiled_fn assertTrue isinstance backend_result torch _dynamo aot_compile BundledAOTAutogradSerializableCallable assert hasattr backend_result compiled_fn serialize assertIsNotNone backend_result compiled_fn serialize test_aot_module_simplified_serializable_inference fn x x sin compiled_fn SerializableCallable = torch compile fn fullgraph=True backend= inductor aot_compile torch randn backend_result = compiled_fn _artifacts compiled_fn assertTrue isinstance backend_result torch _dynamo aot_compile BundledAOTAutogradSerializableCallable assert hasattr backend_result compiled_fn serialize assertIsNotNone backend_result compiled_fn serialize test_fullgraph_capture_with_pytree_module torch _dynamo functional_export dynamo_graph_capture_for_export Module torch nn Module __init__ super __init__ linear = torch nn Linear linear = torch nn Linear linear = torch nn Linear linear = torch nn Linear forward x y linear x + z linear x - w linear x b + v linear x - mod = Module compiled_mod = dynamo_graph_capture_for_export mod torch randn b torch randn torch randn torch randn inputs = torch randn b torch randn torch randn torch randn assertEqual compiled_mod inputs mod inputs test_fullgraph_capture_with_pytree_func torch _dynamo functional_export dynamo_graph_capture_for_export foo x y x + z x - w x b + v x - compiled_foo = dynamo_graph_capture_for_export foo torch randn b torch randn torch randn torch randn inputs = torch randn b torch randn torch randn torch randn assertEqual compiled_foo inputs foo inputs test_aot_compile_with_closure_save_and_load tmp = fn x y x + y + tmp compiled_fn = torch compile fn fullgraph=True aot_compile torch randn torch randn inputs = torch randn torch randn expected = fn inputs actual = compiled_fn inputs assertEqual expected actual compiled_fn save_compiled_function path open path rb f compiled_fn = torch compiler load_compiled_function f actual = compiled_fn inputs assertEqual expected actual test_aot_compile_with_super_call fn = TestVLLMModel compiled_fn = torch compile fn forward fullgraph=True aot_compile torch randn assertEqual fn forward __code__ co_freevars __class__ inputs = torch randn expected = fn inputs actual = compiled_fn fn inputs assertEqual expected actual compiled_fn save_compiled_function path open path rb f compiled_fn = torch compiler load_compiled_function f actual = compiled_fn fn inputs assertEqual expected actual test_aot_compile_with_default_args fn x y= x + x compiled_fn = torch compile fn fullgraph=True aot_compile torch randn inputs = torch randn expected = fn inputs actual = compiled_fn inputs assertEqual expected actual compiled_fn save_compiled_function path open path rb f compiled_fn = torch compiler load_compiled_function f actual = compiled_fn inputs assertEqual expected actual __name__ == __main__ torch _dynamo test_case run_tests run_tests