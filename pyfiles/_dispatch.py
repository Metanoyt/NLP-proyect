Copyright c Meta Platforms Inc affiliates contextlib logging warnings collections abc Sequence typing cast Optional torch torch distributed dist torch distributed tensor _api dtensor torch distributed tensor _random random torch distributed device_mesh DeviceMesh torch distributed tensor _dtensor_spec DTensorSpec TensorMeta torch distributed tensor _op_schema OpInfo OpSchema OutputSpecType torch distributed tensor _random is_rng_supported_mesh torch distributed tensor _redistribute redistribute_local_tensor torch distributed tensor _sharding_prop ShardingPropagator torch distributed tensor _tp_conv convolution_backward_handler convolution_handler torch distributed tensor _utils try_find_mesh_from_args torch distributed tensor placement_types Partial Placement Replicate torch utils _debug_mode get_active_debug_mode torch utils _python_dispatch return_and_correct_aliasing try torch utils _cxx_pytree pytree except ImportError torch utils _pytree pytree type ignore no-redef aten = torch ops aten logger = logging getLogger __name__ is_same_size_handler op_call torch _ops OpOverload args tuple object kwargs dict str object - bool lhs = cast torch Tensor args rhs = cast torch Tensor args lhs shape == rhs shape found_inf_reduce_handler op_call torch _ops OpOverload args tuple object kwargs dict str object - None op_info = dtensor DTensor _op_dispatcher unwrap_to_op_info op_call args kwargs local_tensor_args = pytree tree_unflatten cast list object op_info local_args op_info args_tree_spec type ignore arg-type local_tensor_args = cast tuple object local_tensor_args op_call local_tensor_args op_info local_kwargs grad_dtensor = cast list dtensor DTensor args grad_placements = grad_dtensor placements mesh = grad_dtensor device_mesh found_inf_placements list Placement = placement grad_placements isinstance placement Replicate found_inf_placements append placement found_inf_placements append Partial max target_tensor = cast torch Tensor args spec = DTensorSpec mesh=mesh placements=tuple found_inf_placements tensor_meta=TensorMeta shape=target_tensor size stride=target_tensor stride dtype=target_tensor dtype pyrefly ignore bad-argument-type found_inf_dtensor = dtensor DTensor local_tensor=target_tensor pyrefly ignore unexpected-keyword spec=spec pyrefly ignore unexpected-keyword requires_grad=False pyrefly ignore unexpected-keyword found_inf = found_inf_dtensor full_tensor target_tensor copy_ found_inf OpDispatcher Op dispatching instance handle args kwargs pre-processing un-wrapping sharding propagation redistribute local args local compute post-processing re-wrapping It also handles any op specific logic necessary NOTE Given runtime overhead Tensor subclass __torch_dispatch__ OpDispatcher designed minimize CPU overhead using tricks proper unflattening faster pytree needed leveraging various caching mechanisms implemented sharding propagation redistribute modules The CPU overhead critical eager mode performance one need carefully measure CPU overhead when making significant changes OpDispatcher ShardingPropagator __init__ - None sharding_propagator = ShardingPropagator _random_ops = aten native_dropout default aten normal_ default aten rand_like default aten randn_like default aten randint_like default aten randint_like low_dtype aten randint_like low_dtype_out aten uniform_ default aten bernoulli default aten bernoulli_ float _custom_op_handlers = aten is_same_size default is_same_size_handler aten convolution default convolution_handler aten convolution_backward default convolution_backward_handler aten _amp_foreach_non_finite_check_and_unscale_ default found_inf_reduce_handler This flag used internally control whether we treat torch Tensor non-DTensor implicitly replicated we throw error user NOTE It EXTREMELY UNSAFE turn flag default so we intentionally leave False default property _allow_implicit_replication - bool torch _C _get_dtensor_allow_implicit_replication _allow_implicit_replication setter _allow_implicit_replication value bool - None torch _C _set_dtensor_allow_implicit_replication value dispatch op_call torch _ops OpOverload args tuple object kwargs dict str object - object Main dispatching logic Follows precedence order custom_op_handler registered sharding strategy then rule composite implicit autograd decomposition op_call _custom_op_handlers _custom_op_handlers op_call op_call args kwargs type ignore operator extract local tensor sharding infos OpInfo op_info = unwrap_to_op_info op_call args kwargs try sharding_propagator propagate op_info except NotImplementedError torch _C _dispatch_has_kernel_for_dispatch_key op_call name torch _C DispatchKey CompositeImplicitAutograd When running under inference mode CompositeImplicitAutograd ops show up __torch_dispatch__ so we manually decompose them here out = op_call decompose args kwargs assert out NotImplemented out raise except Exception e raise RuntimeError f e \n\nSharding propagation failed op_info schema e output_sharding = op_info output_sharding assert output_sharding None output sharding should None mesh = op_info compute_mesh participating = mesh get_coordinate None local_results = None participating computation happens current rank mesh normal case output_sharding needs_redistribute If sharding propagation decision needs redistribute perform redistribute args first which could potentially modify args i e allgather certain arg assert output_sharding redistribute_schema None redistribute_local_args op_info output_sharding redistribute_schema output_sharding use_val_from_redistribute_schema local_tensor_args = pytree tree_unflatten cast list object op_info local_args pyrefly ignore bad-argument-type op_info args_tree_spec op_info args_tree_spec op_info local_args run local op computation potentially modified args kwargs local_tensor_args = cast tuple object local_tensor_args op_call _random_ops random _rng_tracker is_rng_supported_mesh mesh Default ` OffsetBasedRNGTracker ` parallelism API did already construct one random _rng_tracker = random OffsetBasedRNGTracker mesh first_arg first_local_arg = cast dtensor DTensor args cast torch Tensor local_tensor_args If user provided generator we hook up our RNG manager we also pop kwargs so op_call does directly use we want op_call fall back default which our RNG manager maybe_user_generator = op_info local_kwargs pop generator None assert maybe_user_generator None isinstance maybe_user_generator torch Generator maybe_user_generator = None rng_context = random _rng_tracker _distribute_region first_arg _spec generator=maybe_user_generator random _rng_tracker first_local_arg is_meta contextlib nullcontext For DTensor random operator run within RNGTracker context ensure random number generator properly distributed rng_context local_results = op_call local_tensor_args op_info local_kwargs normal case run local sharded op computation local_results = op_call local_tensor_args op_info local_kwargs For non-participating device happens rank does belong device mesh we do type scalar set local result None type Tensor List Tensor empty tensor s correct dtype spec = output_sharding output_spec ret_list = op_info schema op _schema returns spec None For scalar type non-participating device has None its local result local_results = None default_tensor spec DTensorSpec - torch Tensor spec tensor_meta None shape = spec tensor_meta shape dtype = spec tensor_meta dtype len shape == scalar tensor torch zeros dtype=dtype non-scalar tensor torch tensor dtype=dtype raise RuntimeError f spec has no tensor metadata isinstance spec DTensorSpec Tensor value local_results = default_tensor spec isinstance spec Sequence List Tensor value local_results = default_tensor s s None None s spec assert isinstance local_results list None local_results ret_type = str ret_list type raise NotImplementedError f type ret_type DTensor op supported output_sharding output_spec None op_call == aten equal default The output equal op bool converting into single value tensor we can use all-reduce min reduce op simulate logical assert local_results None isinstance local_results bool r = torch tensor int local_results local_results None device=mesh device_type dist all_reduce r op=dist ReduceOp MIN local_results = bool r item op_info schema is_inplace_op inplace op should instead re-wrapping output_sharding output_spec None NOTE aten squeeze_ dim inplace op also may change inplace argument s tensor meta Here we choose special case op because far I know only inplace op has such behavior We can extend special case necessary op_call == aten squeeze_ dim output_spec = output_sharding output_spec assert isinstance output_spec DTensorSpec assert isinstance args dtensor DTensor args _spec = output_spec use return_and_correct_aliasing match outer inner aliasing See https github com pytorch pytorch pull return_and_correct_aliasing op_call args kwargs args args None op_info schema is_out_variant_op out variant could possibly have multiple out args i e lu_unpack out output_specs = output_sharding output_spec isinstance output_sharding output_spec tuple output_sharding output_spec out_dts = spec_idx = argument op_call _schema arguments argument is_out out_dt = cast dtensor DTensor kwargs argument name out_dt _spec = cast DTensorSpec output_specs spec_idx out_dts append out_dt spec_idx += assert len out_dts = out variant should have least one out arg tuple out_dts len out_dts out_dts ret = wrap local_results output_sharding output_spec type ignore possibly-undefined participating op_info schema is_view_op return_and_correct_aliasing op_call args kwargs ret ret staticmethod redistribute_local_args op_info OpInfo suggested_input_schema OpSchema use_val_from_redistribute_schema bool - None debug_mode = get_active_debug_mode NOTE s very rare we need reshard kwargs so we intentionally skip op_info args_tree_spec None flatten_args_schema_to_reshard = tuple pytree tree_leaves suggested_input_schema args_schema flatten_args_schema_to_reshard = suggested_input_schema args_schema new_local_args list object = i arg_spec enumerate op_info flat_args_schema reshard_arg_spec = flatten_args_schema_to_reshard i isinstance arg_spec DTensorSpec local_tensor = cast torch Tensor op_info local_args i arg_spec = reshard_arg_spec redistribute_context = debug_mode record_redistribute_calls type ignore union-attr i arg_spec reshard_arg_spec debug_mode None contextlib nullcontext redistribute_context resharded_local_tensor = redistribute_local_tensor local_tensor arg_spec pyrefly ignore bad-argument-type reshard_arg_spec new_local_args append resharded_local_tensor new_local_args append local_tensor use_val_from_redistribute_schema args can updated view related ops we refer update redistribute_schema new_local_args append reshard_arg_spec new_local_args append arg_spec op_info local_args = tuple new_local_args unwrap_to_op_info op_call torch _ops OpOverload args tuple object kwargs dict str object - OpInfo get runtime schema info determine whether use pytree flatten inputs runtime_schema_info = sharding_propagator op_to_schema_info get op_call None runtime_schema_info None runtime_schema_info needs_pytree flatten args kwargs when op says necessary tree_args args_spec = pytree tree_flatten args args_list Sequence object = tree_args args_list args_spec = args None args_schema list object = kwargs_schema dict str object = local_args list object = local_kwargs dict str object = compute_mesh Optional DeviceMesh = None arg args_list isinstance arg dtensor DTensor local_args append arg _local_tensor args_schema append arg _spec compute_mesh None record first compute device mesh args compute_mesh = arg device_mesh isinstance arg torch Tensor compute_mesh = compute_mesh try_find_mesh_from_args op_call args_list args_schema append _try_replicate_spec_for_scalar_tensor op_call arg compute_mesh local_args append arg non DTensor Tensor args i e int float bool just add args_schema local_args args_schema append arg local_args append arg k v kwargs items isinstance v dtensor DTensor local_kwargs k = v _local_tensor kwargs_schema k = v _spec isinstance v torch Tensor compute_mesh = compute_mesh try_find_mesh_from_args op_call args_list kwargs_schema k = _try_replicate_spec_for_scalar_tensor op_call v pyrefly ignore bad-argument-type compute_mesh local_kwargs k = v non DTensor Tensor args i e int float bool just add args_schema local_args kwargs_schema k = v local_kwargs k = v assert compute_mesh None f found no DeviceMesh dtensor args op_call op_info = OpInfo compute_mesh OpSchema op_call pyrefly ignore bad-argument-type pytree tree_unflatten args_schema args_spec args_spec tuple args_schema kwargs_schema schema_info=runtime_schema_info args_schema tuple local_args local_kwargs args_spec op_info staticmethod wrap res object spec OutputSpecType - object isinstance res torch Tensor spec None assert isinstance spec DTensorSpec f output spec does match output Expected DTensorSpec got spec pyrefly ignore bad-argument-type bad-argument-count unexpected-keyword dtensor DTensor res spec requires_grad=res requires_grad output does have DTensorSpec due specific ops must scalar tensor assert res ndim == output tensor should scalar res isinstance res list tuple assert spec None isinstance spec list tuple f output spec does match output Expected list tuple got spec res_list = e s zip res spec res_list append OpDispatcher wrap e s tuple res_list isinstance res tuple res_list res contains only non tensor values i e int float none we simply without rewrapping DTensor res _try_replicate_spec_for_scalar_tensor op_call torch _ops OpOverload tensor_arg torch Tensor compute_mesh DeviceMesh - DTensorSpec util function produce replicate spec scalar tensor arg kwarg tensor_arg numel == tensor_arg ndim == warnings warn Found non-scalar tensor numel= ndim = we implicitly creating replicated DTensor However please consider changing scalar tensor explicitly create DTensor under distributed environment stacklevel= tensor_arg numel == _allow_implicit_replication scalar tensor can safely treated replicated replication_spec = DTensorSpec compute_mesh Replicate compute_mesh ndim tensor_meta=TensorMeta shape=tensor_arg shape stride=tensor_arg stride dtype=tensor_arg dtype raise RuntimeError f op_call got mixed torch Tensor DTensor need convert all torch Tensor DTensor before calling distributed operators Please see https docs pytorch org docs main distributed tensor html#mixed-tensor-and-dtensor-operations more details replication_spec