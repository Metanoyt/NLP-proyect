Owner s oncall quantization os sys unittest torch torch torch ao nn intrinsic quantized nniq torch ao nn quantized nnq torch ao nn quantized dynamic nnqd torch ao quantization quantize_fx quantize_fx torch nn nn torch ao quantization MinMaxObserver PerChannelMinMaxObserver torch fx GraphModule torch testing _internal common_quantization skipIfNoFBGEMM torch testing _internal common_quantized override_qengines qengine_is_fbgemm Testing utils torch testing _internal common_utils IS_AVX _VNNI_SUPPORTED raise_on_run_directly TestCase torch testing _internal quantization_torch_package_models LinearReluFunctional remove_prefix text prefix text startswith prefix text len prefix text get_filenames subname NB we take __file__ module defined test so we place expect directory where test script lives NOT where test common_utils py lives module_id = __class__ __module__ munged_id = remove_prefix id module_id + test_file = os path realpath sys modules module_id __file__ base_name = os path join os path dirname test_file serialized munged_id subname base_name += _ + subname input_file = base_name + input pt state_dict_file = base_name + state_dict pt scripted_module_file = base_name + scripted pt traced_module_file = base_name + traced pt expected_file = base_name + expected pt package_file = base_name + package pt get_attr_targets_file = base_name + get_attr_targets pt input_file state_dict_file scripted_module_file traced_module_file expected_file package_file get_attr_targets_file TestSerialization TestCase Test backward compatibility serialization numerics Copy modified TestCase assertExpected _test_op qmodule subname=None input_size=None input_quantized=True generate=False prec=None new_zipfile_serialization=False r Test quantized modules serialized previously can loaded current code make sure we don t break backward compatibility serialization quantized modules input_file state_dict_file scripted_module_file traced_module_file expected_file _package_file _get_attr_targets_file = get_filenames subname only generate once generate qengine_is_fbgemm input_tensor = torch rand input_size float input_quantized input_tensor = torch quantize_per_tensor input_tensor torch quint torch save input_tensor input_file Temporary fix use _use_new_zipfile_serialization until lands torch save qmodule state_dict state_dict_file _use_new_zipfile_serialization=new_zipfile_serialization torch jit save torch jit script qmodule scripted_module_file torch jit save torch jit trace qmodule input_tensor traced_module_file torch save qmodule input_tensor expected_file input_tensor = torch load input_file weights_only = False sometimes get ScriptObject here qmodule load_state_dict torch load state_dict_file weights_only=False qmodule_scripted = torch jit load scripted_module_file qmodule_traced = torch jit load traced_module_file expected = torch load expected_file assertEqual qmodule input_tensor expected atol=prec assertEqual qmodule_scripted input_tensor expected atol=prec assertEqual qmodule_traced input_tensor expected atol=prec _test_op_graph qmodule subname=None input_size=None input_quantized=True generate=False prec=None new_zipfile_serialization=False r Input floating point module If generate == True traces scripts module quantizes results PTQ saves results If generate == False traces scripts module quantizes results PTQ compares saved results input_file _ scripted_module_file traced_module_file expected_file _package_file _get_attr_targets_file = get_filenames subname only generate once generate qengine_is_fbgemm input_tensor = torch rand input_size float torch save input_tensor input_file convert TorchScript scripted = torch jit script qmodule traced = torch jit trace qmodule input_tensor quantize _eval_fn model data model data qconfig_dict = torch ao quantization default_qconfig scripted_q = torch ao quantization quantize_jit scripted qconfig_dict _eval_fn input_tensor traced_q = torch ao quantization quantize_jit traced qconfig_dict _eval_fn input_tensor torch jit save scripted_q scripted_module_file torch jit save traced_q traced_module_file torch save scripted_q input_tensor expected_file input_tensor = torch load input_file qmodule_scripted = torch jit load scripted_module_file qmodule_traced = torch jit load traced_module_file expected = torch load expected_file assertEqual qmodule_scripted input_tensor expected atol=prec assertEqual qmodule_traced input_tensor expected atol=prec _test_obs obs input_size subname=None generate=False check_numerics=True Test observer code can loaded state_dict input_file state_dict_file _ _ expected_file _package_file _get_attr_targets_file = get_filenames None generate input_tensor = torch rand input_size float torch save input_tensor input_file torch save obs input_tensor expected_file torch save obs state_dict state_dict_file input_tensor = torch load input_file obs load_state_dict torch load state_dict_file expected = torch load expected_file check_numerics assertEqual obs input_tensor expected _test_package fp _module input_size generate=False Verifies files created past torch package work today s FX graph mode quantization transforms input_file _ _scripted_module_file _traced_module_file expected_file package_file get_attr_targets_file = get_filenames None package_name = test resource_name_model = test pkl _do_quant_transforms m torch nn Module input_tensor torch Tensor - torch nn Module example_inputs = input_tensor do quantizaton transforms save result qconfig = torch ao quantization get_default_qconfig fbgemm mp = quantize_fx prepare_fx m qconfig example_inputs=example_inputs mp input_tensor mq = quantize_fx convert_fx mp mq _get_get_attr_target_strings m GraphModule - set str results = set node m graph nodes node op == get_attr results add node target results generate qengine_is_fbgemm input_tensor = torch randn input_size torch save input_tensor input_file save model torch package torch package PackageExporter package_file exp exp intern torch testing _internal quantization_torch_package_models exp save_pickle package_name resource_name_model fp _module do quantization transforms save result mq = _do_quant_transforms fp _module input_tensor get_attrs = _get_get_attr_target_strings mq torch save get_attrs get_attr_targets_file q_result = mq input_tensor torch save q_result expected_file load input tensor input_tensor = torch load input_file expected_output_tensor = torch load expected_file expected_get_attrs = torch load get_attr_targets_file weights_only=False load model package verify output get_attr targets match imp = torch package PackageImporter package_file m = imp load_pickle package_name resource_name_model mq = _do_quant_transforms m input_tensor get_attrs = _get_get_attr_target_strings mq assertTrue get_attrs == expected_get_attrs f get_attrs expected expected_get_attrs got get_attrs output_tensor = mq input_tensor assertTrue torch allclose output_tensor expected_output_tensor override_qengines test_linear module = nnq Linear bias_=True dtype=torch qint _test_op module input_size= generate=False override_qengines test_linear_relu module = nniq LinearReLU bias=True dtype=torch qint _test_op module input_size= generate=False override_qengines test_linear_dynamic module_qint = nnqd Linear bias_=True dtype=torch qint _test_op module_qint qint input_size= input_quantized=False generate=False qengine_is_fbgemm module_float = nnqd Linear bias_=True dtype=torch float _test_op module_float float input_size= input_quantized=False generate=False override_qengines test_conv d module = nnq Conv d kernel_size= stride= padding= dilation= groups= bias=True padding_mode= zeros _test_op module input_size= generate=False override_qengines test_conv d_nobias module = nnq Conv d kernel_size= stride= padding= dilation= groups= bias=False padding_mode= zeros _test_op module input_size= generate=False override_qengines test_conv d_graph module = nn Sequential torch ao quantization QuantStub nn Conv d kernel_size= stride= padding= dilation= groups= bias=True padding_mode= zeros _test_op_graph module input_size= generate=False override_qengines test_conv d_nobias_graph module = nn Sequential torch ao quantization QuantStub nn Conv d kernel_size= stride= padding= dilation= groups= bias=False padding_mode= zeros _test_op_graph module input_size= generate=False override_qengines test_conv d_graph_v tests same thing test_conv d_graph version ConvPackedParams n d module = nn Sequential torch ao quantization QuantStub nn Conv d kernel_size= stride= padding= dilation= groups= bias=True padding_mode= zeros _test_op_graph module input_size= generate=False override_qengines test_conv d_nobias_graph_v tests same thing test_conv d_nobias_graph version ConvPackedParams n d module = nn Sequential torch ao quantization QuantStub nn Conv d kernel_size= stride= padding= dilation= groups= bias=False padding_mode= zeros _test_op_graph module input_size= generate=False override_qengines test_conv d_graph_v tests same thing test_conv d_graph version ConvPackedParams n d module = nn Sequential torch ao quantization QuantStub nn Conv d kernel_size= stride= padding= dilation= groups= bias=True padding_mode= zeros _test_op_graph module input_size= generate=False override_qengines test_conv d_nobias_graph_v tests same thing test_conv d_nobias_graph version ConvPackedParams n d module = nn Sequential torch ao quantization QuantStub nn Conv d kernel_size= stride= padding= dilation= groups= bias=False padding_mode= zeros _test_op_graph module input_size= generate=False override_qengines test_conv d_relu module = nniq ConvReLU d kernel_size= stride= padding= dilation= groups= bias=True padding_mode= zeros _test_op module input_size= generate=False TODO graph mode quantized conv d module override_qengines test_conv d qengine_is_fbgemm module = nnq Conv d kernel_size= stride= padding= dilation= groups= bias=True padding_mode= zeros _test_op module input_size= generate=False TODO graph mode quantized conv d module override_qengines test_conv d_relu qengine_is_fbgemm module = nniq ConvReLU d kernel_size= stride= padding= dilation= groups= bias=True padding_mode= zeros _test_op module input_size= generate=False TODO graph mode quantized conv d module override_qengines unittest skipIf IS_AVX _VNNI_SUPPORTED This test fails machines AVX _VNNI support Ref GH Issue test_lstm LSTMModule torch nn Module __init__ - None super __init__ lstm = nnqd LSTM input_size= hidden_size= num_layers= dtype=torch float forward x x = lstm x x qengine_is_fbgemm mod = LSTMModule _test_op mod input_size= input_quantized=False generate=False new_zipfile_serialization=True test_per_channel_observer obs = PerChannelMinMaxObserver _test_obs obs input_size= generate=False test_per_tensor_observer obs = MinMaxObserver _test_obs obs input_size= generate=False test_default_qat_qconfig Model nn Module __init__ - None super __init__ linear = nn Linear relu = nn ReLU forward x x = linear x x = relu x x model = Model model linear weight = torch nn Parameter torch randn model qconfig = torch ao quantization get_default_qat_qconfig fbgemm ref_model = torch ao quantization QuantWrapper model ref_model = torch ao quantization prepare_qat ref_model _test_obs ref_model input_size= generate=False check_numerics=False skipIfNoFBGEMM test_linear_relu_package_quantization_transforms m = LinearReluFunctional eval _test_package m input_size= generate=False __name__ == __main__ raise_on_run_directly test test_quantization py