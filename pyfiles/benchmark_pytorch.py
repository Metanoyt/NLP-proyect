json time torch Import C++ extension register _consume operator try benchmark_cpp_extension noqa F except ImportError err If extension isn t built script must raise error raise ImportError Failed C++ extension please build using \ncd pt_extension \npython -m pip install err PyTorch performance microbenchmarks This module contains PyTorch-specific functionalities performance microbenchmarks TorchBenchmarkBase torch nn Module This base used create Pytorch operator benchmark module_name name operator being benchmarked test_name name s created concatenating all inputs specific test __init__ super __init__ user_given_name = None _pass_count = _num_inputs_require_grads = _set_backward_test is_backward _is_backward = is_backward auto_set This used automatically set require_grad backward patch It implemented based two counters One counter save number times init has been called The other counter save number times function itself has been called In very first time init called function counts how many inputs require gradient In each following init calls function will only one true value Here example v = torch rand M N K requires_grad=self auto_set v = torch rand M N K requires_grad=self auto_set _is_backward False _pass_count == _num_inputs_require_grads += True _auto_set_counter += _pass_count == _auto_set_counter extract_inputs_tuple inputs_tuple = tuple inputs values torch jit export get_inputs Need convert inputs tuple outside JIT so JIT can infer size inputs inputs_tuple torch jit export forward_impl This supply inputs forward function which will called both eager JIT mode local runs forward get_inputs torch jit export forward_consume iters int _consume used avoid dead-code-elimination optimization _ range iters torch ops operator_benchmark _consume forward_impl forward_impl_eager This supply inputs forward function which will called both eager compile mode local runs forward get_inputs forward_consume_eager iters int Eager version forward_consume without decorators compilation handled torch compile _ range iters torch ops operator_benchmark _consume forward_impl_eager module_name used label operator being benchmarked user_given_name user_given_name __class__ __name__ set_module_name name user_given_name = name test_name kargs globally unique name which can used label specific test This list attributes which will included test name skip_key_list = device test_name_str = key kargs value = kargs key test_name_str append key skip_key_list key + str value type value bool int value name = module_name + _ + _ join test_name_str replace name PyTorchOperatorTestCase This includes all information needed benchmark operator op_bench s user-defined child TorchBenchmarkBase which includes input operator etc test_config namedtuple includes test_name input_shape tag run_backward When run_backward false run_forward method will executed When run_backward true run_forward_eager _output_mean will executed generate output Then run_backward will executed __init__ op_bench test_config test_config = test_config op_bench = op_bench place_holder_tensor = torch ones framework = PyTorch time_series = _jit_forward_graph = None _compile_forward_graph = None _generate_jit_forward_graph generate graph forward function via scripting scripted_op_bench = torch jit script op_bench scripted_op_bench forward_consume _generate_compile_forward_graph generate compiled graph forward function via torch compile compiled_forward_consume = torch compile op_bench forward_consume_eager backend= inductor compiled_forward_consume run_jit_forward num_runs print_per_iter=False cuda_sync=False Run forward path op JIT mode _jit_forward_graph None _jit_forward_graph = _generate_jit_forward_graph _jit_forward_graph num_runs run_compile_forward num_runs print_per_iter=False cuda_sync=False Run forward path op compile mode _compile_forward_graph None _compile_forward_graph = _generate_compile_forward_graph _compile_forward_graph num_runs cuda_sync torch cuda synchronize torch cuda current_device _print_per_iter print last values length = min len time_series i range length print PyTorchObserver + json dumps type test_config test_name metric latency unit ms value str time_series length - i - run_forward num_runs print_per_iter cuda_sync Run forward path op eager mode print_per_iter _ range num_runs start_time = time time output = op_bench forward_impl_eager cuda_sync torch cuda synchronize torch cuda current_device end_time = time time time_series append end_time - start_time e _ range num_runs output = op_bench forward_impl_eager cuda_sync torch cuda synchronize torch cuda current_device _output_mean TODO mingzhe necessary sum up everything myself torch autograd backward do take gradient tensor By default same shape your output tensor all s Mathematically same output summed together So we should able get ride method dummy function gradient calculation mean = output mean run_backward num_runs print_per_iter=False Run backward path op many iterations TODO can we use JIT here reduce python overhead _ range num_runs mean backward retain_graph=True create_pytorch_op_test_case op_bench test_config This method used generate est func_name global unique string For PyTorch add operator M= N= K= tag = long here values members test_case op module_name add framework PyTorch test_config TestConfig test_name= add_M _N _K input_config= M N K tag= long run_backward=False func_name addPyTorchTestConfig test_name= add_M _N _K input_config= M N K tag= long run_backward=False test_case = PyTorchOperatorTestCase op_bench test_config test_config = test_case test_config op = test_case op_bench func_name = f op module_name test_case framework str test_config func_name test_case