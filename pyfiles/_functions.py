warnings itertools chain typing Optional torch torch _utils _get_device_index torch autograd Function torch nn parallel comm Broadcast Function staticmethod forward ctx target_gpus inputs assert all i device type = cpu i inputs Broadcast function implemented CPU tensors target_gpus = _get_device_index x True x target_gpus ctx target_gpus = target_gpus len inputs == ctx num_inputs = len inputs ctx input_device = inputs get_device outputs = comm broadcast_coalesced inputs ctx target_gpus non_differentiables = idx input_requires_grad enumerate ctx needs_input_grad input_requires_grad non_differentiables extend output idx output outputs ctx mark_non_differentiable non_differentiables tuple chain from_iterable outputs staticmethod backward ctx grad_outputs None + ReduceAddCoalesced apply ctx input_device ctx num_inputs grad_outputs ReduceAddCoalesced Function staticmethod forward ctx destination num_inputs grads ctx target_gpus = grads i get_device i range len grads num_inputs grads_ = grads i i + num_inputs i range len grads num_inputs comm reduce_add_coalesced grads_ destination staticmethod backward ctx grad_outputs None None + Broadcast apply ctx target_gpus grad_outputs Gather Function staticmethod forward ctx target_device dim inputs assert all i device type = cpu i inputs Gather function implemented CPU tensors target_device == cpu ctx target_device = cpu target_device = _get_device_index target_device True ctx target_device = target_device ctx dim = dim ctx input_gpus = tuple i get_device i inputs all t dim == t inputs dim == inputs = tuple t view t inputs warnings warn Was asked gather along dimension all input tensors scalars will instead unsqueeze vector stacklevel= ctx unsqueezed_scalar = True ctx unsqueezed_scalar = False ctx input_sizes = tuple i size ctx dim i inputs comm gather inputs ctx dim ctx target_device staticmethod backward ctx grad_output scattered_grads = Scatter apply ctx input_gpus ctx input_sizes ctx dim grad_output ctx unsqueezed_scalar scattered_grads = tuple g g scattered_grads None None + scattered_grads Scatter Function staticmethod forward ctx target_gpus chunk_sizes dim input target_gpus = _get_device_index x True x target_gpus ctx dim = dim ctx input_device = input get_device input device type = cpu - streams = None torch accelerator is_available ctx input_device == - Perform CPU GPU copies background stream streams = _get_stream torch device device device target_gpus outputs = comm scatter input target_gpus chunk_sizes ctx dim streams Synchronize copy stream streams None i output enumerate outputs torch accelerator device_index target_gpus i main_stream = torch accelerator current_stream main_stream wait_stream streams i output record_stream main_stream outputs staticmethod backward ctx grad_output None None None Gather apply ctx input_device ctx dim grad_output background streams used copying _streams Optional list Optional torch Stream = None _get_stream device torch device Get background stream copying between CPU target device global _streams device type == cpu torch accelerator is_available None assert torch accelerator current_accelerator type == device type _streams None _streams = None torch accelerator device_count _streams device index None _streams device index = torch Stream device index _streams device index