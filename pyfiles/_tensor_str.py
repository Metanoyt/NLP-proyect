mypy allow-untyped-defs contextlib dataclasses math textwrap typing Any Optional torch torch inf dataclasses dataclass __PrinterOptions precision int = threshold float = edgeitems int = linewidth int = sci_mode Optional bool = None PRINT_OPTS = __PrinterOptions We could use kwargs will give better docs set_printoptions precision=None threshold=None edgeitems=None linewidth=None profile=None sci_mode=None r Set options printing Items shamelessly taken NumPy Args precision Number digits precision floating point output default = threshold Total number array elements which trigger summarization rather than full ` repr ` default = edgeitems Number array items summary beginning end each dimension default = linewidth The number characters per line purpose inserting line breaks default = Thresholded matrices will ignore parameter profile Sane defaults pretty printing Can override any above options any one ` default ` ` short ` ` full ` sci_mode Enable True disable False scientific notation If None default specified value defined ` torch _tensor_str _Formatter ` This value automatically chosen framework Example Limit precision elements torch set_printoptions precision= torch tensor tensor Limit number elements shown torch set_printoptions threshold= torch arange tensor Restore defaults torch set_printoptions profile= default torch tensor tensor torch arange tensor profile None profile == default PRINT_OPTS precision = PRINT_OPTS threshold = PRINT_OPTS edgeitems = PRINT_OPTS linewidth = profile == short PRINT_OPTS precision = PRINT_OPTS threshold = PRINT_OPTS edgeitems = PRINT_OPTS linewidth = profile == full PRINT_OPTS precision = PRINT_OPTS threshold = inf PRINT_OPTS edgeitems = PRINT_OPTS linewidth = precision None PRINT_OPTS precision = precision threshold None PRINT_OPTS threshold = threshold edgeitems None PRINT_OPTS edgeitems = edgeitems linewidth None PRINT_OPTS linewidth = linewidth PRINT_OPTS sci_mode = sci_mode get_printoptions - dict str Any r Gets current options printing dictionary can passed ` ` kwargs ` ` set_printoptions dataclasses asdict PRINT_OPTS contextlib contextmanager printoptions kwargs r Context manager temporarily changes print options Accepted arguments same func ` set_printoptions ` old_kwargs = get_printoptions set_printoptions kwargs try yield finally set_printoptions old_kwargs tensor_totype t dtype = torch float t is_mps t is_xpu torch xpu get_device_properties t device has_fp t is_maia torch double t dtype=dtype _Formatter __init__ tensor floating_dtype = tensor dtype is_floating_point int_mode = True sci_mode = False max_width = torch no_grad tensor_view = tensor reshape - floating_dtype value tensor_view value_str = f value max_width = max max_width len value_str tensor dtype == torch float _e m fn_x type ignore attr-defined torch float _e m fn_x special does support casts necessary print we choose display uint representation here convenience being able print tensor TODO extend other dtypes without casts defined such bits uint int dtypes tensor_view = tensor_view view torch uint nonzero_finite_vals = torch masked_select tensor_view torch isfinite tensor_view tensor_view ne nonzero_finite_vals numel == no valid number do nothing tensor dtype == torch float _e m fnu type ignore attr-defined float _e m fnu special does define arithmetic ops printing code further file assumes existence various arithmetic ops figure out what print We hack convert float here make printing work correctly TODO also add other float dtypes here after arithmetic support them removed nonzero_finite_vals = nonzero_finite_vals float Convert double float easy calculation HalfTensor overflows e there s no div CPU nonzero_finite_abs = tensor_totype nonzero_finite_vals abs nonzero_finite_min = tensor_totype nonzero_finite_abs min nonzero_finite_max = tensor_totype nonzero_finite_abs max value nonzero_finite_vals value = torch ceil value int_mode = False break sci_mode = nonzero_finite_max nonzero_finite_min nonzero_finite_max e nonzero_finite_min e- PRINT_OPTS sci_mode None PRINT_OPTS sci_mode int_mode int_mode floats all numbers integers we append decimal nonfinites indicate tensor floating type add len account sci_mode value nonzero_finite_vals value_str = f PRINT_OPTS precision e format value max_width = max max_width len value_str value nonzero_finite_vals value_str = f value f max_width = max max_width len value_str + Check scientific representation should used sci_mode value nonzero_finite_vals value_str = f PRINT_OPTS precision e format value max_width = max max_width len value_str value nonzero_finite_vals value_str = f PRINT_OPTS precision f format value max_width = max max_width len value_str width max_width format value floating_dtype sci_mode ret = f max_width PRINT_OPTS precision e format value int_mode ret = f value f math isinf value math isnan value ret += ret = f PRINT_OPTS precision f format value ret = f value max_width - len ret + ret _scalar_str formatter formatter =None formatter None real_str = _scalar_str real formatter imag_str = _scalar_str imag formatter + j lstrip handles negative numbers + - imag_str == + imag_str == - real_str + imag_str real_str + + + imag_str formatter format item _vector_str indent summarize formatter formatter =None length includes spaces comma between elements element_length = formatter width + formatter None width imag_formatter + extra j complex element_length += formatter width + elements_per_line = max math floor PRINT_OPTS linewidth - indent element_length _val_formatter val formatter =formatter formatter =formatter formatter None real_str = formatter format val real imag_str = formatter format val imag + j lstrip handles negative numbers + - imag_str == + imag_str == - real_str + imag_str real_str + + + imag_str formatter format val dtype == torch float _e m fn_x type ignore attr-defined torch float _e m fn_x special does support casts necessary print we choose display uint representation here convenience being able print tensor TODO extend other dtypes without casts defined such bits uint int dtypes = view torch uint summarize PRINT_OPTS edgeitems Deal edge case negative zero zero data = summarize size PRINT_OPTS edgeitems data = _val_formatter val val PRINT_OPTS edgeitems tolist + + _val_formatter val val -PRINT_OPTS edgeitems tolist data = _val_formatter val val tolist data_lines = data i i + elements_per_line i range len data elements_per_line lines = join line line data_lines + + \n + indent + join lines + formatter only used printing complex tensors For complex tensors formatter formatter formatters tensor real tensor imag respesectively _tensor_str_with_formatter indent summarize formatter formatter =None dim = dim dim == _scalar_str formatter formatter dim == _vector_str indent summarize formatter formatter summarize size PRINT_OPTS edgeitems slices = _tensor_str_with_formatter i indent + summarize formatter formatter i range PRINT_OPTS edgeitems + + _tensor_str_with_formatter i indent + summarize formatter formatter i range len - PRINT_OPTS edgeitems len slices = _tensor_str_with_formatter i indent + summarize formatter formatter i range size tensor_str = + \n dim - + indent + join slices + tensor_str + _tensor_str indent numel == has_names There two main codepaths possibly more tensor printing goes through - tensor data can fit comfortably screen - tensor data needs summarized Some codepaths don t fully support named tensors so we send unnamed tensor formatting code workaround = rename None summarize = numel PRINT_OPTS threshold _is_zerotensor = clone handle negative bit is_neg = resolve_neg TODO Remove me when ` masked_select ` implemented FP dtype torch float _e m torch float _e m fnuz torch float _e m fn torch float _e m fnuz = half dtype is_complex handle conjugate bit = resolve_conj real_formatter = _Formatter get_summarized_data real summarize real imag_formatter = _Formatter get_summarized_data imag summarize imag _tensor_str_with_formatter indent summarize real_formatter imag_formatter formatter = _Formatter get_summarized_data summarize _tensor_str_with_formatter indent summarize formatter _add_suffixes tensor_str suffixes indent force_newline tensor_strs = tensor_str last_line_len = len tensor_str - tensor_str rfind \n + suffix suffixes suffix_len = len suffix force_newline last_line_len + suffix_len + PRINT_OPTS linewidth tensor_strs append \n + indent + suffix last_line_len = indent + suffix_len force_newline = False tensor_strs append + suffix last_line_len += suffix_len + tensor_strs append join tensor_strs get_summarized_data dim = dim dim == dim == size PRINT_OPTS edgeitems torch cat PRINT_OPTS edgeitems -PRINT_OPTS edgeitems PRINT_OPTS edgeitems new_empty dim size PRINT_OPTS edgeitems start = i i range PRINT_OPTS edgeitems end = i i range len - PRINT_OPTS edgeitems len torch stack get_summarized_data x x start + end torch stack get_summarized_data x x _str_intern inp tensor_contents=None torch _C _functorch is_functorch_wrapped_tensor inp _functorch_wrapper_str_intern inp tensor_contents=tensor_contents is_plain_tensor = type inp torch Tensor type inp torch nn Parameter inp is_nested prefix = nested_tensor is_plain_tensor prefix = tensor prefix = f type inp __name__ indent = len prefix suffixes = custom_contents_provided = tensor_contents None custom_contents_provided tensor_str = tensor_contents This used extract primal value thus disable forward AD within function TODO albanD This needs updated when more than one level supported tangent = torch autograd forward_ad unpack_dual inp Note Print tensor device A general logic here we only print device when doesn t match device specified default tensor type Currently torch set_default_tensor_type only supports CPU CUDA thus torch _C _get_default_device only returns either cpu cuda In other cases we don t have way set them default yet we should always print out device them device type = torch _C _get_default_device device type == cuda torch cuda current_device = device index device type == mps suffixes append device= + str device + Tensor printing performs tensor operations like slice indexing etc make representable format These operations ipu xla lazy mtia tensor results compilations Hence avoid compilations copying tensor cpu before printing device type xla lazy ipu mtia = cpu TODO add API map real - complex dtypes _default_complex_dtype = torch cdouble torch get_default_dtype == torch double torch cfloat has_default_dtype = dtype torch get_default_dtype _default_complex_dtype torch int torch bool is_sparse suffixes append size= + str tuple shape torch _subclasses fake_tensor FakeTensor is_meta = is_meta isinstance FakeTensor is_meta suffixes append nnz= + str _nnz has_default_dtype suffixes append dtype= + str dtype custom_contents_provided indices_prefix = indices=tensor indices = _indices detach is_meta indices_str = indices_str = _tensor_str indices indent + len indices_prefix is_meta indices numel == indices_str += size= + str tuple indices shape values_prefix = values=tensor values = _values detach is_meta values_str = values_str = _tensor_str values indent + len values_prefix is_meta values numel == values_str += size= + str tuple values shape tensor_str = indices_prefix + indices_str + \n + indent + values_prefix + values_str + layout torch sparse_csr torch sparse_csc torch sparse_bsr torch sparse_bsc torch _subclasses fake_tensor FakeTensor suffixes append size= + str tuple shape is_meta = is_meta isinstance FakeTensor is_meta suffixes append nnz= + str _nnz has_default_dtype suffixes append dtype= + str dtype custom_contents_provided compressed_indices_method plain_indices_method = torch sparse_csr torch Tensor crow_indices torch Tensor col_indices torch sparse_csc torch Tensor ccol_indices torch Tensor row_indices torch sparse_bsr torch Tensor crow_indices torch Tensor col_indices torch sparse_bsc torch Tensor ccol_indices torch Tensor row_indices layout layout torch sparse_csr torch sparse_bsr cdimname pdimname = row column cdimname pdimname = column row compressed_indices_prefix = f c cdimname _indices=tensor compressed_indices = compressed_indices_method detach is_meta compressed_indices_str = compressed_indices_str = _tensor_str compressed_indices indent + len compressed_indices_prefix compressed_indices numel == is_meta compressed_indices_str += size= + str tuple compressed_indices shape plain_indices_prefix = f pdimname _indices=tensor plain_indices = plain_indices_method detach is_meta plain_indices_str = plain_indices_str = _tensor_str plain_indices indent + len plain_indices_prefix plain_indices numel == is_meta plain_indices_str += size= + str tuple plain_indices shape values_prefix = values=tensor values = values detach is_meta values_str = values_str = _tensor_str values indent + len values_prefix values numel == is_meta values_str += size= + str tuple values shape tensor_str = compressed_indices_prefix + compressed_indices_str + \n + indent + plain_indices_prefix + plain_indices_str + \n + indent + values_prefix + values_str + is_quantized suffixes append size= + str tuple shape has_default_dtype suffixes append dtype= + str dtype suffixes append quantization_scheme= + str qscheme qscheme == torch per_tensor_affine qscheme == torch per_tensor_symmetric suffixes append scale= + str q_scale suffixes append zero_point= + str q_zero_point qscheme == torch per_channel_affine qscheme == torch per_channel_symmetric qscheme == torch per_channel_affine_float_qparams suffixes append scale= + str q_per_channel_scales suffixes append zero_point= + str q_per_channel_zero_points suffixes append axis= + str q_per_channel_axis custom_contents_provided tensor_str = _tensor_str dequantize indent is_nested custom_contents_provided indented_str s indent \n join f line line s split \n strs = \n join indented_str str t indent + t torch ops aten unbind int tensor_str = f \n strs \n torch _is_functional_tensor prefix = _to_functional_tensor tensor_str = repr torch _from_functional_tensor Circular problem so we here torch _subclasses fake_tensor FakeTensor is_meta isinstance FakeTensor suffixes append size= + str tuple shape dtype = torch get_default_dtype suffixes append dtype= + str dtype TODO This implies ellipses valid syntax allocating meta tensor FakeTensor which could isn t right now custom_contents_provided tensor_str = numel == is_sparse Explicitly print shape match NumPy behavior dim = suffixes append size= + str tuple shape In empty tensor there no elements infer dtype should int so must shown explicitly dtype = torch get_default_dtype suffixes append dtype= + str dtype custom_contents_provided tensor_str = PRINT_OPTS edgeitems suffixes append size= + str tuple shape has_default_dtype suffixes append dtype= + str dtype custom_contents_provided layout = torch strided tensor_str = _tensor_str to_dense indent tensor_str = _tensor_str indent layout = torch strided suffixes append layout= + str layout Use inp here get original grad_fn one generated forward grad unpacking grad_fn_name = None try grad_fn = inp grad_fn except RuntimeError Accessing grad_fn calls rebasing logic which would cause error tensor view created no-grad mode modified in-place no-grad mode See https github com pytorch pytorch issues grad_fn_name = Invalid grad_fn_name None grad_fn None type ignore possibly-undefined pyrefly ignore unbound-name grad_fn_name = type grad_fn __name__ grad_fn_name == CppFunction pyrefly ignore unbound-name grad_fn_name = grad_fn name rsplit - grad_fn_name None suffixes append f grad_fn= grad_fn_name inp requires_grad suffixes append requires_grad=True has_names suffixes append f names= names tangent None suffixes append f tangent= tangent string_repr = _add_suffixes prefix + tensor_str type ignore possibly-undefined suffixes indent force_newline=self is_sparse Check instance flagged parameter change repr accordingly Unfortunately function has aware detail NB This currently skipped plain tensor parameters maintain BC In future should done those well produce valid repr isinstance torch nn Parameter is_plain_tensor string_repr = f Parameter string_repr string_repr _functorch_wrapper_str_intern tensor tensor_contents=None level = torch _C _functorch maybe_get_level tensor assert level = - torch _C _functorch is_functionaltensor tensor Since we re unwrapping FunctionalTensorWrapper we need make sure s up date first torch _sync tensor value = torch _C _functorch get_unwrapped tensor value_repr = repr value indented_value_repr = textwrap indent value_repr torch _C _functorch is_batchedtensor tensor bdim = torch _C _functorch maybe_get_bdim tensor assert bdim = - f BatchedTensor lvl= level bdim= bdim value=\n indented_value_repr \n torch _C _functorch is_gradtrackingtensor tensor f GradTrackingTensor lvl= level value=\n indented_value_repr \n torch _C _functorch is_functionaltensor tensor f FunctionalTensor lvl= level value=\\\n value_repr raise ValueError We don t know how print please file us issue _str tensor_contents=None torch no_grad torch utils _python_dispatch _disable_current_modes guard = torch _C _DisableFuncTorch noqa F _str_intern tensor_contents=tensor_contents