usr bin python mypy allow-untyped-defs collections io sys types collections abc Callable Iterator Mapping typing Any Optional TypeVar Union typing_extensions Self torch torch distributed rpc rpc torch device dtype nn Tensor torch distributed _remote_device torch distributed nn jit instantiator torch distributed rpc internal _internal_rpc_pickler torch nn Module torch nn parameter Parameter torch utils hooks RemovableHandle __all__ = RemoteModule _grad_t = Union tuple Tensor Tensor See https mypy readthedocs io en latest generics html#generic-methods-and-generic-self use ` T ` annotate ` ` Many methods ` Module ` ` ` we want those values type subclass looser type ` Module ` T = TypeVar T bound= Module _NON_SCRIPTABLE_REMOTE_MODULE_MODULE = instantiator instantiate_non_scriptable_remote_module_template _REMOTE_MODULE_PICKLED_ATTRIBUTES = device is_device_map_set is_scriptable generated_methods module_rref _SerializedRemoteModule = collections namedtuple type ignore misc _SerializedRemoteModule _REMOTE_MODULE_PICKLED_ATTRIBUTES These attributes mostly RemoteModule s parent intentionally pickled A new attribute RemoteModule should either _REMOTE_MODULE_PICKLED_ATTRIBUTES _REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING Otherwise will pickled _REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING = training _parameters _buffers _non_persistent_buffers_set _backward_hooks _backward_pre_hooks _is_full_backward_hook _forward_hooks _forward_hooks_with_kwargs _forward_hooks_always_called _forward_pre_hooks _forward_pre_hooks_with_kwargs _state_dict_hooks _state_dict_pre_hooks _load_state_dict_pre_hooks _load_state_dict_post_hooks _state_dict_pre_hooks _modules The two attributes below generated methods available pickling time forward_async forward RPC handler _instantiate_template module_interface_cls enable_moving_cpu_tensors_to_cuda instantiator instantiate_scriptable_remote_module_template module_interface_cls enable_moving_cpu_tensors_to_cuda _create_module module_cls args kwargs device module = module_cls args kwargs isinstance module nn Module raise ValueError Expect ` module_cls args kwargs ` returns instance nn Module f returns instance type module module device module _create_module_with_interface module_cls args kwargs device module_interface_cls module = _create_module module_cls args kwargs device module_interface_cls None module = torch jit script module rpc RRef module module_interface_cls _param_rrefs module_rref recurse - list rpc RRef Parameter ret list rpc RRef Parameter = rpc RRef param param module_rref local_value parameters recurse ret _raise_not_supported name str - None raise ValueError f Method ` ` name ` ` supported RemoteModule _RemoteModule nn Module __new__ cls args kwargs Use __new__ logging purposes torch _C _log_api_usage_once torch distributed nn api remote_module super __new__ cls __init__ remote_device str module_cls type nn Module args Optional tuple = None kwargs Optional dict str Any = None _module_interface_cls Any = None RemoteModule instance can only created after RPC initialization It creates user-specified module specified remote node It behaves like regular ` ` nn Module ` ` except ` ` forward ` ` method executed remote node It takes care autograd recording ensure backward pass propagates gradients back corresponding remote module It can shared across processors using ` RPC framework https pytorch org docs stable rpc html ` __ without incurring any overheads copying actual module which equivalent ` ~torch distributed rpc RRef ` pointing remote module The arguments ` ` forward_async ` ` ` ` forward ` ` same ` ` forward ` ` method module returned ` ` module_cls ` ` Apart ` ` forward_async ` ` ` ` forward ` ` no other methods supported nn Module now Particularly create hybrid model typically local modules should created outside remote modules rather than submodules any remote module calling ` ` add_module ` ` Hybrid Example HybridModel nn Module __init__ - None nn Module __init__ remote_embedding = RemoteModule local_linear = nn Linear For example ` ` module_cls ` ` returns instance ` ` nn Linear ` ` has ` ` forward ` ` method signature ` ` forward input Tensor - Tensor ` ` generated ` ` RemoteModule ` ` will have methods signature ` ` forward input Tensor - Tensor ` ` ` ` forward_async input Tensor - Future Tensor ` ` note If remote module placed cuda device any input CPU tensors will automatically moved same cuda device GPU tensors returned over wire according device map remote worker TensorPipe RPC backend Args remote_device str Device destination worker where we d like place module The device can local device remote device specified one following remote formats rank rank device ex rank cuda worker_name device ex trainer cuda In addition device field can optional default value cpu module_cls nn Module For example MyModule nn Module forward input input + module_cls = MyModule args Sequence optional args passed ` ` module_cls ` ` kwargs Dict optional kwargs passed ` ` module_cls ` ` _module_interface_cls type optional The TorchScript interface type module created The type object should decorated torch jit interface If provided generated RemoteModule torchscript-able Warning experimental API susceptible frequent changes Returns A remote module instance which wraps ` ~nn Module ` created user-provided ` ` module_cls ` ` has blocking ` ` forward ` ` method asynchronous ` ` forward_async ` ` method returns future ` ` forward ` ` call user-provided module remote side Example Run following code two different processes xdoctest +SKIP distributed On worker torch torch distributed rpc rpc torch nn Tensor torch distributed nn api remote_module RemoteModule rpc init_rpc worker rank= world_size= remote_linear_module = RemoteModule worker cpu nn Linear args= input = torch randn ret_fut = remote_linear_module forward_async input ret = ret_fut wait rpc shutdown On worker torch torch distributed rpc rpc rpc init_rpc worker rank= world_size= rpc shutdown super __init__ enable_moving_cpu_tensors_to_cuda = _prepare_init remote_device Default arguments preparation args = args args None kwargs = kwargs kwargs None _module_interface_cls None Users reply field know generated RemoteModule TorchScript-able is_scriptable = True Instantiate template remote side fut = rpc rpc_async _instantiate_template _module_interface_cls enable_moving_cpu_tensors_to_cuda _init_template _module_interface_cls enable_moving_cpu_tensors_to_cuda Instantiate template remote side fut = rpc rpc_async _instantiate_template _module_interface_cls enable_moving_cpu_tensors_to_cuda Create module remote side fut wait Ensure remote_module_cls available remote side TODO We need change rpc remote make async see branch below For we need able apply _module_interface_cls RRef returned rpc remote See https github com pytorch pytorch issues more context module_rref = rpc rpc_sync _create_module_with_interface module_cls args kwargs device _module_interface_cls is_scriptable = False generated_methods = _NON_SCRIPTABLE_REMOTE_MODULE_MODULE _generated_methods Create module remote side module_rref = rpc remote _create_module module_cls args kwargs device _install_generated_methods _check_attribute_picklability remote_parameters recurse bool = True - list rpc RRef Parameter Return list ` ~torch distributed rpc RRef ` pointing remote module s parameters This can typically used conjunction ` ~torch distributed optim DistributedOptimizer ` Args recurse bool True then returns parameters remote module all submodules remote module Otherwise returns only parameters direct members remote module Returns A list ` ~torch distributed rpc RRef ` ` ` List RRef nn Parameter ` ` remote module s parameters rpc rpc_sync _param_rrefs args= module_rref recurse get_module_rref - rpc RRef nn Module Return ` ~torch distributed rpc RRef ` ` ` RRef nn Module ` ` pointing remote module module_rref torch jit export __getstate__ raise RuntimeError Cannot pickle RemoteModule python pickler RemoteModule can only pickled when using RPC torch jit export __setstate__ state raise RuntimeError Cannot unpickle RemoteModule python pickler RemoteModule can only unpickled when using RPC register_buffer name str tensor Optional Tensor persistent bool = True - None _raise_not_supported register_buffer __name__ register_parameter name str param Optional Parameter - None _raise_not_supported register_parameter __name__ add_module name str module Optional Module - None _raise_not_supported add_module __name__ apply fn Callable Module None - Self type ignore _raise_not_supported apply __name__ cuda device Optional Union int device = None - Self type ignore _raise_not_supported cuda __name__ ipu device Optional Union int device = None - Self type ignore _raise_not_supported ipu __name__ xpu device Optional Union int device = None - Self type ignore _raise_not_supported xpu __name__ cpu - Self type ignore _raise_not_supported cpu __name__ type dst_type Union dtype str - Self type ignore _raise_not_supported type __name__ float - Self type ignore _raise_not_supported float __name__ double - Self type ignore _raise_not_supported double __name__ half - Self type ignore _raise_not_supported half __name__ bfloat - Self type ignore _raise_not_supported bfloat __name__ args kwargs - T type ignore misc type-var _raise_not_supported __name__ register_backward_hook type ignore hook Callable Module _grad_t _grad_t Union None _grad_t pyrefly ignore bad-return - RemovableHandle _raise_not_supported register_backward_hook __name__ register_forward_pre_hook type ignore hook Union Callable T tuple Any Optional Any Callable T tuple Any dict str Any Optional tuple Any dict str Any prepend bool = False with_kwargs bool = False pyrefly ignore bad-return - RemovableHandle _raise_not_supported register_forward_pre_hook __name__ register_forward_hook type ignore override hook Union Callable T tuple Any Any Optional Any Callable T tuple Any dict str Any Any Optional Any prepend bool = False with_kwargs bool = False pyrefly ignore bad-return - RemovableHandle _raise_not_supported register_forward_hook __name__ state_dict args kwargs _raise_not_supported state_dict __name__ load_state_dict state_dict Mapping str Any strict bool = True assign bool = False _raise_not_supported load_state_dict __name__ parameters recurse bool = True - Iterator Parameter raise ValueError Method ` ` parameters ` ` supported RemoteModule Please use ` ` remote_parameters ` ` instead named_parameters type ignore prefix str = recurse bool = True remove_duplicate bool = True pyrefly ignore bad-return - Iterator tuple str Parameter _raise_not_supported named_parameters __name__ buffers recurse bool = True - Iterator Tensor type ignore _raise_not_supported buffers __name__ named_buffers type ignore prefix str = recurse bool = True remove_duplicate bool = True pyrefly ignore bad-return - Iterator tuple str Tensor _raise_not_supported named_buffers __name__ children - Iterator Module type ignore _raise_not_supported children __name__ named_children - Iterator tuple str Module type ignore _raise_not_supported named_children __name__ modules - Iterator Module type ignore _raise_not_supported modules __name__ named_modules memo Optional set Module = None prefix str = remove_duplicate bool = True _raise_not_supported named_modules __name__ train mode bool = True - Self module_rref rpc_sync train type ignore operator union-attr eval - Self module_rref rpc_sync eval type ignore operator union-attr requires_grad_ requires_grad bool = True - Self type ignore _raise_not_supported requires_grad_ __name__ zero_grad set_to_none bool = True - None _raise_not_supported zero_grad __name__ share_memory - Self type ignore _raise_not_supported share_memory __name__ extra_repr - str type ignore _raise_not_supported extra_repr __name__ _prepare_init remote_device_str str - bool Prepare initialization returns whether enable automatically moving CPU tensors CUDA devices Sanity check assert rpc _is_current_rpc_agent_set RemoteModule only works RPC remote_device = _remote_device remote_device_str = remote_device worker_name remote_device worker_name None remote_device rank device = str remote_device device agent = rpc _get_current_rpc_agent If device map remote worker set then enable moving any input CPU tensors same cuda device is_device_map_set = bool agent _get_device_map agent get_worker_info type ignore arg-type ` ` enable_moving_cpu_tensors_to_cuda ` ` less strict than ` ` is_device_map_set ` ` If ` ` enable_moving_cpu_tensors_to_cuda ` ` true device map set then any CPU tensors can still moved cuda device run forward output must moved back CPU before being sent over wire enable_moving_cpu_tensors_to_cuda = torch device device type == cuda enable_moving_cpu_tensors_to_cuda _init_template module_interface_cls enable_moving_cpu_tensors_to_cuda Instantiate template local side generated_module = instantiator instantiate_scriptable_remote_module_template module_interface_cls enable_moving_cpu_tensors_to_cuda generated_methods = generated_module _generated_methods _check_attribute_picklability Check all attribute has explicitly defined whether pickled i e picklability k __dict__ keys k _REMOTE_MODULE_PICKLED_ATTRIBUTES k _REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING raise AttributeError f Attribute k must either ` ` _REMOTE_MODULE_PICKLED_ATTRIBUTES ` ` ` ` _REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING ` ` _install_generated_methods method generated_methods method_name = method __name__ method = torch jit export method setattr method_name types MethodType method staticmethod init_from_module_rref remote_device str module_rref rpc RRef nn Module _module_interface_cls Any = None Besides constructor RemoteModule instance can also initialized given module RRef This alternate initialization method can particularly useful we want create multiple RemoteModule instances share same underlying module reduce memory consumption Moreover also provides workaround passing script RemoteModule over RPC which supported The recommended way follows sender creates RemoteModule sender sends its ` ` module_rref ` ` over RPC receiver calls method initialize another RemoteModule using same ` ` module_rref ` ` Example Run following code two different processes xdoctest +SKIP distributed On worker torch torch distributed rpc rpc torch nn Tensor torch distributed nn api remote_module RemoteModule rpc init_rpc worker rank= world_size= remote_module = RemoteModule worker cpu nn Linear args= remote_module = rpc rpc_sync worker cpu RemoteModule init_from_module_rref worker cpu remote_module get_module_rref rpc shutdown On worker torch torch distributed rpc rpc rpc init_rpc worker rank= world_size= rpc shutdown Args remote_device str Device destination worker where we d like place module The device can local device remote device specified one following remote formats rank rank device ex rank cuda worker_name device ex trainer cuda In addition device field can optional default value cpu module_rref RRef nn Module The module reference shared both caller created remote module _module_interface_cls type optional The TorchScript interface type module created The type object should decorated torch jit interface If provided generated RemoteModule torchscript-able Warning experimental API susceptible frequent changes Returns A remote module instance which wraps ` ~nn Module ` created user-provided ` ` module_rref ` ` has blocking ` ` forward ` ` method asynchronous ` ` forward_async ` ` method returns future ` ` forward ` ` call user-provided module remote side NOTE new attribute added also need add ` ` _REMOTE_MODULE_PICKLED_ATTRIBUTES ` ` pickling unpickling remote_module = object __new__ RemoteModule pyrefly ignore missing-attribute enable_moving_cpu_tensors_to_cuda = remote_module _prepare_init remote_device _module_interface_cls None Users reply field know generated RemoteModule TorchScript-able pyrefly ignore missing-attribute remote_module is_scriptable = True pyrefly ignore missing-attribute remote_module _init_template _module_interface_cls enable_moving_cpu_tensors_to_cuda pyrefly ignore missing-attribute remote_module is_scriptable = False pyrefly ignore missing-attribute remote_module generated_methods = _NON_SCRIPTABLE_REMOTE_MODULE_MODULE _generated_methods pyrefly ignore missing-attribute remote_module module_rref = module_rref pyrefly ignore missing-attribute remote_module _install_generated_methods pyrefly ignore missing-attribute remote_module _check_attribute_picklability remote_module RemoteModule _RemoteModule A RemoteModule instance can only created after RPC initialization It creates user-specified module specified remote node It behaves like regular ` ` nn Module ` ` except ` ` forward ` ` method executed remote node It takes care autograd recording ensure backward pass propagates gradients back corresponding remote module It generates two methods ` ` forward_async ` ` ` ` forward ` ` based signature ` ` forward ` ` method ` ` module_cls ` ` ` ` forward_async ` ` runs asynchronously returns Future The arguments ` ` forward_async ` ` ` ` forward ` ` same ` ` forward ` ` method module returned ` ` module_cls ` ` For example ` ` module_cls ` ` returns instance ` ` nn Linear ` ` has ` ` forward ` ` method signature ` ` forward input Tensor - Tensor ` ` generated ` ` RemoteModule ` ` will have methods signatures &#124; ` ` forward input Tensor - Tensor ` ` &#124; ` ` forward_async input Tensor - Future Tensor ` ` Args remote_device str Device destination worker where we d like place module The format should workername device where device field can parsed torch device type E g trainer cpu trainer ps cuda In addition device field can optional default value cpu module_cls nn Module Class module created remotely For example MyModule nn Module forward input input + module_cls = MyModule args Sequence optional args passed ` ` module_cls ` ` kwargs Dict optional kwargs passed ` ` module_cls ` ` Returns A remote module instance which wraps ` ~nn Module ` created user-provided ` ` module_cls ` ` has blocking ` ` forward ` ` method asynchronous ` ` forward_async ` ` method returns future ` ` forward ` ` call user-provided module remote side Example Run following code two different processes xdoctest +SKIP distributed On worker torch torch distributed rpc rpc torch nn Tensor torch distributed nn api remote_module RemoteModule rpc init_rpc worker rank= world_size= remote_linear_module = RemoteModule worker cpu nn Linear args= input = torch randn ret_fut = remote_linear_module forward_async input ret = ret_fut wait rpc shutdown On worker torch torch distributed rpc rpc rpc init_rpc worker rank= world_size= rpc shutdown Furthermore more practical example combined ` DistributedDataParallel https pytorch org docs stable nn html#torch nn parallel DistributedDataParallel ` __ DDP can found ` tutorial https pytorch org tutorials advanced rpc_ddp_tutorial html ` __ __init__ remote_device str module_cls type nn Module args Optional tuple = None kwargs Optional dict str Any = None super __init__ remote_device module_cls args kwargs _remote_module_receiver remote_module_pickled_attrs Deserializes RemoteModule serialized_remote_module = _SerializedRemoteModule _make remote_module_pickled_attrs m = object __new__ RemoteModule m __dict__ update serialized_remote_module _asdict Unpickling attribute ` module_rref ` must invoke RRef s ` _deserialize ` method pyrefly ignore missing-attribute m module_rref = rpc PyRRef _deserialize m module_rref Install generated methods when unpickled pyrefly ignore missing-attribute method m generated_methods method_name = method __name__ method = torch jit export method setattr m method_name types MethodType method m m _remote_module_reducer remote_module Serialize RemoteModule pickled_attrs = k v remote_module __dict__ items Pickling attribute ` module_rref ` must invoke RRef s ` _serialize ` method k == module_rref pickled_attrs k = v _serialize k _REMOTE_MODULE_PICKLED_ATTRIBUTES pickled_attrs k = v Check unpickled attributes all _REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING k _REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING print f The new attribute ` ` k ` ` RemoteModule ignored during RPC pickling To pickle attribute please add ` ` _REMOTE_MODULE_PICKLED_ATTRIBUTES ` ` Otherwise please explicitly add ` ` _REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING ` ` file=sys stderr _remote_module_receiver tuple pickled_attrs values _recursive_script_module_receiver recursive_script_module_serialized Deserializes RecursiveScriptModule does contain script RemoteModule f = io BytesIO recursive_script_module_serialized m = torch jit load f m _recursive_script_module_reducer recursive_script_module Serialize RecursiveScriptModule does contain script RemoteModule raises error otherwise hasattr recursive_script_module _c module_rref raise RuntimeError Passing script RemoteModule over RPC supported Please create RemoteModule sender send ` module_rref ` receiver create new instance receiver end passing ` module_rref ` f = io BytesIO torch jit save recursive_script_module f _recursive_script_module_receiver f getvalue _internal_rpc_pickler _register_reducer RemoteModule _remote_module_reducer _internal_rpc_pickler _register_reducer torch jit RecursiveScriptModule _recursive_script_module_reducer