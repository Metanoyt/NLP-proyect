This module implements variable tracking PyTorch optimizers during Dynamo tracing The OptimizerVariable provides specialized handling optimizer instances - Optimizing tracing expensive optimizer initialization - Managing optimizer state parameter group tracking - Handling tensor sources guards optimizer state tensors - Supporting CUDA graph execution through static tensor address management - Providing special handling parameter gradients optimizer state tensors Key features include - Efficient initialization tracing via _init_group optimization - Automatic marking optimizer state tensors static CUDA graphs - Proper source tracking parameter groups gradients state tensors - Guard installation optimizer state structure - Support both CPU GPU tensor handling - Cleanup static tensor references via finalizers The module integrates Dynamo s broader tracing system while providing optimizer-specific optimizations safety guarantees logging weakref typing Any Iterable Optional TYPE_CHECKING torch torch _dynamo variables tensor TensorVariable torch _guards Source torch _logging getArtifactLogger torch utils _pytree tree_map_only guards GuardBuilder install_guard source AttrSource ConstDictKeySource DictGetItemSource GetItemSource GlobalWeakRefSource GradSource utils GLOBAL_KEY_PREFIX base VariableTracker constant ConstantVariable dicts ConstDictVariable lists ListVariable misc GetAttrVariable user_defined UserDefinedObjectVariable TYPE_CHECKING torch _dynamo symbolic_convert InstructionTranslator ArgMappingException Exception pass GuardInstallException Exception pass perf_hint_log = getArtifactLogger __name__ perf_hints _is_static_for_cudagraphs x torch Tensor - bool torch _inductor cudagraph_trees get_manager x is_cuda manager = get_manager x device index False is_static_address = torch _dynamo utils get_static_address_type x None manager assert manager current_node None is_static_address manager current_node _is_cuda_graph_recorded_tensor x is_static_address Don t print warning non-cuda tensors True OptimizerVariable UserDefinedObjectVariable _nonvar_fields = grad_to_source tensor_to_source static_tensor_names UserDefinedObjectVariable _nonvar_fields __init__ value torch optim Optimizer grad_to_source Optional dict Any GradSource = None static_tensor_names Optional set str = None tensor_to_source Optional dict torch Tensor Source = None kwargs Any - None super __init__ value kwargs value torch optim Optimizer = value grad_to_source = grad_to_source tensor_to_source = tensor_to_source static_tensor_names = static_tensor_names set call_method tx InstructionTranslator name str args list VariableTracker kwargs dict str VariableTracker - VariableTracker This optimization avoid tracing very slow initialization optimizer name == _init_group hasattr value _init_group Fallback optimizer does have _init_group trace normally super call_method tx name args kwargs try graph_break_if_pending_mutation tx move_step_if_cpu py_args py_kwargs = get_python_args args kwargs ret_val = value _init_group py_args py_kwargs map_sources_and_install_guards tx update_list_args tx args kwargs py_args py_kwargs stash weak_ptr optimizer invalidate code optimizer object dies mangled_name = f __optimizer_ id value tx store_global_weakref_by_id mangled_name value create_finalizer tx This currently safe only because only actual ` ret_val ` s returned ` _init_group ` existing optimizers properties invariant input tensors e g dtype layout Changing these would trigger recompilation hence never result wrong specialization ` ret_val ` ConstantVariable create ret_val except ArgMappingException GuardInstallException _ trace normally we can t map args install guards correctly pass super call_method tx name args kwargs var_getattr tx InstructionTranslator name str - VariableTracker Note allows us intercept call call_method typical case we UserMethodVariable which will directly inline name _init_group step assert source GetAttrVariable name source=AttrSource source name name == param_groups decorators mark_static_address group value param_groups p group params mark_static_address p guard=True _set_capturable tx super var_getattr tx name graph_break_if_pending_mutation tx InstructionTranslator - None If there pending mutations parameter due using closure then we need graph break allow python version parameter update so running _init_group will initialize states correct values g value param_groups p g params side_effects = tx output side_effects variable = side_effects id_to_variable get id p None variable side_effects has_pending_mutation variable exc Unsupported raise Unsupported Pending mutation parameter _set_capturable tx InstructionTranslator - None LazyVariableTracker We only set capturable params cuda state initialized safe_to_set_capturable group dict str Any - bool all_uninitialized = True all_gpu = True p group get params all_gpu = p is_cuda p is_xpu all_uninitialized = p value state capturable group all_uninitialized all_gpu track indices set so we don t need variable tracker realize whole state we handle guarding state specially group value param_groups safe_to_set_capturable group group capturable = True source = source AttrSource source param_groups param_groups_vt = LazyVariableTracker realize_all VariableTracker build tx value param_groups source param_group_vt param_groups_vt items key = ConstDictVariable _HashableTracker ConstantVariable create capturable param_group_vt items key = ConstantVariable create True get_python_args args Any kwargs Any - tuple list Any dict str Any Get python values equivalent variable tracker args map_arg arg Any - Any isinstance arg ConstantVariable arg as_python_constant isinstance arg ListVariable arg items isinstance arg ConstDictVariable isinstance arg source GetItemSource isinstance arg source base AttrSource arg source base member == param_groups value param_groups arg source index raise ArgMappingException new_args = map_arg arg arg args new_kwargs = k map_arg v k v kwargs items new_args new_kwargs If users load old state dictionary s possible step could cpu case move GPU corresponding parameter most cases no-op because state empty move_step_if_cpu - None p state value state items step state state step is_cpu state step = state step p device map_sources_and_install_guards tx InstructionTranslator - None decorators mark_static_address lazy LazyVariableTracker grad_to_source = tensor_to_source = mark_static x Any - None mark_static_address x guard=True tree_map_only torch Tensor mark_static value state Recursively realize variable trackers optim state optim param_groups which recursively install necessary guards params_groups_source = source AttrSource source param_groups param_groups_vt = LazyVariableTracker realize_all VariableTracker build tx value param_groups params_groups_source state_source = source AttrSource source state state_vt = VariableTracker build tx value state state_source We need realize top level state dict populate guard locals state_vt realize assert state_source None tx output guard_on_key_order add state_source Populate grad_to_source tensor_to_source so we can manually update_list_args group group_vt zip value param_groups param_groups_vt items we assume here all params within param group initialized similarly len group params param group params param grad None key_index = None i k enumerate value state keys k param key_index = i break key_index LazyVariableTracker realize_all VariableTracker build tx value state param DictGetItemSource state_source ConstDictKeySource state_source key_index break params_vt = group_vt getitem_const tx ConstantVariable create params all_static = True non_static_grads = p p_vt zip group params params_vt unpack_var_sequence tx param_source = p_vt source tensor_to_source p = param_source grad_source = GradSource param_source grad p grad None grad_to_source p grad = grad_source _is_static_for_cudagraphs p grad all_static = False non_static_grads append grad_source install_guard grad_source make_guard GuardBuilder CONSTANT_MATCH Note avoid spam logs only warn perf hint artifact enabled NB artifacts only enabled debug warning level all_static perf_hint_log isEnabledFor logging DEBUG non_static_grad_names = src name src non_static_grads perf_hint_log warning Grad tensors s will copied during cudagraphs execution If using cudagraphs grad tensor addresses will same across runs use torch _dynamo decorators mark_static_address elide copy non_static_grad_names We have again iterate over state dict collect tensor_to_source dict This used finalizer idx value enumerate value state values p_state_source = DictGetItemSource state_source ConstDictKeySource state_source idx tx output guard_on_key_order add p_state_source inner_idx v enumerate value values isinstance v torch Tensor v grad_to_source v tensor_to_source tensor_to_source v = DictGetItemSource p_state_source ConstDictKeySource p_state_source inner_idx wrap_tensor tx InstructionTranslator tensor_value torch Tensor - TensorVariable Wrap state tensor TensorVariable decorators mark_static_address If we have source tensor already use we have seen tensor before stash use global weak ref source since must optimizer tensor we have missed tensor_value tensor_to_source mark these tensors static cudagraphs mark_static_address tensor_value guard=True source = tensor_to_source tensor_value static_tensor_names add tx output module_key_name source name tensor_value grad_to_source source = grad_to_source tensor_value mark these tensors static cudagraphs mark_static_address tensor_value guard=True global_name = tx store_global_weakref_by_id GLOBAL_KEY_PREFIX tensor_value source = GlobalWeakRefSource global_name static_tensor_names add tx output module_key_name source name VariableTracker build tx tensor_value source update_list_args tx InstructionTranslator args Iterable VariableTracker kwargs Any py_args Iterable Any py_kwargs Any - None Update args kwargs traced optimizer call arg py_arg zip args py_args isinstance arg ListVariable assert isinstance py_arg list py_arg should list optimizer variable i val enumerate py_arg tx output side_effects mutation arg isinstance val torch Tensor arg items append wrap_tensor tx val source = arg source GetItemSource arg source i arg items append VariableTracker build tx val source create_finalizer tx InstructionTranslator - None names_to_delete = static_tensor_names value = value tc = tx output tracing_context init_finalizer gm torch fx GraphModule - None clear_static_tensor_refs - None name names_to_delete gm _buffers pop name None gm _parameters pop name None tc params_flat tc params_flat clear tc params_flat_unwrap_subclasses tc params_flat_unwrap_subclasses clear weakref finalize value clear_static_tensor_refs tx output add_graph_finalizer init_finalizer