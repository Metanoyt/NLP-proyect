mypy allow-untyped-defs r Learning Rate Scheduler __future__ annotations math types warnings bisect bisect_right collections Counter functools partial wraps typing Any cast Literal Optional SupportsFloat TYPE_CHECKING TypedDict Union typing_extensions override Self weakref ref torch inf Tensor optimizer _to_scalar Optimizer TYPE_CHECKING collections abc Callable Iterable Sequence __all__ = LambdaLR MultiplicativeLR StepLR MultiStepLR ConstantLR LinearLR ExponentialLR SequentialLR CosineAnnealingLR ChainedScheduler ReduceLROnPlateau CyclicLR CosineAnnealingWarmRestarts OneCycleLR PolynomialLR LRScheduler EPOCH_DEPRECATION_WARNING = The epoch parameter ` scheduler step ` necessary being deprecated where possible Please use ` scheduler step ` step scheduler During deprecation epoch different None closed form used instead new chainable form where available Please open issue you unable replicate your use case https github com pytorch pytorch issues new choose _format_param name str optimizer Optimizer param Return correctly formatted lr momentum each param group _copy _param _param clone isinstance _param Tensor _param isinstance param list tuple len param = len optimizer param_groups raise ValueError f name must have same length optimizer param_groups f name has len param values param_groups has len optimizer param_groups param = param len optimizer param_groups list map _copy param _param_groups_val_list optimizer Optimizer key str - list Any Create list containing group key each optimizer param_group Prevents aliasing when group key could Tensor Raises KeyError when group key does exist group key clone isinstance group key Tensor group key group optimizer param_groups _update_param_group_val param_group dict str Any key str val float &#124; Tensor Set param_group key val without aliasing assignment when they re both tensors Raises KeyError param_group key does exist isinstance param_group key Tensor param_group key fill_ _to_scalar val param_group key = val LRScheduler r Base all learning rate schedulers Subclasses implement meth ` get_lr ` optionally override meth ` step ` define scheduling behavior Args optimizer Optimizer The optimizer scheduler will adjust learning rates last_epoch int Index last epoch seen scheduler Use ` ` - ` ` default initialize scheduler Only use non-default value when restoring scheduler saved checkpoint warning Initializing scheduler overwrites its optimizer s ` ` param_group lr ` ` \s When restoring checkpoint initialize scheduler before calling your optimizer s meth ` ~torch optim Optimizer load_state_dict ` avoid overwriting loaded learning rates _get_lr_called_within_step bool = False _is_initial bool = False __init__ optimizer Optimizer last_epoch int = - - None noqa D Attach optimizer isinstance optimizer Optimizer raise TypeError f type optimizer __name__ Optimizer optimizer = optimizer Initialize epoch base learning rates last_epoch == - group optimizer param_groups initial_lr = group lr isinstance initial_lr Tensor initial_lr = initial_lr clone group setdefault initial_lr initial_lr i group enumerate optimizer param_groups initial_lr group raise KeyError f param initial_lr specified param_groups i when resuming scheduler last_epoch = \n This typically happens when \n You re trying resume training checkpoint haven t properly loaded optimizer state\n You re using last_epoch = fresh training run recommended base_lrs list float &#124; Tensor = _param_groups_val_list optimizer initial_lr last_epoch = last_epoch Following https github com pytorch pytorch issues We would like ensure ` lr_scheduler step ` called after ` optimizer step ` patch_track_step_called opt Optimizer hasattr opt step _wrapped_by_lr_sched we ve already patched opt step wrap_step step_fn opt_ref = ref optimizer func = step_fn __func__ wraps func wrapper args kwargs opt = opt_ref opt _opt_called = True type ignore union-attr func __get__ opt opt __class__ args kwargs wrapper _wrapped_by_lr_sched = True type ignore attr-defined wrapper opt step = wrap_step opt step type ignore method-assign patch_track_step_called optimizer _initial_step _initial_step - None Initialize step counts perform step _step_count = _initial_mode step state_dict - dict str Any Return state scheduler ` dict ` It contains entry every variable ` ` __dict__ ` ` which optimizer key value key value __dict__ items key = optimizer load_state_dict state_dict dict str Any Load scheduler s state Args state_dict dict scheduler state Should object returned call meth ` state_dict ` __dict__ update state_dict get_last_lr - list float &#124; Tensor r Get most recent learning rates computed scheduler Returns list float &#124; Tensor A ` list ` learning rates entries each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their ` ` group lr ` ` \s note The returned ` ~torch Tensor ` \s copies never alias optimizer s ` ` group lr ` ` \s We always update _last_lr _param_groups_val_list so s clone group lr s If we didn t do user could corrupt their learning rates modifying outputs place _last_lr get_lr - list float &#124; Tensor r Compute next learning rate each optimizer s attr ` ~torch optim Optimizer param_groups ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s note If you re trying inspect most recent learning rate use meth ` get_last_lr ` instead note The returned ` ~torch Tensor ` \s copies never alias optimizer s ` ` group lr ` ` \s raise NotImplementedError step epoch Optional int = None - None Step scheduler Args epoch int optional deprecated If provided sets attr ` last_epoch ` ` ` epoch ` ` uses meth ` _get_closed_form_lr ` available This universally supported Use meth ` step ` without arguments instead note Call method after calling optimizer s meth ` ~torch optim Optimizer step ` Raise warning old pattern detected https github com pytorch pytorch issues _step_count == hasattr optimizer step _wrapped_by_lr_sched warnings warn Seems like ` optimizer step ` has been overridden after learning rate scheduler initialization Please make sure call ` optimizer step ` before ` lr_scheduler step ` See more details https pytorch org docs stable optim html#how-to-adjust-learning-rate UserWarning stacklevel= Just check there two first lr_scheduler step calls before optimizer step getattr optimizer _opt_called False warnings warn Detected call ` lr_scheduler step ` before ` optimizer step ` In PyTorch later you should call them opposite order ` optimizer step ` before ` lr_scheduler step ` Failure do will result PyTorch skipping first value learning rate schedule See more details https pytorch org docs stable optim html#how-to-adjust-learning-rate UserWarning stacklevel= _step_count += epoch None warnings warn EPOCH_DEPRECATION_WARNING UserWarning stacklevel= _update_lr epoch _update_lr epoch Optional int = None _enable_get_lr_call epoch None last_epoch += values = get_lr last_epoch = epoch hasattr _get_closed_form_lr values = cast list Union float Tensor _get_closed_form_lr values = get_lr param_group lr zip optimizer param_groups values strict=True _update_param_group_val param_group lr lr _last_lr list float &#124; Tensor = _param_groups_val_list optimizer lr _warn_get_lr_called_within_step lr_scheduler LRScheduler - None lr_scheduler _get_lr_called_within_step warnings warn To get last learning rate computed scheduler please use ` get_last_lr ` UserWarning stacklevel= Including _LRScheduler backwards compatibility Subclass instead assign because we want __name__ _LRScheduler _LRScheduler assigning would make LRScheduler _LRScheduler LRScheduler pass _enable_get_lr_call __init__ o LRScheduler - None o = o __enter__ - Self o _get_lr_called_within_step = True __exit__ type value traceback - None o _get_lr_called_within_step = False _initial_mode __init__ o LRScheduler o = o __enter__ o _is_initial = True __exit__ type value traceback o _is_initial = False LambdaLR LRScheduler Sets initial learning rate The learning rate each parameter group set initial lr times given function When last_epoch=- sets initial lr lr Args optimizer Optimizer Wrapped optimizer lr_lambda function list A function which computes multiplicative factor given integer parameter epoch list such functions one each group optimizer param_groups last_epoch int The index last epoch Default - Example xdoctest +SKIP Assuming optimizer has two groups num_epochs = lambda = lambda epoch epoch lambda = lambda epoch epoch scheduler = LambdaLR optimizer lr_lambda= lambda lambda epoch range num_epochs train validate scheduler step Alternatively you can use single lambda function all groups scheduler = LambdaLR opt lr_lambda=lambda epoch epoch epoch range num_epochs train validate scheduler step image scripts lr_scheduler_images LambdaLR png __init__ optimizer Optimizer lr_lambda Union Callable int float list Callable int float last_epoch int = - - None noqa D optimizer = optimizer lr_lambdas list Callable int float isinstance lr_lambda list isinstance lr_lambda tuple lr_lambdas = lr_lambda len optimizer param_groups len lr_lambda = len optimizer param_groups raise ValueError f Expected len optimizer param_groups lr_lambdas got len lr_lambda lr_lambdas = list lr_lambda super __init__ optimizer last_epoch override state_dict - dict str Any Return state scheduler ` dict ` It contains entry every variable ` ` __dict__ ` ` which optimizer The learning rate lambda functions will only saved they callable objects they functions lambdas When saving loading scheduler please make sure also save load state optimizer state_dict = key value key value __dict__ items key optimizer lr_lambdas state_dict lr_lambdas = None len lr_lambdas idx fn enumerate lr_lambdas isinstance fn types FunctionType pyrefly ignore unsupported-operation state_dict lr_lambdas idx = fn __dict__ copy state_dict override load_state_dict state_dict dict str Any - None Load scheduler s state When saving loading scheduler please make sure also save load state optimizer Args state_dict dict scheduler state Should object returned call meth ` state_dict ` lr_lambdas = state_dict pop lr_lambdas __dict__ update state_dict Restore state_dict keys order prevent side effects https github com pytorch pytorch issues state_dict lr_lambdas = lr_lambdas idx fn enumerate lr_lambdas fn None lr_lambdas idx __dict__ update fn override get_lr - list float &#124; Tensor r Compute next learning rate each optimizer s attr ` ~torch optim Optimizer param_groups ` Scales attr ` base_lrs ` outputs attr ` lr_lambdas ` attr ` last_epoch ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s note If you re trying inspect most recent learning rate use meth ` get_last_lr ` instead note The returned ` ~torch Tensor ` \s copies never alias optimizer s ` ` group lr ` ` \s _warn_get_lr_called_within_step base_lr lmbda last_epoch lmbda base_lr zip lr_lambdas base_lrs strict=True MultiplicativeLR LRScheduler Multiply learning rate each parameter group factor given specified function When last_epoch=- set initial lr lr Args optimizer Optimizer Wrapped optimizer lr_lambda function list A function which computes multiplicative factor given integer parameter epoch list such functions one each group optimizer param_groups last_epoch int The index last epoch Default - Example xdoctest +SKIP lmbda = lambda epoch scheduler = MultiplicativeLR optimizer lr_lambda=lmbda epoch range train validate scheduler step image scripts lr_scheduler_images MultiplicativeLR png __init__ optimizer Optimizer lr_lambda Union Callable int float list Callable int float last_epoch int = - - None noqa D optimizer = optimizer lr_lambdas list Callable int float isinstance lr_lambda list isinstance lr_lambda tuple lr_lambdas = lr_lambda len optimizer param_groups len lr_lambda = len optimizer param_groups raise ValueError f Expected len optimizer param_groups lr_lambdas got len lr_lambda lr_lambdas = list lr_lambda lr_lambda lr_lambdas callable lr_lambda raise TypeError f lr_lambda should function got type lr_lambda __name__ super __init__ optimizer last_epoch override state_dict - dict str Any Return state scheduler ` dict ` It contains entry every variable ` ` __dict__ ` ` which optimizer The learning rate lambda functions will only saved they callable objects they functions lambdas state_dict = key value key value __dict__ items key optimizer lr_lambdas state_dict lr_lambdas = None len lr_lambdas idx fn enumerate lr_lambdas isinstance fn types FunctionType pyrefly ignore unsupported-operation state_dict lr_lambdas idx = fn __dict__ copy state_dict override load_state_dict state_dict dict str Any - None Load scheduler s state Args state_dict dict scheduler state Should object returned call meth ` state_dict ` lr_lambdas = state_dict pop lr_lambdas __dict__ update state_dict Restore state_dict keys order prevent side effects https github com pytorch pytorch issues state_dict lr_lambdas = lr_lambdas idx fn enumerate lr_lambdas fn None lr_lambdas idx __dict__ update fn override get_lr - list float &#124; Tensor r Compute next learning rate each optimizer s attr ` ~torch optim Optimizer param_groups ` Scales current ` ` group lr ` ` \s each optimizer s attr ` ~torch optim Optimizer param_groups ` outputs attr ` lr_lambdas ` attr ` last_epoch ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s note If you re trying inspect most recent learning rate use meth ` get_last_lr ` instead note The returned ` ~torch Tensor ` \s copies never alias optimizer s ` ` group lr ` ` \s _warn_get_lr_called_within_step _is_initial group lr lmbda last_epoch lmbda group zip lr_lambdas optimizer param_groups strict=True _param_groups_val_list optimizer lr StepLR LRScheduler Decays learning rate each parameter group gamma every step_size epochs Notice such decay can happen simultaneously other changes learning rate outside scheduler When last_epoch=- sets initial lr lr Args optimizer Optimizer Wrapped optimizer step_size int Period learning rate decay gamma float Multiplicative factor learning rate decay Default last_epoch int The index last epoch Default - Example xdoctest +SKIP Assuming optimizer uses lr = all groups lr = epoch lr = = epoch lr = = epoch scheduler = StepLR optimizer step_size= gamma= epoch range train validate scheduler step image scripts lr_scheduler_images StepLR png __init__ optimizer Optimizer step_size int gamma float = last_epoch int = - - None noqa D step_size = step_size gamma = gamma super __init__ optimizer last_epoch override get_lr - list float &#124; Tensor r Compute next learning rate each optimizer s attr ` ~torch optim Optimizer param_groups ` If current epoch non-zero multiple attr ` step_size ` we scale current ` ` group lr ` ` \s optimizer s attr ` ~torch optim Optimizer param_groups ` attr ` gamma ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s note If you re trying inspect most recent learning rate use meth ` get_last_lr ` instead note The returned ` ~torch Tensor ` \s copies never alias optimizer s ` ` group lr ` ` \s _warn_get_lr_called_within_step last_epoch == last_epoch step_size = _param_groups_val_list optimizer lr group lr gamma group optimizer param_groups _get_closed_form_lr - list float &#124; Tensor r Compute learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` attr ` last_epoch ` using closed-form formula Uses attr ` base_lrs ` compute learning rates This method called when epoch passed meth ` step ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s base_lr gamma last_epoch step_size base_lr base_lrs MultiStepLR LRScheduler Decays learning rate each parameter group gamma once number epoch reaches one milestones Notice such decay can happen simultaneously other changes learning rate outside scheduler When last_epoch=- sets initial lr lr Args optimizer Optimizer Wrapped optimizer milestones list List epoch indices Must increasing gamma float Multiplicative factor learning rate decay Default last_epoch int The index last epoch Default - Example xdoctest +SKIP Assuming optimizer uses lr = all groups lr = epoch lr = = epoch lr = epoch = scheduler = MultiStepLR optimizer milestones= gamma= epoch range train validate scheduler step image scripts lr_scheduler_images MultiStepLR png __init__ optimizer Optimizer milestones Iterable int gamma float = last_epoch int = - - None noqa D milestones = Counter milestones gamma = gamma super __init__ optimizer last_epoch override get_lr - list float &#124; Tensor r Compute next learning rate each optimizer s attr ` ~torch optim Optimizer param_groups ` If current epoch attr ` milestones ` decays ` ` group lr ` ` \s optimizer s attr ` ~torch optim Optimizer param_groups ` attr ` gamma ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s note If you re trying inspect most recent learning rate use meth ` get_last_lr ` instead note The returned ` ~torch Tensor ` \s copies never alias optimizer s ` ` group lr ` ` \s note If current epoch appears attr ` milestones ` ` ` n ` ` times we scale attr ` gamma ` power ` ` n ` ` _warn_get_lr_called_within_step last_epoch milestones _param_groups_val_list optimizer lr group lr gamma milestones last_epoch group optimizer param_groups _get_closed_form_lr r Compute learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` attr ` last_epoch ` using closed-form formula Uses attr ` base_lrs ` compute learning rates This method called when epoch passed meth ` step ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s milestones = sorted milestones elements base_lr gamma bisect_right milestones last_epoch base_lr base_lrs ConstantLR LRScheduler Multiply learning rate each parameter group small constant factor The multiplication done until number epoch reaches pre-defined milestone total_iters Notice such multiplication small constant factor can happen simultaneously other changes learning rate outside scheduler When last_epoch=- sets initial lr lr Args optimizer Optimizer Wrapped optimizer factor float The number we multiply learning rate until milestone Default total_iters int The number steps scheduler multiplies learning rate factor Default last_epoch int The index last epoch Default - Example xdoctest +SKIP Assuming optimizer uses lr = all groups lr = epoch == lr = epoch == lr = epoch == lr = epoch == lr = epoch = scheduler = ConstantLR optimizer factor= total_iters= epoch range train validate scheduler step image scripts lr_scheduler_images ConstantLR png __init__ optimizer Optimizer factor float = total_iters int = last_epoch int = - - None noqa D factor factor raise ValueError Constant multiplicative factor expected between factor = factor total_iters = total_iters super __init__ optimizer last_epoch override get_lr - list float &#124; Tensor r Compute next learning rate each optimizer s attr ` ~torch optim Optimizer param_groups ` When attr ` last_epoch ` method scales ` ` group lr ` ` \s each optimizer s attr ` ~torch optim Optimizer param_groups ` attr ` factor ` Once attr ` total_iters ` reached undoes scaling ` ` factor ` ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s note If you re trying inspect most recent learning rate use meth ` get_last_lr ` instead note The returned ` ~torch Tensor ` \s copies never alias optimizer s ` ` group lr ` ` \s _warn_get_lr_called_within_step last_epoch == group lr factor group optimizer param_groups last_epoch = total_iters _param_groups_val_list optimizer lr group lr factor group optimizer param_groups _get_closed_form_lr r Compute learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` attr ` last_epoch ` using closed-form formula Uses attr ` base_lrs ` compute learning rates This method called when epoch passed meth ` step ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s base_lr factor + last_epoch = total_iters - factor base_lr base_lrs LinearLR LRScheduler Decays learning rate each parameter group linearly changing small multiplicative factor The multiplication done until number epoch reaches pre-defined milestone total_iters Notice such decay can happen simultaneously other changes learning rate outside scheduler When last_epoch=- sets initial lr lr Args optimizer Optimizer Wrapped optimizer start_factor float The number we multiply learning rate first epoch The multiplication factor changes towards end_factor following epochs Default end_factor float The number we multiply learning rate end linear changing process Default total_iters int The number iterations multiplicative factor reaches Default last_epoch int The index last epoch Default - Example xdoctest +SKIP Assuming optimizer uses lr = all groups lr = epoch == lr = epoch == lr = epoch == lr = epoch == lr = epoch = scheduler = LinearLR optimizer start_factor= total_iters= epoch range train validate scheduler step image scripts lr_scheduler_images LinearLR png __init__ optimizer Optimizer start_factor float = end_factor float = total_iters int = last_epoch int = - - None noqa D start_factor start_factor = raise ValueError Starting multiplicative factor expected greater than less equal end_factor end_factor raise ValueError Ending multiplicative factor expected between start_factor = start_factor end_factor = end_factor total_iters = total_iters super __init__ optimizer last_epoch override get_lr - list float &#124; Tensor r Compute next learning rate each optimizer s attr ` ~torch optim Optimizer param_groups ` Scales ` ` group lr ` ` \s optimizer s attr ` ~torch optim Optimizer param_groups ` such successive steps interpolate linearly attr ` start_factor ` up attr ` end_factor ` across attr ` total_iters ` steps Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s note If you re trying inspect most recent learning rate use meth ` get_last_lr ` instead note The returned ` ~torch Tensor ` \s copies never alias optimizer s ` ` group lr ` ` \s _warn_get_lr_called_within_step last_epoch == group lr start_factor group optimizer param_groups _is_initial last_epoch total_iters _param_groups_val_list optimizer lr group lr + end_factor - start_factor total_iters start_factor + last_epoch - end_factor - start_factor group optimizer param_groups _get_closed_form_lr r Compute learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` attr ` last_epoch ` using closed-form formula Uses attr ` base_lrs ` compute learning rates This method called when epoch passed meth ` step ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s base_lr start_factor + end_factor - start_factor min total_iters last_epoch total_iters base_lr base_lrs ExponentialLR LRScheduler Decays learning rate each parameter group gamma every epoch When last_epoch=- sets initial lr lr Args optimizer Optimizer Wrapped optimizer gamma float Multiplicative factor learning rate decay last_epoch int The index last epoch Default - Example xdoctest +SKIP scheduler = ExponentialLR optimizer gamma= epoch range train validate scheduler step image scripts lr_scheduler_images ExponentialLR png __init__ optimizer Optimizer gamma float last_epoch int = - - None noqa D gamma = gamma super __init__ optimizer last_epoch override get_lr - list float &#124; Tensor r Compute next learning rate each optimizer s attr ` ~torch optim Optimizer param_groups ` Multiplies current ` ` group lr ` ` \s optimizer s attr ` ~torch optim Optimizer param_groups ` attr ` gamma ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s note If you re trying inspect most recent learning rate use meth ` get_last_lr ` instead note The returned ` ~torch Tensor ` \s copies never alias optimizer s ` ` group lr ` ` \s _warn_get_lr_called_within_step when loading checkpoint we don t want _initial_step called constructor update lr one more step ahead itself _is_initial _param_groups_val_list optimizer lr group lr gamma group optimizer param_groups _get_closed_form_lr r Compute learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` attr ` last_epoch ` using closed-form formula Uses attr ` base_lrs ` compute learning rates This method called when epoch passed meth ` step ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s base_lr gamma last_epoch base_lr base_lrs SequentialLR LRScheduler Contains list schedulers expected called sequentially during optimization process Specifically schedulers will called according milestone points which should provide exact intervals which each scheduler should called given epoch Args optimizer Optimizer Wrapped optimizer schedulers list List chained schedulers milestones list List integers reflects milestone points last_epoch int The index last epoch Default - Example xdoctest +SKIP Assuming optimizer uses lr = all groups lr = epoch == lr = epoch == lr = epoch == lr = epoch == lr = epoch == lr = epoch == scheduler = ConstantLR optimizer factor= total_iters= scheduler = ExponentialLR optimizer gamma= scheduler = SequentialLR optimizer schedulers= scheduler scheduler milestones= epoch range train validate scheduler step image scripts lr_scheduler_images SequentialLR png __init__ optimizer Optimizer schedulers list LRScheduler milestones list int last_epoch int = - - None noqa D len schedulers raise ValueError f __class__ __name__ expects least one scheduler got no scheduler scheduler_idx scheduler enumerate schedulers hasattr scheduler optimizer raise TypeError f __class__ __name__ index scheduler_idx should have ` optimizer ` its attribute isinstance scheduler ReduceLROnPlateau raise ValueError f __class__ __name__ does support ` ReduceLROnPlateau ` scheduler requires additional kwargs specified when calling ` step ` f got one index scheduler_idx given schedulers sequence optimizer = scheduler optimizer raise ValueError f __class__ __name__ expects all schedulers belong same optimizer f got scheduler scheduler __class__ __name__ index scheduler_idx has scheduler optimizer f which different optimizer __class__ __name__ len milestones = len schedulers - raise ValueError Sequential Schedulers expects number schedulers provided one more f than number milestone points got number schedulers len schedulers f number milestones equal len milestones _schedulers = schedulers _milestones = milestones last_epoch = last_epoch + optimizer = optimizer Reset learning rates back initial values group optimizer param_groups _update_param_group_val group lr group initial_lr Undo step performed other schedulers recursive_undo Perform initial step only first scheduler _schedulers _initial_step _last_lr = schedulers get_last_lr recursive_undo sched=None Recursively undo any step performed initialisation schedulers scheds = sched None sched hasattr scheds _schedulers s scheds _schedulers recursive_undo s hasattr scheds last_epoch scheds last_epoch -= step - None type ignore override Perform step last_epoch += idx = bisect_right _milestones last_epoch scheduler = _schedulers idx idx _milestones idx - == last_epoch scheduler _update_lr scheduler step _last_lr = scheduler get_last_lr override state_dict - dict str Any Return state scheduler ` dict ` It contains entry every variable ` ` __dict__ ` ` which optimizer The wrapped scheduler states will also saved state_dict = key value key value __dict__ items key optimizer _schedulers state_dict _schedulers = None len _schedulers idx s enumerate _schedulers pyrefly ignore unsupported-operation state_dict _schedulers idx = s state_dict state_dict override load_state_dict state_dict dict str Any - None Load scheduler s state Args state_dict dict scheduler state Should object returned call meth ` state_dict ` _schedulers = state_dict pop _schedulers __dict__ update state_dict Restore state_dict keys order prevent side effects https github com pytorch pytorch issues state_dict _schedulers = _schedulers idx s enumerate _schedulers _schedulers idx load_state_dict s PolynomialLR LRScheduler Decays learning rate each parameter group using polynomial function given total_iters When last_epoch=- sets initial lr lr Args optimizer Optimizer Wrapped optimizer total_iters int The number steps scheduler decays learning rate Default power float The power polynomial Default Example xdoctest +SKIP undefined vars Assuming optimizer uses lr = all groups lr = epoch == lr = epoch == lr = epoch == lr = epoch = scheduler = PolynomialLR optimizer total_iters= power= epoch range train validate scheduler step image scripts lr_scheduler_images PolynomialLR png __init__ optimizer Optimizer total_iters int = power float = last_epoch int = - - None noqa D total_iters = total_iters power = power super __init__ optimizer last_epoch override get_lr - list float &#124; Tensor r Compute next learning rate each optimizer s attr ` ~torch optim Optimizer param_groups ` Scales ` ` group lr ` ` \s optimizer s attr ` ~torch optim Optimizer param_groups ` such learning rates follow math \texttt base\_lr \cdot \left - \frac \texttt last\_epoch \texttt total\_iters \right ^\texttt power Returns current learning rates unchanged after attr ` total_iters ` reached Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s note If you re trying inspect most recent learning rate use meth ` get_last_lr ` instead note The returned ` ~torch Tensor ` \s copies never alias optimizer s ` ` group lr ` ` \s _warn_get_lr_called_within_step _is_initial last_epoch total_iters _param_groups_val_list optimizer lr decay_factor = - last_epoch total_iters - last_epoch - total_iters power group lr decay_factor group optimizer param_groups _get_closed_form_lr - list float &#124; Tensor r Compute learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` attr ` last_epoch ` using closed-form formula Uses attr ` base_lrs ` compute learning rates This method called when epoch passed meth ` step ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s base_lr - min total_iters last_epoch total_iters power base_lr base_lrs CosineAnnealingLR LRScheduler r Set learning rate each parameter group using cosine annealing schedule The learning rate updated recursively using math \eta_ t+ = \eta_ \min + \eta_t - \eta_ \min \cdot \frac + \cos\left \frac T_ cur + \pi T_ max \right + \cos\left \frac T_ cur \pi T_ max \right This implements recursive approximation closed-form schedule proposed ` SGDR Stochastic Gradient Descent Warm Restarts ` _ math \eta_t = \eta_ \min + \frac \eta_ \max - \eta_ \min \left + \cos\left \frac T_ cur \pi T_ max \right \right where - math ` \eta_t ` learning rate step math ` t ` - math ` T_ cur ` number epochs since last restart - math ` T_ max ` maximum number epochs cycle Note Although SGDR includes periodic restarts implementation performs cosine annealing without restarts so math ` T_ cur = t ` increases monotonically each call meth ` step ` Args optimizer Optimizer Wrapped optimizer T_max int Maximum number iterations eta_min float Minimum learning rate Default last_epoch int The index last epoch Default - _SGDR\ Stochastic Gradient Descent Warm Restarts https arxiv org abs Example xdoctest +SKIP num_epochs = scheduler = CosineAnnealingLR optimizer T_max=num_epochs epoch range num_epochs train validate scheduler step image scripts lr_scheduler_images CosineAnnealingLR png __init__ optimizer Optimizer T_max int eta_min float = last_epoch int = - - None noqa D T_max = T_max eta_min = eta_min super __init__ optimizer last_epoch override get_lr - list float &#124; Tensor r Compute next learning rate each optimizer s attr ` ~torch optim Optimizer param_groups ` Scales ` ` group lr ` ` \s optimizer s attr ` ~torch optim Optimizer param_groups ` such their learning rates approximate math \texttt eta\_min + \frac \texttt base\_lr - \texttt eta\_min \left + \cos\left \pi \cdot \frac \texttt last\_epoch \texttt T\_max \right \right Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s note If you re trying inspect most recent learning rate use meth ` get_last_lr ` instead note The returned ` ~torch Tensor ` \s copies never alias optimizer s ` ` group lr ` ` \s _warn_get_lr_called_within_step _is_initial _param_groups_val_list optimizer lr _step_count == last_epoch eta_min + base_lr - eta_min + math cos last_epoch math pi T_max base_lr group zip base_lrs optimizer param_groups strict=True last_epoch - - T_max T_max == group lr + base_lr - eta_min - math cos math pi T_max base_lr group zip base_lrs optimizer param_groups strict=True + math cos math pi last_epoch T_max + math cos math pi last_epoch - T_max group lr - eta_min + eta_min group optimizer param_groups _get_closed_form_lr - list float &#124; Tensor r Compute learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` attr ` last_epoch ` using closed-form formula Uses attr ` base_lrs ` compute learning rates This method called when epoch passed meth ` step ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s eta_min + base_lr - eta_min + math cos math pi last_epoch T_max base_lr base_lrs ChainedScheduler LRScheduler Chains list learning rate schedulers Takes sequence chainable learning rate schedulers calls their step functions consecutively just one call step Args schedulers sequence sequence chained schedulers optimizer Optimizer optional Wrapped optimizer Default None Example xdoctest +SKIP Assuming optimizer uses lr = all groups lr = epoch == lr = epoch == lr = epoch == lr = epoch == lr = epoch == lr = epoch == scheduler = ConstantLR optimizer factor= total_iters= scheduler = ExponentialLR optimizer gamma= scheduler = ChainedScheduler scheduler scheduler optimizer=optimizer epoch range train validate scheduler step image scripts lr_scheduler_images ChainedScheduler png __init__ schedulers Sequence LRScheduler optimizer Optional Optimizer = None - None noqa D len schedulers raise ValueError f __class__ __name__ expects least one scheduler chained got no scheduler optimizer = optimizer schedulers optimizer scheduler_idx scheduler enumerate schedulers hasattr scheduler optimizer raise TypeError f __class__ __name__ index scheduler_idx should have ` optimizer ` its attribute isinstance scheduler ReduceLROnPlateau raise ValueError f __class__ __name__ does support ` ReduceLROnPlateau ` scheduler requires additional kwargs specified when calling ` step ` f got one index scheduler_idx given schedulers sequence optimizer = scheduler optimizer raise ValueError f __class__ __name__ expects all schedulers belong same optimizer f got scheduler scheduler __class__ __name__ index scheduler_idx has scheduler optimizer f which different optimizer __class__ __name__ _schedulers = schedulers optimizer = optimizer _last_lr = _param_groups_val_list _schedulers - optimizer lr step - None type ignore override Perform step scheduler _schedulers scheduler step _last_lr = _param_groups_val_list _schedulers - optimizer lr override state_dict - dict str Any Return state scheduler ` dict ` It contains entry every variable ` ` __dict__ ` ` which optimizer The wrapped scheduler states will also saved state_dict = key value key value __dict__ items key optimizer _schedulers state_dict _schedulers = None len _schedulers idx s enumerate _schedulers pyrefly ignore unsupported-operation state_dict _schedulers idx = s state_dict state_dict override load_state_dict state_dict dict str Any - None Load scheduler s state Args state_dict dict scheduler state Should object returned call meth ` state_dict ` _schedulers = state_dict pop _schedulers __dict__ update state_dict Restore state_dict keys order prevent side effects https github com pytorch pytorch issues state_dict _schedulers = _schedulers idx s enumerate _schedulers _schedulers idx load_state_dict s ReduceLROnPlateau LRScheduler Reduce learning rate when metric has stopped improving Models often benefit reducing learning rate factor - once learning stagnates This scheduler reads metrics quantity no improvement seen patience number epochs learning rate reduced Args optimizer Optimizer Wrapped optimizer mode str One ` min ` ` max ` In ` min ` mode lr will reduced when quantity monitored has stopped decreasing ` max ` mode will reduced when quantity monitored has stopped increasing Default min factor float Factor which learning rate will reduced new_lr = lr factor Default patience int The number allowed epochs no improvement after which learning rate will reduced For example consider case having no patience ` patience = ` In first epoch baseline established always considered good there s no previous baseline In second epoch performance worse than baseline we have what considered intolerable epoch Since count intolerable epochs greater than patience level learning rate reduced end epoch From third epoch onwards learning rate continues reduced end each epoch performance worse than baseline If performance improves remains same learning rate adjusted Default threshold float Threshold measuring new optimum only focus significant changes Default e- threshold_mode str One ` rel ` ` abs ` In ` rel ` mode dynamic_threshold = best + threshold max mode best - threshold ` min ` mode In ` abs ` mode dynamic_threshold = best + threshold ` max ` mode best - threshold ` min ` mode Default rel cooldown int Number epochs wait before resuming normal operation after lr has been reduced Default min_lr float list A scalar list scalars A lower bound learning rate all param groups each group respectively Default eps float Minimal decay applied lr If difference between new old lr smaller than eps update ignored Default e- Example xdoctest +SKIP optimizer = torch optim SGD model parameters lr= momentum= scheduler = ReduceLROnPlateau optimizer min epoch range train val_loss = validate Note step should called after validate scheduler step val_loss image scripts lr_scheduler_images ReduceLROnPlateau png __init__ optimizer Optimizer mode Literal min max = min factor float = patience int = threshold float = e- threshold_mode Literal rel abs = rel cooldown int = min_lr Union list float float = eps float = e- noqa D factor = raise ValueError Factor should factor = factor Attach optimizer isinstance optimizer Optimizer raise TypeError f type optimizer __name__ Optimizer optimizer = optimizer isinstance min_lr list tuple len min_lr = len optimizer param_groups raise ValueError f expected len optimizer param_groups min_lrs got len min_lr default_min_lr = None min_lrs = list min_lr pyrefly ignore bad-assignment default_min_lr = min_lr min_lrs = min_lr len optimizer param_groups patience = patience cooldown = cooldown eps = eps last_epoch = _last_lr = _param_groups_val_list optimizer lr _init_is_better mode=mode threshold=threshold threshold_mode=threshold_mode _reset _reset Reset num_bad_epochs counter cooldown counter best = mode_worse cooldown_counter = num_bad_epochs = step metrics SupportsFloat epoch=None - None type ignore override Perform step convert ` metrics ` float case s zero-dim Tensor current = float metrics epoch None epoch = last_epoch + warnings warn EPOCH_DEPRECATION_WARNING UserWarning stacklevel= last_epoch = epoch _is_better current best best = current num_bad_epochs = num_bad_epochs += in_cooldown cooldown_counter -= num_bad_epochs = ignore any bad epochs cooldown num_bad_epochs patience _reduce_lr epoch cooldown_counter = cooldown num_bad_epochs = _last_lr = _param_groups_val_list optimizer lr _reduce_lr epoch len optimizer param_groups = len min_lrs default_min_lr None raise RuntimeError The number param groups ` optimizer ` f len optimizer param_groups differs f when ` ReduceLROnPlateau ` initialized f len min_lrs usually due new param group being added optimizer Please modify ` min_lrs ` field match length ` optimizer ` param groups pyrefly ignore bad-assignment min_lrs = default_min_lr len optimizer param_groups i param_group enumerate optimizer param_groups old_lr = float param_group lr new_lr = max old_lr factor min_lrs i old_lr - new_lr eps _update_param_group_val param_group lr new_lr property in_cooldown noqa D cooldown_counter _is_better best noqa D mode == min threshold_mode == rel rel_epsilon = - threshold best rel_epsilon mode == min threshold_mode == abs best - threshold mode == max threshold_mode == rel rel_epsilon = threshold + best rel_epsilon mode == max epsilon_mode == abs best + threshold _init_is_better mode threshold threshold_mode mode min max raise ValueError mode + mode + unknown threshold_mode rel abs raise ValueError threshold mode + threshold_mode + unknown worse value chosen mode mode == min mode_worse = inf mode == max mode_worse = -inf mode = mode threshold = threshold threshold_mode = threshold_mode override load_state_dict state_dict dict str Any - None Load scheduler s state __dict__ update state_dict _init_is_better mode=self mode threshold=self threshold threshold_mode=self threshold_mode CyclicLR LRScheduler r Sets learning rate each parameter group according cyclical learning rate policy CLR The policy cycles learning rate between two boundaries constant frequency detailed paper ` Cyclical Learning Rates Training Neural Networks ` _ The distance between two boundaries can scaled per-iteration per-cycle basis Cyclical learning rate policy changes learning rate after every batch ` step ` should called after batch has been used training This has three built-in policies put forth paper triangular A basic triangular cycle without amplitude scaling triangular A basic triangular cycle scales initial amplitude half each cycle exp_range A cycle scales initial amplitude math ` \text gamma ^ \text cycle iterations ` each cycle iteration This implementation adapted github repo ` bckenstler CLR ` _ Args optimizer Optimizer Wrapped optimizer base_lr float list Initial learning rate which lower boundary cycle each parameter group max_lr float list Upper learning rate boundaries cycle each parameter group Functionally defines cycle amplitude max_lr - base_lr The lr any cycle sum base_lr some scaling amplitude therefore max_lr may actually reached depending scaling function step_size_up int Number training iterations increasing half cycle Default step_size_down int Number training iterations decreasing half cycle If step_size_down None set step_size_up Default None mode str One triangular triangular exp_range Values correspond policies detailed above If scale_fn None argument ignored Default triangular gamma float Constant exp_range scaling function gamma cycle iterations Default scale_fn function Custom scaling policy defined single argument lambda function where = scale_fn x = all x = If specified then mode ignored Default None scale_mode str cycle iterations Defines whether scale_fn evaluated cycle number cycle iterations training iterations since start cycle Default cycle cycle_momentum bool If ` ` True ` ` momentum cycled inversely learning rate between base_momentum max_momentum Default True base_momentum float list Lower momentum boundaries cycle each parameter group Note momentum cycled inversely learning rate peak cycle momentum base_momentum learning rate max_lr Default max_momentum float list Upper momentum boundaries cycle each parameter group Functionally defines cycle amplitude max_momentum - base_momentum The momentum any cycle difference max_momentum some scaling amplitude therefore base_momentum may actually reached depending scaling function Note momentum cycled inversely learning rate start cycle momentum max_momentum learning rate base_lr Default last_epoch int The index last batch This parameter used when resuming training job Since ` step ` should invoked after each batch instead after each epoch number represents total number batches computed total number epochs computed When last_epoch=- schedule started beginning Default - Example xdoctest +SKIP optimizer = torch optim SGD model parameters lr= momentum= scheduler = torch optim lr_scheduler CyclicLR optimizer base_lr= max_lr= step_size_up= data_loader = torch utils data DataLoader epoch range batch data_loader train_batch scheduler step image scripts lr_scheduler_images CyclicLR png _Cyclical Learning Rates Training Neural Networks https arxiv org abs _bckenstler CLR https github com bckenstler CLR __init__ optimizer Optimizer base_lr Union float list float max_lr Union float list float step_size_up int = step_size_down Optional int = None mode Literal triangular triangular exp_range = triangular gamma float = scale_fn Optional Callable float float = None scale_mode Literal cycle iterations = cycle cycle_momentum bool = True base_momentum float = max_momentum float = last_epoch int = - noqa D Attach optimizer isinstance optimizer Optimizer raise TypeError f type optimizer __name__ Optimizer optimizer = optimizer base_lrs = _format_param base_lr optimizer base_lr last_epoch == - lr group zip base_lrs optimizer param_groups strict=True _update_param_group_val group lr lr max_lrs = _format_param max_lr optimizer max_lr pyrefly ignore bad-assignment step_size_up = float step_size_up step_size_down = pyrefly ignore bad-assignment float step_size_down step_size_down None step_size_up pyrefly ignore unsupported-operation total_size = step_size_up + step_size_down step_ratio = step_size_up total_size mode triangular triangular exp_range scale_fn None raise ValueError mode invalid scale_fn None mode = mode gamma = gamma _scale_fn_ref Callable float float _scale_fn_custom = scale_fn scale_mode = scale_mode _init_scale_fn cycle_momentum = cycle_momentum cycle_momentum momentum optimizer defaults betas optimizer defaults raise ValueError optimizer must support momentum beta ` cycle_momentum ` option enabled use_beta = betas optimizer defaults base_momentums = _format_param base_momentum optimizer base_momentum max_momentums = _format_param max_momentum optimizer max_momentum last_epoch == - m_momentum b_momentum group zip max_momentums base_momentums optimizer param_groups strict=True use_beta group betas = m_momentum group betas group momentum = m_momentum group max_momentum = m_momentum group base_momentum = b_momentum super __init__ optimizer last_epoch base_lrs = base_lrs _init_scale_fn _scale_fn_custom None mode == triangular _scale_fn_ref = _triangular_scale_fn scale_mode = cycle mode == triangular _scale_fn_ref = _triangular _scale_fn scale_mode = cycle mode == exp_range _scale_fn_ref = partial _exp_range_scale_fn gamma scale_mode = iterations scale_fn x - float Get scaling policy _scale_fn_custom None _scale_fn_custom x _scale_fn_ref x static method staticmethod _triangular_scale_fn x float - float staticmethod _triangular _scale_fn x float - float x - staticmethod _exp_range_scale_fn gamma float x float - float gamma x override get_lr - list float &#124; Tensor r Compute next learning rate each optimizer s attr ` ~torch optim Optimizer param_groups ` Advances each ` ` group lr ` ` optimizer s attr ` ~torch optim Optimizer param_groups ` along cycle between group s ` ` base_lr ` ` ` ` max_lr ` ` using meth ` scale_fn ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s note If you re trying inspect most recent learning rate use meth ` get_last_lr ` instead note The returned ` ~torch Tensor ` \s copies never alias optimizer s ` ` group lr ` ` \s note This method treats attr ` last_epoch ` index previous batch note When attr ` cycle_momentum ` ` ` True ` ` method has side effect updating optimizer s momentum _warn_get_lr_called_within_step cycle = math floor + last_epoch total_size x = + last_epoch total_size - cycle x = step_ratio scale_factor = x step_ratio scale_factor = x - step_ratio - lrs = base_lr max_lr zip base_lrs max_lrs strict=True base_height = max_lr - base_lr scale_factor scale_mode == cycle lr = base_lr + base_height scale_fn cycle lr = base_lr + base_height scale_fn last_epoch lrs append lr cycle_momentum momentums = base_momentum max_momentum zip base_momentums max_momentums strict=True base_height = max_momentum - base_momentum scale_factor scale_mode == cycle momentum = max_momentum - base_height scale_fn cycle momentum = max_momentum - base_height scale_fn last_epoch momentums append momentum param_group momentum zip optimizer param_groups momentums strict=True use_beta param_group betas = momentum param_group betas param_group momentum = momentum lrs override state_dict - dict str Any noqa D Return state scheduler ` dict ` It contains entry every variable ` ` __dict__ ` ` which optimizer The learning rate lambda functions will only saved they callable objects they functions lambdas When saving loading scheduler please make sure also save load state optimizer state = super state_dict We dropping ` _scale_fn_ref ` attribute because ` weakref WeakMethod ` can t pickled state pop _scale_fn_ref None fn = state pop _scale_fn_custom state _scale_fn_custom = None fn None isinstance fn types FunctionType The _scale_fn_custom will only saved callable object function lambda state _scale_fn_custom = fn __dict__ copy state override load_state_dict state_dict dict str Any - None Load scheduler s state fn = state_dict pop _scale_fn_custom super load_state_dict state_dict fn None _scale_fn_custom __dict__ update fn _init_scale_fn CosineAnnealingWarmRestarts LRScheduler r Set learning rate each parameter group using cosine annealing schedule The math ` \eta_ max ` set initial lr math ` T_ cur ` number epochs since last restart math ` T_ i ` number epochs between two warm restarts SGDR math \eta_t = \eta_ min + \frac \eta_ max - \eta_ min \left + \cos\left \frac T_ cur T_ i \pi\right \right When math ` T_ cur =T_ i ` set math ` \eta_t = \eta_ min ` When math ` T_ cur = ` after restart set math ` \eta_t=\eta_ max ` It has been proposed ` SGDR Stochastic Gradient Descent Warm Restarts ` _ Args optimizer Optimizer Wrapped optimizer T_ int Number iterations until first restart T_mult int optional A factor which math ` T_ i ` increases after restart Default eta_min float optional Minimum learning rate Default last_epoch int optional The index last epoch Default - _SGDR\ Stochastic Gradient Descent Warm Restarts https arxiv org abs Example xdoctest +SKIP optimizer = torch optim SGD model parameters lr= scheduler = torch optim lr_scheduler CosineAnnealingWarmRestarts optimizer T_ = epoch range train validate scheduler step image scripts lr_scheduler_images CosineAnnealingWarmRestarts png __init__ optimizer Optimizer T_ int T_mult int = eta_min float = last_epoch int = - noqa D T_ = isinstance T_ int raise ValueError f Expected positive integer T_ got T_ T_mult isinstance T_mult int raise ValueError f Expected integer T_mult = got T_mult isinstance eta_min float int raise ValueError f Expected float int eta_min got eta_min type type eta_min T_ = T_ T_i = T_ T_mult = T_mult eta_min = eta_min T_cur = last_epoch super __init__ optimizer last_epoch override get_lr - list float &#124; Tensor r Compute next learning rate each optimizer s attr ` ~torch optim Optimizer param_groups ` Computes learning rates optimizer s attr ` ~torch optim Optimizer param_groups ` following math \texttt eta\_min + \frac \texttt base\_lr - \texttt eta\_min \left + \cos\left \pi \cdot \frac \texttt T\_cur \texttt T\_i \right \right Where attr ` T_cur ` number epochs since last restart attr ` T_i ` number epochs between two restarts Both attr ` T_cur ` attr ` T_i ` updated meth ` step ` attr ` T_i ` becomes attr ` T_mult ` times larger after each restart Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s note If you re trying inspect most recent learning rate use meth ` get_last_lr ` instead note The returned ` ~torch Tensor ` \s copies never alias optimizer s ` ` group lr ` ` \s _warn_get_lr_called_within_step eta_min + base_lr - eta_min + math cos math pi T_cur T_i base_lr base_lrs override step epoch=None - None Step could called after every batch update Example xdoctest +SKIP Undefined vars scheduler = CosineAnnealingWarmRestarts optimizer T_ T_mult iters = len dataloader epoch range i sample enumerate dataloader inputs labels = sample inputs sample labels optimizer zero_grad outputs = net inputs loss = criterion outputs labels loss backward optimizer step scheduler step epoch + i iters This function can called interleaved way Example xdoctest +SKIP Undefined vars scheduler = CosineAnnealingWarmRestarts optimizer T_ T_mult epoch range scheduler step scheduler step scheduler step scheduler step instead scheduler epoch None last_epoch epoch = epoch None epoch = last_epoch + T_cur = T_cur + T_cur = T_i T_cur = T_cur T_i T_i = T_i T_mult epoch raise ValueError f Expected non-negative epoch got epoch epoch = T_ T_mult == T_cur = epoch T_ n = int math log epoch T_ T_mult - + T_mult T_cur = epoch - T_ T_mult n - T_mult - T_i = T_ T_mult n T_i = T_ T_cur = epoch last_epoch = math floor epoch _enable_get_lr_call param_group lr zip optimizer param_groups get_lr strict=True _update_param_group_val param_group lr lr _last_lr = _param_groups_val_list optimizer lr _SchedulePhase TypedDict end_step float start_lr str end_lr str start_momentum str end_momentum str OneCycleLR LRScheduler r Sets learning rate each parameter group according cycle learning rate policy The cycle policy anneals learning rate initial learning rate some maximum learning rate then maximum learning rate some minimum learning rate much lower than initial learning rate This policy initially described paper ` Super-Convergence Very Fast Training Neural Networks Using Large Learning Rates ` _ The cycle learning rate policy changes learning rate after every batch ` step ` should called after batch has been used training This scheduler chainable Note also total number steps cycle can determined one two ways listed order precedence A value total_steps explicitly provided A number epochs epochs number steps per epoch steps_per_epoch provided In case number total steps inferred total_steps = epochs steps_per_epoch You must either provide value total_steps provide value both epochs steps_per_epoch The default behaviour scheduler follows fastai implementation cycle which claims unpublished work has shown even better results using only two phases To mimic behaviour original paper instead set ` ` three_phase=True ` ` Args optimizer Optimizer Wrapped optimizer max_lr float list Upper learning rate boundaries cycle each parameter group total_steps int The total number steps cycle Note value provided here then must inferred providing value epochs steps_per_epoch Default None epochs int The number epochs train This used along steps_per_epoch order infer total number steps cycle value total_steps provided Default None steps_per_epoch int The number steps per epoch train This used along epochs order infer total number steps cycle value total_steps provided Default None pct_start float The percentage cycle number steps spent increasing learning rate Default anneal_strategy str cos linear Specifies annealing strategy cos cosine annealing linear linear annealing Default cos cycle_momentum bool If ` ` True ` ` momentum cycled inversely learning rate between base_momentum max_momentum Default True base_momentum float list Lower momentum boundaries cycle each parameter group Note momentum cycled inversely learning rate peak cycle momentum base_momentum learning rate max_lr Default max_momentum float list Upper momentum boundaries cycle each parameter group Functionally defines cycle amplitude max_momentum - base_momentum Note momentum cycled inversely learning rate start cycle momentum max_momentum learning rate base_lr Default div_factor float Determines initial learning rate via initial_lr = max_lr div_factor Default final_div_factor float Determines minimum learning rate via min_lr = initial_lr final_div_factor Default e three_phase bool If ` ` True ` ` use third phase schedule annihilate learning rate according final_div_factor instead modifying second phase first two phases will symmetrical about step indicated pct_start last_epoch int The index last batch This parameter used when resuming training job Since ` step ` should invoked after each batch instead after each epoch number represents total number batches computed total number epochs computed When last_epoch=- schedule started beginning Default - Example xdoctest +SKIP data_loader = torch utils data DataLoader optimizer = torch optim SGD model parameters lr= e- momentum= scheduler = torch optim lr_scheduler OneCycleLR optimizer max_lr= steps_per_epoch=len data_loader epochs= epoch range batch data_loader train_batch optimizer step scheduler step image scripts lr_scheduler_images OneCycleLR png _Super-Convergence\ Very Fast Training Neural Networks Using Large Learning Rates https arxiv org abs __init__ optimizer Optimizer max_lr Union float list float total_steps Optional int = None epochs Optional int = None steps_per_epoch Optional int = None pct_start float = anneal_strategy Literal cos linear = cos cycle_momentum bool = True base_momentum Union float list float = max_momentum Union float list float = div_factor float = final_div_factor float = e three_phase bool = False last_epoch int = - noqa D Validate optimizer isinstance optimizer Optimizer raise TypeError f type optimizer __name__ Optimizer optimizer = optimizer Validate total_steps total_steps None total_steps = isinstance total_steps int raise ValueError f Expected positive integer total_steps got total_steps total_steps = total_steps epochs None steps_per_epoch None isinstance epochs int epochs = raise ValueError f Expected positive integer epochs got epochs isinstance steps_per_epoch int steps_per_epoch = raise ValueError f Expected positive integer steps_per_epoch got steps_per_epoch total_steps = epochs steps_per_epoch raise ValueError You must define either total_steps OR epochs AND steps_per_epoch _schedule_phases list _SchedulePhase three_phase _schedule_phases = end_step float pct_start total_steps - start_lr initial_lr end_lr max_lr start_momentum max_momentum end_momentum base_momentum end_step float pct_start total_steps - start_lr max_lr end_lr initial_lr start_momentum base_momentum end_momentum max_momentum end_step total_steps - start_lr initial_lr end_lr min_lr start_momentum max_momentum end_momentum max_momentum _schedule_phases = end_step float pct_start total_steps - start_lr initial_lr end_lr max_lr start_momentum max_momentum end_momentum base_momentum end_step total_steps - start_lr max_lr end_lr min_lr start_momentum base_momentum end_momentum max_momentum Validate pct_start pct_start pct_start isinstance pct_start float raise ValueError f Expected float between pct_start got pct_start Validate anneal_strategy anneal_strategy cos linear raise ValueError f anneal_strategy must one cos linear instead got anneal_strategy _anneal_func_type = anneal_strategy Initialize learning rate variables max_lrs = _format_param max_lr optimizer max_lr last_epoch == - idx group enumerate optimizer param_groups group initial_lr = max_lrs idx div_factor group max_lr = max_lrs idx group min_lr = group initial_lr final_div_factor Initialize momentum variables cycle_momentum = cycle_momentum cycle_momentum momentum optimizer defaults betas optimizer defaults raise ValueError optimizer must support momentum beta ` cycle_momentum ` option enabled use_beta = betas optimizer defaults max_momentums = _format_param max_momentum optimizer max_momentum base_momentums = _format_param base_momentum optimizer base_momentum last_epoch == - m_momentum b_momentum group zip max_momentums base_momentums optimizer param_groups strict=True use_beta group betas = m_momentum group betas group momentum = m_momentum group max_momentum = m_momentum group base_momentum = b_momentum super __init__ optimizer last_epoch _anneal_func args kwargs hasattr _anneal_func_type _anneal_func_type == cos _annealing_cos args kwargs _anneal_func_type == linear _annealing_linear args kwargs raise ValueError f Unknown _anneal_func_type _anneal_func_type For BC anneal_func args kwargs type ignore attr-defined staticmethod _annealing_cos start end pct Cosine anneal ` start ` ` end ` pct goes cos_out = math cos math pi pct + end + start - end cos_out staticmethod _annealing_linear start end pct Linearly anneal ` start ` ` end ` pct goes end - start pct + start override get_lr - list float &#124; Tensor r Compute next learning rate each optimizer s attr ` ~torch optim Optimizer param_groups ` Finds appropriate attr ` _schedule_phases ` entry current step interpolates between its ` ` start_lr ` ` ` ` end_lr ` ` using meth ` _anneal_func ` Returns list float &#124; Tensor A ` list ` learning rates each optimizer s attr ` ~torch optim Optimizer param_groups ` same types their current ` ` group lr ` ` \s note If you re trying inspect most recent learning rate use meth ` get_last_lr ` instead note The returned ` ~torch Tensor ` \s copies never alias optimizer s ` ` group lr ` ` \s note When attr ` cycle_momentum ` ` ` True ` ` method has side effect updating optimizer s momentum _warn_get_lr_called_within_step lrs = step_num = last_epoch step_num total_steps raise ValueError f Tried step step_num times The specified number total steps total_steps group optimizer param_groups start_step = i phase enumerate _schedule_phases end_step = phase end_step step_num = end_step i == len _schedule_phases - pct = step_num - start_step end_step - start_step computed_lr = _anneal_func group phase start_lr group phase end_lr pct cycle_momentum computed_momentum = _anneal_func group phase start_momentum group phase end_momentum pct break start_step = phase end_step lrs append computed_lr type ignore possibly-undefined cycle_momentum use_beta group betas = computed_momentum group betas type ignore possibly-undefined group momentum = computed_momentum type ignore possibly-undefined lrs