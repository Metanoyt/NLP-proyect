mypy disallow-untyped-defs __future__ annotations dataclasses enum Enum typing Any Callable Optional TYPE_CHECKING Union torch torch _dynamo utils counters get_metrics_context torch _inductor utils GraphPartitionMap InputType torch utils _ordered_set OrderedSet utils is_using_cudagraph_partition TYPE_CHECKING collections abc Sequence Set AbstractSet perf_hint_log = torch _logging getArtifactLogger __name__ perf_hints static_inputs_log = torch _logging getArtifactLogger __name__ cudagraph_static_inputs OutputType = list Optional Union int torch Tensor ModelType = Callable list InputType OutputType dataclasses dataclass frozen=True FunctionID Unique counter function wrapped cudagraphify_impl id int dataclasses dataclass frozen=True PlaceholderInfo A serializable version torch fx Node contains information pertinent placeholder stack traces We use these logging error messages related cudagraphs will cache these results name str stack_trace Optional str This field recursive never cyclic since node never uses itself users list PlaceholderInfo mutating_use_stack_trace Optional str dataclasses dataclass frozen=True WrappedFunction Represents function you want record CUDA graph replay little more metadata so we can identify we have applicable CUDA graph our CUDA graph tree model Callable Any static_input_idxs Sequence int id FunctionID constants tuple torch Tensor placeholders Sequence PlaceholderInfo mutated_input_idxs Sequence int get_mutating_use_stack_trace_from_node placeholder_node torch fx Node - Optional str reinplaced uses might have single non-copy_ use len placeholder_node users == next iter placeholder_node users meta get stack_trace None use placeholder_node users use target torch ops aten copy_ default stack_trace = use meta get stack_trace None stack_trace None get_mutating_use_stack_trace placeholder_info PlaceholderInfo - Optional str placeholder_info mutating_use_stack_trace to_placeholder_info placeholder_node torch fx Node - PlaceholderInfo name = placeholder_node name stack_trace = placeholder_node meta get stack_trace None users = mutating_use_stack_trace = None Only recurse users once since we only care about user s stack traces placeholder_node op == placeholder users = to_placeholder_info i i placeholder_node users mutating_use_stack_trace = get_mutating_use_stack_trace_from_node placeholder_node PlaceholderInfo name stack_trace users mutating_use_stack_trace get_placeholder_info graph torch fx Graph - list PlaceholderInfo to_placeholder_info node node graph nodes node op == placeholder format_default_skip_message reason str - str f skipping cudagraphs due reason get_mutation_stack_trace placeholders Sequence PlaceholderInfo mutation_indices Union AbstractSet int Sequence int - str stack_trace Optional str = idx mutation_indices placeholder = placeholders idx stack_trace = get_mutating_use_stack_trace placeholder break msg = format_default_skip_message f mutated inputs len mutation_indices instances stack_trace f msg Found \n stack_trace msg check_for_mutation func WrappedFunction inputs list InputType is_cuda_graph_recorded_tensor Callable torch Tensor bool - Optional str doesn t work non-trees because warmup run would apply mutation twice torch _inductor config triton cudagraph_trees checking mutation only parameters static inputs mutation_indices Sequence int = idx idx func mutated_input_idxs idx func static_input_idxs is_cuda_graph_recorded_tensor inputs idx type ignore arg-type mutation_indices = func mutated_input_idxs static_inputs_log debug check mutation static input indices s func static_input_idxs static_inputs_log debug check mutation mutation indices s mutation_indices get_mutation_stack_trace func placeholders mutation_indices mutation_indices None _get_use_stack_trace node torch fx Node - Optional str use node users stack_trace = use meta get stack_trace None stack_trace None check_multiple_devices_or_any_cpu_nodes device_node_mapping dict torch device torch fx Node - Optional str meta tensors supported since there no compute device_node_mapping pop torch device meta None dynamo cudagraph does support graph partition is_using_cudagraph_partition graph partition supports splitting cpu op So we can ignore cpu nodes device_node_mapping pop torch device cpu None cpu_node = device_node_mapping get torch device cpu msg = f cpu device cpu_node name stack_trace = _get_use_stack_trace cpu_node format_default_skip_message f msg Found \n stack_trace format_default_skip_message msg len device_node_mapping == next iter device_node_mapping keys type == cuda None keys_repr = repr key key device_node_mapping keys format_default_skip_message f multiple devices join keys_repr check_lowering_disable_cudagraph device_node_mapping dict torch device torch fx Node - Optional str check_multiple_devices_or_any_cpu_nodes device_node_mapping log_cudagraph_skip_and_bump_counter msg str - None perf_hint_log warning msg counters inductor cudagraph_skips += torch _inductor config triton cudagraph_or_error raise RuntimeError msg metrics_context = get_metrics_context metrics_context in_progress metrics_context set cudagraph_skip_reason msg overwrite=True dataclasses dataclass BoxedDeviceIndex value Optional int set device_idx Optional int - None assert device_idx None isinstance device_idx int value = device_idx check_for_mutation_ignore_cuda_graph_managed_tensor gm torch fx GraphModule mutated_inputs OrderedSet str mutated_input_idxs OrderedSet int static_input_idxs Sequence int - Optional str default_msg = format_default_skip_message mutated inputs doesn t work non-trees because warmup run would apply mutation twice torch _inductor config triton cudagraph_trees unique_idxs = OrderedSet static_input_idxs checking mutation only parameters static inputs mutation_indices = idx idx mutated_input_idxs idx unique_idxs has_mutation = len mutation_indices = has_mutation None placeholders = get_placeholder_info gm graph get_mutation_stack_trace placeholders mutation_indices has_mutation = len mutated_inputs = None has_mutation default_msg get_placeholder_stack_trace placeholder PlaceholderInfo - Optional str Gets first non-empty stack trace placeholder its users placeholder stack_trace placeholder stack_trace user placeholder users user stack_trace user stack_trace None CheckInvariantStatus Enum Check invariant succeeded SUCCESS = Previously managed data pointers stable CudagraphManagedIdxMismatch = Static tensor input addresses stable StaticInputIdxMismatch = Expected dead indices before graph live ExpectedDeadIndicesBeforeGraphMismatch = __str__ - str name == CudagraphManagedIdxMismatch cudagraph managed tensor data pointer changed name == StaticInputIdxMismatch static input data pointer changed name == ExpectedDeadIndicesBeforeGraphMismatch expected dead indices before graph live f name value log_data_ptr_mismatch placeholders Sequence PlaceholderInfo inputs list InputType recorded_data_ptr Sequence Optional int target_idxs Sequence int mismatch CheckInvariantStatus - str Logs mismatch between input data pointers recorded data pointers This checks only idxs target_idxs assert len inputs == len recorded_data_ptr len inputs == len placeholders length mismatch between inputs recorded_data_ptr placeholders t_tensors = inputs i i target_idxs t_data_ptrs = recorded_data_ptr i i target_idxs error_msg = f mismatch \n i tensor data_ptr enumerate zip t_tensors t_data_ptrs assert isinstance tensor torch Tensor index = target_idxs i tensor data_ptr = data_ptr placeholder = placeholders index error_msg = f error_msg input name placeholder name f data pointer changed data_ptr tensor data_ptr f input stack trace get_placeholder_stack_trace placeholder \n error_msg maybe_warning_due_to_dynamic_shape fn_cache dict tuple int Callable Any new_int_key Any - bool num_cudagraphs = len fn_cache keys + warn_msg - str CUDAGraph supports dynamic shapes recording new graph each distinct input size Recording too many CUDAGraphs may lead f extra overhead We have observed num_cudagraphs distinct sizes Please consider following options better performance padding inputs few fixed number shapes b set torch _inductor config triton cudagraph_skip_dynamic_graphs=True Set torch _inductor config triton cudagraph_dynamic_shape_warn_limit=None silence warning torch _inductor config triton cudagraph_dynamic_shape_warn_limit num_cudagraphs torch _inductor config triton cudagraph_dynamic_shape_warn_limit perf_hint_log warning warn_msg True False dataclasses dataclass frozen=True CudagraphCachedInfo Info needed realign inputs placeholders Sequence PlaceholderInfo stack_traces list Optional str cudagraph_fail_reasons list str dataclasses dataclass frozen=True CudagraphMetadata Metadata recording CUDA graph placeholders Sequence PlaceholderInfo static_input_idxs OrderedSet int mutated_input_idxs OrderedSet int stack_traces list Optional str constants dict str torch Tensor get_partition_cudagraph_metadata partition_map GraphPartitionMap metadata CudagraphMetadata - CudagraphMetadata Convert cudagraph metadata graph level graph partition level given graph partition info i e mapping partition input output index graph input output index partition_placeholders = partition_static_input_idxs OrderedSet int = OrderedSet partition_mutated_input_idxs OrderedSet int = OrderedSet partition_input_idx graph_input_idx enumerate partition_map input_index_mapping graph_input_idx metadata static_input_idxs partition_static_input_idxs add partition_input_idx graph_input_idx metadata mutated_input_idxs partition_mutated_input_idxs add partition_input_idx graph_input_idx None placeholder = metadata placeholders graph_input_idx create dummy placeholder info since partition input graph input placeholder = PlaceholderInfo name=f partition_ partition_map id _placeholder_ partition_input_idx stack_trace=None users= mutating_use_stack_trace=None partition_placeholders append placeholder partition_stack_traces = graph_output_idx partition_map output_index_mapping graph_output_idx None partition_stack_traces append metadata stack_traces graph_output_idx partition_stack_traces append None partition_constants = name metadata constants name name partition_map constant_names CudagraphMetadata partition_placeholders partition_static_input_idxs partition_mutated_input_idxs partition_stack_traces partition_constants