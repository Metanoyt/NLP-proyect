Owner s module inductor ruff noqa F contextlib functools gc importlib itertools re sys unittest warnings collections defaultdict collections abc Mapping Sequence torch torch _dynamo config dynamo_config torch nn nn torch _dynamo backends debugging aot_eager_decomp_partition_with_mode torch _dynamo utils counters torch _functorch _aot_autograd autograd_cache AOTAutogradCache torch _inductor config torch _inductor codecache FxGraphCache torch _inductor compile_fx compile_fx_inner torch _inductor cudagraph_trees cudagraphify_impl tree_cudagraphify_impl torch _inductor cudagraph_utils FunctionID torch _inductor test_case TestCase InductorTestCase torch _inductor utils run_and_get_code torch _ops OpOverload torch fx experimental proxy_tensor make_fx torch fx immutable_collections immutable_dict torch testing FileCheck torch testing _internal common_cuda TEST_MULTIGPU torch testing _internal common_utils instantiate_parametrized_tests IS_ARM IS_CI IS_LINUX IS_WINDOWS IS_X parametrize skipIfRocm TEST_CUDA_GRAPH torch testing _internal inductor_utils HAS_CUDA_AND_TRITON torch utils _mode_utils no_dispatch torch utils _python_dispatch TorchDispatchMode IS_WINDOWS IS_CI sys stderr write Windows CI does have necessary dependencies test_torchinductor yet\n __name__ == __main__ sys exit raise unittest SkipTest requires sympy functorch filelock importlib import_module functorch importlib import_module filelock aten = torch ops aten requires_multigpu = functools partial unittest skipIf TEST_MULTIGPU requires multiple cuda devices io StringIO get_compile_fn backend backend == cudagraphs functools partial torch compile backend= cudagraphs functools partial torch compile mode= reduce-overhead capture_stderr list Replace sys stderr temporary StringIO __enter__ sys_stderr = sys stderr stringio = StringIO sys stderr = stringio __exit__ args append str stringio getvalue del stringio sys stderr = sys_stderr cdata t t untyped_storage _cdata TestCase InductorTestCase classmethod setUpClass cls super setUpClass cls _stack = contextlib ExitStack cls _stack enter_context config patch debug True cpp min_chunk_size triton autotune_pointwise False too slow implicit_fallbacks False classmethod tearDownClass cls cls _stack close super tearDownClass setUp torch _dynamo reset super setUp tearDown super tearDown torch _dynamo reset HAS_CUDA_AND_TRITON get_all_cudagraph_segments segments = torch cuda memory_snapshot segment segment segments segment segment_pool_id = all_live_blocks blocks_addrs = segment get_all_cudagraph_segments addr = segment address block segment blocks block state == active_allocated blocks_addrs append addr addr += block size blocks_addrs all_live_block_count len all_live_blocks CudaGraphTreeTests TestCase setUp super setUp graph_stack = contextlib ExitStack graph_stack enter_context config patch triton cudagraphs True triton cudagraph_trees True triton fast_path_cudagraph_asserts True too slow triton slow_path_cudagraph_asserts True graph_stack enter_context dynamo_config patch automatic_dynamic_shapes=True device_idx = torch rand device= cuda device index warnings filterwarnings ignore tearDown super tearDown torch _dynamo reset gc collect torch cuda empty_cache graph_stack close assertIsNone get_manager assertEqual all_live_block_count assertEqual len get_all_cudagraph_segments warnings resetwarnings get_manager device_index=None torch _inductor cudagraph_trees get_container device_index device_index device_idx tree_manager get_roots get_manager get_roots curr_node get_manager current_node get_root_children root num_descendants root get_roots cudagraphify_impl args is_inference=True is_backward=False kwargs tree_cudagraphify_impl args kwargs device_index=self device_idx is_inference=is_inference is_backward=is_backward staticmethod run_twc fn args kwargs fn args kwargs fn args kwargs num_checkpoints get_manager debug_checkpointing_counter test_run_simple foo x x x x foo_opt = torch compile foo ones = torch ones device= cuda zeros = torch zeros device= cuda run_twc foo_opt ones run_twc foo_opt zeros assertEqual get_root_children check_rng torch compile mode= reduce-overhead foo torch rand torch manual_seed out = foo out = foo out = foo torch manual_seed assertEqual out foo assertEqual out foo assertEqual out foo torch _inductor config patch fallback_random True test_rng_trees check_rng torch _inductor config patch triton cudagraph_trees False torch _inductor config patch fallback_random True test_rng_non_trees check_rng test_mutation_reinplaced torch nn nn Model nn Module __init__ - None super __init__ forward input other out input = torch logical_xor input=input other=other out=out input x = torch rand dtype=torch float cuda y = torch rand dtype=torch float cuda z = torch rand dtype=torch float cuda model = Model cuda eag = model x y z capture_stderr captured_output opt = torch compile model forward mode= reduce-overhead x y z FileCheck check skipping cudagraphs due mutated inputs instances Found check torch logical_xor run captured_output assertEqual counters inductor cudagraph_skips requires_multigpu parametrize backend inductor cudagraphs test_multiple_devices_msg backend foo x y x + y + foo = get_compile_fn backend foo capture_stderr captured_output foo torch ones device= cuda torch ones torch _inductor config graph_partition graph partition splits cpu ops assertEqual counters inductor cudagraph_skips FileCheck check skipping cudagraphs due cpu device arg _ Found check y + run captured_output assertEqual counters inductor cudagraph_skips capture_stderr captured_output foo torch ones device= cuda torch ones device= cuda FileCheck check skipping cudagraphs due multiple devices run captured_output assertEqual counters inductor cudagraph_skips torch _inductor config graph_partition torch _inductor config patch triton cudagraph_skip_dynamic_graphs True test_skip_symbolic torch compile dynamic=True foo x y x + y capture_stderr captured_output foo torch rand device= cuda torch rand device= cuda FileCheck check skipping cudagraphs due graph symbolic shapes inputs check x + y run captured_output assertEqual counters inductor cudagraph_skips parametrize backend inductor cudagraphs torch _dynamo config patch cudagraph_backend_keep_input_mutation True torch _dynamo config patch cudagraph_backend_support_input_mutation True torch _inductor config patch triton cudagraph_support_input_mutation True test_mutation_on_inp backend foo x x add_ x foo = get_compile_fn backend foo inp torch ones device= cuda capture_stderr captured_output foo inp FileCheck check skipping cudagraphs due mutated inputs instances Found check add_ run captured_output assertEqual counters inductor cudagraph_skips mutation inp doesn t hit cudagraphs assertEqual len get_manager roots mutation parameters buffers hits cudagraphs Mod torch nn Module __init__ - None super __init__ buf = torch ones device= cuda forward x buf add_ x buf + x foo mod x mod x foo = get_compile_fn backend foo mod = Mod mod = Mod _ range assertEqual foo mod inp mod inp assertEqual mod buf mod buf assertIsNotNone get_manager parametrize backend inductor cudagraphs torch _dynamo config patch cudagraph_backend_keep_input_mutation True torch _dynamo config patch cudagraph_backend_support_input_mutation False torch _inductor config patch triton cudagraph_support_input_mutation False test_mutation_cudagraph_managed_tensors_config backend foo x x + mut x x add_ x non_mut x x add mut = get_compile_fn backend mut foo = get_compile_fn backend foo capture_stderr captured_output _ range torch compiler cudagraph_mark_step_begin inp = torch rand device= cuda tmp = foo inp mut_out = mut tmp assertEqual mut_out non_mut foo inp FileCheck check_count skipping cudagraphs due mutated inputs instances Found exactly=True run captured_output parametrize backend inductor cudagraphs torch _dynamo config patch cudagraph_backend_keep_input_mutation True torch _dynamo config patch cudagraph_backend_support_input_mutation True torch _inductor config patch triton cudagraph_support_input_mutation True test_mutation_cudagraph_managed_tensors backend foo x x + mut x x add_ x non_mut x x add mut = get_compile_fn backend mut foo = get_compile_fn backend foo capture_stderr captured_output _ range torch compiler cudagraph_mark_step_begin inp = torch rand device= cuda tmp = foo inp mut_out = mut tmp assertEqual mut_out non_mut foo inp FileCheck check_count skipping cudagraphs due mutated inputs instances Found exactly=True run captured_output assertTrue cudagraph_skips counters inductor torch compiler cudagraph_mark_step_begin inp = torch rand device= cuda tmp = foo inp mut_inp = tmp clone case what previously mutated cudagraph managed tensor no longer now its input eager we should fallback inductor without cudagraphs capture_stderr captured_output mut mut_inp FileCheck check skipping cudagraphs due mutated inputs instances Found check x add_ run captured_output assertEqual mut_inp non_mut foo inp assertEqual counters inductor cudagraph_skips parametrize backend inductor cudagraphs torch _dynamo config patch cudagraph_backend_keep_input_mutation True torch _dynamo config patch cudagraph_backend_support_input_mutation True torch _inductor config patch triton cudagraph_support_input_mutation True test_mutation_cudagraph_managed_tensor_warn backend foo x x add_ fee y z z add inp torch rand device= cuda foo = get_compile_fn backend foo fee = get_compile_fn backend fee capture_stderr captured_output _ range torch compiler cudagraph_mark_step_begin fee inp foo inp FileCheck check_count skipping cudagraphs due mutated inputs instances Found exactly=True run captured_output assertEqual counters inductor cudagraph_skips parametrize backend inductor cudagraphs torch _dynamo config patch cudagraph_backend_keep_input_mutation True torch _dynamo config patch cudagraph_backend_support_input_mutation True torch _inductor config patch triton cudagraph_support_input_mutation True test_mutation_cudagraph_managed_tensor_warn_only_once backend foo x x + mut x x add_ x inp torch rand device= cuda mut = get_compile_fn backend mut foo = get_compile_fn backend foo capture_stderr captured_output Should warn current_node=None mut inp _ range torch compiler cudagraph_mark_step_begin tmp = foo inp mut tmp should warn mut_inp = tmp clone mut mut_inp should warn since mut has warned FileCheck check_count skipping cudagraphs due mutated inputs instances Found exactly=True run captured_output assertEqual counters inductor cudagraph_skips test_index_put fn x y z x = torch zeros_like x x index_put_ y z True fn_c = torch compile mode= reduce-overhead fn i range args x = torch zeros dtype=torch bool device= cuda y = torch arange dtype=torch int device= cuda z = torch ones dtype=torch bool device= cuda x y z i == out code = run_and_get_code fn_c args FileCheck check aten index_put_ check_same True run code out = fn_c args assertEqual fn args out test_function_compiled_multiple_times foo x y = foo x y = foo y y + y foo x torch _dynamo graph_break x x x foo_opt = torch compile foo ones = torch ones device= cuda foo ones foo_opt ones foo_opt ones assertEqual foo_opt ones foo ones paths children = get_root_children one root two children assertEqual children test_end_recording_early foo x y = x x x torch _dynamo graph_break z = x + y z torch compile foo x x + foo_opt = torch compile foo _ range out = foo_opt torch ones device= cuda del out when I tried inducing separate recordings via graph break frame kept interfering keeping outputs alive isn t great simulates logic torch _dynamo mutation_guard GenerationTracker GenerationTracker generation -= out = foo torch ones device= cuda del out foo_opt torch ones device= cuda Two separate traces - one has child one doesn t assertEqual get_root_children test_execution_into_recording foo x y = x + x y sum y + y - foo_opt = torch compile foo inp = torch zeros dtype=torch float device= cuda assertEqual foo_opt inp foo inp assertEqual foo_opt inp foo inp inp add_ out_eager = foo inp out_warmup = foo_opt inp assertEqual out_warmup out_eager warmup should have storage deallocator hooked assertEqual all_live_block_count out_live = foo_opt inp assertEqual out_live out_eager should recording mode storage deallocator hooked assertEqual all_live_block_count warmup should have been freed del out_warmup should recording mode storage deallocator hooked assertEqual all_live_block_count del out_live assertEqual all_live_block_count out = foo_opt inp assertEqual foo inp out should execution mode assertEqual all_live_block_count test_forward_with_skipped_cudagraphed_backward torch compile mode= reduce-overhead foo x x x x _ range inp = torch rand device= cuda requires_grad=True out = foo inp config patch always_complex_memory_overlap_TESTING_ONLY=True back_inp = torch empty_strided device= cuda out backward back_inp we should have cudagraph d backwards new_id = get_manager new_graph_id id assertEqual new_id assertFalse get_manager running_forwards_with_pending_backwards torch _functorch config patch enable_autograd_cache True torch _inductor config patch fx_graph_cache True torch _inductor config patch fx_graph_remote_cache False Currently fx graph cache turned off specialize_float=False torch _dynamo config patch specialize_float True test_cache_hit_forward_miss_backward Test we don t cache cudagraphs skipping cudagraphs backward cache miss torch compile mode= reduce-overhead foo x x x x Run forwards fx graph should cache miss _ range torch _dynamo reset counters clear FxGraphCache clear AOTAutogradCache clear config patch always_complex_memory_overlap_TESTING_ONLY=True inp = torch rand device= cuda requires_grad=True out = foo inp assertEqual counters inductor fxgraph_cache_miss Reset dynamo related caches except FXGraphCache torch _dynamo reset Forwards should cache hit now we still skip cudagraphs inp = torch rand device= cuda requires_grad=True out = foo inp assertEqual counters inductor fxgraph_cache_miss assertEqual counters inductor fxgraph_cache_hit Run backward without complex memory overlap being set Run backward without complex memory overlap reason cache should miss cudagraphs should run because forward skipped back_inp = torch empty_strided device= cuda out backward back_inp assertEqual counters inductor fxgraph_cache_miss Run one more time time AOTAutogradCache will hit assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_saved torch _dynamo reset inp = torch rand device= cuda requires_grad=True out = foo inp back_inp = torch empty_strided device= cuda out backward back_inp assertEqual counters aot_autograd autograd_cache_hit we should have cudagraph d anything assert get_manager None torch _functorch config patch enable_autograd_cache True torch _inductor config patch fx_graph_cache True torch _inductor config patch fx_graph_remote_cache False Currently fx graph cache turned off specialize_float=False torch _dynamo config patch specialize_float True requires_multigpu test_cached_boxed_forward_device_index torch compile mode= reduce-overhead foo x x x x Run device index so we can see cache hit we stay device index torch cuda _DeviceGuard torch cuda set_device inp = torch rand device= cuda requires_grad=True out = foo inp assertEqual counters inductor fxgraph_cache_miss Compile backward save cache back_inp = torch empty_strided device= cuda out backward back_inp assertEqual counters inductor fxgraph_cache_miss assertEqual counters aot_autograd autograd_cache_miss assertEqual counters aot_autograd autograd_cache_saved Reset dynamo rerun few times i range torch _dynamo reset inp = torch rand device= cuda requires_grad=True out = foo inp Should cache hit each time boxed_forward_device_index should still set properly assertEqual counters aot_autograd autograd_cache_hit i + back_inp = torch empty_strided device= cuda out backward back_inp After everything we should have cudagraphs device assertTrue get_manager device_index= None assertFalse get_manager device_index= None torch _functorch config patch enable_autograd_cache True torch _inductor config patch fx_graph_cache True torch _inductor config patch fx_graph_remote_cache False Currently fx graph cache turned off specialize_float=False torch _dynamo config patch specialize_float True test_backward_gets_cached_cudagraphs We pass cpu tensors foo save into cache On subsequent run new process cudagraphs should disabled properly both forward backwards runs torch compile mode= reduce-overhead foo x x x x torch _dynamo reset counters clear FxGraphCache clear AOTAutogradCache clear Use cpu device disable cudagraphs during compilation inp = torch rand device= cpu requires_grad=True out = foo inp assertEqual counters inductor fxgraph_cache_miss back_inp = torch empty_strided device= cpu out backward back_inp assertEqual counters inductor fxgraph_cache_miss Run again new process torch _dynamo reset Forward backward should also disable cudagraphs without compilation inp = torch rand device= cpu requires_grad=True out = foo inp AOTAutogradCache will load forward backward cache immediately so fx_graph_cache_hit will equal assertEqual counters inductor fxgraph_cache_hit assertEqual counters aot_autograd autograd_cache_hit torch _dynamo reset back_inp = torch empty_strided device= cpu out backward back_inp we should have cudagraph d anything assert get_manager None torch _inductor config patch triton skip_cudagraph_warmup True torch _functorch config patch enable_autograd_cache True torch _inductor config patch fx_graph_cache True torch _inductor config patch fx_graph_remote_cache False Currently fx graph cache turned off specialize_float=False torch _dynamo config patch specialize_float True test_cached_forward_backward counters clear AOTAutogradCache clear FxGraphCache clear torch compile foo x torch manual_seed y = x torch sin y torch nn functional dropout x p= inp = torch rand requires_grad=True device= cuda inp = inp detach clone requires_grad_ True out = foo inp out sum backward assertEqual get_root_children three saved tensors should die backward we kept alive output assertEqual curr_node expected_dead_indices_before_graph torch _inductor config graph_partition assertEqual curr_node expected_dead_indices_after_graph assertEqual curr_node expected_dead_indices_after_graph assertFalse get_manager new_graph_id id == assertEqual counters aot_autograd autograd_cache_miss Reset dynamo rerun We should see cache hit now torch _dynamo reset out = foo inp out sum backward assertEqual out out assertEqual inp grad inp grad assertEqual get_root_children assertFalse get_manager new_graph_id id == assertEqual counters aot_autograd autograd_cache_hit parametrize backend inductor cudagraphs test_forward_backward_not_called backend foo x y x_out = x x x torch _dynamo graph_break y_out = y y y x_out y_out foo = get_compile_fn backend foo _ range inps = torch rand requires_grad=True device= cuda _ range x_out y_out = foo inps inps x_out sum backward assertFalse get_manager running_forwards_with_pending_backwards we should have cudagraph d y backward new_id = get_manager new_graph_id id assertEqual new_id _test_unaligned_static_input_impl expected_clones fn x y x + y get_aligned_inputs torch rand device= cuda _ range mod = make_fx fn get_aligned_inputs mode = torch _subclasses FakeTensorMode mode inps = torch rand device= cuda _ range compiled_f = compile_fx_inner mod inps static_input_idxs= cudagraphs=True get_unaligned_inputs torch rand device= cuda _ range CloneCounterMode TorchDispatchMode __init__ - None count = __torch_dispatch__ func types args= kwargs=None kwargs = kwargs None kwargs count += func torch ops aten clone default func args kwargs _ range CloneCounterMode m compiled_f get_unaligned_inputs assertEqual m count expected_clones compiled_f get_aligned_inputs assertEqual m count expected_clones test_unaligned_static_input_trees _test_unaligned_static_input_impl expected_clones= torch _inductor config patch triton cudagraph_trees False test_unaligned_static_input_non_trees _test_unaligned_static_input_impl expected_clones= torch _inductor config patch triton cudagraphs False test_unaligned_static_input_no_cudagraphs _test_unaligned_static_input_impl expected_clones= torch _inductor config patch graph_partition True torch _inductor config patch implicit_fallbacks True test_graph_partition_custom_rule get_num_partitions code code = join code found = re search r partitions=\ \ code assert found None partitions = found group num_partitions = len p p partitions split p num_partitions torch library custom_op mylib bar mutates_args= bar x torch Tensor flag int - torch Tensor x clone bar register_fake _ x flag x clone f x flag x = x + x = bar x flag x = x + x x = torch randn device= cuda f_compiled = torch compile f mode= reduce-overhead fullgraph=True _ code = run_and_get_code f_compiled x True num_partitions = get_num_partitions code assertEqual num_partitions torch library custom_op mylib baz mutates_args= baz x torch Tensor - torch Tensor x clone baz register_fake _ x x clone custom_should_partition_ops takes effect which lead partitions torch _inductor config custom_should_partition_ops = mylib baz f x x = x + x = baz x x = x + x f_compiled = torch compile f mode= reduce-overhead fullgraph=True _ code = run_and_get_code f_compiled x num_partitions = get_num_partitions code assertEqual num_partitions update config should NOT force recompile torch _inductor config custom_should_partition_ops = torch compiler set_stance fail_on_recompile f_compiled x run_and_get_code forces recompile Now we should cache miss recompile only have partition _ code = run_and_get_code f_compiled x num_partitions = get_num_partitions code assertEqual num_partitions test op_overload name takes effect which lead partitions torch _inductor config custom_should_partition_ops = mylib baz default f_compiled = torch compile f mode= reduce-overhead fullgraph=True _ code = run_and_get_code f_compiled x num_partitions = get_num_partitions code assertEqual num_partitions torch _inductor config patch graph_partition True torch _inductor config patch implicit_fallbacks True test_graph_partition_with_memory_plan_reuse BATCH_SIZE = MLP_SIZE = HIDDEN_SIZE = RANDOM_SEED = torch library custom_op silly attention mutates_args= out tags= torch _C Tag cudagraph_unsafe attention q torch Tensor k torch Tensor v torch Tensor out torch Tensor - None out copy_ q + k + v attention register_fake _ q k v out None ParentModel torch nn Module __init__ - None super __init__ forward x torch Tensor - torch Tensor x Attention torch nn Module __init__ mlp_size int hidden_size int - None super __init__ pre_attn = torch nn Linear mlp_size hidden_size bias=False post_attn = torch nn Linear hidden_size mlp_size bias=False rms_norm_weight = torch nn Parameter torch ones hidden_size rms_norm_ref x torch Tensor - torch Tensor x_f = x float x_f torch rsqrt torch mean x_f square dim=- keepdim=True + e- rms_norm_weight x dtype forward x torch Tensor - torch Tensor x = pre_attn x x = rms_norm_ref x attn_output = torch empty_like x torch ops silly attention x x x attn_output x = attn_output x = rms_norm_ref x x = post_attn x x CompiledAttention torch nn Module __init__ mlp_size int hidden_size int - None super __init__ attn = Attention mlp_size hidden_size forward x torch Tensor - torch Tensor attn x CompiledAttentionTwo CompiledAttention forward x torch Tensor - torch Tensor attn x + x SimpleModelWithTwoGraphs ParentModel __init__ mlp_size int hidden_size int - None super __init__ attn_one = CompiledAttention mlp_size=mlp_size hidden_size=hidden_size attn_two = CompiledAttentionTwo mlp_size=mlp_size hidden_size=hidden_size hidden_states = torch zeros BATCH_SIZE MLP_SIZE cuda forward x torch Tensor - torch Tensor bsz = x shape CUDAGraph expects same tensor addresses each run hidden_states bsz copy_ x x = attn_one hidden_states bsz hidden_states bsz copy_ x x = attn_two hidden_states bsz x eager_model = SimpleModelWithTwoGraphs mlp_size=MLP_SIZE hidden_size=HIDDEN_SIZE eval cuda compiled_model = torch compile eager_model mode= reduce-overhead inputs = torch randn BATCH_SIZE MLP_SIZE cuda _ range eager_out = eager_model inputs compiled_out = compiled_model inputs assertEqual eager_out compiled_out torch _inductor config patch graph_partition True torch _inductor config patch triton cudagraph_trees False test_graph_partition_gc _test_dummy foo x x + foo = torch compile foo _ range foo torch randn device= cuda _test_dummy gc collect assertIsNone get_manager test_sparsity foo view_ buf aten _sparse_coo_tensor_with_dims_and_tensors view_ buf dtype=torch float layout=torch sparse_coo device= cuda pin_memory=None foo_opt = torch compile foo view_ = torch zeros dtype=torch int device= cuda buf = torch rand device= cuda _ range assertEqual foo_opt view_ buf foo view_ buf test_accumulate_multiple_recordings foo x y = x + x + x torch _dynamo graph_break y sum = y y foo_opt = torch compile foo two separate compilations recordings out = run_twc foo_opt torch zeros device= cuda out gets manually freed out = run_twc foo_opt torch zeros device= cuda assertEqual all_live_block_count out = run_twc foo_opt torch ones device= cuda assertEqual out foo torch ones device= cuda assertEqual all_live_block_count del out out assertEqual all_live_block_count del out gc collect assertEqual all_live_block_count torch _inductor config patch freezing True test_constant_output Mod torch nn Module __init__ - None super __init__ param = torch nn Parameter torch tensor float i i range device= cuda forward inp param param inp + inp = torch tensor device= cuda m = Mod torch no_grad out_eager = m inp m_comp = torch compile m _ range assertEqual out_eager m_comp inp test_live_outputs_multiple_graphs foo x x = x + x + x y = x + torch _dynamo graph_break z = x x z sum y + y foo_opt = torch compile foo run_twc foo_opt torch zeros device= cuda assertEqual num_checkpoints out = run_twc foo_opt torch ones device= cuda assertEqual all_live_block_count del out assertEqual all_live_block_count we need checkpoint function warmup y + then again record assertEqual num_checkpoints test_expanded_inputs x = torch rand device= cuda expand foo x x + + torch ones device= cuda foo_opt = torch compile foo _ range assertEqual foo_opt x foo x assertFalse get_manager new_graph_id id == torch _inductor config patch triton skip_cudagraph_warmup True test_tensor_dies_between_checkpoint foo args x = args args clear x + x + inp = torch rand device= cuda inp_list = inp foo_cg = cudagraphify_impl foo inp_list foo_cg inp_list foo_cg inp out out = foo_cg inp inp = out del out out foo args x = args args clear x x x assertEqual num_checkpoints foo _cg = cudagraphify_impl foo inp x = foo _cg inp assertEqual num_checkpoints out dies between previous recording new one need manually deallocated after checkpoint assertEqual all_live_block_count del x assertEqual all_live_block_count test_aliased_storage_single_weakref torch compile mode= reduce-overhead foo x x = x x_alias = x y = x y_alias = y torch _dynamo graph_break ind = torch tensor device= cuda x_alias = x ind y_alias = y ind x x_alias x_alias y_alias y_alias _ range outs = foo torch rand device= cuda ptr_to_ref = out untyped_storage data_ptr out untyped_storage _cdata out outs assertEqual len ptr_to_ref out outs assertEqual ptr_to_ref out untyped_storage data_ptr out untyped_storage _cdata del outs del out node = get_manager current_node assertEqual len list node path_live_weakrefs assertFalse get_manager new_graph_id id == test_aliasing_static_ref Mod torch nn Linear forward x weight T x weight T weight m = Mod cuda torch compile mode= reduce-overhead foo mod x mod x torch compile mode= reduce-overhead foo x x param_c = cdata m weight _ range x = torch rand device= cuda requires_grad=True torch compiler cudagraph_mark_step_begin out alias_ alias_ = foo m x assertEqual len param_c cdata alias_ cdata alias_ out = foo out out sum backward assertEqual cdata out cdata out m weight grad = None m bias grad = None node = curr_node first_node = next node _path_from_root torch _inductor config graph_partition graph partition may changed order outputs assertFalse first_node unaliased_in_all_paths assertTrue first_node cached_tensor_outputs None assertFalse first_node unaliased_in_all_paths assertTrue first_node cached_tensor_outputs None torch _inductor config patch implicit_fallbacks True test_multinomial sample_multinomial probs num_samples replacement=True torch multinomial probs num_samples replacement=replacement Create prepare probability tensor GPU probs = torch tensor cuda probs = probs probs sum Sample using function num_skipped = counters inductor cudagraph_skips torch _dynamo utils preserve_rng_state samples = run_twc sample_multinomial probs num_samples= replacement=True torch _dynamo utils preserve_rng_state samples_compiled = run_twc torch compile sample_multinomial probs num_samples= replacement=True assertEqual samples samples_compiled assertEqual num_skipped counters inductor cudagraph_skips skipIfRocm test_checkpointing_resets_persistent_refs torch compile mode= reduce-overhead foo x x x inp torch rand device= cuda requires_grad=False _ range foo inp assertEqual num_checkpoints out = foo inp out_id = id out del out assertEqual id foo inp out_id torch compile mode= reduce-overhead foo x x x x i range out = foo inp torch _dynamo mutation_guard GenerationTracker GenerationTracker generation -= out_alias out = foo out del out_alias assertEqual all_live_block_count del out assertEqual all_live_block_count del out assertEqual all_live_block_count assertEqual num_checkpoints i + new_out = foo inp curr_node = curr_node assertFalse curr_node unaliased_in_all_paths assertFalse out_id == id new_out test_aliased_static_parameter inp = torch rand device= cuda foo args x = args args clear x foo_cg = cudagraphify_impl foo inp _ range out = foo_cg inp assertEqual cdata inp cdata out node = curr_node assertEqual node cached_tensor_outputs None assertEqual node unaliased_in_all_paths False test_warmup_stream_sync foo args x = args args clear x_orig = x _ range x = x x x inp = torch rand device= cuda ref = foo inp torch cuda synchronize user_stream = torch cuda Stream torch cuda stream user_stream foo_cg = cudagraphify_impl foo inp out = foo_cg inp y = out + assertEqual y ref + test_unaligned_static_parameter gen_inp inp = torch ones device= cuda inp foo args x = args args clear x + x foo_cg = cudagraphify_impl foo gen_inp _ range out = foo_cg gen_inp assertEqual out foo gen_inp del out node = curr_node assertEqual node static_input_data_ptrs None test_amp_cache_disabled torch compile foo x x + x _ range out = foo torch rand device= cuda requires_grad=True amp cache cudagraph outputs should disabled t = torch rand device= cuda torch cuda amp autocast run_once = out t out detach zero_ run_twice = out t assertNotEqual run_once run_twice test_remove_hooks_on_cached_tensors torch compile foo x x x inp = torch rand device= cuda requires_grad=True _ range out = foo inp assertIsNone out _backward_hooks out register_hook lambda None today torch compile never outputs leaf tensor which only tensor can register _post_accumulate_grad_hooks add preventative test torch compile foo x torch rand device= cuda requires_grad=True _ range out = foo inp assertIsNone out _post_accumulate_grad_hooks out register_post_accumulate_grad_hook lambda None test_multiple_insert_removal_caching torch _C _set_cached_tensors_enabled True try x = torch rand device= cuda torch _C _add_cached_tensor x assertTrue torch _C _is_cached_tensor x torch _C _add_cached_tensor x torch _C _remove_cached_tensor x assertFalse torch _C _is_cached_tensor x finally torch _C _set_cached_tensors_enabled False test_accumulate_grad cudagraph trees shouldn t interfere accumulation logic compute_grad grad_output create_graph x = torch randn requires_grad=True device= cuda torch compile foo x x + y = foo x y backward grad_output retain_graph=True x_grad = x grad x_grad_clone = x grad clone y backward grad_output create_graph=create_graph x_grad x_grad_clone _ range grad_output = torch ones device= cuda Accumulate in-place when create_graph False x_grad x_grad_clone = compute_grad grad_output create_graph=False assertEqual x_grad x_grad_clone Accumulate out-of-place when create_graph False x_grad x_grad_clone = compute_grad grad_output create_graph=True assertEqual x_grad x_grad_clone test_frozen_fn torch compile foo x x x _ range out = foo torch rand device= cuda assertTrue get_manager new_graph_id id == frozen = torch _dynamo run foo _ range out = frozen torch rand device= cuda didn t do additional recordings assertTrue get_manager new_graph_id id == test_empty_cpu_tensor foo x x x torch tensor foo_opt = torch compile foo x = torch rand device= cuda _ range out_opt = foo_opt x assertEqual foo x out_opt assertTrue get_manager new_graph_id id == test_output_alias inp = torch rand device= cuda foo args x = args args clear out = x + x x x foo_cg = cudagraphify_impl foo inp _ range out_ out_ = foo_cg inp assertEqual cdata out_ cdata out_ del out_ out_ assertEqual len list curr_node path_live_weakrefs assertEqual curr_node cached_tensor_outputs None None test_empty_storage torch compile mode= reduce-overhead foo x x + x + x torch zeros device= cuda torch zeros device= cuda inp = torch rand device= cuda _ range out = foo inp node = curr_node assertEqual len list node path_live_weakrefs torch compile mode= reduce-overhead foo x x + x + x torch rand device= cuda + inp = torch rand device= cuda _ range out = foo inp node = curr_node assertEqual len list node path_live_weakrefs torch _inductor config patch triton skip_cudagraph_warmup True test_aliased_output_checkpoint foo args x = args args clear y = x + x + y y inp = torch rand device= cuda foo_cg = cudagraphify_impl foo inp foo_cg inp foo_cg inp out out out = foo_cg inp inp = out del out out out foo args x = args args clear x x x assertEqual num_checkpoints foo _cg = cudagraphify_impl foo inp x = foo _cg inp assertEqual num_checkpoints out out dies between previous recording new one need manually deallocated after checkpoint assertEqual all_live_block_count del x assertEqual all_live_block_count skipIfRocm unittest skipUnless IS_X IS_LINUX cpp contexts linux only torch _inductor config patch triton cudagraph_trees_history_recording True test_workspace_allocation_error torch _C _cuda_clearCublasWorkspaces prev = torch _inductor cudagraph_trees clear_cublas_manager try torch _inductor cudagraph_trees clear_cublas_manager = contextlib nullcontext torch compile foo x y x x inps = torch rand device= cuda _ range thrown = False try foo inps except Exception e thrown = True IS_ARM assertTrue cuda blas gemm float float str e cuda blas gemm_internal_cublas float float str e assertTrue getCurrentCUDABlasHandle str e getNewWorkspace str e assertTrue thrown finally torch _C _cuda_clearCublasWorkspaces torch _inductor cudagraph_trees clear_cublas_manager = prev torch _inductor cudagraph_trees get_container device_idx tree_manager = None test_peristed_output_livenes torch compile foo x x + x _ range foo torch rand device= cuda node = get_manager current_node assertEqual len list node path_live_weakrefs out = foo torch rand device= cuda assertTrue out node cached_tensor_outputs assertEqual len list node path_live_weakrefs out_ref = out del out assertEqual len list node path_live_weakrefs del out_ref assertEqual len list node path_live_weakrefs torch _inductor config patch triton skip_cudagraph_warmup True test_tensor_no_longer_in_pool foo args x = args args clear x + x + inp = torch rand device= cuda inp_list = inp foo_cg = cudagraphify_impl foo inp_list x x = foo_cg inp_list foo args x = args args clear x x x inp_list = x foo _cg = cudagraphify_impl foo inp_list foo _cg inp_list del x x TODO make configurable x x = foo_cg inp assertEqual num_checkpoints input location has changed should force recompile checkpointing foo _cg torch zeros_like x assertEqual num_checkpoints assertEqual get_root_children torch _inductor config patch triton skip_cudagraph_warmup True test_checkpoint_shared_output_storage_deallocation foo args x = args args clear x_tmp = x + x x inp = torch rand device= cuda inp_list = inp foo_cg = cudagraphify_impl foo inp_list foo_cg inp_list foo_cg inp x x = foo_cg inp inp = x foo args x = args args clear y = x x y y foo _cg = cudagraphify_impl foo inp foo _cg inp assertEqual num_checkpoints assertEqual x untyped_storage data_ptr x untyped_storage data_ptr assertEqual all_live_block_count del x assertEqual all_live_block_count del x assertEqual all_live_block_count torch _inductor config patch triton skip_cudagraph_warmup True test_cleanup test_closure torch compile foo x x + + x foo torch rand device= cuda foo torch rand device= cuda out out = test_closure torch _dynamo reset TODO - deallocate tensor deallocation assertTrue get_manager None del out assertTrue get_manager None del out assertTrue get_manager None torch _inductor config patch triton skip_cudagraph_warmup True test_forward_backward torch compile foo x y = x torch sin y torch nn functional dropout x p= inp = torch rand requires_grad=True device= cuda out = foo inp out sum backward assertEqual get_root_children three saved tensors should die backward we kept alive output assertEqual curr_node expected_dead_indices_before_graph torch _inductor config graph_partition assertEqual curr_node expected_dead_indices_after_graph assertEqual curr_node expected_dead_indices_after_graph assertFalse get_manager new_graph_id id == test_separate_recordings foo_unopt x y x + y foo = torch compile foo_unopt foo_unopt torch ones device= cuda torch ones device= cuda inps = torch ones device= cuda requires_grad=False _ range out = foo inps torch cuda synchronize foo inps torch cuda synchronize foo inps torch cuda synchronize foo_unopt torch ones device= cuda torch ones device= cuda inps = torch rand device= cuda requires_grad=False _ range foo inps foo inps foo inps two separate roots assertEqual get_root_children test_alias_of_parameter AliasMod nn Module __init__ - None super __init__ param = torch nn Parameter torch rand device= cuda forward x param param param + x torch compile mode= reduce-overhead foo mod inp mod inp inp = torch rand device= cuda mod = AliasMod storage_ref = torch multiprocessing reductions StorageWeakRef mod param untyped_storage _ range outs = foo mod inp assertEqual mod inp outs assertFalse storage_ref expired node = get_manager current_node assertEqual len list node path_live_weakrefs torch _dynamo config patch inline_inbuilt_nn_modules False torch _inductor config patch triton cudagraph_support_input_mutation False test_unstable_ptr torch torch compile mode= reduce-overhead foo m inp m inp f l = m = torch nn Linear cuda _ range inp = torch rand device= cuda foo m inp m weight data = torch rand device= cuda assertRaises RuntimeError f requires_multigpu test_manager_per_device test foo args x = args args clear x + inp = torch rand device=f cuda device_idx inp_list = inp foo_cg = cudagraphify_impl foo inp_list _ range assertEqual foo_cg inp foo inp next_idx = device_idx + torch cuda device_count assertTrue get_manager device_index=next_idx None assertFalse get_manager device_index=self device_idx None test assertTrue get_manager device_index=self device_idx None test_error_on_dealloc_use torch compile foo x x x x inp = torch rand device= cuda out = foo inp out = foo inp assertRaisesRegex Exception overwritten subsequent out + out foo inp assertRaisesRegex Exception overwritten subsequent out + out test_error_on_dealloc_use torch compile foo x x x x inp = torch rand device= cuda out = foo inp detach out = foo inp detach assertRaises Exception exc out + out FileCheck check overwritten check x x x run repr exc exception foo inp assertRaises Exception exc out + out FileCheck check overwritten check x x x run repr exc exception unittest skipIf torch backends cudnn is_available requires cudnn test_conv_benchmark torch backends cudnn flags enabled=True benchmark=True deterministic=False m = torch nn Conv d cuda inp = torch randn cuda torch compile foo m inp m inp foo m inp test_single_stream_use torch compile foo x x x x relu inp = torch rand device= cuda requires_grad=True streams = set streams_init = seg stream seg get_all_cudagraph_segments _ range foo inp sum backward inp grad = None streams = seg stream seg get_all_cudagraph_segments - streams_init assertEqual len streams assertFalse get_manager new_graph_id id == torch _dynamo config patch assume_static_by_default False test_dynamic_backward foo x x = torch cat x x torch addmm x x x relu x size opt_foo = torch compile mode= reduce-overhead foo run_test foo inp r s = foo inp r sum backward g = inp grad clone inp grad = None r = r clone r s g run_big_test inp r s g = run_test foo inp r s g = run_test opt_foo inp r s g = run_test opt_foo inp assertEqual r r assertEqual r r assertEqual s s assertEqual s s assertEqual g g assertEqual g g inp = torch randn device= cuda requires_grad=True run_big_test inp inp = torch randn device= cuda requires_grad=True run_big_test inp test_dynamic_warmup COUNTER = f inps i x = inps inps clear nonlocal COUNTER COUNTER += x x = torch randn device= cuda inp_list = x foo_cg = cudagraphify_impl f inp_list foo_cg inp_list warmup foo_cg x record foo_cg x replay assertEqual COUNTER Switching size will require warmup again x = torch randn device= cuda inp_list = x foo_cg inp_list warmup foo_cg x record foo_cg x replay assertEqual COUNTER test_forward_generation foo x x x x foo x x foo_opt = torch compile foo foo _opt = torch compile foo ones = torch ones device= cuda requires_grad=True out = foo_opt ones out = foo _opt out assertEqual all_live_block_count assertTrue get_manager running_forwards_with_pending_backwards out sum backward assertFalse get_manager running_forwards_with_pending_backwards ones grad = None del out del out foo _opt foo_opt ones sum backward out = foo_opt ones detach assertFalse get_manager running_forwards_with_pending_backwards assertFalse get_manager new_graph_id id == test_warn_on_pending_backward torch compile foo x x x x out = foo torch rand device= cuda requires_grad=True out = foo torch rand device= cuda requires_grad=True warnings resetwarnings warnings catch_warnings record=True w out = foo torch rand device= cuda requires_grad=True FileCheck check Unable hit fast path CUDAGraphs because pending run str w assertTrue get_manager new_graph_id id == test_mark_step torch compile foo x x x x torch compiler cudagraph_mark_step_begin out = foo torch rand device= cuda requires_grad=True torch compiler cudagraph_mark_step_begin out = foo torch rand device= cuda requires_grad=True assertFalse get_manager new_graph_id id == torch _dynamo config patch capture_scalar_outputs True test_incompatible_cudagraph_ops_item torch compile mode= reduce-overhead foo x x item NB This doesn t work float because float unbacked codegen currently broken But testing float case here also awkward because we plan Tensor-ify float compute result we d actually expect work cuda graphs capture_stderr captured_output assertEqual foo torch tensor device= cuda assertEqual foo torch tensor device= cuda NOTE test named after incompatible ops skipping due incompatible ops This should get fixed FileCheck check incompatible op aten _local_scalar_dense default run captured_output assertEqual counters inductor cudagraph_skips torch _dynamo config patch compiled_autograd True test_compiled_autograd_static_input_params torch compile mode= reduce-overhead bwd loss loss backward model = torch nn Linear bias=False device= cuda x = torch randn device= cuda _ range out = model x bwd out sum model weight grad = None i= copies warmup i= copies record inputs marked static i copies run assertEqual counters inductor cudagraph_recorded_non_static_inputs torch _dynamo config patch capture_dynamic_output_shape_ops True test_incompatible_cudagraph_ops_nonzero torch compile mode= reduce-overhead foo x x nonzero capture_stderr captured_output assertEqual foo torch tensor device= cuda torch tensor assertEqual foo torch tensor device= cuda torch tensor FileCheck check incompatible op aten nonzero default check foo run captured_output assertEqual counters inductor cudagraph_skips torch _dynamo config patch capture_dynamic_output_shape_ops True test_incompatible_cudagraph_ops_nonzero_graph_breaks torch compile mode= reduce-overhead foo x y = x nonzero skip torch _dynamo graph_break y nonzero skip times due recompile foo torch tensor device= cuda foo torch tensor device= cuda assertEqual counters inductor cudagraph_skips torch _dynamo config patch capture_dynamic_output_shape_ops True test_incompatible_cudagraph_ops_nonzero_backend torch compile backend= cudagraphs foo x x nonzero capture_stderr captured_output assertEqual foo torch tensor device= cuda torch tensor assertEqual foo torch tensor device= cuda torch tensor FileCheck check skipping cudagraphs due incompatible op nonzero run captured_output assertEqual counters inductor cudagraph_skips torch _dynamo config patch capture_dynamic_output_shape_ops True torch _inductor config patch cpp_wrapper True test_skip_cpp_wrapper foo x x + foo_c = torch compile mode= reduce-overhead foo capture_stderr captured_output t = torch rand device= cuda assertEqual foo t foo_c t FileCheck check skipping cudagraphs due cpp wrapper enabled run captured_output assertEqual counters inductor cudagraph_skips test_storage_access_error x = torch rand device= cuda torch _C _set_storage_access_error_msg x custom error msg assertRaisesRegex Exception custom error msg device = x untyped_storage test_side_stream_memory_allocation device = f cuda device_idx multi_stream_allocation args side_stream = torch cuda Stream side_stream wait_stream torch cuda current_stream torch cuda stream side_stream side_stream_buffer = torch ones args device=device dtype=torch float torch cuda current_stream wait_stream side_stream main_stream_buffer = torch ones args device=device dtype=torch float isinstance args list args clear main_stream_buffer side_stream_buffer graphed_multi_stream_func = tree_cudagraphify_impl multi_stream_allocation inputs= static_input_idxs= is_backward=False is_inference=False device_index=self device_idx stack_traces= dummy stack trace dummy stack trace ref_out = torch ones device=device dtype=torch float _ range torch compiler cudagraph_mark_step_begin main_stream_buffer side_stream_buffer = graphed_multi_stream_func assertEqual main_stream_buffer ref_out assertEqual side_stream_buffer ref_out assertEqual get_manager new_graph_id id torch _dynamo config patch inline_inbuilt_nn_modules False torch _inductor config patch triton cudagraph_support_input_mutation False test_static_inputs_address_mutation_log Goo torch nn Module __init__ - None super __init__ linear = torch nn Linear device= cuda forward x - torch Tensor linear x Foo torch nn Module __init__ - None super __init__ static_tensor = torch zeros device= cuda goo = Goo forward x - torch Tensor static_tensor add_ torch ones device= cuda static_tensor + x + goo x foo = Foo foo = torch compile foo mode= reduce-overhead inp = torch rand device= cuda _ range foo inp mutates static input tensors addresses foo static_tensor = torch ones device= cuda foo goo linear bias = torch nn Parameter torch ones device= cuda assertRaisesRegex Exception r s static input data pointer changed \n r input name primals_ data pointer changed input stack trace r input name primals_ data pointer changed input stack trace r forward\n static_tensor add\_\ torch ones\ \ \ device=\ cuda\ \ \ \n curr_node run foo goo linear weight foo goo linear bias foo static_tensor inp _run_iter param fn fwd_output = fn torch ones param fwd_output sum backward grad_output = param grad detach clone param grad = None fwd_output grad_output _assert_equal_multi_loop param fn_eager fn_compiled exp_output exp_grad = _run_iter param fn_eager _ range compiled_output compiled_grad = _run_iter param fn_compiled assertEqual exp_output compiled_output assertEqual exp_grad compiled_grad run_static_input_param_test fn_eager num_graphs torch device cuda fn_compiled = torch compile fn_eager mode= reduce-overhead p = torch nn Parameter torch rand _assert_equal_multi_loop p fn_eager fn_compiled p = torch nn Parameter torch rand _assert_equal_multi_loop p fn_eager fn_compiled Run p again ensure we reuse previous recording _assert_equal_multi_loop p fn_eager fn_compiled assertEqual get_manager new_graph_id id num_graphs _module_test mod name= weight param_wrapping=True torch device cuda fn x mod mod x fn_compiled = torch compile fn mode= reduce-overhead fullgraph=True run_test_iter mod fn fwd_output = fn torch ones mod fwd_output sum backward grad_output = mod weight grad detach clone mod zero_grad fwd_output grad_output run_test exp_output exp_grad = run_test_iter mod fn _ range compiled_output compiled_grad = run_test_iter mod fn_compiled assertEqual exp_output compiled_output assertEqual exp_grad compiled_grad run_test old_attr = getattr mod name modified_attr = torch rand_like old_attr param_wrapping modified_attr = torch nn Parameter modified_attr setattr mod name modified_attr run_test Run original version verify we reuse other recording setattr mod name old_attr run_test Fwd + bwd graphs each version function = graphs assertEqual get_manager new_graph_id id torch _dynamo config patch error_on_recompile True torch _dynamo config patch inline_inbuilt_nn_modules True test_multi_dispatch_single_compile_param_inputs Verify we can record multiple cudagraphs single compiled function param inputs fn x y x y Fwd + bwd graphs each version function = graphs run_static_input_param_test fn torch _dynamo config patch error_on_recompile True torch _dynamo config patch inline_inbuilt_nn_modules True test_multi_dispatch_single_compile_builtin_module Verify we don t recompile when changing param builtin module we record another cudagraph Note Linear builtin module so we enable config setting above _module_test torch nn Linear device= cuda torch _dynamo config patch error_on_recompile True torch _dynamo config patch inline_inbuilt_nn_modules True test_multi_dispatch_single_compile_builtin_module_buffers Verify we don t recompile when changing buffer builtin module we record another cudagraph _module_test torch nn BatchNorm d device= cuda name= running_mean param_wrapping=False torch _inductor config patch triton cudagraphs True torch _dynamo config patch error_on_recompile True torch _dynamo config patch inline_inbuilt_nn_modules True test_multi_dispatch_custom_module Test we can correctly dispatch multiple graphs params custom module change TestModule torch nn Module __init__ param - None super __init__ weight = param forward x weight x _module_test TestModule torch nn Parameter torch rand device= cuda torch _dynamo config patch error_on_recompile True torch _dynamo config patch inline_inbuilt_nn_modules True test_multi_dispatch_custom_module_buffer Test we can correctly dispatch multiple graphs buffers custom module change TestModule torch nn Module __init__ param buf - None super __init__ weight = param buf = torch nn Buffer buf forward x x weight + buf _module_test TestModule torch nn Parameter torch rand device= cuda torch rand device= cuda name= buf param_wrapping=False torch _inductor config patch triton cudagraphs True torch _dynamo config patch error_on_recompile True torch _dynamo config patch inline_inbuilt_nn_modules True test_multi_dispatch_child_node Test we can correctly dispatch multiple graphs child node tree has stable input pointers change fn x p Graph y = x x torch _dynamo graph_break Graph y p We have graphs here Graph \ Graph w p Graph w p then two backward graphs run_static_input_param_test fn torch _dynamo config patch error_on_recompile True torch _dynamo config patch inline_inbuilt_nn_modules True test_multi_dispatch_parent_node fn x p Graph y = x p torch _dynamo graph_break Graph y + x We have graphs here Graph w p Graph w p &#124; &#124; Graph v Graph v There two versions graph because we re-record due different memory state after running two versions Graph then two backward graphs run_static_input_param_test fn torch _dynamo config patch error_on_recompile True torch _dynamo config patch inline_inbuilt_nn_modules False torch _inductor config patch triton cudagraph_support_input_mutation True torch _inductor config patch triton cudagraph_unexpected_rerecord_limit test_fallback_to_eager_if_recompiling_too_many_times Foo torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand device= cuda forward x x param capture_stderr captured_output We have graphs here None \ fwd w p Graph bwd w p Graph bwd w p Graph All other graphs skipped because we hit max recording limit = each node function pair fn_compiled = torch compile Foo mode= reduce-overhead _ range fn_compiled torch rand device= cuda sum backward fn_compiled param grad = None Change static tensor address fn_compiled param data = torch rand device= cuda fn_compiled torch rand device= cuda sum backward assertEqual get_manager new_graph_id id FileCheck check skipping cudagraph due function exceeding max re-recording limit = cudagraph node None due static input data pointer changed run captured_output assertEqual counters inductor cudagraph_skips torch _dynamo config patch error_on_recompile True torch _dynamo config patch inline_inbuilt_nn_modules False torch _inductor config patch triton cudagraph_support_input_mutation True torch _inductor config patch triton cudagraph_unexpected_rerecord_limit test_fallback_to_eager_if_recompiling_too_many_times_warn_only_once Foo torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand device= cuda forward x x param capture_stderr captured_output torch device cuda We have graphs here None \ fwd w p Graph bwd w p Graph bwd w p Graph All other graphs skipped because we hit max recording limit = each node function pair fn_compiled = torch compile Foo mode= reduce-overhead _ range fn_compiled torch rand device= cuda sum backward fn_compiled param grad = None _ range Change static tensor address fn_compiled param data = torch rand device= cuda fn_compiled torch rand device= cuda sum backward fn_compiled param grad = None FileCheck check_count skipping cudagraph due function exceeding max re-recording limit = cudagraph node None due static input data pointer changed exactly=True check_count skipping cudagraph due function exceeding max re-recording limit = cudagraph node None due static input data pointer changed exactly=True run captured_output assertEqual counters inductor cudagraph_skips torch _dynamo config patch inline_inbuilt_nn_modules False torch _inductor config patch triton cudagraph_support_input_mutation True torch _inductor config patch triton cudagraph_unexpected_rerecord_limit test_fallback_to_eager_if_recompiling_too_many_times_due_to_cudagraph_managed_tensor By setting triton cudagraph_support_input_mutation=True we force re-record cudagraph managed tensor addresses changed torch compile mode= reduce-overhead foo x x + torch compile mode= reduce-overhead goo x x _ range torch compiler cudagraph_mark_step_begin inp = torch rand device= cuda y = foo inp z = goo y capture_stderr captured_output torch compiler cudagraph_mark_step_begin x = torch rand device= cuda y = foo x y_clone = y clone z = goo y_clone eager function should run successfully _ range torch compiler cudagraph_mark_step_begin x = torch rand device= cuda y = foo x y_clone = y clone z = goo y_clone FileCheck check_count skipping cudagraph due function exceeding max re-recording limit = cudagraph node due cudagraph managed tensor data pointer changed exactly=True run captured_output assertEqual counters inductor cudagraph_skips torch _dynamo config patch inline_inbuilt_nn_modules False torch _dynamo config patch error_on_recompile True torch _dynamo config patch inline_inbuilt_nn_modules True torch _inductor config patch triton cudagraph_unexpected_rerecord_limit test_not_fallback_to_eager_if_have_not_recompiling_too_many_times fn x y x y We have graphs here None \ fwd w p Graph fwd w p Graph bwd w p Graph bwd w p Graph run_static_input_param_test fn assertEqual counters inductor cudagraph_skips torch _dynamo config patch error_on_recompile True torch _dynamo config patch inline_inbuilt_nn_modules True test_no_rerecord_with_mark_static_address Mod torch nn Module __init__ super __init__ linear = nn Linear forward x linear x mod = Mod cuda fn_eager x marked_static_y torch cos x + mod marked_static_y torch device cuda fn_compiled = torch compile fn_eager mode= reduce-overhead y marked static y = torch randn torch _dynamo mark_static_address y Chanhing pointer x should lead re-records _ range x = torch randn requires_grad=True res = fn_compiled x y res sum backward x grad = None mod linear weight grad = None mod linear bias grad = None One forward one backward assertEqual get_manager new_graph_id id test_tensor_constant_mutation Foo torch nn Module __init__ - None super __init__ tensor_constant = torch ones device= cuda forward x torch Tensor - torch Tensor tensor_constant += x + tensor_constant foo = Foo foo = torch compile foo mode= reduce-overhead inp = torch rand device= cuda _ range foo inp torch _inductor config patch triton cudagraph_support_input_mutation True test_rerecord_if_static_input_address_changed By setting triton cudagraph_support_input_mutation=True we force re-record static tensor addresses changed Goo torch nn Module __init__ - None super __init__ linear = torch nn Linear device= cuda forward x - torch Tensor linear x Foo torch nn Module __init__ - None super __init__ register_buffer static_tensor torch zeros device= cuda goo = Goo forward x - torch Tensor static_tensor add_ torch ones device= cuda static_tensor + x + goo x foo = Foo foo = torch compile foo mode= reduce-overhead inp = torch rand device= cuda _ range foo inp mutates static input tensors addresses foo static_tensor = torch ones device= cuda foo goo linear bias = torch nn Parameter torch ones device= cuda torch _dynamo config inline_inbuilt_nn_modules _ range foo inp Run specific function id avoid dynamo recompiling get_manager run foo goo linear weight foo goo linear bias foo static_tensor inp FunctionID assertEqual get_manager new_graph_id id torch _inductor config patch triton cudagraph_dynamic_shape_warn_limit test_skip_if_dynamic_shape_limit_reached Mod torch nn Module __init__ - None super __init__ linear = torch nn Linear device= cuda forward x torch Tensor - torch Tensor linear x iter batch_size int mod torch nn Module x = torch rand batch_size device= cuda _ range mod x mod = torch compile Mod mode= reduce-overhead capture_stderr captured_output batch_size range iter batch_size mod FileCheck check CUDAGraph supports dynamic shapes recording new graph each distinct input size Recording too many CUDAGraphs may lead extra overhead We have observed distinct sizes Please consider following options better performance padding inputs few fixed number shapes b set torch _inductor config triton cudagraph_skip_dynamic_graphs=True Set torch _inductor config triton cudagraph_dynamic_shape_warn_limit=None silence warning run \n join captured_output torch _inductor config patch triton cudagraph_dynamic_shape_warn_limit test_skip_if_dynamic_shape_limit_reached Mod torch nn Module __init__ - None super __init__ attn = torch nn MultiheadAttention embed_dim= num_heads= device= cuda forward q torch Tensor k torch Tensor v torch Tensor - torch Tensor attn q k v mod = torch compile Mod mode= reduce-overhead iter batch_size int length int q = torch rand batch_size length device= cuda k = torch rand batch_size length device= cuda v = torch rand batch_size length device= cuda _ range mod q k v capture_stderr captured_output batch_size range length range iter batch_size length print captured_output FileCheck check CUDAGraph supports dynamic shapes recording new graph each distinct input size Recording too many CUDAGraphs may lead extra overhead We have observed distinct sizes Please consider following options better performance padding inputs few fixed number shapes b set torch _inductor config triton cudagraph_skip_dynamic_graphs=True Set torch _inductor config triton cudagraph_dynamic_shape_warn_limit=None silence warning run captured_output torch _inductor config patch triton cudagraph_dynamic_shape_warn_limit test_warn_once_if_dynamic_shape_limit_reached Mod torch nn Module __init__ - None super __init__ linear = torch nn Linear device= cuda forward x torch Tensor - torch Tensor linear x iter batch_size int mod torch nn Module x = torch rand batch_size device= cuda _ range mod x mod = torch compile Mod mode= reduce-overhead capture_stderr captured_output batch_size range iter batch_size mod FileCheck check_count CUDAGraph supports dynamic shapes recording new graph each distinct input size Recording too many CUDAGraphs may lead extra overhead We have observed distinct sizes Please consider following options better performance padding inputs few fixed number shapes b set torch _inductor config triton cudagraph_skip_dynamic_graphs=True Set torch _inductor config triton cudagraph_dynamic_shape_warn_limit=None silence warning exactly=True run \n join captured_output torch _inductor config patch cpp_wrapper test_cpp_wrapper f x torch sin x compiled = torch compile f mode= reduce-overhead example_input = torch randn device= cuda compiled_result = run_twc compiled example_input eager_result = f example_input assertEqual compiled_result eager_result torch _inductor config patch graph_partition True test_graph_partition f x y x = x + y = y + y_cpu = y cpu + z = x y x + y + z + y_cpu cuda x y = torch randn device= cuda _ range x_cloned y_cloned = tmp clone tmp x y eager_out = f x y f_compiled = torch compile f mode= reduce-overhead _ range compiled_out = f_compiled x_cloned y_cloned assertEqual eager_out compiled_out graph partitions lead cudagraph assertEqual get_manager new_graph_id id test_graph_partition_view_fallback f x y = x + z = torch ops aten view dtype y torch float _e m fn z_cpu = z cpu u_cuda = z_cpu cuda u_cuda compiled_f = torch compile f mode= reduce-overhead _ range x = torch ones dtype=torch int device= cuda eager_out = f x compiled_out = compiled_f x assertEqual eager_out compiled_out torch _inductor config patch graph_partition True test_graph_partition_log_message foo x y x + y + foo = torch compile foo mode= reduce-overhead capture_stderr captured_output foo torch ones device= cuda torch ones FileCheck check_count cudagraph partition due non gpu ops Found exactly=True check_count x + y + exactly=True check cudagraph partition into partitions run captured_output torch _inductor config patch graph_partition True test_graph_partition_cpu_scalar f x y x + y compiled_f = torch compile f mode= reduce-overhead inputs = torch ones device= cuda torch ones device= cpu i range i == _ code = run_and_get_code compiled_f inputs FileCheck check_count copy_ exactly=True run code compiled_f inputs assertEqual compiled_f inputs f inputs assertEqual get_manager new_graph_id id torch _inductor config patch graph_partition True test_graph_partition_cpu_scalar f x y z x + y x + z compiled_f = torch compile f mode= reduce-overhead inputs = torch ones device= cpu torch ones device= cuda torch ones device= cuda i range i == _ code = run_and_get_code compiled_f inputs FileCheck check_count copy_ exactly=True run code compiled_f inputs assertEqual compiled_f inputs f inputs assertEqual get_manager new_graph_id id torch _inductor config patch graph_partition True test_graph_partition_cpu_scalar f x y cpu_scalar_tensor z = x + y z = z + cpu_scalar_tensor z compiled_f = torch compile f mode= reduce-overhead inputs = torch randn device= cuda torch randn device= cuda torch tensor device= cpu i range i == _ code = run_and_get_code compiled_f inputs FileCheck check_count copy_ exactly=True run code compiled_f inputs assertEqual compiled_f inputs f inputs assertEqual get_manager new_graph_id id torch _inductor config patch graph_partition True test_graph_partition_cpu_scalar cpu_scalar_tensor accessed cpu_scalar which added gpu tensor z This test checks cpu scalar tensors still moved case f x y cpu_scalar_tensor cpu_scalar = cpu_scalar_tensor + z = x + y z = z + cpu_scalar z compiled_f = torch compile f mode= reduce-overhead inputs = torch randn device= cuda torch randn device= cuda torch tensor device= cpu i range i == _ code = run_and_get_code compiled_f inputs FileCheck check_count copy_ exactly=True run code compiled_f inputs assertEqual compiled_f inputs f inputs assertEqual get_manager new_graph_id id torch _inductor config patch graph_partition True turn input mutation support avoid skipping cudagraph dynamo level torch _inductor config patch triton cudagraph_support_input_mutation True test_graph_partition_cpu_scalar_mutation tests input mutation cpu scalar tensor x correctly handled when moving x gpu beginning graph torch compile mode= reduce-overhead foo x y x copy_ y x = torch tensor y = torch tensor device= cuda _ range foo x y assertEqual x torch tensor device= cpu assertEqual y torch tensor device= cuda assertEqual get_manager new_graph_id id torch _inductor config patch graph_partition True test_graph_partition_cpu_scalar_device_put torch compile mode= reduce-overhead foo x y = x cuda z = y cpu z x = torch tensor _ range foo x assertEqual x torch tensor device= cpu torch _inductor config patch graph_partition True test_graph_partition_cpu_scalar_multiple f x y z x + y x + z compiled_f = torch compile f mode= reduce-overhead inputs = torch ones device= cpu torch ones device= cpu torch ones device= cuda i range i == _ code = run_and_get_code compiled_f inputs FileCheck check_regex r copy_ True run code FileCheck check_count copy_ exactly=True run code compiled_f inputs assertEqual compiled_f inputs f inputs assertEqual get_manager new_graph_id id torch _inductor config patch graph_partition True torch _inductor config patch triton cudagraphs False test_graph_partition_reduce_overhead_mode_effectiveness test ` mode= reduce-overhead ` still controls whether cudagraph applied i e cudagraph applied when mode= default f x y x = x + y = y + y_cpu = y cpu + z = x y x + y + z + y_cpu cuda x y = torch randn device= cuda _ range f_compiled = torch compile f _ range _out = f_compiled x y assertEqual get_manager None True torch _inductor config patch graph_partition True test_graph_partition_forward_backward Mod torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x x = x + y = x + y_cpu = y cpu + z = x y inp = x + y + z + y_cpu cuda linear inp model = Mod cuda input_data = torch randn cuda criterion = torch nn CrossEntropyLoss optimizer = torch optim SGD model parameters lr= compiled_model = torch compile model mode= reduce-overhead _ range output = compiled_model input_data loss = criterion output torch randint cuda optimizer zero_grad loss backward optimizer step graph partitions lead fwd cudagraphs bwd cudagraphs assertEqual get_manager new_graph_id id torch _inductor config patch graph_partition True test_graph_partition_cpu_only Mod torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x x = x + y = x + y_cpu = y + z = x y inp = x + y + z + y_cpu linear inp model = Mod cpu input_data = torch randn cpu criterion = torch nn CrossEntropyLoss optimizer = torch optim SGD model parameters lr= compiled_model = torch compile model mode= default _ range output = compiled_model input_data loss = criterion output torch randint cpu optimizer zero_grad loss backward optimizer step cudagraph since all ops cpu assertEqual get_manager None True torch _inductor config patch graph_partition True test_graph_partition_forward_with_skipped_cudagraphed_backward torch compile mode= reduce-overhead foo x x x x _ range inp = torch rand device= cuda requires_grad=True out = foo inp config patch always_complex_memory_overlap_TESTING_ONLY=True back_inp = torch empty_strided device= cuda out backward back_inp we should have cudagraph d backwards new_id = get_manager new_graph_id id assertEqual new_id assertFalse get_manager running_forwards_with_pending_backwards torch _inductor config patch graph_partition True test_graph_partition_forward_backward_not_called tests saved tensor handled correctly foo x y x_out = x x x torch _dynamo graph_break y_out = y y y x_out y_out foo = torch compile foo mode= reduce-overhead _ range inps = torch rand requires_grad=True device= cuda _ range x_out y_out = foo inps inps x_out sum backward assertFalse get_manager running_forwards_with_pending_backwards we should have cudagraph d y backward new_id = get_manager new_graph_id id assertEqual new_id requires_multigpu torch _inductor config patch graph_partition True test_graph_partition_multiple_devices_msg foo x y x + y + foo = torch compile foo mode= reduce-overhead _ range foo torch ones device= cuda torch ones assertEqual counters inductor cudagraph_skips capture_stderr captured_output _ range foo torch ones device= cuda torch ones device= cuda FileCheck check skipping cudagraphs due multiple devices run captured_output assertEqual counters inductor cudagraph_skips new_id = get_manager new_graph_id id assertEqual new_id torch _inductor config patch graph_partition True test_graph_partition_dynamic_shapes foo x x + compiled_foo = torch compile foo mode= reduce-overhead fullgraph=True input_shape range _ range compiled_foo torch randn input_shape device= cuda cudagraphs input shapes assertEqual get_manager new_graph_id id torch _inductor config patch graph_partition True test_graph_partition_cpu_op_and_dynamic_shapes f x y x = x + y = y + y_cpu = y cpu + z = x y x + y + z + y_cpu cuda f_compiled = torch compile f x y = torch ones device= cuda torch randn device= cuda _ range compiled_out = f_compiled x y assertEqual compiled_out f x y x y = torch ones device= cuda torch randn device= cuda _ range compiled_out = f_compiled x y assertEqual compiled_out f x y cudagraphs due dynamic shapes x graph partitions assertEqual get_manager new_graph_id id config patch implicit_fallbacks=True config patch graph_partition False test_skip_cudagraph_unsafe_ops torch library custom_op mylib mysin mutates_args= out_list schema= Tensor x Tensor out_list - Tensor tags= torch _C Tag cudagraph_unsafe mysin x out_list - torch Tensor r = x sin out_list None out_list copy_ r r mysin register_fake _ x out_list - torch Tensor torch empty_like x fn x x = x s = torch empty_like x x = mysin x s x = x x s x = torch randn requires_grad=False device= cuda expected = fn x compiled_f = torch compile fn mode= reduce-overhead fullgraph=True capture_stderr captured_output _ range result = compiled_f x assertEqual result expected FileCheck check incompatible op mylib mysin default run captured_output assertEqual counters inductor cudagraph_skips config patch implicit_fallbacks=True torch _inductor config patch graph_partition True test_graph_partition_custom_op torch library custom_op mylib movement mutates_args= tags= torch _C Tag cudagraph_unsafe movement pic torch Tensor - torch Tensor img = pic cpu cropped_img = img + cropped_img cuda movement register_fake _ pic torch empty_like pic torch library custom_op mylib modify mutates_args= tags= torch _C Tag cudagraph_unsafe modify pic torch Tensor - torch Tensor pic = pic + pic _cpu = pic cpu + pic _cpu cuda + pic modify register_fake _ pic torch empty_like pic torch library custom_op mylib transform mutates_args= transform pic torch Tensor - torch Tensor pic + transform register_fake _ pic torch empty_like pic img = torch randn device= cuda f img x = img + y = movement x z = y + u = transform z v = u + out = modify v out + compiled_f = torch compile f fullgraph=True eager_out = f img compiled_out = compiled_f img assertEqual eager_out compiled_out compiled_f = torch compile f mode= reduce-overhead fullgraph=True eager_out = f img _ range compiled_out = compiled_f img assertEqual eager_out compiled_out splitting custom gives cudagraphs assertEqual get_manager new_graph_id id config patch implicit_fallbacks=True config patch graph_partition True test_graph_partition_custom_op_mutation torch library custom_op mylib mysin mutates_args= out_list schema= Tensor x Tensor out_list - Tensor tags= torch _C Tag cudagraph_unsafe mysin x out_list - torch Tensor r = x sin out_list None out_list copy_ r r mysin register_fake _ x out_list - torch Tensor torch empty_like x fn x x = x s = torch empty_like x x = mysin x s x = x x s x = torch randn requires_grad=False device= cuda expected = fn x compiled_f = torch compile fn mode= reduce-overhead fullgraph=True _ range result = compiled_f x assertEqual result expected splitting custom gives cudagraphs assertEqual get_manager new_graph_id id config patch implicit_fallbacks=True torch _inductor config patch graph_partition True test_graph_partition_custom_op_mutation_late_free torch library custom_op mylib op mutates_args= x schema= Tensor x - Tensor Tensor device_types= cuda op x - tuple torch Tensor torch Tensor x = x + x + x + op register_fake _ x - tuple torch Tensor torch Tensor torch empty_like x torch empty_like x torch library custom_op mylib cg_unsafe_op mutates_args= schema= Tensor x Tensor y Tensor x Tensor y - Tensor device_types= cuda tags= torch _C Tag cudagraph_unsafe cg_unsafe_op x x y y - torch Tensor x + x + y + y cg_unsafe_op register_fake _ x x y y - torch Tensor torch empty_like x f x x = x + x = op x x x = x x y = x + y = x + y = cg_unsafe_op x x y y z = y + x + x z z = op z z = z + z res = cg_unsafe_op z z y y res x = torch randn device= cuda x_cloned = x clone eager_out = f x f_compiled = torch compile f mode= reduce-overhead _ range compiled_out = f_compiled x_cloned assertEqual eager_out compiled_out config patch implicit_fallbacks=True torch _inductor config patch graph_partition True test_graph_partition_custom_op_dynamoc_shapes torch library custom_op mylib movement mutates_args= tags= torch _C Tag cudagraph_unsafe movement pic torch Tensor - torch Tensor img = pic cpu cropped_img = img + cropped_img cuda movement register_fake _ pic torch empty_like pic f img x = img + y = movement x z = y + v = z + v + compiled_f = torch compile f fullgraph=True compiled_f = torch compile f mode= reduce-overhead fullgraph=True run size img = torch randn size size device= cuda eager_out = f img _ range compiled_out = compiled_f img assertEqual eager_out compiled_out run run run splitting custom op x dynamic shapes = assertEqual get_manager new_graph_id id config patch implicit_fallbacks=True torch _inductor config patch graph_partition True test_graph_partition_custom_op_no_split torch library custom_op mylib modify mutates_args= modify x torch Tensor - torch Tensor x + modify register_fake _ pic torch empty_like pic f img x = img + y = modify x z = y + v = z + v + compiled_f = torch compile f fullgraph=True compiled_f = torch compile f mode= reduce-overhead fullgraph=True run size img = torch randn size size device= cuda eager_out = f img _ range compiled_out = compiled_f img assertEqual eager_out compiled_out run run run splitting custom op x dynamic shapes = assertEqual get_manager new_graph_id id torch _inductor config patch graph_partition True test_graph_partition_cpu_tensor_symints f x y x + y + compiled_f = torch compile f mode= reduce-overhead run shape_x shape_y x = torch randn shape_x device= cuda y = torch randn shape_y device= cpu _ range compiled_f x y static shape record NEW cudagraph run shape_x= shape_y= shape_y becomes dynamic shape leading new dynamo graph This new dynamo graph forces NEW cudagraph although tensor y cpu run shape_x= shape_y= tensor y cpu so NO new cudagraph recorded run shape_x= shape_y= shape_x becomes dynamic shape leading new dynamo graph new dynamo graph forces NEW cudagraph run shape_x= shape_y= tensor y cpu so NO new cudagraph recorded run shape_x= shape_y= assertEqual get_manager new_graph_id id torch _inductor config patch graph_partition True test_graph_partition_reorder_cpu_and_gpu f x_cuda y_cpu z_cuda weight_cuda weight_cpu x_cuda = x_cuda + x_cuda = x_cuda weight_cuda x_cuda = x_cuda + x_cuda y_cpu = y_cpu + y_cpu = y_cpu weight_cpu z_cuda = z_cuda + z_cuda = z_cuda weight_cuda z_cuda = z_cuda + z_cuda x_cuda y_cpu z_cuda x_cuda = torch randn device= cuda y_cpu = torch randn device= cpu z_cuda = torch randn device= cuda weight_cuda = torch randn device= cuda weight_cpu = torch randn device= cpu eager_out = f x_cuda y_cpu z_cuda weight_cuda weight_cpu compiled_f = torch compile f mode= reduce-overhead _ range compiled_out = compiled_f x_cuda y_cpu z_cuda weight_cuda weight_cpu assertEqual eager_out compiled_out reorder merges ops cuda into graph partition assertEqual get_manager new_graph_id id torch _inductor config patch graph_partition True test_graph_partition_reorder_cpu_and_gpu_interleave f x_cuda y_cpu z_cuda weight_cuda weight_cpu partition cuda no dependency x_cuda = x_cuda + x_cuda = x_cuda weight_cuda x_cuda = x_cuda + x_cuda partition cpu w dependency partition y_cpu = y_cpu + x_cuda _cpu = x_cuda cpu adds dependency gpu computations y_cpu = y_cpu weight_cpu + x_cuda _cpu partition cuda w o dependency z_cuda = z_cuda + z_cuda = z_cuda weight_cuda z_cuda = z_cuda + z_cuda partition cpu w o dependency y_cpu = y_cpu + y_cpu = y_cpu weight_cpu partition cuda w o dependency u_cuda = z_cuda + u_cuda = u_cuda weight_cuda u_cuda = u_cuda + u_cuda x_cuda y_cpu z_cuda y_cpu u_cuda x_cuda = torch randn device= cuda y_cpu = torch randn device= cpu z_cuda = torch randn device= cuda weight_cuda = torch randn device= cuda weight_cpu = torch randn device= cpu eager_out = f x_cuda y_cpu z_cuda weight_cuda weight_cpu compiled_f = torch compile f mode= reduce-overhead _ range compiled_out = compiled_f x_cuda y_cpu z_cuda weight_cuda weight_cpu assertEqual eager_out compiled_out optimal order partition cpu partition cuda partition cpu since partition depends partition So we have cudagraph total assertEqual get_manager new_graph_id id config patch implicit_fallbacks=True torch _inductor config patch graph_partition True test_graph_partition_reorder_custom_op_with_no_dependency Two reasons We want reuse same mask many masked_fill calls Prevent inductor fusing op into other ops e g masked_fill so we can still reorder scheduler torch library custom_op mylib create_mask mutates_args= tags= torch _C Tag cudagraph_unsafe create_mask padded_size int original_size int device torch device - torch Tensor mask = torch zeros padded_size dtype=torch bool device=device mask original_size = True mask create_mask register_fake _ padded_size original_size device torch empty padded_size dtype=torch bool device=device f padded_tensor original_tensor weight original_size = original_tensor size padded_size = padded_tensor size element wise op so we don t care padding value padded_tensor = padded_tensor + padded_tensor = torch nn functional relu padded_tensor dot product requires padding dot_res = padded_tensor dot weight padded_tensor += dot_res min requires padding inf so we create mask now mask = create_mask padded_size original_size padded_tensor device min_res = torch min torch ops aten masked_fill padded_tensor mask float inf max requires padding inf we can reuse previous mask max_res = torch max torch ops aten masked_fill padded_tensor mask -float inf min_res + max_res + padded_tensor compiled_f = torch compile f mode= reduce-overhead run padded_size original_size padded_tensor = torch randn padded_size device= cuda padded_tensor original_size = original_tensor = torch randn original_size device= meta weight = torch randn padded_size device= cuda eager_out = f padded_tensor original_tensor weight _ range compiled_out = compiled_f padded_tensor original_tensor weight assertEqual eager_out compiled_out although custom op ` create_mask ` happens middle function reorder moves front so we only have partition This leads cudagraph run recompilation leads NEW cudagraph run assertEqual get_manager new_graph_id id config patch implicit_fallbacks=True torch _inductor config patch graph_partition True test_graph_partition_reorder_custom_op_with_no_dependency wrap custom op so fused into other ops torch library custom_op mylib create_size_tensor mutates_args= tags= torch _C Tag cudagraph_unsafe create_size_tensor tensor torch Tensor device torch device - torch Tensor size = tensor size zero = torch zeros device=device zero + size create_size_tensor register_fake _ tensor device size = tensor size zero = torch zeros device=device dtype=torch int zero + size fill padded_tensor torch Tensor original_size torch Tensor value - torch Tensor padded_size = padded_tensor size size_range = torch arange padded_size device=padded_tensor device padded_tensor = torch where size_range = original_size value padded_tensor padded_tensor f padded_tensor original_tensor weight element wise op so we don t care padding value padded_tensor = padded_tensor + padded_tensor = torch nn functional relu padded_tensor dot product requires padding dot_res = padded_tensor dot weight padded_tensor += dot_res min requires padding inf so we create mask now original_size_cuda = create_size_tensor original_tensor cuda padded_tensor = fill padded_tensor original_size_cuda float inf min_res = torch min padded_tensor max requires padding inf we can reuse previous mask padded_tensor = fill padded_tensor original_size_cuda -float inf max_res = torch max padded_tensor min_res + max_res + padded_tensor compiled_f = torch compile f mode= reduce-overhead run padded_size original_size padded_tensor = torch randn padded_size device= cuda padded_tensor original_size = original_tensor = torch randn original_size device= meta weight = torch randn padded_size device= cuda eager_out = f padded_tensor original_tensor weight _ range compiled_out = compiled_f padded_tensor original_tensor weight assert torch allclose eager_out compiled_out although custom op ` create_mask ` happens middle function reorder moves front so we only have partition This leads cudagraph run recompilation leads NEW cudagraph run reuse previous cudagraph run assertEqual get_manager new_graph_id id torch _inductor config patch graph_partition True test_graph_partition_simple f x y x = x + y = y + y_cpu = y cpu + z = x y x + y + z + y_cpu cuda x y = torch ones device= cuda _ range x_cloned y_cloned = tmp clone tmp x y eager_out = f x y f_compiled = torch compile f compiled_out = f_compiled x_cloned y_cloned assertEqual eager_out compiled_out _ code = run_and_get_code f_compiled x_cloned y_cloned config cpp_wrapper FileCheck check partition_ args check recursively_apply_fns = runner recursively_apply_fns run code torch _inductor config patch graph_partition True test_graph_partition_foreach_op fn c = torch _foreach_abs torch mul c compiled_fn = torch compile fn = torch randn device= cuda = torch randn device= cuda eager_out = fn compiled_out = compiled_fn assertEqual eager_out compiled_out torch _inductor config patch graph_partition True test_graph_partition_condition_op f p b true_fn x torch cos x false_fn x torch sin x torch cond p true_fn false_fn b compiled_f = torch compile f static shape p = torch tensor True device= cuda = torch ones device= cuda eager_out = f p compiled_out = compiled_f p assertEqual eager_out compiled_out dynamic shape backed symint p = torch tensor True device= cuda = torch ones device= cuda eager_out = f p compiled_out = compiled_f p assertEqual eager_out compiled_out torch _inductor config patch graph_partition True torch _dynamo config patch capture_scalar_outputs True test_graph_partition_unbacked_symint_multi_output_layout f p size_tensor size_val = size_tensor item b = torch ones size_val device= cuda true_fn x torch cos x torch cos x + false_fn x torch sin x torch sin x + cond_out = torch cond p true_fn false_fn b cond_out + cond_out compiled_f = torch compile f p = torch tensor True device= cuda size_tensor = torch tensor device= cuda eager_out = f p size_tensor compiled_out = compiled_f p size_tensor assertEqual eager_out compiled_out torch _inductor config patch graph_partition True test_graph_partition_symint f x y x = x + y = y + y_cpu = y cpu + z = x y x + y + z + y_cpu cuda f_compiled = torch compile f x y = torch ones device= cuda torch randn device= cuda compiled_out = f_compiled x y assertEqual compiled_out f x y x y = torch ones device= cuda torch randn device= cuda compiled_out = f_compiled x y assertEqual compiled_out f x y torch _inductor config patch graph_partition True test_graph_partition_symint_cat_backward f x w y = torch cat x x dim= z = y w z z T compiled_f = torch compile f shape torch manual_seed eager_x = torch randn shape device= cuda eager_w = torch randn device= cuda requires_grad=True torch manual_seed compiled_x = torch randn shape device= cuda compiled_w = torch randn device= cuda requires_grad=True f eager_x eager_w sum backward compiled_f compiled_x compiled_w sum backward assertEqual eager_w grad compiled_w grad dynamo_config patch capture_dynamic_output_shape_ops True config patch implicit_fallbacks=True torch _inductor config patch graph_partition True test_graph_partition_symint_from_nested_indirect_indexing nested x repeats rank = torch arange repeats numel device=x device index = rank repeat_interleave repeats dim= torch index_select x index=index dim= example_inputs = torch randn device= cuda repeats = torch tensor device= cuda torch _dynamo mark_dynamic repeats create backed symint nested_opt = torch compile nested backend= inductor expect = nested example_inputs actual = nested_opt example_inputs assertEqual expect actual torch _inductor config patch graph_partition True test_graph_partition_symint_from_mutation_index x = torch zeros device= cuda fn n n = - opt_fn = torch compile fn fullgraph=True n range x shape opt_fn n x assertEqual x n - Negative index triggers new compilation opt_fn -x shape x assertEqual x - torch _inductor config patch graph_partition True test_graph_partition_unbacked_symint f x y x = x + y = y + y_cpu = y cpu + z = x y x + y + z + y_cpu cuda f_compiled = torch compile f x y = torch ones device= cuda torch randn device= cuda torch _dynamo decorators mark_unbacked x torch _dynamo decorators mark_unbacked y compiled_out = f_compiled x y eager_out = f x y assertEqual compiled_out eager_out torch _inductor config patch graph_partition True test_graph_partition_dynamic_scalar_inputs f x y integer x = x + y = y + y_cpu = y cpu + z = x y z += integer x + y + z + y_cpu cuda f_compiled = torch compile f x y = torch ones device= cuda torch randn device= cuda torch _dynamo decorators mark_unbacked x torch _dynamo decorators mark_unbacked y compiled_out = f_compiled x y assertEqual compiled_out f x y compiled_out = f_compiled x y assertEqual compiled_out f x y torch _inductor config patch graph_partition True torch _dynamo config patch capture_scalar_outputs True test_graph_partition_item f x y = x + scalar = y item x + y + scalar compiled_f = torch compile f compiled_out = compiled_f torch tensor device= cuda assertEqual compiled_out f torch tensor device= cuda torch _inductor config patch graph_partition True test_graph_partition_buffer_reuse f x y x = x + y = y + y_cpu = y cpu + z = x + y + x y u = y_cpu cuda + y + u_cpu = u cpu + z + u_cpu cuda x y = torch ones device= cuda _ range x_cloned y_cloned = tmp clone tmp x y eager_out = f x y f_compiled = torch compile f compiled_out = f_compiled x_cloned y_cloned assertEqual eager_out compiled_out torch _inductor config patch graph_partition True test_graph_partition_fused_scheduler_node foo x x = x x_alias = x y = x y_alias = y torch _dynamo graph_break ind = torch tensor device= cuda x_alias = x ind y_alias = y ind x x_alias x_alias y_alias y_alias compiled_foo = torch compile foo x = torch rand device= cuda eager_out = foo x compiled_out = compiled_foo x assertEqual eager_out compiled_out test_meta_tensor foobar x y x y foo_c = torch compile mode= reduce-overhead foobar t = torch empty device= meta y = torch rand device= cuda eager_out = foobar t y _ range compiled_out = foo_c t y compiled_out = foo_c t y assertEqual eager_out compiled_out assertEqual get_manager new_graph_id id torch _inductor config patch triton cudagraph_capture_sizes test_cudagraph_capture_sizes f x x + f = torch compile f mode= reduce-overhead run shape x = torch randn shape device= cuda torch _dynamo mark_dynamic x _ range f x i range run i assertEqual get_manager new_graph_id id torch _inductor config patch triton cudagraph_capture_sizes test_cudagraph_capture_sizes f x x + f = torch compile f mode= reduce-overhead run batch_size seq_len d x = torch randn batch_size seq_len d device= cuda torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x _ range f x i range j range run i j assertEqual get_manager new_graph_id id torch _inductor config patch triton cudagraph_capture_sizes test_cudagraph_capture_sizes f x x + f = torch compile f mode= reduce-overhead run batch_size seq_len d x = torch randn batch_size seq_len d device= cuda torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x torch _dynamo mark_dynamic x _ range f x i range j range k range run i j k assertEqual get_manager new_graph_id id torch _inductor config patch triton cudagraph_or_error True test_cudagraph_or_error f x x add_ x f = torch compile f mode= reduce-overhead assertRaises RuntimeError f torch tensor device= cuda TestSAC TestCase _make_observer_mode ObserverMode TorchDispatchMode __init__ super __init__ curr_run = op_outputs = defaultdict list __torch_dispatch__ func OpOverload types Sequence type args Sequence object = kwargs Mapping str object = immutable_dict - object func args kwargs ObserverMode test_simple device = cuda torch _prims rng_prims graphsafe_run_with_rng_state ObserverMode = _make_observer_mode graphsafe_run_with_rng_state py_impl ObserverMode _ mode op args kwargs no_dispatch out = graphsafe_run_with_rng_state op args kwargs mode op_outputs op append out out obs = ObserverMode x = torch randn device=device requires_grad=True y = torch randn device=device requires_grad=True _ range torch _dynamo reset gn x y torch sigmoid torch rand_like x y x fn x y x = torch sin x x = torch utils checkpoint checkpoint gn x y use_reentrant=True x = torch sin x x aot_eager_decomp_partition = functools partial aot_eager_decomp_partition_with_mode mode=obs fn = torch compile fn backend=aot_eager_decomp_partition fn x y sum backward assertEqual len obs op_outputs aten rand default i range assertEqual obs op_outputs aten rand default + i obs op_outputs aten rand default + i assertNotEqual obs op_outputs aten rand default obs op_outputs aten rand default test_cudagraph_uneven_forward_backward torch compile cudagraphs difficult test rng updating bc sensitive duration pending backwards etc short repro mimic runtime wrappers integration show updating backward rng state cudagraphs works forward state = torch cuda get_rng_state perm = torch randperm device= cuda state perm backward rng_state current_state = torch cuda get_rng_state torch cuda set_rng_state rng_state cpu perm = torch randperm device= cuda torch cuda set_rng_state current_state perm normal_test state perm = forward repro_perm = backward state perm repro_perm graphsafe_forward perm = torch randperm device= cuda perm graphsafe_backward generator new_state current_state = generator graphsafe_get_state generator graphsafe_set_state new_state perm = torch randperm device= cuda generator graphsafe_set_state current_state perm graph_test generator capture_cuda_graph capture_cuda_graph graph = torch cuda CUDAGraph state should cloned before graph old_state = generator graphsafe_get_state new_state = old_state clone_state capture_cuda_graph state should register graph graph register_generator_state new_state only capturing backward torch cuda graph graph repro_perm = graphsafe_backward generator new_state some number uneven forwards graphsafe_forward graphsafe_forward graphsafe_forward state prior rng invocation state = generator get_state perm = graphsafe_forward new_state set_state state capture_cuda_graph graph replay repro_perm = graphsafe_backward generator new_state perm repro_perm assertEqual normal_test generator = torch cuda default_generators assertEqual graph_test generator capture_cuda_graph=False assertEqual graph_test generator capture_cuda_graph=True test_cpu_and_cuda_rng device = cuda ObserverMode = _make_observer_mode torch _prims rng_prims graphsafe_run_with_rng_state run_and_save_rng_state run_with_rng_state hop graphsafe_run_with_rng_state run_and_save_rng_state run_with_rng_state make_impl hop hop py_impl ObserverMode _ mode args kwargs no_dispatch out = hop args kwargs op = None inp itertools chain args kwargs values isinstance inp torch _ops OpOverload op = inp break assert op None hop run_and_save_rng_state mode op_outputs op append out mode op_outputs op append out out make_impl hop obs = ObserverMode gn x y torch sigmoid torch rand_like x y x gn x x torch randperm x numel device=x device reshape x shape fn x y z x = torch sin x x = torch utils checkpoint checkpoint gn x y use_reentrant=True x = torch sin x z = torch utils checkpoint checkpoint gn z use_reentrant=True x z cuda aot_eager_decomp_partition = functools partial aot_eager_decomp_partition_with_mode mode=obs fn = torch compile fn backend=aot_eager_decomp_partition x = torch randn device=device requires_grad=True y = torch randn device=device requires_grad=True z = torch randn requires_grad=True fn x y z sum backward op aten rand default aten randperm default assertEqual len obs op_outputs op assertEqual obs op_outputs op obs op_outputs op assertEqual obs op_outputs op device type cpu op == aten randperm default cuda parametrize order list itertools permutations test_uneven_forward_backward order device = cuda ObserverMode = _make_observer_mode torch _prims rng_prims graphsafe_run_with_rng_state graphsafe_run_with_rng_state py_impl ObserverMode _ mode op args kwargs no_dispatch out = graphsafe_run_with_rng_state op args kwargs mode op_outputs mode curr_run op append out out obs = ObserverMode gn x y torch sigmoid torch rand_like x y x gn x x torch randperm x numel device=x device reshape x shape fn x y x = torch sin x x = torch utils checkpoint checkpoint gn x y use_reentrant=True x = torch sin x x = torch utils checkpoint checkpoint gn x use_reentrant=True x aot_eager_decomp_partition = functools partial aot_eager_decomp_partition_with_mode mode=obs fn_c = torch compile fn backend=aot_eager_decomp_partition torch manual_seed outs = i range len order obs curr_run = i x = torch randn device=device requires_grad=True y = torch randn device=device requires_grad=True outs append fn_c x y idx order obs curr_run = idx outs idx sum backward run range len order op aten rand default aten randperm default assertEqual len obs op_outputs run op assertEqual obs op_outputs run op obs op_outputs run op run = assertNotEqual obs op_outputs run - op obs op_outputs run op config patch fallback_random=True config patch test_configs graphsafe_rng_func_ignores_fallback_random True _test_cudagraphs_aot_eager_compat_equal device gn x y torch sigmoid torch rand_like x y x fn x y x = torch sin x x = torch utils checkpoint checkpoint gn x y use_reentrant=True x = torch sin x x outs = grads = outs = grads = compile_fns = lambda fn torch compile fn backend= aot_eager_decomp_partition lambda fn torch compile fn mode= reduce-overhead i compile_fn enumerate compile_fns torch manual_seed _ range x = torch randn device=device requires_grad=True y = torch randn device=device requires_grad=True out = compile_fn fn x y torch cuda synchronize out sum backward i == outs append out clone grads append x grad clone y grad clone outs append out clone grads append x grad clone y grad clone assertEqual outs outs assertEqual grads grads assertEqual counters inductor cudagraph_skips test_cudagraphs_aot_eager_compat_equal _test_cudagraphs_aot_eager_compat_equal torch device cuda requires_multigpu test_cudagraphs_aot_eager_compat_equal_device_one _test_cudagraphs_aot_eager_compat_equal torch device cuda config patch graph_partition=True test_graph_partition_cudagraphs_aot_eager_compat_equal _test_cudagraphs_aot_eager_compat_equal torch device cuda requires_multigpu test_multi_device gn x y torch sigmoid torch rand_like x y x fn x y x = torch sin x x = torch utils checkpoint checkpoint gn x y use_reentrant=True x = torch sin x x multi_fn x y b fn x y fn b x = torch randn device= cuda requires_grad=True y = torch randn device= cuda requires_grad=True = torch randn device= cuda requires_grad=True b = torch randn device= cuda requires_grad=True No errors TODO - get graphs logging couldn t figure out how multi_fn_c = torch compile multi_fn backend= aot_eager_decomp_partition out = multi_fn_c x y b out sum backward test_retain_graph device = cuda ObserverMode = _make_observer_mode torch _prims rng_prims graphsafe_run_with_rng_state graphsafe_run_with_rng_state py_impl ObserverMode _ mode op args kwargs no_dispatch out = graphsafe_run_with_rng_state op args kwargs mode op_outputs op append out out obs = ObserverMode gn x y torch sigmoid torch rand_like x y x fn x y x = torch sin x x = torch utils checkpoint checkpoint gn x y use_reentrant=True x = torch sin x x x = torch randn device=device requires_grad=True y = torch randn device=device requires_grad=True aot_eager_decomp_partition = functools partial aot_eager_decomp_partition_with_mode mode=obs fn = torch compile fn backend=aot_eager_decomp_partition out = fn x y sum out backward retain_graph=True out backward assertEqual len obs op_outputs aten rand default assertEqual obs op_outputs aten rand default obs op_outputs aten rand default assertEqual obs op_outputs aten rand default obs op_outputs aten rand default instantiate_parametrized_tests CudaGraphTreeTests instantiate_parametrized_tests TestSAC __name__ == __main__ torch _inductor test_case run_tests TEST_CUDA_GRAPH __name__ == __main__ sys exit raise unittest SkipTest cuda graph test skipped HAS_CUDA_AND_TRITON run_tests needs= filelock