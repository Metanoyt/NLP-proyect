mypy allow-untyped-defs enum inspect numbers types typing warnings collections abc Callable typing Any cast NamedTuple Optional TYPE_CHECKING torch torch _jit_internal boolean_dispatched torch _ops OpOverload OpOverloadPacket _compatibility compatibility TYPE_CHECKING node Argument __all__ = ArgsKwargsPair check_for_mutable_operation get_signature_for_torch_op create_type_hint type_matches normalize_function normalize_module compatibility is_backward_compatible=False ArgsKwargsPair NamedTuple Simple named tuple wrapping args kwargs pairs args tuple Any kwargs dict str Any _manual_overrides dict Callable list inspect Signature = _nonzero_schemas signatures = nonzero pass signatures append inspect signature nonzero nonzero as_tuple bool type ignore no-redef pass signatures append inspect signature nonzero signatures _manual_overrides torch nonzero = _nonzero_schemas _FakeGlobalNamespace __getattr__ name name == torch torch raise RuntimeError Expected torch namespace lookup _type_eval_globals = Tensor torch Tensor Device torch device Layout torch layout number numbers Number Future torch jit Future AnyEnumType enum Enum QScheme torch qscheme __torch__ _FakeGlobalNamespace NoneType type None Storage torch UntypedStorage t typing TypeVar t k dir typing _type_eval_globals k = getattr typing k _torchscript_type_to_python_type ts_type torch _C JitType - Any Convert TorchScript type Python type including subtypes via eval ing annotation_str _type_eval_globals sets up expressions like List Future map actual types typing List jit Future eval ts_type annotation_str _type_eval_globals _torchscript_schema_to_signature_impl ts_schema torch _C FunctionSchema - inspect Signature inspect Parameter parameters list Parameter = arg ts_schema arguments arg_type = _torchscript_type_to_python_type arg type default = arg default_value arg has_default_value Parameter empty TODO Figure out safe It seems like when generating type signatures PythonArgParser we emit signatures ` input ` instead ` ` first tensor argument name Downstream someone converts positional argument keyword argument name mismatch will break things so here we re going normalize name input name = arg name arg name = input kind = Parameter KEYWORD_ONLY arg kwarg_only Parameter POSITIONAL_OR_KEYWORD keyword therefore must POSITIONAL_ONLY argument name == assert kind == Parameter POSITIONAL_OR_KEYWORD ParameterKind type internal implementation detail inspec package which makes hard do type annotation kind = Parameter POSITIONAL_ONLY type ignore assignment This renders all previous arguments positional only idx p enumerate parameters assert p kind == Parameter POSITIONAL_OR_KEYWORD parameters idx = Parameter name=p name kind=Parameter POSITIONAL_ONLY default=p default annotation=p annotation parameters append Parameter name=name kind=kind default=default annotation=arg_type return_types = _torchscript_type_to_python_type ret type ret ts_schema returns len return_types == return_type = None len return_types == return_type = return_types return_type = tuple return_types inspect Signature parameters return_annotation=return_type _SCHEMA_TO_SIGNATURE_CACHE dict tuple str str inspect Signature = _torchscript_schema_to_signature ts_schema torch _C FunctionSchema - inspect Signature Cached s called hot path FakeTensor dispatch cache_key = ts_schema name ts_schema overload_name cache_val = _SCHEMA_TO_SIGNATURE_CACHE get cache_key cache_val None cache_val res = _torchscript_schema_to_signature_impl ts_schema _SCHEMA_TO_SIGNATURE_CACHE cache_key = res res compatibility is_backward_compatible=False check_for_mutable_operation target Callable args tuple Argument kwargs dict str Argument signatures schemas = get_signature_for_torch_op target return_schemas=True signatures schemas matched_schemas = Iterate through all schema until we find one matches If one matches populate ` new_args_and_kwargs ` new args kwargs values If none matches ` new_args_and_kwargs ` will None candidate_signature schema zip signatures schemas try candidate_signature bind args kwargs matched_schemas append candidate_signature schema except TypeError continue throw_if_mutable schema schema is_mutable raise RuntimeError f Tried trace mutable operation schema FX only supports functional f code so operations mutate operands in-place e g via ` out ` arguments f supported len matched_schemas == Did match any schema Cannot check mutation pass len matched_schemas == Matched exactly one schema unambiguous _ schema_to_check = matched_schemas throw_if_mutable schema_to_check Ambiguous schema match Since mutability checking best effort do nothing pass compatibility is_backward_compatible=False get_signature_for_torch_op op Callable return_schemas bool = False Given operator ` torch ` namespace list ` inspect Signature ` objects corresponding overloads op May ` None ` signature could retrieved Args op Callable An operator ` torch ` namespace look up signature Returns Optional List inspect Signature A list signatures overloads operator None operator signatures could retrieved If return_schemas=True returns tuple containing optional Python signatures optional TorchScript Function signature isinstance op OpOverload schemas = op _schema isinstance op OpOverloadPacket schemas = getattr op overload _schema overload op overloads override = _manual_overrides get op override override None return_schemas None aten_fn = torch jit _builtins _find_builtin op aten_fn None None None return_schemas None schemas = torch _C _jit_get_schemas_for_operator aten_fn signatures = _torchscript_schema_to_signature schema schema schemas signatures schemas return_schemas signatures compatibility is_backward_compatible=False create_type_hint x Produces type hint given argument The func ` create_type_hint ` looks type hint compatible input argument ` x ` If ` x ` ` list ` ` tuple ` looks object list whose type superclass rest uses ` base_type ` ` List ` ` Tuple ` returned If no such object found defaults ` List Any ` If ` x ` neither ` list ` nor ` tuple ` returns ` x ` try isinstance x list tuple todo chilli Figure out right way mypy handle isinstance x list ret_type x list x type ignore valid-type ret_type x tuple x type ignore valid-type len x == ret_type Any base_type = x t x issubclass t base_type continue issubclass base_type t base_type = t ret_type Any ret_type base_type except Exception We tried create type hint list failed warnings warn f We able successfully create type hint type x x compatibility is_backward_compatible=False type_matches signature_type Any argument_type Any sig_origin_type = getattr signature_type __origin__ signature_type signature_type argument_type True Union types signature Given type needs match one contained types Union sig_origin_type typing Union signature_type = argument_type sig_contained = signature_type __args__ any type_matches c argument_type c sig_contained getattr signature_type __origin__ None list sig_el_type = signature_type __args__ int can promoted list int argument_type int sig_el_type int True inspect isclass sig_el_type warnings warn f Does support nested parametric types got signature_type Please file bug False getattr argument_type __origin__ None list issubclass argument_type __args__ sig_el_type is_homogeneous_tuple t getattr t __origin__ None tuple False contained = t __args__ t __args__ == Tuple __args__ == some reason True all c Ellipsis issubclass c sig_el_type c contained Tuple T accepted List T parameters is_homogeneous_tuple argument_type Dtype int schemas signature_type int argument_type torch dtype True signature_type numbers Number argument_type int float True inspect isclass argument_type inspect isclass signature_type issubclass argument_type signature_type False compatibility is_backward_compatible=False normalize_function target Callable args tuple Any kwargs Optional dict str Any = None arg_types Optional tuple Any = None kwarg_types Optional dict str Any = None normalize_to_only_use_kwargs bool = False - Optional ArgsKwargsPair Returns normalized arguments PyTorch functions This means ` args kwargs ` will matched up functional s signature exclusively kwargs positional order ` normalize_to_only_use_kwargs ` True Also populates default values Does support positional-only parameters varargs parameters args kwargs Does support modules May require ` arg_types ` ` kwarg_types ` order disambiguate overloads Args target Callable Function we normalizing args Tuple Any Tuple args function kwargs Optional Dict str Any Dict kwargs function arg_types Optional Tuple Any Tuple arg types args kwarg_types Optional Dict str Any Dict arg types kwargs normalize_to_only_use_kwargs bool Whether normalize only use kwargs Returns Returns normalized_args_and_kwargs ` None ` successful kwargs None kwargs = new_args_and_kwargs = None isinstance target types BuiltinFunctionType isinstance target OpOverloadPacket OpOverload hasattr target _op ExecuTorch s EdgeOpOverload wrapper around PyTorch s OpOverload so we can unwrap here get its schema Can t EdgeOpOverload directly because circular dependency so checking _op existing next best thing target = target _op Repeat condition after checking inner _op field isinstance target types BuiltinFunctionType isinstance target OpOverloadPacket OpOverload target_for_analysis = target target boolean_dispatched HACK ` boolean_dispatch ` used ` torch nn functional ` makes so we have -way dispatch based boolean value Here we check ` true ` ` false ` branches dispatch have exactly same signature If they do use ` true ` branch signature analysis Otherwise leave un-normalized assert isinstance target str dispatched = boolean_dispatched target if_true if_false = dispatched if_true dispatched if_false inspect signature if_true parameters = inspect signature if_false parameters None target_for_analysis = if_true assert callable target_for_analysis sig = inspect signature inspect unwrap target_for_analysis new_args_and_kwargs = _args_kwargs_to_normalized_args_kwargs sig args kwargs normalize_to_only_use_kwargs assert callable target torch_op_schemas = get_signature_for_torch_op target matched_schemas = torch_op_schemas Iterate through all schema until we find one matches If one matches populate ` new_args_and_kwargs ` new args kwargs values If none matches ` new_args_and_kwargs ` will None candidate_signature torch_op_schemas try candidate_signature bind args kwargs matched_schemas append candidate_signature except TypeError continue len matched_schemas == Did match any schema Cannot normalize pass len matched_schemas == Matched exactly one schema unambiguous new_args_and_kwargs = _args_kwargs_to_normalized_args_kwargs matched_schemas args kwargs normalize_to_only_use_kwargs arg_types None kwarg_types None arg_types = arg_types arg_types cast tuple Any kwarg_types = kwarg_types kwarg_types candidate_signature torch_op_schemas sig_matches = True try bound_types = candidate_signature bind arg_types kwarg_types arg_name arg_type bound_types arguments items param = candidate_signature parameters arg_name sig_matches = sig_matches type_matches param annotation arg_type except TypeError sig_matches = False sig_matches new_args_and_kwargs = _args_kwargs_to_normalized_args_kwargs candidate_signature args kwargs normalize_to_only_use_kwargs break Matched more than one schema In situation caller must provide types arguments overload they expect schema_printouts = \n join str schema schema matched_schemas raise RuntimeError f Tried normalize arguments torch typename target f schema match ambiguous Please provide argument types f normalize_arguments call Available schemas \n schema_printouts new_args_and_kwargs compatibility is_backward_compatible=False normalize_module root torch nn Module target str args tuple Any kwargs Optional dict str Any = None normalize_to_only_use_kwargs bool = False - Optional ArgsKwargsPair Returns normalized arguments PyTorch modules This means ` args kwargs ` will matched up functional s signature exclusively kwargs positional order ` normalize_to_only_use_kwargs ` True Also populates default values Does support positional-only parameters varargs parameters args kwargs Args root nn Module root module upon which we query modules target Callable Function we normalizing args Tuple Any Tuple args function kwargs Optional Dict str Any Dict kwargs function normalize_to_only_use_kwargs bool Whether normalize only use kwargs Returns Returns normalized_args_and_kwargs ` None ` successful try submod = root get_submodule target except AttributeError e raise RuntimeError f Tried normalize node target target root did f have target e hasattr submod __class__ __name__ classname = submod __class__ __name__ getattr torch nn classname None == submod __class__ sig = inspect signature inspect unwrap submod forward kwargs None kwargs = new_args_and_kwargs = _args_kwargs_to_normalized_args_kwargs sig args kwargs normalize_to_only_use_kwargs new_args_and_kwargs None _args_kwargs_to_normalized_args_kwargs sig inspect Signature args tuple Any kwargs dict str Any normalize_to_only_use_kwargs bool - Optional ArgsKwargsPair Given call target args kwargs arguments normalized into ArgsKwargsPair None type signature supported normalization Args sig inspect Signature Signature object target args Tuple Arguments appear callsite ` target ` kwargs Dict Keyword arguments appear callsite ` target ` normalize_to_only_use_kwargs bool Whether normalize only use kwargs Returns Optional ArgsKwargsPair Normalized args kwargs ` target ` ` None ` target supported Don t currently support positional-only varargs args kwargs signatures supported_parameter_types = inspect Parameter POSITIONAL_OR_KEYWORD inspect Parameter KEYWORD_ONLY any p kind supported_parameter_types p sig parameters values Add exception one signature which common random uniform i e Tensor float from= float to= Generator generator=None ` ` Python keyword such functions signature should have positional-only args same time they could dispatched kwargs list sig parameters keys = input generator None bound_args = sig bind args kwargs bound_args apply_defaults new_kwargs dict str Any = new_args list Any = i param enumerate sig parameters normalize_to_only_use_kwargs i len args new_args append bound_args arguments param new_kwargs param = bound_args arguments param ArgsKwargsPair tuple new_args new_kwargs