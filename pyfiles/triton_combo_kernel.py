itertools logging textwrap collections defaultdict dataclasses dataclass typing Any Callable cast Optional Union sympy sympy Integer Symbol torch utils _ordered_set OrderedSet config metrics runtime hints DeviceProperties runtime runtime_utils next_power_of_ runtime triton_heuristics RoundRobinComboKernelGrid SequentialComboKernelGrid scheduler BaseSchedulerNode utils Placeholder triton_version_uses_attrs_dict virtualized V common ArgName ConstexprArg DeferredLine IndentedBuffer InplacedBuffer Kernel PythonPrinter RemovedArg SizeArg WorkspaceArg simd prefix_is_reduction SIMDScheduling simd_kernel_features SIMDKernelFeatures triton gen_common_triton_imports TritonKernel triton_utils config_of signature_to_meta log = logging getLogger __name__ pexpr = PythonPrinter doprint LARGE_NUMELS = e BLOCK_UTILIZATION = _default_custom_combo_kernel_horizontal_partition nodes list BaseSchedulerNode triton_scheduling SIMDScheduling kernel_map dict BaseSchedulerNode TritonKernel node_info_map dict BaseSchedulerNode tuple Any Any Any Any - list list BaseSchedulerNode Horizontally partition given list nodes into list list nodes where each sublist represents partition Nodes different partitions implemented different combo kernels Nodes same partition likely implemented same combo kernel subject subsequent restrictions like CUDA limits number args Input arguments nodes list fused scheduler nodes partition triton_scheduling TritonScheduling instance kernel_map map node its kernel node_info_map map node node_schedule tiled_groups numel rnumel Output list list nodes each sublist representing partition The default algorithm partition nodes based following rules nodes same number block dimensions grouped together large pointwise nodes numels greater than LARGE_NUMELS separated other nodes large reduce nodes separated other nodes assert len nodes = first partition nodes based number block dimensions tilings = node_info_map n n nodes max_dims = max len t t tilings nodes_per_ndim list list BaseSchedulerNode = i range max_dims + group_per_dim = n n t zip nodes tilings len t == i reduction = n n group_per_dim kernel_map n inside_reduction kernel_map n persistent_reduction kernel_map n no_x_dim not_reduction = n n group_per_dim n reduction rnumel usually has long execution time BaseSchedulerNode group - - rnumel reduction nodes long_reduction = n n reduction V graph sizevars shape_env has_hint n group - - V graph sizevars size_hint n group - - type ignore arg-type short_reduction = n n reduction n long_reduction long_reduction log warning ComboKernels d long reduction nodes separated len long_reduction large_pointwise = n n not_reduction kernel_map n inside_reduction len kernel_map n numels == V graph sizevars shape_env has_hint kernel_map n numels x V graph sizevars size_hint kernel_map n numels x LARGE_NUMELS large_pointwise TODO benchmark performance when large pointwise nodes combining others log warning ComboKernels d large pointwise nodes separated len large_pointwise not_reduction = n n not_reduction n large_pointwise nodes_per_ndim extend node node large_pointwise nodes_per_ndim extend g g not_reduction short_reduction long_reduction g assert sum len p p nodes_per_ndim == len nodes nodes_per_ndim _custom_combo_kernel_horizontal_partition_algorithm Callable list BaseSchedulerNode SIMDScheduling dict BaseSchedulerNode TritonKernel dict BaseSchedulerNode tuple Any Any Any Any list list BaseSchedulerNode = _default_custom_combo_kernel_horizontal_partition set_custom_combo_kernel_horizontal_partition algorithm Callable list BaseSchedulerNode SIMDScheduling dict BaseSchedulerNode TritonKernel dict BaseSchedulerNode tuple Any Any Any Any list list BaseSchedulerNode - None Sets algorithm used partition nodes into horizontal partitions Nodes different partitions implemented different combo kernels Nodes same partition likely implemented same combo kernel subject subsequent restricts like CUDA limits number args The algorithm should take list nodes list list nodes The default algorithm partition nodes based number block dimensions global _custom_combo_kernel_horizontal_partition_algorithm _custom_combo_kernel_horizontal_partition_algorithm = algorithm dataclass PartitionState partitions list list BaseSchedulerNode cur_partition list BaseSchedulerNode cur_count int finalize - None cur_partition partitions append cur_partition ComboKernel Kernel staticmethod _update_partition partition_state PartitionState node_rw_count int node_info BaseSchedulerNode - None partition_state cur_count + node_rw_count config combo_kernel_max_num_args partition_state partitions append partition_state cur_partition partition_state cur_partition = node_info partition_state cur_count = node_rw_count partition_state cur_count += node_rw_count partition_state cur_partition append node_info staticmethod _base_horizontal_partition subkernel_nodes list BaseSchedulerNode triton_scheduling SIMDScheduling node_info_map dict BaseSchedulerNode tuple Any Any Any Any custom_algorithm bool - list list BaseSchedulerNode Generates list lists node info tuples which consist fused_nodes tiling numel rnumel each subkernel node where each sublist guaranteed exceed CUDA limits number args read writes have same D D blocking strategy TODO support combination kernels different block dimensions assert len subkernel_nodes = mixed_sizes = config combo_kernel_allow_mixed_sizes config combo_kernel_allow_mixed_sizes == custom_algorithm ndim_to_partition_state dict int PartitionState = defaultdict lambda PartitionState yelem_to_partition_state dict int PartitionState = defaultdict lambda PartitionState node subkernel_nodes _node_schedule tiled_groups _numel _rnumel = node_info_map node node_info = node read_writes = node read_writes read_write_count = len read_writes reads + len read_writes writes ndim = len tiled_groups assert ndim = f Combokernel support tile tiled_groups mixed_sizes ndim == y_elem = tiled_groups y partition_state = yelem_to_partition_state y_elem ComboKernel _update_partition partition_state read_write_count node_info assert mixed_sizes ndim = f No mixed sizes tile tiled_groups partition_state = ndim_to_partition_state ndim ComboKernel _update_partition partition_state read_write_count node_info all_partitions = partition_state ndim_to_partition_state values partition_state finalize all_partitions extend partition_state partitions partition_state yelem_to_partition_state values partition_state finalize all_partitions extend partition_state partitions all_partitions staticmethod horizontal_partition nodes list BaseSchedulerNode triton_scheduling SIMDScheduling kernel_map dict BaseSchedulerNode TritonKernel node_info_map dict BaseSchedulerNode tuple Any Any Any Any custom_algorithm bool = False - list list BaseSchedulerNode Generates list lists node info tuples which consist fused_nodes tiling numel rnum each subkernel node where each sublist forms ComboKernel It horizontally partitions nodes into sublists following way call _custom_combo_kernel_horizontal_partition_algorithm custom_algorithm True then call _base_horizontal_partition partition nodes into sublists each sublist guaranteed exceed CUDA limits number args read writes have same D D blocking strategy custom_algorithm raw_partitions = _custom_combo_kernel_horizontal_partition_algorithm nodes triton_scheduling kernel_map node_info_map raw_partitions = nodes Generates list lists node info tuples which consist fused_nodes tiling numel rnumel each subkernel node where each sublist guaranteed exceed CUDA limits number args read writes have same D D blocking strategy all_partitions = raw_partition raw_partitions all_partitions extend ComboKernel _base_horizontal_partition raw_partition triton_scheduling node_info_map custom_algorithm all_partitions SequentialDispatch The dispatcher which dispatches subkernels sequential manner blocks first dispatched st subkernel until filled then nd subkernel so The defines methods specific dispatch algorithm Methods codegen_pid_range codegen pid range each subkernel grid codegen grid size launching combo kernel grid_expr = SequentialComboKernelGrid classmethod codegen_pid_range cls kernel ComboKernel num int code IndentedBuffer - None num == cls _calculate_xblocks kernel code code splice f pid num_xblocks_ num code indent code splice pid_offset = pid code splice f pid num_xblocks_ num code indent code splice f pid_offset = pid - num_xblocks_ num - classmethod _calculate_xblocks cls kernel ComboKernel code IndentedBuffer - None x_numels_list = kernel x_numels_list i range len x_numels_list xnumels no_x_dim = x_numels_list i False isinstance x_numels_list i str cast str x_numels_list i = - isinstance x_numels_list i int cast int x_numels_list i kernel min_x_blocks_list i True xblock_str = f tl cdiv xnumels XBLOCK no_x_dim f xnumels i == code splice f num_xblocks_ i = xblock_str code splice f num_xblocks_ i = num_xblocks_ i - + xblock_str RoundRobinDispatch The dispatcher which dispatches subkernels round robin manner blocks interleavedly dispatched each subkernel execute them parallel The defines methods specific dispatch algorithm Methods codegen_pid_range codegen pid range each subkernel grid codegen grid size launching combo kernel grid_expr = RoundRobinComboKernelGrid classmethod codegen_pid_range cls kernel ComboKernel num int code IndentedBuffer - None num_kernels = len kernel sub_kernels num == cond = cond = code splice f cond pid num_kernels == num code indent code splice f pid_offset = pid num_kernels __init__ enable_autotune bool = False mixed_sizes bool = False - None super __init__ sub_kernels list TritonKernel = iter_vars_count = itertools count grids list list int = min_x_blocks_list list Union int str = x_numels_list list Union int str = enable_autotune = enable_autotune mixed_sizes = mixed_sizes dispatch_class Optional type Union ComboKernel SequentialDispatch ComboKernel RoundRobinDispatch = None block_args list str = there following used when autotuning disabled block_size_ d = Try tuning value block_size_ d = num_warps = block_size_reduce = dynamic_shape_args list str = create_sub_kernel triton_kernel TritonKernel - TritonKernel sub_kernel = triton_kernel pyrefly ignore bad-assignment metrics generated_kernel_count -= sub_kernel args = args sub_kernel iter_vars_count = iter_vars_count sub_kernel cse iter_buffer_ids = cse iter_buffer_ids sub_kernels append sub_kernel sub_kernel staticmethod create_triton_kernel tiling dict str sympy Expr features SIMDKernelFeatures optimize_mask bool - TritonKernel Only allow optimize_mask=True when sequential dispatch used numels except x dimension same each sub kernel TritonKernel tiling features=features pid_cache= tl program_id pid_offset optimize_mask=optimize_mask foreach kernels don t work cooperative reductions override_cooperative_reduction=False codegen_static_numels_sub_kernel code IndentedBuffer sub_kernel TritonKernel num int - list str We get small speedup hard coding numels they static This code stomps passed-in values writing constant top kernel In kernel like KERNEL_NAME in_ptr in_ptr out_ptr xnumel rnumel XBLOCK tl constexpr R _BLOCK tl constexpr We would add xnumel = rnumel = After signature before kernel code we decided make these static As its hardcoded becomes better signal triton how unroll do some static indexing So s so much downstream knows its static numel you just plop constant into kernel grid = uniquify_block_sizes = tree sub_kernel range_trees simplified_tree_numel = V graph sizevars simplify tree numel isinstance simplified_tree_numel Integer int code writeline f tree prefix numel = int simplified_tree_numel assert f tree prefix numel_ num dynamic_shape_args uniquify_block_sizes append f tree prefix numel pyrefly ignore missing-argument tree is_reduction isinstance simplified_tree_numel Integer int grid append int simplified_tree_numel pyrefly ignore bad-argument-type grid append f tree prefix numel_ num tree is_reduction sub_kernel persistent_reduction isinstance simplified_tree_numel Integer int val = int simplified_tree_numel raise RuntimeError Dynamic shape reduction dimension supported val = next_power_of_ val code writeline f RBLOCK_ num tl constexpr = val code writeline f R _BLOCK_ num tl constexpr = val uniquify_block_sizes append R _BLOCK tree prefix == x sub_kernel no_x_dim code writeline f XBLOCK_ num tl constexpr = uniquify_block_sizes append XBLOCK grids append grid uniquify_block_sizes min_x_blocks_sub_kernel sub_kernel TritonKernel num int - None Kernels no_x_dim being true has no tunable XBLOCK They have fixed number X blocks Grid calculation needs make sure they assigned enough number blocks min_x_blocks Union int str = x_numels Union int str = tree sub_kernel range_trees simplified_tree_numel = V graph sizevars simplify tree numel tree prefix == x isinstance simplified_tree_numel Integer int x_numels = int simplified_tree_numel x_numels = f tree prefix numel_ num sub_kernel no_x_dim min_x_blocks = x_numels x_numels = pyrefly ignore unsupported-operation -min_x_blocks isinstance x_numels int pyrefly ignore redundant-cast - + cast str x_numels isinstance simplified_tree_numel Integer int x_numels = int simplified_tree_numel x_numels = f tree prefix numel_ num min_x_blocks_list append min_x_blocks x_numels_list append x_numels select_heuristics sub_kernel TritonKernel - tuple str dict str int size_hints = prefix next_power_of_ V graph sizevars size_hint numel fallback=config unbacked_symint_fallback prefix numel sub_kernel numels items prefix_is_reduction prefix sub_kernel inside_reduction sub_kernel persistent_reduction assert sub_kernel inside_reduction heuristics = persistent_reduction sub_kernel inside_reduction heuristics = reduction heuristics = pointwise heuristics size_hints select_combo_heuristics heuristics_list list str size_hints_list list dict str int - tuple str dict str int TritonKernel enable_autotune foreach size_hints_list sub_kernels reduction heuristics_list i _ = max enumerate size_hints_list key=lambda x x x heuristics_list x == reduction heuristics_list i size_hints_list i sub_kernels i pointwise heuristics_list i _ = max enumerate size_hints_list key=lambda x x x heuristics_list x == pointwise modify size_hint avoid oom check fail may false alarm num_pointwise = len e e heuristics_list e == pointwise num_reduction = len e e heuristics_list e == reduction num_persistent_reduction = len e e heuristics_list e == persistent_reduction assert num_reduction == combining pointwise reduction supported yet heuristics = pointwise_with_reduction num_persistent_reduction pointwise len heuristics_list - num_pointwise = size_hints = size_hints_list i size_hints x = min size_hints x heuristics size_hints_list i sub_kernels i heuristics_list size_hints_list sub_kernels get_mutated_args_sub_kernels - list str mutated_args OrderedSet str = OrderedSet sub_kernel sub_kernels mutation sub_kernel mutations mutation sub_kernel args input_buffers mutated_args add sub_kernel args input_buffers mutation mutation sub_kernel args inplace_buffers mutation V graph removed_buffers mutation sub_kernel removed_buffers mutated_args add cast InplacedBuffer sub_kernel args inplace_buffers mutation inner_name mutation sub_kernel args output_buffers arg = sub_kernel args output_buffers mutation assert isinstance arg RemovedArg mutated_args add arg sorted mutated_args select_dispatch_strategy - None dispatch_class None mixed_sizes used optimize_mask so only allows sequential dispatch Not mixed sizes y dim technically ok use round robin wells mixed_sizes any isinstance e str e x_numels_list str x_numels_list means dynamic shape dispatch_class = ComboKernel SequentialDispatch A negative x_blocks_list element means kernel tunable i e no_x_dim = True x_numels_list = abs cast int e e x_numels_list total = max x_numels_list len x_numels_list needed = sum x_numels_list needed total BLOCK_UTILIZATION Introduced overhead masked blocks less than dispatch_class = ComboKernel RoundRobinDispatch dispatch_class = ComboKernel SequentialDispatch jit_line heuristics str size_hints dict str int selected_kernel TritonKernel signature list Any argdefs list ArgName pointwise_with_reduce bool = False - str can_use_ bit = all k index_dtype == tl int k sub_kernels size_dtype = tl int can_use_ bit tl int i sub enumerate sub_kernels min_x_blocks_sub_kernel sub i select_dispatch_strategy triton_meta = signature signature_to_meta signature size_dtype=size_dtype argdefs=argdefs device DeviceProperties create V graph get_current_device_or_throw constants pyrefly ignore unsupported-operation triton_meta configs = config_of signature mutated_args = get_mutated_args_sub_kernels dispatch = dispatch_class assert dispatch None inductor_meta = grid_type dispatch grid_expr __name__ combo_grid_meta combo_grid_meta kernel_name str Placeholder DESCRIPTIVE_NAME mutated_arg_names mutated_args TritonKernel inductor_meta_common sub_kernel = selected_kernel heuristics == foreach heuristics_line = f triton_heuristics foreach num_warps= num_warps triton_meta= triton_meta r inductor_meta= inductor_meta r triton jit sub_kernel inside_reduction reduction_hint = sub_kernel features get_reduction_hint heuristics_line = f triton_heuristics heuristics size_hints= size_hints r reduction_hint= reduction_hint filename=__file__ triton_meta= triton_meta r inductor_meta= inductor_meta r triton jit tile_hint = len size_hints == tile_hint = tile_hint=TileHint SQUARE tile_hint = tile_hint=TileHint DEFAULT heuristics_line = f triton_heuristics heuristics size_hints= size_hints r tile_hint filename=__file__ triton_meta= triton_meta r inductor_meta= inductor_meta r triton jit heuristics_line codegen_blocks code IndentedBuffer - None block block_args assert block XBLOCK YBLOCK R _BLOCK f block supported without autotuning YBLOCK block_args code splice f XBLOCK tl constexpr = block_size_ d code splice f YBLOCK tl constexpr = block_size_ d code splice f XBLOCK tl constexpr = block_size_ d R _BLOCK block_args code splice f R _BLOCK tl constexpr = block_size_reduce code splice f RBLOCK tl constexpr = block_size_reduce get_block_args - list ConstexprArg Calculate blocks sub_kernels range_trees Update block_args Return block args block_names = sub_kernel sub_kernels TODO we assume all sub_kernels have same block size tree sub_kernel range_trees pyrefly ignore missing-argument tree is_reduction sub_kernel inside_reduction sub_kernel persistent_reduction continue tree prefix == x sub_kernel no_x_dim continue block_names f tree prefix upper BLOCK = tree prefix block_args = list block_names keys ConstexprArg x x block_names keys add_numel_to_args argdefs list ArgName signature list Any - list ArgName num sub_kernel enumerate sub_kernels tree sub_kernel active_range_trees isinstance tree numel Integer int only dynamic shape sizearg = SizeArg f tree prefix numel_ num tree numel signature append sizearg argdefs append ArgName f tree prefix numel_ num dynamic_shape_args append f tree prefix numel_ num argdefs add_numel_to_call_args name str call_args list Any arg_types list Any - None num sub_kernel enumerate sub_kernels tree sub_kernel range_trees numel_name = f tree prefix numel_ num numel_name dynamic_shape_args continue isinstance tree numel Integer Symbol expr = tree numel expr = V graph wrapper_code generate_numel_expr name tree suffix=str num pyrefly ignore missing-argument tree is_reduction sub_kernel inside_reduction call_args append expr arg_types append type expr kernel_benchmark_extra_args - list str extra_args = num sub_kernel enumerate sub_kernels tree sub_kernel range_trees numel_name = f tree prefix numel_ num numel_name dynamic_shape_args continue pyrefly ignore missing-argument tree is_reduction sub_kernel inside_reduction extra_args append str V graph sizevars size_hint tree numel fallback=config unbacked_symint_fallback extra_args codegen_kernel name Optional str = None - str TODO correct use first sub kernel s heuristics heuristics_list size_hints_list = subkernel sub_kernels h s = select_heuristics subkernel heuristics_list append h size_hints_list append s heuristics size_hints selected_kernel = select_combo_heuristics heuristics_list size_hints_list pointwise_with_reduction heuristics = True pointwise heuristics == pointwise_with_reduction False heuristics code = IndentedBuffer code splice gen_common_triton_imports config benchmark_combo_kernel code splice imports_for_benchmark_kernel seen_helpers OrderedSet str = OrderedSet sub_kernel sub_kernels helper sub_kernel helper_functions helper seen_helpers code writeline code splice helper seen_helpers add helper argdefs _ signature _ = args python_argdefs argdefs = add_numel_to_args argdefs signature block_args = get_block_args enable_autotune argdefs extend ArgName x name is_constexpr=True x block_args triton_version_uses_attrs_dict signature extend block_args code splice jit_line heuristics size_hints selected_kernel pointwise_with_reduce=pointwise_with_reduction signature=signature argdefs=argdefs code writeline f name str Placeholder KERNEL_NAME join x full_name x argdefs code indent code splice pid = tl program_id enable_autotune codegen_blocks code num sub_kernel enumerate sub_kernels assert dispatch_class None dispatch_class codegen_pid_range num code code indent uniquify = codegen_static_numels_sub_kernel code sub_kernel num sub_kernel codegen_body uniquified_body = uniquify_block_sizes sub_kernel body num uniquify code splice uniquified_body code splice code indent code splice pass config benchmark_combo_kernel code splice codegen_kernel_benchmark num_gb= code getvalue codegen_kernel_benchmark num_gb float - IndentedBuffer Generates Python code benchmarking combo kernel - Creates example inputs random tensors constants sizes - Runs kernel current GPU stream - Prints runtime ms throughput GB s using ` num_gb ` Args num_gb float The number gigabytes use throughput calculation Returns IndentedBuffer A buffer containing generated Python benchmark code result = IndentedBuffer _argdefs call_args signature _ = args python_argdefs result writelines get_args result indent name_cnt = itertools count var_names = arg_name arg_sig zip call_args signature var_name = f arg_ next name_cnt buf = V graph try_get_buffer arg_name buf size = V graph sizevars size_hints buf get_size fallback=config unbacked_symint_fallback stride = V graph sizevars size_hints buf get_stride fallback=config unbacked_symint_fallback result writeline f var_name = rand_strided size stride device= buf get_device dtype= buf get_dtype noqa B line too long arg_name V graph constants note random seed put V graph constants const_tensor = V graph constants arg_name size = V graph sizevars size_hints const_tensor size fallback=config unbacked_symint_fallback stride = V graph sizevars size_hints const_tensor stride fallback=config unbacked_symint_fallback result writeline f var_name = rand_strided size stride device= const_tensor device dtype= const_tensor dtype type ignore arg-type noqa B line too long isinstance arg_sig SizeArg symval_hint = V graph sizevars size_hint arg_sig expr Force seed_offset so calls same kernel using different seed offset will have same benchmark harness We can dedup kernel definitions case seed_offset arg_sig name symval_hint = result writeline f var_name = symval_hint isinstance arg_sig WorkspaceArg device = V graph get_current_device_or_throw count = V graph sizevars size_hint arg_sig count benchmark harness we ignore arg_sig zero_mode always zero result writeline f var_name = torch zeros count device= device dtype= arg_sig dtype raise KeyError f Don t find buffer const tensor arg_name var_names append var_name dynamic_shape_args var_names extend kernel_benchmark_extra_args result writeline f join var_names result writelines \n \n call args index = V graph get_current_device_or_throw index result indent result writeline f V graph device_ops device_guard index result indent result writeline V graph device_ops set_device index no-op ensure context stream_name = f stream index result writeline f stream_name = get_raw_stream index result writeline f str Placeholder KERNEL_NAME run args stream= stream_name benchmark all configs result writelines \n \n benchmark_all_configs args result indent result writeline f V graph device_ops device_guard index result indent result writeline V graph device_ops set_device index no-op ensure context result writeline f str Placeholder KERNEL_NAME benchmark_all_configs args result writelines \n \n __name__ == __main__ result indent result writeline torch _inductor runtime benchmarking benchmarker result writeline result writeline args = get_args result writeline ms = benchmarker benchmark_gpu lambda call args rep= result writeline f num_gb = num_gb result writeline gb_per_s = num_gb ms e result writeline print f ms f ms num_gb f GB gb_per_s f GB s result imports_for_benchmark_kernel - str textwrap dedent torch _dynamo testing rand_strided torch format V graph device_ops import_get_raw_stream_as get_raw_stream uniquify_block_sizes code IndentedBuffer num_kernel int uniquify list str - IndentedBuffer uniquify code modified = IndentedBuffer initial_indent=code _indent line code _lines isinstance line str blocks = e e uniquify e line modified_line = line block blocks modified_line = modified_line replace block f block _ num_kernel modified writeline modified_line isinstance line DeferredLine blocks = e e uniquify e line line modified_line = line line block blocks modified_line = modified_line replace block f block _ num_kernel new_line = DeferredLine line name modified_line modified writeline new_line modified writeline line modified call_kernel code IndentedBuffer name str - None _ call_args _ arg_types = args python_argdefs wrapper = V graph wrapper_code assert dispatch_class None dynamic_shape_args add_numel_to_call_args name call_args arg_types wrapper generate_kernel_call name call_args triton=True arg_types=arg_types combo_grid_meta - dict str Any dynamic_shape = bool dynamic_shape_args num_kernels = len sub_kernels min_blocks = max min_x_blocks_list num_kernels dynamic_shape None enable_autotune YBLOCK block_args default_config = XBLOCK block_size_ d YBLOCK block_size_ d default_config = XBLOCK block_size_ d default_config = None meta = num_kernels num_kernels min_blocks min_blocks default_config default_config num sub_kernel enumerate sub_kernels meta f no_x_dim_ num = sub_kernel no_x_dim tree sub_kernel range_trees pyrefly ignore missing-argument tree is_reduction numel_name = f tree prefix numel_ num numel_name dynamic_shape_args meta numel_name = None meta numel_name = int V graph sizevars simplify tree numel meta