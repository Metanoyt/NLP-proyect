torch ops aten operators under ` core ` module mypy disable-error-code= misc arg-type type-arg valid-type assignment return-value type-var operator no-untyped-def index pyrefly ignore-errors ruff noqa TCH TCH flake noqa B __future__ annotations typing Optional Sequence TYPE_CHECKING onnxscript onnx_opset type ignore attr-defined opset op opset op opset op torch torch onnx _internal _lazy_import onnxscript_ir ir torch onnx _internal exporter _torchlib _tensor_typing TFloat TReal torch onnx _internal exporter _torchlib _torchlib_registry onnx_impl TYPE_CHECKING onnxscript values Opset aten = torch ops aten onnx_impl aten gelu default trace_only=True opset_introduced= aten_gelu_opset TReal approximate str = none - TReal gelu Tensor bool approximate=False - Tensor op Gelu approximate=approximate onnx_impl aten group_norm default trace_only=True opset_introduced= aten_group_norm input TFloat num_groups int weight Optional TFloat = None bias Optional TFloat = None eps float = e- cudnn_enabled bool = True - TFloat group_norm Tensor input int num_groups Tensor weight=None Tensor bias=None float eps= e- bool cudnn_enabled=True - Tensor c = op Shape input start= end= weight None weight = op ConstantOfShape c value=ir tensor dtype=input dtype bias None bias = op ConstantOfShape c value=ir tensor dtype=input dtype op GroupNormalization input weight bias epsilon=eps num_groups=num_groups onnx_impl aten rms_norm default trace_only=True opset_introduced= aten_rms_norm input TFloat normalized_shape Sequence int weight Optional TFloat = None eps Optional float = None - TFloat rms_norm Tensor input SymInt normalized_shape Tensor weight=None float eps=None - Tensor Default eps value provided eps None eps = torch finfo torch float eps Observed decomp Calculate axis first normalization dimension For normalized_shape D dimensions normalize over last D dimensions Since ONNX RMSNormalization supports negative axis values we use -len normalized_shape which correctly maps first axis normalized dimensions normalized_dims = len normalized_shape axis = -normalized_dims Create weight tensor provided weight None weight = op ConstantOfShape op Shape input value=ir tensor dtype=input dtype op RMSNormalization input weight axis=axis epsilon=eps onnx_impl aten scaled_dot_product_attention default trace_only=True opset_introduced= aten_scaled_dot_product_attention_ query TFloat key TFloat value TFloat attn_mask Optional TFloat = None dropout_p float = is_causal bool = False scale Optional float = None enable_gqa bool = False - TFloat scaled_dot_product_attention Tensor query Tensor key Tensor value Tensor attn_mask=None float dropout_p= bool is_causal=False float scale=None bool enable_gqa=False - Tensor Reference https pytorch org docs stable generated torch nn functional scaled_dot_product_attention html https onnx ai onnx operators onnx__Attention html Attempts convert SDPA Attention onnx op fallbacks onnx graph equivalent following PyTorch code scale_factor = math sqrt Q size - scale None scale attn_mask = torch ones L S dtype=torch bool tril diagonal= is_causal attn_mask attn_mask = attn_mask masked_fill attn_mask -float inf attn_mask dtype == torch bool attn_mask attn_weight = torch softmax Q K transpose - - attn_mask dim=- attn_weight = torch dropout attn_weight dropout_p attn_weight V where Q K V query key value tensors respectively L target sequence length S source sequence length E embedding size assert is_causal is_causal attn_mask None is_causal attn_mask cannot set same time assert len query shape == len key shape == len value shape == only D query key value supported Attention onnx op can only handle non-training scenarios where dropout disabled dropout_p == enable_gqa assert query shape key shape == value shape query shape key shape == SDPA GQA MQA requires q_num_heads kv_num_heads q_num_heads kv_num_heads == assert query shape == key shape == value shape SDPA MHA requires q_num_heads = kv_num_heads NOTE num_heads attributes q_num_heads kv_num_heads should specified D They populated D inputs because information directly comes input shapes ` q_num_heads=query shape ` ` kv_num_heads=key shape ` This dimension usually static could dynamic also given attribute num_heads attributes needed D attention inputs shape B S N H D shape B N S H Y _ _ _ = op Attention query key value attn_mask=attn_mask scale=scale is_causal=is_causal Y scale None scale = _attention_scale query op scale = op CastLike scale query is_causal attn_mask = _causal_attention_mask query key op enable_gqa key value = _attention_repeat_kv_for_group_query query key value op attn_mask None _aten_scaled_dot_product_attention_no_mask_onnx query key value scale dropout_p op _aten_scaled_dot_product_attention_float_mask_onnx query key value attn_mask scale dropout_p op _attention_repeat_kv_for_group_query query TFloat key TFloat value TFloat op Opset - tuple TFloat TFloat Expand key value group query attention repeat_interleave applied key value match number heads query Args query Tensor shape B q_num_heads q_S E key Tensor shape B k_num_heads kv_S E value Tensor shape B v_num_heads kv_S E Returns Tuple expanded_key expanded_value where - expanded_key Tensor shape B q_num_heads kv_S E - expanded_value Tensor shape B q_num_heads kv_S E assert query shape key shape == value shape query shape key shape == SDPA GQA MQA requires q_num_heads kv_num_heads q_num_heads kv_num_heads == NOTE QKV expected D tensors batch_size = op Shape query start= end= B q_num_heads = op Shape query start= end= Hq kv_num_heads = op Shape key start= end= Hk qk_head_size = op Shape key start= end= Dk v_head_size = op Shape value start= end= Dv new_kv_seq_len = op Shape key start= end= T interleave_dim = op Div q_num_heads kv_num_heads Hq Hk two = op Constant value_int= k_unsqueezed = op Unsqueeze key two B Hk T Dk v_unsqueezed = op Unsqueeze value two B Hv T Dv k_expand_shape = op Concat batch_size kv_num_heads interleave_dim new_kv_seq_len qk_head_size axis= k_expand = op Expand k_unsqueezed k_expand_shape v_expand_shape = op Concat batch_size kv_num_heads interleave_dim new_kv_seq_len v_head_size axis= v_expand = op Expand v_unsqueezed v_expand_shape k_attention_shape = op Concat batch_size q_num_heads new_kv_seq_len qk_head_size axis= v_attention_shape = op Concat batch_size q_num_heads new_kv_seq_len v_head_size axis= expanded_key = op Reshape k_expand k_attention_shape expanded_value = op Reshape v_expand v_attention_shape expanded_key expanded_value _attention_scale query TFloat op Opset - TFloat Calculate scale factor attention result Args query Tensor shape L E Returns Scalar scale factor = math sqrt query size - q_shape = op Shape query q_last_dim = op Gather q_shape op Constant value_ints= - embedding_size = op CastLike q_last_dim query one = op Constant value_float= cast_one = op CastLike one query scale = op Div cast_one op Sqrt embedding_size scale _causal_attention_mask query TFloat key TFloat op Opset - TFloat Create causal mask given query key tensors Equivalent mask = torch ones L S dtype=torch bool tril diagonal= attn_mask = torch zeros L S dtype=torch float attn_mask = attn_mask masked_fill mask -float inf Args query Tensor shape L E key Tensor shape S E Returns Tensor shape L S q_shape = op Shape query k_shape = op Shape key target_length = op Slice q_shape op Constant value_ints= - op Constant value_ints= - source_length = op Slice k_shape op Constant value_ints= - op Constant value_ints= - attn_mask = torch ones L S = size = op Concat target_length source_length axis= attn_mask = op Expand op Constant value_float= size attn_mask = op Trilu attn_mask upper= The causal mask has s lower triangle -inf upper triangle attn_mask = op Where op Equal attn_mask op Constant value_float= op Constant value_float=-float inf op Constant value_float= attn_mask = op CastLike attn_mask query attn_mask _aten_scaled_dot_product_attention_no_mask_onnx query TFloat key TFloat value TFloat scale TFloat dropout_p float op Opset - TFloat Swap last two axes key key_last_dim = op Shape key start=- key_second_last_dim = op Shape key start=- end=- key_first_dims = op Shape key end=- Contract dimensions last two so we can transpose static permutation key_squeezed_shape = op Concat op Constant value_ints= - key_second_last_dim key_last_dim axis= key_squeezed = op Reshape key key_squeezed_shape key_squeezed_transposed = op Transpose key_squeezed perm= key_transposed_shape = op Concat key_first_dims key_last_dim key_second_last_dim axis= key_transposed = op Reshape key_squeezed_transposed key_transposed_shape https github com pytorch pytorch blob da c b c fda bce aten src ATen native transformers attention cpp#L Scale q k before matmul stability see https tinyurl com sudb s math query_scaled = op Mul query op Sqrt scale key_transposed_scaled = op Mul key_transposed op CastLike op Sqrt scale key_transposed attn_weight = op Softmax op MatMul query_scaled key_transposed_scaled axis=- attn_weight _ = op Dropout attn_weight dropout_p op MatMul attn_weight value _aten_scaled_dot_product_attention_float_mask_onnx query TFloat key TFloat value TFloat attn_mask TFloat scale TFloat dropout_p float op Opset - TFloat Swap last two axes key key_last_dim = op Shape key start=- key_second_last_dim = op Shape key start=- end=- key_first_dims = op Shape key end=- Contract dimensions last two so we can transpose static permutation key_squeezed_shape = op Concat op Constant value_ints= - key_second_last_dim key_last_dim axis= key_squeezed = op Reshape key key_squeezed_shape key_squeezed_transposed = op Transpose key_squeezed perm= key_transposed_shape = op Concat key_first_dims key_last_dim key_second_last_dim axis= key_transposed = op Reshape key_squeezed_transposed key_transposed_shape https github com pytorch pytorch blob da c b c fda bce aten src ATen native transformers attention cpp#L Scale q k before matmul stability see https tinyurl com sudb s math query_scaled = op Mul query op Sqrt scale key_transposed_scaled = op Mul key_transposed op Sqrt scale attn_weight = op Softmax op Add op MatMul query_scaled key_transposed_scaled attn_mask axis=- attn_weight _ = op Dropout attn_weight dropout_p op MatMul attn_weight value