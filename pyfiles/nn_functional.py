Neural network functional operator implementations math random typing Optional torch torchfuzz operators base Operator torchfuzz tensor_fuzzer Spec TensorSpec is_float_dtype dtype torch dtype - bool Check dtype floating point type dtype torch float torch float torch float torch bfloat EmbeddingOperator Operator Operator torch nn functional embedding __init__ super __init__ torch nn functional embedding property torch_op_name - Optional str Return torch operation name torch nn functional embedding can_produce output_spec Spec - bool Embedding can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False Embedding needs least dimension embedding_dim len output_spec size == False Embedding outputs typically float tensors is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs embedding operation Embedding requires - weight tensor num_embeddings embedding_dim - input tensor integer indices any shape output shape + embedding_dim isinstance output_spec TensorSpec raise ValueError EmbeddingOperator can only produce TensorSpec outputs Output shape should input_shape + embedding_dim len output_spec size == raise ValueError Embedding output must have least dimension embedding_dim = output_spec size - input_shape = output_spec size - Remove last dimension embedding_dim Generate reasonable vocab size s larger than our index generation range This ensures indices generated range will always valid num_embeddings = random randint Always larger than max index Weight tensor num_embeddings embedding_dim weight_spec = TensorSpec size= num_embeddings embedding_dim stride= embedding_dim dtype=output_spec dtype Input tensor integer indices shape produces output shape input_spec = TensorSpec size=input_shape stride=self _calculate_stride input_shape dtype=torch int Indices typically int weight_spec input_spec _calculate_stride size Calculate stride given size size stride = current_stride = dim_size reversed size stride append current_stride current_stride = dim_size tuple reversed stride codegen output_name str input_names list str output_spec Spec - str Generate code embedding operation len input_names = raise ValueError Embedding requires exactly inputs weight input weight_name input_name = input_names Ensure indices integer type clamped valid range This handles any arithmetic operations might produce out-of-bounds indices f output_name = torch nn functional embedding torch clamp input_name torch int weight_name size - weight_name LinearOperator Operator Operator torch nn functional linear __init__ super __init__ torch nn functional linear property torch_op_name - Optional str Return torch operation name torch nn functional linear can_produce output_spec Spec - bool Linear can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False Linear needs least dimension output features len output_spec size == False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs linear operation Linear transformation y = xW^T + b - input in_features - weight out_features in_features - bias out_features optional - output out_features isinstance output_spec TensorSpec raise ValueError LinearOperator can only produce TensorSpec outputs len output_spec size == raise ValueError Linear output must have least dimension out_features = output_spec size - batch_shape = output_spec size - Generate reasonable input features size in_features = random randint Input tensor in_features input_shape = batch_shape + in_features input_spec = TensorSpec size=input_shape stride=self _calculate_stride input_shape dtype=output_spec dtype Weight tensor out_features in_features weight_spec = TensorSpec size= out_features in_features stride= in_features dtype=output_spec dtype Bias tensor out_features - make bias optional probability random random bias_spec = TensorSpec size= out_features stride= dtype=output_spec dtype input_spec weight_spec bias_spec input_spec weight_spec _calculate_stride size Calculate stride given size size stride = current_stride = dim_size reversed size stride append current_stride current_stride = dim_size tuple reversed stride codegen output_name str input_names list str output_spec Spec - str Generate code linear operation isinstance output_spec TensorSpec raise ValueError LinearOperator can only produce TensorSpec outputs Ensure dtype compatibility converting all inputs expected output dtype target_dtype = str output_spec dtype len input_names == input_name weight_name = input_names f output_name = torch nn functional linear input_name target_dtype weight_name target_dtype len input_names == input_name weight_name bias_name = input_names f output_name = torch nn functional linear input_name target_dtype weight_name target_dtype bias_name target_dtype raise ValueError Linear requires inputs input weight optional bias ReLUOperator Operator Operator torch nn functional relu __init__ super __init__ torch nn functional relu property torch_op_name - Optional str Return torch operation name torch nn functional relu can_produce output_spec Spec - bool ReLU can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs ReLU operation ReLU element-wise so input shape matches output shape isinstance output_spec TensorSpec raise ValueError ReLUOperator can only produce TensorSpec outputs Input tensor has same shape dtype output input_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype input_spec codegen output_name str input_names list str output_spec Spec - str Generate code ReLU operation len input_names = raise ValueError ReLU requires exactly input input_name = input_names f output_name = torch nn functional relu input_name SoftmaxOperator Operator Operator torch nn functional softmax __init__ super __init__ torch nn functional softmax property torch_op_name - Optional str Return torch operation name torch nn functional softmax can_produce output_spec Spec - bool Softmax can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False Softmax needs least dimension apply softmax along dimension len output_spec size == False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs softmax operation Softmax element-wise along dimension input shape matches output shape isinstance output_spec TensorSpec raise ValueError SoftmaxOperator can only produce TensorSpec outputs Input tensor has same shape dtype output input_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype input_spec codegen output_name str input_names list str output_spec Spec - str Generate code softmax operation len input_names = raise ValueError Softmax requires exactly input input_name = input_names Use dim=- default last dimension f output_name = torch nn functional softmax input_name dim=- DropoutOperator Operator Operator torch nn functional dropout __init__ super __init__ torch nn functional dropout property torch_op_name - Optional str Return torch operation name torch nn functional dropout can_produce output_spec Spec - bool Dropout can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs dropout operation Dropout element-wise input shape matches output shape isinstance output_spec TensorSpec raise ValueError DropoutOperator can only produce TensorSpec outputs Input tensor has same shape dtype output input_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype input_spec codegen output_name str input_names list str output_spec Spec - str Generate code dropout operation len input_names = raise ValueError Dropout requires exactly input input_name = input_names Use training=False make deterministic testing f output_name = torch nn functional dropout input_name p= training=False LayerNormOperator Operator Operator torch nn functional layer_norm __init__ super __init__ torch nn functional layer_norm property torch_op_name - Optional str Return torch operation name torch nn functional layer_norm can_produce output_spec Spec - bool LayerNorm can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False LayerNorm needs least dimension normalize over len output_spec size == False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs layer_norm operation LayerNorm normalizes over last dimensions specified normalized_shape - input input tensor - weight normalized_shape optional - bias normalized_shape optional isinstance output_spec TensorSpec raise ValueError LayerNormOperator can only produce TensorSpec outputs len output_spec size == raise ValueError LayerNorm output must have least dimension Input tensor has same shape dtype output input_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype For simplicity normalize over last dimension normalized_shape = output_spec size - Weight bias tensors optional probability each specs = input_spec random random LayerNorm weight bias parameters should match input tensor dtype compatibility conversion will handled codegen weight_spec = TensorSpec size=normalized_shape stride= dtype=output_spec dtype specs append weight_spec random random bias_spec = TensorSpec size=normalized_shape stride= dtype=output_spec dtype specs append bias_spec Cast list Spec fix type checking typing cast cast list Spec specs codegen output_name str input_names list str output_spec Spec - str Generate code layer_norm operation len input_names len input_names raise ValueError LayerNorm requires - inputs input optional weight optional bias isinstance output_spec TensorSpec raise ValueError LayerNormOperator can only produce TensorSpec outputs Normalize over last dimension normalized_shape = f output_spec size - Ensure dtype compatibility converting all inputs expected output dtype target_dtype = str output_spec dtype input_name = input_names len input_names == f output_name = torch nn functional layer_norm input_name target_dtype normalized_shape len input_names == weight_name = input_names f output_name = torch nn functional layer_norm input_name target_dtype normalized_shape weight= weight_name target_dtype len input_names == weight_name bias_name = input_names input_names f output_name = torch nn functional layer_norm input_name target_dtype normalized_shape weight= weight_name target_dtype bias= bias_name target_dtype RMSNormOperator Operator Operator torch nn functional rms_norm Root Mean Square Normalization RMSNorm commonly used modern LLMs like LLaMA It normalizes RMS input __init__ super __init__ torch nn functional rms_norm weight = property torch_op_name - Optional str Return torch operation name torch nn functional rms_norm can_produce output_spec Spec - bool RMSNorm can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False RMSNorm needs least dimension normalize over len output_spec size == False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs RMSNorm operation RMSNorm requires - input input tensor - weight normalized_shape optional isinstance output_spec TensorSpec raise ValueError RMSNormOperator can only produce TensorSpec outputs len output_spec size == raise ValueError RMSNorm output must have least dimension Input tensor has same shape dtype output input_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype Weight tensor optional probability normalized_shape = output_spec size - specs = input_spec random random weight_spec = TensorSpec size=normalized_shape stride= dtype=output_spec dtype specs append weight_spec typing cast cast list Spec specs codegen output_name str input_names list str output_spec Spec - str Generate code RMSNorm operation len input_names len input_names raise ValueError RMSNorm requires - inputs input optional weight isinstance output_spec TensorSpec raise ValueError RMSNormOperator can only produce TensorSpec outputs target_dtype = str output_spec dtype input_name = input_names Normalize over last dimension normalized_shape = f output_spec size - len input_names == f output_name = torch nn functional rms_norm input_name target_dtype normalized_shape len input_names == weight_name = input_names f output_name = torch nn functional rms_norm input_name target_dtype normalized_shape weight= weight_name target_dtype GELUOperator Operator Operator torch nn functional gelu Gaussian Error Linear Unit __init__ super __init__ torch nn functional gelu property torch_op_name - Optional str Return torch operation name torch nn functional gelu can_produce output_spec Spec - bool GELU can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs GELU operation GELU element-wise so input shape matches output shape isinstance output_spec TensorSpec raise ValueError GELUOperator can only produce TensorSpec outputs input_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype input_spec codegen output_name str input_names list str output_spec Spec - str Generate code GELU operation len input_names = raise ValueError GELU requires exactly input input_name = input_names f output_name = torch nn functional gelu input_name SigmoidOperator Operator Operator torch sigmoid __init__ super __init__ torch sigmoid property torch_op_name - Optional str Return torch operation name torch sigmoid can_produce output_spec Spec - bool Sigmoid can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs sigmoid operation Sigmoid element-wise so input shape matches output shape isinstance output_spec TensorSpec raise ValueError SigmoidOperator can only produce TensorSpec outputs input_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype input_spec codegen output_name str input_names list str output_spec Spec - str Generate code sigmoid operation len input_names = raise ValueError Sigmoid requires exactly input input_name = input_names f output_name = torch sigmoid input_name TanhOperator Operator Operator torch tanh __init__ super __init__ torch tanh property torch_op_name - Optional str Return torch operation name torch tanh can_produce output_spec Spec - bool Tanh can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs tanh operation Tanh element-wise so input shape matches output shape isinstance output_spec TensorSpec raise ValueError TanhOperator can only produce TensorSpec outputs input_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype input_spec codegen output_name str input_names list str output_spec Spec - str Generate code tanh operation len input_names = raise ValueError Tanh requires exactly input input_name = input_names f output_name = torch tanh input_name BatchNormOperator Operator Operator torch nn functional batch_norm __init__ super __init__ torch nn functional batch_norm property torch_op_name - Optional str Return torch operation name torch nn functional batch_norm can_produce output_spec Spec - bool BatchNorm can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False BatchNorm needs least dimensions batch features len output_spec size False Channel dimension second dimension must greater than output_spec size == False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs batch_norm operation BatchNorm requires - input N C where N batch C channels - running_mean C optional - running_var C optional - weight C optional - bias C optional isinstance output_spec TensorSpec raise ValueError BatchNormOperator can only produce TensorSpec outputs len output_spec size raise ValueError BatchNorm output must have least dimensions Input tensor has same shape dtype output input_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype Channel dimension second dimension num_features = output_spec size specs = input_spec Add running_mean running_var required inference mode running_mean_spec = TensorSpec size= num_features stride= dtype=output_spec dtype running_var_spec = TensorSpec size= num_features stride= dtype=output_spec dtype specs extend running_mean_spec running_var_spec Add weight bias optional probability random random weight_spec = TensorSpec size= num_features stride= dtype=output_spec dtype specs append weight_spec random random bias_spec = TensorSpec size= num_features stride= dtype=output_spec dtype specs append bias_spec typing cast cast list Spec specs codegen output_name str input_names list str output_spec Spec - str Generate code batch_norm operation len input_names len input_names raise ValueError BatchNorm requires - inputs input running_mean running_var optional weight optional bias isinstance output_spec TensorSpec raise ValueError BatchNormOperator can only produce TensorSpec outputs target_dtype = str output_spec dtype input_name = input_names running_mean_name = input_names running_var_name = input_names Use training=False deterministic behavior len input_names == f output_name = torch nn functional batch_norm input_name target_dtype running_mean_name target_dtype running_var_name target_dtype training=False len input_names == weight_name = input_names f output_name = torch nn functional batch_norm input_name target_dtype running_mean_name target_dtype running_var_name target_dtype weight= weight_name target_dtype training=False len input_names == weight_name = input_names bias_name = input_names f output_name = torch nn functional batch_norm input_name target_dtype running_mean_name target_dtype running_var_name target_dtype weight= weight_name target_dtype bias= bias_name target_dtype training=False GroupNormOperator Operator Operator torch nn functional group_norm __init__ super __init__ torch nn functional group_norm property torch_op_name - Optional str Return torch operation name torch nn functional group_norm can_produce output_spec Spec - bool GroupNorm can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False GroupNorm needs least dimensions batch channels len output_spec size False GroupNorm requires more than value per channel For shape N C num_values_per_channel = N prod We need N prod batch_size = output_spec size spatial_size = math prod output_spec size num_values_per_channel = batch_size spatial_size num_values_per_channel = False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs group_norm operation GroupNorm requires - input N C where N batch C channels - weight C optional - bias C optional isinstance output_spec TensorSpec raise ValueError GroupNormOperator can only produce TensorSpec outputs len output_spec size raise ValueError GroupNorm output must have least dimensions Input tensor has same shape dtype output input_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype Channel dimension second dimension num_channels = output_spec size specs = input_spec Add weight bias optional probability random random weight_spec = TensorSpec size= num_channels stride= dtype=output_spec dtype specs append weight_spec random random bias_spec = TensorSpec size= num_channels stride= dtype=output_spec dtype specs append bias_spec typing cast cast list Spec specs codegen output_name str input_names list str output_spec Spec - str Generate code group_norm operation len input_names len input_names raise ValueError GroupNorm requires - inputs input optional weight optional bias isinstance output_spec TensorSpec raise ValueError GroupNormOperator can only produce TensorSpec outputs target_dtype = str output_spec dtype input_name = input_names Determine number groups must divide num_channels evenly num_channels = output_spec size Common choices equal channels instance norm possible_groups = g g num_channels g == num_groups = possible_groups possible_groups len input_names == f output_name = torch nn functional group_norm input_name target_dtype num_groups len input_names == weight_name = input_names f output_name = torch nn functional group_norm input_name target_dtype num_groups weight= weight_name target_dtype len input_names == weight_name = input_names bias_name = input_names f output_name = torch nn functional group_norm input_name target_dtype num_groups weight= weight_name target_dtype bias= bias_name target_dtype LeakyReLUOperator Operator Operator torch nn functional leaky_relu __init__ super __init__ torch nn functional leaky_relu property torch_op_name - Optional str Return torch operation name torch nn functional leaky_relu can_produce output_spec Spec - bool LeakyReLU can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs LeakyReLU operation LeakyReLU element-wise so input shape matches output shape isinstance output_spec TensorSpec raise ValueError LeakyReLUOperator can only produce TensorSpec outputs input_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype input_spec codegen output_name str input_names list str output_spec Spec - str Generate code LeakyReLU operation len input_names = raise ValueError LeakyReLU requires exactly input input_name = input_names f output_name = torch nn functional leaky_relu input_name negative_slope= ELUOperator Operator Operator torch nn functional elu Exponential Linear Unit __init__ super __init__ torch nn functional elu property torch_op_name - Optional str Return torch operation name torch nn functional elu can_produce output_spec Spec - bool ELU can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs ELU operation ELU element-wise so input shape matches output shape isinstance output_spec TensorSpec raise ValueError ELUOperator can only produce TensorSpec outputs input_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype input_spec codegen output_name str input_names list str output_spec Spec - str Generate code ELU operation len input_names = raise ValueError ELU requires exactly input input_name = input_names f output_name = torch nn functional elu input_name SiLUOperator Operator Operator torch nn functional silu Sigmoid Linear Unit also known Swish __init__ super __init__ torch nn functional silu property torch_op_name - Optional str Return torch operation name torch nn functional silu can_produce output_spec Spec - bool SiLU can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs SiLU operation SiLU element-wise so input shape matches output shape isinstance output_spec TensorSpec raise ValueError SiLUOperator can only produce TensorSpec outputs input_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype input_spec codegen output_name str input_names list str output_spec Spec - str Generate code SiLU operation len input_names = raise ValueError SiLU requires exactly input input_name = input_names f output_name = torch nn functional silu input_name ScaledDotProductAttentionOperator Operator Operator torch nn functional scaled_dot_product_attention __init__ super __init__ torch nn functional scaled_dot_product_attention property torch_op_name - Optional str Return torch operation name torch nn functional scaled_dot_product_attention can_produce output_spec Spec - bool Scaled dot product attention can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False SDPA needs least dimensions batch seq_len embed_dim len output_spec size False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs scaled_dot_product_attention SDPA requires - query batch seq_len embed_dim batch num_heads seq_len head_dim - key batch seq_len embed_dim batch num_heads seq_len_kv head_dim - value batch seq_len embed_dim batch num_heads seq_len_kv head_dim Output shape matches query shape isinstance output_spec TensorSpec raise ValueError ScaledDotProductAttentionOperator can only produce TensorSpec outputs len output_spec size raise ValueError SDPA output must have least dimensions Query has same shape output query_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype Key value match query shape simplicity In practice seq_len key value can differ we ll keep simple key_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype value_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype query_spec key_spec value_spec codegen output_name str input_names list str output_spec Spec - str Generate code scaled_dot_product_attention operation len input_names = raise ValueError SDPA requires exactly inputs query key value Ensure dtype compatibility converting all inputs expected output dtype target_dtype = str output_spec dtype query_name key_name value_name = input_names f output_name = torch nn functional scaled_dot_product_attention query_name target_dtype key_name target_dtype value_name target_dtype MultiHeadAttentionForwardOperator Operator Operator torch nn functional multi_head_attention_forward __init__ super __init__ torch nn functional multi_head_attention_forward property torch_op_name - Optional str Return torch operation name torch nn functional multi_head_attention_forward can_produce output_spec Spec - bool Multi-head attention forward can produce tensor outputs floating point dtypes isinstance output_spec TensorSpec False MHA needs least dimensions seq_len batch embed_dim len output_spec size False MHA cannot handle -sized dimensions seq_len batch embed_dim must any dim == dim output_spec size False is_float_dtype output_spec dtype fuzz_inputs_specs output_spec Spec - list Spec Generate input specs multi_head_attention_forward MHA requires - query key value seq_len batch embed_dim - in_proj_weight embed_dim embed_dim combined QKV projection - in_proj_bias embed_dim optional - out_proj_weight embed_dim embed_dim - out_proj_bias embed_dim optional For simplicity we ll use combined in_proj_weight path IMPORTANT The order optional parameters matters codegen We must ensure when we have inputs they order query key value in_proj_weight in_proj_bias out_proj_weight NOT query key value in_proj_weight out_proj_weight out_proj_bias isinstance output_spec TensorSpec raise ValueError MultiHeadAttentionForwardOperator can only produce TensorSpec outputs len output_spec size raise ValueError MHA output must have least dimensions Output shape seq_len batch embed_dim seq_len batch embed_dim = output_spec size Query key value have same shape output query_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype key_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype value_spec = TensorSpec size=output_spec size stride=output_spec stride dtype=output_spec dtype in_proj_weight embed_dim embed_dim in_proj_weight_spec = TensorSpec size= embed_dim embed_dim stride= embed_dim dtype=output_spec dtype out_proj_weight embed_dim embed_dim out_proj_weight_spec = TensorSpec size= embed_dim embed_dim stride= embed_dim dtype=output_spec dtype For simplicity correctness always generate all required tensors This avoids ambiguity codegen about which optional parameters present We ll use simplified signature query key value in_proj_weight out_proj_weight only specs = query_spec key_spec value_spec in_proj_weight_spec out_proj_weight_spec typing cast cast list Spec specs _calculate_stride size Calculate stride given size size stride = current_stride = dim_size reversed size stride append current_stride current_stride = dim_size tuple reversed stride codegen output_name str input_names list str output_spec Spec - str Generate code multi_head_attention_forward operation len input_names = raise ValueError MHA requires exactly inputs query key value in_proj_weight out_proj_weight isinstance output_spec TensorSpec raise ValueError MultiHeadAttentionForwardOperator can only produce TensorSpec outputs target_dtype = str output_spec dtype embed_dim = output_spec size - Determine number heads must divide embed_dim evenly Common choices possible_heads = h h embed_dim h == num_heads = possible_heads possible_heads query_name = input_names key_name = input_names value_name = input_names in_proj_weight_name = input_names out_proj_weight_name = input_names Build function call without optional biases code = f output_name _ = torch nn functional multi_head_attention_forward query_name target_dtype key_name target_dtype value_name target_dtype embed_dim num_heads in_proj_weight_name target_dtype None in_proj_bias None bias_k None bias_v False add_zero_attn dropout_p no dropout testing out_proj_weight_name target_dtype None out_proj_bias training=False Use eval mode deterministic behavior need_weights=False Don t compute attention weights performance code