mypy allow-untyped-defs functools itertools collections abc Callable typing Any torch torch _prims_common utils torch utils _pytree pytree torch _C DispatchKey torch _higher_order_ops utils _maybe_compile_and_run_fn _maybe_run_with_interpreter check_input_alias_and_mutation_return_outputs check_meta_consistency create_bw_fn first_slice_copy first_slice_copy_with_grad materialize_as_graph reenter_make_fx save_tensors_and_symints_for_backward saved_tensors_and_symints split_into_chunks unique_graph_id validate_subgraph_args_types torch _ops HigherOrderOperator torch _subclasses fake_tensor FakeTensorMode torch fx experimental proxy_tensor disable_proxy_modes_tracing ProxyTorchDispatchMode track_tensor_tree aten = torch _ops ops aten wrap_combine_fn_flat args combine_fn spec num_leaves assert len args == num_leaves f Combin_fn received wrong number arguments expected num_leaves got len args lhs = pytree tree_unflatten args num_leaves spec rhs = pytree tree_unflatten args num_leaves spec combine_fn lhs rhs _interleave b dim= https stackoverflow com questions how-can-i-interleave- -pytorch-tensors b_trunc = shape dim == b shape dim + pad = b ndim - dim - + + + b ndim - b ndim - dim - + b = torch nn functional pad b pad stacked = torch stack b dim=dim + interleaved = torch flatten stacked start_dim=dim end_dim=dim + pyrefly ignore unbound-name b_trunc TODO find torch alternative slice_along dim torch jit script work interleaved = aten slice interleaved dim b shape dim + shape dim - interleaved safe_map f args args = list map list args n = len args arg args len arg = n raise ValueError length mismatch list map len args nf f list map nf zip args AssociativeScanOp HigherOrderOperator __init__ super __init__ associative_scan __call__ combine_fn xs additional_inputs There currently issue ScanOp sometimes called additional_inputs being list See https github com pytorch pytorch issues Once issue resolved assertion should only allow tuples tuple cast should removed assert isinstance additional_inputs tuple list additional_inputs must tuple additional_inputs = tuple additional_inputs isinstance additional_inputs list additional_inputs validate_subgraph_args_types additional_inputs super __call__ combine_fn xs additional_inputs pyrefly ignore bad-override gen_schema combine_fn xs additional_inputs torch _higher_order_ops schema HopSchemaGenerator torch _higher_order_ops utils materialize_as_graph For associative scan we need two copies xs combine function The combine function takes two elements returns one element xs_slice = first_slice_copy x x xs xs_slice = first_slice_copy x x xs all_inputs = tuple xs_slice + xs_slice + list additional_inputs combine_gm torch fx GraphModule = materialize_as_graph combine_fn all_inputs _ _ _ mutated_inputs outputs = check_input_alias_and_mutation_return_outputs combine_gm len mutated_inputs raise RuntimeError For associative_scan combine_fn cannot have in-place mutations found f mutated_inputs -th inputs mutated schema_gen = HopSchemaGenerator schema_gen add_arg combine_fn combine_gm idx x enumerate xs schema_gen add_arg f xs idx x idx arg enumerate additional_inputs schema_gen add_arg f additional_input idx arg out outputs schema_gen add_output out schema_gen add_schema_tree_spec combine_fn xs additional_inputs schema_gen gen_schema associative_scan_op = AssociativeScanOp associative_scan combine_fn Callable pytree PyTree pytree PyTree pytree PyTree xs pytree PyTree dim int reverse bool = False combine_mode str = pointwise - torch Tensor r Performs inclusive scan associative combine function warning ` torch associative_scan ` prototype feature PyTorch It currently does support autograd you may run into miscompiles Read more about feature classification https pytorch org blog pytorch-feature-classification-changes #prototype This operator requires runtime code generation so requires support ` ` torch compile ` ` Further only CUDA device codegen supported moment Args combine_fn Callable A binary callable type ` ` Tensor Tensor - Tensor ` ` input pytree ` ` pytree pytree - pytree ` ` This function must pure i e no lifted arguments supported moment satisfy associative property have no side-effects xs torch Tensor The input tensor nested pytree tensors All inputs expected have same shape dim int dimension scan over reverse bool A boolean stating scan should reversed respect ` ` dim ` ` default ` ` False ` ` combine_mode str A string indicating whether ` ` combine_fn ` ` ` ` pointwise ` ` ` ` generic ` ` default ` ` pointwise ` ` If ` ` combine_mode=pointwise ` ` ` ` combine_fn ` ` must pure may only contain pointwise operations ` ` xs ` ` must CUDA tensors In all other cases ` ` combine_mode=generic ` ` should used Note ` ` combine_mode=pointwise ` ` more efficient than ` ` combine_mode=generic ` ` Example add x torch Tensor y torch Tensor x + y cumsum = associative_scan add x dim TODO Support lifted arguments inductor associative_scan TODO Support autograd cases lifted arguments combine_mode=pointwise The reason we flatten xs before calling into dynamo we want create consistent input ordering combine_fn we also want input ordering matches output ordering leaves_xs_orig spec_xs = pytree tree_flatten xs _validate_input cfn lxs d r cm Basic arguments check callable cfn raise ValueError f Combine_fn must callable got cfn isinstance d int raise ValueError Dim must int got + str type d isinstance r bool raise RuntimeError Reverse must bool got + str type r cm pointwise generic raise ValueError f Combine_mode must either pointwise generic got cm cm == pointwise all l device type cuda xpu l lxs raise ValueError For combine_mode= pointwise all input tensors need CUDA XPU Checks xs len lxs == raise ValueError Expected least xs leaf any isinstance x torch Tensor x lxs raise ValueError xs leaves must Tensor any x is_sparse x lxs raise ValueError xs leaves must dense Tensors consider using ` to_dense ` any x ndim = d x lxs raise ValueError All xs leaves must least have dim number dimensions scan dimension any x shape d == x lxs raise ValueError All xs leaves must least have dim number dimensions scan dimension ndim = leaves_xs_orig ndim dim = utils canonicalize_dim ndim dim _validate_input combine_fn leaves_xs_orig dim reverse combine_mode Move scan dim always perform scan dim leaves_xs = torch movedim elem dim elem leaves_xs_orig reverse leaves_xs = torch flip elem elem leaves_xs combine_mode == generic The generic_associative_scan implementation calls combine_fn ` batch ` along scan dimension For example consider add x torch Tensor y torch Tensor x + y leaves = torch tensor which has shape x dim = In first iteration ` _scan ` combine_fn gets invoked combine_fn torch tensor torch tensor The arguments shape x can evaluated parallel along scan dimension combine_fn = functools partial wrap_combine_fn_flat combine_fn=torch vmap combine_fn in_dims= pytree tree_unflatten len leaves_xs spec_xs pytree tree_unflatten len leaves_xs spec_xs out_dims= spec=spec_xs num_leaves=len leaves_xs out = generic_associative_scan combine_fn leaves_xs additional_inputs= out = pytree tree_unflatten out spec_xs combine_fn = functools partial wrap_combine_fn_flat combine_fn=combine_fn spec=spec_xs num_leaves=len leaves_xs run_flattened_associative_scan combine_fn leaves_xs associative_scan_op combine_fn leaves_xs additional_inputs= out = _maybe_compile_and_run_fn run_flattened_associative_scan combine_fn leaves_xs reverse out = pytree tree_map lambda elem elem flip out out = pytree tree_map lambda elem torch movedim elem dim out out generic_associative_scan operator leaves dim= additional_inputs= r This function performs associative_scan operation The algorithm works recursively collecting neighbours ` ` leaves ` ` subsequently applying ` ` operator ` ` all pairs parallel along ` ` dim ` ` The results recursive calls later combined Args operator Callable A binary callable type ` ` Tensor Tensor - Tensor ` ` input pytree ` ` pytree pytree - pytree ` ` This function must pure pointwise satisfy associative property leaves torch Tensor A list torch Tensors converted pytree ` ` xs ` ` provided ` ` associative_scan ` ` All inputs expected have same shape dim int dimension scan over additional_inputs Tuple tensors A tuple lifted parameters global scope This parameter will populated internally Example add x torch Tensor y torch Tensor x + y leaves = torch tensor First iteration _scan - odd_elems - apply operator all neighbours odd_elems = operator torch tensor torch tensor odd_elems = torch tensor Second iteration _scan - odd_elems = operator torch tensor torch tensor odd_elems = torch tensor even_elems - apply operator all odd_elems every second element ` ` elems ` ` starting second element even_elems expanded first element ` ` elems ` ` even_elems = Merges odd_elems even_elems res = torch tensor even_elems - apply operator all odd_elems every second element ` ` elems ` ` starting second element even_elems expanded first element ` ` elems ` ` even_elems = Merges odd_elems even_elems res = torch tensor call_operator args pytree tree_leaves operator args _scan elems Perform actual recursive scan ` ` elems ` ` num_elems = elems shape dim num_elems elems reduced_elems = call_operator aten slice elem dim - elem elems aten slice elem dim None elem elems additional_inputs Recursively compute scan partially reduced tensors odd_elems = _scan reduced_elems num_elems == even_elems = call_operator aten slice e dim - e odd_elems aten slice e dim None e elems additional_inputs even_elems = call_operator odd_elems aten slice e dim None e elems additional_inputs The first element scan same first element original ` elems ` even_elems = torch cat aten slice elem dim result dim=dim result shape numel elem shape dim result result shape numel aten slice elem dim Jax allows ignores concat -dim Pytorch does elem result zip elems even_elems list safe_map functools partial _interleave dim=dim even_elems odd_elems scans = _scan leaves scans trace_associative_scan proxy_mode func_overload combine_fn Callable xs list torch Tensor additional_inputs tuple torch Tensor torch _dynamo utils clone_input disable_proxy_modes_tracing sample_xs = first_slice_copy x x itertools chain xs xs sample_additional_inputs = clone_input x isinstance x torch Tensor x x additional_inputs combine_graph = reenter_make_fx combine_fn sample_xs sample_additional_inputs outputs = None node combine_graph graph nodes node op == output assert outputs None assert len node args == outputs = node args assert outputs None outputs = pytree tree_leaves outputs assert len outputs == len xs f expected combine_fn len xs results got len outputs xs_fake_tensors list torch Tensor &#124; torch SymInt &#124; int = first_slice_copy x x xs output_fake_tensors list torch Tensor &#124; torch SymInt &#124; int = c meta val c outputs check_meta_consistency xs_fake_tensors output_fake_tensors init carry include_contiguity=False _ combine_graph_name = unique_graph_id proxy_mode prefix= associative_scan_combine_graph proxy_mode tracer root register_module combine_graph_name combine_graph args = combine_graph xs additional_inputs proxy_args = pytree tree_map proxy_mode tracer unwrap_proxy args out_proxy = proxy_mode tracer create_proxy call_function func_overload proxy_args name= associative_scan disable_proxy_modes_tracing out = tuple aten clone x x xs track_tensor_tree out out_proxy constant=None tracer=proxy_mode tracer associative_scan_op py_impl DispatchKey CompositeExplicitAutograd associative_scan_op_dense combine_fn xs additional_inputs generic_associative_scan combine_fn xs additional_inputs=additional_inputs AssociativeScanAutogradOp torch autograd Function r associative_scan Example xs = torch arange = combine_fn torch Tensor b torch Tensor b ys = associative_scan comine_fn xs which can unpacked ys = xs = ys = combine_fn ys xs = combine_fn = ysT = combine_fn ys T- xsT = combine_fn = ys = This creates recursive data dependency structure where each output yst depends all prior inputs xs through xst The dependency can visualized Level Input xs xs xs xs xs \ &#124; &#124; &#124; \ &#124; &#124; &#124; Level ys ───────┘ &#124; &#124; \ &#124; \ &#124; Level ys ────────┘ &#124; \ \ Level ys ────────────┘ \ \ Level ys We could get following backward gradient graph Level output g_xs g_xs g_xs g_xs g_xs \ &#124; &#124; &#124; \ &#124; &#124; &#124; Level gl_ys ─ g_ys ──────┘ &#124; &#124; \ &#124; \ &#124; Level gl_ys ─ g_ys ────────┘ &#124; \ \ Level gl_ys ─ g_ys ────────────┘ \ \ Level gl_ys ─ g_ys where gl_y gradient loss respect ys input backward To calculate gradients inputs chain rule suggests g_xs = g_ys g_xs = g_ys bw ys xs = g_ys bwxs g_xs = g_ys bw ys xs = g_ys bwxs g_xs = g_ys bw ys xs = g_ys bwxs g_xs = g_ys bw ys xs = g_ys bwxs Notice bw just single step bw instantaneous gradients whose formula can computed combine_fn For example bw ys xs also abbreviated bwxs computes gradients ∂ ∂xs combine_fn ys xs Similarly bw ys ys also abbreviated bwys computes gradients ∂ ∂ys combine_fn ys xs Let s break down how calculate g_ys recursively substituting unknowns g_ys = gl_ys + g_ys bw ys ys = gl_ys + gl_ys + g_ys bw ys ys bw ys ys = gl_ys + gl_ys bw ys ys + g_ys bw ys ys bw y y = gl_ys + gl_ys bw ys ys + gl_ys bw ys ys bw y y \ + g_ys bw ys ys bw ys ys bw ys ys = gl_ys + gl_ys bw ys ys + gl_ys bw ys ys bw y y \ + gl_ys bw ys ys bw ys ys bw ys ys Let s do same all g_ys g_ys = gl_ys + gl_ys bw ys ys + gl_y bw ys ys bw ys ys g_ys = gl_ys + gl_ys bw ys ys g_ys = gl_ys Notice above can re-written columnwise multiplication y_mat gl_ys g_ys bwys bwys bwys gl_ys g_ys = bwys bwys gl_ys g_ys bwys gl_ys g_ys gl_ys where bwys abbreviation bw ys ys bwys abbreviation bw ys ys bw ys ys so so forth We could effectively compute upper triangular matrix y_mat cumprod bwys bwys bwys then masking out values needed Thus only bwys bwys bwys required compute y_mat References https justintchiu com blog pscan_diff NOTE associative_scan autograd implementation The forward associative_scan can computed following steps Compute forward output associative_scan ys = associative_scan combine_fn xs additional_inputs The backward associative_scan can computed following steps Prepare backward graph We prepare backward graph used backward function We utilize ` ` create_bw_fn ` ` generate joint function combine_fn_bw = create_bw_fn combine_fn operands where operands = ys t- xst additional_inputs Materialize ` ` combine_fn_bw ` ` This required because torch compile torch autograd grad cannot trace through joint backward function dynamically Compute single step bw instantaneous gradients every step t bwys t- bwxst = combine_fn_bw ys t- xst Here we pass upstream gradient obtain local partial derivatives This gives bwys = bw ys ys bw ys ys bw ysT ys T- bwxs = bw ys xs bw ys xs bw ys T- xsT Compute gradient transition matrix y_mat As shown example above each input xst affects all later outputs ysi i ≥ t According chain rule each such path contributes product local gradients g_ysk For example ∂ysT ∂xst = ∂ysT ∂ys T- ∂ys T- ∂ys T- ∂ys t+ ∂yst ∂yst ∂xst = bw ysT ys T- bw ys T- ys T- bw ys t+ yst bw ys t- xst This motivates use cumulative product over bwys compute all such paths efficiently We now construct matrix gradient transition paths Repeat g_y values form base matrix y_mat = bwys bwys bwys bwys bwys bwys bwys bwys bwys bwys bwys bwys Mask lower triangle inclusive s y_mat = bwys bwys bwys bwys bwys bwys Apply cumulative product row-wise y_mat = cumprod y_mat dim= Resulting y_mat = bwys bwys bwys bwys bwys bwys bwys bwys bwys bwys Zero out lower triangle exclusive Final y_mat y_mat = bwys bwys bwys bwys bwys bwys bwys bwys bwys bwys Scale y_mat upstream gradients gl_ys scaled_y_mat = y_mat gl_ys Each entry now holds full contribution ∂L ∂ysj ∂L ∂xsi via path through ysj Reduce scaled_y_mat row-wise sum summed_y_mat = scaled_y_mat sum dim= This accumulates all downstream contributions each xst Scale instantaneous input gradients bwxs g_xs = summed_y_mat bwxs This gives final input gradients g_xs = ∂L ∂xs ∂L ∂xs ∂L ∂xsT NOTE scan partial grad handling If any element xs outputs does require gradients i e requires_grad=False then corresponding gradients will returned tensors zeros same shape element staticmethod pyrefly ignore bad-override forward ctx combine_fn num_xs num_additional_inputs operands ctx _num_xs = num_xs ctx _num_additional_inputs = num_additional_inputs ctx _combine_fn = combine_fn xs additional_inputs = split_into_chunks operands num_xs num_additional_inputs scan_length = xs shape ctx _scan_length = scan_length We snapshot dispatch keys forward materializing bw_graph backward ctx _fw_include_key_set = torch _C _dispatch_tls_local_include_set ctx _fw_exclude_key_set = torch _C _dispatch_tls_local_exclude_set torch _C _AutoDispatchBelowAutograd Compute forward output associative_scan ys = associative_scan_op combine_fn xs additional_inputs save_tensors_and_symints_for_backward ctx list operands + list ys ys staticmethod backward ctx gl_ys r This function computes gradients scan operation For detailed description see document above Args flat_grads torch Tensor The tensor upstream gradients nested pytree tensors E g Gradient loss respect forward output ys The backward associative_scan always performed first dimension dim = scan_length = ctx _scan_length num_xs = ctx _num_xs num_additional_inputs = ctx _num_additional_inputs Extract inputs forward path outputs forward path flat_args = saved_tensors_and_symints ctx xs additional_inputs outs = split_into_chunks flat_args num_xs num_additional_inputs num_xs ndim = outs ndim First_slice_copy does keep original requires_grad flag we need here order compute correcte gradients xs_slices = first_slice_copy_with_grad itertools chain xs xs Construct operands forward fw_operands operands single event t forward fw_operands_slice fw_operands = xs additional_inputs fw_operands_slice = xs_slices additional_inputs Prepare backward graph combine_fn_bw = create_bw_fn ctx _combine_fn fw_operands_slice Materialize ` ` combine_fn_bw ` ` TODO we need materialize bw graphs because dynamo unable trace through joint function when torch compile torch autograd grad combine_fn_bw_gm = materialize_as_graph combine_fn_bw fw_operands_slice first_slice_copy o o outs ctx _fw_include_key_set ctx _fw_exclude_key_set force_enable_grad=True vmap joint graph over scan dimension compute individual gradients each time slice ` ` t ` ` parallel This computation can parallelized these just instantaneous gradients full chain-rule mapped_combine_fn_bw_gm = torch vmap combine_fn_bw_gm Compute single step bw instantaneous gradients every step ` ` t ` ` Use ones_like tensor order scale bwyst bwxst upstream gradients yet Note All bwyst bwxst computed parallel thus tensors bwys bwxs result dummy_upstream_grad = torch ones_like x x gl_ys grads = mapped_combine_fn_bw_gm o roll dim o outs fw_operands dummy_upstream_grad bwys bwxs = split_into_chunks grads num_xs num_xs compute_y_mat bwys torch Tensor - torch Tensor Prepare ones zeros helper mask order easily compute y_mat compute_helper_tril_mask diagonal expand_masks mask _ range ndim - mask = mask unsqueeze - mask tril_mask = torch tril torch ones scan_length scan_length device=bwys device dtype=torch bool diagonal=diagonal tril_mask = expand_masks tril_mask tril_mask = tril_mask expand - - bwys shape tril_mask The ones mask used fill main diagonal all elements below s ones_mask = compute_helper_tril_mask The zero mask used set all elements below main diagonal zeros_mask = compute_helper_tril_mask - Repeat elements bwys form square matrix y_mat = bwys unsqueeze dim repeat_interleave scan_length dim Fill lower triangular part including diagonal h_mat s I e use ones_mask fill s y_mat masked_fill_ ones_mask Compute cumulative products across dim + y_mat = y_mat cumprod dim=dim + Replace elements we filled s before s y_mat masked_fill_ zeros_mask y_mat compute_grad bwxs bwys gl_ys Set first gradient component bwxs per definition torch select bwxs dim fill_ Compute gradient transition matrix y_mat = compute_y_mat bwys scale y_mat upstream gradients gl_ys scaled_y_mat = y_mat gl_ys Reduce y_mat sum along columns get total contributions xs_t summed_y_mat = scaled_y_mat sum dim + Scale bwxs obtain final gradients g_xs g_xs = summed_y_mat bwxs g_xs Stack all leaves gradients along first dimension This useful later gradients those leaves can computed parallel bwxs_stacked_leaves = torch stack bwxs bwys_stacked_leaves = torch stack bwys gl_ys_stacked_leaves = torch stack gl_ys The compute_grad function parallelized across all individual leaves xs these gradients can computed independently each other TODO torch vmap may create composability issues compute_grad_mapped = torch vmap compute_grad g_xs = compute_grad_mapped bwxs_stacked_leaves bwys_stacked_leaves gl_ys_stacked_leaves TODO Currently gradients additional_inputs computed properly None g_xs None num_additional_inputs associative_scan_op py_autograd_impl associative_scan_autograd combine_fn xs additional_inputs num_xs = len xs num_additional_inputs = len additional_inputs num_additional_inputs raise RuntimeError Associative_scan does currently support gradients lifted parameters flat_out = AssociativeScanAutogradOp apply combine_fn num_xs num_additional_inputs tuple xs + tuple additional_inputs flat_out associative_scan_op py_impl ProxyTorchDispatchMode associative_scan_proxy_mode mode combine_fn xs additional_inputs trace_associative_scan mode associative_scan_op combine_fn xs additional_inputs associative_scan_op py_impl FakeTensorMode assoiciative_scan_fake_tensor_mode mode combine_fn xs additional_inputs mode tuple x clone x xs associative_scan_op py_functionalize_impl associative_scan_functionalize ctx combine_fn xs additional_inputs torch _higher_order_ops utils _check_alias_and_mutation unwrapped_xs = ctx unwrap_tensors xs unwrapped_additional_inputs = ctx unwrap_tensors additional_inputs ctx redispatch_to_next functional_combine_fn = ctx functionalize _maybe_run_with_interpreter combine_fn pre_dispatch = hasattr ctx mode ctx mode pre_dispatch sample_unwrapped_xs_sliced = first_slice_copy inp inp itertools chain unwrapped_xs unwrapped_xs sample_inputs = list itertools chain sample_unwrapped_xs_sliced unwrapped_additional_inputs _check_alias_and_mutation combine_fn sample_inputs associative_scan pre_dispatch ret = associative_scan_op functional_combine_fn unwrapped_xs unwrapped_additional_inputs ctx wrap_tensors ret _fake_associative_scan combine_fn xs dim reverse=False inp_leaves spec = pytree tree_flatten xs result_flat list Any = num_leaves = len inp_leaves op = reversed reverse lambda x x ind op range inp_leaves size dim r = inp_leaves leave_ind slice None dim + ind leave_ind range num_leaves ind reverse ind inp_leaves size dim - reverse r = combine_fn pytree tree_unflatten result_flat - spec pytree tree_unflatten r spec r_flat _ = pytree tree_flatten r result_flat append r_flat results = torch stack e leave_ind e op result_flat dim leave_ind range num_leaves pytree tree_unflatten results spec