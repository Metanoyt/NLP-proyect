mypy allow-untyped-defs r Weight Normalization https arxiv org abs typing Any TypeVar typing_extensions deprecated torch _weight_norm norm_except_dim torch nn modules Module torch nn parameter Parameter UninitializedParameter __all__ = WeightNorm weight_norm remove_weight_norm WeightNorm name str dim int __init__ name str dim int - None dim None dim = - name = name dim = dim TODO Make type more specific compute_weight module Module - Any g = getattr module name + _g v = getattr module name + _v _weight_norm v g dim staticmethod deprecated ` torch nn utils weight_norm ` deprecated favor ` torch nn utils parametrizations weight_norm ` category=FutureWarning apply module name str dim int - WeightNorm hook module _forward_pre_hooks values isinstance hook WeightNorm hook name == name raise RuntimeError f Cannot register two weight_norm hooks same parameter name dim None dim = - fn = WeightNorm name dim weight = getattr module name isinstance weight UninitializedParameter raise ValueError The module passed ` WeightNorm ` can t have uninitialized parameters Make sure run dummy forward before applying weight normalization remove w parameter list del module _parameters name add g v new parameters express w g &#124; &#124; v &#124; &#124; v module register_parameter name + _g Parameter norm_except_dim weight dim data module register_parameter name + _v Parameter weight data setattr module name fn compute_weight module recompute weight before every forward module register_forward_pre_hook fn fn remove module Module - None weight = compute_weight module delattr module name del module _parameters name + _g del module _parameters name + _v setattr module name Parameter weight data __call__ module Module inputs Any - None setattr module name compute_weight module T_module = TypeVar T_module bound=Module weight_norm module T_module name str = weight dim int = - T_module r Apply weight normalization parameter given module math \mathbf w = g \dfrac \mathbf v \ &#124; \mathbf v \ &#124; Weight normalization reparameterization decouples magnitude weight tensor its direction This replaces parameter specified attr ` name ` e g ` ` weight ` ` two parameters one specifying magnitude e g ` ` weight_g ` ` one specifying direction e g ` ` weight_v ` ` Weight normalization implemented via hook recomputes weight tensor magnitude direction before every meth ` ~Module forward ` call By default ` ` dim= ` ` norm computed independently per output channel plane To compute norm over entire weight tensor use ` ` dim=None ` ` See https arxiv org abs warning This function deprecated Use func ` torch nn utils parametrizations weight_norm ` which uses modern parametrization API The new ` ` weight_norm ` ` compatible ` ` state_dict ` ` generated old ` ` weight_norm ` ` Migration guide The magnitude ` ` weight_g ` ` direction ` ` weight_v ` ` now expressed ` ` parametrizations weight original ` ` ` ` parametrizations weight original ` ` respectively If bothering you please comment https github com pytorch pytorch issues To remove weight normalization reparametrization use func ` torch nn utils parametrize remove_parametrizations ` The weight no longer recomputed once module forward instead will recomputed every access To restore old behavior use func ` torch nn utils parametrize cached ` before invoking module question Args module Module containing module name str optional name weight parameter dim int optional dimension over which compute norm Returns The original module weight norm hook Example m = weight_norm nn Linear name= weight m Linear in_features= out_features= bias=True m weight_g size torch Size m weight_v size torch Size WeightNorm apply module name dim module remove_weight_norm module T_module name str = weight - T_module r Remove weight normalization reparameterization module Args module Module containing module name str optional name weight parameter Example m = weight_norm nn Linear remove_weight_norm m k hook module _forward_pre_hooks items isinstance hook WeightNorm hook name == name hook remove module del module _forward_pre_hooks k module raise ValueError f weight_norm name found module