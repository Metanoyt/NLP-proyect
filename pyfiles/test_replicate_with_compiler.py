Owner s oncall distributed contextlib functools unittest collections abc Callable copy deepcopy typing Optional Union torch torch distributed dist torch _inductor inductor nn torch _C FileCheck torch _dynamo compiled_autograd torch _dynamo utils counters torch _inductor test_case TestCase InductorTestCase torch _inductor utils run_and_get_triton_code torch distributed _composable replicate replicate torch distributed algorithms ddp_comm_hooks default_hooks ddp_default_hooks torch distributed device_mesh init_device_mesh torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel torch nn parallel distributed DistributedDataParallel DDP torch testing _internal common_distributed DistributedTestBase skip_if_lt_x_gpu sm_is_or_higher_than torch testing _internal common_fsdp get_devtype torch testing _internal common_utils run_tests torch testing _internal distributed fake_pg FakeStore torch testing _internal inductor_utils HAS_GPU torch utils checkpoint checkpoint device_type = str get_devtype DIM = Net nn Module __init__ checkpoint=False super __init__ fc = nn Linear DIM DIM fc = nn Linear DIM DIM fc = nn Linear DIM DIM fc = nn Linear DIM DIM use_checkpoint = checkpoint forward x use_checkpoint _fc = checkpoint fc x use_reentrant=False _fc = fc x fc fc fc _fc compiler_fn no_inductor=False _compiler_fn gm inner_compiler gm_ example_inputs_ no_inductor gm_ inductor compile gm_ example_inputs_ gm = torch compile gm fullgraph=True backend=inner_compiler gm _compiler_fn MultiProcessInductorTestCase DistributedTestBase InductorTestCase A version MultiProcessTestCase derives Inductor TestCase handle isolation inductor cache dir ReplicateTest MultiProcessInductorTestCase property world_size - int min torch get_device_module device_type device_count _test_compile no_sync bool setup_func Optional Callable = None no_inductor bool = False no_compile_forward bool = False checkpoint bool = False device Union str torch device create_pg device torch _dynamo config optimize_ddp = python_reducer torch manual_seed device_type == xpu torch use_deterministic_algorithms True warn_only=True model = Net checkpoint=checkpoint device input = torch randn DIM device=device compiled_replicate_model = replicate deepcopy model no_compile_forward compiled_replicate_model = torch compile compiled_replicate_model fullgraph=False compiled_replicate_optim = torch optim Adam compiled_replicate_model parameters compiled_ddp_model = DDP deepcopy model no_compile_forward compiled_ddp_model = torch compile compiled_ddp_model fullgraph=True compiled_ddp_optim = torch optim Adam compiled_ddp_model parameters model = replicate model optim = torch optim Adam model parameters setup_func setup_func model compiled_replicate_model compiled_ddp_model models = model compiled_replicate_model compiled_ddp_model optims = optim compiled_replicate_optim compiled_ddp_optim sync_contexts = contextlib nullcontext contextlib nullcontext compiled_ddp_model no_sync Run multiple iterations so we could test no_sync i range Setting different random seed so allreduces executed correctly gradients won t correct compared eager DDP torch manual_seed + rank + i input = torch randn DIM device=device model_idx range no_sync i == context = sync_contexts model_idx model_idx = models model_idx set_requires_gradient_sync False context = contextlib nullcontext model_idx = models model_idx set_requires_gradient_sync True context = contextlib nullcontext context bwd_context = contextlib nullcontext model_idx == compiled_autograd _enable compiler_fn no_inductor bwd_context loss = models model_idx input sum loss backward no_sync i == p p p zip model parameters compiled_replicate_model parameters compiled_ddp_model parameters assertEqual p grad p grad assertEqual p grad p grad optim optims optim step optim zero_grad assertEqual tuple model parameters tuple compiled_replicate_model parameters assertEqual tuple model parameters tuple compiled_ddp_model parameters dist destroy_process_group test_compile_cpu Test coalesced_op CPU torch _inductor config _fuse_ddp_communication_passes = fuse_ddp_with_coalesced_op schedule_comm_wait _test_compile no_sync=False device= cpu test_compile_cpu_no_sync Test coalesced_op CPU torch _inductor config _fuse_ddp_communication_passes = fuse_ddp_with_coalesced_op schedule_comm_wait _test_compile no_sync=True device= cpu unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu torch _inductor config patch reorder_for_locality=False reorder_for_peak_memory=False test_compile_gpu _test_compile no_sync=False checkpoint=False device=device_type unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu torch _inductor config patch reorder_for_locality=False reorder_for_peak_memory=False test_compile_gpu_ac _test_compile no_sync=False checkpoint=True device=device_type unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_compile_bf Check device capability wrt bf sm_is_or_higher_than torch device device_type torch version hip None skipTest bf requires sm = setup model compiled_replicate_model compiled_ddp_model - None model register_comm_hook None ddp_default_hooks bf _compress_hook compiled_m = compiled_replicate_model _orig_mod compiled_m register_comm_hook None ddp_default_hooks bf _compress_hook compiled_ddp_model register_comm_hook None ddp_default_hooks bf _compress_hook _test_compile no_sync=False setup_func=setup device=device_type unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_compile_fp setup model compiled_replicate_model compiled_ddp_model - None model register_comm_hook None ddp_default_hooks fp _compress_hook compiled_m = compiled_replicate_model _orig_mod compiled_m register_comm_hook None ddp_default_hooks fp _compress_hook compiled_ddp_model register_comm_hook None ddp_default_hooks fp _compress_hook TODO figure out why we need disable Inductor avoid test errors _test_compile no_sync=False setup_func=setup no_inductor=True device=device_type unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_compile_backward_only _test_compile no_sync=False no_compile_forward=True device=device_type _test_bucketing init_process_group=True loop= init_process_group dist init_process_group backend= gloo rank=self rank world_size=self world_size store=dist FileStore file_name world_size model = Net input = torch randn DIM torch _dynamo config optimize_ddp = python_reducer compiled_replicate_model = torch compile replicate deepcopy model fullgraph=False bwd loss compiled_autograd _enable compiler_fn loss backward i range loop loss = compiled_replicate_model input sum i = loop - Leave last bwd run_and_get_triton_code bwd loss code = run_and_get_triton_code functools partial bwd loss=loss assertEqual counters inductor ddp_buckets code torch _inductor config patch _fuse_ddp_communication_passes= fuse_ddp_with_coalesced_op schedule_comm_wait todo This pass mucks things up since Inductor thinks its inference can apply Should turn off these passes compiled autograd torch _inductor config patch reorder_for_locality=False reorder_for_peak_memory=False The correctness test relies pointless permute ops joint graph does get eliminated pattern_matcher=False test_bucketing_coalesced_op Gradient None code = _test_bucketing assertEqual counters inductor ddp_buckets fc = FileCheck _ range fc check cpp_fused_ check torch ops _c d_functional all_reduce_coalesced_ default _ range fc check torch ops _c d_functional wait_tensor default fc run code Gradient None code = _test_bucketing init_process_group=False loop= assertEqual counters inductor ddp_buckets fc = FileCheck _ range fc check cpp_fused_ check torch ops _c d_functional all_reduce_coalesced_ default _ range fc check torch ops _c d_functional wait_tensor default fc run code torch _inductor config patch _fuse_ddp_communication_passes= fuse_ddp_with_concat_op schedule_comm_wait todo This pass mucks things up since Inductor thinks its inference can apply Should turn off these passes compiled autograd torch _inductor config patch reorder_for_locality=False reorder_for_peak_memory=False The correctness test relies pointless permute ops joint graph does get eliminated pattern_matcher=False test_bucketing_concat_op Gradient None code = _test_bucketing assertEqual counters inductor ddp_buckets fc = FileCheck _ range fc check aten flatten using_ints check cpp_fused_ check torch ops _c d_functional all_reduce_ default _ range fc check torch ops _c d_functional wait_tensor default fc run code Gradient None code = _test_bucketing init_process_group=False loop= assertEqual counters inductor ddp_buckets fc = FileCheck _ range fc check aten flatten using_ints check cpp_fused_ check torch ops _c d_functional all_reduce_ default _ range fc check torch ops _c d_functional wait_tensor default fc run code DDP_TP_Test InductorTestCase setUp Hmm why specific set_device call rank rank = world_size = torch get_device_module device_type set_device device_type store = FakeStore dist init_process_group backend= fake world_size=self world_size rank=self rank store=store tearDown dist destroy_process_group unittest skip Temporarily disabled due SymInt error ` unhashable type non-nested SymInt ` unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch test_ddp_tp ref_model = Net compiled_replicate_model = deepcopy ref_model mesh_ d = init_device_mesh device_type world_size mesh_dim_names= dp tp tp_mesh = mesh_ d tp dp_mesh = mesh_ d dp parallelize_plan = fc ColwiseParallel fc RowwiseParallel fc ColwiseParallel fc RowwiseParallel ref_model = parallelize_module ref_model tp_mesh parallelize_plan ref_model = replicate ref_model device_mesh=dp_mesh compiled_replicate_model = parallelize_module compiled_replicate_model tp_mesh parallelize_plan compiled_replicate_model = replicate compiled_replicate_model device_mesh=dp_mesh compiled_replicate_model = torch compile compiled_replicate_model data = torch randn DIM compiled_autograd _enable compiler_fn loss = compiled_replicate_model data sum TODO We need pre-dispatch tracing backward graph make work https github com pytorch pytorch issues #issuecomment- assertRaisesRegex AssertionError Expected ProxyTensor got torch distributed tensor DTensor loss backward ref_loss = ref_model data sum ref_loss backward p p zip ref_model parameters compiled_replicate_model parameters assertEqual p grad p grad __name__ == __main__ run_tests