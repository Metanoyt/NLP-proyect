mypy allow-untyped-decorators mypy allow-untyped-defs inspect os warnings concurrent futures Future dataclasses dataclass enum Enum typing cast Optional Union typing_extensions deprecated torch torch distributed dist torch distributed _state_dict_utils STATE_DICT_TYPE torch distributed checkpoint _async_executor noqa TC _AsyncCheckpointExecutor torch distributed checkpoint _async_process_executor _ProcessBasedAsyncCheckpointExecutor torch distributed checkpoint _async_thread_executor _ThreadBasedAsyncCheckpointExecutor torch distributed checkpoint _storage_utils _storage_setup torch distributed checkpoint default_planner DefaultSavePlanner torch distributed checkpoint logger _dcp_method_logger torch distributed checkpoint metadata Metadata torch distributed checkpoint planner SavePlan SavePlanner torch distributed checkpoint staging AsyncStager DefaultStager StagingOptions torch distributed checkpoint stateful Stateful torch distributed checkpoint storage StorageWriter WriteResult torch distributed distributed_c d _get_default_group utils _api_bc_check _DistWrapper _profile __all__ = save_state_dict save async_save AsyncCheckpointerType AsyncSaveResponse AsyncCheckpointerType Enum Enum async checkpointer type THREAD = thread PROCESS = process deprecated ` save_state_dict ` deprecated will removed future versions Please use ` save ` instead category=FutureWarning save_state_dict state_dict STATE_DICT_TYPE storage_writer StorageWriter process_group Optional dist ProcessGroup = None coordinator_rank int = no_dist bool = False planner Optional SavePlanner = None - Metadata This method deprecated Please switch save storage_writer reset TODO test returning ` save ` here instead _profile _save_state_dict state_dict storage_writer process_group coordinator_rank no_dist planner _dcp_method_logger log_exceptions=True type ignore arg-type _api_bc_check save state_dict STATE_DICT_TYPE checkpoint_id Union str os PathLike None = None storage_writer Optional StorageWriter = None planner Optional SavePlanner = None process_group Optional dist ProcessGroup = None no_dist bool = False use_collectives bool = True - Metadata Save distributed model SPMD style This function different ` ` torch save ` ` handles ` ` ShardedTensor ` ` ` ` DTensor ` ` having each rank only save their local shards For each ` ` Stateful ` ` object having both ` ` state_dict ` ` ` ` load_state_dict ` ` save will call ` ` state_dict ` ` before serialization warning There no guarantees Backwards Compatibility across PyTorch versions saved state_dicts warning If using ` process_group ` argument make sure only its ranks call ` save_state_dict ` all data state_dict belong note When saving checkpoint FSDP s ` ShardingStrategy HYBRID_SHARD ` only one shard_group should calling ` save_state_dict ` corresponding process group needs passed note If no process group available function assumes intention save state_dict local process note Rank assumed coordinator rank Args state_dict Dict str Any The state_dict save checkpoint_id Union str os PathLike None The ID checkpoint instance The meaning checkpoint_id depends storage It can path folder file It can also key storage key-value store Default ` ` None ` ` storage_writer Optional StorageWriter Instance StorageWriter used perform writes If specified DCP will automatically infer writer based checkpoint_id If checkpoint_id also None exception will raised Default ` ` None ` ` planner Optional SavePlanner Instance SavePlanner If specified default planner will used Default ` ` None ` ` process_group Optional ProcessGroup ProcessGroup used cross-rank synchronization Default ` ` None ` ` no_dist bool If ` ` True ` ` function will assume intent load checkpoint single rank process Default ` ` False ` ` use_collectives bool If ` ` False ` ` function will assume intent save checkpoint without using cross-rank synchronization Default ` ` True ` ` This configuration experimental should used caution It will change format saved checkpoint may backward compatible Returns Metadata Metadata object saved checkpoint Example xdoctest +SKIP my_model = MyModule state_dict = model my_model fs_storage_writer = torch distributed checkpoint FileSystemWriter checkpoint torch distributed checkpoint save state_dict=state_dict storage_writer=fs_storage_writer note save_state_dict uses collectives coordinate writes across ranks For NCCL-based process groups internal tensor representations objects must moved GPU device before communication takes place In case device used given ` ` torch cuda current_device ` ` user s responsibility ensure set so each rank has individual GPU via ` ` torch cuda set_device ` ` torch _C _log_api_usage_once torch distributed checkpoint save no_dist = no_dist dist is_available dist is_initialized no_dist warnings warn torch distributed disabled unavailable uninitialized assuming intent save single process stacklevel= _profile storage_writer = cast StorageWriter _storage_setup storage_writer checkpoint_id reader=False _save_state_dict state_dict=_stateful_to_state_dict state_dict storage_writer=storage_writer process_group=process_group no_dist=no_dist planner=planner use_collectives=use_collectives dataclass AsyncSaveResponse This contains futures staging upload completion It returned async_save staging_completion future indicates when local copy state_dict complete upload_completion future indicates when checkpoint completed saving staging_completion Future None upload_completion Future None _dcp_method_logger log_exceptions=True async_save state_dict STATE_DICT_TYPE checkpoint_id Union str os PathLike None = None storage_writer Optional StorageWriter = None planner Optional SavePlanner = None process_group Optional dist ProcessGroup = None async_checkpointer_type AsyncCheckpointerType = AsyncCheckpointerType THREAD async_stager Optional AsyncStager = None no_dist bool = False use_collectives bool = True - Union Future AsyncSaveResponse Asynchronous version ` ` save ` ` This code first de-stages state_dict staging storage defaults CPU memory then calls ` save ` separate thread warning This feature experimental subject change MUST CALL CLOSE AFTER LAST CHECKPOINT IS SAVED Args state_dict Dict str Any The state_dict save checkpoint_id Union str os PathLike None The ID checkpoint instance The meaning checkpoint_id depends storage It can path folder file It can also key storage key-value store Default ` ` None ` ` storage_writer Optional StorageWriter Instance StorageWriter used perform stage save If specified DCP will automatically infer writer based checkpoint_id If checkpoint_id also None exception will raised Default ` ` None ` ` planner Optional SavePlanner Instance SavePlanner If specified default planner will used Default ` ` None ` ` process_group Optional ProcessGroup ProcessGroup used cross-rank synchronization Default ` ` None ` ` async_checkpointer_type AsyncCheckpointerType whether do checkpoint separate thread process Default ` ` AsyncCheckpointerType THREAD ` ` async_stager AsyncStager provides staging implementation If storage_writer implements AsyncStager async_stager provided async_stager will used staging no_dist bool If ` ` True ` ` function will assume intent save checkpoint single rank process Default ` ` False ` ` use_collectives If False Save checkpoint without rank coordination Default ` ` True ` ` This configuration experimental should used caution It will change format saved checkpoint may backward compatible Returns Future A future holding resultant Metadata object ` save ` Example xdoctest +SKIP my_model = MyModule state_dict = model my_model fs_storage_writer = torch distributed checkpoint FileSystemWriter checkpoint checkpoint_future = torch distributed checkpoint async_save state_dict=state_dict storage_writer=fs_storage_writer do some work checkpoint_future result torch _C _log_api_usage_once torch distributed checkpoint async_save dist is_available dist is_initialized pg = process_group _get_default_group torch device cpu pg _device_types raise AssertionError A CPU backend must enabled async save try initializing process group cpu gloo cuda nccl async_stager None storage_writer None isinstance storage_writer AsyncStager bwc old storage_writers async_stager = storage_writer async_stager = DefaultStager StagingOptions False False False False state_dict = _stateful_to_state_dict state_dict _dcp_method_logger log_exceptions=True stage_state_dict - Union Future STATE_DICT_TYPE STATE_DICT_TYPE async_stager stage state_dict staging_future_or_state_dict = stage_state_dict upload_executor _AsyncCheckpointExecutor = _ProcessBasedAsyncCheckpointExecutor async_checkpointer_type == AsyncCheckpointerType PROCESS _ThreadBasedAsyncCheckpointExecutor upload_future Future = upload_executor execute_save staging_future_or_state_dict checkpoint_id=checkpoint_id pyrefly ignore bad-argument-type storage_writer=storage_writer planner=planner process_group=process_group no_dist=no_dist use_collectives=use_collectives isinstance staging_future_or_state_dict Future staging_future = staging_future_or_state_dict return_staging_future Future None = Future callback original_staging_future Future STATE_DICT_TYPE return_staging_future Future None = return_staging_future try original_staging_future result return_staging_future set_result None except Exception e return_staging_future set_exception e staging_future done staging_future add_done_callback callback return_staging_future set_result None new AsyncSaveResponse users using new ZOC implementation AsyncSaveResponse staging_completion=return_staging_future upload_completion=upload_future _dcp_method_logger log_exceptions=True maybe_synchronize_staging async_stager should_synchronize_after_execute async_stager synchronize_staging maybe_synchronize_staging upload_future _dcp_method_logger log_exceptions=True _stateful_to_state_dict state_dict STATE_DICT_TYPE - STATE_DICT_TYPE Creates shallow copy ` state_dict ` where ` state_dict ` called each Stateful object stateful_state_dict = key elem state_dict items stateful_state_dict key = elem state_dict isinstance elem Stateful elem stateful_state_dict _save_state_dict state_dict STATE_DICT_TYPE storage_writer StorageWriter process_group Optional dist ProcessGroup = None coordinator_rank int = no_dist bool = False planner Optional SavePlanner = None use_collectives bool = True - Metadata torch _C _log_api_usage_once torch distributed checkpoint save_state_dict distW = _DistWrapper process_group no_dist coordinator_rank planner None planner = DefaultSavePlanner planner None raise AssertionError planner None global_metadata = None ckpt_kwargs = ckpt_id = getattr storage_writer checkpoint_id None None ckpt_kwargs checkpoint_id = ckpt_id ckpt_kwargs process_group = distW group _dcp_method_logger ckpt_kwargs local_step planner None raise AssertionError planner None storage_meta = storage_writer storage_meta storage_meta inspect signature planner set_up_planner parameters warnings warn The function definition SavePlanner set_up_planner has been updated include storage_meta argument Please update your implementation include parameter stacklevel= planner set_up_planner state_dict distW is_coordinator type ignore call-arg arg-type planner set_up_planner state_dict=state_dict storage_meta=storage_meta is_coordinator=distW is_coordinator kwargs inspect signature storage_writer set_up_storage_writer parameters storage_writer set_up_storage_writer distW is_coordinator rank=distW rank use_collectives=use_collectives storage_writer set_up_storage_writer distW is_coordinator local_plan = planner create_local_plan local_plan = storage_writer prepare_local_plan local_plan local_plan _dcp_method_logger ckpt_kwargs global_step all_local_plans nonlocal global_metadata planner None raise AssertionError planner None all_local_plans global_metadata = planner create_global_plan all_local_plans all_local_plans = storage_writer prepare_global_plan all_local_plans all_local_plans central_plan Optional SavePlan = None use_collectives central_plan = distW reduce_scatter plan local_step global_step local_plan SavePlan = local_step global_plan list SavePlan = global_step local_plan central_plan = global_plan _dcp_method_logger ckpt_kwargs write_data planner None raise AssertionError planner None central_plan None raise AssertionError central_plan None final_local_plan = planner finish_plan central_plan all_writes = storage_writer write_data final_local_plan planner all_writes wait all_writes value _dcp_method_logger ckpt_kwargs finish_checkpoint all_results global_metadata None raise AssertionError global_metadata None storage_writer finish metadata=global_metadata results=all_results global_metadata use_collectives metadata = distW all_reduce write write_data finish_checkpoint write_results list WriteResult = write_data metadata = finish_checkpoint write_results distW barrier metadata