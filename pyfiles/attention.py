This copy rnn_attention MLPerf some common sizes hardcoded benchmarking some control flow stripped out https github com mlcommons training blob master retired_benchmarks gnmt pytorch seq seq models attention py torch benchmark BahdanauAttention benchmark Benchmark __init__ mode device dtype b t_q t_k n super __init__ mode device dtype b = b t_q = t_q t_k = t_k n = n att_query = rand b t_q n device=device dtype=dtype requires_grad=self requires_grad att_keys = rand b t_k n device=device dtype=dtype requires_grad=self requires_grad normalize_bias = rand n device=device dtype=dtype requires_grad=self requires_grad linear_att = rand n device=device dtype=dtype requires_grad=self requires_grad inputs = att_query att_keys normalize_bias linear_att forward att_query att_keys normalize_bias linear_att Calculate Bahdanau score param att_query b x t_q x n param att_keys b x t_k x n b x t_q x t_k scores b t_k n = att_keys size t_q = att_query size att_query = att_query unsqueeze expand b t_q t_k n att_keys = att_keys unsqueeze expand b t_q t_k n sum_qk = att_query + att_keys + normalize_bias out = torch tanh sum_qk matmul linear_att out reference numpy forward inputs config b t_q t_k n staticmethod module attention memory_workload memsize t t numel t element_size input_size = memsize att_query + memsize att_keys + memsize normalize_bias + memsize linear_att output_size = torch Size b t_q t_k numel io_size = input_size + output_size If matmul fused must write then read ` sum_qk ` intermediate_size = torch Size b t_q t_k n numel sol io_size algorithmic io_size + intermediate_size staticmethod default_configs mlperf_inference = nvidia = mlperf_inference nvidia benchmark register_benchmark_class BahdanauAttention