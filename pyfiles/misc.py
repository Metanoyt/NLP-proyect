mypy ignore-errors This module contains miscellaneous variable tracker implementations various Python types features used Dynamo s symbolic execution These classes help track propagate information about different kinds variables during graph capture Key classes include - SuperVariable Handles super calls method resolution - ExceptionVariable Tracks exception objects - RandomVariable Manages random number generators - GetAttrVariable Tracks attribute access - MethodWrapperVariable Handles method wrappers - PythonModuleVariable Tracks Python modules - NumpyVariable Handles numpy functions types - StringFormatVariable Manages string formatting - DebuggingVariable Handles print logging dataclasses functools inspect itertools random re sys types warnings typing Optional TYPE_CHECKING torch _C torch _numpy tnp torch utils _pytree pytree config graph_break_hints trace_rules variables bytecode_transformation create_call_function create_call_function_ex create_instruction create_parameter_op do_not_convert_to_tracable_parameter exc raise_observed_exception unimplemented unimplemented_v guards GuardBuilder install_guard mutation_guard unpatched_nn_module_init source AttrSource GenericAttrSource GetItemSource TypeMROSource TypeSource WeakRefCallSource utils check_unspec_or_constant_args cmp_name_to_op_mapping identity is_tensor_base_attr_getter istype list_methods proxy_args_kwargs raise_args_mismatch tuple_methods base raise_type_error_exc VariableTracker constant ConstantVariable functions NestedUserFunctionVariable UserFunctionVariable user_defined call_random_fn is_standard_setattr UserDefinedObjectVariable TYPE_CHECKING torch _dynamo codegen PyCodegen torch _dynamo symbolic_convert InstructionTranslator NO_SUCH_SUBOBJ pass SuperVariable VariableTracker _nonvar_fields = VariableTracker _nonvar_fields __init__ typevar objvar=None kwargs - None super __init__ kwargs typevar first argument super In case where no argument provided super __class__ object where super function being called typevar = typevar objvar here must instance subtype typevar In case where super called without arguments first argument current function where super called regular method cls classmethod objvar = objvar reconstruct codegen PyCodegen codegen add_push_null lambda codegen variables BuiltinVariable super codegen typevar objvar None codegen objvar codegen extend_output create_call_function False codegen extend_output create_call_function False _resolved_getattr_and_source tx InstructionTranslator name objvar unimplemented_v gb_type= -arg super implemented context= explanation=f Dynamo failed trace attribute ` name ` accessed f via ` super ` type ` typevar ` object ` objvar ` because one-argument super supported hints= Use two-argument super type object_or_type search_type = typevar as_python_constant The rest function does two things - Walk mro find where attribute comes able provide accurate source - Call getattr get object Find object where function lives When objvar use type when objvar cls use as-is type_to_use = objvar python_type type_to_use_source = TypeSource objvar source objvar source None issubclass type_to_use type type_to_use = objvar value type_to_use_source = objvar source source = None search_mro = type_to_use __mro__ try start_index = search_mro index search_type + except ValueError Corner case where typevar mro objvar https github com python cpython blob Objects typeobject c#L -L getattr super search_type type_to_use name None Implemented based https github com python cpython blob Objects typeobject c#L super has its getattro implementation The key point instead calling getattr checks attribute __dict__ index range start_index len search_mro Dont call getattr just check __dict__ resolved_getattr = search_mro index __dict__ get name NO_SUCH_SUBOBJ resolved_getattr NO_SUCH_SUBOBJ Equivalent something like type L __mro__ attr_name type_to_use_source source = AttrSource GetItemSource TypeMROSource type_to_use_source index name resolved_getattr source unimplemented_v gb_type= Unable resolve super getattr context= explanation=f Dynamo failed trace attribute ` name ` accessed f via ` super ` type ` typevar ` object ` objvar ` because resolved attribute type supported hints= Ensure attribute exists parent Check arguments passed ` super ` var_getattr tx InstructionTranslator name str - VariableTracker Check getattr constant If delay actual work wrapping result GetAttrVariable Mostly super called method so most work delayed call_function We could have just implemented const_getattr However super special when comes finding sources Compared other VTs super requires attr name walk mro find actual source just AttrSource value source = _resolved_getattr_and_source name variables ConstantVariable is_literal value GetAttrVariable name source install_guard source make_guard GuardBuilder CONSTANT_MATCH variables ConstantVariable create value source=source call_method tx InstructionTranslator name args list VariableTracker kwargs dict str VariableTracker - VariableTracker inner_fn source = _resolved_getattr_and_source name This essentially simulates CPython s ` super_getattro ` https github com python cpython blob c d c bcf d edf e ad f b f Objects typeobject c#L -L where ` inner_fn ` VT ` res = _super_lookup_descr ` However ` res ` s type needs checked ` tp_descr_get ` applied has one We currently don t have polyfills all relevant ` tp_descr_get ` so we explicitly handle cases we care about here e g note staticmethod classmethod cases inner_fn object __init__ LambdaVariable identity inner_fn torch nn Module __init__ objvar = objvar side_effects AttributeMutationNew isinstance objvar variables UserDefinedObjectVariable isinstance objvar mutation_type AttributeMutationNew args kwargs do_not_convert_to_tracable_parameter fn_vt = VariableTracker build tx unpatched_nn_module_init source=source fn_vt call_function tx objvar + args kwargs unimplemented_v gb_type= Unsupported super __init__ call context=f call_method name args kwargs explanation= Dynamo encountered super __init__ call f objvar resolved ` torch nn Module __init__ ` call we cannot trace hints= graph_break_hints DIFFICULT objvar source hasattr inner_fn __name__ inner_fn __name__ == __new__ variables UserDefinedClassVariable is_supported_new_method inner_fn user_cls = inner_fn __self__ hasattr user_cls __module__ user_cls __module__ == builtins user_cls_vt = variables BuiltinVariable user_cls user_cls_source = source member user_cls_vt = variables UserDefinedClassVariable user_cls source=user_cls_source user_cls_vt call_method tx __new__ args kwargs isinstance inner_fn staticmethod isinstance inner_fn __func__ types FunctionType fn_vt = VariableTracker build tx inner_fn __func__ source=source fn_vt call_function tx args kwargs isinstance inner_fn classmethod isinstance inner_fn __func__ types FunctionType isinstance objvar variables UserDefinedClassVariable super classmethod called classmethod itself So super converted super __class__ cls bytecode therefore we have propagate cls cls_variable = objvar current function instance method therefore super converted super __class__ We have find type bind cls parent classmethod Note can t typevar because __class__ where method defined which could different type polymorphism cls_source = None objvar source cls_source = TypeSource objvar source cls_variable = VariableTracker build tx objvar value_type cls_source fn_vt = VariableTracker build tx inner_fn __func__ source=AttrSource source __func__ fn_vt call_function tx cls_variable args kwargs isinstance inner_fn types FunctionType fn_vt = VariableTracker build tx inner_fn source=source fn_vt call_function tx objvar + args kwargs isinstance inner_fn types MethodType variables UserMethodVariable inner_fn __func__ objvar source=source call_function tx args kwargs is_standard_setattr inner_fn isinstance objvar UserDefinedObjectVariable objvar method_setattr_standard tx args kwargs inner_fn object __delattr__ attr = args try attr = attr as_python_constant except NotImplementedError exc unimplemented_v gb_type= Non-constant attribute given ` super __delattr__ ` context=f call_method name explanation= Dynamo requires attribute name passed ` super __delattr__ ` constant string hints= Ensure attribute name string literal constant variable from_exc=exc tx output side_effects is_attribute_mutation objvar unimplemented_v gb_type= Attempted super __delattr__ object without mutation tracking context=f call_method name explanation= Dynamo needs track mutations object before ` super __delattr__ ` can used But f object objvar doesn t have attribute mutation tracking enabled hints= Ensure object tracked Dynamo s side effect system graph_break_hints DYNAMO_BUG tx output side_effects store_attr objvar attr variables DeletedVariable variables ConstantVariable None isinstance objvar variables UserDefinedDictVariable inner_fn objvar _dict_methods objvar _dict_vt call_method tx name args kwargs isinstance objvar variables UserDefinedSetVariable inner_fn objvar _set_methods objvar _set_vt call_method tx name args kwargs isinstance objvar variables UserDefinedTupleVariable inner_fn tuple_methods objvar _tuple_vt call_method tx name args kwargs isinstance objvar variables UserDefinedListVariable inner_fn list_methods objvar _list_vt call_method tx name args kwargs inner_fn object __getattribute__ object __getattribute__ has no side-effects We can directly call __getattribute__ access attribute attr_name = args value tx output side_effects has_pending_mutation_of_attr objvar attr_name result = tx output side_effects load_attr objvar attr_name deleted_ok=True isinstance result variables DeletedVariable raise_observed_exception AttributeError tx result try NB - use object __getattribute__ prevent running any user code attr_value = object __getattribute__ objvar value attr_name except AttributeError raise_observed_exception AttributeError tx attr_source = None objvar source None setup object __getattribute__ objvar name source attr_source = GenericAttrSource objvar source attr_name VariableTracker build tx attr_value attr_source inner_fn torch _C _disabled_torch_function_impl See ` THPModule_disable_torch_function ` C impl The signature _disabled_torch_function_impl similar ` __torch_function__ ` just without first ` cls ` argument func types args kwargs func = args tf_kwargs = tf_args = args items hash_key_vt value_vt args items items key_str = hash_key_vt vt as_python_constant tf_kwargs key_str = value_vt tx_old = tx symbolic_torch_function_state torch_function_subclass_enabled tx symbolic_torch_function_state torch_function_subclass_enabled = False try func call_function tx tf_args tf_kwargs finally tx symbolic_torch_function_state torch_function_subclass_enabled = tx_old isinstance inner_fn types MethodDescriptorType inner_fn trace_rules get_tensor_method FunctionType implementation C we support some these e g tensor ops like ` torch Tensor ` fn_var = VariableTracker build tx inner_fn source fn_var call_function tx objvar + args kwargs unimplemented_v gb_type= Attempted call super attribute function method context=f call_method name explanation= Dynamo does know how trace call f ` super name ` because ` super name ` function method attribute hints= Ensure attribute accessed via ` super ` standard method function ExceptionVariable VariableTracker The ExceptionVariable corresponds BaseException Python __init__ exc_type args init_kwargs=None source=None mutation_type=None - None super __init__ source=source mutation_type=mutation_type exc_type = exc_type args = args init_kwargs unimplemented_v gb_type= Keyword args passed exception constructor context=f kwargs init_kwargs explanation= Dynamo does know how handle keyword args passed exception constructor hints= graph_break_hints SUPPORTABLE When raising new exception while another exception already being handled new exception s __context__ attribute automatically set handled exception __context__ = ConstantVariable None Set when user raised exception another raise __cause__ = ConstantVariable None Boolean flag controls whether __context__ attribute set __suppress_context__ = ConstantVariable False Contains call stack where exception raised Dynamo does track traceback So variable always set None __traceback__ = ConstantVariable None set_context context ExceptionVariable __context__ = context reconstruct codegen PyCodegen codegen add_push_null lambda codegen load_import_from builtins exc_type __name__ codegen foreach args codegen call_function len args False codegen_attr name str - None attr = getattr name istype attr ConstantVariable assert attr value True False None attr codegen dup_top codegen attr codegen extend_output codegen rot_n codegen store_attr name codegen_attr __context__ codegen_attr __cause__ codegen_attr __suppress_context__ python_type exc_type call_setattr tx InstructionTranslator name_var VariableTracker val VariableTracker raise_error msg raise_observed_exception TypeError tx args= ConstantVariable msg name = name_var as_python_constant name == __context__ set_context val name == __cause__ isinstance val ConstantVariable val value None isinstance val variables BuiltinVariable variables ExceptionVariable variables UserDefinedExceptionClassVariable variables UserDefinedExceptionObjectVariable __cause__ = val __suppress_context__ = variables ConstantVariable True raise_error exception cause must None derive BaseException name == __suppress_context__ isinstance val ConstantVariable val value True False __suppress_context__ = val raise_error exception cause must None derive BaseException name == __traceback__ isinstance val ConstantVariable val value None __traceback__ = val unimplemented_v gb_type= Set Exception object ` __traceback__ ` attribute not- ` None ` context=f call_setattr name explanation= Dynamo does support setting attribute __traceback__ tracked exception objects anything other than None hints= Avoid setting __traceback__ exception objects within traced code set None unimplemented_v gb_type= Unsupported attribute assignment Exception object context=f call_setattr name explanation= Dynamo does support setting attribute f name tracked exception objects Only ` __context__ ` ` __cause__ ` ` __suppress_context__ ` ` __traceback__ ` supported hints= graph_break_hints SUPPORTABLE variables ConstantVariable None call_method tx name args kwargs name == __setattr__ call_setattr tx args name == with_traceback tb = args call_setattr tx ConstantVariable __traceback__ tb super call_method tx name args kwargs var_getattr tx name name == __context__ __context__ name == __cause__ __cause__ name == __suppress_context__ __suppress_context__ name == __traceback__ variables ConstantVariable None name == args variables ListVariable args source=self source super var_getattr tx name __str__ f __class__ __name__ exc_type __repr__ = __str__ UnknownVariable VariableTracker It could anything DelayGraphBreakVariable UnknownVariable Used insert dummy variable stack do graph break CALL_FUNCTION __init__ msg=None kwargs super __init__ kwargs msg = msg call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker unimplemented_v gb_type= Unsupported function call delayed context=f source source explanation= Dynamo determined graph break should occur f when calling ` source name ` Reason msg hints= ComptimeVariable VariableTracker This variable special lets you execute arbitrary code Dynamo compile time reconstruct codegen PyCodegen raise NotImplementedError comptime special form var_getattr tx InstructionTranslator name str - VariableTracker comptime comptime To support comptime print_graph convenience accessors VariableTracker build tx getattr comptime name source=AttrSource source name call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker comptime ComptimeContext TODO support expression form well Second argument runtime lambda ignored kwargs len args raise_args_mismatch tx comptime most args kwargs f len args args len kwargs kwargs fn = args isinstance fn UserFunctionVariable fn get_function ComptimeContext tx isinstance fn NestedUserFunctionVariable We have manually bind freevars ourselves code = fn get_code fn closure raise_type_error_exc tx f comptime function must have free variables these variables free code co_freevars func = types FunctionType code fn f_globals fn fn_name as_python_constant tuple fn defaults items fn defaults None We could automatically promote free variables into ComptimeVar confusing you access free variable we actually DO have runtime value tuple make_cell ComptimeVar i i fn closure items func ComptimeContext tx raise RuntimeError f unsupported argument comptime type fn variables ConstantVariable create None CellVariable VariableTracker If cell existed before Dynamo tracing started will VariableTracker represents cell content Note all mutation cell i e its content will buffered SideEffects rather than being reflected here One can think ` CellVariable ` special case ` UserDefinedObjectVariable ` pre_existing_contents Optional VariableTracker This set when cell can referenced via ` LOAD STORE_DEREF ` root frame via name e g name ` co_cellvars co_freevars ` local_name Optional str = None __init__ pre_existing_contents Optional VariableTracker = None kwargs - None super __init__ kwargs pre_existing_contents = pre_existing_contents NewGlobalVariable VariableTracker __init__ kwargs - None super __init__ kwargs produce_trampoline_autograd_apply fn_cls trampoline_autograd_apply args kwargs fn_cls apply args kwargs trampoline_autograd_apply _origin = produce_trampoline_autograd_apply trampoline_autograd_apply AutogradFunctionVariable VariableTracker represents torch autograd Function subclass _nonvar_fields = fn_cls VariableTracker _nonvar_fields __init__ fn_cls kwargs - None super __init__ kwargs fn_cls = fn_cls call_apply tx InstructionTranslator args kwargs requires_grad = False visit vt nonlocal requires_grad isinstance vt variables TensorVariable vt requires_grad False requires_grad = True isinstance vt variables NNModuleVariable vt is_training tx requires_grad = True VariableTracker visit visit args kwargs requires_grad torch is_grad_enabled config capture_autograd_function False warnings warn The config capture_autograd_function flag deprecated s now always true torch _functorch autograd_function autograd_function_forward_rewritten torch autograd function _is_setup_context_defined forward_fn = fn_cls forward is_setup_ctx_defined = _is_setup_context_defined fn_cls setup_context is_setup_ctx_defined If setup_context defined we generate new forward function which includes original forward setup_context function trace new forward function forward_fn = autograd_function_forward_rewritten fn_cls forward fn_cls setup_context vjp_fn = fn_cls vjp type ignore attr-defined vjp_fn torch autograd Function vjp unimplemented_v gb_type= Unsupported custom vjp context=f call_apply args kwargs explanation= Dynamo does support tracing ` torch autograd Function ` subclasses define custom ` vjp ` method hints= Remove custom ` vjp ` method possible Use standard ` backward ` instead applicable graph_break_hints SUPPORTABLE jvp_fn = fn_cls jvp type ignore attr-defined jvp_fn torch autograd Function jvp unimplemented_v gb_type= Unsupported custom jvp context=f call_apply args kwargs explanation= Dynamo does support tracing ` torch autograd Function ` subclasses define custom ` jvp ` method hints= Remove custom ` jvp ` method possible graph_break_hints SUPPORTABLE higher_order_ops AutogradFunctionApplyVariable source = source source None source = AttrSource tx import_source fn_cls __module__ fn_cls __name__ val = AutogradFunctionApplyVariable forward_fn fn_cls backward source source=AttrSource source member= apply call_function tx args kwargs Inside AutogradFunctionApplyVariable call_function we use sourceless variable wrapping forward function we don t want generate guards new_forward __closure__ forward rewritten autograd_function_forward_rewritten But we still need generate correct guards original forward setup_context functions so we have add guards manually source fwd_src = AttrSource source forward install_guard fwd_src make_guard GuardBuilder CLOSURE_MATCH is_setup_ctx_defined setup_ctx_src = AttrSource source setup_context install_guard setup_ctx_src make_guard GuardBuilder CLOSURE_MATCH val source source = AttrSource source forward source = None fn = fn_cls forward ctx = AutogradFunctionContextVariable create tx args kwargs args = ctx args isinstance fn types FunctionType sig = inspect signature fn len args - == len sig _parameters args = args Don t use context fn_vt = VariableTracker build tx fn source=source fn_vt call_function tx args kwargs isinstance fn types MethodType variables UserMethodVariable fn __func__ variables UserDefinedClassVariable fn_cls source=source call_function tx args kwargs unimplemented_v gb_type= Non-function method subclass torch autograd Function context=f call_apply args kwargs explanation= Dynamo requires ` forward ` attribute ` torch autograd Function ` subclass standard Python f function method Found type ` type fn __name__ ` instead hints= Ensure ` forward ` method defined regular function instance method call_backward tx InstructionTranslator args kwargs fn = fn_cls backward assert type args value torch _dynamo external_utils FakeBackwardCFunction assert isinstance fn types FunctionType fn_source = AttrSource source backward fn_vt = VariableTracker build tx fn source=fn_source fn_vt call_function tx args kwargs call_function tx InstructionTranslator args kwargs AutogradFunctionVariable fn_cls call_method tx InstructionTranslator name args list VariableTracker kwargs dict str VariableTracker builder wrap_fx_proxy name == apply trace_rules is_callable_allowed fn_cls trampoline_autograd_apply = produce_trampoline_autograd_apply fn_cls wrap_fx_proxy tx=tx proxy=tx output create_proxy call_function trampoline_autograd_apply proxy_args_kwargs args kwargs call_apply tx args kwargs name == backward call_backward tx args kwargs source = AttrSource source name source None None try obj = inspect getattr_static fn_cls name except AttributeError obj = None isinstance obj staticmethod func = obj __get__ fn_cls source None trace_rules lookup func create_with_source func source=source call_function tx args kwargs trace_rules lookup func func call_function tx args kwargs isinstance obj classmethod variables UserMethodVariable obj __func__ source=source call_function tx args kwargs unimplemented_v gb_type= Unsupported autograd Function method context=f call_method name explanation= Dynamo does support calling method f ` name ` directly ` torch autograd Function ` instance Supported methods include ` apply ` ` backward ` static methods methods hints= Ensure method decorated ` staticmethod ` ` classmethod ` s meant called dataclasses dataclass SavedTensorBox tensors list VariableTracker = dataclasses field default_factory=list AutogradFunctionContextVariable UserDefinedObjectVariable Tracks autograd Function context using mutation tracking side_effects py _nonvar_fields = proxy inference saved_tensors UserDefinedObjectVariable _nonvar_fields __init__ value value_type=None inference=False saved_tensors=None needs_input_grad=None non_differentiable=None kwargs - None super __init__ value=value value_type=value_type kwargs inference = inference saved_tensors = saved_tensors needs_input_grad = needs_input_grad non_differentiable = non_differentiable staticmethod create tx InstructionTranslator args=None kwargs=None needs_input_grad = None args kwargs needs_input_grad = tuple isinstance x variables TensorVariable x requires_grad x args out = tx output side_effects track_object_new None torch autograd function FunctionCtx functools partial AutogradFunctionContextVariable inference=True saved_tensors=SavedTensorBox needs_input_grad=needs_input_grad out as_proxy proxy None unimplemented_v gb_type= proxy set context=f as_proxy explanation= Dynamo requires autograd Function context initialized proxy hints= graph_break_hints DYNAMO_BUG proxy call_method tx InstructionTranslator name args list VariableTracker kwargs dict str VariableTracker - VariableTracker name == __setattr__ super call_method tx name args kwargs name == mark_non_differentiable kwargs raise_args_mismatch tx name kwargs f len kwargs kwargs non_differentiable = proxy_args_kwargs args variables ConstantVariable create None name = save_for_backward unimplemented_v gb_type= Unsupported autograd Function context method context=f call_method name explanation= Dynamo does support calling method f ` name ` ` autograd Function ` context objects Supported methods ` __setattr__ ` ` save_for_backward ` ` mark_non_differentiable ` hints= graph_break_hints SUPPORTABLE saved_tensors None unimplemented_v gb_type= Unsupported autograd Function context ` save_for_backward ` context=f call_method name explanation= Dynamo requires ` saved_tensors ` attribute initialized ` autograd Function ` context object hints= Ensure ` saved_tensors ` attribute properly initialized before calling ` save_for_backward ` ` save_for_backward ` only supported newly constructed ` torch autograd function FunctionCtx ` inference kwargs source raise_type_error_exc tx save_for_backward requires source no keyword arguments tx output side_effects track_save_for_backward args In eager mode multiple calls save_for_backward will overwrite previous calls len saved_tensors tensors saved_tensors tensors = arg args saved_tensors tensors append arg variables ConstantVariable create None var_getattr tx InstructionTranslator name name save_for_backward mark_non_differentiable LambdaVariable lambda args kwargs call_method tx name args kwargs name == saved_tensors saved_tensors None variables TupleVariable list saved_tensors tensors name == needs_input_grad needs_input_grad None variables ConstantVariable create needs_input_grad source source = AttrSource source needs_input_grad VariableTracker build tx value needs_input_grad source super var_getattr tx name AutogradEngineVariable UserDefinedObjectVariable Represents torch _C _ImperativeEngine instance __init__ value value_type=None kwargs - None super __init__ value=value value_type=value_type kwargs call_method tx InstructionTranslator name args list VariableTracker kwargs dict str VariableTracker - VariableTracker name == queue_callback torch _dynamo compiled_autograd in_compiled_autograd_region assert tx one_graph tx error_on_graph_break queue_callback only supported when Compiled Autograd enabled fullgraph=True queue_callback method-wrapper no need insert guard fn_vt = VariableTracker build tx torch _dynamo external_utils FakeCompiledAutogradEngine queue_callback fn_vt call_function tx tx output side_effects get_ca_final_callbacks_var args kwargs unimplemented_v gb_type= Unsupported torch _C _ImperativeEngine queue_callback context=f call_method name explanation= queue_callback only supported when Compiled Autograd enabled fullgraph=True hints= unimplemented_v gb_type= Unsupported torch _C _ImperativeEngine method context=f call_method name explanation= Dynamo only supports ` queue_callback ` method f torch _C _ImperativeEngine instance found ` name ` hints= LambdaVariable VariableTracker __init__ fn kwargs - None super __init__ kwargs fn = fn call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker fn args kwargs GetAttrVariable VariableTracker _nonvar_fields = name py_type VariableTracker _nonvar_fields __init__ obj name py_type=None kwargs - None super __init__ kwargs assert isinstance obj VariableTracker assert isinstance name str obj = obj name = name py_type = py_type In some cases we know type ex tensor methods python_type py_type None py_type super python_type __repr__ - str f __class__ __name__ obj name staticmethod create_getattr_proxy base_proxy torch fx Proxy attr getattr base_proxy attr as_proxy GetAttrVariable create_getattr_proxy obj as_proxy name as_python_constant constant = obj as_python_constant try getattr constant name except AttributeError raise NotImplementedError f constant None const_getattr tx InstructionTranslator name isinstance obj variables NNModuleVariable raise NotImplementedError step = tx output get_submodule obj module_key name step __dict__ raise NotImplementedError step = inspect getattr_static step name name step __dict__ raise NotImplementedError inspect getattr_static step name reconstruct codegen PyCodegen codegen obj codegen extend_output codegen create_load_attrs name call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker obj call_method tx name args kwargs call_method tx InstructionTranslator name args list VariableTracker kwargs dict str VariableTracker - VariableTracker name __getitem__ get name == __dict__ kwargs args is_python_constant isinstance obj variables UserDefinedObjectVariable variables NNModuleVariable variables UserDefinedClassVariable obj = obj key = args as_python_constant obj has_key_in_generic_dict tx key redirect var_getattr original obj obj var_getattr tx key Return default value get name == get len args == args variables ConstantVariable None name == __contains__ name == __dict__ len args == args is_python_constant kwargs isinstance obj variables UserDefinedObjectVariable variables NNModuleVariable variables UserDefinedClassVariable obj = obj key = args as_python_constant obj has_key_in_generic_dict tx key variables ConstantVariable True variables ConstantVariable False name == __setitem__ name == __dict__ kwargs isinstance obj variables UserDefinedObjectVariable Bypass any custom setattr we updating ` __dict__ ` itself obj method_setattr_standard tx args args directly_update_dict=True isinstance obj variables NNModuleVariable This matches how ` setattr ` handled NNModuleVariable obj convert_to_unspecialized tx super call_method tx name args kwargs get_forwarded_dict tx assert name == __dict__ isinstance obj variables UserDefinedClassVariable tx output side_effects has_pending_mutation obj obj ban_mutation = True VariableTracker build tx obj value __dict__ source MethodWrapperVariable VariableTracker __init__ method_wrapper kwargs - None super __init__ kwargs method_wrapper = method_wrapper _builtin_fns = call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker is_tensor_base_attr_getter method_wrapper isinstance args variables TensorVariable len args == len kwargs == raise_type_error_exc tx tensor attribute getter takes exactly one argument args var_getattr tx method_wrapper __self__ __name__ method-wrapper variables common __init__ calls For example str foo __init__ method-wrapper These method wrappers point C functions Here we intercept these method-wrappers builtins then call function counterpart directly obtaining object self_obj = method_wrapper __self__ wrapper_name = method_wrapper __name__ TODO dynamo-team - We can perhaps expand scope more names more builtins wrapper_name == __init__ fn_obj = type self_obj __init__ fn_obj object __init__ variables BuiltinVariable object call_method tx wrapper_name self_obj args kwargs super call_function tx args kwargs is_python_constant True as_python_constant method_wrapper GetSetDescriptorVariable VariableTracker __init__ desc kwargs - None super __init__ kwargs desc = desc var_getattr tx InstructionTranslator name name == __get__ source source = AttrSource source __get__ VariableTracker build tx desc __get__ source super var_getattr tx name is_python_constant True as_python_constant desc PythonModuleVariable VariableTracker _nonvar_fields = value is_torch VariableTracker _nonvar_fields __init__ value types ModuleType kwargs - None super __init__ kwargs value = value is_torch = value torch value __name__ startswith torch python_type types ModuleType as_python_constant value __repr__ - str f PythonModuleVariable value call_obj_hasattr tx InstructionTranslator name result = hasattr value name variables ConstantVariable create result var_getattr tx InstructionTranslator name tx output side_effects has_pending_mutation_of_attr name tx output side_effects load_attr name is_torch name value __dict__ try attr_value = getattr value name except AttributeError raise_observed_exception AttributeError tx attr_value = value __dict__ name source = source AttrSource source name VariableTracker build tx attr_value source TypingVariable VariableTracker __init__ value kwargs - None super __init__ kwargs value = value call_method tx InstructionTranslator name args list VariableTracker kwargs dict str VariableTracker - VariableTracker Create new typing variable e g ` List int ` name == __getitem__ len args == new_typing = value args as_python_constant TypingVariable new_typing unimplemented unsupported method call typing variable var_getattr tx InstructionTranslator name str builder SourcelessBuilder VariableBuilder name cmp_name_to_op_mapping variables GetAttrVariable name tx output side_effects has_pending_mutation_of_attr name tx side_effects load_attr name value = getattr value name source attr_source = AttrSource source name VariableBuilder tx attr_source value SourcelessBuilder create tx value as_python_constant value reconstruct codegen PyCodegen - None isinstance value types GenericAlias super reconstruct codegen We re just trying load type here Reconstructing type scratch tricky - type like ` typing List int ` we d need deconstruct origin args The origin ` List int ` ` list ` args ` int ` When we recombine those we get parts back need emit code ` typing List int ` But s worse than - what ` typing ` isn t globals loaded like ` typing _typing _typing List int ` so we really need do something like ` sys modules typing List int ` Argh - what they rewrote global ` int ` So we have do ` sys modules typing List sys modules builtins int ` But where do we get ` sys ` What they never imported have something ELSE called ` sys ` Let s skip all noise just emit simple const codegen append_output codegen create_load_const value functools lru_cache maxsize= get_np_to_tnp_map This generates mapping numpy modules their torch _numpy modules equivalents utils NP_TO_TNP_MODULE np_fn_to_tnp_fn = np_mod tnp_mod NP_TO_TNP_MODULE items fn_name tnp_fn tnp_mod __dict__ items callable tnp_fn some internal details do leak tnp which part numpy API np_fn = getattr np_mod fn_name None np_fn_to_tnp_fn np_fn = tnp_fn np_fn_to_tnp_fn functools lru_cache maxsize= get_tnp_to_np_map This just reverse mapping get_np_to_tnp_map - mapping torch _numpy modules numpy equivalents m = get_np_to_tnp_map v k k v m items NumpyVariable VariableTracker Wrapper around ` numpy ` Currently able trace small subset numpy functions well numpy dtypes constant_fold_functions = tnp issubdtype __init__ value kwargs - None super __init__ kwargs value = value classmethod can_constant_fold_through cls fn mod = fn __module__ split assert len mod = mod == torch _numpy fn cls constant_fold_functions classmethod get_constant_collection_for_func cls fn mod = fn __module__ split assert len mod = mod == torch _numpy np_constant_collections_map get fn call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker config trace_numpy unimplemented f numpy value utils numpy_to_tensor_wrapper tensor NumpyNdarrayVariable func = get_np_to_tnp_map get value func None unimplemented f Can t find numpy function value torch _numpy Please file issue request support function We dealing function produces const collection type np dtype np iinfo np finfo collection_variable_typ = get_constant_collection_for_func func None try collection_variable_typ value x as_python_constant x args k v as_python_constant k v kwargs items except NotImplementedError unimplemented f value __name__ non-const args args kwargs func __module__ == torch _numpy random config use_numpy_random_stream msg = f delegate func __qualname__ NumPy itself via msg += f config use_numpy_random_stream= config use_numpy_random_stream unimplemented msg args kwargs = NumpyNdarrayVariable patch_args func __name__ args kwargs can_constant_fold_through func check_unspec_or_constant_args args kwargs constant fold variables ConstantVariable create as_python_constant x as_python_constant x args k v as_python_constant k v kwargs items TODO Add all functions go constants constants can_constant_fold_through proxy = tx output create_proxy call_function numpy_to_tensor_wrapper func proxy_args_kwargs args kwargs NumpyNdarrayVariable create tx proxy call_method tx InstructionTranslator name args list VariableTracker kwargs dict str VariableTracker - VariableTracker unimplemented numpy as_python_constant value as_proxy config trace_numpy isinstance value type This handles numpy dtype attributes such np float We string we don t want serialize non-PyTorch objects output FX graph In torch _numpy we normalize strings their dtypes when input dtype NumPy does value __name__ super as_proxy Used keep track NULLs pushed stack Python function calls NullVariable VariableTracker __init__ kwargs - None super __init__ kwargs __repr__ - str NullVariable reconstruct codegen PyCodegen sys version_info unimplemented cannot reconstruct NullVariable Python codegen append_output create_instruction PUSH_NULL DeletedVariable VariableTracker Marker used implement delattr StringFormatVariable VariableTracker Represents call str format we delay calling format until after graph _nonvar_fields = format_string VariableTracker _nonvar_fields classmethod create cls format_string sym_args sym_kwargs all x is_python_constant x itertools chain sym_args sym_kwargs values variables ConstantVariable create format_string format v as_python_constant v sym_args k v as_python_constant k v sym_kwargs items cls format_string list sym_args dict sym_kwargs __init__ format_string sym_args sym_kwargs kwargs - None super __init__ kwargs assert isinstance format_string str format_string = format_string sym_args = sym_args sym_kwargs = sym_kwargs __repr__ - str f __class__ __name__ format_string r sym_args r sym_kwargs r reconstruct codegen PyCodegen codegen add_push_null lambda codegen extend_output codegen create_load_const format_string codegen create_load_attr format call_function_ex=True codegen variables TupleVariable sym_args kwargs = variables ConstantVariable create k v k v sym_kwargs items codegen variables ConstDictVariable kwargs codegen extend_output create_call_function_ex True False DebuggingVariable VariableTracker Represents call debugging function like print something registered config reorderable_logging_functions __init__ value kwargs - None super __init__ kwargs value = value staticmethod is_reorderable_logging_function obj callable obj isinstance obj types FunctionType types BuiltinFunctionType obj torch _dynamo config reorderable_logging_functions call_function tx InstructionTranslator args kwargs tx export For export cases we can just make debugging functions no-ops can_reorder_logs value args kwargs unimplemented f Reordering debugging function value f inputs args kwargs yet implemented tx debug_locals append list args reconstruct codegen PyCodegen source reconstruct codegen staticmethod can_reorder_logs fn args kwargs - True Run some additional checks what sort function calls can we actually reorder allowed_input_types = variables TensorVariable variables ConstantVariable StringFormatVariable flat_args = pytree tree_leaves args kwargs arg flat_args isinstance arg allowed_input_types False True LoggingLoggerVariable VariableTracker Represents call any logging Logger methods __init__ value kwargs - None super __init__ kwargs value = value call_method tx InstructionTranslator name args list VariableTracker kwargs dict str VariableTracker - VariableTracker tx export For export cases we can just make debugging functions no-ops method = getattr value name None function = getattr method __func__ None method function intersection torch _dynamo config ignore_logger_methods variables ConstantVariable create None unimplemented Logger supported non-export cases To avoid graph breaks caused logger compile-mode recommended disable logging adding logging methods config ignore_logger_methods ConstantLikeVariable VariableTracker value compile-time constant literal _error_prefix = ConstantLikeVariable try numpy dtype np_dtype floating np_floating generic np_generic except ImportError np_floating = type invalid_type np_dtype = type invalid_type __init__ value kwargs - None super __init__ kwargs value = value as_python_constant value call_method tx InstructionTranslator name args list VariableTracker kwargs dict str VariableTracker - VariableTracker try we only support constant propagation methods cargs = x as_python_constant x args ckwargs = k v as_python_constant k v kwargs items except NotImplementedError unimplemented f _error_prefix name args kwargs result = getattr value name cargs ckwargs variables ConstantVariable is_literal result variables ConstantVariable create result isinstance result re Match ConstantRegexMatchVariable result unimplemented f _error_prefix name - result var_getattr tx InstructionTranslator name str - VariableTracker result = getattr value name isinstance result np_floating result = float result isinstance result np_dtype NumpyDTypeVariable result isinstance result type issubclass result np_generic things like x dtype type NumpyVariable result variables ConstantVariable is_literal result variables ConstantVariable create result GetAttrVariable name RegexPatternVariable ConstantLikeVariable _error_prefix = re Pattern ConstantRegexMatchVariable ConstantLikeVariable _error_prefix = re Match TorchVersionVariable ConstantLikeVariable _error_prefix = torch __version__ __init__ kwargs - None kwargs setdefault value torch __version__ assert kwargs value torch __version__ super __init__ kwargs NumpyTypeInfoVariable ConstantLikeVariable _error_prefix = np iinfo np finfo NumpyDTypeVariable ConstantLikeVariable _error_prefix = np dtype as_proxy Similar how numpy dtype descriptors e g np float handled NumpyVariable np dtype objects serialized strings torch _numpy wrappers will normalize torch dtype This also handles unsupported things nicely i e structured arrays object arrays value type __name__ np_constant_collections_map = tnp finfo NumpyTypeInfoVariable tnp iinfo NumpyTypeInfoVariable tnp dtype NumpyDTypeVariable RandomClassVariable VariableTracker random Random __init__ kwargs - None super __init__ kwargs call_function tx InstructionTranslator args kwargs len args unimplemented random Random arg kwargs unimplemented random Random kwargs seed = variables ConstantVariable create None len args == args RandomVariable seed=seed mutation_type=variables base ValueMutationNew RandomVariable VariableTracker random Random Implemented wrapping VariableTracker around random Random object The supported methods random Random object cannot overridden Assumes random objects behave same given set seed state _nonvar_fields = random VariableTracker _nonvar_fields _supported_fn_names = random randint randrange uniform __init__ rand Optional random Random = None seed Optional VariableTracker = None kwargs - None super __init__ kwargs rand None assert is_supported_random_obj rand random = random Random random setstate rand getstate seed = seed as_python_constant seed None None random = random Random seed python_type random Random as_python_constant random staticmethod is_supported_random_obj val type val random Random False name itertools chain RandomVariable _supported_fn_names seed getstate setstate hasattr val name False meth = getattr val name inspect isbuiltin meth e g random Random random meth = getattr random Random name __get__ val False getattr meth __func__ None getattr random Random name False True staticmethod check_state state assert type state tuple assert type state int assert type state tuple assert all type x int x state assert state None type state float staticmethod wrap_state state RandomVariable check_state state variables TupleVariable variables ConstantVariable create state variables TupleVariable variables ConstantVariable create x x state variables ConstantVariable create state staticmethod unwrap_state state state_obj = state as_python_constant RandomVariable check_state state_obj state_obj call_method tx InstructionTranslator name args list VariableTracker kwargs dict str VariableTracker - VariableTracker name == seed tx output side_effects mutation random seed x as_python_constant x args key val as_python_constant key val kwargs items variables ConstantVariable create None name == getstate wrap_state random getstate name == setstate tx output side_effects mutation random setstate unwrap_state args variables ConstantVariable create None name _supported_fn_names tx output side_effects mutation state = random getstate call_random_meth args kwargs r = random Random r setstate state getattr r name args kwargs random state actually updated call_random_meth so update here calling method getattr random name x as_python_constant x args k v as_python_constant k v kwargs items call_random_fn tx call_random_meth args kwargs super call_method tx name args kwargs reconstruct codegen PyCodegen codegen add_push_null lambda codegen extend_output codegen create_load_python_module random codegen create_load_attr Random codegen call_function False NOTE using add_push_null may result NULL being duplicated so defer push_null call_function codegen dup_top codegen load_attr setstate codegen wrap_state random getstate codegen call_function True codegen pop_top WeakRefVariable VariableTracker staticmethod build tx weakref_value options source = options get source callback = weakref_value __callback__ callback_source = source AttrSource source __callback__ callback_vt = VariableTracker build tx callback callback_source referent = weakref_value source = source WeakRefCallSource source referent_vt = VariableTracker build tx referent source options source = source WeakRefVariable referent_vt callback_vt options __init__ referent_vt callback_vt options super __init__ options referent_vt = referent_vt callback_vt = callback_vt call_function tx InstructionTranslator args list VariableTracker kwargs dict str VariableTracker - VariableTracker referent_vt reconstruct codegen PyCodegen codegen add_push_null lambda codegen load_import_from weakref ref codegen referent_vt codegen callback_vt codegen extend_output create_call_function False