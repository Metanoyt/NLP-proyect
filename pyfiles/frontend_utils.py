mypy ignore-errors warnings collections abc KeysView contextlib contextmanager typing Any Optional torch torch utils _pytree pytree torch _guards detect_fake_mode torch _subclasses FakeTensor FakeTensorMode torch fx experimental proxy_tensor _pytree_subclasses_that_lose_info torch fx experimental symbolic_shapes ShapeEnv torch utils _python_dispatch is_traceable_wrapper_subclass config descriptors BufferAOTInput DifferentiableAOTInput ParamAOTInput schemas AOTConfig FakifiedFlatArgs static_inputs_log = torch _logging getArtifactLogger __name__ cudagraph_static_inputs process_inputs flat_args list Any aot_config AOTConfig fake_mode FakeTensorMode shape_env Optional ShapeEnv ignore_shape_env bool = False - FakifiedFlatArgs fake_mode convert idx x shape_env None ignore_shape_env torch _dynamo source ConstantSource isinstance x int We always specialize scalar values export aot_config is_export x source = ConstantSource f sym_ idx shape_env create_symintnode shape_env create_symbol x source positive=x = hint=x source=source isinstance x torch ScriptObject torch _library fake_class_registry maybe_to_fake_obj fake_mode x isinstance x torch Tensor x isinstance x FakeTensor assert x fake_mode fake_mode x is_traceable_wrapper_subclass x attrs _ = x __tensor_flatten__ all isinstance getattr x attr FakeTensor attr attrs assert all getattr x attr fake_mode fake_mode attr attrs x see note Tensor Fakification Symbol Caching symbolic_context = None source = None trace = True tracing_context = torch _guards TracingContext try_get x tracing_context tensor_to_context symbolic_context = tracing_context tensor_to_context x source = symbolic_context tensor_source We already fakeified tensor Dynamo don t dump trace again trace = False idx aot_config num_params_buffers config static_weight_shapes symbolic_context TODO Ensure codepath never exercised Dynamo fake_mode from_tensor x static_shapes=True result = fake_mode from_tensor x static_shapes=ignore_shape_env symbolic_context=symbolic_context source=source trace=trace result FakifiedFlatArgs convert idx x idx x enumerate flat_args construct_fake_mode flat_args list Any aot_config AOTConfig - tuple FakeTensorMode Optional ShapeEnv fake_mode = detect_fake_mode flat_args fake_mode None shape_env = ShapeEnv aot_config dynamic_shapes None fake_mode = FakeTensorMode shape_env=shape_env shape_env = fake_mode shape_env fake_mode shape_env _try_get_metadata_from_dynamo mod torch nn Module param_keys KeysView str full_args_num int full_args_descs list DifferentiableAOTInput - tuple Optional list torch _guards Source list int Metadata forwarded Dynamo AOTDispatch via special fields GraphModule We first verify ` mod ` does come Dynamo then we handle cases where metadata might missing Returns aot_autograd_arg_pos_to_source used dedup params their guards static_input_indices used identify static inputs cudagraphs Note Assumption Dynamo Metadata This function assumes graph module dynamo provides ` dynamo_compiled_id ` _param_name_to_source every placeholder node has ` _dynamo_source ` attributes When gm modified e g DDPOptimizer via split_module metadata needs propagated order recognized dynamo graph isinstance mod torch fx GraphModule dynamo_compile_id mod meta graph captured dynamo None hasattr mod _param_name_to_source export static_input_indices = i i node enumerate full_args_descs isinstance node ParamAOTInput BufferAOTInput None static_input_indices We now know came dynamo we care about guards so setting up aot_autograd_arg_pos_to_source downstream dedup guards can now done safely Dynamo logic protects sizing below Additionally we mark static indices cudagraphs param_name_to_source = mod _param_name_to_source seen_sources = set aot_autograd_arg_pos_to_source = static_input_indices = Collect new inputs lifted aotdispatch i name enumerate param_keys assert name param_name_to_source f name found source = param_name_to_source name assert source seen_sources source seen_sources add source aot_autograd_arg_pos_to_source append source static_input_indices append i Collect dynamo graph inputs TODO mlazos Revisit still needed With Dynamo install ID matched tensors back into Fx graph might necessary pos node enumerate mod graph find_nodes op= placeholder assert hasattr node _dynamo_source source = node _dynamo_source ` source ` ` specifies source user code ddp optimizer may have intermediate values becoming submodule placeholders which does have source assert source None source seen_sources source seen_sources add source aot_autograd_arg_pos_to_source append source source_name = source name source str source input i dynamo now input i + len extra_params AOT where extra_params params buffers dynamo baked into OutputGraph actual_pos = pos + len param_keys tensor_dict node meta node meta tensor_dict get _dynamo_static_input_type None static_inputs_log debug Adding static input pos s source s actual_pos source_name static_input_indices append actual_pos static_inputs_log debug Non-static input pos s source s actual_pos source_name assert full_args_num == len aot_autograd_arg_pos_to_source aot_autograd_arg_pos_to_source static_input_indices contextmanager _detect_attribute_assignment mod torch nn Module Do allow assignment tensor attributes during export unless attribute registered buffer NN_MODULE_STD_ATTRS = _backward_hooks _backward_pre_hooks _buffers _forward_hooks _forward_hooks_always_called _forward_hooks_with_kwargs _forward_pre_hooks _forward_pre_hooks_with_kwargs _is_full_backward_hook _load_state_dict_post_hooks _load_state_dict_pre_hooks _modules _non_persistent_buffers_set _parameters _state_dict_hooks _state_dict_pre_hooks training NN_MODULE_LAZY_STD_ATTRS = _initialize_hook _load_hook STD_ATTRS = NN_MODULE_STD_ATTRS NN_MODULE_LAZY_STD_ATTRS _get_attributes mod any attributes module standard attributes k v k v mod __dict__ items k STD_ATTRS _get_all_module_attributes mod attributes all modules submodules result = name submodule mod named_modules result name = _get_attributes submodule result _restore_all_module_attributes mod snapshot restore attributes all modules submodules name submodule mod named_modules name snapshot submodule __dict__ update snapshot name save state attributes before enter snapshot = pytree tree_map lambda x x _get_all_module_attributes mod is_leaf=lambda x type x _pytree_subclasses_that_lose_info try yield finally after exit compare state attributes snapshot detect which tensor attributes assigned _collect_assigned_tensor_attributes snapshot new_attrs assigned_tensor_attributes = _compare_values path old_val new_val Recursively compare values handling containers Same object no change old_val new_val old_val None new_val None isinstance new_val torch Tensor assigned_tensor_attributes append path Check s tensor reassigned isinstance new_val torch Tensor assigned_tensor_attributes append path Handle dict containers isinstance old_val dict isinstance new_val dict all_keys = set old_val keys &#124; set new_val keys key all_keys old_item = old_val get key new_item = new_val get key _compare_values f path key r old_item new_item Handle list tuple containers isinstance old_val list tuple isinstance new_val list tuple Different lengths = mutation happened max_len = max len old_val len new_val i range max_len old_item = old_val i i len old_val None new_item = new_val i i len new_val None _compare_values f path i old_item new_item For other types just check they re different objects we don t care about non-tensor mutations module_name snapshot keys &#124; new_attrs keys old_module_attrs = snapshot get module_name new_module_attrs = new_attrs get module_name attr_name old_module_attrs keys &#124; new_module_attrs keys module_prefix = f module_name module_name full_path = f module_prefix attr_name old_val = old_module_attrs get attr_name new_val = new_module_attrs get attr_name _compare_values full_path old_val new_val assigned_tensor_attributes new_attrs = _get_all_module_attributes mod assigned_tensor_attributes = _collect_assigned_tensor_attributes snapshot new_attrs restore state all attributes including e g primitive types _restore_all_module_attributes mod snapshot assigned_tensor_attributes len assigned_tensor_attributes noun verb = attributes noun verb = attribute warnings warn f The tensor noun join assigned_tensor_attributes verb assigned during export Such attributes must registered buffers using ` register_buffer ` API https pytorch org docs stable generated torch nn Module html#torch nn Module register_buffer stacklevel=