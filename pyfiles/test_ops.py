Owner s module onnx Test op correctness comparing PyTorch results Usage pytest test_ops py To run tests specific operator e g torch ceil pytest test_ops py -k ceil To run tests nn operator e g nn functional scaled_dot_product_attention pytest test_ops py -k nn_functional_scaled_dot_product_attention ## Environment variables Set environment variable ` CATCH_ORT_SEGFAULT= ` catch segmentation faults onnxruntime running inference sessions separate process Set ` CREATE_REPRODUCTION_REPORT= ` create markdown files reproduction errors __future__ annotations os typing Optional TYPE_CHECKING error_reproduction numpy np onnx onnxruntime ort onnxscript ops_test_common ops_test_data parameterized torch torch testing _internal common_device_type common_utils torch utils _pytree pytree TYPE_CHECKING unittest collections abc Callable Sequence torch testing _internal opinfo core opinfo_core All dtypes will tested generated symbolic functions complex will flattened float TESTED_DTYPES = torch float torch float Uncomment below item when we really need testing torch bfloat torch float torch bool torch int torch int torch int torch int torch uint NOTE torch complex experimental torch COMPLEX_TYPES = torch complex dtypes_except dtypes torch dtype - Sequence torch dtype Returns all dtypes except ones specified tuple dtype dtype TESTED_DTYPES dtype dtypes _should_skip_xfail_test_sample op_name str sample dtype torch dtype device_type str - tuple Optional str Optional str Returns reason test sample should skipped op_name ops_test_data OP_WITH_SKIPPED_XFAIL_SUBTESTS None None decorator_meta ops_test_data SKIP_XFAIL_SUBTESTS Linear search ops_test_data SKIP_XFAIL_SUBTESTS That s fine because list small decorator_meta op_name == op_name assert decorator_meta matcher None Matcher must defined decorator_meta enabled_if Do skip test decorator meta enabled continue decorator_meta dtypes None dtype decorator_meta dtypes Not applicable dtype continue decorator_meta device_type None decorator_meta device_type = device_type Not applicable device_type continue decorator_meta matcher sample decorator_meta test_behavior decorator_meta reason None None TestFunctionValidity common_utils TestCase parameterized parameterized expand info op name info info ops_test_data TESTED_TORCHLIB_OPS isinstance info op onnxscript OnnxFunction skip_on_empty=True test_script_function_passes_checker _ torchlib_op_info ops_test_data TorchLibOpInfo function_proto = torchlib_op_info op to_function_proto onnx checker check_function function_proto type ignore attr-defined run_test_output_match test_suite unittest TestCase device str dtype torch dtype op opinfo_core OpInfo function_executor Callable tested_op_mapping dict str ops_test_data TorchLibOpInfo Base test method testing each opset used instantiate_device_type_tests Args test_suite The test instance device The PyTorch device instantiate_device_type_tests provides dtype The PyTorch dtype instantiate_device_type_tests provides op The OpInfo instance instantiate_device_type_tests provides function_executor The function executor This function takes function its arguments returns output function tested_op_mapping The mapping op name tested op samples = op sample_inputs device dtype requires_grad=False torchlib_op_info = tested_op_mapping op name Obtain input_wrangler manipulates OpInfo inputs match aten operator signature An example nn functional upsample_nearest d which has different signature than aten operator upsample_nearest d onnx_function = torchlib_op_info op input_wrangler = torchlib_op_info input_wrangler ops_test_common dtype_op_schema_compatible dtype onnx_function op_schema dtype COMPLEX_TYPES test_suite skipTest f dtype dtype supported op op name f Type constraints onnx_function op_schema type_constraints Obtain tolerance op rtol atol = torchlib_op_info get_tolerance dtype i cpu_sample enumerate samples inputs = cpu_sample input cpu_sample args Provide repr subtest because tensors serializable parallel test runs test_suite subTest sample_num=i inputs=repr f Tensor inp shape dtype= inp dtype isinstance inp torch Tensor inp inp inputs kwargs=repr cpu_sample kwargs try device_type = cpu_sample args device type except AttributeError IndexError device_type = cpu test_behavior reason = _should_skip_xfail_test_sample op name cpu_sample dtype device_type ops_test_common normal_xfail_skip_test_behaviors test_behavior reason input_onnx = ops_test_common convert_tensor_to_numpy x x inputs kwargs_onnx = ops_test_common convert_kwargs_for_onnx cpu_sample kwargs input_wrangler input_onnx kwargs_onnx = input_wrangler input_onnx kwargs_onnx torch_output = op inputs cpu_sample kwargs isinstance torch_output torch Tensor torch is_complex torch_output torch_output = torch view_as_real torch_output resolve_conj reference_torch_outputs _ = pytree tree_flatten torch_output op name startswith split op name startswith chunk op name startswith unbind op name atleast_ d_Sequence atleast_ d_Sequence atleast_ d_Sequence Hack handling split chunk unbind which relies SplitToSequence op Split returns Sequence should treats single value So we wrap into tuple TODO justinchuby Find more general solution reference_torch_outputs = reference_torch_outputs test_name = test_suite id function_output model_proto = function_executor test_name reference_torch_outputs opset_version=torchlib_op_info opset_introduced onnx_function input_onnx kwargs_onnx Finally we re-flatten everything TODO add pytree structure comparison flattened_torch_outputs _ = pytree tree_flatten torch_output flattened_function_outputs _ = pytree tree_flatten function_output assert flattened_torch_outputs assert len flattened_torch_outputs == len flattened_function_outputs j torch_output function_output enumerate zip flattened_torch_outputs flattened_function_outputs actual = torch tensor function_output expected = torch_output isinstance torch_output torch Tensor torch tensor torch_output op name ops_test_data NONDETERMINISTIC_OPS j ops_test_data COMPARE_SHAPE_ONLY_OPS op name Check shape dtype only ops known nondeterministic test_suite assertEqual actual shape expected shape test_suite assertEqual actual dtype expected dtype continue Use torch testing opposed np testing ensure dtypes shapes match try torch testing assert_close actual expected rtol=rtol atol=atol equal_nan=True check_device=False except AssertionError e os environ get CREATE_REPRODUCTION_REPORT == test_behavior None error_reproduction create_mismatch_report test_name i model_proto inputs cpu_sample kwargs actual expected e __file__ len flattened_torch_outputs raise AssertionError f Output j mismatch e raise TestOutputConsistencyFullGraph common_utils TestCase Test output consistency between exported ONNX op run graph PyTorch eager mode This parameterized test suite setUp - None torch manual_seed np random seed ort set_seed ops_test_common add_decorate_info ops_test_data OPS_DB TestOutputConsistencyFullGraph test_output_match_opinfo_ skip_or_xfails=ops_test_data EXPECTED_SKIPS_OR_FAILS common_device_type ops type ignore misc info info ops_test_data OPS_DB info name ops_test_data TESTED_OPS allowed_dtypes=TESTED_DTYPES test_output_match_opinfo_ device str dtype torch dtype op opinfo_core OpInfo Base test method testing each op running full ONNX graph run_test_output_match device dtype op ops_test_common graph_executor ops_test_data TORCHLIB_OPINFO_MAPPING ops_test_common add_decorate_info ops_test_data OPS_DB TestOutputConsistencyFullGraph test_complex_output_match_opinfo_ skip_or_xfails=ops_test_data EXPECTED_SKIPS_OR_FAILS common_device_type ops type ignore misc info info ops_test_data OPS_DB info name ops_test_data COMPLEX_FUNCTION_MAPPING allowed_dtypes=COMPLEX_TYPES test_complex_output_match_opinfo_ device str dtype torch dtype op opinfo_core OpInfo Base test method testing each op running full ONNX graph run_test_output_match device dtype op ops_test_common graph_executor ops_test_data COMPLEX_FUNCTION_MAPPING common_device_type instantiate_device_type_tests TestOutputConsistencyFullGraph globals only_for= cpu __name__ == __main__ common_utils run_tests