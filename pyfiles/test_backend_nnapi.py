Owner s oncall jit os sys unittest pathlib Path torch torch _C torch testing _internal common_utils IS_FBCODE raise_on_run_directly skipIfTorchDynamo hacky way skip these tests fbcode during test execution fbcode test_nnapi available during test discovery during test execution So we can t try-catch here otherwise ll think sees tests then fails when tries actually run them IS_FBCODE test_nnapi TestNNAPI HAS_TEST_NNAPI = True torch testing _internal common_utils TestCase TestNNAPI HAS_TEST_NNAPI = False Make helper files test importable pytorch_test_dir = os path dirname os path dirname os path realpath __file__ sys path append pytorch_test_dir Unit Tests Nnapi backend delegate Inherits most tests TestNNAPI which loads Android NNAPI models without delegate API First skip needed IS_WINDOWS IS_MACOS skip tests torch_root = Path __file__ resolve parents lib_path = torch_root build lib libnnapi_backend so skipIfTorchDynamo weird py failures unittest skipIf os path exists lib_path Skipping test libnnapi_backend so found unittest skipIf IS_FBCODE test_nnapi py found TestNnapiBackend TestNNAPI setUp super setUp Save default dtype module = torch nn PReLU default_dtype = module weight dtype Change dtype float since different unit test changed dtype float which supported Android NNAPI delegate Float should typically default other files torch set_default_dtype torch float Load nnapi delegate library torch ops load_library str lib_path Override call_lowering_to_nnapi traced_module args compile_spec = forward inputs args torch _C _jit_to_backend nnapi traced_module compile_spec test_tensor_input Lower simple module args = torch tensor - - unsqueeze - unsqueeze - module = torch nn PReLU traced = torch jit trace module args Argument input single Tensor call_lowering_to_nnapi traced args Argument input Tensor list call_lowering_to_nnapi traced args Test exceptions incorrect compile specs test_compile_spec_santiy args = torch tensor - - unsqueeze - unsqueeze - module = torch nn PReLU traced = torch jit trace module args errorMsgTail = r method_compile_spec should contain Tensor Tensor List which bundles input parameters shape dtype quantization dimorder For input shapes use run load time flexible input method_compile_spec must use following format forward inputs Tensor OR forward inputs c List Tensor No forward key compile_spec = backward inputs args assertRaisesRegex RuntimeError method_compile_spec does contain forward key + errorMsgTail torch _C _jit_to_backend nnapi traced compile_spec No dictionary under forward key compile_spec = forward assertRaisesRegex RuntimeError method_compile_spec does contain dictionary inputs key under it\ s forward key + errorMsgTail torch _C _jit_to_backend nnapi traced compile_spec No inputs key dictionary under forward key compile_spec = forward inputs args assertRaisesRegex RuntimeError method_compile_spec does contain dictionary inputs key under it\ s forward key + errorMsgTail torch _C _jit_to_backend nnapi traced compile_spec No Tensor TensorList under inputs key compile_spec = forward inputs assertRaisesRegex RuntimeError method_compile_spec does contain either Tensor TensorList under it\ s inputs key + errorMsgTail torch _C _jit_to_backend nnapi traced compile_spec compile_spec = forward inputs assertRaisesRegex RuntimeError method_compile_spec does contain either Tensor TensorList under it\ s inputs key + errorMsgTail torch _C _jit_to_backend nnapi traced compile_spec tearDown Change dtype back default Otherwise other unit tests will complain torch set_default_dtype default_dtype __name__ == __main__ raise_on_run_directly test test_jit py