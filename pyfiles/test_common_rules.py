Copyright c Meta Platforms Inc affiliates Owner s oncall distributed torch torch distributed tensor DeviceMesh torch distributed tensor _dtensor_spec DTensorSpec TensorMeta torch distributed tensor _op_schema OpSchema torch distributed tensor _ops _common_rules einop_rule pointwise_rule torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor DTensorContinuousTestBase aten = torch ops aten CommonRulesTest DTensorContinuousTestBase hard code world size we need test least d mesh world_size = _gen_tensor_meta shape empty_tensor = torch empty shape TensorMeta empty_tensor shape empty_tensor stride empty_tensor dtype test_einop_basic_propagation plain einsum mm mesh = DeviceMesh device_type torch arange world_size mm_call = aten mm default propagate col-wise sharding mat mat = - - - mat _tensor_meta = _gen_tensor_meta torch Size mat _tensor_meta = _gen_tensor_meta torch Size mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta output_sharding = einop_rule mk kn- mn OpSchema mm_call mat _spec mat _spec output_spec = output_sharding output_spec assertIsNotNone output_spec assertEqual output_spec dim_map - propagate row-wise sharding mat mat = - - - mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta output_sharding = einop_rule mk kn- mn OpSchema mm_call mat _spec mat _spec output_spec = output_sharding output_spec assertIsNotNone output_spec assertEqual output_spec dim_map - generate partial mat mat = - - mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta output_sharding = einop_rule mk kn- mn OpSchema mm_call mat _spec mat _spec output_spec = output_sharding output_spec assertIsNotNone output_spec assertTrue output_spec placements is_partial test_einop_pointwise_propagation mesh = DeviceMesh device_type torch arange world_size add_call = aten add Tensor addition mat _tensor_meta = _gen_tensor_meta torch Size mat = - mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta output_sharding = einop_rule ij ij- ij OpSchema add_call mat _spec mat _spec output_spec = output_sharding output_spec assertIsNotNone output_spec assertEqual output_spec dim_map - broadcast addition mat _tensor_meta = _gen_tensor_meta torch Size mat = - - mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta mat _tensor_meta = _gen_tensor_meta torch Size mat _spec = DTensorSpec from_dim_map mesh - tensor_meta=mat _tensor_meta output_sharding = einop_rule ijk k- ijk OpSchema add_call mat _spec mat _spec output_spec = output_sharding output_spec assertIsNotNone output_spec assertEqual output_spec dim_map - - broadcast common shape mat _tensor_meta = _gen_tensor_meta torch Size mat _tensor_meta = _gen_tensor_meta torch Size mat _spec = DTensorSpec from_dim_map mesh - - tensor_meta=mat _tensor_meta mat _spec = DTensorSpec from_dim_map mesh - - tensor_meta=mat _tensor_meta output_sharding = einop_rule ijk k- ijk OpSchema add_call mat _spec mat _spec output_spec = output_sharding output_spec assertIsNotNone output_spec assertEqual output_spec dim_map - - test_einop_merge_sharding d mesh einop merge sharding mesh_shape = torch arange world_size reshape world_size world_size mesh = DeviceMesh device_type mesh_shape mm_call = aten mm default mat mat = - - mat _tensor_meta = _gen_tensor_meta torch Size mat _tensor_meta = _gen_tensor_meta torch Size mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta output_sharding = einop_rule mk kn- mn OpSchema mm_call mat _spec mat _spec output_spec = output_sharding output_spec assertIsNotNone output_spec assertEqual output_spec dim_map test_einop_linearity mesh_shape = torch arange world_size reshape world_size world_size mesh = DeviceMesh device_type mesh_shape mm_call = aten mm default mat mat = - - - mat _tensor_meta = _gen_tensor_meta torch Size mat _tensor_meta = _gen_tensor_meta torch Size mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta turn linearity partial sum eligible propagate we suggestion reshard inputs no partial sum i e all_reduce one input output_sharding = einop_rule mk kn- mn OpSchema mm_call mat _spec mat _spec assertIsNone output_sharding output_spec suggestions = output_sharding redistribute_schema assertIsNotNone suggestions suggested_spec = suggestions args_schema assertFalse suggested_spec placements is_partial einop prop linearity mm should give back suggestion converting placements partial output_sharding = einop_rule mk kn- mn OpSchema mm_call mat _spec mat _spec linearity=True assertIsNone output_sharding output_spec suggestions = output_sharding redistribute_schema assertIsNotNone suggestions mat _spec = suggestions args_schema mat mesh dim should become partial now assertTrue mat _spec placements is_partial einop prop linearity point-wise should give back suggestion converting placements partial add_call = aten add Tensor mat mat = - - mat _tensor_meta = _gen_tensor_meta torch Size mat _tensor_meta = _gen_tensor_meta torch Size mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta output_sharding = einop_rule ij ij- ij OpSchema add_call mat _spec mat _spec linearity=True assertIsNone output_sharding output_spec suggestions = output_sharding redistribute_schema assertIsNotNone suggestions mat _spec = suggestions args_schema mat mesh dim should become partial now assertTrue mat _spec placements is_partial test_einop_multi_sharding_on_mesh_dim einop prop multi sharding same mesh dim mesh_shape = torch arange world_size mesh = DeviceMesh device_type mesh_shape mm_call = aten mm default mat mat = - - mat _tensor_meta = _gen_tensor_meta torch Size mat _tensor_meta = _gen_tensor_meta torch Size mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta output_sharding = einop_rule mk kn- mn OpSchema mm_call mat _spec mat _spec output_spec = output_sharding output_spec assertIsNone output_spec assertIsNotNone output_sharding redistribute_schema ensure suggestion reshard second arg all_gather its tensor dim sharding schema_suggestion = output_sharding redistribute_schema assertEqual schema_suggestion args_schema dim_map - assertEqual schema_suggestion args_schema dim_map - - test_einop_errors mesh_shape = torch arange world_size reshape world_size world_size mesh = DeviceMesh device_type mesh_shape add_call = aten add Tensor mat mat = - - mat _tensor_meta = _gen_tensor_meta torch Size mat _tensor_meta = _gen_tensor_meta torch Size mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta assertRaisesRegex RuntimeError sharded two different ways einop_rule ij ij- ij OpSchema add_call mat _spec mat _spec test_pointwise_rules_broadcasting mesh = DeviceMesh device_type torch arange world_size where_call = aten where inp inp inp = - - inp _tensor_meta = _gen_tensor_meta torch Size inp _tensor_meta = _gen_tensor_meta torch Size inp _tensor_meta = _gen_tensor_meta torch Size condition = DTensorSpec from_dim_map mesh inp tensor_meta=inp _tensor_meta self_tensor = DTensorSpec from_dim_map mesh inp tensor_meta=inp _tensor_meta other_tensor = DTensorSpec from_dim_map mesh inp tensor_meta=inp _tensor_meta propagate point-wise sharding broadcasting output_sharding = pointwise_rule OpSchema where_call condition self_tensor other_tensor output_spec = output_sharding output_spec assertIsNotNone output_spec assertEqual output_spec dim_map - test_pointwise_rules_suggestion mesh = DeviceMesh device_type torch arange world_size lerp_call = aten lerp Scalar propagate point-wise sharding inp inp = - - - mat _tensor_meta = _gen_tensor_meta torch Size mat _tensor_meta = _gen_tensor_meta torch Size mat _spec = DTensorSpec from_dim_map mesh inp tensor_meta=mat _tensor_meta mat _spec = DTensorSpec from_dim_map mesh inp tensor_meta=mat _tensor_meta adding positional argument - arg schema output_sharding = pointwise_rule OpSchema lerp_call mat _spec mat _spec - assertIsNone output_sharding output_spec assertIsNotNone output_sharding redistribute_schema ensure suggestion pointwise rules still have positional args DTensorSpec schema_suggestion = output_sharding redistribute_schema assertEqual len schema_suggestion args_schema assertEqual schema_suggestion args_schema - test_pointwise_multi_sharding_on_mesh_dim d mesh pointwise sharding mesh_shape = torch arange world_size reshape world_size world_size mesh = DeviceMesh device_type mesh_shape add_call = aten add Tensor basic case test implicit broadcasting shape alignment mat mat = - mat _tensor_meta = _gen_tensor_meta torch Size mat _tensor_meta = _gen_tensor_meta torch Size mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta output_sharding = pointwise_rule OpSchema add_call mat _spec mat _spec output_spec = output_sharding output_spec assertIsNotNone output_spec assertEqual output_spec dim_map - more advanced case needs reshard one input align sharding mat mat = - - - mat _tensor_meta = _gen_tensor_meta torch Size mat _tensor_meta = _gen_tensor_meta torch Size mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta output_sharding = pointwise_rule OpSchema add_call mat _spec mat _spec output_spec = output_sharding output_spec assertIsNone output_spec assertIsNotNone output_sharding redistribute_schema ensure suggestion reshard first arg all_gather first tensor dim sharding schema_suggestion = output_sharding redistribute_schema assertEqual schema_suggestion args_schema dim_map - - - assertEqual schema_suggestion args_schema dim_map mat test_pointwise_enforce_sharding_multi_sharding_on_mesh_dim d mesh pointwise sharding mesh_shape = torch arange world_size reshape world_size world_size mesh = DeviceMesh device_type mesh_shape add_call = aten add_ Tensor more advanced case needs reshard one input align sharding mat mat = - - - mat _tensor_meta = _gen_tensor_meta torch Size mat _tensor_meta = _gen_tensor_meta torch Size mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta mat _spec = DTensorSpec from_dim_map mesh mat tensor_meta=mat _tensor_meta output_sharding = pointwise_rule OpSchema add_call mat _spec mat _spec output_spec = output_sharding output_spec assertIsNone output_spec assertIsNotNone output_sharding redistribute_schema ensure suggestion reshard second arg we should enforce sharding first arg schema_suggestion = output_sharding redistribute_schema assertEqual schema_suggestion args_schema dim_map mat assertEqual schema_suggestion args_schema dim_map mat __name__ == __main__ run_tests