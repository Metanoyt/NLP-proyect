mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates contextlib functools itertools sys types collections abc Callable Iterator Sequence dataclasses dataclass functools partial wraps typing Any cast Optional TypeVar Union torch torch distributed dist torch nn nn torch nn functional F torch distributed _local_tensor LocalIntNode LocalTensor LocalTensorMode maybe_disable_local_tensor_mode maybe_run_for_local_tensor torch distributed tensor DeviceMesh distribute_tensor DTensor init_device_mesh Placement Replicate Shard torch distributed tensor parallel ColwiseParallel parallelize_module PrepareModuleInput RowwiseParallel SequenceParallel torch testing _internal common_distributed MultiProcContinuousTest MultiProcessTestCase MultiThreadedTestCase run_subtests skip_if_lt_x_gpu TEST_SKIPS torch testing _internal common_utils TEST_CUDA TEST_HPU TEST_PRIVATEUSE TEST_XPU torch utils _pytree tree_flatten tree_unflatten TreeSpec DEVICE_COUNT int TEST_CUDA TEST_XPU TEST_HPU TEST_PRIVATEUSE DEVICE_TYPE = torch accelerator current_accelerator type DEVICE_COUNT = torch accelerator device_count PG_BACKEND = dist Backend default_device_backend_map DEVICE_TYPE DEVICE_TYPE = cpu PG_BACKEND = gloo NUM_DEVICES = We use proxy multiple GPUs exist TEST_CUDA TEST_XPU TEST_HPU TEST_PRIVATEUSE DEVICE_COUNT when we actually have multiple GPUs relax requirement smaller counts NUM_DEVICES = min NUM_DEVICES DEVICE_COUNT T = TypeVar T simple RMSNorm layer testing RMSNormPython torch nn Module __init__ dim int eps float = e- super __init__ eps = eps weight = torch nn Parameter torch ones dim _norm x x torch rsqrt x pow mean - keepdim=True + eps forward x output = _norm x output weight MLPModule nn Module __init__ device bias bool = True super __init__ torch manual_seed net = nn Linear bias=bias device=device relu = nn ReLU net = nn Linear bias=bias device=device forward x net relu net x reset_parameters net reset_parameters net reset_parameters MLPStacked nn Module __init__ device n_layers int = super __init__ layers = nn ModuleList MLPModule device i range n_layers forward x layer layers x = layer x x dataclass ModelArgs n_layers int = vocab_size int = max_seq_len int = dim int = n_heads int = dropout_p float = use_attn_mask bool = True weight_tying bool = True checkpoint_activations bool = False Attention nn Module __init__ args ModelArgs super __init__ assert args dim args n_heads == head_dim = args dim args n_heads n_heads = args n_heads dropout_p = args dropout_p resid_dropout = nn Dropout args dropout_p use_attn_mask = args use_attn_mask wq = nn Linear args dim args dim bias=False wk = nn Linear args dim args dim bias=False wv = nn Linear args dim args dim bias=False wo = nn Linear args dim args dim bias=False forward x bsz seq_len _ = x size queries keys values = wq x wk x wv x queries = queries view bsz seq_len n_heads head_dim keys = keys view bsz seq_len n_heads head_dim values = values view bsz seq_len n_heads head_dim queries = queries transpose bsz n_heads seq_len head_dim keys = keys transpose bsz n_heads seq_len head_dim values = values transpose bsz n_heads seq_len head_dim output = F scaled_dot_product_attention queries keys values None dropout_p training use_attn_mask output = output transpose contiguous view bsz seq_len - resid_dropout wo output FeedForward nn Module __init__ dim hidden_dim dropout_p super __init__ w = nn Linear dim hidden_dim gelu = nn GELU w = nn Linear hidden_dim dim resid_dropout = nn Dropout dropout_p forward x resid_dropout w gelu w x TransformerBlock nn Module __init__ args ModelArgs super __init__ attention_norm = nn LayerNorm args dim attention = Attention args ffn_norm = nn LayerNorm args dim feed_forward = FeedForward args dim hidden_dim= args dim dropout_p=args dropout_p forward x h = x + attention attention_norm x out = h + feed_forward ffn_norm h out A toy transformer model partly inspired nanoGPT model https github com karpathy nanoGPT Transformer nn Module __init__ args ModelArgs super __init__ assert args vocab_size None assert args max_seq_len None model_args = args max_seq_len = args max_seq_len tok_embeddings = nn Embedding args vocab_size args dim pos_embeddings = nn Embedding args max_seq_len args dim dropout = nn Dropout args dropout_p layers = nn ModuleList _ range args n_layers layers append TransformerBlock args norm = nn LayerNorm args dim output = nn Linear args dim args vocab_size bias=False args weight_tying output weight = tok_embeddings weight checkpoint_activations = args checkpoint_activations forward tokens _bsz seq_len = tokens size assert seq_len = max_seq_len h = tok_embeddings tokens pos = torch arange seq_len device=tokens device p = pos_embeddings pos positional embeddings shape seq_len dim h = h + p h = dropout h layer layers checkpoint_activations h = torch utils checkpoint checkpoint layer h use_reentrant=False h = layer h h = norm h output = output h float output staticmethod parallelize module Transformer device_mesh DeviceMesh use_seq_parallel bool local_output_for_attn bool = False - nn Module assert isinstance module Transformer f Requires Transformer got module Parallelize root submodules use_seq_parallel root_plan = tok_embeddings RowwiseParallel input_layouts=Replicate output_layouts=Shard pos_embeddings RowwiseParallel input_layouts=Replicate output_layouts=Shard norm SequenceParallel root_plan = tok_embeddings RowwiseParallel input_layouts=Replicate output_layouts=Replicate pos_embeddings RowwiseParallel input_layouts=Replicate output_layouts=Replicate module_tp = parallelize_module module device_mesh root_plan Parallelize attention feed forward submodules layer module_tp layers layer_parallelize_plan = use_seq_parallel layer_parallelize_plan attention = PrepareModuleInput input_layouts=Shard desired_input_layouts=Replicate shard RMSNorms layer_parallelize_plan attention_norm = SequenceParallel layer_parallelize_plan ffn_norm = SequenceParallel layer_parallelize_plan attention wq = ColwiseParallel use_local_output=local_output_for_attn layer_parallelize_plan attention wk = ColwiseParallel use_local_output=local_output_for_attn layer_parallelize_plan attention wv = ColwiseParallel use_local_output=local_output_for_attn layer_parallelize_plan attention wo = RowwiseParallel output_layouts=Shard use_seq_parallel RowwiseParallel layer_parallelize_plan feed_forward w = ColwiseParallel input_layouts=Shard use_seq_parallel ColwiseParallel layer_parallelize_plan feed_forward w = RowwiseParallel output_layouts=Shard use_seq_parallel RowwiseParallel parallelize_module layer device_mesh layer_parallelize_plan Parallelize output submodule If weight tying enabled we need make sure output weight sharded consistently tok_embeddings weight cost all_reduce operation using RowwiseParallel output_parallelize_plan = ColwiseParallel input_layouts=Shard output_layouts=Replicate use_seq_parallel ColwiseParallel output_layouts=Replicate parallelize_module module_tp output device_mesh output_parallelize_plan local_output_for_attn layer module_tp layers layer attention n_heads = module_tp model_args n_heads device_mesh size Manually set output weight so parameters gradients shared module_tp model_args weight_tying module_tp output weight = module_tp tok_embeddings weight module_tp skip_unless_torch_gpu method T - T Test decorator which skips test unless there s GPU available torch xdoctest +SKIP skip_unless_torch_gpu test_some_method - None The builtin skip_if_no_gpu relies os environ WORLD_SIZE being set cast T skip_if_lt_x_gpu NUM_DEVICES method DTensorContinuousTestBase MultiProcContinuousTest classmethod device_type cls - str enough GPU XPU HPU we can use those devices otherwise we fallback CPU TEST_CUDA TEST_XPU TEST_HPU TEST_PRIVATEUSE DEVICE_COUNT cls world_size cpu DEVICE_TYPE classmethod backend_str cls - str backend = dist get_default_backend_for_device DEVICE_TYPE backend DTensorTestBase MultiProcessTestCase property is_local_tensor_enabled - bool False property world_size - int NUM_DEVICES property device_type - str enough GPU XPU HPU we can use those devices otherwise we fallback CPU TEST_CUDA TEST_XPU TEST_HPU TEST_PRIVATEUSE DEVICE_COUNT world_size cpu DEVICE_TYPE property backend - str backend = dist get_default_backend_for_device DEVICE_TYPE backend init_manual_seed_for_rank - None torch manual_seed rank build_device_mesh - DeviceMesh init_device_mesh device_type world_size init_pg eager_init backend Optional str = None - None nccl backend torch cuda device_count world_size sys exit TEST_SKIPS f multi-gpu- world_size exit_code curr_backend = dist get_default_backend_for_device device_type backend None backend = backend backend nccl gloo mpi f cpu gloo device_type curr_backend hccl xccl fake cpu gloo xpu xccl raise RuntimeError f Backend backend supported device_id = None nccl backend xccl backend set device nccl pg collectives TODO users want enable testing across hosts we may need change part torch accelerator set_device_index rank we only need set device_id nccl backend eager init device_id = torch device f device_type rank eager_init None For nccl backend bind device process device_id None so nccl communicator immediately formed we can use ` ncclCommSplit ` form subgroup avoid unnecessary overhead dist init_process_group backend=backend world_size=self world_size rank=self rank pyre-ignore init_method=f file file_name pyre-ignore device_id=device_id destroy_pg device_id Optional int = None - None Wait all ranks reach here before starting shutdown FIXME dist barrier deadlocks multiple threads NCCL https github com pytorch pytorch issues dist all_reduce torch zeros device= cuda TEST_CUDA cpu FIXME can t use above all_reduce causes hangs bionic focal It hangs test_dtensor py -- DTensorMeshTest test_dtensor_device_mesh_device_conversion device_id None device_id = torch cuda current_device device_type == cuda rank device_type == cpu NOTE when ` device_id ` None barrier will choose accelerator most pripority which means test specifies use CPU testing while CUDA available host barrier will use CUDA To avoid better respect ` device_type ` we add branch enforce barrier use CPU when ` device_type ` CPU other accelerator also available dist barrier dist barrier device_ids= device_id dist destroy_process_group setUp - None super setUp _spawn_processes _test_op_on_dtensor op_call args kwargs - None This function checks ` ` op_call dtensor full_tensor == op_call dtensor full_tensor ` ` Unlike _test_op where DTensor sharding generated DTensorConverter function takes DTensor object directly argument test equality calling op full_tensor DTensor call full_tensor DTensor args kwargs args_flattened args_spec = tree_flatten args full_tensor_args_flattened = tuple arg full_tensor detach clone isinstance arg DTensor arg arg args_flattened full_tensor_args = tree_unflatten full_tensor_args_flattened args_spec full_tensor_kwargs = k v full_tensor isinstance v DTensor v k v kwargs items out_flattened _ = tree_flatten op_call full_tensor_args full_tensor_kwargs d_out_flattened _ = tree_flatten op_call args kwargs d_out_full_tensor_flattened = dt full_tensor dt d_out_flattened assertEqual out_flattened d_out_full_tensor_flattened pyre-ignore _test_op mesh DeviceMesh op_call args kwargs - None out = op_call args kwargs dtc = DTensorConverter mesh args kwargs d_args d_kwargs dtc pyre can t find assertTrue anymore assertEqual dtc successful True d_out = op_call d_args d_kwargs assertEqual d_out full_tensor out run_subtests args kwargs run_subtests args kwargs TestFunc = Callable object wrapper initialize comms processgroup with_comms eager_init Union TestFunc bool = False backend Optional str = None - TestFunc decorator func eager_init bool = False backend Optional str = None wraps func pyre-ignore wrapper args tuple object kwargs dict str Any type ignore misc - None init_pg eager_init backend try func args kwargs type ignore misc except Exception e dist destroy_process_group raise e destroy_pg wrapper decorator func=eager_init callable eager_init partial decorator eager_init=eager_init backend=backend DTensorOpTestBase MultiThreadedTestCase property world_size - int NUM_DEVICES property device_type - str DEVICE_TYPE build_device_mesh init_device_mesh device_type world_size setUp - None super setUp _spawn_threads This converting args kwargs op into distributed args kwargs DTensorConverter __init__ mesh DeviceMesh args tuple object kwargs dict str object - None hit = miss = mesh = mesh args = args kwargs = kwargs flatten_args flatten_args_spec = tree_flatten args flatten_kwargs flatten_kwargs_spec = tree_flatten kwargs flatten_args list object = flatten_args flatten_args_spec TreeSpec = flatten_args_spec flatten_kwargs list object = flatten_kwargs flatten_kwargs_spec TreeSpec = flatten_kwargs_spec choices_for_args = gen_sharding_choices_for_arg arg arg flatten_args isinstance arg torch Tensor choices_for_args extend gen_sharding_choices_for_arg arg arg flatten_kwargs isinstance arg torch Tensor sharding_combs Iterator Sequence Placement = iter itertools product choices_for_args successful - bool hit miss == is_supported_tensor t torch Tensor - bool TODO dist tensor need support quantized sparse tensors quantized tensor might relatively easy sparse tensor have special layouts we need possibly deal until we clear about them we don t officially support them any t is_sparse_csr t is_sparse t is_mkldnn t is_quantized t is_nested torch _is_functional_tensor t t is_neg t is_conj t device type lazy meta We need way test tensor batched there no official APi do torch _C _is_batched t gen_sharding_choices_for_arg arg torch Tensor - Sequence Placement mesh_size = mesh size sharding_choices list Placement = Replicate c d collective does support bool tensor bool tensor we treat replicated arg dtype = torch bool only generating choices replicate sharding evenly dimension could sharded sharding_choices = sharding_choices + Shard i i s enumerate arg shape s s mesh_size == TODO add multi mesh choices all_choices = itertools product mesh ndim sharding_choices sharding_choices __iter__ - DTensorConverter __next__ - tuple tuple object dict str object try next_sharding_choices = next sharding_combs idx = new_args list object = arg flatten_args isinstance arg torch Tensor new_args append to_dist_tensor arg mesh next_sharding_choices idx idx += new_args append arg new_kwargs list object = arg flatten_kwargs isinstance arg torch Tensor new_kwargs append to_dist_tensor arg mesh next_sharding_choices idx idx += new_kwargs append arg tree_unflatten new_args flatten_args_spec tree_unflatten new_kwargs flatten_kwargs_spec except StopIteration e raise StopIteration e to_dist_tensor t torch Tensor mesh DeviceMesh placements list Placement - torch Tensor type t torch Tensor type t nn Parameter type t LocalTensor is_supported_tensor t hit += t ndim == scalar tensor default will replicated r = distribute_tensor t mesh Replicate mesh ndim distribute non-scalar tensors r = distribute_tensor t mesh placements isinstance t nn Parameter r = nn Parameter type ignore assignment r requires_grad=r requires_grad r miss += t torch overrides is_tensor_like t Blindly converting tensor subclasses dist tensor can cause unpredictable problems we explicitly disable conversion now i e we don t support DTensor holding tensor subclass until there s strong reason later miss += t raise RuntimeError f Trying convert DTensor got type t LocalDTensorTestBase DTensorTestBase property is_local_tensor_enabled - bool True _handle_test_skip msg str - None skipTest msg _get_local_tensor_mode LocalTensorMode frozenset range world_size setUp - None super setUp torch autograd _enable_record_function False tearDown - None super tearDown torch autograd _enable_record_function True property rank torch SymInt LocalIntNode r r r range world_size rank setter rank rank pass join_or_run fn wraps fn wrapper fn types MethodType wrapper build_device_mesh - DeviceMesh maybe_disable_local_tensor_mode super build_device_mesh init_pg eager_init backend Optional str = None - None dist init_process_group fake rank= world_size=self world_size _pg = dist distributed_c d _get_default_group destroy_pg device_id Optional int = None - None dist destroy_process_group _pg _pg = None _spawn_processes - None pass run_test test_name str parent_pipe - None getattr test_name init_manual_seed_for_rank - None torch manual_seed make_wrapped fn ctxs functools wraps fn wrapped torch _dynamo reset stack = contextlib ExitStack ctx ctxs callable ctx stack enter_context ctx stack enter_context ctx try out = fn finally stack close out wrapped create_local_tensor_test_class orig_cls skipped_tests=None skipped_tests None skipped_tests = dct = orig_cls __dict__ copy name list dct keys fn = dct name callable fn continue name skipped_tests dct name = lambda skipTest Skipped test name startswith test_ ctxs = lambda test test _get_local_tensor_mode dct name = make_wrapped fn ctxs cls = type orig_cls __name__ + WithLocalTensor LocalDTensorTestBase + orig_cls __bases__ dct cls __file__ = __file__ cls maybe_run_for_local_tensor map_local_tensor_for_rank tensor rank func func tensor rank maybe_run_for_local_tensor map_local_for_rank rank func func rank