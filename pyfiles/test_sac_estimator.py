Owner s oncall distributed unittest torch torch _subclasses fake_tensor FakeTensorMode torch distributed _tools sac_estimator SACEstimator torch testing _internal common_cuda TEST_CUDA torch testing _internal common_utils run_tests skipIfTorchDynamo TestCase torch testing _internal distributed _tensor common_dtensor ModelArgs Transformer TestSACEstimator TestCase _sac_estimation estimate_mode str model torch nn Module inp torch Tensor sace = SACEstimator sace estimate_mode_type=estimate_mode loss = model inp sum loss backward sace pwlf_sac_tradeoff_curve n_segments= save_tradeoff_graphs=False skipIfTorchDynamo https github com pytorch pytorch issues unittest skipIf TEST_CUDA CUDA available test_transformer_sac_estimation Runs basic GPT- model dev = torch cuda current_device vocab_size = bsz seq_len = model_args = ModelArgs n_layers= n_heads= vocab_size=vocab_size max_seq_len=seq_len dim= dropout_p= FakeTensorMode torch device dev model = Transformer model_args inp = torch randint model_args vocab_size bsz model_args max_seq_len device=dev _sac_estimation operator-level-benchmark model inp _sac_estimation operator-level-cost-model model inp skipIfTorchDynamo https github com pytorch pytorch issues unittest skipIf TEST_CUDA CUDA available test_simple_model_sac_estimation This test checks correctness view_ops random_ops inplace_ops Foo torch nn Module __init__ super __init__ fc = torch nn Linear relu = torch nn ReLU inplace=True forward x x = fc x x = relu x x = torch cos_ x x = torch sin_ x x dev = torch cuda current_device FakeTensorMode torch device dev model = Foo x = torch rand device=dev sac_estimator = SACEstimator sac_estimator estimate_mode_type= operator-level-benchmark loss = model x sum loss backward assertEqual sac_estimator sac_mod_stats Foo view_like_ops assertEqual sac_estimator sac_mod_stats Foo rand_ops assertEqual sac_estimator sac_mod_stats Foo inplace_ops __name__ == __main__ run_tests