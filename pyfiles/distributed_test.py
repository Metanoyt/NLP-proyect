mypy allow-untyped-defs copy itertools json math operator os random re sys tempfile time unittest collections defaultdict namedtuple OrderedDict collections abc Callable contextlib contextmanager nullcontext dataclasses dataclass datetime timedelta functools reduce typing Any NamedTuple Union numpy np torch torch cuda torch distributed dist torch distributed algorithms model_averaging averagers averagers torch distributed algorithms model_averaging hierarchical_model_averager hierarchicalSGD torch distributed algorithms model_averaging utils model_averaging_utils torch distributed optim post_localSGD_optimizer post_localSGD_optimizer torch nn nn torch nn functional F torch _utils_internal TEST_MASTER_ADDR MASTER_ADDR TEST_MASTER_PORT MASTER_PORT torch autograd DeviceType torch cuda amp autocast GradScaler torch distributed algorithms ddp_comm_hooks default_hooks default post_localSGD_hook post_localSGD powerSGD_hook powerSGD quantization quantization_hooks torch distributed distributed_c d _get_default_group _get_pg_config get_world_size torch distributed optim _apply_optimizer_in_backward torch distributed utils _sync_module_states _verify_param_shape_across_processes torch nn parallel DistributedDataParallel torch nn parallel distributed _dump_DDP_relevant_env_vars _MixedPrecision torch profiler ExecutionTraceObserver ProfilerActivity torch testing _internal common_distributed captured_output cleanup_temp_dir DistTestCases init_multigpu_helper initialize_temp_directories MultiProcessTestCase nccl_skip_if_lt_x_gpu require_n_gpus_for_nccl_backend requires_nccl_version simple_sparse_reduce_tests skip_if_lt_x_gpu skip_if_no_gpu skip_if_odd_worldsize skip_if_rocm_multiprocess skip_if_small_worldsize TEST_SKIPS verify_ddp_error_logged with_dist_debug_levels with_nccl_blocking_wait torch testing _internal common_utils FILE_SCHEMA instantiate_parametrized_tests IS_FBCODE IS_MACOS IS_SANDCASTLE IS_WINDOWS skip_but_pass_in_sandcastle skip_but_pass_in_sandcastle_if skipIfRocm torch utils _python_dispatch TorchDispatchMode torch utils data distributed DistributedSampler try torchvision HAS_TORCHVISION = True except Exception Covering both ImportError RuntimeError HAS_TORCHVISION = False sys platform == win msvcrt fcntl NetWithBuffers nn Module __init__ - None super __init__ = nn Linear bias=False b = nn Linear bias=False register_buffer buffer torch randn forward x buffer add_ b x Foo __init__ x Can tensor int x = x __eq__ other eq value other isinstance value torch Tensor torch equal value other value == other attr value __dict__ items other_value = other __dict__ attr eq value other_value False True f = Foo f bar = Defer instantiation until seed set so randn returns same values all processes create_collectives_object_test_list key key key nested True f Foo torch randn foo True string nested Allowlist distributed backends where profiling collectives supported PROFILING_SUPPORTED_BACKENDS = dist Backend NCCL dist Backend GLOO dist Backend MPI dist Backend UCC Allowlist distributed backends where profiling supported use_cuda=True CUDA_PROFILING_SUPPORTED_BACKENDS = dist Backend GLOO dist Backend MPI dist Backend NCCL dist Backend UCC Allowlist distributed backends where profiling supported p p ops SEND_RECV_PROFILING_SUPPORTED_BACKENDS = dist Backend MPI dist Backend GLOO dist Backend NCCL dist Backend UCC Dummy NamedTuple data structures test DDP support NamedTuple types EXPECTED_FIELDS = b TestNamedTupleInput_ = namedtuple NamedTuple EXPECTED_FIELDS TestNamedTupleInput_ NamedTuple torch tensor b torch tensor skipIfNoTorchVision = skip_but_pass_in_sandcastle_if HAS_TORCHVISION no torchvision BACKEND = os environ BACKEND INIT_METHOD = os getenv INIT_METHOD env DEFAULT_TIMEOUT = CUSTOMIZED_TIMEOUT = test_DistributedDataParallel get_profiling_event event_name profiler dedup_gpu_user_annotation=False event_list = profiler events isinstance profiler torch profiler profile profiler function_events event event event_list event name endswith event_name event name startswith event_name dedup_gpu_user_annotation event device_type = DeviceType CUDA get_profiler_nccl_meta prof Torch profiler includes nccl metadata inserted operator called record_param_comms We will need test metadata obtained profiler here tf = tempfile NamedTemporaryFile mode= w+t suffix= json delete=False tf close trace_file = tf name prof export_chrome_trace trace_file open trace_file f events = json load f traceEvents print f Trace saved trace_file Comment debug os remove trace_file e e events e get name == record_param_comms Base error message substring unfinished reductions ddp_prev_reduction_unfinished_str = Expected have finished reduction prior iteration Error message substring when find_unused_parameters=True has been passed ddp_recommend_find_unused_params_str = passing keyword argument ` find_unused_parameters=True ` Error message substring when find_unused_parameters=True enabled ddp_find_unused_params_enabled_str = Since ` find_unused_parameters=True ` enabled Error message substring possibility all model outputs being used loss computation ddp_outputs_not_used_in_loss_str = ` forward ` function outputs participate calculating loss Error message substring suggesting use TORCH_DISTRIBUTED_DEBUG ddp_suggest_debug_mode_str = set environment variable TORCH_DISTRIBUTED_DEBUG either INFO DETAIL DDPUnevenTestInput NamedTuple name str model nn Module inp Union torch tensor tuple sync_interval int throw_on_early_termination bool = False hook Callable = None state Any = None _FC nn Module __init__ - None super __init__ fc = nn Linear bias=True fc bias requires_grad = False forward x x = fc x x Net nn Module __init__ - None super __init__ fc = nn Linear bias=False fc = _FC fc = nn Linear bias=False relu = nn ReLU no_grad_param = nn Parameter torch tensor long requires_grad=False forward x x = relu fc x x = relu fc x x = fc x F softmax x dim= LargeNet nn Module __init__ - None super __init__ fc = nn Linear bias=False fc = nn Linear bias=False forward x x = fc x x = fc x x Task nn Module __init__ - None super __init__ p = nn Parameter torch ones forward x p + x BatchNormNet nn Module __init__ affine=True super __init__ fc = nn Linear bias=False bn = nn BatchNorm d affine=affine fc = nn Linear bias=False forward x x = torch reshape fc x - x = bn x x = torch reshape x - x = fc x F softmax x dim= UnusedParamTwoLinLayerNet nn Module __init__ - None super __init__ = nn Linear bias=False b = nn Linear bias=False c = nn Linear bias=False forward x = x b = b x b DictOutputModule nn Module __init__ - None super __init__ module = UnusedParamTwoLinLayerNet forward x predictions = module x loss = predictions + predictions sum predictions predictions loss loss TwoLinLayerNet nn Module __init__ - None super __init__ = nn Linear bias=False b = nn Linear bias=False forward x = x b = b x b EmbeddingNetDifferentParams nn Module A module containing embedding different dimension different parameters depending rank __init__ rank diff_num_params=False super __init__ embedding_dim = diff_num_params rank == embedding = nn Embedding num_embeddings= embedding_dim=embedding_dim lin = nn Linear embedding_dim diff_num_params lin = nn Linear bias=False forward x x = embedding x lin x ControlFlowToyModel nn Module __init__ - None super __init__ lin = nn Linear bias=False lin = nn Linear bias=False forward x Second layer used dependent input x use_second_layer = torch equal x torch ones device=x device use_second_layer lin F relu lin x F relu lin x get_timeout test_id test_name = test_id split - test_name CUSTOMIZED_TIMEOUT CUSTOMIZED_TIMEOUT test_name DEFAULT_TIMEOUT default_pg_timeout = CUSTOM_PG_TIMEOUT = This test runs slowly needs additional time complete otherwise can taken down TORCH_NCCL_ASYNC_ERROR_HANDLING test_ddp_uneven_inputs This test has short timeout since tests being taken down TORCH_NCCL_ASYNC_ERROR_HANDLING which we want happen quickly test_ddp_model_diff_across_ranks This test has short timeout since tests being taken down TORCH_NCCL_ASYNC_ERROR_HANDLING which we want happen quickly test_ddp_has_finalized require_backend_is_available backends check backend backend == dist Backend GLOO dist is_gloo_available backend == dist Backend NCCL dist is_nccl_available backend == dist Backend MPI dist is_mpi_available backend == dist Backend UCC dist is_ucc_available backend DistTestCases backend_feature plugin True False BACKEND backends skip_but_pass_in_sandcastle f Test requires backend BACKEND one backends check dist Backend BACKEND skip_but_pass_in_sandcastle f Test requires backend BACKEND available lambda func func require_world_size world_size int os environ WORLD_SIZE world_size skip_but_pass_in_sandcastle f Test requires world size world_size d lambda func func require_exact_world_size world_size int os environ WORLD_SIZE = world_size skip_but_pass_in_sandcastle f Test requires exact world size world_size d lambda func func contextmanager _lock TEMP_DIR = os environ TEMP_DIR lockfile = os path join TEMP_DIR lockfile open lockfile w lf try sys platform == win msvcrt locking lf fileno msvcrt LK_RLCK yield fcntl flock lf fileno fcntl LOCK_EX yield finally sys platform == win msvcrt locking lf fileno msvcrt LK_UNLCK fcntl flock lf fileno fcntl LOCK_UN lf close contextmanager _rank_temp_file dist get_rank == fd name = tempfile mkstemp os close fd name = None object_list = name dist broadcast_object_list object_list name = object_list try yield name finally dist get_rank == os remove name _build_tensor size value=None dtype=torch float device_id=None value None value = size device_id None torch empty size size size dtype=dtype fill_ value torch empty size size size dtype=dtype fill_ value cuda device_id _build_multidim_tensor dim dim_size value=None dtype=torch float value None value = dim torch empty size= dim_size _ range dim dtype=dtype fill_ value _create_autograd_profiler torch autograd profiler profile record_shapes=True _create_torch_profiler torch profiler profile activities= torch profiler ProfilerActivity CPU record_shapes=True Barrier barrier_id = classmethod init cls cls barrier_id = barrier_dir = os path join os environ TEMP_DIR barrier f_name os listdir barrier_dir os unlink os path join barrier_dir f_name classmethod sync cls wait_for=None timeout= wait_for None wait_for = dist get_world_size cls barrier_id += barrier_dir = os path join os environ TEMP_DIR barrier pid = str os getpid barrier_file = os path join barrier_dir pid _lock open barrier_file w f f write str cls barrier_id start_time = time time while True arrived = _lock f_name os listdir barrier_dir open os path join barrier_dir f_name f data = f read int data = cls barrier_id arrived += arrived == wait_for break time time - start_time timeout raise RuntimeError barrier timeout time sleep TestDistBackend MultiProcessTestCase classmethod setUpClass cls os environ MASTER_ADDR = str MASTER_ADDR Not setting MASTER_PORT get random free port super setUpClass setUp super setUp initialize temp directories initialize_temp_directories initialize Barrier Barrier init Skip code checking following tests they expected crash process due TORCH_NCCL_ASYNC_ERROR_HANDLING skip_return_code_checks = test_ddp_has_finalized __wrapped__ tearDown cleanup_temp_dir super tearDown property init_method f FILE_SCHEMA file_name property destroy_pg_upon_exit - bool Overriding base test do auto destroy PG upon exit False classmethod _run cls rank test_name file_name pipe kwargs BACKEND == nccl torch cuda is_available sys exit TEST_SKIPS no_cuda exit_code = cls test_name rank = rank file_name = file_name torch cuda is_available torch cuda device_count int world_size sys exit TEST_SKIPS f multi-gpu- world_size exit_code try pg_timeout_seconds = CUSTOM_PG_TIMEOUT get test_name default_pg_timeout timeout = timedelta seconds=pg_timeout_seconds dist init_process_group init_method=self init_method backend=BACKEND world_size=int world_size rank=self rank timeout=timeout except RuntimeError e recompile e args sys exit TEST_SKIPS backend_unavailable exit_code raise Execute barrier prior running test ensure every process has finished initialization following test immediately exiting due skip doesn t cause flakiness _barrier run_test test_name pipe _barrier dist destroy_process_group sys exit Needed since MultiProcessTestCase assumes world_size we run these tests under other various world_sizes property world_size os environ WORLD_SIZE DistributedTest _DistTestBase _barrier args kwargs Barrier sync args kwargs _init_group_test kwargs group = group_id = dist new_group group kwargs rank = dist get_rank rank group None rank group group_id rank _init_full_group_test kwargs group = list range dist get_world_size group_id = dist new_group kwargs rank = dist get_rank group group_id rank _init_global_test group = list range dist get_world_size group_id = dist group WORLD rank = dist get_rank group group_id rank _verify_buffers_equal m m verify buffers across models m _buf_dict = dict m module named_buffers name buf m module named_buffers assertEqual buf m _buf_dict name Verify buffers across ranks m _buffers = list m buffers m _buffers = list m buffers buf buf zip m _buffers m _buffers strict=True gathered_bufs = torch empty_like buf _ range dist get_world_size dist all_gather gathered_bufs buf gathered_bufs_m = torch empty_like buf _ range dist get_world_size b gathered_bufs assertEqual b buf dist all_gather gathered_bufs_m buf b gathered_bufs_m assertEqual b buf _sanity_check_profiler_nccl_meta nccl_meta_events Torch profiler includes nccl metadata inserted operator called record_param_comms We test basic fields profiler event correspond nccl communication collectives per_coll_meta = defaultdict list e nccl_meta_events args = e get args collname = args get Collective name assertNotEqual collname assertNotEqual args get dtype per_coll_meta collname append args collname == wait continue assertEqual args Process Group Description default_pg assertNotEqual args Process Group Ranks assertGreaterEqual args get In msg nelems - assertGreaterEqual args get Out msg nelems - assertGreaterEqual args get Group size - assertGreaterEqual args get Global rank start - assertGreaterEqual args get Global rank stride - print per_coll_meta per_coll_meta test_dump_DDP_relevant_env_vars captured_output out _ _dump_DDP_relevant_env_vars lines = out getvalue splitlines format_line var f env var = os environ get var N A Check relevant env vars vars = MASTER_ADDR MASTER_PORT WORLD_SIZE NCCL_TOPO_DUMP_FILE N A TORCH_NCCL_ASYNC_ERROR_HANDLING var vars line = format_line var assertIn line lines Check irrelevant env vars vars = xxx yyy zzz var vars line = format_line var assertNotIn line lines GET RANK test_get_rank test_dir = os path join os environ TEMP_DIR test_dir pid = str os getpid num_processes = dist get_world_size open os path join test_dir pid w f f write str dist get_rank _barrier all_ranks = set f_name os listdir test_dir open os path join test_dir f_name f all_ranks add int f read assertEqual len all_ranks num_processes _barrier dist get_rank == f_name os listdir test_dir os unlink os path join test_dir f_name _barrier test_get_backend dist get_world_size group = group = group_id = dist new_group group backend_str = BACKEND lower assertEqual dist get_backend backend_str dist get_rank group assertEqual dist get_backend group_id backend_str assertRaisesRegex ValueError Invalid process group specified dist get_backend group_id test_Backend_enum_class test parsing backend = BACKEND lower assertEqual dist Backend BACKEND upper backend assertEqual dist Backend BACKEND backend assertRaises ValueError dist Backend None assertRaises ValueError dist Backend assertRaises ValueError dist Backend gloo Test destroy test_destroy_group dist get_world_size group = group = group_id = dist new_group group _barrier dist destroy_process_group group_id Test get rank size group test_get_rank_size_group dist get_world_size group = group = group_id = dist new_group group dist get_rank group assertEqual dist get_world_size group_id assertTrue dist get_rank group_id list range assertEqual dist get_world_size group_id - assertEqual dist get_rank group_id - Test destroy full groups test_destroy_full_group _ group_id _ = _init_full_group_test _barrier dist destroy_process_group group_id Test get rank size full group test_get_rank_size_full_group _ group_id _ = _init_full_group_test assertEqual dist get_world_size group_id dist get_world_size assertEqual dist get_rank group_id dist get_rank _test_barrier_timeout group_id timeout local_rank = dist get_rank group_id Only execute barrier rank == causing timeout local_rank == expected_time = time time + timeout total_seconds In debug mode we execute monitored_barrier before collective so assert dist get_debug_level == dist DebugLevel DETAIL exception_ctx = assertRaisesRegex Exception failed pass monitoredBarrier exception_ctx = assertRaisesRegex Exception Timed out &#124; closed &#124; timeout exception_ctx dist barrier group_id assertGreaterAlmostEqual time time expected_time delta= skip_but_pass_in_sandcastle_if BACKEND = gloo Only gloo backend supports timeouts skip_but_pass_in_sandcastle_if INIT_METHOD startswith file Requires file initialization method + Both tcp env rely TCP store which reinitialization has proven racy test_barrier_timeout_global dist destroy_process_group Explicitly pass world size barrier because we ve just destroyed any state torch distributed _barrier wait_for=int os environ WORLD_SIZE Reinitialize global process group timeout = timedelta seconds= dist init_process_group init_method=INIT_METHOD backend=BACKEND world_size=int os environ WORLD_SIZE rank=self rank timeout=timeout _test_barrier_timeout dist group WORLD timeout skip_if_small_worldsize skip_but_pass_in_sandcastle_if BACKEND = gloo Only gloo backend supports timeouts test_barrier_timeout_group timeout = timedelta seconds= _ group_id _ = _init_group_test timeout=timeout group_id None _test_barrier_timeout group_id timeout skip_but_pass_in_sandcastle_if BACKEND = gloo Only gloo backend supports timeouts test_barrier_timeout_full_group timeout = timedelta seconds= _ group_id _ = _init_full_group_test timeout=timeout group_id None _test_barrier_timeout group_id timeout skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature subgroup f The BACKEND backend does support creating subgroups CUDA devices require_world_size skip_if_lt_x_gpu test_new_subgroups subgroup_size = cur_subgroup subgroups = dist new_subgroups subgroup_size world_size = dist get_world_size assertEqual cur_subgroup size subgroup_size assertEqual len subgroups world_size subgroup_size assertFalse dist _rank_not_in_group cur_subgroup subgroup subgroups dist destroy_process_group subgroup skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature subgroup f The BACKEND backend does support creating subgroups CUDA devices require_exact_world_size test_new_subgroups_with_group_param Initialize global test environment _init_global_test Set up GPU devices each rank init_multigpu_helper dist get_world_size BACKEND Create two subgroups one ranks another ranks cur_subgroup subgroups = dist new_subgroups_by_enumeration ranks_per_subgroup_list= Further divide current subgroup into sub-subgroups size cur_sub_subgroup sub_subgroups = dist new_subgroups group_size= group=cur_subgroup Verify we have sub-subgroups one each rank original subgroup assertEqual len sub_subgroups Verify current process s sub-subgroup has size assertEqual cur_sub_subgroup size Verify current process its assigned sub-subgroup assertFalse dist _rank_not_in_group group=cur_sub_subgroup Clean up destroying all created process groups sub_subgroup sub_subgroups dist destroy_process_group sub_subgroup subgroup subgroups dist destroy_process_group subgroup skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature subgroup f The BACKEND backend does support creating subgroups CUDA devices skip_if_no_gpu test_new_subgroups_group_size_exceeds_world_size assertRaisesRegex ValueError must exceed dist new_subgroups skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature subgroup f The BACKEND backend does support creating subgroups CUDA devices require_world_size skip_if_lt_x_gpu test_new_subgroups_world_size_not_divisible_by_group_size expected_msg = f The world size dist get_world_size must divisible group_size= assertRaisesRegex ValueError re escape expected_msg dist new_subgroups skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature subgroup f The BACKEND backend does support creating subgroups CUDA devices require_world_size skip_if_lt_x_gpu test_new_subgroups_by_enumeration _group _group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND device_id = rank_to_GPU rank cur_subgroup subgroups = dist new_subgroups_by_enumeration ranks_per_subgroup_list= device_id = assertIsNone cur_subgroup assertEqual cur_subgroup size assertEqual len subgroups device_id == device_id == assertEqual cur_subgroup subgroups assertEqual cur_subgroup subgroups subgroup subgroups dist destroy_process_group subgroup skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature subgroup f The BACKEND backend does support creating subgroups CUDA devices require_world_size skip_if_lt_x_gpu test_new_subgroups_by_enumeration_input_rank_exceeds_world_size _group group_id _rank = _init_global_test init_multigpu_helper dist get_world_size BACKEND world_size = get_world_size group_id assertRaisesRegex ValueError The new group s rank should within world_size set init_process_group dist new_subgroups_by_enumeration ranks_per_subgroup_list= world_size skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature subgroup f The BACKEND backend does support creating subgroups CUDA devices skip_if_no_gpu test_new_subgroups_by_enumeration_negative_input_rank _init_global_test assertRaisesRegex ValueError The new group s rank should within world_size set init_process_group dist new_subgroups_by_enumeration ranks_per_subgroup_list= - - - - skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature subgroup f The BACKEND backend does support creating subgroups CUDA devices require_world_size skip_if_lt_x_gpu test_new_subgroups_overlap_not_allowed assertRaisesRegex ValueError Rank has appeared both subgroup dist new_subgroups_by_enumeration ranks_per_subgroup_list= skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature subgroup f The BACKEND backend does support creating subgroups CUDA devices skip_if_lt_x_gpu test_average_parameters rank = dist get_rank rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND device_id = rank_to_GPU rank model = nn Sequential nn Conv d kernel_size= padding= nn ReLU nn Linear bias=False cuda device_id Test global model averaging p model parameters p data = torch ones_like p data model_averaging_utils average_parameters params=model parameters process_group=None Every element will same input p model parameters assertEqual p data torch ones_like p data Test partial model averaging p model parameters p data = torch ones_like p data rank group_nccl = dist new_group ranks= backend= nccl model_averaging_utils average_parameters params=model parameters process_group=group_nccl dist _rank_not_in_group group_nccl Every element device should average i e p model parameters assertEqual p data torch ones_like p data Every element device subgroup should remain same p model parameters assertEqual p data torch ones_like p data rank skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature subgroup f The BACKEND backend does support creating subgroups CUDA devices skip_if_lt_x_gpu test_periodic_model_averager rank = dist get_rank world_size = dist get_world_size rank_to_GPU = init_multigpu_helper world_size BACKEND device_id = rank_to_GPU rank model = nn Linear bias=False cuda device_id param = next model parameters tensor = torch ones_like param data rank expected_avg_tensor = torch ones_like param data sum range world_size world_size period = warmup_steps averager = averagers PeriodicModelAverager period=period warmup_steps=warmup_steps step range Reset parameters every step param data = copy deepcopy tensor params model parameters mock grad params grad = torch ones_like param data averager average_parameters model parameters step = warmup_steps step - warmup_steps period == assertEqual param data expected_avg_tensor No model averaging so parameters updated assertEqual param data tensor skip_if_lt_x_gpu test_periodic_model_averager_param_group rank = dist get_rank world_size = dist get_world_size rank_to_GPU = init_multigpu_helper world_size BACKEND device_id = rank_to_GPU rank model = nn Linear bias=False cuda device_id param = next model parameters opt = torch optim SGD model parameters lr= period = warmup_steps averager = averagers PeriodicModelAverager period=period warmup_steps=warmup_steps step range Reset parameters every step param_group opt param_groups params param_group params mock grad params grad = torch ones_like param data rank params data = torch ones_like param data rank averager average_parameters opt param_groups step = warmup_steps step - warmup_steps period == param_group opt param_groups params param_group params params grad None continue assertEqual param data torch ones_like param data sum range world_size world_size No model averaging so parameters updated param_group opt param_groups params param_group params params grad None continue assertEqual param data torch ones_like param data rank skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature subgroup f The BACKEND backend does support creating subgroups CUDA devices skip_if_lt_x_gpu test_ _level_hierarchical_model_averager_equivalent_to_periodic_model_averager rank = dist get_rank world_size = dist get_world_size rank_to_GPU = init_multigpu_helper world_size BACKEND device_id = rank_to_GPU rank model = nn Linear bias=False cuda device_id param = next model parameters tensor = torch ones_like param data rank expected_avg_tensor = torch ones_like param data sum range world_size world_size period = warmup_steps averager = hierarchicalSGD HierarchicalModelAverager Run global averaging period which equivalent above periodic model averaging test case period_group_size_dict=OrderedDict period world_size warmup_steps=warmup_steps averager = averagers PeriodicModelAverager period=period warmup_steps=warmup_steps step range Reset parameters every step param data = copy deepcopy tensor params model parameters mock grad params grad = torch ones_like param data averager average_parameters model parameters step = warmup_steps step - warmup_steps period == assertEqual param data expected_avg_tensor No model averaging so parameters updated assertEqual param data tensor skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature subgroup f The BACKEND backend does support creating subgroups CUDA devices require_exact_world_size skip_if_lt_x_gpu test_ _level_hierarchical_model_averager rank = dist get_rank world_size = dist get_world_size rank_to_GPU = init_multigpu_helper world_size BACKEND device_id = rank_to_GPU rank model = nn Linear bias=False cuda device_id param = next model parameters tensor = torch ones_like param data rank Set up such hierarchical model averaging follows after first warmup steps run model averaging every steps within each subgroup size run model averaging every steps within each subgroup size run global model averaging every steps If there conflict model averaging step only run highest-level model averaging warmup_steps = subgroup_size = subgroup_avg_period = subgroup_size = subgroup_avg_period = global_avg_period = period_group_size_dict = OrderedDict subgroup_avg_period subgroup_size subgroup_avg_period subgroup_size global_avg_period world_size averager = hierarchicalSGD HierarchicalModelAverager period_group_size_dict=period_group_size_dict warmup_steps=warmup_steps assertEqual dist get_pg_count len period_group_size_dict subgroup = averager period_process_group_dict subgroup_avg_period subgroup = averager period_process_group_dict subgroup_avg_period real_group_ranks_res = _get_pg_config subgroup ranks real_group_ranks_res = _get_pg_config subgroup ranks expect_group_ranks_res = rank subgroup_size subgroup_size + np array list range subgroup_size tolist expect_group_ranks_res = rank subgroup_size subgroup_size + np array list range subgroup_size tolist assertEqual real_group_ranks_res expect_group_ranks_res assertEqual real_group_ranks_res expect_group_ranks_res expected_avg_tensor_within_subgroup = torch ones_like param data sum real_group_ranks_res subgroup_size expected_avg_tensor_within_subgroup = torch ones_like param data sum real_group_ranks_res subgroup_size expected_global_avg_tensor = torch ones_like param data sum range world_size world_size step range Reset parameters every step param data = copy deepcopy tensor params model parameters mock grad params grad = torch ones_like param data averager average_parameters model parameters step == step == Run global model averaging when ` step ` can divided assertEqual param data expected_global_avg_tensor step == step == Run model averaging within subgroup when ` step ` can divided assertEqual param data expected_avg_tensor_within_subgroup step == step == step == step == Run model averaging within subgroup when ` step ` can divided assertEqual param data expected_avg_tensor_within_subgroup No model averaging so parameters updated assertEqual param data tensor Coalescing manager sync mode skip_if_no_gpu skip_but_pass_in_sandcastle_if BACKEND = nccl IS_FBCODE IS_SANDCASTLE Coalescing manager currently tests NCCL only internal test flaky test_coalescing_manager _barrier rank = dist get_rank world_size = dist get_world_size rank_to_GPU = init_multigpu_helper world_size BACKEND device_id = rank_to_GPU rank torch cuda set_device device_id num_colls = size_per_coll = small_tensors = torch ones size_per_coll device=device_id _ range num_colls dist _coalescing_manager i range num_colls dist all_reduce small_tensors i big_tensor = torch ones num_colls size_per_coll device=device_id dist all_reduce big_tensor i range num_colls assertEqual small_tensors i big_tensor i size_per_coll i + size_per_coll _barrier Coalescing manager async mode skip_if_no_gpu skip_but_pass_in_sandcastle_if BACKEND = nccl IS_FBCODE IS_SANDCASTLE Coalescing manager currently tests NCCL only internal test flaky test_coalescing_manager_async _barrier rank = dist get_rank world_size = dist get_world_size rank_to_GPU = init_multigpu_helper world_size BACKEND device_id = rank_to_GPU rank torch cuda set_device device_id num_colls = size_per_coll = small_tensors = torch ones size_per_coll device=device_id _ range num_colls dist _coalescing_manager async_ops=True cm i range num_colls dist all_reduce small_tensors i cm wait big_tensor = torch ones num_colls size_per_coll device=device_id dist all_reduce big_tensor i range num_colls assertEqual small_tensors i big_tensor i size_per_coll i + size_per_coll _barrier NCCL Batch SEND RECV skip_if_no_gpu skip_but_pass_in_sandcastle_if BACKEND = nccl NCCL Batch Send Recv Only requires_nccl_version Need NCCL + send recv test_batch_isend_irecv_nccl _barrier rank = dist get_rank world_size = dist get_world_size rank_to_GPU = init_multigpu_helper world_size BACKEND device_id = rank_to_GPU rank torch cuda set_device device_id p p_op_list = recv_tensors = None _ range world_size expected_tensors = None _ range world_size val os environ TORCH_NCCL_BLOCKING_WAIT = val src range world_size send_tensor = _build_tensor rank + device_id=device_id fill_ src recv_tensors src = _build_tensor src + value=- device_id=device_id fill_ - expected_tensors src = _build_tensor src + value=- device_id=device_id fill_ rank recv_op = dist P POp dist irecv recv_tensors src src p p_op_list append recv_op send_op = dist P POp dist isend send_tensor src p p_op_list append send_op reqs = dist batch_isend_irecv p p_op_list req reqs req wait src range world_size assertEqual recv_tensors src expected_tensors src _barrier skip_if_no_gpu skip_but_pass_in_sandcastle_if BACKEND = nccl NCCL Batch Send Recv Only requires_nccl_version Need NCCL + send recv test_batch_isend_irecv_ring_exchange_nccl _barrier rank = dist get_rank world_size = dist get_world_size rank_to_GPU = init_multigpu_helper world_size BACKEND device_id = rank_to_GPU rank torch cuda set_device device_id send_tensor = _build_tensor world_size device_id=device_id recv_tensor = _build_tensor world_size value=- device_id=device_id send_op = dist P POp dist isend send_tensor rank + world_size recv_op = dist P POp dist irecv recv_tensor rank - + world_size world_size reqs = dist batch_isend_irecv send_op recv_op req reqs req wait _barrier skip_if_no_gpu skip_but_pass_in_sandcastle_if BACKEND = nccl NCCL Batch Send Recv Only requires_nccl_version Need NCCL + send recv test_batch_isend_irecv_self_nccl _barrier Ensure process group has been fully initialized needed first sub-group batch_isend_irecv call dist barrier rank = dist get_rank rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND device_id = rank_to_GPU rank p p_op_list = rank == send_tensor = _build_tensor rank + device_id=device_id recv_tensor = _build_tensor rank + value=- device_id=device_id recv_op = dist P POp dist irecv recv_tensor p p_op_list append recv_op send_op = dist P POp dist isend send_tensor p p_op_list append send_op reqs = dist batch_isend_irecv p p_op_list req reqs req wait _barrier skip_if_no_gpu skip_if_small_worldsize skip_but_pass_in_sandcastle_if BACKEND = nccl NCCL Batch Send Recv Only requires_nccl_version Need NCCL + send recv test_batch_isend_irecv_no_rank_zero_nccl _barrier Ensure process group has been fully initialized needed first sub-group batch_isend_irecv call dist barrier rank = dist get_rank rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND device_id = rank_to_GPU rank torch cuda set_device device_id p p_op_list = rank == peer = rank == peer = rank send_tensor = _build_tensor rank + device_id=device_id recv_tensor = _build_tensor peer + value=- device_id=device_id recv_op = dist P POp dist irecv recv_tensor peer p p_op_list append recv_op send_op = dist P POp dist isend send_tensor peer p p_op_list append send_op reqs = dist batch_isend_irecv p p_op_list req reqs req wait _barrier GLOO Batch SEND RECV CPU skip_but_pass_in_sandcastle_if BACKEND = gloo GLOO Batch Send Recv CPU test_batch_isend_irecv_gloo _barrier rank = dist get_rank p p_op_list = src range dist get_world_size src == rank continue send_tensor = _build_tensor rank + recv_tensor = _build_tensor src + value=- recv_op = dist P POp dist irecv recv_tensor src p p_op_list append recv_op send_op = dist P POp dist isend send_tensor src p p_op_list append send_op reqs = dist batch_isend_irecv p p_op_list req reqs req wait _barrier GLOO Batch SEND RECV CPU provided tags skip_but_pass_in_sandcastle_if BACKEND = gloo GLOO Batch Send Recv CPU test_batch_isend_irecv_gloo_tags _barrier rank = dist get_rank p p_op_list = src range dist get_world_size src == rank continue send_tensor = _build_tensor rank + recv_tensor = _build_tensor src + value=- recv_op = dist P POp dist irecv recv_tensor src tag=src p p_op_list append recv_op send_op = dist P POp dist isend send_tensor src tag=rank p p_op_list append send_op reqs = dist batch_isend_irecv p p_op_list req reqs req wait _barrier NCCL Batch SEND RECV Op Error skip_but_pass_in_sandcastle_if BACKEND = nccl NCCL Batch Send Recv Only requires_nccl_version Need NCCL + send recv test_batch_isend_irecv_op_err _barrier rank = dist get_rank rank == rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND device_id = rank_to_GPU rank assertRaisesRegex ValueError ^Invalid ` ` op ` ` send_tensor = _build_tensor rank + device_id=device_id send_op = dist P POp dist broadcast send_tensor dist batch_isend_irecv send_op NCCL Batch SEND RECV p p_op_list Error skip_but_pass_in_sandcastle_if BACKEND = nccl NCCL Batch Send Recv Only requires_nccl_version Need NCCL + send recv test_batch_isend_irecv_op_list_err _barrier rank = dist get_rank rank == assertRaisesRegex ValueError ^Invalid ` ` p p_op_list ` ` dist batch_isend_irecv NCCL Batch SEND RECV Mixed Backend Error skip_but_pass_in_sandcastle_if BACKEND = nccl NCCL Batch Send Recv Only requires_nccl_version Need NCCL + send recv test_batch_isend_irecv_mixed_backend_err _barrier rank = dist get_rank init_multigpu_helper dist get_world_size BACKEND group_gloo = dist new_group ranks= backend= gloo group_nccl = dist new_group ranks= backend= nccl rank == assertRaisesRegex ValueError All ops need use same group send_tensor = _build_tensor rank + send_op_gloo = dist P POp dist isend send_tensor group_gloo send_op_nccl = dist P POp dist isend send_tensor group_nccl dist batch_isend_irecv send_op_gloo send_op_nccl NCCL SEND RECV skip_if_no_gpu skip_but_pass_in_sandcastle_if BACKEND = nccl NCCL Send Recv Only requires_nccl_version Need NCCL + send recv _test_send_recv_nccl profiler_ctx=None TODO now nccl send recv supported there does seem need have nccl send recv tested separately rank = dist get_rank world_size = dist get_world_size rank_to_GPU = init_multigpu_helper world_size BACKEND device_id = rank_to_GPU rank torch cuda set_device device_id tensor = _build_tensor rank + device_id=device_id profiler_cls = profiler_ctx profiler_ctx None nullcontext profiler_cls prof src range world_size src == rank Send mode dst range world_size dst == rank continue dist send tensor dst Recv mode expected_tensor = _build_tensor src + output_tensor = _build_tensor src + value=- device_id=device_id dist recv output_tensor src assertEqual output_tensor expected_tensor _barrier profiler_ctx None backend = dist get_backend backend SEND_RECV_PROFILING_SUPPORTED_BACKENDS event_name f backend send f backend recv events = get_profiling_event event_name prof dedup_gpu_user_annotation=True assertTrue events Event order deterministic so simply assert their shape found following list expected_shapes = rank + rank range dist get_world_size event events assertTrue event input_shapes expected_shapes skip_if_no_gpu skip_but_pass_in_sandcastle_if BACKEND = nccl NCCL Send Recv Only requires_nccl_version Need NCCL + send recv test_send_recv_nccl _test_send_recv_nccl skip_if_no_gpu skip_but_pass_in_sandcastle_if BACKEND = nccl NCCL Send Recv Only requires_nccl_version Need NCCL + send recv test_send_recv_nccl_autograd_profiler profiler_ctx = torch autograd profiler profile record_shapes=True _test_send_recv_nccl profiler_ctx skip_if_no_gpu skip_but_pass_in_sandcastle_if BACKEND = nccl NCCL Send Recv Only requires_nccl_version Need NCCL + send recv skip_but_pass_in_sandcastle_if IS_FBCODE Kineto fbcode causes hang skip_but_pass_in_sandcastle_if IS_MACOS IS_WINDOWS torch profiler enabled mac windows https github com pytorch pytorch pull test_send_recv_nccl_torch_profiler profiler_ctx = torch profiler profile activities= torch profiler ProfilerActivity CPU torch profiler ProfilerActivity CUDA record_shapes=True _test_send_recv_nccl profiler_ctx SEND RECV _test_send_recv profiler_ctx rank = dist get_rank send_size = rank + tensor = _build_tensor send_size ctx = profiler_ctx profiler_ctx None nullcontext ctx prof src range dist get_world_size src == rank Send mode dst range dist get_world_size dst == rank continue dist send tensor dst Recv mode recv_size = src + expected_tensor = _build_tensor recv_size output_tensor = _build_tensor recv_size value=- dist recv output_tensor src assertEqual output_tensor expected_tensor profiler_ctx None backend = dist get_backend backend SEND_RECV_PROFILING_SUPPORTED_BACKENDS event_name f backend send f backend recv events = get_profiling_event event_name prof Each rank sends recvs all other ranks event_count = sum e count e events expected_event_count = dist get_world_size - assertEqual event_count expected_event_count Event order deterministic so simply assert their shape found following list expected_shapes = rank + rank range dist get_world_size event events assertTrue event is_async assertTrue event input_shapes expected_shapes skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl send recv tested test_send_recv_nccl test_send_recv _test_send_recv profiler_ctx=None skip_but_pass_in_sandcastle_if BACKEND == nccl NCCL send recv tested test_send_recv_nccl test_send_recv_autograd_profiler autograd_profiler_ctx = _create_autograd_profiler _test_send_recv profiler_ctx=autograd_profiler_ctx skip_but_pass_in_sandcastle_if BACKEND == nccl NCCL send recv tested test_send_recv_nccl skip_but_pass_in_sandcastle_if IS_FBCODE Kineto fbcode causes hang skip_but_pass_in_sandcastle_if IS_MACOS IS_WINDOWS torch profiler enabled mac windows https github com pytorch pytorch pull test_send_recv_torch_profiler torch_profiler_ctx = _create_torch_profiler _test_send_recv profiler_ctx=torch_profiler_ctx SEND RECV ANY SOURCE _test_send_recv_any_source profiler_ctx rank = dist get_rank send_recv_size = tensor = _build_tensor send_recv_size value=rank recv_ranks = irecv_ranks = ctx = profiler_ctx profiler_ctx None nullcontext ctx prof dst range dist get_world_size dst == rank Recv mode dst range dist get_world_size dst == rank continue recv recv irecv output_tensor = _build_tensor send_recv_size value=- recv == recv sender = dist recv output_tensor recv_ranks append sender recv == irecv work = dist irecv output_tensor work wait sender = work _source_rank irecv_ranks append sender Assert scalar value sender should equal rank sender equal all values received tensor assertTrue output_tensor eq sender all Send mode dist send tensor dst recv dist send tensor dst irecv profiler_ctx None backend = dist get_backend backend SEND_RECV_PROFILING_SUPPORTED_BACKENDS event_name f backend send f backend recvAnySource events = get_profiling_event event_name prof Each rank sends recvs other rank twice assertEqual sum event count event events dist get_world_size - event events assertTrue event is_async assertEqual event input_shapes send_recv_size Each rank would have world_size - sends verify globally we receive same amount other end recv_ranks_tensor = torch cat torch tensor recv_ranks torch tensor irecv_ranks global_recv_ranks = torch empty_like recv_ranks_tensor _ range dist get_world_size dist all_gather global_recv_ranks recv_ranks_tensor global_recv_ranks_list = tensor global_recv_ranks global_recv_ranks_list += tensor tolist itertools groupby global_recv_ranks_list sort frequency = len list group key group groupby global_recv_ranks_list assertEqual dist get_world_size len frequency assertEqual dist get_world_size - dist get_world_size frequency _barrier skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective sendrecv anysource f BACKEND does support send recv any source test_send_recv_any_source _test_send_recv_any_source profiler_ctx=None skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective sendrecv anysource f BACKEND does support send recv any source test_send_recv_any_source_autograd_profiler autograd_profiler_ctx = _create_autograd_profiler _test_send_recv_any_source profiler_ctx=autograd_profiler_ctx skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective sendrecv anysource f BACKEND does support send recv any source skip_but_pass_in_sandcastle_if IS_FBCODE Kineto fbcode code causes hang skip_but_pass_in_sandcastle_if IS_MACOS IS_WINDOWS torch profiler enabled mac windows https github com pytorch pytorch pull test_send_recv_any_source_torch_profiler torch_profiler_ctx = _create_torch_profiler _test_send_recv_any_source profiler_ctx=torch_profiler_ctx SEND RECV WITH TAG _test_send_recv_with_tag profiler_ctx rank = dist get_rank world_size = dist get_world_size send_recv_size = tensor = _build_tensor send_recv_size value=rank ctx = profiler_ctx profiler_ctx None nullcontext ctx prof dst range world_size dst == rank Recv mode src range world_size src == rank continue output_tensor = _build_tensor send_recv_size value=- dist recv output_tensor src tag=src assertTrue output_tensor eq src all Send mode dist send tensor dst tag=rank profiler_ctx None backend = dist get_backend backend SEND_RECV_PROFILING_SUPPORTED_BACKENDS event_name f backend send f backend recv events = get_profiling_event event_name prof Each rank sends recvs all other ranks event_count = sum e count e events expected_event_count = dist get_world_size - assertEqual event_count expected_event_count event events assertTrue event is_async assertEqual event name event_name assertEqual event input_shapes send_recv_size skip_but_pass_in_sandcastle_if BACKEND == nccl NCCL send recv tested test_send_recv_nccl test_send_recv_with_tag _test_send_recv_with_tag profiler_ctx=None skip_but_pass_in_sandcastle_if BACKEND == nccl NCCL send recv tested test_send_recv_nccl test_send_recv_with_tag_autograd_profiler autograd_profiler_ctx = _create_autograd_profiler _test_send_recv_with_tag profiler_ctx=autograd_profiler_ctx skip_but_pass_in_sandcastle_if BACKEND == nccl NCCL send recv tested test_send_recv_nccl skip_but_pass_in_sandcastle_if IS_FBCODE Kineto fbcode code causes hang skip_but_pass_in_sandcastle_if IS_MACOS IS_WINDOWS torch profiler enabled mac windows https github com pytorch pytorch pull test_send_recv_with_tag_torch_profiler torch_profiler_ctx = _create_torch_profiler _test_send_recv_with_tag profiler_ctx=torch_profiler_ctx ISEND _test_isend profiler_ctx rank = dist get_rank world_size = dist get_world_size ctx = profiler_ctx profiler_ctx None nullcontext ctx prof rank == requests = dist isend _build_tensor dest dest dest range world_size request requests request wait assertTrue request is_completed tensor = _build_tensor rank - dist recv tensor assertEqual tensor _build_tensor rank _barrier profiler_ctx None backend = dist get_backend backend SEND_RECV_PROFILING_SUPPORTED_BACKENDS expected_event_name = f backend send rank == f backend recv events = get_profiling_event expected_event_name prof event_count = sum e count e events expected_count = dist get_world_size - rank == assertEqual expected_count event_count Event ordering guaranteed so simply ensure shapes found following map expected_shapes = r r r range dist get_world_size event events assertTrue event is_async assertEqual event name expected_event_name rank == assertTrue event input_shapes expected_shapes values assertEqual event input_shapes expected_shapes rank skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support isend test_isend _test_isend profiler_ctx=None skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support isend test_isend_autograd_profiler autograd_profiler_ctx = _create_autograd_profiler _test_isend profiler_ctx=autograd_profiler_ctx skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support isend skip_but_pass_in_sandcastle_if IS_FBCODE Kineto fbcode code causes hang skip_but_pass_in_sandcastle_if IS_MACOS IS_WINDOWS torch profiler enabled mac windows https github com pytorch pytorch pull test_isend_torch_profiler torch_profiler_ctx = _create_torch_profiler _test_isend profiler_ctx=torch_profiler_ctx IRECV skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support irecv test_irecv rank = dist get_rank world_size = dist get_world_size rank == expected_tensors = _build_tensor src - src range world_size requests = dist irecv expected_tensors src - src src range world_size src range world_size requests src - wait assertTrue requests src - is_completed assertEqual expected_tensors src - _build_tensor src tensor = _build_tensor rank dist send tensor _barrier BROADCAST _test_broadcast_helper group group_id rank cuda=False rank_to_GPU=None with_options=False dtype value requires_cuda torch float - e- False torch double - e- False torch half - True torch int - False torch uint False torch int - e False torch long - e False requires_cuda cuda continue src group expected_tensor = _build_tensor src + value dtype cuda expected_tensor = expected_tensor cuda rank_to_GPU rank rank == src with_options opts = dist BroadcastOptions opts rootTensor = opts rootRank = src call_dist_op broadcast True group_id broadcast expected_tensor opts call_dist_op broadcast False dist broadcast expected_tensor src group_id tensor = _build_tensor src + - dtype cuda tensor = tensor cuda rank_to_GPU rank with_options opts = dist BroadcastOptions opts rootTensor = opts rootRank = src call_dist_op broadcast True group_id broadcast tensor opts call_dist_op broadcast False dist broadcast tensor src group_id assertEqual tensor size expected_tensor size assertEqual tensor ne expected_tensor max torch tensor False _barrier skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_broadcast group group_id rank = _init_global_test _test_broadcast_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND = gloo BACKEND = nccl Only Gloo Nccl backend supports CUDA allReduce skip_if_no_gpu test_broadcast_cuda group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND device_id = rank_to_GPU rank torch cuda set_device device_id _test_broadcast_helper group group_id rank True rank_to_GPU skip_if_small_worldsize skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_broadcast_group group group_id rank = _init_group_test _test_broadcast_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_broadcast_full_group group group_id rank = _init_full_group_test _test_broadcast_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND = nccl Only NCCL backend supports high priority stream skip_if_no_gpu test_nccl_high_priority_stream group _ rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND device_id = rank_to_GPU rank torch cuda set_device device_id new_port = str MASTER_PORT + os environ MASTER_PORT = new_port gen_iterator = dist rendezvous env rank dist get_world_size store rank size = next gen_iterator store = dist PrefixStore new_port store opts = dist ProcessGroupNCCL Options opts is_high_priority_stream = False group_id = dist ProcessGroupNCCL store rank size opts _test_broadcast_helper group group_id rank True rank_to_GPU True REDUCE _test_reduce_helper group group_id rank op master_value worker_value expected_value cuda=False rank_to_GPU=None src group tensor = _build_tensor src + fill_ master_value rank == src worker_value cuda tensor = tensor cuda rank_to_GPU rank call_dist_op reduce False dist reduce tensor src op group_id tensor_shapes= tensor shape rank == src assertEqual tensor _build_tensor src + expected_value _barrier skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce test_reduce_sum group group_id rank = _init_global_test _test_reduce_helper group group_id rank dist ReduceOp SUM + len group - skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA reduce skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce skip_if_no_gpu test_reduce_sum_cuda group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND device_id = rank_to_GPU rank torch cuda set_device device_id _test_reduce_helper group group_id rank dist ReduceOp SUM + len group - True rank_to_GPU skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce test_reduce_product group group_id rank = _init_global_test _test_reduce_helper group group_id rank dist ReduceOp PRODUCT reduce operator mul len group - skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce test_reduce_min group group_id rank = _init_global_test _test_reduce_helper group group_id rank dist ReduceOp MIN skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce test_reduce_max group group_id rank = _init_global_test _test_reduce_helper group group_id rank dist ReduceOp MAX - skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce skip_if_small_worldsize test_reduce_group_sum group group_id rank = _init_group_test _test_reduce_helper group group_id rank dist ReduceOp SUM + len group - skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce skip_if_small_worldsize test_reduce_group_product group group_id rank = _init_group_test _test_reduce_helper group group_id rank dist ReduceOp PRODUCT reduce operator mul len group - skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce skip_if_small_worldsize test_reduce_group_min group group_id rank = _init_group_test _test_reduce_helper group group_id rank dist ReduceOp MIN skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce skip_if_small_worldsize test_reduce_group_max group group_id rank = _init_group_test _test_reduce_helper group group_id rank dist ReduceOp MAX - skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce test_reduce_full_group_sum group group_id rank = _init_full_group_test _test_reduce_helper group group_id rank dist ReduceOp SUM + len group - skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce test_reduce_full_group_product group group_id rank = _init_full_group_test _test_reduce_helper group group_id rank dist ReduceOp PRODUCT reduce operator mul len group - skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce test_reduce_full_group_min group group_id rank = _init_full_group_test _test_reduce_helper group group_id rank dist ReduceOp MIN skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce test_reduce_full_group_max group group_id rank = _init_full_group_test _test_reduce_helper group group_id rank dist ReduceOp MAX - REDUCE TWICE _test_reduce_twice_helper group group_id rank op master_value worker_value expected_value cuda=False rank_to_GPU=None src group tensors = _build_tensor src + fill_ master_value rank == src worker_value i range cuda i range tensors i = tensors i cuda rank_to_GPU rank call_dist_op reduce False dist reduce tensors src op group_id secondary_op_call=lambda dist reduce tensors src op group_id tensor_shapes= tensors shape rank == src tensor tensors assertEqual tensor _build_tensor src + expected_value _barrier skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce test_reduce_sum_twice group group_id rank = _init_global_test _test_reduce_twice_helper group group_id rank dist ReduceOp SUM + len group - skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA reduce skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce skip_if_no_gpu test_reduce_sum_cuda_twice group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND device_id = rank_to_GPU rank torch cuda set_device device_id _test_reduce_twice_helper group group_id rank dist ReduceOp SUM + len group - True rank_to_GPU skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports reduce_scatter_v skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective reduce f BACKEND does support reduce skip_if_no_gpu test_reduce_scatter_v_cuda _barrier group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND device_id = rank_to_GPU rank input_split_sizes = src + src group start_len = sum input_split_sizes rank end_len = start_len + input_split_sizes rank sum_len = sum input_split_sizes master_value = worker_value = async_val True False tensor = _build_tensor sum_len worker_value device_id=device_id tensor start_len end_len fill_ master_value out_tensor = torch empty input_split_sizes rank sum_len sum_len dtype=torch float fill_ - cuda device_id req = dist reduce_scatter out_tensor list torch split tensor input_split_sizes dist ReduceOp SUM group_id async_val async_val req wait expected_value = + len group - expected_tensor = torch empty input_split_sizes rank sum_len sum_len dtype=torch float expected_tensor = expected_tensor fill_ expected_value cuda device_id assertEqual out_tensor expected_tensor _barrier Test reduce_scatter_tensor accepting single tensor input _reduce_scatter_tensor_helper tensor_out tensor_in group_id rank cuda=True rank_to_GPU=None cuda tensor_in = tensor_in cuda rank_to_GPU rank tensor_out = tensor_out cuda rank_to_GPU rank tensor_shapes = tensor_out shape call_dist_op reduce_scatter_tensor False dist reduce_scatter_tensor tensor_out tensor_in dist ReduceOp SUM group_id False expect_event=False tensor_shapes=tensor_shapes tensor_out skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA reduce_scatter_tensor skip_if_no_gpu test_reduce_scatter_tensor_cuda group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND size = tensor_out = torch zeros size dtype=torch int Concatenated input tensor_in = torch arange len group size tensor_out = _reduce_scatter_tensor_helper tensor_out tensor_in group_id rank True rank_to_GPU Check result expected_tensor = torch arange rank size rank + size len group assertEqual tensor_out expected_tensor _barrier Stacked input tensor_in = torch reshape tensor_in len group size tensor_out = _reduce_scatter_tensor_helper tensor_out tensor_in group_id rank True rank_to_GPU Check result Should same result concatenated case assertEqual tensor_out expected_tensor _barrier call_dist_op profiling_title_postfix is_async op args expect_event=True secondary_op_call=None profile_cuda=False tensor_shapes=None kwargs op_calls = lambda op args kwargs secondary_op_call None op_calls append secondary_op_call autograd_profiler_ctx = torch autograd profiler profile use_cuda=profile_cuda record_shapes=True TODO move test use torch profiler once kineto issues fixed internally autograd_profiler_ctx works = op_call op_call op_calls is_async work works work wait expect_event dist get_backend PROFILING_SUPPORTED_BACKENDS We only interested backend s implementation dispatcher wrapper events = get_profiling_event dist get_backend + profiling_title_postfix autograd_profiler_ctx DETAIL debug mode can use pg wrapper issues more collectives under hood dist get_debug_level = dist DebugLevel DETAIL assertEqual len events len op_calls e events assertTrue e is_async assertEqual e count assertGreaterEqual e cpu_time Verify tensor shapes given DETAIL debug mode can use pg wrapper issues more collectives under hood tensor_shapes None dist get_debug_level = dist DebugLevel DETAIL assertEqual e input_shapes tensor_shapes f event shape e input_shapes vs tensor tensor_shapes ALL REDUCE _test_all_reduce_helper group group_id rank op master_value worker_value expected_value cuda=False rank_to_GPU=None dtype=torch float async_op=False src group curr_value = master_value rank == src worker_value tensor = _build_tensor src + dtype=dtype fill_ curr_value cuda tensor = tensor cuda rank_to_GPU rank tensor dtype == torch complex tensor_shapes = torch view_as_real tensor shape tensor_shapes = tensor shape call_dist_op all_reduce async_op dist all_reduce tensor op group_id async_op=async_op tensor_shapes=tensor_shapes Currently only Gloo backend has profiling tested CUDA enabled Only run cuda profiling test one rank speed up since running different src_rank does affect correctness src == cuda dist get_backend CUDA_PROFILING_SUPPORTED_BACKENDS call_dist_op all_reduce async_op dist all_reduce tensor op group_id async_op=async_op profile_cuda=True tensor_shapes=tensor_shapes _barrier skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_sum group group_id rank = _init_global_test _test_all_reduce_helper group group_id rank dist ReduceOp SUM + len group - skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_sum_async group group_id rank = _init_global_test _test_all_reduce_helper group group_id rank dist ReduceOp SUM + len group - async_op=True skip_but_pass_in_sandcastle_if BACKEND = gloo BACKEND = nccl Only Gloo NCCL backends will have CUDA allReduce tested skip_if_no_gpu test_all_reduce_sum_cuda torch cuda set_device rank group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_reduce_helper group group_id rank dist ReduceOp SUM + len group - True rank_to_GPU skip_but_pass_in_sandcastle_if BACKEND = gloo BACKEND = nccl Only Gloo NCCL backends will have CUDA allReduce tested skip_if_no_gpu test_all_reduce_sum_cuda_async torch cuda set_device rank group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_reduce_helper group group_id rank dist ReduceOp SUM + len group - True rank_to_GPU async_op=True skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_sum_complex group group_id rank = _init_global_test _test_all_reduce_helper group group_id rank dist ReduceOp SUM complex complex complex + complex len group - dtype=torch cfloat skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_complex_unsupported_ops unsupported_ops = dist ReduceOp MAX dist ReduceOp MIN dist ReduceOp PRODUCT dist ReduceOp BAND dist ReduceOp BOR dist ReduceOp BXOR _group group_id _rank = _init_global_test unsupported_op unsupported_ops assertRaisesRegex ValueError all_reduce does support dist all_reduce _build_tensor dtype=torch cfloat unsupported_op group_id skip_but_pass_in_sandcastle_if BACKEND = gloo BACKEND = nccl Only Gloo NCCL backends will have CUDA allReduce tested skip_if_no_gpu test_all_reduce_sum_cuda_complex torch cuda set_device rank group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_reduce_helper group group_id rank dist ReduceOp SUM complex complex complex + complex len group - True rank_to_GPU dtype=torch cfloat skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_product group group_id rank = _init_global_test _test_all_reduce_helper group group_id rank dist ReduceOp PRODUCT reduce operator mul len group - skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_min group group_id rank = _init_global_test _test_all_reduce_helper group group_id rank dist ReduceOp MIN skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_max group group_id rank = _init_global_test _test_all_reduce_helper group group_id rank dist ReduceOp MAX - skip_if_small_worldsize skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_group_sum group group_id rank = _init_group_test _test_all_reduce_helper group group_id rank dist ReduceOp SUM + len group - skip_if_small_worldsize skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_group_product group group_id rank = _init_group_test _test_all_reduce_helper group group_id rank dist ReduceOp PRODUCT reduce operator mul len group - skip_if_small_worldsize skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_group_min group group_id rank = _init_group_test _test_all_reduce_helper group group_id rank dist ReduceOp MIN skip_if_small_worldsize skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_group_max group group_id rank = _init_group_test _test_all_reduce_helper group group_id rank dist ReduceOp MAX - skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_full_group_sum group group_id rank = _init_full_group_test _test_all_reduce_helper group group_id rank dist ReduceOp SUM + len group - skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_full_group_product group group_id rank = _init_full_group_test _test_all_reduce_helper group group_id rank dist ReduceOp PRODUCT reduce operator mul len group - skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_full_group_min group group_id rank = _init_full_group_test _test_all_reduce_helper group group_id rank dist ReduceOp MIN skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_full_group_max group group_id rank = _init_full_group_test _test_all_reduce_helper group group_id rank dist ReduceOp MAX - SPARSE ALL REDUCE _test_sparse_all_reduce_sum fn _group group_id rank = _init_global_test tests = simple_sparse_reduce_tests rank dist get_world_size num_inputs= inputs outputs tests tensors = fn input input inputs dist all_reduce tensors dist ReduceOp SUM group_id assertEqual tensors outputs skip_but_pass_in_sandcastle_if BACKEND = gloo Only Gloo backend support sparse all reduce test_sparse_all_reduce_sum _test_sparse_all_reduce_sum lambda t t skip_but_pass_in_sandcastle_if BACKEND = gloo Only Gloo backend support sparse all reduce skip_if_no_gpu test_sparse_all_reduce_sum_cuda _test_sparse_all_reduce_sum lambda t t clone cuda ALL REDUCE - COALESCED staticmethod _all_reduce_coalesced_sum_test_cases group_size complex complex + group_size - + group_size - complex + complex group_size - torch float torch float torch cfloat staticmethod _all_reduce_coalesced_product_test_cases group_size group_size - group_size - torch float torch float staticmethod _all_reduce_coalesced_min_test_cases group_size torch float torch float staticmethod _all_reduce_coalesced_max_test_cases group_size torch float torch float skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_reduce_coalesced_max_complex_unsupported _group group_id _rank = _init_global_test assertRaisesRegex ValueError all_reduce does support dist all_reduce_coalesced _build_tensor dtype=torch cfloat dist ReduceOp MAX group_id _test_all_reduce_coalesced_helper group group_id rank op cuda=False rank_to_GPU=None test_case_func = dist ReduceOp SUM _all_reduce_coalesced_sum_test_cases dist ReduceOp PRODUCT _all_reduce_coalesced_product_test_cases dist ReduceOp MIN _all_reduce_coalesced_min_test_cases dist ReduceOp MAX _all_reduce_coalesced_max_test_cases op master_values worker_values expected_values dtypes = test_case_func len group src group curr_values = master_values rank == src worker_values tensors = _build_tensor src + val dtype=dtype dtype val zip dtypes curr_values strict=True cuda tensors = t cuda rank_to_GPU rank t tensors tensor_shapes = tensor tensors tensor dtype == torch complex tensor_shapes append torch view_as_real tensor shape tensor_shapes append tensor shape call_dist_op all_reduce False dist all_reduce_coalesced tensors op group_id tensor_shapes=tensor_shapes expected_tensors = _build_tensor src + expected_value dtype=dtype dtype expected_value zip dtypes expected_values strict=True assertEqual tensors expected_tensors _barrier require_backend_is_available gloo test_all_reduce_coalesced_sum group group_id rank = _init_global_test _test_all_reduce_coalesced_helper group group_id rank dist ReduceOp SUM cuda=False rank_to_GPU=None require_backend_is_available gloo test_all_reduce_coalesced_product group group_id rank = _init_global_test _test_all_reduce_coalesced_helper group group_id rank dist ReduceOp PRODUCT cuda=False rank_to_GPU=None require_backend_is_available gloo test_all_reduce_coalesced_min group group_id rank = _init_global_test _test_all_reduce_coalesced_helper group group_id rank dist ReduceOp MIN cuda=False rank_to_GPU=None require_backend_is_available gloo test_all_reduce_coalesced_max group group_id rank = _init_global_test _test_all_reduce_coalesced_helper group group_id rank dist ReduceOp MAX cuda=False rank_to_GPU=None skip_if_small_worldsize require_backend_is_available gloo test_all_reduce_coalesced_group_sum group group_id rank = _init_group_test _test_all_reduce_coalesced_helper group group_id rank dist ReduceOp SUM cuda=False rank_to_GPU=None skip_if_small_worldsize require_backend_is_available gloo test_all_reduce_coalesced_group_product group group_id rank = _init_group_test _test_all_reduce_coalesced_helper group group_id rank dist ReduceOp PRODUCT cuda=False rank_to_GPU=None skip_if_small_worldsize require_backend_is_available gloo test_all_reduce_coalesced_group_min group group_id rank = _init_group_test _test_all_reduce_coalesced_helper group group_id rank dist ReduceOp MIN cuda=False rank_to_GPU=None skip_if_small_worldsize require_backend_is_available gloo test_all_reduce_coalesced_group_max group group_id rank = _init_group_test _test_all_reduce_coalesced_helper group group_id rank dist ReduceOp MAX cuda=False rank_to_GPU=None require_backend_is_available gloo test_all_reduce_coalesced_full_group_sum group group_id rank = _init_full_group_test _test_all_reduce_coalesced_helper group group_id rank dist ReduceOp SUM cuda=False rank_to_GPU=None require_backend_is_available gloo test_all_reduce_coalesced_full_group_product group group_id rank = _init_full_group_test _test_all_reduce_coalesced_helper group group_id rank dist ReduceOp PRODUCT cuda=False rank_to_GPU=None require_backend_is_available gloo test_all_reduce_coalesced_full_group_min group group_id rank = _init_full_group_test _test_all_reduce_coalesced_helper group group_id rank dist ReduceOp MIN cuda=False rank_to_GPU=None require_backend_is_available gloo test_all_reduce_coalesced_full_group_max group group_id rank = _init_full_group_test _test_all_reduce_coalesced_helper group group_id rank dist ReduceOp MAX cuda=False rank_to_GPU=None SCATTER _test_scatter_helper group group_id rank cuda=False rank_to_GPU=None dtype=torch float dest group tensor = _build_tensor dest + - dtype=dtype expected_tensor = _build_tensor dest + rank dtype=dtype tensors = _build_tensor dest + i dtype=dtype i group rank == dest cuda tensor = tensor cuda rank_to_GPU rank tensors = t cuda rank_to_GPU rank t tensors dtype == torch complex tensor_shapes = torch view_as_real t shape t tensors tensor_shapes = t shape t tensors call_dist_op scatter False dist scatter tensor src=dest scatter_list=tensors group=group_id expect_event=False tensor_shapes=tensor_shapes assertEqual tensor expected_tensor _barrier skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND == ucc CPU tensor ops supported UCP TL test_scatter_checks group _group_id rank = _init_global_test one = torch ones Specify scatter_list argument only source rank output = one clone - rank == scatter_list = one clone i i group dist scatter output src= scatter_list=scatter_list dist scatter output src= assertEqual output one rank Don t specify src argument output = one clone - rank == scatter_list = one clone i i group dist scatter output scatter_list=scatter_list dist scatter output assertEqual output one rank skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND == ucc CPU tensor ops supported UCP TL test_scatter group group_id rank = _init_global_test _test_scatter_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA gather skip_if_no_gpu test_scatter_cuda group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_scatter_helper group group_id rank True rank_to_GPU skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND == ucc CPU tensor ops supported UCP TL test_scatter_complex group group_id rank = _init_global_test _test_scatter_helper group group_id rank dtype=torch cfloat skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA gather skip_if_no_gpu test_scatter_cuda_complex group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_scatter_helper group group_id rank True rank_to_GPU dtype=torch cfloat skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND == ucc CPU tensor ops supported UCP TL skip_if_small_worldsize test_scatter_group group group_id rank = _init_group_test _test_scatter_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND == ucc CPU tensor ops supported UCP TL test_scatter_full_group group group_id rank = _init_full_group_test _test_scatter_helper group group_id rank GATHER _test_gather_helper group group_id rank cuda=False rank_to_GPU=None dest group tensor = _build_tensor dest + rank tensors = _build_tensor dest + - i group rank == dest cuda tensor = tensor cuda rank_to_GPU rank tensors = t cuda rank_to_GPU rank t tensors call_dist_op gather False dist gather tensor dst=dest gather_list=tensors group=group_id expect_event=False tensor_shapes= tensors shape len tensors None rank == dest expected_tensors = _build_tensor dest + i i group t t zip tensors expected_tensors strict=True assertEqual t t _barrier skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND == ucc CPU tensor ops supported UCP TL test_gather_checks group _group_id rank = _init_global_test one = torch ones Specify gather_list argument only destination rank rank == gather_list = one clone _ group dist gather one rank dst= gather_list=gather_list i group assertEqual gather_list i one i dist gather one rank dst= Don t specify dst argument rank == gather_list = one clone _ group dist gather one rank gather_list=gather_list i group assertEqual gather_list i one i dist gather one rank skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND == ucc CPU tensor ops supported UCP TL test_gather group group_id rank = _init_global_test _test_gather_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA gather skip_if_no_gpu test_gather_cuda group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_gather_helper group group_id rank True rank_to_GPU skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND == ucc CPU tensor ops supported UCP TL skip_if_small_worldsize test_gather_group group group_id rank = _init_group_test _test_gather_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors skip_but_pass_in_sandcastle_if BACKEND == ucc CPU tensor ops supported UCP TL test_gather_full_group group group_id rank = _init_full_group_test _test_gather_helper group group_id rank ALL GATHER _test_all_gather_helper group group_id rank cuda=False rank_to_GPU=None dtype=torch float dest group tensor = _build_tensor dest + rank dtype=dtype tensors = _build_tensor dest + - dtype=dtype i group allgather = dist all_gather cuda tensor = tensor cuda rank_to_GPU rank tensors = t cuda rank_to_GPU rank t tensors tensors dtype == torch complex tensor_shapes = torch view_as_real tensors shape tensor_shapes = tensors shape call_dist_op all_gather False allgather tensors tensor group_id False tensor_shapes=tensor_shapes expected_tensors = _build_tensor dest + i dtype=dtype i group t t zip tensors expected_tensors strict=True assertEqual t t _barrier skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_gather group group_id rank = _init_global_test _test_all_gather_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA all gather skip_if_no_gpu test_all_gather_cuda group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_gather_helper group group_id rank True rank_to_GPU skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_gather_complex group group_id rank = _init_global_test _test_all_gather_helper group group_id rank dtype=torch cfloat skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA all gather skip_if_no_gpu test_all_gather_cuda_complex group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_gather_helper group group_id rank True rank_to_GPU dtype=torch cfloat skip_if_small_worldsize skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_gather_group group group_id rank = _init_group_test _test_all_gather_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND == nccl Nccl does support CPU tensors test_all_gather_full_group group group_id rank = _init_full_group_test _test_all_gather_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports all_gather_v skip_if_no_gpu test_all_gather_v_cuda _barrier group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND device_id = rank_to_GPU rank output_split_sizes = dst + dst group sum_len = sum output_split_sizes value = async_val True False tensor = torch empty output_split_sizes rank sum_len sum_len dtype=torch float fill_ value cuda device_id out_tensor = _build_tensor sum_len - device_id=device_id req = dist all_gather list torch split out_tensor output_split_sizes tensor group_id async_val async_val req wait expected_value = value expected_tensor = _build_tensor sum_len expected_value device_id=device_id assertEqual out_tensor expected_tensor _barrier Test all_gather accepting single tensor output _all_gather_into_tensor_helper tensor_out tensor_in group_id rank cuda=True rank_to_GPU=None cuda tensor_in = tensor_in cuda rank_to_GPU rank tensor_out = tensor_out cuda rank_to_GPU rank tensor_out dtype == torch complex tensor_shapes = torch view_as_real tensor_in shape tensor_shapes = tensor_in shape call_dist_op all_gather_into_tensor False dist all_gather_into_tensor tensor_out tensor_in group_id False expect_event=False tensor_shapes=tensor_shapes tensor_out skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA all_gather_into_tensor skip_if_no_gpu test_all_gather_into_cat_tensor_cuda group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND size = tensor_in = torch ones size size rank Concatenated output tensor_out = torch ones len group size size - tensor_out = _all_gather_into_tensor_helper tensor_out tensor_in group_id rank True rank_to_GPU Check result Concatenate all blocks into bigger tensor expected_tensor = torch cat torch ones size size i i group assertEqual tensor_out expected_tensor _barrier skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA all_gather_into_tensor skip_if_no_gpu test_all_gather_into_stack_tensor_cuda group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND size = tensor_in = torch ones size size rank Stacked output tensor_out = torch ones len group size size - tensor_out = _all_gather_into_tensor_helper tensor_out tensor_in group_id rank True rank_to_GPU Check result Stack all blocks into bigger tensor expected_tensor = torch stack torch ones size size i i group assertEqual tensor_out expected_tensor _barrier _run_all_gather_coalesced_and_verify output_tensor_lists input_tensors expected_tensors group_id Helper runs all_gather_coalesced returns true output matches expectations tensor_shapes = input_tensor input_tensors input_tensor dtype == torch complex tensor_shapes append torch view_as_real input_tensor shape tensor_shapes append input_tensor shape call_dist_op all_gather False dist all_gather_coalesced output_tensor_lists input_tensors group_id tensor_shapes=tensor_shapes l l zip output_tensor_lists expected_tensors strict=True t t zip l l strict=True torch equal t t False True _test_all_gather_coalesced_helper group group_id rank dtype=torch float TODO Instead we should probably go through _rank_not_in_group mechanism disable sending tensors group_id None test_case_id range Make sure we create tensors incompatible sizes e g x x x sent one batch input_tensors = _build_multidim_tensor tensor_id tensor_id rank + tensor_id dtype=dtype tensor_id range test_case_id output_tensor_lists = _build_multidim_tensor tensor_id tensor_id - dtype=dtype tensor_id range test_case_id _ group expected_tensors = _build_multidim_tensor tensor_id tensor_id rank_iter + tensor_id dtype=dtype tensor_id range test_case_id rank_iter group assert _run_all_gather_coalesced_and_verify output_tensor_lists input_tensors expected_tensors group_id output tensors do match expected outputs _barrier skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective allgather_coalesced f BACKEND does support all_gather_coalesced test_all_gather_coalesced_simple group group_id rank = _init_global_test _test_all_gather_coalesced_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective allgather_coalesced f BACKEND does support all_gather_coalesced test_all_gather_coalesced_complex group group_id rank = _init_global_test _test_all_gather_coalesced_helper group group_id rank dtype=torch cfloat skip_if_small_worldsize skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective allgather_coalesced f BACKEND does support all_gather_coalesced test_all_gather_coalesced_group group group_id rank = _init_group_test _test_all_gather_coalesced_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective allgather_coalesced f BACKEND does support all_gather_coalesced test_all_gather_coalesced_full_group group group_id rank = _init_full_group_test _test_all_gather_coalesced_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective allgather_coalesced f BACKEND does support all_gather_coalesced test_all_gather_coalesced_with_empty group group_id rank = _init_global_test input_tensors = rank torch ones torch ones rank + torch ones torch ones torch ones output_tensors_lists = - torch ones - torch ones - torch ones - torch ones - torch ones _ group expected_tensors = r torch ones torch ones r + torch ones torch ones torch ones r group assert _run_all_gather_coalesced_and_verify output_tensors_lists input_tensors expected_tensors group_id _barrier AllToAll _test_all_to_all_single_equal_split_helper group group_id rank cuda=False rank_to_GPU=None dtype=torch float group_id None size = len group in_tensor = torch ones size size dtype=dtype rank expected_tensor = torch cat torch ones size dtype=dtype i i group out_tensor = torch ones size size dtype=dtype - cuda in_tensor = in_tensor cuda rank_to_GPU rank expected_tensor = expected_tensor cuda rank_to_GPU rank out_tensor = out_tensor cuda rank_to_GPU rank dtype == torch complex tensor_shapes = torch view_as_real in_tensor shape tensor_shapes = in_tensor shape call_dist_op all_to_all False dist all_to_all_single out_tensor in_tensor group=group_id tensor_shapes=tensor_shapes assertEqual out_tensor expected_tensor _barrier _test_all_to_all_single_unequal_split_helper group group_id rank cuda=False rank_to_GPU=None dtype=torch float group_id None size = len group in_splits = i + i group out_splits = rank + _ group in_tensor = torch ones sum in_splits size dtype=dtype rank out_tensor = torch ones rank + size size dtype=dtype expected_tensor = torch cat torch ones rank + size dtype=dtype i i group cuda in_tensor = in_tensor cuda rank_to_GPU rank expected_tensor = expected_tensor cuda rank_to_GPU rank out_tensor = out_tensor cuda rank_to_GPU rank dist all_to_all_single out_tensor in_tensor out_splits in_splits group=group_id assertEqual out_tensor expected_tensor _barrier _test_all_to_all_helper group group_id rank cuda=False rank_to_GPU=None dtype=torch float group_id None size = len group in_splits = i + i group in_tensors = torch ones in_splits i size dtype=dtype rank i _ enumerate group out_tensors = torch ones rank + size dtype=dtype _ group expected_tensors = torch ones rank + size dtype=dtype i i group cuda in_tensors = t cuda rank_to_GPU rank t in_tensors expected_tensors = t cuda rank_to_GPU rank t expected_tensors out_tensors = t cuda rank_to_GPU rank t out_tensors dist all_to_all out_tensors in_tensors group=group_id t t zip out_tensors expected_tensors strict=True assertEqual t t _barrier skip_but_pass_in_sandcastle_if BACKEND = mpi Only MPI supports CPU all_to_all_single test_all_to_all_single_equal_split group group_id rank = _init_global_test _test_all_to_all_single_equal_split_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA all_to_all_single skip_if_no_gpu test_all_to_all_single_equal_split_cuda group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_to_all_single_equal_split_helper group group_id rank True rank_to_GPU skip_but_pass_in_sandcastle_if BACKEND = mpi Only MPI supports CPU all_to_all_single test_all_to_all_single_equal_split_complex group group_id rank = _init_global_test _test_all_to_all_single_equal_split_helper group group_id rank dtype=torch cfloat skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA all_to_all_single skip_if_no_gpu test_all_to_all_single_equal_split_cuda_complex group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_to_all_single_equal_split_helper group group_id rank True rank_to_GPU dtype=torch cfloat skip_but_pass_in_sandcastle_if BACKEND = mpi Only MPI supports CPU all_to_all_single test_all_to_all_single_unequal_split group group_id rank = _init_global_test _test_all_to_all_single_unequal_split_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA all_to_all_single skip_if_no_gpu test_all_to_all_single_unequal_split_cuda group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_to_all_single_unequal_split_helper group group_id rank True rank_to_GPU skip_but_pass_in_sandcastle_if BACKEND = mpi Only MPI supports CPU all_to_all_single test_all_to_all_single_unequal_split_complex group group_id rank = _init_global_test _test_all_to_all_single_unequal_split_helper group group_id rank dtype=torch cfloat skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA all_to_all_single skip_if_no_gpu test_all_to_all_single_unequal_split_cuda_complex group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_to_all_single_unequal_split_helper group group_id rank True rank_to_GPU dtype=torch cfloat skip_but_pass_in_sandcastle_if BACKEND = mpi Only MPI supports all_to_all test_all_to_all group group_id rank = _init_global_test _test_all_to_all_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND = nccl Only NCCL supports CUDA all_to_all skip_if_rocm_multiprocess test_all_to_all_cuda group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_to_all_helper group group_id rank True rank_to_GPU skip_but_pass_in_sandcastle_if BACKEND = mpi Only MPI supports all_to_all test_all_to_all_complex group group_id rank = _init_global_test _test_all_to_all_helper group group_id rank dtype=torch cfloat skip_but_pass_in_sandcastle_if BACKEND = nccl Only NCCL supports CUDA all_to_all skip_if_rocm_multiprocess test_all_to_all_cuda_complex group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_to_all_helper group group_id rank True rank_to_GPU dtype=torch cfloat skip_but_pass_in_sandcastle_if BACKEND = mpi Only MPI supports CPU all_to_all_single skip_if_small_worldsize test_all_to_all_single_equal_split_group group group_id rank = _init_group_test _test_all_to_all_single_equal_split_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA all_to_all_single skip_if_no_gpu skip_if_small_worldsize test_all_to_all_single_equal_split_group_cuda group group_id rank = _init_group_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_to_all_single_equal_split_helper group group_id rank True rank_to_GPU skip_but_pass_in_sandcastle_if BACKEND = mpi Only MPI supports CPU all_to_all_single skip_if_small_worldsize test_all_to_all_single_unequal_split_group group group_id rank = _init_group_test _test_all_to_all_single_unequal_split_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA all_to_all_single skip_if_no_gpu skip_if_small_worldsize test_all_to_all_single_unequal_split_group_cuda group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_to_all_single_unequal_split_helper group group_id rank True rank_to_GPU skip_but_pass_in_sandcastle_if BACKEND = mpi Only MPI supports all_to_all skip_if_small_worldsize test_all_to_all_group group group_id rank = _init_group_test _test_all_to_all_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA all_to_all_single skip_if_small_worldsize skip_if_rocm_multiprocess test_all_to_all_group_cuda group group_id rank = _init_group_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_to_all_helper group group_id rank True rank_to_GPU skip_but_pass_in_sandcastle_if BACKEND = mpi Only MPI supports CPU all_to_all_single test_all_to_all_single_equal_split_full_group group group_id rank = _init_full_group_test _test_all_to_all_single_equal_split_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA all_to_all_single skip_if_no_gpu test_all_to_all_single_equal_split_full_group_cuda group group_id rank = _init_full_group_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_to_all_single_equal_split_helper group group_id rank True rank_to_GPU skip_but_pass_in_sandcastle_if BACKEND = mpi Only MPI supports CPU all_to_all_single test_all_to_all_single_unequal_split_full_group group group_id rank = _init_full_group_test _test_all_to_all_single_unequal_split_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND = nccl Only Nccl supports CUDA all_to_all_single skip_if_no_gpu test_all_to_all_single_unequal_split_full_group_cuda group group_id rank = _init_full_group_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_to_all_single_unequal_split_helper group group_id rank True rank_to_GPU skip_but_pass_in_sandcastle_if BACKEND = mpi Only MPI supports all_to_all test_all_to_all_full_group group group_id rank = _init_full_group_test _test_all_to_all_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND = nccl Only NCCL supports CUDA all_to_all skip_if_rocm_multiprocess test_all_to_all_full_group_cuda group group_id rank = _init_full_group_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_all_to_all_helper group group_id rank True rank_to_GPU BARRIER _test_barrier_helper group group_id rank cuda=False rank_to_GPU=None WAIT_TIME = seconds dest group expected_time = torch DoubleTensor fill_ cuda expected_time = expected_time cuda rank_to_GPU rank dest == rank expected_time fill_ time time + WAIT_TIME dist broadcast expected_time dest group_id time sleep WAIT_TIME + sleep little bit longer dist barrier group_id dist broadcast expected_time dest group_id dist barrier group_id assertGreaterAlmostEqual float time time float expected_time msg=f destination rank dest d my rank rank d + you see failure please report Use higher timeout instance where test runs against subgroup uses CUDA tensor expected time The CUDA initialization participating processes can take long enough barrier timeout trigger process doesn t participate group _barrier timeout= skip_if_no_gpu skip_but_pass_in_sandcastle_if BACKEND == mpi MPI doesn t supports GPU barrier skip_but_pass_in_sandcastle_if BACKEND == ucc IS_SANDCASTLE Skipped internally test_barrier_cuda group group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_barrier_helper group group_id rank True rank_to_GPU skip_if_small_worldsize skip_if_no_gpu skip_but_pass_in_sandcastle_if BACKEND == mpi MPI doesn t supports GPU barrier test_barrier_group_cuda group group_id rank = _init_group_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_barrier_helper group group_id rank True rank_to_GPU skip_if_small_worldsize skip_if_no_gpu skip_but_pass_in_sandcastle_if BACKEND == mpi MPI doesn t supports GPU barrier test_barrier_full_group_cuda group group_id rank = _init_full_group_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND _test_barrier_helper group group_id rank True rank_to_GPU skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective cpu barrier f BACKEND does support CPU barrier test_barrier group group_id rank = _init_global_test _test_barrier_helper group group_id rank skip_if_small_worldsize skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective cpu barrier f BACKEND does support CPU barrier test_barrier_group group group_id rank = _init_group_test _test_barrier_helper group group_id rank skip_but_pass_in_sandcastle_if BACKEND DistTestCases skip_collective cpu barrier f BACKEND does support CPU barrier test_barrier_full_group group group_id rank = _init_full_group_test _test_barrier_helper group group_id rank _model_step model param model parameters param grad None torch no_grad param += param grad param grad = None _model_step_with_zero_grad model param model parameters param grad None torch no_grad param += param grad param grad requires_grad_ False param grad zero_ _prepare_dummy_data local_bs global_bs DDP should divisible WORLD_SIZE world_size = int os environ WORLD_SIZE global_bs = world_size local_bs input_cpu = torch randn global_bs target = torch randn global_bs loss = nn MSELoss global_bs input_cpu target loss END TO END TEST FOR DISTRIBUTEDDATAPARALLEL _test_DDP_helper model input_var target loss scale_factor= memory_format=None model train output = model input_var l = loss output target scale_factor l backward memory_format None assertTrue output is_contiguous memory_format=memory_format _assert_equal_param param_gpu param_DDP assertEqual len param_gpu len param_DDP p_gpu p_DDP zip param_gpu param_DDP strict=True assertEqual p_gpu p_DDP _test_DDP_niter model_base model_DDP input target loss local_bs rank batch_size test_save offset=None world_size= zero_grad=False memory_format=None n_iter= idx range n_iter single cpu gpu training _test_DDP_helper model_base input target loss memory_format=memory_format offset None offset = rank local_bs DDP training DDP scatters subsets input_cpu nodes GPUs _test_DDP_helper model_DDP input offset offset + local_bs target offset offset + local_bs loss world_size local_bs batch_size world_size = memory_format=memory_format Update weights run second iteration shake out errors zero_grad _model_step_with_zero_grad model_base _model_step_with_zero_grad model_DDP _model_step model_base _model_step model_DDP _assert_equal_param list model_base parameters list model_DDP module parameters Shuffle input so DDP input different input = input torch randperm batch_size save model middle reload test_save idx == INIT_METHOD startswith file tempfile NamedTemporaryFile tmp sys platform == win torch save model_DDP tmp tmp seek weights_only=False legacy code saves model model_DDP = torch load tmp weights_only=False torch save model_DDP tmp name weights_only=False legacy code saves model model_DDP = torch load tmp name weights_only=False tempfile TemporaryFile tmp_file torch save model_DDP tmp_file tmp_file seek weights_only=False legacy code saves model saved_model = torch load tmp_file weights_only=False k model_DDP state_dict assertEqual model_DDP state_dict k saved_model state_dict k _test_DistributedDataParallel gpu_subset rank output_device=None gradient_as_bucket_view=False static_graph=False set_static_graph_twice=False Run simple end end DDP model use result single node model baseline cpu training setup model = Net single gpu training setup model_gpu = copy deepcopy model model_gpu cuda gpu_subset DDP training setup model_DDP = copy deepcopy model model_DDP cuda gpu_subset model_DDP = nn parallel DistributedDataParallel model_DDP device_ids=gpu_subset gradient_as_bucket_view=gradient_as_bucket_view static_graph=static_graph set_static_graph_twice model_DDP _set_static_graph test serializable unserializable tempfile NamedTemporaryFile tmp sys platform == win torch save model_DDP tmp tmp seek weights_only=False legacy code saves model model_DDP = torch load tmp weights_only=False torch save model_DDP tmp name weights_only=False legacy code saves model model_DDP = torch load tmp name weights_only=False dummy data initialization local_bs = len gpu_subset global_bs input_cpu target loss = _prepare_dummy_data local_bs check two model parameters over iterations _test_DDP_niter model_gpu model_DDP input_cpu cuda gpu_subset target cuda gpu_subset loss local_bs rank global_bs True _barrier _test_DistributedDataParallelCPU gradient_as_bucket_view=False Run simple end end DDP-CPU model use result single node model baseline _group _group_id rank = _init_global_test cpu training setup model_base = Net DDP-CPU training setup model_DDP = copy deepcopy model_base model_DDP = nn parallel DistributedDataParallel model_DDP gradient_as_bucket_view=gradient_as_bucket_view dummy data initialization local_bs = global_bs input_cpu target loss = _prepare_dummy_data local_bs check two model parameters over iterations _test_DDP_niter model_base model_DDP input_cpu target loss local_bs rank global_bs False zero_grad=True _barrier model_DDP skip_but_pass_in_sandcastle_if BACKEND == nccl nccl does support DDP CPU models test_DistributedDataParallelCPU _test_DistributedDataParallelCPU skip_but_pass_in_sandcastle_if BACKEND == nccl nccl does support DDP CPU models test_DistributedDataParallelCPU_grad_is_view _test_DistributedDataParallelCPU gradient_as_bucket_view=True skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_DistributedDataParallel_requires_grad module without gradients shouldn t accepted assertRaises RuntimeError lambda nn parallel DistributedDataParallel nn Module _barrier skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_lt_x_gpu int os environ WORLD_SIZE test_ddp_zero_output_features ToyModel nn Module __init__ - None super __init__ net = nn Linear relu = nn ReLU net = nn Linear model = ToyModel rank nn parallel DistributedDataParallel model device_ids= rank skip_but_pass_in_sandcastle_if BACKEND == nccl Gloo-only test test_ddp_create_graph Model nn Module __init__ - None super __init__ p = nn Parameter torch tensor forward p pow model = Model ddp_model = torch nn parallel DistributedDataParallel model _ range Verify DDP doesn t throw when ran create_graph=True Although we do warn about potential issues please see https github com pytorch pytorch issues details ddp_model backward create_graph=True grad tensors should require grad assertTrue all param requires_grad param ddp_model parameters skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_lt_x_gpu int os environ WORLD_SIZE test_DistributedDataParallel_non_default_stream stream = torch cuda Stream rank rank = rank torch cuda stream stream net = torch nn parallel DistributedDataParallel torch nn Linear bias=False cuda rank device_ids= rank i range Clear gradients manually grad = net module weight grad grad None grad requires_grad_ False grad zero_ Forward + BW batch = torch tensor rank float cuda rank loss = net batch sum loss backward For each worker gradient weight should worker_rank grad = net module weight grad avg = grad clone All-reducing gradient averages should give us gradient average If then one workers has correctly written back averaged gradient before all-reduce call dist all_reduce avg world_size = int os environ WORLD_SIZE avg div_ world_size expected_grad = sum i i range world_size world_size assertEqual avg expected_grad msg=f Expected gradient expected_grad got avg rank rank skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature cuda f The BACKEND backend does support DDP communication hook CUDA devices skip_if_lt_x_gpu int os environ WORLD_SIZE test_ddp_comm_hook_logging hooks = default allreduce_hook default fp _compress_hook powerSGD powerSGD_hook powerSGD batched_powerSGD_hook quantization_hooks quantization_pertensor_hook quantization_hooks quantization_perchannel_hook cpp_builtin_hooks = dist BuiltinCommHookType ALLREDUCE dist BuiltinCommHookType FP _COMPRESS hook hooks ddp_model = torch nn parallel DistributedDataParallel torch nn Linear bias=False cuda rank device_ids= rank ddp_logging_data = ddp_model _get_ddp_logging_data Hook registered yet so should empty assertEqual ddp_logging_data get comm_hook None ddp_model register_comm_hook None hook ddp_logging_data = ddp_model _get_ddp_logging_data assertEqual ddp_logging_data get comm_hook hook __qualname__ hook cpp_builtin_hooks ddp_model = torch nn parallel DistributedDataParallel torch nn Linear bias=False cuda rank device_ids= rank ddp_logging_data = ddp_model _get_ddp_logging_data Hook registered yet so should empty assertEqual ddp_logging_data get comm_hook None ddp_model _register_builtin_comm_hook hook ddp_logging_data = ddp_model _get_ddp_logging_data assertEqual ddp_logging_data get comm_hook str hook No hook registered ddp_model = torch nn parallel DistributedDataParallel torch nn Linear bias=False cuda rank device_ids= rank ddp_logging_data = ddp_model _get_ddp_logging_data Hook registered yet so should empty assertEqual ddp_logging_data get comm_hook None After second forward pass hook should still empty string _ range inp = torch ones device=self rank loss = ddp_model inp sum loss backward ddp_logging_data = ddp_model _get_ddp_logging_data Note DETAIL debug mode logs DDP logging data stdout thus accesses std map which fills default value type didn t exist assertEqual ddp_logging_data get comm_hook _test_ddp_hook_with_optimizer_parity grad_as_bucket_view static_graph optim_cls optimize_subset functional_optim_args functional_optim_kwargs rank = rank torch cuda set_device rank torch manual_seed rank torch cuda manual_seed rank models_to_test = LargeNet torch randn cuda HAS_TORCHVISION models_to_test append torchvision models resnet torch randn cuda model inp models_to_test Enable determinism cudnn operators torch backends cudnn flags enabled=True deterministic=True benchmark=False Create DDP model runs optimizer fused fashion ddp_model_with_optimizer_hook = torch nn parallel DistributedDataParallel copy deepcopy model cuda device_ids= rank gradient_as_bucket_view=grad_as_bucket_view static_graph=static_graph Create DDP model no hook does optimizer after backward ddp_model_with_no_hook = torch nn parallel DistributedDataParallel copy deepcopy model cuda device_ids= rank gradient_as_bucket_view=grad_as_bucket_view static_graph=static_graph hook_params = ddp_model_with_optimizer_hook parameters no_hook_params = ddp_model_with_no_hook parameters optimize_subset hook_params = list hook_params no_hook_params = list no_hook_params assertGreater len hook_params hook_params = hook_params no_hook_params = no_hook_params Register fused optimizer will run optimizer step allreduce optimize_subset API where optim_params specified ddp_model_with_optimizer_hook _register_fused_optim optim_cls functional_optim_args optim_params=hook_params functional_optim_kwargs API where optim_params omitted ddp_model_with_optimizer_hook _register_fused_optim optim_cls functional_optim_args functional_optim_kwargs optimizer_no_hook = optim_cls no_hook_params functional_optim_args functional_optim_kwargs Verify parameters equal initially hook_param allreduce_param zip ddp_model_with_optimizer_hook parameters ddp_model_with_no_hook parameters strict=True assertEqual hook_param allreduce_param Save old parameters later verify optimizer modified them opt_hook_init_params = copy deepcopy list ddp_model_with_optimizer_hook parameters Run optimizer hook model _ range ddp_model_with_optimizer_hook zero_grad out = ddp_model_with_optimizer_hook inp loss = out sum loss backward dist barrier Run regular model _ range ddp_model_with_no_hook zero_grad out = ddp_model_with_no_hook inp loss = out sum loss backward optimizer_no_hook step dist barrier Now verify parameters equal hook_param allreduce_param zip ddp_model_with_optimizer_hook parameters ddp_model_with_no_hook parameters strict=True assertEqual hook_param allreduce_param Verify optimizer modified appropriate parameter set otherwise they d trivially equal above optimize_subset assertNotEqual opt_hook_init_params next iter ddp_model_with_optimizer_hook parameters Untouched params should equal assertEqual opt_hook_init_params list ddp_model_with_optimizer_hook parameters assertNotEqual opt_hook_init_params list ddp_model_with_optimizer_hook parameters dist barrier Commenting out following tests they cause Sandcastle jobs fail Failure signature AttributeError type object TestDistBackendWithSpawn has no attribute test_ddp_hook_with_optimizer_parity_adamw torch testing _internal common_utils parametrize skip_but_pass_in_sandcastle_if BACKEND == nccl BACKEND == ucc Issues async error handling see https github com pytorch pytorch issues skip_if_lt_x_gpu parametrize grad_as_bucket_view True False parametrize static_graph True False parametrize optimize_subset True False test_ddp_hook_with_optimizer_parity_adamw grad_as_bucket_view static_graph optimize_subset adamw_lr = e- adamw_betas = adamw_eps = e- _test_ddp_hook_with_optimizer_parity grad_as_bucket_view static_graph torch optim AdamW optimize_subset adamw_lr betas=adamw_betas eps=adamw_eps skip_but_pass_in_sandcastle_if BACKEND == nccl BACKEND == ucc Issues async error handling see https github com pytorch pytorch issues skip_if_lt_x_gpu parametrize optimize_subset True False test_ddp_hook_with_optimizer_parity_adam optimize_subset adam_lr = e- adam_betas = adam_eps = e- _test_ddp_hook_with_optimizer_parity True grad bucket view False static graph torch optim Adam optimize_subset adam_lr betas=adam_betas eps=adam_eps skip_but_pass_in_sandcastle_if BACKEND == nccl BACKEND == ucc Issues async error handling see https github com pytorch pytorch issues skip_if_lt_x_gpu parametrize optimize_subset True False test_ddp_hook_with_optimizer_parity_sgd optimize_subset sgd_lr = e- sgd_momentum = sgd_weight_decay = Not testing grad_as_bucket_view static_graph they tested AdamW test above _test_ddp_hook_with_optimizer_parity True grad bucket view False static_graph torch optim SGD optimize_subset sgd_lr momentum=sgd_momentum weight_decay=sgd_weight_decay skip_if_lt_x_gpu test_get_data_parallel_params torch cuda set_device rank model = TwoLinLayerNet cuda Parameters ignore format module_name param_name params_to_ignore = weight torch nn parallel DistributedDataParallel _set_params_and_buffers_to_ignore_for_model model params_to_ignore torch nn parallel DistributedDataParallel model device_ids= rank dp_params = torch nn parallel DistributedDataParallel _get_data_parallel_params model named_params=True name _ dp_params assertNotEqual f module params_to_ignore name test named_params=False just check returns expected no parameters num_ddp_params = len list model parameters - count = dp_params = torch nn parallel DistributedDataParallel _get_data_parallel_params model named_params=False _ dp_params count += assertEqual count num_ddp_params _test_ddp_apply_optim_in_backward optim_cls optim_kwargs init_before gradient_as_bucket_view=True Need seed ensure inputs unique across rank Otherwise allreduce won t have any effect torch manual_seed rank torch cuda manual_seed rank torch cuda set_device rank Test simple linear well ResNet model models_to_test = nn Sequential nn Linear nn Linear nn Linear cuda HAS_TORCHVISION models_to_test append torchvision models resnet cuda j model enumerate models_to_test model_optim_in_bwd = copy deepcopy model model = nn parallel DistributedDataParallel model device_ids= rank gradient_as_bucket_view=gradient_as_bucket_view optim = optim_cls model parameters optim_kwargs init_before _apply_optimizer_in_backward optimizer_class=optim_cls params=model_optim_in_bwd parameters optimizer_kwargs=optim_kwargs model_optim_in_bwd = nn parallel DistributedDataParallel model_optim_in_bwd device_ids= rank gradient_as_bucket_view=gradient_as_bucket_view init_before _apply_optimizer_in_backward optimizer_class=optim_cls params=model_optim_in_bwd parameters optimizer_kwargs=optim_kwargs p p zip model parameters model_optim_in_bwd parameters strict=True assertEqual p p Parameters initially equal Enable determinism cudnn operators torch backends cudnn flags enabled=True deterministic=True benchmark=False i range inp = torch randn device= cuda j == torch randn device= cuda model inp sum backward optim step model_optim_in_bwd inp sum backward runs optimizer well p p zip model parameters model_optim_in_bwd parameters strict=True assertEqual p p f Params equal iteration i assertTrue p grad None f Optim backward grad None i set_to_none regular optimizer match backward case optim zero_grad set_to_none=True skipIfRocm skip_if_lt_x_gpu test_ddp_apply_optim_in_backward optim_cls init_before itertools product torch optim SGD torch optim Adam True False subTest optim_cls=optim_cls _test_ddp_apply_optim_in_backward optim_cls=optim_cls optim_kwargs= lr init_before=init_before skipIfRocm skip_if_lt_x_gpu test_ddp_apply_optim_in_backward_grad_as_bucket_view_false init_before True False _test_ddp_apply_optim_in_backward optim_cls=torch optim SGD optim_kwargs= lr init_before=init_before gradient_as_bucket_view=False skipIfRocm skip_if_lt_x_gpu test_ddp_apply_optim_in_backward_ignored_params torch cuda set_device rank init_before True False subTest init_before=init_before torch manual_seed rank torch cuda manual_seed rank model = TwoLinLayerNet Parameters ignore format module_name param_name params_to_ignore = weight torch nn parallel DistributedDataParallel _set_params_and_buffers_to_ignore_for_model model params_to_ignore init_before _apply_optimizer_in_backward optimizer_class=torch optim SGD params=model parameters optimizer_kwargs= lr net = torch nn parallel DistributedDataParallel model cuda rank device_ids= rank init_before _apply_optimizer_in_backward optimizer_class=torch optim SGD params=model parameters optimizer_kwargs= lr inp = torch randn b = net inp transpose b sum backward weight did go through allreduce so optimizer acted local gradient which should different across ranks Remaining params should equal models = None _ range dist get_world_size dist all_gather_object models model rank _model remainder = models models m remainder assertNotEqual rank _model weight m weight assertEqual list rank _model b parameters list m b parameters assertEqual rank _model bias m bias _get_fp _config - _MixedPrecision _MixedPrecision param_dtype=torch float reduce_dtype=torch float buffer_dtype=torch float skip_if_lt_x_gpu test_ddp_native_mixed_precision_ignored_params rank = rank torch manual_seed rank torch cuda manual_seed rank torch cuda set_device rank model = TwoLinLayerNet model register_buffer buffer torch ones Parameters ignore format module_name param_name to_ignore = weight buffer torch nn parallel DistributedDataParallel _set_params_and_buffers_to_ignore_for_model model to_ignore mp_config = _get_fp _config net = torch nn parallel DistributedDataParallel model rank device_ids= rank mixed_precision=mp_config gradient_as_bucket_view=True to_ignore = f module name name to_ignore expected_ignored = len to_ignore n_ignored = ignored params should have _mp_param _fp_param fields n p itertools chain net named_parameters net named_buffers n to_ignore n_ignored += assertFalse hasattr p _mp_param assertFalse hasattr p _fp_param assertEqual mp_config param_dtype p _mp_param dtype assertEqual torch float p _fp_param dtype assertEqual expected_ignored n_ignored _test_ddp_native_mixed_precision gradient_as_bucket_view set_grad_to_none rank = rank torch manual_seed rank torch cuda manual_seed rank torch cuda set_device rank inp = torch randn mp_config = _get_fp _config MyModel torch nn Module __init__ - None super __init__ m = torch nn Linear register_buffer buffer torch randn p = torch nn Parameter torch randn requires_grad=False forward self_ x noqa B params = self_ m parameters p params assertEqual mp_config param_dtype p dtype assertEqual self_ buffer dtype mp_config buffer_dtype assertEqual mp_config param_dtype x dtype self_ m x + self_ p m = MyModel net = torch nn parallel DistributedDataParallel m rank device_ids= rank mixed_precision=mp_config gradient_as_bucket_view=gradient_as_bucket_view Buffers casted constructor assertEqual net module buffer dtype mp_config buffer_dtype Each param should have mp_param lower precision fp_param higher precision p net parameters assertEqual mp_config param_dtype p _mp_param dtype assertEqual torch float p _fp_param dtype _ range loss = net inp sum loss backward Verify gradient synchronization params grads fp n param net named_parameters assertEqual param dtype torch float param grad None assert n == module p Only param doesn t require grad assertEqual param grad dtype torch float tensor_list = torch zeros_like param grad _ range dist get_world_size net process_group dist all_gather tensor_list param grad g rest = tensor_list tensor_list assertEqual g dtype torch float g_ rest assertEqual g_ dtype torch float assertEqual g g_ net zero_grad set_to_none=set_grad_to_none skip_if_lt_x_gpu test_ddp_native_mixed_precision_no_grad_as_bucket_view_no_set_grad_none _test_ddp_native_mixed_precision gradient_as_bucket_view=False set_grad_to_none=False skip_if_lt_x_gpu test_ddp_native_mixed_precision_grad_as_bucket_view_no_set_grad_none _test_ddp_native_mixed_precision gradient_as_bucket_view=True set_grad_to_none=False skip_if_lt_x_gpu test_ddp_native_mixed_precision_grad_as_bucket_view_set_grad_to_none _test_ddp_native_mixed_precision gradient_as_bucket_view=True set_grad_to_none=True skip_if_lt_x_gpu test_ddp_native_mixed_precision_no_grad_as_bucket_view_set_grad_to_none _test_ddp_native_mixed_precision gradient_as_bucket_view=True set_grad_to_none=True _test_ddp_hook_parity state hook num_validated_iters= rank = rank m = torch nn Linear try process_group = state process_group except AttributeError process_group = state net_with_hook = torch nn parallel DistributedDataParallel copy deepcopy m rank device_ids= rank process_group=process_group net_with_hook register_comm_hook state=state hook=hook net_without_hook = torch nn parallel DistributedDataParallel copy deepcopy m rank device_ids= rank process_group=process_group i range Clear gradients manually g net_without_hook module weight grad net_with_hook module weight grad g None g requires_grad_ False g zero_ Forward + BW batch = torch tensor rank float cuda rank loss = net_without_hook batch sum loss backward For each worker gradient weight should worker_rank grad = net_without_hook module weight grad avg = grad clone expected_grad = sum i i range dist get_world_size dist get_world_size loss_hook = net_with_hook batch sum loss_hook backward grad_hook = net_with_hook module weight grad avg_hook = grad_hook clone i num_validated_iters Verify hook grad expected assertEqual avg_hook item expected_grad msg=f Expected hook grad expected_grad got avg_hook Verify hook grad vanilla allreduce assertEqual avg_hook avg msg=f Expected hook grad close allreduce avg got avg_hook skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature cuda f The BACKEND backend does support DDP communication hook CUDA devices skip_if_lt_x_gpu int os environ WORLD_SIZE test_ddp_hook_parity_allreduce _test_ddp_hook_parity state=None hook=default allreduce_hook skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature cuda f The BACKEND backend does support DDP communication hook CUDA devices skip_if_lt_x_gpu int os environ WORLD_SIZE test_ddp_hook_parity_allreduce_process_group process_group passed both DDP comm hook world_size = dist get_world_size rank_to_GPU = init_multigpu_helper world_size BACKEND gpus = rank_to_GPU int r r range world_size process_group = torch distributed new_group gpus _test_ddp_hook_parity state=process_group hook=default allreduce_hook skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature cuda f The BACKEND backend does support DDP communication hook CUDA devices skip_if_lt_x_gpu int os environ WORLD_SIZE test_ddp_hook_parity_powerSGD warm_start True False powersgd_state = powerSGD PowerSGDState process_group=None matrix_approximation_rank= start_powerSGD_iter= warm_start=warm_start _test_ddp_hook_parity state=powersgd_state hook=powerSGD powerSGD_hook skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature cuda f The BACKEND backend does support DDP communication hook CUDA devices skip_if_lt_x_gpu int os environ WORLD_SIZE test_ddp_hook_parity_post_localSGD Although we start run local SGD iteration since we still use global process group run post-LocalSGD actually still allreduces gradients globally remaining iterations state = post_localSGD PostLocalSGDState process_group=None subgroup=dist group WORLD start_localSGD_iter= _test_ddp_hook_parity state=state hook=post_localSGD post_localSGD_hook Only validate warmup iterations before local SGD applied because when ` post_local_gradient_allreduce ` disabled gradients will synchronized all Note practice model averager has applied run model averaging so local gradient averaging necessary start_localSGD_iter = state = post_localSGD PostLocalSGDState process_group=None subgroup=dist group WORLD start_localSGD_iter=start_localSGD_iter post_local_gradient_allreduce=False _test_ddp_hook_parity state=state hook=post_localSGD post_localSGD_hook num_validated_iters=start_localSGD_iter When ` subgroup ` None equivalent subgroup each node For single-node test environment intra-node process group equivalent global process group world_size == dist get_world_size state = post_localSGD PostLocalSGDState process_group=None subgroup=None start_localSGD_iter= _test_ddp_hook_parity state=state hook=post_localSGD post_localSGD_hook Since we start local SGD later than total number iterations no local SGD actually executed we don t even need provide subgroup case state = post_localSGD PostLocalSGDState process_group=None subgroup=None start_localSGD_iter= _test_ddp_hook_parity state=state hook=post_localSGD post_localSGD_hook _prepare_single_device_module rank process_group devices device_ids global_batch_size gradient_as_bucket_view=False model = Net device = devices devices torch device f cuda rank d ddp_model = DistributedDataParallel copy deepcopy model device device_ids=device_ids process_group=process_group bucket_cap_mb= gradient_as_bucket_view=gradient_as_bucket_view model device input = torch randn global_batch_size device target = torch randn global_batch_size device model ddp_model input target _prepare_cpu_module process_group global_batch_size gradient_as_bucket_view=False model = Net ddp_model = DistributedDataParallel copy deepcopy model process_group=process_group bucket_cap_mb= gradient_as_bucket_view=gradient_as_bucket_view input = torch randn global_batch_size target = torch randn global_batch_size model ddp_model input target _test_accumulate_gradients_no_sync num_iters= ddp_comm_hook=None gradient_as_bucket_view=False This recommended way implement accumulate grads If ` ` ddp_comm_hook ` ` input specified will also register hook ` ` ddp_model ` ` The hook fed into function should change resulting gradients _group group_id rank = _init_global_test world_size = get_world_size FIXME Add testing gloo CUDA BACKEND == mpi BACKEND == gloo global_batch_size = world_size local_batch_size = model ddp_model input target = _prepare_cpu_module group_id global_batch_size gradient_as_bucket_view BACKEND == nccl rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND int_devices = rank_to_GPU rank devices = torch device cuda + str i i int_devices global_batch_size = world_size local_batch_size = len devices model ddp_model input target = _prepare_single_device_module rank group_id devices devices global_batch_size gradient_as_bucket_view ddp_comm_hook None ddp_model register_comm_hook group_id ddp_comm_hook step_model model input target model train output = model input loss = F mse_loss output target output device loss backward ensure accumulate grads works no_grad = no grads accumulated torch no_grad ddp_model no_sync ddp_model train ddp_model input check two model parameters over num_iters iterations iteration range num_iters step_model model input target ddp_input = input rank local_batch_size rank + local_batch_size ddp_target = target rank local_batch_size rank + local_batch_size iteration == accumulate grads locally ddp_model no_sync step_model ddp_model ddp_input ddp_target sync grads step_model ddp_model ddp_input ddp_target i j zip model parameters ddp_model parameters strict=True i requires_grad continue iteration == assertNotEqual i grad j grad assertEqual i grad j grad Shuffle input so DDP input different torch manual_seed + iteration input = input torch randperm global_batch_size skip_but_pass_in_sandcastle_if BACKEND = mpi BACKEND = nccl BACKEND = gloo get_future only supported mpi nccl gloo nccl_skip_if_lt_x_gpu BACKEND test_accumulate_gradients_no_sync Runs _test_accumulate_gradients_no_sync using default inputs _test_accumulate_gradients_no_sync skip_but_pass_in_sandcastle_if BACKEND = mpi BACKEND = nccl BACKEND = gloo get_future only supported mpi nccl gloo nccl_skip_if_lt_x_gpu BACKEND test_accumulate_gradients_no_sync_grad_is_view Runs _test_accumulate_gradients_no_sync using default inputs _test_accumulate_gradients_no_sync gradient_as_bucket_view=True skip_but_pass_in_sandcastle_if BACKEND = mpi BACKEND = nccl BACKEND = gloo get_future only supported mpi nccl gloo nccl_skip_if_lt_x_gpu BACKEND test_accumulate_gradients_no_sync_allreduce_hook Runs multiple iterations _test_accumulate_gradients_no_sync using allreduce hook validates whether future result properly passed gradients reducer world_size = get_world_size allreduce_hook group_id object bucket dist GradBucket - torch futures Future torch Tensor tensors = bucket buffer world_size group_id allreduce tensors get_future then lambda fut fut value _test_accumulate_gradients_no_sync num_iters= ddp_comm_hook=allreduce_hook skip_but_pass_in_sandcastle_if BACKEND = mpi BACKEND = nccl BACKEND = gloo get_future only supported mpi nccl gloo nccl_skip_if_lt_x_gpu BACKEND test_accumulate_gradients_no_sync_allreduce_with_then_hook Runs multiple iterations _test_accumulate_gradients_no_sync using allreduce hook also uses then callbacks In first then callback result multiplied second callback divides result world_size It validates whether final result properly passed gradients reducer world_size = get_world_size allreduce_with_then_hook group_id object bucket dist GradBucket - torch futures Future torch Tensor fut = group_id allreduce bucket buffer get_future mult fut Multiply result fut wait div fut Divide result world_size fut wait world_size fut then mult then div _test_accumulate_gradients_no_sync num_iters= ddp_comm_hook=allreduce_with_then_hook skip_but_pass_in_sandcastle_if BACKEND = mpi BACKEND = nccl BACKEND = gloo get_future only supported mpi nccl gloo nccl_skip_if_lt_x_gpu BACKEND test_get_future mult fut t t fut wait add fut t + t fut wait group group_id rank = _init_global_test input = _build_tensor BACKEND == nccl rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND device_id = rank_to_GPU rank input = input device_id fut = group_id allreduce input get_future res = fut then mult then add wait expected = _build_tensor len group + assertEqual res expected skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_no_gpu test_DistributedDataParallel _group _group_id rank = _init_global_test rank_to_GPU = init_multigpu_helper dist get_world_size BACKEND gpus = list rank_to_GPU rank use_bucket_view static_graph itertools product False True False True _test_DistributedDataParallel gpu_subset=gpus rank=rank gradient_as_bucket_view=use_bucket_view static_graph=static_graph test set static graph twice _test_DistributedDataParallel gpu_subset=gpus rank=rank gradient_as_bucket_view=use_bucket_view static_graph=static_graph set_static_graph_twice=True test output_device _test_DistributedDataParallel gpu_subset=gpus rank=rank output_device=torch device cuda gradient_as_bucket_view=use_bucket_view static_graph=static_graph test device_ids gpus_list = torch device cuda + str i i gpus _test_DistributedDataParallel gpu_subset=gpus_list rank=rank output_device=torch device cuda gradient_as_bucket_view=use_bucket_view static_graph=static_graph _test_DistributedDataParallel_with_amp grad_is_view=False torch manual_seed Creates model optimizer default precision model = Net cuda optimizer = torch optim SGD model parameters lr= Creates GradScaler once beginning training scaler = GradScaler ddp_model = nn parallel DistributedDataParallel model device_ids= rank gradient_as_bucket_view=grad_is_view input = torch randn dist get_world_size cuda target = torch randn dist get_world_size cuda loss_fn = nn MSELoss verify grads none before training p ddp_model parameters assertTrue p None assertTrue p grad None idx range optimizer zero_grad Runs forward pass autocasting autocast output = ddp_model input loss = loss_fn output target Scales loss Calls backward scaled loss create scaled gradients Backward passes under autocast recommended Backward ops run same dtype autocast chose corresponding forward ops scaler scale loss backward verify grads none valid during training p ddp_model parameters p requires_grad assertTrue p grad None assertFalse p grad isnan any assertFalse p grad isinf any scaler step first unscales gradients optimizer s assigned params If these gradients do contain infs NaNs optimizer step then called otherwise optimizer step skipped scaler step optimizer Updates scale next iteration scaler update Shuffle input so DDP input different torch manual_seed + idx input = input torch randperm dist get_world_size ddp_model skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_no_gpu test_DistributedDataParallel_with_amp_and_grad_is_view torch cuda set_device rank ddp_model_grad_not_view = _test_DistributedDataParallel_with_amp grad_is_view=False ddp_model_grad_is_view = _test_DistributedDataParallel_with_amp grad_is_view=True i j zip ddp_model_grad_not_view parameters ddp_model_grad_is_view parameters strict=True assertEqual i j _test_DistributedDataParallel_SyncBatchNorm gpu_subset rank local_bs global_bs offset output_device=None affine=True Run simple end end DDP model use result single node model baseline cpu training setup model = BatchNormNet affine BatchNormNet affine=False single gpu training setup model_gpu = copy deepcopy model model_gpu cuda gpu_subset DDP training setup model_DDP = nn SyncBatchNorm convert_sync_batchnorm copy deepcopy model model_DDP cuda gpu_subset model_DDP = nn parallel DistributedDataParallel model_DDP device_ids=gpu_subset test serializable unserializable tempfile NamedTemporaryFile tmp sys platform == win torch save model_DDP tmp tmp seek weights_only=False legacy code saves model model_DDP = torch load tmp weights_only=False torch save model_DDP tmp name weights_only=False legacy code saves model model_DDP = torch load tmp name weights_only=False data initialization input_cpu = torch randn global_bs target = torch randn global_bs loss = nn MSELoss check two model parameters over iterations _test_DDP_niter model_gpu model_DDP input_cpu cuda gpu_subset target cuda gpu_subset loss local_bs rank global_bs True offset dist get_world_size affine _barrier _test_post_localSGD_optimizer_parity create_averager grad_is_view learning_rate = DDP_NET = Net net = torch nn parallel DistributedDataParallel copy deepcopy DDP_NET cuda device_ids= rank gradient_as_bucket_view=grad_is_view averager = create_averager opt = torch optim SGD net parameters lr=learning_rate net_using_post_localSGD_opt = torch nn parallel DistributedDataParallel copy deepcopy DDP_NET cuda device_ids= rank gradient_as_bucket_view=grad_is_view Process group cannot pickled some environments so cannot deep copy averager See https github com pytorch pytorch pull #pullrequestreview- averager = create_averager post_localSGD_opt = _create_post_localSGD_optimizer net_using_post_localSGD_opt learning_rate averager input = torch randn dist get_world_size cuda target = torch randn dist get_world_size cuda loss_fn = nn MSELoss _ range _perform_a_train_step opt net loss_fn input target averager average_parameters net parameters _perform_a_train_step post_localSGD_opt net_using_post_localSGD_opt loss_fn input target p p zip net parameters net_using_post_localSGD_opt parameters strict=True assertEqual p data p data Also check built-in step counters same prevent bug like assertEqual averager step averager step _create_periodic_model_averager averagers PeriodicModelAverager period= warmup_steps= _create_post_localSGD_optimizer net learning_rate averager post_localSGD_optimizer PostLocalSGDOptimizer optim=torch optim SGD net parameters lr=learning_rate averager=averager _perform_a_train_step optimizer net loss_fn input target optimizer zero_grad output = net input loss = loss_fn output target loss backward optimizer step _test_post_localSGD_optimizer_step_reload create_averager chkpt_file learning_rate = net_using_post_localSGD_opt = torch nn parallel DistributedDataParallel Net cuda device_ids= rank averager = create_averager post_localSGD_opt = _create_post_localSGD_optimizer net_using_post_localSGD_opt learning_rate averager averager = create_averager dummy_post_localSGD_opt = _create_post_localSGD_optimizer net_using_post_localSGD_opt learning_rate averager input = torch randn dist get_world_size cuda target = torch randn dist get_world_size cuda loss_fn = nn MSELoss _ range _perform_a_train_step post_localSGD_opt net_using_post_localSGD_opt loss_fn input target rank == torch save optimizer_state_dict post_localSGD_opt state_dict chkpt_file dist barrier map_location = cuda f cuda rank d checkpoint = torch load chkpt_file map_location=map_location dummy_post_localSGD_opt load_state_dict checkpoint optimizer_state_dict Check we didn t hit trivial case assertNotEqual averager step Check dummy averager initialized correct value assertEqual averager step averager step Remove step entry checkpoint And make sure state dictionary del checkpoint optimizer_state_dict step assertNotIn step checkpoint optimizer_state_dict Check checkpoint without step entry invokes warning assertWarnsRegex expected_warning=UserWarning expected_regex= Loaded state dict does contain step counter averager Setting step counter dummy_post_localSGD_opt load_state_dict checkpoint optimizer_state_dict assertEqual averager step skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_post_localSGD_optimizer_parity torch cuda set_device rank _test_post_localSGD_optimizer_parity _create_periodic_model_averager grad_is_view=False skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_post_localSGD_optimizer_parity_grad_is_view torch cuda set_device rank _test_post_localSGD_optimizer_parity _create_periodic_model_averager grad_is_view=True _create_hierarchical_model_averager period_group_size_dict = OrderedDict dist get_world_size hierarchicalSGD HierarchicalModelAverager period_group_size_dict=period_group_size_dict warmup_steps= skip_if_lt_x_gpu skip_if_odd_worldsize skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_post_localSGD_optimizer_parity_with_hierarchical_sgd torch cuda set_device rank _test_post_localSGD_optimizer_parity _create_hierarchical_model_averager grad_is_view=False skip_if_lt_x_gpu skip_if_odd_worldsize skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_post_localSGD_optimizer_parity_with_hierarchical_sgd_grad_is_view torch cuda set_device rank _test_post_localSGD_optimizer_parity _create_hierarchical_model_averager grad_is_view=True skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_post_localSGD_optimizer_step_reload torch cuda set_device rank _rank_temp_file tmp_file _test_post_localSGD_optimizer_step_reload _create_periodic_model_averager tmp_file skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_no_gpu test_DistributedDataParallel_SyncBatchNorm_Channels_Last _test_DistributedDataParallel_SyncBatchNorm_with_memory_format torch channels_last _test_DistributedDataParallel_SyncBatchNorm_with_memory_format torch channels_last_ d _test_DistributedDataParallel_SyncBatchNorm_with_memory_format memory_format _group _group_id rank = _init_global_test num_processes = dist get_world_size local_bs = bs_offset = int rank global_bs = int num_processes model = nn SyncBatchNorm momentum= model_gpu = copy deepcopy model cuda rank model_DDP = nn parallel DistributedDataParallel model_gpu device_ids= rank shapes = global_bs + memory_format torch channels_last input_gpu = torch randn shapes dtype=torch float cuda rank memory_format=memory_format target_gpu = torch randn shapes dtype=torch float cuda rank memory_format=memory_format loss = nn MSELoss check two model parameters over iterations _test_DDP_niter model_gpu model_DDP input_gpu target_gpu loss local_bs rank global_bs True bs_offset dist get_world_size memory_format=memory_format _barrier skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_no_gpu test_DistributedDataParallel_SyncBatchNorm _group _group_id rank = _init_global_test world_size = dist get_world_size DDP does support replicating BN layers within process hence testing one module replica per process gpus = rank local_bs = bs_offset = int rank global_bs = int world_size _test_DistributedDataParallel_SyncBatchNorm gpu_subset=gpus rank=rank local_bs=local_bs global_bs=global_bs offset=bs_offset test output_device _test_DistributedDataParallel_SyncBatchNorm gpu_subset=gpus rank=rank local_bs=local_bs global_bs=global_bs offset=bs_offset output_device=torch device cuda test device_ids gpus = torch device cuda + str i i gpus _test_DistributedDataParallel_SyncBatchNorm gpu_subset=gpus rank=rank local_bs=local_bs global_bs=global_bs offset=bs_offset output_device=torch device cuda skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_no_gpu test_DistributedDataParallel_SyncBatchNorm_No_Affine _group _group_id rank = _init_global_test world_size = dist get_world_size DDP does support replicating BN layers within process hence testing one module replica per process gpus = rank local_bs = bs_offset = int rank global_bs = int world_size _test_DistributedDataParallel_SyncBatchNorm gpu_subset=gpus rank=rank local_bs=local_bs global_bs=global_bs offset=bs_offset affine=False skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_no_gpu test_DistributedDataParallel_SyncBatchNorm_ D_Input _group _group_id rank = _init_global_test DDP does support replicating BN layers within process hence testing one module replica per process gpus = rank model = nn BatchNorm d single gpu training setup model_gpu = copy deepcopy model model_gpu cuda gpus DDP training setup model_DDP = nn SyncBatchNorm convert_sync_batchnorm copy deepcopy model model_DDP cuda gpus model_DDP = nn parallel DistributedDataParallel model_DDP device_ids=gpus local_bs = len gpus global_bs = dist get_world_size local_bs input_cpu = torch randn global_bs target = torch randn global_bs loss = nn MSELoss disabling cudnn SyncBatchNorm goes through native_batch_norm kernel avoids numerical issue created divergent code path torch backends cudnn flags False check two model parameters over iterations _test_DDP_niter model_gpu model_DDP input_cpu cuda gpus target cuda gpus loss local_bs rank global_bs True _barrier skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_no_gpu require_world_size test_DistributedDataParallel_SyncBatchNorm_Single_Input_Per_Process _group _group_id rank = _init_global_test DDP does support replicating BN layers within process hence testing one module replica per process gpus = rank model = nn BatchNorm d single gpu training setup model_gpu = copy deepcopy model model_gpu cuda gpus DDP training setup model_DDP = nn SyncBatchNorm convert_sync_batchnorm copy deepcopy model model_DDP cuda gpus model_DDP = nn parallel DistributedDataParallel model_DDP device_ids=gpus local_bs = global_bs = dist get_world_size input_cpu = torch randn global_bs target = torch randn global_bs loss = nn MSELoss disabling cudnn SyncBatchNorm goes through native_batch_norm kernel avoids numerical issue created divergent code path torch backends cudnn flags False check two model parameters over iterations _test_DDP_niter model_gpu model_DDP input_cpu cuda gpus target cuda gpus loss local_bs rank global_bs True _barrier skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_no_gpu test_DistributedDataParallel_SyncBatchNorm_Diff_Input_Sizes_Running_Value ONLY_SBN_NET = nn SyncBatchNorm momentum= _group _group_id rank = _init_global_test model = nn parallel DistributedDataParallel ONLY_SBN_NET cuda rank device_ids= rank input_var = i range dist get_world_size input_var_rank = torch cat torch ones i + i - torch ones i + i - dim= input_var append input_var_rank all_input_var = torch cat x permute contiguous view ONLY_SBN_NET num_features - x input_var dim= cuda rank _ range y = model input_var rank cuda rank y mean backward running_mean running_var = model module running_mean model module running_var torch testing assert_close running_mean all_input_var mean torch testing assert_close running_var all_input_var var skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_no_gpu test_DistributedDataParallel_SyncBatchNorm_Diff_Input_Sizes_gradient _group _group_id rank = _init_global_test only do single GPU per process gpus = rank cpu training setup num_processes = dist get_world_size local_bs = rank + bs_offset = int rank + rank global_bs = int num_processes + num_processes _test_DistributedDataParallel_SyncBatchNorm gpu_subset=gpus rank=rank local_bs=local_bs global_bs=global_bs offset=bs_offset skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_no_gpu test_DistributedDataParallel_SyncBatchNorm_half _group _group_id rank = _init_global_test model = BatchNormNet model = model half model = torch nn SyncBatchNorm convert_sync_batchnorm model model = nn parallel DistributedDataParallel model cuda rank device_ids= rank inp = torch randn dtype=torch float device=torch device rank Check forward backward do error dtype mismatch out = model inp assertEqual out dtype torch float out sum backward param model parameters assertEqual param grad dtype torch float _test_ddp_logging_data is_gpu rank = dist get_rank model_DDP = Net is_gpu model_DDP = nn parallel DistributedDataParallel model_DDP cuda rank device_ids= rank model_DDP = nn parallel DistributedDataParallel model_DDP dummy data initialization local_bs = batch_size input target loss = _prepare_dummy_data local_bs is_gpu input = input cuda rank target = target cuda rank model_DDP _set_ddp_runtime_logging_sample_rate idx range offset = rank local_bs DDP training DDP scatters subsets input nodes GPUs _test_DDP_helper model_DDP input offset offset + local_bs target offset offset + local_bs loss _model_step_with_zero_grad model_DDP Verify DDP logging data sampled expected If has ran more than iterations sampled iteration measuring run time stats run time stats idx-th iteration will zeros ddp_logging_data = model_DDP _get_ddp_logging_data idx idx idx == assertGreaterEqual ddp_logging_data get forward_compute_time assertGreaterEqual ddp_logging_data get backward_compute_time assertGreaterEqual ddp_logging_data get backward_comm_time assertGreaterEqual ddp_logging_data get backward_compute_time ddp_logging_data get backward_compute_comm_overlap_time assertGreaterEqual ddp_logging_data get backward_comm_time ddp_logging_data get backward_compute_comm_overlap_time assertEqual ddp_logging_data get iteration idx idx idx-th iteration sampled set runtime stats ddp_logging_data iteration will updated current iteration assertNotEqual ddp_logging_data get iteration idx Shuffle input so DDP input different input = input torch randperm batch_size model_DDP skip_but_pass_in_sandcastle_if BACKEND == nccl nccl does support DDP CPU models test_ddp_logging_data_cpu parse_env var os environ get var N A dist set_debug_level dist DebugLevel INFO _ group_id _ = _init_global_test model_DDP = _test_ddp_logging_data is_gpu=False ddp_logging_data = model_DDP _get_ddp_logging_data assertEqual ddp_logging_data get world_size dist get_world_size assertEqual ddp_logging_data get rank dist get_rank assertEqual ddp_logging_data get module_name Net assertEqual ddp_logging_data get device_ids output_device - default set e g output_device CPU training - assertEqual ddp_logging_data get output_device - assertEqual ddp_logging_data get broadcast_buffers assertEqual ddp_logging_data get bucket_cap_bytes assertEqual ddp_logging_data get find_unused_parameters assertEqual ddp_logging_data get gradient_as_bucket_view assertEqual ddp_logging_data get backend_name dist get_backend group_id assertEqual ddp_logging_data get iteration params = list model_DDP parameters num_params = param_size = params = list filter lambda parameter parameter requires_grad params p params num_params += param_size += p numel p element_size assertEqual ddp_logging_data get dtypes float assertEqual ddp_logging_data get total_parameter_size_bytes param_size assertEqual ddp_logging_data get num_parameter_tensors num_params assertEqual ddp_logging_data get bucket_sizes str param_size assertEqual ddp_logging_data get master_port parse_env MASTER_PORT assertEqual ddp_logging_data get master_addr parse_env MASTER_ADDR assertEqual ddp_logging_data get torch_distributed_debug parse_env TORCH_DISTRIBUTED_DEBUG assertEqual ddp_logging_data get cuda_visible_devices parse_env CUDA_VISIBLE_DEVICES ddp_logging_data get backend_name == gloo assertEqual ddp_logging_data get gloo_socket_ifname parse_env GLOO_SOCKET_IFNAME assertEqual ddp_logging_data get gloo_device_transport parse_env GLOO_DEVICE_TRANSPORT default_gloo_threads = assertEqual ddp_logging_data get gloo_num_threads default_gloo_threads assertEqual ddp_logging_data get nccl_socket_ifname None assertEqual ddp_logging_data get nccl_blocking_wait None assertEqual ddp_logging_data get nccl_async_error_handling None assertEqual ddp_logging_data get nccl_debug None assertEqual ddp_logging_data get nccl_nthreads None assertEqual ddp_logging_data get nccl_ib_timeout None test runtime logging fields Note DETAIL debug mode logs DDP logging data stdout thus accesses std map which fills default value type didn t exist assertEqual ddp_logging_data get unused_parameter_size assertEqual ddp_logging_data get has_rebuilt_buckets assertEqual ddp_logging_data get rebuilt_bucket_sizes str param_size grad_ready_order = ddp_logging_data get prev_iteration_grad_ready_order_indices expected_order = list reversed str x x range assertEqual grad_ready_order join expected_order bucket_indices = ddp_logging_data get rebuilt_per_bucket_param_indices assertEqual bucket_indices join expected_order It hard test accurate latency can test whether latency valid value expected range assertGreaterEqual ddp_logging_data get avg_forward_compute_time assertGreaterEqual ddp_logging_data get avg_backward_compute_time assertGreaterEqual ddp_logging_data get avg_backward_comm_time assertGreaterEqual ddp_logging_data get avg_backward_compute_time ddp_logging_data get avg_backward_compute_comm_overlap_time assertGreaterEqual ddp_logging_data get avg_backward_comm_time ddp_logging_data get avg_backward_compute_comm_overlap_time Test host-side times roughly order we expect fwd_host_side_time = ddp_logging_data get forward_compute_time_start bwd_comp_start_host_side_time = ddp_logging_data get backward_compute_time_start bwd_comp_end_host_side_time = ddp_logging_data get backward_compute_time_end bwd_comm_start_host_side_time = ddp_logging_data get backward_comm_time_start bwd_comm_end_host_side_time = ddp_logging_data get backward_comm_time_end assertGreaterEqual bwd_comm_end_host_side_time bwd_comm_start_host_side_time assertGreaterEqual bwd_comm_start_host_side_time bwd_comp_start_host_side_time assertGreaterEqual bwd_comp_end_host_side_time bwd_comp_start_host_side_time assertGreaterEqual bwd_comp_start_host_side_time fwd_host_side_time test larger net mixed data types verify multiple bucket sizes model = LargeNet model float model fc double model_DDP = nn parallel DistributedDataParallel model bucket_cap_mb= ddp_logging_data = model_DDP _get_ddp_logging_data params = list model_DDP parameters assertEqual ddp_logging_data get bucket_cap_bytes int bucket_sizes = params numel params element_size params numel params element_size assertEqual ddp_logging_data get bucket_sizes join str x x bucket_sizes assertEqual ddp_logging_data get dtypes double float skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_no_gpu test_ddp_logging_data_gpu _group _group_id rank = _init_global_test model_DDP = _test_ddp_logging_data is_gpu=True ddp_logging_data = model_DDP _get_ddp_logging_data assertEqual ddp_logging_data get device_ids str rank assertEqual ddp_logging_data get output_device rank grad_ready_order = ddp_logging_data get prev_iteration_grad_ready_order_indices expected_order = list reversed str x x range assertEqual grad_ready_order join expected_order bucket_indices = ddp_logging_data get rebuilt_per_bucket_param_indices assertEqual bucket_indices join expected_order test runtime logging fields It hard test accurate latency can test whether latency valid value expected range assertGreaterEqual ddp_logging_data get avg_forward_compute_time assertGreaterEqual ddp_logging_data get avg_backward_compute_comm_overlap_time assertGreaterEqual ddp_logging_data get avg_backward_compute_time ddp_logging_data get avg_backward_compute_comm_overlap_time assertGreaterEqual ddp_logging_data get avg_backward_comm_time ddp_logging_data get avg_backward_compute_comm_overlap_time Test host-side times roughly order we expect fwd_host_side_time = ddp_logging_data get forward_compute_time_start bwd_comp_start_host_side_time = ddp_logging_data get backward_compute_time_start bwd_comp_end_host_side_time = ddp_logging_data get backward_compute_time_end bwd_comm_start_host_side_time = ddp_logging_data get backward_comm_time_start bwd_comm_end_host_side_time = ddp_logging_data get backward_comm_time_end assertGreaterEqual bwd_comm_end_host_side_time bwd_comm_start_host_side_time assertGreaterEqual bwd_comm_start_host_side_time bwd_comp_start_host_side_time assertGreaterEqual bwd_comp_end_host_side_time bwd_comp_start_host_side_time assertGreaterEqual bwd_comp_start_host_side_time fwd_host_side_time skip_but_pass_in_sandcastle_if BACKEND == nccl nccl does support DDP CPU models test_static_graph_api_cpu model_DDP = nn parallel DistributedDataParallel Net expected_err = should called before training loop starts assertRaisesRegex RuntimeError expected_err local_bs = _batch_size input target loss = _prepare_dummy_data local_bs offset = dist get_rank local_bs DDP training DDP scatters subsets input nodes GPUs _test_DDP_helper model_DDP input offset offset + local_bs target offset offset + local_bs loss model_DDP _set_static_graph Verify error logged ddp_logging_data verify_ddp_error_logged model_DDP expected_err skipIfNoTorchVision test_SyncBatchNorm_process_group When adopting ` convert_sync_batchnorm ` convert ` nn modules ` need recursively pass ` process_group ` module when ` SyncBatchNorm ` nested sub-module sub-sub-module e g resnet torchvision models process_ids = process_group = torch distributed new_group process_ids res _model = torchvision models resnet res _model_sync = nn SyncBatchNorm convert_sync_batchnorm copy deepcopy res _model process_group process_group_sync = res _model_sync layer bn process_group assertEqual process_group_sync process_group _run_reduction_test tensor expected_tensor op reduction_fn=dist all_reduce dst=None reduction_fn dist all_reduce dst None raise ValueError f Reduction fn reduction_fn must specify dst dst None reduction_fn tensor dst op Only destination rank tensor expected have final result dist get_rank == dst assertEqual tensor expected_tensor reduction_fn tensor op assertEqual tensor expected_tensor require_backend_is_available nccl skip_if_lt_x_gpu test_nccl_backend_bool_allreduce torch cuda set_device rank Run all_reduce PRODUCT element = rank == op dist ReduceOp PRODUCT dist ReduceOp MIN input_tensor = torch tensor element element rank _run_reduction_test input_tensor torch tensor False False rank op Ensure all ranks contributing True cast results correct reduction input_tensor = torch tensor True True rank expected_tensor = input_tensor clone _run_reduction_test input_tensor expected_tensor op Run all_reduce SUM op dist ReduceOp SUM dist ReduceOp MAX input_tensor = torch tensor element element rank _run_reduction_test input_tensor torch tensor True True rank op TODO NCCL backend does work correctly bitwise reduction ops see https github com pytorch pytorch issues Add tests these once supported require_backend_is_available nccl skip_if_lt_x_gpu test_nccl_backend_bool_allgather torch cuda set_device rank inp = True True False True input_tensor = torch tensor inp rank rank Preserve copy tensor compare against after allgather input_tensor_copy = input_tensor clone tensor_list = torch tensor False False rank _ range dist get_world_size dist all_gather tensor_list input_tensor assertEqual len tensor_list dist get_world_size i t enumerate tensor_list expected = torch tensor inp i rank assertEqual t expected Ensure input tensor modified since collective does modify its input assertEqual input_tensor_copy input_tensor require_backend_is_available nccl skip_if_lt_x_gpu int os environ WORLD_SIZE test_nccl_backend_bool_reduce torch cuda set_device rank inp = True True False False Run reduce product op op dist ReduceOp PRODUCT dist ReduceOp MIN make sure rank gets False WORLD_SIZE= match expected tensor input_tensor = torch tensor inp rank + rank expected = torch tensor False False rank _run_reduction_test input_tensor expected op dist reduce dst= Ensure all ranks contributing True cast results correct reduction input_tensor = torch tensor True True rank expected_tensor = input_tensor clone _run_reduction_test input_tensor expected_tensor op dist reduce dst= op dist ReduceOp SUM dist ReduceOp MAX input_tensor = torch tensor inp rank rank expected = torch tensor True True rank rank == input_tensor clone _run_reduction_test input_tensor expected op dist reduce dst= require_backend_is_available nccl skip_if_lt_x_gpu test_nccl_backend_bool_broadcast tensor_size = bcast_tensor = torch tensor random random rank == False _ range tensor_size rank dist broadcast bcast_tensor src= Now allgather ensure tensors equal tensor_list = torch tensor False _ range tensor_size rank _ range dist get_world_size dist all_gather tensor_list bcast_tensor expected = tensor_list tensor tensor_list assertEqual tensor expected skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_lt_x_gpu int os environ WORLD_SIZE test_DistributedSampler_padding Tests padding distributed sampler world_size = dist get_world_size Simulates casual dataset size dataset_size = + world_size + dataset = torch ones rank i i range dataset_size Simulates tiny dataset size dataset_tiny_size = max world_size - dataset_tiny = torch ones rank i i range dataset_tiny_size Specifying drop_last=True will cause tail data dropped dist_sampler = DistributedSampler dataset=dataset drop_last=True local_num_samples local_dataset_size = dist_sampler num_samples dist_sampler total_size The effective dataset size should greatest integer = dataset_size divisible world_size This ensure each rank processes same number samples effective_dataset_size = math ceil dataset_size - world_size world_size dataset_size world_size = dataset_size world_size assertEqual local_num_samples effective_dataset_size assertEqual local_dataset_size local_num_samples world_size indices_list = list iter dist_sampler assertEqual len indices_list local_num_samples validate_global_samples local_num_samples Ensure each rank processes same number samples world_samples = torch LongTensor rank _ range world_size dist all_gather world_samples torch tensor local_num_samples rank world_samples = sample item sample world_samples assertEqual len set world_samples validate_global_samples local_num_samples drop_last=False default will add additional indices sampled increasing effective dataset size dist_sampler_added_samples = DistributedSampler dataset=dataset local_num_samples local_dataset_size = dist_sampler_added_samples num_samples dist_sampler_added_samples total_size The effective dataset size smallest integer = dataset_size divisible world size assertEqual local_num_samples math ceil dataset_size world_size assertEqual local_dataset_size local_num_samples world_size indices_list = list iter dist_sampler_added_samples assertEqual len indices_list local_num_samples Ensure each rank processes same number samples validate_global_samples local_num_samples Ensure additional samples padded even when extremely small dataset given dist_sampler_added_samples_tiny = DistributedSampler dataset=dataset_tiny local_num_samples local_dataset_size = dist_sampler_added_samples_tiny num_samples dist_sampler_added_samples_tiny total_size assertEqual local_num_samples math ceil dataset_tiny_size world_size assertEqual local_dataset_size local_num_samples world_size indices_list = list iter dist_sampler_added_samples_tiny assertEqual len indices_list local_num_samples validate_global_samples local_num_samples _test_allgather_object subgroup=None Only set device NCCL backend since must use GPUs gather_objects = create_collectives_object_test_list backend = os environ BACKEND backend == nccl Case where rank = GPU device next_rank = rank + int world_size torch cuda set_device next_rank If GPU test add object GPU tensor backend == nccl gather_objects append Foo torch randn device= output_gathered = None _ range dist get_world_size dist all_gather_object output_gathered gather_objects rank len gather_objects group=subgroup i val enumerate output_gathered expected = gather_objects i len gather_objects assertEqual val expected require_backend_is_available DistTestCases backend_feature gpu require_n_gpus_for_nccl_backend int os environ WORLD_SIZE os environ BACKEND with_dist_debug_levels levels= OFF INFO DETAIL test_all_gather_object_default_pg _test_allgather_object require_backend_is_available DistTestCases backend_feature gpu require_n_gpus_for_nccl_backend int os environ WORLD_SIZE os environ BACKEND with_dist_debug_levels levels= DETAIL OFF INFO test_all_gather_object_subgroup default = _get_default_group backend = dist get_backend default subgroup = dist new_group backend=backend _test_allgather_object subgroup=subgroup _test_gather_object pg=None Ensure stateful objects can gathered gather_objects = create_collectives_object_test_list my_rank = dist get_rank pg backend = os environ BACKEND backend == nccl Case where rank = GPU device next_rank = rank + int world_size torch cuda set_device next_rank If GPU test add object GPU tensor backend == nccl gather_objects append Foo torch randn device=my_rank output_gathered = None _ range dist get_world_size pg gather_on_rank = dist gather_object gather_objects rank len gather_objects object_gather_list=output_gathered my_rank == gather_on_rank None dst=gather_on_rank group=pg my_rank = gather_on_rank assertEqual output_gathered None _ range dist get_world_size i val enumerate output_gathered expected = gather_objects i len gather_objects assertEqual val expected Validate errors when objects can t pickled Bar pass b = Bar gather_objects = b _ range dist get_world_size assertRaises AttributeError dist all_gather_object None _ range dist get_world_size gather_objects rank group=pg skip_but_pass_in_sandcastle_if BACKEND == ucc CPU tensor ops supported UCP TL require_backend_is_available DistTestCases backend_feature gpu with_dist_debug_levels levels= DETAIL OFF INFO require_exact_world_size test_gather_object _test_gather_object skip_but_pass_in_sandcastle_if BACKEND == ucc CPU tensor ops supported UCP TL require_backend_is_available DistTestCases backend_feature gpu with_dist_debug_levels levels= DETAIL OFF INFO require_exact_world_size test_gather_object_subgroup default = _get_default_group backend = dist get_backend default subgroup = dist new_group backend=backend _test_gather_object subgroup validate_net_equivalence net Helper validate synchronization nets across ranks net_module_states = list net module state_dict values Check all tensors module s state_dict equal t net_module_states tensor_list = torch zeros_like t _ range dist get_world_size dist all_gather tensor_list t tensor tensor_list assertEqual tensor t skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_sync_module_states Test after calling _sync_module_states models across ranks same equal model input rank dim = rank = rank rank_to_broadcast = Seed ensure ranks initialized different initial models torch manual_seed rank model = nn Linear dim dim bias=False net = torch nn parallel DistributedDataParallel model cuda rank device_ids= rank bucket_cap_mb= new_model = nn Linear dim dim bias=False cuda rank net module = copy deepcopy new_model Assert params different net_module_states = list net module state_dict values t net_module_states tensor_list = torch zeros_like t _ range dist get_world_size dist all_gather tensor_list t i tensor enumerate tensor_list i == rank assertEqual t tensor tensor another rank should different assertNotEqual t tensor _sync_module_states module=net module process_group=net process_group broadcast_bucket_size=net broadcast_bucket_size src=rank_to_broadcast params_and_buffers_to_ignore=net parameters_to_ignore Now all model params should same validate_net_equivalence net Since network params broadcast rank_to_broadcast validate they same new_model rank_to_broadcast rank == rank_to_broadcast expected_states = new_model state_dict values t expected zip net_module_states expected_states strict=True assertEqual t expected skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_grad_div_uneven_inputs Test gradient division during training join API If divide_by_initial_world_size=False we scale effective world size when allreducing grads dim = batch = grad_scale = rank = rank model = nn Linear dim dim bias=False inp = torch ones batch dim device=self rank grad_scale net = torch nn parallel DistributedDataParallel model cuda rank device_ids= rank bucket_cap_mb= n_iters = rank n_iters += net join divide_by_initial_world_size=False _ range n_iters loss = net inp sum loss backward The grad always expected_grad since we divide number currently active processes inactive processes contribute zero gradient If we kept dividing static initial world size processes leave grad would smaller expected_grad = torch ones dim dim device=self rank grad_scale param = next iter net parameters assertEqual expected_grad param grad Avoid accumulating grads so s same every iteration net zero_grad torch cuda synchronize device=self rank If divide_by_initial_world_size=True default we always scale grads initial world_size net join divide_by_initial_world_size=True i range n_iters loss = net inp sum loss backward effective_ws = dist get_world_size i = effective_ws -= expected_grad = torch ones dim dim device=self rank grad_scale effective_ws dist get_world_size param = next iter net parameters assertEqual expected_grad param grad Avoid accumulating grad so s same every iteration net zero_grad torch cuda synchronize device=self rank _test_ddp_profiling profiler_ctx profiler_ctx =None Runs DDP based model training captures profiles This test will do two profiler runs An initial basic run check profiler events correctly captured A second profiling pass after running some iterations DDP check robustness thread local state args profiler_ctx Profiler context manager pass profiler_ctx Profiler context manager pass This can left out None which case deepcopy profiler_ctx used Returns prof Instantiated profiler object can used post analysis batch = dim = num_iters = torch cuda set_device rank model = nn Linear dim dim bias=False inp = torch rand batch dim device=self rank net = torch nn parallel DistributedDataParallel model cuda rank device_ids= rank profiler_ctx None profiler_ctx = copy deepcopy profiler_ctx profiler_ctx prof _ range num_iters loss = net inp sum loss backward all_reduce_event_name = f dist get_backend all_reduce events = get_profiling_event all_reduce_event_name prof dedup_gpu_user_annotation=True event_count = sum e count e events assertEqual event_count num_iters event events assertTrue event is_async assertEqual event name all_reduce_event_name broadcast_event_name = f dist get_backend broadcast broadcast_events = get_profiling_event broadcast_event_name prof dedup_gpu_user_annotation=True event_count = sum e count e broadcast_events Broadcast called during rebuild_buckets assertGreaterEqual event_count event broadcast_events assertEqual event name broadcast_event_name Run DDP profiling few iterations then enable profiling single pass ensure recorded This tests thread local state correctly updated net = torch nn parallel DistributedDataParallel model cuda rank device_ids= rank find_unused_parameters=True _ range loss = net inp sum loss backward Now enable profiler profiler_ctx prof loss = net inp sum loss backward events = get_profiling_event all_reduce_event_name prof dedup_gpu_user_annotation=True assertGreaterEqual len events assertGreaterEqual events count assertEqual events name all_reduce_event_name event events assertTrue event is_async Ensure searching unused parameters profiled events = get_profiling_event search_unused_parameters prof assertEqual len events prof require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu skip_but_pass_in_sandcastle Currently failing NVIDIA internal CI test_ddp_profiling_autograd_profiler autograd_profiler_ctx = torch autograd profiler profile _test_ddp_profiling profiler_ctx=autograd_profiler_ctx require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if IS_FBCODE Kineto fbcode code causes hang skip_but_pass_in_sandcastle_if IS_MACOS IS_WINDOWS torch profiler enabled mac windows https github com pytorch pytorch pull test_ddp_profiling_torch_profiler cpu_act = torch profiler ProfilerActivity CPU cuda_act = torch profiler ProfilerActivity CUDA torch_profiler_ctx = torch profiler profile activities= cpu_act cuda_act prof = _test_ddp_profiling profiler_ctx=torch_profiler_ctx dist get_backend = nccl Note comment out os remove trace_file ` get_profiler_nccl_meta ` debug any mismatches nccl_meta_events = get_profiler_nccl_meta prof assertGreater len nccl_meta_events nccl_meta = _sanity_check_profiler_nccl_meta nccl_meta_events additionally check specific collectives test case assertEqual len nccl_meta allreduce assertEqual len nccl_meta wait check allreduce message sizes = nccl_meta allreduce assertEqual Out msg nelems msg=f assertEqual dtype Float msg=f = nccl_meta allreduce assertEqual Out msg nelems msg=f assertEqual dtype Int msg=f _validate_execution_trace_nccl et_file str - None Torch profiler includes nccl metadata inserted operator called record_param_comms We test basic fields these nodes Execution Trace open et_file f et = json load f pg_cfg_node = n n et nodes n name == ## process_group init ## assertGreaterEqual len pg_cfg_node nccl_meta_nodes = n n et nodes n name == record_param_comms assertEqual len nccl_meta_nodes per_coll_meta = defaultdict list Sanity check NCCL metadata nodes n nccl_meta_nodes attrs_list = n get attrs assertGreater len attrs_list attrs = name value attrs_list collname = attrs get collective_name assertNotEqual collname assertNotEqual attrs get dtype per_coll_meta collname append attrs collname == wait continue assertEqual attrs pg_name yes string assertEqual attrs pg_desc default_pg assertEqual attrs pg_size assertGreaterEqual attrs get in_msg_nelems - assertGreaterEqual attrs get out_msg_nelems - assertTrue in_split_size attrs keys assertTrue out_split_size attrs keys assertEqual attrs get global_rank_start - assertEqual attrs get global_rank_stride - print per_coll_meta assertEqual len per_coll_meta allreduce assertEqual len per_coll_meta wait check allreduce message sizes = per_coll_meta allreduce assertEqual out_msg_nelems msg=f assertEqual dtype Float msg=f = per_coll_meta allreduce assertEqual out_msg_nelems msg=f assertEqual dtype Int msg=f require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if IS_FBCODE Kineto fbcode code causes hang skip_but_pass_in_sandcastle_if IS_MACOS IS_WINDOWS torch profiler enabled mac windows https github com pytorch pytorch pull unittest skipIf BACKEND = nccl Tests nccl metadata primarily test_ddp_profiling_execution_trace assertEqual dist get_backend nccl Create temp file save execution trace data fp = tempfile NamedTemporaryFile w+t suffix= et json delete=False fp close et_file = fp name et = ExecutionTraceObserver register_callback et_file first profiler context need have ET torch_profiler_ctx = torch profiler profile activities= ProfilerActivity CPU ProfilerActivity CUDA collect ET second profiler pass torch_profiler_ctx = torch profiler profile activities= ProfilerActivity CPU ProfilerActivity CUDA execution_trace_observer=et _test_ddp_profiling profiler_ctx=torch_profiler_ctx profiler_ctx =torch_profiler_ctx print f Execution trace saved fp name _validate_execution_trace_nccl et_file skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_join_model_equivalence Verifies equivalence model training locally DDP under join context manager batch = dim = learning_rate = model = nn Linear dim dim bias=False inp = torch rand batch dim device=self rank local_model = copy deepcopy model local_model = local_model cuda rank rank_to_iter_mapping = rank rank + rank range dist get_world_size run local model local_iters = sum rank_to_iter_mapping values local_optim = torch optim SGD local_model parameters lr=learning_rate _ range local_iters local_optim zero_grad out = local_model inp loss = out sum loss backward local_optim step run DDP model join API num_iters = rank_to_iter_mapping rank net = torch nn parallel DistributedDataParallel model cuda rank device_ids= rank ddp_optim = torch optim SGD model parameters lr=learning_rate dist get_world_size net join _ range num_iters ddp_optim zero_grad out = net inp loss = out sum loss backward torch cuda synchronize device=self rank ddp_optim step Validate model state dicts equal _ local_tensor _ dist_tensor zip local_model state_dict items net module state_dict items strict=True assertEqual local_tensor dist_tensor _run_uneven_inputs_test test_case iteration_mapping find_unused_params model = test_case model inp = test_case inp rank = rank sync_interval = test_case sync_interval torch cuda set_device rank Ensure all outstanding GPU work completed so test runs independently dist barrier Bucket_cap_mb intentionally low test allreduce scheduling when there many buckets net = torch nn parallel DistributedDataParallel model cuda rank device_ids= rank bucket_cap_mb= find_unused_parameters=find_unused_params Register hook specified test_case hook None net register_comm_hook test_case state test_case hook print f registered hook test_case hook Determine num iters rank via passed mapping num_iters = iteration_mapping rank If we throw when earliest rank terminates we should ensure we iterate minimum number times num_iters_tensor = torch tensor num_iters device=torch cuda current_device dist all_reduce num_iters_tensor op=dist ReduceOp MIN min_num_iters = num_iters_tensor item total_iters = test_case throw_on_early_termination min_num_iters == num_iters Early termination rank s exception_ctx = assertRaisesRegex RuntimeError f Rank rank exhausted all inputs Non early termination rank exception_ctx = assertRaisesRegex RuntimeError Detected least one rank exhausted inputs exception_ctx = nullcontext exception_ctx net join throw_on_early_termination=test_case throw_on_early_termination i range num_iters Use model no_sync disable grad synchronization every sync_interval i sync_interval = context = net no_sync context = nullcontext context isinstance inp tuple loss = net inp sum loss = net inp sum loss backward _model_step net Ensure completion GPU kernels including allreduce If join API properly implemented then should hang since allreduce will hang torch cuda synchronize device=rank total_iters += test_case throw_on_early_termination Ensure we iterated min_num_iters times assertEqual total_iters min_num_iters Ensure we iterated least min_num_iters times assertGreaterEqual total_iters min_num_iters Ensure completion all GPU kernels torch cuda synchronize device=rank When throwing early rank termination we do broadcast model state authoritative rank All models should already sync test_case throw_on_early_termination assertTrue net _authoritative_rank All ranks should have agreed same authoritative_rank final_rank_tensor = torch tensor net _authoritative_rank device=self rank tensor_list = torch zeros_like final_rank_tensor _ range dist get_world_size dist all_gather tensor_list final_rank_tensor max_rank = dist get_world_size - assertSetEqual max_rank tensor item tensor tensor_list Ensure all models same across ranks after all have joined validate_net_equivalence net Ensure running DDP uneven inputs logged ddp_logging_data = net _get_ddp_logging_data assertTrue ddp_logging_data get join_uneven_inputs dist barrier skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_uneven_inputs_stop_iteration_sync_bn Tests uneven inputs join handler correctly throws StopIteration models SyncBN general collective comm when throw_on_early_termination=True ModelWithComm torch nn Module __init__ - None super __init__ lin = nn Linear bias=False forward x x = lin x dist all_reduce x x torch cuda set_device rank model_bn = BatchNormNet model_bn = nn SyncBatchNorm convert_sync_batchnorm copy deepcopy model_bn cuda rank comm_model = ModelWithComm cuda rank model_input = torch randn cuda torch cuda current_device model model_bn comm_model model = torch nn parallel DistributedDataParallel model device_ids= rank min_num_iters = rank = Early termination rank s num_iters = min_num_iters exception_ctx = assertRaisesRegex RuntimeError f Rank rank exhausted all inputs Non early termination rank num_iters = min_num_iters exception_ctx = assertRaisesRegex RuntimeError Detected least one rank exhausted inputs n = exception_ctx model join throw_on_early_termination=True _ range num_iters loss = model model_input sum loss backward _model_step model n += assertEqual n min_num_iters Verify model equivalence validate_net_equivalence model skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_uneven_inputs dim = batch = Create variety models run uneven input tests large_model = nn Sequential nn Conv d nn ReLU nn Conv d nn ReLU nn Conv d nn ReLU small_model = nn Linear dim dim bias=False bn_net = BatchNormNet UnusedParamModule nn Module __init__ unused_params_rank super __init__ t = Task t = Task unused_params_rank = unused_params_rank task_parameters t p t p forward x rank t t x rank = unused_params_rank t x unjoined_rank_with_unused_params_model = UnusedParamModule joined_rank_with_unused_params_model = UnusedParamModule rank = rank models_to_test = Network batchnorm DDPUnevenTestInput name= batch_norm_net model=bn_net inp=torch ones batch device=rank sync_interval= DDPUnevenTestInput name= large_conv_model model=large_model inp=torch ones batch batch dim dim device=rank sync_interval= DDPUnevenTestInput name= small_model model=small_model inp=torch ones batch dim device=rank sync_interval= Unused parameter test where rank does join early has unused params DDPUnevenTestInput name= unjoined_rank_with_unused_params_model model=unjoined_rank_with_unused_params_model inp= torch ones batch device=rank rank sync_interval= Unused parameter test where rank does join early has unused params DDPUnevenTestInput name= joined_rank_with_unused_params_model model=joined_rank_with_unused_params_model inp= torch ones batch device=rank rank sync_interval= Test models have hook installed models_with_hook = DDPUnevenTestInput name= small_model_allreduce_hook model=small_model hook=default allreduce_hook state=None inp=torch ones batch dim device=rank sync_interval= DDPUnevenTestInput name= small_model_power_sgd_hook model=small_model hook=powerSGD powerSGD_hook state=powerSGD PowerSGDState process_group=None matrix_approximation_rank= Config so powerSGD runs immediately instead allreduce start_powerSGD_iter= warm_start=False use_error_feedback=False inp=torch ones batch dim device=rank sync_interval= models_to_test extend models_with_hook Add resnet model we have torchvision installed HAS_TORCHVISION resnet_model = torchvision models resnet models_to_test append DDPUnevenTestInput name= resnet_model model=resnet_model inp=torch ones sync_interval= Test no_sync every iterations models_with_sync = i test_input enumerate models_to_test models_with_sync append DDPUnevenTestInput name=test_input name model=test_input model inp=test_input inp sync_interval=i + throw_on_early_term_tests = test_input models_to_test throw_on_early_term_tests append DDPUnevenTestInput name=test_input name model=test_input model inp=test_input inp sync_interval=test_input sync_interval throw_on_early_termination=True models_to_test extend models_with_sync models_to_test extend throw_on_early_term_tests iteration tests when one process does train model all so we must shadow broadcast calls made when rebuilding buckets baseline_num_iters = iteration_offsets = num_uneven_ranks = dist get_world_size num_uneven_ranks append iteration_mappings = Generate rank num_iters mappings various uneven input scenarios This includes cases where rank joins early all other ranks join later scenarios where multiple ranks join early different iterations later ranks join later num_early_join_ranks num_uneven_ranks baseline_iter baseline_num_iters offset iteration_offsets mapping = dict fromkeys range num_early_join_ranks baseline_iter num_early_join_ranks ranks will join early iterate offset more times than rank test nodes depleting inputs different times num_early_join_ranks rank mapping keys rank mapping rank += offset mapping update dict fromkeys range num_early_join_ranks dist get_world_size baseline_iter + offset iteration_mappings append mapping test_case iteration_mapping itertools product models_to_test iteration_mappings rank == print f Running test test_case name sync interval test_case sync_interval iteration mapping iteration_mapping _run_uneven_inputs_test test_case iteration_mapping find_unused_params= unused_params_model test_case name skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_uneven_input_join_disable tests net join enable=False specified DDP works expected even inputs torch manual_seed rank net = torch nn parallel DistributedDataParallel torch nn Linear cuda rank device_ids= rank inp = torch ones rank n_iters = world_size = dist get_world_size net join enable=False _ range n_iters Clear grads grad = net module weight grad grad None grad requires_grad_ False grad zero_ out = net inp loss = out sum loss backward Validate gradients ensure we divide correct world_size when join mode disabled expected_grad = sum i i range world_size world_size assertEqual net module weight grad item expected_grad join_config = net _join_config assertFalse join_config enable validate_net_equivalence net skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_uneven_input_exception Tests exceptions during training correctly propagated context manager error_str = Intentional error ExceptionModule nn Module __init__ - None super __init__ param = nn Parameter torch ones requires_grad=True forward _ raise ValueError error_str exception_module = ExceptionModule net = torch nn parallel DistributedDataParallel exception_module cuda rank device_ids= rank inp = torch ones assertRaisesRegex ValueError error_str net join out = net inp loss = out sum loss backward _test_broadcast_object_list group=None gather_objects = create_collectives_object_test_list Only set device NCCL backend since must use GPUs Case where rank = GPU device next_rank = rank + int world_size backend = os environ BACKEND backend == nccl torch cuda set_device next_rank src_rank = If GPU test add object GPU tensor backend == nccl gather_objects append Foo torch randn device= IS_FBCODE Create Tensor ^ Bytes storage requirements Only FBCODE testing OOMs OSS gather_objects append Foo torch randn objects = gather_objects rank == src_rank None _ gather_objects Single object test device specified Backend= gloo device=cpu backend = nccl single_obj_list = objects rank = src_rank assertNotEqual single_obj_list gather_objects dist broadcast_object_list single_obj_list src= group=group device=torch device cpu assertEqual single_obj_list gather_objects Single object test device specified Backend= gloo device=current_device+ The test gated fact GPU count same world size avoid case when backend gloo there no multiple GPU devices backend = nccl torch cuda device_count == int world_size single_obj_list = objects rank = src_rank assertNotEqual single_obj_list gather_objects dist broadcast_object_list single_obj_list src= group=group device=torch device next_rank assertEqual single_obj_list gather_objects Single object test device specified Backend= nccl device=current_device+ backend == nccl torch cuda device_count == int world_size single_obj_list = objects rank = src_rank assertNotEqual single_obj_list gather_objects dist broadcast_object_list single_obj_list src= group=group device=torch device next_rank assertEqual single_obj_list gather_objects Single object test backward compatibility device unspecified single_obj_list = objects rank = src_rank assertNotEqual single_obj_list gather_objects dist broadcast_object_list single_obj_list src= group=group assertEqual single_obj_list gather_objects Multiple input objects test rank = src_rank assertNotEqual objects gather_objects dist broadcast_object_list objects src= group=group assertEqual objects gather_objects require_backend_is_available DistTestCases backend_feature gpu require_n_gpus_for_nccl_backend int os environ WORLD_SIZE os environ BACKEND with_dist_debug_levels levels= DETAIL unittest skip Test failing see https github com pytorch pytorch pull test_broadcast_object_list _test_broadcast_object_list require_backend_is_available DistTestCases backend_feature gpu require_n_gpus_for_nccl_backend int os environ WORLD_SIZE os environ BACKEND with_dist_debug_levels levels= DETAIL _test_broadcast_object_list_subgroup default = _get_default_group backend = dist get_backend default subgroup = dist new_group backend=backend _test_broadcast_object_list subgroup _test_ddp_ignore_params_arg static_graph=False TestModel nn Module __init__ rank rank = rank super __init__ fc = nn Linear bias=False Proxy will materialized another architecture later after wrapping model DDP rank == fc = nn Linear bias=False fc = nn Linear bias=False forward x x = fc x x = fc x x device_id = rank Ensure test works both find_unused_parameter broadcast_buffer settings find_unused broadcast_buffers itertools product False True False True model = TestModel rank float device_id Note model can have different shape buffers we pass them ignored well model fc register_buffer ignore_buffer torch zeros + rank device=self rank proxy_params = list model fc parameters model_fc _name = next module_name module_name module model named_modules module model fc proxy_param_names = f model_fc _name param_name param_name _ model fc named_parameters proxy_buffer_names = f model_fc _name buf_name buf_name _ model fc named_buffers Specify we should ignore proxy_params since will materialized later torch nn parallel DistributedDataParallel _set_params_and_buffers_to_ignore_for_model model proxy_param_names + proxy_buffer_names ddp = torch nn parallel DistributedDataParallel model device_ids= device_id find_unused_parameters=find_unused broadcast_buffers=broadcast_buffers static_graph=static_graph Materialize new params These registered DDP thus don t have autograd hooks installed them ddp module fc = nn Linear bias=False device_id local model new materialized parameters local_model = copy deepcopy ddp module cuda rank inp = torch ones dtype=torch float device_id rank + _ range ddp inp sum backward local_model inp sum backward materialized param grad touched DDP so its grad should same running locally materialized_param local_param zip ddp module fc parameters local_model fc parameters strict=True assertEqual materialized_param grad local_param grad fc parameter grad should still different due allreduce synced_param local_param zip ddp module fc parameters local_model fc parameters strict=True assertFalse synced_param grad == local_param grad Proxy module grad should touched proxy_param proxy_params assertTrue proxy_param grad None Synchronize since we run multiple iterations test isolate failure hangs torch cuda synchronize device=self rank require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_ddp_ignore_params_arg _test_ddp_ignore_params_arg static_graph=False _test_ddp_ignore_params_arg static_graph=True with_dist_debug_levels levels= OFF INFO DETAIL require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_ddp_unused_params_rebuild_buckets_exception ToyModel nn Module __init__ - None super __init__ net = nn Linear bias=False net = nn Linear bias=False forward x net x ddp = torch nn parallel DistributedDataParallel ToyModel cuda rank device_ids= rank i range inp = torch rand i On nd iteration will fail during rebuild_buckets we should report error regarding unused parameters since underlying root cause try ddp inp sum backward except RuntimeError e msg = str e verify_ddp_error_logged ddp msg expected_strs = ddp_prev_reduction_unfinished_str ddp_recommend_find_unused_params_str ddp_outputs_not_used_in_loss_str In debug mode should show parameters weren t reduced Without debug mode should show suggestion use debug mode dist get_debug_level == dist DebugLevel OFF expected_strs append ddp_suggest_debug_mode_str unreduced_params = join net weight expected_strs append f did receive grad rank rank unreduced_params s expected_strs assertTrue s msg f Expected s msg assertFalse ddp_find_unused_params_enabled_str msg assertFalse True DDP unused parameters error raised ddp inp sum backward dist barrier require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_ddp_shared_grad_acc_unused_params When find_unused_parameters=True ensure we mark unused parameters even they share gradient accumulators ToyModel nn Module __init__ - None super __init__ net bias net bias all unused params net = nn Linear bias=False bias = nn Parameter torch zeros net bias bias names same underlying parameter so they share same grad acc This caused bug reported https github com pytorch pytorch issues net bias = bias net = nn Linear forward x net x sum torch cuda set_device rank model = ToyModel torch cuda current_device static True False ddp_model = torch nn parallel DistributedDataParallel copy deepcopy model device_ids= rank find_unused_parameters=True static_graph=static inp = torch randn device=self rank _ range loss = ddp_model inp To test https github com pytorch pytorch issues loss = loss backward require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_ddp_device expected_len = TensorWrapper __slots__ = t moved_to_gpu __init__ t t = t moved_to_gpu = False Handlers specific types validation we want do based input type tuple_and_list_validator x assertTrue len x expected_len assertEqual len t device t x assertEqual x device index rank x + x namedtuple_validator x assertEqual x _fields EXPECTED_FIELDS assertEqual x device index x b device index assertEqual x device index rank x + x b custom_type_validator x assertTrue x moved_to_gpu str x t device == cpu x t = x t rank x moved_to_gpu = True x t dict_validator x assertTrue EXPECTED_FIELDS x keys assertTrue EXPECTED_FIELDS x keys assertEqual len t device t x values assertEqual x EXPECTED_FIELDS device index rank x EXPECTED_FIELDS + x EXPECTED_FIELDS validators = TensorWrapper custom_type_validator tuple tuple_and_list_validator list tuple_and_list_validator TestNamedTupleInput_ namedtuple_validator TestNamedTupleInput_ namedtuple_validator dict dict_validator ToyModel torch nn Module __init__ self_ noqa B super __init__ self_ lin = nn Linear bias=False forward self_ x expected_type noqa B Similar scatter recursive single-device case does move tensors they custom type assertTrue isinstance x expected_type fwd_tensor = validators expected_type x self_ lin fwd_tensor model = torch nn parallel DistributedDataParallel ToyModel rank device_ids= rank train_iter inp input_type _ range out = model inp input_type out sum backward CPU tuple input should moved proper device before call forward inp = tuple torch randn _ range expected_len train_iter inp tuple List CPU input should moved proper device before call forward inp = torch randn _ range expected_len train_iter inp list Custom type containing tensor The type maintained device propagated which what happens scatter too inp = TensorWrapper torch randn train_iter inp TensorWrapper NamedTuple input The type should maintained tensor inputs should moved correct device scatter batch = dim = = torch rand batch dim b = torch rand batch dim inp = TestNamedTupleInput_ b train_iter inp type inp inp = TestNamedTupleInput_ b train_iter inp type inp dictionary input inp = EXPECTED_FIELDS EXPECTED_FIELDS b train_iter inp type inp require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_ddp_namedtuple batch = dim = = torch rand batch dim device=self rank b = torch rand batch dim device=self rank NamedTupleModule torch nn Module __init__ self_ noqa B super __init__ self_ lin = nn Linear forward self_ input expected_type noqa B Without NamedTuple support would type tuple assertTrue isinstance input expected_type f Expected type expected_type got type input assertEqual input _fields EXPECTED_FIELDS assertEqual input assertEqual b input b self_ lin torch mul input input b model = torch nn parallel DistributedDataParallel NamedTupleModule cuda rank device_ids= rank inp = TestNamedTupleInput_ b The following would fail DDP does propagate NamedTuples correctly model inp type inp inp = TestNamedTupleInput_ b model inp type inp require_backend_is_available gloo test_grads_same_across_ranks_with_no_sync _group _group_id rank = _init_global_test world_size = dist get_world_size world_size skipTest This test requires least two ranks SimpleConditionalModel nn Module rank uses nn first pass nn second pass uses nn first pass nn second pass __init__ rank super __init__ rank = rank nn = nn Linear nn = nn Linear nn = nn Linear nn = nn Linear state = forward input state == state = rank == nn input nn input state = rank == nn input nn input model = torch nn parallel DistributedDataParallel SimpleConditionalModel rank find_unused_parameters=True mse_loss = nn MSELoss grad_accumulation = microbatch_idx range grad_accumulation microbatch_idx grad_accumulation - context = model no_sync context = nullcontext context input = torch rand output = model forward input target = torch rand loss = mse_loss output target loss backward assertTrue any p grad None p model parameters Gradients can t None any model parameter grads = torch cat p grad view - p model parameters Gather all gradients rank rank == gathered_grads = torch zeros_like grads _ range world_size gathered_grads = dist gather grads gather_list=gathered_grads dst= rank == g gathered_grads assertTrue torch allclose gathered_grads g Gradients same all ranks with_dist_debug_levels levels= OFF INFO DETAIL require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_ddp_control_flow_same_across_ranks Control flow same across ranks batch = dim = world_size = dist get_world_size torch cuda set_device rank model = torch nn parallel DistributedDataParallel ControlFlowToyModel cuda rank device_ids= rank find_unused_parameters=True random_input = torch randn batch dim device=self rank ones_input = torch ones batch dim device=self rank i range i == out = model random_input out = model ones_input loss = out sum loss backward On even iterations nd param goes unused odd iterations used local_used_map = model reducer _get_local_used_map i == expected = torch tensor world_size device=self rank dtype=torch int expected = torch tensor world_size world_size device=self rank dtype=torch int Validate parameter usage variable_usage_tensor = local_used_map assertEqual variable_usage_tensor expected Validate appropriate error message when DDP used find_unused_parameters=False model = torch nn parallel DistributedDataParallel ControlFlowToyModel cuda rank device_ids= rank find_unused_parameters=False i range i == loss = model random_input sum loss backward try loss = model random_input sum loss backward except RuntimeError e msg = str e verify_ddp_error_logged model msg nd linear layer unused unused_param_index = expected_strs = ddp_prev_reduction_unfinished_str ddp_recommend_find_unused_params_str ddp_outputs_not_used_in_loss_str f Parameter indices which did receive grad rank rank unused_param_index In debug mode should show parameters weren t reduced Without debug mode should show suggestion use debug mode dist get_debug_level == dist DebugLevel OFF expected_strs append ddp_suggest_debug_mode_str unreduced_params = join lin weight expected_strs append f did receive grad rank rank unreduced_params s expected_strs assertTrue s msg f Expected s msg assertFalse ddp_find_unused_params_enabled_str msg assertFalse True DDP error raised dist barrier require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_invalid_static_graph torch cuda set_device rank model = torch nn parallel DistributedDataParallel ControlFlowToyModel cuda rank device_ids= rank static_graph=True random_input = torch randn device=self rank ones_input = torch ones device=self rank unused parameter first iteration got used second iteration expected_err = Your training graph has changed iteration assertRaisesRegex RuntimeError expected_err i range i == out = model random_input out = model ones_input loss = out sum loss backward verify_ddp_error_logged model expected_err used parameter first iteration got unused second iteration assertRaisesRegex RuntimeError Expected have finished reduction prior iteration before starting new one This error indicates your training graph has changed iteration e g one parameter used first iteration then got unused second iteration compatible static_graph set True \n Parameter indices which did receive grad i range i = out = model random_input out = model ones_input loss = out sum loss backward verify_ddp_error_logged model Expected have finished reduction with_dist_debug_levels levels= OFF INFO DETAIL require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_ddp_control_flow_different_across_ranks Control flow different across ranks batch = dim = ToyModel nn Module __init__ rank super __init__ lin = nn Linear bias=False lin = nn Linear bias=False rank = rank forward x Control-flow rank input dependent model use_second_layer = torch equal x torch ones batch dim device=x device rank == use_second_layer lin F relu lin x F relu lin x world_size = dist get_world_size torch cuda set_device rank model = torch nn parallel DistributedDataParallel ToyModel rank cuda rank device_ids= rank find_unused_parameters=True random_input = torch randn batch dim device=self rank ones_input = torch ones batch dim device=self rank i range i == out = model random_input out = model ones_input loss = out sum loss backward On even iterations nd param goes unused odd iterations used only rank local_used_map = model reducer _get_local_used_map i == expected = torch tensor world_size device=self rank dtype=torch int expected = torch tensor world_size device=self rank dtype=torch int variable_usage_tensor = local_used_map Validate parameter usage On odd iterations nd param only used rank assertEqual variable_usage_tensor expected Validate appropriate error message when DDP used find_unused_parameters=False model = torch nn parallel DistributedDataParallel ToyModel rank cuda rank device_ids= rank find_unused_parameters=False i range i == loss = model random_input sum loss backward try loss = model random_input sum loss backward except RuntimeError e msg = str e verify_ddp_error_logged model msg unused_param_index = expected_strs = ddp_prev_reduction_unfinished_str ddp_recommend_find_unused_params_str ddp_outputs_not_used_in_loss_str f Parameter indices which did receive grad rank rank unused_param_index In debug mode should show parameters weren t reduced Without debug mode should show suggestion use debug mode dist get_debug_level == dist DebugLevel OFF expected_strs append ddp_suggest_debug_mode_str unreduced_params = join lin weight expected_strs append f did receive grad rank rank unreduced_params s expected_strs assertTrue s msg f Expected s msg assertFalse ddp_find_unused_params_enabled_str msg assertFalse True DDP error raised dist barrier require_backend_is_available gloo test_scatter_object_list src_rank = collectives_object_test_list = create_collectives_object_test_list scatter_list = collectives_object_test_list rank == src_rank None _ collectives_object_test_list world_size = dist get_world_size scatter_list = scatter_list world_size i = while len scatter_list world_size scatter_list append scatter_list i i += output_obj_list = None dist scatter_object_list output_obj_list scatter_list src=src_rank assertEqual output_obj_list collectives_object_test_list rank len collectives_object_test_list Ensure errors raised upon incorrect arguments assertRaisesRegex ValueError Expected argument scatter_object_output_list list size least dist scatter_object_list scatter_list src=src_rank _generate_sparse_tensors_for_bucket_assignment_test tensors = torch empty dtype=torch float torch empty dtype=torch double torch empty dtype=torch float torch empty dtype=torch double torch empty dtype=torch float torch empty dtype=torch double tensors_sparse = t to_sparse t tensors tensors_sparse _test_compute_bucket_assignment_by_size use_logger group_gloo = dist new_group timeout=timedelta seconds= backend=dist Backend GLOO Set TORCH_NCCL_BLOCKING_WAIT use new NCCL group improve test determinism os environ TORCH_NCCL_BLOCKING_WAIT = group_to_use = dist new_group backend=dist get_backend timeout=timedelta seconds= torch cuda set_device rank Create valid model The constructor initializes logger we use later We never actually use rest model - we only need its logger net = EmbeddingNetDifferentParams net = torch nn parallel DistributedDataParallel net rank device_ids= rank process_group=group_to_use we don t pass logger then we can only check exception thrown expected_err = No support sparse tensors assertRaisesRegex RuntimeError expected_err tensors_sparse = _generate_sparse_tensors_for_bucket_assignment_test use_logger dist _compute_bucket_assignment_by_size tensors_sparse logger=net logger dist _compute_bucket_assignment_by_size tensors_sparse use_logger verify_ddp_error_logged net expected_err Perform gloo-based barrier ensure one rank doesn t exit test early which causes failure Barrier sync dist barrier group_gloo require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_compute_bucket_assignment_by_size_sparse_error_without_logger _test_compute_bucket_assignment_by_size use_logger=False require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_compute_bucket_assignment_by_size_sparse_error_with_logger _test_compute_bucket_assignment_by_size use_logger=True _test_verify_model_across_rank use_logger group_gloo = dist new_group timeout=timedelta seconds= backend=dist Backend GLOO group_to_use = dist new_group backend=dist get_backend timeout=timedelta seconds= torch cuda set_device rank Create valid model The constructor initializes logger we use later net = EmbeddingNetDifferentParams net = torch nn parallel DistributedDataParallel net rank device_ids= rank process_group=group_to_use Modify model so number parameters different each rank This will cause RuntimeError thrown below _verify_param_shape_across_processes so we can check correct error thrown logged We can t do constructor above otherwise logger will properly initialized net module lin = nn Linear rank == we pass logger we can verify logged caught = try use_logger _verify_param_shape_across_processes net process_group list net parameters net logger _verify_param_shape_across_processes net process_group list net parameters except Exception caught = As long there one rank catching exception t = torch Tensor caught dist all_reduce t group=group_gloo assertGreater t require_backend_is_available DistTestCases backend_feature gpu skip_but_pass_in_sandcastle_if BACKEND == ucc IS_SANDCASTLE Skipped internally skip_if_lt_x_gpu test_verify_model_across_rank_with_logger _test_verify_model_across_rank use_logger=True require_backend_is_available DistTestCases backend_feature gpu skip_but_pass_in_sandcastle_if BACKEND == ucc IS_SANDCASTLE Skipped internally skip_if_lt_x_gpu test_verify_model_across_rank_without_logger _test_verify_model_across_rank use_logger=False _run_test_ddp_model_with_diff_params net ddp_group group_gloo caught = try net = torch nn parallel DistributedDataParallel net rank device_ids= rank process_group=ddp_group except Exception caught = As long there one rank catching exception t = torch Tensor caught dist all_reduce t group=group_gloo assertGreater t require_backend_is_available DistTestCases backend_feature gpu skip_but_pass_in_sandcastle_if BACKEND == ucc IS_SANDCASTLE Skipped internally skip_if_lt_x_gpu test_ddp_model_diff_shape_across_ranks group_gloo = dist new_group timeout=timedelta seconds= backend=dist Backend GLOO group_to_use = dist new_group backend=dist get_backend timeout=timedelta seconds= torch cuda set_device rank Creates network different sized embedding table different ranks This should throw error during DDP init net = EmbeddingNetDifferentParams rank _run_test_ddp_model_with_diff_params net group_to_use group_gloo require_backend_is_available DistTestCases backend_feature gpu skip_but_pass_in_sandcastle_if BACKEND == ucc IS_SANDCASTLE Skipped internally skip_if_lt_x_gpu test_ddp_model_diff_num_params_across_ranks group_gloo = dist new_group timeout=timedelta seconds= backend=dist Backend GLOO group_to_use = dist new_group backend=dist get_backend timeout=timedelta seconds= torch cuda set_device rank Creates network diff param across ranks reducer should recognize throw appropriate error net = EmbeddingNetDifferentParams rank diff_num_params= rank == _run_test_ddp_model_with_diff_params net group_to_use group_gloo _test_output_unused_in_loss module_cls gradient_as_bucket_view model = module_cls local_net = copy deepcopy model net = torch nn parallel DistributedDataParallel copy deepcopy model cuda rank device_ids= rank find_unused_parameters=True Tests certain parameters getting gradient since output unused loss computation supported Specifically checks grads remain unchanged same local training inp = torch randn Ensure param used loss computation its gradient untouched i e None before None after zero module_cls == DictOutputModule b = local_net inp predictions a_dist b_dist = net inp predictions b = local_net inp a_dist b_dist = net inp loss_dist = b_dist sum loss_dist backward Ensure gradient corresponding parameter touched i e None matches local grad module_cls == DictOutputModule assertTrue net module module weight grad None assertEqual net module module weight grad local_net module weight grad assertTrue net module weight grad None assertEqual net module weight grad local_net weight grad saved_a_local_grad = None saved_a_dist_grad = None net zero_grad local_net zero_grad i range module_cls == DictOutputModule b = local_net inp predictions a_dist b_dist = net inp predictions b = local_net inp a_dist b_dist = net inp i Use both params loss computation Later will go unused we check ensure DDP supports gradients remain same local training t = b t_dist = a_dist b_dist loss = t sum loss_dist = t_dist sum Model output unused loss loss = b sum loss_dist = b_dist sum loss backward loss_dist backward i == Save grads compare them next iterations module_cls == DictOutputModule saved_a_local_grad = local_net module weight grad saved_a_dist_grad = net module module weight grad saved_a_local_grad = local_net weight grad saved_a_dist_grad = net module weight grad assertEqual saved_a_local_grad saved_a_dist_grad i = parameter both models should same change module_cls == DictOutputModule assertEqual net module module weight grad saved_a_dist_grad assertEqual local_net module weight grad saved_a_local_grad assertEqual net module weight grad saved_a_dist_grad assertEqual local_net weight grad saved_a_local_grad Verify grads same local_param dist_param zip local_net parameters net parameters strict=True local_grad = local_param grad dist_grad = dist_param grad assertEqual local_grad dist_grad dist barrier skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_lt_x_gpu test_output_unused_in_loss_tuple_module module_cls = UnusedParamTwoLinLayerNet grad_as_bucket_view True False _test_output_unused_in_loss module_cls grad_as_bucket_view skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_lt_x_gpu test_output_unused_in_loss_dict_module module_cls = DictOutputModule grad_as_bucket_view True False _test_output_unused_in_loss module_cls grad_as_bucket_view skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_lt_x_gpu test_undefined_grad_parity_unused_parameters TODO enable general training use cases https github com pytorch pytorch issues x = torch ones rank net = Net rank local_net = copy deepcopy net net = torch nn parallel DistributedDataParallel net device_ids= rank find_unused_parameters=True out = net x sum local_out = local_net x sum Simulates undefined gradients torch _C _functions UndefinedGrad out backward torch _C _functions UndefinedGrad local_out backward dist_param_name dist_param local_param_name local_param zip net named_parameters local_net named_parameters strict=True dist_grad = dist_param grad local_grad = local_param grad assertEqual dist_grad local_grad f DDP param dist_param_name grad dist_grad does match local param local_param_name grad local_grad _test_different_graph_across_ranks find_unused_parameters=False static_graph=False ToyModel nn Module __init__ rank super __init__ lin = nn Linear bias=False lin = nn Linear bias=False rank = rank forward x rank == lin F relu lin x F relu lin x torch manual_seed torch cuda set_device rank model = ToyModel rank cuda rank ddp_model = torch nn parallel DistributedDataParallel model device_ids= rank find_unused_parameters=find_unused_parameters gradient_as_bucket_view=True static_graph=static_graph random_input = torch randn device=self rank _ range out = ddp_model random_input loss = out sum loss backward ddp_model require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_different_graph_across_ranks base_model = _test_different_graph_across_ranks find_unused_parameters=True assertFalse base_model _get_ddp_logging_data get has_rebuilt_buckets static_model = _test_different_graph_across_ranks static_graph=True assertTrue static_model _get_ddp_logging_data get has_rebuilt_buckets i j zip base_model parameters static_model parameters strict=True assertEqual i j require_backend_is_available gloo skip_but_pass_in_sandcastle_if IS_MACOS IS_WINDOWS MacOS uses uv transport which does have robust error handling tcp transport test_monitored_barrier_gloo tensors = torch ones rank Kick off some allreduce work all ranks _ range dist all_reduce torch cat tensors Run monitored barrier ensure passes timeout = timedelta seconds= dist monitored_barrier timeout=timeout Check monitored_barrier success wait_all_ranks=True _ range dist all_reduce torch cat tensors dist monitored_barrier timeout=timeout wait_all_ranks=True All ranks besides call into barrier rank should report failure while others report gloo error failed_rank = src_rank = rank == src_rank assertRaisesRegex RuntimeError f Rank failed_rank failed pass monitoredBarrier dist monitored_barrier timeout=timeout rank = failed_rank Other ranks should pass barrier since rank failed err_regex = f Rank rank successfully reached monitoredBarrier f received errors while waiting send recv rank f src_rank assertRaisesRegex RuntimeError err_regex dist monitored_barrier timeout=timeout We need barrier since otherwise failed_rank exits too early cause timeout _barrier timeout= require_backend_is_available gloo test_monitored_barrier_gloo_subgroup Tests monitored_barrier works expected non-default process groups failed_rank = timeout = subgroup = dist new_group ranks= rank == failed_rank rank == assertRaisesRegex RuntimeError f Rank failed_rank failed pass monitoredBarrier dist monitored_barrier subgroup timeout Other ranks call into monitored_barrier should noop because they part subgroup Verify there no errors here dist monitored_barrier subgroup timeout _test_monitored_barrier_allreduce_hang wait_all_ranks tests expected behavior when nonzero rank hangs nccl_pg = dist new_group ranks=list range int world_size provide sufficient timeout so communicators can initialized ctor timeout=timedelta seconds= backend=dist Backend NCCL gloo_pg = dist new_group ranks=list range int world_size backend=dist Backend GLOO tensors = torch ones device=self rank rank Let all ranks call allreduce first set up communicators etc Directly simulating error here will run into store issue described https github com pytorch pytorch issues nccl_pg allreduce tensors wait timedelta seconds= All ranks besides call into allreduce This simulate desync across world where some ranks call into monitored_barrier others stuck collective comm In practice we don t need TORCH_NCCL_BLOCKING_WAIT we use test ensure exits cleanly rank = Can get different errors here depending whether gloo-based wrapper PG enabled since wrapper pg will fail collective synchronization check actually call into nccl pg dist get_debug_level == dist DebugLevel DETAIL err_regex = Timed out waiting err_regex = caught collective operation timeout assertRaisesRegex RuntimeError err_regex nccl_pg allreduce tensors wait timedelta seconds= Rank should report first order timed out rank all ranks depending wait_all_ranks flag passed into monitored_barrier wait_all_ranks rank_str = join str i i range int world_size err_regex = f Ranks rank_str failed pass monitoredBarrier expected_first_fail_rank = err_regex = f Rank expected_first_fail_rank failed pass monitoredBarrier monitored_barrier_timeout_seconds = timedelta seconds= assertRaisesRegex RuntimeError err_regex gloo_pg monitored_barrier monitored_barrier_timeout_seconds wait_all_ranks=wait_all_ranks _barrier timeout= with_nccl_blocking_wait require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu int os environ WORLD_SIZE test_monitored_barrier_allreduce_hang tests expected behavior when nonzero rank hangs we want report first timed out rank _test_monitored_barrier_allreduce_hang wait_all_ranks=False with_nccl_blocking_wait require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu int os environ WORLD_SIZE test_monitored_barrier_allreduce_hang_wait_all_ranks Need disable TORCH_NCCL_DUMP_ON_TIMEOUT otherwise test times out os environ TORCH_NCCL_DUMP_ON_TIMEOUT = tests expected behavior when nonzero rank hangs we want report all timed out ranks _test_monitored_barrier_allreduce_hang wait_all_ranks=True require_backend_is_available gloo test_monitored_barrier_gloo_rank_ _timeout tests error when rank exhausts its given timeout process_group = dist new_group ranks=list range int world_size timeout = timedelta seconds= rank == assertRaisesRegex RuntimeError f Rank rank timed out monitoredBarrier process_group monitored_barrier timeout require_backend_is_available gloo skip_if_small_worldsize skip_but_pass_in_sandcastle_if IS_MACOS IS_WINDOWS MacOS uses uv transport which does have robust error handling tcp transport test_monitored_barrier_failure_order Ensure first sorted order rank reported when multiple ranks fail pass monitored_barrier TODO Provide ability wait report all failed ranks expected_first_failed_rank = timeout = timedelta seconds= src_rank = rank == src_rank assertRaisesRegex RuntimeError f Rank expected_first_failed_rank dist monitored_barrier timeout=timeout rank == err_regex = f Rank rank successfully reached monitoredBarrier f received errors while waiting send recv rank f src_rank assertRaisesRegex RuntimeError err_regex dist monitored_barrier timeout=timeout require_backend_is_available gloo skip_if_small_worldsize test_monitored_barrier_wait_all_ranks Tests simple case where rank does call into monitored barrier verifies all ranks reported rank rank == timeout = timedelta seconds= rank_str = join str i i range int world_size err_regex = f Ranks rank_str failed pass monitoredBarrier assertRaisesRegex RuntimeError err_regex dist monitored_barrier timeout=timeout wait_all_ranks=True require_backend_is_available DistTestCases backend_feature gpu with_dist_debug_levels levels= INFO skip_if_lt_x_gpu test_ddp_build_debug_param_to_name_mapping model = TwoLinLayerNet net = torch nn parallel DistributedDataParallel model cuda rank device_ids= rank expected_mapping = weight b weight net_params _ = net _build_params_for_reducer param_to_name_mapping = net _build_debug_param_to_name_mapping net_params assertDictEqual expected_mapping param_to_name_mapping Test when DDP used ignored parameters model = TwoLinLayerNet Parameters ignore format module_name param_name params_to_ignore = weight torch nn parallel DistributedDataParallel _set_params_and_buffers_to_ignore_for_model model params_to_ignore net = torch nn parallel DistributedDataParallel model cuda rank device_ids= rank expected_mapping = b weight net_params _ = net _build_params_for_reducer param_to_name_mapping = net _build_debug_param_to_name_mapping net_params assertDictEqual expected_mapping param_to_name_mapping Test errors raised when DDP module parameters mismatch This generally indicates bug DDP expected happen user applications model = TwoLinLayerNet net = torch nn parallel DistributedDataParallel model cuda rank device_ids= rank net_params _ = net _build_params_for_reducer rank == print type net_params net_params extend torch nn Parameter torch ones torch nn Parameter torch ones assertRaisesRegex ValueError Expected param name mapping net _build_debug_param_to_name_mapping net_params net_params = net_params - assertRaisesRegex ValueError Param name net _build_debug_param_to_name_mapping net_params net_params extend torch nn Parameter torch ones torch nn Parameter torch ones skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel with_dist_debug_levels levels= INFO skip_if_lt_x_gpu test_ddp_build_debug_param_to_name_mapping_requires_grad Net nn Module __init__ - None super __init__ lin = nn Linear Is tracked DDP should show up param name mapping lin bias requires_grad_ False forward x lin x model = Net net = torch nn parallel DistributedDataParallel model cuda rank device_ids= rank expected_mapping = lin weight net_params _ = net _build_params_for_reducer param_to_name_mapping = net _build_debug_param_to_name_mapping net_params assertEqual param_to_name_mapping expected_mapping _test_ddp_multiple_nested_unused_params_error ignore_sparse debug_mode_off = dist get_debug_level == dist DebugLevel OFF SubModule nn Module __init__ - None super __init__ embedding_net = EmbeddingNetDifferentParams lin = TwoLinLayerNet bn = BatchNormNet lin_layer = nn Linear bias=False forward x x = bn x x = lin_layer x x = lin x lin b param unused EmbeddingNetDifferentParams entirely unused embedding_net embedding embedding_net lin unused x MyModel nn Module __init__ - None super __init__ sub_module = SubModule forward x sub_module x model = MyModel sparse_embedding_fqns = ignore_sparse module_name module model named_modules module == model sub_module embedding_net embedding parameter_name _param module named_parameters recurse=False fqn = f module_name parameter_name sparse_embedding_fqns append fqn torch nn parallel DistributedDataParallel _set_params_and_buffers_to_ignore_for_model model sparse_embedding_fqns unused_modules = model sub_module embedding_net lin model sub_module lin b unused_modules = list model sub_module embedding_net modules + model sub_module lin b expected_unused_param_fqns = used_param_fqns = Validate these don t mistakenly show up fqn_to_param_index = index = module_name module model named_modules parameter_name _param module named_parameters recurse=False fqn = f module_name parameter_name fqn_to_param_index fqn = index fqn sparse_embedding_fqns index += module unused_modules expected_unused_param_fqns append fqn ignore_sparse module = model sub_module embedding_net embedding used_param_fqns append fqn net = torch nn parallel DistributedDataParallel model cuda rank device_ids= rank batch dim = inp = torch ones batch dim i range i == out = net inp loss = out sum loss backward try out = net inp loss = out sum loss backward except RuntimeError e e = str e unused_param_substr = e e find did receive grad Validate each unused param fully qualified name shows up error logs We do instead constructing joined string since order parameters can different Reducer In addition validate param indices show up well unused_param_fqn expected_unused_param_fqns assertTrue unused_param_fqn unused_param_substr debug_mode_off assertTrue str fqn_to_param_index unused_param_fqn unused_param_substr f Did find index fqn_to_param_index unused_param_fqn unused_param_fqn Validate used param fqns don t show up error logs used_param_fqn used_param_fqns assertFalse used_param_fqn unused_param_substr Validate ignored param fqns don t show up unused since DDP does track them sparse_param_fqn sparse_embedding_fqns assertFalse sparse_param_fqn unused_param_substr assertTrue False Expected error raised with_dist_debug_levels levels= OFF INFO DETAIL require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_ddp_multiple_nested_unused_params_error _test_ddp_multiple_nested_unused_params_error ignore_sparse=False with_dist_debug_levels levels= OFF INFO DETAIL require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_ddp_multiple_nested_unused_params_err_ignore_params Tests unused parameter reporting when DDP configured ignore certain parameters _test_ddp_multiple_nested_unused_params_error ignore_sparse=True skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_lt_x_gpu test_ddp_inference tests DDP module can run single node no_grad eval setting there no hang rank = rank torch cuda set_device rank model = Net cuda local_model = copy deepcopy model model = torch nn parallel DistributedDataParallel model device_ids= rank syncbn_model = nn SyncBatchNorm momentum= track_running_stats=False cuda local_syncbn_model = copy deepcopy syncbn_model syncbn_model = torch nn parallel DistributedDataParallel syncbn_model device_ids= rank inp = torch randn device=rank inp_syncbn = torch randn device=rank tests = model local_model inp syncbn_model local_syncbn_model inp_syncbn test tests test_model test_local_model test_inp = test rank == test_model eval test_local_model eval _ range assertEqual test_model test_inp test_local_model test_inp Barrier since only rank runs inference Test should much faster than s avoid flakiness _barrier timeout= skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel skip_if_lt_x_gpu unittest skip Test failing see https github com pytorch pytorch pull test_ddp_sync_bn_training_vs_eval rank = rank torch cuda set_device rank Need set track_running_stats=False when track_running_stats=True bn_training False sync could occur eval model model = nn SyncBatchNorm momentum= track_running_stats=False cuda rank model = torch nn parallel DistributedDataParallel model device_ids= rank Test sync occurs training mode torch autograd profiler profile prof _ range inp = torch randn cuda rank out = model inp loss = out sum loss backward SyncBN allgathers stats across all ranks so verify call all_gather profiler BACKEND == nccl all_gather_calls = get_profiling_event _all_gather_base prof all_gather_calls = get_profiling_event all_gather prof assertNotEqual all_gather_calls Only do inference one rank If SyncBN did collective stats sync would hang error model_inference = model module rank == model_inference eval torch autograd profiler profile prof _ range inp = torch randn cuda rank out = model_inference inp loss = out sum loss backward Ensure sync does occur eval mode BACKEND == nccl all_gather_calls = get_profiling_event _all_gather_base prof all_gather_calls = get_profiling_event all_gather prof assertEqual all_gather_calls skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_python_error_logged Most python exceptions DDP raised during init before reducer constructed so we don t have logger those cases However below one example where python error thrown after reducer constructed model = TwoLinLayerNet cuda rank model = torch nn parallel DistributedDataParallel model device_ids= rank expected_err = must callable assertRaisesRegex TypeError expected_err model register_comm_hook verify_ddp_error_logged model expected_err skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_static_graph_nested_types Tests static graph training when outputs just tensors can nested tuple list dict etc rank = rank torch cuda set_device rank NestedOutputModule torch nn Module __init__ - None super __init__ lin = nn Linear bias=False forward inp output_type output_type == tuple lin inp lin inp lin inp output_type == list lin inp lin inp lin inp output_type == dict lin inp b c lin inp get_loss model_output loss = isinstance model_output torch Tensor model_output sum isinstance model_output dict value model_output values loss += get_loss value isinstance model_output tuple list x model_output loss += get_loss x raise ValueError f Unknown model output type type model_output loss model = NestedOutputModule cuda rank model_static_graph = copy deepcopy model model = torch nn parallel DistributedDataParallel model device_ids= rank model_static_graph = torch nn parallel DistributedDataParallel model device_ids= rank static_graph=True inp = torch randn type_mapping = list list tuple tuple dict dict output_type type_mapping keys _ range out = model inp output_type=output_type loss = get_loss out loss backward _model_step model out_static = model_static_graph inp output_type=output_type assertTrue isinstance out_static type_mapping output_type loss_static = get_loss out_static loss_static backward _model_step model_static_graph p p_static zip model parameters model_static_graph parameters strict=True assertEqual p p_static skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_returns_tensor_with_no_grad Tests case where module returns tensor does require grad torch cuda set_device rank MyModel nn Module __init__ - None super __init__ fc = nn Linear bias=False fc = nn Linear bias=False forward x x = fc F relu fc x y = x clone x = x detach assert x requires_grad x y model = MyModel rank inp = torch randn device=self rank find_unused static_graph itertools product True False True False ddp = DistributedDataParallel model device_ids= rank output_device=self rank find_unused_parameters=find_unused static_graph=static_graph _ range out = ddp inp assertFalse out requires_grad o = out + out sum o backward skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_detect_ddp_is_actually_static ToyModel nn Module __init__ - None super __init__ net = nn Linear bias=False net = nn Linear forward x find_unused dynamic find_unused dynamic net net x net x net net x Set unused parameters don t change across iterations torch cuda set_device rank model = ToyModel cuda find_unused True False ddp = torch nn parallel DistributedDataParallel model device_ids= rank find_unused_parameters=find_unused inp = torch randn device= cuda _ range out = ddp inp find_unused=find_unused dynamic=False loss = out sum loss backward assertTrue ddp reducer _ddp_graph_static Set unused parameters dynamically change ddp = torch nn parallel DistributedDataParallel model device_ids= rank find_unused_parameters=True inp = torch randn device= cuda i range out = ddp inp find_unused=True dynamic=i == loss = out sum loss backward assertFalse ddp reducer _ddp_graph_static _test_ddp_new_tensor_in_fwd static_graph Test https github com pytorch pytorch issues MyModel nn Module __init__ - None super __init__ fc = nn Linear bias=False fc = nn Linear bias=False device = fc weight device __init_opt opt = torch randn device=self device opt forward x opt_ opt_ opt_nested x = F relu fc x x = fc x opt_ None opt_ = __init_opt opt_ None opt_ = __init_opt opt_nested None torch is_tensor opt_nested opt_nested = __init_opt Test multiple tensors well newly created tensors within struct x opt_ opt_ tensor opt_nested model = MyModel rank find_unused True False ddp = DistributedDataParallel model device_ids= rank output_device=self rank broadcast_buffers=False find_unused_parameters=find_unused static_graph=static_graph opt = None _ range i range ddp zero_grad x = torch randn device=self rank out opt opt opt = ddp x opt_ =opt opt_ =opt opt_nested=opt i range len opt torch is_tensor opt i assertEqual opt i grad_fn None assertEqual opt i tensor grad_fn None out mean backward skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_new_tensor_in_fwd _test_ddp_new_tensor_in_fwd static_graph=False skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_new_tensor_in_fwd_static_graph _test_ddp_new_tensor_in_fwd static_graph=True _test_ddp_buffer_hook_allreduce return_futures rank = rank torch cuda set_device rank torch manual_seed rank torch cuda manual_seed rank buffer_comm_hook ddp named_buffers buffers = buffer _ buffer named_buffers items futs = dist all_reduce buffer group=ddp process_group async_op=True get_future buffer buffers return_futures futs torch futures collect_all futs wait hook_pre_fwd = torch nn parallel distributed _BufferCommHookLocation PRE_FORWARD hook_post_fwd = torch nn parallel distributed _BufferCommHookLocation POST_FORWARD hook_run_location hook_pre_fwd hook_post_fwd model = NetWithBuffers cuda rank model_ddp = torch nn parallel DistributedDataParallel model device_ids= rank model_ddp _register_buffer_comm_hook model_ddp buffer_comm_hook hook_run_location model_ddp_no_hook = torch nn parallel DistributedDataParallel copy deepcopy model device_ids= rank broadcast_buffers=False inp = torch randn device=rank _ range loss_hook = model_ddp inp sum Since buffer reduction done pre-forward simulate no hook case here Simulate allreduce appropriately depending hook location hook_run_location == hook_pre_fwd model_no_hook_buffers = list model_ddp_no_hook module buffers tensor model_no_hook_buffers dist all_reduce tensor loss_no_hook = model_ddp_no_hook inp sum hook_run_location == hook_post_fwd model_no_hook_buffers = list model_ddp_no_hook module buffers tensor model_no_hook_buffers dist all_reduce tensor torch cuda synchronize return_futures they only awaited DDP end backwards pass maximum overlap return_futures _verify_buffers_equal model_ddp model_ddp_no_hook loss_hook backward loss_no_hook backward Note when custom hooks futures comparison expected work when hook run location pre-forward pass This because hook does async communication forward pass modifies buffer without appropriate synchronization Therefore returning futures custom buffer hooks advised set hook run location post forward return_futures hook_run_location == hook_post_fwd _verify_buffers_equal model_ddp model_ddp_no_hook dist barrier skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_buffer_hook_allreduce_return_future _test_ddp_buffer_hook_allreduce return_futures=True skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_buffer_hook_allreduce _test_ddp_buffer_hook_allreduce return_futures=False skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_broadcast_buffer_via_hook test _distributed_broadcast_coalesced via registered hook equivalent DDP s default broadcast coalesced rank = rank torch cuda set_device rank torch manual_seed rank torch cuda manual_seed rank buffer_comm_hook ddp named_buffers named_buffers Dict str Tensor representing mapping buffer name buffer buffers = buffer _ buffer named_buffers items ddp _default_broadcast_coalesced buffers model = NetWithBuffers cuda rank model_ddp = torch nn parallel DistributedDataParallel model device_ids= rank model_ddp _register_buffer_comm_hook model_ddp buffer_comm_hook model_ddp_no_hook = torch nn parallel DistributedDataParallel copy deepcopy model device_ids= rank inp = torch randn device=rank _ range loss_hook = model_ddp inp sum loss_no_hook = model_ddp_no_hook inp sum _verify_buffers_equal model_ddp model_ddp_no_hook loss_hook backward loss_no_hook backward skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_remove_autograd_hooks SimulateError torch autograd Function staticmethod forward ctx input input staticmethod backward ctx grad_output raise RuntimeError MyModel nn Module __init__ device super __init__ error = True fc = nn Linear cuda device forward inp error fc SimulateError apply inp fc inp Run error trigger backward pass marks fc being marked ready If we don t remove autograd hooks before running below would fail old autograd hook model = MyModel rank input = torch rand requires_grad=True cuda rank model_ddp = torch nn parallel DistributedDataParallel model device_ids= rank assertRaises RuntimeError model_ddp input sum backward Remove autograd hooks old instance model_ddp _remove_autograd_hooks Try another DDP instance without error now model error = False model_ddp = torch nn parallel DistributedDataParallel model device_ids= rank model_ddp input sum backward skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel unittest skip Test failing tracking issue https github com pytorch pytorch issues test_ddp_has_finalized dataclass MyClass obj torch Tensor MyModel nn Module __init__ rank super __init__ rank = rank fc = nn Linear cuda rank fc = nn Linear cuda rank forward inp rank == fc inp MyClass fc inp fc inp fc inp model = MyModel rank input = torch rand requires_grad=True cuda rank ddp = torch nn parallel DistributedDataParallel model device_ids= rank find_unused_parameters=True bucket_cap_mb= One bucket per parameter rank == out _ = ddp input out sum backward out out = ddp input out sum + out sum backward rank == assertRaisesRegex RuntimeError Expected have finished reduction prior iteration ddp _check_reducer_finalized assertRaisesRegex RuntimeError Expected have finished reduction prior iteration ddp input ddp _check_reducer_finalized ddp input The set test_ddp_update_process_group below failed after upgrading CI GPUs GPUs Commented out now Test purpose needs better documentation _run_ddp_update_process_group new_pg get_num_torch_recompiles guard_failures = torch _dynamo utils guard_failures num_recompiles = len guard_failures code code guard_failures len num_recompiles == max num_recompiles SimulateError torch autograd Function staticmethod forward ctx input input staticmethod backward ctx grad_output raise RuntimeError MyModel torch nn Module __init__ device super __init__ MB multiple buckets fc = torch nn Linear cuda device fc = torch nn Linear cuda device fc = torch nn Linear cuda device forward inp error error fc fc fc SimulateError apply inp fc fc fc inp input = torch rand requires_grad=True cuda rank ddp = torch nn parallel DistributedDataParallel MyModel rank device_ids= rank find_unused_parameters=True bucket_cap_mb= model = torch compile ddp run_iteration Run regular iteration out = model input error=False out sum backward torch cuda synchronize Run error assertRaises RuntimeError out = model input error=True out sum backward torch cuda synchronize run_iteration assert == get_num_torch_recompiles new_pg Now reduce world_size run iteration group_size_ = dist new_group ranks= ddp _update_process_group group_size_ rank run_iteration Increase world size run iteration group_size_ = dist new_group ranks= ddp _update_process_group group_size_ rank run_iteration Back default size ddp _update_process_group _get_default_group run_iteration Create default pg smaller size dist destroy_process_group rank dist init_process_group init_method=self init_method backend=BACKEND world_size= rank=self rank - timeout=timedelta seconds=default_pg_timeout ddp _update_process_group _get_default_group run_iteration dist destroy_process_group Need barrier here ensure ranks done _barrier wait_for= Need init pg again _barrier succeed dist init_process_group init_method=self init_method backend=BACKEND world_size= rank=self rank timeout=timedelta seconds=default_pg_timeout Validate no more recompiles assert == get_num_torch_recompiles skip_if_lt_x_gpu require_world_size skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_update_process_group_new_group _run_ddp_update_process_group new_pg=True skip_if_lt_x_gpu require_world_size skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_update_process_group_default_group _run_ddp_update_process_group new_pg=False skip_if_lt_x_gpu require_world_size skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_update_process_group_grad_undefined SimulateError torch autograd Function staticmethod forward ctx input input staticmethod backward ctx grad_output raise RuntimeError MyModel torch nn Module __init__ device super __init__ fc = torch nn Linear cuda device fc = torch nn Linear cuda device fc = torch nn Linear cuda device forward inp error error fc fc fc SimulateError apply inp fc fc inp input = torch rand requires_grad=True cuda rank ddp = torch nn parallel DistributedDataParallel MyModel rank device_ids= rank find_unused_parameters=True bucket_cap_mb= try ddp input True sum backward except RuntimeError ddp _update_process_group _get_default_group Reset grads param ddp parameters param grad = None Run ddp again ddp input False sum backward skip_if_lt_x_gpu require_world_size skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_update_process_group_no_find_unused ddp = torch nn parallel DistributedDataParallel torch nn Linear cuda rank device_ids= rank find_unused_parameters=False ddp _update_process_group _get_default_group skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_broadcast_buffer rank = rank torch cuda set_device rank torch manual_seed rank torch cuda manual_seed rank NetWithBuffers nn Module __init__ - None super __init__ = nn Linear bias=False b = nn Linear bias=False register_buffer buffer torch randn forward x b x model = NetWithBuffers cuda rank model_ddp = torch nn parallel DistributedDataParallel model device_ids= rank inp = torch randn device=rank _ range rank == model_ddp module buffer = model_ddp module buffer + loss = model_ddp inp sum loss backward Ensure all buffers synchronized bufs = torch empty_like model_ddp module buffer _ range dist get_world_size dist all_gather bufs model_ddp module buffer rank_ _buf = bufs buf bufs assertEqual rank_ _buf buf skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND = nccl BACKEND = gloo Only Nccl Gloo backend support DistributedDataParallel test_static_graph_multi_forward Net nn Module __init__ - None super __init__ lin = nn Linear relu = nn ReLU forward x relu lin x torch cuda set_device rank torch manual_seed rank + model = Net cuda rank local_model = copy deepcopy model model = torch nn parallel DistributedDataParallel model device_ids= rank static_graph=True inp = torch ones device= cuda _ range model zero_grad local_model zero_grad = model inp b = model inp loss = sum + b sum loss backward Grads should equal local model ran through inp ` world_size ` times averaged grads rank == inp_clone = inp clone iters = dist get_world_size _ range iters = local_model inp_clone b = local_model inp_clone loss = sum + b sum loss backward p local_model parameters p grad data = p grad iters p_ddp p_local zip model parameters local_model parameters strict=True assertTrue torch allclose p_ddp grad p_local grad f p_ddp grad vs p_local grad dist barrier skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND = nccl BACKEND = gloo Only Nccl Gloo backend support DistributedDataParallel test_sync_bn_logged model = BatchNormNet rank = rank single gpu training setup model_gpu = model cuda rank no_sync_bn = torch nn parallel DistributedDataParallel copy deepcopy model_gpu device_ids= rank ddp_logging_data = no_sync_bn _get_ddp_logging_data sync_bn_logged = ddp_logging_data get has_sync_bn True assertFalse sync_bn_logged model_DDP = nn SyncBatchNorm convert_sync_batchnorm model_gpu model_DDP = torch nn parallel DistributedDataParallel model_DDP device_ids= rank ddp_logging_data = model_DDP _get_ddp_logging_data sync_bn_logged = ddp_logging_data get has_sync_bn False assertTrue sync_bn_logged skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_stateless_api_with_ddp MockModule torch nn Module __init__ - None super __init__ l = torch nn Linear buffer = torch ones register_buffer buffer buffer forward x l x + buffer device = rank module = MockModule device module = torch nn parallel DistributedDataParallel module device_ids= device x = torch rand device weight = torch tensor device=device requires_grad=True bias = torch tensor device=device requires_grad=True buffer = torch tensor device=device parameters = module l weight weight module l bias bias module buffer buffer prev_weight = module module l weight clone prev_buffer = module module buffer clone res = torch func functional_call module parameters x assertEqual x res check weight remain unmodified cur_weight = module module l weight cur_buffer = module module buffer assertEqual cur_weight prev_weight assertEqual cur_buffer prev_buffer run backward pass check gradients res backward assertIsNotNone weight grad assertIsNotNone bias grad Gradient calculated module stated buffers assertIsNone buffer grad assertIsNone module module l weight grad assertIsNone module module l bias grad assertIsNone module module buffer grad require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_ddp_forward_backward_hook DummyTestModel nn Module __init__ - None super __init__ torch manual_seed fc = nn Linear forward x fc x relu_hook module input nn functional relu input gelu_hook module _input output nn functional gelu output celu_hook module _input output nn functional celu output local_model = DummyTestModel ddp_model = DummyTestModel local_model fc register_forward_pre_hook relu_hook local_model fc register_forward_hook gelu_hook ddp_model fc register_forward_pre_hook relu_hook ddp_model fc register_forward_hook gelu_hook local_model fc register_backward_hook celu_hook ddp_model fc register_backward_hook celu_hook ddp_model = DistributedDataParallel ddp_model rank device_ids= rank input_data = torch rand output_local = local_model input_data output_ddp = ddp_model input_data rank assertEqual output_local output_ddp output_local sum backward output_ddp sum backward ddp_grads = p grad p ddp_model parameters assertEqual ddp_grads local_model fc weight grad assertEqual ddp_grads local_model fc bias grad _test_hook_pickling hook hook_state torch manual_seed learning_rate = chkpt_file = tempfile gettempdir + checkpoint pt rank = rank input = torch randn device=rank target = torch randn device=rank net = torch nn Linear rank ddp_model = DistributedDataParallel copy deepcopy net device_ids= rank dummy_ddp_model = DistributedDataParallel copy deepcopy net device_ids= rank optimizer = torch optim SGD ddp_model parameters lr=learning_rate ddp_model register_comm_hook hook_state hook ddp_model train _ range optimizer zero_grad out = ddp_model input loss = F mse_loss out target loss backward optimizer step state = state_dict ddp_model state_dict comm_hook hook comm_hook_state hook_state rank == assertLogs torch distributed captured torch save state chkpt_file Check logger has only one entry assertEqual len captured records Check logger has expected entry assertEqual captured records getMessage NOTE Process group serializable excluded saved state dist barrier map_location = cuda f cuda rank d assertLogs torch distributed captured checkpoint = torch load chkpt_file map_location=map_location Check logger has only one entry assertEqual len captured records Check logger has expected entry assertEqual captured records getMessage NOTE Process group will set default group i e world size \ If different group desired please set ` process_group ` after PowerSGD state loaded dummy_ddp_model load_state_dict checkpoint state_dict dummy_hook = checkpoint comm_hook dummy_hook_state = checkpoint comm_hook_state dummy_optimizer = torch optim SGD dummy_ddp_model parameters lr=learning_rate Check loaded function correct assertEqual dummy_hook __qualname__ hook __qualname__ Check all slots keys restored correctly assertEqual hook_state __slots__ dummy_hook_state __slots__ Check all slots attributes restored correctly Excluding ` ` process_group ` ` ` ` rng ` ` entry dummy_hook_state __slots__ entry = process_group entry = rng assertEqual getattr dummy_hook_state entry getattr hook_state entry Check ` ` process_group ` ` set default assertEqual dummy_hook_state process_group _get_default_group Check random state restored properly ` ` np random RandomState get_state ` ` returns tuple entries ` ` bit_generator ` ` - str ` ` state key ` ` - ndarray dtype uint ` ` state pos ` ` - int ` ` has_gauss ` ` - int ` ` gauss ` ` - float refer https github com numpy numpy blob aad bc fbcc eea f d b numpy random mtrand pyi To make sure random state restored properly all entries should equal original entry entry zip hook_state rng get_state dummy_hook_state rng get_state strict=True np testing assert_array_equal entry entry dummy_ddp_model register_comm_hook dummy_hook_state dummy_hook dummy_ddp_model train _ range optimizer zero_grad dummy_optimizer zero_grad out_origin = ddp_model input out_dummy = dummy_ddp_model input loss_origin = F mse_loss out_origin target loss_dummy = F mse_loss out_dummy target loss_origin backward loss_dummy backward optimizer step dummy_optimizer step Check gradients after epochs same orig_param dummy_param zip ddp_model parameters dummy_ddp_model parameters strict=True assertEqual orig_param grad dummy_param grad dist barrier rank == os remove chkpt_file skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature cuda f The BACKEND backend does support DDP communication hook CUDA devices skip_if_lt_x_gpu int os environ WORLD_SIZE skip_but_pass_in_sandcastle_if True Skipped due flakiness test_ddp_hook_pickling_powerSGD hook = powerSGD powerSGD_hook powersgd_state = powerSGD PowerSGDState process_group=None matrix_approximation_rank= start_powerSGD_iter= _test_hook_pickling hook powersgd_state require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_ddp_device_mesh_initialization Test DDP device_mesh initialization world_size = int os environ WORLD_SIZE torch distributed device_mesh init_device_mesh device_mesh = init_device_mesh cuda world_size pg = _get_default_group torch cuda set_device rank model = TwoLinLayerNet cuda ddp_model = torch nn parallel DistributedDataParallel model device_mesh=device_mesh assertEqual ddp_model device_mesh device_mesh assertRaisesRegex RuntimeError Cannot specify both process_group device_mesh arguments ddp_model = torch nn parallel DistributedDataParallel model process_group=pg device_mesh=device_mesh assertRaisesRegex RuntimeError Only D device mesh supported device_mesh = init_device_mesh cuda world_size ddp_model = torch nn parallel DistributedDataParallel model device_mesh=device_mesh skip_if_lt_x_gpu require_world_size skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_compile_static_graph Tests DDP works torch compile when static_graph=True model = torch nn Linear cuda rank model_clone = copy deepcopy model ddp = torch nn parallel DistributedDataParallel model device_ids= rank ddp_static = torch nn parallel DistributedDataParallel model_clone device_ids= rank static_graph=True ddp = torch compile ddp ddp_static = torch compile ddp_static input = torch rand cuda rank verify output gradient parity _ range out_ddp = ddp input sum out_ddp_static = ddp_static input sum assertEqual out_ddp out_ddp_static out_ddp backward out_ddp_static backward p p zip ddp parameters ddp_static parameters strict=True assertEqual p grad p grad skip_if_lt_x_gpu require_world_size skip_but_pass_in_sandcastle_if BACKEND DistTestCases backend_feature ddp f The BACKEND backend does support DistributedDataParallel test_ddp_sink_noclone Tests we can configure DDP avoid clone OpPatcher TorchDispatchMode __torch_dispatch__ func types args= kwargs=None func_packet = func _overloadpacket func_packet == torch ops aten clone raise RuntimeError clone encountered kwargs = kwargs kwargs func args kwargs MyModel torch nn Module __init__ - None super __init__ fc = torch nn Linear forward input fc input model = MyModel cuda rank ddp = torch nn parallel DistributedDataParallel model device_ids= rank find_unused_parameters=True ddp _set_ddp_sink_clone False input = torch rand cuda rank OpPatcher ddp input sum backward _test_skip_all_reduce_unused_parameters find_unused_parameters=False static_graph=False skip_all_reduce_unused_params=False LargeNet nn Module __init__ - None super __init__ fc = nn Linear bias=False fc unused fc = nn Linear bias=False forward x y = fc x y torch manual_seed torch cuda set_device rank model = LargeNet cuda rank ddp_model = torch nn parallel DistributedDataParallel model find_unused_parameters=find_unused_parameters static_graph=static_graph bucket_cap_mb= skip_all_reduce_unused_params=skip_all_reduce_unused_params random_input = torch randn device=self rank _ range out = ddp_model random_input loss = out sum loss backward ddp_model require_backend_is_available DistTestCases backend_feature gpu skip_if_lt_x_gpu test_skip_all_reduce_unused_parameters base_model = _test_skip_all_reduce_unused_parameters find_unused_parameters=True static_graph=False test_model_ = _test_skip_all_reduce_unused_parameters find_unused_parameters=True static_graph=False skip_all_reduce_unused_params=True assertEqual base_model _get_ddp_logging_data get num_buckets_reduced assertEqual test_model_ _get_ddp_logging_data get num_buckets_reduced i j zip base_model parameters test_model_ parameters strict=True assertEqual i j instantiate_parametrized_tests DistributedTest _DistTestBase