mypy ignore-errors Torch torch jit annotations BroadcastingList BroadcastingList noqa F torch nn functional F torch torch cuda torch jit torch jit _logging torch jit frontend torch testing _internal common_nn module_tests get_new_module_tests torch testing _internal common_utils is_iterable_of_tensors noncontiguous_like collections copy deepcopy typing Any Union math noqa F Testing utils torch inf assert torch get_default_dtype == torch float L = M = S = unpack_variables args isinstance args tuple tuple unpack_variables elem elem args args dont_convert tuple __slots__ = non_differentiable = collections namedtuple non_differentiable tensor create_input call_args requires_grad=True non_contiguous=False call_kwargs=None dtype=torch float device=None isinstance call_args tuple call_args = call_args map_arg arg maybe_non_contig tensor non_contiguous tensor numel tensor clone noncontiguous_like tensor conjugate tensor tensor conj isinstance arg torch Size dont_convert arg isinstance arg tuple len arg == var = conjugate torch randn dtype=dtype device=device var requires_grad = requires_grad var isinstance arg tuple isinstance arg torch Tensor conjugate maybe_non_contig torch randn arg dtype=dtype device=device requires_grad_ requires_grad double check casting isinstance arg non_differentiable isinstance arg tensor torch Tensor conjugate maybe_non_contig arg tensor device=device conjugate maybe_non_contig arg tensor device=device isinstance arg torch Tensor arg is_complex = dtype is_complex raise RuntimeError User provided tensor real test runs complex dtype which supported now NOTE We do clone after detach here because we need able change size storage v afterwards v = conjugate maybe_non_contig arg detach device=device clone v requires_grad = requires_grad v is_floating_point v is_complex v callable arg map_arg arg dtype=dtype device=device arg args_out = tuple map_arg arg arg call_args kwargs_out = k map_arg v k v call_kwargs items call_kwargs args_out kwargs_out NB JIT script tests all nn functional interfaces script mode does support in_place operations yet so no inplace operation tests added removed all deprecated functions method name input size constructing fn args tuple represents shape tensor arg test variant name will used test name suffix inplace skips grad tests optional True nonfusible_nodes fusible_nodes autodiff optional fn determine test should skipped optional fn mapping output part should gradcheck ed optional kwargs function optional get_nn_functional_tests nn_functional_tests = conv d S S S S S S conv d S S S S S S S S conv d S S S S S S S S S S conv_transpose d S S S S S S conv_transpose d S S S S S S S S conv_transpose d S S S S S S S S S S conv_tbc S S S S S S S avg_pool d S S S avg_pool d S S S S True avg_pool d S S S S S fractional_max_pool d S S S S max_pool d S S S max_pool d S S S False True with_indices max_pool d S S S S True aten max_pool d_with_indices max_pool d S S S S False True with_indices True aten max_pool d_with_indices max_pool d S S S S S max_unpool d torch tensor torch tensor max_unpool d torch tensor torch tensor max_unpool d torch tensor torch tensor lp_pool d S S S lp_pool d S S S S lp_pool d S S S S S adaptive_max_pool d S S S adaptive_max_pool d S S S S adaptive_max_pool d S S S S S adaptive_avg_pool d S S S True adaptive_avg_pool d S S S S True adaptive_avg_pool d S S S S S True dropout S S S True aten native_dropout alpha_dropout S S S dropout d S S S dropout d S S S S batched dropout d S S S S dropout d S S S S S batched feature_alpha_dropout S S S threshold S S S True threshold S S S True inplace relu S S S True relu S S S inplace glu S - S - S - hardtanh S S S - True hardtanh S S S - True inplace relu S S S True relu S S S True inplace elu S S S elu S S S True inplace selu S S S selu S S S True inplace celu S S S celu S S S True inplace leaky_relu S S S True leaky_relu S S S inplace rrelu S S False rrelu S S False True inplace hardshrink S S S True tanhshrink S S S softsign S S S softplus S S S True softmin S S S softmax S S S True softmax S S S torch double with_all_args True tanh S S S True sigmoid S S S True silu S S S True log_softmax S S S True linear S S M S True aten linear linear S S M S M addmm True aten linear bilinear S S S S S M torch zeros M S M embedding torch tensor torch rand True embedding_bag torch tensor torch rand torch tensor batch_norm S S non_differentiable torch randn S non_differentiable torch ones S None None True training True aten _batch_norm_impl_index batch_norm S S S non_differentiable torch randn S non_differentiable torch ones S non_differentiable torch randn S non_differentiable torch ones S True size_zero True aten _batch_norm_impl_index batch_norm S S S non_differentiable torch randn S non_differentiable torch ones S non_differentiable torch randn S non_differentiable torch ones S True size_zero_inference True aten _batch_norm_impl_index batch_norm S S non_differentiable torch randn S non_differentiable torch ones S non_differentiable torch randn S non_differentiable torch ones S True with_weight_and_bias_training True aten _batch_norm_impl_index batch_norm S S non_differentiable torch randn S non_differentiable torch ones S None non_differentiable torch ones S True with_only_bias_training True aten _batch_norm_impl_index batch_norm S S non_differentiable torch randn S non_differentiable torch ones S non_differentiable torch randn S None True with_only_weight_training True aten _batch_norm_impl_index batch_norm S S non_differentiable torch randn S non_differentiable torch ones S None None False inference True aten _batch_norm_impl_index batch_norm S S non_differentiable torch randn S non_differentiable torch ones S non_differentiable torch randn S non_differentiable torch ones S False with_weight_and_bias_inference True aten _batch_norm_impl_index batch_norm S S non_differentiable torch randn S non_differentiable torch ones S None non_differentiable torch ones S False with_only_bias_inference True aten _batch_norm_impl_index batch_norm S S non_differentiable torch randn S non_differentiable torch ones S non_differentiable torch randn S None False with_only_weight_inference True aten _batch_norm_impl_index instance_norm S S S non_differentiable torch zeros S non_differentiable torch ones S layer_norm S S S S False aten contiguous aten _batch_norm_impl_index layer_norm S S S S non_differentiable torch rand S with_only_weight False aten contiguous aten _batch_norm_impl_index layer_norm S S S S None non_differentiable torch rand S with_only_bias False aten contiguous aten _batch_norm_impl_index layer_norm S S S S non_differentiable torch rand S non_differentiable torch rand S with_weight_and_bias False aten contiguous aten _batch_norm_impl_index aten addcmul group_norm S S S torch rand local_response_norm S S S nll_loss F log_softmax torch randn dim= torch tensor poisson_nll_loss torch rand S torch rand S poisson_nll_loss torch rand S torch rand S True True full kl_div F log_softmax torch randn S F softmax torch randn S cross_entropy S torch randint S dtype=torch int binary_cross_entropy_with_logits torch empty random_ smooth_l _loss S non_differentiable torch rand S huber_loss S non_differentiable torch rand S l _loss S non_differentiable torch rand S mse_loss S non_differentiable torch rand S smooth_l _loss S torch rand S with_grad huber_loss S torch rand S with_grad l _loss S torch rand S with_grad mse_loss S torch rand S with_grad margin_ranking_loss S S S hinge_embedding_loss S non_differentiable torch rand S soft_margin_loss S non_differentiable torch rand S multilabel_soft_margin_loss S non_differentiable torch rand S cosine_embedding_loss S S S S non_differentiable torch rand S pixel_shuffle pixel_unshuffle affine_grid S torch Size S pad pairwise_distance S S S S pdist S S cosine_similarity S S S S triplet_margin_loss S S S S S S normalize S S S unfold S S S S fold grid_sample S S S S non_differentiable torch rand S S S gumbel_softmax S S True aten softmax aten add aten div aten neg gumbel_softmax S S True hard True aten softmax aten add aten div aten neg multilabel_margin_loss torch tensor - torch tensor multi_margin_loss S S non_differentiable torch randint S S dtype=torch int non_differentiable torch randn S binary_cross_entropy torch randn sigmoid non_differentiable torch rand non_differentiable torch randn binary_cross_entropy torch randn sigmoid non_differentiable torch rand non_differentiable torch randn None None mean size_average ctc_loss torch rand S S S log_softmax detach requires_grad_ torch randint S S S dtype=torch long torch full S S dtype=torch long torch randint S S dtype=torch long upsample torch randn S S M M None with_scale upsample torch randn S S M M with_size interpolate torch zeros view nearest_ d interpolate torch randn S S M M None nearest_ d_with_scale interpolate torch randn S S M M nearest_ d_with_size interpolate torch zeros view area_ d interpolate torch randn S S M M None area_ d_with_scale interpolate torch randn S S M M area_ d_with_size interpolate torch zeros view bilinear_ d interpolate torch randn S S M M None bilinear_ d_with_scale interpolate torch randn S S M M bilinear_ d_with_size interpolate torch zeros view bicubic_ d interpolate torch randn S S M M None bicubic_ d_with_scale interpolate torch randn S S M M bicubic_ d_with_size interpolate torch zeros view nearest_ d interpolate torch randn S M M None nearest_ d_with_scale interpolate torch randn S M M nearest_ d_with_size interpolate torch zeros view area_ d interpolate torch randn S M M None area_ d_with_scale interpolate torch randn S M M area_ d_with_size interpolate torch zeros view linear_ d interpolate torch randn S M M None linear_ d_with_scale interpolate torch randn S M M linear_ d_with_size interpolate torch randn S M M M M None nearest_ d_with_scale interpolate torch randn S M M M M nearest_ d_with_size interpolate torch zeros view area_ d interpolate torch randn S M M M M None area_ d_with_scale interpolate torch randn S M M M M area_ d_with_size interpolate torch zeros view trilinear_ d interpolate torch randn S M M M M None trilinear_ d_with_scale interpolate torch randn S M M M M trilinear_ d_with_size interpolate torch zeros view None nearest None False nearest_ d_not_recompute_scale_factor interpolate torch randn S S M M None nearest None False nearest_ d_with_size_not_recompute_scale_factor interpolate torch randn S S M M None bilinear None False bilinear_ d_with_scale_not_recompute_scale_factor interpolate torch randn S S M M None bilinear None False bilinear_ d_with_size_not_recompute_scale_factor interpolate torch randn S S M M None bicubic None False bicubic_ d_with_scale_not_recompute_scale_factor interpolate torch randn S S M M None bicubic None False bicubic_ d_with_size_not_recompute_scale_factor interpolate torch randn S M M None nearest None False nearest_ d_with_scale_not_recompute_scale_factor interpolate torch randn S M M None nearest None False nearest_ d_with_size_not_recompute_scale_factor interpolate torch randn S M M None linear None False linear_ d_with_scale_not_recompute_scale_factor interpolate torch randn S M M None linear None False linear_ d_with_size_not_recompute_scale_factor interpolate torch randn S M M M M None nearest None False nearest_ d_with_scale_not_recompute_scale_factor interpolate torch randn S M M M M None nearest None False nearest_ d_with_size_not_recompute_scale_factor interpolate torch randn S M M M M None trilinear None False trilinear_ d_with_scale_not_recompute_scale_factor interpolate torch randn S M M M M None trilinear None False trilinear_ d_with_size_not_recompute_scale_factor nn_functional_tests script_template = the_method value_to_literal value isinstance value str Quotes string escapes special characters ascii value isinstance value torch Tensor torch + str value str value get_call method_name func_type args kwargs kwargs_str = join k + = + value_to_literal v k v kwargs items self_arg = args func_type == method args = args argument_str = join args argument_str += len args len kwargs argument_str += kwargs_str func_type == functional func_type == function call = f torch method_name argument_str func_type == method call = f self_arg method_name argument_str func_type == nn_functional call = f torch nn functional method_name argument_str raise TypeError Unsupported function type call get_constant x x == inf math inf x == -inf -math inf x get_script_args args formals list str = tensors list Union torch Tensor list torch Tensor = actuals list str = arg args isinstance arg torch Tensor name = f i len formals formals append name actuals append name tensors append arg is_iterable_of_tensors arg name = f i len formals formals append name + List torch Tensor actuals append name tensors append list arg isinstance arg str actuals append f arg actuals append str get_constant arg formals tensors actuals create script function name func_type output_process_fn returns compiled function example inputs gen_script_fn_and_args method_name func_type args kwargs formals tensors actuals = get_script_args args call = get_call method_name func_type actuals kwargs script = script_template format join formals call CU = torch jit CompilationUnit script CU the_method tensors create script function name func_type returns function takes args kwargs runs compiled function create_script_fn method_name func_type function returns tuple containing original output filtered output used checking gradients script_fn args kwargs fn tensors = gen_script_fn_and_args method_name func_type args kwargs assertExportImport fn graph tensors output = fn tensors skip type annotate function attributes now see https github com python mypy issues script_fn last_graph = fn graph_for tensors type ignore attr-defined output script_fn SplitInputs all_tensors list Any tensor_args list Any nontensor_args list Any arg_types list str tensor_kwargs dict str Any kwarg_order list str nontensor_kwargs dict str Any kwarg_types dict str Any staticmethod _is_tensor_input arg isinstance arg torch Tensor is_iterable_of_tensors arg __init__ args kwargs arg_types = t _is_tensor_input arg s arg args kwarg_types = k t _is_tensor_input v s k v kwargs items tensor_args = arg arg args _is_tensor_input arg nontensor_args = arg arg args _is_tensor_input arg tensor_kwargs = k v k v kwargs items _is_tensor_input v nontensor_kwargs = k v k v kwargs items _is_tensor_input v all_tensors = tensor_args v k v tensor_kwargs items kwarg_order = k k v kwargs items nontensors_match other SplitInputs arg_types = other arg_types False kwarg_types = other kwarg_types False kwarg_order = other kwarg_order False nontensor_args = other nontensor_args False nontensor_kwargs = other nontensor_kwargs False True make new function where all non-tensor arguments args have been partially applied all tensor arguments remain used trace functions when some arguments tensors partial_apply_nontensors fn args kwargs inputs = SplitInputs args kwargs new_fn tensors_ tensors = iter tensors_ full_args = args i s == s next tensors i s enumerate inputs arg_types full_kwargs = k kwargs k s == s next tensors k s inputs kwarg_types items fn full_args full_kwargs new_fn inputs create trace function input fn create_traced_fn fn cache_traced_fn=False traced_fn inputs kwargs ` check_trace ` set False because check_trace run no_grad Also ` check_against_reference ` already does all checks against python function fn_tensors split_inputs = partial_apply_nontensors fn inputs kwargs cache_traced_fn hasattr traced_fn traced traced = torch jit trace fn_tensors split_inputs all_tensors check_trace=False assertExportImport traced graph split_inputs all_tensors output = traced split_inputs all_tensors cache_traced_fn traced_fn traced = traced traced_fn split_inputs = split_inputs Guard check nontensor inputs same during tracing assertTrue traced_fn split_inputs nontensors_match split_inputs output = traced_fn traced split_inputs all_tensors traced = traced_fn traced skip type annotate function attributes now see https github com python mypy issues traced_fn last_graph = traced graph_for split_inputs all_tensors type ignore attr-defined traced_fn graph = traced graph type ignore attr-defined output traced_fn known failing script EXCLUDE_SCRIPT = test_norm_fro_default test_norm_fro_cpu test_norm_nuc test_norm_fro test_norm_nuc_batched aten op has additional cudnn argument test_nn_unfold flaky test - TODO fix test_nn_ctc_loss unknown builtin op test_nn_fold jit doesn t support sparse tensors test_to_sparse test_to_sparse_dim generates script function set example inputs specified test format nn_functional_tests get_nn_functional_compiled_fn_and_inputs name self_size args variant_name= extra_args test_name = test_nn_ + name variant_name = test_name = test_name + _ + variant_name self_variable = create_input self_size need record because methods can change size e g unsqueeze args_variable _kwargs_variable = create_input args self_tensor = deepcopy self_variable data args_tensor = deepcopy unpack_variables args_variable f_args_variable = self_variable + args_variable f_args_tensor = self_tensor + args_tensor noqa F torch _jit_internal _disable_emit_hooks script_fn inputs = gen_script_fn_and_args name nn_functional f_args_variable script_fn inputs EXCLUDE_SCRIPT_MODULES = test_nn_AdaptiveAvgPool d_tuple_none test_nn_AdaptiveAvgPool d_tuple_none test_nn_AdaptiveMaxPool d_tuple_none test_nn_AdaptiveMaxPool d_tuple_none Doesn t use future division so supported test_nn_CrossMapLRN d Derivative aten _scaled_dot_product_flash_attention_backward implemented test_nn_TransformerDecoderLayer_gelu_activation test_nn_TransformerDecoderLayer_relu_activation test_nn_TransformerEncoderLayer_gelu_activation test_nn_TransformerEncoderLayer_relu_activation test_nn_Transformer_multilayer_coder script_method_template = forward create_script_module nn_module constructor_args args kwargs script_module args kwargs _formals tensors actuals = get_script_args args method_args = join + actuals call_args_str = join actuals call = f submodule call_args_str script = script_method_template format method_args call submodule_constants = kwargs get is_constant submodule_constants = submodule Create module use script method TheModule torch jit ScriptModule __constants__ = submodule_constants __init__ - None super __init__ submodule = nn_module constructor_args make_module script module = TheModule check __repr__ str module module define script module module = make_module script assertExportImportModule module tensors module args skip type annotate function attributes now see https github com python mypy issues create_script_module last_graph = module graph type ignore attr-defined module script_module check_alias_annotation method_name args kwargs aten_name func_type= method formals tensors actuals = get_script_args args call = get_call method_name func_type actuals kwargs script = script_template format join formals call CU = torch jit CompilationUnit script clean up IR torch _C _jit_pass_inline CU the_method graph torch _C _jit_pass_constant_propagation CU the_method graph torch _C _jit_check_alias_annotation CU the_method graph tuple tensors aten_name get_nn_module_name_from_kwargs kwargs module_name kwargs kwargs module_name fullname kwargs kwargs fullname constructor kwargs kwargs constructor __name__ get_nn_mod_test_name kwargs fullname kwargs test_name = kwargs fullname test_name = get_nn_module_name_from_kwargs kwargs desc kwargs test_name = f test_name _ kwargs desc f test_nn_ test_name get_nn_module_class_from_kwargs kwargs name = get_nn_module_name_from_kwargs kwargs index = name find _ index == - name name name find _ try_get_nn_module_compiled_mod_and_inputs args kwargs name = get_nn_module_name_from_kwargs kwargs desc kwargs eval kwargs desc eval supported so skip these tests test_name = name desc kwargs test_name = f test_name _ kwargs desc test_name = get_nn_mod_test_name kwargs test_name EXCLUDE_SCRIPT_MODULES constructor kwargs nn_module = kwargs constructor nn_module = getattr torch nn name FunctionalModule str nn_module constructor_args_fn kwargs constructor_args = kwargs constructor_args_fn constructor_args = kwargs get constructor_args Set up inputs tuple sizes constructor fn input_dtype = torch double input_fn kwargs input = kwargs input_fn isinstance input torch Tensor input = input all tensor is_complex tensor input input_dtype = torch cdouble input = kwargs input_size Extra parameters forward extra_args kwargs input = input + kwargs extra_args target_size kwargs input = input + kwargs target_size target_fn kwargs torch is_tensor input input = input input = input + kwargs target_fn args_variable _kwargs_variable = create_input input dtype=input_dtype f_args_variable = deepcopy unpack_variables args_variable out_var = deepcopy f_args_variable _args mod = f_args_variable create_script_module None nn_module constructor_args f_args_variable f_args_variable mod out_var get_all_nn_module_tests additional modules test TODO delete list once we make all nn_tests work additional_module_tests = module_name Bilinear constructor_args S S M input_size S S extra_args S S module_name RNNCell constructor_args S S input_size S S module_name LSTMCell constructor_args S S input_size S S module_name GRUCell constructor_args S S input_size S S module_name MultiheadAttention constructor_args input_size extra_args torch randn torch randn slowTest True module_name Transformer constructor_args input_size extra_args torch randn slowTest True module_tests + get_new_module_tests + additional_module_tests