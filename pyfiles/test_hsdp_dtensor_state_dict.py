Owner s oncall distributed io copy deepcopy torch torch distributed dist torch nn nn torch distributed _shard sharded_tensor ShardedTensor torch distributed device_mesh init_device_mesh torch distributed fsdp FullyShardedDataParallel FSDP torch distributed fsdp api ShardedOptimStateDictConfig ShardedStateDictConfig ShardingStrategy StateDictType torch distributed tensor DTensor Replicate Shard torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_fsdp get_devtype torch testing _internal common_utils parametrize run_tests torch testing _internal distributed _tensor common_dtensor DTensorTestBase skip_if_lt_x_gpu with_comms device_type = torch device get_devtype Simple boring model test interface some corner cases do require complicated wrapping strategy DenseModel torch nn Module __init__ - None super __init__ torch manual_seed net = nn Sequential nn Linear nn ReLU net = nn Sequential nn Linear nn ReLU net = nn Sequential nn Linear nn ReLU net = nn Sequential nn ReLU nn Linear forward x net net net net x get_input device torch rand device=device TODO Consolidate DeviceMesh based FSDP HSDP test cases TestHSDPWithDeviceMeshAndDTensor DTensorTestBase _create_model device device_mesh=None device_id = device_type device_mesh model = FSDP DenseModel device_id device_mesh=device_mesh sharding_strategy=ShardingStrategy HYBRID_SHARD mesh_ d = init_device_mesh device_type world_size intra_node_pg = mesh_ d get_group mesh_dim= inter_node_pg = mesh_ d get_group mesh_dim= model = FSDP DenseModel device_id process_group= intra_node_pg inter_node_pg sharding_strategy=ShardingStrategy HYBRID_SHARD optim = torch optim Adam model parameters lr= model model get_input device_id sum backward optim step model optim with_comms skip_if_lt_x_gpu test_hsdp_init_with_device_mesh device device_id = device_type mesh_ d = init_device_mesh device_type world_size model optim = _create_model device_id mesh_ d FSDP set_state_dict_type model StateDictType SHARDED_STATE_DICT state_dict = model state_dict optim_state_dict = FSDP optim_state_dict model optim v state_dict values assertEqual type v DTensor assertEqual len v placements assertEqual v placements Replicate Shard assertEqual v device_mesh mesh_ d state optim_state_dict state values k v state items k = step assertEqual type v DTensor assertEqual len v placements assertEqual v placements Replicate Shard assertEqual v device_mesh mesh_ d state_dict_type = model get_state_dict_type model If device_mesh used when initializing FSDP field _use_dtensor will automatically set True assertEqual state_dict_type state_dict_config _use_dtensor True assertEqual state_dict_type optim_state_dict_config _use_dtensor True with_comms skip_if_lt_x_gpu parametrize offload_to_cpu True False test_dtensor_sharded_tensor_state_dict_identical device offload_to_cpu device_id = device_type mesh_ d = init_device_mesh device_type world_size model optim = _create_model device_id mesh_ d FSDP set_state_dict_type model StateDictType SHARDED_STATE_DICT state_dict_config=ShardedStateDictConfig offload_to_cpu=offload_to_cpu optim_state_dict_config=ShardedOptimStateDictConfig offload_to_cpu=offload_to_cpu dtensor_sd = model state_dict dtensor_osd = FSDP optim_state_dict model optim ref_model ref_optim = _create_model device_id FSDP set_state_dict_type ref_model StateDictType SHARDED_STATE_DICT state_dict_config=ShardedStateDictConfig offload_to_cpu=offload_to_cpu optim_state_dict_config=ShardedOptimStateDictConfig offload_to_cpu=offload_to_cpu sharded_tensor_sd = ref_model state_dict sharded_tensor_osd = FSDP optim_state_dict ref_model ref_optim Check dtensor sharded_tensor model state dict values identical dtensor_sd_item sharded_tensor_sd_item zip dtensor_sd items sharded_tensor_sd items k v = dtensor_sd_item k v = sharded_tensor_sd_item assertEqual k k assertEqual type v DTensor assertEqual type v ShardedTensor check whether local_tensor same assertEqual v to_local v local_tensor check whether device same assertEqual v to_local device v local_tensor device Check dtensor sharde_tensor optim state dict values identical dtensor_osd_state sharded_tensor_osd_state zip dtensor_osd state items sharded_tensor_osd state items check FQN same assertEqual dtensor_osd_state sharded_tensor_osd_state dtensor_hyper_param sharded_tensor_hyper_param zip dtensor_osd_state items sharded_tensor_osd_state items k v = dtensor_hyper_param k v = sharded_tensor_hyper_param assertEqual k k k = step assertEqual type v DTensor assertEqual type v ShardedTensor check whether local_tensor same assertEqual v to_local v local_tensor check whether device same assertEqual v to_local device v local_tensor device assertEqual v v with_comms skip_if_lt_x_gpu parametrize offload_to_cpu True False test_dtensor_sharded_optim_load_state_dict device offload_to_cpu device_id = device_type mesh_ d = init_device_mesh device_type world_size model optim = _create_model device_id mesh_ d FSDP set_state_dict_type model StateDictType SHARDED_STATE_DICT optim_state_dict_config=ShardedOptimStateDictConfig offload_to_cpu=offload_to_cpu checkpoint = io BytesIO torch save FSDP optim_state_dict model optim checkpoint Deepcopy save current optim_state_dict compare optim_state_dict loaded back below ref_optim_state_dict = deepcopy FSDP optim_state_dict model optim Update parameters so FSDP optim_state_dict will different ref_optim_state_dict model model get_input device_id sum backward optim step Load ref_optim_state_dict back checkpoint seek load_ref_optim_state_dict = torch load checkpoint optim load_state_dict FSDP optim_state_dict_to_load model optim load_ref_optim_state_dict new_optim_state_dict = FSDP optim_state_dict model optim Check whether new_optim_state_dict same ref_optim_state_dict new_optim_state_dict_item ref_optim_state_dict_item zip new_optim_state_dict state items ref_optim_state_dict state items check FQN same assertEqual new_optim_state_dict_item ref_optim_state_dict_item new_optim_hyper_param ref_optim_hyper_param zip new_optim_state_dict_item items ref_optim_state_dict_item items k v = new_optim_hyper_param k v = ref_optim_hyper_param check whether keys same assertEqual k k check whether DTensor same assertEqual v v k = step assertEqual type v DTensor assertEqual type v DTensor with_comms skip_if_lt_x_gpu parametrize offload_to_cpu True False test_dtensor_sharded_model_load_state_dict device offload_to_cpu device_id = device_type mesh_ d = init_device_mesh device_type world_size model optim = _create_model device_id mesh_ d FSDP set_state_dict_type model StateDictType SHARDED_STATE_DICT state_dict_config=ShardedStateDictConfig offload_to_cpu=offload_to_cpu checkpoint = io BytesIO torch save model state_dict checkpoint Deepcopy save current state_dict compare state_dict loaded back below ref_state_dict = deepcopy model state_dict Update parameters so model state_dict will different ref_dtensor_sd model model get_input device_id sum backward optim step Load ref_state_dict back checkpoint seek load_ref_state_dict = torch load checkpoint model load_state_dict load_ref_state_dict new_state_dict = model state_dict Check whether new_state_dict same ref_state_dict k v k v zip ref_state_dict items new_state_dict items check whether fqn same assertEqual k k assertEqual type v DTensor assertEqual type v DTensor check whether DTensor same assertEqual v v with_comms skip_if_lt_x_gpu test_root_module_is_not_FSDP device device_id = device_type FakeMPModel torch nn Module __init__ device_mesh super __init__ torch manual_seed dense = FSDP DenseModel device_id use_orig_params=True sharding_strategy=ShardingStrategy HYBRID_SHARD device_mesh=device_mesh dist get_rank == sparse = nn Sequential nn Linear nn ReLU sparse = nn Sequential nn Linear nn ReLU forward x dist get_rank == sparse = sparse x sparse = sparse x dist all_reduce sparse dense sparse mesh_ d = init_device_mesh device_type world_size model = FakeMPModel device_mesh=mesh_ d device_id optim = torch optim Adam model parameters lr= e- batch = torch rand device=self device_type model batch sum backward optim step osd = optim state_dict FSDP state_dict_type model StateDictType SHARDED_STATE_DICT osd = FSDP optim_state_dict model optim osd param state osd state items dense param assertIsInstance state exp_avg DTensor assertIsInstance state exp_avg_sq DTensor assertEqual state exp_avg placements Replicate Shard assertEqual state exp_avg_sq placements Replicate Shard assertIsInstance state exp_avg torch Tensor assertIsInstance state exp_avg_sq torch Tensor devices = cuda hpu xpu instantiate_device_type_tests TestHSDPWithDeviceMeshAndDTensor globals only_for=devices allow_xpu=True __name__ == __main__ run_tests