Owner s oncall distributed functools math sys torch torch distributed fsdp _traversal_utils traversal_utils torch nn nn torch distributed dist torch distributed fsdp FullyShardedDataParallel FSDP torch distributed fsdp wrap ModuleWrapPolicy transformer_auto_wrap_policy torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp DEVICEInitMode FSDPInitMode FSDPTest TransformerWithSharedParams torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests TEST_WITH_DEV_DBG_ASAN dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit device_type = acc type acc = torch accelerator current_accelerator cpu Model torch nn Module __init__ - None super __init__ layer = torch nn Linear layer _modules = torch nn Linear torch nn Linear torch nn Linear layer = torch nn Sequential layer _modules layer = torch nn Linear layer = torch nn Linear relu = torch nn ReLU forward x z = relu layer x z = relu layer z z = relu layer z z = relu layer z z get_input device torch randn device get_loss input output output sum run_backward loss loss backward IgnoredModule torch nn Module __init__ in_dim int out_dim int - None super __init__ weight = torch nn Parameter torch randn in_dim out_dim forward x x weight ModelWithIgnoredModules Model Adds variable number ` IgnoredModule ` ` ` layer ` ` __init__ num_ignored int - None assert num_ignored = super __init__ layer _modules = torch nn Linear torch nn Linear + IgnoredModule _ range num_ignored + torch nn Linear layer = torch nn Sequential layer _modules TestFSDPIgnoredModules FSDPTest property world_size min torch accelerator device_count _train_model model optim num_iters device=torch device device_type _ range num_iters module = model module isinstance model FSDP model inp = module get_input device output = model inp loss = module get_loss inp output device module run_backward loss optim step skip_if_lt_x_gpu test_ignored_modules_transformer Tests ignored modules parameters flattened transformer model shared parameters run_subtests use_orig_params False True ignore_modules True False use_auto_wrap False True _test_ignored_modules_transformer _test_ignored_modules_transformer use_orig_params bool ignore_modules bool opposed ` ignored_states ` use_auto_wrap bool Initialize FSDP-wrapped transformer model has FSDP ignore ` nn Transformer ` module s parameters model nn Module = TransformerWithSharedParams init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE deterministic=True fsdp_kwargs = process_group process_group use_auto_wrap Unshare output projection weight embedding weight able auto wrap every linear correctly model output_proj weight = nn Parameter model output_proj weight clone fsdp_kwargs auto_wrap_policy = ModuleWrapPolicy nn Linear ignore_modules fsdp_kwargs ignored_modules = model transformer fsdp_kwargs ignored_states = list model transformer parameters wrapper_cls = FSDP wrapped_model = wrapper_cls model fsdp_kwargs Check wrapped model s flattened parameter does include ignored transformer module s parameters nonwrapped_model nn Module = TransformerWithSharedParams init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE deterministic=True use_auto_wrap nonwrapped_model output_proj weight = nn Parameter nonwrapped_model output_proj weight clone total_numel = sum p numel p nonwrapped_model parameters ignored_numel = sum p numel p nonwrapped_model transformer parameters nonignored_numel = total_numel - ignored_numel fsdp_managed_numel = FSDP summon_full_params wrapped_model handle traversal_utils _get_fsdp_handles wrapped_model flat_param = handle flat_param flat_param_numel = flat_param numel use_orig_params Subtract numel contributed alignment padding padding_numel = sum numel numel is_padding zip flat_param _numels_with_padding flat_param _is_padding_mask is_padding flat_param_numel -= padding_numel fsdp_managed_numel += flat_param_numel assertEqual fsdp_managed_numel nonignored_numel Check we can run few iterations optim = torch optim Adam wrapped_model parameters lr= e- _train_model wrapped_model optim skip_if_lt_x_gpu test_ignored_modules_nested Tests passing module nested FSDP modules does error still ignores non-FSDP modules parameters run_subtests use_orig_params False True ignore_modules True False _test_ignored_modules_nested _test_ignored_modules_nested use_orig_params bool ignore_modules bool Initialize FSDP-wrapped nested model first wraps nested sequential s second linear layer ` layer ` then wraps overall model while ignoring nested sequential ` layer ` model = Model device_type fsdp_fn = functools partial FSDP use_orig_params=use_orig_params model layer = fsdp_fn model layer ignore_modules wrapped_model = fsdp_fn model ignored_modules= model layer wrapped_model = fsdp_fn model ignored_states=list model layer parameters Check wrapped model s flattened parameter does include ignored nested sequential s parameters nonwrapped_model = Model total_numel = sum p numel p nonwrapped_model parameters ignored_numel = sum p numel p nonwrapped_model layer parameters nonignored_numel = total_numel - ignored_numel FSDP summon_full_params wrapped_model flat_param = wrapped_model params flat_param_numel = flat_param numel use_orig_params Subtract numel contributed alignment padding padding_numel = sum numel numel is_padding zip flat_param _numels_with_padding flat_param _is_padding_mask is_padding flat_param_numel -= padding_numel assertEqual flat_param_numel nonignored_numel assertEqual flat_param_numel nonignored_numel Check we can run few iterations optim = torch optim Adam wrapped_model parameters lr= e- _train_model wrapped_model optim skip_if_lt_x_gpu test_ignored_states_auto_wrap transformer_policy = functools partial transformer_auto_wrap_policy transformer_layer_cls= nn Sequential run_subtests policy transformer_policy ModuleWrapPolicy nn Sequential ignore_bias True False _test_ignored_states_auto_wrap _test_ignored_states_auto_wrap policy ignore_bias bool model = Model device_type ignored_states = model layer weight ignore_bias ignored_states append model layer bias Construct flat parameters one ` layer ` one model fsdp_model = FSDP model Use ` False ` avoid complexity intra-flat-parameter padding use_orig_params=False auto_wrap_policy=policy ignored_states=ignored_states ref_model = Model expected_layer _unsharded_numel = sum p numel p ref_model layer parameters - ref_model layer weight numel ignore_bias expected_layer _unsharded_numel -= ref_model layer bias numel expected_model_unsharded_numel = sum p numel p ref_model parameters - sum p numel p ref_model layer parameters expected_layer _sharded_numel = math ceil expected_layer _unsharded_numel world_size expected_model_sharded_numel = math ceil expected_model_unsharded_numel world_size assertLessEqual fsdp_model layer module _flat_param numel expected_layer _sharded_numel assertLessEqual fsdp_model module _flat_param numel expected_model_sharded_numel skip_if_lt_x_gpu test_ignored_modules_invalid Tests passing FSDP module ignored module top-level module itself errors model = Model device_type wrap_cls = FSDP model layer = wrap_cls model layer Passing FSDP module ignored module should error assertRaises ValueError msg= ` ignored_modules ` should include FSDP modules wrap_cls model ignored_modules= model layer assertWarnsRegex expected_warning=UserWarning expected_regex= Trying ignore top-level module passed into FSDP constructor itself will result all parameters being ignored FSDP does allow wrap same model twice so create new local model here new_model = Model device_type wrap_cls new_model ignored_modules= new_model skip_if_lt_x_gpu test_diff_ignored_modules_across_ranks Tests ignoring different modules across ranks Args pass_ignored_modules_to_root bool If ` ` False ` ` does pass any ignored modules including those already ignored child FSDP instances root FSDP instance ` ` True ` ` passes all ignored modules representing superset children s ignored modules root FSDP instance run_subtests pass_ignored_modules_to_root False True ignore_modules True False _test_diff_ignored_modules_across_ranks _test_diff_ignored_modules_across_ranks pass_ignored_modules_to_root bool ignore_modules bool To exercise different ` FlatParameter ` enumerations across ranks we wrap ` layer ` FSDP where ` layer ` registered module after ` layer ` which has variable number ignored modules wrap_cls = FSDP model = ModelWithIgnoredModules num_ignored=self rank + device_type layer _ignored_modules = m m model layer modules isinstance m IgnoredModule ignore_kwargs = ignored_modules layer _ignored_modules ignore_modules ignored_states p m layer _ignored_modules p m parameters model layer = wrap_cls model layer ignore_kwargs model layer = wrap_cls model layer model_ignored_modules = m m model modules isinstance m IgnoredModule pass_ignored_modules_to_root ignore_kwargs_top = ignored_modules model_ignored_modules ignore_modules ignored_states p m model_ignored_modules p m parameters wrapped_model = wrap_cls model ignore_kwargs_top optim = torch optim Adam wrapped_model parameters lr= e- _train_model wrapped_model optim skip_if_lt_x_gpu parametrize ignore_modules True False test_ignored_modules_not_under_wrapped_root ignore_modules bool model = Model device_type ignored_modules = list model layer children ignore_kwargs = ignored_modules ignored_modules ignore_modules ignored_states p m ignored_modules p m parameters wrap_cls = FSDP model layer = wrap_cls model layer ignore_kwargs model layer = wrap_cls model layer ignored modules parameters contains submodule under model layer which out local root model layer ignore_kwargs optim = torch optim Adam model parameters lr= e- _train_model model optim skip_if_lt_x_gpu test_ignored_states_check Tests passing invalid ` ` ignored_modules ` ` ` ` ignored_states ` ` raises appropriate error run_subtests ignore_modules True False _test_ignored_states_check _test_ignored_states_check ignore_modules bool model = Model device_type ignored_modules = list model layer children ignored_params = p m ignored_modules p m parameters ignored_states = ignored_params union set ignored_modules ignore_modules Check passing ` ignored_modules ` uniformly ` nn Module ` raises error assertRaisesRegex ValueError ignored_modules expects nn Module list elements got types r \ torch nn parameter Parameter \ FSDP model ignored_modules=ignored_params Check passing both ` ignored_modules ` ` ignored_states ` raises error fold only into ` ignore_modules=True ` assertRaisesRegex ValueError Cannot pass both ignored_modules ignored_states same time FSDP model ignored_modules=ignored_modules ignored_states=ignored_params Check passing ` ignored_states ` uniformly ` nn Parameter ` uniformly ` nn Module ` raises error assertRaisesRegex ValueError ignored_states expects all nn Parameter all nn Module list r elements got types \ torch nn modules linear Linear r torch nn parameter Parameter \ FSDP model ignored_states=ignored_states instantiate_parametrized_tests TestFSDPIgnoredModules __name__ == __main__ run_tests