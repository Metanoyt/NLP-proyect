mypy allow-untyped-defs typing Any Optional sympy torch config runtime hints AttrsDescriptorWrapper utils _type_of expr_fits_within_ bit triton_version_uses_attrs_dict virtualized V common ArgName ConstexprArg KernelArgType SizeArg TensorArg TMADescriptorArg WorkspaceArg should_unwrap_unspec_arg name str V graph is_unspec_arg name Unwrap all devices except CPU V graph get_current_device_or_throw type = cpu True Only unwrap CPU input used output name V graph mutated_buffers True False signature_of arg KernelArgType size_dtype Optional str - str isinstance arg TensorArg TODO Remove fp special handling when Triton supports PyTorch fp dtypes Related PR https github com triton-lang triton pull arg dtype == torch float _e m fn typ = fp e nv arg dtype == torch float _e m typ = fp e arg dtype == torch float _e m fnuz typ = fp e b arg dtype == torch float _e m fnuz typ = fp e b typ = _type_of arg dtype should_unwrap_unspec_arg arg buffer had unwrapped d tensor scalar new_typ = typ lstrip new_typ fp bf fp new_typ typ isinstance arg SizeArg arg expr None triton_version_uses_attrs_dict In newer versions Triton signature includes None args their type marked constexpr constexpr In older versions Triton From triton runtime jit py ` None ` nullptr Implicitly convert i i _arg_equals_ arg triton_version_uses_attrs_dict In new versions Triton we have equal-to- arg s marked constant should marked constexpr signature constexpr isinstance arg expr float sympy Float fp isinstance arg expr bool i integer size_dtype == tl int i size_dtype == tl int i size_dtype None no hint we ll see we know -bit int guard possible int_max = torch iinfo torch int max expr_fits_within_ bit arg expr V graph sizevars check_leq arg expr int_max i i raise NotImplementedError f unhandled size_dtype size_dtype isinstance arg WorkspaceArg _type_of arg dtype isinstance arg TMADescriptorArg arg api_type == experimental nvTmaDesc https github com triton-lang triton blob baed b cf e b bb f b c python triton runtime jit py#L -L assert arg api_type == stable assert arg block_shape None assert arg dtype None inner = _type_of arg dtype strip ` ` fp - fp f tensordesc inner list arg block_shape isinstance arg ConstexprArg constexpr raise NotImplementedError f unhandled type arg arg non_constexpr_signature signature new_signature = arg signature isinstance arg ConstexprArg new_signature append arg new_signature signature_to_meta signature list KernelArgType size_dtype Optional str argdefs list ArgName indices Optional list int = None is_template bool = False - dict str str indices None indices = list range len signature _decide_tl_dtype arg Even ks symbol itself within tl int range s risky use tl int dtype since we may have ks ks later kernels like torch mean when dynamic shape enabled Check config triton use_block_ptr since Triton block pointer does support bit indexing https gist github com shunting c ce f dcde ad If triton metadata template don t use tl int index Templates like flex attention decoding uses block pointers which does support bit indexing config triton use_block_ptr is_template isinstance arg SizeArg arg name startswith ks tl int size_dtype argdefs i name signature_of arg size_dtype=_decide_tl_dtype arg i arg zip indices signature is_unaligned_buffer arg TensorArg buf_name = arg buffer buf_name V graph unaligned_buffers True buf_name V graph graph_inputs See Note Input Alignment handling Inductor For graph inputs recorded V graph unaligned_buffers we know sure tensor aligned False buf_name V graph constants all constants assumed aligned False V graph scheduler layout = V graph scheduler get_buffer_layout buf_name buffer = V graph try_get_buffer buf_name output arg buffer assert buf_name == V kernel output_node name layout = V kernel output_node layout layout = buffer get_layout isinstance layout torch _inductor ir NonOwningLayout layout maybe_guard_aligned False _arg_equals_ arg KernelArgType - bool isinstance arg SizeArg isinstance arg expr int sympy Integer V graph sizevars statically_known_equals arg expr type ignore arg-type equal_ _arg_indices args list KernelArgType indices Optional list int = None - tuple int indices None indices = list range len args equal_to_ = tuple i i arg zip indices args _arg_equals_ arg equal_to_ config_of args list KernelArgType indices Optional list int = None - Any indices None indices = list range len args is_aligned x KernelArgType alignment int include_tensor bool - bool Roughly follow triton code here https github com triton-lang triton blob ed d e b ee ef dd python triton runtime jit py#L -L isinstance x TensorArg include_tensor offset_aligned = V graph sizevars statically_known_multiple_of x offset x dtype itemsize alignment type ignore arg-type offset_aligned is_unaligned_buffer x False isinstance x SizeArg TODO voz These kinda redundant we can solve out statically_known_multiple_of _maybe_evaluate_static x name startswith load_seed_offset False x expr None False isinstance x expr float False V graph sizevars statically_known_multiple_of x expr alignment type ignore arg-type isinstance x WorkspaceArg We allocate workspace ourselves so always aligned True isinstance x TMADescriptorArg ConstexprArg False raise NotImplementedError f unhandled type x x config triton divisible_by_ divisible_by_ = tuple i i arg zip indices args is_aligned arg alignment= include_tensor=True divisible_by_ = equal_to_ = equal_ _arg_indices args indices=indices pyrefly ignore bad-argument-type AttrsDescriptorWrapper divisible_by_ equal_to_