argparse torchvision datasets transforms torch torch nn nn torch nn functional F torch optim optim torch optim lr_scheduler StepLR Net nn Module __init__ super Net __init__ noqa UP conv = nn Conv d conv = nn Conv d dropout = nn Dropout dropout = nn Dropout fc = nn Linear fc = nn Linear forward x x = conv x x = F relu x x = conv x x = F relu x x = F max_pool d x x = dropout x x = torch flatten x x = fc x x = F relu x x = dropout x x = fc x output = F log_softmax x dim= output train args model device train_loader optimizer epoch model train batch_idx data target enumerate train_loader data target = data device target device optimizer zero_grad output = model data loss = F nll_loss output target loss backward optimizer step batch_idx args log_interval == print f Train Epoch epoch f batch_idx len data len train_loader dataset f batch_idx len train_loader f \tLoss loss item f args dry_run break test model device test_loader model eval test_loss = correct = torch no_grad data target test_loader data target = data device target device output = model data test_loss += F nll_loss output target reduction= sum item sum up batch loss pred = output argmax dim= keepdim=True get index max log-probability correct += pred eq target view_as pred sum item test_loss = len test_loader dataset print f \nTest set Average loss test_loss f f Accuracy correct len test_loader dataset f correct len test_loader dataset f \n timed fn start = torch cuda Event enable_timing=True end = torch cuda Event enable_timing=True start record result = fn end record torch cuda synchronize result start elapsed_time end main Training settings parser = argparse ArgumentParser description= PyTorch MNIST Example parser add_argument -- batch-size type=int default= metavar= N help= input batch size training default parser add_argument -- test-batch-size type=int default= metavar= N help= input batch size testing default parser add_argument -- epochs type=int default= metavar= N help= number epochs train default parser add_argument -- lr type=float default= metavar= LR help= learning rate default parser add_argument -- gamma type=float default= metavar= M help= Learning rate step gamma default parser add_argument -- no-cuda action= store_true default=False help= disables CUDA training parser add_argument -- no-mps action= store_true default=False help= disables macOS GPU training parser add_argument -- dry-run action= store_true default=False help= quickly check single pass parser add_argument -- seed type=int default= metavar= S help= random seed default parser add_argument -- log-interval type=int default= metavar= N help= how many batches wait before logging training status parser add_argument -- save-model action= store_true default=False help= For Saving current Model args = parser parse_args use_cuda = args no_cuda torch cuda is_available use_mps = args no_mps torch backends mps is_available torch manual_seed args seed torch backends cuda matmul allow_tf = True use_cuda device = torch device cuda use_mps device = torch device mps device = torch device cpu train_kwargs = batch_size args batch_size test_kwargs = batch_size args test_batch_size use_cuda cuda_kwargs = num_workers pin_memory True shuffle True train_kwargs update cuda_kwargs test_kwargs update cuda_kwargs transform = transforms Compose transforms ToTensor transforms Normalize dataset = datasets MNIST data train=True download=True transform=transform dataset = datasets MNIST data train=False transform=transform train_loader = torch utils data DataLoader dataset train_kwargs test_loader = torch utils data DataLoader dataset test_kwargs model = Net device opt_model = torch compile model mode= max-autotune optimizer = optim Adadelta opt_model parameters lr=args lr scheduler = StepLR optimizer step_size= gamma=args gamma epoch range args epochs + print f Training Time timed lambda train args opt_model device train_loader optimizer epoch print f Evaluation Time timed lambda test opt_model device test_loader scheduler step args save_model torch save opt_model state_dict mnist_cnn pt __name__ == __main__ main