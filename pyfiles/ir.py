__future__ annotations contextlib dataclasses functools itertools logging operator os textwrap traceback collections abc Container Generator Iterable Iterator Sequence contextlib AbstractContextManager nullcontext enum Enum functools partial typing Any Callable cast ClassVar Literal Optional overload SupportsFloat SupportsInt TYPE_CHECKING TypeVar Union typing_extensions assert_never Never override ParamSpec Self TypeAlias TypeIs unittest mock patch sympy sympy Expr Integer Symbol torch _export serde schema export_schema torch _library utils library_utils torch _logging torch fx torch utils _pytree pytree torch _dynamo utils identity torch _export serde serialize GraphModuleSerializer torch _higher_order_ops auto_functionalize can_auto_functionalize torch _inductor metrics torch _inductor utils get_free_symbols torch _prims_common compute_required_storage_length is_boolean_dtype is_float_dtype make_channels_last_strides_for StrideType torch _subclasses fake_tensor get_schema_info torch fx experimental symbolic_shapes _remove_effect_token_unbacked_bindings compute_unbacked_bindings free_symbols free_unbacked_symbols IterateExprs rebind_unbacked resolve_unbacked_bindings ShapeEnv SymTypes torch fx node Node torch utils _ordered_set OrderedSet torch utils _sympy functions CleanDiv FloorDiv Mod ModularIndexing torch utils _sympy symbol SymT config dependencies codegen common BackendFeature CodegenSymbol get_scheduling_for_device index_prevent_reordering Kernel dependencies Dep extract_free_symbols extract_input_node_reduction_ranges extract_read_writes var_builder loop_body LoopBody ops_handler OpCounterCSE OpCountResult ReductionType StoreMode runtime benchmarking benchmarker runtime hints DeviceProperties ReductionHint utils argsort argsort_sym cache_on_self cache_on_self_and_args ceildiv convert_shape_to_inductor convert_shape_to_symint developer_warning do_bench_using_profiling dtype_from_size get_dtype_size get_kernel_metadata GPU_ALIGN_BYTES ir_dataclass is_dynamic is_gpu sympy_dot sympy_index_symbol sympy_index_symbol_with_prefix sympy_product sympy_subs tensor_is_aligned virtualized ops OpsValue V TYPE_CHECKING torch _library fake_class_registry FakeScriptObject torch fx experimental symbolic_shapes SympyBoolean torch fx node Argument codegen cuda cuda_template CUDATemplate codegen wrapper PythonWrapperCodegen graph GraphLowering utils IndentedBuffer CUDATemplate TypeAlias = object try triton triton_version = triton __version__ has_triton = True except ImportError triton_version = None has_triton = False _P = ParamSpec _P _T = TypeVar _T _U = TypeVar _U _V = TypeVar _V _IntLike TypeAlias = Union int Expr _NumLike TypeAlias = Union int float Expr _OpOverloads TypeAlias = Union torch _ops OpOverload torch _ops HigherOrderOperator log = logging getLogger __name__ indent = functools partial textwrap indent prefix= aten = torch ops aten autotune_warmup = int os getenv TORCH_AUTOTUNE_WARMUP autotune_rep = int os getenv TORCH_AUTOTUNE_REP Note Inductor IR Inductor s IR produced executing lowering code see lowering py Each lowering registered particular aten operator expects inputs correspond aten schema However place torch Tensor inputs lowerings expect Inductor TensorBox inputs TensorBox IR represents torch tensors Tensors sometimes single objects owning storage sometimes views another Tensor s storage Mutating tensor operations such add_ affect underlying storage any associated views Other operations such t_ update metadata about current view don t modify underlying storage To model Inductor IR distinguishes between TensorBox View StorageBox Buffer TensorBox top level IR construct any lowering should produce maps torch Tensor output operation But just torch Tensors take different forms TensorBox IR can reference View IR directly reference StorageBox IRs Some Inductor lowerings produce new sets Box es while others such t other view ops may take existing TensorBox point new underlying View IR Tensors directly own storage represented chain TensorBox - StorageBox - Buffer where Buffer simple D allocation StorageBox introduces concept Layout If you mutate data such tensor we swing StorageBox pointer point new buffer leaving old buffer unmodified functionalizing operation Tensors backed views add one more indirection IR TensorBox - View - StorageBox - Buffer In these cases underlying StorageBox Buffer will shared pre-view TensorBox Computation represented Operation nodes each operation producing more output Buffers In case mutations these will new Buffers have mutated buffer listed its get_mutation_names It also possible have InputBuffer which there no corresponding Operation e g may graph input compile time constant _NodeOrNodes TypeAlias = Union int TensorBox dict str TensorBox Symbol IRNode Sequence Optional Union int dict str TensorBox TensorBox Symbol IRNode _is_static x object - TypeIs Union int Integer isinstance x int Integer dataclasses dataclass frozen=True GraphPartitionSignature symbol inputs necessary codegen symbol_inputs OrderedSet sympy Symbol mapping partition input name IRNode Expr Need name str since we cannot get name Expr input_nodes dict str Union IRNode sympy Expr TorchBindObject output_nodes list IRNode mapping partition input name boolean whether deallocating partition function input_deallocation dict str bool skip_cudagraph bool name constants read written graph partition constant_names list str validate_ir node_or_nodes Optional _NodeOrNodes - None _check_tensorbox nodes Optional _NodeOrNodes - None Could expand check deeper properties e g TensorBox points View StorageBox nodes None pass isinstance nodes list tuple node nodes _check_tensorbox node isinstance nodes dict node nodes values _check_tensorbox node assert isinstance nodes ExpandView DynamicScalar AssertScalar TensorBox sympy logic boolalg Boolean Expr int EffectfulKernel ShapeAsConstantBuffer f Found type nodes which supported top level IR node See Note Inductor IR Be picky about accepted data structure don t use pytree here _check_tensorbox node_or_nodes ops_wrapper name str - Callable OpsValue assert isinstance name str type name fn args object kwargs object - OpsValue getattr ops name args kwargs fn inverse_reorder order Sequence int - Callable Sequence _T Sequence _T inv_order = dict zip order range len order reindex index Sequence _T - Sequence _T assert len index == len inv_order index inv_order i i range len index reindex same_reorder order Sequence int - Callable Sequence _T Sequence _T reindex index Sequence _T - Sequence _T assert len index == len order index order i i range len index reindex fuse_reindexing reindex Callable Sequence _U Sequence _V reindex Callable Sequence _T Sequence _U - Callable Sequence _T Sequence _V reindex index Sequence _T - Sequence _V reindex reindex index reindex NHWC_STRIDE_ORDER = NHWDC_STRIDE_ORDER = get_fill_order seq Sequence Union int torch SymInt Expr shape_env Optional ShapeEnv = None - Sequence int Convert strides fill order argsort shape_env None all isinstance s int sympy Integer s seq sorted_idx Sequence int = argsort seq argsort_sym handles unbacked symints help shape_env sorted_idx = argsort_sym shape_env seq sorted_idx stride_order fill_order order Sequence Union int Integer - Sequence int Convert stride order fill order For channel last format stride order = fill order = lookup = pos idx idx pos enumerate order fill_order = lookup i i range len order fill_order get_stride_order seq Sequence Union int torch SymInt Expr shape_env Optional ShapeEnv = None - Sequence int Convert strides stride order sorted_idx Sequence int = get_fill_order seq shape_env out = _ range len seq i elem enumerate sorted_idx out elem = i out overload ir_node_to_tensor x None guard_shape bool = True - None overload ir_node_to_tensor x IRNode guard_shape bool = True - torch Tensor ir_node_to_tensor x Optional IRNode guard_shape bool = True - Optional torch Tensor x None None shape_fn Callable Union int Expr Union int Expr guard_shape shape_fn = V graph sizevars size_hint shape_fn = identity size = shape_fn s s x get_size stride StrideType is_storage_and_layout x stride = shape_fn s s x get_layout stride stride = FlexibleLayout contiguous_strides size dtype = x get_dtype device = x get_device size = convert_shape_to_symint size pyrefly ignore bad-assignment stride = convert_shape_to_symint stride V graph sizevars shape_env suppress_guards t = torch empty_strided size=size stride=stride dtype=dtype device=device zero_ t may_convert_to_optional value Optional Sequence _T - Optional Sequence Optional _T isinstance value list value None makes sure cpp wrapper codegen will generate something like std nullopt instead None value get_device_type x Union IRNode OutputSpec torch device None str - Optional str isinstance x str x None x isinstance x torch device x type isinstance x IRNode OutputSpec get_device_type x get_device pyrefly ignore bad-argument-type assert_never f get_device_type x type x __name__ is_triton x Union IRNode torch device None str - bool device = get_device_type x Special case cpu cuda using method below determine scheduler triton scheduler subclass requires instantiating scheduler them device cpu cuda getattr config f device _backend == triton True False device None device_scheduling = get_scheduling_for_device device None False codegen triton TritonScheduling assert isinstance device_scheduling type type device_scheduling issubclass device_scheduling TritonScheduling is_cpu x Union IRNode torch device None str - bool get_device_type x == cpu is_aligned_realized_tensor x Union Buffer TensorBox alignment int - bool isinstance x IRNode x maybe_get_stride None free_unbacked_symbols x get_stride free_unbacked_symbols x get_size False aligned_strides = sympy And sympy Eq Mod s alignment s x get_stride - aligned_last_dim = sympy Or sympy Eq x get_stride - sympy Le x get_size - is_aligned = sympy And aligned_strides aligned_last_dim Make sure guard recompile when necessary V graph sizevars guard_or_false is_aligned significant_strides_equal strides Sequence _IntLike strides Sequence _IntLike shape Sequence _IntLike - bool Returns true strides equal ignoring dimensions size assert len shape == len strides len strides == len strides dim s s zip shape strides strides V graph sizevars statically_known_leq dim continue V graph sizevars statically_known_equals s s V graph sizevars symbolic_hint s = V graph sizevars symbolic_hint s False True try_match_insignificant_strides tensor IRNode strides Sequence Union int torch SymInt - IRNode Tries match strides tensor those meta_strides Strides insignificant dimensions - size - will updated If there real stride differences NHWC vs NCHW tensor realized then input will returned is_storage_and_layout tensor tensor all V graph sizevars statically_known_equals s s s s zip strides tensor get_stride tensor significant_strides_equal strides tensor get_stride tensor get_size tensor storage old_layout = as_storage_and_layout tensor new_stride = old_layout stride i s enumerate tensor get_size V graph sizevars statically_known_leq s new_stride i = strides i new_layout = FixedLayout old_layout device old_layout dtype old_layout size new_stride old_layout offset old_layout is_pinned TensorBox ReinterpretView data=storage layout=new_layout gm_original_output_strides gm torch fx GraphModule - None output_node = gm graph find_nodes op= output output_node meta user_visible_output_idxs = idx idx _ enumerate output_node args torch _inductor compile_fx record_original_output_strides record_original_output_strides gm get_symbolic_inputs inputs Sequence IRNode - list Expr sym_vars OrderedSet Expr = OrderedSet inp inputs sym_vars &#124; = get_free_symbols inp get_size unbacked_only=False sym_vars &#124; = get_free_symbols inp get_stride unbacked_only=False list sym_vars IRNode Base all intermediate representation IR nodes TorchInductor Note This abstract base Most methods raise NotImplementedError must overridden concrete subclasses _current_origins ClassVar OrderedSet Any = OrderedSet NB These kinda weird origins OrderedSet Any = dataclasses field init=False traces back where IRNode created Inductor traceback Optional list str = dataclasses field init=False origin_node Optional torch fx Node = dataclasses field init=False staticmethod contextlib contextmanager current_origins origins OrderedSet Node - Generator None None None old = IRNode _current_origins IRNode _current_origins = old &#124; origins try yield finally IRNode _current_origins = old staticmethod is_realized_node node IRNode - bool isinstance node ComputedBuffer InputsKernel InputBuffer ReinterpretView TemplateBuffer _post_init_setattr attr str value Any - None Intended use __post_init__ enforcing invariant dataclass If you must can also used setting provenance info We would like try minimize these usages though object __setattr__ attr value __post_init__ - None origins = OrderedSet _current_origins _post_init_setattr origins origins _post_init_setattr traceback traceback format_stack config debug_ir_traceback None _post_init_setattr origin_node None get_read_names - OrderedSet str OrderedSet dep name dep get_reads get_traceback - Optional list str traceback get_origin_node - Optional torch fx Node origin_node get_defining_op - Optional Operation None get_stack_traces - OrderedSet str Return stack traces user model code A single IRNode could correspond multiple lines code stack_traces OrderedSet str = OrderedSet origins = origins isinstance ExternKernel origin_node = get_origin_node origin_node origins = OrderedSet origin_node node origins hasattr node stack_trace node stack_trace nodes backward graph don t have mapping pre_grad_graph stack_traces add node stack_trace pre_grad_nodes = torch _inductor debug _inductor_post_to_pre_grad_nodes get postToPre pyrefly ignore missing-attribute get node name isinstance pre_grad_nodes list continue node_name pre_grad_nodes stack_trace = torch _inductor debug _inductor_pre_grad_node_stack_trace get node_name None stack_trace stack_traces add stack_trace stack_traces common_repr shorten bool = True - Sequence str origins = f origins= getattr origins shorten len origins can get very long origins = f origins get_stack_traces origins stack_trace_str = stack_trace get_stack_traces stack_trace_str append stack_traces = stack_trace_str += stack_trace split \n stack_trace_str append origins + stack_trace_str str_helper lines Sequence object shorten bool = True multiline bool = True - str lines = list lines + list common_repr shorten lines = list map str lines multiline pyrefly ignore no-matching-overload new_lines = indent \n join lines f type __name__ \n new_lines \n f type __name__ lines get_dtype - torch dtype dtype maybe_get_dtype - Optional torch dtype try get_dtype except NotImplementedError None get_layout - Layout raise NotImplementedError f get_layout implemented type maybe_get_layout - Optional Layout try get_layout except NotImplementedError None get_output_spec - OutputSpec get_layout maybe_get_output_spec - Optional OutputSpec try get_output_spec except NotImplementedError None has_tensor_output - bool True single tensor output excludes MultiOutput isinstance maybe_get_output_spec Layout get_size - Sequence Expr raise NotImplementedError f get_size implemented type maybe_get_size - Optional Sequence _IntLike try get_size except NotImplementedError None property shape - Union _IntLike sympy Rel Sequence _IntLike get_size get_numel - Expr sympy_product get_size is_zero_elements - bool V graph sizevars statically_known_true sympy Eq get_numel realize - Optional str If IRNode refers data which has been materialized e g Pointwise Reduction could potentially have more compute fused into realize IRNode into physical memory ending possibility fusing into allowing e g multiple users access data without having recompute Check StorageBox realize particularly notable implementation TODO ezyang I think principle every IRNode should have implementation most time no-op OK you really do have audit each IRNode so now raise error s implemented Note some code graph py will catch thrown error suppress warning raise NotImplementedError f realize NYI type codegen_reference writer Optional IndentedBuffer = None - str raise NotImplementedError f codegen_reference NYI type get_device - Optional torch device None get_device_or_error - torch device device = get_device assert device None device has_exceeded_max_reads - bool False make_loader - Callable Sequence Expr OpsValue raise NotImplementedError type __name__ make_indexer - Callable Sequence Expr Expr raise NotImplementedError type __name__ get_stride - Sequence _IntLike raise NotImplementedError type __name__ maybe_get_stride - Optional Sequence _IntLike try get_stride except NotImplementedError None get_name - str raise NotImplementedError type __name__ maybe_get_name - Optional str try get_name except NotImplementedError None is_input_buffer - bool try get_name V graph graph_inputs except NotImplementedError False has_large_inner_fn threshold Optional int = None - bool False mark_reuse users int - None pass realize_hint - None pass unwrap_view - IRNode raise NotImplementedError type __name__ freeze_layout - None raise NotImplementedError type __name__ freeze_layout_with_stride_order order Sequence int allow_padding bool = False - None raise NotImplementedError type __name__ freeze_layout_with_fill_order order Sequence int - None raise NotImplementedError type __name__ freeze_layout_with_same_order stride Sequence _IntLike - None raise NotImplementedError type __name__ freeze_layout_with_exact_strides exact_strides Sequence _IntLike allow_padding bool = False - None raise NotImplementedError type __name__ get_read_writes - dependencies ReadWrites raise NotImplementedError type __name__ get_reads - OrderedSet Dep get_read_writes reads num_reads - int len get_reads get_storage_numel - _IntLike raise NotImplementedError type __name__ get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol raise NotImplementedError type __name__ get_reduction_type - Optional str raise NotImplementedError type __name__ get_reduction_size - Sequence Expr raise NotImplementedError type __name__ is_extern - bool False is_no_op - bool False constant_to_device device torch device - IRNode raise NotImplementedError type __name__ get_mutation_names - Sequence str raise NotImplementedError type __name__ get_operation_name - str raise NotImplementedError type __name__ get_inputs_that_alias_output - Sequence str raise NotImplementedError type __name__ TYPE_CHECKING property dtype - torch dtype ir_dataclass frozen=False Operation __post_init__ - None operation_name Optional str = None get_device - Optional torch device raise NotImplementedError get_origin_node - Optional torch fx Node assert hasattr origin_node origin_node get_origins - OrderedSet Any assert hasattr origins origins get_operation_name - str assert operation_name None operation_name is_extern - bool False is_no_op - bool False get_read_writes - dependencies ReadWrites raise NotImplementedError is_user_of name str - bool name get_read_names get_read_names - OrderedSet str OrderedSet dep name dep get_reads get_reads - OrderedSet Dep get_read_writes reads get_outputs - list Buffer raise NotImplementedError get_unbacked_symbol_defs - OrderedSet sympy Symbol OrderedSet get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol When unbacked_only=True Returns unbacked symbols which required scope order successfully perform codegen buffer For example buffer corresponds extern kernel call takes i argument would i here This used generate necessary dependencies ensure we actually bind i codegen before you try use Note NOT transitive particular buffer takes input another buffer dynamic shape e g i we will report here because you will already have dependency buffer which will eventually have dependency i necessary When unbacked_only=False Similar ` unbacked_only=True ` including all free symbols instead only free unbacked symbols OrderedSet get_workspace_size - int Gets extra global memory size needed buffer Some algorithms e g group gemm may require extra global memory generated code ir_dataclass Loops IRNode device torch device dtype torch dtype inner_fn Callable Any ranges Sequence _IntLike cache_on_self_and_args Loops get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol OrderedSet union get_free_symbols e unbacked_only e ranges inner_fn_free_symbols unbacked_only _to_str names Sequence str - str str_helper f device type str dtype inner_fn_str + f name = getattr name name names + f origin_node= origin_node r __post_init__ - None super __post_init__ __str__ - str _to_str ranges __repr__ = __str__ get_device - Optional torch device device get_origin_node - Optional torch fx Node origin_node get_size - Sequence Expr ranges get_pointwise_size - Sequence Expr ranges classmethod create cls args Any kwargs Any - Union TensorBox ShapeAsConstantBuffer origin_node = kwargs pop origin_node None tb = kwargs pop traceback None r = cls args kwargs Need explicitly set origin_node here propagate down todo chilli I think would better IRNode directly set origin_node r _post_init_setattr origin_node origin_node r _post_init_setattr traceback tb r traceback TensorBox create r staticmethod _index ranges Sequence _IntLike prefix SymT = SymT INDEX - Sequence Expr sympy S Zero s == sympy_index_symbol_with_prefix prefix n n s enumerate ranges cache_on_self inner_fn_opcount - OpCountResult opcounter = OpCounterCSE V MockHandler V set_ops_handler opcounter patch object FlexibleLayout allow_indexing True inner_fn inner_fn_args opcounter getvalue inner_fn_args - Sequence Sequence _IntLike _index ranges cache_on_self inner_fn_str - str V KernelFormatterHandler ir_to_string inner_fn inner_fn_args has_large_inner_fn threshold Optional int = None - bool threshold None threshold = threshold = max threshold config realize_opcount_threshold inner_fn_opcount num_ops threshold inner_fn_free_symbols unbacked_only bool = False - OrderedSet Symbol index = _index ranges extract_free_symbols inner_fn index unbacked_only=unbacked_only get_reads - OrderedSet Dep patch object FlexibleLayout allow_indexing True get_reduction_type extract_read_writes make_loader get_size get_reduction_size reads extract_read_writes make_loader get_size reads get_read_names - OrderedSet str OrderedSet inner_fn_opcount read_buffers num_reads - int len inner_fn_opcount read_buffers get_reduction_size - Sequence Expr raise NotImplementedError f get_reduction_size implemented type get_reduction_type - Optional str raise NotImplementedError f get_reduction_type implemented type constant_to_device device torch device - IRNode raise NotImplementedError f constant_to_device implemented type nop_loader_fn idx Union Expr Sequence Expr dtype torch dtype - OpsValue dtype is_floating_point ops constant float nan dtype ops constant dtype ir_dataclass Pointwise Loops make_loader - Callable Sequence Expr OpsValue Make zero-element loops into no-op is_zero_elements partial nop_loader_fn dtype=self dtype inner_fn __str__ - str _to_str ranges __repr__ = __str__ get_reduction_size - Sequence sympy Expr get_reduction_type - Optional str None store_output output_name Optional str indexer Callable Sequence Expr Never vars Sequence Expr - None loader = make_loader ops store output_name unnamed indexer vars loader vars constant_to_device device torch device - IRNode Move given device Requires all reads constants loader = make_loader loader = patch object ConstantBuffer override_device device loader Pointwise device=device dtype=self dtype inner_fn=loader ranges=self ranges ir_dataclass Scatter Pointwise output_indexer Callable Sequence Expr Expr scatter_mode StoreMode = None constant_to_device device torch device - IRNode Move given device Requires all reads constants loader = make_loader loader = patch object ConstantBuffer override_device device loader Scatter device=device dtype=self dtype inner_fn=loader ranges=self ranges output_indexer=self output_indexer scatter_mode=self scatter_mode store_output output_name Optional str indexer Callable Sequence Expr Never vars Sequence Expr - Any loader = make_loader output_name None output_name = unnamed ops store output_name indexer output_indexer vars loader vars mode=self scatter_mode REDUCTION_COMBINE_FN dict str Callable OpsValue = any ops_wrapper logical_or max ops_wrapper maximum min ops_wrapper minimum prod ops_wrapper mul sum ops_wrapper add dot ops_wrapper add xor_sum ops_wrapper bitwise_xor get_reduction_combine_fn reduction_type str dtype torch dtype arg_break_ties_left bool = True - Callable object reduction_type REDUCTION_COMBINE_FN REDUCTION_COMBINE_FN reduction_type reduction_type argmax argmin argmax_combine_fn tuple object object b tuple object object - tuple OpsValue OpsValue a_value a_index = b_value b_index = b reduction_type == argmin mask = ops lt a_value b_value mask = ops gt a_value b_value equal = ops eq a_value b_value is_float_dtype dtype a_isnan = ops ne a_value a_value b_isnan = ops ne b_value b_value mask = ops logical_or mask ops gt a_isnan b_isnan equal = ops logical_or equal ops logical_and a_isnan b_isnan tie = ops lt a_index b_index arg_break_ties_left ops gt a_index b_index mask = ops logical_or mask ops logical_and equal tie ops where mask a_value b_value ops where mask a_index b_index argmax_combine_fn reduction_type == welford_combine welford_combine_fn tuple OpsValue OpsValue OpsValue b tuple OpsValue OpsValue OpsValue - tuple OpsValue OpsValue OpsValue a_mean a_m a_weight = b_mean b_m b_weight = b delta = b_mean - a_mean new_weight = a_weight + b_weight w _over_w = b_weight new_weight a_mean + delta w _over_w a_m + b_m + delta delta a_weight w _over_w new_weight welford_combine_fn raise NotImplementedError f unknown reduction_type= reduction_type ir_dataclass Reduction Loops reduction_ranges Sequence _IntLike reduction_type ReductionType dtype represents dst dtype src_dtype torch dtype reduction_hint ReductionHint __str__ - str _to_str ranges reduction_ranges reduction_type __repr__ = __str__ cache_on_self_and_args Reduction get_free_symbol_uses unbacked_only bool = False - OrderedSet Symbol super get_free_symbol_uses unbacked_only &#124; OrderedSet union get_free_symbols e unbacked_only e reduction_ranges get_reduction_size - Sequence Expr reduction_ranges get_reduction_type - Optional str reduction_type store_reduction output_name Optional str indexer Callable Sequence Expr Never vars Sequence Expr reduction_vars Sequence Symbol - None value = ops reduction dtype src_dtype reduction_type inner_fn vars reduction_vars ops store_reduction output_name unnamed indexer vars value index_length - int len ranges + len reduction_ranges inner_fn_args - Sequence Sequence Expr index = _index ranges rindex = _index reduction_ranges SymT R _INDEX index rindex inner_fn_free_symbols unbacked_only bool = False - OrderedSet Symbol index = _index ranges rindex = _index reduction_ranges SymT R _INDEX extract_free_symbols inner_fn index rindex unbacked_only=unbacked_only constant_to_device device torch device - IRNode Move given device Requires all reads constants loader = make_loader loader = patch object ConstantBuffer override_device device loader Reduction device=device dtype=self dtype inner_fn=loader ranges=self ranges reduction_ranges=self reduction_ranges reduction_type=self reduction_type src_dtype=self src_dtype reduction_hint=ReductionHint DEFAULT staticmethod num_splits device torch device dst_dtype torch dtype src_dtype torch dtype inner_fn Callable _P OpsValue ranges Sequence _IntLike reduction_ranges Sequence _IntLike reduction_type Union ReductionType Literal scan reduction_numel Expr input_node Optional IRNode = None - tuple ReductionHint _IntLike reduction_numel_hint = V graph sizevars symbolic_hint reduction_numel numel_hint = V graph sizevars symbolic_hint sympy_product ranges should_split = reduction_type == scan V graph has_feature device BackendFeature REDUCE_TO_SINGLE_ELEMENT reduction_type argmax argmin config split_reductions _is_static reduction_numel_hint _is_static numel_hint We don t support unbacked symints ReductionHint DEFAULT reduction_type == dot Don t split when doing native matmul ReductionHint DEFAULT props = DeviceProperties create device num_sm = props multi_processor_count min_elements_per_thread = should_split inner_reduction_splits Callable int int int = functools partial V choices reduction_split_factor device inner_reduction=True outer_reduction_splits Callable int int int = functools partial V choices reduction_split_factor device inner_reduction=False inner_reduction_splits reduction_numel_hint int numel_hint int - int outer_reduction_splits = inner_reduction_splits easy cases numel_hint == split = inner_reduction_splits reduction_numel_hint numel_hint split == No need split ReductionHint INNER split input_node None isinstance input_node TensorBox patch object FlexibleLayout allow_indexing True new_ranges new_reduction_ranges = extract_input_node_reduction_ranges input_node new_ranges None new_reduction_ranges None extracted_numel_hint = V graph sizevars symbolic_hint sympy_product new_ranges + new_reduction_ranges reduction_numel_hint == extracted_numel_hint log debug Use previous IRNode s range reduction_ranges instead split current ranges s current reduction ranges s current split d new ranges s new reduction ranges s ranges reduction_ranges split new_ranges new_reduction_ranges If input_node its dependent nodes also Reduction nodes use reduction_sizes node its dependent nodes directly ReductionHint INNER - ReductionHint INNER split reduction_numel_hint = min_elements_per_thread numel_hint = num_sm ReductionHint DEFAULT r = Reduction device=device dtype=dst_dtype inner_fn=inner_fn ranges=ranges reduction_ranges=reduction_ranges reduction_type=reduction_type reduction_type = scan sum src_dtype=src_dtype reduction_hint=ReductionHint DEFAULT get_read_indices r Reduction - tuple Sequence Expr bool device = r get_device assert device None cb = ComputedBuffer name=None layout=FlexibleLayout device=device dtype=r get_dtype size=r get_size data=r read_writes = cb get_read_writes try finding full size producer TODO will fail something like N N sum would also possibly wrong producers different contiguity we hope those cases rare assert read_writes range_vars None range_vars = r r read_writes range_vars isinstance r Expr isinstance r sympy Number indices = changed = False md sorted read_writes reads key=lambda x x name all r md index free_symbols r range_vars indices append md index md name V graph name_to_buffer buf = V graph name_to_buffer md name original_stride = getattr buf layout stride None buf decide_layout getattr buf layout stride None = original_stride changed = True indices changed indices changed = get_read_indices r changed indices _ = get_read_indices r len indices == TODO determine splits when all inputs broadcast ReductionHint DEFAULT _ reduction_vars ranges = dependencies index_vars_squeeze r get_size r get_reduction_size num_outer = num_inner = i indices j = V graph sizevars simplify_with_ranges i ranges strides = V graph sizevars stride_hints j reduction_vars list ranges keys outer = all s s strides outer num_outer += num_inner += num_inner num_outer ReductionHint INNER inner_reduction_splits reduction_numel_hint numel_hint ReductionHint OUTER outer_reduction_splits reduction_numel_hint numel_hint staticmethod _unroll_reduction_fn inner_fn Callable Sequence _IntLike Sequence _IntLike OpsValue reduction_ranges Sequence _IntLike reduction_type str src_dtype torch dtype - Callable Sequence _IntLike OpsValue Convert inner_fn reduction pointwise reduction_ranges = V graph sizevars guard_int_seq reduction_ranges combine_fn = get_reduction_combine_fn reduction_type src_dtype fn index Sequence _IntLike - Any functools reduce combine_fn value_fn index rindex rindex itertools product range x x reduction_ranges value_fn Callable Sequence _IntLike Sequence _IntLike Any reduction_type argmin argmax flatten_index = _fixed_indexer reduction_ranges FlexibleLayout contiguous_strides reduction_ranges value_fn index Sequence _IntLike rindex Sequence _IntLike - tuple OpsValue OpsValue rindex = sympy expand i i rindex inner_fn index rindex ops index_expr flatten_index rindex torch int lambda index fn index value_fn = inner_fn fn classmethod pyrefly ignore bad-override create cls device torch device dst_dtype torch dtype src_dtype torch dtype inner_fn Callable Any ranges Sequence Expr reduction_ranges Sequence Expr reduction_type ReductionType reduction_hint ReductionHint = ReductionHint DEFAULT input_node Optional IRNode = None - Union TensorBox ShapeAsConstantBuffer Create reduction node May split reduction multiple layers expose more parallelism reduction_numel = V graph sizevars simplify sympy_product reduction_ranges reduction_numel == N B This hack generate literal given type Ideally we should fixing ` constant ` triton py breaks due hardcoded dtypes other places py_cnst val object - Union bool float int dst_dtype == torch bool bool val dst_dtype is_floating_point assert isinstance val SupportsFloat type val float val assert isinstance val SupportsInt type val int val rtypes_to_inits = sum py_cnst xor_sum py_cnst prod py_cnst any py_cnst all desugared ` any val ` assert reduction_type rtypes_to_inits keys f reduction_type supported zero-dimension tensors const_fn index int - OpsValue ops constant rtypes_to_inits reduction_type dst_dtype Pointwise create device=device dtype=src_dtype inner_fn=const_fn ranges=list ranges reduction_numel == reduction actually pointwise op reduction_type argmin argmax fn index int - OpsValue ops constant dst_dtype fn index int - OpsValue reduction_index = sympy S Zero _ reduction_ranges inner_fn index reduction_index Pointwise create device=device dtype=dst_dtype inner_fn=fn ranges=ranges isinstance reduction_numel Integer V graph sizevars size_hint_or_throw reduction_numel config unroll_reductions_threshold sympy_product ranges = is_gpu device type reduction_type = dot When native matmul don t unroll dot reduction NB This works around https github com pytorch pytorch issues since turning reductions into pointwise ops can exacerbate problem Pointwise create device=device dtype=dst_dtype inner_fn=cls _unroll_reduction_fn inner_fn reduction_ranges reduction_type src_dtype ranges=ranges triton doesn t support reduce single element well so break up hint split = cls num_splits device dst_dtype src_dtype inner_fn ranges reduction_ranges reduction_type reduction_numel input_node _maybe_increase_split split int - int don t apply min_num_split constraint static shape case _is_static reduction_numel split split max split config min_num_split split split = _maybe_increase_split split intermediate reduction split can contain complex indexing num_splits will fail correctly set hint reuse passed hint available reduction_hint == ReductionHint DEFAULT reduction_hint = hint split == - assert input_node None patch object FlexibleLayout allow_indexing True new_ranges new_reduction_ranges = extract_input_node_reduction_ranges input_node assert new_ranges None assert new_reduction_ranges None cls create_multilayer_existing_ranges device dst_dtype src_dtype inner_fn ranges reduction_ranges new_ranges new_reduction_ranges reduction_type reduction_hint split triton doesn t support reduce single element well so break up out = cls create_multilayer device dst_dtype src_dtype inner_fn ranges reduction_ranges reduction_type split reduction_hint input_node Find reduction get split split_reduction = None config triton mix_order_reduction isinstance out TensorBox _find_split_reduction cur_node TensorBox - Optional ComputedBuffer read_names = cur_node get_read_names len read_names = None bufname = next iter read_names bufname V graph name_to_buffer None buf = V graph name_to_buffer bufname isinstance buf ComputedBuffer None assert buf data get_reduction_type None buf split_reduction = _find_split_reduction out split_reduction If reduction split more than layers say there layers we always have correct setting layer top layer The setting layer may incorrect s fine since they never get used TODO should we skip setting these fields layer assert isinstance split_reduction data Reduction f type split_reduction data split_reduction _split_size = split_reduction data reduction_ranges split_reduction _original_inner_fn = inner_fn split_reduction _original_ranges = ranges split_reduction _original_reduction_ranges = reduction_ranges out out = TensorBox create Reduction device=device dtype=dst_dtype inner_fn=inner_fn ranges=ranges reduction_ranges=reduction_ranges reduction_type=reduction_type src_dtype=src_dtype reduction_hint=reduction_hint out staticmethod default_accumulator reduction_type str dtype torch dtype - Union _NumLike Sequence _NumLike reduction_type max argmax is_float_dtype dtype float -inf is_boolean_dtype dtype False torch iinfo dtype min reduction_type min argmin is_float_dtype dtype float inf is_boolean_dtype dtype True torch iinfo dtype max zero = False is_boolean_dtype dtype one = True is_boolean_dtype dtype sum zero prod one dot zero xor_sum zero any zero welford_reduce zero zero zero welford_combine zero zero zero online_softmax_reduce float -inf zero reduction_type staticmethod default_value reduction_type str dtype torch dtype - Union _NumLike Sequence _NumLike reduction_type == welford_reduce Reduction default_accumulator reduction_type dtype staticmethod _multilayer_second_step_hint split _IntLike numel_hint int reduction_hint ReductionHint - ReductionHint split == - reduction_hint split = numel_hint = reduction_hint == ReductionHint OUTER ReductionHint OUTER_TINY split = numel_hint = reduction_hint == ReductionHint OUTER ReductionHint OUTER_TINY reduction_hint classmethod check_for_split_dense_dim_reindexing cls reduction_numel _IntLike input_node Optional IRNode - Optional int If we reducing over full tensor non-dense last dimension reindex so we reduce over dense dimension initially just handle complete reduction case input_node None None V graph sizevars statically_known_equals input_node get_numel reduction_numel None input_node realize try finalize layout as_storage_and_layout input_node except NotImplementedError None strides = input_node get_stride i s enumerate strides - V graph sizevars statically_known_equals s i None classmethod _multilayer_wrap_loader cls loader Callable OpsValue reduction_ranges Sequence _IntLike reduction_numel _IntLike split _IntLike block_size _IntLike default Union _NumLike Sequence _NumLike input_node Optional IRNode = None - Callable object dense_index = cls check_for_split_dense_dim_reindexing reduction_numel input_node reindex = View dynamic_reshape_indexer reduction_ranges reduction_numel dense_index need_mask = V graph sizevars statically_known_true sympy Eq reduction_numel split wrapper_fn index Sequence Symbol reduction_index Sequence Symbol - OpsValue reduction_index = reduction_index new_index reduction_block = index indices = block_size reduction_block + reduction_index body - OpsValue loader new_index reindex indices need_mask index_dtype = dtype_from_size reduction_numel mask = ops lt ops index_expr indices index_dtype ops index_expr reduction_numel index_dtype ops masked mask body default body wrapper_fn classmethod _multilayer_wrap_loader_existing_ranges cls loader Callable Sequence Expr Sequence Expr OpsValue original_ranges Sequence Expr original_reduction_ranges Sequence Expr new_ranges Sequence Integer new_reduction_ranges Sequence Integer - Callable Sequence sympy Expr Sequence sympy Expr OpsValue assert all r == r original_ranges f Only enabled numel_hint == found original_ranges= reindex = View dynamic_reshape_indexer original_reduction_ranges tuple new_ranges + tuple new_reduction_ranges wrapper_fn merged_index Sequence Expr new_reduction_index Sequence Expr - OpsValue original_idx = merged_index len original_ranges new_index = merged_index len original_ranges loader original_idx reindex tuple new_index + tuple new_reduction_index wrapper_fn classmethod create_multilayer_helper cls device torch device dst_dtype torch dtype src_dtype torch dtype wrapper_fn Callable Any original_ranges Sequence Expr original_reduction_ranges Sequence Expr new_ranges list Expr new_reduction_ranges list Integer reduction_type ReductionType split _IntLike reduction_hint ReductionHint - Union TensorBox ShapeAsConstantBuffer Break large reduction up into multiple smaller reductions recursively triton will automatically compute reductions fp reducing over fp bf within kernel keep intermediate fp so keep whole reduction fp reduce precision breaking up kernel into multiple layers intermediate_dtype = dst_dtype dst_dtype torch float torch bfloat torch float intermediate = Reduction create device intermediate_dtype src_dtype wrapper_fn new_ranges new_reduction_ranges reduction_type reduction_hint intermediate realize intermediate_loader = intermediate make_loader intermediate_fn index Sequence _IntLike reduction_index Sequence _IntLike - OpsValue intermediate_loader index reduction_index numel_hint = V graph sizevars size_hint sympy_product original_ranges reduction_hint = cls _multilayer_second_step_hint split numel_hint reduction_hint assert original_ranges == new_ranges len original_ranges TensorBox create Reduction device=device dtype=dst_dtype inner_fn=intermediate_fn ranges=original_ranges reduction_ranges=new_ranges len original_ranges reduction_type=reduction_type src_dtype=src_dtype reduction_hint=reduction_hint classmethod create_multilayer cls device torch device dst_dtype torch dtype src_dtype torch dtype inner_fn Callable Any ranges Sequence Expr reduction_ranges Sequence Expr reduction_type ReductionType split _IntLike reduction_hint ReductionHint input_node Optional IRNode = None - Union TensorBox ShapeAsConstantBuffer Break large reduction up into multiple smaller reductions recursively TODO jansel realize reduction so we can do dynamic indexing reduction_numel = sympy_product reduction_ranges block_size = FloorDiv reduction_numel + split - split default = cls default_value reduction_type dst_dtype wrapper_fn = cls _multilayer_wrap_loader inner_fn reduction_ranges reduction_numel split block_size default input_node cls create_multilayer_helper device dst_dtype src_dtype wrapper_fn ranges reduction_ranges ranges split block_size reduction_type split reduction_hint classmethod create_multilayer_existing_ranges cls device torch device dst_dtype torch dtype src_dtype torch dtype inner_fn Callable Any original_ranges Sequence Expr original_reduction_ranges Sequence Expr new_ranges list Integer new_reduction_ranges list Integer reduction_type ReductionType reduction_hint ReductionHint - Union TensorBox ShapeAsConstantBuffer Break large reduction up into multiple smaller reductions recursively wrapper_fn = cls _multilayer_wrap_loader_existing_ranges inner_fn original_ranges original_reduction_ranges new_ranges new_reduction_ranges cls create_multilayer_helper device dst_dtype src_dtype wrapper_fn original_ranges original_reduction_ranges original_ranges new_ranges new_reduction_ranges reduction_type - reduction_hint _fixed_indexer size Sequence int stride Optional Sequence int = None offset Expr = Integer - Callable Sequence Expr Expr A closure containing math read given element indexer index Sequence int - int assert stride None len index == len stride assert len index == len size result = offset idx st sz zip index stride size sz = result = result + idx st result indexer INNER_FN_TY TypeAlias = Callable Sequence Expr Sequence Expr OpsValue MultiOutputReduction Reduction output_index int __init__ device torch device dst_dtype torch dtype inner_fns Union INNER_FN_TY Sequence INNER_FN_TY ranges Sequence Integer reduction_ranges Sequence Integer reduction_type ReductionType src_dtype torch dtype reduction_hint ReductionHint output_index int callable inner_fns inner_fns = inner_fns loader Callable Sequence Expr Sequence Expr Any len inner_fns == loader = inner_fns loader idx Sequence Expr reduction_idx Sequence Expr - tuple OpsValue tuple fn idx reduction_idx fn inner_fns super __init__ device=device dtype=dst_dtype inner_fn=loader ranges=ranges reduction_ranges=reduction_ranges reduction_type=reduction_type src_dtype=src_dtype reduction_hint=reduction_hint output_index = output_index store_reduction output_name Optional str indexer Callable Sequence Expr Never vars Sequence Expr reduction_vars Sequence Symbol - Any values = ops reduction dtype src_dtype reduction_type inner_fn vars reduction_vars assert isinstance values tuple list type values value = values output_index ops store_reduction output_name unnamed indexer vars value OnlineSoftmaxReduction MultiOutputReduction classmethod create type ignore override cls device torch device dst_dtype torch dtype src_dtype torch dtype inner_fn Callable Any ranges Sequence Expr reduction_ranges Sequence Expr num_output int reduction_hint ReductionHint = ReductionHint DEFAULT input_node Optional IRNode = None - Sequence Union TensorBox ShapeAsConstantBuffer Create reduction disregarding splitting results = tuple TensorBox create MultiOutputReduction device dst_dtype inner_fn ranges reduction_ranges online_softmax_reduce src_dtype reduction_hint output_idx output_idx range num_output t results t realize results WelfordReduction MultiOutputReduction classmethod create type ignore override cls device torch device dtype torch dtype inner_fns Sequence Callable Any ranges list Integer reduction_ranges list Integer reduction_type ReductionType reduction_hint ReductionHint = ReductionHint DEFAULT - Sequence Union TensorBox ShapeAsConstantBuffer assert reduction_type welford_reduce welford_combine reduction_numel = V graph sizevars simplify sympy_product reduction_ranges const val int - Union TensorBox ShapeAsConstantBuffer inner_fn idx Sequence Expr - OpsValue ops constant val dtype Pointwise create device=device dtype=dtype inner_fn=inner_fn ranges=list ranges reduction_numel == mean = const m = const weight = const mean m weight reduction_numel == copy loader Callable Sequence Expr Sequence Expr OpsValue - Union TensorBox ShapeAsConstantBuffer inner_fn idx Sequence Expr - OpsValue reduction_index = sympy S Zero _ reduction_ranges loader idx reduction_index Pointwise create device=device dtype=dtype inner_fn=inner_fn ranges=list ranges reduction_type == welford_reduce copy inner_fns const const tuple copy fn fn inner_fns TODO Unrolled reduction isinstance reduction_numel Integer V graph sizevars size_hint reduction_numel config unroll_reductions_threshold sympy_product ranges = Pointwise create device dst_dtype cls _unroll_reduction_fn inner_fn reduction_ranges reduction_type src_dtype ranges triton doesn t support reduce single element well so break up hint split = Reduction num_splits device dtype dtype inner_fns ranges reduction_ranges reduction_type=reduction_type reduction_numel=reduction_numel intermediate reduction split can contain complex indexing num_splits will fail correctly set hint reuse passed hint available reduction_hint == ReductionHint DEFAULT reduction_hint = hint split triton doesn t support reduce single element well so break up cls create_multilayer device dtype inner_fns ranges reduction_ranges reduction_type split reduction_hint results = TensorBox create WelfordReduction device dtype inner_fns ranges reduction_ranges reduction_type dtype reduction_hint output_idx output_idx range t results t realize results staticmethod default_value reduction_type str dtype torch dtype - Union _NumLike Sequence _NumLike classmethod create_multilayer type ignore override cls device torch device dtype torch dtype inner_fns Sequence Callable Any ranges list Integer reduction_ranges list Integer reduction_type ReductionType split _IntLike reduction_hint ReductionHint - Sequence Union TensorBox ShapeAsConstantBuffer Break large reduction up into multiple smaller reductions recursively reduction_numel = sympy_product reduction_ranges need_mask = V graph sizevars statically_known_true sympy Eq reduction_numel split need_mask reduction_type = welford_combine If we need mask then welford_reduce doesn t work because masked inputs shouldn t count towards welford weight constant idx Sequence Expr reduction_idx Sequence Expr value int - OpsValue ops constant value dtype cls create_multilayer device=device dtype=dtype inner_fns= inner_fns partial constant value= partial constant value= ranges=ranges reduction_ranges=reduction_ranges reduction_type= welford_combine split=split reduction_hint=reduction_hint block_size = FloorDiv reduction_numel + split - split intermediates = WelfordReduction create device dtype tuple cls _multilayer_wrap_loader loader reduction_ranges reduction_numel split block_size default= loader inner_fns ranges split block_size reduction_type reduction_hint i intermediates i realize intermediate_loader_fn index Sequence Expr reduction_index Sequence Expr loader Callable Sequence Expr OpsValue - OpsValue loader index reduction_index numel_hint = V graph sizevars size_hint sympy_product ranges reduction_hint = cls _multilayer_second_step_hint split numel_hint reduction_hint WelfordReduction create device dtype tuple partial intermediate_loader_fn loader=i make_loader i intermediates ranges split welford_reduce turns one input into three outputs which combined welford_combine welford_combine reduction_hint ir_dataclass Scan Loops scan_ranges list Integer size list Integer combine_fn Callable tuple Any tuple Any tuple Any reindex Callable Sequence _IntLike Sequence _IntLike Sequence _IntLike reduction_hint ReductionHint output_index int output_index indexes following tuples dtypes tuple torch dtype inner_fns tuple Callable Any HACK we mimic reduction cache_on_self_and_args Scan get_free_symbol_uses unbacked_only bool = False - OrderedSet Symbol TODO Can combine_fn reindex close over unbacked symbols If so we need explicitly represent closure so we can pull out unbacked symbols here super get_free_symbol_uses unbacked_only &#124; OrderedSet union get_free_symbols e unbacked_only e scan_ranges &#124; OrderedSet union get_free_symbols e unbacked_only e size __post_init__ - None assert len ranges + len scan_ranges == len size super __post_init__ store_reduction output_name Optional str indexer Callable Sequence _IntLike Never vars Sequence Expr scan_vars Sequence Symbol - Any idx = reindex vars scan_vars values = tuple inner_fn idx inner_fn inner_fns result = ops scan dtypes combine_fn values ops store output_name unnamed indexer idx result output_index get_reduction_type - Optional str scan_op custom get_reduction_size - Sequence Expr scan_ranges get_size - Sequence Expr size get_pointwise_size - Sequence Expr ranges index_length - int len ranges + len scan_ranges inner_fn_args - Sequence Sequence _IntLike index = _index ranges rindex = _index scan_ranges SymT R _INDEX idx = reindex index rindex idx inner_fn_free_symbols unbacked_only bool = False - OrderedSet Symbol index = _index ranges rindex = _index scan_ranges SymT R _INDEX idx = reindex index rindex extract_free_symbols inner_fn idx unbacked_only=unbacked_only classmethod create type ignore override cls device torch device dtypes tuple torch dtype inner_fns tuple Callable Sequence Expr Any size list Integer axis int combine_fn Callable tuple Any tuple Any tuple Any reduction_hint ReductionHint = ReductionHint DEFAULT Whether we have option fallback aten can_fallback_to_aten bool = True kwargs Any - Sequence Optional Union TensorBox ShapeAsConstantBuffer pointwise_ranges = size axis size axis + scan_ranges = size axis V graph has_feature device BackendFeature SCAN None len dtypes len dtypes V graph has_feature device BackendFeature TUPLE_REDUCTION None len dtypes sizevars = V graph sizevars scan_numel = sizevars simplify sympy_product scan_ranges assert len dtypes == len inner_fns Scan single element just copy sizevars statically_known_true sympy Le scan_numel Pointwise create device=device dtype=dtypes output_index inner_fn=inner_fns output_index ranges=size output_index range len dtypes reduction_hint num_splits = cls num_splits device=device dtype=dtypes inner_fn=inner_fns axis=axis pointwise_ranges=pointwise_ranges scan_ranges=scan_ranges combine_fn=combine_fn scan_numel=scan_numel scan_type = Scan num_splits supports_split = pyrefly ignore unsupported-operation torch version hip None has_triton triton_version = len dtypes == supports_split can_fallback_to_aten Fallback ATen None len dtypes num_splits = scan_type = SplitScan reindex index Sequence Expr scan_index Sequence Expr - list Expr assert len scan_index == len scan_ranges assert len index == len pointwise_ranges index axis scan_index index axis results = TensorBox create scan_type device=device dtype=dtypes output_index dtypes=dtypes inner_fn=inner_fns output_index inner_fns=inner_fns size=size ranges=pointwise_ranges scan_ranges=scan_ranges combine_fn=combine_fn reindex=reindex reduction_hint=reduction_hint output_index=output_index kwargs output_index range len dtypes result results result realize results classmethod num_splits cls device torch device dtype torch dtype inner_fn Callable Sequence Expr OpsValue axis int pointwise_ranges list Integer scan_ranges list Integer combine_fn Callable tuple Any tuple Any tuple Any scan_numel Expr - tuple ReductionHint _IntLike TODO custom splitting heuristic scan wrapper_fn idx Sequence Expr reduction_idx Sequence Expr - OpsValue inner_fn idx axis reduction_idx idx axis Reduction num_splits device=device dst_dtype=dtype src_dtype=dtype inner_fn=wrapper_fn ranges=pointwise_ranges reduction_ranges=scan_ranges reduction_type= scan reduction_numel=scan_numel This signifies scan op should go through TritonSplitScanKernel codegen CUDA ir_dataclass SplitScan Scan pass ir_dataclass Sort Loops Sorts tuple key value pairs sort_ranges list Integer size list Integer reindex Callable Sequence Expr Sequence Expr Sequence Expr reduction_hint ReductionHint output_index int output_index indexes following tuples dtypes tuple torch dtype inner_fns tuple Callable Any stable bool descending bool HACK we mimic reduction cache_on_self_and_args Sort get_free_symbol_uses unbacked_only bool = False - OrderedSet Symbol super get_free_symbol_uses unbacked_only &#124; OrderedSet union get_free_symbols e unbacked_only e sort_ranges &#124; OrderedSet union get_free_symbols e unbacked_only e size __post_init__ - None assert len ranges + len sort_ranges == len size super __post_init__ store_reduction output_name Optional str indexer Callable Sequence Expr Expr vars Sequence Expr reduction_vars Sequence Expr - Any idx = reindex vars reduction_vars values = tuple inner_fn idx inner_fn inner_fns result = ops sort dtypes values stable descending ops store output_name unnamed indexer idx result output_index get_reduction_type - Optional str sort get_reduction_size - Sequence Expr sort_ranges get_size - Sequence Expr size get_pointwise_size - Sequence Expr ranges index_length - int len ranges + len sort_ranges inner_fn_args - Sequence Sequence Expr index = _index ranges rindex = _index sort_ranges SymT R _INDEX idx = reindex index rindex idx inner_fn_free_symbols unbacked_only bool = False - OrderedSet Symbol index = _index ranges rindex = _index sort_ranges SymT R _INDEX idx = reindex index rindex extract_free_symbols inner_fn idx unbacked_only=unbacked_only classmethod create type ignore override cls device torch device dtypes tuple torch dtype inner_fns tuple Callable list Expr Any size list Integer axis int stable bool descending bool reduction_hint ReductionHint = ReductionHint DEFAULT kwargs Any - Sequence Optional Union TensorBox ShapeAsConstantBuffer pointwise_ranges = size axis size axis + sort_ranges = size axis V graph has_feature device BackendFeature SORT None len dtypes sizevars = V graph sizevars sort_numel = sizevars simplify sympy_product sort_ranges Heuristic smallest rblock where triton usually outperforms aten sort It also isn t bandwidth bound so fusion unlikely help max_rblock = is_persistent_kernel = config triton persistent_reductions sizevars statically_known_true sympy Le sort_numel max_rblock is_persistent_kernel We only support persistent triton kernels None len dtypes assert len dtypes == len inner_fns Sort single element just copy sizevars statically_known_true sympy Le sort_numel Pointwise create device=device dtype=dtypes output_index inner_fn=inner_fns output_index ranges=size output_index range len dtypes reindex index Sequence Expr sort_index Sequence Expr - list Expr assert len sort_index == len sort_ranges assert len index == len pointwise_ranges index axis sort_index index axis results = TensorBox create Sort device=device dtype=dtypes output_index dtypes=dtypes inner_fn=inner_fns output_index inner_fns=inner_fns size=size ranges=pointwise_ranges sort_ranges=sort_ranges reindex=reindex reduction_hint=reduction_hint output_index=output_index stable=stable descending=descending kwargs output_index range len dtypes result results result realize results is_storage_and_layout x IRNode - bool try as_storage_and_layout x freeze=False True except NotImplementedError False is_contiguous_storage_and_layout x IRNode - bool try _buffer layout = as_storage_and_layout x freeze=False pad stride here so we will NOT claim tensor contiguous padding gonna happen layout should_pad_strides layout pad_strides layout is_contiguous except NotImplementedError False as_storage_and_layout x IRNode freeze bool = True want_contiguous bool = False stride_order Optional Sequence Union int Integer = None allow_padding bool = False exact_strides Optional Sequence Union int Integer = None - tuple StorageBox Layout Try simplify x into StorageBox Layout allow_padding only affect how we apply stride_order When allow_padding True we have freedom add padding when applying stride_order isinstance x TensorBox as_storage_and_layout x data freeze=freeze want_contiguous=want_contiguous stride_order=stride_order allow_padding=allow_padding exact_strides=exact_strides isinstance x StorageBox _ layout = as_storage_and_layout x data freeze=freeze want_contiguous=want_contiguous stride_order=stride_order allow_padding=allow_padding exact_strides=exact_strides x x data get_layout isinstance x Buffer freeze want_contiguous x freeze_layout assert x get_layout is_contiguous stride_order None x freeze_layout_with_stride_order stride_order allow_padding=allow_padding exact_strides None x freeze_layout_with_exact_strides exact_strides allow_padding=allow_padding x decide_layout StorageBox x x get_layout isinstance x ReinterpretView making base x contiguous stride_ordered will necessarily make ReinterpretView either so don t pass along those arguments buffer _ = as_storage_and_layout x data freeze=freeze buffer x layout raise NotImplementedError is_stride_order_storage_and_layout x IRNode stride_order Sequence Union int Integer - bool try _buffer layout = as_storage_and_layout x freeze=False layout is_stride_ordered stride_order except NotImplementedError False is_unaligned node IRNode - bool isinstance node TensorBox StorageBox is_unaligned node data isinstance node ReinterpretView layout = node layout has_unaligned_layout = V graph sizevars statically_known_multiple_of layout offset get_dtype_size layout dtype GPU_ALIGN_BYTES is_unaligned node data has_unaligned_layout isinstance node Buffer node get_name V graph unaligned_buffers assume aligned otherwise False ir_dataclass BaseView IRNode data IRNode cache_on_self_and_args BaseView get_free_symbol_uses unbacked_only bool = False - OrderedSet Symbol data get_free_symbol_uses unbacked_only make_reindexer - Callable Sequence Expr Sequence Expr raise NotImplementedError f make_reindexer NYI make_indexer - Callable Sequence Expr Expr inner = data make_indexer reindex = make_reindexer indexer idx Sequence Expr - Expr inner reindex idx indexer make_loader - Callable Sequence Expr OpsValue inner = data make_loader reindex = make_reindexer loader idx Sequence Expr - OpsValue inner reindex idx loader property dtype - torch dtype data get_dtype get_layout - Layout data get_layout get_device - Optional torch device data get_device get_origin_node - Optional torch fx Node None get_name - str data get_name get_pointwise_size - Sequence Expr get_size mark_reuse users int - None data mark_reuse users has_exceeded_max_reads - bool data has_exceeded_max_reads realize - Optional str data realize realize_hint - None data realize_hint get_storage_numel - _IntLike data get_storage_numel is_extern - bool data is_extern is_module_buffer - bool assert isinstance data BaseView type data data is_module_buffer get_read_names - OrderedSet str data get_read_names get_reads - OrderedSet Dep patch object FlexibleLayout allow_indexing True extract_read_writes make_loader get_size reads unwrap_view - IRNode x IRNode = while isinstance x BaseView x = x data x constant_to_device device torch device - IRNode Move given device Requires all reads constants loader = make_loader loader = patch object ConstantBuffer override_device device loader Pointwise device=device dtype=self get_dtype inner_fn=loader ranges=self get_size ir_dataclass ExpandView BaseView size Sequence Expr staticmethod _normalize_size x IRNode new_size Sequence _IntLike - Sequence _IntLike Replace ` - ` correct sizes sizevars = V graph sizevars new_size = sympy expand s s new_size old_size = x get_size old_size = None len new_size - len old_size + list old_size assert len new_size == len old_size i range len new_size new_size i == - assert old_size i None new_size i = old_size i old_size i None V graph sizevars is_size_one_or_false old_size i pass Sanity check Expect broadcast compatibility NB new_size i == old_size i expected already guarded because meta formula expected have taught us equality pyrefly ignore unsupported-operation assert sizevars size_hint new_size i - old_size i fallback= == f Broadcast failed ExpandView x get_size new_size dimension i new_size classmethod create cls x IRNode new_size Sequence _IntLike - BaseView new_size = cls _normalize_size x new_size is_storage_and_layout x storage old_layout = as_storage_and_layout x skip = len new_size - len old_layout size assert skip = new_stride = sympy S Zero skip stride size zip old_layout stride old_layout size new_stride append stride V graph sizevars is_size_one_or_false size sympy S Zero new_layout = FixedLayout old_layout device old_layout dtype list new_size new_stride old_layout offset old_layout is_pinned ReinterpretView data=storage layout=new_layout ExpandView data=x size=new_size get_size - Sequence Expr size make_reindexer - Callable Sequence Expr Sequence Expr target = get_size actual = data get_size skip = len target - len actual reindex index Sequence Expr - Sequence Expr index = list index skip assert len index == len actual i range len actual actual i == zero out broadcast dimension index i = sympy S Zero index reindex ir_dataclass PermuteView BaseView dims list Expr classmethod create cls x IRNode dims Sequence int - BaseView dims = cls _map_neg_dims dims assert OrderedSet dims == OrderedSet range len dims is_storage_and_layout x storage old_layout = as_storage_and_layout x new_layout = FixedLayout old_layout device old_layout dtype old_layout size i i dims old_layout stride i i dims old_layout offset old_layout is_pinned ReinterpretView data=storage layout=new_layout PermuteView data=x dims=dims classmethod _map_neg_dims cls dims Sequence int - list int dim dim = len dims + dim dim dims get_size - Sequence Expr assert OrderedSet _map_neg_dims dims == OrderedSet range len dims size = data get_size size i i dims make_reindexer - Callable Sequence Expr Sequence Expr inv = j i i j enumerate dims inv = inv i i range len dims assert OrderedSet inv == OrderedSet range len dims reindex index Sequence Expr - Sequence Expr index i i inv reindex ir_dataclass SqueezeView BaseView classmethod create cls x IRNode dim Optional int = None - IRNode is_storage_and_layout x storage old_layout = as_storage_and_layout x new_size = new_stride = dim None assert isinstance dim int type dim assert = dim dim len old_layout size i size stride enumerate zip old_layout size old_layout stride dim None Only append dim squeezed out V graph sizevars is_size_one_or_false size new_size append size new_stride append stride i = dim new_size append size new_stride append stride assert size == expected squeezed size new_layout = FixedLayout old_layout device old_layout dtype new_size new_stride old_layout offset old_layout is_pinned ReinterpretView data=storage layout=new_layout dim None View create x s s x get_size V graph sizevars is_size_one_or_false s assert x get_size dim == View create x s i s enumerate x get_size i = dim staticmethod squeezer size Sequence Expr - tuple list int Callable Sequence Expr tuple Expr new_size = s s size s = not_one = i i s enumerate size s = length = len size reindex index Sequence Expr - tuple Expr assert len index == len not_one f index not_one new_index = sympy S Zero length idx s zip not_one index new_index idx = s tuple new_index new_size reindex __init__ data Any - None raise AssertionError use SqueezeView create ir_dataclass GenericView BaseView size Sequence Expr reindex Callable Sequence Expr Sequence Expr make_reindexer - Callable Sequence Expr Sequence Expr reindex reindex_str - str index_old = sympy_index_symbol_with_prefix SymT INDEX n n range len size index_new = list reindex index_old f lambda join map str index_old index_new __str__ - str str_helper data f size= size f reindex= reindex_str __repr__ = __str__ classmethod create cls x IRNode new_size Sequence Expr reindex Callable Sequence Expr Sequence Expr - BaseView cls data=x size=list new_size reindex=reindex get_size - Sequence Expr size ir_dataclass View GenericView staticmethod handle_negative_index idx Expr size Expr - Expr idx = sympy expand idx size = sympy expand size evaluate_expr = V graph sizevars shape_env evaluate_expr evaluate_expr sympy Lt idx idx = idx + size idx classmethod create cls x IRNode new_size Sequence Expr - IRNode type ignore override assert isinstance new_size Sequence type new_size old_size new_size = cls resolve_negative_size x get_size new_size Skip pointless views V graph sizevars statically_known_list_equals old_size new_size x unbacked_symbols_in_sizes = False len free_unbacked_symbols old_size len free_unbacked_symbols new_size unbacked_symbols_in_sizes = True new_size fake_reindex index Any - tuple int tuple len old_size cls data=x size=list new_size reindex=fake_reindex TODO new FixedTransferLayout output layout constrained input layout is_contiguous_storage_and_layout x unbacked_symbols_in_sizes unbacked_symbols_in_sizes is_contiguous_storage_and_layout x realize x otherwise dynamic_reshape_indexer below will fail due size_hint s inability process unbacked SymInts TODO unbacked should diverge backed determining striding Need require contiguous here instead realize see https github com pytorch pytorch issues x = ExternKernel require_contiguous x storage old_layout = as_storage_and_layout x want_contiguous=True new_layout = FixedLayout old_layout device old_layout dtype new_size FlexibleLayout contiguous_strides new_size old_layout offset old_layout is_pinned ReinterpretView data=storage layout=new_layout reindex = cls dynamic_reshape_indexer old_size new_size cls data=x size=list new_size reindex=reindex staticmethod resolve_negative_size old_size Sequence Expr new_size Sequence Expr - tuple list Expr list Expr new_size = V graph sizevars simplify x x new_size old_size = V graph sizevars simplify x x old_size new_size = list new_size i range len new_size new_size i == - new_size i = sympy S One new_size i = CleanDiv sympy_product old_size sympy_product new_size break V graph sizevars check_equals sympy_product old_size sympy_product new_size old_size new_size classmethod dynamic_reshape_indexer cls old_size Sequence _IntLike new_size Sequence _IntLike dense_dim Optional int = None - Callable Sequence _T Sequence _V try reindex = cls _dynamic_reshape_indexer old_size new_size dense_dim except AssertionError IndexError optimistic algorithm failed lets do fallback flat = sympy_product old_size reindex = cls _dynamic_reshape_indexer old_size flat reindex = cls _dynamic_reshape_indexer flat new_size reindex = fuse_reindexing reindex reindex reindex staticmethod _dynamic_reshape_indexer old_size Sequence Expr new_size Sequence Expr dense_dim Optional int = None - Callable Sequence Expr Sequence Expr Perform reshape entirely modifying indexing math size_hint = V graph sizevars size_hint TODO These symbols may escape they don t assert so treat them temporary vars = sympy_index_symbol_with_prefix SymT VIEW i i range len new_size stack_new = list zip vars new_size stack_old = list old_size process dense dim first reordering_dense_dim = dense_dim None dense_dim = len stack_old - len new_size == reordering_dense_dim assert dense_dim None mypy old_dim = stack_old pop dense_dim stack_old append old_dim view_expr = while stack_new stack_old size_old = stack_old pop var size_new = stack_new pop size_old == view_expr append sympy S Zero stack_new append var size_new re-add size_new == stack_old append size_old re-add size_hint size_new == size_hint size_old view_expr append var V graph sizevars check_equals size_new size_old size_hint size_new size_hint size_old while size_hint size_new size_hint size_old var size_new = stack_new pop var = var size_new + var size_new = size_new size_new view_expr append var V graph sizevars check_equals size_new size_old size_hint size_new size_hint size_old divisor = sympy S One modulus = size_old view_expr append ModularIndexing var divisor modulus divisor = divisor modulus while size_hint size_new size_hint size_old modulus = stack_old pop view_expr append ModularIndexing var divisor modulus divisor = divisor modulus size_old = size_old modulus V graph sizevars check_equals size_new size_old raise AssertionError while stack_old size_old = stack_old pop V graph sizevars check_equals size_old view_expr append sympy S Zero while stack_new var size_new = stack_new pop V graph sizevars check_equals size_new dense_dim None len new_size == view_expr reverse Move last expression dense dim its original position dense_expr = view_expr pop view_expr insert dense_dim dense_expr view_expr reverse assert len view_expr == len old_size reindex index Sequence Expr - Sequence Expr assert len index == len vars len index len vars replacements = dict zip vars index tuple sympy_subs x replacements x view_expr reindex ir_dataclass ReinterpretView BaseView Pretend our storage has different layout layout Layout __post_init__ - None super __post_init__ isinstance data BaseView object __setattr__ data data unwrap_view __str__ - str str_helper data layout __repr__ = __str__ get_name - str data get_name get_device - Optional torch device layout device get_origin_node - Optional torch fx Node None property dtype - torch dtype layout dtype get_size - Sequence Expr list layout size get_stride - Sequence Expr list layout stride make_loader - Callable Sequence Expr OpsValue loader index Sequence Expr - OpsValue indexer = layout make_indexer tmp_loader = ops load get_name indexer index layout dtype = data dtype ops to_dtype_bitcast tmp_loader dtype data dtype tmp_loader loader make_indexer - Callable Sequence Expr Expr layout make_indexer get_layout - Layout layout freeze_layout - None pass cache_on_self_and_args ReinterpretView get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol get_free_symbols layout size unbacked_only &#124; get_free_symbols layout stride unbacked_only &#124; get_free_symbols layout offset unbacked_only codegen_reference writer Optional IndentedBuffer = None - str reinterpret_tensor similar as_strided except - offset added existing offset rather than replacing - view tracking disabled similar unsafe_view V graph wrapper_code codegen_reinterpret_view data layout size layout stride layout offset writer writeline writer None V graph wrapper_code writeline dtype=self layout dtype num_reads - int ir_dataclass DtypeView BaseView Pretend our storage has different type target_dtype torch dtype classmethod create cls x IRNode new_dtype torch dtype - BaseView is_storage_and_layout x storage old_layout = as_storage_and_layout x new_layout = FixedLayout old_layout device new_dtype old_layout size old_layout stride old_layout offset old_layout is_pinned ReinterpretView data=storage layout=new_layout DtypeView data=x target_dtype=new_dtype __str__ - str str_helper data target_dtype __repr__ = __str__ property dtype - torch dtype target_dtype get_size - Sequence Expr data get_size make_loader - Callable Sequence Expr OpsValue inner = data make_loader loader idx Sequence Expr - OpsValue ops to_dtype_bitcast inner idx target_dtype data dtype loader SliceView View classmethod normalize_start_end cls x IRNode dim int start int end int - tuple int int Normalize start end such both range x get_size dim start = end sizevars = V graph sizevars dim_size = x get_size dim any free_unbacked_symbols x x start end dim_size min_func = sympy Min max_func = sympy Max min_func = sizevars evaluate_min max_func = sizevars evaluate_max clamp x Expr lower int upper int - Expr clamped_lower = x sizevars statically_known_geq x lower max_func x lower clamped_full = clamped_lower sizevars statically_known_leq clamped_lower upper min_func clamped_lower upper clamped_full clamp_wrap val Union int None lower int upper int default Union Expr int - Union Expr int val None TODO rec can really happen default val = cls handle_negative_index val dim_size clamp val lower upper start = clamp_wrap start dim_size end = clamp_wrap end start dim_size dim_size start end classmethod create type ignore override cls x IRNode dim int start int end int step int = clamp bool = True - IRNode step = sympy expand step assert isinstance step Expr step step try start == end = - step == x except TypeError pass new_size = list x get_size NB Ordinarily we default clamping We only don t clamp split_with_sizes For split_with_sizes sizes should already valid failing situation ok since invalid sizes could trigger silent errors clamp start end = cls normalize_start_end x dim start end new_size dim = FloorDiv end - start + step - step is_storage_and_layout x Fast path storage old_layout = as_storage_and_layout x new_stride = list old_layout stride new_stride dim = new_stride dim step new_layout = FixedLayout old_layout device old_layout dtype new_size new_stride old_layout offset + old_layout stride dim start old_layout is_pinned ReinterpretView data=storage layout=new_layout reindex index Sequence Expr - Sequence Expr assert len index == len new_size f wrong ndim index new_size index = list index index dim = index dim step + start index redirect generic view SliceView data=x size=new_size reindex=reindex ir_dataclass BaseConstant IRNode dtype torch dtype device torch device get_size - Sequence Expr get_device - Optional torch device device get_origin_node - Optional torch fx Node None get_reads - OrderedSet Dep OrderedSet ir_dataclass Constant BaseConstant value Any dtype torch dtype device torch device make_loader - Callable Sequence Expr OpsValue loader index Sequence Expr - OpsValue ops constant value dtype loader realize - Optional str pass constant_to_device device torch device - IRNode Constant value=self value dtype=self dtype device=device ir_dataclass IndexingConstant BaseConstant index Any dtype torch dtype device torch device make_loader - Callable Sequence Expr OpsValue loader index Sequence Expr - OpsValue ops index_expr index dtype loader constant_to_device device torch device - IRNode IndexingConstant index=self index dtype=self dtype device=device is_contiguous_strides_for_shape stride Sequence _IntLike shape Sequence _IntLike - bool expected_stride = expected_stride_max = x y reversed tuple zip shape stride x == continue V graph sizevars statically_known_equals y expected_stride V graph sizevars statically_known_equals y expected_stride_max False expected_stride_max = sympy Max x expected_stride = x True get_align_for_dtype dtype torch dtype - int config padding_alignment_bytes dtype itemsize OutputSpec Abstract base Layout MultiOutputLayout NoneLayout Represents memory layout output Operation get_device - Optional torch device raise NotImplementedError type __name__ storage_size - int raise NotImplementedError type __name__ get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol raise NotImplementedError type __name__ ir_dataclass Layout OutputSpec Layout base Carries tensor meta-information including offset whether pinned __init__ device torch device dtype torch dtype size Sequence Expr stride Optional Sequence Expr = None offset Expr = Integer is_pinned bool = False - None stride None stride = FlexibleLayout contiguous_strides size pyrefly ignore read-only device = device dtype = dtype assert len size == len stride f size= size stride= stride assert all isinstance s Expr int s size _size = size _stride = stride _offset = offset is_pinned = is_pinned is_pinned implies cpu assert is_pinned device type == cpu property size - Sequence Expr _size size setter size value Sequence Expr - None _size = value property stride - Sequence Expr _stride stride setter stride value Sequence Expr - None _stride = value property offset - Expr _offset offset setter offset value Expr - None _offset = value __str__ - str offset = offset = offset = f offset= offset device_index_str = device index None f device index is_pinned_str = is_pinned is_pinned_str = f is_pinned= is_pinned f type __name__ device type device_index_str dtype f size= size stride= stride offset is_pinned_str __repr__ = __str__ get_device - torch device device get_example - torch Tensor V fake_mode torch empty_strided convert_shape_to_symint size convert_shape_to_symint stride dtype=self dtype device=self device pin_memory=self is_pinned is_contiguous - bool is_contiguous_strides_for_shape stride size staticmethod is_channels_last_contiguous shape Sequence _IntLike strides Sequence _IntLike - bool ndim = len shape ndim shape == False left right size zip strides make_channels_last_strides_for shape shape size = left = right False True is_transposed - bool left right size zip stride reversed FlexibleLayout contiguous_strides list reversed size size size = left = right False True is_stride_ordered order Sequence int - bool assert len stride == len order ignore dimensions size they dont affect layout non_ _indices = i i dim enumerate size V graph sizevars size_hint dim fallback= = stride = stride i i non_ _indices order Sequence int = order i i non_ _indices sorted_indices arr Sequence int - Sequence int sorted_arr = sorted arr sorted_arr index element element arr since we may have removed dimensions need re-sort re-index order order = sorted_indices order reorder stride given order stride_ordered = - len order i range len order stride_ordered order i = stride i check ascending order i range len order - expr = stride_ordered i stride_ordered i + isinstance expr bool expr = V graph _shape_env evaluate_expr stride_ordered i stride_ordered i + size_oblivious=True expr False True is_channels_last_stride_ordered - bool create channels_last order NCHW NCDHW C first order order = + list reversed range len stride - order = len order + order is_stride_ordered order staticmethod _pad_strides in_strides Sequence int size Sequence Expr dtype torch dtype - Sequence int The padding does change stride order makes sure all strides larger than threshold multiple align align = get_align_for_dtype dtype len in_strides == in_strides config pad_channels_last Layout is_channels_last_contiguous size in_strides in_strides current_fx_node = V get_current_node hasattr current_fx_node meta current_fx_node meta get dislike_padding False in_strides Skip padding strides dynamic shapes based config pad_dynamic_shape Checking both shape strides there cases where only one dynamic is_dynamic = all isinstance s int sympy Integer s itertools chain in_strides size config pad_dynamic_shapes is_dynamic in_strides shape_env = V graph _shape_env hasattr V graph _shape_env None contains_unbacked_symints expr sympy Expr &#124; int - bool shape_env None False isinstance expr sympy Expr False any shape_env is_unbacked_symint s s expr free_symbols Skip padding strides when contains unbacked symints now shape_env any contains_unbacked_symints s s in_strides in_strides stride_order = get_stride_order in_strides shape_env fill_order = stride_order fill_order stride_order new_strides = _ range len in_strides since we pad when layout flexible we can decide smallest stride new_strides fill_order = padded = False rank idx enumerate fill_order start= prev_idx = fill_order rank - stride = new_strides prev_idx size prev_idx Static stride meets padding conditions OR Dynamic stride config pad_dynamic_shape=True require_padding = isinstance stride int sympy Integer stride config padding_stride_threshold stride align = isinstance stride sympy Expr config pad_dynamic_shapes new_strides idx = stride require_padding new_strides idx = ceildiv stride align align padded = True padded Consider tensor shape Avoid strides like being padded equivalent strides in_strides pyrefly ignore bad-assignment metrics num_comprehensive_padding += new_strides pad_strides - None assert isinstance FlexibleLayout type assert stride None stride = _pad_strides stride size dtype should_pad_strides - bool config comprehensive_padding isinstance FlexibleLayout as_fixed - FixedLayout isinstance FixedLayout should_pad_strides pad_strides FixedLayout device dtype size stride offset is_pinned make_indexer - Callable Sequence Expr Expr assert FlexibleLayout allow_indexing f convert type __name__ FixedLayout first as_fixed make_indexer __eq__ other object - bool isinstance other Layout device == other device dtype == other dtype size == other size stride == other stride offset == other offset is_pinned == other is_pinned storage_size - Expr compute_required_storage_length size stride offset type ignore arg-type cache_on_self_and_args Layout get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol get_free_symbols size unbacked_only &#124; get_free_symbols stride unbacked_only &#124; get_free_symbols offset unbacked_only FixedLayout Layout A Tensor layout we cannot change make_indexer - Callable Sequence Expr Expr A closure containing math read given element _fixed_indexer size stride offset FlexibleLayout Layout A Tensor layout we allowed change Assumption layout change should NOT add remove free symbols allow_indexing = False WARNING This doesn t handle zero size tensors correctly staticmethod contiguous_strides sizes Sequence int - list Expr len sizes == reversed_strides = sympy S One size reversed sizes reversed_strides append size reversed_strides - list reversed reversed_strides staticmethod fill_ordered sizes Sequence int order Sequence int - list Expr Create stride based order dimensions should filled In format channels last would assert OrderedSet range len sizes == OrderedSet order sizes order next_stride = sympy S One strides = None len order i order strides i = next_stride next_stride = next_stride sizes i strides staticmethod stride_ordered sizes Sequence int order Sequence int - Sequence Expr Create stride based sorted order permuted range In format channels last would assert OrderedSet range len sizes == OrderedSet order fill_order = stride_order fill_order order FlexibleLayout fill_ordered sizes fill_order staticmethod stride_ordered_for_memory_format sizes Sequence int memory_format torch memory_format - Sequence Expr Create stride based memory format Memory format translasted into stride order so channels_last same FlexibleLayout stride_ordered sizes This interface does support memory_format ` torch preserve_format ` which should used deduce format another source memory_format == torch channels_last FlexibleLayout stride_ordered sizes NHWC_STRIDE_ORDER memory_format == torch channels_last_ d FlexibleLayout stride_ordered sizes NHWDC_STRIDE_ORDER memory_format == torch contiguous_format FlexibleLayout contiguous_strides sizes log debug stride_ordered_for_memory_format unsuppored memory_format s memory_format raise NotImplementedError staticmethod same_ordered sizes Sequence int stride Sequence _IntLike - Sequence Expr Create stride has same stride order given stride For example given stride fill order should assert len sizes == len stride stride = V graph sizevars size_hint_or_throw x x stride fill_order = sorted range len stride key=stride __getitem__ FlexibleLayout fill_ordered sizes fill_order property size - Sequence Expr _size size setter size value Sequence Expr - None assert_free_symbol_uses_unchanged size value _size = value property stride - Sequence Expr _stride stride setter stride value Sequence Expr - None assert_free_symbol_uses_unchanged stride value _stride = value property offset - Expr _offset offset setter offset value Expr - None assert_free_symbol_uses_unchanged offset value _offset = value as_stride_order order Sequence int allow_padding bool = False - FixedLayout new_stride = stride_ordered size order should_pad_strides allow_padding new_stride = _pad_strides new_stride size dtype FixedLayout device dtype size new_stride offset is_pinned as_exact_strides exact_strides Sequence _IntLike allow_padding bool = False - FixedLayout new_stride = exact_strides should_pad_strides allow_padding new_stride = _pad_strides new_stride size dtype FixedLayout device dtype size new_stride offset is_pinned as_fill_order order Sequence int - FixedLayout new_stride Sequence int = fill_ordered size order should_pad_strides new_stride = _pad_strides new_stride size dtype FixedLayout device dtype size new_stride offset is_pinned as_same_order stride Sequence _IntLike - FixedLayout new_stride = same_ordered size stride should_pad_strides new_stride = _pad_strides new_stride size dtype FixedLayout device dtype size new_stride offset is_pinned get_initial_free_symbol_uses - dict tuple str bool sympy Symbol initial_free_symbols = name size stride offset unbacked_only True False key = name unbacked_only initial_free_symbols key = OrderedSet get_free_symbols getattr name unbacked_only initial_free_symbols assert_free_symbol_uses_unchanged name str value IterateExprs - None unbacked_only True False old_free_symbols = initial_free_symbols name unbacked_only new_free_symbols = OrderedSet get_free_symbols value unbacked_only assert new_free_symbols == old_free_symbols f Expected free symbols unchanged got new_free_symbols vs old_free_symbols __init__ device torch device dtype torch dtype size Sequence Expr stride_order Optional Sequence Union int Integer = None is_pinned bool = False - None stride_order strides = FlexibleLayout fill_ordered size stride_order strides = FlexibleLayout contiguous_strides size super __init__ device dtype size strides is_pinned=is_pinned record initial free symbols check we do add new free symbols later when modifying sizes strides offsets initial_free_symbols = get_initial_free_symbol_uses NonOwningLayout Layout Is view into storage another tensor __init__ view Union BaseView TensorBox - None layout = view get_layout super __init__ layout device layout dtype layout size layout stride view = view make_indexer - Callable Sequence Expr Expr as_fixed make_indexer maybe_guard_aligned - bool offset = view get_layout offset offset == True utils ALIGNMENT V graph sizevars statically_known_multiple_of offset ALIGNMENT cache_on_self_and_args NonOwningLayout get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol assert isinstance view ReinterpretView box = view data assert isinstance box StorageBox type box input_buffer = box data assert isinstance input_buffer Buffer type box input_buffer layout get_free_symbol_uses unbacked_only CommBufferType Enum SYMM_MEM = symm_mem CommBufferLayout FixedLayout A layout signifies buffer comm buffer In terms striding layout identical ` FixedLayout ` Buffers layout do participate in-place reuse - can neither source nor target in-place reuse For detailed motivation usage layout see NOTE lowering-time collective optimization comm_buffer_type CommBufferType group_name str __init__ layout FlexibleLayout comm_buffer_type CommBufferType group_name str isinstance layout FlexibleLayout raise AssertionError A ` CommBufferLayout ` can only initialized f ` FlexibleLayout ` got layout fixed = layout as_fixed super __init__ device=fixed device dtype=fixed dtype size=fixed size stride=fixed stride offset=fixed offset is_pinned=fixed is_pinned comm_buffer_type = comm_buffer_type group_name = group_name ir_dataclass NoneLayout OutputSpec This janky I figured out what fields populate just running model I interested adding properties methods needed This doesn t inherit Layout because Layout assumes you have stuff like sizes I don t really have anything here If you have ir Node NoneLayout you probably need setup dependencies manually scheduler device Optional torch device size list int = dataclasses field default_factory=lambda stride list int = dataclasses field default_factory=lambda storage_size - int as_fixed - OutputSpec get_device - Optional torch device device MutationLayoutSHOULDREMOVE Layout __init__ target IRNode - None super __init__ target get_device_or_error target get_dtype target get_size None target = target name = get_buffer get_name V graph mark_buffer_mutated name property stride - Sequence Expr type ignore override real_layout stride stride setter type ignore override stride value Never - None pass ignore setting stride storage_size - Expr real_layout storage_size get_buffer - Buffer unwrap_views target Any - Any isinstance target MutationLayoutSHOULDREMOVE unwrap_views target target isinstance target BaseView unwrap_views target unwrap_view isinstance target MutableBox unwrap_views target data target result = unwrap_views target assert isinstance result Buffer type result result real_layout - Layout layout = get_buffer layout assert isinstance layout Layout layout classmethod realize_into cls src IRNode dst IRNode unsafe_alias bool = False - IRNode dst realize NOTE We must realize users ` dst ` before we realize ` src ` since realization order determines scheduling order Otherwise src s mutation would scheduled before existing users dst V graph mark_buffer_mutated dst get_name isinstance src TensorBox src = src data We copy contents src into dst In most cases should fused into single kernel scheduler NOTE We cannot change src s layout mutate dst directly would alias src dst which correct further mutations dst would effect users src However there no more users dst we can alias src dst src realize_hint unsafe_alias node = Pointwise create device=src get_device dtype=src get_dtype inner_fn=src make_loader ranges= V graph sizevars check_equals_and_simplify b b zip src get_size dst get_size assert isinstance node BaseView MutableBox src = node data src realize assert hasattr src data src assert isinstance src data layout FlexibleLayout type src data layout src data layout = MutationLayoutSHOULDREMOVE dst src data as_fixed - Self type ignore override make_indexer - Callable Sequence Expr Expr target make_indexer ir_dataclass frozen=False Buffer IRNode CodegenSymbol Name sometimes None e g ForceInPlace where there isn t meaningful name name Optional str layout OutputSpec Multi-output buffers will define outputs List Buffer Confusingly MultiOutput does NOT define __post_init__ - None super __post_init__ _post_init_setattr origin_node None make_indexer - Callable Sequence Expr Expr get_layout make_indexer get_name - str assert name name get_example - Union torch Tensor torch SymInt isinstance layout Layout layout get_example raise NotImplementedError type layout __name__ get_device - Optional torch device get_output_spec get_device get_defining_op - Optional Operation None property dtype - torch dtype get_layout dtype get_size - Sequence Expr get_layout size get_stride - list Expr get_layout stride get_offset - Expr get_layout offset get_layout - Layout isinstance layout Layout layout raise NotImplementedError type layout __name__ get_output_spec - OutputSpec layout get_storage_numel - int get_numel get_is_pinned - bool get_layout is_pinned freeze_layout - None isinstance layout Layout isinstance layout NonOwningLayout layout = layout as_fixed freeze_layout_with_stride_order order Sequence int allow_padding bool = False - None assert isinstance layout FlexibleLayout type layout layout = layout as_stride_order order allow_padding=allow_padding freeze_layout_with_fill_order order Sequence int - None assert isinstance layout FlexibleLayout type layout layout = layout as_fill_order order freeze_layout_with_same_order stride Sequence int - None assert isinstance layout FlexibleLayout type layout layout = layout as_same_order stride freeze_layout_with_exact_strides exact_strides Sequence int allow_padding bool = False - None assert isinstance layout FlexibleLayout type layout layout = layout as_exact_strides exact_strides allow_padding=allow_padding is_zero_elements - bool V graph sizevars statically_known_true sympy Eq get_numel make_loader - Callable Sequence Expr OpsValue Loading zero-element buffer no-op is_zero_elements partial nop_loader_fn dtype=self get_dtype loader index Sequence Expr - OpsValue indexer = make_indexer ops load name unnamed indexer index loader codegen_reference writer Optional IndentedBuffer = None - str get_name decide_layout - None pass get_inputs_that_alias_output - Sequence str isinstance layout NonOwningLayout layout view get_name get_mutation_names - Sequence str isinstance layout MutationLayoutSHOULDREMOVE layout target get_name get_read_names - OrderedSet str OrderedSet get_name cache_on_self_and_args Buffer get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol OrderedSet get_unbacked_symbol_defs - OrderedSet sympy Symbol OrderedSet realize - Optional str pass should_allocate - bool Returns False default False ir_dataclass frozen=False OperationBuffer Buffer Operation An operation produces single output buffer get_outputs - list Buffer get_defining_op - Operation Skip implementation Buffer get_operation_name = Operation get_operation_name __post_init__ - None Buffer __post_init__ Operation __post_init__ InputBuffer Buffer num_reads - int DonatedBuffer InputBuffer Represents donated buffer which saved tensor alias any fwd inputs fwd user outputs bwd outputs We generally cannot inplace reuse input tensor memory during backward since might used another function However donated buffer can inplace reused during backward save memory ConstantBuffer InputBuffer override_device Optional torch device = None make_loader - Callable Sequence Expr OpsValue loader index Sequence Expr - OpsValue indexer = get_layout make_indexer ops load V graph constant_name get_name override_device indexer index loader constant_to_device device torch device - IRNode ConstantBuffer name=V graph constant_name get_name device layout=self layout ir_dataclass NoneAsConstantBuffer IRNode get_reads - OrderedSet Dep OrderedSet cache_on_self_and_args NoneAsConstantBuffer get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol OrderedSet codegen_reference writer Optional IndentedBuffer = None - str V graph wrapper_code none_str get_output_spec - OutputSpec NoneLayout device=None has_tensor_output - bool False ir_dataclass ShapeAsConstantBuffer IRNode expr Expr cache_on_self_and_args ShapeAsConstantBuffer get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol get_free_symbols expr unbacked_only codegen_reference writer Optional IndentedBuffer = None - str V graph wrapper_code codegen_sizevar expr has_tensor_output - bool False ir_dataclass frozen=False ComputedBuffer OperationBuffer Represents buffer computed during kernel execution rather than being input data Loops _force_realize ClassVar bool = False fields split reduction _split_size Optional int = None _original_inner_fn Optional Callable Any = None _original_ranges Optional Sequence _IntLike = None _original_reduction_ranges Optional Sequence _IntLike = None contextlib contextmanager with_original_inner_fn - Iterator None assert _split_size None assert _original_inner_fn None assert _original_ranges None assert _original_reduction_ranges None assert isinstance data Reduction f type data old_data = data old_layout = layout try new_data = Reduction device=old_data device dtype=old_data dtype inner_fn=self _original_inner_fn ranges=self _original_ranges reduction_ranges=self _original_reduction_ranges reduction_type=old_data reduction_type src_dtype=old_data src_dtype reduction_hint=old_data reduction_hint data = new_data layout does matter since we skip tl store later layout = FixedLayout old_data device old_data dtype _original_ranges get_default_sizes_body clear_cache yield finally data = old_data layout = old_layout staticmethod contextlib contextmanager force_realize - Iterator None old_value = ComputedBuffer _force_realize try ComputedBuffer _force_realize = True yield finally ComputedBuffer _force_realize = old_value get_computed_buffer_name - Optional str Returns name exists otherwise returns name data node exists If neither exist returns None name None name hasattr data name data name None num_reads - int data num_reads get_reads - OrderedSet Dep data get_reads get_read_names - OrderedSet str data get_read_names get_read_writes - dependencies ReadWrites isinstance data Reduction Scan Sort Pointwise dependencies ReadWrites reads=OrderedSet writes=OrderedSet index_exprs=OrderedSet patch object FlexibleLayout allow_indexing True data get_reduction_type extract_read_writes get_store_function data get_pointwise_size data get_reduction_size extract_read_writes get_store_function data get_size cache_on_self_and_args ComputedBuffer get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol Ordinarily we d like just peek arguments list ComputedBuffers have no argument list Morally logic needs synchronized KernelArgs size calls which responsible making symbols make there way kernel arguments precisely passing one those symbols establishes dependency However we haven t started codegen yet so we can t directly reuse logic One thing you might wonder enough ComputedBuffer denoting reduction over i Empirically enough unusual reason we only need accurate dependencies item call s impossible end up reduction over i item call without regular non-reduction buffer first result = layout get_free_symbol_uses unbacked_only &#124; data get_free_symbol_uses unbacked_only has_store_function result &#124; = get_read_writes get_free_symbol_uses unbacked_only result make_loader - Callable Sequence Expr OpsValue get_reduction_type name V graph mutated_buffers num_reads == _force_realize inline op rather than generating ops load data make_loader super make_loader has_store_function - bool isinstance data Reduction Scan Sort Pointwise get_store_function - Callable None indexer = get_layout as_fixed make_indexer isinstance data Reduction Scan Sort partial data store_reduction name indexer assert isinstance data Pointwise type data partial data store_output name indexer get_fill_order - Optional list int If our layout still flexible try determine stride order based stride orders reads TODO jansel A better algorithm here would look downstream consumers value try do global graph-level layout optimization This also something just begging autotuned isinstance layout FlexibleLayout index_vars reduction_vars _ = dependencies index_vars_squeeze data get_pointwise_size data get_reduction_size reads = get_read_writes reads only consider reads buffer same size ignore StarDeps because they don t contribute stride information assert all isinstance r dependencies StarDep dependencies MemoryDep r reads reads = sympy_subs r index v sympy S Zero v reduction_vars v = r reads isinstance r dependencies MemoryDep reads isinstance data Scan Sort indices = data reindex index_vars reduction_vars indices = index_vars stride_lengths = V graph sizevars stride_hints expr indices expr reads scheduler pick_loop_order pick_loop_order stride_lengths get_size None decide_layout - None isinstance layout FlexibleLayout order = get_fill_order order freeze_layout_with_fill_order order freeze_layout cache_on_self get_default_sizes_body - tuple tuple list Expr list Expr LoopBody tuple list Expr list Expr args var_ranges = dependencies index_vars_squeeze get_pointwise_size get_reduction_size prefix= q patch object ConstantBuffer override_device get_device body = LoopBody get_store_function args get_reduction_type args var_ranges args index_vars = reduce_vars list Any = index_size = reduce_size = v s var_ranges items v args assert reduce_vars index_vars append v index_size append s assert v args reduce_vars append v reduce_size append s index_size reduce_size body index_vars reduce_vars simplify_and_reorder extra_indexing_constraints Optional tuple dict Any Any list Any = None recompute_sizes_body_func Optional Callable Any = None - tuple tuple list Expr list Expr Optional LoopBody This main place where we do loop transformations backend-agnostic way Here we Remove any dimensions Fuse contiguous dimensions together Reorder dimensions based stride orders Optional argument extra_indexing_constraints can used append additional indexing expressions existing ones derived buffer s body This can useful fuse scheduler nodes compatible ranges e g s s s s s CPU preventing indexing simplifications obtaining index reduce ranges scheduler node compatible other nodes Optional argument recompute_sizes_body_func can used recompute sizes body default body This can useful append additional loop transformations index_size reduce_size body index_vars reduce_vars = get_default_sizes_body recompute_sizes_body_func index_size reduce_size body index_vars reduce_vars = recompute_sizes_body_func index_size reduce_size body index_vars reduce_vars index_formulas = body indexing_exprs values extra_indexing_constraints None assert isinstance extra_indexing_constraints tuple len extra_indexing_constraints == extra_indexing_ranges extra_indexing_expr = extra_indexing_constraints assert isinstance extra_indexing_ranges dict type extra_indexing_ranges assert isinstance extra_indexing_expr list type extra_indexing_expr assert all isinstance f Expr f extra_indexing_expr expected_var_ranges = body var_ranges assert expected_var_ranges == extra_indexing_ranges expected_var_ranges extra_indexing_ranges remove already existing expressions extra_indexing_expr = e e extra_indexing_expr e index_formulas index_formulas += extra_indexing_expr memory_addrs = body get_write_exprs V graph has_feature BackendFeature PREFER_STORE_LOOP_ORDER memory_addrs extend body get_read_exprs simplify_and_reorder x_vars Sequence sympy Symbol support_vars Sequence sympy Symbol sizes Sequence int simplify_loops bool - tuple list int Callable Sequence int Sequence int Callable Sequence int Sequence int newsizes reindex reindex = _apply_loop_reordering x_vars support_vars sizes memory_addrs When using native matmul codegen assumes following loop order regardless stride A B z - y - x - r C z y x += A z y r B z r x z - x - y - r C z y x += A z y r B z r x The critical point position z batch axis bmm It fine swap y x axes e g z y x r z x y r reordering z axis e g y x z r breaks codegen Therefore loop reordering changes z location bmm should reverted default This may always produce optimal loop order when strides do align default assumption TODO Consider extending tl dot codegen support arbitrary loop orders get_reduction_type == dot len sizes == order = list range len sizes default order z axis outermost use default reorder reindex order = newsizes = sizes i i order reindex = same_reorder order reindex = inverse_reorder order NHWC reindex = reindex = x_vars = reindex x_vars simplify_loops newsizes reindex _prune = V graph sizevars _simplify_loops x_vars newsizes index_prevent_reordering index_formulas x_vars newsizes reindex = fuse_reindexing reindex reindex reindex = reindex newsizes reindex reindex support_vars = index_vars + reduce_vars should_merge_loops = is_gpu get_device_type config loop_ordering_after_fusion iter_ranges iter_reindex _ = simplify_and_reorder index_vars support_vars index_size should_merge_loops Like iteration dimensions we may also want delay merging reduction dimensions E g we reduce tensor M N K its M N dimensions followed pointwise kernel merging M N dimension too early makes hard decide what loop order we should pick piontwise kernel so fusible reduction reduce_ranges reduce_reindex _ = simplify_and_reorder reduce_vars support_vars reduce_size should_merge_loops retrace loop body simplification reordering applied iter_vars reduce_vars var_ranges = dependencies index_vars_no_squeeze iter_ranges reduce_ranges prefix= p body = LoopBody body iter_reindex iter_vars reduce_reindex reduce_vars var_ranges iter_vars reduce_vars iter_ranges reduce_ranges body staticmethod _apply_loop_reordering index_vars Sequence sympy Symbol support_vars Sequence sympy Symbol sizes Sequence int memory_addrs list sympy Expr priority_idx Optional list int = None - tuple list int Callable Sequence int Sequence int Callable Sequence int Sequence int Shuffle order loops around hopefully improve performance scheduler pick_loop_order priority_idx None priority_idx = try strides = V graph sizevars stride_hints expr index_vars support_vars expr memory_addrs assert len strides == len memory_addrs len strides == len index_vars order = list reversed pick_loop_order strides sizes priority_idx except Exception config debug log warning Did simplify complex index \n s\n s dict zip index_vars sizes memory_addrs order = list range len sizes sizes = sizes i i order sizes same_reorder order inverse_reorder order get_pointwise_size - Sequence Expr data get_pointwise_size get_reduction_size - Sequence Expr data get_reduction_size get_reduction_type - Optional str data get_reduction_type is_no_op - bool data is_zero_elements should_allocate - bool True constant_to_device device torch device - IRNode Move given device Requires all reads constants data constant_to_device device TemplateBuffer OperationBuffer Represents Triton future other type template operator we can fuse epilogue onto __init__ layout OutputSpec inputs Sequence IRNode make_kernel_render Optional Callable Any - None super __init__ name=None layout=layout inputs = InputsKernel unwrap_storage inputs make_kernel_render = make_kernel_render name = V graph register_buffer V graph register_operation get_read_writes - dependencies ReadWrites extract_read_writes normalize=True extract_read_writes normalize bool = False - dependencies ReadWrites name = get_name indexer = get_layout make_indexer dummy index Sequence Any rindex Sequence Any - Any assert len rindex == ops store name indexer index fake deps = dependencies extract_read_writes dummy get_size normalize=normalize inp inputs assert isinstance inp ReinterpretView Buffer type inp assert isinstance inp layout Layout type inp layout indexer = inp layout make_indexer dummy index Sequence Any rindex Sequence Any - Any assert len rindex == pyrefly ignore missing-attribute ops load inp get_name indexer index deps reads &#124; = dependencies extract_read_writes dummy inp get_size normalize=normalize reads deps get_reduction_size - Sequence Expr sympy S One get_reduction_type - Optional str None should_allocate - bool True simplify_and_reorder extra_indexing_constraints Optional tuple dict Any Any list Any = None recompute_sizes_body_func Optional Callable Any = None - tuple tuple Sequence Expr list Expr Optional LoopBody get_size None TritonTemplateBuffer TemplateBuffer __init__ layout Layout inputs Sequence IRNode make_kernel_render Optional Callable _P _T mutated_inputs Optional Iterable IRNode = None allowed_prologue_inps Optional OrderedSet str = None - None NOTE TritonTemplates multiple outputs We want ability TritonTemplates output multiple tensors Triton kernels have no notion outputs done creating tensors then mutated kernel Currently our STORE_OUTPUT codegen doesn t support creating multinode outputs triton templates We work around creating extra input buffer during lowering we mark them mutated inputs super __init__ layout inputs make_kernel_render mutated_inputs = mutated_inputs outputs list Buffer = mutated_inputs None Ensure mutated inputs only allowed certain nodes allowed_set = torch ops higher_order flex_attention torch ops higher_order flex_attention_backward current_node = V graph current_node target assert current_node allowed_set f Mutated inputs only allowed allowed_set got current_node assert isinstance inputs IRNode type inputs device = inputs get_device outputs += MutationOutput NoneLayout device=device buf buf mutated_inputs allowed_prologue_inps = allowed_prologue_inps allowed_prologue_inps OrderedSet subgraph_inps Optional list Optional Union IRNode sympy Expr = None subgraph_outs Optional list Optional IRNode = None cache_on_self_and_args TritonTemplateBuffer get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol res = super get_free_symbol_uses unbacked_only subgraph_outs = subgraph_outs subgraph_outs subgraph_inps = subgraph_inps subgraph_inps inp subgraph_inps isinstance inp sympy Expr res update get_free_symbols inp unbacked_only isinstance inp IRNode res update inp get_free_symbol_uses unbacked_only assert inp None out subgraph_outs isinstance out IRNode res update out get_free_symbol_uses unbacked_only assert out None res get_outputs - list Buffer outputs get_allowed_prologue_inps - OrderedSet str allowed_prologue_inps __str__ - str out = f TritonTemplateBuffer layout= layout out PrimitiveInfoType = Union int float bool str list Union int str float bool ChoiceCaller Represents possible choice used autotune_process py During autotuning benchmark first called get benchmark result choice selected output_node called get output_node Children classes TritonTemplateCaller CUDATemplateCaller __init__ name str input_nodes list Buffer layout Layout description str - None super __init__ name = name layout = layout input_nodes = input_nodes An additional description used describe choice useful knowing what autotuning choosing description = description failed bool = False A place store annotations can read post benchmarking Use shuttle information between ChoieCaller generation end benchmarking annotations dict Any Any = benchmark args Any out torch Tensor - float algo = to_callable benchmark_configs = warmup autotune_warmup rep autotune_rep config profile_bandwidth_with_do_bench_using_profiling do_bench_using_profiling lambda algo args benchmark_configs type ignore arg-type benchmarker benchmark algo args out out benchmark_configs call_name - str raise NotImplementedError to_callable - Callable Any raise NotImplementedError kernel_hash_key - str Hash key underlying kernel By default we assume there no runtime params so kernel hash key defaults choice caller s hash key hash_key hash_key - str raise NotImplementedError output_node - Union TensorBox ShapeAsConstantBuffer raise NotImplementedError info_dict - dict str Union PrimitiveInfoType list PrimitiveInfoType Information returned here logged autotune log file when enabled autoheuristic_id - str unsupported_choice mark_failed - None Mark choice failed so can removed later Useful when we decouple compilation tuning failed = True TritonTemplateCallerBase ChoiceCaller get_make_kernel_render - Any raise NotImplementedError MultiTemplateBuffer TritonTemplateBuffer Represents Buffer multiple backing implementation choices Choices can TritonTemplates ExternKernels During scheduling there potential epilogue we will benchmark each choices epilogue determine implementation Otherwise fastest base choice will chosen __init__ layout Layout inputs Sequence IRNode choice_timings_fn Callable Optional int dict ChoiceCaller float unfiltered_choices list ChoiceCaller allowed_prologue_inps OrderedSet str - None super __init__ layout=layout inputs=inputs make_kernel_render=None allowed_prologue_inps=allowed_prologue_inps _choice_timings_fn = choice_timings_fn _choice_timings dict Optional int dict ChoiceCaller float = original_inputs = inputs _output_plannable = all isinstance choice TritonTemplateCallerBase isinstance choice torch _inductor select_algorithm ExternKernelCaller choice has_out_variant choice unfiltered_choices _make_kernel_renders dict Optional int Any = property output_plannable - bool Are all possible choices TritonTemplates Extern Kernels out variants _output_plannable choice_timings hint_override Optional int = None - dict ChoiceCaller float hint_override _choice_timings _choice_timings hint_override = _choice_timings_fn hint_override _choice_timings hint_override contextlib contextmanager swap_as_triton_caller caller TritonTemplateCallerBase - Iterator None assert isinstance caller torch _inductor select_algorithm TritonTemplateCaller type caller assert layout == caller layout render = make_kernel_render make_kernel_render = caller get_make_kernel_render try yield finally make_kernel_render = render finalize_as_triton_caller caller TritonTemplateCallerBase - None assert isinstance caller torch _inductor select_algorithm TritonTemplateCaller type caller assert get_size == caller layout size assert get_stride == caller layout stride make_kernel_render = caller get_make_kernel_render get_min_choice hint_override Optional int = None - tuple ChoiceCaller float timings = choice_timings hint_override=hint_override min_choice = min timings key=timings get type ignore arg-type min_choice timings min_choice finalize_as_triton_callers callers dict Optional int TritonTemplateCallerBase - None Finalize multiple callers different hint overrides hint_override caller callers items _make_kernel_renders hint_override = caller get_make_kernel_render Set default one without hint override make_kernel_render = _make_kernel_renders None CUDATemplateBuffer TemplateBuffer __init__ layout Layout inputs Sequence IRNode make_kernel_render Callable _P _T workspace_size int template CUDATemplate supports_epilogue_fusion bool - None super __init__ layout inputs make_kernel_render Global memory bytes needed template workspace_size = workspace_size template = template supports_epilogue_fusion = supports_epilogue_fusion get_workspace_size - int workspace_size workspace_size None emulate_store_fn - None output get_outputs ops store output get_name None None CppTemplateBuffer TemplateBuffer __init__ layout Layout inputs Sequence IRNode make_kernel_render Callable _P _T template CUDATemplate choice Any - None super __init__ layout inputs make_kernel_render template = template choice = choice outputs Optional list Buffer = None get_layout - Layout isinstance layout MultiOutputLayout assert isinstance outputs Iterable type outputs pyrefly ignore index-error first_output = outputs assert isinstance first_output Buffer type first_output layout = first_output layout assert isinstance layout Layout type layout layout super get_layout CuteDSLTemplateBuffer TemplateBuffer Buffer CuteDSL CUTLASS Python DSL template kernels Similar other template buffers specialized CuteDSL operations __init__ layout Layout inputs Sequence IRNode make_kernel_render Callable _P _T template Any mutated_inputs Optional Iterable IRNode = None - None super __init__ layout inputs make_kernel_render template = template mutated_inputs = mutated_inputs outputs list Buffer = mutated_inputs None assert isinstance inputs IRNode type inputs device = inputs get_device outputs += MutationOutput NoneLayout device=device buf buf mutated_inputs get_outputs - list Buffer outputs is_node_sequence nodes Sequence Union IRNode Sequence IRNode - TypeIs Sequence IRNode all isinstance n IRNode n nodes ir_dataclass frozen=False InputsKernel OperationBuffer inputs Sequence Union IRNode Sequence IRNode input_name i int - str input = inputs i assert isinstance input IRNode input get_name get_read_writes - dependencies ReadWrites reads = OrderedSet dependencies Dep StarDep = dependencies StarDep input inputs isinstance input Sequence reads update StarDep x get_name x input isinstance input ShapeAsConstantBuffer Skip creating dependency symbolics they re visible globally continue reads add StarDep input get_name writes = OrderedSet dependencies Dep StarDep buf get_name buf get_outputs dependencies ReadWrites reads=reads writes=writes index_exprs=OrderedSet get_reads - OrderedSet Dep get_read_writes reads classmethod unwrap_storage_for_input cls x IRNode - IRNode isinstance x TensorBox x = x data isinstance x StorageBox x = x data isinstance x BaseView isinstance x ReinterpretView x = ExternKernel realize_input x isinstance x TensorBox when converting ReinterpretView fails realize_input call above result will wrapped into TensorBox StorageBox pair result cls copy_input call so we should unwrap recursively cls unwrap_storage_for_input x isinstance x TorchBindObject x assert isinstance x Buffer ReinterpretView type x x staticmethod unwrap_storage inputs Sequence Union IRNode Sequence IRNode - list Union IRNode Sequence IRNode inputs_new list Union IRNode Sequence IRNode = x inputs isinstance x Sequence x = InputsKernel unwrap_storage_for_input i i x x = InputsKernel unwrap_storage_for_input x inputs_new append x inputs_new is_extern - bool True num_reads - int cache_on_self_and_args InputsKernel get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol r = OrderedSet sympy Symbol inp inputs isinstance inp IRNode r &#124; = inp get_free_symbol_uses unbacked_only inner_inp inp r &#124; = inner_inp get_free_symbol_uses unbacked_only r NopKernel InputsKernel is_no_op - bool True get_reads - OrderedSet Dep OrderedSet ConcatKernel NopKernel There isn t actually real kernel concat we just change storage upstream data classmethod create cls inputs Sequence IRNode dim int - StorageBox Create concat kernel inputs device = inputs get_device dtype = inputs get_dtype new_size = list inputs get_size offsets_start = offsets_end = new_size dim assert = dim len new_size i range len inputs input_size = inputs i get_size offsets_start append new_size dim assert len input_size == len new_size assert inputs i get_dtype == dtype assert inputs i get_device == device j range len new_size j == dim new_size j = new_size j + input_size j new_size j = V graph sizevars check_equals_and_simplify new_size j input_size j offsets_end append new_size dim output_stride Sequence int = FlexibleLayout contiguous_strides new_size config comprehensive_padding Ensure output stride matches alignment requirements output_stride = Layout _pad_strides output_stride new_size inputs dtype If any inputs CL format use CL format output i range len inputs x = inputs i is_storage_and_layout x layout = x get_layout isinstance layout FixedLayout Layout is_channels_last_contiguous layout size layout stride use CL stride output output_stride = make_channels_last_strides_for new_size break any_input_is_storage_and_layout = any is_storage_and_layout x x inputs fx_node_args = V graph current_node args assert isinstance fx_node_args list type fx_node_args If any inputs has meta tensor meta tensor CL format use CL format output any_input_is_storage_and_layout False any val arg meta arg meta val is_contiguous memory_format=torch channels_last arg meta val is_contiguous memory_format=torch channels_last_ d arg fx_node_args output_stride = make_channels_last_strides_for new_size is_pinned = all is_storage_and_layout x x get_layout is_pinned x inputs assert device None concat_kernel = ConcatKernel name=None layout=FixedLayout device=device dtype=dtype size=new_size stride=output_stride is_pinned=is_pinned inputs= kernel = StorageBox concat_kernel op_names = i inp enumerate inputs assert isinstance inp BaseView MutableBox type inp input_buffer = cls realize_into inp SliceView create kernel dim offsets_start i offsets_end i clamp=False assert isinstance input_buffer Buffer type input_buffer assert isinstance concat_kernel inputs list type concat_kernel inputs concat_kernel inputs append input_buffer isinstance inp data BaseView input_unwrapped = inp data unwrap_view input_unwrapped = inp data isinstance input_unwrapped StorageBox input_unwrapped is_input_buffer dev = inp get_device None is_gpu dev type is_dynamic input_buffer op_names append input_buffer get_operation_name len op_names V graph has_feature device BackendFeature FOREACH V graph register_operation_list op_names concat_kernel name = V graph register_buffer concat_kernel concat_kernel inputs = cls unwrap_storage concat_kernel inputs V graph register_operation concat_kernel kernel classmethod can_realize_into_without_copy cls src IRNode dst Optional IRNode = None - bool isinstance src TensorBox unwrap TensorBox cls can_realize_into_without_copy src data dst assert isinstance src BaseView StorageBox type src isinstance src data MultiTemplateBuffer isinstance src data layout FixedLayout src data output_plannable False we call can_realize_into_without_copy cat lowering before we ve decided output format optimistically assume layout matches dst None True otherwise check equality layouts len src get_stride = len dst get_stride False all V graph sizevars statically_known_equals s s s s zip src get_stride dst get_stride hasattr src data layout isinstance src data layout FlexibleLayout isinstance src data ExternKernelAlloc cache_on_self_and_args ConcatKernel get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol NopKernel get_free_symbol_uses unbacked_only classmethod realize_into cls src IRNode dst IRNode - IRNode Attempt turn into ReinterpretView rather than assert This has concessions around layout as_storage_and_layout can cause us go flexible fixed layout isinstance dst ReinterpretView is_storage_and_layout dst storage layout = as_storage_and_layout dst dst = ReinterpretView data=storage layout=layout assert isinstance dst ReinterpretView type dst isinstance src TensorBox unwrap TensorBox cls realize_into src data dst isinstance src StorageBox src realize ExternKernelAlloc has specific requirements output layout should create copy assert hasattr src data layout cls can_realize_into_without_copy src dst pyrefly ignore missing-attribute src data layout = NonOwningLayout dst src data introduce copy pw = Pointwise create device=src get_device dtype=src get_dtype inner_fn=src make_loader ranges= V graph sizevars check_equals_and_simplify b b zip src get_size dst get_size cls realize_into pw dst should_allocate - bool True ir_dataclass frozen=False ExternKernel InputsKernel A represents Kernels which directly lowered Inductor Loop Level IR such custom operators aten operators which we fallback constant_args Sequence Any = kwargs dict str Any = dataclasses field default_factory=dict output_view Optional ReinterpretView = None python_kernel_name Optional str = None cpp_kernel_name Optional str = None FIXME some cases we sill need explicitly pass ordered_kwargs_for_cpp_kernel We shouldn t need do since information can retrieved op_overload _schema ordered_kwargs_for_cpp_kernel Iterable str = dataclasses field default_factory=list op_overload Optional _OpOverloads = None arg_properties Optional list dict str Any = None allarg_properties dict str dict str Any = dataclasses field default_factory=dict kwarg_properties Optional dict str dict str Any = None unbacked_bindings dict sympy Symbol pytree KeyPath = dataclasses field default_factory=dict mutation_outputs list MutationOutput = dataclasses field default_factory=list __init__ name Optional str layout OutputSpec inputs Sequence Union IRNode Sequence IRNode constant_args Sequence Any = kwargs Optional dict str Any = None output_view Optional ReinterpretView = None python_kernel_name Optional str = None cpp_kernel_name Optional str = None ordered_kwargs_for_cpp_kernel Iterable str = op_overload Optional _OpOverloads = None - None super __init__ name=name layout=layout inputs=inputs constant_args = constant_args kwargs = kwargs kwargs output_view = output_view op_overload = op_overload set_cpp_kernel_name cpp_kernel_name set_python_kernel_name python_kernel_name ordered_kwargs_for_cpp_kernel = ordered_kwargs_for_cpp_kernel collect_arg_kwarg_properties unbacked_bindings = mutation_outputs = fx_node = V graph current_node get_outputs - list Buffer mutation_outputs get_unbacked_symbol_defs - OrderedSet sympy Symbol OrderedSet collect_arg_kwarg_properties - None op_overload torch _ops OpOverload we can use its schema collect additional information args kwargs e g type default value help cpp wrapper codegen arg_properties = name x name type x real_type default_value x default_value x op_overload _schema arguments x kwarg_only isinstance op_overload torch _ops OpOverload i range len inputs allarg_properties = x name type x real_type default_value x default_value x op_overload _schema arguments isinstance op_overload torch _ops OpOverload FIXME kwargs does always match kwargs defined schema so sometimes ordered_kwargs_for_cpp_kernel explicitly passed isinstance op_overload torch _ops OpOverload ordered_kwargs_for_cpp_kernel ordered_kwargs_for_cpp_kernel = x name x op_overload _schema arguments x kwarg_only schema_kwargs = x x op_overload _schema arguments x kwarg_only schema_kwargs = decide_layout - None isinstance layout FlexibleLayout apply_constraint freeze_layout codegen_comment wrapper PythonWrapperCodegen kernel_name Optional str = None - None origin_str _detailed_origin_str = get_kernel_metadata wrapper origin_str wrapper make_comment origin_str kernel_name kernel_name = try_get_kernel_name kernel_name debug set_kernel_post_grad_provenance_tracing debug_handle = set_kernel_post_grad_provenance_tracing kernel_name is_extern=True wrapper write_provenance_debug_handle kernel_name debug_handle codegen wrapper PythonWrapperCodegen - None raise NotImplementedError set_cpp_kernel_name cpp_kernel_name Optional str = None - None cpp_kernel_name = cpp_kernel_name V graph cpp_wrapper isinstance op_overload torch _ops OpOverload kernel = op_overload cpp_kernel_name None Try construct cpp_kernel_name op_overload kernel namespace == aten Calling default kernel name can lead ambiguous behavior like following example repeat_interleave const Tensor repeats std optional int _t output_size=std nullopt repeat_interleave const Tensor int _t repeats std optional int _t dim=std nullopt std optional int _t output_size=std nullopt opname = kernel __name__ split kernel _overloadname == default kernel __name__ replace _ cpp_kernel_name = f _ops opname call cpp_kernel_name = kernel _schema name set_python_kernel_name python_kernel_name Optional str - None python_kernel_name = python_kernel_name python_kernel_name None kernel = op_overload kernel None pass isinstance kernel torch _ops HigherOrderOperator python_kernel_name = f torch ops higher_order kernel __name__ python_kernel_name = f kernel __module__ replace _ops ops kernel __name__ try_get_kernel_name - Optional str codegen cpp_wrapper_cpu CppWrapperCpu device = d type d = get_device V graph device_type V graph fx_wrapper python_kernel_name V graph cpp_wrapper assert isinstance V graph wrapper_code CppWrapperCpu type V graph wrapper_code cpp_kernel_name None None V graph wrapper_code get_c_shim_func_name cpp_kernel_name device python_kernel_name get_kernel_name - str name = try_get_kernel_name assert name None name staticmethod copy_input x IRNode - Union TensorBox ShapeAsConstantBuffer pw = Pointwise create device=x get_device dtype=x get_dtype inner_fn=x make_loader ranges=x get_size origin_node=x get_origin_node traceback=x get_traceback pw realize pw classmethod process_kernel cls kernel _OpOverloads args Any kwargs Any - tuple Any list Any list Any Callable Any Any Any Optional dict sympy Symbol pytree KeyPath binded_args = args args kwargs kwargs args_flat args_spec = pytree tree_flatten binded_args is_arg_tensor = tensor_args can either tensor torchbind objects tensor_args = non_tensor_args list Any = arg args_flat is_arg_tensor append isinstance arg IRNode isinstance arg GeneratorState is_arg_tensor - tensor_args append arg isinstance arg Expr arg = V graph sizevars shape_env create_symintnode arg hint=None non_tensor_args append arg unflatten_args new_tensor_args Sequence _T new_non_tensor_args Sequence _T - tuple list _T dict str _T result = it_tensors = iter new_tensor_args it_non_tensors = iter new_non_tensor_args is_tensor is_arg_tensor is_tensor result append next it_tensors result append next it_non_tensors r = pytree tree_unflatten result args_spec r get args r get kwargs tensor_args = cls realize_input x x tensor_args freeze layout otherwise our output stride calculation might become incorrect x tensor_args is_storage_and_layout x as_storage_and_layout x freeze=True Rerun fake tensor propagation because Inductor may have changed strides inputs we need determine accurately what output stride will example_args list Union torch Tensor torch _C ScriptObject FakeScriptObject torch Generator = We need retain constant values fake tensors we originally propagated graph because some operators running without constant would trigger error DataDependentException x tensor_args x view constant we need realize view we can t pass constant into kernel directly isinstance x BaseView x get_name V graph constants example_args append V graph constants x get_name isinstance x BaseView x get_name V graph torchbind_constants example_args append V graph torchbind_constants x get_name isinstance x TorchBindObject example_args append x get_value isinstance x torch _inductor ir GeneratorState device_index = x device index assert x device type == cuda device_index None example_args append torch cuda default_generators device_index clone_state example_args append ir_node_to_tensor x guard_shape=True new_args new_kwargs = unflatten_args example_args non_tensor_args example_output = kernel new_args new_kwargs unbacked_bindings Optional dict sympy Symbol pytree KeyPath = None shape_env = V fake_mode shape_env node_meta_val = V current_node meta get val ctx AbstractContextManager None = nullcontext V current_node target torch _higher_order_ops effects with_effects remove first effect token meta val meta unbacked_bindings node_meta_val = node_meta_val ctx = _remove_effect_token_unbacked_bindings V current_node ctx rebind_unbacked shape_env V current_node example_output unbacked_bindings = compute_unbacked_bindings shape_env example_output node_meta_val example_out_li = example_output isinstance example_output list tuple example_output t example_out_li isinstance t torch Tensor t is_sparse msg = sparsity handled Please file issue sparse inference weights stack_trace = V graph current_node meta get stack_trace None msg = f msg Found \n stack_trace V graph disable_cudagraphs_reason = msg example_output tensor_args non_tensor_args unflatten_args unbacked_bindings classmethod convert_to_reinterpret_view cls x IRNode - ReinterpretView In order pass extern kernel we need ReinterpretView View This allows us avoid some unneeded copies assert isinstance x BaseView type x isinstance x ReinterpretView x NOTE Don t use extract_read_writes here fails when make_loader inlines computation x_unwrap_view = x unwrap_view buf = V graph get_buffer x_unwrap_view get_name assert buf None x_unwrap_view_fx_node = buf get_origin_node Prefer channels last format according how format set eager x_unwrap_view_fx_node None val x_unwrap_view_fx_node meta isinstance x_unwrap_view ReinterpretView Buffer MutableBox isinstance x_unwrap_view layout FlexibleLayout x_unwrap_view_fx_node meta val is_contiguous memory_format=torch channels_last x_unwrap_view_fx_node meta val is_contiguous memory_format=torch channels_last_ d x_unwrap_view freeze_layout_with_same_order make_channels_last_strides_for x_unwrap_view get_size x_unwrap_view freeze_layout index_args var_ranges = dependencies index_vars_squeeze x get_size prefix= r range_vars = index_args index = x make_indexer range_vars index = V graph sizevars simplify_with_ranges index var_ranges strides = V graph sizevars stride_vars index range_vars offset = V graph sizevars offset_var index range_vars expected = sympy_dot range_vars strides + offset index = expected log debug convert_to_reinterpret_view failed stride= s offset= s index= s strides offset index raise NotImplementedError ReinterpretView data=x data layout=FixedLayout device=x get_device_or_error dtype=x get_dtype size=x get_size stride=strides offset=offset is_pinned=False classmethod realize_input cls x IRNode - IRNode x None NoneAsConstantBuffer isinstance x Expr sympy logic boolalg Boolean int ShapeAsConstantBuffer expr=x isinstance x Constant V graph add_tensor_constant torch tensor x value dtype=x get_dtype device=x get_device isinstance x ConstantBuffer x isinstance x TensorBox cls realize_input x data isinstance x ReinterpretView ReinterpretView data=cls realize_input x data layout=x get_layout isinstance x BaseView x realize is_storage_and_layout x unwrap_view try cls convert_to_reinterpret_view x except NotImplementedError pass isinstance x StorageBox TODO jansel impose layout preference realized buffer x realize x isinstance x NonTensorObj ShapeAsConstantBuffer x cls copy_input x classmethod require_stride cls x IRNode - IRNode is_storage_and_layout x len x get_stride == x stride x get_stride stride == x cls copy_input x classmethod require_strides cls x IRNode order Optional Sequence int = None exact_strides Optional Sequence _IntLike = None allow_padding bool = False - IRNode assert order None exact_strides None Layout generally doesn t matter some consuming external ops might have requirements x get_numel exact_strides x require x have layout is_storage_and_layout x isinstance x get_layout FlexibleLayout order If FlexibleLayout already has size stride required order freeze FixedLayout using its current size stride The behavior using its current size stride given order can different size stride has ambiguilty example D input where iC = size= s stride= If required order channels last current size stride already satisfies order However freezing required order layout will changed size= s stride= which actually necessary use_current_stride_order = is_stride_order_storage_and_layout x order free_unbacked_symbols x get_layout stride fix flexiblelayout FixedLayout stride_order as_storage_and_layout x freeze=True want_contiguous=False stride_order= get_stride_order V graph sizevars size_hints_or_throw x get_layout stride use_current_stride_order order allow_padding=allow_padding x If exact_strides given freeze FlexibleLayout FixedLayout exact_strides as_storage_and_layout x freeze=True want_contiguous=False stride_order=None allow_padding=allow_padding exact_strides=exact_strides x isinstance x get_layout FixedLayout NonOwningLayout order x get_layout is_stride_ordered order exact_strides significant_strides_equal exact_strides x get_layout stride x get_size try_match_insignificant_strides x exact_strides exact_strides None x isinstance mutation_layout = x get_layout MutationLayoutSHOULDREMOVE isinstance real_layout = mutation_layout real_layout FlexibleLayout raise AssertionError MutationLayoutSHOULDREMOVE s real layout shouldn t FlexibleLayout isinstance real_layout FixedLayout order real_layout is_stride_ordered order exact_strides significant_strides_equal exact_strides real_layout stride x get_size x TODO - Storage InputBuffer isinstance x InputBuffer order x get_layout is_stride_ordered order exact_strides significant_strides_equal exact_strides x get_layout stride x get_size x isinstance x TensorBox isinstance x data BaseView isinstance x data ReinterpretView is_storage_and_layout unwrap_view = x unwrap_view hasattr unwrap_view data isinstance unwrap_view data ExternKernelAlloc try x data = cls convert_to_reinterpret_view x data order cls require_stride_order x order allow_padding=allow_padding exact_strides cls require_exact_strides x exact_strides allow_padding=allow_padding except NotImplementedError pass Preserve ExpandView representation would lost during copy_input Without representation expand inductor IR codegen we end up launching grid full size tensor doing redundant computation across expanded dims TODO could also good have codegen fix recognize overlapping elements expanded_dims Optional list int = None orig_size = x get_size exact_strides None sizevars = V graph sizevars expanded_dims = i i range len x get_size sizevars statically_known_equals exact_strides i sizevars statically_known_geq x get_size i dim expanded_dims x = torch _inductor lowering slice_ x dim Although clone inductor good about fusing clones into previous operations they weren t realized their layouts flexible x = cls copy_input x as_storage_and_layout x freeze=True want_contiguous=False stride_order=order allow_padding=allow_padding exact_strides=exact_strides order assert is_stride_order_storage_and_layout x order expanded_dims assert orig_size None exact_strides None x = torch _inductor lowering expand x orig_size expand will sometimes may change insignificant strides so match them back try_match_insignificant_strides x exact_strides x classmethod require_exact_strides cls x IRNode exact_strides Sequence _IntLike allow_padding bool = False - IRNode cls require_strides x exact_strides=exact_strides allow_padding=allow_padding classmethod require_stride_order cls x IRNode order Sequence int allow_padding bool = False - IRNode cls require_strides x order=order allow_padding=allow_padding classmethod require_channels_last cls x IRNode - IRNode cls require_stride_order x NHWC_STRIDE_ORDER classmethod require_channels_last_ d cls x IRNode - IRNode cls require_stride_order x NHWDC_STRIDE_ORDER classmethod require_contiguous cls x IRNode - IRNode is_mkldnn_tensor x IRNode - bool try name = x get_name except AttributeError NotImplementedError False name V graph constants V graph constants name is_mkldnn TODO move more proper places is_mkldnn_tensor x x cls require_exact_strides x FlexibleLayout contiguous_strides x get_size classmethod require_contiguous_strides cls x IRNode - IRNode TODO combine require_contiguous after https github com pytorch pytorch pull lands cls require_exact_strides x FlexibleLayout contiguous_strides x get_size apply_constraint - None pass fill_non_provided_args args Sequence Any kwargs dict str Any - Sequence Any Previously we want maintain forward-compatibility skipping default args serialized artifacts fbcode However some our shim interfaces require default values being OrderedSet Discussed Sherlock offline we decided allow serializing default args into C++ wrapper code now We will refine part we see real FC requirement More details related FC can found https docs google com document d FzWm-sHYwmRi x_g kOxd KaYquUsA-L JwOn ys edit usp=sharing assert isinstance args Sequence type args isinstance args list args = list args assert arg_properties ExternKernel arg_properties should empty n_args = len args n_pos_args = len arg_properties For cpp wrapper some positional args provided we need check they re kwargs use their default value n_args n_pos_args log debug s has d unprovided positional arguments Will check they keyword arguments will use default values op_overload n_pos_args - n_args i range n_args n_pos_args arg_name = arg_properties i name args append kwargs arg_name arg_name kwargs arg_properties i default_value args codegen_const_args names Optional list str = None - list str V graph cpp_wrapper result = Aten ops follow convention tensor args before non-tensor args which case following len inputs + i logic works But may true other ops case caller needs pass list const arg names arg_properties lookup name_to_arg_properties = None names arg_properties assert len constant_args == len names names passed codegen_const_args does match constant_args name_to_arg_properties = arg get name arg arg arg_properties i x enumerate constant_args name_to_arg_properties None assert names None prop = name_to_arg_properties get names i type_ = prop get type prop None idx = len inputs + i type_ = arg_properties idx get type arg_properties idx len arg_properties None result append V graph wrapper_code val_to_arg_str x type_ result V graph wrapper_code val_to_arg_str constant_args codegen_args - list str V graph cpp_wrapper op_overload None cpp wrapper needs special logic fill missing args default values inputs = fill_non_provided_args inputs constant_args kwargs fill_non_provided_args has handled constant args so no need codegen later need_codegen_constant_args = False inputs = inputs need_codegen_constant_args = True args = i x enumerate inputs V graph cpp_wrapper assert arg_properties i len arg_properties Invalid access ExternKernel arg_properties type_ = arg_properties i get type args append V graph wrapper_code val_to_arg_str x type_ args append V graph wrapper_code val_to_arg_str x need_codegen_constant_args args extend codegen_const_args args get_kwargs_value arg_name str kwargs Any - Any Given argument name queries values order any provided kwargs function kwargs member any available default arguments allarg_properties arg_name kwargs kwargs get arg_name arg_name kwargs kwargs get arg_name arg = allarg_properties get arg_name None arg get default_value raise AssertionError f arg_name allarg_properties codegen_kwargs skip_out bool = False - list str V graph cpp_wrapper op_overload None len schema_kwargs == All args should have been generated fill_non_provided_args codegen_args kwargs = arg_name ordered_kwargs_for_cpp_kernel skip_out arg_name == out ExternKernelOut has its own logic inserting out parameter continue v = get_kwargs_value arg_name isinstance v Expr kwargs append v assert allarg_properties None type_ = allarg_properties get arg_name get type kwargs append V graph wrapper_code val_to_arg_str v type_ kwargs = f k = V graph wrapper_code val_to_arg_str v k v kwargs items kwargs get_op_name - str fx_node None target = fx_node target op_namespace = getattr target __module__ unknown_namespace op_namespace = op_namespace replace _ops ops op_namespace = op_namespace rsplit op_name = f op_namespace target op_name = unknown_op op_name codegen_size_asserts wrapper PythonWrapperCodegen - None config size_asserts V graph cpp_wrapper comparing strides size tensor tricky Ignore them now sympy_product get_size == size = V graph wrapper_code codegen_shape_tuple get_size stride = V graph wrapper_code codegen_shape_tuple get_stride op_name = get_op_name wrapper writeline f assert_size_stride get_name size stride op_name r codegen_alignment_asserts wrapper PythonWrapperCodegen - None config alignment_asserts V graph cpp_wrapper name = get_name aligned = name V graph unaligned_buffers op_name = get_op_name aligned wrapper writeline f assert_alignment name GPU_ALIGN_BYTES op_name r wrapper writeline f buffer name op op_name assumed aligned codegen_memory_tracking wrapper PythonWrapperCodegen - None Track outputs fallback operators config test_configs track_memory_lifecycle config test_configs track_memory_lifecycle V graph cpp_wrapper wrapper write_memory_track_allocation_once name = get_name wrapper writeline f track_tensor name name get_group_stride - tuple list Sequence Expr list Expr get output sizes strides template_codegen _size = get_size _stride = get_stride iter_ranges = _size output tensor reduce_range = because no reduction _size _stride canonicalize - tuple Expr Sequence Expr Manually get canonicalization output index manually generate index formula conv sizevars = V graph sizevars sizes = get_size strides = get_stride strides = sizevars size_hint x x strides TODO I can t tell symbols here temporary index_vars = sympy_index_symbol f d i i range len sizes reorder index vars according stride index_order = sorted range len strides key=strides __getitem__ reverse=True lookup = pos idx idx pos enumerate index_order order = lookup i i range len lookup index_vars = index_vars i i order indexer = make_indexer index = indexer index_vars new_sizes reindex _prune = V graph sizevars _simplify_loops index_vars sizes index assign new variables each dimension deal numbering mismatches d d d could become d d -- which won t match d d _ add_var = var_builder c replacement = dict zip index_vars reindex add_var x x new_sizes index = sympy_subs sympy expand index replacement index tuple new_sizes cache_on_self_and_args ExternKernel get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol NB It s necessary check regular inputs we automatically have dependencies them maybe_get_symbols = maybe_free_unbacked_symbols unbacked_only maybe_free_symbols r = InputsKernel get_free_symbol_uses unbacked_only arg constant_args r &#124; = maybe_get_symbols arg arg kwargs values r &#124; = maybe_get_symbols arg r __str__ - str kernel_name = getattr python_kernel_name None lines = f python_kernel_name= kernel_name r lines += f field name = getattr field name field dataclasses fields lines append f origin_node= origin_node r str_helper lines __repr__ = __str__ ir_dataclass frozen=False ExternKernelOut ExternKernel codegen wrapper PythonWrapperCodegen - None wrapper generate_extern_kernel_out __init__ layout Layout inputs Sequence IRNode constant_args Sequence Any = kwargs Optional dict str Any = None output_view Optional ReinterpretView = None python_kernel_name Optional str = None cpp_kernel_name Optional str = None ordered_kwargs_for_cpp_kernel Sequence Any = op_overload Optional _OpOverloads = None - None unwrapped_inputs = unwrap_storage inputs assert isinstance unwrapped_inputs Sequence type unwrapped_inputs super __init__ None layout unwrapped_inputs constant_args kwargs None python_kernel_name cpp_kernel_name ordered_kwargs_for_cpp_kernel op_overload name = V graph register_buffer V graph register_operation should_allocate - bool True RandomSeeds ExternKernelOut __init__ count int device torch device - None limits = torch iinfo torch int super __init__ layout=FixedLayout device=device dtype=torch int size= count inputs= constant_args= limits min limits max count python_kernel_name= aten randint low_out FIXME Ideally we should only use _ops randint_low_out call here signature different randint_out Again we can simplify code when only keeping ABI-compatible version cpp_kernel_name= _ops randint_low_out call op_overload=aten randint low_out ExternKernelAlloc ExternKernel codegen wrapper PythonWrapperCodegen - None wrapper generate_extern_kernel_alloc __init__ layout OutputSpec inputs Sequence IRNode constant_args Sequence Any = kwargs Optional dict str Any = None python_kernel_name Optional str = None cpp_kernel_name Optional str = None ordered_kwargs_for_cpp_kernel Sequence Any = op_overload Optional _OpOverloads = None - None unwrapped_inputs = unwrap_storage inputs assert all isinstance i IRNode i unwrapped_inputs super __init__ None layout cast Sequence IRNode unwrapped_inputs constant_args kwargs None python_kernel_name cpp_kernel_name ordered_kwargs_for_cpp_kernel op_overload We need output buffers generating kernel arguments abi-compatible mode where we retrieve outputs pass each individual output through abi-compatible interface outputs Sequence Any = name = V graph register_buffer V graph register_operation should_allocate - bool False apply_constraint - None raise NotImplementedError MutationOutput Buffer An output buffer represents mutation pre-existing buffer __init__ layout OutputSpec mutated_node IRNode mutating_node Operation - None super __init__ name=None layout=layout mutated_node_name = mutated_node get_name V graph mark_buffer_mutated mutated_node_name mutation_names = mutated_node_name mutating_node Operation = mutating_node name = V graph register_buffer get_defining_op - Operation mutating_node get_mutation_names - Sequence str mutation_names should_allocate - bool False get_mutation_buffers - Sequence IRNode mutation_names = get_mutation_names buf buf V graph try_get_buffer name name mutation_names buf None TMADescriptor ExternKernel An IR node representing generic host-side TMA descriptor Triton API Mostly useful user-defined Triton kernels relying host-side TMA can principle used Inductor s Triton templates too See TMADescriptorExperimental TMADescriptorStable two implementations old API new API TMA descriptors immutable we can dedup them input args _CACHE dict Any TMADescriptor = classmethod _create_impl cls tensor IRNode tma_meta tuple str tuple Any - TMADescriptor assert len tma_meta == tma_meta == experimental TMADescriptorExperimental tensor tma_meta assert tma_meta == stable TMADescriptorStable tensor tma_meta classmethod create cls tensor IRNode tma_meta tuple str tuple Any - TMADescriptor key = id tensor tma_meta key cls _CACHE cls _CACHE key = cls _create_impl tensor tma_meta cls _CACHE key __init__ tensor IRNode inputs Sequence Any constant_args Sequence Any - None super __init__ None link back underlying tensor terms ownership avoid getting underlying tensor deleted before TMADescriptor node can deleted NonOwningLayout ReinterpretView data=tensor layout=tensor get_layout cast Sequence Buffer inputs tuple constant_args None tensor = tensor name = V graph register_buffer V graph register_operation codegen wrapper PythonWrapperCodegen - None wrapper generate_tma_descriptor get_tensor - IRNode tensor TMADescriptorExperimental TMADescriptor new host-side TMA Descriptor API ones obtained via create_ d d _tma_descriptor calls See also TMADescriptorStable new API __init__ tensor IRNode dims list Union int torch SymInt block_dims list Union int torch SymInt element_size Optional int = None - None assert len dims assert len dims == len block_dims element_size None element_size = tensor get_dtype itemsize dims = dims block_dims = block_dims element_size = element_size rank = len dims inputs = tensor constant_args = dims block_dims element_size super __init__ tensor=tensor inputs=inputs constant_args=constant_args TMADescriptorStable TMADescriptor new host-side TMA descriptor API ones obtained via TensorDescriptor from_tensor See also TMADescriptorExperimental old API __init__ tensor IRNode block_shape list Union int torch SymInt block_shape = block_shape super __init__ tensor=tensor inputs= tensor constant_args=block_shape SubgraphBuffer ExternKernel __init__ layout Layout input_nodes list Buffer gm torch fx GraphModule example_inputs list Any subgraph_name str super __init__ None layout input_nodes gm = gm example_inputs = example_inputs name = V graph register_buffer V graph register_operation subgraph = V graph make_subgraph gm example_inputs subgraph_name assert is_node_sequence inputs sym_inputs = get_symbolic_inputs inputs sym_inp sym_inputs subgraph graph_inputs sym_inp name = sym_inp subgraph graph_input_names append sym_inp name sym_inputs = sym_var name sym_var sym_inputs torch _inductor config inductor_config V set_graph_handler subgraph Don t bother autotuning Triton here inductor_config patch max_autotune=False max_autotune_gemm=False max_autotune_gemm_backends= ATEN subgraph run example_inputs codegen wrapper PythonWrapperCodegen - None CodegenGraph __init__ graph GraphLowering graph = graph name = graph name assert is_node_sequence inputs outer_inputs = t codegen_reference t inputs wrapper codegen_subgraph_with_flattened_outputs CodegenGraph subgraph sym_inputs outer_inputs name UserDefinedTritonKernel ExternKernel get_kernel_and_metadata - tuple Kernel Any list str list str triton runtime autotuner Autotuner torch _higher_order_ops triton_kernel_wrap kernel_side_table kernel = kernel_side_table get_kernel kernel_idx configs = restore_value_args list str = reset_to_zero_args list str = isinstance kernel Autotuner https github com triton-lang triton pull changes kernel restore_idx kernel restore_value hasattr kernel restore_idx restore_value_args extend kernel fn arg_names i i kernel restore_idx assert hasattr kernel restore_value restore_value_args extend kernel restore_value hasattr kernel reset_idx i kernel reset_idx reset_to_zero_args append kernel fn arg_names i assert hasattr kernel reset_to_zero reset_to_zero_args extend kernel reset_to_zero configs = kernel configs kernel = kernel fn kernel configs restore_value_args reset_to_zero_args override codegen wrapper PythonWrapperCodegen - None Overrides parent member See https github com pytorch pytorch issues torch _inductor utils triton_version_uses_attrs_dict kernel configs restore_value_args reset_to_zero_args = get_kernel_and_metadata Definition kernel new_name triton_meta extra_launch_args = wrapper define_user_defined_triton_kernel kernel configs kwargs restore_value_args reset_to_zero_args grid named_args = k get_kwargs_value k k ordered_kwargs_for_cpp_kernel arg_names = p name p kernel params type ignore attr-defined constexprs = p num p kernel params p is_constexpr type ignore attr-defined constexpr_names = OrderedSet arg_names i i constexprs args list Any = arg_types list Any = raw_keys_filtered list Any = raw_args_filtered list Any = name arg itertools chain named_args items zip itertools repeat extra_launch_args name constexpr_names triton_version_uses_attrs_dict see - we don t pass constexpr args speed up runtime continue raw_keys_filtered append name raw_args_filtered append arg isinstance arg IRNode args append arg codegen_reference arg_types append arg get_dtype isinstance arg int float bool sympy Expr args append arg arg_types append type arg name constexpr_names insert dummy value constexpr args unsupported type constexprs will end up getting baked into kernel compile time args append - arg_types append int arg None Filter out None args see https github com pytorch pytorch issues Two cases None arg The arg already tl constexpr so leave The arg tl constexpr so we have remove triton_version_uses_attrs_dict args append - arg_types append int raw_keys_filtered pop raw_args_filtered pop raise NotImplementedError f Unsupported arg type type arg arg codegen_comment wrapper new_name wrapper generate_kernel_call new_name args arg_types=arg_types raw_args=raw_args_filtered raw_keys=raw_keys_filtered triton_meta=triton_meta triton=True device=self get_device original_fxnode_name=self fx_node name cache_on_self_and_args UserDefinedTritonKernel get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol add unbacked symbols used grid ones used kwargs latter generated ExternKernel super get_free_symbol_uses unbacked_only &#124; get_free_symbols grid unbacked_only get_unbacked_symbol_defs - OrderedSet sympy Symbol OrderedSet __init__ kernel_idx int grid Any tma_descriptor_metadata dict str Any kernel_args dict str Any - None inputs list IRNode = kwargs dict str IRNode = constant_args list IRNode = k v kernel_args items isinstance v TensorBox t = InputsKernel unwrap_storage_for_input realize_input v k tma_descriptor_metadata t = TMADescriptor create t tma_descriptor_metadata k inputs append t kwargs k = t constant_args append v kwargs k = v assert len inputs = device = inputs get_device assert isinstance inputs Sequence type inputs super __init__ None NoneLayout device=self device inputs tuple constant_args kwargs kernel_idx = kernel_idx grid = grid kernel configs _ _ = get_kernel_and_metadata If we autotuning all arguments will passed assert hasattr kernel arg_names ordered_kwargs_for_cpp_kernel = arg arg kernel arg_names arg kernel_args torch _higher_order_ops triton_kernel_wrap identify_mutated_tensors autotuned_kwargs = configs kwargs len configs mutable_args = kernel_args key key identify_mutated_tensors kernel kernel_args autotuned_kwargs tma_descriptor_metadata mutation_outputs = MutationOutput NoneLayout device=self device buf buf mutable_args V graph register_operation get_outputs - list Buffer list mutation_outputs get_device - Optional torch device device InplaceBernoulliFallback ExternKernel This needs custom handle mutation properly codegen wrapper PythonWrapperCodegen - None assert all isinstance t IRNode t inputs x = cast IRNode t codegen_reference t inputs V graph cpp_wrapper Inductor doesn t really support aten Generator so Generator kwarg always NULL here which needs explicitly generated cpp wrapper wrapper writeline f get_kernel_name x join map repr constant_args NULL wrapper ending wrapper writeline f get_kernel_name x join map repr constant_args wrapper ending should_allocate - bool False get_mutation_names - Sequence str input_name get_unbacked_symbol_defs - OrderedSet sympy Symbol OrderedSet __init__ op_overload _OpOverloads x IRNode constant_args Any - None super __init__ None NoneLayout device=x get_device unwrap_storage x constant_args op_overload=op_overload V graph mark_buffer_mutated x get_name name = V graph register_buffer V graph register_operation Used deal torch complex types InplaceCopyFallback ExternKernel This needs custom handle mutation properly codegen wrapper PythonWrapperCodegen - None dst src non_blocking = codegen_args wrapper codegen_device_copy src dst non_blocking should_allocate - bool False get_mutation_names - Sequence str input_name get_unbacked_symbol_defs - OrderedSet sympy Symbol OrderedSet __init__ layout OutputSpec inputs Sequence IRNode constant_args Sequence Any - None super __init__ None layout inputs constant_args python_kernel_name= aten copy_ cpp_kernel_name= aoti_torch_copy_ V graph mark_buffer_mutated inputs get_name name = V graph register_buffer V graph register_operation classmethod create cls dst IRNode src IRNode non_blocking bool = False - InplaceCopyFallback inputs = cls realize_input t t dst src constant_args = non_blocking result = InplaceCopyFallback NoneLayout device=dst get_device inputs constant_args result MutatingFirstArgExternKernel ExternKernel This needs custom handle mutation properly codegen wrapper PythonWrapperCodegen - None assert is_node_sequence inputs argrefs = t codegen_reference t inputs map repr constant_args wrapper writeline f get_kernel_name join argrefs wrapper ending should_allocate - bool False get_mutation_names - Sequence str input_name get_unbacked_symbol_defs - OrderedSet sympy Symbol OrderedSet has_side_effects - bool True ResizeStorageBytes MutatingFirstArgExternKernel __init__ variable IRNode new_size int - None assert isinstance new_size int TODO dynamic shapes super __init__ None NoneLayout device=variable get_device unwrap_storage variable constant_args= new_size V graph mark_buffer_mutated variable get_name name = V graph register_buffer V graph register_operation python_kernel_name = inductor_ops resize_storage_bytes_ cpp_kernel_name = torch inductor resize_storage_bytes_ assert isinstance variable BaseView StorageBox TensorBox type variable V graph never_reuse_buffers add variable data get_name SetSourceTensorKernel ExternKernelAlloc __init__ self_tensor IRNode storage_tensor IRNode - None storage_tensor freeze_layout super __init__ storage_tensor get_layout self_tensor storage_tensor python_kernel_name= torch ops aten set_ source_Tensor op_overload=torch ops aten set_ source_Tensor assert isinstance self_tensor BaseView StorageBox TensorBox type self_tensor V graph never_reuse_buffers add self_tensor data get_name V graph never_reuse_buffers add storage_tensor get_name V graph never_reuse_buffers add get_name device = storage_tensor get_device mutation_outputs = MutationOutput NoneLayout device=device self_tensor MutationOutput NoneLayout device=device storage_tensor get_inputs_that_alias_output - Sequence str input_name input_name ScatterFallback ExternKernel This needs custom handle mutation properly This handles both aten scatter_ aten scatter_reduce_ It also handle case ` src ` being scalar properly codegen wrapper PythonWrapperCodegen - None wrapper generate_scatter_fallback should_allocate - bool False get_mutation_names - list str inp = inputs assert isinstance inp IRNode inp get_name get_unbacked_symbol_defs - OrderedSet sympy Symbol OrderedSet __init__ op_overload _OpOverloads x IRNode dim int index IRNode src IRNode reduce Optional str = None include_self bool = True - None src_is_tensor = isinstance src TensorBox constant_args tuple Any src_is_tensor tensors = realize_input t t x index src constant_args = dim tensors = realize_input t t x index constant_args = dim src super __init__ None NoneLayout device=x get_device unwrap_storage tensors constant_args reduce reduce include_self include_self python_kernel_name=str op_overload ordered_kwargs_for_cpp_kernel= reduce include_self op_overload=op_overload V graph mark_buffer_mutated x get_name name = V graph register_buffer V graph register_operation IndexPutFallback ExternKernel This needs custom handle mutation indices properly codegen wrapper PythonWrapperCodegen - None wrapper generate_index_put_fallback should_allocate - bool False get_mutation_names - Sequence str input_name get_unbacked_symbol_defs - OrderedSet sympy Symbol OrderedSet __init__ op_overload torch _ops OpOverload x IRNode indices list Any values Sequence Any accumulate Any - None indices = indices valid_indices = i i indices i None pyrefly ignore bad-argument-type tensors = realize_input x x x values valid_indices cpp_kernel_name = aoti_torch_index_put_out super __init__ None NoneLayout device=x get_device unwrap_storage tensors accumulate python_kernel_name= aten index_put_ cpp_kernel_name=cpp_kernel_name op_overload=op_overload V graph mark_buffer_mutated input_name name = V graph register_buffer V graph register_operation DeviceCopy ExternKernelOut classmethod create cls x IRNode device torch device non_blocking bool - IRNode x is_extern all r V graph constants r x get_read_names config aot_inductor use_runtime_constant_folding x constant_to_device device V graph add_device_info device x_device = x get_device assert x_device None V graph add_device_info x_device developer_warning DeviceCopy input program constant_args = non_blocking Device Copy should keep same layout input x = ExternKernel require_contiguous x stride = None x get_size x get_stride may unimplemented x s size empty stride = x get_stride is_destination_pinned = is_gpu x_device type device type == cpu non_blocking is_source_pinned = x_device type == cpu is_gpu device type non_blocking is_source_pinned is_storage_and_layout x x get_layout is_pinned = True DeviceCopy FixedLayout device x get_dtype x get_size stride is_pinned=is_destination_pinned cls realize_input x constant_args codegen wrapper PythonWrapperCodegen - None args = codegen_args assert len args == output_view wrapper codegen_device_copy args output_view codegen_reference args wrapper codegen_device_copy args codegen_reference args DynamicSelectStorageOffset ExternKernel The result computing dynamic selection index determined follows when index select operation unbacked actual index calculation ambiguous negative indices index + size versus non-negative indices just index To resolve we allocate unbacked SymInt represent storage offset decompose select operation into call as_strided computing storage offset runtime node get_reads - OrderedSet Dep OrderedSet should_allocate - bool False __init__ unbacked_offset_symbol sympy Symbol index sympy Symbol base_offset Union sympy Symbol int base_dim_stride Union sympy Symbol int size Union sympy Symbol int clamp bool - None super __init__ None NoneLayout device=torch device cpu This node codegen following unbacked_offset_symbol = base_offset + base_dim_stride index index = index + size unbacked_offset_symbol = unbacked_offset_symbol index = index base_offset = base_offset base_dim_stride = base_dim_stride size = size clamp = clamp get_unbacked_symbol_defs - OrderedSet sympy Symbol OrderedSet unbacked_offset_symbol cache_on_self_and_args DynamicSelectStorageOffset get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol get_free_symbols index unbacked_only codegen wrapper PythonWrapperCodegen - None wrapper codegen_dynamic_select_index clamp=self clamp DynamicSliceSize ExternKernel Computes output size slice call handling correct semantics codegen We do flexible handling unbacked indices data-dependent error Slicing has semantics indices i e x start could start -x size - x negative out-of-bounds start -x size - x x size + start negative slicing start x size - x start standard slicing start = x size - empty slice positive out-of-bounds If appropriate semantics known beforehand output size computed based start end indices If unbacked indices new unbacked symbol created represent output size codegen handles computing correct case get_reads - OrderedSet Dep OrderedSet should_allocate - bool False __init__ unbacked_size_symbol sympy Symbol start Union sympy Symbol int end Union sympy Symbol int step Union sympy Symbol int size Union sympy Symbol int super __init__ None NoneLayout device=torch device cpu This node codegen unbacked_size_symbol = unbacked_size_symbol start = start end = end step = step size = size get_unbacked_symbol_defs - OrderedSet sympy Symbol OrderedSet unbacked_size_symbol cache_on_self_and_args DynamicSliceSize get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol get_free_symbols start unbacked_only union get_free_symbols end unbacked_only codegen wrapper PythonWrapperCodegen - None wrapper codegen_dynamic_slice_size DynamicScalar ExternKernel The result call aten _local_scalar_dense get_reads - OrderedSet Dep OrderedSet should_allocate - bool False __init__ sym sympy Symbol keypath pytree KeyPath data IRNode - None data realize super __init__ None NoneLayout device=torch device cpu unwrap_storage data sym = sym keypath = keypath get_unbacked_symbol_defs - OrderedSet sympy Symbol OrderedSet sym codegen wrapper PythonWrapperCodegen - None wrapper codegen_dynamic_scalar AssertScalar ExternKernel The result call aten _assert_scalar get_reads - OrderedSet Dep OrderedSet should_allocate - bool False __init__ scalar SympyBoolean msg str - None super __init__ Buffer name layotu None NoneLayout device=torch device cpu InputsKernel inputs scalar = scalar msg = msg has_side_effects - bool True cache_on_self_and_args AssertScalar get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol get_free_symbols scalar unbacked_only codegen wrapper PythonWrapperCodegen - None config scalar_asserts NB It EXTREMELY important simplify scalar under assertion here because simplify done respect runtime asserts So you have u == runtime asserts you subsequently try simplify u == you will get True because we ve already runtime assert ed s true But we re code generating actual runtime assert here symbol = next iter get_free_symbol_uses unbacked_only=False V graph fx_wrapper TODO fix pass V graph cpp_wrapper symbol_str = f std to_string symbol sizevar = V graph wrapper_code codegen_cpp_sizevar scalar simplify=False TODO when we start compiling C++ annotate unlikely wrapper writeline f sizevar throw std runtime_error Expected msg received + symbol_str sizevar = V graph wrapper_code codegen_python_sizevar scalar simplify=False wrapper writeline f sizevar wrapper writeline f raise RuntimeError repr msg No one should ever use buffer uniformity define variable assign None wrapper writeline f get_name = None ir_dataclass frozen=False ExternKernelNode name str node export_schema Node FallbackKernel ExternKernelAlloc A represents fallback kernel handling operators directly support inductor It currently supports functional ops view ops inplace aten ops mutating ops auto-functionalizable __init__ layout OutputSpec kernel _OpOverloads tensor_args Sequence IRNode nontensor_args Sequence Any unflatten_args Callable Any kwargs Optional dict str Any = None unbacked_bindings Optional dict sympy Symbol pytree KeyPath = None - None super __init__ layout tuple tensor_args tuple nontensor_args op_overload=kernel use_runtime_dispatch = False unbacked_bindings = unbacked_bindings assert isinstance kernel torch _ops OpOverload torch _ops HigherOrderOperator f Fails create FallbackKernel kernel type kernel supported op_overload = kernel unflatten_args = unflatten_args kwargs = kwargs None kwargs assert python_kernel_name None V graph warn_fallback python_kernel_name args aliased alias_names list str = args mutated AND returned op mutation_names list str = isinstance op_overload torch _ops HigherOrderOperator We assume here HOPs FallbackKernel functional This may always true HOPs must individually opt-in FallbackKernel so please check you opt-in _c d_functional op_overload name _c d_functional kernels lowered into _CollectiveKernel which derives FallbackKernel cpp codegen The kernels don t pass can_auto_functionalize check their mutation handled properly _CollectiveKernel schema = op_overload _schema NOTE FallbackKernel supported operators We only support three types operators - functional ops - view ops - inplace aten ops - mutating ops auto-functionalizable That operator may mutate any number inputs its outputs may alias any inputs The unsupported cases usually do show up here because AOTAutograd functionalized them away only way in-place op show up here lowering pass introduced torch _library utils mutates_and_returns_first_arg op_overload mutation_names append tensor_args get_name schema is_mutable can_auto_functionalize kernel raise NotImplementedError f NYI Can t generate FallbackKernel kernel args kwargs = unflatten_args inputs constant_args handle_aliasing_and_mutation info torch _C Argument arg Any - None Assertions make sure we didn t mismatch args isinstance info type torch ListType assert isinstance arg list tuple type arg library_utils is_tensor_like_type info type PyTorch also accepts None scalar types args marked Tensor We re going check all them here assert isinstance arg tuple list arg None info alias_info None add_alias t IRNode - None alias_names append t get_name assert info alias_info None info alias_info is_write mutation_outputs append MutationOutput NoneLayout device=t get_device t library_utils is_tensorlist_like_type info type arg None optional_tensor_arg arg add_alias optional_tensor_arg assert library_utils is_tensor_like_type info type pyrefly ignore bad-argument-type add_alias arg info arg torch _library utils zip_schema schema args kwargs handle_aliasing_and_mutation info arg get_read_writes - dependencies ReadWrites read_writes = super get_read_writes op_overload torch _prims rng_prims graphsafe_run_with_rng_state arg constant_args isinstance arg GeneratorState read_writes = read_writes with_read dependencies StarDep arg get_name read_writes codegen_unbacked_symbol_defs wrapper PythonWrapperCodegen - None wrapper codegen_unbacked_symbol_defs_for_outputs get_name outputs getattr unbacked_bindings None get_unbacked_symbol_defs - Container sympy Symbol type ignore override unbacked_bindings = getattr unbacked_bindings None resolved = resolve_unbacked_bindings V graph sizevars shape_env unbacked_bindings assert resolved None resolved keys OrderedSet codegen_args - list str dataclasses dataclass Shim ref Any __repr__ - str ref assert is_node_sequence inputs tensor_args = Shim x codegen_reference x inputs args kwargs = unflatten_args tensor_args constant_args V graph cpp_wrapper isinstance op_overload torch _ops OpOverload args = fill_non_provided_args args kwargs args = V graph wrapper_code val_to_arg_str x param real_type param x zip op_overload _schema arguments args args = V graph wrapper_code val_to_arg_str x x args let codegen_kwargs handle kwargs kwargs update kwargs args staticmethod find_device tensor_args Optional Sequence torch Tensor example_output Sequence Any - Any non_torch_bind_tensor_args = t t tensor_args isinstance t TorchBindObject tensor_args None non_torch_bind_tensor_args assert tensor_args devices = arg get_device arg tensor_args arg get_device devices isinstance example_output torch Tensor example_output device isinstance example_output list tuple device_set = OrderedSet FallbackKernel find_device None x x example_output Remove None devices = device device device_set device len devices == devices device devices assert isinstance device torch device is_gpu device type device devices None has_side_effects - bool isinstance op_overload torch _ops HigherOrderOperator False get_schema_info op_overload is_mutable get_inputs_that_alias_output - Sequence str assert isinstance op_overload torch _ops OpOverload torch _ops HigherOrderOperator f Fails create FallbackKernel op_overload f type op_overload supported See Note FallbackKernel supported operators mutating op auto-functionalizable its outputs does NOT alias any inputs isinstance op_overload torch _ops HigherOrderOperator _c d_functional op_overload name op_overload _schema is_mutable can_auto_functionalize op_overload alias_names get_mutation_names - Sequence str assert len mutation_names = mutation_names export_extern_kernel_node type ignore no-untyped-def ProxyExecutor Design Note We export ExternFallbackNodes custom ops into serialized file run host side proxy executor address ABI problem This currently only implemented fbcode Eventually we will also make work OSS Detailed design doc can found https docs google com document d wC DOZFaYym t Esz X yxlLI RDnSiyRbUus bkJ edit usp=sharing log debug Extern kernel node added node s target s get_name op_overload assert isinstance FallbackKernel type args kwargs = unflatten_args inputs constant_args args = fill_non_provided_args args kwargs ordered_kwargs = get_kwargs_value key kwargs key ordered_kwargs_for_cpp_kernel target = op_overload V graph aot_mode No need serialize cpp wrapper JIT mode args ordered_kwargs serializer = GraphModuleSerializer None type ignore arg-type named_arguments = serializer serialize_inputs target args kwargs serialize_outputs handle_single_output return_type Union torch TensorType torch ListType torch JitType output Union IRNode Sequence IRNode - export_schema Argument isinstance return_type torch TensorType torch NoneType For single Tensor None out = output isinstance output list tuple assert len output == out = output isinstance return_type torch TensorType assert isinstance out IRNode export_schema Argument create as_tensor=export_schema TensorArgument name=out get_name NoneType assert out None export_schema Argument create as_none=True isinstance return_type torch ListType isinstance return_type getElementType torch TensorType assert isinstance output Sequence type output For single TensorList export_schema Argument create as_tensors= export_schema TensorArgument name=out get_name out output isinstance return_type torch OptionalType isinstance return_type getElementType torch TensorType For OptionalTensor output None export_schema Argument create as_optional_tensor=export_schema OptionalTensorArgument create as_none=True assert isinstance output IRNode export_schema Argument create as_optional_tensor=export_schema OptionalTensorArgument create as_tensor=export_schema TensorArgument name=output get_name isinstance return_type torch IntType export_schema Argument create as_int=output raise RuntimeError f Unsupported type type return_type isinstance target torch _higher_order_ops torchbind CallTorchBind returns = target schema args args returns returns = target _schema returns type ignore union-attr len returns == NOTE special handling all_reduce_coalesced_ s value all_reduce_coalesced_ list tensors via mutation_outputs outputs = outputs outputs mutation_outputs return_type = returns real_type output_arguments = handle_single_output return_type outputs For tuple returns e g - Tensor Tensor - Tesnor Tensor Not generating output args mutation_outputs output_arguments = handle_single_output return_schema real_type type ignore attr-defined output return_schema output zip returns outputs assert op_overload None node = ExternKernelNode name=self get_name node=export_schema Node target=self op_overload name inputs=named_arguments outputs=output_arguments metadata= V extern_kernel_nodes append node args ordered_kwargs override codegen wrapper PythonWrapperCodegen - None Overrides parent member See https github com pytorch pytorch issues kernel = op_overload assert kernel None kernel namespace == aten Aten Fallback Ops assert isinstance kernel torch _ops OpOverload type kernel V graph cpp_wrapper torchgen aoti fallback_ops inductor_fallback_ops str kernel inductor_fallback_ops C shim v torchgen-ed which should cover all aten ops If you do hit missed op please update fallback_ops py log warning s missing c-shim implementation using proxy executor fallback kernel use_runtime_dispatch = True kernel namespace == _quantized Internal Quantized Fallback Ops assert isinstance kernel torch _ops OpOverload type kernel V graph cpp_wrapper For non-aten OpOverload i e custom ops If op custom_ops_to_c_shims generate direct function call use_runtime_dispatch = kernel config aot_inductor custom_ops_to_c_shims Handle special case where complex number input C-shim kernel scalar input The torchgen ed shim API will use type double which incompatible complex numbers forcing fallback runtime dispatch V graph cpp_wrapper isinstance kernel torch _ops OpOverload use_runtime_dispatch is_number t torch JitType - bool isinstance t torch OptionalType is_number t getElementType isinstance t torch NumberType Using unflatten_args bit hack all complex arguments we care about constant_args calling unflatten_args puts them correct order without triggering codegen args kwargs = unflatten_args inputs constant_args Append kwarg values args ordered_kwargs_for_cpp_kernel guaranteed set since OpOverload kernel args_iter = itertools chain args get_kwargs_value k kwargs k ordered_kwargs_for_cpp_kernel use_runtime_dispatch = any isinstance v complex is_number real_type v zip args_iter kernel _schema arguments codegen_comment wrapper use_runtime_dispatch exported_args = export_extern_kernel_node assert python_kernel_name None assert op_overload None wrapper generate_fallback_kernel_with_runtime_lookup get_name python_kernel_name lambda codegen_args codegen_kwargs op_overload exported_args NOTE special handling all_reduce_coalesced_ s value outputs outputs mutation_outputs wrapper generate_fallback_kernel isinstance layout Layout codegen_size_asserts wrapper codegen_alignment_asserts wrapper codegen_memory_tracking wrapper codegen_unbacked_symbol_defs wrapper staticmethod tensor_to_layout output torch Tensor - FixedLayout is_pinned = False try is_pinned = output is_pinned except RuntimeError dispatch implemented pass FixedLayout output device output dtype convert_shape_to_inductor output size convert_shape_to_inductor output stride is_pinned=is_pinned classmethod create cls kernel _OpOverloads args Any kwargs Any - FallbackKernel Create instance FallbackKernel _OpOverloads fake_incorrect_kernels = aten _fused_moving_avg_obs_fq_helper_functional kernel fake_incorrect_kernels context = cast AbstractContextManager None V graph fake_mode context = nullcontext context example_output tensor_args non_tensor_args unflatten_args unbacked_bindings = cls process_kernel kernel args kwargs We need extra check input alignment since example inputs we created always aligned has_unaligned_input = any is_unaligned arg arg tensor_args device = cls find_device tensor_args example_output device isinstance kernel torch _higher_order_ops torchbind CallTorchBind use CPU device torchbind methods don t take output any tensor e g size device = torch device cpu example_output None packed = cls NoneLayout device=device kernel tensor_args non_tensor_args unflatten_args unbacked_bindings=unbacked_bindings assert device Not sure where find device info packed = cls MultiOutputLayout device=device kernel tensor_args non_tensor_args unflatten_args unbacked_bindings=unbacked_bindings generate_output output Any indices list tuple Any int - Any isinstance output list tuple type output generate_output output i indices + type output i i range len output isinstance output dict key generate_output val indices + type output key key val output items isinstance output torch Tensor buf = MultiOutput cls tensor_to_layout output packed indices config assume_unaligned_fallback_output has_unaligned_input tensor_is_aligned output V graph unaligned_buffers add buf name type ignore arg-type buf isinstance output int output isinstance output torch SymInt output node expr assert output None f FallbackKernel output type type output supported None outputs = generate_output example_output isinstance outputs list tuple packed outputs = outputs isinstance outputs dict packed outputs = tuple outputs packed outputs = outputs pyrefly ignore bad-return outputs apply_constraint - None super apply_constraint ir_dataclass frozen=False ComplexView FallbackKernel View complex number two dtyped numbers vice versa should_allocate - bool False get_inputs_that_alias_output - Sequence str Signal codegen our output buffer isn t safe reuse input_name __init__ layout OutputSpec kernel _OpOverloads tensor_args Sequence IRNode nontensor_args Sequence Any unflatten_args Callable Any unbacked_bindings Optional dict sympy Symbol pytree KeyPath = None - None super __init__ layout kernel tensor_args nontensor_args unflatten_args unbacked_bindings=unbacked_bindings MemoryCheckKernel FallbackKernel Custom kernel memory checking generates direct function calls TODO - custom op erroring str inputs should able custom op directly codegen wrapper PythonWrapperCodegen - None Override codegen write direct function call Extract our arguments nontensor_args wrapper write_memory_track_allocation_once alive_list dead_list is_final_step = constant_args alive_repr = repr alive_list dead_repr = repr dead_list is_final_step wrapper writeline note dont currently distinguish between buffers returned dealloc d last step call = f check_memory_step allocated= alive_repr freed= dead_repr is_final_step= is_final_step call = f check_memory_step allocated= alive_repr freed= dead_repr wrapper writeline call ir_dataclass MultiOutputLayout OutputSpec device torch device get_device - Optional torch device device MultiOutput ExternKernel codegen wrapper PythonWrapperCodegen - None wrapper codegen_multi_output skip_size_stride_alignment_checks codegen_size_asserts wrapper codegen_alignment_asserts wrapper __init__ layout OutputSpec input IRNode indices list tuple Any skip_size_stride_alignment_checks bool = False - None super __init__ None layout input name = V graph register_buffer V graph register_operation indices = indices skip_size_stride_alignment_checks = skip_size_stride_alignment_checks cache_on_self_and_args MultiOutput get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol input_node = inputs assert isinstance input_node IRNode input_node input_node get_free_symbol_uses unbacked_only should_allocate - bool len inputs == isinstance inputs CppTemplateBuffer Grouped GEMM get_inputs_that_alias_output - Sequence str inp get_name inp inputs isinstance inp FallbackKernel len inp get_inputs_that_alias_output We just use normal dataclass MutableBox TensorBox StorageBox since they re mainly lowering-time constructs we expect mutate such dataclasses dataclass MutableBox IRNode TensorBox StorageBox allow in-place mutation Tensors data IRNode has_exceeded_max_reads - bool data has_exceeded_max_reads get_device - Optional torch device data get_device make_loader - Callable Sequence Expr OpsValue data make_loader make_indexer - Callable Sequence Expr Expr data make_indexer get_stride - Sequence _IntLike data get_stride get_name - str data get_name has_large_inner_fn threshold Optional int = None - bool data has_large_inner_fn threshold mark_reuse users int - None data mark_reuse users realize_hint - None data realize_hint unwrap_view - IRNode data unwrap_view is_input_buffer - bool data is_input_buffer freeze_layout - None data freeze_layout freeze_layout_with_stride_order order Sequence int allow_padding bool = False - None data freeze_layout_with_stride_order order allow_padding freeze_layout_with_fill_order order Sequence int - None data freeze_layout_with_fill_order order freeze_layout_with_same_order stride Sequence _IntLike - None data freeze_layout_with_same_order stride freeze_layout_with_exact_strides exact_strides Sequence _IntLike allow_padding bool = False - None data freeze_layout_with_exact_strides exact_strides allow_padding get_read_writes - dependencies ReadWrites data get_read_writes get_reads - OrderedSet Dep data get_reads num_reads - int data num_reads get_storage_numel - _IntLike data get_storage_numel get_reduction_type - Optional str data get_reduction_type get_reduction_size - Sequence Expr data get_reduction_size is_extern - bool data is_extern is_no_op - bool data is_no_op constant_to_device device torch device - IRNode data constant_to_device device get_mutation_names - Sequence str data get_mutation_names get_operation_name - str data get_operation_name get_inputs_that_alias_output - Sequence str data get_inputs_that_alias_output realize - Optional str data realize cache_on_self_and_args MutableBox get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol data get_free_symbol_uses unbacked_only get_read_names - OrderedSet str data get_read_names get_defining_op - Optional Operation data get_defining_op codegen_reference writer Optional IndentedBuffer = None - str data codegen_reference writer property layout - OutputSpec we intentionally call get_output_spec rather than get_layout since Buffer layout OutputSpec data get_output_spec get_layout - Layout data get_layout get_output_spec - OutputSpec data get_output_spec get_size - Sequence Expr data get_size property dtype - torch dtype data dtype __str__ - str isinstance data MutableBox line = f type __name__ type data __name__ endl = inner = data data line = f type __name__ inner = data endl = lines = line indent str inner endl \n join lines __repr__ = __str__ TensorBox MutableBox staticmethod create data IRNode - Union TensorBox ShapeAsConstantBuffer isinstance data ShapeAsConstantBuffer data TensorBox StorageBox data StorageBox MutableBox StorageBox allow in-place mutation Tensors is_input_buffer - bool isinstance data InputBuffer ReinterpretView data get_name V graph graph_inputs False is_module_buffer - bool isinstance data ConstantBuffer data get_name V graph constants realize - Optional str IRNode is_realized_node data data get_name assert isinstance data Pointwise Reduction Scan Sort type data origin_node = data get_origin_node traceback = data get_traceback device = data get_device assert device None data = ComputedBuffer name=None layout=FlexibleLayout device=device dtype=self data get_dtype size=self data get_size is_pinned=False data=self data data name = V graph register_buffer data V graph register_operation data data origins = origins data origin_node = origin_node data traceback = traceback data name realize_hint - None Called buffers we expect forced realize later isinstance data Pointwise Reduction data inner_fn_opcount nontrivial_read_count realize has_accumulated_enough_reads_by_size threshold int - bool torch _inductor utils is_nonfreeable_buffers size_of_reads = V graph get_dep_size_hint dep dep get_reads is_nonfreeable_buffers dep size_of_reads False total_size = sum size_of_reads max_size = max size_of_reads min_size = min size_of_reads total_size = threshold total_size max_size = max_size == min_size has_exceeded_max_reads - bool isinstance data Pointwise num_reads config realize_acc_reads_threshold has_large_inner_fn config realize_acc_reads_size_threshold None has_accumulated_enough_reads_by_size config realize_acc_reads_size_threshold should_realize_on_reuse users int - bool A heuristic decide we should realize tensor used multiple times users isinstance data Pointwise Reduction is_cpu data Heuristic realizing reused result heavy ops cpu opcount = data inner_fn_opcount heavy_ops = exp sigmoid list heavy ops any x opcount used_ops x heavy_ops True num_reads config realize_reads_threshold has_large_inner_fn False mark_reuse users int - None should_realize_on_reuse users realize num_reads - int data num_reads ir_dataclass frozen=False Subgraph IRNode name str graph_module torch fx GraphModule graph Optional GraphLowering = None _has_aliased_buffers buffers Sequence IRNode - bool buffers = buffer unwrap_view isinstance buffer ReinterpretView buffer buffer buffers assuming same buffer represented same IRNode object len OrderedSet id buffer buffer buffers len buffers ir_dataclass frozen=False InvokeSubgraph ExternKernel Ir node invoke_subgraph HOP subgraph Optional Subgraph = None operands Optional Sequence IRNode = None outputs Optional Sequence IRNode = None __init__ subgraph Subgraph operands Sequence IRNode layout MultiOutputLayout - None super __init__ name=None layout=layout inputs=operands subgraph = subgraph name = V graph register_buffer V graph register_operation classmethod create cls subgraph Subgraph operands IRNode - list Union ShapeAsConstantBuffer NoneAsConstantBuffer MultiOutput For each operand get realized input force have same strides subgraph inputs then use InvokeSubgraph lowering constrain_to_fake_tensor TODO anijain - Support sym expr operands future current_node = V graph current_node fake_operands = None eager_input_vals = current_node meta get eager_input_vals eager_input_vals args_values kwargs_values We need args invoke_subgraph fake_operands = eager_input_vals For partitioned backward graph we do have eager_input_vals Here we rely recorded example values fx_operands = current_node args fake_operands = x meta val x fx_operands type ignore union-attr Realize inputs Also intermediates can have different strides than inputs subgraph So force intermediates have same strides subgraph inputs pyrefly ignore annotation-mismatch operands list IRNode = cls realize_input x x operands new_operands list IRNode = idx operand enumerate operands isinstance operand ShapeAsConstantBuffer GeneratorState new_operands append operand new_operands append constrain_to_fake_tensor operand fake_operands idx pyrefly ignore bad-assignment operands = new_operands subgraph graph None create lower subgraphs subgraph graph = V graph make_subgraph gm=subgraph graph_module example_inputs=fake_operands subgraph_name=subgraph name V set_graph_handler subgraph graph subgraph graph run fake_operands outputs = subgraph graph graph_outputs Find device - operands could integers shapes so we can t use operands device = None operand operands isinstance operand ShapeAsConstantBuffer device = operand get_device break assert device None invoke_subgraph = InvokeSubgraph subgraph=subgraph operands=operands layout=MultiOutputLayout device=device create_output output IRNode ind int - Union ShapeAsConstantBuffer NoneAsConstantBuffer MultiOutput isinstance output ShapeAsConstantBuffer NoneAsConstantBuffer output device = output get_device assert device None MultiOutput FixedLayout device=device dtype=output get_dtype size=output get_size stride=output get_stride offset=output get_layout offset is_pinned=output get_layout is_pinned invoke_subgraph type ignore has-type list ind skip_size_stride_alignment_checks=True outs = create_output output i i output enumerate outputs invoke_subgraph outputs = outs type ignore assignment outs codegen wrapper PythonWrapperCodegen - None wrapper codegen_invoke_subgraph ir_dataclass frozen=False Conditional ExternKernel predicate Optional IRNode = None operands Optional Sequence IRNode = None true_subgraph Optional Subgraph = None false_subgraph Optional Subgraph = None outputs Optional Sequence MultiOutput = None __init__ predicate IRNode operands Sequence IRNode true_subgraph Subgraph false_subgraph Subgraph layout MultiOutputLayout unbacked_bindings Optional dict sympy Symbol pytree KeyPath - None predicate = predicate operands = operands true_subgraph = true_subgraph false_subgraph = false_subgraph sym_args tensor_args = _split_by_sym_type predicate operands super __init__ name=None layout=layout inputs=tensor_args constant_args=sym_args unbacked_bindings None unbacked_bindings = unbacked_bindings name = V graph register_buffer V graph register_operation staticmethod _maybe_expr s Union int torch SymInt - Union int sympy Expr isinstance s int s s node expr classmethod create cls predicate TensorBox true_fn Subgraph false_fn Subgraph operands list Union TensorBox ShapeAsConstantBuffer - Sequence IRNode Create Sequence IRNodes conditional statement see lowering cond pyrefly ignore bad-assignment predicate = cls realize_input predicate pyrefly ignore bad-assignment operands = cls realize_input x x operands fx_operands Argument = V graph current_node args - assert isinstance fx_operands Sequence type fx_operands assert all isinstance n Node n fx_operands fake_operands = cast Node x meta val x fx_operands subgraph true_fn false_fn subgraph graph None create lower subgraphs subgraph graph = V graph make_subgraph gm=subgraph graph_module example_inputs=fake_operands subgraph_name=subgraph name V set_graph_handler subgraph graph subgraph graph run fake_operands assert true_fn graph None assert false_fn graph None true_outputs = true_fn graph graph_outputs false_outputs = false_fn graph graph_outputs name outputs true_fn true_outputs false_fn false_outputs _has_aliased_buffers true_outputs raise AssertionError Output aliasing currently supported compiled torch cond f The outputs name subgraph torch cond aliased outputs make sure true false outputs structurally equivalent assert len true_outputs == len false_outputs true_outputs false_outputs i t_o f_o enumerate zip true_outputs false_outputs assert t_o get_device == f_o get_device i t_o f_o assert t_o get_dtype == f_o get_dtype i t_o f_o assert t_o get_layout offset == f_o get_layout offset i t_o f_o device = next o get_device o predicate + operands isinstance o ShapeAsConstantBuffer unbacked_bindings = resolve_unbacked_bindings V graph sizevars shape_env V graph current_node meta get unbacked_bindings None assert device None cannot determine device conditional = Conditional predicate=predicate operands=operands true_subgraph=true_fn false_subgraph=false_fn layout=MultiOutputLayout device=device unbacked_bindings=unbacked_bindings outputs = MultiOutput FixedLayout device=device dtype=output get_dtype size= Conditional _maybe_expr sz sz merged_output size stride= Conditional _maybe_expr sz sz merged_output stride offset=output get_layout offset is_pinned=output get_layout is_pinned conditional list i true false outputs equivalent we can use either them here template i output merged_output enumerate zip true_outputs V graph current_node meta val conditional outputs = outputs type ignore assignment outputs codegen wrapper PythonWrapperCodegen - None wrapper codegen_conditional wrapper codegen_unbacked_symbol_defs_for_outputs get_name outputs getattr unbacked_bindings get_unbacked_symbol_defs - OrderedSet sympy Symbol unbacked_bindings = getattr unbacked_bindings None resolved = resolve_unbacked_bindings V graph sizevars shape_env unbacked_bindings assert resolved None OrderedSet resolved keys OrderedSet _split_by_sym_type args list Any - tuple list ShapeAsConstantBuffer list Any non_sym_args = sym_args = arg args isinstance arg ShapeAsConstantBuffer sym_args append arg expr non_sym_args append arg sym_args non_sym_args ir_dataclass frozen=False WhileLoop ExternKernel The IR node while_loop while_loop_stack_output It supports input mutation carried_inputs Optional Sequence IRNode = None additional_inputs Optional Sequence IRNode = None cond_subgraph Optional Subgraph = None body_subgraph Optional Subgraph = None outputs Optional Sequence MultiOutput = None __init__ carried_inputs Sequence IRNode additional_inputs Sequence IRNode cond_subgraph Subgraph body_subgraph Subgraph layout MultiOutputLayout unbacked_bindings Optional dict sympy Symbol pytree KeyPath stack_output bool - None carried_inputs = carried_inputs additional_inputs = additional_inputs cond_subgraph = cond_subgraph body_subgraph = body_subgraph sym_args tensor_args = _split_by_sym_type carried_inputs additional_inputs super __init__ name=None layout=layout inputs=tensor_args constant_args=sym_args unbacked_bindings None unbacked_bindings = unbacked_bindings stack_output = stack_output name = V graph register_buffer V graph register_operation Accidental aliasing can created due cse where empty buffers we allocated backward use gets csed into same buffer function fx_graph_cse See test_scan_multiple_layers_gradient concrete example staticmethod _clone_aliased_inputs carried_inputs Sequence IRNode - Sequence IRNode _has_aliased_buffers carried_inputs carried_inputs Import clone lowering module Unwrap views get underlying buffers comparison unwrapped_buffers = buffer unwrap_view isinstance buffer ReinterpretView buffer buffer carried_inputs Track which buffers we ve seen their indices seen_buffers OrderedSet int = OrderedSet result list Union IRNode TensorBox ShapeAsConstantBuffer = original_input unwrapped_buffer zip carried_inputs unwrapped_buffers id unwrapped_buffer seen_buffers result append ExternKernel copy_input original_input seen_buffers add id unwrapped_buffer result append original_input result staticmethod _maybe_wrap_as_tensor_box out IRNode - IRNode isinstance out TensorBox out isinstance out StorageBox ReinterpretView TensorBox out isinstance out MultiOutput TensorBox create out raise RuntimeError f NYI unsupported output type type out classmethod create cls cond_fn Subgraph body_fn Subgraph carried_inputs Sequence IRNode additional_inputs Sequence IRNode stack_output bool - Union IRNode Sequence IRNode create while_loop IR node stack_output controls whether stack each iterations output which necessary training torch _higher_order_ops utils check_input_alias_and_mutation _require_exact_strides tensor_boxes Sequence IRNode fake_tensors list Union int torch SymInt torch Tensor - list IRNode assert len tensor_boxes == len fake_tensors ret = tb fk zip tensor_boxes fake_tensors isinstance fk torch Tensor Subgraph lowering always StorageBox graph_outputs because realizes outputs However require_exact_strides expecting TensorBox e g require_exact_strides when expand happens fake tensor s stride storage box might have different stride so lowering slice_ used make stride consistent expects input TensorBox So we wrap inputs tensor boxes they re yet new_tb = WhileLoop _maybe_wrap_as_tensor_box tb ret append ExternKernel require_exact_strides new_tb fk stride allow_padding=False ret append tb ret fx_carried_inputs = V graph current_node args - fx_additional_inputs = V graph current_node args - fx_all_inputs = fx_carried_inputs + fx_additional_inputs type ignore operator fake_all_inputs = x meta val x fx_all_inputs type ignore union-attr fake_carried_inputs = x meta val x fx_carried_inputs type ignore union-attr fake_additional_inputs = x meta val x fx_additional_inputs type ignore union-attr carried_inputs_ = cls realize_input x x carried_inputs carried_inputs_ = WhileLoop _clone_aliased_inputs carried_inputs_ carried_inputs_ = _require_exact_strides carried_inputs_ fake_carried_inputs additional_inputs_ = cls realize_input x x additional_inputs additional_inputs_ = _require_exact_strides additional_inputs_ fake_additional_inputs all_inputs = carried_inputs_ + additional_inputs_ subgraph cond_fn body_fn subgraph graph None create lower subgraphs assert isinstance fx_all_inputs Sequence type fx_all_inputs subgraph graph = V graph make_subgraph gm=subgraph graph_module example_inputs=fx_all_inputs type ignore arg-type subgraph_name=subgraph name V set_graph_handler subgraph graph subgraph graph run fake_all_inputs For body_fn we require its output have exact same stride inputs because previous output input next iteration This cannot automatically done graph lowering because body_fn s graph outputs user-facing so special handling strides user-facing output graph lowering applicable subgraph body_fn assert len subgraph graph graph_outputs == len fake_carried_inputs subgraph graph graph_outputs = _require_exact_strides type ignore assignment subgraph graph graph_outputs fake_carried_inputs assert cond_fn graph body_fn graph cond_outputs = cond_fn graph graph_outputs body_outputs = body_fn graph graph_outputs _has_aliased_buffers body_outputs raise AssertionError Output aliasing currently supported compiled torch while_loop f The outputs body_fn subgraph torch while_loop aliased body_outputs make sure cond_fn returns boolean scalar Tensor assert len cond_outputs == cond_outputs p = cond_outputs isinstance p ShapeAsConstantBuffer assert p get_dtype == torch bool p assert len p get_size == p assert len all_inputs torch while_loop assumed have least one operand device = all_inputs get_device assert device None make linter happy make sure carried_inputs_ body outputs structurally equivalent assert len carried_inputs_ == len body_outputs carried_inputs_ body_outputs i op bo enumerate zip carried_inputs_ body_outputs _guard_list_equals lhs_exprs Sequence Union int sympy Expr rhs_exprs Sequence Union int sympy Expr - None assert len lhs_exprs == len rhs_exprs lhs rhs zip lhs_exprs rhs_exprs V graph sizevars check_equals lhs rhs _guard_list_equals op get_size bo get_size _guard_list_equals op get_stride bo get_stride assume all carried_inputs_ outputs same device MultiOutputLayout below requires single device assert op get_device == bo get_device i op bo device assert op get_dtype == bo get_dtype i op bo assert device None unbacked_bindings = resolve_unbacked_bindings V graph sizevars shape_env V graph current_node meta get unbacked_bindings None while_loop = WhileLoop carried_inputs=carried_inputs_ additional_inputs=additional_inputs_ cond_subgraph=cond_fn body_subgraph=body_fn asserted above there least one operand layout=MultiOutputLayout device=device unbacked_bindings=unbacked_bindings stack_output=stack_output assert body_fn graph None isinstance body_fn graph module torch fx GraphModule make linter happy Handling input mutations mutated_idxs = check_input_alias_and_mutation body_fn graph module fake_all_inputs mutated_idx_set = OrderedSet mutated_idxs mutated_inputs = all_inputs idx idx mutated_idx_set Create all outputs first mutated_inputs_iter = iter mutated_inputs all_outputs list IRNode = while_loop outputs = while_loop mutation_outputs = stack_output assert len mutated_idx_set == NYI while_loop_stack_output input mutations idx output enumerate V graph current_node meta val Create MultiOutput regular outputs multi_out = MultiOutput FixedLayout device=output device type ignore arg-type dtype=output dtype size= Conditional _maybe_expr sz sz output size stride= Conditional _maybe_expr st st output stride while_loop list idx while_loop outputs append multi_out all_outputs append multi_out idx output enumerate body_outputs idx mutated_idx_set assert idx len carried_inputs only carries can mutated Create MutationOutput mutated inputs mutated_input = next mutated_inputs_iter while_loop mutation_outputs append MutationOutput mutated_input layout mutated_input while_loop type ignore attr-defined union-attr all_outputs append mutated_input multi_out = MultiOutput FixedLayout device=output get_device type ignore arg-type dtype=output get_dtype size=output get_size stride=output get_stride offset=output get_layout offset while_loop list idx while_loop outputs append multi_out all_outputs append multi_out inp out zip carried_inputs all_outputs inp get_name V graph graph_inputs carried input while_loop graph input can returned when number iterations zero due we can t generally reuse output buffers corresponding graph inputs inputs may end up being mutated V graph never_reuse_buffers add out get_name all_outputs codegen wrapper PythonWrapperCodegen - None wrapper codegen_while_loop stack_output wrapper codegen_unbacked_symbol_defs_for_outputs get_name outputs getattr unbacked_bindings get_unbacked_symbol_defs - OrderedSet sympy Symbol unbacked_bindings = getattr unbacked_bindings None resolved = resolve_unbacked_bindings V graph sizevars shape_env unbacked_bindings assert resolved None OrderedSet resolved keys OrderedSet EffectfulKernel FallbackKernel __init__ layout OutputSpec kernel _OpOverloads tensor_args Sequence IRNode nontensor_args Sequence Any unflatten_args Callable Any kwargs Optional dict str Any = None unbacked_bindings Optional dict sympy Symbol pytree KeyPath = None - None super __init__ layout kernel tensor_args nontensor_args unflatten_args kwargs=None unbacked_bindings=unbacked_bindings torch _higher_order_ops effects get_effect_key uncovered_args = value isinstance TorchBindObject tensor_args effect_type = get_effect_key kernel nontensor_args uncovered_args kwargs assert effect_type None effect_type = effect_type prev_effect_buffer = V graph effectful_ops get effect_type None V graph effectful_ops effect_type = get_read_writes - dependencies ReadWrites read_writes = super get_read_writes prev_effect_buffer None read_writes reads add dependencies StarDep prev_effect_buffer get_name read_writes has_side_effects - bool True NonTensorObj IRNode cache_on_self_and_args NonTensorObj get_free_symbol_uses unbacked_only bool = False - OrderedSet sympy Symbol OrderedSet ir_dataclass TorchBindObject NonTensorObj name str value Union FakeScriptObject torch ScriptObject get_name - str name codegen_reference writer Optional IndentedBuffer = None - str name get_value - Union FakeScriptObject torch ScriptObject value get_real_obj - torch ScriptObject isinstance value torch ScriptObject value value real_obj get_buf_bytes - int Returns sum all tensors flattened object real_script_obj = get_real_obj assert hasattr real_script_obj __obj_flatten__ flat_dict = dict real_script_obj __obj_flatten__ flat_elems = pytree tree_flatten flat_dict flat_sizes = x element_size x numel x flat_elems isinstance x torch Tensor functools reduce operator add flat_sizes ir_dataclass GeneratorState NonTensorObj name str device torch device get_name - str name codegen_reference writer Optional IndentedBuffer = None - str name _CollectiveKernel FallbackKernel should_allocate - bool False has_side_effects - bool True This identical FallbackKernel set_cpp_kernel minus part checks against input aliasing mutation set_cpp_kernel_name cpp_kernel_name Optional str = None - None assert type op_overload torch _ops OpOverload Setting cpp kernel needs valid op_overload kernel = op_overload cpp_kernel_name None cpp_kernel_name = cpp_kernel_name cpp_kernel_name = kernel _schema name ordered_kwargs_for_cpp_kernel = x name x kernel _schema arguments x kwarg_only NOTE In-Place Collective Safety Between initiation completion in-place collective input buffers subject both volatile reads volatile writes They must read written reused another kernel To ensure constraints we model collective - wait_tensor two-step mutation input buffers classmethod create_inplace cls kernel _OpOverloads inputs Union IRNode list IRNode args Any kwargs Any - None V graph fake_mode _example_output tensor_args non_tensor_args unflatten_args unbacked_bindings = cls process_kernel kernel inputs args kwargs assert unbacked_bindings f kernel unbacked_bindings tensor_arg tensor_args tensor_arg realize V graph mark_buffer_mutated tensor_arg get_name device = tensor_args get_device packed = cls NoneLayout device=device kernel tensor_args non_tensor_args unflatten_args inps = pytree tree_leaves inputs packed mutation_outputs extend MutationOutput NoneLayout device=device buf packed buf inps For inplace collective ops input guaranteed alias returned value op packed alias_names extend inp get_name inp inps out kwargs packed mutation_outputs append MutationOutput NoneLayout device=device kwargs out packed For out-variant collective ops ` out= ` arg guaranteed alias returned value op packed alias_names append kwargs out get_name NOTE Out-of-Place Collective Safety Between initiation completion out-of-place collective Input buffers - Are subject volatile reads - Can read another kernel - Must written reused another kernel Output buffers - Are subject volatile writes - Must read written reused another kernel To ensure safety input buffers without sacrificing read availability we add input buffers read deps wait_tensor kernels To ensure safety output buffers we model wait_tensor mutation output buffer Note we also assumes user program being correct output buffer consumed kernels other than wait_tensor TODO yifu add pre-grad pass validate correctness collective usage user program classmethod create_out_of_place cls kernel _OpOverloads inputs Union TensorBox list TensorBox args Any kwargs Any - Union list MultiOutput _CollectiveKernel V graph fake_mode example_output tensor_args non_tensor_args unflatten_args unbacked_bindings = cls process_kernel kernel inputs args kwargs assert unbacked_bindings f kernel unbacked_bindings tensor_arg tensor_args tensor_arg realize isinstance example_output list device = cls find_device tensor_args example_output assert device None packed = cls MultiOutputLayout device=device kernel tensor_args non_tensor_args unflatten_args packed outputs = MultiOutput cls tensor_to_layout tensor packed list i i tensor enumerate example_output buf tensor zip packed outputs example_output config assume_unaligned_fallback_output tensor_is_aligned tensor V graph unaligned_buffers add buf name type ignore arg-type packed outputs packed = cls cls tensor_to_layout example_output kernel tensor_args non_tensor_args unflatten_args config assume_unaligned_fallback_output tensor_is_aligned example_output V graph unaligned_buffers add packed name type ignore arg-type packed outputs = packed packed _AllReduce_Kernel _CollectiveKernel __init__ layout OutputSpec kernel _OpOverloads tensor_args Sequence IRNode nontensor_args Sequence Any unflatten_args Callable Any kwargs Optional dict str Any = None unbacked_bindings Optional dict sympy Symbol pytree KeyPath = None - None super __init__ layout kernel tensor_args nontensor_args unflatten_args kwargs=None unbacked_bindings=unbacked_bindings set_cpp_kernel_name aoti_torch_cpu__c d_functional_all_reduce_ codegen wrapper PythonWrapperCodegen - None wrapper include_extra_header torch csrc inductor aoti_torch c shim_cpu h wrapper generate_extern_kernel_alloc isinstance layout Layout codegen_size_asserts wrapper _AllReduceKernel _CollectiveKernel __init__ layout OutputSpec kernel _OpOverloads tensor_args Sequence IRNode nontensor_args Sequence Any unflatten_args Callable Any kwargs Optional dict str Any = None unbacked_bindings Optional dict sympy Symbol pytree KeyPath = None - None super __init__ layout kernel tensor_args nontensor_args unflatten_args kwargs=None unbacked_bindings=unbacked_bindings set_cpp_kernel_name aoti_torch_cpu__c d_functional_all_reduce codegen wrapper PythonWrapperCodegen - None wrapper include_extra_header torch csrc inductor aoti_torch c shim_cpu h wrapper generate_extern_kernel_alloc isinstance layout Layout codegen_size_asserts wrapper _WaitKernel _CollectiveKernel __init__ layout OutputSpec kernel _OpOverloads tensor_args Sequence IRNode nontensor_args Sequence Any unflatten_args Callable Any kwargs Optional dict str Any = None unbacked_bindings Optional dict sympy Symbol pytree KeyPath = None - None super __init__ layout kernel tensor_args nontensor_args unflatten_args kwargs=None unbacked_bindings=unbacked_bindings set_cpp_kernel_name aoti_torch_cpu__c d_functional_wait_tensor codegen wrapper PythonWrapperCodegen - None wrapper include_extra_header torch csrc inductor aoti_torch c shim_cpu h wrapper generate_extern_kernel_alloc isinstance layout Layout codegen_size_asserts wrapper get_volatile_reads - Sequence IRNode inp = inputs assert isinstance inp IRNode isinstance inp _CollectiveKernel Out-of-place single-output i = inp inputs assert isinstance i IRNode type i i isinstance inp MultiOutput This can two things Out-of-place multi-output coll In-place coll inputs coming another MultiOutput coll = inp inputs Case isinstance coll _CollectiveKernel _ idx = inp indices pyrefly ignore bad-return coll inputs idx Case In-place requires no additional deps handling volatile reads since inputs mutated classmethod create_wait cls kernel _OpOverloads inp TensorBox - None V graph fake_mode _example_output tensor_args non_tensor_args unflatten_args unbacked_bindings = cls process_kernel kernel inp assert unbacked_bindings f kernel unbacked_bindings packed = cls NoneLayout device=inp get_device kernel tensor_args non_tensor_args unflatten_args packed mutation_outputs append MutationOutput NoneLayout device=inp get_device inp packed get_read_writes - dependencies ReadWrites read_writes = super get_read_writes See Out-of-Place Collective Safety volatile_reads = get_volatile_reads vr volatile_reads read_writes reads add dependencies StarDep vr get_name read_writes NB recursive structure here reflects val_to_arg_str avoid calling free_unbacked_symbols exotic types don t get pexpr treatment maybe_free_unbacked_symbols s object - OrderedSet Symbol isinstance s SymTypes Expr This branch should impossible position free_unbacked_symbols s isinstance s tuple list r = OrderedSet sympy Symbol t s r &#124; = maybe_free_unbacked_symbols t r isinstance s torch Tensor This branch impossible constant-args position free_unbacked_symbols s OrderedSet maybe_free_symbols s object - OrderedSet Symbol isinstance s SymTypes Expr This branch should impossible position free_symbols s isinstance s tuple list r = OrderedSet sympy Symbol t s r &#124; = maybe_free_symbols t r isinstance s torch Tensor This branch impossible constant-args position free_symbols s OrderedSet assign_origin_node result Any n torch fx Node - None This complete doesn t have origin_node tracking best effort The logic here critically relies direct TensorBox - StorageBox denoting non-view we don t bother trying get views work Feel free add any extra cases needed Note we can t YOLO tree_map over result because there buffers view involved we might able validly assign origin_node here isinstance result TensorBox isinstance result data StorageBox isinstance result data data Loops result data data _post_init_setattr origin_node n isinstance result data data Buffer result data data _post_init_setattr origin_node n isinstance result data data ComputedBuffer isinstance result data data data Loops result data data data _post_init_setattr origin_node n Not really multi-output can straightforwardly recurse isinstance result data data MultiOutput result data data indices isinstance result data data inputs Buffer result data data inputs _post_init_setattr origin_node n