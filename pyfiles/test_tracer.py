Owner s oncall jit ruff noqa F copy io os sys unittest torch torch nn nn torch nn functional F torch autograd Function Variable torch testing FileCheck Make helper files test importable pytorch_test_dir = os path dirname os path dirname os path realpath __file__ sys path append pytorch_test_dir warnings Standard library collections namedtuple itertools chain typing Dict List Optional Tuple torch Tensor torch testing _internal common_cuda with_tf _off torch testing _internal common_utils enable_profiling_mode_for_profiling_tests IS_SANDCASTLE raise_on_run_directly skipIfCompiledWithoutNumpy skipIfCrossRef skipIfTorchDynamo suppress_warnings TemporaryFileName torch testing _internal jit_utils _tmp_donotuse_dont_inline_everything _trace enable_cpu_fuser JitTestCase make_global RUN_CUDA RUN_CUDA_MULTI_GPU skipIfTorchDynamo Not suitable test TorchDynamo TestTracer JitTestCase unittest skipIf RUN_CUDA requires CUDA test_large_nbr_kernel_args Recurrence nn Module __init__ seq_len super __init__ seq_len = seq_len forward input input = input transpose Main loop output = i range seq_len b = input i output append b output = torch cat output view input size output size output = output transpose output input_size = batch_size = seq_len = rec = Recurrence seq_len input = torch rand batch_size seq_len input_size torch cuda set_device rec = rec cuda input = input cuda traced_rec = torch jit trace rec input test_trace_legacy_ctor MyModule nn Module forward x x + torch FloatTensor traced_rec = torch jit trace MyModule torch randn test_simple x = torch tensor requires_grad=True y = torch tensor requires_grad=True f x y torch sigmoid torch tanh x x + y checkTrace f x y test_trace_checking_with_global_name MyClass torch nn Module forward xs List Tensor y = torch cat xs dim= y model = MyClass Simulate these inputs being globals like they would e g they defined outermost scope script global input input input = torch ones input = torch ones m = torch jit trace model input input test_trace_aliased_parameter M nn Module __init__ x super __init__ x = nn Parameter x forward y x + y m = M torch rand r = torch jit trace m m x t = torch rand assertEqual r t m x + t test_trace_nested_fn TracedInlineDecision torch nn Module forward x flag torch jit script make_decision flag x flag x torch zeros_like x x = torch neg x make_decision flag x decision = TracedInlineDecision torch jit trace decision torch rand torch tensor True dtype=torch bool check_trace=True test_trace_single_tuple x = torch tensor f x x jit_f = torch jit trace f x assert f x == jit_f x fails test_trace_out_operator_with_two_output example_input = torch rand out_ out_ = torch cummax example_input run_cummax example_input out_ out_ output_ output_ = torch cummax example_input out= out_ out_ output_ output_ trace_model = torch jit trace run_cummax example_input out_ out_ test_trace_namedtuple Point = namedtuple point x y f p type p tuple p = Point p p x + p y p = Point torch randn torch randn traced = torch jit trace f p assertEqual f p traced p test_trace_topk M torch nn Module forward x y x topk y dim= mod = M inputs = torch randint torch tensor traced_func = torch jit trace mod inputs test_inputs = torch randint torch tensor eager_out = mod test_inputs traced_out = traced_func test_inputs assertNotWarn lambda traced_func test_inputs Shouldn t throw slicing related warn here assertEqual eager_out traced_out test_inputs = torch randint torch tensor eager_out = mod test_inputs traced_out = traced_func test_inputs assertNotWarn lambda traced_func test_inputs Shouldn t throw slicing related warn here assertEqual eager_out traced_out test_typeas_trace_check = torch tensor requires_grad=True b = torch tensor requires_grad=True f x y x type_as y trace = torch jit trace f b test_trace_index x = torch tensor requires_grad=True y = torch tensor dtype=torch int fn x y x y fn_traced = torch jit trace fn x y assertEqual fn x y fn_traced x y Backwards tracing broken indexing constant because s internally implemented using as_strided we attempted trace its derivative which currently supported It currently works because slice now marked traceable test_trace_index_constant x = torch tensor requires_grad=True fn x x run f y = f x grad = torch autograd grad y x clone y grad traced_fn = torch jit trace fn torch ones assertEqual run fn run traced_fn test_index_put ten = torch zeros mask = torch tensor True True True True False False True True False test_fn ten mask ten mask = torch ones ten traced_test_fn = torch jit trace test_fn ten mask ten = torch rand assertEqual test_fn ten mask traced_test_fn ten mask test_canonicalize_tensor_iterator x = torch randn f x x = x + x = x - x = x x = x x traced = torch jit trace f x f x graph = traced graph_for x There should int constants right sides operators plus one alpha argument add sub assertTrue str traced graph_for x count int = prim Constant == suppress_warnings test_constant x = torch randn requires_grad=True f x x matmul torch diag torch tensor checkTrace f x torch ones requires_grad=True test_wrapped_number Scalar s get converted wrapped tensors default tensor type Wrapped tensors behave differently certain promotion operations float_tensor double - float wrapped_float double - double This can cause issues check-trace handled correctly ` aten isclose ` foobar x = - result = x torch ones dtype=torch float result scripted = torch jit trace foobar check_trace=True test_inplace_transplant x = torch tensor requires_grad=True fn x y = x clone y add_ y add_ y g _ = torch jit _get_trace_graph fn x run_pass dce g FileCheck check_count aten clone exactly=True check_count aten add_ exactly=True check_next run str g assertExportImport g x test_inplace_flags InplaceFn Function staticmethod forward ctx x ctx mark_dirty x x add_ staticmethod backward ctx go go RegularFn Function staticmethod forward ctx x x add staticmethod backward ctx go go x = torch tensor requires_grad=True fn x y = RegularFn apply x y = InplaceFn apply y y = InplaceFn apply y y = RegularFn apply y y trace_graph _ = torch jit _get_trace_graph fn x _force_outplace=True run_pass dce trace_graph ops = list trace_graph nodes op ops assertTrue op hasAttribute inplace inplace_flags = False True True False op is_inplace zip ops inplace_flags assertEqual op i inplace is_inplace test_inplace_check MyInplaceFn Function staticmethod forward x x add_ mark_dirty x x staticmethod backward grad grad fn x MyInplaceFn apply x x = torch randn ge = torch jit trace fn x _force_outplace=True check_trace=False assertRaisesRegex RuntimeError inplace MyInplaceFn ge x test_force_outplace_check_fill f x torch empty x shape fill_ x = torch randn ft = torch jit trace f x _force_outplace=True assertEqual f x ft x test_force_outplace_check_zero f x torch empty x shape zero_ x = torch randn ft = torch jit trace f x _force_outplace=True assertEqual f x ft x do_trace_size requires_grad fn x x view x shape x size x = torch randn requires_grad=requires_grad y = torch randn requires_grad=requires_grad Check behaves expected traced_fn = torch jit trace fn x assertEqual traced_fn y fn y assertEqual traced_fn x fn x test_trace_size do_trace_size False test different graph_executor path happens when gradients required sizes involved test_trace_size_with_grad do_trace_size True test_trace_numel fn x x numel x = torch randn y = torch randn traced_fn = torch jit trace fn x assertEqual traced_fn y fn y assertEqual traced_fn x fn x do_trace_arange requires_grad arange x torch arange x shape arange_scalar x torch arange arange_start_end x torch arange start=x shape end=x shape + x = torch randn requires_grad=requires_grad y = torch randn requires_grad=requires_grad Check behaves expected traced_arange = torch jit trace arange x assertEqual traced_arange y arange y assertEqual traced_arange x arange x traced_arange_scalar = torch jit trace arange_scalar x assertEqual traced_arange_scalar y arange_scalar y assertEqual traced_arange_scalar x arange_scalar x traced_arange_start_end = torch jit trace arange_start_end x assertEqual traced_arange_start_end y arange_start_end y assertEqual traced_arange_start_end x arange_start_end x test_trace_arange do_trace_arange False test different graph_executor path happens when gradients required sizes involved test_trace_arange_with_grad do_trace_arange True Test trace torch full x shape doesn t store shape constant test_trace_full_dynamic_shape full_with_shape_like x torch full x shape x = torch randn ge = torch jit trace full_with_shape_like example_inputs=x y = torch randn assertEqual ge y shape y shape assertEqual ge x shape x shape Test trace setitem doesn t store shapes constants Fix https github com pytorch pytorch issues test_trace_slice_setitem_dynamic_shape slice_setitem x y x = y + x x = torch randn traced = torch jit trace slice_setitem x x x = torch randn assertEqual traced x clone x slice_setitem x clone x Suppression we intentionally slicing tensor we don t care will constantified suppress_warnings do_trace_slice requires_grad slice x results = i range results append x x size - i i x size i tuple results slice_select x results = i range results append x i x size - tuple results x = torch randn requires_grad=requires_grad y = torch randn requires_grad=requires_grad Check behaves expected traced_slice = torch jit trace slice x assertEqual traced_slice y slice y assertEqual traced_slice x slice x traced_slice_select = torch jit trace slice_select x assertEqual traced_slice_select y slice_select y assertEqual traced_slice_select x slice_select x test_trace_slice do_trace_slice False test different graph_executor path happens when gradients required sizes involved test_trace_slice_with_grad do_trace_slice True test_trace_casts casts = lambda x x byte lambda x x float lambda x x cpu lambda x x device= cpu lambda x x dtype=torch int lambda x x device= cpu dtype=torch float lambda x x x assertContainsCast trace assertEqual sum n kind == aten n trace graph nodes cast casts trace = torch jit trace cast torch randn assertContainsCast trace x = torch randn assertEqual trace x cast x to_tensor x y x y to_tensor_trace = torch jit trace to_tensor torch randn torch randn assertContainsCast to_tensor_trace x y = torch randn torch randn assertEqual to_tensor_trace x y to_tensor x y skipIfCompiledWithoutNumpy skipIfCrossRef test_trace_warn fn x int x Warning y = x y Warning pass q = x x z = q y float z Warning z tolist Warning z numpy Warning _ torch ones Warning pass z + warnings catch_warnings record=True warns traced_fn = torch jit trace fn torch tensor warn warns assertIs warn category torch jit TracerWarning warns = str w message w warns assertIn Python integer warns assertIn Python boolean warns assertIn Python float warns assertIn Python list warns assertIn NumPy array warns assertIn Iterating over warns test_trace_tuple fn x y x x y x y x y = torch randn torch ones torch randn traced_fn = torch jit trace fn x y assertEqual traced_fn x y fn x y should tuple nested within another tuple FileCheck check_count prim TupleConstruct exactly=True check_next run str traced_fn graph assertExportImport traced_fn graph x y test_trace_random f mean std torch normal mean std traced = torch jit trace f torch zeros torch ones check_trace=False mean std = torch zeros torch ones torch random fork_rng devices= output = f mean std traced_output = traced mean std assertEqual output traced_output test_trace_tensor_factory run kwargs inputs_require_grads = kwargs pop inputs_require_grads True fn x x + torch ones kwargs input_kwargs = kwargs copy out input_kwargs del input_kwargs out input = torch ones input_kwargs checkTrace fn input inputs_require_grads=inputs_require_grads check we recorded ones did just record constant tfn = torch jit trace fn input assertTrue ones str tfn graph run run dtype=torch int inputs_require_grads=False run out=torch tensor RUN_CUDA run device= cuda RUN_CUDA_MULTI_GPU run device= cuda test_trace_indexed_assignment stuff x y x = x clone x = y x example = torch rand checkTrace stuff example example + TODO implement unittest expectedFailure test_output_unflatten Check outputs traced functions retain original structure nesting fn x x x x + x + x checkTrace fn torch randn test_input_flatten Check inputs traced functions flattened fn x t y z = t x y z inputs = torch randn torch randn torch randn checkTrace fn inputs test_input_dict_empty test d pass assertRaises RuntimeError checkTrace test test_input_dict_remembers_keys Check trace remembers which keys dict input TestModule torch nn Module forward dict_input dict_input x input_ = x torch tensor m = TestModule m_traced = torch jit trace m input_ assertEqual m_traced input_ torch tensor should work change values keys input_same_key_different_value = x torch tensor assertEqual m_traced input_same_key_different_value torch tensor error use something doesn t have ` x ` input_different_key = y torch tensor assertRaises RuntimeError m_traced input_different_key s okay have additional elements dictionary so long x there input_additional_key = x torch tensor y torch tensor assertEqual m_traced input_additional_key torch tensor test_input_dict_insertion_order Check dictionary access doesn t care about insertion order TestModule torch nn Module forward dict_input dict_input x dict_input y input_x_then_y = input_x_then_y x = torch tensor input_x_then_y y = torch tensor m = TestModule m_traced = torch jit trace m input_x_then_y assertEqual m_traced input_x_then_y torch tensor torch tensor input_y_then_x = input_y_then_x y = torch tensor input_y_then_x x = torch tensor assertEqual m_traced input_y_then_x torch tensor torch tensor test_input_dict_recursive TestModule torch nn Module forward dict_input dict_input x input_ = x torch tensor m = TestModule m_traced = torch jit trace m input_ input_ = x torch tensor assertEqual m_traced input_ torch tensor test_input_dict_checkTrace_mut test d d x tanh_ d x inputs = x torch rand y torch rand checkTrace test inputs inputs_require_grads=False test_input_dict_unify test d d int d float inputs = int torch ones dtype=torch int float torch ones dtype=torch float checkTrace test inputs inputs_require_grads=False test_input_tuple_of_dicts test t d = t d x y inputs = x y torch rand checkTrace test inputs inputs allow_unused=True test_input_dict_of_dicts test d d x y nested_input = y torch rand unified_nested = y torch rand inputs = x nested_input force_unify unified_nested checkTrace test inputs allow_unused=True test_input_dict_of_lists test d d x inputs = x torch rand checkTrace test inputs test_input_list_toplevel_flatten test t t torch add t t inputs = torch ones torch rand checkTrace test inputs test_input_list_toplevel_flatten_direct Test torch nn Module forward t t torch add t t inputs = torch ones torch rand torch jit trace Test inputs test_input_list_of_tuples test l l inputs = torch ones checkTrace test inputs test_input_dict_empty_list test d pass inputs = assertRaisesRegex RuntimeError List trace checkTrace test inputs test_input_list_mixed_type test d pass inputs = torch rand torch ones torch ones assertRaisesRegex RuntimeError consistent checkTrace test inputs test_conv x = torch ones g outputs inputs = torch jit _get_trace_graph nn Conv d bias=False x return_inputs=True m = createFunctionFromGraph g assertEqual outputs m inputs test_max_pool x = torch rand max_pool d x F max_pool d x + trace = torch jit trace max_pool d x graph = trace graph_for x FileCheck check aten max_pool d run graph assertEqual max_pool d x trace x test_nested_inplace x = torch randn g outputs inputs = torch jit _get_trace_graph lambda x F threshold x inplace=True x return_inputs=True m = createFunctionFromGraph g assertEqual outputs m inputs FileCheck check threshold_ run str g assertExportImport g x test_repeated_input fn b + b ge = checkTrace fn torch randn inputs = set ge graph inputs three instead because export checkTrace adds ` ` module argument assertTrue len inputs == test_repeated_output fn b z = + b z z ge = checkTrace fn torch randn _ range tuple_output = list ge graph outputs tuple_inputs = list tuple_output node inputs assertTrue tuple_inputs == tuple_inputs test_inplace_copy x = torch randn requires_grad=True f x out = torch zeros x size out copy_ x out g outputs inputs = torch jit _get_trace_graph f x return_inputs=True run_pass dce g m = createFunctionFromGraph g assertEqual outputs m inputs assertExportImport g x test_inplace_copy_force_outplace x = torch randn requires_grad=True f x out = torch zeros x size out copy_ x out g outputs inputs = torch jit _get_trace_graph f x return_inputs=True _force_outplace=True run_pass dce g m = createFunctionFromGraph g assertEqual outputs m inputs assertExportImport g x FileCheck check expand_as run str g test_shared_param MyModule torch nn Module __init__ - None super __init__ b = = nn Parameter torch randn forward x x + b m = MyModule g _ = torch jit _get_trace_graph m torch randn run_pass dce g assertEqual len list g inputs FileCheck check mul check add run str g run_ge_tests optimize use_cuda enable_profiling_mode_for_profiling_tests torch jit optimized_execution optimize rand args t = torch rand args float use_cuda t = t cuda t checkTrace lambda b b + b rand rand rand rand trivial identity checkTrace lambda b b rand rand foo t = t t t checkTrace foo rand unused input checkTrace lambda b rand rand allow_unused=True test outputs do get used grad checkTrace foo rand drop= test autograd fallback checkTrace lambda b b - b + b rand rand test_ge_unoptimized run_ge_tests False False unittest skipIf IS_SANDCASTLE NYI fuser support Sandcastle enable_cpu_fuser test_ge_optimized enable_profiling_mode_for_profiling_tests run_ge_tests True False unittest skipIf RUN_CUDA requires CUDA test_ge_cuda run_ge_tests True True more manual test graph executor can used scratchpad test_ge foo b b - b + b V = Variable b = V torch rand V torch rand ge = torch jit trace foo b b = V torch rand requires_grad=True V torch rand requires_grad=True r = ge b da db = torch autograd grad r + b create_graph=True l = da db + db db g result = torch autograd grad l da db r = foo b da db = torch autograd grad r + b create_graph=True assertEqual da da assertEqual db db l = da db + db db g result = torch autograd grad l da db assertEqual g result g result test_trace_annotation _trace torch rand foo + + x = torch randn assertEqual foo x x + x + x unittest skipIf RUN_CUDA calls cuda By default Ampere later GPUs nn Linear computes float tensors TF precision We want float tensors computed full precision order use default precision with_tf _off test_traced_module_cuda Model nn Module __init__ num_features num_layers super __init__ num_layers = num_layers layers = nn Linear num_features num_features nn Sigmoid _ range num_layers submodule = nn Sequential chain layers forward x i range num_layers x = submodule i x + x x model = Model x = torch randn traced_model = torch jit trace model x We re missing some attributes these modules had initially Make sure we can still get __repr__ model __repr__ XXX indexing sequentials broken linear_submodule = next iter traced_model submodule _modules values All attributes aren t parameters should raise assertRaises AttributeError linear_submodule in_features linear_submodule weight linear_submodule weight = nn Parameter torch randn linear_submodule weight shape assertRaises RuntimeError del linear_submodule weight Submodules can t called assertRaises RuntimeError linear_submodule x Type casts linear_submodule cuda traced_model float cuda cuda_out = traced_model x float cuda traced_model cpu cpu_out = traced_model x float assertEqual cpu_out cuda_out traced_model cuda cuda_out = traced_model x float cuda traced_model cpu cpu_out = traced_model x float assertEqual cpu_out cuda_out traced_model torch get_default_dtype state_dict + load_state_dict state = k v clone k v traced_model state_dict items new_state = k v clone fill_ k v state items out = traced_model x traced_model load_state_dict new_state out_ones = traced_model x traced_model load_state_dict state out_state = traced_model x assertEqual out out_state assertNotEqual out out_ones unittest skipIf RUN_CUDA uses cuda test_type_same_device Model torch nn Module __init__ - None super __init__ dtype = torch float forward x=None h = x type dtype h = Model b = torch jit trace example_inputs= torch ones device=torch device cuda FileCheck check_not device run b code test_export_no_reorder func b b - b + b recording_inputs = torch tensor dtype=torch float requires_grad=True torch tensor dtype=torch float requires_grad=True ge = torch jit trace func recording_inputs ge = getExportImportCopy ge outputs_ge = ge recording_inputs outputs_ge = ge recording_inputs grad_ge = torch autograd grad outputs_ge recording_inputs grad_ge = torch autograd grad outputs_ge recording_inputs assertTrue outputs_ge == outputs_ge assertTrue grad_ge == grad_ge test_python_function MyFn Function staticmethod forward ctx x x + staticmethod backward ctx grad_output grad_output _trace torch zeros fn x MyFn apply x + + x = torch tensor y = torch randn requires_grad=True fn x fn y test_python_function_tup MyFn Function staticmethod forward ctx x x + x - staticmethod backward ctx grad_output grad_output grad_output _trace torch zeros fn x b = MyFn apply x + + b + x = torch tensor y = torch randn requires_grad=True fn x fn y test_trace_detach foo x w torch matmul x w detach traced = torch jit trace foo torch rand torch rand FileCheck check matmul check detach run str traced graph x w = torch rand torch rand requires_grad=True traced_result = traced x w assertEqual foo x w traced_result assertFalse traced_result requires_grad assertIsNone traced_result grad_fn test_trace_detach_redispatch foo x w y = torch matmul x w assert y requires_grad y = y detach Make sure trace kernel redispatches right lower kernel assert y requires_grad y x w = torch rand torch rand requires_grad=True With ` check_trace=True ` will run ` torch no_grad ` break assert torch jit trace foo x w check_trace=False test_trace_detach_inplace foo x w y = torch matmul x w y detach_ y traced = torch jit trace foo torch rand torch rand FileCheck check matmul check detach run str traced graph x w = torch rand torch rand requires_grad=True traced_result = traced x w assertEqual foo x w traced_result assertFalse traced_result requires_grad assertIsNone traced_result grad_fn test_trace_detach_inplace_redispatch foo x w y = torch matmul x w assert y requires_grad y detach_ Make sure trace kernel redispatches right lower kernel assert y requires_grad y x w = torch rand torch rand requires_grad=True With ` check_trace=True ` will run ` torch no_grad ` break assert torch jit trace foo x w check_trace=False test_trace_slice_full_dim foo x x + traced = torch jit trace foo torch rand test_x = torch rand assertEqual foo test_x traced test_x test_trace_dict_input Bar torch nn Module __init__ - None super __init__ foo = Foo forward b foo b b Foo torch nn Module forward x x x b x = torch rand torch rand model = Bar checkTrace model x test_trace_dict_output TraceDictStrTensor torch nn Module forward b b b TraceDictTensorTensor torch nn Module forward b b b x = torch rand torch rand assertRaisesRegex RuntimeError r Encountering dict output torch jit trace TraceDictStrTensor x traced_dict_str_mod = torch jit trace TraceDictStrTensor x strict=False assertEqual traced_dict_str_mod x x b x traced_dict_tensor_mod = torch jit trace TraceDictTensorTensor x strict=False assertEqual traced_dict_tensor_mod x x x x x test_trace_with_tensor_list_output f torch zeros torch zeros assertWarnsRegex torch jit TracerWarning cause trace incorrect torch jit trace f traced_non_strict_f = torch jit trace f strict=False assertEqual traced_non_strict_f f test_trace_with_number_list_output f assertRaisesRegex RuntimeError r Only tensors +can output traced functions traced_f = torch jit trace f test_trace_with_nested_tensor_list_output f torch zeros torch zeros assertRaisesRegex RuntimeError r Only tensors +can output traced functions traced_f = torch jit trace f test_trace_with_nested_strided_tensor_output torch jit script nt_construct values kv_lengths kv_lengths_list List int = kv_lengths tolist torch _nested_tensor_from_tensor_list list values split kv_lengths_list dim= None None None None f x offsets kv_lengths = offsets - offsets - nt_construct x kv_lengths cos x = torch rand offsets = torch tensor ref = f x offsets f_t = torch jit trace f x offsets res = f_t x offsets assertEqual ref res x = torch rand offsets = torch tensor assertEqual f x offsets f_t x offsets test_trace_variable_instantiation random_foo x Variable Variable x + random_foo_traced = torch jit trace random_foo torch rand x = torch rand assertEqual random_foo x random_foo_traced x test_trace_slice_expr_complete_type random_foo x x + random_foo_traced = torch jit trace random_foo torch rand torch jit script random_bar x random_foo_traced x x = torch rand assertEqual random_bar x x + test_trace_inline_shape testing peephole optimization size turned into constant script fn torch jit script tensor_size x torch Tensor - torch Tensor torch tensor x size assertEqual tensor_size torch rand torch tensor traced_tensor_size = torch jit trace tensor_size torch rand assertEqual traced_tensor_size torch rand torch tensor torch jit script use_device x torch zeros_like x device=x device foo x use_device x traced_tensor_size = torch jit trace foo torch rand run_pass inline traced_tensor_size graph FileCheck check prim device run traced_tensor_size graph test_trace_save fn x x + check func TemporaryFileName fname func save fname loaded = torch jit load fname input = torch randn assertEqual func input loaded input out = torch jit trace fn torch ones check out test_trace_optioanl_dtype Test torch nn Module forward torch arange traced = torch jit trace Test torch allclose traced Test test_trace_save_load_copy Test torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x conv x traced = torch jit trace Test torch rand buffer = io BytesIO torch jit save traced buffer buffer seek loaded = torch jit load buffer should work copy copy loaded copy deepcopy loaded test_trace_export_fns Foo torch nn Module __init__ - None super __init__ = torch jit export __getstate__ training torch jit export __setstate__ state = state training = state forward x x + f = Foo traced = torch jit trace f torch rand expected_names = __getstate__ __setstate__ check mod assertTrue all name mod _c _method_names name expected_names check traced imported = getExportImportCopy traced check imported test_trace_export_fns_recursive Foo torch nn Module __init__ - None super __init__ = torch jit export __getstate__ training torch jit export __setstate__ state = state training = state forward x x + Wrapper torch nn Module __init__ - None super __init__ foo = Foo forward x foo x f = Wrapper traced = torch jit trace f torch rand expected_names = __getstate__ __setstate__ check mod assertTrue all name mod _c _method_names name expected_names check traced foo imported = getExportImportCopy traced check imported foo Note Bar s forward can only traced scripted Bar nn Module torch jit export addTwo x x + forward input lambda + input noqa PLC When tracing Bar submodule we only want script exported methods we want keep forwards still being traced WrapperExports torch nn Module __init__ - None super __init__ bar = Bar torch jit export addOne x x + forward x bar x f = WrapperExports traced = torch jit trace f torch rand expected_names = addOne check traced test_trace_autograd_function TestFunc torch autograd Function staticmethod forward ctx input torch neg input staticmethod backward ctx grad_output torch neg grad_output TracedModule torch nn Module forward x torch relu TestFunc apply x Wrapper torch nn Module __init__ - None super __init__ tm = TracedModule forward x tm x traced = torch jit trace Wrapper torch rand test_trace_multi_output_function An autograd Function two outputs It swaps inputs so we can check shape handling correct TorchScript Foo torch autograd Function staticmethod forward ctx x y y x staticmethod backward ctx du dv dv du Bar torch nn Module forward x y x = x relu y = y relu z = Foo apply x y z x = torch rand dtype=torch double y = torch rand dtype=torch double Generate JIT IR traced = torch jit trace Bar x y print traced graph Expected output schema custom autograd Function schema = Double strides= requires_grad= device=cpu Double strides= requires_grad= device=cpu = ^Foo See expected schema exists FileCheck check schema run traced graph Also examine graph runnable produces right result u v = traced x y assertEqual u y assertEqual v x test_interpolate_trace test nn Module __init__ - None super __init__ conv = nn Conv d kernel_size= padding= forward x y = conv x w = nn functional interpolate y mode= bilinear align_corners=False scale_factor= w f = test no failure g = torch jit trace f torch zeros x = torch zeros constants baked assertEqual g x f x _tmp_donotuse_dont_inline_everything test_trace_optional torch jit script test x Optional Tensor x None torch zeros x test_none test None test_tensor test torch zeros f_none = torch jit trace test_none assertEqual f_none torch zeros f_tensor = torch jit trace test_tensor assertEqual f_tensor torch zeros graph = f_tensor graph FileCheck check name= test check_next prim CallFunction run graph test_trace_nested_datatypes torch jit script foo x x + x - x + x - bar x list_stuff = foo x list_stuff list_stuff traced = torch jit trace bar torch rand x = torch rand assertEqual bar x traced x _tmp_donotuse_dont_inline_everything test_call_traced_fn_from_traced_module _trace torch rand traced_fn x torch neg x TracedModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand forward x traced_fn torch mm x param tm = torch jit trace TracedModule torch rand Note neg op traced function should properly inlined FileCheck check aten mm check name= traced_fn check_next prim CallFunction run str tm graph _tmp_donotuse_dont_inline_everything test_call_traced_module_from_traced_module TracedModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand forward x torch mm x param TracedModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand mod = torch jit trace TracedModule torch rand forward x mod torch mm x param + tm = torch jit trace TracedModule torch rand FileCheck check aten mm check prim CallMethod check_same forward check aten add run str tm graph test_index_put_trace_with_view _trace torch rand torch tensor torch rand test_index_put target indices rhs target indices = rhs target FileCheck check aten view check index_put_ run str test_index_put graph test_index_put_trace_without_view _trace torch rand torch tensor torch rand test_index_put target indices rhs target indices = rhs target FileCheck check_not aten view check index_put_ run str test_index_put graph suppress_warnings test_trace_checker_dot_data assertRaisesRegex torch jit TracingCheckError r Tensor-valued Constant nodes differed value across invocations _trace torch rand check_inputs= torch rand foo x y = x data x + y suppress_warnings test_trace_checker_control_flow foo x _ range x size x = torch neg x x assertRaisesRegex torch jit TracingCheckError r Graphs differed across invocations torch jit trace foo torch randn check_inputs= torch randn suppress_warnings test_trace_checker_memoization assertRaisesRegex torch jit TracingCheckError r Graphs differed across invocations foo x hasattr foo cache foo cache = torch neg x x + foo cache traced = torch jit trace foo torch rand check_inputs= torch rand test_trace_checker_slice_lhs foo x i range x i = torch zeros x checkTrace foo torch rand inputs_require_grads=False test_trace_checker_inplace_on_view foo x x view - add_ -x view - x assertWarnsRegex torch jit TracerWarning Output nr traced function does match corresponding output Python function torch jit trace foo torch rand check_inputs= torch rand _force_outplace=True test_lhs_index_fails foo x x = x assertWarnsRegex torch jit TracerWarning cause trace incorrect torch jit trace foo torch rand _force_outplace=True test_lhs_index_trivial foo y x y = x y checkTrace foo torch rand torch rand inputs_require_grads=False test_inplace_warn foo x x view - add_ -x view - x assertWarnsRegex torch jit TracerWarning cause trace incorrect torch jit trace foo torch rand _force_outplace=True suppress_warnings test_trace_checker_dropout_train foo x torch dropout x p= train=True assertWarnsRegex torch jit TracerWarning Output nr traced function does match corresponding output Python function torch jit trace foo torch rand check_inputs= torch rand assertWarnsRegex torch jit TracerWarning Trace had nondeterministic nodes torch jit trace foo torch rand check_inputs= torch rand test_trace_checker_dropout_notrain input = torch rand _trace input foo x torch dropout x p= train=False assertEqual foo input input test_trace_contiguous foo x x contiguous view x = torch rand traced = torch jit trace foo x y = traced x assertNotEqual x storage data_ptr y storage data_ptr This tests logic THPVariable_contiguous There short-circuiting code prevents us even getting VariableType contiguous since optimization prevents us acquiring GIL touching device We needed add tracing logic directly into THPVariable_contiguous function only path where we skipping dispatch into contiguous We should see aten contiguous trace test_trace_contiguous_short_circuit foo x x contiguous x = torch rand traced = torch jit trace foo x FileCheck check aten contiguous run str traced graph test_trace_inverse foo x ~x foo_traced = torch jit trace foo torch zeros dtype=torch uint eg = torch zeros dtype=torch uint assertEqual foo_traced eg foo eg test_trace_modulelist MySubmod torch nn Module __init__ - None super __init__ relu = torch nn ReLU forward x relu x MyMod torch nn Module __init__ - None super __init__ ml = torch nn ModuleList MySubmod MySubmod forward x mod ml x = mod x x traced = torch jit trace MyMod torch rand test_trace_fork_join_and_module MySubmod torch nn Module __init__ - None super __init__ relu = torch nn ReLU forward x relu x torch neg x Mod torch nn Module __init__ - None super __init__ ml = torch nn ModuleList MySubmod i range forward x futs = i range futs append torch jit _fork ml i x results = i range results append torch jit _wait futs i torch stack results m = Mod traced = torch jit trace m torch rand test_trace_invert_module_hierarchy MySubmod torch nn Module __init__ - None super __init__ relu = torch nn ReLU forward x relu x torch neg x MyFunctionalMod torch nn Module forward x submod submod x Mod torch nn Module __init__ - None super __init__ sm = MySubmod fm = MyFunctionalMod forward x fm x sm torch jit trace Mod torch rand skipIfCrossRef test_trace_records_names foo bar baz baz = bar + quick_brown_fox = torch neg baz _ range yeet = quick_brown_fox - yeet traced = torch jit trace foo torch rand torch rand graph_str = str traced graph assert bar graph_str assert baz graph_str assert quick_brown_fox graph_str skipIfTorchDynamo Not suitable test TorchDynamo test_tracing_hooks Net nn Module forward x x + x test_hook is_post_hook hook fc n = Net is_post_hook n register_forward_hook hook n register_forward_pre_hook hook module = torch jit trace n torch tensor eager_input = torch tensor eager_out = n eager_input fc run module forward graph input = torch tensor output = module input assertEqual input eager_input assertEqual output eager_out hook_no_return mod input output input add_ output sub_ fc = FileCheck check add check add_ check sub_ test_hook True hook_no_return fc hook_return mod input output input add_ output - fc = FileCheck check add check add_ check sub test_hook True hook_return fc b = torch tensor captured_hook mod input output output - b fc = FileCheck check add check sub test_hook True captured_hook fc pre_hook_no_ret mod input input add_ fc = FileCheck check add_ check add test_hook False pre_hook_no_ret fc pre_hook_ret mod input input - fc = FileCheck check sub check add test_hook False pre_hook_ret fc test_tracing_backward_hook_error Net nn Module forward x x + x n = Net backward_hook module grad_input grad_output pass n register_backward_hook backward_hook assertRaisesRegex Exception backward hooks assigned torch jit trace n torch tensor test_tracing_multiple_methods Net nn Module __init__ - None super __init__ conv = nn Conv d forward x conv x weighted_kernel_sum weight weight conv weight example_weight = torch rand example_forward_input = torch rand inputs = forward example_forward_input weighted_kernel_sum example_weight n = Net module = torch jit trace_module n inputs check_inputs = _ range check_weight = torch rand check_forward_input = torch rand check_inputs append forward check_forward_input weighted_kernel_sum check_weight module = torch jit trace_module n inputs check_trace=True check_inputs=check_inputs assertTrue module _c _has_method forward assertTrue module _c _has_method weighted_kernel_sum module = torch jit trace n forward example_forward_input module = torch jit trace n forward example_forward_input check_trace=True check_inputs= example_forward_input assertRaisesRegex AttributeError trace doesn t support compiling individual module s functions module = torch jit trace n weighted_kernel_sum inputs test_tensor_with_grad_as_constant param = torch randn requires_grad_ x = torch randn f x x + param assertRaisesRegex RuntimeError Cannot insert Tensor requires grad constant torch jit trace f x test_non_tensor_tracing f x x + param noqa F assertRaisesRegex RuntimeError r Type Tuple\ int\ cannot traced torch jit trace f test_trace_skip_none_submodule TestModule torch nn Module __init__ - None super __init__ submod = torch nn Linear submod = None forward inputs inputs m = TestModule tm = torch jit trace m torch tensor assertFalse hasattr tm submod test_trace_with_conditional_property Net nn Module __init__ attr=None super __init__ attr None _attr = attr attr_name = _attr property attr getattr attr_name forward x x x = torch ones torch jit trace Net x test_trace_func_argument_names_captured fn first_arg torch Tensor second_arg torch Tensor - torch Tensor first_arg + second_arg traced_fn = torch jit trace fn torch ones torch ones FileCheck check first_arg check_next second_arg run str traced_fn graph test_trace_partial_func_argument_names_captured fn first_arg torch Tensor second_arg= - torch Tensor first_arg + second_arg traced_fn = torch jit trace fn torch ones FileCheck check first_arg check_not second_arg run str traced_fn graph test_trace_module_argument_names_captured TestModule nn Module __init__ - None super __init__ conv = nn Conv d forward first_arg torch Tensor second_arg torch Tensor conv first_arg + second_arg m = TestModule example_input = torch ones torch ones Explicitly tracing module s forward method traced_module_forward = torch jit trace m forward example_input FileCheck check first_arg check_next second_arg run str traced_module_forward graph Tracing module s directly traced_module = torch jit trace m example_input FileCheck check first_arg check_next second_arg run str traced_module graph test_trace_checking_with_deprecated_name MyClass torch nn Module __init__ - None super MyClass __init__ forward x y deprecated_arguments len deprecated_arguments raise RuntimeError f Got unexpected arguments deprecated_arguments x + y model = MyClass m = torch jit trace model torch ones torch ones m = torch jit trace model example_kwarg_inputs= x torch ones y torch ones strict=False test_trace_with_tuple_tensor MyClass torch nn Module __init__ - None super MyClass __init__ forward x y x + y + y model = MyClass traced_model = torch jit trace model torch ones torch ones torch ones input_dict = x torch tensor y torch tensor torch tensor assertEqual model input_dict traced_model input_dict traced_model = torch jit trace model example_kwarg_inputs= x torch ones y torch ones torch ones assertEqual model input_dict traced_model input_dict test_trace_no_duplicated_lifted_input_output Normalize nn Module __init__ - None super __init__ norm = nn GroupNorm num_groups= num_channels= forward x y y None y = x y = norm y y = y y G nn Module __init__ - None super __init__ norm = Normalize forward x A = norm x None B = F relu A A B Net nn Module __init__ - None super __init__ g = G norm_ = Normalize forward x hs = g x A B = hs h = norm_ B A h net = Net net = net eval x = torch randn traced = torch jit trace net x FileCheck check_not prim TupleUnpack run str traced graph skipIfTorchDynamo Not suitable test TorchDynamo TestMixTracingScripting JitTestCase test_trace_script torch jit script func x Tuple Tensor Tensor - Tensor x + x torch jit script func x List Tensor - Tensor x + x = torch randn b = torch randn checkTrace func b checkTrace func b torch jit script func x Tensor method str = bilinear align_corners bool = True - Tensor hw = x shape F interpolate x hw mode=method align_corners=align_corners inp = torch rand checkTrace func inp torch jit script func x Tensor List Optional str - Tensor len == x + x test_trace_mixed_by_script_with_dict_output torch jit script return_dict input torch Tensor - Dict str torch Tensor foo input + TraceModule torch nn Module forward input dict = return_dict input dict foo + dict foo x = torch ones tm = torch jit trace TraceModule x assertEqual tm x x + + x + test_trace_of_script torch jit script foo c b = bool == b = b + c = torch ones dtype=torch float _trace torch zeros dtype=torch float use b foo b - + test we propagated shapes through function assertTrue Dynamic str use graph assertEqual use torch ones dtype=torch float assertEqual use torch zeros dtype=torch float test_trace_with_size _trace torch zeros foo x x + torch jit script bar x y = int foo x == y = y + assertEqual bar torch ones test_tracing_slicing _trace torch zeros foo_trace x x - - torch jit script foo_script x x - - foo x x - - = torch arange b = torch arange assertEqual foo_trace foo_script assertEqual foo_trace foo assertNotEqual foo_trace foo_trace b test_tracing_indexing _trace torch zeros foo_trace x x - torch jit script foo_script x x - foo x x - = torch arange b = torch arange assertEqual foo_script foo_trace assertEqual foo_trace foo assertNotEqual foo_trace foo_trace b test_trace_hierarchy Test we preserve module hierarchy ScriptModule submodule during tracing AnotherScriptMod torch jit ScriptModule __init__ - None super __init__ param = torch nn Parameter torch rand torch jit script_method bar torch zeros SomeScriptMod torch jit ScriptModule __init__ - None super __init__ asm = AnotherScriptMod torch jit script_method foo torch zeros torch jit script_method bar torch zeros TraceMe torch nn Module __init__ - None super __init__ ssm = SomeScriptMod forward x ssm bar + x orig = TraceMe traced = torch jit trace orig torch rand each these checks check BOTH underlying _C ScriptModule object has expected method param well Python object wraps assertTrue traced ssm _c _has_method foo assertTrue hasattr traced ssm foo imported = getExportImportCopy traced assertTrue imported ssm _c _has_method foo assertTrue hasattr imported ssm foo assertTrue imported ssm asm _c _has_method bar assertTrue hasattr imported ssm asm bar assertTrue hasattr imported ssm asm param test_trace_parameter Param nn Module __init__ - None super __init__ register_parameter bias nn Parameter torch empty forward x x M torch jit ScriptModule __init__ model super __init__ traced = torch jit trace model torch rand torch jit script_method forward x traced x M nn Module __init__ model super __init__ module = M model forward x module x M torch jit ScriptModule __init__ model super __init__ traced = torch jit trace M model torch rand torch jit script_method forward x traced x torch jit optimized_execution False module = M Param f = io BytesIO torch jit save module f _tmp_donotuse_dont_inline_everything test_call_script_fn_from_traced_module torch jit script scripted_fn x torch neg x TracedModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand forward x scripted_fn torch mm x param tm = torch jit trace TracedModule torch rand FileCheck check aten mm check name= scripted_fn check prim CallFunction run str tm graph _tmp_donotuse_dont_inline_everything test_call_script_module_from_traced_module ScriptMod torch jit ScriptModule __init__ - None super __init__ param_foo = torch nn Parameter torch rand torch jit script_method forward x torch mm x param_foo TracedModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand mod = ScriptMod forward x mod torch mm x param + tm = torch jit trace TracedModule torch rand FileCheck check aten mm check prim CallMethod check_same forward check aten add run str tm graph _tmp_donotuse_dont_inline_everything test_call_traced_fn_from_script_fn _trace torch rand traced_fn x torch neg x torch jit script script_fn x traced_fn x + FileCheck check prim CallFunction check aten add run str script_fn graph test_call_traced_mod_from_script_fn assertRaisesRegex RuntimeError Cannot call ScriptModule submodule caller TracedModule torch nn Module forward x torch mm x torch zeros tm = torch jit trace TracedModule torch rand torch jit script script_fn x tm x + _tmp_donotuse_dont_inline_everything test_call_tracing_fn_from_script_module _trace torch rand traced_fn x torch neg x ScriptMod torch jit ScriptModule __init__ - None super __init__ param = torch nn Parameter torch rand torch jit script_method forward x traced_fn torch mm x param sm = ScriptMod FileCheck check aten mm check prim CallFunction run str sm forward graph _tmp_donotuse_dont_inline_everything test_call_tracing_mod_from_script_module TracedMod torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand forward x torch mm x param ScriptMod torch jit ScriptModule __init__ - None super __init__ param = torch nn Parameter torch rand tm = torch jit trace TracedMod torch rand torch jit script_method forward x tm torch mm x param sm = ScriptMod FileCheck check aten mm check prim CallMethod run str sm graph test_script_inline_trace_multiple_args M torch nn Module forward input input input + input M torch jit ScriptModule __init__ - None super __init__ m = torch jit trace M torch zeros torch zeros torch jit script_method forward inp m inp inp torch jit optimized_execution False m = M m torch zeros test_trace_dict_mix_script testB torch nn Module __init__ - None super __init__ linear = torch nn Linear forward feature_map Dict str List Tensor - Tensor output = j feature_map values output append linear j torch stack output testA torch nn Module __init__ - None super __init__ b = torch jit script testB forward input_map Dict str List Tensor - Tensor feature_map = i j input_map items feature_map i = j b feature_map input_map = torch rand torch rand torch rand torch rand model = testA traced_model = torch jit trace model input_map new_input_map = torch rand torch randn torch rand torch rand assertEqual model new_input_map traced_model new_input_map test_trace_script_returning_complex_dict Tracing over script function returning dictionary should work The dictionary can should able contain other containers like tuple recursively ReturnsDict torch nn Module forward id_score_list Dict str Tuple torch Tensor torch Tensor torch Tensor - Dict str Tuple torch Tensor torch Tensor torch Tensor do some random operations then dict same structure v = id_score_list idx_keys = v - weights = v result = v idx_keys weights result ChecksDict torch nn Module forward input Dict str Tuple torch Tensor torch Tensor torch Tensor v = input v + TestModule torch nn Module __init__ checks_dict returns_dict super __init__ checks_dict = checks_dict returns_dict = returns_dict forward input Dict str Tuple torch Tensor torch Tensor torch Tensor foo = returns_dict input checks_dict foo input = torch tensor torch tensor dtype=torch int torch tensor input = torch tensor torch tensor dtype=torch int torch tensor checks_dict = torch jit script ChecksDict returns_dict = torch jit script ReturnsDict eager_module = TestModule checks_dict returns_dict traced_module = torch jit trace eager_module input assertEqual traced_module input eager_module input assertEqual traced_module input eager_module input test_trace_returning_dict_with_tensor_tuples Tracing over module returning dictionary whose values tuples tensors should work ReturnsDict torch nn Module forward k torch Tensor v torch Tensor - Dict str Tuple torch Tensor torch Tensor x = k y = v result = imakey x y result ReturnsBadDict torch nn Module forward k torch Tensor v torch Tensor - Dict str Tuple torch Tensor float x = k result = imakey x result mod = ReturnsDict traced_module = torch jit trace mod torch ones torch ones strict=False out = traced_module torch ones torch ones expected = imakey torch tensor torch tensor assertEqual out expected assertRaisesRegex RuntimeError cannot understood tracer only outputs matching mod = ReturnsBadDict traced_module = torch jit trace mod torch ones torch ones strict=False test_trace_linear m = torch nn Linear inp = torch rand checkTrace m inp g = torch jit trace m inp graph FileCheck check aten linear run g test_traced_module_implements_interface torch jit interface TestModuleInterface nn Module forward first_arg torch Tensor second_arg torch Tensor - torch Tensor pass make_global TestModuleInterface TestModule nn Module __init__ - None super __init__ conv = nn Conv d forward first_arg torch Tensor second_arg torch Tensor - torch Tensor conv first_arg + second_arg fn_takes_interface x TestModuleInterface ones = torch ones x forward ones ones scripted_test_module = torch jit script TestModule checkScript fn_takes_interface scripted_test_module test_traced_module_contains_scripted_interface_types LeafModule torch nn Module __init__ - None super __init__ weight = torch nn Parameter torch rand forward input torch Tensor input + weight LowerModuleImpl torch nn Module __init__ - None super __init__ leaf = LeafModule forward input torch Tensor - torch Tensor leaf input torch jit interface LowerModuleInterface torch nn Module forward input torch Tensor - torch Tensor pass MiddleModule torch nn Module lower LowerModuleInterface __init__ feature_processor_modules=None super __init__ lower = LowerModuleImpl forward input lower input WrapperModule torch nn Module __init__ m super __init__ middle = m forward input middle input TopModule torch nn Module __init__ - None super __init__ m = MiddleModule m = torch jit script m sub = m sub = WrapperModule m forward input torch Tensor sub input + sub input top = TopModule top_example_input = torch ones torch jit trace top top_example_input test_jit_trace_callfunction_return_shapes torch jit script function gets inserted CallFunction node torch jit script inner_fn x torch cat x x outer_fn x y inner_fn x + y relu x y = torch rand dtype=torch float _ range fn_t = torch jit trace outer_fn x y expect CallFunction node type has shape information FileCheck check Float check check CallFunction run fn_t graph n fn_t graph nodes n kind == prim CallFunction assertTrue n output isCompleteTensor __name__ == __main__ raise_on_run_directly test test_jit py