mypy allow-untyped-defs Distributed Collective Communication c d collections abc contextlib ctypes hashlib io itertools logging os pickle sys time warnings collections namedtuple collections abc Callable datetime timedelta typing Any Optional TYPE_CHECKING Union typing_extensions deprecated torch torch _C _DistStoreError DistStoreError torch _C _distributed_c d _DistributedBackendOptions _register_process_group _resolve_process_group _unregister_all_process_groups _unregister_process_group AllgatherOptions AllreduceCoalescedOptions AllreduceOptions AllToAllOptions BarrierOptions BroadcastOptions DebugLevel GatherOptions get_debug_level PrefixStore ProcessGroup ReduceOp ReduceOptions ReduceScatterOptions ScatterOptions Store Work torch _utils_internal set_pytorch_distributed_envs_from_justknobs torch monitor _WaitCounter torch overrides handle_torch_function has_torch_function torch utils _typing_utils not_none c d_logger _exception_logger _time_logger constants default_pg_nccl_timeout default_pg_timeout rendezvous register_rendezvous_handler rendezvous noqa F __all__ = Backend BackendConfig GroupMember P POp all_gather all_gather_coalesced all_gather_object all_reduce all_reduce_coalesced all_to_all all_to_all_single barrier batch_isend_irecv broadcast send_object_list recv_object_list broadcast_object_list destroy_process_group gather gather_object get_backend_config get_backend get_default_backend_for_device get_rank get_world_size get_pg_count group init_process_group irecv is_gloo_available is_initialized is_mpi_available is_backend_available is_nccl_available is_torchelastic_launched is_ucc_available is_xccl_available isend monitored_barrier new_group new_subgroups new_subgroups_by_enumeration recv reduce reduce_scatter scatter scatter_object_list send supports_complex AllreduceCoalescedOptions AllreduceOptions AllToAllOptions BarrierOptions BroadcastOptions GatherOptions PrefixStore ProcessGroup ReduceOp ReduceOptions ReduceScatterOptions ScatterOptions Store DebugLevel get_debug_level Work default_pg_timeout get_group_rank get_global_rank get_process_group_ranks reduce_op all_gather_into_tensor reduce_scatter_tensor get_node_local_rank split_group _MPI_AVAILABLE = True _NCCL_AVAILABLE = True _GLOO_AVAILABLE = True _UCC_AVAILABLE = True _XCCL_AVAILABLE = True _pickler = pickle Pickler _unpickler = pickle Unpickler Change __module__ all imported types torch _C _distributed_c d public _export_c_types - None _public_types_to_change_module = AllreduceCoalescedOptions AllreduceOptions AllToAllOptions BarrierOptions BroadcastOptions GatherOptions PrefixStore ProcessGroup ReduceOp ReduceOptions ReduceScatterOptions ScatterOptions Store DebugLevel get_debug_level Work type _public_types_to_change_module type __module__ = torch distributed distributed_c d _export_c_types try torch _C _distributed_c d ProcessGroupMPI ProcessGroupMPI __module__ = torch distributed distributed_c d __all__ += ProcessGroupMPI except ImportError _MPI_AVAILABLE = False try torch _C _distributed_c d ProcessGroupNCCL ProcessGroupNCCL __module__ = torch distributed distributed_c d __all__ += ProcessGroupNCCL except ImportError _NCCL_AVAILABLE = False try torch _C _distributed_c d _ProcessGroupWrapper ProcessGroupGloo ProcessGroupGloo __module__ = torch distributed distributed_c d __all__ += ProcessGroupGloo except ImportError _GLOO_AVAILABLE = False try torch _C _distributed_c d ProcessGroupUCC ProcessGroupUCC __module__ = torch distributed distributed_c d __all__ += ProcessGroupUCC except ImportError _UCC_AVAILABLE = False try torch _C _distributed_c d ProcessGroupXCCL ProcessGroupXCCL __module__ = torch distributed distributed_c d __all__ += ProcessGroupXCCL except ImportError _XCCL_AVAILABLE = False logger = logging getLogger __name__ PG_WRAPPER_STORE_PREFIX = pg_wrapper Some reduce ops supported complex numbers will result error We currently provide complex support distributed API viewing complex tensors real torch view_as_real meaning calling these unsupported ops will garbage values rather than error out e g max + i + i = + i We d like calls unsupported ops error out accordingly rather than returning garbage values supports_complex reduceOp ReduceOp - bool Return true reduce ops supported False otherwise denyList = ReduceOp MAX ReduceOp MIN ReduceOp PRODUCT ReduceOp BAND ReduceOp BOR ReduceOp BXOR reduceOp denyList TODO refactor into enum strenum Backend str noqa SLOT An enum-like backends Available backends GLOO NCCL UCC MPI XCCL other registered backends The values lowercase strings e g ` ` gloo ` ` They can accessed attributes e g ` ` Backend NCCL ` ` This can directly called parse string e g ` ` Backend backend_str ` ` will check ` ` backend_str ` ` valid parsed lowercase string so It also accepts uppercase strings e g ` ` Backend GLOO ` ` returns ` ` gloo ` ` note The entry ` ` Backend UNDEFINED ` ` present only used initial value some fields Users should neither use directly nor assume its existence UNDEFINED = undefined GLOO = gloo NCCL = nccl UCC = ucc MPI = mpi XCCL = xccl _BackendPlugin = namedtuple _BackendPlugin creator_fn extended_api _plugins dict str _BackendPlugin = backend_list = UNDEFINED GLOO NCCL XCCL UCC MPI rd-party devices can register default backend support here default_device_backend_map dict str str = cpu GLOO cuda NCCL xpu XCCL mps GLOO backend_capability dict str list str = GLOO cpu cuda NCCL cuda XCCL xpu UCC cpu cuda MPI cpu cuda backend_type_map dict str ProcessGroup BackendType = UNDEFINED ProcessGroup BackendType UNDEFINED GLOO ProcessGroup BackendType GLOO NCCL ProcessGroup BackendType NCCL XCCL ProcessGroup BackendType XCCL UCC ProcessGroup BackendType UCC MPI ProcessGroup BackendType MPI __new__ cls name str Create new instance isinstance name str raise ValueError Backend constructor parameter must string-ish value = getattr Backend name upper Backend UNDEFINED value == Backend UNDEFINED value = name lower value classmethod register_backend cls name func extended_api=False devices Optional Union str list str = None - None Register new backend given name instantiating function This method used rd party ` ` ProcessGroup ` ` extension register new backends Args name str Backend name ` ` ProcessGroup ` ` extension It should match one ` ` init_process_group ` ` func function Function handler instantiates backend The function should implemented backend extension takes four arguments including ` ` store ` ` ` ` rank ` ` ` ` world_size ` ` ` ` timeout ` ` extended_api bool optional Whether backend supports extended argument structure Default ` ` False ` ` If set ` ` True ` ` backend will get instance ` ` c d DistributedBackendOptions ` ` process group options object defined backend implementation device str list str optional device type backend supports e g cpu cuda etc If ` None ` assuming both cpu cuda note This support rd party backend experimental subject change This takes care CUSTOM Out-of-tree backend types update backend_list indicates availability hasattr Backend name upper setattr Backend name upper name lower name lower Backend backend_list Backend backend_list append name lower devices None device devices device Backend default_device_backend_map Backend default_device_backend_map device = name lower Backend backend_type_map name lower = ProcessGroup BackendType CUSTOM Update device capability matrix Backend devices None This more backward support groups like ` threaded ` assume default devices cpu cuda warn warnings warn f Device capability name unspecified assuming ` cpu ` ` cuda ` ` xpu ` Please specify via ` devices ` argument ` register_backend ` stacklevel= Backend backend_capability name lower = cpu cuda xpu torch xpu is_available cpu cuda isinstance devices str Single device string specified Simply convert list Backend backend_capability name lower = devices Backend backend_capability name lower = devices Backend _plugins name upper = Backend _BackendPlugin func extended_api BackendConfig Backend configuration __init__ backend Backend Init device_backend_map dict str Backend = pyrefly ignore bad-assignment backend = str backend backend == Backend UNDEFINED Detect accelerator machine If no accelerator available returns CPU device_type = torch _C _get_accelerator type try backend_str = Backend default_device_backend_map device_type device_backend_map device_type = Backend backend_str except KeyError raise ValueError f We detected accelerator device_type your machine f But we don t know which communication backend use accelerator f Please specify ` backend ` argument ` init_process_group ` call None backend lower Backend backend_list Cases when backend single string without device types e g nccl gloo ucc mpi supported_devices = Backend backend_capability backend lower backend_val = Backend backend device_backend_map = dict fromkeys supported_devices backend_val backend lower Backend specified device backend format make sure backend string correct format device_type backend device_type backend e g cpu gloo cuda nccl backend_str_error_message = f The custom backend string argument invalid backend Custom backend string experimental feature where backend string must format device_type backend device_type backend e g cpu gloo cuda nccl parse backend string populate device_backend_map device_backend_pair_str backend lower split device_backend_pair = device_backend_pair_str split len device_backend_pair = raise ValueError f Invalid device backend pairing \ device_backend_pair_str backend_str_error_message pyrefly ignore bad-assignment device backend = device_backend_pair device device_backend_map raise ValueError f Duplicate device type device \ backend string backend backend_str_error_message device_backend_map device = Backend backend User specified single backend name whose device capability unknown assuming can support default devices PyTorch cpu cuda warnings warn f Device capability backend unknown assuming ` cpu ` ` cuda ` You can specify ` device backend ` format ` init_process_group ` call stacklevel= backend_val = Backend backend device_backend_map = cpu backend_val cuda backend_val xpu backend_val logger info Using backend config s device_backend_map __repr__ Return all device backend pairs separated commas join f device backend device backend device_backend_map items get_device_backend_map - dict str Backend Return backend map device device_backend_map _reduce_op r Deprecated enum-like For reduction operations ` ` SUM ` ` ` ` PRODUCT ` ` ` ` MIN ` ` ` ` MAX ` ` ` ~torch distributed ReduceOp ` recommended use instead __init__ - None __members__ dict storing key-value pairs enum classes k v ReduceOp RedOpType __members__ items setattr k v __members__ = ReduceOp RedOpType __members__ deprecated ` torch distributed reduce_op ` deprecated please use ` torch distributed ReduceOp ` instead category=FutureWarning __getattribute__ key object __getattribute__ key reduce_op = _reduce_op P POp A build point-to-point operations ` ` batch_isend_irecv ` ` This builds type P P operation communication buffer peer rank Process Group tag Instances will passed ` ` batch_isend_irecv ` ` point-to-point communications Args op Callable A function send data receive data peer process The type ` ` op ` ` either ` ` torch distributed isend ` ` ` ` torch distributed irecv ` ` tensor Tensor Tensor send receive peer int optional Destination source rank group ProcessGroup optional The process group work If None default process group will used tag int optional Tag match send recv group_peer int optional Destination source rank __init__ op Callable tensor torch Tensor peer Optional int = None group Optional ProcessGroup = None tag int = group_peer Optional int = None Init op = op tensor = tensor group = _group_or_default_group group peer = _canonicalize_group_rank group peer group_peer return_global=True tag = tag group_peer = _canonicalize_group_rank group peer group_peer __new__ cls op Callable tensor torch Tensor peer Optional int = None group Optional ProcessGroup = None tag int = group_peer Optional int = None Create new instance _check_op op _check_single_tensor tensor tensor object __new__ cls __repr__ my_group_rank = get_rank group op_name = op __name__ group_name = group group_name group default_pg send op_name s = my_group_rank d = group_peer recv op_name s = group_peer d = my_group_rank super __repr__ f P POp op_name pg= group_name group_src= s group_dst= d tensor shape tensor dtype _CollOp A capture collective operations Args op Callable A collective function e g ` ` torch distributed all_reduce ` ` tensor Tensor Tensor operate dst_tensor Tensor optional Provided when source destination tensors same redop ReduceOp optional reduce operation root int optional root broadcast reduce __init__ op Callable tensor torch Tensor dst_tensor Optional torch Tensor = None redop Optional ReduceOp = None root Optional int = None op = op tensor = tensor dst_tensor = dst_tensor redop = redop root = root DO NOT USE THESE FIELDS DIRECTLY Use them through _world object make sure _world override mechanism _pg_map dict ProcessGroup tuple str Store = _pg_names dict ProcessGroup str = _pg_group_ranks dict ProcessGroup dict int int = For pg map ProcessGroup BackendConfig _pg_backend_config dict ProcessGroup str = _group_count = _tags_to_pg dict str list ProcessGroup = _pg_to_tag dict ProcessGroup str = _backend Optional str = None _World Container c d process group state This used during registration lookup PG state warning This experimental API intended expose inner workings c d subject change __init__ - None _default_pg = None _pg_coalesce_state dict ProcessGroup list _CollOp = property default_pg - Optional ProcessGroup Process group includes all ranks cluster This default ProcessGroup used c d APIs when ProcessGroup needed None provided _default_pg default_pg setter default_pg value - None _default_pg = value property pg_map - dict ProcessGroup tuple str Store Provide Mapping ProcessGroup backend name store For NCCL GLOO pg map ProcessGroup Backend Store For MPI pg map ProcessGroup Backend None TODO don t expose map expose fine grained ops global _pg_map _pg_map property pg_names - dict ProcessGroup str Process group s names map ProcessGroup str TODO don t expose map expose fine grained ops global _pg_names _pg_names property pg_group_ranks - dict ProcessGroup dict int int Process group s global rank local rank mapping TODO don t expose map expose fine grained ops global _pg_group_ranks _pg_group_ranks property pg_backend_config - dict ProcessGroup str Process group s backend config TODO don t expose map expose fine grained ops global _pg_backend_config _pg_backend_config property group_count - int Process group count default naming TODO don t expose group_count use something instead global _group_count _group_count group_count setter group_count value int - None Use compute name ProcessGroups when using global synchronization global _group_count _group_count = value property tags_to_pg - dict str list ProcessGroup global _tags_to_pg _tags_to_pg property pg_to_tag - dict ProcessGroup str global _pg_to_tag _pg_to_tag property pg_coalesce_state - dict ProcessGroup list _CollOp _pg_coalesce_state property pg_config_info - list dict str Any Return list dict process groups backends Along their unique IDs configurations types ranks config_info list dict str Any = default_pg_size = _get_group_size None pg pg_map keys ranks = pg_group_ranks pg config_info append pg_name pg_names pg pg_desc pg group_desc backend_config pg_backend_config pg ranks list ranks keys len ranks = default_pg_size ranks empty list when all ranks involved pg group_size len ranks group_count group_count config_info _world = _World Holds singleton instance ` ` _World ` ` used c Experimental extension point override _WorldMeta type Meta ` ` group ` ` ` ` GroupMember ` ` Allows them have property ` ` WORLD ` ` Points default PG once initialized property WORLD cls - Optional ProcessGroup _world default_pg WORLD setter WORLD cls pg Optional ProcessGroup _world default_pg = pg group metaclass=_WorldMeta Group Placeholder GroupMember metaclass=_WorldMeta Group member NON_GROUP_MEMBER = - _get_default_timeout backend Backend - timedelta see note nccl vs other backend timeout constants py backend == Backend NCCL isinstance default_pg_nccl_timeout timedelta TODO moco benchmark CPU initializes pgnccl backend today triggered assert CI before changed warning We should fix moco model warnings warn Attempted get default timeout nccl backend NCCL support compiled stacklevel= default_pg_timeout default_pg_nccl_timeout default_pg_timeout _check_valid_timeout timeout Any - None isinstance timeout timedelta raise TypeError f Expected timeout argument type datetime timedelta got timeout Default process group state _default_pg_init_method Optional str = None STORE_BASED_BARRIER_PREFIX = store_based_barrier_key _get_object_coll_device group Optional ProcessGroup = None - str note This internal helper does have backward compatibility please use caution Return device type use ` ` group ` ` object collectives barrier There selection rules If user specifies exactly one backend ` ` init_process_group ` ` call use backend Else user specifies multiple device backend pairs init_process_group If cpu among those pairs use cpu because object cpu memory Otherwise use first backend sort random pick Args group ProcessGroup optional The process group work If None default process group will used Returns str The device type use object collective ` ` group ` ` group = group _get_default_group isinstance group ProcessGroup warnings warn f You using Backend type group ProcessGroup This usage deprecated since PyTorch Please use public API PyTorch Distributed instead stacklevel= Provide backward compatibility cases where ` group ` passed actually Backend like ` ProcessGroupGloo ` rather than ` ProcessGroup ` PT sense isinstance group ProcessGroupGloo RPC uses Gloo object collectives cpu raise ValueError f Expecting ProcessGroup got type group ` ` group _device_types ` ` property pybind returns devices cpu cuda etc supported ` ` group ` ` Can multiple ` ` group ` ` supports multiple devices devices = group _device_types len devices == User fixed exactly one backend ` init_process_group ` devices type len devices == No backend has been registered PG maybe because no collective has been run We pick cpu default hopefully would lazily init Gloo other available cpu backend cpu torch device cpu devices There multiple backends PG cpu among them cpu preferred object cpu memory No need device copy cpu No cpu backend list Randomly pick first backend devices type _get_pg_default_device group Optional ProcessGroup = None - torch device note This method will deprecated only stays backward-compatiblity reason Alternatives - If you need find device object collectives please use ` _get_object_coll_device group ` - If you need query device types supported group please use ` _device_capability group ` Return device type registered ` ` group ` ` For example ` init_process_group nccl ` called returned value would ` torch device cuda ` Errors out no device has been registered Args group ProcessGroup optional The process group work If None default process group will used Returns torch device The device type registered ` ` group ` ` warnings warn ` _get_pg_default_device ` will deprecated only stays backward-compatiblity reason If you need find device object collectives please use ` _get_object_coll_device ` If you need query device types supported group please use ` _device_capability group ` stacklevel= group = group _get_default_group isinstance group ProcessGroup Provide backward compatibility cases where ` group ` passed actually Backend like ` ProcessGroupGloo ` rather than ` ProcessGroup ` PT sense warnings warn f You using Backend type group ProcessGroup This usage deprecated since PyTorch Please use public API PyTorch Distributed instead FutureWarning stacklevel= Most users create Gloo private API object collectives torch device cpu ` ` group _device_types ` ` property pybind returns devices cpu cuda etc supported ` ` group ` ` Can multiple ` ` group ` ` supports multiple devices devices = group _device_types len devices == User fixed exactly one backend ` init_process_group ` devices len devices == raise RuntimeError Default device found because no backend has been registered ProcessGroup There multiple backends PG torch device cpu devices rv = torch device cpu rv = devices warnings warn Multiple backends registered ProcessGroup We cannot f determine which one default Returning rv Please consider using other APIs stacklevel= rv _device_capability group Optional ProcessGroup = None - list str Return device type s supported ` ` group ` ` Args group ProcessGroup optional The process group query If None default process group will used Returns List str A list device types supported ` ` group ` ` group = group _get_default_group device type device group _device_types _time_logger _store_based_barrier rank store group_name rendezvous_count timeout logging_interval=timedelta seconds= - None Store based barrier synchronizing processes Barrier based store which used synchronizing processes after ` ` init_process_group ` ` ` ` new_group ` ` Intended used only those two methods generic alternative ` ` barrier ` ` store_key = f STORE_BASED_BARRIER_PREFIX group_name store add store_key logger debug Added key s store rank s store_key rank Now wait all workers check store world_size = rendezvous_count worker_count = store add store_key last_worker_key = f store_key last_worker worker_count == world_size store set last_worker_key adjust timeout least secs + sec per thousand ranks reduce odds timeout value empirically found while scale testing logging_interval = max logging_interval timedelta seconds= + world_size start = time time while True try This will throw exception after logging_interval which we print out status group time out officially throwing runtime error store wait last_worker_key logging_interval break except RuntimeError e worker_count = store add store_key Print status periodically keep track logger debug noqa G Waiting store based barrier initialize process group s seconds rank s key s world_size= s num_workers_joined= s timeout= s error= s time time - start rank store_key world_size worker_count timeout e timedelta seconds= time time - start timeout raise DistStoreError noqa B Timed out initializing process group store based barrier f rank rank key store_key world_size= world_size f num_workers_joined= worker_count timeout= timeout error= e logger info Rank s Completed store-based barrier key s s nodes rank store_key world_size _rank_not_in_group group Optional ProcessGroup - bool Check current process s rank given group group None False group == GroupMember NON_GROUP_MEMBER _warn_not_in_group op_name - None global_rank = - GroupMember WORLD None GroupMember WORLD rank warnings warn f Running op_name global rank global_rank which does belong given group stacklevel= get_group_rank group ProcessGroup global_rank int - int Translate global rank into group rank ` ` global_rank ` ` must part ` ` group ` ` otherwise raises RuntimeError Args group ProcessGroup ProcessGroup find relative rank global_rank int Global rank query Returns Group rank ` ` global_rank ` ` relative ` ` group ` ` N B calling function default process group returns identity group GroupMember WORLD global_rank group _world pg_group_ranks raise ValueError f Group group registered please create group torch distributed new_group API group_ranks = _world pg_group_ranks group global_rank group_ranks raise ValueError f Global rank global_rank part group group group_ranks global_rank get_global_rank group ProcessGroup group_rank int - int Translate group rank into global rank ` ` group_rank ` ` must part ` group ` otherwise raises RuntimeError Args group ProcessGroup ProcessGroup find global rank group_rank int Group rank query Returns Global rank ` ` group_rank ` ` relative ` ` group ` ` N B calling function default process group returns identity group GroupMember WORLD group_rank group _world pg_group_ranks raise ValueError f Group group registered please create group torch distributed new_group API rank grp_rank _world pg_group_ranks group items grp_rank == group_rank rank raise ValueError f Group rank group_rank part group group TODO remove once ecosystem moves away deprecated ` torch distributed distributed_c d _get_global_rank ` deprecated please use ` torch distributed distributed_c d get_global_rank ` instead category=FutureWarning _get_global_rank group rank - int Use get_global_rank method deprecated get_global_rank group rank get_process_group_ranks group Optional ProcessGroup - list int Get all ranks associated ` ` group ` ` Args group Optional ProcessGroup ProcessGroup get all ranks If None default process group will used Returns List global ranks ordered group rank list _world pg_group_ranks group _get_default_group keys _get_group_size group - int Get given group s world size group GroupMember WORLD group None default_pg = _get_default_group default_pg size group size _get_group_size_by_name group_name str - int group = _resolve_process_group group_name group size _resolve_group_name_by_ranks_and_tag ranks list int tag str - str TODO yifu remove function once ranks + tag supported identifier process group functional collectives group = _find_pg_by_ranks_and_tag tag ranks group None raise ValueError group group_name _check_single_tensor param param_name - None Check parameter ` ` param_name ` ` single tensor isinstance param torch Tensor raise TypeError f Invalid function argument Expected parameter ` param_name ` type torch Tensor got type param instead _check_tensor_list param param_name - None Check parameter ` ` param_name ` ` list tensors isinstance param list raise TypeError f Invalid function argument Expected parameter ` param_name ` type List torch Tensor got type param instead all isinstance p torch Tensor p param raise TypeError f Invalid function argument Expected parameter ` param_name ` type List torch Tensor got type param elements type type p p param _group_or_default_group group Optional ProcessGroup = None - ProcessGroup group None group GroupMember WORLD group = _get_default_group group _canonicalize_group_rank group ProcessGroup global_rank Optional int = None group_rank Optional int = None return_global bool = False - int Helper method take _either_ global rank group rank produce group rank If return_global true produce global rank instead group rank group_rank None global_rank None raise ValueError Can t specify both group_rank global_rank return_global get_global_rank group group_rank global_rank None raise ValueError Must specify global_rank group_rank return_global global_rank group_rank = get_group_rank group global_rank group_rank _check_not_self_rank group ProcessGroup rank int rank_type str group rank == rank raise ValueError f Invalid rank_type rank rank_type rank should same rank current process _as_iterable obj - collections abc Iterable obj isinstance obj list obj _ensure_all_tensors_same_dtype tensors - None last_dtype = None pyrefly ignore bad-assignment tensor itertools chain from_iterable map _as_iterable tensors tensor_dtype = tensor dtype Mixing complex its element type allowed tensor_dtype is_complex tensor_dtype = torch float tensor_dtype == torch complex torch complex last_dtype None last_dtype = tensor_dtype last_dtype = tensor_dtype raise ValueError Invalid usage tensors different dtypes f Found last_dtype tensor dtype _check_op op - None Check ` ` op ` ` either isend irecv op isend irecv raise ValueError Invalid ` ` op ` ` Expected ` ` op ` ` type ` ` torch distributed isend ` ` ` ` torch distributed irecv ` ` _check_p p_op_list p p_op_list - None Check ` ` p p_op_list ` ` list P POp instances Also check all ops use same group isinstance p p_op_list list all isinstance p p_op P POp p p_op p p_op_list raise ValueError Invalid ` ` p p_op_list ` ` Each op expected type ` ` torch distributed P POp ` ` group = p p_op_list group all group == p p_op group p p_op p p_op_list raise ValueError All ops need use same group is_mpi_available - bool Check MPI backend available _MPI_AVAILABLE is_nccl_available - bool Check NCCL backend available _NCCL_AVAILABLE is_gloo_available - bool Check Gloo backend available _GLOO_AVAILABLE is_ucc_available - bool Check UCC backend available _UCC_AVAILABLE is_xccl_available - bool Check XCCL backend available _XCCL_AVAILABLE _check_single_backend_availability backend_name str - bool Helper function check single backend available available_func = getattr torch distributed f is_ str backend_name lower _available None available_func available_func str backend_name lower Backend backend_list is_backend_available backend str - bool Check backend availability Checks given backend available supports built-in backends third-party backends through function ` ` Backend register_backend ` ` Args backend str Backend name Returns bool Returns true backend available otherwise false If backend has ` ` is_backend_available ` ` function result function directly backend lower composite backend like cpu gloo backend_config = BackendConfig Backend backend device_backend_map = backend_config get_device_backend_map all _check_single_backend_availability str backend_name backend_name device_backend_map values Handle simple backend strings like nccl gloo _check_single_backend_availability backend is_initialized - bool Check default process group has been initialized GroupMember WORLD None is_torchelastic_launched - bool Check whether process launched ` ` torch distributed elastic ` ` aka torchelastic The existence ` ` TORCHELASTIC_RUN_ID ` ` environment variable used proxy determine whether current process launched torchelastic This reasonable proxy since ` ` TORCHELASTIC_RUN_ID ` ` maps rendezvous id which always non-null value indicating job id peer discovery purposes os getenv TORCHELASTIC_RUN_ID None _is_barrier_after_init - int Environment variable control whether process group should perform barrier after its init Default value i e no barrier If you experience issue setting you may set ` TORCH_DIST_INIT_BARRIER= ` add barrier int os getenv TORCH_DIST_INIT_BARRIER _get_default_group - ProcessGroup Get default process group created init_process_group is_initialized raise ValueError Default process group has been initialized please make sure call init_process_group TYPE_CHECKING not_none GroupMember WORLD GroupMember WORLD _get_default_store - Store Get default store created init_process_group is_initialized raise ValueError Default process group has been initialized please make sure call init_process_group default_pg = _get_default_group _ default_store = _world pg_map default_pg default_store _update_default_pg pg - None _world default_pg = pg rank = pg rank pg None pg = GroupMember NON_GROUP_MEMBER - torch _C _distributed_c d _set_global_rank rank get_backend_config group Optional ProcessGroup = None - str Return backend configuration given process group Args group ProcessGroup optional The process group work The default general main process group If another specific group specified calling process must part attr ` group ` Returns The backend configuration given process group lower case string pg = group _get_default_group _rank_not_in_group pg raise ValueError Invalid process group specified backend_config = _world pg_backend_config get pg str not_none backend_config get_backend group Optional ProcessGroup = None - Backend Return backend given process group Args group ProcessGroup optional The process group work The default general main process group If another specific group specified calling process must part attr ` group ` Returns The backend given process group lower case string pg = group _get_default_group _rank_not_in_group pg raise ValueError Invalid process group specified pg_store = _world pg_map get pg None pg_store None raise ValueError f Process group pg initialized world group map Please initialize group first Backend not_none pg_store get_default_backend_for_device device Union str torch device - str Return default backend given device Args device Union str torch device The device get default backend Returns The default backend given device lower case string isinstance device torch device device_str = device type device_str = torch device device type backend = Backend default_device_backend_map get device_str backend None raise ValueError f Default backend registered device device backend _get_process_group_uid pg ProcessGroup - int backend = None try backend = pg _get_backend torch device cuda except RuntimeError pass is_nccl_available isinstance backend ProcessGroupNCCL backend uid - _get_pg_config group Optional ProcessGroup = None - dict str Any Return pg configuration given process group pg = group _get_default_group pg_name _get_process_group_name pg pg_desc pg group_desc backend_config get_backend_config pg pg_size _get_group_size pg ranks get_process_group_ranks pg _get_all_pg_configs - list dict str Any Return pg configuration all process groups config_info list dict str Any = _get_pg_config pg pg _world pg_map keys config_info get_pg_count - int Return number process groups _world group_count get_node_local_rank fallback_rank Optional int = None - int Return local rank current process relative node Semantically useful concept mapping processes devices For example node accelerator you could use node local rank decide which accelerator device bind process In practice actual assignment node local ranks handled process launcher outside pytorch communicated via ` LOCAL_RANK ` environment variable Torchrun will automatically populate ` LOCAL_RANK ` other launchers may If ` LOCAL_RANK ` unspecified API will fall back provided kwarg fallback_rank specified otherwise will raise error The intent allow writing application runs either single multi device contexts without error LOCAL_RANK os environ int os environ LOCAL_RANK fallback_rank None int fallback_rank raise RuntimeError LOCAL_RANK environment Consider passing fallback_rank allow ` get_node_local_rank ` work assuming you running multi-device context want code run locally instead _add_ephemeral_timeout_for_all_pgs timeout timedelta - None This API adds ephemeral timeout extension all PGs locally one rank The timeout gets reset when first collective issued after API called finished NOTE We only support set timeout cuda backends now NOTE While feature provides flexibility specific scenarios introduces statefulness timeout setting Therefore advisable use API sparingly consider alternative approaches such directly setting timeout utilizing barrier collective one can set any timeout barrier whenever feasible Args timeout timedelta The delta timeout extend Returns None pg _world pg_map keys devices = pg _device_types torch device cuda devices backend = pg _get_backend torch device cuda is_nccl_available isinstance backend ProcessGroupNCCL backend _add_ephemeral_timeout timeout _set_pg_timeout timeout timedelta group Optional ProcessGroup = None - None Set timeout given process group when users want use different timeout instead default values Args timeout timedelta Timeout operations executed against process group which users want set Default value minutes NCCL minutes other backends This duration after which collectives will aborted asynchronously process will crash This done since CUDA execution async no longer safe continue executing user code since failed async NCCL operations might result subsequent CUDA operations running corrupted data When TORCH_NCCL_BLOCKING_WAIT set process will block wait timeout group ProcessGroup optional The process group work The default general main process group If another specific group specified calling process must part attr ` group ` Returns None group None group = _get_default_group _rank_not_in_group group raise ValueError Invalid process group specified assert isinstance group ProcessGroup devices = group _device_types backends = set torch device cpu devices is_gloo_available backend = group _get_backend torch device cpu isinstance backend ProcessGroupGloo backends add backend torch device cuda devices backend = group _get_backend torch device cuda is_nccl_available isinstance backend ProcessGroupNCCL backends add backend type ignore arg-type is_gloo_available isinstance backend ProcessGroupGloo backends add backend type ignore arg-type len backends == warnings warn Set timeout now only supported either nccl gloo stacklevel= backend backends backend _set_default_timeout timeout _exception_logger _time_logger init_process_group backend Optional str = None init_method Optional str = None timeout Optional timedelta = None world_size int = - rank int = - store Optional Store = None group_name str = pg_options Optional Any = None device_id Optional Union torch device int = None _ranks Optional list int = None - None Initialize default distributed process group This will also initialize distributed package There main ways initialize process group Specify ` ` store ` ` ` ` rank ` ` ` ` world_size ` ` explicitly Specify ` ` init_method ` ` URL string which indicates where how discover peers Optionally specify ` ` rank ` ` ` ` world_size ` ` encode all required parameters URL omit them If neither specified ` ` init_method ` ` assumed env Args backend str Backend optional The backend use Depending build-time configurations valid values include ` ` mpi ` ` ` ` gloo ` ` ` ` nccl ` ` ` ` ucc ` ` ` ` xccl ` ` one registered third-party plugin Since ` ` backend ` ` provided c d will use backend registered device type indicated ` device_id ` kwarg provided The known default registrations today ` ` nccl ` ` ` ` cuda ` ` ` ` gloo ` ` ` ` cpu ` ` ` ` xccl ` ` ` ` xpu ` ` If neither ` ` backend ` ` nor ` ` device_id ` ` provided c d will detect accelerator run-time machine use backend registered detected accelerator ` ` cpu ` ` This field can given lowercase string e g ` ` gloo ` ` which can also accessed via ` Backend ` attributes e g ` ` Backend GLOO ` ` If using multiple processes per machine ` ` nccl ` ` backend each process must have exclusive access every GPU uses sharing GPUs between processes can result deadlock NCCL invalid usage ` ` ucc ` ` backend experimental Default backend device can queried func ` get_default_backend_for_device ` init_method str optional URL specifying how initialize process group Default env no ` ` init_method ` ` ` ` store ` ` specified Mutually exclusive ` ` store ` ` world_size int optional Number processes participating job Required ` ` store ` ` specified rank int optional Rank current process should number between ` ` world_size ` ` - Required ` ` store ` ` specified store Store optional Key value store accessible all workers used exchange connection address information Mutually exclusive ` ` init_method ` ` timeout timedelta optional Timeout operations executed against process group Default value minutes NCCL minutes other backends This duration after which collectives will aborted asynchronously process will crash This done since CUDA execution async no longer safe continue executing user code since failed async NCCL operations might result subsequent CUDA operations running corrupted data When TORCH_NCCL_BLOCKING_WAIT set process will block wait timeout group_name str optional deprecated Group name This argument ignored pg_options ProcessGroupOptions optional process group options specifying what additional options need passed during construction specific process groups As now only options we support ` ` ProcessGroupNCCL Options ` ` ` ` nccl ` ` backend ` ` is_high_priority_stream ` ` can specified so nccl backend can pick up high priority cuda streams when there re compute kernels waiting For other available options config nccl See https docs nvidia com deeplearning nccl user-guide docs api types html#ncclconfig-t device_id torch device &#124; int optional single specific device process will work allowing backend-specific optimizations Currently has two effects only under NCCL communicator immediately formed calling ` ` ncclCommInit ` ` immediately rather than normal lazy call sub-groups will use ` ` ncclCommSplit ` ` when possible avoid unnecessary overhead group creation If you want know NCCL initialization error early you can also use field If ` int ` provided API assumes accelerator type compile time will used _ranks The ranks process group If provided process group name will hash all ranks group note To enable ` ` backend == Backend MPI ` ` PyTorch needs built source system supports MPI note Support multiple backends experimental Currently when no backend specified both ` ` gloo ` ` ` ` nccl ` ` backends will created The ` ` gloo ` ` backend will used collectives CPU tensors ` ` nccl ` ` backend will used collectives CUDA tensors A custom backend can specified passing string format device_type backend_name device_type backend_name e g cpu gloo cuda custom_backend global _world global _backend global _default_pg_init_method GroupMember WORLD None raise ValueError trying initialize default process group twice set_pytorch_distributed_envs_from_justknobs Depending order some trace_rules functions may evaluated during phase In such case these functions may correctly add distributed related rules due circular dependency We need clear lru_cache during runtime ensure correctness these trace_rules Since API must called before all distributed code being compiled clearing cache here should safe torch _dynamo sys modules torch _dynamo trace_rules clear_lru_cache assert store None init_method None Cannot specify both init_method store store None assert world_size world_size must positive using store assert rank = rank must non-negative using store init_method None init_method = env Get compile-time accelerator type None indicates no accelerator support acc = torch accelerator current_accelerator Auto complete device id isinstance device_id int acc None raise ValueError device_id int no accelerator support found current compilation Please use different compiled version supports your accelerator device_id = torch device acc type device_id Sanity check device_id device_id None device_id type = cpu Type acc None device_id type = acc type raise ValueError f device_id device_id does match current compilation s accelerator support acc Please use different compiled version supports your accelerator Index device_id index None raise ValueError Please use device_id index Range device_id index = torch accelerator device_count raise ValueError f device_id device_id out range Please use device index less than f number accelerators available torch accelerator device_count logger info Using device s device_id If user did provide backend string provided device id e g init_process_group device_id=device we try figure out backend name based device type backend None device_id None Note rd-party devices can register default backend through default map below backend = Backend default_device_backend_map get device_id type If we still cannot figure out e g init_process_group we set ` undefined ` rely lazy init backend None backend = undefined Convert string into ` Backend ` type backend = Backend backend timeout None timeout = _get_default_timeout backend _check_valid_timeout timeout Group name visible users unless they access internals c d This means we can ignore value they provide exposed public way _ranks None len _ranks == group_name = _process_group_name use_hashed_name=False group_name = _process_group_name _ranks use_hashed_name=True backend == Backend MPI world_size = - rank = - warnings warn f For MPI backend world_size world_size rank rank ignored since they assigned MPI runtime stacklevel= default_pg _ = _new_process_group_helper - - backend Store Placeholder value since store cannot None group_name timeout=timeout group_desc= default_pg backward compatible API store None backend == fake torch testing _internal distributed fake_pg FakeStore store = FakeStore rendezvous_iterator = rendezvous not_none init_method rank world_size timeout=timeout store rank world_size = next rendezvous_iterator store set_timeout timeout Use PrefixStore avoid accidental overrides keys used different systems e g RPC case store multi-tenant store = PrefixStore default_pg store default_pg _ = _new_process_group_helper world_size rank backend store group_name backend_options=pg_options timeout=timeout device_id=device_id group_desc= default_pg _update_default_pg default_pg _world pg_group_ranks GroupMember WORLD = type ignore index i i i range GroupMember WORLD size type ignore attr-defined _backend = _world pg_map not_none GroupMember WORLD _default_pg_init_method = init_method old_hook = sys excepthook excepthook_prefix = f rank get_rank _distributed_excepthook args old_stderr = sys stderr sys stderr = buf = io StringIO try old_hook args finally sys stderr = old_stderr msg = buf getvalue msg = \n join f excepthook_prefix s s = s msg split \n sys stderr write msg sys stderr flush sys excepthook = _distributed_excepthook _is_barrier_after_init == barrier end ensure once we method all process groups including global variables any updated correctly all ranks Update large-scale runs barrier esp store-based barrier may costly unscalable Also lot cases these barriers may unnecessary proven green CI after removal An environment variable ` TORCH_DIST_INIT_BARRIER ` has been added which enables barrier only when set logger debug Performing barrier after ProcessGroup initialization since TORCH_DIST_INIT_BARRIER = backend == Backend MPI MPI backend doesn t use store barrier Use store based barrier here since barrier used bunch default devices messes up NCCL internal state _store_based_barrier rank store group_name world_size timeout _get_split_source pg split_from = None pg bound_device_id split_from = pg _get_backend pg bound_device_id pg _world default_pg try pyrefly ignore missing-attribute split_from = pg _get_backend torch device cuda except RuntimeError no cuda device associated backend pass split_from split_from supports_splitting None If necessary find backend split peeling process group wrappers our potentially wrapped process group while _GLOO_AVAILABLE isinstance split_from _ProcessGroupWrapper split_from = split_from wrapped_pg split_from _new_process_group_helper group_size group_rank global_ranks_in_group backend store group_name backend_options=None timeout=None pg_tag=None device_id=None group_desc=None Create new distributed process group This function must called ALL processes global group even calling process part newly created group In case function returns GroupMember NON_GROUP_MEMBER This function called ` ` global_ranks_in_group == ` ` default group global _world group_name _world pg_names values raise ValueError The specified group name has already been created please use different group name device_id None device_id index None device_id type == cpu raise ValueError init_process_group device_id parameter must accelerator index Note _new_process_group_helper only called init_process_group which always provides timeout value _check_valid_timeout timeout pg_tag None creating same tag rank set results same underlying PG existing_group = _find_pg_by_ranks_and_tag pg_tag global_ranks_in_group existing_group _ prefix_store = _world pg_map existing_group existing_group prefix_store group_desc = undefined group_desc None group_desc The list group ranks empty we re creating default group is_default_group = len global_ranks_in_group == nccl potentially other backends allow creation communicators based pre-existing ones which can save initialization time Due lazy initialization communicators some backends we have careful only split when we know default PG has already started communicator initialization We know we have bound device id default pg eager initialized is_initialized _get_default_group bound_device_id split_from = _get_split_source _get_default_group split_from = None If subgroup which means group_ranks specified we check current process member new group is_default_group global_rank = _get_default_group rank global_rank global_ranks_in_group If we using ` ncclCommSplit ` similar split other APIs create communicator we will need call ` ncclCommSplit ` all ranks new group s parent group even those new group This requirement NCCL API otherwise we would get out sync split_from split_from perform_nocolor_split _get_default_group bound_device_id GroupMember NON_GROUP_MEMBER None prefix_store = PrefixStore f group_name store The backend PG will set later based what s inside BackendConfig timeout set each backend s option pg ProcessGroup = ProcessGroup prefix_store group_rank group_size backend_config = BackendConfig backend Set default backend when single backend passed str backend str backend assert backend Backend backend_type_map f Unknown backend type backend backend == Backend UNDEFINED Currently when backend UNDEFINED only one backend will initialized we use nccl cuda available gloo default backend so we can correctly call getDefaultBackend which ProcessGroup Backend NCCL backend_config get_device_backend_map values pg _set_default_backend ProcessGroup BackendType NCCL pg _set_default_backend ProcessGroup BackendType GLOO pg _set_default_backend Backend backend_type_map backend In order correctly call pg _has_hooks we should set default backend when multi backend passed Backend NCCL backend_config device_backend_map values pg _set_default_backend ProcessGroup BackendType NCCL Backend _plugins keys custom_backend = next iter Backend _plugins keys custom_backend backend_config device_backend_map values pg _set_default_backend ProcessGroup BackendType CUSTOM pg _set_default_backend ProcessGroup BackendType GLOO device_id pg bound_device_id = device_id backend_class torch _C _distributed_c d Backend device backend_str backend_config get_device_backend_map items Use group name prefix default store such single store can reused multiple groups backend_prefix_store = PrefixStore f device prefix_store backend_str == Backend MPI is_mpi_available raise RuntimeError Distributed package doesn t have MPI built MPI only included you build PyTorch source host has MPI installed backend_class = ProcessGroupMPI create global_ranks_in_group backend_type = ProcessGroup BackendType MPI backend_class GroupMember NON_GROUP_MEMBER None create new process group accurate rank size pg rank == - pg size == - pg = ProcessGroup backend_prefix_store backend_class rank backend_class size pg _set_default_backend backend_type backend_str == Backend GLOO TODO remove check after lazy initialization supported pg_options None raise RuntimeError GLOO options supported is_gloo_available raise RuntimeError Distributed package doesn t have Gloo built backend_class = ProcessGroupGloo backend_prefix_store group_rank group_size pyrefly ignore bad-argument-type timeout=timeout backend_class options global_ranks_in_group = global_ranks_in_group backend_class options group_name = group_name backend_type = ProcessGroup BackendType GLOO backend_str == Backend NCCL is_nccl_available raise RuntimeError Distributed package doesn t have NCCL built backend_options None assert isinstance backend_options ProcessGroupNCCL Options Expected backend_options argument type ProcessGroupNCCL Options backend_options _timeout = timeout warnings warn backend_options _timeout specified timeout kwarg has default value will always override stacklevel= default backend_options NCCL backend_options = ProcessGroupNCCL Options backend_options is_high_priority_stream = False pyrefly ignore bad-argument-type backend_options _timeout = timeout split_from backend_options split_from = split_from backend_options split_color = _process_group_color global_ranks_in_group backend_options global_ranks_in_group = global_ranks_in_group backend_options group_name = group_name backend_class = ProcessGroupNCCL backend_prefix_store group_rank group_size backend_options backend_type = ProcessGroup BackendType NCCL backend_str == Backend UCC is_ucc_available TODO once UCC plugin fully deprecated remove is_ucc_available above elif-condition raise RuntimeError is_ucc_available returns false backend_class = ProcessGroupUCC backend_prefix_store group_rank group_size pyrefly ignore bad-argument-type timeout=timeout backend_type = ProcessGroup BackendType UCC backend_str == Backend XCCL is_xccl_available raise RuntimeError Distributed package doesn t have XCCL built backend_options = ProcessGroupXCCL Options backend_options global_ranks_in_group = global_ranks_in_group backend_options group_name = group_name pyrefly ignore bad-argument-type backend_options _timeout = timeout backend_class = ProcessGroupXCCL backend_prefix_store group_rank group_size backend_options backend_type = ProcessGroup BackendType XCCL assert backend_str upper Backend _plugins f Unknown c d backend type backend_str upper backend_plugin = Backend _plugins backend_str upper creator_fn = backend_plugin creator_fn extended_api = backend_plugin extended_api backend_type = ProcessGroup BackendType CUSTOM extended_api backend_class = creator_fn backend_prefix_store group_rank group_size timeout dist_backend_opts = _DistributedBackendOptions dist_backend_opts store = backend_prefix_store dist_backend_opts group_rank = group_rank dist_backend_opts group_size = group_size pyrefly ignore bad-argument-type dist_backend_opts timeout = timeout dist_backend_opts group_id = group_name dist_backend_opts global_ranks_in_group = global_ranks_in_group backend_class = creator_fn dist_backend_opts backend_options Set sequence numbers gloo nccl backends backend_str == Backend GLOO assert isinstance backend_class ProcessGroupGloo backend_class _set_sequence_number_for_group backend_str == Backend NCCL assert isinstance backend_class ProcessGroupNCCL backend_class _set_sequence_number_for_group If type subclass ProcessGroup then process group immediately TODO This defaults old behavior PythonProcessGroups which overwrites ProcessGroup instance issubclass type backend_class ProcessGroup pg = backend_class type ignore assignment break Process group wrapper initialization supported PGs when TORCH_DISTRIBUTED_DEBUG set backend_str Backend GLOO Backend NCCL Backend UCC backend_str upper Backend _plugins In debug mode GLOO available wrap wrapper PG enables enhanced collective checking debuggability get_debug_level == DebugLevel DETAIL _GLOO_AVAILABLE logger info TORCH_DISTRIBUTED_DEBUG set DETAIL GLOO available Build Gloo create wrapper process group debug mode aid collective desynchronization debugging backend_class = _create_process_group_wrapper wrapped_pg=backend_class store_prefix=group_name store=backend_prefix_store rank=group_rank world_size=group_size pyrefly ignore bad-argument-type timeout=timeout register only single backend when all get_device_backend_map values same len set backend_config get_device_backend_map values == device backend_config get_device_backend_map keys pg _register_backend torch device device backend_type backend_class break out outer loop create any more backends break pg _register_backend torch device device backend_type backend_class set group_name group_dsec backend assert group_name None assert group_desc None pg _set_group_name group_name pg _set_group_desc group_desc device_id pg _get_backend device_id supports_splitting eager_backend = pg _get_backend device_id eager_backend eager_connect_single_device device_id update global state _world pg_map pg = backend prefix_store _world pg_names pg = group_name _register_process_group group_name pg _world pg_backend_config pg = str backend_config default tag user PGs pg_tag None pg_tag = f ptd group_name _world tags_to_pg setdefault append pg pg_tag = f user pg_tag _world tags_to_pg setdefault pg_tag append pg _world pg_to_tag pg = pg_tag pg prefix_store destroy_process_group group Optional ProcessGroup = None Destroy given process group deinitialize distributed package Args group ProcessGroup optional The process group destroyed group WORLD given all process groups including default one will destroyed global _world group == GroupMember NON_GROUP_MEMBER group None pg = GroupMember WORLD pg = group assert pg None _world pg_map get pg None None raise ValueError Invalid process group specified When users register Python onCompletion hooks those hooks will run different thread than main thread Today ProcessGroup dtor does wait thread However dtor might finish after Python Interpreter exits After grabbing GIL Python hook will crash We can either revive interpreter when running hooks keep main one alive until all works hooks done The current implementation does latter Therefore we explicitly call _wait_for_pending_works here wait pending hooks finish type pg ProcessGroup pg _has_hooks pg _wait_for_pending_works group None group == GroupMember WORLD shutdown all backends order pg names shutting down order because ncclCommAbort collective call some versions NCCL pg_to_shutdown sorted _world pg_names key=lambda x _world pg_names x reverse=True pg_to_shutdown shutdown _update_default_pg None _world pg_map clear _world pg_names clear _world pg_group_ranks clear _world pg_backend_config clear _world pg_to_tag clear _world tags_to_pg clear _world pg_coalesce_state clear _unregister_all_process_groups when process group doesn t have explicit name only WORLD default process group can have explicit name we use global _world group_count generate name We need reset counter destruction allow consistent value generated when we re-create process groups after some trainers recover failure We only reset when WORLD being destroyed because process group good state we aren t dealing failures _world group_count = pg shutdown del _world pg_map pg del _world pg_names pg del _world pg_group_ranks pg del _world pg_backend_config pg pg _world pg_coalesce_state keys warnings warn Some coalesced collectives haven t been launched when ProcessGroup destroyed They will cleaned stacklevel= del _world pg_coalesce_state pg tag = _world pg_to_tag get pg del _world pg_to_tag pg tag None try _world tags_to_pg tag remove pg tag startswith ptd _world tags_to_pg remove pg except Exception pass _unregister_process_group pg group_name _abort_process_group group Optional ProcessGroup = None Abort given process group If group WORLD i e ` None ` given all process groups including default one will aborted Args group ProcessGroup optional The process group aborted note API experimental currently only works NCCL backend note API should used ` TORCH_NCCL_ASYNC_ERROR_HANDLING ` turned off i e set Otherwise ProcessGroupNCCL s watchdog may automatically handle errors timeouts you including aborting ProcessGroup global _world group == GroupMember NON_GROUP_MEMBER pg = group GroupMember WORLD assert pg None _world pg_map get pg None None raise ValueError Invalid process group specified has been destroyed try backend = pg _get_backend torch device cuda except RuntimeError backend = None group None group == GroupMember WORLD Abort all backends within ncclGroupStart &#124; End semantic This ensures different NCCL communicators abort calls won t deadlock each other For details please see https github com pytorch pytorch issues is_nccl_available isinstance backend ProcessGroupNCCL backend _group_start pg_to_abort sorted _world pg_names key=lambda x _world pg_names x reverse=True pg_to_abort abort is_nccl_available isinstance backend ProcessGroupNCCL backend _group_end _update_default_pg None _world pg_map clear _world pg_names clear _world pg_group_ranks clear _world pg_backend_config clear _world pg_to_tag clear _world tags_to_pg clear _world pg_coalesce_state clear _unregister_all_process_groups when process group doesn t have explicit name only WORLD default process group can have explicit name we use global _world group_count generate name We need reset counter destruction allow consistent value generated when we re-create process groups after some trainers recover failure We only reset when WORLD being destroyed because process group good state we aren t dealing failures _world group_count = pg abort del _world pg_map pg del _world pg_names pg del _world pg_group_ranks pg del _world pg_backend_config pg pg _world pg_coalesce_state keys warnings warn Some coalesced collectives haven t been launched when ProcessGroup aborted They will cleaned stacklevel= del _world pg_coalesce_state pg tag = _world pg_to_tag get pg del _world pg_to_tag pg tag None try _world tags_to_pg tag remove pg tag startswith ptd _world tags_to_pg remove pg except Exception pass _unregister_process_group pg group_name get_rank group Optional ProcessGroup = None - int Return rank current process provided ` ` group ` ` default otherwise Rank unique identifier assigned each process within distributed process group They always consecutive integers ranging ` ` world_size ` ` Args group ProcessGroup optional The process group work If None default process group will used Returns The rank process group - part group _rank_not_in_group group - default_pg = _get_default_group group None group GroupMember WORLD default_pg rank get_group_rank group default_pg rank get_world_size group Optional ProcessGroup = None - int Return number processes current process group Args group ProcessGroup optional The process group work If None default process group will used Returns The world size process group - part group _rank_not_in_group group - _get_group_size group isend tensor torch Tensor dst Optional int = None group Optional ProcessGroup = None tag int = group_dst Optional int = None - Optional Work Send tensor asynchronously warning Modifying ` ` tensor ` ` before request completes causes undefined behavior warning ` ` tag ` ` supported NCCL backend Unlike send which blocking isend allows src == dst rank i e send Args tensor Tensor Tensor send dst int Destination rank global process group regardless ` ` group ` ` argument group ProcessGroup optional The process group work If None default process group will used tag int optional Tag match send remote recv group_dst int optional Destination rank ` ` group ` ` Invalid specify both ` ` dst ` ` ` ` group_dst ` ` Returns A distributed request object None part group group = _group_or_default_group group group_dst = _canonicalize_group_rank group dst group_dst _check_single_tensor tensor tensor _rank_not_in_group group _warn_not_in_group isend None tensor is_complex tensor = torch view_as_real tensor group send tensor group_dst tag irecv tensor torch Tensor src Optional int = None group Optional ProcessGroup = None tag int = group_src Optional int = None - Optional Work Receives tensor asynchronously warning ` ` tag ` ` supported NCCL backend Unlike recv which blocking irecv allows src == dst rank i e recv Args tensor Tensor Tensor fill received data src int optional Source rank global process group regardless ` ` group ` ` argument Will receive any process unspecified group ProcessGroup optional The process group work If None default process group will used tag int optional Tag match recv remote send group_src int optional Destination rank ` ` group ` ` Invalid specify both ` ` src ` ` ` ` group_src ` ` Returns A distributed request object None part group _check_single_tensor tensor tensor _rank_not_in_group group _warn_not_in_group irecv None tensor is_complex tensor = torch view_as_real tensor group = _group_or_default_group group src None group_src None group recv_anysource tensor tag group_src = _canonicalize_group_rank group src group_src group recv tensor group_src tag _exception_logger send tensor torch Tensor dst Optional int = None group Optional ProcessGroup = None tag int = group_dst Optional int = None - None Send tensor synchronously warning ` ` tag ` ` supported NCCL backend Args tensor Tensor Tensor send dst int Destination rank global process group regardless ` ` group ` ` argument Destination rank should same rank current process group ProcessGroup optional The process group work If None default process group will used tag int optional Tag match send remote recv group_dst int optional Destination rank ` ` group ` ` Invalid specify both ` ` dst ` ` ` ` group_dst ` ` group = _group_or_default_group group group_dst = _canonicalize_group_rank group dst group_dst _check_not_self_rank group group_dst destination work = isend tensor group=group tag=tag group_dst=group_dst work None work wait _exception_logger recv tensor torch Tensor src Optional int = None group Optional ProcessGroup = None tag int = group_src Optional int = None - int Receives tensor synchronously warning ` ` tag ` ` supported NCCL backend Args tensor Tensor Tensor fill received data src int optional Source rank global process group regardless ` ` group ` ` argument Will receive any process unspecified group ProcessGroup optional The process group work If None default process group will used tag int optional Tag match recv remote send group_src int optional Destination rank ` ` group ` ` Invalid specify both ` ` src ` ` ` ` group_src ` ` Returns Sender rank - part group work = irecv tensor src=src group=group tag=tag group_src=group_src work None - work wait src None group_src None group_src = work _source_rank group = _group_or_default_group group _check_not_self_rank group group_src source src = get_global_rank group group_src src _IllegalWork Work __getattribute__ name name is_success exception wait source_rank _source_rank result synchronize raise ValueError f Illegal call name IllegalWork object _CoalescingManager __init__ - None works list Work = append work Optional Work = None work works append work wait work works work wait contextlib contextmanager _coalescing_manager group Optional ProcessGroup = None device Optional torch device = None async_ops bool = False Context manager used coalesce collectives P P operations when possible Args group ` ProcessGroup ` optional The process group work If None default process group will used device ` torch device ` optional Default None set device there isn t ` _coalesced ` implementation backend async_ops ` bool ` optional whether coalesced ops async ops Examples xdoctest +SKIP no rank Synchronous ops _coalescing_manager i range num_colls dist all_reduce tensors i Asynchronous ops _coalescing_manager async_ops=True cm i range num_colls dist all_reduce tensors i cm wait warning func ` _coalescing_manager ` currently do support coalescing all-reduces different reduce operators e g ` ReduceOp SUM ` mixed ` ReduceOp PRODUCT ` group = group _get_default_group op_list = _world pg_coalesce_state setdefault group op_list raise ValueError ProcessGroup has non-empty op list start coalescing device group _start_coalescing device cm = _CoalescingManager yield cm work = None op_list = _world pg_coalesce_state pop group op_list Collectives supporting Fast Path coalescing captured See implementation corresponding collective APIs Currently supported - coalesced ` all_reduce ` - coalesced ` all_gather_into_tensor ` - coalesced ` reduce_scatter_tensor ` op = op_list op op all_reduce tensors = op tensor op op_list all_reduce_opts = AllreduceCoalescedOptions all_reduce_opts reduceOp = not_none op_list redop all_reduce_opts asyncOp = async_ops work = group allreduce_coalesced tensors all_reduce_opts op all_gather_into_tensor inputs = outputs = op op_list inputs append op tensor outputs append not_none op dst_tensor all_gather_opts = AllgatherOptions all_gather_opts asyncOp = async_ops work = group allgather_into_tensor_coalesced outputs inputs op reduce_scatter_tensor inputs = outputs = op op_list inputs append op tensor outputs append not_none op dst_tensor reduce_opts = ReduceScatterOptions reduce_opts reduceOp = not_none op_list redop reduce_opts asyncOp = async_ops work = group reduce_scatter_tensor_coalesced outputs inputs reduce_opts raise AssertionError f Coalescing manager does support fast-path coalescing op f yet op still recorded op list This internal error c d device Old style letting each coll inside context manager call into C++ counterpart via python binding work = group _end_coalescing device async_ops cm append work work None Backward compatible backends don t sync CPP level work wait Otherwise backend has sync ed CPP level _TimeEstimator __init__ - None estimated_time Optional float = None contextlib contextmanager _time_estimator group Optional ProcessGroup = None device Optional torch device = None Context manager used estimate time collectives Within context manager nothing actually run backend just simulates collective time only Args group ` ProcessGroup ` optional The process group work If None default process group will used device ` torch device ` optional Default None set device there isn t ` _coalesced ` implementation backend Examples xdoctest +SKIP no rank Synchronous ops _time_estimator cm i range num_colls dist all_reduce tensors i estimate time stored cm estimated_time warning func ` _time_estimator ` currently only support NCCL backend can easily extended other backends Also NCCL communicator needs created because only real communicator can we do accurate estimation The communicator internally has knowledge about links runs e g intra-node inter-node whether links NVLink PCI-e IB TODO We need also support torch inductor time estimator group = group _get_default_group device = device _get_pg_default_device group backend = group _get_backend device backend supports_time_estimate raise NotImplementedError f collective time estimator supported current version backend backend backend _start_time_estimate type ignore attr-defined cm = _TimeEstimator yield cm cm estimated_time = backend _end_time_estimate type ignore attr-defined batch_isend_irecv p p_op_list list P POp - list Work Send Receive batch tensors asynchronously list requests Process each operations ` ` p p_op_list ` ` corresponding requests NCCL Gloo UCC backend currently supported Args p p_op_list A list point-to-point operations type each operator ` ` torch distributed P POp ` ` The order isend irecv list matters needs match corresponding isend irecv remote end Returns A list distributed request objects returned calling corresponding op op_list Examples xdoctest +SKIP no rank send_tensor = torch arange dtype=torch float + rank recv_tensor = torch randn dtype=torch float send_op = dist P POp dist isend send_tensor rank + world_size recv_op = dist P POp dist irecv recv_tensor rank - + world_size world_size reqs = batch_isend_irecv send_op recv_op req reqs req wait recv_tensor tensor Rank tensor Rank note Note when API used NCCL PG backend users must set current GPU device ` torch cuda set_device ` otherwise will lead unexpected hang issues In addition API first collective call ` ` group ` ` passed ` ` dist P POp ` ` all ranks ` ` group ` ` must participate API call otherwise behavior undefined If API call first collective call ` ` group ` ` batched P P operations involving only subset ranks ` ` group ` ` allowed _check_p p_op_list p p_op_list group = p p_op_list group group None group = _get_default_group device = p p_op_list tensor device peer_kwarg op P POp - dict str int key = group_dst op op isend group_src key op group_peer type group ProcessGroup group _get_backend device supports_coalescing NCCL style coalescing _coalescing_manager group device async_ops=True cm p p_op p p_op_list p p_op op p p_op tensor group=p p_op group tag=p p_op tag peer_kwarg p p_op cm works backend support coalescing reqs = p p_op p p_op_list work = p p_op op p p_op tensor group=p p_op group tag=p p_op tag peer_kwarg p p_op work reqs append work reqs _exception_logger broadcast tensor torch Tensor src Optional int = None group Optional ProcessGroup = None async_op bool = False group_src Optional int = None Broadcasts tensor whole group ` ` tensor ` ` must have same number elements all processes participating collective Args tensor Tensor Data sent ` ` src ` ` rank current process tensor used save received data otherwise src int Source rank global process group regardless ` ` group ` ` argument group ProcessGroup optional The process group work If None default process group will used async_op bool optional Whether op should async op group_src int Source rank ` ` group ` ` Must specify one ` ` group_src ` ` ` ` src ` ` both Returns Async work handle async_op set True None async_op part group group = _group_or_default_group group group_src = _canonicalize_group_rank group src group_src return_global=False _check_single_tensor tensor tensor _rank_not_in_group group _warn_not_in_group broadcast opts = BroadcastOptions opts rootRank = group_src opts rootTensor = opts asyncOp = async_op tensor is_complex tensor = torch view_as_real tensor work = group broadcast tensor opts async_op work work None Backward compatible backends don t sync CPP level work wait Otherwise backend has sync ed CPP level _exception_logger all_reduce tensor op=ReduceOp SUM group=None async_op=False Reduces tensor data across all machines way all get final result After call ` ` tensor ` ` going bitwise identical all processes Complex tensors supported Args tensor Tensor Input output collective The function operates in-place op optional One values ` ` torch distributed ReduceOp ` ` enum Specifies operation used element-wise reductions group ProcessGroup optional The process group work If None default process group will used async_op bool optional Whether op should async op Returns Async work handle async_op set True None async_op part group Examples xdoctest +SKIP no rank All tensors below torch int type We have process groups ranks device = torch device f cuda rank tensor = torch arange dtype=torch int device=device + + rank tensor tensor device= cuda Rank tensor device= cuda Rank dist all_reduce tensor op=ReduceOp SUM tensor tensor device= cuda Rank tensor device= cuda Rank All tensors below torch cfloat type We have process groups ranks tensor = torch tensor + j + j dtype=torch cfloat device=device + rank + j tensor tensor + j + j device= cuda Rank tensor + j + j device= cuda Rank dist all_reduce tensor op=ReduceOp SUM tensor tensor + j + j device= cuda Rank tensor + j + j device= cuda Rank Dynamo has built-in logic map legacy distributed ops functional collectives Let s redirect torch function mode can mimic logic outside Dynamo e g non-strict export implements such torch function mode relevant_args = tensor has_torch_function relevant_args handle_torch_function all_reduce relevant_args tensor op=op group=group async_op=async_op _check_single_tensor tensor tensor _rank_not_in_group group _warn_not_in_group all_reduce tensor is_complex supports_complex op raise ValueError f all_reduce does support op complex tensors tensor = torch view_as_real tensor opts = AllreduceOptions opts reduceOp = op opts asyncOp = async_op group None group = _get_default_group group _world pg_coalesce_state keys We coalescing context do issue single operation just append collective representation coll = _CollOp all_reduce tensor None op None _world pg_coalesce_state group append coll async_op _IllegalWork None work = group allreduce tensor opts async_op work work None Backward compatible backends don t sync CPP level work wait Otherwise backend has sync ed CPP level _exception_logger deprecated ` torch distributed all_reduce_coalesced ` will deprecated If you must use please revisit our documentation later https pytorch org docs main distributed html#collective-functions category=FutureWarning all_reduce_coalesced tensors op=ReduceOp SUM group=None async_op=False WARNING time individual shape checking implemented across nodes For example rank node passes torch rand torch rand rank node passes torch rand torch rand torch rand allreduce operation will proceed without complaint erroneous outputs This lack shape checking results significant performance improvements users function should take extra care ensure each node passes tensors whose shapes match across nodes Reduces each tensor tensors residing same device across all machines such way all get final result After call each tensor tensors going bitwise identical all processes Complex tensors supported Args tensors Union List Tensor Tensor Input output collective The function operates in-place op Optional ReduceOp One values ` ` torch distributed ReduceOp ` ` enum Specifies operation used element-wise reductions group ProcessGroup optional The process group work If None default process group will used async_op Optional bool Whether op should async op Returns Async work handle async_op set True None async_op part group isinstance tensors torch Tensor tensors = tensors _check_tensor_list tensors tensor _ensure_all_tensors_same_dtype tensors _rank_not_in_group group _warn_not_in_group all_reduce_coalesced any t is_complex t tensors supports_complex op raise ValueError f all_reduce does support op complex tensors tensors = t t is_complex torch view_as_real t t tensors opts = AllreduceCoalescedOptions opts reduceOp = op opts asyncOp = async_op group = group _get_default_group work = group allreduce_coalesced tensors opts async_op work get_future work None Backward compatible backends don t sync CPP level work wait Otherwise backend has sync ed CPP level _exception_logger reduce tensor torch Tensor dst Optional int = None op=ReduceOp SUM group Optional ProcessGroup = None async_op bool = False group_dst Optional int = None Reduces tensor data across all machines Only process rank ` ` dst ` ` going receive final result Args tensor Tensor Input output collective The function operates in-place dst int Destination rank global process group regardless ` ` group ` ` argument op optional One values ` ` torch distributed ReduceOp ` ` enum Specifies operation used element-wise reductions group ProcessGroup optional The process group work If None default process group will used async_op bool optional Whether op should async op group_dst int Destination rank ` ` group ` ` Must specify one ` ` group_dst ` ` ` ` dst ` ` both Returns Async work handle async_op set True None async_op part group group = _group_or_default_group group group_dst = _canonicalize_group_rank group dst group_dst return_global=False _check_single_tensor tensor tensor _rank_not_in_group group _warn_not_in_group reduce opts = ReduceOptions opts reduceOp = op opts rootRank = group_dst opts asyncOp = async_op work = group reduce tensor opts async_op work work None Backward compatible backends don t sync CPP level work wait Otherwise backend has sync ed CPP level _object_to_tensor obj device group _WaitCounter pytorch wait_counter c d _object_to_tensor guard f = io BytesIO _pickler f dump obj byte_storage = torch ByteStorage _from_buffer f getvalue type ignore attr-defined Do replace ` torch ByteTensor ` ` torch LongTensor ` torch tensor specifying dtype Otherwise will cause X slowdown See https github com pytorch pytorch issues byte_tensor = torch ByteTensor byte_storage device get_debug_level == DebugLevel DETAIL is_nccl_available backend = get_backend group backend == Backend NCCL hash = torch _C _distributed_c d _hash_tensors byte_tensor logger warning _object_to_tensor size s hash value s byte_tensor numel hash local_size = torch LongTensor byte_tensor numel device byte_tensor local_size _tensor_to_object tensor tensor_size group _WaitCounter pytorch wait_counter c d _tensor_to_object guard get_debug_level == DebugLevel DETAIL is_nccl_available backend = get_backend group backend == Backend NCCL hash = torch _C _distributed_c d _hash_tensors tensor logger warning _tensor_to_object size s hash value s tensor numel hash tensor = tensor cpu buf = tensor numpy tobytes tensor_size _unpickler io BytesIO buf load _exception_logger all_gather_object object_list obj group=None Gathers picklable objects whole group into list Similar func ` all_gather ` Python objects can passed Note object must picklable order gathered Args object_list list Any Output list It should correctly sized size group collective will contain output obj Any Pickable Python object broadcast current process group ProcessGroup optional The process group work If None default process group will used Default ` ` None ` ` Returns None If calling rank part group output collective will populated into input ` ` object_list ` ` If calling rank part group passed ` ` object_list ` ` will unmodified note Note API differs slightly func ` all_gather ` collective since does provide ` ` async_op ` ` handle thus will blocking call note For NCCL-based processed groups internal tensor representations objects must moved GPU device before communication takes place In case device used given ` ` torch cuda current_device ` ` user s responsibility ensure set so each rank has individual GPU via ` ` torch cuda set_device ` ` warning Object collectives have number serious performance scalability limitations See ref ` object_collectives ` details warning func ` all_gather_object ` uses ` ` pickle ` ` module implicitly which known insecure It possible construct malicious pickle data which will execute arbitrary code during unpickling Only call function data you trust warning Calling func ` all_gather_object ` GPU tensors well supported inefficient incurs GPU - CPU transfer since tensors would pickled Please consider using func ` all_gather ` instead Example xdoctest +SKIP need process group init Note Process group initialization omitted each rank torch distributed dist Assumes world_size gather_objects = foo any picklable object output = None _ gather_objects dist all_gather_object output gather_objects dist get_rank output foo _rank_not_in_group group _warn_not_in_group all_gather_object current_device = _get_object_coll_device group input_tensor local_size = _object_to_tensor obj current_device group Gather all local sizes This so we can find max size index until correct size when deserializing tensors group_size = get_world_size group=group object_sizes_tensor = torch zeros group_size dtype=torch long device=current_device object_size_list = object_sizes_tensor i unsqueeze dim= i range group_size Allgather tensor sizes all_gather object_size_list local_size group=group max_object_size = int max object_size_list item type ignore type-var Resize tensor max size across all ranks input_tensor resize_ max_object_size coalesced_output_tensor = torch empty max_object_size group_size dtype=torch uint device=current_device Output tensors nonoverlapping views coalesced_output_tensor output_tensors = coalesced_output_tensor max_object_size i max_object_size i + i range group_size all_gather output_tensors input_tensor group=group Deserialize outputs back object i tensor enumerate output_tensors tensor = tensor type torch uint tensor_size = object_size_list i object_list i = _tensor_to_object tensor tensor_size group _exception_logger gather_object obj Any object_gather_list Optional list Any = None dst Optional int = None group Optional ProcessGroup = None group_dst Optional int = None Gathers picklable objects whole group single process Similar func ` gather ` Python objects can passed Note object must picklable order gathered Args obj Any Input object Must picklable object_gather_list list Any Output list On ` ` dst ` ` rank should correctly sized size group collective will contain output Must ` ` None ` ` non-dst ranks default ` ` None ` ` dst int optional Destination rank global process group regardless ` ` group ` ` argument If both ` ` dst ` ` ` ` group_dst ` ` None default global rank group ProcessGroup optional The process group work If None default process group will used Default ` ` None ` ` group_dst int optional Destination rank ` ` group ` ` Invalid specify both ` ` dst ` ` ` ` group_dst ` ` Returns None On ` ` dst ` ` rank ` ` object_gather_list ` ` will contain output collective note Note API differs slightly gather collective since does provide async_op handle thus will blocking call note For NCCL-based processed groups internal tensor representations objects must moved GPU device before communication takes place In case device used given ` ` torch cuda current_device ` ` user s responsibility ensure set so each rank has individual GPU via ` ` torch cuda set_device ` ` warning Object collectives have number serious performance scalability limitations See ref ` object_collectives ` details warning func ` gather_object ` uses ` ` pickle ` ` module implicitly which known insecure It possible construct malicious pickle data which will execute arbitrary code during unpickling Only call function data you trust warning Calling func ` gather_object ` GPU tensors well supported inefficient incurs GPU - CPU transfer since tensors would pickled Please consider using func ` gather ` instead Example xdoctest +SKIP need process group init Note Process group initialization omitted each rank torch distributed dist Assumes world_size gather_objects = foo any picklable object output = None _ gather_objects dist gather_object gather_objects dist get_rank output dist get_rank == None dst= On rank output foo group = _group_or_default_group group dst None group_dst None dst = group_dst = _canonicalize_group_rank group dst group_dst return_global=False _rank_not_in_group group _warn_not_in_group gather_object Ensure object_gather_list specified appropriately my_group_rank = group rank _validate_output_list_for_rank my_group_rank group_dst object_gather_list current_device = _get_object_coll_device group input_tensor local_size = _object_to_tensor obj current_device group Gather all local sizes This so we can find max size index until correct size when deserializing tensors group_size = get_world_size group=group object_sizes_tensor = torch zeros group_size dtype=torch long device=current_device object_size_list = object_sizes_tensor i unsqueeze dim= i range group_size Allgather tensor sizes An all-gather needed here despite being gather since each rank needs broadcast tensor same maximal size all_gather object_size_list local_size group=group max_object_size = int max object_size_list item type ignore type-var Resize tensor max size across all ranks input_tensor resize_ max_object_size Avoid populating output tensors result won t gathered rank my_group_rank == group_dst coalesced_output_tensor = torch empty max_object_size group_size dtype=torch uint device=current_device Output tensors nonoverlapping views coalesced_output_tensor output_tensors = coalesced_output_tensor max_object_size i max_object_size i + i range group_size All ranks call gather equal-sized tensors gather input_tensor gather_list=output_tensors my_group_rank == group_dst None type ignore possibly-undefined group_dst=group_dst group=group my_group_rank = group_dst assert object_gather_list None Must provide object_gather_list dst rank pyrefly ignore unbound-name i tensor enumerate output_tensors tensor = tensor type torch uint tensor_size = object_size_list i object_gather_list i = _tensor_to_object tensor tensor_size group _exception_logger send_object_list object_list list Any dst Optional int = None group Optional ProcessGroup = None device Optional torch device = None group_dst Optional int = None use_batch bool = False Sends picklable objects ` ` object_list ` ` synchronously Similar func ` send ` Python objects can passed Note all objects ` ` object_list ` ` must picklable order sent Args object_list List Any List input objects sent Each object must picklable Receiver must provide lists equal sizes dst int Destination rank send ` ` object_list ` ` Destination rank based global process group regardless ` ` group ` ` argument group ProcessGroup optional The process group work If None default process group will used Default ` ` None ` ` device ` ` torch device ` ` optional If None objects serialized converted tensors which moved ` ` device ` ` before sending Default ` ` None ` ` group_dst int optional Destination rank ` ` group ` ` Must specify one ` ` dst ` ` ` ` group_dst ` ` both use_batch bool optional If True use batch p p operations instead regular send operations This avoids initializing -rank communicators uses existing entire group communicators See batch_isend_irecv usage assumptions Default ` ` False ` ` Returns ` ` None ` ` note For NCCL-based process groups internal tensor representations objects must moved GPU device before communication takes place In case device used given ` ` torch cuda current_device ` ` user s responsibility ensure set so each rank has individual GPU via ` ` torch cuda set_device ` ` warning Object collectives have number serious performance scalability limitations See ref ` object_collectives ` details warning func ` send_object_list ` uses ` ` pickle ` ` module implicitly which known insecure It possible construct malicious pickle data which will execute arbitrary code during unpickling Only call function data you trust warning Calling func ` send_object_list ` GPU tensors well supported inefficient incurs GPU - CPU transfer since tensors would pickled Please consider using func ` send ` instead Example xdoctest +SKIP need process group init Note Process group initialization omitted each rank torch distributed dist Assumes backend NCCL device = torch device cpu dist get_rank == Assumes world_size objects = foo any picklable object dist send_object_list objects dst= device=device objects = None None None dist recv_object_list objects src= device=device objects foo group = _group_or_default_group group group_dst = _canonicalize_group_rank group dst group_dst _check_not_self_rank group group_dst destination _rank_not_in_group group _warn_not_in_group send_object_list Current device selection To preserve backwards compatibility ` ` device ` ` default ` ` None ` ` which case we run current logic device selection i e ` ` current_device ` ` CUDA backend NCCL otherwise CPU device In case ` ` None ` ` we move size object tensors sent device current_device = device _get_object_coll_device group Serialize object_list elements tensors src rank tensor_list size_list = zip _object_to_tensor obj current_device group obj object_list object_sizes_tensor = torch cat size_list Send object sizes use_batch batch_isend_irecv P POp isend object_sizes_tensor group_peer=group_dst group=group pop wait send object_sizes_tensor group_dst=group_dst group=group Concatenate send serialized object tensors Note torch cat will do extra memory copy current device tensor_list has only one element we can skip copy len tensor_list == type ignore possibly-undefined object_tensor = tensor_list object_tensor = torch cat tensor_list use_batch batch_isend_irecv P POp isend object_tensor group_peer=group_dst group=group pop wait send object_tensor group_dst=group_dst group=group _exception_logger recv_object_list object_list list Any src Optional int = None group Optional ProcessGroup = None device Optional torch device = None group_src Optional int = None use_batch bool = False Receives picklable objects ` ` object_list ` ` synchronously Similar func ` recv ` can receive Python objects Args object_list List Any List objects receive into Must provide list sizes equal size list being sent src int optional Source rank which recv ` ` object_list ` ` Source rank based global process group regardless ` ` group ` ` argument Will receive any rank set None Default ` ` None ` ` group ProcessGroup optional The process group work If None default process group will used Default ` ` None ` ` device ` ` torch device ` ` optional If None receives device Default ` ` None ` ` group_src int optional Destination rank ` ` group ` ` Invalid specify both ` ` src ` ` ` ` group_src ` ` use_batch bool optional If True use batch p p operations instead regular send operations This avoids initializing -rank communicators uses existing entire group communicators See batch_isend_irecv usage assumptions Default ` ` False ` ` Returns Sender rank - rank part group If rank part group ` ` object_list ` ` will contain sent objects ` ` src ` ` rank note For NCCL-based process groups internal tensor representations objects must moved GPU device before communication takes place In case device used given ` ` torch cuda current_device ` ` user s responsibility ensure set so each rank has individual GPU via ` ` torch cuda set_device ` ` warning Object collectives have number serious performance scalability limitations See ref ` object_collectives ` details warning func ` recv_object_list ` uses ` ` pickle ` ` module implicitly which known insecure It possible construct malicious pickle data which will execute arbitrary code during unpickling Only call function data you trust warning Calling func ` recv_object_list ` GPU tensors well supported inefficient incurs GPU - CPU transfer since tensors would pickled Please consider using func ` recv ` instead Example xdoctest +SKIP need process group init Note Process group initialization omitted each rank torch distributed dist Assumes backend NCCL device = torch device cpu dist get_rank == Assumes world_size objects = foo any picklable object dist send_object_list objects dst= device=device objects = None None None dist recv_object_list objects src= device=device objects foo group = _group_or_default_group group group_src = _canonicalize_group_rank group src group_src _check_not_self_rank group group_src source _rank_not_in_group group _warn_not_in_group recv_object_list - Current device selection To preserve backwards compatibility ` ` device ` ` default ` ` None ` ` which case we run current logic device selection i e ` ` current_device ` ` CUDA backend NCCL otherwise CPU device In case ` ` None ` ` we move size object tensors received device current_device = device _get_object_coll_device group object_sizes_tensor = torch empty len object_list dtype=torch long device=current_device Receive object sizes use_batch work = batch_isend_irecv P POp irecv object_sizes_tensor group_peer=group_src group=group pop work wait rank_sizes = get_global_rank group group_src rank_sizes = recv object_sizes_tensor group=group group_src=group_src Tensor receive serialized objects into object_tensor = torch empty type ignore call-overload torch sum object_sizes_tensor item type ignore arg-type dtype=torch uint device=current_device use_batch work = batch_isend_irecv P POp irecv object_tensor group_peer=group_src group=group pop work wait rank_objects = get_global_rank group group_src rank_objects = recv object_tensor group=group group_src=group_src assert rank_sizes == rank_objects Mismatch ranks object sizes objects Deserialize objects using their stored sizes offset = i obj_size enumerate object_sizes_tensor obj_view = object_tensor offset offset + obj_size obj_view = obj_view type torch uint offset += obj_size object_list i = _tensor_to_object obj_view obj_size group rank_objects _exception_logger broadcast_object_list object_list list Any src Optional int = None group Optional ProcessGroup = None device Optional torch device = None group_src Optional int = None Broadcasts picklable objects ` ` object_list ` ` whole group Similar func ` broadcast ` Python objects can passed Note all objects ` ` object_list ` ` must picklable order broadcasted Args object_list List Any List input objects broadcast Each object must picklable Only objects ` ` src ` ` rank will broadcast each rank must provide lists equal sizes src int Source rank which broadcast ` ` object_list ` ` Source rank based global process group regardless ` ` group ` ` argument group ProcessGroup optional The process group work If None default process group will used Default ` ` None ` ` device ` ` torch device ` ` optional If None objects serialized converted tensors which moved ` ` device ` ` before broadcasting Default ` ` None ` ` group_src int Source rank ` ` group ` ` Must specify one ` ` group_src ` ` ` ` src ` ` both Returns ` ` None ` ` If rank part group ` ` object_list ` ` will contain broadcasted objects ` ` src ` ` rank note For NCCL-based process groups internal tensor representations objects must moved GPU device before communication takes place In case device used given ` ` torch cuda current_device ` ` user s responsibility ensure set so each rank has individual GPU via ` ` torch cuda set_device ` ` note Note API differs slightly func ` broadcast ` collective since does provide ` ` async_op ` ` handle thus will blocking call warning Object collectives have number serious performance scalability limitations See ref ` object_collectives ` details warning func ` broadcast_object_list ` uses ` ` pickle ` ` module implicitly which known insecure It possible construct malicious pickle data which will execute arbitrary code during unpickling Only call function data you trust warning Calling func ` broadcast_object_list ` GPU tensors well supported inefficient incurs GPU - CPU transfer since tensors would pickled Please consider using func ` broadcast ` instead Example xdoctest +SKIP need process group init Note Process group initialization omitted each rank torch distributed dist dist get_rank == Assumes world_size objects = foo any picklable object objects = None None None Assumes backend NCCL device = torch device cpu dist broadcast_object_list objects src= device=device objects foo group = _group_or_default_group group src None group_src None src = group_src = _canonicalize_group_rank group src group_src return_global=False _rank_not_in_group group _warn_not_in_group broadcast_object_list Current device selection To preserve backwards compatibility ` ` device ` ` default ` ` None ` ` which case we run current logic device selection i e ` ` current_device ` ` CUDA backend NCCL otherwise CPU device In case ` ` None ` ` we move size object tensors broadcasted device current_device = device _get_object_coll_device group my_group_rank = group rank Serialize object_list elements tensors src rank my_group_rank == group_src tensor_list size_list = zip _object_to_tensor obj current_device group obj object_list object_sizes_tensor = torch cat size_list object_sizes_tensor = torch empty len object_list dtype=torch long device=current_device Broadcast object sizes broadcast object_sizes_tensor group_src=group_src group=group Concatenate broadcast serialized object tensors Note torch cat will do extra memory copy current device tensor_list has only one element we can skip copy my_group_rank == group_src len tensor_list == type ignore possibly-undefined pyrefly ignore unbound-name object_tensor = tensor_list pyrefly ignore unbound-name object_tensor = torch cat tensor_list object_tensor = torch empty type ignore call-overload torch sum object_sizes_tensor item type ignore arg-type dtype=torch uint device=current_device broadcast object_tensor group_src=group_src group=group Deserialize objects using their stored sizes offset = my_group_rank = group_src i obj_size enumerate object_sizes_tensor obj_view = object_tensor offset offset + obj_size obj_view = obj_view type torch uint offset += obj_size object_list i = _tensor_to_object obj_view obj_size group _exception_logger scatter_object_list scatter_object_output_list list Any scatter_object_input_list Optional list Any = None src Optional int = None group Optional ProcessGroup = None group_src Optional int = None Scatters picklable objects ` ` scatter_object_input_list ` ` whole group Similar func ` scatter ` Python objects can passed On each rank scattered object will stored first element ` ` scatter_object_output_list ` ` Note all objects ` ` scatter_object_input_list ` ` must picklable order scattered Args scatter_object_output_list List Any Non-empty list whose first element will store object scattered rank scatter_object_input_list List Any optional List input objects scatter Each object must picklable Only objects ` ` src ` ` rank will scattered argument can ` ` None ` ` non-src ranks src int Source rank which scatter ` ` scatter_object_input_list ` ` Source rank based global process group regardless ` ` group ` ` argument If both ` ` src ` ` ` ` group_src ` ` None default global rank group ProcessGroup optional The process group work If None default process group will used Default ` ` None ` ` group_src int optional Source rank ` ` group ` ` Invalid specify both ` ` src ` ` ` ` group_src ` ` Returns ` ` None ` ` If rank part group ` ` scatter_object_output_list ` ` will have its first element set scattered object rank note Note API differs slightly scatter collective since does provide ` ` async_op ` ` handle thus will blocking call warning Object collectives have number serious performance scalability limitations See ref ` object_collectives ` details warning func ` scatter_object_list ` uses ` ` pickle ` ` module implicitly which known insecure It possible construct malicious pickle data which will execute arbitrary code during unpickling Only call function data you trust warning Calling func ` scatter_object_list ` GPU tensors well supported inefficient incurs GPU - CPU transfer since tensors would pickled Please consider using func ` scatter ` instead Example xdoctest +SKIP need process group init Note Process group initialization omitted each rank torch distributed dist dist get_rank == Assumes world_size objects = foo any picklable object Can any list non-src ranks elements used objects = None None None output_list = None dist scatter_object_list output_list objects src= Rank i gets objects i For example rank output_list group = _group_or_default_group group src None group_src None src = group_src = _canonicalize_group_rank group src group_src return_global=False _rank_not_in_group group _warn_not_in_group scatter_object_list isinstance scatter_object_output_list list len scatter_object_output_list raise ValueError Expected argument scatter_object_output_list list size least my_group_rank = group rank pg_device = _get_object_coll_device group my_group_rank == group_src scatter_object_input_list None raise ValueError source rank must provide non-None scatter_object_input_list tensor_list tensor_sizes = zip _object_to_tensor obj pg_device group obj scatter_object_input_list tensor_list tensor_sizes = list tensor_list list tensor_sizes Src rank broadcasts maximum tensor size This because all ranks expected call into scatter equal-sized tensors max_tensor_size = max tensor_sizes type ignore possibly-undefined tensor tensor_list type ignore possibly-undefined tensor resize_ max_tensor_size max_tensor_size = torch tensor dtype=torch long device=pg_device broadcast max_tensor_size group_src=group_src group=group Scatter actual serialized objects pyrefly ignore no-matching-overload output_tensor = torch empty max_tensor_size item dtype=torch uint device=pg_device scatter output_tensor scatter_list=None my_group_rank = group_src tensor_list type ignore possibly-undefined group_src=group_src group=group Scatter per-object sizes trim tensors when deserializing back object obj_tensor_size = torch tensor dtype=torch long device=pg_device scatter obj_tensor_size scatter_list=None my_group_rank = group_src tensor_sizes type ignore possibly-undefined group_src=group_src group=group Deserialize back object scatter_object_output_list = _tensor_to_object output_tensor obj_tensor_size group _exception_logger all_gather tensor_list tensor group=None async_op=False Gathers tensors whole group list Complex uneven sized tensors supported Args tensor_list list Tensor Output list It should contain correctly-sized tensors used output collective Uneven sized tensors supported tensor Tensor Tensor broadcast current process group ProcessGroup optional The process group work If None default process group will used async_op bool optional Whether op should async op Returns Async work handle async_op set True None async_op part group Examples xdoctest +SKIP need process group init All tensors below torch int dtype We have process groups ranks device = torch device f cuda rank tensor_list = torch zeros dtype=torch int device=device _ range tensor_list tensor device= cuda tensor device= cuda Rank tensor device= cuda tensor device= cuda Rank tensor = torch arange dtype=torch int device=device + + rank tensor tensor device= cuda Rank tensor device= cuda Rank dist all_gather tensor_list tensor tensor_list tensor device= cuda tensor device= cuda Rank tensor device= cuda tensor device= cuda Rank All tensors below torch cfloat dtype We have process groups ranks tensor_list = torch zeros dtype=torch cfloat device=device _ range tensor_list tensor + j + j device= cuda tensor + j + j device= cuda Rank tensor + j + j device= cuda tensor + j + j device= cuda Rank tensor = torch tensor + j + j dtype=torch cfloat device=device + rank + j tensor tensor + j + j device= cuda Rank tensor + j + j device= cuda Rank dist all_gather tensor_list tensor tensor_list tensor + j + j device= cuda tensor + j + j device= cuda Rank tensor + j + j device= cuda tensor + j + j device= cuda Rank Dynamo has built-in logic map legacy distributed ops functional collectives Let s redirect torch function mode can mimic logic outside Dynamo e g non-strict export implements such torch function mode relevant_args = tensor has_torch_function relevant_args handle_torch_function all_gather relevant_args tensor_list tensor group=group async_op=async_op _check_tensor_list tensor_list tensor_list _check_single_tensor tensor tensor _ensure_all_tensors_same_dtype tensor_list tensor _rank_not_in_group group _warn_not_in_group all_gather tensor_list = t t is_complex torch view_as_real t t tensor_list tensor = tensor tensor is_complex torch view_as_real tensor group = group _get_default_group opts = AllgatherOptions opts asyncOp = async_op work = group allgather tensor_list tensor opts async_op work work None Backward compatible backends don t sync CPP level work wait Otherwise backend has sync ed CPP level _exception_logger all_gather_into_tensor output_tensor input_tensor group=None async_op=False Gather tensors all ranks put them single output tensor This function requires all tensors same size each process Args output_tensor Tensor Output tensor accommodate tensor elements all ranks It must correctly sized have one following forms i concatenation all input tensors along primary dimension definition concatenation see ` ` torch cat ` ` ii stack all input tensors along primary dimension definition stack see ` ` torch stack ` ` Examples below may better explain supported output forms input_tensor Tensor Tensor gathered current rank Different ` ` all_gather ` ` API input tensors API must have same size across all ranks group ProcessGroup optional The process group work If None default process group will used async_op bool optional Whether op should async op Returns Async work handle async_op set True None async_op part group Examples xdoctest +SKIP need process group init All tensors below torch int dtype CUDA devices We have two ranks device = torch device f cuda rank tensor_in = torch arange dtype=torch int device=device + + rank tensor_in tensor device= cuda Rank tensor device= cuda Rank Output concatenation form tensor_out = torch zeros world_size dtype=torch int device=device dist all_gather_into_tensor tensor_out tensor_in tensor_out tensor device= cuda Rank tensor device= cuda Rank Output stack form tensor_out = torch zeros world_size dtype=torch int device=device dist all_gather_into_tensor tensor_out tensor_in tensor_out tensor device= cuda Rank tensor device= cuda Rank Dynamo has built-in logic map legacy distributed ops functional collectives Let s redirect torch function mode can mimic logic outside Dynamo e g non-strict export implements such torch function mode relevant_args = input_tensor has_torch_function relevant_args handle_torch_function all_gather_into_tensor relevant_args output_tensor input_tensor group=group async_op=async_op _check_single_tensor input_tensor input_tensor _check_single_tensor output_tensor output_tensor _rank_not_in_group group _warn_not_in_group all_gather_into_tensor output_tensor = output_tensor output_tensor is_complex torch view_as_real output_tensor input_tensor = input_tensor input_tensor is_complex torch view_as_real input_tensor opts = AllgatherOptions opts asyncOp = async_op group = group _get_default_group group _world pg_coalesce_state keys We coalescing context do issue single operation just append collective representation coll = _CollOp all_gather_into_tensor input_tensor output_tensor _world pg_coalesce_state group append coll async_op _IllegalWork None work = group _allgather_base output_tensor input_tensor opts async_op work work None Backward compatible backends don t sync CPP level work wait Otherwise backend has sync ed CPP level _exception_logger deprecated ` torch distributed _all_gather_base ` private function will deprecated Please use ` torch distributed all_gather_into_tensor ` instead category=FutureWarning _all_gather_base output_tensor input_tensor group=None async_op=False Single tensor all gather Gathers single tensor all ranks puts them single output tensor Args output_tensor Tensor Output tensor It should contain correctly-sized tensors used output collective input_tensor Tensor Tensor broadcast current process group ProcessGroup optional The process group work If None default process group will used async_op bool optional Whether op should async op Returns Async work handle async_op set True None async_op part group warning ` _all_gather_base ` private function Users should use ` all_gather_into_tensor ` instead all_gather_into_tensor output_tensor input_tensor group async_op _exception_logger deprecated ` torch distributed all_gather_coalesced ` will deprecated If you must use please revisit our documentation later https pytorch org docs main distributed html#collective-functions category=FutureWarning all_gather_coalesced output_tensor_lists input_tensor_list group=None async_op=False Gathers input tensors whole group list coalesced manner Complex tensors supported Args output_tensor_lists list list Tensor Output list It should contain correctly-sized tensors used output collective input_tensor_list list Tensor Tensors broadcast current process At least one tensor has non empty group ProcessGroup optional The process group work If None default process group will used async_op bool optional Whether op should async op Returns Async work handle async_op set True None async_op part group Example we have process groups ranks rank passes input_tensor_list = output_tensor_lists = - - - - - - - - - - - - - - rank passes input_tensor_list = output_tensor_lists = - - - - - - - - - - - - - - both rank get output_tensor_lists = WARNING time individual shape checking implemented across nodes For example rank node passes torch rand torch rand rank node passes torch rand torch rand torch rand all_gather_coalesced operation will proceed without complaint erroneous outputs This lack shape checking results significant performance improvements users function should take extra care ensure each node passes tensors whose shapes match across nodes We only check basic compatibility C++ params here C++ code will do shape type checking _rank_not_in_group group _warn_not_in_group all_gather_coalesced _check_tensor_list input_tensor_list input_tensor_list _ensure_all_tensors_same_dtype input_tensor_list isinstance output_tensor_lists list raise TypeError Invalid function argument output_tensor_lists should list output_tensor_list output_tensor_lists _check_tensor_list output_tensor_list output_tensor_lists _ensure_all_tensors_same_dtype output_tensor_list output_tensor_lists = t t is_complex torch view_as_real t t l l output_tensor_lists input_tensor_list = t t is_complex torch view_as_real t t input_tensor_list group = group _get_default_group opts = AllgatherOptions opts asyncOp = async_op work = group allgather_coalesced output_tensor_lists input_tensor_list opts async_op work get_future work None Backward compatible backends don t sync CPP level work wait Otherwise backend has sync ed CPP level _validate_output_list_for_rank my_rank dst gather_list dst == my_rank gather_list raise ValueError Argument ` ` gather_list ` ` must specified destination rank gather_list raise ValueError Argument ` ` gather_list ` ` must NOT specified non-destination ranks _exception_logger gather tensor torch Tensor gather_list Optional list torch Tensor = None dst Optional int = None group Optional ProcessGroup = None async_op bool = False group_dst Optional int = None Gathers list tensors single process This function requires all tensors same size each process Args tensor Tensor Input tensor gather_list list Tensor optional List appropriately same-sized tensors use gathered data default None must specified destination rank dst int optional Destination rank global process group regardless ` ` group ` ` argument If both ` ` dst ` ` ` ` group_dst ` ` None default global rank group ProcessGroup optional The process group work If None default process group will used async_op bool optional Whether op should async op group_dst int optional Destination rank ` ` group ` ` Invalid specify both ` ` dst ` ` ` ` group_dst ` ` Returns Async work handle async_op set True None async_op part group note Note all Tensors gather_list must have same size Example xdoctest +SKIP no rank We have process groups ranks tensor_size = device = torch device f cuda rank tensor = torch ones tensor_size device=device + rank dist get_rank == gather_list = torch zeros_like tensor device=device i range gather_list = None dist gather tensor gather_list dst= Rank gets gathered data gather_list tensor device= cuda tensor device= cuda Rank None Rank _check_single_tensor tensor tensor Parameter ` ` gather_list ` ` may left unspecified non-dst ranks gather_list _check_tensor_list gather_list gather_list gather_list = _ensure_all_tensors_same_dtype tensor gather_list group = _group_or_default_group group _rank_not_in_group group _warn_not_in_group gather dst None group_dst None dst = group_dst = _canonicalize_group_rank group dst group_dst return_global=False my_group_rank = group rank _validate_output_list_for_rank my_group_rank group_dst gather_list output_tensors = gather_list group_dst == my_group_rank input_tensors = tensor opts = GatherOptions opts rootRank = group_dst opts asyncOp = async_op work = group gather output_tensors input_tensors opts async_op work work None Backward compatible backends don t sync CPP level work wait Otherwise backend has sync ed CPP level _exception_logger scatter tensor torch Tensor scatter_list Optional list torch Tensor = None src Optional int = None group Optional ProcessGroup = None async_op bool = False group_src Optional int = None Scatters list tensors all processes group Each process will receive exactly one tensor store its data ` ` tensor ` ` argument Complex tensors supported Args tensor Tensor Output tensor scatter_list list Tensor List tensors scatter default None must specified source rank src int Source rank global process group regardless ` ` group ` ` argument If both ` ` src ` ` ` ` group_src ` ` None default global rank group ProcessGroup optional The process group work If None default process group will used async_op bool optional Whether op should async op group_src int optional Source rank ` ` group ` ` Invalid specify both ` ` src ` ` ` ` group_src ` ` Returns Async work handle async_op set True None async_op part group note Note all Tensors scatter_list must have same size Example xdoctest +SKIP need process group init Note Process group initialization omitted each rank torch distributed dist tensor_size = device = torch device f cuda rank output_tensor = torch zeros tensor_size device=device dist get_rank == Assumes world_size Only tensors all which must same size t_ones = torch ones tensor_size device=device t_fives = torch ones tensor_size device=device scatter_list = t_ones t_fives scatter_list = None dist scatter output_tensor scatter_list src= Rank i gets scatter_list i output_tensor tensor device= cuda Rank tensor device= cuda Rank _check_single_tensor tensor tensor Parameter ` ` scatter_list ` ` may left unspecified non-src ranks scatter_list _check_tensor_list scatter_list scatter_list scatter_list = _ensure_all_tensors_same_dtype tensor scatter_list group = _group_or_default_group group src None group_src None src = group_src = _canonicalize_group_rank group src group_src return_global=False _rank_not_in_group group _warn_not_in_group scatter scatter_list = t t is_complex torch view_as_real t t scatter_list tensor = tensor tensor is_complex torch view_as_real tensor my_group_rank = group rank group_src == my_group_rank scatter_list raise ValueError Argument ` ` scatter_list ` ` must specified source rank input_tensors = scatter_list output_tensors = tensor scatter_list raise ValueError Argument ` ` scatter_list ` ` must NOT specified non-source ranks input_tensors = output_tensors = tensor opts = ScatterOptions opts rootRank = group_src opts asyncOp = async_op work = group scatter output_tensors input_tensors opts async_op work work None Backward compatible backends don t sync CPP level work wait Otherwise backend has sync ed CPP level _exception_logger reduce_scatter output input_list op=ReduceOp SUM group=None async_op=False Reduces then scatters list tensors all processes group Args output Tensor Output tensor input_list list Tensor List tensors reduce scatter op optional One values ` ` torch distributed ReduceOp ` ` enum Specifies operation used element-wise reductions group ProcessGroup optional The process group work If None default process group will used async_op bool optional Whether op should async op Returns Async work handle async_op set True None async_op part group _check_single_tensor output output _check_tensor_list input_list input_list _ensure_all_tensors_same_dtype output input_list _rank_not_in_group group _warn_not_in_group reduce_scatter opts = ReduceScatterOptions opts reduceOp = op opts asyncOp = async_op group = group _get_default_group work = group reduce_scatter output input_list opts async_op work work None Backward compatible backends don t sync CPP level work wait Otherwise backend has sync ed CPP level _exception_logger reduce_scatter_tensor output input op=ReduceOp SUM group=None async_op=False Reduces then scatters tensor all ranks group Args output Tensor Output tensor It should have same size across all ranks input Tensor Input tensor reduced scattered Its size should output tensor size times world size The input tensor can have one following shapes i concatenation output tensors along primary dimension ii stack output tensors along primary dimension For definition concatenation see ` ` torch cat ` ` For definition stack see ` ` torch stack ` ` group ProcessGroup optional The process group work If None default process group will used async_op bool optional Whether op should async op Returns Async work handle async_op set True None async_op part group Examples xdoctest +SKIP need process group init All tensors below torch int dtype CUDA devices We have two ranks device = torch device f cuda rank tensor_out = torch zeros dtype=torch int device=device Input concatenation form tensor_in = torch arange world_size dtype=torch int device=device tensor_in tensor device= cuda Rank tensor device= cuda Rank dist reduce_scatter_tensor tensor_out tensor_in tensor_out tensor device= cuda Rank tensor device= cuda Rank Input stack form tensor_in = torch reshape tensor_in world_size tensor_in tensor device= cuda Rank tensor device= cuda Rank dist reduce_scatter_tensor tensor_out tensor_in tensor_out tensor device= cuda Rank tensor device= cuda Rank Dynamo has built-in logic map legacy distributed ops functional collectives Let s redirect torch function mode can mimic logic outside Dynamo e g non-strict export implements such torch function mode relevant_args = input has_torch_function relevant_args handle_torch_function reduce_scatter_tensor relevant_args output input op=op group=group async_op=async_op _check_single_tensor output output _check_single_tensor input input _rank_not_in_group group _warn_not_in_group reduce_scatter_tensor opts = ReduceScatterOptions opts reduceOp = op opts asyncOp = async_op group = group _get_default_group Check we coalescing context If we do issue single operation just append collective representation group _world pg_coalesce_state keys coll = _CollOp reduce_scatter_tensor input output op None _world pg_coalesce_state group append coll async_op _IllegalWork None work = group _reduce_scatter_base output input opts async_op work work None Backward compatible backends don t sync CPP level work wait Otherwise backend has sync ed CPP level deprecated ` torch distributed _reduce_scatter_base ` private function will deprecated Please use ` torch distributed reduce_scatter_tensor ` instead category=FutureWarning _reduce_scatter_base output input op=ReduceOp SUM group=None async_op=False Reduces then scatters flattened tensor all processes group Args output Tensor Output tensor input Tensor Input tensor size output tensor size times world size group ProcessGroup optional The process group work If None default process group will used async_op bool optional Whether op should async op Returns Async work handle async_op set True None async_op part group warning ` _reduce_scatter_base ` private function Users should use ` reduce_scatter_tensor ` instead reduce_scatter_tensor output input op group async_op _exception_logger all_to_all_single output input output_split_sizes=None input_split_sizes=None group=None async_op=False Split input tensor then scatter split list all processes group Later received tensors concatenated all processes group returned single output tensor Complex tensors supported Args output Tensor Gathered concatenated output tensor input Tensor Input tensor scatter output_split_sizes list Int optional Output split sizes dim specified None empty dim ` ` output ` ` tensor must divide equally ` ` world_size ` ` input_split_sizes list Int optional Input split sizes dim specified None empty dim ` ` input ` ` tensor must divide equally ` ` world_size ` ` group ProcessGroup optional The process group work If None default process group will used async_op bool optional Whether op should async op Returns Async work handle async_op set True None async_op part group warning ` all_to_all_single ` experimental subject change Examples xdoctest +SKIP Undefined rank input = torch arange + rank input tensor Rank tensor Rank tensor Rank tensor Rank output = torch empty dtype=torch int dist all_to_all_single output input output tensor Rank tensor Rank tensor Rank tensor Rank Essentially similar following operation scatter_list = list input chunk world_size gather_list = list output chunk world_size i range world_size dist scatter gather_list i scatter_list i == rank src = i Another example uneven split input tensor Rank tensor Rank tensor Rank tensor Rank input_splits Rank Rank Rank Rank output_splits Rank Rank Rank Rank output = dist all_to_all_single output input output_splits input_splits output tensor Rank tensor Rank tensor Rank tensor Rank Another example tensors torch cfloat type input = torch tensor + j + j + j + j dtype=torch cfloat + rank + j input tensor + j + j + j + j Rank tensor + j + j + j + j Rank tensor + j + j + j + j Rank tensor + j + j + j + j Rank output = torch empty dtype=torch int dist all_to_all_single output input output tensor + j + j + j + j Rank tensor + j + j + j + j Rank tensor + j + j + j + j Rank tensor + j + j + j + j Rank Dynamo has built-in logic map legacy distributed ops functional collectives Let s redirect torch function mode can mimic logic outside Dynamo e g non-strict export implements such torch function mode relevant_args = input has_torch_function relevant_args handle_torch_function all_to_all_single relevant_args output input output_split_sizes=output_split_sizes input_split_sizes=input_split_sizes group=group async_op=async_op _rank_not_in_group group _warn_not_in_group all_to_all_single opts = AllToAllOptions opts asyncOp = async_op _check_single_tensor output output _check_single_tensor input input _ensure_all_tensors_same_dtype output input input is_complex input = torch view_as_real input output is_complex output = torch view_as_real output output_split_sizes = output_split_sizes None output_split_sizes input_split_sizes = input_split_sizes None input_split_sizes group = group _get_default_group work = group alltoall_base output input output_split_sizes input_split_sizes opts async_op work work None Backward compatible backends don t sync CPP level work wait Otherwise backend has sync ed CPP level _exception_logger all_to_all output_tensor_list input_tensor_list group=None async_op=False Scatters list input tensors all processes group gathered list tensors output list Complex tensors supported Args output_tensor_list list Tensor List tensors gathered one per rank input_tensor_list list Tensor List tensors scatter one per rank group ProcessGroup optional The process group work If None default process group will used async_op bool optional Whether op should async op Returns Async work handle async_op set True None async_op part group warning ` all_to_all ` experimental subject change Examples xdoctest +SKIP Undefined rank input = torch arange + rank input = list input chunk input tensor tensor tensor tensor Rank tensor tensor tensor tensor Rank tensor tensor tensor tensor Rank tensor tensor tensor tensor Rank output = list torch empty dtype=torch int chunk dist all_to_all output input output tensor tensor tensor tensor Rank tensor tensor tensor tensor Rank tensor tensor tensor tensor Rank tensor tensor tensor tensor Rank Essentially similar following operation scatter_list = input gather_list = output i range world_size dist scatter gather_list i scatter_list i == rank src=i input tensor Rank tensor Rank tensor Rank tensor Rank input_splits Rank Rank Rank Rank output_splits Rank Rank Rank Rank input = list input split input_splits input tensor tensor tensor tensor Rank tensor tensor tensor tensor Rank tensor tensor tensor tensor Rank tensor tensor tensor tensor Rank output = dist all_to_all output input output tensor tensor tensor tensor Rank tensor tensor tensor tensor Rank tensor tensor tensor tensor Rank tensor tensor tensor tensor Rank Another example tensors torch cfloat type input = torch tensor + j + j + j + j dtype=torch cfloat + rank + j input = list input chunk input tensor + j tensor + j tensor + j tensor + j Rank tensor + j tensor + j tensor + j tensor + j Rank tensor + j tensor + j tensor + j tensor + j Rank tensor + j tensor + j tensor + j tensor + j Rank output = list torch empty dtype=torch int chunk dist all_to_all output input output tensor + j tensor + j tensor + j tensor + j Rank tensor + j tensor + j tensor + j tensor + j Rank tensor + j tensor + j tensor + j tensor + j Rank tensor + j tensor + j tensor + j tensor + j Rank _rank_not_in_group group _warn_not_in_group all_to_all opts = AllToAllOptions opts asyncOp = async_op _check_tensor_list output_tensor_list output_tensor_list _check_tensor_list input_tensor_list input_tensor_list _ensure_all_tensors_same_dtype output_tensor_list input_tensor_list input_tensor_list = t t is_complex torch view_as_real t t input_tensor_list output_tensor_list = t t is_complex torch view_as_real t t output_tensor_list group = group _get_default_group work = group alltoall output_tensor_list input_tensor_list opts async_op work work None Backward compatible backends don t sync CPP level work wait Otherwise backend has sync ed CPP level _exception_logger barrier group Optional ProcessGroup = GroupMember WORLD async_op=False device_ids=None Synchronize all processes This collective blocks processes until whole group enters function async_op False async work handle called wait Args group ProcessGroup optional The process group work If None default process group will used async_op bool optional Whether op should async op device_ids int optional List device GPU ids Only one id expected Returns Async work handle async_op set True None async_op part group note ` ProcessGroupNCCL ` now blocks cpu thread till completion barrier collective note ` ProcessGroupNCCL ` implements barrier all_reduce -element tensor A device must chosen allocating tensor The device choice made checking order first device passed ` device_ids ` arg barrier None device passed init_process_group None device first used process group another collective tensor inputs has been performed device index indicated global rank mod local device count group = group _get_default_group _rank_not_in_group group _warn_not_in_group barrier opts = BarrierOptions opts asyncOp = async_op Detect accelerator machine If no accelerator available returns CPU device = torch _C _get_accelerator isinstance device_ids list opts device_ids = device_ids use only first device id pyrefly ignore read-only opts device = torch device device type device_ids getattr group bound_device_id None None Use device id ` init_process_group device_id= ` opts device = group bound_device_id type ignore assignment device type == cpu _get_object_coll_device group == cpu pyrefly ignore read-only opts device = torch device cpu Use current device set user If user did set any may use default device causing issues like hang all processes creating context device pyrefly ignore read-only opts device = device group rank == warnings warn warn only once barrier using device under current context You can specify ` device_id ` ` init_process_group ` mute warning stacklevel= work = group barrier opts=opts async_op work work None Backward compatible backends don t sync CPP level work wait Otherwise backend has sync ed CPP level monitored_barrier group Optional ProcessGroup = GroupMember WORLD timeout=None wait_all_ranks=False Synchronize processes similar ` ` torch distributed barrier ` ` consider configurable timeout It able report ranks did pass barrier within provided timeout Specifically non-zero ranks will block until send recv processed rank Rank will block until all send recv other ranks processed will report failures ranks failed respond time Note one rank does reach monitored_barrier example due hang all other ranks would fail monitored_barrier This collective will block all processes ranks group until whole group exits function successfully making useful debugging synchronizing However can have performance impact should only used debugging scenarios require full synchronization points host-side For debugging purposes barrier can inserted before application s collective calls check any ranks desynchronized note Note collective only supported GLOO backend Args group ProcessGroup optional The process group work If ` ` None ` ` default process group will used timeout datetime timedelta optional Timeout monitored_barrier If ` ` None ` ` default process group timeout will used wait_all_ranks bool optional Whether collect all failed ranks By default ` ` False ` ` ` ` monitored_barrier ` ` rank will throw first failed rank encounters order fail fast By setting ` ` wait_all_ranks=True ` ` ` ` monitored_barrier ` ` will collect all failed ranks throw error containing information about all failed ranks Returns ` ` None ` ` Example xdoctest +SKIP need process group init Note Process group initialization omitted each rank torch distributed dist dist get_rank = dist monitored_barrier Raises exception indicating rank did call into monitored_barrier Example wait_all_ranks=True dist get_rank == dist monitored_barrier wait_all_ranks=True Raises exception indicating ranks world_size - did call into monitored_barrier Need call rank group before using group otherwise Invalid process group error raised _rank_not_in_group group _warn_not_in_group monitored_barrier get_backend group = Backend GLOO raise ValueError monitored_barrier only implemented GLOO backend timeout None timeout = _get_default_timeout get_backend group isinstance timeout float TODO whc apparently some existing test case monitored_barrier passes timeout float format warnings warn Please specify timeout arg timedelta f Converting current value timeout assuming represents seconds stacklevel= timeout = timedelta seconds=timeout _check_valid_timeout timeout group_to_use = _get_default_group group None group group_to_use monitored_barrier type ignore attr-defined timeout wait_all_ranks=wait_all_ranks _create_process_group_wrapper wrapped_pg torch _C _distributed_c d Backend store_prefix str store Store rank int world_size int timeout timedelta = default_pg_timeout assert _GLOO_AVAILABLE ProcessGroupWrapper unsupported without GLOO backend whc appears just gloo backend so ` default_pg_timeout ` appropriate Create separate prefix store helper process group prefix = f PG_WRAPPER_STORE_PREFIX store_prefix store = PrefixStore prefix store helper_pg = ProcessGroupGloo store rank world_size timeout=timeout Wrap underlying pg ProcessGroupWrapper wrapped_pg = _ProcessGroupWrapper wrapped_pg helper_pg wrapped_pg helper function deterministically hashing list ranks unique string _hash_ranks_to_str ranks list int - str rank_join str = _ join map str ranks In case there already PG same rank composition unique_str = _ join rank_join str len _world pg_names hashlib sha bytes unique_str utf- usedforsecurity=False hexdigest Takes list ranks computes integer color _process_group_color ranks list int - int Convert list tuple make hashable pyrefly ignore bad-assignment ranks = tuple ranks hash_value = hash ranks Split color must - non-negative integer - type compatible C s int because we pybinding latter Thus we limit hash value within c_int s max value max_c_int = ctypes sizeof ctypes c_int - color = abs hash_value max_c_int color _process_group_name ranks use_hashed_name Create name process group global _world use_hashed_name pg_name = _hash_ranks_to_str ranks pg_name = str _world group_count _world group_count += TODO why group count incremented only path pg_name _get_backend_from_str backend Optional str = None - Backend Default same backend global process group backend specified backend backend = get_backend _get_default_group Backend backend _is_safe_to_split - bool Checks safe split any process group world This only safe default pg has bound device id otherwise users must aware pg only splittable after first collective issued _get_default_group bound_device_id None _time_logger split_group parent_pg Optional ProcessGroup = None split_ranks Optional list = None timeout Optional timedelta = None pg_options Optional Any = None group_desc Optional str = None - Optional ProcessGroup Create new process group split given parent process group warning This experimental API Only ` ` NCCL ` ` custom plugin backends supported Other backends will raise error Users API must guarantee all ranks parent group enter API call split sub groups same across all ranks parent group Args parent_pg ProcessGroup optional The parent process group If None default process group will used Users need guarantee parent group fully initialized e g communicators initialized split_ranks list list int split ranks which list list ranks Users need make sure validity split ranks such one split represented one inner list ints does overlap any other split Note ranks each split group rank instead global rank parent pg For example parent group has ranks split_ranks can Note also valid split which case ranks would non-group member timeout timedelta optional see ` init_process_group ` details default value pg_options ProcessGroupOptions optional Additional options need passed during construction specific process groups i e ` ` is_high_priority_stream ` ` can specified so process group can pick up high priority cuda streams group_desc str optional string describe process group Returns ProcessGroup current rank within one split subgroup given split_ranks None current rank part any split_ranks ` check inputs split_ranks None len split_ranks == raise ValueError split_ranks cannot None empty global _world default_pg = _get_default_group device_id = default_pg bound_device_id device_id raise RuntimeError No device associated default pg safe split any process groups global_rank = default_pg rank global_world_size = default_pg size parent_pg parent_pg = default_pg parent_pg _world pg_group_ranks raise ValueError f Group parent_pg registered parent_global_to_group_ranks = _world pg_group_ranks parent_pg parent_group_to_global_ranks = group_rank global_rank global_rank group_rank parent_global_to_group_ranks items global_rank parent_global_to_group_ranks raise ValueError f Global rank global_rank part parent group parent_pg parent_group_rank = parent_global_to_group_ranks global_rank parent_backend = parent_pg _get_backend torch device cuda parent backend does support splitting raise error currently API only support NCCL backend parent_backend parent_backend supports_splitting raise RuntimeError No backend parent process group its backend does support splitting set group_desc before color no_cloor split hasattr parent_backend comm_split_count group_desc None group_desc = f parent_pg group_desc split parent_backend comm_split_count type ignore attr-defined parent_backend_str _ = _world pg_map parent_pg same type backend parent process group backend = Backend parent_backend_str backend_config = BackendConfig backend pg_options None default pg_options same parent process group pg_options = parent_backend options timeout defaulting validation used all new_groups new_subgroups variants which may just pass their timeout value None timeout None timeout = _get_default_timeout backend _check_valid_timeout timeout find my group ranks my group local rank split_ranks ranks which any split PGs we just pass first split group None will returned my_group = split_ranks split_group split_ranks len split_group == raise ValueError split group cannot empty len split_group global_world_size raise ValueError split group s size should less equal world_size set init_process_group len split_group = len set split_group raise ValueError split group cannot have duplicate ranks split_group = sorted split_group parent_group_rank split_group my_group = split_group break use_hashed_name True ensure subgroups have unique names This needed some backends e g Gloo use group name PrefixStore prefix initialization splits Thus names have unique avoid key collisions group_name = _process_group_name my_group use_hashed_name=True split_pg = parent_pg split_group my_group timeout=timeout opts=pg_options group_name=group_name group_desc=group_desc split_pg None None global_ranks_in_my_group = parent_group_to_global_ranks rank rank my_group split_pg bound_device_id = device_id type ignore union-attr split_backend_class = split_pg _get_backend torch device cuda split_backend_class _set_sequence_number_for_group assert split_pg group_name == group_name f group name should set group_name got split_pg group_name update global state _world pg_map split_pg = backend split_pg get_group_store _world pg_names split_pg = group_name _register_process_group group_name split_pg _world pg_backend_config split_pg = str backend_config pg_tag = f ptd group_name _world tags_to_pg setdefault pg_tag append split_pg _world pg_to_tag split_pg = pg_tag Create global rank group rank mapping _world pg_group_ranks split_pg = global_rank group_rank group_rank global_rank enumerate global_ranks_in_my_group split_pg _time_logger new_group ranks=None timeout=None backend=None pg_options=None use_local_synchronization=False group_desc=None device_id Optional torch device = None Create new distributed group This function requires all processes main group i e all processes part distributed job enter function even they going members group Additionally groups should created same order all processes warning Safe concurrent usage When using multiple process groups ` ` NCCL ` ` backend user must ensure globally consistent execution order collectives across ranks If multiple threads within process issue collectives explicit synchronization necessary ensure consistent ordering When using async variants torch distributed communication APIs work object returned communication kernel enqueued separate CUDA stream allowing overlap communication computation Once one more async ops have been issued one process group they must synchronized other cuda streams calling ` work wait ` before using another process group See ` Using multiple NCCL communicators concurrently https docs nvidia com deeplearning nccl user-guide docs usage communicators html#using-multiple-nccl-communicators-concurrently ` more details Args ranks list int List ranks group members If ` ` None ` ` will set all ranks Default ` ` None ` ` timeout timedelta optional see ` init_process_group ` details default value backend str Backend optional The backend use Depending build-time configurations valid values ` ` gloo ` ` ` ` nccl ` ` By default uses same backend global group This field should given lowercase string e g ` ` gloo ` ` which can also accessed via ` Backend ` attributes e g ` ` Backend GLOO ` ` If ` ` None ` ` passed backend corresponding default process group will used Default ` ` None ` ` pg_options ProcessGroupOptions optional process group options specifying what additional options need passed during construction specific process groups i e ` ` nccl ` ` backend ` ` is_high_priority_stream ` ` can specified so process group can pick up high priority cuda streams For other available options config nccl See https docs nvidia com deeplearning nccl user-guide docs api types html#ncclconfig-tuse_local_synchronization bool optional perform group-local barrier end process group creation This different non-member ranks don t need call into API don t join barrier group_desc str optional string describe process group device_id torch device optional single specific device bind process The ` new_group ` call will try initialize communication backend immediately device field given Returns A handle distributed group can given collective calls GroupMember NON_GROUP_MEMBER rank part ` ` ranks ` ` N B use_local_synchronization doesn t work MPI N B While use_local_synchronization=True can significantly faster larger clusters small process groups care must taken since changes cluster behavior non-member ranks don t join group barrier N B use_local_synchronization=True can lead deadlocks when each rank creates multiple overlapping process groups To avoid make sure all ranks follow same global creation order _new_group_with_tag ranks timeout backend pg_options None use_local_synchronization=use_local_synchronization group_desc=group_desc device_id=device_id _new_group_with_tag ranks=None timeout=None backend=None backend_options=None pg_tag=None use_local_synchronization=False group_desc=None device_id Optional torch device = None Variant ` ` new_group ` ` exposes tag creation N B The mechanism experimental tied functional collectives effort see ` ` torch distributed _functional_collectives ` ` reference how use global _world default_pg = _get_default_group device_id None device_id = default_pg bound_device_id default_pg bound_device_id None assert device_id == default_pg bound_device_id Mismatched bound device between new pg default pg default_backend default_store = _world pg_map default_pg global_rank = default_pg rank global_world_size = default_pg size Default same backend global process group backend specified backend backend = default_backend backend = Backend backend timeout defaulting validation used all new_groups new_subgroups variants which may just pass their timeout value None timeout None timeout = _get_default_timeout backend _check_valid_timeout timeout use_local_synchronization MPI backend doesn t have have way us perform partial sync backend == Backend MPI raise ValueError MPI backend doesn t support use_local_synchronization=True ranks None get_rank ranks None checks input ranks ranks None ranks = sorted ranks group_world_size = len ranks group_world_size global_world_size raise ValueError new group s world size should less equal world size set init_process_group check ranks sanity rank ranks rank rank = global_world_size raise ValueError The new group s rank should within world_size set init_process_group global_rank ranks group_rank = ranks index global_rank group_rank = None ranks = list range global_world_size group_world_size = global_world_size group_rank = global_rank group_name = _process_group_name ranks use_hashed_name=use_local_synchronization pg pg_store = _new_process_group_helper group_world_size group_rank ranks backend default_store group_name backend_options=backend_options timeout=timeout pg_tag=pg_tag device_id=device_id group_desc=group_desc Create global rank group rank mapping _world pg_group_ranks pg = global_rank group_rank group_rank global_rank enumerate ranks _is_barrier_after_init == barrier end ensure once we method all process groups including global variables any updated correctly all ranks Update large-scale runs barrier esp store-based barrier may costly unscalable Also lot cases these barriers may unnecessary proven green CI after removal An environment variable ` TORCH_DIST_INIT_BARRIER ` has been added which enables barrier only when set logger info Performing barrier after ProcessGroup initialization since TORCH_DIST_INIT_BARRIER = backend == Backend MPI MPI doesn t have store barrier barrier_store = pg_store use_local_synchronization default_store world_size = len ranks use_local_synchronization get_world_size Use store based barrier here since barrier used bunch default devices messes up NCCL internal state _store_based_barrier global_rank barrier_store group_name world_size timeout pg new_subgroups group_size=None group=None timeout=None backend=None pg_options=None group_desc=None Create subgroups equal size By default creates intra-machine subgroups where each which contains all ranks machine based assumption each machine has same number devices This convenience API calls ` ` new_group ` ` generate multiple subgroups It requires all processes main group i e all processes part distributed job enter function even they going members group warning If ` ` group_size ` ` passed world size must divisible ` ` group_size ` ` If no ` ` group_size ` ` passed believe you creating group based CUDA determining group size number CUDA devices all machines have same number devices subgroup division will different across nodes can cause unexpected behaviors Therefore you creating subgroup does depend CUDA such Gloo CPU please pass ` ` group_size ` ` correctly warning See warning ` Safe concurrent usage ` ` new_group ` API important details about using multiple process groups concurrently safe manner Args group_size int optional The size each subgroup If ` ` None ` ` default subgroup size equal number devices each machine based assumption each machine has exactly same number devices Default ` ` None ` ` group ProcessGroup optional The process group work If ` ` None ` ` default process group will used Default ` ` None ` ` timeout timedelta optional see ` init_process_group ` details default value backend str Backend optional The backend use Depending build-time configurations valid values ` ` gloo ` ` ` ` nccl ` ` By default uses same backend global group This field should given lowercase string e g ` ` gloo ` ` which can also accessed via ` Backend ` attributes e g ` ` Backend GLOO ` ` If ` ` None ` ` passed backend corresponding default process group will used Default ` ` None ` ` pg_options ProcessGroupOptions optional process group options specifying what additional options need passed during construction specific process groups i e ` ` nccl ` ` backend ` ` is_high_priority_stream ` ` can specified so process group can pick up high priority cuda streams group_desc str optional A string describing group Each subgroup will inherit its group_desc Returns The subgroup containing current rank all subgroups used cleanup Examples Create intra-machine subgroups xdoctest +SKIP need process group init cur_subgroup subgroups = dist new_subgroups Allreduce within machine rank = dist get_rank tensor = torch ones device=rank rank dist all_reduce tensor group=cur_subgroup tensor tensor Assume CUDA devices per machine sum range Cleanup subgroup subgroups dist destroy_process_group subgroup group_size None torch cuda is_available raise ValueError Default group size only takes effect when CUDA available If your subgroup using backend does depend CUDA please pass group_size correctly group_size = torch cuda device_count group_size = raise ValueError f The arg group_size group_size must positive world_size = get_world_size group=group world_size group_size raise ValueError f The arg group_size group_size must exceed world size world_size world_size group_size = raise ValueError f The world size world_size must divisible group_size= TODO Use itertools batched get_process_group_ranks group=group group_size instead when Python supported ranks = get_process_group_ranks group=group ranks_per_subgroup_list = ranks i i + group_size i range len ranks group_size new_subgroups_by_enumeration ranks_per_subgroup_list timeout=timeout backend=backend pg_options=pg_options group_desc=group_desc new_subgroups_by_enumeration ranks_per_subgroup_list timeout=None backend=None pg_options=None group_desc=None Create subgroups dividing global world The division specified nested list ranks The subgroups cannot have overlap some ranks may have any subgroup This convenience API calls ` ` new_group ` ` generate multiple subgroups It requires all processes main group i e all processes part distributed job enter function even they going members group warning See warning ` Safe concurrent usage ` ` new_group ` API important details about using multiple process groups concurrently safe manner Args ranks_per_subgroup_list list list int A nested list ranks group members timeout timedelta optional see ` init_process_group ` details default value backend str Backend optional The backend use Depending build-time configurations valid values ` ` gloo ` ` ` ` nccl ` ` By default uses same backend global group This field should given lowercase string e g ` ` gloo ` ` which can also accessed via ` Backend ` attributes e g ` ` Backend GLOO ` ` If ` ` None ` ` passed backend corresponding default process group will used Default ` ` None ` ` pg_options ProcessGroupOptions optional process group options specifying what additional options need passed during construction specific process groups i e ` ` nccl ` ` backend ` ` is_high_priority_stream ` ` can specified so process group can pick up high priority cuda streams group_desc str optional A string describing group Each subgroup will inherit its group_desc Returns The subgroup containing current rank all subgroups used cleanup Examples Create two subgroups where each has processes xdoctest +SKIP need process group init cur_subgroup subgroups = dist new_subgroups ranks= rank = dist get_rank tensor = torch ones device=rank rank dist all_reduce tensor group=cur_subgroup tensor tensor Subgroup ranks tensor Subgroup ranks ranks_per_subgroup_list None len ranks_per_subgroup_list == raise ValueError The arg ranks_per_subgroup_list cannot empty subgroups = cur_subgroup = None Create mapping rank subgroup check there any subgroup overlap rank_to_ranks_dict = type ignore var-annotated ranks ranks_per_subgroup_list subgroup = new_group ranks=ranks timeout=timeout backend=backend pg_options=pg_options group_desc=group_desc subgroups append subgroup my_rank = get_rank rank ranks rank rank_to_ranks_dict raise ValueError f Rank rank has appeared both subgroup rank_to_ranks_dict rank ranks rank_to_ranks_dict rank = ranks my_rank == rank cur_subgroup = subgroup logger info Rank s assigned subgroup s rank ranks cur_subgroup subgroups _find_pg_by_ranks_and_tag tag str ranks list int - Optional ProcessGroup len tag tag startswith ptd tag startswith user tag = f user tag group _world tags_to_pg get tag group size = len ranks continue group_ranks = get_process_group_ranks group good = all r group_ranks r ranks good group None _find_or_create_pg_by_ranks_and_tag tag str ranks list int stride int - ProcessGroup assert len ranks stride == f Ranks length len ranks must divisible stride stride my_rank = get_rank my_ranks = None stride == len ranks my_ranks = ranks copy assert my_rank my_ranks rankset doesn t include current node i range len ranks stride rank_set = ranks i i + stride my_rank rank_set my_ranks = rank_set assert my_ranks None rankset doesn t include current node my_ranks = sorted my_ranks pg = _find_pg_by_ranks_and_tag tag my_ranks pg None pg tag == raise ValueError Cannot automatically create PG empty tag TODO copy settings timeout default PG _new_group_with_tag my_ranks pg_tag=tag _get_group_tag pg ProcessGroup - str Return tag associated ` ` pg ` ` tag = _world pg_to_tag pg tag = tag removeprefix user tag _get_process_group_name pg ProcessGroup - str _world pg_names get pg None _get_process_group_store pg ProcessGroup - Store _world pg_map pg