Owner s module inductor math os torch torch _inductor config inductor_config torch nn functional F torch _dynamo utils rmse same torch _inductor test_case run_tests TestCase torch _inductor utils run_and_get_code torch testing _internal common_utils instantiate_parametrized_tests IS_LINUX parametrize torch testing _internal inductor_utils GPU_TYPE HAS_CUDA_AND_TRITON DO_PERF_TEST = os environ get DO_PERF_TEST == USE_LARGE_INPUT = os environ get USE_LARGE_INPUT == DO_PERF_TEST _prepare_softmax x dim xmax = x amax dim=dim keepdim=True xsum = x - xmax exp sum dim=dim keepdim=True xmax xsum TestOnlineSoftmax TestCase do_test_acc_and_perf op DO_PERF_TEST N = V = padded version gpt N V = small value avoid OOM CI f x op x dim=- x = torch randn N V dtype=torch bfloat device=GPU_TYPE opt_f = torch compile f expected = f x actual = opt_f x assertTrue same expected actual tol= e- DO_PERF_TEST triton testing do_bench eager_ms = do_bench lambda f x opt_ms = do_bench lambda opt_f x print f eager_ms= print f opt_ms= test_softmax do_test_acc_and_perf torch softmax test_log_softmax do_test_acc_and_perf torch log_softmax inductor_config patch use_fast_math=True test_prepare_softmax_perf do_test_acc_and_perf _prepare_softmax get_softmax_wrapper V= use_log_softmax=False device=GPU_TYPE N = torch compile f x use_log_softmax torch log_softmax x dim=- torch softmax x dim=- x = torch randn N V dtype=torch bfloat device=device out source_codes = run_and_get_code f x source_codes test_codegen_ pass_softmax_due_to_disable inductor_config patch online_softmax=False wrapper_code = get_softmax_wrapper assertEqual wrapper_code count r _offset parametrize V parametrize use_log_softmax False True test_codegen_online_softmax use_log_softmax V wrapper_code = get_softmax_wrapper use_log_softmax=use_log_softmax V=V assertEqual wrapper_code count r _offset test_no_online_softmax_for_cpu code = get_softmax_wrapper V= device= cpu CPU need explicit loop across different rows For GPU parallelized hardware assertEqual code count int _t test_codegen_softmax_persistent_reduction Persistent reduction has no loops wrapper_code = get_softmax_wrapper assertEqual wrapper_code count r _offset inductor_config patch triton persistent_reductions False test_sdpa Make sure online softmax here does conflict sdpa patterns q k v = torch randn device=GPU_TYPE dtype=torch bfloat _ range f q k v torch matmul q k transpose - - div math sqrt k shape - softmax dim=- matmul v opt_f = torch compile f ref = f q k v act code = run_and_get_code opt_f q k v assertTrue torch allclose ref act atol= e- rtol= e- assertTrue aten _scaled_dot_product_ code parametrize nrow parametrize dim - test_prepare_softmax dim nrow x = torch randn nrow dtype=torch bfloat device=GPU_TYPE act code = run_and_get_code torch compile _prepare_softmax x dim ref = _prepare_softmax x dim assertTrue same ref act tol= e- nrow == dim == split reduction triggered We have multiple kernels assertTrue code count triton = nrow == dim == persistent reduction triggered expected_num_loop = A single loop due online softmax expected_num_loop = assertEqual code count r _offset expected_num_loop test_split_reduction We don t split online_softmax_reduce now Check Split online_softmax_reduce note code When split promsing we fallback now This just manual example rather than something we see practice tensor shape trigger split reduction x = torch randn dtype=torch bfloat device=GPU_TYPE ref = torch softmax x dim=- act code = run_and_get_code torch compile torch softmax x dim=- assertTrue torch allclose ref act atol= e- rtol= e- assertTrue code count triton = assertTrue online_softmax_reduce code parametrize dtype torch bfloat torch half torch float test_prepare_softmax_acc_with_fp dtype USE_LARGE_INPUT M N = M N = x = torch randn M N device=GPU_TYPE dtype=dtype ref_fp = _prepare_softmax x dtype=torch float dim=- ref = _prepare_softmax x dim=- res code = run_and_get_code torch compile _prepare_softmax x dim=- assertTrue online_softmax_reduce code Max should exactly equal assertEqual ref res assertEqual ref dtype=torch float ref_fp ref_error = rmse ref_fp ref item res_error = rmse ref_fp res item My local tests even shows smaller res_error ref_error= res_error= bf ref_error= res_error= fp ref_error= res_error= fp print f ref_error= f res_error= f assertTrue res_error ref_error + Is good enough make CI stable parametrize fn torch log_softmax torch softmax parametrize dtype torch bfloat torch half torch float test_softmax_acc_with_fp dtype fn USE_LARGE_INPUT M N = M N = x = torch randn M N device=GPU_TYPE dtype=dtype ref_fp = fn x dtype=torch float dim=- ref = fn x dim=- res code = run_and_get_code torch compile fn x dim=- assertTrue online_softmax_reduce code ref_error = rmse ref_fp ref item res_error = rmse ref_fp res item For torch softmax I get almost ref_error res_error all dtypes It s because each value very small since each row add up For torch log_softmax ref_error= res_error= bf ref_error= res_error= fp ref_error= res_error= fp print f ref_error= f res_error= f assertTrue res_error ref_error + Is good enough make CI stable test_softmin The rnumel== kind reduction should unrolled f x F softmin x dim= x = torch randn device=GPU_TYPE ref = f x act code = run_and_get_code torch compile f x assertTrue torch allclose ref act assertTrue online_softmax_reduce code test_causal_mask f x x softmax dim=- x = torch randn device=GPU_TYPE mask = torch tril torch ones device=GPU_TYPE x masked_fill_ mask == float -inf ref = f x act = torch compile f x assertTrue ref isnan any assertTrue act isnan any assertTrue torch allclose ref act test_tb_speech_transformer_attn This example extracted speech_transformer Since online softmax use max partial elements entire row input contains -inf s possible max those partial elements -inf even entire row has non -inf value In cause online softmax will need do things like float -inf - float -inf which becomes nan We fixed interpreting float -inf - float -inf we found both operands float -inf torch manual_seed f x mask x = torch where mask float -inf x xmax = x amax dim=- keepdim=True xsum = x - xmax exp sum dim=- keepdim=True xsum x = torch randn device=GPU_TYPE mask = torch randint device=GPU_TYPE == mask = mask view ref = f x mask act = torch compile f x mask assertTrue ref isnan any assertTrue act isnan any assertTrue torch allclose ref act inductor_config patch split_reductions=False test_ d_tiled_online_softmax f x y x y softmax dim=- M N K = x = torch randn K N M device=GPU_TYPE permute y = torch randn K M N device=GPU_TYPE permute opt_f = torch compile f torch testing assert_close f x y opt_f x y atol= e- rtol= e- instantiate_parametrized_tests TestOnlineSoftmax __name__ == __main__ IS_LINUX HAS_CUDA_AND_TRITON run_tests