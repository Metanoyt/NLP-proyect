Generates ADInplaceOrViewType h cpp NOTE If any changes being made ADInplaceOrView codegen please also check updates needed torch csrc autograd autograd_not_implemented_fallback cpp The fallback expected mimic codegen so we should keep two sync __future__ annotations torchgen api cpp torchgen api autograd dispatch_strategy gen_differentiable_outputs NativeFunctionWithDifferentiabilityInfo torchgen api types BaseCType Binding boolT ConstRefCType CType DispatcherSignature intArrayRefT longT OptionalCType symIntArrayRefT SymIntT tensorT torchgen code_template CodeTemplate torchgen context with_native_function torchgen model NativeFunction SchemaKind SelfArgument TensorOptionsArguments Type torchgen utils FileManager context with_native_function_with_differentiability_info gen_trace_type get_return_value MANUAL_AUTOGRAD tie_return_values type_wrapper_name See NOTE Autograd View Variables variable h details If you update list VIEW_FUNCTIONS RETURNS_VIEWS_OF_INPUT you MUST also update public list view ops accordingly docs source tensor_view rst Note all ATen functions exposed public e g alias sparse_coo_tensor_with_dims_and_tensors A map function name = name argument all outputs view VIEW_FUNCTIONS_WITH_METADATA_CHANGE = view_as_complex view_as_real _conj _neg_view _nested_get_values _nested_view_from_buffer _nested_view_from_jagged VIEW_FUNCTIONS = numpy_T alias as_strided diagonal expand permute select slice slice_inverse split split_with_sizes squeeze t transpose unfold unsqueeze flatten view unbind _indices _values indices values crow_indices col_indices ccol_indices row_indices sparse_coo ctor output should really views both indices values we only supports making view single variable indices discrete anyways FIXME clone indices construction sparse_coo_tensor_with_dims_and_tensors values _reshape_alias _test_autograd_multiple_dispatch_view key VIEW_FUNCTIONS_WITH_METADATA_CHANGE VIEW_FUNCTIONS key = note some VIEW_FUNCTIONS just compositions view functions above list contains both root view functions any purely composed viewing functions used JIT determine when operator may view its inputs however they may sometimes copy e g ` contiguous ` RETURNS_VIEWS_OF_INPUT = set VIEW_FUNCTIONS keys union chunk detach contiguous reshape reshape_as expand_as view_as real imag narrow movedim tensor_split swapdims swapaxes mT mH adjoint matrix_H These functions we consider views purposes validating StorageImpl TensorImpl gen_variable_type ` _unsafe_view ` included VIEW_FUNCTIONS above because view purposes ADInplaceOrView kernel we do want call as_view See NOTE Unsafe View more info ALL_VIEW_FUNCTIONS = VIEW_FUNCTIONS _unsafe_view ARRAYREF_TO_VEC = CodeTemplate \ auto $ vec = $ arg vec OPTIONAL_TO_VAL = CodeTemplate \ auto $ val = $ arg value_or $ default CALL_DISPATCH = CodeTemplate \ _ops $ unambiguous_name call $ unpacked_args REVERSE_VIEW_DISPATCH = CodeTemplate \ $ reverse_name $ unpacked_args MULTI_OUTPUT_VIEW_ITERATION = CodeTemplate \ auto $ view_idx c irange $ var size $ body SETUP_REPLAY_VIEW_IF_NOT_SUPPORT_AS_STRIDED_OR_VIEW_WITH_METADATA_CHANGE = CodeTemplate \ std unique_ptr torch autograd ViewFunc func nullptr std function Tensor const Tensor rev_func=nullptr $ is_view_with_metadata_change &#124; &#124; unsafeGetTensorImpl - support_as_strided &#124; &#124; unsafeGetTensorImpl - is_python_dispatch &#124; &#124; c AutogradState get_tls_state get_view_replay_enabled $ replay_view_func $ reverse_replay_view_func REPLAY_VIEW_FUNC = CodeTemplate \ func = std make_unique $ view_func_name $ view_func_args REVERSE_REPLAY_VIEW_LAMBDA_FUNC = CodeTemplate \ rev_func = = const Tensor $ input_view $ reverse_replay_view_call METHOD_DEFINITION = CodeTemplate \ $ return_type $ type_wrapper_name $ formals $ type_definition_body WRAPPER_REGISTRATION = CodeTemplate \ m impl $ unqual_operator_name_with_overload TORCH_FN $ class_type $ type_wrapper_name AUTOGRAD_NOT_IMPLEMENTED_REGISTRATION = CodeTemplate \ m impl $ unqual_operator_name_with_overload torch autograd autogradNotImplementedFallback INPLACE_REDISPATCH = CodeTemplate \ AutoDispatchBelowADInplaceOrView guard _ops $ unambiguous_name redispatch $ unpacked_args ASSIGN_RETURN_VALUE = CodeTemplate \ $ return_values = $ rhs_value VIEW_REDISPATCH = CodeTemplate \ $ assign_return_values AutoDispatchBelowADInplaceOrView guard _ops $ unambiguous_name redispatch $ unpacked_args TMP_VAR = _tmp FIXME Ideally these functions should methods Type we have comment codegen model py there saying these concepts well defined Thus we put version commonly used autograd codegen here is_tensor_type t Type - bool TODO Should handle optional here t is_tensor_like t is_list_like None is_tensor_list_type t Type - bool TODO Should handle optional here t is_tensor_like t is_list_like None UNPACK_TENSOR = CodeTemplate \ auto$ ref $ arg_name _ = unpack$ suffix $ arg_name $ arg_name $ arg_pos unpacked_name arg_name str - str arg_name + _ e g select int - select_copy_int_inverse inverse_view_name f NativeFunction - str copy_variant = f f root_name _copy overload = f f func name overload_name overload = overload = _ + overload f copy_variant overload _inverse extract_bindings f NativeFunction - list Binding r f func schema_order_arguments r cpp argument method=False symint=True cpp_no_default_args=set faithful=False has_tensor_options=False with_native_function unpack_args f NativeFunction - tuple list str list Binding body list str = unpacked_bindings list Binding = i binding enumerate extract_bindings f assert isinstance binding argument SelfArgument isinstance binding argument TensorOptionsArguments raise RuntimeError VariableKernel shouldn t take TensorOptions is_nullable = binding argument type is_nullable binding argument type is_tensor_like is_nullable unpacked_bindings append binding continue is_tensor_list = is_tensor_list_type binding argument type ref = is_nullable is_tensor_list suffix = _opt is_nullable is_tensor_list body append UNPACK_TENSOR substitute arg_name=binding name arg_pos=i suffix=suffix ref= ref unpacked_bindings append Binding name=unpacked_name binding name nctype=binding nctype argument=binding argument default=binding default body unpacked_bindings get_base_name f NativeFunction - str f func name name base TODO should str f func name name get_view_info f NativeFunction - str &#124; None base_name = get_base_name f view_info = VIEW_FUNCTIONS get base_name view_info None base_name RETURNS_VIEWS_OF_INPUT view_info = view_info emit_view_func f NativeFunction bindings list Binding view_idx str &#124; None = None - str Generate additional lambda function recover views backward when as_strided supported See Note View + Inplace update base tensor View + Inplace update view tensor more details TODO Clean logic up we get rid reverse view funcs reify them input_base = input_base replay_view_func = updated_args list str = known_view_arg_simple_types list CType = BaseCType longT OptionalCType BaseCType longT BaseCType SymIntT OptionalCType BaseCType SymIntT BaseCType boolT BaseCType intArrayRefT BaseCType symIntArrayRefT ConstRefCType BaseCType tensorT ConstRefCType OptionalCType BaseCType tensorT binding bindings arg arg_type = binding name binding nctype type arg == updated_args append input_base continue arg_type known_view_arg_simple_types known_types_str = join str t t known_view_arg_simple_types raise TypeError f You adding arg_type arg argument op cpp name f func addition known types f known_types_str Please update list materialize so can closed over value also add test pytorch xla test test_operations py where code exercised arg_type == BaseCType intArrayRefT arg_type == BaseCType symIntArrayRefT It s safe close over IntArrayRef value since reference type so materialize vector close over value arg_vec = arg + _vec replay_view_func += ARRAYREF_TO_VEC substitute arg=arg vec=arg_vec updated_args append arg_vec arg_type == OptionalCType BaseCType longT Materialize int _t int _t arg_value = arg + _val replay_view_func += OPTIONAL_TO_VAL substitute arg=arg val=arg_value default= updated_args append arg_value arg_type == ConstRefCType BaseCType tensorT arg_type == ConstRefCType OptionalCType BaseCType tensorT NB Closing over tensor If user modifies tensor will silently incorrect The proper thing do store version counter copy write updated_args append arg updated_args append arg gen_view_funcs view_func_name view_func_args = b name b bindings b name = view_idx None view_func_args append f view_idx replay_view_func += REPLAY_VIEW_FUNC substitute view_func_name=view_func_name f include_namespace=True view_func_args=view_func_args input_view = input_view reverse_unpacked_args = f input_view inverse_return_mode= functionalization InverseReturnMode AlwaysView view_idx None f view_idx skip input_base arg updated_args torchgen api functionalization reverse_name reverse_replay_view_call = REVERSE_VIEW_DISPATCH substitute reverse_name=reverse_name f include_namespace=True unpacked_args=reverse_unpacked_args reverse_replay_view_func = REVERSE_REPLAY_VIEW_LAMBDA_FUNC substitute input_view=input_view reverse_replay_view_call=reverse_replay_view_call is_view_with_metadata_change = true cpp name f func VIEW_FUNCTIONS_WITH_METADATA_CHANGE false SETUP_REPLAY_VIEW_IF_NOT_SUPPORT_AS_STRIDED_OR_VIEW_WITH_METADATA_CHANGE substitute is_view_with_metadata_change=is_view_with_metadata_change replay_view_func=replay_view_func reverse_replay_view_func=reverse_replay_view_func emit_view_body fn NativeFunctionWithDifferentiabilityInfo var str - tuple str str See NOTE Autograd View Variables variable h details f = fn func base_name = get_base_name f view_info = get_view_info f call = differentiable_outputs = gen_differentiable_outputs fn differentiable_output_vars = r name r differentiable_outputs isinstance view_info str raise TypeError f The view info should string base_name view_info len differentiable_output_vars == no output differentiable indices SparseTensors example rhs_value = f as_view view_info var f is_bw_differentiable false is_fw_differentiable false len differentiable_output_vars == Single differentiable output Tensor Tensor return_info = differentiable_outputs We only support simple Tensor TensorList functions views is_tensor_type return_info type is_tensor_list_type return_info type raise RuntimeError f base_name differentiable views can only Tensor Tensor See Note View + Inplace detection get_creation_meta_in_mode original str - str creation_meta_with_grad_mode = f GradMode is_enabled original CreationMeta NO_GRAD_MODE f InferenceMode is_enabled CreationMeta INFERENCE_MODE creation_meta_with_grad_mode Only allow rebasing history we single Tensor If we no grad block raise warning See NOTE View + Inplace detection more details about logic is_tensor_list_type return_info type creation_meta = get_creation_meta_in_mode CreationMeta MULTI_OUTPUT_NODE view_idx = view_idx view_func = emit_view_func f extract_bindings f view_idx=view_idx strip as_view_call = f as_view base view_info output var view_idx is_bw_differentiable true is_fw_differentiable true view_func std move func rev_view_func rev_func f creation_meta creation_meta call += MULTI_OUTPUT_VIEW_ITERATION substitute var=var view_idx=view_idx body=f view_func \n as_view_call rhs_value = f std move var call += emit_view_func f extract_bindings f view_idx=None creation_meta = get_creation_meta_in_mode CreationMeta DEFAULT rhs_value = f as_view base view_info output var is_bw_differentiable true is_fw_differentiable true f view_func std move func rev_view_func rev_func creation_meta creation_meta This could supported we don t need moment so keeping things simple raise RuntimeError Function multiple differentiable output when least one them view supported call rhs_value modifies_arguments f NativeFunction - bool f func kind SchemaKind inplace SchemaKind out with_native_function_with_differentiability_info emit_inplace_or_view_body fn NativeFunctionWithDifferentiabilityInfo - list str f = fn func inplace_view_body list str = dispatcher_sig = DispatcherSignature from_schema f func dispatcher_exprs = dispatcher_sig exprs code-generated ADInplaceOrView kernels plumb recompute dispatch keys directly through kernel performance See Note Plumbing Keys Through The Dispatcher details dispatch_key_set = ks c after_ADInplaceOrView_keyset redispatch_args = join dispatch_key_set + expr dispatcher_exprs Note calls slow dispatching variants manual_cpp_binding ops We could probably work harder ensure fast variants called instead perf benefit would minimal modifies_arguments f inplace op inplace_view_body append INPLACE_REDISPATCH substitute unambiguous_name=f func name unambiguous_name unpacked_args=redispatch_args r cpp return_names f inplace_view_body append f increment_version r assert get_view_info f None inplace_view_body append VIEW_REDISPATCH substitute assign_return_values= auto + TMP_VAR + = unambiguous_name=f func name unambiguous_name unpacked_args=redispatch_args call rhs_value = emit_view_body fn TMP_VAR inplace_view_body append call assert rhs_value None inplace_view_body append ASSIGN_RETURN_VALUE substitute return_values=tie_return_values f rhs_value=rhs_value f func returns inplace_view_body append f get_return_value f inplace_view_body with_native_function gen_formals f NativeFunction - str join code-generated autograd kernels plumb recompute dispatch keys directly through kernel performance See Note Plumbing Keys Through The Dispatcher details c DispatchKeySet ks + f cpp argument_type binds= __placeholder__ symint=True cpp_type name f func schema_order_arguments with_native_function_with_differentiability_info inplace_or_view_method_definition fn NativeFunctionWithDifferentiabilityInfo - str &#124; None f = fn func get_view_info f None For functions modify their inputs don t them we can t give them autograd support See https github com pytorch pytorch issues modifies_arguments f len f func returns == None METHOD_DEFINITION substitute return_type=cpp returns_type f func returns symint=True cpp_type type_wrapper_name=type_wrapper_name f formals=gen_formals f type_definition_body=emit_inplace_or_view_body fn with_native_function_with_differentiability_info inplace_or_view_method_registration fn NativeFunctionWithDifferentiabilityInfo - str &#124; None f = fn func get_view_info f None modifies_arguments f len f func returns == None WRAPPER_REGISTRATION substitute unqual_operator_name_with_overload=f func name type_wrapper_name=type_wrapper_name f class_type= ADInplaceOrView use_derived fn NativeFunctionWithDifferentiabilityInfo - bool f = fn func name = cpp name f func name MANUAL_AUTOGRAD dispatch_strategy fn == use_derived gen_inplace_or_view_type_env fn NativeFunctionWithDifferentiabilityInfo - dict str list str definition = inplace_or_view_method_definition fn registration = inplace_or_view_method_registration fn ops_headers f #include ATen ops fn func root_name _ops h definition None inplace_or_view_method_definitions definition definition None inplace_or_view_wrapper_registrations registration registration None gen_inplace_or_view_type out str native_yaml_path str tags_yaml_path str fns_with_infos list NativeFunctionWithDifferentiabilityInfo template_path str - None NOTE see Note Sharded File top VariableType cpp template regarding sharding generated files fm = FileManager install_dir=out template_dir=template_path dry_run=False fm write_sharded ADInplaceOrViewType cpp fn fn fns_with_infos use_derived fn key_fn=lambda fn fn func root_name base_env= generated_comment + f generated fm template_dir_for_comments ADInplaceOrViewType cpp env_callable=gen_inplace_or_view_type_env num_shards= sharded_keys= ops_headers inplace_or_view_method_definitions inplace_or_view_wrapper_registrations