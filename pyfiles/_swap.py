logging operator types collections defaultdict typing Optional torch torch fx _pytree fx_pytree torch utils _pytree pytree torch export exported_program ConstantArgument ExportedProgram ModuleCallSignature torch fx passes tools_common legalize_graph NodeList torch fx passes utils fuser_utils erase_nodes fuse_as_graphmodule log = logging getLogger __name__ _get_getitem_users node torch fx Node - set torch fx Node node_users = list node users keys getitem_users = set user node_users user op == output continue assert user op == call_function user target operator getitem f Expected getitem node user node instead got user getitem_users update list user users keys getitem_users _try_remove_connecting_pytrees curr_module_node torch fx Node - None We want try remove extraneous pytree flatten unflatten calls between modules calls Instead having following graph foo num_users= = call_module target=foo args = getitem_ getitem_ kwargs = tree_flatten_spec num_users= = call_function target=torch fx _pytree tree_flatten_spec args = foo _spec_ kwargs = getitem_ num_users= = call_function target=operator getitem args = tree_flatten_spec kwargs = tree_unflatten_ num_users= = call_function target=torch utils _pytree tree_unflatten args = getitem_ _spec_ kwargs = getitem_ num_users= = call_function target=operator getitem args = tree_unflatten_ kwargs = getitem_ num_users= = call_function target=operator getitem args = tree_unflatten_ kwargs = getitem_ num_users= = call_function target=operator getitem args = getitem_ kwargs = bar num_users= = call_module target=bar args = getitem_ kwargs = We could do following we know all outputs ` foo ` feed into ` bar ` graph foo num_users= = call_module target=foo args = getitem_ getitem_ kwargs = bar num_users= = call_module target=bar args = getitem_ kwargs = Currently optimization only works case where all outputs ` foo ` go directly into ` bar ` ` bar ` has no other inputs noqa B log debug Trying remove pytrees module call s curr_module_node curr_module_users = list curr_module_node users keys assert len curr_module_users == f Expected only one user module node instead got list curr_module_users flatten_node = curr_module_users assert flatten_node op == call_function flatten_node target fx_pytree tree_flatten_spec flatten_getitem_users = _get_getitem_users flatten_node len flatten_getitem_users = log debug More than one user found flatten node s s Unable fuse another unflatten call flatten_node flatten_getitem_users unflatten_node = next iter flatten_getitem_users unflatten_node op == call_function unflatten_node target pytree tree_unflatten log debug Flatten node s s user pytree tree_unflatten Instead s Passing flatten_node unflatten_node i arg enumerate unflatten_node args type ignore union-attr arg-type arg flatten_node users log debug Module s s outputs all directly used inputs subsequent module Unable fuse connecting flatten unflatten The inputs subsequent module s curr_module_node unflatten_node args pyrefly ignore missing-attribute arg op == call_function pyrefly ignore missing-attribute arg target operator getitem pyrefly ignore missing-attribute arg args == i log debug Module s s outputs all directly used same order outputted Unable fuse connecting flatten unflatten The inputs subsequent module s curr_module_node unflatten_node args Unflatten has two levels getitem because gets args kwargs unflatten_getitem_getitem_users = set unflatten_getitem_users = _get_getitem_users unflatten_node unflatten_getitem_user unflatten_getitem_users unflatten_getitem_getitem_users update list unflatten_getitem_user users keys len unflatten_getitem_getitem_users = log debug More than one user found unflatten node s s Unable fuse another flatten call unflatten_node unflatten_getitem_getitem_users next_module_node = next iter unflatten_getitem_getitem_users next_module_node op = call_module log debug Unflatten node s s user call_module Instead s Passing unflatten_node next_module_node Directly put outputs current module into next module next_module_node args = curr_module_node _remove_extraneous_pytrees gm torch fx GraphModule - None Remove extraneous pytree flatten unflatten calls We try couple optimizations here Remove pytree flatten unflatten calls between modules TODO Remove module s in_spec + initial unflatten call TODO Remove module s out_spec + final flatten call node gm graph nodes node op == call_module node target = _guards_fn _try_remove_connecting_pytrees node gm graph eliminate_dead_code _construct_inputs gm torch fx GraphModule signature ModuleCallSignature node_name_map dict str torch fx Node - tuple list torch fx Node dict str torch fx Node tree_unflatten_args list Optional torch fx Node = input_ signature inputs isinstance input_ ConstantArgument input_ value None Constants should directly embedded into graph used inputs tree_unflatten_args append None input_ name node_name_map For unused inputs tree_unflatten_args append None tree_unflatten_args append node_name_map input_ name Insert unflatten call unflatten _generate_unflatten unflatten_node = _generate_unflatten gm tree_unflatten_args signature in_spec assert signature in_spec num_children == assert signature in_spec type tuple args_spec kwargs_spec = signature in_spec children assert args_spec type tuple assert kwargs_spec type dict args_node = gm graph call_function operator getitem unflatten_node args_nodes = gm graph call_function operator getitem args_node i i range args_spec num_children kwargs_node = gm graph call_function operator getitem unflatten_node kwargs_nodes = k gm graph call_function operator getitem kwargs_node k k kwargs_spec context args_nodes kwargs_nodes _insert_call_module gm torch fx GraphModule args_nodes list torch fx Node kwargs_nodes dict str torch fx Node module_to_swap torch nn Module name str - torch fx Node unflatten _assign_attr _AttrKind _assign_attr module_to_swap gm name _AttrKind MODULE module_node = gm graph call_module name tuple args_nodes kwargs_nodes type ignore arg-type module_node _deconstruct_outputs gm torch fx GraphModule signature ModuleCallSignature module_node torch fx Node node_name_map dict str torch fx Node orig_outputs tuple torch fx Node - None unflatten _generate_flatten_spec flatten_node = _generate_flatten_spec gm module_node signature out_spec i orig_output enumerate orig_outputs Use Proxy record getitem access proxy_out = torch fx Proxy flatten_node i node type ignore index orig_output replace_all_uses_with proxy_out propagate_meta=True node_name_map orig_output name = proxy_out _swap_module_helper gm torch fx GraphModule modules_to_swap dict str torch nn Module module_call_graph dict str ModuleCallSignature - torch fx GraphModule log debug Starting graph log debug gm graph legalize_graph gm partitions dict str NodeList = defaultdict list node_name_map dict str torch fx Node = node name node node gm graph nodes TODO Handle duplicate module case node gm graph nodes nn_module_stack = node meta get nn_module_stack path _ nn_module_stack values path modules_to_swap partitions path append node break name nodes partitions items Given graph like following we want swap out submodule foo graph x num_users= = placeholder target=x y num_users= = placeholder target=y add num_users= = call_function target=torch ops aten add Tensor args = y x kwargs = nn_module_stack = foo foo torch nn Module sub num_users= = call_function target=torch ops aten sub Tensor args = y add kwargs = nn_module_stack = bar bar torch nn Module sub We will first partition out foo s subgraph graph x num_users= = placeholder target=x y num_users= = placeholder target=y add num_users= = call_function target=torch ops aten add Tensor args = y x kwargs = add And then insert unflatten + call_module + flatten replace subgraph graph x num_users= = placeholder target=x y num_users= = placeholder target=y _spec_ num_users= = get_attr target=_spec_ tree_unflatten num_users= = call_function target=torch utils _pytree tree_unflatten args = x y _spec_ kwargs = getitem num_users= = call_function target=operator getitem args = tree_unflatten kwargs = getitem_ num_users= = call_function target=operator getitem args = getitem kwargs = getitem_ num_users= = call_function target=operator getitem args = getitem kwargs = getitem_ num_users= = call_function target=operator getitem args = tree_unflatten kwargs = foo num_users= = call_module target=foo args = getitem_ getitem_ kwargs = _spec_ num_users= = get_attr target=_spec_ tree_flatten_spec num_users= = call_function target=torch fx _pytree tree_flatten_spec args = None _spec_ kwargs = getitem_ num_users= = call_function target=operator getitem args = tree_flatten_spec kwargs = sub num_users= = call_function target=torch ops aten sub Tensor args = y getitem_ kwargs = sub The ` tree_unflatten ` call will construct tensor inputs into input format needed swapped eager module The ` call_module ` node should now reference swapped torch nn Module The ` tree_flatten_spec ` call will deconstruct eager outputs swapped module into tensors noqa B submod_name = name replace _ sub_gm orig_inputs orig_outputs = fuse_as_graphmodule gm nodes f fused_ submod_name log debug Fused subgraph nodes log debug sub_gm graph signature ModuleCallSignature = module_call_graph name args_nodes kwargs_nodes = _construct_inputs gm signature node_name_map module_node = _insert_call_module gm args_nodes kwargs_nodes modules_to_swap name name _deconstruct_outputs gm signature module_node node_name_map orig_outputs erase_nodes gm nodes log debug Swapped graph log debug gm graph legalize_graph gm log debug Before removing extraneous pytrees log debug gm graph _remove_extraneous_pytrees gm log debug After removing extraneous pytrees log debug gm graph gm recompile gm _fix_input_output_signature gm torch fx GraphModule signature ModuleCallSignature - None Given unlifted module calling ep module we want remove pytree processing graph module s PyTreeCodeGen instead make nodes inside graph This allows us do some optimizations like remove these pytree calls unnecessary makes PyTree part more obvious graph passes torch export unflatten _generate_flatten _generate_unflatten Remove registered pytree codegen because we will take care through inserting pytree nodes into graph gm graph _codegen = torch fx graph CodeGen old_placeholders = node node gm graph nodes node op == placeholder new_placeholders = forward_arg_names = signature forward_arg_names forward_arg_names None forward_arg_names = assert signature in_spec num_children == arg_spec = signature in_spec child kwarg_spec = signature in_spec child assert arg_spec type tuple assert kwarg_spec type dict i range arg_spec num_children forward_arg_names append f arg_ i forward_arg_names extend kwarg_spec context arg forward_arg_names gm graph inserting_before old_placeholders new_placeholders append gm graph placeholder arg Insert flatten call inputs gm graph inserting_before old_placeholders flat_node = _generate_flatten gm tuple new_placeholders i old_placeholder enumerate old_placeholders old_placeholder op = call_function old_placeholder target = operator getitem old_placeholder args = flat_node i Insert unflatten call outputs output_node = next node node gm graph nodes node op == output gm graph inserting_before output_node unflat = _generate_unflatten gm output_node args signature out_spec output_node args = unflat gm recompile _swap_modules ep ExportedProgram modules_to_swap dict str torch nn Module - torch fx GraphModule Unlifts given ExportedProgram into fx GraphModule then swaps previously traced modules new eager modules specified Returns fx GraphModule custom forward function Args ep ExportedProgram Exported program modify modules_to_swap Dict str torch nn Module Mapping module fqn eager module swap The specified module fqn should have also been specified ` preserve_module_call_signature ` argument torch export so we know how restore calling convention argument run_with_interpreter Whether run graph using fx Interpreter Setting true will help result better error messages easier debugging has found result QPS drop module_call_graph = entry fqn entry signature entry ep module_call_graph entry signature gm = ep module gm validate_inputs = False type ignore assignment gm graph eliminate_dead_code type ignore operator union-attr assert isinstance gm torch fx GraphModule _fix_input_output_signature gm ep module_call_graph signature gm module_call_graph = ep module_call_graph gm train = types MethodType type gm train gm type ignore assignment gm eval = types MethodType type gm eval gm type ignore assignment assert isinstance gm torch fx GraphModule gm = _swap_module_helper gm modules_to_swap module_call_graph gm