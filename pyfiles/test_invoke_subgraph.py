Owner s module higher order operators flake noqa B flake noqa E unittest unittest mock mock parameterized parameterized_class torch torch _dynamo torch _functorch torch _inductor torch _inductor decomposition torch utils _pytree pytree functorch compile aot_function nop torch _dynamo testing AotEagerAndRecordGraphs EagerAndRecordGraphs empty_line_normalizer InductorAndRecordGraphs normalize_gm torch _higher_order_ops schema find_hop_schema torch _inductor config inductor_config torch _inductor pattern_matcher CallFunctionVarArgs PatternMatcherPass register_graph_pattern torch testing _internal common_utils run_tests skipIfTorchDynamo TEST_WITH_CROSSREF TestCase torch testing _internal inductor_utils GPU_TYPE HAS_GPU torch testing _internal triton_utils requires_cuda_and_triton requires_gpu nested_compile_region = torch compiler nested_compile_region HAS_GPU triton skipIfTorchDynamo Not torch _dynamo test TestInvokeSubgraph TestCase test_simple gn x y torch mul x y fn x y nested_compile_region gn x y x = torch randn requires_grad=True y = torch randn requires_grad=True ref = gn x y x_clone = x detach clone requires_grad_ True y_clone = y detach clone requires_grad_ True res = fn x_clone y_clone Run backward ref sum backward res sum backward assertEqual ref res assertEqual x grad x_clone grad assertEqual y grad y_clone grad test_aot_function gn x y torch mul x y fn x y nested_compile_region gn x y x = torch randn requires_grad=True y = torch randn requires_grad=True ref = gn x y x_clone = x detach clone requires_grad_ True y_clone = y detach clone requires_grad_ True aot_fn = aot_function fn nop res = aot_fn x_clone y_clone Run backward ref sum backward res sum backward assertEqual ref res assertEqual x grad x_clone grad assertEqual y grad y_clone grad test_multiple nested_compile_region cos x torch cos x nested_compile_region sin x torch sin x fn x = cos x b = sin cos b x = torch randn requires_grad=True ref = fn x aot_fn = aot_function fn nop res = aot_fn x assertEqual ref res skipIfTorchDynamo Not torch _dynamo test TestInvokeSubgraphCompile TestCase count_unique_get_attr_nodes gm args expected subgraph_attr_names = set node gm graph nodes node op == get_attr subgraph_attr_names add node target assertEqual len subgraph_attr_names expected test_simple nested_compile_region gn x y torch mul x y fn x y gn x y x = torch randn requires_grad=True y = torch randn requires_grad=True ref = fn x y x_clone = x detach clone requires_grad_ True y_clone = y detach clone requires_grad_ True res = torch compile fn backend= inductor fullgraph=True x_clone y_clone Run backward ref sum backward res sum backward assertEqual ref res assertEqual x grad x_clone grad assertEqual y grad y_clone grad test_module_forward Mod torch nn Module __init__ super __init__ c = nested_compile_region forward x y torch mul x y sin + c mod = Mod fn x y mod x y + mod x y x = torch randn requires_grad=True y = torch randn requires_grad=True ref = fn x y x_clone = x detach clone requires_grad_ True y_clone = y detach clone requires_grad_ True res = torch compile fn backend= inductor fullgraph=True x_clone y_clone Run backward ref sum backward res sum backward assertEqual ref res assertEqual x grad x_clone grad assertEqual y grad y_clone grad test_gen_schema Mod torch nn Module __init__ super __init__ c = nested_compile_region forward x y torch mul x y sin + c mod = Mod fn x y mod x y + mod x y x = torch randn requires_grad=True y = torch randn requires_grad=True x_clone = x detach clone requires_grad_ True y_clone = y detach clone requires_grad_ True backend = AotEagerAndRecordGraphs res = torch compile fn backend=backend fullgraph=True x_clone y_clone res sum backward assertEqual len backend fw_graphs assertEqual len backend bw_graphs fw_schema = find_hop_schema backend fw_graphs torch ops higher_order invoke_subgraph bw_schema = find_hop_schema backend bw_graphs torch ops higher_order invoke_subgraph assertExpectedInline str fw_schema invoke_subgraph Any subgraph str identifier Tensor arg Tensor arg - Tensor Tensor Tensor assertExpectedInline str fw_schema invoke_subgraph Any subgraph str identifier Tensor arg Tensor arg - Tensor Tensor Tensor assertExpectedInline str bw_schema invoke_subgraph Any subgraph str identifier Tensor arg Tensor arg Tensor arg - Tensor Tensor assertExpectedInline str bw_schema invoke_subgraph Any subgraph str identifier Tensor arg Tensor arg Tensor arg - Tensor Tensor test_gen_schema_with_buffer_mutation Mod torch nn Module __init__ super __init__ c = register_buffer buf torch ones requires_grad=False nested_compile_region forward x y buf add_ torch mul x y sin + c + buf mod_ref = Mod mod = Mod fn mod x y mod x y + mod x y x = torch randn requires_grad=True y = torch randn requires_grad=True ref = fn mod_ref x y x_clone = x detach clone requires_grad_ True y_clone = y detach clone requires_grad_ True backend = EagerAndRecordGraphs torch no_grad res = torch compile fn backend=backend fullgraph=True mod x_clone y_clone assertEqual len backend graphs fw_schema = find_hop_schema backend graphs torch ops higher_order invoke_subgraph TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_x_ f L_y_ f L_mod_buffers_buf_ f l_x_ = L_x_ l_y_ = L_y_ l_mod_buffers_buf_ = L_mod_buffers_buf_ subgraph_ = subgraph_ invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_mod_buffers_buf_ l_x_ l_y_ subgraph_ = None getitem f = invoke_subgraph invoke_subgraph = None subgraph_ = subgraph_ invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_mod_buffers_buf_ l_x_ l_y_ subgraph_ = l_mod_buffers_buf_ = l_x_ = l_y_ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None add f = getitem + getitem_ getitem = getitem_ = None add subgraph_ torch nn Module forward l_mod_buffers_buf_ f l_x_ f l_y_ f add_ f = l_mod_buffers_buf_ add_ add_ = None mul f = torch mul l_x_ l_y_ l_x_ = l_y_ = None sin f = mul sin mul = None add f = sin + sin = None add_ f = add + l_mod_buffers_buf_ add = l_mod_buffers_buf_ = None add_ assertExpectedInline str fw_schema invoke_subgraph Any subgraph str identifier Tensor arg Tensor arg Tensor arg - Tensor assertExpectedInline str fw_schema invoke_subgraph Any subgraph str identifier Tensor arg Tensor arg Tensor arg - Tensor assertEqual res ref assertEqual mod buf mod_ref buf test_auto_functionalize Mod torch nn Module __init__ super __init__ c = register_buffer buf torch ones requires_grad=False nested_compile_region forward x y torch mul x y sin c buf mod_ref = Mod mod = Mod fn mod x y mod x y + mod x y x = torch randn requires_grad=True y = torch randn requires_grad=True ref = fn mod_ref x y x_clone = x detach clone requires_grad_ True y_clone = y detach clone requires_grad_ True backend = AotEagerAndRecordGraphs res = torch compile fn backend=backend fullgraph=True mod x_clone y_clone res sum backward assertEqual len backend fw_graphs assertEqual len backend bw_graphs assertEqual ref res assertExpectedInline normalize_gm backend fw_graphs print_readable print_output=False \ GraphModule torch nn Module forward primals_ f primals_ f primals_ f partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ primals_ primals_ partitioned_fw_subgraph_ _ = None getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ getitem f = invoke_subgraph_ invoke_subgraph_ = None partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ primals_ primals_ partitioned_fw_subgraph_ _ = primals_ = primals_ = primals_ = None getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ invoke_subgraph_ = None add f = torch ops aten add Tensor getitem getitem_ getitem = getitem_ = None add getitem_ getitem_ getitem_ getitem_ getitem_ getitem_ partitioned_fw_subgraph_ _ torch nn Module forward primals_ f primals_ f primals_ f mul f = torch ops aten mul Tensor primals_ primals_ sin f = torch ops aten sin default mul mul = None mul_ f = torch ops aten mul Tensor sin sin = None mul_ f = torch ops aten mul Tensor mul_ primals_ mul_ = None mul_ primals_ primals_ primals_ ignore_empty_lines=True assertExpectedInline normalize_gm backend bw_graphs print_readable print_output=False \ GraphModule torch nn Module forward getitem_ f getitem_ f getitem_ f getitem_ f getitem_ f getitem_ f tangents_ f partitioned_bw_subgraph_ _ = partitioned_bw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_bw_subgraph_ _ partitioned_bw_subgraph_ _ getitem_ getitem_ getitem_ tangents_ partitioned_bw_subgraph_ _ = getitem_ = getitem_ = getitem_ = None getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ invoke_subgraph_ = None partitioned_bw_subgraph_ _ = partitioned_bw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_bw_subgraph_ _ partitioned_bw_subgraph_ _ getitem_ getitem_ getitem_ tangents_ partitioned_bw_subgraph_ _ = getitem_ = getitem_ = getitem_ = tangents_ = None getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ invoke_subgraph_ = None add_ f = torch ops aten add Tensor getitem_ getitem_ getitem_ = getitem_ = None add_ f = torch ops aten add Tensor getitem_ getitem_ getitem_ = getitem_ = None add_ add_ None partitioned_bw_subgraph_ _ torch nn Module forward primals_ f primals_ f primals_ f tangents_ f mul_ f = torch ops aten mul Tensor tangents_ primals_ tangents_ = primals_ = None mul_ f = torch ops aten mul Tensor mul_ mul_ = None mul f = torch ops aten mul Tensor primals_ primals_ cos f = torch ops aten cos default mul mul = None mul_ f = torch ops aten mul Tensor mul_ cos mul_ = cos = None mul_ f = torch ops aten mul Tensor mul_ primals_ primals_ = None mul_ f = torch ops aten mul Tensor mul_ primals_ mul_ = primals_ = None mul_ mul_ None ignore_empty_lines=True test_buffer_mutation_works_under_no_grad Mod torch nn Module __init__ super __init__ register_buffer buf torch ones requires_grad=False nested_compile_region forward x y buf add_ torch mul x y sin buf mod_ref = Mod mod = Mod fn mod x y mod x y + mod x y x = torch randn requires_grad=True y = torch randn requires_grad=True ref = fn mod_ref x y x_clone = x detach clone requires_grad_ True y_clone = y detach clone requires_grad_ True torch no_grad res = torch compile fn fullgraph=True mod x_clone y_clone assertEqual ref res assertEqual mod_ref buf mod buf mod = Mod x_clone = x detach clone requires_grad_ True y_clone = y detach clone requires_grad_ True torch inference_mode res = torch compile fn fullgraph=True mod x_clone y_clone assertEqual ref res assertEqual mod_ref buf mod buf mod = Mod x_clone = x detach clone requires_grad_ False y_clone = y detach clone requires_grad_ False res = torch compile fn fullgraph=True mod x_clone y_clone assertEqual ref res assertEqual mod_ref buf mod buf test_buffer_mutation_errors_under_training Mod torch nn Module __init__ super __init__ register_buffer buf torch ones requires_grad=False nested_compile_region forward x y buf add_ torch mul x y sin buf mod = Mod fn mod x y mod x y + mod x y x = torch randn requires_grad=True y = torch randn requires_grad=True assertRaisesRegex RuntimeError does currently support training in-place input buffer mutations torch compile fn backend= inductor fullgraph=True mod x y test_list nested_compile_region gn x y torch mul x y torch add x y fn x y lst = gn x y lst append torch sin x lst + lst + lst x = torch randn requires_grad=True y = torch randn requires_grad=True ref = fn x y x_clone = x detach clone requires_grad_ True y_clone = y detach clone requires_grad_ True res = torch compile fn backend= inductor fullgraph=True x_clone y_clone Run backward ref sum backward res sum backward assertEqual ref res assertEqual x grad x_clone grad assertEqual y grad y_clone grad test_tuple_of_tuple nested_compile_region gn x y torch mul x y torch add x y fn x y tup = gn x y tup + tup x = torch randn requires_grad=True y = torch randn requires_grad=True ref = fn x y x_clone = x detach clone requires_grad_ True y_clone = y detach clone requires_grad_ True res = torch compile fn backend= inductor fullgraph=True x_clone y_clone Run backward ref sum backward res sum backward assertEqual ref res assertEqual x grad x_clone grad assertEqual y grad y_clone grad unittest skip FunctionCtx ops cacheable right now test_differing_strides_for_grad_outs CustomOp torch autograd Function staticmethod forward ctx x torch sin x staticmethod backward ctx grad_out = grad_out view torch cos torch reshape nested_compile_region gn x CustomOp apply x fn x = gn x Force stride changes so backward view causes failure contiguous called b = torch permute b x = torch randn requires_grad=True ref = torch permute gn x x_clone = x clone detach requires_grad_ True opt_fn = torch compile fn backend= aot_eager res = opt_fn x_clone Run backward ref sum backward res sum backward assertEqual ref res assertEqual x grad x_clone grad requires_cuda_and_triton test_sdpa nested_compile_region gn q k v torch nn functional scaled_dot_product_attention q k v attn_mask=None dropout_p= is_causal=True fn q k v torch nn attention sdpa_kernel torch nn attention SDPBackend FLASH_ATTENTION gn q k v q = torch randn device= cuda dtype=torch bfloat requires_grad=True k = torch randn device= cuda dtype=torch bfloat requires_grad=True v = torch randn device= cuda dtype=torch bfloat requires_grad=True ref = fn q k v opt_fn = torch compile fn backend= inductor fullgraph=True res = opt_fn q k v res sum backward assertEqual ref res res = opt_fn q k v res sum backward test_symint_from_fwd_to_bwd nested_compile_region gn x y = torch sum x keepdim=True view y shape y shape torch matmul y fn x y gn x y opt_fn = torch compile fn backend= inductor fullgraph=True x = torch randn requires_grad=True y = torch randn requires_grad=True ref = fn x y res = opt_fn x y assertEqual ref res x = torch randn requires_grad=True y = torch randn requires_grad=True ref = fn x y res = opt_fn x y assertEqual ref res res sum backward x = torch randn requires_grad=True y = torch randn requires_grad=True ref = fn x y res = opt_fn x y assertEqual ref res res sum backward inductor_config patch fx_graph_cache False test_dropout_checks_joint_graph ` dropout ` tests joint graph passes just partitioner ran hop graphs Inductor rng functionalization happens joint graph passes Without running joint graph passes we would get error like AssertionError should have been handled replace_random py nested_compile_region gn x torch nn functional dropout torch sin x p= nested_compile_region hn x torch sin x fn x gn x + hn x x = torch randn requires_grad=True Difficult check results here because we random does match between eager Triton res = torch compile fn backend= inductor fullgraph=True x noqa F torch compiler reset backend = InductorAndRecordGraphs res = torch compile fn backend=backend fullgraph=True x res sum backward TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend inductor_graphs print_readable print_output=False \ GraphModule torch nn Module forward primals_ f partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ partitioned_fw_subgraph_ _ = None getitem_ b = invoke_subgraph_ getitem_ f = invoke_subgraph_ getitem f = invoke_subgraph_ invoke_subgraph_ = None partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ partitioned_fw_subgraph_ _ = primals_ = None getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ invoke_subgraph_ = None add f = torch ops aten add Tensor getitem getitem_ getitem = getitem_ = None add getitem_ getitem_ getitem_ partitioned_fw_subgraph_ _ torch nn Module forward primals_ f sin f = torch ops aten sin default primals_ inductor_seeds_default i = torch ops prims inductor_seeds default device type= cpu inductor_lookup_seed_default i = torch ops prims inductor_lookup_seed default inductor_seeds_default inductor_seeds_default = None inductor_random_default f = torch ops prims inductor_random default inductor_lookup_seed_default rand inductor_lookup_seed_default = None gt b = torch ops aten gt Scalar inductor_random_default inductor_random_default = None mul f = torch ops aten mul Tensor gt sin sin = None mul_ f = torch ops aten mul Tensor mul mul = None mul_ primals_ gt partitioned_fw_subgraph_ _ torch nn Module forward primals_ f sin f = torch ops aten sin default primals_ sin primals_ ignore_empty_lines=True inductor_config patch fx_graph_cache False test_dropout_checks_joint_graph_inference Checks joint graph results inductor seeds just inference graph nested_compile_region gn x torch nn functional dropout torch sin x p= fn x gn x backend = InductorAndRecordGraphs x = torch randn requires_grad=False torch compile fn backend=backend fullgraph=True x TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend inductor_graphs print_readable print_output=False \ lambda torch nn Module forward arg _ f repeated_subgraph = repeated_subgraph invoke_subgraph = torch ops higher_order invoke_subgraph repeated_subgraph subgraph_ arg _ repeated_subgraph = arg _ = None getitem f = invoke_subgraph invoke_subgraph = None getitem repeated_subgraph torch nn Module forward arg _ f inductor_seeds_default i = torch ops prims inductor_seeds default device type= cpu inductor_lookup_seed_default i = torch ops prims inductor_lookup_seed default inductor_seeds_default inductor_seeds_default = None inductor_random_default f = torch ops prims inductor_random default inductor_lookup_seed_default rand inductor_lookup_seed_default = None gt b = torch ops aten gt Scalar inductor_random_default inductor_random_default = None sin f = torch ops aten sin default arg _ arg _ = None mul f = torch ops aten mul Tensor gt sin gt = sin = None mul_ f = torch ops aten mul Tensor mul mul = None mul_ ignore_empty_lines=True test_dedupe nested_compile_region gn x y torch mul x y fn x y = gn x y gn y x = torch randn requires_grad=True y = torch randn requires_grad=True ref = fn x y x_clone = x detach clone requires_grad_ True y_clone = y detach clone requires_grad_ True backend = AotEagerAndRecordGraphs res = torch compile fn backend=backend fullgraph=True x_clone y_clone Run backward ref sum backward res sum backward assertEqual ref res assertEqual x grad x_clone grad assertEqual y grad y_clone grad Check Dynamo AOT graphs have just one subgraph module assertEqual len backend graphs assertEqual len backend fw_graphs assertEqual len backend bw_graphs count_unique_get_attr_nodes backend graphs count_unique_get_attr_nodes backend fw_graphs count_unique_get_attr_nodes backend bw_graphs TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ subgraph_ = subgraph_ invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ l_y_ subgraph_ = l_x_ = None f = invoke_subgraph invoke_subgraph = None subgraph_ = subgraph_ invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_y_ subgraph_ = = l_y_ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None getitem_ subgraph_ torch nn Module forward l_x_ f l_y_ f mul f = torch mul l_x_ l_y_ l_x_ = l_y_ = None mul assertExpectedInline normalize_gm backend fw_graphs print_readable print_output=False \ GraphModule torch nn Module forward primals_ f primals_ f partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ primals_ partitioned_fw_subgraph_ _ = primals_ = None getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ getitem f = invoke_subgraph_ invoke_subgraph_ = None partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ getitem primals_ partitioned_fw_subgraph_ _ = getitem = primals_ = None getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ invoke_subgraph_ = None getitem_ getitem_ getitem_ getitem_ getitem_ partitioned_fw_subgraph_ _ torch nn Module forward primals_ f primals_ f mul f = torch ops aten mul Tensor primals_ primals_ mul primals_ primals_ ignore_empty_lines=True test_dce nested_compile_region gn x x = torch sin x should dce d y = torch cos x noqa F x fn x gn x backend = AotEagerAndRecordGraphs torch compile fn backend=backend fullgraph=True torch randn requires_grad=False TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend fw_graphs print_readable print_output=False \ lambda torch nn Module forward arg _ f repeated_subgraph = repeated_subgraph invoke_subgraph = torch ops higher_order invoke_subgraph repeated_subgraph subgraph_ arg _ repeated_subgraph = arg _ = None getitem f = invoke_subgraph invoke_subgraph = None getitem repeated_subgraph torch nn Module forward arg _ f sin f = torch ops aten sin default arg _ arg _ = None sin test_nonlocal_update counter = nested_compile_region gn x y nonlocal counter torch mul x y counter fn x y nonlocal counter counter = = gn x y counter = gn y x = torch randn requires_grad=True y = torch randn requires_grad=True ref = fn x y x_clone = x detach clone requires_grad_ True y_clone = y detach clone requires_grad_ True res = torch compile fn backend= inductor fullgraph=True x_clone y_clone Run backward ref sum backward res sum backward assertEqual ref res assertEqual x grad x_clone grad assertEqual y grad y_clone grad torch _dynamo reset backend = AotEagerAndRecordGraphs torch compile fn backend=backend fullgraph=True x_clone y_clone TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ subgraph_ = subgraph_ invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ l_y_ subgraph_ = l_x_ = None f = invoke_subgraph invoke_subgraph = None subgraph_ = subgraph_ invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_y_ subgraph_ = = l_y_ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None getitem_ subgraph_ torch nn Module forward l_x_ f l_y_ f mul f = torch mul l_x_ l_y_ l_x_ = l_y_ = None child f = mul mul = None child subgraph_ torch nn Module forward f l_y_ f mul f = torch mul l_y_ = l_y_ = None child f = mul mul = None child inductor_config patch fx_graph_cache False test_view_to_reshape nested_compile_region gn x x = torch sin x x = x view torch sin x fn x gn x x = torch randn requires_grad=False torch _dynamo reset backend = InductorAndRecordGraphs torch compile fn backend=backend fullgraph=True x TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend inductor_graphs print_readable print_output=False \ lambda torch nn Module forward arg _ f repeated_subgraph = repeated_subgraph invoke_subgraph = torch ops higher_order invoke_subgraph repeated_subgraph subgraph_ arg _ repeated_subgraph = arg _ = None getitem f = invoke_subgraph invoke_subgraph = None getitem repeated_subgraph torch nn Module forward arg _ f sin f = torch ops aten sin default arg _ arg _ = None view f = torch ops aten reshape default sin sin = None sin_ f = torch ops aten sin default view view = None sin_ test_normalize_gm nested_compile_region gn x y Different graph give different names intermediate nodes _ range x = x y x fn x y _ range x = gn x y x backend = AotEagerAndRecordGraphs opt_fn = torch compile fn backend=backend fullgraph=True x = torch randn requires_grad=True y = torch randn requires_grad=True opt_fn x y TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ subgraph_ = subgraph_ invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ l_y_ subgraph_ = l_x_ = None x f = invoke_subgraph invoke_subgraph = None subgraph_ = subgraph_ invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ x l_y_ subgraph_ = x = None x_ f = invoke_subgraph_ invoke_subgraph_ = None subgraph_ = subgraph_ invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ x_ l_y_ subgraph_ = x_ = None x_ f = invoke_subgraph_ invoke_subgraph_ = None subgraph_ = subgraph_ invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ x_ l_y_ subgraph_ = x_ = None x_ f = invoke_subgraph_ invoke_subgraph_ = None subgraph_ = subgraph_ invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ x_ l_y_ subgraph_ = x_ = l_y_ = None x_ f = invoke_subgraph_ invoke_subgraph_ = None x_ subgraph_ torch nn Module forward l_x_ f l_y_ f x f = l_x_ l_y_ l_x_ = None x_ f = x l_y_ x = None x_ f = x_ l_y_ x_ = None x_ f = x_ l_y_ x_ = None x_ f = x_ l_y_ x_ = l_y_ = None x_ test_input_mutation nested_compile_region gn x y x add_ torch mul x y fn x y gn x y x = torch randn requires_grad=False y = torch randn requires_grad=False opt_fn = torch compile fn backend= inductor fullgraph=True x_clone = x clone assertEqual opt_fn x y fn x_clone y test_input_mutation_mutiple_times nested_compile_region gn x y x add_ torch mul x y fn x y z = gn x y _ range z += gn x y z x = torch randn requires_grad=False x_clone = x clone y = torch randn requires_grad=False opt_fn = torch compile fn backend= inductor fullgraph=True torch no_grad out = opt_fn x y exp_out = fn x_clone y assertEqual exp_out out assertEqual x_clone x test_input_mutation_mutiple_times_fake_tensor_cahche_hit nested_compile_region gn x y x add_ torch mul x y fn x y z = gn x y _ range z += gn x y z x = torch randn requires_grad=False x_clone = x clone y = torch randn requires_grad=False backend = AotEagerAndRecordGraphs opt_fn = torch compile fn backend=backend fullgraph=True fake_prop_count = _mock_invoke_subgraph mode subgraph identifier operands nonlocal fake_prop_count fake_prop_count += operands clone mock patch torch _higher_order_ops utils registered_hop_fake_fns torch ops higher_order invoke_subgraph _mock_invoke_subgraph torch no_grad out = opt_fn x y Fake propagation occurs only twice subsequent calls using cached results First fake propagation collect_metadata_analysis AOT - Uses original Dynamo graph - Flow functionalization - fake tensor Second fake propagation _create_graph AOT - Uses materialized graph includes epilogue operations - Flow functionalization - proxy - fake tensor The key difference second time we materialize graph epilogue operations included proxy key Since dynamo graph module functional + epilogue format cache key should different preventing cache reuse between these two phases assertEqual fake_prop_count exp_out = fn x_clone y assertEqual exp_out out assertEqual x_clone x test_input_mutation_inference_mode nested_compile_region gn x y x add_ torch mul x y fn x y z = torch cos x torch inference_mode gn torch cos z y opt_fn = torch compile fn backend= inductor fullgraph=True x = torch randn requires_grad=False y = torch randn requires_grad=False assertRaisesRegex RuntimeError Inplace update inference tensor outside InferenceMode allowed opt_fn x y test_simple_module mod = torch nn Linear nested_compile_region gn x torch cos x mod x fn x out = gn x out + out opt_fn = torch compile fn backend= inductor fullgraph=True requires_grad False deliberately force None joint_graph outputs x = torch randn requires_grad=False x_clone = x detach clone requires_grad_ False ref = fn x res = opt_fn x_clone ref sum backward res sum backward assertEqual ref res assertEqual x grad x_clone grad test_fail_with_direct_invoke_subgraph torch _higher_order_ops invoke_subgraph gn x torch sin x fn x invoke_subgraph gn None x opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn requires_grad=True assertRaisesRegex torch _dynamo exc Unsupported Directly using invoke_subgraph opt_fn x test_input_output_aliasing nested_compile_region gn x y x torch mul x y fn x y outs = gn x y outs outs x = torch randn requires_grad=False y = torch randn requires_grad=False opt_fn = torch compile fn backend= inductor fullgraph=True assertRaisesRegex torch _dynamo exc UncapturedHigherOrderOpError Encountered aliasing during higher order op tracing opt_fn x y test_input_input_aliasing nested_compile_region gn x y torch mul x y fn x gn x x view x = torch randn requires_grad=False opt_fn = torch compile fn backend= inductor fullgraph=True assertRaisesRegex torch _dynamo exc UncapturedHigherOrderOpError Encountered aliasing during higher order op tracing opt_fn x test_output_output_aliasing nested_compile_region gn x z = torch cos x z z view fn x gn x x = torch randn requires_grad=False opt_fn = torch compile fn backend= inductor fullgraph=True assertRaisesRegex torch _dynamo exc UncapturedHigherOrderOpError Encountered aliasing during higher order op tracing opt_fn x test_mod_attr_aliasing MutateParam torch nn Module __init__ super __init__ = torch ones forward x add_ torch mul x nested_compile_region gn x mod x fn x y gn x y mod = MutateParam x = torch randn requires_grad=False y = torch randn requires_grad=False opt_fn = torch compile fn backend= inductor fullgraph=True compiled_out = opt_fn x y reset constant attr mod = torch ones assertEqual compiled_out fn x y test_redundant_compile_region nested_compile_region nested_compile_region gn x torch sin x fn x gn x + gn x backend = AotEagerAndRecordGraphs opt_fn = torch compile fn backend=backend fullgraph=True x = torch randn requires_grad=True ref = fn x res = opt_fn x assertEqual ref res TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ subgraph_ = subgraph_ invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ subgraph_ = None getitem f = invoke_subgraph invoke_subgraph = None subgraph_ = subgraph_ invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ subgraph_ = l_x_ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None add f = getitem + getitem_ getitem = getitem_ = None add subgraph_ torch nn Module forward l_x_ f sin f = torch sin l_x_ l_x_ = None sin test_kwargs_only nested_compile_region gn x y x y x = torch randn requires_grad=False y = torch randn requires_grad=False fn x y gn x y=y ref = fn x y opt_fn = torch compile fn backend= inductor fullgraph=True res = opt_fn x y assertEqual ref res test_module_method Mod torch nn Module __init__ super __init__ linear = torch nn Linear nested_compile_region helper x linear x forward x x + helper x helper x + x mod = Mod backend = AotEagerAndRecordGraphs opt_mod = torch compile mod backend=backend fullgraph=True x = torch randn requires_grad=True ref = mod x res = opt_mod x assertEqual ref res TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_x_ f L_self_modules_linear_parameters_weight_ f L_self_modules_linear_parameters_bias_ f l_x_ = L_x_ l_self_modules_linear_parameters_weight_ = L_self_modules_linear_parameters_weight_ l_self_modules_linear_parameters_bias_ = L_self_modules_linear_parameters_bias_ subgraph_ = subgraph_ invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ l_self_modules_linear_parameters_weight_ l_self_modules_linear_parameters_bias_ subgraph_ = None getitem f = invoke_subgraph invoke_subgraph = None subgraph_ = subgraph_ invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ l_self_modules_linear_parameters_weight_ l_self_modules_linear_parameters_bias_ subgraph_ = l_self_modules_linear_parameters_weight_ = l_self_modules_linear_parameters_bias_ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None mul f = getitem getitem_ getitem = getitem_ = None add f = l_x_ + mul mul = None add_ f = add + l_x_ add = l_x_ = None add_ subgraph_ torch nn Module forward l_x_ f l_self_modules_linear_parameters_weight_ f l_self_modules_linear_parameters_bias_ f linear f = torch _C _nn linear l_x_ l_self_modules_linear_parameters_weight_ l_self_modules_linear_parameters_bias_ l_x_ = l_self_modules_linear_parameters_weight_ = l_self_modules_linear_parameters_bias_ = None linear test_module SubMod torch nn Module __init__ super __init__ forward x torch sin x Mod torch nn Module __init__ super __init__ submod = nested_compile_region SubMod forward x x + submod x submod x + x mod = Mod backend = AotEagerAndRecordGraphs opt_mod = torch compile mod backend=backend fullgraph=True x = torch randn requires_grad=True ref = mod x res = opt_mod x assertEqual ref res TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ subgraph_ = subgraph_ invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ subgraph_ = None getitem f = invoke_subgraph invoke_subgraph = None subgraph_ = subgraph_ invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ subgraph_ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None mul f = getitem getitem_ getitem = getitem_ = None add f = l_x_ + mul mul = None add_ f = add + l_x_ add = l_x_ = None add_ subgraph_ torch nn Module forward l_x_ f sin f = torch sin l_x_ l_x_ = None sin requires_cuda_and_triton test_return_none torch nn functional F weight = torch ones device= cuda dtype=torch float requires_grad=True ones = torch ones device= cuda dtype=torch float nested_compile_region fn x train F dropout x weight train torch _dynamo optimize_assert inductor run x train=True fn x train r = run ones train=False r sum backward weight grad clone test_return_none_from_fwd nested_compile_region gn x x None x fn x ys = gn x ys + ys opt_fn = torch compile fn backend= inductor fullgraph=True x = torch randn requires_grad=True x_clone = x detach clone requires_grad_ True ref = fn x res = opt_fn x_clone ref sum backward res sum backward assertEqual ref res assertEqual x grad x_clone grad backend = AotEagerAndRecordGraphs opt_fn = torch compile fn backend=backend fullgraph=True x = torch randn requires_grad=True res = opt_fn x_clone res sum backward assertEqual len backend graphs assertEqual len backend fw_graphs assertEqual len backend bw_graphs count_unique_get_attr_nodes backend graphs count_unique_get_attr_nodes backend fw_graphs count_unique_get_attr_nodes backend bw_graphs TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ subgraph_ = subgraph_ invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ subgraph_ = l_x_ = None getitem f = invoke_subgraph getitem_ f = invoke_subgraph invoke_subgraph = None add f = getitem + getitem_ getitem = getitem_ = None add subgraph_ torch nn Module forward l_x_ f child f = l_x_ child_ f = l_x_ l_x_ = None child child_ assertExpectedInline normalize_gm backend fw_graphs print_readable print_output=False \ GraphModule torch nn Module forward primals_ f partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ partitioned_fw_subgraph_ _ = primals_ = None getitem f = invoke_subgraph_ getitem_ f = invoke_subgraph_ invoke_subgraph_ = None add f = torch ops aten add Tensor getitem getitem_ getitem = getitem_ = None add partitioned_fw_subgraph_ _ torch nn Module forward primals_ f mul f = torch ops aten mul Tensor primals_ mul_ f = torch ops aten mul Tensor primals_ primals_ = None mul mul_ assertExpectedInline normalize_gm backend bw_graphs print_readable print_output=False \ GraphModule torch nn Module forward tangents_ f partitioned_bw_subgraph_ _ = partitioned_bw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_bw_subgraph_ _ partitioned_bw_subgraph_ _ tangents_ tangents_ partitioned_bw_subgraph_ _ = tangents_ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None getitem_ partitioned_bw_subgraph_ _ torch nn Module forward tangents_ f tangents_ f mul_ f = torch ops aten mul Tensor tangents_ mul_ f = torch ops aten mul Tensor tangents_ tangents_ = None add f = torch ops aten add Tensor mul_ mul_ mul_ = mul_ = None add test_dynamic nested_compile_region gn x torch sin x fn x gn x + gn x x = torch randn requires_grad=True torch _dynamo mark_dynamic x ref = fn x opt_fn = torch compile fn backend= inductor fullgraph=True res = opt_fn x assertEqual ref res test_complex Observed Wan nested_compile_region gn x torch sin x fn x gn x + gn x x = torch randn dtype=torch complex ref = fn x opt_fn = torch compile fn backend= inductor fullgraph=True res = opt_fn x assertEqual ref res torch _dynamo config patch capture_scalar_outputs=True test_pending_unbacked nested_compile_region gn x u = x item x u fn x gn x x = torch randn torch _dynamo mark_dynamic x ref = fn x opt_fn = torch compile fn backend= eager fullgraph=True Inductor fails cpp compilation error res = opt_fn x assertEqual ref res torch _dynamo config patch capture_scalar_outputs=True test_unbacked nested_compile_region gn x y b = x item y b clone fn x y gn x y x = torch tensor y = torch randn ref = fn x y opt_fn = torch compile fn backend= eager fullgraph=True Inductor fails assertion error when lowering aten sym_constrain_range_for_size default res = opt_fn x y assertEqual ref res torch _dynamo config patch capture_scalar_outputs=True test_unbacked nested_compile_region gn x y b = x item torch _check b = torch _check b y shape y b clone fn x y gn x y x = torch tensor y = torch randn ref = fn x y opt_fn = torch compile fn backend= eager fullgraph=True Inductor fails assertion error when lowering aten sym_constrain_range_for_size default res = opt_fn x y assertEqual ref res test_bwd_partitioning nested_compile_region gn x y z = torch matmul x y torch sin z fn x y torch sin gn x y backend = AotEagerAndRecordGraphs opt_fn = torch compile fn backend=backend fullgraph=True x = torch randn requires_grad=True y = torch randn requires_grad=True x_clone = x detach clone requires_grad_ True y_clone = y detach clone requires_grad_ True ref = fn x y res = opt_fn x_clone y_clone ref sum backward res sum backward assertEqual ref res assertEqual x grad x_clone grad assertEqual y grad y_clone grad TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend fw_graphs print_readable print_output=False \ GraphModule torch nn Module forward primals_ f primals_ f partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ primals_ partitioned_fw_subgraph_ _ = primals_ = primals_ = None getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ getitem f = invoke_subgraph_ invoke_subgraph_ = None sin f = torch ops aten sin default getitem cos f = torch ops aten cos default getitem getitem = None sin getitem_ getitem_ getitem_ cos partitioned_fw_subgraph_ _ torch nn Module forward primals_ f primals_ f mm f = torch ops aten mm default primals_ primals_ sin f = torch ops aten sin default mm t f = torch ops aten t default primals_ primals_ = None t_ f = torch ops aten t default primals_ primals_ = None sin mm t t_ assertExpectedInline normalize_gm backend bw_graphs print_readable print_output=False \ GraphModule torch nn Module forward getitem_ f getitem_ f getitem_ f cos f tangents_ f mul f = torch ops aten mul Tensor tangents_ cos tangents_ = cos = None partitioned_bw_subgraph_ _ = partitioned_bw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_bw_subgraph_ _ partitioned_bw_subgraph_ _ getitem_ getitem_ getitem_ mul partitioned_bw_subgraph_ _ = getitem_ = getitem_ = getitem_ = mul = None getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ invoke_subgraph_ = None getitem_ getitem_ partitioned_bw_subgraph_ _ torch nn Module forward mm f t f t_ f tangents_ f cos f = torch ops aten cos default mm mm = None mul f = torch ops aten mul Tensor tangents_ cos tangents_ = cos = None mm_ f = torch ops aten mm default t mul t = None mm_ f = torch ops aten mm default mul t_ mul = t_ = None mm_ mm_ test_const_tensor nested_compile_region gn x torch tensor dtype=torch float x fn x gn x + gn x x = torch randn requires_grad=True opt_fn = torch compile fn backend= aot_eager fullgraph=True ref = fn x res = opt_fn x assertEqual ref res test_ac fn x torch cos x nested_compile_region fn _checkpoint x torch utils checkpoint checkpoint fn x use_reentrant=False fn x torch sin x nested_compile_region fn _checkpoint x torch utils checkpoint checkpoint fn x use_reentrant=False fn x fn _checkpoint x repeat same fn _checkpoint see we dedupe + fn _checkpoint x Check new fn _checkpoint goes through different HOP + fn _checkpoint x x = torch randn requires_grad=True ref = fn x x_clone = x clone detach requires_grad_ True backend = AotEagerAndRecordGraphs res = torch compile fn backend=backend fullgraph=True x_clone Run backward ref sum backward res sum backward assertEqual ref res assertEqual x grad x_clone grad Check Dynamo AOT graphs have just one subgraph module assertEqual len backend graphs assertEqual len backend fw_graphs assertEqual len backend bw_graphs count_unique_get_attr_nodes backend graphs count_unique_get_attr_nodes backend fw_graphs count_unique_get_attr_nodes backend bw_graphs res = torch compile fn backend= inductor fullgraph=True x_clone assertEqual ref res torch _inductor config patch fallback_random=True test_ac_rng fn x torch cos torch nn functional dropout x p= nested_compile_region fn _checkpoint x torch utils checkpoint checkpoint fn x use_reentrant=False fn x fn _checkpoint x + fn _checkpoint x x = torch randn requires_grad=True torch manual_seed ref = fn x ref sum backward x_clone = x clone detach requires_grad_ True backend = AotEagerAndRecordGraphs torch manual_seed res = torch compile fn backend=backend fullgraph=True x_clone res sum backward assertEqual ref res assertEqual x grad x_clone grad Check Dynamo AOT graphs have just one subgraph module assertEqual len backend graphs assertEqual len backend fw_graphs assertEqual len backend bw_graphs torch manual_seed res = torch compile fn backend= inductor fullgraph=True x_clone assertEqual ref res res sum backward requires_gpu test_ac_rng_cudagraphs fn q k v torch nn functional scaled_dot_product_attention q k v attn_mask=None dropout_p= is_causal=True nested_compile_region fn _checkpoint q k v torch utils checkpoint checkpoint fn q k v use_reentrant=False fn q k v fn _checkpoint q k v + fn _checkpoint q cos k v q = torch randn device=GPU_TYPE dtype=torch bfloat requires_grad=True k = torch randn device=GPU_TYPE dtype=torch bfloat requires_grad=True v = torch randn device=GPU_TYPE dtype=torch bfloat requires_grad=True res = torch compile fn backend= inductor fullgraph=True mode= reduce-overhead q k v res sum backward test_fake_tensor_checking nested_compile_region gn x torch sin x fn x y x y different shapes so we should use different graph gn x gn y backend = AotEagerAndRecordGraphs opt_fn = torch compile fn backend=backend fullgraph=True x = torch randn requires_grad=True y = torch randn requires_grad=True ref = fn x y res = opt_fn x y assertEqual ref res TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ subgraph_ = subgraph_ invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ subgraph_ = l_x_ = None getitem f = invoke_subgraph invoke_subgraph = None subgraph_ = subgraph_ invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_y_ subgraph_ = l_y_ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None getitem getitem_ subgraph_ torch nn Module forward l_x_ f sin f = torch sin l_x_ l_x_ = None sin subgraph_ torch nn Module forward l_y_ f sin f = torch sin l_y_ l_y_ = None sin test_return_size run dynamic torch compiler reset nested_compile_region gn x y = x + z = x shape y z fn x z = gn x z = gn x z + z z x = torch randn requires_grad=True x_clone = x detach clone requires_grad_ True ref = fn x opt_fn = torch compile fn backend= inductor fullgraph=True dynamic=dynamic res = opt_fn x_clone assertEqual ref res ref sum backward res sum backward assertEqual x grad x_clone grad run dynamic=True run dynamic=False test_different_symint Tests check same subgraph called different symints use different graphs nested_compile_region gn x torch sin x fn x = gn x Get first half tensor b = torch narrow size gn b opt_fn = torch compile fn fullgraph=True x = torch randn requires_grad=True torch _dynamo mark_dynamic x ref = fn x res = opt_fn x torch _dynamo reset backend = AotEagerAndRecordGraphs opt_fn = torch compile fn backend=backend fullgraph=True x = torch randn requires_grad=True torch _dynamo mark_dynamic x ref = fn x res = opt_fn x assertEqual ref res TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward s Sym s L_x_ f s l_x_ = L_x_ subgraph_ = subgraph_ invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ s l_x_ subgraph_ = l_x_ = None f s = invoke_subgraph invoke_subgraph = None floordiv Sym s = s b f s = torch narrow floordiv = floordiv = None subgraph_ = subgraph_ invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ s b subgraph_ = s = b = None getitem_ f s = invoke_subgraph_ invoke_subgraph_ = None getitem_ subgraph_ torch nn Module forward s Sym s l_x_ f s sin f s = torch sin l_x_ l_x_ = None sin subgraph_ torch nn Module forward s Sym s b f s sin f s = torch sin b b = None sin test_autograd_function CustomOp torch autograd Function staticmethod forward ctx x ctx save_for_backward x torch sin x staticmethod backward ctx grad_out x = ctx saved_tensors x torch cos grad_out nested_compile_region gn x CustomOp apply x fn x gn x + gn x backend = AotEagerAndRecordGraphs opt_fn = torch compile fn backend=backend fullgraph=True x = torch randn requires_grad=True x_clone = x detach clone requires_grad_ True ref = fn x res = opt_fn x_clone ref sum backward res sum backward assertEqual ref res assertEqual x grad x_clone grad TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ subgraph_ = subgraph_ invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ subgraph_ = None getitem f = invoke_subgraph invoke_subgraph = None subgraph_ = subgraph_ invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ subgraph_ = l_x_ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None add f = getitem + getitem_ getitem = getitem_ = None add subgraph_ torch nn Module forward l_x_ f fwd_body_ = fwd_body_ bwd_body_ = bwd_body_ autograd_function_apply f = torch ops higher_order autograd_function_apply fwd_body_ bwd_body_ l_x_ args_tensor_mask = True non_differentiable_idx = fwd_body_ = bwd_body_ = l_x_ = None autograd_function_apply fwd_body_ torch nn Module forward ctx torch autograd function Function x f _set_grad_enabled = torch _C _set_grad_enabled False _set_grad_enabled = None sin f = torch sin x _set_grad_enabled_ = torch _C _set_grad_enabled True _set_grad_enabled_ = None sin x bwd_body_ torch nn Module forward ctx torch autograd function Function grad_out f x f _set_grad_enabled = torch _C _set_grad_enabled False _set_grad_enabled = None cos f = torch cos grad_out grad_out = None mul f = x cos x = cos = None _set_grad_enabled_ = torch _C _set_grad_enabled True _set_grad_enabled_ = None mul requires_gpu test_triton_kernel_native torch testing _internal triton_utils add_kernel call_triton_add x torch Tensor y torch Tensor output torch Tensor grid_type int num= positional=False n_elements = output numel grid_fn meta triton cdiv num meta BLOCK_SIZE grid_type == grid = x numel grid_type == grid = lambda meta triton cdiv n_elements meta BLOCK_SIZE grid = grid_fn positional add_kernel grid x y output n_elements add_kernel grid x y output n_elements BLOCK_SIZE= output nested_compile_region gn x y o = torch zeros_like x call_triton_add x y o o sin fn x y x = x sin y = y sin z = gn x y gn z y t = torch rand device=GPU_TYPE t = torch rand device=GPU_TYPE ref = fn t t backend = AotEagerAndRecordGraphs opt_fn = torch compile fn backend=backend fullgraph=True assertEqual opt_fn t t ref NOTE THAT THIS TEST DOES NOT REALLY WORK We wanted one invoke_subgraph called twice because constant_args_idx changing graph graph equivalence fails TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_x_ f L_y_ f l_x_ = L_x_ l_y_ = L_y_ x f = l_x_ sin l_x_ = None y f = l_y_ sin l_y_ = None subgraph_ = subgraph_ invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ x y subgraph_ = x = None z f = invoke_subgraph invoke_subgraph = None subgraph_ = subgraph_ invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ z y subgraph_ = z = y = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None getitem_ subgraph_ torch nn Module forward x f y f o f = torch zeros_like x triton_kernel_wrapper_mutation = torch ops higher_order triton_kernel_wrapper_mutation kernel_idx = constant_args_idx = grid = tma_descriptor_metadata = kwargs = in_ptr x in_ptr y out_ptr o x = y = triton_kernel_wrapper_mutation = None sin f = o sin o = None sin subgraph_ torch nn Module forward z f y f o f = torch zeros_like z triton_kernel_wrapper_mutation = torch ops higher_order triton_kernel_wrapper_mutation kernel_idx = constant_args_idx = grid = tma_descriptor_metadata = kwargs = in_ptr z in_ptr y out_ptr o z = y = triton_kernel_wrapper_mutation = None sin f = o sin o = None sin torch _dynamo config patch capture_dynamic_output_shape_ops=True test_unbacked_symbol nested_compile_region gn x torch sin torch nonzero x fn x gn x + gn x x = torch randn requires_grad=True Inductor fails lowering error opt_fn = torch compile fn backend= aot_eager fullgraph=True ref = fn x res = opt_fn x assertEqual ref res test_different_strides_in_backward nested_compile_region gn x torch cos x fn x = gn x = gn b = torch sin c = gn b c = gn c c sum + c sum opt_fn = torch compile fn fullgraph=True x = torch randn requires_grad=True torch _dynamo mark_dynamic x x_clone = x detach clone requires_grad_ True torch _dynamo mark_dynamic x_clone ref = fn x res = opt_fn x_clone ref sum backward res sum backward assertEqual ref res torch compiler reset backend = AotEagerAndRecordGraphs opt_fn = torch compile fn backend=backend fullgraph=True x = torch randn requires_grad=True torch _dynamo mark_dynamic x x_clone = x detach clone requires_grad_ True torch _dynamo mark_dynamic x_clone ref = fn x res = opt_fn x_clone ref sum backward res sum backward assertEqual ref res assertEqual x grad x_clone grad TEST_WITH_CROSSREF assertExpectedInline normalize_gm backend fw_graphs print_readable print_output=False \ GraphModule torch nn Module forward primals_ Sym s primals_ f s partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ primals_ partitioned_fw_subgraph_ _ = primals_ = None getitem_ Sym s = invoke_subgraph_ getitem_ f s = invoke_subgraph_ getitem f s = invoke_subgraph_ invoke_subgraph_ = None partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ getitem partitioned_fw_subgraph_ _ = getitem = None getitem_ Sym s = invoke_subgraph_ getitem_ f s = invoke_subgraph_ getitem_ f s = invoke_subgraph_ invoke_subgraph_ = None sin f s = torch ops aten sin default getitem_ partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ sin partitioned_fw_subgraph_ _ = sin = None getitem_ Sym s = invoke_subgraph_ getitem_ f s = invoke_subgraph_ getitem_ f s = invoke_subgraph_ invoke_subgraph_ = None partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ getitem_ partitioned_fw_subgraph_ _ = None getitem_ Sym s = invoke_subgraph_ getitem_ f s = invoke_subgraph_ getitem_ f s = invoke_subgraph_ invoke_subgraph_ = None sum_ f = torch ops aten sum default getitem_ getitem_ = None sum_ f = torch ops aten sum default getitem_ getitem_ = None add_ f = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None cos f s = torch ops aten cos default getitem_ getitem_ = None add_ getitem_ getitem_ getitem_ getitem_ cos primals_ getitem_ getitem_ getitem_ getitem_ partitioned_fw_subgraph_ _ torch nn Module forward primals_ Sym s primals_ f s cos f s = torch ops aten cos default primals_ cos primals_ primals_ partitioned_fw_subgraph_ _ torch nn Module forward primals_ Sym s primals_ f s cos f s = torch ops aten cos default primals_ cos primals_ primals_ ignore_empty_lines=True assertExpectedInline normalize_gm backend bw_graphs print_readable print_output=False \ GraphModule torch nn Module forward primals_ Sym s getitem_ Sym s getitem_ Sym s getitem_ Sym s getitem_ Sym s getitem_ f s getitem_ f s getitem_ f s getitem_ f s cos f s tangents_ f expand f s = torch ops aten expand default tangents_ primals_ tangents_ = primals_ = None partitioned_bw_subgraph_ _ = partitioned_bw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_bw_subgraph_ _ partitioned_bw_subgraph_ _ getitem_ getitem_ expand partitioned_bw_subgraph_ _ = getitem_ = getitem_ = None getitem_ f s = invoke_subgraph_ invoke_subgraph_ = None add_ f s = torch ops aten add Tensor expand getitem_ expand = getitem_ = None partitioned_bw_subgraph_ _ = partitioned_bw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_bw_subgraph_ _ partitioned_bw_subgraph_ _ getitem_ getitem_ add_ partitioned_bw_subgraph_ _ = getitem_ = getitem_ = add_ = None getitem_ f s = invoke_subgraph_ invoke_subgraph_ = None mul_ f s = torch ops aten mul Tensor getitem_ cos getitem_ = cos = None partitioned_bw_subgraph_ _ = partitioned_bw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_bw_subgraph_ _ partitioned_bw_subgraph_ _ getitem_ getitem_ mul_ partitioned_bw_subgraph_ _ = getitem_ = getitem_ = mul_ = None getitem_ f s = invoke_subgraph_ invoke_subgraph_ = None partitioned_bw_subgraph_ _ = partitioned_bw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_bw_subgraph_ _ partitioned_bw_subgraph_ _ getitem_ getitem_ getitem_ partitioned_bw_subgraph_ _ = getitem_ = getitem_ = getitem_ = None getitem_ f s = invoke_subgraph_ invoke_subgraph_ = None None getitem_ partitioned_bw_subgraph_ _ torch nn Module forward primals_ Sym s primals_ f s tangents_ f s sin f s = torch ops aten sin default primals_ primals_ = None neg f s = torch ops aten neg default sin sin = None mul_ f s = torch ops aten mul Tensor tangents_ neg tangents_ = neg = None None mul_ partitioned_bw_subgraph_ _ torch nn Module forward primals_ Sym s primals_ f s tangents_ f s sin f s = torch ops aten sin default primals_ primals_ = None neg f s = torch ops aten neg default sin sin = None mul_ f s = torch ops aten mul Tensor tangents_ neg tangents_ = neg = None None mul_ ignore_empty_lines=True test_div nested_compile_region gn x div = torch div rounding_mode= trunc div torch ones div x fn x gn x x = torch randn requires_grad=True opt_fn = torch compile fn fullgraph=True ref = fn x res = opt_fn x assertEqual ref res requires_gpu test_preserves_strides _CustomPass PatternMatcherPass __init__ - None super __init__ __call__ g torch fx Graph apply g g = _CustomPass called = False x = torch randn device=GPU_TYPE other = torch randn device=GPU_TYPE register_graph_pattern CallFunctionVarArgs torch ops aten permute pass_dict=g _ match args kwargs flat_args spec = pytree tree_flatten args kwargs decomp flat_args args kwargs = pytree tree_unflatten flat_args spec torch ops mylib force_channels_last torch ops aten permute args kwargs nonlocal called called = True match replace_by_example decomp flat_args torch _inductor config torch library _scoped_library mylib FRAGMENT lib lib define force_channels_last Tensor x - Tensor tags= torch _C Tag flexible_layout impl x x clone memory_format=torch channels_last lib impl force_channels_last impl CompositeExplicitAutograd lib define add_op Tensor x Tensor y - Tensor impl x y out = y clone contiguous strides out add_ x transpose - - out meta x y torch empty_like y memory_format=torch contiguous_format lib impl add_op impl CompositeExplicitAutograd lib impl add_op meta Meta nested_compile_region gn y z torch ops mylib add_op default y z f x other y = x transpose contiguous transpose z = y sin transpose gn y z config patch post_grad_custom_post_pass=g f_compile = torch compile f fullgraph=True assertEqual f x other f_compile x other assertTrue called requires_gpu test_preserves_output_strides Have graph pass changes strides output op invoke_subgraph check output strides preserved x = torch randn device=GPU_TYPE other = torch randn device=GPU_TYPE _CustomPass PatternMatcherPass __init__ - None super __init__ __call__ g torch fx Graph apply g g = _CustomPass called = False register_graph_pattern CallFunctionVarArgs torch ops aten permute pass_dict=g _ match args kwargs flat_args spec = pytree tree_flatten args kwargs decomp flat_args args kwargs = pytree tree_unflatten flat_args spec torch ops mylib force_channels_last torch ops aten permute args kwargs nonlocal called called = True match replace_by_example decomp flat_args torch _inductor config torch library _scoped_library mylib FRAGMENT lib lib define force_channels_last Tensor x - Tensor tags= torch _C Tag flexible_layout impl x x clone memory_format=torch channels_last lib impl force_channels_last impl CompositeExplicitAutograd lib define add_op Tensor x Tensor y - Tensor impl x y Check input strides preserved This helps testing HOP preserves output strides assert x stride == assert y stride == out = y clone contiguous strides out add_ x transpose - - out meta x y torch empty_like y memory_format=torch contiguous_format lib impl add_op impl CompositeExplicitAutograd lib impl add_op meta Meta nested_compile_region gn x other y = x transpose contiguous transpose z = y sin transpose y z f x other y z = gn x other torch ops mylib add_op default y z config patch post_grad_custom_post_pass=g f_compile = torch compile f fullgraph=True assertEqual f x other f_compile x other assertTrue called skipIfTorchDynamo Not torch _dynamo test parameterized_class strict False strict True class_name_func=lambda cls _ params f cls __name__ Strict params strict Nonstrict TestInvokeSubgraphExport TestCase test_simple_func nested_compile_region gn x y torch mul x y M torch nn Module forward x y x = gn x y x = gn x y x x = torch randn requires_grad=True y = torch randn requires_grad=True ep = torch export export M x y strict=self strict assertTrue torch allclose ep module x y M x y assertEqual len list ep graph_module named_modules assertExpectedInline empty_line_normalizer normalize_gm ep graph_module print_readable print_output=False \ GraphModule torch nn Module forward x f y f repeated_subgraph = repeated_subgraph invoke_subgraph = torch ops higher_order invoke_subgraph repeated_subgraph subgraph_ x y repeated_subgraph = x = None getitem f = invoke_subgraph invoke_subgraph = None repeated_subgraph _ = repeated_subgraph invoke_subgraph_ = torch ops higher_order invoke_subgraph repeated_subgraph _ subgraph_ getitem y repeated_subgraph _ = getitem = y = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None getitem_ repeated_subgraph torch nn Module forward arg _ f arg _ f mul f = torch ops aten mul Tensor arg _ arg _ arg _ = arg _ = None mul test_unbacked nested_compile_region gn x y b = x item torch _check b y shape y b clone M torch nn Module forward x y res = _ range res append gn x y torch cat res x = torch tensor y = torch randn ep = torch export export M x y strict=self strict ep = ep run_decompositions assertTrue torch allclose ep module x y M x y assertEqual len list ep graph_module named_modules test_pending_unbacked M torch nn Module nested_compile_region gn x u = x item x u forward x _ range x = gn x x ep = torch export export M torch randn strict=self strict dynamic_shapes= x torch export Dim DYNAMIC ep = ep run_decompositions assertEqual len list ep graph_module named_modules ep = torch export export M torch randn requires_grad=True strict=self strict dynamic_shapes= x torch export Dim DYNAMIC ep = ep run_decompositions assertEqual len list ep graph_module named_modules test_simple_method M torch nn Module nested_compile_region gn x y torch mul x y forward x y x = gn x y x = gn x y x x = torch randn requires_grad=True y = torch randn requires_grad=True ep = torch export export M x y strict=self strict assertTrue torch allclose ep module x y M x y assertEqual len list ep graph_module named_modules test_multiple_module b = torch randn N torch nn Module __init__ super __init__ register_buffer buf b nested_compile_region forward x y x y + buf M torch nn Module __init__ super __init__ mod_list = torch nn ModuleList N _ range forward x y m mod_list x = m x y x x = torch randn requires_grad=True y = torch randn requires_grad=True ep = torch export export M x y strict=self strict assertTrue torch allclose ep module x y M x y assertEqual len list ep graph_module named_modules NegativeTesting TestCase test_graph_break nested_compile_region gn x torch _dynamo graph_break torch cos x fn x gn x x = torch randn requires_grad=True assertRaisesRegex RuntimeError torch compile requires ` nested_compile_region ` decorated function capturable into single graph torch compile fn backend= eager x __name__ == __main__ run_tests