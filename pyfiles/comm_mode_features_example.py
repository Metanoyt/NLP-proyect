To run example use following command torchrun -- standalone -- nnodes= -- nproc-per-node= comm_mode_features_example py -e MLP_operation_tracing argparse os typing TYPE_CHECKING Union torch torch nn nn torch distributed tensor DeviceMesh torch distributed tensor debug CommDebugMode torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel torch testing _internal distributed _tensor common_dtensor MLPModule MLPStacked ModelArgs NUM_DEVICES Transformer torch utils checkpoint checkpoint TYPE_CHECKING collections abc Callable get_device_type - str device_type = cpu torch accelerator device_count = device_type = getattr torch accelerator current_accelerator type cpu device_type c d_functional = torch ops c d_functional aten = torch ops aten supported_ops = aten view default aten _to_copy default CommDebugModeExample Checks set keys ground truth dictionary set produced advanced_module_tracker same order __init__ world_size int rank int - None world_size = world_size rank = rank device_type = get_device_type _MLP_model_setup model_type type parallelize_plan Union None dict = None - tuple nn Module torch Tensor Creates MLP MLPStacked model examples parallelize_plan None parallelize_plan = net ColwiseParallel net RowwiseParallel device_mesh = DeviceMesh device_type torch arange NUM_DEVICES inp_size = inp = torch rand inp_size device=self device_type model = model_type device_type model = parallelize_module model device_mesh parallelize_plan model inp _transformer_model_setup is_seq_parallel bool = False - tuple nn Module torch Tensor Creates transformer model examples device_mesh = DeviceMesh device_type torch arange NUM_DEVICES model_args = ModelArgs model = Transformer model_args device=self device_type model = Transformer parallelize model device_mesh is_seq_parallel inp_size = inp = torch randint model_args vocab_size inp_size device=self device_type model inp example_MLP_distributed_sharding_display - None Example obtaining all module s FQN parameters given distributed model printing sharding info Expected output MLPModule net weight Shard dim= MLPModule net bias Shard dim= MLPModule net weight Shard dim= MLPModule net bias Replicate torch manual_seed model inp = _MLP_model_setup model_type=MLPModule comm_mode = CommDebugMode comm_mode output_tp = model inp output_tp sum backward print comm_mode get_sharding_info example_MLPStacked_distributed_sharding_display - None Example obtaining all module s FQN parameters given distributed model nested modules printing sharding info Expected output MLPStacked layers net weight Shard dim= MLPStacked layers net bias Shard dim= MLPStacked layers net weight Shard dim= MLPStacked layers net bias Replicate MLPStacked layers net weight Shard dim= MLPStacked layers net bias Shard dim= MLPStacked layers net weight Shard dim= MLPStacked layers net bias Replicate torch manual_seed parallelize_plan = MLPStacked layers net ColwiseParallel MLPStacked layers net RowwiseParallel MLPStacked layers net ColwiseParallel MLPStacked layers net RowwiseParallel model inp = _MLP_model_setup model_type=MLPStacked parallelize_plan=parallelize_plan comm_mode = CommDebugMode comm_mode output_tp = model inp output_tp sum backward print comm_mode get_sharding_info example_MLP_module_tracing - None Example code demonstrate CommModeDebug s module level tracing using MLP model Prints table module level collective tracing information logs table comm_mode_log txt Expected Output Global FORWARD PASS c d_functional all_reduce MLPModule FORWARD PASS c d_functional all_reduce MLPModule net MLPModule relu MLPModule net FORWARD PASS c d_functional all_reduce torch manual_seed model inp = _MLP_model_setup model_type=MLPModule comm_mode = CommDebugMode comm_mode output_tp = model inp output_tp sum backward print module level collective tracing information print comm_mode generate_comm_debug_tracing_table noise_level= comm_mode log_comm_debug_tracing_table_to_file noise_level= example_transformer_module_tracing - None Example code demonstrate CommModeDebug s module level tracing using distributed Transformer model Prints table module level collective tracing information logs table comm_mode_log txt Expected output Global FORWARD PASS c d_functional all_reduce c d_functional all_gather_into_tensor Transformer FORWARD PASS c d_functional all_reduce c d_functional all_gather_into_tensor Transformer tok_embeddings FORWARD PASS c d_functional all_reduce Transformer pos_embeddings FORWARD PASS c d_functional all_reduce Transformer dropout Transformer layers FORWARD PASS c d_functional all_reduce Transformer layers attention_norm Transformer layers attention FORWARD PASS c d_functional all_reduce Transformer layers attention wq Transformer layers attention wk Transformer layers attention wv Transformer layers attention wo FORWARD PASS c d_functional all_reduce Transformer layers attention resid_dropout Transformer layers ffn_norm Transformer layers feed_forward FORWARD PASS c d_functional all_reduce Transformer layers feed_forward w Transformer layers feed_forward gelu Transformer layers feed_forward w FORWARD PASS c d_functional all_reduce Transformer layers feed_forward resid_dropout Transformer layers FORWARD PASS c d_functional all_reduce Transformer layers attention_norm Transformer layers attention FORWARD PASS c d_functional all_reduce Transformer layers attention wq Transformer layers attention wk Transformer layers attention wv Transformer layers attention wo FORWARD PASS c d_functional all_reduce Transformer layers attention resid_dropout Transformer layers ffn_norm Transformer layers feed_forward FORWARD PASS c d_functional all_reduce Transformer layers feed_forward w Transformer layers feed_forward gelu Transformer layers feed_forward w FORWARD PASS c d_functional all_reduce Transformer layers feed_forward resid_dropout Transformer norm Transformer output FORWARD PASS c d_functional all_gather_into_tensor torch manual_seed model inp = _transformer_model_setup comm_mode = CommDebugMode comm_mode model inp print module level collective tracing information print comm_mode generate_comm_debug_tracing_table noise_level= comm_mode log_comm_debug_tracing_table_to_file noise_level= example_MLP_operation_tracing - None Example code demonstrate CommModeDebug s module operation level tracing using distributed MLP model Prints table module opoeration level collective tracing information logs table comm_mode_log txt Expected output Global FORWARD PASS c d_functional all_reduce aten view default aten sum default aten ones_like default BACKWARD PASS aten expand default MLPModule module type torch testing _internal distributed _tensor common_dtensor MLPModule FORWARD PASS c d_functional all_reduce aten view default aten view default aten view default MLPModule net module type torch nn modules linear Linear Parameter List weight Shard dim= bias Shard dim= FORWARD PASS aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default aten view default aten t default shape torch Size sharding Shard dim= device mesh DeviceMesh aten t default aten addmm default shape torch Size torch Size torch Size sharding Shard dim= Replicate Shard dim= device mesh DeviceMesh aten addmm default aten view default BACKWARD PASS aten t default shape torch Size sharding Shard dim= device mesh DeviceMesh aten t default aten mm default shape torch Size torch Size sharding Shard dim= Replicate device mesh DeviceMesh aten mm default aten t default shape torch Size sharding Shard dim= device mesh DeviceMesh aten t default aten sum dim_IntList shape torch Size sharding Shard dim= device mesh DeviceMesh aten sum dim_IntList aten view default shape torch Size sharding Shard dim= device mesh DeviceMesh aten view default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default aten t default shape torch Size sharding Shard dim= device mesh DeviceMesh aten t default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default MLPModule relu module type torch nn modules activation ReLU FORWARD PASS aten view default aten relu default aten detach default BACKWARD PASS aten detach default aten threshold_backward default MLPModule net module type torch nn modules linear Linear Parameter List weight Shard dim= bias Replicate FORWARD PASS c d_functional all_reduce aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default aten detach default shape torch Size sharding Replicate device mesh DeviceMesh aten detach default aten detach default aten detach default shape torch Size sharding Replicate device mesh DeviceMesh aten detach default aten detach default aten detach default shape torch Size sharding Replicate device mesh DeviceMesh aten detach default aten detach default aten detach default shape torch Size sharding Replicate device mesh DeviceMesh aten detach default aten detach default aten view default aten view default shape torch Size sharding Shard dim= device mesh DeviceMesh aten view default aten t default shape torch Size sharding Shard dim= device mesh DeviceMesh aten t default aten addmm default shape torch Size torch Size torch Size sharding Replicate Shard dim= Shard dim= device mesh DeviceMesh aten div Tensor aten addmm default _c d_functional all_reduce default aten view default BACKWARD PASS aten t default shape torch Size sharding Shard dim= device mesh DeviceMesh aten t default aten mm default shape torch Size torch Size sharding Replicate Shard dim= device mesh DeviceMesh aten mm default aten t default shape torch Size sharding Replicate device mesh DeviceMesh aten t default aten mm default shape torch Size torch Size sharding Replicate Shard dim= device mesh DeviceMesh aten mm default aten t default shape torch Size sharding Shard dim= device mesh DeviceMesh aten t default aten sum dim_IntList shape torch Size sharding Replicate device mesh DeviceMesh aten sum dim_IntList aten view default shape torch Size sharding Replicate device mesh DeviceMesh aten view default aten detach default shape torch Size sharding Replicate device mesh DeviceMesh aten detach default aten detach default shape torch Size sharding Replicate device mesh DeviceMesh aten detach default aten detach default aten t default shape torch Size sharding Shard dim= device mesh DeviceMesh aten t default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default shape torch Size sharding Shard dim= device mesh DeviceMesh aten detach default aten detach default torch manual_seed model inp = _MLP_model_setup model_type=MLPModule comm_mode = CommDebugMode comm_mode output_tp = model inp output_tp sum backward print operation level collective tracing information print comm_mode generate_comm_debug_tracing_table noise_level= comm_mode log_comm_debug_tracing_table_to_file noise_level= example_transformer_operation_tracing is_seq_parallel bool = False - None Example code demonstrate CommModeDebug s module operation level tracing using distributed transformer model Prints table module opoeration level collective tracing information excluding trivial operations logs table transformer_operation_log txt torch manual_seed model inp = _transformer_model_setup comm_mode = CommDebugMode comm_mode model inp print operation level collective tracing information print comm_mode generate_comm_debug_tracing_table noise_level= comm_mode log_comm_debug_tracing_table_to_file noise_level= file_name= transformer_operation_log txt example_MLP_json_dump - None Example code demonstrate CommModeDebug s json dump using MLP model Sends information default comm_mode_log json file torch manual_seed model inp = _MLP_model_setup model_type=MLPModule comm_mode = CommDebugMode comm_mode output_tp = model inp output_tp sum backward comm_mode generate_json_dump example_transformer_json_dump is_seq_parallel bool = False - None Example code demonstrate CommModeDebug s json dump using transformer model excluding trivial operations Sends information user-passed transformer_log json file torch manual_seed model inp = _transformer_model_setup comm_mode = CommDebugMode comm_mode model inp comm_mode generate_json_dump file_name= transformer_log json noise_level= comm_mode generate_json_dump file_name= transformer_log_ json noise_level= example_activation_checkpointing - None Example code showing CommDebugMode able differentiate between backward passes activation checkpointing Sends information default comm_mode_log json file The output example output shown below Global FORWARD PASS aten sum default aten ones_like default BACKWARD PASS aten expand default Foo module type __main__ CommDebugModeExample example_activation_checkpointing locals Foo FORWARD PASS aten relu default aten empty memory_format aten empty memory_format aten relu default BACKWARD PASS aten threshold_backward default Foo linears module type torch nn modules linear Linear FORWARD PASS aten addmm default BACKWARD PASS aten mm default aten sum dim_IntList Foo linears module type torch nn modules linear Linear FORWARD PASS aten addmm default ACTIVATION CHECKPOINTING aten mm default aten mm default aten sum dim_IntList aten threshold_backward default Foo torch nn Module __init__ n_layers int dim int use_ac bool = False super __init__ linears = torch nn ModuleList use_ac = use_ac _ range n_layers linears append torch nn Linear dim dim forward x torch Tensor - torch Tensor i block enumerate linears i = use_ac x = checkpoint block x preserve_rng_state=True use_reentrant=False x = block x assert x None x = torch nn functional relu x x bsz = dim = n_layers = model = Foo n_layers dim True x = torch randn bsz dim comm_mode = CommDebugMode comm_mode model x sum backward print comm_mode generate_comm_debug_tracing_table noise_level= comm_mode log_comm_debug_tracing_table_to_file noise_level= comm_mode generate_json_dump noise_level= run_example world_size int rank int example_name str - None set manual seed initializing all functions instantiated_example = CommDebugModeExample world_size rank dict stores example code function names name_to_example_code dict str Callable None = MLP_distributed_sharding_display instantiated_example example_MLP_distributed_sharding_display MLPStacked_distributed_sharding_display instantiated_example example_MLPStacked_distributed_sharding_display MLP_module_tracing instantiated_example example_MLP_module_tracing transformer_module_tracing instantiated_example example_transformer_module_tracing MLP_operation_tracing instantiated_example example_MLP_operation_tracing transformer_operation_tracing instantiated_example example_transformer_operation_tracing MLP_json_dump instantiated_example example_MLP_json_dump transformer_json_dump instantiated_example example_transformer_json_dump activation_checkpointing instantiated_example example_activation_checkpointing name_to_example_code example_name __name__ == __main__ script launched via torchrun which automatically manages ProcessGroup rank = int os environ RANK world_size = int os environ WORLD_SIZE assert world_size == our example uses worker ranks parser = argparse ArgumentParser description= comm_mode_feature examples formatter_class=argparse RawTextHelpFormatter example_prompt = choose one comm_mode_feature example below \n \t MLP_distributed_sharding_display\n \t MLPStacked_distributed_sharding_display\n \t MLP_module_tracing\n \t transformer_module_tracing\n \t MLP_operation_tracing\n \t transformer_operation_tracing\n \t MLP_json_dump\n \t transformer_json_dump\n \t activation_checkpointing\n e g you want try MLPModule sharding display example please input MLP_distributed_sharding_display \n parser add_argument -e -- example help=example_prompt required=True example = parser parse_args example run_example world_size rank example