This module provides debugging backends TorchDynamo help diagnose troubleshoot compilation execution issues It includes Key Debugging Backends - eager Simple pass-through backend runs models eager mode - eager_noexcept Similar eager additional exception handling - eager_debug Adds schema validation checks custom operators - aot_eager Uses AOT Autograd nop compiler debugging - aot_eager_decomp_partition Uses TorchInductor decompositions debugging - torchscript Compiles using TorchScript debugging JIT-related issues Testing Development Tools - Backends inducing specific errors compile runtime accuracy - ExplainOutput detailed graph compilation analysis - Utilities cross-referencing mode management - Tools graph detail inspection break reason analysis These backends primarily used Debugging graph breaks compilation failures Testing error handling recovery mechanisms Analyzing performance bottlenecks Validating operator schemas decompositions dataclasses functools logging collections abc Callable Iterable importlib import_module typing Any Optional TYPE_CHECKING Union torch functorch compile min_cut_rematerialization_partition torch _guards torch _dynamo output_graph GraphCompileReason torch _functorch config functorch_config torch _functorch compilers ts_compile common aot_autograd registry CompiledFn CompilerFn register_debug_backend register_backend TYPE_CHECKING torch fx node Target log = logging getLogger __name__ register_backend eager gm torch fx GraphModule fake_tensor_inputs list torch Tensor kwargs Any - Callable Any kwargs log warning eager backend ignoring extra kwargs s kwargs gm forward make_eager_backend_with_torch_function_mode mode torch overrides TorchFunctionMode - Callable Any make_eager_backend_with_torch_function_modes mode make_eager_backend_with_torch_function_modes modes Iterable torch overrides TorchFunctionMode - Callable Any Used trace HOPs cond while eager execution metadata TF mode mutates vars outside scope HOP we can t have graph breaks HOP so we need externally run mode trace contextlib ExitStack fn gm torch fx GraphModule fake_tensor_inputs list torch Tensor kwargs Any - Callable Any wrapper args Any kwargs Any - Any ExitStack stack mode modes stack enter_context mode gm forward args kwargs wrapper fn register_backend eager_noexcept gm torch fx GraphModule fake_tensor_inputs list torch Tensor kwargs Any - Callable Any kwargs log warning eager_noexcept backend ignoring extra kwargs s kwargs This backend intended check dynamo-generated GraphModules do cause errors inner args Any - Any try gm args except Exception e raise torch _dynamo exc TorchDynamoException Unexpected exception when running generated GraphModule e inner register_backend pre_dispatch_eager gm torch fx GraphModule fake_tensor_inputs list torch Tensor kwargs Any - torch fx GraphModule kwargs log warning pre_dispatch_eager backend ignoring extra kwargs s kwargs torch fx experimental proxy_tensor make_fx runnable_gm args Any - Any torch fx Interpreter gm run args pre_dispatch_gm = make_fx runnable_gm pre_dispatch=True fake_tensor_inputs pre_dispatch_gm print_readable pre_dispatch_gm register_backend eager_debug gm torch fx GraphModule fake_tensor_inputs list torch Tensor kwargs Any - Callable Any kwargs log warning eager_debug backend ignoring extra kwargs s kwargs torch _subclasses schema_check_mode SchemaCheckMode We could add more debugging bits here Right now backend can used check error custom dispatcher ops have incorrect schemas inner args Any - Any SchemaCheckMode torch fx Interpreter gm run args inner register_backend name= ts type ignore misc torchscript gm torch fx GraphModule fake_tensor_inputs list torch Tensor - torch jit ScriptModule torch jit script gm used boxed call discard inputs when they no longer needed boxed_nop fx_g torch fx GraphModule example_inputs list torch Tensor - Callable Any torch fx graph _BoxedCodeGen Set graph use boxed codegen fx_g graph set_codegen _BoxedCodeGen fx_g recompile Wrap forward method function so we can set _boxed_call attribute forward_fn = fx_g forward run args Any - Any forward_fn args run _boxed_call = True type ignore attr-defined run boxed_nop_with_mode fx_g torch fx GraphModule example_inputs list torch Tensor mode torch overrides TorchFunctionMode - Callable Any torch fx graph _BoxedCodeGen Set graph use boxed codegen fx_g graph set_codegen _BoxedCodeGen fx_g recompile Create wrapper runs mode forward_fn = fx_g forward run args Any - Any mode forward_fn args run _boxed_call = True type ignore attr-defined run fake_crossref_boxed_nop fx_g torch fx GraphModule example_inputs list torch Tensor ignore_op_fn Optional Callable torch _ops OpOverload bool = None - Callable Any torch fx graph _BoxedCodeGen Set graph use boxed codegen fx_g graph set_codegen _BoxedCodeGen fx_g recompile Create wrapper runs mode forward_fn = fx_g forward run args Any - Any torch _subclasses CrossRefFakeMode ignore_op_fn forward_fn args run _boxed_call = True type ignore attr-defined run ignore_builtins op torch _ops OpOverload - bool op namespace aten prims prim get_nop_func - Callable torch fx GraphModule list torch Tensor Callable Any torch _functorch config fake_tensor_crossref boxed_nop torch _functorch config fake_tensor_crossref == all fake_crossref_boxed_nop assert torch _functorch config fake_tensor_crossref == custom_ops functools partial fake_crossref_boxed_nop ignore_op_fn=ignore_builtins Useful debugging purpose aot_eager uses AOT Autograd backend nop compiler It helpful debugging aot_eager gm torch fx GraphModule fake_tensor_inputs list torch Tensor fw_compiler Optional Callable Any = None bw_compiler Optional Callable Any = None kwargs Any - Callable Any aot_autograd fw_compiler=fw_compiler boxed_nop bw_compiler=bw_compiler boxed_nop partition_fn=min_cut_rematerialization_partition keep_inference_input_mutations=True gm fake_tensor_inputs kwargs register_backend name= aot_eager compiler_fn=aot_eager aot_eager_default_partitioner = aot_autograd fw_compiler=boxed_nop keep_inference_input_mutations=True register_backend name= aot_eager_default_partitioner compiler_fn=aot_eager_default_partitioner Uses TorchInductor AOT Autograd decomps partitioner isolate aot vs inductor problems aot_eager_decomp_partition just replaces inductor compiler nop help isolate inductor vs aot_eager errors aot_eager_decomp_partition gm torch fx GraphModule fake_tensor_inputs list torch Tensor kwargs Any - Callable Any kwargs log warning aot_eager_decomp_partition backend ignoring extra kwargs s kwargs torch _inductor compiler_bisector CompilerBisector config_patches = unlift_effect_tokens True bisect_changes = CompilerBisector get_config_change aot_eager_decomp_partition config_patches update bisect_changes type ignore arg-type functorch_config patch config_patches aot_autograd these taken memory_efficient_fusion fw_compiler=get_nop_func bw_compiler=get_nop_func NB lambda here delay inductor decompositions=lambda import_module torch _inductor compile_fx select_decomp_table partition_fn=functools partial min_cut_rematerialization_partition compiler= inductor gm fake_tensor_inputs register_backend name= aot_eager_decomp_partition compiler_fn=aot_eager_decomp_partition aot_eager_decomp_partition_with_mode similar aot_eager_decomp_partition except takes TorchDispatchMode mode run fw bw mode aot_eager_decomp_partition_with_mode gm torch fx GraphModule fake_tensor_inputs list torch Tensor mode Any kwarg Any - Callable Any aot_autograd these taken memory_efficient_fusion fw_compiler=functools partial boxed_nop_with_mode mode=mode bw_compiler=functools partial boxed_nop_with_mode mode=mode NB lambda here delay inductor decompositions=lambda import_module torch _inductor compile_fx select_decomp_table partition_fn=functools partial min_cut_rematerialization_partition compiler= inductor gm fake_tensor_inputs register_backend name= aot_eager_decomp_partition_with_mode compiler_fn=aot_eager_decomp_partition_with_mode type ignore arg-type aot_eager_decomp_partition_crossref gm torch fx GraphModule fake_tensor_inputs list torch Tensor kwargs Any - Callable Any config set respect otherwise only test custom_ops custom_op bad metas always manifest error whereas aten will only sometimes default use less noisy option config_val = custom_ops functorch_config fake_tensor_crossref functorch_config fake_tensor_crossref functorch_config patch fake_tensor_crossref=config_val aot_eager_decomp_partition gm fake_tensor_inputs kwargs register_backend name= aot_eager_decomp_partition_crossref compiler_fn=aot_eager_decomp_partition_crossref AOT Autograd torchscript backend Default partitioner aot_ts uses torchscript backend We can use both nnc nvfuser using relevant fuser torch jit fuser aot_ts = aot_autograd fw_compiler=ts_compile register_backend name= aot_ts compiler_fn=aot_ts These buggy backends used inducing bugs so we can test our repro extraction minifier scripts ReluCompileError Exception pass TestingOnlyCompileError Exception pass register_backend relu_compile_error_TESTING_ONLY gm torch fx GraphModule example_inputs list torch Tensor - torch fx GraphModule node gm graph nodes node target torch relu raise ReluCompileError gm register_backend relu_runtime_error_TESTING_ONLY gm torch fx GraphModule example_inputs list torch Tensor - torch fx GraphModule node gm graph nodes node target torch relu node target = torch _assert node args = False ReluRuntimeError gm recompile gm register_backend relu_accuracy_error_TESTING_ONLY gm torch fx GraphModule example_inputs list torch Tensor - torch fx GraphModule node gm graph nodes node target torch relu node target = torch add node args = node args gm recompile gm register_backend non_leaf_compile_error_TESTING_ONLY gm torch fx GraphModule example_inputs list torch Tensor - torch fx GraphModule Require least one non-trivial thing graph see https github com pytorch pytorch issues node gm graph nodes node op == call_function break gm t example_inputs t is_leaf raise TestingOnlyCompileError gm dataclasses dataclass ExplainOutput This output func ` torch _dynamo explain ` There no reason create directly graphs list torch fx GraphModule graph_count int graph_break_count int break_reasons list GraphCompileReason op_count int ops_per_graph Optional list list Target = None out_guards Optional list _guards Guard = None compile_times Optional str = None __str__ - str output = f Graph Count graph_count \n output += f Graph Break Count graph_break_count \n output += f Op Count op_count \n output += Break Reasons \n idx break_reason enumerate break_reasons output += f Break Reason idx + \n output += f Reason break_reason reason \n output += User Stack \n frame_summary break_reason user_stack output += f frame_summary \n ops_per_graph None output += Ops per Graph \n idx ops enumerate ops_per_graph output += f Ops idx + \n op ops output += f op \n out_guards None output += Out Guards \n i guard enumerate out_guards output += f Guard i + \n output += f str guard compile_times None output += f Compile Times compile_times \n output _explain_graph_detail gm torch fx GraphModule graphs list torch fx GraphModule op_count int ops_per_graph list list Target break_reasons list GraphCompileReason - tuple torch fx GraphModule list torch fx GraphModule int list list Target list GraphCompileReason This function utility which processes torch fx GraphModule accumulates information about its ops graph breaks other details It intended used ExplainWithBackend ` torch _dynamo explain ` provide details Dynamo s graph capture Parameters gm torch fx GraphModule The GraphModule processed graphs list A list accumulates all GraphModules processed op_count int The total count operations all GraphModules processed so far ops_per_graph list A list accumulates operations each GraphModule break_reasons list A list accumulates reasons breaks each GraphModule Returns tuple A tuple containing processed GraphModule updated lists graphs operations per graph break reasons updated operation count graphs append gm ops = node target node gm graph nodes node op == call_function op_count += len ops ops_per_graph append ops gm compile_subgraph_reason graph_break type ignore union-attr break_reasons append gm compile_subgraph_reason type ignore arg-type gm graphs op_count ops_per_graph break_reasons ExplainWithBackend This intended used backend ` torch compile ` It composable other backends When used way accumulates information about graph breaks ops other info provides string representation summarizing information Attributes backend str The name backend use optimization graphs list A list graphs captured TorchDynamo op_count int The total number operations all optimized graphs break_reasons list A list graph break reasons stack traces Example Usage fn x x = torch sigmoid x x torch _dynamo reset eb = ExplainWithBackend inductor optimized_fn = torch compile fn backend=eb result = optimized_fn torch randn print eb output __init__ backend Union CompilerFn str - None registry lookup_backend backend = lookup_backend backend graphs list torch fx GraphModule = op_count = break_reasons list GraphCompileReason = __call__ gm torch fx GraphModule example_inputs list torch Tensor - CompiledFn ops_per_graph list list Target = gm graphs op_count _ break_reasons = _explain_graph_detail gm graphs op_count ops_per_graph break_reasons backend gm example_inputs output - ExplainOutput graph_count = len graphs output = ExplainOutput graphs graph_count graph_count - break_reasons op_count output