mypy allow-untyped-defs builtins collections contextlib copy functools inspect logging math os warnings collections abc Callable itertools chain types CodeType FunctionType ModuleType typing Any get_args NamedTuple Optional TypeAlias Union torch torch utils _pytree pytree torch _C ScriptObject type ignore attr-defined torch _library fake_class_registry FakeScriptObject _compatibility compatibility _lazy_graph_module _make_graph_module graph _PyTreeCodeGen _PyTreeInfo Graph graph_module GraphModule node Argument base_types map_aggregate proxy ParameterProxy Proxy Scope ScopeContextManager TracerBase log = logging getLogger __name__ HAS_VARSTUFF = inspect CO_VARARGS &#124; inspect CO_VARKEYWORDS These need run global scope handle nested calls correctly _orig_module_call Callable = torch nn Module __call__ _orig_module_getattr Callable = torch nn Module __getattr__ _proxyable_classes dict type None = _is_fx_tracing_flag = False _ConstantAttributeType TypeAlias = Union torch Tensor torch ScriptObject FakeScriptObject pytree TreeSpec _constant_attribute_types = get_args _ConstantAttributeType We only want print once avoid flooding logs functools lru_cache is_fx_tracing_warning log warning is_fx_tracing will true both fx symbolic_trace torch export Please use is_fx_tracing_symbolic_tracing specifically fx symbolic_trace torch compiler is_compiling specifically torch export compile is_fx_tracing is_fx_tracing_warning _is_fx_tracing_flag is_fx_symbolic_tracing _is_fx_tracing_flag torch compiler is_compiling compatibility is_backward_compatible=True ProxyableClassMeta type ProxyableClassMeta allows you make construction given Python symbolically traceable For example torch torch fx TensorPair metaclass=torch fx ProxyableClassMeta __init__ left right left right = left right add other l = left + other left r = right + other right TensorPair l r mul other l = left other left r = right other right TensorPair l r use_tensor_pair_ctor x TensorPair y torch Tensor s = x add TensorPair y y s mul x x = TensorPair torch randn torch randn y = torch randn ref_out = use_tensor_pair_ctor x y traced = torch fx symbolic_trace use_tensor_pair_ctor print traced code forward x __main___TensorPair y torch Tensor tensor_pair = __main___TensorPair y y y = None add = x add tensor_pair tensor_pair = None mul = add mul x add = x = None mul From example we can see construction ` ` TensorPair ` ` defined ` ` ProxyableClassMeta ` ` metaclass can recorded symbolic tracing __init__ cls name bases attrs _proxyable_classes setdefault cls super __init__ name bases attrs __call__ cls args kwargs instance = cls __new__ cls type ignore call-overload is_fx_tracing cls __init__ instance args kwargs type ignore misc instance found_proxies = check_proxy isinstance Proxy found_proxies append map_aggregate args check_proxy map_aggregate kwargs check_proxy len found_proxies = tracer = found_proxies tracer tracer create_proxy call_function cls args kwargs cls __init__ instance args kwargs type ignore misc instance _patch_function fn FunctionType nargs int - FunctionType co = fn __code__ co_flags = co co_flags ~HAS_VARSTUFF co_args tuple hasattr co co_qualname Python- + code signature co_args = nargs co co_nlocals co co_stacksize co_flags co co_code co co_consts co co_names co co_varnames co co_filename co co_name co co_qualname type ignore attr-defined co co_firstlineno co co_linetable co co_exceptiontable type ignore attr-defined co co_freevars co co_cellvars hasattr co co_posonlyargcount co_args = nargs co co_nlocals co co_stacksize co_flags co co_code co co_consts co co_names co co_varnames co co_filename co co_name co co_firstlineno co co_lnotab co co_freevars co co_cellvars co_args = nargs co co_nlocals co co_stacksize co_flags co co_code co co_consts co co_names co co_varnames co co_filename co co_name co co_firstlineno co co_lnotab co co_freevars co co_cellvars new_code = CodeType co_args type ignore arg-type FunctionType new_code fn __globals__ fn __name__ fn __defaults__ fn __closure__ we need insert placeholder nodes args kwargs we can t call function normally otherwise would try unpack them instead let s make python think args kwargs normal variables compatibility is_backward_compatible=False PHBase Object representing input placeholder ` concrete_args ` __repr__ PH PH = PHBase compatibility is_backward_compatible=False PHWithMeta PHBase Object representing input placeholder ` concrete_args ` __init__ ph_key Optional str = None super __init__ Provide hey user identify placeholder node during analysis ph_key = ph_key _transfer_attrs fr attr_name dir fr attr_val = getattr fr attr_name callable attr_val attr_name startswith __ hasattr attr_name setattr attr_name attr_val compatibility is_backward_compatible=True Tracer TracerBase Reference https github com pytorch pytorch issues The first line docstring overrides one Sphinx generates documentation We need so Sphinx doesn t leak ` math ` s path build environment e g ` module math leaked path Tracer autowrap_modules= math autowrap_functions= ` ` Tracer ` ` implements symbolic tracing functionality ` ` torch fx symbolic_trace ` ` A call ` ` symbolic_trace m ` ` equivalent ` ` Tracer trace m ` ` Tracer can subclassed override various behaviors tracing process The different behaviors can overridden described docstrings methods Not checking BC API because default value ` autowrap_modules ` includes local filepath ` math ` module which would jitter across machines compatibility is_backward_compatible=True __init__ autowrap_modules tuple ModuleType = math autowrap_functions tuple Callable = param_shapes_constant bool = False - None This method s signature overridden first line docstring If method s signature modified signature overrides also should modified accordingly Construct Tracer object Args autowrap_modules Tuple ModuleType defaults ` math ` Python modules whose functions should wrapped automatically without needing use fx wrap Backward-compatibility parameter guaranteed autowrap_functions Tuple Callable defaults ` ` Python functions should wrapped automatically without needing use fx wrap Backward compatibility parameter guaranteed param_shapes_constant bool When flag set calls shape size few other shape like attributes module s parameter will evaluated directly rather than returning new Proxy value attribute access Backward compatibility parameter guaranteed super __init__ Functions we will eagerly wrap when we see them while tracing captures both ` math sqrt ` ` math sqrt ` automatically _autowrap_function_ids set int = id value name value chain from_iterable m __dict__ items m autowrap_modules name startswith _ callable value _autowrap_function_ids update id f f autowrap_functions Python modules apply autowrap start addition modules we see while tracing _autowrap_search list ModuleType = list autowrap_modules param_shapes_constant = param_shapes_constant submodule_paths Optional dict torch nn Module str = None root_module_name str = Maps containing module s name operator name scope = Scope None Records module call stack module_stack = collections OrderedDict num_calls dict str int = Mapping node name module scope node_name_to_scope dict str tuple str type = _qualname_counter dict str int = collections defaultdict int compatibility is_backward_compatible=True get_fresh_qualname prefix str - str Gets fresh name prefix returns This function ensures will clash existing attribute graph The idea here module doesn t have prefix all we should reset counter start beginning It s little bit hacky doesn t cover all cases precise naming prefixes isn t correctness issue just niceness issue qualname = f prefix hasattr root qualname _qualname_counter prefix = qualname i = _qualname_counter prefix while True qualname = f prefix i i += hasattr root qualname break _qualname_counter prefix = i qualname compatibility is_backward_compatible=True create_arg Any - Argument A method specify behavior tracing when preparing values used arguments nodes ` ` Graph ` ` By default behavior includes Iterate through collection types e g tuple list dict recursively call ` ` create_args ` ` elements Given Proxy object reference underlying IR ` ` Node ` ` Given non-Proxy Tensor object emit IR various cases For Parameter emit ` ` get_attr ` ` node referring Parameter For non-Parameter Tensor store Tensor away special attribute referring attribute This method can overridden support more types Args Any The value emitted ` ` Argument ` ` ` ` Graph ` ` Returns The value ` ` ` ` converted into appropriate ` ` Argument ` ` The base tracer used construct Graphs when there no associated module hierarchy so can never create parameter references The default tracer adds ability refer parameters when tracing modules isinstance torch nn Parameter n p root named_parameters p create_node get_attr n raise NameError parameter member module isinstance torch Tensor n_ p_ root named_buffers p_ create_node get_attr n_ isinstance torch nn Module n_ p_ root named_modules p_ create_node get_attr n_ For NamedTuple instances appear literally args we emit node construct NamedTuple use Node argument isinstance tuple hasattr _fields args = tuple create_arg elem elem create_node call_function __class__ args Tensors do have reliable string repr which they can constructed we probably don t want rely either so any constant Tensor values we encounter first search they attribute some module module hierarchy If so emit get_attr retrieve tensor Otherwise we ll store away tensor value into special attribute Module s t we can retrieve get_attr isinstance _constant_attribute_types qualname Optional str = tensor_attrs get Tensor found Module hierarchy stow away special attribute set qualname refer qualname isinstance torch Tensor base_name = _tensor_constant isinstance FakeScriptObject ScriptObject base_name = _torchbind_obj isinstance pytree TreeSpec base_name = _tree_spec_constant raise RuntimeError f cannot create constant arg type type qualname = get_fresh_qualname base_name assert isinstance qualname str tensor_attrs = qualname setattr root qualname create_node get_attr qualname type _proxyable_classes This instance proxyable which we did witness its construction Intern constant attribute TODO binary search qualname = get_fresh_qualname f _ __class__ __name__ _constant_ assert isinstance qualname str setattr root qualname create_node get_attr qualname super create_arg compatibility is_backward_compatible=True is_leaf_module m torch nn Module module_qualified_name str - bool A method specify whether given ` ` nn Module ` ` leaf module Leaf modules atomic units appear IR referenced ` ` call_module ` ` calls By default Modules PyTorch standard library namespace torch nn leaf modules All other modules traced through their constituent ops recorded unless specified otherwise via parameter Args m Module The module being queried about module_qualified_name str The path root module For example you have module hierarchy where submodule ` ` foo ` ` contains submodule ` ` bar ` ` which contains submodule ` ` baz ` ` module will appear qualified name ` ` foo bar baz ` ` here m __module__ startswith torch nn m __module__ startswith torch ao nn isinstance m torch nn Sequential compatibility is_backward_compatible=True path_of_module mod torch nn Module - str Helper method find qualified name ` ` mod ` ` Module hierarchy ` ` root ` ` For example ` ` root ` ` has submodule named ` ` foo ` ` which has submodule named ` ` bar ` ` passing ` ` bar ` ` into function will string foo bar Args mod str The ` ` Module ` ` retrieve qualified name Prefer O algorithm submodule_paths path = submodule_paths get mod path None raise NameError module installed submodule assert isinstance path str path O N^ fallback case we didn t store submodule paths n p root named_modules mod p n raise NameError module installed submodule compatibility is_backward_compatible=True call_module m torch nn Module forward Callable Any args tuple Any kwargs dict str Any - Any Method specifies behavior ` ` Tracer ` ` when encounters call ` ` nn Module ` ` instance By default behavior check called module leaf module via ` ` is_leaf_module ` ` If emit ` ` call_module ` ` node referring ` ` m ` ` ` ` Graph ` ` Otherwise call ` ` Module ` ` normally tracing through operations its ` ` forward ` ` function This method can overridden -- example -- create nested traced GraphModules any other behavior you would want while tracing across ` ` Module ` ` boundaries Args m Module The module which call being emitted forward Callable The forward method ` ` Module ` ` invoked args Tuple args module callsite kwargs Dict kwargs module callsite Return The value Module call In case ` ` call_module ` ` node emitted ` ` Proxy ` ` value Otherwise whatever value returned ` ` Module ` ` invocation module_qualified_name = path_of_module m ScopeContextManager scope Scope module_qualified_name type m _scope module_stack ordered dict so writing then deleting entry equivalent push pop list num_calls = num_calls get module_qualified_name module_key = f _scope module_path num_calls num_calls _scope module_path module_stack module_key = module_qualified_name _scope module_type num_calls module_qualified_name = num_calls + is_leaf_module m module_qualified_name ret_val = forward args kwargs ret_val = create_proxy call_module module_qualified_name args kwargs key _ = module_stack popitem last=True assert key == module_key f Unexpected key key ret_val compatibility is_backward_compatible=False getattr attr str attr_val Any parameter_proxy_cache dict str Any Method specifies behavior ` ` Tracer ` ` when we call getattr call ` ` nn Module ` ` instance By default behavior proxy value attribute It also stores proxy value ` ` parameter_proxy_cache ` ` so future calls will reuse proxy rather than creating new one This method can overridden -- example -- proxies when querying parameters Args attr str The name attribute being queried attr_val Any The value attribute parameter_proxy_cache Dict str Any A cache attr names proxies Return The value getattr call maybe_get_proxy_for_attr attr_val collection_to_search parameter_proxy_cache n p collection_to_search attr_val p n parameter_proxy_cache kwargs = proxy_factory_fn inspect signature create_proxy parameters kwargs proxy_factory_fn = pyrefly ignore unsupported-operation None param_shapes_constant lambda node ParameterProxy node n attr_val val_proxy = create_proxy get_attr n kwargs type ignore arg-type parameter_proxy_cache n = val_proxy parameter_proxy_cache n None isinstance attr_val torch nn Parameter maybe_parameter_proxy = maybe_get_proxy_for_attr attr_val root named_parameters parameter_proxy_cache maybe_parameter_proxy None maybe_parameter_proxy proxy_buffer_attributes isinstance attr_val torch Tensor maybe_buffer_proxy = maybe_get_proxy_for_attr attr_val root named_buffers parameter_proxy_cache maybe_buffer_proxy None maybe_buffer_proxy attr_val This method will refactored compatibility is_backward_compatible=False create_args_for_root root_fn is_module concrete_args=None Create ` ` placeholder ` ` nodes corresponding signature ` ` root ` ` Module This method introspects root s signature emits those nodes accordingly also supporting ` ` args ` ` ` ` kwargs ` ` In some cases function method has been decorated wrapper defined via ` ` functools wraps ` ` In case outer code object will likely contain actual parameters we care about so unwrap function get innermost callable fn_for_analysis = inspect unwrap root_fn co = fn_for_analysis __code__ total_args = co co_argcount + co co_kwonlyargcount orig_args = list co co_varnames names_iter = iter co co_varnames args list Any = skip_arg_idx = is_module total_args == raise RuntimeError ` ` ` ` argument cannot part args expansion skip_arg_idx = next names_iter skip args append root sig = inspect signature fn_for_analysis This covers very specific case where we passing flat concrete_args tuple our traced fn takes args kwargs In case just take concrete_args pass them through name_idx = isinstance concrete_args tuple len concrete_args co co_flags HAS_VARSTUFF total_args == concrete_arg concrete_args out = create_proxy placeholder f input_ name_idx isinstance concrete_arg PHBase concrete_arg = PH Transfer attrs case where you re using placeholder other than singleton PH PH has no attributes transfer Proxies created out placeholders Transfer any metadata put placeholders form attributes set user placeholder underlying nodes proxy unwrapped user metadata should hold _transfer_attrs fr=concrete_arg to=out node args append out name_idx += root_fn args arg_names = next names_iter idx range skip_arg_idx total_args isinstance concrete_args tuple len arg_names = len concrete_args raise RuntimeError f Tracing expected len arg_names arguments got len concrete_args concrete arguments concrete_args = dict zip arg_names concrete_args proxy_placeholder name _proxy_placeholder name concrete_args sig fn_for_analysis args extend proxy_placeholder names names arg_names co co_kwonlyargcount co co_flags HAS_VARSTUFF TODO type annotations args kwargs co co_flags inspect CO_VARARGS args append proxy_placeholder + next names_iter co co_flags inspect CO_VARKEYWORDS args append proxy_placeholder + next names_iter root_fn = _patch_function root_fn len args flat_args in_spec = pytree tree_flatten tuple args all child is_leaf child in_spec children In case we have pytree-flattened inputs ` concrete_args ` generate flattening wrapper around original root function graph _codegen = _PyTreeCodeGen type ignore has-type _PyTreeInfo orig_args total_args in_spec None flatten_fn args tree_args = pytree tree_unflatten list args in_spec tree_out = root_fn tree_args out_args out_spec = pytree tree_flatten tree_out assert isinstance graph _codegen _PyTreeCodeGen type ignore has-type graph _codegen pytree_info = graph _codegen pytree_info _replace out_spec=out_spec out_args flatten_fn flat_args root_fn args compatibility is_backward_compatible=True trace root Union torch nn Module Callable Any concrete_args Optional dict str Any = None - Graph Trace ` ` root ` ` corresponding FX ` ` Graph ` ` representation ` ` root ` ` can either ` ` nn Module ` ` instance Python callable Note after call ` ` root ` ` may different ` ` root ` ` passed here For example when free function passed ` ` trace ` ` we will create ` ` nn Module ` ` instance use root add embedded constants Args root Union Module Callable Either ` ` Module ` ` function traced through Backwards-compatibility parameter guaranteed concrete_args Optional Dict str any Concrete arguments should treated Proxies This parameter experimental its backwards-compatibility NOT guaranteed Returns A ` ` Graph ` ` representing semantics passed-in ` ` root ` ` global _is_fx_tracing_flag old_is_fx_tracing_flag = _is_fx_tracing_flag _is_fx_tracing_flag = True try isinstance root torch nn Module do real recompilation _LazyGraphModule before retracing since trace method can trace _lazy_forward method Got error https gist github com shunting c e ae ac c d without torch fx _lazy_graph_module _LazyGraphModule _LazyGraphModule force_recompile root root = root assert hasattr type root traced_func_name f traced_func_name= traced_func_name doesn t exist type root __name__ fn = getattr type root traced_func_name root_module_name = root _get_name submodule_paths = mod name name mod root named_modules root = torch nn Module fn = root tracer_cls Optional type Tracer = getattr __class__ None graph = Graph tracer_cls=tracer_cls hasattr fn __code__ code = fn __code__ graph _co_fields = co_name code co_name co_filename code co_filename co_firstlineno code co_firstlineno When we encounter Tensor value s parameter we look some other attribute model Construct dict mapping Tensor values qualified name here efficiency This used downstream create_arg tensor_attrs dict _ConstantAttributeType str = collect_tensor_attrs m torch nn Module prefix_atoms list str k v m __dict__ items isinstance v _constant_attribute_types tensor_attrs v = join prefix_atoms + k k v m named_children collect_tensor_attrs v prefix_atoms + k collect_tensor_attrs root assert isinstance fn FunctionType fn_globals = fn __globals__ run before gets patched fn args = create_args_for_root fn isinstance root torch nn Module concrete_args parameter_proxy_cache dict str Proxy = Reduce number get_attr calls Method dispatch parameters recorded unless s directly used Thus we need insert proxy when __getattr__ requests parameter functools wraps _orig_module_getattr module_getattr_wrapper mod attr attr_val = _orig_module_getattr mod attr getattr attr attr_val parameter_proxy_cache functools wraps _orig_module_call module_call_wrapper mod args kwargs forward args kwargs _orig_module_call mod args kwargs _autowrap_check patcher type ignore has-type getattr getattr mod forward mod __globals__ _autowrap_function_ids call_module mod forward args kwargs _new_patcher patcher allow duplicate patches support case nested calls patcher patch_method torch nn Module __getattr__ module_getattr_wrapper deduplicate=False patcher patch_method torch nn Module __call__ module_call_wrapper deduplicate=False _patch_wrapped_functions patcher _autowrap_check patcher fn_globals _autowrap_function_ids module _autowrap_search _autowrap_check patcher module __dict__ _autowrap_function_ids create_node output output create_arg fn args type_expr=fn __annotations__ get None submodule_paths = None except RuntimeError e isinstance e args str data-dependent e args partial_fx_graph = graph python_code root_module= verbose=True src e partial_fx_graph = partial_fx_graph type ignore attr-defined raise raise finally _is_fx_tracing_flag = old_is_fx_tracing_flag graph __deepcopy__ memo _autowrap_search contains modules which cannot deepcopied new_tracer = Tracer __new__ Tracer k v __dict__ items k == _autowrap_search new_obj = copy copy v new_obj = copy deepcopy v memo new_tracer __dict__ k = new_obj new_tracer _proxy_placeholder name concrete_args sig fn_for_analysis concrete_args None name concrete_args cnt = replace_ph x nonlocal cnt cnt += param = sig parameters name default tuple Any = param default inspect Parameter empty param default out = create_proxy placeholder f name _ str cnt default isinstance x PHBase x = PH Transfer attrs case where you re using placeholder other than singleton PH PH has no attributes transfer Proxies created out placeholders Transfer any metadata put placeholders form attributes set user placeholder underlying nodes proxy unwrapped user metadata should hold _transfer_attrs fr=x to=out node out Union int bool == bool Python = type x bool type x base_types type x torch Tensor torch _assert out == x f name has been specialized have value x got another value x None args = out f name has been specialized have value None got another value create_proxy call_function _assert_is_none args warnings warn f Was able add assertion guarantee correct input name f specialized function It up user make sure your inputs match f inputs you specialized function x pytree tree_map replace_ph concrete_args name name == default tuple Any = param = sig parameters name default = type ignore assignment param default inspect Parameter empty param default create_proxy placeholder name default type_expr=fn_for_analysis __annotations__ get name None Dictionary id globals dict function name = globals_dict patch purposes wrap API We key globals dict id function name ensure we re wrapping given function only once _wrapped_fns_to_patch dict tuple int str dict = List methods classes wrap type function name currently only works Tensor methods aren t traced properly _wrapped_methods_to_patch list tuple type str = os environ get FX_PATCH_GETITEM == This change needed trace models like PositionalEmbedding BERT https github com pytorch benchmark blob master torchbenchmark models BERT_pytorch bert_pytorch model embedding position py causes issues quantization documented here https github com pytorch pytorch issues once fixed we can make default behavior _wrapped_methods_to_patch append torch Tensor __getitem__ _find_proxy objects_to_search Recursively search data structure Proxy None found proxy = None find_proxy x nonlocal proxy isinstance x Proxy proxy = x map_aggregate objects_to_search find_proxy proxy _create_wrapped_func orig_fn functools wraps orig_fn wrapped args kwargs Given closed-over ` ` orig_function ` ` invoke search args kwargs Proxy object If there one emit ` ` call_function ` ` node preserve call leaf function directly Otherwise just results function call function being traced proxy = _find_proxy args kwargs proxy None return_proxy = proxy tracer create_proxy call_function orig_fn args kwargs return_proxy node meta is_wrapped = True return_proxy orig_fn args kwargs wrapped _create_wrapped_method cls name orig_fn = getattr cls name functools wraps orig_fn wrapped args kwargs Search args kwargs Proxy object If there one emit ` ` call_method ` ` node preserve call method directly Otherwise just results function call function being traced proxy = _find_proxy args kwargs proxy None proxy tracer create_proxy call_method name args kwargs orig_fn args kwargs wrapped _PatchedFn NamedTuple frame_dict Any fn_name str orig_fn Any new_fn Any revert raise NotImplementedError patch raise NotImplementedError _PatchedFnSetItem _PatchedFn revert frame_dict fn_name = orig_fn patch frame_dict fn_name = new_fn _PatchedFnDel _PatchedFn revert del frame_dict fn_name patch frame_dict fn_name = new_fn _PatchedFnSetAttr _PatchedFn revert setattr frame_dict fn_name orig_fn patch setattr frame_dict fn_name new_fn _Patcher __init__ - None super __init__ patches_made list _PatchedFn = visited set int = set patch frame_dict dict str Any name str new_fn Callable deduplicate bool = True Replace frame_dict name new_fn until we exit context manager new_fn __fx_already_patched = deduplicate type ignore attr-defined name frame_dict hasattr builtins name patches_made append _PatchedFnDel frame_dict name None new_fn patches_made - patch getattr frame_dict name __fx_already_patched False already patched no need do again patches_made append _PatchedFnSetItem frame_dict name frame_dict name new_fn patches_made - patch patch_method cls type name str new_fn Callable deduplicate bool = True Replace object_or_dict name new_fn until we exit context manager new_fn __fx_already_patched = deduplicate type ignore attr-defined orig_fn = getattr cls name getattr orig_fn __fx_already_patched False already patched no need do again patches_made append _PatchedFnSetAttr cls name orig_fn new_fn patches_made - patch visit_once thing Any Return True first call thing otherwise false idx = id thing idx visited False visited add idx True revert_all_patches Remove all stored patcheds It doesn t modify patches_made patch patches_made patch revert patches_made reapply_all_patches Patch all stored patcheds It doesn t modify patches_made patch patches_made patch patch patches_made __enter__ __exit__ exc_type exc_val exc_tb Undo all changes made via patch patch_method while patches_made unpatch reverse order handle duplicates correctly patches_made pop revert visited clear CURRENT_PATCHER Optional _Patcher = None contextlib contextmanager _new_patcher global CURRENT_PATCHER prior_patcher = CURRENT_PATCHER try CURRENT_PATCHER = _Patcher yield CURRENT_PATCHER finally Clear all patches made when using current patcher assert CURRENT_PATCHER None CURRENT_PATCHER revert_all_patches CURRENT_PATCHER = prior_patcher contextlib contextmanager _maybe_revert_all_patches current_patcher = CURRENT_PATCHER patches_made = None patches_removed = None try current_patcher None patches_removed = current_patcher revert_all_patches yield finally current_patcher None patches_made = current_patcher reapply_all_patches assert patches_made == patches_removed CURRENT_PATCHER changed during revert_all_patches _patch_wrapped_functions patcher _Patcher Go through ` ` _wrapped_fn_patch_table ` ` each frame object wrap listed global functions ` _create_wrapped_func ` wrapper _ name frame_dict _wrapped_fns_to_patch copy items name frame_dict hasattr builtins name orig_fn = getattr builtins name orig_fn = frame_dict name patcher patch frame_dict name _create_wrapped_func orig_fn cls name _wrapped_methods_to_patch patcher patch_method cls name _create_wrapped_method cls name _autowrap_check patcher _Patcher frame_dict dict str Any function_ids set int Some methods like ` math sqrt ` common enough we want automatically wrap them we see them This method searches scope them patches them found patcher visit_once frame_dict name value frame_dict items name startswith _ callable value id value function_ids patcher patch frame_dict name _create_wrapped_func value compatibility is_backward_compatible=True wrap fn_or_name Union str Callable This function can called module-level scope register fn_or_name leaf function A leaf function will preserved CallFunction node FX trace instead being traced through foo bar baz py my_custom_function x y x x + y y torch fx wrap my_custom_function fn_to_be_traced x y When symbolic tracing below call my_custom_function will inserted into graph rather than tracing my_custom_function x y This function can also equivalently used decorator foo bar baz py torch fx wrap my_custom_function x y x x + y y A wrapped function can thought leaf function analogous concept leaf modules they functions left calls FX trace rather than traced through Args fn_or_name Union str Callable The function name global function insert into graph when s called callable fn_or_name isinstance fn_or_name str raise RuntimeError Unsupported type global function Must either callable string name callable fn_or_name assert isinstance fn_or_name str make mypy happy fn_name = fn_or_name __name__ assert isinstance fn_or_name str fn_or_name must global function string name fn_name = fn_or_name currentframe = inspect currentframe assert currentframe None f = currentframe f_back assert f None f f_code co_name = module raise NotImplementedError wrap must called top level module consider implementing Callable version via _autowrap_function_ids _autowrap_search semantics would slightly different would add support ` x wrapped_function ` _wrapped_fns_to_patch id f f_globals fn_name = f f_globals fn_or_name compatibility is_backward_compatible=True symbolic_trace root Union torch nn Module Callable Any concrete_args Optional dict str Any = None - GraphModule Symbolic tracing API Given ` ` nn Module ` ` function instance ` ` root ` ` function will ` ` GraphModule ` ` constructed recording operations seen while tracing through ` ` root ` ` ` ` concrete_args ` ` allows you partially specialize your function whether s remove control flow data structures For example f b b == True FX can typically trace through due presence control flow However we can use ` concrete_args ` specialize value ` b ` trace through f = fx symbolic_trace f concrete_args= b False assert f False == Note although you can still pass different values ` b ` they will ignored We can also use ` concrete_args ` eliminate data-structure handling our function This will use pytrees flatten your input To avoid overspecializing pass ` fx PH ` values shouldn t specialized For example f x out = v x values out += v out f = fx symbolic_trace f concrete_args= x fx PH b fx PH c fx PH assert f b c == Args root Union torch nn Module Callable Module function traced converted into Graph representation concrete_args Optional Dict str any Inputs partially specialized Returns GraphModule Module created recorded operations ` ` root ` ` tracer = Tracer graph = tracer trace root concrete_args name = root __class__ __name__ isinstance root torch nn Module root __name__ _make_graph_module tracer root graph name wrap _assert_is_none value msg assert value None msg