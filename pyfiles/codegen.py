This module provides utilities generating Python bytecode PyTorch s Dynamo system It includes functionality - Constructing bytecode sequences Python operations - Managing stack operations variable tracking - Handling graph outputs their conversions - Supporting different Python versions + + + - Converting high-level operations low-level bytecode instructions - Managing constant loading attribute access - Supporting function creation closure handling collections dataclasses re sys types collections Counter collections abc Callable Iterable typing Any Optional TYPE_CHECKING Union torch nn torch utils _ordered_set OrderedSet config graph_break_hints utils bytecode_transformation add_push_null add_push_null_call_function_ex create_binary_subscr create_build_tuple create_call_function create_call_function_ex create_call_method create_dup_top create_instruction create_load_const create_load_method create_rot_n Instruction exc IncorrectUsage unimplemented_v source AttrSource ChainedSource DictGetItemSource Source utils is_safe_constant rot_n_helper variables base ValueMutationExisting VariableTracker variables functions ContextlibContextManagerLocalGeneratorObjectVariable LocalGeneratorObjectVariable variables nn_module NNModuleVariable variables tensor NumpyNdarrayVariable SymNodeVariable TensorVariable UnspecializedPythonVariable variables torch_function TensorWithTFOverrideVariable TYPE_CHECKING torch _dynamo variables builder GraphArg symbolic_convert InstructionTranslatorBase dataclasses dataclass GraphOutputEntry index int variable VariableTracker PyCodegen Helper uses constructing Python bytecode __init__ tx InstructionTranslatorBase root Optional torch nn Module = None graph_output_var Optional str = None tempvars Optional dict Union VariableTracker Source Any = None overridden_sources Optional dict Source Source = None - None root = root top_of_stack Optional Union VariableTracker Source = None uses Counter Union VariableTracker Source = collections Counter graph_outputs dict int GraphOutputEntry = _output list Instruction = This determines which VariableTracker Source should stored locals maps VariableTracker Source local variable name Note could map None initially which case we ll overwrite map real temporary names via ` add_cache ` tempvars dict Union VariableTracker Source Any = tempvars tx = tx graph_output_var = graph_output_var code_options = tx output code_options cell_and_freevars = tx cell_and_freevars new_var = tx output new_var value_from_source bool = True This serves way codegen use different source we need because sometimes we can t easily modify original source without affecting other components e g guards overridden_sources dict Source Source = overridden_sources restore_stack stack_values list Any value_from_source bool = True - None prev = value_from_source value_from_source = value_from_source try foreach stack_values finally value_from_source = prev graph_output_vars - list VariableTracker x variable x graph_outputs values call_reconstruct value Union VariableTracker Source GraphArg - None res = value reconstruct assert res None f reconstruct =None value add_push_null gen_fn Callable None call_function_ex bool = False - None ` gen_fn ` generates instructions via PyCodegen methods push single callable stack ` add_push_null ` pushes NULL stack before after instructions generated ` gen_fn ` depending Python version Will attempt use NULL push bit instructions such bits LOAD_GLOBAL + LOAD_ATTR + LOAD_SUPER_ATTR old_len = len _output sys version_info gen_fn may DUP_TOP instead TOS cleared Will cause problems since NULL will pushed right before generated instructions = clear_tos gen_fn inplace modify _output added_insts = _output old_len del _output old_len call_function_ex _output extend add_push_null_call_function_ex added_insts _output extend add_push_null added_insts sys version_info = NULL will top stack clear_tos __call__ value Union VariableTracker Source None allow_cache bool = True - None Generate code such top-of-stack TOS set value ` allow_cache ` controls behavior following manner ` value ` can either VariableTracker Source If ` value ` ` Source ` ` allow_cache ` must True invariant asserted below If source reconstructed earlier we will reuse generated code loading top stack tempvars If ` value ` ` VariableTracker ` we have following cases ` allow_cache=True ` If value source None we will emit code based ` value source ` handle aliasing b If value source None example reconstructing local list returned compiled function we will reconstruct variable tracker w o any source emit bytecode generates new python object In both cases value source being None value reconstructed earlier we will reuse generated code loading top stack tempvars ` allow_cache=False ` - This special case allow_cache defaults True If value source None we reconstruct variable tracker emit new python object You might wonder what about aliasing The place where we use config also has followup code where original python object assigned new python value handle aliasing check side_effects py search allow_cache=False b If value source None allowed Notable effects ` top_of_stack ` will set ` value ` we don t codegen ` value ` based source ` uses value ` will increment unless we codegen via ` top_of_stack ` cached ` tempvars ` b ` value ` has special VT types like ` NNModuleVariable ` etc assert value None isinstance value Source If source needs overridden use new one source = overridden_sources get value value assert allow_cache True allow_cache must True Source top_of_stack value _output append create_dup_top tempvars get source None _output append create_load tempvars source top_of_stack = source uses source += try call_reconstruct source except NotImplementedError unimplemented_v gb_type= Reconstruction failure source reconstruct implemented context=str source explanation=f Dynamo has no bytecode reconstruction implemented type source variable source hints= graph_break_hints DYNAMO_BUG source tempvars _output append create_dup_top add_cache source top_of_stack = source assert isinstance value VariableTracker output = _output graph_outputs = graph_outputs allow_cache top_of_stack value output append create_dup_top tempvars get value None output append create_load tempvars value top_of_stack = value value is_realized isinstance value ContextlibContextManagerLocalGeneratorObjectVariable raise IncorrectUsage NYI Returning contextmanager object torch compile function Dynamo normally prefers codegen source account aliasing value source None allow_cache value is_realized isinstance value LocalGeneratorObjectVariable There s corner case export instance computation graph just identity input tensor Dynamo would just emit ` LOAD_FAST ` input source rather than generating identity FX graph However export wants maximize graph capture case above export _wants to_ obtain identity FX graph despite appears unnecessarily expensive ` torch compile ` so we have following option override Dynamo s preference codegen source Moreover option applies recursively cases like input tensor being returned new dictionary And why ` ValueMutationExisting ` check Not sure so leaving keep old behavior when ` value_from_source ` introduced TODO sort out invariants among side effect codegen export isinstance value mutation_type ValueMutationExisting value_from_source value source value is_python_constant is_safe_constant value as_python_constant output append create_load_const value as_python_constant isinstance value TensorWithTFOverrideVariable graph_outputs_key = add_graph_output value add_push_null lambda load_import_from utils __name__ to_subclass load_graph_output graph_outputs graph_outputs_key index output append create_load_global value global_mangled_class_name tx type ignore arg-type add=True output extend create_call_function False isinstance value SymNodeVariable value python_type float tx export This little unusual force output convention Tensor here Don t do export because apparently load bearing export tests I am bit doubtful actually works real world NB It works add_graph_output computed expression as_tensor here because we memoize as_tensor calls SymNodeVariable graph_outputs_key = add_graph_output value as_tensor tx torch float gen_fn - None load_graph_output graph_outputs graph_outputs_key index output append create_load_attr item add_push_null gen_fn output extend create_call_function False isinstance value TensorVariable SymNodeVariable UnspecializedPythonVariable NumpyNdarrayVariable graph_outputs_key = add_graph_output value isinstance value NumpyNdarrayVariable add_push_null lambda load_import_from utils __name__ to_numpy_helper load_graph_output graph_outputs graph_outputs_key index output extend create_call_function False isinstance value UnspecializedPythonVariable value need_unwrap gen_fn - None load_graph_output graph_outputs graph_outputs_key index output append create_load_attr item add_push_null gen_fn output extend create_call_function False load_graph_output graph_outputs graph_outputs_key index isinstance value NNModuleVariable parts = value module_key split parts code_options co_varnames output append create_load parts parts = parts assert root None output append create_load_const_unchecked root part parts output append create_load_attr part uses value += try call_reconstruct value except NotImplementedError unimplemented_v gb_type= Reconstruction failure context=str value explanation=f Dynamo has no bytecode reconstruction implemented sourceless variable value hints= If Dynamo attempting trace statement your code attempting variable Dynamo cannot reconstruct then remove statement graph_break_hints CAUSED_BY_EARLIER_GRAPH_BREAK Report issue PyTorch you need reconstrtuction support Note objects don t have reconstruction rules may fundamentally unreconstructable allow_cache value tempvars _output append create_dup_top add_cache value top_of_stack = value add_graph_output value VariableTracker - int graph_outputs_key = id value as_proxy graph_outputs_key graph_outputs graph_outputs graph_outputs_key = GraphOutputEntry len graph_outputs value graph_outputs_key load_graph_output index int - None output = _output assert graph_output_var None output append create_load graph_output_var output append create_load_const index output append create_binary_subscr add_cache value Union VariableTracker Source - None var = new_var tempvars value = var _output append create_store var foreach items Iterable Union VariableTracker Source - None i items i create_binary_subscr - Instruction create_binary_subscr setup_globally_cached name str value Any - list Instruction Store value new global name = re sub r ^a-zA-Z - _ + _ name f_globals = tx f_globals name f_globals assert id f_globals name == id value f_globals name = value create_load_global name add=True clear_tos - None top_of_stack = None append_output inst Instruction - None assert isinstance inst Instruction _output append inst clear_tos extend_output insts list Instruction - None assert all isinstance x Instruction x insts _output extend insts clear_tos get_instructions - list Instruction _output create_load name str - Instruction assert name code_options co_varnames f name missing create_instruction LOAD_FAST argval=name create_load_closure name str - Instruction assert name cell_and_freevars inst_name = LOAD_FAST sys version_info = LOAD_CLOSURE create_instruction inst_name argval=name create_load_deref name str - Instruction assert name cell_and_freevars create_instruction LOAD_DEREF argval=name create_store name str - Instruction assert name code_options co_varnames f name missing create_instruction STORE_FAST argval=name create_store_deref name str - Instruction assert name cell_and_freevars create_instruction STORE_DEREF argval=name create_load_global name str add bool = False - Instruction add tx output update_co_names name assert name code_options co_names f name co_names create_instruction LOAD_GLOBAL argval=name create_load_const value Any - Instruction create_load_const value create_load_const_unchecked value Any - Instruction create_load_const value checked=False load_method name str - None tx output update_co_names name append_output create_load_method name call_method nargs int - None extend_output create_call_method nargs create_load_attr name str - Instruction name code_options co_names code_options co_names += name create_instruction LOAD_ATTR argval=name load_attr name str - None append_output create_load_attr name create_load_attrs names str - list Instruction create_load_attr name name names split create_store_attr name str - Instruction name code_options co_names code_options co_names += name create_instruction STORE_ATTR argval=name store_attr name str - None append_output create_store_attr name load_function_name fn_name str push_null bool num_on_stack int = - list Instruction Load global fn_name stack num_on_stack down output = push_null sys version_info = output extend add_push_null create_load_global fn_name add=True num_on_stack output extend rot_n num_on_stack + rot_n num_on_stack + output extend create_load_global fn_name add=True rot_n num_on_stack + output rot_n n int - list Instruction try create_rot_n n except AttributeError desired rotate bytecode doesn t exist generate equivalent bytecode create_build_tuple n create_load_const_unchecked rot_n_helper n create_rot_n create_call_function_ex False False create_instruction UNPACK_SEQUENCE arg=n pop_top - None append_output create_instruction POP_TOP call_function nargs int push_null bool - None extend_output create_call_function nargs push_null=push_null dup_top - None append_output create_dup_top store varname str - None append_output create_store varname load_deref varname str - None append_output create_load_deref varname make_function_with_closure fn_name str code types CodeType - None Creates closure code object ` code ` Expects TOS tuple cells use closure TOS will popped create closure Args - fn_name name function - code code object function does include tuple cells TOS output = _output output append create_load_const code sys version_info output append create_load_const fn_name sys version_info = output extend create_instruction MAKE_FUNCTION create_instruction SET_FUNCTION_ATTRIBUTE arg= x output append create_instruction MAKE_FUNCTION arg= x clear_tos create_load_python_module mod types ModuleType - Instruction Generate LOAD_GLOBAL instruction fetch given python module output = tx output global_scope = output global_scope name = re sub r ^ mod __name__ global_scope get name None mod create_load_global name add=True prefix = f ___module_ name global_name = tx output install_global_by_id prefix mod create_load_global global_name add=True mark_source_temp source Source - None Mark source temp variable so can reused source tempvars tempvars source = None make_call_generated_code fn_name str - None Call generated code function stored fn_name extend_output load_function_name fn_name True graphargs = tx output graphargs seen_sources OrderedSet Source = OrderedSet collect_temp_source source Source - None source seen_sources This source used least twice so can reused mark_source_temp source Dont trace source further This prevents us marking too many nodes temp sources seen_sources add source isinstance source ChainedSource collect_temp_source source base isinstance source DictGetItemSource isinstance source index Source collect_temp_source source index Collect all sources used more than once so we can generate tmp variables generated pre-graph bytecode This essentially implements CSE arg graphargs arg source None collect_temp_source arg source cm_var = None config record_runtime_overhead Record pregraph bytecode start add_push_null lambda load_import_from utils __name__ record_pregraph_bytecode_enter extend_output create_call_function False cm_var = new_var store cm_var arg graphargs arg pass_arg_as_tensor add_push_null lambda extend_output create_load_python_module torch create_load_attr _as_tensor_fullprec call_reconstruct arg extend_output create_call_function False call_reconstruct arg config record_runtime_overhead Record pregraph bytecode end add_push_null lambda load_import_from utils __name__ record_pregraph_bytecode_exit assert cm_var None extend_output create_load cm_var extend_output create_call_function False pop_top extend_output create_call_function len graphargs False create_import_name module_name str - Instruction create_instruction IMPORT_NAME argval=module_name load_import_from module_name str object_name str - None source = AttrSource tx import_source module_name object_name Note This approach somewhat aggressive because typically source marked tempvar only when used more than once In case we re marking tempvar without performing analysis However simple solution many cases load imports reused multiple times mark_source_temp source source create_call_function_kw nargs int kw_names Iterable str push_null bool - list Instruction sys version_info = output = create_call_function nargs push_null assert output - opname == CALL output insert - create_load_const kw_names output - = create_instruction CALL_KW arg=nargs output sys version_info = output = create_call_function nargs push_null sys version_info = idx = - expected_inst = CALL idx = - expected_inst = PRECALL assert output idx opname == expected_inst kw_names_inst = create_instruction KW_NAMES argval=kw_names output insert idx kw_names_inst output create_load_const kw_names create_instruction CALL_FUNCTION_KW arg=nargs create_delete value object - Instruction create_instruction DELETE_FAST argval=value