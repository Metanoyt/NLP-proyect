typing Any Optional torch torch nn functional F expanded_weights_impl implements_per_sample_grads expanded_weights_utils forward_helper set_grad_sample_if_exists standard_kwargs implements_per_sample_grads F embedding EmbeddingPerSampleGrad torch autograd Function staticmethod pyrefly ignore bad-override forward ctx Any kwarg_names list str _ Any expanded_args_and_kwargs Any - torch Tensor expanded_args expanded_kwargs = standard_kwargs kwarg_names expanded_args_and_kwargs len expanded_args shape == raise RuntimeError f Expanded Weights needs input batch size got D tensor expanded_args output = forward_helper F embedding expanded_args expanded_kwargs ctx input ctx weight = expanded_args ctx padding_idx ctx scale_grad_by_freq = expanded_kwargs padding_idx expanded_kwargs scale_grad_by_freq ctx sparse = expanded_kwargs sparse output staticmethod pyrefly ignore bad-override backward ctx Any grad_output torch Tensor - tuple Optional torch Tensor input weight = ctx input ctx weight padding_idx scale_grad_by_freq sparse = ctx padding_idx ctx scale_grad_by_freq ctx sparse weight_per_sample_grad weight torch Tensor - torch Tensor batch_size = input shape embedding_dim = weight shape index = input unsqueeze - expand input shape embedding_dim reshape batch_size - embedding_dim grad_sample = torch zeros type ignore attr-defined batch_size weight shape device=weight device dtype=grad_output dtype grad_sample scatter_add_ index grad_output reshape batch_size - embedding_dim results list Optional torch Tensor = results append None kwarg names results append None op reference input requires_grad bw_fn = torch ops aten embedding_backward results append bw_fn grad_output input weight shape padding_idx scale_grad_by_freq sparse results append None weight doesn t compute batched gradients no other arguments differentiable saved forward results = results + None set grad_sample field weight per sample gradients set_grad_sample_if_exists weight weight_per_sample_grad tuple results