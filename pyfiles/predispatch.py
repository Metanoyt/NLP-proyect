mypy ignore-errors Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree This module contains pre-dispatch wrappers functorch operations enable proper tracing PT non-strict export compile fx graph torch torch _C _functorch _add_batch_dim _add_batch_dim_impl _remove_batch_dim _remove_batch_dim_impl _vmap_decrement_nesting _vmap_decrement_nesting_impl _vmap_increment_nesting _vmap_increment_nesting_impl _add_batch_dim batch_dim level Thin wrapper around torch _C _add_batch_dim used proxy PT export compile fx graph torch _export utils _maybe_find_pre_dispatch_tf_mode_for_export mode = _maybe_find_pre_dispatch_tf_mode_for_export batch_dim = ndim + batch_dim batch_dim batch_dim mode torch overrides handle_torch_function _add_batch_dim batch_dim level res = _add_batch_dim_impl batch_dim level res _remove_batch_dim level batch_size out_dim Thin wrapper around torch _C _remove_batch_dim used proxy PT export compile fx graph torch _export utils _maybe_find_pre_dispatch_tf_mode_for_export mode = _maybe_find_pre_dispatch_tf_mode_for_export mode torch overrides handle_torch_function _remove_batch_dim level batch_size out_dim res = _remove_batch_dim_impl level batch_size out_dim res _vmap_increment_nesting batch_size randomness Thin wrapper around torch _C _vmap_increment_nesting used proxy export compile graph torch _export utils _maybe_find_pre_dispatch_tf_mode_for_export mode = _maybe_find_pre_dispatch_tf_mode_for_export mode torch overrides handle_torch_function _vmap_increment_nesting batch_size batch_size randomness res = _vmap_increment_nesting_impl batch_size randomness res _vmap_decrement_nesting Thin wrapper around torch _C _vmap_increment_nesting used proxy export compile graph torch _export utils _maybe_find_pre_dispatch_tf_mode_for_export mode = _maybe_find_pre_dispatch_tf_mode_for_export mode torch overrides handle_torch_function _vmap_decrement_nesting _vmap_decrement_nesting_impl Global variables lazy_load_decompositions DECOMPOSITIONS_LOADED = False DECOMPOSITIONS_LOCK = None Will initialized when needed VMAP_DECOMPOSITIONS_LIB = None lazy_load_decompositions Lazy loading vmap decompositions pre-dispatch support torch _export utils _maybe_find_pre_dispatch_tf_mode_for_export mode = _maybe_find_pre_dispatch_tf_mode_for_export mode torch overrides handle_torch_function lazy_load_decompositions global DECOMPOSITIONS_LOADED DECOMPOSITIONS_LOCK VMAP_DECOMPOSITIONS_LIB DECOMPOSITIONS_LOADED Initialize lock needed DECOMPOSITIONS_LOCK None threading DECOMPOSITIONS_LOCK = threading Lock DECOMPOSITIONS_LOCK DECOMPOSITIONS_LOADED os os environ get PYTORCH_JIT == __debug__ DECOMPOSITIONS_LOADED = True use alternate way register operator into decomposition table _register_jit_decomposition doesn t work some operators e g addr because Tensor types generated cannot unioned torchscript decomp should type OpOverload VMAP_DECOMPOSITIONS_LIB = torch library Library aten IMPL FuncTorchBatched torch _decomp decomposition_table _register_python_decomposition_vmap decomp decomp decomposition_table VMAP_DECOMPOSITIONS_LIB impl decomp decomposition_table decomp raise RuntimeError f could find decomposition decomp _register_python_decomposition_vmap torch ops aten mse_loss_backward default _register_python_decomposition_vmap torch ops aten smooth_l _loss_backward default _register_python_decomposition_vmap torch ops aten huber_loss_backward default _register_python_decomposition_vmap torch ops aten nll_loss_forward default _register_python_decomposition_vmap torch ops aten nll_loss d_forward default _register_python_decomposition_vmap torch ops aten nll_loss_backward default _register_python_decomposition_vmap torch ops aten nll_loss d_backward default _register_python_decomposition_vmap torch ops aten addr default DECOMPOSITIONS_LOADED = True