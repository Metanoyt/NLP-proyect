This module provides decorators utilities controlling TorchDynamo s behavior during compilation functools inspect weakref collections abc Callable dataclasses dataclass types TracebackType typing Any Optional overload TYPE_CHECKING TypeVar Union typing_extensions ParamSpec torch torch compiler is_compiling torch utils _contextlib _DecoratorContextManager torch utils _python_dispatch is_traceable_wrapper_subclass trace_rules variables comptime comptime eval_frame _set_stance DisableContext DynamoStance innermost_fn RunOnlyContext skip_code exc IncorrectUsage external_utils get_nonrecursive_disable_wrapper wrap_dunder_call_ctx_manager utils _get_error_on_graph_break _set_error_on_graph_break is_function TYPE_CHECKING types FunctionType torch _C _dynamo eval_frame noqa F reset_code set_eval_frame set_guard_complete_hook set_guard_error_hook unsupported variables VariableTracker name dir torch _C _dynamo eval_frame name startswith __ continue globals name = getattr torch _C _dynamo eval_frame name _P = ParamSpec _P _R = TypeVar _R FuncType = Callable Any F = TypeVar F bound=FuncType run fn Optional Callable _P _R = None - Any Don t do any dynamic compiles just use prior optimizations fn None fn = innermost_fn fn assert callable fn RunOnlyContext fn RunOnlyContext disable fn=None recursive=True reason=None wrapping=True type ignore no-untyped-def Decorator disable TorchDynamo If recursive=True Dynamo completely skipped decorated function frame well recursively invoked functions If recursive=False Dynamo skips frames associated function code still process recursively invoked frames If reason provided will printed when Dynamo attempts trace disabled function recursive fn None fn = innermost_fn fn assert callable fn DisableContext msg=reason wrapping=wrapping fn DisableContext msg=reason wrapping=wrapping wrap fn Callable _P _R - Callable _P _R fn = innermost_fn fn assert callable fn nonrecursive_disable_wrapper = get_nonrecursive_disable_wrapper fn nonrecursive_disable_wrapper _torchdynamo_disable = True type ignore attr-defined nonrecursive_disable_wrapper _torchdynamo_disable_msg = reason type ignore attr-defined nonrecursive_disable_wrapper _torchdynamo_orig_callable = fn type ignore attr-defined pyrefly ignore bad-return nonrecursive_disable_wrapper fn None wrap wrap fn _nonrecursive_disable_wrapper_code = disable lambda None recursive=False __code__ type ignore attr-defined skip_code _nonrecursive_disable_wrapper_code skip fn Optional Callable _P _R = None - Callable Any Skip frames associated function code still process recursively invoked frames fn None skip fn = innermost_fn fn assert callable fn skip_code fn __code__ fn _torchdynamo_disable = True type ignore attr-defined fn set_stance _DecoratorContextManager Decorator context manager function set current stance compiler Stances documented corresponding function torch compiler __init__ py _dynamo_forbidden = True __init__ stance str = default skip_guard_eval_unsafe bool = False force_backend Union str Callable Any None = None - None force_backend None stance = default raise RuntimeError non-default stance cannot have force_backend set stance = DynamoStance stance skip_guard_eval_unsafe force_backend prev = _set_stance stance __call__ fn F - F _set_stance prev wrapper = super __call__ fn forbid wrapper graph wrapper _dynamo_forbidden = True type ignore attr-defined wrapper __enter__ - None _set_stance stance __exit__ exc_type Optional type BaseException exc_val Optional BaseException exc_tb Optional TracebackType - None _set_stance prev clone - set_stance __class__ stance stance force_backend=self stance backend assume_constant_result fn type ignore no-untyped-def fn _dynamo_marked_constant = True type ignore attr-defined fn allow_in_graph fn type ignore no-untyped-def Tells compiler frontend Dynamo skip symbolic introspection function instead directly write graph when encountered See func ` torch compiler allow_in_graph ` s docstring full documentation WARNING API can footgun please read documentation carefully isinstance fn list tuple allow_in_graph x x fn assert callable fn allow_in_graph expects callable trace_rules lookup_callable fn = variables TorchInGraphFunctionVariable fn_id = id fn trace_rules _disallowed_callable_ids remove fn_id trace_rules _allowed_callable_ids add fn_id Avoid id reuse which creates subtle bugs deregister - None trace_rules _allowed_callable_ids remove fn_id weakref finalize fn deregister fn nonstrict_trace traceable_fn Callable _P _R - Callable _P _R Like ` allow_in_graph ` following enhancements differences Supports user-defined inputs long has been registered pytree Reads global captured tensors forces underlying graph treat those tensors constant we _assume_ they will updated This similar FX tracing In resulting Dynamo graph call ` nonstrict_trace ` -ed function will represented call ` torch _higher_order_ops flat_apply ` which takes ` nonstrict_trace ` -ed function pytree-flattened inputs Only returned function traceable original function will Moreover ` nonstrict_trace ` can used inside ` torch compile ` region NOTE like ` allow_in_graph ` aliasing information neither preserved between inputs themselves nor between inputs outputs assert callable traceable_fn nonstrict_trace expects callable functools wraps traceable_fn wrapped args _P args kwargs _P kwargs - _R traceable_fn args kwargs wrapped_id = id wrapped This line allows us reuse much ` allow_in_graph ` impl trace_rules _allowed_callable_ids add wrapped_id This line allows us diverge impl ` allow_in_graph ` trace_rules _nonstrict_trace_callable_ids add wrapped_id Avoid id reuse which creates subtle bugs deregister - None trace_rules _allowed_callable_ids remove wrapped_id trace_rules _nonstrict_trace_callable_ids remove wrapped_id weakref finalize wrapped deregister wrapped _disallow_in_graph_helper throw_if_not_allowed bool - Callable Any inner fn Any - Any isinstance fn list tuple disallow_in_graph x x fn assert callable fn disallow_in_graph expects callable throw_if_not_allowed trace_rules lookup_callable fn = variables TorchInGraphFunctionVariable trace_rules lookup fn = variables TorchInGraphFunctionVariable raise IncorrectUsage disallow_in_graph expected used already allowed callable like torch ops Allowed callables means callables TorchDynamo puts as-is extracted graph trace_rules _allowed_callable_ids remove id fn trace_rules _nonstrict_trace_callable_ids remove id fn trace_rules _disallowed_callable_ids add id fn fn inner disallow_in_graph fn Callable Any - Any Customize which functions TorchDynamo will exclude generated graph force graph break torch _dynamo disallow_in_graph torch sub torch _dynamo optimize fn x = torch add x x = torch sub x x = torch add x x fn Will break graph ` torch sub ` give two graphs each single ` torch add ` op _disallow_in_graph_helper throw_if_not_allowed=True fn _disallow_in_graph_helper throw_if_not_allowed=False graph_break msg str = - None Force graph break NOTE primarily used internal debugging purposes _disallow_in_graph_helper throw_if_not_allowed=False skip_frame msg str = - None Force skipped frame _disallow_in_graph_helper throw_if_not_allowed=False step_unsupported msg str = - None Force step unsupported graph break which results compiling traced FX graph so far then skipping rest frame In order get expected behavior there should least ops part code contained any try blocks forbid_in_graph fn Any - Any Customize which functions TorchDynamo will assert present while tracing If you want graph break function instead use disallow_in_graph TODO voz We now have allow_in_graph disallow_in_graph forbid_in_graph - some more robust documentation would amiss isinstance fn list tuple forbid_in_graph x x fn assert callable fn forbid_in_graph applies only callables pyrefly ignore missing-attribute fn _dynamo_forbidden = True fn substitute_in_graph original_fn Callable _P _R can_constant_fold_through bool = False skip_signature_check bool = False type embedded Python interpreter is_embedded_type bool = False internal use only - Callable Callable _P _R Callable _P _R Register polyfill handler function usually C function C extension used place original function when inlining original function graph note The polyfill handler only used when inlining original function It used when original function called directly In eager mode decorated function calls performant C function rather than polyfill handler The polyfill handler function will called place original function when inlining original function The polyfill handler should have same signature same behavior original function Args original_fn callable The original function usually C function register polyfill handler can_constant_fold_through bool optional Whether polyfill handler can constant folded through That polyfill handler pure function its arguments constant result polyfill handler can constant folded during compilation Defaults ` ` False ` ` skip_signature_check bool optional Whether skip signature check between original function polyfill handler Defaults ` ` False ` ` Returns A decorator registers polyfill handler original function Example xdoctest +SKIP conflict tests duplicate polyfill handlers operator operator indexOf torch compile operator indexOf fullgraph=True Traceback most recent call last torch _dynamo exc Unsupported torch compiler substitute_in_graph operator indexOf indexOf b i item enumerate item b item == b i raise ValueError sequence index x x sequence torch compile operator indexOf fullgraph=True is_function original_fn is_embedded_type inspect isclass original_fn raise TypeError f substitute_in_graph expects function got type original_fn r is_embedded_type inspect isclass original_fn raise TypeError f substitute_in_graph expects got type original_fn r variables builder ITERTOOLS_POLYFILLED_TYPE_IDS ITERTOOLS_TYPE_IDS id original_fn ITERTOOLS_TYPE_IDS ITERTOOLS_POLYFILLED_TYPE_IDS add id original_fn wrapper traceable_fn Callable _P _R - Callable _P _R is_function traceable_fn raise TypeError f substitute_in_graph expects function got type traceable_fn r skip_signature_check try original_sig = inspect signature original_fn except ValueError pass traceable_sig = inspect signature traceable_fn sig_ident sig inspect Signature - tuple tuple str set str dict str Any Ignore annotations parameters type tuple p name p sig parameters values p kind p KEYWORD_ONLY name args kwargs important p VAR_POSITIONAL p VAR_KEYWORD p name p sig parameters values p kind == p KEYWORD_ONLY p name p default p sig parameters values name args kwargs important p kind p VAR_POSITIONAL p VAR_KEYWORD wildcard_sig = inspect signature lambda args kwargs None sig_ident original_sig = sig_ident traceable_sig sig_ident original_sig = sig_ident wildcard_sig sig_ident traceable_sig = sig_ident wildcard_sig raise TypeError f Signature mismatch between original_fn traceable_fn f original_sig = traceable_sig torch _dynamo guards GuardBuilder torch _dynamo trace_rules _polyfilled_function_ids get_torch_obj_rule_map torch _dynamo variables PolyfilledFunctionVariable torch _dynamo variables builder VariableBuilder id_dispatch_map = VariableBuilder _id_dispatch id original_fn id_dispatch_map raise ValueError f Duplicate dispatch rule original_fn already registered VariableBuilder s id dispatch map id original_fn _polyfilled_function_ids raise ValueError f Duplicate polyfilled object original_fn rule_map dict Any type VariableTracker = get_torch_obj_rule_map original_fn rule_map raise ValueError f Duplicate object original_fn different rules f PolyfilledFunctionVariable rule_map original_fn polyfill_handlers dict Callable Any FunctionType polyfill_handlers = PolyfilledFunctionVariable _get_polyfill_handlers original_fn polyfill_handlers raise ValueError f Duplicate polyfill handlers original_fn f already handled polyfill_handlers original_fn Need wrap function because we may cannot assign __torch_dynamo_polyfill__ C++ function functools wraps traceable_fn wrapped args _P args kwargs _P kwargs - _R original_fn args kwargs dispatch_fn VariableBuilder value Callable _P _R - PolyfilledFunctionVariable inspect isclass value guard_type = GuardBuilder CLASS_MATCH inspect ismodule value guard_type = GuardBuilder MODULE_MATCH guard_type = GuardBuilder ID_MATCH PolyfilledFunctionVariable value source=self source install_guards guard_type id_dispatch_map id original_fn = id_dispatch_map id wrapped = dispatch_fn _polyfilled_function_ids add id original_fn _polyfilled_function_ids add id wrapped rule_map original_fn = rule_map wrapped = PolyfilledFunctionVariable polyfill_handlers original_fn = polyfill_handlers wrapped = wrapped type ignore assignment wrapped __torch_dynamo_original__ = original_fn type ignore attr-defined wrapped __torch_dynamo_polyfill__ = traceable_fn type ignore attr-defined wrapped __torch_dynamo_can_constant_fold_through__ = can_constant_fold_through type ignore attr-defined wrapped type ignore return-value wrapper Helper function flatten tensor subclass apply function all inner tensors match outer dim Used reduce duplication across various marking APIs _apply_func_to_inner_tensors_of_same_dim func Callable Any t object args Any kwargs Any - None assert is_traceable_wrapper_subclass t attrs _ctx = t __tensor_flatten__ assert isinstance t torch Tensor attr attrs inner = getattr t attr inner dim == t dim func inner args kwargs dataclass frozen=True _DimRange This represents dimension tensor corresponding min max values can take Don t create directly instead use func ` mark_dynamic ` dim int min int max int forbid_in_graph mark_unbacked t Any index Union int list Any tuple Any hint_override Optional int = None strict bool = False specialize_on Optional list Any = None - None Mark tensor having unbacked dimension This changes semantics operations - The size specified dimension will always reported equal zero one - Assertions index will turned into runtime asserts - Attempting get real value dimension will raise exception - In effect dimension treated data-dependent its value unknown Args t Any The tensor mark having unbacked dimension index int list tuple int The dimension s mark unbacked Can single integer list tuple integers hint_override Optional int default=None An optional integer override size hint dimension This only used inductor backend size hint queries such during autotuning strict bool default=False If True error will raised unbacked dimension specialized By default strict=False specialization allowed will proceed without error specialize_on Optional list Any default=None A list specialization criteria e g lambdas dimension If provided Dynamo will generate specialized compiled regions each criterion addition generic trace You could have copied mark_dynamic behavior I m convinced s what you want assert is_traceable_wrapper_subclass t implemented yet isinstance index int strict hasattr t _dynamo_strict_unbacked_indices t _dynamo_strict_unbacked_indices = set t _dynamo_strict_unbacked_indices add index hasattr t _specialized_on t _specialize_on = hasattr t _dynamo_unbacked_indices t _dynamo_unbacked_indices = set hasattr t _dynamo_hint_overrides t _dynamo_hint_overrides = hint_override t _dynamo_hint_overrides index = hint_override FX tracers don t respect forbid_in_graph choke following error since passes proxies TypeError Attribute object does support item assignment isinstance t _specialize_on dict t _specialize_on index = specialize_on specialize_on None t _dynamo_unbacked_indices add index assert isinstance index list tuple i index mark_unbacked t i forbid_in_graph mark_dynamic t Any index Union int list Any tuple Any hint_override Optional int = None min Optional int = None max Optional int = None specialize_on Optional list Any = None - None Mark tensor having dynamic dim set corresponding min max range dim Note - state mark_dynamic The behavior having dynamic dimension tensor governed few factors torch _dynamo config dynamic_shapes True False dynamic_shapes=True - dynamic_shapes must True mark_dynamic work dynamic_shapes=False - This config will raise exception when used conjunction mark_dynamic We will eventually support If dimension fully constrained - does allow more than single value both eager torch compile torch _dynamo optimize mode export mode torch _dynamo export we will raise error If dimension partially constrained - allowing least values full unbounded range shapes eager we will pass through export will raise error Attempts trace function will explicitly raise As such all calls mark_dynamic must made before torch compile If hint_override passed hint_override specified dimension will replace provided value first example input official size hint If specialize_on passed we will perform single generic Dynamo trace followed multiple specialized compilations addition single generic compilation NB For now we only support per dimension specialization other words we do generate cross product specializations At runtime we will dispatch specialized compiled region input matches specialization criteria For example mark_dynamic specialize_on= lambda x x == lambda x x == This approach results one Dynamo trace two backend compilations When input dimension equals runtime execution will directed specialized compiled region Performance measurements indicate - x speedups depending specific specialization model architecture is_traceable_wrapper_subclass t default behavior mirror mark_dynamic all inner tensors same dim t TODO Make configurable via supported public API _apply_func_to_inner_tensors_of_same_dim mark_dynamic t index min=min max=max isinstance index int hasattr t _dynamo_dynamic_indices pyrefly ignore missing-attribute t _dynamo_dynamic_indices = set pyrefly ignore missing-attribute t _dynamo_dynamic_range = set pyrefly ignore missing-attribute t _dynamo_hint_overrides = hasattr t _specialize_on pyrefly ignore missing-attribute t _specialize_on = hint_override pyrefly ignore missing-attribute t _dynamo_hint_overrides index = hint_override TODO voz Should we bounds check pyrefly ignore missing-attribute t _dynamo_dynamic_indices add index t _dynamo_dynamic_range add _DimRange index min max type ignore arg-type FX tracers don t respect forbid_in_graph choke following error since passes proxies TypeError Attribute object does support item assignment pyrefly ignore missing-attribute isinstance t _specialize_on dict t _specialize_on index = specialize_on specialize_on None assert isinstance index list tuple i index mark_dynamic t i min=min max=max mark_dynamic t i min=min max=max specialize_on=specialize_on forbid_in_graph maybe_mark_dynamic t Any index Union int list Any tuple Any - None Mark tensor having dynamic dim don t enforce i e dimension ends up getting specialized don t error is_traceable_wrapper_subclass t default behavior mirror maybe_mark_dynamic all inner tensors same dim t TODO Make configurable via supported public API _apply_func_to_inner_tensors_of_same_dim maybe_mark_dynamic t index isinstance index int hasattr t _dynamo_weak_dynamic_indices pyrefly ignore missing-attribute t _dynamo_weak_dynamic_indices = set TODO voz Should we bounds check pyrefly ignore missing-attribute t _dynamo_weak_dynamic_indices add index assert isinstance index list tuple i index maybe_mark_dynamic t i mark_static t Any index Optional Union int list Any tuple Any = None - None Mark tensor having static dim mark nn module static For tensors =========== This will prevent us attempting compile dynamically when dynamic=True can improve trace-time performance This has lower precedence than mark_dynamic Unlike mark_dynamic can done inside graph which case induces specialization tensor For nn Module classes ===================== For static nn Module classes TorchDynamo assumes module instance attributes will modified after compilation This will ensure TorchDynamo keeps integer attributes CONSTANT symints From TorchDynamo implementation side instances static-marked nn Module will converted UnspecializedBuiltinNNModuleVariable which have same properties Note we still have guard attributes because different instances nn Module can have different values attributes The key point here attributes static is_compiling index None s t size comptime force_static s comptime force_static t size index is_traceable_wrapper_subclass t default behavior mirror mark_static all inner tensors same dim t TODO Make configurable via supported public API _apply_func_to_inner_tensors_of_same_dim mark_static t index pyrefly ignore bad-argument-type isinstance t torch Tensor issubclass t torch nn Module pyrefly ignore missing-attribute t _dynamo_marked_static = True pyrefly ignore bad-return t isinstance t torch Tensor raise TypeError f mark_static expects tensor nn Module received type t isinstance index int hasattr t _dynamo_static_indices t _dynamo_static_indices = set type ignore attr-defined TODO voz Should we bounds check t _dynamo_static_indices add index type ignore attr-defined index None i range t dim mark_static t i assert isinstance index list tuple i index mark_static t i forbid_in_graph mark_static_address t Any guard bool = False - None Marks input tensor whose address should treated constant across calls same dynamo-compiled function This indicates cudagraphs extra allocation needed input The data_ptr will guarded guard=True cause full recompile data_ptr changes Note If address changes cudagraphs will re-record guard=False isinstance t torch Tensor raise TypeError f mark_static_address expects tensor received type t guard t _dynamo_static_input_type = guarded type ignore attr-defined t _dynamo_static_input_type = unguarded type ignore attr-defined One day Dynamo will support tracing into einops directly no allow_in_graph needed Note PyTorch supports multiple versions einops so when day comes we still need really careful about version matches _allow_in_graph_einops - None einops try requires einops torch = einops _torch_specific type ignore attr-defined noqa F _ops_were_registered_in_torchdynamo einops will call op registration logic imported except ImportError einops = allow_in_graph einops rearrange allow_in_graph einops reduce hasattr einops repeat allow_in_graph einops repeat available since einops hasattr einops einsum allow_in_graph einops einsum available since einops hasattr einops pack allow_in_graph einops pack available since einops hasattr einops unpack allow_in_graph einops unpack available since einops Note carefully avoids eagerly einops trace_rules add_module_init_func einops _allow_in_graph_einops Proxy torch _dynamo config patching - so dynamo can identify context managers decorators created patch_dynamo_config compared ones created raw torch _dynamo config patch DynamoConfigPatchProxy __init__ config_patch Any - None config_patch = config_patch property changes - dict str Any config_patch changes Decorator implementation simply sets up ` ` context manager Placed external_utils so we can trace through __call__ = wrap_dunder_call_ctx_manager __enter__ - None config_patch __enter__ __exit__ exc_type Optional type BaseException exc_val Optional BaseException exc_tb Optional TracebackType - None config_patch __exit__ exc_type exc_val exc_tb Criteria patchable config - Config values must constants i e int float str bool None - particular NO list set dict - Traceable config patches only useful configs change dynamo behavior symbolic_convert below - e g patching recompile_limit won t really do anything - For patching configs affect Dynamo behavior above symbolic_convert ensure Dynamo behaves soundly even tracing done different config - e g careful patching guard-related configs configs may have changed between guard creation evaluation _allowed_config_patches = verbose verify_correctness rewrite_assert_with_torch_assert capture_scalar_outputs allow_unspec_int_on_nn_module skip_torchrec dont_skip_tracing nested_graph_breaks config name _allowed_config_patches assert hasattr config name nonexistent config del config _patch_dynamo_config_check changes dict str Any - None k v changes items k _allowed_config_patches raise ValueError f patch_dynamo_config does support patching config k torch _dynamo utils is_safe_constant v raise ValueError f patch_dynamo_config does support patching config k f non-safe-constant value v TODO also implement nonrecursive patch_dynamo_config dont_skip_tracing Unlike config patch we also need accept tuple input order deal context manager reconstruction patch_dynamo_config arg Optional Union str dict str Any tuple tuple str Any = None arg Any = None kwargs Any - DynamoConfigPatchProxy A wrapper around torch _dynamo config patch can traced Dynamo temporarily change config values DURING tracing See _allowed_config_patches list allowed config patches Arguments same torch _dynamo config patch Can used decorator context manager User code SHOULD NOT MODIFY value function WARNING changing Dynamo config during tracing can lead unpredictable tracing behavior Proceed only advised isinstance arg tuple arg = dict arg config_patch = torch _dynamo config patch arg arg kwargs _patch_dynamo_config_check config_patch changes check valid patching using config_patch changes DynamoConfigPatchProxy config_patch overload dont_skip_tracing fn None = None - DynamoConfigPatchProxy overload dont_skip_tracing fn Callable _P _R - Callable _P _R dont_skip_tracing fn Optional Any = None - Any Context manager decorator trace into functions intentionally marked developers skipped when tracing This decorator will also apply recursively invoked functions ctx = patch_dynamo_config dont_skip_tracing=True fn ctx fn ctx overload disable_nested_graph_breaks fn None = None - DynamoConfigPatchProxy overload disable_nested_graph_breaks fn Callable _P _R - Callable _P _R disable_nested_graph_breaks fn Optional Any = None - Any Context manager decorator disable nested graph breaks when tracing function any nested functions Used when nested graph breaks causing problems ctx = patch_dynamo_config nested_graph_breaks=False fn ctx fn ctx ErrorOnGraphBreakDecoratorContextManager __init__ error_on_graph_break bool - None error_on_graph_break = error_on_graph_break __call__ = wrap_dunder_call_ctx_manager __enter__ - None prev_error_on_graph_break = _get_error_on_graph_break _set_error_on_graph_break error_on_graph_break __exit__ exc_type Optional type BaseException exc_val Optional BaseException exc_tb Optional TracebackType - None _set_error_on_graph_break prev_error_on_graph_break error_on_graph_break error_on_graph_break bool - ErrorOnGraphBreakDecoratorContextManager Context manager decorator toggle torch compile s ` error_on_graph_break ` setting compile time If ` fullgraph ` set then ` error_on_graph_break ` does nothing i e ` fullgraph = True ` takes higher precedence If ` fullgraph ` False then ` error_on_graph_break ` determines whether ` torch compile ` throws error upon encountering graph break attempts continue tracing ` error_on_graph_break ` can toggled during compile time decorator allow graph breaks some compiled regions others One key difference ` fullgraph ` ` error_on_graph_break = True ` does NOT guarantee single graph captured compiled function The default value torch compile s ` error_on_graph_break ` setting False ErrorOnGraphBreakDecoratorContextManager error_on_graph_break