mypy allow-untyped-defs itertools logging operator collections defaultdict collections abc Sequence contextlib nullcontext dataclasses dataclass typing Any Callable cast torch torch fx node torch _C _dynamo guards compute_overlapping_tensors torch _dispatch python enable_python_dispatcher torch _dynamo utils ReinplaceCounters ReInplaceTrigger torch _guards detect_fake_mode torch _higher_order_ops triton_kernel_wrap kernel_side_table triton_kernel_wrapper_functional torch _inductor config inductor_prims torch _inductor fx_utils get_node_storage is_node_realized torch _inductor lowering inplaceable_foreach_ops inplaceable_foreach_ops_lowerings torch _inductor virtualized V torch fx experimental symbolic_shapes compute_unbacked_bindings GuardOnDataDependentSymNode torch fx immutable_collections immutable_dict immutable_list torch fx passes reinplace _is_view_op torch utils _pytree pytree torch utils _ordered_set OrderedSet log = logging getLogger __name__ aten = torch ops aten dataclass frozen=True InplaceableOp inplace_op Callable Any mutated_arg int extra_check Callable torch fx Node bool = lambda node True _SCATTER_OP_TO_VIEW = torch ops aten diagonal_scatter default torch ops aten diagonal default torch ops aten select_scatter default torch ops aten select int torch ops aten slice_scatter default torch ops aten slice Tensor torch ops aten as_strided_scatter default torch ops aten as_strided default _VIEW_OP_TO_SCATTER = v k k v _SCATTER_OP_TO_VIEW items graph_call_function graph torch fx Graph fn args kwargs fake_args fake_kwargs = pytree tree_map lambda node node meta val isinstance node torch fx Node node args kwargs V fake_mode fake_result = fn fake_args fake_kwargs node = graph call_function fn args kwargs node meta val = fake_result node dataclass ViewOp target torch _ops OpOverload args tuple Any kwargs dict str Any _inplace_generalized_scatter inp torch Tensor src torch Tensor view_ops list ViewOp - torch Tensor tmp = inp view view_ops fake_args fake_kwargs = pytree tree_map lambda node node meta val isinstance node torch fx Node node view args view kwargs slice select can allocate new unbacked symints those won t reflected output function hence shall ignored fake_mode = detect_fake_mode fake_args fake_mode shape_env ignore_fresh_unbacked_symbols fake_mode fake_mode shape_env nullcontext tmp = view target tmp fake_args fake_kwargs try tmp copy_ src except RuntimeError e raise RuntimeError f shape error scatter op can broadcast src shape tmp shape e inp _generalized_scatter inp torch Tensor src torch Tensor view_ops list ViewOp - torch Tensor out = inp clone _inplace_generalized_scatter out src view_ops _decompose_scatter_functional_helper graph torch fx Graph inp torch Tensor src torch Tensor view_ops list ViewOp - torch fx Node view_op view_ops_tail = view_ops view_ops view_ops_tail view = graph_call_function graph view_op target inp view_op args view_op kwargs src = _decompose_scatter_functional_helper graph view src view_ops type ignore assignment graph_call_function graph _VIEW_OP_TO_SCATTER view_op target inp src view_op args view_op kwargs _decompose_scatter_functional graph torch fx Graph node torch fx Node - torch fx Node Decompose _generalized_scatter sequence view_scatter operations e g _generalized_scatter inp src aten slice aten slice - will become view = aten slice inp view_updated = aten slice_scatter view src - inp_updated = aten slice_scatter inp view_updated assert node target _generalized_scatter _decompose_scatter_functional_helper graph node args type ignore arg-type _decompose_scatter_mutating graph torch fx Graph node torch fx Node - torch fx Node Decompose _generalized_scatter using mutations e g _generalized_scatter inp src aten slice aten slice - will become inp_updated = aten clone inp slice = aten slice inp_updated slice = aten slice slice - slice copy_ src assert node target _generalized_scatter _inplace_generalized_scatter inp src view_ops = node args assert node kwargs node target _generalized_scatter inp = graph_call_function graph aten clone inp tmp = inp view view_ops type ignore union-attr tmp = graph_call_function graph view target tmp view args view kwargs type ignore union-attr we need set unbacked bindings could have been created view ops V fake_mode shape_env symbol_to_path = compute_unbacked_bindings V fake_mode shape_env tmp meta val tmp meta unbacked_bindings = symbol_to_path graph_call_function graph aten copy_ default tmp src inp type ignore return-value View ops whose view_scatter op lowered into mutations anyway so never pessimisation decompose _ALWAYS_MUTATING_SCATTER_OPS = OrderedSet aten as_strided default aten diagonal default scatter_always_uses_mutation node torch fx Node - bool _ _ view_ops = node args view_ops = cast Sequence torch fx node Argument view_ops any target _ALWAYS_MUTATING_SCATTER_OPS view view_ops isinstance target = getattr view target None torch _ops OpOverload should_reinplace_scatter node torch fx Node - bool Choose between mutating functional scatter decompositions Reinplacing view scatter ops can pessimising blocks fusion input output tensor computations However still profitable input output would have been realized anyway inp _src _view_ops = node args Mutating scatter ops unconditionally realize input output scatter_always_uses_mutation node True is_node_realized inp is_node_realized node type ignore arg-type True If output copied back into input forces both realized output user input inp op placeholder get_attr any type ignore union-attr user target aten copy_ default user args inp user node users True Otherwise assume fusions will make functional variants profitable False decompose_generalized_scatter graph torch fx Graph - None Replace _generalized_scatter normal aten ops node itertools chain graph find_nodes op= call_function target=_generalized_scatter graph find_nodes op= call_function target=_inplace_generalized_scatter use_mutation = node target _inplace_generalized_scatter scatter_always_uses_mutation node graph inserting_before node use_mutation new_node = _decompose_scatter_mutating graph node new_node = _decompose_scatter_functional graph node node replace_all_uses_with new_node graph erase_node node canonicalize_view_scatter_ops graph torch fx Graph - None This canonicalizes view scatter ops into generalized form defined scatter inp src views tmp = inp clone view views tmp = view tmp tmp copy_ src We also fuse consecutive view scatter ops form = scatter view src view b = scatter view which can rewritten b = scatter src view view = view b This both more efficient we only do single scatter also easier reinplace since there only one use ` ` node_to_view_base dict torch fx Node torch fx Node = node_to_view_op dict torch fx Node list ViewOp = defaultdict list handle_views node torch fx Node inp = node args node_to_view_base node = node_to_view_base get inp inp type ignore arg-type assignment node_to_view_op node = node_to_view_op inp type ignore index ViewOp node target type ignore arg-type args=node args kwargs=node kwargs handle_view_scatter node torch fx Node assert len node args = inp src = node args assert isinstance node target torch _ops OpOverload scatter_view_op = ViewOp _SCATTER_OP_TO_VIEW node target args=node args kwargs=node kwargs can_fuse src target _generalized_scatter type ignore union-attr False src_inp _src_src _src_scatter_view_op = src args type ignore union-attr inp_base = node_to_view_base get inp inp type ignore arg-type src_base = node_to_view_base get src_inp src_inp type ignore arg-type inp_base src_base node_to_view_op src_inp == type ignore index node_to_view_op inp type ignore index scatter_view_op can_fuse graph inserting_before node new_node = graph_call_function graph _generalized_scatter inp src scatter_view_op node replace_all_uses_with new_node graph erase_node node _src_inp src_src src_scatter_view_op = src args type ignore union-attr graph inserting_before src type ignore arg-type new_node = graph_call_function graph _generalized_scatter inp src_src scatter_view_op src_scatter_view_op type ignore misc node replace_all_uses_with new_node graph erase_node node src users type ignore union-attr new_src = graph_call_function graph _SCATTER_OP_TO_VIEW node target new_node node args node kwargs handle_views new_src src replace_all_uses_with new_src type ignore union-attr graph erase_node src type ignore arg-type node graph nodes _is_view_op node target handle_views node node target _SCATTER_OP_TO_VIEW handle_view_scatter node inplaceable_ops dict Callable Any InplaceableOp = aten index_put default InplaceableOp aten index_put_ default aten _unsafe_index_put default InplaceableOp inductor_prims _unsafe_index_put_ _generalized_scatter InplaceableOp _inplace_generalized_scatter extra_check=should_reinplace_scatter try c d_functional = torch ops _c d_functional inplaceable_collective_ops dict Callable Any InplaceableOp = c d_functional all_reduce default InplaceableOp c d_functional all_reduce_ default c d_functional all_reduce_coalesced default InplaceableOp c d_functional all_reduce_coalesced_ default inplaceable_ops update inplaceable_collective_ops except AttributeError _c d_functional ops only available when torch built USE_DISTRIBUTED= pass inplaceable_foreach_ops dict torch _ops OpOverload InplaceableOp = outplace_op inplace_op inplaceable_foreach_ops_lowerings items inplaceable_foreach_ops outplace_op = InplaceableOp inplace_op inplaceable_triton_ops = OrderedSet triton_kernel_wrapper_functional Operators don t depend tensor data META_ONLY_OPS = OrderedSet aten sym_size int aten sym_stride int aten sym_numel default aten sym_storage_offset default reinplace_inplaceable_ops_core graph torch fx Graph - None Reinplaces in-placeable operations If there no uses view mutated arg after current node possible inplace op This above algorithm could justified observing side effects While we traverse graph forwards direction only latter nodes could view side effects current node If current node used later well no view node used later graph then safe inplace there would no way observe side effects This condition slightly different graph inputs where they can only inplaced above condition true there s copy_ epilogue signals caller wants observe mutation Unlike JIT Inductor AOTInductor currently unlifts weights buffers input args so instead checking mutation placeholder AOTInductor checks mutation get_attr This subject change future copy_args_to_copy_nodes = maps argument first copy_ node mutates copy_nodes = mutated_inputs = OrderedSet Any storage_to_nodes = defaultdict list node_order dict Any int = i node enumerate reversed graph nodes node_order node = len graph nodes - i - storage_to_nodes get_node_storage node append node node target aten copy_ default node args op placeholder get_attr dst = node args src = node args If target getitem indexes possible clone then skip over src target operator getitem src args target == triton_kernel_wrapper_functional src args kwargs kwargs src args == node args src args target inplaceable_foreach_ops src args target torch ops higher_order auto_functionalized src = src args copy_args_to_copy_nodes dst src = node copy_nodes dst = node mutated_inputs add node args any_use_of_views_after_node node shared_view_nodes copy_node mutated_arg node_loc = node_order node copy_node_loc = node_order copy_node copy_node None None is_meta_only_user node _is_view_op node target all is_meta_only_user u u node users node target META_ONLY_OPS view shared_view_nodes user view users user_loc = node_order user Skip all users before node user_loc = node_loc continue Ignore uses after copy_ epilogue node where input has already been mutated anyway copy_node_loc None copy_node_loc = user_loc continue Reinplacing does change shape metadata is_meta_only_user user continue If our graph looks like foo mutated_arg mutated_arg copy_ other then s safe us reinplace foo because mutated_arg will get overwritten anyways user target torch ops aten copy_ default mutated_arg user args continue True False can_inplace node mutated_arg ls should list tensors all shares same storage _overlap ls - bool try len compute_overlapping_tensors ls = except GuardOnDataDependentSymNode If we fail data dependent error we assume they all overlap True isinstance mutated_arg list tuple TODO Using _overlap here causes several issues unique_storages = OrderedSet get_node_storage arg arg mutated_arg len unique_storages = len mutated_arg At least two Tensors mutated_arg alias each other so we can t reinplace We can probably do better reinplace one them clone other requires more work mutable List Tensor common False all can_inplace node arg arg mutated_arg get_node_storage mutated_arg None False shared_view_nodes = storage_to_nodes get_node_storage mutated_arg Only keep tensor might overlap mutated_arg shared_view_nodes = v v shared_view_nodes _overlap mutated_arg meta val v meta val mutated_arg op placeholder get_attr Get first copy_ node mutates mutated_arg copy_node = copy_nodes get mutated_arg copy_node None There no copy_ back candidate mutated_arg which graph input Therefore semantics program does mutate mutated_arg so we cannot re-inplace False any_use_of_views_after_node node shared_view_nodes copy_node=copy_node mutated_arg=mutated_arg False True any view op placeholder get_attr view shared_view_nodes This should never happen auto_functionalize_v non-inference mode since all mutated_arg bases If mutated arg view any inputs graph do allow inplacing This would require more sophisticated algorithm handle False any_use_of_views_after_node node shared_view_nodes copy_node=None mutated_arg=mutated_arg log_inplace_results node_name old_tensors_to_clone tensors_to_clone missed_args missed_nodes trigger Total size possibly_missed_reinplacing_opportunities tensors static shapes missed_bytes = bytes node t = node meta get val None t None isinstance t element_size int isinstance t numel int t element_size t numel node missed_nodes isinstance node list tuple n node missed_bytes += bytes n missed_bytes += bytes node log info For node s attempted reinplace s We unable reinplace s s non-empty possible missed reinplacing opportunities may bad memory usage performance Total size missed opportunities static shapes s bytes node_name old_tensors_to_clone tensors_to_clone missed_args missed_bytes ReinplaceCounters add_missed_opportunities trigger len missed_args ReinplaceCounters add_missed_bytes trigger missed_bytes replace_dict dict torch fx Node torch fx Node = reinplace_and_refine_tensors_to_clone old_tensors_to_clone kwargs node_name trigger tensors_to_clone list str = storage_of_reinplaced_args = OrderedSet int &#124; None Those used count possibly_missed_reinplacing_opportunities missed_nodes = missed_args = TODO logic can made more precise using _overlap tensor_with_same_storage_already_reinplaced arg isinstance arg list tuple any get_node_storage storage_of_reinplaced_args arg get_node_storage mutated_arg storage_of_reinplaced_args arg old_tensors_to_clone assert arg kwargs mutated_arg = kwargs arg Let s say we have - op x y mutates both x y - new_x new_y = functional_op x y functional variant If we presented functional_op x x we must reinplace into op x x because then would writing same Tensor Instead s OK reinplace one them clone other y = x clone op x y This also applies we have views functional_op x x should reinplace into op x x should_attempt_reinplace = tensor_with_same_storage_already_reinplaced mutated_arg should_attempt_reinplace can_inplace node mutated_arg In general we probably do need those optimizations copy_node = copy_args_to_copy_nodes get mutated_arg node copy_node None replace_dict copy_node = copy_node args trigger = ReInplaceTrigger AUTO_FUNC_V user node users For auto_functionalize_v arg index base where base index i corresponds output atindex size out +i This used compare string integers before auto_functionalize_v Not sure needed inplaceable_triton_ops user target operator getitem user args == arg replace_dict user = mutated_arg isinstance mutated_arg list tuple mutated_arg storage_of_reinplaced_args add get_node_storage storage_of_reinplaced_args add get_node_storage mutated_arg should_attempt_reinplace missed_args append arg missed_nodes append mutated_arg tensors_to_clone append arg log_inplace_results node_name old_tensors_to_clone tensors_to_clone missed_args missed_nodes trigger tensors_to_clone node graph nodes inplaceable_op = inplaceable_ops get node target None None mutated_arg = node args inplaceable_op mutated_arg can_inplace node mutated_arg inplaceable_op extra_check node TODO yifu doesn t properly remove copy epilogues ops mutate multiple inputs Need revise copy node tracking logic support case copy_node = copy_args_to_copy_nodes get mutated_arg node copy_node None replace_dict copy_node = copy_node args node target = inplaceable_op inplace_op node target torch ops higher_order auto_functionalized_v _mutable_op = node args kwargs = node kwargs all_bases = kwargs _all_bases bases_to_clone = range len all_bases base_tensors_dct = dict enumerate all_bases new_bases_to_clone list int = reinplace_and_refine_tensors_to_clone bases_to_clone base_tensors_dct node target ReInplaceTrigger AUTO_FUNC_V Stash metadata There pass later where we decompose auto_functionalized into clones + mutable op metadata tells decomp only clone following inputs node meta only_clone_these_tensors = new_bases_to_clone node target torch ops higher_order auto_functionalized _mutable_op = node args torch _higher_order_ops auto_functionalize get_mutable_args tensors_to_clone _ = get_mutable_args _mutable_op Don t try reinplace Tensor &#124; None args None tensors_to_clone = t t tensors_to_clone node kwargs t None tensors_to_clone = reinplace_and_refine_tensors_to_clone tensors_to_clone node kwargs _mutable_op _name ReInplaceTrigger AUTO_FUNC_V Stash metadata There pass later where we decompose auto_functionalized into clones + mutable op metadata tells decomp only clone following inputs node meta only_clone_these_tensors = tensors_to_clone node target inplaceable_triton_ops kernel_idx = node kwargs kernel_idx kernel = kernel_side_table get_kernel kernel_idx triton runtime autotuner Autotuner triton runtime jit JITFunction isinstance kernel JITFunction kernel_name = kernel fn __name__ isinstance kernel Autotuner config is_fbcode Autotuner has different implementations AMD NV torch version hip None kernel_name = kernel base_fn __name__ kernel_name = kernel fn __name__ kernel_name = kernel base_fn __name__ raise AssertionError Unknown triton kernel type inplaceable_triton_ops take additional argument called tensors_to_clone which contain list tensors clone This pass iterates over them sees which ones safe eliminate i e no longer need clones tensors_to_clone = reinplace_and_refine_tensors_to_clone node kwargs tensors_to_clone node kwargs kwargs kernel_name ReInplaceTrigger TRITON_OPS kwargs = dict node kwargs kwargs tensors_to_clone = tensors_to_clone node kwargs = immutable_dict kwargs eager_input_vals node meta We changed kwargs so we need update eager_input_vals something sane args kwargs = node meta eager_input_vals new_kwargs = kwargs new_kwargs tensors_to_clone = immutable_list tensors_to_clone new_kwargs = immutable_dict new_kwargs node meta eager_input_vals = args new_kwargs inplaceable_op = inplaceable_foreach_ops get node target None None mutated_args = node args inplaceable_op mutated_arg all arg node copy_args_to_copy_nodes arg mutated_args continue can_inplace node mutated_args arg mutated_args copy_node = copy_args_to_copy_nodes arg node replace_dict copy_node = copy_node args node target = inplaceable_op inplace_op node replacement replace_dict items while replacement replace_dict replacement = replace_dict replacement replace_dict node = replacement node replace_all_uses_with replacement graph erase_node node reinplace_inplaceable_ops fake_tensor_updater torch _inductor fx_utils FakeTensorUpdater graph torch fx Graph - None enable_python_dispatcher canonicalize_view_scatter_ops graph canonicalize_view_scatter_ops adds new operations graph We run fake_tensor_updater update alias information Correct alias information required ` reinplace_inplaceable_ops_core ` fake_tensor_updater incremental_update reinplace_inplaceable_ops_core graph decompose_generalized_scatter graph