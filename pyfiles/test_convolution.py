Owner s module nn itertools math os unittest warnings itertools product torch torch autograd forward_ad fwAD torch backends cudnn cudnn torch nn nn torch nn functional F torch testing make_tensor _get_cudnn_version Safely get cuDNN version returning None unavailable try torch backends cudnn version except RuntimeError None torch testing _internal common_cuda TEST_CUDA TEST_CUDNN tf _on_and_off torch testing _internal common_device_type disablecuDNN disableMkldnn dtypes dtypesIfCUDA dtypesIfMPS expectedFailureMPS instantiate_device_type_tests largeTensorTest onlyCPU onlyCUDA onlyNativeDeviceTypes precisionOverride skipCPUIfNoMkldnn skipCUDAIfMiopen skipCUDAIfNoCudnn skipCUDAIfNoMiopen skipCUDAIfRocm skipMeta skipMPS torch testing _internal common_dtype floating_and_complex_types_and floating_types_and torch testing _internal common_nn _test_module_empty_input NNTestCase torch testing _internal common_utils download_file dtype prec_DONTUSE gradcheck GRADCHECK_NONDET_TOL gradgradcheck instantiate_parametrized_tests MACOS_VERSION MI _ARCH parametrize parametrize_test run_tests set_default_dtype skipIfRocmArch subtest TEST_SCIPY TEST_WITH_ROCM xfailIf AMPERE_OR_ROCM = TEST_WITH_ROCM torch cuda is_tf _supported TEST_WITH_ROCM os environ PYTORCH_MIOPEN_SUGGEST_NHWC = os environ PYTORCH_MIOPEN_SUGGEST_NHWC_BATCHNORM = TEST_SCIPY scipy ndimage scipy signal TestConvolutionNN NNTestCase _do_cuda_memory_leak_check = True _do_cuda_non_default_stream = True test_conv_backcompat torch serialization SourceChangeWarning This file generated running PyTorch Python torch torch nn m = nn Conv d torch save m legacy_conv d pt NB This Pickle also contains some Unicode data path = download_file https download pytorch org test_data legacy_conv d pt warnings catch_warnings warnings simplefilter ignore SourceChangeWarning weights_only=False legacy code saves model m = torch load path encoding= utf- weights_only=False input = torch randn dtype=torch float assertEqual m input size test_huge_padding Conv dModule torch nn Module __init__ super __init__ conv = nn Conv d in_channels= out_channels= kernel_size= stride= padding= add_module name= conv module=self conv input_data = torch randn model = Conv dModule assertRaisesRegex RuntimeError r Given padding= dimension expected padding most model conv input_data ConvTransposed dModule torch nn Module __init__ super __init__ conv_transposed d = nn ConvTranspose d in_channels= out_channels= kernel_size= stride= padding= add_module name= conv_transposed d module=self conv_transposed d input_data = torch randn model = ConvTransposed dModule assertRaisesRegex RuntimeError r Given padding= dimension expected padding most model conv_transposed d input_data test_invalid_conv d dtype torch half torch bfloat torch float torch double torch cfloat torch cdouble module = nn Conv d in_channels= out_channels= kernel_size= stride= bias=True dtype input = torch randn dtype assertRaisesRegex RuntimeError r Calculated padded input size per channel \ \ + r Kernel size \ \ Kernel size can\ t greater than actual input size module input Negative stride check module = nn Conv d in_channels= out_channels= kernel_size= stride=- bias=True dtype input = torch randn dtype assertRaisesRegex RuntimeError non-positive stride supported module input test_mismatch_shape_conv d dtype torch float torch cfloat x = torch randn dtype=dtype w = torch randn dtype=dtype assertRaisesRegex RuntimeError r Expected D \ unbatched\ D \ batched\ input conv d got + r input size \ \ F conv d x w test_conv d_discontiguous_weight dtype torch float torch cfloat Test https github com pytorch pytorch issues x = torch ones dtype=dtype weight = torch arange reshape dtype assertFalse weight is_contiguous y = torch nn functional conv d x weight None torch backends mkldnn is_available Disable MKLDNN explicitly so either NNPACK THCNN will used torch backends mkldnn flags enabled=False y_ = torch nn functional conv d x weight None assertEqual y y_ assertEqual y sum test_invalid_conv d dtype torch half torch bfloat torch float torch double torch cfloat torch cdouble module = torch nn Conv d kernel_size= dilation= stride= dtype input = torch empty dtype assertRaises RuntimeError lambda module input module = nn Conv d in_channels= out_channels= kernel_size= stride= bias=True input = torch randn assertRaisesRegex RuntimeError r Calculated padded input size per channel \ x \ + r Kernel size \ x \ Kernel size can\ t greater than actual input size module input Negative stride check module = nn Conv d in_channels= out_channels= kernel_size= stride=- bias=True dtype input = torch randn dtype assertRaisesRegex RuntimeError non-positive stride supported module input Zero stride check module = nn Conv d in_channels= out_channels= kernel_size= stride= bias=True dtype input = torch randn dtype assertRaisesRegex RuntimeError non-positive stride supported module input test_invalid_conv d dtype torch half torch bfloat torch float torch double torch cfloat torch cdouble module = torch nn Conv d kernel_size= dilation= stride= dtype input = torch empty dtype assertRaises RuntimeError lambda module input Negative stride check module = torch nn Conv d kernel_size= stride=- input = torch empty assertRaisesRegex RuntimeError non-positive stride supported module input test_conv_invalid_groups assertRaisesRegex ValueError groups must positive integer torch nn Conv d kernel_size= dilation= stride= groups= assertRaisesRegex ValueError groups must positive integer torch nn Conv d kernel_size= dilation= stride= groups=- assertRaisesRegex ValueError groups must positive integer torch nn Conv d kernel_size= dilation= stride= groups=- test_conv_aten_invalid_groups test low-level aten ops invalid groups parameter grad_output = torch randn dtype=torch double input = torch randn dtype=torch double weight = torch randn dtype=torch double bias_sizes = stride = padding = dilation = transposed = True output_padding = output_mask = True True True test groups= assertRaisesRegex RuntimeError expected groups greater than got groups= torch ops aten convolution_backward grad_output input weight bias_sizes stride padding dilation transposed output_padding output_mask test groups=- assertRaisesRegex RuntimeError expected groups greater than got groups=- torch ops aten convolution_backward grad_output input weight bias_sizes stride padding dilation transposed output_padding - output_mask test_conv d_overflow_values input = torch full dtype=torch float requires_grad=False weight = torch full e dtype=torch float requires_grad=False stride = assertRaisesRegex ValueError Padding height too large torch ops aten slow_conv d input weight kernel_size= bias=None stride=stride padding= assertRaisesRegex RuntimeError Kernel height x width product too large torch ops aten slow_conv d input weight kernel_size= bias=None stride=stride padding= test_Conv d_module_same_padding Compare module against functional without strides dilation asymmetric padding x = torch rand module = nn Conv d in_channels= out_channels= kernel_size= padding= same expect = F conv d x module weight module bias padding= same assertEqual expect module x Test dilation symmetric padding module = nn Conv d in_channels= out_channels= kernel_size= padding= same dilation= expect = F conv d x module weight module bias padding= same dilation= assertEqual expect module x Test non-zero padding_mode requiring explicit padding module = nn Conv d in_channels= out_channels= kernel_size= padding= same padding_mode= replicate x_padded = F pad x mode= replicate expect = F conv d x_padded module weight module bias padding= valid assertEqual expect module x assertEqual x size expect size Test connstruction invalid padding string raises assertRaisesRegex ValueError Invalid padding string module = nn Conv d in_channels= out_channels= kernel_size= padding= foo Test connstruction same padding strides raises assertRaisesRegex ValueError padding= same module = nn Conv d in_channels= out_channels= kernel_size= padding= same stride= test_Conv d_module_same_padding Compare module against functional without strides dilation both symmetric asymmetric padding x = torch rand module = nn Conv d in_channels= out_channels= kernel_size= padding= same expect = F conv d x module weight module bias padding= same assertEqual expect module x dilation symmetric padding module = nn Conv d in_channels= out_channels= kernel_size= padding= same dilation= expect = F conv d x module weight module bias padding= same dilation= assertEqual expect module x Test non-zero padding_mode requiring explicit padding module = nn Conv d in_channels= out_channels= kernel_size= padding= same padding_mode= reflect x_padded = F pad x mode= reflect expect = F conv d x_padded module weight module bias padding= valid assertEqual expect module x assertEqual x size expect size Test connstruction invalid padding string raises assertRaisesRegex ValueError Invalid padding string module = nn Conv d in_channels= out_channels= kernel_size= padding= foo Test connstruction same padding strides raises assertRaisesRegex ValueError padding= same module = nn Conv d in_channels= out_channels= kernel_size= padding= same stride= assertRaisesRegex ValueError padding= same module = nn Conv d in_channels= out_channels= kernel_size= padding= same stride= assertRaisesRegex ValueError padding= same module = nn Conv d in_channels= out_channels= kernel_size= padding= same stride= test_Conv d_module_same_padding Compare module against functional x = torch rand without dilation both symmetric asymmetric padding module = nn Conv d in_channels= out_channels= kernel_size= padding= same expect = F conv d x module weight module bias padding= same assertEqual expect module x dilation both symmetric asymmetric padding module = nn Conv d in_channels= out_channels= kernel_size= padding= same dilation= expect = F conv d x module weight module bias padding= same dilation= assertEqual expect module x Test non-zero padding_mode requiring explicit padding module = nn Conv d in_channels= out_channels= kernel_size= padding= same padding_mode= circular x_padded = F pad x mode= circular expect = F conv d x_padded module weight module bias padding= valid assertEqual expect module x assertEqual x size expect size Test connstruction invalid padding string raises assertRaisesRegex ValueError Invalid padding string module = nn Conv d in_channels= out_channels= kernel_size= padding= foo Test connstruction same padding strides raises assertRaisesRegex ValueError padding= same module = nn Conv d in_channels= out_channels= kernel_size= padding= same stride= assertRaisesRegex ValueError padding= same module = nn Conv d in_channels= out_channels= kernel_size= padding= same stride= assertRaisesRegex ValueError padding= same module = nn Conv d in_channels= out_channels= kernel_size= padding= same stride= assertRaisesRegex ValueError padding= same module = nn Conv d in_channels= out_channels= kernel_size= padding= same stride= unittest skipIf TEST_CUDA CUDA available test_thnn_conv_strided_padded_dilated convfn dims transposed torch nn functional conv d False torch nn functional conv_transpose d True torch nn functional conv d False torch nn functional conv_transpose d True stride padding dilation kwargs = stride stride padding padding dilation dilation inp_shape = + dims weight_shape = + dims inputs = torch randn inp_shape dtype=torch double device= cuda requires_grad=True weight = torch randn weight_shape dtype=torch double device= cuda requires_grad=True bias = torch randn dtype=torch double device= cuda requires_grad=True torch backends cudnn flags enabled=False res = convfn inputs weight bias kwargs res_cpu = convfn inputs cpu weight cpu bias cpu kwargs assertEqual res res_cpu torch backends cudnn flags enabled=False torch autograd gradcheck lambda x w b convfn x w b kwargs inputs weight bias torch autograd gradcheck lambda x w b convfn x w b kwargs inputs cpu weight cpu bias cpu test_Conv d_inconsistent_types inputs = torch randn dtype=torch float weights = torch randn dtype=torch double inconsistent types should raise exception assertRaises RuntimeError lambda nn functional conv d inputs weights should work same type nn functional conv d inputs float weights float unittest skipIf TEST_CUDA CUDA available test_Conv d_inconsistent_types_on_GPU_without_cudnn inputs = torch randn dtype=torch float device= cuda weights = torch randn dtype=torch double device= cuda bias = torch randn dtype=torch double device= cuda torch backends cudnn flags enabled=False inconsistent types should raise exception assertRaises RuntimeError lambda nn functional conv d inputs weights assertRaises RuntimeError lambda nn functional conv d inputs weights float bias should work same type nn functional conv d inputs float weights float bias float test_Conv d_ x in_channels = mod = torch nn Conv d bias=False dtype=torch double input = torch randn in_channels requires_grad=True dtype=torch double enabled False True torch backends mkldnn flags enabled=enabled gradcheck F conv d input mod weight test_Conv d_OneDNN run_once group_val= dilation= ifm = torch ones group_val dtype=torch float weights = torch ones group_val dtype=torch float op = torch nn Conv d in_channels=group_val out_channels=group_val kernel_size= stride= padding= dilation= dilation dilation groups=group_val bias=False padding_mode= zeros op weight data = weights res = op ifm grad_in = torch ones res shape dtype=torch float res backward grad_in op weight grad gorup_val dilation torch backends mkldnn flags enabled=False without_onednn = run_once gorup_val dilation torch backends mkldnn flags enabled=True with_onednn = run_once gorup_val dilation assertEqual without_onednn with_onednn unittest skipIf TEST_CUDA CUDA available unittest skipIf TEST_CUDNN CUDNN available test_cudnn_non_contiguous x = torch randn cuda x = x permute contiguous permute m = torch nn Conv d in_channels= out_channels= kernel_size= bias=True cuda m x unittest skipIf TEST_CUDA CUDA available unittest skipIf TEST_CUDNN CUDNN available test_cudnn_not_mutate_stride weight = torch randn x = torch randn memory_format=torch channels_last weight_stride = weight stride conv x weight torch convolution x weight stride= padding= dilation= transposed=False output_padding= groups= bias=None should have run nhwc without mutating input strides out_nhwc = conv x weight assertEqual weight stride weight_stride assertTrue out_nhwc is_contiguous memory_format=torch channels_last x = x contiguous memory_format=torch contiguous_format out_c = conv x weight assertTrue out_c is_contiguous memory_format=torch contiguous_format assertEqual out_c out_nhwc assertEqual weight stride weight_stride unittest skipIf TEST_CUDA CUDA available unittest skipIf TEST_CUDNN CUDNN available test_Conv d_inconsistent_types_on_GPU_with_cudnn inputs = torch randn dtype=torch float device= cuda weights = torch randn dtype=torch double device= cuda bias = torch randn dtype=torch double device= cuda torch backends cudnn flags enabled=True inconsistent types should raise exception assertRaises RuntimeError lambda nn functional conv d inputs weights assertRaises RuntimeError lambda nn functional conv d inputs weights float bias should work same type nn functional conv d inputs float weights float bias float test_Conv d_missing_argument c = nn Conv d assertRaises TypeError lambda c None test_Conv d_backward_twice input = torch randn c = nn Conv d o = c input o sum backward assertRaisesRegex RuntimeError Specify retain_graph=True lambda o sum backward test_conv_modules_raise_error_on_incorrect_input_size dtype torch half torch bfloat torch double torch float modules = nn Conv d dtype nn ConvTranspose d dtype nn Conv d dtype nn ConvTranspose d dtype nn Conv d dtype nn ConvTranspose d dtype invalid_input_dims = invalid_dims module zip invalid_input_dims modules dims invalid_dims input = torch empty torch Size dims assertRaises RuntimeError lambda module input test_conv_shapecheck test should_raise module input_size dtype input = torch empty input_size dtype should_raise assertRaises RuntimeError lambda module input just run ensure no exception raised module input dtype torch half torch bfloat torch float torch double torch cfloat torch cdouble Conv d test True nn Conv d dtype dtype test True nn Conv d stride= dtype dtype test False nn Conv d dtype dtype test False nn Conv d stride= dtype dtype test False nn Conv d stride= padding= dtype dtype Conv d test True nn Conv d dtype dtype test False nn Conv d dtype dtype test False nn Conv d padding= dtype dtype Conv D test True nn Conv d dtype dtype test False nn Conv d dtype dtype test False nn Conv d padding= dtype dtype test_ConvTranspose d_output_size m = nn ConvTranspose d i = torch randn h range w range = h = = w = output = m i output_size= h w assertEqual output size h w assertRaises ValueError lambda m i h w test_ConvTranspose d_output_size_downsample_upsample b c hid_c = h range w range k range d range s range p range conv = nn Conv d in_channels=c out_channels=hid_c kernel_size=k stride=s padding=p dilation=d t_conv = nn ConvTranspose d in_channels=hid_c out_channels=c kernel_size=k stride=s padding=p dilation=d i = torch randn b c h w out = t_conv conv i output_size=i shape assertEqual out size i size test_ConvTranspose d_correct_output_size Check ConvTranspose d can take d output_size m = nn ConvTranspose d i = torch rand m i output_size= unittest skipIf TEST_CUDA CUDA available test_ConvTranspose d_half_cublas_gemm torch backends cudnn flags enabled=False inputs = torch randn device= cuda dtype=torch half deconv = nn ConvTranspose d stride= padding= output_padding= cuda half output = deconv inputs output mean backward For https github com pytorch pytorch pull Almost identical above ` test_Conv d_naive_groups ` torch backends cudnn flags enabled=True deterministic=True benchmark=False torch backends miopen flags immediate=True tf _on_and_off test_Conv d_groups_nobias dev_dtypes = cpu torch float TEST_CUDA dev_dtypes += cuda torch float cuda torch half AMPERE_OR_ROCM dev_dtypes += cuda torch bfloat device dtype dev_dtypes m = nn Conv d kernel_size= groups= bias=False device dtype i = torch randn device=device dtype=dtype requires_grad=True output = m i grad_output = torch randn device=device dtype=dtype output backward grad_output m = nn Conv d kernel_size= bias=False device dtype m weight data copy_ m weight data i = i data contiguous requires_grad_ True output = m i output backward grad_output contiguous m = nn Conv d kernel_size= bias=False device dtype m weight data copy_ m weight data i = i data contiguous requires_grad_ True output = m i output backward grad_output contiguous assertEqual output torch cat output output assertEqual i grad data torch cat i grad data i grad data atol=dtype prec_DONTUSE dtype rtol= assertEqual m weight grad data torch cat m weight grad data m weight grad data atol= e- dtype == torch half dtype prec_DONTUSE dtype rtol= Almost identical above ` test_Conv d_naive_groups ` Covering special case when group input-channel group output-channel multiple See also https github com pytorch pytorch pull #issuecomment- https github com pytorch pytorch pull #issuecomment- torch backends cudnn flags enabled=True deterministic=True benchmark=False torch backends miopen flags immediate=True tf _on_and_off test_Conv d_groups_nobias_v torch manual_seed dev_dtypes = cpu torch float TEST_CUDA dev_dtypes += cuda torch float cuda torch half AMPERE_OR_ROCM dev_dtypes += cuda torch bfloat device dtype dev_dtypes m = nn Conv d kernel_size= groups= bias=False device dtype i = torch randn device=device dtype=dtype requires_grad=True output = m i grad_output = torch randn device=device dtype=dtype output backward grad_output m = nn Conv d kernel_size= bias=False device dtype m weight data copy_ m weight data i = i data contiguous requires_grad_ True output = m i output backward grad_output contiguous m = nn Conv d kernel_size= bias=False device dtype m weight data copy_ m weight data i = i data contiguous requires_grad_ True output = m i output backward grad_output contiguous assertEqual output torch cat output output assertEqual i grad data torch cat i grad data i grad data atol=dtype prec_DONTUSE dtype rtol= assertEqual m weight grad data torch cat m weight grad data m weight grad data atol= e- dtype == torch half dtype prec_DONTUSE dtype rtol= CPU-only test group conv d fast implementation using bmm See https github com pytorch pytorch pull test_Conv d_groups_nobias torch manual_seed m = nn Conv d kernel_size= groups= bias=False cpu torch float i = torch randn device= cpu dtype=torch float requires_grad=True output = m i grad_output = torch randn device= cpu dtype=torch float output backward grad_output m = nn Conv d kernel_size= bias=False cpu torch float m weight data copy_ m weight data i = i data contiguous requires_grad_ True output = m i output backward grad_output contiguous m = nn Conv d kernel_size= bias=False cpu torch float m weight data copy_ m weight data i = i data contiguous requires_grad_ True output = m i output backward grad_output contiguous assertEqual output torch cat output output assertEqual i grad data torch cat i grad data i grad data atol=dtype prec_DONTUSE torch float rtol= assertEqual m weight grad data torch cat m weight grad data m weight grad data atol=dtype prec_DONTUSE torch float rtol=dtype prec_DONTUSE torch float test_Conv d_groups_wbias torch manual_seed m = nn Conv d kernel_size= groups= bias=True cpu torch float i = torch randn device= cpu dtype=torch float requires_grad=True output = m i grad_output = torch randn device= cpu dtype=torch float output backward grad_output m = nn Conv d kernel_size= bias=True cpu torch float m weight data copy_ m weight data m bias data copy_ m bias data i = i data contiguous requires_grad_ True output = m i output backward grad_output contiguous m = nn Conv d kernel_size= bias=True cpu torch float m weight data copy_ m weight data m bias data copy_ m bias data i = i data contiguous requires_grad_ True output = m i output backward grad_output contiguous assertEqual output torch cat output output assertEqual i grad data torch cat i grad data i grad data atol=dtype prec_DONTUSE torch float rtol=dtype prec_DONTUSE torch float assertEqual m weight grad data torch cat m weight grad data m weight grad data atol=dtype prec_DONTUSE torch float rtol=dtype prec_DONTUSE torch float assertEqual m bias grad data torch cat m bias grad data m bias grad data atol=dtype prec_DONTUSE torch float rtol=dtype prec_DONTUSE torch float test_conv_tbc set_default_dtype torch double inp = torch randn requires_grad=True weight = torch randn requires_grad=True bias = torch randn requires_grad=True gradcheck lambda i w b pad F conv_tbc i w b pad inp weight bias unittest skipIf TEST_CUDA CUDA unavailable unittest skipIf TEST_CUDNN needs cudnn test_grouped_conv_cudnn_nhwc_support order catch hols grouped convolution nhwc support earlier cudnn version input = torch randn dtype=torch float device= cuda memory_format=torch channels_last weight = torch randn dtype=torch float device= cuda memory_format=torch channels_last torch convolution input weight None False input = torch randn dtype=torch float device= cuda memory_format=torch channels_last torch convolution input weight None True unittest expectedFailure unittest skipIf TEST_CUDA CUDA unavailable unittest skipIf TEST_CUDNN needs cudnn test_conv_cudnn_memory_layout_dominance desired behavior here have memory_layout conv weight dominant layout output which same current behavior we ll fix following up PRs remove ` expectedFailure ` tag input = torch randint dtype=torch float device= cuda requires_grad=True conv = nn Conv d cuda float out = conv input assertTrue out is_contiguous input = input contiguous memory_format=torch channels_last out = conv input assertTrue out is_contiguous conv weight data = conv weight contiguous memory_format=torch channels_last out = conv input assertTrue out is_contiguous memory_format=torch channels_last input = input contiguous out = conv input assertTrue out is_contiguous memory_format=torch channels_last unittest skipIf TEST_CUDA CUDA unavailable test_cudnn_noncontiguous_weight Noncontiguous weights must contiguous before being passed cuDNN input = torch tensor dtype=torch double device= cuda view weights = torch tensor dtype=torch double device= cuda expand weights = torch tensor dtype=torch double device= cuda expand contiguous assertEqual F conv d input weights bias=None stride= dilation= F conv d input weights bias=None stride= dilation= run_grad_conv_test func_forward func_backward dim= gradient= input kern inp_size batch stride padding chan_in chan_out dilation product has_bias True False input_shape = batch chan_in weight_shape = chan_out chan_in _ range dim input_shape append inp_size weight_shape append kern input = torch randn input_shape requires_grad=True weight = torch randn weight_shape requires_grad=True has_bias bias = torch randn chan_out requires_grad=True output = func_forward input weight stride=stride padding=padding dilation=dilation bias=bias gradient_o = torch randn output shape gradient_w = torch autograd grad output input gradient == input weight gradient_o assertEqual gradient_w func_backward input_shape gradient == input input weight_shape gradient == weight weight gradient_o stride=stride padding=padding dilation=dilation test_grad_conv d_input run_grad_conv_test F conv d F grad conv d_input input test_grad_conv d_weight run_grad_conv_test F conv d F grad conv d_weight weight test_grad_conv d_input run_grad_conv_test F conv d F grad conv d_input input test_grad_conv d_weight run_grad_conv_test F conv d F grad conv d_weight weight test_grad_conv d_input run_grad_conv_test F conv d F grad conv d_input input test_grad_conv d_weight run_grad_conv_test F conv d F grad conv d_weight weight unittest skipIf torch _nnpack_available NNPACK unavailable test_nnpack_conv kern inp_size batch stride padding chan_in chan_out product has_bias True False input_shape = batch chan_in weight_shape = chan_out chan_in _ range input_shape append inp_size weight_shape append kern input = torch randn input_shape requires_grad=True dtype=torch float weight = torch randn weight_shape requires_grad=True dtype=torch float has_bias bias = torch randn chan_out requires_grad=True dtype=torch float output = torch _nnpack_spatial_convolution input weight stride=stride padding=padding bias=bias output_expected = torch nn functional conv d input weight stride=stride padding=padding bias=bias assertEqual output output_expected atol= e- rtol= gradient_o = torch randn output shape dtype=torch float grads = torch autograd grad output input weight gradient_o grads_expected = torch autograd grad output_expected input weight gradient_o gr gr_expected zip grads grads_expected assertEqual gr gr_expected atol= e- rtol= test_conv_padding_mode assertRaisesRegex ValueError padding_mode must one nn Conv d padding_mode= xyz assertRaisesRegex ValueError padding_mode must one nn Conv d padding_mode= assertRaisesRegex ValueError Only zeros nn ConvTranspose d padding_mode= reflect test_functional_grad_conv Conv D input = torch randn requires_grad=True weight = torch randn requires_grad=True output = F conv d input weight dilation= grad_output = torch randn output shape grad_input_autograd grad_weight_autograd = torch autograd grad output input weight grad_output grad_input_functional = torch nn grad conv d_input input shape weight grad_output dilation= assertEqual grad_input_functional grad_input_autograd grad_weight_functional = torch nn grad conv d_weight input weight shape grad_output dilation= assertEqual grad_weight_functional grad_weight_autograd Conv D input = torch randn requires_grad=True weight = torch randn requires_grad=True output = F conv d input weight dilation= grad_output = torch randn output shape grad_input_autograd grad_weight_autograd = torch autograd grad output input weight grad_output grad_input_functional = torch nn grad conv d_input input shape weight grad_output dilation= assertEqual grad_input_functional grad_input_autograd grad_weight_functional = torch nn grad conv d_weight input weight shape grad_output dilation= assertEqual grad_weight_functional grad_weight_autograd Conv D input = torch randn requires_grad=True weight = torch randn requires_grad=True output = F conv d input weight dilation= grad_output = torch randn output shape grad_input_autograd grad_weight_autograd = torch autograd grad output input weight grad_output grad_input_functional = torch nn grad conv d_input input shape weight grad_output dilation= assertEqual grad_input_functional grad_input_autograd grad_weight_functional = torch nn grad conv d_weight input weight shape grad_output dilation= assertEqual grad_weight_functional grad_weight_autograd test_functional_grad_conv d BATCH_SIZE = IN_CH = OUT_CH = SPATIAL = _test_conv d stride kernel_size groups dilation padding = kernel_size input = torch empty BATCH_SIZE IN_CH SPATIAL SPATIAL uniform_ - requires_grad_ True weight = torch empty OUT_CH IN_CH groups kernel_size kernel_size uniform_ - requires_grad_ True output = F conv d input weight stride=stride padding=padding dilation=dilation groups=groups grad_output = torch randn output shape grad_input_autograd grad_weight_autograd = torch autograd grad output input weight grad_output grad_input_functional = torch nn grad conv d_input input shape weight grad_output stride=stride padding=padding dilation=dilation groups=groups assertEqual grad_input_functional grad_input_autograd grad_weight_functional = torch nn grad conv d_weight input weight shape grad_output stride=stride padding=padding dilation=dilation groups=groups assertEqual grad_weight_functional grad_weight_autograd strides = kernel_sizes = groups = dilates = s k g d product strides kernel_sizes groups dilates _test_conv d s k g d test_permute_conv d_issue_ reproducer radius int image = torch rand image = image permute kernel_x = torch zeros radius + device=image device image = torch nn functional conv d image kernel_x groups=image shape - i range This should fail reproducer radius=i test_conv d_issue_ This should fail F conv d torch ones torch ones groups= test_conv d_issue_ weight = torch ones bias = torch ones stride padding dilation groups = input = torch rand input = input transpose This should fail F conv d input weight bias stride padding dilation groups TestConvolutionNNDeviceType NNTestCase run_conv_double_back_test kern stride padding chan_in chan_out batch_size inp_size dilation no_weight groups= use_cuda=False use_bias=True dtype=torch double use_cuda device = torch device cuda device = torch device cpu x = torch randn batch_size chan_in inp_size inp_size device=device dtype=dtype requires_grad=True weight = torch randn chan_out chan_in groups kern kern device=device dtype=dtype requires_grad=not no_weight use_bias bias = torch randn chan_out device=device dtype=dtype requires_grad=True bias = None func inputs use_bias lx lweight lbias = inputs lx lweight = inputs lbias = None We disable cudnn during forward avoid finite difference imprecision issues cudnn flags enabled=False out = F conv d lx lweight lbias stride padding dilation groups out use_bias inputs = x weight bias inputs = x weight dummy_out = func inputs grad_y = torch randn_like dummy_out device=device dtype=dtype requires_grad=True Issue test mkldnn double backward don t run gradgradcheck due imprecision issues dtype == torch float g = torch autograd grad dummy_out sum x create_graph=True g requires_grad gradgradcheck func inputs grad_y onlyCUDA skipCUDAIfNoCudnn dtypes floating_and_complex_types_and torch half torch bfloat AMPERE_OR_ROCM parametrize_test dilation test_Conv d_deterministic_cudnn device dtype dilation inputs = torch randn device=device dtype=dtype requires_grad=True cudnn flags enabled=True benchmark=True deterministic=True conv = torch nn Conv d dilation=dilation device dtype conv = torch nn Conv d dilation=dilation device dtype conv bias data copy_ conv bias data conv weight data copy_ conv weight data out = conv inputs out = conv inputs assertEqual out out atol= rtol= y = torch randn out size device=device dtype=dtype out backward y out backward y assertEqual conv bias grad data conv bias grad data atol= rtol= assertEqual conv weight grad data conv weight grad data atol= rtol= onlyCUDA dtypes floating_types_and torch half torch bfloat AMPERE_OR_ROCM test_Conv d_large_workspace device dtype These sizes require huge cuDNN workspaces Make sure we choose reasonable algorithm does run out memory sizes = run_test benchmark torch backends cudnn flags enabled=True benchmark=benchmark conv = torch nn Conv d kernel_size= padding= device dtype size sizes x = torch randn size device=device dtype=dtype out = conv x detach clone requires_grad_ out backward torch ones_like out run_test benchmark=False run_test benchmark=True onlyCUDA dtypes torch half torch float test_ConvTranspose d_large_output_padding device dtype net = torch nn ConvTranspose d kernel_size= stride= padding= output_padding= device=device dtype=dtype net = torch nn ConvTranspose d kernel_size= stride= padding= output_padding= device=device dtype=dtype net = torch nn ConvTranspose d kernel_size= stride= padding= output_padding= device=device dtype=dtype x = torch rand device=device dtype=dtype requires_grad=True x = net x x = net x x = net x x backward torch randn_like x torch cuda synchronize onlyCUDA dtypes torch float torch double torch half Very similar test_Conv d_naive_groups special care handle number groups == number input channels torch backends cudnn flags enabled=True deterministic=True benchmark=False torch backends miopen flags immediate=True tf _on_and_off test_Conv d_depthwise_naive_groups device dtype depth_multiplier m = nn Conv d depth_multiplier kernel_size= groups= device dtype i = torch randn device= cuda dtype=dtype div_ requires_grad_ output = m i grad_output = torch randn depth_multiplier device=device dtype=dtype output backward grad_output offset = depth_multiplier m = nn Conv d depth_multiplier kernel_size= device dtype m weight data = m weight data offset clone m bias data = m bias data offset clone i = i detach clone requires_grad_ output = m i output backward grad_output offset contiguous m = nn Conv d depth_multiplier kernel_size= device dtype m weight data copy_ m weight data offset m bias data copy_ m bias data offset i = i detach clone requires_grad_ output = m i output backward grad_output offset contiguous assertEqual output torch cat output output atol=dtype prec_DONTUSE dtype rtol= assertEqual i grad data torch cat i grad data i grad data atol=dtype prec_DONTUSE dtype rtol= assertEqual m bias grad data torch cat m bias grad data m bias grad data atol=dtype prec_DONTUSE dtype rtol= assertEqual m weight grad data torch cat m weight grad data m weight grad data atol=dtype prec_DONTUSE dtype rtol= onlyCUDA dtypes torch float torch double torch half torch backends cudnn flags enabled=True deterministic=True benchmark=False torch backends miopen flags immediate=True tf _on_and_off test_Conv d_depthwise_naive_groups device dtype depth_multiplier m = nn Conv d depth_multiplier kernel_size= groups= device dtype i = torch randn device= cuda dtype=dtype div_ requires_grad_ output = m i grad_output = torch randn depth_multiplier device=device dtype=dtype output backward grad_output offset = depth_multiplier m = nn Conv d depth_multiplier kernel_size= device dtype m weight data = m weight data offset clone m bias data = m bias data offset clone i = i detach clone requires_grad_ output = m i output backward grad_output offset contiguous m = nn Conv d depth_multiplier kernel_size= device dtype m weight data copy_ m weight data offset m bias data copy_ m bias data offset i = i detach clone requires_grad_ output = m i output backward grad_output offset contiguous is_cuda_sm = device startswith cuda torch cuda get_device_capability == atol rtol = e- e- dtype == torch float is_cuda_sm dtype prec_DONTUSE dtype assertEqual output torch cat output output atol=atol rtol=rtol assertEqual i grad data torch cat i grad data i grad data atol=dtype prec_DONTUSE dtype rtol= assertEqual m bias grad data torch cat m bias grad data m bias grad data atol=dtype prec_DONTUSE dtype rtol= assertEqual m weight grad data torch cat m weight grad data m weight grad data atol=atol rtol=rtol onlyCUDA dtypes floating_types_and torch half torch bfloat AMPERE_OR_ROCM test_noncontig_conv_grad device dtype FIXME remove after adding non-contiguous grad tests all modules module = nn Conv d kernel_size= padding= device dtype input = torch randn dtype=dtype device=device requires_grad=True output = module input grad = torch randn dtype=dtype device=device assert grad is_contiguous output backward grad retain_graph=True assertIsNotNone input grad result = input grad data clone input grad data zero_ output backward grad contiguous assertEqual result input grad data atol=dtype prec_DONTUSE dtype rtol= onlyCUDA dtypes torch double torch backends cudnn flags enabled=True deterministic=True benchmark=False torch backends miopen flags immediate=True test_conv_double_backward device dtype Double backward only runs DoubleTensor due precision reason batch_size = kern inp_size dilations stride padding chan_in chan_out dilation product dilations no_weight = stride == result = run_conv_double_back_test kern stride padding chan_in chan_out batch_size inp_size dilation no_weight use_cuda=True dtype=dtype assertTrue result Conv double backward test failed parameters + \nkern + str kern + \nstride + str stride + \npadding + str padding + \nchan_in + str chan_in + \nchan_out + str chan_out + \nbatch_size + str batch_size + \ninp_size + str inp_size + \ndilation + str dilation test_conv_double_backward_no_bias kern = stride = chan_in chan_out = batch_size = inp_size = padding = dilation = no_weight = False use_bias = True result = run_conv_double_back_test kern stride padding chan_in chan_out batch_size inp_size dilation no_weight use_bias=use_bias assertTrue result Conv double backward test failed parameters + \nkern + str kern + \nstride + str stride + \npadding + str padding + \nchan_in + str chan_in + \nchan_out + str chan_out + \nbatch_size + str batch_size + \ninp_size + str inp_size + \ndilation + str dilation test_conv_double_backward_groups kern = stride = padding = chan_in chan_out = batch_size = inp_size = dilation = no_weight = False groups = result = run_conv_double_back_test kern stride padding chan_in groups chan_out groups batch_size inp_size dilation no_weight groups=groups assertTrue result Conv double backward test failed parameters + \nkern + str kern + \nstride + str stride + \npadding + str padding + \nchan_in + str chan_in + \nchan_out + str chan_out + \nbatch_size + str batch_size + \ninp_size + str inp_size + \ndilation + str dilation + \ngroups + str groups test_conv_double_backward_stride batch_size = Cannot provide ggW when stride kern inp_size dilations stride padding chan_in chan_out dilation product dilations no_weight = False run_conv_double_back_test kern stride padding chan_in chan_out batch_size inp_size dilation no_weight dtypes torch float torch cfloat dtypesIfMPS torch float MACOS_VERSION torch float torch cfloat Complex supported MacOS torch backends cudnn flags enabled=True deterministic=True benchmark=False torch backends miopen flags immediate=True test_conv d_same_padding device dtype Test padding= same outputs correct shape test_args = in_size range kernel_size dilation range stride in_size k_size dilation stride itertools product test_args x = torch rand in_size device=device dtype=dtype y = torch rand k_size device=device dtype=dtype z = F conv d x y padding= same dilation=dilation stride=stride assertEqual z size int math ceil in_size stride Compare F conv d padding= same output against manual padding Without strides dilation x = torch rand device=device dtype=dtype y = torch rand device=device dtype=dtype expect = F conv d x y padding= actual = F conv d x y padding= same assertEqual expect actual With dilation x = torch rand device=device dtype=dtype y = torch rand device=device dtype=dtype expect = F conv d x y padding= dilation= actual = F conv d x y padding= same dilation= assertEqual expect actual Dilation asymmetric padding expect = F conv d x y padding= dilation= actual = F conv d x y padding= same dilation= assertEqual expect actual tf _on_and_off dtypesIfMPS torch float MACOS_VERSION torch float torch cfloat Complex supported MacOS dtypes torch float torch cfloat test_conv d_same_padding device dtype Compare F conv d padding= same output against manual padding Without strides dilation x = torch rand device=device dtype=dtype y = torch rand device=device dtype=dtype expect = F conv d x y padding= actual = F conv d x y padding= same assertEqual expect actual With dilation y = torch rand device=device dtype=dtype expect = F conv d x y padding= dilation= actual = F conv d x y padding= same dilation= assertEqual expect actual Dilation asymmetric padding y = torch rand device=device dtype=dtype expect = F conv d x y padding= dilation= actual = F conv d x y padding= same dilation= assertEqual expect actual dtypes torch float torch cfloat test_conv d_same_padding device dtype dtype torch cfloat rtol atol = e- e- rtol atol = None None Compare F conv d padding= same output against manual padding Without strides dilation x = torch rand device=device dtype=dtype y = torch rand device=device dtype=dtype expect = F conv d x y padding= actual = F conv d x y padding= same assertEqual expect actual rtol=rtol atol=atol With dilation expect = F conv d x y padding= dilation= actual = F conv d x y padding= same dilation= assertEqual expect actual rtol=rtol atol=atol Dilation asymmetric padding y = torch rand device=device dtype=dtype expect = F conv d x y padding= dilation= actual = F conv d x y padding= same dilation= assertEqual expect actual rtol=rtol atol=atol dtypes torch float torch cfloat dtypesIfMPS torch float MACOS_VERSION torch float torch cfloat Complex supported MacOS test_conv d_valid_padding device dtype Test F conv d padding= valid same no padding x = torch rand device=device dtype=dtype y = torch rand device=device dtype=dtype expect = F conv d x y actual = F conv d x y padding= valid assertEqual expect actual dtypes torch float torch cfloat dtypesIfMPS torch float MACOS_VERSION torch float torch cfloat Complex supported MacOS test_conv d_valid_padding device dtype Test F conv d padding= valid same no padding x = torch rand device=device dtype=dtype y = torch rand device=device dtype=dtype expect = F conv d x y actual = F conv d x y padding= valid assertEqual expect actual dtypes torch float torch cfloat test_conv d_valid_padding device dtype Test F conv d padding= valid same no padding x = torch rand dtype=dtype device=device y = torch rand dtype=dtype device=device expect = F conv d x y actual = F conv d x y padding= valid assertEqual expect actual dtypes torch float torch cfloat dtypesIfMPS torch float MACOS_VERSION torch float torch cfloat Complex supported MacOS test_conv d_same_padding_backward device dtype Test F conv d gradients work padding= same x = torch rand dtype=dtype device=device requires_grad=True y = torch rand dtype=dtype device=device requires_grad=True Symmetric padding z = F conv d x y padding= dilation= z sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None z = F conv d x y padding= same dilation= z sum abs backward assertEqual gx_expect x grad assertEqual gy_expect y grad x grad y grad = None None Asymmetric padding z = F conv d x y padding= z sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None z = F conv d x y padding= same z sum abs backward assertEqual gx_expect x grad assertEqual gy_expect y grad dtypes torch float torch cfloat dtypesIfMPS torch float MACOS_VERSION torch float torch cfloat Complex supported MacOS tf _on_and_off test_conv d_same_padding_backward device dtype Test F conv d gradients work padding= same x = torch rand device=device dtype=dtype requires_grad=True y = torch rand device=device dtype=dtype requires_grad=True Symmetric padding z = F conv d x y padding= dilation= z sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None z = F conv d x y padding= same dilation= z sum abs backward assertEqual gx_expect x grad assertEqual gy_expect y grad x grad y grad = None None Asymmetric padding y = torch rand device=device dtype=dtype requires_grad=True z = F conv d x y padding= z sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None z = F conv d x y padding= same z sum abs backward assertEqual gx_expect x grad assertEqual gy_expect y grad dtypes torch double torch cdouble dtypesIfMPS torch float torch cfloat Double complex double supported MPS expectedFailureMPS https github com pytorch pytorch issues test_conv d_same_padding_backward device dtype check_forward_ad = torch device device type = xla Test F conv d gradients work padding= same x = torch rand dtype=dtype device=device requires_grad=True y = torch rand dtype=dtype device=device requires_grad=True Symmetric padding z = F conv d x y padding= dilation= z sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None z = F conv d x y padding= same dilation= z sum abs backward assertEqual gx_expect x grad assertEqual gy_expect y grad x grad y grad = None None gradcheck lambda x y F conv d x y padding= same dilation= x y check_forward_ad=check_forward_ad nondet_tol= e- torch device device type = cuda https github com pytorch pytorch issues gradgradcheck lambda x y F conv d x y padding= same dilation= x y check_fwd_over_rev=True Asymmetric padding y = torch rand dtype=dtype device=device requires_grad=True z = F conv d x y padding= z sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None z = F conv d x y padding= same z sum abs backward assertEqual gx_expect x grad assertEqual gy_expect y grad gradcheck lambda x y F conv d x y padding= same x y check_forward_ad=check_forward_ad nondet_tol= e- torch device device type = cuda https github com pytorch pytorch issues gradgradcheck lambda x y F conv d x y padding= same x y check_fwd_over_rev=True dtypes torch float torch cfloat dtypesIfMPS torch float MACOS_VERSION torch float torch cfloat Complex supported MacOS test_conv d_valid_padding_backward device dtype Test F conv d gradients work padding= valid x = torch rand dtype=dtype device=device requires_grad=True y = torch rand dtype=dtype device=device requires_grad=True F conv d x y padding= sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None F conv d x y padding= valid sum abs backward gx_actual gy_actual = x grad y grad assertEqual gx_expect gx_actual assertEqual gy_expect gy_actual unittest skipIf TEST_SCIPY Scipy required test dtypes torch float torch cfloat dtypesIfMPS torch float MACOS_VERSION torch float torch cfloat Complex supported MacOS parametrize_test mode valid same test_conv d_vs_scipy device dtype mode t = make_tensor device=device dtype=dtype feat_dim = t shape weight_even = make_tensor device=device dtype=dtype weight_odd = make_tensor device=device dtype=dtype _test t weight mode SciPy expects two -D inputs t_a = t view - cpu numpy w_a = weight view - cpu numpy expected = scipy signal convolve t_a w_a mode=mode kwargs = padding mode mode == same ` same ` padding PyTorch conv d different SciPy p = weight shape t = torch nn functional pad t p p We have already taken care padding kwargs pop padding second input flipped SciPy s convolve weight_flipped = torch flip weight actual = torch nn functional conv d t weight_flipped kwargs squeeze mode == same actual = actual feat_dim assertEqual actual expected atol= e- rtol= e- Global dtype test suite torch double This leads change type-promotion conv d outputs ` complex ` ` complex ` input set_default_dtype torch float _test t weight_even mode _test t weight_odd mode unittest skipIf TEST_SCIPY Scipy required test dtypes torch float torch cfloat dtypesIfMPS torch float MACOS_VERSION torch float torch cfloat Complex supported MacOS parametrize_test mode valid same test_conv d_vs_scipy device dtype mode t = make_tensor device=device dtype=dtype weight_even = make_tensor device=device dtype=dtype weight_odd = make_tensor device=device dtype=dtype _test t weight mode SciPy expects two -D inputs t_a = t squeeze cpu numpy w_a = weight squeeze squeeze cpu numpy expected = scipy signal convolve d t_a w_a mode=mode kwargs = padding mode mode == same ` same ` padding PyTorch conv d different SciPy left_right_pad = weight shape top_bottom_pad = weight shape p = left_right_pad left_right_pad top_bottom_pad top_bottom_pad t = torch nn functional pad t p We have already taken care padding kwargs pop padding second input flipped SciPy s convolve d weight_flipped = torch flip weight actual = torch nn functional conv d t weight_flipped kwargs squeeze mode == same actual = actual assertEqual actual expected rtol= e- atol= e- Global dtype test suite torch double This leads change type-promotion conv d outputs ` complex ` ` complex ` input set_default_dtype torch float _test t weight_even mode _test t weight_odd mode unittest skipIf TEST_SCIPY Scipy required test skipMPS Results CI inconsistent forced skip dtypes torch float torch cfloat parametrize_test mode valid same test_conv d_vs_scipy device dtype mode t = make_tensor device=device dtype=dtype weight_even = make_tensor device=device dtype=dtype weight_odd = make_tensor device=device dtype=dtype _test t weight mode SciPy expects two -D inputs t_a = t squeeze cpu numpy w_a = weight squeeze squeeze cpu numpy expected = scipy signal convolve t_a w_a mode=mode kwargs = padding mode mode == same ` same ` padding PyTorch conv d different SciPy left_right_pad = weight shape top_bottom_pad = weight shape front_back_pad = weight shape p = left_right_pad left_right_pad top_bottom_pad top_bottom_pad front_back_pad front_back_pad t = torch nn functional pad t p We have already taken care padding kwargs pop padding second input flipped SciPy s convolve weight_flipped = torch flip weight actual = torch nn functional conv d t weight_flipped kwargs squeeze mode == same actual = actual torch cuda is_tf _supported dtype == torch float dtype == torch complex assertEqual actual expected atol= rtol= assertEqual actual expected rtol= e- atol= e- Global dtype test suite torch double This leads change type-promotion conv d outputs ` complex ` ` complex ` input set_default_dtype torch float _test t weight_even mode _test t weight_odd mode dtypes torch float torch complex dtypesIfMPS torch float MACOS_VERSION torch float torch cfloat Complex supported MacOS test_conv d_valid_padding_backward device dtype Test F conv d gradients work padding= valid x = torch rand device=device dtype=dtype requires_grad=True y = torch rand device=device dtype=dtype requires_grad=True F conv d x y padding= sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None F conv d x y padding= valid sum abs backward gx_actual gy_actual = x grad y grad assertEqual gx_expect gx_actual assertEqual gy_expect gy_actual dtypes torch double torch cdouble dtypesIfMPS torch float torch cfloat Double complex double supported MPS expectedFailureMPS https github com pytorch pytorch issues test_conv d_valid_padding_backward device dtype check_forward_ad = torch device device type = xla Test F conv d gradients work padding= valid x = torch rand dtype=dtype device=device requires_grad=True y = torch rand dtype=dtype device=device requires_grad=True F conv d x y padding= sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None F conv d x y padding= valid sum abs backward gx_actual gy_actual = x grad y grad assertEqual gx_expect gx_actual assertEqual gy_expect gy_actual gradcheck lambda x y F conv d x y padding= valid x y check_forward_ad=check_forward_ad gradgradcheck lambda x y F conv d x y padding= valid x y check_fwd_over_rev=check_forward_ad parametrize_test arg_str= N arg_values= subtest arg_values= name= ConvTranspose d subtest arg_values= name= ConvTranspose d test_conv_transpose_with_output_size_and_no_batch_dim device N For inputs no batch dim verify output correct shape when output_size set See https github com pytorch pytorch issues inp = torch randn N == device=device output_size = N == ConvTransposeNd = getattr nn f ConvTranspose N d m = ConvTransposeNd kernel_size= stride= padding= bias=False device=device output = m inp output_size=output_size assertEqual output shape output_size skipMeta parametrize_test input_shape transposed dilated groups layout backend_expected === slow === subtest False False torch strided torch _C _ConvBackend Slow d decorators= onlyNativeDeviceTypes disableMkldnn disablecuDNN name= slow d subtest True False torch strided torch _C _ConvBackend SlowTranspose d decorators= onlyNativeDeviceTypes disableMkldnn disablecuDNN name= slow d_transposed subtest False True torch strided torch _C _ConvBackend SlowDilated d decorators= onlyNativeDeviceTypes disableMkldnn disablecuDNN name= slow d_dilated subtest True True torch strided torch _C _ConvBackend SlowTranspose d decorators= onlyNativeDeviceTypes disableMkldnn disablecuDNN name= slow d_dilated_transposed subtest False False torch strided torch _C _ConvBackend Slow d decorators= onlyNativeDeviceTypes disableMkldnn disablecuDNN name= slow d subtest True False torch strided torch _C _ConvBackend SlowTranspose d decorators= onlyNativeDeviceTypes disableMkldnn disablecuDNN name= slow d_transposed subtest False True torch strided torch _C _ConvBackend SlowDilated d decorators= onlyNativeDeviceTypes disableMkldnn disablecuDNN name= slow d_dilated subtest True True torch strided torch _C _ConvBackend SlowTranspose d decorators= onlyNativeDeviceTypes disableMkldnn disablecuDNN name= slow d_dilated_transposed subtest False False torch strided torch _C _ConvBackend Slow d decorators= onlyCPU disableMkldnn name= slow d_cpu CUDA doesn t have slow D implementation so goes dilated D implementation instead subtest False False torch strided torch _C _ConvBackend SlowDilated d decorators= onlyCUDA disablecuDNN name= slow d_cuda FIXME RuntimeError CUDA out memory subtest True False torch strided torch _C _ConvBackend SlowTranspose d decorators= onlyNativeDeviceTypes disableMkldnn disablecuDNN name= slow d_transposed subtest False True torch strided torch _C _ConvBackend SlowDilated d decorators= onlyNativeDeviceTypes disableMkldnn disablecuDNN name= slow d_dilated FIXME RuntimeError CUDA out memory subtest True True torch strided torch _C _ConvBackend SlowTranspose d decorators= onlyNativeDeviceTypes disableMkldnn disablecuDNN name= slow d_dilated_transposed subtest False False torch strided torch _C _ConvBackend Empty decorators= onlyNativeDeviceTypes disableMkldnn name= empty_batch d subtest False False torch strided torch _C _ConvBackend Empty decorators= onlyNativeDeviceTypes disableMkldnn name= empty_channel d subtest False False torch strided torch _C _ConvBackend Empty decorators= onlyNativeDeviceTypes disableMkldnn name= empty_batch_channel d subtest False False torch strided torch _C _ConvBackend Empty decorators= onlyNativeDeviceTypes disableMkldnn name= empty_batch d subtest False False torch strided torch _C _ConvBackend Empty decorators= onlyNativeDeviceTypes disableMkldnn name= empty_channel d subtest False False torch strided torch _C _ConvBackend Empty decorators= onlyNativeDeviceTypes disableMkldnn name= empty_batch_channel d subtest False False torch strided torch _C _ConvBackend Empty decorators= onlyNativeDeviceTypes disableMkldnn name= empty_batch d subtest False False torch strided torch _C _ConvBackend Empty decorators= onlyNativeDeviceTypes disableMkldnn name= empty_channel d subtest False False torch strided torch _C _ConvBackend Empty decorators= onlyNativeDeviceTypes disableMkldnn name= empty_batch_channel d === cuda === Note disablecuDNN disables miopen well subtest False False torch strided torch _C _ConvBackend CudaDepthwise d decorators= onlyCUDA disablecuDNN name= cuda_depthwise d subtest False False torch strided torch _C _ConvBackend CudaDepthwise d decorators= onlyCUDA disablecuDNN name= cuda_depthwise d subtest False False torch strided torch _C _ConvBackend CudaDepthwise d decorators= onlyCUDA disablecuDNN name= cuda_depthwise d === cudnn === subtest False False torch strided torch _C _ConvBackend Cudnn decorators= onlyCUDA skipCUDAIfNoCudnn skipCUDAIfMiopen name= cudnn d subtest False False torch strided torch _C _ConvBackend Cudnn decorators= onlyCUDA skipCUDAIfNoCudnn skipCUDAIfMiopen name= cudnn d subtest False False torch strided torch _C _ConvBackend Cudnn decorators= onlyCUDA skipCUDAIfNoCudnn skipCUDAIfMiopen name= cudnn d subtest True False torch strided torch _C _ConvBackend CudnnTranspose decorators= onlyCUDA skipCUDAIfNoCudnn skipCUDAIfMiopen name= cudnn d_transposed subtest True False torch strided torch _C _ConvBackend CudnnTranspose decorators= onlyCUDA skipCUDAIfNoCudnn skipCUDAIfMiopen name= cudnn d_transposed FIXME RuntimeError CUDA out memory subtest True False torch strided torch _C _ConvBackend CudnnTranspose decorators= onlyCUDA skipCUDAIfNoCudnn skipCUDAIfMiopen name= cudnn d_transposed === miopen === subtest False False torch strided torch _C _ConvBackend Miopen decorators= onlyCUDA skipCUDAIfNoMiopen name= miopen d subtest False False torch strided torch _C _ConvBackend Miopen decorators= onlyCUDA skipCUDAIfNoMiopen name= miopen d subtest False False torch strided torch _C _ConvBackend Miopen decorators= onlyCUDA skipCUDAIfNoMiopen name= miopen d subtest True False torch strided torch _C _ConvBackend MiopenTranspose decorators= onlyCUDA skipCUDAIfNoMiopen name= miopen d_transposed subtest True False torch strided torch _C _ConvBackend MiopenTranspose decorators= onlyCUDA skipCUDAIfNoMiopen name= miopen d_transposed subtest True False torch strided torch _C _ConvBackend MiopenTranspose decorators= onlyCUDA skipCUDAIfNoMiopen name= miopen d_transposed subtest False False torch strided torch _C _ConvBackend MiopenDepthwise decorators= onlyCUDA skipCUDAIfNoMiopen name= miopen_depthwise d subtest False False torch strided torch _C _ConvBackend MiopenDepthwise decorators= onlyCUDA skipCUDAIfNoMiopen name= miopen_depthwise d subtest False False torch strided torch _C _ConvBackend MiopenDepthwise decorators= onlyCUDA skipCUDAIfNoMiopen name= miopen_depthwise d === mkldnn === subtest False False torch _mkldnn torch _C _ConvBackend Mkldnn decorators= onlyCPU skipCPUIfNoMkldnn name= mkldnn d subtest False False torch _mkldnn torch _C _ConvBackend Mkldnn decorators= onlyCPU skipCPUIfNoMkldnn name= mkldnn d subtest False False torch _mkldnn torch _C _ConvBackend Mkldnn decorators= onlyCPU skipCPUIfNoMkldnn name= mkldnn d Transposed convolution broken mkldnn See https github com pytorch pytorch issues subtest True False torch _mkldnn torch _C _ConvBackend Mkldnn decorators= onlyCPU skipCPUIfNoMkldnn unittest expectedFailure name= mkldnn d_transposed subtest True False torch _mkldnn torch _C _ConvBackend Mkldnn decorators= onlyCPU skipCPUIfNoMkldnn unittest expectedFailure name= mkldnn d_transposed subtest True False torch _mkldnn torch _C _ConvBackend Mkldnn decorators= onlyCPU skipCPUIfNoMkldnn unittest expectedFailure name= mkldnn d_transposed subtest False True torch strided torch _C _ConvBackend Mkldnn decorators= onlyCPU skipCPUIfNoMkldnn name= mkldnn d_cpu_input subtest False True torch strided torch _C _ConvBackend Mkldnn decorators= onlyCPU skipCPUIfNoMkldnn name= mkldnn d_cpu_input subtest False True torch strided torch _C _ConvBackend Mkldnn decorators= onlyCPU skipCPUIfNoMkldnn name= mkldnn d_cpu_input subtest False False torch _mkldnn torch _C _ConvBackend MkldnnEmpty decorators= onlyCPU skipCPUIfNoMkldnn name= mkldnn_empty_batch d subtest False False torch _mkldnn torch _C _ConvBackend MkldnnEmpty decorators= onlyCPU skipCPUIfNoMkldnn name= mkldnn_empty_channel d subtest False False torch _mkldnn torch _C _ConvBackend MkldnnEmpty decorators= onlyCPU skipCPUIfNoMkldnn name= mkldnn_empty_batch_channel d subtest False False torch _mkldnn torch _C _ConvBackend MkldnnEmpty decorators= onlyCPU skipCPUIfNoMkldnn name= mkldnn_empty_batch d subtest False False torch _mkldnn torch _C _ConvBackend MkldnnEmpty decorators= onlyCPU skipCPUIfNoMkldnn name= mkldnn_empty_channel d subtest False False torch _mkldnn torch _C _ConvBackend MkldnnEmpty decorators= onlyCPU skipCPUIfNoMkldnn name= mkldnn_empty_batch_channel d subtest False False torch _mkldnn torch _C _ConvBackend MkldnnEmpty decorators= onlyCPU skipCPUIfNoMkldnn name= mkldnn_empty_batch d subtest False False torch _mkldnn torch _C _ConvBackend MkldnnEmpty decorators= onlyCPU skipCPUIfNoMkldnn name= mkldnn_empty_channel d subtest False False torch _mkldnn torch _C _ConvBackend MkldnnEmpty decorators= onlyCPU skipCPUIfNoMkldnn name= mkldnn_empty_batch_channel d Note Tests mobile backends currently supported This comprises NnpackSpatial Winograd x Depthwise Xnnpack d backends Testing these requires ability gate tests whether PyTorch built USE_MOBILE= Test both bias no bias parametrize_test has_bias False True Test both stride= stride cases parametrize_test strided False True Test both contiguous non-contiguous inputs parametrize_test contiguous False True expectedFailureMPS No double support test_conv_backend device input_shape has_bias strided contiguous transposed dilated groups layout backend_expected Build up inputs dtype = torch float C_in C_out dim kernel_size = input_shape len input_shape - x = torch randn input_shape device=device dtype=dtype requires_grad=True weight = torch randn C_in transposed C_out C_out groups transposed C_in groups kernel_size _ range dim device=device dtype=dtype requires_grad=True bias = torch randn C_out device=device dtype=dtype requires_grad=True has_bias None _make_noncontiguous inp inp None None old_requires_grad = inp requires_grad inp = torch repeat_interleave inp dim=- inp = inp detach requires_grad_ old_requires_grad inp contiguous x = _make_noncontiguous x weight = _make_noncontiguous weight bias = _make_noncontiguous bias layout torch _mkldnn x = x to_mkldnn Note weight bias supported mkldnn tensors during training stride = dim strided dim padding = dim dilation = dim dilated dim output_padding = dim inputs = x weight bias stride padding dilation transposed output_padding groups Ensure correct backend selected backend_actual = torch _C _select_conv_backend inputs assertEqual backend_actual backend_expected Ensure backward call succeeds convolution = torch ops aten convolution output = convolution inputs grad_output = torch randn output shape device=device dtype=dtype contiguous grad_output = _make_noncontiguous grad_output layout torch _mkldnn grad_output = grad_output to_mkldnn output backward grad_output mkldnn doesn t support gradcheck layout torch _mkldnn backend_actual = torch _C _ConvBackend Empty FIXME forward AD fails Forward AD forward-over-reverse AD smoke test float TODO remove we introduce per-op gradient tests float fwAD dual_level dual_inputs = fwAD make_dual i torch rand_like i isinstance i torch Tensor i i inputs Forward AD output = convolution dual_inputs Forward over reverse AD grad_output_d = fwAD make_dual torch rand_like output torch rand_like output has_bias torch autograd grad output x weight bias grad_output_d torch autograd grad output x weight grad_output_d Convert float gradcheck x = x torch float detach requires_grad_ True weight = weight torch float detach requires_grad_ True bias None bias = bias torch float detach requires_grad_ True inputs = x weight bias stride padding dilation transposed output_padding groups Set some backend-specific validation settings gradcheck_nondet_tol = torch backends cudnn is_available cuDNN introduces non-determinism gradcheck_nondet_tol = GRADCHECK_NONDET_TOL assertTrue gradcheck convolution inputs nondet_tol=gradcheck_nondet_tol double backward doesn t support bias gradients bias None bias requires_grad_ False assertTrue gradgradcheck convolution inputs nondet_tol=gradcheck_nondet_tol onlyCPU test_conv_contiguous_for_oneDNN See https github com pytorch pytorch issues dtype torch float torch bfloat torch half conv = nn Conv d kernel_size= stride= padding= dilation= groups= bias=True padding_mode= zeros dtype=dtype x = torch rand dtype=dtype x = torch transpose x x = x torch backends mkldnn is_available y = conv x Disable MKLDNN explicitly torch backends mkldnn flags enabled=False y_ = conv x assertEqual y y_ onlyCPU test_conv_ic _channels_last_for_oneDNN See https github com pytorch pytorch issues N will call OneDNN path dtype torch float torch bfloat torch half conv = torch nn Conv d kernel_size= padding= bias=False conv = conv memory_format=torch channels_last dtype=dtype x = torch rand dtype=dtype torch backends mkldnn is_available y = conv x Disable MKLDNN explicitly torch backends mkldnn flags enabled=False y_ = conv x assertEqual y y_ dtypes torch float torch cfloat test_conv_empty_channel device dtype in_channels = mod = torch nn Conv d in_channels stride= dtype=dtype device inp = torch randn device=device dtype=dtype _test_module_empty_input mod inp check_size=False assertRaisesRegex RuntimeError Given groups= weight inp = torch randn device=device dtype=dtype mod inp mod = torch nn Conv d in_channels stride= dtype=dtype device inp = torch randn device=device dtype=dtype _test_module_empty_input mod inp check_size=False assertRaisesRegex RuntimeError Given groups= weight inp = torch randn device=device dtype=dtype mod inp mod = torch nn Conv d in_channels stride= dtype=dtype device inp = torch randn device=device dtype=dtype _test_module_empty_input mod inp check_size=False assertRaisesRegex RuntimeError Given groups= weight inp = torch randn device=device dtype=dtype mod inp test_group_conv_empty device mod = torch nn Conv d stride= kernel_size= padding= groups= device inp = torch randn device=device _test_module_empty_input mod inp check_size=False device_type == cuda has_cudnn torch backends cudnn flags enabled=False _test_module_empty_input mod inp check_size=False test_group_convTranspose_empty device mod = torch nn ConvTranspose d stride= kernel_size= padding= groups= device inp = torch randn device=device _test_module_empty_input mod inp check_size=False device_type == cuda has_cudnn torch backends cudnn flags enabled=False _test_module_empty_input mod inp check_size=False test_convTranspose_empty device mod = torch nn ConvTranspose d stride= kernel_size= padding= device inp = torch randn device=device _test_module_empty_input mod inp check_size=False device_type == cuda has_cudnn torch backends cudnn flags enabled=False _test_module_empty_input mod inp check_size=False onlyCUDA largeTensorTest GB test_conv_large_nosplit device Here we just test convolution correctly route fallback implementation does crash The correctness fallback implementation should covered other tests dtype = torch half device_type == cuda torch float conv = nn Conv d device dtype input_large = torch randn dtype=dtype device=device conv input_large conv = torch nn Conv d device dtype input_large = torch randn dtype=dtype device=device conv input_large test_conv_noncontig_weights device dim grouped False True nc = groups = grouped w = torch randn dim device=device w = w expand nc int nc groups + list w shape w = w detach requires_grad_ x = torch randn nc + dim device=device requires_grad=True y = getattr F f conv dim d x w groups=groups y sum backward y = getattr F f conv_transpose dim d x w groups=groups y sum backward test_conv_noncontig_weights_and_bias device need floats exercise https github com pytorch pytorch issues bias True False conv = nn Conv d kernel_size= stride= padding= bias=bias device torch float input_nc = torch randn device=device dtype=torch float input_c = input_nc contiguous weight_nc = torch randn device=device dtype=torch float conv weight = nn Parameter weight_nc weight_c = conv weight contiguous bias bias_nc = torch randn device=device dtype=torch float conv bias = nn Parameter bias_nc bias_c = conv bias contiguous out = conv input_nc conv weight = nn Parameter weight_c bias conv bias = nn Parameter bias_c out = conv input_c assertEqual out out onlyCUDA largeTensorTest GB test_conv_transposed_large device dtype = torch half device_type == cuda torch float conv = nn ConvTranspose d bias=False device dtype input_large = torch randn dtype=dtype device=device forward ret = conv input_large maxdiff = ret narrow - conv input_large narrow abs_ max item maxdiff = ret narrow - conv input_large narrow abs_ max item maxdiff = ret narrow - conv input_large narrow abs_ max item maxdiff = ret narrow - conv input_large narrow abs_ max item device_type == cuda cuDNN may use algorithms such FFT don t guarantee diff assertEqual maxdiff atol= e- rtol= e- assertEqual maxdiff atol= e- rtol= e- assertEqual maxdiff atol= e- rtol= e- assertEqual maxdiff atol= e- rtol= e- assertEqual maxdiff assertEqual maxdiff assertEqual maxdiff assertEqual maxdiff onlyCUDA largeTensorTest GB test_conv_large device dtype = torch half device_type == cuda torch float conv = nn Conv d bias=False device dtype input_large = torch randn dtype=dtype device=device forward ret = conv input_large assertEqual ret conv input_large assertEqual ret conv input_large assertEqual ret conv input_large backward conv zero_grad When computing backward we using ` max dim= ` ` create some sparsity Without sparsity rounding error would too large large e- satisfy creterion e- ` assertEqual ` ret view - max dim= values sum backward del ret grad = conv weight grad detach clone conv zero_grad conv input_large view - max dim= values sum backward conv input_large view - max dim= values sum backward conv input_large view - max dim= values sum backward grad = conv weight grad detach clone gradients order hundreds we need scale order one so we can compare scale = grad abs mean grad = grad scale grad = grad scale assertEqual grad grad atol= e- rtol= e- onlyCUDA largeTensorTest GB cpu largeTensorTest GB cuda test_conv_large_batch_ device in_channels = dim = out_channels = kernel_size = stride = padding = input_tensor = torch ones in_channels dim dim cuda half model = nn Conv d in_channels out_channels kernel_size stride padding cuda half output = model input_tensor _model_cpu = model cpu float output_cpu = model input_tensor float cpu assertEqual output cpu float output_cpu atol= e- rtol= e- onlyCUDA skipCUDAIfNoCudnn test_contig_wrong_stride_cudnn device x has have batch_size test contiguous checks x = torch randn device=device stride = list x stride stride = change stride dimension tensor still contiguous because size x set_ x storage x size stride assertTrue x is_contiguous F conv_transpose d x torch randn device=device F conv d x torch randn device=device skipIfRocmArch MI _ARCH onlyCUDA tf _on_and_off test_Conv d_size_ _kernel device x_cpu = torch randn conv_cpu = torch nn Conv d kernel_size= y_cpu = conv_cpu x_cpu y = torch rand_like y_cpu y_cpu backward y cudnn flags enabled=False conv_cuda = torch nn Conv d kernel_size= device conv_cuda bias data copy_ conv_cpu bias data conv_cuda weight data copy_ conv_cpu weight data y_cuda = conv_cuda x_cpu device y_cuda backward y device assertEqual y_cpu y_cuda atol= e- rtol= exact_device=False assertEqual conv_cpu bias grad data conv_cuda bias grad data atol= e- rtol= exact_device=False assertEqual conv_cpu weight grad data conv_cuda weight grad data atol= e- rtol= exact_device=False skipIfRocmArch MI _ARCH onlyCUDA tf _on_and_off test_ConvTranspose d_size_ _kernel device x_cpu = torch randn conv_cpu = torch nn ConvTranspose d kernel_size= y_cpu = conv_cpu x_cpu y = torch rand_like y_cpu y_cpu backward y cudnn flags enabled=False conv_cuda = torch nn ConvTranspose d kernel_size= device conv_cuda bias data copy_ conv_cpu bias data conv_cuda weight data copy_ conv_cpu weight data y_cuda = conv_cuda x_cpu device y_cuda backward y device assertEqual y_cpu y_cuda atol= e- rtol= exact_device=False assertEqual conv_cpu bias grad data conv_cuda bias grad data atol= e- rtol= exact_device=False assertEqual conv_cpu weight grad data conv_cuda weight grad data atol= e- rtol= exact_device=False onlyCUDA test_ConvTranspose d_size_ _kernel device set_default_dtype torch double x_cpu = torch randn conv_cpu = torch nn ConvTranspose d kernel_size= y_cpu = conv_cpu x_cpu y = torch rand_like y_cpu y_cpu backward y cudnn flags enabled=False conv_cuda = torch nn ConvTranspose d kernel_size= device conv_cuda bias data copy_ conv_cpu bias data conv_cuda weight data copy_ conv_cpu weight data y_cuda = conv_cuda x_cpu device y_cuda backward y device assertEqual y_cpu y_cuda atol= e- rtol= exact_device=False assertEqual conv_cpu bias grad data conv_cuda bias grad data atol= e- rtol= exact_device=False assertEqual conv_cpu weight grad data conv_cuda weight grad data atol= e- rtol= exact_device=False dtypesIfCUDA floating_types_and torch half torch bfloat AMPERE_OR_ROCM dtypes torch float torch backends cudnn flags enabled=True deterministic=True benchmark=False torch backends miopen flags immediate=True tf _on_and_off test_Conv d_naive_groups device dtype Check grouped convolutions matches two half convolutions m = nn Conv d kernel_size= groups= device dtype i = torch randn device=device dtype=dtype requires_grad=True output = m i grad_output = torch randn device=device dtype=dtype output backward grad_output m = nn Conv d kernel_size= device dtype m weight data copy_ m weight data m bias data copy_ m bias data i = i data contiguous requires_grad_ True output = m i output backward grad_output contiguous m = nn Conv d kernel_size= device dtype m weight data copy_ m weight data m bias data copy_ m bias data i = i data contiguous requires_grad_ True output = m i output backward grad_output contiguous assertEqual output torch cat output output assertEqual i grad data torch cat i grad data i grad data atol=dtype prec_DONTUSE dtype rtol= assertEqual m bias grad data torch cat m bias grad data m bias grad data atol=dtype prec_DONTUSE dtype rtol= assertEqual m weight grad data torch cat m weight grad data m weight grad data atol=dtype prec_DONTUSE dtype rtol= dtypes torch double torch cdouble dtypesIfMPS torch float torch cfloat expectedFailureMPS https github com pytorch pytorch issues test_Conv d_backward_depthwise device dtype x = torch randn device=device dtype=dtype requires_grad=True weight = torch randn device=device dtype=dtype requires_grad=True conv d_depthwise x weight torch nn functional conv d x weight bias=None stride= groups= cudnn_enabled False True torch backends cudnn flags enabled=cudnn_enabled torch autograd gradcheck conv d_depthwise x weight onlyCPU dtypes torch float torch double test_conv_thnn_nhwc device dtype helper mod n c h w out_channels kernel_size dilation groups input_format weight_format input = torch randint - n c h w dtype=dtype device=device memory_format=input_format input requires_grad_ conv = mod c out_channels kernel_size dilation=dilation groups=groups device= cpu dtype=dtype memory_format=weight_format p conv parameters p data = torch randint_like p - ref_input = input detach clone contiguous requires_grad_ ref_conv = mod c out_channels kernel_size dilation=dilation groups=groups load_state_dict will restore stride memory_layout ref_conv weight ref_conv load_state_dict conv state_dict ref_conv = ref_conv device= cpu dtype=dtype memory_format=torch contiguous_format out = conv input ref_out = ref_conv ref_input grad = torch randint_like out - ref_grad = grad detach clone contiguous out backward grad ref_out backward ref_grad assertTrue out is_contiguous memory_format=torch channels_last assertTrue ref_out is_contiguous assertEqual out ref_out exact_dtype=False assertEqual conv weight grad ref_conv weight grad exact_dtype=False assertEqual conv bias grad ref_conv bias grad exact_dtype=False assertEqual input grad ref_input grad exact_dtype=False torch backends mkldnn flags enabled=False formats = torch channels_last torch channels_last torch channels_last torch contiguous_format torch contiguous_format torch channels_last input_format weight_format formats non-dilated conv thnn_conv d normal path im col helper nn Conv d out_channels= kernel_size= dilation= groups= input_format=input_format weight_format=weight_format helper nn Conv d out_channels= kernel_size= dilation= groups= input_format=input_format weight_format=weight_format test when input channel converted channels last helper nn Conv d out_channels= kernel_size= dilation= groups= input_format=torch contiguous_format weight_format=torch channels_last non-dilated conv thnn_conv d fast path skip im col helper nn Conv d out_channels= kernel_size= dilation= groups= input_format=input_format weight_format=weight_format ic == oc == here so need stick input CL activate channels last helper nn Conv d out_channels= kernel_size= dilation= groups= input_format=torch channels_last weight_format=weight_format dilated conv slow_conv_dilated d helper nn Conv d out_channels= kernel_size= dilation= groups= input_format=input_format weight_format=weight_format helper nn Conv d out_channels= kernel_size= dilation= groups= input_format=input_format weight_format=weight_format transposed-conv slow_conv_transpose d helper nn ConvTranspose d out_channels= kernel_size= dilation= groups= input_format=input_format weight_format=weight_format helper nn ConvTranspose d out_channels= kernel_size= dilation= groups= input_format=input_format weight_format=weight_format helper nn ConvTranspose d out_channels= kernel_size= dilation= groups= input_format=input_format weight_format=weight_format helper nn ConvTranspose d out_channels= kernel_size= dilation= groups= input_format=input_format weight_format=weight_format onlyCUDA dtypes torch half torch float torch cfloat test_conv_cudnn_nhwc device dtype helper n c h w out_channels kernel_size groups randint dtype=torch cfloat fails RuntimeError check_random_bounds handles only integral floating-point boolean types must create randint randint_like using default int then cast desired input = torch randint - n c h w dtype=torch int device=device dtype memory_format=torch channels_last input requires_grad_ conv = nn Conv d c out_channels kernel_size groups=groups device= cuda dtype=dtype memory_format=torch channels_last p conv parameters p data = torch randint_like p - dtype=torch int p dtype use FP channels-first conv reference ref_input = input detach clone contiguous double requires_grad_ ref_conv = nn Conv d c out_channels kernel_size groups=groups load_state_dict will restore stride memory_layout ref_conv weight ref_conv load_state_dict conv state_dict ref_conv = ref_conv device= cuda dtype=torch double memory_format=torch contiguous_format out = conv input ref_out = ref_conv ref_input grad = torch randint_like out - dtype=torch int out dtype ref_grad = grad detach clone double contiguous out backward grad ref_out backward ref_grad assertTrue out is_contiguous memory_format=torch channels_last assertTrue input grad is_contiguous memory_format=torch channels_last assertTrue conv weight grad is_contiguous memory_format=torch channels_last assertTrue ref_out is_contiguous assertTrue ref_input grad is_contiguous assertTrue ref_conv weight grad is_contiguous assertEqual out ref_out exact_dtype=False assertEqual conv weight grad ref_conv weight grad exact_dtype=False assertEqual conv bias grad ref_conv bias grad exact_dtype=False assertEqual input grad ref_input grad exact_dtype=False helper out_channels= kernel_size= groups= helper out_channels= kernel_size= groups= helper out_channels= kernel_size= groups= helper out_channels= kernel_size= groups= onlyCUDA dtypes torch half torch float test_conv_cudnn_ndhwc device dtype helper n c d h w out_channels kernel_size groups input = torch randint - n c d h w dtype=dtype device=device memory_format=torch channels_last_ d input requires_grad_ conv = nn Conv d c out_channels kernel_size groups=groups device= cuda dtype=dtype memory_format=torch channels_last_ d p conv parameters p data = torch randint_like p - use FP channels-first conv reference ref_input = input detach clone contiguous double requires_grad_ ref_conv = nn Conv d c out_channels kernel_size groups=groups load_state_dict will restore stride memory_layout ref_conv weight ref_conv load_state_dict conv state_dict ref_conv = ref_conv device= cuda dtype=torch double memory_format=torch contiguous_format out = conv input ref_out = ref_conv ref_input grad = torch randint_like out - ref_grad = grad detach clone double contiguous out backward grad ref_out backward ref_grad assertTrue out is_contiguous memory_format=torch channels_last_ d assertTrue input grad is_contiguous memory_format=torch channels_last_ d assertTrue conv weight grad is_contiguous memory_format=torch channels_last_ d assertTrue ref_out is_contiguous assertTrue ref_input grad is_contiguous assertTrue ref_conv weight grad is_contiguous assertEqual out ref_out exact_dtype=False assertEqual conv weight grad ref_conv weight grad exact_dtype=False assertEqual conv bias grad ref_conv bias grad exact_dtype=False assertEqual input grad ref_input grad exact_dtype=False helper out_channels= kernel_size= groups= helper out_channels= kernel_size= groups= helper out_channels= kernel_size= groups= helper out_channels= kernel_size= groups= _run_conv layer device inp grad ref_conv ref_input ref_out input_format weight_format grad_format output_format conv = layer inp size grad size ref_conv weight size float device load_state_dict will restore stride memory_layout ref_conv weight conv load_state_dict ref_conv state_dict weight_data = conv weight detach clone contiguous memory_format=weight_format conv weight data = weight_data resize_ weight_data size memory_format=weight_format input = inp clone contiguous memory_format=input_format input resize_ input size memory_format=input_format input = input requires_grad_ grad = grad contiguous memory_format=grad_format grad resize_ grad size memory_format=grad_format out = conv input out backward grad assertTrue out is_contiguous memory_format=output_format assertEqual out ref_out assertEqual conv weight grad ref_conv weight grad assertEqual conv bias grad ref_conv bias grad assertEqual input grad ref_input grad _test_conv_cudnn_nhwc_nchw layer n c h w k filter_size device data = torch randint n c h w dtype=torch float device=device ref_input = data clone contiguous requires_grad_ True ref_conv = layer c k filter_size float device ref_out = ref_conv ref_input grad = torch randint ref_out size dtype=torch float device= cuda ref_out backward grad w_f torch contiguous_format torch channels_last g_f torch contiguous_format torch channels_last input_format torch contiguous_format torch channels_last output_format = torch contiguous_format Older versions CudNN have Channels Last support disabled torch backends cudnn version = input_format == torch channels_last output_format = torch channels_last This because we have N weight cannot handle ambiguous memory_format w_f == torch channels_last layer nn Conv d filter_size c = output_format = torch channels_last layer nn ConvTranspose d filter_size k = output_format = torch channels_last _run_conv layer device data grad ref_conv ref_input ref_out input_format w_f g_f output_format onlyCUDA tf _on_and_off test_conv_cudnn_mismatch_memory_format device configs = n c h w k filter_size configs _test_conv_cudnn_nhwc_nchw nn Conv d n c h w k filter_size device _test_conv_cudnn_nhwc_nchw nn ConvTranspose d n c h w k filter_size device onlyCUDA skipCUDAIfNoCudnn dtypes torch float torch double torch float torch bfloat test_conv_cudnn_nhwc_support device dtype input = torch randn dtype=dtype device= cuda requires_grad=True weight = torch randn dtype=dtype device= cuda requires_grad=True weight = weight memory_format=torch channels_last o = torch conv d input weight None assertTrue o is_contiguous memory_format=torch channels_last o sum backward Test faster algorithms used inference produce same results Validates depthwise x bug reported https github com pytorch pytorch issues onlyCPU dtypes torch float test_conv d_no_grad device dtype batch groups input = torch rand batch groups dtype=dtype device=device m = nn Conv d groups kernel_size= groups=groups dtype=dtype device=device torch no_grad output_ng = m input output = m input assertEqual output output_ng rtol= e- atol= e- onlyCUDA skipCUDAIfNoCudnn dtypes torch float torch float torch backends cudnn flags enabled=True deterministic=True benchmark=False precisionOverride torch half torch float e- test_cudnn_convolution_relu device dtype batch groups image_size kernel_size memory_format product torch channels_last torch contiguous_format image_size kernel_size continue inp = torch rand batch groups image_size dtype=dtype device=device w = torch randn groups kernel_size dtype=dtype device=device inp = inp memory_format=memory_format w = w memory_format=memory_format conv d_out = torch conv d inp w None torch version hip cudnn_out = torch miopen_convolution_relu inp w None cudnn_out = torch cudnn_convolution_relu inp w None assertTrue cudnn_out is_contiguous memory_format=memory_format torch cuda is_tf _supported dtype == torch float assertEqual conv d_out relu cudnn_out atol= e- rtol= assertEqual conv d_out relu cudnn_out onlyCUDA skipCUDAIfNoCudnn dtypes torch float torch float torch backends cudnn flags enabled=True deterministic=True benchmark=False precisionOverride torch half torch float e- test_cudnn_convolution_add_relu device dtype batch groups image_size kernel_size memory_format product torch channels_last torch contiguous_format image_size kernel_size continue inp = torch rand batch groups image_size dtype=dtype device=device w = torch randn groups kernel_size dtype=dtype device=device inp = inp memory_format=memory_format w = w memory_format=memory_format conv d_out = torch conv d inp w None alpha = z = torch randn_like conv d_out z = z memory_format=memory_format torch version hip cudnn_out = torch miopen_convolution_add_relu inp w z alpha None cudnn_out = torch cudnn_convolution_add_relu inp w z alpha None assertTrue cudnn_out is_contiguous memory_format=memory_format torch cuda is_tf _supported dtype == torch float assertEqual F relu conv d_out + alpha z cudnn_out atol= e- rtol= assertEqual F relu conv d_out + alpha z cudnn_out onlyCUDA test_convert_conv d_weight_memory_format device input = torch randint dtype=torch float device=device model = nn Sequential nn Conv d nn BatchNorm d device float memory_format torch channels_last torch contiguous_format model = nn utils convert_conv d_weight_memory_format model memory_format out = model input assertTrue out is_contiguous memory_format=memory_format model = nn Sequential nn ConvTranspose d nn BatchNorm d device float memory_format torch channels_last torch contiguous_format model = nn utils convert_conv d_weight_memory_format model memory_format out = model input assertTrue out is_contiguous memory_format=memory_format onlyCUDA test_convert_conv d_weight_memory_format device input = torch randint dtype=torch float device=device model = nn Sequential nn ConvTranspose d nn BatchNorm d device float memory_format torch channels_last_ d torch contiguous_format model = nn utils convert_conv d_weight_memory_format model memory_format out = model input assertTrue out is_contiguous memory_format=memory_format test_conv_double_backward_strided_with_ D_input_and_weight device Test _convolution_double_backward outputs correct grad shapes D input weight when stride This ad-hoc regression test specific case uncovered during convolution consolidation effort The test can safely deleted _convolution_double_backward removed input = torch randn device=device weight = torch randn device=device bias = torch randn device=device stride = padding = dilation = transposed = False output_padding = groups = output = torch ops aten convolution input weight bias stride padding dilation transposed output_padding groups ggI = torch randn input shape device=device ggW = torch randn weight shape device=device ggB = torch randn bias shape device=device gO = torch randn output shape device=device output_mask = True True True grad_grad_output grad_input grad_weight = torch ops aten _convolution_double_backward ggI ggW ggB gO weight input stride padding dilation transposed output_padding groups output_mask Make sure correct shapes computed assertEqual grad_grad_output shape gO shape assertEqual grad_input shape input shape assertEqual grad_weight shape weight shape skipCUDAIfRocm onlyCUDA largeTensorTest GB largeTensorTest GB cpu tf _on_and_off test_conv d_ bit_indexing device x = torch rand m = torch nn Conv d kernel_size= padding= stride= bias=False yref = m x y = m device=device x device=device assertEqual yref y skipCUDAIfRocm onlyCUDA largeTensorTest GB cuda test_conv d_cudnn_broken device dtype torch half torch bfloat x = torch rand dtype=dtype device=device m = torch nn Conv d kernel_size= padding= stride= bias=False dtype=dtype device=device torch backends cudnn flags enabled=False yref = m x y = m x assertEqual yref y skipCUDAIfRocm onlyCUDA largeTensorTest GB largeTensorTest GB cpu TODO eqy Remove once fixed cuDNN we can dispatch again xfailIf _get_cudnn_version None _get_cudnn_version test_depthwise_conv_ bit_indexing device x = torch randn dtype=torch half memory_format=torch channels_last c = nn Conv d kernel_size= stride= padding= groups= dtype=torch half memory_format=torch channels_last yref = c x y = c device=device x device=device assertEqual yref y atol= e- rtol= e- del y yref try batch-splittable case x = x reshape x = x contiguous memory_format=torch channels_last yref = c x y = c device=device x device=device assertEqual yref y atol= e- rtol= e- instantiate_device_type_tests TestConvolutionNNDeviceType globals allow_mps=True instantiate_parametrized_tests TestConvolutionNN __name__ == __main__ run_tests