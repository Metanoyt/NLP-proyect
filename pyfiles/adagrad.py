mypy allow-untyped-defs typing cast Optional Union torch torch Tensor optimizer _default_to_fused_or_foreach _device_dtype_check_for_fused _differentiable_doc _foreach_doc _get_scalar_dtype _get_value _maximize_doc _params_doc _to_scalar _use_grad_for_differentiable _view_as_real Optimizer ParamsT __all__ = Adagrad adagrad Adagrad Optimizer __init__ params ParamsT lr Union float Tensor = e- lr_decay float = weight_decay float = initial_accumulator_value float = eps float = e- foreach Optional bool = None maximize bool = False differentiable bool = False fused Optional bool = None isinstance lr Tensor lr numel = raise ValueError Tensor lr must -element = lr raise ValueError f Invalid learning rate lr = lr_decay raise ValueError f Invalid lr_decay value lr_decay = weight_decay raise ValueError f Invalid weight_decay value weight_decay = initial_accumulator_value raise ValueError f Invalid initial_accumulator_value value initial_accumulator_value = eps raise ValueError f Invalid epsilon value eps defaults = lr lr lr_decay lr_decay eps eps weight_decay weight_decay initial_accumulator_value initial_accumulator_value foreach foreach maximize maximize differentiable differentiable fused fused super __init__ params defaults fused differentiable raise RuntimeError ` fused ` does support ` differentiable ` foreach raise RuntimeError ` fused ` ` foreach ` cannot ` True ` together _need_device_dtype_check_for_fused = True group param_groups p group params state = state p state step = torch zeros dtype=_get_scalar_dtype is_fused=group fused device=p device group fused torch tensor dtype=_get_scalar_dtype init_value = complex initial_accumulator_value initial_accumulator_value torch is_complex p initial_accumulator_value state sum = torch full_like p init_value memory_format=torch preserve_format __setstate__ state super __setstate__ state define fused MYPY error Name fused may undefined fused = None group param_groups group setdefault foreach None group setdefault maximize False group setdefault differentiable False fused = group setdefault fused None state_values = list state values step_is_tensor = len state_values = torch is_tensor state_values step step_is_tensor s state_values s step = torch tensor float s step dtype=_get_scalar_dtype is_fused=fused share_memory Calls tensor share_memory_ state sum tensors group param_groups p group params state = state p state sum share_memory_ _init_group group params_with_grad grads state_sums state_steps has_sparse_grad has_complex = False False p group params p grad None group fused getattr _need_device_dtype_check_for_fused True _device_dtype_check_for_fused p cuda_unsupported=True _need_device_dtype_check_for_fused = False has_sparse_grad &#124; = p grad is_sparse has_complex &#124; = torch is_complex p params_with_grad append p grads append p grad state = state p state_sums append state sum state_steps append state step has_sparse_grad has_complex _use_grad_for_differentiable step closure=None Perform single optimization step Args closure Callable optional A closure reevaluates model returns loss loss = None closure None torch enable_grad loss = closure group param_groups params_with_grad list Tensor = grads list Tensor = state_sums list Tensor = state_steps list Tensor = has_sparse_grad has_complex = _init_group group params_with_grad grads state_sums state_steps adagrad params_with_grad grads state_sums state_steps lr=group lr weight_decay=group weight_decay lr_decay=group lr_decay eps=group eps has_sparse_grad=has_sparse_grad foreach=group foreach maximize=group maximize differentiable=group differentiable has_complex=has_complex fused=group fused grad_scale=getattr grad_scale None found_inf=getattr found_inf None loss Adagrad __doc__ = r Implements Adagrad algorithm math \begin aligned \rule mm pt \\ \textbf input \gamma \text lr \ \theta_ \text params \ f \theta \text objective \ \lambda \text weight decay \\ \hspace mm \tau \text initial accumulator value \ \eta\text lr decay \\ \textbf initialize state\_sum_ \leftarrow \tau \\ - ex \rule mm pt \\ \textbf \ t= \ \textbf \ \ldots \ \textbf do \\ \hspace mm g_t \leftarrow \nabla_ \theta f_t \theta_ t- \\ \hspace mm \tilde \gamma \leftarrow \gamma + t- \eta \\ \hspace mm \textbf \ \lambda \neq \\ \hspace mm g_t \leftarrow g_t + \lambda \theta_ t- \\ \hspace mm state\_sum_t \leftarrow state\_sum_ t- + g^ _t \\ \hspace mm \theta_t \leftarrow \theta_ t- - \tilde \gamma \frac g_t \sqrt state\_sum_t +\epsilon \\ \rule mm pt \\ - ex \bf \ \theta_t \\ - ex \rule mm pt \\ - ex \end aligned For further details regarding algorithm we refer ` Adaptive Subgradient Methods Online Learning Stochastic Optimization ` _ + rf Args _params_doc lr float Tensor optional learning rate default e- lr_decay float optional learning rate decay default weight_decay float optional weight decay L penalty default initial_accumulator_value float optional initial value sum squares gradients default eps float optional term added denominator improve numerical stability default e- _foreach_doc _maximize_doc _differentiable_doc fused bool optional whether fused implementation CPU only used Currently ` torch float ` ` torch float ` ` torch float ` ` torch bfloat ` supported default None Please note fused implementations does support sparse complex gradients _Adaptive Subgradient Methods Online Learning Stochastic Optimization http jmlr org papers v duchi html adagrad params list Tensor grads list Tensor state_sums list Tensor state_steps list Tensor fused Optional bool = None grad_scale Optional Tensor = None found_inf Optional Tensor = None kwonly args defaults supported functions compiled torchscript issue setting these kwargs now functional API compiled torch distributed optim has_sparse_grad bool = False foreach Optional bool = None differentiable bool = False has_complex bool = False lr float weight_decay float lr_decay float eps float maximize bool r Functional API performs Adagrad algorithm computation See ` ~torch optim Adagrad ` details all isinstance t torch Tensor t state_steps raise RuntimeError API has changed ` state_steps ` argument must contain list singleton tensors Respect when user inputs False True foreach fused We only want change default when neither have been user-specified Note we default foreach pass False use_fused This mistake -- we want give fused impl bake-in time before making default even typically faster fused None foreach None _ foreach = _default_to_fused_or_foreach params differentiable use_fused=False fused None fused = False foreach None foreach = False foreach torch jit is_scripting raise RuntimeError torch jit script supported foreach optimizers fused torch jit is_scripting raise RuntimeError torch jit script supported fused optimizers fused torch jit is_scripting func = _fused_adagrad foreach torch jit is_scripting func = _multi_tensor_adagrad func = _single_tensor_adagrad func params grads state_sums state_steps lr=lr weight_decay=weight_decay lr_decay=lr_decay eps=eps has_sparse_grad=has_sparse_grad maximize=maximize differentiable=differentiable has_complex=has_complex grad_scale=grad_scale found_inf=found_inf _make_sparse grad grad_indices values size = grad size torch sparse_coo_tensor grad_indices values size _single_tensor_adagrad params list Tensor grads list Tensor state_sums list Tensor state_steps list Tensor grad_scale Optional Tensor found_inf Optional Tensor lr float weight_decay float lr_decay float eps float has_sparse_grad bool maximize bool differentiable bool has_complex bool grad_scale None found_inf None raise AssertionError Expected grad_scale found_inf None torch jit is_scripting lr = _to_scalar lr param grad state_sum step_t zip params grads state_sums state_steps strict=True update step step_t += step = _get_value step_t grad = grad maximize -grad weight_decay = grad is_sparse raise RuntimeError weight_decay option compatible sparse gradients grad = grad add param alpha=weight_decay clr = lr + step - lr_decay grad is_sparse grad = grad coalesce update non-linear so indices must unique grad_indices = grad _indices grad_values = grad _values state_sum add_ _make_sparse grad grad_indices grad_values pow std = state_sum sparse_mask grad std_values = std _values sqrt_ add_ eps param add_ _make_sparse grad grad_indices grad_values std_values alpha=-clr is_complex = torch is_complex param is_complex grad = torch view_as_real grad state_sum = torch view_as_real state_sum param = torch view_as_real param state_sum addcmul_ grad grad value= differentiable std = state_sum sqrt + eps std = state_sum sqrt add_ eps param addcdiv_ grad std value=-clr is_complex param = torch view_as_complex param state_sum = torch view_as_complex state_sum _multi_tensor_adagrad params list Tensor grads list Tensor state_sums list Tensor state_steps list Tensor grad_scale Optional Tensor found_inf Optional Tensor lr float weight_decay float lr_decay float eps float has_sparse_grad bool maximize bool differentiable bool has_complex bool differentiable raise AssertionError _foreach ops don t support autograd grad_scale None found_inf None raise AssertionError Expected grad_scale found_inf None Foreach functions will throw errors given empty lists len params == lr = _to_scalar lr grouped_tensorlists = Optimizer _group_tensors_by_device_and_dtype params grads state_sums state_steps type ignore list-item device_params_ device_grads_ device_state_sums_ device_state_steps_ _ grouped_tensorlists values device_params = cast list Tensor device_params_ device_grads = cast list Tensor device_grads_ device_state_sums = cast list Tensor device_state_sums_ device_state_steps = cast list Tensor device_state_steps_ device_has_sparse_grad = has_sparse_grad any grad is_sparse grad device_grads device_has_sparse_grad _single_tensor_adagrad device_params device_grads device_state_sums device_state_steps lr=lr weight_decay=weight_decay lr_decay=lr_decay eps=eps has_sparse_grad=True maximize=maximize differentiable=differentiable has_complex=has_complex grad_scale=grad_scale found_inf=found_inf continue Handle complex parameters has_complex _view_as_real device_params device_grads device_state_sums maximize device_grads = torch _foreach_neg device_grads type ignore assignment Update steps If steps CPU foreach will fall back slow path which for-loop calling t add over over will then wrapped into Tensor over over again which slower than we just wrapped once now The alpha required assure we go right overload torch compiler is_compiling device_state_steps is_cpu torch _foreach_add_ device_state_steps torch tensor device= cpu alpha= torch _foreach_add_ device_state_steps weight_decay = Reuse intermediate memory device_grads already allocated maximize maximize torch _foreach_add_ device_grads device_params alpha=weight_decay device_grads = torch _foreach_add type ignore assignment device_grads device_params alpha=weight_decay minus_clr = -lr + _get_value step - lr_decay step device_state_steps torch _foreach_addcmul_ device_state_sums device_grads device_grads value= std = torch _foreach_sqrt device_state_sums torch _foreach_add_ std eps weight_decay = maximize Again reuse intermediate memory device_grads already allocated torch _foreach_mul_ device_grads minus_clr numerator = device_grads numerator = torch _foreach_mul device_grads minus_clr type ignore assignment torch _foreach_addcdiv_ device_params numerator std _fused_adagrad params list Tensor grads list Tensor state_sums list Tensor state_steps list Tensor grad_scale Optional Tensor found_inf Optional Tensor lr float weight_decay float lr_decay float eps float has_sparse_grad bool maximize bool differentiable bool has_complex bool - None params has_sparse_grad has_complex raise RuntimeError ` fused ` does support sparse grad complex param differentiable raise RuntimeError adagrad fused=True does support differentiable=True lr = _to_scalar lr grad_scale_dict = grad_scale device grad_scale grad_scale None None found_inf_dict = found_inf device found_inf found_inf None None grouped_tensors = Optimizer _group_tensors_by_device_and_dtype params grads state_sums state_steps type ignore list-item device _ device_params_ device_grads_ device_state_sums_ device_state_steps_ _ grouped_tensors items device_params = cast list Tensor device_params_ device_grads = cast list Tensor device_grads_ device_state_sums = cast list Tensor device_state_sums_ device_state_steps = cast list Tensor device_state_steps_ device_grad_scale device_found_inf = None None grad_scale None grad_scale_dict None device grad_scale_dict grad_scale_dict device = grad_scale device non_blocking=True type ignore index device_grad_scale = grad_scale_dict device type ignore index found_inf None found_inf_dict None found_inf found_inf_dict found_inf_dict device = found_inf device non_blocking=True type ignore index device_found_inf = found_inf_dict device type ignore index torch _foreach_add_ device_state_steps torch _fused_adagrad_ device_params device_grads device_state_sums device_state_steps lr=lr lr_decay=lr_decay weight_decay=weight_decay eps=eps maximize=maximize grad_scale=device_grad_scale found_inf=device_found_inf device_found_inf None torch _foreach_sub_ device_state_steps device_found_inf len device_state_steps