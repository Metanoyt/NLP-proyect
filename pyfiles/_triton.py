functools hashlib typing Any functools cache has_triton_package - bool try triton noqa F True except ImportError False functools cache get_triton_version fallback tuple int int = - tuple int int try triton major minor = tuple int v v triton __version__ split major minor except ImportError fallback functools cache _device_supports_tma - bool torch torch cuda is_available torch cuda get_device_capability = torch version hip functools cache has_triton_experimental_host_tma - bool has_triton_package _device_supports_tma try triton tools experimental_descriptor noqa F create_ d_tma_descriptor create_ d_tma_descriptor True except ImportError pass False functools cache has_triton_tensor_descriptor_host_tma - bool has_triton_package _device_supports_tma try triton tools tensor_descriptor noqa F TensorDescriptor True except ImportError pass False functools cache has_triton_tma - bool has_triton_tensor_descriptor_host_tma has_triton_experimental_host_tma functools cache has_triton_tma_device - bool has_triton_package torch torch cuda is_available torch cuda get_device_capability = torch version hip torch xpu is_available old API try triton language extra cuda noqa F experimental_device_tensormap_create d experimental_device_tensormap_create d True except ImportError pass new API try triton language make_tensor_descriptor noqa F True except ImportError pass False functools cache has_datacenter_blackwell_tma_device - bool torch torch cuda is_available torch cuda get_device_capability = torch cuda get_device_capability torch version hip has_triton_tma_device has_triton_tensor_descriptor_host_tma False functools lru_cache None has_triton_stable_tma_api - bool has_triton_package torch torch cuda is_available torch cuda get_device_capability = torch version hip torch xpu is_available try triton language make_tensor_descriptor noqa F True except ImportError pass False functools cache has_triton - bool has_triton_package False torch _inductor config triton_disable_device_detection triton_disable_device_detection False torch _dynamo device_interface get_interface_for_device cuda_extra_check device_interface Any - bool device_interface Worker get_device_properties major = cpu_extra_check device_interface Any - bool triton backends cpu triton backends backends _return_true device_interface Any - bool True triton_supported_devices = cuda cuda_extra_check xpu _return_true cpu cpu_extra_check mtia _return_true is_device_compatible_with_triton - bool device extra_check triton_supported_devices items device_interface = get_interface_for_device device device_interface is_available extra_check device_interface True False is_device_compatible_with_triton functools cache triton_backend - Any triton compiler compiler make_backend triton runtime driver driver target = driver active get_current_target make_backend target functools cache triton_hash_with_backend - str torch _inductor runtime triton_compat triton_key backend = triton_backend key = f triton_key - backend hash Hash upper case so can t contain any Python keywords hashlib sha key encode utf- hexdigest upper