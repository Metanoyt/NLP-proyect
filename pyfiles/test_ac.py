Owner s oncall pt random unittest math prod torch torch _functorch config config torch testing _internal common_utils run_tests TEST_WITH_ROCM TestCase torch testing _internal inductor_utils HAS_CUDA_AND_TRITON torch utils _triton has_triton torch utils checkpoint checkpoint torch utils flop_counter FlopCounterMode register_flop_formula has_triton note we only triton test test fails relu_kernel_ inp_ptr out_ptr sz BLOCK_SIZE tl constexpr NameError tl defined triton triton language tl compile_with_ac f memory_budget torch compile f backend= aot_eager_decomp_partition get_act_mem f out = f out backward start_mem = torch cuda memory_stats requested_bytes all current out = f cur_mem = torch cuda memory_stats requested_bytes all current act_mem = cur_mem - start_mem out backward act_mem get_bw_flops f Normalized so square matmul returns f backward out = f FlopCounterMode display=False mode out backward mode get_total_flops create_pair B_I O results B_I O memory requires B_I B_I O flops arithmetic intensity B_I x = torch randn B_I B_I requires_grad=True w = torch randn B_I O requires_grad=True x w get_mem_and_flops f memory_budget=None Returns megabytes rounded decimal point FLOPs Note each value size torch float MiB torch _dynamo reset config patch activation_memory_budget=memory_budget memory_budget None f = torch compile f backend= aot_eager_decomp_partition We round nearest th megabyte round get_act_mem f get_bw_flops f MemoryBudgetTest TestCase setUp super setUp torch set_default_device cuda test_rematerializes_cheap f x w x = x cos x = torch mm x w x sum x = torch randn requires_grad=True w = torch randn requires_grad=True call f x w eager_mem eager_flops = get_mem_and_flops call assertEqual eager_mem mem_ flops_ = get_mem_and_flops call memory_budget= Recomputing ` cos ` free here assertEqual mem_ assertEqual eager_flops flops_ mem_ flops_ = get_mem_and_flops call memory_budget= We can just recompute ` x cos ` here only depend inputs assertEqual mem_ assertEqual flops_ eager_flops test_matmul_even_chain f x ws x = x cos w ws x = torch mm x w cos x sum x = torch randn requires_grad=True ws = torch randn requires_grad=True _ range call f x ws _ eager_flops = get_mem_and_flops call budget range mem flops = get_mem_and_flops call memory_budget=budget budget = We start saving matmuls assertEqual mem budget assertEqual flops eager_flops + - budget budget We re only recomputing ` cos ` operations assertEqual mem assertEqual flops eager_flops budget == assertEqual mem assertEqual flops eager_flops test_matmul_uneven_chain This function constructed so we saving one input size in_dim each w In addition every matmul has same ratio compute memory saved so test essentially testing our knapsack solving f x ws xs = torch mm x w cos w ws sum x sum x xs x = torch randn requires_grad=True make_weights w_shapes ws = dim w_shapes ws append torch randn dim requires_grad=True ws weight_configs = + + + + + + + + + + + + + + + + + + + + random seed random_arr = random randint _ range exact_sums = i range random shuffle random_arr exact_sums append sum random_arr i weight_configs append random_arr exact_sums weight_shapes exact_solves weight_configs ws = make_weights weight_shapes call f x ws eager_mem _ = get_mem_and_flops call total_mem = sum weight_shapes assertEqual eager_mem sum weight_shapes mem_achieved exact_solves mem _ = get_mem_and_flops call memory_budget=mem_achieved total_mem assertEqual mem mem_achieved needs CUDA test file all needs CUDA unittest skipIf has_triton test needs triton test_custom_triton_kernel triton jit relu_kernel_ inp_ptr out_ptr sz BLOCK_SIZE tl constexpr pid = tl program_id block = tl arange BLOCK_SIZE + pid BLOCK_SIZE msk = block sz inp = tl load inp_ptr + block mask=msk relu = tl where inp inp tl store out_ptr + block relu mask=msk torch _library triton_op testac triton_relu mutates_args= triton_relu x torch Tensor - torch Tensor y = torch empty_like x sz = y numel BLOCK_SIZE = grid = triton cdiv sz BLOCK_SIZE torch _library capture_triton relu_kernel_ grid x y sz BLOCK_SIZE y torch _library triton_op testac triton_relu_backward mutates_args= triton_relu_backward grad_out torch Tensor - torch Tensor grad_x = torch empty_like grad_out sz = grad_out numel BLOCK_SIZE = grid = triton cdiv sz BLOCK_SIZE I know wrong whatever torch _library capture_triton relu_kernel_ grid grad_out grad_x sz BLOCK_SIZE grad_x _triton_relu_backward ctx grad_out torch Tensor - torch Tensor triton_relu_backward grad_out _triton_relu_setup_context ctx inputs output pass triton_relu register_autograd _triton_relu_backward setup_context=_triton_relu_setup_context register_flop_formula torch ops testac triton_relu torch ops testac triton_relu_backward triton_relu_flops inp_shape args kwargs prod inp_shape f x ws x = torch ops testac triton_relu x w ws x = torch ops testac triton_relu torch mm x w x sum x = torch randn requires_grad=True device= cuda ws = torch randn requires_grad=True device= cuda _ range call f x ws expected = call budget range memory_budget = budget torch _dynamo reset config patch activation_memory_budget=memory_budget memory_budget None f_compile = torch compile call backend= aot_eager_decomp_partition assertEqual expected f_compile test_prioritize_cheaper_matmul f xs ws xs = torch mm x w cos x w zip xs ws sum x sum x xs x w = create_pair x w = create_pair call f x x w w eager_mem eager_flops = get_mem_and_flops call assertEqual eager_mem assertEqual eager_flops comp_mem comp_flops = get_mem_and_flops call memory_budget= assertEqual comp_mem We recomputing x w here assertEqual comp_flops eager_flops + config patch activation_memory_budget_runtime_estimator= profile test_profile f x ws x = x cos w ws x = torch mm x w cos x sum x = torch randn requires_grad=True ws = torch randn requires_grad=True _ range call f x ws _ eager_flops = get_mem_and_flops call mem flops = get_mem_and_flops call memory_budget= We start saving matmuls assertEqual mem assertEqual flops eager_flops + test_prioritize_cheaper_matmul f xs ws xs = torch mm x w cos x w zip xs ws sum x sum x xs data = xs ws = zip create_pair b b data call f xs ws eager_mem eager_flops = get_mem_and_flops call assertEqual eager_mem assertEqual eager_flops mem flops = get_mem_and_flops call memory_budget= eager_mem Save w w assertEqual mem We re recomputing w cheap one assertEqual flops - eager_flops mem flops = get_mem_and_flops call memory_budget= eager_mem Save w Note even though saving w gets us closer our memory limit w actually more FLOPs than w assertEqual mem assertEqual flops - eager_flops + test_attention_vs_linear f x w orig_shape = x shape x = x reshape x shape x shape I know isn t technically right lol x = torch nn functional scaled_dot_product_attention x x x is_causal=False reshape orig_shape x = torch mm x w x = x cos x sum try_seq_length S D expected_recompute x = torch randn S D requires_grad=True w = torch randn D D requires_grad=True call f x w FlopCounterMode display=False mode call mm_flops = mode get_flop_counts Global torch ops aten mm attn_flops = mode get_total_flops - mm_flops mm_flops = attn_flops = eager_mem eager_flops = get_mem_and_flops call assertEqual eager_mem S D mem flops = get_mem_and_flops call memory_budget= Force recompute one mm attn assertEqual mem S D expected_recompute == attn expected_flops = attn_flops expected_flops = mm_flops assertEqual flops - eager_flops expected_flops General behind test sequence length D then attention more expensive than linear try_seq_length mm try_seq_length attn try_seq_length mm try_seq_length mm try_seq_length attn try_seq_length mm try_seq_length attn test_manual_ac test manual checkpoint boundaries respected when autoac set f x tmp = torch matmul x x T tmp = torch matmul tmp tmp tmp = torch matmul tmp tmp out = torch matmul tmp x out g x x = checkpoint f x use_reentrant=False x = checkpoint f x use_reentrant=False x x = torch randn requires_grad=True call g x sum eager_mem eager_flops = get_mem_and_flops call give memory budget logic value should cause run recompute matmuls mem flops = get_mem_and_flops call memory_budget= assertEqual mem eager_mem assertEqual flops eager_flops __name__ == __main__ I m using cuda memory allocator verify memory allocations HAS_CUDA_AND_TRITON TEST_WITH_ROCM run_tests