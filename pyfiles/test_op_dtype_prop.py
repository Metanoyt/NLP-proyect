Owner s module inductor importlib os re sys torch torch _dynamo utils disable_cache_limit torch _inductor config torch _inductor codegen triton OpDtypeSupport torch _inductor test_case TestCase InductorTestCase torch _inductor utils run_and_get_code run_and_get_triton_code triton_type torch fx operator_schemas get_signature_for_torch_op torch testing FileCheck torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_methods_invocations op_db torch testing _internal common_utils parametrize torch testing _internal inductor_utils GPU_TYPE requires_gpu Make helper files test importable pytorch_test_dir = os path dirname os path dirname os path realpath __file__ sys path append pytorch_test_dir importlib import_module functorch importlib import_module filelock torch _inductor lowering lowerings torch testing _internal common_device_type ops torch testing _internal inductor_utils HAS_GPU unique_pointwise_op_names = set op lowerings isinstance op torch _ops OpOverload continue torch Tag pointwise op tags continue op _schema is_mutable continue op_name = op name split - split unique_pointwise_op_names add op_name pointwise_ops = op op op_db op name unique_pointwise_op_names reduction op variant_test_name TestCase InductorTestCase ops pointwise_ops allowed_dtypes= torch float torch float torch int torch int torch bool config patch triton codegen_upcast_to_fp False TODO enable config patch test_configs runtime_triton_dtype_assert True config patch test_configs runtime_triton_shape_assert True config patch test_configs static_cpp_dtype_assert True disable_cache_limit test_op_dtype_propagation op dtype run op args kwargs op args kwargs sample_inputs_itr = op sample_inputs GPU_TYPE dtype requires_grad=False sample_input sample_inputs_itr args = sample_input input + sample_input args kwargs = sample_input kwargs out = run op get_op args kwargs test_configs runtime_triton_dtype_assert does work well dynamic shape so far Consider following cases torch add both lhs rhs int tensor there also integer alpha argument In dynamic shape case alpha passed ks argument To safe we use tl int ks s dtype But dtype alpha also decided tl int during lowering when we promote alpha ir Constant Ideally resolve problem we should track assignment like alpha = ks so we know alpha actually tl int rather than tl int out_c = torch compile run dynamic=False op get_op args kwargs assertEqual out out_c requires_gpu parametrize upcast_to_fp False True config patch triton use_block_ptr True test_codegen_upcast_to_fp upcast_to_fp torch compile func b c d b c d inps = torch rand device=GPU_TYPE dtype=torch float config patch triton codegen_upcast_to_fp upcast_to_fp func_opt = torch compile func backend= inductor code = run_and_get_triton_code func_opt inps fp _cast_in_code = tl float code assertEqual fp _cast_in_code upcast_to_fp requires_gpu parametrize input_shape parametrize reduction_func torch prod torch sum torch argmax torch argmin torch min torch max parametrize input_dtype torch float torch bfloat config patch triton use_block_ptr True test_low_precision_reduction input_shape reduction_func input_dtype torch compile func b c d reduction_func b c d inps = torch rand input_shape device=GPU_TYPE dtype=input_dtype config patch triton codegen_upcast_to_fp False func_opt = torch _dynamo optimize inductor func code = run_and_get_triton_code func_opt inps assertTrue tl float code assertEqual func inps func_opt inps test_op_dtype_support Triton codegen upcasts values float certain ops Check those ops have accurate dtype information op_name rsqrt sqrt isnan floor ceil tan atan atanh sigmoid log log cosh sinh acosh asinh asin acos asinh erf lgamma sin cos exp expm exp abs hypot nextafter These ops do support float bfloat supported_dtypes = OpDtypeSupport supported_dtypes op_name assertNotIn torch float supported_dtypes assertNotIn torch bfloat supported_dtypes These ops should support float float assertIn torch float supported_dtypes assertIn torch float supported_dtypes requires_gpu parametrize op_name OpDtypeSupport supported_dtypes parametrize load_upcast_to_fp False True parametrize input_dtype torch float torch bfloat config patch triton use_block_ptr True test_dtype_aware_codegen op_name str load_upcast_to_fp input_dtype Test dtype aware codegen some tl math libdevice calls Operands should upcast float output should downcast float Check op s output should upcasted downcasted supported_dtypes = OpDtypeSupport supported_dtypes op_name convert_output = OpDtypeSupport convert_outputs op_name assertNotIn input_dtype supported_dtypes Retrieve corresponding torch op torch_op_name = op_name removeprefix libdevice_ op = getattr torch torch_op_name Edge case torch round maps libdevice nearbyint triton_op_name_overrides = round nearbyint torch sqrt lowers tl sqrt_rn after switching away libdevice sqrt sqrt sqrt_rn override = triton_op_name_overrides get op_name triton_op_name = override override None torch_op_name Get number args op Take minimum over all signatures isolate required args signatures = get_signature_for_torch_op op num_args = min len signature parameters signature signatures Test codegen check casts inps = torch rand device=GPU_TYPE dtype=input_dtype num_args tl_dtype_str = str input_dtype replace torch tl config patch triton codegen_upcast_to_fp load_upcast_to_fp compiled = torch compile op backend= inductor code = run_and_get_triton_code compiled inps Search code regex Example code libdevice floor tmp tl float tl float output_cast = rf \ to\ tl_dtype_str \ convert_output pattern = rf triton_op_name \ \ to\ tl\ float \ \ output_cast cast_in_code = re search pattern code re MULTILINE None assertNotEqual cast_in_code load_upcast_to_fp config patch triton codegen_upcast_to_fp False test_binary_math_mixed_precision Test binary math operator where only one input needs upcast Create inputs different dtypes inputs = torch randn device=GPU_TYPE dtype=dtype dtype torch float torch float func = torch hypot compiled = torch compile backend= inductor func result code = run_and_get_code compiled inputs Check accuracy ref = func inputs assertTrue torch allclose ref result Check exactly one upcast num_upcasts = code count tl float assertEqual num_upcasts There should no downcast since input promoted float assertNotIn tl float code config patch test_configs static_cpp_dtype_assert True config patch test_configs runtime_triton_dtype_assert True config patch test_configs runtime_triton_shape_assert True config patch triton codegen_upcast_to_fp False test_downcast_div_mod fn x y x y x y x y = torch rand dtype=torch float device=GPU_TYPE _ range out code = run_and_get_code torch compile fn x y FileCheck check static_assert check_same dtype run code assertEqual fn x y out config patch test_configs static_cpp_dtype_assert True config patch test_configs runtime_triton_dtype_assert True config patch test_configs runtime_triton_shape_assert True test_constant fn torch full device=GPU_TYPE dtype=torch float out code = run_and_get_code torch compile fn FileCheck check static_assert check_same dtype run code assertEqual fn out config patch test_configs runtime_triton_dtype_assert True config patch test_configs runtime_triton_shape_assert True config patch test_configs static_cpp_dtype_assert True config patch triton persistent_reductions False test_any fn x torch any x x = torch rand device=GPU_TYPE torch bool out code = run_and_get_code torch compile fn x assertEqual fn x out config patch test_configs runtime_triton_dtype_assert True config patch test_configs runtime_triton_shape_assert True config patch test_configs static_cpp_dtype_assert True test_assoc_scan torch _higher_order_ops associative_scan associative_scan x = torch randn device=GPU_TYPE dtype check correctly associative_scan lambda acc curr acc + torch abs curr x dim=- combine_mode= pointwise parametrize upcast_to_fp False True parametrize dtype torch float torch bfloat test_upcast_rank_ _cpu dtype torch dtype upcast_to_fp bool Test whether we implicitly upcast CPU tensors rank float Test broadcasting rank- CPU tensor rank x = torch randn dtype=dtype device= cpu y = torch randn dtype=dtype device=GPU_TYPE assertEqual len x shape assertEqual len y shape inps = x y func = torch add config patch triton codegen_upcast_to_fp upcast_to_fp compiled = torch compile func result code = run_and_get_code compiled inps Check numerics ref = func inps assertTrue torch allclose result ref Inductor upcasts CPU arguments rank float Check downcast original dtype num_downcasts = code count f triton_type dtype assertEqual num_downcasts upcast_to_fp instantiate_device_type_tests TestCase globals only_for= cuda xpu allow_xpu=True __name__ == __main__ torch _inductor test_case run_tests HAS_GPU run_tests needs= filelock