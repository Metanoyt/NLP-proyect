mypy allow-untyped-defs logging operator typing Optional Union torch torch export _trace torch _ops OpOverload torch ao quantization fx _decomposed dequantize_per_channel dequantize_per_tensor quantize_per_tensor torch ao quantization utils calculate_qmin_qmax torch fx graph_module _assign_attr log = logging getLogger __name__ Those values will need carried over multiple operators _INPUT_Q_DTYPE Optional Union torch dtype torch fx Node = None _SCALE Optional Union float torch fx Node = None _ZERO_POINT Optional Union float torch fx Node = None int_to_valid_dtype val int - torch dtype torch _export converter _TORCH_ENUM_TO_DTYPE No circular isinstance val torch dtype val dtype = _TORCH_ENUM_TO_DTYPE val dtype == torch quint torch uint dtype == torch qint torch int dtype fx_enum_to_dtype gm torch fx GraphModule val int - torch fx Node gm graph call_function int_to_valid_dtype val insert_quantized_node gm torch fx GraphModule val_node torch fx Node scale_node Union float torch fx Node zero_point_node Union float torch fx Node qmin_node Union float int torch fx Node qmax_node Union float int torch fx Node dtype_node Union torch dtype torch fx Node qscheme Optional torch qscheme - torch fx Node gm graph call_function quantize_per_tensor val_node scale_node zero_point_node qmin_node qmax_node dtype_node get_dequantized val torch Tensor scale Union float torch Tensor zero_point Union float torch Tensor qmin Union float int qmax Union float int dtype torch dtype axis Optional int qscheme Optional torch qscheme - torch Tensor qscheme torch per_tensor_affine dequantize_per_tensor val scale type ignore arg-type zero_point type ignore arg-type qmin type ignore arg-type qmax type ignore arg-type dtype qscheme torch per_channel_affine dequantize_per_channel val scale type ignore arg-type zero_point type ignore arg-type axis type ignore arg-type qmin type ignore arg-type qmax type ignore arg-type dtype raise RuntimeError f Unsupported dequantization scheme qscheme insert_dequantized_node gm torch fx GraphModule val_node torch fx Node scale_node Union float torch fx Node zero_point_node Union float torch fx Node qmin_node Union float int torch fx Node qmax_node Union float int torch fx Node dtype_node Union torch dtype torch fx Node axis_node Optional Union int torch fx Node qscheme Optional torch qscheme - torch fx Node qscheme torch per_tensor_affine gm graph call_function dequantize_per_tensor val_node scale_node zero_point_node qmin_node qmax_node dtype_node qscheme torch per_channel_affine gm graph call_function dequantize_per_channel val_node scale_node zero_point_node axis_node qmin_node qmax_node dtype_node raise RuntimeError f Unsupported dequantization scheme qscheme get_qmin_qmax dtype torch dtype - tuple Union int float Union int float calculate_qmin_qmax None None False dtype False type ignore arg-type insert_qmin_qmax_node gm torch fx GraphModule dtype_node Union torch dtype torch fx Node - tuple torch fx Node torch fx Node q_min_max_node = gm graph call_function calculate_qmin_qmax None None False dtype_node False qmin_node = gm graph call_function operator getitem q_min_max_node qmax_node = gm graph call_function operator getitem q_min_max_node qmin_node qmax_node get_script_object gm torch nn Module node torch fx Node - torch _C ScriptObject assert isinstance node torch fx Node assert node op == get_attr attr_name = node target assert isinstance attr_name str mod = gm attr attr_name split mod = getattr mod attr assert isinstance mod torch _C ScriptObject mod insert_weight_and_bias_get_attr_node_from_get_attr_to_scriptobject gm torch fx GraphModule param_node torch fx Node - tuple torch fx Node Optional torch fx Node Directly inline tensor get_attr fx node mod = get_script_object gm param_node w_qtensor b_qtensor = mod unpack type ignore attr-defined w_attr_name b_attr_name = f dequantized_ param_node target _w f dequantized_ param_node target _b insert_weight_and_bias_get_attr_node gm w_qtensor b_qtensor w_attr_name b_attr_name insert_weight_and_bias_get_attr_node_from_get_attr_to_qtensor gm torch fx GraphModule get_attr_to_weight_node torch fx Node get_attr_to_bias_node Optional torch fx Node - tuple torch fx Node Optional torch fx Node assert isinstance get_attr_to_weight_node target str w_qtensor = getattr gm get_attr_to_weight_node target w_attr_name = f dequantized_ get_attr_to_weight_node target _w get_attr_to_bias_node None assert isinstance get_attr_to_bias_node target str b_qtensor = getattr gm get_attr_to_bias_node target b_attr_name = f dequantized_ get_attr_to_bias_node target _b b_qtensor b_attr_name = None insert_weight_and_bias_get_attr_node gm w_qtensor b_qtensor w_attr_name b_attr_name insert_weight_and_bias_get_attr_node gm torch fx GraphModule w_qtensor torch Tensor b_qtensor Optional torch Tensor w_attr_name str b_attr_name str - tuple torch fx Node Optional torch fx Node w_tensor = get_tensor_from_qtensor w_qtensor _assign_attr w_tensor gm w_attr_name w_tensor_attr = gm graph get_attr w_attr_name b_qtensor None b_tensor = get_tensor_from_qtensor b_qtensor dequant=False _assign_attr b_tensor gm b_attr_name b_tensor_attr = gm graph get_attr b_attr_name b_tensor_attr = None w_tensor_attr b_tensor_attr get_tensor_from_qtensor qtensor torch Tensor dequant bool = True - torch Tensor Manual conversion because qint used anymore qtensor dtype torch qint torch quint tensor = qtensor int_repr tensor = qtensor Weights need dequantization scaling zero_point adjustment bias does need dequant qscheme = qtensor qscheme qscheme == torch per_channel_affine scale zero_point axis = qtensor q_per_channel_scales qtensor q_per_channel_zero_points qtensor q_per_channel_axis scale zero_point axis = qtensor q_scale type ignore assignment qtensor q_zero_point type ignore assignment None dtype = tensor dtype qmin qmax = get_qmin_qmax dtype get_dequantized tensor scale zero_point qmin qmax dtype axis qscheme tensor insert_fused_activation_node gm torch fx GraphModule opname str fx_node torch fx Node - torch fx Node opname conv d_relu conv d_relu linear_relu add_relu mul_relu fx_node = gm graph call_function torch ops aten relu fx_node fx_node _conv d_op_with_squeeze inp torch Tensor weight torch Tensor bias Optional torch Tensor stride list int padding list int dilation list int groups int - torch Tensor In quantized version conv d emulated using conv d squeeze unsqueeze operations before after conv d operation match dimension weights Reference https github com pytorch pytorch blob eca cb fbe bb fa afe bceecd c aten src ATen native quantized cpu qconv cpp#L noqa B s_inp = torch ops aten unsqueeze inp conv d_res = torch ops aten conv d s_inp weight bias stride padding dilation groups uns_conv d_res = torch ops aten squeeze conv d_res uns_conv d_res _transform_conv_with_packedparam gm torch fx GraphModule node torch fx Node Conv specific transformation function assert isinstance node target torch _ops OpOverload opname = node target _opname scale_node zero_point_node = node args node args op_f = torch ops aten conv d opname conv d conv d_relu _conv d_op_with_squeeze inp_node param_node = node args node args assert isinstance inp_node torch fx Node assert isinstance param_node torch fx Node param_node op == call_function Using Conv dPrepackParam conv_prepack We directly skip packing call inline weights bias w_node b_node = param_node args param_node args assert isinstance w_node torch fx Node assert b_node None isinstance b_node torch fx Node param_ param_ = insert_weight_and_bias_get_attr_node_from_get_attr_to_qtensor gm w_node b_node op_res_node = gm graph call_function op_f inp_node param_ param_ param_node args Using ConvPrepackedParam param = get_script_object gm param_node param_ param_ = insert_weight_and_bias_get_attr_node_from_get_attr_to_scriptobject gm param_node type ignore assignment op_res_node = gm graph call_function op_f inp_node param_ param_ param stride type ignore attr-defined param padding type ignore attr-defined param dilation type ignore attr-defined param groups type ignore attr-defined op_res_node scale_node zero_point_node _transform_linear_with_packedparam gm torch fx GraphModule node torch fx Node Linear specific transformation function scale_node zero_point_node = node args node args inp_node param_node = node args node args assert isinstance inp_node torch fx Node assert isinstance param_node torch fx Node param_node op == call_function Using LinearPrepackParam linear_prepack We directly skip packing call inline weights bias w_node b_node = param_node args param_node args assert isinstance w_node torch fx Node assert b_node None isinstance b_node torch fx Node param_ param_ = insert_weight_and_bias_get_attr_node_from_get_attr_to_qtensor gm w_node b_node op_res_node = gm graph call_function torch ops aten linear inp_node param_ param_ param_node args Using LinearPackedParams param_ param_ = insert_weight_and_bias_get_attr_node_from_get_attr_to_scriptobject gm param_node type ignore assignment op_res_node = gm graph call_function torch ops aten linear inp_node param_ param_ op_res_node scale_node zero_point_node _transform_op_where_last_two_arguments_are_scale_and_zero_point gm torch fx GraphModule node torch fx Node This transformation function can used function where last two parameters scale zero point Additionally function s parameters do need any unpacking to_standard_op = mul torch ops aten mul mul_relu torch ops aten mul add torch ops aten add add_relu torch ops aten add softmax torch ops aten softmax cat torch ops aten cat hardswish torch ops aten hardswish assert isinstance node target torch _ops OpOverload opname args = node target _opname node args scale_node zero_point_node = args - args - op_res_node = gm graph call_function to_standard_op opname tuple args - op_res_node scale_node zero_point_node _transform_scalar_arithmetic gm torch fx GraphModule node torch fx Node Transform scalar overload basic arithmetic to_standard_op = mul torch ops aten mul Scalar add torch ops aten add Scalar assert isinstance node target torch _ops OpOverload opname args = node target _opname node args op_res_node = gm graph call_function to_standard_op opname args op_res_node _SCALE _ZERO_POINT _transform_prepacked_op gm torch fx GraphModule node torch fx Node Transformation functions under prepacked namespace where they share same handling logic OpContext contains all parameters assert isinstance node target torch _ops OpOverload opname args = node target _opname node args op_f = None opname == conv d_clamp_run op_f = torch ops aten conv d opname == linear_clamp_run op_f = torch ops aten linear raise RuntimeError f Invalid operator opname assert isinstance args torch fx Node so = get_script_object gm args func_args = func_args += args func_args += so unpack type ignore attr-defined opname == conv d_clamp_run func_args += torch ops prepacked unpack_prepacked_sizes_conv d so op_res_node = gm graph call_function op_f tuple func_args op_res_node _transform_batch_norm gm torch fx GraphModule node torch fx Node args = node args scale_node zero_point_node = args - args - op_res_node = gm graph call_function torch ops aten native_batch_norm args - False args - op_res_node = gm graph call_function operator getitem op_res_node op_res_node scale_node zero_point_node fx_transform_quantized_op_to_standard_op gm torch fx GraphModule node torch fx Node - torch fx Node global _SCALE _ZERO_POINT _INPUT_Q_DTYPE assert isinstance node target torch _ops OpOverload opname overload = node target _opname node target _overloadname key = f opname overload opname_to_transform_f = conv d new _transform_conv_with_packedparam conv d_relu new _transform_conv_with_packedparam conv d default _transform_conv_with_packedparam conv d_relu default _transform_conv_with_packedparam conv d new _transform_conv_with_packedparam conv d_relu new _transform_conv_with_packedparam conv d default _transform_conv_with_packedparam conv d_relu default _transform_conv_with_packedparam linear default _transform_linear_with_packedparam linear_relu default _transform_linear_with_packedparam add default _transform_op_where_last_two_arguments_are_scale_and_zero_point add_relu default _transform_op_where_last_two_arguments_are_scale_and_zero_point mul default _transform_op_where_last_two_arguments_are_scale_and_zero_point mul_relu default _transform_op_where_last_two_arguments_are_scale_and_zero_point softmax default _transform_op_where_last_two_arguments_are_scale_and_zero_point cat default _transform_op_where_last_two_arguments_are_scale_and_zero_point hardswish default _transform_op_where_last_two_arguments_are_scale_and_zero_point batch_norm d default _transform_batch_norm mul Scalar _transform_scalar_arithmetic add Scalar _transform_scalar_arithmetic f key opname_to_transform_f raise RuntimeError f Unsupported quantized op during transformation key op_res_node scale_node zero_point_node = opname_to_transform_f f key gm node Add fused activation layer op_res_node = insert_fused_activation_node gm opname op_res_node _SCALE _ZERO_POINT = scale_node zero_point_node assert _INPUT_Q_DTYPE None qmin_node qmax_node = insert_qmin_qmax_node gm _INPUT_Q_DTYPE q_fx_node = insert_quantized_node gm op_res_node scale_node zero_point_node qmin_node qmax_node _INPUT_Q_DTYPE torch per_tensor_affine dq_fx_node = insert_dequantized_node gm q_fx_node scale_node zero_point_node qmin_node qmax_node _INPUT_Q_DTYPE None torch per_tensor_affine dq_fx_node replace_quantized_ops_with_standard_ops gm torch fx GraphModule Replace legacy quantized ops aten quantize_per_tensor quantized conv PT ops quantize_decomposed quantize_per_tensor aten conv Before x &#124; &#124; - aten q &#124; &#124; - quantized conv d &#124; &#124; - quantized linear &#124; &#124; - aten dq &#124; &#124; - y After x &#124; &#124; - qd q - qd dq &#124; &#124; - aten conv d - qd q - qd dq &#124; &#124; aten linear - qd q - qd dq &#124; &#124; - y qd == quantized_decomposed library q = quantize dq = dequantize ^ &#124; getattr w getattr b Conv dParamPrepack During each iteration transformation spits out transformed operator its quantized output its dequantized value together We did because dequantization need use scale zero point parameters quantization recover approximate original value After each iteration new dequantization node will used input next node e g dq - linear For operators like conv d linear their weights bias packed quantized format ScriptObject During transformation we unpack those objects get their dequantized tensor populate those attributes module use getattr access them One exception transformation conv_prepack linear_prepack Those calls pack weight bias constant tensors into ScriptObject which then used subsequent conv d linear calls During transformation we directly skip transforming conv_prepack linear_prepack We check whether ScriptObject quantized conv d linear conv_prepack linear_prepack If we then inline those parameters operator converting them getattr fx node For prepacked conv d_clamp_run prepacked linear_clamp_run we directly convert them aten conv d aten linear without need doing de quantization Three global variables defined _INPUT_Q_DTYPE _SCALE _ZERO_POINT _INPUT_Q_DTYPE determines de quantization data type which same across entire program only shows up very first quantization call _SCALE _ZERO_POINT used only when operators do have those specified E g mul Scalar global _INPUT_Q_DTYPE quantized = False last_quantized_node = None pyrefly ignore bad-assignment node gm graph nodes isinstance node target OpOverload gm graph inserting_before node namespace opname = node target namespace node target _opname namespace == quantized opname conv_prepack linear_prepack quantized = True fx_node = fx_transform_quantized_op_to_standard_op gm node node replace_all_uses_with fx_node last_quantized_node = fx_node namespace == prepacked quantized = True fx_node = _transform_prepacked_op gm node node replace_all_uses_with fx_node last_quantized_node = fx_node namespace == aten opname == quantize_per_tensor inp_node scale_node zero_point_node dtype_node = node args dtype_node = fx_enum_to_dtype gm dtype_node _INPUT_Q_DTYPE = dtype_node qmin_node qmax_node = insert_qmin_qmax_node gm dtype_node q_fx_node = insert_quantized_node gm inp_node scale_node zero_point_node qmin_node qmax_node dtype_node torch per_tensor_affine dq_fx_node = insert_dequantized_node gm q_fx_node scale_node zero_point_node qmin_node qmax_node dtype_node None torch per_tensor_affine node replace_all_uses_with dq_fx_node last_quantized_node = dq_fx_node namespace == aten opname == dequantize assert last_quantized_node None node replace_all_uses_with last_quantized_node last_quantized_node = node Post-processing again remove legacy ScriptObjects quantizated tensors stored attributes buffer This used clean up GraphModule trigger tracing errors like missing __obj_flatten__ functions _clean_attr mod torch nn Module submod mod modules attr_names_to_clean = set k v submod __dict__ items isinstance v torch ScriptObject attr_names_to_clean add k k == _buffers buffer_name_to_clean = set pyrefly ignore missing-attribute b_name b_value v items isinstance b_value torch Tensor b_value dtype torch qint torch quint buffer_name_to_clean add b_name b_name buffer_name_to_clean pyrefly ignore missing-attribute v pop b_name None attr_name attr_names_to_clean delattr submod attr_name quantized TODO SetAttr + quantized ops will result incorrect program This flag used temporarily bypass test cases The deadcode elimination pass needed remove legacy quantized ops Otherwise retracing will throw errors However current way SetAttr does inplace update attributes so pass regard them dead code remove them Below example GraphModule before after dead code elimination pass GraphModule torch nn Module forward x_ No stacktrace found following nodes data = data data = None data_ = data add_tensor = torch ops aten add Tensor data_ x_ alpha = data_ = None data_ = data copy_ = torch_Tensor_copy_ data_ add_tensor data_ = add_tensor = copy_ = None data_ = data add_tensor_ = torch ops aten add Tensor x_ data_ alpha = x_ = data_ = None add_tensor_ GraphModule torch nn Module forward x_ No stacktrace found following nodes data_ = data add_tensor_ = torch ops aten add Tensor x_ data_ alpha = x_ = data_ = None add_tensor_ gm graph eliminate_dead_code _clean_attr gm