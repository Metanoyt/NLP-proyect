mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates math collections abc Sequence dataclasses dataclass enum Enum typing cast Optional Union torch torch distributed device_mesh DeviceMesh torch distributed tensor _dtensor_spec DTensorSpec torch distributed tensor _op_schema OpSchema OpSpec OpStrategy PlacementList RuntimeSchemaInfo TupleStrategy torch distributed tensor _ops utils as_list expand_to_full_mesh_op_strategy generate_redistribute_costs is_tensor_evenly_shardable is_tensor_evenly_shardable_on_dim normalize_dim normalize_dims register_op_strategy torch distributed tensor _utils normalize_to_torch_size torch distributed tensor placement_types Partial Placement Replicate Shard aten = torch ops aten Reduction Enum NONE = MEAN = SUM = dataclass frozen=True NormReduction norm_type Union int float str ReductionOpType = Union NormReduction str dataclass frozen=True _NormPartial Partial This placement used partial vector norm For p-norms where p inf -inf p-norm over n elements computes sum_i x_i^p ^ p where sum i= n The reduction op p-norm itself For example consider ranks tensor sharded dim- -norm Rank t t &#124; Rank t t After computing -norm per gradient partial placement Rank sqrt t ^ + t ^ &#124; Rank sqrt t ^ + t ^ Converting partial replicate wants ultimately get Rank sqrt t ^ + t ^ + t ^ + t ^ This can achieved computing -norm each rank s result This holds similarly inf -inf norm For -norm reduction op sum norm_type Union int float str = __init__ norm_type Union int float str = reduce_op = None norm_type float inf inf reduce_op = max norm_type float -inf -inf reduce_op = min isinstance norm_type int float reduce_op = sum raise NotImplementedError f Unsupported norm type norm_type super __init__ reduce_op object __setattr__ norm_type norm_type _partition_value tensor torch Tensor mesh DeviceMesh mesh_dim int - torch Tensor For example consider ranks replicated tensor -norm Ranks sqrt t ^ + t ^ + t ^ To convert replicated partial we want f x such sqrt t ^ + t ^ + t ^ = sqrt f t ^ + f t ^ + f t ^ = sqrt sqrt f t ^ + f t ^ + f t ^ One such f x f x = x sqrt This generalizes d ranks p-norm f x = x d^ p reduce_op max min tensor reduce_op == sum norm_type == raise NotImplementedError f Unsupported norm type norm_type norm_type == tensor mesh size mesh_dim isinstance norm_type int float raise AssertionError f Expected int float got type norm_type tensor math pow mesh size mesh_dim norm_type raise NotImplementedError reduce_op _reduce_shard_value tensor torch Tensor mesh DeviceMesh mesh_dim int shard_spec Placement - torch Tensor isinstance shard_spec Shard raise AssertionError f Expected Shard got type shard_spec tensor = _pre_reduce_transform tensor reduced_tensor = super _reduce_shard_value tensor mesh mesh_dim shard_spec _post_reduce_transform reduced_tensor _reduce_value tensor torch Tensor mesh DeviceMesh mesh_dim int - torch Tensor tensor = _pre_reduce_transform tensor reduced_tensor = super _reduce_value tensor mesh mesh_dim _post_reduce_transform reduced_tensor _pre_reduce_transform tensor torch Tensor - torch Tensor reduce_op == sum isinstance norm_type int float raise AssertionError f Expected int float got type norm_type norm_type = norm_type = pyrefly ignore unsupported-operation tensor norm_type tensor _post_reduce_transform tensor torch Tensor - torch Tensor reduce_op == sum isinstance norm_type int float raise AssertionError f Expected int float got type norm_type norm_type = norm_type = pyrefly ignore unsupported-operation tensor norm_type tensor __eq__ other object - bool isinstance other _NormPartial False norm_type == other norm_type __hash__ - int + hash norm_type _infer_reduction_dims dims_arg object ndim int - Optional list int dims_arg None None dims = cast list int as_list dims_arg dims = cast list int normalize_dims dims ndim empty_dims = - ndim == dims_arg empty_dims None dims _infer_reduce_dims_map reduction_dims list int input_ndim int keep_dim=False - list int reduction_dims_map = new_dim_count = input_dim range input_ndim input_dim reduction_dims keep_dim input dim reduction dims mark - reduction_dims_map append - otherwise mark new dim reduction_dims_map append new_dim_count new_dim_count += reduction_dims_map _replicate_dims_start_at placements Sequence Placement start_dim int = - tuple Placement new_placements list Placement = p placements p is_partial isinstance p Shard p dim = start_dim new_placements append Replicate make replicate new_placements append p keep placement tuple new_placements new_placements which align placements skip skipped_dim _skip_dim placements tuple Placement skipped_dim int - tuple Placement new_placements list Placement = p placements isinstance p Shard p dim = skipped_dim new_placements append Shard p dim - new_placements append p tuple new_placements replicate_reduction_dims placements tuple Placement reduction_dims list int - tuple Placement replicate reduction dims reduction_linear new_placements list Placement = p placements p is_partial new_placements append Replicate isinstance p Shard p dim reduction_dims new_placements append Replicate new_placements append p tuple new_placements map_placements_after_reduction placements tuple Placement reduction_dims list int reduction_dims_map list int reduction_op ReductionOpType - tuple Placement Map each placement based output shape after reduction new_placements list Placement = placement placements isinstance placement Replicate Partial new_placements append placement isinstance placement Shard raise AssertionError f Expected Shard got type placement shard_dim = placement dim new_shard_dim = reduction_dims_map shard_dim new_shard_dim == - shard_dim reduction_dims new_shard_dim collapsed its reduction dims i e case where keepdims=True we generate partial new_placements append get_placement_from_reduction_op reduction_op new_placements append Shard new_shard_dim tuple new_placements get_placement_from_reduction_op reduction_op ReductionOpType - Placement isinstance reduction_op NormReduction _NormPartial norm_type=reduction_op norm_type Partial reduction_op common_reduction_strategy input_strategy OpStrategy reduce_dims list int keep_dim bool = False reduction_linear bool = True reduction_op ReductionOpType = sum - OpStrategy reduction_linear means reduction ` f ` follows rule f f f b = f b reduction linear should super set linearity default follow reduction input strategy reduction_strategy = OpStrategy op_spec input_strategy strategies reduction_op == avg output_spec = op_spec output_spec local_shape = list output_spec tensor_meta shape type ignore union-attr dim reduce_dims is_tensor_evenly_shardable_on_dim local_shape output_spec dim reduce avg linear unevenly sharded tensors reduction_linear = False break p op_spec output_spec placements when partial reduction op matches global reduction op we can delay redistribution i e max max isinstance p Partial p reduce_op = reduction_op reduction_linear = False break reduction_linear input placements strategy should clear out pending sum sharding reduction dimension input_placements = replicate_reduction_dims op_spec output_spec placements reduce_dims input_placements = op_spec output_spec placements input_spec = DTensorSpec mesh=input_strategy mesh placements=input_placements tensor_meta=op_spec output_spec tensor_meta reduce_dims_map = _infer_reduce_dims_map reduce_dims input_spec ndim keep_dim out_placements = map_placements_after_reduction input_spec placements reduce_dims reduce_dims_map reduction_op redistribute_cost = generate_redistribute_costs input_strategy input_spec reduction_strategy strategies append OpSpec output_specs=DTensorSpec mesh=input_strategy mesh placements=out_placements input_specs= input_spec redistribute_cost=redistribute_cost reduction_strategy LINEAR_REDUCTION_OP_MAP = aten all default product aten all dim product aten sum default sum aten sum dim_IntList sum aten any default sum aten any dim sum aten any out sum These only valid when there no padding aten prod default product aten prod dim_int product aten prod int_out product avg only linear when there no padding aten mean default avg aten mean dim avg aten mean out avg aten max default max aten max dim max aten max out max aten min default min aten min dim min aten min out min aten amax default max aten amax out max aten amin default min aten amin out min register_op_strategy list LINEAR_REDUCTION_OP_MAP keys schema_info=RuntimeSchemaInfo linear_reduction_strategy op_schema OpSchema - OpStrategy args_schema = op_schema args_schema input_strategy = args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy dims = None len op_schema args_schema dims = _infer_reduction_dims args_schema input_strategy ndim reduce_dims = list range input_strategy ndim dims None dims keep_dim = len op_schema args_schema bool op_schema args_schema reduction_op = LINEAR_REDUCTION_OP_MAP op_schema op common_reduction_strategy input_strategy reduce_dims keep_dim=keep_dim reduction_linear=True reduction_op=reduction_op register_op_strategy aten cumsum default schema_info=RuntimeSchemaInfo cumsum_strategy op_schema OpSchema - OpStrategy args_schema = op_schema args_schema input_strategy = args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy dim = args_schema isinstance dim int raise AssertionError f Expected int got type dim common_reduction_strategy input_strategy dim keep_dim=True reduction_linear=False register_op_strategy aten var correction aten var correction_out schema_info=RuntimeSchemaInfo keepdim var_reduction_strategy op_schema OpSchema - OpStrategy args_schema = op_schema args_schema input_strategy = args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy dims = None len op_schema args_schema dims = _infer_reduction_dims args_schema input_strategy ndim reduce_dims = list range input_strategy ndim dims None dims keep_dim = cast bool op_schema kwargs_schema get keepdim False common_reduction_strategy input_strategy reduce_dims keep_dim=keep_dim reduction_linear=False register_op_strategy aten linalg_vector_norm default schema_info=RuntimeSchemaInfo vector_norm_strategy op_schema OpSchema - OpStrategy args_schema = op_schema args_schema input_strategy = args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy norm_type = args_schema len args_schema isinstance norm_type int float str raise AssertionError f Expected int float str got type norm_type dim = args_schema len args_schema None keepdim = args_schema len args_schema False dims = _infer_reduction_dims dim input_strategy ndim reduce_dims = list range input_strategy ndim dims None dims reduction_linear = all all p is_partial p op_spec output_spec placements op_spec input_strategy strategies common_reduction_strategy input_strategy reduce_dims keep_dim=cast bool keepdim reduction_linear=reduction_linear reduction_op=NormReduction norm_type register_op_strategy aten _foreach_norm Scalar schema_info=RuntimeSchemaInfo needs_pytree=True foreach_norm_strategy op_schema OpSchema - TupleStrategy args_schema = op_schema args_schema input_tuple_strategy = args_schema isinstance input_tuple_strategy TupleStrategy raise AssertionError f Expected TupleStrategy got type input_tuple_strategy norm_type = args_schema len args_schema isinstance norm_type int float str raise AssertionError f Expected int float str got type norm_type output_tuple_strategy_children list OpStrategy = op_strategy input_tuple_strategy children isinstance op_strategy OpStrategy raise AssertionError f Expected OpStrategy got type op_strategy reduce_dims = list range op_strategy ndim reduction_linear = all all p is_partial p op_spec output_spec placements op_spec op_strategy strategies output_strategy = common_reduction_strategy op_strategy reduce_dims reduction_linear=reduction_linear reduction_op=NormReduction norm_type output_tuple_strategy_children append output_strategy TupleStrategy output_tuple_strategy_children register_op_strategy aten _linalg_svd default aten linalg_qr default TODO The diagonal ops can have improved sharding strategy shard placements does require redistributing replicate aten diagonal_copy default aten diag_embed default aten diag default aten diagonal default aten tril default aten triu default aten _linalg_eigh default aten upsample_bicubic d default aten upsample_bilinear d default aten upsample_linear d default aten upsample_nearest d default aten upsample_trilinear d default TODO support full F interpolate set options schema_info=RuntimeSchemaInfo linalg_replicate_strategy op_schema OpSchema - OpStrategy Since we do have simple way compute some linear algebra operations like SVD QR decomposition always fall back replicate args_schema = op_schema args_schema input_strategy = args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy mesh = input_strategy mesh output_strategies list OpSpec = placement_strategy input_strategy strategies replicate_placements = tuple Replicate _ range mesh ndim replicate_spec = DTensorSpec mesh=mesh placements=replicate_placements tensor_meta=placement_strategy output_spec tensor_meta redistribute_cost = generate_redistribute_costs input_strategy replicate_spec replicate_strategy = OpSpec output_specs=replicate_spec input_specs= replicate_spec redistribute_cost=redistribute_cost output_strategies append replicate_strategy OpStrategy output_strategies register_op_strategy aten _log_softmax default aten _softmax default aten _safe_softmax default schema_info=RuntimeSchemaInfo softmax_strategy op_schema OpSchema - OpStrategy input_strategy softmax_dim _ = op_schema args_schema input_strategy = cast OpStrategy input_strategy softmax_dim = cast int softmax_dim softmax_dim = normalize_dim softmax_dim input_strategy ndim output_strategy = OpStrategy input_placement_strategy input_strategy strategies redistribute_costs = input_src_spec = input_placement_strategy output_spec make sure input replicated along softmax dim input_target_spec = DTensorSpec mesh=input_strategy mesh placements=replicate_reduction_dims input_src_spec placements softmax_dim tensor_meta=input_src_spec tensor_meta redistribute_costs append generate_redistribute_costs input_strategy input_target_spec output_target_spec = input_target_spec output_strategy strategies append OpSpec output_specs=output_target_spec input_specs= input_target_spec redistribute_cost=redistribute_costs output_strategy register_op_strategy aten _log_softmax_backward_data default aten _softmax_backward_data default schema_info=RuntimeSchemaInfo softmax_backward_strategy op_schema OpSchema - OpStrategy grad_out_strategy out_strategy softmax_dim _ = op_schema args_schema grad_out_strategy = cast OpStrategy grad_out_strategy out_strategy = cast OpStrategy out_strategy softmax_dim = cast int softmax_dim softmax_dim = normalize_dim softmax_dim grad_out_strategy ndim grad_in_strategy = OpStrategy grad_out_placement_strat out_placement_strat zip grad_out_strategy strategies out_strategy strategies follow sharding grad_out out depending which has more shards grad_out_src_spec = grad_out_placement_strat output_spec out_src_spec = out_placement_strat output_spec src_spec = grad_out_src_spec grad_out_src_spec num_shards = out_src_spec num_shards out_src_spec make sure inputs replicated along softmax dim tgt_spec = DTensorSpec mesh=grad_out_strategy mesh placements=replicate_reduction_dims src_spec placements softmax_dim new_grad_out_spec = DTensorSpec mesh=tgt_spec mesh placements=tgt_spec placements tensor_meta=grad_out_src_spec tensor_meta new_out_spec = DTensorSpec mesh=tgt_spec mesh placements=tgt_spec placements tensor_meta=out_src_spec tensor_meta redist_grad_out_cost = generate_redistribute_costs grad_out_strategy tgt_spec redist_out_cost = generate_redistribute_costs out_strategy tgt_spec grad_in_strategy strategies append OpSpec output_specs=tgt_spec input_specs= new_grad_out_spec new_out_spec redistribute_cost= redist_grad_out_cost redist_out_cost grad_in_strategy register_op_strategy aten nll_loss_forward default aten nll_loss d_forward default schema_info=RuntimeSchemaInfo nll_loss_forward_strategy op_schema OpSchema - OpStrategy mesh = op_schema get_mesh_from_args len op_schema args_schema == raise AssertionError f Expected args got len op_schema args_schema input_strategy target_strategy weight_strategy reduction _ = op_schema args_schema input_strategy = cast OpStrategy input_strategy target_strategy = cast OpStrategy target_strategy reduction = cast int reduction input_shape = input_strategy shape channel_dim = len input_shape = output_strategy = OpStrategy idx input_placement_strategy enumerate input_strategy strategies op_args_target_specs = redistribute_costs = make sure input replicated along channel dim input_src_spec = input_placement_strategy output_spec input_expected_spec = DTensorSpec mesh=mesh placements=replicate_reduction_dims input_src_spec placements channel_dim tensor_meta=input_src_spec tensor_meta op_args_target_specs append input_expected_spec redistribute_costs append generate_redistribute_costs input_strategy input_expected_spec target doesn t have channel dim follows input other dims target_src_spec = target_strategy strategies idx output_spec target_expected_spec = DTensorSpec mesh=mesh placements=_skip_dim input_expected_spec placements channel_dim tensor_meta=target_src_spec tensor_meta op_args_target_specs append target_expected_spec redistribute_costs append generate_redistribute_costs target_strategy target_expected_spec weight tensor given has Tensor size input_shape channel_dim make sure replicated weight_strategy None isinstance weight_strategy OpStrategy raise AssertionError f Expected OpStrategy got type weight_strategy weight_src_spec = weight_strategy strategies idx output_spec weight_expected_spec = DTensorSpec mesh=mesh placements=_replicate_dims_start_at weight_src_spec placements tensor_meta=weight_src_spec tensor_meta op_args_target_specs append weight_expected_spec redistribute_costs append generate_redistribute_costs weight_strategy weight_expected_spec reduction == Reduction NONE value output_expected_spec = target_expected_spec total_weight_expected_spec = DTensorSpec mesh=mesh placements=tuple Replicate mesh ndim reduction == Reduction MEAN value reduction_op = avg is_tensor_evenly_shardable target_expected_spec shape target_expected_spec raise ValueError The intermediate results nll_loss cannot evenly sharded \ resulting biased mean result reduction == Reduction SUM value reduction_op = sum reduce_dims = list range target_expected_spec ndim reduce_dims_map = _infer_reduce_dims_map reduce_dims target_expected_spec ndim keep_dim=False out_placements = map_placements_after_reduction target_expected_spec placements reduce_dims reduce_dims_map reduction_op output_expected_spec = DTensorSpec mesh=mesh placements=out_placements whether reduction sum mean total weight has summed up replicated total_weight_placements = map_placements_after_reduction target_expected_spec placements reduce_dims reduce_dims_map sum total_weight_expected_spec = DTensorSpec mesh=mesh placements=total_weight_placements output_strategy strategies append OpSpec output_specs= output_expected_spec total_weight_expected_spec input_specs=op_args_target_specs redistribute_cost=redistribute_costs output_strategy register_op_strategy aten nll_loss_backward default aten nll_loss d_backward default schema_info=RuntimeSchemaInfo nll_loss_backward_strategy op_schema OpSchema - OpStrategy backward op does need validate mesh since forward op has already done mesh = op_schema get_mesh_from_args validate=False len op_schema args_schema == raise AssertionError f Expected args got len op_schema args_schema grad_out_strategy input_strategy target_strategy weight_strategy reduction _ total_weight_strategy = op_schema args_schema grad_out_strategy = cast OpStrategy grad_out_strategy input_strategy = cast OpStrategy input_strategy target_strategy = cast OpStrategy target_strategy reduction = cast int reduction total_weight_strategy = cast OpStrategy total_weight_strategy input_shape = input_strategy shape channel_dim = len input_shape = grad_in_strategy = OpStrategy idx input_placement_strategy enumerate input_strategy strategies op_args_target_specs = redistribute_costs = make sure input replicated along channel dim input_src_spec = input_placement_strategy output_spec input_expected_spec = DTensorSpec mesh=mesh placements=replicate_reduction_dims input_src_spec placements channel_dim tensor_meta=input_src_spec tensor_meta op_args_target_specs append input_expected_spec redistribute_costs append generate_redistribute_costs input_strategy input_expected_spec target doesn t have channel dim follows input other dims target_src_spec = target_strategy strategies idx output_spec target_expected_spec = DTensorSpec mesh=mesh placements=_skip_dim input_expected_spec placements channel_dim tensor_meta=target_src_spec tensor_meta op_args_target_specs append target_expected_spec redistribute_costs append generate_redistribute_costs target_strategy target_expected_spec grad_out follows target there no reduction otherwise should replicated scalar grad_out_src_spec = grad_out_strategy strategies idx output_spec reduction == Reduction NONE value grad_out_expected_spec = target_expected_spec grad_out_expected_spec = DTensorSpec mesh=mesh placements=_replicate_dims_start_at grad_out_src_spec placements tensor_meta=grad_out_src_spec tensor_meta op_args_target_specs insert grad_out_expected_spec redistribute_costs insert generate_redistribute_costs grad_out_strategy grad_out_expected_spec weight tensor given has Tensor size input_shape channel_dim make sure replicated weight_strategy None isinstance weight_strategy OpStrategy raise AssertionError f Expected OpStrategy got type weight_strategy weight_src_spec = weight_strategy strategies idx output_spec weight_expected_spec = DTensorSpec mesh=mesh placements=_replicate_dims_start_at weight_src_spec placements tensor_meta=weight_src_spec tensor_meta op_args_target_specs append weight_expected_spec redistribute_costs append generate_redistribute_costs weight_strategy weight_expected_spec total_weight should always replicated total_weight_src_spec = total_weight_strategy strategies idx output_spec total_weight_expected_spec = DTensorSpec mesh=mesh placements=_replicate_dims_start_at total_weight_src_spec placements tensor_meta=total_weight_src_spec tensor_meta op_args_target_specs append total_weight_expected_spec redistribute_costs append generate_redistribute_costs total_weight_strategy total_weight_expected_spec grad_in_expected_spec = input_expected_spec grad_in_strategy strategies append OpSpec output_specs=grad_in_expected_spec input_specs=op_args_target_specs redistribute_cost=redistribute_costs grad_in_strategy _common_norm_forward_strategy op_schema OpSchema rms_norm bool = False - OpStrategy Common forward strategy logic layer_norm rms_norm mesh = op_schema get_mesh_from_args rms_norm layer_norm args input normalized_shape weight bias eps None weight bias their corresponding objects will None well layer_norm_strategy returns one OpStrategy triple values out mean rstd len op_schema args_schema == raise AssertionError f Expected args got len op_schema args_schema input_strategy normalized_shape weight_strategy bias_strategy _ = op_schema args_schema rms_norm args input normalized_shape weight eps len op_schema args_schema == raise AssertionError f Expected args got len op_schema args_schema input_strategy normalized_shape weight_strategy _ = op_schema args_schema bias_strategy = None current norm implementation requires all input DTensor s sharding must form OpStrategy isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy isinstance normalized_shape int Sequence torch Size raise AssertionError f Expected int Sequence torch Size got type normalized_shape normalized_size = normalize_to_torch_size normalized_shape input_ndim = input_strategy ndim axis = input_ndim - len normalized_size we use OpStrategy because output values out mean rstd should have same placements output_strategy = OpStrategy idx input_placement_strategy enumerate input_strategy strategies op_args_target_specs = redistribute_costs = input_src_spec = input_placement_strategy output_spec input tensor we replicate inner dims necessary TODO we can avoid forcing redistribution once we figure out how decompose layer norm input_target_spec = DTensorSpec mesh=mesh placements=_replicate_dims_start_at input_src_spec placements axis tensor_meta=input_src_spec tensor_meta op_args_target_specs append input_target_spec redistribute_costs append generate_redistribute_costs input_strategy input_target_spec weight_strategy None isinstance weight_strategy OpStrategy raise AssertionError f Expected OpStrategy got type weight_strategy weight_src_spec = weight_strategy strategies idx output_spec weight tensor we replicate all dims necessary TODO we can avoid forcing redistribution once we figure out how decompose layer norm weight_target_spec = DTensorSpec mesh=mesh placements=_replicate_dims_start_at weight_src_spec placements tensor_meta=weight_src_spec tensor_meta op_args_target_specs append weight_target_spec redistribute_costs append generate_redistribute_costs weight_strategy weight_target_spec bias_strategy None isinstance bias_strategy OpStrategy raise AssertionError f Expected OpStrategy got type bias_strategy bias_src_spec = bias_strategy strategies idx output_spec bias tensor we replicate all dims necessary TODO we can avoid forcing redistribution once we figure out how decompose layer norm bias_target_spec = DTensorSpec mesh=mesh placements=_replicate_dims_start_at bias_src_spec placements tensor_meta=bias_src_spec tensor_meta op_args_target_specs append bias_target_spec redistribute_costs append generate_redistribute_costs bias_strategy bias_target_spec output spec same input spec output_target_spec = input_target_spec output_strategy strategies append OpSpec output_specs=output_target_spec input_specs=op_args_target_specs redistribute_cost=redistribute_costs output_strategy register_op_strategy aten native_layer_norm default schema_info=RuntimeSchemaInfo layer_norm_strategy op_schema OpSchema - OpStrategy _common_norm_forward_strategy op_schema register_op_strategy aten _fused_rms_norm default schema_info=RuntimeSchemaInfo fused_rms_norm_strategy op_schema OpSchema - OpStrategy _common_norm_forward_strategy op_schema rms_norm=True _common_norm_backward_strategy op_schema OpSchema rms_norm bool = False - OpStrategy Common backward strategy logic layer_norm rms_norm backward op does need validate mesh since forward op has already done mesh = op_schema get_mesh_from_args validate=False rms_norm layer_norm args grad_out input normalized_shape mean rstd weight bias output_mask For None weight bias their corresponding objects will None well len op_schema args_schema == raise AssertionError f Expected args got len op_schema args_schema grad_out_strategy input_strategy normalized_shape mean_strategy rstd_strategy weight_strategy bias_strategy output_mask = op_schema args_schema rms_norm args grad_out input normalized_shape rstd len op_schema args_schema == raise AssertionError f Expected args got len op_schema args_schema grad_out_strategy input_strategy normalized_shape rstd_strategy weight_strategy output_mask = op_schema args_schema mean_strategy = None bias_strategy = None isinstance grad_out_strategy OpStrategy raise AssertionError f Expected OpStrategy got type grad_out_strategy isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy isinstance rstd_strategy OpStrategy raise AssertionError f Expected OpStrategy got type rstd_strategy mean_strategy None isinstance mean_strategy OpStrategy raise AssertionError f Expected OpStrategy got type mean_strategy isinstance normalized_shape int Sequence torch Size raise AssertionError f Expected int Sequence torch Size got type normalized_shape normalized_size = normalize_to_torch_size normalized_shape input_ndim = input_strategy ndim axis = input_ndim - len normalized_size outer_dims = list range axis rms_norm isinstance output_mask list len output_mask == raise AssertionError f Expected output_mask list length got type output_mask f length len output_mask isinstance output_mask list N A isinstance output_mask list len output_mask == raise AssertionError f Expected output_mask list length got type output_mask f length len output_mask isinstance output_mask list N A output tuple d_input d_weight d_bias out_tuple_strategy = OpStrategy idx input_placement_strategy enumerate input_strategy strategies args OpSpec output_specs_list list Optional DTensorSpec = input_specs_list list DTensorSpec = redistribute_costs = input_src_spec = input_placement_strategy output_spec arg grad_out TODO change strategy following rule d_input basically product element-wise mul grad_out rstd normalized input among which rstd normalized input x_hat should have same sharding placements grad_out s sharding determined pointwise result x_hat weight bias TODO now grad_out spec follows input spec we may need change apply pointwise rule over grad_out input weight grad_out_target_spec = DTensorSpec mesh=mesh placements=_replicate_dims_start_at input_src_spec placements axis tensor_meta=input_src_spec tensor_meta input_specs_list append grad_out_target_spec redistribute_costs append generate_redistribute_costs grad_out_strategy grad_out_target_spec output_specs_list append grad_out_target_spec output_mask None arg input input_target_spec = DTensorSpec mesh=mesh placements=_replicate_dims_start_at input_src_spec placements axis tensor_meta=input_src_spec tensor_meta input_specs_list append input_target_spec redistribute_costs append generate_redistribute_costs input_strategy input_target_spec arg mean rms_norm mean_strategy None raise AssertionError Expected mean_strategy None mean_src_spec = mean_strategy strategies idx output_spec input_specs_list append mean_src_spec redistribute_costs append _ mean_strategy strategies arg rstd rstd_src_spec = rstd_strategy strategies idx output_spec input_specs_list append rstd_src_spec redistribute_costs append _ rstd_strategy strategies _add_target_input_spec strategy - DTensorSpec shared logic setting weight bias target input specs isinstance strategy OpStrategy raise AssertionError f Expected OpStrategy got type strategy src_spec = strategy strategies idx output_spec no need redistribute since they should replicated forward pass input_specs_list append src_spec redistribute_costs append _ strategy strategies src_spec arg weight d_weight = sum grad_out input - mean rstd outer_dim keepdim=False For RMS norm mean so s just sum grad_out input rstd outer_dim keepdim=False weight_strategy None weight_src_spec = _add_target_input_spec weight_strategy TODO now d_weight spec follows input spec w reduction we may need change pointwise rule over grad_out input then apply reduction inp_placements = _replicate_dims_start_at input_src_spec placements axis reduce_dims_map = _infer_reduce_dims_map outer_dims input_src_spec ndim False out_placements = map_placements_after_reduction inp_placements outer_dims reduce_dims_map sum weight_out_spec = DTensorSpec mesh=mesh placements=out_placements tensor_meta=weight_src_spec tensor_meta output_specs_list append weight_out_spec output_mask None rms_norm error_msg = output_mask should ` True ` while weight argument ` None ` native_layer_norm_backward error_msg = output_mask should ` True ` while weight argument ` None ` _fused_rms_norm_backward output_mask False raise AssertionError error_msg output_specs_list append None arg bias d_bias = sum grad_out outer_dim keepdim=False rms_norm bias_strategy None bias_src_spec = _add_target_input_spec bias_strategy d_bias spec follows reduction over grad_out inp_placements = _replicate_dims_start_at grad_out_target_spec placements axis reduce_dims_map = _infer_reduce_dims_map outer_dims grad_out_target_spec ndim False out_placements = map_placements_after_reduction inp_placements outer_dims reduce_dims_map sum bias_out_spec = DTensorSpec mesh=mesh placements=out_placements tensor_meta=bias_src_spec tensor_meta output_specs_list append bias_out_spec output_mask None output_mask False raise AssertionError output_mask should ` True ` while bias argument ` None ` native_layer_norm_backward output_specs_list append None out_tuple_strategy strategies append OpSpec output_specs=tuple output_specs_list input_specs=input_specs_list redistribute_cost=redistribute_costs out_tuple_strategy register_op_strategy aten native_layer_norm_backward default schema_info=RuntimeSchemaInfo layer_norm_bwd_strategy op_schema OpSchema - OpStrategy _common_norm_backward_strategy op_schema register_op_strategy aten _fused_rms_norm_backward default schema_info=RuntimeSchemaInfo fused_rms_norm_bwd_strategy op_schema OpSchema - OpStrategy _common_norm_backward_strategy op_schema rms_norm=True sort_strategy op_schema OpSchema sort_dim int - OpStrategy input_strategy = cast OpStrategy op_schema args_schema sort_dim = normalize_dim sort_dim input_strategy ndim single_mesh_dim_strategies = all_replicate PlacementList = Replicate single_mesh_dim_strategies append all_replicate dim range input_strategy ndim dim = sort_dim dim_shardings PlacementList = Shard dim single_mesh_dim_strategies append dim_shardings expand_to_full_mesh_op_strategy input_strategy mesh op_schema single_mesh_dim_strategies input_index= register_op_strategy aten topk default schema_info=RuntimeSchemaInfo topk_strategy op_schema OpSchema - OpStrategy topk_dim = cast int op_schema args_schema len op_schema args_schema - sort_strategy op_schema topk_dim register_op_strategy aten sort default schema_info=RuntimeSchemaInfo sort_default_strategy op_schema OpSchema - OpStrategy mostly copy paste topk_strategy input_strategy = op_schema args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy sort_dim = - len op_schema args_schema sort_dim = cast int op_schema args_schema sort_strategy op_schema sort_dim register_op_strategy aten sort stable schema_info=RuntimeSchemaInfo static_kwargkey= dim descending stable sort_stable_strategy op_schema OpSchema - OpStrategy mostly copy paste topk_strategy input_strategy = op_schema args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy sort_dim = - dim op_schema kwargs_schema sort_dim = cast int op_schema kwargs_schema dim sort_strategy op_schema sort_dim register_op_strategy aten histc default strategy choice depends value min max kwargs which position schema_info=RuntimeSchemaInfo histc_strategy op_schema OpSchema - OpStrategy input_strategy = cast OpStrategy op_schema args_schema single_mesh_dim_strategies list PlacementList = single_mesh_dim_strategies append Replicate Replicate histc can support sharded input partial output any input dim provided min max values user-specified If user-specified true min max data each local tensor will used compute bin boundaries which will same across ranks leading incorrect final result len op_schema args_schema == dim range input_strategy ndim dim_shardings PlacementList = Partial Shard dim single_mesh_dim_strategies append dim_shardings expand_to_full_mesh_op_strategy input_strategy mesh op_schema single_mesh_dim_strategies register_op_strategy aten logsumexp default schema_info=RuntimeSchemaInfo static_argnum position where non-Tensor args beings static_argnum= static_kwargkey name kwargs hash which determines whether sharding prop can cached static_kwargkey= keepdim logsumexp_strategy op_schema OpSchema - OpStrategy Implements sharding propagation strategy logsumexp args_schema contains all DTensor args e g dim keepdim args_schema = op_schema args_schema len args_schema raise AssertionError f Expected more than arg input dim required got len args_schema input_strategy = args_schema isinstance input_strategy OpStrategy raise AssertionError f Expected OpStrategy got type input_strategy dims_arg = args_schema reduce_dims = _infer_reduction_dims dims_arg input_strategy ndim reduce_dims None raise AssertionError Expected reduce_dims None keep_dim = cast bool op_schema kwargs_schema get keepdim False common_reduction_strategy input_strategy reduce_dims keep_dim=keep_dim reduction_linear=False