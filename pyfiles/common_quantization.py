mypy ignore-errors r Importing file includes common utility methods base classes checking quantization api properties resulting modules torch torch ao nn intrinsic quantized dynamic nniqd torch ao nn quantized nnq torch ao nn quantized dynamic nnqd torch distributed dist torch nn nn torch nn functional F functorch experimental control_flow torch ao nn intrinsic _FusedModule torch ao quantization convert default_dynamic_qat_qconfig default_dynamic_qconfig default_dynamic_quant_observer default_embedding_qat_qconfig default_observer default_per_channel_qconfig default_qconfig default_symmetric_qnnpack_qat_qconfig default_weight_observer DeQuantStub float_qparams_weight_only_qconfig get_default_qat_qconfig get_default_qat_qconfig_mapping get_default_qconfig get_default_qconfig_mapping PerChannelMinMaxObserver propagate_qconfig_ QConfig QConfigMapping quantize quantize_dynamic_jit quantize_jit QuantStub QuantType QuantWrapper torch ao quantization backend_config get_executorch_backend_config torch ao quantization quantization_mappings get_default_dynamic_quant_module_mappings get_default_qat_module_mappings get_default_qconfig_propagation_list torch ao quantization quantize_pt e _convert_to_reference_decomposed_fx convert_pt e prepare_pt e prepare_qat_pt e torch ao quantization quantizer xnnpack_quantizer get_symmetric_quantization_config XNNPACKQuantizer torch export export torch jit mobile _load_for_lite_interpreter torch testing _internal common_quantized override_quantized_engine torch testing _internal common_utils TEST_WITH_ROCM TestCase try torch ao ns fx ns_types NSSingleResultValuesType NSSubgraph graph mode quantization based fx torch ao quantization quantize_fx convert_fx convert_to_reference_fx prepare_fx prepare_qat_fx torch fx GraphModule torch fx graph Node HAS_FX = True except ImportError HAS_FX = False contextlib copy functools io os unittest typing Any Optional Union collections abc Callable numpy np torch _dynamo torchdynamo torch ao quantization quantizer x _inductor_quantizer xiq torch ao quantization quantizer xpu_inductor_quantizer xpuiq torch ao quantization quantizer x _inductor_quantizer X InductorQuantizer torch ao quantization quantizer xpu_inductor_quantizer XPUInductorQuantizer torch testing FileCheck NodeSpec Used checking GraphModule Node __init__ op target op call_function &#124; call_module target call_function target would function call_module target would type PyTorch module op = op target = target classmethod call_function cls target NodeSpec call_function target classmethod call_method cls target NodeSpec call_method target classmethod call_module cls target NodeSpec call_module target __hash__ hash op target __eq__ other isinstance other NodeSpec NotImplemented op == other op target == other target __repr__ repr op + + repr target get_supported_device_types cpu cuda torch cuda is_available TEST_WITH_ROCM cpu test_only_eval_fn model calib_data r Default evaluation function takes torch utils data Dataset list input Tensors run model dataset inp calib_data model inp _default_loss_fn = torch nn CrossEntropyLoss test_only_train_fn model train_data loss_fn=_default_loss_fn r Default train function takes torch utils data Dataset train model dataset optimizer = torch optim Adam model parameters lr= train_loss correct total = _ range model train data target train_data optimizer zero_grad output = model data loss = loss_fn output target loss backward optimizer step train_loss += loss item _ predicted = torch max output total += target size correct += predicted == target sum item train_loss correct total AverageMeter Computes stores average current value __init__ name fmt= f name = name fmt = fmt reset reset val = avg = sum = count = update val n= val = val sum += val n count += n avg = sum count __str__ fmtstr = name val + fmt + avg + fmt + fmtstr format __dict__ accuracy output target topk= Computes accuracy over k top predictions specified values k torch no_grad maxk = max topk batch_size = target size _ pred = output topk maxk True True pred = pred t correct = pred eq target view - expand_as pred res = k topk correct_k = correct k view - float sum keepdim=True res append correct_k mul_ batch_size res train_one_epoch model criterion optimizer data_loader device ntrain_batches model train cnt image target enumerate data_loader start= print end= image target = image device target device output = model image loss = criterion output target optimizer zero_grad loss backward optimizer step accuracy output target topk= cnt = ntrain_batches ddp_setup rank world_size os environ MASTER_ADDR = localhost os environ MASTER_PORT = initialize process group dist init_process_group gloo rank=rank world_size=world_size ddp_cleanup dist destroy_process_group run_ddp rank world_size prepared ddp_setup rank world_size prepared cuda prepared = torch nn parallel DistributedDataParallel prepared device_ids= rank prepared rank model_with_ddp = prepared optimizer = torch optim SGD model_with_ddp parameters lr= train_one_epoch model_with_ddp criterion optimizer dataset rank noqa F ddp_cleanup convert_dynamic module convert module get_default_dynamic_quant_module_mappings inplace=True prepare_dynamic model qconfig_dict=None propagate_qconfig_ model qconfig_dict _make_conv_test_input batch_size in_channels_per_group input_feature_map_size out_channels_per_group groups kernel_size X_scale X_zero_point W_scale W_zero_point use_bias use_channelwise in_channels = in_channels_per_group groups out_channels = out_channels_per_group groups X_value_min X_value_max = X_init = torch randint X_value_min X_value_max batch_size in_channels + input_feature_map_size X = X_scale X_init - X_zero_point float X_q = torch quantize_per_tensor X scale=X_scale zero_point=X_zero_point dtype=torch quint W_scale = W_scale out_channels W_zero_point = W_zero_point out_channels Resize W_scale W_zero_points arrays equal out_channels W_scale = W_scale out_channels W_zero_point = W_zero_point out_channels For testing we use small values weights activations so no overflow occurs vpmaddubsw instruction If overflow occurs qconv implementation there no overflow In reference we can t exactly match results reference Please see comment qconv implementation file aten src ATen native quantized cpu qconv cpp more details W_value_min W_value_max = - The operator expects them format out_channels in_channels groups + kernel_size W_init = torch randint W_value_min W_value_max out_channels in_channels_per_group + kernel_size b_init = torch randint out_channels use_channelwise W_shape = - + len kernel_size W_scales_tensor = torch tensor W_scale dtype=torch float W_zero_points_tensor = torch tensor W_zero_point dtype=torch float W = W_scales_tensor reshape W_shape W_init float - W_zero_points_tensor reshape W_shape float b = X_scale W_scales_tensor b_init float W_q = torch quantize_per_channel W W_scales_tensor double W_zero_points_tensor long dtype=torch qint W = W_scale W_init - W_zero_point float b = X_scale W_scale b_init float W_q = torch quantize_per_tensor W scale=W_scale zero_point=W_zero_point dtype=torch qint X X_q W W_q b use_bias None _make_conv_add_extra_input_tensor scale zero_point sizes X_value_min X_value_max = X_init = torch randint X_value_min X_value_max sizes Infer size tensor do add X = scale X_init - zero_point float X_q = torch quantize_per_tensor X scale=scale zero_point=zero_point dtype=torch quint X X_q skipIfNoFBGEMM fn reason = Quantized operations require FBGEMM FBGEMM only optimized CPUs instruction set support AVX newer isinstance fn type fbgemm torch backends quantized supported_engines fn __unittest_skip__ = True fn __unittest_skip_why__ = reason fn functools wraps fn wrapper args kwargs fbgemm torch backends quantized supported_engines raise unittest SkipTest reason fn args kwargs wrapper skipIfNoQNNPACK fn reason = Quantized operations require QNNPACK isinstance fn type qnnpack torch backends quantized supported_engines fn __unittest_skip__ = True fn __unittest_skip_why__ = reason fn functools wraps fn wrapper args kwargs qnnpack torch backends quantized supported_engines raise unittest SkipTest reason fn args kwargs wrapper withQNNPACKBackend fn TODO future PR consider combining skipIfNoQNNPACK will require testing existing callsites reason = Quantized operations require QNNPACK isinstance fn type qnnpack torch backends quantized supported_engines fn __unittest_skip__ = True fn __unittest_skip_why__ = reason fn functools wraps fn wrapper args kwargs qnnpack torch backends quantized supported_engines raise unittest SkipTest reason override_quantized_engine qnnpack fn args kwargs wrapper skipIfNoONEDNN fn reason = Quantized operations require ONEDNN isinstance fn type onednn torch backends quantized supported_engines fn __unittest_skip__ = True fn __unittest_skip_why__ = reason fn functools wraps fn wrapper args kwargs onednn torch backends quantized supported_engines raise unittest SkipTest reason fn args kwargs wrapper skipIfNoONEDNNBF fn reason = Quantized operations require BF support isinstance fn type torch ops mkldnn _is_mkldnn_bf _supported fn __unittest_skip__ = True fn __unittest_skip_why__ = reason fn functools wraps fn wrapper args kwargs torch ops mkldnn _is_mkldnn_bf _supported raise unittest SkipTest reason fn args kwargs wrapper skipIfNoX fn reason = Quantized operations require X isinstance fn type x torch backends quantized supported_engines fn __unittest_skip__ = True fn __unittest_skip_why__ = reason fn functools wraps fn wrapper args kwargs x torch backends quantized supported_engines raise unittest SkipTest reason fn args kwargs wrapper skipIfNoDynamoSupport fn reason = dynamo doesn t support isinstance fn type torchdynamo is_dynamo_supported fn __unittest_skip__ = True fn __unittest_skip_why__ = reason fn functools wraps fn wrapper args kwargs torchdynamo is_dynamo_supported raise unittest SkipTest reason fn args kwargs wrapper skipIfNoInductorSupport fn reason = inductor doesn t support isinstance fn type torchdynamo is_inductor_supported fn __unittest_skip__ = True fn __unittest_skip_why__ = reason fn functools wraps fn wrapper args kwargs torchdynamo is_inductor_supported raise unittest SkipTest reason fn args kwargs wrapper try torchvision noqa F HAS_TORCHVISION = True except ImportError HAS_TORCHVISION = False skip_if_no_torchvision = unittest skipIf HAS_TORCHVISION no torchvision get_script_module model tracing data torch jit trace model data tracing torch jit script model lengths_to_offsets t offset_type=np int use_begin_offset=True Convert lengths offsets embedding_bag tt = np zeros t shape + dtype=offset_type tt = t tt = torch from_numpy np cumsum tt dtype=offset_type use_begin_offset tt - tt _group_quantize_tensor w n_bit= q_group_size= assert w dim == w = w transpose contiguous assert q_group_size assert w shape - q_group_size == to_quant = w reshape - q_group_size assert torch isnan to_quant sum == max_val = to_quant amax dim= keepdim=True min_val = to_quant amin dim= keepdim=True max_int = n_bit - min_int = scales = max_val - min_val clamp min= e- max_int assert torch isnan scales sum == zeros = min_val + scales n_bit - assert torch isnan zeros sum == out = to_quant sub min_val div scales round clamp_ min_int max_int assert torch isnan out sum == out = out dtype=torch int reshape w shape out device = torch device cpu out = out &#124; out torch uint Scales zeros same q-group should contiguous so we can load -bit word scales = scales view w shape - zeros = zeros view w shape - scales_and_zeros = torch cat scales reshape scales size scales size zeros reshape zeros size zeros size transpose contiguous out scales_and_zeros _group_quantize_tensor_symmetric w n_bit= groupsize= W shape K x N We transpose W Quantization applied N x K w = w transpose contiguous assert w dim == assert groupsize assert w shape - groupsize == Calculate scale zeros to_quant = w reshape - groupsize max_val = to_quant abs amax dim= keepdim=True eps = torch finfo max_val dtype eps max_int = n_bit - - For -bit scales = max_val clamp min=eps max_int zeros = torch zeros_like scales Quantize weight scales = scales torch float reshape w shape - zeros = zeros torch float reshape w shape - scales = scales reshape - zeros = zeros reshape - max_int = n_bit - w_int = to_quant div scales add torch int clamp max=max_int We pack signed int values unsigned uint container This reduces weight size half improves load perf out_uint = w_int &#124; w_int torch uint scales_and_zeros = scales squeeze contiguous out_uint scales_and_zeros _dynamically_quantize_per_channel x quant_min quant_max target_dtype source https github com meta-pytorch gpt-fast blob main quantize py default setup affine quantization activations x_dtype = x dtype x = x float eps = torch finfo torch float eps get min max min_val max_val = torch aminmax x dim= calculate scales zero_points based min max reference https fburl com code srbiybme min_val_neg = torch min min_val torch zeros_like min_val max_val_pos = torch max max_val torch zeros_like max_val device = min_val_neg device reference https fburl com code wll rk max_val_pos = torch max -min_val_neg max_val_pos scales = max_val_pos float quant_max - quant_min ensure scales same dtype original tensor scales = torch clamp scales min=eps x dtype zero_points = torch zeros min_val_neg size dtype=torch int device=device quantize based qmin qmax scales zp x_div = x scales unsqueeze - x_round = torch round x_div x_zp = x_round + zero_points unsqueeze - quant = torch clamp x_zp quant_min quant_max target_dtype quant scales x_dtype zero_points QuantizationTestCase used base testing quantization modules QuantizationTestCase TestCase setUp super setUp calib_data = torch rand dtype=torch float _ range train_data = torch rand dtype=torch float torch randint dtype=torch long _ range img_data_ d = torch rand dtype=torch float _ range img_data_ d = torch rand dtype=torch float _ range img_data_ d = torch rand dtype=torch float _ range img_data_ d_train = torch rand dtype=torch float torch randint dtype=torch long _ range img_data_ d_train = torch rand dtype=torch float torch randint dtype=torch long _ range img_data_ d_train = torch rand dtype=torch float torch randint dtype=torch long _ range img_data_dict = img_data_ d img_data_ d img_data_ d Quant types produce statically quantized ops static_quant_types = QuantType STATIC QuantType QAT All quant types fx based graph mode quantization all_quant_types = QuantType DYNAMIC QuantType STATIC QuantType QAT checkNoPrepModules module r Checks module does contain child modules quantization preparation e g quant dequant observer assertFalse hasattr module quant assertFalse hasattr module dequant checkNoQconfig module r Checks module does contain qconfig assertFalse hasattr module qconfig child module children checkNoQconfig child checkHasPrepModules module r Checks module contains child modules quantization preparation e g quant dequant observer assertTrue hasattr module module assertTrue hasattr module quant assertTrue hasattr module dequant checkObservers module propagate_qconfig_list=None prepare_custom_config_dict=None r Checks module module s leaf descendants have observers preparation quantization propagate_qconfig_list None propagate_qconfig_list = get_default_qconfig_propagation_list prepare_custom_config_dict None prepare_custom_config_dict = float_to_observed_module_class_mapping = prepare_custom_config_dict get float_to_observed_custom_module_class check module leaf module ignoring activation_post_process attribute is_leaf_module module submodule_name_count = name _ module named_children name = activation_post_process submodule_name_count += submodule_name_count == hasattr module qconfig module qconfig None is_leaf_module module isinstance module torch nn Sequential type module propagate_qconfig_list type module float_to_observed_module_class_mapping keys isinstance module torch ao quantization DeQuantStub assertTrue hasattr module activation_post_process module + str type module + do have observer we don t need check observers child modules qat modules type module get_default_qat_module_mappings values type module float_to_observed_module_class_mapping values isinstance module _FusedModule child module children type child nn Dropout continue checkObservers child propagate_qconfig_list prepare_custom_config_dict checkQuantDequant mod r Checks mod has nn Quantize nn DeQuantize submodules inserted assertEqual type mod quant nnq Quantize assertEqual type mod dequant nnq DeQuantize checkWrappedQuantizedLinear mod r Checks mod has been swapped nnq Linear module bias qint module has Quantize DeQuantize submodules assertEqual type mod module nnq Linear checkQuantDequant mod checkQuantizedLinear mod assertEqual type mod nnq Linear checkDynamicQuantizedLinear mod dtype r Checks mod has been swapped nnqd Linear module bias float assertEqual type mod nnqd Linear assertEqual mod _packed_params dtype dtype checkDynamicQuantizedLinearRelu mod dtype r Checks mod has been swapped nnqd Linear module bias float assertEqual type mod nniqd LinearReLU assertEqual mod _packed_params dtype dtype check_eager_serialization ref_model loaded_model x Check state dict serialization torch save APIs model_dict = ref_model state_dict b = io BytesIO torch save model_dict b b seek weights_only=False we sometimes get ScriptObject here weird loaded_dict = torch load b weights_only=False loaded_model load_state_dict loaded_dict ref_out = ref_model x load_out = loaded_model x check_outputs ref_out load_out assertEqual ref_out load_out isinstance ref_out tuple assertEqual ref_out load_out assertEqual ref_out load_out assertEqual ref_out load_out check_outputs ref_out load_out b = io BytesIO torch save ref_model b b seek weights_only=False legacy code saves model loaded = torch load b weights_only=False load_out = loaded x check_outputs ref_out load_out check_weight_bias_api ref_model weight_keys bias_keys weight = ref_model get_weight bias = ref_model get_bias assertEqual weight_keys ^ weight keys set assertEqual bias_keys ^ bias keys set checkDynamicQuantizedLSTM mod reference_module_type dtype r Checks mod has been swapped nnqd LSTM type module bias float wt_dtype_map = torch qint quantized_dynamic torch float quantized_fp assertEqual type mod reference_module_type packed_params mod _all_weight_values assertEqual packed_params param __getstate__ wt_dtype_map dtype checkLinear mod assertEqual type mod torch nn Linear checkDynamicQuantizedModule mod reference_module_type dtype r Checks mod has been swapped nnqd Linear module bias float wt_dtype_map = torch qint quantized_dynamic torch float quantized_fp assertEqual type mod reference_module_type hasattr mod _all_weight_values packed_params mod _all_weight_values assertEqual packed_params param __getstate__ wt_dtype_map dtype checkScriptable orig_mod calib_data check_save_load=False scripted = torch jit script orig_mod _checkScriptable orig_mod scripted calib_data check_save_load Use first calib_data entry trace input traced = torch jit trace orig_mod calib_data _checkScriptable orig_mod traced calib_data check_save_load Call twice once scripted module once traced module _checkScriptable orig_mod script_mod calib_data check_save_load _checkModuleCorrectnessAgainstOrig orig_mod script_mod calib_data Test save load buffer = io BytesIO torch jit save script_mod buffer buffer seek loaded_mod = torch jit load buffer Pending __get_state_ __set_state__ support See tracking task https github com pytorch pytorch issues check_save_load _checkModuleCorrectnessAgainstOrig orig_mod loaded_mod calib_data _checkModuleCorrectnessAgainstOrig orig_mod test_mod calib_data inp calib_data ref_output = orig_mod inp scripted_output = test_mod inp assertEqual scripted_output ref_output checkGraphModeOp module inputs quantized_op tracing=False debug=False check=True eval_mode=True dynamic=False qconfig=None debug print Testing str module qconfig_dict = get_default_qconfig torch backends quantized engine eval_mode module = module eval dynamic qconfig_dict = default_dynamic_qconfig qconfig None qconfig model = get_script_module module tracing inputs eval debug print input graph model graph models = outputs = debug True False dynamic models debug = quantize_dynamic_jit model qconfig_dict debug=debug make sure runs outputs debug = models debug inputs module under test can contain in-place ops we depend input data staying constant comparisons inputs_copy = copy deepcopy inputs models debug = quantize_jit model qconfig_dict test_only_eval_fn inputs_copy inplace=False debug=debug make sure runs outputs debug = models debug inputs debug print debug graph models True graph print non debug graph models False graph check debug non-debug option should have same numerics assertEqual outputs True outputs False non debug graph should produce quantized op FileCheck check quantized_op run models False graph models False checkGraphModuleNodes graph_module expected_node=None expected_node_occurrence=None expected_node_list=None Check GraphModule contains target node Args graph_module GraphModule instance we want check expected_node expected_node_occurrence expected_node_list see docs checkGraphModeFxOp nodes_in_graph = node_list = modules = dict graph_module named_modules remove_duplicate=False node graph_module graph nodes n = None node op == call_function node op == call_method n = NodeSpec node op node target node op == call_module n = NodeSpec node op type modules node target n None node_list append n n nodes_in_graph nodes_in_graph n += nodes_in_graph n = expected_node None assertTrue expected_node nodes_in_graph node + str expected_node + found graph module expected_node_occurrence None expected_node occurrence expected_node_occurrence items occurrence = assertTrue expected_node nodes_in_graph Check failed node + str expected_node + found assertTrue nodes_in_graph expected_node == occurrence Check failed node + str expected_node + Expected occurrence + str occurrence + Found occurrence + str nodes_in_graph expected_node assertTrue expected_node nodes_in_graph Check failed node + str expected_node + expected no occurrence found expected_node_list None cur_index = n node_list cur_index == len expected_node_list n == expected_node_list cur_index cur_index += assertTrue cur_index == len expected_node_list Check failed graph + printGraphModule graph_module print_str=False + Expected ordered list + str expected_node_list printGraphModule graph_module print_str=True modules = dict graph_module named_modules remove_duplicate=False node_infos = n graph_module graph nodes node_info = join map repr n op n name n target n args n kwargs n op == call_module node_info += module type + repr type modules n target node_infos append node_info str_to_print = \n join node_infos print_str print str_to_print str_to_print HAS_FX assert_types_for_matched_subgraph_pairs matched_subgraph_pairs dict str tuple NSSubgraph NSSubgraph expected_types dict str tuple tuple Callable Callable tuple Callable Callable gm_a GraphModule gm_b GraphModule - None Verifies types specified expected_types match underlying objects pointed nodes matched_subgraph_pairs An example successful test case matched_subgraph_pairs = x graph_a_conv_ _node graph_b_conv_ _node expected_types = x nn Conv d nnq Conv d The function tests key equivalence verifies types instance checks _get_underlying_op_type node Node gm GraphModule - Union Callable str node op == call_module mod = getattr gm node target type mod assert node op call_function call_method node target assertTrue len matched_subgraph_pairs == len expected_types f Expected length results match got len matched_subgraph_pairs len expected_types k v expected_types items expected_types_a expected_types_b = v exp_type_start_a exp_type_end_a = expected_types_a exp_type_start_b exp_type_end_b = expected_types_b subgraph_a subgraph_b = matched_subgraph_pairs k act_type_start_a = _get_underlying_op_type subgraph_a start_node gm_a act_type_start_b = _get_underlying_op_type subgraph_b start_node gm_b act_type_end_a = _get_underlying_op_type subgraph_a end_node gm_a act_type_end_b = _get_underlying_op_type subgraph_b end_node gm_b types_match = exp_type_start_a act_type_start_a exp_type_end_a act_type_end_a exp_type_start_b act_type_start_b exp_type_end_b act_type_end_b assertTrue types_match f Type mismatch k expected exp_type_start_a exp_type_end_a exp_type_start_b exp_type_end_b f got act_type_start_a act_type_end_a act_type_start_b act_type_end_b assert_ns_compare_dict_valid act_compare_dict dict str dict str dict str Any - None Verifies act_compare_dict output Numeric Suite APIs valid each layer results recorded two models number seen tensors match shapes each pair seen tensors match layer_name result_type_to_data act_compare_dict items result_type layer_data result_type_to_data items assertTrue len layer_data == f Layer layer_name does have exactly two model results model_name_ model_name_ = layer_data keys res_idx range len layer_data model_name_ layer_data_ = layer_data model_name_ res_idx layer_data_ = layer_data model_name_ res_idx assertTrue layer_data_ type == layer_data_ type f Layer layer_name model_name_ model_name_ do have same type assertTrue len layer_data_ values == len layer_data_ values f Layer layer_name model_name_ model_name_ do have same number seen Tensors F conv d weight has rank toq conv d unpacked weight has rank For now skip length check conv d only is_weight_functional_conv d = result_type == NSSingleResultValuesType WEIGHT value conv d layer_data_ prev_node_target_type conv d layer_data_ prev_node_target_type is_weight_functional_conv d idx range len layer_data_ values values_ = layer_data_ values idx values_ = layer_data_ values idx isinstance values_ torch Tensor assertTrue values_ shape == values_ shape f Layer layer_name model_name_ model_name_ + f have shape mismatch idx idx isinstance values_ list values_ = values_ values_ = values_ assertTrue values_ shape == values_ shape f Layer layer_name model_name_ model_name_ + f have shape mismatch idx idx assert isinstance values_ tuple f unhandled type type values_ assert len values_ == assert len values_ == assert values_ shape == values_ shape assert values_ shape == values_ shape assert values_ shape == values_ shape verify ref_node_name valid ref_node_name_ = layer_data_ ref_node_name ref_node_name_ = layer_data_ ref_node_name prev_node_name_ = layer_data_ prev_node_name prev_node_name_ = layer_data_ prev_node_name layer_data_ type == NSSingleResultValuesType NODE_OUTPUT value assertTrue ref_node_name_ == prev_node_name_ assertTrue ref_node_name_ == prev_node_name_ layer_data_ type == NSSingleResultValuesType NODE_INPUT value assertTrue ref_node_name_ = prev_node_name_ assertTrue ref_node_name_ = prev_node_name_ checkGraphModeFxOp model inputs quant_type expected_node=None expected_node_occurrence=None expected_node_list=None is_reference=False print_debug_info=False custom_qconfig_dict=None prepare_expected_node=None prepare_expected_node_occurrence=None prepare_expected_node_list=None prepare_custom_config=None backend_config=None Quantizes model graph mode quantization fx check quantized model contains quantized_node Args model floating point torch nn Module inputs one positional sample input arguments model expected_node NodeSpec e g NodeSpec call_function torch quantize_per_tensor expected_node_occurrence dict NodeSpec expected number occurrences int e g NodeSpec call_function torch quantize_per_tensor NodeSpec call_method dequantize expected_node_list list NodeSpec used check order occurrence Node e g NodeSpec call_function torch quantize_per_tensor NodeSpec call_module nnq Conv d NodeSpec call_function F hardtanh_ NodeSpec call_method dequantize is_reference True enables reference mode print_debug_info True prints debug info custom_qconfig_dict overrides default qconfig_dict prepare_expected_node same expected_node prepare prepare_expected_node_occurrence same expected_node_occurrence prepare prepare_expected_node_list same expected_node_list prepare Returns A dictionary following structure prepared prepared model quantized quantized non-reference model quantized_reference quantized reference model result result either quantized quantized_reference model depending is_reference argument TODO make img_data single example instead list type inputs list inputs = inputs quant_type == QuantType QAT qconfig_mapping = get_default_qat_qconfig_mapping torch backends quantized engine model train quant_type == QuantType STATIC qconfig_mapping = get_default_qconfig_mapping torch backends quantized engine model eval qconfig = default_dynamic_qconfig qconfig_mapping = QConfigMapping set_global qconfig model eval quant_type == QuantType QAT prepare = prepare_qat_fx prepare = prepare_fx overwrite qconfig_dict custom_qconfig_dict custom_qconfig_dict None assert type custom_qconfig_dict QConfigMapping dict custom_qconfig_dict should QConfigMapping dict isinstance custom_qconfig_dict QConfigMapping qconfig_mapping = custom_qconfig_dict qconfig_mapping = QConfigMapping from_dict custom_qconfig_dict prepared = prepare model qconfig_mapping example_inputs=inputs prepare_custom_config=prepare_custom_config backend_config=backend_config quant_type = QuantType DYNAMIC prepared inputs print_debug_info print print quant type \n quant_type print original model \n model print print prepared model \n prepared checkGraphModuleNodes prepared prepare_expected_node prepare_expected_node_occurrence prepare_expected_node_list prepared_copy = copy deepcopy prepared qgraph = convert_fx copy deepcopy prepared qgraph_reference = convert_to_reference_fx copy deepcopy prepared result = qgraph inputs result_reference = qgraph_reference inputs qgraph_copy = copy deepcopy qgraph qgraph_reference_copy = copy deepcopy qgraph_reference qgraph_to_check = qgraph_reference is_reference qgraph print_debug_info print print quantized model \n qgraph_to_check printGraphModule qgraph_to_check print checkGraphModuleNodes qgraph_to_check expected_node expected_node_occurrence expected_node_list prepared prepared_copy quantized qgraph_copy quantized_reference qgraph_reference_copy quantized_output result quantized_reference_output result_reference checkEmbeddingSerialization qemb num_embeddings embedding_dim indices offsets set_qconfig is_emb_bag dtype=torch quint Test serialization dynamic EmbeddingBag module using state_dict is_emb_bag inputs = indices offsets inputs = indices emb_dict = qemb state_dict b = io BytesIO torch save emb_dict b b seek loaded_dict = torch load b embedding_unpack = torch ops quantized embedding_bag_unpack Check unpacked weight values explicitly key emb_dict isinstance emb_dict key torch _C ScriptObject assert isinstance loaded_dict key torch _C ScriptObject emb_weight = embedding_unpack emb_dict key loaded_weight = embedding_unpack loaded_dict key assertEqual emb_weight loaded_weight Check state dict serialization torch save APIs is_emb_bag loaded_qemb = nnq EmbeddingBag num_embeddings=num_embeddings embedding_dim=embedding_dim include_last_offset=True mode= sum dtype=dtype loaded_qemb = nnq Embedding num_embeddings=num_embeddings embedding_dim=embedding_dim dtype=dtype check_eager_serialization qemb loaded_qemb inputs loaded_qemb load_state_dict loaded_dict assertEqual embedding_unpack qemb _packed_params _packed_weight embedding_unpack loaded_qemb _packed_params _packed_weight Test JIT serialization checkScriptable qemb inputs check_save_load=True Test from_float call is_emb_bag float_embedding = torch nn EmbeddingBag num_embeddings=num_embeddings embedding_dim=embedding_dim include_last_offset=True scale_grad_by_freq=False mode= sum float_embedding = torch nn Embedding num_embeddings=num_embeddings embedding_dim=embedding_dim set_qconfig float_qparams_observer = PerChannelMinMaxObserver with_args dtype=dtype qscheme=torch per_channel_affine_float_qparams ch_axis= float_embedding qconfig = QConfig activation=default_dynamic_quant_observer weight=float_qparams_observer prepare_dynamic float_embedding float_embedding inputs is_emb_bag q_embeddingbag = nnq EmbeddingBag from_float float_embedding expected_name = QuantizedEmbeddingBag q_embeddingbag = nnq Embedding from_float float_embedding expected_name = QuantizedEmbedding q_embeddingbag inputs assertTrue expected_name str q_embeddingbag QuantizationLiteTestCase QuantizationTestCase _create_quantized_model model_class type torch nn Module kwargs Creates quantized model testing mobile script modules qengine = qnnpack override_quantized_engine qengine FIXME rec shouldn t qconfig passed quantize qconfig = torch ao quantization get_default_qconfig qengine noqa F model = model_class kwargs model = quantize model test_only_eval_fn calib_data model _compare_script_and_mobile model torch nn Module input torch Tensor Compares numerical outputs script lite modules qengine = qnnpack override_quantized_engine qengine script_module = torch jit script model script_module_result = script_module input max_retry = retry range max_retry + retries ` max_retry ` times breaks iff succeeds throws exception try buffer = io BytesIO script_module _save_to_buffer_for_lite_interpreter buffer seek mobile_module = _load_for_lite_interpreter buffer mobile_module_result = mobile_module input torch testing assert_close script_module_result mobile_module_result mobile_module_forward_result = mobile_module forward input torch testing assert_close script_module_result mobile_module_forward_result mobile_module_run_method_result = mobile_module run_method forward input torch testing assert_close script_module_result mobile_module_run_method_result except AssertionError e retry == max_retry raise e continue break PT EQuantizationTestCase QuantizationTestCase Base QuantizationTestCase PT some helper methods _MAP_TO_FX_TRACED_OPS = torch ops quantized_decomposed quantize_per_tensor torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed dequantize_per_tensor torch ops quantized_decomposed dequantize_per_tensor default torch ops quantized_decomposed quantize_per_channel torch ops quantized_decomposed quantize_per_channel default torch ops quantized_decomposed dequantize_per_channel torch ops quantized_decomposed dequantize_per_channel default torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_tensor tensor torch ops quantized_decomposed dequantize_per_tensor tensor _test_quantizer model example_inputs quantizer expected_node_occurrence expected_node_list=None check_against_fx_quant=False fx_qconfig_mapping=None export_with_dynamic_shape=False is_qat=False is_debug_mode=False training_ir_node_occurrence=None resetting dynamo cache torch _dynamo reset m_eager = model eval program capture m = copy deepcopy m_eager dynamic_shapes = tuple torch export Dim dim i == None i range len example_inputs m = export m example_inputs dynamic_shapes=dynamic_shapes export_with_dynamic_shape None strict=True module is_qat m = prepare_qat_pt e m quantizer m = prepare_pt e m quantizer is_debug_mode print prepared model m Calibrate m example_inputs m = convert_pt e m is_debug_mode print quantized model m pt _quant_output = m example_inputs ns = NodeSpec node_occurrence = ns call_function k v k v expected_node_occurrence items expected_node_list None expected_node_list = node_list = ns call_function n n expected_node_list checkGraphModuleNodes m expected_node_occurrence=node_occurrence expected_node_list=node_list check_against_fx_quant qconfig_mapping = fx_qconfig_mapping backend_config = get_executorch_backend_config m_copy = copy deepcopy m_eager m_fx = prepare_fx m_copy qconfig_mapping example_inputs backend_config=backend_config m_fx example_inputs m_fx = _convert_to_reference_decomposed_fx m_fx backend_config=backend_config m_fx = export m_fx example_inputs dynamic_shapes=dynamic_shapes export_with_dynamic_shape None strict=True module node_occurrence = k v PT EQuantizationTestCase _MAP_TO_FX_TRACED_OPS items k expected_node_occurrence node_occurrence ns call_function v = expected_node_occurrence k training_ir_node_occurrence None node_occurrence = ns call_function k v k v training_ir_node_occurrence items checkGraphModuleNodes m_fx expected_node_occurrence=node_occurrence fx_quant_output = m_fx example_inputs assertEqual fx_quant_output pt _quant_output m _quantize m quantizer example_inputs is_qat bool = False resetting dynamo cache torch _dynamo reset m = export m example_inputs strict=True module is_qat m = prepare_qat_pt e m quantizer m = prepare_pt e m quantizer m example_inputs m = convert_pt e m m _get_pt e_quantized_linear is_per_channel=False - torch fx GraphModule M torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x quantizer = XNNPACKQuantizer operator_config = get_symmetric_quantization_config is_per_channel=is_per_channel quantizer set_global operator_config example_inputs = torch randn m = M eval _quantize m quantizer example_inputs Below series toy models use testing quantization SingleLayerLinearModel torch nn Module __init__ - None super __init__ fc = torch nn Linear dtype=torch float forward x x = fc x x get_example_inputs - tuple Any torch rand AnnotatedSingleLayerLinearModel torch nn Module __init__ qengine= fbgemm super __init__ qconfig = torch ao quantization get_default_qconfig qengine fc = QuantWrapper torch nn Linear dtype=torch float forward x x = fc x x get_example_inputs - tuple Any torch rand SingleLayerLinearDynamicModel torch nn Module __init__ qengine= fbgemm super __init__ qconfig = torch ao quantization get_default_qconfig qengine fc = torch nn Linear dtype=torch float forward x x = fc x x get_example_inputs - tuple Any torch rand LinearAddModel nn Module __init__ - None super __init__ fc = torch nn Linear dtype=torch float fc = torch nn Linear dtype=torch float forward x x = fc x x = torch add x x = fc x x get_example_inputs - tuple Any torch rand RNNDynamicModel torch nn Module __init__ mod_type super __init__ qconfig = default_dynamic_qconfig mod_type == GRU mod = torch nn GRU dtype=torch float mod_type == LSTM mod = torch nn LSTM dtype=torch float forward x x = mod x x RNNCellDynamicModel torch nn Module __init__ mod_type super __init__ qconfig = default_dynamic_qconfig mod_type == GRUCell mod = torch nn GRUCell dtype=torch float mod_type == LSTMCell mod = torch nn LSTMCell dtype=torch float mod_type == RNNReLU mod = torch nn RNNCell nonlinearity= relu dtype=torch float mod_type == RNNTanh mod = torch nn RNNCell nonlinearity= tanh dtype=torch float forward x x = mod x x LSTMwithHiddenDynamicModel torch nn Module __init__ qengine= fbgemm super __init__ qconfig = torch ao quantization get_default_qconfig qengine lstm = torch nn LSTM dtype=torch float forward x hid x hid = lstm x hid x hid ConvModel torch nn Module __init__ - None super __init__ conv = torch nn Conv d bias=False dtype=torch float forward x x = conv x x get_example_inputs - tuple Any torch rand ConvTransposeModel torch nn Module __init__ - None super __init__ conv = torch nn ConvTranspose d bias=False dtype=torch float forward x x = conv x x get_example_inputs - tuple Any torch rand AnnotatedConvModel torch nn Module __init__ qengine super __init__ qconfig = torch ao quantization get_default_qconfig qengine conv = torch nn Conv d bias=False dtype=torch float quant = QuantStub dequant = DeQuantStub forward x x = quant x x = conv x x = dequant x x get_example_inputs - tuple Any torch rand AnnotatedConvTransposeModel torch nn Module __init__ qengine super __init__ qconfig = torch ao quantization get_default_qconfig qengine conv = torch nn ConvTranspose d bias=False dtype=torch float quant = QuantStub dequant = DeQuantStub forward x x = quant x x = conv x x = dequant x x get_example_inputs - tuple Any torch rand ConvBnModel torch nn Module __init__ - None super __init__ conv = torch nn Conv d bias=False dtype=torch float bn = torch nn BatchNorm d dtype=torch float forward x x = conv x x = bn x x get_example_inputs - tuple Any torch rand AnnotatedConvBnModel torch nn Module __init__ - None super __init__ qconfig = default_qconfig conv = torch nn Conv d bias=False dtype=torch float bn = torch nn BatchNorm d dtype=torch float quant = QuantStub dequant = DeQuantStub forward x x = quant x x = conv x x = bn x x = dequant x x get_example_inputs - tuple Any torch rand ConvBnReLUModel torch nn Module __init__ - None super __init__ conv = torch nn Conv d bias=False dtype=torch float bn = torch nn BatchNorm d dtype=torch float relu = nn ReLU inplace=True forward x x = conv x x = bn x x = relu x x get_example_inputs - tuple Any torch rand AnnotatedConvBnReLUModel torch nn Module __init__ qengine= fbgemm super __init__ qconfig = torch ao quantization get_default_qconfig qengine conv = torch nn Conv d bias=False dtype=torch float bn = torch nn BatchNorm d dtype=torch float relu = nn ReLU inplace=True quant = QuantStub dequant = DeQuantStub forward x x = quant x x = conv x x = bn x x = relu x x = dequant x x fuse_model TODO remove check define two fuse_modules function module training torch ao quantization fuse_modules_qat conv bn relu inplace=True torch ao quantization fuse_modules conv bn relu inplace=True get_example_inputs - tuple Any torch rand TwoLayerConvModel torch nn Module __init__ - None super __init__ conv = torch nn Conv d bias=False dtype=torch float conv = torch nn Conv d bias=False dtype=torch float forward x x = conv x x = conv x x get_example_inputs - tuple Any torch rand TwoLayerLinearModel torch nn Module __init__ - None super __init__ fc = torch nn Linear dtype=torch float fc = torch nn Linear dtype=torch float forward x x = fc x x = fc x x get_example_inputs - tuple Any torch rand LinearModelWithSubmodule nn Module __init__ - None super __init__ subm = TwoLayerLinearModel fc = nn Linear forward x x = subm x x = fc x x get_example_inputs - tuple Any subm get_example_inputs AnnotatedTwoLayerLinearModel torch nn Module __init__ - None super __init__ fc = torch nn Linear dtype=torch float fc = QuantWrapper torch nn Linear dtype=torch float fc qconfig = torch ao quantization get_default_qconfig fbgemm forward x x = fc x x = fc x x get_example_inputs - tuple Any torch rand ActivationsTestModel torch nn Module __init__ - None super __init__ qconfig = torch ao quantization get_default_qconfig fbgemm quant = torch ao quantization QuantStub hardswish = torch nn Hardswish dtype=torch float elu = torch nn ELU dtype=torch float dequant = torch ao quantization DeQuantStub forward x x = quant x x = hardswish x x = elu x x = dequant x x LinearReluModel torch nn Module __init__ - None super __init__ fc = torch nn Linear dtype=torch float relu = torch nn ReLU forward x x = relu fc x x get_example_inputs - tuple Any torch rand LinearReluLinearModel torch nn Module __init__ - None super __init__ fc = torch nn Linear dtype=torch float relu = torch nn ReLU fc = torch nn Linear dtype=torch float forward x x = fc x x = relu x x = fc x x get_example_inputs - tuple Any torch rand LinearReluAddModel torch nn Module __init__ - None super __init__ fc = torch nn Linear dtype=torch float relu = torch nn ReLU fc = torch nn Linear dtype=torch float forward x x = fc x x = relu x x = torch add x x = fc x relu = torch nn ReLU x get_example_inputs - tuple Any torch rand LinearBnLeakyReluModel torch nn Module __init__ with_bn=True super __init__ linear = nn Linear bn d = nn BatchNorm d leaky_relu = nn LeakyReLU with_bn = with_bn forward x x = linear x with_bn x = bn d x x = leaky_relu x x get_example_inputs - tuple Any torch rand LinearTanhModel torch nn Module __init__ - None super __init__ linear = nn Linear tanh = nn Tanh forward x x = linear x x = tanh x x get_example_inputs - tuple Any torch rand ConvBnAddReluModel torch nn Module __init__ with_bn=True with_relu=True left_conv=True two_conv=True use_torch_add=True super __init__ conv = nn Conv d conv = nn Conv d bn = nn BatchNorm d relu = nn ReLU with_bn = with_bn with_relu = with_relu two_conv = two_conv left_conv = left_conv use_torch_add = use_torch_add forward x x two_conv use_torch_add with_bn x = torch add bn conv x conv x x = torch add conv x conv x with_bn x = bn conv x + conv x x = conv x + conv x use_torch_add left_conv with_bn x = torch add bn conv x x x = torch add conv x x with_bn x = torch add x bn conv x x = torch add x conv x left_conv with_bn x = bn conv x + x x = conv x + x with_bn x = x + bn conv x x = x + conv x with_relu x = relu x x get_example_inputs - tuple Any torch rand torch rand TODO fc should conv ConvReluModel torch nn Module __init__ - None super __init__ fc = torch nn Conv d dtype=torch float relu = torch nn ReLU forward x x = relu fc x x get_example_inputs - tuple Any torch rand TODO fc should conv ConvReluConvModel torch nn Module __init__ - None super __init__ fc = torch nn Conv d dtype=torch float relu = torch nn ReLU fc = torch nn Conv d dtype=torch float forward x x = fc x x = relu x x = fc x x get_example_inputs - tuple Any torch rand TODO fc should conv ConvReluAddModel torch nn Module __init__ - None super __init__ fc = torch nn Conv d dtype=torch float relu = torch nn ReLU fc = torch nn Conv d dtype=torch float forward x x = fc x x = relu x x = torch add x x = fc x relu = torch nn ReLU x get_example_inputs - tuple Any torch rand NormalizationTestModel torch nn Module __init__ - None super __init__ quant = torch ao quantization QuantStub fc = torch nn Linear dtype=torch float layer_norm = torch nn LayerNorm group_norm = torch nn GroupNorm instance_norm d = torch nn InstanceNorm d instance_norm d = torch nn InstanceNorm d instance_norm d = torch nn InstanceNorm d forward x x = quant x x = fc x x = layer_norm x x = group_norm x unsqueeze - repeat x = instance_norm d x x = instance_norm d x unsqueeze - x = instance_norm d x unsqueeze - x NestedModel torch nn Module __init__ - None super __init__ sub = LinearReluModel sub = TwoLayerLinearModel fc = torch nn Linear dtype=torch float forward x x = sub x x = sub x x = fc x x AnnotatedNestedModel torch nn Module __init__ qengine super __init__ sub = LinearReluModel sub = TwoLayerLinearModel fc = QuantWrapper torch nn Linear dtype=torch float fc qconfig = default_qconfig sub fc = QuantWrapper sub fc qengine == fbgemm sub fc qconfig = default_per_channel_qconfig sub fc qconfig = default_qconfig forward x x = sub x x = sub x x = fc x x AnnotatedSubNestedModel torch nn Module __init__ - None super __init__ sub = LinearReluModel sub = QuantWrapper TwoLayerLinearModel fc = QuantWrapper torch nn Linear dtype=torch float fc qconfig = default_qconfig sub qconfig = default_qconfig forward x x = sub x x = sub x x = fc x x AnnotatedCustomConfigNestedModel torch nn Module __init__ - None super __init__ sub = LinearReluModel sub = TwoLayerLinearModel fc = QuantWrapper torch nn Linear dtype=torch float fc qconfig = default_qconfig sub qconfig = default_qconfig custom_options = dtype torch quint qscheme torch per_tensor_affine custom_qconfig = QConfig activation=default_observer with_args custom_options weight=default_weight_observer sub fc qconfig = custom_qconfig sub fc = QuantWrapper sub fc sub fc = QuantWrapper sub fc forward x x = sub x x = sub x x = fc x x QuantSubModel torch nn Module __init__ - None super __init__ sub = LinearReluModel sub = QuantWrapper TwoLayerLinearModel sub qconfig = default_qconfig fc = torch nn Linear dtype=torch float fc qconfig = default_qconfig forward x x = sub x x = sub x x = fc x x InnerModule torch nn Module __init__ - None super __init__ fc = torch nn Linear dtype=torch float relu = torch nn ReLU fc = torch nn Linear dtype=torch float relu = torch nn ReLU forward x relu fc relu fc x fuse_modules fusable_layers = named_children = list named_children idx current_name layer enumerate named_children isinstance layer torch nn Linear idx = len named_children - break isinstance named_children idx + torch nn ReLU fusable_layers append current_name named_children idx + TODO remove check define two fuse_modules function module training torch ao quantization fuse_modules_qat fusable_layers inplace=True torch ao quantization fuse_modules fusable_layers inplace=True FunctionalLinear torch nn Module __init__ - None super __init__ weight = torch rand bias = torch zeros forward x F linear x weight bias get_example_inputs - tuple Any torch rand SingleLayerFunctionalLinearModel torch nn Module __init__ - None super __init__ linear = FunctionalLinear forward x x = linear x x get_example_inputs - tuple Any linear get_example_inputs TwoLayerFunctionalLinearModel torch nn Module __init__ - None super __init__ linear = FunctionalLinear linear = FunctionalLinear forward x x = linear x x = linear x x get_example_inputs - tuple Any linear get_example_inputs FunctionalLinearAddModel torch nn Module __init__ - None super __init__ linear = FunctionalLinear linear = FunctionalLinear forward x x = linear x x = torch add x x = linear x x get_example_inputs - tuple Any linear get_example_inputs FunctionalLinearReluModel nn Module __init__ - None super __init__ linear = FunctionalLinear forward x x = linear x x = F relu x x get_example_inputs - tuple Any linear get_example_inputs FunctionalLinearReluLinearModel nn Module __init__ - None super __init__ linear = FunctionalLinear relu = nn ReLU linear = FunctionalLinear forward x x = linear x x = relu x x = linear x x get_example_inputs - tuple Any linear get_example_inputs FunctionalConv d torch nn Module __init__ - None super __init__ weight = torch rand bias = torch rand stride = padding = dilation = groups = forward x F conv d x weight bias stride padding dilation groups get_example_inputs - tuple Any torch rand SingleLayerFunctionalConvModel torch nn Module __init__ - None super __init__ conv = FunctionalConv d forward x x = conv x x get_example_inputs - tuple Any conv get_example_inputs TwoLayerFunctionalConvModel torch nn Module __init__ - None super __init__ conv = FunctionalConv d conv = FunctionalConv d forward x x = conv x x = conv x x get_example_inputs - tuple Any conv get_example_inputs FunctionalConvReluModel nn Module __init__ - None super __init__ conv = FunctionalConv d forward x x = conv x x = F relu x x get_example_inputs - tuple Any conv get_example_inputs FunctionalConvReluConvModel nn Module __init__ - None super __init__ conv = FunctionalConv d relu = nn ReLU conv = FunctionalConv d forward x x = conv x x = relu x x = conv x x get_example_inputs - tuple Any conv get_example_inputs SkipQuantModel torch nn Module r We can skip quantization explicitly setting qconfig submodule None __init__ - None super __init__ sub = InnerModule fc = torch nn Linear dtype=torch float forward x fc sub x fuse_modules sub fuse_modules AnnotatedSkipQuantModel torch nn Module r We can skip quantization explicitly setting qconfig submodule None __init__ qengine super __init__ qconfig = torch ao quantization get_default_qconfig qengine sub = QuantWrapper InnerModule fc = torch nn Linear dtype=torch float don t quantize fc fc qconfig = None forward x fc sub x fuse_modules sub module fuse_modules QuantStubModel torch nn Module r A Module manually inserted ` QuantStub ` ` DeQuantStub ` __init__ - None super __init__ qconfig = torch ao quantization get_default_qconfig qnnpack quant = QuantStub dequant = DeQuantStub fc = torch nn Linear dtype=torch float forward x x = quant x x = fc x dequant x ManualLinearQATModel torch nn Module r A Module manually inserted ` QuantStub ` ` DeQuantStub ` __init__ qengine super __init__ qconfig = torch ao quantization get_default_qat_qconfig qengine quant = QuantStub dequant = DeQuantStub fc = torch nn Linear dtype=torch float fc = torch nn Linear dtype=torch float forward x x = quant x x = fc x x = fc x dequant x ManualDropoutQATModel torch nn Module r A Module manually inserted ` QuantStub ` ` DeQuantStub ` __init__ qengine super __init__ qconfig = torch ao quantization get_default_qat_qconfig qengine quant = QuantStub dequant = DeQuantStub fc = torch nn Linear dtype=torch float dropout = torch nn Dropout forward x x = quant x x = fc x x = dropout x dequant x ManualLinearDynamicQATModel torch nn Module r A Module uses dynamic QAT default __init__ qconfig=None super __init__ qconfig = qconfig default_dynamic_qat_qconfig fc = torch nn Linear dtype=torch float fc = torch nn Linear dtype=torch float forward x x = fc x x = fc x x ManualConvLinearQATModel torch nn Module r A module manually inserted ` QuantStub ` ` DeQuantStub ` contains both linear conv modules __init__ qconfig=None super __init__ qconfig = qconfig qconfig torch ao quantization get_default_qat_qconfig qnnpack quant = QuantStub dequant = DeQuantStub conv = torch nn Conv d kernel_size= dtype=torch float fc = torch nn Linear dtype=torch float fc = torch nn Linear dtype=torch float forward x x = quant x x = conv x x = x view - contiguous x = fc x x = fc x dequant x ManualConvLinearSymmQATModel ManualConvLinearQATModel r Same ManualConvLinearQATModule Symmetric Quantization Supported only qnnpack __init__ - None super __init__ default_symmetric_qnnpack_qat_qconfig ManualEmbeddingBagLinear nn Module __init__ - None super __init__ emb = nn EmbeddingBag num_embeddings= embedding_dim= mode= sum emb qconfig = default_embedding_qat_qconfig quant = QuantStub dequant = DeQuantStub linear = nn Linear dtype=torch float qconfig = get_default_qat_qconfig qnnpack forward input torch Tensor offsets Optional torch Tensor = None per_sample_weights Optional torch Tensor = None x = emb input offsets per_sample_weights x = quant x x = linear x dequant x DeFusedEmbeddingBagLinear nn Module r A module simulate QAT embedding bag linear layer module uses separate embedding bagging op similar which described EmbeddingBag documentation https pytorch org docs stable generated torch nn EmbeddingBag html __init__ - None super __init__ emb = nn Embedding num_embeddings= embedding_dim= emb qconfig = default_embedding_qat_qconfig bagging_op = torch sum quant = QuantStub dequant = DeQuantStub linear = nn Linear dtype=torch float qconfig = get_default_qat_qconfig qnnpack forward input torch Tensor - torch Tensor x = bagging_op emb input dim= x = quant x x = linear x dequant x SubModelForFusion nn Module __init__ - None super __init__ conv = nn Conv d bias=None dtype=torch float bn = nn BatchNorm d dtype=torch float forward x x = conv x x = bn x x SubModelWithoutFusion nn Module __init__ - None super __init__ conv = nn Conv d bias=None dtype=torch float relu = nn ReLU inplace=False dtype=torch float forward x relu conv x ModelForFusion nn Module __init__ qconfig super __init__ conv = nn Conv d bias=None dtype=torch float bn = nn BatchNorm d dtype=torch float relu = nn ReLU inplace=True dtype=torch float sub = SubModelForFusion sub = SubModelWithoutFusion fc = nn Linear dtype=torch float quant = QuantStub dequant = DeQuantStub qconfig = qconfig conv = nn Conv d bias=None dtype=torch float relu = nn ReLU inplace=False dtype=torch float bn = nn BatchNorm d dtype=torch float relu = nn ReLU inplace=True dtype=torch float conv = nn Conv d dtype=torch float bn = nn BatchNorm d dtype=torch float relu = nn ReLU inplace=True dtype=torch float don t quantize sub sub qconfig = None fc qconfig = None forward x x = x squeeze x = quant x x = conv x x = bn x x = relu x x = x unsqueeze y = x unsqueeze x = conv x x = bn x x = relu x x = sub x x = dequant x x = sub x x = x reshape - contiguous x = fc x y = conv y y = relu y y = bn y y = relu y y = dequant y x ConvBNReLU nn Sequential __init__ - None super __init__ nn Conv d bias=False nn BatchNorm d nn ReLU inplace=False ModelWithSequentialFusion nn Module __init__ - None super __init__ conv = nn Conv d relu = nn ReLU inplace=False layers = ConvBNReLU _ range features = nn Sequential layers head = nn Linear nn ReLU inplace=False classifier = nn Sequential head seq = nn Sequential quant = QuantStub dequant = DeQuantStub forward x x = quant x x = conv x x = relu x x = features x x = torch reshape x - x = classifier x x = seq x x = dequant x x ModelForFusionWithBias nn Module __init__ - None super __init__ conv = nn Conv d bias=True dtype=torch float bn = nn BatchNorm d dtype=torch float relu = nn ReLU inplace=True dtype=torch float conv = nn Conv d bias=True dtype=torch float bn = nn BatchNorm d dtype=torch float quant = QuantStub dequant = DeQuantStub forward x x = quant x x = conv x x = bn x x = relu x x = conv x x = bn x x = dequant x x ModelForLinearBNFusion nn Module __init__ - None super __init__ fc = nn Linear bn = nn BatchNorm d nn init uniform_ bn weight nn init uniform_ bn bias forward x bn fc x DummyObserver torch nn Module calculate_qparams forward x x ModelForConvTransposeBNFusion nn Module __init__ - None super __init__ conv = nn ConvTranspose d bn = nn BatchNorm d conv = nn ConvTranspose d bn = nn BatchNorm d conv = nn ConvTranspose d bn = nn BatchNorm d forward x x = conv x x = bn x x = x unsqueeze x = conv x x = bn x x = x unsqueeze x = conv x x = bn x x ModelWithFunctionals torch nn Module __init__ - None super __init__ mycat = nnq FloatFunctional myadd = nnq FloatFunctional myadd_relu = nnq FloatFunctional mymatmul = nnq FloatFunctional Tracing doesn t work yet c ops scalar inputs https github com pytorch pytorch issues my_scalar_add = nnq FloatFunctional my_scalar_mul = nnq FloatFunctional forward x y = mycat cat x x x z = myadd add y y w = myadd_relu add_relu z z u = mymatmul matmul w w T Tracing doesn t work yet c ops scalar inputs https github com pytorch pytorch issues w = my_scalar_add add_scalar w - w = my_scalar_mul mul_scalar w u ResNetBase torch nn Module __init__ - None super __init__ norm_layer = nn BatchNorm d inplanes = conv = nn Conv d inplanes inplanes bias=False bn = norm_layer inplanes relu = nn ReLU relu = nn ReLU downsample = torch nn Identity myop = nn quantized FloatFunctional avgpool = nn AdaptiveAvgPool d fc = torch nn Linear inplanes forward x out = conv x out = bn out out = relu out identity = downsample x out = myop add out identity out = relu out out = avgpool out out = torch flatten out out = fc out out fuse_model TODO remove check define two fuse_model function module training torch ao quantization fuse_modules_qat conv bn relu inplace=True torch ao quantization fuse_modules conv bn relu inplace=True ModelMultipleOps torch nn Module __init__ - None super __init__ norm_layer = nn BatchNorm d inplanes = conv = nn Conv d inplanes inplanes bias=False conv = nn Conv d inplanes inplanes bias=False bn = norm_layer inplanes relu = nn ReLU relu = nn ReLU downsample = torch nn Identity skip_add = nn quantized FloatFunctional cat = nn quantized FloatFunctional avgpool = nn AdaptiveAvgPool d fc = nn Linear forward x out = conv x out = bn out out = relu out identity = downsample x out = skip_add add out identity out = relu out out = avgpool out out = conv out out = torch nn functional max_pool d out out = cat cat out out out = out reshape - out = fc out out Model ensure consistency fake quant true quant Average pooling mean operations modelled accurately fake-quant so model does contain those operations ModelMultipleOpsNoAvgPool torch nn Module __init__ - None super __init__ norm_layer = nn BatchNorm d inplanes = conv = nn Conv d inplanes inplanes bias=False conv = nn Conv d inplanes inplanes bias=False bn = norm_layer inplanes relu = nn ReLU relu = nn ReLU skip_add = nn quantized FloatFunctional cat = nn quantized FloatFunctional maxpool = nn MaxPool d fc = nn Linear forward x out = conv x out = bn out out = relu out skip = conv x out = skip_add add out skip out = relu out out = maxpool out out = conv out out = torch nn functional max_pool d out out = cat cat out out out = out reshape - out = fc out out EmbeddingBagModule torch nn Module __init__ - None super __init__ emb = torch nn EmbeddingBag num_embeddings= embedding_dim= include_last_offset=True scale_grad_by_freq=False mode= sum forward indices offsets per_sample_weights emb indices offsets per_sample_weights EmbeddingModule torch nn Module __init__ - None super __init__ emb = torch nn Embedding num_embeddings= embedding_dim= forward indices emb indices EmbeddingWithStaticLinear torch nn Module __init__ - None super __init__ emb = torch nn EmbeddingBag num_embeddings= embedding_dim= fc = torch nn Linear emb qconfig = float_qparams_weight_only_qconfig qconfig = default_qconfig quant = QuantStub dequant = DeQuantStub forward indices offsets linear_in emb = emb indices offsets q_x = quant linear_in fc = fc q_x fc = dequant fc features = torch cat fc + emb dim= features DenseTopMLP nn Module __init__ dense_dim dense_out embedding_dim top_out_in top_out_out - None super __init__ dense_mlp = nn Sequential nn Linear dense_dim dense_out top_mlp = nn Sequential nn Linear dense_out + embedding_dim top_out_in nn Linear top_out_in top_out_out forward sparse_feature torch Tensor dense torch Tensor - torch Tensor dense_feature = dense_mlp dense features = torch cat dense_feature + sparse_feature dim= out = top_mlp features out thin wrapper around embedding bag because tracing inside nn Embedding bag supported moment top level EmbBagWrapper nn Module __init__ num_embeddings embedding_dim super __init__ emb_bag = nn EmbeddingBag num_embeddings embedding_dim mode= sum forward indices offsets emb_bag indices offsets SparseNNModel nn Module _NUM_EMBEDDINGS = _EMBEDDING_DIM = _DENSE_DIM = _DENSE_OUTPUT = _TOP_OUT_IN = _TOP_OUT_OUT = _TOP_MLP_DIM = __init__ - None super __init__ model_sparse = EmbBagWrapper _NUM_EMBEDDINGS _EMBEDDING_DIM dense_top = DenseTopMLP _DENSE_DIM _DENSE_OUTPUT _EMBEDDING_DIM _TOP_OUT_IN _TOP_OUT_OUT forward sparse_indices torch Tensor sparse_offsets torch Tensor dense torch Tensor - torch Tensor sparse_feature = model_sparse sparse_indices sparse_offsets out = dense_top sparse_feature dense out TestHelperModules ControlFlow torch nn Module forward xs torch Tensor pred torch Tensor pred torch Tensor y torch Tensor - torch Tensor true_nested y torch Tensor - torch Tensor y = y + y y = torch mm y y y false_nested y torch Tensor - torch Tensor torch mm y y true_fn x torch Tensor pred torch Tensor - torch Tensor z = control_flow cond pred true_nested false_nested x x + z false_fn x torch Tensor _ - torch Tensor x cos map_fn x torch Tensor pred torch Tensor pred torch Tensor y torch Tensor - torch Tensor x = x cos y = control_flow cond pred true_fn false_fn y pred x = x + y x sin y = torch mm y y control_flow map map_fn xs pred pred y example_inputs torch ones torch tensor False torch tensor False torch ones Conv dPropAnnotaton torch nn Module __init__ - None super __init__ conv = torch nn Conv d linear = torch nn Linear forward x x = conv x x = x view - x = torch nn functional hardtanh x - x = linear x x Conv dWithObsSharingOps torch nn Module __init__ - None super __init__ conv = torch nn Conv d hardtanh = torch nn Hardtanh adaptive_avg_pool d = torch nn AdaptiveAvgPool d forward x x = conv x x = adaptive_avg_pool d x x = hardtanh x x = torch mean x x Conv dWithTwoLinearPermute torch nn Module __init__ - None super __init__ conv = torch nn Conv d linear = torch nn Linear bias=False linear = torch nn Linear forward x conv_out = conv x permute_out = torch permute conv_out linear linear permute_out Conv dWithTwoLinear torch nn Module __init__ - None super __init__ conv = torch nn Conv d linear = torch nn Linear bias=False linear = torch nn Linear forward x conv_out = conv x reshape_out = torch reshape conv_out linear linear reshape_out ConvLinearWPermute torch nn Module __init__ - None super __init__ conv = torch nn Conv d linear = torch nn Linear forward x conv_out = conv x permute_out = torch permute conv_out linear permute_out TwoLinearModule torch nn Module __init__ - None super __init__ linear = torch nn Linear bias=False linear = torch nn Linear forward x linear linear x example_inputs torch randn ConvMaxPool d torch nn Module __init__ - None super __init__ conv = torch nn Conv d pool = torch nn MaxPool d forward x x = conv x x = pool x x ConvWithAdaptiveAvgPool d torch nn Module __init__ - None super __init__ conv = torch nn Conv d adaptive_avg_pool d = torch nn AdaptiveAvgPool d forward x x = conv x x = adaptive_avg_pool d x x ConvWithBNRelu torch nn Module __init__ relu dim= bn=True bias=True padding= super __init__ convs = torch nn Conv d torch nn Conv d torch nn Conv d bns = torch nn BatchNorm d torch nn BatchNorm d torch nn BatchNorm d conv = convs dim bias=bias padding=padding bn bn = bns dim bn = torch nn Identity relu relu = torch nn ReLU relu = torch nn Identity forward x x = conv x x = bn x relu x ConvTWithBNRelu torch nn Module __init__ relu dim= bn=True bias=True super __init__ convts = torch nn ConvTranspose d torch nn ConvTranspose d bns = torch nn BatchNorm d torch nn BatchNorm d convt = convts dim bias=bias bn bn = bns dim bn = torch nn Identity relu relu = torch nn ReLU relu = torch nn Identity forward x x = convt x x = bn x relu x Conv dThenConv d torch nn Module __init__ - None super __init__ conv d = torch nn Conv d conv d = torch nn Conv d forward x x = conv d x x = x squeeze x = conv d x x example_inputs torch randn Conv dWithCat torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv = torch nn Conv d forward x y x = conv x y = conv y z = torch cat x y dim= z Conv dWithTwoCat torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv = torch nn Conv d forward x x x x x = conv x x = conv x y = torch cat x x dim= z = x + x w = torch cat z y w Conv dWithSplit torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv = torch nn Conv d forward x x = conv x use split so we get list Tensors x x = torch split x dim= y = torch cat x x dim= y example_inputs torch randn ThreeAdd torch nn Module forward x x x x y = x + x z = x + x w = y + z w EmbeddingModule torch nn Module __init__ - None super __init__ emb = torch nn Embedding num_embeddings= embedding_dim= forward indices emb indices EmbeddingConvLinearModule torch nn Module __init__ - None super __init__ emb = torch nn Embedding num_embeddings= embedding_dim= conv = torch nn Conv d linear = torch nn Linear forward indices embeddings = emb indices embeddings = torch unsqueeze embeddings dim= embeddings = torch permute embeddings conv_out = conv embeddings conv_out = torch permute conv_out conv_out = torch squeeze conv_out dim= linear conv_out AddInplaceAdd torch nn Module forward x y x = x + y x += y x MulInplaceMul torch nn Module forward x y x = x y x = y x AddMulScalar torch nn Module forward x x = x + x = x x += x = x ConvBnReLU dAndLinearReLU torch nn Module __init__ - None super __init__ conv_bn_relu = TestHelperModules ConvWithBNRelu relu=True linear = torch nn Linear bias=False relu = torch nn ReLU forward x x = conv_bn_relu x permute_out = torch permute x linear_out = linear permute_out linear_out GroupwiseConv d torch nn Module __init__ - None super __init__ conv = torch nn Conv d groups= forward x conv x example_inputs torch randn LinearReluModel torch nn Module __init__ - None super __init__ fc = torch nn Linear dtype=torch float relu = torch nn ReLU forward x x = relu fc x x _generate_qdq_quantized_model mod inputs is_qat=False is_dynamic=False quantizer=None get_default_quantizer is_qat is_dynamic inputs has_xpu = any isinstance input torch Tensor input device type == xpu input inputs has_xpu quantizer = XPUInductorQuantizer assert is_qat is_dynamic QAT dynamic quantization supported XPU backend currently quantizer set_global xpuiq get_default_xpu_inductor_quantization_config quantizer = X InductorQuantizer quantizer set_global xiq get_default_x _inductor_quantization_config is_qat=is_qat is_dynamic=is_dynamic quantizer maybe_no_grad = contextlib nullcontext is_qat torch no_grad maybe_no_grad export_model = export mod inputs strict=True module check_guards=False quantizer = quantizer quantizer get_default_quantizer is_qat is_dynamic inputs prepare_model = prepare_qat_pt e export_model quantizer is_qat prepare_pt e export_model quantizer prepare_model inputs torch ao quantization move_exported_model_to_eval prepare_model convert_model = convert_pt e prepare_model convert_model