Owner s module unknown math os collections defaultdict typing Any TYPE_CHECKING typing_extensions Self torch torch utils _pytree pytree torch _guards active_fake_mode torch _inductor utils get_device_tflops get_gpu_dram_gbps torch _subclasses fake_tensor FakeTensorMode torch distributed _tools mod_tracker ModTracker torch utils _mode_utils no_dispatch torch utils _python_dispatch TorchDispatchMode torch utils flop_counter flop_registry TYPE_CHECKING collections abc Callable aten = torch ops aten This value hard-coded here https github com pytorch pytorch blob fba d f ff ab e ad fd c cuda CUDACachingAllocator cpp#L _PYTORCH_MIN_ALLOCATE = int os environ get PYTORCH_NO_CUDA_MEMORY_CACHING == No fall-back kernel needed exists view ops _VIEW_OPS = aten lift_fresh aten t aten transpose aten view aten detach aten _unsafe_view aten split aten adjoint aten as_strided aten diagonal aten expand aten expand_as aten movedim aten permute aten select aten squeeze aten mT aten mH aten real aten imag aten view_as aten unflatten aten unfold aten unbind aten unsqueeze aten vsplit aten hsplit aten split_with_sizes aten swapaxes aten swapdims aten chunk We can ignore benchmarking tensor create ops _CREATE_OPS = aten randint aten randn aten rand aten randn_like aten rand_like aten randint_like aten arange aten ones_like aten zeros_like _IGNORE_OPS = _VIEW_OPS &#124; _CREATE_OPS __all__ = RuntimeEstimator RuntimeEstimator TorchDispatchMode Estimates GPU runtime milliseconds using various estimation methods under ` ` FakeTensorMode ` ` This provides ` ` TorchDispatchMode ` ` based context manager can used estimate eager runtime PyTorch functions It supports two estimation modes benchmarking ` operator-level-benchmark ` roofline cost modeling ` operator-level-cost-model ` For modules executed under context manager aggregates forward backward operation runtimes also records their execution orders Attributes mod_runtimes Dict str Dict str float A dictionary module runtimes The key outer dictionary fully qualified name FQN module For each module forward backward runtimes operations aggregated inner dictionary keyed fw bw mod_fw_pre_order List str List module FQNs pre-forward execution order mod_bw_pre_order List str List module FQNs pre-backward execution order mod_fw_post_order List str List module FQNs post-forward execution order mod_bw_post_order List str List module FQNs post-backward execution order total_runtime float The total estimated runtime milliseconds Note The benchmarking estimate mode will execute kernels GPU assumes every operation can run isolation without causing OOM error It also designed used only under ` ` FakeTensorMode ` ` Currently wrapper tensor sub-classes such ` ` DTensor ` ` won t produce correct estimates We plan support them future PRs We only estimate compute time your code has communication will considered Again we will support future PRs Example usage code-block python runtime_estimator = RuntimeEstimator FakeTensorMode module = optimizer = inp = runtime_estimator estimate_mode_type= operator-level-cost-model loss = module inp loss backward optimizer step optimizer zero_grad runtime_estimator display_modulewise_stats _float_types set torch dtype = torch float torch bfloat torch float torch float _no_fallback_kernel set torch _ops _OpNamespace = set fake_mode FakeTensorMode __init__ - None super __init__ _estimate Callable _estimate_mode_type str _mod_tracker = ModTracker mod_runtimes dict str dict str float = defaultdict lambda defaultdict lambda mod_fw_pre_order list str = mod_bw_pre_order list str = mod_fw_post_order list str = mod_bw_post_order list str = total_runtime float = Adapted https github com pytorch pytorch blob b b ee bd b bf c dd torch _subclasses fake_tensor py#L noqa PGH B NB returns fake tensors classmethod _maybe_run_and_benchmark_fallback_kernel type ignore no-untyped-def cls func args kwargs orig_not_implemented_exception Runs benchmarks fallback kernel given function Args func Callable The function benchmark args Tuple The arguments pass function kwargs Dict str Any The keyword arguments pass function orig_not_implemented_exception Exception The original exception raise fallback kernel implemented Returns Tuple Any float A tuple containing result function mean operation time milliseconds these should all supported just safe avoid fallback operators which inplace modify metadata because input fake tensors would umodified torch Tag inplace_view func tags type ignore attr-defined raise orig_not_implemented_exception inp_impls = flat_args args_spec = pytree tree_flatten args kwargs Don t use in_kernel_invocation_manager fake_mode we want do REAL compute meta device no_dispatch to_real_tensor e type ignore no-untyped-def cls fake_mode is_our_fake e e dtype cls _float_types out = torch rand_like e device=e fake_device out = torch ones_like e device=e fake_device e is_sparse out _coalesced_ e is_coalesced inp_impls id out = e out e flat_args = to_real_tensor flat_args args kwargs = pytree tree_unflatten flat_args args_spec r = func args kwargs warmup_iters actual_iters = _ range warmup_iters func args kwargs start_event = torch cuda Event enable_timing=True end_event = torch cuda Event enable_timing=True start_event record torch cuda current_stream _ range actual_iters func args kwargs end_event record torch cuda current_stream torch cuda synchronize cuda_time = start_event elapsed_time end_event mean_op_time = cuda_time actual_iters storages = set e flat_args isinstance e torch Tensor e is_sparse storages add e _typed_storage _cdata TODO also check metadata change inputs proper aliasing metadata relationship between outputs inputs will set up bc conversion device unless we can reuse input impl map_out e type ignore no-untyped-def id e inp_impls isinstance e torch Tensor e is_sparse e _typed_storage _cdata storages raise orig_not_implemented_exception isinstance e torch Tensor id e inp_impls inp_impls id e cls fake_mode fake_tensor_converter from_real_tensor cls fake_mode e e pytree tree_map map_out r mean_op_time classmethod _benchmark_estimate cls func args kwargs - tuple Any float type ignore no-untyped-def Estimates runtime function using benchmarking Args func The function estimate args The arguments pass function kwargs The keyword arguments pass function res The result function Returns Tuple Any float A tuple containing result function mean operation time milliseconds assert isinstance cls fake_mode FakeTensorMode Initialize Assign FakeTensorMode before using function mean_op_time = func _overloadpacket _VIEW_OPS try res mean_op_time = cls _maybe_run_and_benchmark_fallback_kernel func args kwargs NotImplementedError res mean_op_time except NotImplementedError cls _no_fallback_kernel add func _overloadpacket res = func args kwargs res mean_op_time Adapted https github com pytorch pytorch blob b b ee bd b bf c dd torch _inductor scheduler py#L noqa PGH B classmethod _roofline_estimate cls func args kwargs - tuple Any float type ignore no-untyped-def Estimates runtime function using roofline cost model Args func The function estimate args The arguments pass function kwargs The keyword arguments pass function out The output function Returns Tuple Any float A tuple containing result function mean operation time milliseconds assert torch cuda is_available Roofline estimation needs access CUDA capabilities make estimations get_num_bytes t torch Tensor - int Calculates memory consumption tensor Args t torch Tensor The input tensor Returns int The memory consumption tensor bytes num_bytes = t untyped_storage nbytes mem_consumed = math ceil num_bytes _PYTORCH_MIN_ALLOCATE _PYTORCH_MIN_ALLOCATE mem_consumed get_compute_time func_packet args kwargs out out_dtypes - float type ignore no-untyped-def Estimates compute time aten operator Args func_packet The operator overload packet args The arguments operator kwargs The keyword arguments operator out The output operator out_dtypes The output data types Returns float The estimated compute time nanoseconds func_packet flop_registry assert len out_dtypes == f Only support single out dtype got out_dtypes func_packet dtype = out_dtypes pop This actually gives peta-FLOPs s hence multiply e get FLOPs s peak_gpu_flops = get_device_tflops dtype e We can expect achieve theoretical peak flops factor = peak_empirical_flops = factor peak_gpu_flops flop_count_func = flop_registry func_packet We divide factor get MACs multiply accumulate flop_count = flop_count_func args kwargs out_val=out We multiply e get time nano seconds compute_time = flop_count peak_empirical_flops e compute_time get_transfer_time flat_args_kwargs flat_outs - float type ignore no-untyped-def Estimates memory transfer time input output tensors Args flat_args_kwargs List torch Tensor The flat list arguments keyword arguments flat_outs List torch Tensor The flat list outputs Returns float The estimated memory transfer time nanoseconds gpu_memory_bandwidth = get_gpu_dram_gbps read_bytes = sum get_num_bytes t t flat_args_kwargs isinstance t torch Tensor write_bytes = sum get_num_bytes t t flat_outs isinstance t torch Tensor counted_bytes = read_bytes + write_bytes The GPU memory bandwidth GB s so transfer time nanoseconds transfer_time = counted_bytes gpu_memory_bandwidth transfer_time Roofline Cost Model Explanation The roofline cost model estimates execution time operator based device s empirical maximum FLOPs sec pi device DRAM bandwidth beta Variables - pi Maximum empirical FLOPs sec device - beta Maximum empirical device DRAM bandwidth bytes sec device - I Arithmetic intensity operator FLOPs bytes - op_flops FLOPs required operator - op_bytes Bytes transferred DRAM operator Calculation Steps Calculate arithmetic intensity I = op_flops op_bytes Calculate estimated FLOPs sec est_flops_sec = min pi beta I Calculate estimated operator time estimated_op_time = op_flops est_flops_sec This simplifies estimated_op_time = max op_flops pi op_flops beta I Further simplifying estimated_op_time = max op_flops pi op_bytes beta Simplified Formulas - compute_time = op_flops pi - transfer_time = op_bytes beta - estimated_op_time = max compute_time transfer_time kwargs = kwargs kwargs out = func args kwargs op_time = func_packet = func _overloadpacket func_packet _IGNORE_OPS flat_args_kwargs args_spec = pytree tree_flatten args kwargs flat_outs out_spec = pytree tree_flatten out transfer_time = get_transfer_time flat_args_kwargs flat_outs out_dtypes = t dtype t flat_outs isinstance t torch Tensor t dtype cls _float_types args kwargs = pytree tree_unflatten flat_args_kwargs args_spec out = pytree tree_unflatten flat_outs out_spec compute_time = get_compute_time func_packet args kwargs out out_dtypes We get estimated time max transfer time compute time We divide e get time ms op_time = max transfer_time compute_time e out op_time display_modulewise_stats depth int = - None Displays module-wise statistics collected ` ` RuntimeEstimator ` ` Prints pre-forward pre-backward execution orders Displays module-wise forward backward runtimes milliseconds Args depth int The maximum depth module hierarchy display default print Pre-Forward Execution Order mod_fqn mod_fw_pre_order mod_depth = mod_fqn count + mod_depth depth continue print mod_fqn print Pre-Backward Execution Order mod_fqn mod_bw_pre_order mod_depth = mod_fqn count + mod_depth depth continue print mod_fqn mod_fqn runtimes mod_runtimes items mod_depth = mod_fqn count + mod_depth depth continue print f mod_fqn fw runtimes get fw f ms bw runtimes get bw f ms __torch_dispatch__ func types args= kwargs=None type ignore no-untyped-def TODO sanketpurandare Flatten tensors desugaring tensor subclasses TODO sanketpurandare Add logic incorporating communication time res op_time = _estimate func args kwargs par _mod_tracker parents _mod_tracker is_bw mod_runtimes par bw += op_time mod_runtimes par fw += op_time total_runtime += op_time res __call__ estimate_mode_type str - Self Sets estimate mode type Currently supported modes - operator-level-benchmark Estimates runtime using operator benchmarking - operator-level-cost-model Estimates runtime using roofline cost model Args estimate_mode_type str The type estimate mode use Returns RuntimeEstimator The runtime estimator instance Raises NotImplementedError If estimate mode type supported estimate_mode_type == operator-level-benchmark _estimate = RuntimeEstimator _benchmark_estimate estimate_mode_type == operator-level-cost-model _estimate = RuntimeEstimator _roofline_estimate raise NotImplementedError f estimate_mode_type estimate_mode_type supported _estimate_mode_type = estimate_mode_type __enter__ - Self fake_mode = active_fake_mode assert isinstance fake_mode FakeTensorMode No FakeTensorMode found designed used under FakeTensorMode RuntimeEstimator fake_mode = fake_mode total_runtime = mod_runtimes = defaultdict lambda defaultdict lambda mod_fw_pre_order clear mod_bw_pre_order clear mod_fw_post_order clear mod_bw_post_order clear _mod_tracker register_user_hooks pre_fw_hook=lambda mod inp mod_fw_pre_order append _mod_tracker get_known_fqn mod pre_bw_hook=lambda mod g_out mod_bw_pre_order append _mod_tracker get_known_fqn mod post_fw_hook=lambda mod inp out mod_fw_post_order append _mod_tracker get_known_fqn mod post_bw_hook=lambda mod g_inp mod_bw_post_order append _mod_tracker get_known_fqn mod _mod_tracker __enter__ super __enter__ pyrefly ignore bad-override __exit__ args Any - None print f Estimated _estimate_mode_type f total_time total_runtime f ms len _no_fallback_kernel print no_fallback_kernel list _no_fallback_kernel super __exit__ args _mod_tracker clear_user_hooks _mod_tracker __exit__