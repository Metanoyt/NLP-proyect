mypy allow-untyped-defs __future__ annotations copy torch torch nn functional F torch ao quantization observer PerChannelMinMaxObserver torch ao quantization quantizer quantizer QuantizationAnnotation QuantizationSpec Quantizer torch ao quantization quantizer xnnpack_quantizer_utils OperatorConfig OperatorPatternType QuantizationConfig __all__ = get_embedding_operators_config EmbeddingQuantizer get_embedding_operators_config - OperatorConfig weight_quantization_spec = QuantizationSpec dtype=torch uint qscheme=torch per_channel_affine_float_qparams ch_axis= observer_or_fake_quant_ctr=PerChannelMinMaxObserver with_args eps= - quantization_config = QuantizationConfig None None weight_quantization_spec None ops list OperatorPatternType = torch nn Embedding ops append F embedding supported_config_and_operators = OperatorConfig config=quantization_config operators=ops copy deepcopy supported_config_and_operators EmbeddingQuantizer Quantizer __init__ - None super __init__ classmethod get_supported_quantization_configs cls - list QuantizationConfig op_configs set QuantizationConfig = spec spec _ cls get_supported_operators list op_configs classmethod get_supported_operator_for_quantization_config cls quantization_config QuantizationConfig - list OperatorPatternType config ops cls get_supported_operators note assumes each entry cls supported_spec_and_operators corresponds one spec e g we don t have spec op_list spec op_list spec op_list where first second entry have same spec did merge op list config == quantization_config ops annotate model torch fx GraphModule - torch fx GraphModule just handling global spec now _annotate_embedding_ops model graph model _annotate_embedding_ops graph torch fx Graph - None embedding_config OperatorConfig = get_embedding_operators_config node graph nodes Keep node parsing based annotations instead module partitioners just example alternate ways annotating node op == call_function node target torch ops aten embedding default embedding_config config weight None raise ValueError Embedding config must have valid weight quantization spec node meta quantization_annotation = QuantizationAnnotation input_qspec_map= node args embedding_config config weight validate model torch fx GraphModule - None pass classmethod get_supported_operators cls - list OperatorConfig get_embedding_operators_config