Owner s module inductor logging os re shutil sys tempfile unittest pathlib Path torch torch _inductor config test_operators torch _inductor utils fresh_cache torch testing _internal common_utils skipIfWindows torch testing _internal inductor_utils GPU_TYPE HAS_GPU torch testing _internal logging_utils multiple_logs_to_string try try test_torchinductor except ImportError test_torchinductor manual=fbcode caffe test inductor test_inductor-library except unittest SkipTest __name__ == __main__ sys exit raise filesize filename Path assert filename exists f filename missing os stat filename st_size config patch trace enabled True TestDebugTrace test_torchinductor TestCase test_debug_trace torch compile fn b = test_operators realize + + torch matmul b pre_fusion_stream post_fusion_stream ctx = multiple_logs_to_string torch _inductor debug ir_pre_fusion ir_post_fusion TODO aakhundov make work fresh_cache instead force_disable_caches currently latter enabled we get ` inductor fxgraph_cache_hit ` counters so cache actually hit test fails config patch trace debug_dir tempfile mkdtemp force_disable_caches True assertLogs logging getLogger torch _inductor debug level=logging WARNING cm ctx fn torch randn torch randn m = None log_line cm output Search warning message debug trace file path m = re match r WARNING debug trace log_line m break assertTrue m debug trace file path found logs For type checking have ensure s none assert m None filename = Path m group assertTrue filename is_dir assertGreater filesize filename fx_graph_readable py assertGreater filesize filename fx_graph_runnable py assertGreater filesize filename fx_graph_transformed py assertGreater filesize filename output_code py pre_fusion_logs = pre_fusion_stream getvalue strip assertExpectedInline pre_fusion_logs \ BEFORE FUSION op SchedulerNode ComputedBuffer op writes = MemoryDep buf c c op unmet_dependencies = op met_dependencies = MemoryDep arg _ c c op outputs = buf ComputedBuffer buf layout = FixedLayout cpu torch float size= stride= buf users = NodeUser node=SchedulerNode name= op can_inplace=True is_weak=False op group device = cpu op group iteration = op sizes = arg _ _layout = FixedLayout cpu torch float size= stride= buf _layout = FixedLayout cpu torch float size= stride= op _loop_body var_ranges = p index = p body ops get_index = get_index index load = ops load arg _ get_index constant = ops constant torch float add = ops add load constant get_index_ = get_index index store = ops store buf get_index_ add None store op SchedulerNode ComputedBuffer op writes = MemoryDep buf c c op unmet_dependencies = MemoryDep buf c c op met_dependencies = op outputs = buf ComputedBuffer buf layout = FixedLayout cpu torch float size= stride= buf users = NodeUser node=ExternKernelSchedulerNode name= op can_inplace=False is_weak=False op group device = cpu op group iteration = op sizes = buf _layout = FixedLayout cpu torch float size= stride= buf _layout = FixedLayout cpu torch float size= stride= op _loop_body var_ranges = p index = p body ops get_index = get_index index load = ops load buf get_index constant = ops constant torch float add = ops add load constant get_index_ = get_index index store = ops store buf get_index_ add None store op ExternKernelSchedulerNode ExternKernelOut op writes = StarDep name= buf mode=None op unmet_dependencies = StarDep name= buf mode=None op met_dependencies = StarDep name= arg _ mode=None op outputs = buf ExternKernelOut buf layout = FixedLayout cpu torch float size= stride= buf users = NodeUser node=OUTPUT can_inplace=False is_weak=False op node kernel = extern_kernels mm post_fusion_logs = post_fusion_stream getvalue strip assertExpectedInline post_fusion_logs \ AFTER FUSION op _op FusedSchedulerNode SchedulerNode SchedulerNode op _op writes = MemoryDep buf c c MemoryDep buf c c op _op unmet_dependencies = op _op met_dependencies = MemoryDep arg _ c c op _op outputs = buf ComputedBuffer buf layout = FixedLayout cpu torch float size= stride= buf users = NodeUser node=SchedulerNode name= op can_inplace=True is_weak=False buf ComputedBuffer buf layout = FixedLayout cpu torch float size= stride= buf users = NodeUser node=ExternKernelSchedulerNode name= op can_inplace=False is_weak=False op _op snodes = op SchedulerNode ComputedBuffer op writes = MemoryDep buf c c op unmet_dependencies = op met_dependencies = MemoryDep arg _ c c op outputs = buf ComputedBuffer buf layout = FixedLayout cpu torch float size= stride= buf users = NodeUser node=SchedulerNode name= op can_inplace=True is_weak=False op group device = cpu op group iteration = op sizes = arg _ _layout = FixedLayout cpu torch float size= stride= buf _layout = FixedLayout cpu torch float size= stride= op _loop_body var_ranges = p index = p body ops get_index = get_index index load = ops load arg _ get_index constant = ops constant torch float add = ops add load constant get_index_ = get_index index store = ops store buf get_index_ add None store op _op snodes = op SchedulerNode ComputedBuffer op writes = MemoryDep buf c c op unmet_dependencies = MemoryDep buf c c op met_dependencies = op outputs = buf ComputedBuffer buf layout = FixedLayout cpu torch float size= stride= buf users = NodeUser node=ExternKernelSchedulerNode name= op can_inplace=False is_weak=False op group device = cpu op group iteration = op sizes = buf _layout = FixedLayout cpu torch float size= stride= buf _layout = FixedLayout cpu torch float size= stride= op _loop_body var_ranges = p index = p body ops get_index = get_index index load = ops load buf get_index constant = ops constant torch float add = ops add load constant get_index_ = get_index index store = ops store buf get_index_ add None store op ExternKernelSchedulerNode ExternKernelOut op writes = StarDep name= buf mode=None op unmet_dependencies = StarDep name= buf mode=None op met_dependencies = StarDep name= arg _ mode=None op outputs = buf ExternKernelOut buf layout = FixedLayout cpu torch float size= stride= buf users = NodeUser node=OUTPUT can_inplace=False is_weak=False op node kernel = extern_kernels mm intentionally only cleanup success so debugging test easier shutil rmtree filename AOT compiler have supported windows yet skipIfWindows test_debug_printer_const Test having const example_input does break debug printer Model torch nn Module forward x ks x sum example_inputs = torch tensor dtype=torch int const input will filtered examples _ = torch _export aot_compile Model example_inputs unittest skipIf HAS_GPU requires GPU test_debug_multi_tempalte ToyModel torch nn Module __init__ - None super __init__ l = torch nn Linear relu = torch nn ReLU forward x relu l x no failure assertLogs logging getLogger torch _inductor debug level=logging WARNING fresh_cache m = ToyModel device=GPU_TYPE m = torch compile m mode= max-autotune input_tensor = torch randn device=GPU_TYPE m input_tensor __name__ == __main__ torch _inductor test_case run_tests torch testing _internal inductor_utils HAS_CPU HAS_CPU run_tests needs= filelock