mypy allow-untyped-defs pyre-unsafe argparse io os random shlex subprocess time numpy np torch torch distributed dist torch distributed autograd dist_autograd torch distributed rpc rpc torch multiprocessing mp torch nn nn torch optim optim torch distributed optim DistributedOptimizer torch distributed rpc RRef TensorPipeRpcBackendOptions torch distributed rpc backend_registry BackendType torch nn parallel DistributedDataParallel DDP Config NUM_TRAINERS = NUM_PS = NUM_EMBEDDINGS = EMBEDDING_DIM = WARMUP_CYCLES = HybridModel torch nn Module r The model consists sparse part dense part The dense part nn Linear module replicated across all trainers using DistributedDataParallel The sparse part has nn EmbeddingBags stored multiple parameter servers The model holds Remote Reference embedding tables parameter servers __init__ emb_rref_list device super __init__ emb_rref_list = emb_rref_list fc = torch nn Linear fc = torch nn Linear relu = torch nn ReLU fc = torch nn Linear fc = torch nn Linear fc = torch nn Linear sec = nn Sequential fc fc relu fc fc fc ddp = DDP sec device device_ids= device device = device forward indices offsets emb_lookups = emb_rref emb_rref_list emb_lookups append emb_rref rpc_sync forward indices offsets embedding_sum input offsets emb_lookups_cat = torch cat emb_lookups dim= Make sure combined PS dimension always bigger equal than FC input assert NUM_PS EMBEDDING_DIM = dim_normalizer = int NUM_PS EMBEDDING_DIM emb_lookups_reshaped = emb_lookups_cat reshape type ignore possibly-undefined pyrefly ignore unbound-name emb_lookups_cat shape dim_normalizer ddp emb_lookups_reshaped _retrieve_embedding_parameters emb_rref RRef p p emb_rref local_value parameters _print_header _print_cont \n _print_cont _ _print_cont f sec epoch s epoch sec s _print_cont \n _print_benchmark prefix nelem measurements measurements = sorted measurements _print_cont f prefix s p v = np percentile measurements p _print_cont f p p d v f s nelem v d s _print_cont \n _print_cont msg print msg end= flush=True _run_printable cmd proc = subprocess run shlex split cmd capture_output=True check=False type ignore call-overload assert proc returncode == buffer = io BytesIO torch save proc stdout decode utf- buffer input_tensor = torch ByteTensor list buffer getvalue output = buffer = io BytesIO np asarray input_tensor tobytes output append torch load buffer output _run_trainer emb_rref_list rank r Each trainer runs forward pass which involves embedding lookup parameter servers running nn Linear locally During backward pass DDP responsible aggregating gradients dense part nn Linear distributed autograd ensures gradients updates propagated parameter servers Setup model model = HybridModel emb_rref_list rank Retrieve all model parameters rrefs DistributedOptimizer Retrieve parameters all embedding tables current trainer model_parameter_rrefs = ind emb_rref enumerate emb_rref_list ps_name = f ps ind model_parameter_rrefs extend rpc rpc_sync ps_name _retrieve_embedding_parameters args= emb_rref model parameters only includes local parameters model_parameter_rrefs extend RRef param param model parameters Setup distributed optimizer opt = DistributedOptimizer optim SGD model_parameter_rrefs lr= criterion = torch nn CrossEntropyLoss get_next_batch rank _ range num_indices = random randint indices = torch LongTensor num_indices random_ NUM_EMBEDDINGS Generate offsets offsets = start = batch_size = while start num_indices offsets append start start += random randint batch_size += offsets_tensor = torch LongTensor offsets target = torch LongTensor batch_size random_ cuda rank yield indices offsets_tensor target measurements = Include warm-up cycles during training _ range + WARMUP_CYCLES start = time time batch_size = create distributed autograd context indices offsets target get_next_batch rank batch_size += len target dist_autograd context context_id output = model indices offsets loss = criterion output target Run distributed backward pass dist_autograd backward context_id loss Run distributed optimizer Gradients propagated all way parameter servers opt step context_id Not necessary zero grads each iteration creates different distributed autograd context which hosts different grads measurements append time time - start print Training done epoch format epoch Throw away warm-up measurements measurements = measurements WARMUP_CYCLES rank measurements batch_size type ignore possibly-undefined run_worker rank world_size r Initialize RPC calls function shuts down RPC Using different port numbers TCP init_method init_rpc init_process_group avoid port conflicts rpc_backend_options = TensorPipeRpcBackendOptions rpc_backend_options init_method = tcp localhost Rank Master rank == NUM_TRAINERS + NUM_PS rpc init_rpc master rank=rank backend=BackendType TENSORPIPE type ignore attr-defined world_size=world_size Build Embedding tables Parameter Servers emb_rref_list = index = while index NUM_PS ps_name = f ps index emb_rref = rpc remote ps_name torch nn EmbeddingBag args= NUM_EMBEDDINGS EMBEDDING_DIM kwargs= mode sum emb_rref_list append emb_rref index += Run training loop trainers futs = trainer_rank range NUM_TRAINERS trainer_name = f trainer trainer_rank fut = rpc rpc_async trainer_name _run_trainer args= emb_rref_list trainer_rank futs append fut _print_header measurements_all_trainers = batch_size_all_trainers = Wait all training finish fut futs rank measurements batch_size = fut wait _print_benchmark f Trainer rank batch_size measurements batch_size_all_trainers += batch_size measurements_all_trainers append measurements _print_benchmark All batch_size_all_trainers measurements_all_trainers Rank - Trainers rank = rank NUM_PS Initialize process group Distributed DataParallel trainers dist init_process_group backend=dist Backend GLOO rank=rank world_size=NUM_TRAINERS init_method= tcp localhost Initialize RPC Trainer just waits RPCs master trainer_name = f trainer rank rpc init_rpc trainer_name rank=rank world_size=world_size rpc_backend_options=rpc_backend_options Rank - Parameter Servers rank = NUM_TRAINERS rank NUM_TRAINERS + NUM_PS ps_name = f ps rank - NUM_TRAINERS rpc init_rpc ps_name rank=rank world_size=world_size backend=BackendType TENSORPIPE type ignore attr-defined rpc_backend_options=rpc_backend_options parameter server do nothing block until all rpcs finish rpc shutdown __name__ == __main__ Initializing distributed environment output = _run_printable nvidia-smi topo -m print ------------------------------------------- print Info print ------------------------------------------- print print f PyTorch version torch __version__ print f CUDA version torch version cuda print print ------------ nvidia-smi topo -m ----------- print print output print ------------------------------------------- print PyTorch Distributed Benchmark DDP RPC print ------------------------------------------- Cmd arguments enable automated runs e g Chronos SSH etc parser = argparse ArgumentParser description= PyTorch DDP RPC Benchmark parser add_argument -- master-addr type=str default= localhost help= Address master node parser add_argument -- master-port type=str default= help= Master port parser add_argument -- number-trainers type=int default=NUM_TRAINERS help= Number Trainer Nodes parser add_argument -- number-ps type=int default=NUM_PS help= Number Parameter Servers parser add_argument -- number-embeddings type=int default=NUM_EMBEDDINGS help= Number test embeddings generated parser add_argument -- embedding-dim type=int default=EMBEDDING_DIM help= Number embedding dimensions parser add_argument -- warmup-cycles type=int default=WARMUP_CYCLES help= Number cycles warm-up each process before running benchmark args = parser parse_args os environ MASTER_ADDR = args master_addr os environ MASTER_PORT = args master_port NUM_TRAINERS = args number_trainers NUM_PS = args number_ps NUM_EMBEDDINGS = args number_embeddings EMBEDDING_DIM = args embedding_dim WARMUP_CYCLES = args warmup_cycles Defaults trainers rank - parameter servers rank - master rank world_size = NUM_TRAINERS + NUM_PS + Trainers + PS + Master mp spawn run_worker args= world_size nprocs=world_size join=True