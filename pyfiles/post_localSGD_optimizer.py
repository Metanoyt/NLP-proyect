mypy allow-untyped-defs warnings torch torch distributed algorithms model_averaging averagers averagers PostLocalSGDOptimizer torch optim Optimizer r Wraps arbitrary ` torch optim Optimizer ` runs ` post-local SGD https arxiv org abs ` _ This optimizer runs local optimizer every step After warm-up stage averages parameters periodically after local optimizer applied Args optim The local optimizer averager A model averager instance run post-localSGD algorithm Example xdoctest +SKIP undefined variables torch torch distributed dist torch distributed algorithms model_averaging averagers averagers torch nn nn torch distributed optim PostLocalSGDOptimizer torch distributed algorithms ddp_comm_hooks post_localSGD_hook PostLocalSGDState post_localSGD_hook model = nn parallel DistributedDataParallel module device_ids= rank output_device=rank Register post-localSGD communication hook state = PostLocalSGDState process_group=None subgroup=None start_localSGD_iter= model register_comm_hook state post_localSGD_hook Create post-localSGD optimizer wraps local optimizer Note ` ` warmup_steps ` ` used ` ` PostLocalSGDOptimizer ` ` must same ` ` start_localSGD_iter ` ` used ` ` PostLocalSGDState ` ` local_optim = torch optim SGD params=model parameters lr= opt = PostLocalSGDOptimizer optim=local_optim averager=averagers PeriodicModelAverager period= warmup_steps= In first steps DDP runs global gradient averaging every step After steps DDP runs gradient averaging within each subgroup intra-node default post-localSGD optimizer runs global model averaging every steps after applying local optimizer step range opt zero_grad loss = loss_fn output labels loss backward opt step __init__ optim torch optim Optimizer averager averagers ModelAverager optim = optim param_groups = optim param_groups averager = averager property state type ignore override optim state __repr__ optim __repr__ state_dict r This same ` torch optim Optimizer ` meth ` state_dict ` adds extra entry record model averager s step checkpoint ensure reload does cause unnecessary warm up again optim_state_dict = optim state_dict optim_state_dict step = averager step optim_state_dict load_state_dict state_dict r This same ` torch optim Optimizer ` meth ` load_state_dict ` also restores model averager s step value one saved provided ` ` state_dict ` ` If there no ` ` step ` ` entry ` ` state_dict ` ` will raise warning initialize model averager s step optim load_state_dict state_dict step state_dict averager step = state_dict step warnings warn Loaded state dict does contain step counter averager Setting step counter stacklevel= averager step = step type ignore override r Performs single optimization step parameter update optim step averager average_parameters params=self param_groups zero_grad set_to_none bool = True type ignore override optim zero_grad set_to_none=set_to_none add_param_group param_group optim add_param_group param_group