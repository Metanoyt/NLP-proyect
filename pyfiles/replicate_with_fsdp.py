mypy allow-untyped-defs __future__ annotations logging typing Optional TYPE_CHECKING Union torch torch distributed dist torch nn nn torch distributed _composable_state _get_module_state _insert_module_state torch distributed device_mesh _get_device_handle torch distributed fsdp _fully_shard _fsdp_api MixedPrecisionPolicy OffloadPolicy torch distributed fsdp _fully_shard _fsdp_common detect_compiled_autograd HSDPMeshInfo torch distributed fsdp _fully_shard _fsdp_init _get_device_from_mesh _get_managed_states _get_post_forward_mesh_info _init_default_fully_shard_mesh _move_states_to_device torch distributed fsdp _fully_shard _fsdp_param_group FSDPParamGroup torch distributed fsdp _fully_shard _fsdp_state _register_group_forward_hooks FSDPState torch distributed fsdp _fully_shard _fully_shard _unimplemented_deepcopy FSDPModule torch distributed tensor DeviceMesh init_device_mesh torch distributed utils _get_root_modules contract _get_registry contract TYPE_CHECKING collections abc Callable torch distributed tensor Shard cls_to_replicate_cls dict type type = _ROOT_MODULE_PREFIX = logger = logging getLogger torch distributed _composable replicate_with_fsdp _ReplicateStateContext This has state shared across Replicate states __init__ - None All Replicate states root state s module tree all_states list _ReplicateState = Iteration s forward root runs once-per-forward logic root may overall root set lazy initialization cases where only submodule runs forward e g encoder-only eval iter_forward_root Optional _ReplicateState = None Final callback should only queued once per backward post_backward_final_callback_queued bool = False Whether finalize backward backward s final callback is_last_backward bool = True Optional user-provided event recorded after optimizer all-gather streams wait root pre-forward post_optim_event Optional torch Event = None _get_module_replicate_state module nn Module - Optional _ReplicateState Checks module state ReplicateState state = _get_module_state module isinstance state _ReplicateState state None _ReplicateState FSDPState Replicate state functionality adapted FSDP state In future could experiment inheriting instead __init__ - None super __init__ _state_ctx = _ReplicateStateContext type ignore assignment Define separate init since ` __init__ ` called contract init modules tuple nn Module device torch device mp_policy MixedPrecisionPolicy auto_reshard_after_forward bool - None module modules _insert_module_state module _modules = modules pyrefly ignore read-only _device = device _device_handle = _get_device_handle device type _mp_policy = mp_policy _auto_reshard_after_forward = auto_reshard_after_forward len modules == _pre_forward_hook_handle = modules register_forward_pre_hook _pre_forward prepend=True with_kwargs=True _post_forward_hook_handle = modules register_forward_hook _post_forward prepend=False hook_handle = _register_group_forward_hooks modules _pre_forward _post_forward _modules_to_run_forward _pre_forward_hook_handle = hook_handle _post_forward_hook_handle = hook_handle _lazy_init - None Lazy initialization represents when all modules parallelisms have finalized e g Replicate has been applied all desired modules This means we can determine which state root we do so st state run forward _is_root None no-op already initialized _is_root = True len _modules raise RuntimeError f Replicate requires single root module got _modules detect_compiled_autograd root_module = _modules visited_states set _ReplicateState = set module_name module root_module named_modules state = _get_module_replicate_state module None continue module root_module state visited_states state _is_root None raise RuntimeError Replicate state has already been lazily initialized f module_name \nReplicate requires running forward through root module first state _is_root = False _state_ctx all_states append state pyrefly ignore bad-argument-type visited_states add state _fsdp_param_group _auto_reshard_after_forward For root do reshard after forward since training parameters would freed all-gathered immediately _fsdp_param_group post_forward_mesh_info = None _init_fqns _init_shared_state Run parameter group lazy inits after initializing FQNs improved error messages state _state_ctx all_states type ignore assignment state _fsdp_param_group type ignore union-attr state _fsdp_param_group lazy_init type ignore union-attr replicate_impl module mesh DeviceMesh device_id Optional Union int torch device = None reshard_after_forward Optional Union bool int = None shard_placement_fn Optional Callable nn Parameter Optional Shard = None mp_policy MixedPrecisionPolicy = MixedPrecisionPolicy offload_policy OffloadPolicy = OffloadPolicy ignored_params Optional set nn Parameter = None torch _C _log_api_usage_once torch distributed _composable replicate_with_fsdp isinstance module nn ModuleList nn ModuleDict raise ValueError f replicate does support containers do implement forward module mesh = mesh _init_default_fully_shard_mesh mesh ndim = raise ValueError f replicate expects D DeviceMesh got mesh mesh mesh_dim_names None raise AssertionError Please init D mesh HSDP mesh_dim_names specified mesh_info = HSDPMeshInfo mesh shard_mesh_dim= replicate_mesh_dim= device = _get_device_from_mesh mesh auto_reshard_after_forward = reshard_after_forward None If user does provide ` ` reshard_after_forward ` ` we set True During lazy_init we identify which module root override its value False post_forward_mesh_info = _get_post_forward_mesh_info reshard_after_forward auto_reshard_after_forward True type ignore arg-type mesh_info arg_module = module modules = module isinstance module nn Module tuple _get_root_modules module state = replicate state modules type ignore attr-defined see state init modules device mp_policy auto_reshard_after_forward managed_modules = _get_managed_modules modules ignored_params params buffers = _get_managed_states managed_modules ignored_params _move_states_to_device params buffers device params state _fsdp_param_group = FSDPParamGroup params modules mesh_info post_forward_mesh_info device shard_placement_fn mp_policy offload_policy Place Replicate leftmost highest priority method resolution order module modules cls = module __class__ new_cls = cls_to_replicate_cls get cls new_cls dct = __deepcopy__ _unimplemented_deepcopy new_cls = type f Replicate cls __name__ ReplicateModule cls dct cls_to_replicate_cls cls = new_cls module __class__ = new_cls arg_module contract state_cls=_ReplicateState replicate module nn Module kwargs - nn Module r Replicates module Args module torch nn Module module replicate Example xdoctest +REQUIRES module torch _C _distributed_c d module = nn Linear replicate module device_id kwargs isinstance kwargs device_id int torch device raise RuntimeError Expected device_id int torch device f got type kwargs device_id is_composable_with_replicate module raise RuntimeError Cannot apply ` replicate ` Module already managed ` fully_shard ` device_mesh = kwargs pop device_mesh None device_mesh None device_mesh = replicate_mesh module = replicate_impl module mesh=device_mesh kwargs module ReplicateModule FSDPModule __new__ cls args kwargs Override ` ` __new__ ` ` remove FSDP directly construct original cases like indexing into container module Use index since dynamically constructed ` FSDP ` index ` FSDPModule ` itself orig_cls = cls __mro__ = orig_cls __new__ orig_cls args kwargs __init__ args kwargs _get_managed_modules root_modules tuple nn Module ignored_params Optional set nn Parameter = None - list nn Module modules list nn Module = root_modules_set = set root_modules Track visisted modules avoid visiting shared modules multiple times visited_modules set nn Module = set dfs module nn Module - None Runs DFS collect managed modules recursing into modules non-composable API ` ` replicate ` ` already applied is_composable_with_replicate module module root_modules_set _get_module_replicate_state module None nested ` fully_shard ` module visited_modules add module submodule module children submodule visited_modules dfs submodule modules append module root_module root_modules dfs root_module ignored_params None modules adjusted_modules = _adjust_managed_modules modules ignored_params adjusted_modules is_composable_with_replicate module nn Module - bool Checks replicate can applied module registry = _get_registry module registry None True Registry keys function name fully_shard registry replicate_mesh Creates device mesh replicate user doesn t provide one dist distributed_c d is_initialized dist distributed_c d init_process_group default_pg = dist distributed_c d _get_default_group device = torch _C _get_accelerator mesh = init_device_mesh device type mesh_shape= default_pg size mesh_dim_names= replicate shard mesh _adjust_managed_modules modules list nn Module ignored_params set nn Parameter - list nn Module Adjust given list managed modules removing those all parameters ignored ignore_decision dict nn Module bool = new_modules = module modules ignored = _ignore_module module ignored_params ignore_decision ignored new_modules append module new_modules _ignore_module module nn Module ignored_params set nn Parameter ignore_decision dict nn Module bool - bool Decide safe ignore module applying replicate module ignore_decision ignore_decision module len list module buffers recurse=False Cannot ignore module any buffer ignore_decision module = False False _ param module named_parameters recurse=False param ignored_params least one param ignored So module shouldn t ignore_decision module = False False Need consider descendants module child list module children ignore_child = _ignore_module child ignored_params ignore_decision ignore_child Cannot ignore module one its children ignored ignore_decision module = False False Safe ignore module ignore_decision module = True True