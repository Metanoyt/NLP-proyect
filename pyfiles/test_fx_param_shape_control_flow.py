Owner s module fx torch torch fx torch testing _internal common_utils raise_on_run_directly TestCase MyModuleBase torch nn Module forward x matrx = get_mul_matrix no_relu torch mm x matrx torch relu torch mm x matrx get_mul_matrix param no_relu raise Exception implemented noqa TRY MyModuleParamShape MyModuleBase __init__ in_channels super __init__ param = torch nn Parameter torch randn in_channels no_relu param shape MyModuleParamSize MyModuleBase __init__ in_channels super __init__ param = torch nn Parameter torch randn in_channels no_relu param size MyModuleParamDim MyModuleBase __init__ param super __init__ param = param get_mul_matrix param param dim == param no_relu param dim == MyModuleParamNDim MyModuleBase __init__ param super __init__ param = param get_mul_matrix param param ndim == param no_relu param ndim == MyModuleParamNumEl MyModuleBase __init__ in_channels super __init__ param = torch nn Parameter torch randn in_channels no_relu param numel MyModuleParamNElement MyModuleBase __init__ in_channels super __init__ param = torch nn Parameter torch randn in_channels no_relu param nelement TestConstParamShapeInControlFlow TestCase verify_mm_relu_mods mm_only_mod relu_mod Verify one module only does mm op while other performs both mm relu ops cascade x = torch randn torch testing assert_close mm_only_mod x torch mm x mm_only_mod get_mul_matrix tracer = torch fx Tracer param_shapes_constant=True traced_graph = tracer trace mm_only_mod verify graph module calculates same result graph_mod_mm = torch fx GraphModule mm_only_mod traced_graph torch testing assert_close graph_mod_mm x torch mm x mm_only_mod get_mul_matrix Make new module different parameter shape go down different code path x = torch randn torch testing assert_close relu_mod x torch relu torch mm x relu_mod get_mul_matrix tracer = torch fx Tracer param_shapes_constant=True traced_graph = tracer trace relu_mod verify graph module calculates same result graph_mod_relu = torch fx GraphModule relu_mod traced_graph torch testing assert_close graph_mod_relu x torch relu torch mm x relu_mod get_mul_matrix graph _node_targets = n target n traced_graph nodes graph _node_targets = n target n traced_graph nodes second graph has extra relu function call node assert torch mm graph _node_targets torch mm graph _node_targets assert torch relu graph _node_targets torch relu graph _node_targets test_param_shape_const mymod = MyModuleParamShape in_channels= mymod = MyModuleParamShape in_channels= verify_mm_relu_mods mymod mymod test_param_size_const mymod = MyModuleParamSize in_channels= mymod = MyModuleParamSize in_channels= verify_mm_relu_mods mymod mymod test_param_dim_const mymod = MyModuleParamDim torch nn Parameter torch randn mymod = MyModuleParamDim torch nn Parameter torch randn verify_mm_relu_mods mymod mymod test_param_ndim_const mymod = MyModuleParamNDim torch nn Parameter torch randn mymod = MyModuleParamNDim torch nn Parameter torch randn verify_mm_relu_mods mymod mymod test_param_numel_const mymod = MyModuleParamNumEl in_channels= mymod = MyModuleParamNumEl in_channels= verify_mm_relu_mods mymod mymod test_param_nelement_const mymod = MyModuleParamNElement in_channels= mymod = MyModuleParamNElement in_channels= verify_mm_relu_mods mymod mymod __name__ == __main__ raise_on_run_directly test test_fx py