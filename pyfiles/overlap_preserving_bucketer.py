logging collections defaultdict dataclasses dataclass typing Any Literal Optional torch fx fx torch _dynamo utils counters torch _inductor augmented_graph_helper AugmentedGraphHelper torch _inductor fx_passes bucketing bucket_key BucketMode is_all_gather_into_tensor is_all_gather is_reduce_scatter_tensor is_reduce_scatter is_wait_tensor torch _inductor fx_passes overlap_scheduling CollBucket CollectiveInfo get_group_name is_compute_node torch utils _ordered_set OrderedSet bucket_log = logging getLogger __name__ dataclass WhyNoBucket name str name str reason str args tuple Any __init__ node fx Node node fx Node - None name = node name name = node name reason = args = __call__ reason str args Any - None bucket_log isEnabledFor logging DEBUG bucket_log debug cannot bucket s s + reason noqa G name name args is_collective_or_wait n fx Node - bool Check node collective start wait is_wait_tensor n True Collective starts have exactly one use wait_tensor len n users == user = next iter n users keys is_wait_tensor user True False dataclass PGEvent Represents important event process group timeline Either collective start wait hiding compute Each node linked its prev next these dependencies reflected augmented graph We want enforce sequential ordering collective starts waits because NCCL collectives same process group execute same CUDA stream creating implicit dependencies between all operations PG A wait particular collective will implicitly force realization all collectives enqueued prior collective node fx Node event_type Literal compute starts waits position int prev Optional PGEvent = None next Optional PGEvent = None property is_start - bool event_type == starts property is_wait - bool event_type == waits property is_compute - bool event_type == compute unlink - tuple Optional PGEvent Optional PGEvent Remove event linked list prev next prev_event next_event = prev next prev prev next = next next next prev = prev prev = None next = None prev_event next_event insert_between prev_event Optional PGEvent next_event Optional PGEvent - None Insert event between prev_event next_event linked list prev_event prev_event next = prev = prev_event next_event next_event prev = next = next_event OverlapPreservingBucketer Buckets collective operations while preserving compute-collective overlap relationships Uses augmented graph track dependencies between compute collective operations __init__ graph fx Graph collective_info dict fx Node CollectiveInfo node_ancestors dict fx Node OrderedSet fx Node scheduled OrderedSet fx Node max_bucket_memory_gb float = max_coll_distance int = insert_overlap_deps bool = False bucket_mode BucketMode = custom_ops_multidtype graph = graph collective_info = collective_info node_ancestors = node_ancestors scheduled = scheduled max_bucket_memory_gb = max_bucket_memory_gb node_idx = n i i n enumerate scheduled aug_graph = AugmentedGraphHelper graph node_ancestors max_coll_distance = max_coll_distance insert_overlap_deps = insert_overlap_deps bucket_mode = bucket_mode node_to_event dict fx Node PGEvent = pg_to_timeline_head dict str Optional PGEvent = build_timelines _add_hiding_interval_constraints build_timelines - dict str Optional PGEvent Construct each process groups ordered series event all_pgs OrderedSet str = OrderedSet start collective_info pg = get_group_name start all_pgs add pg pg_timeline dict str Optional PGEvent = pg all_pgs pg_timeline pg = build_timeline pg pg_timeline build_timeline pg str - Optional PGEvent Build timeline important events starts waits hiding compute process group constrain ordering augmented graph Sequential dependencies added between all events because NCCL collectives same process group execute same CUDA stream enforcing LIFO semantics where later-issued collectives must complete before earlier ones can finish head = None prev_event = None position = node scheduled node_type = None Determine node relevant PG node collective_info get_group_name node == pg node_type = starts is_wait_tensor node wait_input = node args isinstance wait_input fx Node get_group_name wait_input == pg node_type = waits is_compute_node node node_type = compute node_type None continue event = PGEvent node=node event_type=node_type position=position type ignore arg-type event insert_between prev_event None Add sequential dependency augmented graph prev_event aug_graph add_extra_dep n=event node dep=prev_event node head = event prev_event = event position += head _populate_node_to_event pg str - None Populate node_to_event mapping specific PG s timeline node_to_event clear head = pg_to_timeline_head pg curr = head while curr None node_to_event curr node = curr curr = curr next _add_hiding_interval_constraints - None Add hiding interval constraints start - compute - wait start info collective_info items info hiding_node info is_exposed Enforce start - compute - wait aug_graph add_extra_dep n=info hiding_node dep=start aug_graph add_extra_dep n=info wait_node dep=info hiding_node bucket_collectives - None Main entry point bucketing collectives Group collectives PG first pg_collectives dict str OrderedSet fx Node = defaultdict OrderedSet start collective_info pg = get_group_name start pg_collectives pg add start all_buckets list CollBucket = pg collectives pg_collectives items Populate node_to_event PG s timeline _populate_node_to_event pg Group bucket key within PG grouped_collectives dict object OrderedSet fx Node = defaultdict OrderedSet start collectives key = bucket_key start bucket_mode key None grouped_collectives key add start Find buckets PG key collective_group grouped_collectives items bucket_log debug bucketing collective group key s s key n name n collective_group buckets = _find_buckets collective_group all_buckets extend buckets Apply bucketing transformations Dependencies tracked aug_graph extra_deps during bucketing coll_bucket all_buckets len coll_bucket collectives = continue counters inductor collective_buckets += _apply_bucket coll_bucket Extract all dependencies augmented graph This includes - Sequential timeline deps added during build_timeline - Hiding interval deps added during _add_hiding_interval_constraints - All transferred deps bucketing transferred during _apply_bucket additional_deps = aug_graph get_all_extra_deps Apply topological sort all dependencies torch _dynamo graph_deduplication _stable_topological_sort _stable_topological_sort graph additional_deps After topological sort preserve dependencies using effect tokens Only preserve edges where NOT both nodes collective starts waits insert_overlap_deps filtered_deps dict fx Node OrderedSet fx Node = node deps additional_deps items filtered_node_deps OrderedSet fx Node = OrderedSet only preserve comm-comptue overlap now although we could more generally constrain dep deps is_collective_or_wait node is_collective_or_wait dep filtered_node_deps add dep filtered_node_deps filtered_deps node = filtered_node_deps _preserve_dependencies_with_tokens filtered_deps graph lint _find_buckets collective_group OrderedSet fx Node - list CollBucket Find valid buckets within group similar collectives max_bucket_bytes = int max_bucket_memory_gb buckets = processed OrderedSet fx Node = OrderedSet Sort collectives node index efficient distance checking sorted_collectives = sorted collective_group key=lambda n node_idx n start_node sorted_collectives start_node processed continue Initialize bucket first collective bucket_info = CollBucket collectives= start_node total_bytes=self collective_info start_node size_bytes processed add start_node start_node_idx = node_idx start_node Check candidates sorted order break when beyond max distance candidate sorted_collectives candidate processed continue candidate_idx = node_idx candidate Check candidate within max distance bucket start distance = abs candidate_idx - start_node_idx distance max_coll_distance Since sorted all remaining candidates will too far candidate_idx start_node_idx break continue candidate_bytes = collective_info candidate size_bytes bucket_info total_bytes + candidate_bytes max_bucket_bytes continue _can_add_to_bucket bucket_info candidate bucket_info collectives append candidate bucket_info total_bytes += candidate_bytes processed add candidate len bucket_info collectives buckets append bucket_info buckets _ancestor_dep n fx Node n fx Node - bool Check there s ancestor relationship between two nodes n node_ancestors n n node_ancestors n _get_intervals event PGEvent - tuple Optional tuple int int Optional tuple int int Get execution_interval hiding_interval collective event Returns execution_interval hiding_interval where - execution_interval start_pos wait_pos None - hiding_interval start_pos compute_pos None no hiding node Works both start wait events looking up collective info For start events directly use node event is_start coll = event node For wait events look up start node event s args event is_wait wait_input = event node args isinstance wait_input fx Node None None coll = wait_input None None coll collective_info None None info = collective_info coll start_event = node_to_event coll wait_event = node_to_event info wait_node execution_interval = start_event position wait_event position hiding_interval = None info hiding_node hiding_interval = start_event position node_to_event info hiding_node position execution_interval hiding_interval _preserves_hiding_intervals bucket_info CollBucket candidate fx Node start_pos fx Node wait_pos fx Node why WhyNoBucket - bool Check start_pos wait_pos doesn t violate any hiding intervals collectives Collects all execution hiding intervals affected timeline regions then checks All bucket hiding compute stays between new start wait No other collective s compute interval enclosed bucket execution interval No other collective s execution interval encloses bucket compute intervals Collect all collectives being bucketed all_bucketed_colls = candidate + list bucket_info collectives all_bucketed_waits = collective_info coll wait_node coll all_bucketed_colls Collect hiding compute positions bucket bucket_hiding_compute_positions = coll all_bucketed_colls hiding_node = collective_info coll hiding_node bucket_hiding_compute_positions append node_to_event hiding_node position Get new positions new_start_event = node_to_event start_pos new_wait_event = node_to_event wait_pos Check All bucket hiding compute must between new start wait compute_pos bucket_hiding_compute_positions new_start_event position compute_pos new_wait_event position why hiding compute pos d between start d wait d compute_pos new_start_event position new_wait_event position False get_wait n fx Node - fx Node collective_info n wait_node get_pos n fx Node - int node_to_event n position latest_start_pos = max get_pos candidate get_pos bucket_info collectives earliest_wait_pos = min get_pos get_wait candidate get_pos get_wait bucket_info collectives Bucket execution interval bucket_execution_interval = new_start_event position new_wait_event position Because collectives same PG operate under LIFO semantics s only possible us force early realization unrelated collective delaying start raising wait We search interval old_start - new_start see would forcing another collective realized prior its hiding nodes Similarly we search old_wait - new_wait reverse direction check same thing execution_intervals = bucket_execution_interval hiding_intervals = bucket_execution_interval pos pos bucket_hiding_compute_positions curr_event = new_start_event next while curr_event None curr_event position latest_start_pos curr_event node all_bucketed_colls curr_event node all_bucketed_waits exec_interval hiding_interval = _get_intervals curr_event exec_interval execution_intervals append exec_interval hiding_interval hiding_intervals append hiding_interval curr_event = curr_event next curr_event = new_wait_event prev while curr_event None curr_event position earliest_wait_pos curr_event node all_bucketed_colls curr_event node all_bucketed_waits exec_interval hiding_interval = _get_intervals curr_event exec_interval execution_intervals append exec_interval hiding_interval hiding_intervals append hiding_interval curr_event = curr_event prev Check no hiding interval should enclosed any execution interval enclosed_interval inner tuple int int outer tuple int int - bool outer inner inner outer hiding_interval hiding_intervals execution_interval execution_intervals enclosed_interval hiding_interval execution_interval why hiding interval s enclosed execution interval s hiding_interval execution_interval False True remove_from_event node fx Node - tuple Optional PGEvent Optional PGEvent Remove node timeline prev_event next_event event = node_to_event node assert event is_compute Cannot remove compute events timeline prev_event next_event = event unlink Remove augmented graph dependency prev_event aug_graph remove_extra_dep n=node dep=prev_event node next_event aug_graph remove_extra_dep n=next_event node dep=node Add bypass dependency prev_event next_event aug_graph add_extra_dep n=next_event node dep=prev_event node prev_event next_event restore_to_event node fx Node prev_event Optional PGEvent next_event Optional PGEvent - None Restore node timeline after failed merge attempt event = node_to_event node Reinsert into linked list event insert_between prev_event next_event prev_event aug_graph add_extra_dep n=node dep=prev_event node next_event prev_event aug_graph add_extra_dep n=next_event node dep=node Remove bypass dependency prev_event next_event aug_graph remove_extra_dep n=next_event node dep=prev_event node _try_timeline_position bucket_info CollBucket candidate fx Node start_pos fx Node wait_pos fx Node why WhyNoBucket - bool Try specific timeline position candidate Returns True valid merges successful candidate_info = collective_info candidate candidate_wait = candidate_info wait_node Quick check does violate hiding intervals _preserves_hiding_intervals bucket_info candidate start_pos wait_pos why False Determine which start needs move existing_coll = bucket_info collectives start_pos == existing_coll start_to_move = candidate assert start_pos == candidate start_to_move = existing_coll Remove start timeline start_prev start_next = remove_from_event start_to_move Check starts can merged aug_graph has_path existing_coll candidate aug_graph has_path candidate existing_coll Restore start constraints restore_to_event start_to_move start_prev start_next why path exists between starts False Merge starts aug_graph merge_to_set existing_coll candidate Determine which wait needs move existing_wait = collective_info existing_coll wait_node candidate_wait = collective_info candidate wait_node wait_pos == existing_wait wait_to_move = candidate_wait wait_to_move = existing_wait Remove wait timeline wait_prev wait_next = remove_from_event wait_to_move Check waits can merged aug_graph has_path existing_wait candidate_wait aug_graph has_path candidate_wait existing_wait Restore wait constraints restore_to_event wait_to_move wait_prev wait_next Unmerge start we just merged aug_graph unmerge_node candidate Restore start constraints restore_to_event start_to_move start_prev start_next why path exists between waits False Merge waits - success aug_graph merge_to_set existing_wait candidate_wait Update node_to_event moved nodes target_start_event = node_to_event start_pos target_wait_event = node_to_event wait_pos node_to_event candidate = target_start_event node_to_event candidate_wait = target_wait_event node_to_event existing_coll = target_start_event node_to_event existing_wait = target_wait_event True _has_ancestor_conflicts bucket_info CollBucket candidate fx Node - bool Check candidate has ancestor conflicts bucket collectives Returns True there conflicts candidate_info = collective_info candidate candidate_wait = candidate_info wait_node coll bucket_info collectives Check collectives ancestors each other _ancestor_dep coll candidate True Check waits ancestors each other coll_wait = collective_info coll wait_node _ancestor_dep candidate_wait coll_wait True Check existing hiding node conflicts candidate wait hiding_node = collective_info coll hiding_node _ancestor_dep hiding_node candidate_wait True Check candidate hiding node conflicts existing wait new_hiding_node = candidate_info hiding_node _ancestor_dep new_hiding_node coll_wait True False _can_add_to_bucket bucket_info CollBucket candidate fx Node - bool Check candidate can added bucket without breaking comm compute overlap Strategy Try all timeline positions - combinations existing_start candidate_start x existing_wait candidate_wait For each position verify Hiding intervals preserved - any start hiding_compute wait interval no other collective s start wait pair falls between start hiding_compute which would force realization break overlap due LIFO semantics Topologically valid no dependency cycles Return True any timeline position satisfies both constraints existing_coll = bucket_info collectives why = WhyNoBucket existing_coll candidate candidate_info = collective_info candidate Step Quick check using precomputed ancestors These ancestors computed prior adding augmented dependencies updated so any these checks fail then merge will topologically valid even ignoring comm compute overlap _has_ancestor_conflicts bucket_info candidate why has ancestor conflicts False Step Try different rail positions existing_wait = collective_info existing_coll wait_node candidate_start = candidate candidate_wait = candidate_info wait_node Try combinations order likelihood succeed early start later wait most likely work combinations = existing_coll candidate_wait Move candidate start early keep wait late existing_coll existing_wait Move candidate start early move wait early candidate_start candidate_wait Keep both place candidate_start existing_wait Keep start place move wait early i start_pos wait_pos enumerate combinations _try_timeline_position bucket_info candidate start_pos wait_pos why bucket_log debug bucketed s s using timeline position d start= s wait= s candidate name existing_coll name i + start_pos name wait_pos name True why all timeline positions failed False _apply_bucket bucket_info CollBucket - None Apply bucketing transformation Dependencies added aug_graph extra_deps transferred old nodes torch _inductor fx_passes bucketing is_all_reduce_tensor merge_all_gather_bucket merge_all_reduce_bucket merge_reduce_scatter_bucket bucket = bucket_info collectives Collect old nodes BEFORE they re erased old_starts = list bucket old_waits = collective_info n wait_node n bucket Find where place bucketed operations next_node = bucket while next_node bucket next_node = next_node next Don t use wait_insertion_point - let merge functions place waits naturally The wait_insertion_point feature tries move waits specific location can cause issues when location one nodes being erased Create bucketed collective will erase old nodes is_all_gather bucket new_nodes replacements = merge_all_gather_bucket graph bucket insert_before=next_node mode= custom_ops is_all_reduce_tensor bucket new_nodes replacements = merge_all_reduce_bucket graph bucket mode= custom_ops insert_before=next_node assert is_reduce_scatter bucket new_nodes replacements = merge_reduce_scatter_bucket graph bucket insert_before=next_node mode= custom_ops Get new nodes new_waits = n n new_nodes is_wait_tensor n assert len new_waits == new_wait = new_waits new_start = new_wait args assert isinstance new_start fx Node Create mapping all erased nodes their replacements erased_to_new = old_start old_starts erased_to_new old_start = new_start old_wait old_waits erased_to_new old_wait = new_wait Transfer all dependencies old nodes new nodes aug_graph transfer_erased_node_deps erased_to_new _preserve_dependencies_with_tokens additional_deps dict fx Node OrderedSet fx Node - None Preserve dependencies using effect tokens with_effects higher-order op Uses standalone token_dependencies utility consistent behavior across different overlap scheduling approaches torch _inductor fx_passes control_dependencies preserve_node_ordering preserve_node_ordering graph additional_deps