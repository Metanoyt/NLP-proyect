mypy allow-untyped-defs logging typing TYPE_CHECKING Union torch config inductor_config kernel_inputs MMKernelInputs lowering lowerings select_algorithm autotune_select_algorithm ExternKernelChoice TritonTemplate utils use_aten_gemm_kernels use_triton_template virtualized V mm_common mm_args mm_grid TYPE_CHECKING torch _inductor ir ChoiceCaller torch _inductor select_algorithm KernelTemplate log = logging getLogger __name__ aten = torch ops aten aten_mm_plus_mm = ExternKernelChoice torch ops inductor _mm_plus_mm torch inductor _mm_plus_mm mm_plus_mm_template = TritonTemplate name= mm_plus_mm grid=mm_grid debug=False source=r def_kernel A B C D M = size A N = size B K = size A M N == early exit due zero-size input s K = size C stride_am = stride A stride_ak = stride A stride_bk = stride B stride_bn = stride B stride_cm = stride C stride_ck = stride C stride_dk = stride D stride_dn = stride D based triton ops matmul pid = tl program_id INDEX_DTYPE grid_m = M + BLOCK_M - BLOCK_M grid_n = N + BLOCK_N - BLOCK_N re-order program ID better L performance width = GROUP_M grid_n group_id = pid width group_size = min grid_m - group_id GROUP_M GROUP_M pid_m = group_id GROUP_M + pid group_size pid_n = pid width group_size tl assume pid_m = tl assume pid_n = rm = pid_m BLOCK_M + tl arange BLOCK_M rn = pid_n BLOCK_N + tl arange BLOCK_N stride_am == stride_ak == M stride_am == K stride_ak == stride_cm == stride_ck == M stride_cm == K stride_ck == ram = tl max_contiguous tl multiple_of rm M BLOCK_M BLOCK_M ram = rm M stride_bk == stride_bn == K stride_bk == N stride_bn == stride_dk == stride_dn == K stride_dk == N stride_dn == rbn = tl max_contiguous tl multiple_of rn N BLOCK_N BLOCK_N rbn = rn N rk = tl arange BLOCK_K A = A + ram None stride_am + rk None stride_ak B = B + rk None stride_bk + rbn None stride_bn C = C + ram None stride_cm + rk None stride_ck D = D + rk None stride_dk + rbn None stride_dn acc = tl zeros BLOCK_M BLOCK_N dtype=ACC_TYPE k range K -BLOCK_K First matmul A B EVEN_K = tl load A b = tl load B = tl load A mask=rk None k other= b = tl load B mask=rk None k other= acc += tl dot b allow_tf =ALLOW_TF A += BLOCK_K stride_ak B += BLOCK_K stride_bk k range K -BLOCK_K Second matmul C D EVEN_K c = tl load C d = tl load D c = tl load C mask=rk None k other= d = tl load D mask=rk None k other= acc += tl dot c d allow_tf =ALLOW_TF C += BLOCK_K stride_ck D += BLOCK_K stride_dk idx_m = rm None idx_n = rn None mask = idx_m M idx_n N inductor generates suffix store_output idx_m idx_n acc mask val_shape= BLOCK_M BLOCK_N cache_codegen_enabled_for_template=True tuned_mm_plus_mm mat mat mat mat layout=None Computes mm mat mat + mm mat mat TODO coconutruben integrate into MMKernelInputs when all callsites use m n k layout mat mat = mm_args mat mat layout=layout m n _ layout mat mat = mm_args mat mat layout=layout Optimization optional because we can always just do fusion m n == m n == V graph sizevars statically_known_list_equals mat get_size mat get_size V graph sizevars statically_known_list_equals mat get_size mat get_size inductor_config triton native_matmul TODO jansel support different K values when fixed https github com triton-lang triton issues lowerings aten add lowerings aten mm mat mat lowerings aten mm mat mat Create MMKernelInputs MM Plus MM matrices indices first pair Note This special case matrices we use first pair M N K extraction kernel_inputs = MMKernelInputs mat mat mat mat mat _idx= mat _idx= assert layout == layout options tune choices list ChoiceCaller = Collect all templates unified call templates_to_use list Union ExternKernelChoice KernelTemplate = use_aten_gemm_kernels templates_to_use append aten_mm_plus_mm use_triton_template layout check_max_autotune=False templates_to_use append mm_plus_mm_template Single unified call all templates choices extend V choices get_template_configs kernel_inputs templates_to_use mm_plus_mm autotune_select_algorithm mm_plus_mm choices kernel_inputs nodes layout