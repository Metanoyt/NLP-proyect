Generates VariableType h cpp If any changes being made VariableType codegen please also check updates needed torch csrc autograd autograd_not_implemented_fallback cpp VariableType subclass Type provides binding code necessary provide differentiable version ATen operators There number different things we could mean - Given non-differentiable forward implementation we might directly associate backward implementation make differentiable This common case - Some functions don t need backwards implementation because backpropagation will never propagate beyond them There number different reasons why may case - The function has no differentiable inputs - The function s output differentiable - The function has no data dependency its input - Some function don t need backwards implementation because they implemented composition other differentiable ATen functions These dispatched directly Type superclass which will turn dispatch back VariableType its differentiable subcomponents __future__ annotations re typing TYPE_CHECKING torchgen api cpp torchgen api autograd DifferentiableInput dispatch_strategy ForwardDerivative gen_differentiable_outputs is_differentiable NativeFunctionWithDifferentiabilityInfo SavedAttribute torchgen api types ArrayRefCType BaseCppType BaseCType Binding intArrayRefT iTensorListRefT ListCType MutRefCType OptionalCType scalarT SpecialArgName stringT symIntArrayRefT TENSOR_LIST_LIKE_CTYPES tensorListT tensorT TupleCType VectorCType torchgen code_template CodeTemplate torchgen context native_function_manager with_native_function with_native_function_and torchgen model Argument BaseType ListType NativeFunction SchemaKind SelfArgument TensorOptionsArguments torchgen utils FileManager mapMaybe context with_native_function_with_differentiability_info_and_key gen_inplace_or_view_type ALL_VIEW_FUNCTIONS ASSIGN_RETURN_VALUE AUTOGRAD_NOT_IMPLEMENTED_REGISTRATION gen_formals get_base_name get_view_info is_tensor_list_type is_tensor_type METHOD_DEFINITION modifies_arguments TMP_VAR unpack_args unpacked_name use_derived WRAPPER_REGISTRATION gen_trace_type get_return_value MANUAL_AUTOGRAD_AND_TRACER MANUAL_BACKEND tie_return_values type_wrapper_name TYPE_CHECKING collections abc Callable Sequence We don t set modify grad_fn these methods Generally they tensors have requires_grad=False In-place functions listed here will examine modify requires_grad grad_fn NB does NOT include overload name DONT_REQUIRE_DERIVATIVE = These only depend input Tensor s shape device data empty_like ones_like full_like zeros_like rand_like randn_like new_empty new_empty_strided new_full new_zeros new_ones These only implemented integral types __and__ __iand__ __ilshift__ __ior__ __irshift__ __ixor__ __lshift__ __or__ __rshift__ __xor__ These work integral data types hence don t require derivative _sobol_engine_draw _sobol_engine_ff _sobol_engine_scramble_ _sobol_engine_initialize_state_ This unsafe method meant out reach autograd _coalesced_ Quantize functions should record gradients quantize_per_tensor quantize_per_channel Functions integers should have output require gradients argmax argmin argsort searchsorted bucketize Functions booleans differentiable isnan isposinf isneginf isinf signbit isin allclose Functions none differentiable record_stream These functions differentiable logical_and logical_xor logical_not logical_or This function returns nested_tensor shape tensor non-differentiable _nested_tensor_size _nested_tensor_strides _nested_tensor_storage_offsets The C - R functions time adding still being audited tested will error out C - C R - C functions which backward correctly implemented tested GRADIENT_IMPLEMENTED_FOR_COMPLEX = fill t t_copy view reshape reshape_as view_as view_copy roll clone block_diag diag_embed repeat expand expand_copy flip fliplr flipud rot nanmean nansum transpose transpose_copy permute permute_copy squeeze squeeze_copy unsqueeze unsqueeze_copy resize resize_as tril triu chunk zero_ eq_ ne_ add __radd__ sum _conj sin cos mul sinc sinh cosh __rmul__ sgn asin acos sub div cat view_as_complex index_put neg complex select where as_strided as_strided_copy as_strided_scatter slice constant_pad_nd unbind unbind_copy split split_with_sizes unsafe_split split_with_sizes_backward dot vdot cholesky triangular_solve mm _unsafe_view mv outer bmm diagonal alias atan log log log p log logaddexp logsumexp logcumsumexp reciprocal tan pow rsqrt tanh tanh_backward asinh acosh atanh take fill_ exp exp expm nonzero mean std_mean var_mean inverse solve linalg_cholesky addcmul addcdiv matrix_exp linalg_matrix_exp _linalg_eigh cholesky_solve linalg_qr _linalg_svd _fft_c c _fft_r c linalg_solve sqrt stack gather index_select index_add_ linalg_inv linalg_inv_ex baddbmm addbmm addmm addmv addr linalg_householder_product ormqr reflection_pad d reflection_pad d reflection_pad d linalg_cholesky_ex linalg_eig diagonal_copy diagonal_scatter alias_copy select_backward diagonal_backward slice_backward reflection_pad d_backward reflection_pad d_backward reflection_pad d_backward _sparse_sparse_matmul replication_pad d replication_pad d replication_pad d put put_ _to_copy replication_pad d_backward replication_pad d_backward replication_pad d_backward diag masked_scatter masked_select index_add index_fill trace polar cumsum rsub eig lerp linalg_vector_norm cumprod prod index_copy lu unfold unfold_backward index masked_fill masked_scatter_backward linalg_cross lu_unpack renorm _conj_physical linalg_lu_factor_ex scatter scatter_add sigmoid sigmoid_backward sparse_mask trapezoid cumulative_trapezoid conj_physical_ _neg_view _reshape_alias _reshape_copy _linalg_det lu_solve linalg_solve_triangular linalg_pinv linalg_lstsq unfold_copy col im im col cholesky_inverse to_sparse sparse_sampled_addmm linalg_lu pixel_shuffle pixel_unshuffle channel_shuffle linalg_lu_solve _linalg_slogdet _linalg_solve_ex _unsafe_index _unsafe_index_put _unsafe_masked_index _unsafe_masked_index_put_accumulate GRADIENT_IMPLEMENTED_FOR_SPARSE_COMPLEX = _to_dense _coalesce coalesce values _sparse_coo_tensor_with_dims_and_tensors _sparse_addmm GRADIENT_IMPLEMENTED_FOR_COMPLEX update GRADIENT_IMPLEMENTED_FOR_SPARSE_COMPLEX Some operators invalidate grad_accumulator Let s reset RESET_GRAD_ACCUMULATOR = set_ resize_ NOTE TensorImpl Storage Pointer Sanity Checks We check following properties A function should never change input tensors underlying c TensorImpl pointers c Storage pointers even modifies its input tensors via inplace out-variants If function does modify its arguments we also check following properties pertaining its output Its TensorImpl has use_count If function view function has same StorageImpl input aliased Otherwise its StorageImpl has use_count The following code templates implement checks invariant SAVE_TENSOR_STORAGE = CodeTemplate \ auto $ tensor_name _storage_saved = $ tensor_name has_storage std optional Storage $ tensor_name storage std nullopt If tensor_name == out_tensor_name used enforce otherwise used ENFORCE_SAME_TENSOR_STORAGE = CodeTemplate \ $ tensor_name _storage_saved has_value impl dispatch_mode_enabled impl tensor_has_dispatch $ tensor_name impl tensor_has_dispatch $ out_tensor_name TORCH_INTERNAL_ASSERT $ tensor_name _storage_saved value is_alias_of $ out_tensor_name storage SAVE_TENSORLIST_STORAGE = CodeTemplate \ std vector std optional Storage $ tensorlist_name _storage_saved $ tensorlist_name size const Tensor tensor $ tensorlist_name $ tensorlist_name _storage_saved push_back tensor has_storage std optional Storage tensor storage std nullopt ENFORCE_SAME_TENSORLIST_STORAGE = CodeTemplate \ size_t i= i $ tensorlist_name size impl dispatch_mode_enabled i++ $ tensorlist_name _storage_saved i has_value impl tensorlist_has_dispatch $ tensorlist_name TORCH_INTERNAL_ASSERT $ tensorlist_name _storage_saved i value is_alias_of $ tensorlist_name i storage SAVE_OPTIONALTENSORLIST_STORAGE = CodeTemplate \ std vector std optional Storage $ tensorlist_name _storage_saved $ tensorlist_name size const std optional Tensor tensor $ tensorlist_name $ tensorlist_name _storage_saved push_back tensor has_value tensor- has_storage std optional Storage tensor- storage std nullopt ENFORCE_SAME_OPTIONALTENSORLIST_STORAGE = CodeTemplate \ size_t i= i $ tensorlist_name size impl dispatch_mode_enabled i++ $ tensorlist_name _storage_saved i has_value impl tensorlist_has_dispatch $ tensorlist_name TORCH_INTERNAL_ASSERT $ tensorlist_name _storage_saved i value is_alias_of static_cast std optional Tensor $ tensorlist_name i - storage SAVE_TENSOR_IMPL = CodeTemplate \ c intrusive_ptr TensorImpl $ tensor_name _impl_saved $ tensor_name defined $ tensor_name _impl_saved = $ tensor_name getIntrusivePtr ENFORCE_SAME_TENSOR_IMPL = CodeTemplate \ $ tensor_name _impl_saved impl dispatch_mode_enabled impl tensor_has_dispatch $ tensor_name TORCH_INTERNAL_ASSERT $ tensor_name _impl_saved == $ tensor_name getIntrusivePtr ENFORCE_TENSOR_IMPL_USE_COUNT_LT_OR_EQ_ONE = CodeTemplate \ impl dispatch_mode_enabled impl tensor_has_dispatch $ tensor_name TORCH_INTERNAL_ASSERT $ tensor_name use_count = function $ fn_name ENFORCE_TENSOR_STORAGE_USE_COUNT_EQUALS_ONE = CodeTemplate \ $ tensor_name has_storage impl dispatch_mode_enabled impl tensor_has_dispatch $ tensor_name TORCH_INTERNAL_ASSERT $ tensor_name storage use_count == function $ fn_name SAVE_TENSORLIST_IMPL = CodeTemplate \ std vector c intrusive_ptr TensorImpl $ tensorlist_name _impl_saved $ tensorlist_name size size_t i= i $ tensorlist_name size i++ $ tensorlist_name i defined $ tensorlist_name _impl_saved i = $ tensorlist_name i getIntrusivePtr ENFORCE_SAME_TENSORLIST_IMPL = CodeTemplate \ size_t i= i $ tensorlist_name size impl dispatch_mode_enabled i++ $ tensorlist_name _impl_saved i impl tensorlist_has_dispatch $ tensorlist_name TORCH_INTERNAL_ASSERT $ tensorlist_name _impl_saved i == $ tensorlist_name i getIntrusivePtr SAVE_OPTIONALTENSORLIST_IMPL = CodeTemplate \ std vector c intrusive_ptr TensorImpl $ tensorlist_name _impl_saved $ tensorlist_name size size_t i= i $ tensorlist_name size i++ std optional Tensor t = $ tensorlist_name i t has_value t- defined $ tensorlist_name _impl_saved i = t- getIntrusivePtr ENFORCE_SAME_OPTIONALTENSORLIST_IMPL = CodeTemplate \ size_t i= i $ tensorlist_name size impl dispatch_mode_enabled i++ $ tensorlist_name _impl_saved i TORCH_INTERNAL_ASSERT $ tensorlist_name _impl_saved i == static_cast std optional Tensor $ tensorlist_name i - getIntrusivePtr The following list contains functions we don t enforce invariant DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE = These functions expected change impl storage input tensors set_ _cudnn_rnn_flatten_weight _unsafe_masked_index _unsafe_masked_index_put_accumulate DONT_ENFORCE_TENSOR_IMPL_USE_COUNT = These non-inplace non-out functions tensors use_count Therefore they MAY necessarily one its inputs as-is See https github com pytorch pytorch issues more information _embedding_bag _embedding_bag_forward_only q_per_channel_scales q_per_channel_zero_points lu_unpack _cudnn_rnn_backward The below failed StorageImpl use_count check we skip tensor_impl check just case _cudnn_rnn dequantize_self lift should never actually called requires_grad=True tensor lift lift_fresh lift_fresh_copy Nested Tensors related functions _nested_tensor_size should never actually called requires_grad=True tensor _nested_tensor_size _nested_tensor_strides _nested_tensor_storage_offsets DONT_ENFORCE_STORAGE_IMPL_USE_COUNT = These non-view functions tensors storage use_count = _slow_conv d_forward slow_conv d_forward channel_shuffle If input returned as-is output we cannot guarantee its storage_impl use count either DONT_ENFORCE_TENSOR_IMPL_USE_COUNT END CHECKS FOR TensorImpl Storage Pointer Sanity Checks DECLARE_GRAD_FN = CodeTemplate \ std shared_ptr $ op grad_fn DECLARE_VECTOR_OF_GRAD_FN = CodeTemplate \ std vector std shared_ptr $ op grad_fns SETUP_ANY_REQUIRES_GRAD = CodeTemplate \ maybe_unused auto _any_requires_grad = compute_requires_grad $ args_with_derivatives $ extra_differentiability_conditions SETUP_DERIVATIVE = CodeTemplate \ _any_requires_grad $ setup SETUP_NONE_REQUIRES_GRAD = CodeTemplate \ compute_requires_grad $ args_to_check throw_error_out_requires_grad $ base_name ASSIGN_GRAD_FN = CodeTemplate \ grad_fn = std shared_ptr $ op new $ op $ op_ctor deleteNode grad_fn- set_next_edges collect_next_edges $ args_with_derivatives note crcrpar ` compute_requires_grad ` template below supplied arguments indexed ` i ` while ` SETUP_ANY_REQUIRES_GRAD ` above takes whole tensors scalars ASSIGN_VECTOR_OF_GRAD_FN = CodeTemplate \ const auto i c irange $ irange const auto ith_requires_grad = compute_requires_grad $ args_with_derivatives check_inplace i ith_requires_grad grad_fns push_back - std shared_ptr $ op ith_requires_grad nullptr auto grad_fn = std shared_ptr $ op new $ op $ op_ctor deleteNode grad_fn- set_next_edges collect_next_edges $ args_with_derivatives grad_fn CALL_REDISPATCH = CodeTemplate \ redispatch $ api_name $ unpacked_args If non-variable operation has values we use ` tmp ` variable hold values temporarily pass values variables outside ` AutoDispatchBelowAutograd ` guard block DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES_JVP_DECOMP = CodeTemplate \ auto $ tmp_var = $ any_has_forward_grad static c OperatorName full_name aten $ op_name $ op_overload static std optional c OperatorHandle opt_op = c Dispatcher singleton findSchema full_name impl run_jit_decomposition_with_args_for_jvp $ return_types $ op_name opt_op ks $ arg_names $ guard $ base_type_call DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES = CodeTemplate \ auto $ tmp_var = $ guard $ base_type_call DISPATCH_TO_NON_VAR_TYPE_WITHOUT_RETURN_VALUES = CodeTemplate \ $ guard $ base_type_call SET_HISTORY = CodeTemplate \ grad_fn $ fn _history $ differentiable_outputs grad_fn LOOP_OVER_VECTOR_OF_GRAD_FNS = CodeTemplate \ grad_fns empty $ preamble const auto i c irange grad_fns size auto grad_fn = grad_fns i grad_fn = nullptr $ statements CONDITIONAL = CodeTemplate \ $ cond $ statements RUN_ONLY_IN_DEBUG_MODE = CodeTemplate \ #ifndef NDEBUG $ statements #endif FW_DERIVATIVE_CHECK_TEMPLATE = CodeTemplate \ isFwGradDefined $ req_inp \ FW_DERIVATIVE_SIZE_CHECK_TEMPLATE = CodeTemplate \ TORCH_CHECK size == $ inp_name size Tensor lists must have same number tensors got size $ inp_name size FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE = CodeTemplate \ isFwGradDefinedTensorList $ req_inp \ FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE = CodeTemplate \ auto $ inp_name _t_raw = toNonOptFwGrad $ inp auto $ inp_name _tensor = toNonOptTensor $ inp auto $ inp_name _t = $ inp_name _t_raw defined &#124; &#124; $ inp_name _tensor defined $ inp_name _t_raw $ zeros_fn $ inp_name _tensor sym_sizes $ inp_name _tensor options FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE = CodeTemplate \ auto $ inp_name _p = toNonOptPrimal $ inp FW_DERIVATIVE_SETTER_TENSOR = CodeTemplate \ $ out_arg _new_fw_grad_opt has_value $ out_arg _new_fw_grad_opt value defined $ out_arg defined The hardcoded here will need updated once we support multiple levels $ out_arg _set_fw_grad $ out_arg _new_fw_grad_opt value level is_inplace_op $ is_inplace FW_DERIVATIVE_SETTER_TENSOR_FOREACH = CodeTemplate \ const auto i c irange $ out_arg _new_fw_grad_opts size auto $ out_arg _new_fw_grad_opt = $ out_arg _new_fw_grad_opts i $ out_arg _new_fw_grad_opt has_value $ out_arg _new_fw_grad_opt value defined $ out_arg i defined The hardcoded here will need updated once we support multiple levels $ out_arg i _set_fw_grad $ out_arg _new_fw_grad_opt value level is_inplace_op $ is_inplace FW_DERIVATIVE_SETTER_MULTI_OUTPUT = CodeTemplate \ $ all_res _new_fw_grad_opt has_value std get $ idx $ all_res _new_fw_grad_opt value defined $ out_arg defined $ out_arg _set_fw_grad std get $ idx $ all_res _new_fw_grad_opt value level is_inplace_op false FW_DERIVATIVE_SETTER_TENSOR_LIST = CodeTemplate \ $ out_arg _new_fw_grad_opt has_value auto $ out_arg _new_fw_grad = $ out_arg _new_fw_grad_opt value TORCH_INTERNAL_ASSERT $ out_arg size == $ out_arg _new_fw_grad size const auto i c irange $ out_arg size $ out_arg _new_fw_grad i defined $ out_arg i defined The hardcoded here will need updated once we support multiple levels $ out_arg i _set_fw_grad $ out_arg _new_fw_grad i level is_inplace_op $ is_inplace FW_DERIVATIVE_TEMPLATE = CodeTemplate \ $ fw_grad_opt_definition $ requires_fw_grad $ unpacked_arguments $ out_arg _new_fw_grad_opt = $ formula FW_DERIVATIVE_FOREACH_TEMPLATE = CodeTemplate \ $ fw_grad_opt_definition const auto i c irange $ vector_of_optional_tensor size $ any_has_forward_grad_for_current_index $ unpacked_arguments $ vector_of_optional_tensor i = $ formula FW_DERIVATIVE_FORBID_TEMPLATE = CodeTemplate \ TORCH_CHECK_NOT_IMPLEMENTED $ cond Trying use forward AD $ name does support $ msg FW_DERIVATIVE_FORBID_LIST_TEMPLATE = CodeTemplate \ const auto _t $ arg TORCH_CHECK_NOT_IMPLEMENTED $ cond Trying use forward AD $ name does support $ msg gen_variable_type out str native_yaml_path str tags_yaml_path str fns_with_diff_infos list NativeFunctionWithDifferentiabilityInfo template_path str used_keys set str - None VariableType h VariableType cpp body This Type subclass differentiable tensors The implementation each function dispatches base tensor type compute output The grad_fn attached differentiable functions fm = FileManager install_dir=out template_dir=template_path dry_run=False fm write VariableType h lambda generated_comment + f generated fm template_dir_for_comments VariableType h helper generates TORCH_LIBRARY_IMPL macro each dispatch key appears derivatives yaml wrapper_registrations used_keys set str - str library_impl_macro_list list str = key sorted used_keys dispatch_key = key key == Default dispatch_key = Autograd library_impl_macro = f TORCH_LIBRARY_IMPL aten dispatch_key m + \n + $ + f wrapper_registrations_ key + \n library_impl_macro_list += library_impl_macro \n\n join library_impl_macro_list Generate new template VariableType cpp which replaces $ wrapper_registrations per key TORCH_LIBRARY_IMPL macros each key appears derivatives yaml fm = FileManager install_dir=out + templates template_dir=template_path dry_run=False fm write VariableType cpp lambda type_derived_method_definitions \n\n join $ + f type_derived_method_definitions_ key + key sorted used_keys wrapper_registrations wrapper_registrations used_keys Generate final VariableType_ cpp files generated template fm = FileManager install_dir=out template_dir=out + templates dry_run=False sharded_keys = set f type_derived_method_definitions_ key key sorted used_keys + f wrapper_registrations_ key key sorted used_keys NOTE see Note Sharded File top VariableType cpp template regarding sharding generated files fm write_sharded VariableType cpp fn fn fns_with_diff_infos use_derived fn key_fn=lambda fn cpp name fn func func base_env= generated_comment + f generated fm template_dir_for_comments VariableType cpp env_callable=gen_variable_type_func num_shards= sharded_keys=sharded_keys with_native_function_and gen_wrapper_registration f NativeFunction key str = Default - str WRAPPER_REGISTRATION substitute unqual_operator_name_with_overload=f func name type_wrapper_name=type_wrapper_name f key class_type= VariableType gen_variable_type_func fn NativeFunctionWithDifferentiabilityInfo - dict str list str f = fn func result = native_function_manager f name = cpp name f func formals = gen_formals f fn info None str f func name name RESET_GRAD_ACCUMULATOR get_base_name f DONT_REQUIRE_DERIVATIVE len gen_differentiable_outputs fn cpp name f func DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE type_wrapper_name f DONT_ENFORCE_STORAGE_IMPL_USE_COUNT type_wrapper_name f DONT_ENFORCE_TENSOR_IMPL_USE_COUNT NOTE Registering AutogradNotImplemented boxed kernel When there no derivatives yaml entry we register generic boxed NotImplemented kernel set grad_fn NotImplemented so forward proceeds usual error properly produced backward TODO would nice have these special cases There several cases where still let codegen handle ops need reset grad accumulator we let codegen handle case because list currently only accessible Python User explicitly specifies DONT_REQUIRE_DERIVATIVE This basically makes autograd fallthrough NDEBUG checks This can useful when all outputs integral When there no differentiable outputs This similar There certain ops where we skip certain NDEBUG checks similar type_definition = wrapper_registration = AUTOGRAD_NOT_IMPLEMENTED_REGISTRATION substitute unqual_operator_name_with_overload=f func name result type_derived_method_definitions_Default = type_definition result wrapper_registrations_Default = wrapper_registration fn info key = Default type_definition = METHOD_DEFINITION substitute return_type=cpp returns_type f func returns symint=True cpp_type type_wrapper_name=type_wrapper_name f key type_definition_body=emit_body fn key formals=formals wrapper_registration = gen_wrapper_registration f key result f type_derived_method_definitions_ key = type_definition result f wrapper_registrations_ key = wrapper_registration key fn info keys type_definition = METHOD_DEFINITION substitute return_type=cpp returns_type f func returns symint=True cpp_type type_wrapper_name=type_wrapper_name f key type_definition_body=emit_body fn key formals=formals wrapper_registration = gen_wrapper_registration f key result f type_derived_method_definitions_ key = type_definition result f wrapper_registrations_ key = wrapper_registration See Note Manual Backend kernels assert name MANUAL_BACKEND == f manual_kernel_registration If you want register kernel Autograd you must make op abstract In other words op must have dispatch section native_functions yaml name MANUAL_AUTOGRAD_AND_TRACER fn info any info has_derivatives info fn info values msg = f There s formula name its functional variant derivatives yaml f It s required add dispatch section explicit supported backends e g CPU CUDA f CompositeExplicitAutograd native_functions yaml Please see f https github com pytorch pytorch tree master aten src ATen native#choosing-the-right-dispatch-keyword f instructions choose right dispatch keyword assert f is_abstract msg result _foreach_ops_without_differentiability_info = No reference backward available due lack ` maximum minimum tensor scalar ` _foreach_maximum Scalar _foreach_maximum ScalarList _foreach_minimum Scalar _foreach_minimum ScalarList No reference backward available addcdiv addcmul don t support Tensor scaling factor _foreach_addcdiv Tensor _foreach_addcmul Tensor _foreach_copy _foreach_ops_with_different_arity = These ops lack ` alpha ` scaling factor applied right hand side argument _foreach_add Scalar _foreach_add ScalarList _foreach_sub Scalar _foreach_sub ScalarList with_native_function_with_differentiability_info_and_key emit_body fn NativeFunctionWithDifferentiabilityInfo key str = Default - list str assert dispatch_strategy fn == use_derived f = fn func info = fn info key fn info None fw_derivatives = fn fw_derivatives get key fn fw_derivatives name = cpp name f func inplace = f func kind == SchemaKind inplace is_out_fn = f func kind == SchemaKind out returns_void = len f func returns == base_name = get_base_name f view_info = get_view_info f is_foreach = name startswith _foreach is_inplace_foreach = is_foreach inplace is_inplace_foreach inplace_foreacharg refarg dict Argument Argument = refargname inplace_foreacharg dict str Argument = base_name_and_overload_name = f func name name base f func name overload_name info None assert base_name_and_overload_name _foreach_ops_without_differentiability_info f join base_name_and_overload_name should have differentiability info assert len f func arguments flat_non_out == len info func func arguments flat_non_out base_name_and_overload_name _foreach_ops_with_different_arity f join base_name_and_overload_name has len f func arguments flat_non_out args f reference has len info func func arguments flat_non_out foreach_arg ref_arg zip f func arguments flat_non_out info func func arguments flat_non_out foreach_arg_type = foreach_arg type isinstance foreach_arg_type ListType foreach_arg_type = foreach_arg_type elem assert foreach_arg_type == ref_arg type inplace_foreacharg refarg foreach_arg = ref_arg refargname inplace_foreacharg ref_arg name = foreach_arg gen_differentiable_input arg Argument &#124; SelfArgument &#124; TensorOptionsArguments - DifferentiableInput &#124; None isinstance arg TensorOptionsArguments None Argument = arg argument isinstance arg SelfArgument arg TODO ` cpp_type ` only keep byte-for-byte compatible old codegen should remove NB This clone cpp argument - TensorOptionsArguments faithful binds handled properly they irrelevant codegen cpp_type = cpp argument_type binds=a name symint=True cpp_type is_differentiable name type info None DifferentiableInput name=a name type=a type cpp_type=cpp_type with_native_function gen_differentiable_inputs f NativeFunction - list DifferentiableInput arguments = list f func arguments non_out is_inplace_foreach info None i arg enumerate f func arguments flat_non_out arg inplace_foreacharg refarg note crcrpar From what I understand what matters only name Thus originally I only replace argument only when names different TODO crcrpar Make simpler mapped_arg = inplace_foreacharg refarg arg arguments i = Argument mapped_arg name mapped_arg type mapped_arg default mapped_arg annotation list mapMaybe gen_differentiable_input arguments find_args_with_derivatives differentiable_inputs list DifferentiableInput - list DifferentiableInput Find arguments have derivative definitions info None info has_derivatives differentiable_inputs names = name d info derivatives name d var_names differentiable = arg arg differentiable_inputs arg name names len differentiable = len names missing = names - arg name arg differentiable raise RuntimeError f Missing arguments derivatives missing info name differentiable differentiable_inputs = gen_differentiable_inputs f args_with_derivatives = find_args_with_derivatives differentiable_inputs differentiable_outputs = gen_differentiable_outputs fn key undifferentiable = base_name DONT_REQUIRE_DERIVATIVE name DONT_REQUIRE_DERIVATIVE requires_derivative = undifferentiable len differentiable_inputs len differentiable_outputs note crcrpar In-place foreach functions void function is_inplace_foreach info None info has_derivatives requires_derivative out= ops allowed have zero returns which cause requires_derivative False we shouldn t error out though out= ops autograd just redispatch len f func returns raise RuntimeError f ERROR derivative ignored name -- specified autograd function without derivative note crcrpar In-place foreach functions do support forward AD requires_derivative len fw_derivatives is_inplace_foreach assert sum len derivative var_names derivative fw_derivatives == len differentiable_outputs Expected number forward derivatives implemented match number differentiable outputs NB This only applies when least one forward derivative implemented Not implementing any forward derivatives also okay we would require inputs op have associated tangents case try_jit_decomposition = requires_derivative len fw_derivatives == modifies_arguments f returns_void emit_save_inputs - list str setup list str = info None info has_derivatives setup has_tensorlist_arg = any is_tensor_list_type arg type arg args_with_derivatives We don t want save tensors we know they will never used when computing derivative so we add guards those statements guard_for arg SavedAttribute - str &#124; None assert info None It s hard determine edge offset we have TensorLists NOTE crcrpar in-place foreach functions arguments include tensorlist their derivatives don t use so let them bypass check has_tensorlist_arg is_inplace_foreach None Empirical evaluation cases where we insert those guards backward show they somewhat useless E g there s no need guard some values captured forward because they had require_grad backward function even gets executed I don t have any good ideas detecting those cases so I simply disabled checks backward info name None If there s single derivative we could compute we already have requires_grad check sufficient len args_with_derivatives = None We really only care about trimming down amount tensors we save arg nctype type = BaseCType tensorT None We want emit simple guards so we only allow checking one input enough determine whether we need value used_in = d d info derivatives arg d saved_inputs assert len used_in len used_in = None derivative = used_in Case multioutput formulas TODO process all derivative formulas len derivative var_names = wrap_opt_if_start = derivative formula find f wrap_opt_if arg nctype name wrap_opt_if_start == - None wrap_opt_if_match = re match rf wrap_opt_if\ arg nctype name \ derivative formula wrap_opt_if_start assert wrap_opt_if_match None Condition between wrap_opt_if var_name condition_slice = slice len rf wrap_opt_if\ arg nctype name - wrap_opt_if_condition = wrap_opt_if_match group condition_slice strip replace grad_input_mask num grad_fn- should_compute_output num wrap_opt_if_condition = re sub r grad_input_mask\ \d+ \ r grad_fn- should_compute_output \ wrap_opt_if_condition f wrap_opt_if_condition Figure out offset edge uses variable derivative_var_name = derivative var_names edge_off enumerate args_with_derivatives name == derivative_var_name break raise AssertionError f grad_fn- should_compute_output edge_off is_inplace_foreach save_input_stmts = save_variables info all_saved_inputs False guard_for save_input_stmts setup append LOOP_OVER_VECTOR_OF_GRAD_FNS substitute preamble= statements=save_input_stmts setup extend save_variables info all_saved_inputs False guard_for arg args_with_derivatives is_tensor_list_type arg type setup append f grad_fn- arg name _size_ = arg name size setup setup_derivative differentiable_inputs list DifferentiableInput - list str body list str = is_out_fn For out functions ensure no input output requires grad body append DECLARE_GRAD_FN substitute op= Node body append SETUP_NONE_REQUIRES_GRAD substitute base_name=base_name args_to_check= arg name arg differentiable_inputs body append SETUP_NONE_REQUIRES_GRAD substitute base_name=base_name args_to_check= arg name arg differentiable_outputs body op = info op info None info has_derivatives NotImplemented setup = is_inplace_foreach setup extend ASSIGN_GRAD_FN substitute op=op op_ctor= info None info has_derivatives f cpp name f func args_with_derivatives= arg name arg args_with_derivatives split \n note crcrpar Assuming in-place foreach function s self_arg always TensorList list_like_arg = args = arg name arg args_with_derivatives i arg enumerate args is_inplace_foreach info None arg refargname inplace_foreacharg foreach_arg = refargname inplace_foreacharg arg args i = foreach_arg name + i isinstance foreach_arg type ListType arg == list_like_arg args i = arg + i setup extend ASSIGN_VECTOR_OF_GRAD_FN substitute op=op op_ctor= info None info has_derivatives f cpp name f func args_with_derivatives=args irange=f list_like_arg size split \n setup extend emit_save_inputs body extend emit_check_no_requires_grad differentiable_inputs args_with_derivatives declare_grad_fn_template = DECLARE_GRAD_FN is_inplace_foreach DECLARE_VECTOR_OF_GRAD_FN body append declare_grad_fn_template substitute op=op body append SETUP_DERIVATIVE substitute setup=setup body emit_check_if_in_complex_autograd_allowlist - list str body list str = base_name GRADIENT_IMPLEMENTED_FOR_COMPLEX body arg differentiable_outputs name = arg name TODO should ` arg type is_tensor_like ` arg cpp_type == Tensor arg cpp_type TENSOR_LIST_LIKE_CTYPES body append f throw_error_for_complex_autograd name base_name body emit_check_no_requires_grad tensor_args list DifferentiableInput args_with_derivatives list DifferentiableInput - list str Checks arguments without derivatives don t require grad body list str = arg tensor_args arg args_with_derivatives continue arg_name = arg name info arg_name info non_differentiable_arg_names continue arg_name == output Double-backwards definitions sometimes take input output only define derivative input continue body append f check_no_requires_grad arg_name arg_name name body emit_original_self_definition - list str body list str = inplace is_inplace_foreach body append std vector std optional Tensor original_selfs size body append std optional Tensor original_self all_forward_grad_cond = derivative fw_derivatives derivative required_original_self_value all_forward_grad_cond append get_any_has_forward_grad_name derivative var_names all_forward_grad_cond is_inplace_foreach body append f &#124; &#124; join all_forward_grad_cond body append original_self = clone body append current_all_forward_grad_cond = f cond i cond all_forward_grad_cond body append const auto i c irange size body append f &#124; &#124; join current_all_forward_grad_cond body append original_selfs i = i clone body append body append body save_variables saved_variables Sequence SavedAttribute is_output bool guard_for Callable SavedAttribute str &#124; None = lambda name None - Sequence str assign saved variables generated grad_fn stmts list str = arg sorted saved_variables key=lambda sa str sa nctype name name = arg nctype name name isinstance arg nctype name SpecialArgName arg nctype name foreacharg Argument &#124; None = None is_foreacharg_list_type bool = False type = arg nctype type expr = arg expr stmts_prepend = None is_inplace_foreach info None todo crcrpar See we can add some check e g ` assert foreacharg None ` now example assert would fail name_to_query = name split _scalar_type name_to_query refargname inplace_foreacharg foreacharg = refargname inplace_foreacharg name_to_query is_foreacharg_list_type = isinstance foreacharg type ListType foreacharg None name_in_expr = f foreacharg name i is_foreacharg_list_type src_name = name _scalar_type src_name split_src_name = src_name split _scalar_type assert len split_src_name == src_name = split_src_name expr = expr replace src_name name_in_expr type == BaseCType tensorT type == OptionalCType BaseCType tensorT type == MutRefCType OptionalCType BaseCType tensorT is_output type == BaseCType scalarT note crcrpar Here ` expr ` generated scratch ` arg expr ` ignored var = name name += _ var == inplace original_self_var = original_self is_inplace_foreach original_selfs i self_var = var is_inplace_foreach var + i stmts_prepend = f original_self_var has_value original_self_var = self_var clone var = f original_self_var value assert is_output inplace is_output assert name == result_ var = i is_inplace_foreach is_foreacharg_list_type is_inplace_view = f var is_view expr = f SavedVariable var str is_output lower is_inplace_view expr = f SavedVariable var str is_output lower foreacharg None original_selfs expr pyrefly ignore unbound-name expr = expr replace src_name name_in_expr type == BaseCType tensorListT type == ListCType OptionalCType BaseCType tensorT type == BaseCType iTensorListRefT type == VectorCType BaseCType tensorT See Note nuanced type out-of-place foreach functions type == VectorCType BaseCType tensorT assert is_foreach is_output expr = f make_saved_variable_list name str is_foreach is_output lower name += _ type == BaseCType intArrayRefT expr = expr + vec type == BaseCType symIntArrayRefT expr = expr + vec type == BaseCType stringT expr = f std string expr type == OptionalCType BaseCType stringT expr = f expr has_value std optional std string std string expr value std nullopt type == ArrayRefCType elem=BaseCType type=BaseCppType ns= name= Scalar expr = expr + vec guard = guard_for arg guard None stmts_prepend stmts append f stmts_prepend stmts append f grad_fn- name = expr stmts append f guard stmts_prepend stmts append f stmts_prepend stmts append f grad_fn- name = expr stmts append stmts Generates Dispatcher redispatch call into dispatcher We do mainly performance reasons - Pre-compute full DispatchKeySet This saves dispatcher having read TLS - redispatch avoids redundant call RecordFunction which already called right before we entered autograd kernel emit_dispatch_call f NativeFunction input_base str unpacked_args Sequence str - str Dispatch call via function namespace method Tensor code-generated autograd kernels plumb recompute dispatch keys directly through kernel performance Ops also always have function variant redispatch API See Note Plumbing Keys Through The Dispatcher details dispatch_key_set = ks c after_autograd_keyset call = CALL_REDISPATCH substitute api_name=cpp name f func faithful_name_for_out_overloads=True symint_overload=f func has_symint unpacked_args= dispatch_key_set + list unpacked_args call wrap_output f NativeFunction unpacked_bindings list Binding var str - str call = rhs_value str &#124; None = None any r type is_tensor_like r f func returns rhs_value = var rhs_value = f std move var assert rhs_value None call += ASSIGN_RETURN_VALUE substitute return_values=tie_return_values f rhs_value=rhs_value call check_tensorimpl_and_storage call str unpacked_bindings list Binding - str See NOTE TensorImpl Storage Pointer Sanity Checks stmts_before_call list str = stmts_after_call list str = cpp name f func DONT_ENFORCE_SAME_TENSOR_IMPL_OR_STORAGE call Check properties inputs enforce unpacked_binding unpacked_bindings arg = unpacked_binding name noref_cpp_type = unpacked_binding nctype type remove_const_ref noref_cpp_type == BaseCType tensorListT noref_cpp_type == BaseCType iTensorListRefT stmts_before_call += SAVE_TENSORLIST_STORAGE substitute tensorlist_name=arg SAVE_TENSORLIST_IMPL substitute tensorlist_name=arg stmts_after_call += ENFORCE_SAME_TENSORLIST_STORAGE substitute tensorlist_name=arg ENFORCE_SAME_TENSORLIST_IMPL substitute tensorlist_name=arg noref_cpp_type == ListCType OptionalCType BaseCType tensorT stmts_before_call += SAVE_OPTIONALTENSORLIST_STORAGE substitute tensorlist_name=arg SAVE_OPTIONALTENSORLIST_IMPL substitute tensorlist_name=arg stmts_after_call += ENFORCE_SAME_OPTIONALTENSORLIST_STORAGE substitute tensorlist_name=arg ENFORCE_SAME_OPTIONALTENSORLIST_IMPL substitute tensorlist_name=arg noref_cpp_type == BaseCType tensorT stmts_before_call += SAVE_TENSOR_STORAGE substitute tensor_name=arg SAVE_TENSOR_IMPL substitute tensor_name=arg stmts_after_call += ENFORCE_SAME_TENSOR_STORAGE substitute tensor_name=arg out_tensor_name=arg ENFORCE_SAME_TENSOR_IMPL substitute tensor_name=arg assert stmts_before_call stmts_after_call stmts_before_call stmts_after_call Check properties outputs enforce f func kind SchemaKind inplace SchemaKind out base_name = f func name name base TODO should str f func name name aliased_arg_name = ALL_VIEW_FUNCTIONS get base_name None aliased_arg_name None aliased_arg_name = unpacked_name aliased_arg_name i ret ret_name enumerate zip f func returns cpp return_names f noref_cpp_type = cpp return_type ret symint=True remove_const_ref noref_cpp_type == BaseCType tensorT aliased_arg_name None assert i == Expect non-CompositeImplicitAutograd view function base single output stmts_after_call += ENFORCE_SAME_TENSOR_STORAGE substitute tensor_name=aliased_arg_name out_tensor_name=ret_name type_wrapper_name f DONT_ENFORCE_STORAGE_IMPL_USE_COUNT stmts_after_call += ENFORCE_TENSOR_STORAGE_USE_COUNT_EQUALS_ONE substitute tensor_name=ret_name fn_name=type_wrapper_name f type_wrapper_name f DONT_ENFORCE_TENSOR_IMPL_USE_COUNT stmts_after_call += ENFORCE_TENSOR_IMPL_USE_COUNT_LT_OR_EQ_ONE substitute tensor_name=ret_name fn_name=type_wrapper_name f Currently we don t have any functions following types we should update checks once we do noref_cpp_type == ListCType OptionalCType BaseCType tensorT raise AssertionError f Please add use_count checks noref_cpp_type noref_cpp_type == BaseCType tensorListT raise AssertionError f Please add use_count checks noref_cpp_type stmts_before_call stmts_after_call call = RUN_ONLY_IN_DEBUG_MODE substitute statements=stmts_before_call + call + RUN_ONLY_IN_DEBUG_MODE substitute statements=stmts_after_call call emit_call f NativeFunction unpacked_bindings list Binding try_jit_decomposition bool - str We only care about adding ` AutoDispatchBelowAutograd ` guard non-variable dispatch which corresponds use_derived strategy The purpose guard make sure baseType operations still dispatch non-Variable type even arguments passed now Variables See NOTE Treating Variables non-Variables type dispatch details unpacked_args = b name b unpacked_bindings base_type_call = emit_dispatch_call f self_ unpacked_args get_view_info f None modifies_arguments f guard = AutoDispatchBelowAutograd guard guard = AutoDispatchBelowADInplaceOrView guard any_has_forward_grad = get_any_has_fw_grad_cond derivative=None requires_derivative false return_types = join cpp return_type symint=True cpp_type f func returns len f func returns return_types = f std tuple return_types arg_names = name cpp arguments f func arguments faithful=True symint=True method=False cpp_no_default_args=set modifies_arguments f returns_void try_jit_decomposition call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES_JVP_DECOMP substitute base_type_call=base_type_call tmp_var=TMP_VAR guard=guard any_has_forward_grad=any_has_forward_grad op_name=cpp name f func op_overload=f func name overload_name return_types=return_types arg_names=arg_names call = DISPATCH_TO_NON_VAR_TYPE_WITH_TMP_RETURN_VALUES substitute base_type_call=base_type_call tmp_var=TMP_VAR guard=guard call += wrap_output f unpacked_bindings TMP_VAR assert try_jit_decomposition call = DISPATCH_TO_NON_VAR_TYPE_WITHOUT_RETURN_VALUES substitute base_type_call=base_type_call guard=guard call = check_tensorimpl_and_storage call unpacked_bindings call emit_history - str fn = rebase modifies_arguments f view_info None set output_names = r name r differentiable_outputs TODO flatten allocates std vector which could expensive outs = CodeTemplate flatten_tensor_args $ outs substitute outs=output_names is_inplace_foreach is_inplace_foreach SET_HISTORY substitute fn=fn differentiable_outputs=outs LOOP_OVER_VECTOR_OF_GRAD_FNS substitute preamble= f auto differentiable_outputs = outs \n f TORCH_INTERNAL_ASSERT differentiable_outputs size == grad_fns size statements=f fn _history differentiable_outputs i grad_fns i emit_save_outputs - str is_out_fn out functions don t currently support differentiation info None info has_derivatives stmts = save_variables info all_saved_outputs True len stmts == is_inplace_foreach CONDITIONAL substitute cond= grad_fn statements=stmts LOOP_OVER_VECTOR_OF_GRAD_FNS substitute preamble= statements=stmts emit_any_requires_grad - list str extra_condition = info info output_differentiability_conditions assert len info output_differentiability_conditions == extra_condition = f _any_requires_grad = info output_differentiability_conditions names_of_args_with_derivatives = arg name arg args_with_derivatives is_inplace_foreach info None i arg enumerate names_of_args_with_derivatives f_arg r_arg inplace_foreacharg refarg items arg == r_arg name names_of_args_with_derivatives i = f_arg name SETUP_ANY_REQUIRES_GRAD substitute args_with_derivatives=names_of_args_with_derivatives extra_differentiability_conditions=extra_condition get_any_has_forward_grad_name var_names tuple str - str len var_names == f _any_has_forward_grad_ var_names f _any_has_forward_grad_ _ join var_names emit_any_has_forward_grad - list str content list str = is_foreach derivative fw_derivatives requires_fw_grad = get_any_has_fw_grad_cond derivative=derivative info info output_differentiability_conditions assert len info output_differentiability_conditions == requires_fw_grad = f info output_differentiability_conditions requires_fw_grad content append f maybe_unused auto get_any_has_forward_grad_name derivative var_names = requires_fw_grad derivative fw_derivatives bool_vector_name = get_any_has_forward_grad_name derivative var_names cur_derivative_conditions = inp differentiable_inputs derivative required_inputs_fw_grad None continue inp name derivative required_inputs_fw_grad continue inp_name = inp name inplace refargname inplace_foreacharg inp name name inp_type = inp type inplace refargname inplace_foreacharg inp name type is_list_type = is_tensor_list_type inp_type is_list_type inp_name = content append FW_DERIVATIVE_SIZE_CHECK_TEMPLATE substitute inp_name=inp_name cur_derivative_conditions append pyrefly ignore bad-argument-type FW_DERIVATIVE_CHECK_TEMPLATE substitute req_inp=inp_name + i cur_derivative_conditions append pyrefly ignore bad-argument-type FW_DERIVATIVE_CHECK_TEMPLATE substitute req_inp=inp_name content append f std vector bool bool_vector_name size content append const auto i c irange size content append f bool_vector_name i = &#124; &#124; join cur_derivative_conditions content append content emit_check_inplace - list str inplace f check_inplace arg name _any_requires_grad arg differentiable_outputs emit_fw_derivatives - list str content list str = fw_grad_setters list str = derivative fw_derivatives res = derivative var_names f func name name inplace assert len res == Expected number outputs function inplace TODO update when inplace namings unified res = assert derivative required_inputs_fw_grad None unpacked_arguments = inp differentiable_inputs inp_name = inp name is_input_tensorlist = is_foreach is_tensor_list_type inp type inplace refargname inplace_foreacharg inp name type input_suffix = i is_input_tensorlist is_inplace_foreach inp name refargname inplace_foreacharg inp_name = refargname inplace_foreacharg inp name name zeros_fn = zeros_symint inplace inp name == _efficientzerotensor_symint inp name derivative required_inputs_fw_grad unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE substitute inp_name=inp name inp=inp_name + input_suffix zeros_fn=zeros_fn inp name derivative required_inputs_primal unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE substitute inp_name=inp name inp=inp_name + input_suffix derivative required_original_self_value input_suffix = s i is_inplace_foreach unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE substitute inp_name= original_self inp= original_self + input_suffix pyrefly ignore unbound-name zeros_fn=zeros_fn unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE substitute inp_name= original_self inp= original_self + input_suffix inplace derivative is_reusing_outplace_formula The gradient wasn t already cloned do grad mode enabled unpacked_arguments += self_t = GradMode is_enabled self_t clone self_t inplace is_inplace_str = true is_inplace_str = false requires_fw_grad = get_any_has_forward_grad_name derivative var_names all isinstance var_type BaseType var_type is_tensor_like var_type derivative var_types Is there way get BaseType BaseCType len derivative var_types == opt_res_grad_type = OptionalCType BaseCType tensorT cpp_type is_foreach fw_grad_setters append FW_DERIVATIVE_SETTER_TENSOR substitute out_arg=res is_inplace=is_inplace_str assert res == result inplace fw_grad_setters append FW_DERIVATIVE_SETTER_TENSOR_FOREACH substitute out_arg=res is_inplace=is_inplace_str requires_fw_grad += f derivative var_names defined tuple_type = TupleCType BaseCType tensorT len derivative var_types opt_res_grad_type = OptionalCType tuple_type cpp_type idx single_res enumerate res fw_grad_setters append FW_DERIVATIVE_SETTER_MULTI_OUTPUT substitute idx=idx all_res= _ join res out_arg=single_res isinstance derivative var_types ListType derivative var_types is_tensor_like assert len derivative var_types == Expected number outputs function returns ListType is_foreach opt_res_grad_type = OptionalCType VectorCType BaseCType tensorT cpp_type fw_grad_setters append FW_DERIVATIVE_SETTER_TENSOR_LIST substitute out_arg=res is_inplace=is_inplace_str TODO crcrpar Should = foreach specific logic refactored somehow Only out-place foreach functions have entries ` tools autograd derivatives yaml ` can reach here opt_res_grad_type = OptionalCType BaseCType tensorT cpp_type fw_grad_setters append FW_DERIVATIVE_SETTER_TENSOR_FOREACH substitute out_arg=res is_inplace=is_inplace_str raise RuntimeError Unsupported output type forward derivative is_foreach fw_grad_opt_definition = f opt_res_grad_type _ join res _new_fw_grad_opt = std nullopt View ops create fw_grad already view base s fw_grad so just use content append FW_DERIVATIVE_TEMPLATE substitute fw_grad_opt_definition=fw_grad_opt_definition requires_fw_grad=requires_fw_grad formula=derivative formula out_arg= _ join res unpacked_arguments=unpacked_arguments note crcrpar Assuming ` ` TensorList fw_grad_opt_definition = f std vector opt_res_grad_type _ join res _new_fw_grad_opts size std nullopt foreach_forward_grad_formula = derivative formula _foreach_arg Argument &#124; DifferentiableInput inplace _foreach_arg _ref_arg inplace_foreacharg refarg items note crcrpar Massage only Scalar ArrayRef Scalar here is_tensor_type _foreach_arg type is_tensor_list_type _foreach_arg type pattern = _foreach_arg name isinstance _foreach_arg type ListType pattern += i foreach_forward_grad_formula = foreach_forward_grad_formula replace _ref_arg name pattern result foreach_forward_grad_formula result i foreach_forward_grad_formula foreach_forward_grad_formula = foreach_forward_grad_formula replace result result i content append FW_DERIVATIVE_FOREACH_TEMPLATE substitute fw_grad_opt_definition=fw_grad_opt_definition vector_of_optional_tensor=f _ join res _new_fw_grad_opts any_has_forward_grad_for_current_index= &#124; &#124; join get_any_has_forward_grad_name derivative var_names + i derivative fw_derivatives formula=foreach_forward_grad_formula unpacked_arguments=unpacked_arguments Set all grads end avoid https github com pytorch pytorch issues content append \n join fw_grad_setters content get_any_has_fw_grad_cond derivative ForwardDerivative &#124; None - str Produces condition string e g isFwGradDefined grad_output &#124; &#124; isFwGradDefined output derivative None If derivative NOT provided cond will check fw_grad ALL differentiable inputs - Used out_fn case when we want forbid fw derivatives - Used case where fw_derivative defined we want To check there decomposition registered jvp to_check list str = inp list mapMaybe gen_differentiable_input f func arguments non_out + list f func arguments out type ignore operator is_tensor_type inp type to_check append FW_DERIVATIVE_CHECK_TEMPLATE substitute req_inp=inp name is_tensor_list_type inp type to_check append FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE substitute req_inp=inp name raise RuntimeError f Unsupported input type name when forbidding forward AD usage f &#124; &#124; join to_check If derivative provided use information determine which inputs check fw_grad assert derivative required_inputs_fw_grad None len derivative required_inputs_fw_grad == Handle functions like stack For these we don t unpack anything always call user function len differentiable_inputs == is_tensor_list_type differentiable_inputs type raise RuntimeError f No differentiable input name differentiable Tensor provided forward AD formula does use any input tangent even though forward gradient formula has been defined This case should only happen function take single TensorList input All other cases supported right now any_has_fw_grad = true any_has_fw_grad = &#124; &#124; join FW_DERIVATIVE_TENSORLIST_CHECK_TEMPLATE is_tensor_list_type inp type FW_DERIVATIVE_CHECK_TEMPLATE substitute req_inp=inp name inp differentiable_inputs inp name derivative required_inputs_fw_grad any_has_fw_grad = f any_has_fw_grad any_has_fw_grad emit_forbid_fw_derivatives is_out_fn bool = False - str is_out_fn msg = because out= function msg = because has been implemented yet \\nPlease file issue PyTorch https github com pytorch pytorch issues new template=feature-request yml so we can prioritize its implementation cond = get_any_has_fw_grad_cond derivative=None FW_DERIVATIVE_FORBID_TEMPLATE substitute cond=cond name=name msg=msg cond = body list str = unpack_args_stats unpacked_bindings = unpack_args f body extend unpack_args_stats requires_derivative body extend emit_any_requires_grad body extend emit_any_has_forward_grad body extend emit_check_inplace body extend emit_original_self_definition body extend setup_derivative differentiable_inputs body append emit_call f unpacked_bindings try_jit_decomposition requires_derivative set_flags has appear after version_counter because rebase_history requires counter incremented before called body append emit_history body extend emit_check_if_in_complex_autograd_allowlist is_out_fn body append emit_forbid_fw_derivatives is_out_fn=True requires_derivative try_jit_decomposition len fw_derivatives body extend emit_fw_derivatives body append emit_forbid_fw_derivatives requires_derivative Save only after forward AD has been set up body append emit_save_outputs str f func name name RESET_GRAD_ACCUMULATOR ` inplace ` implies there exactly one output named ` ` so we can keep generated code easy If you need ` reset_grad_accumulator ` operator s ` inplace ` you can remove assert code generation will get more elaborate assert inplace body append reset_grad_accumulator returns_void body append f get_return_value f body