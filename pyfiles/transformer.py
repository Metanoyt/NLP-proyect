mypy allow-untyped-defs copy warnings collections abc Callable typing Any Optional Union torch torch nn functional F torch Tensor torch nn init xavier_uniform_ activation MultiheadAttention container ModuleList dropout Dropout linear Linear module Module normalization LayerNorm __all__ = Transformer TransformerEncoder TransformerDecoder TransformerEncoderLayer TransformerDecoderLayer _generate_square_subsequent_mask sz int device Optional torch device = None dtype Optional torch dtype = None - Tensor r Generate square causal mask sequence The masked positions filled float -inf Unmasked positions filled float torch triu torch full sz sz float -inf dtype=dtype device=device diagonal= _get_seq_len src Tensor batch_first bool - Optional int src is_nested None src_size = src size len src_size == unbatched S E src_size batched B S E batch_first S B E seq_len_pos = batch_first src_size seq_len_pos Transformer Module r A basic transformer layer This Transformer layer implements original Transformer architecture described ` Attention Is All You Need https arxiv org abs ` _ paper The intent layer reference implementation foundational understanding thus contains only limited features relative newer Transformer architectures Given fast pace innovation transformer-like architectures we recommend exploring ` tutorial https pytorch org tutorials intermediate transformer_building_blocks html ` _ build efficient transformer layer building blocks core using higher level libraries ` PyTorch Ecosystem https landscape pytorch org ` _ Args d_model number expected features encoder decoder inputs default= nhead number heads multiheadattention models default= num_encoder_layers number sub-encoder-layers encoder default= num_decoder_layers number sub-decoder-layers decoder default= dim_feedforward dimension feedforward network model default= dropout dropout value default= activation activation function encoder decoder intermediate layer can string relu gelu unary callable Default relu custom_encoder custom encoder default=None custom_decoder custom decoder default=None layer_norm_eps eps value layer normalization components default= e- batch_first If ` ` True ` ` then input output tensors provided batch seq feature Default ` ` False ` ` seq batch feature norm_first ` ` True ` ` encoder decoder layers will perform LayerNorms before other attention feedforward operations otherwise after Default ` ` False ` ` after bias If set ` ` False ` ` ` ` Linear ` ` ` ` LayerNorm ` ` layers will learn additive bias Default ` ` True ` ` Examples transformer_model = nn Transformer nhead= num_encoder_layers= src = torch rand tgt = torch rand out = transformer_model src tgt Note A full example apply nn Transformer module word language model available https github com pytorch examples tree master word_language_model __init__ d_model int = nhead int = num_encoder_layers int = num_decoder_layers int = dim_feedforward int = dropout float = activation Union str Callable Tensor Tensor = F relu custom_encoder Optional Any = None custom_decoder Optional Any = None layer_norm_eps float = e- batch_first bool = False norm_first bool = False bias bool = True device=None dtype=None - None factory_kwargs = device device dtype dtype super __init__ torch _C _log_api_usage_once f torch nn modules __class__ __name__ custom_encoder None encoder = custom_encoder encoder_layer = TransformerEncoderLayer d_model nhead dim_feedforward dropout activation layer_norm_eps batch_first norm_first bias factory_kwargs encoder_norm = LayerNorm d_model eps=layer_norm_eps bias=bias pyrefly ignore bad-argument-type factory_kwargs encoder = TransformerEncoder encoder_layer num_encoder_layers encoder_norm custom_decoder None decoder = custom_decoder decoder_layer = TransformerDecoderLayer d_model nhead dim_feedforward dropout activation layer_norm_eps batch_first norm_first bias factory_kwargs decoder_norm = LayerNorm d_model eps=layer_norm_eps bias=bias pyrefly ignore bad-argument-type factory_kwargs decoder = TransformerDecoder decoder_layer num_decoder_layers decoder_norm _reset_parameters d_model = d_model nhead = nhead batch_first = batch_first forward src Tensor tgt Tensor src_mask Optional Tensor = None tgt_mask Optional Tensor = None memory_mask Optional Tensor = None src_key_padding_mask Optional Tensor = None tgt_key_padding_mask Optional Tensor = None memory_key_padding_mask Optional Tensor = None src_is_causal Optional bool = None tgt_is_causal Optional bool = None memory_is_causal bool = False - Tensor r Take process masked source target sequences note If boolean tensor provided any src tgt memory _mask arguments positions ` ` True ` ` value allowed participate attention which opposite definition attr ` attn_mask ` func ` torch nn functional scaled_dot_product_attention ` Args src sequence encoder required tgt sequence decoder required src_mask additive mask src sequence optional tgt_mask additive mask tgt sequence optional memory_mask additive mask encoder output optional src_key_padding_mask Tensor mask src keys per batch optional tgt_key_padding_mask Tensor mask tgt keys per batch optional memory_key_padding_mask Tensor mask memory keys per batch optional src_is_causal If specified applies causal mask ` ` src_mask ` ` Default ` ` None ` ` try detect causal mask Warning ` ` src_is_causal ` ` provides hint ` ` src_mask ` ` causal mask Providing incorrect hints can result incorrect execution including forward backward compatibility tgt_is_causal If specified applies causal mask ` ` tgt_mask ` ` Default ` ` None ` ` try detect causal mask Warning ` ` tgt_is_causal ` ` provides hint ` ` tgt_mask ` ` causal mask Providing incorrect hints can result incorrect execution including forward backward compatibility memory_is_causal If specified applies causal mask ` ` memory_mask ` ` Default ` ` False ` ` Warning ` ` memory_is_causal ` ` provides hint ` ` memory_mask ` ` causal mask Providing incorrect hints can result incorrect execution including forward backward compatibility Shape - src math ` S E ` unbatched input math ` S N E ` ` batch_first=False ` ` N S E ` ` batch_first=True ` - tgt math ` T E ` unbatched input math ` T N E ` ` batch_first=False ` ` N T E ` ` batch_first=True ` - src_mask math ` S S ` math ` N\cdot\text num\_heads S S ` - tgt_mask math ` T T ` math ` N\cdot\text num\_heads T T ` - memory_mask math ` T S ` - src_key_padding_mask math ` S ` unbatched input otherwise math ` N S ` - tgt_key_padding_mask math ` T ` unbatched input otherwise math ` N T ` - memory_key_padding_mask math ` S ` unbatched input otherwise math ` N S ` Note src tgt memory _mask ensures position math ` i ` allowed attend unmasked positions If BoolTensor provided positions ` ` True ` ` allowed attend while ` ` False ` ` values will unchanged If FloatTensor provided will added attention weight src tgt memory _key_padding_mask provides specified elements key ignored attention If BoolTensor provided positions value ` ` True ` ` will ignored while position value ` ` False ` ` will unchanged - output math ` T E ` unbatched input math ` T N E ` ` batch_first=False ` ` N T E ` ` batch_first=True ` Note Due multi-head attention architecture transformer model output sequence length transformer same input sequence i e target length decoder where math ` S ` source sequence length math ` T ` target sequence length math ` N ` batch size math ` E ` feature number Examples xdoctest +SKIP output = transformer_model src tgt src_mask=src_mask tgt_mask=tgt_mask is_batched = src dim == batch_first src size = tgt size is_batched raise RuntimeError batch number src tgt must equal batch_first src size = tgt size is_batched raise RuntimeError batch number src tgt must equal src size - = d_model tgt size - = d_model raise RuntimeError feature number src tgt must equal d_model memory = encoder src mask=src_mask src_key_padding_mask=src_key_padding_mask is_causal=src_is_causal output = decoder tgt memory tgt_mask=tgt_mask memory_mask=memory_mask tgt_key_padding_mask=tgt_key_padding_mask memory_key_padding_mask=memory_key_padding_mask tgt_is_causal=tgt_is_causal memory_is_causal=memory_is_causal output staticmethod generate_square_subsequent_mask sz int device Optional torch device = None dtype Optional torch dtype = None - Tensor r Generate square causal mask sequence The masked positions filled float -inf Unmasked positions filled float _generate_square_subsequent_mask sz dtype=dtype device=device _reset_parameters - None r Initiate parameters transformer model p parameters p dim xavier_uniform_ p TransformerEncoder Module r TransformerEncoder stack N encoder layers This TransformerEncoder layer implements original architecture described ` Attention Is All You Need https arxiv org abs ` _ paper The intent layer reference implementation foundational understanding thus contains only limited features relative newer Transformer architectures Given fast pace innovation transformer-like architectures we recommend exploring ` tutorial https pytorch org tutorials intermediate transformer_building_blocks html ` _ build efficient layers building blocks core using higher level libraries ` PyTorch Ecosystem https landscape pytorch org ` _ warning All layers TransformerEncoder initialized same parameters It recommended manually initialize layers after creating TransformerEncoder instance Args encoder_layer instance TransformerEncoderLayer required num_layers number sub-encoder-layers encoder required norm layer normalization component optional enable_nested_tensor True input will automatically convert nested tensor convert back output This will improve overall performance TransformerEncoder when padding rate high Default ` ` True ` ` enabled Examples encoder_layer = nn TransformerEncoderLayer d_model= nhead= transformer_encoder = nn TransformerEncoder encoder_layer num_layers= src = torch rand out = transformer_encoder src __constants__ = norm __init__ encoder_layer TransformerEncoderLayer num_layers int norm Optional Module = None enable_nested_tensor bool = True mask_check bool = True - None super __init__ torch _C _log_api_usage_once f torch nn modules __class__ __name__ layers = _get_clones encoder_layer num_layers num_layers = num_layers norm = norm attribute saves value providedat object construction enable_nested_tensor = enable_nested_tensor attribute controls whether nested tensors used use_nested_tensor = enable_nested_tensor mask_check = mask_check enc_layer = encoder_layer why_not_sparsity_fast_path = isinstance encoder_layer torch nn TransformerEncoderLayer why_not_sparsity_fast_path = f enc_layer TransformerEncoderLayer encoder_layer norm_first why_not_sparsity_fast_path = f enc_layer norm_first True encoder_layer self_attn batch_first why_not_sparsity_fast_path = f enc_layer self_attn batch_first True + use batch_first better inference performance encoder_layer self_attn _qkv_same_embed_dim why_not_sparsity_fast_path = f enc_layer self_attn _qkv_same_embed_dim True encoder_layer self_attn in_proj_bias None why_not_sparsity_fast_path = f enc_layer self_attn passed bias=False encoder_layer activation_relu_or_gelu why_not_sparsity_fast_path = f enc_layer activation_relu_or_gelu True encoder_layer norm eps = encoder_layer norm eps why_not_sparsity_fast_path = f enc_layer norm eps equal enc_layer norm eps encoder_layer self_attn num_heads == why_not_sparsity_fast_path = f enc_layer self_attn num_heads odd enable_nested_tensor why_not_sparsity_fast_path warnings warn f enable_nested_tensor True use_nested_tensor False because why_not_sparsity_fast_path stacklevel= use_nested_tensor = False forward src Tensor mask Optional Tensor = None src_key_padding_mask Optional Tensor = None is_causal Optional bool = None - Tensor r Pass input through encoder layers turn Args src sequence encoder required mask mask src sequence optional src_key_padding_mask mask src keys per batch optional is_causal If specified applies causal mask ` ` mask ` ` Default ` ` None ` ` try detect causal mask Warning ` ` is_causal ` ` provides hint ` ` mask ` ` causal mask Providing incorrect hints can result incorrect execution including forward backward compatibility Shape see docs ` ~torch nn Transformer ` src_key_padding_mask = F _canonical_mask mask=src_key_padding_mask mask_name= src_key_padding_mask other_type=F _none_or_dtype mask other_name= mask target_type=src dtype mask = F _canonical_mask mask=mask mask_name= mask other_type=None other_name= target_type=src dtype check_other=False output = src convert_to_nested = False first_layer = layers src_key_padding_mask_for_layers = src_key_padding_mask why_not_sparsity_fast_path = str_first_layer = layers batch_first = first_layer self_attn batch_first is_fastpath_enabled = torch backends mha get_fastpath_enabled do_mask_check = getattr mask_check True is_fastpath_enabled why_not_sparsity_fast_path = torch backends mha get_fastpath_enabled True hasattr use_nested_tensor why_not_sparsity_fast_path = use_nested_tensor attribute present use_nested_tensor why_not_sparsity_fast_path = use_nested_tensor set init True first_layer training why_not_sparsity_fast_path = f str_first_layer training mode src dim = why_not_sparsity_fast_path = f input batched expected src dim got src dim src_key_padding_mask None why_not_sparsity_fast_path = src_key_padding_mask None This check avoids call torch _nested_tensor_from_mask_left_aligned breaks torch compile do_mask_check torch compiler is_compiling why_not_sparsity_fast_path = mask_check enabled torch compile torch export do_mask_check torch _nested_tensor_from_mask_left_aligned src src_key_padding_mask logical_not why_not_sparsity_fast_path = mask_check enabled src src_key_padding_mask left aligned output is_nested why_not_sparsity_fast_path = NestedTensor input supported mask None why_not_sparsity_fast_path = src_key_padding_mask mask both supplied torch is_autocast_enabled why_not_sparsity_fast_path = autocast enabled why_not_sparsity_fast_path tensor_args = src first_layer self_attn in_proj_weight first_layer self_attn in_proj_bias first_layer self_attn out_proj weight first_layer self_attn out_proj bias first_layer norm weight first_layer norm bias first_layer norm weight first_layer norm bias first_layer linear weight first_layer linear bias first_layer linear weight first_layer linear bias _supported_device_type = cpu cuda torch utils backend_registration _privateuse _backend_name torch overrides has_torch_function tensor_args why_not_sparsity_fast_path = some Tensor argument has_torch_function src device type _supported_device_type why_not_sparsity_fast_path = f src device neither one _supported_device_type torch is_grad_enabled any x requires_grad x tensor_args why_not_sparsity_fast_path = grad enabled least one query input output projection weights biases requires_grad why_not_sparsity_fast_path src_key_padding_mask None convert_to_nested = True output = torch _nested_tensor_from_mask output src_key_padding_mask logical_not mask_check=False src_key_padding_mask_for_layers = None seq_len = _get_seq_len src batch_first is_causal = _detect_is_causal_mask mask is_causal seq_len mod layers output = mod output src_mask=mask is_causal=is_causal src_key_padding_mask=src_key_padding_mask_for_layers convert_to_nested output = output to_padded_tensor src size norm None output = norm output output TransformerDecoder Module r TransformerDecoder stack N decoder layers This TransformerDecoder layer implements original architecture described ` Attention Is All You Need https arxiv org abs ` _ paper The intent layer reference implementation foundational understanding thus contains only limited features relative newer Transformer architectures Given fast pace innovation transformer-like architectures we recommend exploring ` tutorial https pytorch org tutorials intermediate transformer_building_blocks html ` _ build efficient layers building blocks core using higher level libraries ` PyTorch Ecosystem https landscape pytorch org ` _ warning All layers TransformerDecoder initialized same parameters It recommended manually initialize layers after creating TransformerDecoder instance Args decoder_layer instance TransformerDecoderLayer required num_layers number sub-decoder-layers decoder required norm layer normalization component optional Examples decoder_layer = nn TransformerDecoderLayer d_model= nhead= transformer_decoder = nn TransformerDecoder decoder_layer num_layers= memory = torch rand tgt = torch rand out = transformer_decoder tgt memory __constants__ = norm __init__ decoder_layer TransformerDecoderLayer num_layers int norm Optional Module = None - None super __init__ torch _C _log_api_usage_once f torch nn modules __class__ __name__ layers = _get_clones decoder_layer num_layers num_layers = num_layers norm = norm forward tgt Tensor memory Tensor tgt_mask Optional Tensor = None memory_mask Optional Tensor = None tgt_key_padding_mask Optional Tensor = None memory_key_padding_mask Optional Tensor = None tgt_is_causal Optional bool = None memory_is_causal bool = False - Tensor r Pass inputs mask through decoder layer turn Args tgt sequence decoder required memory sequence last layer encoder required tgt_mask mask tgt sequence optional memory_mask mask memory sequence optional tgt_key_padding_mask mask tgt keys per batch optional memory_key_padding_mask mask memory keys per batch optional tgt_is_causal If specified applies causal mask ` ` tgt mask ` ` Default ` ` None ` ` try detect causal mask Warning ` ` tgt_is_causal ` ` provides hint ` ` tgt_mask ` ` causal mask Providing incorrect hints can result incorrect execution including forward backward compatibility memory_is_causal If specified applies causal mask ` ` memory mask ` ` Default ` ` False ` ` Warning ` ` memory_is_causal ` ` provides hint ` ` memory_mask ` ` causal mask Providing incorrect hints can result incorrect execution including forward backward compatibility Shape see docs ` ~torch nn Transformer ` output = tgt seq_len = _get_seq_len tgt layers self_attn batch_first tgt_is_causal = _detect_is_causal_mask tgt_mask tgt_is_causal seq_len mod layers output = mod output memory tgt_mask=tgt_mask memory_mask=memory_mask tgt_key_padding_mask=tgt_key_padding_mask memory_key_padding_mask=memory_key_padding_mask tgt_is_causal=tgt_is_causal memory_is_causal=memory_is_causal norm None output = norm output output TransformerEncoderLayer Module r TransformerEncoderLayer made up self-attn feedforward network This TransformerEncoderLayer implements original architecture described ` Attention Is All You Need https arxiv org abs ` _ paper The intent layer reference implementation foundational understanding thus contains only limited features relative newer Transformer architectures Given fast pace innovation transformer-like architectures we recommend exploring ` tutorial https pytorch org tutorials intermediate transformer_building_blocks html ` _ build efficient layers building blocks core using higher level libraries ` PyTorch Ecosystem https landscape pytorch org ` _ TransformerEncoderLayer can handle either traditional torch tensor inputs Nested Tensor inputs Derived classes expected similarly accept both input formats Not all combinations inputs currently supported TransformerEncoderLayer while Nested Tensor prototype state If you implementing custom layer you may derive either Module TransformerEncoderLayer If your custom layer supports both torch Tensors Nested Tensors inputs make its implementation derived TransformerEncoderLayer If your custom Layer supports only torch Tensor inputs derive its implementation Module Args d_model number expected features input required nhead number heads multiheadattention models required dim_feedforward dimension feedforward network model default= dropout dropout value default= activation activation function intermediate layer can string relu gelu unary callable Default relu layer_norm_eps eps value layer normalization components default= e- batch_first If ` ` True ` ` then input output tensors provided batch seq feature Default ` ` False ` ` seq batch feature norm_first ` ` True ` ` layer norm done prior attention feedforward operations respectively Otherwise s done after Default ` ` False ` ` after bias If set ` ` False ` ` ` ` Linear ` ` ` ` LayerNorm ` ` layers will learn additive bias Default ` ` True ` ` Examples encoder_layer = nn TransformerEncoderLayer d_model= nhead= src = torch rand out = encoder_layer src Alternatively when ` ` batch_first ` ` ` ` True ` ` encoder_layer = nn TransformerEncoderLayer d_model= nhead= batch_first=True src = torch rand out = encoder_layer src Fast path forward will use special optimized implementation described ` FlashAttention Fast Memory-Efficient Exact Attention IO-Awareness ` _ all following conditions met - Either autograd disabled using ` ` torch inference_mode ` ` ` ` torch no_grad ` ` no tensor argument ` ` requires_grad ` ` - training disabled using ` ` eval ` ` - batch_first ` ` True ` ` input batched i e ` ` src dim == ` ` - activation one ` ` relu ` ` ` ` gelu ` ` ` ` torch functional relu ` ` ` ` torch functional gelu ` ` - most one ` ` src_mask ` ` ` ` src_key_padding_mask ` ` passed - src ` NestedTensor https pytorch org docs stable nested html ` _ neither ` ` src_mask ` ` nor ` ` src_key_padding_mask ` ` passed - two ` ` LayerNorm ` ` instances have consistent ` ` eps ` ` value will naturally case unless caller has manually modified one without modifying other If optimized implementation use ` NestedTensor https pytorch org docs stable nested html ` _ can passed ` ` src ` ` represent padding more efficiently than using padding mask In case ` NestedTensor https pytorch org docs stable nested html ` _ will returned additional speedup proportional fraction input padding can expected _ ` FlashAttention Fast Memory-Efficient Exact Attention IO-Awareness ` https arxiv org abs __constants__ = norm_first __init__ d_model int nhead int dim_feedforward int = dropout float = activation Union str Callable Tensor Tensor = F relu layer_norm_eps float = e- batch_first bool = False norm_first bool = False bias bool = True device=None dtype=None - None factory_kwargs = device device dtype dtype super __init__ self_attn = MultiheadAttention d_model nhead dropout=dropout bias=bias batch_first=batch_first factory_kwargs Implementation Feedforward model linear = Linear d_model dim_feedforward bias=bias factory_kwargs dropout = Dropout dropout linear = Linear dim_feedforward d_model bias=bias factory_kwargs norm_first = norm_first pyrefly ignore bad-argument-type norm = LayerNorm d_model eps=layer_norm_eps bias=bias factory_kwargs pyrefly ignore bad-argument-type norm = LayerNorm d_model eps=layer_norm_eps bias=bias factory_kwargs dropout = Dropout dropout dropout = Dropout dropout Legacy string support activation function isinstance activation str activation = _get_activation_fn activation We can t test activation forward TorchScript so stash some information about instead activation F relu isinstance activation torch nn ReLU activation_relu_or_gelu = activation F gelu isinstance activation torch nn GELU activation_relu_or_gelu = activation_relu_or_gelu = activation = activation __setstate__ state super __setstate__ state hasattr activation activation = F relu forward src Tensor src_mask Optional Tensor = None src_key_padding_mask Optional Tensor = None is_causal bool = False - Tensor r Pass input through encoder layer Args src sequence encoder layer required src_mask mask src sequence optional src_key_padding_mask mask src keys per batch optional is_causal If specified applies causal mask ` ` src mask ` ` Default ` ` False ` ` Warning ` ` is_causal ` ` provides hint ` ` src_mask ` ` causal mask Providing incorrect hints can result incorrect execution including forward backward compatibility Shape see docs ` ~torch nn Transformer ` src_key_padding_mask = F _canonical_mask mask=src_key_padding_mask mask_name= src_key_padding_mask other_type=F _none_or_dtype src_mask other_name= src_mask target_type=src dtype src_mask = F _canonical_mask mask=src_mask mask_name= src_mask other_type=None other_name= target_type=src dtype check_other=False is_fastpath_enabled = torch backends mha get_fastpath_enabled why_not_sparsity_fast_path = is_fastpath_enabled why_not_sparsity_fast_path = torch backends mha get_fastpath_enabled True src dim = why_not_sparsity_fast_path = f input batched expected src dim got src dim training why_not_sparsity_fast_path = training enabled self_attn batch_first why_not_sparsity_fast_path = self_attn batch_first True self_attn in_proj_bias None why_not_sparsity_fast_path = self_attn passed bias=False self_attn _qkv_same_embed_dim why_not_sparsity_fast_path = self_attn _qkv_same_embed_dim True activation_relu_or_gelu why_not_sparsity_fast_path = activation_relu_or_gelu True norm eps = norm eps why_not_sparsity_fast_path = norm eps equal norm eps src is_nested src_key_padding_mask None src_mask None why_not_sparsity_fast_path = neither src_key_padding_mask nor src_mask supported NestedTensor input self_attn num_heads == why_not_sparsity_fast_path = num_head odd torch is_autocast_enabled why_not_sparsity_fast_path = autocast enabled any len getattr m _forward_hooks + len getattr m _forward_pre_hooks m modules why_not_sparsity_fast_path = forward pre- hooks attached module why_not_sparsity_fast_path tensor_args = src self_attn in_proj_weight self_attn in_proj_bias self_attn out_proj weight self_attn out_proj bias norm weight norm bias norm weight norm bias linear weight linear bias linear weight linear bias We have use list comprehensions below because TorchScript does support generator expressions _supported_device_type = cpu cuda torch utils backend_registration _privateuse _backend_name torch overrides has_torch_function tensor_args why_not_sparsity_fast_path = some Tensor argument has_torch_function all x device type _supported_device_type x tensor_args why_not_sparsity_fast_path = some Tensor argument s device neither one f _supported_device_type torch is_grad_enabled any x requires_grad x tensor_args why_not_sparsity_fast_path = grad enabled least one query input output projection weights biases requires_grad why_not_sparsity_fast_path merged_mask mask_type = self_attn merge_masks src_mask src_key_padding_mask src torch _transformer_encoder_layer_fwd src self_attn embed_dim self_attn num_heads self_attn in_proj_weight self_attn in_proj_bias self_attn out_proj weight self_attn out_proj bias activation_relu_or_gelu == norm_first norm eps norm weight norm bias norm weight norm bias linear weight linear bias linear weight linear bias merged_mask mask_type see Fig https arxiv org pdf v pdf x = src norm_first x = x + _sa_block norm x src_mask src_key_padding_mask is_causal=is_causal x = x + _ff_block norm x x = norm x + _sa_block x src_mask src_key_padding_mask is_causal=is_causal x = norm x + _ff_block x x self-attention block _sa_block x Tensor attn_mask Optional Tensor key_padding_mask Optional Tensor is_causal bool = False - Tensor x = self_attn x x x attn_mask=attn_mask key_padding_mask=key_padding_mask need_weights=False is_causal=is_causal dropout x feed forward block _ff_block x Tensor - Tensor x = linear dropout activation linear x dropout x TransformerDecoderLayer Module r TransformerDecoderLayer made up self-attn multi-head-attn feedforward network This TransformerDecoderLayer implements original architecture described ` Attention Is All You Need https arxiv org abs ` _ paper The intent layer reference implementation foundational understanding thus contains only limited features relative newer Transformer architectures Given fast pace innovation transformer-like architectures we recommend exploring ` tutorial https pytorch org tutorials intermediate transformer_building_blocks html ` _ build efficient layers building blocks core using higher level libraries ` PyTorch Ecosystem https landscape pytorch org ` _ Args d_model number expected features input required nhead number heads multiheadattention models required dim_feedforward dimension feedforward network model default= dropout dropout value default= activation activation function intermediate layer can string relu gelu unary callable Default relu layer_norm_eps eps value layer normalization components default= e- batch_first If ` ` True ` ` then input output tensors provided batch seq feature Default ` ` False ` ` seq batch feature norm_first ` ` True ` ` layer norm done prior attention multihead attention feedforward operations respectively Otherwise s done after Default ` ` False ` ` after bias If set ` ` False ` ` ` ` Linear ` ` ` ` LayerNorm ` ` layers will learn additive bias Default ` ` True ` ` Examples decoder_layer = nn TransformerDecoderLayer d_model= nhead= memory = torch rand tgt = torch rand out = decoder_layer tgt memory Alternatively when ` ` batch_first ` ` ` ` True ` ` decoder_layer = nn TransformerDecoderLayer d_model= nhead= batch_first=True memory = torch rand tgt = torch rand out = decoder_layer tgt memory __constants__ = norm_first __init__ d_model int nhead int dim_feedforward int = dropout float = activation Union str Callable Tensor Tensor = F relu layer_norm_eps float = e- batch_first bool = False norm_first bool = False bias bool = True device=None dtype=None - None factory_kwargs = device device dtype dtype super __init__ self_attn = MultiheadAttention d_model nhead dropout=dropout batch_first=batch_first bias=bias factory_kwargs multihead_attn = MultiheadAttention d_model nhead dropout=dropout batch_first=batch_first bias=bias factory_kwargs Implementation Feedforward model linear = Linear d_model dim_feedforward bias=bias factory_kwargs dropout = Dropout dropout linear = Linear dim_feedforward d_model bias=bias factory_kwargs norm_first = norm_first pyrefly ignore bad-argument-type norm = LayerNorm d_model eps=layer_norm_eps bias=bias factory_kwargs pyrefly ignore bad-argument-type norm = LayerNorm d_model eps=layer_norm_eps bias=bias factory_kwargs pyrefly ignore bad-argument-type norm = LayerNorm d_model eps=layer_norm_eps bias=bias factory_kwargs dropout = Dropout dropout dropout = Dropout dropout dropout = Dropout dropout Legacy string support activation function isinstance activation str activation = _get_activation_fn activation activation = activation __setstate__ state activation state state activation = F relu super __setstate__ state forward tgt Tensor memory Tensor tgt_mask Optional Tensor = None memory_mask Optional Tensor = None tgt_key_padding_mask Optional Tensor = None memory_key_padding_mask Optional Tensor = None tgt_is_causal bool = False memory_is_causal bool = False - Tensor r Pass inputs mask through decoder layer Args tgt sequence decoder layer required memory sequence last layer encoder required tgt_mask mask tgt sequence optional memory_mask mask memory sequence optional tgt_key_padding_mask mask tgt keys per batch optional memory_key_padding_mask mask memory keys per batch optional tgt_is_causal If specified applies causal mask ` ` tgt mask ` ` Default ` ` False ` ` Warning ` ` tgt_is_causal ` ` provides hint ` ` tgt_mask ` ` causal mask Providing incorrect hints can result incorrect execution including forward backward compatibility memory_is_causal If specified applies causal mask ` ` memory mask ` ` Default ` ` False ` ` Warning ` ` memory_is_causal ` ` provides hint ` ` memory_mask ` ` causal mask Providing incorrect hints can result incorrect execution including forward backward compatibility Shape see docs ` ~torch nn Transformer ` see Fig https arxiv org pdf v pdf x = tgt norm_first x = x + _sa_block norm x tgt_mask tgt_key_padding_mask tgt_is_causal x = x + _mha_block norm x memory memory_mask memory_key_padding_mask memory_is_causal x = x + _ff_block norm x x = norm x + _sa_block x tgt_mask tgt_key_padding_mask tgt_is_causal x = norm x + _mha_block x memory memory_mask memory_key_padding_mask memory_is_causal x = norm x + _ff_block x x self-attention block _sa_block x Tensor attn_mask Optional Tensor key_padding_mask Optional Tensor is_causal bool = False - Tensor x = self_attn x x x attn_mask=attn_mask key_padding_mask=key_padding_mask is_causal=is_causal need_weights=False dropout x multihead attention block _mha_block x Tensor mem Tensor attn_mask Optional Tensor key_padding_mask Optional Tensor is_causal bool = False - Tensor x = multihead_attn x mem mem attn_mask=attn_mask key_padding_mask=key_padding_mask is_causal=is_causal need_weights=False dropout x feed forward block _ff_block x Tensor - Tensor x = linear dropout activation linear x dropout x _get_clones module N FIXME copy deepcopy defined nn module ModuleList copy deepcopy module i range N _get_activation_fn activation str - Callable Tensor Tensor activation == relu F relu activation == gelu F gelu raise RuntimeError f activation should relu gelu activation _detect_is_causal_mask mask Optional Tensor is_causal Optional bool = None size Optional int = None - bool Return whether given attention mask causal Warning If ` ` is_causal ` ` ` ` None ` ` its value will returned If user supplies incorrect ` ` is_causal ` ` hint ` ` is_causal=False ` ` when mask fact causal attention mask may lead reduced performance relative what would achievable ` ` is_causal=True ` ` ` ` is_causal=True ` ` when mask fact causal attention mask may lead incorrect unpredictable execution - some scenarios causal mask may applied based hint other execution scenarios specified mask may used The choice may appear deterministic number factors like alignment hardware SKU etc influence decision whether use mask rely hint ` ` size ` ` None check whether mask causal mask provided size Otherwise checks any causal mask Prevent type refinement make_causal = is_causal True is_causal None mask None sz = size size None mask size - causal_comparison = _generate_square_subsequent_mask sz device=mask device dtype=mask dtype Do use ` torch equal ` so we handle batched masks broadcasting comparison mask size == causal_comparison size make_causal = bool mask == causal_comparison all make_causal = False make_causal