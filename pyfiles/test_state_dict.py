Owner s oncall distributed copy functools sys collections abc Callable itertools chain product typing Union torch torch distributed dist torch nn nn torch distributed _composable replicate torch distributed _shard sharded_tensor ShardedTensor torch distributed algorithms _checkpoint checkpoint_wrapper apply_activation_checkpointing torch distributed checkpoint state_dict ptd_state_dict torch distributed checkpoint state_dict _patch_model_state_dict _patch_optimizer_state_dict get_model_state_dict get_optimizer_state_dict get_state_dict set_model_state_dict set_optimizer_state_dict StateDictOptions torch distributed device_mesh init_device_mesh torch distributed fsdp fully_shard FullyShardedDataParallel FSDP ShardingStrategy StateDictType torch distributed fsdp wrap ModuleWrapPolicy torch distributed optim _apply_optimizer_in_backward torch distributed tensor DTensor torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel torch nn parallel DistributedDataParallel DDP torch optim Optimizer torch testing _internal common_dist_composable CompositeParamModel UnitModule torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN torch testing _internal distributed _tensor common_dtensor DTensorTestBase MultiProcessTestCase with_comms torch testing _internal distributed common_state_dict FusionEmbedding FusionEmbeddingWithHook FusionEmbeddingWithModifier VerifyStateDictMixin torch utils _pytree tree_all tree_all_only device_type = acc type acc = torch accelerator current_accelerator cpu dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit TestStateDict DTensorTestBase VerifyStateDictMixin Tests state_dict load_state_dict property world_size - int min torch accelerator device_count _test_save_load init_model_optim Callable test_frozen bool = False flatten_optimizer bool = False - None options = StateDictOptions ignore_frozen_params=test_frozen flatten_optimizer_state_dict=flatten_optimizer Initialize original model distributed model model optim copy_optim dist_model dist_optim = init_model_optim Train steps _dist_optim = dist_optim isinstance dist_optim list dist_optim _ range optim zero_grad d_optim _dist_optim d_optim zero_grad batch = torch rand device=device_type model batch sum backward dist_model batch sum backward optim step d_optim _dist_optim d_optim step We need ensure gradients don t exist invariant using DSD optim zero_grad Get state_dict compare result msd = model state_dict osd = optim state_dict dist_msd dist_osd = get_state_dict dist_model optimizers=dist_optim options=options _verify_msd msd dist_msd options _verify_osd_by_load model optim copy_optim dist_osd flatten_optimizer _verify_osd model optim osd dist_osd Initialize completely new model simulate checkpoint load _ _ _ dist_model dist_optim = init_model_optim Simulate DCP distributed load We need first get state_dict pass them DCP load saved state_dict storage Then finally we can call set_state_dict isinstance dist_optim list dist_optim = dist_optim test_frozen We won t able load partial state_dict back Since we already have state_dict saved before no need call DCP We can directly load them back This assert ensure optimizer state storage initialized assertEqual len curr_dist_osd STATE len dist_osd STATE set_model_state_dict dist_model model_state_dict=dist_msd options=options set_optimizer_state_dict dist_model optimizers=dist_optim optim_state_dict=dist_osd options=options Check new state_dict same dist_msd dist_osd = get_state_dict dist_model optimizers=dist_optim options=options _verify_msd msd dist_msd options TODO Ditto _verify_osd_by_load model optim copy_optim dist_osd flatten_optimizer _verify_osd model optim osd dist_osd Test _patch_model_state_dict _patch_optimizer_state_dict _patch_model_state_dict dist_model options=options _patch_optimizer_state_dict dist_model optimizers=dist_optim options=options dist_msd = dist_model state_dict dist_osd = dist_optim state_dict _verify_msd msd dist_msd options _verify_osd_by_load model optim copy_optim dist_osd flatten_optimizer _verify_osd model optim osd dist_osd _test_fsdp use_orig_params bool use_dtensor bool wrapping tuple nn Module = compile_model bool = False optimizer_class type Optimizer - None use_orig_params TODO remove after we complete composable API side change device_mesh use_dtensor init_model_optim use_dtensor device_mesh = init_device_mesh device_type world_size orig_model = CompositeParamModel device=torch device device_type orig_optim = optimizer_class orig_model parameters lr= e- foreach=True copy_optim = optimizer_class orig_model parameters lr= e- foreach=True wrapping strategy = set wrapping strategy = UnitModule use_dtensor device_mesh = init_device_mesh device_type world_size dist_model = FSDP copy deepcopy orig_model auto_wrap_policy=ModuleWrapPolicy strategy use_orig_params=use_orig_params device_mesh=device_mesh dist_model = FSDP copy deepcopy orig_model auto_wrap_policy=ModuleWrapPolicy strategy use_orig_params=use_orig_params compile_model dist_model = torch compile dist_model dist_optim = optimizer_class dist_model parameters lr= e- foreach=True orig_model orig_optim copy_optim dist_model dist_optim _test_save_load init_model_optim with_comms skip_if_lt_x_gpu test_fsdp - None run_subtests use_orig_params True False use_dtensor True False wrapping nn Linear UnitModule optimizer_class torch optim Adam torch optim AdamW torch optim SGD _test_fsdp with_comms skip_if_lt_x_gpu test_compiled_fsdp - None run_subtests use_orig_params True use_dtensor False wrapping optimizer_class torch optim Adam torch optim AdamW _test_fsdp _test_fsdp reshard_after_forward Union bool int optimizer_class type Optimizer compile_model bool foreach bool = True init_model_optim orig_model = CompositeParamModel device=torch device device_type orig_optim = optimizer_class orig_model parameters lr= e- foreach=foreach copy_optim = optimizer_class orig_model parameters lr= e- foreach=foreach dist_model = fully_shard copy deepcopy orig_model reshard_after_forward=reshard_after_forward compile_model dist_model = torch compile dist_model dist_optim = optimizer_class dist_model parameters lr= e- foreach=foreach orig_model orig_optim copy_optim dist_model dist_optim _test_save_load init_model_optim with_comms skip_if_lt_x_gpu test_fsdp - None run_subtests reshard_after_forward True False optimizer_class torch optim Adam torch optim AdamW compile_model True False _test_fsdp _test_ddp use_composable bool optimizer_class type Optimizer - None init_model_optim orig_model = CompositeParamModel device=torch device device_type orig_optim = optimizer_class orig_model parameters lr= e- copy_optim = optimizer_class orig_model parameters lr= e- use_composable dist_model = replicate copy deepcopy orig_model dist_model = DDP copy deepcopy orig_model dist_optim = optimizer_class dist_model parameters lr= e- orig_model orig_optim copy_optim dist_model dist_optim _test_save_load init_model_optim with_comms skip_if_lt_x_gpu test_ddp - None run_subtests use_composable True False optimizer_class torch optim Adam torch optim AdamW torch optim SGD _test_ddp _test_fsdp_ddp optimizer_class type Optimizer optim_in_backward bool = False test_frozen bool = False - None init_model_optim orig_model = CompositeParamModel device=torch device device_type test_frozen param chain orig_model u parameters orig_model u parameters param requires_grad = False orig_optim = optimizer_class orig_model parameters lr= e- copy_optim = optimizer_class orig_model parameters lr= e- dist_model = copy deepcopy orig_model dist_model l = DDP dist_model l dist_model = FSDP copy deepcopy orig_model auto_wrap_policy=ModuleWrapPolicy UnitModule use_orig_params=optim_in_backward ignored_modules= dist_model l optim_in_backward _apply_optimizer_in_backward optimizer_class dist_model parameters lr e- dist_optim = p _in_backward_optimizers p dist_model parameters dist_optim = optimizer_class dist_model parameters lr= e- orig_model orig_optim copy_optim dist_model dist_optim _test_save_load init_model_optim test_frozen with_comms skip_if_lt_x_gpu test_fsdp_ddp - None run_subtests optimizer_class torch optim Adam torch optim AdamW _test_fsdp_ddp _test_single_gpu optimizer_class type Optimizer - None init_model_optim orig_model = CompositeParamModel device=torch device device_type orig_optim = optimizer_class orig_model parameters lr= e- copy_optim = optimizer_class orig_model parameters lr= e- model_copy = copy deepcopy orig_model optim_copy = optimizer_class model_copy parameters lr= e- orig_model orig_optim copy_optim model_copy optim_copy _test_save_load init_model_optim skip_if_lt_x_gpu test_single_gpu - None _test_single_gpu torch optim Adam _test_single_gpu torch optim AdamW _test_strict parallelism str - None model = CompositeParamModel device=torch device device_type parallelism == DDP model = DDP model model = fully_shard model model_state_dict = get_model_state_dict model model_state_dict abc = torch zeros assertRaisesRegex RuntimeError Unexpected key set_model_state_dict model model_state_dict=model_state_dict key_iter = iter model_state_dict keys key key_iter key = abc break model_state_dict pop key incompatible_keys = set_model_state_dict model model_state_dict=model_state_dict options=StateDictOptions strict=False assertEqual incompatible_keys missing_keys key assertEqual incompatible_keys unexpected_keys abc model_state_dict pop abc assertRaisesRegex RuntimeError Missing key set_model_state_dict model model_state_dict=model_state_dict with_comms skip_if_lt_x_gpu test_strict - None run_subtests parallelism DDP fully_shard _test_strict _test_cpu_offload_full_state_dict optimizer_class type Optimizer - None orig_model = CompositeParamModel device=torch device device_type device_mesh = init_device_mesh device_type world_size dist_model = FSDP copy deepcopy orig_model auto_wrap_policy=ModuleWrapPolicy UnitModule use_orig_params=True device_mesh=device_mesh dist_optim = optimizer_class dist_model parameters lr= e- mst ost = get_state_dict dist_model dist_optim options=StateDictOptions cpu_offload=True cpu_device = torch device cpu is_cpu v isinstance v DTensor v device == cpu_device isinstance v ShardedTensor shards = v local_shards shards True shards tensor device == cpu_device v device == cpu_device assertTrue tree_all_only torch Tensor DTensor ShardedTensor is_cpu mst assertTrue tree_all_only torch Tensor DTensor ShardedTensor is_cpu ost mst ost = get_state_dict dist_model dist_optim options=StateDictOptions full_state_dict=True assertTrue tree_all lambda v isinstance v DTensor ShardedTensor mst assertTrue tree_all lambda v isinstance v DTensor ShardedTensor ost mst ost = get_state_dict dist_model dist_optim options=StateDictOptions full_state_dict=True cpu_offload=True rank == assertTrue tree_all_only torch Tensor DTensor ShardedTensor is_cpu mst assertTrue tree_all_only torch Tensor DTensor ShardedTensor is_cpu ost assertEqual mst assertEqual ost with_comms skip_if_lt_x_gpu test_cpu_offload_full_state_dict - None run_subtests optimizer_class torch optim Adam torch optim AdamW _test_cpu_offload_full_state_dict with_comms skip_if_lt_x_gpu test_activation_ckpt_fqns_ddp - None Tests activation checkpointing prefixes removed module names model = CompositeParamModel device=torch device device_type original_keys = get_model_state_dict model keys apply_activation_checkpointing model model = DDP model new_keys = get_model_state_dict model keys assertEqual original_keys new_keys with_comms skip_if_lt_x_gpu test_activation_ckpt_fqns_fsdp - None run_subtests use_orig_params True False _test_activation_ckpt_fqns_fsdp _test_activation_ckpt_fqns_fsdp use_orig_params bool - None Tests activation checkpointing prefixes removed module names model = CompositeParamModel device=torch device device_type original_keys = get_model_state_dict model keys apply_activation_checkpointing model model = FSDP model use_orig_params=use_orig_params new_keys = get_model_state_dict model keys assertEqual original_keys new_keys skip_if_lt_x_gpu test_extra_state - None model = CompositeParamModel device=torch device device_type get_extra_state MyState set_extra_state state UnitModule get_extra_state = get_extra_state UnitModule set_extra_state = set_extra_state target_model = copy deepcopy model set_model_state_dict target_model get_model_state_dict target_model assertEqual model state_dict u _extra_state MyState assertEqual model state_dict get_model_state_dict target_model skip_if_lt_x_gpu test_non_persistent_buffers - None model = CompositeParamModel device=torch device device_type model register_buffer dont_save_me torch rand device=device_type persistent=False target_model = copy deepcopy model set_model_state_dict target_model get_model_state_dict target_model assertEqual model state_dict get_model_state_dict target_model _test_broadcast_from_rank wrapper - None model = CompositeParamModel device=torch device device_type optim = torch optim Adam model parameters fsdp_model = wrapper copy deepcopy model fsdp_optim = torch optim Adam fsdp_model parameters batch = torch rand device=device_type model batch sum backward optim step states optim_states = get_state_dict model optim fsdp_model batch sum backward fsdp_optim step check equal fsdp_states = get_model_state_dict fsdp_model options=StateDictOptions full_state_dict=True fsdp_optim_states = get_optimizer_state_dict fsdp_model fsdp_optim options=StateDictOptions full_state_dict=True equal assertEqual states fsdp_states assertEqual optim_states fsdp_optim_states assertNotEqual states fsdp_states assertNotEqual optim_states fsdp_optim_states check equal=True fsdp_model batch sum backward fsdp_optim step check equal=False Drop states simulate loading rank dist get_rank load_states = load_states = load_optim_states = load_states = copy deepcopy states load_states = copy deepcopy states load_optim_states = copy deepcopy optim_states set_model_state_dict fsdp_model model_state_dict=load_states options=StateDictOptions broadcast_from_rank =True full_state_dict=True set_optimizer_state_dict fsdp_model fsdp_optim optim_state_dict=load_optim_states options=StateDictOptions broadcast_from_rank =True full_state_dict=True check equal=True Verify ` strict ` flag load_states = load_states load_states key = next iter load_states keys load_states pop key assertRaisesRegex RuntimeError Missing key set_model_state_dict fsdp_model model_state_dict=load_states options=StateDictOptions broadcast_from_rank =True full_state_dict=True with_comms skip_if_lt_x_gpu test_broadcast_from_rank - None device_mesh = init_device_mesh device_type world_size hsdp_device_mesh = init_device_mesh device_type world_size run_subtests wrapper functools partial fully_shard mesh=device_mesh functools partial FSDP device_mesh=device_mesh functools partial FSDP device_mesh=hsdp_device_mesh sharding_strategy=ShardingStrategy HYBRID_SHARD _test_broadcast_from_rank with_comms skip_if_lt_x_gpu test_fsdp_root_not_initialized - None This test verifies FSDP root initialized we should still able get state_dict without errors because fsdp_model state_dict will trigger FSDP initialization device_mesh = init_device_mesh device_type world_size model = CompositeParamModel device=torch device device_type fsdp_model = FSDP copy deepcopy model device_mesh=device_mesh fsdp_optim = torch optim Adam fsdp_model parameters get_model_state_dict fsdp_model get_optimizer_state_dict fsdp_model fsdp_optim with_comms skip_if_lt_x_gpu test_optim_state_dict_param_matching - None This test verifies parameters between optim optim_state_dict initial_lr added optim_state_dict new optim We test whether initial_lr appear optim after set_optimizer_state_dict torch manual_seed model = nn Sequential nn Linear device=device_type bias=False _ range layer model fully_shard layer fully_shard model optim = torch optim Adam model parameters lr= e- torch optim lr_scheduler LambdaLR optim lr_lambda= lambda epoch epoch opt_state_dict = ptd_state_dict get_optimizer_state_dict model optim options=ptd_state_dict StateDictOptions full_state_dict=True cpu_offload=True dist get_rank == assertTrue initial_lr opt_state_dict param_groups optim = torch optim Adam model parameters lr= e- assertTrue initial_lr optim param_groups ptd_state_dict set_optimizer_state_dict model optim optim_state_dict=opt_state_dict options=ptd_state_dict StateDictOptions broadcast_from_rank =True full_state_dict=True dist get_rank == assertTrue initial_lr optim param_groups with_comms skip_if_lt_x_gpu test_flattened_osd - None Test flattened optimizer state dictionaries different combinations flatten_optimizer_state_dict flag saving loading This test verifies We can save optimizer state dict without flattening We can load optimizer state dict without flattening The resulting optimizer state equivalent regardless flattening options flatten_to_save flatten_to_load product True False repeat= device_mesh = init_device_mesh device_type world_size model = CompositeParamModel device=torch device device_type fsdp_model = fully_shard copy deepcopy model mesh=device_mesh fsdp_optim = torch optim AdamW fsdp_model parameters batch = torch rand device=device_type fsdp_model batch sum backward fsdp_optim step fsdp_optim zero_grad Get optimizer state dict without flattening option osd = get_optimizer_state_dict fsdp_model fsdp_optim options=StateDictOptions flatten_optimizer_state_dict=flatten_to_save Create new optimizer load state osd fsdp_optim = torch optim AdamW fsdp_model parameters set_optimizer_state_dict fsdp_model optimizers=fsdp_optim optim_state_dict=osd options=StateDictOptions flatten_optimizer_state_dict=flatten_to_load Verify loaded optimizer state matches original assertEqual fsdp_optim state_dict fsdp_optim state_dict _test_deprecate_partial - None model = CompositeParamModel device=torch device device_type model_state_dict = get_model_state_dict model model_state_dict = copy deepcopy model_state_dict assertWarnsRegex FutureWarning Getting submodules only model optim state_dict deprecated model_state_dict = get_model_state_dict model submodules= model l model_state_dict = copy deepcopy model_state_dict assertWarnsRegex FutureWarning Getting submodules only model optim state_dict deprecated model_state_dict = get_model_state_dict model submodules= model l options=StateDictOptions keep_submodule_prefixes=False model_state_dict = copy deepcopy model_state_dict assertEqual len model_state_dict assertEqual len model_state_dict key model_state_dict keys full_fqn = f l key value = model_state_dict full_fqn value = model_state_dict full_fqn value = model_state_dict key assertEqual value value assertEqual value value zeros_state_dict = k torch zeros_like v k v model_state_dict items model load_state_dict zeros_state_dict set_model_state_dict model model_state_dict=model_state_dict options=StateDictOptions strict=False assertEqual model l weight model_state_dict l weight assertEqual model l bias model_state_dict l bias model load_state_dict zeros_state_dict assertWarnsRegex FutureWarning Passing model_state_dict set_model_state_dict model model_state_dict= model l model_state_dict options=StateDictOptions strict=False assertEqual model l weight model_state_dict l weight assertEqual model l bias model_state_dict l bias _test_deprecate_fsdp_api - None device_mesh = init_device_mesh device_type world_size model = CompositeParamModel device=torch device device_type fsdp_model = FSDP copy deepcopy model device_mesh=device_mesh assertWarnsRegex FutureWarning r FSDP state_dict_type\ \ FSDP set_state_dict_type\ \ being deprecated FSDP state_dict_type fsdp_model StateDictType FULL_STATE_DICT fsdp_model state_dict assertRaisesRegex AssertionError FutureWarning triggered assertWarnsRegex FutureWarning r FSDP state_dict_type\ \ FSDP set_state_dict_type\ \ being deprecated get_model_state_dict model with_comms skip_if_lt_x_gpu test_deprecate_api - None _test_deprecate_partial _test_deprecate_fsdp_api with_comms skip_if_lt_x_gpu test_shared_weight TiedEmbeddingModel nn Module __init__ vocab_size embedding_dim super __init__ embedding = nn Embedding vocab_size embedding_dim decoder = nn Linear embedding_dim vocab_size decoder weight = embedding weight Tying weights forward input input = input torch int embedded = embedding input output = decoder embedded output init_model_optim device_mesh = init_device_mesh device_type world_size orig_model = TiedEmbeddingModel torch device device_type orig_optim = torch optim AdamW orig_model parameters lr= e- copy_optim = torch optim AdamW orig_model parameters lr= e- dist_model = FSDP copy deepcopy orig_model device_mesh=device_mesh dist_optim = torch optim AdamW dist_model parameters lr= e- orig_model orig_optim copy_optim dist_model dist_optim _test_save_load init_model_optim run_subtests init_model_optim init_model_optim flatten_optimizer True False _test_save_load with_comms skip_if_lt_x_gpu test_setting_meta_device_model - None This test verifies we can set model state dict meta device model torch manual_seed torch device meta meta_model = nn Sequential nn Linear bias=False _ range layer meta_model fully_shard layer fully_shard meta_model torch device cpu cpu_model = nn Sequential nn Linear bias=False _ range full_sd = cpu_model state_dict set_model_state_dict meta_model model_state_dict=full_sd options=StateDictOptions full_state_dict=True strict=False meta_model_state_dict = meta_model state_dict cpu_model_state_dict = get_model_state_dict cpu_model cpu_model_key cpu_model_value cpu_model_state_dict items meta_model_value = meta_model_state_dict cpu_model_key full_tensor device=cpu_model_value device assertEqual cpu_model_value meta_model_value with_comms skip_if_lt_x_gpu test_setting_meta_device_model_broadcasting_and_memory - None This test verifies we can set model state dict meta device model With correlated changes state_dict meta device model should accepted broadcasting get copied successfully torch manual_seed torch device meta meta_model = nn Sequential nn Linear bias=False _ range layer meta_model fully_shard layer fully_shard meta_model torch device cpu cpu_model = nn Sequential nn Linear bias=False _ range full_sd = cpu_model state_dict set_model_state_dict meta_model model_state_dict=full_sd options=StateDictOptions broadcast_from_rank =True full_state_dict=True strict=False meta_model_state_dict = meta_model state_dict cpu_model_state_dict = get_model_state_dict cpu_model cpu_model_key cpu_model_value cpu_model_state_dict items meta_model_value = meta_model_state_dict cpu_model_key full_tensor device=cpu_model_value device assertEqual cpu_model_value meta_model_value Memory allocated reserved lower due change _distribute_tensors view clone This test would fail view due higher memory cost memory_allocated = torch get_device_module device_type memory_allocated memory_reserved = torch get_device_module device_type memory_reserved assertTrue memory_allocated = assertTrue memory_reserved = with_comms skip_if_lt_x_gpu test_set_cpu_model_state_dict_broadcast_from_rank - None torch manual_seed model = nn Linear expected_state_dict = k v detach clone k v model state_dict items state_dict = expected_state_dict torch distributed get_rank == model _apply lambda t torch zeros_like t set_model_state_dict model state_dict options=StateDictOptions full_state_dict=True broadcast_from_rank =True actual_name tensor expected_name expected_tensor zip model state_dict items expected_state_dict items assert actual_name == expected_name torch testing assert_close tensor expected_tensor msg=expected_name with_comms skip_if_lt_x_gpu test_multi_device_load_model_state_dict - None torch manual_seed torch device meta meta_submodel = nn Linear bias=False torch device cpu cpu_submodel = nn Linear bias=False torch device device_type acc_submodel = nn Linear bias=False two_device_model_with_meta = nn Sequential meta_submodel acc_submodel two_device_model_without_meta = nn Sequential cpu_submodel acc_submodel torch device cpu model_to_set = nn Sequential nn Linear bias=False _ range full_sd = model_to_set state_dict set_model_state_dict two_device_model_with_meta model_state_dict=full_sd options=StateDictOptions broadcast_from_rank =True full_state_dict=True strict=False assertRaisesRegex ValueError Multiple devices found set_model_state_dict two_device_model_without_meta model_state_dict=full_sd options=StateDictOptions broadcast_from_rank =True full_state_dict=True strict=False with_comms skip_if_lt_x_gpu test_state_dict_with_hook_on_keys - None torch device meta metamodel = FusionEmbedding torch device device_type gpumodel = FusionEmbeddingWithHook gpumodel_state_dict = get_model_state_dict gpumodel assertRaisesRegex RuntimeError Missing key set_model_state_dict metamodel gpumodel_state_dict torch device meta metamodel_modified = FusionEmbeddingWithModifier set_model_state_dict metamodel_modified gpumodel_state_dict with_comms skip_if_lt_x_gpu test_multi_param_groups - None TestModel torch nn Module __init__ super __init__ fc = torch nn Linear fc = torch nn Linear forward x fc fc x device_mesh = init_device_mesh device_type world_size model = TestModel device_type parallelize_module model device_mesh fc ColwiseParallel use_local_output=False fc RowwiseParallel use_local_output=False _test_multi optim_kwargs full_state_dict broadcast_from_rank cpu_offload broadcast_from_rank full_state_dict optim = torch optim AdamW optim_kwargs optim zero_grad model torch randn device=device_type sum backward optim step optim zero_grad options = torch distributed checkpoint state_dict StateDictOptions full_state_dict=full_state_dict broadcast_from_rank =broadcast_from_rank cpu_offload=cpu_offload optim_state_dict = get_optimizer_state_dict model optim options=options new_optim = torch optim AdamW optim_kwargs set_optimizer_state_dict model new_optim optim_state_dict options=options assertEqual optim param_groups new_optim param_groups assertEqual optim state new_optim state _multi_optim_kwargs = params params model fc weight params model fc weight lr lr _multi_optim_kwargs_empty_pg = params params model fc weight model fc weight params lr empty pg group here lr run_subtests optim_kwargs _multi_optim_kwargs_empty_pg _multi_optim_kwargs full_state_dict False True broadcast_from_rank False True TODO cpu_offload will cause get_optimizer_state_dict complain tensors GPU cpu_offload False _test_multi TestNoComm MultiProcessTestCase setUp - None super setUp _spawn_processes skip_if_lt_x_gpu test_no_dist - None model = CompositeParamModel device=torch device device_type optim = torch optim AdamW model parameters lr= e- assertFalse dist is_initialized msd = get_model_state_dict model options=StateDictOptions full_state_dict=True cpu_offload=True v msd values assertFalse v is_cuda assertEqual model state_dict msd set_model_state_dict model model state_dict osd = get_optimizer_state_dict model optim options=StateDictOptions full_state_dict=True cpu_offload=True set_optimizer_state_dict model optim osd set_optimizer_state_dict model optim optim state_dict __name__ == __main__ run_tests