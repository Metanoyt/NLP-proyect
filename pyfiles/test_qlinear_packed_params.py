usr bin env python Owner s oncall mobile tempfile torch torch ao nn sparse quantized dynamic linear Linear torch testing _internal common_quantization skipIfNoFBGEMM skipIfNoQNNPACK torch testing _internal common_quantized override_cpu_allocator_for_qnnpack override_quantized_engine qengine_is_qnnpack torch testing _internal common_utils TestCase TestQlinearPackedParams TestCase qlinear_packed_params_test allow_non_zero_zero_points=False copied https pytorch org docs stable sparse html#csr-tensor-operations so row col block indices match example blocks scaled rows weight_fp = torch Tensor row_block_size = col_block_size = out_features = weight_fp shape scales = zero_points = i + allow_non_zero_zero_points i range out_features dtype = torch qint wide_weight_fp = torch zeros tile width Fbgemm wide_weight_fp = wide_weight_fp = wide_weight_fp = per_tensor_small = torch quantize_per_tensor weight_fp scales zero_points dtype True x + allow_non_zero_zero_points x per_channel_small = torch quantize_per_channel weight_fp torch Tensor scales torch Tensor zero_points torch int axis = dtype False x + i allow_non_zero_zero_points i x enumerate per_tensor_large = torch quantize_per_tensor wide_weight_fp scales zero_points dtype True x + allow_non_zero_zero_points x weight is_per_tensor_quantized expected_row_block_indices expected_col_block_indices expected_weights per_tensor_small per_channel_small per_tensor_large lin = Linear out_features=weight shape in_features=weight shape row_block_size=row_block_size col_block_size=col_block_size bias=True dtype=dtype bias = torch ones size= weight shape lin set_weight_bias weight bias row_block_size col_block_size serialized = lin _packed_params _packed_params __getstate__ _ version bias_ out_features_block_size_ in_features_block_size_ weight_scales_ weight_zero_points_ quantization_scheme_ row_block_indices_ col_block_indices_ weights_ output_channels_ input_channels_ = serialized Test Serialization assertEqual bias_ bias assertEqual out_features_block_size_ row_block_size assertEqual in_features_block_size_ col_block_size assertEqual weight_scales_ scales is_per_tensor_quantized scales assertEqual weight_zero_points_ zero_points is_per_tensor_quantized zero_points assertEqual quantization_scheme_ is_per_tensor_quantized assertEqual row_block_indices_ expected_row_block_indices assertEqual col_block_indices_ expected_col_block_indices assertEqual weights_ tolist v + v expected_weights weights serialized + assertEqual output_channels_ weight shape assertEqual input_channels_ weight shape Test Unpacking weights_ bias_ out_features_block_size_ in_features_block_size_ = lin _weight_bias assertEqual torch dequantize weights_ torch dequantize weight assertEqual bias_ bias assertEqual out_features_block_size_ row_block_size assertEqual in_features_block_size_ col_block_size Test Deserialization tempfile TemporaryFile file_buff torch save lin file_buff file_buff seek lin = torch load file_buff assertEqual lin _weight_bias lin _weight_bias Serialize - Deserialize - Serialize should match Serialize assertEqual serialized lin _packed_params _packed_params __getstate__ Test op output preserved serialize - deserialize qengine_is_qnnpack x = torch rand size= weight shape y = lin x y = lin x assertEqual y y skipIfNoFBGEMM test_qlinear_packed_params_fbgemm torch manual_seed override_quantized_engine fbgemm qlinear_packed_params_test allow_non_zero_zero_points=False skipIfNoQNNPACK test_qlinear_packed_params_qnnpack torch manual_seed override_quantized_engine qnnpack override_cpu_allocator_for_qnnpack qengine_is_qnnpack qlinear_packed_params_test allow_non_zero_zero_points=True test_qlinear_packed_params_fbgemm_qnnpack_cross_compatibility torch manual_seed weight_fp = torch Tensor row_block_size = col_block_size = out_features = weight_fp shape scales = zero_points = _ range out_features dtype = torch qint make_lin_get_state_weight_bias_and_save weight = torch quantize_per_tensor weight_fp scales zero_points dtype lin = Linear out_features=weight shape in_features=weight shape row_block_size=row_block_size col_block_size=col_block_size bias=True dtype=dtype bias = torch ones size= weight shape lin set_weight_bias weight bias row_block_size col_block_size state = lin _packed_params _packed_params __getstate__ weight_bias = lin _weight_bias file_buff = tempfile TemporaryFile torch save lin file_buff file_buff seek state weight_bias file_buff load_get_state_weight_bias f_b lin = torch load f_b state = lin _packed_params _packed_params __getstate__ weight_bias = lin _weight_bias f_b close state weight_bias packed_params_data_with_int _indices data_as_state_and_weight_bias st weight_bias = data_as_state_and_weight_bias s s = st s _updated = tuple row col block indices respectively v i = i = v torch int i v enumerate list s s _updated s weight_bias Test Fbgemm - Qnnpack override_quantized_engine fbgemm packed_params_data_ file_buff_ = make_lin_get_state_weight_bias_and_save override_quantized_engine qnnpack override_cpu_allocator_for_qnnpack qengine_is_qnnpack packed_params_data_ b = load_get_state_weight_bias file_buff_ assertEqual packed_params_data_with_int _indices packed_params_data_ packed_params_data_with_int _indices packed_params_data_ b Test Qnnpack - Fbgemm override_quantized_engine qnnpack override_cpu_allocator_for_qnnpack qengine_is_qnnpack packed_params_data_ file_buff_ = make_lin_get_state_weight_bias_and_save override_quantized_engine fbgemm packed_params_data_ b = load_get_state_weight_bias file_buff_ assertEqual packed_params_data_with_int _indices packed_params_data_ packed_params_data_with_int _indices packed_params_data_ b