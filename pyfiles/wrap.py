mypy allow-untyped-defs inspect itertools logging typing Any Optional torch torch utils _pytree pytree torch _higher_order_ops utils reenter_make_fx torch _logging warning_once torch _ops HigherOrderOperator torch fx GraphModule torch fx experimental proxy_tensor ProxyTorchDispatchMode track_tensor_tree torch types _dtype log = logging getLogger __name__ uid = itertools count Used testing HigherOrderOperator mechanism Wrap HigherOrderOperator __init__ - None super __init__ wrap __call__ func args kwargs Dynamo already traces body HigherOrderOp beforehand when so no need trace into torch _dynamo noqa F torch _dynamo disable disable wrapper result = func args kwargs result wrapper wrap = Wrap WrapWithSetGradEnabled HigherOrderOperator __init__ - None super __init__ wrap_with_set_grad_enabled __call__ enable_grad wrapped_func args kwargs Dynamo already traces body HigherOrderOp beforehand when so no need trace into torch _dynamo noqa F torch _dynamo disable disable wrapper prev = torch is_grad_enabled torch set_grad_enabled enable_grad res = wrapped_func args kwargs torch set_grad_enabled prev res wrapper wrap_with_set_grad_enabled = WrapWithSetGradEnabled WrapWithAutocast HigherOrderOperator __init__ super __init__ wrap_with_autocast __call__ device_type str dtype Optional _dtype enabled bool cache_enabled Optional bool wrapped_func args kwargs Dynamo already traces body HigherOrderOp beforehand when so no need trace into torch _dynamo noqa F torch _dynamo disable disable wrapper torch autocast device_type dtype enabled cache_enabled wrapped_func args kwargs wrapper wrap_with_autocast = WrapWithAutocast This HOP allows you bypass dynamo tracing wrapper function while still tracing inner function Takes two callables The first ` wrapper_fn ` accepts ` inner_fn ` returns callable same signature The second ` inner_fn ` itself Any extra args kwargs forwarded ` wrapper_fn inner_fn ` when executed DynamoBypassingWrapper HigherOrderOperator __init__ super __init__ dynamo_bypassing_wrapper __call__ wrapper_fn_or_key inner_fn args kwargs Dynamo already traces body HigherOrderOp beforehand when so no need trace into torch _dynamo noqa F torch _dynamo disable is_compiling = isinstance wrapper_fn_or_key str is_compiling assert isinstance inner_fn torch fx GraphModule wrapper_fn = inner_fn meta wrapper_fn_or_key wrapper_fn = wrapper_fn_or_key disable wrapper wrapper_fn inner_fn args kwargs wrapper dynamo_bypassing_wrapper = DynamoBypassingWrapper WrapActivationCheckpoint HigherOrderOperator This operator used wrap torch utils checkpoint This avoids TorchDynamo look into saved tensor hooks directly passes control AOT Autograd which ok tracing saved tensor hooks As result AOT tracing torch utils checkpoint code we have backward graph recomputed forward nodes However we might deprecate operator soon The difficulty arises functionalization rng ops Today there two different functionalization rng ops - one AOT autograd other Inductor And they difficult map each other The rng states also complicate pattern matching Inductor Due ease implementation we currently inclined towards functionalization Inductor level which means duplication recomputation done compiler pass partitioners See TagActivationCheckpoint more information __init__ - None super __init__ wrap_activation_checkpoint cacheable=False __call__ function args kwargs use_reentrant set False because op going traced And we ensure AOT Autograd traces through non reentrant version checkpointing torch fx traceback fx_traceback torch fx Interpreter kwargs use_reentrant = False kwargs preserve_rng_state = False Using interpreter allows preservation metadata through torch compile stack fx_traceback preserve_node_meta torch utils checkpoint checkpoint checkpoint Interpreter function run args kwargs wrap_activation_checkpoint = WrapActivationCheckpoint TagActivationCheckpoint HigherOrderOperator This operator supposed used only torch compile stack This accepts Fx graph module which needs checkpointed This operator adds recomputable tag nodes Fx graph should recomputed The goal Avoid using Dynamo trace through saved tensor hooks For selective checkpointing case let AOTAutograd trace through saved tensor hooks has special logic TorchDispatchMode override usual saved_tensor_hooks fn logic order tag nodes Rely partitioners actually duplicate nodes This sits well torch compile stack because time graph reaches partitioner inductor has already run its functionalization rng ops setting fixed seed each random op see ` replace_random_passes ` Therefore duplication nodes design respects rng states forward recomputed forward backward __init__ - None super __init__ tag_activation_checkpoint cacheable=False staticmethod divide_kwargs kwargs checkpoint fn can have mixed kwargs between checkpointed fn checkpoint fn itself For example gn x y z=None = torch matmul x y z None torch matmul z fn x y z torch cos checkpoint gn x y use_reentrant=False z=z In above case z belongs checkpointed function gn use_reentrant belongs checkpoint function This function splits kwargs into checkpoint_kwargs gmod_kwargs checkpointed_fn_kwargs We do sorting ensure same graph run run better debuggability It required correctness torch utils checkpoint checkpoint ckpt_signature = inspect signature checkpoint checkpoint_keys = set name ckpt_signature parameters name function args kwargs continue checkpoint_keys add name ` preserve_rng_state ` regular kwarg checkpoint_keys add preserve_rng_state checkpoint_kwargs = name kwargs name name kwargs keys name checkpoint_keys gmod_kwargs = name kwargs name name kwargs keys name checkpoint_keys checkpoint_kwargs gmod_kwargs staticmethod tag_nodes gmod is_sac torch utils checkpoint CheckpointPolicy unique_graph_id = next uid node gmod graph nodes node op call_function call_method call_module node meta ac_graph_id = unique_graph_id is_sac For selective checkpointing we will populate tag later _CachingTorchDispatchMode node meta recompute = None Under vanilla activation checkpointing all nodes should recomputed node meta recompute = CheckpointPolicy PREFER_RECOMPUTE gmod __call__ gmod args kwargs dispatch_key_set = torch _ops _compute_keyset args kwargs non_fallthrough_keys dispatch_key = dispatch_key_set highestPriorityTypeId dispatch_key == torch _C DispatchKey PreDispatch super __call__ gmod args kwargs tag_activation_checkpoint_impl gmod args kwargs tag_activation_checkpoint = TagActivationCheckpoint tag_activation_checkpoint_impl gmod args kwargs torch fx traceback fx_traceback torch fx Interpreter _checkpoint_context_fn gmod meta warning_once log Detected context_fn passed torch utils checkpoint under torch compile Please make sure checkpointed region does contain in-place ops e g torch relu_ use_reentrant set False because op going traced And we ensure AOT Autograd traces through non reentrant version checkpointing kwargs use_reentrant = False preserve_rng_state set False because we want prevent AOTAutograd tracing through ` torch random fork_rng ` op which supported yet under CUDA This doesn t mean we don t preserve RNG state Instead we will always preserve RNG state regardless flag doing RNG functionalization via ` replace_random_passes ` Inductor instead AOTAutograd kwargs preserve_rng_state = False kwargs context_fn = gmod meta _checkpoint_context_fn We first tag all nodes recompute graph then we undo recompute tag specific nodes _CachingTorchDispatchMode torch utils checkpoint py gmod = TagActivationCheckpoint tag_nodes gmod is_sac=True Using interpreter allows preservation metadata through torch compile stack fx_traceback preserve_node_meta torch utils checkpoint checkpoint checkpoint Interpreter gmod run args kwargs gmod = TagActivationCheckpoint tag_nodes gmod is_sac=False Using interpreter allows preservation metadata through torch compile stack TODO We want use same ` checkpoint Interpreter gmod run args kwargs ` here ` context_fn = None ` case depends in-place op support TorchDispatchMode + torch compile details in-place op issue run ` test_compile_selective_checkpoint_inplace_op ` unit test fx_traceback preserve_node_meta Interpreter gmod run args tag_activation_checkpoint py_impl ProxyTorchDispatchMode proxy_mode_key proxy_mode ProxyTorchDispatchMode gmod GraphModule args Any kwargs Any - tuple torch Tensor torch fx traceback fx_traceback torch fx Interpreter assert proxy_mode pre_dispatch post-dispatch mode should have inlined Autograd key example_out = tag_activation_checkpoint gmod args kwargs proxy_args = pytree tree_map proxy_mode tracer unwrap_proxy args type ignore union-attr proxy_kwargs = pytree tree_map proxy_mode tracer unwrap_proxy kwargs type ignore union-attr qualname = proxy_mode tracer get_fresh_qualname wrap_body type ignore union-attr TODO tmanlaibaatar don t we need flat_apply here Dynamo already traced gmod body without kwargs flat_args _ = pytree tree_flatten args fx_traceback preserve_node_meta gmod_aten = reenter_make_fx Interpreter gmod run flat_args gmod_aten meta _checkpoint_context_fn = gmod meta _checkpoint_context_fn proxy_mode tracer root register_module qualname gmod_aten type ignore union-attr proxy_gmod = proxy_mode tracer unwrap_proxy gmod_aten type ignore union-attr call-overload out_proxy = proxy_mode tracer create_proxy call_function tag_activation_checkpoint proxy_gmod proxy_args proxy_kwargs track_tensor_tree example_out out_proxy constant=None tracer=proxy_mode tracer