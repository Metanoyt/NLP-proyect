Owner s oncall distributed torch torch distributed dist torch distributed checkpoint dist_cp torch distributed checkpoint default_planner DefaultLoadPlanner DefaultSavePlanner torch distributed fsdp FullyShardedDataParallel FSDP torch distributed fsdp fully_sharded_data_parallel StateDictType torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor DTensorTestBase with_comms torch testing _internal distributed checkpoint_utils with_temp_dir FsdpModelStateCheckpoint DTensorTestBase property backend curr_backend = dist get_default_backend_for_device device_type f cpu gloo device_type curr_backend _test_fsdp_model_state process_group - None CHECKPOINT_DIR = temp_dir model = FSDP torch nn Linear device= meta model torch rand device=dist get_rank sum backward FSDP state_dict_type model StateDictType SHARDED_STATE_DICT state_dict = model model state_dict dist_cp save state_dict=state_dict storage_writer=dist_cp FileSystemWriter CHECKPOINT_DIR planner=DefaultSavePlanner model_ = FSDP torch nn Linear device= meta process_group=process_group FSDP summon_full_params model FSDP summon_full_params model_ assertNotEqual model weight model_ weight assertNotEqual model bias model_ bias now load model ensure values same FSDP state_dict_type model_ StateDictType SHARDED_STATE_DICT state_dict = model model_ state_dict dist_cp load state_dict=state_dict storage_reader=dist_cp FileSystemReader CHECKPOINT_DIR planner=DefaultLoadPlanner model_ load_state_dict state_dict model FSDP summon_full_params model FSDP summon_full_params model_ assertEqual model weight model_ weight assertEqual model bias model_ bias skip_if_lt_x_gpu with_comms with_temp_dir test_fsdp_model_state_no_resharding _test_fsdp_model_state process_group=None _create_new_dist_group world_size = dist get_world_size group = i i range world_size i == group = i i range world_size i = create new fsdp group resharding fsdp_ = dist new_group ranks=group fsdp_ = dist new_group ranks=group dist get_rank == my_fsdp = fsdp_ my_fsdp = fsdp_ my_fsdp skip_if_lt_x_gpu with_comms with_temp_dir test_fsdp_model_state_with_resharding _test_fsdp_model_state process_group=self _create_new_dist_group __name__ == __main__ run_tests