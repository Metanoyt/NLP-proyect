mypy allow-untyped-defs inspect logging collections OrderedDict collections abc Callable typing Any Optional torch torch fx _compatibility compatibility torch fx _utils lazy_format_graph_code torch fx graph_module GraphModule torch fx node Node __all__ = Partition split_module log = _LOGGER = logging getLogger __name__ compatibility is_backward_compatible=True Partition __init__ name str name str = name submod_name = f submod_ name node_names list str = inputs dict str None = outputs dict str None = dependencies dict str None = dependents dict str None = graph torch fx graph Graph = torch fx graph Graph environment dict Node Node = targets dict str Any = __repr__ - str f name name \n f nodes node_names \n f inputs inputs \n f outputs outputs \n f partitions depended dependencies \n f partition dependents dependents _get_attr_from_qualname mod torch nn Module qualname str - Any attr_val = mod atom qualname split type ignore union-attr hasattr attr_val atom raise AttributeError f Node target qualname found attr_val = getattr attr_val atom attr_val Creates subgraphs out main graph compatibility is_backward_compatible=True split_module m GraphModule root_m torch nn Module split_callback Callable Node int qualname_map Optional dict str str = None keep_original_order Optional bool = False keep_original_node_name Optional bool = False keep_original_input_name bool = True partition_affix Optional str = None Creates subgraphs out main graph Args m GraphModule Graph module split root_m torch nn Module root nn module Not currently used Included because root nn module usually transformed via torch fx _symbolic_trace symbolic_trace see example below split_callback Callable Node int Callable function maps given Node instance numeric partition identifier split_module will use function policy which operations appear which partitions output Module qualname_map Optional Dict str str optional output parameter returns mapping new target names module after split old target names original module keep_original_order Optional bool keep original order GraphModule use Topological order new constructed GraphModule keep_original_node_name Optional bool If partitioned graphs should have same node names original graph keep_original_input_name bool If partitioned graphs should have same input names original graph partition_affix Optional str If specified submodules names will contain affix e g submod_ affix _ idx Returns GraphModule module after split Example This sample setup torch torch fx _symbolic_trace symbolic_trace torch fx graph_module GraphModule torch fx node Node torch fx passes split_module split_module MyModule torch nn Module __init__ - None super __init__ param = torch nn Parameter torch rand linear = torch nn Linear forward x y z = linear x + param clamp min= max= w = linear y clamp min= max= z + w symbolically trace model my_module = MyModule my_module_traced = symbolic_trace my_module random mod partitioning partition_counter = NPARTITIONS = mod_partition node Node global partition_counter partition = partition_counter NPARTITIONS partition_counter = partition_counter + NPARTITIONS partition split module module submodules module_with_submodules = split_module my_module_traced my_module mod_partition Output looks like Original graph broken into partitions print module_with_submodules GraphModule submod_ GraphModule linear Linear in_features= out_features= bias=True submod_ GraphModule linear Linear in_features= out_features= bias=True submod_ GraphModule forward x y param = param submod_ = submod_ x param y x = param = y = None getitem = submod_ getitem_ = submod_ submod_ = None submod_ = submod_ getitem getitem_ getitem = getitem_ = None getitem_ = submod_ getitem_ = submod_ submod_ = None submod_ = submod_ getitem_ getitem_ getitem_ = getitem_ = None submod_ Output split module same output input traced module This example within test setting orig_out = my_module_traced x y submodules_out = module_with_submodules x y assertEqual orig_out submodules_out True log debug s lazy_format_graph_code pre split_module m colored=True construct_graph node Node base_mod_env dict str Node base_mod_attrs dict str torch fx graph_module GraphModule node op == placeholder default_value = node args len node args inspect Signature empty keep_original_node_name args = default_value inspect Signature empty default_value base_mod_env node name = base_mod_graph create_node placeholder node name args=args type ignore arg-type type_expr=node type base_mod_env node name = base_mod_graph placeholder node target type ignore arg-type type_expr=node type default_value=default_value base_mod_env node name meta = node meta copy node op == get_attr base_mod_env node name = base_mod_graph get_attr node target type ignore arg-type base_mod_env node name meta = node meta copy assert isinstance node target str attr_val = _get_attr_from_qualname m node target base_mod_attrs node target = attr_val type ignore index base_mod_env base_mod_attrs sympy partitions dict str Partition = orig_nodes dict str Node = symbol_to_node dict sympy Symbol Node = record_cross_partition_use def_node Node use_node Optional Node torch fx experimental symbolic_shapes free_symbols defined = getattr def_node _fx_partition None used = getattr use_node _fx_partition None log debug record_cross_partition_use s s s s def_node name defined use_node name use_node None - used defined = used defined None def_partition = partitions defined def_partition outputs setdefault def_node name used None def_partition dependents setdefault used used None use_partition = partitions used use_partition inputs setdefault def_node name We have made def_node input use_partition If input has symbolic symbols its size those also must made inputs partition def_val = def_node meta get example_value None s sorted free_symbols def_val key=str s_node = symbol_to_node s use_partition inputs setdefault s_node name symbol_to_node s op = placeholder If node defines symbol placeholder we must make output partition Note may different partition than defined Although doesn t really make difference correctness since defined guaranteed have symbol scope can you just get less optimal codegen case s_defined = getattr s_node _fx_partition None s_defined None s_def_partition = partitions s_defined s_def_partition outputs setdefault s_node name s_def_partition dependents setdefault used use_partition dependencies setdefault s_defined defined None use_partition dependencies setdefault defined instantiate_node_partition_mapping node partition_idx = split_callback node partition_name = str partition_idx partition_affix None For example user specifies partition_affix = pp then partition name will pp_ pp_ etc partition_name = _ join partition_affix partition_name log debug instantiate_node_partition_mapping s s node name partition_name add node partitions partition = partitions get partition_name partition None partitions partition_name = partition = Partition partition_name partition node_names append node name node _fx_partition = partition_name Global State Nodes nodes which their global state effects taint all downstream nodes while they active GLOBAL_STATE_NODES = torch amp _enter_autocast torch amp _exit_autocast torch _C _set_grad_enabled For grad regions ------------------------ first region we do nothing subsequent regions we insert set_grad beginning grad_regions OrderedDict Node set int = OrderedDict For autocast regions ------------------------ first region we will only insert _exit end intermediate regions we will insert both _enter beginning _exit end last region we will only insert _enter beginning We will do so order which autocasts instantiated autocast_regions OrderedDict Node set int = OrderedDict autocast_exits dict Node Optional Node = active_grad = None active_autocasts = set node m graph nodes This will prefer placeholder bindings because those come first This little dangerous though possible unbacked symbol used without any binding site which case we will get KeyError able find I d like fix having passes runtime_assert establish some invariants I can rely later needs some extra work Quick fix first See https github com pytorch pytorch issues val = node meta get example_value None isinstance val torch SymInt torch SymFloat isinstance s = val node expr sympy Symbol s symbol_to_node symbol_to_node val node expr = node node op placeholder get_attr output continue instantiate_node_partition_mapping node node op == call_function node target GLOBAL_STATE_NODES node target torch _C _set_grad_enabled assert len node args == assert isinstance node args bool active_grad = node grad_regions active_grad = set split_callback node node target torch amp _enter_autocast Should all python constants assert all isinstance arg Node arg node args active_autocasts add node autocast_regions node = set split_callback node autocast_exits node = None node target torch amp _exit_autocast assert len node args == autocast_regions node args add split_callback node active_autocasts remove node args autocast_exits node args = node active_grad None grad_regions active_grad add split_callback node active_autocasts autocast_regions add split_callback node assert all v None v autocast_exits values autocast must exit pyrefly ignore bad-assignment autocast_regions = k sorted v k v autocast_regions items pyrefly ignore bad-assignment grad_regions = k sorted v k v grad_regions items _LOGGER isEnabledFor logging DEBUG _LOGGER debug autocast_regions s autocast_regions _LOGGER debug grad_regions s grad_regions assert_monotonically_increasing = bool autocast_regions bool grad_regions split nodes into partitions highest_partition = - node m graph nodes orig_nodes node name = node TODO currently placeholders parameters aren t put into random partitions rather they re added graphs where they used down below node op placeholder get_attr continue node op == output torch fx graph map_arg node args lambda n record_cross_partition_use n None continue assert_monotonically_increasing pid = split_callback node assert highest_partition = pid autocast set_grad_enabled require monotonically increasing partitions f highest highest_partition node s pid highest_partition = pid do capture cross-partition dependencies global state nodes they will self-contained - their setup unwind will isolated each partition submodule node target GLOBAL_STATE_NODES torch fx graph map_arg node args lambda def_node record_cross_partition_use def_node node torch fx graph map_arg node kwargs lambda def_node record_cross_partition_use def_node node noqa B original_partition_order = list partitions keys find partitions no dependencies root_partitions list str = partition_name partition partitions items len partition dependencies root_partitions append partition_name check partitions circular dependencies create topological partition ordering sorted_partitions list str = while root_partitions root_partition = root_partitions pop sorted_partitions append root_partition dependent partitions root_partition dependents partitions dependent dependencies pop root_partition noqa B partitions dependent dependencies root_partitions append dependent len sorted_partitions = len partitions raise RuntimeError cycle exists between partitions Enter prelude regions_mapping autocast_regions grad_regions node regions regions_mapping items assert len regions pyrefly ignore index-error partitions str regions environment node = node pyrefly ignore index-error r regions partition = partitions str r new_node = partition graph create_node op=node op target=node target args=tuple arg arg node args kwargs= type_expr=node type new_node meta = node meta copy really good idea copy partition environment node = new_node add placeholders partition inputs partition_name sorted_partitions partition = partitions partition_name new_inputs dict str None = counter = inp partition inputs orig_node = orig_nodes inp We don t pass get_attr nodes inputs partition instead set them targets use getattr within module add_placeholder keep_original_input_name name = inp nonlocal counter name = f arg_ counter counter += placeholder = partition graph placeholder name type_expr=orig_nodes inp type new_inputs inp = None placeholder orig_node op == get_attr assert isinstance orig_node target str orig_attr = _get_attr_from_qualname m orig_node target isinstance orig_attr torch nn Module placeholder = partition graph get_attr orig_node target partition targets orig_node target = orig_attr placeholder = add_placeholder placeholder = add_placeholder placeholder meta = orig_nodes inp meta copy partition environment orig_nodes inp = placeholder partition inputs = new_inputs Transform nodes collect targets partition s submodule node m graph nodes hasattr node _fx_partition partition = partitions node _fx_partition swap out old graph nodes kw args references new nodes submodule environment = partition environment gathered_args = torch fx graph map_arg node args lambda n environment n gathered_kwargs = torch fx graph map_arg node kwargs lambda n environment n node op call_module get_attr target = node target target_attr = _get_attr_from_qualname m node target target = node target replace _ partition targets target = target_attr Fill passed-in mapping new qualname old qualname qualname_map None When creating split module later submodules will have path prefix matching corresponding partition s submod_name qualname = f partition submod_name target qualname_map qualname = node target assert isinstance gathered_args tuple assert isinstance gathered_kwargs dict name = node name keep_original_node_name None new_node = partition graph create_node op=node op target=target args=gathered_args kwargs=gathered_kwargs type_expr=node type name=name new_node meta = node meta copy partition environment node = new_node Exit epilogue regions_mapping autocast_regions node reversed regions_mapping regions = regions_mapping node assert len regions pyrefly ignore index-error r regions - partition = partitions str r exit_node = autocast_exits node assert exit_node None Missing exit node new_node = partition graph create_node op=exit_node op target=exit_node target args= partition environment node kwargs= type_expr=exit_node type new_node meta = exit_node meta copy really good idea copy original module environment dict mapping node names nodes orig_mod_env dict str Node = Set up values construct base module base_mod_env dict str Node = base_mod_graph torch fx graph Graph = torch fx graph Graph base_mod_attrs dict str torch fx graph_module GraphModule = keep_original_order node m graph nodes base_mod_env base_mod_attrs = construct_graph node base_mod_env base_mod_attrs Go through graph construct mapping dict node m graph nodes orig_mod_env node name = node Do some things iterating over partitions topological order again Finish off submodule Graphs setting corresponding outputs Construct GraphModules each submodule Construct base graph emitting calls those submodules topological order original order specified keep_original_order construct_order_partitions = sorted_partitions keep_original_order original_partition_order already_constructed_attr_nodes = set We actually need insert placeholder nodes original order otherwise graph signature will wrong original_order = node node m graph nodes node op == placeholder partition_name construct_order_partitions partition = partitions partition_name Set correct output values output_vals = tuple partition environment orig_nodes name name partition outputs skip output node generation there no output values num_output_vals = len output_vals num_output_vals == partition graph output output_vals num_output_vals partition graph output output_vals Invariant - Graph should always have output node partition graph output keep_original_order first get attr nodes required partition orig_mod_attr_nodes list Node = orig_mod_env key key partition inputs key original_order node original_order node already_constructed_attr_nodes continue already added attr base graph base_mod_env _based_mod_attrs = construct_graph node base_mod_env base_mod_attrs already_constructed_attr_nodes add node Construct GraphModule partition node orig_mod_attr_nodes type ignore attr-defined node already_constructed_attr_nodes continue base_mod_env base_mod_attrs = construct_graph node base_mod_env base_mod_attrs already_constructed_attr_nodes add node base_mod_attrs partition submod_name = torch fx graph_module GraphModule partition targets partition graph noqa B Emit call base graph submodule output_val = base_mod_graph call_module partition submod_name tuple base_mod_env name name partition inputs num_outputs = len partition outputs num_outputs Unpack multiple values submodule output_val_proxy = torch fx proxy Proxy output_val i output_name enumerate partition outputs base_mod_env output_name = output_val_proxy i node type ignore index num_outputs == base_mod_env next iter partition outputs = output_val When keep_original_order=True graph doesn t have any ` call_function ` node then ` base_mod_graph ` ` base_mod_env ` ` base_mod_attrs ` never populated For case we call ` construct_graph ` here which takes care updating them keep_original_order base_mod_env node m graph nodes base_mod_env base_mod_attrs = construct_graph node base_mod_env base_mod_attrs Add output node ` base_mod_graph ` i e split graph which will returned node m graph nodes node op == output base_mod_graph output torch fx graph map_arg node args lambda n base_mod_env n name noqa B ret = torch fx graph_module GraphModule base_mod_attrs base_mod_graph log debug s lazy_format_graph_code post split_module ret colored=True ret