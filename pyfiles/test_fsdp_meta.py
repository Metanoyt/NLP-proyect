Owner s oncall distributed itertools sys typing Union torch torch distributed dist torch nn nn torch distributed fsdp FullyShardedDataParallel FSDP MixedPrecision torch distributed fsdp wrap always_wrap_policy always_wrap enable_wrap ModuleWrapPolicy wrap torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests skip_but_pass_in_sandcastle_if TEST_WITH_DEV_DBG_ASAN _TORCHDISTX_AVAIL = True try torchdistx deferred_init except ImportError _TORCHDISTX_AVAIL = False dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit device_type = acc type acc = torch accelerator current_accelerator cpu _reset_params_if_meta is_meta bool model nn Module For torchdistX init we don t need call reset_params deferred_init model materialize equivalent model is_meta module model modules Assume module has ` reset_parameters ` iff has directly managed parameters buffers hasattr module reset_parameters module reset_parameters MyLinear nn Linear Linear layer deterministic reset_parameters testing __init__ args kwargs super __init__ args kwargs reset_parameters args kwargs torch manual_seed torch no_grad Use initialization method depends shape torch nn init xavier_uniform_ weight MyBuffer nn Module __init__ device torch device super __init__ buf = torch nn Buffer torch empty device=device reset_parameters args kwargs torch manual_seed Use initialization method depends shape torch nn init xavier_uniform_ buf MyModel nn Module __init__ device torch device super __init__ lin = MyLinear bias=False device=device lin = MyLinear bias=False device=device buf_mod = MyBuffer device forward x lin lin x NestedModel nn Module __init__ device super __init__ lin = MyLinear bias=False device=device lin = wrap lin lin = MyLinear bias=False device=device l = MyModel device=device l = wrap l forward x l lin lin x _init_with_reset_params module nn Module to_empty + reset_parameters init function example modules initialized device= meta has_meta_states = any t is_meta t itertools chain module parameters recurse=False module buffers recurse=False has_meta_states device = torch device device_type torch accelerator current_device_index module to_empty device=device recurse=False module reset_parameters _init_with_torchdistX module nn Module torchdistX-based deferred module initialization function example using ` ` materialize_module ` ` assert _TORCHDISTX_AVAIL check_fn k isinstance k FSDP deferred_init materialize_module module check_fn=check_fn TestFSDPWithMetaDevice FSDPTest property world_size property process_group dist distributed_c d _get_default_group _compare_fsdp fsdp fsdp FSDP summon_full_params fsdp FSDP summon_full_params fsdp p p zip fsdp parameters fsdp parameters assertTrue torch allclose p p f p vs p _test_simple_model_with_meta_device meta_module_fn init_fn=None Create model meta device wrap FSDP model = meta_module_fn is_meta = next model parameters is_meta fsdp_meta = FSDP model auto_wrap_policy=always_wrap param_init_fn=init_fn meta_opt = torch optim SGD fsdp_meta parameters lr= e- Test make sure same model parameters regular FSDP approach regular = MyModel device=device_type _reset_params_if_meta is_meta regular fsdp_regular = FSDP regular auto_wrap_policy=always_wrap regular_opt = torch optim SGD fsdp_regular parameters lr= e- _compare_fsdp fsdp_meta fsdp_regular inp = torch randn device=device_type fsdp_meta inp sum backward fsdp_regular inp sum backward meta_opt step regular_opt step _compare_fsdp fsdp_meta fsdp_regular Test meta init works all submodules contained only single FSDP unit model = meta_module_fn fsdp_meta = FSDP model param_init_fn=init_fn meta_opt = torch optim SGD fsdp_meta parameters lr= e- regular = MyModel device=device_type _reset_params_if_meta is_meta regular fsdp_regular = FSDP regular auto_wrap_policy=always_wrap regular_opt = torch optim SGD fsdp_regular parameters lr= e- Run forward + backward pass + optimizer step fsdp_meta inp sum backward fsdp_regular inp sum backward meta_opt step regular_opt step _compare_fsdp fsdp_meta fsdp_regular skip_if_lt_x_gpu test_simple_model_with_meta_device_reset_params meta_module_fn MyModel device= meta _test_simple_model_with_meta_device meta_module_fn _init_with_reset_params skip_if_lt_x_gpu test_simple_model_with_meta_device_default_init meta_module_fn MyModel device= meta _test_simple_model_with_meta_device meta_module_fn skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if _TORCHDISTX_AVAIL Test requires torchdistX https github com pytorch torchdistX test_simple_model_with_torchdistX_default_init meta_module_fn deferred_init deferred_init MyModel device=device_type _test_simple_model_with_meta_device meta_module_fn skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if _TORCHDISTX_AVAIL Test requires torchdistX https github com pytorch torchdistX test_simple_model_with_torchdistX_init_fn meta_module_fn deferred_init deferred_init MyModel device=device_type _test_simple_model_with_meta_device meta_module_fn init_fn=_init_with_torchdistX _test_nested_model_with_meta_device auto_wrap meta_module_fn init_fn=None auto_wrap module = meta_module_fn is_meta = next module parameters is_meta next module buffers is_meta fsdp_meta = FSDP module auto_wrap_policy=always_wrap param_init_fn=init_fn meta_opt = torch optim SGD fsdp_meta parameters lr= e- module_regular = NestedModel device=device_type _reset_params_if_meta is_meta module_regular fsdp_regular = FSDP module_regular auto_wrap_policy=always_wrap regular_opt = torch optim SGD fsdp_regular parameters lr= e- enable_wrap wrapper_cls=FSDP param_init_fn=init_fn module = meta_module_fn is_meta = next module parameters is_meta Non FSDP modules will still initialized because they bubble up part larger FSDP unit fsdp_meta = wrap module meta_opt = torch optim SGD fsdp_meta parameters lr= e- Init reset parameters before wrapping so reset_params matches up meta device s initialization module_regular = NestedModel device=device_type _reset_params_if_meta is_meta module_regular enable_wrap wrapper_cls=FSDP module_regular lin = wrap module_regular lin module_regular l = wrap module_regular l fsdp_regular = wrap module_regular regular_opt = torch optim SGD fsdp_regular parameters lr= e- Compare before training _compare_fsdp fsdp_meta fsdp_regular inp = torch randn device=device_type fsdp_meta inp sum backward fsdp_regular inp sum backward meta_opt step regular_opt step _compare_fsdp fsdp_meta fsdp_regular skip_if_lt_x_gpu parametrize auto_wrap True False test_nested_model_with_meta_device_reset_params auto_wrap meta_module_fn NestedModel device= meta _test_nested_model_with_meta_device auto_wrap=auto_wrap meta_module_fn=meta_module_fn init_fn=_init_with_reset_params skip_if_lt_x_gpu parametrize auto_wrap True False test_nested_model_with_meta_device_default_init auto_wrap meta_module_fn NestedModel device= meta _test_nested_model_with_meta_device auto_wrap=auto_wrap meta_module_fn=meta_module_fn skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if _TORCHDISTX_AVAIL Test requires torchdistX https github com pytorch torchdistX parametrize auto_wrap True False test_nested_model_with_torchdistX_default_init auto_wrap meta_module_fn deferred_init deferred_init NestedModel device=device_type _test_nested_model_with_meta_device auto_wrap=auto_wrap meta_module_fn=meta_module_fn skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if _TORCHDISTX_AVAIL Test requires torchdistX https github com pytorch torchdistX parametrize auto_wrap True False test_nested_model_with_torchdistX_init_fn auto_wrap meta_module_fn deferred_init deferred_init NestedModel device=device_type _test_nested_model_with_meta_device auto_wrap=auto_wrap meta_module_fn=meta_module_fn init_fn=_init_with_torchdistX _test_bad_arg meta_module_fn mod = meta_module_fn assertRaisesRegex ValueError callable FSDP mod param_init_fn= skip_if_lt_x_gpu skip_but_pass_in_sandcastle_if _TORCHDISTX_AVAIL Test requires torchdistX https github com pytorch torchdistX test_bad_arg_torchdistx meta_module_fn deferred_init deferred_init NestedModel device_type _test_bad_arg meta_module_fn skip_if_lt_x_gpu test_bad_arg_meta meta_module_fn NestedModel device= meta _test_bad_arg meta_module_fn skip_if_lt_x_gpu test_meta_device_with_mixed_precision Tests meta device initialization ` ` param_init_fn ` ` when specifying mixed precision ` ` param_dtype=torch float ` ` FakeLinear nn Module __init__ in_dim int out_dim int device Union torch device str - None super __init__ weight = nn Parameter torch randn in_dim out_dim device=device forward x torch Tensor - torch Tensor x weight Model nn Module __init__ - None super __init__ lin = nn Linear device= meta lin = FakeLinear device= meta relu = nn ReLU forward x torch Tensor - torch Tensor lin relu lin x _module_init_fn module nn Module isinstance module nn Linear torch nn init normal_ module weight mean= std= module bias None torch nn init zeros_ module bias _param_init_fn module nn Module - None TODO ` module to_empty ` generally correct meta device initialization https github com pytorch pytorch issues module to_empty device=torch device device_type module apply model _module_init_fn model = Model Wrap ` lin ` top level ` model ` create nested FSDP instances where each instance has parameters FSDP model auto_wrap_policy=ModuleWrapPolicy nn Linear mixed_precision=MixedPrecision param_dtype=torch float reduce_dtype=torch float param_init_fn=_param_init_fn device_id=torch accelerator current_device_index instantiate_parametrized_tests TestFSDPWithMetaDevice __name__ == __main__ run_tests