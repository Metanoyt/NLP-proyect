collections copy dataclasses functools inspect itertools logging operator threading collections defaultdict collections abc Callable Sequence typing Any Optional TYPE_CHECKING Union typing_extensions Never sympy torch fx fx torch utils _pytree pytree torch SymInt Tensor torch _C DispatchKey torch _higher_order_ops utils redirect_to_mode torch _ops HigherOrderOperator torch _prims_common clone_preserve_strides torch _subclasses fake_tensor FakeTensorMode torch fx experimental proxy_tensor disable_proxy_modes_tracing ProxyTorchDispatchMode track_tensor_tree torch fx experimental symbolic_shapes guard_scalar torch types IntLikeType torch utils checkpoint _CachedTorchDispatchMode _CachingTorchDispatchMode TYPE_CHECKING triton _C libtriton ir module TritonIRModule operation TritonIROperation torch _dynamo symbolic_convert InstructionTranslator torch _dynamo variables constant ConstantVariable torch _dynamo variables functions TritonKernelVariable torch _subclasses functional_tensor BaseFunctionalizeAPI torch fx proxy Proxy torch utils _triton has_triton TritonMetaParamsType = dict str int TritonGridTupleType = tuple Union int sympy Expr SymInt TritonGridCallableType = Callable TritonMetaParamsType tuple int TritonGridType = Union TritonGridTupleType TritonGridCallableType has_triton triton runtime autotuner Autotuner Config TritonConfig triton runtime jit JITFunction Autotuner type ignore no-redef pass JITFunction type ignore no-redef pass TritonKernelType = Union Autotuner JITFunction mypy specifically complains TritonAutotunerType valid type Autotuner inside Union TritonAutotunerType = Union Autotuner log = logging getLogger torch _dynamo e g host-side Triton TMA API call ` ` create_ d_tma_descriptor ptr ` ` metadata will look like ` ` experimental ` ` TMAExperimentalMetadata = tuple str type TMA should experimental tuple list IntLikeType dims list IntLikeType block_dims IntLikeType element_size e g host-side Triton TMA API call ` ` TensorDescriptor from_tensor ptr ` ` metadata will look like ` ` stable ` ` TMAStableMetadata = tuple str type TMA experimental stable tuple list IntLikeType block_shape create_tma_experimental_metadata dims list IntLikeType block_dims list IntLikeType element_size IntLikeType - TMAExperimentalMetadata experimental dims block_dims element_size maybe_unpack_tma_experimental_metadata tma_meta Union TMAExperimentalMetadata TMAStableMetadata - Optional tuple list IntLikeType list IntLikeType IntLikeType tma_meta len tma_meta = None tma_meta == experimental tma_meta type ignore return-value None create_tma_stable_metadata block_shape list IntLikeType - TMAStableMetadata stable block_shape maybe_unpack_tma_stable_metadata tma_meta Union TMAExperimentalMetadata TMAStableMetadata - Optional tuple list IntLikeType tma_meta len tma_meta = None tma_meta == stable tma_meta type ignore return-value None TMADescriptorMetadata maps kernel parameter names metadata allows reconstructing TMA descriptors underlying tensors passed kernel arguments fx graph instead TMA descriptors Since there two TMA APIs old experimental API new stable API each entry dict tuple starts string either experimental stable The second entry tuple another tuple data depends API type see TMAExperimentalMetadata TMAStableMetadata above These stored raw tuples instead classes ease serialization TMADescriptorMetadata = dict str kernel parameter name Union TMAExperimentalMetadata TMAStableMetadata ############################################################################### Kernel Side Table We cannot put Triton Kernels into FX graph graph nodes do support arbitrary functions Use side table We use two dicts so fetching both kernel id O KernelSideTable id_to_kernel dict int TritonKernelType = kernel_to_id dict TritonKernelType int = constant_args dict int dict str Any = lock = threading Lock Returns index table add_kernel kernel TritonKernelType - int lock kernel kernel_to_id kernel_to_id kernel idx = len id_to_kernel id_to_kernel idx = kernel kernel_to_id kernel = idx idx Returns triton kernel given index get_kernel idx int - TritonKernelType No need lock here fetching dict atomic assert idx id_to_kernel id_to_kernel idx Not every constant arg can added graph Use side table constant args add_constant_args args dict str Any - int lock idx = len constant_args constant_args idx = args idx Returns constant args get_constant_args idx int - dict str Any No need lock here fetching dict atomic assert idx constant_args constant_args idx Resets table only meant used unit tests This only safe assuming single threaded execution reset_table - None id_to_kernel = kernel_to_id = constant_args = kernel_side_table = KernelSideTable ############################################################################### Mutation Tracker dataclasses dataclass frozen=True Param idx int dataclasses dataclass frozen=True Intermediate idx int fake - bool idx dataclasses dataclass frozen=True Op name str fn_call_name Optional str args list Union Param Intermediate ret Intermediate = dataclasses field repr=False used scf yield see Note scf yield fix-up sub_idx Optional int = None used tt elementwise_inline_asm ` is_pure = True ` assumes asm block has no side-effects is_pure bool = False __post_init__ - None name == tt call assert fn_call_name None assert fn_call_name None generate_ttir kernel TritonKernelType kwargs dict str Any tma_descriptor_metadata TMADescriptorMetadata - tuple TritonIRModule list str Uses Triton s internal code generation create TTIR sympy triton triton runtime jit triton compiler compiler ASTSource triton runtime autotuner Autotuner triton runtime jit JITFunction torch _inductor utils get_triton_attrs_descriptor_version triton_version_uses_attrs_dict TritonAttrsDescriptorVersion torch utils _triton has_triton_tensor_descriptor_host_tma triton_version = get_triton_attrs_descriptor_version torch _inductor ir torch _subclasses fake_tensor FakeTensor isinstance kernel Autotuner len kernel configs If we autotuning then doesn t matter which version gets picked tracing purposes so lets pick first one kwargs = kwargs kernel configs kwargs kernel = kernel fn assert isinstance kernel JITFunction context = triton _C libtriton ir context target = triton runtime driver active get_current_target backend = triton compiler compiler make_backend target options = backend parse_options ignore backend-specific kwargs same way native Triton code https github com triton-lang triton blob bb d e c e dd cba db efff python triton runtime jit py#L -L why important user-defined Triton kernels AMD https github com pytorch pytorch issues name list kwargs name kernel arg_names name options __dict__ kwargs pop name len kwargs = len kernel arg_names raise ValueError Incorrect number arguments passed kernel f passed list kwargs keys expected kernel arg_names Replace all SymExprs regular value TTIR generation Replace all FakeTensor TensorBox real tensors These replacements needed triton s type key config functions ordered_args dict str Any = name kernel arg_names = kwargs name isinstance torch SymInt torch SymFloat torch SymBool sympy Expr ordered_args name = stable_meta = maybe_unpack_tma_stable_metadata pyrefly ignore bad-argument-type tma_descriptor_metadata get name None None triton tools tensor_descriptor TensorDescriptor block_shape = stable_meta torch _C _DisableTorchDispatch need -byte aligned strides elements_per_dim = max dtype itemsize base_tensor = torch empty elements_per_dim len block_shape dtype=a dtype ordered_args name = TensorDescriptor from_tensor base_tensor block_shape isinstance FakeTensor torch _inductor ir TensorBox torch _C _DisableTorchDispatch ordered_args name = torch empty dtype=a dtype ordered_args name = is_stable_tensor_descriptor_arg arg Any - bool has_triton_tensor_descriptor_host_tma triton tools tensor_descriptor TensorDescriptor isinstance arg TensorDescriptor True False is_tensor_like_arg arg Any - bool isinstance arg Tensor is_stable_tensor_descriptor_arg arg True False Note one would expect each input triton kernel maps one input parameter TTIR This _not_ true TMA descriptors one TMA descriptor gets converted into one TMA descriptor input N strides rank-N tensor N sizes rank-N tensor To account we inject some fake arg names placeholders stride size parameters get_tensor_names name str arg Any - list str isinstance arg Tensor name is_stable_tensor_descriptor_arg arg stable_meta = maybe_unpack_tma_stable_metadata tma_descriptor_metadata name assert stable_meta None block_shape = stable_meta tensor_rank = len block_shape names = name names extend name + f STRIDE PLACEHOLDER i i range tensor_rank names extend name + f SIZE PLACEHOLDER i i range tensor_rank names ordered_tensor_names = list itertools chain from_iterable get_tensor_names name arg name arg ordered_args items _get_specialization args type ignore no-untyped-def Support multiple triton versions This code basically copies JITFunction run logic get attrs construct ASTSource triton_version == TritonAttrsDescriptorVersion V _COMPILER kernel _get_config args triton_version TritonAttrsDescriptorVersion V _BACKENDS TritonAttrsDescriptorVersion V _BACKENDS_TUPLE triton backends compiler AttrsDescriptor noqa F target = triton runtime driver active get_current_target backend_ = triton compiler compiler make_backend target backend_ get_attrs_descriptor args kernel params assert get_triton_attrs_descriptor_version == TritonAttrsDescriptorVersion V _DICT specialize_impl switched create_specialize_impl https github com triton-lang triton pull hasattr triton runtime jit create_specialize_impl try Latest versions Triton take specialize_extra arg create_specialize_impl specialize_impl = triton runtime jit create_specialize_impl specialize_extra=backend get_arg_specialization except TypeError Unknown arg ` specialize_extra ` Older versions Triton take specialize_extra arg specialize_impl specialize_impl = functools partial triton runtime jit create_specialize_impl specialize_extra=backend get_arg_specialization create_specialize_impl removed https github com triton-lang triton pull switch native_specialize_impl instead hasattr triton runtime jit native_specialize_impl triton backends BaseBackend triton runtime jit native_specialize_impl _native_specialize_impl arg Any is_const bool = False specialize_value bool = True align bool = True - Callable native_specialize_impl BaseBackend arg is_const specialize_value align specialize_impl = _native_specialize_impl triton runtime jit specialize_impl specialize_impl_orig specialize_impl = functools partial specialize_impl_orig specialize_extra=backend get_arg_specialization triton _utils find_paths_if get_iterable_path logic copied binder = create_function_from_signature signature params backend attrvals = arg kp zip args kernel params kp is_constexpr attrvals append arg spec = specialize_impl arg is_const=kp is_const specialize_value=not kp do_not_specialize align=not kp do_not_specialize_on_alignment pyrefly ignore unsupported-operation attrvals append spec attrs = find_paths_if attrvals lambda _ x isinstance x str attrs = k backend parse_attr get_iterable_path attrvals k k attrs attrs specialization = _get_specialization ordered_args values constants = name arg name arg ordered_args items is_tensor_like_arg arg mangle_type = getattr triton runtime jit mangle_type None None get_signature_value idx int arg Any - str kernel params idx is_constexpr constexpr pyrefly ignore not-callable mangle_type arg get_signature_value idx int arg Any - str kernel _type_of kernel key_of arg triton_version_uses_attrs_dict In newer versions Triton signature includes constexpr args signature = name get_signature_value i arg i name arg enumerate ordered_args items In older versions Triton signature does include constexpr args constexprs = p num p kernel params p is_constexpr signature = name get_signature_value i arg i name arg enumerate ordered_args items i constexprs triton _C libtriton ir load_dialects context backend load_dialects context src = ASTSource kernel signature constants specialization Triton changes ASTSource make_ir take arguments Handle backward compatibility here make_ir_sig_params = len inspect signature src make_ir parameters get_codegen_implementation_sig_params = len inspect signature backend get_codegen_implementation parameters make_ir_sig_params == ttir_module = src make_ir options context make_ir_sig_params == codegen_fns = backend get_codegen_implementation ttir_module = src make_ir options codegen_fns context make_ir_sig_params == codegen_args = options get_codegen_implementation_sig_params == codegen_fns = backend get_codegen_implementation codegen_args module_map = backend get_module_map ttir_module = src make_ir options codegen_fns module_map context codegen_args = options get_codegen_implementation_sig_params == codegen_fns = backend get_codegen_implementation codegen_args module_map = backend get_module_map ttir_module = src make_ir target options codegen_fns module_map context ttir_module verify raise RuntimeError Verification TTIR module has failed ttir_module ordered_tensor_names ttir_to_functions ttir_module TritonIRModule - dict str dict Intermediate list Op Walk ` ttir_module ` bottom up mine ` functions ` structured MLIR entities representing Triton kernel mlir Operation mlir Block mlir Region functions dict str dict Intermediate list Op = block id -- op result Intermediate -- one more ops op_stack dict int dict Intermediate list Op = defaultdict lambda defaultdict list region_id_to_block_ids dict int list int = defaultdict list block_id_to_block_arg_ids dict int list int = replacements dict int Union Intermediate Param = reindex_map dict int int = next_fake_intermediate = reindex idx int - int idx reindex_map reindex_map idx = len reindex_map reindex_map idx mlir_to_functions op TritonIROperation - None name str = op get_name name == builtin module wraps all tt func ops operand_ids list int = reindex op get_operand i id i range op get_num_operands result_ids list int = reindex op get_result i id i range op get_num_results child_block_ids list int = i op get_region i id i range op get_num_regions walk bottom-up region_id_to_block_ids i must populated time we process enclosing op child_block_ids extend region_id_to_block_ids i parent_block_id = - parent_block = op get_block parent_block None parent_block_id = parent_block id parent_block_id block_id_to_block_arg_ids block_id_to_block_arg_ids parent_block_id = i range parent_block get_num_arguments block_id_to_block_arg_ids parent_block_id append reindex parent_block get_argument i id region info collected via ops parent blocks used later when region s encloding op traversed parent_region = parent_block get_parent parent_region None region_id_to_block_ids parent_region id append parent_block_id nonlocal next_fake_intermediate name == tt func function ops gather inline ops all child blocks fn_ops = defaultdict list child_block_id child_block_ids result block_fn_ops op_stack pop child_block_id items block_fn_op block_fn_ops fn_ops result append block_fn_op replace corresponding Intermediates child op args function args Params i idx enumerate block_id_to_block_arg_ids child_block_ids replacements idx = Param i fn_op_list fn_ops values fn_op fn_op_list i range len fn_op args arg = fn_op args i seen = set break cycles there can transitive replacements likely no cycles we keep ` seen ` set just case while isinstance arg Intermediate arg idx replacements arg idx seen seen add arg idx arg = fn_op args i = replacements arg idx next function capture starts empty replacements replacements clear fn_name = op get_str_attr sym_name functions fn_name = fn_ops child_block_ids name scf scf scf while tt reduce tt scan blocked ops inline enclosed ops into parent block + rewire last op each child block block result return_ops = block_id child_block_ids name == scf example result = scf iv = lb ub step step iter_args arg = init - i block args iv arg op operands lb ub step init ` arg ` mapping ` init ` i idx enumerate block_id_to_block_arg_ids block_id i == next_fake_intermediate -= replacements idx = Intermediate next_fake_intermediate replacements idx = Intermediate operand_ids i + name == scf while example = scf while arg = arg = arg = c _i _ block args arg arg arg op operands c _i _ ` arg ` mapping ` ` ` arg ` mapping ` ` i idx enumerate block_id_to_block_arg_ids block_id replacements idx = Intermediate operand_ids i name == scf scf block args ignored pass they may used operands ops inside block nested blocks inlined current block now they replaced new fake Intermediates avoid operand returned any other op fn error downstream analysis idx block_id_to_block_arg_ids block_id next_fake_intermediate -= replacements idx = Intermediate next_fake_intermediate assert name tt reduce tt scan wire block arguments op arguments num_operands = len operand_ids block_arg_ids = block_id_to_block_arg_ids block_id assert len block_arg_ids == num_operands f name expected have twice many block arguments op arguments f operand_ids= block_arg_ids= i idx enumerate block_arg_ids tt reduce tt scan op N arguments block arguments comprise N reduced values followed N current values corresponding N op args replacements idx = Intermediate operand_ids i num_operands block_id op_stack block_ops = op_stack pop block_id block_ops continue last_ret last_ops = block_ops popitem all op name scf yield tt reduce tt scan op last_ops last_ops all ops treat them separately return_ops extend last_ops otherwise last_ops block block_ops last_ret = last_ops op_result child_ops block_ops items op_stack parent_block_id op_result extend child_ops scf_results = Intermediate idx idx result_ids return_ops all op name == scf yield len result_ids == len op args op return_ops Note scf yield fix-up TL DR our scf yield takes N args then we ll create N scf yield ops handle each args Context During mutation analysis analysis pass will identify mutating ops e g tt store then DFS upwards towards parameters function Specifically analysis pass looks mutated arg tt store then looks its source ops then recurses arguments each source ops In case scf scf we may have multiple ops each passed arg scf yield = scf - tt ptr f tt ptr f scf yield scf yield And each returns scf we d naively assign source op each values scf yields But scf yields take _all_ returns arguments Therefore _any_ values scf mutated then analysis pass would mark _all_ yield args mutated Solution For purposes analysis pass we create N yield ops - one each return-val yield-arg In example above we ll have two scf yield s each branch scf return_op return_ops i scf_result yield_arg enumerate zip scf_results return_op args sub_yield_op = Op return_op name return_op fn_call_name yield_arg return_op ret sub_idx=i op_stack parent_block_id scf_result append sub_yield_op scf_result scf_results return_op return_ops op_stack parent_block_id scf_result append return_op raise RuntimeError f Unknown blocked function name Can t capture TTIR callee = None name == tt call callee = op get_flat_symbol_ref_attr callee args list Union Param Intermediate = Intermediate operand operand operand_ids block_ops = op_stack parent_block_id is_pure = False Handle case tt elementwise_inline_asm set ` is_pure ` mutation analysis name == tt elementwise_inline_asm is_pure = op get_bool_attr pure result_ids result_id result_ids res = Intermediate result_id block_ops res append Op name callee args res is_pure=is_pure next_fake_intermediate -= fake_res = Intermediate next_fake_intermediate block_ops fake_res append Op name callee args fake_res is_pure=is_pure ttir_module walk mlir_to_functions functions MemoizeWithCycleCheck fn Callable Any cache dict tuple Any Any __init__ fn Callable Any - None fn = fn reset __call__ functions dict str dict Intermediate list Op fn_name str args Any - list bool key tuple Any = fn_name args key cache cache key = None cache key = fn functions fn_name args cache key None raise RuntimeError Recursion supported cache key reset - None cache = MemoizeWithCycleCheck get_tma_stores functions dict str dict Intermediate list Op fn_name str - set Union Intermediate Param Identifies all intermediates parameters written ` tt experimental_descriptor_store ` It tracks only specific values written via experimental_descriptor_store input values ` tt reinterpret_tensor_descriptor ` used construct direct inputs tt experimental_descriptor_store - any recursive values used construct those values For example tt reinterpret_tensor_descriptor Intermediate idx= Intermediate idx= = tt experimental_descriptor_store Intermediate idx= function will Intermediate idx= Intermediate idx= However Intermediate idx= = arith addptr Intermediate idx= Intermediate idx= Intermediate idx= = tt experimental_descriptor_store Intermediate idx= tt experimental_descriptor_store Intermediate idx= function will mark only idx= idx= idx= idx= If intermediate parameter passed into function written via experimental_descriptor_store within function argument function will also marked result set Union Intermediate Param = set ops = functions fn_name op_list ops values op op_list op name == tt call assert op fn_call_name functions pyrefly ignore bad-argument-type tma_stores = get_tma_stores functions op fn_call_name i inp enumerate op args Param idx=i tma_stores result add inp op name == tt experimental_descriptor_store assert len op args = result add op args op name == tt descriptor_store assert len op args = result add op args val list result val ops isinstance val Intermediate continue op ops val op name == tt reinterpret_tensor_descriptor assert len op args = result add op args result MemoizeWithCycleCheck analyze_kernel_mutations functions dict str dict Intermediate list Op fn_name str num_args int - list bool Analyzes graph detect all sinks predefined list sinks using triton s MemWrite trait list NOTE What triton exposed From each sink traverses CFG backwards identify all input pointers mutated Name mutation op mutated parameter indices List Triton Github include triton Dialect Triton IR TritonOps td All OPs have MemWrite trait What Triton exposed MUTATION_OPS = tt store tt atomic_cas tt atomic_rmw tt experimental_descriptor_store tt experimental_tensormap_create tt descriptor_store Ops we want bail out UNKNOWN_OPS = tt elementwise_inline_asm stack list Union Param Intermediate = visited = set ops = functions fn_name tma_stores = get_tma_stores functions fn_name op_list ops values op op_list If we encounter operation effects cannot reliably analyzed e g ` tt elementwise_inline_asm ` we assume does mutate any input parameters op name UNKNOWN_OPS op name == tt elementwise_inline_asm op is_pure continue raise RuntimeError f ttir analysis hit op we do know how analyze op name op name == tt experimental_tensormap_create Note how we implement experimental_descriptor_store mutation analysis on-device TMA experimental_tensormap_store b stores b location specified descriptor memory To track we first find all intermediates params which we store via experimental_tensormap_store get_tma_stores called above Then during analysis we wait find corresponding experimental_tensormap_create exists which point we will mark global_ptr mutated done below assert len op args = op args tma_stores stack append op args op name == tt call assert op fn_call_name functions mutations = analyze_kernel_mutations functions pyrefly ignore bad-argument-type op fn_call_name len op args stack extend arg arg mutated zip op args mutations mutated stack extend op args idx idx MUTATION_OPS get op name The following iterative DFS algorithm mutated = False num_args while stack arg = stack pop arg visited continue visited add arg isinstance arg Param arg idx = num_args This argument defined kernel passed continue mutated arg idx = True isinstance arg Intermediate arg fake op ops arg Skip arguments load op name = tt load stack extend op args mutated identify_mutated_tensors kernel TritonKernelType kwargs dict str Any tma_descriptor_metadata TMADescriptorMetadata - list str Given triton kernel arguments kernel function Retrieves TTIR converted version kernel Triton s API Parses TTIR creates control flow graph Analyzes graph detect all input tensor mutations ttir_module = None functions = None try ttir_module ordered_tensor_names = generate_ttir kernel kwargs tma_descriptor_metadata extract functions TTIR using MLIR bindings exposed Triton code functions = ttir_to_functions ttir_module assert functions None kernel_name = next iter functions keys Triton codegen modifies name pyrefly ignore missing-attribute assert kernel fn __name__ kernel_name Reset cache between top level invocations The cache analyze kernel mutations mainly used cycle detection so each top level invocation needs clean cache analyze_kernel_mutations reset get_tma_stores reset mutations = analyze_kernel_mutations functions kernel_name len ordered_tensor_names ordered_tensor_names i i mutated enumerate mutations mutated except Exception log warning Encountered exception identify_mutated_tensors assuming every input mutated exc_info=True ttir_module None log debug TTIR \n s str ttir_module functions None log debug functions name fn functions items log debug ===\t s\t=== name ret ops fn items log debug s\t= \t s ret ops key key value kwargs items isinstance value Tensor ############################################################################### Triton Kernel Wrappers Used wrapping Triton Kernel TritonKernelWrapperMutation HigherOrderOperator __init__ - None super __init__ triton_kernel_wrapper_mutation cacheable=True __call__ kernel_idx int constant_args_idx int grid list TritonGridType tma_descriptor_metadata TMADescriptorMetadata kwargs dict str Any - Any super __call__ kernel_idx=kernel_idx constant_args_idx=constant_args_idx grid=grid tma_descriptor_metadata=tma_descriptor_metadata kwargs=kwargs triton_kernel_wrapper_mutation = TritonKernelWrapperMutation Used wrapping Triton Kernel functional manner TritonKernelWrapperFunctional HigherOrderOperator __init__ - None super __init__ triton_kernel_wrapper_functional cacheable=True __call__ kernel_idx int constant_args_idx int grid list TritonGridType tma_descriptor_metadata TMADescriptorMetadata kwargs dict str Any tensors_to_clone list str - dict str Any super __call__ kernel_idx=kernel_idx constant_args_idx=constant_args_idx grid=grid tma_descriptor_metadata=tma_descriptor_metadata kwargs=kwargs tensors_to_clone=tensors_to_clone triton_kernel_wrapper_functional = TritonKernelWrapperFunctional triton_kernel_wrapper_mutation py_impl DispatchKey CompositeExplicitAutograd triton_kernel_wrapper_mutation_dense kernel_idx int constant_args_idx int grid list TritonGridType tma_descriptor_metadata TMADescriptorMetadata kwargs dict str Any - None torch _inductor codegen wrapper user_defined_kernel_grid_fn_code kernel = kernel_side_table get_kernel kernel_idx constant_args = kernel_side_table get_constant_args constant_args_idx len grid == grid_fn = grid fn_name code = user_defined_kernel_grid_fn_code pyrefly ignore missing-attribute kernel fn __name__ pyrefly ignore missing-attribute kernel configs grid namespace dict str Any = exec code namespace grid_fn = namespace fn_name tma_descriptor_metadata we need launch kernel here we unwrap tma_descriptor_metadata create TMA descriptors replace tensors kwargs corresponding TMA descriptors before launching kwargs = kwargs copy k v tma_descriptor_metadata items tensor = kwargs k exp_meta = maybe_unpack_tma_experimental_metadata v None triton tools experimental_descriptor noqa F create_ d_tma_descriptor create_ d_tma_descriptor dims block_dims element_size = exp_meta create_tma_descriptor = create_ d_tma_descriptor len dims == create_ d_tma_descriptor kwargs k = create_tma_descriptor tensor data_ptr dims block_dims element_size stable_meta = maybe_unpack_tma_stable_metadata v assert stable_meta None triton tools tensor_descriptor TensorDescriptor block_shape = stable_meta kwargs k = TensorDescriptor from_tensor tensor block_shape move many positional arguments dicts args we can circumvent bug kwargs pre_ post_hook https github com triton-lang triton issues TODO remove when Triton issue above fixed args = copy kwargs constant_args here avoid mutating original inputs kwargs = kwargs copy constant_args = constant_args copy pyrefly ignore missing-attribute name kernel arg_names name kwargs args append kwargs pop name name constant_args args append constant_args pop name break pyrefly ignore index-error kernel grid_fn args kwargs constant_args triton_kernel_wrapper_mutation py_impl FakeTensorMode triton_kernel_wrapper_mutation_fake_tensor_mode mode FakeTensorMode kernel_idx int constant_args_idx int grid list TritonGridType tma_descriptor_metadata TMADescriptorMetadata kwargs dict str Any - None mode None triton_kernel_wrapper_mutation py_impl DispatchKey Meta _ kernel_idx int constant_args_idx int grid list TritonGridType tma_descriptor_metadata TMADescriptorMetadata kwargs dict str Any - None None trace_triton_kernel_wrapper proxy_mode ProxyTorchDispatchMode func_overload Callable Any node_args dict str Any - Optional dict str Any disable_proxy_modes_tracing out = func_overload node_args proxy_args = pytree tree_map proxy_mode tracer unwrap_proxy type ignore union-attr node_args out_proxy = proxy_mode tracer create_proxy call_function func_overload proxy_args name=func_overload __name__ + _proxy ret = track_tensor_tree out out_proxy constant=None tracer=proxy_mode tracer ret triton_kernel_wrapper_mutation py_impl ProxyTorchDispatchMode triton_kernel_wrapper_mutation_proxy_torch_dispatch_mode mode ProxyTorchDispatchMode kernel_idx int constant_args_idx int grid list TritonGridType tma_descriptor_metadata TMADescriptorMetadata kwargs dict str Any - None trace_triton_kernel_wrapper mode triton_kernel_wrapper_mutation kernel_idx kernel_idx constant_args_idx constant_args_idx grid grid tma_descriptor_metadata tma_descriptor_metadata kwargs kwargs None get_mutated_tensors kernel_idx int constant_args_idx int kwargs dict str Any tma_descriptor_metadata TMADescriptorMetadata - list str kernel = kernel_side_table get_kernel kernel_idx constant_args = kernel_side_table get_constant_args constant_args_idx identify_mutated_tensors kernel kwargs constant_args tma_descriptor_metadata triton_kernel_wrapper_mutation py_functionalize_impl triton_kernel_wrapper_mutation_functionalize ctx BaseFunctionalizeAPI kernel_idx int constant_args_idx int grid list TritonGridType tma_descriptor_metadata TMADescriptorMetadata kwargs dict str Any - None unwrapped_kwargs = ctx unwrap_tensors kwargs type ignore arg-type TODO oulgen Preexisting bug two kernel inputs views each other one gets mutated kernel later another gets mutated they no longer equal Fix graph breaking condition earlier dynamo tensors_to_clone = get_mutated_tensors kernel_idx constant_args_idx unwrapped_kwargs tma_descriptor_metadata ctx redispatch_to_next unwrapped_outputs = triton_kernel_wrapper_functional kernel_idx=kernel_idx constant_args_idx=constant_args_idx grid=grid tma_descriptor_metadata=tma_descriptor_metadata kwargs=unwrapped_kwargs tensors_to_clone=tensors_to_clone assert set unwrapped_outputs keys issubset set kwargs keys key output_arg unwrapped_outputs items isinstance output_arg Tensor continue input_arg = kwargs key assert isinstance input_arg Tensor ctx replace input_arg output_arg indicate above replace hidden autograd ctx mark_mutation_hidden_from_autograd input_arg ctx commit_update input_arg ctx sync input_arg None triton_kernel_wrapper_functional py_impl DispatchKey CompositeExplicitAutograd triton_kernel_wrapper_functional_dense kernel_idx int constant_args_idx int grid list TritonGridType tma_descriptor_metadata TMADescriptorMetadata kwargs dict str Any tensors_to_clone list str - dict str Any TODO oulgen For performance reasons we want ensure these ` clone_preserve_strides ` calls never executed runtime inductor should always optimize them away Requires https github com pytorch pytorch issues kwargs = key clone_preserve_strides val key tensors_to_clone val key val kwargs items triton_kernel_wrapper_mutation kernel_idx=kernel_idx constant_args_idx=constant_args_idx grid=grid tma_descriptor_metadata=tma_descriptor_metadata kwargs=kwargs key val key val kwargs items key tensors_to_clone triton_kernel_wrapper_functional py_impl FakeTensorMode triton_kernel_wrapper_functional_fake_tensor_mode mode FakeTensorMode kernel_idx int constant_args_idx int grid list TritonGridType tma_descriptor_metadata TMADescriptorMetadata kwargs dict str Any tensors_to_clone list str - dict str Any TODO oulgen For performance reasons we want ensure these ` clone_preserve_strides ` calls never executed runtime inductor should always optimize them away Requires https github com pytorch pytorch issues mode key clone_preserve_strides val key val kwargs items key tensors_to_clone triton_kernel_wrapper_functional py_impl ProxyTorchDispatchMode triton_kernel_wrapper_functional_proxy_torch_dispatch_mode mode ProxyTorchDispatchMode kernel_idx int constant_args_idx int grid list TritonGridType tma_descriptor_metadata TMADescriptorMetadata kwargs dict str Any tensors_to_clone list str - dict str Any ret = trace_triton_kernel_wrapper mode triton_kernel_wrapper_functional kernel_idx kernel_idx constant_args_idx constant_args_idx grid grid tma_descriptor_metadata tma_descriptor_metadata kwargs kwargs tensors_to_clone tensors_to_clone assert ret None ret triton_kernel_wrapper_functional py_functionalize_impl triton_kernel_wrapper_functional_functionalize ctx BaseFunctionalizeAPI kernel_idx int constant_args_idx int grid list TritonGridType tma_descriptor_metadata TMADescriptorMetadata kwargs dict str Any tensors_to_clone list str - dict str Any unwrapped_kwargs = ctx unwrap_tensors kwargs type ignore arg-type ctx redispatch_to_next outputs = triton_kernel_wrapper_functional kernel_idx=kernel_idx constant_args_idx=constant_args_idx grid=grid tma_descriptor_metadata=tma_descriptor_metadata kwargs=unwrapped_kwargs tensors_to_clone=tensors_to_clone ctx wrap_tensors outputs type ignore return-value arg-type triton_kernel_wrapper_mutation fallthrough DispatchKey PythonDispatcher type ignore attr-defined triton_kernel_wrapper_mutation fallthrough DispatchKey PythonTLSSnapshot type ignore attr-defined triton_kernel_wrapper_mutation fallthrough DispatchKey ADInplaceOrView triton_kernel_wrapper_mutation fallthrough DispatchKey BackendSelect triton_kernel_wrapper_mutation fallthrough DispatchKey AutocastCPU type ignore attr-defined triton_kernel_wrapper_mutation fallthrough DispatchKey AutocastCUDA type ignore attr-defined triton_kernel_wrapper_mutation fallthrough DispatchKey AutogradCUDA triton_kernel_wrapper_mutation fallthrough DispatchKey AutogradCPU triton_kernel_wrapper_functional fallthrough DispatchKey PythonDispatcher type ignore attr-defined triton_kernel_wrapper_functional fallthrough DispatchKey PythonTLSSnapshot type ignore attr-defined triton_kernel_wrapper_functional fallthrough DispatchKey ADInplaceOrView triton_kernel_wrapper_functional fallthrough DispatchKey BackendSelect triton_kernel_wrapper_functional fallthrough DispatchKey AutocastCPU type ignore attr-defined triton_kernel_wrapper_functional fallthrough DispatchKey AutocastCUDA type ignore attr-defined triton_kernel_wrapper_functional fallthrough DispatchKey AutogradCUDA triton_kernel_wrapper_functional fallthrough DispatchKey AutogradCUDA triton_kernel_wrapper_functional fallthrough DispatchKey AutogradCPU Adds SAC support triton ops redirect_to_mode triton_kernel_wrapper_mutation _CachingTorchDispatchMode redirect_to_mode triton_kernel_wrapper_mutation _CachedTorchDispatchMode ############################################################################### The TritonHOPifier transforms call triton kernel into call triton_kernel_wrapper_mutation HOP TritonHOPifier Orchestrator converting user-defined triton kernel into call triton_kernel_wrapper_mutation HOP It has two main use cases When Dynamo sees triton kernel wraps into TritonKernelVariable uses TritonHOPifier convert calls TritonKernelVariable into call HOP In order capture user-defined triton kernel while performing tracing via make_fx non-strict export user must annotate their triton kernel ` wrap_triton ` decorator The decorator uses TritonHOPifier convert calls triton kernel into call HOP which can then traced Because Dynamo has its own calling conventions e g invoking user-defined function TritonHOPifier abstract can overridden its subclasses raise_unsupported msg str - Never raise NotImplementedError abstract method is_callable maybe_callable Any - bool raise NotImplementedError abstract method get_value val Any - Any raise NotImplementedError abstract method call_grid type ignore no-untyped-def grid meta tx - Union tuple Union int sympy Expr SymInt tuple Proxy raise NotImplementedError abstract method wrap_user_defined_obj user_obj Any tx Optional InstructionTranslator variable Optional Union TritonKernelVariable TraceableTritonKernelWrapper name str - Any raise NotImplementedError abstract method call_user_defined_fn user_fn Callable Any args list kwargs dict tx Optional InstructionTranslator variable Optional Union TritonKernelVariable TraceableTritonKernelWrapper - Any raise NotImplementedError abstract method maybe_unpack_configs configs list TritonConfig tx Optional InstructionTranslator - list TritonConfig raise NotImplementedError abstract method maybe_unpack_heuristic_result result Any - Any raise NotImplementedError abstract method staticmethod do_prune_configs type ignore no-untyped-def autotuner TritonAutotunerType early_config_prune Optional Callable perf_model Optional Callable top_k float configs list named_args dict kwargs dict - list TritonConfig Reimplement autotuner prune_configs here see https github com triton-lang triton blob e b b b c d d e e b python triton runtime autotuner py noqa E B We do avoid calling prune_configs which turn calls early_config_prune perf_model These both user-defined functions which can contain side effects so we want sandbox them Dynamo early_config_prune configs = early_config_prune configs named_args kwargs perf_model we assert top_k float before calling isinstance top_k float top_k = top_k = int len configs top_k isinstance top_k int Slice index must integer SupportsIndex None raise TypeError Error while pruning configs top_k must either float = int len configs top_k est_timing = config float perf_model named_args kwargs config all_kwargs config configs configs = config config sorted est_timing key=operator itemgetter top_k configs call_HOP type ignore no-untyped-def variable grids combined_args dict str Any tx - Optional ConstantVariable raise NotImplementedError abstract method check_grid type ignore no-untyped-def grid - Union tuple Union int sympy Expr SymInt tuple Proxy raise NotImplementedError abstract method init_variable variable Union TraceableTritonKernelWrapper TritonKernelVariable kernel TritonKernelType kernel_idx Optional int grid Optional TritonGridType - None triton runtime autotuner Autotuner assert kernel None variable kernel = kernel variable kernel_idx = kernel_side_table add_kernel kernel assert kernel_idx None variable kernel_idx == kernel_idx pyrefly ignore bad-assignment variable grid = grid isinstance kernel Autotuner torch torch _dynamo We only support configs keys restore_value arguments triton autotune Make sure other arguments defaulted defaults = inspect signature Autotuner __init__ parameters Newer version triton change attribute name warmup num_warmup rep num_rep The call get_first_attr maintain backward-compatibility defaults_ok attr str alternates tuple str values tuple Any - bool attr defaults True value = torch _dynamo utils get_first_attr kernel attr alternates value == defaults attr default True value values torch _inductor config unsafe_ignore_unsupported_triton_autotune_args defaults_ok num_warmups warmup None defaults_ok num_reps rep None defaults_ok use_cuda_graph False raise_unsupported Only configs keys restore_value reset_to_zero supported triton autotune torch _inductor config unsafe_ignore_unsupported_triton_autotune_args pre_hook requires running arbitrary code runtime which we cannot handle time https github com pytorch pytorch issues we can t support pre_hook post_hook user defined triton kernels moment they require ability execute code runtime AOTI can t support hasattr kernel user_defined_pre_hook kernel user_defined_pre_hook False hasattr kernel user_defined_post_hook kernel user_defined_post_hook False Check Config passed autotuner configs any cfg pre_hook None cfg kernel configs raise_unsupported pre_hook post_hook supported triton Autotune triton Config call_getitem variable Union TritonKernelVariable TraceableTritonKernelWrapper args Sequence Any - Union TritonKernelVariable TraceableTritonKernelWrapper __getitem__ should only called we don t already have grid Only grid needs passed variable grid None len args = raise_unsupported Triton kernels should called only single grid type variable kernel=variable kernel kernel_idx=variable kernel_idx grid=args call_run variable Union TritonKernelVariable TraceableTritonKernelWrapper args Sequence Any kwargs dict str Any tx Optional InstructionTranslator - Optional ConstantVariable grid kwargs raise_unsupported Triton kernel requires called grid grid = kwargs pop grid kwargs pop warmup None rewrite kernel run args grid=grid kernel grid args call_triton_kernel type variable kernel=variable kernel kernel_idx=variable kernel_idx grid=grid args kwargs tx call_triton_kernel variable Union TritonKernelVariable TraceableTritonKernelWrapper args Sequence Any kwargs dict str Any tx Optional InstructionTranslator - Optional ConstantVariable triton JITFunction triton runtime autotuner autotune Autotuner Config Heuristics Check num_ctas kwargs num_ctas kwargs raise_unsupported Passing num_ctas directly Triton kernel supported Please use Config triton autotune instead Make sure kernel has grid variable grid None raise_unsupported Triton kernels should always called grid raise exception there multiple triton autotune decorators iter_kernel = variable kernel autotuner_count = while isinstance iter_kernel JITFunction isinstance iter_kernel Autotuner autotuner_count += autotuner_count raise_unsupported Passing multiple triton autotune decorators supported Please use single triton autotune decorator instead iter_kernel = iter_kernel fn Process triton heuristics decorator - We know there only autotuner decorator here - We can apply heuristic all triton Configs order decorators appear This way when config selected heuristics have already been applied - Decorators appear before autotuner already processed correctly isinstance variable kernel Autotuner isinstance variable kernel fn Heuristics unwrap heuristics decorator we don t need anymore variable kernel == Autotuner variable kernel fn == Heuristics There can arbitrarily many heuristics wrappers here variable kernel fn == JITFunction Copy configs we going modifying them new_configs = copy deepcopy variable kernel configs named_args = dict zip variable kernel arg_names args Iterate through all heuristics wrappers come after autotune wrapper iter_kernel = variable kernel fn while isinstance iter_kernel Heuristics For each config apply heuristic fn s config_idx range len new_configs kwarg_key heuristic_fn iter_kernel values items Run heuristics combined configs + kwargs heuristic_result = call_user_defined_fn heuristic_fn named_args kwargs new_configs config_idx __dict__ kwargs tx variable Update kwargs each config maybe_unpack_heuristic_result raises unsupported value non-constant new_configs config_idx __dict__ kwargs kwarg_key = maybe_unpack_heuristic_result heuristic_result iter_kernel = iter_kernel fn assert isinstance iter_kernel JITFunction prune_configs_by = perf_model variable kernel perf_model early_config_prune variable kernel early_config_prune configs_top_k variable kernel configs_top_k new_kernel = autotune configs=new_configs key= prune_configs_by=prune_configs_by iter_kernel create new variable contain new wrapped kernel skip kernel_idx get new record kernel side table new_var = type variable new_kernel None variable grid call_triton_kernel new_var args kwargs tx SPECIAL_CONFIG_NAMES = num_warps num_stages num_ctas num_consumer_groups num_buffers_warp_spec num_cpu_threads move special config names configs out kwargs special_kwargs = name SPECIAL_CONFIG_NAMES name kwargs remove special kwargs ` kwargs ` val = kwargs pop name special_kwargs name = get_value val special_kwargs isinstance variable kernel Autotuner there Autotuner already set special kwargs each its configs new_configs = copy deepcopy variable kernel configs config new_configs config __dict__ update special_kwargs prune_configs_by = perf_model variable kernel perf_model early_config_prune variable kernel early_config_prune configs_top_k variable kernel configs_top_k new_kernel = autotune configs=new_configs key= prune_configs_by=prune_configs_by variable kernel fn there no Autotuner wrap kernel into new one single config special kwargs new_config = Config kwargs= special_kwargs new_kernel = autotune configs= new_config key= variable kernel create new variable contain new wrapped kernel skip kernel_idx get new record kernel side table new_var = type variable new_kernel None variable grid call_triton_kernel new_var args kwargs tx isinstance variable kernel Autotuner special_param_names = name SPECIAL_CONFIG_NAMES name variable kernel fn arg_names special_param_names append name special_param_names If Triton kernel has SPECIAL_CONFIG_NAMES parameters those should passed kernel configs behavior Triton runtime those values get folded into kernel arguments iff there parameters same name Normally values those parameters defined outside ` kwargs ` part autotuning configs Here we move them ` kwargs ` part they re absent there facilitate passing them arguments kernel downstream updated = False new_configs = copy deepcopy variable kernel configs config new_configs name special_param_names name config __dict__ kwargs assert name config __dict__ f name must autotuning configs used kernel parameter config __dict__ kwargs name = config __dict__ name updated = True updated prune_configs_by = perf_model variable kernel perf_model early_config_prune variable kernel early_config_prune configs_top_k variable kernel configs_top_k new_kernel = autotune configs=new_configs prune_configs_by=prune_configs_by key= variable kernel fn new_var = type variable new_kernel None variable grid call_triton_kernel new_var args kwargs tx These default values upstream Triton see https github com triton-lang triton blob e b b b c d d e e b python triton runtime autotuner py noqa E B default_perf_model = None default_early_config_prune = None run prune_configs_by isinstance variable kernel Autotuner variable kernel perf_model = default_perf_model variable kernel early_config_prune = default_early_config_prune Prune configs named_args = dict zip variable kernel arg_names args The source information important here so guards installed correctly wrapped_early_configs_prune = wrap_user_defined_obj variable kernel early_config_prune tx variable early_config_prune wrapped_perf_model = wrap_user_defined_obj variable kernel perf_model tx variable perf_model wrapped_configs_top_k = wrap_user_defined_obj variable kernel configs_top_k tx variable configs_top_k wrapped_configs = wrap_user_defined_obj variable kernel configs tx variable configs pruned_configs = call_user_defined_fn do_prune_configs variable wrapped_early_configs_prune wrapped_perf_model wrapped_configs_top_k wrapped_configs named_args kwargs tx variable pruned_configs = maybe_unpack_configs pruned_configs tx after pruning configs create new autotuner object these configs recurse new_kernel = autotune configs=pruned_configs key= variable kernel fn create new variable contain new wrapped kernel skip kernel_idx get new record kernel side table new_var = type variable new_kernel None variable grid call_triton_kernel new_var args kwargs tx Both grid s meta well kernel we need combined args kwargs combined normalized combined_args_raw = dict zip variable kernel arg_names args kwargs precompute grid kernel configs = config kwargs config variable kernel configs isinstance variable kernel Autotuner grids = config_args configs If grid function then lets execute convert list grid = variable grid assert grid None is_callable grid Populate special meta argument call grid function meta = combined_args_raw config_args grid = call_grid grid meta tx type ignore arg-type grids append check_grid grid i range len grids isinstance grids i tuple raise_unsupported Only tuple grids supported inductor expects all grids -tuple so lets make len grids i == grids i = grids i len grids i == grids i = grids i grids i len grids i raise_unsupported Grid can have most rank assert len grids = isinstance variable kernel JITFunction constexprs = p num p variable kernel params p is_constexpr arg_names = p name p variable kernel params If we looking triton autotune decorator nested function should JITFunction This because we don t support triton heuristics nested triton autotune decorators yet assert isinstance variable kernel Autotuner constexprs = p num p variable kernel fn params p is_constexpr arg_names = p name p variable kernel fn params idx arg_name enumerate arg_names idx constexprs arg_name combined_args_raw Note Specialize tl constexpr args user-defined triton kernels This arg marked tl constexpr That means triton will recompile every time value changes https github com pytorch pytorch issues One option correctly pass symints so symbolic expressions defined when triton code being executed But since triton will have recompile either way we instead just specialize value Depending type ` variable ` we might expect different types symbolic args either SymNodeVariables TritonKernelVariables SymInts TracingTritonKernelWrapper combined_args_raw arg_name = variable specialize_symbolic combined_args_raw arg_name call_HOP variable grids combined_args_raw tx ############################################################################### Helpers wrap_triton API makes user-defined triton kernel traceable into graph via make_fx non-strict export coming soon TracingTritonHOPifier TritonHOPifier raise_unsupported msg str - Never raise RuntimeError msg is_callable maybe_callable Any - bool callable maybe_callable get_value val Any - Any val call_grid grid TritonGridCallableType meta TritonMetaParamsType tx None - tuple Union int sympy Expr SymInt assert tx None assert isinstance meta dict assert callable grid grid meta wrap_user_defined_obj user_obj Any tx Optional InstructionTranslator variable Optional Union TritonKernelVariable TraceableTritonKernelWrapper name str - Any assert tx None user_obj call_user_defined_fn user_fn Callable Any args list kwargs dict tx Optional InstructionTranslator variable Optional Union TritonKernelVariable TraceableTritonKernelWrapper - Any assert isinstance args list assert isinstance kwargs dict assert callable user_fn user_fn args kwargs maybe_unpack_configs configs list TritonConfig tx Optional InstructionTranslator - list TritonConfig assert isinstance configs list configs maybe_unpack_heuristic_result result Any - Any result check_grid grid TritonGridType - tuple Union int sympy Expr SymInt isinstance grid collections abc Sequence raise RuntimeError wrap_triton can only handle grids resolve Sequence int normalize tuple tuple grid store_non_graphable_args combined_args dict str Any - tuple dict int Some args cannot stored FX graph Put them side table is_graphable val Any - bool isinstance val fx node base_types fx Node non_graphable_args = k v k v combined_args items is_graphable v graphable_args = k v k v combined_args items is_graphable v constant_args_idx = kernel_side_table add_constant_args non_graphable_args graphable_args constant_args_idx call_HOP variable TraceableTritonKernelWrapper grids list TritonGridTupleType combined_args dict str Any tx None - None assert tx None assert isinstance variable TraceableTritonKernelWrapper graphable_args constant_args_idx = store_non_graphable_args combined_args assert isinstance variable kernel_idx int triton_kernel_wrapper_mutation kernel_idx=variable kernel_idx constant_args_idx=constant_args_idx grid=grids type ignore arg-type TMA descriptor capturing yet supported non-dynamo tracing tma_descriptor_metadata= kwargs=graphable_args tracing_triton_hopifier_singleton = TracingTritonHOPifier TraceableTritonKernelWrapper kernel TritonKernelType kernel_idx Optional int grid Optional TritonGridType __init__ kernel TritonKernelType kernel_idx Optional int grid Optional TritonGridType - None kernel = None grid = None tracing_triton_hopifier_singleton init_variable kernel kernel_idx grid assert kernel None __getitem__ args Sequence Any - TraceableTritonKernelWrapper tracing_triton_hopifier_singleton call_getitem args type ignore return-value run args Sequence Any kwargs dict str Any - Any torch _library triton is_wrap_triton_enabled is_wrap_triton_enabled tracing_triton_hopifier_singleton call_run args kwargs None assert kernel None pyrefly ignore missing-attribute kernel run args kwargs __call__ args Sequence Any kwargs dict str Any - Any torch _library triton is_wrap_triton_enabled is_wrap_triton_enabled tracing_triton_hopifier_singleton call_triton_kernel args kwargs None assert kernel None pyrefly ignore index-error kernel grid args kwargs specialize_symbolic arg Sequence Any - Any torch See Note Specialize tl constexpr args user-defined triton kernels isinstance arg torch SymInt torch SymBool torch SymFloat guard_scalar arg arg