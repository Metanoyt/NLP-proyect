Owner s module nestedtensor ruff noqa F ast io itertools math os random sys tempfile unittest functools partial typing Optional numpy np torch torch _dynamo torch _dynamo testing torch nn torch nn functional F torch nested _internal nested_tensor buffer_from_jagged jagged_from_list nested_view_from_values_offsets NestedTensor ViewNestedFromBuffer torch testing _internal common_cuda PLATFORM_SUPPORTS_FUSED_ATTENTION SM OrLater SM OrLater tf _on_and_off torch testing _internal common_device_type dtypes dtypesIfCUDA instantiate_device_type_tests onlyCPU onlyCUDA ops PYTORCH_CUDA_MEMCHECK skipCPUIf skipCUDAIf skipCUDAIfRocm skipMeta torch testing _internal common_dtype floating_types_and_half torch testing _internal common_utils decorateIf freeze_rng_state gradcheck instantiate_parametrized_tests IS_FBCODE IS_WINDOWS markDynamoStrictTest NestedTensorTestCase parametrize run_tests serialTest skipIfSlowGradcheckEnv skipIfTorchDynamo subtest TEST_WITH_ROCM xfailIfTorchDynamo torch testing _internal opinfo core BinaryUfuncInfo ReductionOpInfo sample_skips_and_xfails SkipRule XFailRule torch testing _internal opinfo definitions nested _sample_njts njt_op_db torch utils _pytree tree_flatten tree_map_only torch utils checkpoint checkpoint create_selective_checkpoint_contexts Tests ported pytorch nestedtensor This makes porting as_nested_tensor easier future _iter_constructors yield as_nested_tensor yield torch nested nested_tensor Returns True function recompiles between inputs inputs specified dynamic setting _recompiles_for_inputs fn inputs inputs dynamic=True compile_count = counter gm example_inputs compile_count += gm compiled_f = torch compile fn fullgraph=True backend=counter dynamic=dynamic compiled_f inputs compiled_f inputs compile_count Helper function generate pair random nested tensors one contiguous other they appear have same entries output nested tensor consists ` len ragged_sizes ` matrices matrices i shape == ragged_sizes i random_nt_noncontiguous_pair ragged_sizes device= cpu dtype=torch float xs = size ragged_sizes xs append torch randn size device=device dtype=dtype contiguous nested tensor ys = x xs ys append x transpose - - nt_contiguous = torch nested nested_tensor ys noncontiguous nested tensor n = len ragged_sizes nt_noncontiguous = torch nested nested_tensor xs transpose - - nt_contiguous nt_noncontiguous Helper functions pad noncontiguous nested tensor can replaced once to_padded_tensor supports noncontiguous memory noncontiguous_to_padded_tensor input shape=None tensors = input unbind ntensors = len tensors assert ntensors shape None shape = size tensors shape shape append size i range ntensors new_shape = tensors i shape j range len shape shape j = max shape j new_shape j shape = ntensors + shape result = tensors new_zeros shape itensor range ntensors tensor = tensors itensor view = result itensor idim range tensor dim view = view narrow idim tensor size idim view copy_ tensor result Helper function generate random nested tensor random_nt device dtype num_tensors max_dims min_dims=None layout=torch strided require_non_empty=True min_dims None min_dims = tuple len max_dims assert len max_dims == len min_dims min_dim max_dim zip min_dims max_dims assert max_dim min_dim random_nt max_dim must greater than min_dim assert min_dim = random_nt min_dim must non-negative require_non_empty assert min_dim == max_dim == random_nt zero cannot only possible value require_non_empty True require_non_empty Select random idx will required non-empty non_zero_idx = torch randint low= high=num_tensors size= item ts = i _ enumerate range num_tensors tensor_dims = min_dim max_dim zip min_dims max_dims new_min_dim = min_dim require_non_empty i == non_zero_idx min_dim == new_min_dim = tensor_dims append torch randint low=new_min_dim high=max_dim size= item t = torch randn tensor_dims device=device dtype=dtype ts append t torch nested nested_tensor ts device=device dtype=dtype layout=layout Alternate approach generating random NT dims should something like None None indicating random ragged structure should used random_nt_from_dims dims device=None dtype=None layout=torch strided requires_grad=False sizes = d d None torch randint size= item d dims d range dims torch nested nested_tensor torch randn size size sizes device=device dtype=dtype layout=layout requires_grad=requires_grad Creates NT matching another NT s number components shape ragged structure all dims specified - random_nt_from_similar other dims=None dims None torch randn_like other assert len dims == other dim assert dims == - dims == other size ret_sizes = t other unbind other_size = t shape ret_size = i d enumerate dims d == - ret_size append other_size i ret_size append d ret_sizes append ret_size torch nested nested_tensor torch randn size size ret_sizes device=other device makes naming nice tests parametrize over layout layout_name layout e g torch jagged - jagged layout __repr__ split - get_op_name layout e g OpOverload op= aten sum overload= dim_IntList - sum layout __name__ split split _ - Helper function test_dummy_mha_with_nt torch fx wrap convert_dense_to_nested_tensor_legacy values offsets = torch arange values shape values shape + values shape device=values device metadata_cache = max_seqlen values shape min_seqlen nt = ViewNestedFromBuffer apply values view - values shape - offsets metadata_cache nt Helper function test_dummy_mha_with_nt torch fx wrap convert_jagged_to_nested_tensor_legacy values torch Tensor offsets torch Tensor max_length int - torch Tensor metadata_cache = max_seqlen max_length min_seqlen nt = ViewNestedFromBuffer apply values offsets metadata_cache nt Helper function test_dummy_mha_with_nt torch fx wrap convert_nt_to_jagged_legacy nt buffer_from_jagged nt Helper function test_dummy_mha_with_nt torch fx wrap convert_dense_to_nested_tensor values nt = torch nested as_nested_tensor values layout=torch jagged nt Helper function test_dummy_mha_with_nt torch fx wrap convert_jagged_to_nested_tensor values torch Tensor offsets torch Tensor max_length int - torch Tensor nt = torch nested nested_tensor_from_jagged values offsets lengths=None min_seqlen= max_seqlen=max_length nt Helper function test_dummy_mha_with_nt convert_nt_to_jagged nt nt values markDynamoStrictTest TestNestedTensor NestedTensorTestCase parametrize batch_size parametrize max_seq_len parametrize vocab_size test_ d_nested_tensor batch_size max_seq_len vocab_size data = nested_tensor_ref_list = _ range batch_size max_seq_len == length = length = np random randint low= high=max_seq_len row = list np random randint low= high=vocab_size size= length data append row nested_tensor_ref_list append torch Tensor row nested_tensor = torch nested nested_tensor data dtype=torch int nested_tensor_list = nested_tensor unbind id range batch_size assertEqual nested_tensor_list id nested_tensor_ref_list id type torch int parametrize batch_size parametrize max_seq_len parametrize vocab_size test_ d_nested_tensor batch_size max_seq_len vocab_size data = nested_tensor_ref_list = _ range batch_size max_seq_len == length = length = np random randint low= high=max_seq_len row = list np random randint low= high=vocab_size size= length row = list item np arange max_seq_len item row data append row nested_tensor_ref_list append torch Tensor row nested_tensor = torch nested nested_tensor data dtype=torch int nested_tensor_list = nested_tensor unbind id range batch_size assertEqual nested_tensor_list id nested_tensor_ref_list id type torch int parametrize batch_size parametrize max_seq_len parametrize vocab_size test_ d_nested_tensor_float batch_size max_seq_len vocab_size data = nested_tensor_ref_list = _ range batch_size max_seq_len == length = length = np random randint low= high=max_seq_len row = list np random randint low= high=vocab_size size= length astype float row = list item np arange max_seq_len item row data append row nested_tensor_ref_list append torch Tensor row nested_tensor = torch nested nested_tensor data dtype=torch float nested_tensor_list = nested_tensor unbind id range batch_size assertEqual nested_tensor_list id nested_tensor_ref_list id type torch float torch inference_mode _test_unbind_case b nt = torch nested nested_tensor b b = nt unbind assertTrue assertTrue b b nt = torch nested nested_tensor b dtype=a dtype b = nt unbind assertEqual assertEqual b b = torch randn add_ nt = torch nested nested_tensor assertEqual nt unbind torch inference_mode test_unbind_ _test_unbind_case torch tensor torch tensor torch inference_mode test_unbind_ _test_unbind_case torch tensor torch tensor torch inference_mode test_unbind_ _test_unbind_case torch tensor torch tensor torch inference_mode test_unbind_ _test_unbind_case torch tensor torch tensor torch inference_mode test_unbind_dim _test_fn unbind_fn = torch rand b = torch rand nt = torch nested nested_tensor b assertRaises RuntimeError lambda unbind_fn nt Both these tests necessary because we re using torch_function _test_fn lambda x dim x unbind dim TODO Re-enable once using torch_dispatch _test_fn lambda x dim torch unbind x dim torch inference_mode test_nested_tensor assertRaises TypeError lambda torch nested nested_tensor torch tensor assertRaises TypeError lambda torch nested nested_tensor torch inference_mode test_nested_tensor_matching_dim assertRaisesRegex RuntimeError Found dimension Tensor index dimension Tensor index lambda torch nested nested_tensor torch tensor torch tensor assertRaisesRegex RuntimeError Found dimension Tensor index dimension Tensor index lambda torch nested nested_tensor torch tensor torch tensor torch tensor torch inference_mode test_default_nested_tensor assertRaises TypeError lambda torch nested nested_tensor default_nested_tensor = torch nested nested_tensor default_tensor = torch tensor assertEqual default_nested_tensor nested_dim assertEqual default_nested_tensor nested_size assertEqual default_nested_tensor dim default_tensor dim assertEqual default_nested_tensor layout default_tensor layout assertEqual default_nested_tensor device default_tensor device assertEqual default_nested_tensor dtype default_tensor dtype assertEqual default_nested_tensor requires_grad default_tensor requires_grad assertIsNone default_tensor grad TODO Re-enable once we have performance driven use case implementation assertEqual default_nested_tensor is_pinned default_tensor is_pinned torch inference_mode test_dim constructor _iter_constructors = constructor assertEqual dim = constructor torch tensor assertEqual dim = constructor torch tensor assertEqual dim unittest skipIf IS_FBCODE numel virtual fbcode torch inference_mode test_numel constructor _iter_constructors = constructor assertEqual numel = constructor torch tensor torch tensor assertEqual numel = constructor torch randn assertEqual numel = constructor torch randn torch randn assertEqual numel = constructor torch randn torch randn assertEqual numel = constructor torch randn torch randn assertEqual numel Interesting edge case = constructor torch randn torch randn assertEqual numel torch inference_mode test_size constructor _iter_constructors = constructor assertRaisesRegex RuntimeError NestedTensorImpl doesn t support sizes lambda size test_size_dim = torch nested nested_tensor assertEqual size = torch nested nested_tensor torch tensor assertEqual size = torch nested nested_tensor torch tensor torch tensor assertEqual size = torch nested nested_tensor torch rand torch rand assertEqual size assertEqual size assertRaisesRegex RuntimeError Given dimension irregular does have size lambda size = torch nested nested_tensor torch rand torch rand assertEqual size assertRaisesRegex RuntimeError Given dimension irregular does have size lambda size assertEqual size unittest skipIf IS_FBCODE stride virtual fbcode torch inference_mode test_stride constructor _iter_constructors = constructor assertRaisesRegex RuntimeError NestedTensorImpl doesn t support strides lambda stride unittest skipIf IS_FBCODE is_contiguous virtual fbcode torch inference_mode test_is_contiguous Test empty case nt_empty = torch nested nested_tensor assert nt_empty is_contiguous assertEqual nt_empty nt_empty contiguous nt_contiguous nt_noncontiguous = random_nt_noncontiguous_pair Test contiguous case assert nt_contiguous is_contiguous assertEqual nt_contiguous nt_contiguous contiguous Test non_contiguous case assert nt_noncontiguous is_contiguous assertEqual nt_contiguous nt_noncontiguous contiguous Test querying memory_format assertTrue nt_contiguous is_contiguous memory_format=torch contiguous_format assertTrue nt_noncontiguous is_contiguous memory_format=torch contiguous_format torch inference_mode test_repr_string = torch nested nested_tensor expected = nested_tensor \n\n assertEqual str expected assertEqual repr expected = torch nested nested_tensor torch tensor expected = nested_tensor \n tensor \n assertEqual str expected assertEqual repr expected = torch nested nested_tensor torch tensor torch tensor expected = nested_tensor \n tensor \n tensor \n assertEqual str expected assertEqual repr expected test_to_padded_tensor_on_empty_tensor nt = torch nested nested_tensor empty = torch nested to_padded_tensor nt assertEqual empty torch tensor test_nested_namespace nt = torch nested nested_tensor torch randn torch randn result = nt to_padded_tensor nested_namespace_result = torch nested to_padded_tensor nt assertEqual result nested_namespace_result test_to ntensors = nt = random_nt torch device cpu torch float ntensors test_copy_behavior t non_blocking=False assertIs t t t non_blocking=non_blocking assertIs t t t dtype non_blocking=non_blocking assertIs t t torch empty_like t non_blocking=non_blocking assertIsNot t t t non_blocking=non_blocking copy=True assertIsNot t t t dtype non_blocking=non_blocking copy=True assertIsNot t t torch empty_like t non_blocking=non_blocking copy=True devices = t device t device type == cuda t device index == - devices append f cuda torch cuda current_device t device index == torch cuda current_device devices append cuda device devices assertIs t t device non_blocking=non_blocking assertIs t t device t dtype non_blocking=non_blocking assertIsNot t t device non_blocking=non_blocking copy=True assertIsNot t t device t dtype non_blocking=non_blocking copy=True test_copy_behavior nt assertEqual nt device nt cpu device assertEqual nt device nt cpu dtype=torch float device assertIs torch float nt cpu dtype=torch float dtype assertEqual nt device nt torch float device assertIs torch float nt dtype=torch float dtype test_data_ptr getter assertEqual getter nt getter nt cpu assertEqual getter nt getter nt dtype=nt dtype device=nt device copy=False assertEqual getter nt getter nt cpu copy=False assertNotEqual getter nt getter nt cpu copy=True test_data_ptr lambda nt nt data_ptr torch cuda is_available non_blocking True False cuda cuda cuda torch cuda device_count == cuda nt = random_nt cuda torch float ntensors test_copy_behavior nt non_blocking assertEqual nt device nt cuda non_blocking=non_blocking device assertEqual nt device nt cpu non_blocking=non_blocking device assertEqual nt device nt cuda non_blocking=non_blocking device assertIs torch int nt cpu dtype=torch int non_blocking=non_blocking dtype assertEqual nt device nt cpu dtype=torch int non_blocking=non_blocking device assertIs torch int nt dtype=torch int dtype assertEqual nt device nt dtype=torch int device test_copy_ ntensors = nt = random_nt torch device cpu torch float ntensors nt_copy = torch empty_like nt nt_copy copy_ nt nt_ub nt_copy_ub zip nt unbind nt_copy assertEqual nt_ub nt_copy_ub nt_error = torch nested nested_tensor torch tensor assertRaisesRegex RuntimeError copy_ only supports tensors same size Nested implementations lambda nt_error copy_ nt torch cuda is_available nt = random_nt torch device cuda torch float ntensors nt_copy = torch empty_like nt device=torch device cpu nt_copy copy_ nt non_blocking=True torch cuda current_stream torch cuda current_device synchronize nt_ub nt_copy_ub zip nt unbind nt_copy assertEqual nt_ub nt_copy_ub nt_copy = torch empty_like nt device=torch device cpu nt_copy copy_ nt non_blocking=False nt_ub nt_copy_ub zip nt unbind nt_copy assertEqual nt_ub nt_copy_ub test_fill_ ntensors = nt = random_nt torch device cpu torch float ntensors nt fill_ nt_ub nt unbind t = torch empty_like nt_ub t fill_ assertEqual nt_ub t fill_tensor = torch tensor assertRaisesRegex RuntimeError fill_ only supports -dimension value tensor lambda nt fill_ fill_tensor nt fill_ fill_tensor nt_ub nt unbind t = torch empty_like nt_ub t fill_ assertEqual nt_ub t test_zero_ ntensors = nt = random_nt torch device cpu torch float ntensors nt zero_ nt_ub nt unbind t = torch empty_like nt_ub t fill_ assertEqual nt_ub t parametrize func torch ones_like torch zeros_like torch randn_like name_fn=lambda f f __name__ test_like_functions func ntensors = nt = random_nt torch device cpu torch float ntensors torch manual_seed nt_like = func nt torch manual_seed nt_ub nt_like unbind t_like = func nt_ub assertEqual nt_ub t_like test_cat dim= success case No constraints ragged structures matching x = random_nt_from_dims None y = random_nt_from_dims None output = torch cat x y dim= out_component xy_component zip output unbind itertools chain x unbind y unbind assertEqual out_component xy_component dim=- success case shape B D x = random_nt_from_dims None shape B D same structure x dim=- differs y = random_nt_from_similar x dims= - - should shape B D + D when supported output = torch cat x y dim=- out_component x_component y_component zip output unbind x unbind y unbind assertEqual out_component torch cat x_component y_component dim=- dim between - success case x = random_nt_from_dims None same structure x dim= differs y = random_nt_from_similar x dims= - - - output = torch cat x y dim= out_component x_component y_component zip output unbind x unbind y unbind assertEqual out_component torch cat x_component y_component dim= error case mixed NT dense inputs x = random_nt_from_dims None y = torch randn assertRaisesRegex RuntimeError expected each tensor given list nested torch cat x y dim=- error case NTs different dims x = random_nt_from_dims None y = random_nt_from_dims None assertRaisesRegex RuntimeError expected all nested tensors have matching ragged structures outside concatenated dim torch cat x y dim=- error case non-contiguous NT x y = random_nt_noncontiguous_pair dtype=torch float transpose put ragged dim next batch dim x y = x transpose - - y transpose - - assertRaisesRegex RuntimeError only contiguous nested tensors supported torch cat x y dim=- error case multiple ragged dims inputs x = random_nt_from_dims None None y = random_nt_from_similar x assertRaisesRegex RuntimeError only nested tensors single ragged dim next batch dim supported torch cat x y dim=- error case ragged dim next batch dim x = random_nt_from_dims None y = random_nt_from_similar x assertRaisesRegex RuntimeError only nested tensors single ragged dim next batch dim supported torch cat x y dim= error case NTs different batch sizes x = random_nt_from_dims None y = random_nt_from_dims None assertRaisesRegex RuntimeError expected all nested tensors have matching ragged structures outside concatenated dim torch cat x y dim=- error case NTs different ragged structures x = torch nested nested_tensor torch randn torch randn torch randn y = torch nested nested_tensor torch randn torch randn torch randn assertRaisesRegex RuntimeError expected all nested tensors have matching ragged structures outside concatenated dim torch cat x y dim=- https github com pytorch pytorch issues test_jagged_with_dim_error x = torch nested nested_tensor torch ones torch ones layout=torch jagged assertRaisesRegex RuntimeError supported NestedTensor dim= torch cat x x assertRaisesRegex RuntimeError supported NestedTensor dim= torch stack x x test_nested_view_from_buffer_overflow_errors buffer = torch tensor sizes = torch tensor - - dtype=torch int strides = torch tensor x x x dtype=torch int offsets = torch tensor x x x dtype=torch int assertRaisesRegex RuntimeError r Storage size calculation overflowed sizes=\ \ strides=\ \ nt = torch _nested_view_from_buffer buffer sizes strides offsets markDynamoStrictTest TestNestedTensorDeviceType NestedTensorTestCase Helper function generate pair random nested tensors nested tensors have same shapes random_nt_pair device dtype num_tensors max_dims ts = ts = _ range num_tensors tensor_dims = tuple torch randint low= high=max_dim size= item max_dim max_dims t = torch randn tensor_dims device=device dtype=dtype t = torch randn tensor_dims device=device dtype=dtype ts append t ts append t torch nested nested_tensor ts device=device dtype=dtype torch nested nested_tensor ts device=device dtype=dtype dtypes floating_types_and_half test_detach device dtype = torch randn device=device dtype=dtype requires_grad=False b = torch randn device=device dtype=dtype requires_grad=False x = torch nested nested_tensor b requires_grad=True x_detach = x detach z = x_detach assertFalse x_detach requires_grad assertFalse z requires_grad = torch randn device=device dtype=dtype requires_grad=True b = torch randn device=device dtype=dtype requires_grad=True x = torch nested as_nested_tensor b y = x y = y detach assertFalse y requires_grad assertIsNone y grad_fn z = x + y torch nested to_padded_tensor z sum backward This incorrect gradient we assume s what user wanted detach advanced option assertEqual grad torch ones device=device dtype=dtype assertEqual b grad torch ones device=device dtype=dtype dtypes torch float torch double torch half parametrize requires_grad False True parametrize weights_only False True test_serialization device dtype requires_grad weights_only compare_metadata nt nt assertEqual nt _nested_tensor_size nt _nested_tensor_size assertEqual nt _nested_tensor_strides nt _nested_tensor_strides assertEqual nt _nested_tensor_storage_offsets nt _nested_tensor_storage_offsets nt_contiguous nt_noncontiguous = random_nt_noncontiguous_pair nt_contiguous nt_noncontiguous buffer = io BytesIO serialized = torch save buffer buffer seek b = torch load buffer weights_only=weights_only should both conceptually equal metadata equivalent assertEqual b compare_metadata b should conceptually equal necessarily metadata equivalent assertEqual b nt_contiguous assertEqual b nt_noncontiguous dtypes torch float torch float torch double test_unbind_noncontiguous device dtype nt_contiguous nt_noncontiguous = random_nt_noncontiguous_pair device dtype ub_contiguous = nt_contiguous unbind ub_noncontiguous = nt_noncontiguous unbind assertEqual len ub_contiguous len ub_noncontiguous n = len ub_contiguous i range n assertEqual ub_contiguous i ub_noncontiguous i dtypes torch float skipMeta test_to_then_from_padded_tensor_no_transform device dtype t = torch randn device=device dtype=dtype ts = list torch unbind t ts = ts - nt = torch nested nested_tensor ts device=device dtype=dtype padded = torch nested to_padded_tensor nt nt_to = torch _nested_from_padded_and_nested_example padded nt t t zip nt unbind nt_to unbind assertEqual t t assertEqual nt device nt_to device dtypes torch float dtypesIfCUDA torch float torch half skipMeta torch inference_mode test_layer_norm device dtype _test size Simple shapes test t = torch randn size device=device dtype=dtype requires_grad=False t = torch randn size device=device dtype=dtype requires_grad=False ts = t t t t nt = torch nested nested_tensor ts device=device dtype=dtype layer_norm = torch nn LayerNorm size device=device dtype=dtype nt_result = layer_norm nt nt_subresult t zip nt_result unbind ts t_result = layer_norm t reshape - size squeeze assertEqual nt_subresult t_result More complex nt test different lengths each tensor t = torch randn size device=device dtype=dtype requires_grad=False t = torch randn size device=device dtype=dtype requires_grad=False t = torch randn size device=device dtype=dtype requires_grad=False ts = t t t t t nt = torch nested nested_tensor ts device=device dtype=dtype layer_norm = torch nn LayerNorm size device=device dtype=dtype nt_result = layer_norm nt nt_subresult t zip nt_result unbind ts t_result = layer_norm t reshape - size squeeze assertEqual nt_subresult t_result size = Test multidimensional tensors after irregular dim run only smaller dimensions ensure fast execution t = torch randn size size device=device dtype=dtype requires_grad=False t = torch randn size size device=device dtype=dtype requires_grad=False t = torch randn size size device=device dtype=dtype requires_grad=False ts = t t t t t nt = torch nested nested_tensor ts device=device dtype=dtype layer_norm = torch nn LayerNorm size size device=device dtype=dtype nt_result = layer_norm nt nt_subresult t zip nt_result unbind ts t_result = layer_norm t reshape - size size squeeze assertEqual nt_subresult t_result Test where normalizing dimensions all layer_norm = torch nn LayerNorm size device=device dtype=dtype nt_result = layer_norm nt nt_subresult t zip nt_result unbind ts t_result = layer_norm t reshape - size size squeeze assertEqual nt_subresult t_result size _test size dtypes torch float dtypesIfCUDA torch float torch half skipMeta torch inference_mode test_layer_norm_breaking device dtype size = t = torch randn size size device=device dtype=dtype requires_grad=False t = torch randn size size device=device dtype=dtype requires_grad=False t = torch randn size size device=device dtype=dtype requires_grad=False ts = t t t t t nt = torch nested nested_tensor ts device=device dtype=dtype layer_norm = torch nn LayerNorm size size device=device dtype=dtype assertRaisesRegex RuntimeError normalized_shape extends into irregular dimensions nested tensor lambda layer_norm nt layer_norm = torch nn LayerNorm size + size device=device dtype=dtype assertRaisesRegex RuntimeError The shape dimension lambda layer_norm nt parametrize layout torch strided torch jagged name_fn=layout_name test_embedding device layout inputs = torch randint L device=device dtype=torch int L torch randint x = torch nested nested_tensor inputs device=device dtype=torch int layout=layout emb = torch nn Embedding device=device y = emb x layout == torch jagged y backward torch randn_like y torch _dynamo disable check inputs y ys = y unbind i inp enumerate inputs assertEqual emb inp ys i check inputs y dtypes torch int torch int torch int torch int torch uint torch float torch float torch bfloat torch double test_jagged_max_dtypes device dtype x = torch nested nested_tensor torch arange n dtype=dtype device=device n layout=torch jagged result_max = x max dim= expected_max = torch tensor dtype=dtype device=device assertEqual result_max values expected_max dtypes torch int torch int torch int torch int torch uint torch float torch float torch bfloat torch double test_jagged_min_dtypes device dtype x = torch nested nested_tensor torch arange n dtype=dtype device=device n layout=torch jagged result_min = x min dim= expected_min = torch tensor dtype=dtype device=device assertEqual result_min values expected_min dtypes torch int torch int torch int torch int torch uint torch float torch float torch bfloat torch double test_jagged_amax_dtypes device dtype x = torch nested nested_tensor torch arange n dtype=dtype device=device n layout=torch jagged result_amax = x amax dim= expected_amax = torch tensor dtype=dtype device=device assertEqual result_amax expected_amax dtypes torch int torch int torch int torch int torch uint torch float torch float torch bfloat torch double test_jagged_amin_dtypes device dtype x = torch nested nested_tensor torch arange n dtype=dtype device=device n layout=torch jagged result_amin = x amin dim= expected_amin = torch tensor dtype=dtype device=device assertEqual result_amin expected_amin dtypes torch int torch int torch int torch int torch uint torch float torch float torch bfloat torch double test_jagged_argmax_dtypes device dtype x = torch nested nested_tensor torch arange n dtype=dtype device=device n layout=torch jagged result_argmax = x argmax dim= expected_argmax = torch tensor dtype=torch long device=device assertEqual result_argmax expected_argmax dtypes torch int torch int torch int torch int torch uint torch float torch float torch bfloat torch double test_jagged_argmin_dtypes device dtype x = torch nested nested_tensor torch arange n dtype=dtype device=device n layout=torch jagged result_argmin = x argmin dim= expected_argmin = torch tensor dtype=torch long device=device assertEqual result_argmin expected_argmin skipMeta torch inference_mode dtypes floating_types_and_half test_masked_fill device dtype nested tensor nested tensor nt mask = random_nt_pair device dtype mask = torch nested nested_tensor m m mask unbind ref = torch nested nested_tensor t masked_fill m t m zip nt unbind mask unbind out = nt masked_fill mask assertEqual ref out dtypes torch float torch float test_to_padded_tensor_simple device dtype t = torch randn device=device dtype=dtype ts = list torch unbind t ts = ts - nt = torch nested nested_tensor ts device=device dtype=dtype padding_value padded = torch nested to_padded_tensor nt padding_value correct_output = t clone padding_value == correct_output - = torch zeros_like correct_output - correct_output - = torch ones_like correct_output - assertEqual padded correct_output assertEqual padded device torch device device assertEqual padded dtype dtype dtypes torch float torch float test_to_padded_tensor_output_size device dtype t = torch randn device=device dtype=dtype output_size = ts = list torch unbind t ts = ts - nt = torch nested nested_tensor ts device=device dtype=dtype padding_value padded = torch nested to_padded_tensor nt padding_value output_size=output_size correct_output = torch ones output_size device=device dtype=dtype padding_value correct_output = t clone padding_value == correct_output = torch zeros_like correct_output correct_output = torch ones_like correct_output assertEqual padded correct_output assertEqual padded device torch device device assertEqual padded dtype dtype dtypes torch float torch float torch double test_to_padded_tensor_dim device dtype ts = torch randn device=device dtype=dtype torch randn device=device dtype=dtype torch randn device=device dtype=dtype nt = torch nested nested_tensor ts device=device dtype=dtype pad = correct_output = t ts next_output = torch ones_like ts pad correct_output append next_output next_output t size copy_ t correct_output = torch stack correct_output padded = torch nested to_padded_tensor nt pad assertEqual padded correct_output dtypes torch float torch float torch double test_to_padded_tensor_dim device dtype ts = torch randn device=device dtype=dtype torch randn device=device dtype=dtype torch randn device=device dtype=dtype nt = torch nested nested_tensor ts device=device dtype=dtype pad = correct_output = t ts next_output = torch ones_like ts pad correct_output append next_output next_output t size t size copy_ t correct_output = torch stack correct_output padded = torch nested to_padded_tensor nt pad assertEqual padded correct_output dtypes torch float torch float torch double test_to_padded_tensor_dim device dtype ts = torch randn device=device dtype=dtype torch randn device=device dtype=dtype torch randn device=device dtype=dtype nt = torch nested nested_tensor ts device=device dtype=dtype pad = correct_output = t ts next_output = torch ones_like ts pad correct_output append next_output next_output t size t size t size copy_ t correct_output = torch stack correct_output padded = torch nested to_padded_tensor nt pad assertEqual padded correct_output TODO test noncontiguous to_padded_tensor For now tests functionality noncontiguous_to_padded_tensor error message to_padded_tensor since to_padded_tensor does support noncontiguous buffer yet dtypes torch float torch float torch double torch inference_mode test_to_padded_tensor_noncontiguous device dtype nt_contiguous nt_noncontiguous = random_nt_noncontiguous_pair device dtype test noncontiguous_to_padded_tensor functionality assertEqual torch nested to_padded_tensor nt_contiguous noncontiguous_to_padded_tensor nt_noncontiguous test to_padded_tensor error message assertRaisesRegex RuntimeError r now to_padded_tensor only supports contiguous nested tensor lambda torch nested to_padded_tensor nt_noncontiguous skipMeta test_device_checks device nt = torch nested nested_tensor device=device is_cuda = cuda str device assertEqual nt is_cuda is_cuda skipIfTorchDynamo Not suitable test TorchDynamo test_share_memory device = torch randn device=device b = torch randn device=device nt = torch nested nested_tensor b layout=torch jagged Guard CUDA tensors cuda device result = nt share_memory_ assertIs result nt result = nt share_memory_ assertIs result nt Verify shared memory assertTrue nt is_shared dtypes torch float torch float torch double test_nested_tensor_indexing device dtype edge case empty nested tensor nt = torch nested nested_tensor assertRaises IndexError lambda nt normal case x = torch randn device=device dtype=dtype x = torch randn device=device dtype=dtype nt = torch nested nested_tensor x x single index only support integer batch dimension assertEqual nt x assertEqual nt - x assertRaises IndexError lambda nt assertRaises IndexError lambda nt - assertRaises NotImplementedError lambda nt assertEqual nt nt tuple indices only support integer batch dimension + all possible indexing original tensor dimensions assertEqual nt x assertEqual nt x assertEqual nt x assertRaises IndexError lambda nt assertRaises NotImplementedError lambda nt test select non-batch dimensions assertEqual nt select x select assertEqual nt select x select assertRaises IndexError lambda nt select assertEqual nt select x select assertEqual nt select x select assertRaises IndexError lambda nt select make sure indexing returns view nt fill_ answer = torch tensor device=device dtype=dtype expand assertEqual nt answer nt fill_ answer = torch tensor device=device dtype=dtype expand assertEqual nt answer Test indexing works when requires_grad_ True previously failing because backward kernel select int uses sizes nt = torch nested nested_tensor x x requires_grad_ True assertEqual nt x assertEqual nt - x grad_x = torch randn device=device dtype=dtype nt backward grad_x expected_grad = torch nested nested_tensor grad_x torch zeros device=device dtype=dtype assertEqual nt grad expected_grad parametrize func subtest torch nn functional relu name= relu subtest torch nn functional relu_ name= relu_ subtest torch nn functional gelu name= gelu subtest torch _C _nn gelu_ name= gelu_ subtest torch tanh name= tanh subtest torch tanh_ name= tanh_ subtest torch neg name= neg subtest torch nn functional silu name= silu subtest partial torch nn functional silu inplace=True name= silu_ subtest torch abs name= abs subtest torch abs_ name= abs_ subtest torch sgn name= sgn subtest torch logical_not name= logical_not subtest torch sin name= sin subtest torch cos name= cos subtest torch isinf name= isinf subtest torch isposinf name= isposinf subtest torch isneginf name= isneginf subtest torch isnan name= isnan subtest torch sqrt name= sqrt test_unary_funcs device func nt nt_noncontiguous = random_nt_noncontiguous_pair device=device dtype=torch float nested_result = func nt assertTrue nested_result is_nested t t_res zip nt unbind nested_result unbind assertEqual func t t_res assertRaisesRegex RuntimeError NestedTensor must contiguous get buffer lambda func nt_noncontiguous test_is_any_true_jagged device B Fin = start = torch zeros B dtype=torch int device=device lengths = torch tensor dtype=torch int device=device NestedTensor reduction should operate same data values subTest dispatch_matches_values_buffer cond = torch tensor True False False True True False False False True False False False dtype=torch bool device=device nt = torch nested narrow cond dim= start=start length=lengths layout=torch jagged out_nt = torch ops aten _is_any_true default nt item out_vals = torch ops aten _is_any_true default nt values item assertEqual out_nt out_vals Verify jagged boolean behavior subTest all_false_returns_false cond_false = torch zeros B Fin dtype=torch bool device=device nt_false = torch nested narrow cond_false dim= start=start length=lengths layout=torch jagged assertFalse torch ops aten _is_any_true default nt_false item subTest one_true_returns_true cond_mixed = torch zeros B Fin dtype=torch bool device=device cond_mixed = True nt_mixed = torch nested narrow cond_mixed dim= start=start length=lengths layout=torch jagged assertTrue torch ops aten _is_any_true default nt_mixed item test_is_all_true_jagged device B Fin = start = torch zeros B dtype=torch int device=device lengths = torch tensor dtype=torch int device=device NestedTensor reduction should operate same data values subTest dispatch_matches_values_buffer cond = torch tensor True True True False False False True True False False False False dtype=torch bool device=device nt = torch nested narrow cond dim= start=start length=lengths layout=torch jagged out_nt = torch ops aten _is_all_true default nt item out_vals = torch ops aten _is_all_true default nt values item assertEqual out_nt out_vals Verify jagged boolean behavior subTest all_true_returns_true cond_true = torch ones B Fin dtype=torch bool device=device nt_true = torch nested narrow cond_true dim= start=start length=lengths layout=torch jagged assertTrue torch ops aten _is_all_true default nt_true item subTest any_false_returns_false cond_mixed = torch ones B Fin dtype=torch bool device=device cond_mixed = False nt_mixed = torch nested narrow cond_mixed dim= start=start length=lengths layout=torch jagged assertFalse torch ops aten _is_all_true default nt_mixed item parametrize func subtest torch ge name= ge subtest torch eq name= eq test_binary_ops_with_scalar device func nt_contiguous nt_noncontiguous = random_nt_noncontiguous_pair device=device dtype=torch float scalar = should work regardless contiguity nt nt_contiguous nt_noncontiguous nested_result = func nt scalar assertTrue nested_result is_nested t t_res zip nt unbind nested_result unbind assertEqual func t scalar t_res dtypes floating_types_and_half test_nested_tensor_chunk device dtype Transformer use case = torch randn device=device dtype=dtype b = torch randn device=device dtype=dtype c = torch randn device=device dtype=dtype a_chunks = chunk dim=- b_chunks = b chunk dim=- c_chunks = c chunk dim=- a_nt = a_chunks b_chunks c_chunks b_nt = a_chunks b_chunks c_chunks c_nt = a_chunks b_chunks c_chunks nt = torch nested nested_tensor b c chunked = nt chunk dim=- assertEqual chunked torch nested nested_tensor a_nt assertEqual chunked torch nested nested_tensor b_nt assertEqual chunked torch nested nested_tensor c_nt chunk chunked assertFalse chunk is_contiguous Failure chunking ragged dimensions assertRaisesRegex RuntimeError Chunk nested tensors currently only supported last dimension lambda torch chunk nt dim= assertRaisesRegex RuntimeError Chunk nested tensors currently only supported last dimension lambda torch chunk nt dim= Failure non-contiguous nt _ nt_noncontiguous = random_nt_noncontiguous_pair device dtype assertRaisesRegex RuntimeError chunk expects ` ` contiguous lambda torch chunk nt_noncontiguous dim=- Failure when calling non divisible n_chunks assertRaisesRegex RuntimeError Chunk nested tensors only supported nested tensors trailing dimension divisible chunks lambda torch chunk nt dim=- Failure when calling backward chunk = torch randn device=device dtype=dtype requires_grad=True b = torch randn device=device dtype=dtype requires_grad=True nt_grad = torch nested as_nested_tensor b chunked = torch chunk nt_grad dim=- assertRaisesRegex RuntimeError Nested Strided Tensor doesn t support chunk backward lambda chunked backward chunked clone dtypes floating_types_and_half test_nested_tensor_split_with_sizes device dtype = torch randn device=device dtype=dtype b = torch randn device=device dtype=dtype c = torch randn device=device dtype=dtype split_sizes = a_splits = split_with_sizes split_sizes dim=- b_splits = b split_with_sizes split_sizes dim=- c_splits = c split_with_sizes split_sizes dim=- nt = torch nested nested_tensor b c nt_splits = nt split_with_sizes split_sizes dim=- i nt_split enumerate nt_splits assertEqual nt_split torch nested nested_tensor a_splits i b_splits i c_splits i dense_strides = torch stack torch tensor a_splits i stride torch tensor b_splits i stride torch tensor c_splits i stride assertEqual nt_split _nested_tensor_strides dense_strides assertFalse nt_split is_contiguous Failure calling ragged dimensions assertRaisesRegex RuntimeError split_with_sizes nested tensors currently only supported last dimension lambda torch split_with_sizes nt split_sizes dim= Failure calling non-last dimension assertRaisesRegex RuntimeError split_with_sizes nested tensors currently only supported last dimension lambda torch split_with_sizes nt split_sizes dim= Failure non-contiguous nt _ nt_noncontiguous = random_nt_noncontiguous_pair device dtype assertRaisesRegex RuntimeError split_with_sizes expects ` ` contiguous lambda torch split_with_sizes nt_noncontiguous split_sizes dim=- Failure when calling split_sizes don t cover full dim size bad_split_sizes = don t add up assertRaisesRegex RuntimeError split_with_sizes expects split_sizes sum exactly lambda torch split_with_sizes nt bad_split_sizes dim=- dtypes torch float torch float torch double torch inference_mode test_nested_tensor_indexing_noncontiguous device dtype nt_contiguous nt_noncontiguous = random_nt_noncontiguous_pair device dtype assertEqual nt_contiguous size nt_noncontiguous size n = nt_contiguous size i range n assertEqual nt_contiguous i nt_noncontiguous i dtypes torch float torch float skipMeta torch inference_mode parametrize transpose True False test_nested_tensor_add device dtype transpose transpose = torch randn device=device dtype=dtype b = torch rand device=device dtype=dtype c = transpose - - contiguous d = b transpose - - contiguous nt = torch nested nested_tensor b b nt = torch nested nested_tensor c d c d transpose - - nt nt = random_nt_pair device dtype ref = torch nested nested_tensor t + t t t zip nt unbind nt unbind out = nt + nt assertEqual ref out dtypes torch float torch float skipMeta torch inference_mode parametrize transpose True False test_nested_tensor_sub device dtype transpose transpose = torch randn device=device dtype=dtype b = torch rand device=device dtype=dtype c = transpose - - contiguous d = b transpose - - contiguous nt = torch nested nested_tensor b b nt = torch nested nested_tensor c d c d transpose - - nt nt = random_nt_pair device dtype ref = torch nested nested_tensor t - t t t zip nt unbind nt unbind out = nt - nt assertEqual ref out onlyCUDA dtypes torch float torch float torch inference_mode parametrize embedding_dim test_nested_tensor_dense_elementwise device dtype embedding_dim _test_add_mul nt t ref_add = torch nested nested_tensor t + t t t zip nt unbind t unbind ref_mul = torch nested nested_tensor t t t t zip nt unbind t unbind assertEqual nt add t ref_add assertEqual nt mul t ref_mul batch_size = seq_lens = torch randint low= high= size= batch_size B D B D case ts = torch randn seq_len embedding_dim seq_len seq_lens nt = torch nested nested_tensor ts device=device dtype=dtype t = torch randn batch_size embedding_dim device=device dtype=dtype _test_add_mul nt t B B case ts = torch randn seq_len seq_len seq_lens nt = torch nested nested_tensor ts device=device dtype=dtype t = torch randn batch_size device=device dtype=dtype _test_add_mul nt t dtypes torch float torch float skipMeta torch inference_mode test_nested_tensor_mul device dtype nested tensor nested tensor nt nt = random_nt_pair device dtype ref = torch nested nested_tensor t t t t zip nt unbind nt unbind out = nt nt assertEqual ref out nested tensor scalar number = scalar = torch tensor number dtype device ref = torch nested nested_tensor t number t nt unbind out_number = nt number out_number = number nt out_scalar = nt scalar out_scalar = scalar nt assertEqual out_number ref assertEqual out_number ref assertEqual out_scalar ref assertEqual out_scalar ref error case numel == dim vector = torch tensor number dtype device assertRaisesRegex RuntimeError Expected both other nested got nested non-nested other lambda nt mul vector assertRaisesRegex RuntimeError Expected both other nested got non-nested nested other lambda vector mul nt dtypes torch float torch float skipMeta torch inference_mode test_nested_tensor_div device dtype nt nt = random_nt_pair device dtype scale = ref = torch nested nested_tensor t scale t nt unbind out = nt assertEqual ref out ref_transposed = ref transpose out = nt transpose assertEqual ref_transposed out ref = torch nested nested_tensor t t t t zip nt unbind nt unbind out = nt nt assertEqual ref out out = nt transpose nt transpose assertEqual ref transpose out nt_transpose_copy = torch nested nested_tensor t transpose t nt unbind assertRaisesRegex RuntimeError div requires strides match when given NestedTensors lambda nt_transpose_copy transpose nt nt = torch nested nested_tensor torch randn i i device=device dtype=dtype nt_chunks = nt chunk - assertRaisesRegex RuntimeError div requires offsets match when given NestedTensors lambda nt_chunks nt_chunks dtypes torch float torch float skipMeta torch inference_mode test_nested_tensor_add_in_place device dtype nt nt = random_nt_pair device dtype ref = torch nested nested_tensor t + t t t zip nt unbind nt unbind nt += nt assertEqual ref nt dtypes torch float torch float skipMeta torch inference_mode test_nested_tensor_mul_in_place device dtype nested tensor nested tensor nt nt = random_nt_pair device dtype ref = torch nested nested_tensor t t t t zip nt unbind nt unbind nt = nt assertEqual ref nt nested tensor scalar number = scalar = torch tensor number dtype device ref = torch nested nested_tensor t number t nt unbind out_number = nt clone out_number = number out_scalar = nt clone out_scalar = scalar assertEqual out_number ref assertEqual out_scalar ref assertRaisesRegex RuntimeError r output shape \ \ doesn t match broadcast shape \ \ lambda scalar mul_ nt error case numel == dim vector = torch tensor number dtype device assertRaisesRegex RuntimeError Expected both other nested got nested non-nested other lambda nt mul_ vector assertRaisesRegex RuntimeError Expected both other nested got non-nested nested other lambda vector mul_ nt onlyCPU skipMeta dtypes torch float test_nested_tensor_sum_dim device dtype params = test_sum device dtype ntensors max_sizes dim keepdim=True nt = random_nt device dtype ntensors max_sizes require_non_empty=False nt = nt clone ub = nt unbind nt requires_grad_ True t requires_grad_ True t ub nt_sum = nt sum dim=dim keepdim=keepdim ub _sum = t sum - keepdim=keepdim t ub assertEqual nt_sum torch nested nested_tensor ub _sum test backward generate gradient tensor has same size output size = nt_sum _nested_tensor_size gt = i range ntensors gt append torch randn size i tolist device=device dtype=dtype gt = torch nested nested_tensor gt clone nt_sum backward gt t g zip ub _sum gt t backward g assertEqual nt grad torch nested nested_tensor t grad t ub ntensors max_sizes params test_sum device dtype ntensors max_sizes len max_sizes Test error inputs assertRaisesRegex RuntimeError NestedTensor can only reduced across last torch nested nested_tensor torch tensor torch tensor sum keepdim=True assertRaisesRegex RuntimeError NestedTensor only allows reduction single torch nested nested_tensor torch tensor torch tensor sum keepdim=True assertRaisesRegex RuntimeError NestedTensor always requires keepdim=True now torch nested nested_tensor torch tensor torch tensor sum - dtypes torch float torch float test_contiguous device dtype Since we don t have access buffer python harder show what we testing When we call chunk consistent dim NT chunk_size resulting tensors views original NT whose numels now less than size buffer Clone previously creating new NT buffer same size original nt_contiguous = torch nested nested_tensor torch randn device=device dtype=dtype torch randn device=device dtype=dtype Split up last dimension which has consistent size into chunks chunks = nt_contiguous chunk dim=- Check chunks contiguous after calling contiguous chunk chunks assertFalse chunk is_contiguous assertTrue chunk contiguous is_contiguous dtypes torch float torch float skipMeta test_clone device dtype nt = random_nt device dtype nt = nt clone Verify values match assertEqual nt nt Verify modifying nt doesn t affect nt nt mul_ nt ub = nt unbind ub = nt unbind i range len ub assertNotEqual ub i ub i nt clone memory_format=torch preserve_format msg = Nested tensor clone supports Preserve Contiguous memory formats called clone memory format ChannelsLast assertRaisesRegex RuntimeError msg nt clone memory_format=torch channels_last cannot test torch float because RuntimeError bernoulli_scalar_cpu_ implemented Half decorateIf xfailIfTorchDynamo lambda params params layout == torch jagged dtypes torch float torch double parametrize layout torch strided torch jagged name_fn=layout_name test_dropout device dtype layout edge case empty nested tensor TODO support empty NT jagged layout layout == torch strided nt = torch nested nested_tensor layout=layout y = torch nn functional dropout nt assertEqual nt y normal nested tensor ntensors = layout == torch jagged nt = random_nt device dtype ntensors layout=layout nt = random_nt device dtype ntensors layout=layout edge case invalid dropout assertRaises ValueError lambda torch nn Dropout - assertRaises ValueError lambda torch nn Dropout assertRaises ValueError lambda torch nn functional dropout nt - assertRaises ValueError lambda torch nn functional dropout nt edge case no dropout dropouter = torch nn Dropout y = dropouter nt y = torch nn functional dropout nt assertEqual nt y assertEqual nt y edge case all dropout dropouter = torch nn Dropout y = dropouter nt y = torch nn functional dropout nt nt = torch zeros_like nt assertEqual nt y assertEqual nt y normal case normal dropout p = y = torch nn functional dropout nt p expect = nt clone layout == torch jagged expect = torch where y == y nt expect = - p assertEqual y expect expect = nt clone i range ntensors actual_tensor = y i view - expect_tensor = expect i view - j range actual_tensor shape actual_tensor j item == expect_tensor j = expect_tensor j = - p assertEqual y expect freeze_rng_state dropouter = torch nn Dropout p y = dropouter nt freeze_rng_state y = torch nn functional dropout nt p assertEqual y y dtypes torch float torch double test_dropout_noncontiguous device dtype ntensors = nt = random_nt device dtype ntensors nt = nt transpose - - p = freeze_rng_state dropouter = torch nn Dropout p y = dropouter nt freeze_rng_state y = torch nn functional dropout nt p transpose - - assertEqual y y cannot test torch float because RuntimeError softmax_kernel_impl implemented Half dtypes torch float torch double test_softmax device dtype normal nested tensor ntensors = nt = random_nt device dtype ntensors error case softmax across nested dimension assertRaisesRegex RuntimeError Cannot apply softmax across nested dimension lambda torch nn functional softmax nt assertRaisesRegex RuntimeError Cannot apply softmax across nested dimension lambda torch nn functional softmax nt - error case dimension out range assertRaises IndexError lambda torch nn functional softmax nt assertRaises IndexError lambda torch nn functional softmax nt - normal case should equal padding -inf softmaxer = torch nn Softmax y = softmaxer nt y = torch nn functional softmax nt assertEqual y y pt = torch nested to_padded_tensor nt float -inf entire slice padded then softmax will = nan however physically speaking should expect = torch nn functional softmax pt nan_to_num_ assertEqual torch nested to_padded_tensor y expect edge case empty nested tensor nt = torch nested nested_tensor y = torch nn functional softmax nt assertEqual nt y edge case nesting scalars nt = torch nested nested_tensor torch tensor torch tensor assertRaises RuntimeError lambda torch nn functional softmax nt assertRaises IndexError lambda torch nn functional softmax nt dtypes torch float torch double torch inference_mode test_softmax_noncontiguous device dtype nt_contiguous nt_noncontiguous = random_nt_noncontiguous_pair device dtype assertEqual torch nn functional softmax nt_contiguous - torch nn functional softmax nt_noncontiguous - _test_bmm device dtype error case D tensors nt = torch nested nested_tensor device=device dtype=dtype nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype assertRaisesRegex RuntimeError batch must D tensor lambda nt bmm nt assertRaisesRegex RuntimeError batch must D tensor lambda nt bmm nt assertRaisesRegex RuntimeError batch must D tensor lambda nt bmm nt assertRaisesRegex RuntimeError batch must D tensor lambda nt bmm nt assertRaisesRegex RuntimeError batch must D tensor lambda nt bmm nt assertRaisesRegex RuntimeError batch must D tensor lambda nt bmm nt assertRaisesRegex RuntimeError batch must D tensor lambda nt bmm nt assertRaisesRegex RuntimeError batch must D tensor lambda nt bmm nt error case incompatible batch size nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype nt = torch nested nested_tensor torch randn torch randn torch randn device=device dtype=dtype assertRaisesRegex RuntimeError Expected size st dimension batch tensor got lambda nt bmm nt assertRaisesRegex RuntimeError Expected size st dimension batch tensor got lambda nt bmm nt error case underlying matrices cannot multiplied nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype assertRaisesRegex RuntimeError r -th nested matrices batch cannot multiplied \ x x \ lambda nt bmm nt normal nested tensor nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype actual = torch nested to_padded_tensor nt bmm nt expect = torch nested to_padded_tensor nt bmm torch nested to_padded_tensor nt dtype == torch float assertEqual actual expect rtol= e- atol= e- assertEqual actual expect nested tensor bmm normal tensor nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype nt = torch rand dtype=dtype device=device actual = torch nested to_padded_tensor nt bmm nt expect = torch nested to_padded_tensor nt bmm nt dtype == torch float assertEqual actual expect rtol= e- atol= e- assertEqual actual expect nested tensor bmm normal tensor non-contiguous view nt = torch rand dtype=dtype device=device nt = nt transpose actual = torch nested to_padded_tensor nt bmm nt expect = torch nested to_padded_tensor nt bmm nt dtype == torch float assertEqual actual expect rtol= e- atol= e- assertEqual actual expect normal tensor bmm nested tensor nt = torch rand dtype=dtype device=device nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype actual = torch nested to_padded_tensor nt bmm nt expect = nt bmm torch nested to_padded_tensor nt dtype == torch float assertEqual actual expect rtol= e- atol= e- assertEqual actual expect test tensorcore path nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype actual = torch nested to_padded_tensor nt bmm nt expect = torch nested to_padded_tensor nt bmm torch nested to_padded_tensor nt dtype == torch float assertEqual actual expect rtol= e- atol= e- assertEqual actual expect onlyCUDA dtypes torch float torch double torch float torch bfloat tf _on_and_off test_bmm_cuda device dtype _test_bmm device dtype onlyCPU cannot test torch float because RuntimeError addmm_impl_cpu_ implemented Half dtypes torch float torch double test_bmm_cpu device dtype _test_bmm device dtype cannot test torch float because RuntimeError addmm_impl_cpu_ implemented Half dtypes torch float torch double test_bmm_noncontiguous device dtype nt _contiguous nt _noncontiguous = random_nt_noncontiguous_pair device dtype nt _contiguous nt _noncontiguous = random_nt_noncontiguous_pair device dtype assertEqual nt _contiguous transpose - - bmm nt _contiguous nt _noncontiguous transpose - - bmm nt _noncontiguous dtypes torch float torch double tf _on_and_off test_matmul_with_bmm_path device dtype unbind_rebind_matmul nt nt t s = nt unbind t s = nt unbind out_ts = t matmul t t t zip t s t s torch nested nested_tensor out_ts N n_head head_dim N n_head head_dim Ns = n_heads = np random randint head_dim = t s = t s = N Ns _ range N seq_len = np random randint seq_len = np random randint t s append torch randn n_heads seq_len head_dim t s append torch randn n_heads head_dim seq_len nt = torch nested nested_tensor t s device=device dtype=dtype nt = torch nested nested_tensor t s device=device dtype=dtype assertEqual torch matmul nt nt unbind_rebind_matmul nt nt test noncontiguous t s = t s = _ range N seq_len = np random randint t s append torch randn seq_len n_heads head_dim t s append torch randn seq_len n_heads head_dim nt = torch nested nested_tensor t s device=device dtype=dtype transpose nt = torch nested nested_tensor t s device=device dtype=dtype transpose transpose assertEqual torch matmul nt nt unbind_rebind_matmul nt nt cannot test torch float because RuntimeError bmm implemented Half dtypes torch float torch double test_matmul device dtype error case one nested other nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype t = torch randn device=device dtype=dtype assertRaisesRegex RuntimeError Expected both nested got nested non-nested other lambda torch matmul nt t assertRaisesRegex RuntimeError Expected both nested got non-nested nested other lambda torch matmul t nt error case +D tensors nt = torch nested nested_tensor device=device dtype=dtype nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype assertRaisesRegex RuntimeError r matmul For nested tensors only inputs = dims currently supported st input has rank - + lambda torch matmul nt nt assertRaisesRegex RuntimeError r matmul For nested tensors only inputs = dims currently supported st input has rank - + lambda torch matmul nt nt assertRaisesRegex RuntimeError r matmul For nested tensors only inputs = dims currently supported st input has rank - + lambda torch matmul nt nt assertRaisesRegex RuntimeError r matmul For nested tensors only inputs = dims currently supported st input has rank - + lambda torch matmul nt nt assertRaisesRegex RuntimeError r matmul For nested tensors only inputs = dims currently supported st input has rank - + lambda torch matmul nt nt assertRaisesRegex RuntimeError r matmul For nested tensors only inputs = dims currently supported st input has rank - + lambda torch matmul nt nt assertRaisesRegex RuntimeError r matmul For nested tensors only inputs = dims currently supported nd input has rank - + lambda torch matmul nt nt assertRaisesRegex RuntimeError r matmul For nested tensors only inputs = dims currently supported nd input has rank - + lambda torch matmul nt nt error case incompatible batch size nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype nt = torch nested nested_tensor torch randn torch randn torch randn device=device dtype=dtype assertRaisesRegex RuntimeError r matmul Expected size st dimension nd input tensor - + got - + lambda torch matmul nt nt assertRaisesRegex RuntimeError r matmul Expected size st dimension nd input tensor - + got - + lambda torch matmul nt nt error case incompatible wrong batch sizes shouldn t even broadcast nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype assertRaisesRegex RuntimeError matmul For nested tensors batch dimensions must have same sizes lambda torch matmul nt nt error case incompatible batch sizes should technically broadcast nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype assertRaisesRegex RuntimeError matmul For nested tensors batch dimensions must have same sizes lambda torch matmul nt nt error case underlying matrices cannot multiplied nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype assertRaisesRegex RuntimeError matmul Nested tensors cannot matrix multiplied lambda torch matmul nt nt normal nested tensor D nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype actual = torch nested to_padded_tensor torch matmul nt nt expect = torch matmul torch nested to_padded_tensor nt torch nested to_padded_tensor nt assertEqual actual expect normal nested tensor D testing batch_size= nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype actual = torch nested to_padded_tensor torch matmul nt nt expect = torch matmul torch nested to_padded_tensor nt torch nested to_padded_tensor nt assertEqual actual expect normal nested tensor D nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype actual = torch nested to_padded_tensor torch matmul nt nt expect = torch matmul torch nested to_padded_tensor nt torch nested to_padded_tensor nt assertEqual actual expect only supported CUDA now dtypes torch float torch double test_matmul_nt_with_broadcasted_t device dtype NT B C D T D E broadcasting case nt = random_nt_from_dims None device=device dtype=dtype t = torch randn device=device dtype=dtype output = torch matmul nt t should equivalent matmul-ing each component dense tensor assertEqual nt size output size component out_component zip nt output assertEqual out_component torch matmul component t cannot test torch float because RuntimeError bmm implemented Half dtypes torch float torch double test_matmul_noncontiguous device dtype nt _contiguous nt _noncontiguous = random_nt_noncontiguous_pair device dtype nt _contiguous nt _noncontiguous = random_nt_noncontiguous_pair device dtype assertEqual torch matmul nt _contiguous transpose - - nt _contiguous torch matmul nt _noncontiguous transpose - - nt _noncontiguous dtypes torch float torch double test_linear device dtype = torch randn device=device dtype=dtype b = torch randn device=device dtype=dtype c = torch randn device=device dtype=dtype nt = torch nested nested_tensor b c weight = torch randn device=device dtype=dtype bias = torch randn device=device dtype=dtype success case torch functional F linear nt weight bias invalid nested tensor dimension msg = r Linear requires nested_tensor dim == dense_matrix dim == Nested tensor dim Dense tensor dim nt = torch nested nested_tensor torch randn device=device dtype=dtype torch randn device=device dtype=dtype assertRaisesRegex RuntimeError msg torch functional F linear nt weight bias invalid weight shape msg = r Linear requires nested_tensor dim == dense_matrix dim == Nested tensor dim Dense tensor dim weight = torch randn device=device dtype=dtype assertRaisesRegex RuntimeError msg torch functional F linear nt weight bias inconsistent last dim nested tensor msg = r Expected all tensors nested tensor have same trailing dimension instead last dimension equals nt = torch nested nested_tensor torch randn device=device dtype=dtype torch randn device=device dtype=dtype assertRaisesRegex RuntimeError msg torch functional F linear nt weight bias Mismatch nested tensor last dim weight dimension weight = torch randn device=device dtype=dtype msg = r Shape mismatch NestedTensor Linear Expected input s \ nested tensor\ last_dim r equal weight size\ \ got last_dim = weight size\ \ = assertRaisesRegex RuntimeError msg torch functional F linear nt weight bias Nested tensor input nested weight nt_weight = nt clone msg = r Linear does support nested weight when input nested tensor assertRaisesRegex RuntimeError msg torch functional F linear nt nt_weight bias TODO test noncontiguous linear For now tests error message linear since linear does support noncontiguous buffer yet dtypes torch float torch double test_linear_noncontiguous device dtype nt_contiguous nt_noncontiguous = random_nt_noncontiguous_pair device dtype weight = torch randn device=device dtype=dtype assertRaisesRegex RuntimeError r now linear only supports contiguous nested tensor lambda torch nn functional linear nt_noncontiguous weight dtypes torch float torch float torch double test_to_padded_tensor_zero_numel_errors device dtype ts = torch ones torch ones nt = torch nested nested_tensor ts device=device dtype=dtype layout=torch strided assertRaisesRegex RuntimeError r least one constituent tensor should have non-zero numel lambda torch nested to_padded_tensor nt dtypes torch float torch float torch double test_transpose device dtype nt = random_nt device dtype error case transpose nested dimension assertRaisesRegex RuntimeError Nested tensor dimension cannot transposed lambda nt transpose assertRaisesRegex RuntimeError Nested tensor dimension cannot transposed lambda nt transpose - error case dimension out range assertRaises IndexError lambda nt transpose assertRaises IndexError lambda nt transpose - - normal case ntT = nt transpose - - ptT_from_ntT = noncontiguous_to_padded_tensor ntT pt = torch nested to_padded_tensor nt ptT = pt transpose - - assertEqual ptT ptT_from_ntT dtypes torch float torch float torch double test_squeeze_unsqueeze device dtype = torch arange reshape b = torch arange reshape nt = torch nested nested_tensor b device=device dtype=dtype error case squeeze no dimension assertRaisesRegex RuntimeError For nested tensors squeeze without dim argument lambda nt squeeze error case squeeze nested dimension assertRaisesRegex RuntimeError For nested tensors squeezing dimension lambda nt squeeze error case dimension out range assertRaises IndexError lambda nt squeeze error case squeeze nested tensor singleton tensors c = torch ones nt_singleton = torch nested nested_tensor c c device=device dtype=dtype assertRaisesRegex RuntimeError For nested tensors squeezing nested tensor singleton lambda nt_singleton squeeze squeezing dim which does have size should no-op nt = nt squeeze - assertEqual nt nt test cases should work nt_sizes = nt _nested_tensor_size nt_strides = nt _nested_tensor_strides i range - i == cannot unsqueeze batch dim continue nt_unsqueezed = nt unsqueeze i negative dim will correspond unsqueeze applied dim = dim + nt dim + wrapped_i = i + nt dim + i i col_index into nt size tensor requires subtraction ignore batch dim size_idx = wrapped_i - assertEqual nt_unsqueezed _nested_tensor_size size_idx torch ones dtype=torch long unsqueezed_stride = nt_unsqueezed _nested_tensor_strides size_idx i == nt ndim i == - assertEqual unsqueezed_stride torch ones dtype=torch long stride_col_after = nt_strides size_idx size_col_after = nt_sizes size_idx assertEqual unsqueezed_stride stride_col_after size_col_after nt_squeezed = nt_unsqueezed squeeze i assertEqual nt_squeezed nt assertEqual nt_squeezed _nested_tensor_size nt_sizes assertEqual nt_squeezed _nested_tensor_strides nt_strides dtypes torch float torch float torch double test_transpose_inference_mode_interaction device dtype nt = random_nt device dtype Construct default mode transpose while inference mode torch inference_mode ntT = nt transpose - - ptT_from_ntT = noncontiguous_to_padded_tensor ntT pt = torch nested to_padded_tensor nt ptT = pt transpose - - assertEqual ptT ptT_from_ntT Construct transpose while inference mode torch inference_mode nt = random_nt device dtype ntT = nt transpose - - ptT_from_ntT = noncontiguous_to_padded_tensor ntT pt = torch nested to_padded_tensor nt ptT = pt transpose - - assertEqual ptT ptT_from_ntT dtypes torch float torch float torch double test_view device dtype nt = random_nt device dtype error case empty shape assertRaisesRegex RuntimeError r shape \ \ invalid nested tensor lambda nt view error case empty nested tensor nt_empty = torch nested nested_tensor assertRaisesRegex RuntimeError empty nested tensor cannot reshaped lambda nt_empty view - error case - batch size assertRaisesRegex RuntimeError r view For now nested view cannot change infer implicit batch dimension lambda nt view - assertRaisesRegex RuntimeError r shape \ \ invalid input size - + lambda nt view normal case x = torch randn device=device dtype=dtype x = torch randn device=device dtype=dtype nt = torch nested nested_tensor x x pt = torch nested to_padded_tensor nt error case trying reshape batch dim legit shape assertRaisesRegex RuntimeError r For now nested view cannot change infer implicit batch dimension lambda nt transpose - - view - inherit only ragged dimension - - nt = nt view - - - pt = pt view - assertEqual noncontiguous_to_padded_tensor nt pt more than one - even old dims should fail attempts do - we ban inherit old behavior dimension assertRaisesRegex RuntimeError r only one dimension can inferred lambda nt view - - dtypes torch float torch float torch double test_view_inference_mode_interaction device dtype Construct default mode view while inference mode nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype torch inference_mode ntT = nt view - ptT_from_ntT = noncontiguous_to_padded_tensor ntT pt = torch nested to_padded_tensor nt ptT = pt view - assertEqual ptT ptT_from_ntT Construct view while inference mode torch inference_mode nt = torch nested nested_tensor torch randn torch randn device=device dtype=dtype ntT = nt view - ptT_from_ntT = noncontiguous_to_padded_tensor ntT pt = torch nested to_padded_tensor nt ptT = pt view - assertEqual ptT ptT_from_ntT dtypes torch float torch float torch double test_reshape device dtype nt = random_nt device dtype error case empty shape assertRaisesRegex RuntimeError r shape \ \ invalid nested tensor lambda nt reshape error case empty nested tensor nt_empty = torch nested nested_tensor assertRaisesRegex RuntimeError empty nested tensor cannot reshaped lambda nt_empty reshape - error case - batch size assertRaisesRegex RuntimeError r reshape For now nested reshape cannot change infer implicit batch dimension lambda nt reshape - assertRaisesRegex RuntimeError r shape \ \ invalid input size - + lambda nt reshape normal case x = torch randn device=device dtype=dtype x = torch randn device=device dtype=dtype nt = torch nested nested_tensor x x pt = torch nested to_padded_tensor nt error case trying reshape batch dim legit shape assertRaisesRegex RuntimeError r reshape For now nested reshape cannot change infer implicit batch dimension lambda nt transpose - - reshape - inherit only ragged dimension - - nt = nt reshape - - - pt = pt reshape - assertEqual noncontiguous_to_padded_tensor nt pt more than one - even old dims should fail attempts do - we ban inherit old behavior dimension assertRaisesRegex RuntimeError r only one dimension can inferred lambda nt reshape - - test_nested_masked_select device t = torch randn device=device mask = torch tensor False device=device njt = torch nested masked_select t mask assertEqual njt values torch tensor device=device assertEqual njt offsets torch tensor device=device mask = torch tensor False False True device=device njt = torch nested masked_select t mask assertEqual njt values t - atol= rtol= assertEqual njt offsets torch tensor device=device mask = torch tensor False False True True False True False False True device=device njt = torch nested masked_select t mask assertEqual njt values t masked_select mask assertEqual njt offsets torch tensor device=device t = torch randn device=device mask = torch tensor True False True True False True True False True False True True False True True True True True device=device njt = torch nested masked_select t mask assertEqual njt values t masked_select mask assertEqual njt offsets torch tensor device=device dtypes torch float torch float torch double test_narrow device dtype nt = random_nt_from_dims None None None device=device dtype=dtype narrow dim= start end bounds = start end bounds length = end - start narrowed = nt narrow dim= start=start length=length ensure output view assertTrue narrowed _base nt nc c zip narrowed unbind nt unbind start end assertEqual nc c dim = supported dim range nt dim assertRaisesRegex RuntimeError only dim= supported nested tensors nt narrow dim=dim start= length= error case non-contiguous NT _ nt_noncont = random_nt_noncontiguous_pair assertRaisesRegex RuntimeError only contiguous nested tensors supported nt_noncont narrow dim= start= length= parametrize input_dim tf _on_and_off test_scaled_dot_product_attention device input_dim rand_tensor shape torch randn shape device=device E = input_dim == Shape N L E ragged L query = torch nested nested_tensor rand_tensor E rand_tensor E rand_tensor E Shape N S E ragged S key = torch nested nested_tensor rand_tensor E rand_tensor E rand_tensor E value = torch nested nested_tensor rand_tensor E rand_tensor E rand_tensor E input_dim == In D case L S ragged Shape N N L E ragged N L query = torch nested nested_tensor rand_tensor E rand_tensor E rand_tensor E Shape N N S E ragged N S key = torch nested nested_tensor rand_tensor E rand_tensor E rand_tensor E value = torch nested nested_tensor rand_tensor E rand_tensor E rand_tensor E fail f Invalid input_dim input_dim encountered SDP test rand_mask size torch randint size=size dtype=torch bool device=device Shape N L S ragged L S matching above attn_mask = torch nested nested_tensor rand_mask rand_mask rand_mask dropout_p = no dropout reproducibility Success case no attn_mask set is_causal=False actual = torch nn functional scaled_dot_product_attention query key value attn_mask=None is_causal=False dropout_p=dropout_p expected_outputs = q k v zip query unbind key unbind value unbind output = torch nn functional scaled_dot_product_attention q unsqueeze k unsqueeze v unsqueeze attn_mask=None dropout_p=dropout_p expected_outputs append output squeeze expected_output_nested = torch nested nested_tensor expected_outputs assertEqual actual expected_output_nested Error case explicit attn_mask set assertRaisesRegex RuntimeError supported when explicit attn_mask set torch nn functional scaled_dot_product_attention query key value attn_mask=attn_mask dropout_p=dropout_p Error case is_causal=True assertRaisesRegex RuntimeError supported when is_causal=True torch nn functional scaled_dot_product_attention query key value dropout_p=dropout_p is_causal=True dtypes torch float torch float torch double test_empty_like device dtype ntensors = nt = random_nt device dtype ntensors Create empty same device original nested tensor nt_empty = torch empty_like nt assert nt is_same_size nt_empty assertEqual nt dtype nt_empty dtype assertEqual nt device nt_empty device assertEqual nt layout nt_empty layout torch cuda is_available device == cpu nt_cuda = torch empty_like nt device= cuda assertEqual torch device cuda type nt_cuda device type nt_cpu = torch empty_like nt device= cpu assertEqual torch device cpu type nt_cpu device type Check changing dtype empty_like nested tensor output dtype_set = torch float torch float torch double other_dtype dtype_set - dtype nt_empty_other_dtype = torch empty_like nt dtype=other_dtype assertEqual nt dtype dtype assertEqual nt_empty_other_dtype dtype other_dtype assertEqual nt device nt_empty device assertEqual nt layout nt_empty layout Create tensor autograd nt_empty_req_grad = torch empty_like nt requires_grad=True assertEqual nt_empty_req_grad requires_grad True Test noncontiguous tensor does fail copy nt_cont nt_noncont = random_nt_noncontiguous_pair nt_empty = torch empty_like nt_cont assert nt_cont is_same_size nt_empty nt_empty_non_contig = torch empty_like nt_noncont assert nt_noncont is_same_size nt_empty_non_contig Test contiguous memory format option nt_empty_contig = torch empty_like nt_cont memory_format=torch contiguous_format assert nt_cont is_same_size nt_empty_contig assert nt_empty_contig is_contiguous nt_empty_non_contig = torch empty_like nt_noncont memory_format=torch contiguous_format assert nt_noncont is_same_size nt_empty_non_contig assert nt_empty_non_contig is_contiguous Test other memory formats fail assertRaises RuntimeError lambda torch empty_like nt_cont memory_format=torch channels_last assertRaises RuntimeError lambda torch empty_like nt_noncont memory_format=torch channels_last assertRaises RuntimeError lambda torch empty_like nt_cont memory_format=torch channels_last_ d assertRaises RuntimeError lambda torch empty_like nt_noncont memory_format=torch channels_last_ d markDynamoStrictTest TestNestedTensorAutograd NestedTensorTestCase Note Gradcheck args check_batched_grad=False common_utils testing version gradcheck includes default parameters used testing ops gradcheck However nested tensor does support stack op therefore we turn off these tests _create_leaf_nested_tensor_from_list tensor_device requires_grad=False torch nested nested_tensor torch randn torch randn requires_grad=requires_grad device=tensor_device _create_nested_tensor_from_list tensor_device requires_grad=False torch nested as_nested_tensor torch randn requires_grad=requires_grad torch randn requires_grad=requires_grad device=tensor_device _create_nested_tensor_from_mask tensor_device requires_grad=False data = torch randn requires_grad=requires_grad device=tensor_device mask = torch ones_like data bool torch _nested_tensor_from_mask data mask test_as_nested_tensor_propagates_gradients device = torch arange dtype=torch float device=device b = torch arange dtype=torch float device=device nt = torch nested as_nested_tensor b tensors requires_grad=False leaves assertTrue nt is_leaf assertTrue nt requires_grad = torch arange dtype=torch float requires_grad=True device=device b = torch arange dtype=torch float requires_grad=True device=device nt = torch nested as_nested_tensor b fake_grad = torch nested nested_tensor torch ones_like torch zeros_like b device=device nt backward fake_grad assertEqual grad fake_grad assertEqual b grad fake_grad test_nested_tensor_generates_leaf device = torch arange dtype=torch float requires_grad=True device=device b = torch arange dtype=torch float requires_grad=True device=device nt = torch nested nested_tensor b requires_grad=False assertTrue nt is_leaf assertTrue nt requires_grad nt = torch nested nested_tensor b requires_grad=True assertTrue nt is_leaf assertTrue nt requires_grad fake_grad = torch nested nested_tensor torch ones_like torch zeros_like b device=device nt backward fake_grad assertEqual nt grad fake_grad assertEqual grad None assertEqual b grad None test_set_requires_grad_from_list device nt = _create_nested_tensor_from_list device nt requires_grad_ assert nt requires_grad test_set_requires_grad_from_mask device nt = _create_nested_tensor_from_mask device nt requires_grad_ assert nt requires_grad test_backward_for_add_op device nt_ = _create_nested_tensor_from_mask device nt_ = _create_nested_tensor_from_mask device nt_ requires_grad_ c = nt_ + nt_ assert nt_ requires_grad assert c requires_grad grad_output = _create_nested_tensor_from_mask device c backward grad_output Grad check doesn t work nested yet d dnt_ nt + nt_ = grad_output assertEqual nt_ grad grad_output test_backward_for_sub_op device nt_ = _create_nested_tensor_from_mask device nt_ = _create_nested_tensor_from_mask device nt_ requires_grad_ nt_ requires_grad_ c = nt_ - nt_ assert nt_ requires_grad assert nt_ requires_grad assert c requires_grad grad_output = _create_nested_tensor_from_mask device c backward grad_output assertEqual nt_ grad grad_output assertEqual nt_ grad - grad_output test_backward_sub_strided device = torch nested nested_tensor torch randn torch randn requires_grad=True device=device b = torch nested nested_tensor torch randn torch randn requires_grad=True device=device c = - b transpose - - grad_output = c clone c backward grad_output assertEqual grad grad_output assertEqual b grad - grad_output transpose - - test_backward_add_strided device = torch nested nested_tensor torch randn torch randn requires_grad=True device=device b = torch nested nested_tensor torch randn torch randn requires_grad=True device=device c = + b transpose - - grad_output = c clone c backward grad_output assertEqual grad grad_output assertEqual b grad grad_output transpose - - Test Factory Functions test_nested_tensor_to_padded_tensor device padding_val nt = _create_leaf_nested_tensor_from_list tensor_device=device requires_grad=True out = torch nested to_padded_tensor nt padding_val grad_output = torch ones out shape device=device out backward grad_output assertEqual nt grad torch nested nested_tensor torch ones torch ones device=device test_nested_tensor_from_mask_and_to_padded device N L D = mask = torch ones N L device=device i range N end = torch randint L - device=device mask i end = mask = mask = mask bool data = torch randn N L D requires_grad=True dtype=torch float device=device grad_test_func inpt nt = torch _nested_tensor_from_mask inpt mask This implicitly tests to_padded_tensor grads torch nested to_padded_tensor nt assert gradcheck grad_test_func inputs=data check_batched_grad=False test_nested_tensor_from_padded device nested_size = torch tensor padded_tensor = torch randn dtype=torch float device=device padded_tensor = padded_tensor requires_grad_ grad_test_func tensor nested_size nt = torch _nested_from_padded tensor nested_size fuse_transform_ =False This implicitly tests to_padded_tensor grads torch nested to_padded_tensor nt data = padded_tensor nested_size assert gradcheck grad_test_func inputs=data check_batched_grad=False test_nested_tensor_from_padded_fused device nested_size = torch tensor padded_tensor = torch randn dtype=torch float device=device padded_tensor = padded_tensor requires_grad_ grad_test_func tensor nested_size nt = torch _nested_from_padded tensor nested_size fuse_transform_ =True This implicitly tests to_padded_tensor grads torch nested to_padded_tensor nt data = padded_tensor nested_size assert gradcheck grad_test_func inputs=data check_batched_grad=False test_nested_tensor_from_list device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c c = torch nested as_nested_tensor b c This implictily tests to_padded_tensor grads torch nested to_padded_tensor c data = b c assert gradcheck grad_test_func inputs=data check_batched_grad=False parametrize layout torch strided torch jagged name_fn=layout_name test_dropout_backward layout layout == torch jagged nt = torch nested nested_tensor torch randn torch randn requires_grad=True layout=layout nt = torch nested nested_tensor torch randn torch randn requires_grad=True layout=layout p = y = torch nn functional dropout nt p y backward nt detach clone assertEqual nt grad y test_nested_tensor_bmm_gradcheck device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device d = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c d nt = torch nested as_nested_tensor b nt = torch nested as_nested_tensor c d result = nt bmm nt torch nested to_padded_tensor result data = b c d assert torch autograd gradcheck grad_test_func inputs=data tf _on_and_off test_nested_tensor_bmm_backward device nt = torch nested nested_tensor torch randn torch randn requires_grad=True device=device nt = torch nested nested_tensor torch randn torch randn requires_grad=True device=device torch no_grad pt = torch nested to_padded_tensor nt requires_grad_ True pt = torch nested to_padded_tensor nt requires_grad_ True ynt = nt bmm nt ypt = pt bmm pt ynt backward ynt clone ypt backward ypt clone assertEqual torch nested to_padded_tensor nt grad pt grad assertEqual torch nested to_padded_tensor nt grad pt grad test_nested_tensor_matmul_gradcheck device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device d = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c d nt = torch nested as_nested_tensor b nt = torch nested as_nested_tensor c d result = torch matmul nt nt torch nested to_padded_tensor result data = b c d assert torch autograd gradcheck grad_test_func inputs=data test_nested_tensor_matmul_backward device nt = torch nested nested_tensor torch randn torch randn requires_grad=True device=device nt = torch nested nested_tensor torch randn torch randn requires_grad=True device=device torch no_grad pt = torch nested to_padded_tensor nt requires_grad_ True pt = torch nested to_padded_tensor nt requires_grad_ True ynt = torch matmul nt nt ypt = torch matmul pt pt ynt backward ynt clone ypt backward ypt clone assertEqual torch nested to_padded_tensor nt grad pt grad assertEqual torch nested to_padded_tensor nt grad pt grad test_nested_tensor_transpose_gradcheck device = torch randn requires_grad=True device=device b = torch randn requires_grad=True device=device grad_test_func b nt = torch nested as_nested_tensor b result = nt transpose - - transpose - - torch nested to_padded_tensor result data = b assert torch autograd gradcheck grad_test_func inputs=data eps= e- test_nested_tensor_transpose_backward device nt = torch nested nested_tensor torch randn torch randn requires_grad=True device=device torch no_grad pt = torch nested to_padded_tensor nt requires_grad_ True ynt = nt transpose - - ypt = pt transpose - - ynt backward ynt clone ypt backward ypt clone assertEqual torch nested to_padded_tensor nt grad pt grad test_nested_tensor_reshape_gradcheck device = torch randn requires_grad=True device=device b = torch randn requires_grad=True device=device grad_test_func b nt = torch nested as_nested_tensor b result = nt reshape - torch nested to_padded_tensor result data = b assert torch autograd gradcheck grad_test_func inputs=data eps= e- test_nested_tensor_reshape_backward nt = torch nested nested_tensor torch randn torch randn requires_grad=True torch no_grad pt = torch nested to_padded_tensor nt requires_grad_ True ynt = nt reshape - ypt = pt reshape - ynt backward ynt clone ypt backward ypt clone assertEqual torch nested to_padded_tensor nt grad pt grad test_nested_tensor_squeeze_backward device nt = torch nested nested_tensor torch randn torch randn requires_grad=True device=device torch no_grad pt = torch nested to_padded_tensor nt requires_grad_ True ynt = nt squeeze - ypt = pt squeeze - ynt backward ynt clone ypt backward ypt clone assertEqual torch nested to_padded_tensor nt grad pt grad test_nested_tensor_squeeze_gradcheck device = torch randn dtype=torch float requires_grad=True device=device b = torch randn dtype=torch float requires_grad=True device=device grad_test_func b nt = torch nested as_nested_tensor b result = nt squeeze - torch nested to_padded_tensor result assert torch autograd gradcheck grad_test_func inputs= b eps= e- test_nested_tensor_unsqueeze_backward device nt = torch nested nested_tensor torch randn torch randn requires_grad=True device=device torch no_grad pt = torch nested to_padded_tensor nt requires_grad_ True ynt = nt unsqueeze ypt = pt unsqueeze ynt backward ynt clone ypt backward ypt clone assertEqual torch nested to_padded_tensor nt grad pt grad test_nested_tensor_unsqueeze_gradcheck device = torch randn dtype=torch float requires_grad=True device=device b = torch randn dtype=torch float requires_grad=True device=device grad_test_func b nt = torch nested as_nested_tensor b result = nt unsqueeze - torch nested to_padded_tensor result assert torch autograd gradcheck grad_test_func inputs= b eps= e- test_nested_tensor_linear device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device weight = torch randn requires_grad=True dtype=torch float device=device bias = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c weight bias=None nt = torch nested as_nested_tensor b c This implicitly tests to_padded_tensor grads d = torch functional F linear nt weight bias torch nested to_padded_tensor d data = b c weight bias assert gradcheck grad_test_func inputs=data check_batched_grad=False Test linear no bias added data = b c weight assert gradcheck grad_test_func inputs=data check_batched_grad=False test_nested_tensor_linear_plus_transpose device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device weight = torch randn requires_grad=True dtype=torch float device=device bias = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c weight bias=None nt = torch nested as_nested_tensor b c This implicitly tests to_padded_tensor grads d = torch functional F linear nt weight bias d = d transpose - - contiguous torch nested to_padded_tensor d data = b c weight bias assert gradcheck grad_test_func inputs=data check_batched_grad=False Test linear no bias added data = b c weight assert gradcheck grad_test_func inputs=data check_batched_grad=False test_nested_tensor_softmax device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c dim nt = torch nested as_nested_tensor b c This implicitly tests to_padded_tensor grads d = torch functional F softmax nt dim=dim torch nested to_padded_tensor d softmax over last dim data = b c - assert gradcheck grad_test_func inputs=data check_batched_grad=False test_nested_tensor_linear_backward device = torch randn requires_grad=False device=device b = torch randn requires_grad=False device=device c = torch randn requires_grad=False device=device weight = torch randn requires_grad=True device=device bias = torch randn requires_grad=True device=device nt = torch nested as_nested_tensor b c device=device out = torch functional F linear nt weight bias out backward out clone assert weight grad None assert bias grad None assert grad None assert b grad None assert c grad None test_values_grad_with_broadcast device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c nt = torch nested as_nested_tensor b c buffer = nt values buffer sum data = b c assert gradcheck grad_test_func inputs=data check_batched_grad=False test_to_buffer_series_ops_grad_with_broadcast device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c nt = torch nested as_nested_tensor b c buffer = nt values buffer = buffer buffer exp data = b c assert gradcheck grad_test_func inputs=data check_batched_grad=False test_unbind_flow_through device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c nt = torch nested as_nested_tensor b c ntT = nt transpose - - unbound = ntT unbind d = unbound d = torch pow d d data = b c assert gradcheck grad_test_func inputs=data check_batched_grad=False test_split_with_sizes_flow_through device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c nt = torch nested as_nested_tensor b c splits = nt split_with_sizes dim=- unbound = splits unbind d = unbound d = torch pow d d data = b c assert gradcheck grad_test_func inputs=data check_batched_grad=False test_indexing_backward device x = torch randn x = torch randn nt = torch nested nested_tensor x x device=device requires_grad=True assertEqual nt x assertEqual nt - x grad_x = torch randn device=device nt backward grad_x expected_grad = torch nested nested_tensor grad_x torch zeros device=device assertEqual nt grad expected_grad test_masked_fill_backward device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c nt = torch nested as_nested_tensor b c mask = nt detach clone bool out = nt masked_fill mask out = torch nested to_padded_tensor out out data = b c assert gradcheck grad_test_func inputs=data check_batched_grad=False test_gelu_backward device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c nt = torch nested as_nested_tensor b c nt_gelu = torch nn functional gelu nt torch nested to_padded_tensor nt_gelu data = b c assert gradcheck grad_test_func inputs=data check_batched_grad=False test_relu_backward device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c nt = torch nested as_nested_tensor b c nt_relu = torch nn functional relu nt torch nested to_padded_tensor nt_relu data = b c assert gradcheck grad_test_func inputs=data check_batched_grad=False test_selu_backward device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c nt = torch nested as_nested_tensor b c nt_relu = torch nn functional silu nt torch nested to_padded_tensor nt_relu data = b c assert gradcheck grad_test_func inputs=data check_batched_grad=False test_abs_backward device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c nt = torch nested as_nested_tensor b c nt_abs = torch abs nt torch nested to_padded_tensor nt_abs data = b c assert gradcheck grad_test_func inputs=data check_batched_grad=False Previously would error when input NT doesn t require grad NotImplementedError Cannot access storage UndefinedTensorImpl test_layer_norm_backward_edge_case device size = = torch randn size requires_grad=False dtype=torch float device=device nt = torch nested nested_tensor nt_layer_norm = torch nn LayerNorm nt size - device=device dtype=torch float out = nt_layer_norm nt out backward out clone test_accumulate_grad_different_strides device = torch rand requires_grad=True dtype=torch float device=device b = torch rand requires_grad=True dtype=torch float device=device grad_test_func b nt_ = torch nested as_nested_tensor b nt_ = nt_ clone out = torch nn functional scaled_dot_product_attention nt_ nt_ nt_ torch nested to_padded_tensor out data = b assert gradcheck grad_test_func inputs=data check_batched_grad=False https github com pytorch pytorch issues skipIfSlowGradcheckEnv parametrize size test_layer_norm_backward device size = torch randn size requires_grad=True dtype=torch float device=device b = torch randn size requires_grad=True dtype=torch float device=device c = torch randn size requires_grad=True dtype=torch float device=device grad_test_func b c nt = torch nested as_nested_tensor b c layer_norm = torch nn LayerNorm nt size - device=device dtype=torch float nt_layer_norm = layer_norm nt torch nested to_padded_tensor nt_layer_norm data = b c assert gradcheck grad_test_func inputs=data check_batched_grad=False https github com pytorch pytorch issues skipIfSlowGradcheckEnv Could either mark slow reduce size parametrize size test_layer_norm_backward_ d device size = torch randn size size requires_grad=True dtype=torch float device=device b = torch randn size size requires_grad=True dtype=torch float device=device c = torch randn size size requires_grad=True dtype=torch float device=device grad_test_func b c nt = torch nested as_nested_tensor b c layer_norm = torch nn LayerNorm size size nt size - device=device dtype=torch float nt_layer_norm = layer_norm nt torch nested to_padded_tensor nt_layer_norm data = b c assert gradcheck grad_test_func inputs=data check_batched_grad=False Found torch testing _comparison py default_atol = torch float e- torch bfloat e- torch float e- default_rtol = torch float e- torch bfloat e- torch float e- get_rtol true_value torch Tensor computed_value torch Tensor - float deviation = true_value - computed_value deviation = torch abs deviation true_value Fill nans default rtol torch nan_to_num_ deviation nan=default_rtol computed_value dtype deviation max item get_atol true_value torch Tensor computed_value torch Tensor - float deviation = true_value - computed_value atol = torch abs deviation max item atol get_tolerances true_value torch Tensor computed_value torch Tensor fudge_factor Optional float = None - tuple float float Returns absolute relative tolerances comparing two tensors fudge_factor = fudge_factor fudge_factor None atol = get_atol true_value computed_value rtol = get_rtol true_value computed_value atol = fudge_factor max atol default_atol computed_value dtype rtol = fudge_factor max rtol default_rtol computed_value dtype torch isclose has weird behavior around see https github com pytorch pytorch issues rtol e rtol = default_rtol computed_value dtype atol rtol We can probably parametrizing existing tests instead having separate test we begin support more ops Also maybe rewrite OpInfos markDynamoStrictTest TestNestedTensorSubclass NestedTensorTestCase TODO consolidate below _get_list_for_jagged_tensor nested_size device requires_grad=True Ds = nested_size out = s nested_size out append torch randn s Ds requires_grad=requires_grad device=device dtype=torch float out _get_example_tensor_lists include_list_of_lists=True include_requires_grad=True include_inner_dim_size_ =False include_ d_tensor=False _make_tensor shape include_requires_grad=include_requires_grad requires_grad=True torch randn shape requires_grad= requires_grad include_requires_grad False Purposefully introduce mixed requires_grad settings components when include_requires_grad=True example_lists = B D B= _make_tensor _make_tensor requires_grad=False _make_tensor requires_grad=False _make_tensor B D_ D_ B= _make_tensor _make_tensor _make_tensor requires_grad=False _make_tensor _make_tensor B D_ D_ D_ B= _make_tensor _make_tensor _make_tensor requires_grad=False _make_tensor _make_tensor _make_tensor include_list_of_lists example_lists append B D B= list form _make_tensor requires_grad=False tolist _make_tensor tolist _make_tensor tolist include_inner_dim_size_ example_lists append _make_tensor _make_tensor requires_grad=False _make_tensor requires_grad=False _make_tensor B example_lists append _make_tensor _make_tensor requires_grad=False _make_tensor requires_grad=False _make_tensor B include_ d_tensor example_lists append _make_tensor _make_tensor requires_grad=False _make_tensor requires_grad=False _make_tensor B example_lists dtypes torch float parametrize contiguity contig noncontig_transposed noncontig_with_holes name_fn=lambda c c parametrize weights_only True False test_serialization device dtype contiguity weights_only Test cases contiguous non-contiguous transposed non-contiguous holes contiguity == contig nt = random_nt_from_dims None device=device dtype=dtype layout=torch jagged contiguity == noncontig_transposed nt = random_nt_from_dims None device=device dtype=dtype layout=torch jagged transpose - - contiguity == noncontig_with_holes nt = torch nested nested_tensor_from_jagged values=torch randn device=device dtype=dtype offsets=torch tensor device=device dtype=torch int these lengths specify holes lengths=torch tensor device=device dtype=torch int raise ValueError invalid contiguity specified test_serialization Access sizes strides ensure cache doesn t break serialization See https github com pytorch pytorch issues nt size nt stride tempfile TemporaryFile f torch save nt f f seek nt_loaded = torch load f weights_only=weights_only assertIsNot nt nt_loaded we expect new offsets tensor - different nested int upon load assertEqualIgnoringNestedInts nt nt_loaded assertEqual nt _ragged_idx nt_loaded _ragged_idx ensure shapes equal except nested int nt_rest_of_shape = nt shape nt _ragged_idx nt shape nt _ragged_idx + nt_loaded_rest_of_shape = nt_loaded shape nt_loaded _ragged_idx nt_loaded shape nt_loaded _ragged_idx + assertEqual nt_rest_of_shape nt_loaded_rest_of_shape ensure metadata cache carried through serialization assertEqual nt _metadata_cache nt_loaded _metadata_cache ensure lengths carried through present assertEqual nt _lengths nt_loaded _lengths test_tensor_attributes device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device nt = torch nested as_nested_tensor b c layout=torch jagged _offsets = nt offsets op torch ops aten is_non_overlapping_and_dense default torch ops aten sym_size default torch ops aten dim default torch ops aten numel default torch ops aten sym_numel default torch ops aten sym_stride default torch ops aten sym_storage_offset default op nt assertRaisesRegex RuntimeError directly calling torch ops aten size torch ops aten size default nt nested_int = torch nested _internal nested_tensor get_tensor_symint _offsets coeff= assertEqual nt size nested_int assertEqual nt shape nested_int assertEqual nt dim assertEqual nt numel parametrize nt_dim test_linear device nt_dim nt_dim == fixed_shape = nt_dim == fixed_shape = nt_dim == fixed_shape = = torch randn fixed_shape requires_grad=True dtype=torch float device=device b = torch randn fixed_shape requires_grad=True dtype=torch float device=device c = torch randn fixed_shape requires_grad=True dtype=torch float device=device weight = torch randn requires_grad=True dtype=torch float device=device bias = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c weight bias nt = torch nested as_nested_tensor b c layout=torch jagged out = torch nn functional linear nt weight bias out values gradcheck grad_test_func inputs= b c weight bias check_batched_grad=False onlyCUDA dtypes torch float serialTest test_linear_backward_memory_usage device dtype Verify linear_backward doesn t use more memory than should higher dim input sizes See https github com pytorch pytorch issues B D max_seq_len = torch _C _cuda_clearCublasWorkspaces m = torch nn Linear D D device=device nt = torch nested as_nested_tensor torch rand size= seq_len D seq_len torch randint max_seq_len size= B layout=torch jagged device=device B j D - B j D higher dim input size nt = nt unsqueeze - linear_backward should explode max memory usage torch cuda reset_max_memory_allocated m nt sum backward expect under GB max memory allocated max_after_gb = torch cuda max_memory_allocated assertEqual max_after_gb test_unary_pointwise device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device grad_test_func b c nt = torch nested as_nested_tensor b c layout=torch jagged out = torch nn functional silu nt sin cos out values gradcheck grad_test_func inputs= b c check_batched_grad=False test_unary_pointwise_transposed_inputs device b c = torch randn i + requires_grad=True dtype=torch float device=device i range nt = torch nested nested_tensor detach b detach c detach layout=torch jagged nt_t = nt transpose assertFalse nt_t is_contiguous out = torch nn functional silu nt_t sin cos assertEqual out is_contiguous torch nn functional silu b transpose - - sin cos is_contiguous assertEqual nt_t shape out shape b c = torch randn i + requires_grad=True dtype=torch float device=device i range grad_test_func b c nt = torch nested as_nested_tensor b c layout=torch jagged nt_t = nt transpose out = torch nn functional silu nt_t sin cos out values gradcheck grad_test_func inputs= b c check_batched_grad=False test_binary_pointwise device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device Incorrect usage shape check will fail offsets tensor same exact tensor object nt = torch nested as_nested_tensor b c layout=torch jagged nt = torch nested as_nested_tensor b c layout=torch jagged assertRaisesRegex RuntimeError cannot call binary pointwise function inputs shapes lambda nt nt Correct usage chain calls using same offsets tensor object grad_test_func b c nt = torch nested as_nested_tensor b c layout=torch jagged TODO Switch public API takes values offsets once exists nt offsets = jagged_from_list b c nt offsets out = nt nt out values gradcheck grad_test_func inputs= b c check_batched_grad=False test_binary_pointwise_transposed device b c = torch randn i + dtype=torch float device=device i range nt offsets = jagged_from_list b c None nt offsets = jagged_from_list b c offsets nt _t = nt transpose nt _t = nt transpose out = nt _t nt _t assertFalse nt _t is_contiguous assertEqual out is_contiguous b transpose - - b transpose - - is_contiguous assertEqual out shape nt _t shape assertRaisesRegex RuntimeError cannot call binary pointwise function mul Tensor inputs shapes lambda nt nt _t b c = torch randn i + requires_grad=True dtype=torch float device=device i range Correct usage chain calls using same offsets tensor object grad_test_func b c nt offsets = jagged_from_list b c None nt offsets = jagged_from_list b c offsets nt _t = nt transpose nt _t = nt transpose out = nt _t nt _t out values gradcheck grad_test_func inputs= b c check_batched_grad=False test_binary_pointwise_with_nested_int_second_arg device See https github com pytorch pytorch issues nt = random_nt_from_dims None device=device dtype=torch float layout=torch jagged assertRaisesRegex RuntimeError invalid argument nt nt size assertRaisesRegex RuntimeError invalid argument nt + nt size test_split device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device nt = torch nested as_nested_tensor b c layout=torch jagged out = torch split nt - assertEqual len out assertEqualIgnoringNestedInts out torch nested as_nested_tensor b c layout=torch jagged assertEqualIgnoringNestedInts out torch nested as_nested_tensor b c layout=torch jagged assertRaisesRegex RuntimeError r split\ \ supported NestedTensor ragged dim torch split nt test_split_with_sizes device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device nt = torch nested as_nested_tensor b c layout=torch jagged out = torch split nt - assertEqual len out assertEqualIgnoringNestedInts out torch nested as_nested_tensor b c layout=torch jagged assertEqualIgnoringNestedInts out torch nested as_nested_tensor b c layout=torch jagged assertRaisesRegex RuntimeError r split_with_sizes\ \ supported NestedTensor ragged dim torch split nt test_softmax device nt = random_nt_from_dims None device=device dtype=torch float layout=torch jagged requires_grad=True operate dim= output = nt softmax dim= torch _dynamo disable _compare_to_ref nt output dim in_component out_component zip nt unbind output unbind assertEqual in_component softmax dim=dim out_component dim= - dim= after unbind _compare_to_ref nt output dim= operate dim=- output = nt softmax dim=- torch _dynamo disable assertEqual output output _compare_to_ref nt output dim=- grad_test_func b nt = torch nested as_nested_tensor b layout=torch jagged out = nt softmax dim=- out values = torch rand requires_grad=True dtype=torch float device=device b = torch rand requires_grad=True dtype=torch float device=device gradcheck grad_test_func inputs= b check_batched_grad=False test_views_inherit_ragged_dim device view nt = random_nt_from_dims None device=device dtype=torch float layout=torch jagged inherit ragged dim via - view = nt view - assertEqual nt shape view shape inherit batch ragged dims via - view = nt view - - assertEqual nt shape view shape expand nt = random_nt_from_dims None device=device dtype=torch float layout=torch jagged inherit batch ragged dims via - view = nt expand - - assertEqual nt shape view shape test_view_ragged_idx_not_one device nt = random_nt_from_dims None device=device dtype=torch float layout=torch jagged view_transposed = nt transpose view nt size assertEqual nt size view_transposed size assertEqual view_transposed _base nt _base test_unsafe_view device nt = random_nt_from_dims None device=device dtype=torch float layout=torch jagged basic view view = torch ops aten _unsafe_view nt - assertEqual nt size tuple view size _unsafe_view differs view view information tracked assertTrue view _base None test unsafe_view when ragged_idx = currently only supports identity view nt_t = nt transpose view = torch ops aten _unsafe_view nt_t nt size assertEqual nt size tuple view size assertTrue view _base None xfailIfTorchDynamo parametrize requires_grad False True test_reshape_decomp device requires_grad contiguous NT should result view nt = random_nt_from_dims None device=device dtype=torch float layout=torch jagged detach requires_grad_ requires_grad view = nt reshape - - assertEqual view shape nt shape assertTrue view _is_view view _base nt make sure gradients flow back requires_grad view backward torch ones_like view assertEqual nt grad torch ones_like nt non-contiguous NT should result contiguous copy nt = random_nt_from_dims None device=device dtype=torch float layout=torch jagged requires_grad=requires_grad nt_noncontig = nt transpose - - assertFalse nt_noncontig is_contiguous copy = nt_noncontig reshape - - assertTrue copy is_contiguous assertEqual copy shape nt shape make sure gradients flow back requires_grad copy backward torch ones_like copy assertEqual nt grad torch ones_like nt test_flatten_decomp device nt = random_nt_from_dims None device=device dtype=torch float layout=torch jagged flattened = nt flatten - - assertEqual flattened shape nt view - shape nt = random_nt_from_dims None device=device dtype=torch float layout=torch jagged flattened = nt flatten - - assertEqual flattened shape nt view - shape test_chunk device none NJT case t = torch randn requires_grad=True t_list = t chunk dim= loss = t_list sum + t_list sum loss backward normal case D = B = nt = random_nt_from_dims B None D device=device dtype=torch float layout=torch jagged requires_grad=True NUM_CHUNKS = chunks = nt chunk NUM_CHUNKS dim=- assertEqual len chunks NUM_CHUNKS i range NUM_CHUNKS assertEqual chunks i shape - D NUM_CHUNKS test chunk_backward values = torch randn dtype=torch float device=device requires_grad=True offsets = torch tensor device=device grad_test_func values offsets nt = torch nested nested_tensor_from_jagged values offsets chunks = nt chunk dim=- chunks values sum assert gradcheck grad_test_func inputs= values offsets check_batched_grad=False chunk batch dim chunks = nt chunk NUM_CHUNKS dim= assertEqual len chunks NUM_CHUNKS chunk_size = math ceil B NUM_CHUNKS i range NUM_CHUNKS i NUM_CHUNKS - assertEqual chunks i shape chunk_size assertEqual chunks i shape B - chunk_size NUM_CHUNKS - offsets_expected = nt _offsets i chunk_size + i + chunk_size + - nt _offsets i chunk_size assertEqual chunks i _offsets offsets_expected assertEqual nt _values torch cat x _values x chunks dim= doesn t support backward chunk dim= yet loss = chunks values sum + chunks values sum + chunks values sum loss backward chunk ragged dim supported assertRaisesRegex RuntimeError chunk supported NestedTensor ragged dim nt chunk dim= test_squeeze device B = D = squeeze middle dim nt = random_nt_from_dims B None D device=device dtype=torch float layout=torch jagged j = nt shape dim_arg - out = nt squeeze dim_arg assertEqual out shape B j D assertEqual out unsqueeze - nt squeeze last dim nt = random_nt_from_dims B None device=device dtype=torch float layout=torch jagged j = nt shape dim_arg - out = nt squeeze dim_arg assertEqual out shape B j assertEqual out unsqueeze - nt squeeze batch dim supported assertRaisesRegex RuntimeError squeeze supported NestedTensor dim= nt squeeze squeeze ragged dim supported assertRaisesRegex RuntimeError squeeze supported NestedTensor ragged dim nt squeeze test_binary_pointwise_broadcasting device B j ts = _get_list_for_jagged_tensor device requires_grad=True B j + - B j B j + - B j B j + - B j Unsupported B j + - B j t_sizes = unsupported today grad_test_func t ts nt = torch nested as_nested_tensor list ts layout=torch jagged out = nt + t out values t_size t_sizes t = torch rand t_size requires_grad=True device=device dtype=torch float gradcheck grad_test_func inputs= t ts check_batched_grad=False test_threshold_backward device ts = _get_list_for_jagged_tensor device=device requires_grad=False ts = _get_list_for_jagged_tensor device=device requires_grad=False nt offsets = jagged_from_list ts None nt offsets = jagged_from_list ts offsets buf = nt values detach clone buf = nt values detach clone res_nt = torch ops aten threshold_backward nt nt res_dense = torch ops aten threshold_backward buf buf assertEqual res_dense res_nt values onlyCUDA dtypes torch float test_record_stream device dtype _create_nt values = torch ones device= cuda offsets = torch tensor device= cuda dtype=torch int lengths = offsets diff nt = torch nested nested_tensor_from_jagged values offsets lengths data_ptrs = nt _values data_ptr nt _offsets data_ptr nt _lengths data_ptr nt data_ptrs fn record_stream nt data_ptrs = _create_nt s = torch cuda Stream torch cuda stream s emulate doing something long via sleep per_ms = e torch cuda _sleep int per_ms record_stream nt record_stream s data_ptrs expect memory reuse when record_stream run data_ptrs = fn record_stream=False nt nt_data_ptrs = _create_nt assertEqual data_ptrs nt_data_ptrs del nt torch cuda synchronize expect memory preserved no reuse when record_stream run data_ptrs = fn record_stream=True nt nt_data_ptrs = _create_nt assertEqual len data_ptrs intersection nt_data_ptrs dtypes torch float parametrize func torch ops aten sum dim_IntList torch ops aten mean dim name_fn=get_op_name parametrize keepdim False True parametrize requires_grad False True parametrize components_require_grad False True test_jagged_op_different_output_shape_dim device dtype keepdim requires_grad components_require_grad func Operator passes when reducing valid reduction dimensions This test operators which output tensor shape different input tensor get_op_name func == mean keepdim op_name = get_op_name func ts = _get_list_for_jagged_tensor device=device requires_grad=True B j verify correctness shapes assuming ragged_idx == op_name == sum reduce_dims = batch ragged None None non-batch non-batch batch ragged non-batch batch ragged non-batch batch ragged non-batch non-batch None None non-batch dims expected shape expected keepdim shape reduce_dim_expected where j represented None op_name == mean reduce_dims = None None None None rd ref_shape_no_keepdim ref_shape_keepdim _ reduce_dims nt = torch nested as_nested_tensor ts layout=torch jagged out = func nt dim=rd keepdim=keepdim ref_shape = ref_shape_keepdim keepdim ref_shape_no_keepdim torch compiler is_compiling using torch dynamo assertEqual len out shape len ref_shape o r zip out shape ref_shape r None assertEqual o r assertTrue isinstance o torch SymInt verify correctness values tensor_lists = _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad include_inner_dim_size_ =True tensor_list reduce_dim_tuple itertools product tensor_lists reduce_dims nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad reduce_dim _ _ reduce_dim_expected = reduce_dim_tuple nt dim reduce_dim - out_actual = func nt dim=reduce_dim keepdim=keepdim nt _ragged_idx reduce_dim raggedness reduced away out_expected = func nt values dim=reduce_dim_expected keepdim=keepdim assertTrue torch allclose out_actual out_expected raggedness preserved out_expected = func nt values dim=reduce_dim_expected assertTrue torch allclose out_actual values view - out_expected view - dtypes torch float parametrize requires_grad False True parametrize components_require_grad False True parametrize func torch nn functional softmax torch nn functional log_softmax name_fn=lambda func func __name__ test_softmax_dim device dtype requires_grad components_require_grad func Softmax passes when reducing valid reduction dimensions ts = _get_list_for_jagged_tensor device=device requires_grad=True B j output_shape = None verify correctness shapes assuming ragged_idx == reduce_dims = reduction dimension effective reduction dimension baseline reduce_dim _ reduce_dims nt = torch nested as_nested_tensor ts layout=torch jagged out_actual = func nt dim=reduce_dim torch _dynamo disable assertEqual len out_actual shape len output_shape disable running dynamo dim_actual dim_expected zip out_actual shape output_shape dim_expected None assertEqual dim_actual dim_expected assertTrue isinstance dim_actual torch SymInt verify correctness values tensor_lists = _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad include_inner_dim_size_ =True tensor_list reduce_dim_tuple itertools product tensor_lists reduce_dims nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad reduce_dim reduce_dim_expected = reduce_dim_tuple nt dim reduce_dim nested tensor out_actual = func nt dim=reduce_dim dense tensor dimensions less than out_actual out_expected = func nt values dim=reduce_dim_expected assertTrue torch allclose out_actual values view - out_expected view - dtypes torch float parametrize func torch ops aten sum dim_IntList torch ops aten mean dim name_fn=get_op_name parametrize keepdim False True parametrize requires_grad False True parametrize components_require_grad False True test_op_dim_reduce_ragged_idx_ _different_output_shape device dtype keepdim requires_grad components_require_grad func Operator NestedTensor passes when trying reduce across ragged dimension where ragged_idx == This test operators which output tensor shape different input tensor get_op_name func == mean keepdim op_name = get_op_name func tensor_lists = _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad include_inner_dim_size_ =True B reduce_dim = ragged tensor_list tensor_lists nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad out_actual = func nt dim=reduce_dim keepdim=keepdim out_expected = torch cat func t dim= reduce_dim - unsqueeze t nt unbind keepdim out_expected = out_expected unsqueeze reduce_dim assertFalse out_actual is_nested f op_name result reducing nested tensor along ragged dimension dense tensor output dense tensor assertEqual out_actual out_expected dtypes torch float parametrize requires_grad False True parametrize components_require_grad False True test_softmax_dim_reduce_ragged_idx_ device dtype requires_grad components_require_grad Softmax NestedTensor passes when trying reduce across ragged dimension where ragged_idx == tensor_lists = _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad include_inner_dim_size_ =True B include_ d_tensor=True B reduce_dim = ragged tensor_list tensor_lists nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad out_actual = torch nn functional softmax nt dim=reduce_dim out_expected = torch cat torch nn functional softmax t dim=reduce_dim - t nt unbind assertTrue out_actual is_nested softmax result reducing nested tensor along ragged dimension nested tensor output nested tensor assertTrue torch allclose out_actual values out_expected dtypes torch float parametrize requires_grad False True parametrize components_require_grad False True parametrize func torch nn functional softmax torch nn functional log_softmax name_fn=lambda func func __name__ test_softmax_reduce_batch_dim device dtype requires_grad components_require_grad func Softmax NestedTensor fails when trying reduce across batch dimension tensor_lists = _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad include_inner_dim_size_ =True B reduce_dim = batch tensor_list tensor_lists nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad assertRaisesRegex RuntimeError supported when reducing across batch dimension NestedTensor out = func nt dim=reduce_dim dtypes torch float parametrize requires_grad False True parametrize components_require_grad False True test_layer_norm_reduce_ragged_idx_ device dtype requires_grad components_require_grad Layer normalization NestedTensor passes when trying normalize across ragged dimension where ragged_idx == requires_grad = False does currently work dynamo tests throws error AssertionError SymInts must use SymNodeVariable If underlying value static we will create ConstantVariable specialize torch _dynamo is_compiling requires_grad tensor_lists = _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad include_inner_dim_size_ =True B tensor_list tensor_lists nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad nt dim = layer norm only works tensors more dimensions normalized_shape = nt shape nt _ragged_idx out_actual = torch nn functional layer_norm nt normalized_shape=normalized_shape out_expected = torch cat torch nn functional layer_norm t normalized_shape=t shape t nt unbind e g D tensor B M performs layer normalization B D tensors M assertTrue out_actual is_nested layer_norm result reducing nested tensor along ragged dimension nested tensor output nested tensor assertEqual out_actual _values shape out_expected shape assertTrue torch allclose out_actual values out_expected dtypes torch float parametrize requires_grad False True parametrize components_require_grad False True test_layer_norm_ d_input device dtype requires_grad components_require_grad Layer normalization NestedTensor fails when trying operate -dimensional tensor tensor_lists = _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad include_inner_dim_size_ =True B include_ d_tensor=True B tensor_list tensor_lists nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad nt dim = assertRaisesRegex RuntimeError supported NestedTensor objects fewer dimensions out = torch nn functional layer_norm nt normalized_shape= nt shape nt _ragged_idx dtypes torch float parametrize requires_grad False True parametrize components_require_grad False True test_layer_norm_operate_on_batch_dim device dtype requires_grad components_require_grad Layer normalization NestedTensor fails when trying operate batch dimension tensor_lists = _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad include_inner_dim_size_ =True B include_ d_tensor=True B tensor_list tensor_lists nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad nt dim cannot perform layer normalization D tensors assertRaisesRegex RuntimeError supported when normalizing over batch dimension NestedTensor out = torch nn functional layer_norm nt normalized_shape=nt shape dtypes torch float parametrize func torch ops aten sum dim_IntList torch ops aten mean dim name_fn=get_op_name parametrize transpose_offset transpose consecutive dimensions transpose nonconsecutive dimensions parametrize keepdim False True parametrize requires_grad False True parametrize components_require_grad False True test_op_dim_reduce_ragged_idx_greater_than_ _different_output_shape device dtype keepdim requires_grad components_require_grad func transpose_offset Operator NestedTensor passes when trying reduce across transposed ragged dimension i e ragged_idx This test operators which output tensor shape different input tensor get_op_name func == mean keepdim op_name = get_op_name func tensor_lists = _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad include_inner_dim_size_ =True B include_ d_tensor=True B tensor_list tensor_lists nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad nt dim nt _ragged_idx + transpose_offset nt_transposed = nt transpose nt _ragged_idx nt _ragged_idx + transpose_offset reduce_dim = nt_transposed _ragged_idx ragged out_actual = func nt_transposed dim=reduce_dim keepdim=keepdim out_expected = torch cat func t dim= reduce_dim - unsqueeze t nt_transposed unbind keepdim out_expected = out_expected unsqueeze reduce_dim assertFalse out_actual is_nested f op_name result reducing nested tensor along ragged dimension dense tensor output dense tensor assertEqual out_actual out_expected dtypes torch float parametrize transpose_offset transpose consecutive dimensions transpose nonconsecutive dimensions parametrize requires_grad False True parametrize components_require_grad False True test_softmax_dim_reduce_ragged_idx_greater_than_ _same_output_shape device dtype requires_grad components_require_grad transpose_offset Softmax NestedTensor fails when trying reduce across transposed ragged dimension i e ragged_idx This test operators which output tensor same shape input tensor tensor_lists = _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad include_inner_dim_size_ =True B tensor_list tensor_lists nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad nt dim nt _ragged_idx + transpose_offset nt_transposed = nt transpose nt _ragged_idx nt _ragged_idx + transpose_offset reduce_dim = nt_transposed _ragged_idx ragged assertRaisesRegex RuntimeError supported when reducing along ragged dimension ragged_idx NestedTensor out = torch nn functional softmax nt_transposed dim=reduce_dim dtypes torch float parametrize func torch ops aten sum dim_IntList torch ops aten mean dim name_fn=get_op_name parametrize keepdim False True parametrize requires_grad False True parametrize components_require_grad False True test_op_dim_transpose_non_ragged_dim_different_output_shape device dtype keepdim requires_grad components_require_grad func Operator passes when reducing transposed nested tensors valid reduction dimensions This test operators which output tensor shape different input tensor get_op_name func == mean keepdim verify correctness shapes assuming ragged_idx == get_op_name func == sum reduce_dims = batch ragged None None non-batch non-batch batch ragged non-batch batch ragged non-batch batch ragged non-batch non-batch None None non-batch dims expected shape expected keepdim shape reduce_dim_expected where j represented None get_op_name func == mean reduce_dims = None None None None verify correctness values tensor_lists = _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad tensor_list reduce_dim_tuple itertools product tensor_lists reduce_dims nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad transpose - - reduce_dim _ _ reduce_dim_expected = reduce_dim_tuple nt dim max reduce_dim - nt _ragged_idx + ensure transposed dimensions non-batch non-ragged dimensions out_actual = func nt dim=reduce_dim keepdim=keepdim nt _ragged_idx reduce_dim raggedness reduced away out_expected = func nt values dim=reduce_dim_expected keepdim=keepdim assertTrue torch allclose out_actual out_expected raggedness preserved out_expected = func nt values dim=reduce_dim_expected assertTrue torch allclose out_actual values view - out_expected view - dtypes torch float parametrize requires_grad False True parametrize components_require_grad False True test_softmax_dim_transpose_non_ragged_dim device dtype requires_grad components_require_grad Softmax passes when reducing transposed nested tensors valid reduction dimensions This test operators which output tensor same shape input tensor verify correctness shapes assuming ragged_idx == reduce_dims = reduction dimension effective reduction dimension baseline verify correctness values tensor_lists = _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad include_inner_dim_size_ =True B tensor_list reduce_dim_tuple itertools product tensor_lists reduce_dims nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad transpose - - reduce_dim reduce_dim_expected = reduce_dim_tuple nt dim max reduce_dim nt _ragged_idx + out_actual = torch nn functional softmax nt dim=reduce_dim nested tensor out_expected = torch nn functional softmax nt values dim=reduce_dim_expected dense tensor dimensions less than out_actual assertTrue torch allclose out_actual values view - out_expected view - dtypes torch float parametrize keepdim False True parametrize requires_grad False True parametrize components_require_grad False True test_sum_dim_reduce_ragged_and_non_batch device dtype keepdim requires_grad components_require_grad Sum NestedTensor fails when trying reduce across ragged non-batch dimensions tensor_lists = _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad reduce_dims = ragged non-batch ragged non-batch tensor_list reduce_dim itertools product tensor_lists reduce_dims nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad nt dim reduce_dim - assertRaisesRegex RuntimeError reducing along ragged non-batch dimension supported out = torch sum nt dim=reduce_dim keepdim=keepdim dtypes torch float parametrize keepdim False True parametrize requires_grad False True parametrize components_require_grad False True test_sum_dim_reduce_batch_and_non_batch device dtype keepdim requires_grad components_require_grad Sum NestedTensor fails when trying reduce across batch non-batch dimensions tensor_lists = _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad reduce_dims = batch non-batch batch non-batch tensor_list reduce_dim itertools product tensor_lists reduce_dims nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad nt dim reduce_dim - assertRaisesRegex RuntimeError reducing along batch dimension ragged dimension + supported out = torch sum nt dim=reduce_dim keepdim=keepdim dtypes torch float parametrize func torch ops aten sum dim_IntList torch ops aten mean dim name_fn=get_op_name parametrize keepdim False True parametrize requires_grad False True parametrize components_require_grad False True test_op_dim_reduce_batch_only_different_output_shape device dtype keepdim requires_grad components_require_grad func Operator NestedTensor fails when trying reduce across batch dimension get_op_name func == mean keepdim tensor_lists = _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad reduce_dim = batch tensor_list tensor_lists nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad assertRaisesRegex RuntimeError reducing along batch dimension ragged dimension + supported out = func nt dim=reduce_dim keepdim=keepdim dtypes torch float parametrize func torch ops aten sum dim_IntList torch ops aten mean dim name_fn=get_op_name parametrize keepdim False True parametrize requires_grad False True parametrize components_require_grad False True test_op_dim_with_lengths_different_output_shape device dtype keepdim requires_grad components_require_grad func Operator NestedTensor fails when trying reduce nested tensor lengths i e nested tensor holes reducing ragged dimension This test operators which output tensor different shape than input tensor get_op_name func == mean keepdim reduce_dims = lengths = torch randint device=device offsets = torch zeros device=device dtype=torch int torch cumsum lengths dim= out=offsets values = torch randn offsets - item device=device dtype=dtype requires_grad=requires_grad nt_with_holes = torch nested nested_tensor_from_jagged values offsets lengths=offsets diff - arbitrary subtraction create holes reduce_dim reduce_dims nt_with_holes dim reduce_dim - nt_with_holes _ragged_idx reduce_dim assertRaisesRegex RuntimeError reducing across ragged dimension supported + non-contiguous nested tensors holes out = func nt_with_holes dim=reduce_dim keepdim=keepdim out = func nt_with_holes dim=reduce_dim keepdim=keepdim dtypes torch float parametrize requires_grad False True parametrize components_require_grad False True test_softmax_dim_with_lengths device dtype requires_grad components_require_grad Softmax NestedTensor fails when trying reduce nested tensor lengths i e nested tensor holes reducing ragged dimension reduce_dims = lengths = torch randint device=device offsets = torch zeros device=device dtype=torch int torch cumsum lengths dim= out=offsets values = torch randn offsets - item device=device dtype=dtype requires_grad=requires_grad nt_with_holes = torch nested nested_tensor_from_jagged values offsets lengths=offsets diff - arbitrary subtraction create holes reduce_dim reduce_dims nt_with_holes dim reduce_dim nt_with_holes _ragged_idx == reduce_dim assertRaisesRegex RuntimeError supported where lengths None + reducing across ragged dimension NestedTensor out = torch nn functional softmax nt_with_holes dim=reduce_dim out = torch nn functional softmax nt_with_holes dim=reduce_dim skipIfTorchDynamo ragged_size = nt_with_holes shape nt_with_holes _ragged_idx does currently work + dynamo tests throws error ` AssertionError SymInts must use SymNodeVariable + If underlying value static we will create ConstantVariable specialize ` dtypes torch float parametrize requires_grad False True parametrize components_require_grad False True test_layer_norm_with_lengths device dtype requires_grad components_require_grad Layer normalization NestedTensor fails when trying operate nested tensor lengths i e nested tensor holes operating ragged dimension create components nested tensor lengths = torch randint device=device offsets = torch zeros device=device dtype=torch int torch cumsum lengths dim= out=offsets values = torch randn offsets - item device=device dtype=dtype requires_grad=requires_grad nt_with_holes = torch nested nested_tensor_from_jagged values offsets lengths=offsets diff - arbitrary subtraction create holes ragged_size = nt_with_holes shape nt_with_holes _ragged_idx normalized_shapes = normalization non-ragged dimension passes ragged_size normalization ragged dimension fails normalized_shape normalized_shapes ragged_size normalized_shape assertRaisesRegex RuntimeError supported where lengths None operating ragged dimension NestedTensor out = torch nn functional layer_norm nt_with_holes normalized_shape=normalized_shape out = torch nn functional layer_norm nt_with_holes normalized_shape=normalized_shape unittest skipIf PYTORCH_CUDA_MEMCHECK is_pinned uses failure detect pointer property onlyCUDA test_pin_memory device nt_contiguous nt_noncontiguous = random_nt_noncontiguous_pair nt nt_contiguous nt_noncontiguous assertFalse nt is_pinned pinned = nt pin_memory assertTrue pinned is_pinned assertEqual nt pinned assertNotEqual nt data_ptr pinned data_ptr test pin_memory already pinned tensor has no effect assertIs pinned pinned pin_memory assertEqual pinned data_ptr pinned pin_memory data_ptr torch compiler disable _validate_nt nt device dtype layout requires_grad dim batch_size contiguous cached_min_seqlen=None cached_max_seqlen=None base=None ref_nt=None Validate bunch properties after NT construction device = torch device device assertEqual nt dim dim assertEqual nt device device assertEqual nt dtype dtype assertEqual nt layout layout assertEqual nt requires_grad requires_grad assertEqual nt is_contiguous contiguous layout == torch jagged assertEqual nt _values device device assertEqual nt _offsets device device assertEqual nt shape batch_size assertTrue isinstance nt shape torch SymInt base None assertTrue nt _is_view nt _base base replay_cache = nt _view_func torch randn_like nt _base _metadata_cache assertEqual min_seqlen replay_cache cached_min_seqlen None assertEqual max_seqlen replay_cache cached_max_seqlen None assertEqual min_seqlen nt _metadata_cache cached_min_seqlen None assertEqual max_seqlen nt _metadata_cache cached_max_seqlen None cached_min_seqlen None assertEqual nt _min_seqlen cached_min_seqlen cached_max_seqlen None assertEqual nt _max_seqlen cached_max_seqlen ref_nt None assertEqual nt size ref_nt size n n zip nt unbind ref_nt unbind assertEqual n n dtypes torch float torch double torch half parametrize requires_grad False True parametrize components_require_grad False True test_jagged_layout_construction_nested_tensor device dtype requires_grad components_require_grad tensor_list _get_example_tensor_lists include_list_of_lists=True include_requires_grad=components_require_grad nt = torch nested nested_tensor tensor_list device=device dtype=dtype layout=torch jagged requires_grad=requires_grad expected_dim = torch as_tensor tensor_list dim + expected_batch_size = len tensor_list expected_contiguous = True expected_min_seqlen = min torch tensor t isinstance t list t shape t tensor_list expected_max_seqlen = max torch tensor t isinstance t list t shape t tensor_list _validate_nt nt device dtype torch jagged requires_grad expected_dim expected_batch_size expected_contiguous expected_min_seqlen expected_max_seqlen Make sure grads -don t- flow back into original tensors nested_tensor requires_grad nt backward torch ones_like nt t tensor_list t = t isinstance t torch Tensor torch as_tensor t assertTrue t grad None dtypes torch float torch double torch half parametrize components_require_grad False True test_jagged_layout_construction_as_nested_tensor device dtype components_require_grad NB as_nested_tensor tensor_list doesn t support lists lists tensor_list tensor_list _get_example_tensor_lists include_list_of_lists=False include_requires_grad=components_require_grad nt = torch nested as_nested_tensor tensor_list device=device dtype=dtype layout=torch jagged nt requires_grad=True should set least one component requires grad expected_dim = tensor_list dim + expected_batch_size = len tensor_list expected_contiguous = True expected_min_seqlen = min torch tensor t isinstance t list t shape t tensor_list expected_max_seqlen = max torch tensor t isinstance t list t shape t tensor_list _validate_nt nt device dtype torch jagged components_require_grad expected_dim expected_batch_size expected_contiguous expected_min_seqlen expected_max_seqlen Make sure grads flow back into original tensors as_nested_tensor components_require_grad nt backward torch ones_like nt t tensor_list t requires_grad assertEqual t grad torch ones_like t assertTrue t grad None xfailIfTorchDynamo unittest skipIf PYTORCH_CUDA_MEMCHECK is_pinned uses failure detect pointer property onlyCUDA test_jagged_layout_construction_with_pinned_memory device tensor_list _get_example_tensor_lists nt = torch nested nested_tensor tensor_list layout=torch jagged device= cpu pin_memory=True expected_dim = torch as_tensor tensor_list dim + expected_batch_size = len tensor_list expected_min_seqlen = min torch tensor t isinstance t list t shape t tensor_list expected_max_seqlen = max torch tensor t isinstance t list t shape t tensor_list _validate_nt nt device= cpu dtype=torch float layout=torch jagged requires_grad=False dim=expected_dim batch_size=expected_batch_size contiguous=True cached_min_seqlen=expected_min_seqlen cached_max_seqlen=expected_max_seqlen assertTrue nt is_pinned dtypes torch float torch double torch half parametrize requires_grad False True parametrize values_is_view False True test_jagged_view_from_values_offsets device dtype requires_grad values_is_view values_is_view make values view base base = torch randn device=device dtype=dtype requires_grad=requires_grad values = base flatten - values = torch randn device=device dtype=dtype requires_grad=requires_grad offsets = torch tensor device=device dtype=torch int nt = nested_view_from_values_offsets values offsets expected_dim = values dim + expected_batch_size = offsets shape - expected_base = base values_is_view values lengths = offsets diff _validate_nt nt device dtype torch jagged requires_grad expected_dim expected_batch_size ensure NT proper view base=expected_base contiguous=True no min max passed expect metadata cache empty cached_min_seqlen=None cached_max_seqlen=None requires_grad Make sure grads flow back nt backward torch ones_like nt torch compiler disable _check_grad t assertTrue t grad None assertEqual t grad torch ones_like t _check_grad base values_is_view values dtypes torch float parametrize pass_min_max False True test_nested_tensor_from_jagged device dtype pass_min_max === construct values offsets === values = torch randn device=device dtype=dtype offsets = torch tensor device=device dtype=torch int compute min max seqlen lengths = offsets diff min_seqlen = lengths min item max_seqlen = lengths max item pass_min_max nt = torch nested nested_tensor_from_jagged values offsets=offsets min_seqlen=min_seqlen max_seqlen=max_seqlen nt = torch nested nested_tensor_from_jagged values offsets=offsets _validate_nt nt device dtype torch jagged requires_grad=False dim= batch_size= contiguous=True cached_min_seqlen= min_seqlen pass_min_max None cached_max_seqlen= max_seqlen pass_min_max None base=values === construct values offsets lengths === lengths = torch tensor device=device compute min max seqlen min_seqlen = lengths min item max_seqlen = lengths max item pass_min_max nt = torch nested nested_tensor_from_jagged values offsets=offsets lengths=lengths min_seqlen=min_seqlen max_seqlen=max_seqlen nt = torch nested nested_tensor_from_jagged values offsets=offsets lengths=lengths when both offsets lengths specified expect non-contiguous _validate_nt nt device dtype torch jagged requires_grad=False dim= batch_size= contiguous=False cached_min_seqlen= min_seqlen pass_min_max None cached_max_seqlen= max_seqlen pass_min_max None base=values assertIs nt lengths lengths === construct values lengths === values = torch randn device=device dtype=dtype lengths = torch tensor device=device compute min max seqlen min_seqlen = lengths min item max_seqlen = lengths max item pass_min_max nt = torch nested nested_tensor_from_jagged values lengths=lengths min_seqlen=min_seqlen max_seqlen=max_seqlen nt = torch nested nested_tensor_from_jagged values lengths=lengths now only lengths specified convert offsets integrate best existing kernels expected_offsets = torch tensor device=device expected_nt = torch nested nested_tensor_from_jagged values offsets=expected_offsets _validate_nt nt device dtype torch jagged requires_grad=False dim= batch_size= contiguous=True cached_min_seqlen= min_seqlen pass_min_max None cached_max_seqlen= max_seqlen pass_min_max None base=values ref_nt=expected_nt error case no offsets lengths assertRaisesRegex RuntimeError At least one offsets lengths required torch nested nested_tensor_from_jagged values offsets=None lengths=None assertRaisesRegex ValueError Expected jagged_dim = got torch nested nested_tensor_from_jagged values lengths=lengths jagged_dim= onlyCPU test_nested_tensor_from_jagged_fx_trace device fn x y torch nested nested_tensor_from_jagged x y user_unwrapped x y fn x y assertRaisesRegex RuntimeError torch nested nested_tensor_from_jagged does support tracing fx symbolic_trace torch fx symbolic_trace user_unwrapped dtypes torch float torch double torch half parametrize dim range parametrize layout torch strided torch jagged name_fn=lambda l f layout_ str l split parametrize requires_grad False True parametrize contiguous False True test_as_nested_tensor_from_tensor device dtype dim layout requires_grad contiguous dim == t = torch tensor requires_grad=requires_grad t = torch randn _ range dim requires_grad=requires_grad assert t dim == dim dim - dim tensors can t converted NTs assertRaisesRegex RuntimeError Expected tensor argument have dim nt = torch nested as_nested_tensor t device=device dtype=dtype layout=layout orig_t = t contiguous t = t transpose nt = torch nested as_nested_tensor t device=device dtype=dtype layout=layout expected_dim = t dim expected_batch_size = t size expected_seqlen = t size layout == torch jagged None _validate_nt nt device dtype layout requires_grad=requires_grad dim=dim batch_size=expected_batch_size contiguous=True cached_min_seqlen=expected_seqlen cached_max_seqlen=expected_seqlen torch device device == t device dtype == t dtype contiguous should non-copying view case assertTrue nt _is_view nt _base t should have equivalent components construction unbound tensor list nt_from_unbind = torch nested as_nested_tensor list t unbind device=device dtype=dtype layout=layout assertEqualIgnoringNestedInts nt nt_from_unbind ensure call NT same properties returns NT directly nt = torch nested as_nested_tensor nt device=device dtype=dtype layout=layout assertTrue nt nt ensure call device=None uses input tensor device nt = torch nested as_nested_tensor t device=device dtype=dtype device=None dtype=None layout=layout _validate_nt nt device dtype layout requires_grad=requires_grad dim=dim batch_size=expected_batch_size contiguous=True cached_min_seqlen=expected_seqlen cached_max_seqlen=expected_seqlen we don t support conversion between layouts way atm other_layout = torch strided layout == torch jagged torch jagged assertRaisesRegex RuntimeError Converting between nested tensor layouts supported torch nested as_nested_tensor nt device=device dtype=dtype layout=other_layout requires_grad make sure gradients flow back into inputs nt backward torch ones_like nt assertEqual orig_t grad torch ones_like orig_t dtypes torch float test_construction_from_list device dtype torch fx experimental symbolic_shapes is_nested_int success case single ragged dim anywhere batch dim nt_dim ragged_dim range nt_dim B = shapes = list range + nt_dim - _ range B b range B subtract convert component dim space shapes b ragged_dim - = torch randint device=device dtype=torch int item components = torch randn shape device=device dtype=dtype shape shapes nt = torch nested nested_tensor components layout=torch jagged assertEqual nt dim nt_dim assertEqual nt _ragged_idx ragged_dim d range nt_dim assertEqual d == ragged_dim is_nested_int nt shape d error case empty list assertRaisesRegex RuntimeError Cannot construct nested tensor empty tensor list torch nested nested_tensor layout=torch jagged error case list zero-dim tensors assertRaisesRegex RuntimeError Cannot construct nested tensor list zero-dim tensors torch nested nested_tensor torch tensor device=device dtype=dtype torch tensor device=device dtype=dtype torch tensor device=device dtype=dtype layout=torch jagged error case multiple ragged dims assertRaisesRegex RuntimeError Cannot represent given tensor list nested tensor jagged layout torch nested nested_tensor torch randn device=device dtype=dtype torch randn device=device dtype=dtype layout=torch jagged error case components multiple devices cuda device assertRaisesRegex RuntimeError When constructing nested tensor all tensors list must same device torch nested nested_tensor torch randn device=device dtype=dtype torch randn device= cpu dtype=dtype layout=torch jagged error case components multiple dtypes assertRaisesRegex RuntimeError When constructing nested tensor all tensors list must have same dtype torch nested nested_tensor torch randn device=device dtype=dtype torch randn device=device dtype=torch float layout=torch jagged error case components multiple dims assertRaisesRegex RuntimeError When constructing nested tensor all tensors list must have same dim torch nested nested_tensor torch randn device=device dtype=dtype torch randn device=device dtype=dtype layout=torch jagged dtypes torch double torch half onlyCUDA test_device_dtype_transfer_updates_offsets device dtype tensor_list _get_example_tensor_lists orig_device = torch device cpu orig_dtype = torch float nt = torch nested nested_tensor tensor_list layout=torch jagged device=orig_device dtype=orig_dtype assertEqual torch int nt offsets dtype nt = nt device=device dtype=dtype offsets should still int new device assertEqual nt values device nt offsets device assertEqual torch int nt offsets dtype test_unbind device tensor_list _get_example_tensor_lists nt = torch nested nested_tensor tensor_list layout=torch jagged device=device ragged_idx = out = nt unbind assertEqual len out len tensor_list i t enumerate out assertEqual t tensor_list i parametrize ragged_idx test_unbind_transpose device ragged_idx tensor_list _get_example_tensor_lists nt = torch nested nested_tensor tensor_list layout=torch jagged device=device ragged_idx nt dim nt = nt transpose ragged_idx set ragged_idx out = nt unbind assertEqual len out len tensor_list i t enumerate out assertEqual t transpose ragged_idx - tensor_list i transpose back each element result test_unbind_transpose_ragged_idx_last_dim device tensor_list _get_example_tensor_lists nt = torch nested nested_tensor tensor_list layout=torch jagged device=device transpose - set ragged_idx = last dimension out = nt unbind assertEqual len out len tensor_list i t enumerate out assertEqual t transpose - tensor_list i transpose back each element result test_unbind_lengths device values = torch randn device=device offsets = torch tensor device=device lengths = torch tensor device=device nt = torch nested nested_tensor_from_jagged values offsets=offsets lengths=lengths D nested tensor tensor_list = i range offsets shape - tensor_list append values offsets i offsets i + lengths i out = nt unbind assertEqual len out len tensor_list i t enumerate out assertEqual t tensor_list i test_unbind_lengths_ragged_idx_ device values = torch randn device=device offsets = torch tensor device=device lengths = torch tensor device=device ragged_idx = nt = torch nested _internal nested_tensor NestedTensor values offsets=offsets lengths=lengths _ragged_idx=ragged_idx D nested tensor tensor_list = i range offsets shape - tensor_list append values offsets i offsets i + lengths i out = nt unbind assertEqual len out len tensor_list i t enumerate out assertEqual t tensor_list i test_unbind_lengths_ragged_idx_equals_ _bad_dim device values = torch randn device=device offsets = torch tensor device=device lengths = torch tensor device=device ragged_idx = nt = torch nested _internal nested_tensor NestedTensor values offsets=offsets lengths=lengths _ragged_idx=ragged_idx D nested tensor assertRaisesRegex RuntimeError r unbind\ \ nested tensor offsets lengths lambda nt unbind test_unbind_lengths_ragged_idx_ device values = torch randn device=device offsets = torch tensor device=device lengths = torch tensor device=device ragged_idx = nt = torch nested _internal nested_tensor NestedTensor values offsets=offsets lengths=lengths _ragged_idx=ragged_idx D nested tensor tensor_list = i range offsets shape - tensor_list append values offsets i offsets i + lengths i out = nt unbind assertEqual len out len tensor_list i t enumerate out assertEqual t tensor_list i test_unbind_lengths_ragged_idx_ device values = torch randn device=device offsets = torch tensor device=device lengths = torch tensor device=device ragged_idx = nt = torch nested _internal nested_tensor NestedTensor values offsets=offsets lengths=lengths _ragged_idx=ragged_idx D nested tensor tensor_list = i range offsets shape - tensor_list append values offsets i offsets i + lengths i out = nt unbind assertEqual len out len tensor_list i t enumerate out assertEqual t tensor_list i skipIfTorchDynamo TorchDynamo raises error ragged_idx == earlier than Torch test_unbind_lengths_ragged_idx_ device values = torch randn device=device offsets = torch tensor device=device lengths = torch tensor device=device ragged_idx = nt = torch nested _internal nested_tensor NestedTensor values offsets=offsets lengths=lengths _ragged_idx=ragged_idx D nested tensor tensor_list = i range offsets shape - tensor_list append values offsets i offsets i + lengths i assertRaisesRegex RuntimeError r unbind\ \ nested tensor out bounds lambda nt unbind test_narrow device starts = torch tensor device=device dtype=torch int lengths = torch tensor device=device dtype=torch int buffer = torch arange device=device dtype=torch int unsqueeze expand - clone detach nt = torch nested narrow buffer starts lengths layout=torch jagged assertTrue nt _is_view nt _base buffer TODO Use approach when unbind functional unbinded_nt = nt unbind i range starts shape assertEqual torch arange starts i starts i + lengths i device=device dtype=torch int unbinded_nt i i range starts shape assertEqual torch arange starts i starts i + lengths i device=device dtype=torch int nt values nt offsets i nt offsets i + nt lengths i test_njt_cat device offsets = torch tensor device=device dtype=torch int values_ = torch randn dtype=torch float device=device requires_grad=True values_ = torch randn dtype=torch float device=device requires_grad=True grad_test_func values_ values_ offsets nt_ = torch nested nested_tensor_from_jagged values_ offsets nt_ = torch nested nested_tensor_from_jagged values_ offsets nt_ = torch cat nt_ nt_ dim=- nt_ values assert gradcheck grad_test_func inputs= values_ values_ offsets check_batched_grad=False test_is_contiguous device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device nt_contiguous = torch nested as_nested_tensor b c layout=torch jagged starts_nc = torch tensor device=device dtype=torch int lengths_nc = torch tensor device=device dtype=torch int narrow_base = torch arange device=device dtype=torch int unsqueeze expand - clone nt_noncontiguous = torch nested narrow narrow_base starts_nc lengths_nc layout=torch jagged starts_c = torch tensor device=device dtype=torch int lengths_c = torch tensor device=device dtype=torch int nt_contiguous_narrow = torch nested narrow narrow_base starts_c lengths_c layout=torch jagged Test contiguous case assert nt_contiguous is_contiguous Test narrow case assert nt_noncontiguous is_contiguous assert nt_contiguous_narrow is_contiguous Test querying memory_format assertTrue nt_contiguous is_contiguous memory_format=torch contiguous_format assertTrue nt_noncontiguous is_contiguous memory_format=torch contiguous_format assertTrue nt_contiguous_narrow is_contiguous memory_format=torch contiguous_format test_layout_under_torch_dispatch_mode torch testing _internal logging_tensor capture_logs_with_logging_tensor_mode nt = random_nt_from_dims None torch device cpu torch float layout=torch jagged capture_logs_with_logging_tensor_mode assertEqual nt layout torch jagged skipIfTorchDynamo Not suitable test TorchDynamo parametrize func torch empty_like torch randn_like name_fn=lambda f f __name__ test_like_shape func nt = random_nt_from_dims None torch device cpu torch float layout=torch jagged nt_like = func nt nt_ub nt_like unbind t_like = func nt_ub assertEqual nt_ub shape t_like shape skipIfTorchDynamo Not suitable test TorchDynamo parametrize func torch empty_like torch full_like torch ones_like torch rand_like torch randint_like torch randn_like torch zeros_like name_fn=lambda f f __name__ test_like_value func device dtype = torch float func torch randint_like torch int nt _sample_njts device=device dtype=dtype extra_kwarg_sets = func torch full_like extra_kwarg_sets = fill_value func torch randint_like extra_kwarg_sets = high low high only test changing dtype device CUDA - CPU because CUDA might available when running test CPU change_dtype_device_settings = False True cuda device False change_dtype_device change_dtype_device_settings change_dtype_device new_dtype = torch float func torch randint_like torch int new_device = cpu cuda device device new_layout = torch strided extra_kwargs extra_kwarg_sets extra_kwargs update dtype new_dtype device new_device layout new_layout extra_kwargs extra_kwarg_sets nt_like = func nt extra_kwargs assertEqual nt shape nt_like shape change_dtype_device assertNotEqual nt device nt_like device assertNotEqual nt device nt_like dtype layout should ignored since only torch jagged supported assertEqual torch jagged nt_like layout assertEqual nt device nt_like device assertEqual nt dtype nt_like dtype assertEqual nt layout nt_like layout assertEqual nt layout torch jagged don t bother trying compare random empty values func torch empty_like torch rand_like torch randn_like torch randint_like nt_ub nt_like unbind t_like = func nt_ub extra_kwargs assertEqual nt_ub t_like test_noncontiguous_pointwise device = torch randn requires_grad=True dtype=torch float device=device b = torch randn requires_grad=True dtype=torch float device=device c = torch randn requires_grad=True dtype=torch float device=device nt = torch nested nested_tensor b c layout=torch jagged transpose ragged dim transposed = nt transpose assertFalse transposed is_contiguous clone = transposed clone check_nt_equality x y assertEqual x values y values assertEqual x offsets y offsets assertEqual x _ragged_idx y _ragged_idx assertEqual x shape y shape assertFalse clone is_contiguous check_nt_equality clone transposed clone_contig = transposed clone memory_format=torch contiguous_format assertTrue clone_contig is_contiguous check_nt_equality clone_contig transposed detached = transposed detach assertFalse clone is_contiguous check_nt_equality detached transposed test_permute device nt = random_nt_from_dims None device torch float layout=torch jagged nt_shape = nt shape nt_inner_shape = nt values shape assertRaisesRegex ValueError r permute\ \ number dimensions tensor input \ \ + r does match length desired ordering dimensions \ \ nt permute assertRaisesRegex ValueError r permute\ \ duplicate dims allowed nt permute - assertRaisesRegex ValueError Permute supported batch dimension jagged NT nt permute nt_permute = nt permute - assertEqual nt_permute shape nt_shape nt_shape nt_shape nt_shape assertEqual nt_permute values shape nt_inner_shape nt_inner_shape nt_inner_shape assertEqual nt_permute _ragged_idx assertEqual nt_permute permute nt test_to_dtype device nt = random_nt_from_dims None device torch float layout=torch jagged nt_after = nt torch float assertEqual torch float nt dtype assertEqual torch float nt_after dtype assertEqual torch float nt_after values dtype assertEqual torch int nt_after offsets dtype noncontiguous_nt = nt transpose noncontiguous_nt_after = noncontiguous_nt torch bfloat assertEqual torch bfloat noncontiguous_nt_after dtype assertEqual torch bfloat noncontiguous_nt_after values dtype assertEqual torch int noncontiguous_nt_after offsets dtype test_to_copy device nt = torch nested nested_tensor torch randn i + requires_grad=True dtype=torch float device=device i range layout=torch jagged nt_copy_dtype = torch ops aten _to_copy nt dtype=torch float assertEqual torch float nt_copy_dtype dtype nt_t = nt transpose nt_t_copy_dtype = torch ops aten _to_copy nt_t dtype=torch float assertEqual torch float nt_t_copy_dtype dtype test_copy_ device offsets = torch tensor device=device = torch nested nested_tensor_from_jagged torch zeros device=device offsets b = torch nested nested_tensor_from_jagged torch ones device=device offsets copy_ b torch _dynamo disable assertEqual b offsets_ = torch tensor device=device c = torch nested nested_tensor_from_jagged torch ones device=device offsets_ should work even though nested ints different due unbound-based copy copy_ c fail when tensors have different sizes = transpose assertRaisesRegex RuntimeError expected compatible input src shapes got copy_ b This can t happen opinfo tests due subprocess creation unittest skipIf TEST_WITH_ROCM In ROCm kernel asserts disabled due performance overhead test_index_put_error device subprocess subTest r = subprocess call sys executable -c \ torch offsets = torch tensor device= cuda lengths = torch tensor device= cuda indices = torch tensor device= cuda torch tensor device= cuda torch tensor device= cuda = torch nested nested_tensor_from_jagged torch zeros device= cuda offsets lengths indices = torch cuda synchronize assertTrue r = skipIfTorchDynamo Dynamo doesn t know how trace prof events test_profiler_sequence_nr torch profiler profile prof values = torch randn requires_grad=True offsets = torch tensor values = values l = torch nn Linear nt = torch nested nested_tensor_from_jagged values offsets nt = l nt val = nt values loss = val sum loss backward fwd_seq_nrs = evt prof events linear evt name lower backward evt name lower evt sequence_nr = - fwd_seq_nrs append evt sequence_nr bwd_seq_nrs = evt prof events linear evt name lower backward evt name lower evaluate_function evt name lower evt sequence_nr = - bwd_seq_nrs append evt sequence_nr There should only one such event sequence number PythonTLSSnapshot event - note s terrible we end up multiple events same sequence number - so we could relax check becomes inconvenient maintain property assertEqual len fwd_seq_nrs assertEqual len bwd_seq_nrs assertEqual fwd_seq_nrs bwd_seq_nrs test_is_same_size device get_ _tensors torch randn i + requires_grad=True dtype=torch float device=device i range nt offsets = jagged_from_list get_ _tensors None nt offsets = jagged_from_list get_ _tensors offsets nt offsets = jagged_from_list get_ _tensors None nt offsets = jagged_from_list get_ _tensors offsets check_size nt nt nt nt assertTrue torch ops aten is_same_size nt nt assertTrue torch ops aten is_same_size nt nt assertFalse torch ops aten is_same_size nt nt check_size nt nt nt nt nt _t nt _t nt _t nt _t = x transpose x nt nt nt nt check_size nt _t nt _t nt _t nt _t skipIfTorchDynamo compiles internally unittest skipIf IS_WINDOWS reason= Windows yet supported torch compile skipCUDAIf SM OrLater GPU capability SM test_specialize_dynamic_shape device values = torch randn device=device offsets = torch tensor device=device like_values = torch randn_like values marks values dynamic nt = torch nested nested_tensor_from_jagged values offsets fn values same_size here dynamic shape specialized same_size s shape https github com pytorch pytorch issues make sure doesn t error out torch compile values + same_size assertEqual fn values like_values torch compile fn values like_values skipIfTorchDynamo compiles internally unittest skipIf IS_WINDOWS reason= Windows yet supported torch compile skipCUDAIf SM OrLater GPU capability SM test_specialize_dynamic_shape_recompile device generate_inp total_len values = torch randn total_len device=device offsets = torch tensor total_len device=device like_values = torch randn_like values values offsets like_values check_results ref_fn res_fn args values offsets like_values = args may add dynamic shape markings goal test make sure whatever markings there we eventually stop recompiling shape changes nt = torch nested nested_tensor_from_jagged values offsets assertEqual ref_fn values like_values res_fn values like_values fn values same_size values + same_size compile_counter = torch _dynamo testing CompileCounter compiled_fn = torch compile fn backend=compile_counter fullgraph=True check_results fn compiled_fn generate_inp assertEqual compile_counter frame_count check_results fn compiled_fn generate_inp we ll probably recompile here dynamic shapes - s okay though frame_count_ = compile_counter frame_count assertIn frame_count_ make sure now we ve already compiled dynamic shapes so additional shapes should trigger additional recompiles check_results fn compiled_fn generate_inp assertEqual compile_counter frame_count frame_count_ Note Math fallback doesn t work bfloat CUDA Note ROCm doesn t support flash attention mem_efficient attention NT unittest skipIf TEST_WITH_ROCM ROCm doesn t support flash attention mem_efficient attention NT tf _on_and_off dtypes torch float torch bfloat torch float SM OrLater torch float torch float test_sdpa device dtype batch_size = emb_dims = n_heads = head_dims = emb_dims n_heads sen = torch randn emb_dims dtype=dtype device=device sen = torch randn emb_dims dtype=dtype device=device query = torch nn Linear emb_dims emb_dims bias=False device=device dtype=dtype key = torch nn Linear emb_dims emb_dims bias=False device=device dtype=dtype value = torch nn Linear emb_dims emb_dims bias=False device=device dtype=dtype Simplest case sentence no batching x_d = sen unsqueeze x_nt = torch nested as_nested_tensor sen layout=torch jagged See note below why we detach here q_d = query x_d view batch_size - n_heads head_dims detach requires_grad_ True q_d _t = q_d transpose k_d = key x_d view batch_size - n_heads head_dims detach requires_grad_ True k_d _t = k_d transpose v_d = value x_d view batch_size - n_heads head_dims detach requires_grad_ True v_d _t = v_d transpose q_nt = query x_nt view x_nt size n_heads head_dims detach requires_grad_ True q_nt_t = q_nt transpose k_nt = key x_nt view x_nt size n_heads head_dims detach requires_grad_ True k_nt_t = k_nt transpose v_nt = value x_nt view x_nt size n_heads head_dims detach requires_grad_ True v_nt_t = v_nt transpose High Precision Math Reference q_d _f = q_d torch float k_d _f = k_d torch float v_d _f = v_d torch float q_d _f _t = q_d _f transpose k_d _f _t = k_d _f transpose v_d _f _t = v_d _f transpose out_ref = torch ops aten _scaled_dot_product_attention_math q_d _f _t k_d _f _t v_d _f _t grads_ref = torch autograd grad out_ref sum q_d _f k_d _f v_d _f Low Precision Math Reference out_lp_ref = torch ops aten _scaled_dot_product_attention_math q_d _t k_d _t v_d _t grads_lp_ref = torch autograd grad out_lp_ref sum q_d k_d v_d Compute tolerances output_ref_atol output_ref_rtol = get_tolerances out_ref out_lp_ref fudge factor smaller GPUs e g A A grad_q_ref_atol grad_q_ref_rtol = get_tolerances grads_ref grads_lp_ref grad_k_ref_atol grad_k_ref_rtol = get_tolerances grads_ref grads_lp_ref grad_v_ref_atol grad_v_ref_rtol = get_tolerances grads_ref grads_lp_ref grad_atols = grad_q_ref_atol grad_k_ref_atol grad_v_ref_atol grad_rtols = grad_q_ref_rtol grad_k_ref_rtol grad_v_ref_rtol attn_d = torch nn functional scaled_dot_product_attention q_d _t k_d _t v_d _t transpose attn_nt = torch nn functional scaled_dot_product_attention q_nt_t k_nt_t v_nt_t transpose assertEqual attn_d attn_nt unbind unsqueeze atol=output_ref_atol rtol=output_ref_rtol Simple case sentences no extra params x_d = sen unsqueeze x_nt = torch nested as_nested_tensor sen sen layout=torch jagged NB we make sure leaf tensor we compute gradients view-ed tensor before transposed This because today we cannot backward through view unbind transposed tensor q_d = query x_d view batch_size - n_heads head_dims detach requires_grad_ True q_d _t = q_d transpose k_d = key x_d view batch_size - n_heads head_dims detach requires_grad_ True k_d _t = k_d transpose v_d = value x_d view batch_size - n_heads head_dims detach requires_grad_ True v_d _t = v_d transpose q_nt = query x_nt view x_nt size n_heads head_dims detach requires_grad_ True q_nt_t = q_nt transpose k_nt = key x_nt view x_nt size n_heads head_dims detach requires_grad_ True k_nt_t = k_nt transpose v_nt = value x_nt view x_nt size n_heads head_dims detach requires_grad_ True v_nt_t = v_nt transpose attn_d = torch nn functional scaled_dot_product_attention q_d _t k_d _t v_d _t transpose d _grads = torch autograd grad attn_d sum q_d k_d v_d d _grads = torch autograd grad attn_d sum q_d k_d v_d Simple case batch_size = seq_len = q_ = torch randn dtype=dtype device=device q_nt_ = torch nested as_nested_tensor q_ layout=torch jagged q_nt_ = q_nt_ transpose attn_out = torch nn functional scaled_dot_product_attention q_nt_ q_nt_ q_nt_ assertEqual attn_out shape q_nt_ shape parametrize skip_backward True False check_forward_backward skip_backward=False skip_backward attn_nt = torch nn functional scaled_dot_product_attention q_nt_t k_nt_t v_nt_t transpose x_nt requires_grad = False q_nt requires_grad = False k_nt requires_grad = False v_nt requires_grad = False tq = q_nt_t detach tk = k_nt_t detach tv = v_nt_t detach torch no_grad attn_nt = torch nn functional scaled_dot_product_attention tq tk tv transpose attn_nts = attn_nt unbind assertEqual attn_d attn_nts unsqueeze atol=output_ref_atol rtol=output_ref_rtol assertEqual attn_d attn_nts unsqueeze atol=output_ref_atol rtol=output_ref_rtol skip_backward nt_grads = torch autograd grad attn_nt values sum q_nt k_nt v_nt nt_grad d _grad d _grad grad_atol grad_rtol zip nt_grads d _grads d _grads grad_atols grad_rtols unbound_nt_grads = nt_grad unbind assertEqual d _grad unbound_nt_grads unsqueeze atol=grad_atol rtol=grad_rtol assertEqual d _grad unbound_nt_grads unsqueeze atol=grad_atol rtol=grad_rtol Default check_forward_backward Test dispatcher works calling only mem-effn math they safe all devices torch backends cuda sdp_kernel enable_flash=False enable_mem_efficient=True enable_math=True check_forward_backward Test math fallback torch backends cuda sdp_kernel enable_flash=False enable_mem_efficient=False enable_math=True Math fallback doesn t work bfloat CUDA because group_gemm_dispatch implemented BFloat str device startswith cuda dtype == torch bfloat check_forward_backward check_cudnn = os getenv TORCH_CUDNN_SDPA_NESTED_TENSOR_ENABLED == cuda str device check_cudnn dtype == torch float dtype == torch bfloat torch nn attention sdpa_kernel torch nn attention SDPBackend CUDNN_ATTENTION check_forward_backward skipIfTorchDynamo SDPA test compiles internally unittest skipIf IS_WINDOWS reason= Windows yet supported torch compile skipCUDAIf SM OrLater GPU capability SM Guarding sqrt doesn t work ROCm skipCUDAIfRocm onlyCUDA dtypes torch float torch bfloat torch float SM OrLater torch float torch float test_sdpa_compile device dtype batch_size = emb_dims = n_heads = head_dims = emb_dims n_heads sen = torch randn emb_dims dtype=dtype device=device sen = torch randn emb_dims dtype=dtype device=device query = torch nn Linear emb_dims emb_dims bias=False device=device dtype=dtype key = torch nn Linear emb_dims emb_dims bias=False device=device dtype=dtype value = torch nn Linear emb_dims emb_dims bias=False device=device dtype=dtype Simplest case sentence no batching x_d = sen unsqueeze x_d = sen unsqueeze x_nt = torch nested as_nested_tensor sen sen layout=torch jagged q_d = query x_d view batch_size - n_heads head_dims transpose k_d = key x_d view batch_size - n_heads head_dims transpose v_d = value x_d view batch_size - n_heads head_dims transpose q_d = query x_d view batch_size - n_heads head_dims transpose k_d = key x_d view batch_size - n_heads head_dims transpose v_d = value x_d view batch_size - n_heads head_dims transpose q_nt = query x_nt view x_nt size n_heads head_dims detach transpose k_nt = key x_nt view x_nt size n_heads head_dims detach transpose v_nt = value x_nt view x_nt size n_heads head_dims detach transpose High Precision Math Reference q_d _f = q_d torch float k_d _f = k_d torch float v_d _f = v_d torch float out_ref = torch ops aten _scaled_dot_product_attention_math q_d _f k_d _f v_d _f Low Precision Math Reference out_lp_ref = torch ops aten _scaled_dot_product_attention_math q_d k_d v_d output_ref_atol output_ref_rtol = get_tolerances out_ref out_lp_ref fudge_factor= attn_d = torch nn functional scaled_dot_product_attention q_d k_d v_d transpose attn_d = torch nn functional scaled_dot_product_attention q_d k_d v_d transpose compiled_sdpa = torch compile torch nn functional scaled_dot_product_attention attn_nt = compiled_sdpa q_nt k_nt v_nt transpose attn_nts = attn_nt unbind assertEqual attn_d attn_nts unsqueeze atol=output_ref_atol rtol=output_ref_rtol assertEqual attn_d attn_nts unsqueeze atol=output_ref_atol rtol=output_ref_rtol dtypes torch float torch double torch half test_sdpa_with_constant_sequence_length device dtype shape B P S D B batch size P ragged number prompts S constant sequence length D embedding size query = random_nt_from_dims None device=device dtype=dtype layout=torch jagged requires_grad=True key = random_nt_from_similar query value = random_nt_from_similar query output = F scaled_dot_product_attention query key value assertTrue isinstance output NestedTensor output values sum backward query_dense = query detach clone requires_grad_ True should equivalent just running buffers through output_dense = F scaled_dot_product_attention query_dense values key values value values torch _dynamo disable assertEqual output _values output_dense output_dense sum backward torch _dynamo disable assertEqual query grad query_dense grad onlyCUDA unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Platform doesn t support flash mem-efficient attention dtypes torch float torch bfloat torch float SM OrLater torch float torch float test_sdpa_with_packed_in_proj device dtype shape B D input_packed = random_nt_from_dims None device=device dtype=dtype layout=torch jagged Do input projection num_heads = should multiple efficient kernels e g flash mem-efficient head_dim = qkv_linear = torch nn Linear num_heads head_dim device=device dtype=dtype in_proj input_packed qkv_linear=qkv_linear qkv_post_proj = qkv_linear input_packed these non-contiguous trigger _is_safe_to_get_storage_as_tensor q k v = qkv_post_proj chunk dim=- q = q unflatten - num_heads head_dim transpose - - k = k unflatten - num_heads head_dim transpose - - v = v unflatten - num_heads head_dim transpose - - q k v q k v = in_proj input_packed output = F scaled_dot_product_attention q k v attn_mask=None compare individually running unbound components through in_component out_component zip input_packed unbind output transpose - - unbind q k v = in_proj in_component out = F scaled_dot_product_attention q k v transpose - - Low Precision Math Reference out_lp_ref = torch ops aten _scaled_dot_product_attention_math q k v transpose - - output_ref_atol output_ref_rtol = get_tolerances out out_lp_ref fudge_factor= assertEqual out out_component atol=output_ref_atol rtol=output_ref_rtol skipIfTorchDynamo SDPA test compiles internally unittest skipIf IS_WINDOWS reason= Windows yet supported torch compile skipCUDAIf SM OrLater GPU capability SM mha_varlen_fwd supported ROCm skipCUDAIfRocm onlyCUDA dtypes torch float torch bfloat torch float SM OrLater torch float torch float test_sdpa_backwards device dtype values = torch randn requires_grad=True device=device dtype=dtype offsets = torch tensor device=device dtype=torch int torch compile f values offsets nt = convert_jagged_to_nested_tensor values offsets max_length= nt = nt transpose - - purposefully graph break trigger view replay subclass view input torch tensor item output = F scaled_dot_product_attention nt nt nt transpose - - convert_nt_to_jagged output output = f values offsets output sum backward assertEqual values grad torch ones_like values unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Platform doesn t support flash mem-efficient attention skipCUDAIf SM OrLater GPU capability SM skipCUDAIfRocm onlyCUDA skipIfTorchDynamo unittest skipIf IS_WINDOWS reason= Windows yet supported torch compile test_sdpa_autocast device fn_nt values values offsets nt = convert_jagged_to_nested_tensor values offsets max_length= nt = convert_jagged_to_nested_tensor values offsets max_length= nt = nt transpose nt = nt transpose F scaled_dot_product_attention nt nt nt fn_dense x x x = x view transpose x = x view transpose F scaled_dot_product_attention x x x values = torch randn device=device dtype=torch float values = torch randn device=device dtype=torch float offsets = torch arange + device=device dtype=torch int x = values clone x = values clone torch autocast device_type= cuda dtype=torch float out_dense_eager = fn_dense x x out_dense_compiled = torch compile fn_dense x x out_nt_eager = fn_nt values values offsets out_nt_compiled = torch compile fn_nt values values offsets assertEqual out_dense_eager out_dense_compiled assertEqual out_dense_eager transpose out_nt_eager values transpose view assertEqual out_dense_eager transpose out_nt_compiled values transpose view get_values tuple x detach clone requires_grad_ True x values values v _dense_eager v _dense_eager = get_values v _dense_compile v _dense_compile = get_values v _nt_eager v _nt_eager = get_values v _nt_compile v _nt_compile = get_values torch autocast device_type= cuda dtype=torch float loss_dense_eager = fn_dense v _dense_eager v _dense_eager sum loss_dense_compile = torch compile fn_dense v _dense_compile v _dense_compile sum loss_nt_eager = fn_nt v _nt_eager v _nt_eager offsets values sum loss_nt_compile = torch compile fn_nt v _nt_compile v _nt_compile offsets values sum loss_dense_eager backward loss_dense_compile backward loss_nt_eager backward loss_nt_compile backward assertEqual v _dense_eager grad v _dense_compile grad assertEqual v _dense_eager grad v _nt_eager grad atol= e- rtol= e- assertEqual v _dense_eager grad v _nt_compile grad atol= e- rtol= e- assertEqual v _dense_eager grad v _dense_compile grad assertEqual v _dense_eager grad v _nt_eager grad atol= e- rtol= e- assertEqual v _dense_eager grad v _nt_compile grad atol= e- rtol= e- unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Platform doesn t support flash mem-efficient attention skipCUDAIf SM OrLater GPU capability SM skipCUDAIfRocm onlyCUDA skipIfTorchDynamo test_sdpa_flop_counter device torch utils flop_counter FlopCounterMode get_flops nt flop_counter = FlopCounterMode display=False flop_counter ret = torch nn functional scaled_dot_product_attention nt nt nt ret values sum backward flop_counter get_total_flops values = torch randn requires_grad=True device=device dtype=torch float offsets = torch arange + device=device dtype=torch int nt = convert_jagged_to_nested_tensor values offsets max_length= transpose values_meta = torch randn requires_grad=True device= meta dtype=torch float offsets_meta = torch arange + device= meta dtype=torch int nt_meta = convert_jagged_to_nested_tensor values_meta offsets_meta max_length= transpose assertEqual get_flops nt get_flops nt_meta skipIfTorchDynamo test_nested_tensor_activation_checkpoint device values = torch randn requires_grad=True device=device dtype=torch float lengths = torch tensor device=device dtype=torch int offsets = F pad lengths pad= cumsum dim= fn values offsets nt = convert_jagged_to_nested_tensor values offsets max_length= convert_nt_to_jagged nt sum checkpoint fn values offsets use_reentrant=False backward assertIsNotNone values grad context_fn = partial create_selective_checkpoint_contexts torch ops aten cumsum default values grad = None fn values lengths offsets = F pad lengths pad= cumsum dim= nt = convert_jagged_to_nested_tensor values offsets max_length= convert_nt_to_jagged nt sum checkpoint fn values lengths use_reentrant=False context_fn=context_fn backward assertIsNotNone values grad Internally-defined NT use cases lifted here maximum test realism TODO Remove these when ViewNestedFromBuffer etc deprecated skipCUDAIfRocm needed skipIfTorchDynamo compiles internally unittest skipIf IS_WINDOWS reason= Windows yet supported torch compile skipCUDAIf SM OrLater GPU capability SM parametrize use_legacy_api True False skipCPUIf True SPDA Math NT fallback causes failure see issue unittest skipIf RelWithAssert torch __config__ show failing debug build see https github com pytorch pytorch pull context test_dummy_mha_with_nt device use_legacy_api bs = d = d = d = n_heads = d_head = d n_heads max_length_ = max_length_ = torch manual_seed mha torch nn Module __init__ use_legacy_api - None super __init__ torch manual_seed linear = torch nn Linear d d device=device use_legacy_api = use_legacy_api forward query value offsets value = linear value use_legacy_api key = convert_jagged_to_nested_tensor_legacy value offsets max_length_ value = convert_jagged_to_nested_tensor_legacy value offsets max_length_ query = convert_dense_to_nested_tensor_legacy query key = convert_jagged_to_nested_tensor value offsets max_length_ value = convert_jagged_to_nested_tensor value offsets max_length_ query = convert_dense_to_nested_tensor query q = query view bs - n_heads d_head transpose k = key view bs - n_heads d_head transpose v = value view bs - n_heads d_head transpose torch nn attention sdpa_kernel torch nn attention SDPBackend FLASH_ATTENTION torch nn attention SDPBackend EFFICIENT_ATTENTION attn_output = torch nn functional scaled_dot_product_attention q k v attn_mask=None dropout_p= is_causal=False attn_output = attn_output transpose use_legacy_api attn_output = convert_nt_to_jagged_legacy attn_output attn_output = convert_nt_to_jagged attn_output attn_output key _max_seqlen value _max_seqlen query = torch rand bs d d device=device value = torch rand d requires_grad=True device=device total_length must than max_length otherwise flash_attn backward will fail offsets = torch tensor device=device m = mha use_legacy_api symbolic_traced torch fx GraphModule = torch fx symbolic_trace m m = torch compile symbolic_traced attn_output cached_key_max_seqlen cached_value_max_seqlen = m query value offsets loss = attn_output sum Check NT can fx traced torch compile backward works loss backward Check value requires_grad lost after tracing compiling value_grad = value grad save comparison later assertIsNotNone value_grad check max_seqlen cached properly assertEqual cached_key_max_seqlen max_length_ assertEqual cached_value_max_seqlen max_length_ check output numerically equivalent eager mode m_eager = mha use_legacy_api value grad = None attn_output_eager _ _ = m_eager query value offsets attn_output_eager sum backward assertTrue torch allclose attn_output_eager attn_output assertTrue torch allclose value_grad value grad Helper function generate random query key value NJTs B n_heads D format If noncontig_with_holes True results will non-contiguous holes i e have both offsets lengths specified _rand_qkv device dtype noncontig_with_holes=False q_and_kv_match=True batch_size = n_heads = D = _rand_nt noncontig_with_holes=noncontig_with_holes sentence_lengths = random randint _ range batch_size - total = sum sentence_lengths shape B D_total where D_total = n_heads D nt = torch nested nested_tensor torch randn l n_heads D device=device dtype=dtype l sentence_lengths layout=torch jagged noncontig_with_holes nt = torch nested nested_tensor_from_jagged nt _values nt _offsets - introduce holes lengths=nt _offsets diff - jagged_dim=nt _ragged_idx min_seqlen=nt _min_seqlen max_seqlen=nt _max_seqlen nt query = _rand_nt q_and_kv_match key = torch randn_like query value = torch randn_like query key = _rand_nt value = torch randn_like key shape B D_total - B n_heads D query = query unflatten - n_heads D transpose detach requires_grad_ key = key unflatten - n_heads D transpose detach requires_grad_ value = value unflatten - n_heads D transpose detach requires_grad_ query key value dtypes torch float test_apply_ device dtype nt = random_nt_from_dims None device=device dtype=dtype layout=torch jagged requires_grad=True f x x device = cpu assertRaisesRegex TypeError apply_ only implemented CPU tensors nt apply_ f before = nt _values detach clone nt apply_ f expected = f before assertEqual expected nt _values apply_ should swap values in-place without appending autograd graph assertIsNone nt grad assertIsNone nt _values grad_fn onlyCUDA dtypes torch float torch float torch half parametrize contiguity noncontig_transposed noncontig_with_holes name_fn=lambda c c test_noncontiguous_to device dtype contiguity Dense tensors preserve non-contiguity through calls i e strides preserved Test analogous behavior NJTs non-contiguous transposed non-contiguous holes contiguity == noncontig_transposed nt = random_nt_from_dims None device=device dtype=dtype layout=torch jagged transpose - - contiguity == noncontig_with_holes nt = torch nested nested_tensor_from_jagged values=torch randn device=device dtype=dtype offsets=torch tensor device=device dtype=torch int these lengths specify holes lengths=torch tensor device=device dtype=torch int raise ValueError invalid contiguity specified test_noncontiguous_to test dtype conversion dtype_conversions = torch float torch half torch float torch float torch half torch float other_dtype = dtype_conversions dtype nt = nt dtype=other_dtype assertEqual nt dtype other_dtype assertEqual nt is_contiguous nt is_contiguous assertEqual nt _values is_contiguous nt _values is_contiguous assertEqual nt shape nt shape expect no change offsets lengths assertEqual nt _offsets nt _offsets assertEqual nt _lengths nt _lengths test device conversion other_device = torch device cpu nt = nt device=other_device assertEqual nt device other_device assertEqual nt is_contiguous nt is_contiguous assertEqual nt _values is_contiguous nt _values is_contiguous assertEqual nt shape nt shape expect device change offsets lengths assertEqual nt _offsets device other_device nt _lengths None assertEqual nt _lengths device other_device dtypes torch float test_autograd_function_with_None_grad device dtype MyFunction torch autograd Function staticmethod forward ctx inp ctx save_for_backward inp out = inp + out = inp out out staticmethod backward ctx grad_out grad_out inp = ctx saved_tensors grad_out + grad_out f = MyFunction apply nt = random_nt_from_dims None device=device dtype=dtype layout=torch jagged requires_grad=True Only use one autograd Function outputs downstream so grad other output None We re testing engine can allocate correctly-shaped NJT zeros grad other output case out _ = f nt out backward torch ones_like out dtypes torch float torch float torch half test_jagged_padded_dense_conversion_kernels device dtype values = torch randn device=device dtype=dtype offsets = torch tensor device=device dtype=torch int max_length = offsets diff max item padding_value = convert jagged - padded dense padded = torch ops aten _jagged_to_padded_dense_forward values offsets max_length padding_value batch_size = offsets shape - expected_padded_shape = batch_size max_length values shape - assertEqual padded shape expected_padded_shape convert padded dense - jagged total_L = values shape output_jagged = torch ops aten _padded_dense_to_jagged_forward padded offsets total_L should equivalent original values assertEqual values output_jagged success case truncate max length needed trunc_max_length = max_length - trunc_padded = torch ops aten _jagged_to_padded_dense_forward values offsets trunc_max_length padding_value assertEqual padded trunc_max_length trunc_padded specific CPU impls device == cpu error case multiple offsets cpu since CPU kernels don t support more now assertRaisesRegex RuntimeError only single jagged dim supported torch ops aten _jagged_to_padded_dense_forward values offsets offsets max_length max_length padding_value assertRaisesRegex RuntimeError only single jagged dim supported torch ops aten _padded_dense_to_jagged_forward padded offsets offsets total_L error case D offsets offsets d = offsets unsqueeze - assertRaisesRegex RuntimeError expected D offsets torch ops aten _jagged_to_padded_dense_forward values offsets d max_length padding_value assertRaisesRegex RuntimeError expected D offsets torch ops aten _padded_dense_to_jagged_forward padded offsets d total_L error case final offset = total_L offsets_wrong = offsets detach clone offsets_wrong - = total_L + assertRaisesRegex RuntimeError final offset should match total_L value torch ops aten _padded_dense_to_jagged_forward padded offsets_wrong total_L error case D padded input padded_wrong = padded flatten detach clone assertRaisesRegex RuntimeError expected padded dim = torch ops aten _padded_dense_to_jagged_forward padded_wrong offsets total_L error case batch item has length max length max_length above here offsets_wrong = torch tensor device=device dtype=torch int assertRaisesRegex RuntimeError found batch item length torch ops aten _padded_dense_to_jagged_forward padded offsets_wrong total_L dtypes torch float skipIfTorchDynamo Test compiles internally unittest skipIf sys version_info = torch compile supported python + unittest skipIf IS_WINDOWS reason= Windows yet supported torch compile skipCUDAIf SM OrLater GPU capability SM skipCUDAIfRocm test_compile_preserves_metadata_cache device dtype shape B D nt = random_nt_from_dims None device=device dtype=dtype layout=torch jagged requires_grad=True expect min max seqlen stored here cache = dict nt _metadata_cache torch compile f nt q = nt transpose - - output = F scaled_dot_product_attention q q q transpose - - output output = f nt output backward torch ones_like output assertEqual output _metadata_cache cache dtypes torch float skipIfTorchDynamo Test compiles internally unittest skipIf sys version_info = torch compile supported python + unittest skipIf IS_WINDOWS reason= Windows yet supported torch compile skipCUDAIf SM OrLater GPU capability SM skipCUDAIfRocm test_compile_with_dynamic_max_seq_len device dtype shape B D max seq len nt = torch nested nested_tensor torch randn torch randn torch randn layout=torch jagged max seq len nt = torch nested nested_tensor torch randn torch randn torch randn layout=torch jagged f nt TODO Replace public API when we can use properties torch ones_like nt nt _get_max_seqlen dynamic False True None assertFalse _recompiles_for_inputs f nt nt dynamic=dynamic dtypes torch float skipIfTorchDynamo Test compiles internally unittest skipIf sys version_info = torch compile supported python + unittest skipIf IS_WINDOWS reason= Windows yet supported torch compile skipCUDAIf SM OrLater GPU capability SM skipCUDAIfRocm test_compile_with_dynamic_min_seq_len device dtype shape B D min seq len nt = torch nested nested_tensor torch randn torch randn torch randn layout=torch jagged min seq len nt = torch nested nested_tensor torch randn torch randn torch randn layout=torch jagged f nt TODO Replace public API when we can use properties torch ones_like nt nt _get_min_seqlen dynamic False True None assertFalse _recompiles_for_inputs f nt nt dynamic=dynamic dtypes torch float skipIfTorchDynamo Test compiles internally unittest skipIf sys version_info = torch compile supported python + unittest skipIf IS_WINDOWS reason= Windows yet supported torch compile skipCUDAIf SM OrLater GPU capability SM skipCUDAIfRocm test_compile_with_propagated_dynamic_max_seq_len device dtype shape B D max seq len nt = torch nested nested_tensor torch randn torch randn torch randn layout=torch jagged max seq len nt = torch nested nested_tensor torch randn torch randn torch randn layout=torch jagged f nt nt = nt sin + TODO Replace public API when we can use properties torch ones_like nt nt _get_max_seqlen ref = f nt output = torch compile f fullgraph=True dynamic=False nt assertEqual ref output dynamic False True None assertFalse _recompiles_for_inputs f nt nt dynamic=dynamic test_dropout_inference_mode device seq_len = embed_dim = nt = torch nested nested_tensor torch randn seq_len embed_dim device=device torch randn seq_len embed_dim device=device layout=torch jagged device=device torch inference_mode torch nn functional dropout nt p= dtypes torch float torch double torch half test_unbind_backward device dtype nt = torch nested nested_tensor torch randn device=device torch randn device=device torch randn device=device layout=torch jagged requires_grad=True b c = nt unbind b sum backward torch _dynamo disable check nt expected_grad = torch zeros_like nt expected_grad unbind add_ assertEqual nt grad expected_grad check nt dtypes torch float torch double torch half torch bool parametrize nt_dim parametrize requires_grad False True test_to_padded_tensor device dtype nt_dim requires_grad dtype torch bool requires_grad grads supported bool nt_dim == post_seq_len_shape = nt_dim == post_seq_len_shape = nt_dim == post_seq_len_shape = nt = torch nested nested_tensor torch randint n post_seq_len_shape device=device dtype=dtype dtype torch bool torch randn n post_seq_len_shape device=device dtype=dtype n range layout=torch jagged requires_grad=requires_grad PADDING_VAL = expected_padded = nt _values new_full post_seq_len_shape PADDING_VAL i component enumerate nt unbind expected_padded i component shape copy_ component padded = nt to_padded_tensor PADDING_VAL assertEqual expected_padded padded convert padded dense - NJT torch nested _internal nested_tensor nested_from_padded nt = nested_from_padded padded nt offsets assertEqual nt nt requires_grad dtype torch bool ensure gradients flow through conversions nt backward torch ones_like nt assertEqual nt grad torch ones_like nt blows up due test parametrization otherwise torch _dynamo utils disable_cache_limit skipIfTorchDynamo SDPA test compiles internally unittest skipIf IS_WINDOWS reason= Windows yet supported torch compile skipCUDAIf SM OrLater GPU capability SM skipCUDAIfRocm dtypes torch float torch double torch half parametrize nt_dim parametrize requires_grad False True test_to_padded_tensor_compile device dtype nt_dim requires_grad dtype torch bool requires_grad grads supported bool nt_dim == post_seq_len_shape = nt_dim == post_seq_len_shape = nt_dim == post_seq_len_shape = nt = torch nested nested_tensor torch randint n post_seq_len_shape device=device dtype=dtype dtype torch bool torch randn n post_seq_len_shape device=device dtype=dtype n range layout=torch jagged requires_grad=requires_grad f x x sin + torch nested _internal nested_tensor nested_from_padded torch compile fullgraph=True g nt _g nt PADDING_VAL = padded = nt to_padded_tensor PADDING_VAL padded = f padded NB sum_S must specified use lowering dense - jagged get full fusion nested_from_padded padded nt offsets sum_S=nt values shape NB use checkpointing force fusion torch utils checkpoint checkpoint _g nt use_reentrant=False expected_output = f nt requires_grad expected_output backward torch ones_like expected_output expected_grad = nt grad detach clone nt grad = None torch _inductor utils run_and_get_code compiled_output generated_code = run_and_get_code g nt requires_grad compiled_output backward torch ones_like compiled_output compiled_grad = nt grad detach clone assertEqual compiled_grad expected_grad rtol= e- atol= e- assertEqual compiled_output expected_output rtol= e- atol= e- === Verify computation fusion happens === Fallback op call - fusion didn t happen fallback_op_calls_present = any torch ops aten _padded_dense_to_jagged_forward default generated_code i torch ops aten _jagged_to_padded_dense_forward default generated_code i i range len generated_code NB Fusion isn t supported CPU assertEqual cuda device fallback_op_calls_present i range len generated_code Examine buffer construction lines generated code determine whether fusion occurred If fusion happens D buffer shape B max_seqlen D should never materialized buffer_constructions = line strip line generated_code i split \n empty_strided_cuda line buffer_dims = buffer dim == number elements tensor size tuple arg len ast parse t body value args elts t buffer_constructions cuda device assertFalse any d == d buffer_dims dtypes torch float skipIfTorchDynamo Test compiles internally unittest skipIf sys version_info = torch compile supported python + unittest skipIf IS_WINDOWS reason= Windows yet supported torch compile skipCUDAIf SM OrLater GPU capability SM skipCUDAIfRocm test_compile_padded_dense_conversion_preserves_metadata_cache device dtype shape B D nt = random_nt_from_dims None device=device dtype=dtype layout=torch jagged requires_grad=True expect min max seqlen stored here cache = dict nt _metadata_cache torch compile g nt padded = nt to_padded_tensor intermediate = padded sin + torch nested _internal nested_tensor nested_from_padded nested_from_padded intermediate nt offsets min_seqlen=nt _min_seqlen max_seqlen=nt _max_seqlen sum_S=nt values shape output = g nt output backward torch ones_like output assertEqual output _metadata_cache cache See https github com pytorch pytorch issues dtypes torch float test_composite_op_in_inference_mode device dtype expect view nt = random_nt_from_dims None device=device dtype=dtype layout=torch jagged requires_grad=True torch inference_mode output = nt reshape - assertEqual output shape nt shape assertTrue output _is_view expect copy nt = random_nt_from_dims None device=device dtype=dtype layout=torch jagged requires_grad=True transpose - - torch inference_mode output = nt reshape - assertEqual output shape nt shape assertFalse output _is_view dtypes torch float test_composite_op_with_custom_mode device dtype torch utils _python_dispatch TorchDispatchMode simple passthrough TorchDispatchMode CustomDispatchMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None func args kwargs nt = random_nt_from_dims None device=device dtype=dtype layout=torch jagged requires_grad=True CustomDispatchMode res = nt reshape - assertEqual res shape nt shape skipIfTorchDynamo compiles internally unittest skipIf IS_WINDOWS reason= Windows yet supported torch compile skipCUDAIf SM OrLater GPU capability SM dtypes torch float torch _dynamo config patch capture_dynamic_output_shape_ops=True torch _dynamo config patch capture_scalar_outputs=True test_broadcast_shapes_on_in_graph_constructed_njt device dtype Tests guard isn t wrongly installed freshly-created nested int when broadcast_shapes used NJT shapes See https github com pytorch pytorch issues more context nt = torch nested nested_tensor torch randn torch randn torch randn layout=torch jagged device=device dtype=dtype values = nt _values detach clone offsets = nt _offsets detach clone torch compile fullgraph=True f values offsets nt = torch nested nested_tensor_from_jagged values offsets NB torch where utilizes broadcast_shapes underneath torch where nt torch ones_like nt torch zeros_like nt output = f values offsets assertTrue output is_nested assertEqual nt shape - output shape - nt_component output_component zip nt unbind output unbind assertEqual nt_component shape output_component shape The following lists specify skips xfails particular SampleInputs Note these attempted matched top bottom only one most will matched so order matters The guiding general principle here should one xfail skip per bug all possible FORWARD_SKIPS_AND_XFAILS = implemented XFailRule error_type=NotImplementedError op_match_fn=lambda device op op full_name unary needs log_sigmoid_forward which returns tuple nn functional logsigmoid nn functional prelu needs rrelu_with_noise nn functional rrelu binary __rsub__ complex floor_divide polar rsub reduction count_nonzero linalg vector_norm nansum std std unbiased var var unbiased hash_tensor name= not_implemented expected torch where support has some limitations condition must NJT no dense tensors higher dim than NJT XFailRule error_type=ValueError error_msg= expected condition jagged layout NestedTensor op_match_fn=lambda device op op full_name == where sample_match_fn=lambda device sample sample kwargs condition is_nested XFailRule error_type=ValueError error_msg= broadcasting nested tensors dense tensors equal higher dim op_match_fn=lambda device op op full_name == where sample_match_fn=lambda device sample sample input is_nested sample input dim = sample kwargs condition dim sample kwargs other is_nested sample kwargs other dim = sample kwargs condition dim expected masked ops don t support jagged layout XFailRule error_type=ValueError error_msg= expects strided op_match_fn=lambda device op op full_name masked amax masked amin masked argmax masked argmin masked logsumexp masked mean masked norm masked prod masked std masked sum masked var name= no_masked_jagged_support Op doesn t support lengths being present XFailRule error_type=ValueError error_msg= expected input contiguous jagged layout NestedTensor op_match_fn=lambda device op op full_name == nn functional linear sample_match_fn=lambda device sample sample input _lengths None name= no_linear_noncontig_holes_support nanmean sometimes hits unimplemented nansum path other times hits unimplemented sum path XFailRule error_type=NotImplementedError op_match_fn=lambda device op op full_name == nanmean sample_match_fn=lambda device sample noncontig_holes sample name dim sample kwargs isinstance sample kwargs dim int sample kwargs dim == sample input _ragged_idx isinstance sample kwargs dim tuple list sample input _ragged_idx sample kwargs dim name= nansum_unimplemented expected reducing across ragged dimension supported non-contiguous nested tensors holes XFailRule error_type=RuntimeError error_msg= reducing across ragged dimension supported non-contiguous nested tensors holes op_match_fn=lambda device op min reduction_with_dim max reduction_with_dim aren t associated ReductionOpInfo entries sadly even though they re reductions isinstance op ReductionOpInfo reduction_with_dim op full_name sample_match_fn=lambda device sample noncontig_holes sample name dim sample kwargs isinstance sample kwargs dim int sample kwargs dim == sample input _ragged_idx isinstance sample kwargs dim tuple list sample input _ragged_idx sample kwargs dim name= ragged_dim_reduction_noncontig_holes expected index_put doesn t work non-contiguous NJTs without ragged dimension indices XFailRule error_type=RuntimeError error_msg= If ragged dimension part indices only works contiguous NJTs op_match_fn=lambda device op op full_name == index_put sample_match_fn=lambda device sample sample input is_contiguous len sample kwargs indices - sample input _ragged_idx name= index_put_noncontig_holes_no_ragged_dim_indices select only supports dim= non-contiguous holes NJTs now XFailRule op_match_fn=lambda device op op full_name == select sample_match_fn=lambda device sample sample kwargs dim = noncontig_holes sample name name= unsupported_select_on_non_batch_dim_with_noncontig_holes these don t work non-contiguous NJTs yet XFailRule error_type=ValueError error_msg= expected contiguous jagged layout NestedTensor op_match_fn=lambda device op op full_name chunk masked_select narrow split split_with_sizes squeeze sample_match_fn=lambda device sample sample input _lengths None sample input _ragged_idx = name= missing_noncontig_support these don t work ragged dim yet XFailRule error_type=RuntimeError error_msg= supported NestedTensor ragged dim op_match_fn=lambda device op op full_name chunk narrow select split sample_match_fn=lambda device sample ragged_dim sample name name= ragged_dim_unsupported XFailRule error_type=RuntimeError error comes usage view decomp error_msg= does support ragged_idx = except when op_match_fn=lambda device op op full_name == unflatten sample_match_fn=lambda device sample noncontig_transposed sample name name= unflatten_ragged_dim_unsupported these don t work batch dim yet XFailRule error_type=RuntimeError error_msg= supported NestedTensor dim= op_match_fn=lambda device op op full_name narrow split split_with_sizes unsqueeze sample_match_fn=lambda device sample batch_dim sample name name= batch_dim_unsupported XFailRule error_type=RuntimeError error comes usage view decomp error_msg= cannot view shape op_match_fn=lambda device op op full_name == unflatten sample_match_fn=lambda device sample batch_dim sample name name= unflatten_batch_dim_unsupported expected bmm matmul sometimes use to_padded_tensor fallback which isn t supported non-contig NJTs holes XFailRule error_type=RuntimeError error_msg= supported nested tensors holes op_match_fn=lambda device op op full_name bmm matmul sample_match_fn=lambda device sample noncontig_holes sample name other name matmul arg mat name bmm arg sample input dim == sample kwargs get other sample kwargs get mat dim name= mm_noncontig_holes some jiterator op failures due unsupported jagged layout XFailRule error_type=RuntimeError error_msg= unsupported tensor layout op_match_fn=lambda device op op full_name jiterator_binary jiterator_binary_return_by_ref jiterator_unary name= no_jiterator_jagged_support Bug when broadcasting binary op non-contiguous holes NJT + dense tensor ragged dim XFailRule error_type=RuntimeError error_msg= cannot call binary pointwise function inputs shapes op_match_fn=lambda device op isinstance op BinaryUfuncInfo sample_match_fn=lambda device sample noncontig_holes sample name broadcasting over ragged sample name name= binary_noncontig_holes_broadcasting_ _over_ragged BACKWARD_SKIPS_AND_XFAILS = segfaults so skip It s trying use NST logic NJT SkipRule op_match_fn=lambda device op op full_name == split_with_sizes name= split_with_sizes_backward_segfault FORWARD_SKIPS_AND_XFAILS Backwards generally broken non-contiguous NJTs holes Rather than determine exceptions detail just skip now Fix ensure summing over gradients during backwards after broadcasting takes into account holes lengths SkipRule op_match_fn=lambda device op isinstance op BinaryUfuncInfo op full_name mean where unsqueeze sample_match_fn=lambda device sample noncontig_holes sample name name= broken_noncontig_holes_backward mean need examine backwards formula XFailRule error_type=RuntimeError error_msg= SymIntArrayRef expected contain only concrete integers op_match_fn=lambda device op op full_name mean sample_match_fn=lambda device sample full reduction sample name normal dim reduction sample name name= broken_mean_backward RuntimeError expand cannot expand shape j - j noncontig transposed inputs mean XFailRule error_type=RuntimeError error_msg= cannot expand shape op_match_fn=lambda device op op full_name == mean sample_match_fn=lambda device sample normal dim reduction sample name noncontig_transposed sample name name= broken_mean_backward unsqueeze backward tries call squeeze noncontig transposed s supported XFailRule error_type=ValueError error_msg= expected contiguous jagged layout NestedTensor op_match_fn=lambda device op op full_name == unsqueeze sample_match_fn=lambda device sample noncontig_transposed sample name ragged_dim sample name name= broken_unsqueeze_backward RuntimeError view cannot view shape j j unflatten XFailRule error_type=RuntimeError error_msg= cannot view shape op_match_fn=lambda device op op full_name unflatten sample_match_fn=lambda device sample noncontig_holes sample name name= broken_unflatten_backward sum backward implemented non-full reductions XFailRule error_type=NotImplementedError error_msg= aten _nested_sum_backward default op_match_fn=lambda device op op full_name == sum sample_match_fn=lambda device sample full reduction sample name name= broken_sum_backward squeeze invalid gradient shape need check formula XFailRule error_type=RuntimeError error_msg= returned invalid gradient index op_match_fn=lambda device op op full_name == squeeze sample_match_fn=lambda device sample sample name == D_contig_with_seqlen_cache normal_dim sample kwargs dim == name= broken_squeeze_backward sgn masked_select backwards formulas don t work all XFailRule error_type=RuntimeError error_msg= NestedTensor does support directly calling torch ops aten size op_match_fn=lambda device op op full_name sgn masked_select name= broken_sgn_masked_select_backward select grad_output NJT non-batch-dim operation XFailRule error_type=ValueError error_msg= expected grad_output tensor op_match_fn=lambda device op op full_name == select sample_match_fn=lambda device sample batch_dim sample name name= broken_select_backward prod completely broken every way XFailRule op_match_fn=lambda device op op full_name == prod name= broken_prod_backward pow float_power use where underneath broken NT T broadcasting cases XFailRule error_type=ValueError error_msg= expected condition jagged layout NestedTensor op_match_fn=lambda device op op full_name pow float_power sample_match_fn=lambda device sample NT T sample name name= broken_pow_backward __rpow__ backward also broken reverse T NT broadcasting cases XFailRule error_type=ValueError error_msg= expected condition jagged layout NestedTensor op_match_fn=lambda device op op full_name == __rpow__ sample_match_fn=lambda device sample T NT sample name name= broken_rpow_backward linear some formula problem when bias used seems platform-specific fails locally CI SkipRule result use_count = INTERNAL ASSERT FAILED op_match_fn=lambda device op op full_name == nn functional linear sample_match_fn=lambda device sample bias sample name name= broken_linear_backward narrow unimplemented backward XFailRule error_type=RuntimeError error_msg= derivative aten narrow implemented op_match_fn=lambda device op op full_name == narrow name= broken_narrow_backward min max need factory function support ragged dim reductions where output dense sizes still contain nested int XFailRule error_type=RuntimeError error_msg= SymIntArrayRef expected contain only concrete integers op_match_fn=lambda device op op full_name max reduction_with_dim min reduction_with_dim sample_match_fn=lambda device sample ragged dim sample name name= broken_min_max_reduction_with_dim_backward_on_ragged_dim copysign formula broken T NT broadcasting XFailRule error_type=RuntimeError error_msg= SymIntArrayRef expected contain only concrete integers op_match_fn=lambda device op op full_name == copysign sample_match_fn=lambda device sample T NT sample name name= broken_copysign_backward amin amax broken host ways I don t think s good use time try sift through SkipRule op_match_fn=lambda device op op full_name amin amax name= broken_amin_amax_backward XFailRule error_type=RuntimeError error_msg= reducing across ragged dimension supported non-contiguous op_match_fn=lambda device op isinstance op BinaryUfuncInfo doesn t happen these ops some reason op full_name copysign max binary maximum min binary minimum sample_match_fn=lambda device sample NT T broadcasting all s sample name noncontig_holes sample name name= binary_noncontig_holes_ragged_dim_reduction XFailRule error_type=RuntimeError error_msg= reducing across ragged dimension supported non-contiguous op_match_fn=lambda device op op full_name == nn functional rms_norm sample_match_fn=lambda device sample sample input _lengths None name= rms_norm_noncontig_holes_ragged_dim_reduction expected autodiff complex dtype supported XFailRule error_type=RuntimeError error_msg= _nested_view_from_jagged does support automatic differentiation outputs complex dtype op_match_fn=lambda device op op full_name cdouble cfloat chalf name= no_complex_autodiff Bug need use correct nested int shape XFailRule error_type=RuntimeError error_msg= Function CloneBackward returned invalid gradient op_match_fn=lambda device op op full_name == clone sample_match_fn=lambda device sample sample kwargs get memory_format None == torch contiguous_format name= clone_wrong_nested_int_for_gradient some min max ops use masked_fill_ underneath sometimes which isn t implemented XFailRule error_type=NotImplementedError error_msg= aten masked_fill_ Scalar op_match_fn=lambda device op op full_name max binary min binary minimum maximum copysign name= unimplemented_masked_fill COMPILE_FORWARD_SKIPS_AND_XFAILS = FORWARD_SKIPS_AND_XFAILS Bug cross-device conversions result new nested ints within compile only XFailRule error_type=AssertionError error_msg= The values attribute shape do match op_match_fn=lambda device op op full_name == sample_match_fn=lambda device sample - cpu sample name name= cross_device_transfer_wrong_nested_int_in_compile clone - preserve format non-contiguous NJT holes currently uses unbind leading data-dependent expression Should fixed via torch _check XFailRule error_type=torch _dynamo exc Unsupported Ne u u unhinted Ne u u Size-like symbols u u error_msg= Could guard data-dependent expression op_match_fn=lambda device op op full_name == clone sample_match_fn=lambda device sample noncontig_holes sample name sample kwargs get memory_format None == torch contiguous_format name= clone_unbind_data_dependency chunk broken several ways batch dim revisit after similar data-dependency issues handled narrow SkipRule op_match_fn=lambda device op op full_name == chunk sample_match_fn=lambda device sample batch_dim sample name name= broken_chunk_compile_backward_on_batch_dim select batch dim currently uses unbind leading data-dependent error torch compile needs addressed via torch _check XFailRule error_type=torch _dynamo exc InternalTorchDynamoError error_msg= Pending unbacked symbols op_match_fn=lambda device op op full_name == select sample_match_fn=lambda device sample batch_dim sample name name= broken_select_backward_unbacked COMPILE_BACKWARD_SKIPS_AND_XFAILS = non-contiguous holes inputs + torch compile doesn t work great today need torch _check statements Skip these handle them later SkipRule op_match_fn=lambda device op True sample_match_fn=lambda device sample noncontig_holes sample name name= noncontig_holes_data_dependency mean weird bug XFailRule error_type=torch _dynamo exc BackendCompilerFailed error_msg= NestedIntNode object has no attribute sub op_match_fn=lambda device op op full_name == mean sample_match_fn=lambda device sample full reduction sample name normal dim reduction sample name name= broken_mean_compile_backward min max weird bug XFailRule error_type=AttributeError error_msg= NestedIntNode object has no attribute add op_match_fn=lambda device op op full_name max reduction_with_dim min reduction_with_dim sample_match_fn=lambda device sample ragged dim sample name name= broken_min_max_compile_backward fails data-dependent guards OR Unknown layout record_stream_any_impl need fix torch _check etc XFailRule op_match_fn=lambda device op op full_name == sample_match_fn=lambda device sample - cpu sample name name= to_data_dependency copysign formula broken T NT broadcasting XFailRule error_type=AttributeError error_msg= NestedIntNode object has no attribute add op_match_fn=lambda device op op full_name == copysign sample_match_fn=lambda device sample T NT sample name name= broken_copysign_compile_backward compile these complex ops use view_as_real which isn t implemented XFailRule error_type=NotImplementedError error_msg= aten view_as_real default op_match_fn=lambda device op op full_name cdouble cfloat chalf name= unimplemented_view_as_real COMPILE_FORWARD_SKIPS_AND_XFAILS BACKWARD_SKIPS_AND_XFAILS COMPARE_TENSOR_COMPONENT_EQUALITY = masked_select expected output different shape masked_select OpInfo-based NJT tests These tests utilize NJT-specific op_db generated standard op_db Note certain tradeoffs made wrt coverage vs time spent running tests All tests run dtype=torch float only TestNestedTensorOpInfo NestedTensorTestCase TODO move _gen_grad_outputs out_val isinstance out_val list tuple need_grad_outs = tuple o o out_val o grad_fn None grad_outputs = tuple torch ones_like o o out_val o grad_fn None need_grad_outs grad_outputs out_val torch ones_like out_val ops op op njt_op_db op supports_njt allowed_dtypes= torch float tf _on_and_off sample_skips_and_xfails FORWARD_SKIPS_AND_XFAILS test_forward device dtype op sample subtest_ctx skip_xfail_ctx op sample_inputs device=device dtype=dtype requires_grad=False use_subtests=True subtest_ctx skip_xfail_ctx compare reference expect different nested int out = op op sample input sample args sample kwargs out_ref = op ref op sample assertEqualIgnoringNestedInts out out_ref op _extra_op_data is_view tree_map_only NestedTensor lambda x assertTrue x _is_view out TODO Revisit once https github com pytorch pytorch pull lands TODO Add xfails other inplace ops instead hardcoding op inplace_variant index_put op full_name op inplace_variant sample input sample args sample kwargs assertEqualIgnoringNestedInts sample input out_ref ops op op njt_op_db op supports_njt op supports_autograd allowed_dtypes= torch float tf _on_and_off sample_skips_and_xfails BACKWARD_SKIPS_AND_XFAILS test_backward device dtype op sample subtest_ctx skip_xfail_ctx op sample_inputs device=device dtype=dtype requires_grad=True use_subtests=True subtest_ctx skip_xfail_ctx compare reference expect different nested int out = op op sample input sample args sample kwargs out_ref = op ref op sample assertEqualIgnoringNestedInts out out_ref op _extra_op_data is_view tree_map_only NestedTensor lambda x assertTrue x _is_view out inps _ = tree_flatten sample input sample args sample kwargs g_inps = inp inp inps isinstance inp torch Tensor inp requires_grad len g_inps need_grad_outs grad_outputs = _gen_grad_outputs out grads = torch autograd grad need_grad_outs inputs=g_inps grad_outputs=grad_outputs need_grad_outs grad_outputs = _gen_grad_outputs out_ref grads_ref = torch autograd grad need_grad_outs inputs=g_inps grad_outputs=grad_outputs assertEqualNoncontigAware grads grads_ref ops op op njt_op_db op supports_njt allowed_dtypes= torch float torch _dynamo config patch capture_dynamic_output_shape_ops=True needed avoid data dependent operator aten _local_scalar_dense default torch _dynamo config patch capture_scalar_outputs=True sample_skips_and_xfails COMPILE_FORWARD_SKIPS_AND_XFAILS test_compile_forward device dtype op sample subtest_ctx skip_xfail_ctx op sample_inputs device=device dtype=dtype requires_grad=False use_subtests=True subtest_ctx skip_xfail_ctx torch compiler reset op_fn = op op f args kwargs op_fn args kwargs compiled_f = torch compile f fullgraph=True backend= aot_eager_decomp_partition out_ref = f sample input sample args sample kwargs out_compile = compiled_f sample input sample args sample kwargs op _extra_op_data is_view tree_map_only NestedTensor lambda x assertTrue x _is_view out_ref op full_name COMPARE_TENSOR_COMPONENT_EQUALITY assertEqualIgnoringNestedInts out_compile out_ref assertEqual out_compile out_ref TODO Revisit once https github com pytorch pytorch pull lands TODO Add xfails other inplace ops instead hardcoding op inplace_variant index_put op full_name op_fn = op inplace_variant in_f args kwargs op_fn args kwargs compiled_in_f = torch compile in_f fullgraph=True backend= aot_eager_decomp_partition compiled_in_f sample input sample args sample kwargs op full_name COMPARE_TENSOR_COMPONENT_EQUALITY assertEqualIgnoringNestedInts sample input out_ref assertEqual sample input out_ref ops op op njt_op_db op supports_njt op supports_autograd allowed_dtypes= torch float torch _dynamo config patch capture_dynamic_output_shape_ops=True needed avoid data dependent operator aten _local_scalar_dense default torch _dynamo config patch capture_scalar_outputs=True sample_skips_and_xfails COMPILE_BACKWARD_SKIPS_AND_XFAILS test_compile_backward device dtype op sample subtest_ctx skip_xfail_ctx op sample_inputs device=device dtype=dtype requires_grad=True use_subtests=True subtest_ctx skip_xfail_ctx torch compiler reset op_fn = op op f args kwargs op_fn args kwargs compiled_f = torch compile f fullgraph=True backend= aot_eager_decomp_partition out_ref = f sample input sample args sample kwargs out_compile = compiled_f sample input sample args sample kwargs op _extra_op_data is_view tree_map_only NestedTensor lambda x assertTrue x _is_view out_ref op full_name COMPARE_TENSOR_COMPONENT_EQUALITY assertEqualIgnoringNestedInts out_compile out_ref assertEqual out_compile out_ref inps _ = tree_flatten sample input sample args sample kwargs g_inps = inp inp inps isinstance inp torch Tensor inp requires_grad len g_inps need_grad_outs grad_outputs = _gen_grad_outputs out_compile grads_compile = torch autograd grad need_grad_outs inputs=g_inps grad_outputs=grad_outputs need_grad_outs grad_outputs = _gen_grad_outputs out_ref grads_ref = torch autograd grad need_grad_outs inputs=g_inps grad_outputs=grad_outputs assertEqualNoncontigAware grads_compile grads_ref torch _dynamo config patch capture_dynamic_output_shape_ops=True needed avoid data dependent operator aten _local_scalar_dense default torch _dynamo config patch capture_scalar_outputs=True skipIfTorchDynamo Dynamo fails pending unbacked symints assertEqual ref_y item test_nested_tensor_non_contiguous_mutation fn x x x = x _inp base = torch zeros v = base t torch nested nested_tensor_from_jagged v offsets=torch tensor torch ones ref_x ref_x = _inp ref_y = fn ref_x ref_x assertEqual ref_y item y = torch compile fn fullgraph=True backend= aot_eager _inp assertEqual y test_nested_tensor_input_mutation_backward See Note AOTAutograd Tangent Subclassness mutated inputs NJT tangent always subclass See torch csrc autograd python_function cpp use_zeros_like This test checks AOTD correctly guess NJT tangent NJT fn x x mul_ x + _inp v = torch zeros requires_grad=True torch nested nested_tensor_from_jagged v offsets=torch tensor clone ref_x = _inp ref_y = fn ref_x ref_y sum backward x = _inp y = torch compile fn fullgraph=True backend= aot_eager x y sum backward torch nested _internal nested_int NestedIntNode TestNestedInt torch testing _internal common_utils TestCase test_comparisons = torch SymInt NestedIntNode b = torch SymInt NestedIntNode c = torch SymInt NestedIntNode d = assertTrue == assertTrue == b assertFalse = assertFalse = b assertFalse == c assertTrue = c assertFalse == d assertTrue = d assertFalse d == assertTrue d = ge assertTrue = assertTrue = b assertTrue b = assertRaises ValueError _ = = c assertRaises ValueError _ = c = assertRaises ValueError _ = c = assertTrue c = assertTrue c = assertFalse c = lt assertFalse assertFalse b assertFalse b assertRaises ValueError _ = c assertRaises ValueError _ = c assertRaises ValueError _ = assertRaises ValueError _ = assertTrue le assertTrue = assertTrue b = assertTrue = b assertRaises ValueError _ = = c assertRaises ValueError _ = c = assertRaises ValueError _ = = c assertTrue c = assertTrue c = assertFalse c = gt assertFalse assertFalse b assertFalse b assertRaises ValueError _ = c assertRaises ValueError _ = c assertRaises ValueError _ = assertRaises ValueError _ = assertTrue test_with_factor = torch SymInt NestedIntNode b = torch SymInt NestedIntNode eq assertFalse == b assertFalse = b assertTrue b = assertTrue = b assertFalse b = ne assertTrue = b mul assertTrue == b assertTrue = b assertTrue == instantiate_parametrized_tests TestNestedTensor instantiate_device_type_tests TestNestedTensorDeviceType globals instantiate_device_type_tests TestNestedTensorAutograd globals instantiate_device_type_tests TestNestedTensorSubclass globals instantiate_device_type_tests TestNestedTensorOpInfo globals __name__ == __main__ run_tests