__future__ annotations contextlib dataclasses enum functools logging re threading traceback unittest mock weakref abc abstractmethod collections defaultdict contextlib contextmanager dataclasses dataclass typing Any Generic NamedTuple Optional TYPE_CHECKING TypeVar Union torch torch utils _pytree pytree torch utils _python_dispatch is_traceable_wrapper_subclass torch utils _traceback CapturedTraceback format_frame torch utils weak WeakTensorKeyDictionary log = logging getLogger __name__ TYPE_CHECKING collections abc Callable Generator Iterator types CodeType sympy torch _dynamo backends distributed DDPOptimizerContext torch _dynamo codegen PyCodegen torch _functorch _aot_autograd schemas ViewAndMutationMeta torch _subclasses fake_tensor FakeTensorMode torch _guards definitional source truth general purpose guard structures An important thing keep mind here preservation layering There should no dynamo notions no guard installation notions here COMPILE_ID_PATTERN = re compile r ^ P frame_id \d+ P frame_compile_id \d+ $ CA_COMPILE_ID_PATTERN = re compile r ^ P compiled_autograd_id \d+ P frame_id \d+ P frame_compile_id \d+ $ Note Updating CompiledId CompiledId represents unique program-level identifier we want keep property codebase evolves This property relied even outside pytorch repo e g tlparse other internal tooling The in-memory format can freely changed those dependencies only consume string serialization The string form should Program-level uid CompileId can uniquely identify compiled graph Storage efficient This object logged nearly every entry We should elide symbols when possible Compact The string form directly displayed some tools Special symbols okay dataclass frozen=True kw_only=True slots=True CompileId frame_id int &#124; None This id per-frame counts how many times we ve compiled frame This could have been global id having per-frame gives you better intuitive sense how many recompiles have occurred so far frame_compile_id int &#124; None torch compiling compiled autograd graph compiled_autograd_id int &#124; None = None TODO consider also tracking recompilation count See Note Updating CompileId __str__ - str NOTE Keep sync both from_string tlparse repo compiled_autograd_id None assert frame_id None == frame_compile_id None frame_str = frame_id None frame_str = f frame_id frame_compile_id f compiled_autograd_id frame_str assert frame_id None frame_compile_id None f frame_id frame_compile_id classmethod from_string cls compile_id Optional str - Optional CompileId Factory method creates CompileId its string representation Keep sync __str__ method compile_id None None try pattern COMPILE_ID_PATTERN CA_COMPILE_ID_PATTERN match = pattern match compile_id groups = match groupdict k v groups items v None groups k = int v cls groups type ignore arg-type raise ValueError except Exception e raise ValueError f Invalid compile_id compile_id e TraceId NamedTuple compile_id CompileId This starts off every time we restart analysis goes up one attempt int __str__ - str Keep sync tlparse repo attempt == str compile_id f compile_id _ attempt GuardSource enum Enum LOCAL = GLOBAL = LOCAL_SPECIALIZED_NN_MODULE = GLOBAL_SPECIALIZED_NN_MODULE = CONSTANT = RANDOM_VALUE = SHAPE_ENV = LOCAL_FSDP_MODULE = GLOBAL_FSDP_MODULE = BACKWARD_STATE = EPHEMERAL = SYNTHETIC_LOCAL = LOCAL_UNSPECIALIZED_NN_MODULE = GLOBAL_UNSPECIALIZED_NN_MODULE = LOCAL_UNSPECIALIZED_BUILTIN_NN_MODULE = GLOBAL_UNSPECIALIZED_BUILTIN_NN_MODULE = is_fsdp_module - bool GuardSource GLOBAL_FSDP_MODULE GuardSource LOCAL_FSDP_MODULE is_specialized_nn_module - bool torch _dynamo config config config _unsafe_skip_fsdp_module_guards GuardSource GLOBAL_SPECIALIZED_NN_MODULE GuardSource LOCAL_SPECIALIZED_NN_MODULE is_fsdp_module GuardSource GLOBAL_SPECIALIZED_NN_MODULE GuardSource LOCAL_SPECIALIZED_NN_MODULE is_unspecialized_nn_module - bool GuardSource GLOBAL_UNSPECIALIZED_NN_MODULE GuardSource LOCAL_UNSPECIALIZED_NN_MODULE GuardSource GLOBAL_UNSPECIALIZED_BUILTIN_NN_MODULE GuardSource LOCAL_UNSPECIALIZED_BUILTIN_NN_MODULE is_unspecialized_builtin_nn_module - bool GuardSource GLOBAL_UNSPECIALIZED_BUILTIN_NN_MODULE GuardSource LOCAL_UNSPECIALIZED_BUILTIN_NN_MODULE is_local - bool GuardSource LOCAL GuardSource LOCAL_SPECIALIZED_NN_MODULE GuardSource LOCAL_FSDP_MODULE GuardSource LOCAL_UNSPECIALIZED_NN_MODULE GuardSource LOCAL_UNSPECIALIZED_BUILTIN_NN_MODULE Base GuardBuilder role The GuardBuilderBase role represent scope within which build guard The name little confusing its builder sake avoiding lot renames keeping original reference torchdynamo s GuardBuilder Note create_fn invoked GuardBuilderBase Guard A GuardBuilder chosen based GuardSource s select function There value keeping GuardBuilderBase empty keep layering clean GuardBuilderBase pass dataclasses dataclass frozen=True SLoc framework_loc traceback FrameSummary &#124; str &#124; None maybe_user_loc str &#124; None __str__ - str floc = framework_loc isinstance framework_loc str format_frame framework_loc maybe_user_loc None f maybe_user_loc floc f floc ShapeGuard NamedTuple expr sympy logic boolalg Boolean sloc SLoc size_oblivious bool dataclasses dataclass slots=True Guard originating_source source called make_guard method construct guard object The property name specifies what exactly guard guarding The meaning name dependent create_fn you must look use-site inside create_fn know what name means That being said although you might think just name name usually arbitrary Python expression will evaluated all globals locals you create LOCAL guard extract Python object we want perform guard tests This evaluation typically happens GuardBuilder eval In these cases name typically produced originating_source name confused GuardSource - property source Occasionally name valid Python expression sometimes meaningless Example create_fns like include GRAD_MODE SHAPE_ENV originating_source Source create_fn Callable GuardBuilderBase Guard None Export only These values written time guard check_fn creation guard_types Optional list str = None code_list Optional list str = None obj_weakref Optional object = None guarded_class_weakref Optional weakref ReferenceType Any = None stack Optional CapturedTraceback = None user_stack Optional traceback StackSummary = None _hash Optional int = None _unserializable bool = False __hash__ - int _hash None _hash = hash name source id create_fn _hash sort_key - tuple bool int int str int Put duplicate input guards end The duplicate guards have two sources while guard name only considers one source is_duplicate_input = isinstance create_fn functools partial create_fn func torch _dynamo guards GuardBuilder DUPLICATE_INPUT is_duplicate_input source value source - len name name inner_create_fn __code__ co_firstlineno __lt__ other Guard - bool sort_key other sort_key inner_create_fn - Callable GuardBuilderBase Guard Any isinstance create_fn functools partial create_fn func create_fn property name - str originating_source name property source - GuardSource originating_source guard_source staticmethod weakref_to_str obj_weakref object - str This workaround Python weakref bug ` obj_weakref ` instance returned ` weakref ref ` ` str obj_weakref ` buggy original obj overrides __getattr__ e g MyConfig dict __getattr__ x x obj = MyConfig offset= obj_weakref = weakref ref obj str obj_weakref raise error KeyError __name__ isinstance obj_weakref weakref ReferenceType obj = obj_weakref obj None f weakref hex id obj_weakref obj __class__ __name__ hex id obj f weakref hex id obj_weakref dead str obj_weakref __repr__ - str s = f source name lower source repr name inner_create_fn __name__ guard_types guard_types code code_list obj_weakref weakref_to_str obj_weakref guarded_class guarded_class_weakref s __str__ - str output = f Name repr name \n source = source name lower source output += f Source source \n output += f Create Function inner_create_fn __name__ \n output += f Guard Types guard_types \n output += f Code List code_list \n output += f Object Weakref weakref_to_str obj_weakref \n output += f Guarded Class Weakref guarded_class_weakref \n output create builder GuardBuilderBase - Any try create_fn builder except Exception log exception Error while creating guard \n s str rstrip stack log error Created \n s join stack format - rstrip raise is_specialized_nn_module - bool source is_specialized_nn_module is_fsdp_module - bool source is_fsdp_module is_local - bool source is_local create_fn_name - str isinstance create_fn functools partial create_fn = create_fn func type ignore attr-defined create_fn = create_fn create_fn __name__ set_export_info guard_type str guarded_class Optional weakref ReferenceType Any code_list list str obj_weakref object - None guard_types guard_types = guard_types append guard_type assert guarded_class_weakref guarded_class None Guarded id must identical None guarded_class_weakref = guarded_class code_list code_list = code_list code_list extend code_list Some objects ephemeral e g list slice If we have multiple guards same object weakref can die between invocation set_export_info calls So dead weakref also acceptable assert obj_weakref obj_weakref None callable obj_weakref obj_weakref None Guarded object must identical None ephemeral dead weakref obj_weakref = obj_weakref T = TypeVar T Parent structure guard env expressions A GuardEnvExpr can have any subtype Note All subtypes must handled exhaustively torch _dynamo guards _parse_guard_env_guards avoid RuntimeError dataclasses dataclass frozen=True GuardEnvExpr pass A representing pair duplicate inputs input_pos_a input_pos_b input positions we have deduped dataclasses dataclass frozen=True DuplicateInputs GuardEnvExpr input_source_a Source input_source_b Source __post_init__ - None assert input_source_a = input_source_b A representing storage overlap relations among inputs aliases same storage Given set tensors alias same storage guard checks whether they actually have overlapping storages While non_overlapping_sources represent input tensors definitely don t have any storage overlapping any other input overlapping_sources represent tensors either Do overlap some other input tensor Might overlap some other input tensor we sure dataclasses dataclass frozen=True StorageOverlap GuardEnvExpr overlapping_sources list Source non_overlapping_sources list Source Checkpointable interface driving state snapshotting left purposely vague now copy_graphstate - T somewhat legacy name expected emit snapshot any type can also taken restore_graphstate T calls When snapshot moment implementation detail upstream callers Checkpointable does provide any guarantees around consistency idempotency safety calling its APIs yet In future will have closer coupling generic Checkpoint management system Checkpointable Generic T abstractmethod copy_graphstate - T abstractmethod restore_graphstate state T - None GuardsCheckpointState The GuardCheckpointState - T Checkpointable T GuardsContext dynamo_guards set Guard = set __init__ dynamo_guards set Guard - None dynamo_guards = dynamo_guards diff other GuardsCheckpointState - Optional set Guard Produces delta against another GuardsCheckpointState Returns None no delta found otherwise set mismatched Guard type objects r = dynamo_guards difference other dynamo_guards len r == None r __eq__ other object - bool isinstance other GuardsCheckpointState False diff other None ModuleContextCheckpointState nn_modules dict str torch nn Module = __init__ nn_modules dict str torch nn Module - None nn_modules = nn_modules diff other ModuleContextCheckpointState - Optional set str Produces delta against another ModuleContextCheckpointState Returns None no delta found otherwise set mismatched module key names r = set nn_modules keys difference set other nn_modules keys len r == None r __eq__ other object - bool isinstance other ModuleContextCheckpointState False diff other None ModuleContext Checkpointable ModuleContextCheckpointState __init__ - None nn_modules dict str Any = copy_graphstate - ModuleContextCheckpointState ModuleContextCheckpointState dict nn_modules restore_graphstate state ModuleContextCheckpointState - None assert isinstance state ModuleContextCheckpointState nn_modules = state nn_modules GlobalContextCheckpointState global_state dict str tuple Callable Any = __init__ global_states dict str tuple Callable Any - None global_state = global_states diff other GlobalContextCheckpointState - Optional set str Produces delta against another GlobalContextCheckpointState Returns None no delta found otherwise set mismatched global key names r = set global_state keys difference set other global_state keys len r == None r __eq__ other object - bool isinstance other GlobalContextCheckpointState False diff other None GlobalContext Checkpointable GlobalContextCheckpointState This keeps track global torch state during tracing function For example torch is_grad_enabled _supported_global_states = grad_enabled autocast_enabled autocast_cpu_enabled autocast_gpu_dtype autocast_cpu_dtype autocast_cache_enabled __init__ - None global_state dict str tuple Callable Any = copy_graphstate - GlobalContextCheckpointState GlobalContextCheckpointState global_state restore_graphstate state GlobalContextCheckpointState - None assert isinstance state GlobalContextCheckpointState global_state = state global_state assert len global_state == len _supported_global_states set global_state keys == _supported_global_states Global state mismatch func args global_state values func args Like Set Guard will record user stack all guards time they installed their destination GuardsSet __init__ inner Optional set Guard = None - None inner None inner = set inner = inner __iter__ - Iterator Guard iter inner __len__ - int len inner Subtraction along bool typically used determine delta added guards between checkpoints higher order ops __sub__ other GuardsSet - GuardsSet GuardsSet inner - other inner __bool__ - bool bool inner add guard Guard collect_debug_stack bool = True skip int = - None guard inner collect_debug_stack guard stack None guard stack = CapturedTraceback extract skip= + skip guard user_stack None guard user_stack = TracingContext extract_stack inner add guard update others set Guard - None o others g o add g skip= remove_guards_with_source source Source - None Delete all guards contains given source _dynamo source is_from_source inner = g g inner is_from_source g originating_source source A GuardsContext checkpointable representation all guards current tracing context It s lifecycle bound tracing context should never instantiated directly outside For passing around internal state representations object prefer extract them copy_graphstate produce GuardsCheckpointState GuardsContext Checkpointable GuardsCheckpointState __init__ - None dynamo_guards GuardsSet = GuardsSet aotautograd_guards list GuardEnvExpr = copy_graphstate - GuardsCheckpointState GuardsCheckpointState set dynamo_guards inner restore_graphstate state GuardsCheckpointState - None NB steals passed state assert isinstance state GuardsCheckpointState dynamo_guards = GuardsSet state dynamo_guards HopSubgraphCache abstractmethod add_dynamo_installed_submodule fn_id int identifier str - None abstractmethod get_dynamo_installed_submodules fn_id int - list str abstractmethod add_autograd_key_entry identifier str key Callable - None abstractmethod get_autograd_key_entry identifier str - Optional Callable abstractmethod add_proxy_dispatch_entry identifier str key Callable - None abstractmethod get_proxy_dispatch_entry identifier str - Optional Callable abstractmethod add_lazy_bwd_entry identifier str tangent_metadata tuple object gmod torch fx GraphModule - int abstractmethod get_lazy_bwd_entry identifier str tangent_metadata tuple object - tuple Optional torch fx GraphModule Optional int InvokeSubgraphCache HopSubgraphCache __init__ - None autograd_cache dict str Callable = proxy_dispatch_cache dict str Callable = dynamo_installed_submodules dict int list str = defaultdict list lazy_bwd_cache dict str dict tuple object tuple torch fx GraphModule int = defaultdict dict add_dynamo_installed_submodule fn_id int identifier str - None dynamo_installed_submodules fn_id append identifier get_dynamo_installed_submodules fn_id int - list str dynamo_installed_submodules get fn_id add_autograd_key_entry identifier str key Callable - None autograd_cache identifier = key get_autograd_key_entry identifier str - Optional Callable autograd_cache get identifier None add_proxy_dispatch_entry identifier str key Callable - None proxy_dispatch_cache identifier = key get_proxy_dispatch_entry identifier str - Optional Callable proxy_dispatch_cache get identifier None add_lazy_bwd_entry identifier str tangent_metadata tuple object gmod torch fx GraphModule - int Save number existing graph modules dictionary get suffix num_gmods = len lazy_bwd_cache identifier lazy_bwd_cache identifier tangent_metadata = gmod num_gmods num_gmods get_lazy_bwd_entry identifier str tangent_metadata tuple object - tuple Optional torch fx GraphModule Optional int identifier lazy_bwd_cache None None lazy_bwd_cache identifier get tangent_metadata None None HopDispatchSetCache __init__ - None Delayed avoid circular dependency torch _higher_order_ops invoke_subgraph invoke_subgraph hop_cache_map = invoke_subgraph InvokeSubgraphCache get_cache op torch _ops HigherOrderOperator - HopSubgraphCache &#124; None op hop_cache_map None hop_cache_map op type ignore index _TLS = threading local TracingContext source truth all currently accumulated information needed trace Its lifecycle kept when using TorchDynamo other systems open managing their own TracingContext mind The purpose TracingContext dumping ground god object rather avoid having plumb complex subsystems across multiple verticals Ex A common example guard accumulation between dynamo shape_env aot_autograd inductor Accessing current tracing context via TracingContext get allows users accumulate their own guards processing without needing know how plumb objects back up where frame interpretation happened Note you can end up multiple TracingContext single compilation frame we reset TracingContext whenever we restart analysis CompileContext more overarching context encompasses multiple restarts CompileContext staticmethod get - CompileContext assert _TLS compile_context None _TLS compile_context staticmethod try_get - CompileContext &#124; None getattr _TLS compile_context None __init__ compile_id Optional CompileId - None assert compile_id None isinstance compile_id CompileId compile_id CompileId &#124; None = compile_id attempt = Verbose ShapeEnv guards produced shape_env_guards list str = staticmethod current_compile_id - Optional CompileId = CompileContext try_get None None compile_id staticmethod current_trace_id - Optional TraceId = CompileContext try_get None None compile_id None None TraceId compile_id attempt TracingContext Provides currently installed TracingContext None Note staticmethod invocations outside ` tracing ` see below valid will None staticmethod try_get - TracingContext &#124; None getattr _TLS tracing_context None staticmethod get - TracingContext ctx = TracingContext try_get ctx raise RuntimeError TracingContext get must called within ongoing trace __init__ fake_mode Optional FakeTensorMode - None guards_context = GuardsContext module_context = ModuleContext global_context = GlobalContext previously_inlined_functions dict Any Any = dict previously_cleaned_instructions dict Any Any = dict fake_mode Optional FakeTensorMode = fake_mode frame_summary_stack list traceback FrameSummary = This morally part frame_summary_stack kept separate clarity As we process frame variable gets updated keep track what line we function We make function call gets cleared frame location pushed frame_summary_stack prepping variable inner frame s progress loc_in_frame Optional tuple str int str = None only set after aot_autograd fw_metadata Optional ViewAndMutationMeta = None only set when DDPOptimizer used ddp_optimizer_ctx Optional DDPOptimizerContext = None only set after aot_autograd aot_graph_name Optional list str = None params_flat Optional list Any = None params_flat_unwrap_subclasses Optional list Any = None params_unwrapped_to_flat_index Optional list Any = None extended calling convention backend compiler aot_autograd Per output what compiler specified stride output None no stride known This always HINT never SymInt would better SymInt I can t conveniently get Inductor atm Also careful accidentally induce guards SymInt you ever do change aot_autograd py you should check permutations preferentially output_strides list tuple int &#124; None &#124; None = None When True whenever we encounter int Dynamo tracing we will force unspec force size-like unbacked integer This currently used when processing certain lists ints known size-like may have entries we must specialize force_unspec_int_unbacked_size_like = False See note Tensor Fakification Symbol Caching tensor_to_context = WeakTensorKeyDictionary If true Aot Autograd will output Fake Tensors appropriate meta first invocation see note Returning Fake Tensors First AOT Autograd Call fakify_first_call = False hop_dispatch_set_cache = HopDispatchSetCache list code objects inlined functions traced_code list CodeType = clear - None Look note output_graph py function ` save_global_state ` context clearing global context global_context global_state = previously_inlined_functions clear previously_cleaned_instructions clear staticmethod contextmanager patch kwargs Any - Generator None None None prior = ctx = TracingContext get key kwargs keys KeyError invalid entry prior key = getattr ctx key key val kwargs items setattr ctx key val try yield finally key val prior items setattr ctx key val staticmethod extract_stack - traceback StackSummary = TracingContext try_get None traceback StackSummary stack = frame_summary_stack loc_in_frame None stack = stack + _populate_loc_in_frame_summary traceback StackSummary from_list stack _populate_loc_in_frame_summary - traceback FrameSummary assert loc_in_frame None filename lineno frame_name = loc_in_frame traceback FrameSummary filename lineno frame_name lookup_line=False Call when you want call into some code isn t necessarily associated current frame state staticmethod contextlib contextmanager clear_frame - Generator None None None tc = TracingContext get unittest mock patch object tc frame_summary_stack unittest mock patch object tc loc_in_frame None try yield except Exception e Prevent real_stack getting attached The invariant Exception real_stack we ve appropriately attached user stack we no longer need attach anything Because we cannot conveniently interpose when exception thrown we instead interpose everywhere we set what user stack set using context manager However our compiler stack does tail calls when calls into user compiler which point parent exception frames would incorrectly attach incorrect frame However somehow someone raised exception scope had stack example because they restoring user stack state appropriately they process node node we should respect Thus we cannot unconditionally set None hasattr e real_stack e real_stack = None type ignore attr-defined raise staticmethod contextlib contextmanager current_frame frame_summary Optional traceback FrameSummary - Generator None None None frame_summary can None solely take advantage real_stack attachment thrown exceptions tc = TracingContext get frame_summary None tc frame_summary_stack append frame_summary old = tc loc_in_frame tc loc_in_frame = None try yield except Exception e hasattr e real_stack e real_stack = tc extract_stack type ignore attr-defined raise finally frame_summary None tc frame_summary_stack pop tc loc_in_frame = old staticmethod contextlib contextmanager report_output_strides - Generator Optional list Optional tuple int None None tc = TracingContext try_get tc None yield None old_output_strides = tc output_strides tc output_strides = try yield tc output_strides finally tc output_strides = old_output_strides staticmethod set_current_loc filename str lineno int frame_name str - None Save current location frame Lazily generate framesummary TracingContext get loc_in_frame = filename lineno frame_name staticmethod get_traced_code - Optional list CodeType tc = TracingContext try_get tc None None tc traced_code contextmanager compile_context context Optional CompileContext - Generator Optional CompileContext None None old_context = getattr _TLS compile_context None _TLS compile_context = context try yield context finally _TLS compile_context = old_context contextmanager tracing context Optional TracingContext - Generator Optional TracingContext None None This function installs passed tracing context dynamic scoped global variable Calls TracingContext get while under ` tracing ` context will None old_context = getattr _TLS tracing_context None _TLS tracing_context = context try yield context except Exception e hasattr e real_stack context None e real_stack = context extract_stack type ignore attr-defined raise finally context None context fake_mode None context fake_mode shape_env None context fake_mode shape_env cleanup _TLS tracing_context = old_context Subclasses can found torch _dynamo source py TODO voz Consider toplevel torch _source py dataclasses dataclass frozen=True Source is_dict_key - bool False is_ephemeral - bool False reconstruct codegen PyCodegen - None raise NotImplementedError guard_source - GuardSource raise NotImplementedError name - str raise NotImplementedError make_guard fn Callable Any - Guard guard_source GuardSource CONSTANT raise NotImplementedError Guard fn is_specialized_nn_module - bool guard_source is_specialized_nn_module subguards_allowed - bool True you can guard attributes guard_source = GuardSource SYNTHETIC_LOCAL Subclasses can found torch _dynamo source py dataclasses dataclass frozen=True ChainedSource Source base Source is_dict_key - bool Recurse until you either hit ConstDictKey Source base is_dict_key is_ephemeral - bool base is_ephemeral get_base - Source current Source = while isinstance current ChainedSource current = current base current detect_fake_mode inputs Any = None - Optional FakeTensorMode Attempts detect what current fake mode If there one ambiently available TracingContext we preferentially use Otherwise we heuristically detect fake mode via following sources order priority - Currently active fake mode stack - Fake mode associated passed tensors inputs does have flattened torch _subclasses fake_tensor FakeTensor FakeTensorMode get_plain_tensors fake_modes = context = TracingContext try_get fake_mode = context fake_mode fake_mode None fake_modes append fake_mode tracing context torch utils _python_dispatch _get_current_dispatch_mode_stack i m enumerate reversed _get_current_dispatch_mode_stack isinstance m FakeTensorMode pyrefly ignore bad-argument-type fake_modes append m active fake mode i flat_inputs = pytree tree_leaves inputs i flat_input enumerate flat_inputs isinstance flat_input FakeTensor pyrefly ignore bad-argument-type fake_modes append flat_input fake_mode fake tensor input i is_traceable_wrapper_subclass flat_input out list Union torch Tensor int torch SymInt = get_plain_tensors flat_input out=out type ignore arg-type fake_tensors list FakeTensor = x x out isinstance x FakeTensor fake_modes extend pyrefly ignore bad-argument-type tensor fake_mode f subclass input i ix ix tensor enumerate fake_tensors fake_modes fake_mode desc i = fake_modes m desc i fake_modes assert fake_mode m f fake mode fake_mode desc i doesn t match mode m desc i \n\n pyrefly ignore missing-attribute f fake mode desc i allocated \n fake_mode stack \n pyrefly ignore missing-attribute f fake mode desc i allocated \n m stack pyrefly ignore bad-return fake_mode None active_fake_mode - Optional FakeTensorMode Inspects dispatch mode stack active fake mode returns Returns None no fake mode active torch _subclasses fake_tensor FakeTensorMode torch utils _python_dispatch _get_current_dispatch_mode_stack _ m enumerate reversed _get_current_dispatch_mode_stack isinstance m FakeTensorMode m None