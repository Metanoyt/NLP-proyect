mypy ignore-errors torch functools partial torch testing make_tensor torch testing _internal opinfo core OpInfo SampleInput torch testing _internal common_dtype all_types_and numpy np Note autograd Function db This collection autograd Function test cases written OpInfos so they can easily consumed OpInfo-based tests check subsystem supports autograd Function Axes - saves output input intermediate non-tensor - inputs output x single tensor tensors arbitrary objects - Uses mark_dirty mark_non_differentiable once_differentiable to_numpy tensor tensor cpu numpy NumpyCube torch autograd Function staticmethod forward input input_np = to_numpy input dinput = torch tensor input_np device=input device torch tensor input_np device=input device dinput staticmethod setup_context ctx inputs output ctx save_for_backward inputs output ctx save_for_forward inputs output staticmethod backward ctx grad_output grad_saved input dinput = ctx saved_tensors NumpyMul apply grad_output dinput + NumpyMul apply grad_saved input staticmethod vmap info in_dims input result = NumpyCube apply input result in_dims in_dims staticmethod jvp ctx input_tangent input dinput = ctx saved_tensors NumpyMul apply input_tangent dinput NumpyMul apply input_tangent input CubeGenVmap torch autograd Function generate_vmap_rule = True staticmethod forward x x x staticmethod setup_context ctx inputs outputs ctx save_for_backward inputs outputs ctx save_for_forward inputs outputs staticmethod backward ctx grad_output grad_saved _input dinput = ctx saved_tensors result = grad_output dinput + dinput result staticmethod jvp ctx input_tangent input dinput = ctx saved_tensors MulGenVmap apply input_tangent dinput NumpyMul apply input_tangent input sample_inputs_numpy_cube opinfo device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg low= high= args= NumpyCubeNotComposable torch autograd Function staticmethod forward input input_np = to_numpy input torch tensor input_np device=input device input_np staticmethod setup_context ctx inputs output _ input_np = output ctx input_np = input_np ctx device = inputs device staticmethod torch autograd function once_differentiable backward ctx grad_output grad_saved result_np = ctx input_np torch tensor result_np device=ctx device NumpyMul torch autograd Function staticmethod forward x y torch tensor to_numpy x to_numpy y device=x device staticmethod setup_context ctx inputs output ctx save_for_backward inputs ctx save_for_forward inputs staticmethod backward ctx grad_output x y = ctx saved_tensors gx = None ctx needs_input_grad gx = NumpyMul apply grad_output y gy = None ctx needs_input_grad gy = NumpyMul apply grad_output x gx gy staticmethod vmap info in_dims x y x_bdim y_bdim = in_dims x = x movedim x_bdim - x_bdim None x unsqueeze - y = y movedim y_bdim - y_bdim None y unsqueeze - result = NumpyMul apply x y result = result movedim - result staticmethod jvp ctx x_tangent y_tangent x y = ctx saved_tensors x_tangent y + y_tangent x sample_inputs_numpy_mul opinfo device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad Broadcasting yield SampleInput make_arg low= high= args= make_arg low= high= sample_inputs_numpy_mul_scalar opinfo device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg low= high= args= kwargs= scalar MulGenVmap torch autograd Function generate_vmap_rule = True staticmethod forward x y x y staticmethod setup_context ctx inputs outputs ctx save_for_backward inputs ctx save_for_forward inputs staticmethod backward ctx grad_output x y = ctx saved_tensors gx = None ctx needs_input_grad gx = MulGenVmap apply grad_output y gy = None ctx needs_input_grad gy = MulGenVmap apply grad_output x gx gy staticmethod jvp ctx x_tangent y_tangent x y = ctx saved_tensors x_tangent y + y_tangent x NumpyExp_ torch autograd Function staticmethod forward x x_np = to_numpy x np exp x_np x_np x staticmethod setup_context ctx inputs output x = inputs ctx mark_dirty x ctx save_for_backward output ctx save_for_forward output staticmethod backward ctx grad_output output = ctx saved_tensors NumpyMul apply grad_output output staticmethod vmap info in_dims x NumpyExp_ apply x x in_dims staticmethod jvp ctx x_tangent Doesn t call numpy operations because I didn t want write NumpyMul_ output = ctx saved_tensors x_tangent mul_ output x_tangent NumpySort torch autograd Function staticmethod forward x dim device = x device x = to_numpy x ind = np argsort x axis=dim ind_inv = np argsort ind axis=dim torch tensor x device=device torch tensor ind device=device torch tensor ind_inv device=device staticmethod setup_context ctx inputs output _x dim = inputs _ ind ind_inv = output ctx mark_non_differentiable ind ind_inv ctx save_for_backward ind ind_inv ctx save_for_forward ind ind_inv ctx dim = dim staticmethod backward ctx grad_output _ _ ind ind_inv = ctx saved_tensors NumpyTake apply grad_output ind_inv ind ctx dim None staticmethod vmap info in_dims x dim x_bdim _ = in_dims x = x movedim x_bdim wrap dim dim = dim dim = dim + x dim - NumpySort apply x dim + staticmethod jvp ctx x_tangent _ ind ind_inv = ctx saved_tensors NumpyTake apply x_tangent ind ind_inv ctx dim None None SortGenVmap torch autograd Function generate_vmap_rule = True staticmethod forward x dim ind = torch argsort x dim=dim ind_inv = torch argsort ind axis=dim result = torch take_along_dim x ind dim=dim result ind ind_inv staticmethod setup_context ctx inputs outputs x dim = inputs _ ind ind_inv = outputs ctx mark_non_differentiable ind ind_inv ctx save_for_backward ind ind_inv ctx save_for_forward ind ind_inv ctx dim = dim staticmethod backward ctx grad_output _ _ ind ind_inv = ctx saved_tensors TakeGenVmap apply grad_output ind_inv ind ctx dim None staticmethod jvp ctx x_tangent _ ind ind_inv = ctx saved_tensors TakeGenVmap apply x_tangent ind ind_inv ctx dim None None sample_inputs_numpy_sort opinfo device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg args= sample_inputs_numpy_take opinfo device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad tensor = make_arg dim = _ ind ind_inv = NumpySort apply tensor yield SampleInput tensor args= ind ind_inv dim NumpyTake torch autograd Function staticmethod forward x ind ind_inv dim device = x device x = to_numpy x ind = to_numpy ind torch tensor np take_along_axis x ind dim device=device staticmethod setup_context ctx inputs output _x ind ind_inv dim = inputs ctx save_for_backward ind ind_inv ctx save_for_forward ind ind_inv ctx dim = dim staticmethod backward ctx grad_output ind ind_inv = ctx saved_tensors result = NumpyTake apply grad_output ind_inv ind ctx dim result None None None staticmethod vmap info in_dims x ind ind_inv dim x_bdim ind_bdim ind_inv_bdim _ = in_dims wrap dim logical_dim = x dim x_bdim None x_bdim - dim = dim dim = dim + logical_dim expand_bdim x x_bdim x_bdim None x expand info batch_size x shape x movedim x_bdim x = expand_bdim x x_bdim ind = expand_bdim ind ind_bdim ind_inv = expand_bdim ind_inv ind_inv_bdim NumpyTake apply x ind ind_inv dim + staticmethod jvp ctx x_tangent ind_tangent ind_inv_tangent _ assert ind_tangent None assert ind_inv_tangent None ind ind_inv = ctx saved_tensors NumpyTake apply x_tangent ind ind_inv ctx dim TakeGenVmap torch autograd Function generate_vmap_rule = True staticmethod forward x ind ind_inv dim torch take_along_dim x ind dim staticmethod setup_context ctx inputs outputs _x ind ind_inv dim = inputs ctx save_for_backward ind ind_inv ctx save_for_forward ind ind_inv ctx dim = dim staticmethod backward ctx grad_output ind ind_inv = ctx saved_tensors result = TakeGenVmap apply grad_output ind_inv ind ctx dim result None None None staticmethod jvp ctx x_tangent ind_tangent ind_inv_tangent _ ind ind_inv = ctx saved_tensors TakeGenVmap apply x_tangent ind ind_inv ctx dim Select torch autograd Function staticmethod forward x idx x idx staticmethod setup_context ctx inputs output x idx = inputs ctx x_shape = x shape ctx idx = idx staticmethod backward ctx grad_output result = grad_output new_zeros ctx x_shape result ctx idx = grad_output result None staticmethod vmap info in_dims x idx x_bdim _ = in_dims x = x movedim x_bdim Select apply x idx staticmethod jvp ctx x_tangent _ Select apply x_tangent ctx idx SelectGenVmap torch autograd Function generate_vmap_rule = True staticmethod forward x idx x idx staticmethod setup_context ctx inputs outputs x idx = inputs ctx x_shape = x shape ctx idx = idx staticmethod backward ctx grad_output result = grad_output new_zeros ctx x_shape result ctx idx = grad_output result None staticmethod jvp ctx x_tangent _ SelectGenVmap apply x_tangent ctx idx sample_inputs_select opinfo device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg args= ScaleGradGenVmap torch autograd Function generate_vmap_rule = True scale = staticmethod forward x x clone staticmethod setup_context ctx inputs outputs pass staticmethod backward ctx grad_output grad_output ScaleGradGenVmap scale staticmethod jvp ctx x_tangent x_tangent ScaleGradGenVmap scale ZeroGradientsGenVmap torch autograd Function generate_vmap_rule = True staticmethod forward x y x clone y clone staticmethod setup_context ctx inputs outputs pass staticmethod backward ctx gx gy Intentionally returning torch zeros instead zeros_like new_zeros Also intentionally None Intentionally too-large gradient torch zeros gx shape dtype=gx dtype device=gx device torch zeros gy shape dtype=gy dtype device=gy device staticmethod jvp ctx gx gy Intentionally returning torch zeros instead zeros_like new_zeros Also intentionally None torch zeros gx shape dtype=gx dtype device=gx device torch zeros gy shape dtype=gy dtype device=gy device sample_inputs_forward_default_args opinfo device dtype requires_grad kwargs make_arg = partial make_tensor device=device dtype=dtype requires_grad=requires_grad yield SampleInput make_arg ForwardHasDefaultArgs torch autograd Function staticmethod forward x idx= x idx staticmethod setup_context ctx inputs output x idx = inputs ctx x_shape = x shape ctx idx = idx staticmethod backward ctx grad_output result = grad_output new_zeros ctx x_shape result ctx idx = grad_output result None staticmethod vmap info in_dims x idx x_bdim _ = in_dims x = x movedim x_bdim ForwardHasDefaultArgs apply x idx staticmethod jvp ctx x_tangent _ ForwardHasDefaultArgs apply x_tangent ctx idx autograd_function_db = OpInfo NumpyCubeAutogradFunction op=NumpyCube apply supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_numpy_cube dtypes=all_types_and torch bool torch half supports_out=False OpInfo NumpyExpMarkDirtyAutogradFunction op=lambda x NumpyExp_ apply x clone inplace_variant=NumpyExp_ apply supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_numpy_cube dtypes=all_types_and torch bool torch half supports_out=False OpInfo NumpyMulAutogradFunction op=NumpyMul apply supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_numpy_mul dtypes=all_types_and torch bool torch half supports_out=False OpInfo NumpyCubeNotComposableAutogradFunction op=lambda x NumpyCubeNotComposable apply x supports_forward_ad=False supports_fwgrad_bwgrad=False sample_inputs_func=sample_inputs_numpy_cube dtypes=all_types_and torch bool torch half supports_out=False OpInfo NumpySortAutogradFunction op=NumpySort apply supports_forward_ad=False supports_fwgrad_bwgrad=False sample_inputs_func=sample_inputs_numpy_sort dtypes=all_types_and torch bool torch half supports_out=False gradcheck_wrapper=lambda y ind y OpInfo NumpyTakeAutogradFunction op=NumpyTake apply supports_forward_ad=False supports_fwgrad_bwgrad=False sample_inputs_func=sample_inputs_numpy_take dtypes=all_types_and torch bool torch half supports_out=False OpInfo SelectAutogradFunction op=Select apply supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_select dtypes=all_types_and torch bool torch half supports_out=False OpInfo CubeGenVmapAutogradFunction op=CubeGenVmap apply supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_numpy_cube dtypes=all_types_and torch bool torch half supports_out=False OpInfo MulGenVmapAutogradFunction op=MulGenVmap apply supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_numpy_mul dtypes=all_types_and torch bool torch half supports_out=False OpInfo SortGenVmapAutogradFunction op=SortGenVmap apply supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_numpy_sort dtypes=all_types_and torch bool torch half supports_out=False gradcheck_wrapper=lambda y ind y OpInfo SelectGenVmapAutogradFunction op=SelectGenVmap apply supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_select dtypes=all_types_and torch bool torch half supports_out=False OpInfo ScaleGradGenVmapAutogradFunction op=ScaleGradGenVmap apply supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_numpy_cube dtypes=all_types_and torch bool torch half supports_out=False OpInfo ZeroGradientsGenVmapAutogradFunction op=ZeroGradientsGenVmap apply supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_numpy_mul dtypes=all_types_and torch bool torch half supports_out=False OpInfo ForwardHasDefaultArgsAutogradFunction op=ForwardHasDefaultArgs apply supports_forward_ad=True supports_fwgrad_bwgrad=True sample_inputs_func=sample_inputs_forward_default_args dtypes=all_types_and torch bool torch half supports_out=False