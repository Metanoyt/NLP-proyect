typing Any Optional numpy np sklearn tree _tree type ignore import-untyped DecisionTreeNode __init__ feature Optional str = None threshold Optional float = None left Optional DecisionTreeNode = None right Optional DecisionTreeNode = None class_probs Any = None num_samples int = node_id int = - None feature = feature threshold = threshold left = left right = right class_probs = class_probs num_samples = num_samples id = node_id is_leaf - bool left None right None DecisionTree Custom decision tree implementation mimics some sklearn API The purpose able perform transformations such custom pruning which does seem easy sklearn __init__ sklearn_tree Any feature_names list str - None feature_names = feature_names root = _convert_sklearn_tree sklearn_tree tree_ classes_ list str = sklearn_tree classes_ _convert_sklearn_tree sklearn_tree Any node_id int = - DecisionTreeNode class_probs = sklearn_tree value node_id num_samples = sklearn_tree n_node_samples node_id sklearn_tree feature node_id = _tree TREE_UNDEFINED feature_index = sklearn_tree feature node_id feature = feature_names feature_index left = _convert_sklearn_tree sklearn_tree sklearn_tree children_left node_id right = _convert_sklearn_tree sklearn_tree sklearn_tree children_right node_id DecisionTreeNode feature=feature threshold=sklearn_tree threshold node_id left=left right=right class_probs=class_probs num_samples=num_samples node_id=node_id DecisionTreeNode class_probs=class_probs num_samples=num_samples node_id=node_id prune df Any target_col str k int - None root = _prune_tree root df target_col k _prune_tree node DecisionTreeNode df Any target_col str k int - DecisionTreeNode node is_leaf node left_df = df df node feature = node threshold right_df = df df node feature node threshold number unique classes left right subtrees left_counts = left_df target_col nunique right_counts = right_df target_col nunique ranking we want ensure we least k classes so we have less than k classes left right subtree we remove split make node leaf node left_counts k right_counts k DecisionTreeNode class_probs=node class_probs assert node left None expected left child exist node left = _prune_tree node left left_df target_col k assert node right None expected right child exist node right = _prune_tree node right right_df target_col k node to_dot - str dot = digraph DecisionTree \n dot += node fontname= helvetica \n dot += edge fontname= helvetica \n dot += _node_to_dot root dot += dot _node_to_dot node DecisionTreeNode parent_id int = edge_label str = - str node None node_id = id node Format class_probs array line breaks class_probs_str = _format_class_probs_array node class_probs node num_samples node is_leaf label = class_probs_str shape = box feature_name = f node feature label = f feature_name = node threshold f \\n class_probs_str shape = oval dot = f node_id label= label shape= shape \n parent_id = dot += f parent_id - node_id label= edge_label \n node is_leaf assert node left None expected left child exist dot += _node_to_dot node left node_id = assert node right None expected right child exist dot += _node_to_dot node right node_id dot _format_class_prob num float - str num == f num f _format_class_probs_array class_probs Any num_samples int max_per_line int = - str add line breaks avoid very long lines flat_class_probs = class_probs flatten formatted = _format_class_prob v v flat_class_probs lines = formatted i i + max_per_line i range len formatted max_per_line f num_samples= num_samples \\n + \\n join join line line lines predict X Any - Any predictions = _predict_single x _ x X iterrows np array predictions predict_proba X Any - Any np array _predict_proba_single x _ x X iterrows _get_leaf X Any - DecisionTreeNode node = root while node is_leaf X node feature = node threshold assert node left None expected left child exist node = node left assert node right None expected right child exist node = node right node _predict_single x Any - str node = _get_leaf x map index name classes_ np argmax node class_probs _predict_proba_single x Any - Any node = _get_leaf x node class_probs apply X Any - Any ids = _apply_single x _ x X iterrows np array ids _apply_single x Any - int node = _get_leaf x node id codegen dummy_col_ _col_val dict str tuple str Any lines list str unsafe_leaves list int - None generates python code decision tree codegen_node node DecisionTreeNode depth int - None indent = depth + node is_leaf lines append handle_leaf node indent unsafe_leaves name = node feature threshold = node threshold name dummy_col_ _col_val orig_name value = dummy_col_ _col_val name predicate = f indent str context get_value orig_name = value assert threshold == f expected threshold threshold predicate = f indent context get_value name = threshold lines append predicate assert node left None expected left child exist codegen_node node left depth + lines append f indent assert node right None expected right child exist codegen_node node right depth + handle_leaf node DecisionTreeNode indent str unsafe_leaves list int - str This generates code leaf node decision tree If leaf unsafe learned heuristic will unsure i e None node id unsafe_leaves f indent None class_probas = node class_probs f indent best_probas_and_indices class_probas best_probas_and_indices class_probas Any - str Given list tuples proba idx function returns string which tuples sorted proba descending order E g Given class_probas= function returns we generate list tuples proba idx sorted proba descending order idx index choice we only generate tuple proba probas_indices_sorted = sorted proba index index proba enumerate class_probas proba key=lambda x x reverse=True probas_indices_sorted_str = join f value f index value index probas_indices_sorted f probas_indices_sorted_str codegen_node root