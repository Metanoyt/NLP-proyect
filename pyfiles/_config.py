os sys typing Optional compile_ignored debug Fails hard instead graph breaking guard data dependent errors no_data_dependent_graph_break = os environ get TORCHDYNAMO_NO_DATA_DEPENDENT_GRAPH_BREAK == compile_ignored debug Uses z validating guard optimizations transformations translation_validation = os environ get TORCHDYNAMO_TRANSLATION_VALIDATION == Timeout milliseconds z finding solution compile_ignored debug translation_validation_timeout = int os environ get TORCHDYNAMO_TRANSLATION_VALIDATION_TIMEOUT Disables bisection translation validation Translation validation bisection enabled default translation validation also enabled This should help finding guard simplification issues However since validation uses Z bisecting might take lot time Set configuration option so avoid bisecting compile_ignored debug translation_validation_no_bisect = os environ get TORCHDYNAMO_TRANSLATION_NO_BISECT == Checks whether replaying ShapeEnv events freshly constructed one yields ShapeEnv same state This should used only testing check_shape_env_recorded_events = False TODO Perhaps consider allowing unions configs below so you can hit multiple reps same time Give extended debug information string representation guard matches For example set Ne s whenever we issue guard we will generate full Python C++ backtrace compile_ignored debug extended_debug_guard_added = os environ get TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED None Give extended debug information when particular symbol allocated For example set u whenever we create symbol we will generate full Python C++ backtrace compile_ignored debug extended_debug_create_symbol = os environ get TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL None Give extended debug information C++ backtrace all extended debug settings well errors The C++ backtrace slow very spammy so we don t include default even when you re requesting extended debug compile_ignored debug extended_debug_cpp = os environ get TORCHDYNAMO_EXTENDED_DEBUG_CPP = Give extended debug information line code when torch function called during export This useful showing progress detecting where export might stuck Currently only works strict=False compile_ignored debug extended_debug_current_loc = os environ get TORCHEXPORT_EXTENDED_DEBUG_CURRENT_LOC == compile_ignored debug Show warning every specialization print_specializations = False wraps un equalities Not after recording correct expression FX graph This should incorrectly construct divisible replacement lists incorrectly issue guards inject_EVALUATE_EXPR_flip_equality_TESTING_ONLY = False compile_ignored debug Validate ShapeEnv s version key updated correctly validate_shape_env_version_key = False If we produce more than many guards symbol force symbol get specialized bail out many guards mention particular symbol This may slightly more aggressive than true number guards issued we test we ve hit limit on-the-fly whereas we may do further simplifications final guard issuance time make guards irrelevant symbol_guard_limit_before_specialize Optional int = None This flag changes whether we should use same symbolic variable represent input sizes same use_duck_shape = True Controls registration torch nonzero meta device When True nonzero returns tensor shape numel dim assuming all elements none-zero Default False prevent unintended registration Set True enable meta_nonzero_assume_all_nonzero = False Applies size-oblivious reasoning backed symbols This allocates inf range backed size symbols relies size-oblivious semantics avoid specialization guards marking them size-like Currently experimental option export backed_size_oblivious = False Skip dtype check meta registrations Only used systems does its own dtype checking skip_dtype_check_in_meta_registrations = False torch utils _config_module install_config_module install_config_module sys modules __name__