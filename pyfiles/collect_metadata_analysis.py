mypy allow-untyped-defs This module one analysis modules - takes input function graph some preexisting properties returns some data useful deciding how further proceed compilation construct runtime wrappers In particular analysis here constructs view mutation metadata running functionalized version graph under compilation collections contextlib logging collections abc Callable typing Optional torch torch utils _pytree pytree torch Tensor torch _guards detect_fake_mode torch _logging getArtifactLogger torch _subclasses functional_tensor FunctionalTensor FunctionalTensorMode torch _subclasses meta_utils safe_is_leaf torch fx experimental symbolic_shapes is_concrete_int torch multiprocessing reductions StorageWeakRef torch utils _python_dispatch is_traceable_wrapper_subclass transform_subclass descriptors AOTInput AOTOutput InputMutationAOTOutput IntermediateBaseAOTOutput PlainAOTOutput TangentAOTInput functional_utils are_all_mutations_hidden_from_autograd are_all_mutations_under_no_grad_or_inference_mode from_fun has_data_mutation has_metadata_mutation MetadataKey to_fun ViewMetaSequence was_inductor_storage_resized schemas InputAliasInfo MemoryFormatMeta MutationType OutputAliasInfo OutputType ViewAndMutationMeta subclass_utils create_subclass_meta utils _get_autocast_states KNOWN_TYPES simple_wraps strict_zip zip = strict_zip log = logging getLogger __name__ static_input_logger = getArtifactLogger torch _dynamo cudagraph_static_inputs Note Tangents memory format We assume tangents memory format similar corresponding output s memory_format The idea we technically making guess about strides our tangents while we trace out joint If runtime specified tangents will have same memory format predicted traced tangents we coerce them runtime traced tangents memory format Coercing collecting traced tangents memory format one recursive traversal mypy ignore-errors coerce_tangent_and_suggest_memory_format x Tensor updated = False isinstance x Tensor x None updated out = x detach is_subclass = is_traceable_wrapper_subclass out memory_format = MemoryFormatMeta from_tensor out pyrefly ignore missing-attribute memory_format memory_format None = out pyrefly ignore bad-argument-type out = out contiguous memory_format=memory_format memory_format updated = out For subclass we keep memory format outer strides beginning list out_memory_format = memory_format is_subclass memory_format Note Tangents memory format Part In same way what strides do we assigns our tangents question we can answer therefore have guess we trace backward ahead-of-time The same applies any tensor subclass metadata when we have tangents subclasses To handle situation we have two new methods tensor subclass can implement __coerce_tangent_metadata__ Given subclass non-standard metadata turn into new subclass normal metadata The main example here DTensor _Partial placement If we have forward output _Partial placement corresponding tangent Replicate Shard placement we have no way convert tangent back _Partial placement This method lets us avoid problem entirely allowing subclasses ensure we can never have tangent problematic metadata we cannot convert __coerce_same_metadata_as_tangent__ metadata Given subclass target differing metadata convert have same metadata target With DTensor being main example we can use convert DTensor Replicate placement into one Shard placement case we guessed wrong traced tangents Shard placement compile time is_subclass hasattr out __coerce_tangent_metadata__ out = out __coerce_tangent_metadata__ type ignore attr-defined is_subclass pyrefly ignore missing-attribute attrs = out __tensor_flatten__ attr attrs elem = getattr out attr new_elem new_elem_memory_format elem_updated = coerce_tangent_and_suggest_memory_format elem pyrefly ignore missing-attribute out_memory_format append new_elem_memory_format elem_updated setattr out attr new_elem out out_memory_format updated This version functionalization specifically designed AOTAutograd use case Unlike functorch s variant doesn t use functorch level system instead directly uses PyTorch s conventional dispatcher hit functionalization key In particular means FunctionalTensorWrapper can have autograd data stored directly In typical AOTAutograd usage dispatch key order will look like Autograd - Functionalization ~~~~ Proxy Mode - Fake Tensor outer tensor inner tensor Returns - ViewAndMutationMeta telling us metadata about inputs outputs The list outputs forward only outputs we need pass tangents into backward Specifically aliased outputs forward get regenerated don t participate compiled backward function run_functionalized_fw_and_collect_metadata f flat_args_descs list AOTInput keep_input_mutations bool TODO refactor kill flag is_train bool = False Note guaranteed set when running under dynamo static_input_indices Optional list int = None pre_dispatch bool = False - Callable ViewAndMutationMeta memo dict Tensor Tensor = _to_fun t isinstance t Tensor t memo memo t r = to_fun t memo t = r r t simple_wraps f inner flat_args This function meant run forward which expects flat list tensor symint other args assert all isinstance tuple KNOWN_TYPES flat_args input_info list InputAliasInfo = output_info list OutputAliasInfo = prior_grad_enabled = torch is_grad_enabled prior_autocast_states = _get_autocast_states See Note Disabling Functionalize TLS Above Python Functionalization disable_above = torch _C _ExcludeDispatchKeyGuard torch _C DispatchKeySet torch _C DispatchKey Functionalize It doesn t matter we run under predispatch because only figuring out metadata mode = FunctionalTensorMode _allow_token_discovery=True suppress_pending = contextlib nullcontext fake_mode = detect_fake_mode fake_mode shape_env = fake_mode shape_env suppress_pending = shape_env ignore_fresh_unbacked_symbols disable_above mode suppress_pending precondition The passed function already handles unflattening inputs + flattening outputs flat_f_args = pytree tree_map _to_fun flat_args flat_f_args_descs = flat_args_descs flat_f_outs = f flat_f_args Assert f does NOT have AOTOutputs easy mistake make You need drop second output before calling function assert pytree tree_any lambda x isinstance x AOTOutput flat_f_outs f f returned AOTOutput when shouldn t Did you remember wrap function without_output_descs before passing here NB just setup input descriptors we will recreate these descriptors same convention when we actually do trace flat_f_outs_descs = PlainAOTOutput i i range len flat_f_outs We didn t do any tracing so we don t need process unbacked symbols they will just disappear into ether Also prevent memoization applying fake_mode fake_mode epoch += fake_mode reset_nt_tensor_id_counter prior_autocast_states = _get_autocast_states raise RuntimeError AOTAutograd does support tracing graphs mutate autocast state Dynamo will only insert autocast context managers e g torch autocast into graph which will unwind all their mutations autocast state before graph exits If you encounter error while using torch compile please file bug Inspect state input tensor functional wrapper detect input mutation info If inp i has metadata-only mutation then maybe_inputs_with_mutated_metadata i contains updated version arg f_arg zip flat_args flat_f_args NB Mutation non-contiguous tensor subclass input can result mismatch strides between functionalized arg inner tensors non-functionalized arg inner tensors This problem inner tensor stride change may reflected correctly outer tensor so disallow now mutates_data = has_data_mutation f_arg mutates_metadata = has_metadata_mutation f_arg arg check_only_storage_mutation=False mutates_metadata is_traceable_wrapper_subclass arg raise RuntimeError Metadata mutations currently allowed tensor subclasses mutates_storage_metadata = has_metadata_mutation f_arg arg check_only_storage_mutation=True mutations_hidden_from_autograd = are_all_mutations_hidden_from_autograd f_arg mutations_under_no_grad_or_inference_mode = mutates_data are_all_mutations_under_no_grad_or_inference_mode f_arg mutation_inductor_storage_resize = was_inductor_storage_resized f_arg mutates_storage_metadata mutates_data = False requires_grad = isinstance f_arg torch Tensor f_arg requires_grad input_info append InputAliasInfo is_leaf=isinstance arg Tensor safe_is_leaf arg mutates_data=mutates_data mutates_metadata=mutates_metadata mutations_hidden_from_autograd=mutations_hidden_from_autograd mutates_storage_metadata=mutates_storage_metadata mutations_under_no_grad_or_inference_mode=mutations_under_no_grad_or_inference_mode mutation_inductor_storage_resize=mutation_inductor_storage_resize requires_grad=requires_grad keep_input_mutations=keep_input_mutations If function involves creating tensor returning view such its _base intermediate We need make sure our graph returns _base graph output we manually recreate view user Why The backend compiler free incorrectly set requires_grad base tensor we obligated properly set requires-gradness real output inp_storage_refs = StorageWeakRef inpt untyped_storage idx idx inpt enumerate flat_f_args isinstance inpt Tensor We need inp tensor id s able tell outputs inputs inp_tensor_ids = id inpt inpt flat_f_args isinstance inpt Tensor We need output tensor id s tell any output _base ` attributes other outputs This also dict because we need know output s index so we can regenerate alias out_tensor_ids = id o i i o enumerate flat_f_outs Keep track which outputs alias other outputs out_tensor_alias_counts collections defaultdict = collections defaultdict int This tells us given group outputs alias each other whether they e g all came unbind call num_aliased_tensors_that_are_multi_output_views collections defaultdict = collections defaultdict int out_storage_to_metadata_key_to_tensors collections defaultdict Optional StorageWeakRef collections defaultdict MetadataKey set torch Tensor = collections defaultdict lambda collections defaultdict set curr_storage = None o flat_f_outs isinstance o torch Tensor curr_storage = StorageWeakRef o untyped_storage out_tensor_alias_counts curr_storage += Note AOTAutograd differentiable outputs alias each other multi-output view call This optimization top alias intermediates logic which you can read more about under Note AOT Autograd outputs aliasing inputs intermediates Before describing optimization important AOTAutograd have good perf around multi-output views HOWEVER - There more generic change AOTAutograd we d like make subsumes case around using pre-dispatch tracing partition out graph so we can faithfully replay all views without having regenerate them runtime - It s loosely described doc more details will added soon https docs google com document d DlfFq TKbuAn zyJxLfoW-X qkkm PLdHFtySo QAk edit - Once change lands we should just rip out optimization since It will fully unnecessary Although only few lines code bit difficult reason about its correctness autograd engine all cases What optimization Consider below case f x intermediate = x mul x intermediate here require grad o o o = intermediate unbind - intermediate o o o Now intermediate base handling AOTAutograd implies we must do following intermediate extra output compiled graph regenerate each aliased output off intermediate outside autograd Function The reason AOTAutograd ordinarily does safety autograd engine needs know o through o all aliased we blindly o through o autograd Function information will hidden In particular mutating one alias might require autograd update autograd metadata other aliases like their grad_fn example when autograd engine needs do view-replay However intermediate_base logic can bad backward performance we sometimes generate as_strided calls during intermediate base logic which can have slow backward formula Is possible find set conditions where safe hide output aliasing autograd For set outputs graph alias each other o_ o_k consider They came same multi-output view op e g o_ o_k = intermediate unbind If there any other aliases o_ through o_k example above intermediate most can escape graph e g there some other graph input output o_other aliases these outputs o_ o_k all require_grad they all share same _base their _base requires grad This condition important because s what causes slowness intermediate_base codepath aot_autograd Ordinarily o_ o_k would all get grad_fn aot_autograd s view-replay might give each output AsStridedBackward its grad_fn K AsStridedBackward calls will much slower than single UnbindBackward In setup possible mutate one outputs o_i way would affect autograd meta other aliases Claim No Consider few example which I m pretty sure cover all cases mutation w r t autograd What happens we mutate any o_ through o_k directly Autograd raises error RuntimeError Output UnbindBackward view being modified inplace This view output function returns multiple views Such functions do allow output views modified inplace You should replace inplace operation out-of-place one b What we take view o_k mutate o_k view o_k shape mul_ Autograd raises same error- multi-output-view ness alias propagates future views c What we mutate o_k under no_grad Autograd raises same error d What we detach mutate e g o_k detach mul_ Autograd allows autograd updates all alias s grad_fn s error functions when accessed Autograd raises same error e What we try mutate another alias o_ o_k created multi-output view We promised there most one such alias e g intermediate example above You can mutate intermediate eager mode will change grad_fn o_ o_k error fn s Since intermediate only non-multi-output-alias there no other aliases ` intermediate ` around produced compiled fn have valid grad_fn Coming back optimization Given possible mutating one these aliases affect autograd metadata another alias without causing error eager mode we will simple hide aliasing autograd during torch compile all above conditions met This has slight downside s possible write some bad code autograd will raise error eager fail during torch compile has benefit code has much better performance NOTE when we eventually update AOTAutograd do view graph slicing defined here https docs google com document d DlfFq TKbuAn zyJxLfoW-X qkkm PLdHFtySo QAk edit then optimization will probably matter less might ok remove is_cur_tensor_multi_out_view = isinstance o FunctionalTensor torch _functionalize_is_multi_output_view type ignore attr-defined o elem is_cur_tensor_multi_out_view num_aliased_tensors_that_are_multi_output_views curr_storage += o requires_grad out_storage_to_metadata_key_to_tensors curr_storage MetadataKey make o add o maps id intermediate base its index output compiled forward intermediate_base_tensor_id_to_output_idx dict int int = intermediate_bases list torch Tensor = intermediate_bases_descs list AOTInput = Why Do We Care If Storage Changed It s important understand implications storage changes complex scenarios Take example f x x_storage = x untyped_storage non_leaf_tensor = torch ones requires_grad=True clone Using no_grad _unsafe_preserve_version_counter simulate data = operation torch no_grad torch autograd _unsafe_preserve_version_counter x x set_ non_leaf_tensor untyped_storage out = x view - Restoring x its original storage again simulating data = operation torch no_grad torch autograd _unsafe_preserve_version_counter x x set_ x_storage out In scenario x out have different shapes stored different memory addresses aka no aliasing However due how set_ more specificlaly set functionalized defined preserve eager semantics autograd engine mistakenly assumes x out aliased treating x out _base This misinterpretation leads alias_of_input flag causing unnecessary as_strided call generated which could lead issues later code o desc zip flat_f_outs flat_f_outs_descs functional_tensor_storage_changed = isinstance o FunctionalTensor torch _functionalize_was_storage_changed type ignore attr-defined o elem curr_storage = None isinstance o torch Tensor StorageWeakRef o untyped_storage outs_with_identical_metadata_that_require_grad = isinstance o Tensor curr curr out_storage_to_metadata_key_to_tensors curr_storage MetadataKey make o o curr See Note Accessing grad_fn FunctionalTensor In-place operations views will trigger lazy rebase autograd graph runs during access grad_fn The rebase logic will invoke view ops FunctionalTensors so we must enable FunctionalTensorMode here ensure these op calls succeed grad_fn = None isinstance o Tensor FunctionalTensorMode grad_fn = o grad_fn is_result_of_custom_autograd_fn = False Need check both custom cpp CppFunction python BackwardCFunction autograd fns type grad_fn __name__ == CppFunction is_result_of_custom_autograd_fn = True isinstance grad_fn torch autograd function BackwardCFunction is_result_of_custom_autograd_fn = True isinstance o Tensor output_type = OutputType non_alias base_idx = None curr_storage inp_storage_refs grad_fn None is_result_of_custom_autograd_fn output_type = OutputType custom_function_view base_idx = None curr_storage inp_storage_refs functional_tensor_storage_changed pyrefly ignore index-error base_idx = inp_storage_refs curr_storage is_input_tensor = id o inp_tensor_ids num_aliased_outs = out_tensor_alias_counts curr_storage num_multi_output_view_outs = num_aliased_tensors_that_are_multi_output_views curr_storage num_aliased_outs_that_are_not_multi_output_views = num_aliased_outs - num_multi_output_view_outs grad_fn None num_aliased_outs_that_are_not_multi_output_views == See Note AOTAutograd differentiable outputs alias each other multi-output view call In particular given f x list x unbind The main reason we ordinarily try regenerate these output aliases outside compiled autograd Function because any outputs later mutated autograd needs perform view-replay regenerate them However autograd does allow users mutate multi-output views any way can change autograd metadata other aliases So we hide aliasing autograd here log debug Encountered AOTAutograd case differentiable outputs \ alias each other multi-output view call output_type = OutputType non_alias is_input_tensor output_type = OutputType is_input output_type = OutputType alias_of_input functional_tensor_storage_changed id o inp_tensor_ids When there set_ input we cannot rely checking storages detect we returning input since inputs storage different assert curr_storage None base_idx = inp_storage_refs curr_storage output_type = OutputType is_input We only need handle intermediate base case when both intermediate base output require gradients See Note AOT Autograd outputs aliasing inputs intermediates o _base None o requires_grad o _base requires_grad num_aliased_outs = out_tensor_alias_counts curr_storage num_multi_output_view_outs = num_aliased_tensors_that_are_multi_output_views curr_storage num_aliased_outs_that_are_not_multi_output_views = num_aliased_outs - num_multi_output_view_outs Note AOTAutograd differentiable outputs alias each other multi-output view call out_tensor_alias_counts curr_storage == num_aliased_outs_that_are_not_multi_output_views = Note Intermediate Bases Optimization Normally we have output aliases intermediate we need add extra intermediate base logic further down prevent autograd yelling us user later tries mutate output However common case here we have output aliases intermediate doesn t alias any other outputs In case autograd shouldn t have worry about aliasing all output mutated there no other live aliases autograd worry about The intermediate bases can hurt inductor perf forcing more variables become outputs So optimization we won t do intermediate base handling case Instead we ll hide aliasing autograd using aten _unsafe_view out_tensor_alias_counts curr_storage = num_aliased_outs_that_are_not_multi_output_views = log debug Encountered AOTAutograd case differentiable outputs alias each other \ multi-output view call output_type = OutputType unsafe_view_alias base_idx = None First check o s _base existing output maybe_existing_out_idx = out_tensor_ids get id o _base None maybe_existing_out_idx None Special case where output alias graph intermediate intermediate itself also user output output_type = OutputType alias_of_intermediate_base_is_user_output base_idx = maybe_existing_out_idx Next check o s _base intermediate base we already returned maybe_existing_base_output_idx = intermediate_base_tensor_id_to_output_idx get id o _base None maybe_existing_base_output_idx None output_type = OutputType alias_of_intermediate base_idx = maybe_existing_base_output_idx Otherwise take o _base explicitly output compiled graph new_out_idx = len intermediate_bases base_idx = new_out_idx Indicate logic later when we trace joint particular output should get s _base appended forward graph outputs output_type = OutputType alias_of_intermediate_save_as_output intermediate_base_tensor_id_to_output_idx id o _base = new_out_idx intermediate_bases append o _base NB The desc we picked here guaranteed synchronized one graph_capture_wrappers py because we SPECIFICALLY notated output alias_of_intermediate_save_as_output intermediate_bases_descs append TangentAOTInput IntermediateBaseAOTOutput desc See https github com pytorch pytorch issues case This protects against specific case where user fn returns output output detach out_tensor_alias_counts curr_storage len outs_with_identical_metadata_that_require_grad o requires_grad In theory we could use any these tensors regenerate aliased outputs since they all alias each other have identical metadata out_alias = outs_with_identical_metadata_that_require_grad existing_out_idx = out_tensor_ids id out_alias output_type = OutputType alias_of_intermediate_base_is_user_output base_idx = existing_out_idx output_type = OutputType non_alias base_idx = None isinstance o torch Tensor dynamic_dims = i i s enumerate o shape is_concrete_int s dynamic_dims = None Save current FunctionalTensor output This will used runtime reconstructing output views their respective base tensors The FunctionalTensor will saved one conditions below true view_meta_sequence = None If output_type either i alias_of_intermediate ii alias_of_intermediate_save_as_output iii alias_of_intermediate_base_is_user_output No need worry about in-place view operations here since functionalization step elimitates mutations i e we have access actual base tensor before in-place operation applied output_type OutputType alias_of_intermediate OutputType alias_of_intermediate_save_as_output OutputType alias_of_intermediate_base_is_user_output If output_type alias_of_input no in-place view operationthe run input base tensor In case we need check metadata mutation because runtime explicitly reconstructs inputs before actually reconstructing outputs Due in-place view operations fully reconstructed input may output base tensor anymore output_type == OutputType alias_of_input base_idx None input_info base_idx mutates_metadata isinstance o FunctionalTensor view_meta_sequence = ViewMetaSequence o out_info = OutputAliasInfo output_type=output_type raw_type=type o base_idx=base_idx dynamic_dims=dynamic_dims requires_grad=isinstance o torch Tensor o requires_grad view_meta_sequence=view_meta_sequence output_info append out_info See Note AOT Autograd Views avoid tangents aliasing inputs view_avoid_dupes_with_primals t isinstance t Tensor is_traceable_wrapper_subclass t transform_subclass t lambda _ inner_t view_avoid_dupes_with_primals inner_t isinstance t Tensor t view t shape t This analysis function returns only outputs meant tangents backwards Anything aliases inputs returned fw due metadata mutations outputs alias inputs intermediates regenerated later used directly autograd graph _plain_fake_tensor_like_subclass x pyrefly ignore bad-context-manager detect_fake_mode torch empty x shape dtype=x dtype device=x device layout=x layout _is_subclass_mutated_input_tangent_always_subclass inp isinstance inp torch nested _internal nested_tensor NestedTensor torch _functorch config disable_guess_zero_tangent_for_mutated_input_subclass f_input_tangents_pairs = Note AOTAutograd Tangent Subclassness mutated inputs Generally when creating tangents trace we assume tangents will have same subclass-ness their forward outs however tangents correspond input mutations practice more likely these tangents will plain tensors zeros runtime so we tweak our guess assume these tangents should always plaint tensors Example f x x mul_ x + out = f x out sum backward In above code we will have tangent x_updated_tangent which will plain tensor zeros unless x used some compute after executing f However there exceptions logic If view created mutated input used backward The tangent subclass input will subclass tensor Example f b mul_ b mul_ b view b shape + b a_out b_out = f Subclass b sum backward We can deduce easily now so introducing debug config able turn off specific cases NJT guarantees have its tangent NJT because has dedicated integration Autograd See torch csrc autograd python_function cpp use_zeros_like _plain_fake_tensor_like_subclass inp is_traceable_wrapper_subclass inp _is_subclass_mutated_input_tangent_always_subclass inp inp TangentAOTInput InputMutationAOTOutput inp_desc inp inp_desc info zip flat_f_args flat_f_args_descs input_info info mutation_type == MutationType MUTATED_OUT_GRAPH info mutates_data info requires_grad f_input_tangents f_input_tangents_descs = x x f_input_tangents_pairs x x f_input_tangents_pairs f_output_tangents_pairs = o TangentAOTInput desc o info desc zip flat_f_outs output_info flat_f_outs_descs info output_type OutputType non_alias OutputType unsafe_view_alias OutputType custom_function_view issubclass info raw_type torch Tensor info requires_grad f_output_tangents f_output_tangents_descs = x x f_output_tangents_pairs x x f_output_tangents_pairs intermediate bases also included backward graph f_tangents = f_input_tangents + f_output_tangents + intermediate_bases f_tangents_descs = f_input_tangents_descs + f_output_tangents_descs + intermediate_bases_descs TODO I m pretty sure you don t need tree_map here traced_tangents = pytree tree_map from_fun f_tangents traced_tangents = pytree tree_map view_avoid_dupes_with_primals traced_tangents traced_tangents = coerce_tangent_and_suggest_memory_format tt i tt enumerate traced_tangents NB update maps above ever change structure Also might helpful add coercion information tangent desc traced_tangents_descs = f_tangents_descs nonlocal static_input_indices static_input_indices = static_input_indices torch _dynamo compiled_autograd in_compiled_autograd_region passed_indices = set static_input_indices static_input_indices = i i arg enumerate flat_args isinstance arg torch nn Parameter i passed_indices static_input_logger debug static input indices metadata analysis s static_input_indices f_mutated_inputs = inp inp info zip flat_f_args input_info info mutation_type == MutationType MUTATED_OUT_GRAPH f_metadata_mutated_inputs = inp inp info zip flat_f_args input_info info mutates_metadata This logic annoyingly re-figures out exactly what outputs compiled fw graph will When handling subclasses we need info about all outputs compiled forward graph so we know precisely which graph outputs wrap back into tensor subclasses Ideally we would refactor so have is_train flag have separate inference training paths decide which inputs output ask subclass info However we currently stash indexing information each SubclassMeta about its order graph outputs list f_fw_graph_outs = list flat_f_outs is_train keep_input_mutations f_fw_graph_outs = f_mutated_inputs + f_fw_graph_outs even when keep_input_mutations True we never keep metadata-only mutations fw graph f_fw_graph_outs = f_metadata_mutated_inputs + f_fw_graph_outs is_train f_fw_graph_outs = f_fw_graph_outs + intermediate_bases fw_graph_outs = pytree tree_map from_fun f_fw_graph_outs grad_enabled_mutation = None torch is_grad_enabled = prior_grad_enabled grad_enabled_mutation = torch is_grad_enabled torch set_grad_enabled prior_grad_enabled Restore prior state after tracing log debug grad_mode mutation encountered graph Will emit mutation epilogue set grad_mode= s grad_enabled_mutation metadata = ViewAndMutationMeta input_info=input_info output_info=output_info num_intermediate_bases=len intermediate_bases keep_input_mutations=keep_input_mutations traced_tangents=traced_tangents traced_tangents_descs=traced_tangents_descs subclass_inp_meta=create_subclass_meta flat_args subclass_fw_graph_out_meta=create_subclass_meta fw_graph_outs subclass_tangent_meta=create_subclass_meta traced_tangents count_symints=False with_memory_format=True is_train=is_train grad_enabled_mutation=grad_enabled_mutation static_input_indices=static_input_indices tokens=mode _tokens metadata inner