Owner s module autograd types unittest warnings torch torch autograd functional autogradF torch testing _internal common_cuda TEST_CUDA torch testing _internal common_utils gradcheck gradgradcheck instantiate_parametrized_tests parametrize run_tests subtest TestCase torch testing _internal logging_tensor LoggingTensor Utilities parametrizing tensor constructors used autograd tests TODO maybe move somewhere so other tests can also use NB Not all factory functions included A complete list can found here https pytorch org cppdocs notes tensor_creation html base_ctors_dict = ones torch ones zeros torch zeros randn torch randn rand torch rand tensor torch tensor base_ctors = types SimpleNamespace base_ctors_dict wrap_with_logging_tensor ctor wrapper args kwargs requires_grad = kwargs pop requires_grad False LoggingTensor ctor args kwargs requires_grad=requires_grad wrapper logging_tensor_ctors_dict = k wrap_with_logging_tensor ctor k ctor base_ctors_dict items logging_tensor_ctors = types SimpleNamespace logging_tensor_ctors_dict base_and_logging_tensor = parametrize ctors subtest base_ctors name= base_tensor subtest logging_tensor_ctors name= logging_tensor FIXME_base_and_xfail_logging_tensor = parametrize ctors subtest base_ctors name= base_tensor subtest logging_tensor_ctors name= logging_tensor decorators= unittest expectedFailure NB This equivalent having both parametrize vectorized True False FIXME_base_and_xfail_logging_tensor except non-vectorized logging_tensor case actually expected succeed FIXME_xfail_vectorized_logging_tensor = parametrize vectorize ctors subtest True base_ctors name= vectorized_base_tensor subtest False base_ctors name= base_tensor subtest True logging_tensor_ctors name= vectorized_logging_tensor decorators= unittest expectedFailure subtest False logging_tensor_ctors name= logging_tensor vectorized_logging_tensor = parametrize vectorize ctors subtest True base_ctors name= vectorized_base_tensor subtest False base_ctors name= base_tensor subtest True logging_tensor_ctors name= vectorized_logging_tensor subtest False logging_tensor_ctors name= logging_tensor TestAutogradFunctional TestCase _assert_same_struct res base base res should Tensors tuple Tensors same size isinstance base torch Tensor assertTrue isinstance res torch Tensor assertEqual base size res size isinstance base tuple assertTrue isinstance res tuple assertEqual len base len res el_base el_res zip base res assertTrue isinstance el_base torch Tensor assertTrue isinstance el_res torch Tensor assertEqual el_base size el_res size Wrong base raise RuntimeError The base given ` _assert_same_struct ` doesn t have right structure _assert_interleaved_struct res base base base base can Tensors tuples Tensors If they tuples res should tuple well The indexing works follows base base being - tuple tuple res i j k l = base i k base j l - tuple Tensor res i k l = base i k base l - Tensor tuple res i j l = base i base j l - Tensor Tensor res k l = base k base l isinstance base torch Tensor isinstance base torch Tensor assertTrue isinstance res torch Tensor assertEqual res size base size + base size isinstance base tuple isinstance base torch Tensor assertTrue isinstance res tuple assertEqual len res len base el_res el_base zip res base assertTrue isinstance el_res torch Tensor assertTrue isinstance el_base torch Tensor assertEqual el_res size el_base size + base size isinstance base torch Tensor isinstance base tuple assertTrue isinstance res tuple assertEqual len res len base el_res el_base zip res base assertTrue isinstance el_res torch Tensor assertTrue isinstance el_base torch Tensor assertEqual el_res size base size + el_base size isinstance base tuple isinstance base tuple assertTrue isinstance res tuple assertEqual len res len base el_res el_base zip res base assertTrue isinstance el_res tuple assertEqual len res len base el_el_res el_base zip el_res base assertTrue isinstance el_el_res torch Tensor assertTrue isinstance el_base torch Tensor assertEqual el_el_res size el_base size + el_base size Wrong bases raise RuntimeError The bases given ` _assert_interleaved_struct ` don t have right structure base_and_logging_tensor test_vjp_err_check ctors foo narrow bar narrow bar inp = ctors rand v = ctors ones assertRaisesRegex TypeError The inputs given vjp must either Tensor res = autogradF vjp foo inp v assertRaisesRegex TypeError The outputs user-provided function given vjp must res = autogradF vjp bar inp v assertRaisesRegex RuntimeError The vector v can only None user-provided function returns res = autogradF vjp foo inp assertRaisesRegex RuntimeError The given v should contain single Tensor res = autogradF vjp foo inp torch ones_like inp torch ones_like inp assertRaisesRegex RuntimeError v has invalid size should torch Size res = autogradF vjp foo inp v res = autogradF vjp foo inp v _assert_same_struct res inp base_and_logging_tensor test_vjp_err_check_strict ctors foo detach bar Make non-leaf Tensor requires_grad connected input long float requires_grad_ clone inp = ctors rand v = ctors rand assertRaisesRegex RuntimeError Output user-provided function does require gradients res = autogradF vjp foo inp v strict=True res = autogradF vjp foo inp v strict=False _assert_same_struct res inp assertEqual res abs sum assertRaisesRegex RuntimeError The output user-provided function independent input res = autogradF vjp bar inp v strict=True res = autogradF vjp bar inp v strict=False _assert_same_struct res inp assertEqual res abs sum The Jacobian does depend input foo clone inp requires_grad_ assertRaisesRegex RuntimeError jacobian user-provided function independent input res = autogradF vjp foo inp v create_graph=True strict=True res = autogradF vjp foo inp v create_graph=True strict=False _assert_same_struct res inp assertEqual res v base_and_logging_tensor test_vjp_no_grad ctors reducer x x sum dim= inputs = ctors rand v = ctors ones torch no_grad res = autogradF vjp reducer inputs v assertIsNone res grad_fn assertIsNone res grad_fn assertNotEqual res ctors zeros inputs requires_grad_ v requires_grad_ torch no_grad res = autogradF vjp reducer inputs v create_graph=True assertIsNotNone res grad_fn assertIsNotNone res grad_fn assertNotEqual res ctors zeros base_and_logging_tensor test_vjp_output ctors reducer x x sum dim= inputs = ctors rand v = ctors ones res = autogradF vjp reducer inputs v _assert_same_struct res inputs assertIsNone res grad_fn assertIsNone res grad_fn adder x y x + y inputs = ctors rand ctors rand v = ctors ones out vjp_val = autogradF vjp adder inputs v _assert_same_struct vjp_val inputs assertIsNone out grad_fn assertIsNone vjp_val grad_fn assertIsNone vjp_val grad_fn adder x y x + y x + y inputs = ctors rand ctors rand v = ctors tensor ctors tensor out vjp_val = autogradF vjp adder inputs v _assert_same_struct vjp_val inputs assertIsNone out grad_fn assertIsNone out grad_fn assertIsNone vjp_val grad_fn assertIsNone vjp_val grad_fn base_and_logging_tensor test_vjp_scalar ctors reducer x x sum inputs = ctors rand v = ctors ones res = autogradF vjp reducer inputs v _assert_same_struct res v _assert_same_struct res inputs res = autogradF vjp reducer inputs _assert_same_struct res v _assert_same_struct res inputs expander x x unsqueeze repeat inputs = ctors rand v = ctors ones res = autogradF vjp expander inputs v _assert_same_struct res v _assert_same_struct res inputs base_and_logging_tensor test_vjp_create_graph ctors reducer x x sum dim= inputs = ctors rand dtype=torch double v = ctors ones dtype=torch double inputs requires_grad_ v requires_grad_ res = autogradF vjp reducer inputs v create_graph=True _assert_same_struct res inputs assertIsNotNone res grad_fn assertIsNotNone res grad_fn gradcheck lambda inp v autogradF vjp reducer inputs v create_graph=True inputs v gradgradcheck lambda inp v autogradF vjp reducer inputs v create_graph=True inputs v adder x y x + y x y inputs = ctors rand dtype=torch double requires_grad=True ctors rand dtype=torch double requires_grad=True v = ctors tensor dtype=torch double requires_grad=True ctors tensor dtype=torch double requires_grad=True gradcheck lambda args autogradF vjp adder args args create_graph=True inputs + v gradgradcheck lambda args autogradF vjp adder args args create_graph=True inputs + v foo args x y = args v = args x = x cos val grad = autogradF vjp adder x y v create_graph=True val exp + val exp + grad exp + grad exp + x exp + y exp gradcheck foo inputs + v gradgradcheck foo inputs + v base_and_logging_tensor test_jvp_err_check ctors foo narrow bar narrow bar inp = ctors rand v = ctors rand assertRaisesRegex TypeError The inputs given jvp must either Tensor res = autogradF jvp foo inp v assertRaisesRegex TypeError The outputs user-provided function given jvp must res = autogradF jvp bar inp v assertRaisesRegex RuntimeError The vector v can only None input user-provided function res = autogradF jvp foo inp assertRaisesRegex RuntimeError The given v should contain single Tensor res = autogradF jvp foo inp v v assertRaisesRegex RuntimeError v has invalid size should torch Size res = autogradF jvp foo inp v res = autogradF jvp foo inp v _assert_same_struct res foo inp base_and_logging_tensor test_jvp_err_check_strict ctors foo detach bar Make non-leaf Tensor requires_grad connected input long float requires_grad_ clone inp = ctors rand v = ctors rand assertRaisesRegex RuntimeError Output user-provided function does require gradients res = autogradF jvp foo inp v strict=True res = autogradF jvp foo inp v strict=False _assert_same_struct res res assertEqual res abs sum assertRaisesRegex RuntimeError The output user-provided function independent input res = autogradF jvp bar inp v strict=True res = autogradF jvp bar inp v strict=False _assert_same_struct res res assertEqual res abs sum The Jacobian does depend input foo clone inp requires_grad_ assertRaisesRegex RuntimeError jacobian user-provided function independent input res = autogradF jvp foo inp v create_graph=True strict=True res = autogradF jvp foo inp v create_graph=True strict=False _assert_same_struct res inp assertEqual res v base_and_logging_tensor test_jvp_no_grad ctors reducer x x sum dim= inputs = ctors rand v = ctors ones torch no_grad res = autogradF jvp reducer inputs v assertIsNone res grad_fn assertIsNone res grad_fn assertNotEqual res ctors zeros inputs requires_grad_ v requires_grad_ torch no_grad res = autogradF jvp reducer inputs v create_graph=True assertIsNotNone res grad_fn assertIsNotNone res grad_fn assertNotEqual res ctors zeros base_and_logging_tensor test_jvp_output ctors reducer x x sum dim= inputs = ctors rand v = ctors ones res = autogradF jvp reducer inputs v _assert_same_struct res res assertIsNone res grad_fn assertIsNone res grad_fn adder x y x + y inputs = ctors rand ctors rand v = ctors ones ctors ones out jvp_val = autogradF jvp adder inputs v _assert_same_struct jvp_val out assertIsNone out grad_fn assertIsNone jvp_val grad_fn assertIsNone jvp_val grad_fn adder x y x + y x + y inputs = ctors rand ctors rand v = ctors tensor ctors tensor out jvp_val = autogradF jvp adder inputs v _assert_same_struct jvp_val out assertIsNone out grad_fn assertIsNone out grad_fn assertIsNone jvp_val grad_fn assertIsNone jvp_val grad_fn base_and_logging_tensor test_jvp_scalar ctors reducer x x sum inputs = ctors rand v = ctors ones res = autogradF jvp reducer inputs v _assert_same_struct res ctors zeros _assert_same_struct res res expander x x unsqueeze repeat inputs = ctors rand v = ctors ones res = autogradF jvp expander inputs v _assert_same_struct res ctors zeros _assert_same_struct res res res = autogradF jvp expander inputs _assert_same_struct res ctors zeros _assert_same_struct res res base_and_logging_tensor test_jvp_create_graph ctors reducer x x sum dim= inputs = ctors rand dtype=torch double v = ctors ones dtype=torch double inputs requires_grad_ v requires_grad_ res = autogradF jvp reducer inputs v create_graph=True _assert_same_struct res res assertIsNotNone res grad_fn assertIsNotNone res grad_fn gradcheck lambda inp v autogradF jvp reducer inp v create_graph=True inputs v gradgradcheck lambda inp v autogradF jvp reducer inp v create_graph=True inputs v adder x y x + y x y inputs = ctors rand dtype=torch double requires_grad=True ctors rand dtype=torch double requires_grad=True v = ctors tensor dtype=torch double requires_grad=True ctors tensor dtype=torch double requires_grad=True gradcheck lambda args autogradF jvp adder args args create_graph=True inputs + v gradgradcheck lambda args autogradF jvp adder args args create_graph=True inputs + v foo args x y = args v = args x = x cos val grad = autogradF jvp adder x y v create_graph=True val exp + val exp + grad exp + grad exp + x exp + y exp gradcheck foo inputs + v gradgradcheck foo inputs + v _test_construct_standard_basis_for inputs numels = tuple tensor numel tensor inputs results = autogradF _construct_standard_basis_for inputs numels result inp zip results inputs assertEqual result dtype inp dtype assertEqual result device inp device results = torch cat result device= cpu dtype=torch float result results dim= expected = torch eye results shape dtype=torch float assertEqual results expected base_and_logging_tensor test_construct_standard_basis_for ctors test_cases = ctors randn ctors randn ctors randn ctors randn ctors randn ctors randn ctors randn ctors randn ctors randn ctors randn ctors randn ctors randn ctors randn ctors randn ctors randn ctors randn dtype=torch float ctors randn dtype=torch float inputs test_cases _test_construct_standard_basis_for inputs unittest skipIf TEST_CUDA test requires CUDA base_and_logging_tensor test_construct_standard_basis_for_cuda ctors test_cases = ctors randn ctors randn device= cuda ctors randn device= cuda ctors randn inputs test_cases _test_construct_standard_basis_for inputs _test_vectorize_raises_no_warnings api ctors vmap experimental prototype When someone calls torch vmap raises python warning This test checks autogradF jacobian hessian don t raise experimental prototype warning nice public-facing API raise warning no matter how called foo sum x = ctors randn warnings catch_warnings record=True wa api foo x vectorize=True assertEqual len wa base_and_logging_tensor test_jacobian_vectorize_raises_no_warnings ctors _test_vectorize_raises_no_warnings autogradF jacobian ctors base_and_logging_tensor test_hessian_vectorize_raises_no_warnings ctors _test_vectorize_raises_no_warnings autogradF hessian ctors parametrize vectorize True False base_and_logging_tensor test_jacobian_err_check vectorize ctors foo narrow bar narrow bar inp = ctors rand assertRaisesRegex TypeError The inputs given jacobian must either Tensor res = autogradF jacobian foo inp vectorize=vectorize assertRaisesRegex TypeError The outputs user-provided function given jacobian must res = autogradF jacobian bar inp vectorize=vectorize res = autogradF jacobian foo inp vectorize=vectorize _assert_interleaved_struct res foo inp inp foo b b narrow inp = ctors rand ctors rand res = autogradF jacobian foo inp vectorize=vectorize _assert_interleaved_struct res foo inp inp base_and_logging_tensor test_jacobian_err_check_strict ctors foo detach bar Make non-leaf Tensor requires_grad connected input long float requires_grad_ clone inp = ctors rand assertRaisesRegex RuntimeError Output user-provided function does require gradients res = autogradF jacobian foo inp strict=True res = autogradF jacobian foo inp strict=False _assert_interleaved_struct res foo inp inp assertEqual res abs sum assertRaisesRegex RuntimeError Output user-provided function independent input res = autogradF jacobian bar inp strict=True res = autogradF jacobian bar inp strict=False _assert_interleaved_struct res foo inp inp assertEqual res abs sum The Jacobian does depend input foo clone inp requires_grad_ assertRaisesRegex RuntimeError jacobian user-provided function independent input res = autogradF jacobian foo inp create_graph=True strict=True res = autogradF jacobian foo inp create_graph=True strict=False _assert_interleaved_struct res inp inp assertEqual res torch eye base_and_logging_tensor test_jacobian_err_check_strict_vectorize ctors foo x x inp = ctors rand assertRaisesRegex RuntimeError supported together autogradF jacobian foo inp strict=True vectorize=True base_and_logging_tensor test_jacobian_no_grad ctors exp_reducer x x exp sum dim= inputs = ctors rand torch no_grad res = autogradF jacobian exp_reducer inputs assertIsNone res grad_fn assertNotEqual res ctors zeros torch no_grad res = autogradF jacobian exp_reducer inputs create_graph=True assertIsNotNone res grad_fn assertNotEqual res ctors zeros vectorized_logging_tensor test_jacobian_output vectorize ctors exp_reducer x x exp sum dim= inputs = ctors rand res = autogradF jacobian exp_reducer inputs vectorize=vectorize _assert_interleaved_struct res exp_reducer inputs inputs assertIsNone res grad_fn identity x x clone inputs = ctors rand res = autogradF jacobian identity inputs vectorize=vectorize _assert_interleaved_struct res identity inputs inputs assertIsNone res grad_fn assertEqual res torch eye add_exp_reducer x y x + y exp sum dim= inputs = ctors rand ctors rand res = autogradF jacobian add_exp_reducer inputs vectorize=vectorize _assert_interleaved_struct res add_exp_reducer inputs inputs assertIsNone res grad_fn assertIsNone res grad_fn vectorized_logging_tensor test_jacobian_scalar vectorize ctors reducer x x sum inputs = ctors rand res = autogradF jacobian reducer inputs vectorize=vectorize _assert_same_struct res inputs expander x x unsqueeze repeat inputs = ctors rand res = autogradF jacobian expander inputs vectorize=vectorize _assert_same_struct res ctors zeros parametrize vectorize True False base_and_logging_tensor test_jacobian_create_graph vectorize ctors exp_reducer x x exp sum dim= inputs = ctors rand dtype=torch double requires_grad=True res = autogradF jacobian exp_reducer inputs create_graph=True vectorize=vectorize _assert_interleaved_struct res exp_reducer inputs inputs assertIsNotNone res grad_fn gradcheck lambda inp autogradF jacobian exp_reducer inp create_graph=True vectorize=vectorize inputs gradgradcheck lambda inp autogradF jacobian exp_reducer inp create_graph=True vectorize=vectorize inputs add_exp_reducer x y x + y exp sum dim= inputs = ctors rand dtype=torch double requires_grad=True ctors rand dtype=torch double requires_grad=True res = autogradF jacobian add_exp_reducer inputs create_graph=True vectorize=vectorize _assert_interleaved_struct res add_exp_reducer inputs inputs assertIsNotNone res grad_fn assertIsNotNone res grad_fn gradcheck lambda inp autogradF jacobian add_exp_reducer inp create_graph=True vectorize=vectorize inputs gradgradcheck lambda inp autogradF jacobian add_exp_reducer inp create_graph=True vectorize=vectorize inputs foo x y x = x cos val jac = autogradF jacobian add_exp_reducer x y create_graph=True vectorize=vectorize res = val exp sum + val exp sum + jac exp sum res = res + jac exp sum + x exp sum + y exp sum res gradcheck foo inputs gradgradcheck foo inputs _check_jacobian_vectorize_correctness f inputs test_forward_ad=True expected = autogradF jacobian f inputs vectorize=False result_backward_mode = autogradF jacobian f inputs vectorize=True assertEqual result_backward_mode expected test_forward_ad result_forward_mode = autogradF jacobian f inputs strategy= forward-mode vectorize=True assertEqual result_forward_mode expected base_and_logging_tensor test_jacobian_vectorize_correctness_simple ctors f x x x = ctors randn _check_jacobian_vectorize_correctness f x base_and_logging_tensor test_jacobian_vectorize_correctness_multi_input ctors f x y x cos x y sin x = ctors randn y = ctors randn _check_jacobian_vectorize_correctness f x y base_and_logging_tensor test_jacobian_vectorize_correctness_multi_input_multi_output ctors f x y x x y x x sum y y sum x = ctors randn y = ctors randn _check_jacobian_vectorize_correctness f x y base_and_logging_tensor test_jacobian_vectorize_correctness_unrelated_outputs ctors f x y x y x y x = ctors randn y = ctors randn _check_jacobian_vectorize_correctness f x y base_and_logging_tensor test_jacobian_vectorize_correctness_zero_dim ctors zero-dim output f x y x sum y sum x y x = ctors randn y = ctors randn _check_jacobian_vectorize_correctness f x y zero-dim input g x torch stack x x x x = ctors randn _check_jacobian_vectorize_correctness g x Mixed zero-dim input zero-dim output h x y y sum x y x = ctors randn y = ctors randn _check_jacobian_vectorize_correctness h x y unittest skipIf TEST_CUDA test requires CUDA base_and_logging_tensor test_jacobian_vectorize_correctness_different_devices ctors f x y x y x y cuda x = ctors randn y = ctors randn _check_jacobian_vectorize_correctness f x y base_and_logging_tensor test_jacobian_vectorize_correctness_different_dtype ctors f x y x y float x y double x = ctors randn y = ctors randn The Jacobian computed using forward AD has dtype output Jacobian computed reverse AD has dtype input _check_jacobian_vectorize_correctness f x y test_forward_ad=False _check_hessian_vectorize_correctness f inputs expected = autogradF hessian f inputs vectorize=False result = autogradF hessian f inputs vectorize=True assertEqual result expected result_forward_mode = autogradF hessian f inputs outer_jacobian_strategy= forward-mode vectorize=True assertEqual result_forward_mode expected base_and_logging_tensor test_hessian_vectorize_correctness_simple ctors f x x sum x = ctors randn _check_hessian_vectorize_correctness f x base_and_logging_tensor test_hessian_vectorize_correctness_multi_input ctors f x y z x relu x y sin z sum x = ctors randn y = ctors randn z = ctors randn _check_hessian_vectorize_correctness f x y z base_and_logging_tensor test_hessian_vectorize_correctness_unrelated_outputs ctors output unrelated one input f x y x sum x = ctors randn y = ctors randn _check_hessian_vectorize_correctness f x y output unrelated all inputs f x y ctors ones x = ctors randn y = ctors randn _check_hessian_vectorize_correctness f x y parametrize vectorize True False base_and_logging_tensor test_hessian_err_check vectorize ctors foo narrow exp sum bar narrow bar bar narrow bar narrow narrow inp = ctors rand assertRaisesRegex TypeError The inputs given hessian must either Tensor res = autogradF hessian foo inp vectorize=vectorize assertRaisesRegex TypeError The outputs user-provided function given hessian must res = autogradF hessian bar inp vectorize=vectorize err_msg_out = The Tensor returned function given hessian should contain single element assertRaisesRegex RuntimeError err_msg_out res = autogradF hessian bar inp vectorize=vectorize assertRaisesRegex RuntimeError The function given hessian should single Tensor res = autogradF hessian bar inp vectorize=vectorize res = autogradF hessian foo inp vectorize=vectorize _assert_interleaved_struct res inp inp foo b b narrow narrow sum inp = ctors rand ctors rand res = autogradF hessian foo inp vectorize=vectorize _assert_interleaved_struct res inp inp base_and_logging_tensor test_hessian_err_check_strict ctors foo detach sum bar Make non-leaf Tensor requires_grad connected input long float requires_grad_ clone sum bar A Linear function which jacobian independent input sum inp = ctors rand assertRaisesRegex RuntimeError Output user-provided function does require gradients res = autogradF hessian foo inp strict=True res = autogradF hessian foo inp strict=False _assert_interleaved_struct res inp inp assertEqual res abs sum assertRaisesRegex RuntimeError jacobian user-provided function respect input res = autogradF hessian bar inp strict=True res = autogradF hessian bar inp strict=False _assert_interleaved_struct res inp inp assertEqual res abs sum assertRaisesRegex RuntimeError jacobian user-provided function respect input res = autogradF hessian bar inp strict=True res = autogradF hessian bar inp strict=False _assert_interleaved_struct res inp inp assertEqual res abs sum base_and_logging_tensor test_hessian_err_check_strict_vectorize ctors foo x x sum inp = ctors rand assertRaisesRegex RuntimeError supported together autogradF hessian foo inp strict=True vectorize=True base_and_logging_tensor test_hessian_no_grad ctors pow_reducer x x pow sum inputs = ctors rand torch no_grad res = autogradF hessian pow_reducer inputs assertIsNone res grad_fn assertIsNone res grad_fn assertIsNone res grad_fn assertIsNone res grad_fn assertNotEqual res ctors zeros torch no_grad res = autogradF hessian pow_reducer inputs create_graph=True assertIsNotNone res grad_fn assertIsNotNone res grad_fn assertIsNotNone res grad_fn assertIsNotNone res grad_fn assertNotEqual res ctors zeros vectorized_logging_tensor test_hessian_output vectorize ctors pow_reducer x x pow sum inputs = ctors rand res = autogradF hessian pow_reducer inputs vectorize=vectorize _assert_interleaved_struct res inputs inputs assertIsNone res grad_fn add_pow_reducer x y x + y pow sum inputs = ctors rand ctors rand res = autogradF hessian add_pow_reducer inputs vectorize=vectorize _assert_interleaved_struct res inputs inputs assertIsNone res grad_fn assertIsNone res grad_fn assertIsNone res grad_fn assertIsNone res grad_fn parametrize vectorize True False base_and_logging_tensor test_hessian_scalar vectorize ctors reducer x x sum inputs = ctors rand res = autogradF hessian reducer inputs vectorize=vectorize _assert_interleaved_struct res inputs inputs inputs = ctors rand res = autogradF hessian reducer inputs vectorize=vectorize _assert_same_struct res inputs bad_reducer x x sum view inputs = ctors rand res = autogradF hessian bad_reducer inputs vectorize=vectorize _assert_interleaved_struct res inputs inputs parametrize vectorize True False base_and_logging_tensor test_hessian_create_graph vectorize ctors pow_reducer x x pow sum inputs = ctors rand dtype=torch double requires_grad=True res = autogradF hessian pow_reducer inputs create_graph=True vectorize=vectorize _assert_interleaved_struct res inputs inputs assertIsNotNone res grad_fn gradcheck lambda inp autogradF hessian pow_reducer inp create_graph=True vectorize=vectorize inputs gradgradcheck lambda inp autogradF hessian pow_reducer inp create_graph=True vectorize=vectorize inputs add_pow_reducer x y x + y pow sum inputs = ctors rand dtype=torch double requires_grad=True ctors rand dtype=torch double requires_grad=True res = autogradF hessian add_pow_reducer inputs create_graph=True vectorize=vectorize _assert_interleaved_struct res inputs inputs assertIsNotNone res grad_fn assertIsNotNone res grad_fn assertIsNotNone res grad_fn assertIsNotNone res grad_fn flatten inp tuple el_lvl el_lvl inp el_lvl el_lvl gradcheck lambda inp flatten autogradF hessian add_pow_reducer inp create_graph=True vectorize=vectorize inputs gradgradcheck lambda inp flatten autogradF hessian add_pow_reducer inp create_graph=True vectorize=vectorize inputs foo x y x = x cos val hess = autogradF hessian add_pow_reducer x y create_graph=True vectorize=vectorize res = val cos sum + val cos sum + hess cos sum res = res + hess cos sum + x cos sum + y cos sum res gradcheck foo inputs gradgradcheck foo inputs base_and_logging_tensor test_vhp_err_check ctors foo narrow exp sum bar narrow bar bar narrow inp = ctors rand v = ctors rand assertRaisesRegex TypeError The inputs given vhp must either Tensor res = autogradF vhp foo inp v assertRaisesRegex TypeError The outputs user-provided function given vhp must res = autogradF vhp bar inp v err_msg_out = The Tensor returned function given vhp should contain single element assertRaisesRegex RuntimeError err_msg_out res = autogradF vhp bar inp v assertRaisesRegex RuntimeError v has invalid size res = autogradF vhp foo inp ctors rand assertRaisesRegex TypeError The v given vhp must either Tensor tuple Tensors res = autogradF vhp foo inp v res = autogradF vhp foo inp v _assert_same_struct res inp foo b b narrow narrow sum inp = ctors rand ctors rand v = ctors rand ctors rand res = autogradF vhp foo inp v _assert_same_struct res inp base_and_logging_tensor test_vhp_err_check_strict ctors foo detach sum bar Make non-leaf Tensor requires_grad connected input long float requires_grad_ clone sum bar A Linear function which jacobian independent input sum inp = ctors rand v = ctors rand assertRaisesRegex RuntimeError Output user-provided function does require gradients res = autogradF vhp foo inp v strict=True res = autogradF vhp foo inp v strict=False _assert_same_struct res inp assertEqual res abs sum assertRaisesRegex RuntimeError The output user-provided function independent input res = autogradF vhp bar inp v strict=True res = autogradF vhp bar inp v strict=False _assert_same_struct res inp assertEqual res abs sum assertRaisesRegex RuntimeError jacobian user-provided function respect input res = autogradF vhp bar inp v strict=True res = autogradF vhp bar inp v strict=False _assert_same_struct res inp assertEqual res abs sum base_and_logging_tensor test_vhp_no_grad ctors reducer x x exp sum inputs = ctors rand v = ctors ones torch no_grad res = autogradF vhp reducer inputs v assertIsNone res grad_fn assertIsNone res grad_fn assertNotEqual res ctors zeros torch no_grad res = autogradF vhp reducer inputs v create_graph=True assertIsNotNone res grad_fn assertIsNotNone res grad_fn assertNotEqual res ctors zeros base_and_logging_tensor test_vhp_output ctors foo narrow exp sum inputs = ctors rand v = ctors ones res = autogradF vhp foo inputs v _assert_same_struct res inputs assertIsNone res grad_fn assertIsNone res grad_fn bar b + b narrow exp sum inputs = ctors rand ctors rand v = ctors ones ctors ones out vhp_val = autogradF vhp bar inputs v _assert_same_struct vhp_val inputs assertIsNone out grad_fn assertIsNone vhp_val grad_fn assertIsNone vhp_val grad_fn base_and_logging_tensor test_vhp_scalar ctors reducer x x sum inputs = ctors rand v = ctors ones res = autogradF vhp reducer inputs v _assert_same_struct res inputs inputs = ctors rand v = ctors rand res = autogradF vhp reducer inputs v _assert_same_struct res inputs res = autogradF vhp reducer inputs _assert_same_struct res inputs bad_reducer x x sum view inputs = ctors rand v = ctors rand res = autogradF vhp bad_reducer inputs v _assert_same_struct res inputs base_and_logging_tensor test_vhp_create_graph ctors foo narrow exp sum inputs = ctors rand dtype=torch double requires_grad=True v = ctors ones dtype=torch double requires_grad=True res = autogradF vhp foo inputs v create_graph=True _assert_same_struct res inputs assertIsNotNone res grad_fn assertIsNotNone res grad_fn gradcheck lambda inp v autogradF vhp foo inp v create_graph=True inputs v gradgradcheck lambda inp v autogradF vhp foo inp v create_graph=True inputs v bar b + b narrow exp sum inputs = ctors rand dtype=torch double requires_grad=True ctors rand dtype=torch double requires_grad=True v = ctors ones dtype=torch double requires_grad=True ctors ones dtype=torch double requires_grad=True out vhp_val = autogradF vhp bar inputs v create_graph=True _assert_same_struct vhp_val inputs assertIsNotNone out grad_fn assertIsNotNone vhp_val grad_fn assertIsNotNone vhp_val grad_fn gradcheck lambda args autogradF vhp bar args args create_graph=True inputs + v gradgradcheck lambda args autogradF vhp bar args args create_graph=True inputs + v foo args x y = args v = args x = x cos val grad = autogradF vhp bar x y v create_graph=True val cos + grad cos sum + grad cos + x cos sum + y cos gradcheck foo inputs + v gradgradcheck foo inputs + v base_and_logging_tensor test_hvp_err_check ctors foo narrow exp sum bar narrow bar bar narrow inp = ctors rand v = ctors rand res = autogradF hvp foo inp v assertRaisesRegex TypeError The inputs given hvp must either Tensor res = autogradF hvp foo inp v assertRaisesRegex TypeError The outputs user-provided function given hvp must res = autogradF hvp bar inp v err_msg_out = The Tensor returned function given hvp should contain single element assertRaisesRegex RuntimeError err_msg_out res = autogradF hvp bar inp v assertRaisesRegex RuntimeError v has invalid size res = autogradF hvp foo inp ctors rand assertRaisesRegex TypeError The v given hvp must either Tensor tuple Tensors res = autogradF hvp foo inp v res = autogradF hvp foo inp v _assert_same_struct res inp foo b b narrow narrow sum inp = ctors rand ctors rand v = ctors rand ctors rand res = autogradF hvp foo inp v _assert_same_struct res inp base_and_logging_tensor test_hvp_err_check_strict ctors foo detach sum bar Make non-leaf Tensor requires_grad connected input long float requires_grad_ clone sum bar A Linear function which jacobian independent input sum inp = ctors rand v = ctors rand assertRaisesRegex RuntimeError Output user-provided function does require gradients res = autogradF hvp foo inp v strict=True res = autogradF hvp foo inp v strict=False _assert_same_struct res inp assertEqual res abs sum assertRaisesRegex RuntimeError The output user-provided function independent input res = autogradF hvp bar inp v strict=True res = autogradF hvp bar inp v strict=False _assert_same_struct res inp assertEqual res abs sum assertRaisesRegex RuntimeError jacobian user-provided function respect input res = autogradF hvp bar inp v strict=True res = autogradF hvp bar inp v strict=False _assert_same_struct res inp assertEqual res abs sum base_and_logging_tensor test_hvp_no_grad ctors reducer x x exp sum inputs = ctors rand v = ctors ones torch no_grad res = autogradF hvp reducer inputs v assertIsNone res grad_fn assertIsNone res grad_fn assertNotEqual res ctors zeros torch no_grad res = autogradF hvp reducer inputs v create_graph=True assertIsNotNone res grad_fn assertIsNotNone res grad_fn assertNotEqual res ctors zeros base_and_logging_tensor test_hvp_output ctors foo narrow exp sum inputs = ctors rand v = ctors ones res = autogradF hvp foo inputs v _assert_same_struct res inputs assertIsNone res grad_fn assertIsNone res grad_fn bar b + b narrow exp sum inputs = ctors rand ctors rand v = ctors ones ctors ones out hvp_val = autogradF hvp bar inputs v _assert_same_struct hvp_val inputs assertIsNone out grad_fn assertIsNone hvp_val grad_fn assertIsNone hvp_val grad_fn base_and_logging_tensor test_hvp_scalar ctors reducer x x exp sum inputs = ctors rand v = ctors ones res = autogradF hvp reducer inputs v _assert_same_struct res inputs inputs = ctors rand v = ctors rand res = autogradF hvp reducer inputs v _assert_same_struct res inputs res = autogradF hvp reducer inputs _assert_same_struct res inputs bad_reducer x x exp sum view inputs = ctors rand v = ctors rand res = autogradF hvp bad_reducer inputs v _assert_same_struct res inputs base_and_logging_tensor test_hvp_create_graph ctors foo narrow exp sum inputs = ctors rand dtype=torch double requires_grad=True v = ctors ones dtype=torch double requires_grad=True res = autogradF hvp foo inputs v create_graph=True _assert_same_struct res inputs assertIsNotNone res grad_fn assertIsNotNone res grad_fn gradcheck lambda inp v autogradF hvp foo inp v create_graph=True inputs v gradgradcheck lambda inp v autogradF hvp foo inp v create_graph=True inputs v bar b + b narrow exp sum inputs = ctors rand dtype=torch double requires_grad=True ctors rand dtype=torch double requires_grad=True v = ctors ones dtype=torch double requires_grad=True ctors ones dtype=torch double requires_grad=True out hvp_val = autogradF hvp bar inputs v create_graph=True _assert_same_struct hvp_val inputs assertIsNotNone out grad_fn assertIsNotNone hvp_val grad_fn assertIsNotNone hvp_val grad_fn gradcheck lambda args autogradF hvp bar args args create_graph=True inputs + v gradgradcheck lambda args autogradF hvp bar args args create_graph=True inputs + v foo args x y = args v = args x = x cos val grad = autogradF hvp bar x y v create_graph=True val cos + grad cos sum + grad cos + x cos sum + y cos gradcheck foo inputs + v gradgradcheck foo inputs + v base_and_logging_tensor test_jacobian_match_vjp_jvp ctors foo x x + x sum inputs = ctors rand v = ctors rand jac = autogradF jacobian foo inputs jvp = autogradF jvp foo inputs v vjp = autogradF vjp foo inputs v assertEqual jvp torch mm jac v unsqueeze squeeze assertEqual vjp torch mm v unsqueeze jac squeeze base_and_logging_tensor test_hessian_match_vhp_hvp ctors foo narrow exp sum inputs = ctors rand v = ctors rand hes = autogradF hessian foo inputs hvp = autogradF hvp foo inputs v vhp = autogradF vhp foo inputs v assertEqual hvp torch mm hes v unsqueeze squeeze assertEqual vhp torch mm v unsqueeze hes squeeze instantiate_parametrized_tests TestAutogradFunctional __name__ == __main__ run_tests