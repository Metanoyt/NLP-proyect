Owner s oncall distributed sys functools partial wraps torch torch distributed dist dist is_available print Distributed available skipping tests file=sys stderr sys exit torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_distributed DistributedTestBase TEST_SKIPS torch testing _internal common_utils run_tests skipIfHpu TEST_CUDA TEST_HPU TEST_WITH_DEV_DBG_ASAN TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit TEST_HPU DEVICE = hpu TEST_CUDA DEVICE = cuda DEVICE = cpu device_module = torch get_device_module DEVICE device_count = device_module device_count BACKEND = dist get_default_backend_for_device DEVICE with_comms func=None func None partial with_comms wraps func wrapper args kwargs DEVICE = cpu device_count world_size sys exit TEST_SKIPS f multi-gpu- world_size exit_code kwargs device = DEVICE pg = create_pg device=DEVICE try func args kwargs finally torch distributed destroy_process_group wrapper TestObjectCollectives DistributedTestBase with_comms test_all_gather_object device output = None dist get_world_size dist all_gather_object object_list=output obj=self rank i v enumerate output assertEqual i v f rank rank with_comms test_gather_object device output = None dist get_world_size rank == None dist gather_object obj=self rank object_gather_list=output rank == i v enumerate output assertEqual i v f rank rank skipIfHpu with_comms test_send_recv_object_list device val = rank == None object_list = val dist get_world_size rank == dist send_object_list object_list rank == dist recv_object_list object_list rank assertEqual object_list assertEqual None object_list with_comms test_broadcast_object_list device val = rank == None object_list = val dist get_world_size TODO test broadcast_object_list s device argument dist broadcast_object_list object_list=object_list assertEqual object_list with_comms test_scatter_object_list device input_list = list range dist get_world_size rank == None output_list = None dist scatter_object_list scatter_object_output_list=output_list scatter_object_input_list=input_list assertEqual rank output_list Test Object Collectives With Sub Pg setup_sub_pg rank = dist get_rank base_rank = rank - rank ranks = base_rank base_rank + my_pg = dist new_group ranks use_local_synchronization=True rank ranks my_pg skipIfHpu with_comms test_subpg_scatter_object device rank ranks my_pg = setup_sub_pg out_list = None dist scatter_object_list out_list ranks src=ranks group=my_pg assertEqual rank out_list skipIfHpu with_comms test_subpg_all_gather_object device rank ranks my_pg = setup_sub_pg out_list = None len ranks dist all_gather_object out_list rank group=my_pg assertEqual ranks out_list skipIfHpu with_comms test_subpg_gather_object device rank ranks my_pg = setup_sub_pg out_list = None len ranks rank == ranks None dist gather_object rank out_list dst=ranks group=my_pg rank == ranks assertEqual ranks out_list skipIfHpu with_comms test_subpg_broadcast_object device rank ranks my_pg = setup_sub_pg out_list = None rank == ranks out_list = rank dist broadcast_object_list out_list src=ranks group=my_pg assertEqual ranks out_list devices = cpu cuda hpu instantiate_device_type_tests TestObjectCollectives globals only_for=devices __name__ == __main__ run_tests