mypy allow-untyped-defs typing Optional Union torch torch _C _get_privateuse _backend_name _rename_privateuse _backend torch overrides handle_torch_function has_torch_function_unary __all__ = rename_privateuse _backend generate_methods_for_privateuse _backend TODO Should use ` torch _C _get_privateuse _backend_name ` get renamed-backend name ` privateuse ` func will cause error torch jit script so we use global variable named ` _privateuse _backend_name ` _privateuse _backend_name = privateuseone rename_privateuse _backend backend_name str - None r Rename privateuse backend device make more convenient use device name within PyTorch APIs The steps In C++ implement kernels various torch operations register them PrivateUse dispatch key In python call torch utils rename_privateuse _backend foo You can now use foo ordinary device string python Note API can only called once per process Attempting change external backend after s already been set will result error Note AMP If you want support AMP your device you can register custom backend module The backend must register custom backend module ` ` torch _register_device_module foo BackendModule ` ` BackendModule needs have following API s ` ` get_amp_supported_dtype - List torch dtype ` ` get supported dtypes your foo device AMP maybe foo device supports one more dtype Note random If you want support set seed your device BackendModule needs have following API s ` ` _is_in_bad_fork - bool ` ` Return ` ` True ` ` now bad_fork ` ` False ` ` ` ` manual_seed_all seed int - None ` ` Sets seed generating random numbers your devices ` ` device_count - int ` ` Returns number foo s available ` ` get_rng_state device Union int str torch device = foo - Tensor ` ` Returns list ByteTensor representing random number states all devices ` ` set_rng_state new_state Tensor device Union int str torch device = foo - None ` ` Sets random number generator state specified foo device And there some common funcs ` ` is_available - bool ` ` Returns bool indicating foo currently available ` ` current_device - int ` ` Returns index currently selected device For more details see https pytorch org tutorials advanced extend_dispatcher html#get-a-dispatch-key-for-your-backend For existing example see https github com bdhirsh pytorch_open_registration_example Example xdoctest +SKIP failing torch utils rename_privateuse _backend foo This will work assuming you ve implemented right C++ kernels implement torch ones = torch ones device= foo _rename_privateuse _backend backend_name global _privateuse _backend_name _privateuse _backend_name = backend_name _check_register_once module attr hasattr module attr raise RuntimeError f The custom device module module has already been registered attr _normalization_device custom_backend_name str device Optional Union int str torch device = None - int _get_current_device_index _get_device_index = current_device hasattr torch custom_backend_name hasattr getattr torch custom_backend_name _get_device_index getattr getattr torch custom_backend_name _get_device_index The default device index device None _get_current_device_index isinstance device str means parameter passed string format foo convert str object torch device object then process uniformly isinstance device str device = torch device device variable device can only torch device type int type isinstance device torch device device type = custom_backend_name raise RuntimeError f Invalid device must custom_backend_name device device index None device_idx = _get_current_device_index device_idx = device index isinstance device int we can take index number directly device_idx = device device_idx _generate_tensor_methods_for_privateuse _backend custom_backend_name str - None property type ignore misc wrap_tensor_backend torch Tensor - bool has_torch_function_unary TODO mypy doesn t support property see https github com python mypy issues handle_torch_function wrap_tensor_backend __get__ type ignore attr-defined device type == custom_backend_name _check_register_once torch Tensor f is_ custom_backend_name wrap_tensor_backend fget __name__ = f is_ custom_backend_name type ignore attr-defined setattr torch Tensor f is_ custom_backend_name wrap_tensor_backend wrap_tensor_to torch Tensor device Optional Union int torch device = None non_blocking=False kwargs - torch Tensor r Perform Tensor device conversion Call operator implementation note If ` ` ` ` Tensor already has correct ` torch device ` then ` ` ` ` returned Otherwise returned tensor copy ` ` ` ` desired ` torch device ` Args device int optional specified all parameters will copied device non_blocking bool If ` ` True ` ` source pinned memory copy will asynchronous respect host Otherwise argument has no effect kwargs dict For compatibility may contain key ` ` memory_format ` ` argument has_torch_function_unary handle_torch_function wrap_tensor_to device=device non_blocking=False kwargs device_idx = _normalization_device custom_backend_name device device=torch device f custom_backend_name device_idx non_blocking=non_blocking kwargs _check_register_once torch Tensor custom_backend_name wrap_tensor_to __name__ = custom_backend_name setattr torch Tensor custom_backend_name wrap_tensor_to _generate_module_methods_for_privateuse _backend custom_backend_name str - None Generate Module attributes methods depends Tensor methods so we need check whether Tensor methods already registered hasattr torch Tensor custom_backend_name raise RuntimeError f Can automatically generate custom_backend_name method torch nn Module f Because torch Tensor doesn t has method custom_backend_name f For error you can try setting for_tensor=True wrap_module_to torch nn modules module T device Optional Union int torch device = None - torch nn modules module T r Move all model parameters buffers custom device This also makes associated parameters buffers different objects So should called before constructing optimizer module will live device while being optimized note This method modifies module in-place Args device int optional specified all parameters will copied device pyrefly ignore missing-attribute _apply lambda t getattr t custom_backend_name device _check_register_once torch nn Module custom_backend_name setattr torch nn Module custom_backend_name wrap_module_to _generate_packed_sequence_methods_for_privateuse _backend custom_backend_name str - None Generate PackedSequence Module attributes methods depends Tensor methods so we need check whether Tensor methods already registered hasattr torch Tensor f is_ custom_backend_name hasattr torch Tensor custom_backend_name raise RuntimeError f Can automatically generate is_ custom_backend_name f custom_backend_name method torch nn utils rnn PackedSequence f Because torch Tensor doesn t has method is_ custom_backend_name f custom_backend_name f For error you can try setting for_tensor=True property type ignore misc wrap_tensor_backend torch nn utils rnn PackedSequence - bool data device type == custom_backend_name _check_register_once torch nn utils rnn PackedSequence f is_ custom_backend_name setattr torch nn utils rnn PackedSequence f is_ custom_backend_name wrap_tensor_backend wrap_module_to torch nn utils rnn PackedSequence args kwargs - torch nn utils rnn PackedSequence r Move all model parameters buffers custom device This also makes associated parameters buffers different objects So should called before constructing optimizer module will live device while being optimized note This method modifies module in-place Args device int optional specified all parameters will copied device ex = torch tensor dtype=self data dtype device=self data device pyrefly ignore not-iterable args kwargs ex device type == custom_backend_name pyrefly ignore not-iterable args kwargs kwargs update device custom_backend_name pyrefly ignore not-iterable args kwargs _check_register_once torch nn utils rnn PackedSequence custom_backend_name setattr torch nn utils rnn PackedSequence custom_backend_name wrap_module_to _generate_storage_methods_for_privateuse _backend custom_backend_name str unsupported_dtype Optional list torch dtype = None - None Attribute registered _StorageBase UntypedStorage obtains through inheritance property type ignore misc wrap_storage_backend torch storage _StorageBase - bool r Return internal ` torch UntypedStorage ` device type == custom_backend_name _check_register_once torch storage _StorageBase f is_ custom_backend_name setattr torch storage _StorageBase f is_ custom_backend_name wrap_storage_backend wrap_storage_to device=None non_blocking=False r Return copy object custom device memory If object already device memory correct device then no copy performed original object returned Args device int The destination device id Defaults current device non_blocking bool If ` ` True ` ` source pinned memory copy will asynchronous respect host Otherwise argument has no effect There should judgment related storage device judgment related storage type depends extended function so part temporarily omitted automatic generation device_idx = _normalization_device custom_backend_name device getattr f is_ custom_backend_name storage has already expected device get_device == device_idx For sparse storage custom need extend implementation themselves is_sparse raise RuntimeError f Can support sparse storage move custom_backend_name backend create untyped_storage copy data untyped_storage = torch UntypedStorage size device=torch device f custom_backend_name device_idx untyped_storage copy_ non_blocking untyped_storage _check_register_once torch storage _StorageBase custom_backend_name setattr torch storage _StorageBase custom_backend_name wrap_storage_to Register corresponding attribute TypedStorage When TypedStorage removed registration also removed property type ignore misc wrap_typed_storage_backend torch storage TypedStorage - bool torch storage _warn_typed_storage_removal _untyped_storage device type == custom_backend_name _check_register_once torch TypedStorage f is_ custom_backend_name setattr torch storage TypedStorage f is_ custom_backend_name wrap_typed_storage_backend wrap_typed_storage_to torch storage TypedStorage device=None non_blocking=False kwargs - torch storage TypedStorage torch storage _warn_typed_storage_removal unsupported_dtype dtype unsupported_dtype raise RuntimeError f Cannot create custom_backend_name storage f dtype dtype supported backend custom_backend_storage torch UntypedStorage = getattr _untyped_storage custom_backend_name device non_blocking kwargs _new_wrapped_storage custom_backend_storage _check_register_once torch TypedStorage custom_backend_name setattr torch TypedStorage custom_backend_name wrap_typed_storage_to generate_methods_for_privateuse _backend for_tensor bool = True for_module bool = True for_packed_sequence bool = True for_storage bool = False unsupported_dtype Optional list torch dtype = None - None r Automatically generate attributes methods custom backend after rename privateuse backend In default scenario storage-related methods will generated automatically When you implement kernels various torch operations register them PrivateUse dispatch key And call function torch rename_privateuse _backend foo rename your backend name At point you can easily register specific methods attributes calling function Just like torch Tensor foo torch Tensor is_foo torch Storage foo torch Storage is_foo Note We recommend you use generic functions check devices equal device= We provide these methods convenience only they will monkey patched onto objects so will properly typed For Storage methods generate you need support sparse data storage you need extend implementation yourself Args for_tensor bool whether register related methods torch Tensor for_module bool whether register related methods torch nn Module for_storage bool whether register related methods torch Storage unsupported_dtype List torch dtype takes effect only when storage method needs generated indicating storage does support torch dtype type Example xdoctest +SKIP failing torch utils rename_privateuse _backend foo torch utils generate_methods_for_privateuse _backend Then automatically generate backend-related attributes methods = torch tensor foo is_foo hasattr torch nn Module foo custom_backend_name = _get_privateuse _backend_name for_tensor _generate_tensor_methods_for_privateuse _backend custom_backend_name for_module _generate_module_methods_for_privateuse _backend custom_backend_name for_storage _generate_storage_methods_for_privateuse _backend custom_backend_name unsupported_dtype for_packed_sequence _generate_packed_sequence_methods_for_privateuse _backend custom_backend_name _get_custom_mod_func func_name str r Return func named ` func_name ` defined custom device module If defined ` None ` And func registered ` torch utils rename_privateuse _backend foo ` ` torch _register_device_module foo BackendModule ` If custom device module func defined will give warning error message Args func_name str callable func named func_name defined custom device module Example DummyfooModule staticmethod is_available True staticmethod func_name args kwargs torch utils rename_privateuse _backend foo torch _register_device_module foo DummyfooModule foo_is_available_func = torch utils backend_registration _get_custom_mod_func is_available foo_is_available_func foo_is_available = foo_is_available_func func_ = torch utils backend_registration _get_custom_mod_func func_name func_ result = func_ args kwargs Attention This function meant used directly users which why marked private It convenience function backend implementers more easily call hooks into their backend extensions isinstance func_name str raise AssertionError f func_name must ` str ` got ` type func_name ` backend_name = _get_privateuse _backend_name custom_device_mod = getattr torch backend_name None function = getattr custom_device_mod func_name None custom_device_mod None function None message = f Try call torch backend_name func_name The backend must register custom backend message += f module ` torch _register_device_module backend_name BackendModule ` And message += f BackendModule needs have following API s \n ` func_name args kwargs ` \n raise RuntimeError message function _DummyBackendModule is_initialized True is_available True current_device _is_in_bad_fork False manual_seed_all seed int pass device_count _DummyPrivateUse Hook torch _C _acc PrivateUse Hooks is_available True has_primary_context dev_id True is_built True _DummyDeviceGuard torch _C _acc DeviceGuard type_ torch _C _autograd DeviceType PrivateUse _setup_privateuseone_for_python_backend rename=None backend_module=None hook=None device_guard=None This function will prepare PrivateUse dispatch key used python backend WARNING API experimental might change without notice Formally registers things Pytorch expects registered backend C++ have including device guards hooks backend modules what after call one can use ` torch library ` write Ops dispatch key expect behave like backend registered C++ See unit test test test_privateuseone_python_backend py more details Args rename str &#124; None passed we will rename privateuseone backend name given backend_module object &#124; None passed None we will use DummyBackendModule hook object &#124; None passed None we will use DummyPrivateUse Hook device_guard object &#124; None passed None we will use DummyDeviceGuard NOTE ordering which these functions called important rename None torch utils rename_privateuse _backend rename rename = privateuseone torch utils generate_methods_for_privateuse _backend backend_module None backend_module = _DummyBackendModule hook None hook = _DummyPrivateUse Hook device_guard None device_guard = _DummyDeviceGuard torch _register_device_module rename backend_module torch _C _acc register_python_privateuseone_hook hook torch _C _acc register_python_privateuseone_device_guard device_guard