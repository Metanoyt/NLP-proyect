Copyright c Meta Platforms Inc affiliates string typing cast Optional torch torch distributed tensor _dtensor_spec DTensorSpec TensorMeta torch distributed tensor _op_schema OpSchema OutputSharding torch distributed tensor _ops utils prod torch distributed tensor _utils compute_local_shape_and_global_offset _replace_char_in_str string str new_char str idx int - str string idx + new_char + string idx + _gen_reshard_suggestions op_schema OpSchema input_dims list str input_specs tuple DTensorSpec dim_to_sharding dict str int pending_sum list int - OutputSharding suggested_arg_specs list DTensorSpec = input_dim input_spec zip input_dims input_specs dim_map = dim_to_sharding dim dim input_dim suggested_arg_specs append DTensorSpec from_dim_map mesh=input_spec mesh dim_map=dim_map sums=pending_sum tensor_meta=input_spec tensor_meta suggested_schema = OpSchema op_schema op tuple suggested_arg_specs suggested_schema _inplace_rewrap_schema_suggestion op_schema OutputSharding None redistribute_schema=suggested_schema einop_rule equation str op_schema OpSchema linearity bool = False enforce_sharding Optional dict str int = None - OutputSharding Propagate sharding inputs output ops whose data moves according einsum notation This mostly borrowed zdevito s sharding simulator Examples mk kn- mn - einsum ij ij- ij - addition ij j- ij - broadcasted addition ij- i - reduction Other ops could use propagation algorithm when applied note einsum propagation only deal list specs DTensor specs only works list tensors linearity einop_rule means calling op ` f ` follows rule f + b = f + f b In case we can propagate partial sum note linearity einop only applies partial sum other operations like min max which associative linear parse einop equation extract arg specs inputs outputs = equation split - input_dims output_dims = inputs split outputs split input_specs = op_schema args_spec NOTE only support single output unless needed future output_dim = output_dims dim_to_sharding dict str int = dim_to_size dict str int = record pending sum key mesh dimension value pending sum counter across input specs pending_sums_counter dict int int = seen_shardings dict int str = needs_reshard = False merge_sharding dim str int b int - int merge sharding inputs s able merge i e we can merge replicate shard shard will trigger reshard operation = b == - b == - reshard replicate match sharded one nonlocal needs_reshard needs_reshard = True = - b TODO further merge sharding properly i e reshard one input replicate raise RuntimeError f equation dim dim sharded two different ways b input_dim input_spec zip input_dims input_specs deal partial sums input_sums = input_spec sums sum_dim input_sums sum_dim pending_sums_counter seen_shardings sum_dim = + update pending sum counter pending sum mesh dimension occurrence each input pending_sums_counter sum_dim = pending_sums_counter get sum_dim + idx dim mesh_dim enumerate zip input_dim input_spec dim_map enforce_sharding dim enforce_sharding enforce_sharding dim = mesh_dim needs_reshard = True dim_to_sharding dim = enforce_sharding dim dim_to_size dim = input_spec shape idx dim dim_to_sharding dim_to_sharding dim = mesh_dim dim_to_size dim = input_spec shape idx dim_to_sharding dim = merge_sharding dim dim_to_sharding dim mesh_dim assert dim_to_size dim == input_spec shape idx after merging sharding we check there re multiple sharding same mesh dim merged_sharding_for_dim = dim_to_sharding dim merged_sharding_for_dim = - merged_sharding_for_dim seen_shardings dim = seen_shardings merged_sharding_for_dim needs_reshard = True seen_shardings merged_sharding_for_dim += dim seen_shardings merged_sharding_for_dim = dim pending_sums_counter linearity reshard suggestion no pending sum because we already properly merge sharding reshard suggestion legit use _gen_reshard_suggestions op_schema input_dims input_specs dim_to_sharding It s op support linearity all input arguments partial we fail sharding propagation suggestion make all inputs partial corresponding mesh dim all inputs should partial mesh dims order execute locally delay sum reduction value pending_sums_counter values value = len input_specs needs_reshard = True mesh_dim dims seen_shardings items len dims we found different input dims being sharded same mesh dim order perform local op computation we need reshard inputs base some simple heuristics now we simply pick one least comm volume i e input least size TODO consider more advanced heuristic pick best sharding costs = d dims cost = input_dim input_spec zip input_dims input_specs d input_dim input_spec dim_map input_dim index d == mesh_dim assert input_spec tensor_meta None global_shape = input_spec tensor_meta shape local_shape _ = compute_local_shape_and_global_offset global_shape input_spec mesh input_spec placements cost += prod local_shape input_spec mesh size mesh_dim pyrefly ignore bad-argument-type costs append cost d_to_keep_sharding = dims costs index max costs d dims update dim_to_sharding keep sharding dim highest comm make rest dims replicate d = d_to_keep_sharding dim_to_sharding d = - pending_sums = list pending_sums_counter keys needs_reshard _gen_reshard_suggestions op_schema input_dims input_specs dim_to_sharding pending_sums generate output pending sum dim sharded appears input output dim shard_on_mesh dim_to_sharding items dim output_dims shard_on_mesh = - pending_sums append shard_on_mesh no need reshard we directly generate output sharding output_dim_map = output_shape = dim output_dim dim == find output dim singleton dimension mark sharding shape output_dim_map append - output_shape append output_dim_map append dim_to_sharding dim output_shape append dim_to_size dim XXX since we still need have intermediate shape calculation we need pass shape here We should remove once sharding decomp works ops like addmm assert input_specs tensor_meta None tensor_meta = TensorMeta torch Size output_shape input_specs tensor_meta stride input_specs tensor_meta dtype OutputSharding DTensorSpec from_dim_map input_specs mesh output_dim_map pending_sums tensor_meta=tensor_meta pointwise_rule op_schema OpSchema linearity bool = False - OutputSharding Propagate sharding pointwise operations Examples ij ij- ij - addition mul ij j- ij - broadcasted addition alphabet = string ascii_lowercase find max_dim first case we need broadcasting input_specs = op_schema args_spec max_dim = max input ndim input input_specs dimchars = singleton_counter list int = max_dim input input_specs start_dim = max_dim - input ndim p = alphabet start_dim max_dim handle broadcasting common shape case see https pytorch org docs stable notes broadcasting html If any dimensions singleton dimension i e we mark dim char special distinguish non-singleton dimension so sharding propagation should just ignore singleton dimension len input_specs i range max_dim i start_dim treat leading miss dim chars singleton singleton_counter i += input shape i - start_dim == mark singleton dim char special einop rule singleton_counter i += p = _replace_char_in_str p i - start_dim dimchars append p out_dimchars = alphabet max_dim check we replace all inputs dim char singleton dimension we replace all inputs we also need replace output dimension output_dim_idx range len out_dimchars singleton_counter output_dim_idx == len input_specs out_dimchars = _replace_char_in_str out_dimchars output_dim_idx fmt = f join p p dimchars - out_dimchars enforce_sharding dict str int = op_schema is_inplace_op follow_spec = op_schema args_spec enforce_sharding update zip out_dimchars follow_spec dim_map op_schema is_out_variant_op follow_spec = cast DTensorSpec op_schema kwargs_schema out enforce_sharding update zip out_dimchars follow_spec dim_map einop_rule fmt op_schema linearity=linearity enforce_sharding=enforce_sharding