Owner s module distributions io numbers Number pytest torch torch autograd grad torch autograd functional jacobian torch distributions constraints Dirichlet Independent Normal TransformedDistribution torch distributions transforms _InverseTransform AbsTransform AffineTransform ComposeTransform CorrCholeskyTransform CumulativeDistributionTransform ExpTransform identity_transform IndependentTransform LowerCholeskyTransform PositiveDefiniteTransform PowerTransform ReshapeTransform SigmoidTransform SoftmaxTransform SoftplusTransform StickBreakingTransform TanhTransform Transform torch distributions utils tril_matrix_to_vec vec_to_tril_matrix torch testing _internal common_utils run_tests get_transforms cache_size transforms = AbsTransform cache_size=cache_size ExpTransform cache_size=cache_size PowerTransform exponent= cache_size=cache_size PowerTransform exponent=- cache_size=cache_size PowerTransform exponent=torch tensor normal_ cache_size=cache_size PowerTransform exponent=torch tensor normal_ cache_size=cache_size SigmoidTransform cache_size=cache_size TanhTransform cache_size=cache_size AffineTransform cache_size=cache_size AffineTransform - cache_size=cache_size AffineTransform torch randn torch randn cache_size=cache_size AffineTransform torch randn torch randn cache_size=cache_size SoftmaxTransform cache_size=cache_size SoftplusTransform cache_size=cache_size StickBreakingTransform cache_size=cache_size LowerCholeskyTransform cache_size=cache_size CorrCholeskyTransform cache_size=cache_size PositiveDefiniteTransform cache_size=cache_size ComposeTransform AffineTransform torch randn torch randn cache_size=cache_size ComposeTransform AffineTransform torch randn torch randn cache_size=cache_size ExpTransform cache_size=cache_size ComposeTransform AffineTransform cache_size=cache_size AffineTransform torch randn torch randn cache_size=cache_size AffineTransform - cache_size=cache_size AffineTransform torch randn torch randn cache_size=cache_size ReshapeTransform IndependentTransform AffineTransform torch randn torch randn cache_size=cache_size CumulativeDistributionTransform Normal transforms += t inv t transforms transforms reshape_transform transform shape Needed squash batch dims testing jacobian isinstance transform AffineTransform isinstance transform loc Number transform try AffineTransform transform loc expand shape transform scale expand shape cache_size=transform _cache_size except RuntimeError AffineTransform transform loc reshape shape transform scale reshape shape cache_size=transform _cache_size isinstance transform ComposeTransform reshaped_parts = p transform parts reshaped_parts append reshape_transform p shape ComposeTransform reshaped_parts cache_size=transform _cache_size isinstance transform inv AffineTransform reshape_transform transform inv shape inv isinstance transform inv ComposeTransform reshape_transform transform inv shape inv transform Generate pytest ids transform_id x assert isinstance x Transform name = f Inv type x _inv __name__ isinstance x _InverseTransform f type x __name__ f name cache_size= x _cache_size generate_data transform torch manual_seed while isinstance transform IndependentTransform transform = transform base_transform isinstance transform ReshapeTransform torch randn transform in_shape isinstance transform inv ReshapeTransform torch randn transform inv out_shape domain = transform domain while isinstance domain constraints independent domain constraints real_vector domain = domain base_constraint codomain = transform codomain x = torch empty positive_definite_constraints = constraints lower_cholesky constraints positive_definite domain positive_definite_constraints x = torch randn x = x tril - + x diag exp diag_embed domain constraints positive_definite x x T x codomain positive_definite_constraints torch randn domain constraints real x normal_ domain constraints real_vector For corr_cholesky last dim vector must size dim dim x = torch empty x = x normal_ x domain constraints positive x normal_ exp domain constraints unit_interval x uniform_ isinstance domain constraints interval x = x uniform_ x = x mul_ domain upper_bound - domain lower_bound add_ domain lower_bound x domain constraints simplex x = x normal_ exp x = x sum - True x domain constraints corr_cholesky x = torch empty x = x normal_ tril x = x norm dim=- keepdim=True x diagonal dim =- copy_ x diagonal dim =- abs x raise ValueError f Unsupported domain domain TRANSFORMS_CACHE_ACTIVE = get_transforms cache_size= TRANSFORMS_CACHE_INACTIVE = get_transforms cache_size= ALL_TRANSFORMS = TRANSFORMS_CACHE_ACTIVE + TRANSFORMS_CACHE_INACTIVE + identity_transform pytest mark parametrize transform ALL_TRANSFORMS ids=transform_id test_inv_inv transform ids=transform_id assert transform inv inv transform pytest mark parametrize x TRANSFORMS_CACHE_INACTIVE ids=transform_id pytest mark parametrize y TRANSFORMS_CACHE_INACTIVE ids=transform_id test_equality x y x y assert x == y assert x = y assert identity_transform == identity_transform inv pytest mark parametrize transform ALL_TRANSFORMS ids=transform_id test_with_cache transform transform _cache_size == transform = transform with_cache assert transform _cache_size == x = generate_data transform requires_grad_ try y = transform x except NotImplementedError pytest skip Not implemented y = transform x assert y y pytest mark parametrize transform ALL_TRANSFORMS ids=transform_id pytest mark parametrize test_cached True False test_forward_inverse transform test_cached x = generate_data transform requires_grad_ assert transform domain check x all verify input data valid try y = transform x except NotImplementedError pytest skip Not implemented assert y shape == transform forward_shape x shape test_cached x = transform inv y should implemented least caching try x = transform inv y clone bypass cache except NotImplementedError pytest skip Not implemented assert x shape == transform inverse_shape y shape y = transform x transform bijective verify function inverse assert torch allclose x x atol= e- equal_nan=True \n join f transform t inv t - error f x = x f y = t x = y f x = t inv y = x verify weaker function pseudo-inverse assert torch allclose y y atol= e- equal_nan=True \n join f transform t t inv t - error f x = x f y = t x = y f x = t inv y = x f y = t x = y test_compose_transform_shapes transform = ExpTransform transform = SoftmaxTransform transform = LowerCholeskyTransform assert transform event_dim == assert transform event_dim == assert transform event_dim == assert ComposeTransform transform transform event_dim == assert ComposeTransform transform transform event_dim == assert ComposeTransform transform transform event_dim == transform = ExpTransform transform = SoftmaxTransform transform = LowerCholeskyTransform base_dist = Normal torch zeros torch ones base_dist = Dirichlet torch ones base_dist = Normal torch zeros torch ones pytest mark parametrize batch_shape event_shape dist base_dist base_dist TransformedDistribution base_dist transform TransformedDistribution base_dist transform TransformedDistribution base_dist transform transform TransformedDistribution base_dist transform transform TransformedDistribution base_dist transform transform TransformedDistribution base_dist transform transform TransformedDistribution base_dist transform transform TransformedDistribution base_dist transform transform TransformedDistribution base_dist transform TransformedDistribution base_dist transform TransformedDistribution base_dist transform TransformedDistribution base_dist transform transform TransformedDistribution base_dist transform transform TransformedDistribution base_dist transform transform TransformedDistribution base_dist transform transform TransformedDistribution base_dist transform transform TransformedDistribution base_dist transform transform base_dist TransformedDistribution base_dist transform TransformedDistribution base_dist transform transform TransformedDistribution base_dist transform transform TransformedDistribution base_dist transform transform TransformedDistribution base_dist transform transform test_transformed_distribution_shapes batch_shape event_shape dist assert dist batch_shape == batch_shape assert dist event_shape == event_shape x = dist rsample try dist log_prob x should crash except NotImplementedError pytest skip Not implemented pytest mark parametrize transform TRANSFORMS_CACHE_INACTIVE ids=transform_id test_jit_fwd transform x = generate_data transform requires_grad_ f x transform x try traced_f = torch jit trace f x except NotImplementedError pytest skip Not implemented check different inputs x = generate_data transform requires_grad_ assert torch allclose f x traced_f x atol= e- equal_nan=True pytest mark parametrize transform TRANSFORMS_CACHE_INACTIVE ids=transform_id test_jit_inv transform y = generate_data transform inv requires_grad_ f y transform inv y try traced_f = torch jit trace f y except NotImplementedError pytest skip Not implemented check different inputs y = generate_data transform inv requires_grad_ assert torch allclose f y traced_f y atol= e- equal_nan=True pytest mark parametrize transform TRANSFORMS_CACHE_INACTIVE ids=transform_id test_jit_jacobian transform x = generate_data transform requires_grad_ f x y = transform x transform log_abs_det_jacobian x y try traced_f = torch jit trace f x except NotImplementedError pytest skip Not implemented check different inputs x = generate_data transform requires_grad_ assert torch allclose f x traced_f x atol= e- equal_nan=True pytest mark parametrize transform ALL_TRANSFORMS ids=transform_id test_jacobian transform x = generate_data transform try y = transform x actual = transform log_abs_det_jacobian x y except NotImplementedError pytest skip Not implemented Test shape target_shape = x shape x dim - transform domain event_dim assert actual shape == target_shape Expand required transform = reshape_transform transform x shape ndims = len x shape event_dim = ndims - transform domain event_dim x_ = x view - + x shape event_dim n = x_ shape Reshape squash batch dims single batch dim transform = reshape_transform transform x_ shape Transforms unit jacobian isinstance transform ReshapeTransform isinstance transform inv ReshapeTransform expected = x new_zeros x shape x dim - transform domain event_dim expected = x new_zeros x shape x dim - transform domain event_dim Transforms off-diagonal elements transform domain event_dim == jac = jacobian transform x_ assert off-diagonal elements zero assert torch allclose jac jac diagonal diag_embed expected = jac diagonal abs log reshape x shape Transforms non- off-diagonal elements isinstance transform CorrCholeskyTransform jac = jacobian lambda x tril_matrix_to_vec transform x diag=- x_ isinstance transform inv CorrCholeskyTransform jac = jacobian lambda x transform vec_to_tril_matrix x diag=- tril_matrix_to_vec x_ diag=- isinstance transform StickBreakingTransform jac = jacobian lambda x transform x - x_ jac = jacobian transform x_ Note jacobian will have shape batch_dims y_event_dims batch_dims x_event_dims However batches independent so can converted into batch_dims event_dims event_dims after reshaping event dims see above give batched square matrix whose determinant can computed gather_idx_shape = list jac shape gather_idx_shape - = gather_idxs = torch arange n reshape n + len jac shape - expand gather_idx_shape jac = jac gather - gather_idxs squeeze - out_ndims = jac shape - jac = jac out_ndims Remove extra zero-valued dims inverse stick-breaking expected = torch slogdet jac logabsdet assert torch allclose actual expected atol= e- pytest mark parametrize event_dims ids=str test_compose_affine event_dims transforms = AffineTransform torch zeros e event_dim=e e event_dims transform = ComposeTransform transforms assert transform codomain event_dim == max event_dims assert transform domain event_dim == max event_dims base_dist = Normal transform domain event_dim base_dist = base_dist expand transform domain event_dim dist = TransformedDistribution base_dist transform parts assert dist support event_dim == max event_dims base_dist = Dirichlet torch ones transform domain event_dim base_dist = base_dist expand transform domain event_dim - dist = TransformedDistribution base_dist transforms assert dist support event_dim == max event_dims pytest mark parametrize batch_shape ids=str test_compose_reshape batch_shape transforms = ReshapeTransform ReshapeTransform ReshapeTransform ReshapeTransform transform = ComposeTransform transforms assert transform codomain event_dim == assert transform domain event_dim == data = torch randn batch_shape + assert transform data shape == batch_shape + dist = TransformedDistribution Normal data transforms assert dist batch_shape == batch_shape assert dist event_shape == assert dist support event_dim == pytest mark parametrize sample_shape ids=str pytest mark parametrize transform_dim pytest mark parametrize base_batch_dim pytest mark parametrize base_event_dim pytest mark parametrize num_transforms test_transformed_distribution base_batch_dim base_event_dim transform_dim num_transforms sample_shape shape = torch Size base_dist = Normal base_dist = base_dist expand shape - base_batch_dim - base_event_dim base_event_dim base_dist = Independent base_dist base_event_dim transforms = AffineTransform torch zeros shape - transform_dim ReshapeTransform ReshapeTransform transforms = transforms num_transforms transform = ComposeTransform transforms Check validation __init__ base_batch_dim + base_event_dim transform domain event_dim pytest raises ValueError TransformedDistribution base_dist transforms d = TransformedDistribution base_dist transforms Check sampling sufficiently expanded x = d sample sample_shape assert x shape == sample_shape + d batch_shape + d event_shape num_unique = len set x reshape - tolist assert num_unique = x numel Check log_prob shape full samples log_prob = d log_prob x assert log_prob shape == sample_shape + d batch_shape Check log_prob shape partial samples y = x while y dim len d event_shape y = y log_prob = d log_prob y assert log_prob shape == d batch_shape test_save_load_transform Evaluating ` log_prob ` will create weakref ` _inv ` which cannot pickled Here we check ` __getstate__ ` correctly handles weakref we can evaluate density after dist = TransformedDistribution Normal AffineTransform x = torch linspace log_prob = dist log_prob x stream = io BytesIO torch save dist stream stream seek torch serialization safe_globals TransformedDistribution AffineTransform Normal other = torch load stream assert torch allclose log_prob other log_prob x pytest mark parametrize transform ALL_TRANSFORMS ids=transform_id test_transform_sign transform Transform try sign = transform sign except NotImplementedError pytest skip Not implemented x = generate_data transform requires_grad_ y = transform x sum derivatives = grad y x assert torch less torch as_tensor derivatives sign all __name__ == __main__ run_tests