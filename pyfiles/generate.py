dataclasses itertools platform time typing Optional torchao common Experiment register_experiment mixtral_moe_model ConditionalFeedForward Transformer MixtralMoE mixtral_moe_quantize ConditionalFeedForwardInt WeightOnlyInt QuantHandler MixtralMoEWeightOnlyInt QuantHandler model Transformer LLaMA quantize WeightOnlyInt QuantHandler LLaMAWeightOnlyInt QuantHandler torch torch _inductor config torch _inductor config coordinate_descent_tuning = True torch _inductor config triton unique_kernel_names = True torch _inductor config fx_graph_cache = True Experimental feature reduce compilation times will default future torch _inductor config assert_indirect_indexing = False compiled = False dataclasses dataclass GPTModelConfig name str module type mode Optional str quantizer type token_per_sec float memory_bandwidth float compilation_time float batch_size Optional int = None device_sync device cuda device torch cuda synchronize device cpu device pass print f device= device yet supported get_arch_name - str torch cuda is_available torch cuda get_device_name This returns x _ arm aarch platform machine multinomial_sample_one_no_sync probs_sort Does multinomial sampling without cuda synchronization q = torch empty_like probs_sort exponential_ torch argmax probs_sort q dim=- keepdim=True dtype=torch int logits_to_probs logits temperature float = top_k Optional int = None logits = logits max temperature e- top_k None v _ = torch topk logits min top_k logits size - pivot = v select - - unsqueeze - logits = torch where logits pivot -float Inf logits probs = torch nn functional softmax logits dim=- probs sample logits temperature float = top_k Optional int = None probs = logits_to_probs logits - temperature top_k idx_next = multinomial_sample_one_no_sync probs idx_next probs prefill model torch nn Module x torch Tensor input_pos torch Tensor sampling_kwargs - torch Tensor input_pos B S logits = model x input_pos sample logits sampling_kwargs decode_one_token model torch nn Module x torch Tensor input_pos torch Tensor sampling_kwargs - tuple torch Tensor torch Tensor input_pos B assert input_pos shape - == logits = model x input_pos sample logits sampling_kwargs decode_n_tokens model torch nn Module cur_token torch Tensor input_pos torch Tensor num_new_tokens int sampling_kwargs new_tokens new_probs = i range num_new_tokens torch nn attention sdpa_kernel torch nn attention SDPBackend MATH Actually better Inductor codegen attention here next_token next_prob = decode_one_token model cur_token input_pos sampling_kwargs input_pos += new_tokens append next_token clone new_probs append next_prob clone cur_token = next_token view - new_tokens new_probs torch no_grad generate model torch nn Module prompt torch Tensor max_new_tokens int sampling_kwargs - torch Tensor device dtype = prompt device prompt dtype T = prompt size T_new = T + max_new_tokens max_seq_length = min T_new model config block_size torch device device model setup_caches max_batch_size= max_seq_length=max_seq_length create empty tensor expected final shape fill current tokens empty = torch empty T_new dtype=dtype device=device empty T = prompt seq = empty input_pos = torch arange T device=device next_token = prefill model prompt view - input_pos sampling_kwargs seq T = next_token input_pos = torch tensor T device=device dtype=torch int generated_tokens _ = decode_n_tokens model next_token view - input_pos max_new_tokens - sampling_kwargs seq T + = torch cat generated_tokens seq _load_model x GPTModelConfig device= cuda precision=torch bfloat torch device meta model = x module from_name x name model = model dtype=precision x mode == int print Using int weight-only quantization model = x quantizer model convert_for_runtime state_dict = model state_dict k v state_dict items state_dict k = torch nn Parameter torch randn v shape device=device dtype=v dtype requires_grad=v requires_grad model load_state_dict state_dict assign=True model eval Only count activated parameters buffers _get_model_size model model_size = name child model named_children isinstance child torch nn Embedding model_size += sum p numel p dtype itemsize p itertools chain child parameters child buffers Remove inactivated experts model size mixture experts architecture since only activated experts loaded hasattr model config num_experts config = model config submodule model modules isinstance submodule ConditionalFeedForward ConditionalFeedForwardInt model_size -= sum p numel p dtype itemsize p itertools chain submodule parameters child buffers config num_experts - config num_activated_experts config num_experts model_size run_experiment x GPTModelConfig num_samples int = max_new_tokens int = top_k int = temperature float = device str = cuda - None print f Loading model x name t = time time model = _load_model x device=device device_sync device=device MKG print f Time load model time time - t f seconds prompt = torch tensor device=device dtype=torch int prompt_length = prompt size torch manual_seed model_size = _get_model_size model aggregate_metrics = tokens_per_sec memory_bandwidth start = - compilation_time = None x mode == autoquant print Using autoquant model = torchao autoquant model manual=True error_on_unseen=False generate model prompt max_new_tokens temperature=temperature top_k=top_k model finalize_autoquant x mode == autoquant_v print Using autoquant_v torchao prototype quantization autoquant_v autoquant_v p = prompt view - T = prompt size T_new = T + max_new_tokens max_seq_length = min T_new model config block_size input_pos = torch arange T device=device example_input = p input_pos torch device device model setup_caches max_batch_size= max_seq_length=max_seq_length model = autoquant_v model manual=True error_on_unseen=False example_input=example_input batch_size=x batch_size torch compiler cudagraph_mark_step_begin generate model prompt max_new_tokens temperature=temperature top_k=top_k model finalize_autoquant global decode_one_token prefill compiled compiled compiled = True decode_one_token = torch compile decode_one_token mode= reduce-overhead fullgraph=True prefill = torch compile prefill fullgraph=True i range start num_samples device_sync device=device MKG torch compiler cudagraph_mark_step_begin t = time perf_counter y = generate model prompt max_new_tokens temperature=temperature top_k=top_k i == - compilation_time = time perf_counter - t print f Compilation time compilation_time f seconds continue device_sync device=device MKG t = time perf_counter - t tokens_generated = y size - prompt_length tokens_sec = tokens_generated t aggregate_metrics tokens_per_sec append tokens_sec aggregate_metrics memory_bandwidth append model_size tokens_sec e token_per_sec = torch mean torch tensor aggregate_metrics tokens_per_sec item memory_bandwidth = torch mean torch tensor aggregate_metrics memory_bandwidth item print f Average tokens sec token_per_sec f tokens sec print f Average bandwidth achieved memory_bandwidth f GB s print f Memory used torch cuda max_memory_reserved e f GB token_per_sec memory_bandwidth compilation_time token_per_sec memory_bandwidth target numbers A - GB which different typical A - GB register_experiment name= llama _ b_bf run_llama _ b_bf device str = cuda model = GPTModelConfig Llama- - b-chat-hf LLaMA bfloat LLaMAWeightOnlyInt QuantHandler token_per_sec memory_bandwidth compilation_time = run_experiment model device=device Experiment model name token_per_sec model token_per_sec f token_per_sec f model mode device get_arch_name True Experiment model name memory_bandwidth GB s model memory_bandwidth f memory_bandwidth f model mode device get_arch_name True Experiment model name compilation_time s model compilation_time f compilation_time f model mode device get_arch_name True token_per_sec memory_bandwidth target numbers A - GB which different typical A - GB register_experiment name= llama _ b_int run_llama _ b_int device str = cuda model = GPTModelConfig Llama- - b-chat-hf LLaMA int LLaMAWeightOnlyInt QuantHandler token_per_sec memory_bandwidth compilation_time = run_experiment model device=device Experiment model name token_per_sec model token_per_sec f token_per_sec f model mode device get_arch_name True Experiment model name memory_bandwidth GB s model memory_bandwidth f memory_bandwidth f model mode device get_arch_name True Experiment model name compilation_time s model compilation_time f compilation_time f model mode device get_arch_name True token_per_sec memory_bandwidth target numbers A - GB which different typical A - GB register_experiment name= mixtral_ x b_int run_mixtral_ x b_int device str = cuda We reduced original number layers adapt CI memory limitation model = GPTModelConfig Mixtral- x B-v MixtralMoE int MixtralMoEWeightOnlyInt QuantHandler token_per_sec memory_bandwidth compilation_time = run_experiment model device=device Experiment model name token_per_sec model token_per_sec f token_per_sec f model mode device get_arch_name True Experiment model name memory_bandwidth GB s model memory_bandwidth f memory_bandwidth f model mode device get_arch_name True Experiment model name compilation_time s model compilation_time f compilation_time f model mode device get_arch_name True token_per_sec memory_bandwidth target numbers A - GB which different typical A - GB run_llama _ b_autoquant device str = cuda model = GPTModelConfig Llama- - b-chat-hf LLaMA autoquant None token_per_sec memory_bandwidth compilation_time = run_experiment model device=device Experiment model name token_per_sec model token_per_sec f token_per_sec f model mode device get_arch_name True Experiment model name memory_bandwidth GB s model memory_bandwidth f memory_bandwidth f model mode device get_arch_name True Experiment model name compilation_time s model compilation_time f compilation_time f model mode device get_arch_name True token_per_sec memory_bandwidth target numbers A - GB which different typical A - GB run_mixtral_ x b_autoquant device str = cuda We reduced original number layers adapt CI memory limitation model = GPTModelConfig Mixtral- x B-v MixtralMoE autoquant None token_per_sec memory_bandwidth compilation_time = run_experiment model device=device Experiment model name token_per_sec model token_per_sec f token_per_sec f model mode device get_arch_name True Experiment model name memory_bandwidth GB s model memory_bandwidth f memory_bandwidth f model mode device get_arch_name True Experiment model name compilation_time s model compilation_time f compilation_time f model mode device get_arch_name True token_per_sec memory_bandwidth target numbers A - GB which different typical A - GB run_llama _ b_autoquant_v device str = cuda model = GPTModelConfig Llama- - b-chat-hf LLaMA autoquant_v None batch_size token_per_sec memory_bandwidth compilation_time = run_experiment model device=device Experiment model name token_per_sec model token_per_sec f token_per_sec f model mode device get_arch_name True Experiment model name memory_bandwidth GB s model memory_bandwidth f memory_bandwidth f model mode device get_arch_name True Experiment model name compilation_time s model compilation_time f compilation_time f model mode device get_arch_name True token_per_sec memory_bandwidth target numbers A - GB which different typical A - GB run_mixtral_ x b_autoquant_v device str = cuda We reduced original number layers adapt CI memory limitation model = GPTModelConfig Mixtral- x B-v MixtralMoE autoquant_v None batch_size token_per_sec memory_bandwidth compilation_time = run_experiment model device=device Experiment model name token_per_sec model token_per_sec f token_per_sec f model mode device get_arch_name True Experiment model name memory_bandwidth GB s model memory_bandwidth f memory_bandwidth f model mode device get_arch_name True Experiment model name compilation_time s model compilation_time f compilation_time f model mode device get_arch_name True