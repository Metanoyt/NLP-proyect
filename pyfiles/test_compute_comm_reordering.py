Owner s module inductor unittest unittest mock patch torch torch _dynamo torch _dynamo logging torch _dynamo test_case some reason importing functional collectives after dynamo breaks collectives handling torch distributed _functional_collectives _functional_collectives torch _C FileCheck torch _dynamo utils same torch _inductor ir scheduler torch _inductor comm_analysis baseLat hwLat llMaxBws NCCL_ALGO NCCL_HW NCCL_PROTO NVIDIA_GPU_TYPE torch _inductor utils run_and_get_triton_code torch testing _internal common_distributed _dynamo_dist_per_rank_init at_least_x_gpu DynamoDistributedMultiProcTestCase requires_accelerator_dist_backend torch testing _internal common_fsdp get_devtype torch testing _internal inductor_utils HAS_GPU device_type = str get_devtype get_snode_runtime_for_reorder_compute_test snode NOTE custom cost model show compute reordering algorithm working Collective kernels isinstance snode node ir _CollectiveKernel isinstance snode node ir _WaitKernel High-arithmetic-intensity compute kernels isinstance snode node ir ExternKernel All other kernels create_grouped_node_for_allreduce_and_its_deps snodes name_to_snode = snode node name snode snode snodes all_reduce_snodes = snode snode snodes isinstance snode node ir _CollectiveKernel snode node op_overload == torch ops _c d_functional all_reduce_ default assert len all_reduce_snodes == all_reduce_snode = all_reduce_snodes all_reduce_dep_snodes = name_to_snode node name node all_reduce_snode node inputs assert len all_reduce_dep_snodes == all_reduce_dep_snode = all_reduce_dep_snodes grouped_snode = scheduler GroupedSchedulerNode create all_reduce_dep_snode all_reduce_snode new_snode_order = new_snode_order append grouped_snode snode snodes snode grouped_snode snodes continue new_snode_order append snode new_snode_order requires_accelerator_dist_backend unittest skipIf torch _inductor config triton native_matmul native matmul fused surrounding ops TestComputeCommReorderingMultiProc DynamoDistributedMultiProcTestCase Run correctness checks multi-proc runner mark minimum GPUs run under get_world_trs tag ranks list range world_size group_size world_size property world_size - int hack no matter whether we have gpus just run works around issue skipif workers unpredictable #s gpu unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch patch object torch _inductor config allow_buffer_reuse True TODO somehow inductor bg compile threads causing hangs exit distributed work dtor patch object torch _inductor config compile_threads patch object torch _inductor config reorder_for_locality False patch object torch _inductor config reorder_for_compute_comm_overlap True patch object torch _inductor config reorder_for_compute_comm_overlap_passes sink_waits test_sink_waits func ar = _functional_collectives all_reduce sum b = torch matmul torch matmul ar b _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs = torch ones dtype=torch float device=device_type + rank compiled = torch compile func code = run_and_get_triton_code compiled inputs Verify wait_tensor sinked below st matmul above nd matmul FileCheck check torch ops _c d_functional all_reduce_ default check extern_kernels mm check torch ops _c d_functional wait_tensor default check extern_kernels mm run code out = compiled inputs correct = func inputs assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch patch object torch _inductor config allow_buffer_reuse True TODO somehow inductor bg compile threads causing hangs exit distributed work dtor patch object torch _inductor config compile_threads patch object torch _inductor config reorder_for_locality False patch object torch _inductor config reorder_for_compute_comm_overlap True patch object torch _inductor config reorder_for_compute_comm_overlap_passes raise_comms test_raise_comms func b = torch matmul c = torch relu b d = torch matmul c c e = _functional_collectives all_reduce b sum torch matmul d e _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs = torch ones dtype=torch float device=device_type + rank compiled = torch compile func code = run_and_get_triton_code compiled inputs Verify all_reduce_ has been raised above nd matmul below st matmul Note all_reduce_ directly writes output buffer st matmul which input first relu Therefore all_reduce_ should scheduled after first relu FileCheck check extern_kernels mm check triton_poi_fused_relu check torch ops _c d_functional all_reduce_ default check_same buf mm use buf prior wait_tensor check extern_kernels mm check_not buf check torch ops _c d_functional wait_tensor default check extern_kernels mm run code out = compiled inputs correct = func inputs assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch patch object torch _inductor config allow_buffer_reuse True TODO somehow inductor bg compile threads causing hangs exit distributed work dtor patch object torch _inductor config compile_threads patch object torch _inductor config reorder_for_compute_comm_overlap True patch object torch _inductor config reorder_for_compute_comm_overlap_passes sink_waits raise_comms test_sink_waits_raise_comms func tag ranks group_size b = torch matmul c = torch relu b d = torch matmul c c e = _functional_collectives all_reduce b sum f = torch relu d g = torch matmul f f torch mm e g _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs = torch ones dtype=torch float device=device_type + rank compiled = torch compile func code = run_and_get_triton_code compiled inputs get_world_trs Things verify - The clone prologue all_reduce_ should fused any relus - The all_reduce_ its prologue should raised above nd matmul below st matmul - The wait_tensor should sinked below rd matmul above th matmul FileCheck check extern_kernels mm check triton_poi_fused_all_reduce_ check torch ops _c d_functional all_reduce_ default check triton_poi_fused_relu check extern_kernels mm check triton_poi_fused_relu check extern_kernels mm check torch ops _c d_functional wait_tensor default check extern_kernels mm run code out = compiled inputs get_world_trs correct = func inputs get_world_trs assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch patch object torch _inductor config allow_buffer_reuse True TODO somehow inductor bg compile threads causing hangs exit distributed work dtor patch object torch _inductor config compile_threads patch object torch _inductor config reorder_for_compute_comm_overlap True patch object torch _inductor config reorder_for_compute_comm_overlap_passes reorder_compute_for_overlap patch object torch _inductor config runtime_estimations_mms_benchmark False test_reorder_compute_for_overlap func tag ranks group_size ar = _functional_collectives all_reduce sum ranks tag g = torch matmul c = torch relu d = torch matmul c c f = d c ar fr = _functional_collectives all_reduce f sum ranks tag e = torch matmul d + ar + fr g e _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs = torch ones dtype=torch float device=device_type + rank compiled = torch compile func code = run_and_get_triton_code compiled inputs get_world_trs NOTE after scheduling first all_reduce we first schedule ops c d ARE required second all_reduce DO NOT depend first all_reduce then we schedule ops g ARE NOT required second all_reduce DO NOT depend first all_reduce then we schedule ops f ARE required second all_reduce DO depend first all_reduce then we schedule second all_reduce And then schedule all ops depend second all_reduce FileCheck check torch ops _c d_functional all_reduce_ default check triton_poi_fused_relu check extern_kernels mm check extern_kernels mm check torch ops _c d_functional wait_tensor default check triton_poi_fused_all_reduce_mul check torch ops _c d_functional all_reduce_ default check torch ops _c d_functional wait_tensor default check triton_poi_fused_add check extern_kernels mm run code out = compiled inputs get_world_trs correct = func inputs get_world_trs assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch patch object torch _inductor config allow_buffer_reuse True TODO somehow inductor bg compile threads causing hangs exit distributed work dtor patch object torch _inductor config compile_threads patch object torch _inductor config reorder_for_compute_comm_overlap True patch object torch _inductor config reorder_for_compute_comm_overlap_passes reorder_compute_for_overlap patch object torch _inductor config estimate_op_runtime get_snode_runtime_for_reorder_compute_test test_reorder_compute_for_overlap_custom_runtime_estimation func tag ranks group_size ar = _functional_collectives all_reduce sum ranks tag g = torch matmul c = torch relu d = torch matmul c c f = d c ar fr = _functional_collectives all_reduce f sum ranks tag e = torch matmul d + ar + fr g e _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs = torch ones dtype=torch float device=device_type + rank compiled = torch compile func code = run_and_get_triton_code compiled inputs get_world_trs NOTE after scheduling first all_reduce we first schedule ops c d ARE required second all_reduce DO NOT depend first all_reduce then we schedule ops g ARE NOT required second all_reduce DO NOT depend first all_reduce then we schedule ops f ARE required second all_reduce DO depend first all_reduce then we schedule second all_reduce And then schedule all ops depend second all_reduce FileCheck check torch ops _c d_functional all_reduce_ default check triton_poi_fused_relu check extern_kernels mm check extern_kernels mm check torch ops _c d_functional wait_tensor default check triton_poi_fused_all_reduce_mul check torch ops _c d_functional all_reduce_ default check torch ops _c d_functional wait_tensor default check triton_poi_fused_add check extern_kernels mm run code out = compiled inputs get_world_trs correct = func inputs get_world_trs assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch unittest skipIf torch _inductor config triton native_matmul native matmul fused surrounding ops TODO somehow inductor bg compile threads causing hangs exit distributed work dtor patch object torch _inductor config compile_threads patch object torch _inductor config _pre_fusion_custom_pass create_grouped_node_for_allreduce_and_its_deps test_grouped_scheduler_node func tag ranks group_size add = + div = add ar = _functional_collectives all_reduce div sum ranks tag Normally we would fuse ` add = + ` ` div = add ` ` mul = ` together into single fused op here unit test we intentionally put ` add ` ` div ` ` ar ` computation into GroupedSchedulerNode which prevents them being fused any other ops mul = mm = torch matmul mul ar mm _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=not at_least_x_gpu inputs = torch ones dtype=torch float device=device_type + rank compiled = torch compile func code = run_and_get_triton_code compiled inputs get_world_trs Expectations ` add = + ` ` div = add ` still fused which means fusion still happens among nodes within GroupedSchedulerNode ` mul = ` fused ` add ` ` div ` because latter two within GroupedSchedulerNode thus prevented being fused any outside ops FileCheck check triton_poi_fused_add_all_reduce_div_ check _c d_functional all_reduce_ check triton_poi_fused_mul_ run code out = compiled inputs get_world_trs correct = func inputs get_world_trs assertTrue same out correct unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch torch _inductor config patch force_disable_caches=True test_inductor_default_comms_ordering pg_info = get_world_trs tag = pg_info tag ranks = pg_info ranks group_size = pg_info group_size g = torch ones device=device_type g = torch ones device=device_type g = torch ones device=device_type assert_pass graph all_reduces need remain order assertExpectedInline graph \ graph arg _ num_users= = placeholder target=arg _ arg _ num_users= = placeholder target=arg _ arg _ num_users= = placeholder target=arg _ all_reduce num_users= = call_function target=torch ops _c d_functional all_reduce default args = arg _ avg kwargs = all_reduce_ num_users= = call_function target=torch ops _c d_functional all_reduce default args = arg _ avg kwargs = all_reduce_ num_users= = call_function target=torch ops _c d_functional all_reduce default args = arg _ avg kwargs = wait_tensor num_users= = call_function target=torch ops _c d_functional wait_tensor default args = all_reduce_ kwargs = wait_tensor_ num_users= = call_function target=torch ops _c d_functional wait_tensor default args = all_reduce_ kwargs = wait_tensor_ num_users= = call_function target=torch ops _c d_functional wait_tensor default args = all_reduce kwargs = wait_tensor wait_tensor_ wait_tensor_ noqa B torch _inductor config post_grad_custom_post_pass = assert_pass torch compile fn g g g handle = torch ops c d_functional all_reduce g avg tag ranks group_size handle = torch ops c d_functional all_reduce g avg tag ranks group_size handle = torch ops c d_functional all_reduce g avg tag ranks group_size wait them different order grad = torch ops _c d_functional wait_tensor default handle grad = torch ops _c d_functional wait_tensor default handle grad = torch ops _c d_functional wait_tensor default handle grad grad grad _dynamo_dist_per_rank_init rank world_size backend device_type fake_pg=True fn g g g test_nccl_heuristics assert len baseLat == len NCCL_ALGO assert all len x == len NCCL_PROTO x baseLat assert len hwLat == len NCCL_HW assert all len x == len NCCL_ALGO x hwLat assert all len y == len NCCL_PROTO x hwLat y x assert len llMaxBws == len NVIDIA_GPU_TYPE __name__ == __main__ torch _dynamo test_case run_tests run_tests