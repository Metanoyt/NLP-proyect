mypy allow-untyped-defs itertools warnings typing_extensions deprecated torch torch cuda torch autograd _disable_profiler_legacy _enable_profiler_legacy DeviceType ProfilerConfig ProfilerState torch autograd profiler_util _filter_name _filter_stack_entry _rewrite_name EventList FunctionEvent MEMORY_EVENT_NAME __all__ = profile deprecated ` torch autograd profiler_legacy profile ` deprecated will removed future release Please use ` torch profiler ` instead category=None TODO change ` FutureWarning ` profile DEPRECATED use torch profiler instead __init__ enabled=True use_cuda=False record_shapes=False with_flops=False profile_memory=False with_stack=False with_modules=False enabled bool = enabled enabled use_cuda = use_cuda function_events = None entered = False record_shapes = record_shapes with_flops = with_flops record_shapes &#124; = with_flops profile_memory = profile_memory with_stack = with_stack with_modules = with_modules use_cuda torch cuda is_available warnings warn CUDA available disabling CUDA profiling stacklevel= use_cuda = False use_cuda profiler_kind = ProfilerState CUDA profiler_kind = ProfilerState CPU config ProfilerConfig profiler_kind record_shapes profile_memory with_stack with_flops with_modules avoid exposing _ExperimentalConfig legacy public API torch _C _profiler _ExperimentalConfig __enter__ enabled entered raise RuntimeError Profiler context manager reentrant entered = True _start_trace _start_trace _enable_profiler_legacy config __exit__ exc_type exc_val exc_tb enabled use_cuda torch cuda synchronize records = _disable_profiler_legacy parsed_results = _parse_legacy_records records pyrefly ignore bad-assignment function_events = EventList parsed_results use_device= cuda use_cuda None profile_memory=self profile_memory with_flops=self with_flops pyrefly ignore missing-attribute function_events _build_tree False __repr__ function_events None unfinished profiler_legacy profile repr function_events __str__ function_events None unfinished profile profiler_legacy profile str function_events _check_finish function_events None raise RuntimeError Profiler didn t finish running table sort_by=None row_limit= max_src_column_width= max_name_column_width= max_shapes_column_width= header=None top_level_events_only=False _check_finish function_events None raise AssertionError Expected profiling results function_events table sort_by=sort_by row_limit=row_limit max_src_column_width=max_src_column_width max_name_column_width=max_name_column_width max_shapes_column_width=max_shapes_column_width header=header top_level_events_only=top_level_events_only table __doc__ = EventList table __doc__ export_chrome_trace path _check_finish function_events None raise AssertionError Expected profiling results function_events export_chrome_trace path export_chrome_trace __doc__ = EventList export_chrome_trace __doc__ export_stacks path str metric str = self_cpu_time_total _check_finish function_events None raise AssertionError Expected profiling results with_stack raise AssertionError export_stacks requires with_stack=True function_events export_stacks path metric key_averages group_by_input_shape=False group_by_stack_n= _check_finish function_events None raise AssertionError Expected profiling results function_events key_averages group_by_input_shape group_by_stack_n key_averages __doc__ = EventList key_averages __doc__ total_average _check_finish function_events None raise AssertionError Expected profiling results function_events total_average total_average __doc__ = EventList total_average __doc__ property self_cpu_time_total Return CPU time sum times across all events _check_finish function_events None raise AssertionError Expected profiling results function_events self_cpu_time_total _parse_legacy_records thread_records _get_record_key record Return tuple correlating start end records ` _parse_legacy_records ` record handle record node_id start_record = None functions = __start_profile guaranteed first so we must find here record itertools chain from_iterable thread_records name = record name start_record None name == __start_profile start_record = record start_record None start_record is_remote raise AssertionError Expected valid local start_record thread_record_list thread_records accumulated memory allocations per handle cpu_memory_allocs = cuda_memory_allocs = ranges per handle range_starts = filtered_handles = set prev_record = None record thread_record_list record_key = _get_record_key record _filter_name record name record_key filtered_handles filtered_handles add record_key continue record kind == push workaround reduce double logging operator wrappers redispatch prev_record None duplicate = prev_record name == record name prev_record kind == record kind prev_record node_id == record node_id duplicate filtered_handles add record_key continue range_starts record_key = record cpu_memory_allocs record_key = cuda_memory_allocs record_key = record kind == pop record_key range_starts raise AssertionError f Expected record key record_key exist range_starts This means pop event did have corresponding push start = range_starts record_key cpu_memory_usage = cpu_memory_allocs record_key cuda_memory_usage = cuda_memory_allocs record_key is_async = start is_async start thread_id = record thread_id is_remote_event = record is_remote start_flops = start flops fe = FunctionEvent id=record handle node_id=record node_id name=_rewrite_name name=start name with_wildcard=True trace_name=_rewrite_name name=start name with_wildcard=False thread=start thread_id start_us=start_record cpu_elapsed_us start end_us=start_record cpu_elapsed_us record fwd_thread=start fwd_thread_id input_shapes=start shapes stack= entry entry start stack _filter_stack_entry entry scope=start scope use_device= cuda start has_cuda None cpu_memory_usage=cpu_memory_usage device_memory_usage=cuda_memory_usage is_async=is_async is_remote=is_remote_event sequence_nr=start sequence_nr device_type=DeviceType CPU is_legacy=True flops=start_flops note async events have only cpu total time is_async start has_cuda duration = start cuda_elapsed_us record duration fe append_kernel start name start device duration functions append fe del range_starts record_key del cpu_memory_allocs record_key del cuda_memory_allocs record_key record kind == memory_alloc num_open_handles_cpu = len cpu_memory_allocs num_open_handles_cuda = len cuda_memory_allocs num_open_handles_cpu = num_open_handles_cuda raise AssertionError f Expected CPU CUDA memory allocation handles match f got num_open_handles_cpu CPU num_open_handles_cuda CUDA handle cpu_memory_allocs keys cpu_memory_allocs handle += record cpu_memory_usage handle cuda_memory_allocs keys cuda_memory_allocs handle += record cuda_memory_usage num_open_handles_cpu == output event top-level memory event fe = FunctionEvent id= name=MEMORY_EVENT_NAME trace_name=None thread= start_us= end_us= stack= cpu_memory_usage=record cpu_memory_usage device_memory_usage=record cuda_memory_usage is_legacy=True functions append fe prev_record = record Sort functions start time then end time ascending This ensures -- case nested events which have same start time which may happen due granularity given clock tick -- we always show outermost nested call first This adds stability how FunctionEvents appear functions sort key=lambda evt evt time_range start -evt time_range end functions