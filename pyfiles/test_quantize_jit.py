Owner s oncall quantization ruff noqa F torch io itertools unittest torch torch jit torch jit quantized torch nn nn torch nn functional F torch ao quantization torch ao quantization default_dynamic_qconfig default_histogram_observer default_observer default_per_channel_weight_observer default_qconfig default_weight_observer float _dynamic_qconfig fuse_modules get_default_qconfig per_channel_dynamic_qconfig PlaceholderObserver QConfig quantize quantize_dynamic quantize_dynamic_jit quantize_jit torch ao quantization quantize_jit torch ao quantization quantize_jit convert_dynamic_jit convert_jit fuse_conv_bn_jit prepare_dynamic_jit prepare_jit script_qconfig torch jit _recursive wrap_cpp_module torch testing FileCheck Annotated models torch testing _internal common_quantization AnnotatedConvBnModel AnnotatedConvModel AnnotatedConvTransposeModel AnnotatedNestedModel AnnotatedSingleLayerLinearModel AnnotatedSkipQuantModel ConvBnModel ConvModel ConvTransposeModel default_per_channel_qconfig get_script_module NestedModel QuantizationTestCase SingleLayerLinearModel skipIfNoFBGEMM SkipQuantModel test_only_eval_fn Testing utils torch testing _internal common_quantized override_qengines qengine_is_fbgemm qengine_is_qnnpack torch testing _internal common_utils raise_on_run_directly set_default_dtype torch testing _internal jit_utils attrs_with_prefix get_forward get_forward_graph Standard library TestQuantizeJitPasses QuantizationTestCase Test graph mode quantization passes used quantize_jit test_skip_dequant_constant_prop M torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x conv x m = torch jit script M observer = default_per_channel_weight_observer with_args ch_axis= qconfig_dict = QConfig activation=default_observer weight=observer m = prepare_jit m qconfig_dict data = torch randn dtype=torch float m data m = convert_jit m debug=True freezed = torch jit freeze m freezed data After freezing weight becomes Constant We have pattern original graph Constant f _weight - quant - dequant After skipping dequant during Constant Propagation resulting graph will Constant int _weight - dequant FileCheck check_count aten quantize_per_tensor exactly=True run freezed graph FileCheck check_count aten quantize_per_channel exactly=True run freezed graph FileCheck check_count aten dequantize exactly=True run freezed graph FileCheck check aten quantize_per_tensor check_next aten dequantize check_not aten quantize_per_channel check aten dequantize check_next aten conv d check_next aten quantize_per_tensor check_next aten dequantize run freezed graph test_foldbn_trivial bn_module = torch nn BatchNorm d torch nn BatchNorm d conv_module = torch nn Conv d torch nn Conv d Test trivial case TestModule torch nn Module __init__ dim super __init__ conv = conv_module dim bn = bn_module dim num_features= bn eps = forward x x = conv x x = bn x x options = itertools product True False data = torch rand torch rand Check transformation doesn t change numerics tracing dim options eager = TestModule dim eval x = data dim scripted_or_traced = get_script_module eager tracing x eval Check original script module s forward we have two CallMethod nodes One them should conv forward other bn forward FileCheck check_count prim CallMethod name= forward exactly=True run str get_forward scripted_or_traced _c graph Run FoldConvBatchnorm pass scripted_or_traced = fuse_conv_bn_jit scripted_or_traced Check after pass one CallMethods gone supposedly bn forward FileCheck check_count prim CallMethod name= forward exactly=True run str get_forward_graph scripted_or_traced _c Check transformation doesn t change numerics assertEqual eager x scripted_or_traced x test_foldbn_trivial_nobias bn_module = torch nn BatchNorm d torch nn BatchNorm d conv_module = torch nn Conv d torch nn Conv d Test trivial case TestModule torch nn Module __init__ dim super __init__ conv = conv_module dim bias=False bn = bn_module dim num_features= make sure new bias zero bn eps = bn bias = torch nn Parameter torch rand forward x x = conv x x = bn x x options = itertools product True False data = torch rand torch rand tracing dim options eager = TestModule dim eval x = data dim scripted_or_traced = get_script_module eager tracing x eval Check original script module s forward we have two CallMethod nodes One them should conv forward other bn forward FileCheck check_count prim CallMethod name= forward exactly=True run str get_forward_graph scripted_or_traced _c Run FoldConvBatchnorm pass scripted_or_traced = fuse_conv_bn_jit scripted_or_traced Check after pass one CallMethods gone supposedly bn forward FileCheck check_count prim CallMethod name= forward exactly=True run str get_forward_graph scripted_or_traced _c Check transformation doesn t change numerics assertEqual eager x scripted_or_traced x test_foldbn_in_submodule bn_module = torch nn BatchNorm d torch nn BatchNorm d conv_module = torch nn Conv d torch nn Conv d Test we find Conv-BN patterns submodules SubModule torch nn Module __init__ dim super __init__ conv = conv_module dim bn = bn_module dim num_features= forward x x = conv x x = bn x x TestModule torch nn Module __init__ dim super __init__ sub = SubModule dim forward x x = sub x x options = itertools product True False data = torch rand torch rand tracing dim options eager = TestModule dim eval x = data dim scripted_or_traced = get_script_module eager tracing x eval FileCheck check_count prim CallMethod name= forward exactly=True run str get_forward_graph scripted_or_traced sub _c scripted_or_traced = fuse_conv_bn_jit scripted_or_traced FileCheck check_count prim CallMethod name= forward exactly=True run str get_forward_graph scripted_or_traced sub _c assertEqual eager x scripted_or_traced x test_foldbn_shared_classtype bn_module = torch nn BatchNorm d torch nn BatchNorm d conv_module = torch nn Conv d torch nn Conv d TestModule torch nn Module __init__ dim bias=False super __init__ conv = conv_module dim bias=bias bn = bn_module dim num_features= bn running_mean fill_ - bn bias = torch nn Parameter torch rand make sure new bias zero bn eps = conv = conv_module dim bias=bias bn = bn_module dim num_features= bn eps = relu = torch nn ReLU forward x x = conv x x = bn x x = relu x x = conv x x = bn x x = relu x x options = itertools product True False True False data = torch rand torch rand tracing dim bias options eager = TestModule dim bias eval x = data dim scripted_or_traced = get_script_module eager tracing x folded = fuse_conv_bn_jit scripted_or_traced assertEqual eager x scripted_or_traced x test_foldbn_no_fusion Test we don t fuse cases when module type does match CustomConv torch nn Module forward x x CustomBn torch nn Module forward x x M torch nn Module __init__ - None super __init__ conv = CustomConv bn = CustomBn forward x bn conv x m = torch jit script M m = fuse_conv_bn_jit m FileCheck check_count prim CallMethod exactly=True run m graph set_default_dtype torch double test_foldbn_complex_cases This test case attempt try combinations conv d conv d bias nobias well BatchNorm affine no-affine along varying number layers only works when default dtype double bn_module = torch nn BatchNorm d torch nn BatchNorm d conv_module = torch nn Conv d torch nn Conv d SubModule torch nn Module __init__ dim num_blocks enable_bias enable_affine super __init__ layers = _ range num_blocks layers append conv_module dim bias=enable_bias bn_obj = bn_module dim num_features= affine=enable_affine enable_affine bn_obj weight = torch nn Parameter torch rand_like bn_obj weight bn_obj bias = torch nn Parameter torch rand_like bn_obj bias bn_obj running_mean = torch rand_like bn_obj running_mean bn_obj running_var = torch rand_like bn_obj running_var layers append bn_obj layers = nn Sequential layers forward x layers x TestModule torch nn Module __init__ dim num_blocks enable_bias enable_affine super __init__ sub = SubModule dim num_blocks enable_bias enable_affine forward x x = sub x x options = itertools product True False True False True False data = torch rand torch rand tracing dim enable_bias enable_bn_affine num_layers options eager = TestModule dim num_layers enable_bias enable_bn_affine eval x = data dim scripted_or_traced = get_script_module eager tracing x eval FileCheck check_count prim CallMethod name= forward num_layers exactly=True run str get_forward_graph scripted_or_traced sub layers _c scripted_or_traced = fuse_conv_bn_jit scripted_or_traced FileCheck check_count prim CallMethod name= forward num_layers exactly=True run str get_forward_graph scripted_or_traced sub layers _c assertEqual eager x scripted_or_traced x test_fuse_linear FunctionalLinear torch nn Module __init__ weight bias super __init__ weight = weight bias = bias forward x res = torch matmul x weight t bias None res add_ bias res x = torch rand w = torch rand b = torch rand x = torch rand w = torch rand b = torch rand x = torch rand w = torch rand b = torch rand has_bias x weight b itertools product True False x w b x w b x w b bias = b has_bias None model = torch jit trace FunctionalLinear weight bias x node model graph nodes node kind == aten matmul source_range_ = node sourceRange torch _C _jit_pass_fuse_linear model graph node model graph nodes node kind == aten linear source_range_ = node sourceRange FileCheck check aten linear run model graph check_not = aten matmul aten addmm aten add_ aten t cn check_not FileCheck check_not cn run model graph make sure runs assertTrue source_range_ == source_range_ model x check matmuls fused Matmul torch nn Module __init__ weight super __init__ weight = weight forward x torch matmul x weight x = torch rand w = torch rand model = torch jit trace Matmul w x torch _C _jit_pass_fuse_linear model graph check d matmul fused FileCheck check aten matmul run model graph FileCheck check_not aten linear run model graph make sure runs model x test_insert_observers M torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x conv x m = torch jit script M qconfig_dict = default_qconfig m = prepare_jit m qconfig_dict input output conv assert len attrs_with_prefix m _observer_ == weight assert len attrs_with_prefix m conv _observer_ == test_insert_observers_interface torch jit interface SubInterface torch nn Module addOne inp - torch Tensor pass Sub torch nn Module __init__ - None super __init__ fc = torch nn Linear addOne inp fc inp + forward x addOne x M torch nn Module __init__ - None super __init__ conv = torch nn Conv d sub = Sub forward x sub conv x m = torch jit script M qconfig_dict = sub conv default_qconfig m = prepare_jit m qconfig_dict test_insert_observers_interface_unshare_type torch jit interface OperatorIf nn Module forward inp torch Tensor - torch Tensor pass Operator nn Module __init__ super __init__ = forward inp torch Tensor - torch Tensor inp + Inner nn Module op OperatorIf __init__ op super __init__ op = op forward inp op inp Outer nn Module __init__ - None super __init__ inner_a = Inner Operator inner_b = Inner Operator forward inp inner_a inp + inner_b inp qconfig_dict = inner_a default_qconfig inner_b default_qconfig eager_model = Outer tracing True False x = torch rand script_model = get_script_module eager_model tracing x make sure runs prepare_jit script_model qconfig_dict test_insert_observers_child_qconfig Sub torch nn Module __init__ - None super __init__ fc = torch nn Linear forward x fc x M torch nn Module __init__ - None super __init__ conv = torch nn Conv d sub = Sub forward x sub conv x m = torch jit script M qconfig_dict = sub fc default_qconfig m = prepare_jit m qconfig_dict input output sub assert len attrs_with_prefix m _observer_ == quantized assert len attrs_with_prefix m conv _observer_ == no observers since we observe outer most call site assert len attrs_with_prefix m sub _observer_ == weight linear assert len attrs_with_prefix m sub fc _observer_ == unittest skipUnless fbgemm torch backends quantized supported_engines Quantized operations require FBGEMM FBGEMM only optimized CPUs instruction set support avx newer test_insert_observers_skip_values ConvFunctionalReLU torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x F relu conv x ConvReLUModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d relu = torch nn ReLU forward x relu conv x AddReLUModule torch nn Module __init__ - None super __init__ relu = torch nn ReLU conv = torch nn Conv d float forward x out = conv x out += x relu out AddFunctionalReLU torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x out = conv x out += x F relu out attrs_with_prefix module prefix x x _ module _modules _c items x startswith prefix qconfig_dict = default_qconfig m = torch jit script ConvFunctionalReLU m = prepare_jit m qconfig_dict observer weight conv assert len attrs_with_prefix m conv _observer_ == observer input conv output relu assert len attrs_with_prefix m _observer_ == m = torch jit script ConvReLUModule m = prepare_jit m qconfig_dict observer input conv output relu assert len attrs_with_prefix m _observer_ == observer weight conv assert len attrs_with_prefix m conv _observer_ == observer output relu assert len attrs_with_prefix m relu _observer_ == m = torch jit script AddReLUModule qconfig_dict = default_qconfig m = prepare_jit m qconfig_dict assert len attrs_with_prefix m _observer == assert len attrs_with_prefix m relu _observer == FileCheck check aten add_ check_not Observer = prim GetAttr name= _observer_ check ReLU = prim GetAttr run str get_forward_graph m _c m = torch jit script AddFunctionalReLU qconfig_dict = default_qconfig m = prepare_jit m qconfig_dict assert len attrs_with_prefix m _observer == FileCheck check aten add_ check_not Observer = prim GetAttr name= _observer_ check CallFunction check Observer = prim GetAttr name= _observer_ run str get_forward_graph m _c test_insert_observers_weight_dtype M torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x F relu conv x m = torch jit script M qconfig_dict = default_qconfig m = prepare_jit m qconfig_dict activation_dtypes = obs getattr dtype x obs m _modules _c items x startswith _observer_ weight_dtypes = obs getattr dtype x obs m conv _modules _c items x startswith _observer_ assert len activation_dtypes == Expected have activation dtype assert len weight_dtypes == Expected have weight dtype assert next iter activation_dtypes = next iter weight_dtypes Expected activation dtype different wegiht dtype test_insert_observers_for_reused_weight M torch nn Module forward x y weight x = F conv d x weight y = F conv d y weight x + y m = torch jit script M eval m = prepare_jit m default_qconfig x y weight one output each F conv d one output add assert len attrs_with_prefix m _observer == test_insert_observers_shared_class_type M torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x conv conv x m = torch jit script M qconfig_dict = default_qconfig m = prepare_jit m qconfig_dict conv conv shares same type we need make sure we didn t quantize type twice conv _observers = attrs_with_prefix m conv _observer_ conv _observers = attrs_with_prefix m conv _observer_ assert len conv _observers == Expected have observer submodules assert len conv _observers == Expected have observer submodules assert conv _observers == conv _observers Expect conv conv have same observers since type shared test_insert_observers_for_general_ops Make sure we skip observers ops doesn t require observation e g flatten M torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x x = conv x x = torch flatten x x m = torch jit script M qconfig_dict = default_qconfig m = prepare_jit m qconfig_dict input output conv assert len attrs_with_prefix m _observer_ == FileCheck check Observer = prim GetAttr name= _observer_ check prim GetAttr name= conv check prim CallMethod check Observer = prim GetAttr name= _observer_ check aten flatten check_not Observer = prim GetAttr name= _observer_ run m graph TODO too long split test_insert_observers py remove insrt_observers prefix test_insert_observers_propagate_observed Make sure we propagate observed property through general ops M torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x x = conv x x = torch flatten x we don t want insert observer input conv because output conv already observed x = conv x x m = torch jit script M qconfig_dict = default_qconfig m = prepare_jit m qconfig_dict input output conv assert len attrs_with_prefix m _observer_ == FileCheck check Observer = prim GetAttr name= _observer_ check prim GetAttr name= conv check prim CallMethod check Observer = prim GetAttr name= _observer_ check aten flatten check_not Observer = prim GetAttr name= _observer_ check prim GetAttr name= conv check Observer = prim GetAttr name= _observer_ run m graph test_insert_observers_propagate_observed_in_submodule Make sure we propagate observed property through general ops M torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float avgpool = torch nn AdaptiveAvgPool d forward x x = conv x x = avgpool x we don t want insert observer input conv because output conv already observed x = conv x x m = torch jit script M qconfig_dict = default_qconfig m = prepare_jit m qconfig_dict input output conv assert len attrs_with_prefix m _observer_ == FileCheck check Observer = prim GetAttr name= _observer_ check prim GetAttr name= conv check prim CallMethod check Observer = prim GetAttr name= _observer_ check prim CallMethod check_not Observer = prim GetAttr name= _observer_ check prim GetAttr name= conv check Observer = prim GetAttr name= _observer_ run m graph test_insert_observers_propagate_observed_for_function channel_shuffle x torch Tensor groups int - torch Tensor batchsize num_channels height width = x data size channels_per_group = num_channels groups reshape x = x view batchsize groups channels_per_group height width x = torch transpose x contiguous flatten x = x view batchsize - height width x M torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x x = conv x x = channel_shuffle x x = conv x x data = torch rand dtype=torch float torch randint dtype=torch long _ range m = torch jit script M eval m = prepare_jit m default_qconfig we want test channel_shuffle going pass observed property output conv input conv so we don t insert observers input conv assert len attrs_with_prefix m _observer_ == test_insert_observers_for_if QuantProp torch nn Module __init__ use_skip super __init__ conv = torch nn Conv d float use_skip = use_skip forward x use_skip x = conv x torch reshape x x shape x = conv x torch reshape x x shape Res torch nn Module __init__ use_skip super __init__ conv = torch nn Conv d float use_skip = use_skip forward x use_skip conv x conv x M torch nn Module __init__ - None super __init__ quant_prop = QuantProp True res = Res False forward x x = quant_prop x x = res x x data = torch rand dtype=torch float result = False True tracing True False tracing m = torch jit trace M data eval m = torch jit script M eval m = prepare_jit m default_qconfig assert len attrs_with_prefix m _observer_ == result tracing assert len attrs_with_prefix m quant_prop _observer_ == result tracing assert len attrs_with_prefix m res _observer_ == result tracing test_insert_observers_for_nested_if Res torch nn Module __init__ use_skip super __init__ conv = torch nn Conv d float cond = use_skip use_skip = use_skip forward x use_skip cond conv x conv x conv x M torch nn Module __init__ - None super __init__ res = Res True res = Res False forward x x = res x x = res x x data = torch rand dtype=torch float result = True False tracing True False tracing m = torch jit trace M data eval m = torch jit script M eval m = prepare_jit m default_qconfig assert len attrs_with_prefix m _observer_ == result tracing test_insert_observers_for_if_consistent_observation check quantization works long output all branches quantized observed consistently M torch nn Module __init__ cond super __init__ conv = torch nn Conv d float cond = cond forward x x = conv x x already observed cond x = torch flatten x x M torch nn Module __init__ cond super __init__ conv = torch nn Conv d float conv = torch nn Conv d float cond = cond forward x x = conv x cond x = conv x x will observed branch x = torch flatten x since output both branch quantized node quantized consistently x data = torch rand dtype=torch float options = list itertools product True False True False cond tracing options tracing m = torch jit trace M cond data m = torch jit script M cond m = prepare_jit m default_qconfig assert len attrs_with_prefix m _observer_ == cond tracing options tracing m = torch jit trace M cond data m = torch jit script M cond m = prepare_jit m default_qconfig num_observers = tracing cond assert len attrs_with_prefix m _observer_ == num_observers test_insert_quant_dequant M torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x conv x is_per_channel True False m = torch jit script M observer = default_per_channel_weight_observer with_args ch_axis= is_per_channel default_observer qconfig_dict = QConfig activation=observer weight=observer m = prepare_jit m qconfig_dict data = torch randn dtype=torch float m data m = convert_jit m debug=True assert len m _modules _c items == Expected have single submodule conv make sure quantized model executable m data quant_func = aten quantize_per_channel is_per_channel aten quantize_per_tensor FileCheck check_count quant_func exactly=True run m graph test_insert_quant_dequant_shared_class_type M torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x conv conv x is_per_channel True False m = torch jit script M observer = default_per_channel_weight_observer with_args ch_axis= is_per_channel default_observer qconfig = QConfig activation=observer weight=observer qconfig_dict = qconfig m = prepare_jit m qconfig_dict observers input output value between conv conv assert len attrs_with_prefix m _observer_ == Expected have obervers observer weight assert len attrs_with_prefix m conv _observer_ == Expected have obervers observer weight assert len attrs_with_prefix m conv _observer_ == Expected have obervers data = torch randn dtype=torch float m data m = convert_jit m debug=True m data assert m conv _c _type == m conv _c _type check all observers have been removed assert len attrs_with_prefix m _observer_ == Expected have obervers assert len attrs_with_prefix m conv _observer_ == Expected have obervers assert len attrs_with_prefix m conv _observer_ == Expected have obervers quant_func = aten quantize_per_channel is_per_channel aten quantize_per_tensor module conv conv conv = m _c getattr module quantize weight FileCheck check quant_func check_next aten dequantize check prim CallMethod name= _conv_forward check run get_forward_graph conv no quantize node _conv_forward FileCheck check_not quant_func check aten conv d check_not quant_func check run conv _get_method _conv_forward graph test_dedup_module_uses M torch nn Module __init__ - None super __init__ relu = torch nn ReLU forward x x = relu x x -= relu x data = torch randn m = torch jit script M ref_res = m data assert len x x _ m _modules _c items x startswith relu == Expected have relu modules after dedup module uses torch _C _jit_pass_dedup_module_uses m _c m = torch jit _recursive wrap_cpp_module m _c res = m data assert len x x _ m _modules _c items x startswith relu == Expected have relu modules after dedup module uses assertEqual res ref_res test_replicate_dequantize M torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x x = torch dequantize x r = conv x r += x r x = torch randn dtype=torch float x = torch quantize_per_tensor x torch quint m = torch jit script M ref_res = m x FileCheck check_count aten dequantize exactly=True run m graph torch _C _jit_pass_replicate_dequantize m graph FileCheck check_count aten dequantize exactly=True run m graph res = get_forward m _c x assertEqual res ref_res test_replicate_dequantize_in_block M torch nn Module __init__ cond super __init__ conv = torch nn Conv d float cond = cond forward x x = torch dequantize x cond x = conv x x = x + x x = torch randn dtype=torch float x = torch quantize_per_tensor x torch quint m = torch jit script M True ref_res = m x FileCheck check_count aten dequantize exactly=True run m graph torch _C _jit_pass_replicate_dequantize m graph FileCheck check_count aten dequantize exactly=True run m graph check dequantize right before CallMethod conv FileCheck check aten dequantize check_next CallMethod run m graph check dequantize right before add FileCheck check aten dequantize check aten dequantize check_next aten add run m graph res = get_forward m _c x assertEqual res ref_res test_swap_functional_linear TODO This pass replaces any function called linear aten linear No longer necessary also quite surprising linear input weight bias torch nn functional linear input weight bias M torch nn Module forward x weight bias x = torch dequantize x weight = torch dequantize weight x = linear x weight bias x = torch quantize_per_tensor x scale= zero_point= dtype=torch quint x x = torch rand dtype=torch float x = torch quantize_per_tensor x scale= zero_point= dtype=torch quint weight = torch rand dtype=torch float weight = torch quantize_per_tensor weight scale= zero_point= dtype=torch qint bias = torch rand dtype=torch float m = torch jit script M ref_res = m x weight bias FileCheck check CallFunction run m graph torch _C _jit_pass_swap_functional_linear m graph FileCheck check aten linear check_not CallFunction run m graph res = m x weight bias assertEqual res ref_res test_replicate_quantize_for_if We want move quantize nodes output prim If inside prim If blocks so we can match quantization patterns Res torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float use_skip = True forward x torch Tensor cond bool - torch Tensor avoid being frozen use_skip = cond use_skip conv x conv x M torch nn Module __init__ - None super __init__ res = Res res = Res forward x x = res x True x = res x False x data = torch rand dtype=torch float qconfig_dict = default_qconfig m = torch jit script M eval m = quantize_jit m qconfig_dict test_only_eval_fn data make sure patterns both branches fused FileCheck check_count quantized conv d exactly=True run m graph test_finalize_for_linear M torch nn Module __init__ - None super __init__ fc = torch nn Linear float forward x fc x data = torch rand dtype=torch float qconfig_dict = default_qconfig model = torch jit script M eval model = quantize_jit model qconfig_dict test_only_eval_fn data make sure there only one quantize_per_tensor input linear_prepack folded FileCheck check_count aten quantize_per_tensor exactly=True check_not quantized linear_prepack check quantized linear run model graph test_inplace_option tracing True False model = get_script_module torch nn Conv d float tracing img_data_ d qconfig_dict = default_qconfig quantize_jit model qconfig_dict test_only_eval_fn img_data_ d inplace=True FileCheck check quantized conv d run model graph FileCheck check_not aten conv d run model graph test_finalize_debug M torch nn Module __init__ - None super __init__ conv = torch nn Conv d float avgpool = torch nn AvgPool d forward x x = conv x x = avgpool x x data = torch rand dtype=torch float qconfig_dict = default_qconfig model = torch jit script M eval model = quantize_jit model qconfig_dict test_only_eval_fn data debug=True FileCheck check_not quantized conv d check aten conv d check aten avg_pool d check aten q_scale check_next aten q_zero_point check_next prim dtype check_next aten quantize_per_tensor check aten dequantize run model graph test_module_list SimpleLinearLayer torch nn Module __init__ - None super __init__ fc = torch nn Linear float forward x fc x ComplexModel torch nn Module __init__ - None super __init__ layers = torch nn ModuleList SimpleLinearLayer i range forward x torch Tensor - list torch Tensor states = layer layers val = layer x states append val states data = torch rand dtype=torch float qconfig_dict = default_qconfig model = torch jit script ComplexModel eval model = prepare_jit model qconfig_dict assert len attrs_with_prefix model _observer == model data model = convert_jit model debug=False FileCheck check quantized linear check quantized linear run model graph test_conv_trace M torch nn Module __init__ - None super __init__ conv d = torch nn Conv d float conv d = torch nn Conv d float conv d = torch nn Conv d float forward x y z = conv d x b = conv d y c = conv d z b c qconfig_dict = default_qconfig inputs = torch rand dtype=torch float torch rand dtype=torch float torch rand dtype=torch float model = torch jit trace M inputs eval m = prepare_jit model qconfig_dict FileCheck check aten conv d check_not aten _convolution run str get_forward_graph m conv d _c FileCheck check aten conv d check_not aten _convolution run str get_forward_graph m conv d _c FileCheck check aten conv d check_not aten _convolution run str get_forward_graph m conv d _c test_convtranspose_trace M torch nn Module __init__ - None super __init__ convtranspose d = torch nn ConvTranspose d float convtranspose d = torch nn ConvTranspose d float convtranspose d = torch nn ConvTranspose d float forward x y z = convtranspose d x b = convtranspose d y c = convtranspose d z b c qconfig_dict = default_qconfig inputs = torch rand dtype=torch float torch rand dtype=torch float torch rand dtype=torch float model = torch jit trace M inputs eval m = prepare_jit model qconfig_dict FileCheck check aten conv_transpose d check_not aten _convolution run str get_forward_graph m convtranspose d _c FileCheck check aten conv_transpose d check_not aten _convolution run str get_forward_graph m convtranspose d _c FileCheck check aten conv_transpose d check_not aten _convolution run str get_forward_graph m convtranspose d _c unittest skipUnless fbgemm torch backends quantized supported_engines Quantized operations require FBGEMM FBGEMM only optimized CPUs instruction set support avx newer test_replicate_dequant_same_value Mul torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x x = conv x x x data = torch rand dtype=torch float qconfig_dict = default_qconfig model = torch jit script Mul eval m = quantize_jit model qconfig_dict test_only_eval_fn data FileCheck check quantized mul check_not aten mul run m graph test_interface_with_fork SubModule torch nn Module __init__ - None super __init__ embedding = torch nn EmbeddingBag num_embeddings= embedding_dim= include_last_offset=True sparse=False mode= sum forward x y embedding x y OrigMod torch nn Module __init__ - None super __init__ embedding = torch nn EmbeddingBag num_embeddings= embedding_dim= include_last_offset=True sparse=False mode= sum forward x y embedding x y torch jit interface ModInterface torch nn Module forward x torch Tensor y torch Tensor - torch Tensor pass TestModule torch nn Module proxy_mod ModInterface __init__ - None super __init__ proxy_mod = OrigMod sub = SubModule forward x y = proxy_mod x y b = sub x y b MainModule torch nn Module __init__ - None super __init__ test = TestModule forward x y fut = torch jit _fork test forward x y z = torch jit _wait fut z indices = torch tensor offsets = torch tensor m = torch jit trace MainModule indices offsets m eval int _qconfig = QConfig activation=PlaceholderObserver with_args dtype=torch float custom_op_name= embedding_bag_byte weight=PlaceholderObserver with_args custom_op_name= embedding_bag_byte m = prepare_jit m int _qconfig m = convert_jit m FileCheck check quantized embedding_bag_byte_rowwise_offsets run m graph skipIfNoFBGEMM test_quantize_fork_wait Tests case where fork wait calls different subgraphs Calling inline fork-wait only removes fork call leaves aten wait calls graph Tensor input instead Future Tensor MainModule nn Module __init__ - None super __init__ fork_ops = ForkModule init_values x shared_module = fork_ops x fork_dict = shared_module forward x val = torch jit _wait fork_ops x val TestModule torch nn Module forward x w = torch ones b = torch zeros torch nn functional linear x w b ForkModule nn Module __init__ - None super __init__ test = TestModule forward x fut = torch jit _fork test forward x fut model = MainModule eval traced = torch jit trace model torch randn model = prepare_dynamic_jit traced default_qconfig model = convert_dynamic_jit model FileCheck check quantized linear_dynamic run model graph Make sure model save works b = io BytesIO torch jit save model b TestQuantizeJitOps QuantizationTestCase Test graph mode post training static quantization works individual ops end end skipIfNoFBGEMM test_linear ModuleLinear torch nn Module __init__ has_relu=False f_relu=False super __init__ linear = torch nn Linear float has_relu f_relu relu = F relu relu = torch nn ReLU relu = torch nn Identity forward x relu linear x FuncLinear torch nn Module __init__ has_relu=False f_relu=False super __init__ w = torch randn b = torch randn has_relu f_relu relu = F relu relu = torch nn ReLU relu = torch nn Identity forward x relu F linear x w b data = torch rand dtype=torch float model tracing itertools product ModuleLinear has_relu=False FuncLinear has_relu=False True False model = checkGraphModeOp model data quantized linear tracing FileCheck check_count aten quantize_per_tensor exactly=True run model graph FileCheck check_not quantized linear_prepack run model graph f_relu tracing itertools product True False True False model ModuleLinear has_relu=True f_relu=f_relu FuncLinear has_relu=True f_relu=f_relu model = checkGraphModeOp model data quantized linear_relu tracing checker = FileCheck check_not aten linear check_not aten relu check_not quantized linear check_not quantized relu run model graph skipIfNoFBGEMM test_quantized_conv conv_module = torch nn Conv d torch nn Conv d torch nn Conv d Conv torch nn Module __init__ dim super __init__ conv = conv_module dim float forward x conv x options = itertools product True False dim tracing options model = checkGraphModeOp Conv dim img_data_dict dim f quantized conv dim d tracing make sure there only one quantize_per_tensor input conv d_prepack folded FileCheck check_count aten quantize_per_tensor exactly=True run model graph FileCheck check_not f quantized conv dim d_prepack run model graph skipIfNoFBGEMM test_quantized_conv_relu tests conv d_relu conv d_relu conv d_relu conv_module = torch nn Conv d torch nn Conv d torch nn Conv d ConvNdRelu torch nn Module __init__ dim inplace super __init__ conv = conv_module dim float relu = torch nn ReLU inplace forward x relu conv x ConvNdFunctionalRelu torch nn Module __init__ dim super __init__ conv = conv_module dim float forward x F relu conv x ConvNdInplaceFunctionalRelu torch nn Module __init__ dim super __init__ conv = conv_module dim float forward x F relu conv x True options = itertools product True False dim tracing options orig_m ConvNdRelu dim True ConvNdRelu dim False ConvNdFunctionalRelu dim ConvNdInplaceFunctionalRelu dim conv_name = f conv dim d m = checkGraphModeOp orig_m img_data_dict dim f quantized conv dim d_relu tracing=tracing FileCheck check_not f aten conv dim d check_not aten relu check_not f quantized conv dim d check_not quantized relu run m graph skipIfNoFBGEMM test_quantized_add_alpha Test quant fusion multiple aten add using same constant alpha third argument QuantizedAdd torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y z = x + y w = y + z z + w data = torch randn dtype=torch float torch randn dtype=torch float tracing True False m = checkGraphModeOp QuantizedAdd data quantized add tracing FileCheck check_count quantized add exactly=True run m graph FileCheck check_not aten add check_not aten add_ run m graph skipIfNoFBGEMM test_quantized_add_relu_alpha Test quant fusion multiple aten add using same constant alpha third argument add_relu pattern AddRelu torch nn Module __init__ inplace super __init__ conv = torch nn Conv d float conv = torch nn Conv d float relu = torch nn ReLU inplace forward x y x = conv x y = conv y x = x + y x = relu x x = x + y relu x InplaceAddRelu torch nn Module __init__ inplace super __init__ conv = torch nn Conv d float conv = torch nn Conv d float relu = torch nn ReLU inplace forward x y x = conv x y = conv y x += y x = relu x x += y relu x AddFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x = x + y x = F relu x x = x + y F relu x InplaceAddFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x += y x = F relu x x += y F relu x AddInplaceFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x = x + y x = F relu x True x = x + y F relu x True InplaceAddInplaceFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x += y x = F relu x True x += y F relu x True data = torch rand dtype=torch float torch rand dtype=torch float m_orig AddRelu True AddRelu False InplaceAddRelu True InplaceAddRelu False AddFunctionalRelu InplaceAddFunctionalRelu AddInplaceFunctionalRelu InplaceAddInplaceFunctionalRelu tracing True False m = checkGraphModeOp m_orig data quantized add_relu tracing=tracing FileCheck check_count quantized add_relu exactly=True run m graph FileCheck check_not aten add check_not aten add_ check_not aten relu check_not aten relu_ check_not quantized add check_not quantized relu run m graph skipIfNoFBGEMM test_quantized_add QuantizedAdd torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x + y QuantizedInplaceAdd torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x += y x NonQuantizedAdd torch nn Module forward x y x + y NonQuantizedInplaceAdd torch nn Module forward x y x += y x data = torch randn dtype=torch float torch randn dtype=torch float m quantized QuantizedAdd True QuantizedInplaceAdd True NonQuantizedAdd False NonQuantizedInplaceAdd False tracing True False op = quantized add quantized aten add m = checkGraphModeOp m data op tracing TODO remove after refactor checkGraphModeOp quantized FileCheck check_not aten add check_not aten add_ run m graph FileCheck check_not quantized add run m graph skipIfNoFBGEMM test_quantized_add_scalar QuantizedAddScalar torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x x = conv x x + QuantizedInplaceAddScalar torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x x = conv x x += x NonQuantizedAddScalar torch nn Module forward x x + NonQuantizedInplaceAddScalar torch nn Module forward x x += x data = torch randn dtype=torch float m quantized QuantizedAddScalar True QuantizedInplaceAddScalar True NonQuantizedAddScalar False NonQuantizedInplaceAddScalar False tracing True False op = quantized add_scalar quantized aten add we don t check numerical consistency add_scalar since s supported m = checkGraphModeOp m data op tracing check=False TODO remove after refactor checkGraphModeOp quantized FileCheck check_not aten add check_not aten add_ run m graph FileCheck check_not quantized add_scalar run m graph skipIfNoFBGEMM test_quantized_add_relu AddRelu torch nn Module __init__ inplace super __init__ conv = torch nn Conv d float conv = torch nn Conv d float relu = torch nn ReLU inplace forward x y x = conv x y = conv y x = x + y relu x InplaceAddRelu torch nn Module __init__ inplace super __init__ conv = torch nn Conv d float conv = torch nn Conv d float relu = torch nn ReLU inplace forward x y x = conv x y = conv y x += y relu x AddFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x = x + y F relu x InplaceAddFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x += y F relu x AddInplaceFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x = x + y F relu x True InplaceAddInplaceFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x += y F relu x True data = torch rand dtype=torch float torch rand dtype=torch float m AddRelu True AddRelu False InplaceAddRelu True InplaceAddRelu False AddFunctionalRelu InplaceAddFunctionalRelu AddInplaceFunctionalRelu InplaceAddInplaceFunctionalRelu tracing True False m = checkGraphModeOp m data quantized add_relu tracing FileCheck check_not aten add check_not aten add_ check_not aten relu check_not aten relu_ check_not quantized add check_not quantized relu run m graph skipIfNoFBGEMM test_quantized_add_scalar_relu AddScalarRelu torch nn Module __init__ inplace super __init__ conv = torch nn Conv d float relu = torch nn ReLU inplace forward x x = conv x relu x + InplaceAddScalarRelu torch nn Module __init__ inplace super __init__ conv = torch nn Conv d float relu = torch nn ReLU inplace forward x x = conv x x += relu x AddScalarFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x x = conv x F relu x + InplaceAddScalarFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x x = conv x x += F relu x AddScalarInplaceFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x x = conv x F relu x + True InplaceAddScalarInplaceFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x x = conv x x += F relu x True data = torch rand dtype=torch float m AddScalarRelu True AddScalarRelu False InplaceAddScalarRelu True InplaceAddScalarRelu False AddScalarFunctionalRelu InplaceAddScalarFunctionalRelu AddScalarInplaceFunctionalRelu InplaceAddScalarInplaceFunctionalRelu tracing True False quantized add_scalar_relu quantized add_scalar_relu_out TODO split after refactor checkGraphModeOp m = checkGraphModeOp m data quantized add_scalar_relu tracing check=False FileCheck check_not aten add check_not aten add_ check_not aten relu check_not aten relu_ check_not quantized add_scalar check_not quantized relu run m graph skipIfNoFBGEMM test_quantized_cat quantization output cat will depend input cat we only quantize output cat when its inputs quantized QuantizedCat torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y torch cat x y NonQuantizedCat torch nn Module forward x y torch cat x y data = torch randn dtype=torch float torch randn dtype=torch float tracing True False m = checkGraphModeOp QuantizedCat data quantized cat tracing FileCheck check_not aten cat run m graph m = checkGraphModeOp NonQuantizedCat data aten cat tracing FileCheck check_not quantized cat run m graph skipIfNoFBGEMM test_qbatch_norm bn_module = torch nn BatchNorm d torch nn BatchNorm d torch nn BatchNorm d M torch nn Module __init__ dim super __init__ bn = bn_module dim torch float forward x bn x options = itertools product True False tracing dim options model = checkGraphModeOp M dim img_data_dict dim quantized batch_norm tracing FileCheck check_not aten batch_norm run model graph skipIfNoFBGEMM test_qbatch_norm_relu_BNRelu bn_module = torch nn BatchNorm d torch nn BatchNorm d BNRelu torch nn Module __init__ dim inplace super __init__ bn = bn_module dim torch float relu = torch nn ReLU inplace=inplace forward x relu bn x options = itertools product True False tracing dim options instance BNRelu dim True BNRelu dim False model = checkGraphModeOp instance img_data_dict dim quantized batch_norm_relu tracing FileCheck check_not aten batch_norm check_not aten relu check_not aten relu_ run model graph skipIfNoFBGEMM test_qbatch_norm_relu_BNFuncRelu bn_module = torch nn BatchNorm d torch nn BatchNorm d BNFuncRelu torch nn Module __init__ dim super __init__ bn = bn_module dim torch float forward x F relu bn x False options = itertools product True False tracing dim options instance = BNFuncRelu dim model = checkGraphModeOp instance img_data_dict dim quantized batch_norm_relu tracing FileCheck check_not aten batch_norm check_not aten relu check_not aten relu_ run model graph skipIfNoFBGEMM test_qbatch_norm_relu_BNFuncInplaceRelu bn_module = torch nn BatchNorm d torch nn BatchNorm d BNFuncInplaceRelu torch nn Module __init__ dim super __init__ bn = bn_module dim torch float forward x F relu bn x True options = itertools product True False tracing dim options instance = BNFuncInplaceRelu dim model = checkGraphModeOp instance img_data_dict dim quantized batch_norm_relu tracing FileCheck check_not aten batch_norm check_not aten relu check_not aten relu_ run model graph skipIfNoFBGEMM test_quantized_mul QuantizedMul torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x y QuantizedInplaceMul torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x = y x NonQuantizedMul torch nn Module forward x y x y NonQuantizedInplaceMul torch nn Module forward x y x = y x data = torch randn dtype=torch float torch randn dtype=torch float m quantized QuantizedMul True QuantizedInplaceMul True NonQuantizedMul False NonQuantizedInplaceMul False tracing True False op = quantized mul quantized aten mul m = checkGraphModeOp m data op tracing TODO remove after refactor checkGraphModeOp quantized FileCheck check_not aten mul check_not aten mul_ run m graph FileCheck check_not quantized mul run m graph skipIfNoFBGEMM test_quantized_mul_scalar QuantizedMulScalar torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x x = conv x x QuantizedInplaceMulScalar torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x x = conv x x = x NonQuantizedMulScalar torch nn Module forward x x NonQuantizedInplaceMulScalar torch nn Module forward x x = x data = torch randn dtype=torch float m quantized QuantizedMulScalar True QuantizedInplaceMulScalar True NonQuantizedMulScalar False NonQuantizedInplaceMulScalar False tracing True False op = quantized mul_scalar quantized aten mul we don t check numerical consistency add_scalar since s supported m = checkGraphModeOp m data op tracing check=False TODO remove after refactor checkGraphModeOp quantized FileCheck check_not aten mul check_not aten mul_ run m graph FileCheck check_not quantized mul_scalar run m graph skipIfNoFBGEMM test_quantized_mul_relu MulRelu torch nn Module __init__ inplace super __init__ conv = torch nn Conv d float conv = torch nn Conv d float relu = torch nn ReLU inplace forward x y x = conv x y = conv y x = x y relu x InplaceMulRelu torch nn Module __init__ inplace super __init__ conv = torch nn Conv d float conv = torch nn Conv d float relu = torch nn ReLU inplace forward x y x = conv x y = conv y x = y relu x MulFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x = x y F relu x InplaceMulFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x = y F relu x MulInplaceFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x = x y F relu x True InplaceMulInplaceFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y x = y F relu x True data = torch rand dtype=torch float torch rand dtype=torch float m MulRelu True MulRelu False InplaceMulRelu True InplaceMulRelu False MulFunctionalRelu InplaceMulFunctionalRelu MulInplaceFunctionalRelu InplaceMulInplaceFunctionalRelu tracing True False m = checkGraphModeOp m data quantized mul_relu tracing FileCheck check_not aten mul check_not aten mul_ check_not aten relu check_not aten relu_ check_not quantized mul check_not quantized relu run m graph skipIfNoFBGEMM test_quantized_mul_scalar_relu MulScalarRelu torch nn Module __init__ inplace super __init__ conv = torch nn Conv d float relu = torch nn ReLU inplace forward x x = conv x relu x InplaceMulScalarRelu torch nn Module __init__ inplace super __init__ conv = torch nn Conv d float relu = torch nn ReLU inplace forward x x = conv x x = relu x MulScalarFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x x = conv x F relu x InplaceMulScalarFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x x = conv x x = F relu x MulScalarInplaceFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x x = conv x F relu x True InplaceMulScalarInplaceFunctionalRelu torch nn Module __init__ - None super __init__ conv = torch nn Conv d float forward x x = conv x x = F relu x True data = torch randn dtype=torch float m MulScalarRelu True MulScalarRelu False InplaceMulScalarRelu True InplaceMulScalarRelu False MulScalarFunctionalRelu InplaceMulScalarFunctionalRelu MulScalarInplaceFunctionalRelu InplaceMulScalarInplaceFunctionalRelu tracing True False quantized mul_scalar_relu quantized mul_scalar_relu_out m = checkGraphModeOp m data quantized mul_scalar_relu tracing check=False FileCheck check_not aten mul check_not aten mul_ check_not aten relu check_not aten relu_ check_not quantized mul_scalar check_not quantized relu run m graph override_qengines test_hardswish FunctionalHardswish torch nn Module __init__ inplace super __init__ inplace = inplace forward input torch nn functional hardswish input inplace=self inplace modules = torch nn Hardswish FunctionalHardswish True FunctionalHardswish False test_case itertools product True False modules tracing m = test_case m = checkGraphModeOp m img_data_ d quantized hardswish tracing FileCheck check_not aten hardswish check_not aten hardswish_ run m graph override_qengines test_elu FunctionalELU torch nn Module __init__ inplace=False super __init__ inplace = inplace forward input torch nn functional elu input inplace=self inplace modules = torch nn ELU FunctionalELU test_case itertools product True False True False modules tracing inplace mod_class = test_case m = mod_class inplace=inplace m = checkGraphModeOp m img_data_ d quantized elu tracing FileCheck check_not aten elu check_not aten elu_ run m graph override_qengines test_layer_norm data = torch rand dtype=torch float _ range layer_norm = torch nn LayerNorm tracing True False m = checkGraphModeOp layer_norm data quantized layer_norm tracing FileCheck check_not aten layer_norm run m graph override_qengines test_group_norm data = torch rand dtype=torch float _ range group_norm = torch nn GroupNorm tracing True False m = checkGraphModeOp group_norm data quantized group_norm tracing FileCheck check_not aten group_norm run m graph override_qengines test_instance_norm data_ d = torch rand dtype=torch float _ range data_ d = torch rand dtype=torch float _ range data_ d = torch rand dtype=torch float _ range data = data_ d data_ d data_ d instance_norm_modules = torch nn InstanceNorm d torch nn InstanceNorm d torch nn InstanceNorm d options = itertools product True False dim tracing options instance_norm = instance_norm_modules dim m = checkGraphModeOp instance_norm data dim quantized instance_norm tracing FileCheck check_not aten instance_norm run m graph skipIfNoFBGEMM test_dequantize_tuple Make sure dequantize can support Tuple tensor M torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x torch Tensor - tuple torch Tensor torch Tensor x = conv x x = conv x x x tracing True False checkGraphModeOp M img_data_ d quantized conv d tracing skipIfNoFBGEMM test_clamp M torch nn Module __init__ - None super __init__ conv = torch nn Conv d float relu = torch nn ReLU relu _ = torch nn ReLU True hardtanh = torch nn Hardtanh hardtanh_ = torch nn Hardtanh inplace=True forward x x = conv x x = relu x relu _ x x = F relu x x = torch clamp x - x = x clamp - x = x clamp_ - Enable when quantized ` clamp_ ` ready x = hardtanh x hardtanh_ x x = F hardtanh x F hardtanh_ x x data = torch rand dtype=torch float options = itertools product aten clamp aten hardtanh aten hardtanh_ True False op tracing options m = checkGraphModeOp M data op tracing FileCheck check_count aten quantize_per_tensor exactly=True run m graph FileCheck check_count aten dequantize exactly=True run m graph test_general_shape_ops A test checks dequantize will swapped all supported general shape ops like aten flatten without actually checking execution these ops M torch nn Module __init__ - None super __init__ maxpool d = torch nn MaxPool d kernel_size= maxpool d = torch nn MaxPool d kernel_size= maxpool d = torch nn MaxPool d kernel_size= dropout = torch nn Dropout conv = torch nn Conv d conv = torch nn Conv d relu = torch nn ReLU forward x x = conv x add_scalar x = x + mul_scalar x = x add_scalar_out x += mul_scalar_out x = add_scalar_relu x = x + x = F relu x add_scalar_relu_out x += x = F relu x mul_scalar_relu x = x x = F relu x mul_scalar_relu_out x = x = F relu x x = maxpool d x x = maxpool d x x = maxpool d x x = torch flatten x x = torch max x x = torch min x x = x reshape - x = x resize_ x numel x = x view - prim ListConstruct xs = x x prim ListUnpack x y = xs prim TupleConstruct xs = x x prim TupleUnpack x y = xs x = x transpose x = x contiguous x y = torch chunk x x = F dropout x x = dropout x x _ = torch sort x x = x permute x = torch repeat_interleave x x = relu x x = F relu x x relu_ x = x squeeze x squeeze_ x = torch squeeze x x = x unsqueeze x unsqueeze_ x = torch unsqueeze x x = x detach x detach_ x = x repeat y = y append x z = torch stack y z = z z x _ = z x = conv x x data = torch rand This model executable since we just put all ops same forward therefore we only test scripting m = torch jit script M qconfig = script_qconfig default_qconfig dummy data suppress warning get_forward qconfig activation data get_forward qconfig weight data m = wrap_cpp_module torch _C _jit_pass_insert_observers m _c forward qconfig inplace=False m = convert_jit m This checks dequantize output first conv being propagated end so we don t insert extra observers also successfully fused two quantized conv d patterns one quantize_per_tensor input FileCheck check_count aten quantize_per_tensor exactly=True run m graph FileCheck check_count quantized conv d exactly=True run m graph FileCheck check_count aten dequantize exactly=True run m graph FileCheck check quantized add_scalar check quantized mul_scalar run m graph test_general_value_ops A test checks correct patterns produced all supported general value ops like aten avg_pool d \ without actually checking execution these ops M torch nn Module __init__ - None super __init__ conv = torch nn Conv d avg_pool d = torch nn AvgPool d avg_pool d = torch nn AvgPool d avg_pool d = torch nn AvgPool d adaptive_avg_pool d = torch nn AdaptiveAvgPool d adaptive_avg_pool d = torch nn AdaptiveAvgPool d adaptive_avg_pool d = torch nn AdaptiveAvgPool d leaky_relu = torch nn LeakyReLU hardsigmoid = torch nn Hardsigmoid sigmoid = torch nn Sigmoid tanh = torch nn Tanh forward x x = conv x x = avg_pool d x x = avg_pool d x x = avg_pool d x x = adaptive_avg_pool d x x = adaptive_avg_pool d x x = adaptive_avg_pool d x x = F avg_pool d x x = F avg_pool d x x = F avg_pool d x x = F adaptive_avg_pool d x x = F adaptive_avg_pool d x x = F adaptive_avg_pool d x x = torch mean x x = torch mean x False x = x mean x = x mean True interpolate node will introduce quantize_per_tensor ops x = F interpolate x mode= nearest interpolate node x = F upsample x interpolate node x = F upsample_nearest x interpolate node x = F interpolate x mode= linear common node x = F upsample_bilinear x common node x = leaky_relu x x = F leaky_relu x x leaky_relu_ x = hardsigmoid x x = F hardsigmoid x x hardsigmoid_ x = sigmoid x x = torch sigmoid x F sigmoid deprecated x = x sigmoid x sigmoid_ x = tanh x F tanh deprecated x = torch tanh x x = x tanh x tanh_ x = conv x x This model executable since we just put all ops same forward therefore we only test scripting m = torch jit script M qconfig = script_qconfig default_qconfig dummy data suppress warning data = torch rand get_forward qconfig activation data get_forward qconfig weight data m = wrap_cpp_module torch _C _jit_pass_insert_observers m _c forward qconfig inplace=False Checking model before finalize contain unfused patterns numerically matches model after quantize checking number aten quantize_per_tensor functions conv has quantize_per_tensor activations weight N general value op between conv we should have N + quantize_per_tensor between these ops m = convert_jit m debug=True NB This Needs updated when we add more ops test mapping number quant op number these ops example ` ` key means type op we ll have quantize_per_tensor num_op_by_num_quant = num_quantize_per_tensor = output num_quant num_op num_op_by_num_quant items num_quantize_per_tensor += num_op num_quant num_quantize_per_tensor -= constant propagation removes some prepacks FileCheck check_count aten quantize_per_tensor num_quantize_per_tensor exactly=True run m graph This checks dequantize output first conv being propagated end so we don t insert extra observers also successfully fused two quantized conv d patterns one quantize_per_tensor input m = convert_jit m debug=False FileCheck check_count aten quantize_per_tensor exactly=True run m graph FileCheck check_count quantized conv d exactly=True check aten dequantize run m graph override_qengines test_conv_with_benchmark_flag r Verifies convolutions get quantized when torch backends cudnn benchmark enabled qengine_is_qnnpack torch backends cudnn flags enabled=True m = torch nn Sequential torch nn Conv d m eval m = torch jit trace m torch rand qconfig = torch ao quantization get_default_qconfig qnnpack prepared_model = torch ao quantization prepare_jit m qconfig prepared_model torch rand converted_model = torch ao quantization convert_jit prepared_model FileCheck check quantized conv d run converted_model graph skipIfNoFBGEMM test_cat_linear LinearModel torch nn Module __init__ - None super __init__ weight = torch randn forward x y = torch cat x y b = F linear weight c = F linear b weight b c model = LinearModel eval qconfig = default_qconfig float_model = torch jit script model prepared_model = prepare_jit float_model qconfig prepared_model torch rand torch rand converted_model = convert_jit prepared_model FileCheck check quantized linear check quantized linear run converted_model graph TestQuantizeDynamicJitPasses QuantizationTestCase test_prepare_dynamic M torch nn Module __init__ - None super __init__ fc = torch nn Linear forward x fc x model = torch jit script M qconfig float _dynamic_qconfig default_dynamic_qconfig m = prepare_dynamic_jit model qconfig observer weight assert len attrs_with_prefix m fc _observer_ == qconfig == float _dynamic_qconfig observer_name = PlaceholderObserver = prim GetAttr name= _observer_ FileCheck check observer_name run m fc graph input FC dynamic quant assert len attrs_with_prefix m _observer_ == observer_name = Observer = prim GetAttr name= _observer_ FileCheck check observer_name check prim GetAttr name= fc check prim CallMethod check_not observer_name run m graph test_prepare_dynamic_child_qconfig Sub torch nn Module __init__ - None super __init__ fc = torch nn Linear forward x fc x M torch nn Module __init__ - None super __init__ conv = torch nn Conv d sub = Sub forward x sub conv x m = torch jit script M only quantize child module m = prepare_dynamic_jit m sub fc default_dynamic_qconfig input sub dynamic quant assert len attrs_with_prefix m _observer_ == quantized assert len attrs_with_prefix m conv _observer_ == no observers since we observe outer most call site assert len attrs_with_prefix m sub _observer_ == weight linear assert len attrs_with_prefix m sub fc _observer_ == FileCheck check prim GetAttr name= sub check prim CallMethod check Observer = prim GetAttr name= _observer_ check prim CallMethod check_not Observer = prim GetAttr name= _observer_ run m graph test_insert_quant_dequant_linear_dynamic M torch nn Module __init__ - None super __init__ fc = torch nn Linear float fc = torch nn Linear float forward x x = fc x fc x is_per_channel True False m = torch jit script M qconfig = per_channel_dynamic_qconfig is_per_channel True default_dynamic_qconfig m = quantize_dynamic_jit m qconfig debug=True assert len m _modules _c items == Expected have two submodule linear wt_quant_func = aten quantize_per_channel is_per_channel aten quantize_per_tensor act_quant_func = aten quantize_per_tensor quantizing activations FileCheck check aten _choose_qparams_per_tensor check_next act_quant_func check_next aten dequantize check aten _choose_qparams_per_tensor check_next act_quant_func check_next aten dequantize check wt_quant_func check_next aten dequantize check_not wt_quant_func check run m graph override_qengines test_dynamic_multi_op M torch nn Module __init__ - None super __init__ fc = torch nn Linear dtype=torch float forward x x = x + fc x x = torch randn tracing True False model = checkGraphModeOp M x quantized linear_dynamic tracing=tracing dynamic=True add op dynamically quantized FileCheck check aten add run model graph override_qengines test_dynamic_quant_multi_uses M torch nn Module __init__ - None super __init__ fc = torch nn Linear float forward x size = x size size = x size fc x size size x = torch randn tracing True False model = checkGraphModeOp M x quantized linear_dynamic tracing=tracing dynamic=True FileCheck check_not aten _choose_qparams_per_tensor run model graph override_qengines test_dynamic_shared_weights myMod torch nn Module __init__ weight super __init__ linear = nn Linear linear weight = weight forward x linear x DynamicModel torch nn Module __init__ - None super __init__ weight = torch nn Parameter torch ones mod = myMod weight forward x y = mod x z = torch nn functional linear y weight z model = torch jit script DynamicModel eval data = torch randn dtype=torch float quant_ops = mod counts = op count zip quant_ops counts qconfig_dict = op default_dynamic_qconfig m = quantize_dynamic_jit model qconfig_dict out_graph = m data FileCheck check_count quantized linear_dynamic count exactly=True check_not aten _choose_qparams_per_tensor run m graph Explicitly call forward model before convert m = prepare_dynamic_jit model qconfig_dict m data m = convert_dynamic_jit m debug=False out_ref = m data assertEqual out_graph out_ref override_qengines test_dynamic_with_if Res torch nn Module __init__ - None super __init__ weight = torch nn Parameter torch ones forward x torch Tensor cond bool - torch Tensor cond torch nn functional linear x weight torch nn functional linear x weight M torch nn Module __init__ - None super __init__ res = Res res = Res forward x x = res x True x = res x False x model = torch jit script M eval data = torch randn dtype=torch float qconfig_dict = default_dynamic_qconfig tracing True False m = checkGraphModeOp M data quantized linear_dynamic tracing=tracing dynamic=True FileCheck check_count quantized linear_dynamic exactly=True check_not aten _choose_qparams_per_tensor run m graph Check make sure weight observers run correctly ref_qparams = qconfig = script_qconfig default_dynamic_qconfig wt_module = wrap_cpp_module qconfig weight wt model res weight model res weight wt_module wt qparams = wt_module calculate_qparams ref_qparams append qparams item qparams item m = quantize_dynamic_jit model qconfig_dict debug=True graph_params = x obs m _modules _c items x == res graph_params append obs getattr weight _scale_ obs getattr weight _zero_point_ x == res graph_params append obs getattr weight _scale_ obs getattr weight _zero_point_ assertEqual ref_qparams graph_params test_dynamic_weight_observer M torch nn Module __init__ - None super __init__ fc = torch nn Linear float fc = torch nn Linear float forward x x = fc x fc x qconfig_dict = default_dynamic_qconfig eager_model = M eval tracing True False x = torch rand model = get_script_module eager_model tracing x ref_qparams = wt model fc weight model fc weight wt_module = default_dynamic_qconfig weight wt_module wt qparams = wt_module calculate_qparams ref_qparams append qparams item qparams item model = quantize_dynamic_jit model qconfig_dict debug=True graph_qparams = x obs model _modules _c items n = x == fc tracing graph_qparams append obs getattr f weight n _scale_ obs getattr f weight n _zero_point_ assertEqual ref_qparams graph_qparams test_convert_dynamic_fp M torch nn Module __init__ - None super __init__ fc = torch nn Linear forward x fc x m = torch jit script M m = quantize_dynamic_jit m float _dynamic_qconfig debug=True FileCheck check aten _saturate_weight_to_fp check aten linear check_not aten dequantize check_not aten quantize run m graph test_quantize_dynamic_fp M torch nn Module __init__ - None super __init__ fc = torch nn Linear forward x fc x m = torch jit script M m = quantize_dynamic_jit m float _dynamic_qconfig FileCheck check quantized linear_dynamic_fp check_not aten linear check_not aten dequantize check_not aten quantize run m graph TestQuantizeDynamicJitOps QuantizationTestCase Test graph mode post training dynamic quantization works individual ops end end override_qengines test_linear FunctionalLinear torch nn Module __init__ weight bias super __init__ weight = weight bias = bias forward x F linear x weight bias x = torch rand tracing True False model = checkGraphModeOp torch nn Linear x quantized linear_dynamic tracing=tracing dynamic=True weight = torch rand b = torch rand tracing has_bias itertools product True False True False bias = b has_bias None model = checkGraphModeOp FunctionalLinear weight bias x quantized linear_dynamic tracing=tracing dynamic=True skipIfNoFBGEMM test_embedding_bag M torch nn Module __init__ weights super __init__ embedding = torch nn EmbeddingBag num_embeddings= embedding_dim= include_last_offset=True sparse=True _weight=weights mode= sum embedding = torch nn EmbeddingBag num_embeddings= embedding_dim= include_last_offset=True sparse=True _weight=weights mode= sum forward indices offsets indices offsets e = embedding indices offsets e = embedding indices offsets e e weights = torch randn dtype=torch float module = M weights indices = torch tensor offsets = torch tensor dummy_inputs = indices offsets indices offsets trace True False trace m = torch jit trace module dummy_inputs m = torch jit script module int _qconfig = QConfig activation=PlaceholderObserver with_args dtype=torch float custom_op_name= embedding_bag_ bit weight=PlaceholderObserver with_args custom_op_name= embedding_bag_ bit int _qconfig = QConfig activation=PlaceholderObserver with_args dtype=torch float custom_op_name= embedding_bag_byte weight=PlaceholderObserver with_args custom_op_name= embedding_bag_byte m = prepare_jit m embedding int _qconfig embedding int _qconfig m = convert_jit m FileCheck check quantized embedding_bag_ bit_rowwise_offsets check quantized embedding_bag_byte_rowwise_offsets run m graph m dummy_inputs Ensure attempting quantize EmbeddingBag throws error padding_idx None skipIfNoFBGEMM test_embedding_bag_padding_idx_error M torch nn Module __init__ weights super __init__ embedding = torch nn EmbeddingBag num_embeddings= embedding_dim= include_last_offset=True sparse=True _weight=weights mode= sum padding_idx= forward indices offsets e = embedding indices offsets e weights = torch randn dtype=torch float module = M weights indices = torch tensor offsets = torch tensor dummy_inputs = indices offsets int _qconfig = QConfig activation=PlaceholderObserver with_args dtype=torch float custom_op_name= embedding_bag_ bit weight=PlaceholderObserver with_args custom_op_name= embedding_bag_ bit int _qconfig = QConfig activation=PlaceholderObserver with_args dtype=torch float custom_op_name= embedding_bag_byte weight=PlaceholderObserver with_args custom_op_name= embedding_bag_byte error_msg = r Expected aten embedding_bag padding_idx input None trace qconfig itertools product True False int _qconfig int _qconfig trace m = torch jit trace module dummy_inputs m = torch jit script module m = prepare_jit m embedding qconfig assertRaisesRegex RuntimeError error_msg m = convert_jit m TestQuantizeJit QuantizationTestCase override_qengines test_single_linear r Compare result quantizing single linear layer eager mode graph mode eager mode annotated_linear_model = AnnotatedSingleLayerLinearModel torch backends quantized engine eval linear_model = SingleLayerLinearModel eval copy weight eager mode so we can compare result two quantized models later linear_model fc weight = torch nn Parameter annotated_linear_model fc module weight detach linear_model fc bias = torch nn Parameter annotated_linear_model fc module bias detach model_eager = quantize annotated_linear_model test_only_eval_fn calib_data qconfig_dict = get_default_qconfig torch backends quantized engine model_traced = torch jit trace linear_model calib_data model_script = torch jit script linear_model result_eager = model_eager calib_data model_under_test model_traced model_script model_quantized = quantize_jit model_under_test qconfig_dict test_only_eval_fn calib_data inplace=False assertEqual model_quantized calib_data result_eager skipIfNoFBGEMM test_observer_with_ignored_function r Test observers ignored function make sure works graph mode eager mode annotated_linear_model = AnnotatedSingleLayerLinearModel fbgemm eval qconfig QConfig activation=default_observer weight=default_weight_observer QConfig activation=default_histogram_observer weight=default_weight_observer QConfig activation=default_observer weight=default_per_channel_weight_observer annotated_linear_model qconfig = qconfig linear_model = SingleLayerLinearModel eval copy weight eager mode so we can compare result two quantized models later linear_model fc weight = torch nn Parameter annotated_linear_model fc module weight detach linear_model fc bias = torch nn Parameter annotated_linear_model fc module bias detach model_eager = quantize annotated_linear_model test_only_eval_fn calib_data qconfig_dict = qconfig model_traced = torch jit trace linear_model calib_data model_script = torch jit script linear_model result_eager = model_eager calib_data model_under_test model_traced model_script model_quantized = quantize_jit model_under_test qconfig_dict test_only_eval_fn calib_data inplace=False assertEqual model_quantized calib_data result_eager override_qengines test_conv r Compare result quantizing conv layer eager mode graph mode eager mode annotated_conv_model = AnnotatedConvModel torch backends quantized engine eval conv_model = ConvModel eval copy weight eager mode so we can compare result two quantized models later conv_model conv weight = torch nn Parameter annotated_conv_model conv weight detach model_eager = quantize annotated_conv_model test_only_eval_fn img_data_ d qconfig_dict = get_default_qconfig torch backends quantized engine model_traced = torch jit trace conv_model img_data_ d model_script = torch jit script conv_model result_eager = model_eager img_data_ d model_under_test model_traced model_script model_quantized = quantize_jit model_under_test qconfig_dict test_only_eval_fn img_data_ d inplace=False assertEqual model_quantized img_data_ d result_eager override_qengines test_conv_transpose r Compare result quantizing conv_transpose layer eager mode graph mode qengine_is_qnnpack Currently only qnnpack supported eager mode annotated_conv_model = AnnotatedConvTransposeModel torch backends quantized engine eval conv_model = ConvTransposeModel eval copy weight eager mode so we can compare result two quantized models later conv_model conv weight = torch nn Parameter annotated_conv_model conv weight detach model_eager = quantize annotated_conv_model test_only_eval_fn img_data_ d qconfig_dict = get_default_qconfig torch backends quantized engine model_traced = torch jit trace conv_model img_data_ d model_script = torch jit script conv_model result_eager = model_eager img_data_ d model_under_test model_traced model_script model_quantized = quantize_jit model_under_test qconfig_dict test_only_eval_fn img_data_ d inplace=False assertEqual model_quantized img_data_ d result_eager override_qengines test_conv_bn r Compare result quantizing conv + bn layer eager mode graph mode eager mode conv_model = AnnotatedConvBnModel eval conv_model_to_script = ConvBnModel eval copy weight eager mode so we can compare result two quantized models later conv_model_to_script conv weight = torch nn Parameter conv_model conv weight detach fuse_modules conv_model conv bn inplace=True model_eager = quantize conv_model test_only_eval_fn img_data_ d qconfig_dict = default_qconfig model_script = quantize_jit torch jit script conv_model_to_script qconfig_dict test_only_eval_fn img_data_ d inplace=False result_eager = model_eager img_data_ d result_script = model_script img_data_ d assertEqual result_eager result_script override_qengines test_nested Eager mode eager_model = AnnotatedNestedModel torch backends quantized engine eval Graph mode script_model = NestedModel eval Copy weights eager_model script_model sub fc weight = torch nn Parameter eager_model sub fc weight detach script_model sub fc bias = torch nn Parameter eager_model sub fc bias detach script_model sub fc weight = torch nn Parameter eager_model sub fc module weight detach script_model sub fc bias = torch nn Parameter eager_model sub fc module bias detach script_model sub fc weight = torch nn Parameter eager_model sub fc weight detach script_model sub fc bias = torch nn Parameter eager_model sub fc bias detach script_model fc weight = torch nn Parameter eager_model fc module weight detach script_model fc bias = torch nn Parameter eager_model fc module bias detach model_eager = quantize eager_model test_only_eval_fn calib_data qconfig_dict = sub fc default_per_channel_qconfig qengine_is_fbgemm default_qconfig fc default_qconfig model_traced = torch jit trace script_model calib_data model_script = torch jit script script_model result_eager = model_eager calib_data model_under_test model_traced model_script model_quantized = quantize_jit model_under_test qconfig_dict test_only_eval_fn calib_data inplace=False assertEqual model_quantized calib_data result_eager override_qengines test_skip_quant Test None qconfig Eager mode eager_model = AnnotatedSkipQuantModel torch backends quantized engine eval Graph mode script_model = SkipQuantModel eval Copy weights eager_model script_model sub fc weight = torch nn Parameter eager_model sub module fc weight detach script_model sub fc bias = torch nn Parameter eager_model sub module fc bias detach script_model sub fc weight = torch nn Parameter eager_model sub module fc weight detach script_model sub fc bias = torch nn Parameter eager_model sub module fc bias detach script_model fc weight = torch nn Parameter eager_model fc weight detach script_model fc bias = torch nn Parameter eager_model fc bias detach eager_model fuse_modules model_eager = quantize eager_model test_only_eval_fn calib_data qconfig_dict = get_default_qconfig torch backends quantized engine fc None model_traced = torch jit trace script_model calib_data model_script = torch jit script script_model result_eager = model_eager calib_data model_under_test model_traced model_script model_quantized = quantize_jit model_under_test qconfig_dict test_only_eval_fn calib_data inplace=False assertEqual model_quantized calib_data result_eager override_qengines test_single_linear_dynamic r Compare result dynamic quantization single linear layer eager mode graph mode qengine_is_qnnpack eager mode annotated_linear_model = AnnotatedSingleLayerLinearModel qnnpack eval linear_model = SingleLayerLinearModel eval copy weight eager mode so we can compare result two quantized models later linear_model fc weight = torch nn Parameter annotated_linear_model fc module weight detach linear_model fc bias = torch nn Parameter annotated_linear_model fc module bias detach qconfig_dict = default_dynamic_qconfig model_eager = quantize_dynamic annotated_linear_model qconfig_dict model_traced = torch jit trace linear_model calib_data model_script = torch jit script linear_model result_eager = model_eager calib_data model_under_test model_traced model_script model_quantized = quantize_dynamic_jit model_under_test qconfig_dict assertEqual model_quantized calib_data result_eager Check make sure choose_qparams- quant- dequant- linear numerically equivalent final quantized model model_fake_quantized = quantize_dynamic_jit model_under_test qconfig_dict debug=True assertEqual model_fake_quantized calib_data result_eager skipIfNoFBGEMM test_linear_dynamic_fp linear_model = SingleLayerLinearModel eval Create weight tensor values beyond fp max x = torch ones linear_model fc weight = torch nn Parameter x warnings model_eager = quantize_dynamic linear_model dtype=torch float result_eager = model_eager calib_data trace True warnings catch_warnings record=True w quantized_model = checkGraphModeOp linear_model calib_data quantized linear_dynamic_fp tracing=trace dynamic=True qconfig=float _dynamic_qconfig compare result eager mode assertEqual quantized_model calib_data result_eager __name__ == __main__ raise_on_run_directly test test_quantization py