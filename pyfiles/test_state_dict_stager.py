Owner s oncall distributed dataclasses os tempfile unittest datetime timedelta torch torch distributed dist torch distributed _shard sharded_tensor init_from_local_shards Shard ShardedTensorShard ShardedTensor ShardMetadata torch distributed _tensor DTensor torch distributed _tensor placement_types Replicate Shard torch distributed checkpoint _state_dict_stager StateDictStager torch distributed checkpoint staging _ReplicationStager torch distributed tensor DeviceMesh distribute_tensor torch testing _internal common_distributed HAS_ACCELERATOR requires_accelerator_dist_backend skip_if_lt_x_gpu torch testing _internal common_utils run_tests TestCase torch testing _internal distributed _tensor common_dtensor DTensorTestBase with_comms device_type = acc type acc = torch accelerator current_accelerator cpu create_cpu_state_dict state_dict cpu_state_dict = key value state_dict items cpu_state_dict key = value cpu cpu_state_dict compare_state_dicts gpu_state_dict cpu_state_dict rtol= e- atol= e- Compare two state dictionaries one GPU one CPU otherwise same This function checks tensors both state dictionaries have same values shapes dtypes etc ignoring device difference It also checks tensors share storage one state dict also share storage other Args gpu_state_dict The state dictionary tensors GPU cpu_state_dict The state dictionary tensors CPU rtol Relative tolerance comparing tensor values atol Absolute tolerance comparing tensor values Returns bool True state dictionaries equivalent False otherwise str Error message state dictionaries equivalent empty string otherwise Track storage data pointers check storage sharing gpu_storage_ptrs = cpu_storage_ptrs = compare_objects gpu_obj cpu_obj path= If objects tensors compare them isinstance gpu_obj torch Tensor isinstance cpu_obj torch Tensor Check devices expected gpu_obj device type = device_type False f Expected accelerator tensor got gpu_obj device type tensor path cpu_obj device type = cpu False f Expected CPU tensor got cpu_obj device type tensor path gpu_obj storage_offset = cpu_obj storage_offset False f Storage offset mismatch path gpu_obj storage_offset vs cpu_obj storage_offset torch equal gpu_obj cpu cpu_obj False f Tensors same path Track storage sharing gpu_storage_ptr = gpu_obj storage data_ptr cpu_storage_ptr = cpu_obj storage data_ptr gpu_storage_ptr gpu_storage_ptrs This GPU tensor shares storage another tensor Check corresponding CPU tensors also share storage cpu_storage_ptr = gpu_storage_ptrs gpu_storage_ptr False f Storage sharing mismatch GPU tensors share storage CPU tensors don t path First time seeing storage gpu_storage_ptrs gpu_storage_ptr = cpu_storage_ptr cpu_storage_ptrs cpu_storage_ptr = gpu_storage_ptr True If objects dictionaries compare them recursively isinstance gpu_obj dict isinstance cpu_obj dict gpu_obj keys = cpu_obj keys False f Dictionary keys mismatch path gpu_obj keys vs cpu_obj keys key gpu_obj result error = compare_objects gpu_obj key cpu_obj key f path key path key result False error True If objects lists tuples sets compare them recursively isinstance gpu_obj list tuple set isinstance cpu_obj list tuple set len gpu_obj = len cpu_obj False f Collection length mismatch path len gpu_obj vs len cpu_obj type gpu_obj type cpu_obj False f Collection type mismatch path type gpu_obj vs type cpu_obj i gpu_item cpu_item enumerate zip gpu_obj cpu_obj result error = compare_objects gpu_item cpu_item f path i result False error True If objects custom classes compare their attributes hasattr gpu_obj __dict__ hasattr cpu_obj __dict__ type gpu_obj type cpu_obj False f Object type mismatch path type gpu_obj vs type cpu_obj result error = compare_objects gpu_obj __dict__ cpu_obj __dict__ f path __dict__ result False error True For other types use direct equality comparison type gpu_obj type cpu_obj False f Type mismatch path type gpu_obj vs type cpu_obj gpu_obj = cpu_obj False f Value mismatch path gpu_obj vs cpu_obj True Start recursive comparison result error = compare_objects gpu_state_dict cpu_state_dict result error dataclasses dataclass TestStruct tensor torch Tensor dataclasses dataclass NestedTensorStruct tensor torch Tensor value int = dataclasses dataclass ComplexDataClass tensor torch Tensor name str values list float nested NestedTensorStruct dataclasses dataclass frozen=True FrozenDataClass tensor torch Tensor value int = TestStateDictStager TestCase unittest skipIf HAS_ACCELERATOR No accelerator test_views test_configs = False False pin_memory=False share_memory=False True False pin_memory=True share_memory=False False True pin_memory=False share_memory=True True True pin_memory=True share_memory=True pin_memory share_memory test_configs subTest pin_memory=pin_memory share_memory=share_memory tensor = torch randn device_type tensor = tensor view tensor = torch randn device_type state_dict = tensor tensor tensor tensor recursive tensor tensor type TestStruct tensor =tensor narrow assert state_dict tensor storage data_ptr == state_dict tensor storage data_ptr stager = StateDictStager pin_memory=pin_memory share_memory=share_memory cpu_state_dict = stager stage state_dict Calculate stats num_storages = len stager _cached_storage_mapping num_bytes = sum storage nbytes storage stager _cached_storage_mapping values Validate tensor count bytes expected_storage_cnt = assert num_storages == expected_storage_cnt f Expected expected_storage_cnt storages got num_storages Calculate expected bytes Note Only unique storages counted byte count expected_bytes = tensor numel tensor element_size + tensor numel tensor tensor share storage tensor element_size tensor its narrow view share storage assert num_bytes == expected_bytes f Expected expected_bytes bytes got num_bytes Verify CPU state dict equivalent original GPU state dict result error = compare_state_dicts state_dict cpu_state_dict assert result f State dicts equivalent error Additional checks storage sharing assert cpu_state_dict tensor device == torch device cpu assert cpu_state_dict tensor device == torch device cpu assert cpu_state_dict tensor storage data_ptr == cpu_state_dict tensor storage data_ptr recursive = cpu_state_dict recursive assert recursive tensor device == torch device cpu assert recursive type tensor device == torch device cpu assert recursive tensor storage data_ptr == recursive type tensor storage data_ptr unittest skipIf HAS_ACCELERATOR No accelerator test_caching Test StateDictStager correctly caches reuses storages test_configs = False False pin_memory=False share_memory=False True False pin_memory=True share_memory=False False True pin_memory=False share_memory=True True True pin_memory=True share_memory=True pin_memory share_memory test_configs subTest pin_memory=pin_memory share_memory=share_memory Create test tensors state dict tensor = torch randn device_type tensor = tensor view tensor = torch randn device_type state_dict = tensor tensor tensor tensor recursive tensor tensor type TestStruct tensor =tensor narrow Create StateDictStager instance stager = StateDictStager pin_memory=pin_memory share_memory=share_memory First call stage staging context cpu_state_dict = stager stage state_dict Get number cached storages after first stage num_storages = len stager _cached_storage_mapping Verify first result correct result error = compare_state_dicts state_dict cpu_state_dict assert result f First state dict equivalent original error Modify original tensors tensor fill_ tensor fill_ Second call stage staging context cpu_state_dict = stager stage state_dict Get number cached storages after second stage num_storages = len stager _cached_storage_mapping Verify second CPU state dict equivalent modified original state dict result error = compare_state_dicts state_dict cpu_state_dict assert result f Second state dict equivalent modified original error Verify number cached storages hasn t changed assert num_storages == num_storages f Storage count changed num_storages vs num_storages Verify tensors second state dict have same storage pointers first assert cpu_state_dict tensor storage data_ptr == cpu_state_dict tensor storage data_ptr Storage pointers should match tensor assert cpu_state_dict tensor storage data_ptr == cpu_state_dict tensor storage data_ptr Storage pointers should match tensor assert cpu_state_dict recursive tensor storage data_ptr == cpu_state_dict recursive tensor storage data_ptr Storage pointers should match tensor Modify original tensors again different values tensor fill_ Third call stage staging context cpu_state_dict = stager stage state_dict Verify third CPU state dict reflects updated values assert torch all cpu_state_dict tensor == Updated values should reflected cached state dict assert torch all cpu_state_dict tensor == Updated values should reflected cached state dict unittest skipIf HAS_ACCELERATOR No accelerator test_tensor_attrs Test tensor attributes preserved during stage StateDictStager tensor = torch randn device_type tensor = tensor view tensor = torch randn device_type Add custom attributes tensors tensor = tensor b = tensor c = state_dict = tensor tensor tensor tensor recursive tensor tensor type TestStruct tensor =tensor narrow stager = StateDictStager pin_memory=True share_memory=True cpu_state_dict = stager stage state_dict Verify tensor attributes preserved assert hasattr cpu_state_dict tensor Tensor attribute preserved assert cpu_state_dict tensor == Tensor attribute has incorrect value assert hasattr cpu_state_dict tensor b Tensor attribute b preserved assert cpu_state_dict tensor b == Tensor attribute b has incorrect value assert hasattr cpu_state_dict recursive tensor c Tensor attribute c preserved assert cpu_state_dict recursive tensor c == Tensor attribute c has incorrect value unittest skipIf HAS_ACCELERATOR No accelerator test_different_dtypes Test StateDictStager works correctly tensors different data types Create tensors different dtypes tensors = float torch randn dtype=torch float device_type float torch randn dtype=torch float device_type int torch randint - dtype=torch int device_type int torch randint - dtype=torch int device_type bool torch randint dtype=torch bool device_type Create state dict these tensors state_dict = tensors copy stager = StateDictStager cpu_state_dict = stager stage state_dict Verify all tensors have been correctly copied CPU right dtypes dtype_name original_tensor tensors items cpu_tensor = cpu_state_dict dtype_name assertEqual cpu_tensor device type cpu f Tensor dtype_name should CPU assertEqual cpu_tensor dtype original_tensor dtype f Tensor dtype_name has incorrect dtype assertTrue torch allclose cpu_tensor original_tensor cpu f Tensor dtype_name has incorrect values unittest skipIf HAS_ACCELERATOR No accelerator test_empty_tensors Test StateDictStager works correctly empty tensors test_configs = False False pin_memory=False share_memory=False True False pin_memory=True share_memory=False False True pin_memory=False share_memory=True True True pin_memory=True share_memory=True pin_memory share_memory test_configs subTest pin_memory=pin_memory share_memory=share_memory Create empty tensors different shapes tensors = empty_ d torch tensor dtype=torch float device_type empty_ d torch tensor dtype=torch float reshape device_type empty_ d torch tensor dtype=torch float reshape device_type empty_ d torch tensor dtype=torch float reshape device_type zero_dim torch tensor device_type scalar tensor Create state dict these tensors state_dict = tensors copy cpu_state_dict = StateDictStager pin_memory share_memory stage state_dict Verify all tensors have been correctly copied CPU tensor_name original_tensor tensors items cpu_tensor = cpu_state_dict tensor_name assertEqual cpu_tensor device type cpu f Tensor tensor_name should CPU assertEqual cpu_tensor shape original_tensor shape f Tensor tensor_name has incorrect shape assertEqual cpu_tensor dtype original_tensor dtype f Tensor tensor_name has incorrect dtype unittest skipIf HAS_ACCELERATOR No accelerator test_complex_storage_sharing Test StateDictStager correctly handles complex storage sharing scenarios Create base tensor base_tensor = torch randn device_type Create various views slices share storage view = base_tensor view view = base_tensor view slice = base_tensor slice = base_tensor slice = view Create state dict these tensors state_dict = base base_tensor view view view view slice slice slice slice slice slice cpu_state_dict = StateDictStager stage state_dict Verify all tensors have been correctly copied CPU result error = compare_state_dicts state_dict cpu_state_dict assertTrue result f State dicts equivalent error Verify storage sharing preserved All these tensors should share same storage storage_ptr = cpu_state_dict base storage data_ptr assertEqual cpu_state_dict view storage data_ptr storage_ptr view should share storage base assertEqual cpu_state_dict view storage data_ptr storage_ptr view should share storage base assertEqual cpu_state_dict slice storage data_ptr storage_ptr slice should share storage base assertEqual cpu_state_dict slice storage data_ptr storage_ptr slice should share storage base assertEqual cpu_state_dict slice storage data_ptr storage_ptr slice should share storage base Verify modifying base tensor affects all views slices cpu_state_dict base fill_ assertTrue torch all cpu_state_dict view == view should reflect changes base assertTrue torch all cpu_state_dict view == view should reflect changes base assertTrue torch all cpu_state_dict slice == slice should reflect changes base assertTrue torch all cpu_state_dict slice == slice should reflect changes base assertTrue torch all cpu_state_dict slice == slice should reflect changes base unittest skipIf HAS_ACCELERATOR No accelerator test_dataclasses Create tensors tensor = torch randn device_type tensor = torch randn device_type tensor = torch randn device_type tensor = torch randn device_type Create dataclass instances nested = NestedTensorStruct tensor=tensor complex_dc = ComplexDataClass tensor=tensor name= test values= nested=nested frozen_dc = FrozenDataClass tensor=tensor Create state dict these dataclasses state_dict = regular_tensor tensor complex_dataclass complex_dc frozen_dataclass frozen_dc Stage state dict stager = StateDictStager pin_memory=False share_memory=False cpu_state_dict = stager stage state_dict Verify regular tensor assertEqual cpu_state_dict regular_tensor device type cpu assertTrue torch allclose cpu_state_dict regular_tensor tensor cpu Verify complex dataclass complex_cpu = cpu_state_dict complex_dataclass assertEqual complex_cpu name test assertEqual complex_cpu values assertEqual complex_cpu tensor device type cpu assertTrue torch allclose complex_cpu tensor tensor cpu Verify nested dataclass inside complex dataclass nested_cpu = complex_cpu nested assertEqual nested_cpu value assertEqual nested_cpu tensor device type cpu assertTrue torch allclose nested_cpu tensor tensor cpu Verify frozen dataclass frozen_cpu = cpu_state_dict frozen_dataclass assertEqual frozen_cpu value assertEqual frozen_cpu tensor device type cpu assertTrue torch allclose frozen_cpu tensor tensor cpu Verify modifying original tensors doesn t affect staged ones tensor fill_ tensor fill_ tensor fill_ assertFalse torch allclose complex_cpu tensor tensor cpu assertFalse torch allclose nested_cpu tensor tensor cpu assertFalse torch allclose frozen_cpu tensor tensor cpu test_cpu_storage_independence Test ensures CPU tensors passed StateDictStager actually cloned Create test tensors tensor = torch randn tensor = torch randn Create state dict these tensors state_dict = tensor tensor tensor tensor cpu_state_dict = StateDictStager stage state_dict cpu_tensor = cpu_state_dict tensor cpu_tensor = cpu_state_dict tensor Verify CPU tensors have different storage pointers than original tensors assertNotEqual tensor storage data_ptr cpu_tensor storage data_ptr CPU tensor should have different storage pointer than original tensor assertNotEqual tensor storage data_ptr cpu_tensor storage data_ptr CPU tensor should have different storage pointer than original tensor assertTrue torch allclose tensor cpu_tensor CPU tensor should have same values original tensor assertTrue torch allclose tensor cpu_tensor CPU tensor should have same values original tensor Modify original CPU tensors validate staged tensors modified cloned_orginial = tensor clone cloned_orginia = tensor clone tensor fill_ tensor fill_ assertFalse torch allclose cloned_orginial tensor assertTrue torch allclose cloned_orginial cpu_tensor CPU tensor should have same values original tensor assertTrue torch allclose cloned_orginia cpu_tensor CPU tensor should have same values original tensor unittest skipIf HAS_ACCELERATOR No accelerator test_tensor_pinned_and_shared Test verifies tensors actually pinned shared using tensor is_pinned tensor is_shared methods Create test tensors tensor = torch randn device_type tensor = torch randn device_type Create state dict these tensors state_dict = tensor tensor tensor tensor Test all combinations pin_memory share_memory test_configs = False False pin_memory=False share_memory=False True False pin_memory=True share_memory=False False True pin_memory=False share_memory=True True True pin_memory=True share_memory=True pin_memory share_memory test_configs subTest pin_memory=pin_memory share_memory=share_memory Create stager specific configuration stager = StateDictStager pin_memory=pin_memory share_memory=share_memory cpu_state_dict = stager stage state_dict Get staged tensors cpu_tensor = cpu_state_dict tensor cpu_tensor = cpu_state_dict tensor Verify tensor device assertEqual cpu_tensor device type cpu Staged tensor should CPU assertEqual cpu_tensor device type cpu Staged tensor should CPU Verify tensor values assertTrue torch allclose cpu_tensor tensor cpu CPU tensor should have same values original tensor assertTrue torch allclose cpu_tensor tensor cpu CPU tensor should have same values original tensor Verify pinned memory status assertEqual cpu_tensor is_pinned pin_memory f Tensor pinned status should pin_memory assertEqual cpu_tensor is_pinned pin_memory f Tensor pinned status should pin_memory Verify shared memory status assertEqual cpu_tensor is_shared share_memory f Tensor shared status should share_memory assertEqual cpu_tensor is_shared share_memory f Tensor shared status should share_memory Verify storage sharing consistent tensor sharing share_memory When share_memory True storage should also shared assertTrue cpu_tensor storage is_shared When share_memory=True tensor storage should shared assertTrue cpu_tensor storage is_shared When share_memory=True tensor storage should shared When share_memory False storage should shared assertFalse cpu_tensor storage is_shared When share_memory=False tensor storage should shared assertFalse cpu_tensor storage is_shared When share_memory=False tensor storage should shared TestDTensorStateDictStager DTensorTestBase with_comms requires_accelerator_dist_backend skip_if_lt_x_gpu test_dtensor Test StateDictStager works correctly DTensors Create DTensor device_mesh = dist DeviceMesh device_type list range dist get_world_size tensor = torch randn device=self device_type dtensor = DTensor from_local tensor device_mesh Shard dtensor = dtensor + dtensor = dtensor state_dict = dtensor dtensor stager = StateDictStager pin_memory=True share_memory=True cpu_state_dict = stager stage state_dict Verify original DTensor has expected values assertTrue torch allclose dtensor to_local tensor + assertTrue torch allclose cpu_state_dict dtensor to_local dtensor to_local cpu assertEqual cpu_state_dict dtensor _spec dtensor _spec assertEqual cpu_state_dict dtensor size dtensor size TestReplicationStager DTensorTestBase Test suite _ReplicationStager functionality Tests replication state_dict across training ranks using CPU tensors only property backend - str cpu gloo cuda nccl _create_simple_state_dict rank int - dict Create simple state_dict CPU tensors deterministically unique per rank Args rank The rank number create unique tensors Returns dict A state dictionary CPU tensors Create unique tensors each rank torch manual_seed + rank Different seed per rank layer weight torch randn device= cpu layer bias torch randn device= cpu layer weight torch randn device= cpu layer bias torch randn device= cpu nested param torch randn device= cpu buffer torch randn device= cpu scalar torch tensor float rank device= cpu _verify_simple_state_dict_replication replicated_dict dict rank int partner_rank int Verify replication worked correctly Args replicated_dict The replicated state_dict received partner rank Current rank partner_rank Partner rank we should have received Create expected state_dict what partner rank would have created expected_dict = _create_simple_state_dict partner_rank compare_tensors actual expected path= isinstance actual dict isinstance expected dict assertEqual actual keys expected keys f Keys mismatch path key actual compare_tensors actual key expected key f path key path key isinstance actual torch Tensor isinstance expected torch Tensor assertEqual actual device type cpu f Tensor path should CPU assertEqual actual shape expected shape f Shape mismatch path assertEqual actual dtype expected dtype f Dtype mismatch path assertTrue torch equal actual expected f Values mismatch path assertEqual actual expected f Value mismatch path compare_tensors replicated_dict expected_dict _create_dtensor_state_dict rank int device_mesh DeviceMesh - dict Create state_dict DTensor regular tensors deterministic testing due DTensor Shard Replicate placements Args rank Current rank device_mesh DeviceMesh DTensor creation Returns dict State dictionary DTensors Create large global tensor deterministic values Each position contains unique value encodes both position rank info global_size = global_tensor = torch arange global_size dtype=torch float reshape global_size Create DTensor Shard - each rank gets different rows sharded_dtensor = distribute_tensor global_tensor device_mesh Shard Create DTensor Replicate - all ranks have same data replicated_global = torch full float global_size dtype=torch float device= cpu replicated_dtensor = distribute_tensor replicated_global device_mesh Replicate sharded_param sharded_dtensor replicated_param replicated_dtensor rank_scalar torch tensor float rank device= cpu _verify_dtensor_replication replicated_dict dict rank int partner_rank int Verify DTensor replication accuracy checking local shards global reconstruction Args replicated_dict Replicated state_dict received partner rank Current rank partner_rank Partner rank we should have received Verify sharded DTensor sharded_param replicated_dict replicated_sharded = replicated_dict sharded_param assertIsInstance replicated_sharded DTensor Should receive DTensor Get local shard replicated DTensor replicated_local = replicated_sharded to_local Create expected local shard what partner rank would have expected_global = torch arange dtype=torch float reshape Calculate expected shard rank s position world_size = dist get_world_size shard_size = world_size start_idx = partner_rank shard_size end_idx = partner_rank + shard_size expected_local = expected_global start_idx end_idx assertTrue torch equal replicated_local expected_local Sharded DTensor value mismatch Verify DTensor metadata preserved assertEqual replicated_sharded _spec placements __class__ __name__ Shard DTensor should maintain Shard placement Verify replicated DTensor replicated_param replicated_dict replicated_replicated = replicated_dict replicated_param assertIsInstance replicated_replicated DTensor Should receive DTensor Get local data replicated DTensor replicated_local = replicated_replicated to_local Expected value should global_size expected_value = float expected_tensor = torch full expected_value dtype=torch float device= cpu assertTrue torch equal replicated_local expected_tensor Replicated DTensor value mismatch Verify DTensor metadata preserved assertEqual replicated_replicated _spec placements __class__ __name__ Replicate DTensor should maintain Replicate placement Verify regular tensors rank_scalar replicated_dict assertEqual replicated_dict rank_scalar item float partner_rank f Rank scalar should partner_rank got replicated_dict rank_scalar item _create_sharded_tensor_state_dict rank int world_size int - dict Create state_dict ShardedTensor deterministic testing Args rank Current rank world_size Total world size Returns dict State dictionary ShardedTensor Create deterministic local shard rank global_size = shard_size = global_size world_size start_idx = rank shard_size end_idx = rank + shard_size Create local tensor deterministic values local_tensor = torch arange start_idx end_idx dtype=torch float device= cpu reshape shard_size Create ShardedTensor using init_from_local_shards sharded_tensor = init_from_local_shards ShardedTensorShard tensor=local_tensor metadata=ShardMetadata shard_offsets= start_idx shard_sizes= shard_size placement=f rank rank cpu global_size sharded_tensor sharded_tensor rank_scalar torch tensor float rank device= cpu _verify_sharded_tensor_replication replicated_dict dict rank int partner_rank int Verify ShardedTensor replication accuracy checking local shards metadata Args replicated_dict Replicated state_dict received partner rank Current rank partner_rank Partner rank we should have received Verify sharded tensor sharded_tensor replicated_dict replicated_sharded = replicated_dict sharded_tensor assertIsInstance replicated_sharded ShardedTensor Should receive ShardedTensor Get local shard replicated ShardedTensor local_shards = replicated_sharded local_shards assertEqual len local_shards Should have exactly one local shard local_shard = local_shards replicated_local = local_shard tensor Create expected local shard what partner rank would have world_size = dist get_world_size global_size = shard_size = global_size world_size start_idx = partner_rank shard_size end_idx = partner_rank + shard_size expected_local = torch arange start_idx end_idx dtype=torch float device= cpu reshape shard_size assertTrue torch equal replicated_local expected_local Sharded tensor value mismatch Verify shard metadata preserved expected_metadata = ShardMetadata shard_offsets= start_idx shard_sizes= shard_size placement=f rank partner_rank cpu assertEqual local_shard metadata shard_offsets expected_metadata shard_offsets Shard offsets should match assertEqual local_shard metadata shard_sizes expected_metadata shard_sizes Shard sizes should match Verify regular tensors rank_scalar replicated_dict assertEqual replicated_dict rank_scalar item float partner_rank f Rank scalar should partner_rank got replicated_dict rank_scalar item with_comms skip_if_lt_x_gpu test_replication_basic Test basic replication functionality world_size= world_size = dist get_world_size current_rank = dist get_rank Create unique DTensor state_dict rank state_dict = _create_simple_state_dict current_rank Initialize replication stager stager = _ReplicationStager pg=dist new_group backend=dist Backend GLOO timeout=timedelta seconds= device=torch device cpu Perform replication replicated_dict = stager stage state_dict Calculate expected partner rank partner_rank = current_rank + world_size world_size Verify DTensor replication _verify_simple_state_dict_replication replicated_dict current_rank partner_rank Clean up stager close with_comms skip_if_lt_x_gpu test_replication_dtensors Test replication DTensor mixed tensor types world_size = dist get_world_size current_rank = dist get_rank Create CPU-based DeviceMesh DTensor device_mesh = DeviceMesh cpu list range world_size Create DTensor state_dict which includes different tensor types state_dict = _create_dtensor_state_dict current_rank device_mesh Initialize replication stager stager = _ReplicationStager pg=dist group WORLD timeout=timedelta seconds= device=torch device cpu Perform replication result = stager stage state_dict Wait completion concurrent futures Future isinstance result Future replicated_dict = result result replicated_dict = result Calculate expected partner partner_rank = current_rank + world_size world_size Verify all DTensor types correctly replicated _verify_dtensor_replication replicated_dict current_rank partner_rank Clean up stager close with_comms skip_if_lt_x_gpu test_replication_sharded_tensors Test replication ShardedTensor mixed tensor types world_size = dist get_world_size current_rank = dist get_rank Create ShardedTensor state_dict rank state_dict = _create_sharded_tensor_state_dict current_rank world_size Initialize replication stager stager = _ReplicationStager pg=dist group WORLD timeout=timedelta seconds= device=torch device cpu Perform replication result = stager stage state_dict Wait completion concurrent futures Future isinstance result Future replicated_dict = result result replicated_dict = result Calculate expected partner partner_rank = current_rank + world_size world_size Verify all ShardedTensor types correctly replicated _verify_sharded_tensor_replication replicated_dict current_rank partner_rank Clean up stager close with_comms skip_if_lt_x_gpu test_replication_persistence Test persistence functionality _ReplicationStager world_size = dist get_world_size current_rank = dist get_rank Test Default storage directory auto-generated tempdir tempfile TemporaryDirectory _ Create state_dict rank state_dict = _create_simple_state_dict current_rank Initialize stager default storage_dir None stager = _ReplicationStager pg=dist group WORLD timeout=timedelta seconds= device=torch device cpu storage_dir=None Let create its own tempdir Perform replication trigger persistence stager stage state_dict Calculate expected partner rank partner_rank = current_rank + world_size world_size Verify file created correct naming convention expected_path = stager _get_persisted_path current_rank partner_rank assertTrue os path exists expected_path f Persisted file should exist expected_path Verify storage directory created assertTrue os path isdir stager _storage_dir Storage directory should exist assertTrue stager _storage_dir startswith tempfile gettempdir Default storage directory should system temp directory Load verify persisted state_dict matches received one loaded_state_dict = torch load expected_path _verify_simple_state_dict_replication loaded_state_dict current_rank partner_rank Clean up stager close Test Custom storage directory tempfile TemporaryDirectory custom_storage_dir Create custom subdirectory custom_subdir = os path join custom_storage_dir custom_replication_test Create state_dict rank state_dict = _create_simple_state_dict current_rank Initialize stager custom storage_dir stager = _ReplicationStager pg=dist group WORLD timeout=timedelta seconds= device=torch device cpu storage_dir=custom_subdir Perform replication trigger persistence stager stage state_dict Verify custom storage directory created used assertEqual stager _storage_dir custom_subdir Should use custom storage directory assertTrue os path isdir custom_subdir Custom storage directory should created Verify file created custom directory expected_path = stager _get_persisted_path current_rank partner_rank assertTrue os path exists expected_path f Persisted file should exist custom directory expected_path Load verify persisted state_dict loaded_state_dict = torch load expected_path _verify_simple_state_dict_replication loaded_state_dict current_rank partner_rank Clean up stager close __name__ == __main__ run_tests