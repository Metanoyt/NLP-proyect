Owner s module dynamo flake noqa E C F C C collections collections abc contextlib functools inspect itertools keyword math operator random sys types typing unittest dataclasses dataclass field typing Any Generic TypeVar typing_extensions NamedTuple unittest mock patch numpy np torch torch _dynamo test_case torch _dynamo testing torch sub torch _dynamo exc Unsupported torch _dynamo testing CompileCounterWithBackend EagerAndRecordGraphs normalize_gm torch _dynamo utils ifdynstaticdefault range_iterator same torch _dynamo variables ConstantVariable SkipFunctionVariable torch _dynamo variables lists RangeVariable torch nn functional F torch testing _internal common_cuda TEST_MULTIGPU torch testing _internal common_utils instantiate_parametrized_tests parametrize torch testing _internal inductor_utils HAS_GPU Defines all kernels tests torch testing _internal triton_utils noqa F device_type = acc type acc = torch accelerator current_accelerator True cpu T = TypeVar T d = torch ones e = torch nn Linear flag = True CustomDictSubclass collections OrderedDict pass clip = functools partial torch clip min= max= constant b - b + + call f args kwargs f args kwargs _variable = update_global x global _variable _variable += Check updated global variable value picked up x _variable pos_only_fn args kwargs _pos_only_fn args kwargs _pos_only_fn b= kwargs b + kwargs get - kwargs get b kwargs b kwargs contextlib contextmanager update_global_ctx x try yield update_global x finally pass func_with_default b some_default_arg=True some_default_arg - b make_test fn=None expected_frame_count= fn None lambda fn make_test fn expected_frame_count=expected_frame_count nargs = len inspect signature fn parameters test_fn torch _dynamo testing standard_test fn=fn nargs=nargs expected_frame_count=expected_frame_count test_fn MyCls = torch jit script_if_tracing inline_script_if_tracing x x + torch jit ignore inline_ignore x x + torch jit unused inline_unused x x + functools lru_cache inline_lru_cache_fn_with_default_args x y _=None torch sin x y torch jit script_if_tracing inline_script_if_tracing_fn_with_default_args x y c= torch cos x y + c FunctionTests torch _dynamo test_case TestCase make_test test_inline_jit_annotations x x = inline_script_if_tracing x x = inline_ignore x x = inline_unused x make_test test_inline_script_if_tracing_fn_with_default_args b inline_script_if_tracing_fn_with_default_args b make_test test_inline_lru_cache_fn_with_default_args b inline_lru_cache_fn_with_default_args b test_lru_cache_warning_issued_during_tracing warnings functools lru_cache lru_cache foo x x + warnings catch_warnings record=True w warnings simplefilter always torch compile foo backend= eager torch randn warning w warning_message = str warning message Dynamo detected call ` functools lru_cache ` -wrapped warning_message break assertTrue False Expected warning about lru_cache found make_test test_add b + b make_test test_add_ b a_copy = torch tensor a_copy add_ b alpha= make_test test_addcdiv b c dynamo decomposes avoid graph break when value kwarg populated torch addcdiv b c value= make_test test_addcdiv_ b c a_copy = torch tensor a_copy addcdiv_ b c value= make_test test_is_not_null b None b None + b test_foreach_lerp_ fn x y s torch _foreach_lerp_ x y s cnt = torch _dynamo testing CompileCounter fn_opt = torch compile backend=cnt fullgraph=True fn expected = fn torch ones torch ones torch ones torch ones torch tensor actual = fn_opt torch ones torch ones torch ones torch ones torch tensor assertTrue same expected actual test_broadcast_foreach_pow torch _dynamo utils same fn x y torch _foreach_pow x y cnt = torch _dynamo testing CompileCounter fn_opt = torch compile backend=cnt fullgraph=True fn inps = torch tensor torch tensor torch tensor actual = fn_opt inps expected = fn inps assertTrue same actual expected assertTrue cnt frame_count test_addcmul_ copy deepcopy torch _dynamo utils same fn x y z s x addcmul_ y z value=s cnt = torch _dynamo testing CompileCounter fn_opt = torch compile backend=cnt fullgraph=True fn inps = torch ones torch ones + torch rand torch tensor inps_ = deepcopy inps actual = fn_opt inps expected = fn inps_ assertTrue same actual expected assertEqual cnt frame_count make_test test_functools_partial b clip + b make_test test_itertools_product b v = x i itertools product b v = v + x i v test_itertools_product_args torch compile backend= eager fullgraph=True fn args kwargs torch tensor list itertools product args kwargs assertRaises Unsupported fn fake_arg= make_test test_itertools_product_various_iterators b itertools product b zip map lambda x x filter lambda x True test_itertools_permutations_basic fn torch tensor list itertools permutations actual = torch compile fn backend= eager fullgraph=True expected = fn assertEqual actual expected test_itertools_permutations_args torch compile backend= eager fullgraph=True fn args kwargs torch tensor list itertools permutations args kwargs assertRaises Unsupported fn assertRaises Unsupported fn assertRaises Unsupported fn fake_arg= make_test test_itertools_permutations_various_iterators b itertools permutations b itertools permutations zip itertools permutations map lambda x x itertools permutations filter lambda x True make_test test_itertools_filterfalse_basic b x itertools filterfalse lambda x x - += x make_test test_itertools_chain b v = x itertools chain b v = v + x v make_test test_itertools_chain_from_iterable b v = x itertools chain from_iterable b v = v + x v test_itertools_reconstruct fn = itertools repeat = itertools count _ range += next += next opt_fn = torch compile fn backend= eager fullgraph=True i i = fn torch ones b = opt_fn torch ones assertEqual next i next assertEqual next i next assertEqual b make_test test_obj_eq b v = + b MyCls == None noqa E - MyCls = None noqa E v = v sin MyCls == MyCls - MyCls = MyCls v + - make_test test_cls_eq b v = + b MyCls == None noqa E - MyCls = None noqa E v = v sin MyCls = MyCls - MyCls == MyCls v + - make_test test_obj_is b v = + b MyCls None noqa E - MyCls None noqa E v = v sin MyCls MyCls - MyCls MyCls v + - make_test test_cls_is b v = + b MyCls None noqa E - MyCls None noqa E v = v sin MyCls MyCls - MyCls MyCls v + - make_test test_itertools_combinations b combs = size itertools combinations combs append torch ones size combs make_test test_itertools_pairwise pairs = size itertools pairwise pairs append torch ones size pairs test_itertools_compress fn itertools compress ABCDEF opt_fn = torch compile fn backend= eager fullgraph=True assertListEqual list opt_fn list fn test_itertools_compress_tensors fn itertools compress torch tensor torch tensor torch tensor opt_fn = torch compile fn backend= eager fullgraph=True assertListEqual list opt_fn list fn make_test test_np_iinfo max_dim = np iinfo np int max + max_dim make_test test_np_finfo min_dim = np finfo np float min + min_dim make_test test_constant b c - b c + make_test test_constant b c - b c + make_test test_constant b = c = d = b + c - d + make_test test_constant b c = d = c d - b b - make_test test_cls_hasattr x hasattr MyCls x = x + hasattr MyCls b x = x + x make_test test_finfo b torch iinfo torch int bits == torch finfo dtype min b make_test test_globalfn b sub b make_test test_viatorch b torch sub b make_test test_viamethod b sub b make_test test_indirect b t = sub t b make_test test_indirect b t = sub args = b t args make_test test_indirect b t = sub args = b kwargs = t args kwargs make_test test_methodcall b c constant b c make_test test_methodcall b constant a=b b=a + make_test test_methodcall b constant b= + b test_is_integer torch compile backend= eager fullgraph=True forward t m t m is_integer t t = torch tensor assertEqual forward t item assertEqual forward t item parametrize method num_type as_integer_ratio int bit_length int conjugate int as_integer_ratio float conjugate float hex float is_integer float test_number_method method num_type forward t m t getattr m method t wrapped = torch compile backend= eager fullgraph=True forward i m = num_type i t = torch tensor actual = wrapped t m expected = forward t m assertEqual actual expected make_test test_device_constant + torch ones device=torch device cpu make_test test_tuple b args = b sub args make_test test_tuple b args = b sub args make_test test_tuple_map b t = tuple map torch sin b t + t test_size_tuple_add fn size = torch Size assert isinstance size + size torch Size assert isinstance size + tuple assert isinstance size + torch Size fn compiled_fn = torch compile fn backend= eager fullgraph=True compiled_fn make_test test_is_in_onnx_export x y torch onnx is_in_onnx_export x - y + make_test test_is_fx_tracing x y torch fx _symbolic_trace is_fx_tracing x - y + make_test test_listarg b torch cat b make_test test_listarg b torch cat b dim= make_test test_listarg b kwargs = tensors b dim torch cat kwargs make_test test_listarg b torch cat tensors= b dim= make_test test_listarg b args = b kwargs = dim torch cat args kwargs test_list_slice Mock __init__ ets = counter = torch compile backend= eager run x ets = ets - ets append x torch sin x mock = Mock mock run torch randn assertEqual len mock ets make_test test_deque b d = collections deque b d append + d extend b d insert foo tmp = d pop another_deque = collections deque tmp d extendleft another_deque another_deque clear d extend another_deque d = setitem d = d copy d append d popleft empty = collections deque d extend empty d make_test test_slice make_test test_slice make_test test_slice make_test test_slice make_test test_slice make_test test_slice torch unsqueeze make_test test_range torch tensor range size make_test test_range x y r = x + y _ range x size + r = r y r make_test test_unpack b = - b make_test test_unpack packed = b = packed - b make_test test_unpack packed = b = packed - b make_test test_fn_with_self_set b avg_pool d odd one __self__ set F avg_pool d torch unsqueeze torch unsqueeze b kernel_size= padding= make_test test_return_tuple b - b b - b make_test test_globalvar b - b + d make_test test_globalmodule x e x make_test test_inline_with_default b c func_with_default b c make_test test_inner_function x fn x torch add x x fn x make_test test_transpose_for_scores x new_x_shape = x size - + x = x view new_x_shape x permute make_test test_return_tuple x torch add x x x make_test test_load_global_bool x flag torch add x x x make_test test_len_tensor x z = len x torch add x z make_test test_len_constant_list x z = len torch add x z make_test test_len_constant_dict x z = len foo bar torch add x z make_test test_dict_copy x z = dict foo x + z make_test test_dict_keys x d = x keys = d keys d = x + d = aa keys keys keys d keys == keys make_test test_dict_values x d = x values = d values d = x + d = x + len values make_test test_dict_setdefault x d = b d setdefault d == x + x - make_test test_dict_setdefault x d = b d setdefault c d c == x + x - make_test test_dict_setdefault x d = b d setdefault c d c None x + x - make_test test_dict_update_kwargs x d = d update b= x d d b make_test test_defaultdict_setdefault x d = collections defaultdict fromkeys b d = d b = d setdefault d == x + x - make_test test_defaultdict_setdefault x d = collections defaultdict fromkeys b d = d b = d setdefault c d c == x + x - make_test test_defaultdict_setdefault x d = collections defaultdict fromkeys b d = d b = d setdefault c d c None x + x - test_dict_id_guard d = collections OrderedDict d = d fn x Iteration forces DictGuardManager k d x = x d k d k x opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn assertEqual fn x opt_fn x make_test test_callable_lambda x callable lambda x True x + x - make_test test_callable_torch x callable torch abs x + x - make_test test_callable_builtin x callable sum x + x - test_callable_class CallableClass __call__ pass NotCallableClass pass torch compile backend= eager fullgraph=True fn x arg callable arg x x + torch compile backend= eager fullgraph=True fn x arg callable arg x x + input = torch randn f fn fn assertEqual f input NotCallableClass input + assertEqual f input CallableClass input f fn input passing tensor scalars assertEqual f input input + assertEqual f input input + assertEqual f input True input + assertEqual f input input input + test_callable_list torch compile backend= eager fullgraph=True fn x arg callable arg x x + input = torch randn assertEqual fn input input + assertEqual fn input input + test_pos_only_args_with_same_name_in_star_kwargs opt_fn = torch compile pos_only_fn backend= eager fullgraph=True = torch randn b = torch randn x = torch randn y = torch randn assertEqual pos_only_fn opt_fn assertEqual pos_only_fn a=x opt_fn a=x assertEqual pos_only_fn b=y opt_fn b=y assertEqual pos_only_fn b=b a=x opt_fn b=b a=x assertEqual pos_only_fn a=x b=y opt_fn a=x b=y assertEqual pos_only_fn b a=x b=y opt_fn b a=x b=y make_test test_len_constant_misc_iterables x = len b = len test str c = + b torch add x c make_test test_dict_kwargs x z = dict text_embed=x + other=x + z make_test test_ordered_dict_kwargs x z = collections OrderedDict sample=torch ones z make_test test_custom_dict_kwargs x z = CustomDictSubclass sample=torch ones z make_test test_float x y = float noqa UP y += float torch add x y make_test test_is_floating_point x y = x + torch is_floating_point y torch is_floating_point input=y make_test test_dtype x x dtype == torch float x + make_test test_get_default_dtype x x dtype == torch get_default_dtype x + x - make_test test_get_autocast_gpu_dtype x dtype = torch get_autocast_gpu_dtype x type dtype make_test test_is_any_autocast_enabled x torch _C _is_any_autocast_enabled x + x - make_test test_is_checkpoint_valid x torch autograd _is_checkpoint_valid x + x - make_test test_list_compare_polyfill x b c - = b x = x + c == b x = x + c b x = x + c b x = x + c = b x = x + c = b x = x + c x make_test test_list_compare_polyfill_non_lists x conds = Non-list instances only work eq ne b c conds append = b conds - x = x + c conds append == b conds - x = x + c x conds make_test test_promote_types x x dtype == torch promote_types torch int torch float x + x - make_test test_cublas_allow_tf x torch backends cuda matmul allow_tf x sin + x cos - make_test test_get_calculate_correct_fan x fan_in = torch nn init _calculate_correct_fan x fan_in x + fan_in make_test test_is_complex x torch is_complex x x + x - make_test test_tensor_is_complex x x is_complex x + x - make_test test_tensor_size x fn = torch Tensor size fn x + make_test test_tensor_dim x fn = torch Tensor dim fn x + test_is_inference_recompilation fn x x is_inference x + x - torch inference_mode x_inference = torch randn cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts fullgraph=False x = torch randn assertEqual fn x opt_fn x assertEqual cnts frame_count assertEqual fn x_inference opt_fn x_inference assertEqual cnts frame_count Recompiles test_is_inference_mode_global_recompilation fn x torch is_inference_mode_enabled x + x - cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts fullgraph=False x = torch randn assertEqual fn x opt_fn x assertEqual cnts frame_count make_test test_get_privateuse _name x torch _C _get_privateuse _backend_name == privateuseone x + x - make_test test_device x x is_cuda x + unittest skipIf torch cuda is_available requires cuda make_test test_get_device_properties_tensor_device x = cuda prop = torch cuda get_device_properties x device prop major == x + prop multi_processor_count x + prop max_threads_per_multi_processor make_test test_tensor_type b m = torch float b type m type unittest skipIf HAS_GPU requires gpu make_test test_tensor_type b m = device_type m + b type m type make_test test_tensor_type b m = type torch HalfTensor b type m type make_test test_tensor_type b m = type torch HalfTensor b type m type unittest skipIf torch cuda is_available requires cuda make_test test_tensor_type b m = type torch cuda HalfTensor b type m type make_test test_tensor_element_size element_size + element_size - element_size - element_size + element_size make_test test_ndim x x ndim == x ndimension == x dim == x + make_test test_T x torch ones_like x T make_test test_mT x torch ones_like x mT make_test test_is_sparse x x is_sparse x + make_test test_shape x x shape == x + make_test test_shape x x size == x + make_test test_del b c = + d = c + del c b + d make_test test_chunks x chunk_size = assert x shape chunk_size == assert x shape chunk_size == x chunk_size - x chunk_size make_test test_import x y torch torch sub sub torch add x y y make_test test_isinstance x results = isinstance x list results append x sin results append x cos isinstance x tuple results append x sin results append x cos isinstance x collections abc Sequence results append x sin results append x cos isinstance x typing Sequence results append x sin results append x cos isinstance x tuple list typing Sequence results append x sin results append x cos TODO add sourceless builder types UnionType sys version_info = isinstance x list &#124; tuple results append x sin results append x cos results make_test test_return_dict x y z = x + y y False x x z z x b z c x make_test test_return_dict x y tmp = x x tmp z = x + y y tmp y = y tmp z append False tmp make_test test_funcdef_closure x y x = x + y + inner z nonlocal x y y = x + z + x = y + z + inner inner x y make_test test_module_constant x y r = x + y _ range torch _dynamo testing three r = r y r make_test test_inline_softmax x y This common some huggingface models torch nn Softmax dim=- x + y make_test test_dtype_compare b dtype == torch float + dtype == torch float - b make_test test_build_list_unpack b = x + x b = x - x b torch cat dim=- make_test test_tensor_len b + b + len + b __len__ make_test test_pop b ll = b ll append + ll extend b + + b ll pop - ll pop ll pop v v = ll v - v make_test test_list_convert b ll = + b ll = tuple ll tmp = b + ll = list ll v v = ll v - v + tmp make_test test_list_add b l = b l = being LOAD_CONST bytecode l = l + l l + l make_test test_list_index_with_constant_tensor b l = b + b + l torch as_tensor make_test test_startswith b x = + b foobar startswith foo test constant __module__ x = x + x make_test test_dict_ops b tmp = + b b + assert tmp get zzz None v = tmp pop b + tmp get + tmp get missing + tmp pop missing tmp update d tmp c = v + tmp d c tmp missing tmp tmp c - tmp + len tmp make_test test_inline_jit__unwrap_optional x torch jit _unwrap_optional x None torch ones x sin make_test test_zip_longest x list = list = b list = True False True False torch sin x + list itertools zip_longest list list list fillvalue=None test_torch_size_as_dict_key fn x cached x shape cached cached x shape = x x + cached x shape opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn x = torch randn cached = ref = fn x cached ref = fn x cached cached = res = opt_fn x cached res = opt_fn x cached assertEqual ref res assertEqual ref res test_dict_param_keys a_param = torch nn Parameter torch ones fn tmp = a_param tmp + tmp a_param test = make_test fn test test_dict_mutable_map collections abc MutableMapping TensorDict MutableMapping __init__ - None _dict = add key value _dict key = value items _dict items __delitem__ key del _dict key __getitem__ key _dict key __iter__ iter _dict __len__ len _dict __setitem__ key value _dict key = value tensor_dict = TensorDict tensor_dict add torch ones fn x copy_tensordict = dict tensor_dict x copy_tensordict opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn ref = fn x res = opt_fn x assertEqual ref res test_unpack_mutable_map collections abc MutableMapping TensorDict MutableMapping __init__ - None _dict = add key value _dict key = value items _dict items __delitem__ key del _dict key __getitem__ key _dict key __iter__ iter _dict __len__ len _dict __setitem__ key value _dict key = value tensor_dict = TensorDict tensor_dict add torch ones gn x a= x fn x gn x tensor_dict opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn ref = fn x res = opt_fn x assertEqual ref res _test_default_dict_helper factory dd = collections defaultdict factory param = torch nn Parameter torch ones fn x dd = x + dd param = dd c = x dd b dd x = torch randn ref = fn x opt_fn = torch _dynamo optimize_assert eager fn res = opt_fn x assertTrue same ref res assertTrue same ref res assertTrue same ref c res c assertTrue same ref param res param test_default_dict_dict _test_default_dict_helper dict test_default_dict_list _test_default_dict_helper list test_default_dict_tuple _test_default_dict_helper tuple test_default_dict_set _test_default_dict_helper set test_default_dict_lambda _test_default_dict_helper lambda dict noqa C test_default_dict_closure factory dict noqa C _test_default_dict_helper factory test_class_dict A x = y = __init__ - None = = A fn x x type __dict__ x + x + opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn assertEqual fn x opt_fn x test_default_dict_constr param = torch nn Parameter torch ones fn x dd = collections defaultdict lambda dict noqa C dd = x + dd param = dd c = x dd update b x dd update d x - e x + dd update zip ab x + x + dd b dd x = torch randn ref = fn x opt_fn = torch _dynamo optimize_assert eager fn res = opt_fn x assertTrue same ref res assertTrue same ref res assertTrue same ref b res b assertTrue same ref c res c assertTrue same ref d res d assertTrue same ref e res e assertTrue same ref param res param test_dict_tuple_lazy_guard torch compile backend= eager fn x y torch sin x y fn torch randn Changing value other key should causing recompilation unittest mock patch torch _dynamo config error_on_recompile True fn torch randn fn torch randn Changing value index should cause recompilation unittest mock patch torch _dynamo config error_on_recompile True fn torch randn make_test test_call_dict x d = dict noqa C d x = x + d = collections OrderedDict d x = x + d x + d x + make_test test_call_dict x d = dict noqa C d x = x d = collections OrderedDict d isinstance d collections OrderedDict x + x - make_test test_call_dict x my_list = x b x + c x + d = dict my_list d = x + d = collections OrderedDict my_list d c = x + d + d c + make_test test_call_dict x my_list = x b x + c x + d = dict my_list d = x + d = collections OrderedDict my_list d c = x + d + d c + make_test test_call_dict x my_list = iter x b x + c x + d = dict my_list d = x + d = collections OrderedDict my_list d c = x + d + d c + make_test test_dict_fromkeys x y lst = b d = dict fromkeys lst d = dict fromkeys d x + d = collections defaultdict fromkeys iter d x - d = collections OrderedDict fromkeys tuple lst value=y d d b + d + d b + d + d b + make_test test_dict_copy x my_list = x b x + c x + d = dict my_list d = x + d = d copy d = x - d b = x + d = collections OrderedDict my_list d c = x + d = d copy d c = x - d d + d b + d c d c + make_test test_dict_update x y z d = x b y d update y - d update b z + c z d update zip ab z + y + od = collections OrderedDict a=x b=y + od update y + od update b z + c z - od update zip ab z - x + d od + od c + d b + od b d c make_test test_min_max b c = + b = sum b = b sum = min max b = max min b max b - min b + c make_test test_symbool_to_int x roughly pattern found einops unpack sum s == - s x size == x + x - make_test test_map_sum b c d sum map lambda x x + b c d make_test test_sum b c d sum b c d make_test test_sum_with_start_arg b c d sum b c d make_test test_sum_with_start_kwarg b c d sum b c d start=a make_test expected_frame_count= test_sum_shortcut sum make_test expected_frame_count= test_sum_shortcut_with_start_arg sum - make_test expected_frame_count= test_sum_shortcut_with_start_kwarg sum start=- make_test test_reduce b c d functools reduce operator add b c d make_test test_reduce_with_initial b c d functools reduce operator add b c d make_test test_reduce_with_single x functools reduce lambda b b x make_test expected_frame_count= test_reduce_with_single_with_initial x y functools reduce lambda b b y x make_test expected_frame_count= test_reduce_with_none_initial x functools reduce lambda b b x None make_test test_tuple_contains b v = v = b v = c vals = v v v vals = d e f vals b vals + b - b make_test test_set_in_frozenset x var = set abc other = set frozenset abc var other x + x - make_test test_set_update_bytecode x This produces bytecode SET_UPDATE since python var = apple banana cherry isinstance var set x + x - make_test test_set_update_list_with_duplicated_items x list = apple banana apple list = orange banana len list list == x + x - test_set_keys_view collections abc KeysView StringKeys KeysView __init__ keys keys = keys __getitem__ key keys __getitem__ key __iter__ yield keys __repr__ f type __name__ keys __len__ len keys __contains__ item keys __contains__ item = StringKeys fn x set_a = set len set_a x opt_fn = torch compile fn backend= eager fullgraph=True x = torch rand assertEqual fn x opt_fn x test_constant_set s = set fn x torch cos x len s opt_fn = torch compile fn backend= eager fullgraph=True x = torch rand assertEqual fn x opt_fn x This should cause recompilation s add assertEqual fn x opt_fn x test_set_add s = set fn x s add torch cos x len x opt_fn = torch compile fn backend= eager fullgraph=True x = torch rand assertEqual fn x opt_fn x assertEqual len s make_test test_tuple_iadd b output = b output += + b - b output make_test test_unpack_ex x output = x x + x + x + b cd = output - b cd make_test test_unpack_ex x output = x x + x + x + ab c d = output c - d ab make_test test_unpack_ex x output = x x + x + x + bc d = output - d bc make_test test_const_tuple_add x output = x x + x + x + output = + output + output + output make_test test_const_tuple_add x output = x x + x + x + output = None + output + None output + output make_test test_list_truth b tmp = tmp + b - b make_test test_list_reversed b tmp = + + + + b + next iter reversed tmp make_test test_list_sorted x tmp = x + sorted tmp sorted tmp key=None sorted tmp reverse=True sorted tmp key=None reverse=True make_test test_list_sorted x y = john A jane B dave B x + sorted y sorted y key=None sorted y reverse=True sorted y key=None reverse=True sorted y key=lambda student student sorted y key=lambda student student reverse=True make_test test_tuple_sorted x tmp = x + sorted tmp sorted tmp key=None sorted tmp reverse=True sorted tmp key=None reverse=True make_test test_dict_sorted x tmp = D B E F x + sorted tmp sorted tmp key=None sorted tmp reverse=True sorted tmp key=None reverse=True make_test test_dict_items_sorted x tmp = D B E F x + ordered key value pair sorted tmp items sorted tmp items key=None sorted tmp items reverse=True sorted tmp items key=None reverse=True ordered key sorted tmp items key=operator itemgetter sorted tmp items key=operator itemgetter reverse=True ordered value sorted tmp items key=operator itemgetter sorted tmp items key=operator itemgetter reverse=True make_test test_sorted_const_key_non_const_items x y tmp = x x - x - y + torch ones sorted tmp sorted tmp items key=operator itemgetter sorted tmp items key=operator itemgetter reverse=True test_dict_hasattr fn x hasattr x x cpu hasattr x items torch cos x x opt_fn = torch compile fn backend= eager fullgraph=True x = dict a=torch randn assertEqual fn x opt_fn x x = torch randn assertEqual fn x opt_fn x make_test test_list_clear b tmp = + + tmp clear tmp append + b tmp make_test test_not_list + make_test test_islice_chain b tmp = + + tmp = + + b = list itertools islice itertools chain tmp tmp c = next itertools islice tmp None - b c make_test test_slice_eq b tmp = + b + s = slice isinstance s slice s == slice None tmp s tmp s make_test test_namedtuple b mytuple = collections namedtuple mytuple x y xy tmp = mytuple b + b mytuple tmp x tmp tmp xy + b make_test test_namedtuple_defaults b mytuple = collections namedtuple mytuple x y xy defaults= None None tmp = mytuple xy=b mytuple tmp x tmp tmp xy + b make_test test_namedtuple_replace b mytuple = collections namedtuple mytuple x y t = mytuple b t _replace x=b t x + t y make_test test_namedtuple_fields b mytuple = collections namedtuple mytuple x y mytuple _fields == x y + b - b MyNamedTuple NamedTuple first torch Tensor second torch Tensor add - torch Tensor first + second staticmethod static_method - int classmethod class_method cls - str cls __name__ MyGenericNamedTuple NamedTuple Generic T first T second T add - T first + second staticmethod static_method - int classmethod class_method cls - str cls __name__ MyNamedTupleSubclass MyNamedTuple pass MyGenericNamedTupleSubclass MyGenericNamedTuple T pass make_test test_namedtuple_user_methods b mytuple = FunctionTests MyNamedTuple b mytuple add mytuple static_method mytuple class_method make_test test_namedtuple_replace b mytuple = FunctionTests MyNamedTuple b replaced = mytuple _replace first=b mytuple first + mytuple second + replaced first + replaced second make_test test_generic_namedtuple_user_methods b mytuple = FunctionTests MyGenericNamedTuple b mytuple add mytuple static_method mytuple class_method make_test test_namedtuple_hasattr b mytuple = FunctionTests MyNamedTuple b isinstance_namedtuple obj - bool isinstance obj tuple hasattr obj _asdict hasattr obj _fields isinstance_namedtuple mytuple + b - b make_test test_generic_namedtuple_hasattr b mytuple = FunctionTests MyGenericNamedTuple b isinstance_namedtuple obj - bool isinstance obj tuple hasattr obj _asdict hasattr obj _fields isinstance_namedtuple mytuple + b - b make_test test_namedtuple_subclass b mytuple = FunctionTests MyNamedTupleSubclass b mytuple x = mytuple y = b mytuple z = b mytuple z = hasattr mytuple x mytuple x + mytuple y mytuple z make_test test_generic_namedtuple_subclass b mytuple = FunctionTests MyGenericNamedTupleSubclass b mytuple x = mytuple y = b mytuple z = b mytuple z = hasattr mytuple x mytuple x + mytuple y mytuple z make_test test_sourceless_build_method_type b cls = collections namedtuple Foo x y sourceless variable The type ` cls _make ` method type callable getattr cls _make None + b - b make_test test_torch_size_hasattr x hasattr x shape _fields x + x - make_test test_is_quantized b is_quantized + b make_test test_fstrings b x = tmp = f x f bar tmp startswith + b make_test test_fstrings x tmp = f x shape bar tmp startswith x + make_test test_fstrings x tmp = f x __class__ __name__ foo tmp startswith Tensor x + make_test test_fstrings x tmp = f x shape bar tmp x + make_test test_fstrings x tmp = f x shape bar tmp + haha x + make_test test_fstrings x tmp = f x shape + x shape tmp x + make_test test_tensor_new_with_size x y = torch rand z = x new y size assert z size == y size make_test test_tensor_new_with_shape x y = torch rand z = x new y shape assert z size == y size make_test test_jit_annotate x y = torch jit annotate Any x + y + make_test test_is_contiguous_memory_format tensor torch jit is_scripting None tensor is_contiguous memory_format=torch contiguous_format tensor + test_is_contiguous_frame_counts data = torch rand torch rand torch rand contiguous memory_format=torch channels_last torch rand torch rand torch rand contiguous memory_format=torch channels_last torch rand torch rand - - dynamo should recompile all inputs static shapes mode expected_frame_counts_static = dynamo should recompile items dynamic shapes mode expected_frame_counts_dynamic = expected_frame_counts = ifdynstaticdefault expected_frame_counts_static expected_frame_counts_dynamic dynamic = ifdynstaticdefault False True func x x is_contiguous x + x is_contiguous memory_format=torch channels_last x + x + cnt = torch _dynamo testing CompileCounter cfunc = torch _dynamo optimize_assert cnt dynamic=dynamic func assert cnt frame_count == i x enumerate data expected = func x output = cfunc x assertTrue same output expected assert cnt frame_count == expected_frame_counts i make_test test_list_slice_assignment x m = m = len m - x + make_test test_distributed_is_available x torch distributed is_available x + x - unittest skipIf torch distributed is_available requires distributed package make_test test_distributed_is_initialized x torch distributed is_initialized x + x - make_test test_torch_distributions_functions x normal = torch distributions Normal x torch tensor independent = torch distributions Independent normal independent log_prob x make_test test_context_wrapping_nested_functions_no_closure x torch no_grad augment x torch Tensor - torch Tensor x + augment x make_test test_match_sequence point = match point case case y - y case x + x case x y + x - y make_test test_match_mapping_and_match_keys x param = match param case param x param case b param x param test_math_radians func x x + math radians cnt = torch _dynamo testing CompileCounter cfunc = torch _dynamo optimize_assert cnt func assert cnt frame_count == x = torch rand expected = func x output = cfunc x assertTrue same output expected assert cnt frame_count == make_test test_numpy_meshgrid x y r r = np meshgrid x numpy y numpy torch from_numpy r torch from_numpy r make_test test_torch_from_numpy x = x numpy b = torch from_numpy b size == torch tensor True torch tensor False make_test test_numpy_size x = x numpy size make_test test_numpy_attributes x = x numpy itemsize strides shape ndim size torch from_numpy T torch from_numpy real torch from_numpy imag make_test test_mean_sum_np x torch Tensor x_mean = np mean x numpy x_sum = np sum x_mean x_sum_array = np asarray x_sum torch from_numpy x_sum_array make_test test_return_numpy_ndarray x = x numpy T make_test test_return_multiple_numpy_ndarray x = x numpy T imag real make_test test_ndarray_method x = x numpy copy make_test test_ndarray_transpose x = x numpy transpose make_test test_ndarray_reshape x = x numpy reshape size make_test test_ndarray_methods_returning_scalar x = x numpy max axis= all axis= make_test test_ndarray_builtin_functions x = x numpy + - make_test test_numpy_dtype_argument_to_function x np ones_like x dtype=np float make_test test_numpy_dtype_call_in_function x dt = np dtype float np full_like x dtype=dt make_test test_numpy_linalg x np linalg norm x numpy axis= make_test test_numpy_fft x np fft fftshift x numpy make_test test_numpy_random x = np random randn x - x make_test test_partials_torch_op_kwarg x par_mul = functools partial torch mul other=torch ones par_mul x make_test test_partials_torch_op_arg x par_mul = functools partial torch mul torch ones par_mul x make_test test_partials_udf_arg x par_mul = functools partial udf_mul torch ones par_mul x make_test test_list_add_then_mutate x my_list = x y = x my_list = my_list + x my_list append y sum my_list make_test test_list_expand_lhs x sum x make_test test_in_not_in x mylist = x myotherlist = assert mylist assert myotherlist sum mylist make_test test_is x y exc = ValueError abcd try raise exc except Exception e assert e exc x + y make_test test_is_not x y exc = ValueError abcd exc = TypeError abc try raise exc except Exception e assert e exc x + y make_test test_are_functorch_transforms_active x torch _C _are_functorch_transforms_active x + x - make_test test_partials_udf_kwarg x par_mul = functools partial udf_mul y=torch ones par_mul x make_test test_partials_udf_kwarg_module x y par_mod = functools partial udf_module mod=SmallNN par_mod x=x y=y make_test test_partials_udf_kwarg_method x y par_mod = functools partial udf_module mod=SmallNN forward par_mod x=x y=y make_test test_partials_lambda x multiply = lambda x y x y triple = functools partial multiply y= triple x unittest skipUnless torch distributed is_available requires torch distributed make_test test_flat_param_same_storage_size x y torch distributed fsdp _flat_param flat_param flat_param _same_storage_size x x = x + x = x - flat_param _same_storage_size y y = y + y = y - x y parametrize attr True __subclasshook__ __lt__ __hash__ __ge__ __le__ __gt__ __dict__ __getattribute__ __setattr__ __doc__ __repr__ __dir__ __init__ __new__ __class__ __eq__ __delattr__ __reduce__ __module__ __format__ __str__ __sizeof__ __ne__ __call__ __reduce_ex__ __init_subclass__ args keywords func False __code__ __kwdefaults__ __defaults__ __name__ __annotations__ __get__ __builtins__ __qualname__ __globals__ __closure__ test_partials_hasattr attr fn t f = lambda x y torch sin x + torch cos y p = functools partial f y=t hasattr p attr p t torch zeros_like t t = torch randn counter = torch _dynamo testing CompileCounter opt_fn = torch compile fullgraph=True backend=counter fn assertEqual opt_fn t fn t assertGreater counter frame_count unittest expectedFailure test_partials_hasattr_set_attr fn t f = lambda x y torch sin x + torch cos y p = functools partial f y=t p __name__ = test hasattr p __name__ p t torch zeros_like t t = torch randn counter = torch _dynamo testing CompileCounter opt_fn = torch compile fullgraph=True backend=counter fn assertEqual opt_fn t fn t test_filter fn inputs out = inputs inp filter lambda x x requires_grad inputs out = out inp out input = torch arange dtype=torch bfloat input = torch arange dtype=torch bfloat requires_grad_ True inputs = input input opt_fn = torch compile fullgraph=True fn assertEqual opt_fn inputs fn inputs test_filter_fallback fn inputs out = inputs inp filter lambda x x == inputs out = out inp out input = torch ones dtype=torch bfloat input = torch arange dtype=torch bfloat inputs = input input opt_fn = torch compile fn assertEqual opt_fn inputs fn inputs torch _dynamo reset assertRaises torch _dynamo exc Unsupported opt_fn = torch compile fullgraph=True fn opt_fn inputs test_filter_infinite_iterator fn x x = x + x list zip range filter lambda y y itertools count list zip range filter lambda y y itertools count inputs = torch ones opt_fn = torch compile fn backend= eager fullgraph=True assertTupleEqual opt_fn inputs fn inputs test_filter_reconstruct fn filter lambda x x + x zip + opt_fn = torch compile fn backend= eager fullgraph=True m = opt_fn torch ones n = fn torch ones assertIsInstance m filter assertEqual list m list n test_filter_graph_break_reconstruct fn x y x sum x + y x y backend = EagerAndRecordGraphs cnts = CompileCounterWithBackend backend opt_fn = torch compile fn backend=cnts = torch zeros b = torch ones assertEqual opt_fn b fn b torch _dynamo config assume_static_by_default assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ sum_ f = l_x_ sum l_x_ = None gt b = sum_ sum_ = None gt assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward s Sym s L_x_ f s l_x_ = L_x_ sum_ f = l_x_ sum l_x_ = None gt b = sum_ sum_ = None gt test_filter_with_graph_break f += g x nonlocal += x m = filter g += next m won t graph break torch _dynamo graph_break += next m will graph break cnts = torch _dynamo testing CompileCounter opt_f = torch compile f backend=cnts assertEqual f torch ones opt_f torch ones assertEqual cnts frame_count make_test test_getattr x fn y y + try _exit = type fn __exit__ except AttributeError x sin x cos unittest expectedFailure test_getattr_metaclass Meta type __getattr__ cls name len name C metaclass=Meta attr = torch compile backend= eager fullgraph=True fn t t + C attr + C dynamic_attr t = torch randn y = fn t assertEqual y t + + test_two_point_iter fn x y = map lambda n n + range i x = x + i y = y + next x y opt_fn = torch compile fn backend= eager fullgraph=True x = torch ones y = torch ones assertEqual fn x y opt_fn x y Test dict_keys passed along corresponding dict object test_dict_key_set d = b fn x d keys c keys x + d c x + x = torch zeros opt_fn = torch compile fullgraph=True backend= eager fn assertEqual opt_fn x d d keys fn x d d keys d update c opt_fn = torch compile fullgraph=True backend= eager fn assertEqual opt_fn x d d keys fn x d d keys Test only dict_keys passed into compiled region test_dict_key_set d = b fn x keys c keys x - x + x = torch zeros opt_fn = torch compile fullgraph=True backend= eager fn assertEqual opt_fn x d keys fn x d keys d update c opt_fn = torch compile fullgraph=True backend= eager fn assertEqual opt_fn x d keys fn x d keys test_dict_key_set = domains d attr d attr b = domains keys fn x b e b x += domains e attr x x = torch ones opt_fn = torch compile fullgraph=True backend= eager fn assertEqual opt_fn x b fn x b domains update d attr opt_fn = torch compile fullgraph=True backend= eager fn assertEqual opt_fn x b fn x b test_list_setitem fn int some_array = some_array = torch ones some_array opt_fn = torch compile fullgraph=True backend= eager dynamic=True fn assertEqual opt_fn fn assertEqual opt_fn fn test_list_setitem_slice fn int some_array = some_array + = torch ones some_array opt_fn = torch compile fullgraph=True backend= eager dynamic=True fn assertEqual opt_fn fn assertEqual opt_fn fn test_pow_int fn b torch pow b x = torch ones opt_fn = torch compile fullgraph=True backend= eager dynamic=True fn assertEqual opt_fn x fn x test_tensor_size_indexed_by_symint fn x y index = x shape - x + y shape index x = torch rand y = torch rand opt_fn = torch compile backend= eager fullgraph=True fn assertEqual opt_fn x y fn x y test_partials_as_input_partials_lambda fn f f x f x f x multiply = lambda x y x y lambda = functools partial multiply y= lambda = functools partial multiply y= cnts = torch _dynamo testing CompileCounter torch compile fn backend=cnts fullgraph=True lambda lambda torch randn assertEqual cnts frame_count test_partials_as_input_partials_mod fn f f x f x f x lambda = functools partial SmallNN y=torch randn lambda = functools partial SmallNN y=torch randn cnts = torch _dynamo testing CompileCounter x = torch randn dynamo_result = torch compile fn backend=cnts fullgraph=True lambda lambda x assertEqual cnts frame_count eager_result = fn lambda lambda x assertEqual eager_result dynamo_result test_partials_as_input_UDF fn f f x f x f x lambda = functools partial udf_mul y=torch randn lambda = functools partial udf_mul y=torch randn cnts = torch _dynamo testing CompileCounter x = torch randn dynamo_result = torch compile fn backend=cnts fullgraph=True lambda lambda x assertEqual cnts frame_count eager_result = fn lambda lambda x assertEqual eager_result dynamo_result test_partials_graph_break_reconstruct fn udf_mul_ udf_mul_ x lambda = functools partial udf_mul_ y=x lambda = functools partial udf_mul_ y=x print break torch mul lambda x lambda x backend = EagerAndRecordGraphs cnts = CompileCounterWithBackend backend x = torch randn dynamo_result = torch compile fn backend=cnts udf_mul udf_mul x eager_result = fn udf_mul udf_mul x assertEqual eager_result dynamo_result torch _dynamo config assume_static_by_default assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_lambda _keywords_y_ f l_lambda _keywords_y_ = L_lambda _keywords_y_ mul f = l_lambda _keywords_y_ l_lambda _keywords_y_ mul_ f = l_lambda _keywords_y_ l_lambda _keywords_y_ l_lambda _keywords_y_ = None mul_ f = torch mul mul mul_ mul = mul_ = None mul_ assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward s Sym s L_lambda _keywords_y_ f s s l_lambda _keywords_y_ = L_lambda _keywords_y_ mul f s s = l_lambda _keywords_y_ l_lambda _keywords_y_ mul_ f s s = l_lambda _keywords_y_ l_lambda _keywords_y_ l_lambda _keywords_y_ = None mul_ f s s = torch mul mul mul_ mul = mul_ = None mul_ test_partials_graph_break_reconstruct_mix fn udf_mul_ udf_add_ x lambda = functools partial udf_mul_ y=x lambda = functools partial udf_add_ x print break torch mul lambda x lambda x backend = EagerAndRecordGraphs cnts = CompileCounterWithBackend backend x = torch randn dynamo_result = torch compile fn backend=cnts udf_mul udf_add x eager_result = fn udf_mul udf_add x assertEqual eager_result dynamo_result torch _dynamo config assume_static_by_default assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_lambda _keywords_y_ f l_lambda _keywords_y_ = L_lambda _keywords_y_ mul f = l_lambda _keywords_y_ l_lambda _keywords_y_ add f = l_lambda _keywords_y_ + l_lambda _keywords_y_ l_lambda _keywords_y_ = None mul_ f = torch mul mul add mul = add = None mul_ assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward s Sym s L_lambda _keywords_y_ f s s l_lambda _keywords_y_ = L_lambda _keywords_y_ mul f s s = l_lambda _keywords_y_ l_lambda _keywords_y_ add f s s = l_lambda _keywords_y_ + l_lambda _keywords_y_ l_lambda _keywords_y_ = None mul_ f s s = torch mul mul add mul = add = None mul_ test_partials_graph_break_reconstruct_mix_no_source fn udf_mul_ x udf_add_ = lambda x y x + y lambda = functools partial udf_mul_ y=x lambda = functools partial udf_add_ x print break torch mul lambda x lambda x backend = EagerAndRecordGraphs cnts = CompileCounterWithBackend backend x = torch randn dynamo_result = torch compile fn backend=cnts udf_mul x eager_result = fn udf_mul x assertEqual eager_result dynamo_result torch _dynamo config assume_static_by_default assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_lambda _keywords_y_ f l_lambda _keywords_y_ = L_lambda _keywords_y_ mul f = l_lambda _keywords_y_ l_lambda _keywords_y_ add f = l_lambda _keywords_y_ + l_lambda _keywords_y_ l_lambda _keywords_y_ = None mul_ f = torch mul mul add mul = add = None mul_ assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward s Sym s L_lambda _keywords_y_ f s s l_lambda _keywords_y_ = L_lambda _keywords_y_ mul f s s = l_lambda _keywords_y_ l_lambda _keywords_y_ add f s s = l_lambda _keywords_y_ + l_lambda _keywords_y_ l_lambda _keywords_y_ = None mul_ f s s = torch mul mul add mul = add = None mul_ test_partials_graph_break_reconstruct_args_and_kwargs fn udf_mul_ x lambda = functools partial udf_mul_ x z=x lambda = functools partial udf_mul_ z=x torch mul lambda lambda backend = EagerAndRecordGraphs cnts = CompileCounterWithBackend backend x = torch randn dynamo_result = torch compile fn backend=cnts udf_mul x eager_result = fn udf_mul x assertEqual eager_result dynamo_result torch _dynamo config assume_static_by_default assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward L_x_ f l_x_ = L_x_ mul f = l_x_ mul_ f = mul l_x_ mul = None mul_ f = l_x_ l_x_ = None mul_ f = torch mul mul_ mul_ mul_ = mul_ = None mul_ assertExpectedInline normalize_gm backend graphs print_readable print_output=False \ GraphModule torch nn Module forward s Sym s L_x_ f s s l_x_ = L_x_ mul f s s = l_x_ mul_ f s s = mul l_x_ mul = None mul_ f s s = l_x_ l_x_ = None mul_ f s s = torch mul mul_ mul_ mul_ = mul_ = None mul_ test_partials_recompilation fn f f x f x f x lambda = functools partial udf_mul y=torch randn lambda = functools partial udf_mul y=torch randn cnts = torch _dynamo testing CompileCounter x = torch randn fn = torch compile fn backend=cnts fullgraph=True fn lambda lambda x assertEqual cnts frame_count fn lambda lambda x assertEqual cnts frame_count No recompile Tensor udf_mul guarded lambda = functools partial udf_mul y=torch randn x = torch randn fn lambda lambda x assertEqual cnts frame_count Recompile Tensor size changed multiply = lambda x y x y lambda = functools partial multiply y=torch randn x = torch randn fn lambda lambda x assertEqual cnts frame_count Recompile func id changed fn f f args f args f args cnts = torch _dynamo testing CompileCounter x = torch randn fn = torch compile fn backend=cnts fullgraph=True fn lambda lambda x assertEqual cnts frame_count start over lambda = functools partial multiply y= x=torch randn fn lambda lambda assertEqual cnts frame_count Recompile Different kwarg keys lambda = functools partial multiply x = torch randn fn lambda lambda x assertEqual cnts frame_count Recompile Different arg keys lambda = lambda x x + x fn lambda lambda x assertEqual cnts frame_count Recompile input no longer functools partial test_manual_seed torch compile foo torch manual_seed torch randint assertEqual foo foo assertEqual foo foo test_partial_across_graph_break_uninvoked functools partial bar x kwargs x + x torch compile backend= eager dynamic=True foo x i inner print graph_break op x op = partial bar dim= x = inner op = partial bar other= inner + x foo torch rand test_no_recompile_inner_function forward inp g y inp + y print graph break g torch rand cnts = torch _dynamo testing CompileCounter opt_fn = torch compile forward backend=cnts input = torch rand _ = opt_fn input _ = opt_fn input _ = opt_fn input Should have recompiled assertEqual cnts frame_count test_no_recompile_inner_lambda forward inp g = lambda y inp + y print graph break g torch rand cnts = torch _dynamo testing CompileCounter opt_fn = torch compile forward backend=cnts input = torch rand _ = opt_fn input _ = opt_fn input _ = opt_fn input Should have recompiled assertEqual cnts frame_count test_complex_closure torch compile forward y x z y + z x input = torch rand input = torch rand res = forward input input assertTrue same res input + input test_non_inlined_closure torch compile program x y one = lambda x y x + y inner Force no inlining torch _dynamo graph_break one x y res = inner one = lambda x y x - y res += inner res input = torch randn input = torch randn assertTrue same program input input input + input parametrize int_or_float int float test_np_constant_collections_as_input int_or_float info_func = getattr np f int_or_float info dt_string_arg = f int_or_float np_dt_attr = getattr np dt_string_arg dt_args = dt_string_arg np_dt_attr arg_variants_iter = itertools chain dt_args map np dtype dt_args map info_func dt_args func b info_or_dt + info_func info_or_dt max opt_fn = torch compile func = torch randn b = torch randn eager_result = func b dt_args arg arg_variants_iter opt_result = opt_fn b arg assertTrue same opt_result eager_result parametrize typ info_func int np iinfo float np finfo name_fn=lambda t _ t __name__ test_np_constant_collections_guards typ info_func func_info info + info max func_dtype dt + info_func dt max dt_args = np dtype typ np ones dtype=typ dtype np dtype np dtype typ name np dtype typ __name__ cnts_ = torch _dynamo testing CompileCounter opt_fn_dtype = torch compile func_dtype backend=cnts_ = torch zeros dtype=typ arg dt_args opt_fn_dtype arg each should produce identical arg assertEqual cnts_ frame_count cnts_ = torch _dynamo testing CompileCounter opt_fn_info = torch compile func_info backend=cnts_ info_args = info_func dt dt dt_args arg info_args opt_fn_info arg each should produce identical arg assertEqual cnts_ frame_count typ float dt_extra = np dtype np float dt_extra = np dtype np int info_extra = info_func dt_extra eager_result_dtype = func_dtype dt_extra compile_result_dtype = opt_fn_dtype dt_extra assertEqual cnts_ frame_count assertEqual eager_result_dtype compile_result_dtype eager_result_info = func_info info_extra compile_result_info = opt_fn_info info_extra assertEqual cnts_ frame_count assertEqual eager_result_info compile_result_info test_compare_constant_and_tensor op operator lt operator le operator gt operator ge operator ne operator eq operator is_ operator is_not subTest op=op fn x op - x opt_fn = torch compile fullgraph=True fn x = torch randn assertEqual opt_fn x fn x test_pos fn x y operator pos x +y opt_fn = torch compile fullgraph=True dynamic=True fn test x y assertEqual opt_fn x y fn x y test torch ones test torch ones test - - test - test True False test torch ones dtype=torch float test_index fn x t v = operator index x torch mul t v test b assertEqual opt_fn b fn b dynamic True False torch _dynamo reset opt_fn = torch compile fn dynamic=dynamic t = torch ones test t test - t test t test False t test True t test_truth fn x y operator truth x bool y opt_fn = torch compile dynamic=False fn test x y assertEqual opt_fn x y fn x y test test - True test - test True False test torch ones test torch zeros test torch ones torch ones test_unary_fold_op op operator abs abs operator neg operator pos operator truth subTest op=op fn = range - list map op opt_fn = torch compile fn fullgraph=True assertEqual opt_fn fn test_unary_fold_op_seq op operator length_hint subTest op=op fn = tuple range - i i range tuple map op opt_fn = torch compile fn fullgraph=True assertEqual opt_fn fn test_attrgetter attrs shape data shape device shape device shape data shape subTest attrs=attrs fn x y getter = operator attrgetter attrs getter x getter y opt_fn = torch compile fullgraph=True fn x = torch randn y = torch randn assertEqual opt_fn x y fn x y test_itemgetter items slice slice subTest items=items fn x y getter = operator itemgetter items getter x getter y opt_fn = torch compile fullgraph=True fn x = torch randn y = torch randn assertEqual opt_fn x y fn x y test_methodcaller name args kwargs size size add torch randn add torch randn alpha subTest name=name args=args kwargs=kwargs fn x y caller = operator methodcaller name args kwargs caller x caller y opt_fn = torch compile fullgraph=True fn x = torch randn y = torch randn assertEqual opt_fn x y fn x y gen_random_range_args args_count = random randint args = random randint - _ range args_count args_count == args == args = args test_range_iterator_graph_break torch compile backend= eager fn x = range __iter__ y = x + next torch _dynamo graph_break y + next + next x = torch tensor y = fn x assertEqual y x + + + test_range_iterator_graph_break_ torch compiler disable g y y + next + next torch compile backend= eager fn x = range __iter__ y = x + next z = g y k = next assert k == z + k x = torch tensor z = fn x assertEqual z x + + + + make_test test_range_iterator b = range __iter__ isinstance range_iterator + b - b make_test test_range_iterator_ b should pass once we stop having three different paths call_iter = iter range isinstance range_iterator + b - b test_range_length test args expected=None r = range args range_variable = RangeVariable ConstantVariable create v v args assertEqual len r range_variable range_length expected None assertEqual len r expected test expected= test expected= test - expected= test expected= test expected= step test expected= negative step test - expected= test - Fuzz testing _ range args = gen_random_range_args print testing args test args test_indexed_range test range index expected=None range_variable = RangeVariable ConstantVariable create v v range start range stop range step assertEqual range index range_variable apply_index index as_python_constant expected None assertEqual range index expected test range expected= test range expected= Fuzz testing _ range range_args = gen_random_range_args r = range range_args len r == continue index = random randint len r - print testing r index test r index test_sliced_range test range slice expected=None range_variable = RangeVariable ConstantVariable create v v range start range stop range step assertEqual range slice range_variable apply_slice slice as_python_constant expected None assertEqual range slice expected test range slice expected=range test range slice None None expected=range test range slice - None expected=range test range slice - expected=range test range slice expected=range test range slice - expected=range test range - - - slice None - expected=range - rand_slice flip_coin out random randint == r_item allow_zero=True i = random randint - allow_zero i == i = flip_coin i = None i arg_count = random randint arg_count == slice r_item arg_count == slice r_item r_item slice r_item r_item r_item False Fuzz testing _ range range_args = gen_random_range_args r = range range_args generate random slice s = rand_slice print testing r s test r s test_range_with_slice_index fn x acc = k range acc = acc k x acc opt_fn = torch compile fullgraph=True fn x = torch ones assertEqual opt_fn x fn x test_range_with_index fn x acc = acc = acc range x acc opt_fn = torch compile fullgraph=True fn x = torch ones assertEqual opt_fn x fn x test_rand_inlined torch compile backend= eager dynamic=True fn idx_size = idx_size random randint = random randint t = tuple idx_size src_size = random randint + s s idx_size noqa F idx = torch empty t noqa F fn test_rand_tensor_partial collections namedtuple functools partial SdpaShape = namedtuple Sdpa_Shape batch num_heads seq_len head_dim torch compile backend= eager func make_tensor = partial torch rand device= cpu dtype=torch float requires_grad=True bsz num_heads seq_len_q seq_len_kv head_dim = make_q_tensor = partial make_tensor SdpaShape bsz num_heads seq_len_q head_dim make_kv_tensor = partial make_tensor SdpaShape bsz num_heads seq_len_kv head_dim t = make_q_tensor t = make_kv_tensor t = t + t noqa F func test_to torch compile backend= eager fn t = torch ones y = t meta noqa F fn test_elipsis torch compile backend= eager fullgraph=True fn ind val ind = val arr = np zeros assertEqual fn arr np s_ np ones np ones arr = np array assertEqual fn arr np s_ np zeros np array arr = np array assertEqual fn arr np s_ np zeros np array arr = np array assertEqual fn arr np s_ np array np array arr = np array assertEqual fn arr np s_ np array np array test_round fn t t + round t = torch randn e = fn t g = torch compile fn backend= eager fullgraph=True t assertEqual e g test_map_return fn b map lambda x x + b opt_fn = torch compile fn backend= eager fullgraph=True m = opt_fn torch randn torch randn assertIsInstance m map make_test test_map_max b max map lambda x x sum b make_test test_map_max_const max map lambda x x + make_test test_map_list b list map lambda x x + b make_test test_map_tuple b tuple map lambda x x + b make_test test_map_iter b = iter map lambda x x + b next make_test test_map_zip_dict d = dict zip map lambda x x + map lambda x x - y y list d + noqa RUF make_test test_map_dict_fromkeys dict fromkeys map lambda x x + + make_test test_map_set set map lambda x x + + test_map_sum defined earlier make_test test_map_reduce b functools reduce lambda x y x + y map lambda x x + b make_test test_map_sorted sorted map lambda x x + + make_test test_map_list_extend b c l = l extend map lambda x x + b c l make_test test_map_list_slice_assign b c d e l = b c l = map lambda x x + d e l make_test test_map_deque_extendleft b c d = collections deque d extendleft map lambda x x + b c d make_test test_map_str_join join map lambda x x b c + test_map_with_graph_break f += g x nonlocal += x + m = map g += next m won t graph break torch _dynamo graph_break += next m will graph break cnts = torch _dynamo testing CompileCounter opt_f = torch compile f backend=cnts assertEqual f torch ones opt_f torch ones assertEqual cnts frame_count test_map_reconstruct fn map lambda x x + x zip + opt_fn = torch compile fn backend= eager fullgraph=True m = opt_fn torch ones assertIsInstance m map assertEqual list m list fn torch ones test_zip_reconstruct fn zip map lambda x x + + opt_fn = torch compile fn backend= eager fullgraph=True m = opt_fn torch ones assertIsInstance m zip assertEqual list m list fn torch ones make_test test_map_partial_unpack b y = f x nonlocal y y += x l = list zip b map f noqa F + y make_test test_map_call_function_ex b f x y x + y f map lambda x x + b make_test test_map_unpack_twice b m = map lambda x x + b l = list m l = list m l l make_test test_enumerate b list enumerate b start= + make_test test_map_enumerate b list enumerate map lambda x x + b start= + make_test test_map_infinite b list map lambda x y x + y b itertools count make_test test_map_unpack_vars b x y = map lambda x x + b x + y make_test test_map_list_extend y = inner z z + y - y extend map inner range + y make_test test_map_deque_extendleft y = collections deque inner z z + y y extendleft map inner range + y test_unsqueeze_inplace fn x torch Tensor unsqueeze_ x dim= + self_fn x x unsqueeze_ dim= + v = torch ones device= cpu identical tensor since modify inplace v = torch ones device= cpu opt_fn = torch compile fn opt_self_fn = torch compile self_fn assertEqual v v assertEqual opt_fn v opt_self_fn v test_enumerate_custom MyClass __iter__ = __next__ raise StopIteration += fn x i enumerate MyClass x += i + x opt_fn = torch compile fn backend= eager fullgraph=True assertEqual fn torch ones opt_fn torch ones unittest skip https github com pytorch pytorch pull exposed bug test_enumerate_reconstruct fn b enumerate b start= opt_fn = torch compile fn backend= eager fullgraph=True inps = torch randn torch randn = fn inps = opt_fn inps assertIsInstance enumerate assertEqual list list test_returning_recursive_func torch compile backend= eager fullgraph=True run x f f x + f res f = run torch zeros assertTrue same res torch ones assertTrue f f test_functools_partial_binding Foo __init__ x x = x functools lru_cache noqa B incr val x += val fn x f = Foo f incr x + f x x = torch randn opt_fn = torch compile fn backend= eager fullgraph=True assertEqual fn x opt_fn x test_functools_cache_guard Foo functools lru_cache noqa B run val c= val c f = Foo fn x f run x x = torch randn opt_fn = torch compile fn backend= eager fullgraph=True assertEqual fn x opt_fn x test_torch_get_device_module f mod = torch get_device_module mod = torch get_device_module cpu mod = torch get_device_module torch device device_type mod mod mod assertEqual f torch compile f backend= eager fullgraph=True torch compile backend= eager fullgraph=True f torch get_device_module foo= cpu assertRaises Unsupported f torch compile backend= eager fullgraph=True f torch get_device_module cpu device= cpu assertRaises Unsupported f torch compile backend= eager fullgraph=True f torch get_device_module asdf assertRaises Unsupported f test changing torch get_device_module super rare case due lru_cache torch compile backend= eager fullgraph=True f torch get_device_module f new_device = cpu torch _C _get_accelerator == torch device cuda cuda old_get_device_module = torch get_device_module new_get_device_module device=None device old_get_device_module device getattr torch new_device NOTE torch get_device_module __wrapped__ guarded torch get_device_module patch torch get_device_module new_get_device_module print torch get_device_module assertEqual f getattr torch new_device synchronize causes graph break so no fullgraph=True torch compile backend= eager f mod = torch get_device_module mod synchronize mod f test_torch_source global torch g = torch get_device_module torch compile backend= eager fullgraph=True f g try old_torch = torch torch = assertEqual torch assertIsInstance f types ModuleType finally torch = old_torch udf_mul x y x y udf_mul x y z x y z udf_add x y x + y SmallNN torch nn Module forward x y combined = torch cat x y dim= out = torch nn ReLU combined out = torch nn ReLU out out udf_module mod x y mod x y global_func_with_default_tensor_args x=torch zeros kw_x=torch zeros x add_ kw_x add_ x kw_x ModuleWithDefaultTensorArgsMethod torch nn Module forward x=torch zeros kw_x=torch zeros x add_ kw_x add_ x kw_x WrapperModule torch nn Module __init__ - None super __init__ m = ModuleWithDefaultTensorArgsMethod forward m DefaultsTests torch _dynamo test_case TestCase test_func_default_tensor_args Tests we indeed reference mutate one default tensor arg stored globally allocated function object both orig compiled function func global_func_with_default_tensor_args cnts = torch _dynamo testing CompileCounter compiled_func = torch compile func backend=cnts i range i == x kw_x = func x kw_x = compiled_func inner func mutates += each call assertTrue same x torch ones_like x + i assertTrue same kw_x torch ones_like kw_x + i Calling compiled_func twice does recompile assertEqual cnts frame_count assertEqual cnts op_count But change guarded default tensor we do recompile patch object global_func_with_default_tensor_args __defaults__ torch ones x kw_x = compiled_func assertEqual cnts frame_count assertEqual cnts op_count patch object global_func_with_default_tensor_args __kwdefaults__ kw_x torch ones x kw_x = compiled_func assertEqual cnts frame_count assertEqual cnts op_count torch _dynamo config patch assume_dunder_attributes_remain_unchanged=False test_meth_default_tensor_args Tests we indeed reference mutate one default tensor arg stored globally allocated function object both orig compiled function mod = WrapperModule cnts = torch _dynamo testing CompileCounter compiled_mod = torch compile mod backend=cnts i range i == x kw_x = mod x kw_x = compiled_mod inner func mutates += each call assertTrue same x torch ones_like x + i assertTrue same kw_x torch ones_like kw_x + i Calling compiled_func twice does recompile assertEqual cnts frame_count assertEqual cnts op_count But change guarded default tensor we do recompile patch object ModuleWithDefaultTensorArgsMethod forward __defaults__ torch ones x kw_x = compiled_mod assertEqual cnts frame_count assertEqual cnts op_count patch object ModuleWithDefaultTensorArgsMethod forward __kwdefaults__ kw_x torch ones x kw_x = compiled_mod assertEqual cnts frame_count assertEqual cnts op_count test_func_default_torch_args Tests other types torch types function default size dtype device func_with_default_torch_args dt=torch float ds=torch Size dd=torch device cpu torch ones ds dtype=dt device=dd func func_with_default_torch_args cnts = torch _dynamo testing CompileCounter compiled_func = torch compile func backend=cnts out = func compiled_out = compiled_func assertEqual out dtype compiled_out dtype assertEqual out device compiled_out device assertEqual out size compiled_out size assertEqual cnts frame_count assertEqual cnts op_count test_dataclass_factory dataclass Output scalar int = named_tensors dict str torch Tensor = field default_factory=dict lists list torch Tensor = field default_factory=list scale scalar fn x Check default dict assignment = Output Check dataclass methods can inlined scaled_value = scale Check normal assignment works b = Output named_tensors= x x Check default int assignment c = Output Check default members properly initialized isinstance named_tensors dict x = torch sin x Change dataclass c scalar = c named_tensors x = x Return dataclaass well check reconstruction c torch cos x scaled_value + b named_tensors x + c scalar cnts = torch _dynamo testing CompileCounter compiled_fn = torch compile fn backend=cnts fullgraph=True x = torch randn eager_dataclass out = fn x compiled_dataclass compiled_out = compiled_fn x assertEqual eager_dataclass scalar compiled_dataclass scalar assertEqual eager_dataclass named_tensors x compiled_dataclass named_tensors x assertTrue same out compiled_out assertEqual cnts frame_count assertEqual cnts op_count test_dataclass_nested dataclass Base outer_a int outer_b int dataclass Derived Base inner_a Any = field default_factory=list fn x l = Derived l outer_a x opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn res = fn x ref = opt_fn x assertEqual ref res test_listlike_of_tensors_contains_constant listlike set list fn x x add_ s = listlike x res = s res opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn ref = opt_fn x res = fn x assertEqual ref res test_cast_tensor_single_elem torch _dynamo config patch capture_scalar_outputs True t val float float float True int int False int fails due = comparison sym_int bool complex no casting sym_bool no sym_complex fn x x = x + t x opt_fn = torch compile fn backend= eager fullgraph=True dynamic=False x = torch tensor val res = fn x ref = opt_fn x assertEqual ref res Cannot handle non single-elem assertRaises ValueError fn torch tensor val assertRaises torch _dynamo exc TorchRuntimeError opt_fn torch tensor val test_set_construction fn x y = x add_ s = set x s add y len s opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn res = fn x ref = opt_fn x assertEqual ref res parametrize _type set frozenset name_fn=lambda t t __name__ test_set_call___init__ _type make_test fn b s = _type apple banana cherry s __init__ google microsoft apple frozenset should remain same while set gets updated banana s + b - b fn parametrize method_name copy difference intersection symmetric_difference union test_frozenset_return_type method_name make_test fn b set = frozenset apple banana cherry set = frozenset google microsoft apple method_name == copy result = set copy result = getattr set method_name set type result frozenset x = + b x = - b x fn test_frozenset_construction fn x s = frozenset x t = frozenset s len t opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn res = fn x ref = opt_fn x assertEqual ref res test_frozenset_reconstruction d = f = frozenset d f = torch randn fn x k = frozenset torch _dynamo graph_break d k x opt_fn = torch compile fn backend= eager x = torch randn res = fn x ref = opt_fn x assertEqual ref res test_frozenset_illegal_call_method fn_add s = frozenset s add len s fn_pop s = frozenset s pop len s fn_update s = frozenset s update len s fn_remove s = frozenset s remove len s fn_discard s = frozenset s discard len s fn_clear s = frozenset s clear len s fn fn_add fn_pop fn_update fn_remove fn_discard fn_clear torch _dynamo reset opt_fn = torch compile fn backend= eager fullgraph=True assertRaises torch _dynamo exc InternalTorchDynamoError opt_fn test_is_tensor_tensor fn x y x y x x + y fn_opt = torch compile backend= eager fullgraph=True dynamic=True fn x = torch zeros y = torch ones assertEqual fn x y fn_opt x y assertEqual fn x x fn_opt x x test_is_not_tensor_tensor fn x y x y x x + y fn_opt = torch compile backend= eager fullgraph=True dynamic=True fn x = torch zeros y = torch ones assertEqual fn x y fn_opt x y assertEqual fn x x fn_opt x x test_is_mutated_tensor_tensor fn x y = x add_ x y fn_opt = torch compile backend= eager fullgraph=True dynamic=True fn z = torch ones assertEqual fn z fn_opt z test_is_mutated_tensor_tensor_across_graph_break fn x y = x add_ cond = x y x add_ The real tensor values recovered when graph breaking Hence we recover invariant torch _dynamo graph_break x add_ x y cond fn_opt = torch compile backend= eager dynamic=True fn z = torch ones assertEqual fn z fn_opt z test_is_mutated_tensor_tensor fn x y = x add_ y x fn_opt = torch compile backend= eager fullgraph=True dynamic=True fn z = torch ones assertEqual fn z fn_opt z test_is_init_in_compile_mutated_tensor_tensor fn x z = x clone y = z add_ y z fn_opt = torch compile backend= eager fullgraph=True dynamic=True fn z = torch ones assertEqual fn z fn_opt z test_is_init_in_compile_vmapped_mutated_tensor_tensor fn z x = z clone y = torch vmap torch Tensor acos_ x _ = y z y x fn_opt = torch compile backend= eager fullgraph=True dynamic=True fn z = torch ones assertEqual fn z fn_opt z test_is_vmapped_mutated_tensor_tensor fn x y = torch vmap torch Tensor acos_ x y x fn_opt = torch compile backend= eager fullgraph=True dynamic=True fn z = torch ones assertEqual fn z fn_opt z test_is_init_in_compile_vmapped_mutated_tensor_tensor_multi_arg fn y z = y clone b = z clone g b acos_ b acos_ c d = torch vmap g b c b d fn_opt = torch compile backend= eager fullgraph=True dynamic=True fn y = torch ones z = torch ones assertEqual fn y z fn_opt y z assertEqual fn y y fn_opt y y test_in_set_would_fail_broadcast param = torch zeros param = torch zeros tensor_list = set tensor_list add param assert param tensor_list fn param param param add_ tensor_list = set param param tensor_list cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts fullgraph=True assertEqual opt_fn param param fn param param assertEqual cnts frame_count Test aliased assertEqual opt_fn param param fn param param assertEqual cnts frame_count Recompiles test_in_set_inplace param = torch zeros param = torch zeros tensor_list = set tensor_list add param assert param tensor_list fn param param y = param add_ Tensor method z = torch Tensor add_ y torch function tensor_list = set param y tensor_list z tensor_list cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts fullgraph=True assertEqual opt_fn param param fn param param assertEqual cnts frame_count Test aliased assertEqual opt_fn param param fn param param assertEqual cnts frame_count Recompiles test_reconstructed_name lst = torch _dynamo disable disallowed g lst append g __name__ f g disallowed g opt_f = torch compile f backend= eager opt_f f assertEqual len lst assertEqual lst lst test_zip_strict fn x ys zs x = x clone y z zip ys zs strict=True x += y z x opt_fn = torch compile fn backend= eager nopython_fn = torch compile fn backend= eager fullgraph=True x = torch ones ys = zs = assertEqual opt_fn x ys zs fn x ys zs If nopython should raise UserError assertRaisesRegex torch _dynamo exc UserError zip nopython_fn x ys zs assertRaisesRegex torch _dynamo exc UserError zip nopython_fn x ys zs Should cause fallback allow graph break assertRaisesRegex ValueError zip opt_fn x ys zs assertRaisesRegex ValueError zip opt_fn x ys zs unittest skipIf TEST_MULTIGPU detected only one GPU test_gpu_current_device fn x y = torch empty dtype=torch float device=torch accelerator current_device_index y copy_ x torch sin y + y device index counter = torch _dynamo testing CompileCounter opt_fn = torch compile backend=counter fullgraph=True fn torch accelerator device_index x = torch randn assertEqual opt_fn x fn x assertEqual counter frame_count torch accelerator device_index assertEqual opt_fn x fn x assertEqual counter frame_count test_fn_with_attr fn x fn pred torch relu x torch abs x + t = torch ones counter = torch _dynamo testing CompileCounter fn pred = True opt_fn_ = torch compile fullgraph=True backend=counter fn assertEqual opt_fn_ t fn t assertEqual counter frame_count fn pred = False opt_fn_ = torch compile fullgraph=True backend=counter fn assertEqual opt_fn_ t fn t assertEqual counter frame_count test_str_handler_for_user_defined_object Confirms handler behaviour ` str ` same between eager dynamo Compares user defined object custom ` __str__ ` method without CustomStr __str__ ok foo_custom_str x = CustomStr x str eager_custom_str = foo_custom_str torch ones dynamo_custom_str = torch compile foo_custom_str fullgraph=True torch ones assertEqual eager_custom_str dynamo_custom_str assertEqual eager_custom_str ok DefaultStr pass foo_default_str x = DefaultStr x str eager_default_str = foo_default_str torch ones dynamo_default_str = torch compile foo_default_str fullgraph=True torch ones Check tensor output eager dynamo modes same assertEqual eager_default_str dynamo_default_str Check name without memory address same both modes eager_class_name = eager_default_str split object dynamo_class_name = dynamo_default_str split object assertEqual eager_class_name dynamo_class_name test_pybind_object fn x pybind_obj pybind_obj result torch cos x torch sin x opt_fn = torch compile fn backend= eager fullgraph=True pybind_obj = torch _C _dynamo guards GuardDebugInfo True a== x = torch randn assertEqual opt_fn x pybind_obj fn x pybind_obj pybind_obj = torch _C _dynamo guards GuardDebugInfo False a== x = torch randn assertEqual opt_fn x pybind_obj fn x pybind_obj test_tree_map fn b index call index mapped_attributes = torch utils _pytree tree_map_only torch Tensor lambda x x index b mapped_attributes call index = torch randn b = torch randn compiled_fn = torch compile fn fullgraph=True compiled_res = compiled_fn b torch tensor reference_res = fn b torch tensor assertTrue same compiled_res reference_res test_fx_map_aggregate fn inputs f torch fx node map_aggregate inputs f opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn torch randn torch randn f y y ref = fn x f res = opt_fn x f assertEqual ref res Check type res immutable_list assertTrue type ref type res x = torch randn b torch randn torch randn ref = fn x f res = opt_fn x f assertEqual ref res assertTrue type ref type res test_fx_immutable_list_mutation_not_allowed fn inputs x f=lambda x x immutable_inputs = torch fx immutable_collections immutable_list inputs try immutable_inputs append x except TypeError pass torch fx node map_aggregate immutable_inputs f opt_fn = torch compile fn backend= eager fullgraph=True inputs = torch randn torch randn torch randn x = torch randn assertEqual fn inputs x opt_fn inputs x test_udf_tuple MyTuple tuple noqa SLOT len_mulitply_ len __contains__ val Ensure overridden method traced checked = True super __contains__ val fn x tup tup x = torch cos x x = torch sin x x tup len_mulitply_ opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn ref_tup = MyTuple ref = fn x ref_tup res_tup = MyTuple res = opt_fn x res_tup assertEqual ref res assertTrue ref_tup checked assertTrue res_tup checked test_udf_tuple_construction MyTuple tuple noqa SLOT pass fn x tup = MyTuple tup x = torch cos x x = torch sin x x tup opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn ref_x ref_tup = fn x res_x res_tup = opt_fn x assertEqual ref_x res_x assertEqual ref_tup res_tup test_udf_tuple_construction_custom_new MyTuple tuple noqa SLOT __new__ cls args kwargs super __new__ cls fn x tup = MyTuple tup x = torch cos x x = torch sin x x tup opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn ref_x ref_tup = fn x res_x res_tup = opt_fn x assertEqual ref_x res_x assertEqual ref_tup res_tup test_udf_namedtuple MyTuple NamedTuple torch Tensor b torch Tensor PairTensor MyTuple __new__ cls b super __new__ cls b __add__ other PairTensor + other b + other b fn pair pair pair + pair opt_fn = torch compile fn backend= eager fullgraph=True pair = PairTensor torch randn torch randn pair = PairTensor torch randn torch randn ref = fn pair pair res = opt_fn pair pair assertEqual ref res assertEqual ref b res b test_udf_tuple_reconstruction MyTuple tuple noqa SLOT pass fn x klass x = x sc_tuple = tuple __new__ klass x isinstance sc_tuple MyTuple sc_tuple attr = sc_tuple opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn ref = fn x MyTuple res = opt_fn x MyTuple assertEqual ref res assertTrue isinstance res MyTuple assertEqual ref attr res attr ref = fn x tuple res = opt_fn x tuple assertEqual ref res assertTrue isinstance res tuple test_udf_list MyList list noqa SLOT len_mulitply_ len __contains__ val Ensure overridden method traced checked = True super __contains__ val __getitem__ idx Tests reconstruction logic does call overridden __getitem__ method raise RuntimeError Should called fn x lst lst x = torch cos x x = torch sin x lst append x lst len_mulitply_ opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn ref_lst = MyList ref = fn x ref_lst res_lst = MyList res = opt_fn x res_lst assertEqual ref res assertEqual len ref_lst len res_lst assertTrue ref_lst checked assertTrue res_lst checked test_udf_list_slice MyList list noqa SLOT len_mulitply_ len fn x lst lst append x lst len_mulitply_ sum lst opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn ref_lst = MyList ref = fn x ref_lst res_lst = MyList res = opt_fn x res_lst assertEqual ref res assertEqual len ref_lst len res_lst test_udf_list_reconstruction MyList list noqa SLOT __new__ cls args kwargs super __new__ cls args kwargs pass fn x klass x = x sc_list = list __new__ klass sc_list append x isinstance sc_list MyList sc_list attr = sc_list opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn ref = fn x MyList res = opt_fn x MyList assertEqual ref res assertTrue isinstance res MyList assertEqual ref attr res attr ref = fn x list res = opt_fn x list assertEqual ref res assertTrue isinstance res list test_sys_recursionlimit fn x x sin sys getrecursionlimit opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn assertEqual fn x opt_fn x test_keyword fn x word keyword iskeyword word torch sin x torch cos x opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn word = None assertEqual fn x word opt_fn x word word = dynamo assertEqual fn x word opt_fn x word test_func_attrs f x= y= pass fn x try f dynamo + except AttributeError x = torch sin x code = f __code__ defaults = f __defaults__ x len defaults code co_argcount opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn assertEqual fn x opt_fn x test_functools_partial_id gn b + b partial_gn = functools partial gn a= fn x d = id partial_gn partial_gn b=x d id partial_gn opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn assertEqual fn x opt_fn x test_functional_compile get_torch_functional_functions s = set name torch functional __all__ method = getattr torch functional name s add method s functions = get_torch_functional_functions assertTrue len functions func functions compiled_func = torch compile func assertTrue callable compiled_func test_skip_function_call_very_weird_value weird noqa UP __getattribute__ name name == __qualname__ raise AttributeError test w = weird = SkipFunctionVariable value=w assertRaises Unsupported call_function None test_inspect_method_source Mod torch nn Module __init__ super __init__ check x x forward x x mod = Mod fn x inspect signature mod check parameters items mod x opt_fn = torch compile fn backend= eager fullgraph=True x = torch randn ref = fn x res = opt_fn x assertEqual ref res test_property_class_transmute PropertyGetter __call__ obj True p = property PropertyGetter Mod torch nn Module forward x p x + raise RuntimeError whoops mod = Mod mod __class__ = type mod __class__ __name__ mod __class__ p p opt_mod = torch compile mod backend= eager fullgraph=True x = torch randn assertEqual opt_mod x x + test_property_functools_partial p_getter obj delta int Use instance state + bound constant getattr obj flag + delta Mod torch nn Module __init__ flag int super __init__ flag = flag fget functools partial object p = property functools partial p_getter delta= forward x p calls p_getter delta= x + raise RuntimeError whoops mod = Mod flag= opt_mod = torch compile mod backend= eager fullgraph=True x = torch randn assertEqual opt_mod x x + instantiate_parametrized_tests FunctionTests instantiate_parametrized_tests DefaultsTests __name__ == __main__ torch _dynamo test_case run_tests run_tests