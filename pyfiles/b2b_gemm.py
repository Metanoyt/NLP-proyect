mypy allow-untyped-defs functools collections deque torch torch utils _ordered_set OrderedSet torch utils _pytree tree_map _dynamo utils counters ir ComputedBuffer FixedLayout FlexibleLayout InputBuffer ShapeAsConstantBuffer StorageBox Subgraph TensorBox lowering lowerings pattern_matcher Arg CallFunction Match PatternMatcherPass register_graph_pattern select_algorithm autotune_select_algorithm ExternKernelChoice SymbolicGridFn TritonTemplate TritonTemplateCaller utils ceildiv B B_GEMM_PASS = PatternMatcherPass pass_name= b b_gemm_pass SymbolicGridFn b b_gemm_grid M P meta cdiv cdiv M meta BLOCK_SIZE_M cdiv P meta BLOCK_SIZE_P b b_gemm_left_template = TritonTemplate name= b b_gemm_left grid=b b_gemm_grid debug=False source=r def_kernel A B C B B_GEMM_LEFT_TRITON_ENTRANCE dynamic shapes M = size A N = size A O = size C P = size C dynamic strides stride_am = stride A stride_an = stride A stride_bn = stride B stride_bo = stride B stride_co = stride C stride_cp = stride C output block counts num_m_block = tl cdiv M BLOCK_SIZE_M num_p_block = tl cdiv P BLOCK_SIZE_P internal block counts num_n_block = tl cdiv N BLOCK_SIZE_N num_o_block = tl cdiv O BLOCK_SIZE_O output block ids pid = tl program_id axis= m_block_id = pid num_p_block p_block_id = pid num_p_block accumulator acc = tl zeros BLOCK_SIZE_M BLOCK_SIZE_P dtype=tl float main loop offs_m = m_block_id BLOCK_SIZE_M + tl arange BLOCK_SIZE_M offs_p = p_block_id BLOCK_SIZE_P + tl arange BLOCK_SIZE_P subgraph A B C offs_o = tl arange BLOCK_SIZE_O _ range num_o_block c_mask = offs_o None O offs_p None P c_ptrs = C + offs_o None stride_co + offs_p None stride_cp c = tl load c_ptrs mask=c_mask other= tl float BLOCK_SIZE_O BLOCK_SIZE_P acc_ab = tl zeros BLOCK_SIZE_M BLOCK_SIZE_O dtype=tl float offs_n = tl arange BLOCK_SIZE_N __ range num_n_block a_mask = offs_m None M offs_n None N a_ptrs = A + offs_m None stride_am + offs_n None stride_an = tl load a_ptrs mask=a_mask other= tl float BLOCK_SIZE_M BLOCK_SIZE_N b_mask = offs_n None N offs_o None O b_ptrs = B + offs_n None stride_bn + offs_o None stride_bo b = tl load b_ptrs mask=b_mask other= tl float BLOCK_SIZE_N BLOCK_SIZE_O acc_ab += tl dot b out_dtype=tl float offs_n += BLOCK_SIZE_N apply subgraph modification subgraph_number= output_name= post_subgraph_acc_ab inner_mm= acc_ab &#124; indent_except_first acc += tl dot post_subgraph_acc_ab c out_dtype=tl float offs_o += BLOCK_SIZE_O type conversion acc = acc tl float store preparation idx_m = offs_m None idx_p = offs_p None out_mask = idx_m M idx_p P store_output idx_m idx_p acc out_mask val_shape= BLOCK_SIZE_M BLOCK_SIZE_P b b_gemm_right_template = TritonTemplate name= b b_gemm_right grid=b b_gemm_grid debug=False source=r def_kernel A B C B B_GEMM_RIGHT_TRITON_ENTRANCE dynamic shapes M = size A N = size A O = size C P = size C dynamic strides stride_am = stride A stride_an = stride A stride_bn = stride B stride_bo = stride B stride_co = stride C stride_cp = stride C output block counts num_m_block = tl cdiv M BLOCK_SIZE_M num_p_block = tl cdiv P BLOCK_SIZE_P internal block counts num_n_block = tl cdiv N BLOCK_SIZE_N num_o_block = tl cdiv O BLOCK_SIZE_O output block ids pid = tl program_id axis= m_block_id = pid num_p_block p_block_id = pid num_p_block accumulator acc = tl zeros BLOCK_SIZE_M BLOCK_SIZE_P dtype=tl float main loop two cases offs_m = m_block_id BLOCK_SIZE_M + tl arange BLOCK_SIZE_M offs_p = p_block_id BLOCK_SIZE_P + tl arange BLOCK_SIZE_P A subgraph B C offs_n = tl arange BLOCK_SIZE_N _ range num_n_block a_mask = offs_m None M offs_n None N a_ptrs = A + offs_m None stride_am + offs_n None stride_an = tl load a_ptrs mask=a_mask other= tl float BLOCK_SIZE_M BLOCK_SIZE_N acc_bc = tl zeros BLOCK_SIZE_N BLOCK_SIZE_P dtype=tl float offs_o = tl arange BLOCK_SIZE_O __ range num_o_block b_mask = offs_n None N offs_o None O b_ptrs = B + offs_n None stride_bn + offs_o None stride_bo b = tl load b_ptrs mask=b_mask other= tl float BLOCK_SIZE_N BLOCK_SIZE_O c_mask = offs_o None O offs_p None P c_ptrs = C + offs_o None stride_co + offs_p None stride_cp c = tl load c_ptrs mask=c_mask other= tl float BLOCK_SIZE_O BLOCK_SIZE_P acc_bc += tl dot b c out_dtype=tl float offs_o += BLOCK_SIZE_O apply subgraph modification subgraph_number= output_name= post_subgraph_acc_bc inner_mm= acc_bc &#124; indent_except_first acc += tl dot post_subgraph_acc_bc out_dtype=tl float offs_n += BLOCK_SIZE_N type conversion acc = acc tl float store preparation idx_m = offs_m None idx_p = offs_p None out_mask = idx_m M idx_p P store_output idx_m idx_p acc out_mask val_shape= BLOCK_SIZE_M BLOCK_SIZE_P Note load_ratio_left load_ratio_right only calculating numbers trivial subgraph case i e A B C A B C load_ratio_left M int N int O int P int m int n int o int p int - float compute ratio estimated numbers loads baseline b bgemm M N O P matrix sizes m n o p block sizes &#124; &#124; baseline lower bound &#124; b bgemm &#124; load &#124; M N + N O + M O + O P &#124; M m P p O o o p + N n m n + n o &#124; store &#124; M O + M P &#124; M P b bgemm always better stores loads we need find out beneficial cases using function base = M N + N O + M O + O P gemm = ceildiv M m ceildiv P p ceildiv O o o p + ceildiv N n m n + n o base gemm load_ratio_right M int N int O int P int m int n int o int p int - float compute ratio estimated numbers loads baseline b bgemm M N O P matrix sizes m n o p block sizes &#124; &#124; baseline lower bound &#124; b bgemm &#124; load &#124; N O + O P + M N + N P &#124; M m P p N n m n + O o n o + o p &#124; store &#124; N P + M P &#124; M P b bgemm always better stores loads we need find out beneficial cases using function base = N O + O P + M N + N P gemm = ceildiv M m ceildiv P p ceildiv N n m n + ceildiv O o n o + o p base gemm block sizes limited hardware shared memory intuitively optimization works when intermediate matrix large we assign large block sizes large dimensions b b_gemm_configs = BLOCK_SIZE_M BLOCK_SIZE_N BLOCK_SIZE_O BLOCK_SIZE_P num_stages num_warps BLOCK_SIZE_M BLOCK_SIZE_N BLOCK_SIZE_O BLOCK_SIZE_P num_stages num_warps BLOCK_SIZE_M BLOCK_SIZE_N BLOCK_SIZE_O BLOCK_SIZE_P num_stages num_warps BLOCK_SIZE_M BLOCK_SIZE_N BLOCK_SIZE_O BLOCK_SIZE_P num_stages num_warps BLOCK_SIZE_M BLOCK_SIZE_N BLOCK_SIZE_O BLOCK_SIZE_P num_stages num_warps BLOCK_SIZE_M BLOCK_SIZE_N BLOCK_SIZE_O BLOCK_SIZE_P num_stages num_warps BLOCK_SIZE_M BLOCK_SIZE_N BLOCK_SIZE_O BLOCK_SIZE_P num_stages num_warps BLOCK_SIZE_M BLOCK_SIZE_N BLOCK_SIZE_O BLOCK_SIZE_P num_stages num_warps BLOCK_SIZE_M BLOCK_SIZE_N BLOCK_SIZE_O BLOCK_SIZE_P num_stages num_warps BLOCK_SIZE_M BLOCK_SIZE_N BLOCK_SIZE_O BLOCK_SIZE_P num_stages num_warps BLOCK_SIZE_M BLOCK_SIZE_N BLOCK_SIZE_O BLOCK_SIZE_P num_stages num_warps BLOCK_SIZE_M BLOCK_SIZE_N BLOCK_SIZE_O BLOCK_SIZE_P num_stages num_warps is_b b_gemm_good_on is_left_assoc bool A_node torch fx Node B_node torch fx Node C_node torch fx Node - bool checks whether sizes good b b_gemm basic checks all val A_node meta val B_node meta val C_node meta False fake_tensors = A_node meta val B_node meta val C_node meta val torch _subclasses fake_tensor FakeTensor A B C = fake_tensors check_all_attr_true objects attr all hasattr obj attr getattr obj attr obj objects check_all_attr_true fake_tensors is_cuda check_all_attr_true fake_tensors is_xpu False all len A shape == len B shape == len C shape == False A shape == B shape B shape == C shape False size checks we only dispatch B B-GEMM when average load ratio M N = A shape O P = C shape ratios = is_left_assoc config b b_gemm_configs ratio = load_ratio_left M N O P config BLOCK_SIZE_M config BLOCK_SIZE_N config BLOCK_SIZE_O config BLOCK_SIZE_P ratios append ratio config b b_gemm_configs ratio = load_ratio_right M N O P config BLOCK_SIZE_M config BLOCK_SIZE_N config BLOCK_SIZE_O config BLOCK_SIZE_P ratios append ratio ratios sort reverse=True average_ratio = r ratios top choices average_ratio = r average_ratio = average_ratio average_ratio even average_ratio close number stores always better unoptimized_b b_gemm is_left_assoc bool subgraph Subgraph A torch Tensor B torch Tensor C torch Tensor out torch Tensor - torch Tensor The unoptimized version used fallback when b b_gemm kernel beneficial is_left_assoc torch mm subgraph graph_module torch mm A B C out=out torch mm A subgraph graph_module torch mm B C out=out out unoptimized_choice = ExternKernelChoice unoptimized_b b_gemm build_subgraph_buffer args list TensorBox subgraph Subgraph This function adapted kernel flex_attention py The goal take required args produce subgraph buffer The subgraph buffer ComputedBuffer will inlined into triton template Args args The args passed into subgraph subgraph The Subgraph ir which produce output node cnt = env = node subgraph graph_module graph nodes node op == placeholder env node = args cnt cnt += node op == call_function For call_function we use default lowerings pass already created TensorBoxes args args kwargs = tree_map lambda x env get x x node args node kwargs env node = lowerings node target args kwargs node op == output convert_output_node_to_buffer output output None None output_node = output output_buffer = env output_node assert isinstance output_buffer TensorBox The output node B B-GEMM s subgraph must TensorBox got type output_buffer assert isinstance output_buffer data StorageBox The output node B B-GEMM s subgraph must StorageBox got type output_buffer device = output_buffer data get_device assert device None subgraph_buffer = ComputedBuffer name=None layout=FlexibleLayout device=device dtype=output_buffer data get_dtype size=output_buffer data get_size data=output_buffer data data type ignore arg-type subgraph_buffer node args should single element representing output subgraph tree_map convert_output_node_to_buffer node args raise ValueError B B-GEMM passed subgraph no output node create_placeholder name str dtype torch dtype device torch device - TensorBox &#124; ShapeAsConstantBuffer Creates placeholder input buffers producing subgraph_output input_buffer = InputBuffer name=name layout=FixedLayout device dtype TensorBox create input_buffer tuned_b b_gemm is_left_assoc bool subgraph Subgraph A torch _inductor ir TensorBox B torch _inductor ir TensorBox C torch _inductor ir TensorBox layout=None - torch _inductor ir TensorBox call realize get rid Pointwise A realize B realize C realize layout = FixedLayout A get_device_or_error A get_dtype A shape C shape type ignore index placeholders = create_placeholder inner_mm A get_dtype A get_device_or_error subgraph_buffer = build_subgraph_buffer placeholders type ignore arg-type list-item subgraph choices list TritonTemplateCaller = config b b_gemm_configs is_left_assoc b b_gemm_left_template maybe_append_choice choices input_nodes= A B C layout=layout subgraphs= subgraph_buffer config b b_gemm_right_template maybe_append_choice choices input_nodes= A B C layout=layout subgraphs= subgraph_buffer config add unoptimized choice mitigate performance degradation choices append unoptimized_choice bind A B C layout is_left_assoc=is_left_assoc subgraph=subgraph autotune autotune_select_algorithm b b_gemm choices A B C layout match inner mm potential b b_gemm register_graph_pattern CallFunction torch ops aten mm Arg Arg pyrefly ignore bad-argument-type pass_dict=B B_GEMM_PASS b b_gemm_handler match Match mat torch fx Node mat torch fx Node - None match args list torch fx Node is_pointwise_node node torch fx Node - bool node op == call_function isinstance node target torch _ops OpOverload torch Tag pointwise node target tags is_mm node torch fx Node - bool node target torch ops aten mm default inner MM inner_mm = match nodes - find candidate outer MM which will re-checked below ensure every path reaches In real A f B C every path starting B C must reach A _ outer_mm = None node = inner_mm while len node users node = next iter node users is_mm node outer_mm = node break is_pointwise_node node continue break outer_mm find unique input node outer_mm representing f B C A f B C we call f_node when pattern simply A B C f_node just inner_mm f_node = inner_mm while next iter f_node users outer_mm f_node = next iter f_node users all_reach_via_pointwise_with_no_other_inputs src torch fx Node dst torch fx Node - tuple bool OrderedSet torch fx Node check whether every user path src reaches dst via pointwise nodes no other input nodes intermediates dst Boolean value subgraph node set including src dst which only makes sense when Boolean value True visited = OrderedSet torch fx Node input_counter dict torch fx Node int = all_reachable = True queue = deque src while queue node = queue popleft node visited node dst visited add node node src is_pointwise_node node user node users keys nodes other than dst bookkeep their users input counts user input_counter input_counter user = len user all_input_nodes input_counter user -= continue BFS queue append user visited add node all_reachable = False break all_reachable all count == count input_counter values visited check inner_mm reaches f_node every user path via pointwise nodes no outside input_nodes ok subgraph_node_set = all_reach_via_pointwise_with_no_other_inputs inner_mm f_node ok check inner_mm s inputs f_node s outputs len inner_mm all_input_nodes == len f_node users == point nodes between inner_mm f_node both included all used internally inside A subgraph B C i e they neither have other users nor have other inputs original graph module graph module = inner_mm graph inner_mm graph owning_module construct new sub graph subgraph_node_list list torch fx Node = ordered list nodes used node removal later new_graph torch fx Graph = torch fx Graph node_remapping dict torch fx Node torch fx Node = new_input_anchor torch fx Node inner_mm changed input node new_output_anchor torch fx Node f_node used construct output node new_input_node torch fx Node new_output_node torch fx Node node graph nodes preserve order nodes node subgraph_node_set subgraph_node_list append node new_node = new_graph node_copy node lambda x node_remapping get x x node_remapping node = new_node node inner_mm new_input_anchor = new_node node f_node new_output_anchor = new_node pyrefly ignore unbound-name new_input_anchor new_output_anchor subgraph non-trivial update input node pyrefly ignore unbound-name new_graph inserting_before new_input_anchor new_input_node = new_graph placeholder name= subgraph_input pyrefly ignore unbound-name new_input_node meta update new_input_anchor meta pyrefly ignore unbound-name new_input_anchor replace_all_uses_with new_input_node pyrefly ignore unbound-name new_graph erase_node new_input_anchor add output node pyrefly ignore unbound-name new_output_node = new_graph output new_output_anchor pyrefly ignore unbound-name new_output_node meta update new_output_anchor meta subgraph trivial e g A B C update input node pyrefly ignore unbound-name new_graph inserting_before new_input_anchor new_input_node = new_graph placeholder name= subgraph_input pyrefly ignore unbound-name new_input_node meta update new_input_anchor meta pyrefly ignore unbound-name new_input_anchor replace_all_uses_with new_input_node pyrefly ignore unbound-name new_graph erase_node new_input_anchor update output node don t use new_output_anchor since has been erased new_output_node = new_graph output new_input_node new_output_node meta update new_input_node meta new_graph lint construct subgraph subgraph = Subgraph name= subgraph graph_module=torch fx GraphModule module new_graph two cases subgraph A B C called left_assoc A subgraph B C called right_assoc is_left_assoc = outer_mm args f_node find nodes A B C check sizes A torch fx Node B torch fx Node C torch fx Node is_left_assoc A = inner_mm args type ignore assignment B = inner_mm args type ignore assignment C = outer_mm args type ignore assignment A = outer_mm args type ignore assignment B = inner_mm args type ignore assignment C = inner_mm args type ignore assignment is_b b_gemm_good_on is_left_assoc A B C finally update original graph counters inductor b b_gemm += graph = match graph graph inserting_before outer_mm function = functools partial tuned_b b_gemm is_left_assoc subgraph function __name__ = tuned_b b_gemm __name__ type ignore attr-defined function _inductor_lowering_function = True type ignore attr-defined replacement torch fx Node = graph call_function function A B C match kwargs replacement meta update outer_mm meta outer_mm replace_all_uses_with replacement erase unnecessary nodes graph erase_node outer_mm node reversed subgraph_node_list graph erase_node node graph lint