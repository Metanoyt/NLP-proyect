Owner s module unknown typing Any torch ao pruning BaseSparsifier torch torch nn functional F torch nn ImplementedSparsifier BaseSparsifier __init__ kwargs dict str Any - None super __init__ defaults=kwargs update_mask module nn Module tensor_name str kwargs dict str Any - None module parametrizations weight mask = type ignore index union-attr linear_state = state linear weight linear_state step_count = linear_state get step_count + MockSparseLinear nn Linear This MockSparseLinear check convert functionality It same normal Linear layer except different type well additional from_dense method classmethod from_dense cls mod nn Linear - MockSparseLinear linear = cls mod in_features mod out_features linear rows_are_subset subset_tensor torch Tensor superset_tensor torch Tensor - bool Checks see all rows subset tensor present superset tensor i = row subset_tensor while i len superset_tensor torch equal row superset_tensor i i += break False True SimpleLinear nn Module r Model only Linear layers without biases some wrapped Sequential some following Sequential Used test basic pruned Linear-Linear fusion __init__ - None super __init__ seq = nn Sequential nn Linear bias=False nn Linear bias=False nn Linear bias=False linear = nn Linear bias=False linear = nn Linear bias=False forward x torch Tensor - torch Tensor x = seq x x = linear x x = linear x x LinearBias nn Module r Model only Linear layers alternating layers biases wrapped Sequential Used test pruned Linear-Bias-Linear fusion __init__ - None super __init__ seq = nn Sequential nn Linear bias=True nn Linear bias=False nn Linear bias=True nn Linear bias=True nn Linear bias=False forward x torch Tensor - torch Tensor x = seq x x LinearActivation nn Module r Model only Linear layers some bias some Sequential some following Activation functions modules between each Linear Sequential each outside layer Used test pruned Linear Bias -Activation-Linear fusion __init__ - None super __init__ seq = nn Sequential nn Linear bias=True nn ReLU nn Linear bias=False nn Tanh nn Linear bias=True linear = nn Linear bias=True act = nn ReLU linear = nn Linear bias=False act = nn Tanh forward x torch Tensor - torch Tensor x = seq x x = linear x x = act x x = linear x x = act x x LinearActivationFunctional nn Module r Model only Linear layers some bias some Sequential some following Activation functions modules between each Linear Sequential functional activationals called between each outside layer Used test pruned Linear Bias -Activation-Linear fusion __init__ - None super __init__ seq = nn Sequential nn Linear bias=True nn ReLU nn Linear bias=False nn ReLU nn Linear bias=True linear = nn Linear bias=True linear = nn Linear bias=False linear = nn Linear bias=False act = nn ReLU forward x torch Tensor - torch Tensor x = seq x x = linear x x = F relu x x = linear x x = F relu x x = linear x x = F relu x x SimpleConv d nn Module r Model only Conv d layers all without bias some Sequential some following Used test pruned Conv d-Conv d fusion __init__ - None super __init__ seq = nn Sequential nn Conv d bias=False nn Conv d bias=False conv d = nn Conv d bias=False conv d = nn Conv d bias=False forward x torch Tensor - torch Tensor x = seq x x = conv d x x = conv d x x Conv dBias nn Module r Model only Conv d layers some bias some Sequential some outside Used test pruned Conv d-Bias-Conv d fusion __init__ - None super __init__ seq = nn Sequential nn Conv d bias=True nn Conv d bias=True nn Conv d bias=False conv d = nn Conv d bias=True conv d = nn Conv d bias=False forward x torch Tensor - torch Tensor x = seq x x = conv d x x = conv d x x Conv dActivation nn Module r Model only Conv d layers some bias some Sequential some following Activation function modules between each Sequential layer functional activations called in-between each outside layer Used test pruned Conv d-Bias-Activation-Conv d fusion __init__ - None super __init__ seq = nn Sequential nn Conv d bias=True nn ReLU nn Conv d bias=True nn Tanh nn Conv d bias=False nn ReLU conv d = nn Conv d bias=False conv d = nn Conv d bias=True forward x torch Tensor - torch Tensor x = seq x x = conv d x x = F relu x x = conv d x x = F hardtanh x x Conv dPadBias nn Module r Model only Conv d layers all bias some padding some Sequential some following Activation function modules between each layer Used test bias propagated correctly special case pruned Conv d-Bias- Activation Conv d fusion when second Conv d layer has padding __init__ - None super __init__ seq = nn Sequential nn Conv d padding= bias=True nn ReLU nn Conv d bias=False nn ReLU nn Conv d padding= bias=True nn ReLU nn Conv d padding= bias=True nn ReLU nn Conv d bias=True nn Tanh conv d = nn Conv d padding= bias=True act = nn ReLU conv d = nn Conv d padding= bias=True act = nn Tanh forward x torch Tensor - torch Tensor x = seq x x = conv d x x = act x x = conv d x x = act x x Conv dPool nn Module r Model only Conv d layers all bias some Sequential some following Activation function modules between each layer Pool d modules between each layer Used test pruned Conv d-Pool d-Conv d fusion __init__ - None super __init__ seq = nn Sequential nn Conv d kernel_size= padding= bias=True nn MaxPool d kernel_size= stride= padding= nn ReLU nn Conv d kernel_size= padding= bias=True nn Tanh nn AvgPool d kernel_size= stride= padding= conv d = nn Conv d kernel_size= padding= bias=True maxpool = nn MaxPool d kernel_size= stride= padding= af = nn ReLU conv d = nn Conv d kernel_size= padding= bias=True conv d = nn Conv d kernel_size= padding= bias=True forward x torch Tensor - torch Tensor x = seq x x = conv d x x = maxpool x x = af x x = conv d x x = F avg_pool d x kernel_size= stride= padding= x = F relu x x = conv d x x Conv dPoolFlattenFunctional nn Module r Model Conv d layers all bias some Sequential some following then Pool d functional Flatten followed Linear layer Activation functions Pool ds between each layer also Used test pruned Conv d-Pool d-Flatten-Linear fusion __init__ - None super __init__ seq = nn Sequential nn Conv d kernel_size= padding= bias=True nn MaxPool d kernel_size= stride= padding= nn ReLU nn Conv d kernel_size= padding= bias=True nn Tanh nn AvgPool d kernel_size= stride= padding= conv d = nn Conv d kernel_size= padding= bias=True af = nn ReLU conv d = nn Conv d kernel_size= padding= bias=True avg_pool = nn AdaptiveAvgPool d fc = nn Linear bias=True forward x torch Tensor - torch Tensor x = seq x x = conv d x x = F max_pool d x kernel_size= stride= padding= x = af x x = conv d x x = avg_pool x x = torch flatten x test functional flatten x = fc x x Conv dPoolFlatten nn Module r Model Conv d layers all bias some Sequential some following then Pool d Flatten module followed Linear layer Activation functions Pool ds between each layer also Used test pruned Conv d-Pool d-Flatten-Linear fusion __init__ - None super __init__ seq = nn Sequential nn Conv d kernel_size= padding= bias=True nn MaxPool d kernel_size= stride= padding= nn ReLU nn Conv d kernel_size= padding= bias=True nn Tanh nn AvgPool d kernel_size= stride= padding= conv d = nn Conv d kernel_size= padding= bias=True af = nn ReLU conv d = nn Conv d kernel_size= padding= bias=True avg_pool = nn AdaptiveAvgPool d flatten = nn Flatten fc = nn Linear bias=True forward x torch Tensor - torch Tensor x = seq x x = conv d x x = F max_pool d x kernel_size= stride= padding= x = af x x = conv d x x = avg_pool x x = flatten x x = fc x x LSTMLinearModel nn Module Container module encoder recurrent module linear __init__ input_dim int hidden_dim int output_dim int num_layers int - None super __init__ lstm = nn LSTM input_dim hidden_dim num_layers linear = nn Linear hidden_dim output_dim forward input torch Tensor - tuple torch Tensor torch Tensor output _hidden = lstm input decoded = linear output decoded output LSTMLayerNormLinearModel nn Module Container module LSTM LayerNorm linear __init__ input_dim int hidden_dim int output_dim int num_layers int - None super __init__ lstm = nn LSTM input_dim hidden_dim num_layers norm = nn LayerNorm hidden_dim linear = nn Linear hidden_dim output_dim forward x torch Tensor - tuple torch Tensor torch Tensor x state = lstm x x = norm x x = linear x x state