Owner s oncall quantization ruff noqa F collections OrderedDict contextlib torch torch nn functional F torch nn nn torch ao nn quantized nnq torch ao nn quantized reference nnqr torch ao nn quantized dynamic nnqd torch ao nn intrinsic nni torch ao nn intrinsic quantized nniq torch ao nn intrinsic quantized dynamic nniqd torch multiprocessing mp torch fx graph_module _USER_PRESERVED_ATTRIBUTES_KEY graph mode quantization based fx torch ao quantization quantize_fx prepare_fx convert_fx convert_to_reference_fx _convert_to_reference_decomposed_fx prepare_qat_fx fuse_fx torch ao quantization fx quantize_handler DefaultNodeQuantizeHandler torch ao quantization fx match_utils _is_match MatchAllNode torch ao quantization QuantType torch ao quantization quant_type _get_quant_type_to_str torch ao quantization QuantStub DeQuantStub QuantWrapper default_qconfig default_dynamic_qconfig default_per_channel_qconfig default_qat_qconfig default_reuse_input_qconfig default_symmetric_qnnpack_qconfig default_symmetric_qnnpack_qat_qconfig per_channel_dynamic_qconfig float _dynamic_qconfig float _static_qconfig float_qparams_weight_only_qconfig float_qparams_weight_only_qconfig_ bit get_default_qconfig get_default_qat_qconfig get_default_qconfig_mapping get_default_qat_qconfig_mapping fuse_modules fuse_modules_qat prepare prepare_qat convert quantize_dynamic default_placeholder_observer default_weight_observer PerChannelMinMaxObserver FixedQParamsFakeQuantize FixedQParamsObserver FusedMovingAvgObsFakeQuantize FakeQuantize MovingAverageMinMaxObserver HistogramObserver ReuseInputObserver QConfig default_embedding_qat_qconfig torch ao quantization backend_config get_fbgemm_backend_config get_qnnpack_backend_config BackendConfig BackendPatternConfig DTypeConfig DTypeWithConstraints ObservationType torch ao quantization backend_config native get_test_only_legacy_native_backend_config torch ao quantization qconfig_mapping _get_symmetric_qnnpack_qconfig_mapping _get_symmetric_qnnpack_qat_qconfig_mapping _GLOBAL_DICT_KEY _MODULE_NAME_DICT_KEY _MODULE_NAME_OBJECT_TYPE_ORDER_DICT_KEY _MODULE_NAME_REGEX_DICT_KEY _OBJECT_TYPE_DICT_KEY QConfigMapping torch ao quantization fx qconfig_mapping_utils _get_object_type_qconfig _get_module_name_qconfig _get_module_name_regex_qconfig _maybe_adjust_qconfig_for_module_name_object_type_order torch ao quantization fx pattern_utils _DEFAULT_FUSION_PATTERNS _DEFAULT_QUANTIZATION_PATTERNS _DEFAULT_OUTPUT_FAKE_QUANTIZE_MAP _DEFAULT_OUTPUT_OBSERVER_MAP _register_fusion_pattern _register_quant_pattern get_default_output_activation_post_process_map torch ao quantization fx custom_config STANDALONE_MODULE_NAME_DICT_KEY STANDALONE_MODULE_CLASS_DICT_KEY FLOAT_TO_OBSERVED_DICT_KEY OBSERVED_TO_QUANTIZED_DICT_KEY NON_TRACEABLE_MODULE_NAME_DICT_KEY NON_TRACEABLE_MODULE_CLASS_DICT_KEY INPUT_QUANTIZED_INDEXES_DICT_KEY OUTPUT_QUANTIZED_INDEXES_DICT_KEY PRESERVED_ATTRIBUTES_DICT_KEY FuseCustomConfig ConvertCustomConfig PrepareCustomConfig StandaloneModuleConfigEntry torch ao quantization fx lstm_utils torch ao quantization fx utils _reroute_tuple_getitem_pattern NodeInfo torch ao quantization fake_quantize default_fixed_qparams_range_ _fake_quant default_fixed_qparams_range_neg _fake_quant torch ao quantization observer default_fixed_qparams_range_ _observer default_fixed_qparams_range_neg _observer MinMaxObserver _is_activation_post_process test utils hypothesis given settings hypothesis strategies st torch testing _internal common_cuda TEST_MULTIGPU TEST_CUDA torch testing _internal common_quantization LinearReluLinearModel LinearReluModel LinearBnLeakyReluModel LinearTanhModel ConvBnAddReluModel QuantizationTestCase skipIfNoFBGEMM skipIfNoQNNPACK skip_if_no_torchvision train_one_epoch run_ddp test_only_eval_fn test_only_train_fn ModelForConvTransposeBNFusion get_supported_device_types skipIfNoONEDNN torch testing _internal common_quantization LinearModelWithSubmodule ResNetBase RNNDynamicModel RNNCellDynamicModel torch testing _internal common_quantized supported_qengines override_qengines override_quantized_engine torch testing _internal common_utils TemporaryFileName IS_ARM skipIfTorchDynamo torch testing _internal common_quantization NodeSpec ns torch testing FileCheck copy itertools operator unittest io typing Callable Optional BinaryOp torch nn Module __init__ binary_op ibinary_op is_inplace is_scalar ibinary_op means inplace binary op super __init__ conv = torch nn Conv d float conv = torch nn Conv d float is_scalar = is_scalar op = ibinary_op ibinary_op is_inplace binary_op forward x y x = conv x y = is_scalar conv y x = x + y x = op x y x = y + x x = op y x x BinaryOpNonQuantizedInput torch nn Module __init__ binary_op ibinary_op is_inplace is_scalar ibinary_op means inplace binary op super __init__ is_scalar = is_scalar op = ibinary_op ibinary_op is_inplace binary_op forward x y y = is_scalar y x = op x y x BinaryOpRelu torch nn Module __init__ binary_op ibinary_op is_inplace relu_callable is_scalar ibinary_op means inplace binary op super __init__ conv = torch nn Conv d float conv = torch nn Conv d float op = ibinary_op ibinary_op is_inplace binary_op relu_callable = relu_callable is_scalar = is_scalar relu_callable torch nn ReLU relu = torch nn ReLU relu = relu_callable forward x y x = conv x y = is_scalar conv y x = op x y x = relu x x = op y x x = relu x x torch fx wrap _user_func_with_complex_return_type x list torch split x TestFuseFx QuantizationTestCase test_fuse_conv_bn_relu M torch nn Module __init__ - None super __init__ conv d = nn Conv d conv d = nn Conv d conv d = nn Conv d bn d = nn BatchNorm d bn d = nn BatchNorm d bn d = nn BatchNorm d conv d = nn Conv d conv d = nn Conv d conv d = nn Conv d bn d = nn BatchNorm d bn d = nn BatchNorm d bn d = nn BatchNorm d relu = nn ReLU forward x x = conv d x x = bn d x x = conv d x x = bn d x x = conv d x x = bn d x x = conv d x x = bn d x x = relu x x = conv d x x = bn d x x = relu x x = conv d x x = bn d x x = relu x x test train mode m = M train currently we don t check module configured qconfig before fusion TODO we decide do future test needs updated train mode fuse_fx called prepare_qat_fx m = prepare_qat_fx m example_inputs= torch randn expected_nodes = ns call_module nni ConvBn d ns call_module nni ConvBn d ns call_module nni ConvBn d ns call_module nni ConvBnReLU d ns call_module nni ConvBnReLU d ns call_module nni ConvBnReLU d expected_occurrence = ns call_module nn ReLU checkGraphModuleNodes m expected_node_list=expected_nodes expected_node_occurrence=expected_occurrence test eval mode m = M eval fuse_fx top level api only supports eval mode m = fuse_fx m expected_nodes = ns call_module nn Conv d ns call_module nn Conv d ns call_module nn Conv d ns call_module nni ConvReLU d ns call_module nni ConvReLU d ns call_module nni ConvReLU d ConvBnRelu d fused expected_occurrence = ns call_module nn ReLU checkGraphModuleNodes m expected_node_list=expected_nodes expected_node_occurrence=expected_occurrence test_fuse_linear_bn_eval M torch nn Module __init__ - None super __init__ linear = nn Linear bn d = nn BatchNorm d forward x x = linear x x = bn d x x test eval mode m = M eval fuse_fx top level api only supports eval mode m = fuse_fx m expected_nodes = ns call_module nn Linear expected_occurrence = ns call_module nn BatchNorm d checkGraphModuleNodes m expected_node_list=expected_nodes expected_node_occurrence=expected_occurrence skipIfNoONEDNN test_fuse_linear_bn_leaky_relu_onednn linear - bn - leaky_relu fused onednn backend only torch ao quantization backend_config get_onednn_backend_config expected_nodes = ns call_module nni LinearLeakyReLU expected_occurrence = ns call_module nn BatchNorm d ns call_module nn LeakyReLU with_bn True False test eval mode m = LinearBnLeakyReluModel with_bn eval fuse_fx top level api only supports eval mode m = fuse_fx m backend_config=get_onednn_backend_config checkGraphModuleNodes m expected_node_list=expected_nodes expected_node_occurrence=expected_occurrence test_linear_bn_leaky_relu_not_fused_by_default Make sure linear - bn - leaky_relu fused default with_bn True False test eval mode m = LinearBnLeakyReluModel with_bn eval fuse_fx top level api only supports eval mode m = fuse_fx m expected_nodes = ns call_module nn Linear ns call_module nn LeakyReLU expected_occurrence = ns call_module nni LinearLeakyReLU checkGraphModuleNodes m expected_node_list=expected_nodes expected_node_occurrence=expected_occurrence skipIfNoONEDNN test_fuse_linear_tanh_for_onednn_backend linear - tanh fused onednn backend only torch ao quantization backend_config get_onednn_backend_config expected_nodes = ns call_module nni LinearTanh expected_occurrence = ns call_module nn Linear ns call_module nn Tanh test eval mode m = LinearTanhModel eval fuse_fx top level api only supports eval mode m = fuse_fx m backend_config=get_onednn_backend_config checkGraphModuleNodes m expected_node_list=expected_nodes expected_node_occurrence=expected_occurrence test_linear_tanh_not_fused_by_default Make sure linear - tanh fused default test eval mode m = LinearTanhModel eval fuse_fx top level api only supports eval mode m = fuse_fx m expected_nodes = ns call_module nn Linear ns call_module nn Tanh expected_occurrence = ns call_module nni LinearTanh checkGraphModuleNodes m expected_node_list=expected_nodes expected_node_occurrence=expected_occurrence test_fuse_conv_bn_add_relu_onednn conv - bn - add - relu fused onednn backend only torch ao quantization backend_config get_onednn_backend_config options = itertools product True False with_bn True False with_relu True False conv left True False with_two_conv True False use_torch_add with_bn with_relu left_conv two_conv use_torch_add options expected_nodes = ns call_module nni ConvAddReLU d with_relu nni ConvAdd d expected_occurrence = ns call_module nni ConvAddReLU d with_relu nni ConvAdd d ns call_module nn BatchNorm d test eval mode m = ConvBnAddReluModel with_bn=with_bn with_relu=with_relu left_conv=left_conv two_conv=two_conv use_torch_add=use_torch_add eval m = fuse_fx m backend_config=get_onednn_backend_config checkGraphModuleNodes m expected_node_list=expected_nodes expected_node_occurrence=expected_occurrence test_fuse_conv_bn_add_relu_by_default options = itertools product True False with_bn True False with_relu True False conv left True False with_two_conv True False use_torch_add with_bn with_relu left_conv two_conv use_torch_add options test eval mode expected_nodes = ns call_module nn Conv d expected_occurrence = ns call_module nni ConvAdd d m = ConvBnAddReluModel with_bn=with_bn with_relu=with_relu left_conv=left_conv two_conv=two_conv use_torch_add=use_torch_add eval m = fuse_fx m checkGraphModuleNodes m expected_node_list=expected_nodes expected_node_occurrence=expected_occurrence skipIfNoONEDNN test_fuse_conv_bn_add_relu_lowering Test fusion lowering Conv d - bn - ReLU FX For onednn backedn only torch ao quantization backend_config get_onednn_backend_config qconfig_mapping = get_default_qconfig_mapping onednn override_quantized_engine onednn options = itertools product True False with_bn True False with_relu True False conv left True False two_conv True False use_torch_add with_bn with_relu left_conv two_conv use_torch_add options node_occurrence = ns call_function torch quantize_per_tensor two_conv ns call_method dequantize ns call_module nniq ConvAddReLU d with_relu nniq ConvAdd d ns call_module nn Conv d ns call_module nn ReLU node_occurrence_ref = ns call_function torch quantize_per_tensor ns call_method dequantize test eval mode m = ConvBnAddReluModel with_bn=with_bn with_relu=with_relu left_conv=left_conv two_conv=two_conv use_torch_add=use_torch_add eval example_x = m get_example_inputs m = prepare_fx m qconfig_mapping example_inputs=example_x backend_config=get_onednn_backend_config m_copy = copy deepcopy m m = convert_fx m backend_config=get_onednn_backend_config m_ref = convert_to_reference_fx m_copy checkGraphModuleNodes m expected_node_occurrence=node_occurrence checkGraphModuleNodes m_ref expected_node_occurrence=node_occurrence_ref m example_x test_fuse_convtranspose_bn_eval m = ModelForConvTransposeBNFusion eval m = fuse_fx m expected_nodes = ns call_module nn ConvTranspose d ns call_module nn ConvTranspose d ns call_module nn ConvTranspose d expected_occurrence = ns call_module nn BatchNorm d ns call_module nn BatchNorm d ns call_module nn BatchNorm d checkGraphModuleNodes m expected_node_list=expected_nodes expected_node_occurrence=expected_occurrence test_fuse_module_relu M torch nn Module __init__ - None super __init__ conv d = nn Conv d conv d = nn Conv d conv d = nn Conv d bn d = nn BatchNorm d bn d = nn BatchNorm d bn d = nn BatchNorm d relu = nn ReLU forward x x = conv d x x = relu x x = conv d x x = relu x x = conv d x x = relu x x = bn d x x = relu x x = bn d x x = relu x x = bn d x x = relu x x m = M eval m = fuse_fx m expected_nodes = ns call_module nni ConvReLU d ns call_module nni ConvReLU d ns call_module nni ConvReLU d ns call_module nni BNReLU d ns call_module nni BNReLU d checkGraphModuleNodes m expected_node_list=expected_nodes skipIfNoFBGEMM test_qconfig_fused_module TODO add test all fused modules qconfig_dict = None object_type nn Linear default_qconfig nn ReLU default_qconfig F relu default_qconfig linearRelu_node_list = ns call_function torch quantize_per_tensor ns call_module nniq LinearReLU ns call_method dequantize linearReluLinear_node_list = ns call_function torch quantize_per_tensor ns call_module nniq LinearReLU ns call_module nnq Linear ns call_method dequantize tests = LinearReluModel linearRelu_node_list LinearReluLinearModel linearReluLinear_node_list M node_list tests m = M eval example_inputs = torch rand prepared = prepare_fx m qconfig_dict example_inputs=example_inputs prepared example_inputs quantized = convert_fx prepared checkGraphModuleNodes quantized expected_node_list=node_list test_problematic_fuse_example LinearRelu nn Sequential __init__ - None super __init__ nn Linear nn ReLU M torch nn Module __init__ - None super __init__ lin_relu = LinearRelu linear = nn Linear forward x x = lin_relu x x = linear x x model = M eval these qconfigs somehow fail equality where default_qconfig does qconfig_dict = None object_type torch nn Linear get_default_qconfig fbgemm torch nn ReLU get_default_qconfig fbgemm m = prepare_fx model qconfig_dict example_inputs= torch randn checkGraphModuleNodes m expected_node=ns call_module torch ao nn intrinsic modules fused LinearReLU unittest skip Temporarily skipping test case will enable after simple pattern format supported test_fuse_addtional_fuser_method MyConvReLU torch nn Module pass my_conv_relu_fuser conv relu MyConvReLU M torch nn Module __init__ - None super __init__ conv = torch nn Conv d relu = torch nn ReLU forward x relu conv x m = M eval m = fuse_fx m fuse_custom_config= additional_fuser_method_mapping torch nn Conv d torch nn ReLU my_conv_relu_fuser checkGraphModuleNodes m expected_node=ns call_module MyConvReLU test_fuse_custom_pattern M torch nn Module __init__ use_torch_add=True super __init__ conv = torch nn Conv d bn = torch nn BatchNorm d relu = torch nn ReLU maxpool = torch nn MaxPool d use_torch_add add = torch add add = operator add forward x y = x y = maxpool x x = conv x x = bn x x = add y x x = relu x x use_torch_add True False m = M use_torch_add eval fuse_conv_bn_relu is_qat relu add_pattern _ _ bn_pattern = add_pattern bn conv = bn_pattern conv conv_bn_res_relu_config = BackendPatternConfig \ _set_pattern_complex_format nn ReLU torch add MatchAllNode nn BatchNorm d nn Conv d \ set_fuser_method fuse_conv_bn_relu conv_bn_res_relu_config = BackendPatternConfig \ _set_pattern_complex_format nn ReLU operator add MatchAllNode nn BatchNorm d nn Conv d \ set_fuser_method fuse_conv_bn_relu backend_config = BackendConfig \ set_backend_pattern_config conv_bn_res_relu_config \ set_backend_pattern_config conv_bn_res_relu_config m = fuse_fx m backend_config=backend_config assertEqual type m conv torch nn Conv d check bn relu gone since we replaced whole pattern conv assertFalse hasattr m bn assertFalse hasattr m relu test_fusion_pattern_with_multiple_inputs This test tests two keys backend_config root_node_getter extra_inputs_getter root_node_getter used identify root module node pattern node we ll keep after fusion extra_inputs_getter will list node needs added fused node extra inputs M torch nn Module __init__ - None super __init__ conv = torch nn Conv d bn = torch nn BatchNorm d relu = torch nn ReLU maxpool = torch nn MaxPool d forward x y = x y = maxpool x x = conv x x = bn x x = torch add x y x = relu x x m = M eval fuse_conv_bn_relu is_qat relu add_pattern _ bn_pattern _ = add_pattern bn conv = bn_pattern conv conv_bn_res_relu_root_node_getter pattern relu add_pattern = pattern _ bn_pattern _ = add_pattern bn conv = bn_pattern conv conv_bn_res_relu_extra_inputs_getter pattern get inputs pattern extra inputs inputs root node assumed copied over root node fused node relu add_pattern = pattern _ bn_pattern extra_input = add_pattern bn conv = bn_pattern extra_input conv_bn_res_relu_config = BackendPatternConfig \ _set_pattern_complex_format nn ReLU torch add nn BatchNorm d nn Conv d MatchAllNode \ set_fuser_method fuse_conv_bn_relu \ _set_root_node_getter conv_bn_res_relu_root_node_getter \ _set_extra_inputs_getter conv_bn_res_relu_extra_inputs_getter backend_config = BackendConfig set_backend_pattern_config conv_bn_res_relu_config m = fuse_fx m backend_config=backend_config assertEqual type m conv torch nn Conv d check bn relu gone since we replaced whole pattern conv assertFalse hasattr m bn assertFalse hasattr m relu check conv module has two inputs named_modules = dict m named_modules node m graph nodes node op == call_module type named_modules node target torch nn Conv d assertTrue len node args == msg= Expecting fused op have two arguments test_fusion_pattern_with_matchallnode This test tests node matched MatchAllNode will regared input instead module fused For instance we have two patterns nn ReLU torch add MatchAllNode nn Conv d nn ReLU nn Conv d And we wanna fuse following model Conv d - ReLU + Conv d ------ Add - ReLU ReLU first row matched MatchAllNode residual pattern But won t fused part pattnern It needs properly fused upstream Conv d M torch nn Module __init__ - None super __init__ conv = torch nn Conv d relu = torch nn ReLU conv = torch nn Conv d relu = torch nn ReLU forward x y = conv x y = relu y x = conv x x = torch add x y x = relu x x m = M eval fuse_conv_relu is_qat conv relu conv fuse_conv_res_relu is_qat relu add_pattern _ conv _ = add_pattern conv conv_res_relu_root_node_getter pattern relu _ conv _ = pattern conv conv_res_relu_extra_inputs_getter pattern relu _ _ extra_input = pattern extra_input conv_relu_config = BackendPatternConfig nn Conv d nn ReLU \ set_fuser_method fuse_conv_relu conv_res_relu_config = BackendPatternConfig \ _set_pattern_complex_format nn ReLU torch add nn Conv d MatchAllNode \ set_fuser_method fuse_conv_res_relu \ _set_root_node_getter conv_res_relu_root_node_getter \ _set_extra_inputs_getter conv_res_relu_extra_inputs_getter backend_config = BackendConfig \ set_backend_pattern_config conv_relu_config \ set_backend_pattern_config conv_res_relu_config m = fuse_fx m backend_config=backend_config assertEqual type m conv torch nn Conv d assertEqual type m conv torch nn Conv d check relu gone since we replaced both patterns conv assertFalse hasattr m relu assertFalse hasattr m relu skipIfNoFBGEMM TestQuantizeFx QuantizationTestCase test_pattern_match test MatchAllNode conv - bn - add - relu pattern M torch nn Module __init__ - None super __init__ conv = nn Conv d bn = nn BatchNorm d relu = nn ReLU forward x y x = conv x x = bn x x = x + y x = relu x x pattern = nn ReLU operator add nn BatchNorm d nn Conv d MatchAllNode m = torch fx symbolic_trace M modules = dict m named_modules n m graph nodes n op == call_module type modules n target nn ReLU assertTrue _is_match modules n pattern test_pattern_match_constant M torch nn Module forward x x _ = torch ops aten max_pool d_with_indices default x x pattern = operator getitem torch ops aten max_pool d_with_indices default m = torch fx symbolic_trace M eliminate code get second output maxpool so pattern can matched m graph eliminate_dead_code modules = dict m named_modules n m graph nodes n op == call_function n target == operator getitem assertTrue _is_match modules n pattern test_fused_module_qat_swap Tmp torch nn Module __init__ - None super __init__ tmp = torch nn Linear relu = torch nn ReLU forward x x = tmp x relu x M torch nn Module __init__ - None super __init__ mods = torch nn Sequential Tmp torch nn Linear mods = torch nn Linear forward x = mods x x = torch add x x = mods x x = torch add x x model = M train qconfig_dict = None object_type torch nn Linear default_qat_qconfig torch nn ReLU default_qat_qconfig prepared = prepare_qat_fx model qconfig_dict example_inputs= torch randn assertTrue isinstance getattr prepared mods tmp torch ao nn intrinsic qat LinearReLU _get_conv_linear_test_cases is_reference Returns list test cases format is_dynamic ModuleClass module_constructor_inputs inputs quantized_node weight_prepack_op FunctionalConv d torch nn Module __init__ weight super __init__ weight = torch nn Parameter weight stride = padding = dilation = groups = forward x F conv d x weight None stride padding dilation groups Conv d torch nn Module __init__ args super __init__ conv = torch nn Conv d args forward x conv x conv d_input = torch rand conv d_weight = torch rand conv d_module_args = FunctionalConv d torch nn Module __init__ weight super __init__ weight = torch nn Parameter weight stride = padding = dilation = groups = forward x F conv d x weight None stride padding dilation groups Conv d torch nn Module __init__ args super __init__ conv = torch nn Conv d args forward x conv x conv d_input = torch rand conv d_weight = torch rand conv d_module_args = FunctionalConv d torch nn Module __init__ weight super __init__ weight = torch nn Parameter weight stride = padding = dilation = groups = forward x F conv d x weight None stride padding dilation groups Conv d torch nn Module __init__ args super __init__ conv = torch nn Conv d args forward x conv x conv d_input = torch rand conv d_weight = torch rand conv d_module_args = Linear torch nn Module __init__ weight super __init__ weight = torch nn Parameter weight forward x F linear x weight linear_input = torch rand linear_weight = torch rand LinearModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x linear_module_input = torch rand is_dynamic ModuleClass module_constructor_inputs inputs quantized_node weight_prepack_node tests = False FunctionalConv d conv d_weight conv d_input ns call_function torch nn functional conv d is_reference torch ops quantized conv d ns call_function torch ops quantized conv d_prepack False FunctionalConv d conv d_weight conv d_input ns call_function torch nn functional conv d is_reference torch ops quantized conv d ns call_function torch ops quantized conv d_prepack False FunctionalConv d conv d_weight conv d_input ns call_function torch nn functional conv d is_reference torch ops quantized conv d ns call_function torch ops quantized conv d_prepack False Conv d conv d_module_args conv d_input ns call_module nnqr Conv d is_reference nnq Conv d None False Conv d conv d_module_args conv d_input ns call_module nnqr Conv d is_reference nnq Conv d None False Conv d conv d_module_args conv d_input ns call_module nnqr Conv d is_reference nnq Conv d None True Linear linear_weight linear_input None is_reference ns call_function torch ops quantized linear_dynamic ns call_function torch ops quantized linear_prepack False Linear linear_weight linear_input ns call_function torch nn functional linear is_reference torch ops quantized linear ns call_function torch ops quantized linear_prepack True LinearModule linear_module_input ns call_module nnqr Linear is_reference ns call_module nnqd Linear None False LinearModule linear_module_input ns call_module nnqr Linear is_reference nnq Linear None tests skipIfNoFBGEMM test_conv_linear_not_reference Test quantizing conv linear tests = _get_conv_linear_test_cases is_reference=False is_dynamic ModuleClass module_constructor_inputs inputs quantized_node weight_prepack_node tests quant_type = QuantType DYNAMIC is_dynamic QuantType STATIC node_occurrence = weight_prepack_node node_occurrence weight_prepack_node = checkGraphModeFxOp ModuleClass module_constructor_inputs inputs quant_type expected_node=quantized_node expected_node_occurrence=node_occurrence is_reference=False skipIfNoFBGEMM test_conv_linear_reference Test quantizing functional conv linear reference option tests = _get_conv_linear_test_cases is_reference=True _get_keys prefix is_dynamic all_keys = prefix + + k k weight_qscheme weight_dtype is_dynamic all_keys extend prefix + + k k weight_scale weight_zero_point all_keys is_dynamic ModuleClass module_constructor_inputs inputs quantized_node weight_prepack_node tests quant_type = QuantType DYNAMIC is_dynamic QuantType STATIC node_occurrence = weight_prepack_node node_occurrence weight_prepack_node = result_dict = checkGraphModeFxOp ModuleClass module_constructor_inputs inputs quant_type expected_node=quantized_node expected_node_occurrence=node_occurrence is_reference=True qr = result_dict quantized_reference checkWeightQParams model module_name linear conv hasattr model module_name assertTrue hasattr qr get_submodule module_name weight_qscheme assertTrue hasattr qr get_submodule module_name weight_scale assertTrue hasattr qr get_submodule module_name weight_zero_point assertTrue Reference qr get_submodule module_name _get_name checkSerDeser model is_dynamic module_name linear conv hasattr model module_name make sure serialization works state_dict = copy deepcopy model state_dict all_keys = _get_keys module_name is_dynamic key all_keys assertTrue key state_dict check load_state_dict restores states module = getattr model module_name prev_scale = module weight_scale module weight_scale = None model load_state_dict state_dict module = getattr model module_name assertTrue torch equal prev_scale module weight_scale checkWeightQParams qr qr = copy deepcopy qr make sure qparams preserved after copy checkWeightQParams qr checkSerDeser qr is_dynamic _get_conv_transpose_test_cases use_relu is_reference Returns list test cases format is_dynamic ModuleClass module_constructor_inputs inputs quantized_node weight_prepack_op FunctionalConvTranspose d torch nn Module __init__ weight super __init__ weight = torch nn Parameter weight stride = padding = output_padding = dilation = groups = forward x y = F conv_transpose d x weight None stride padding output_padding groups dilation use_relu y = F relu y y ConvTranspose d torch nn Module __init__ args super __init__ deconv = torch nn ConvTranspose d args relu = torch nn ReLU forward x y = deconv x use_relu y = relu y y conv_transpose d_input = torch rand conv_transpose d_weight = torch rand conv_transpose d_module_args = FunctionalConvTranspose d torch nn Module __init__ weight super __init__ weight = torch nn Parameter weight stride = padding = output_padding = dilation = groups = forward x y = F conv_transpose d x weight None stride padding output_padding groups dilation use_relu y = F relu y y ConvTranspose d torch nn Module __init__ args super __init__ deconv = torch nn ConvTranspose d args relu = torch nn ReLU forward x y = deconv x use_relu y = relu y y conv_transpose d_input = torch rand conv_transpose d_weight = torch rand conv_transpose d_module_args = FunctionalConvTranspose d torch nn Module __init__ weight super __init__ weight = torch nn Parameter weight stride = padding = output_padding = dilation = groups = forward x y = F conv_transpose d x weight None stride padding output_padding groups dilation use_relu y = F relu y y ConvTranspose d torch nn Module __init__ args super __init__ deconv = torch nn ConvTranspose d args relu = torch nn ReLU forward x y = deconv x use_relu y = relu y y conv_transpose d_input = torch rand conv_transpose d_weight = torch rand conv_transpose d_module_args = is_dynamic ModuleClass module_constructor_inputs inputs quantized_node weight_prepack_node tests = False FunctionalConvTranspose d conv_transpose d_weight conv_transpose d_input ns call_function torch nn functional conv_transpose d is_reference torch ops quantized conv_transpose d ns call_function torch ops quantized conv_transpose d_prepack False FunctionalConvTranspose d conv_transpose d_weight conv_transpose d_input ns call_function torch nn functional conv_transpose d is_reference torch ops quantized conv_transpose d ns call_function torch ops quantized conv_transpose d_prepack False FunctionalConvTranspose d conv_transpose d_weight conv_transpose d_input ns call_function torch nn functional conv_transpose d is_reference torch ops quantized conv_transpose d ns call_function torch ops quantized conv_transpose d_prepack False ConvTranspose d conv_transpose d_module_args conv_transpose d_input ns call_module nnqr ConvTranspose d is_reference nnq ConvTranspose d None False ConvTranspose d conv_transpose d_module_args conv_transpose d_input ns call_module nnqr ConvTranspose d is_reference nnq ConvTranspose d None False ConvTranspose d conv_transpose d_module_args conv_transpose d_input ns call_module nnqr ConvTranspose d is_reference nnq ConvTranspose d None tests skipIfNoFBGEMM test_conv_transpose_not_reference Test quantizing transposed conv tests = _get_conv_transpose_test_cases use_relu=False is_reference=False is_dynamic ModuleClass module_constructor_inputs inputs quantized_node weight_prepack_node tests quant_type = QuantType DYNAMIC is_dynamic QuantType STATIC node_occurrence = weight_prepack_node node_occurrence weight_prepack_node = checkGraphModeFxOp ModuleClass module_constructor_inputs inputs quant_type expected_node=quantized_node expected_node_occurrence=node_occurrence is_reference=False skipIfNoFBGEMM test_conv_transpose_reference Test quantizing transposed conv reference option tests = _get_conv_transpose_test_cases use_relu=False is_reference=True _get_keys prefix is_dynamic all_keys = prefix + + k k weight_qscheme weight_dtype is_dynamic all_keys extend prefix + + k k weight_scale weight_zero_point all_keys is_dynamic ModuleClass module_constructor_inputs inputs quantized_node weight_prepack_node tests quant_type = QuantType DYNAMIC is_dynamic QuantType STATIC node_occurrence = weight_prepack_node node_occurrence weight_prepack_node = result_dict = checkGraphModeFxOp ModuleClass module_constructor_inputs inputs quant_type expected_node=quantized_node expected_node_occurrence=node_occurrence is_reference=True qr = result_dict quantized_reference checkWeightQParams model module_name = deconv hasattr model module_name assertTrue hasattr qr get_submodule module_name weight_qscheme assertTrue hasattr qr get_submodule module_name weight_scale assertTrue hasattr qr get_submodule module_name weight_zero_point assertTrue Reference qr get_submodule module_name _get_name checkSerDeser model is_dynamic module_name = deconv hasattr model module_name make sure serialization works state_dict = copy deepcopy model state_dict all_keys = _get_keys module_name is_dynamic key all_keys assertTrue key state_dict check load_state_dict restores states module = getattr model module_name prev_scale = module weight_scale module weight_scale = None model load_state_dict state_dict module = getattr model module_name assertTrue torch equal prev_scale module weight_scale checkWeightQParams qr qr = copy deepcopy qr make sure qparams preserved after copy checkWeightQParams qr checkSerDeser qr is_dynamic test_conv_transpose_relu_not_reference Test quantizing transposed conv + relu Fusion relu supported tests = _get_conv_transpose_test_cases use_relu=True is_reference=False is_dynamic ModuleClass module_constructor_inputs inputs quantized_node weight_prepack_node tests quant_type = QuantType DYNAMIC is_dynamic QuantType STATIC node_occurrence = weight_prepack_node node_occurrence weight_prepack_node = quantized_node op == call_module node_occurrence ns call_module nn ReLU = node_occurrence ns call_function F relu = checkGraphModeFxOp ModuleClass module_constructor_inputs inputs quant_type expected_node=quantized_node expected_node_occurrence=node_occurrence is_reference=False skipIfNoFBGEMM test_conv_transpose_relu_reference Test quantizing transposed conv reference option Fusion relu supported tests = _get_conv_transpose_test_cases use_relu=True is_reference=True _get_keys prefix is_dynamic all_keys = prefix + + k k weight_qscheme weight_dtype is_dynamic all_keys extend prefix + + k k weight_scale weight_zero_point all_keys is_dynamic ModuleClass module_constructor_inputs inputs quantized_node weight_prepack_node tests quant_type = QuantType DYNAMIC is_dynamic QuantType STATIC node_occurrence = weight_prepack_node node_occurrence weight_prepack_node = quantized_node op == call_module node_occurrence ns call_module nn ReLU = node_occurrence ns call_function F relu = result_dict = checkGraphModeFxOp ModuleClass module_constructor_inputs inputs quant_type expected_node=quantized_node expected_node_occurrence=node_occurrence is_reference=True qr = result_dict quantized_reference checkWeightQParams model module_name = deconv hasattr model module_name assertTrue hasattr qr get_submodule module_name weight_qscheme assertTrue hasattr qr get_submodule module_name weight_scale assertTrue hasattr qr get_submodule module_name weight_zero_point assertTrue Reference qr get_submodule module_name _get_name checkSerDeser model is_dynamic module_name = deconv hasattr model module_name make sure serialization works state_dict = copy deepcopy model state_dict all_keys = _get_keys module_name is_dynamic key all_keys assertTrue key state_dict check load_state_dict restores states module = getattr model module_name prev_scale = module weight_scale module weight_scale = None model load_state_dict state_dict module = getattr model module_name assertTrue torch equal prev_scale module weight_scale checkWeightQParams qr qr = copy deepcopy qr make sure qparams preserved after copy checkWeightQParams qr checkSerDeser qr is_dynamic skipIfNoFBGEMM test_dynamic_quant_weight_observer Test weight observer run convert step M torch nn Module __init__ weight super __init__ weight = torch nn Parameter weight forward x F linear x weight m = M torch rand eval qconfig = default_dynamic_qconfig qconfig_dict = qconfig example_inputs = torch rand prepared = prepare_fx m qconfig_dict example_inputs=example_inputs quantized = convert_to_reference_fx prepared qparams = quantized _scale_ quantized _zero_point_ weight_obs = qconfig weight weight_obs quantized weight Get actual value avoid tensor size mismatch error torch Size vs torch Size ref_qparams = weight_obs calculate_qparams item weight_obs calculate_qparams item assertEqual qparams ref_qparams test_conv_bn_relu Tests fusion quantization Conv - Bn Conv - Bn - ReLU convs = nn Conv d nn Conv d nn Conv d bns = nn BatchNorm d nn BatchNorm d nn BatchNorm d quantized_convs = nnq Conv d nnq Conv d nnq Conv d quantized_conv_relus = nniq ConvReLU d nniq ConvReLU d nniq ConvReLU d M torch nn Module __init__ dim has_relu super __init__ conv = convs dim bn = bns dim relu = nn ReLU has_relu nn Identity has_relu = has_relu quant = QuantStub dequant = DeQuantStub forward x x = quant x x = conv x x = bn x has_relu x = relu x x = dequant x x options = itertools product True False static_quant_types dim has_relu quant_type options expected_node = ns call_module quantized_conv_relus dim has_relu quantized_convs dim m = M dim has_relu m_eager = copy deepcopy m result_dict = checkGraphModeFxOp m img_data_dict dim quant_type expected_node=expected_node result = result_dict quantized_output check numerics qengine = torch backends quantized engine quant_type == QuantType STATIC m_eager eval qconfig = get_default_qconfig qengine prepare_fn = prepare is_qat = False m_eager train qconfig = get_default_qat_qconfig qengine prepare_fn = prepare_qat is_qat = True fuse_list = conv bn has_relu fuse_list append relu is_qat fuse_modules_qat m_eager fuse_list inplace=True fuse_modules m_eager fuse_list inplace=True m_eager qconfig = qconfig m_eager = prepare_fn m_eager prepared_fx = result_dict prepared m_eager img_data_dict dim m_eager = convert m_eager result_eager = m_eager img_data_dict dim assertEqual result result_eager test_linear_bn M torch nn Module __init__ - None super __init__ linear = nn Linear bn = nn BatchNorm d quant = QuantStub dequant = DeQuantStub forward x x = quant x x = linear x x = bn x x = dequant x x data = torch randn quant_type static_quant_types expected_node = ns call_module nnq Linear m = M m_eager = copy deepcopy m result_dict = checkGraphModeFxOp m data quant_type expected_node=expected_node result = result_dict quantized_output check numerics vs eager mode fuse_list = linear bn qengine = torch backends quantized engine quant_type == QuantType STATIC m_eager eval qconfig = get_default_qconfig qengine prepare_fn = prepare fuse_modules m_eager fuse_list inplace=True m_eager train qconfig = get_default_qat_qconfig qengine prepare_fn = prepare_qat fuse_modules_qat m_eager fuse_list inplace=True m_eager qconfig = qconfig m_eager = prepare_fn m_eager m_eager data m_eager = convert m_eager result_eager = m_eager data assertEqual result result_eager skipIfNoFBGEMM test_dynamic_quant_fp override_quantized_engine fbgemm Linear torch nn Module __init__ weight super __init__ weight = torch nn Parameter weight forward x F linear x weight linear_input = torch rand linear_weight = torch rand LinearModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x linear_module_input = torch rand tests = Linear linear_weight linear_input ns call_function torch ops quantized linear_dynamic_fp ns call_function torch ops quantized linear_prepack_fp LinearModule linear_module_input ns call_module nnqd Linear None ModuleClass module_constructor_inputs inputs quantized_node weight_prepack_node tests is_reference True False node_occurrence = weight_prepack_node node_occurrence weight_prepack_node = m = ModuleClass module_constructor_inputs eval qconfig_dict = float _dynamic_qconfig m = prepare_fx m qconfig_dict example_inputs=inputs convert_fn = convert_to_reference_fx is_reference convert_fx m = convert_fn m checkGraphModuleNodes m expected_node_occurrence=node_occurrence unittest skipIf TEST_MULTIGPU multi-GPU supported unittest skipIf TEST_CUDA CUDA unavailable override_qengines test_qat_prepare_device_affinity Tests FX QAT prepare pass respects device affinity Model nn Module __init__ - None super __init__ conv = nn Conv d bn = nn BatchNorm d relu = nn ReLU forward x x = conv x x = bn x x = relu x x model = Model qengine = torch backends quantized engine qconfig_dict = torch ao quantization get_default_qat_qconfig qengine device = torch device cuda model device example_inputs = torch randn device=device QAT prepare model = prepare_qat_fx model qconfig_dict example_inputs=example_inputs ensure running input CUDA works without any needed changes model example_inputs ensure all buffers parameters device we expect model_devices = p device p model parameters &#124; \ p device p model buffers assertEqual len model_devices model_device = next iter model_devices assertEqual model_device device skipIfNoFBGEMM test_dict_output Make sure quantization runs models dictionary output M torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x output conv x input example_inputs = input torch randn m = M eval qconfig_dict = default_qconfig m = prepare_fx m qconfig_dict example_inputs=example_inputs m example_inputs m = convert_fx m m example_inputs override_qengines test_attention Make sure quantization runs corner case attention module M torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x x = conv x q k v = x chunk dim= q = q contiguous view - transpose k = k contiguous view - transpose v = v contiguous view - transpose torch _assert k size == key size should equal r = torch mm k v q k + r example_inputs = torch randn m = M eval qconfig_dict = None object_type nn Conv d default_qconfig make sure runs m = prepare_fx m qconfig_dict example_inputs=example_inputs m example_inputs m = convert_fx m m example_inputs _test_standalone_module interface_config prepare_count_check standalone_prepare_count_check convert_count_check standalone_convert_count_check Test standalone module different quantized input quantized output configurations StandaloneModule torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x conv x M torch nn Module __init__ - None super __init__ conv = torch nn Conv d standalone = StandaloneModule forward x x = conv x x = standalone x x RefM torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv = torch nn Conv d forward x x = conv x x = conv x x example_inputs = torch randn instantiate M RefM align parameters original_m = M eval original_ref_m = RefM eval original_ref_m conv weight = torch nn Parameter original_m conv weight detach original_ref_m conv bias = torch nn Parameter original_m conv bias detach original_ref_m conv weight = torch nn Parameter original_m standalone conv weight detach original_ref_m conv bias = torch nn Parameter original_m standalone conv bias detach is_name True False sm_example_inputs = example_inputs is_name prepare_config = standalone_module_name standalone None sm_example_inputs interface_config None prepare_config = standalone_module_class StandaloneModule None sm_example_inputs interface_config None original_m_copy = copy deepcopy original_m original_ref_m_copy = copy deepcopy original_ref_m qconfig_dict = default_qconfig check prepared model m = prepare_fx original_m_copy qconfig_dict example_inputs=example_inputs prepare_custom_config=prepare_config calibration m example_inputs checkGraphModuleNodes m expected_node_occurrence=prepare_count_check checkGraphModuleNodes m standalone expected_node_occurrence=standalone_prepare_count_check check converted quantized model m = convert_fx m checkGraphModuleNodes m expected_node_occurrence=convert_count_check checkGraphModuleNodes m standalone expected_node_occurrence=standalone_convert_count_check res = m example_inputs quantize reference model ref_m = prepare_fx original_ref_m_copy qconfig_dict example_inputs=example_inputs ref_m example_inputs ref_m = convert_fx ref_m ref_res = ref_m example_inputs assertEqual res ref_res test_standalone_module_float_interface float_interface_config = input_quantized_idxs float input output_quantized_idxs float output interface_config = float_interface_config input output first conv observer standalone module will inserted standalone module itself prepare_count_check = ns call_module torch ao quantization MinMaxObserver input output conv standalone module standalone_prepare_count_check = ns call_module torch ao quantization MinMaxObserver convert_count_check = ns call_function torch quantize_per_tensor ns call_module nnq Conv d ns call_method dequantize standalone_convert_count_check = standalone module will take float input output so we ll see quantize dequantize modoule ns call_function torch quantize_per_tensor ns call_module nnq Conv d ns call_method dequantize _test_standalone_module interface_config prepare_count_check standalone_prepare_count_check convert_count_check standalone_convert_count_check test_standalone_module_quantized_interface quantized_interface_config = input_quantized_idxs quantized input output_quantized_idxs quantized output interface_config = quantized_interface_config observer input output first conv prepare_count_check = ns call_module torch ao quantization MinMaxObserver output conv standalone module standalone_prepare_count_check = ns call_module torch ao quantization MinMaxObserver convert_count_check = quantizing input conv ns call_function torch quantize_per_tensor ns call_module nnq Conv d dequantizing output standalone module ns call_method dequantize standalone_convert_count_check = quantization input happens parent module quantization output happens quantized conv module ns call_function torch quantize_per_tensor ns call_module nnq Conv d dequantization output happens parent module ns call_method dequantize _test_standalone_module interface_config prepare_count_check standalone_prepare_count_check convert_count_check standalone_convert_count_check skipIfNoFBGEMM test_qconfig_none M torch nn Module __init__ - None super __init__ conv = nn Conv d conv = nn Conv d forward x x = conv x x = conv x x m = M eval qconfig_dict = default_qconfig module_name conv None example_inputs = torch randn m = prepare_fx m qconfig_dict example_inputs=example_inputs m example_inputs m = convert_fx m m example_inputs first conv quantized second conv quantized node_list = ns call_function torch quantize_per_tensor ns call_module nnq Conv d ns call_method dequantize ns call_module nn Conv d checkGraphModuleNodes m expected_node_list=node_list test_qconfig_module_type M torch nn Module __init__ - None super __init__ conv = nn Conv d linear = nn Linear forward x x = conv x x = x reshape - x = linear x x m = M eval qconfig_dict = object_type torch nn Conv d default_qconfig example_inputs = torch randn m = prepare_fx m qconfig_dict example_inputs=example_inputs m example_inputs m = convert_fx m m example_inputs conv quantized linear quantized node_list = ns call_function torch quantize_per_tensor ns call_module nnq Conv d ns call_method dequantize ns call_module nn Linear checkGraphModuleNodes m expected_node_list=node_list test_qconfig_qat_module_type LinearRelu nn Sequential __init__ - None super __init__ nn Linear nn ReLU M torch nn Module __init__ - None super __init__ lin_relu = LinearRelu linear = nn Linear forward x x = lin_relu x x = linear x x model = M train qconfig_dict = None object_type torch nn Linear default_qat_qconfig torch nn ReLU default_qat_qconfig example_inputs = torch rand m = prepare_qat_fx model qconfig_dict example_inputs=example_inputs m example_inputs m = convert_fx m m example_inputs node_list = ns call_function torch quantize_per_tensor ns call_module nniq LinearReLU ns call_module nnq Linear ns call_method dequantize checkGraphModuleNodes m expected_node_list=node_list test_qconfig_function M torch nn Module forward x y x + y m = M eval qconfig_dict = object_type operator add default_qconfig data = torch randn example_inputs = data data m = prepare_fx m qconfig_dict example_inputs m example_inputs m = convert_fx m m example_inputs first conv quantized second conv quantized node_list = ns call_function torch quantize_per_tensor ns call_function torch ops quantized add ns call_method dequantize checkGraphModuleNodes m expected_node_list=node_list test_qconfig_module_name_regex M torch nn Module __init__ - None super __init__ conv = nn Conv d conv = nn Conv d forward x x = conv x x = conv x x m = M eval qconfig_dict = module_name_regex conv default_qconfig example_inputs = torch randn m = prepare_fx m qconfig_dict example_inputs=example_inputs m example_inputs m = convert_fx m m example_inputs first conv quantized second conv quantized node_list = ns call_function torch quantize_per_tensor ns call_module nnq Conv d ns call_module nnq Conv d ns call_method dequantize checkGraphModuleNodes m expected_node_list=node_list test_qconfig_precedence device get_supported_device_types M torch nn Module __init__ - None super __init__ linear = nn Linear conv = nn Conv d module_conv = nn Conv d module_conv = nn Conv d forward x global x = linear x global + object_type -- object_type x = conv x global + object_type + module_name_regex -- module_name_regex x = module_conv x global + object_type + module_name_regex + module_name -- module_name x = module_conv x x m = M device eval global_qconfig = default_qconfig object_type_qconfig = default_dynamic_qconfig module_name_regex_qconfig = float _dynamic_qconfig module_name_qconfig = default_qat_qconfig qconfig_dict = global_qconfig object_type nn Conv d object_type_qconfig module_name_regex module_conv module_name_regex_qconfig module_name module_conv module_name_qconfig m_prep = prepare_fx m qconfig_dict example_inputs= torch randn assertEqual m_prep linear qconfig activation p func global_qconfig activation p func assertEqual m_prep linear qconfig weight p func global_qconfig weight p func assertEqual m_prep conv qconfig activation p func object_type_qconfig activation p func assertEqual m_prep conv qconfig weight p func object_type_qconfig weight p func assertEqual m_prep module_conv qconfig activation p func module_name_regex_qconfig activation p func assertEqual m_prep module_conv qconfig weight p func module_name_regex_qconfig weight p func assertEqual m_prep module_conv qconfig activation p func module_name_qconfig activation p func assertEqual m_prep module_conv qconfig weight p func module_name_qconfig weight p func test_qconfig_module_name_object_type_order M torch nn Module __init__ - None super __init__ fc = nn Linear fc = nn Linear forward x x = fc x x = fc x x = torch add x x x = torch add x x x M torch nn Module __init__ - None super __init__ fc = nn Linear fc = nn Linear m = M forward x x = fc x x = fc x x = torch add x x x = torch add x x x = m x x M torch nn Module __init__ - None super __init__ fc = nn Linear fc = nn Linear m = M forward x x = fc x x = fc x x = torch add x x x = torch add x x x = m x x m = M eval qconfig_dict = module_name_object_type_order test various FQNs global single child multiple children nn Linear torch ao quantization default_qconfig torch add torch ao quantization default_qconfig m nn Linear torch ao quantization default_qconfig m torch add torch ao quantization default_qconfig m m nn Linear torch ao quantization default_qconfig m m torch add torch ao quantization default_qconfig example_inputs = torch randn m = prepare_fx m qconfig_dict example_inputs=example_inputs m example_inputs m = convert_fx m m example_inputs node_list = m ns call_function torch quantize_per_tensor ns call_module nnq Linear ns call_method dequantize ns call_module nn Linear ns call_function torch quantize_per_tensor ns call_function torch ops quantized add ns call_method dequantize ns call_function torch add m ns call_module nn Linear ns call_function torch quantize_per_tensor ns call_module nnq Linear ns call_method dequantize ns call_function torch add ns call_function torch quantize_per_tensor ns call_function torch ops quantized add m ns call_module nnq Linear ns call_method dequantize ns call_module nn Linear ns call_function torch quantize_per_tensor ns call_function torch ops quantized add ns call_method dequantize ns call_function torch add checkGraphModuleNodes m expected_node_list=node_list test function order overrides global qconfig M torch nn Module __init__ - None super __init__ fc = nn Linear fc = nn Linear forward x x = fc x x = fc x x = torch add x x x = torch add x x x m = M eval qconfig_dict = torch ao quantization default_qconfig module_name_object_type_order nn Linear None torch add None example_inputs = torch randn m = prepare_fx m qconfig_dict example_inputs=example_inputs m example_inputs m = convert_fx m m example_inputs node_list = ns call_function torch quantize_per_tensor ns call_module nnq Linear ns call_method dequantize ns call_module nn Linear ns call_function torch quantize_per_tensor ns call_function torch ops quantized add ns call_method dequantize ns call_function torch add checkGraphModuleNodes m expected_node_list=node_list override_qengines test_qconfig_dict_with_fused_modules LinearReLUModel torch nn Module __init__ relu super __init__ linear = torch nn Linear relu = relu forward x x = linear x x = relu x x ConvReLUModel torch nn Module __init__ relu super __init__ conv = torch nn Conv d relu = relu forward x x = conv x x = relu x x ConvBnReLUModel torch nn Module __init__ relu super __init__ conv = torch nn Conv d bn = torch nn BatchNorm d relu = relu forward x x = conv x x = bn x x = relu x x model LinearReLUModel ConvReLUModel ConvBnReLUModel relu torch nn ReLU torch nn functional relu torch relu m = model relu eval qengine = torch backends quantized engine qconfig_dict = torch ao quantization get_default_qconfig_mapping qengine should crash https github com pytorch pytorch issues prepare_fx m qconfig_dict example_inputs= torch randn TODO move QConfigMapping tests test quantization core test_qconfig_mapping_set_global qconfig = get_default_qconfig qconfig_mapping = QConfigMapping assertEqual qconfig_mapping global_qconfig None qconfig_mapping set_global qconfig assertEqual qconfig_mapping global_qconfig qconfig test_qconfig_mapping_set_object_type qconfig = get_default_qconfig qconfig = get_default_qconfig qconfig = get_default_qconfig assertNotEqual qconfig qconfig assertNotEqual qconfig qconfig qconfig_mapping = QConfigMapping assertEqual len qconfig_mapping object_type_qconfigs Insert some entries qconfig_mapping set_object_type torch nn Linear qconfig qconfig_mapping set_object_type torch nn ReLU qconfig assertEqual len qconfig_mapping object_type_qconfigs assertEqual qconfig_mapping object_type_qconfigs torch nn Linear qconfig assertEqual qconfig_mapping object_type_qconfigs torch nn ReLU qconfig Override existing key qconfig_mapping set_object_type torch nn Linear qconfig assertEqual qconfig_mapping object_type_qconfigs torch nn Linear qconfig assertEqual qconfig_mapping object_type_qconfigs torch nn ReLU qconfig assertEqual _get_object_type_qconfig qconfig_mapping torch nn Linear None qconfig assertEqual _get_object_type_qconfig qconfig_mapping torch nn ReLU None qconfig assertEqual _get_object_type_qconfig qconfig_mapping nomatch None None test_qconfig_mapping_set_module_name_regex qconfig = get_default_qconfig qconfig = get_default_qconfig qconfig = get_default_qconfig assertNotEqual qconfig qconfig assertNotEqual qconfig qconfig qconfig_mapping = QConfigMapping assertEqual len qconfig_mapping module_name_regex_qconfigs Insert some entries qconfig_mapping set_module_name_regex foo bar qconfig qconfig_mapping set_module_name_regex foo qconfig assertEqual len qconfig_mapping module_name_regex_qconfigs assertEqual qconfig_mapping module_name_regex_qconfigs foo bar qconfig assertEqual qconfig_mapping module_name_regex_qconfigs foo qconfig Override existing key qconfig_mapping set_module_name_regex foo bar qconfig assertEqual qconfig_mapping module_name_regex_qconfigs foo bar qconfig assertEqual qconfig_mapping module_name_regex_qconfigs foo qconfig assertEqual _get_module_name_regex_qconfig qconfig_mapping foo bar None qconfig assertEqual _get_module_name_regex_qconfig qconfig_mapping foobar None qconfig assertEqual _get_module_name_regex_qconfig qconfig_mapping foobaz None qconfig assertEqual _get_module_name_regex_qconfig qconfig_mapping foo None qconfig assertEqual _get_module_name_regex_qconfig qconfig_mapping nomatch None None test_qconfig_mapping_set_module_name qconfig = get_default_qconfig qconfig = get_default_qconfig qconfig = get_default_qconfig assertNotEqual qconfig qconfig assertNotEqual qconfig qconfig qconfig_mapping = QConfigMapping assertEqual len qconfig_mapping module_name_qconfigs Insert some entries qconfig_mapping set_module_name mod qconfig qconfig_mapping set_module_name mod qconfig assertEqual len qconfig_mapping module_name_qconfigs assertEqual qconfig_mapping module_name_qconfigs mod qconfig assertEqual qconfig_mapping module_name_qconfigs mod qconfig Override existing key qconfig_mapping set_module_name mod qconfig assertEqual qconfig_mapping module_name_qconfigs mod qconfig assertEqual qconfig_mapping module_name_qconfigs mod qconfig assertEqual _get_module_name_qconfig qconfig_mapping mod None qconfig assertEqual _get_module_name_qconfig qconfig_mapping mod None qconfig assertEqual _get_module_name_qconfig qconfig_mapping nomatch None None test_qconfig_mapping_set_module_name_object_type_order qconfig = get_default_qconfig qconfig = get_default_qconfig qconfig = get_default_qconfig assertNotEqual qconfig qconfig assertNotEqual qconfig qconfig qconfig_mapping = QConfigMapping assertEqual len qconfig_mapping module_name_object_type_order_qconfigs Insert some entries qconfig_mapping set_module_name_object_type_order mod torch nn Linear qconfig qconfig_mapping set_module_name_object_type_order mod torch nn ReLU qconfig assertEqual len qconfig_mapping module_name_object_type_order_qconfigs key = mod torch nn Linear key = mod torch nn ReLU assertEqual next iter qconfig_mapping module_name_object_type_order_qconfigs key assertEqual list qconfig_mapping module_name_object_type_order_qconfigs key assertEqual qconfig_mapping module_name_object_type_order_qconfigs key qconfig assertEqual qconfig_mapping module_name_object_type_order_qconfigs key qconfig assertEqual _maybe_adjust_qconfig_for_module_name_object_type_order qconfig_mapping mod torch nn Linear None qconfig assertEqual _maybe_adjust_qconfig_for_module_name_object_type_order qconfig_mapping mod torch nn ReLU None qconfig Override existing key qconfig_mapping set_module_name_object_type_order mod torch nn Linear qconfig assertEqual len qconfig_mapping module_name_object_type_order_qconfigs assertEqual next iter qconfig_mapping module_name_object_type_order_qconfigs key assertEqual list qconfig_mapping module_name_object_type_order_qconfigs key assertEqual qconfig_mapping module_name_object_type_order_qconfigs key qconfig assertEqual qconfig_mapping module_name_object_type_order_qconfigs key qconfig assertEqual _maybe_adjust_qconfig_for_module_name_object_type_order qconfig_mapping mod torch nn Linear None qconfig assertEqual _maybe_adjust_qconfig_for_module_name_object_type_order qconfig_mapping mod torch nn ReLU None qconfig No match assertEqual _maybe_adjust_qconfig_for_module_name_object_type_order qconfig_mapping mod torch nn Linear None None assertEqual _maybe_adjust_qconfig_for_module_name_object_type_order qconfig_mapping mod torch nn Linear None None assertEqual _maybe_adjust_qconfig_for_module_name_object_type_order qconfig_mapping mod torch nn Conv d None None _get_qconfig_dict_for_qconfig_mapping_test global_qconfig qconfig qconfig Return dummy qconfig_dict test QConfigMapping s to_dict from_dict methods _GLOBAL_DICT_KEY global_qconfig _OBJECT_TYPE_DICT_KEY torch nn Linear qconfig torch nn ReLU qconfig _MODULE_NAME_REGEX_DICT_KEY foo bar qconfig foo qconfig _MODULE_NAME_DICT_KEY bazbaz qconfig borbor qconfig _MODULE_NAME_OBJECT_TYPE_ORDER_DICT_KEY bazbaz torch nn Linear qconfig foofoo torch nn ReLU qconfig assertRaises ValueError context m = prepare_fx m qconfig_dict example_inputs= torch randn noqa F assertTrue Expected qconfig_dict have following keys str context exception assertTrue But found \ object_typo\ instead str context exception test_qconfig_mapping_from_dict global_qconfig = QConfig global qconfig = QConfig one qconfig = QConfig two qconfig_dict = _get_qconfig_dict_for_qconfig_mapping_test global_qconfig qconfig qconfig qconfig_dict undefined_dict_key = qconfig qconfig qconfig_mapping = QConfigMapping from_dict qconfig_dict assertEqual qconfig_mapping global_qconfig global_qconfig assertEqual qconfig_mapping object_type_qconfigs OrderedDict torch nn Linear qconfig torch nn ReLU qconfig assertEqual qconfig_mapping module_name_regex_qconfigs OrderedDict foo bar qconfig foo qconfig assertEqual qconfig_mapping module_name_qconfigs OrderedDict bazbaz qconfig borbor qconfig assertEqual qconfig_mapping module_name_object_type_order_qconfigs OrderedDict bazbaz torch nn Linear qconfig foofoo torch nn ReLU qconfig test_qconfig_mapping_to_dict global_qconfig = QConfig global qconfig = QConfig one qconfig = QConfig two qconfig_mapping = QConfigMapping set_global global_qconfig \ set_object_type torch nn Linear qconfig \ set_object_type torch nn ReLU qconfig \ set_module_name_regex foo bar qconfig \ set_module_name_regex foo qconfig \ set_module_name bazbaz qconfig \ set_module_name borbor qconfig \ set_module_name_object_type_order bazbaz torch nn Linear qconfig \ set_module_name_object_type_order foofoo torch nn ReLU qconfig qconfig_dict = _get_qconfig_dict_for_qconfig_mapping_test global_qconfig qconfig qconfig assertEqual qconfig_mapping to_dict qconfig_dict test_qconfig_mapping_repr assertTrue isinstance get_default_qconfig_mapping __repr__ str test_default_qconfig_mapping_override_global M torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x conv x m = M eval my_qconfig = QConfig activation=MinMaxObserver weight=default_weight_observer qconfig_mapping = get_default_qconfig_mapping Override global qconfig old_global_qconfig = qconfig_mapping global_qconfig qconfig_mapping set_global my_qconfig Verify correct qconfig used example_inputs = torch randn m = prepare_fx m qconfig_mapping example_inputs assertTrue isinstance old_global_qconfig activation HistogramObserver assertTrue isinstance my_qconfig activation MinMaxObserver assertTrue hasattr m activation_post_process_ assertTrue hasattr m activation_post_process_ assertTrue isinstance m activation_post_process_ MinMaxObserver assertTrue isinstance m activation_post_process_ MinMaxObserver Dummy classes PrepareCustomConfig testing _DummyStandaloneModule pass _DummyFloatModule pass _DummyObservedModule pass _DummyQuantizedModule pass _DummyNonTraceableModule pass _DummyNonTraceableModule pass test_prepare_custom_config_set_standalone_module_name qconfig_mapping = QConfigMapping example_inputs = torch randn child_prepare_custom_config = PrepareCustomConfig backend_config = BackendConfig my_backend config_entry = StandaloneModuleConfigEntry qconfig_mapping example_inputs child_prepare_custom_config backend_config prepare_custom_config = PrepareCustomConfig assertEqual len prepare_custom_config standalone_module_names prepare_custom_config set_standalone_module_name module qconfig_mapping example_inputs child_prepare_custom_config backend_config assertEqual list prepare_custom_config standalone_module_names keys module assertEqual prepare_custom_config standalone_module_names module config_entry test_prepare_custom_config_set_standalone_module_class qconfig_mapping = QConfigMapping example_inputs = torch randn child_prepare_custom_config = PrepareCustomConfig backend_config = BackendConfig my_backend config_entry = StandaloneModuleConfigEntry qconfig_mapping example_inputs child_prepare_custom_config backend_config prepare_custom_config = PrepareCustomConfig assertEqual len prepare_custom_config standalone_module_classes prepare_custom_config set_standalone_module_class _DummyStandaloneModule qconfig_mapping example_inputs child_prepare_custom_config backend_config assertEqual len prepare_custom_config standalone_module_classes assertTrue _DummyStandaloneModule prepare_custom_config standalone_module_classes assertEqual prepare_custom_config standalone_module_classes _DummyStandaloneModule config_entry test_prepare_custom_config_set_float_to_observed_mapping prepare_custom_config = PrepareCustomConfig assertEqual len prepare_custom_config float_to_observed_mapping prepare_custom_config set_float_to_observed_mapping _DummyFloatModule _DummyObservedModule QuantType STATIC assertEqual len prepare_custom_config float_to_observed_mapping assertEqual list prepare_custom_config float_to_observed_mapping keys QuantType STATIC assertEqual len prepare_custom_config float_to_observed_mapping QuantType STATIC assertTrue _DummyFloatModule prepare_custom_config float_to_observed_mapping QuantType STATIC assertEqual prepare_custom_config float_to_observed_mapping QuantType STATIC _DummyFloatModule _DummyObservedModule test_prepare_custom_config_set_non_traceable_module_names prepare_custom_config = PrepareCustomConfig assertEqual len prepare_custom_config non_traceable_module_names prepare_custom_config set_non_traceable_module_names module module assertEqual prepare_custom_config non_traceable_module_names module module test_prepare_custom_config_set_non_traceable_module_classes prepare_custom_config = PrepareCustomConfig assertEqual len prepare_custom_config non_traceable_module_classes prepare_custom_config set_non_traceable_module_classes _DummyNonTraceableModule _DummyNonTraceableModule assertEqual prepare_custom_config non_traceable_module_classes _DummyNonTraceableModule _DummyNonTraceableModule test_prepare_custom_config_set_input_quantized_indexes prepare_custom_config = PrepareCustomConfig assertEqual len prepare_custom_config input_quantized_indexes prepare_custom_config set_input_quantized_indexes assertEqual prepare_custom_config input_quantized_indexes test_prepare_custom_config_set_output_quantized_indexes prepare_custom_config = PrepareCustomConfig assertEqual len prepare_custom_config output_quantized_indexes prepare_custom_config set_output_quantized_indexes assertEqual prepare_custom_config output_quantized_indexes test_prepare_custom_config_set_preserved_attributes prepare_custom_config = PrepareCustomConfig assertEqual len prepare_custom_config preserved_attributes prepare_custom_config set_preserved_attributes attr attr assertEqual prepare_custom_config preserved_attributes attr attr _get_dummy_prepare_custom_config_dict Return dummy prepare_custom_config_dict test PrepareCustomConfig s to_dict from_dict methods STANDALONE_MODULE_NAME_DICT_KEY module QConfigMapping torch randn PrepareCustomConfig BackendConfig my_backend STANDALONE_MODULE_CLASS_DICT_KEY _DummyStandaloneModule QConfigMapping torch randn PrepareCustomConfig BackendConfig my_backend FLOAT_TO_OBSERVED_DICT_KEY static _DummyFloatModule _DummyObservedModule NON_TRACEABLE_MODULE_NAME_DICT_KEY module module NON_TRACEABLE_MODULE_CLASS_DICT_KEY _DummyNonTraceableModule _DummyNonTraceableModule INPUT_QUANTIZED_INDEXES_DICT_KEY OUTPUT_QUANTIZED_INDEXES_DICT_KEY PRESERVED_ATTRIBUTES_DICT_KEY attr attr test_prepare_custom_config_from_dict prepare_custom_config_dict = _get_dummy_prepare_custom_config_dict sm_name qm ei pcc bcd = prepare_custom_config_dict STANDALONE_MODULE_NAME_DICT_KEY sm_class qm ei pcc bcd = prepare_custom_config_dict STANDALONE_MODULE_CLASS_DICT_KEY sm_config_entry = StandaloneModuleConfigEntry qm ei pcc bcd sm_config_entry = StandaloneModuleConfigEntry qm ei pcc bcd prepare_custom_config = PrepareCustomConfig from_dict prepare_custom_config_dict Standalone modules assertEqual len prepare_custom_config standalone_module_names assertTrue sm_name prepare_custom_config standalone_module_names assertEqual prepare_custom_config standalone_module_names sm_name sm_config_entry assertEqual len prepare_custom_config standalone_module_classes assertTrue sm_class prepare_custom_config standalone_module_classes assertEqual prepare_custom_config standalone_module_classes sm_class sm_config_entry Float observed mapping assertEqual len prepare_custom_config float_to_observed_mapping assertEqual list prepare_custom_config float_to_observed_mapping keys QuantType STATIC assertEqual len prepare_custom_config float_to_observed_mapping QuantType STATIC assertTrue _DummyFloatModule prepare_custom_config float_to_observed_mapping QuantType STATIC assertEqual prepare_custom_config float_to_observed_mapping QuantType STATIC _DummyFloatModule _DummyObservedModule Other assertEqual prepare_custom_config non_traceable_module_names module module assertEqual prepare_custom_config non_traceable_module_classes _DummyNonTraceableModule _DummyNonTraceableModule assertEqual prepare_custom_config input_quantized_indexes assertEqual prepare_custom_config output_quantized_indexes assertEqual prepare_custom_config preserved_attributes attr attr test_prepare_custom_config_to_dict prepare_custom_config_dict = _get_dummy_prepare_custom_config_dict sm_name qm ei pcc bcd = prepare_custom_config_dict STANDALONE_MODULE_NAME_DICT_KEY sm_class qm ei pcc bcd = prepare_custom_config_dict STANDALONE_MODULE_CLASS_DICT_KEY prepare_custom_config = PrepareCustomConfig \ set_standalone_module_name sm_name qm ei pcc bcd \ set_standalone_module_class sm_class qm ei pcc bcd \ set_float_to_observed_mapping _DummyFloatModule _DummyObservedModule \ set_non_traceable_module_names module module \ set_non_traceable_module_classes _DummyNonTraceableModule _DummyNonTraceableModule \ set_input_quantized_indexes \ set_output_quantized_indexes \ set_preserved_attributes attr attr PrepareCustomConfig to_dict also converts internal QConfigMappings PrepareCustomConfigs dicts prepare_custom_config_dict STANDALONE_MODULE_NAME_DICT_KEY = sm_name qm to_dict ei pcc to_dict bcd prepare_custom_config_dict STANDALONE_MODULE_CLASS_DICT_KEY = sm_class qm to_dict ei pcc to_dict bcd assertEqual prepare_custom_config to_dict prepare_custom_config_dict test_convert_custom_config_set_observed_to_quantized_mapping convert_custom_config = ConvertCustomConfig assertEqual len convert_custom_config observed_to_quantized_mapping convert_custom_config set_observed_to_quantized_mapping _DummyObservedModule _DummyQuantizedModule QuantType STATIC assertEqual len convert_custom_config observed_to_quantized_mapping assertEqual list convert_custom_config observed_to_quantized_mapping keys QuantType STATIC assertTrue _DummyObservedModule convert_custom_config observed_to_quantized_mapping QuantType STATIC assertEqual convert_custom_config observed_to_quantized_mapping QuantType STATIC _DummyObservedModule _DummyQuantizedModule test_convert_custom_config_set_preserved_attributes convert_custom_config = ConvertCustomConfig assertEqual len convert_custom_config preserved_attributes convert_custom_config set_preserved_attributes attr attr assertEqual convert_custom_config preserved_attributes attr attr _get_dummy_convert_custom_config_dict Return dummy convert_custom_config_dict test ConvertCustomConfig s to_dict from_dict methods OBSERVED_TO_QUANTIZED_DICT_KEY static _DummyObservedModule _DummyQuantizedModule PRESERVED_ATTRIBUTES_DICT_KEY attr attr test_convert_custom_config_from_dict convert_custom_config_dict = _get_dummy_convert_custom_config_dict convert_custom_config = ConvertCustomConfig from_dict convert_custom_config_dict assertEqual len convert_custom_config observed_to_quantized_mapping assertEqual list convert_custom_config observed_to_quantized_mapping keys QuantType STATIC assertEqual len convert_custom_config observed_to_quantized_mapping QuantType STATIC assertTrue _DummyObservedModule convert_custom_config observed_to_quantized_mapping QuantType STATIC assertEqual convert_custom_config observed_to_quantized_mapping QuantType STATIC _DummyObservedModule _DummyQuantizedModule assertEqual convert_custom_config preserved_attributes attr attr test_convert_custom_config_to_dict convert_custom_config = ConvertCustomConfig \ set_observed_to_quantized_mapping _DummyObservedModule _DummyQuantizedModule \ set_preserved_attributes attr attr assertEqual convert_custom_config to_dict _get_dummy_convert_custom_config_dict test_fuse_custom_config_set_preserved_attributes fuse_custom_config = FuseCustomConfig assertEqual len fuse_custom_config preserved_attributes fuse_custom_config set_preserved_attributes attr attr assertEqual fuse_custom_config preserved_attributes attr attr test_fuse_custom_config_from_dict fuse_custom_config_dict = PRESERVED_ATTRIBUTES_DICT_KEY attr attr fuse_custom_config = FuseCustomConfig from_dict fuse_custom_config_dict assertEqual fuse_custom_config preserved_attributes attr attr test_fuse_custom_config_to_dict fuse_custom_config_dict = PRESERVED_ATTRIBUTES_DICT_KEY attr attr fuse_custom_config = FuseCustomConfig set_preserved_attributes attr attr assertEqual fuse_custom_config to_dict fuse_custom_config_dict test_remove_qconfig M torch nn Module __init__ - None super __init__ avg_pool = torch nn AvgPool d forward x avg_pool x m = M eval qconfig_dict = default_qconfig example_inputs = torch randn m = prepare_fx m qconfig_dict example_inputs=example_inputs m example_inputs m = convert_fx m m example_inputs name module m named_modules assertFalse hasattr module qconfig qconfig removed + name test_return_none M torch nn Module forward x pass m = M eval qconfig_dict = torch ao quantization default_qconfig m = prepare_fx m qconfig_dict example_inputs= torch randn m = convert_fx m test_default_quant_after_none_qconfig Make sure default quant inserted properly M torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv = torch nn Conv d forward x x = conv x x = x transpose x = conv x m = M eval qconfig_dict = default_qconfig module_name conv None m = prepare_fx m qconfig_dict example_inputs= torch randn m = convert_fx m test_qconfig_for_call_method Sub torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x x = x transpose x = conv x x transpose M torch nn Module __init__ - None super __init__ sub = Sub conv = torch nn Conv d conv = torch nn Conv d forward x x = conv x x = sub x x = conv x x transpose qconfig_dict = default_qconfig module_name sub None since sub configured have qconfig None we should dequantize output conv quantize input conv dequantize after conv should happen after transpose since configured default_qconfig nodes Sub module instance quantized node_list = ns call_function torch quantize_per_tensor ns call_module nnq Conv d ns call_method dequantize ns call_method transpose ns call_module nn Conv d ns call_method transpose ns call_function torch quantize_per_tensor ns call_module nnq Conv d ns call_method transpose ns call_method dequantize qconfig_dict = None module_name sub default_qconfig Only nodes Sub module instance quantized first transpose quantized because input quantized node_list = ns call_module nn Conv d ns call_function torch quantize_per_tensor ns call_method transpose ns call_module nnq Conv d ns call_method transpose ns call_method dequantize ns call_module nn Conv d ns call_method transpose qconfig_dict node_list qconfig_dict node_list qconfig_dict node_list example_inputs = torch randn m = M eval m = prepare_fx m qconfig_dict example_inputs=example_inputs m torch randn m = convert_fx m checkGraphModuleNodes m expected_node_list=node_list make sure runs m example_inputs test_qconfig_for_call_func Linear torch nn Module __init__ - None super __init__ w = torch ones b = torch zeros forward x torch nn functional linear x w b M torch nn Module __init__ - None super __init__ mods = torch nn Sequential Linear Linear mods = Linear forward x x = mods x x = mods x x model = M eval example_inputs = torch rand qconfig_dict = default_qconfig module_name mods None m = prepare_fx model qconfig_dict example_inputs=example_inputs m example_inputs m = convert_fx m node_list = ns call_function torch quantize_per_tensor ns call_function torch ops quantized linear ns call_function torch ops quantized linear ns call_method dequantize ns call_function torch nn functional linear checkGraphModuleNodes m expected_node_list=node_list m torch rand test_preserve_attributes M torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x conv x m = M m eval m preserved_attr = prepare_custom_config_dict = preserved_attributes preserved_attr example_inputs = torch randn m = prepare_fx m default_qconfig example_inputs=example_inputs prepare_custom_config=prepare_custom_config_dict assertAttrPreserved m assertTrue hasattr m preserved_attr assertEqual m preserved_attr assertAttrPreserved m convert_custom_config_dict = preserved_attributes preserved_attr m = convert_fx m convert_custom_config=convert_custom_config_dict assertAttrPreserved m skipIfNoFBGEMM test_qat_and_script model = LinearModelWithSubmodule train qengine = torch backends quantized engine qconfig_dict = torch ao quantization get_default_qat_qconfig qengine x = torch randn example_inputs = x model = prepare_qat_fx model qconfig_dict example_inputs=example_inputs ensure scripting works scripted = torch jit script model run one round make sure model runs scripted x FileCheck check_count FakeQuantize = prim GetAttr name= exactly=True \ run scripted graph disable fake_quant observer epoch range epoch == scripted apply torch ao quantization disable_observer epoch == scripted apply torch ao quantization disable_fake_quant ensure fake_quant observer have been disabled matches = fake_quant_enabled observer_enabled key v scripted state_dict items any x key x matches assertEqual v torch tensor dtype=torch int enable them back scripted apply torch ao quantization enable_fake_quant scripted apply torch ao quantization enable_observer key v scripted state_dict items any x key x matches assertEqual v torch tensor dtype=torch int skipIfNoFBGEMM test_save_observer_state_dict orig = LinearModelWithSubmodule eval model = orig qconfig_dict = torch ao quantization get_default_qconfig fbgemm x = torch randn model = prepare_fx model qconfig_dict example_inputs= x run through input model x save state_dict model obs_dict = torch ao quantization get_observer_state_dict model quant = convert_fx model b = io BytesIO torch save obs_dict b Load stats into new model weights_only True False b seek model_ = orig model_ = prepare_fx model_ qconfig_dict example_inputs= x loaded_dict = torch load b weights_only=weights_only torch ao quantization load_observer_state_dict model_ loaded_dict quant_ = convert_fx model_ Verify loaded state dict produces same results assertEqual quant x quant_ x skipIfNoFBGEMM test_custom_module_class CustomModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x ObservedCustomModule torch nn Module __init__ linear super __init__ linear = linear forward x linear x classmethod from_float cls float_module assert hasattr float_module qconfig observed = cls float_module linear observed qconfig = float_module qconfig observed StaticQuantCustomModule torch nn Module __init__ linear super __init__ linear = linear forward x linear x classmethod from_observed cls observed_module assert hasattr observed_module qconfig assert hasattr observed_module activation_post_process observed_module linear activation_post_process = \ observed_module activation_post_process quantized = cls nnq Linear from_float observed_module linear quantized DynamicQuantCustomModule torch nn Module __init__ linear super __init__ linear = linear forward x linear x classmethod from_observed cls observed_module assert hasattr observed_module qconfig observed_module linear qconfig = observed_module qconfig quantized = cls nnqd Linear from_float observed_module linear quantized M torch nn Module __init__ - None super __init__ linear = torch nn Linear custom = CustomModule forward x x = linear x x = custom x x RefM torch nn Module __init__ - None super __init__ linear = torch nn Linear linear = torch nn Linear forward x x = linear x x = linear x x instantiate M RefM align parameters original_m = M eval original_ref_m = RefM eval original_ref_m linear weight = torch nn Parameter original_m linear weight detach original_ref_m linear bias = torch nn Parameter original_m linear bias detach original_ref_m linear weight = torch nn Parameter original_m custom linear weight detach original_ref_m linear bias = torch nn Parameter original_m custom linear bias detach _qconfig = QConfig activation=MinMaxObserver with_args dtype=torch qint quant_min= quant_max= weight=default_weight_observer test_configs = static default_qconfig StaticQuantCustomModule static_a _qconfig StaticQuantCustomModule dynamic default_dynamic_qconfig DynamicQuantCustomModule quant_type QuantType STATIC QuantType DYNAMIC key = _get_quant_type_to_str quant_type qconfig quantized_module_class num_observers = test_configs key qconfig_dict = qconfig key == static prepare_custom_config_dict = float_to_observed_custom_module_class static CustomModule ObservedCustomModule convert_custom_config_dict = observed_to_quantized_custom_module_class static ObservedCustomModule quantized_module_class prepare_custom_config_dict = non_traceable_module_class CustomModule convert_custom_config_dict = observed_to_quantized_custom_module_class dynamic CustomModule quantized_module_class example_inputs = torch randn check prepared model m = prepare_fx copy deepcopy original_m qconfig_dict example_inputs=example_inputs prepare_custom_config=prepare_custom_config_dict calibration m example_inputs all activation observers inserted top level module count_check = ns call_module torch ao quantization MinMaxObserver num_observers checkGraphModuleNodes m expected_node_occurrence=count_check check converted quantized model m = convert_fx m convert_custom_config=convert_custom_config_dict quant_type == QuantType STATIC count_check = ns call_function torch quantize_per_tensor ns call_module nnq Linear ns call_method dequantize checkGraphModuleNodes m expected_node_occurrence=count_check assertEqual type m custom quantized_module_class res = m example_inputs quantize reference model ref_m = prepare_fx copy deepcopy original_ref_m qconfig_dict example_inputs=example_inputs ref_m example_inputs ref_m = convert_fx ref_m ref_res = ref_m example_inputs assertEqual res ref_res skipIfNoFBGEMM test_custom_module_class_input_has_multiple_users Tests flow still works when input custom module has multiple users CustomModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x ObservedCustomModule torch nn Module __init__ linear super __init__ linear = linear forward x linear x classmethod from_float cls float_module assert hasattr float_module qconfig observed = cls float_module linear observed qconfig = float_module qconfig observed StaticQuantCustomModule torch nn Module __init__ linear super __init__ linear = linear forward x linear x classmethod from_observed cls observed_module assert hasattr observed_module qconfig assert hasattr observed_module activation_post_process observed_module linear activation_post_process = \ observed_module activation_post_process quantized = cls nnq Linear from_float observed_module linear quantized M torch nn Module __init__ - None super __init__ linear = torch nn Linear custom = CustomModule forward x x = custom x x = linear x x + x prepare_custom_config_dict = float_to_observed_custom_module_class static CustomModule ObservedCustomModule convert_custom_config_dict = observed_to_quantized_custom_module_class static ObservedCustomModule StaticQuantCustomModule m = M eval example_inputs = torch randn m = prepare_fx m default_qconfig example_inputs=example_inputs prepare_custom_config=prepare_custom_config_dict make sure works m = convert_fx m convert_custom_config=convert_custom_config_dict make sure runs m example_inputs skipIfNoFBGEMM test_custom_module_class_input_has_duplicate_nodes Tests flow still works when graph has multiple nodes same custom module target CustomModule torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x ObservedCustomModule torch nn Module __init__ linear super __init__ linear = linear forward x linear x classmethod from_float cls float_module assert hasattr float_module qconfig observed = cls float_module linear observed qconfig = float_module qconfig observed StaticQuantCustomModule torch nn Module __init__ linear super __init__ linear = linear forward x linear x classmethod from_observed cls observed_module assert hasattr observed_module qconfig assert hasattr observed_module activation_post_process observed_module linear activation_post_process = \ observed_module activation_post_process quantized = cls nnq Linear from_float observed_module linear quantized M torch nn Module __init__ - None super __init__ custom = CustomModule forward x x = custom x x = custom x x + x prepare_custom_config_dict = float_to_observed_custom_module_class static CustomModule ObservedCustomModule convert_custom_config_dict = observed_to_quantized_custom_module_class static ObservedCustomModule StaticQuantCustomModule m = M eval example_inputs = torch randn m = prepare_fx m default_qconfig example_inputs=example_inputs prepare_custom_config=prepare_custom_config_dict make sure works m = convert_fx m convert_custom_config=convert_custom_config_dict make sure runs m example_inputs skipIfNoFBGEMM test_non_traceable_module NonTraceable torch nn Module forward x k x keys print x k x NonTraceable torch nn Module forward x data dependent control flow traceable i x print i x M torch nn Module __init__ - None super __init__ m = NonTraceable m = NonTraceable forward x x = m x x = m x x m = M eval qconfig_dict = default_qconfig prepare_custom_config_dict = non_traceable_module_name m non_traceable_module_class NonTraceable m = prepare_fx m qconfig_dict example_inputs= key torch randn prepare_custom_config=prepare_custom_config_dict node_occurrence = ns call_module NonTraceable ns call_module NonTraceable make sure these modules traced checkGraphModuleNodes m expected_node_occurrence=node_occurrence test_prepared_model_deepcopy Ensures copy deepcopy works correctly prepared model M torch nn Module __init__ - None super __init__ conv = torch nn Conv d _foobar = foobar foobar = foobar forward x x = conv x x m = M m eval qconfig_dict = torch ao quantization default_qconfig example_inputs = torch randn prepared = prepare_fx m qconfig_dict example_inputs=example_inputs calibrate prepared example_inputs copy prepared_copy = copy deepcopy prepared quantize should run no errors quantized = convert_fx prepared_copy test_quantized_model_type Test state_dict deepcopy works properly quantized model M torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x example_inputs = torch rand m = M eval m = prepare_fx m default_qconfig example_inputs=example_inputs m = convert_fx m test deepcopy m_copy = copy deepcopy m assertEqual m_copy example_inputs m example_inputs test state_dict state_dict = m state_dict m_new = M eval m_new = prepare_fx m_new default_qconfig example_inputs=example_inputs m_new = convert_fx m_new m_new load_state_dict state_dict assertEqual m_new example_inputs m example_inputs test_dequantize r Test make sure dequantize node placed before non-quantizable node M torch nn Module __init__ - None super __init__ conv = torch nn Conv d act = torch nn GELU forward x x = conv x act x data = torch rand dtype=torch float quant_type static_quant_types node_list = ns call_module nnq Conv d ns call_method dequantize ns call_module nn GELU checkGraphModeFxOp M eval data quant_type expected_node_list=node_list test_sequential M torch nn Module __init__ - None super __init__ convs = torch nn Sequential torch nn Conv d torch nn Conv d forward x x = convs x x data = torch rand dtype=torch float quant_type static_quant_types node_list = ns call_module nnq Conv d ns call_module nnq Conv d checkGraphModeFxOp M eval data quant_type expected_node_list=node_list _test_quantized_inputs_outputs prepare_custom_config_dict prepare_count_check convert_count_check Test option have inputs outputs graph quantized M torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv = torch nn Conv d forward x x = conv x x = conv x x quantized input quantized output m = M qconfig_dict = torch ao quantization default_qconfig example_inputs = torch randn m eval mp = torch ao quantization quantize_fx prepare_fx m qconfig_dict example_inputs=example_inputs prepare_custom_config=prepare_custom_config_dict checkGraphModuleNodes mp expected_node_occurrence=prepare_count_check mp example_inputs mq = torch ao quantization quantize_fx convert_fx mp checkGraphModuleNodes mq expected_node_occurrence=convert_count_check test_quantized_input_quantized_output prepare_custom_config_dict = input_quantized_idxs output_quantized_idxs prepare_count_check = ns call_module torch ao quantization MinMaxObserver convert_count_check = ns call_function torch quantize_per_tensor ns call_method dequantize _test_quantized_inputs_outputs prepare_custom_config_dict prepare_count_check convert_count_check test_fp _input_quantized_output prepare_custom_config_dict = output_quantized_idxs prepare_count_check = ns call_module torch ao quantization MinMaxObserver convert_count_check = ns call_function torch quantize_per_tensor ns call_method dequantize _test_quantized_inputs_outputs prepare_custom_config_dict prepare_count_check convert_count_check test_quantized_input_fp _output prepare_custom_config_dict = input_quantized_idxs prepare_count_check = ns call_module torch ao quantization MinMaxObserver convert_count_check = ns call_function torch quantize_per_tensor ns call_method dequantize _test_quantized_inputs_outputs prepare_custom_config_dict prepare_count_check convert_count_check test_fp _input_fp _output prepare_custom_config_dict = prepare_count_check = ns call_module torch ao quantization MinMaxObserver convert_count_check = ns call_function torch quantize_per_tensor ns call_method dequantize _test_quantized_inputs_outputs prepare_custom_config_dict prepare_count_check convert_count_check skipIfNoFBGEMM test_convtranspose_per_channel_fails_early r Verifies attempting quantize ConvTranspose module per-Channel weight observers fails prepare step opposed convert step m = torch nn Sequential torch nn ConvTranspose d m eval qconfig_dict = torch ao quantization get_default_qconfig fbgemm assertRaises AssertionError context mp = prepare_fx m qconfig_dict example_inputs= torch randn assertTrue str context exception == Per channel weight observer supported yet ConvTranspose n d skipIfNoFBGEMM test_qparams_buffers Linear torch nn Module __init__ - None super __init__ w = torch ones b = torch zeros forward x torch nn functional linear x w b M torch nn Module __init__ - None super __init__ mods = torch nn Sequential Linear Linear mods = Linear forward x x = mods x x = mods x x model = M eval qconfig_dict = default_qconfig example_inputs = torch rand m = prepare_fx model qconfig_dict example_inputs=example_inputs m example_inputs m = convert_fx m keys = m state_dict keys quant_scale_count = quant_zero_point = scale_count = zero_point_count = k keys input_scale k quant_scale_count = quant_scale_count + input_zero_point k quant_zero_point = quant_zero_point + scale k scale_count = scale_count + zero_point k zero_point_count = zero_point_count + Expect each quantized linear op have scale zero point assertTrue scale_count == Expect each quantized linear op have scale state_dict assertTrue zero_point_count == Expect each quantized linear op have zero_point state_dict m example_inputs ensure scriptable scripted = torch jit script m scripted_keys = scripted state_dict keys scripted mods _ _packed_weight_ = m state_dict mods _ _packed_weight_ non_packed_weight_keys = key key keys _packed_weight key assertTrue set scripted_keys == set non_packed_weight_keys Expected scripted model preserve state_dict non-packed weight attributes TODO probably don t want hardcode attribute names since they generated attr_name mods _ _input_scale_ mods _ _input_zero_point_ mods _ _scale_ mods _ _zero_point_ mods _ _scale_ mods _ _zero_point_ mods _scale_ mods _zero_point_ assertTrue hasattr m attr_name attr_name + found skipIfNoFBGEMM test_packed_weight_fused_op Linear torch nn Module __init__ - None super __init__ w = torch ones b = torch zeros forward x F linear x w b M torch nn Module __init__ - None super __init__ mods = torch nn Sequential Linear Linear mods = Linear relu = F relu forward x x = mods x x = mods x x = relu x x model = M eval example_inputs = torch rand qconfig_dict = default_qconfig m = prepare_fx model qconfig_dict example_inputs=example_inputs m example_inputs m = convert_fx m assert hasattr m mods _ _packed_weight_ assert hasattr m mods _ _packed_weight_ assert hasattr m mods _packed_weight_ skipIfNoFBGEMM test_mul_add_fp _config override_quantized_engine fbgemm Linear torch nn Module __init__ - None super __init__ w = torch ones b = torch zeros forward x torch nn functional linear x w b M torch nn Module __init__ - None super __init__ mods = torch nn Sequential Linear Linear mods = Linear forward x x = x x = x + x = mods x x = mods x x model = M eval qconfig_dict = float _dynamic_qconfig example_inputs = torch rand m = prepare_fx model qconfig_dict example_inputs=example_inputs m = convert_fx m make sure runs m example_inputs test_getattr_with_nontensor_result Verifies binary ops get quantized correctly some args nodes Tensors such ` x ndim ` pattern M torch nn Module forward x dims = x ndim dims_sub = dims - dims_sub = dims_sub - x = torch add x dims_sub x M torch nn Module forward x dims = x ndim dims_sub = dims - mul = dims_sub dims_list = - x size + mul x = x view dims_list x M torch nn Module forward x shape = x shape x = x view shape x cls M M M m = cls eval example_inputs = torch rand m example_inputs qconfig_dict = torch ao quantization default_qconfig mp = prepare_fx m qconfig_dict example_inputs=example_inputs mp torch rand mc = convert_fx mp _NonReferenceTestModel nn Module __init__ func lin_in lin_out super __init__ conv = nn Conv d pool = nn MaxPool d lin = nn Linear lin_in lin_out func = func forward x y z x = pool F relu conv x x = torch flatten x x = func x y z x = lin x x This function looks node specified NodeInfo key node_info_to_non_tensor_args checks args specified indices observed since they non tensors If args those indices tuple list which do show up nodes function checks individual elements tuple list recursively _check_not_observed model node_info_to_non_tensor_args helper function easier recursion checks whether arg_node observed _check_node_not_observed model arg_node node isinstance arg_node tuple list new_node arg_node _check_node_not_observed model new_node node arg_node op == call_module assertTrue _is_activation_post_process getattr model arg_node target f Arg arg_node node node observed float tensor node model graph nodes indices = node_info_to_non_tensor_args get NodeInfo node op node target index indices index len node args arg_node = node args index _check_node_not_observed model arg_node node This test checks model gets prepared correct doesn t have observers specific ops see _check_not_observed prepared model runs _test_dtype_propagation model node_info_to_non_tensor_args args model eval qconfig_dict = torch ao quantization get_default_qconfig fbgemm prepared_model = prepare_fx model qconfig_dict example_inputs=tuple args _check_not_observed prepared_model node_info_to_non_tensor_args prepared_model args test_masked_fill_nontensor_args_not_observed func x y z x masked_fill y z model = _NonReferenceTestModel func args = torch randn torch randn node_info_to_non_tensor_args = NodeInfo call_method masked_fill _test_dtype_propagation model node_info_to_non_tensor_args args test_permute_nontensor_args_not_observed func x y z x permute y z model = _NonReferenceTestModel func args = torch randn node_info_to_non_tensor_args = NodeInfo call_method permute _test_dtype_propagation model node_info_to_non_tensor_args args test_repeat_nontensor_args_not_observed func x y z x repeat y z model = _NonReferenceTestModel func args = torch randn node_info_to_non_tensor_args = NodeInfo call_method repeat _test_dtype_propagation model node_info_to_non_tensor_args args test_reshape_nontensor_args_not_observed func x y z x reshape - y model = _NonReferenceTestModel func args = torch randn None node_info_to_non_tensor_args = NodeInfo call_method reshape _test_dtype_propagation model node_info_to_non_tensor_args args test_size_nontensor_args_not_observed func x y z x reshape - x size y model = _NonReferenceTestModel func args = torch randn None node_info_to_non_tensor_args = NodeInfo call_method size _test_dtype_propagation model node_info_to_non_tensor_args args test_transpose_nontensor_args_not_observed func x y z x transpose y z model = _NonReferenceTestModel func args = torch randn node_info_to_non_tensor_args = NodeInfo call_method transpose _test_dtype_propagation model node_info_to_non_tensor_args args test_torch_transpose_nontensor_args_not_observed TODO make torch transpose traceable fx when using variable nontensor arguments func = lambda x y z torch transpose x y z error func x y z torch transpose x model = _NonReferenceTestModel func node_info_to_non_tensor_args = NodeInfo call_method torch transpose args = torch randn _test_dtype_propagation model node_info_to_non_tensor_args args test_unsqueeze_nontensor_args_not_observed func x y z x unsqueeze y model = _NonReferenceTestModel func args = torch randn None node_info_to_non_tensor_args = NodeInfo call_method unsqueeze _test_dtype_propagation model node_info_to_non_tensor_args args test_unsqueeze__nontensor_args_not_observed func x y z x unsqueeze_ y model = _NonReferenceTestModel func args = torch randn None node_info_to_non_tensor_args = NodeInfo call_method unsqueeze_ _test_dtype_propagation model node_info_to_non_tensor_args args test_torch_unsqueeze_nontensor_args_not_observed TODO make torch unsqueeze scriptable fx when using variable nontensor arguments func = lambda x y z torch unsqueeze x y error func x y z torch unsqueeze x model = _NonReferenceTestModel func args = torch randn None node_info_to_non_tensor_args = NodeInfo call_method torch unsqueeze _test_dtype_propagation model node_info_to_non_tensor_args args test_view_nontensor_args_not_observed func x y z x view - y model = _NonReferenceTestModel func args = torch randn None node_info_to_non_tensor_args = NodeInfo call_method view _test_dtype_propagation model node_info_to_non_tensor_args args test_propagate_dtypes_for_known_nodes_list_args func x y z x reshape y model = _NonReferenceTestModel func args = torch randn - None node_info_to_non_tensor_args = NodeInfo call_method reshape _test_dtype_propagation model node_info_to_non_tensor_args args test_propagate_dtypes_for_known_nodes_split_list_args func x y z x reshape y z model = _NonReferenceTestModel func args = torch randn - node_info_to_non_tensor_args = NodeInfo call_method reshape _test_dtype_propagation model node_info_to_non_tensor_args args test_propagate_dtypes_for_known_nodes_tuple_args func x y z x reshape y model = _NonReferenceTestModel func args = torch randn - None node_info_to_non_tensor_args = NodeInfo call_method reshape _test_dtype_propagation model node_info_to_non_tensor_args args test_propagate_dtypes_for_known_nodes_split_tuple_args func x y z x reshape y z model = _NonReferenceTestModel func args = torch randn - node_info_to_non_tensor_args = NodeInfo call_method reshape _test_dtype_propagation model node_info_to_non_tensor_args args test_propagate_dtypes_for_known_nodes_dict_args func x y z x transpose y first y second model = _NonReferenceTestModel func args = torch randn first second None node_info_to_non_tensor_args = NodeInfo call_method transpose _test_dtype_propagation model node_info_to_non_tensor_args args test_propagate_dtypes_for_known_nodes_dict_tuple_args reshape_module nn Module forward x y z x reshape y shape model = _NonReferenceTestModel reshape_module args = torch randn shape - None node_info_to_non_tensor_args = NodeInfo call_method reshape _test_dtype_propagation model node_info_to_non_tensor_args args test_propagate_dtypes_for_known_nodes_dict_split_tuple_args func x y z x reshape y first y second model = _NonReferenceTestModel func args = torch randn first - second None node_info_to_non_tensor_args = NodeInfo call_method transpose _test_dtype_propagation model node_info_to_non_tensor_args args test_assert_on_size_after_quant_layer Verifies calculating size quantized tensor works correctly quantization passes M torch nn Module __init__ - None super __init__ conv = nn Conv d forward x x = conv x torch _assert x size == foobar x m = M eval example_inputs = torch rand m example_inputs qconfig_dict = torch ao quantization default_qconfig mp = prepare_fx m qconfig_dict example_inputs=example_inputs mp example_inputs mc = convert_fx mp mc example_inputs test_fp _sum Verifies fp sum works correctly s before after quantized layers M torch nn Module __init__ - None super __init__ conv = nn Conv d forward x x = conv x x = torch stack x x = torch sum x x M torch nn Module __init__ - None super __init__ conv = nn Conv d conv = nn Conv d forward x x = conv x x = torch stack x x = torch sum x dim= x = conv x x cls M M m = cls eval example_inputs = torch rand m example_inputs qconfig_dict = torch ao quantization default_qconfig mp = prepare_fx m qconfig_dict example_inputs=example_inputs mp example_inputs mc = convert_fx mp mc example_inputs test_fusion_pattern_unquantized Ensure leaving possible fusion pattern multiple nodes unquantized runs through APIs without errors Child torch nn Module __init__ - None super __init__ relu = nn ReLU forward x x = torch add x x = torch nn functional relu x x Parent torch nn Module __init__ - None super __init__ child = Child conv = nn Conv d forward x x = child x x = conv x x m = Parent eval qconfig_dict = torch ao quantization default_qconfig module_name child None example_inputs = torch rand mp = prepare_fx m qconfig_dict example_inputs=example_inputs mp example_inputs mc = convert_fx mp test_state_dict Make sure packed params appear state_dict test linear packed weight M torch nn Module __init__ - None super __init__ w = torch rand b = torch rand forward x F linear x w b m = M eval qconfig_dict = default_qconfig m = prepare_fx m qconfig_dict example_inputs= torch randn m = convert_fx m state_dict = m state_dict assertTrue _packed_weight_ state_dict test conv packed weight M torch nn Module __init__ - None super __init__ w = torch rand b = torch rand stride = padding = dilation = groups = forward x F conv d x w b stride padding dilation groups m = M eval qconfig_dict = default_qconfig m = prepare_fx m qconfig_dict example_inputs= torch randn m = convert_fx m state_dict = m state_dict assertTrue _packed_weight_ state_dict test load ref_weight ref_bias = torch ops quantized conv d_unpack state_dict _packed_weight_ data = torch rand ref_res = m data m = M eval m = prepare_fx m qconfig_dict data m = convert_fx m res = m data weight bias = m _packed_weight_ unpack check random model weight bias does match ref weight bias assertNotEqual weight ref_weight assertNotEqual bias ref_bias assertNotEqual res ref_res m load_state_dict state_dict checkModel m data ref_weight ref_bias ref_res res = m data weight bias = m _packed_weight_ unpack check weight bias matches after load state_dict assertEqual weight ref_weight assertEqual bias ref_bias assertEqual res ref_res checkModel m data ref_weight ref_bias ref_res Test save disk load back m = M eval m = prepare_fx m qconfig_dict example_inputs= data m = convert_fx m m load_state_dict state_dict TemporaryFileName fname torch save m state_dict fname weights_only=False loading ScriptModule m load_state_dict torch load fname weights_only=False checkModel m data ref_weight ref_bias ref_res skipIfNoFBGEMM test_preserve_qconfig Test make sure temporary config option preserve qconfig attributes model works override_quantized_engine fbgemm Linear torch nn Module __init__ - None super __init__ w = torch ones b = torch zeros forward x torch nn functional linear x w b M torch nn Module __init__ - None super __init__ mods = torch nn Sequential Linear Linear mods = torch nn Sigmoid forward x x = mods x x = mods x x model = M eval qconfig_dict = object_type torch nn functional linear float _dynamic_qconfig example_inputs = torch rand m = prepare_fx model qconfig_dict example_inputs=example_inputs m example_inputs m = convert_fx m _remove_qconfig=False assertTrue hasattr m mods qconfig test_not_used Test quantizing used value M torch nn Module forward x x = x + x x sigmoid_ x m = M eval qconfig_mapping = get_default_qconfig_mapping set_global float _static_qconfig make sure quantization runs m = prepare_fx m qconfig_mapping example_inputs= torch randn m = convert_fx m test_qparams_fqn Test FQN input_scale zero_point set first linear use Linear torch nn Module __init__ - None super __init__ w = torch ones b = torch zeros forward x torch nn functional linear x w b M torch nn Module __init__ - None super __init__ mods = torch nn Sequential Linear Linear forward x x = torch cat x tmp = x size x = mods x y = x tmp y model = M eval qconfig_dict = None object_type torch nn functional linear default_qconfig torch nn functional relu default_qconfig example_inputs = torch rand m = prepare_fx model qconfig_dict example_inputs=example_inputs m example_inputs m = convert_fx m keys = m state_dict keys m torch randn TODO probably don t want hardcode attribute names since they generated attr_name mods _ _input_scale_ mods _ _input_zero_point_ mods _ _scale_ mods _ _zero_point_ mods _ _scale_ mods _ _zero_point_ assertTrue hasattr m attr_name attr_name + found test_no_obs_between_unmatched_node_and_copy_node Verifies observer inserted between unmatched node node matched CopyNodeQuantizeHandler This done because observers require activations Tensors there no guarantee output unmatched node Tensor M nn Module __init__ - None super __init__ relu = nn ReLU forward x x = _user_func_with_complex_return_type x x = x + x x m = M eval qconfig_dict = torch ao quantization default_qconfig example_inputs = torch randn mp = prepare_fx m qconfig_dict example_inputs=example_inputs observer inserted after _user_func_with_complex_return_type following call will fail mp example_inputs mc = convert_fx mp mc example_inputs test_fold_quant_dequant Test sequence quant-dequant nodes graph get folded we erase extra dequant nodes M torch nn Module __init__ - None super __init__ w = torch ones b = torch zeros forward x x = torch cat x tmp = x size x = torch nn functional linear x w b y = x tmp y model = M eval qconfig_dict = None object_type torch nn functional linear default_qconfig example_inputs = torch rand m = prepare_fx model qconfig_dict example_inputs=example_inputs m example_inputs m = convert_fx m keys = m state_dict keys m example_inputs dequant = quant = n m graph nodes n op == call_method n target == dequantize dequant = dequant + n op == call_function n target == torch quantize_per_tensor quant = quant + assertEqual dequant assertEqual quant test_quant_output_always_observed If output hardcoded quantized ensure there always observer even last non-output node quantizeable qconfig_dict = torch ao quantization get_default_qat_qconfig fbgemm prepare_custom_config_dict = output_quantized_idxs example_inputs = torch randn non-quantizeable node quantized output M torch nn Module __init__ - None super __init__ identity = torch nn Identity forward x x = identity x x m = M checkGraphModeFxOp m example_inputs QuantType QAT prepare_expected_node_occurrence= ns call_module torch ao quantization FusedMovingAvgObsFakeQuantize expected_node_occurrence= ns call_function torch quantize_per_tensor prepare_custom_config=prepare_custom_config_dict quantizeable node quantized output M torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x x = conv x x m = M checkGraphModeFxOp m example_inputs QuantType QAT prepare_expected_node_occurrence= one weights one activations ns call_module torch ao quantization FusedMovingAvgObsFakeQuantize expected_node_occurrence= ns call_function torch quantize_per_tensor prepare_custom_config=prepare_custom_config_dict quantizeable node quantized dictionary output M torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x x = conv x output x m = M checkGraphModeFxOp m example_inputs QuantType QAT prepare_expected_node_occurrence= one weights one activations ns call_module torch ao quantization FusedMovingAvgObsFakeQuantize expected_node_occurrence= ns call_function torch quantize_per_tensor prepare_custom_config=prepare_custom_config_dict test_deepcopy_preserve_attributes M torch nn Module __init__ - None super __init__ attr = forward x x m = M eval m = prepare_fx m default_qconfig example_inputs= torch randn prepare_custom_config= preserved_attributes attr preserved attributes also stored meta so doesn t get lost during deepcopy assertTrue hasattr m attr assertTrue attr m meta _USER_PRESERVED_ATTRIBUTES_KEY m = copy deepcopy m assertTrue hasattr m attr assertTrue attr m meta _USER_PRESERVED_ATTRIBUTES_KEY m = convert_fx m convert_custom_config= preserved_attributes attr assertTrue hasattr m attr assertTrue attr m meta _USER_PRESERVED_ATTRIBUTES_KEY m = copy deepcopy m assertTrue hasattr m attr assertTrue attr m meta _USER_PRESERVED_ATTRIBUTES_KEY test_output_lists_and_dicts Verify specifying complicated output types does crash M torch nn Module __init__ - None super __init__ conv = nn Conv d forward x x = conv x foo x foo x m = M eval qconfig_dict = torch ao quantization default_qconfig mp = prepare_fx m qconfig_dict example_inputs= torch randn mc = convert_fx mp test_shape_followed_by_quantized_op Make sure shape does dequantize Tensor before next operator M torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv = torch nn Conv d forward x x = conv x s = x shape torch _assert s == x shape x = conv x x make sure quantization runs m = M eval example_inputs = torch randn m = prepare_fx m default_qconfig example_inputs=example_inputs m = convert_fx m m example_inputs node_occurrence = ns call_function torch quantize_per_tensor ns call_method dequantize checkGraphModuleNodes m expected_node_occurrence=node_occurrence test_trace_quantize_per_tensor M torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x x = conv x x m = M eval m = prepare_fx m default_qconfig example_inputs= torch randn m = convert_fx m Make sure runs without error m = torch fx Transformer m transform test_copy_node_has_shared_actpp_instance Test output CopyNode have same observer fake_quant instance input M torch nn Module __init__ - None super __init__ avgpool d = torch nn AvgPool d kernel_size= forward x x = avgpool d x x quant_type static_quant_types m = M Checks we have observer both input output occurrence_map = QuantType STATIC ns call_module torch ao quantization MinMaxObserver QuantType QAT ns call_module torch ao quantization FakeQuantize quant_type == QuantType QAT m train prepare = prepare_qat_fx qconfig = default_qat_qconfig actpp_module_class = torch ao quantization FakeQuantize m eval prepare = prepare_fx qconfig = default_qconfig actpp_module_class = torch ao quantization MinMaxObserver example_inputs = torch randn m = prepare m qconfig example_inputs=example_inputs check there duplicated observer instance actpp_module_count = module m modules remove_duplicate=False isinstance module actpp_module_class actpp_module_count += assertEqual actpp_module_count actpp_module_count = module m modules isinstance module actpp_module_class actpp_module_count += assertEqual actpp_module_count m_copy = copy deepcopy m m = convert_fx m m_reference = convert_to_reference_fx m_copy checks non-reference quantized model node_occurrence = ns call_function torch quantize_per_tensor ns call_method dequantize node_list = ns call_function torch quantize_per_tensor ns call_module torch nn AvgPool d ns call_method dequantize checkGraphModuleNodes m expected_node_occurrence=node_occurrence expected_node_list=node_list checks reference quantized model copy nodes we ll have dequant - copy_node - quant patterns which will fused later backend lowering step node_occurrence = ns call_function torch quantize_per_tensor ns call_method dequantize node_list = ns call_function torch quantize_per_tensor ns call_method dequantize ns call_module torch nn AvgPool d ns call_function torch quantize_per_tensor ns call_method dequantize checkGraphModuleNodes m_reference expected_node_occurrence=node_occurrence expected_node_list=node_list test_linear_qint _activation Test support qint activation reference pattern M torch nn Module __init__ - None super __init__ conv = torch nn Conv d linear = torch nn Linear forward x x = conv x x = torch flatten x x = linear x x m = M eval example_inputs = torch rand m = prepare_fx m torch ao quantization QConfig activation=torch ao quantization HistogramObserver with_args qscheme=torch per_tensor_symmetric dtype=torch qint weight=torch ao quantization default_per_channel_weight_observer example_inputs=example_inputs m = convert_to_reference_fx m m example_inputs test_preserve_tuple Test tuple input type preserved LSTM nn Module __init__ - None super __init__ lstm = nn LSTM forward inputs torch Tensor state list torch Tensor h = state c = state lstm inputs h c m = LSTM eval example_inputs = torch randn torch randn torch randn m = prepare_fx m default_qconfig example_inputs=example_inputs make sure arg lstm module tuple n m graph nodes n target == lstm assertEqual type n args tuple _test_static_lstm_helper model prepare_node_occurrence convert_node_occurrence Helper method validate graph model static LSTM qconfig_mapping = get_default_qconfig_mapping prepare_custom_config = PrepareCustomConfig \ set_float_to_observed_mapping torch nn LSTM torch ao nn quantizable LSTM convert_custom_config = ConvertCustomConfig \ set_observed_to_quantized_mapping torch ao nn quantizable LSTM torch ao nn quantized LSTM example_inputs = torch rand torch rand torch randn model = prepare_fx model qconfig_mapping example_inputs prepare_custom_config=prepare_custom_config checkGraphModuleNodes model expected_node_occurrence=prepare_node_occurrence model example_inputs model = convert_fx model convert_custom_config=convert_custom_config checkGraphModuleNodes model expected_node_occurrence=convert_node_occurrence model example_inputs test_static_lstm Test statically quantized custom module LSTM followed ops consume individual tensors output tuple MyModel nn Module __init__ - None super __init__ lstm = nn LSTM linear = nn Linear linear = nn Linear linear = nn Linear forward inputs torch Tensor h torch Tensor c torch Tensor out h _out c _out = lstm inputs h c out = linear out h _out = linear h _out c _out = linear c _out out h _out c _out m = MyModel prepare_node_occurrence = ns call_module torch ao nn quantizable LSTM convert_node_occurrence = ns call_module torch ao nn quantized LSTM ns call_function torch quantize_per_tensor lstm dequantize lstm dequantize lstm dequantize ns call_method dequantize lstm lstm lstm lstm ns call_function operator getitem No tuples consumed ns call_function tuple _test_static_lstm_helper m prepare_node_occurrence convert_node_occurrence test_static_lstm_consume_tuple Test statically quantized custom module LSTM followed module consumes output tuple either whole part ModuleAfterLSTM nn Module __init__ - None super __init__ identity = torch nn Identity forward x identity x ConsumeWholeTuple nn Module __init__ - None super __init__ lstm = nn LSTM module_after_lstm = ModuleAfterLSTM forward inputs torch Tensor h torch Tensor c torch Tensor x = lstm inputs h c x = module_after_lstm x consume tuple output hidden hidden x ConsumeHiddenTuple ConsumeWholeTuple forward inputs torch Tensor h torch Tensor c torch Tensor x = lstm inputs h c x = module_after_lstm x consume tuple hidden hidden x Test consuming whole tuple output hidden hidden m = ConsumeWholeTuple prepare_node_occurrence = ns call_module torch ao nn quantizable LSTM convert_node_occurrence = ns call_module torch ao nn quantized LSTM ns call_function torch quantize_per_tensor lstm dequantize lstm dequantize lstm dequantize ns call_method dequantize lstm lstm lstm lstm ns call_function operator getitem tuple output_dq tuple hidden _dq hidden _dq ns call_function tuple _test_static_lstm_helper m prepare_node_occurrence convert_node_occurrence Test consuming just hidden tuple hidden hidden m = ConsumeHiddenTuple convert_node_occurrence = ns call_module torch ao nn quantized LSTM ns call_function torch quantize_per_tensor lstm dequantize lstm dequantize ns call_method dequantize lstm lstm lstm ns call_function operator getitem tuple hidden _dq hidden _dq ns call_function tuple _test_static_lstm_helper m prepare_node_occurrence convert_node_occurrence test_static_lstm_with_custom_fixed_qparams Test statically quantized LSTM custom fixed qparams assigned each inner submodules This flow requires users extend ` torch ao nn quantizable LSTM ` use child custom module mapping MyModel torch nn Module __init__ - None super __init__ my_lstm = torch nn LSTM forward inputs torch Tensor h torch Tensor c torch Tensor x = my_lstm inputs h c x Construct BackendConfig supports qint certain ops TODO build BackendConfig scratch instead modifying existing one qint _dtype_config = DTypeConfig input_dtype=torch qint output_dtype=torch qint my_backend_config = get_qnnpack_backend_config config my_backend_config configs config pattern torch nn Sigmoid torch nn Tanh torch add torch mul config add_dtype_config qint _dtype_config UserObservedLSTM torch ao nn quantizable LSTM Example user provided LSTM implementation assigns fixed qparams inner ops classmethod from_float cls float_lstm assert isinstance float_lstm cls _FLOAT_MODULE uint - linear_output_obs_ctr = FixedQParamsObserver with_args scale= - zero_point= dtype=torch qint uint sigmoid_obs_ctr = FixedQParamsObserver with_args scale= - zero_point= dtype=torch qint uint - tanh_obs_ctr = FixedQParamsObserver with_args scale= - zero_point= dtype=torch qint int - cell_state_obs_ctr = FixedQParamsObserver with_args scale= - zero_point= dtype=torch qint uint - hidden_state_obs_ctr = FixedQParamsObserver with_args scale= - zero_point= dtype=torch quint example_inputs = torch rand torch rand torch randn torch ao quantization fx lstm_utils _get_lstm_with_individually_observed_parts float_lstm=float_lstm example_inputs=example_inputs backend_config=my_backend_config linear_output_obs_ctr=linear_output_obs_ctr sigmoid_obs_ctr=sigmoid_obs_ctr tanh_obs_ctr=tanh_obs_ctr cell_state_obs_ctr=cell_state_obs_ctr hidden_state_obs_ctr=hidden_state_obs_ctr UserQuantizedLSTM torch ao nn quantized LSTM Example user provided LSTM implementation produces reference quantized module ` UserObservedLSTM ` classmethod from_observed cls observed_lstm assert isinstance observed_lstm cls _FLOAT_MODULE torch ao quantization fx lstm_utils _get_reference_quantized_lstm_module observed_lstm=observed_lstm backend_config=my_backend_config FX graph mode quantization m = MyModel qconfig_mapping = get_default_qconfig_mapping qnnpack example_inputs = torch rand torch rand torch randn prepare_custom_config = PrepareCustomConfig \ set_float_to_observed_mapping torch nn LSTM UserObservedLSTM convert_custom_config = ConvertCustomConfig \ set_observed_to_quantized_mapping torch ao nn quantizable LSTM UserQuantizedLSTM prepared = prepare_fx m qconfig_mapping example_inputs prepare_custom_config backend_config=my_backend_config prepared example_inputs converted = convert_fx prepared convert_custom_config backend_config=my_backend_config converted example_inputs Find patterns dq - op - q_to_specific_dtype graph verify qparams dtypes set correctly quantize ops node_name_to_expected_quantize_args = igates None None torch quint hgates None None torch quint add - torch qint gates add input_gate - torch qint forget_gate - torch qint cell_gate - torch qint output_gate - torch qint mul - torch qint fgate_cx mul mul_ - torch qint igate_cgate mul add_ - torch qint fgate_cx_igate_cgate add mul_ - torch quint ogate_cy mul cell = converted my_lstm layers get_submodule layer_fw cell matched_names = set node cell graph nodes node name node_name_to_expected_quantize_args continue matched_names add node name Match preceding dequantize assertTrue all arg target == dequantize arg node args Match following quantize specific qparams dtypes expected_scale expected_zp expected_dtype = node_name_to_expected_quantize_args node name user node users keys assertEqual user target torch quantize_per_tensor expected_scale None assertEqual getattr cell user args target expected_scale expected_zp None assertEqual getattr cell user args target expected_zp assertEqual user args - expected_dtype Ensure all patterns matched assertEqual matched_names set node_name_to_expected_quantize_args keys test_reroute_tuple_getitem_patterns The following graph should redirect output ` b ` After transformation all other nodes including inputs ` ` ` c ` no longer needed b c &#124; \\ \\ tuple \\ tuple \\ \\ &#124; \\ &#124; \\ &#124; \\ getitem getitem &#124; \\ &#124; getitem getitem &#124; \\ \\ tuple \\ \\ tuple &#124; getitem &#124; getitem &#124; output Construct graph manually because symbolic_trace does insert tuple getitem nodes graph = torch fx Graph = graph create_node placeholder b = graph create_node placeholder b c = graph create_node placeholder c bc = graph call_function tuple args= b c abc = graph call_function tuple args= bc Break down tuple reconstruct again = graph call_function operator getitem args= abc bc = graph call_function operator getitem args= abc b = graph call_function operator getitem args= bc c = graph call_function operator getitem args= bc bc = graph call_function tuple args= b c abc = graph call_function tuple args= bc Output tuple bc = graph call_function operator getitem args= abc b = graph call_function operator getitem args= bc output = graph output b Do reroute _reroute_tuple_getitem_pattern graph Assert output reroutes ` b ` directly all other nodes can removed output_ancestors = gather_ancestors current_node noqa E arg current_node args output_ancestors append arg gather_ancestors arg gather_ancestors output assertEqual output_ancestors b assertEqual output args b test_relu_lowering M torch nn Module forward x torch nn functional relu x m = M eval m = prepare_fx m default_qconfig example_inputs= torch randn m_copy = copy deepcopy m m = convert_fx m m_ref = convert_to_reference_fx m_copy node_occurrence = ns call_function torch quantize_per_tensor ns call_method dequantize node_occurrence_ref = ns call_function torch quantize_per_tensor ns call_method dequantize checkGraphModuleNodes m expected_node_occurrence=node_occurrence checkGraphModuleNodes m_ref expected_node_occurrence=node_occurrence_ref skipIfNoFBGEMM test_dynamic_with_fusion Tests dynamic quantization APIs work Linear + Relu fusion override_quantized_engine fbgemm LinearRelu torch nn Module __init__ - None super __init__ linear = torch nn Linear relu = torch nn ReLU forward x x = linear x relu x Linear torch nn Module __init__ - None super __init__ w = torch ones b = torch zeros forward x torch nn functional linear x w b M torch nn Module __init__ - None super __init__ mods = torch nn Sequential LinearRelu LinearRelu mods = Linear relu = F relu forward x x = mods x x = mods x x = relu x x dynamic_quantized_ops = float _dynamic_qconfig torch ops quantized linear_relu_dynamic_fp default_dynamic_qconfig torch ops quantized linear_relu_dynamic qconfig float _dynamic_qconfig default_dynamic_qconfig model = M eval qconfig_dict = qconfig example_inputs = torch rand m = prepare_fx model qconfig_dict example_inputs=example_inputs m = convert_fx m m example_inputs node_list = ns call_module nniqd LinearReLU ns call_module nniqd LinearReLU ns call_function dynamic_quantized_ops qconfig checkGraphModuleNodes m expected_node_list=node_list skipIfNoFBGEMM test_dynamic_with_fusion_multiple_uses Tests dynamic quantization APIs work Linear + Relu fusion override_quantized_engine fbgemm LinearRelu torch nn Module __init__ - None super __init__ linear = torch nn Linear relu = torch nn ReLU forward x x = linear x relu x M torch nn Module __init__ - None super __init__ linear_relu = LinearRelu forward x x = linear_relu x x = linear_relu x x qconfig float _dynamic_qconfig default_dynamic_qconfig model = M eval qconfig_dict = qconfig example_inputs = torch randn m = prepare_fx model qconfig_dict example_inputs=example_inputs m = convert_fx m m example_inputs node_list = ns call_module nniqd LinearReLU ns call_module nniqd LinearReLU checkGraphModuleNodes m expected_node_list=node_list skipIfNoFBGEMM test_dynamic_linear_input_multiple_use Tests input dynamic linear being used multiple ops override_quantized_engine fbgemm LinearRelu torch nn Module __init__ - None super __init__ linear = torch nn Linear relu = torch nn ReLU forward x x = linear x relu x M torch nn Module __init__ - None super __init__ mod = LinearRelu mod = LinearRelu forward x y = mod x y = mod x y + y qconfig float _dynamic_qconfig default_dynamic_qconfig model = M eval qconfig_dict = qconfig example_inputs = torch rand m = prepare_fx model qconfig_dict example_inputs=example_inputs m = convert_fx m m example_inputs node_list = ns call_module nniqd LinearReLU ns call_module nniqd LinearReLU checkGraphModuleNodes m expected_node_list=node_list test_ref_linear_module Make sure numerics models ref linear module matches models fbgemm qnnpack module M torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x M torch nn Module __init__ - None super __init__ linear = torch nn Linear relu = torch nn ReLU forward x relu linear x M M M m = M eval example_inputs = torch randn m = prepare_fx m default_qconfig example_inputs=example_inputs m_copy = copy deepcopy m m = convert_fx m m_ref = convert_to_reference_fx m_copy result = m example_inputs result_ref = m_ref example_inputs assertTrue torch equal result result_ref test_ref_conv_module Make sure numerics models ref conv module matches models fbgemm qnnpack module convs = nn Conv d nn Conv d nn Conv d M torch nn Module __init__ dim super __init__ conv = convs dim forward x conv x M torch nn Module __init__ dim super __init__ conv = convs dim relu = torch nn ReLU forward x relu conv x dim M itertools product M M m = M dim eval data = img_data_dict dim m = prepare_fx m default_qconfig example_inputs= data m_copy = copy deepcopy m m = convert_fx m m_ref = convert_to_reference_fx m_copy result = m data result_ref = m_ref data assertTrue torch equal result result_ref test_sub_scalar M torch nn Module forward x x = x + x = x - x = x + x = x - x m = M eval m = prepare_fx m default_qconfig example_inputs= torch rand m = convert_fx m occurrence = ns call_function torch quantize_per_tensor ns call_method dequantize checkGraphModuleNodes m expected_node_occurrence=occurrence test_observer_fqn Test make sure observer FQN based quantizable op module observing uses modules FQN determine observer name Linear torch nn Module __init__ - None super __init__ w = torch ones b = torch zeros forward x torch nn functional linear x w b M torch nn Module __init__ - None super __init__ mods = torch nn Sequential Linear Linear mods = Linear mods = torch nn Linear forward x x = mods x x = torch add x x = mods x y = torch add x z = torch mul x = mods y z model = M eval prepared = prepare_fx model default_qconfig example_inputs= torch randn name_list = name mod prepared named_modules isinstance mod torch ao quantization observer MinMaxObserver name_list append name expected_name_list = activation_post_process_ activation_post_process_ activation_post_process_ activation_post_process_ activation_post_process_ activation_post_process_ activation_post_process_ activation_post_process_ assert name_list == expected_name_list test_conv_lowering convs = nn Conv d nn Conv d nn Conv d qconvs = nn quantized Conv d nn quantized Conv d nn quantized Conv d M torch nn Module __init__ dim super __init__ conv = convs dim forward x conv x dim range len convs + m = M dim eval data = img_data_dict dim m = prepare_fx m default_qconfig example_inputs= data m_ref = copy deepcopy m m_ref = convert_to_reference_fx m_ref m = convert_fx m out_ref = m_ref data out = m data check reference pattern quantized conv module fused expected_node_occurrence = ns call_function torch quantize_per_tensor ns call_module qconvs dim ns call_method dequantize checkGraphModuleNodes m expected_node_occurrence=expected_node_occurrence checking result match assertTrue torch equal out_ref out test_convert_qconfig_mapping Linear torch nn Module __init__ - None super __init__ w = torch ones b = torch zeros forward x torch nn functional linear x w b M torch nn Module __init__ - None super __init__ mods = torch nn Sequential Linear Linear mods = torch nn Linear forward x x = mods x x = torch add x z = torch mul x x = mods z x model = M train check module_name object_type qconfig_dict = None object_type nn functional linear get_default_qat_qconfig fbgemm torch add get_default_qat_qconfig fbgemm nn Linear get_default_qat_qconfig fbgemm example_inputs = torch rand prepared = prepare_qat_fx model qconfig_dict example_inputs=example_inputs prepared example_inputs check == module_name convert_qconfig_dict = None object_type nn functional linear get_default_qat_qconfig fbgemm torch add get_default_qat_qconfig fbgemm nn Linear get_default_qat_qconfig fbgemm module_name mods None node_occurrence = ns call_function torch quantize_per_tensor ns call_function torch nn functional linear ns call_function torch ops quantized linear ns call_function torch ops quantized add ns call_method dequantize order_check = ns call_function torch nn functional linear ns call_function torch quantize_per_tensor ns call_function torch ops quantized linear ns call_function torch ops quantized add ns call_method dequantize ns call_function torch quantize_per_tensor ns call_module nnq Linear ns call_method dequantize check == object_type convert_qconfig_dict = None object_type nn functional linear get_default_qat_qconfig fbgemm torch add get_default_qat_qconfig fbgemm nn Linear None node_occurrence = ns call_function torch quantize_per_tensor ns call_function torch ops quantized linear ns call_function torch ops quantized add ns call_function torch mul ns call_method dequantize order_check = ns call_function torch quantize_per_tensor ns call_function torch ops quantized linear ns call_function torch ops quantized linear ns call_function torch ops quantized add ns call_method dequantize ns call_function torch mul ns call_module nn Linear converted = convert_fx prepared qconfig_mapping=convert_qconfig_dict converted torch rand checkGraphModuleNodes converted expected_node_occurrence=node_occurrence expected_node_list=order_check _assertFixedQParamsFakeQuantizeEqual fq fq assertEqual fq _observer_ctr fq _observer_ctr test_register_patterns cleanUp del _DEFAULT_FUSION_PATTERNS dummy_fusion del _DEFAULT_QUANTIZATION_PATTERNS dummy_quant del _DEFAULT_QUANTIZATION_PATTERNS dummy_quant del _DEFAULT_QUANTIZATION_PATTERNS dummy_quant del _DEFAULT_OUTPUT_OBSERVER_MAP dummy_quant del _DEFAULT_OUTPUT_OBSERVER_MAP dummy_quant del _DEFAULT_OUTPUT_FAKE_QUANTIZE_MAP dummy_quant del _DEFAULT_OUTPUT_FAKE_QUANTIZE_MAP dummy_quant addCleanup cleanUp _register_fusion_pattern dummy_fusion DummyFusion pass _register_quant_pattern dummy_quant DummyQuant pass _register_quant_pattern dummy_quant default_fixed_qparams_range_ _observer DummyQuant pass _register_quant_pattern dummy_quant default_fixed_qparams_range_neg _observer DummyQuant pass assertEqual _DEFAULT_FUSION_PATTERNS dummy_fusion DummyFusion assertEqual _DEFAULT_QUANTIZATION_PATTERNS dummy_quant DummyQuant assertEqual _DEFAULT_QUANTIZATION_PATTERNS dummy_quant DummyQuant assertEqual _DEFAULT_QUANTIZATION_PATTERNS dummy_quant DummyQuant assertEqual _DEFAULT_OUTPUT_OBSERVER_MAP dummy_quant default_fixed_qparams_range_ _observer assertEqual _DEFAULT_OUTPUT_OBSERVER_MAP dummy_quant default_fixed_qparams_range_neg _observer _assertFixedQParamsFakeQuantizeEqual _DEFAULT_OUTPUT_FAKE_QUANTIZE_MAP dummy_quant default_fixed_qparams_range_ _fake_quant _assertFixedQParamsFakeQuantizeEqual _DEFAULT_OUTPUT_FAKE_QUANTIZE_MAP dummy_quant default_fixed_qparams_range_neg _fake_quant output_fake_quantize_map = get_default_output_activation_post_process_map is_training=True output_observer_map = get_default_output_activation_post_process_map is_training=False assertEqual output_observer_map get dummy_quant default_fixed_qparams_range_neg _observer _assertFixedQParamsFakeQuantizeEqual output_fake_quantize_map get dummy_quant default_fixed_qparams_range_neg _fake_quant test_reuse_input_qconfig M torch nn Module __init__ - None super __init__ conv = torch nn Conv d forward x x = conv x x = x reshape x M torch nn Module forward x x = x reshape x options = itertools product M M True False M is_qat options m = M eval example_inputs = torch randn m = prepare_fx m get_default_qconfig_mapping example_inputs=example_inputs m = convert_fx m node_list = ns call_function torch quantize_per_tensor ns call_module nnq Conv d ns call_method reshape ns call_method dequantize checkGraphModuleNodes m expected_node_list=node_list m = M eval m = prepare_fx m get_default_qconfig_mapping example_inputs=example_inputs m = convert_fx m node_occurrence = ns call_function torch quantize_per_tensor ns call_method dequnatize node_list = ns call_method reshape checkGraphModuleNodes m expected_node_occurrence=node_occurrence expected_node_list=node_list test_stack_trace_preserved_linear M nn Module __init__ - None super __init__ linear = nn Linear forward x x = linear x x m = M eval mp = prepare_fx m get_default_qconfig_mapping example_inputs= torch randn found_stack_trace = False n mp graph nodes n op == call_module n target == linear found_stack_trace = n stack_trace None break assertTrue found_stack_trace test reference model mq = convert_to_reference_fx copy deepcopy mp found_stack_trace = False n mq graph nodes n op == call_module n target == linear found_stack_trace = n stack_trace None break assertTrue found_stack_trace f stack trace found node n format_node is_reference True test quantized model mq = convert_fx mp found_stack_trace = False n mq graph nodes n op == call_module n target == linear found_stack_trace = n stack_trace None break assertTrue found_stack_trace f stack trace found node n format_node is_reference False test_qat_skip_untraced UnTraceableModuleClass nn Module __init__ - None super __init__ linear = nn Linear forward x linear x UnTraceableModuleName nn Module __init__ - None super __init__ linear = nn Linear forward x linear x M nn Module __init__ - None super __init__ untraceable_module_class = UnTraceableModuleClass untraceable_module_name = UnTraceableModuleClass forward x x = untraceable_module_class x x = untraceable_module_name x x mod = M qconfig_dict = torch ao quantization get_default_qat_qconfig prepare_custom_config_dict = non_traceable_module_class UnTraceableModuleClass non_traceable_module_name untraceable_module_name example_inputs = torch randn mod_prep = torch ao quantization quantize_fx prepare_qat_fx mod train qconfig_dict example_inputs=example_inputs prepare_custom_config=prepare_custom_config_dict mod_prep = torch ao quantization quantize_fx prepare_qat_fx mod train qconfig_dict example_inputs=example_inputs prepare_custom_config=prepare_custom_config_dict assertTrue isinstance mod_prep untraceable_module_class linear torch nn Linear assertTrue isinstance mod_prep untraceable_module_name linear torch nn Linear assertTrue type mod_prep untraceable_module_class linear torch ao nn qat modules linear Linear prepare_qat_fx shold convert anything inside untraced module classes assertTrue type mod_prep untraceable_module_name linear torch ao nn qat modules linear Linear prepare_qat_fx shold convert anything inside modules named untraced_module_names test_qconfig_dict_setup M torch nn Module __init__ - None super __init__ Conv d = torch nn Conv d Conv d = torch nn Conv d Conv d = torch nn Conv d ConvTranspose d = torch nn ConvTranspose d ConvTranspose d = torch nn ConvTranspose d ConvTranspose d = torch nn ConvTranspose d Linear = torch nn Linear forward x x = Conv d x x = Conv d x x = Conv d x x = ConvTranspose d x x = ConvTranspose d x x = ConvTranspose d x x = Linear x x = torch nn functional conv d x torch rand x = torch nn functional conv d x torch rand x = torch nn functional conv d x torch rand x = torch nn functional linear x torch rand x backends = qnnpack fbgemm func get_default_qconfig_mapping get_default_qat_qconfig_mapping backend backends m = M eval qconfig_dict = func backend m = prepare_fx m qconfig_dict example_inputs= torch randn mod m modules _is_activation_post_process mod mod dtype == torch quint backend == fbgemm lower_bnd = upper_bnd = lower_bnd = upper_bnd = issubclass type mod FakeQuantize assertEqual mod activation_post_process quant_min lower_bnd assertEqual mod activation_post_process quant_max upper_bnd assertEqual mod quant_min lower_bnd assertEqual mod quant_max upper_bnd test_prepare_mode LinearModel torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x _test prepare_fn qconfig_dict m = LinearModel m = copy deepcopy m m train example_inputs = torch randn prepare_fn m qconfig_dict example_inputs=example_inputs m = copy deepcopy m m eval prepare_fn m qconfig_dict example_inputs=example_inputs Ensure prepare_fx prepare_qat_fx work both training eval modes _test prepare_fx get_default_qconfig_mapping _test prepare_qat_fx get_default_qat_qconfig_mapping _validate_qconfig_against_backend_config_constraints model torch nn Module qconfig QConfig backend_config BackendConfig satisfies_constraints bool qconfig_name Optional str = None Helper method validate whether ` qconfig ` satisfies constraints specified ` backend_config ` qconfig_mapping = QConfigMapping set_object_type torch nn Linear qconfig example_inputs = torch rand dtype=torch float model = prepare_fx model qconfig_mapping example_inputs backend_config=backend_config model example_inputs model = convert_fx model backend_config=backend_config satisfies_constraints expected_node_occurrence = ns call_module torch ao nn quantized Linear ns call_module torch nn Linear expected_node_occurrence = ns call_module torch ao nn quantized Linear ns call_module torch nn Linear try checkGraphModuleNodes model expected_node_occurrence=expected_node_occurrence except AssertionError e qconfig_name None print f ERROR Validation QConfig qconfig_name failed raise e test_backend_config_quantization_range Check quantization ranges specified through BackendConfig reflected observers inserted into model MyModel torch nn Module __init__ - None super __init__ linear = torch nn Linear float forward x linear x dtype_config = DTypeConfig input_dtype=DTypeWithConstraints dtype=torch quint quant_min_lower_bound= quant_max_upper_bound= output_dtype=DTypeWithConstraints dtype=torch quint quant_min_lower_bound= quant_max_upper_bound= weight_dtype=DTypeWithConstraints dtype=torch qint quant_min_lower_bound=- quant_max_upper_bound= bias_dtype=torch float backend_config = BackendConfig \ set_backend_pattern_config BackendPatternConfig torch nn Linear set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT noqa E add_dtype_config dtype_config set_root_module torch nn Linear set_reference_quantized_module nnqr Linear validate_qconfig qconfig QConfig satisfies_constraints bool _validate_qconfig_against_backend_config_constraints MyModel qconfig backend_config satisfies_constraints Case QConfig ranges fit within backend ranges OK qconfig = QConfig activation=MinMaxObserver with_args quant_min= quant_max= dtype=torch quint weight=MinMaxObserver with_args quant_min=- quant_max= dtype=torch qint qscheme=torch per_tensor_symmetric validate_qconfig qconfig satisfies_constraints=True Case QConfig activation range falls outside backend range should fail qconfig = QConfig activation=MinMaxObserver with_args quant_min= quant_max= dtype=torch quint weight=MinMaxObserver with_args dtype=torch qint qscheme=torch per_tensor_symmetric validate_qconfig qconfig satisfies_constraints=False Case QConfig weight range falls outside backend range should fail qconfig = QConfig activation=MinMaxObserver with_args dtype=torch quint weight=MinMaxObserver with_args quant_min=- quant_max= dtype=torch qint qscheme=torch per_tensor_symmetric validate_qconfig qconfig satisfies_constraints=False Case QConfig doesn t specify range should fail qconfig = QConfig activation=ReuseInputObserver weight=ReuseInputObserver validate_qconfig qconfig satisfies_constraints=False test_backend_config_scale_min Test QConfig eps validation against BackendConfig s min scale value MyModel torch nn Module __init__ - None super __init__ linear = torch nn Linear float forward x linear x dtype_config = DTypeConfig input_dtype=DTypeWithConstraints dtype=torch quint scale_min_lower_bound= - output_dtype=DTypeWithConstraints dtype=torch quint scale_min_lower_bound= - weight_dtype=DTypeWithConstraints dtype=torch qint scale_min_lower_bound= - bias_dtype=torch float backend_config = BackendConfig \ set_backend_pattern_config BackendPatternConfig torch nn Linear set_observation_type ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT noqa E add_dtype_config dtype_config set_root_module torch nn Linear set_reference_quantized_module nnqr Linear validate_qconfig qconfig QConfig satisfies_constraints bool _validate_qconfig_against_backend_config_constraints MyModel qconfig backend_config satisfies_constraints Case QConfig min scale value == backend min scale value OK qconfig = QConfig activation=MinMaxObserver with_args dtype=torch quint eps= - weight=MinMaxObserver with_args dtype=torch qint qscheme=torch per_tensor_symmetric eps= - validate_qconfig qconfig satisfies_constraints=True Case QConfig min scale value backend min scale value OK qconfig = QConfig activation=MinMaxObserver with_args dtype=torch quint eps= - weight=MinMaxObserver with_args dtype=torch qint qscheme=torch per_tensor_symmetric eps= - validate_qconfig qconfig satisfies_constraints=True Case QConfig activation min scale value backend min scale value should fail qconfig = QConfig activation=MinMaxObserver with_args dtype=torch quint eps= - weight=MinMaxObserver with_args dtype=torch qint qscheme=torch per_tensor_symmetric validate_qconfig qconfig satisfies_constraints=False Case QConfig weight min scale value backend min scale value should fail qconfig = QConfig activation=MinMaxObserver with_args dtype=torch quint weight=MinMaxObserver with_args dtype=torch qint qscheme=torch per_tensor_symmetric eps= - validate_qconfig qconfig satisfies_constraints=False Case QConfig doesn t specify eps should fail qconfig = QConfig activation=FixedQParamsObserver with_args scale= zero_point= weight=FixedQParamsObserver with_args scale= zero_point= validate_qconfig qconfig satisfies_constraints=False test_qnnpack_backend_config Test whether default QNNPACK QConfigs compatible QNNPACK BackendConfig MyModel torch nn Module __init__ - None super __init__ linear = torch nn Linear float forward x linear x all_qconfigs list tuple QConfig str = get_default_qconfig qnnpack version= default_qnnpack_qconfig_v get_default_qat_qconfig qnnpack version= default_qat_qnnpack_qconfig_v get_default_qat_qconfig qnnpack version= default_qat_qnnpack_qconfig_v default_symmetric_qnnpack_qconfig default_symmetric_qnnpack_qconfig default_symmetric_qnnpack_qat_qconfig default_symmetric_qnnpack_qat_qconfig TODO Test these QConfigs once they fixed see https github com pytorch pytorch issues default_per_channel_symmetric_qnnpack_qconfig default_per_channel_symmetric_qnnpack_qconfig default_per_channel_symmetric_qnnpack_qat_qconfig default_per_channel_symmetric_qnnpack_qat_qconfig backend_config = get_qnnpack_backend_config qconfig qconfig_name all_qconfigs _validate_qconfig_against_backend_config_constraints MyModel qconfig backend_config satisfies_constraints=True qconfig_name=qconfig_name test_symmetric_qnnpack_qconfig_mapping Test whether ` torch ao quantization qconfig_mapping _get_symmetric_qnnpack_qconfig_mapping ` works QNNPACK BackendConfig qnnpack supported_qengines MyModel torch nn Module __init__ - None super __init__ linear = torch nn Linear float forward x linear x override_quantized_engine qnnpack qconfig_mapping = _get_symmetric_qnnpack_qconfig_mapping example_inputs = torch rand dtype=torch float backend_config = get_qnnpack_backend_config model = MyModel model = prepare_fx model qconfig_mapping example_inputs backend_config=backend_config model example_inputs model = convert_fx model backend_config=backend_config expected_node_occurrence = ns call_module torch ao nn quantized Linear ns call_module torch nn Linear checkGraphModuleNodes model expected_node_occurrence=expected_node_occurrence model example_inputs test_symmetric_qnnpack_qat_qconfig_mapping Test whether ` torch ao quantization qconfig_mapping _get_symmetric_qnnpack_qat_qconfig_mapping ` works QNNPACK BackendConfig qnnpack supported_qengines MyModel torch nn Module __init__ - None super __init__ linear = torch nn Linear float forward x linear x override_quantized_engine qnnpack qconfig_mapping = _get_symmetric_qnnpack_qat_qconfig_mapping example_inputs = torch rand dtype=torch float backend_config = get_qnnpack_backend_config model = MyModel model = prepare_fx model qconfig_mapping example_inputs backend_config=backend_config model example_inputs model = convert_fx model backend_config=backend_config expected_node_occurrence = ns call_module torch ao nn quantized Linear ns call_module torch nn Linear checkGraphModuleNodes model expected_node_occurrence=expected_node_occurrence model example_inputs test_get_executorch_backend_config torch ao quantization backend_config get_executorch_backend_config make sure runs executorch_backend_config = get_executorch_backend_config test_backend_config_check_for_weight_and_bias Test make sure backend_config check weight bias runs when qconfig None ops weight bias previously error hit because we first check input check weight bias skipped M torch nn Module __init__ - None super __init__ weight = torch tensor bias = torch tensor forward x torch addmm bias x weight m = M eval qconfig_mapping = QConfigMapping observation_type = ObservationType OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT weighted_op_quint _dtype_config = DTypeConfig input_dtype=torch quint output_dtype=torch quint weight_dtype=torch qint bias_dtype=torch float dtype_configs = weighted_op_quint _dtype_config backend_pattern_config = BackendPatternConfig torch addmm \ set_observation_type observation_type \ set_dtype_configs dtype_configs \ _set_input_type_to_index weight bias backend_config = BackendConfig \ set_backend_pattern_config backend_pattern_config example_inputs = torch rand make sure runs m = prepare_fx m qconfig_mapping example_inputs backend_config=backend_config test_get_default_qconfig_valid_backend Checks AssertionError raised when non expected backend input specified invalid_backends = imaginary_backend invalid_backend invalid_backends assertRaisesRegex AssertionError supported qconfig = get_default_qconfig invalid_backend assertRaisesRegex AssertionError supported qconfig = get_default_qat_qconfig invalid_backend assertRaisesRegex AssertionError supported qconfig_mapping = get_default_qconfig_mapping invalid_backend assertRaisesRegex AssertionError supported qconfig_mapping = get_default_qat_qconfig_mapping invalid_backend test__convert_to_reference_decomposed_fx M torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x m = M eval qconfig_mapping = get_default_qconfig_mapping fbgemm example_inputs = torch randn m = prepare_fx m qconfig_mapping example_inputs m_ref = copy deepcopy m m_ref = convert_to_reference_fx m_ref m = _convert_to_reference_decomposed_fx m expected_occurrence = ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default checkGraphModuleNodes m expected_node_occurrence=expected_occurrence make sure runs res_ref = m_ref example_inputs res = m example_inputs assertEqual res res_ref skipIfNoQNNPACK test__convert_to_reference_decomposed_fx_dynamic_quant M torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x avoid reduce_range override_quantized_engine qnnpack m = M eval qconfig_mapping = get_default_qconfig_mapping fbgemm \ set_object_type torch nn Linear default_dynamic_qconfig example_inputs = torch randn m = prepare_fx m qconfig_mapping example_inputs m example_inputs m_ref = copy deepcopy m m_ref = convert_to_reference_fx m_ref m = _convert_to_reference_decomposed_fx m expected_occurrence = ns call_function torch ops quantized_decomposed choose_qparams tensor ns call_function torch ops quantized_decomposed quantize_per_tensor tensor ns call_function torch ops quantized_decomposed dequantize_per_tensor tensor checkGraphModuleNodes m expected_node_occurrence=expected_occurrence make sure runs res_ref = m_ref example_inputs res = m example_inputs assertEqual res res_ref test__convert_to_reference_decomposed_fx_per_channel_quant M torch nn Module forward x weight bias F linear x weight bias m = M eval qconfig_mapping = get_default_qconfig_mapping fbgemm \ set_object_type F linear default_per_channel_qconfig example_inputs = torch randn torch randn torch randn m = prepare_fx m qconfig_mapping example_inputs m example_inputs m_ref = copy deepcopy m m_ref = convert_to_reference_fx m_ref m = _convert_to_reference_decomposed_fx m expected_occurrence = input output activations ns call_function torch ops quantized_decomposed quantize_per_tensor default ns call_function torch ops quantized_decomposed dequantize_per_tensor default weight ns call_function torch ops quantized_decomposed quantize_per_channel default ns call_function torch ops quantized_decomposed dequantize_per_channel default checkGraphModuleNodes m expected_node_occurrence=expected_occurrence make sure runs res_ref = m_ref example_inputs res = m example_inputs assertEqual res res_ref test_change_backend_config_for_fixed_qparam_ops Making sure we can skip validation qconfigs fixedqparam ops based BackendConfig M nn Module __init__ - None super __init__ tanh = torch nn Tanh forward x torch Tensor x = tanh x x model = M eval we set global default_qconfig which will ignored since backend we defined doesn t support anything make sure we don t validate qconfig when BackendConfig does have fixed qparam op related configurations qconfig_mapping = QConfigMapping set_global default_qconfig backend_config = BackendConfig make sure runs model = prepare_fx model qconfig_mapping=qconfig_mapping example_inputs= torch randn backend_config=backend_config test_channel_shuffle_lowering Three versions channel shuffle M torch nn Module __init__ - None super __init__ op = torch nn ChannelShuffle forward x op x + x + x M torch nn Module forward x torch channel_shuffle x + x + x M torch nn Module forward x torch nn functional channel_shuffle x + x + x x = torch randn torch channel_shuffle equivalent torch nn functional channel_shuffle model_node_pairs = M eval ns call_module torch nn ChannelShuffle M eval ns call_function torch channel_shuffle M eval ns call_function torch channel_shuffle m node model_node_pairs m = prepare_fx m default_qconfig example_inputs= x m_copy = copy deepcopy m m = convert_fx m m_ref = convert_to_reference_fx m_copy node_occurrence = node ns call_function torch quantize_per_tensor ns call_method dequantize node_occurrence_ref = node ns call_function torch quantize_per_tensor ns call_method dequantize checkGraphModuleNodes m expected_node_occurrence=node_occurrence checkGraphModuleNodes m_ref expected_node_occurrence=node_occurrence_ref test_match_pattern_with_multiple_args Test we can match pattern has multiple arguments Pattern shape \ transpose observed - reshape - output observed - where ` reshape ` has two arguments _get_pattern_configs backend_pattern_configs = observation_type = ObservationType OUTPUT_SHARE_OBSERVER_WITH_INPUT weighted_op_quint _dtype_config = DTypeConfig input_dtype=torch quint output_dtype=torch quint weight_dtype=torch qint bias_dtype=torch float dtype_configs = weighted_op_quint _dtype_config root_node_getter node_pattern reshape transpose shape = node_pattern transpose backend_pattern_configs append BackendPatternConfig _set_pattern_complex_format torch reshape torch transpose MatchAllNode noqa E set_observation_type observation_type set_dtype_configs dtype_configs _set_root_node_getter root_node_getter backend_pattern_configs backend_config = BackendConfig set_backend_pattern_configs _get_pattern_configs M torch nn Module forward x x = torch transpose x x = torch reshape x - x m = M eval qconfig_mapping = QConfigMapping set_global default_qconfig example_inputs = torch randn m = prepare_fx m qconfig_mapping example_inputs backend_config=backend_config node_occurrence = one input pattern one output pattern ns call_module MinMaxObserver checkGraphModuleNodes m expected_node_occurrence=node_occurrence _test_linear_activation_fusion_lowering_helper module example_inputs qconfig_mapping backend_config fused_module root_module activation_module node_occurrence = ns call_function torch quantize_per_tensor ns call_method dequantize ns call_module fused_module ns call_module root_module ns call_module activation_module node_occurrence_ref = ns call_function torch quantize_per_tensor ns call_method dequantize m = module eval m = prepare_fx m qconfig_mapping example_inputs=example_inputs backend_config=backend_config m_copy = copy deepcopy m m = convert_fx m backend_config=backend_config m_ref = convert_to_reference_fx m_copy checkGraphModuleNodes m expected_node_occurrence=node_occurrence checkGraphModuleNodes m_ref expected_node_occurrence=node_occurrence_ref m example_inputs skipIfNoONEDNN test_linear_leaky_relu_lowering Test fusion lowering Linear - bn - LeakyReLU FX For onednn backedn only torch ao quantization backend_config get_onednn_backend_config qconfig_mapping = get_default_qconfig_mapping onednn override_quantized_engine onednn with_bn True False m = LinearBnLeakyReluModel with_bn _test_linear_activation_fusion_lowering_helper m m get_example_inputs qconfig_mapping get_onednn_backend_config nniq LinearLeakyReLU nn Linear nn LeakyReLU skipIfNoONEDNN test_linear_tanh_lowering Test fusion lowering Linear - Tanh FX For onednn backedn only torch ao quantization backend_config get_onednn_backend_config qconfig_mapping = get_default_qconfig_mapping onednn TODO Currently s required separate ops fused op module have same qconfig Need able support fusion ops different qconfigs Since tanh must have fixed_qparams_qconfig while linear should use global qconfig we need set qconfigs them manually here fusion cannot put such configs onednn s default qconfig_mapping Known issue Cannot fuse linear - tanh quantize standalone tanh same time qconfig = get_default_qconfig onednn qconfig_mapping set_object_type torch nn Linear qconfig qconfig_mapping set_object_type torch nn Tanh qconfig override_quantized_engine onednn m = LinearTanhModel _test_linear_activation_fusion_lowering_helper m m get_example_inputs qconfig_mapping get_onednn_backend_config nniq LinearTanh nn Linear nn Tanh override_qengines test_linear_size_view M torch nn Module __init__ use_relu=False super __init__ linear = torch nn Linear relu = torch nn ReLU use_relu = use_relu forward x x = linear x use_relu x = relu x x view x size use_relu False True model_fp = M use_relu eval qengine = torch backends quantized engine qconfig_mapping = get_default_qconfig_mapping qengine x = torch randn model_fp x prepared_model = prepare_fx model_fp qconfig_mapping x prepared_model x quantized_model = convert_fx prepared_model node_occurrence = ns call_module nnq Linear use_relu ns call_module nniq LinearReLU use_relu ns call_function torch quantize_per_tensor ns call_method dequantize checkGraphModuleNodes quantized_model expected_node_occurrence=node_occurrence override_qengines test_linear_shape_view M torch nn Module __init__ use_relu=False super __init__ linear = torch nn Linear relu = torch nn ReLU use_relu = use_relu forward x x = linear x use_relu x = relu x x view x shape use_relu False True model_fp = M use_relu eval qengine = torch backends quantized engine qconfig_mapping = get_default_qconfig_mapping qengine x = torch randn model_fp x prepared_model = prepare_fx model_fp qconfig_mapping x prepared_model x quantized_model = convert_fx prepared_model node_occurrence = ns call_module nnq Linear use_relu ns call_module nniq LinearReLU use_relu ns call_function torch quantize_per_tensor ns call_method dequantize checkGraphModuleNodes quantized_model expected_node_occurrence=node_occurrence test_mixed_dtypes Test multiple dtypes can used same model different layers dtypes will converted correctly between layers MyModule torch nn Module __init__ - None super __init__ linear = torch nn Linear linear = torch nn Linear sigmoid = torch nn Sigmoid tanh = torch nn Tanh float_functional = torch ao nn quantized FloatFunctional forward x torch Tensor x = linear x qint x = linear x quint linear = x x = sigmoid x back qint x = tanh x back quint x = float_functional add linear x adding two quint s together x make_qconfig scale zp dtype QConfig activation=FixedQParamsObserver with_args scale=scale zero_point=zp dtype=dtype weight=torch ao quantization default_weight_observer Set up QConfigMapping specifies different qparams dtypes different layers qconfig_mapping = QConfigMapping \ set_global get_default_qconfig qnnpack \ set_module_name linear make_qconfig torch qint \ set_module_name linear make_qconfig torch quint \ set_object_type torch nn Sigmoid make_qconfig torch qint \ set_object_type torch nn Tanh make_qconfig torch quint Set up BackendConfig supports dtypes configured above QConfigMapping weighted_op_qint _dtype_config = DTypeConfig input_dtype=torch qint output_dtype=torch qint weight_dtype=torch qint bias_dtype=torch float fixed_qparams_op_quint _dtype_config = DTypeConfig input_dtype=torch quint output_dtype=torch quint fixed_qparams_op_qint _dtype_config = DTypeConfig input_dtype=torch qint output_dtype=torch qint backend_config = get_qnnpack_backend_config config backend_config configs config pattern == torch nn Linear config add_dtype_config weighted_op_qint _dtype_config config pattern torch nn Sigmoid torch nn Tanh config add_dtype_config fixed_qparams_op_quint _dtype_config config add_dtype_config fixed_qparams_op_qint _dtype_config Produce reference quantized model m = MyModule example_inputs = torch rand prepared = prepare_fx m qconfig_mapping example_inputs backend_config=backend_config prepared example_inputs calibrate converted = convert_to_reference_fx prepared backend_config=backend_config converted example_inputs Verify reference model correct Reference model until add should fp _input - q_to_int - dq - linear _fp - q_to_int - dq - q_to_uint - dq - linear _fp - q_to_uint - dq linear _dq - q_to_int - dq - sigmoid_fp - q_to_int - dq - q_to_uint - dq - tanh_fp - q_to_uint - dq tanh_dq Complete reference model add should linear _dq tanh_dq - add_fp - q_to_uint - dq - fp _output target_to_expected_dtypes = linear torch qint linear torch quint sigmoid torch qint tanh torch quint torch add torch quint Find patterns dq - op_fp - q_to_specific_dtype graph linear _node = tanh_node = None node converted graph nodes node target target_to_expected_dtypes continue Match preceding dequantize assertTrue len node args == len node args == assertTrue all arg target == dequantize arg node args Match following quantize specific dtypes assertEqual len node users user = next iter node users keys assertEqual user target torch quantize_per_tensor assertEqual user args - target_to_expected_dtypes node target Match dq - torch add linear _dq tanh_dq - q node target == linear linear _node = node node target == tanh tanh_node = node node target == torch add linear _dq tanh_dq = node args assertEqual tanh_dq args args tanh_node assertEqual linear _dq args args linear _node test_lowering_functional_conv_with_kwargs dim_to_op = F conv d F conv d F conv d dim_to_qop = torch ops quantized conv d torch ops quantized conv d torch ops quantized conv d Mod nn Module __init__ in_channels out_channels kernel_size dimension super __init__ dim = dimension op = dim_to_op dimension kernel_sizes = kernel_size dim weight = nn Parameter torch randn out_channels in_channels kernel_sizes forward input op input weight bias=None stride= dim padding= dim dilation= dim groups= dimension model = Mod dimension model eval qconfig_mapping = get_default_qconfig_mapping input_shape = dimension example_inputs = torch randn input_shape prepared_model = prepare_fx model qconfig_mapping example_inputs prepared_model example_inputs quantized_model = convert_fx prepared_model This should pass quantized_model example_inputs Ensure quantized model has expected op node_occurrence = ns call_function dim_to_qop dimension checkGraphModuleNodes quantized_model expected_node_occurrence=node_occurrence test_lowering_functional_conv_transpose_with_kwargs dim_to_op = F conv_transpose d F conv_transpose d F conv_transpose d dim_to_qop = torch ops quantized conv_transpose d torch ops quantized conv_transpose d torch ops quantized conv_transpose d Mod nn Module __init__ in_channels out_channels kernel_size dimension super __init__ dim = dimension op = dim_to_op dimension kernel_sizes = kernel_size dim weight = nn Parameter torch randn in_channels out_channels kernel_sizes forward input op input weight bias=None stride= dim padding= dim output_padding= dim dilation= dim groups= dimension model = Mod dimension model eval qconfig_mapping = get_default_qconfig_mapping input_shape = dimension example_inputs = torch randn input_shape prepared_model = prepare_fx model qconfig_mapping example_inputs prepared_model example_inputs quantized_model = convert_fx prepared_model This should pass quantized_model example_inputs Ensure quantized model has expected op node_occurrence = ns call_function dim_to_qop dimension checkGraphModuleNodes quantized_model expected_node_occurrence=node_occurrence test_lowering_functional_linear_with_kwargs Mod nn Module __init__ in_channels out_channels super __init__ weight = nn Parameter torch randn out_channels in_channels forward input F linear input weight bias=None model = Mod model eval qconfig_mapping = get_default_qconfig_mapping example_inputs = torch randn prepared_model = prepare_fx model qconfig_mapping example_inputs prepared_model example_inputs quantized_model = convert_fx prepared_model This should pass quantized_model example_inputs Ensure quantized model has expected op node_occurrence = ns call_function torch ops quantized linear checkGraphModuleNodes quantized_model expected_node_occurrence=node_occurrence skipIfNoFBGEMM test_keep_original_weights SubModule nn Module A simple submodule containing linear layer __init__ input_dim output_dim super __init__ w = nn Parameter torch randn input_dim output_dim b = nn Parameter torch randn input_dim forward x F linear x w b MainModule nn Module The main module containing submodule __init__ input_dim hidden_dim output_dim super __init__ submodule_ = SubModule hidden_dim input_dim setattr submodule &#124; SubModule hidden_dim hidden_dim setattr submodule SubModule hidden_dim hidden_dim setattr submodule SubModule hidden_dim hidden_dim setattr submodule SubModule hidden_dim hidden_dim _w = nn Parameter torch randn output_dim hidden_dim forward x x = submodule_ x x = getattr submodule &#124; x x = getattr submodule x x = getattr submodule x x = getattr submodule x x = F linear x _w x input_dim = hidden_dim = output_dim = model = MainModule input_dim hidden_dim output_dim model eval example_inputs = torch randn input_dim _ = model example_inputs qconfig_mapping = QConfigMapping set_object_type nn functional linear float _dynamic_qconfig prepared_model = prepare_fx model qconfig_mapping example_inputs prepared_model example_inputs quantized_model = convert_fx prepared_model keep_original_weights=True assertTrue len quantized_model original_weights_lookup == assertTrue submodule_ _packed_weight_ quantized_model original_weights_lookup torch testing assert_close quantized_model original_weights_lookup submodule_ _packed_weight_ model submodule_ w torch testing assert_close quantized_model original_weights_lookup submodule_ _packed_weight_ model submodule_ b assertTrue submodule_ _packed_weight_ quantized_model original_weights_lookup torch testing assert_close quantized_model original_weights_lookup submodule_ _packed_weight_ getattr model submodule &#124; w torch testing assert_close quantized_model original_weights_lookup submodule_ _packed_weight_ getattr model submodule &#124; b assertTrue submodule_ _packed_weight_ quantized_model original_weights_lookup torch testing assert_close quantized_model original_weights_lookup submodule_ _packed_weight_ getattr model submodule w torch testing assert_close quantized_model original_weights_lookup submodule_ _packed_weight_ getattr model submodule b assertTrue submodule_ _packed_weight_ quantized_model original_weights_lookup torch testing assert_close quantized_model original_weights_lookup submodule_ _packed_weight_ getattr model submodule w torch testing assert_close quantized_model original_weights_lookup submodule_ _packed_weight_ getattr model submodule b assertTrue submodule_ _packed_weight_ quantized_model original_weights_lookup torch testing assert_close quantized_model original_weights_lookup submodule_ _packed_weight_ getattr model submodule w torch testing assert_close quantized_model original_weights_lookup submodule_ _packed_weight_ getattr model submodule b assertTrue _packed_weight_ quantized_model original_weights_lookup torch testing assert_close quantized_model original_weights_lookup _packed_weight_ model _w torch testing assert_close quantized_model original_weights_lookup _packed_weight_ None skipIfNoFBGEMM TestQuantizeFxOps QuantizationTestCase setUp super setUp custom_qconfig = torch ao quantization QConfig activation=torch ao quantization observer HistogramObserver with_args qscheme=torch per_tensor_symmetric dtype=torch qint weight=torch ao quantization default_per_channel_weight_observer common_quant_patterns = torch nn ConvTranspose d DefaultNodeQuantizeHandler torch nn ConvTranspose d DefaultNodeQuantizeHandler torch nn ELU DefaultNodeQuantizeHandler torch nn LeakyReLU DefaultNodeQuantizeHandler torch nn Hardswish DefaultNodeQuantizeHandler torch nn InstanceNorm d DefaultNodeQuantizeHandler torch nn InstanceNorm d DefaultNodeQuantizeHandler torch nn InstanceNorm d DefaultNodeQuantizeHandler torch nn LayerNorm DefaultNodeQuantizeHandler torch nn SiLU DefaultNodeQuantizeHandler torch nn Mish DefaultNodeQuantizeHandler torch nn GELU DefaultNodeQuantizeHandler torch nn Softmax DefaultNodeQuantizeHandler torch nn functional elu DefaultNodeQuantizeHandler torch nn functional hardswish DefaultNodeQuantizeHandler torch nn functional instance_norm DefaultNodeQuantizeHandler torch nn functional layer_norm DefaultNodeQuantizeHandler torch nn functional leaky_relu DefaultNodeQuantizeHandler torch nn functional silu DefaultNodeQuantizeHandler torch nn functional mish DefaultNodeQuantizeHandler torch nn functional gelu DefaultNodeQuantizeHandler torch nn functional softmax DefaultNodeQuantizeHandler torch sum DefaultNodeQuantizeHandler Unit tests individual ops skipIfNoFBGEMM test_linear_module override_quantized_engine fbgemm LinearModel torch nn Module __init__ - None super __init__ linear = torch nn Linear float forward x linear x LinearReLUModel torch nn Module __init__ f_relu=False super __init__ linear = torch nn Linear float f_relu relu = F relu relu = torch nn ReLU forward x x = linear x x = relu x x LinearBnModel torch nn Module __init__ - None super __init__ linear = torch nn Linear float bn = torch nn BatchNorm d forward x x = linear x x = bn x x Test linear data = torch rand dtype=torch float quant_type all_quant_types model = LinearModel quantized_module = nnqd Linear quant_type == QuantType DYNAMIC nnq Linear quantized_node = ns call_module quantized_module result_dict = checkGraphModeFxOp model data quant_type quantized_node quant_type static_quant_types assertEqual result_dict quantized_output result_dict quantized_reference_output TODO enable test dynamic quant Test linear-relu f_relu quant_type itertools product True False QuantType STATIC QuantType QAT model = LinearReLUModel f_relu quantized_node = ns call_module nniq LinearReLU result_dict = checkGraphModeFxOp model data quant_type quantized_node assertEqual result_dict quantized_output result_dict quantized_reference_output Test linear-bn data = torch rand dtype=torch float quant_type static_quant_types model = LinearBnModel quantized_node = ns call_module nnq Linear result_dict = checkGraphModeFxOp model data quant_type quantized_node assertEqual result_dict quantized_output result_dict quantized_reference_output skipIfNoFBGEMM test_functional_linear override_quantized_engine fbgemm FuncLinear torch nn Module __init__ use_bias has_relu f_relu super __init__ w = torch randn b = torch randn use_bias = use_bias has_relu f_relu relu_or_id = F relu relu_or_id = torch nn ReLU relu_or_id = torch nn Identity forward x use_bias x = F linear x w b x = F linear x w x = relu_or_id x x data = torch rand dtype=torch float quant_type_to_qlinear_fun = QuantType DYNAMIC ns call_function torch ops quantized linear_dynamic QuantType STATIC ns call_function torch ops quantized linear QuantType QAT ns call_function torch ops quantized linear quant_type_to_qlinear_relu_fun = we don t have linear_relu_dynamic QuantType DYNAMIC ns call_function torch ops quantized linear_relu_dynamic QuantType STATIC ns call_function torch ops quantized linear_relu QuantType QAT ns call_function torch ops quantized linear_relu options = itertools product all_quant_types True False use_bias True False has_relu True False functional relu quant_type use_bias has_relu f_relu options when has_relu False we using nn Identity we will insert observer fake_quant output nn Identity since copy node s why we have extra observer fake_quant when has_relu False quant_type_to_prepare_expected_node_occurrence = QuantType DYNAMIC ns call_module torch ao quantization PlaceholderObserver ns call_module torch ao quantization MinMaxObserver There should observers after input weight activation one more observer torch nn Identity when there no relu QuantType STATIC ns call_module torch ao quantization HistogramObserver has_relu ns call_module torch ao quantization PerChannelMinMaxObserver There should observers after input weight activation QuantType QAT ns call_module torch ao quantization FusedMovingAvgObsFakeQuantize has_relu model = FuncLinear use_bias has_relu f_relu has_relu qlinear_fun = quant_type_to_qlinear_relu_fun quant_type qlinear_fun = quant_type_to_qlinear_fun quant_type quant_type = QuantType DYNAMIC num_dequantize = we will have extra quantize_per_tensor_dynamic + dequantize nn Identity right now will fixed after we use backend_config configure default pt backend num_dequantize = int has_relu convert_node_occurrence = ns call_function torch quantize_per_tensor quant_type = QuantType DYNAMIC qlinear_fun ns call_method dequantize num_dequantize quant_type = QuantType DYNAMIC prepare_expected_node_occurrence = \ quant_type_to_prepare_expected_node_occurrence quant_type result_dict = checkGraphModeFxOp model data quant_type qlinear_fun prepare_expected_node_occurrence=prepare_expected_node_occurrence expected_node_occurrence=convert_node_occurrence quant_type = QuantType DYNAMIC assertEqual result_dict quantized_output result_dict quantized_reference_output Ensure packed weights lowered models folded assertIn _packed_weight_ result_dict quantized state_dict keys skipIfNoFBGEMM test_linear_dynamic_fp override_quantized_engine fbgemm FuncLinear torch nn Module __init__ use_bias has_relu f_relu super __init__ w = torch randn b = torch randn use_bias = use_bias has_relu f_relu relu = F relu relu = torch nn ReLU relu = torch nn Identity forward x use_bias x = F linear x w b x = F linear x w x = relu x x data = torch rand dtype=torch float options = itertools product True False use_bias True False has_relu True False functional relu True False is_reference use_bias has_relu f_relu is_reference options model = FuncLinear use_bias has_relu f_relu is_reference qlinear_fun = ns call_function torch nn functional linear has_relu qlinear_fun = ns call_function torch ops quantized linear_relu_dynamic_fp qlinear_fun = ns call_function torch ops quantized linear_dynamic_fp prepare_node_occurrence = activation weight ns call_module torch ao quantization PlaceholderObserver convert_node_occurrence = qlinear_fun weight ns call_method is_reference checkGraphModeFxOp model data QuantType DYNAMIC qlinear_fun is_reference=is_reference custom_qconfig_dict= float _dynamic_qconfig prepare_expected_node_occurrence=prepare_node_occurrence expected_node_occurrence=convert_node_occurrence test_linear_static_fp FuncLinear torch nn Module __init__ use_bias has_relu f_relu super __init__ w = torch randn b = torch randn use_bias = use_bias has_relu f_relu relu = F relu relu = torch nn ReLU relu = torch nn Identity forward x use_bias x = F linear x w b x = F linear x w x = relu x x data = torch rand dtype=torch float options = itertools product True False use_bias True False has_relu True False functional relu True False is_reference backend_config = get_test_only_legacy_native_backend_config use_bias has_relu f_relu is_reference options model = FuncLinear use_bias has_relu f_relu linear_fun = ns call_function torch nn functional linear when has_relu False we using nn Identity we will insert observer fake_quant output nn Identity since copy node s why we have extra observer fake_quant when has_relu False prepare_node_occurrence = activation weight bias output ns call_module torch ao quantization PlaceholderObserver + int use_bias + int has_relu We have extra dequantize when is_reference True has_relu False since when has_relu False we have nn Identity model which CopyNode we would add extra quant - dequant CopyNode reference patterns convert_node_occurrence = we don t support static fp ops so linear function unfused linear_fun activation weight bias output ns call_method + int use_bias + int has_relu is_reference ns call_method dequantize + int use_bias + int has_relu is_reference checkGraphModeFxOp model data QuantType DYNAMIC linear_fun is_reference=is_reference custom_qconfig_dict= float _static_qconfig prepare_expected_node_occurrence=prepare_node_occurrence expected_node_occurrence=convert_node_occurrence backend_config=backend_config skipIfNoFBGEMM test_conv_module conv_module = torch nn Conv d torch nn Conv d torch nn Conv d ConvWrapper torch nn Module __init__ dim super __init__ conv = conv_module dim float forward x conv x options = itertools product static_quant_types quantized_nodes = dim ns call_module nnq Conv d ns call_module nnq Conv d ns call_module nnq Conv d dim quant_type options checkGraphModeFxOp ConvWrapper dim img_data_dict dim quant_type quantized_nodes dim skipIfNoFBGEMM test_functional_conv override_quantized_engine fbgemm Test function conv functional conv + relu convs = torch nn functional conv d torch nn functional conv d torch nn functional conv d FuncConv torch nn Module __init__ dim use_bias has_relu f_relu super __init__ dim = dim w = torch randn tuple dim + b = torch randn use_bias None stride = tuple dim padding = tuple dim dilation = tuple dim groups = use_bias = use_bias has_relu f_relu relu = F relu relu = torch nn ReLU relu = torch nn Identity forward x x = convs dim x w b stride padding dilation groups x = relu x x quant_type_to_qconv_fun = QuantType STATIC ns call_function torch ops quantized conv d ns call_function torch ops quantized conv d ns call_function torch ops quantized conv d QuantType QAT ns call_function torch ops quantized conv d ns call_function torch ops quantized conv d ns call_function torch ops quantized conv d quant_type_to_qconv_relu_fun = QuantType STATIC ns call_function torch ops quantized conv d_relu ns call_function torch ops quantized conv d_relu ns call_function torch ops quantized conv d_relu QuantType QAT ns call_function torch ops quantized conv d_relu ns call_function torch ops quantized conv d_relu ns call_function torch ops quantized conv d_relu options = itertools product dims static_quant_types True False use_bias True False has_relu True False functional relu dim quant_type use_bias has_relu f_relu options when has_relu False we using nn Identity we will insert observer fake_quant output nn Identity since copy node s why we have extra observer fake_quant when has_relu False quant_type_to_prepare_expected_node_occurrence = QuantType DYNAMIC There should observers after input weight activation QuantType STATIC ns call_module torch ao quantization HistogramObserver has_relu ns call_module torch ao quantization PerChannelMinMaxObserver There should observers after input weight activation QuantType QAT ns call_module torch ao quantization FusedMovingAvgObsFakeQuantize has_relu data_dims = + dim data = torch randn tuple data_dims dtype=torch float model = FuncConv dim use_bias has_relu f_relu has_relu qconv_fun = quant_type_to_qconv_relu_fun quant_type dim qconv_fun = quant_type_to_qconv_fun quant_type dim convert_node_occurrence = ns call_function torch quantize_per_tensor qconv_fun ns call_method dequantize prepare_expected_node_occurrence = \ quant_type_to_prepare_expected_node_occurrence quant_type result_dict = checkGraphModeFxOp model data quant_type qconv_fun prepare_expected_node_occurrence=prepare_expected_node_occurrence expected_node_occurrence=convert_node_occurrence quant_type = QuantType DYNAMIC assertEqual result_dict quantized_output result_dict quantized_reference_output Ensure packed weights lowered models folded assertIn _packed_weight_ result_dict quantized state_dict keys skipIfNoFBGEMM test_quantized_conv_relu tests conv d_relu conv d_relu conv d_relu conv_module = torch nn Conv d torch nn Conv d torch nn Conv d ConvNdRelu torch nn Module __init__ dim inplace super __init__ conv = conv_module dim float relu = torch nn ReLU inplace forward x relu conv x ConvNdFunctionalRelu torch nn Module __init__ dim super __init__ conv = conv_module dim float forward x F relu conv x ConvNdInplaceFunctionalRelu torch nn Module __init__ dim super __init__ conv = conv_module dim float forward x F relu conv x True options = itertools product static_quant_types quantized_nodes = dim ns call_module nniq ConvReLU d ns call_module nniq ConvReLU d ns call_module nniq ConvReLU d dim quant_type options m ConvNdRelu dim True ConvNdRelu dim False ConvNdFunctionalRelu dim ConvNdInplaceFunctionalRelu dim checkGraphModeFxOp m img_data_dict dim quant_type quantized_nodes dim _test_binary_op_int _impl binary_op ibinary_op quantized_op data = torch randn dtype=torch float torch randn dtype=torch float options = itertools product True False True False True False quant_type = QuantType STATIC testing default int static quant is_inplace is_scalar is_reference options is_reference node_list = ns call_method dequantize ns call_function binary_op ns call_function torch quantize_per_tensor quantized_node = None node_list = None quantized_node = ns call_function quantized_op checkGraphModeFxOp BinaryOp binary_op ibinary_op is_inplace is_scalar data quant_type quantized_node expected_node_list=node_list is_reference=is_reference This tests binary op should quantized even when feed quantized input checkGraphModeFxOp BinaryOpNonQuantizedInput binary_op ibinary_op is_inplace is_scalar data quant_type quantized_node expected_node_list=node_list is_reference=is_reference _test_binary_op_float _impl binary_op ibinary_op data = torch randn dtype=torch float torch randn dtype=torch float quant_type = QuantType STATIC testing fp static quant we producing fp patterns options = itertools product True False True False custom_qconfig_dict = object_type binary_op float _static_qconfig backend_config = get_test_only_legacy_native_backend_config is_inplace is_scalar options node_occurrence = output_conv output_add output_add scalar output_conv output_conv output_add output_add non-scalar ns call_method is_scalar checkGraphModeFxOp BinaryOp binary_op ibinary_op is_inplace is_scalar data quant_type expected_node_occurrence=node_occurrence custom_qconfig_dict=custom_qconfig_dict backend_config=backend_config node_occurrence = input_add output_add scalar input_add input_add output_add non-scalar ns call_method is_scalar checkGraphModeFxOp BinaryOpNonQuantizedInput binary_op ibinary_op is_inplace is_scalar data quant_type expected_node_occurrence=node_occurrence custom_qconfig_dict=custom_qconfig_dict backend_config=backend_config _test_binary_op_relu_int _impl binary_op ibinary_op quantized_op data = torch rand dtype=torch float torch rand dtype=torch float quant_type = QuantType STATIC quantized_node = ns call_function quantized_op options = itertools product True False nn ReLU F relu torch relu True False is_inplace_op relu_callable is_scalar options model = BinaryOpRelu binary_op ibinary_op is_inplace_op relu_callable is_scalar checkGraphModeFxOp model data quant_type quantized_node _test_binary_op_relu_float _impl binary_op ibinary_op data = torch rand dtype=torch float torch rand dtype=torch float quant_type = QuantType STATIC options = itertools product True False nn ReLU F relu torch relu True False custom_qconfig_dict = float _static_qconfig object_type torch nn Conv d None backend_config = get_test_only_legacy_native_backend_config is_inplace_op is_functional_relu is_scalar options node_occurrence = ns call_method is_scalar model = BinaryOpRelu binary_op ibinary_op is_inplace_op is_functional_relu is_scalar checkGraphModeFxOp model data quant_type custom_qconfig_dict=custom_qconfig_dict expected_node_occurrence=node_occurrence backend_config=backend_config skipIfNoFBGEMM test_add _test_binary_op_int _impl operator add operator iadd torch ops quantized add _test_binary_op_float _impl operator add operator iadd unittest skip This no longer needed right now can enable later new api test_sub _test_binary_op_float _impl operator sub operator isub _test_binary_op_float _impl torch sub None unittest skip This no longer needed right now can enable later new api test_div _test_binary_op_float _impl operator truediv operator itruediv _test_binary_op_float _impl torch div None skipIfNoFBGEMM test_mul _test_binary_op_int _impl operator mul operator imul torch ops quantized mul _test_binary_op_float _impl operator mul operator imul unittest skip This no longer needed right now can enable later new api test_sum Sum torch nn Module forward x x = torch sum x keepdim=True x = torch sum x x data = torch randn dtype=torch float quant_type = QuantType STATIC testing fp static quant we producing fp patterns custom_qconfig_dict = object_type torch sum float _static_qconfig node_occurrence = input_sum output_sum output_sum ns call_method checkGraphModeFxOp Sum data quant_type expected_node_occurrence=node_occurrence custom_qconfig_dict=custom_qconfig_dict unittest skip This no longer needed right now can enable later new api test_bmm BMMMethod torch nn Module forward x y x bmm y data = torch randn dtype=torch float torch randn dtype=torch float quant_type = QuantType STATIC testing fp static quant we producing fp patterns custom_qconfig_dict = object_type torch bmm float _static_qconfig bmm float _static_qconfig node_occurrence = input_bmm input_bmm output_bmm ns call_method checkGraphModeFxOp BinaryOpNonQuantizedInput torch bmm None False False data quant_type expected_node_occurrence=node_occurrence custom_qconfig_dict=custom_qconfig_dict TODO support call_method bmm we can transform call_method bmm call_function torch bmm checkGraphModeFxOp BMMMethod data quant_type expected_node_occurrence=node_occurrence custom_qconfig_dict=custom_qconfig_dict print_debug_info=True skipIfNoFBGEMM test_add_relu _test_binary_op_relu_int _impl operator add operator iadd torch ops quantized add_relu _test_binary_op_relu_float _impl operator add operator iadd skipIfNoFBGEMM test_add_relu_multiple_uses_of_relu Sub torch nn Module __init__ - None super __init__ relu = torch nn ReLU inplace=True M torch nn Module __init__ - None super __init__ sub = Sub forward x y x = x + y x = sub relu x x = x + y x = sub relu x x m = M eval example_inputs = torch randn torch randn m = prepare_fx m default_qconfig example_inputs=example_inputs m = convert_fx m node_occurrence = ns call_function torch quantize_per_tensor ns call_function torch ops quantized add_relu ns call_method dequantize checkGraphModuleNodes m expected_node_occurrence=node_occurrence check model scriptable m = torch jit script m check model runnable m example_inputs skipIfNoFBGEMM test_mul_relu _test_binary_op_relu_int _impl operator mul operator imul torch ops quantized mul_relu _test_binary_op_relu_float _impl operator mul operator imul TODO future PR make more generic _test_quantized_add_mul_qat model example_inputs expected_node_occurrence qconfig_dict = torch ao quantization get_default_qat_qconfig fbgemm mp = prepare_qat_fx model qconfig_dict example_inputs=example_inputs checkGraphModuleNodes mp expected_node_occurrence=expected_node_occurrence skipIfNoFBGEMM test_quantized_add_qat M torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv = torch nn Conv d forward x x = torch add x x = conv x x = torch add x x = torch relu x x = conv x x m = M example_inputs = torch randn expected_node_occurrence = ns call_module torch ao quantization FusedMovingAvgObsFakeQuantize _test_quantized_add_mul_qat m example_inputs expected_node_occurrence skipIfNoFBGEMM test_quantized_mul_qat M torch nn Module __init__ - None super __init__ conv = torch nn Conv d conv = torch nn Conv d forward x x = torch mul x x = conv x x = torch mul x x = torch relu x x = conv x x m = M example_inputs = torch randn expected_node_occurrence = ns call_module torch ao quantization FusedMovingAvgObsFakeQuantize _test_quantized_add_mul_qat m example_inputs expected_node_occurrence test_int _input_no_unnecessary_fq If inputs graph quantized only node does need activation observer verifies activation observer inserted M nn Module __init__ scalar super __init__ scalar = scalar add_func = torch ao nn quantized FloatFunctional forward x add_func add_scalar x scalar m = M mp = torch ao quantization quantize_fx prepare_qat_fx m torch ao quantization get_default_qat_qconfig fbgemm example_inputs= torch randn prepare_custom_config= input_quantized_idxs expected_node_occurrence = ns call_module torch ao quantization FusedMovingAvgObsFakeQuantize checkGraphModuleNodes mp expected_node_occurrence=expected_node_occurrence skipIfNoFBGEMM test_cat quantization output cat will depend input cat we only quantize output cat when its inputs quantized M torch nn Module __init__ - None super __init__ conv = torch nn Conv d float conv = torch nn Conv d float forward x y x = conv x y = conv y torch cat x y example_inputs = torch randn dtype=torch float torch randn dtype=torch float quantized_node = ns call_function torch cat options = itertools product static_quant_types True False quant_type is_reference options is_reference converted_node_list = ns call_method dequantize ns call_function torch cat ns call_function torch quantize_per_tensor converted_node_occurrence = inputs outputs two conv output cat ns call_method dequantize ns call_function torch cat inputs outputs two conv output cat ns call_function torch quantize_per_tensor converted_node_list = None converted_node_occurrence = output cat ns call_method dequantize ns call_function torch cat two inputs ns call_function torch quantize_per_tensor checkGraphModeFxOp M example_inputs quant_type quantized_node expected_node_list=converted_node_list expected_node_occurrence=converted_node_occurrence is_reference=is_reference check cat using same observer input output m = M eval m = prepare_fx m default_qconfig example_inputs=example_inputs two inputs one output torch cat using same observer so we have observers s replicated all_observers = len dict m named_modules remove_duplicate=False distinct_observers = len dict m named_modules assertEqual all_observers distinct_observers + make sure converted model runs m = convert_fx m m example_inputs skipIfNoFBGEMM test_qbatch_norm bn_module = TODO quantized batchnorm d module missing torch nn BatchNorm d torch nn BatchNorm d torch nn BatchNorm d M torch nn Module __init__ dim super __init__ bn = bn_module dim torch float forward x bn x options = itertools product static_quant_types True False quantized_nodes = False ns call_module nnq BatchNorm d ns call_module nnq BatchNorm d ns call_module nnq BatchNorm d True ns call_module nn BatchNorm d ns call_module nn BatchNorm d ns call_module nn BatchNorm d quant_type dim is_reference options checkGraphModeFxOp M dim img_data_dict dim quant_type quantized_nodes is_reference dim is_reference=is_reference skipIfNoFBGEMM test_qbatch_norm_relu bn_module = torch nn BatchNorm d torch nn BatchNorm d BNRelu torch nn Module __init__ dim inplace super __init__ bn = bn_module dim torch float relu = torch nn ReLU inplace=inplace forward x relu bn x BNFuncRelu torch nn Module __init__ dim super __init__ bn = bn_module dim torch float forward x F relu bn x False BNFuncInplaceRelu torch nn Module __init__ dim super __init__ bn = bn_module dim torch float forward x F relu bn x True options = itertools product static_quant_types True False quantized_nodes = True ns call_module nni BNReLU d ns call_module nni BNReLU d False ns call_module nniq BNReLU d ns call_module nniq BNReLU d quant_type dim is_reference options instance BNRelu dim True BNRelu dim False BNFuncRelu dim BNFuncInplaceRelu dim checkGraphModeFxOp instance img_data_dict dim quant_type quantized_nodes is_reference dim is_reference=is_reference _test_activation_impl float_module float_op quantized_module quantized_op Test activation op inplace options float_op can torch op functional op M torch nn Module __init__ is_module inplace super __init__ is_module = is_module inplace = inplace is_module op = float_module inplace op = float_op forward input is_module op input op input inplace options = itertools product True False True False static_quant_types True False quantized_nodes = is_module True is_reference True ns call_module float_module False ns call_module quantized_module False True ns call_function float_op False ns call_function quantized_op is_module is_inplace quant_type is_reference options checkGraphModeFxOp M is_module is_inplace img_data_ d quant_type quantized_nodes is_module is_reference is_reference=is_reference test_hardswish _test_activation_impl nn Hardswish F hardswish nnq Hardswish torch ops quantized hardswish test_elu _test_activation_impl nn ELU F elu nnq ELU torch ops quantized elu test_leaky_relu _test_activation_impl nn LeakyReLU F leaky_relu nnq LeakyReLU torch ops quantized leaky_relu test_prelu M torch nn Module __init__ num_param int super __init__ op = torch nn PReLU num_parameters=num_param forward input op input X = torch randn dtype=torch float options = itertools product static_quant_types True False quantized_nodes = is_reference True ns call_module torch nn PReLU False ns call_module torch ao nn quantized PReLU num_parameter quant_type is_reference options checkGraphModeFxOp M num_parameter X quant_type quantized_nodes is_reference is_reference=is_reference _test_norm_impl float_module float_op op_args data quantized_module quantized_op skip_op_arg_for_functional=False Test normalization op float_op can torch op functional op op_args list positional argument module op M torch nn Module __init__ is_module super __init__ is_module = is_module is_module op = float_module op_args op = float_op forward input is_module op input args = input skip_op_arg_for_functional args += op_args op args options = itertools product True False static_quant_types quantized_nodes = is_module True ns call_module quantized_module False ns call_function quantized_op is_module quant_type options checkGraphModeFxOp M is_module data quant_type quantized_nodes is_module _test_norm_float _impl float_module float_op op_args data skip_op_arg_for_functional=False Test normalization op float_op can torch op functional op op_args list positional argument module op M torch nn Module __init__ is_module super __init__ is_module = is_module is_module op = float_module op_args op = float_op forward input is_module op input args = input skip_op_arg_for_functional args += op_args op args options = itertools product True False static_quant_types qconfig_dict = object_type float_module float _static_qconfig float_op float _static_qconfig node_occurrence = ns call_method is_module quant_type options checkGraphModeFxOp M is_module data quant_type custom_qconfig_dict=qconfig_dict expected_node_occurrence=node_occurrence test_layer_norm data = torch rand dtype=torch float _test_norm_impl nn LayerNorm F layer_norm data nnq LayerNorm torch ops quantized layer_norm test_instance_norm data_ d = torch rand dtype=torch float data_ d = torch rand dtype=torch float data_ d = torch rand dtype=torch float data_dict = data_ d data_ d data_ d instance_norm_modules = nn InstanceNorm d nn InstanceNorm d nn InstanceNorm d quantized_instance_norm_modules = nnq InstanceNorm d nnq InstanceNorm d nnq InstanceNorm d dim data = data_dict dim module = instance_norm_modules dim quantized_module = quantized_instance_norm_modules dim _test_norm_impl module F instance_norm data quantized_module torch ops quantized instance_norm skip_op_arg_for_functional=True test_norm_weight_bias Linear torch nn Module __init__ - None super __init__ w = torch ones b = torch zeros forward x torch nn functional linear x w b M torch nn Module __init__ - None super __init__ mods = Linear scale = torch randn bias = torch randn forward x x = mods x y = F layer_norm x weight=self scale bias=self bias y model = M expected_occurrence = ns call_function torch quantize_per_tensor ns call_function torch ops quantized linear ns call_function torch ops quantized layer_norm ns call_method dequantize checkGraphModeFxOp model torch rand QuantType STATIC expected_node_occurrence=expected_occurrence custom_qconfig_dict=get_default_qconfig_mapping to_dict _test_default_node_quant_handler_ops module functional qconfig is_reference=True node_list=None additional_quant_pattern_dict=None M torch nn Module __init__ mod func super __init__ module = mod functional = func forward x x = module x x = functional x x node_list None node_list = additional_quant_pattern_dict None additional_quant_pattern_dict = data = torch randn quant_type = QuantType STATIC prepare_custom_qconfig_dict = additional_quant_pattern additional_quant_pattern_dict qconfig_dict = qconfig m = M module functional eval m_prep = prepare_fx m qconfig_dict prepare_custom_qconfig_dict m_prep data convert_fn = convert_to_reference_fx is_reference convert_fx m_quant = convert_fn m_prep is_reference=is_reference m_quant data checkGraphModuleNodes m_quant expected_node_list=node_list unittest skip TODO reenable backend_config api test_gelu_normal module = torch nn GELU functional = torch nn functional gelu qconfig = torch ao quantization get_default_qconfig fbgemm is_reference = False node_list = ns call_module module ns call_function functional _test_default_node_quant_handler_ops module functional qconfig is_reference node_list unittest skip TODO reenable backend_config api test_softmax_normal module = torch nn Softmax functional = torch nn functional softmax qconfig = torch ao quantization get_default_qconfig fbgemm is_reference = False node_list = ns call_module torch ao nn quantized Softmax ns call_function functional _test_default_node_quant_handler_ops module functional qconfig is_reference node_list unittest skip This no longer needed right now can enable later new api test_gelu_reference module = torch nn GELU functional = torch nn functional gelu qconfig = torch ao quantization get_default_qconfig fbgemm is_reference = True node_list = ns call_function torch quantize_per_tensor ns call_method dequantize ns call_module module ns call_function torch quantize_per_tensor ns call_method dequantize ns call_function functional ns call_function torch quantize_per_tensor ns call_method dequantize TODO change these use backend_config additional_patterns = torch nn GELU DefaultNodeQuantizeHandler torch nn functional gelu DefaultNodeQuantizeHandler _test_default_node_quant_handler_ops module functional qconfig is_reference node_list additional_patterns _test_default_node_quant_handler_ops module functional custom_qconfig is_reference node_list additional_quant_pattern_dict=self common_quant_patterns unittest skip This no longer needed right now can enable later new api test_softmax_reference module = torch nn Softmax functional = torch nn functional softmax qconfig = torch ao quantization get_default_qconfig fbgemm is_reference = True node_list = ns call_function torch quantize_per_tensor ns call_method dequantize ns call_module module ns call_function torch quantize_per_tensor ns call_method dequantize ns call_function functional ns call_function torch quantize_per_tensor ns call_method dequantize additional_patterns = torch nn Softmax DefaultNodeQuantizeHandler torch nn functional softmax DefaultNodeQuantizeHandler _test_default_node_quant_handler_ops module functional qconfig is_reference node_list additional_patterns _test_default_node_quant_handler_ops module functional custom_qconfig is_reference node_list additional_quant_pattern_dict=self common_quant_patterns unittest skip This no longer needed right now can enable later new api test_silu_reference module = torch nn SiLU functional = torch nn functional silu qconfig = float _static_qconfig is_reference = True node_list = ns call_method ns call_method dequantize ns call_module module ns call_method ns call_method dequantize ns call_function functional ns call_method ns call_method dequantize _test_default_node_quant_handler_ops module functional qconfig is_reference node_list node_list = ns call_function torch quantize_per_tensor ns call_method dequantize ns call_module module ns call_function torch quantize_per_tensor ns call_method dequantize ns call_function functional ns call_function torch quantize_per_tensor ns call_method dequantize _test_default_node_quant_handler_ops module functional custom_qconfig is_reference node_list additional_quant_pattern_dict=self common_quant_patterns unittest skip This no longer needed right now can enable later new api test_mish_reference module = torch nn Mish functional = torch nn functional mish qconfig = float _static_qconfig is_reference = True node_list = ns call_method ns call_method dequantize ns call_module module ns call_method ns call_method dequantize ns call_function functional ns call_method ns call_method dequantize _test_default_node_quant_handler_ops module functional qconfig is_reference node_list node_list = ns call_function torch quantize_per_tensor ns call_method dequantize ns call_module module ns call_function torch quantize_per_tensor ns call_method dequantize ns call_function functional ns call_function torch quantize_per_tensor ns call_method dequantize _test_default_node_quant_handler_ops module functional custom_qconfig is_reference node_list additional_quant_pattern_dict=self common_quant_patterns test_bmm_int_reference int supported bmm so we won t produce reference pattern M torch nn Module __init__ - None super __init__ bmm = torch bmm forward x y out = bmm x y out data_x = torch randn data_y = torch randn example_inputs = data_x data_y qconfig_dict = torch ao quantization get_default_qconfig fbgemm is_reference = True node_list = ns call_function torch bmm m = M eval m_prep = prepare_fx m qconfig_dict example_inputs=example_inputs m_prep example_inputs convert_fn = convert_to_reference_fx is_reference convert_fx m_quant = convert_fn m_prep m_quant example_inputs checkGraphModuleNodes m_quant expected_node_list=node_list skipIfNoFBGEMM test_clamp M torch nn Module __init__ - None super __init__ conv = torch nn Conv d float relu = torch nn ReLU relu _ = torch nn ReLU True hardtanh = torch nn Hardtanh hardtanh_ = torch nn Hardtanh inplace=True forward x x = conv x x = relu x relu _ x x = F relu x x = torch clamp x - x = x clamp - x = x clamp_ - Enable when quantized ` clamp_ ` ready x = hardtanh x hardtanh_ x x = F hardtanh x x data = torch rand dtype=torch float list node should occur order node_list = ns call_function torch quantize_per_tensor ns call_module nnq Conv d ns call_method dequantize quant_type static_quant_types checkGraphModeFxOp M data quant_type expected_node_list=node_list test_fixed_qparams_ops_fp M torch nn Module __init__ - None super __init__ sigmoid = torch nn Sigmoid tanh = torch nn Tanh forward x x = sigmoid x x = torch sigmoid x x = x sigmoid x = tanh x x = torch tanh x x = x tanh x data = torch randn dtype=torch float quant_type = QuantType STATIC TODO use get_default_qconfig_mapping once handles fp qconfig_mapping = QConfigMapping set_global float _static_qconfig backend_config = get_test_only_legacy_native_backend_config node_occurrence = ns call_method checkGraphModeFxOp M data quant_type custom_qconfig_dict=qconfig_mapping expected_node_occurrence=node_occurrence backend_config=backend_config test_fixed_qparams_ops_qint M torch nn Module __init__ - None super __init__ sigmoid = torch nn Sigmoid tanh = torch nn Tanh forward x x = sigmoid x x = torch sigmoid x x = x sigmoid x = tanh x x = torch tanh x x = x tanh x data = torch randn dtype=torch float quant_type = QuantType STATIC qconfig = torch ao quantization QConfig activation=HistogramObserver with_args qscheme=torch per_tensor_symmetric dtype=torch quint weight=default_weight_observer qconfig_mapping = get_default_qconfig_mapping set_global qconfig node_occurrence = ns call_function torch quantize_per_tensor ns call_method dequantize checkGraphModeFxOp M data quant_type custom_qconfig_dict=qconfig_mapping expected_node_occurrence=node_occurrence is_reference=True test_fixed_qparams_ops_wrong_qconfig Test wrong qconfigs fixed qparams ops results ops being quantized M torch nn Module __init__ - None super __init__ sigmoid = torch nn Sigmoid tanh = torch nn Tanh forward x x = sigmoid x x = torch sigmoid x x = x sigmoid x = tanh x x = torch tanh x x = x tanh x data = torch randn dtype=torch float qconfig_mapping = QConfigMapping set_global default_qconfig m = M eval node_occurrence = ns call_function torch quantize_per_tensor ns call_method dequantize checkGraphModeFxOp m data QuantType STATIC custom_qconfig_dict=qconfig_mapping expected_node_occurrence=node_occurrence is_reference=True assertTrue isinstance m sigmoid torch nn Sigmoid assertTrue isinstance m tanh torch nn Tanh skipIfNoFBGEMM test_general_shape_ops A test checks dequantize will swapped all supported general shape ops like aten flatten without actually checking execution these ops M torch nn Module __init__ - None super __init__ maxpool d = torch nn MaxPool d kernel_size= maxpool d = torch nn MaxPool d kernel_size= maxpool d = torch nn MaxPool d kernel_size= dropout = torch nn Dropout conv = torch nn Conv d conv = torch nn Conv d relu = torch nn ReLU forward x x = conv x add_scalar x = x + mul_scalar x = x add_scalar_out x += mul_scalar_out x = add_scalar_relu x = x + x = F relu x add_scalar_relu_out x += x = F relu x mul_scalar_relu x = x x = F relu x mul_scalar_relu_out x = x = F relu x x = maxpool d x x = maxpool d x x = maxpool d x x = torch flatten x x = x reshape - x = x resize_ x x = x view - prim ListConstruct xs = x x prim ListUnpack x y = xs prim TupleConstruct xs = x x prim TupleUnpack x y = xs x = x transpose x = x contiguous chunk supported since observer only supports observing single Tensor currently x y = torch chunk x x = F dropout x x = dropout x x = x permute x = x repeat_interleave x = torch repeat_interleave x x = relu x x = F relu x x = F relu x inplace=True x = x relu x relu_ x = x squeeze x squeeze_ x = torch squeeze x x = x unsqueeze x unsqueeze_ x = torch unsqueeze x x = x detach x detach_ x = x repeat y = y append x z = torch stack y z = z z x _ = z x = conv x x example_inputs = torch rand This model executable since we just put all ops same forward m = M eval qconfig_dict = default_qconfig prepared = prepare_fx m qconfig_dict example_inputs=example_inputs runnable quantized = convert_fx prepared This checks dequantize output first conv being propagated end so we don t insert extra observers also successfully fused two quantized conv d patterns one quantize_per_tensor input check exact counts quantize dequantize count_check = input conv two outputs getitem ns call_function torch quantize_per_tensor output model two outputs getitem ns call_method dequantize order_check = ns call_function torch quantize_per_tensor ns call_module nnq Conv d ns call_module nnq Conv d ns call_method dequantize checkGraphModuleNodes quantized expected_node_occurrence=count_check expected_node_list=order_check Checking is_reference output m = M eval qconfig_dict = default_qconfig prepared = prepare_fx m qconfig_dict example_inputs=example_inputs runnable quantized = convert_to_reference_fx prepared skipIfNoFBGEMM test_ave_pool_with_custom_cfg A test checks correct patterns produced avg_pool d customized config M torch nn Module __init__ - None super __init__ avg_pool d = torch nn AvgPool d forward x x = avg_pool d x x This model executable since we just put all ops same forward m = M eval nothing fuse so skipping fuse step qconfig_dict = default_qconfig example_inputs = torch randn prepared = prepare_fx m qconfig_dict example_inputs=example_inputs prepare_custom_config= input_quantized_idxs runnable quantized = convert_fx prepared This checks dequantize output first conv being propagated end so we don t insert extra observers check exact counts quantize dequantize count_check = ns call_method dequantize order_check = ns call_module nn AvgPool d ns call_method dequantize checkGraphModuleNodes quantized expected_node_occurrence=count_check expected_node_list=order_check skipIfNoFBGEMM test_general_value_ops A test checks correct patterns produced all supported general value ops like aten avg_pool d \ without actually checking execution these ops M torch nn Module __init__ - None super __init__ conv = torch nn Conv d avg_pool d = torch nn AvgPool d avg_pool d = torch nn AvgPool d avg_pool d = torch nn AvgPool d adaptive_avg_pool d = torch nn AdaptiveAvgPool d adaptive_avg_pool d = torch nn AdaptiveAvgPool d adaptive_avg_pool d = torch nn AdaptiveAvgPool d forward x x = conv x x = avg_pool d x x = avg_pool d x x = avg_pool d x x = adaptive_avg_pool d x x = adaptive_avg_pool d x x = adaptive_avg_pool d x x = F avg_pool d x x = F avg_pool d x x = F avg_pool d x x = F adaptive_avg_pool d x x = F adaptive_avg_pool d x x = F adaptive_avg_pool d x x = torch mean x x = torch mean x False x = x mean x = x mean True x = F interpolate x mode= nearest x = F interpolate x mode= linear x = conv x x This model executable since we just put all ops same forward m = M eval nothing fuse so skipping fuse step qconfig_dict = default_qconfig example_inputs = torch randn prepared = prepare_fx m qconfig_dict example_inputs=example_inputs runnable quantized = convert_fx prepared This checks dequantize output first conv being propagated end so we don t insert extra observers check exact counts quantize dequantize count_check = ns call_function torch quantize_per_tensor ns call_method dequantize order_check = ns call_function torch quantize_per_tensor ns call_module nnq Conv d ns call_module nnq Conv d ns call_method dequantize checkGraphModuleNodes quantized expected_node_occurrence=count_check expected_node_list=order_check test_copy_node_fp _input CopyNode works both fp int inputs test make sure CopyNode can successfully quantized both cases M torch nn Module forward x x = x relu x m = M eval m = prepare_fx m default_reuse_input_qconfig example_inputs= torch randn m = convert_fx m make sure runs m torch rand test_getitem Make sure we only insert observer getitem following node matched needs quantized M torch nn Module forward xs x = xs x m = M eval example_inputs = torch rand qconfig_mapping = get_default_qconfig_mapping m = prepare_fx m qconfig_mapping example_inputs=example_inputs checkGraphModuleNodes m expected_node_occurrence= ns call_module torch ao quantization MinMaxObserver m = convert_fx m m example_inputs M torch nn Module forward xs x = xs x = torch sigmoid x x m = M eval example_inputs = torch rand qconfig_mapping = get_default_qconfig_mapping m = prepare_fx m qconfig_mapping example_inputs=example_inputs checkGraphModuleNodes m expected_node_occurrence= ns call_module torch ao quantization FixedQParamsObserver m = convert_fx m checkGraphModuleNodes m expected_node_list= ns call_function torch quantize_per_tensor ns call_method dequantize m example_inputs testing prepare recognizes non-Tensor input getitem M torch nn Module forward x s = x shape n c = s x = torch sigmoid x x m = M eval example_inputs = torch rand qconfig_mapping = get_default_qconfig_mapping m = prepare_fx m qconfig_mapping example_inputs=example_inputs checkGraphModuleNodes m expected_node_occurrence= ns call_module torch ao quantization FixedQParamsObserver m = convert_fx m checkGraphModuleNodes m expected_node_list= ns call_function torch quantize_per_tensor ns call_method dequantize m example_inputs skipIfNoFBGEMM test_fixed_qparams_ops M torch nn Module __init__ - None super __init__ conv = torch nn Conv d sigmoid = torch nn Sigmoid hardsigmoid = torch nn Hardsigmoid tanh = torch nn Tanh softmax = torch nn Softmax dim= forward x x = conv x F sigmoid deprecated x = sigmoid x x = torch sigmoid x x = x sigmoid x = hardsigmoid x x = F hardsigmoid x x = F hardsigmoid x inplace=True x = tanh x F tanh deprecated x = torch tanh x x = x tanh TODO future PR handle F softmax x = softmax x x eval_mode True False This model executable since we just put all ops same forward m = M eval_mode m eval qconfig_mapping = get_default_qconfig_mapping prepare = prepare_fx fq_count = m train qconfig_mapping = get_default_qat_qconfig_mapping prepare = prepare_qat_fx fq_count = nothing fuse so skipping fuse step m_copy = copy deepcopy m example_inputs = torch rand prepared = prepare m qconfig_mapping example_inputs=example_inputs prepared_copy = copy deepcopy prepared check prepare does change model result eval_mode assertEqual m_copy example_inputs prepared_copy example_inputs check correct number activation_post_process inserted expected_activation_post_process = FixedQParamsObserver eval_mode FixedQParamsFakeQuantize count_check = ns call_module expected_activation_post_process fq_count checkGraphModuleNodes prepared expected_node_occurrence=count_check runnable quantized = convert_fx prepared quantized_reference = convert_to_reference_fx prepared_copy This checks dequantize output first conv being propagated end so we don t insert extra observers check exact counts quantize dequantize count_check = ns call_function torch quantize_per_tensor ns call_method dequantize order_check = ns call_function torch quantize_per_tensor ns call_module nnq Conv d ns call_module nn Sigmoid ns call_module nnq Softmax ns call_method dequantize checkGraphModuleNodes quantized expected_node_occurrence=count_check expected_node_list=order_check reference_count_check = ns call_function torch quantize_per_tensor ns call_method dequantize reference_order_check = ns call_function torch quantize_per_tensor ns call_method dequantize ns call_module nnqr Conv d ns call_function torch quantize_per_tensor ns call_method dequantize ns call_module nn Sigmoid ns call_function torch quantize_per_tensor ns call_method dequantize ns call_module nn Softmax ns call_function torch quantize_per_tensor ns call_method dequantize checkGraphModuleNodes quantized_reference expected_node_occurrence=reference_count_check expected_node_list=reference_order_check Verify softmax scale zero_point correct assertTrue quantized softmax scale - = e- assertTrue quantized softmax zero_point == test_float_functional TorchAdd nn Module Wrapper around torch add so all ops can found build __init__ - None super __init__ add_func = nnq FloatFunctional forward x y add_func add x y M torch nn Module __init__ - None super __init__ ff = TorchAdd ff = nnq FloatFunctional ff = nnq FloatFunctional ff = nnq FloatFunctional ff = nnq FloatFunctional ff = nnq FloatFunctional forward x x = ff x x x = ff add_scalar x x = ff mul x x x = ff mul_scalar x x = ff add_relu x x x = ff cat x x example_inputs = torch rand Note QAT test succeeded chance make actually work we need fix eager mode FloatFunctional removing activation_post_process add_scalar mul_scalar quant_type static_quant_types m = M ref_m = torch ao quantization QuantWrapper M is_qat = quant_type == QuantType QAT is_qat m train ref_m train qconfig = default_qat_qconfig expected_act_post_process = torch ao quantization FakeQuantize m eval ref_m eval qconfig = default_qconfig expected_act_post_process = torch ao quantization MinMaxObserver prepare_fx_function = prepare_qat_fx is_qat prepare_fx qconfig_dict = qconfig m = prepare_fx_function m qconfig_dict example_inputs=example_inputs node_occurrence = ns call_module expected_act_post_process ns call_module torch ao nn quantized FloatFunctional checkGraphModuleNodes m expected_node_occurrence=node_occurrence m example_inputs node_list = ns call_function torch quantize_per_tensor ns call_function torch ops quantized add ns call_function torch ops quantized add ns call_function torch ops quantized mul ns call_function torch ops quantized mul ns call_function torch ops quantized add_relu ns call_function torch cat ns call_method dequantize m = convert_fx m checkGraphModuleNodes m expected_node_list=node_list make sure numerics match eager mode ref_m qconfig = qconfig prepare_function = prepare_qat is_qat prepare ref_m = prepare_function ref_m ref_m example_inputs ref_m = convert ref_m FX Graph Mode Eager Mode now diverages numerics add_scalar mul_scalar assertEqual m data ref_m data test_embedding M torch nn Module __init__ - None super __init__ emb = torch nn Embedding num_embeddings= embedding_dim= forward indices emb indices qconfig_type float_qparams_weight_only_qconfig float_qparams_weight_only_qconfig_ bit model = M eval indices = torch tensor example_inputs = indices quantized_node = ns call_module nnq Embedding check dynamic quant checkGraphModeFxOp model example_inputs QuantType DYNAMIC quantized_node custom_qconfig_dict= qconfig_type model = M eval configs = qconfig_type ns call_module nnq Embedding None ns call_module nn Embedding default_qconfig ns call_module nn Embedding check static quantization qconfig node configs qconfig_dict = qconfig m = prepare_fx model qconfig_dict example_inputs=example_inputs checkGraphModuleNodes m expected_node_occurrence= ns call_module torch ao quantization MinMaxObserver m = convert_fx m checkGraphModuleNodes m expected_node=node make sure runs m example_inputs test_embedding_bag M torch nn Module __init__ - None super __init__ emb = torch nn EmbeddingBag num_embeddings= embedding_dim= include_last_offset=True forward indices offsets emb indices offsets indices = torch tensor offsets = torch tensor quantized_node = ns call_module nnq EmbeddingBag example_inputs = indices offsets dtype torch quint torch quint x model = M eval float_qparams_observer = PerChannelMinMaxObserver with_args dtype=dtype qscheme=torch per_channel_affine_float_qparams ch_axis= float_qparams_qconfig = QConfig activation=default_placeholder_observer weight=float_qparams_observer checkGraphModeFxOp model example_inputs QuantType DYNAMIC quantized_node custom_qconfig_dict= float_qparams_qconfig check works None static qconfig qconfig None default_qconfig qconfig_dict = default_qconfig m = M eval m = prepare_fx model qconfig_dict example_inputs=example_inputs checkGraphModuleNodes m expected_node_occurrence= ns call_module torch ao quantization MinMaxObserver m = convert_fx m checkGraphModuleNodes m expected_node=ns call_module nn EmbeddingBag make sure runs m example_inputs _test_rnn_impl qconfigs M module_type_strs module_types sample_input options = itertools product qconfigs module_type_strs qconfig module_type_str options model_eager = M module_type_str eval model_graph = copy deepcopy model_eager torch backends quantized engine == qnnpack \ qconfig float _dynamic_qconfig continue fp dynamic quant supported qnnpack eager_qconfig_dict = dict fromkeys module_types qconfig model_eager = quantize_dynamic model_eager qconfig_spec=eager_qconfig_dict graph_qconfig_dict = object_type x qconfig x module_types model_graph = prepare_fx model_graph graph_qconfig_dict example_inputs= sample_input model_graph = convert_fx model_graph assertEqual model_eager sample_input model_graph sample_input checkScriptable model_graph sample_input True override_qengines test_rnn_cell torch backends quantized engine fbgemm qnnpack qconfigs = per_channel_dynamic_qconfig default_dynamic_qconfig float _dynamic_qconfig module_type_strs = LSTMCell GRUCell RNNTanh RNNReLU module_types = torch nn LSTMCell torch nn GRUCell torch nn RNNCell sample_input = torch tensor - - - dtype=torch float _test_rnn_impl qconfigs RNNCellDynamicModel module_type_strs module_types sample_input override_qengines test_rnn torch backends quantized engine fbgemm qnnpack qconfigs = per_channel_dynamic_qconfig default_dynamic_qconfig float _dynamic_qconfig module_type_strs = LSTM GRU module_types = torch nn LSTM torch nn GRU niter = sample_input = torch tensor - - - dtype=torch float unsqueeze repeat niter _test_rnn_impl qconfigs RNNDynamicModel module_type_strs module_types sample_input _test_conv_transpose_impl float_cls Callable q_cls Callable data torch Tensor override_quantized_engine qnnpack Create fp versions FX Eager models m = torch nn Sequential float_cls m = torch nn Sequential float_cls m load_state_dict m state_dict m = torch ao quantization QuantWrapper m FX graph result_dict = checkGraphModeFxOp m data QuantType STATIC expected_node_occurrence= ns call_module q_cls q_result = result_dict quantized_output Eager m qconfig = get_default_qconfig torch backends quantized engine m eval m p = torch ao quantization prepare m m p data m q = torch ao quantization convert m p q_result = m q data verify results match assertEqual q_result q_result unittest skipUnless qnnpack supported_qengines This Pytorch Build has been built does support QNNPACK test_conv_transpose_ d _test_conv_transpose_impl torch nn ConvTranspose d nnq ConvTranspose d torch randn unittest skipUnless qnnpack supported_qengines This Pytorch Build has been built does support QNNPACK test_conv_transpose_ d _test_conv_transpose_impl torch nn ConvTranspose d nnq ConvTranspose d torch randn test_reshape_fp M torch nn Module __init__ w b super __init__ w = w b = b forward x x = torch nn functional linear x w x = x reshape - x = torch nn functional linear x w x w = torch randn b = torch randn m = M w b eval qconfig_dict = reshape will quantized fp requested qconfig float _static_qconfig object_type torch nn functional linear default_qconfig backend_config = get_test_only_legacy_native_backend_config example_inputs = torch randn m = prepare_fx m qconfig_dict example_inputs=example_inputs backend_config=backend_config expected_occurrence = input weight first second linear output first second linear ns call_module torch ao quantization MinMaxObserver we insert placeholder observer both input output reshape ns call_module torch ao quantization PlaceholderObserver checkGraphModuleNodes m expected_node_occurrence=expected_occurrence m = convert_fx m backend_config=backend_config expected_occurrence = ns call_function torch quantize_per_tensor dequantize after first linear before reshape before output ns call_method dequantize before reshape fp ns call_method ns call_function torch ops quantized linear checkGraphModuleNodes m expected_node_occurrence=expected_occurrence make sure runs m torch randn test_multiple_qconfigs_for_single_value Test multiple qconfigs single value M torch nn Module __init__ w b super __init__ w = w b = b forward x x = torch nn functional linear x w x = torch sigmoid x x w = torch randn b = torch randn m = M w b eval TODO use get_default_qconfig_mapping once handles fp qconfig_mapping = QConfigMapping \ set_global float _static_qconfig \ set_object_type torch nn functional linear default_qconfig example_inputs = torch randn backend_config = get_test_only_legacy_native_backend_config m = prepare_fx m qconfig_mapping example_inputs=example_inputs backend_config=backend_config expected_occurrence = input weight linear output linear ns call_module torch ao quantization MinMaxObserver input output sigmoid ns call_module torch ao quantization PlaceholderObserver checkGraphModuleNodes m expected_node_occurrence=expected_occurrence make sure runs m = convert_fx m expected_occurrence = ns call_function torch quantize_per_tensor ns call_method dequantize ns call_method checkGraphModuleNodes m expected_node_occurrence=expected_occurrence test_boolean_tensor Make sure we don t insert observer boolean Tensors M torch nn Module forward x mask mask = mask unsqueeze mask = mask unsqueeze x = x masked_fill mask x m = M eval example_inputs = torch rand torch rand bool m = prepare_fx m default_qconfig example_inputs=example_inputs expected_occurrence = ns call_module torch ao quantization MinMaxObserver checkGraphModuleNodes m expected_node_occurrence=expected_occurrence m = convert_fx m m example_inputs test_chunk M torch nn Module forward x x y = torch chunk x x = x + y x m = M eval example_inputs = torch rand m = prepare_fx m default_qconfig example_inputs=example_inputs m example_inputs m = convert_fx m m example_inputs make sure everything runs test_ref_pattern_multi_use M torch nn Module __init__ - None super __init__ linear = torch nn Linear linear = torch nn Linear forward x y = linear x z = linear x = torch mul z b = torch add z y b m = M eval qconfig_dict = None object_type torch nn Linear get_default_qconfig fbgemm torch nn ReLU get_default_qconfig fbgemm example_inputs = torch randn m = prepare_fx m qconfig_dict example_inputs=example_inputs m = convert_fx m expected_occurrence = ns call_function torch quantize_per_tensor ns call_module nnq Linear ns call_method dequantize ns call_function torch add ns call_function torch mul checkGraphModuleNodes m expected_node_occurrence=expected_occurrence test_qmatmul M torch nn Module forward x y z = torch matmul x y z m = M eval example_inputs = torch randn torch randn qconfig_dict = get_default_qconfig_mapping fbgemm mp = prepare_fx m qconfig_dict example_inputs=example_inputs mp example_inputs mq = convert_fx mp expected_occurrence = ns call_function torch matmul ns call_function torch ops quantized matmul checkGraphModuleNodes mq expected_node_occurrence=expected_occurrence verify no crash res = mq example_inputs test_pixel_shuffle MyBias nn Module __init__ - None super __init__ bias = nn Parameter torch randn MyModel nn Module __init__ - None super __init__ conv = nn Conv d bias=False bias = MyBias forward x x = conv x x = nn functional pixel_shuffle x x = x view - bias = bias bias x + bias backend_config = get_qnnpack_backend_config qconfig_mapping = get_default_qconfig_mapping qnnpack model = MyModel m = prepare_fx model qconfig_mapping=qconfig_mapping example_inputs= torch randn backend_config=backend_config m = convert_fx m expected_occurrence = ns call_function torch quantize_per_tensor ns call_method dequantize checkGraphModuleNodes m expected_node_occurrence=expected_occurrence test_pixel_shuffle_module - None MyBias nn Module __init__ - None super __init__ bias = nn Parameter torch randn MyModel nn Module __init__ - None super __init__ conv = nn Conv d bias=False ps = nn PixelShuffle upscale_factor= bias = MyBias forward x x = conv x x = ps x x = x view - bias = bias bias x + bias backend_config = get_qnnpack_backend_config qconfig_mapping = get_default_qconfig_mapping qnnpack model = MyModel m = prepare_fx model qconfig_mapping=qconfig_mapping example_inputs= torch randn backend_config=backend_config m = convert_fx m expected_occurrence = ns call_function torch quantize_per_tensor ns call_method dequantize ns call_module nn PixelShuffle checkGraphModuleNodes m expected_node_occurrence=expected_occurrence test_pixel_unshuffle MyBias nn Module __init__ - None super __init__ bias = nn Parameter torch randn MyModel nn Module __init__ - None super __init__ conv = nn Conv d bias=False bias = MyBias forward x x = conv x x = nn functional pixel_unshuffle x bias = bias bias x + bias backend fbgemm qnnpack backend == fbgemm backend_config = get_fbgemm_backend_config backend_config = get_qnnpack_backend_config qconfig_mapping = get_default_qconfig_mapping backend model = MyModel m = prepare_fx model qconfig_mapping=qconfig_mapping example_inputs= torch randn backend_config=backend_config m = convert_fx m expected_occurrence = ns call_function torch quantize_per_tensor ns call_method dequantize checkGraphModuleNodes m expected_node_occurrence=expected_occurrence test_pixel_unshuffle_module - None MyBias nn Module __init__ - None super __init__ bias = nn Parameter torch randn MyModel nn Module __init__ - None super __init__ conv = nn Conv d bias=False unshuffle = nn PixelUnshuffle downscale_factor= bias = MyBias forward x x = conv x x = unshuffle x bias = bias bias x + bias backend fbgemm qnnpack backend == fbgemm backend_config = get_fbgemm_backend_config backend_config = get_qnnpack_backend_config qconfig_mapping = get_default_qconfig_mapping backend model = MyModel m = prepare_fx model qconfig_mapping=qconfig_mapping example_inputs= torch randn backend_config=backend_config m = convert_fx m expected_occurrence = ns call_function torch quantize_per_tensor ns call_method dequantize ns call_module nn PixelUnshuffle checkGraphModuleNodes m expected_node_occurrence=expected_occurrence test_narrow MyBias nn Module __init__ - None super __init__ bias = nn Parameter torch randn MyModel nn Module __init__ - None super __init__ conv = nn Conv d bias=False bias = MyBias forward x x = conv x x = torch narrow x bias = bias bias x + bias backend fbgemm qnnpack backend == fbgemm backend_config = get_fbgemm_backend_config backend_config = get_qnnpack_backend_config qconfig_mapping = get_default_qconfig_mapping backend model = MyModel m = prepare_fx model qconfig_mapping=qconfig_mapping example_inputs= torch randn backend_config=backend_config m = convert_fx m expected_occurrence = ns call_function torch quantize_per_tensor ns call_method dequantize checkGraphModuleNodes m expected_node_occurrence=expected_occurrence TestQuantizeFxModels QuantizationTestCase skipIfNoFBGEMM unittest skipIf TEST_CUDA gpu available test_static_gpu_convert_basic Net nn Module __init__ - None super __init__ relu = nn ReLU conv = nn Conv d linear = nn Linear forward x x = relu conv x y = linear x view - y input = torch randn cuda example_inputs = input model = Net cuda eval qconfig_dict = torch ao quantization get_default_qconfig fbgemm model_prepared = prepare_fx model qconfig_dict example_inputs=example_inputs model_prepared example_inputs model_quantized = convert_to_reference_fx model_prepared out = model_quantized example_inputs assertEqual out device type cuda skipIfNoFBGEMM unittest skipIf TEST_CUDA gpu available test_switch_device_prepare_convert Net nn Module __init__ - None super __init__ relu = nn ReLU conv = nn Conv d linear = nn Linear forward x x = relu conv x y = linear x view - y device cuda cpu device_after = cuda device == cpu cpu input = torch randn device model = Net device eval qconfig_dict = torch ao quantization get_default_qconfig fbgemm model_prepared = prepare_fx model qconfig_dict example_inputs= input model_prepared input model_prepared device_after model_quantized = convert_to_reference_fx model_prepared out = model_quantized input device_after assertEqual out device type device_after skipIfNoFBGEMM unittest skipIf TEST_CUDA gpu available test_prepare_serialize_switch_device_convert Net nn Module __init__ - None super __init__ conv = nn Conv d linear = nn Linear forward x x = conv x y = linear x view - y device cuda cpu device_after cuda cpu input = torch randn device model = Net device eval qconfig_dict = torch ao quantization get_default_qconfig fbgemm model_prepared_first = prepare_fx model qconfig_dict example_inputs= input model_prepared_second = prepare_fx model qconfig_dict example_inputs= input model_prepared_first input state_dict = model_prepared_first state_dict del model_prepared_first model_prepared_second load_state_dict state_dict model_prepared_second device_after model_quantized = convert_to_reference_fx model_prepared_second out = model_quantized input device_after assertEqual out device type device_after skipIfTorchDynamo too slow skip_if_no_torchvision test_model_dropout torchvision models m = models mobilenet_v _small qconfig_mapping = torch ao quantization get_default_qat_qconfig_mapping fbgemm example_inputs = torch randn mp = prepare_qat_fx m qconfig_mapping example_inputs=example_inputs mp example_inputs override_quantized_engine qnnpack IS_ARM contextlib nullcontext mq = convert_fx mp mq example_inputs _test_model_impl mode name model eager_quantizable_model check_with_eager=True diff_of_quant=None diff_from_eager=None diff_of_quant None diff_from_eager None diff_of_quant = diff_from_eager = mode diff_of_quant mode diff_from_eager diff_of_quant mode = diff_from_eager mode = input_tensor = torch rand input_tensor_inception = torch rand output_value = torch randint print quantizing name mode mode name == inception_v input_value = input_tensor_inception input_value = input_tensor qconfig = default_qconfig mode == static default_qat_qconfig qconfig_dict = qconfig script = torch jit script model make sure graph module script module both runanble original_out = model input_value is_not_tuple_out = isinstance original_out tuple script_out = script input_value set train just before quantization prepare_fx_fn = prepare_fx mode = static model train prepare_fx_fn = prepare_qat_fx prepared = prepare_fx_fn model qconfig_dict mode == ddp mp spawn run_ddp args= world_size prepared noqa F nprocs=world_size noqa F join=True mode == qat assert prepared training prepared must training mode qat optimizer = torch optim SGD prepared parameters lr= criterion = nn CrossEntropyLoss train_one_epoch prepared criterion optimizer input_value output_value torch device cpu _ range prepared input_value print after observation root prepared root qgraph = convert_fx prepared print after quantization root qgraph root print after quantization code qgraph src qgraph eval qgraph_script = torch jit script qgraph print quantized scripted qgraph_script graph qgraph_out = qgraph input_value qgraph_script = qgraph_script input_value is_not_tuple_out diff_of_quant mode name = original_out - qgraph_out abs max assert torch allclose qgraph_out qgraph_script graph scripted graph print tuple output eager_quantizable_model None comparing eager mode quantization qeager = eager_quantizable_model ref_out = qeager input_value qeager qconfig = qconfig mode == static qeager fuse_model prepare qeager inplace=True qeager train qeager fuse_model prepare_qat qeager inplace=True calibration mode == ddp mp spawn run_ddp args= world_size qeager noqa F nprocs=world_size noqa F join=True mode == qat assert qeager training qeager should training mode qat optimizer = torch optim SGD qeager parameters lr= train_one_epoch qeager criterion optimizer input_value output_value torch device cpu _ range qeager input_value print ref after observation qeager convert qeager inplace=True qeager eval print ref after quantization qeager qeager_out = qeager input_value qeager_script = torch jit script qeager qscript_out = qeager_script input_value is_not_tuple_out diff_from_eager mode name = qeager_out - qgraph_out abs max check_with_eager assertEqual diff_from_eager mode name Result graph mode quantization + eager mode quantization model + name + should match Mode + mode + diff + str diff_from_eager mode name _test_building_block quant_type BB eager = BB float graph = copy deepcopy eager quant_type == QuantType STATIC qconfig = default_qconfig eager_prepare = prepare graph_prepare = prepare_fx eager eval graph eval calibrate_or_train = test_only_eval_fn data = img_data_ d is_qat = False assert quant_type == QuantType QAT qconfig = default_qat_qconfig eager_prepare = prepare_qat graph_prepare = prepare_qat_fx eager train graph train calibrate_or_train = test_only_train_fn data = img_data_ d_train is_qat = True hasattr eager fuse_model eager fuse_model eager = QuantWrapper eager eager qconfig = qconfig eager = eager_prepare eager qconfig_dict = qconfig graph = graph_prepare graph qconfig_dict example_inputs= data eager_out = eager data graph_out = graph data Eager Mode FX Graph Mode QAT now differ numerics both Post Training QAT because FX Graph Mode uses same fake_quant instances input output CopyNode assertEqual eager_out graph_out calibrate_or_train eager data calibrate_or_train graph data eager = convert eager graph = convert_fx graph eager_out = eager data graph_out = graph data override_qengines test_resnet_base models = ResNetBase options = itertools product static_quant_types models quant_type M options _test_building_block quant_type M skip_if_no_torchvision skipIfNoFBGEMM unittest skip skip now since tbb failed test_torchvision torchvision models torchvision models quantization quantized_models torchvision models quantization utils _replace_relu get_available_classification_models models k k v models __dict__ items callable v k lower == k k = _ model_list = get_available_classification_models models quantized_model_list = get_available_classification_models quantized_models quantized_model_list = set quantized_model_list test eager graph consistency model_list = quantized_model_list mobilenet inception_v googlenet qat working due AdaptiveAveragePool qat we might observe output AdaptiveAveragePool future re-enable test fx_eager_not_matching = mobilenet_v qat inception_v qat googlenet qat because relu replaced relu mobilenetv diff_of_quant = diff_from_eager = modes = static qat options = itertools product modes model_list mode name options pretrained = name quantized_model_list load pretrained model compare quantized model kwargs = turn off transform input inception_v since s quantized eager mode fx graph mode we can t skip quantizing method right now might supported future name inception_v googlenet kwargs transform_input = False eager_quantizable_model = None name quantized_model_list eager_quantizable_model = quantized_models __dict__ name pretrained=False quantize=False kwargs eval float compare eager mode quantized model when available pretrained = eager_quantizable_model None model = models __dict__ name pretrained=pretrained kwargs eval float name == mobilenet_v _replace_relu model disable aux logits hasattr model aux_logits model aux_logits = False model AuxLogits = None eager_quantizable_model eager_quantizable_model aux_logits = False eager_quantizable_model AuxLogits = None check_with_eager = name mode fx_eager_not_matching _test_model_impl mode name model eager_quantizable_model check_with_eager diff_of_quant diff_from_eager print_diffs diffs mode diffs_for_mode diffs items print mode mode name diff diffs_for_mode items print name diff print differences between float quantized print_diffs diff_of_quant print ---------------------- print differences between graph mode eager mode print_diffs diff_from_eager print ---------------------- skip_if_no_torchvision skipIfNoFBGEMM unittest skip TODO Test always failing - https github com pytorch pytorch issues test_resnet _ddp torchvision models torchvision models quantization quantized_models eager_quantizable_model = quantized_models __dict__ name pretrained=False quantize=False eval float noqa F model = models __dict__ name pretrained=False eval float noqa F _test_model_impl ddp resnet model eager_quantizable_model override_qengines test_qat_embeddingbag_linear device get_supported_device_types EmbeddingBagLinear torch nn Module __init__ - None super __init__ emb = torch nn EmbeddingBag num_embeddings= embedding_dim= mode= sum linear = torch nn Linear dtype=torch float forward input torch Tensor offsets Optional torch Tensor = None per_sample_weights Optional torch Tensor = None x = emb input offsets per_sample_weights x = linear x x qengine = torch backends quantized engine qconfig_dict = QConfigMapping \ set_global get_default_qat_qconfig qengine \ set_object_type torch nn EmbeddingBag default_embedding_qat_qconfig train_indices = torch randint torch randn _ range eval_output = torch randint model = EmbeddingBagLinear train prepared_fx_model = prepare_qat_fx model qconfig_dict example_inputs= train_indices test_only_train_fn prepared_fx_model train_indices quant_model = convert_fx prepared_fx_model qconfig_mapping=qconfig_dict checkQuantized model Make sure EmbeddingBag now quantized EmbeddingBag assertTrue type model emb nn quantized EmbeddingBag Also test Linear has been quantized assertTrue type model linear nnq Linear test_only_eval_fn model eval_output checkScriptable model eval_output checkNoQconfig model checkQuantized quant_model override_qengines test_qat_embedding_linear device get_supported_device_types EmbeddingLinear torch nn Module __init__ - None super __init__ emb = torch nn Embedding num_embeddings= embedding_dim= linear = torch nn Linear dtype=torch float forward input torch Tensor x = torch sum emb input dim= x = linear x x qengine = torch backends quantized engine qconfig_dict = get_default_qat_qconfig qengine object_type torch nn Embedding default_embedding_qat_qconfig train_indices = torch randint torch randn _ range eval_output = torch randint model = EmbeddingLinear train prepared_fx_model = prepare_qat_fx model qconfig_dict example_inputs= train_indices test_only_train_fn prepared_fx_model train_indices quant_model = convert_fx prepared_fx_model qconfig_mapping=qconfig_dict checkQuantized model Make sure EmbeddingBag now quantized EmbeddingBag assertTrue type model emb nn quantized Embedding Also test Linear has been quantized assertTrue type model linear nnq Linear test_only_eval_fn model eval_output checkScriptable model eval_output checkNoQconfig model checkQuantized quant_model given device=st sampled_from cpu cuda torch cuda is_available cpu settings deadline=None override_qengines test_qat_functional_linear device torch backends quantized engine fbgemm qnnpack Linear torch nn Module __init__ - None super __init__ w = torch ones b = torch zeros forward x torch nn functional linear x w b M torch nn Module __init__ - None super __init__ mods = torch nn Sequential Linear Linear mods = Linear forward x x = mods x x = mods x x model = M train ref_fake_quant = FakeQuantize with_args observer=MovingAverageMinMaxObserver quant_min= quant_max= dtype=torch quint reduce_range=False ref_weight_fake_quant = FakeQuantize with_args observer=MovingAverageMinMaxObserver quant_min=- quant_max= dtype=torch qint reduce_range=False ref_qat_qconfig = QConfig activation=ref_fake_quant weight=ref_weight_fake_quant qconfig_dict = ref_qat_qconfig example_inputs = torch randn prepared_ref = prepare_qat_fx model qconfig_dict example_inputs=example_inputs custom_fake_quant = FusedMovingAvgObsFakeQuantize with_args observer=MovingAverageMinMaxObserver quant_min= quant_max= dtype=torch quint reduce_range=False custom_weight_fake_quant = FusedMovingAvgObsFakeQuantize with_args observer=MovingAverageMinMaxObserver quant_min=- quant_max= dtype=torch qint reduce_range=False custom_qconfig = QConfig activation=custom_fake_quant weight=custom_weight_fake_quant custom_qconfig_dict = custom_qconfig prepared = prepare_qat_fx model custom_qconfig_dict example_inputs=example_inputs prepared device prepared_ref device prepared apply torch ao quantization disable_fake_quant prepared apply torch ao quantization disable_observer prepared_ref apply torch ao quantization disable_fake_quant prepared_ref apply torch ao quantization disable_observer inp = torch randn device=device requires_grad=True i range i == prepared apply torch ao quantization enable_observer prepared_ref apply torch ao quantization enable_observer i == prepared apply torch ao quantization enable_fake_quant prepared_ref apply torch ao quantization enable_fake_quant inp = torch randn device=device requires_grad=True out_ref = prepared_ref inp out = prepared inp torch testing assert_close out out_ref try backward pass labels = torch randn device=device loss = out - labels sum grad = torch autograd grad loss inp loss_ref = out_ref - labels sum grad_ref = torch autograd grad loss_ref inp torch testing assert_close grad grad_ref fbgemm torch backends quantized supported_engines During lowering step convert fold_weight calls quantized linear_prepack which doesn t support QuantizedCuda backend prepared cpu prepared_ref cpu converted = convert_fx prepared converted_ref = convert_fx prepared_ref inp = torch rand out = converted inp out_ref = converted_ref inp torch testing assert_close out out_ref __name__ == __main__ raise RuntimeError This test file meant run directly use \n\n \tpython test test_quantization py TESTNAME\n\n instead