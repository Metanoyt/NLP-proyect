Owner s oncall distributed operator os sys threading functools reduce unittest skip SkipTest torch torch autograd torch distributed dist torch _C _distributed_c d ReduceOp dist is_available print Distributed available skipping tests file=sys stderr sys exit torch testing _internal common_distributed MultiThreadedTestCase skip_if_lt_x_gpu spawn_threads_and_init_comms torch testing _internal common_utils IS_SANDCASTLE run_tests TestCase device_type = acc type acc = torch accelerator current_accelerator cpu DEFAULT_WORLD_SIZE = TestCollectivesWithWrapper TestCase spawn_threads_and_init_comms world_size= test_broadcast_object_list val = dist get_rank == None object_list = val dist get_world_size dist broadcast_object_list object_list=object_list assertEqual object_list test_collective_error_on_rank_zero spawn_threads_and_init_comms world_size= _test_method input_tensor = torch ones dist get_rank perform st all gather output_tensors = torch empty_like input_tensor _ range dist get_world_size dist all_gather output_tensors input_tensor dist get_rank == raise AssertionError Mimic real test failure fail rank dist all_gather output_tensors input_tensor perform nd all gather assertRaises RuntimeError _test_method test_collective_error_on_rank_non_zero spawn_threads_and_init_comms world_size= _test_method input_tensor = torch ones dist get_rank perform st all gather output_tensors = torch empty_like input_tensor _ range dist get_world_size dist all_gather output_tensors input_tensor dist get_rank == raise AssertionError Mimic real test failure fail rank dist all_gather output_tensors input_tensor perform nd all gather assertRaises RuntimeError _test_method test_collective_error_on_rank_non_zero_all spawn_threads_and_init_comms world_size= _test_method input_tensor = torch ones dist get_rank perform st all gather output_tensors = torch empty_like input_tensor _ range dist get_world_size dist all_gather output_tensors input_tensor dist get_rank raise AssertionError Mimic real test failure fail all non-zero rank dist all_gather output_tensors input_tensor perform nd all gather assertRaises RuntimeError _test_method test_skip spawn_threads_and_init_comms world_size= skip check skip exception can captured correctly _test_method pass IS_SANDCASTLE assertRaises SkipTest _test_method spawn_threads_and_init_comms world_size= test_all_to_all_single_tensor rank = dist get_rank world_size = dist get_world_size send = torch full world_size rank sizes = torch ones world_size dtype=torch int out = torch zeros world_size dtype=send dtype dist all_to_all_single out send sizes sizes assertEqual out tolist list zip range world_size range world_size spawn_threads_and_init_comms world_size= test_all_to_all_single_list rank = dist get_rank world_size = dist get_world_size send = torch full world_size rank sizes = world_size out = torch zeros world_size dtype=send dtype dist all_to_all_single out send sizes sizes assertEqual out tolist list zip range world_size range world_size spawn_threads_and_init_comms world_size= test_all_to_all_single_none rank = dist get_rank world_size = dist get_world_size send = torch full world_size rank out = torch zeros world_size dtype=send dtype dist all_to_all_single out send assertEqual out tolist list zip range world_size range world_size TestCollectivesWithBaseClass MultiThreadedTestCase property world_size setUp os environ TORCH_DIST_INIT_BARRIER = super setUp _spawn_threads tearDown super tearDown os environ TORCH_DIST_INIT_BARRIER = test_allgather input_tensor = torch ones dist get_rank output_tensors = torch empty_like input_tensor _ range world_size dist all_gather output_tensors input_tensor rank out_tensor enumerate output_tensors assertEqual out_tensor torch ones rank test_broadcast input_tensor = torch ones dist get_rank rank range world_size cloned_input = input_tensor clone dist broadcast cloned_input src=rank assertEqual cloned_input torch ones rank test_scatter dist get_rank == scatter_list = torch ones rank rank range world_size scatter_list = None output_tensor = torch empty dist scatter output_tensor scatter_list assertEqual output_tensor torch ones dist get_rank test_reduce_scatter to_reduce_scatter = torch ones rank rank range world_size output_tensor = torch empty dist reduce_scatter output_tensor to_reduce_scatter expected_tensor = torch ones dist get_rank world_size assertEqual output_tensor expected_tensor output_tensor = torch empty dist reduce_scatter output_tensor to_reduce_scatter op=dist ReduceOp AVG expected_tensor = torch ones dist get_rank assertEqual output_tensor expected_tensor test_broadcast_object_list val = dist get_rank == None object_list = val dist get_world_size print f dist get_rank - dist get_world_size dist broadcast_object_list object_list=object_list assertEqual object_list test_all_reduce output = torch ones dist get_rank dist all_reduce output res_num = + world_size - world_size assertEqual output torch ones res_num test_all_to_all rank = rank world_size = world_size input_tensor_list = torch ones x x range rank world_size rank + world_size output_tensor_list = torch empty_like tensor tensor input_tensor_list dist all_to_all output_tensor_list input_tensor_list expected_tensor_list = torch ones x x range rank world_size world_size world_size assertEqual expected_tensor_list output_tensor_list test_all_reduce_ops tensor = torch tensor dist get_rank + dist all_reduce tensor op=ReduceOp PRODUCT expected = reduce operator mul range world_size + assertEqual expected tensor item tensor = torch tensor dist get_rank + dist all_reduce tensor op=ReduceOp MIN assertEqual tensor item tensor = torch tensor dist get_rank + dist all_reduce tensor op=ReduceOp MAX assertEqual world_size tensor item tensor = torch tensor dist get_rank + dist all_reduce tensor op=ReduceOp BAND expected = reduce operator and_ range world_size + assertEqual expected tensor item tensor = torch tensor dist get_rank + dist all_reduce tensor op=ReduceOp BOR expected = reduce operator or_ range world_size + assertEqual expected tensor item tensor = torch tensor dist get_rank + dist all_reduce tensor op=ReduceOp BXOR expected = reduce operator xor range world_size + assertEqual expected tensor item test_assert_equal_on_rank RNG shared across threads So instead asserting all threads we only assert rank self_tensor = torch rand rank_ _tensor = self_tensor clone dist broadcast rank_ _tensor src= assertEqualOnRank rank_ _tensor self_tensor rank= assertNotEqualOnRank rank_ _tensor self_tensor rank= test_subpg subpg = dist new_group subpg = dist new_group current_rank = dist get_rank output = torch ones current_rank call all_reduce subpg subpg concurrently current_rank dist all_reduce output group=subpg dist all_reduce output group=subpg current_rank assertEqual output torch ones assertEqual output torch ones test_using_pg_from_another_thread stuff_in_other_thread pg x = torch rand requires_grad=True dist all_reduce x group=pg t = threading Thread target=stuff_in_other_thread args= dist group WORLD t start t join test_gather dist get_rank == gather_list = torch empty _ range world_size gather_list = None input_tensor = torch ones dist get_rank dist gather input_tensor gather_list dist get_rank == i range world_size assertEqual gather_list i torch ones i test_all_reduce_coalesced t = torch ones dist get_rank t = torch ones dist get_rank dist all_reduce_coalesced t t res_num = + world_size - world_size assertEqual t torch ones res_num assertEqual t torch ones res_num skip_if_lt_x_gpu test_bwd_sees_fwd_pg fwd_tid = threading current_thread ident MyFunc torch autograd Function staticmethod forward ctx rank result = rank ctx save_for_backward result rank assert int rank item == dist get_rank result staticmethod backward ctx grad_output result rank = ctx saved_tensors bwd_tid = threading current_thread ident assertEqual fwd_tid bwd_tid f bwd running same thread fwd rank rank item assertTrue dist is_initialized assertEqual int rank item dist get_rank dist all_reduce result assertEqual int result item + + + grad_output result x = torch tensor dist get_rank dtype=torch float device=device_type requires_grad=True x = MyFunc apply x x sum backward __name__ == __main__ run_tests