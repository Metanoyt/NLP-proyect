mypy allow-untyped-defs __future__ annotations copy functools typing_extensions typing Any Optional TYPE_CHECKING torch torch _dynamo torchdynamo torch nn functional F torch ao quantization fake_quantize FakeQuantize FusedMovingAvgObsFakeQuantize torch ao quantization observer HistogramObserver MinMaxObserver MovingAverageMinMaxObserver MovingAveragePerChannelMinMaxObserver PerChannelMinMaxObserver PlaceholderObserver torch ao quantization quantizer QuantizationSpec Quantizer torch ao quantization quantizer utils _get_module_name_filter torch ao quantization quantizer xnnpack_quantizer_utils _convert_scalars_to_attrs OP_TO_ANNOTATOR OperatorConfig OperatorPatternType propagate_annotation QuantizationConfig torch fx _compatibility compatibility TYPE_CHECKING collections abc Callable torch ao quantization qconfig _ObserverOrFakeQuantizeConstructor torch fx Node __all__ = XNNPACKQuantizer get_symmetric_quantization_config _get_dynamo_graph function Callable inputs - torch fx Graph gm _ = torchdynamo export function aten_graph=True inputs gm graph eliminate_dead_code gm graph _get_linear_patterns input_size list int in_channels = input_size - out_channels = hard coding should matter weight = torch ones out_channels in_channels bias = torch ones out_channels act = torch ones input_size linear_op act weight bias=None F linear act weight bias pattern_w_bias = _get_dynamo_graph linear_op act weight bias pattern_wo_bias = _get_dynamo_graph linear_op act weight pattern_w_bias pattern_wo_bias _supported_symmetric_quantized_operators - dict str list OperatorPatternType supported_operators dict str list OperatorPatternType = Both conv linear should able handle relu + hardtanh fusion since those clamp ops conv d torch nn Conv d torch nn ReLU torch nn Conv d F relu F conv d torch nn ReLU F conv d F relu linear torch nn Linear F linear add torch add adaptive_avg_pool d torch nn AdaptiveAvgPool d F adaptive_avg_pool d copy deepcopy supported_operators _get_supported_symmetric_config_and_operators - list OperatorConfig supported_config_and_operators list OperatorConfig = quantization_config get_symmetric_quantization_config get_symmetric_quantization_config is_qat=True get_symmetric_quantization_config is_per_channel=True get_symmetric_quantization_config is_per_channel=True is_qat=True ops = _supported_symmetric_quantized_operators supported_config_and_operators extend OperatorConfig quantization_config pattern_list pattern_list ops values copy deepcopy supported_config_and_operators functools lru_cache get_symmetric_quantization_config is_per_channel bool = False is_qat bool = False is_dynamic bool = False act_qmin int = - act_qmax int = weight_qmin int = - weight_qmax int = extra_args dict str Any = eps - is_qat is_dynamic act_observer_or_fake_quant_ctr = FakeQuantize dynamic_quant_observer = MovingAverageMinMaxObserver with_args averaging_constant= extra_args observer = dynamic_quant_observer act_observer_or_fake_quant_ctr = FusedMovingAvgObsFakeQuantize type ignore assignment is_dynamic act_observer_or_fake_quant_ctr = PlaceholderObserver type ignore assignment act_observer_or_fake_quant_ctr = HistogramObserver type ignore assignment act_quantization_spec = QuantizationSpec dtype=torch int quant_min=act_qmin quant_max=act_qmax qscheme=torch per_tensor_affine is_dynamic=is_dynamic observer_or_fake_quant_ctr=act_observer_or_fake_quant_ctr with_args extra_args weight_qscheme = torch per_channel_symmetric is_per_channel torch per_tensor_symmetric weight_observer_or_fake_quant_ctr _ObserverOrFakeQuantizeConstructor = MinMaxObserver is_qat TODO qat + per channel weight_observer_or_fake_quant_ctr = FusedMovingAvgObsFakeQuantize is_per_channel weight_observer_or_fake_quant_ctr = PerChannelMinMaxObserver extra_args dict str Any = eps - is_qat weight_qscheme == torch per_tensor_symmetric extra_args observer = MovingAverageMinMaxObserver extra_args observer = MovingAveragePerChannelMinMaxObserver type ignore dict-item weight_quantization_spec = QuantizationSpec dtype=torch int quant_min=weight_qmin quant_max=weight_qmax qscheme=weight_qscheme ch_axis= is_dynamic=False observer_or_fake_quant_ctr=weight_observer_or_fake_quant_ctr with_args extra_args bias_quantization_spec = None is_dynamic quantization_config = QuantizationConfig act_quantization_spec None weight_quantization_spec bias_quantization_spec is_qat quantization_config = QuantizationConfig act_quantization_spec act_quantization_spec weight_quantization_spec bias_quantization_spec is_qat quantization_config _get_supported_config_and_operators - list OperatorConfig _get_supported_symmetric_config_and_operators _get_module_type_filter tp Callable Get module_type_filter function given module type filter accepts node checks node comes module has certain module type For example node linear_op = call_function comes module type Block - Sub - Linear module_type_filter = _get_module_type_filter Sub submodule type ` Sub ` under ` Block ` submodule print module_type_filter node True node submodule ` Sub ` same ` Block ` ` Linear ` well tp_str = tp __module__ + + tp __qualname__ module_type_filter n Node - bool example L__self___sub L sub Sub L__self___sub_linear L sub linear torch nn modules linear Linear nn_module_stack = n meta get nn_module_stack types = _ t nn_module_stack values export returns str older APIs e g capture_pre_autograd_graph type Handle both cases isinstance t type t = t __module__ + + t __qualname__ types append t tp_str types module_type_filter _get_not_module_type_or_name_filter tp_list list Callable module_name_list list str - Callable Node bool module_type_filters = _get_module_type_filter tp tp tp_list module_name_list_filters = _get_module_name_filter m m module_name_list not_module_type_or_name_filter n Node - bool any f n f module_type_filters + module_name_list_filters not_module_type_or_name_filter compatibility is_backward_compatible=False typing_extensions deprecated XNNPACKQuantizer deprecated Please use xnnpack quantizer ExecuTorch https github com pytorch executorch tree main backends xnnpack quantizer instead XNNPACKQuantizer Quantizer DEPRECATED XNNPACKQuantizer marked deprecated It will removed future It has been moved executorch backends xnnpack quantizer xnnpack_quantizer XNNPACKQuantizer Please use new quantizer instead supported_config_and_operators = _get_supported_config_and_operators STATIC_QAT_ONLY_OPS = conv_bn_relu conv_bn conv_transpose_bn_relu conv_transpose_bn static quantization ops both PTQ QAT Preserve order fusions come before singular ops STATIC_OPS = linear_relu linear conv_relu conv conv_transpose_relu adaptive_avg_pool d TODO move BoltNNQuantizer gru_io_only add_relu add mul_relu mul cat DYNAMIC_OPS = linear __init__ - None super __init__ global_config Optional QuantizationConfig = None operator_type_config dict torch _ops OpOverloadPacket Optional QuantizationConfig = module_type_config dict Callable Optional QuantizationConfig = module_name_config dict str Optional QuantizationConfig = classmethod get_supported_quantization_configs cls - list QuantizationConfig op_configs set QuantizationConfig = spec spec _ cls supported_config_and_operators list op_configs classmethod get_supported_operator_for_quantization_config cls quantization_config Optional QuantizationConfig - list OperatorPatternType quantization_config None all_ops = _ ops cls supported_config_and_operators all_ops extend ops all_ops config ops cls supported_config_and_operators note assumes each entry cls supported_spec_and_operators corresponds one spec e g we don t have spec op_list spec op_list spec op_list where first second entry have same spec did merge op list config == quantization_config ops set_global quantization_config QuantizationConfig - XNNPACKQuantizer global_config = quantization_config set_operator_type operator_type torch _ops OpOverloadPacket quantization_config QuantizationConfig - XNNPACKQuantizer operator_type_config operator_type = quantization_config set_module_type module_type Callable quantization_config QuantizationConfig Set quantization_config submodule type ` module_type ` example quantizer set_module_name Sub quantizer set_module_name nn Linear will quantize all supported operator operator patterns submodule module type given ` quantization_config ` module_type_config module_type = quantization_config set_module_name module_name str quantization_config Optional QuantizationConfig Set quantization_config submodule name ` module_name ` example quantizer set_module_name blocks sub will quantize all supported operator operator patterns submodule module name given ` quantization_config ` assert quantization_config None quantization_config == None supported yet module_name_config module_name = quantization_config transform_for_annotation model torch fx GraphModule - torch fx GraphModule Transforms scalar values tensor attributes _convert_scalars_to_attrs model annotate model torch fx GraphModule - torch fx GraphModule just handling global spec now hacked handling dynamic linear quant will fix later global_config global_config input_activation is_dynamic type ignore union-attr model = _annotate_for_dynamic_quantization_config model model = _annotate_for_static_quantization_config model propagate_annotation model model _annotate_all_static_patterns model torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - torch fx GraphModule TODO implement support None canceling out previous annotations quantization_config None model quantization_config is_qat op STATIC_QAT_ONLY_OPS OP_TO_ANNOTATOR op model quantization_config filter_fn op STATIC_OPS OP_TO_ANNOTATOR op model quantization_config filter_fn model _annotate_all_dynamic_patterns model torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional Callable Node bool = None - torch fx GraphModule TODO implement support None canceling out previous annotations quantization_config None model op DYNAMIC_OPS OP_TO_ANNOTATOR op model quantization_config filter_fn model _annotate_for_static_quantization_config model torch fx GraphModule - torch fx GraphModule module_name_list = list module_name_config keys module_name config module_name_config items _annotate_all_static_patterns model config _get_module_name_filter module_name tp_list = list module_type_config keys module_type config module_type_config items _annotate_all_static_patterns model config _get_module_type_filter module_type _annotate_all_static_patterns model global_config _get_not_module_type_or_name_filter tp_list module_name_list model _annotate_for_dynamic_quantization_config model torch fx GraphModule - torch fx GraphModule module_name_list = list module_name_config keys module_name config module_name_config items _annotate_all_dynamic_patterns model config _get_module_name_filter module_name tp_list = list module_type_config keys module_type config module_type_config items _annotate_all_dynamic_patterns model config _get_module_type_filter module_type _annotate_all_dynamic_patterns model global_config _get_not_module_type_or_name_filter tp_list module_name_list model validate model torch fx GraphModule - None pass classmethod get_supported_operators cls - list OperatorConfig cls supported_config_and_operators