Copyright c Meta Platforms Inc affiliates Owner s oncall distributed pathlib tempfile unittest numpy testing assert_array_equal torch torch nn functional F torch distributed _functional_collectives AsyncCollectiveTensor torch distributed device_mesh init_device_mesh torch distributed tensor DeviceMesh distribute_tensor DTensor Partial Replicate Shard torch distributed tensor _api _shard_tensor torch distributed tensor _dtensor_spec DTensorSpec ShardOrderEntry TensorMeta torch distributed tensor debug CommDebugMode torch distributed tensor experimental implicit_replication torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel torch distributed tensor placement_types _StridedShard torch testing make_tensor torch testing _internal common_utils IS_FBCODE run_tests skipIfHpu torch testing _internal distributed _tensor common_dtensor create_local_tensor_test_class DTensorTestBase map_local_tensor_for_rank with_comms c d_functional = torch ops c d_functional DummyMLP torch nn Module __init__ device super __init__ net = torch nn Linear device=device relu = torch nn ReLU net = torch nn Linear device=device forward x net F relu net x reset_parameters args kwargs torch no_grad net weight fill_ net weight fill_ net bias fill_ net bias fill_ DTensorTest DTensorTestBase with_comms test_dtensor_constructor device_mesh = build_device_mesh placements = Shard local_tensor = torch randn requires_grad=True spec = DTensorSpec device_mesh tuple placements tensor_meta=TensorMeta torch Size world_size local_tensor stride local_tensor dtype dist_tensor = DTensor local_tensor spec requires_grad=True assertEqual dist_tensor size torch Size world_size assertWarnsRegex UserWarning To construct DTensor local_tensor spec requires_grad=False with_comms test_meta_dtensor device_mesh = build_device_mesh dist_specs = Shard Replicate meta_tensor = torch randn device= meta dist_spec dist_specs Test distribute_tensor meta tensor meta_dtensor = distribute_tensor meta_tensor device_mesh dist_spec assertTrue meta_dtensor is_meta meta_dtensor = torch empty_like meta_dtensor device=self device_type torch nn init constant_ meta_dtensor value_tensor = torch empty_like meta_dtensor to_local fill_ assertFalse meta_dtensor is_meta assertEqual meta_dtensor device type device_type assertEqual meta_dtensor to_local value_tensor Test from_local meta tensor meta_dtensor = DTensor from_local meta_tensor device_mesh dist_spec meta_dtensor = torch empty_like meta_dtensor device=self device_type torch nn init constant_ meta_dtensor assertEqual meta_dtensor device type device_type value_tensor = torch empty_like meta_dtensor to_local fill_ assertEqual meta_dtensor to_local value_tensor with_comms test_modules_w_meta_dtensor model = DummyMLP meta device_mesh = build_device_mesh parallelize_plan = net ColwiseParallel net RowwiseParallel model_tp = parallelize_module model device_mesh parallelize_plan model_tp to_empty device=self device_type model_tp reset_parameters optim = torch optim SGD model_tp parameters lr= model_regular = DummyMLP device_type model_regular_tp = parallelize_module model_regular device_mesh parallelize_plan optim_regular = torch optim SGD model_regular_tp parameters lr= model_regular_tp reset_parameters torch manual_seed inp = torch randn device=self device_type output = model_tp inp output_regular = model_regular_tp inp assertEqual output output_regular output sum backward output_regular sum backward optim step optim_regular step torch manual_seed inp = torch randn device=self device_type assertEqual model_tp inp model_regular_tp inp with_comms test_dtensor_stride device_mesh = build_device_mesh shard _spec = Shard local_tensor = torch randn dist_tensor = DTensor from_local local_tensor device_mesh shard _spec won t affect stride assertEqual dist_tensor stride shard _spec = Shard local_tensor = torch randn dist_tensor = DTensor from_local local_tensor device_mesh shard _spec will affect stride after DT initialized assertEqual dist_tensor stride world_size initialized transposed mat local_tensor = torch randn local_tensor_t = local_tensor permute assertEqual local_tensor_t stride dist_tensor = DTensor from_local local_tensor_t device_mesh shard _spec global_stride = world_size world_size assertEqual dist_tensor stride global_stride with_comms test_from_local device_mesh = build_device_mesh shard_spec = Shard local_tensor = torch randn sharded_tensor = DTensor from_local local_tensor device_mesh shard_spec assertEqual sharded_tensor size torch Size world_size replica_spec = Replicate ddp_tensor = DTensor from_local local_tensor device_mesh replica_spec assertEqual ddp_tensor size local_tensor size partial_spec = Partial partial_tensor = DTensor from_local local_tensor device_mesh partial_spec assertEqual partial_tensor size local_tensor size test dist tensor works torch Tensor during backwards local_tensor_with_grad = torch randn requires_grad=True do some operations local tensor local_tensor_temp = local_tensor_with_grad create dist tensor non leaf local tensor dist tensor created should also non leaf node dist_tensor = DTensor from_local local_tensor_temp device_mesh shard_spec assertFalse dist_tensor is_leaf do some random operations dist tensor output = dist_tensor assertIsInstance output DTensor trigger backward dist tensor directly local_grad = torch ones grad_output = DTensor from_local local_grad device_mesh shard_spec run backward directly dist tensor output backward grad_output check gradients flow back original torch Tensor assertIsNotNone local_tensor_with_grad grad expected_grad = torch ones assertEqual local_tensor_with_grad grad expected_grad DTensor from_local should raise error ` local_tensor ` argument DTensor local_tensor = torch ones dtensor = DTensor from_local local_tensor device_mesh shard_spec assertRaisesRegex RuntimeError local_tensor argument only accepts torch Tensor DTensor from_local dtensor device_mesh shard_spec with_comms test_from_local_uneven_sharding device_mesh = build_device_mesh uneven_dim _size = world_size + global_tensor = torch randn uneven_dim _size shard_placement = Shard tensor_list _ = shard_placement _split_tensor global_tensor device_mesh size mesh_dim= with_padding=False contiguous=True dtensor = DTensor from_local map_local_tensor_for_rank tensor_list rank lambda tl r tl r device_mesh Shard shape=global_tensor size stride=global_tensor stride assertEqual dtensor size global_tensor size assertEqual dtensor stride global_tensor stride with_comms test_from_local_uneven_sharding_raise_error device_mesh = build_device_mesh uneven_dim _size = world_size + global_tensor = torch randn uneven_dim _size shard_placement = Shard tensor_list _ = shard_placement _split_tensor global_tensor device_mesh size mesh_dim= with_padding=False contiguous=True assertRaisesRegex RuntimeError Please pass both shape stride same time DTensor from_local map_local_tensor_for_rank tensor_list rank lambda tl r tl r device_mesh Shard shape=global_tensor size assertRaisesRegex RuntimeError Please pass both shape stride same time DTensor from_local map_local_tensor_for_rank tensor_list rank lambda tl r tl r device_mesh Shard stride=global_tensor stride with_comms test_from_local_negative_dim device_mesh = build_device_mesh placements = Shard - local_tensor = torch randn sharded_tensor = DTensor from_local local_tensor device_mesh placements assertEqual sharded_tensor placements dim with_comms test_to_local device_mesh = build_device_mesh placements = Shard local_tensor_with_grad = torch randn device=self device_type requires_grad=True dist_tensor_shape = torch Size world_size spec = DTensorSpec mesh=device_mesh placements=placements tensor_meta=TensorMeta dist_tensor_shape local_tensor_with_grad stride local_tensor_with_grad dtype sharded_tensor = DTensor local_tensor_with_grad spec requires_grad=True assertEqual sharded_tensor size dist_tensor_shape assertEqual sharded_tensor to_local local_tensor_with_grad test dist tensor works torch Tensor during backwards dist tensor created leaf node do some operation dist tensor temp_st = sharded_tensor do some operation local tensor dist tensor new_tensor_with_grad = torch randn device=self device_type requires_grad=True res = temp_st to_local + new_tensor_with_grad call backward directly torch Tensor see works propagating through dist tensor res sum backward assertIsNotNone sharded_tensor grad assertEqual sharded_tensor grad to_local torch ones test case when grad stride different fwd input res = sharded_tensor to_local model = torch nn ReLU res register_hook lambda grad grad t target = torch randn device=self device_type mae_loss = torch nn L Loss output = mae_loss model res target The manual change grad stride leads failure copy op afterwards so we need try-catch here try output backward except RuntimeError assertEqual sharded_tensor grad stride world_size test case under no-grad we directly local tensor torch no_grad local_no_grad = sharded_tensor to_local assert local_no_grad sharded_tensor _local_tensor with_comms test_to_local_grad_hint device_mesh = build_device_mesh placements = Shard global_tensor = torch ones requires_grad=True sharded_dtensor = distribute_tensor global_tensor device_mesh placements comm_mode = CommDebugMode comm_mode local_out = sharded_dtensor redistribute placements= Replicate to_local grad_placements= Partial local_out backward torch ones_like local_out assertEqual comm_mode comm_counts c d_functional all_gather_into_tensor assertEqual comm_mode comm_counts c d_functional reduce_scatter_tensor replica_grad = sharded_dtensor grad full_tensor assertEqual replica_grad global_tensor world_size with_comms test_full_tensor_sync device_mesh = build_device_mesh placements = Shard global_tensor = torch ones requires_grad=True sharded_dtensor = distribute_tensor global_tensor device_mesh placements full_out = sharded_dtensor full_tensor assertFalse isinstance full_out AsyncCollectiveTensor assertEqual full_out global_tensor with_comms test_full_tensor_grad_hint device_mesh = build_device_mesh placements = Shard global_tensor = torch ones requires_grad=True sharded_dtensor = distribute_tensor global_tensor device_mesh placements local_out = sharded_dtensor full_tensor grad_placements= Partial local_out sum backward replica_grad = sharded_dtensor grad full_tensor assertEqual replica_grad global_tensor world_size with_comms test_dtensor_new_empty_strided device_mesh = build_device_mesh local_tensor = torch randn requires_grad=True device=self device_type my_dtensor = distribute_tensor local_tensor device_mesh Shard new_strided_dtensor = my_dtensor new_empty_strided requires_grad=True test op produces new dtensor autograd works assertEqual new_strided_dtensor shape my_dtensor shape new_strided_dtensor sum backward assertIsNotNone new_strided_dtensor grad assertIsInstance new_strided_dtensor grad DTensor test backward new_empty_strided sharding works correctly my_dtensor to_local sum backward local_tensor sum backward assertEqual my_dtensor grad new_strided_dtensor grad assertEqual my_dtensor grad redistribute placements= Replicate to_local local_tensor grad with_comms test_dtensor_async_output Tests output some dtensor operations isn t used any compute output should AsyncCollectiveTensor representing fact we haven t synced collective yet mesh = build_device_mesh fn dt dt_out_redistribute = dt redistribute mesh Replicate async_op=True Make sure we haven t synced yet TODO figure out why returning None assertTrue _tensor_needs_wait dt_out_redistribute dt_out_redistribute_view = dt_out_redistribute view dt_out_redistribute shape local_tensor = dt_out_redistribute_view to_local local_tensor x = torch ones device=self device_type dt = distribute_tensor x mesh Shard out = fn dt Make sure we haven t synced yet assertEqual type out AsyncCollectiveTensor assertFalse out completed out_view = out view - Assert output ` AsyncCollectiveTensor ` assertEqual type out_view AsyncCollectiveTensor assertFalse out completed Use data requiring sync ref = torch ones device=self device_type + ref = ref view - out_data = out_view + assertEqual type out_data torch Tensor assertEqual out_data ref test async_op = False default sync_out = dt redistribute mesh Replicate assertFalse isinstance sync_out AsyncCollectiveTensor assertEqual sync_out to_local x with_comms test_from_local_then_to_local test ensure end end torch Tensor - dist tensor - torch Tensor works device_mesh = build_device_mesh placements = Shard step construct construct local tensor local_tensor_with_grad = torch randn device=self device_type requires_grad=True do some operations local tensor local_tensor_temp = local_tensor_with_grad + step create dist tensor non leaf local tensor dist tensor created should also non leaf node dist_tensor = DTensor from_local local_tensor_temp device_mesh placements assertFalse dist_tensor is_leaf do some random operations dist tensor output = dist_tensor assertIsInstance output DTensor step do some operation local tensor dist tensor new_tensor_with_grad = torch randn device=self device_type requires_grad=True res = output to_local + new_tensor_with_grad call backward directly torch Tensor see works propagating all way back original torch Tensor res sum backward assertIsNotNone local_tensor_with_grad grad expected_grad = torch ones assertEqual local_tensor_with_grad grad expected_grad with_comms test_dtensor_spec_read_only_after_set device_mesh = build_device_mesh placements = Shard local_tensor = torch randn sharded_tensor = DTensor from_local local_tensor device_mesh placements modify placements dist_tensor s spec should changed placements = Replicate assertTrue sharded_tensor placements placements assertNotEqual sharded_tensor placements placements with_comms test_dtensor_spec_hash device_mesh = build_device_mesh placements = Shard local_tensor = torch randn local_tensor = torch randn sharded_tensor = DTensor from_local local_tensor device_mesh placements sharded_tensor = DTensor from_local local_tensor device_mesh placements note DTensorSpec without real tensor data so hash would same long mesh placements tensor properties same assertEqual hash sharded_tensor _spec hash sharded_tensor _spec change placements would change hash local_tensor = torch ones replica_spec = Replicate replica_tensor = DTensor from_local local_tensor device_mesh replica_spec run_check=False assertNotEqual hash sharded_tensor _spec hash replica_tensor _spec with_comms test_dtensor_properties device_mesh = build_device_mesh placements = Shard local_tensor = torch randn sharded_tensor = DTensor from_local local_tensor device_mesh placements assertEqual sharded_tensor device type device_type with_comms test_dtensor_save_load io device_mesh = build_device_mesh placements = Shard local_tensor = torch randn sharded_tensor = DTensor from_local local_tensor device_mesh placements buffer = io BytesIO torch save sharded_tensor buffer buffer seek reloaded_st = torch load buffer weights_only=False assertEqual sharded_tensor reloaded_st buffer seek reloaded_st = torch load buffer weights_only=True assertEqual sharded_tensor reloaded_st skipIfHpu with_comms unittest skipIf IS_FBCODE subprocess torch fails ModuleNotFoundError No module named torch fbcode test_dtensor_save_load_import should_import True False device_mesh = build_device_mesh placements = Shard local_tensor = torch randn sharded_tensor = DTensor from_local local_tensor device_mesh placements tempfile NamedTemporaryFile f torch save sharded_tensor f import_string = torch distributed tensor should_import filename = pathlib Path f name err_msg = _pickle UnpicklingError Weights only load failed ` ` torch distributed tensor ` ` must imported load DTensors should_import None _attempt_load_from_subprocess filename import_string err_msg with_comms test_shard_tensor ws = world_size device_mesh = build_device_mesh full_tensor = torch arange ws ws reshape ws ws Shard row placements = Shard sharded_tensor = _shard_tensor full_tensor placements device_mesh assertEqual sharded_tensor size torch Size ws ws assertEqual sharded_tensor placements placements local_tensor = sharded_tensor to_local assertEqual local_tensor map_local_tensor_for_rank full_tensor rank lambda ft r ft range r r + Shard column placements = Shard sharded_tensor = _shard_tensor full_tensor placements device_mesh assertEqual sharded_tensor size torch Size ws ws assertEqual sharded_tensor placements placements local_tensor = sharded_tensor to_local assertEqual local_tensor map_local_tensor_for_rank full_tensor rank lambda ft r ft range r r + assert full tensor changed assertEqual full_tensor torch arange ws ws reshape ws ws with_comms test_shard_tensor_ d ws = world_size full_tensor = torch arange ws reshape ws device_mesh = DeviceMesh device_type full_tensor Shard row column placements = Shard Shard sharded_tensor = _shard_tensor full_tensor placements device_mesh assertEqual sharded_tensor size torch Size ws assertEqual sharded_tensor placements placements local_tensor = sharded_tensor to_local assertEqual local_tensor item rank DTensorTestWithLocalTensor = create_local_tensor_test_class DTensorTest skipped_tests= Async output local mode supported test_dtensor_async_output Disabling saving loading local mode since requires deeper integration test_dtensor_save_load test_dtensor_save_load_import DTensorMeshTest DTensorTestBase property world_size sub_mesh_assert_equal mesh exp_in_mesh exp_out_of_mesh tensor rank mesh assertEqual tensor exp_in_mesh assertEqual tensor exp_out_of_mesh with_comms test_dtensor_device_mesh_device_conversion construct cuda device mesh mesh = build_device_mesh construct cpu local tensor cuda device mesh should automatically convert dist tensor cuda placements = Shard local_tensor = torch randn dist_tensor = DTensor from_local local_tensor mesh placements assertEqual dist_tensor device type device_type assertEqual dist_tensor to_local device type device_type with_comms test_dtensor_api_device_mesh_context_manager build_device_mesh mesh placements = Shard local_tensor = torch randn sharded_tensor = DTensor from_local local_tensor device_mesh=mesh placements=placements build_device_mesh placements = Shard local_tensor = torch randn sharded_tensor = DTensor from_local local_tensor placements=placements replica_spec = Replicate replica_tensor = sharded_tensor redistribute placements=replica_spec assertEqual replica_tensor size torch Size world_size build_device_mesh placements = Shard global_shape = torch Size world_size global_tensor = torch randn global_shape sharded_tensor = distribute_tensor global_tensor placements=placements assertEqual sharded_tensor to_local shape torch Size mesh_ d = DeviceMesh device_type torch arange world_size reshape mesh_ d shard_ d_spec = Shard Replicate tensor_ d = distribute_tensor global_tensor placements=shard_ d_spec assertEqual tensor_ d to_local shape torch Size sharded_after_ d = distribute_tensor global_tensor placements=placements assertEqual sharded_after_ d to_local shape torch Size with_comms test_dtensor_ d_mesh mesh_tensor = torch arange world_size reshape construct cuda device mesh mesh = DeviceMesh device_type mesh_tensor construct dist tensor d device mesh test works placements = Shard Shard local_tensor = torch randn dist_tensor = DTensor from_local local_tensor mesh placements assertEqual dist_tensor size torch Size mesh size mesh size assertEqual dist_tensor device type device_type assertEqual dist_tensor to_local device type device_type shard same tensor dimension we should correctly construct global tensor size shard_same_dim_spec = Shard Shard local_tensor = torch randn dist_tensor = DTensor from_local local_tensor mesh shard_same_dim_spec assertEqual dist_tensor size torch Size world_size with_comms test_device_mesh_nd construct cuda device mesh mesh_tensor = torch arange world_size reshape mesh = DeviceMesh device_type mesh_tensor construct dist tensor d device mesh test works placements = Shard Shard Shard local_tensor = torch randn dist_tensor = DTensor from_local local_tensor mesh placements assertEqual dist_tensor size torch Size assertEqual dist_tensor device type device_type assertEqual dist_tensor to_local device type device_type construct dist tensor d device mesh some shards same dim placements = Shard Shard Shard local_tensor = torch randn dist_tensor = DTensor from_local local_tensor mesh placements assertEqual dist_tensor size torch Size assertEqual dist_tensor device type device_type assertEqual dist_tensor to_local device type device_type with_comms test_dtensor_spec_local_shard_offset device_mesh = DeviceMesh device_type torch arange world_size reshape tensor_shape = world_size world_size sharding specs its corresponding local shard offsets shard_spec_and_offsets = Shard Replicate world_size rank Shard Replicate world_size rank Replicate Shard world_size rank Replicate Shard world_size rank torch distributed tensor _utils compute_local_shape_and_global_offset loop through all sharding specs check local shard offsets logical_tensor = torch randn tensor_shape placements expected_shard_offsets shard_spec_and_offsets dtensor = distribute_tensor logical_tensor device_mesh placements _ offset = compute_local_shape_and_global_offset dtensor shape device_mesh dtensor placements assertEqual expected_shard_offsets offset with_comms test_from_local_sub_mesh mesh = DeviceMesh device_type local_tensor = torch ones dtensor = DTensor from_local local_tensor mesh Shard assertEqual dtensor size torch Size sub_mesh_assert_equal mesh mesh torch ones torch tensor dtensor to_local test dtensor created submesh operation should only applied local shard inside mesh whole world so only really run computation dtensor = dtensor + sub_mesh_assert_equal mesh mesh torch ones + torch tensor dtensor to_local with_comms test_default_value_sub_mesh mesh = DeviceMesh device_type test scalar value local_tensor = torch ones local_tensor = torch ones dtensor = DTensor from_local local_tensor mesh Shard dtensor = DTensor from_local local_tensor mesh Shard local_res = dtensor equal dtensor equal returns local result sub_mesh_assert_equal mesh mesh True True local_res test -d tensor value local_tensor = torch ones dtensor = DTensor from_local local_tensor mesh Shard sum sub_mesh_assert_equal mesh mesh torch tensor torch tensor dtensor to_local test List torch Tensor value local_tensor = torch ones dtensor = DTensor from_local local_tensor mesh Shard dtensor_list = dtensor split dim= sub_mesh_assert_equal mesh mesh torch ones torch tensor dt to_local dt dtensor_list with_comms test_redistribute_sub_mesh mesh = DeviceMesh device_type test redistribute submesh local_tensor = torch ones sharded_dtensor = DTensor from_local local_tensor mesh Shard replicated_dtensor = sharded_dtensor redistribute placements= Replicate sub_mesh_assert_equal mesh mesh torch ones torch tensor replicated_dtensor to_local sharded_again = replicated_dtensor redistribute placements= Shard sub_mesh_assert_equal mesh mesh torch ones torch tensor sharded_again to_local with_comms test_implicit_replication mesh = build_device_mesh local_tensor = torch ones sharded_dtensor = DTensor from_local local_tensor mesh Shard implicit_replication We put scalar tensor left operand so we can test out when non-dtensor arg args list out_dt = torch ones device=self device_type + sharded_dtensor assertEqual out_dt placements Shard assertEqual out_dt shape world_size local_shard = out_dt to_local assertEqual local_shard shape assertEqual local_shard torch ones + torch ones with_comms test_vmap_embedding mesh = build_device_mesh batch_size seq_len = output_dim = indices = torch zeros batch_size seq_len dtype=torch int indices = indices = indices = indices = DTensor from_local indices mesh Shard emb = torch randn batch_size output_dim dtype=torch float emb = DTensor from_local emb mesh Shard result = torch vmap F embedding indices emb expected = F embedding indices i emb i i range batch_size expected = torch stack expected local_result = result to_local local_expected = expected to_local assertEqual local_result local_expected unittest expectedFailure with_comms test_inplace_on_local_tensor_view mesh = build_device_mesh seq = vocab = leaf = torch randn seq vocab device=self device_type requires_grad=True dtensor_leaf = DTensor from_local leaf mesh Shard dtensor_vocab_parallel_logits = dtensor_leaf make non-leaf vocab_parallel_logits = dtensor_vocab_parallel_logits to_local logits_max = torch randn seq device=self device_type vocab_parallel_logits -= logits_max unsqueeze dim= with_comms test_auto_implicit_replication mesh = build_device_mesh local_tensor = torch ones world_size device=self device_type sharded_dtensor = DTensor from_local local_tensor mesh Shard automatically turn tensor DTensor replicate when ndim = numel = ndim_ _tensor = torch tensor device=self device_type add_scalar_tensor_with_dtensor ndim_ _tensor + sharded_dtensor result = add_scalar_tensor_with_dtensor to_local assertEqual result local_tensor + ndim_ _tensor assertNotWarn add_scalar_tensor_with_dtensor Found non-scalar tensor numel= ndim = automatically turn tensor DTensor replicate when ndim = numel = numel_ _tensor = torch tensor device=self device_type assertEqual numel_ _tensor + sharded_dtensor to_local numel_ _tensor + local_tensor unittest expectedFailure with_comms test_dtensor_cond mesh = build_device_mesh make_dtensor shape dtype device distribute_tensor make_tensor shape dtype=dtype device=device device_mesh=mesh placements=None x = make_dtensor dtype=torch bfloat device= cuda Fails AssertionError P torch cond x lambda x lambda torch zeros_like x with_comms test_metadata_consistency_check device_mesh = build_device_mesh placements = Shard Create local tensor specific metadata check dtype change local_tensor = torch randn requires_grad=True dtype=torch float rank == local_tensor = local_tensor dtype=torch float assertRaises ValueError DTensor from_local local_tensor device_mesh placements run_check=True try DTensor from_local local_tensor device_mesh placements run_check=False except ValueError fail Unexpected ValueError raised run_check=False Create local tensor specific metadata check requires_grad change local_tensor = torch randn requires_grad=True dtype=torch float rank == local_tensor requires_grad = False assertRaises ValueError DTensor from_local local_tensor device_mesh placements run_check=True try DTensor from_local local_tensor device_mesh placements run_check=False except ValueError fail Unexpected ValueError raised run_check=False Create local tensor specific metadata check stride change local_tensor = torch randn requires_grad=True dtype=torch float rank == local_tensor = local_tensor t transpose changes stride assertRaises ValueError DTensor from_local local_tensor device_mesh placements run_check=True try DTensor from_local local_tensor device_mesh placements run_check=False except ValueError fail Unexpected ValueError raised run_check=False DTensorMeshTestWithLocalTensor = create_local_tensor_test_class DTensorMeshTest skipped_tests= Test asserts must rewritten local tensor test_from_local_sub_mesh test_default_value_sub_mesh test_redistribute_sub_mesh Local tensor mode doesn t support tensors different types different ranks test_metadata_consistency_check TestDTensorPlacementTypes DTensorTestBase property world_size _create_tensor size Keep everything deterministic torch manual_seed tensor = torch rand size device_type == cuda tensor cuda tensor with_comms test_split_tensor_ D - None mesh = build_device_mesh shard_placement = Shard size range tensor = _create_tensor size splitted_tensor_list pad_sizes = shard_placement _split_tensor tensor mesh size with_padding=True contiguous=True size == when tensor size there no padding needed all ranks expected_pad_sizes = world_size assert_array_equal expected_pad_sizes pad_sizes is_tensor_empty = splitted_tensor numel splitted_tensor splitted_tensor_list expected_is_tensor_empty = True world_size assert_array_equal expected_is_tensor_empty is_tensor_empty expected_pad_sizes = idx size idx _ enumerate range world_size assert_array_equal expected_pad_sizes pad_sizes torch distributed tensor _collective_utils unpad_tensor unpadded_list = unpad_tensor tensor shard_placement dim pad_sizes i pad_sizes i tensor i tensor enumerate splitted_tensor_list expected_is_tensor_empty = idx size idx _ enumerate range world_size is_tensor_empty = unpadded_tensor numel unpadded_tensor unpadded_list assert_array_equal expected_is_tensor_empty is_tensor_empty TestDTensorPlacementTypesWithLocalTensor = create_local_tensor_test_class TestDTensorPlacementTypes TestDTensorSpec DTensorTestBase property world_size test_dtensor_spec_print assertExpectedInline DTensorSpec format_shard_order_str Shard Shard Shard None S S S assertExpectedInline DTensorSpec format_shard_order_str Shard Shard Shard ShardOrderEntry tensor_dim= mesh_dims= ShardOrderEntry tensor_dim= mesh_dims= ShardOrderEntry tensor_dim= mesh_dims= S S S assertExpectedInline DTensorSpec format_shard_order_str Shard Shard Shard ShardOrderEntry tensor_dim= mesh_dims= S S S assertExpectedInline DTensorSpec format_shard_order_str Replicate Replicate Replicate None RRR assertExpectedInline DTensorSpec format_shard_order_str Replicate Replicate Shard None RRS with_comms test_dtensor_spec_with_invalid_shard_order mesh_shape = world_size mesh = init_device_mesh device_type mesh_shape tensor_local = torch randn device=self device_type tensor_global = DTensor from_local tensor_local mesh Shard Shard Shard tensor_global _spec shard_order = ShardOrderEntry tensor_dim= mesh_dims= ShardOrderEntry tensor_dim= mesh_dims= assertRaisesRegex AssertionError r shard_order has empty mesh dim tensor_global _spec shard_order = ShardOrderEntry tensor_dim= mesh_dims= ShardOrderEntry tensor_dim= mesh_dims= assertRaisesRegex AssertionError tensor dim should sorted shard_order tensor_global _spec shard_order = ShardOrderEntry tensor_dim= mesh_dims= ShardOrderEntry tensor_dim= mesh_dims= assertRaisesRegex AssertionError r placement\ \d+\ doesn t have matching shard shard_order tensor_global _spec shard_order = ShardOrderEntry tensor_dim= mesh_dims= ShardOrderEntry tensor_dim= mesh_dims= assertRaisesRegex AssertionError r shard_order has invalid mesh dim \ \d +\ tensor_global _spec shard_order = ShardOrderEntry tensor_dim= mesh_dims= ShardOrderEntry tensor_dim= mesh_dims= assertRaisesRegex AssertionError r shard_order has invalid tensor dim - \d+ tensor_global _spec shard_order = ShardOrderEntry tensor_dim= mesh_dims= ShardOrderEntry tensor_dim=- mesh_dims= with_comms test_dtensor_spec_update mesh_shape = world_size mesh = init_device_mesh device_type mesh_shape tensor_local = torch randn device=self device_type tensor_global_ = DTensor from_local tensor_local mesh Shard Shard Shard tensor_global_ = DTensor from_local tensor_local mesh Shard Shard Shard assertNotEqual id tensor_global_ id tensor_global_ assertEqual hash tensor_global_ _spec hash tensor_global_ _spec assertEqual tensor_global_ _spec tensor_global_ _spec using default shard_order tensor_global_ _spec shard_order = ShardOrderEntry tensor_dim= mesh_dims= ShardOrderEntry tensor_dim= mesh_dims= hash should recomputed DTensorSpec __setattr__ assertNotEqual hash tensor_global_ _spec hash tensor_global_ _spec assertNotEqual tensor_global_ _spec tensor_global_ _spec with_comms test_dtensor_spec_default_shard_order_generation mesh_shape = world_size mesh = init_device_mesh device_type mesh_shape tensor_local = torch randn device=self device_type tensor_global = DTensor from_local tensor_local mesh Shard Shard Shard assertEqual tensor_global _spec shard_order ShardOrderEntry tensor_dim= mesh_dims= ShardOrderEntry tensor_dim= mesh_dims= tensor_global = DTensor from_local tensor_local mesh Replicate Replicate Replicate assertEqual tensor_global _spec shard_order shard order omit partial tensor_global = DTensor from_local tensor_local mesh Partial Replicate Replicate assertEqual tensor_global _spec shard_order shard_order doesn t work _StridedShard tensor_global = DTensor from_local tensor_local mesh Replicate _StridedShard split_factor= Shard assertEqual tensor_global _spec shard_order with_comms test_default_shard_order mesh_shape = world_size mesh = init_device_mesh device_type mesh_shape tensor_local = torch randn device=self device_type tensor_global = DTensor from_local tensor_local mesh Shard Shard Shard DTensorSpec automatically builds default left-to-right order assertEqual tensor_global _spec shard_order ShardOrderEntry tensor_dim= mesh_dims= ShardOrderEntry tensor_dim= mesh_dims= assertTrue DTensorSpec is_default_device_order tensor_global _spec shard_order manually set shard_order exchange mesh dim tensor_global _spec shard_order = ShardOrderEntry tensor_dim= mesh_dims= ShardOrderEntry tensor_dim= mesh_dims= assertFalse DTensorSpec is_default_device_order tensor_global _spec shard_order TestDTensorSpecWithLocalTensor = create_local_tensor_test_class TestDTensorSpec __name__ == __main__ run_tests