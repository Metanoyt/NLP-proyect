mypy allow-untyped-defs copy functools hashlib heapq itertools logging math operator os os path re collections defaultdict collections abc Callable dataclasses dataclass replace typing Any Optional TYPE_CHECKING Union torch torch _inductor inductor_prims torch distributed torch fx fx torch utils _pytree pytree torch _dynamo utils counters is_node_meta_valid torch _functorch _activation_checkpointing ac_logging_utils create_structured_trace_for_min_cut_info torch _inductor config inductor_config torch _logging trace_structured torch _subclasses fake_tensor extract_tensor_metadata torch fx experimental _backward_state BackwardState torch fx experimental proxy_tensor is_sym_node py_sym_types torch fx experimental sym_node magic_methods method_to_operator torch fx experimental symbolic_shapes find_symbol_binding_fx_nodes free_symbols hint_int is_symbol_binding_fx_node statically_known_false statically_known_true torch fx passes graph_drawer torch utils _ordered_set OrderedSet torch utils checkpoint CheckpointPolicy config _activation_checkpointing graph_info_provider GraphInfoProvider _activation_checkpointing knapsack dp_knapsack greedy_knapsack ilp_knapsack _activation_checkpointing knapsack_evaluator KnapsackEvaluator _aot_autograd descriptors AOTOutput SavedForBackwardsAOTOutput _aot_autograd logging_utils get_aot_graph_name _aot_autograd utils get_cuda_generator_meta_val is_with_effects compile_utils fx_graph_cse get_aten_target raise_getitems TYPE_CHECKING sympy AOT_PARTITIONER_DEBUG bool = config debug_partitioner log logging Logger = logging getLogger __name__ aten = torch ops aten prims = torch ops prims dataclass OpTypes Class keeping track different operator categories fusible_ops OrderedSet Callable compute_intensive_ops OrderedSet Callable random_ops OrderedSet Callable view_ops OrderedSet Callable recomputable_ops OrderedSet Callable is_fusible node fx Node get_aten_target node fusible_ops is_compute_intensive node fx Node get_aten_target node compute_intensive_ops is_random node fx Node get_aten_target node random_ops is_view node fx Node get_aten_target node view_ops is_recomputable node fx Node get_aten_target node recomputable_ops dataclass NodeInfo Be careful about iterating over these explicitly their order may deterministic inputs list fx Node _required_fw_nodes OrderedSet fx Node required_bw_nodes OrderedSet fx Node unclaimed_nodes OrderedSet fx Node fw_order dict fx Node int Effectively maps which our primals parameters static_lifetime_input_nodes OrderedSet fx Node functools cached_property required_fw_nodes - list fx Node sorted n n _required_fw_nodes key=lambda n fw_order n is_required_fw n fx Node - bool n _required_fw_nodes is_required_bw n fx Node - bool n required_bw_nodes is_unclaimed n fx Node - bool n unclaimed_nodes get_fw_order n fx Node - int assert n _required_fw_nodes f Node n fw nodes fw_order n dataclass MinCutOptions ban_if_used_far_apart bool ban_if_long_fusible_chains bool ban_if_materialized_backward bool ban_if_not_in_allowlist bool ban_if_reduction bool must_recompute node fx Node - bool node meta get recompute None CheckpointPolicy MUST_RECOMPUTE CheckpointPolicy PREFER_RECOMPUTE has_recomputable_ops fx_g fx GraphModule - bool node fx_g graph nodes must_recompute node True False has_recomputable_rng_ops fx_g fx GraphModule - bool node fx_g graph nodes must_recompute node hasattr node target tags torch Tag nondeterministic_seeded node target tags True False sym_node_size node fx Node - int isinstance node meta val torch SymInt torch SymBool assert isinstance node meta val torch SymFloat InvalidNodeBase __repr__ Invalid Node InvalidNode = InvalidNodeBase _extract_graph_with_inputs_outputs joint_graph fx Graph inputs list fx Node outputs list fx Node outputs_descs list AOTOutput subgraph Optional str = None ignore_must_be_in_fw_bw bool = False - fx Graph Given graph extracts out subgraph takes specified nodes inputs returns specified outputs This includes specifying non-placeholder nodes inputs The general strategy initialize all inputs proxies we encounter them trace through graph only keeping values which take valid proxies Then all dead code eliminated new_graph = fx Graph env = Add new placeholder nodes order specified inputs node inputs new_node = new_graph placeholder node name Can t use node_copy here we may turning previous call_function into placeholders new_node meta = node meta pyrefly ignore unsupported-operation env node = new_node node joint_graph nodes ignore_must_be_in_fw_bw _must_be_in_backward node subgraph = backward node inputs env node = InvalidNode type ignore assignment continue _must_be_in_forward node subgraph = forward node inputs env node = InvalidNode type ignore assignment continue node env Node must one our inputs Any member env which wasn t input start must have been created loop won t joint_graph nodes continue node op == placeholder env node = InvalidNode type ignore assignment node op == call_function all_args = pytree arg_tree_leaves node args node kwargs all_args = isinstance env x InvalidNodeBase x all_args isinstance x fx Node any all_args env node = InvalidNode type ignore assignment continue pyrefly ignore unsupported-operation bad-argument-type env node = new_graph node_copy node lambda x env x node op == get_attr pyrefly ignore unsupported-operation bad-argument-type env node = new_graph node_copy node lambda x env x node op == output pass output_values = x outputs isinstance x fx Node x env raise RuntimeError f Node x couldn t found env assert isinstance env x InvalidNodeBase f Node x invalid output output_values append env x output_values append x out = new_graph output tuple output_values out meta desc = outputs_descs new_graph eliminate_dead_code new_graph lint new_graph _is_primal node fx Node - bool node op == placeholder tangents str node target _is_bwd_seed_offset node _is_fwd_seed_offset node _is_tangent node fx Node - bool node op == placeholder tangents str node target _is_bwd_seed_offset node fx Node - bool node op == placeholder bwd_seed str node target bwd_base_offset str node target _is_fwd_seed_offset node fx Node - bool node op == placeholder fwd_seed str node target fwd_base_offset str node target _is_backward_state node fx Node - bool node op == placeholder isinstance node meta get val BackwardState _has_tag_is_backward node fx Node - bool node meta get partitioner_tag None == is_backward _has_tag_must_be_in_forward node fx Node - bool node meta get partitioner_tag None == must_be_in_forward _has_tag_must_be_in_backward node fx Node - bool node meta get partitioner_tag None == must_be_in_backward _must_be_in_forward node fx Node - bool _has_tag_must_be_in_forward node True is_mutable = is_with_effects node isinstance node target torch _ops OpOverload node target _schema is_mutable _has_tag_is_backward node _has_tag_must_be_in_backward node is_mutable _must_be_in_backward node fx Node - bool _has_tag_must_be_in_backward node True is_mutable = is_with_effects node isinstance node target torch _ops OpOverload node target _schema is_mutable _has_tag_is_backward node is_mutable _extract_fwd_bwd_outputs joint_module fx GraphModule num_fwd_outputs - tuple list fx Node list fx Node list AOTOutput list AOTOutput outputs = pytree arg_tree_leaves node args node joint_module graph find_nodes op= output outputs_descs = pytree arg_tree_leaves next iter joint_module graph find_nodes op= output meta get desc None len outputs fwd_outputs = outputs num_fwd_outputs bwd_outputs = outputs num_fwd_outputs fwd_outputs_descs = outputs_descs num_fwd_outputs bwd_outputs_descs = outputs_descs num_fwd_outputs fwd_outputs bwd_outputs fwd_outputs_descs bwd_outputs_descs _remove_by_name saved_values list fx Node name str saved_value saved_values saved_value name == name saved_values remove saved_value break find_first_sym_node fwd_module_outputs Union list fx Node tuple fx Node - int idx = len fwd_module_outputs i range len fwd_module_outputs - - - is_sym_node fwd_module_outputs i idx = i + break idx calculate_quantization_scaling graph torch fx Graph node torch fx Node max float = min float = e- position int = graph inserting_after node abs_node = graph call_function torch ops aten abs default args= node abs_node meta val = torch ops aten abs default node meta val abs_node meta tensor_meta = extract_tensor_metadata abs_node meta val graph inserting_after abs_node amax_node = graph call_function torch ops aten amax default args= abs_node - True amax_node meta val = torch ops aten amax default abs_node meta val - True amax_node meta tensor_meta = extract_tensor_metadata amax_node meta val graph inserting_after amax_node amax_ _node = graph call_function torch ops prims convert_element_type default args= amax_node torch float amax_ _node meta val = torch ops prims convert_element_type default amax_node meta val torch float amax_ _node meta tensor_meta = extract_tensor_metadata amax_ _node meta val graph inserting_after amax_ _node clamp_min_node = graph call_function torch ops aten clamp_min default args= amax_ _node min clamp_min_node meta val = torch ops aten clamp_min default amax_ _node meta val min clamp_min_node meta tensor_meta = extract_tensor_metadata clamp_min_node meta val graph inserting_after clamp_min_node reciprocal_node = graph call_function torch ops aten reciprocal default args= clamp_min_node reciprocal_node meta val = torch ops aten reciprocal default clamp_min_node meta val reciprocal_node meta tensor_meta = extract_tensor_metadata reciprocal_node meta val graph inserting_after reciprocal_node mul_node = graph call_function torch ops aten mul Tensor args= reciprocal_node max mul_node meta val = torch ops aten mul Tensor reciprocal_node meta val max mul_node meta tensor_meta = extract_tensor_metadata mul_node meta val graph inserting_after mul_node scale_node = graph call_function torch ops prims convert_element_type default args= mul_node torch float name=f fp _scale_pos_ position _ node name scale_node meta val = torch ops prims convert_element_type default mul_node meta val torch float scale_node meta tensor_meta = extract_tensor_metadata scale_node meta val scale_node perform_quantization graph torch fx Graph node torch fx Node scale_node torch fx Node quant_type torch dtype clamp_min float clamp_max float position int - torch fx Node graph inserting_after scale_node target_node_ = graph call_function torch ops prims convert_element_type default args= node torch float target_node_ meta val = torch ops prims convert_element_type default node meta val torch float target_node_ meta tensor_meta = extract_tensor_metadata target_node_ meta val graph inserting_after target_node_ scaled_target_node = graph call_function torch ops aten mul Tensor args= target_node_ scale_node scaled_target_node meta val = torch ops aten mul Tensor target_node_ meta val scale_node meta val scaled_target_node meta tensor_meta = extract_tensor_metadata scaled_target_node meta val graph inserting_after scaled_target_node clamp_min_scaled_node = graph call_function torch ops aten clamp_min default args= scaled_target_node clamp_min clamp_min_scaled_node meta val = torch ops aten clamp_min default scaled_target_node meta val clamp_min clamp_min_scaled_node meta tensor_meta = extract_tensor_metadata clamp_min_scaled_node meta val graph inserting_after clamp_min_scaled_node clamp_max_scaled_node = graph call_function torch ops aten clamp_max default args= clamp_min_scaled_node clamp_max clamp_max_scaled_node meta val = torch ops aten clamp_max default clamp_min_scaled_node meta val clamp_max clamp_max_scaled_node meta tensor_meta = extract_tensor_metadata clamp_max_scaled_node meta val graph inserting_after clamp_max_scaled_node quant_activation_node = graph call_function torch ops prims convert_element_type default args= clamp_max_scaled_node quant_type name=f fp _quant_pos_ position _ node name quant_activation_node meta val = torch ops prims convert_element_type default clamp_max_scaled_node meta val quant_type quant_activation_node meta tensor_meta = extract_tensor_metadata quant_activation_node meta val quant_activation_node calculate_tensor_size tensor torch Tensor - float Calculate size PyTorch tensor megabytes MB Args tensor torch Tensor Input tensor Returns float Memory size MB Get number elements size per element num_elements = tensor numel element_size = tensor element_size num_elements element_size get_allowed_dtypes - list torch dtype allowed_dtypes = torch _inductor config post_grad_fusion_options activation_quantization_aten_pass get allowed_dtypes torch bfloat allowed_dtypes = getattr torch dtype split - dtype allowed_dtypes split allowed_dtypes should_quantize node torch fx Node - bool allowed_dtypes = get_allowed_dtypes is_node_meta_valid node node meta val dtype allowed_dtypes False size_threshold = torch _inductor config post_grad_fusion_options activation_quantization_aten_pass get size_in_mb calculate size node size_in_mb = calculate_tensor_size node meta val torch _inductor config post_grad_fusion_options activation_quantization_aten_pass get skip_dynamo_guards False size_in_mb = size_threshold case we always quantize tensors dynamic shapes torch _inductor config post_grad_fusion_options activation_quantization_aten_pass get quantize_dynamic_shape False statically_known_true size_in_mb = size_threshold statically_known_false size_in_mb = size_threshold case we always quantize tensors dynamic shapes statically_known_true size_in_mb = size_threshold get_quant_type - torch dtype quant_type = torch _inductor config post_grad_fusion_options activation_quantization_aten_pass get quant_type torch float _e m getattr torch quant_type split - calculate_range dtype torch dtype - tuple Calculate range values given torch dtype Args dtype torch dtype The input dtype Returns tuple A tuple containing minimum maximum values info = torch finfo dtype info min info max quantize_activation_fw graph torch fx Graph - None output = graph find_nodes op= output fwd_outputs = output args quant_type = get_quant_type clamp_min clamp_max = calculate_range quant_type position_to_quant = dict tensor_scale_nodes sym_scale_nodes = position node enumerate fwd_outputs check activation node node saved quantization node meta get saved_for_quantization False case use scaling torch _inductor config post_grad_fusion_options activation_quantization_aten_pass get use_scaling True calculating scale scale_node = calculate_quantization_scaling graph node clamp_max e- position converting fp quant_node = perform_quantization graph node scale_node quant_type clamp_min clamp_max position is_sym_node scale_node tensor_scale_nodes append scale_node sym_scale_nodes append scale_node case do use scaling graph inserting_after node quant_node = graph call_function torch ops prims convert_element_type default args= node quant_type name=f fp _quant_pos_ position _ node name quant_node meta val = torch ops prims convert_element_type default node meta val quant_type quant_node meta tensor_meta = extract_tensor_metadata quant_node meta val position_to_quant position = quant_node Use position-based lookup building output only update node args remain all other users unchanged output_updated_args = position_to_quant get i node i node enumerate fwd_outputs add scale nodes output find first sym_node output pyrefly ignore bad-argument-type idx = find_first_sym_node output_updated_args scale_nodes = tensor_scale_nodes + sym_scale_nodes scale_nodes output_updated_args = output_updated_args idx + scale_nodes + output_updated_args idx output update_arg tuple output_updated_args counters inductor activation_quantization_fwd_aten_pass += quantize_activation_bw graph torch fx Graph - None bw_inputs = node node graph nodes node op == placeholder activation_node = None node bw_inputs node meta get saved_for_quantization False node meta pop saved_for_quantization dequant_type = node meta pop dequant_type dequantize node torch _inductor config post_grad_fusion_options activation_quantization_aten_pass get use_scaling False case use scaling graph inserting_after node find corresponding scale node scale_name = fp _scale_ + node name replace fp _quant_ scale_node = next bwd_input bwd_input bw_inputs bwd_input name == scale_name graph inserting_after scale_node activation_node = graph call_function torch ops prims convert_element_type default args= node dequant_type activation_node meta val = torch ops prims convert_element_type default node meta val dequant_type activation_node meta tensor_meta = extract_tensor_metadata activation_node meta val graph inserting_after activation_node divided_target_node_ = graph call_function torch ops aten div Tensor args= activation_node scale_node divided_target_node_ meta val = torch ops aten div Tensor activation_node meta val scale_node meta val divided_target_node_ meta tensor_meta = extract_tensor_metadata divided_target_node_ meta val graph inserting_after divided_target_node_ dequant_node = graph call_function torch ops prims convert_element_type default args= divided_target_node_ dequant_type dequant_node meta val = torch ops prims convert_element_type default divided_target_node_ meta val dequant_type dequant_node meta tensor_meta = extract_tensor_metadata dequant_node meta val graph inserting_after node dequant_node = graph call_function torch ops prims convert_element_type default args= node dequant_type name= dequant_ + str node name dequant_node meta val = torch ops prims convert_element_type default node meta val dequant_type dequant_node meta tensor_meta = extract_tensor_metadata dequant_node meta val find users node replace them new node except dequant_node user list node users keys user = dequant_node user = activation_node user replace_input_with node dequant_node counters inductor activation_quantization_bwd_aten_pass += perform_fp _activation_quantization fwd_module fx GraphModule bwd_module fx GraphModule bwd_module_inputs dict str fx Node - None trace_structured artifact metadata_fn=lambda name before_activation_quantization_fwd_aten_pass encoding string payload_fn=lambda fwd_module print_readable print_output=False include_stride=True include_device=True quantize_activation_fw fwd_module graph trace_structured artifact metadata_fn=lambda name after_activation_quantization_fwd_aten_pass encoding string payload_fn=lambda fwd_module print_readable print_output=False include_stride=True include_device=True trace_structured artifact metadata_fn=lambda name before_activation_quantization_bwd_aten_pass encoding string payload_fn=lambda bwd_module print_readable print_output=False include_stride=True include_device=True quant_fwd_module_outputs = fwd_module graph find_nodes op= output args update corresponding bwd_inputs due fwd_outputs quantization fwd_node quant_fwd_module_outputs fp _quant_ fwd_node name bwd_input = bwd_module_inputs re sub r ^fp _quant_pos_\d+_ fwd_node name bwd_module graph inserting_after bwd_input quant_bwd_input = bwd_module graph placeholder name=fwd_node name dequant_type = bwd_input meta dequant_type quant_bwd_input meta update fwd_node meta quant_bwd_input meta saved_for_quantization = True quant_bwd_input meta dequant_type = dequant_type bwd_input replace_all_uses_with quant_bwd_input bwd_module graph erase_node bwd_input update bwd_inputs quantization scaling used torch _inductor config post_grad_fusion_options activation_quantization_aten_pass get use_scaling True quant_bwd_module_inputs = list bwd_module graph find_nodes op= placeholder update corresponding bwd input nodes find last non-tangent node bwd_input_loc = quant_bwd_module_inputs - bw_input reversed quant_bwd_module_inputs _is_tangent bw_input bwd_input_loc = bw_input break scaled_fwd_module_outputs = fwd_module graph find_nodes op= output args fwd_node scaled_fwd_module_outputs fp _scale_ fwd_node name fwd node scale node bwd_module graph inserting_after bwd_input_loc scale_bwd_input = bwd_module graph placeholder name=fwd_node name scale_bwd_input meta update fwd_node meta bwd_input_loc = scale_bwd_input quantize_activation_bw bwd_module graph trace_structured artifact metadata_fn=lambda name after_activation_quantization_bwd_aten_pass encoding string payload_fn=lambda bwd_module print_readable print_output=False include_stride=True include_device=True enable_activation_quantization saved_values list fx Node fwd_module fx GraphModule bwd_module fx GraphModule static_lifetime_input_nodes Optional OrderedSet fx Node = None - None inductor_config post_grad_fusion_options get activation_quantization_aten_pass None None static_input_names = node name node static_lifetime_input_nodes static_lifetime_input_nodes saved_values_names = node name node node saved_values torch _inductor config post_grad_fusion_options activation_quantization_aten_pass get exclude_primals False saved_values_names = node name node node saved_values primals node name fwd_module_outputs = fwd_module graph find_nodes op= output args bwd_module_inputs = node name node node bwd_module graph find_nodes op= placeholder should_perform_fp _quant = False node fwd_module_outputs node name saved_values_names should_quantize node node name static_input_names log debug Skipping quantization static input s node name continue node meta saved_for_quantization = True node meta dequant_type = node meta val dtype some fwd outputs bwd inputs share same object bwd_module_inputs node name meta saved_for_quantization = True bwd_module_inputs node name meta dequant_type = node meta val dtype should_perform_fp _quant = True should_perform_fp _quant perform_fp _activation_quantization fwd_module bwd_module bwd_module_inputs _extract_fwd_bwd_modules joint_module fx GraphModule saved_values list fx Node saved_sym_nodes list fx Node num_fwd_outputs int static_lifetime_input_nodes Optional OrderedSet fx Node = None - tuple fx GraphModule fx GraphModule fwd_outputs bwd_outputs fwd_outputs_descs bwd_outputs_descs = _extract_fwd_bwd_outputs joint_module num_fwd_outputs=num_fwd_outputs placeholders = joint_module graph find_nodes op= placeholder primal_inputs = filter _is_primal placeholders tangent_inputs = filter _is_tangent placeholders fwd_seed_offset_inputs = filter _is_fwd_seed_offset placeholders bwd_seed_offset_inputs = filter _is_bwd_seed_offset placeholders backward_state_inputs = filter _is_backward_state placeholders bwd_graph = _extract_graph_with_inputs_outputs joint_module graph saved_sym_nodes + saved_values + tangent_inputs + bwd_seed_offset_inputs bwd_outputs bwd_outputs_descs backward distributed_enabled = torch distributed is_available node bwd_graph find_nodes op= placeholder This filter out saved values don t actually end up being used backwards pass node users _remove_by_name saved_values node name _remove_by_name saved_sym_nodes node name wait_tensor bit special we have dead activation used bw dead activation actually collective then collective will generally followed wait_tensor call we need peak one node further see wait_tensor dead well distributed_enabled all n target torch ops _c d_functional wait_tensor default len n users == n node users _remove_by_name saved_values node name _remove_by_name saved_sym_nodes node name _is_backward_state node BackwardState saved directly _remove_by_name saved_values node name assert backward_state_inputs Now we have finalized list saved values we need ensure we propagate all symbols which referenced backwards inputs These directly used graph required downstream sizevar assignment saved_symbols OrderedSet sympy Symbol = OrderedSet saved_sym_nodes_binding = saved_sym_nodes_derived = Some symbols may already bound directly saved_sym_nodes keep track them so we don t re-bind them node saved_sym_nodes symbol = is_symbol_binding_fx_node node symbol saved_symbols add symbol saved_sym_nodes_binding append node saved_sym_nodes_derived append node Now go through all prospective backward inputs track any other symbols we need bind symbol_bindings = find_symbol_binding_fx_nodes joint_module graph node itertools chain saved_sym_nodes_derived saved_values tangent_inputs val node meta continue new_symbols = free_symbols node meta val - saved_symbols NB Deterministic order please s sorted new_symbols key=lambda s s name NB For well formed graphs symbol should always present we also have ways produce ill-formed graphs e g direct make_fx usages so don t choke case s symbol_bindings continue saved_sym_nodes_binding append symbol_bindings s saved_symbols &#124; = new_symbols Update saved_sym_nodes now reordered have all bindings front This can also used later figure out position saved sym nodes output fwd graph saved_sym_nodes clear saved_sym_nodes extend saved_sym_nodes_binding + saved_sym_nodes_derived Now we re-generate fwd bwd graphs NB This might increase compilation time I doubt matters fwd_graph = _extract_graph_with_inputs_outputs joint_module graph primal_inputs + fwd_seed_offset_inputs fwd_outputs + saved_values + saved_sym_nodes fwd_outputs_descs + SavedForBackwardsAOTOutput i i range len saved_values + len saved_sym_nodes forward bwd_graph = _extract_graph_with_inputs_outputs joint_module graph saved_sym_nodes + saved_values + tangent_inputs + bwd_seed_offset_inputs + backward_state_inputs bwd_outputs bwd_outputs_descs backward fwd_module = fx _lazy_graph_module _make_graph_module joint_module fwd_graph bwd_module = fx _lazy_graph_module _make_graph_module joint_module bwd_graph enable_activation_quantization saved_values fwd_module bwd_module static_lifetime_input_nodes fwd_module bwd_module default_partition joint_module fx GraphModule _joint_inputs num_fwd_outputs static_lifetime_input_indices Optional list int = None static_lifetime_input_nodes Optional OrderedSet fx Node = None - tuple fx GraphModule fx GraphModule Partitions attr ` joint_module ` manner closely resembles behavior observed original ` ` forward ` ` ` ` backward ` ` callable i e resulting forward graph contains those operators executed original ` ` forward ` ` callable passed func ` aot_function ` The default partitioner collects operators between forward inputs forward outputs This helps finding tensors which have stashed backward pass These stashed tensors become output generated forward graph The remaining operators then placed backward graph warning This API experimental likely change Args joint_module fx GraphModule The joint forward backward graph This result AOT Autograd tracing Returns Returns generated forward backward Fx graph modules has_recomputable_ops joint_module min_cut_rematerialization_partition joint_module _joint_inputs num_fwd_outputs=num_fwd_outputs static_lifetime_input_indices=static_lifetime_input_indices primal_inputs = list filter _is_primal joint_module graph nodes fwd_seed_offset_inputs = list filter _is_fwd_seed_offset joint_module graph nodes inputs = primal_inputs + fwd_seed_offset_inputs fwd_outputs bwd_outputs fwd_outputs_descs bwd_outputs_descs = _extract_fwd_bwd_outputs joint_module num_fwd_outputs=num_fwd_outputs forward_only_graph = _extract_graph_with_inputs_outputs joint_module graph inputs fwd_outputs fwd_outputs_descs forward forward_node_names = OrderedSet node name node forward_only_graph nodes node op = output order = node idx idx node enumerate joint_module graph nodes saved_values = saved_sym_nodes = is_mutated_later_in_fw node _has_tag_is_backward node False tensor_arg_aliases = x x node args isinstance x fx Node val x meta isinstance x meta val torch Tensor while len tensor_arg_aliases = tensor_arg_aliases pop u users isinstance u target torch _ops OpOverload continue If we witness mutation our node later mutation must backward then our node needs computed forward otherwise we will compute mutated values one args mutated u target _schema is_mutable mutation happens later order u order node mutation happened during forward _has_tag_is_backward u _has_tag_must_be_in_backward u idx alias_info enumerate u target _schema arguments alias_info is_write u args idx True u target is_view tensor_arg_aliases append u False node joint_module graph nodes node name forward_node_names node isn t required forward any its arguments later mutated forward then must have been run forward node s arg saved backward we would have mutated saved value NB doesn t handle nodes where input list tensors one those tensors later mutated is_mutated_later_in_fw node saved_values append node continue is_sym_node node Symints must kept separate tensors so PythonFunction only calls save_for_backward tensors stashes symints autograd ctx saved_sym_nodes append node tensor_meta node meta node op == call_function isinstance node meta get val torch _subclasses FakeTensor Since we can t save tuple tensor values we need flatten out what we re saving users = node users assert all user target operator getitem user users saved_values extend users backward_usages = n n node users n name forward_node_names tensor_meta node meta all is_sym_node n n backward_usages If we have tensor forward where only its sizes strides needed backward actual tensor data then will lot cheaper save only sizes strides actual tensor Note saving tensor could also cause compilation problems If user mutated input forward uses its sizes strides backward then we would obligated clone input before saving appease autograd This how we originally found bug saved_sym_nodes extend backward_usages saved_values append node saved_values = list dict fromkeys saved_values keys saved_sym_nodes = list dict fromkeys saved_sym_nodes keys _extract_fwd_bwd_modules joint_module saved_values saved_sym_nodes=saved_sym_nodes num_fwd_outputs=num_fwd_outputs static_lifetime_input_nodes=static_lifetime_input_nodes INT_INF = int e _tensor_nbytes numel int dtype - int numel dtype itemsize _size_of node fx Node - int object_nbytes x - int isinstance x torch Tensor _tensor_nbytes hint_int x numel fallback= x dtype val node meta val = node meta val isinstance val py_sym_types NB The fallback values here meaningless maybe we should respect torch _inductor config unbacked_symint_fallback layering violation isinstance val list tuple sum object_nbytes n n val isinstance val dict sum object_nbytes n _ n val items isinstance val torch Tensor object_nbytes val raise RuntimeError f Unknown metadata type type val node node node op == get_attr node target torch ops aten _assert_scalar default raise RuntimeError f Node node didn t have ` val ` metadata we should always have ` val ` metadata nodes Used some investigative purposes _count_ops graph fx Graph collections defaultdict cnt dict str int = defaultdict int node graph nodes node op == call_function cnt node target __name__ += log info s sorted cnt items key=operator itemgetter reverse=True functools cache pointwise_ops ops = attr_name dir torch ops aten opoverloadpacket = getattr torch ops aten attr_name isinstance opoverloadpacket torch _ops OpOverloadPacket continue overload opoverloadpacket overloads op_overload = getattr opoverloadpacket overload torch Tag pointwise op_overload tags currently aot autograd uses packet overload ops append opoverloadpacket break ops sort_depths args depth_map dict fx Node int - list tuple fx Node int arg_depths = arg depth_map arg arg args isinstance arg torch fx node Node sorted arg_depths items key=operator itemgetter reverse=True reordering_to_mimic_autograd_engine gm fx GraphModule - fx GraphModule This pass finds first bwd node graph looking users tangents then reorders graph walking node all way end graph At each op traversal we insert op new graph try bring only relevant subgraph other non-bwd edges relevant op This closely mimics behavior autograd engine Why pass required first place This artifact how partitioners work today The starting point partitioner joint graph which fwd then bwd graph In case checkpointing we keep portions fwd graph their original place joint graph while obtaining bwd graph As result resulting bwd graph has copies recomputed fwd subgraphs followed original bwd graph If we run naively leads bad memory footprint because fwd subgraphs live way longer duration than necessary This pass reorders operations such we prioritize ops original bwd graph while only realizing those ops fwd graph necessary any given point graph new_graph = fx Graph env dict fx Node fx Node = Add new placeholder nodes order specified inputs node gm graph find_nodes op= placeholder env node = new_graph node_copy node lambda x env x order = node idx idx node enumerate gm graph nodes insert_node_in_graph node cur_nodes = node insertable_nodes OrderedSet fx Node = OrderedSet while len cur_nodes node = cur_nodes pop node insertable_nodes node env continue insertable_nodes add node Bias traversal towards nodes have higher depth - prioritizes critical path first cur_nodes += node all_input_nodes pyrefly ignore bad-assignment insertable_nodes = sorted insertable_nodes key=lambda n order n node insertable_nodes env node = new_graph node_copy node lambda x env x Find first bwd node graph tangent_inputs = list filter _is_tangent gm graph nodes first_node_in_bwd = None minimum_order = math inf tangent tangent_inputs user tangent users order user minimum_order minimum_order = order user first_node_in_bwd = user If gradInp does depend upon gradOut we may find any nodes backwards pass first_node_in_bwd None gm Build graph op-by-op starting node all way end copy_ can using tangents all we must copy node list gm graph nodes order first_node_in_bwd node op == call_function node target torch ops aten copy_ default insert_node_in_graph node node list gm graph nodes order first_node_in_bwd insert_node_in_graph node The output node already built traversal new_gm = torch fx GraphModule gm new_graph new_gm apply_graphsafe_rng_functionalization fw_module torch fx GraphModule bw_module torch fx GraphModule fw_node torch fx Node bw_node torch fx Node device torch device rng_count int last_fwd_input torch fx Node last_bwd_input torch fx Node Note CUDA Graph Safe RNG Functionalization CUDA Graph capture doesn t work get_rng_state set_rng_state because these functions operate CPU values while CUDA Graph RNG capture uses on-device CUDA tensors To solve we use graphsafe_set_state CUDA Generator registered CUDA Graph before capture begins graphsafe_set_state updates generator s pointer reference different GeneratorImpl ensuring subsequent calls correctly forwarded desired generator its cuda-tensor RNG state during graph capture For each RNG operation s forward backward pair - We create two generators initialized identical values - Each forward backward call advances its respective generator equally - This keeps generators synchronized so forward backward operations use matching RNG values When forward called multiple times before backward causing desynchronization - We save forward RNG state - We update backward Generator s state before executing backward Before each CUDA Graph replay replay_prologue updates captured RNG pointers current states ensuring backward Generator changes reflected during replay This function modifies both forward backward computation graphs Creating RNG state placeholders both passes Updating forward node use graph-safe RNG state Updating backward node use graph-safe RNG state For more details https github com pytorch pytorch issues device_idx = device index assert device_idx None fw_graph = fw_module graph bw_graph = bw_module graph graphsafe_run_with_rng_state = torch _prims rng_prims graphsafe_run_with_rng_state Handle forward pass Note Generator arguments AOTDispatcher Generator arguments AOTDispatcher added support graphsafe rng functionalization See note above CUDA Graph Safe RNG Functionalization fw_module graph inserting_after last_fwd_input fwd_rng_state = fw_module graph placeholder f fwd_rng_state_ rng_count fwd_rng_state meta val = get_cuda_generator_meta_val device_idx last_fwd_input = fwd_rng_state Handle backward pass bw_module graph inserting_after last_bwd_input bwd_rng_state = bw_module graph placeholder f bwd_rng_state_ rng_count above clone so meta val generator will contain tensors bwd_rng_state meta val = get_cuda_generator_meta_val device_idx last_bwd_input = bwd_rng_state Update forward node fw_kwargs = dict fw_node kwargs fw_kwargs rng_state = fwd_rng_state fw_module graph inserting_after fw_node functional_fw_node = fw_graph create_node call_function graphsafe_run_with_rng_state args= fw_node target fw_node args type ignore arg-type kwargs=fw_kwargs fw_node replace_all_uses_with functional_fw_node fw_graph erase_node fw_node Update backward node bwd_kwargs = dict bw_node kwargs bwd_kwargs rng_state = bwd_rng_state bw_graph inserting_before bw_node rng_output = bw_graph create_node call_function graphsafe_run_with_rng_state args= bw_node target bw_node args type ignore arg-type kwargs=bwd_kwargs bw_node replace_all_uses_with rng_output bw_graph erase_node bw_node last_fwd_input last_bwd_input functionalize_rng_ops joint_module fx GraphModule fw_module fx GraphModule bw_module fx GraphModule num_sym_nodes int - tuple fx GraphModule fx GraphModule During user-driven activation checkpointing we have ensure rng op fwd yields same output recomputed rng op bwd To do we use functionalize wrappers wrap random ops share rng state between fwd bwd graphs There main steps do Step - Construct mapping rng node between fwd its counterpart bwd Step - Modify fwd pass such Replace rand run_and_save_rng_state wrapper Replace users original op output op Collect all rng_state - output each op make them output nodes Special care needs taken here because fwd outputs has symints very end Step - Modify bwd pass such Add input nodes just before tangents stashed rng states Replace rand run_with_save_rng_state wrappers Use stashed states inputs these ops Unique id generate name uid = itertools count get_rng_ops gmod random_nodes = node gmod graph nodes node op == call_function hasattr node target tags torch Tag nondeterministic_seeded node target tags random_nodes node name = node random_nodes get_device node - Optional torch device Check example value node outputs find device type val node meta None candidates = node meta val isinstance candidates tuple candidates = candidates candidate candidates isinstance candidate torch Tensor candidate device type == cuda candidate device torch device cpu get_sample_rng_state device Optional torch device torch _guards detect_fake_mode noqa F fake_mode = detect_fake_mode assert fake_mode None fake_mode device None device type == cuda fake_mode from_tensor torch cuda get_rng_state fake_mode from_tensor torch get_rng_state Step - Construct mapping rng node between fwd its counterpart bwd joint_graph_rng_ops = get_rng_ops joint_module fw_graph_rng_ops = get_rng_ops fw_module bw_graph_rng_ops = get_rng_ops bw_module recomputable_rng_ops_map = node joint_module graph nodes must_recompute node hasattr node target tags torch Tag nondeterministic_seeded node target tags base_node = joint_graph_rng_ops node name fw_node = fw_graph_rng_ops node name bw_node = bw_graph_rng_ops node name recomputable_rng_ops_map base_node = fwd fw_node bwd bw_node run_and_save_rng = torch _prims rng_prims run_and_save_rng_state run_with_rng_state = torch _prims rng_prims run_with_rng_state bw_tangent_start_node = None node bw_module graph find_nodes op= placeholder tangent node name bw_tangent_start_node = node break bw_tangent_start_node None raise RuntimeError Couldn t find tangent node graph inputs This unexpected please file bug you see fw_rng_state_outputs = last_fwd_input = next reversed fw_module graph find_nodes op= placeholder last_bwd_input = next reversed bw_module graph find_nodes op= placeholder devices = OrderedSet get_device node_pair fwd node_pair recomputable_rng_ops_map values pyrefly ignore unbound-name devices discard torch device cpu multiple cuda devices won t work cudagraphs anyway fallback non graphsafe rng checkpointing multi_cuda_devices = len devices changes numerics so fallback_random set we will use pyrefly ignore unbound-name ind_config = torch _inductor config use_rng_graphsafe_rng_functionalization = config graphsafe_rng_functionalization multi_cuda_devices ind_config fallback_random ind_config test_configs graphsafe_rng_func_ignores_fallback_random rng_count node_pair enumerate recomputable_rng_ops_map values Step - Modify fwd pass such fw_node = node_pair fwd bw_node = node_pair bwd device = get_device fw_node fw_graph = fw_module graph bw_graph = bw_module graph use_rng_graphsafe_rng_functionalization device None device type == cuda last_fwd_input last_bwd_input = apply_graphsafe_rng_functionalization fw_module bw_module fw_node bw_node device rng_count last_fwd_input last_bwd_input fw_graph inserting_before fw_node functional_fw_node = fw_graph create_node call_function run_and_save_rng args= fw_node target fw_node args kwargs=fw_node kwargs state = fw_graph create_node call_function operator getitem args= functional_fw_node kwargs= state meta val = get_sample_rng_state device rng_output = fw_graph create_node call_function operator getitem args= functional_fw_node kwargs= Copy meta data original node rng_output meta = copy copy fw_node meta fw_node replace_all_uses_with rng_output fw_graph erase_node fw_node fw_rng_state_outputs append state Step - Modify bwd pass such bw_graph inserting_before bw_tangent_start_node state_name = f rng_state_output_ next uid bw_rng_state_node = bw_graph placeholder state_name bw_rng_state_node meta val = get_sample_rng_state device bw_graph inserting_before bw_node rng_output = bw_graph create_node call_function run_with_rng_state args= bw_rng_state_node bw_node target bw_node args kwargs=bw_node kwargs bw_node replace_all_uses_with rng_output bw_graph erase_node bw_node Add rng states output fwd graph AOT Autograd assumes symints end forward graph outputs So insert new rng states accordingly fw_rng_state_outputs fw_output_node = next iter fw_module graph find_nodes op= output fw_outputs = fw_output_node args sym_node_start_idx = len fw_outputs - num_sym_nodes outputs = fw_outputs sym_node_start_idx + tuple fw_rng_state_outputs + fw_outputs sym_node_start_idx fw_module graph output outputs fw_module graph erase_node fw_output_node fw_module recompile bw_module recompile fw_module bw_module force_save_collectives joint_module fx GraphModule - None By default partitioner allowed recompute collectives unless they come user-annotated AC region See Note Recomputing collectives partitioner node joint_module graph nodes isinstance node target torch _ops OpOverload node target namespace == _c d_functional must_recompute node node meta recompute = CheckpointPolicy MUST_SAVE force_save_bw_mutation_src joint_module fx GraphModule - None If we have mutations same primal forward backward We must recompute source mutation apply twice has_mutation_in_bw OrderedSet torch fx Node = OrderedSet node reversed joint_module graph nodes node op == output continue is_copy_ = node target torch ops aten copy_ default is_copy_ _has_tag_must_be_in_backward node has_mutation_in_bw add node args _has_tag_must_be_in_forward node node args has_mutation_in_bw node args meta recompute = CheckpointPolicy MUST_SAVE We use invariant aotdispatch joint graph That we emit copy_ only end We do want iterate through all joint graph so break first non-output non-copy_ node break cleanup_recompute_tags joint_module fx GraphModule - fx GraphModule If there two consecutive checkpointed blocks no operator between we would still want stash tensor boundary checkpointed blocks The following pass makes last output node non-recomputable allow node joint_module graph nodes must_recompute node user node users must_recompute user ac_graph_id user meta ac_graph_id node meta user meta ac_graph_id node meta ac_graph_id node meta recompute = CheckpointPolicy MUST_SAVE node meta get has_backward_hook False any must_recompute user user node users If node AC region output has backward hook we intentionally choose save This work around circular dependencies Traceable FSDP +AC Example ` ` ` out = fully_shard utils checkpoint module x norm_out = layer_norm out ` ` ` Here there circular dependency In backward grad_input layer_norm aka ` out_grad ` actually dependent ` out ` ` out ` depends ` out ` s backward hook created FSDP which does all-gather ` module ` weights order recomputed ` out ` s backward hook case all eager backward hooks depends ` out_grad ` - circular dependency Solution check whether ` out ` has backward hook so intentionally save ` out ` forward graph outputs With we can break above circular dependency node meta recompute = CheckpointPolicy MUST_SAVE joint_module solve_min_cut joint_graph fx Graph node_info NodeInfo min_cut_options MinCutOptions dont_ban Optional OrderedSet fx Node = None dont_ban None dont_ban = OrderedSet op_types = get_default_op_list AOT_PARTITIONER_DEBUG joint_module_ops = OrderedSet str node target _overloadpacket node joint_graph nodes node op == call_function hasattr node target _overloadpacket ops_ignored = joint_module_ops - OrderedSet str i i op_types recomputable_ops log info Ops banned re-materialization s ops_ignored can_fuse_into_auto_functionalized b b target = torch ops higher_order auto_functionalized False mutable_op = b args mutable_arg_names _ = torch _higher_order_ops auto_functionalize get_mutable_args mutable_op name mutable_arg_names arg = b kwargs name arg True isinstance arg list arg True False can_fuse_into_triton_kernel_wrapper_functional b b target = torch ops higher_order triton_kernel_wrapper_functional False mutable_arg_names = b kwargs tensors_to_clone name mutable_arg_names arg = b kwargs kwargs name arg True False is_fusible b We can perform memory fusion into cat cat cannot producer fusion get_aten_target b == aten cat True can_fuse_into_auto_functionalized b True can_fuse_into_triton_kernel_wrapper_functional b True target operator getitem args target torch ops higher_order triton_kernel_wrapper_functional output user triton kernel then default we will able fuse b into False op_types is_fusible op_types is_fusible b try networkx nx except ImportError e raise RuntimeError Need networkx installed perform smart recomputation heuristics e is_materialized_backwards node op_types is_view node False cur_nodes = OrderedSet node while len cur_nodes cur = cur_nodes pop user cur users node_info is_required_fw user is_fusible cur user True op_types is_view user cur_nodes add user False should_ban_recomputation node node op = call_function False node target operator getitem False node meta get recompute None == CheckpointPolicy MUST_SAVE True config recompute_views op_types is_view node False node target aten lift_fresh_copy default aten lift_fresh default False min_cut_options ban_if_not_in_allowlist op_types is_recomputable node True op_types is_random node op_types is_compute_intensive node True If node must materialized backwards pass then we should never recompute This pretty subtle point In general assumption we make recomputing node backwards pass free However node must materialized backwards pass then recomputing never free min_cut_options ban_if_materialized_backward is_materialized_backwards node log debug materialized backwards s s node tuple node users True Arbitrary hack sometimes seems help things The above modification appears have made heuristic lot less critical performance NB As PR hack no longer seems necessary node dist_from_bw node dist_from_bw config max_dist_from_bw True If output op x smaller arbitrary choice then we don t allow recomputation The idea here things like reductions saving output reduction very cheap small makes sure we don t do things like recompute normalizations backwards min_cut_options ban_if_reduction input_tensors_size = sum _size_of i i node args isinstance i fx Node output_size = _size_of node output_size input_tensors_size False is_materialized node node op == placeholder True all is_fusible node user user node users get_node_weight node static_lifetime_input_nodes - float config treat_parameters_as_free_to_save node static_lifetime_input_nodes mem_sz = _size_of node config recompute_views op_types is_view node If ` config recompute_views=True ` we don t save views This generally good idea since views free recompute makes bit simpler analyze NB If they re free recompute e g nested tensors I think we should modify checks view_ops ` is_view ` check Basically nested tensors ` aten view ` view op math inf isinstance node meta val py_sym_types We never want save symfloats isinstance node meta val torch SymInt INT_INF Heuristic bias towards nodes closer backwards pass Complete guess about current value mem_sz = int mem_sz max min node dist_from_bw is_materialized node mem_sz mem_sz nx_graph = nx DiGraph banned_nodes OrderedSet fx Node = OrderedSet ban_recomputation_if_allowed node op_types is_view node False node dont_ban collectives always banned recompute overriding ` dont_ban ` particular activation memory budget logic allowed recompute collectives is_collective = isinstance node target torch _ops OpOverload node target namespace == _c d_functional config unsafe_allow_optimization_of_collectives is_collective False This bans recomputation node unless we ve been forced user annotation must_recompute node False val node meta isinstance node meta val torch SymFloat False banned_nodes add node A node will only ever recomputed there path ancestor node backwards path through node doesn t go through any saved value If node saved then condition possible nx_graph add_edge source node name + _in capacity=math inf True node joint_graph nodes node op == output continue node node_info required_bw_nodes node node_info inputs nx_graph add_edge node name + _in sink capacity=math inf continue If someone saves input backward as-is backward returns tensor as-is grad input then node x would both required_bw_node input In case we connect x_in source x_out sink assign proper weight x_in-x_out edge so x would part cut nodes A case where happens NestedTensor saves offset tensor part singleton int sizes nx_graph add_edge node name + _out sink capacity=math inf must_recompute node If user explicitly says they want recompute node we honor adding inf-capacity edge X_in sink This way X_in node guaranteed part subgraph contains sink after cut thus guaranteeing X op will recomputed nx_graph add_edge node name + _in sink capacity=math inf continue _is_primal node _is_fwd_seed_offset node ban_recomputation_if_allowed node If node can t recomputed too expensive involves randomness we prevent being recomputed adding inf edge source We only need ban nodes fw pass those only ones would recomputed node_info is_required_fw node should_ban_recomputation node ban_recomputation_if_allowed node Checks node actually tuple Can simplified just isinstance check we always use faketensors is_non_tensor_node = val node meta tensor_meta node meta val node meta isinstance node meta val torch Tensor is_sym_node node weight = float sym_node_size node is_non_tensor_node weight = isinstance node meta get val BackwardState math inf weight = get_node_weight node node_info static_lifetime_input_nodes Creates weights node edge nx_graph add_edge node name + _in node name + _out capacity=weight user node users nx_graph add_edge node name + _out user name + _in capacity=math inf todo chilli This most questionable heuristics banning recompute Some example models look where helps perf poolformer_m mixer_b _ cait_m _ The rough idea here you have some node used both node nearby downstream well node far downstream we recompute both downstream nodes we re unlikely able fuse both downstream nodes together Thus we shouldn t aim recompute far downstream nodes depend node That intuition far downstream captured whether there s unfusible op along chain somewhere It could probably improved properly analyzing what s going backwards pass instead only relying whether s unfusible forwards find_first_unfusible start_nodes list fx Node max_range int - int Finds first unfusible node chain nodes starting ` start_nodes ` returns its position sorted_nodes list tuple int fx Node bool = n start_nodes heapq heappush sorted_nodes node_info get_fw_order n n True while len sorted_nodes _ node node_is_fusible = heapq heappop sorted_nodes node_is_fusible node_info get_fw_order node user node users node_info is_required_fw user node_info get_fw_order user max_range continue val tuple int fx Node bool = node_info get_fw_order user user is_fusible node user val sorted_nodes heapq heappush sorted_nodes val max_range min_cut_options ban_if_used_far_apart used_node node_info required_fw_nodes orders = node_info get_fw_order user user used_node users node_info is_required_fw user fw_users = user user used_node users node_info is_required_fw user len orders first_unfusible_use = find_first_unfusible fw_users max orders user tuple used_node users node_info is_required_fw user node_info get_fw_order user first_unfusible_use is_fusible used_node user user banned_nodes continue log info used above below fusible s s - s - s s used_node node_info get_fw_order used_node first_unfusible_use user node_info get_fw_order user ban_recomputation_if_allowed user This heuristic fairly straightforward The idea although cheap recompute bandwidth-bound ops we don t want end up situation where we have long chain pointwise ops beginning end model like say residual connections todo I m totally sure why heuristic matters It s possible working around Inductor fusion decisions s patch over suboptimal partitioning decisions Some models improves perf cait_m _ mixer_b _ poolformer_m min_cut_options ban_if_long_fusible_chains visited OrderedSet fx Node = OrderedSet start_node joint_graph nodes node_info is_required_fw start_node continue fusible list tuple int fx Node = node_info get_fw_order start_node start_node start_order = node_info get_fw_order start_node while len fusible _ cur = heapq heappop fusible cur visited continue visited add cur arbitrary choice try prevent degenerate cases node_info get_fw_order cur start_order + len fusible == log info too long s s s s cur start_node node_info get_fw_order cur node_info get_fw_order start_node ban_recomputation_if_allowed cur break user cur users node_info is_required_fw user is_fusible cur user user banned_nodes heapq heappush fusible node_info get_fw_order user user try cut_value partition = nx minimum_cut nx_graph source sink except Exception log info Failed compute min-cut following graph log info \n join nx readwrite edgelist generate_edgelist nx_graph visualize_min_cut_graph nx_graph raise reachable non_reachable = partition cutset OrderedSet tuple str str = OrderedSet u nbrs n nx_graph n n reachable cutset update u v v nbrs v non_reachable cut_nodes OrderedSet str = OrderedSet node_in node_out cutset assert node_in - == node_out - node_name = node_in - cut_nodes add node_name name_to_node = get_name_to_node joint_graph To make stuff deterministic node_idx = node idx idx node enumerate joint_graph nodes saved_values = sorted name_to_node node node cut_nodes key=lambda x node_idx x saved_values banned_nodes visualize_min_cut_graph nx_graph networkx nx pydot dot_format = nx nx_pydot to_pydot nx_graph to_string dot_graph = pydot graph_from_dot_data dot_format type ignore index edge dot_graph get_edges weight = nx_graph edge get_source edge get_destination capacity Set edge label weight edge set_label str weight type ignore union-attr Color edges weight inf red weight == float inf edge set_color red type ignore union-attr log info Visualizing failed graph min_cut_failed svg dot_graph write_svg min_cut_failed svg type ignore union-attr get_default_op_list - OpTypes default_recomputable_ops list Callable = aten add aten sub aten div aten atan aten mul aten max aten min aten pow aten remainder aten fmod aten __and__ aten __or__ aten __xor__ aten __lshift__ aten __rshift__ aten eq aten ne aten ge aten gt aten le aten lt aten abs aten bitwise_not aten ceil aten floor aten frac aten neg aten relu aten round aten silu aten trunc aten log aten log aten log p aten log aten lgamma aten exp aten expm aten erf aten erfc aten cos aten acos aten cosh aten sin aten asin aten sinh aten tan aten atan aten tanh aten atanh aten sqrt aten rsqrt aten reciprocal aten sigmoid aten softplus aten threshold aten threshold_backward aten clamp aten where aten lerp aten addcmul aten gelu aten gelu_backward aten sum aten mean aten _grad_sum_to_size aten sum_to_size aten amax aten aten type_as operator getitem aten squeeze aten unsqueeze aten rsub aten _to_copy noqa E B recomputable_view_ops = aten squeeze aten unsqueeze aten alias recomputable_view_ops += aten view aten slice aten t prims broadcast_in_dim aten expand aten as_strided aten permute aten select aten split view_ops = recomputable_view_ops default_recomputable_ops += prims div prims convert_element_type aten clone aten _to_copy aten full_like prims var prims sum aten var aten std prims broadcast_in_dim aten select aten _unsafe_view aten view aten expand aten slice aten reshape aten broadcast_tensors aten scalar_tensor aten ones aten new_zeros aten lift_fresh_copy aten arange aten triu aten var_mean aten isinf aten any aten full aten as_strided aten zeros aten empty aten empty_like aten argmax aten maximum prims iota prims _low_memory_max_pool_offsets_to_indices noqa E B Natalia said we should allow recomputing indexing default_recomputable_ops += aten index aten gather default_recomputable_ops += view_ops default_recomputable_ops += pointwise_ops default_recomputable_ops += aten zeros_like default_recomputable_ops += method_to_operator m m magic_methods recomputable_ops = OrderedSet default_recomputable_ops random_ops = OrderedSet Callable Any aten native_dropout aten rand_like aten randn_like compute_intensive_ops = aten mm aten convolution aten convolution_backward aten bmm aten addmm aten _scaled_dot_product_flash_attention aten _scaled_dot_product_efficient_attention aten _flash_attention_forward aten _efficient_attention_forward aten upsample_bilinear d aten _scaled_mm noqa E B fusible_ops = recomputable_ops &#124; random_ops OpTypes fusible_ops OrderedSet compute_intensive_ops random_ops OrderedSet view_ops recomputable_ops get_name_to_node graph fx Graph name_to_node = node graph nodes name_to_node node name = node name_to_node _optimize_runtime_with_given_memory joint_graph fx Graph memory list float runtimes list float max_memory float node_info NodeInfo all_recomputable_banned_nodes list fx Node - tuple float list int list int SOLVER = config activation_memory_budget_solver SOLVER == greedy greedy_knapsack memory runtimes max_memory SOLVER == ilp ilp_knapsack memory runtimes max_memory SOLVER == dp dp_knapsack memory runtimes max_memory SOLVER == dynamic_memory_budget_dp log warning dynamic_memory_budget_dp experimental solver It does guarantee performance improvements Additionally guaranteed stable graph_info_provider = GraphInfoProvider inialize_from_graph joint_graph=joint_graph all_recomputable_banned_nodes=all_recomputable_banned_nodes recorded_knapsack_input_memories=memory recorded_knapsack_input_runtimes=runtimes dp_knapsack memory runtimes KnapsackEvaluator graph_info_provider=graph_info_provider get_knee_point_memory_budget knapsack_algo=dp_knapsack max_mem_budget=max_memory callable SOLVER saved_node_idx recomp_node_idx = SOLVER memory joint_graph max_memory node_info all_recomputable_banned_nodes saved_node_idx recomp_node_idx raise RuntimeError f Not aware memory budget knapsack solver SOLVER torch utils _mode_utils no_dispatch replace symbols size strides their hints without guarding _remove_symbols_without_guarding x torch Tensor fallback int - torch Tensor shape = list x shape realize_symbol d hint_int d fallback=fallback shape = realize_symbol s s shape stride = realize_symbol s s x stride x new_empty_strided shape stride=stride estimate_runtime node RUNTIME_MODE = config activation_memory_budget_runtime_estimator materialize_arg x isinstance x fx Node isinstance x meta val torch Tensor _remove_symbols_without_guarding x meta val fallback= isinstance x fx Node isinstance x meta val torch SymInt hint_int x meta val fallback= isinstance x fx Node isinstance x meta val torch SymFloat isinstance x fx Node isinstance x meta val torch SymBool True x RUNTIME_MODE == testing RUNTIME_MODE == profile no_dispatch torch _inductor runtime benchmarking benchmarker args kwargs = pytree tree_map materialize_arg node args node kwargs ms = benchmarker benchmark_gpu lambda node target args kwargs ms RUNTIME_MODE == flops todo chilli Normalize also ms torch utils flop_counter FlopCounterMode args kwargs = pytree tree_map materialize_arg node args node kwargs FlopCounterMode display=False mode node target args kwargs counted_flops = mode get_total_flops max counted_flops raise RuntimeError f Not aware runtime estimator RUNTIME_MODE choose_saved_values_set joint_graph fx Graph node_info NodeInfo memory_budget= - list fx Node memory_budget memory_budget raise RuntimeError f The valid ranges memory budget = m = The provided value memory_budget min_cut_options = MinCutOptions ban_if_used_far_apart=config ban_recompute_used_far_apart ban_if_long_fusible_chains=config ban_recompute_long_fusible_chains ban_if_materialized_backward=config ban_recompute_materialized_backward ban_if_not_in_allowlist=config ban_recompute_not_in_allowlist ban_if_reduction=config ban_recompute_reductions config aggressive_recomputation min_cut_options = replace min_cut_options ban_if_used_far_apart=False ban_if_long_fusible_chains=False ban_if_materialized_backward=False ban_if_not_in_allowlist=False memory_budget == node_info inputs runtime_optimized_saved_values _ = solve_min_cut joint_graph node_info min_cut_options runtime_optimized_saved_values memory_budget == runtime_optimized_saved_values estimate_activations_size saved_values list fx Node - float sum map _size_of saved_values e min_act_size = estimate_activations_size node_info inputs max_act_size = estimate_activations_size runtime_optimized_saved_values The optimized choice smaller than inputs anyways max_act_size = min_act_size runtime_optimized_saved_values get_normalized_size sz sz e max_act_size - min_act_size get_mem_ratio activations list fx Node estimate_activations_size activations - min_act_size max_act_size - min_act_size more_aggressive_options = replace min_cut_options ban_if_used_far_apart=False ban_if_long_fusible_chains=False ban_if_materialized_backward=False more_aggressive_saved_values _ = solve_min_cut joint_graph node_info more_aggressive_options get_mem_ratio more_aggressive_saved_values memory_budget more_aggressive_saved_values aggressive_options = replace more_aggressive_options ban_if_not_in_allowlist=False aggressive_recomputation_saved_values banned_nodes = solve_min_cut joint_graph node_info aggressive_options get_mem_ratio aggressive_recomputation_saved_values memory_budget aggressive_recomputation_saved_values torch _inductor fx_utils get_node_storage input_storages = OrderedSet get_node_storage node node node_info inputs get_recomputable_banned_nodes banned_nodes OrderedSet fx Node - list fx Node i i banned_nodes Only allow recomputing nodes actually required BW i dist_from_bw int e type ignore attr-defined get_node_storage i input_storages recomputable_banned_nodes = get_recomputable_banned_nodes banned_nodes must_save_nodes = i i recomputable_banned_nodes i meta get recompute False == CheckpointPolicy MUST_SAVE recomputable_banned_nodes = i i recomputable_banned_nodes i must_save_nodes default runtime_optimized_saved_values more aggressive more_aggressive_saved_values full aggressive aggressive_recomputation_saved_values all_recomputable_banned_nodes = sorted recomputable_banned_nodes key=_size_of reverse=True len all_recomputable_banned_nodes == node_info inputs + must_save_nodes memories_banned_nodes = get_normalized_size _size_of i i all_recomputable_banned_nodes runtimes_banned_nodes = estimate_runtime node node all_recomputable_banned_nodes torch utils _mode_utils no_dispatch get_saved_values_knapsack memory_budget node_info joint_graph no_dispatch expected_runtime saved_node_idxs recomputable_node_idxs = _optimize_runtime_with_given_memory joint_graph memories_banned_nodes runtimes_banned_nodes max memory_budget node_info all_recomputable_banned_nodes dont_ban OrderedSet fx Node = OrderedSet idx recomputable_node_idxs idx all_recomputable_banned_nodes try dont_ban add all_recomputable_banned_nodes idx except BaseException noqa B pass assert dont_ban issubset all_recomputable_banned_nodes saved_values _ = solve_min_cut joint_graph node_info aggressive_options dont_ban AOT_PARTITIONER_DEBUG create_structured_trace_for_min_cut_info joint_graph=joint_graph all_recomputable_banned_nodes=all_recomputable_banned_nodes saved_node_idxs=saved_node_idxs recomputable_node_idxs=recomputable_node_idxs expected_runtime=expected_runtime memories_banned_nodes= _size_of i i all_recomputable_banned_nodes normalized_memories_banned_nodes=memories_banned_nodes runtimes_banned_nodes=runtimes_banned_nodes min_cut_saved_values=saved_values saved_values expected_runtime config visualize_memory_budget_pareto estimate_for_budget b saved_values expected_runtime = get_saved_values_knapsack b node_info=node_info joint_graph=joint_graph b sum runtimes_banned_nodes - expected_runtime get_mem_ratio saved_values options = estimate_for_budget estimate_for_budget options = options bisects = options options while bisects lhs rhs = bisects pop rhs - lhs e- options append lhs options append rhs continue mid = estimate_for_budget lhs + rhs mid = lhs bisects append lhs mid mid = rhs bisects append mid rhs options sort matplotlib pyplot plt x_values = item item options y_values = item item options Plotting values updated axis labels chart title plt figure figsize= plt plot x_values y_values marker= o Adding labels each point i txt enumerate x_values plt annotate f txt f txt y_values i textcoords= offset points xytext= ha= center plt xlabel Memory Budget plt ylabel Runtime Recomputed Components plt title Pareto Frontier Memory Budget vs Recomputation Runtime plt grid True fig = plt gcf plt show fig_dir = os getcwd config memory_budget_pareto_dir None fig_dir = config memory_budget_pareto_dir os makedirs fig_dir exist_ok=True rank_suffix = torch distributed is_available torch distributed is_initialized rank_suffix = f _rank_ torch distributed get_rank fig_name = os path join fig_dir f memory_budget_pareto rank_suffix _ get_aot_graph_name svg fig savefig fig_name log warning Generated Pareto frontier curve s fig_name todo chilli Estimated doesn t align exactly actual - actual usually less memory than estimated i m guessing actually quite unsure about s because estimated just only including tensors we actually banned recompute there may other tensors we choose save get_saved_values_knapsack memory_budget=memory_budget node_info=node_info joint_graph=joint_graph _sync_decision_cross_ranks joint_graph torch fx Graph saved_values list torch fx Node use same policy across different GPUs torch _subclasses fake_tensor unset_fake_temporarily has_collectives joint_graph node joint_graph nodes isinstance node target torch _ops OpOverload node target namespace _c d_functional c d_functional True False has_same_nodes joint_graph proxy check graph same across different GPUs We only consider name order nodes A more robust way would check hash whole graph disregarding input shapes reasonable first-order approximation node_str = join x name x joint_graph nodes inputs = hashlib sha node_str encode utf- hexdigest all_inputs = None _ range torch distributed get_world_size no_dispatch unset_fake_temporarily TODO maybe use different process group torch distributed all_gather_object all_inputs inputs all all_inputs == x x all_inputs torch distributed is_available torch distributed is_initialized torch distributed get_world_size has_collectives joint_graph has_same_nodes joint_graph no_dispatch unset_fake_temporarily objects = x name x saved_values saved_ops_names_all_ranks list list str = _ range torch distributed get_world_size torch distributed all_gather_object saved_ops_names_all_ranks objects name_to_node = get_name_to_node joint_graph saved_sizes list int = saved_ops_with_sizes dict str int = idx saved_ops_names enumerate saved_ops_names_all_ranks saved_nodes = name_to_node op_name op_name saved_ops_names saved_size = node saved_nodes size_of_node = _size_of node saved_size += size_of_node idx == torch distributed get_rank saved_ops_with_sizes node name = size_of_node saved_ops_with_sizes total size = saved_size saved_sizes append saved_size saved_sizes_tensor = torch tensor saved_sizes device=torch distributed distributed_c d _get_object_coll_device torch distributed all_reduce saved_sizes_tensor op=torch distributed distributed_c d ReduceOp MAX picked_rank_idx = int torch argmin saved_sizes_tensor item sync_decision_cross_ranks_str = f picked_rank_idx= picked_rank_idx saved_nodes current rank= saved_ops_with_sizes trace_structured artifact metadata_fn=lambda name aot_joint_graph_sync_decision_cross_ranks encoding string payload_fn=lambda sync_decision_cross_ranks_str saved_values = name_to_node n n saved_ops_names_all_ranks picked_rank_idx saved_values thread_graphsafe_rng_from_hops module is_backward Graph-safe RNG lets torch compile use CUDA Graphs graphs RNG ops For graphs without HOPs partitioner adds placeholder nodes fwd_rng_state_ bw_rng_state_ forward backward graphs At runtime AOTDispatcher retrieves these RNG states passes them compiled graphs This works well no-HOP graphs With HOPs partitioner runs recursively first partitions HOP producing forward backward HOP subgraphs then stitches them back into outer joint graph For HOPs contain RNG ops outer joint graph now includes HOP subgraph modules extra RNG placeholders We must thread these placeholders through outer module partitioned forward backward graphs  function does exactly It collects RNG placeholder nodes HOPs creates corresponding placeholders outer forward backward graphs There catch short period joint graph  bad  state The HOP subgraphs expect additional inputs because new placeholders outer graph call sites don t yet provide them We can t fix joint graph because joint graph s input signature fixed primals tangents As compromise we keep joint graph somewhat bad state some time once outer forward backward graphs partitioned insert corresponding RNG placeholders wire up calls rng_count = rng_string = bwd_rng_state is_backward fwd_rng_state last_input = next reversed module graph find_nodes op= placeholder hop_node module graph find_nodes op= call_function target=torch ops higher_order invoke_subgraph subgraph = getattr module hop_node args target isinstance subgraph fx GraphModule new_rng_inputs = placeholder_node subgraph graph find_nodes op= placeholder rng_string placeholder_node name Found rng state placeholder hop graph lets add corresponding node outer graph module graph inserting_after last_input rng_state = module graph placeholder f rng_string _ rng_count rng_count += rng_state meta val = placeholder_node meta val last_input = rng_state new_rng_inputs append rng_state new_rng_inputs Pass new args include new_rng_inputs module graph inserting_after hop_node new_hop_node_with_fixed_args = module graph create_node call_function torch ops higher_order invoke_subgraph hop_node args new_rng_inputs type ignore arg-type hop_node replace_all_uses_with new_hop_node_with_fixed_args propagate_meta=True Setup eager_input_vals eager_vals = hop_node meta get eager_input_vals eager_vals eager_args eager_kwargs = eager_vals new_eager_args = eager_args inp meta val inp new_rng_inputs new_hop_node_with_fixed_args meta eager_input_vals = new_eager_args eager_kwargs module graph erase_node hop_node module min_cut_rematerialization_partition joint_module fx GraphModule _joint_inputs compiler= inductor num_fwd_outputs static_lifetime_input_indices Optional list int = None - tuple fx GraphModule fx GraphModule Partitions joint graph such backward recomputes forward Recomputing helps trading off memory bandwidth computation To create fwd bwd graph we copy joint graph manually set outputs just original forward backward outputs And then we run resulting graphs through dead code elimination warning This API experimental likely change Args joint_module fx GraphModule The joint forward backward graph This result AOT Autograd tracing _joint_inputs The inputs joint graph This unused compiler This option determines default set recomputable ops Currently there two options ` ` nvfuser ` ` ` ` inductor ` ` recomputable_ops This optional set recomputable ops If None then set ops will used instead default set ops num_fwd_outputs The number outputs forward graph Returns Returns generated forward backward Fx graph modules joint_module graph eliminate_dead_code joint_module recompile fx_g = joint_module graph add CSE pass config cse cse_graph = fx_graph_cse fx_g joint_module graph = cse_graph joint_graph = joint_module graph graph_has_recomputable_ops = has_recomputable_ops joint_module graph_has_recomputable_rng_ops = has_recomputable_rng_ops joint_module graph_has_recomputable_ops joint_module = cleanup_recompute_tags joint_module config unsafe_allow_optimization_of_collectives force_save_collectives joint_module force_save_bw_mutation_src joint_module classify_nodes joint_module static_lifetime_input_indices name_to_node = get_name_to_node joint_module graph required_bw_nodes OrderedSet fx Node = OrderedSet node joint_module graph nodes node op == placeholder tangents node target required_bw_nodes add node _must_be_in_backward node required_bw_nodes add node node required_bw_nodes required_bw_nodes update node users primal_inputs = list filter _is_primal joint_module graph nodes fwd_seed_offset_inputs = list filter _is_fwd_seed_offset joint_module graph nodes inputs = primal_inputs + fwd_seed_offset_inputs fwd_outputs bwd_outputs fwd_outputs_descs bwd_outputs_descs = _extract_fwd_bwd_outputs joint_module num_fwd_outputs=num_fwd_outputs required_bw_nodes update o o bwd_outputs o None o op = output forward_only_graph = _extract_graph_with_inputs_outputs joint_module graph inputs fwd_outputs fwd_outputs_descs forward required_fw_nodes OrderedSet fx Node = OrderedSet name_to_node node name node forward_only_graph nodes node op = output unclaimed_nodes OrderedSet fx Node = OrderedSet node node joint_module graph nodes node required_fw_nodes node required_bw_nodes static_lifetime_input_nodes = OrderedSet p i p enumerate primal_inputs i static_lifetime_input_indices fw_cnt = fw_order = node joint_module graph nodes node required_fw_nodes fw_order node = fw_cnt fw_cnt += NodeInfo inputs required_fw_nodes required_bw_nodes unclaimed_nodes fw_order static_lifetime_input_nodes static_lifetime_input_indices None static_lifetime_input_indices = node_info = classify_nodes joint_module static_lifetime_input_indices networkx blows up graphs no required backward nodes Since there s nothing partition anyway default partitioner can handle case send our graph over default partitioner len node_info required_bw_nodes == default_partition joint_module _joint_inputs num_fwd_outputs=num_fwd_outputs static_lifetime_input_indices=static_lifetime_input_indices static_lifetime_input_nodes=node_info static_lifetime_input_nodes node reversed joint_module graph nodes node op == output node dist_from_bw = int e node_info is_required_fw node node dist_from_bw = node dist_from_bw = int e user node users node dist_from_bw = min node dist_from_bw user dist_from_bw + memory_budget = config activation_memory_budget node joint_graph nodes isinstance node meta get memory_budget None float memory_budget = node meta memory_budget break saved_values = choose_saved_values_set joint_graph node_info memory_budget=memory_budget pyrefly ignore unbound-name config _sync_decision_cross_ranks saved_values = _sync_decision_cross_ranks joint_graph saved_values save_for_backward tensors stashes symints autograd ctx saved_sym_nodes = list filter is_sym_node saved_values saved_values = list filter lambda n is_sym_node n saved_values NB saved_sym_nodes will mutated reflect actual saved symbols fw_module bw_module = _extract_fwd_bwd_modules joint_module saved_values pyrefly ignore bad-argument-type saved_sym_nodes=saved_sym_nodes num_fwd_outputs=num_fwd_outputs static_lifetime_input_nodes=node_info static_lifetime_input_nodes graph_has_recomputable_ops graph_has_recomputable_rng_ops fw_module bw_module = functionalize_rng_ops joint_module fw_module bw_module len saved_sym_nodes bw_module = reordering_to_mimic_autograd_engine bw_module raise all getitem ops early possible helpful memory especially case aot_eager backend fw_module = raise_getitems fw_module bw_module = raise_getitems bw_module fw_module = thread_graphsafe_rng_from_hops fw_module is_backward=False bw_module = thread_graphsafe_rng_from_hops bw_module is_backward=True AOT_PARTITIONER_DEBUG Calculate sorted sizes saved values sorted_sizes = sorted _size_of i str i i saved_values Log total theoretical activations stored total_activations_size_gb = sum _size_of i i saved_values e log info Theoretical Activations Stored f GB total_activations_size_gb Log theoretical per activation storage sizes log info Theoretical Per Activation Storage Sizes s sorted_sizes fw_module_nodes = OrderedSet node name node fw_module graph nodes node op == call_function bw_module_nodes = OrderedSet node name node bw_module graph nodes node op == call_function remat_nodes = fw_module_nodes bw_module_nodes counts dict str int = defaultdict int node fw_module graph nodes node name remat_nodes hasattr node target _overloadpacket counts str node target _overloadpacket += log info remat fw bw d d d len remat_nodes len fw_module_nodes len bw_module_nodes rematerialized_ops = sorted counts items key=operator itemgetter reverse=True log info Count Ops Rematerialized s rematerialized_ops fw_module bw_module draw_graph traced torch fx GraphModule fname str figname str = fx_graph clear_meta bool = True prog Optional Union str list str = None parse_stack_trace bool = False dot_graph_shape Optional str = None - None clear_meta new_graph = copy deepcopy traced graph traced = fx GraphModule traced new_graph node traced graph nodes node meta = base ext = os path splitext fname ext ext = + config torch_compile_graph_format log info Writing FX graph file s s base ext g = graph_drawer FxGraphDrawer traced figname parse_stack_trace=parse_stack_trace dot_graph_shape=dot_graph_shape x = g get_main_dot_graph write_method = getattr x write_ + ext lstrip fname = f base ext prog None write_method fname write_method fname prog=prog