Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD license found LICENSE file root directory source tree Generates combination kernels - implementations registry Kernels ordered see ` sort_index ` when dispatching we select first kernel list supports inputs argparse collections itertools dataclasses dataclass field pathlib Path typing Optional TypeVar DTYPES = f float f cutlass half_t bf cutlass bfloat _t SM = Sm kernels support up Sm KERNEL_IMPL_TEMPLATE = __global__ void __launch_bounds__ CPP_CLASS kNumThreads CPP_CLASS kMinBlocksPerSm NAME typename CPP_CLASS Params p #ifdef __CUDA_ARCH__ #if __CUDA_ARCH__ = SM #if __CUDA_ARCH__ SM_MAX p advance_to_block CPP_CLASS attention_kernel p #endif #endif printf FATAL kernel ` NAME ` sm SM -sm SM_MAX built sm d\\n int __CUDA_ARCH__ + #endif dataclass order=True FwdKernel sort_index tuple int = field init=False repr=False aligned bool dtype str sm_range tuple int int q int k int max_k int supports_dropout bool = True supports_bias bool = True dispatch_cond Optional str = None __post_init__ - None Set kernel selection priority The lowest value matches inputs will selected sort_index = First select aligned kernel aligned Then keep output RF max_k k Prefer kernels without dropout bias available supports_dropout supports_bias property _aligned_suffix - str aligned aligned notaligned property name - str acc = rf max_k = k gmem f fmha_cutlassF_ dtype _ _aligned_suffix _ q x k _ acc _sm sm_range property cpp_class - str template_args = join DTYPES dtype f cutlass arch Sm sm_range true aligned false str q str k str max_k true supports_dropout false true supports_bias false f AttentionKernel template_args property impl_group - str Maps file which will contain implementation f dtype _ _aligned_suffix property cpp_impl - str KERNEL_IMPL_TEMPLATE format CPP_CLASS=self cpp_class NAME=self name SM=self sm_range SM_MAX=self sm_range classmethod get_all cls - list FwdKernel kernels list FwdKernel = aligned dtype sm sm_max itertools product True False DTYPES keys itertools pairwise SM Remove some kernels we don t use dtype == bf sm continue aligned sm = continue q k max_k We get better perf x A sm kernels append cls aligned=aligned dtype=dtype sm_range= sm sm_max q=q k=k max_k=max_k kernels dataclass order=True BwdKernel sort_index tuple int = field init=False repr=False sm_range tuple int int dtype str aligned bool apply_dropout bool preload_mmas bool block_i int block_j int max_k int dispatch_cond Optional str = None keys_queries_aligned_to_blocksizes bool = False __post_init__ - None Set kernel selection priority The lowest value matches inputs will selected sort_index = First select aligned kernel aligned Take kernel without dropout possible apply_dropout Then take smallest maxK max_k highest block_i -self block_i finally avoid bounds-checks possible keys_queries_aligned_to_blocksizes property _aligned_suffix - str aligned aligned notaligned property name - str dropout_suffix = _dropout apply_dropout seqlen_aligned_suffix = _seqaligned keys_queries_aligned_to_blocksizes f fmha_cutlassB_ dtype _ _aligned_suffix f _ block_i x block_j _k max_k dropout_suffix seqlen_aligned_suffix _sm sm_range property cpp_class - str template_args = join f cutlass arch Sm sm_range DTYPES dtype true aligned false true apply_dropout false true preload_mmas false str block_i str block_j str max_k keys_queries_aligned_to_blocksizes template_args += true f AttentionBackwardKernel template_args property impl_group - str Maps file which will contain implementation dropout_suffix = _dropout apply_dropout f dtype _ _aligned_suffix _k max_k dropout_suffix property cpp_impl - str KERNEL_IMPL_TEMPLATE format CPP_CLASS=self cpp_class NAME=self name SM=self sm_range SM_MAX=self sm_range classmethod get_all cls - list BwdKernel kernels list BwdKernel = aligned dtype sm sm_max apply_dropout max_k itertools product True False DTYPES keys itertools pairwise SM True False dtype == bf sm continue aligned sm = continue is_half = dtype bf f bi_values = Some architectures have more shmem can use We still need fallback GPUs less shmem Sm Sm sm = sm = is_half max_k bi_values append bi bi_values output_in_rf = is_half max_k = bi preload_mmas = is_half sm = output_in_rf bj = preload_mmas max_k kernels append cls aligned=aligned dtype=dtype sm_range= sm sm_max apply_dropout=apply_dropout preload_mmas=preload_mmas block_i=bi block_j=bj max_k=max_k A few specialized kernels faster apply_dropout max_k is_half aligned continue sm continue kernels append cls aligned=aligned dtype=dtype sm_range= sm sm_max apply_dropout=apply_dropout preload_mmas=preload_mmas block_i=bi block_j=bj max_k=max_k keys_queries_aligned_to_blocksizes=True Add some specialized kernels stable diffusion BW K= This only kernel can keep outputs RF Sm Sm so s much faster than x one dtype f bf kernels append cls aligned=True dtype=dtype sm_range= SM SM index + apply_dropout=False preload_mmas=True block_i= block_j= max_k= Sm has faster kernel case dispatch_cond= cc == &#124; &#124; cc == kernels T = TypeVar T FwdKernel BwdKernel write_decl_impl kernels list T family_name str impl_file str autogen_dir Path disable_def Optional str = None - None cpp_file_header = Copyright c Meta Platforms Inc affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree This file auto-generated See generate_kernels py kernels sort implfile_to_kernels dict str list T = collections defaultdict list cat_to_kernels dict tuple str int int list T = collections defaultdict list dispatch_all = declarations = cpp_file_header + #pragma once\n declarations += f #ifndef disable_def \n declarations += f #include impl_file \n declarations += using namespace PyTorchMemEffAttention \n Declaration kernel functions k kernels implfile_to_kernels k impl_group append k cat_to_kernels k dtype k sm_range k sm_range append k cat_dt cat_sm cat_sm_max kernels cat_to_kernels items declarations += f ======== cat_dt sm cat_sm ========\n declarations += \n join k cpp_impl split rstrip + k kernels dispatch_category_fn = f dispatch_ family_name _ cat_dt _sm cat_sm declarations += f \n\ntemplate typename T void dispatch_category_fn T cb int cc \n k kernels _call = f cb k cpp_class k name \n k dispatch_cond None _call = f k dispatch_cond _call declarations += f _call declarations += \n\n dispatch_all += f std is_same_v DT DTYPES cat_dt cat_sm = cc cc cat_sm_max dispatch_category_fn cb cc declarations += f template typename DT typename T void dispatch_ family_name T cb int cc = dispatch_all declarations += f #endif disable_def \n Write declarations family header autogen_dir f family_name h write_text declarations f f_kernels implfile_to_kernels items impl_cu = cpp_file_header impl_cu += f #ifndef disable_def \n impl_cu += f #include impl_file \n impl_cu += using namespace PyTorchMemEffAttention \n k f_kernels impl_cu += k cpp_impl impl_cu += f #endif disable_def \n autogen_dir f family_name _ f cu write_text impl_cu main output_dir Optional str - None output_dir None output_dir = Path __file__ parent output_dir = Path output_dir write_decl_impl FwdKernel get_all cutlassF impl_file= ATen native transformers cuda mem_eff_attention kernel_forward h autogen_dir=output_dir write_decl_impl BwdKernel get_all cutlassB impl_file= ATen native transformers cuda mem_eff_attention kernel_backward h autogen_dir=output_dir __name__ == __main__ parser = argparse ArgumentParser prog= generate_kernels description= Generate mem-eff kernels template instantiations Set optional output directory parser add_argument -o -- output_dir required=False help= Where generate kernels will default ATen native transformers cuda mem_eff_attention kernels args = parser parse_args main args output_dir