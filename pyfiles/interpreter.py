mypy allow-untyped-defs inspect contextlib contextmanager typing Any Optional TYPE_CHECKING Union torch torch fx traceback fx_traceback torch _logging trace_structured torch hub tqdm config _compatibility compatibility _lazy_graph_module _make_graph_module _symbolic_trace Tracer graph Graph graph_module GraphModule node Argument map_aggregate map_arg Node Target proxy Proxy TYPE_CHECKING collections abc Iterator __all__ = Interpreter Transformer compatibility is_backward_compatible=True Interpreter An Interpreter executes FX graph Node-by-Node This pattern can useful many things including writing code transformations well analysis passes Methods Interpreter can overridden customize behavior execution The map overridable methods terms call hierarchy run + -- run_node + -- placeholder + -- get_attr + -- call_function + -- call_method + -- call_module + -- output Example Suppose we want swap all instances ` ` torch neg ` ` ` ` torch sigmoid ` ` vice versa including their ` ` Tensor ` ` method equivalents We could subclass Interpreter like so NegSigmSwapInterpreter Interpreter call_function target Target args Tuple kwargs Dict - Any target torch sigmoid torch neg args kwargs super call_function target args kwargs call_method target Target args Tuple kwargs Dict - Any target == neg call_self args_tail = args call_self sigmoid args_tail kwargs super call_method target args kwargs fn x torch sigmoid x neg gm = torch fx symbolic_trace fn input = torch randn result = NegSigmSwapInterpreter gm run input torch testing assert_close result torch neg input sigmoid Args module torch nn Module The module executed garbage_collect_values bool Whether delete values after their last use within Module s execution This ensures optimal memory usage during execution This can disabled example examine all intermediate values execution looking ` ` Interpreter env ` ` attribute graph Optional Graph If passed interpreter will execute graph instead ` module graph ` using provided ` module ` argument satisfy any requests state compatibility is_backward_compatible=True __init__ module torch nn Module garbage_collect_values bool = True graph Optional Graph = None module = module submodules = dict module named_modules graph None graph = graph graph = module graph type ignore assignment env dict Node Any = name = Interpreter garbage_collect_values = garbage_collect_values extra_traceback = True garbage_collect_values Run through reverse nodes record first instance use given node This represents last use node execution order program which we will use free unused values node_to_last_use dict Node Node = user_to_last_uses dict Node list Node = register_last_uses n Node user Node n node_to_last_use node_to_last_use n = user user_to_last_uses setdefault user append n node reversed graph nodes n node _input_nodes register_last_uses n node compatibility is_backward_compatible=True run args initial_env Optional dict Node Any = None enable_io_processing bool = True - Any Run ` module ` via interpretation result Args args The arguments Module run positional order initial_env Optional Dict Node Any An optional starting environment execution This dict mapping ` Node ` any value This can used example pre-populate results certain ` Nodes ` so do only partial evaluation within interpreter enable_io_processing bool If true we process inputs outputs graph s process_inputs process_outputs function first before using them Returns Any The value returned executing Module env = initial_env initial_env None Positional function args consumed left-to-right ` placeholder ` nodes Use iterator keep track position extract those values enable_io_processing args = graph process_inputs args args_iter Iterator Any = iter args pbar = tqdm total=len graph nodes desc=f name str list graph nodes config verbose_progress initial= position= leave=True disable=config disable_progress delay= node graph nodes pbar update node env Short circuit we have value This could used example partial evaluation where caller has pre-populated ` env ` values subset program continue try env node = run_node node except Exception e extra_traceback msg = f While executing node format_node msg = f e args \n\n msg e args str msg msg += f \nOriginal traceback \n node stack_trace isinstance module GraphModule module graph None isinstance module graph torch fx Graph trace_structured artifact metadata_fn=lambda name fx_interpreter_error encoding string payload_fn=lambda f msg \nGraphModule f module print_readable print_output=False include_stride=True type ignore operator msg += \nUse tlparse see full graph msg += https github com pytorch tlparse tab=readme-ov-file#tlparse-parse-structured-pt -logs e args = msg + e args isinstance e KeyError raise RuntimeError e args e raise garbage_collect_values to_delete user_to_last_uses get node del env to_delete node op == output output_val = env node graph process_outputs output_val enable_io_processing output_val compatibility is_backward_compatible=True boxed_run args_list Run ` module ` via interpretation result This uses boxed calling convention where you pass list arguments which will cleared interpreter This ensures input tensors promptly deallocated args_iter = iter args_list env = n graph nodes n op == placeholder env n = next args_iter args_list clear run initial_env=env contextmanager _set_current_node node fx_traceback set_current_meta node f Interpreter_ __class__ __name__ yield compatibility is_backward_compatible=True run_node n Node - Any Run specific node ` ` n ` ` result Calls into placeholder get_attr call_function call_method call_module output depending ` ` node op ` ` Args n Node The Node execute Returns Any The result executing ` ` n ` ` _set_current_node n args kwargs = fetch_args_kwargs_from_env n assert isinstance args tuple assert isinstance kwargs dict getattr n op n target args kwargs Main Node running APIs compatibility is_backward_compatible=True placeholder target Target args tuple Argument kwargs dict str Any - Any Execute ` ` placeholder ` ` node Note stateful ` ` Interpreter ` ` maintains internal iterator over arguments passed ` ` run ` ` method returns next iterator Args target Target The call target node See ` Node https pytorch org docs main fx html#torch fx Node ` __ details semantics args Tuple Tuple positional args invocation kwargs Dict Dict keyword arguments invocation Returns Any The argument value retrieved assert isinstance target str target startswith For starred parameter e g ` args ` retrieve all remaining values args list list args_iter try next args_iter except StopIteration si len args args raise RuntimeError f Expected positional argument parameter target one passed si compatibility is_backward_compatible=True get_attr target Target args tuple Argument kwargs dict str Any - Any Execute ` ` get_attr ` ` node Will retrieve attribute value ` ` Module ` ` hierarchy ` ` module ` ` Args target Target The call target node See ` Node https pytorch org docs main fx html#torch fx Node ` __ details semantics args Tuple Tuple positional args invocation kwargs Dict Dict keyword arguments invocation Return Any The value attribute retrieved assert isinstance target str fetch_attr target compatibility is_backward_compatible=True call_function target Target args tuple Argument kwargs dict str Any - Any Execute ` ` call_function ` ` node result Args target Target The call target node See ` Node https pytorch org docs main fx html#torch fx Node ` __ details semantics args Tuple Tuple positional args invocation kwargs Dict Dict keyword arguments invocation Return Any The value returned function invocation assert isinstance target str Execute function result target args kwargs compatibility is_backward_compatible=True call_method target Target args tuple Argument kwargs dict str Any - Any Execute ` ` call_method ` ` node result Args target Target The call target node See ` Node https pytorch org docs main fx html#torch fx Node ` __ details semantics args Tuple Tuple positional args invocation kwargs Dict Dict keyword arguments invocation Return Any The value returned method invocation args ` ` object method call self_obj args_tail = args Execute method result assert isinstance target str getattr self_obj target args_tail kwargs compatibility is_backward_compatible=True call_module target Target args tuple Argument kwargs dict str Any - Any Execute ` ` call_module ` ` node result Args target Target The call target node See ` Node https pytorch org docs main fx html#torch fx Node ` __ details semantics args Tuple Tuple positional args invocation kwargs Dict Dict keyword arguments invocation Return Any The value returned module invocation Retrieve executed args kwargs values environment Execute method result assert isinstance target str submod = fetch_attr target submod args kwargs compatibility is_backward_compatible=True output target Target args tuple Argument kwargs dict str Any - Any Execute ` ` output ` ` node This really just retrieves value referenced ` ` output ` ` node returns Args target Target The call target node See ` Node https pytorch org docs main fx html#torch fx Node ` __ details semantics args Tuple Tuple positional args invocation kwargs Dict Dict keyword arguments invocation Return Any The value referenced output node args Helper methods compatibility is_backward_compatible=True fetch_attr target str Fetch attribute ` ` Module ` ` hierarchy ` ` module ` ` Args target str The fully-qualified name attribute fetch Return Any The value attribute target_atoms = target split attr_itr = module i atom enumerate target_atoms hasattr attr_itr atom raise RuntimeError f Node referenced nonexistent target join target_atoms i + attr_itr = getattr attr_itr atom attr_itr compatibility is_backward_compatible=True fetch_args_kwargs_from_env n Node - tuple tuple dict Fetch concrete values ` ` args ` ` ` ` kwargs ` ` node ` ` n ` ` current execution environment Args n Node The node which ` ` args ` ` ` ` kwargs ` ` should fetched Return Tuple Tuple Dict ` ` args ` ` ` ` kwargs ` ` concrete values ` ` n ` ` args = map_nodes_to_values n args n assert isinstance args tuple kwargs = map_nodes_to_values n kwargs n assert isinstance kwargs dict args kwargs compatibility is_backward_compatible=True map_nodes_to_values args Argument n Node - Argument Recursively descend through ` ` args ` ` look up concrete value each ` ` Node ` ` current execution environment Args args Argument Data structure within which look up concrete values n Node Node which ` ` args ` ` belongs This only used error reporting load_arg n_arg Node - Any n_arg env raise RuntimeError f Node n referenced nonexistent value n_arg Run Graph lint f diagnose such issues env n_arg map_arg args load_arg compatibility is_backward_compatible=True Transformer Interpreter ` ` Transformer ` ` special type interpreter produces new ` ` Module ` ` It exposes ` ` transform ` ` method returns transformed ` ` Module ` ` ` ` Transformer ` ` does require arguments run ` ` Interpreter ` ` does ` ` Transformer ` ` works entirely symbolically Example Suppose we want swap all instances ` ` torch neg ` ` ` ` torch sigmoid ` ` vice versa including their ` ` Tensor ` ` method equivalents We could subclass ` ` Transformer ` ` like so NegSigmSwapXformer Transformer call_function target Target args Tuple Argument kwargs Dict str Any - Any target torch sigmoid torch neg args kwargs super call_function target args kwargs call_method target Target args Tuple Argument kwargs Dict str Any - Any target == neg call_self args_tail = args call_self sigmoid args_tail kwargs super call_method target args kwargs fn x torch sigmoid x neg gm = torch fx symbolic_trace fn transformed torch nn Module = NegSigmSwapXformer gm transform input = torch randn torch testing assert_close transformed input torch neg input sigmoid Args module GraphModule The ` ` Module ` ` transformed compatibility is_backward_compatible=True __init__ module super __init__ module new_graph = Graph new_graph set_codegen module graph _codegen TransformerTracer Tracer __init__ graph Graph super __init__ graph = graph tensor_attrs dict torch Tensor str = type ignore assignment is_leaf_module _ __ - bool True tracer = TransformerTracer new_graph tracer root = module compatibility is_backward_compatible=True placeholder target Target args tuple Argument kwargs dict str Any - Proxy Execute ` ` placeholder ` ` node In ` ` Transformer ` ` overridden insert new ` ` placeholder ` ` into output graph Args target Target The call target node See ` Node https pytorch org docs main fx html#torch fx Node ` __ details semantics args Tuple Tuple positional args invocation kwargs Dict Dict keyword arguments invocation assert isinstance target str default_value = next iter args args inspect Signature empty Proxy new_graph placeholder target default_value=default_value tracer compatibility is_backward_compatible=True get_attr target Target args tuple Argument kwargs dict str Any - Proxy Execute ` ` get_attr ` ` node In ` ` Transformer ` ` overridden insert new ` ` get_attr ` ` node into output graph Args target Target The call target node See ` Node https pytorch org docs main fx html#torch fx Node ` __ details semantics args Tuple Tuple positional args invocation kwargs Dict Dict keyword arguments invocation assert isinstance target str tracer create_proxy get_attr target args kwargs compatibility is_backward_compatible=True call_module target Target args tuple Argument kwargs dict str Any - Any Override so leaf module policy ` tracer ` respected assert isinstance target str submod = fetch_attr target tracer call_module submod submod forward args kwargs compatibility is_backward_compatible=True call_function target Target args tuple Argument kwargs dict str Any - Any Override so functions wrapped still wrapped tracer create_proxy call_function target args kwargs compatibility is_backward_compatible=True transform - GraphModule Transform ` ` module ` ` transformed ` ` GraphModule ` ` fx_traceback preserve_node_meta result = super run enable_io_processing=False result None strip_proxy Union Argument Proxy - Any node isinstance Proxy new_output_node = new_graph output map_aggregate result strip_proxy also preserve metadata old output node exists old_output_node = list graph nodes - assert old_output_node op == output k v old_output_node meta items new_output_node meta k = v _make_graph_module module new_graph