functools pickle collections abc Callable Iterable Iterator typing Optional TypeVar torch utils _import_utils import_dill torch utils data datapipes _hook_iterator _SnapshotState torch utils data datapipes _typing _DataPipeMeta _IterDataPipeMeta torch utils data datapipes utils common _deprecation_warning _iter_deprecated_functional_names _map_deprecated_functional_names torch utils data dataset Dataset IterableDataset dill = import_dill HAS_DILL = dill None __all__ = DataChunk DFIterDataPipe IterDataPipe MapDataPipe _T = TypeVar _T _T_co = TypeVar _T_co covariant=True UNTRACABLE_DATAFRAME_PIPES = batch As returns DataChunks groupby As returns DataChunks _dataframes_as_tuples As unpacks DF trace_as_dataframe As used mark DF tracing DataChunk list _T __init__ items Iterable _T - None items = list items super __init__ items items = items as_str indent str = - str indent + + join str i i iter + __iter__ - Iterator _T yield super __iter__ raw_iterator - Iterator _T yield items IterDataPipe IterableDataset _T_co metaclass=_IterDataPipeMeta r Iterable-style DataPipe All DataPipes represent iterable data samples should subclass This style DataPipes particularly useful when data come stream when number samples too large fit them all memory ` ` IterDataPipe ` ` lazily initialized its elements computed only when ` ` next ` ` called iterator ` ` IterDataPipe ` ` All subclasses should overwrite meth ` __iter__ ` which would iterator samples DataPipe Calling ` ` __iter__ ` ` ` ` IterDataPipe ` ` automatically invokes its method ` ` reset ` ` which default performs no operation When writing custom ` ` IterDataPipe ` ` users should override ` ` reset ` ` necessary The common usages include resetting buffers pointers various state variables within custom ` ` IterDataPipe ` ` Note Only ` one ` iterator can valid each ` ` IterDataPipe ` ` time creation second iterator will invalidate first one This constraint necessary because some ` ` IterDataPipe ` ` have internal buffers whose states can become invalid there multiple iterators The code example below presents details how constraint looks practice If you have any feedback related constraint please see ` GitHub IterDataPipe Single Iterator Issue ` _ These DataPipes can invoked two ways using constructor applying their functional form onto existing ` ` IterDataPipe ` ` recommended available most all DataPipes You can chain multiple ` IterDataPipe ` together form pipeline will perform multiple operations succession _GitHub IterDataPipe Single Iterator Issue https github com pytorch data issues Note When subclass used ` ~torch utils data DataLoader ` each item DataPipe will yielded ` ~torch utils data DataLoader ` iterator When attr ` num_workers ` each worker process will have different copy DataPipe object so often desired configure each copy independently avoid having duplicate data returned workers func ` ~torch utils data get_worker_info ` when called worker process returns information about worker It can used either dataset s meth ` __iter__ ` method ` ~torch utils data DataLoader ` s attr ` worker_init_fn ` option modify each copy s behavior Examples General Usage xdoctest +SKIP torchdata datapipes iter IterableWrapper Mapper dp = IterableWrapper range map_dp_ = Mapper dp lambda x x + Using constructor map_dp_ = dp map lambda x x + Using functional form recommended list map_dp_ list map_dp_ filter_dp = map_dp_ filter lambda x x == list filter_dp Single Iterator Constraint Example torchdata datapipes iter IterableWrapper Mapper source_dp = IterableWrapper range = iter source_dp list = iter source_dp = iter source_dp The creation new iterator invalidates ` ` next next Further usage ` ` will raise ` RunTimeError ` functions dict str Callable = reduce_ex_hook Optional Callable = None getstate_hook Optional Callable = None str_hook Optional Callable = None repr_hook Optional Callable = None _valid_iterator_id Optional int = None _number_of_samples_yielded int = _snapshot_state _SnapshotState = _SnapshotState NotStarted _fast_forward_iterator Optional Iterator = None __iter__ - Iterator _T_co pyrefly ignore bad-return __getattr__ attribute_name attribute_name IterDataPipe functions attribute_name _iter_deprecated_functional_names kwargs = _iter_deprecated_functional_names attribute_name _deprecation_warning kwargs f = IterDataPipe functions attribute_name function = functools partial f functools update_wrapper wrapper=function wrapped=f assigned= __doc__ function raise AttributeError f __class__ __name__ object has no attribute attribute_name classmethod register_function cls function_name function cls functions function_name = function classmethod register_datapipe_as_function cls function_name cls_to_register enable_df_api_tracing=False function_name cls functions raise Exception noqa TRY f Unable add DataPipe function name function_name already taken class_function cls enable_df_api_tracing source_dp args kwargs result_pipe = cls source_dp args kwargs isinstance result_pipe IterDataPipe enable_df_api_tracing isinstance source_dp DFIterDataPipe function_name UNTRACABLE_DATAFRAME_PIPES result_pipe = result_pipe trace_as_dataframe result_pipe function = functools partial class_function cls_to_register enable_df_api_tracing functools update_wrapper wrapper=function wrapped=cls_to_register assigned= __doc__ cls functions function_name = function __getstate__ Serialize ` lambda ` functions when ` dill ` available If doesn t cover your custom DataPipe s use case consider writing custom methods ` __getstate__ ` ` __setstate__ ` use ` pickle dumps ` serialization state = __dict__ IterDataPipe getstate_hook None IterDataPipe getstate_hook state state __reduce_ex__ args kwargs IterDataPipe reduce_ex_hook None try IterDataPipe reduce_ex_hook except NotImplementedError pass super __reduce_ex__ args kwargs classmethod set_getstate_hook cls hook_fn IterDataPipe getstate_hook None hook_fn None raise RuntimeError Attempt override existing getstate_hook IterDataPipe getstate_hook = hook_fn classmethod set_reduce_ex_hook cls hook_fn IterDataPipe reduce_ex_hook None hook_fn None raise RuntimeError Attempt override existing reduce_ex_hook IterDataPipe reduce_ex_hook = hook_fn __repr__ repr_hook None repr_hook Instead showing torch MapperIterDataPipe object x name str __class__ __qualname__ __str__ str_hook None str_hook Instead showing torch MapperIterDataPipe object x name str __class__ __qualname__ __dir__ auto-completion REPL e g Jupyter notebook list super __dir__ + list functions keys reset - None r Reset ` IterDataPipe ` initial state By default no-op For subclasses ` IterDataPipe ` depending their functionalities they may want override method implementations may clear buffers reset pointers DataPipe The ` reset ` method always called when ` __iter__ ` called part ` hook_iterator ` DFIterDataPipe IterDataPipe _is_dfpipe True MapDataPipe Dataset _T_co metaclass=_DataPipeMeta r Map-style DataPipe All datasets represent map keys data samples should subclass Subclasses should overwrite meth ` __getitem__ ` supporting fetching data sample given unique key Subclasses can also optionally overwrite meth ` __len__ ` which expected size dataset many ` ~torch utils data Sampler ` implementations default options ` ~torch utils data DataLoader ` These DataPipes can invoked two ways using constructor applying their functional form onto existing ` MapDataPipe ` recommend available most all DataPipes Note ` ~torch utils data DataLoader ` default constructs index sampler yields integral indices To make work map-style DataPipe non-integral indices keys custom sampler must provided Example xdoctest +SKIP torchdata datapipes map SequenceWrapper Mapper dp = SequenceWrapper range map_dp_ = dp map lambda x x + Using functional form recommended list map_dp_ map_dp_ = Mapper dp lambda x x + Using constructor list map_dp_ batch_dp = map_dp_ batch batch_size= list batch_dp functions dict str Callable = reduce_ex_hook Optional Callable = None getstate_hook Optional Callable = None str_hook Optional Callable = None repr_hook Optional Callable = None __getattr__ attribute_name attribute_name MapDataPipe functions attribute_name _map_deprecated_functional_names kwargs = _map_deprecated_functional_names attribute_name _deprecation_warning kwargs f = MapDataPipe functions attribute_name function = functools partial f functools update_wrapper wrapper=function wrapped=f assigned= __doc__ function raise AttributeError f __class__ __name__ object has no attribute attribute_name classmethod register_function cls function_name function cls functions function_name = function classmethod register_datapipe_as_function cls function_name cls_to_register function_name cls functions raise Exception noqa TRY f Unable add DataPipe function name function_name already taken class_function cls source_dp args kwargs result_pipe = cls source_dp args kwargs result_pipe function = functools partial class_function cls_to_register functools update_wrapper wrapper=function wrapped=cls_to_register assigned= __doc__ cls functions function_name = function __getstate__ Serialize ` lambda ` functions when ` dill ` available If doesn t cover your custom DataPipe s use case consider writing custom methods ` __getstate__ ` ` __setstate__ ` use ` pickle dumps ` serialization state = __dict__ MapDataPipe getstate_hook None MapDataPipe getstate_hook state state __reduce_ex__ args kwargs MapDataPipe reduce_ex_hook None try MapDataPipe reduce_ex_hook except NotImplementedError pass super __reduce_ex__ args kwargs classmethod set_getstate_hook cls hook_fn MapDataPipe getstate_hook None hook_fn None raise RuntimeError Attempt override existing getstate_hook MapDataPipe getstate_hook = hook_fn classmethod set_reduce_ex_hook cls hook_fn MapDataPipe reduce_ex_hook None hook_fn None raise RuntimeError Attempt override existing reduce_ex_hook MapDataPipe reduce_ex_hook = hook_fn __repr__ repr_hook None repr_hook Instead showing torch MapperMapDataPipe object x name str __class__ __qualname__ __str__ str_hook None str_hook Instead showing torch MapperMapDataPipe object x name str __class__ __qualname__ __dir__ auto-completion REPL e g Jupyter notebook list super __dir__ + list functions keys _DataPipeSerializationWrapper __init__ datapipe _datapipe = datapipe __getstate__ use_dill = False try value = pickle dumps _datapipe except Exception HAS_DILL pyrefly ignore missing-attribute value = dill dumps _datapipe use_dill = True raise value use_dill __setstate__ state value use_dill = state use_dill pyrefly ignore missing-attribute _datapipe = dill loads value _datapipe = pickle loads value __len__ try len _datapipe except Exception e raise TypeError f type __name__ instance doesn t have valid length e _IterDataPipeSerializationWrapper _DataPipeSerializationWrapper IterDataPipe __init__ datapipe IterDataPipe _T_co super __init__ datapipe pyrefly ignore invalid-type-var _datapipe_iter Optional Iterator _T_co = None __iter__ - _IterDataPipeSerializationWrapper _datapipe_iter = iter _datapipe __next__ - _T_co type ignore type-var _datapipe_iter None raise AssertionError Iterator has been initialized call __iter__ before __next__ next _datapipe_iter _MapDataPipeSerializationWrapper _DataPipeSerializationWrapper MapDataPipe __getitem__ idx _datapipe idx