Intermediate layer between ` Timer ` ` valgrind ` collections enum dataclasses itertools os pickle re shutil subprocess sys textwrap typing cast Any NamedTuple Optional Union TYPE_CHECKING collections abc Callable collections abc Iterator torch torch utils benchmark utils common cpp_jit torch utils benchmark utils _stubs CallgrindModuleType operator __all__ = FunctionCount FunctionCounts CallgrindStats CopyIfCallgrind TYPE_CHECKING CompletedProcessType = subprocess CompletedProcess str CompletedProcessType = subprocess CompletedProcess FunctionCount NamedTuple TODO Rename count field count int type ignore assignment function str dataclasses dataclass repr=False eq=False frozen=True FunctionCounts Container manipulating Callgrind results It supports Addition subtraction combine diff results Tuple-like indexing A ` denoise ` function which strips CPython calls which known non-deterministic quite noisy Two higher order methods ` filter ` ` transform ` custom manipulation _data tuple FunctionCount inclusive bool truncate_rows bool = True For normal use torch _tensor_str PRINT_OPTS linewidth determines print settings This simply allow hermetic unit tests _linewidth Optional int = None __iter__ - Iterator FunctionCount yield _data __len__ - int len _data __getitem__ item Any - Union FunctionCount FunctionCounts data Union FunctionCount tuple FunctionCount = _data item FunctionCounts cast tuple FunctionCount data inclusive truncate_rows=False isinstance data tuple data __repr__ - str count_len = c _ Account sign string length count_len = max count_len len str c + int c lines = linewidth = _linewidth torch _tensor_str PRINT_OPTS linewidth fn_str_len = max linewidth - count_len - c fn len fn fn_str_len left_len = int fn_str_len - fn = fn left_len + + fn - fn_str_len - left_len - lines append f c count_len fn truncate_rows len lines lines = lines + rjust count_len + + lines - inclusive lines extend f Total sum \n join super __repr__ + lines __add__ other FunctionCounts - FunctionCounts _merge other lambda c c __sub__ other FunctionCounts - FunctionCounts _merge other operator neg __mul__ other Union int float - FunctionCounts _from_dict fn int c other c fn _data inclusive transform map_fn Callable str str - FunctionCounts Apply ` map_fn ` all function names This can used regularize function names e g stripping irrelevant parts file path coalesce entries mapping multiple functions same name which case counts added together etc counts collections defaultdict str int = collections defaultdict int c fn _data counts map_fn fn += c _from_dict counts inclusive filter filter_fn Callable str bool - FunctionCounts Keep only elements where ` filter_fn ` applied function name returns True FunctionCounts tuple i i filter_fn i function inclusive sum - int sum c c _ denoise - FunctionCounts Remove known noisy instructions Several instructions CPython interpreter rather noisy These instructions involve unicode dictionary lookups which Python uses map variable names FunctionCounts generally content agnostic container however sufficiently important obtaining reliable results warrant exception filter lambda fn dictobject c lookdict_unicode fn _merge second FunctionCounts merge_fn Callable int int - FunctionCounts inclusive = second inclusive raise AssertionError Cannot merge inclusive exclusive counts counts collections defaultdict str int = collections defaultdict int c fn counts fn += c c fn second counts fn += merge_fn c _from_dict counts inclusive staticmethod _from_dict counts dict str int inclusive bool - FunctionCounts flat_counts = FunctionCount c fn fn c counts items c FunctionCounts tuple sorted flat_counts reverse=True inclusive dataclasses dataclass repr=False eq=False frozen=True CallgrindStats Top level container Callgrind results collected Timer Manipulation generally done using FunctionCounts which obtained calling ` CallgrindStats stats ` Several convenience methods provided well most significant ` CallgrindStats as_standardized ` task_spec common TaskSpec number_per_run int built_with_debug_symbols bool baseline_inclusive_stats FunctionCounts baseline_exclusive_stats FunctionCounts stmt_inclusive_stats FunctionCounts stmt_exclusive_stats FunctionCounts stmt_callgrind_out Optional str __repr__ - str base_stats = baseline_exclusive_stats output = f super __repr__ task_spec summarize All Noisy symbols removed Instructions counts denoise=False counts denoise=True Baseline base_stats sum base_stats denoise sum number_per_run runs per measurement task_spec num_threads thread s task_spec num_threads strip built_with_debug_symbols output += textwrap dedent Warning PyTorch built debug symbols Source information may limited Rebuild REL_WITH_DEB_INFO= more detailed results output stats inclusive bool = False - FunctionCounts Returns detailed function counts Conceptually FunctionCounts returned can thought tuple count path_and_function_name tuples ` inclusive ` matches semantics callgrind If True counts include instructions executed children ` inclusive=True ` useful identifying hot spots code ` inclusive=False ` useful reducing noise when diffing counts two different runs See CallgrindStats delta more details stmt_inclusive_stats inclusive stmt_exclusive_stats counts denoise bool = False - int Returns total number instructions executed See ` FunctionCounts denoise ` explanation ` denoise ` arg stats = stmt_exclusive_stats stats denoise denoise stats sum FIXME Once minimum version type annotate ` other ` per PEP delta other CallgrindStats inclusive bool = False - FunctionCounts Diff two sets counts One common reason collect instruction counts determine effect particular change will have number instructions needed perform some unit work If change increases number next logical question why This generally involves looking what part code increased instruction count This function automates process so one can easily diff counts both inclusive exclusive basis stats inclusive=inclusive - other stats inclusive=inclusive as_standardized - CallgrindStats Strip library names some prefixes function strings When comparing two different sets instruction counts stumbling block can path prefixes Callgrind includes full filepath when reporting function should However can cause issues when diffing profiles If key component such Python PyTorch built separate locations two profiles which can result something resembling tmp first_build_dir thing c foo tmp first_build_dir thing c bar aten src Aten function_that_actually_changed - tmp second_build_dir thing c bar - tmp second_build_dir thing c foo Stripping prefixes can ameliorate issue regularizing strings causing better cancellation equivalent call sites when diffing strip stats FunctionCounts - FunctionCounts transforms = PyTorch may have been built different locations r ^ +build \ \ build r ^ + + re escape build aten build aten Python Objects come CPython r ^ + + re escape Python Python r ^ + + re escape Objects Objects Strip library name e g ` libtorch so ` r \s\ +\ $ before after transforms stats = stats transform lambda fn re sub before after fn stats CallgrindStats task_spec=self task_spec number_per_run=self number_per_run built_with_debug_symbols=self built_with_debug_symbols baseline_inclusive_stats=strip baseline_inclusive_stats baseline_exclusive_stats=strip baseline_exclusive_stats stmt_inclusive_stats=strip stmt_inclusive_stats stmt_exclusive_stats=strip stmt_exclusive_stats ` as_standardized ` will change symbol names so contents will no longer map directly ` callgrind out ` stmt_callgrind_out=None Serialization enum Enum PICKLE = TORCH = TORCH_JIT = _GLOBALS_ALLOWED_TYPES dict Serialization tuple Any = Serialization PICKLE str bytes bool int float complex Serialization TORCH_JIT torch jit ScriptFunction torch jit ScriptModule Serialization TORCH torch nn Module CopyIfCallgrind Signal global may replaced deserialized copy See ` GlobalsBridge ` why matters __init__ value Any setup Optional str = None method supported_types _GLOBALS_ALLOWED_TYPES items any isinstance value t t supported_types _value Any = value _setup Optional str = setup _serialization Serialization = method break supported_str = \n join getattr t __name__ repr t t chain _GLOBALS_ALLOWED_TYPES values raise ValueError f Unsupported type type value \n f ` collect_callgrind ` restricts globals following types \n f textwrap indent supported_str property value - Any _value property setup - Optional str _setup property serialization - Serialization _serialization staticmethod unwrap_all globals dict str Any - dict str Any k v value isinstance v CopyIfCallgrind v k v globals items GlobalsBridge Handle transfer certain globals when collecting Callgrind statistics Key takeaway Any globals passed must wrapped ` CopyIfCallgrind ` work ` Timer collect_callgrind ` Consider following code snippet ` ` ` pickle timeit Counter value = __call__ value += counter = Counter timeit Timer counter globals= counter counter timeit print counter value timeit Timer counter globals= counter pickle loads pickle dumps counter timeit print counter value Still ` ` ` In first case ` stmt ` executed using objects ` globals ` however addition serialization deserialization changes semantics may meaningfully change behavior This practical consideration when collecting Callgrind statistics Unlike ` exec ` based execution which ` timeit ` uses under hood which can share in-memory data structures caller Callgrind collection requires entirely new process order run under Valgrind This means any data structures used statement execution will have serialized deserialized subprocess In order avoid surprising semantics user invisible process boundaries what can passed through ` globals ` severely restricted ` Timer collect_callgrind ` It expected most setup should achievable albeit perhaps less ergonomically passing ` setup ` string There however exceptions One such TorchScripted functions Because they require concrete file source code possible define them using ` setup ` string Another group torch nn Modules whose construction can complex prohibitively cumbersome coerce into ` setup ` string Finally most builtin types sufficiently well behaved sufficiently common warrant allowing well e g ` globals= n ` very convenient Fortunately all have well defined serialization semantics This responsible enabling Valgrind subprocess use elements ` globals ` so long they allowed type Caveats The user required acknowledge serialization wrapping elements ` globals ` ` CopyIfCallgrind ` While ScriptFunction ScriptModule expected save load quite robustly up user ensure nn Module can un-pickle successfully ` torch Tensor ` ` np ndarray ` deliberately excluded The serialization deserialization process perturbs representation tensor ways could result incorrect measurements For example tensor lives pinned CPU memory fact would preserved dump will turn change performance certain CUDA operations __init__ globals dict str Any data_dir str - None _globals dict str CopyIfCallgrind = _data_dir = data_dir os path exists data_dir os mkdir data_dir globals get torch torch torch raise ValueError ` collect_callgrind ` does support mocking out ` torch ` name value globals items name torch __builtins__ Torch will imported collection script __builtins__ added Timer continue isinstance value CopyIfCallgrind raise ValueError ` collect_callgrind ` requires globals wrapped ` CopyIfCallgrind ` so serialization explicit _globals name = value construct - str load_lines = name wrapped_value _globals items wrapped_value setup None pyrefly ignore bad-argument-type load_lines append textwrap dedent wrapped_value setup wrapped_value serialization == Serialization PICKLE path = os path join _data_dir f name pkl load_lines append pyrefly ignore bad-argument-type f open repr path rb f \n name = pickle load f open path wb f pickle dump wrapped_value value f wrapped_value serialization == Serialization TORCH path = os path join _data_dir f name pt TODO Figure out we can use torch serialization add_safe_globals here Using weights_only=False after change https dev-discuss pytorch org t bc-breaking-change-torch-load-is-being-flipped-to-use-weights-only-true-by-default-in-the-nightlies-after- pyrefly ignore bad-argument-type load_lines append f name = torch load repr path weights_only=False torch save wrapped_value value path wrapped_value serialization == Serialization TORCH_JIT path = os path join _data_dir f name pt pyrefly ignore bad-argument-type load_lines append f name = torch jit load repr path open path wb f torch jit save wrapped_value value f type ignore no-untyped-call raise NotImplementedError f Unknown serialization method wrapped_value serialization \n join load_lines _ValgrindWrapper __init__ - None _bindings_module Optional CallgrindModuleType = None valgrind_symbols = _valgrind_supported_platform _valgrind_toggle _valgrind_toggle_and_dump_stats all hasattr torch _C symbol symbol valgrind_symbols _supported_platform bool = torch _C _valgrind_supported_platform print Callgrind bindings present ` torch _C ` JIT-ing bindings _bindings_module = cpp_jit get_compat_bindings all hasattr _bindings_module symbol symbol valgrind_symbols raise AssertionError JIT-compiled callgrind bindings missing required symbols _supported_platform = _bindings_module _valgrind_supported_platform _commands_available dict str bool = _supported_platform Only bother checking supported platforms cmd valgrind callgrind_control callgrind_annotate _commands_available cmd = subprocess run which cmd capture_output=True check=False returncode _build_type Optional str = None build_search = re search BUILD_TYPE= + torch __config__ show type ignore no-untyped-call build_search None _build_type = build_search groups split _validate - None _supported_platform raise OSError Valgrind supported platform missing_cmds = cmd cmd available _commands_available items available missing_cmds raise OSError Missing + join missing_cmds collect_callgrind task_spec common TaskSpec globals dict str Any number int repeats int collect_baseline bool is_python bool retain_out_file bool - tuple CallgrindStats Collect stats attach reference run which can used filter interpreter overhead _validate is_python collect_baseline raise AssertionError collect_baseline only supported Python timers task_stats baseline_stats = _invoke task_spec=task_spec globals=globals number=number repeats=repeats collect_baseline=collect_baseline is_python=is_python retain_out_file=retain_out_file len task_stats = repeats raise AssertionError Unexpected number task stats returned _invoke tuple CallgrindStats task_spec=task_spec number_per_run=number built_with_debug_symbols=self _build_type == RelWithDebInfo baseline_inclusive_stats=baseline_stats baseline_exclusive_stats=baseline_stats stmt_inclusive_stats=stmt_inclusive_stats stmt_exclusive_stats=stmt_exclusive_stats stmt_callgrind_out=out_contents stmt_inclusive_stats stmt_exclusive_stats out_contents task_stats _invoke task_spec common TaskSpec globals dict str Any number int repeats int collect_baseline bool is_python bool retain_out_file bool - tuple tuple FunctionCounts FunctionCounts Optional str Core invocation method Callgrind collection Valgrind operates effectively replacing CPU emulated version which allows instrument any code cost severe performance degradation This has practical effect order collect Callgrind statistics new process has created running under ` valgrind ` The steps process Create scratch directory Codegen run script _ValgrindWrapper _construct_script Inside run script Validate Python torch match parent process Validate indeed running under valgrind Execute ` setup ` warm up ` stmt ` Begin collecting stats Run ` stmt ` loop Stop collecting stats Parse run results Cleanup scratch directory working_dir = common _make_temp_dir prefix= callgrind data_dir = os path join working_dir data script_file = os path join working_dir timer_callgrind py callgrind_out = os path join working_dir callgrind out error_log = os path join working_dir error txt stat_log = os path join working_dir callgrind_stat txt stdout_stderr_log = os path join working_dir stdout_stderr log run args list str kwargs Any - tuple CompletedProcessType str https thraxil org users anders posts Subprocess-Hanging-PIPE-is-your-enemy f_stdout_stderr = open stdout_stderr_log wb try invocation = subprocess run args stdout=f_stdout_stderr stderr=subprocess STDOUT kwargs open stdout_stderr_log f invocation f read finally f_stdout_stderr close try is_python _bindings_module None shutil copy _bindings_module __file__ os path join working_dir os path split _bindings_module __file__ script_file = os path join working_dir timer_callgrind py open script_file w f f write _construct_script task_spec globals=GlobalsBridge globals data_dir number=number repeats=repeats collect_baseline=collect_baseline error_log=error_log stat_log=stat_log bindings=self _bindings_module run_loop_cmd = python script_file collect_baseline raise AssertionError collect_baseline must False non-Python timers run_loop_exec = cpp_jit compile_callgrind_template stmt=task_spec stmt setup=task_spec setup global_setup=task_spec global_setup run_loop_cmd = run_loop_exec -- number str number -- number-warmup str min number -- repeats str repeats -- number-threads str task_spec num_threads valgrind_invocation valgrind_invocation_output = run valgrind -- tool=callgrind f -- callgrind-out-file= callgrind_out -- dump-line=yes -- dump-instr=yes -- instr-atstart=yes -- collect-atstart=no + run_loop_cmd valgrind_invocation returncode error_report = os path exists error_log open error_log f error_report = f read error_report error_report = Unknown error \n + valgrind_invocation_output raise OSError f Failed collect callgrind profile \n error_report parse_output fpath str inclusive bool - FunctionCounts _annotate_invocation annotate_invocation_output = run callgrind_annotate f -- inclusive= yes inclusive no -- threshold= -- show-percs=no fpath check=True total_pattern = re compile r ^ - + \s+PROGRAM TOTALS begin_pattern = re compile r Ir\s+file function function_pattern = re compile r ^\s - + \s+ + + $ ScanState enum Enum SCANNING_FOR_TOTAL = SCANNING_FOR_START = PARSING = scan_state = ScanState SCANNING_FOR_TOTAL fn_counts = l annotate_invocation_output splitlines keepends=False scan_state == ScanState SCANNING_FOR_TOTAL total_match = total_pattern match l total_match program_totals = int total_match groups replace scan_state = ScanState SCANNING_FOR_START scan_state == ScanState SCANNING_FOR_START begin_pattern match l scan_state = ScanState PARSING scan_state = ScanState PARSING raise AssertionError Failed enter PARSING state while parsing callgrind_annotate output fn_match = function_pattern match l fn_match ir_str file_function = fn_match groups ir = int ir_str replace ir == program_totals type ignore possibly-undefined Callgrind includes some top level red herring symbols when program dumps multiple profiles continue fn_counts append FunctionCount ir file_function re match r -+ l Ignore heading separator lines continue break scan_state = ScanState PARSING raise AssertionError f Failed parse fpath FunctionCounts tuple sorted fn_counts reverse=True inclusive=inclusive read_results i int - tuple FunctionCounts FunctionCounts Optional str i == repeats collect_baseline Null baseline FunctionCounts inclusive=True FunctionCounts inclusive=False None fpath = f callgrind_out i + Callgrind one-indexes files callgrind_out_contents Optional str = None retain_out_file open fpath f callgrind_out_contents = f read parse_output fpath inclusive=True parse_output fpath inclusive=False callgrind_out_contents tuple read_results i i range repeats + finally shutil rmtree working_dir staticmethod _construct_script task_spec common TaskSpec globals GlobalsBridge number int repeats int collect_baseline bool error_log str stat_log str bindings Optional CallgrindModuleType - str block_stmt stmt str indent int = - str Partially unroll benchmark loop The naive template looks something like _ range number stmt However loop Python surprisingly expensive significantly increases number background Python instructions So instead we partially unroll loops block size chosen keep instruction overhead ` range ` low while also ballooning size generated file block_size = loop_count = number block_size loop_count == There no point having ` _ range ` rather than just ` ` lets us save shave few background instructions loop_count = remainder = number - block_size loop_count blocked_stmt = loop_count unrolled_stmts = textwrap indent \n join stmt block_size blocked_stmt += f _ range loop_count \n unrolled_stmts \n remainder blocked_stmt += \n join stmt remainder textwrap indent blocked_stmt indent pass_baseline = callgrind_bindings _valgrind_toggle \n f block_stmt pass \n callgrind_bindings _valgrind_toggle_and_dump_stats textwrap dedent r gc os pickle subprocess sys time Mitigate https github com pytorch pytorch issues which can sometimes cause subprocess call fail numpy np torch torch set_num_threads num_threads bindings_import PID = os getpid log_failure msg open error_log_repr wt f f write msg sys exit check_result completed_process completed_process returncode log_failure f Command failed join completed_process args completed_process ============================================================================= == Check subprocess matches parent ===================================== ============================================================================= os path realpath sys executable = parent_interpreter log_failure Interpreter mismatch \n f os path realpath sys executable \n vs \n parent_interpreter torch __file__ = torch_file log_failure PyTorch does match expected file \n f torch __file__ \n vs \n torch_file ============================================================================= == User specified setup ===================================================== ============================================================================= Load serialized globals load_globals User setup str setup _ range warmup_number indented_stmt ============================================================================= == Callgrind management ===================================================== ============================================================================= open stat_log wb stat_file If many instances callgrind running once output ` callgrind_control ` may exceed kb which would cause ` subprocess PIPE ` deadlock So instead we use file callgrind_stat = check_result subprocess run callgrind_control -- stat stdout=stat_file stderr=subprocess STDOUT open stat_log rt stat_file stat_lines = stat_file read splitlines f PID PID python __file__ stat_lines log_failure Process does appear running callgrind gc collect time sleep ============================================================================= == User code block ========================================================== ============================================================================= _ range repeats callgrind_bindings _valgrind_toggle blocked_stmt callgrind_bindings _valgrind_toggle_and_dump_stats gc collect baseline strip format indented_stmt=textwrap indent task_spec stmt blocked_stmt=block_stmt task_spec stmt indent= baseline= pass_baseline collect_baseline number=number repeats=repeats load_globals=globals construct setup=task_spec setup warmup_number=min number num_threads=task_spec num_threads error_log_repr=repr error_log stat_log=stat_log parent_interpreter=os path realpath sys executable torch_file=torch __file__ bindings_import= torch _C callgrind_bindings bindings None f bindings __name__ callgrind_bindings CALLGRIND_SINGLETON Optional _ValgrindWrapper = None wrapper_singleton - _ValgrindWrapper global CALLGRIND_SINGLETON CALLGRIND_SINGLETON None CALLGRIND_SINGLETON = _ValgrindWrapper CALLGRIND_SINGLETON