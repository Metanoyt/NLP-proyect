Owner s module nn math random string unittest functools reduce operator mul torch torch nn functional F torch nn init init torch testing _internal common_utils run_tests skipIfNoLapack skipIfTorchDynamo slowTest TEST_SCIPY TestCase TEST_SCIPY scipy stats TestNNInit TestCase setUp super setUp random seed _is_normal tensor mean std samples = tensor view - tolist p_value = stats kstest samples norm args= mean std p_value _is_trunc_normal tensor mean std b scipy s trunc norm suited data drawn N so we need transform our data test using scipy z_samples = tensor view - - mean std z_samples = z_samples tolist = - mean std b = b - mean std p_value = stats kstest z_samples truncnorm args= b p_value _is_uniform tensor b samples = tensor view - tolist p_value = stats kstest samples uniform args= b - p_value _create_random_nd_tensor dims size_min size_max size = random randint size_min size_max _ range dims tensor = torch zeros size tensor _random_float b b - random random + test_calculate_gain_linear fn linear conv d conv d conv d conv_transpose d conv_transpose d conv_transpose d gain = init calculate_gain fn assertEqual gain test_calculate_gain_nonlinear fn sigmoid tanh relu leaky_relu gain = init calculate_gain fn fn == sigmoid assertEqual gain fn == tanh assertEqual gain fn == relu sqrt assertEqual gain fn == leaky_relu sqrt + slope^ assertEqual gain fn == selu assertEqual gain test_calculate_gain_leaky_relu param None gain = init calculate_gain leaky_relu param param None Default slope assertEqual gain param == No slope = same gain normal ReLU assertEqual gain param == assertEqual gain param == assertEqual gain test_calculate_gain_leaky_relu_only_accepts_numbers param True b assertRaises ValueError init calculate_gain leaky_relu param test_calculate_gain_only_accepts_valid_nonlinearities n Generate random strings lengths definitely aren t supported random_string = join random choice string ascii_lowercase i range n assertRaises ValueError init calculate_gain random_string unittest skipIf TEST_SCIPY Scipy found skipIfTorchDynamo scipy kstest failing under dynamo test_uniform dims input_tensor = _create_random_nd_tensor dims size_min= size_max= = _random_float - b = + _random_float init uniform_ input_tensor a=a b=b assert _is_uniform input_tensor b unittest skipIf TEST_SCIPY Scipy found skipIfTorchDynamo scipy kstest failing under dynamo test_normal dims input_tensor = _create_random_nd_tensor dims size_min= size_max= mean = _random_float - std = _random_float init normal_ input_tensor mean=mean std=std assert _is_normal input_tensor mean std unittest skipIf TEST_SCIPY Scipy found skipIfTorchDynamo scipy kstest failing under dynamo test_trunc_normal dims input_tensor = _create_random_nd_tensor dims size_min= size_max= mean = _random_float - std = _random_float = _random_float mean - std mean b = _random_float mean mean + std init trunc_normal_ input_tensor mean=mean std=std a=a b=b assert _is_trunc_normal input_tensor mean std b unittest skipIf TEST_SCIPY Scipy found skipIfTorchDynamo scipy kstest failing under dynamo test_trunc_normal_generator gen = torch Generator gen manual_seed input_tensor = torch empty init trunc_normal_ input_tensor generator=gen ref = torch empty torch manual_seed init trunc_normal_ ref assertEqual input_tensor ref assert _is_trunc_normal input_tensor mean= std= a= b= test_constant dims input_tensor = _create_random_nd_tensor dims size_min= size_max= val = _random_float init constant_ input_tensor val assertEqual input_tensor input_tensor clone fill_ val test_ones_and_zeros init_fn_ val zip init ones_ init zeros_ dims input_tensor = _create_random_nd_tensor dims size_min= size_max= init_fn_ input_tensor assertEqual input_tensor input_tensor clone fill_ val test_eye input_tensor = _create_random_nd_tensor size_min= size_max= init eye_ input_tensor Check every single element i range input_tensor size j range input_tensor size i == j assert input_tensor i j == assert input_tensor i j == test_eye_only_works_on_ d_inputs dims assertRaises ValueError tensor = _create_random_nd_tensor dims size_min= size_max= init eye_ tensor test_dirac_properties dims groups prepare random tensor random sizes fits groups c d e = random randint _ range b = random randint groups same range groups all range allowed make sure first dim divides groups input_tensor = torch randn groups b c d e dims init dirac_ input_tensor groups c_out c_in = input_tensor size groups input_tensor size min_d = min c_out c_in Check number nonzeros equivalent smallest dim each group assert torch nonzero input_tensor size == min_d groups Check sum values can have precision issues hence assertEqual also equivalent assertEqual input_tensor sum min_d groups test_dirac_identity groups batch in_c out_c size kernel_size = in_c out_c must divide groups eff_out_c = out_c groups Test D input_var = torch randn batch in_c size filter_var = torch zeros eff_out_c in_c kernel_size filter_var = torch cat filter_var groups init dirac_ filter_var groups output_var = F conv d input_var filter_var input_tensor output_tensor = input_var data output_var data Variables do support nonzero g range groups Assert in_c outputs preserved per each group assertEqual input_tensor - output_tensor eff_out_c g eff_out_c g + in_c Assert extra outputs assert torch nonzero output_tensor eff_out_c g + in_c eff_out_c g + numel == Test D input_var = torch randn batch in_c size size filter_var = torch zeros eff_out_c in_c kernel_size kernel_size filter_var = torch cat filter_var groups init dirac_ filter_var groups output_var = F conv d input_var filter_var input_tensor output_tensor = input_var data output_var data Variables do support nonzero g range groups Assert in_c outputs preserved per each group assertEqual input_tensor - - output_tensor eff_out_c g eff_out_c g + in_c Assert extra outputs assert torch nonzero output_tensor eff_out_c g + in_c eff_out_c g + numel == Test D input_var = torch randn batch in_c size size size filter_var = torch zeros eff_out_c in_c kernel_size kernel_size kernel_size filter_var = torch cat filter_var groups init dirac_ filter_var groups output_var = F conv d input_var filter_var input_tensor output_tensor = input_var data output_var data g range groups Assert in_c outputs preserved per each group assertEqual input_tensor - - - output_tensor eff_out_c g eff_out_c g + in_c Assert extra outputs assert torch nonzero output_tensor eff_out_c g + in_c eff_out_c g + numel == test_dirac_only_works_on_ _ _ d_inputs dims assertRaises ValueError tensor = _create_random_nd_tensor dims size_min= size_max= init dirac_ tensor test_xavier_uniform_errors_on_inputs_smaller_than_ d dims tensor = _create_random_nd_tensor dims size_min= size_max= assertRaises ValueError init xavier_uniform_ tensor test_xavier_normal_errors_on_inputs_smaller_than_ d dims tensor = _create_random_nd_tensor dims size_min= size_max= assertRaises ValueError init xavier_normal_ tensor unittest skipIf TEST_SCIPY Scipy found slowTest test_xavier_uniform use_gain True False dims input_tensor = _create_random_nd_tensor dims size_min= size_max= gain = use_gain gain = _random_float init xavier_uniform_ input_tensor gain=gain init xavier_uniform_ input_tensor fan_in = input_tensor size fan_out = input_tensor size input_tensor dim fan_in = input_tensor numel fan_out = input_tensor numel expected_std = gain math sqrt fan_in + fan_out bounds = expected_std math sqrt assert _is_uniform input_tensor -bounds bounds unittest skipIf TEST_SCIPY Scipy found skipIfTorchDynamo scipy kstest failing under dynamo test_xavier_normal use_gain True False dims input_tensor = _create_random_nd_tensor dims size_min= size_max= gain = use_gain gain = _random_float init xavier_normal_ input_tensor gain=gain init xavier_normal_ input_tensor fan_in = input_tensor size fan_out = input_tensor size input_tensor dim fan_in = input_tensor numel fan_out = input_tensor numel expected_std = gain math sqrt fan_in + fan_out assert _is_normal input_tensor expected_std test_kaiming_uniform_errors_on_inputs_smaller_than_ d dims assertRaises ValueError tensor = _create_random_nd_tensor dims size_min= size_max= init kaiming_uniform_ tensor test_kaiming_normal_errors_on_inputs_smaller_than_ d dims assertRaises ValueError tensor = _create_random_nd_tensor dims size_min= size_max= init kaiming_normal_ tensor test_kaiming_uniform_warning_on_ element_tensor tensor = torch empty assertWarnsRegex UserWarning Initializing zero-element tensors no-op _ = init kaiming_uniform_ tensor test_kaiming_normal_warning_on_ element_tensor tensor = torch empty assertWarnsRegex UserWarning Initializing zero-element tensors no-op _ = init kaiming_normal_ tensor unittest skipIf TEST_SCIPY Scipy found skipIfTorchDynamo scipy kstest failing under dynamo test_kaiming_uniform use_a True False dims mode fan_in fan_out input_tensor = _create_random_nd_tensor dims size_min= size_max= use_a = _random_float init kaiming_uniform_ input_tensor a=a mode=mode = init kaiming_uniform_ input_tensor mode=mode fan_in = input_tensor size fan_out = input_tensor size input_tensor dim fan_in = input_tensor numel fan_out = input_tensor numel mode == fan_in n = fan_in n = fan_out expected_std = math sqrt + n bounds = expected_std math sqrt assert _is_uniform input_tensor -bounds bounds unittest skipIf TEST_SCIPY Scipy found skipIfTorchDynamo scipy kstest failing under dynamo test_kaiming_normal use_a True False dims mode fan_in fan_out input_tensor = _create_random_nd_tensor dims size_min= size_max= use_a = _random_float init kaiming_normal_ input_tensor a=a mode=mode = init kaiming_normal_ input_tensor mode=mode fan_in = input_tensor size fan_out = input_tensor size input_tensor dim fan_in = input_tensor numel fan_out = input_tensor numel mode == fan_in n = fan_in n = fan_out expected_std = math sqrt + n assert _is_normal input_tensor expected_std test_sparse_only_works_on_ d_inputs dims assertRaises ValueError sparsity = _random_float tensor = _create_random_nd_tensor dims size_min= size_max= init sparse_ tensor sparsity unittest skipIf TEST_SCIPY Scipy found skipIfTorchDynamo scipy kstest failing under dynamo test_sparse_default_std use_random_std True False input_tensor = _create_random_nd_tensor size_min= size_max= rows = input_tensor size sparsity = _random_float std = default std use_random_std std = _random_float init sparse_ input_tensor sparsity=sparsity std=std init sparse_ input_tensor sparsity=sparsity col_idx range input_tensor size column = input_tensor col_idx assert column column == nelement = math ceil sparsity rows assert _is_normal input_tensor input_tensor = std skipIfNoLapack test_orthogonal use_gain True False tensor_size input_tensor = torch zeros tensor_size gain = use_gain gain = _random_float init orthogonal_ input_tensor gain=gain init orthogonal_ input_tensor rows cols = tensor_size reduce mul tensor_size flattened_tensor = input_tensor view rows cols rows cols assertEqual torch mm flattened_tensor t flattened_tensor torch eye cols gain atol= e- rtol= assertEqual torch mm flattened_tensor flattened_tensor t torch eye rows gain atol= e- rtol= test_deprecation x = torch randn fn init normal x assertWarnsRegex FutureWarning deprecated msg= methods suffixed underscore should deprecated fn __name__ == __main__ run_tests