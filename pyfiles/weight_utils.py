collections abc Callable typing Optional torch torch ao nn intrinsic nni torch ao nn intrinsic qat nniqat torch ao nn intrinsic quantized nniq torch ao nn qat nnqat torch ao nn quantized nnq torch ao nn quantized dynamic nnqd torch nn nn torch nn functional F torch fx GraphModule torch fx graph Node ns_types NSSingleResultType NSSingleResultValuesType utils get_target_type_str getattr_from_fqn return_first_non_observer_node toq = torch ops quantized mod_weight_detach mod nn Module - torch Tensor mod weight detach type ignore operator mod_ _weight_detach mod nn Module - torch Tensor mod weight detach type ignore index mod_weight_bias_ mod nn Module - torch Tensor mod _weight_bias type ignore operator get_lstm_weight mod nn Module - list torch Tensor res = idx param_name enumerate mod _flat_weights_names type ignore arg-type weight_ih_l param_name weight_hh_l param_name param_value = mod _flat_weights idx detach type ignore index union-attr res append param_value res get_qlstm_weight mod nn Module - list torch Tensor res = weight_value mod _all_weight_values type ignore union-attr res append weight_value param __getstate__ __getstate__ res append weight_value param __getstate__ __getstate__ res get_conv_mod_weight mod nn Module - torch Tensor isinstance mod nn Conv d nn Conv d nn Conv d mod weight detach isinstance mod nni ConvReLU d nni ConvReLU d nni ConvReLU d mod weight detach type ignore operator mod _weight_bias type ignore operator get_linear_mod_weight mod nn Module - torch Tensor isinstance mod nn Linear mod weight detach isinstance mod nni LinearReLU mod weight detach type ignore operator mod _weight_bias type ignore operator get_lstm_mod_weights mod nn Module - list torch Tensor TODO future PR make more generic handle everything isinstance mod nn LSTM res = idx param_name enumerate mod _flat_weights_names weight_ih_l param_name weight_hh_l param_name param_value = mod _flat_weights idx detach type ignore index union-attr res append param_value res isinstance mod nnqd LSTM raise AssertionError f type type mod handled yet res = weight_value mod _all_weight_values res append weight_value param __getstate__ __getstate__ type ignore index res append weight_value param __getstate__ __getstate__ type ignore index res get_conv_fun_weight node Node gm GraphModule - torch Tensor traverse backwards weight arg accounting any observers weight_arg_node = node args isinstance weight_arg_node Node raise AssertionError f Expected Node got type weight_arg_node weight_node = return_first_non_observer_node weight_arg_node gm isinstance weight_node Node raise AssertionError f Expected Node got type weight_node weight_node op = get_attr raise AssertionError f Expected get_attr got weight_node op weight = getattr_from_fqn gm weight_node target type ignore arg-type weight detach get_qconv_fun_weight node Node gm GraphModule - torch Tensor qconv state arg qconv_state_node = node args isinstance qconv_state_node Node raise AssertionError f Expected Node got type qconv_state_node qconv_state_node op = get_attr raise AssertionError f Expected get_attr got qconv_state_node op qconv_state_obj = getattr_from_fqn gm qconv_state_node target type ignore arg-type qconv_state_obj weight get_linear_fun_weight node Node gm GraphModule - torch Tensor traverse backwards weight arg accounting any observers supported patterns weight - obs - linear weight - torch float - dequantize - linear linear_second_arg = node args isinstance linear_second_arg Node raise AssertionError f Expected Node got type linear_second_arg linear_second_arg op == call_module weight - obs - linear weight_arg_node = node args isinstance weight_arg_node Node raise AssertionError f Expected Node got type weight_arg_node weight_node = weight_arg_node args isinstance weight_node Node raise AssertionError f Expected Node got type weight_node weight_node op = get_attr raise AssertionError f Expected get_attr got weight_node op weight = getattr_from_fqn gm weight_node target type ignore arg-type weight detach linear_second_arg op == call_method weight - torch float - dequantize - linear linear_second_arg op = call_method raise AssertionError f Expected call_method got linear_second_arg op dequant_node = node args isinstance dequant_node Node raise AssertionError f Expected Node got type dequant_node to_fp _node = dequant_node args isinstance to_fp _node Node raise AssertionError f Expected Node got type to_fp _node extract dtype so we can cast before returning target_dtype = to_fp _node args weight_node = to_fp _node args isinstance weight_node Node raise AssertionError f Expected Node got type weight_node weight_node op = get_attr raise AssertionError f Expected get_attr got weight_node op weight = getattr_from_fqn gm weight_node target type ignore arg-type weight fp cast weight detach target_dtype linear_second_arg op = get_attr raise AssertionError f Expected get_attr got linear_second_arg op weight = getattr_from_fqn gm linear_second_arg target type ignore arg-type weight detach get_qlinear_fun_weight node Node gm GraphModule - torch Tensor packed weight arg packed_weight_node = node args isinstance packed_weight_node Node raise AssertionError f Expected Node got type packed_weight_node packed_weight_node op = get_attr raise AssertionError f Expected get_attr got packed_weight_node op packed_weight = getattr_from_fqn gm packed_weight_node target type ignore arg-type TODO future PR why does packed_weight unpack work weight _bias _name = packed_weight __getstate__ weight get_op_to_type_to_weight_extraction_fn - dict str dict Callable Callable op_to_type_to_weight_extraction_fn dict str dict Callable Callable = call_module Conv d nn Conv d mod_weight_detach nni ConvReLU d mod_ _weight_detach nnq Conv d mod_weight_bias_ nnqat Conv d mod_weight_detach nniqat ConvBn d mod_weight_detach nniqat ConvBnReLU d mod_weight_detach nniqat ConvReLU d mod_weight_detach nniq ConvReLU d mod_weight_bias_ Conv d nn Conv d mod_weight_detach nni ConvReLU d mod_ _weight_detach nnq Conv d mod_weight_bias_ nnqat Conv d mod_weight_detach nniqat ConvBn d mod_weight_detach nniqat ConvBnReLU d mod_weight_detach nniqat ConvReLU d mod_weight_detach nniq ConvReLU d mod_weight_bias_ Conv d nn Conv d mod_weight_detach nni ConvReLU d mod_ _weight_detach nnq Conv d mod_weight_bias_ nnqat Conv d mod_weight_detach nniqat ConvBn d mod_weight_detach nniqat ConvBnReLU d mod_weight_detach nniqat ConvReLU d mod_weight_detach nniq ConvReLU d mod_weight_bias_ Linear nn Linear mod_weight_detach nnq Linear mod_weight_bias_ nni LinearReLU mod_ _weight_detach nniq LinearReLU mod_weight_bias_ nnqat Linear mod_weight_detach nnqd Linear mod_weight_bias_ nniqat LinearReLU mod_weight_detach nniqat LinearBn d mod_weight_detach nn modules linear NonDynamicallyQuantizableLinear mod_weight_detach LSTM nn LSTM get_lstm_weight nnqd LSTM get_qlstm_weight call_function Conv F conv d get_conv_fun_weight F conv d get_conv_fun_weight F conv d get_conv_fun_weight toq conv d get_qconv_fun_weight toq conv d get_qconv_fun_weight toq conv d get_qconv_fun_weight toq conv d_relu get_qconv_fun_weight toq conv d_relu get_qconv_fun_weight toq conv d_relu get_qconv_fun_weight Linear F linear get_linear_fun_weight toq linear get_qlinear_fun_weight toq linear_relu get_qlinear_fun_weight op_to_type_to_weight_extraction_fn extract_weight_from_node node Node gm GraphModule op_to_type_to_weight_extraction_fn Optional dict str dict Callable Callable = None - Optional NSSingleResultType res_type = NSSingleResultValuesType WEIGHT value Not all graphmodules have _node_name_to_scope so only fill out exists fqn = None hasattr gm _node_name_to_scope fqn = gm _node_name_to_scope node name type ignore index op_to_type_to_weight_extraction_fn None op_to_type_to_weight_extraction_fn = get_op_to_type_to_weight_extraction_fn ref_node_type = get_target_type_str node gm extracting weights these always same prev_node_type = ref_node_type node op == call_function function_mapping = op_to_type_to_weight_extraction_fn call_function target_fn_type weight_extraction_fn function_mapping items node target == target_fn_type weight = weight_extraction_fn node gm type res_type values weight prev_node_name node name prev_node_target_type prev_node_type ref_node_name node name ref_node_target_type ref_node_type index_within_arg index_of_arg fqn fqn node op == call_module call_module we need look up modules do type check isinstance node target str raise AssertionError f Expected str got type node target mod = getattr_from_fqn gm node target module_mapping = op_to_type_to_weight_extraction_fn call_module target_mod_type weight_extraction_fn module_mapping items type mod target_mod_type weight = weight_extraction_fn mod type res_type values weight prev_node_name node name prev_node_target_type prev_node_type ref_node_name node name ref_node_target_type ref_node_type index_within_arg index_of_arg fqn fqn None