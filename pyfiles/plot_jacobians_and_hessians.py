============================= Jacobians hessians more ============================= Computing jacobians hessians useful number non-traditional deep learning models It difficult annoying compute these quantities efficiently using standard autodiff system like PyTorch Autograd functorch provides ways computing various higher-order autodiff quantities efficiently functools partial torch torch nn functional F torch manual_seed ###################################################################### Setup Comparing functorch vs naive approach -------------------------------------------------------------------- Let s start function we d like compute jacobian This simple linear function non-linear activation predict weight bias x F linear x weight bias tanh Here s some dummy data weight bias feature vector D = weight = torch randn D D bias = torch randn D x = torch randn D Let s think ` ` predict ` ` function maps input ` ` x ` ` R^D - R^D PyTorch Autograd computes vector-Jacobian products In order compute full Jacobian R^D - R^D function we would have compute row-by-row using different unit vector each time xp = x clone requires_grad_ unit_vectors = torch eye D compute_jac xp jacobian_rows = torch autograd grad predict weight bias xp xp vec vec unit_vectors torch stack jacobian_rows jacobian = compute_jac xp Instead computing jacobian row-by-row we can use ` ` vmap ` ` get rid for-loop vectorize computation We can t directly apply vmap PyTorch Autograd instead functorch provides ` ` vjp ` ` transform functorch vjp vmap _ vjp_fn = vjp partial predict weight bias x ft_jacobian = vmap vjp_fn unit_vectors assert torch allclose ft_jacobian jacobian In another tutorial composition reverse-mode AD vmap gave us per-sample-gradients In tutorial composing reverse-mode AD vmap gives us Jacobian computation Various compositions vmap autodiff transforms can give us different interesting quantities functorch provides ` ` jacrev ` ` convenience function performs vmap-vjp composition compute jacobians ` ` jacrev ` ` accepts argnums argument says which argument we would like compute Jacobians respect functorch jacrev ft_jacobian = jacrev predict argnums= weight bias x assert torch allclose ft_jacobian jacobian Let s compare performance two ways compute jacobian The functorch version much faster becomes even faster more outputs there In general we expect vectorization via ` ` vmap ` ` can help eliminate overhead give better utilization your hardware torch utils benchmark Timer without_vmap = Timer stmt= compute_jac xp globals=globals with_vmap = Timer stmt= jacrev predict argnums= weight bias x globals=globals print without_vmap timeit print with_vmap timeit It s pretty easy flip problem around say we want compute Jacobians parameters our model weight bias instead input ft_jac_weight ft_jac_bias = jacrev predict argnums= weight bias x ###################################################################### reverse-mode Jacobian jacrev vs forward-mode Jacobian jacfwd -------------------------------------------------------------------- We offer two APIs compute jacobians jacrev jacfwd - jacrev uses reverse-mode AD As you saw above composition our vjp vmap transforms - jacfwd uses forward-mode AD It implemented composition our jvp vmap transforms jacfwd jacrev can substituted each other have different performance characteristics As general rule thumb you re computing jacobian R^N - R^M function there many more outputs than inputs i e M N then jacfwd preferred otherwise use jacrev There exceptions rule non-rigorous argument follows In reverse-mode AD we computing jacobian row-by-row while forward-mode AD which computes Jacobian-vector products we computing column-by-column The Jacobian matrix has M rows N columns functorch jacfwd jacrev Benchmark more inputs than outputs Din = Dout = weight = torch randn Dout Din bias = torch randn Dout x = torch randn Din using_fwd = Timer stmt= jacfwd predict argnums= weight bias x globals=globals using_bwd = Timer stmt= jacrev predict argnums= weight bias x globals=globals print f jacfwd time using_fwd timeit print f jacrev time using_bwd timeit Benchmark more outputs than inputs Din = Dout = weight = torch randn Dout Din bias = torch randn Dout x = torch randn Din using_fwd = Timer stmt= jacfwd predict argnums= weight bias x globals=globals using_bwd = Timer stmt= jacrev predict argnums= weight bias x globals=globals print f jacfwd time using_fwd timeit print f jacrev time using_bwd timeit ###################################################################### Hessian computation functorch hessian -------------------------------------------------------------------- We offer convenience API compute hessians functorch hessian Hessians jacobian jacobian which suggests one can just compose functorch s jacobian transforms compute one Indeed under hood ` ` hessian f ` ` simply ` ` jacfwd jacrev f ` ` Depending your model you may want use ` ` jacfwd jacfwd f ` ` ` ` jacrev jacrev f ` ` instead compute hessians functorch hessian TODO make sure PyTorch has tanh_backward implemented jvp hess = hessian predict argnums= weight bias x hess = jacfwd jacfwd predict argnums= argnums= weight bias x hess = jacrev jacrev predict argnums= argnums= weight bias x ###################################################################### Batch Jacobian Batch Hessian -------------------------------------------------------------------- In above examples we ve been operating single feature vector In some cases you might want take Jacobian batch outputs respect batch inputs where each input produces independent output That given batch inputs shape B N function goes B N - B M we would like Jacobian shape B M N The easiest way do sum over batch dimension then compute Jacobian function predict_with_output_summed weight bias x predict weight bias x sum batch_size = Din = Dout = weight = torch randn Dout Din bias = torch randn Dout x = torch randn batch_size Din batch_jacobian = jacrev predict_with_output_summed argnums= weight bias x If you instead have function goes R^N - R^M inputs batched you compose vmap jacrev compute batched jacobians compute_batch_jacobian = vmap jacrev predict argnums= in_dims= None None batch_jacobian = compute_batch_jacobian weight bias x assert torch allclose batch_jacobian batch_jacobian Finally batch hessians can computed similarly It s easiest think about them using vmap batch over hessian computation some cases sum trick also works compute_batch_hessian = vmap hessian predict argnums= in_dims= None None batch_hess = compute_batch_hessian weight bias x