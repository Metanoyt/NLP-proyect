Owner s oncall distributed copy functools itertools os sys unittest typing Any Optional torch torch nn nn torch distributed dist torch distributed fsdp BackwardPrefetch CPUOffload FullyShardedDataParallel FSDP MixedPrecision ShardingStrategy StateDictType torch distributed fsdp _common_utils clean_tensor_name torch distributed fsdp _flat_param _FSDP_SKIP_WRITEBACK_CHECK _FSDP_USE_FULL_PREC_IN_EVAL torch distributed fsdp _init_utils NO_RESHARD_AFTER_FORWARD_STRATEGIES torch distributed fsdp wrap always_wrap_policy ModuleWrapPolicy torch nn TransformerDecoderLayer TransformerEncoderLayer torch nn parallel distributed DistributedDataParallel DDP torch testing _internal common_cuda TEST_CUDA torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp DEVICEInitMode FSDPInitMode FSDPTest TransformerWithSharedParams torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests TEST_WITH_DEV_DBG_ASAN TEST_XPU TestCase torch testing _internal inductor_utils HAS_GPU dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit device_type = acc type acc = torch accelerator current_accelerator cpu TestFSDPUseOrigParamsMultipleParamGroups FSDPTest Tests multiple parameter groups property world_size - int _get_param_groups model nn Module - list dict str Any Constructs separate parameter groups weights biases other parameters param_groups = params weight_decay lr e- params weight_decay lr e- params param_name param model named_parameters weight param_name param_groups params append param bias param_name param_groups params append param param_groups params append param param_groups _get_optim model nn Module optim_class type torch optim Optimizer multi_tensor bool - torch optim Optimizer Constructs Adam optimizer three parameter groups one weights one biases one everything each different weight decay learning rates param_groups = _get_param_groups model optim_class param_groups lr= e- foreach=multi_tensor _get_ddp_transformer find_unused_params bool - DDP Returns transformer shared parameters wrapped DDP model = TransformerWithSharedParams init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE deterministic=True ddp_model = DDP model device_ids= rank find_unused_parameters=find_unused_params ddp_model _get_fsdp_transformer_and_optim device_init_mode DEVICEInitMode init_optim_before_wrap bool optim_class type torch optim Optimizer multi_tensor bool sharding_strategy ShardingStrategy backward_prefetch Optional BackwardPrefetch cpu_offload CPUOffload - tuple FSDP torch optim Optimizer Returns transformer shared parameters wrapped FSDP corresponding optimizer Each transformer layer has multiple linear layers so policy combination parameter group construction ensures different hyperparameter settings within one ` FlatParameter ` fsdp_kwargs = auto_wrap_policy ModuleWrapPolicy TransformerEncoderLayer TransformerDecoderLayer use_orig_params True sharding_strategy sharding_strategy backward_prefetch backward_prefetch cpu_offload cpu_offload model = TransformerWithSharedParams init process_group FSDPInitMode NO_FSDP device_init_mode deterministic=True init_optim_before_wrap fsdp_optim = _get_optim model optim_class multi_tensor fsdp_model = FSDP model process_group fsdp_kwargs fsdp_model = FSDP model process_group fsdp_kwargs fsdp_optim = _get_optim fsdp_model optim_class multi_tensor device_init_mode == DEVICEInitMode DEVICE_AFTER fsdp_model cpu_offload offload_params fsdp_model = fsdp_model device=device_type fsdp_model fsdp_optim _check_train_parity ddp_model DDP ddp_optim torch optim Optimizer fsdp_model FSDP fsdp_optim torch optim Optimizer set_to_none bool num_iters int = Checks training parity between DDP FSDP device = torch device device_type i range num_iters iter_losses = model optim ddp_model ddp_optim fsdp_model fsdp_optim module = model module Test two different ` zero_grad ` timings i == optim zero_grad set_to_none=set_to_none pre-forward inp = module get_input device output = model inp loss = module get_loss inp output device iter_losses append loss i == optim zero_grad set_to_none=set_to_none pre-backward module run_backward loss Perform DDP optimizer step CPU match FSDP needed model ddp_model fsdp_model cpu_offload offload_params model torch device cpu optim step model ddp_model fsdp_model cpu_offload offload_params model device torch testing assert_close iter_losses iter_losses iter_losses clear _check_ddp_fsdp_param_parity ddp_model fsdp_model _check_ddp_fsdp_param_parity ddp_model DDP fsdp_model FSDP FSDP summon_full_params fsdp_model n p n p zip ddp_model module named_parameters fsdp_model named_parameters Allow FSDP prefixes assertEqual n clean_tensor_name n torch testing assert_close p p _get_sharding_strategy_from_str sharding_strategy_str str - ShardingStrategy sharding_strategy_str == no_shard sharding_strategy = ShardingStrategy NO_SHARD sharding_strategy_str == shard_grad_op sharding_strategy = ShardingStrategy SHARD_GRAD_OP sharding_strategy_str == full_shard sharding_strategy = ShardingStrategy FULL_SHARD raise ValueError f Invalid string sharding_strategy_str sharding_strategy unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu test_fsdp_compile run_subtests sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP ShardingStrategy NO_SHARD skip_fsdp_guards True False _test_fsdp_compile _test_fsdp_compile sharding_strategy ShardingStrategy skip_fsdp_guards bool torch _dynamo config skip_fsdp_guards = skip_fsdp_guards fsdp_kwargs = auto_wrap_policy ModuleWrapPolicy TransformerEncoderLayer TransformerDecoderLayer use_orig_params True sharding_strategy sharding_strategy backward_prefetch BackwardPrefetch BACKWARD_PRE cpu_offload CPUOffload False base_model = TransformerWithSharedParams init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_BEFORE deterministic=True ref_model = FSDP copy deepcopy base_model process_group fsdp_kwargs ref_optim = torch optim Adam ref_model parameters lr= e- model = FSDP copy deepcopy base_model process_group fsdp_kwargs model = torch compile model optim = torch optim Adam model parameters lr= e- _ range losses = inp = ref_model get_input torch device device_type _model _optim ref_model ref_optim model optim _optim zero_grad loss = _model inp sum losses append loss loss backward _optim step assertEqual losses losses skip_if_lt_x_gpu parametrize sharding_strategy_str no_shard shard_grad_op full_shard test_diff_hyperparams sharding_strategy_str str Tests FSDP parity DDP when using multiple parameter groups different hyperparameter settings sharding_strategy = _get_sharding_strategy_from_str sharding_strategy_str run_subtests device_init_mode DEVICEInitMode DEVICE_BEFORE DEVICEInitMode DEVICE_AFTER init_optim_before_wrap False True optim_class torch optim AdamW multi_tensor False True set_to_none False True backward_prefetch None BackwardPrefetch BACKWARD_PRE BackwardPrefetch BACKWARD_POST skip_writeback_check False True _test_diff_hyperparams cpu_offload=CPUOffload offload_params=False sharding_strategy=sharding_strategy skip_if_lt_x_gpu parametrize sharding_strategy_str no_shard shard_grad_op full_shard test_diff_hyperparams_cpu_offload sharding_strategy_str str Tests FSDP parity DDP when using multiple parameter groups different hyperparameter settings CPU offloading enabled This separate meth ` test_diff_hyperparams ` because CPU offloading has some issues subtesting some specific subtesting configs e g ` ` offload_params=False ` ` followed ` ` True ` ` vice versa sharding_strategy = _get_sharding_strategy_from_str sharding_strategy_str skip_writeback_check False True _test_diff_hyperparams device_init_mode=DEVICEInitMode DEVICE_BEFORE init_optim_before_wrap=False optim_class=torch optim Adam multi_tensor=False set_to_none=False backward_prefetch=BackwardPrefetch BACKWARD_PRE cpu_offload=CPUOffload offload_params=True sharding_strategy=sharding_strategy skip_writeback_check=skip_writeback_check _test_diff_hyperparams device_init_mode DEVICEInitMode init_optim_before_wrap bool optim_class type torch optim Optimizer multi_tensor bool set_to_none bool backward_prefetch Optional BackwardPrefetch cpu_offload CPUOffload sharding_strategy ShardingStrategy skip_writeback_check bool Args init_optim_before_wrap bool If ` ` True ` ` initializes FSDP optimizer before wrapping model FSDP otherwise initializes FSDP optimizer after wrapping model FSDP We permit both forms initialization give users flexibility device_init_mode == DEVICEInitMode DEVICE_AFTER cpu_offload offload_params supported skip_writeback_check os environ _FSDP_SKIP_WRITEBACK_CHECK = ddp_model = _get_ddp_transformer find_unused_params=False ddp_optim = _get_optim ddp_model optim_class multi_tensor fsdp_model fsdp_optim = _get_fsdp_transformer_and_optim device_init_mode=device_init_mode init_optim_before_wrap=init_optim_before_wrap optim_class=optim_class multi_tensor=multi_tensor sharding_strategy=sharding_strategy backward_prefetch=backward_prefetch cpu_offload=cpu_offload _check_train_parity ddp_model ddp_optim fsdp_model fsdp_optim set_to_none skip_if_lt_x_gpu test_diff_trainability Tests FSDP parity DDP when using multiple parameter groups freezing parameters one parameter group run_subtests multi_tensor False True sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP ShardingStrategy NO_SHARD _test_diff_trainability _test_diff_trainability multi_tensor bool sharding_strategy ShardingStrategy optim_class = torch optim Adam ddp_model = _get_ddp_transformer find_unused_params=True ddp_optim = _get_optim ddp_model optim_class multi_tensor fsdp_model fsdp_optim = _get_fsdp_transformer_and_optim device_init_mode=DEVICEInitMode DEVICE_BEFORE init_optim_before_wrap=False optim_class=optim_class multi_tensor=multi_tensor sharding_strategy=sharding_strategy backward_prefetch=BackwardPrefetch BACKWARD_PRE cpu_offload=None Freeze all biases which happen same parameter group param_name param ddp_model named_parameters bias param_name param requires_grad_ False param_name param fsdp_model named_parameters bias param_name param requires_grad_ False _check_train_parity ddp_model ddp_optim fsdp_model fsdp_optim False skip_if_lt_x_gpu test_multiple_optimizers Tests using two optimizers where only one sets gradients ` ` None ` ` run_subtests sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP _test_multiple_optimizers _test_multiple_optimizers sharding_strategy ShardingStrategy ddp_model = _get_ddp_transformer find_unused_params=True ddp_param_groups = _get_param_groups ddp_model assert len ddp_param_groups == f len ddp_param_groups fsdp_model _ = _get_fsdp_transformer_and_optim ignore returned optimizer device_init_mode=DEVICEInitMode DEVICE_BEFORE init_optim_before_wrap=False optim_class=torch optim Adam ignored multi_tensor=False ignored sharding_strategy=sharding_strategy backward_prefetch=BackwardPrefetch BACKWARD_PRE cpu_offload=None fsdp_param_groups = _get_param_groups fsdp_model assert len fsdp_param_groups == f len fsdp_param_groups ddp_optims = fsdp_optims = For transformer model every parameter either weight bias so we only use first two parameter groups Moreover we use Adam AdamW particular since they both use bias correction dependent step which incremented even parameter has zero gradient gradient ` None ` This test we differentiating between zero ` None ` gradient correctly optim_ctors = functools partial torch optim Adam lr= e- functools partial torch optim AdamW lr= e- optim_ctor ddp_param_group fsdp_param_group zip optim_ctors ddp_param_groups fsdp_param_groups ddp_optims append optim_ctor ddp_param_group params fsdp_optims append optim_ctor fsdp_param_group params device = torch device device_type Check there exists ` FlatParameter ` has both weight bias rank s shard has_both = False fsdp_module FSDP fsdp_modules fsdp_model handle = fsdp_module _handle handle continue flat_param = handle flat_param assert flat_param _params None has_weight = False has_bias = False param fqn zip flat_param _params flat_param _fqns weight fqn param numel has_weight = True bias fqn param numel has_bias = True has_both &#124; = has_weight has_bias assert has_both f Rank rank does have ` FlatParameter ` both weight bias its shard meaning test vacuous Run one iteration generate gradients run_iter iter_losses = model optims ddp_model ddp_optims fsdp_model fsdp_optims module = model module inp = module get_input device output = model inp loss = module get_loss inp output device iter_losses append loss module run_backward loss optim optims optim step torch testing assert_close iter_losses iter_losses iter_losses clear _check_ddp_fsdp_param_parity ddp_model fsdp_model run_iter Only set weights gradients None ddp_optims zero_grad set_to_none=True fsdp_optims zero_grad set_to_none=True inp = ddp_model module get_input device ddp_output = ddp_model inp fsdp_output = fsdp_model inp Check FSDP correctly exposes gradients even after forward namely ` None ` weights non- ` None ` biases sharding_strategy NO_RESHARD_AFTER_FORWARD_STRATEGIES Skip check since we do expose gradients after forward these strategies ddp_n ddp_p fsdp_n fsdp_p zip ddp_model module named_parameters fsdp_model named_parameters assertEqual ddp_n clean_tensor_name fsdp_n fsdp_p numel == Not rank s shard assertTrue fsdp_p grad None continue ddp_p grad None assertTrue fsdp_p grad None assertEqual ddp_p flatten fsdp_p flatten assertEqual ddp_p grad flatten fsdp_p grad flatten _check_ddp_fsdp_param_parity ddp_model fsdp_model Finish iteration backward pass optimizer step ddp_loss = ddp_model module get_loss inp ddp_output device fsdp_loss = fsdp_model module get_loss inp fsdp_output device ddp_model module run_backward ddp_loss fsdp_model module run_backward fsdp_loss optim itertools chain ddp_optims fsdp_optims optim step _check_ddp_fsdp_param_parity ddp_model fsdp_model Run one more iteration confirm bias corrections correct run_iter _check_ddp_fsdp_param_parity ddp_model fsdp_model TestFSDPUseOrigParamsUnshardReshard FSDPTest Tests unshard reshard flow property world_size - int _get_fsdp_models_and_optims sharding_strategy ShardingStrategy cpu_offload CPUOffload - tuple FSDP torch optim Optimizer FSDP torch optim Optimizer Returns pair FSDP model optimizer ` ` use_orig_params=False ` ` ` ` True ` ` respectively LR = e- fsdp_kwargs = sharding_strategy sharding_strategy cpu_offload cpu_offload use_orig_params False fsdp_model = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE fsdp_kwargs=fsdp_kwargs deterministic=True optim = torch optim Adam fsdp_model parameters foreach=False lr=LR fsdp_kwargs use_orig_params = True fsdp_model_orig_params = TransformerWithSharedParams init process_group FSDPInitMode RECURSIVE DEVICEInitMode DEVICE_BEFORE fsdp_kwargs=fsdp_kwargs deterministic=True optim_orig_params = torch optim Adam fsdp_model_orig_params parameters foreach=False lr=LR fsdp_model optim fsdp_model_orig_params optim_orig_params _check_fsdp_parameter_parity fsdp FSDP fsdp FSDP - None Checks two FSDP instances have same model parameters FSDP summon_full_params fsdp FSDP summon_full_params fsdp n p n p zip fsdp named_parameters fsdp named_parameters assertEqual n n torch testing assert_close p p _get_fsdp_parity_subtest_config sharding_strategy ShardingStrategy NO_SHARD ShardingStrategy SHARD_GRAD_OP ShardingStrategy FULL_SHARD skip_if_lt_x_gpu parametrize offload_params False True test_multiple_forward offload_params bool Tests ` ` use_orig_params=True ` ` has parity ` ` False ` ` when running multiple forward passes before backward pass cpu_offload = CPUOffload offload_params=offload_params run_subtests _get_fsdp_parity_subtest_config _test_multiple_forward cpu_offload=cpu_offload skip_if_lt_x_gpu _test_multiple_forward sharding_strategy ShardingStrategy cpu_offload CPUOffload fsdp_model optim fsdp_model_orig_params optim_orig_params = _get_fsdp_models_and_optims sharding_strategy cpu_offload device = torch device device_type _ range inp = fsdp_model get_input device _inp = fsdp_model get_input device inp = tuple t + torch ones_like t t _inp make different ` inp ` For these loss lists elem baseline elem test losses = losses = losses = _model _optim fsdp_model optim fsdp_model_orig_params optim_orig_params _optim zero_grad loss = _model inp losses append loss loss = _model inp losses append loss loss = loss + loss sum losses append loss _model run_backward loss _optim step assertEqual losses losses assertEqual losses losses assertEqual losses losses _check_fsdp_parameter_parity fsdp_model fsdp_model_orig_params skip_if_lt_x_gpu parametrize offload_params False True test_summon_between_two_forwards offload_params bool Tests ` ` use_orig_params=True ` ` has parity ` ` False ` ` when running forward pass meth ` summon_full_params ` another forward pass before backward pass cpu_offload = CPUOffload offload_params=offload_params run_subtests _get_fsdp_parity_subtest_config _test_summon_between_two_forwards cpu_offload=cpu_offload _test_summon_between_two_forwards sharding_strategy ShardingStrategy cpu_offload CPUOffload fsdp_model optim fsdp_model_orig_params optim_orig_params = _get_fsdp_models_and_optims sharding_strategy cpu_offload device = torch device device_type _ range optim zero_grad optim_orig_params zero_grad inp = fsdp_model get_input device loss = fsdp_model inp loss_orig_params = fsdp_model_orig_params inp assertEqual loss loss_orig_params Calls into ` summon_full_params ` _check_fsdp_parameter_parity fsdp_model fsdp_model_orig_params inp = fsdp_model get_input device loss = fsdp_model inp loss_orig_params = fsdp_model_orig_params inp assertEqual loss loss_orig_params loss = loss + loss sum loss_orig_params = loss_orig_params + loss_orig_params sum fsdp_model run_backward loss fsdp_model_orig_params run_backward loss_orig_params optim step optim_orig_params step _check_fsdp_parameter_parity fsdp_model fsdp_model_orig_params TestFSDPUseOrigParamsParamAccess FSDPTest Tests original parameter access property world_size Force world size since tests hard code FSDP sharding strategy check sharded parameter parity skip_if_lt_x_gpu test_access_params_after_forward Tests accessing original parameters after forward before backward Notably supported when ` ` use_orig_params=False ` ` However ` ` True ` ` FSDP exposes flattened sharded original parameters making possible run_subtests sharding_strategy ShardingStrategy NO_SHARD ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP _test_access_params_after_forward _test_access_params_after_forward sharding_strategy ShardingStrategy NOTE This test needs changed FSDP sharding algorithm changes It still valuable until such change sanity check ` use_orig_params=True ` implementation Model nn Module __init__ - None super __init__ torch manual_seed = numel - pad - each rank lin = nn Linear bias=False + + = numel - pad - each rank where intra- ` FlatParameter ` alignment padding weight rank weight alignment padding bias rank lin = nn Linear forward x torch Tensor - torch Tensor z = lin x z = nn functional relu z z = lin z z get_input device torch device - tuple torch Tensor torch randn device get_loss inp out out sum check_parameter_parity ddp_model DDP fsdp_model FSDP between_fwd_and_bwd bool assert rank f Expects world size got world_size n p n p zip ddp_model module named_parameters fsdp_model named_parameters assertEqual n clean_tensor_name n sharding_strategy == ShardingStrategy NO_SHARD For ` NO_SHARD ` do nothing since original parameters unflattened pass between_fwd_and_bwd sharding_strategy NO_RESHARD_AFTER_FORWARD_STRATEGIES For no reshard after forward strategies do nothing since FSDP did use sharded views after forward pass Otherwise case parameter see model definition n == lin weight rank == p = p flatten rank == p = p flatten n == lin weight rank == p = p flatten rank == p = p flatten n == lin bias rank == p = torch empty device=p device rank == p = p flatten torch testing assert_close p p ddp_model = DDP Model device=device_type device_ids= rank fsdp_model = FSDP Model device=device_type sharding_strategy=sharding_strategy auto_wrap_policy=always_wrap_policy use_orig_params=True LR = e- ddp_optim = torch optim Adam ddp_model parameters lr=LR fsdp_optim = torch optim Adam fsdp_model parameters lr=LR device = torch device device_type inp = fsdp_model get_input device ddp_out = ddp_model inp fsdp_out = fsdp_model inp check_parameter_parity ddp_model fsdp_model True ddp_loss = ddp_model module get_loss inp ddp_out fsdp_loss = fsdp_model get_loss inp fsdp_out ddp_loss backward fsdp_loss backward ddp_optim step fsdp_optim step check_parameter_parity ddp_model fsdp_model False inp = fsdp_model get_input device ddp_out = ddp_model inp fsdp_out = fsdp_model inp check_parameter_parity ddp_model fsdp_model True TestFSDPUseOrigParamsWriteback FSDPTest Tests parameter gradient writeback Model nn Module __init__ device torch device super __init__ torch manual_seed lin = nn Linear bias=True device=device lin = nn Linear bias=True device=device forward x torch Tensor - torch Tensor z = lin x z = nn functional relu z z = lin z z get_input device torch device - tuple torch Tensor torch randn device get_loss inp out out sum property world_size Force world size since tests hard code FSDP sharding strategy _check_param_parity ddp_model DDP fsdp_model FSDP FSDP summon_full_params fsdp_model n p n p zip ddp_model module named_parameters fsdp_model named_parameters assertEqual n n torch testing assert_close p p skip_if_lt_x_gpu test_param_writeback Tests changes original parameters written back run_subtests change_first_weight True False first vs second ` weight ` change_data True False change ` data ` vs variable itself _test_param_writeback _test_param_writeback change_first_weight bool change_data bool transform_param param nn Parameter - nn Parameter nn Parameter torch ones_like param Check writeback propagates ddp_model = DDP TestFSDPUseOrigParamsWriteback Model torch device device_type device_ids= rank fsdp_model = FSDP TestFSDPUseOrigParamsWriteback Model torch device device_type use_orig_params=True ddp = ddp_model module brevity fsdp = fsdp_model module change_first_weight change_data ddp lin weight data = transform_param ddp lin weight fsdp lin weight data = transform_param fsdp lin weight ddp lin weight = transform_param ddp lin weight fsdp lin weight = transform_param fsdp lin weight change_data ddp lin weight data = transform_param ddp lin weight fsdp lin weight data = transform_param fsdp lin weight ddp lin weight = transform_param ddp lin weight fsdp lin weight = transform_param fsdp lin weight _check_param_parity ddp_model fsdp_model triggers writeback skip_if_lt_x_gpu test_grad_writeback Tests changes original parameters gradients written back run_subtests change_first_weight_grad False True change_data False True change ` data ` vs variable itself set_to_none False True _test_grad_writeback _test_grad_writeback change_first_weight_grad bool change_data bool set_to_none bool change_data set_to_none well-defined transform_grad param nn Parameter - nn Parameter None set_to_none torch ones_like param ddp_model = DDP TestFSDPUseOrigParamsWriteback Model torch device device_type device_ids= rank fsdp_model = FSDP TestFSDPUseOrigParamsWriteback Model torch device device_type use_orig_params=True LR = e- TODO If we add ` summon_full_params with_grads=True ` then replace following For now we use optimizer step surrogate checking gradients written back ddp_optim = torch optim Adam ddp_model parameters lr=LR fsdp_optim = torch optim Adam fsdp_model parameters lr=LR Generate initial gradient inp = fsdp_model get_input torch device device_type ddp_out = ddp_model inp fsdp_out = fsdp_model inp ddp_out sum backward fsdp_out sum backward Change gradient through original parameters ddp = ddp_model module brevity fsdp = fsdp_model module change_first_weight_grad change_data ddp lin weight grad data = transform_grad ddp lin weight fsdp lin weight grad None fsdp lin weight grad data = transform_grad fsdp lin weight ddp lin weight grad = transform_grad ddp lin weight fsdp lin weight grad = transform_grad fsdp lin weight change_data ddp lin weight grad data = transform_grad ddp lin weight fsdp lin weight grad None fsdp lin weight grad data = transform_grad fsdp lin weight ddp lin weight grad = transform_grad ddp lin weight fsdp lin weight grad = transform_grad fsdp lin weight ddp_optim step fsdp_optim step _check_param_parity ddp_model fsdp_model triggers writeback Intentionally do zero gradient check writeback inp = fsdp_model get_input torch device device_type ddp_out = ddp_model inp fsdp_out = fsdp_model inp ddp_out sum backward fsdp_out sum backward ddp_optim step fsdp_optim step _check_param_parity ddp_model fsdp_model triggers writeback skip_if_lt_x_gpu test_writeback_shape_mismatch fsdp_model = FSDP TestFSDPUseOrigParamsWriteback Model torch device device_type use_orig_params=True Check writing back mismatched shape errors fsdp = fsdp_model module brevity assert rank f Expects world size got world_size assertRaisesRegex RuntimeError Cannot writeback Change gradient new one added each dimension force shape mismatch when writing back rank == Change ` lin weight grad ` since exists rank lin _weight_shape = list fsdp lin weight shape dim_index range len lin _weight_shape lin _weight_shape dim_index += fsdp lin weight = nn Parameter torch randn torch Size lin _weight_shape device=fsdp lin weight device fsdp lin weight grad = torch randn torch Size lin _weight_shape device=fsdp lin weight device rank == Change ` lin weight grad ` since exists partially rank lin _weight_shape = list fsdp lin weight shape dim_index range len lin _weight_shape lin _weight_shape dim_index += fsdp lin weight = nn Parameter torch randn torch Size lin _weight_shape device=fsdp lin weight device fsdp lin weight grad = torch randn torch Size lin _weight_shape device=fsdp lin weight device FSDP summon_full_params fsdp_model triggers writeback skip_if_lt_x_gpu test_writeback_between_fwd_and_bwd_for_no_reshard_raises fsdp_kwargs = sharding_strategy ShardingStrategy SHARD_GRAD_OP auto_wrap_policy ModuleWrapPolicy nn Linear use_orig_params True fsdp_wrapper = functools partial FSDP fsdp_kwargs Test changing parameter storage no longer view into flat parameter fsdp_model = fsdp_wrapper TestFSDPUseOrigParamsWriteback Model torch device device_type inp = fsdp_model get_input torch device device_type loss = fsdp_model inp sum fsdp_model lin weight data = fsdp_model lin weight clone assert_msg = FSDP does support changing parameters between forward backward assertRaisesRegex AssertionError assert_msg loss backward Test changing parameter variable itself fsdp_model = fsdp_wrapper TestFSDPUseOrigParamsWriteback Model torch device device_type inp = fsdp_model get_input torch device device_type loss = fsdp_model inp sum fsdp_model lin _fsdp_wrapped_module weight = nn Parameter fsdp_model lin weight clone assertRaisesRegex AssertionError assert_msg loss backward skip_if_lt_x_gpu test_no_reshard_and_mixed_precision Tests writeback does falsely get triggered few configurations exercising sharded view skipping logic - Train forward - full-precision unshard - train forward - Train forward - eval forward - Train forward backward - eval forward - model checkpoint run_subtests use_full_prec_in_eval False True _test_no_reshard_and_mixed_precision _test_no_reshard_and_mixed_precision use_full_prec_in_eval bool use_full_prec_in_eval os environ _FSDP_USE_FULL_PREC_IN_EVAL = fsdp_kwargs = sharding_strategy ShardingStrategy SHARD_GRAD_OP auto_wrap_policy ModuleWrapPolicy nn Linear mixed_precision MixedPrecision param_dtype=torch float use_orig_params True Train forward - full-precision unshard - train forward fsdp_model = FSDP TestFSDPUseOrigParamsWriteback Model torch device device_type fsdp_kwargs inp = fsdp_model get_input torch device device_type fsdp_model inp FSDP summon_full_params fsdp_model fsdp_model inp sum Train forward - eval forward fsdp_model train fsdp_model inp fsdp_model eval fsdp_model inp Train forward backward - eval forward - model checkpoint fsdp_model train fsdp_model inp sum backward fsdp_model eval fsdp_model inp FSDP state_dict_type fsdp_model StateDictType SHARDED_STATE_DICT sd = fsdp_model state_dict fsdp_model load_state_dict sd fsdp_model inp sum backward TestFSDPUseOrigParamsFQNs FSDPTest skip_if_lt_x_gpu test_named_parameters_in_forward Tests calling ` ` named_parameters ` ` during forward returns FQNs ` ` Tensor ` ` s corresponding original parameters param_shapes = None None assert_equal_fn = assertEqual Model nn Module __init__ - None super __init__ lin = nn Linear forward x torch Tensor - torch Tensor nonlocal param_shapes Allow FSDP prefixes param_names = clean_tensor_name tup tup named_parameters params = tup tup named_parameters assert param_shapes None param_shapes None ` param_sizes ` should set assert_equal_fn param_names lin weight lin bias assert_equal_fn params shape param_shapes assert_equal_fn params shape param_shapes lin x model = Model device=device_type Save unsharded original parameter shapes check shapes match forward pass param_shapes = model lin weight shape param_shapes = model lin bias shape fsdp_model = FSDP model use_orig_params=True inp = torch randn device=torch device device_type fsdp_model inp TestFSDPUseOrigParamsNoSync FSDPTest property world_size - int skip_if_lt_x_gpu test_no_sync_correctness Tests basic ` ` no_sync ` ` setup comparing ` ` use_orig_params=True ` ` against ` ` use_orig_params=False ` ` run_subtests sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP ShardingStrategy NO_SHARD _test_no_sync_correctness _test_no_sync_correctness sharding_strategy ShardingStrategy model = nn Linear bias=False device=device_type fsdp_kwargs = sharding_strategy sharding_strategy model_use_flat_params = FSDP copy deepcopy model use_orig_params=False fsdp_kwargs model_use_orig_params = FSDP model use_orig_params=True fsdp_kwargs optim_use_flat_params = torch optim AdamW model_use_flat_params parameters foreach=True optim_use_orig_params = torch optim AdamW model_use_orig_params parameters foreach=True _check_param_grad_parity _baseline_model nn Module _test_model nn Module This assumes model ` ` nn Linear bias=False ` ` i e single D weight parameter able directly compare baseline test models On rank baseline includes element padding assertEqual len list _baseline_model parameters assertEqual len list _test_model parameters flat_param orig_param zip _baseline_model parameters _test_model parameters Baseline permitted have padding assertGreaterEqual flat_param numel orig_param numel unpadded_param_numel = orig_param numel For ` NO_SHARD ` ` use_orig_params=True ` presents unflattened parameters while ` False ` presents flattened ones torch testing assert_close flat_param unpadded_param_numel orig_param flatten Gradient numel different right after ` no_sync ` since gradient unsharded while parameter sharded unpadded_grad_numel = orig_param grad numel For ` use_orig_params=False ` unsharded gradient flattened while ` True ` unflattened torch testing assert_close flat_param grad unpadded_grad_numel reshape orig_param grad shape orig_param grad inp = torch randn device=device_type grad = torch randn device=device_type Compute some reference gradients using one forward backward out_use_flat_params = model_use_flat_params inp out_use_orig_params = model_use_orig_params inp torch testing assert_close out_use_flat_params out_use_orig_params out_use_flat_params backward grad out_use_orig_params backward grad _check_param_grad_parity model_use_flat_params model_use_orig_params ref_grads_use_flat_params = param grad detach clone param model_use_flat_params parameters ref_grads_use_orig_params = param grad detach clone param model_use_orig_params parameters param grad None Run forward backward ` no_sync ` optim_use_flat_params zero_grad set_to_none=True optim_use_orig_params zero_grad set_to_none=True model model_use_flat_params model_use_orig_params model no_sync out = model inp out backward grad _check_param_grad_parity model_use_flat_params model_use_orig_params Run forward backward outside ` no_sync ` model model_use_flat_params model_use_orig_params out = model inp out backward grad _check_param_grad_parity model_use_flat_params model_use_orig_params Check since we accumulated gradients across iterations new gradients x reference gradients grads_use_flat_params = param grad detach clone param model_use_flat_params parameters grads_use_orig_params = param grad detach clone param model_use_orig_params parameters param grad None grad ref_grad zip grads_use_flat_params ref_grads_use_flat_params torch testing assert_close grad ref_grad grad ref_grad zip grads_use_orig_params ref_grads_use_orig_params torch testing assert_close grad ref_grad skip_if_lt_x_gpu test_no_sync_mixed_precision Tests dtypes expected when using ` ` no_sync ` ` ` ` use_orig_params=True ` ` parameter mixed precision run_subtests sharding_strategy ShardingStrategy FULL_SHARD ShardingStrategy SHARD_GRAD_OP ShardingStrategy NO_SHARD _test_no_sync_mixed_precision _test_no_sync_mixed_precision sharding_strategy ShardingStrategy model = nn Linear device=device_type mixed_precision = MixedPrecision param_dtype=torch float reduce_dtype=torch float fsdp_kwargs = sharding_strategy sharding_strategy mixed_precision mixed_precision use_orig_params True fsdp_model = FSDP model fsdp_kwargs inp = torch randn device=device_type fsdp_model no_sync For each these ` no_sync ` backward passes check gradients low precision parameter dtype FP fsdp_model inp sum backward param fsdp_model parameters param grad None assertEqual param grad dtype torch float fsdp_model inp sum backward param fsdp_model parameters param grad None assertEqual param grad dtype torch float For backward pass outside ` no_sync ` check gradients cast full precision preparation optimizer step fsdp_model inp sum backward param fsdp_model parameters param grad None assertEqual param grad dtype torch float TestFSDPUseOrigParamsInit FSDPTest skip_if_lt_x_gpu test_non_uniform_requires_grad model = nn Sequential nn Linear device=device_type nn Linear device=device_type Freeze biases only flatten both weights biases into same ` FlatParameter ` exercise non-uniform ` requires_grad ` model bias requires_grad = False model bias requires_grad = False fsdp_model = FSDP model use_orig_params=True assertTrue fsdp_model weight requires_grad assertFalse fsdp_model bias requires_grad assertTrue fsdp_model weight requires_grad assertFalse fsdp_model bias requires_grad Define large enough trigger stack corruption NUM_SIZE _TENSORS = TestMultiTensorApply TestCase test_multi_tensor_apply_size _tensors_cpu size _tensors = torch empty device= cpu _ range NUM_SIZE _TENSORS Check does segfault torch _foreach_mul_ size _tensors unittest skipIf TEST_CUDA TEST_XPU no cuda no xpu test_multi_tensor_apply_size _tensors_cuda size _tensors = torch empty device=device_type _ range NUM_SIZE _TENSORS Check does segfault torch _foreach_mul_ size _tensors instantiate_parametrized_tests TestFSDPUseOrigParamsMultipleParamGroups instantiate_parametrized_tests TestFSDPUseOrigParamsUnshardReshard instantiate_parametrized_tests TestFSDPUseOrigParamsParamAccess instantiate_parametrized_tests TestFSDPUseOrigParamsFQNs instantiate_parametrized_tests TestFSDPUseOrigParamsNoSync __name__ == __main__ run_tests