========================== Per-sample-gradients ========================== What -------------------------------------------------------------------- Per-sample-gradient computation computing gradient each every sample batch data It useful quantity differential privacy optimization research torch torch nn nn torch nn functional F torch manual_seed Here s simple CNN SimpleCNN nn Module __init__ super __init__ conv = nn Conv d conv = nn Conv d fc = nn Linear fc = nn Linear forward x x = conv x x = F relu x x = conv x x = F relu x x = F max_pool d x x = torch flatten x x = fc x x = F relu x x = fc x output = F log_softmax x dim= output = x output loss_fn predictions targets F nll_loss predictions targets Let s generate batch dummy data Pretend we re working MNIST dataset where images we have minibatch size device = cuda num_models = batch_size = data = torch randn batch_size device=device targets = torch randint device=device In regular model training one would forward batch examples then call backward compute gradients model = SimpleCNN device=device predictions = model data loss = loss_fn predictions targets loss backward Conceptually per-sample-gradient computation equivalent each sample data perform forward backward pass get gradient compute_grad sample target sample = sample unsqueeze target = target unsqueeze prediction = model sample loss = loss_fn prediction target torch autograd grad loss list model parameters compute_sample_grads data targets sample_grads = compute_grad data i targets i i range batch_size sample_grads = zip sample_grads sample_grads = torch stack shards shards sample_grads sample_grads per_sample_grads = compute_sample_grads data targets sample_grads per-sample-grad model conv weight model conv weight shape notice how there one gradient per sample batch total print per_sample_grads shape ###################################################################### Per-sample-grads using functorch -------------------------------------------------------------------- We can compute per-sample-gradients efficiently using function transforms First let s create stateless functional version ` ` model ` ` using ` ` functorch make_functional_with_buffers ` ` functorch grad make_functional_with_buffers vmap fmodel params buffers = make_functional_with_buffers model Next let s define function compute loss model given single input rather than batch inputs It important function accepts parameters input target because we will transforming over them Because model originally written handle batches we ll use ` ` torch unsqueeze ` ` add batch dimension compute_loss params buffers sample target batch = sample unsqueeze targets = target unsqueeze predictions = fmodel params buffers batch loss = loss_fn predictions targets loss Now let s use ` ` grad ` ` create new function computes gradient respect first argument compute_loss i e params ft_compute_grad = grad compute_loss ` ` ft_compute_grad ` ` computes gradient single sample target pair We can use ` ` vmap ` ` get compute gradient over entire batch samples targets Note in_dims= None None because we wish map ` ` ft_compute_grad ` ` over th dimension data targets use same params buffers each ft_compute_sample_grad = vmap ft_compute_grad in_dims= None None Finally let s used our transformed function compute per-sample-gradients ft_per_sample_grads = ft_compute_sample_grad params buffers data targets per_sample_grad ft_per_sample_grad zip per_sample_grads ft_per_sample_grads assert torch allclose per_sample_grad ft_per_sample_grad atol= e- rtol= e- A quick note there limitations around what types functions can transformed vmap The best functions transform ones pure functions function where outputs only determined inputs have no side effects e g mutation vmap unable handle mutation arbitrary Python data structures able handle many in-place PyTorch operations