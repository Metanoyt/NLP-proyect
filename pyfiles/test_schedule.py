Copyright c Meta Platforms Inc affiliates Owner s oncall distributed copy csv logging os model_registry MultiMLP torch torch _dynamo OptimizedModule torch distributed pipelining Schedule F B ScheduleDualPipeV ScheduleGPipe ScheduleInterleaved F B ScheduleInterleavedZeroBubble ScheduleLoopedBFS ScheduleZBVZeroBubble torch distributed pipelining _utils generate_stage_to_rank_mapping torch distributed pipelining schedules _Action _add_send_recv _add_unshard_reshard _format_pipeline_order _merge_bw _PipelineSchedule _PipelineScheduleRuntime _simulate_comms_compute _validate_schedule B F get_schedule_class I PipelineScheduleSingle RECV_F RESHARD SEND_B UNSHARD W torch distributed pipelining stage _PipelineStageBase PipelineStage torch testing _internal common_distributed requires_accelerator_dist_backend torch testing _internal common_utils check_leaked_tensors instantiate_parametrized_tests parametrize run_tests TestCase torch testing _internal distributed fake_pg FakeStore ARTIFACTS_DIR = os path join os path dirname os path abspath __file__ artifacts device = acc type acc = torch accelerator current_accelerator cpu logger = logging getLogger __name__ torch manual_seed MockPipelineStage _PipelineStageBase __init__ args kwargs Mock necessary attributes submod = None num_stages = kwargs get num_stages group_size = kwargs get group_size group_rank = kwargs get group_rank group = kwargs get group _create_grad_recv_info args kwargs None _prepare_forward_infra n_microbatches pass _prepare_backward_infra n_microbatches pass ScheduleTest TestCase test_get_schedule_class List all expected schedule names schedule_names = F B f b Interleaved F B INTERLEAVED F B GPipe LoopedBFS PipelineScheduleSingle PipelineScheduleMulti Test each schedule name name schedule_names subTest name=name schedule_class = get_schedule_class name assertIsNotNone schedule_class f Class name should None assertTrue issubclass schedule_class _PipelineSchedule f name should subclass _PipelineSchedule error_case = ScheduleThatDoesNotExist name error_case Test original name included error message assertRaisesRegex ValueError f name get_schedule_class name parametrize ScheduleClass Schedule F B ScheduleGPipe ScheduleInterleaved F B ScheduleInterleavedZeroBubble ScheduleLoopedBFS test_schedule_with_single_stage ScheduleClass Test schedules only single stage work expected all schedules store = FakeStore torch distributed init_process_group backend= fake rank= world_size= store=store d_hid batch_size = n_stages = device = cpu full_mod = MultiMLP d_hid n_layers=n_stages full_mod device x = torch randn batch_size d_hid device=device ref_mod = copy deepcopy full_mod torch no_grad y = ref_mod x Add small perturbation target = y + torch randn batch_size d_hid device=device loss_fn y target torch nn functional cross_entropy y target Run reference _ range ref_mod zero_grad ref_out = ref_mod x ref_loss = loss_fn ref_out target ref_loss backward submod_name = layers stage_module = full_mod get_submodule submod_name Create pipeline stage wrap submodule num_microbatches = stages = PipelineStage stage_module n_stages device issubclass ScheduleClass PipelineScheduleSingle stages = stages Attach schedule schedule = ScheduleClass stages num_microbatches loss_fn=loss_fn Run _ range Zero gradients stage_module zero_grad losses = out = schedule step x target=target losses=losses Check output torch testing assert_close out ref_out Check loss Since reduction used loss function above mean we use mean here reduce microbatch losses into single value too pipe_loss = torch stack losses mean torch testing assert_close pipe_loss ref_loss Check gradients Get corresponding submodule reference model ref_submod = ref_mod get_submodule submod_name Check gradients per parameter name p stage_module named_parameters ref_p = ref_submod get_parameter name try torch testing assert_close p grad ref_p grad rtol= e- atol= e- except AssertionError print f Gradient test failed name p grad vs ref_p grad raise torch distributed destroy_process_group parametrize ScheduleClass Schedule F B ScheduleGPipe ScheduleInterleaved F B ScheduleInterleavedZeroBubble ScheduleLoopedBFS test_schedule_eval_then_train ScheduleClass Test simply runs evaluation followed training store = FakeStore torch distributed init_process_group backend= fake rank= world_size= store=store d_hid batch_size = n_stages = device = cpu full_mod = MultiMLP d_hid n_layers=n_stages full_mod device x = torch randn batch_size d_hid device=device target = torch randn batch_size d_hid device=device loss_fn y target torch nn functional cross_entropy y target submod_name = layers stage_module = full_mod get_submodule submod_name Create pipeline stage wrap submodule num_microbatches = stages = PipelineStage stage_module n_stages device issubclass ScheduleClass PipelineScheduleSingle stages = stages Attach schedule schedule = ScheduleClass stages num_microbatches loss_fn=loss_fn Run eval _ range Zero gradients stage_module zero_grad losses = schedule eval x target=target losses=losses Run training try _ range losses = schedule step x target=target losses=losses finally torch distributed destroy_process_group parametrize ScheduleClass ScheduleInterleavedZeroBubble ScheduleZBVZeroBubble ScheduleDualPipeV test_zero_bubble_schedule_errors_with_compile ScheduleClass Test zero bubble schedules raise error when used torch compile store = FakeStore torch distributed init_process_group backend= fake rank= world_size= store=store n_stages = device = torch device cpu model = MultiMLP n_layers=n_stages full_mod compiled_model = torch compile model assertTrue isinstance compiled_model OptimizedModule stage = PipelineStage compiled_model n_stages device try assertRaises RuntimeError ScheduleClass stage finally torch distributed destroy_process_group instantiate_parametrized_tests ScheduleTest TestSchedulePlan TestCase setUp Define list test cases varying num_local_stages num_microbatches group_size These should succeed since num_microbatches group_size == test_cases = small number stages large microbatches large groups odd num pipeline stages odd group_sizes n_mb non divisible group_size parametrize ScheduleClass ScheduleInterleaved F B ScheduleLoopedBFS test_pipeline_order ScheduleClass num_local_stages num_microbatches group_size test_cases subTest num_local_stages=num_local_stages num_microbatches=num_microbatches group_size=group_size num_microbatches group_size = continue logger info num_local_stages= d num_microbatches= d group_size= d num_local_stages num_microbatches group_size num_stages = num_local_stages group_size stages = MockPipelineStage group_size=group_size num_stages=num_stages i range num_local_stages schedule = ScheduleClass stages num_microbatches _formatted_pipeline_order = _format_pipeline_order schedule pipeline_order stage_to_rank stage stage group_size comms_sch = _add_send_recv schedule pipeline_order stage_to_rank=stage_to_rank num_stages=num_stages _simulate_comms_compute comms_sch stage_to_rank=stage_to_rank num_stages=num_stages parametrize ScheduleClass ScheduleInterleaved F B ScheduleInterleavedZeroBubble test_pipeline_order_flex_and_zero_bubble ScheduleClass num_local_stages num_microbatches group_size test_cases subTest num_local_stages=num_local_stages num_microbatches=num_microbatches group_size=group_size warmups_ops_last_stage = num_local_stages - num_microbatches max num_microbatches group_size warmup_ops = warmups_ops_last_stage + group_size - warmup_ops = min warmup_ops num_microbatches num_local_stages num_stages = num_local_stages group_size stages = MockPipelineStage group_size=group_size num_stages=num_stages i range num_local_stages schedule = ScheduleClass stages num_microbatches _format_pipeline_order schedule pipeline_order stage_to_rank stage stage group_size comms_sch = _add_send_recv schedule pipeline_order stage_to_rank=stage_to_rank num_stages=num_stages print _format_pipeline_order comms_sch _simulate_comms_compute comms_sch stage_to_rank=stage_to_rank num_stages=num_stages parametrize ScheduleClass ScheduleDualPipeV ScheduleZBVZeroBubble test_pipeline_order_for_v_schedules ScheduleClass num_local_stages num_microbatches group_size test_cases subTest num_local_stages=num_local_stages num_microbatches=num_microbatches group_size=group_size num_stages = num_local_stages group_size stages = MockPipelineStage group_size=group_size num_stages=num_stages i range num_local_stages V schedules only support stages per rank so num_local_stages ensure error thrown num_local_stages = assertRaises ValueError ScheduleClass stages num_microbatches continue DualPipeV requires num_microbatches = num_stages ScheduleClass == ScheduleDualPipeV num_microbatches num_stages assertRaises ValueError ScheduleClass stages num_microbatches continue Create schedule validate schedule = ScheduleClass stages num_microbatches _validate_schedule schedule pipeline_order group_size num_stages num_microbatches instantiate_parametrized_tests TestSchedulePlan TestScheduleCsv TestCase parametrize ScheduleClass csv_name ScheduleDualPipeV dualpipev_ rank_ mb test_csv_compare ScheduleClass csv_name Test schedules matches expected CSV This regression test ensure schedule changed unintentionally num_local_stages = group_size = num_stages = num_local_stages group_size stages = MockPipelineStage group_size=group_size num_stages=num_stages _ range num_local_stages num_microbatches = schedule = ScheduleClass stages num_microbatches comms_csv = os path join ARTIFACTS_DIR f csv_name csv sch = schedule pipeline_order Uncomment regenerate reference output schedule _dump_csv test csv compute_only sch_ref = open comms_csv newline= ref rank row enumerate csv reader ref sch_ref rank = _Action from_str s s row rank sch_ref timestep b enumerate zip sch rank sch_ref rank assertEqual b f Mismatch timestep= a= expected b instantiate_parametrized_tests TestScheduleCsv TestScheduleLowering TestCase Tests lowering passes convert simple compute-only FBW schedules into compute+comms schedules _parse_actions actions list str - list _Action _Action from_str s s actions parametrize action_str_and_ref F _Action F I _Action I W _Action W B _Action B UNSHARD _Action UNSHARD None RESHARD _Action RESHARD None SEND_B _Action SEND_B RECV_F _Action RECV_F test_action_parse action_str_and_ref Test actions can parsed strings round-tripped back same strings act_str ref = action_str_and_ref act = _Action from_str act_str assertEqual act ref assertEqual act_str act __repr__ parametrize test_info compute F F B B comms UNSHARD F F B B RESHARD compute F F F F B B B B comms UNSHARD UNSHARD F F F F B B RESHARD B B RESHARD test_unshard_reshard test_info Test lowering pass takes compute only schedule only F B W ops adds FSDP unshard reshard operations schedule This just part process adding communication ops producing complete schedule compute_sch = _parse_actions test_info compute expected_comms_sch = _parse_actions test_info comms comms_sch = _add_unshard_reshard compute_sch expected actual zip expected_comms_sch comms_sch assertEqual expected actual f Mismatch expected action expected found actual f \nWhole Schedule comms_sch parametrize test_info compute F F F I I W I W W comms F F F I I W B W test_merge_bw test_info Test pass merges adjacent I W operations into B operation compute_sch = _parse_actions test_info compute expected_merged_sch = _parse_actions test_info comms merged_sch = _merge_bw compute_sch expected actual zip expected_merged_sch merged_sch assertEqual expected actual f Mismatch expected action expected found actual f \nWhole Schedule merged_sch parametrize test_info schedule simple_ _rank_ _stage compute F F B B F B F B comms F SEND_F F SEND_F RECV_B B RECV_B B RECV_F RECV_F F B SEND_B F B SEND_B stage_to_rank lambda stage_idx stage_idx num_stages simulated_steps schedule v_ _rank_ _stage compute F F F B F B B W B W W W F F F F B B B B W W W W comms F SEND_F F SEND_F RECV_F F B SEND_B RECV_F F B SEND_B RECV_B B W RECV_B B W W W RECV_F interesting gets scheduled up front expected RECV_F F F SEND_F F ditto RECV_B F SEND_F B ditto RECV_B B SEND_B B B SEND_B W W W W stage_to_rank lambda stage_idx stage_idx num_stages simulated_steps test_send_recv test_info Tests lowering pass adds send recv ops compute-only schedule compute_sch = rank _parse_actions test_info compute rank rank test_info compute expected_comms_sch = rank _parse_actions test_info comms rank rank test_info comms comms_sch = _add_send_recv compute_sch test_info stage_to_rank test_info num_stages rank expected_comms_sch i expected actual enumerate zip expected_comms_sch rank comms_sch rank assertEqual expected actual f Mismatch rank rank position i f \nExpected expected_comms_sch rank f \nActual comms_sch rank assertEqual len comms_sch rank len expected_comms_sch rank simulated_schedule = _simulate_comms_compute comms_sch stage_to_rank=test_info stage_to_rank num_stages=test_info num_stages _dump_chrometrace simulated_schedule lowered_comms json print _format_pipeline_order simulated_schedule num_steps = max len simulated_schedule rank rank simulated_schedule assertEqual num_steps test_info simulated_steps parametrize csv_name zb p_ rank_ stagep test_csv csv_name _dump_csv pipeline_order_with_comms filename str Dump CSV representation compute + comms schedule into file provided filename open filename w newline= csvfile writer = csv writer csvfile rank pipeline_order_with_comms writer writerow pipeline_order_with_comms rank compute_sch = open os path join ARTIFACTS_DIR f csv_name _compute csv newline= csvfile rank row enumerate csv reader csvfile compute_sch rank = _Action from_str s s row print _format_pipeline_order compute_sch num_model_chunks = pipeline_parallel_size = num_stages = num_model_chunks pipeline_parallel_size rank compute_sch compute_sch rank = _merge_bw compute_sch rank comms_sch = _add_send_recv compute_sch stage_to_rank=lambda chunk_index chunk_index pipeline_parallel_size num_stages=num_stages comms_csv = os path join ARTIFACTS_DIR f csv_name _comms csv Uncomment regenerate reference output _dump_csv comms_sch comms_csv sch_ref = open comms_csv newline= ref rank row enumerate csv reader ref sch_ref rank = _Action from_str s s row rank sch_ref timestep b enumerate zip comms_sch rank sch_ref rank assertEqual b f Mismatch timestep= a= expected b simulated_schedule = _simulate_comms_compute comms_sch stage_to_rank=lambda s s pipeline_parallel_size num_stages=num_stages num_steps = max len simulated_schedule rank rank simulated_schedule print _format_pipeline_order simulated_schedule assertEqual num_steps requires_accelerator_dist_backend nccl xccl test_grad_with_v_schedule We have special case V schedules where adjacent stages same rank E g rank stage stage rank stage stage The special case involves using send recv ops directly passing tensors between colocated stages This test runs single rank just tests stage stage portion both F B comparing gradients reference model layers store = FakeStore torch distributed init_process_group backend= fake rank= world_size= store=store d_hid = batch_size = n_stages = full_mod = MultiMLP d_hid n_layers=n_stages full_mod device ref_mod = copy deepcopy full_mod x = torch randn batch_size d_hid device=device torch no_grad y = ref_mod x Add small perturbation target = y + torch randn batch_size d_hid device=device loss_fn = torch nn MSELoss reduction= sum Run reference _ range ref_mod zero_grad ref_out = ref_mod x ref_loss = loss_fn ref_out target ref_loss backward stage_indices = submod_names = f layers i i stage_indices stage_modules = full_mod get_submodule submod_name submod_name submod_names Create pipeline stage wrap submodule num_microbatches = stages = PipelineStage stage_module stage_idx n_stages device stage_module stage_idx zip stage_modules stage_indices Attach schedule schedule = _PipelineScheduleRuntime stages num_microbatches loss_fn=loss_fn scale_grads=False schedule _prepare_schedule_with_comms _parse_actions F F F F B B B B format= compute_comms Run check_leaked_tensors garbage_tensors _ range Zero gradients stage_module stage_modules stage_module zero_grad losses = out = schedule step x target=target losses=losses assertEqual len garbage_tensors Found leaked tensors check logs above debug info Check output torch testing assert_close out ref_out Check loss Since reduction used loss function above sum we use sum here reduce microbatch losses into single value too pipe_loss = sum losses torch testing assert_close pipe_loss ref_loss Check gradients stage_module submod_name zip stage_modules submod_names Get corresponding submodule reference model ref_submod = ref_mod get_submodule submod_name Check gradients per parameter name p stage_module named_parameters ref_p = ref_submod get_parameter name try torch testing assert_close p grad ref_p grad rtol= e- atol= e- except AssertionError print f Gradient test failed name p grad vs ref_p grad raise torch distributed destroy_process_group requires_accelerator_dist_backend nccl xccl test_grad_with_split_b_w Ensure separate dInput dWeight computations correctly executed This test runs single rank just tests single stage microbatches separate B W operations store = FakeStore torch distributed init_process_group backend= fake rank= world_size= store=store d_hid = batch_size = n_stages = full_mod = MultiMLP d_hid n_layers=n_stages full_mod device ref_mod = copy deepcopy full_mod x = torch randn batch_size d_hid device=device torch no_grad y = ref_mod x Add small perturbation target = y + torch randn batch_size d_hid device=device loss_fn = torch nn MSELoss reduction= sum Run reference _ range ref_mod zero_grad ref_out = ref_mod x ref_loss = loss_fn ref_out target ref_loss backward stage_indices = submod_names = f layers i i stage_indices stage_modules = full_mod get_submodule submod_name submod_name submod_names Create pipeline stage wrap submodule num_microbatches = stages = PipelineStage stage_module stage_idx n_stages device stage_module stage_idx zip stage_modules stage_indices Attach schedule schedule = _PipelineScheduleRuntime stages num_microbatches loss_fn=loss_fn scale_grads=False schedule _prepare_schedule_with_comms _parse_actions F F I I W W format= compute_comms Run check_leaked_tensors garbage_tensors _ range Zero gradients stage_module stage_modules stage_module zero_grad losses = out = schedule step x target=target losses=losses assertEqual len garbage_tensors Found leaked tensors check logs above debug info Check output torch testing assert_close out ref_out Check loss Since reduction used loss function above sum we use sum here reduce microbatch losses into single value too pipe_loss = sum losses torch testing assert_close pipe_loss ref_loss Check gradients stage_module submod_name zip stage_modules submod_names Get corresponding submodule reference model ref_submod = ref_mod get_submodule submod_name Check gradients per parameter name p stage_module named_parameters ref_p = ref_submod get_parameter name try torch testing assert_close p grad ref_p grad rtol= e- atol= e- except AssertionError print f Gradient test failed name p grad vs ref_p grad raise torch distributed destroy_process_group TestValidateSchedule TestCase test_valid_schedule schedule_actions = _Action F _Action B _Action F _Action B _Action F _Action I _Action W _Action F _Action I _Action W pp_group_size = num_stages = num_microbatches = actions schedule_actions _validate_schedule actions pp_group_size num_stages num_microbatches test_invalid_schedule_missing_rank actions = _Action F _Action B pp_group_size = num_stages = num_microbatches = assertRaises AssertionError _validate_schedule actions pp_group_size num_stages num_microbatches test_invalid_schedule_missing_action actions = _Action F _Action F pp_group_size = num_stages = num_microbatches = assertRaises AssertionError _validate_schedule actions pp_group_size num_stages num_microbatches ScheduleUtilTests TestCase test_generate_stage_to_rank_mapping stage_to_rank = generate_stage_to_rank_mapping assertEqual stage_to_rank stage_to_rank = generate_stage_to_rank_mapping assertEqual stage_to_rank stage_to_rank = generate_stage_to_rank_mapping assertEqual stage_to_rank stage_to_rank = generate_stage_to_rank_mapping style= v assertEqual stage_to_rank stage_to_rank = generate_stage_to_rank_mapping style= v assertEqual stage_to_rank stage_to_rank = generate_stage_to_rank_mapping style= v assertEqual stage_to_rank instantiate_parametrized_tests TestScheduleLowering __name__ == __main__ run_tests