mypy allow-untyped-defs contextlib dataclasses logging textwrap typing Any Callable Optional sympy torch torch _inductor config torch _inductor codegen common CSE CSEVariable IndentedBuffer Kernel ValueRanges torch _inductor ir Buffer ComputedBuffer InputBuffer torch _inductor ops_handler StoreMode torch _inductor utils OrderedSet torch _inductor virtualized V utils sympy_index_symbol cutedsl_op_overrides CuteDSLOpOverrides TODO setting main kernel w suffix We have should probably just auto generate MAIN_SUFFIX = main log = logging getLogger __name__ kernel_code_log = torch _logging getArtifactLogger __name__ kernel_code CuteDSLKernelWrapper Wrapper provide run interface CuteDSL kernels __init__ kernel_fn Callable Any kernel_path Optional str = None kernel_fn = kernel_fn kernel_path = kernel_path kernel_code_log info CuteDSL kernel path s kernel_path run args stream=None kwargs Execute CuteDSL kernel Args args Arguments pass kernel function stream CUDA stream pass kernel function kwargs Additional keyword arguments kernel Returns Result kernel execution kernel_fn args stream=stream kwargs dataclasses dataclass CuteDSLSubgraphInfo Minimal subgraph info CuteDSL kernels body IndentedBuffer template_mask Optional str = None template_out Optional str = None cse Optional CSE Any = None __post_init__ only_copy_if_non_none_fields = cse to_dict field name getattr field name field dataclasses fields CuteDSLTemplateKernel Kernel Template kernel implementation CuteDSL CUTLASS Python DSL Handles code generation argument management CuteDSL CUDA kernels Provides CuteDSL-specific functionality tensor conversion kernel configuration __init__ kernel_name str input_nodes list Buffer output_node Buffer subgraphs Optional list Buffer = None - None Call parent Kernel constructor super __init__ kernel_name = kernel_name input_nodes = input_nodes output_node = output_node subgraphs = subgraphs subgraph_bodies dict str CuteDSLSubgraphInfo = Template attributes body IndentedBuffer = IndentedBuffer template_mask Optional str = None template_out Optional str = None template_indices Optional list Any = None render_hooks dict str Any = TODO Additional attributes needed template system prologue_fused_inputs OrderedSet str = OrderedSet prologue_fused_inputs_preserve_zero OrderedSet str = OrderedSet named_input_nodes dict str Buffer = Create named input nodes mapping i input_node enumerate input_nodes node_name = getattr input_node name f input_ i named_input_nodes node_name = input_node cse = CSE name_prefix= tmp Track all tensor buffers added during modification processing collected_tensor_buffers list str = kexpr expr sympy Expr - str Convert sympy expression CuteDSL string representation str expr gen_imports - str Generate common imports CuteDSL templates imports = IndentedBuffer imports splice torch cutlass cutlass cute cute cutlass cute runtime from_dlpack cuda bindings driver cuda cutlass _mlir dialects math mlir_math operator torch _inductor codegen cutedsl _cutedsl_utils ssa_to_indexable result_to_ssa imports getvalue gen_defines kwargs - str Generate CuteDSL parameter definitions kwargs similar Triton s gen_defines params = IndentedBuffer name val kwargs items params writeline f name cutlass Constexpr = val params getvalue render template kwargs torch _inductor select_algorithm PartialRender Render kernel using template returning PartialRender object hooks Available hooks jinja rendering template_env = def_kernel def_kernel gen_defines lambda gen_defines kwargs get_output get_output get_tensor_buffers get_tensor_buffers unpack_buffers unpack_buffers modification modification Render template environment provided kwargs rendered_code = template render kernel_name=self kernel_name input_nodes=self input_nodes output_node=self output_node template_env kwargs Always prepend common imports imports = gen_imports full_code = imports + rendered_code PartialRender full_code render_hooks contextlib contextmanager set_subgraph_body body_name str Set active subgraph body template processing assert all hasattr field name field dataclasses fields CuteDSLSubgraphInfo old_state = key name getattr key name key dataclasses fields CuteDSLSubgraphInfo body_name subgraph_bodies subgraph_bodies body_name = CuteDSLSubgraphInfo body=IndentedBuffer template_mask=None template_out=None cse=None subgraph = subgraph_bodies body_name key value subgraph to_dict items value None key getattr subgraph only_copy_if_non_none_fields continue setattr key value try yield finally Save current state back subgraph subgraph_bodies body_name = CuteDSLSubgraphInfo key name getattr key name key dataclasses fields CuteDSLSubgraphInfo Restore old state key value old_state items setattr key value contextlib contextmanager create_subgraph_body body_name str clear_cse bool = False Create new subgraph body template processing assert body_name subgraph_bodies f Subgraph body body_name already exists new_cse = cse clone clear_cse None subgraph_bodies body_name = CuteDSLSubgraphInfo body=IndentedBuffer template_mask=None template_out=None cse=new_cse set_subgraph_body body_name yield def_kernel argnames Define kernel function signature CuteDSL templates renames = IndentedBuffer initial_indent= i input_node enumerate input_nodes buf_name = input_node get_name args input buf_name Template aliasing converts template variables e g input_a function args e g arg_input_a generates rename statements so template code can use original names i len argnames template_name = argnames i arg_name = f arg_ template_name args input_buffers buf_name = arg_name renames writeline f template_name = arg_name output_node args output output_node get_name hook Deferred execution arg definitions must collected after template processing adds all args arg_defs _ = args python_argdefs code = IndentedBuffer code writeline f Kernel function signature kernel_name params = x full_name x arg_defs + stream code writeline f kernel_name _ MAIN_SUFFIX join params code indent code splice renames getvalue code getvalue assert DEF_KERNEL render_hooks Placeholder-based rendering hook will called when template encounters DEF_KERNEL render_hooks DEF_KERNEL = hook DEF_KERNEL get_output Get actual argument name output buffer assert output_node Output node must exist get output buffer name buf_name = output_node get_name output = args output_buffers get buf_name None output None raise ValueError f Output buffer buf_name found args output get_tensor_buffers Get list tensor buffer names collected during modifications collected_tensor_buffers unpack_buffers buffer_list_name str indent_width int = Generate buffer unpacking code via render hook hook tensor_buffers = get_tensor_buffers tensor_buffers Generate unpacking assignments in_ptr = buffers etc unpacking_lines = i buffer_name enumerate tensor_buffers pyrefly ignore bad-argument-type unpacking_lines append f buffer_name = buffer_list_name i indent = indent_width \n + indent + \n + indent join unpacking_lines Register hook placeholder placeholder = UNPACK_BUFFERS TODO I think double invoking fine specific hook assert placeholder render_hooks render_hooks placeholder = hook placeholder call_kernel name str node=None Call kernel function Simplified version TritonTemplateKernel call_kernel wrapper = V graph wrapper_code _ call_args _ arg_types = args python_argdefs TODO triton should really swapped w ` python ` wrapper generate_kernel_call name call_args triton=True arg_types=arg_types _get_subgraph subgraph_number int Get subgraph number modification processing assert isinstance subgraph_number int assert isinstance subgraphs list assert subgraph_number len subgraphs f Invalid subgraph number provided create_modification subgraph_number must len subgraphs assert body getvalue == Body should clear before adding modification subgraphs subgraph_number modification subgraph_number int output_name Optional str mask Optional str = None fixed_inputs - str Generate CuteDSL code subgraph modification Find unique name avoid collisions between multiple modifications same subgraph num = while f mod_ subgraph_number _ num subgraph_bodies num += create_subgraph_body f mod_ subgraph_number _ num clear_cse=True subgraph = _get_subgraph subgraph_number modification_handler = ModificationWrapperCuteDSL subgraph_number fixed_inputs mask V set_kernel_handler V set_ops_handler modification_handler assert isinstance subgraph ComputedBuffer list f Expected ComputedBuffer List ComputedBuffer got type subgraph isinstance subgraph list raise NotImplementedError Scatter graphs supported CuteDSL isinstance subgraph data InputBuffer grad_score_mod can InputBuffers out = subgraph data make_loader Inline pointwise lowering into template out = subgraph data inner_fn output_name None assert out None f Expected computation result named output output_name body writeline f output_name = out value Side-effect only no output assignment currently only scatter operations raise NotImplementedError Side-effect only modifications yet supported CuteDSL Add Buffers added during modification collected_tensor_buffers extend modification_handler tensor_buffers body getvalue ModificationWrapperCuteDSL V WrapperHandler type ignore name-defined Wrapper handler enables CuteDSL code generation during subgraph modifications This sits between PyTorch IR CuteDSL code generation providing Operation substitution converts PyTorch ops CuteDSL equivalents via CuteDSLOpOverrides Placeholder handling resolves fixed_inputs during template processing Limited operation support currently restricted pointwise operations __init__ kernel subgraph_number int fixed_inputs dict str Any mask Optional str cutedsl_ops = CuteDSLOpOverrides super __init__ cutedsl_ops name = f CuteDSLPlaceholderSubstitution_ subgraph_number kernel = kernel fixed_inputs = fixed_inputs mask = mask Track tensor buffers get added during modification processing tensor_buffers list str = _get_input_dtype name str - torch dtype Get dtype input kernel s named_input_nodes name kernel named_input_nodes kernel named_input_nodes name dtype TODO Fallback common dimension names - should replaced proper dtype tracking torch float name b h m n torch int load name str index sympy Expr Handle loading tensor fixed template args input CuteDSL name fixed_inputs var = _add_kernel_input name buffer = V graph get_buffer name var_dtype = buffer dtype cute_dtype = CuteDSLOpOverrides TORCH_TO_CUTE_DTYPE get var_dtype cutlass Float renamed_index = kernel rename_indexing index idx_var = _emit_scalar_fragment kernel kexpr renamed_index cutlass Int torch int val_frag = kernel cse newvar dtype=var_dtype kernel body writeline f val_frag = cute make_fragment cute_dtype kernel body writeline f val_frag = var idx_var final_expr = f val_frag load var_dtype torch float torch bfloat config triton codegen_upcast_to_fp final_expr = f final_expr cutlass Float var_dtype = torch float out = kernel cse generate kernel body final_expr dtype=var_dtype bounds=ValueRanges unknown out value = fixed_inputs name dtype = _get_input_dtype name kernel cse generate kernel body value bounds=ValueRanges unknown dtype=dtype _emit_scalar_fragment expr_str str cute_dtype str torch_dtype torch dtype - str Convert SSA expression indexable scalar tensor loads Workaround lack gather support SSA values cannot used directly indices This generates code convert SSA â†’ indexable scalar result = kernel cse newvar dtype=torch_dtype kernel body writeline f result = ssa_to_indexable expr_str cute_dtype str result indirect_indexing index_var str size check wrap_neg=True Convert index variable symbolic form sympy_index_symbol str index_var pyrefly ignore bad-override store name str index sympy Expr value CSEVariable mode StoreMode = None - str raise NotImplementedError Store operations supported - CuteDSL limited read-only operations _add_kernel_input name str Add name input kernel input ref Get remapped name will used kernel remapped_name = kernel args input name Track remapped name later collection remapped_name tensor_buffers tensor_buffers append remapped_name remapped_name _process_indexing index Process rename indexing adding symbols kernel inputs renamed = kernel rename_indexing index kernel kexpr renamed _default name str args tuple Any kwargs dict str Any - Any try getattr _inner name args kwargs except NotImplementedError e bar = = msg = textwrap dedent f bar UNSUPPORTED CUTEDSL OPERATION name bar This operation yet implemented Inductor Please open issue https github com pytorch pytorch issues following information Operation name Args args r Kwargs kwargs r Title your issue CuteDSL Missing operation name bar strip raise NotImplementedError msg e