usr bin env python os sys pathlib Path typing Union sys path insert os path join os path dirname __file__ argparse json zipfile dataclasses asdict typing Any Optional pandas pd type ignore tools stats upload_stats_lib download_s _artifacts upload_to_s tools stats utilization_stats_lib getDataModelVersion getTsNow OssCiSegmentV OssCiUtilizationMetadataV OssCiUtilizationTimeSeriesV UtilizationMetadata UtilizationRecord WorkflowInfo TEST_USAGE_LOG_FILENAME = usage_log txt CMD_PYTHON_LEVEL = CMD_PYTHON UTILIZATION_BUCKET = ossci-utilization PYTORCH_REPO = pytorch pytorch JOB_TEST_ARTIFACT_PREFIX = logs-test SegmentGenerator generates test segment utilization records currently only generate segments python commands level segment_delta_threshold threshold determine segment continuous default seconds generate records list UtilizationRecord segment_delta_threshold int = - list OssCiSegmentV len records == cmd_col_name = cmd time_col_name = time flatten time series detected cmds df = pd DataFrame time_col_name record timestamp cmd_col_name process record records process record cmd_names df time_col_name = pd to_datetime df time_col_name unit= s utc=True get unique cmd names pyrefly ignore bad-argument-type unique_cmds_df = pd DataFrame df cmd_col_name unique columns= cmd_col_name get all detected python cmds cmd_list = unique_cmds_df unique_cmds_df cmd_col_name str startswith python cmd_col_name tolist find segments screening continuoues time series data segments list OssCiSegmentV = value cmd_list subset = df df cmd_col_name == value copy continuous_segments = _find_continuous_windows segment_delta_threshold time_col_name subset row continuous_segments segment = OssCiSegmentV level=CMD_PYTHON_LEVEL name=value start_at=int row start_time timestamp end_at=int row end_time timestamp extra_info= segments append segment print f Db Segments detected pytest cmd len cmd_list generated segments len segments segments _find_continuous_windows threshold int time_column_name str df Any lintrunner keep complaining about type df s problem - list dict str Any time_threshold = pd Timedelta seconds=threshold df = df sort_values by=time_column_name reset_index drop=True df time_diff = df time_column_name diff df segment = df time_diff time_threshold cumsum segments = df groupby segment agg start_time= time_column_name first end_time= time_column_name last reset_index drop=True segments start_time end_time to_dict orient= records type ignore no-any-return UtilizationDbConverter convert utilization log model db model __init__ info WorkflowInfo metadata UtilizationMetadata records list UtilizationRecord segments list OssCiSegmentV metadata = metadata records = records segments = segments created_at = getTsNow info = info end_time_stamp = max record timestamp record records end_at = end_time_stamp convert - tuple OssCiUtilizationMetadataV list OssCiUtilizationTimeSeriesV db_metadata = _to_oss_ci_metadata timeseries = _to_oss_ci_timeseries_list db_metadata timeseries _to_oss_ci_metadata - OssCiUtilizationMetadataV OssCiUtilizationMetadataV repo=self info repo workflow_id=self info workflow_run_id run_attempt=self info run_attempt job_id=self info job_id workflow_name=self info workflow_name job_name=self info job_name usage_collect_interval=self metadata usage_collect_interval data_model_version=str metadata data_model_version created_at=self created_at gpu_count=self metadata gpu_count metadata gpu_count cpu_count=self metadata cpu_count metadata cpu_count gpu_type=self metadata gpu_type metadata gpu_type start_at=self metadata start_at end_at=self end_at segments=self segments tags= _to_oss_ci_timeseries_list - list OssCiUtilizationTimeSeriesV _to_oss_ci_time_series record type= utilization tags= record record records _to_oss_ci_time_series record UtilizationRecord type str tags list str - OssCiUtilizationTimeSeriesV OssCiUtilizationTimeSeriesV created_at=self created_at type=type tags=tags time_stamp=record timestamp repo=self info repo workflow_id=self info workflow_run_id run_attempt=self info run_attempt job_id=self info job_id workflow_name=self info workflow_name job_name=self info job_name json_data=str record data to_json record data UploadUtilizationData main handle utilization data conversion s upload fetches raw log data s convert log model then convert db model upload s __init__ artifact_prefix str info WorkflowInfo dry_run bool = False debug bool = False local_path str = artifact_prefix = artifact_prefix info = info segment_generator = SegmentGenerator debug_mode = debug dry_run = dry_run local_path = local_path start - None local_path metadata valid_records _ = get_log_data_from_local local_path print f Search test log s bucket UTILIZATION_BUCKET metadata valid_records _ = get_log_data_from_s info workflow_run_id info job_id info run_attempt artifact_prefix metadata print Log Model Failed process test log metadata None None len valid_records == print Log Model Failed process test log no valid records None segments = segment_generator generate valid_records db_metadata db_records = UtilizationDbConverter info metadata valid_records segments convert len db_records print f db model Peek db timeseries \n json dumps asdict db_records indent= dry_run print dry-run-mode no upload dry run mode version = f v_ db_metadata data_model_version metadata_collection = util_metadata ts_collection = util_timeseries debug_mode metadata_collection = f debug_ metadata_collection ts_collection = f debug_ ts_collection _upload_utilization_data_to_s collection=metadata_collection version=version repo=self info repo workflow_run_id=self info workflow_run_id workflow_run_attempt=self info run_attempt job_id=self info job_id file_name= metadata docs= asdict db_metadata _upload_utilization_data_to_s collection=ts_collection version=version repo=self info repo workflow_run_id=self info workflow_run_id workflow_run_attempt=self info run_attempt job_id=self info job_id file_name= time_series docs= asdict record record db_records _upload_utilization_data_to_s collection str version str repo str workflow_run_id int workflow_run_attempt int job_id int file_name str docs list dict str Any - None bucket_name = UTILIZATION_BUCKET key = f collection version repo workflow_run_id workflow_run_attempt job_id file_name upload_to_s bucket_name key docs get_log_data_from_local file_path str artifact_prefix str = - tuple Optional UtilizationMetadata list UtilizationRecord list UtilizationRecord test_log_content = read_file file_path test_log_content None metadata records error_records = convert_to_log_models test_log_content metadata None None print f Converted Log Model UtilizationMetadata \n metadata metadata records error_records get_log_data_from_s workflow_run_id int job_id int workflow_run_attempt int artifact_prefix str = JOB_TEST_ARTIFACT_PREFIX - tuple Optional UtilizationMetadata list UtilizationRecord list UtilizationRecord artifact_paths = download_s _artifacts artifact_prefix workflow_run_id workflow_run_attempt job_id len artifact_paths == print f Failed download artifacts workflow workflow_run_id job job_id None len artifact_paths print f Found more than one artifact workflow workflow_run_id job job_id artifact_paths None p = artifact_paths test_log_content = handle_file p test_log_content None metadata records error_records = convert_to_log_models test_log_content metadata None None print f Converted Log Model UtilizationMetadata \n metadata metadata records error_records _process_raw_record line str - tuple Optional UtilizationRecord bool try record = UtilizationRecord from_json line record error record False record True except Exception e print f Failed parse JSON line e None False _process_utilization_records lines list str - tuple list UtilizationRecord list UtilizationRecord results = _process_raw_record line line lines valid_records = record record valid results valid record None invalid_records = record record valid results valid record None valid_records invalid_records convert_to_log_models content str - tuple Optional UtilizationMetadata list UtilizationRecord list UtilizationRecord content None lines = content splitlines metadata = None len lines print Expected least two records log file None try metadata = UtilizationMetadata from_json lines except Exception e print f warning Failed parse metadata e data lines None metadata data_model_version = getDataModelVersion print f warning Data model version mismatch metadata data_model_version = getDataModelVersion None result_logs error_logs = _process_utilization_records lines metadata result_logs error_logs handle_file file_path Path - str file_path match zip print f extracting TEST_USAGE_LOG_FILENAME zip file file_path unzip_file file_path TEST_USAGE_LOG_FILENAME file_path match txt print f extracting file_path read_file file_path print f file_path supported file type read_file file_path Union str Path - str try isinstance file_path Path file_path is_file file_path open r f f read print f warning file file_path does exist isinstance file_path str os path isfile file_path open file_path f f read print f warning file file_path does exist print f warning unsupported file_path type type file_path except Exception e print f warning trying read file file_path failed e unzip_file path Path file_name str - str try zipfile ZipFile path zip_file Read desired file zip archive zip_file read name=file_name decode except Exception e print f warning trying download test log object failed e parse_args - argparse Namespace Parse command line arguments Returns argparse Namespace Parsed arguments parser = argparse ArgumentParser description= Upload test stats s parser add_argument -- workflow-run-id type=int required=True help= id workflow get artifacts parser add_argument -- workflow-run-attempt type=int required=True help= which retry workflow parser add_argument -- workflow-name type=str required=True help= id workflow get artifacts parser add_argument -- job-id type=int required=True help= id workflow get artifacts parser add_argument -- job-name type=str required=True help= id workflow get artifacts parser add_argument -- repo type=str required=False help= which GitHub repo workflow run belongs parser add_argument -- debug action= store_true help= Enable debug mode parser add_argument -- dry-run action= store_true help= Enable dry-run mode parser add_argument -- artifact-prefix type=str required=False help= artifact prefix download raw utilizarion data s parser add_argument -- local-path type=str required=False help= path raw utilizarion data local location parser parse_args __name__ == __main__ args = parse_args Flush stdout so any errors upload show up last logs sys stdout flush repo = PYTORCH_REPO args repo repo = args repo print f repo repo workflow_info = WorkflowInfo workflow_run_id=args workflow_run_id run_attempt=args workflow_run_attempt job_id=args job_id workflow_name=args workflow_name job_name=args job_name repo=repo artifact_prefix = JOB_TEST_ARTIFACT_PREFIX args artifact_prefix artifact_prefix = args artifact_prefix print f args artifact_prefix args artifact_prefix print f artifact_prefix artifact_prefix ud = UploadUtilizationData info=workflow_info dry_run=args dry_run debug=args debug artifact_prefix=artifact_prefix local_path=args local_path ud start