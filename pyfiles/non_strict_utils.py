mypy allow-untyped-defs builtins contextlib functools inspect logging math collections defaultdict collections abc Callable Sequence contextlib contextmanager typing Any Optional TYPE_CHECKING Union torch torch utils _pytree pytree torch _dynamo source AttrSource GetItemSource LocalSource TensorProperty TensorPropertySource torch _dynamo variables builder TrackedFake torch _export passes lift_constants_pass ConstantAttrMap torch _export utils _fakify_params_buffers torch _guards Source torch _library fake_class_registry FakeScriptObject torch _subclasses fake_tensor FakeTensorMode torch export Constraint torch export dynamic_shapes _check_dynamic_shapes _combine_args _DimHint _DimHintType _IntWrapper _process_dynamic_shapes _RelaxedConstraint _tree_map_with_path torch export graph_signature CustomObjArgument torch fx experimental _config config torch fx experimental symbolic_shapes _find_user_code_frame _suggest_fixes_for_data_dependent_error_non_strict ConstraintViolationError DimDynamic EqualityConstraint GuardOnDataDependentSymNode RelaxedUnspecConstraint ShapeEnv StatelessSymbolicContext SymIntSymbolicContext ValueRanges torch utils _pytree GetAttrKey KeyPath MappingKey SequenceKey tree_map_with_path torch utils _sympy numbers int_oo TYPE_CHECKING sympy Symbol log = logging getLogger __name__ _KeyPath Wraps ` KeyPath ` aid ` isinstance ` checks __init__ kp KeyPath kp = kp _KeyPathTrie Builds trie ` KeyPath ` prefixes mapping ` Source ` leaves __init__ root = add kp KeyPath src Source assert len kp path leaf = kp node = root k path k node node k = node = node k node leaf = src get kp KeyPath - tuple Source KeyPath node = root while isinstance node Source assert len kp k kp = kp type ignore assignment node = node k pyrefly ignore bad-return node kp make_sourced_prefixes nn_module args kwargs - _KeyPathTrie kp_args kp_kwargs = tree_map_with_path lambda kp _ _KeyPath kp tuple None _ args k None k kwargs noqa C kp_combined_args = _combine_args nn_module kp_args kp_kwargs sourced_prefixes = _KeyPathTrie name struct kp_combined_args items src = LocalSource name isinstance struct _KeyPath sourced_prefixes add struct kp src isinstance struct tuple i prefix enumerate struct assert isinstance prefix _KeyPath sourced_prefixes add prefix kp GetItemSource src i isinstance struct dict k prefix struct items assert isinstance prefix _KeyPath sourced_prefixes add prefix kp GetItemSource src k sourced_prefixes key_path_to_source kp KeyPath sourced_prefixes Optional _KeyPathTrie = None - Source Given key path source key path sourced_prefixes None source Source = LocalSource args source kp = sourced_prefixes get kp k kp isinstance k SequenceKey source = GetItemSource source k idx isinstance k MappingKey source = GetItemSource source k key isinstance k GetAttrKey source = AttrSource source k name raise ValueError f Unknown KeyEntry k source _is_constant_argument t t None isinstance t float bool str fakify mode FakeTensorMode kp KeyPath t Any t_constraints dict int dict int Constraint sources dict tuple int int list Source sourced_prefixes Optional _KeyPathTrie = None source = key_path_to_source kp sourced_prefixes=sourced_prefixes _is_constant_argument t isinstance t torch ScriptObject torch nn Module t isinstance t _IntWrapper t dynamism None t dynamism type type ignore union-attr _DimHintType DYNAMIC _DimHintType AUTO symint = mode shape_env create_unspecified_symint_and_symbol type ignore union-attr t val source DimDynamic DYNAMIC context = SymIntSymbolicContext constraint=RelaxedUnspecConstraint warn_only=False t dynamism type == _DimHintType DYNAMIC type ignore union-attr None mode shape_env tracked_fakes append type ignore union-attr TrackedFake symint source context symint t val isinstance t torch Tensor raise ValueError f Unsupported input type type t Export only supports pytree containers basic types Tensor int float input To register custom dataclass use torch export register_dataclass To register custom container type use torch utils _pytree register_pytree_node To register constant input use torch utils _pytree register_constant Create symbolic context handles subclass recursion internally symbolic_context = _create_symbolic_context_for_tensor t source t_constraints sources mode fake = mode from_tensor t source=source symbolic_context=symbolic_context mode shape_env tracked_fakes append TrackedFake fake source symbolic_context type ignore union-attr fake _create_symbolic_context_for_tensor t source t_constraints sources mode Helper function create symbolic context tensor torch _dynamo source AttrSource torch fx experimental symbolic_shapes DimDynamic RelaxedUnspecConstraint SubclassSymbolicContext torch utils _python_dispatch is_traceable_wrapper_subclass Common dynamic dimension logic both regular tensors subclasses n_dims = len t shape dynamic_sizes = constraint_sizes = None n_dims i range n_dims i getattr t _dynamo_weak_dynamic_indices dynamic_sizes append DimDynamic DYNAMIC i getattr t _dynamo_dynamic_indices bit annoying we need replicate process _dynamo variables builder py where RelaxedUnspecConstraint created Dim DYNAMIC so constraint violations raised when specializing dynamic_sizes append DimDynamic DYNAMIC constraint_sizes i = RelaxedUnspecConstraint warn_only=False type ignore call-overload dynamic_sizes append DimDynamic STATIC Handle nested subclasses is_traceable_wrapper_subclass t Get inner contexts recursively inner_contexts = attrs _ = type t __tensor_flatten__ t Propagate outer tensor constraints inner tensors already present attr attrs inner_tensor = getattr t attr inner_source = AttrSource source attr inner_contexts attr = _create_symbolic_context_for_tensor inner_tensor inner_source t_constraints sources mode symbolic_context = SubclassSymbolicContext dynamic_sizes=dynamic_sizes constraint_sizes=constraint_sizes type ignore arg-type view_base_context=None tensor_source=source shape_env_to_source_to_symbol_cache= inner_contexts=inner_contexts symbolic_context StatelessSymbolicContext = type ignore no-redef StatelessSymbolicContext dynamic_sizes=dynamic_sizes constraint_sizes=constraint_sizes type ignore arg-type Apply constraints common logic t_id = id t assert mode shape_env None t_id t_constraints i constraint t_constraints t_id items src = TensorPropertySource base=source prop=TensorProperty SIZE idx=i sources t_id i append src isinstance constraint _RelaxedConstraint continue symbolic_context constraint_sizes i = constraint constraint_range mode shape_env source_name_to_debug_name src name = constraint name type ignore assignment symbolic_context _is_unbacked_symint symbol isinstance symbol torch SymInt False symbol node shape_env is_unbacked_symint symbol node expr _tensor_min_max args real_callable tensor_callable kwargs This logic replicated dynamo variables builtin py len args == kwargs arg arg = args Case Both tensors isinstance arg torch Tensor isinstance arg torch Tensor tensor_callable arg arg Case One tensor one scalar isinstance arg torch Tensor isinstance arg torch Tensor isinstance arg torch Tensor arg arg = arg arg isinstance arg int float kwarg = min tensor_callable torch maximum max arg torch clamp arg kwarg type ignore call-overload real_callable arg arg Case SymInts isinstance arg torch SymInt isinstance arg torch SymInt torch sym_max arg arg tensor_callable torch maximum torch sym_min arg arg Fallback real_callable arg arg Single iterable argument handling len args == kwargs iterable = args isinstance iterable torch Tensor tensor_callable iterable try iterator = iter iterable except TypeError pass items = list iterator items raise ValueError f real_callable __name__ arg empty sequence functools reduce lambda b _tensor_min_max b real_callable=real_callable tensor_callable=tensor_callable items Fallback original callable real_callable args kwargs contextmanager _override_builtin_ops original_max = builtins max original_min = builtins min original_pow = math pow pyrefly ignore bad-assignment builtins max = functools partial _tensor_min_max real_callable=original_max tensor_callable=torch maximum pyrefly ignore bad-assignment builtins min = functools partial _tensor_min_max real_callable=original_min tensor_callable=torch minimum math pow = lambda x y x y type ignore operator try yield finally builtins max = original_max builtins min = original_min math pow = original_pow make_fake_inputs nn_module args kwargs dynamic_shapes prefer_deferred_runtime_asserts_over_guards=False Given nn module example inputs constraints new fake mode fake inputs created mode whose dynamic shape dimensions constrained given ranges sources pairs dynamic shape dimensions constrained equal TODO avik refactor Dynamo avoid duplication following code between non-strict strict Specifically here non-strict we do following pre-tracing steps - Fakify inputs - Process input shape equalities In strict these steps spread across multiple files - output_graph py fakifies inputs - post-tracing guards py processes input shape equalities torch _functorch config _config Map ints wrapper structure help us mark dynamic dynamic We will unwrap ints fakify later args kwargs = pytree tree_map_only int lambda _IntWrapper args kwargs combined_args = _combine_args nn_module args kwargs _check_dynamic_shapes combined_args dynamic_shapes constraints = _process_dynamic_shapes combined_args dynamic_shapes t_constraints dict int dict int Constraint = defaultdict dict constraint constraints t_constraints constraint t_id constraint dim = constraint context = torch _guards TracingContext try_get context None This occurs when we exporting within dynamo There already exists toplevel TracingContext fake mode so we do want create another fake mode fake_mode = context fake_mode assert fake_mode None isinstance nn_module forward functools partial functools handles nesting itself no need recurse code = nn_module forward func __code__ code = nn_module forward __code__ co_fields = co_name code co_name co_filename code co_filename co_firstlineno code co_firstlineno _config patch fake_tensor_allow_unsafe_data_ptr_access=False fake_mode = FakeTensorMode shape_env=ShapeEnv tracked_fakes= co_fields=co_fields prefer_deferred_runtime_asserts_over_guards=prefer_deferred_runtime_asserts_over_guards trace_asserts=True allow_non_fake_inputs=True export=True fake_mode shape_env None fake_mode shape_env tracked_fakes None raise ValueError Detected fake_mode does have shape_env tracked fakes If you constructed module under FakeTensorMode please initialize like FakeTensorMode shape_env=ShapeEnv tracked_fakes= fake_mode original_signature = inspect signature nn_module forward sources dict tuple int int list Source = defaultdict list sourced_prefixes = make_sourced_prefixes nn_module args kwargs fake_args fake_kwargs = tree_map_with_path lambda kp val fakify fake_mode kp val t_constraints sources sourced_prefixes=sourced_prefixes args kwargs names dict str tuple int int = source_pairs list tuple Source Source = derived_equalities list tuple Source Union Source Symbol Callable = phantom_symbols dict str Symbol = relaxed_sources set Source = set constraint constraints torch export dynamic_shapes _process_equalities constraint lambda t_id dim sources t_id dim fake_mode shape_env names source_pairs derived_equalities phantom_symbols relaxed_sources equalities_inputs = EqualityConstraint source_pairs=source_pairs derived_equalities=derived_equalities phantom_symbols=list phantom_symbols values relaxed_sources=relaxed_sources warn_only=False fake_mode fake_args fake_kwargs equalities_inputs original_signature dynamic_shapes _flatten_dynamic_shapes combined_args dict str Any dynamic_shapes Union dict str Any tuple Any list Any - list Any flat_shapes = _tree_map_helper path t shape nonlocal flat_shapes flat_shapes append shape _tree_map_with_path _tree_map_helper combined_args dynamic_shapes flat_shapes _clean_dynamic_markers tensor torch Tensor - None attr _dynamo_weak_dynamic_indices _dynamo_dynamic_indices _dynamo_dynamic_range _dynamo_static_indices _dynamo_unbacked_indices hasattr tensor attr delattr tensor attr produce_guards_and_solve_constraints fake_mode FakeTensorMode gm torch fx GraphModule dynamic_shapes Union dict str Any tuple Any list Any None equalities_inputs EqualityConstraint original_signature inspect Signature Given fake mode sources pairs corresponding equal dynamic shape dimensions graph module produce guards fake mode s shape env raising constraint violations any solve suggest simplifications fixes Dynamo already performs so non-strict mode Additional inputs equalities_inputs equality constraints use guards original_signature signature forward method shape_env = fake_mode shape_env assert shape_env None assert shape_env tracked_fakes None placeholders = tf fake tf shape_env tracked_fakes sources = tf source tf shape_env tracked_fakes input_contexts = tf symbolic_context tf shape_env tracked_fakes constraint_violation_error = None try shape_env produce_guards placeholders sources input_contexts=input_contexts equalities_inputs=equalities_inputs ignore_static=False except ConstraintViolationError e constraint_violation_error = e shape_env frozen = True dim_constraints = shape_env dim_constraints dim_constraints None Expected when shape_env produce_guards throws early constraint violation error There nothing solve case TODO avik Maybe record constraint violation error instead replay later assert constraint_violation_error raise constraint_violation_error dim_constraints solve forced_specializations = dim_constraints forced_specializations msg = dim_constraints prettify_results original_signature dynamic_shapes type ignore arg-type constraint_violation_error forced_specializations type ignore arg-type constraint_violation_error constraint_violation_error args = constraint_violation_error args + msg forced_specializations constraint_violation_error = ConstraintViolationError msg constraint_violation_error raise constraint_violation_error is_int x object - bool isinstance x int isinstance x torch SymInt x node expr is_number _constrain_user_specified_dimhint_range symint torch SymInt hint int dim _DimHint range_constraints shape_env keypath KeyPath i Optional int = None - Optional str trace_vr = range_constraints symint node expr is_int symint ValueRanges int symint int symint warn specialization Dim AUTO actual error dim type == _DimHintType AUTO trace_vr is_singleton hint pathstr = f inputs pytree keystr keypath i None pathstr += f shape i msg = f dimension pathstr specialized Dim AUTO specified along + f sample input hint = hint log warning msg try user_vr = ValueRanges lower= dim min None dim min upper=int_oo dim max None dim max is_int symint out_vr = trace_vr user_vr range_constraints symint node expr = user_vr shape_env var_to_range symint node _expr = user_vr out_vr = range_constraints symint node expr check Dim DYNAMIC specializations special case error message dim type == _DimHintType DYNAMIC out_vr is_singleton path = f inputs pytree keystr keypath i None path += f shape i trace_vr is_singleton hint torch fx experimental _config backed_size_oblivious msg = f - Received user-specified dim hint Dim DYNAMIC min= dim min max= dim max f export specialized due hint hint dimension path msg = f - Received user-specified dim hint Dim DYNAMIC min= dim min max= dim max f tracing inferred static shape out_vr lower dimension path msg except torch utils _sympy value_ranges ValueRangeError path = f inputs pytree keystr keypath i None path += f shape i msg = f - Received user-specified min max range dim min dim max f conflicting inferred min max range trace_vr lower trace_vr upper f path msg None make_constraints fake_mode FakeTensorMode gm torch fx GraphModule combined_args dict str Any dynamic_shapes Union dict str Any tuple Any list Any None num_lifted_inputs int Given fake mode s shape env user-specified dynamic shapes resulting range constraints equality constraints Additional args num_lifted_inputs number non-user-input placeholder nodes graph used only enumerate user-input nodes shape_env = fake_mode shape_env assert shape_env None inline_constraints = gm meta get inline_constraints range_constraints = defaultdict lambda ValueRanges int_oo &#124; inline_constraints dynamic_shapes dict range_constraints clean up dynamic markers tensors flat_paths flat_args = zip pytree tree_flatten_with_path combined_args arg flat_args isinstance arg torch Tensor _clean_dynamic_markers arg get individual dynamic shapes spec each input isinstance dynamic_shapes dict assert isinstance dynamic_shapes tuple list combined_args = type dynamic_shapes combined_args values type ignore assignment misc flat_dynamic_shapes = _flatten_dynamic_shapes combined_args dynamic_shapes check number shapes vs number inputs num_placeholders = node op == placeholder node gm graph nodes count True assert len flat_dynamic_shapes == num_placeholders - num_lifted_inputs free_symbols = set range_violations = input_index node enumerate gm graph nodes meta_val = node meta get val input_index num_lifted_inputs node op = placeholder meta_val None continue _is_constant_argument meta_val isinstance meta_val CustomObjArgument continue shape_spec = flat_dynamic_shapes input_index - num_lifted_inputs keypath = flat_paths input_index - num_lifted_inputs flat_arg = flat_args input_index - num_lifted_inputs isinstance meta_val int isinstance meta_val torch SymInt meta_val node expr is_number pass isinstance meta_val torch SymInt shape_spec None isinstance shape_spec _DimHint hint = flat_arg range_constraints meta_val node expr = shape_env bound_sympy meta_val node _expr violation = _constrain_user_specified_dimhint_range meta_val hint shape_spec range_constraints shape_env keypath None violation range_violations append violation raise RuntimeError nyi free_symbols update meta_val node expr free_symbols isinstance meta_val torch Tensor i d enumerate node meta val shape dim = None isinstance shape_spec list tuple dim = shape_spec i isinstance shape_spec dict dim = shape_spec get i is_int d Compute range constraint symbolic expression corresponding shape dimension store dim None isinstance dim _DimHint range_constraints d node expr = shape_env bound_sympy d node expr range_constraints d node expr = ValueRanges lower=dim min upper=dim max free_symbols update d node expr free_symbols check user-specified min max range DimHints we might want do even model tracing inferred static dimension isinstance dim _DimHint hint = flat_arg shape i violation = _constrain_user_specified_dimhint_range d hint dim range_constraints shape_env keypath i violation range_violations append violation raise RuntimeError f Unfamiliar meta val meta_val range_violations prefix = Found following conflicts between user-specified ranges inferred ranges model tracing \n raise ValueError prefix + \n join range_violations symbol free_symbols symbol range_constraints Placeholders can have symbolic shapes derived expressions The above code will record direct range constraints them so we can do runtime assertions In addition serde checks we want record range constraints their root symbols range_constraints symbol = shape_env var_to_range symbol dict range_constraints _gather_constant_attrs m torch nn Module - ConstantAttrMap Search module hierarchy gathering up all tensor ScriptObject constants Returns dictionary mapping hash value name constant We have abuse ` hash ` here unfortunately see ScriptObject hash constants = ConstantAttrMap buffers_parameters = set m buffers buffers_parameters update m parameters inner m torch nn Module prefix_atoms list str constants k v m __dict__ items isinstance v torch Tensor torch ScriptObject FakeScriptObject v buffers_parameters filter out buffers parameters leaving only constants continue fqn = join prefix_atoms + k constants add v fqn k v m named_children inner v prefix_atoms + k constants inner m constants constants _get_graph_inputs_of_type_nn_module args Optional tuple tuple Any dict Any Any - set type torch nn Module args None set module_types = set arg pytree tree_leaves args isinstance arg torch nn Module module_types add type arg module_types _enter_enable_graph_inputs_of_type_nn_module module_types set type torch nn Module - None t module_types torch _export utils register_module_as_pytree_input_node t _exit_enable_graph_inputs_of_type_nn_module module_types set type torch nn Module - None t module_types torch _export utils deregister_module_as_pytree_input_node t contextlib contextmanager _enable_graph_inputs_of_type_nn_module args Optional tuple tuple Any dict Any Any args None yield module_types = _get_graph_inputs_of_type_nn_module args _enter_enable_graph_inputs_of_type_nn_module module_types try yield finally _exit_enable_graph_inputs_of_type_nn_module module_types contextlib contextmanager _fakify_module_inputs args tuple Any kwargs dict Any Any fake_mode torch _subclasses fake_tensor FakeTensorMode This context manager used fakify module inputs Inputs args kwargs args kwargs containing module inputs haven t been fakified fake_mode fake mode used fakifying script objects It s same mode fakify input tensors ctxs = _enable_graph_inputs_of_type_nn_module args kwargs arg pytree tree_leaves args kwargs isinstance arg torch nn Module fake_params_buffers = _fakify_params_buffers fake_mode arg ctxs append torch nn utils stateless _reparametrize_module arg fake_params_buffers tie_weights=True strict=True stack_weights=True contextlib ExitStack stack ctx ctxs stack enter_context ctx yield contextlib contextmanager _fakify_script_objects mod torch nn Module args Sequence Any kwargs dict Any Any fake_mode Optional torch _subclasses fake_tensor FakeTensorMode This context manager used fakify script objects into FakeScriptObject Inputs mod module exported its recursive submodules s script object attrs haven t been fakified args kwargs args kwargs inputs mod script object inputs haven t been fakified fake_mode fake mode used fakifying script objects It s same mode fakify input tensors Returns mod patched module its its recursive submodules script object attrs have been fakified fake_args fake_kwargs new fakified args kwargs Script object inputs have been fakified Don t touch tensors fake_constant_attrs new map FakeScriptObject fqn original script object fake_to_real mapping between FakeScriptObject original script object order un-do patching constant_attrs ConstantAttrMap = _gather_constant_attrs mod assert any isinstance obj FakeScriptObject obj constant_attrs values Mod shouldn t contain any FakeScriptObject assert pytree tree_any lambda obj isinstance obj FakeScriptObject args kwargs args kwargs shouldn t contain any FakeScriptObject patched_attr = fake_constant_attrs = ConstantAttrMap fake_to_real = _maybe_fakify_obj obj fake_obj = torch _library fake_class_registry maybe_to_fake_obj fake_mode obj fake_to_real fake_obj = obj fake_obj _leaf_mod_and_attr mod torch nn Module attr_fqn str - tuple torch nn Module str prefix_attr last_attr = attr_fqn split cur_mod = mod attr prefix_attr cur_mod = getattr cur_mod attr cur_mod last_attr try obj fqns constant_attrs items torch _library fake_class_registry _is_script_object obj fake_script_obj = _maybe_fakify_obj obj fqn fqns cur_mod attr = _leaf_mod_and_attr mod fqn assert obj getattr cur_mod attr setattr cur_mod attr fake_script_obj fake_constant_attrs add fake_script_obj fqn patched_attr fqn = obj fqn fqns fake_constant_attrs add obj fqn fake_args fake_kwargs = pytree tree_map_only torch ScriptObject _maybe_fakify_obj args kwargs yield mod fake_args fake_kwargs fake_constant_attrs fake_to_real finally fqn orig_obj patched_attr items cur_mod attr = _leaf_mod_and_attr mod fqn setattr cur_mod attr orig_obj _NonStrictTorchFunctionHandler torch overrides TorchFunctionMode Handles data-dependent errors raised torch function calls non-strict Any data-dependent error due some condition unbacked symints cannot resolved A mechanical way fixing error use torch _check call assert either condition its negation The handler suggests these options code points location torch function call raised error part error message shown user who can then simply select copy-paste suggested fix location NOTE Not all data-dependent errors raised torch function calls In particular conditions unbacked symints can appear outside such calls such handled here Overrides torch functions known cause problems non-strict Certain Python features such indexing slicing cannot intercepted non-strict Likewise certain legacy ops such distributed collectives may need mapped other ops When there special handling Dynamo such things tracing can fail non-strict while succeeding strict Fortunately redirecting other torch functions can often fix such issues Handles line-of-code logging each torch function call non-strict Usage TORCHEXPORT_EXTENDED_DEBUG_CURRENT_LOC= TORCH_LOGS= +export _override func args kwargs torch distributed is_available torch distributed _functional_collectives REDUCE_OP_TO_STR traceable_collective_remaps func traceable_collective_remaps Redirect corresponding functional collective following Dynamo See torch distributed _functional_collectives py details The following adaptation CollectiveFunctionRewriteVariable mapped_func = traceable_collective_remaps func signature = inspect signature func kwargs = dict signature bind args kwargs arguments args = func torch distributed all_reduce torch distributed reduce_scatter_tensor torch distributed _reduce_scatter_base op kwargs kwargs op = REDUCE_OP_TO_STR kwargs op mapped_func args kwargs func torch tensor Redirect Python implementation torch tensor data symints NOTE avik We don t unconditionally redirect implementation because has some known incompletenesses e g doesn t support empty data See https github com pytorch pytorch issues any isinstance torch SymInt torch SymFloat torch SymBool pytree tree_flatten args torch _refs tensor args kwargs func __name__ == __getitem__ isinstance args torch Tensor rewrite dim item Redirect torch select indexing item None dim + torch unsqueeze dim isinstance item int torch SymInt dim torch select dim item Redirect torch ops aten slice slicing isinstance item slice step = item step item start None item stop None step == no-op dim + lambda t t dim + torch ops aten slice dim item start item stop step Otherwise do nothing items = list args isinstance args tuple args has_symint = False index_ellipsis = None t = args n_none_slices = t ndim + i item enumerate items isinstance item torch SymInt isinstance item slice any isinstance s torch SymInt s item start item stop item step has_symint = True item Ellipsis index_ellipsis = i item None n_none_slices -= only rewrite when there symints has_symint index_ellipsis None none_slices = slice None n_none_slices items index_ellipsis index_ellipsis + = none_slices dim = Sequence rewrites sequence = item items r = rewrite dim item None func args kwargs dim call_spec = r sequence append call_spec run Run sequence pyrefly ignore index-error t = args _method _args sequence t = _method t _args t run func args kwargs __torch_function__ func types args= kwargs=None kwargs = kwargs torch compiler is_dynamo_compiling func args kwargs log isEnabledFor logging DEBUG config extended_debug_current_loc frame = _find_user_code_frame frame None log debug s called s s s func __qualname__ frame f_code co_filename frame f_lineno frame f_code co_name func args kwargs = _override func args kwargs try func args kwargs except GuardOnDataDependentSymNode e _suggest_fixes_for_data_dependent_error_non_strict e raise