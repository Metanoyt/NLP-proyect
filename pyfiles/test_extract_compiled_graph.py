Owner s oncall jit unittest torch _lazy ts_backend init init_ts_backend init_ts_backend copy dis inspect re contextlib contextmanager torch torch fx nn torch _lazy config torch _lazy extract_compiled_graph extract_compiled_graph ModuleConstScale nn Module forward ModuleSub nn Module forward b - b ModuleAddcmul nn Module addcmul function takes Scalar which results special TSData containing Scalar rather than Tensor forward b c torch addcmul b c value= ModuleReturnMulti nn Module forward b b + - The default fx tracer will convert torch randn constant We may need custom tracer ModuleEagerTensor nn Module __init__ - None super __init__ forward b = torch randn device= cpu eager device + b The module planned cover case Fx graph eager tensor default device It s harder than ModuleEagerTensor because we can just override device argument Lazy since there no explicit device argument Unfortunately default fx tracer convert value forward method constant Comment out now ModuleReturnEagerTensorOnDefaultDevice nn Module __init__ - None super __init__ forward torch tensor dtype=torch float ModuleReturnDupTensor nn Module Handle corner case same tensor appears multiple times returned tuple torchbench like drq will hit corner case when running thru torchdynamo forward b c = + b - b c + c ModuleInplaceUpdate nn Module forward b sub_ b b - b + contextmanager force_fallback_ctx_mgr fallback_op oldconfig = config get_force_fallback config set_force_fallback fallback_op try yield None finally config set_force_fallback oldconfig contextmanager nop_ctx_mgr try yield None finally pass gen_rand_args mod args = _ range len inspect signature mod forward parameters args append torch randn args allclose expected actual unwrap cont isinstance cont list tuple len cont == cont cont expected = unwrap expected actual = unwrap actual isinstance expected torch Tensor isinstance actual torch Tensor torch allclose expected actual isinstance expected tuple list isinstance actual tuple list len expected == len actual all torch allclose b b zip expected actual raise RuntimeError Unexpected types verify_reusing_compiled_graph mod exception_msg_pattern ncase= args = gen_rand_args mod mod args dis dis mod forward try optimized_mod = extract_compiled_graph fx symbolic_trace mod args except RuntimeError e exception_msg_pattern None raise e reraise exception exception_message = str e re search exception_msg_pattern exception_message raise RuntimeError f Exception message does match required pattern exception_message e We done test case expects exception exception_msg_pattern None raise RuntimeError f Expect exception matching pattern exception_msg_pattern print value optimized_mod optimized_mod args check correctness failed_index = i range ncase rand_args = gen_rand_args mod rand_args_copy = copy deepcopy rand_args expected = mod rand_args actual = optimized_mod rand_args_copy allclose expected actual print f Incorrect results expected expected actual actual failed_index append i continue make sure arguments match after calling model forward method handle inplace updates allclose rand_args rand_args_copy print f Incorrect updated arguments expected rand_args actual rand_args_copy failed_index append i continue len failed_index raise RuntimeError f Failed len failed_index ncase cases maketest module_cls exception_msg_pattern=None ctxmgr=None wrapper nonlocal ctxmgr ctxmgr ctxmgr = nop_ctx_mgr ctxmgr verify_reusing_compiled_graph module_cls exception_msg_pattern wrapper OptimizeTest unittest TestCase test_sub = maketest ModuleSub Same test_sub force aten sub fallback We expect exception caught because LTC fallback test_ltc_fallback = maketest ModuleSub exception_msg_pattern= fallback aten sub ctxmgr=force_fallback_ctx_mgr aten sub test_const_scale = maketest ModuleConstScale test_addcmul = maketest ModuleAddcmul test_return_multi = maketest ModuleReturnMulti test_return_dup_tensor = maketest ModuleReturnDupTensor test_inplace_update = maketest ModuleInplaceUpdate __name__ == __main__ raise RuntimeError This test currently used should enabled discover_tests py required