mypy allow-untyped-defs math typing Optional torch distributed _shard metadata ShardMetadata _check_shard_metadata_pair_overlap shard ShardMetadata shard ShardMetadata Checks two shards overlap For each dim each shard check one shard resides other end second shard respect dim As example D shard we would check one shard above left other shard ndims = len shard shard_offsets i range ndims shard shard_offsets i = shard shard_offsets i + shard shard_sizes i False shard shard_offsets i = shard shard_offsets i + shard shard_sizes i False True _find_nd_overlapping_shards shards list ShardMetadata sharded_dims list int - Optional tuple int int Each rank has len sharded_dims tuples Each tuple represent begin end inclusive pair dimension shard_intervals = s shard_offsets dim s shard_offsets dim + s shard_sizes dim - dim sharded_dims s shards i range len shards shard_i = shard_intervals i j range i + len shards shard_j = shard_intervals j For each dim each shard check one shard resides other end second shard respect dim As example D shard we would check one shard above left other shard overlap = True interval_i interval_j zip shard_i shard_j interval_i interval_j interval_j interval_i overlap = False break overlap i j None _find_ d_overlapping_shards shards list ShardMetadata dim int - Optional tuple int int begin end index_in_shards Begin end inclusive intervals = s shard_offsets dim s shard_offsets dim + s shard_sizes dim - i i s enumerate shards intervals sort i range len shards - intervals i = intervals i + intervals i intervals i + None validate_non_overlapping_shards_metadata shards list ShardMetadata Ensures none shards overlap each other Args shards List ShardMetadata List ` ShardMetadata ` objects representing each shard Raises ` ` ValueError ` ` there s overlap any two shards shards len shards == sharded_dims list int = dim range len shards shard_offsets i range len shards shards i shard_offsets dim = shards shard_offsets dim shards i shard_sizes dim = shards shard_sizes dim sharded_dims append dim break pair Optional tuple int int = None len sharded_dims == shard all zeros we should consider pass all_zeros bool = all strictly limited all offsets pass could loose later shard shard_offsets == len shards shard_offsets math prod shard shard_sizes == one dimension shard shards all_zeros All shards same all dims partitioned Choose any pair = len sharded_dims == Shards partitioned over only one dimension Overlap can found using O nlogn overlapping interval algorithm pair = _find_ d_overlapping_shards shards sharded_dims Shards partitioned over more than one dimension Fall back pair-wise check Even though O nlogn algorithms line sweep exist D overlap implementation trivial may justify time saving most cases pair = _find_nd_overlapping_shards shards sharded_dims pair raise ValueError f Shards shards pair shards pair overlap check_tensor shards_metadata tensor_dims - None Checks shards_metadata compatible provided tensor dims Args shards_metadata List ShardMetadata List ` ShardMetadata ` objects representing each shard tensor tensor_dims Sequence int Dimensions tensor verify Raises ` ` ValueError ` ` compatible If tensor s volume matches total volume all shards all shard boundaries within tensor dims we have compatible sharding spec tensor Note we have already verified we don t have overlapping shards tensor_rank = len tensor_dims shards_rank = len shards_metadata shard_offsets tensor_rank = shards_rank raise ValueError f Rank tensor tensor_rank shards rank shards_rank total_shard_volume = shard shards_metadata shard_volume = i shard_length enumerate shard shard_sizes shard_volume = shard_length shard shard_offsets i + shard shard_sizes i tensor_dims i raise ValueError f Shard offset shard shard_offsets i length f shard shard_sizes i exceeds tensor dim tensor_dims i shard shard total_shard_volume += shard_volume tensor_volume = size tensor_dims tensor_volume = size total_shard_volume = tensor_volume TODO Can we improve error message point out gaps raise ValueError f Total volume shards total_shard_volume f does match tensor volume tensor_volume other words f all individual shards do cover entire tensor get_split_size dim_size chunks Computes split size inline ` ` torch chunk ` ` Args dim_size int Size dimension being chunked chunks int Number chunks create ` ` dim_size ` ` Returns An int indicating split size use dim_size + chunks - chunks get_chunked_dim_size dim_size split_size idx Computes dim size chunk provided ` ` idx ` ` given ` ` dim_size ` ` ` ` split_size ` ` Args dim_size int Size dimension being chunked split_size int The chunk size each chunk ` ` dim_size ` ` idx int The index chunk whose dim size being requested Returns An int indicating dim size chunk max min dim_size split_size idx + - split_size idx get_chunk_sharding_params sharding_dim_size world_size spec rank Generate start pos offset length current rank chunk sharding Args sharding_dim_size int The dimension length which we shard world_size int number ranks spec ` torch distributed _shard sharding_spec ChunkShardingSpec ` sharding spec rank int cuda process Returns start_pos int start position sharded tensor given rank chunk_size int chunk size sharded tensor given rank split_size = get_split_size sharding_dim_size world_size current_offsets = start_pos = current_offsets idx placement enumerate spec placements chunk_size = get_chunked_dim_size sharding_dim_size split_size idx rank == placement rank start_pos = current_offsets break current_offsets += chunk_size start_pos chunk_size type ignore possibly-undefined