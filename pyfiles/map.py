mypy allow-untyped-defs functools collections abc Callable typing Union typing_extensions TypeVarTuple torch torch utils _pytree pytree torch _C DispatchKey torch _dispatch python suspend_functionalization torch _higher_order_ops utils _maybe_run_with_interpreter reenter_make_fx torch _ops HigherOrderOperator torch _subclasses fake_tensor FakeTensorMode torch _subclasses functional_tensor disable_functional_mode torch fx experimental proxy_tensor disable_proxy_modes_tracing ProxyTorchDispatchMode track_tensor_tree utils _from_fun _stack_pytree _unstack_pytree create_bw_fn fill_none_with_masks filter_with_masks materialize_as_graph save_tensors_and_symints_for_backward saved_tensors_and_symints split_into_chunks MapImpl HigherOrderOperator __init__ super __init__ map_impl __call__ args kwargs super __call__ args kwargs map_impl = MapImpl map f Callable pytree PyTree tuple pytree PyTree pytree PyTree xs Union pytree PyTree torch Tensor args TypeVarTuple r Performs map f xs Intuitively you can think semantic being out = idx len xs size xs_sliced = xs select idx out append f xs_sliced args torch stack out warning ` torch _higher_order_ops map ` prototype feature PyTorch It currently does support autograd you may run into miscompiles Read more about feature classification https pytorch org blog pytorch-feature-classification-changes #prototype Args f Callable callable takes input x could either single Tensor nested dict list tensors some additional inputs xs inputs re mapped over We ll iterate over first dim each x perform f each slice args additional arguments provided each step f They could also omitted map able automatically figure out read dependency Return stacked output each step f Example f xs xs + xs + const + const xs = torch randn torch randn const = torch randn const = torch randn returns tensor shape torch _higher_order_ops map f xs flat_xs xs_spec = pytree tree_flatten xs flat_args args_spec = pytree tree_flatten args all isinstance t torch Tensor t flat_xs raise RuntimeError f Mapped xs can only consist tensors Got xs flat_xs shapes = xs shape xs flat_xs leading_dim_size = shapes leading_dim_size == raise RuntimeError Leading dimensions mapped xs cannot any cur_shape = leading_dim_size cur_shape shapes raise RuntimeError f Leading dimensions mapped xs must consistent Got shapes shapes run_flattened_map f flat_xs flat_args wrapped_fn flat_args f xs_tree_spec args_tree_spec num_xs xs = pytree tree_unflatten flat_args num_xs xs_tree_spec args = pytree tree_unflatten flat_args num_xs args_tree_spec f xs args inner_f = functools partial wrapped_fn f=f xs_tree_spec=xs_spec args_tree_spec=args_spec num_xs=len flat_xs map_impl inner_f flat_xs flat_args torch _higher_order_ops utils _maybe_compile_and_run_fn _maybe_compile_and_run_fn run_flattened_map f flat_xs flat_args MapAutogradOp torch autograd Function staticmethod pyrefly ignore bad-override forward ctx f num_mapped_args flat_args ctx _f = f ctx _num_mapped_args = num_mapped_args ctx _num_pos_args = len flat_args - num_mapped_args We snapshot dispatch keys forward materializing bw_graph backward ctx _fw_include_key_set = torch _C _dispatch_tls_local_include_set ctx _fw_exclude_key_set = torch _C _dispatch_tls_local_exclude_set save_tensors_and_symints_for_backward ctx flat_args torch _C _AutoDispatchBelowAutograd map_impl f flat_args num_mapped_args flat_args num_mapped_args staticmethod backward ctx flat_grads fw_args = saved_tensors_and_symints ctx num_mapped_args = ctx _num_mapped_args num_pos_args = ctx _num_pos_args num_grads = len flat_grads fw_mapped_args pos_args = split_into_chunks fw_args num_mapped_args num_pos_args bw_f = create_bw_fn ctx _f fw_args grads_tensor_masks = Create wrapper around thefor bw_f bw_f_wrapper args nonlocal grads_tensor_masks Dissect args re-order them ` ` ctx _bw_f ` ` args provided wrapper composed fw_mapped_args flat_grads pos_args The content ` ` bw_f_tangents ` ` upstream gradients i e flat_grads The content ` ` bw_f_primals ` ` fw_args i e fw_mapped_args pos_args The bw_f requires bw_f_primals bw_f_tangents fw_m_args bw_f_tangents pos_args = split_into_chunks args num_mapped_args num_grads num_pos_args bw_f_primals = fw_m_args pos_args gradients = bw_f bw_f_primals bw_f_tangents grads_tensor_masks = True isinstance out torch Tensor out out gradients filter_with_masks gradients grads_tensor_masks construct_args_single_step_bw unwrapped_mapped_xs = pytree tree_map _from_fun fw_mapped_args example_xs = _unstack_pytree unwrapped_mapped_xs unwrapped_grads = pytree tree_map _from_fun flat_grads example_grads = _unstack_pytree unwrapped_grads example_pos_args = _from_fun arg isinstance arg torch Tensor arg arg pos_args example_xs example_grads example_pos_args suspend_functionalization disable_functional_mode disable_proxy_modes_tracing args_single_step_bw = construct_args_single_step_bw TODO we need materialize bw graphs because dynamo unable trace through joint function when torch compile torch autograd grad fn_bw_gm = materialize_as_graph bw_f_wrapper args_single_step_bw ctx _fw_include_key_set ctx _fw_exclude_key_set force_enable_grad=True grads = map_impl fn_bw_gm fw_mapped_args + flat_grads pos_args None None fill_none_with_masks grads grads_tensor_masks trace_map proxy_mode func_overload f xs pos_args disable_proxy_modes_tracing example_input = _unstack_pytree xs body_graph = f body_graph = reenter_make_fx body_graph example_input pos_args next_name = proxy_mode tracer get_fresh_qualname body_graph_ proxy_mode tracer root register_module next_name body_graph fake_outs = map_impl body_graph xs pos_args node_args = body_graph list xs list pos_args proxy_args = pytree tree_map proxy_mode tracer unwrap_proxy node_args out_proxy = proxy_mode tracer create_proxy call_function func_overload proxy_args name= map_impl track_tensor_tree fake_outs out_proxy constant=None tracer=proxy_mode tracer map_impl py_impl DispatchKey CompositeExplicitAutograd map_dense f xs pos_args pytrees = f inp pos_args inp _unstack_pytree xs _stack_pytree pytrees map_impl py_autograd_impl map_autograd f xs pos_args num_mapped_args = len xs flat_out = MapAutogradOp apply f num_mapped_args xs pos_args flat_out map_impl py_impl ProxyTorchDispatchMode map_proxy_torch_dispatch_mode mode f xs args trace_map mode map_impl f xs args map_impl py_impl FakeTensorMode map_fake_tensor_mode mode f xs args mode map_dense f xs args map_impl py_functionalize_impl map_functionalize ctx f xs pos_args torch _higher_order_ops utils _check_alias_and_mutation unwrapped_xs = ctx unwrap_tensors xs unwrapped_args = ctx unwrap_tensors pos_args wrapped_fn = ctx functionalize _maybe_run_with_interpreter f ctx redispatch_to_next example_inputs = _unstack_pytree unwrapped_xs unwrapped_args pre_dispatch = hasattr ctx mode ctx mode pre_dispatch _check_alias_and_mutation f example_inputs map pre_dispatch map_return = map_impl wrapped_fn unwrapped_xs unwrapped_args ctx wrap_tensors map_return _fake_map f x args functorch experimental control_flow _stack_pytree _unstack_pytree x_pytrees = _unstack_pytree x zs = xp x_pytrees zs append f xp args _stack_pytree zs