Owner s module autograd contextlib warnings numpy np torch torch library _scoped_library torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests TestCase contextlib contextmanager autograd_fallback_mode mode prev = torch _C _get_autograd_fallback_mode try torch _C _set_autograd_fallback_mode mode yield finally torch _C _set_autograd_fallback_mode prev TestAutogradFallback TestCase test_ns = _test_autograd_fallback setUp super setUp libraries = tearDown hasattr torch ops test_ns delattr torch ops test_ns lib libraries lib _destroy del libraries get_op name getattr getattr torch ops test_ns name default get_lib result = torch library Library test_ns FRAGMENT noqa TOR libraries append result result parametrize mode nothing warn test_no_grad mode autograd_fallback_mode mode lib = get_lib lib define foo Tensor Tensor b int c - Tensor lib impl foo lambda b c + b + c CPU op = get_op foo warnings catch_warnings warnings simplefilter error torch no_grad = torch randn requires_grad=True b = torch randn requires_grad=True out = op b assertFalse out requires_grad warnings catch_warnings warnings simplefilter error = torch randn b = torch randn out = op b assertFalse out requires_grad parametrize mode nothing warn test_no_autograd_kernel mode autograd_fallback_mode mode lib = get_lib lib define foo Tensor Tensor b int c - Tensor op = get_op foo foo_impl b c result = detach numpy + b detach numpy + c torch tensor result lib impl foo foo_impl CPU Some inputs requiring grad = torch randn requires_grad=False b = torch randn requires_grad=True out = op b sum _check_ctx mode mode_nothing_raises=True out backward assertIsNone b grad _check_ctx mode mode_nothing_raises=False mode == warn assertWarnsRegex UserWarning autograd kernel registered assert mode == nothing mode_nothing_raises assertRaisesRegex RuntimeError does require grad contextlib nullcontext parametrize mode nothing warn test_no_autograd_kernel_inplace mode autograd_fallback_mode mode input modified in-place gets returned output lib = get_lib lib define foo Tensor Tensor b y - Tensor Tensor b op = get_op foo foo_impl x y torch no_grad x sin_ y cos_ x y lib impl foo foo_impl CPU x = torch randn requires_grad=True w = x clone v = x clone y = w y = v z z = op y y tensor w v z z y y _check_ctx mode tensor sum backward retain_graph=True no outputs we don t do anything Maybe we should future This common failure mode lib define bar Tensor - op = get_op bar bar_impl x torch no_grad x sin_ lib impl bar bar_impl CPU warnings catch_warnings warnings simplefilter error x = torch randn requires_grad=True y = x clone op y y backward assertEqual x grad torch ones_like x parametrize mode nothing warn test_cpu_return_self mode autograd_fallback_mode mode To clear none these situations OK will lead other problems down line We re testing them because fairly common actually do these things _scoped_library test_ns FRAGMENT lib lib define foo Tensor - Tensor lib impl foo lambda x x CPU op = get_op foo x = torch randn requires_grad=True y = op x sum _check_ctx mode y backward assertEqual x grad torch ones_like x lib define bar Tensor - Tensor lib impl bar lambda x x CPU op = get_op bar x = torch randn requires_grad=True y = op x sum _check_ctx mode y backward assertEqual x grad torch ones_like x parametrize mode nothing warn test_composite_registered_to_cpu mode autograd_fallback_mode mode _scoped_library test_ns FRAGMENT lib lib define foo Tensor - Tensor lib impl foo lambda x x sin sum CPU op = get_op foo x = torch randn requires_grad=True y = op x _check_ctx mode y backward assertEqual x grad x cos parametrize mode nothing warn test_autograd_function_registered_to_cpu mode autograd_fallback_mode mode _scoped_library test_ns FRAGMENT lib lib define foo Tensor - Tensor NumpySin torch autograd Function staticmethod forward ctx x ctx save_for_backward x torch tensor np sin x cpu numpy staticmethod backward ctx gx x = ctx saved_tensors gx x cos lib impl foo NumpySin apply CPU op = get_op foo x = torch randn requires_grad=True y = op x sum _check_ctx mode y backward assertEqual x grad x cos parametrize mode nothing warn test_inplace_autograd_function_registered_to_cpu mode autograd_fallback_mode mode _scoped_library test_ns FRAGMENT lib lib define foo Tensor - Tensor NumpySin_ torch autograd Function staticmethod forward ctx x ctx save_for_backward x clone x_np = x detach numpy np sin x_np out=x_np ctx mark_dirty x x staticmethod backward ctx gx x = ctx saved_tensors gx x cos lib impl foo NumpySin_ apply CPU op = get_op foo x = torch randn requires_grad=True z = x clone w = z y = op w expected = torch zeros_like x expected = x cos _check_ctx mode gx = torch autograd grad y x torch ones_like y retain_graph=True assertEqual gx expected expected = torch ones_like x expected = x cos _check_ctx mode gx = torch autograd grad z x torch ones_like z assertEqual gx expected parametrize mode nothing warn test_inplace_on_tensor_that_does_not_require_grad mode We don t do anything special we don t rebase history See NOTE autograd fallback in-place operations why autograd_fallback_mode mode _scoped_library test_ns FRAGMENT lib Correct usage lib define foo Tensor Tensor other - Tensor foo_impl x y x_d = x detach y = y detach x_d add_ y x lib impl foo foo_impl CPU foo = get_op foo Incorrect usage user doesn t tensor as-is lib define bar Tensor Tensor other - Tensor bar_impl x y x_d = x detach y = y detach x_d add_ y x_d clone lib impl bar bar_impl CPU bar = get_op bar User mutated input tensor didn t lib define baz Tensor Tensor other - baz_impl x y x_d = x detach y = y detach x_d add_ y lib impl baz baz_impl CPU baz = get_op baz Test in-place non-view op foo bar baz x = torch randn y = torch randn requires_grad=True assertRaisesRegex RuntimeError does require grad z = x clone op z y torch autograd grad z y torch ones_like z allow_unused=True Test in-place view op foo bar baz x = torch randn y = torch randn requires_grad=True assertRaisesRegex RuntimeError does require grad z = x op z y torch autograd grad z x torch ones_like z allow_unused=True parametrize mode nothing warn test_post_autograd_returns_leaf mode autograd_fallback_mode mode lib = get_lib lib define foo Tensor - Tensor Tensor op = get_op foo lib impl foo lambda clone detach clone requires_grad_ CPU x = torch randn requires_grad=True _ z = op x _check_ctx mode z sum backward parametrize mode nothing warn test_undefined_inputs_outputs mode autograd_fallback_mode mode lib = get_lib lib define foo Tensor Tensor b - Tensor Tensor op = get_op foo foo_impl b None b clone lib impl foo foo_impl CPU x = torch randn requires_grad=True NB PyTorch dispatcher treats None undefined Tensor _ z = op None x _check_ctx mode z sum backward parametrize mode nothing warn test_undefined_grads mode autograd_fallback_mode mode lib = get_lib lib define foo Tensor Tensor b - Tensor Tensor op = get_op foo foo_impl b sin b cos lib impl foo foo_impl CPU x = torch randn requires_grad=True y = torch randn w z = op x y w = torch _C _functions UndefinedGrad w z = torch _C _functions UndefinedGrad z _check_ctx mode z + w sum backward parametrize mode nothing warn test_base_does_not_require_grad mode autograd_fallback_mode mode lib = get_lib lib define foo Tensor x - Tensor op = get_op foo foo_impl torch no_grad zero_ lib impl foo foo_impl CPU x = torch randn y = x y requires_grad_ w = y assertTrue w _base x Hook should registered w w _base op w _check_ctx mode w sum backward parametrize mode nothing warn test_post_autograd_returns_mix_of_requires_grad_tensors mode autograd_fallback_mode mode lib = get_lib lib define foo Tensor Tensor b - Tensor Tensor Tensor op = get_op foo foo_impl b torch no_grad x = clone z = b clone y = b x y z lib impl foo foo_impl CPU = torch randn requires_grad=True b = torch randn requires_grad=True x y z = op b _check_ctx mode mode_nothing_raises=True torch autograd grad x b torch ones_like x allow_unused=True retain_graph=True _check_ctx mode mode_nothing_raises=False torch autograd grad y b torch ones_like y allow_unused=True retain_graph=True _check_ctx mode mode_nothing_raises=True torch autograd grad z b torch ones_like z allow_unused=True retain_graph=True parametrize mode nothing warn test_supports_tensor_lists mode autograd_fallback_mode mode lib = get_lib lib define foo Tensor - Tensor op = get_op foo foo_impl x y z = torch no_grad x + y + z x y z lib impl foo foo_impl CPU x = torch randn requires_grad=True y = torch randn requires_grad=True z = torch randn requires_grad=True b = op x y z _check_ctx mode mode_nothing_raises=True torch autograd grad x y z torch ones_like allow_unused=True retain_graph=True _check_ctx mode mode_nothing_raises=True torch autograd grad b x y z torch ones_like b allow_unused=True retain_graph=True instantiate_parametrized_tests TestAutogradFallback __name__ == __main__ run_tests