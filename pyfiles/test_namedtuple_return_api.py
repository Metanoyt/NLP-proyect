Owner s module unknown os re yaml textwrap torch torch testing _internal common_utils TestCase run_tests collections namedtuple path = os path dirname os path realpath __file__ aten_native_yaml = os path join path aten src ATen native native_functions yaml all_operators_with_namedtuple_return = max min aminmax median nanmedian mode kthvalue svd qr geqrf slogdet sort topk linalg_inv_ex triangular_solve cummax cummin linalg_eigh _linalg_eigh _unpack_dual linalg_qr linalg_svd _linalg_svd linalg_slogdet _linalg_slogdet fake_quantize_per_tensor_affine_cachemask fake_quantize_per_channel_affine_cachemask linalg_lstsq linalg_eig linalg_cholesky_ex frexp lu_unpack histogram histogramdd _fake_quantize_per_tensor_affine_cachemask_tensor_qparams _fused_moving_avg_obs_fq_helper linalg_lu_factor linalg_lu_factor_ex linalg_lu _linalg_det _lu_with_info linalg_ldl_factor_ex linalg_ldl_factor linalg_solve_ex _linalg_solve_ex all_operators_with_namedtuple_return_skip_list = _scaled_dot_product_flash_attention _scaled_dot_product_fused_attention_overrideable _scaled_dot_product_flash_attention_for_cpu _scaled_dot_product_efficient_attention _scaled_dot_product_cudnn_attention TestNamedTupleAPI TestCase test_import_return_types torch return_types noqa F exec torch return_types test_native_functions_yaml operators_found = set regex = re compile r ^ \w \ &#124; \ open aten_native_yaml file f yaml safe_load file read f = f func ret = f split - strip name = regex findall f name all_operators_with_namedtuple_return operators_found add name continue _backward name name endswith _forward continue ret startswith continue ret == continue name all_operators_with_namedtuple_return_skip_list continue ret = ret - split r ret r = r strip assertEqual len r split only allowlisted operators allowed have named type got + name assertEqual all_operators_with_namedtuple_return operators_found textwrap dedent Some elements ` all_operators_with_namedtuple_return ` test_namedtuple_return_api py could found Do you forget update test_namedtuple_return_api py after renaming some operator test_namedtuple_return = torch randn per_channel_scale = torch randn per_channel_zp = torch zeros dtype=torch int op = namedtuple op operators input names hasout operators = op operators= max min median nanmedian mode sort topk cummax cummin input= names= values indices hasout=True op operators= kthvalue input= names= values indices hasout=True op operators= svd input= names= U S V hasout=True op operators= linalg_svd _linalg_svd input= names= U S Vh hasout=True op operators= slogdet linalg_slogdet input= names= sign logabsdet hasout=True op operators= _linalg_slogdet input= names= sign logabsdet LU pivots hasout=True op operators= qr linalg_qr input= names= Q R hasout=True op operators= geqrf input= names= tau hasout=True op operators= triangular_solve input= names= solution cloned_coefficient hasout=True op operators= linalg_eig input= names= eigenvalues eigenvectors hasout=True op operators= linalg_eigh input= L names= eigenvalues eigenvectors hasout=True op operators= _linalg_eigh input= L names= eigenvalues eigenvectors hasout=True op operators= linalg_cholesky_ex input= names= L info hasout=True op operators= linalg_inv_ex input= names= inverse info hasout=True op operators= linalg_solve_ex input= names= result info hasout=True op operators= _linalg_solve_ex input= names= result LU pivots info hasout=True op operators= linalg_lu_factor input= names= LU pivots hasout=True op operators= linalg_lu_factor_ex input= names= LU pivots info hasout=True op operators= linalg_ldl_factor input= names= LD pivots hasout=True op operators= linalg_ldl_factor_ex input= names= LD pivots info hasout=True op operators= linalg_lu input= names= P L U hasout=True op operators= fake_quantize_per_tensor_affine_cachemask input= names= output mask hasout=False op operators= fake_quantize_per_channel_affine_cachemask input= per_channel_scale per_channel_zp names= output mask hasout=False op operators= _unpack_dual input= names= primal tangent hasout=False op operators= linalg_lstsq input= names= solution residuals rank singular_values hasout=False op operators= frexp input= names= mantissa exponent hasout=True op operators= lu_unpack input= torch tensor dtype=torch int True True names= P L U hasout=True op operators= histogram input= names= hist bin_edges hasout=True op operators= histogramdd input= names= hist bin_edges hasout=False op operators= _fake_quantize_per_tensor_affine_cachemask_tensor_qparams input= torch tensor torch tensor dtype=torch int torch tensor names= output mask hasout=False op operators= _fused_moving_avg_obs_fq_helper input= torch tensor torch tensor torch tensor torch tensor torch tensor torch tensor names= output mask hasout=False op operators= _linalg_det input= names= result LU pivots hasout=True op operators= aminmax input= names= min max hasout=True op operators= _lu_with_info input= names= LU pivots info hasout=False get_func f Return either torch f torch linalg f where f string mod = torch f startswith linalg_ mod = torch linalg f = f f startswith _ mod = torch _VF getattr mod f None check_namedtuple tup names Check namedtuple tup has given names i name enumerate names assertIs getattr tup name tup i check_torch_return_type f names Check return_type exists torch return_types they can constructed return_type = getattr torch return_types f inputs = torch randn _ names assertEqual type return_type inputs return_type op operators f op operators check namedtuple returned calling torch f func = get_func f func ret = func op input check_namedtuple ret op names check_torch_return_type f op names check out= variant exists func op hasout ret = func op input out=tuple ret check_namedtuple ret op names check_torch_return_type f + _out op names check Tensor f method exists meth = getattr f None meth ret = meth op input check_namedtuple ret op names all_covered_operators = x y operators x y operators assertEqual all_operators_with_namedtuple_return all_covered_operators textwrap dedent The set covered operators does match ` all_operators_with_namedtuple_return ` test_namedtuple_return_api py Do you forget add test operator __name__ == __main__ run_tests