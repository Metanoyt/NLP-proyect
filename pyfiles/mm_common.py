mypy allow-untyped-defs logging collections abc Sequence typing Any torch torch _inductor select_algorithm realize_inputs SymbolicGridFn torch _inductor utils get_current_backend sympy_product torch _inductor virtualized V torch fx experimental symbolic_shapes has_free_unbacked_symbols config codegen wrapper PythonWrapperCodegen ir _IntLike Layout TensorBox log = logging getLogger __name__ SymbolicGridFn mm_grid m n meta cdiv The CUDA grid size matmul triton templates cdiv m meta BLOCK_M cdiv n meta BLOCK_N SymbolicGridFn persistent_mm_grid M int N int meta dict str Any cdiv min Defines grid persistent kernels min meta NUM_SMS cdiv M meta BLOCK_M cdiv N meta BLOCK_N SymbolicGridFn persistent_grouped_mm_grid args meta = args - meta NUM_SMS acc_type dtype dtype torch float torch bfloat tl float f tl dtype replace torch mm_args mat mat others layout=None out_dtype=None use_ x _dim=False mat _transposed=False Common arg processing mm bmm addmm etc mat mat = realize_inputs mat mat b m k = mat get_size mat _transposed b n k = mat get_size b k n = mat get_size b = V graph sizevars check_equals_and_simplify b b zip b b use_ x _dim k = k k = V graph sizevars check_equals_and_simplify k k layout None torch _inductor ir FixedLayout out_dtype None out_dtype = mat get_dtype layout = FixedLayout mat get_device out_dtype b m n assert out_dtype None out_dtype ignored layout specified lowering expand others = realize_inputs expand x layout size x others m n k layout mat mat others addmm_epilogue dtype alpha beta epilogue acc bias alpha = acc = V ops mul acc V ops constant alpha dtype beta = bias = V ops mul bias V ops constant beta dtype V ops add acc bias epilogue scale_mm_epilogue Create epilogue function applies scaling matrix multiplication result using given scale factors Args dtype The data type output scale_a Scale factor matrix A scale_b Scale factor matrix B Returns Epilogue function takes accumulator applies scaling epilogue acc inv_a_scale inv_b_scale bias=None The epilogue function receives accumulator result mat mat applies scaling factors In original scaled_mm we use inverse scales so we multiply them mul_scales = V ops mul inv_a_scale inv_b_scale mul_acc = V ops mul acc mul_scales bias None V ops add mul_acc bias mul_acc epilogue use_native_matmul mat mat config triton native_matmul False If tma matmul don t do native matmul config triton enable_persistent_tma_matmul torch utils _triton has_triton_tma_device raise AssertionError native matmul doesn t support tma codegen yet Currently only enable native matmul default indexing TODO support block ptr config triton use_block_ptr raise AssertionError native matmul doesn t support block_ptr codegen yet Currently only enable native matmul triton GPU device_type = mat get_device type device_type cuda xpu get_current_backend device_type == triton False Currently tl dot only supports following dtypes triton_supported_dtype = torch int torch uint torch float torch bfloat torch float mat dtype triton_supported_dtype False mat dtype triton_supported_dtype False M K K N m k n = mat get_size - mat get_size - mat get_size - If shape has unbacked symbols don t do native matmul This related behavior statically_known_multiple_of unbacked symints Since statically_known_multiple_of just returns False unbacked symbols due expensive cost codegen fails when there unbacked symbol In particular fails _split_iteration_ranges codegen simd py See https github com pytorch pytorch pull any map has_free_unbacked_symbols m k n False Consider shape m k n TODO support when size = V graph sizevars statically_known_leq m V graph sizevars statically_known_leq k V graph sizevars statically_known_leq n False True _is_static_problem layout Layout - tuple bool bool Check input tensors output layout have static shapes non-zero sizes Args layout Output layout object size attribute Returns Tuple bool bool is_static is_nonzero is_static True all shapes statically known is_nonzero True all dimensions non-zero static_shape = True static_size = PythonWrapperCodegen statically_known_list_of_ints_or_none layout size static_size None nonzero = True s layout size sz = PythonWrapperCodegen statically_known_int_or_none s sz None sz == nonzero = False break False nonzero numel = dim static_size numel = dim nonzero = numel static_shape nonzero check_supported_striding mat_a TensorBox mat_b TensorBox - None is_row_major stride Sequence _IntLike - bool stride - == is_col_major stride Sequence _IntLike - bool stride - == has_zero_dim size Sequence _IntLike - bool bool size == size == Check mat_a stride requirements torch _check is_row_major mat_a get_stride has_zero_dim mat_a get_size lambda f mat_a must row_major got stride mat_a get_stride Check mat_b stride requirements torch _check is_col_major mat_b get_stride has_zero_dim mat_b get_size lambda f mat_b must col_major got stride mat_b get_stride is_batch_stride_largest_or_zero mat mat layout - bool Checking batch stride largest stride sizes = mat get_size mat get_size layout size strides = mat get_stride mat get_stride layout stride size stride zip sizes strides assert len size == len stride == Expect D tensors stride = stride = sympy_product size False True