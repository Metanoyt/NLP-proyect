__future__ annotations itertools typing TYPE_CHECKING torchgen api cpp torchgen api types DispatcherSignature torchgen code_template CodeTemplate torchgen context with_native_function torchgen model Argument NativeFunction SchemaKind TensorOptionsArguments torchgen utils FileManager TYPE_CHECKING collections abc Sequence Note Manual Backend kernels For these ops we want manually register dispatch key Backend skip codegen-ed registration all keys before Backend For codegen means - op set below must match ops manual_kernel_registration=True native_functions yaml where we skip codegen backend kernels - all ops below part MANUAL_AUTOGRAD skip codegen Autograd kernel registration - all ops below part MANUAL_TRACER skip codegen Tracer kernel registration Note we still register dispatch key Profiler these ops keeping untouched now You can find manual registration torch csrc autograd VariableTypeManual cpp MANUAL_BACKEND = options data set_data is_leaf output_nr _version retain_grad _backward requires_grad_ For these ops we want skip codegen-ed registration both Autograd Tracer keys You can find manual registration torch csrc autograd VariableTypeManual cpp MANUAL_AUTOGRAD_AND_TRACER = resize_ resize_as_ detach detach_ copy_ _fw_primal _make_dual Currently MANUAL_AUTOGRAD MANUAL_TRACER share same set ops union MANUAL_BACKEND MANUAL_AUTOGRAD_AND_TRACER You can find manual registration torch csrc autograd VariableTypeManual cpp MANUAL_AUTOGRAD = MANUAL_TRACER = MANUAL_BACKEND &#124; MANUAL_AUTOGRAD_AND_TRACER These functions we don t want record tracing because we always want trace their constituent parts This temporary hack lieue proper scopes where subsequent compilation passes can ask unfolding demand Only concrete ATen methods can disabled way will have NO EFFECT otherwise DONT_RECORD_TRACE = convolution conv d conv d conv d conv_transpose d conv_transpose d conv_transpose d lstm_cell gru_cell rnn_tanh_cell rnn_relu_cell FIXME figure out better way when we support sparse tensors jit _coalesced should_trace f NativeFunction - bool Operations involving Storage Type traceable moment any str arg type Storage Type arg f func schema_order_arguments False We can t trace functions which don t have any Tensor TensorList returns any r type is_tensor_like r f func returns False f func name name base DONT_RECORD_TRACE SELECT = CodeTemplate \ $ cond $ true $ false OP_NAME = CodeTemplate \ op_name = c Symbol fromQualString aten $ trace_name These functions have their names recorded under trace renamed RENAME_TRACE = zero zeros_like replacing aten zero_ aten zeros_like fill full_like replacing aten fill_ aten full_like format_trace_op_name f NativeFunction - str TODO byte-for-byte compatible old codegen behavior - should clean up f func kind SchemaKind functional SchemaKind out f func name name dunder_method special case _out functions in-place out-of-place ops overloaded same name JIT trace_name = str f func name name trace_name = RENAME_TRACE get trace_name trace_name OP_NAME substitute trace_name=trace_name otherwise in-place op we need emit both in- out-of-place versions outplace_trace_name = f func name name base inplace_trace_name = cpp name f func outplace_trace_name = RENAME_TRACE get outplace_trace_name outplace_trace_name inplace_trace_name = RENAME_TRACE get inplace_trace_name inplace_trace_name SELECT substitute cond= tracer_state- force_outplace true=OP_NAME substitute trace_name=outplace_trace_name false=OP_NAME substitute trace_name=inplace_trace_name ADD_TRACE_INPUT = CodeTemplate jit tracer addInputs node $ name $ input format_trace_inputs f NativeFunction - str dispatch_trace_input arg Argument &#124; TensorOptionsArguments - Sequence str isinstance arg TensorOptionsArguments name = options ADD_TRACE_INPUT substitute name=name input= c optTypeMetaToScalarType options dtype_opt ADD_TRACE_INPUT substitute name=name input= options layout ADD_TRACE_INPUT substitute name=name input= options device ADD_TRACE_INPUT substitute name=name input= options pinned_memory name = arg name str arg type == Tensor f jit tracer addInputs node name name ADD_TRACE_INPUT substitute name=name input=name args list Argument &#124; TensorOptionsArguments = list f func schema_order_arguments f func is_out_fn _out functions take result separate argument we don t want trace argument directly Instead we trace its TensorOptions So first we need remove out argument list arguments trace num_out_args = len f func arguments out args = args -num_out_args trace_inputs = itertools chain from_iterable dispatch_trace_input arg arg args f func is_out_fn _out functions handle result argument differently inplace outplace For inplace just add input end confirm JIT schema inplace = ADD_TRACE_INPUT substitute name=f func arguments out i name input=f func arguments out i name pyrefly ignore unbound-name i range num_out_args outplace do nothing except function factory Factories bit special because their out-of-place overloads take extra TensorOptions argument which missing _out function has_tensor_return = any r type is_tensor_like r f func returns has_tensor_input_arg = any type is_tensor_like f func arguments flat_non_out is_factory_method = f category_override == factory has_tensor_return has_tensor_input_arg HACK preserve old codegen behavior - old codegen set ` is_factory_method ` flag whole family ops same basename any them factory method For most cases whole family ops indeed all factory method - normal only exception So we handle specially here avoid cloning old logic f func name name base == normal is_factory_method = True is_factory_method outplace = ADD_TRACE_INPUT substitute name= out input= c optTypeMetaToScalarType out options dtype_opt ADD_TRACE_INPUT substitute name= out input= out options layout ADD_TRACE_INPUT substitute name= out input= out options device ADD_TRACE_INPUT substitute name= out input= out options pinned_memory outplace = trace_inputs = itertools chain trace_inputs SELECT substitute cond= tracer_state- force_outplace true= \n join outplace false= \n join inplace \n join trace_inputs ` torch jit trace ` have undocumented keyword argument ` _force_outplace ` which force jit replace functions outplace variants example ` aten add_ ` becomes ` aten add ` This replacement implemented in-place minimum modifications arguments stack assumes outplace call has same arguments inplace version However there no such substitutions available ` aten fill_ ` ` aten zero_ ` operators we never implemented ` aten fill ` ` aten zero ` So jit tracing hack replacing ` aten zero_ ` ` aten zeros_like ` replacing ` aten fill_ ` ` aten full_like ` But they potentially can have different arguments we also have hack into stack add missing ones A possible alternative would - Add ` aten fill ` ` aten zero ` - Or keep ` aten zeros_like ` arguments aligned ` aten zero_ ` arguments inside ` native_functions yaml ` RENAME_TRACE_ADD_ARGS = fill \ jit tracer addInputs node options std optional ScalarType jit tracer addInputs node options layout_or_default std nullopt jit tracer addInputs node options device_or_default std nullopt jit tracer addInputs node options pinned_memory_or_default std nullopt std optional MemoryFormat memory_format = c MemoryFormat Preserve jit tracer addInputs node memory_format memory_format zero \ jit tracer addInputs node options std optional ScalarType jit tracer addInputs node options layout_or_default std nullopt jit tracer addInputs node options device_or_default std nullopt jit tracer addInputs node options pinned_memory_or_default std nullopt std optional MemoryFormat memory_format = c MemoryFormat Preserve jit tracer addInputs node memory_format memory_format INPLACE_GUARD = CodeTemplate \ jit tracer ensureUniqueIfOutOfPlaced $ name $ mutable_input PRE_RECORD_TRACE = CodeTemplate \ torch jit Node node = nullptr std shared_ptr jit tracer TracingState tracer_state jit tracer isTracing tracer_state = jit tracer getTracingState Symbol op_name $ set_op_name node = tracer_state- createNode op_name num_outputs= jit tracer recordSourceLocation node $ add_trace_inputs tracer_state- insertNode node $ inplace_guard jit tracer setTracingState nullptr format_prerecord_trace f NativeFunction - str should_trace f TODO clean up old codegen behavior is_inplace = f func kind SchemaKind inplace SchemaKind out f func name name dunder_method add_args = RENAME_TRACE_ADD_ARGS get f func name name base is_inplace additional_inputs = SELECT substitute cond= tracer_state- force_outplace true=add_args false= add_args PRE_RECORD_TRACE substitute set_op_name=format_trace_op_name f add_trace_inputs=format_trace_inputs f + additional_inputs inplace_guard=INPLACE_GUARD substitute name=cpp name f func mutable_input=f func arguments out name f func arguments out is_inplace POST_RECORD_TRACE = CodeTemplate \ tracer_state jit tracer setTracingState std move tracer_state $ add_trace_outputs format_postrecord_trace f NativeFunction - str should_trace f For outplacing ops _out overloads require special handling move output argument value f func is_out_fn output_names_outplace = arg name arg f func arguments out output_names_inplace = cpp return_names f Code size optimization common case value same both variants output_names_outplace == output_names_inplace outputs = f jit tracer addOutput node n n output_names_outplace POST_RECORD_TRACE substitute add_trace_outputs=outputs selection = SELECT substitute cond= force_outplace true= \n join f jit tracer addOutput node n n output_names_outplace false= \n join f jit tracer addOutput node n n output_names_inplace POST_RECORD_TRACE substitute add_trace_outputs=selection output_names = cpp return_names f outputs = f jit tracer addOutput node n n output_names POST_RECORD_TRACE substitute add_trace_outputs=outputs tie_return_values f NativeFunction - str len f func returns == f auto f func returns name result names = cpp return_names f f auto join names get_return_value f NativeFunction - str names = cpp return_names f len f func returns == names f func kind == SchemaKind out f std forward_as_tuple join names moved = join f std move name name names f std make_tuple moved TRACE_DISPATCH = CodeTemplate \ $ assign_return_values _ops $ unambiguous_name redispatch $ unpacked_args emit_trace_body f NativeFunction - list str trace_body list str = trace_body append format_prerecord_trace f dispatcher_sig = DispatcherSignature from_schema f func dispatcher_exprs = dispatcher_sig exprs code-generated tracing kernels plumb recompute dispatch keys directly through kernel performance See Note Plumbing Keys Through The Dispatcher details dispatch_key_set = ks c DispatchKeySet c DispatchKeySet FULL_AFTER c DispatchKey Tracer redispatch_args = join dispatch_key_set + expr dispatcher_exprs assign_return_values = f tie_return_values f = f func kind SchemaKind functional SchemaKind mutable f func returns Note calls slow dispatching variants manual_cpp_binding ops We could probably work harder ensure fast variants called instead perf benefit would minimal trace_body append TRACE_DISPATCH substitute assign_return_values=assign_return_values unambiguous_name=f func name unambiguous_name unpacked_args=redispatch_args trace_body append format_postrecord_trace f f func returns trace_body append f get_return_value f trace_body METHOD_DEFINITION = CodeTemplate \ $ return_type $ type_wrapper_name $ formals $ type_definition_body type_wrapper_name f NativeFunction key str = Default - str f func name overload_name name = f cpp name f func _ f func name overload_name name = cpp name f func The key argument only used gen_variable_type where we need fns per autograd dispatch key In gen_trace_type gen_inplace_view_type where only one fn per native_fn must generated key argument should passed We do append key Default so generated functions before per-dispatch-key derivatives added retain same names key = Default name = name + f _ key name with_native_function method_definition f NativeFunction - str assert cpp name f func MANUAL_TRACER formals = join code-generated tracing kernels plumb recompute dispatch keys directly through kernel performance See Note Plumbing Keys Through The Dispatcher details c DispatchKeySet ks + f cpp argument_type binds= __placeholder__ symint=True cpp_type name f func schema_order_arguments METHOD_DEFINITION substitute return_type=cpp returns_type f func returns symint=True cpp_type type_wrapper_name=type_wrapper_name f formals=formals type_definition_body=emit_trace_body f WRAPPER_REGISTRATION = CodeTemplate \ m impl $ name TORCH_FN $ class_type $ type_wrapper_name with_native_function method_registration f NativeFunction - str assert cpp name f func MANUAL_TRACER WRAPPER_REGISTRATION substitute name=f func name type_wrapper_name=type_wrapper_name f class_type= TraceType gen_trace_type_func fn NativeFunction - dict str list str ops_headers f #include ATen ops fn root_name _ops h trace_method_definitions method_definition fn trace_wrapper_registrations method_registration fn gen_trace_type out str native_functions list NativeFunction template_path str - None NOTE see Note Sharded File top VariableType cpp template regarding sharding generated files fm = FileManager install_dir=out template_dir=template_path dry_run=False fm write_sharded TraceType cpp fn fn native_functions cpp name fn func MANUAL_TRACER key_fn=lambda fn fn root_name base_env= generated_comment + f generated fm template_dir_for_comments TraceType cpp env_callable=gen_trace_type_func num_shards= sharded_keys= ops_headers trace_method_definitions trace_wrapper_registrations