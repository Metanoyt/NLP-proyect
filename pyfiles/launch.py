mypy allow-untyped-defs r Module ` ` torch distributed launch ` ` ` ` torch distributed launch ` ` module spawns up multiple distributed training processes each training nodes warning This module going deprecated favor ref ` torchrun launcher-api ` The utility can used single-node distributed training which one more processes per node will spawned The utility can used either CPU training GPU training If utility used GPU training each distributed process will operating single GPU This can achieve well-improved single-node training performance It can also used multi-node distributed training spawning up multiple processes each node well-improved multi-node distributed training performance well This will especially beneficial systems multiple Infiniband interfaces have direct-GPU support since all them can utilized aggregated communication bandwidth In both cases single-node distributed training multi-node distributed training utility will launch given number processes per node ` ` -- nproc-per-node ` ` If used GPU training number needs less equal number GPUs current system ` ` nproc_per_node ` ` each process will operating single GPU GPU GPU nproc_per_node - How use module Single-Node multi-process distributed training python -m torch distributed launch -- nproc-per-node=NUM_GPUS_YOU_HAVE YOUR_TRAINING_SCRIPT py -- arg -- arg -- arg all other arguments your training script Multi-Node multi-process distributed training e g two nodes Node IP has free port python -m torch distributed launch -- nproc-per-node=NUM_GPUS_YOU_HAVE -- nnodes= -- node-rank= -- master-addr= -- master-port= YOUR_TRAINING_SCRIPT py -- arg -- arg -- arg all other arguments your training script Node python -m torch distributed launch -- nproc-per-node=NUM_GPUS_YOU_HAVE -- nnodes= -- node-rank= -- master-addr= -- master-port= YOUR_TRAINING_SCRIPT py -- arg -- arg -- arg all other arguments your training script To look up what optional arguments module offers python -m torch distributed launch -- help Important Notices This utility multi-process distributed single-node multi-node GPU training currently only achieves best performance using NCCL distributed backend Thus NCCL backend recommended backend use GPU training In your training program you must parse command-line argument ` ` -- local-rank=LOCAL_PROCESS_RANK ` ` which will provided module If your training program uses GPUs you should ensure your code only runs GPU device LOCAL_PROCESS_RANK This can done Parsing local_rank argument xdoctest +SKIP argparse parser = argparse ArgumentParser parser add_argument -- local-rank -- local_rank type=int args = parser parse_args Set your device local rank using either torch cuda set_device args local_rank before your code runs torch cuda device args local_rank your code run versionchanged The launcher will passes ` ` -- local-rank= rank ` ` argument your script From PyTorch onwards dashed ` ` -- local-rank ` ` preferred over previously used underscored ` ` -- local_rank ` ` For backward compatibility may necessary users handle both cases their argument parsing code This means including both ` ` -- local-rank ` ` ` ` -- local_rank ` ` argument parser If only ` ` -- local_rank ` ` provided launcher will trigger error error unrecognized arguments -- local-rank= rank For training code only supports PyTorch + including ` ` -- local-rank ` ` should sufficient In your training program you supposed call following function beginning start distributed backend It strongly recommended ` ` init_method=env ` ` Other init methods e g ` ` tcp ` ` may work ` ` env ` ` one officially supported module torch distributed init_process_group backend= YOUR BACKEND init_method= env In your training program you can either use regular distributed functions use func ` torch nn parallel DistributedDataParallel ` module If your training program uses GPUs training you would like use func ` torch nn parallel DistributedDataParallel ` module here how configure model = torch nn parallel DistributedDataParallel model device_ids= args local_rank output_device=args local_rank Please ensure ` ` device_ids ` ` argument set only GPU device id your code will operating This generally local rank process In other words ` ` device_ids ` ` needs ` ` args local_rank ` ` ` ` output_device ` ` needs ` ` args local_rank ` ` order use utility Another way pass ` ` local_rank ` ` subprocesses via environment variable ` ` LOCAL_RANK ` ` This behavior enabled when you launch script ` ` -- use-env=True ` ` You must adjust subprocess example above replace ` ` args local_rank ` ` ` ` os environ LOCAL_RANK ` ` launcher will pass ` ` -- local-rank ` ` when you specify flag warning ` ` local_rank ` ` NOT globally unique only unique per process machine Thus don t use decide you should e g write networked filesystem See https github com pytorch pytorch issues example how things can go wrong you don t do correctly typing_extensions deprecated _deprecated torch distributed run get_args_parser run parse_args args parser = get_args_parser parser add_argument -- use-env -- use_env default=False action= store_true help= Use environment variable pass local rank For legacy reasons default value False If set True script will pass -- local-rank argument will instead set LOCAL_RANK parser parse_args args launch args args no_python args use_env raise ValueError When using -- no-python flag you must also set -- use-env flag run args _deprecated The module torch distributed launch deprecated\n will removed future Use torchrun \n Note -- use-env set default torchrun \n If your script expects ` -- local-rank ` argument set please\n change read ` os environ LOCAL_RANK ` instead See \n https pytorch org docs stable distributed html#launch-utility \n further instructions\n category=FutureWarning main args=None args = parse_args args launch args __name__ == __main__ main