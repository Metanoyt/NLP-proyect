__future__ annotations difflib os textwrap dataclasses dataclass typing TYPE_CHECKING torchgen aoti fallback_ops aten_shimified_ops inductor_fallback_ops torchgen api types DispatcherSignature torchgen api types signatures CppSignature CppSignatureGroup torchgen context method_with_native_function torchgen model Argument BackendIndex BaseTy BaseType DispatchKey FunctionSchema is_cuda_dispatch_key ListType NativeFunction NativeFunctionsGroup OperatorName OptionalType Type Variant torchgen utils FileManager mapMaybe TYPE_CHECKING collections abc Sequence typing Optional base_type_to_c_type = BaseTy Tensor AtenTensorHandle BaseTy bool int _t Use int pass bool BaseTy int int _t BaseTy SymInt int _t Inductor-generated code won t see SymInt BaseTy Scalar double Use double pass both integer floating point BaseTy float double TODO how about other floating point types BaseTy str const char BaseTy DeviceIndex int _t BaseTy Layout int _t Represent enum int BaseTy MemoryFormat int _t Represent enum int BaseTy ScalarType int _t Represent enum int BaseTy Generator AtenGeneratorHandle base_type_to_aten_type = BaseTy Tensor Tensor BaseTy bool bool BaseTy int int _t BaseTy SymInt c SymInt BaseTy Scalar c Scalar BaseTy float double BaseTy str std string_view BaseTy DeviceIndex c DeviceIndex BaseTy Layout c Layout BaseTy MemoryFormat c MemoryFormat BaseTy ScalarType c ScalarType BaseTy Generator Generator base_type_to_callsite_expr = BaseTy Tensor resolve_tensor_dispatch_flags BaseTy bool BaseTy int BaseTy SymInt BaseTy Scalar BaseTy float BaseTy str BaseTy DeviceIndex static_cast c DeviceIndex BaseTy Layout static_cast c Layout BaseTy MemoryFormat static_cast c MemoryFormat BaseTy ScalarType static_cast c ScalarType BaseTy Generator generator_handle_to_generator_pointer convert args C types names declarations expressions function bodies convert_arg_type_and_name typ Type name str is_write bool = False - tuple list str list str list str list str isinstance typ BaseType typ name base_type_to_c_type typ name == BaseTy Tensor is_write For output tensors our normal call resolve_tensor_dispatch_flags results rvalue tensor which can t passed Tensor Override case specifically callsite_expr = f tensor_handle_to_tensor_pointer name callsite_expr = f base_type_to_callsite_expr typ name name base_type_to_callsite_expr typ name name base_type_to_c_type typ name name base_type_to_aten_type typ name callsite_expr typ name == BaseTy Device int _t int _t name name + _index_ c Device f c Device static_cast c DeviceType name static_cast c DeviceIndex name _index_ TODO BaseTy Dimname etc raise NotImplementedError f TODO add support arg type repr typ isinstance typ OptionalType c_types names aten_types callsite_exprs = convert_arg_type_and_name typ elem name j = index names new_aten_types = new_callsite_exprs = aten_type aten_types Use pointer denote optional type c_types j = c_types j + aten_type startswith c ArrayRef ArrayRef passed pointer + size no need add size argument new_aten_types append f std optional aten_type base_type = aten_type len c ArrayRef - new_callsite_exprs append f pointer_to_optional_list base_type names j names j + j += aten_type == c Device Device passed device_type + device_index new_aten_types append std optional c Device new_callsite_exprs append f pointer_to_optional_device names j names j + j += aten_type == Tensor new_aten_types append f std optional aten_type new_callsite_exprs append f resolve_tensor_dispatch_flags names j j += new_aten_types append f std optional aten_type new_callsite_exprs append f pointer_to_optional aten_type names j j += c_types names new_aten_types new_callsite_exprs isinstance typ ListType Need explicitly pass list pointer + length c_types names aten_types _ = convert_arg_type_and_name typ elem name assert len c_types == ListType unsupported element type + repr typ The list content should never modified c_types = f const c_types c_types append int _t name = names names append name + _len_ atype = aten_types callsite_exprs = atype == bool no converter std vector bool c ArrayRef bool construct std array bool N instead assert typ size None callsite_exprs append f pointer_to_list typ size name atype == Tensor is_write callsite_exprs append f resolve_tensor_list_dispatch_flags name name _len_ atype == std optional Tensor convert std vector std optional Tensor c List std optional Tensor callsite_exprs append f c List atype c ArrayRef atype resolve_tensor_list_dispatch_flags name name _len_ callsite_exprs append f pointer_to_list atype name name _len_ aten_types = f c ArrayRef t t aten_types c_types names aten_types callsite_exprs raise NotImplementedError f Argument type repr typ supported zip_type_and_name types list str names list str - list str typ + + name typ name zip types names Generate argument declarations callsite expressions gen_arguments flat_arguments Sequence Argument skipped_args set str - tuple list str list str types list str = new_names list str = callsite_exprs list str = arg flat_arguments arg name skipped_args callsite_exprs append std nullopt continue new_types names _ new_callsite_exprs = convert_arg_type_and_name arg type arg name arg is_write types extend new_types new_names extend names callsite_exprs extend new_callsite_exprs zip_type_and_name types new_names callsite_exprs Return values passed out pointer arguments because all C shim functions expected AOTITorchError Generate returns declarations callsite expressions gen_returns schema FunctionSchema - tuple list str list str types = names = idx ret enumerate schema returns names append f ret idx isinstance ret type BaseType ret type name base_type_to_c_type types append base_type_to_c_type ret type name + raise NotImplementedError f TODO add support type repr ret type convert_return typ BaseType val str - str typ name == BaseTy Tensor f new_tensor_handle std move val typ name == BaseTy SymInt f val expect_int typ name == BaseTy Scalar f val toDouble val ret_pointer_can_be_null = False unambiguous_name = schema name unambiguous_name name _functional_sym_constrain_range _scaled_dot_product_cudnn_attention _scaled_dot_product_efficient_attention_backward _scaled_dot_product_efficient_attention _scaled_dot_product_flash_attention _scaled_dot_product_fused_attention_overrideable _thhn_fused_lstm_cell_backward_impl convolution_backward grid_sampler_ d_backward grid_sampler_ d_backward linear_backward name unambiguous_name ret_pointer_can_be_null = True break callsite_exprs list str = idx ret enumerate schema returns tmp = tmp_result len names == f std get idx tmp_result assert isinstance ret type BaseType rval = convert_return ret type tmp ret_pointer_can_be_null callsite_exprs append f names idx names idx = rval callsite_exprs append f names idx = rval zip_type_and_name types names callsite_exprs gen py generates header first then src so caching result here avoid duplicate work declaration_definition_cache dict tuple str str str tuple str str = gen_declaration_and_definition schema FunctionSchema device str backend_call str version_info dict str list str - tuple str str base_name = schema name unambiguous_name global declaration_definition_cache base_name device backend_call declaration_definition_cache declaration_definition_cache base_name device backend_call Check validity version_info The format should look like v new_arg v new_arg new_arg indexed_version_info dict int list str = ver_str new_args sorted version_info items assert ver_str startswith v f Version number base_name ver_str starting v try ver_id = int ver_str except ValueError e raise AssertionError f Version number base_name ver_str valid integer after v e assert ver_id indexed_version_info f ver_str base_name has already been defined indexed_version_info ver_id = new_args declarations list str = definitions list str = skipped_args set str = set ver_id new_args sorted indexed_version_info items reverse=True Iterate reverse order so latest version op will get generated first all arguments included while set to-be-trimmed args carried down generate earlier version op func_name = base_name ver_id == f base_name _v ver_id schema is_out_fn out_variant has out arguments front s ok ignore values because C shim functions only AOTITorchError args callsite_exprs = gen_arguments schema arguments out schema arguments flat_non_out skipped_args ret_assignments list str = args callsite_exprs = gen_arguments schema arguments flat_all skipped_args ignore values inplace ops ret_declarations ret_assignments = schema name name inplace gen_returns schema args extend ret_declarations declaration = textwrap dedent f AOTITorchError aoti_torch_ device _ func_name join args tmp_result = auto tmp_result = ret_assignments indent = \t\t ret_assignments_str = \n join indent + r r ret_assignments ret_assignments definition = textwrap dedent f declaration AOTI_TORCH_CONVERT_EXCEPTION_TO_ERROR_CODE tmp_result backend_call join callsite_exprs + ret_assignments_str + textwrap dedent skipped_args update new_args declarations append f AOTI_TORCH_EXPORT declaration definitions append definition declaration_definition_cache base_name device backend_call = \n join declarations \n join definitions declaration_definition_cache base_name device backend_call gen_static_dispatch_backend_call_signature sig CppSignature &#124; DispatcherSignature f NativeFunction - CppSignature sig = DispatcherSignature from_schema f func cpp_sigs = CppSignatureGroup from_native_function f method=False fallback_binding=False sig symint f func has_symint cpp_sig = cpp_sigs symint_signature cpp_sig = cpp_sigs signature assert cpp_sig None cpp_sig gen_static_dispatch_backend_call f NativeFunction backend_index Optional BackendIndex = None - str sig = DispatcherSignature from_schema f func cpp_sig = gen_static_dispatch_backend_call_signature sig f backend_index None Check symint function function only has method variants sig symint f func has_symint has_function_variant = Variant function f variants has_function_variant Functions both function method variants can use _symint version e g narrow - narrow_symint BUT Method-only functions symint parameters should use symint namespace Remove _symint suffix since symint namespace uses base name e g new_empty - symint new_empty c SymInt base_name = cpp_sig name base_name = base_name removesuffix _symint Remove _symint suffix f symint base_name c SymInt f cpp_sig name f backend_index dispatch_key lower cpp_sig name get_backend_index_for_aoti func NativeFunction func_group_mapping dict OperatorName NativeFunctionsGroup dispatch_key Optional DispatchKey backend_indices dict DispatchKey BackendIndex extend_aoti_c_shim bool - BackendIndex &#124; None backend_index = None dispatch_key None backend_index backend_indices dispatch_key has_kernel func func structured_delegate None func structured_delegate func_group_mapping backend_indices dispatch_key has_kernel func_group_mapping func structured_delegate backend_index = backend_indices dispatch_key extend out-of-tree kernels we don t need duplicatly create C shim wrappers other dispatch keys extend_aoti_c_shim backend_index backend_indices DispatchKey CompositeExplicitAutograd has_kernel func We need create C shim wrappers CompositeExplicitAutograd kernels backend_index = backend_indices DispatchKey CompositeExplicitAutograd backend_indices DispatchKey CompositeExplicitAutogradNonFunctional has_kernel func We need create C shim wrappers CompositeExplicitAutogradNonFunctional kernels backend_index = backend_indices DispatchKey CompositeExplicitAutogradNonFunctional backend_indices DispatchKey CompositeImplicitAutograd has_kernel func backend_index = backend_indices DispatchKey CompositeImplicitAutograd backend_index get_header_for_aoti func NativeFunction func_group_mapping dict OperatorName NativeFunctionsGroup dispatch_key Optional DispatchKey backend_indices dict DispatchKey BackendIndex extend_aoti_c_shim bool - str &#124; None backend_index = get_backend_index_for_aoti func func_group_mapping dispatch_key backend_indices extend_aoti_c_shim backend_index None dispatch_key None f #include ATen ops func root_name h None f #include ATen ops func root_name _ backend_index dispatch_key lower _dispatch h get_fallback_op_name func NativeFunction - str f func namespace func func name name func func name overload_name func func name overload_name f func namespace func func name name default gen_c_shim func NativeFunction version_info dict str list str func_group_mapping dict OperatorName NativeFunctionsGroup dispatch_key Optional DispatchKey backend_indices dict DispatchKey BackendIndex header bool extend_aoti_c_shim bool - str &#124; None backend_index = get_backend_index_for_aoti func func_group_mapping dispatch_key backend_indices extend_aoti_c_shim backend_index None dispatch_key None None schema = func func device = aten dispatch_key None dispatch_key lower backend_call = gen_static_dispatch_backend_call func backend_index try header declaration _ = gen_declaration_and_definition schema device backend_call version_info declaration _ definition = gen_declaration_and_definition schema device backend_call version_info definition except NotImplementedError None dataclass frozen=True ShimGenerator inductor_fallback_ops dict str dict str list str func_group_mapping dict OperatorName NativeFunctionsGroup dispatch_key Optional DispatchKey backend_indices dict DispatchKey BackendIndex header bool True generate h False generate cpp extend_aoti_c_shim bool method_with_native_function __call__ func NativeFunction - str &#124; None version_info = inductor_fallback_ops get_fallback_op_name func result = gen_c_shim func version_info func_group_mapping dispatch_key backend_indices header extend_aoti_c_shim result gen_aoti_c_shim native_functions Sequence NativeFunction inductor_fallback_ops dict str dict str list str func_group_mapping dict OperatorName NativeFunctionsGroup dispatch_key Optional DispatchKey backend_indices dict DispatchKey BackendIndex header bool extend_aoti_c_shim bool includes str = - str body = \n join list mapMaybe ShimGenerator inductor_fallback_ops func_group_mapping dispatch_key backend_indices header extend_aoti_c_shim native_functions device = aten dispatch_key None dispatch_key lower include_device_functions = #include ATen Functions h dispatch_key None f #include ATen str dispatch_key Functions h aten_warning = \n\n This file corresponds aten_shimified_ops list torchgen aoti fallback_ops py\n dispatch_key None warning = WARNING THIS FILE IS AUTOGENERATED BY torchgen DO NOT MODIFY BY HAND See https github com pytorch pytorch blob e c e cf e torchgen gen py#L -L details header warning + aten_warning + textwrap dedent #pragma once #include torch csrc inductor aoti_torch c shim h #ifdef __cplusplus extern C #endif + body + textwrap dedent #ifdef __cplusplus extern C #endif warning + aten_warning + textwrap dedent f #include torch csrc inductor aoti_torch generated extend extend_aoti_c_shim c_shim_ device h #include torch csrc inductor aoti_torch utils h #ifndef AT_PER_OPERATOR_HEADERS include_device_functions #include ATen CompositeExplicitAutogradFunctions h #include ATen CompositeExplicitAutogradNonFunctionalFunctions h #include ATen CompositeImplicitAutogradFunctions h #else + includes + textwrap dedent #endif AT_PER_OPERATOR_HEADERS using namespace torch aot_inductor + body gen_aoti_c_shim_files aoti_fm FileManager aoti_backends set Optional DispatchKey native_functions Sequence NativeFunction backend_indices dict DispatchKey BackendIndex structured_native_functions Sequence NativeFunctionsGroup extra_cuda_headers str extend_aoti_c_shim bool update_aoti_c_shim bool - None structured_func_group_dict = func_group structured_native_functions func func_group functions func structured_delegate None structured_func_group_dict func structured_delegate = func_group break dispatch_key aoti_backends Use aten_shimified_ops aten backend inductor_fallback_ops others fallback_ops_dict = aten_shimified_ops dispatch_key None inductor_fallback_ops fallbacks = func native_functions op_name = get_fallback_op_name func op_name fallback_ops_dict fallbacks op_name = func fallback_native_functions = tuple value _ value sorted fallbacks items Use aten device name when dispatch_key Generic device_name = aten dispatch_key None dispatch_key lower header files checked ABI-compatibility checking header_file_name = f c_shim_ device_name h new_header = gen_aoti_c_shim fallback_native_functions fallback_ops_dict structured_func_group_dict dispatch_key backend_indices header=True extend_aoti_c_shim=extend_aoti_c_shim includes= update_aoti_c_shim aoti_fm write header_file_name lambda new_header try open os path join aoti_fm install_dir header_file_name old_file old_header = old_file read old_header = new_header diff = \n join difflib unified_diff old_header splitlines new_header splitlines fromfile= expected tofile= actual lineterm= raise RuntimeError f The generated AOTInductor C shim header files have unexpectedly changed This indicates AOTInductor fallback operator ABI backward compatibility breakage Only limited number situations allowed You added fallback op inductor_fallback_ops list torchgen aoti fallback_ops py If s case run ` python torchgen gen py -- update-aoti-c-shim ` add new entry existing C shim header files You added new default argument existing fallback op This clearly BC breaking change AOTInductor land You need annotate new default argument torchgen aoti fallback_ops py then run ` python torchgen gen py -- update-aoti-c-shim ` update C shim header files creating different versions fallback op See https github com pytorch pytorch pull example diff except FileNotFoundError print f os path join aoti_fm install_dir header_file_name found cpp files always generated on-the-fly headers_for_aoti - str headers = func fallback_native_functions header = get_header_for_aoti func structured_func_group_dict dispatch_key backend_indices extend_aoti_c_shim=extend_aoti_c_shim header None headers append header \n join sorted set headers extra_headers = extra_cuda_headers dispatch_key None is_cuda_dispatch_key dispatch_key aoti_fm write f c_shim_ device_name cpp lambda gen_aoti_c_shim fallback_native_functions fallback_ops_dict structured_func_group_dict dispatch_key backend_indices header=False extend_aoti_c_shim=extend_aoti_c_shim includes=headers_for_aoti + \n + extra_headers