Owner s module dataloader ruff noqa F ctypes errno faulthandler functools gc itertools math operator os signal sys tempfile time unittest warnings torch torch utils data datapipes dp torch multiprocessing mp torch _utils ExceptionWrapper torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_utils IS_CI IS_JETSON IS_MACOS IS_S X IS_SANDCASTLE IS_WINDOWS load_tests parametrize run_tests skipIfNoDill skipIfXpu slowTest TEST_CUDA TEST_NUMPY TEST_WITH_ASAN TEST_WITH_TSAN TestCase xfailIfLinux torch utils data _utils ChainDataset ConcatDataset DataLoader dataloader Dataset IterableDataset IterDataPipe StackDataset Subset TensorDataset torch utils data _utils MP_STATUS_CHECK_INTERVAL torch utils data datapipes iter IterableWrapper torch utils data dataset random_split try psutil HAS_PSUTIL = True except ModuleNotFoundError HAS_PSUTIL = False psutil = None err_msg = psutil found Some critical data loader tests relying e g TestDataLoader test_proper_exit will run IS_CI raise ModuleNotFoundError err_msg None warnings warn err_msg try numpy np HAS_NUMPY = True except ModuleNotFoundError HAS_NUMPY = False np = None skipIfNoNumpy = unittest skipIf HAS_NUMPY no NumPy load_tests torch testing _internal common_utils used automatically filter tests sharding sandcastle This line silences flake warnings load_tests = load_tests noqa PLW TEST_CUDA_IPC = torch cuda is_available sys platform = darwin sys platform = win IS_JETSON TEST_WITH_ROCM https github com pytorch pytorch issues TEST_MULTIGPU = TEST_CUDA_IPC torch cuda device_count We want use ` spawn ` able because some our tests check data loader terminates gracefully To prevent hanging testing process such data loaders run separate subprocess We also want test ` pin_memory=True ` configuration thus ` spawn ` required launch such processes they initialize CUDA context Mixing different start method recipe disaster e g using fork ` mp Event ` spawn ` mp Process ` segfaults So we set globally avoid bugs Get multiprocessing context because some test third party library will set start_method when imported setting again triggers ` RuntimeError ` mp = mp get_context method= spawn s timeout Yes environments where physical CPU resources shared e g CI time inter-process communication can highly varying With ~ s timeout we have observed flakiness some CI builds see pytorch pytorch# pytorch pytorch# We follow CPython multiprocessing setup set timeout s here https github com python cpython blob e f bdf ee c e bfa Lib test _test_multiprocessing py#L JOIN_TIMEOUT = seconds supported_multiprocessing_contexts = None + list torch multiprocessing get_all_start_methods The following collate functions defined globally here pickle purposes collate_fn returns batch cloned _clone_collate b x clone x b collate_fn returns batch sparse coo tensors cloned _sparse_coo_collate b lst = x b t = x clone lst append t Force sparse tensor invariants checks check_pinning=True reproduces gh- torch _validate_sparse_coo_tensor_args t _indices t _values t size t is_coalesced check_pinning=False lst unittest skipIf TEST_WITH_TSAN Fails TSAN following error starting new threads after multi-threaded fork supported Dying set die_after_fork= override TestDatasetRandomSplit TestCase test_lengths_must_equal_dataset_size assertRaises ValueError random_split test_splits_have_correct_size splits = random_split assertEqual len splits assertEqual len splits assertEqual len splits splits = random_split assertEqual len splits assertEqual len splits assertEqual len splits Odd size splits assertEqual len random_split range generator=torch Generator manual_seed Odd sized round-robin splits splits = random_split range generator=torch Generator manual_seed assertEqual len splits assertEqual len splits assertEqual len splits assertEqual len splits test_splits_are_mutually_exclusive data = splits = random_split data all_values = all_values extend list splits all_values extend list splits data sort all_values sort assertListEqual data all_values splits = random_split data all_values = all_values extend list splits all_values extend list splits data sort all_values sort assertListEqual data all_values data = splits = random_split data all_values = all_values extend list splits all_values extend list splits data sort all_values sort assertListEqual data all_values test_splits_indexing_type r Indices generated random_split should integer type CustomDataset __init__ test_object custom_list data = custom_list test_object = test_object __getitem__ key test_object assertEqual type key int data key __len__ len data x = dataset = CustomDataset x dataset = random_split dataset data_loader = DataLoader dataset _batch data_loader pass fractional splitting dataset = CustomDataset x dataset = random_split dataset data_loader = DataLoader dataset _batch data_loader pass test_splits_reproducibility assertEqual list x x random_split range generator=torch Generator manual_seed assertEqual random_split range generator=torch Generator manual_seed random_split range generator=torch Generator manual_seed assertEqual random_split range generator=torch Generator manual_seed random_split range generator=torch Generator manual_seed assertEqual random_split range generator=torch Generator manual_seed random_split range generator=torch Generator manual_seed test_incomplete_fractional_splits assertRaises ValueError should raise since sum fractions random_split assertRaises ValueError should raise since fraction random_split test_splits_generator A random_split without specific generator should affect default one state = torch get_rng_state = torch rand torch set_rng_state state random_split range b = torch rand assertNotEqual b A random_split specific generator should affect default one state = torch get_rng_state = torch rand torch set_rng_state state random_split range generator=torch Generator manual_seed b = torch rand assertEqual b test_slicing_of_subset_of_dataset Testing slicing subset initialized dataset dataset = TensorDataset torch tensor subset_of_dataset = Subset dataset assertEqual subset_of_dataset dataset assertEqual subset_of_dataset dataset assertEqual subset_of_dataset - dataset - Testing slicing subset random split subset subset = random_split dataset assertEqual subset dataset subset indices assertEqual subset dataset subset indices assertEqual subset - dataset subset indices - test_slicing_of_subset_of_subset Testing slicing subset initialized subset dataset = TensorDataset torch tensor subset_of_dataset = Subset dataset subset_of_subset = Subset subset_of_dataset assertEqual subset_of_subset dataset assertEqual subset_of_subset dataset assertEqual subset_of_subset - dataset - Testing slicing subset subset random split subset subset = random_split dataset subset_of_subset subset_of_subset = random_split subset idx = subset indices i i subset_of_subset indices assertEqual subset_of_subset dataset idx copy assertEqual subset_of_subset dataset idx assertEqual subset_of_subset - dataset idx - CUDACountingDataset Dataset __init__ n super __init__ n = n __getitem__ i torch as_tensor i device= cuda __len__ n CountingDataset Dataset __init__ n super __init__ n = n __getitem__ i i __len__ n CountingIterableDataset IterableDataset __init__ n super __init__ n = n __iter__ iter range n __len__ n unittest skipIf TEST_WITH_TSAN Fails TSAN following error starting new threads after multi-threaded fork supported Dying set die_after_fork= override TestTensorDataset TestCase test_len source = TensorDataset torch randn torch randperm assertEqual len source test_getitem t = torch randn l = torch randn source = TensorDataset t l i range assertEqual t i source i assertEqual l i source i test_getitem_ d t = torch randn l = torch randn source = TensorDataset t l i range assertEqual t i source i assertEqual l i source i test_single_tensor t = torch randn source = TensorDataset t assertEqual len source i range assertEqual t i source i test_many_tensors t = torch randn t = torch randn t = torch randn t = torch randn source = TensorDataset t t t t assertEqual len source i range assertEqual t i source i assertEqual t i source i assertEqual t i source i assertEqual t i source i unittest skipIf TEST_WITH_TSAN Fails TSAN following error starting new threads after multi-threaded fork supported Dying set die_after_fork= override TestStackDataset TestCase test_empty assertRaisesRegex ValueError At least one dataset should passed StackDataset test_mixed assertRaisesRegex ValueError Supported either StackDataset TensorDataset torch randn a=TensorDataset torch randn test_size_mismatch assertRaisesRegex ValueError Size mismatch between datasets StackDataset TensorDataset torch randn TensorDataset torch randn assertRaisesRegex ValueError Size mismatch between datasets StackDataset a=TensorDataset torch randn b=TensorDataset torch randn test_len source = StackDataset TensorDataset torch randn TensorDataset torch randn assertEqual len source source = StackDataset TensorDataset torch randn assertEqual len source source = StackDataset a=TensorDataset torch randn b=TensorDataset torch randn assertEqual len source source = StackDataset a=TensorDataset torch randn assertEqual len source test_single t = TensorDataset torch randn source = StackDataset t i range assertEqual t i source i source = StackDataset a=t i range assertEqual t i source i test_getitem t = TensorDataset torch randn l = TensorDataset torch randn source = StackDataset t l i range assertEqual t i source i assertEqual l i source i source = StackDataset a=t b=l i range assertEqual t i source i assertEqual l i source i b test_getitems GetItemsDataset Dataset __init__ - None data = torch randn __getitem__ item data item __getitems__ items data items __len__ t = GetItemsDataset l = source = StackDataset t l batch = source __getitems__ i range assertEqual t i batch i assertEqual l i batch i source = StackDataset t=t l=l batch = source __getitems__ i range assertEqual t i batch i t assertEqual l i batch i l test_getitems_raises_index_error GetItemsDataset Dataset __init__ - None data = torch randn __getitem__ item data item __getitems__ items data items __len__ t = GetItemsDataset l = source = StackDataset t l assertRaises IndexError source __getitems__ test_getitems_value_error GetItemsDataset Dataset __init__ - None data = torch randn __getitem__ item data item __getitems__ items data items - less __len__ t = GetItemsDataset l = source = StackDataset t l assertRaisesRegex ValueError Nested dataset s output size mismatch Expected got source __getitems__ unittest skipIf TEST_WITH_TSAN Fails TSAN following error starting new threads after multi-threaded fork supported Dying set die_after_fork= override TestConcatDataset TestCase test_concat_two_singletons result = ConcatDataset assertEqual len result assertEqual result assertEqual result test_concat_two_non_singletons result = ConcatDataset assertEqual len result assertEqual result assertEqual result test_concat_two_non_singletons_with_empty Adding empty dataset somewhere correctly handled result = ConcatDataset assertEqual len result assertEqual result assertEqual result test_concat_raises_index_error result = ConcatDataset assertRaises IndexError one goes result test_add_dataset d = TensorDataset torch rand torch rand d = TensorDataset torch rand torch rand d = TensorDataset torch rand torch rand result = d + d + d assertEqual len result assertEqual d - result abs sum assertEqual d - result abs sum assertEqual d - result abs sum test_iterable_dataset_err d = TensorDataset torch rand torch rand = CountingIterableDataset = CountingIterableDataset assertRaisesRegex AssertionError does support IterableDataset ConcatDataset d assertRaisesRegex AssertionError does support IterableDataset ConcatDataset assertRaisesRegex AssertionError does support IterableDataset ConcatDataset d takes dummy var so can also used ` worker_init_fn ` set_faulthander_if_available _=None faulthandler enable sys __stderr__ IS_WINDOWS windows does have faulthandler register chain=False prevents default behavior killing process faulthandler register signal SIGUSR file=sys __stderr__ chain=False set_faulthander_if_available Process ` pid ` must have called ` set_faulthander_if_available ` print_traces_of_all_threads pid IS_WINDOWS use custom signal available os kill pid signal SIGUSR otherwise we can still use handler given faulthandler enable cost killing process os kill pid signal SIGSEGV wait parent process give subprocess some time print time sleep The following ` ErrorTrackingProcess ` stores first encountered exception its ` exception ` attribute Inspired https stackoverflow com ErrorTrackingProcess mp Process Why no args py doesn t support fn x args key=val kwargs Setting disable_stderr=True may generate lot unrelated error outputs could helpful debugging __init__ disable_stderr=True kwargs super __init__ kwargs _pconn _cconn = mp Pipe _exception = None disable_stderr = disable_stderr run set_faulthander_if_available disable_stderr Disable polluting stderr errors supposed happen open os devnull w devnull os dup devnull fileno sys stderr fileno try super run _cconn send None except Exception _cconn send ExceptionWrapper sys exc_info raise print_traces_of_all_threads assert is_alive can only use print_traces_of_all_threads process alive assert disable_stderr do disable stderr you use print_traces_of_all_threads On platforms without ` SIGUSR ` ` set_faulthander_if_available ` sets ` faulthandler enable ` ` print_traces_of_all_threads ` may kill process So let s poll exception first _ = exception print_traces_of_all_threads pid property exception _pconn poll _exception = _pconn recv _exception None None _exception exc_type _exception exc_msg ESRCH means os kill can t finds alive proc send_signal signum ignore_ESRCH=False try os kill pid signum except OSError e ignore_ESRCH e errno = errno ESRCH raise ErrorDataset Dataset __init__ size size = size __len__ size SegfaultDataset Dataset __init__ size size = size __getitem__ idx ctypes string_at __len__ size SleepDataset Dataset __init__ size sleep_sec size = size sleep_sec = sleep_sec slept = False __getitem__ idx slept time sleep sleep_sec slept = True idx __len__ size SeedDataset Dataset __init__ size size = size __getitem__ idx torch initial_seed __len__ size WorkerSpecificIterableDataset IterableDataset __init__ sizes_for_all_workers sizes_for_all_workers = sizes_for_all_workers __iter__ worker_info = torch utils data get_worker_info assert worker_info None iter range sizes_for_all_workers worker_info id __len__ sum sizes_for_all_workers Inspired https stackoverflow com If all workers will call ` sync_once ` they will blocked until all workers reach call i e acting like barrier This can used ensure each worker least processes one data SynchronizedDataset Dataset __init__ size batch_size num_workers assert size = num_workers batch_size count = mp Value i lock=True barrier = mp Semaphore num_workers = num_workers size = size sync_once count get_lock count value += count value == num_workers barrier release barrier acquire barrier release __getitem__ idx raise NotImplementedError __len__ size EmptyTensorDataset torch utils data Dataset __init__ len len = len __len__ len __getitem__ any torch empty SynchronizedSeedDataset SynchronizedDataset __getitem__ idx sync_once torch initial_seed _test_timeout persistent_workers dataset = SleepDataset dataloader = DataLoader dataset batch_size= num_workers= timeout= persistent_workers=persistent_workers _ = next iter dataloader _test_timeout_pin_memory persistent_workers dataset = SleepDataset dataloader = DataLoader dataset batch_size= num_workers= timeout= pin_memory=True persistent_workers=persistent_workers _ = next iter dataloader _test_large_sampler_indices persistent_workers See test_large_sampler_indices https github com pytorch pytorch issues dataloader = torch utils data DataLoader EmptyTensorDataset batch_size= persistent_workers=persistent_workers num_workers= = iter dataloader x assert x numel == raise RuntimeError My Error disable_stderr worker_id r Avoids printing ERROR Unexpected segmentation fault encountered worker workers Since worker signal handler prints low-level write has done OS level via dup This used worker_init_fn test_segfault sys stderr flush flush library buffers dup knows nothing about Can t use with-block because otherwise fd will closed when function ends open os devnull w devnull os dup devnull fileno sys stderr fileno _test_segfault dataset = SegfaultDataset dataloader = DataLoader dataset batch_size= num_workers= worker_init_fn=disable_stderr _ = next iter dataloader _test_no_segfault dataset = num_threads = torch get_num_threads num_threads torch set_num_threads torch set_num_threads num_threads mp_ctx = torch multiprocessing get_context method= fork dataloader = DataLoader dataset num_workers= worker_init_fn=disable_stderr multiprocessing_context=mp_ctx _ = next iter dataloader TestProperExitDataset Dataset __init__ size error_event size = size error_event = error_event __len__ size __getitem__ idx worker_info = torch utils data get_worker_info error_event None error_event is_set worker_info id == worker_info num_workers - only error last worker raise RuntimeError Worker error torch tensor idx TestProperExitIterableDataset IterableDataset __init__ size error_event error_event = error_event size = size remaining = size __len__ size __iter__ __next__ worker_info = torch utils data get_worker_info error_event None error_event is_set worker_info id == worker_info num_workers - only error last worker raise RuntimeError Worker error remaining -= remaining raise StopIteration torch tensor - See TestDataLoader test_proper_exit usage _test_proper_exit is_iterable_dataset use_workers pin_memory exit_method hold_iter_reference loader_setup_event tester_setup_event persistent_workers num_workers = use_workers exit_method == worker_error exit_method == worker_kill assert use_workers True exit_method == worker_error worker_error_event = mp Event worker_error_event = None is_iterable_dataset ds = TestProperExitIterableDataset worker_error_event ds = TestProperExitDataset worker_error_event loader = DataLoader ds batch_size= shuffle=False num_workers=num_workers pin_memory=pin_memory worker_init_fn=set_faulthander_if_available persistent_workers=persistent_workers error_it = use_workers magical per-worker prefetch number FIXME change after number becomes configurable is_iterable_dataset assert len ds num_workers error_it + + assert len loader error_it + + num_workers is_iterable_dataset assert len ds error_it + assert len loader error_it + = iter loader use_workers workers = _workers kill_pid pid psutil_p = psutil Process pid psutil_p kill psutil_p wait JOIN_TIMEOUT assert psutil_p is_running i _ enumerate i == hold_iter_reference del del loader loader_setup_event set tester_setup_event wait ensure workers still alive use_workers w workers assert w is_alive worker_error_event None worker_error_event set i == error_it exit_method == loader_error raise RuntimeError Loader error exit_method == loader_kill kill_pid os getpid exit_method == worker_kill kill_pid workers - pid kill last worker hold_iter_reference Tries trigger __del__ clean-up rather than automatic exiting daemonic children Technically should automatically triggered I don t want rely implementation detail Python gc gc collect TestWorkerInfoDataset SynchronizedDataset __getitem__ idx sync_once torch tensor value Should used worker_init_fn TestWorkerInfoDataset See _test_get_worker_info below usage _test_worker_info_init_fn worker_id worker_info = torch utils data get_worker_info assert worker_id == worker_info id worker_init_fn worker_info should have consistent id assert worker_id worker_info num_workers worker_init_fn worker_info should have valid id assert worker_info seed == torch initial_seed worker_init_fn worker_info should have consistent seed dataset = worker_info dataset assert isinstance dataset TestWorkerInfoDataset worker_info should have correct dataset copy assert hasattr dataset value worker_info should have correct dataset copy test WorkerInfo attributes read-only try worker_info id = except RuntimeError e assert str e == Cannot assign attributes WorkerInfo objects try worker_info = except RuntimeError e assert str e == Cannot assign attributes WorkerInfo objects k id num_workers seed dataset assert f k = repr worker_info dataset value = worker_id os getpid _test_get_worker_info get_worker_info returns None main proc assert torch utils data get_worker_info None num_workers = batch_size = dataset = TestWorkerInfoDataset batch_size num_workers dataloader = DataLoader dataset batch_size=batch_size num_workers=num_workers worker_init_fn=_test_worker_info_init_fn = iter dataloader data = d data append d noqa PERF worker_pids = w pid w _workers data = torch cat data d data each ` d ` worker_id worker_pid pair which set _test_worker_info_init_fn assert d == worker_pids d get_worker_info returns None main proc after data loading assert torch utils data get_worker_info None main proc dataset never assigned attribute assert hasattr dataset value try _ = dataset except AttributeError raise RuntimeError Expected AttributeError test custom init function init_fn worker_id torch manual_seed used test_error_in_init ErrorIterableDataset IterableDataset __iter__ raise RuntimeError Error __iter__ used test_error_in_init error_worker_init_fn _ raise RuntimeError Error worker_init_fn BulkLoadingDataset Dataset __init__ length length = length __getitem__ indices assert isinstance indices list tuple torch as_tensor indices __len__ length BulkLoadingSampler torch utils data Sampler __init__ dataset batch_size dataset = dataset batch_size = batch_size __iter__ x torch randperm len dataset split batch_size yield x tolist __len__ int math ceil len dataset float batch_size TestMultiEpochDataset IterableDataset __init__ length length = length __iter__ worker_info = torch utils data get_worker_info assert worker_info None worker_id = worker_info id _ range length worker_info num_workers yield worker_id __len__ length CustomList list pass CustomDict dict pass row_processor row np add row filter_len row len row == unittest skipIf TEST_WITH_TSAN Fails TSAN following error starting new threads after multi-threaded fork supported Dying set die_after_fork= override unittest skipIf TEST_WITH_ASAN DataLoader tests hang ASAN see https github com pytorch pytorch issues TestDataLoader TestCase setUp super setUp data = torch randn labels = torch randperm repeat dataset = TensorDataset data labels persistent_workers = False _get_data_loader dataset kwargs persistent_workers = kwargs get persistent_workers persistent_workers persistent_workers kwargs get num_workers == persistent_workers = False kwargs persistent_workers = persistent_workers DataLoader dataset kwargs _test_sequential loader batch_size = loader batch_size batch_size None idx sample target enumerate loader assertEqual sample data idx assertEqual target labels idx assertEqual idx len dataset - i sample target enumerate loader idx = i batch_size assertEqual sample data idx idx + batch_size assertEqual target labels idx idx + batch_size assertEqual i math floor len dataset - batch_size _test_shuffle loader found_data = dict fromkeys range data size found_labels = dict fromkeys range labels size batch_size = loader batch_size batch_size None i batch_samples batch_targets enumerate loader sample target = batch_samples batch_targets data_point_idx data_point enumerate data data_point eq sample all assertFalse found_data data_point_idx found_data data_point_idx += break assertEqual target labels data_point_idx found_labels data_point_idx += assertEqual sum found_data values i + assertEqual sum found_labels values i + assertEqual i len dataset - i batch_samples batch_targets enumerate loader sample target zip batch_samples batch_targets data_point_idx data_point enumerate data data_point eq sample all assertFalse found_data data_point_idx found_data data_point_idx += break assertEqual target labels data_point_idx found_labels data_point_idx += assertEqual sum found_data values i + batch_size assertEqual sum found_labels values i + batch_size assertEqual i math floor len dataset - batch_size _test_error loader = iter loader errors = while True try next except NotImplementedError errors += except StopIteration assertEqual errors math ceil float len loader dataset loader batch_size test_error_in_init num_workers loader = _get_data_loader ErrorIterableDataset num_workers=num_workers assertRaisesRegex RuntimeError Error __iter__ list iter loader loader = _get_data_loader dataset num_workers= worker_init_fn=error_worker_init_fn assertRaisesRegex RuntimeError Error worker_init_fn list iter loader test_typing Make sure there no TypeError SomeDatasetClass Dataset list torch Tensor pass _create_dataloader is_train bool - DataLoader list torch Tensor pass unittest skipIf IS_SANDCASTLE subprocess doesn t work FB internal CI unittest skipIf IS_WINDOWS No resource module Windows test_fd_limit_exceeded See NOTE DataLoader Linux open files limit subprocess subprocess check_output sys executable -c \ torch resource torch utils data DataLoader IterableDataset RandomDataset IterableDataset __init__ len size super RandomDataset __init__ len = len size = size __iter__ __next__ len = raise StopIteration len -= torch randn size try keep_fds_alive = resource setrlimit resource RLIMIT_NOFILE random_t DataLoader RandomDataset multiprocessing_context= fork num_workers= random_t max dim= keep_fds_alive append random_t except RuntimeError e assert ulimit -n str e assert set_sharing_strategy str e test_invalid_assign_after_init dl = _get_data_loader dataset attr batch_size sampler batch_sampler drop_last dataset fn setattr dl attr assertRaises ValueError fn test_sequential_nonbatch _test_sequential _get_data_loader dataset batch_size=None test_sequential_batch _test_sequential _get_data_loader dataset _test_sequential _get_data_loader dataset batch_size= test_bulk_loading_nobatch n = bs = ds = BulkLoadingDataset n sampler = BulkLoadingSampler ds batch_size= num_workers dl = _get_data_loader ds num_workers=num_workers batch_size=None sampler=sampler pin_memory=TEST_CUDA assertFalse dl _auto_collation samples = list dl assertEqual samples is_pinned TEST_CUDA assertEqual set torch cat samples tolist set range n test_growing_dataset dataset = torch ones _ range dataloader_seq = _get_data_loader dataset shuffle=False dataloader_shuffle = _get_data_loader dataset shuffle=True dataset append torch ones assertEqual len dataloader_seq assertEqual len dataloader_shuffle unittest skipIf TEST_CUDA CUDA unavailable test_sequential_pin_memory loader = _get_data_loader dataset batch_size= pin_memory=True input target loader assertTrue input is_pinned assertTrue target is_pinned unittest skipIf TEST_CUDA_IPC CUDA IPC available test_multiple_dataloaders multiprocessing_context supported_multiprocessing_contexts loader _it = iter _get_data_loader dataset num_workers= loader _it = iter _get_data_loader dataset num_workers= multiprocessing_context=multiprocessing_context next loader _it next loader _it next loader _it next loader _it next loader _it next loader _it del loader _it del loader _it This case pass Intel GPU currently expected failure other device please don t forget remove skip when remove xfailIfLinux skipIfXpu This case passes s x too please don t forget remove skip when remove xfailIfLinux unittest skipIf IS_S X Unexpectedly succeeds s x https github com pytorch pytorch issues xfailIfLinux test_segfault p = ErrorTrackingProcess target=_test_segfault p start p join JOIN_TIMEOUT try assertFalse p is_alive assertNotEqual p exitcode IS_WINDOWS assertIsInstance p exception OSError assertRegex str p exception r access violation reading assertIsInstance p exception RuntimeError assertRegex str p exception r DataLoader worker \ pid \d+\ killed signal finally p terminate Tests child process forked DataLoader segfaults due having more than threads parent process after least one set_num_threads invocation parent process After forking set_num_threads child process entails handling some inherited data-structures Caffe thread-pool parent process culminating segfault Reference https github com pytorch pytorch issues unittest skipIf IS_WINDOWS Needs fork test_no_segfault p = ErrorTrackingProcess target=_test_no_segfault p start p join JOIN_TIMEOUT try assertFalse p is_alive p exception assertIsInstance p exception RuntimeError assertRegex str p exception r DataLoader worker \ pid \d+\ killed signal fail Segfault occurred worker process after fork finally p terminate test_timeout TEST_CUDA This test runs subprocess which can only initialize CUDA spawn _test_timeout_pin_memory pin_memory=True initializes CUDA when iterator constructed targets = _test_timeout _test_timeout_pin_memory targets = _test_timeout target targets p = ErrorTrackingProcess target=target args= persistent_workers p start p join JOIN_TIMEOUT try assertFalse p is_alive assertNotEqual p exitcode assertIsInstance p exception RuntimeError assertRegex str p exception r DataLoader timed out after \d+ seconds finally p terminate test_large_sampler_indices Test data loader cleanly exit when process errors having reference iterator using sampler yields big elements s t _index_queues putters block More context https github com pytorch pytorch issues p = ErrorTrackingProcess target=_test_large_sampler_indices args= persistent_workers p start p join JOIN_TIMEOUT try assertFalse p is_alive assertNotEqual p exitcode assertIsInstance p exception RuntimeError assertRegex str p exception r My Error finally p terminate test_invalid_ctor_args_combinations general assertRaisesRegex ValueError num_workers option should non-negative _get_data_loader dataset num_workers=- assertRaisesRegex ValueError timeout option should non-negative _get_data_loader dataset timeout=- disable auto-batching assertRaisesRegex ValueError batch_size=None option disables auto-batching mutually exclusive _get_data_loader dataset batch_size=None drop_last=True valid_ctx = list torch multiprocessing get_all_start_methods - assertRaisesRegex ValueError r multi-process loading \ num_workers \ got _get_data_loader dataset num_workers= multiprocessing_context=valid_ctx assertRaisesRegex ValueError should specify valid start method _get_data_loader dataset num_workers= multiprocessing_context= bad assertRaisesRegex TypeError multiprocessing_context option should valid context _get_data_loader dataset num_workers= multiprocessing_context=object map-style sampler = torch utils data SequentialSampler dataset batch_sampler = torch utils data BatchSampler sampler False assertRaisesRegex ValueError sampler option mutually exclusive shuffle _get_data_loader dataset batch_size= sampler=sampler shuffle=True assertRaisesRegex ValueError sampler option mutually exclusive shuffle _get_data_loader dataset batch_sampler=batch_sampler sampler=sampler shuffle=True assertRaisesRegex ValueError sampler option mutually exclusive shuffle _get_data_loader dataset batch_sampler=batch_sampler sampler=sampler shuffle= assertRaisesRegex ValueError batch_sampler option mutually exclusive _get_data_loader dataset batch_size= batch_sampler=batch_sampler assertRaisesRegex ValueError batch_sampler option mutually exclusive _get_data_loader dataset shuffle=True batch_sampler=batch_sampler assertRaisesRegex ValueError batch_sampler option mutually exclusive _get_data_loader dataset drop_last=True batch_sampler=batch_sampler assertRaisesRegex ValueError batch_sampler option mutually exclusive _get_data_loader dataset drop_last= batch_sampler=batch_sampler iterable-style dataset = CountingIterableDataset assertRaisesRegex ValueError DataLoader IterableDataset expected unspecified shuffle _get_data_loader dataset shuffle=True assertRaisesRegex ValueError DataLoader IterableDataset expected unspecified shuffle _get_data_loader dataset shuffle= assertRaisesRegex ValueError DataLoader IterableDataset expected unspecified sampler _get_data_loader dataset sampler=torch utils data SequentialSampler dataset assertRaisesRegex ValueError DataLoader IterableDataset expected unspecified sampler _get_data_loader dataset sampler= assertRaisesRegex ValueError DataLoader IterableDataset expected unspecified batch_sampler _get_data_loader dataset batch_sampler=torch utils data BatchSampler torch utils data SequentialSampler dataset False assertRaisesRegex ValueError DataLoader IterableDataset expected unspecified batch_sampler _get_data_loader dataset batch_sampler= test_builtin_collection_conversion coll_ty list tuple num_workers map-style dataset dataset = CountingDataset no auto-batching fetched = coll_ty _get_data_loader dataset batch_size=None num_workers=num_workers assertEqual fetched coll_ty range auto-batching fetched = coll_ty _get_data_loader dataset batch_size= num_workers=num_workers assertEqual fetched coll_ty torch tensor i i + i range iterable-style dataset dataset = CountingIterableDataset no auto-batching fetched = coll_ty _get_data_loader dataset batch_size=None num_workers=num_workers assertEqual fetched coll_ty range auto-batching IterableDataset isn t configured each worker so equality test below valid we cannot have more than workers assert num_workers invalid test fetched = coll_ty _get_data_loader dataset batch_size= num_workers=num_workers assertEqual fetched coll_ty torch tensor i i + i range test_iterable_style_dataset no auto-batching single process loading dataset = CountingIterableDataset dataloader = _get_data_loader dataset batch_size=None fetched = list dataloader assertEqual len fetched i d enumerate fetched non-batched should convert ints into tensors assertIsInstance d int assertEqual d i DataLoader should match len iterable-style dataset implemented assertEqual len dataloader len dataset no auto-batching multiprocessing loading num_workers = sizes_for_all_workers = expected = sorted functools reduce operator iadd list range s s sizes_for_all_workers assert len sizes_for_all_workers == num_workers invalid test case prefetch_factor dataset = WorkerSpecificIterableDataset sizes_for_all_workers dataloader = _get_data_loader dataset num_workers=num_workers batch_size=None worker_init_fn=set_faulthander_if_available prefetch_factor=prefetch_factor dataloader_iter = iter dataloader fetched = sorted dataloader_iter b zip fetched expected non-batched should convert ints into tensors assertIsInstance int assertEqual b DataLoader should match len iterable-style dataset implemented assertEqual len dataloader len dataset When loading more than len dataset data after accessing len dataloader we should get warning See NOTE IterableDataset __len__ dataset = CountingIterableDataset dataloader = _get_data_loader dataset num_workers=num_workers worker_init_fn=set_faulthander_if_available prefetch_factor=prefetch_factor = iter dataloader _ range assertNotWarn lambda next Should warn before accessing len dataloader assertEqual len dataloader len dataset assertEqual len dataloader = iter dataloader _ range assertNotWarn lambda next Should warn before exceeding length _ range assertWarnsRegex UserWarning r - + samples have been fetched\ For multiprocessing data-loading msg= Should always warn after exceeding length next no auto-batching test workers exit gracefully workers = dataloader_iter _workers del dataloader_iter del dataloader try w workers w join JOIN_TIMEOUT assertFalse w is_alive assertEqual w exitcode finally w workers w terminate auto-batching single process loading dataset = CountingIterableDataset fetched = list _get_data_loader dataset batch_size= assertEqual len fetched assertEqual fetched tolist list range assertEqual fetched tolist list range assertEqual fetched tolist list range auto-batching multiprocessing loading num_workers = sizes_for_all_workers = expected = sorted functools reduce operator iadd list range s s sizes_for_all_workers assert len sizes_for_all_workers == num_workers invalid test case prefetch_factor dataset = WorkerSpecificIterableDataset sizes_for_all_workers worker should batches worker should batches worker should batches dataloader = _get_data_loader dataset num_workers=num_workers batch_size= prefetch_factor=prefetch_factor dataloader_iter = iter dataloader fetched = list dataloader_iter assertEqual len fetched fetched = tuple t tolist t fetched assertEqual fetched tuple range tuple range tuple range tuple range auto-batching test workers exit gracefully workers = dataloader_iter _workers del dataloader_iter del dataloader try w workers w join JOIN_TIMEOUT assertFalse w is_alive assertEqual w exitcode finally w workers w terminate auto-batching drop_last single process loading dataset = CountingIterableDataset fetched = list _get_data_loader dataset batch_size= drop_last=True assertEqual len fetched assertEqual fetched tolist list range assertEqual fetched tolist list range auto-batching drop_last multiprocessing loading num_workers = sizes_for_all_workers = expected = sorted functools reduce operator iadd list range s s sizes_for_all_workers assert len sizes_for_all_workers == num_workers invalid test case prefetch_factor dataset = WorkerSpecificIterableDataset sizes_for_all_workers worker should batches worker should batches worker should batches dataloader = _get_data_loader dataset num_workers=num_workers batch_size= drop_last=True worker_init_fn=set_faulthander_if_available prefetch_factor=prefetch_factor dataloader_iter = iter dataloader fetched = list dataloader_iter assertEqual len fetched fetched = tuple t tolist t fetched assertEqual fetched tuple range tuple range auto-batching drop_last test workers exit gracefully workers = dataloader_iter _workers del dataloader_iter del dataloader try w workers w join JOIN_TIMEOUT assertFalse w is_alive assertEqual w exitcode finally w workers w terminate test_chain_iterable_style_dataset chaining concatenation dataset = CountingIterableDataset dataset = CountingIterableDataset expected = list range + list range num_workers chained_dataset dataset + dataset ChainDataset dataset dataset fetched = list _get_data_loader chained_dataset num_workers=num_workers assertEqual len fetched len expected e d zip expected fetched assertIsInstance d torch Tensor assertEqual e d assertRaisesRegex AssertionError ChainDataset only supports IterableDataset list iter dataset + dataset assertRaisesRegex AssertionError ChainDataset only supports IterableDataset list iter ChainDataset dataset dataset unittest skipIf TEST_CUDA_IPC CUDA IPC available test_multiprocessing_contexts reference = torch arange torch arange torch arange torch arange counting_ds_n = dl_common_args = dict num_workers= batch_size= pin_memory= TEST_CUDA ctx supported_multiprocessing_contexts windows jetson devices don t support sharing cuda tensor ROCm does yet fully support IPC ctx spawn forkserver TEST_CUDA IS_WINDOWS IS_JETSON ds_cls = CUDACountingDataset ds_cls = CountingDataset assertEqual reference list _get_data_loader ds_cls counting_ds_n multiprocessing_context=ctx dl_common_args ctx None test ctx object ctx = mp get_context ctx assertEqual reference list _get_data_loader ds_cls counting_ds_n multiprocessing_context=ctx dl_common_args _test_multiprocessing_iterdatapipe with_dill Testing make sure function global scope e g imported library can serialized used multiprocess DataLoader reference = torch as_tensor dtype=torch int torch as_tensor dtype=torch int datapipe IterDataPipe = IterableWrapper datapipe = datapipe map row_processor datapipe = datapipe filter lambda row len row == with_dill datapipe filter filter_len dl_common_args = dict num_workers= batch_size= shuffle=True pin_memory= TEST_CUDA ctx supported_multiprocessing_contexts assertEqual reference t type torch int t _get_data_loader datapipe multiprocessing_context=ctx dl_common_args ctx None test ctx object ctx = mp get_context ctx assertEqual reference t type torch int t _get_data_loader datapipe multiprocessing_context=ctx dl_common_args skipIfNoNumpy unittest skipIf TEST_CUDA_IPC CUDA IPC available test_multiprocessing_iterdatapipe _test_multiprocessing_iterdatapipe with_dill=False unittest expectedFailure skipIfNoNumpy unittest skipIf TEST_CUDA_IPC CUDA IPC available skipIfNoDill test_multiprocessing_iterdatapipe_with_dill _test_multiprocessing_iterdatapipe with_dill=True test_worker_seed num_workers = batch_size = dataset = SynchronizedSeedDataset num_workers batch_size num_workers dataloader = _get_data_loader dataset batch_size=batch_size num_workers=num_workers seeds = set seeds update batch batch dataloader assertEqual len seeds num_workers test_worker_seed_reproducibility get_dataloader DataLoader dataset batch_size=batch_size num_workers=num_workers generator=torch Generator manual_seed num_workers = batch_size = dataset = SynchronizedSeedDataset num_workers batch_size num_workers assertEqual int batch batch get_dataloader int batch batch get_dataloader test_multi_epochs_reproducibility num_workers = batch_size = num_epochs = dataset = TestMultiEpochDataset batch_size num_workers dataloader = _get_data_loader dataset batch_size=batch_size shuffle=False num_workers=num_workers _ range num_epochs batch_idx sample enumerate dataloader assertEqual sample tolist batch_idx num_workers batch_size test_worker_init_fn dataset = SeedDataset dataloader = _get_data_loader dataset batch_size= num_workers= worker_init_fn=init_fn batch dataloader assertEqual batch assertEqual batch test_get_worker_info p = ErrorTrackingProcess target=_test_get_worker_info p start p join JOIN_TIMEOUT try assertFalse p is_alive assertEqual p exitcode finally p terminate test_shuffle _test_shuffle _get_data_loader dataset shuffle=True test_shuffle_batch_none _test_shuffle DataLoader dataset batch_size=None shuffle=True test_shuffle_batch _test_shuffle _get_data_loader dataset batch_size= shuffle=True test_shuffle_reproducibility fn lambda DataLoader dataset shuffle=True num_workers= generator=torch Generator manual_seed lambda DataLoader dataset shuffle=True num_workers= generator=torch Generator manual_seed assertEqual list fn list fn test_sequential_workers _test_sequential _get_data_loader dataset num_workers= test_seqential_batch_workers _test_sequential _get_data_loader dataset batch_size= num_workers= test_seqential_batch_workers_prefetch _test_sequential DataLoader dataset batch_size= num_workers= prefetch_factor= test_shuffle_workers _test_shuffle _get_data_loader dataset shuffle=True num_workers= test_shuffle_batch_workers _test_shuffle _get_data_loader dataset batch_size= shuffle=True num_workers= test_shuffle_batch_workers_prefetch _test_shuffle DataLoader dataset batch_size= shuffle=True num_workers= prefetch_factor= test_random_sampler collections Counter torch utils data RandomSampler sample_stat sampler num_samples counts = Counter sampler count_repeated = sum val val counts values count_repeated min counts keys max counts keys sum counts values test sample replacement n = len dataset + ensure least one sample drawn more than once sampler_with_replacement = RandomSampler dataset replacement=True num_samples=n count_repeated minval maxval count_total = sample_stat sampler_with_replacement n assertTrue count_repeated assertTrue minval = assertTrue maxval len dataset assertTrue count_total == n test sample without replacement without specified num_samples sampler_without_replacement = RandomSampler dataset count_repeated minval maxval count_total = sample_stat sampler_without_replacement len dataset assertTrue count_repeated == assertTrue minval == assertTrue maxval == len dataset - assertTrue count_total == len dataset test sample without replacement specified num_samples n = len dataset sampler_without_replacement = RandomSampler dataset num_samples=n count_repeated minval maxval count_total = sample_stat sampler_without_replacement len dataset assertTrue count_repeated == len dataset assertTrue minval == assertTrue maxval == len dataset - assertTrue count_total == n n = len dataset - sampler_without_replacement = RandomSampler dataset num_samples=n count_repeated minval maxval count_total = sample_stat sampler_without_replacement len dataset assertTrue count_repeated == assertTrue minval = assertTrue maxval len dataset assertTrue count_total == n n = len dataset + sampler_without_replacement = RandomSampler dataset num_samples=n count_repeated minval maxval count_total = sample_stat sampler_without_replacement len dataset assertTrue count_repeated == assertTrue minval == assertTrue maxval == len dataset - assertTrue count_total == n raise error when replacement non-boolean assertRaisesRegex TypeError replacement should boolean value got replacement= RandomSampler dataset replacement= test_random_sampler_len_with_replacement torch utils data RandomSampler add extra samples num_samples = len dataset + sampler = RandomSampler dataset replacement=True num_samples=num_samples test len method assertEqual num_samples len sampler test iteration count_num_samples = sum _ sampler assertEqual num_samples count_num_samples test dataloader batch_size = batch_size = count_num_samples_in_data_loader = len _get_data_loader dataset batch_size=batch_size sampler=sampler assertEqual num_samples count_num_samples_in_data_loader test dataloader batch_size = batch_size = count_num_samples_in_data_loader = len _get_data_loader dataset batch_size=batch_size sampler=sampler assertEqual int math ceil float num_samples batch_size count_num_samples_in_data_loader test_random_sampler_len_without_replacement torch utils data RandomSampler add extra samples num_samples = len dataset + sampler = RandomSampler dataset replacement=False num_samples=num_samples test len method assertEqual num_samples len sampler test iteration count_num_samples = sum _ sampler assertEqual num_samples count_num_samples test dataloader batch_size = batch_size = count_num_samples_in_data_loader = len _get_data_loader dataset batch_size=batch_size sampler=sampler assertEqual num_samples count_num_samples_in_data_loader test dataloader batch_size = batch_size = count_num_samples_in_data_loader = len _get_data_loader dataset batch_size=batch_size sampler=sampler assertEqual num_samples batch_size + num_samples batch_size count_num_samples_in_data_loader test_distributed_sampler_invalid_rank torch utils data distributed DistributedSampler dataset = torch IntTensor range assertRaisesRegex ValueError Invalid rank sampler = DistributedSampler dataset assertRaisesRegex ValueError Invalid rank sampler = DistributedSampler dataset - test_duplicating_data_with_drop_last torch utils data distributed DistributedSampler num_processes = num_batches = data_set = torch IntTensor range num_batches scanned_data = torch IntTensor i range num_processes s = DistributedSampler data_set num_processes i d_loader = _get_data_loader data_set batch_size=int num_batches num_processes drop_last=True sampler=s data d_loader scanned_data = torch cat scanned_data data assertEqual scanned_data size scanned_data unique size test_sampler_reproducibility torch utils data RandomSampler SubsetRandomSampler WeightedRandomSampler weights = fn lambda RandomSampler dataset num_samples= replacement=True generator=torch Generator manual_seed lambda RandomSampler dataset replacement=False generator=torch Generator manual_seed lambda WeightedRandomSampler weights num_samples= replacement=True generator=torch Generator manual_seed lambda WeightedRandomSampler weights num_samples= replacement=False generator=torch Generator manual_seed lambda SubsetRandomSampler range generator=torch Generator manual_seed assertEqual list fn list fn sampler RandomSampler dataset num_samples= replacement=True RandomSampler dataset replacement=False WeightedRandomSampler weights num_samples= replacement=True WeightedRandomSampler weights num_samples= replacement=False SubsetRandomSampler range torch manual_seed l = list sampler + list sampler torch manual_seed l = list sampler + list sampler assertEqual l l its = iter sampler iter sampler ls = idx range len sampler i range idx == torch manual_seed ls i append next its i assertEqual ls ls _test_sampler kwargs indices = range using regular iterable dl = _get_data_loader dataset sampler=indices batch_size= kwargs assertEqual len dl i input _target enumerate dl assertEqual len input assertEqual input data i + i + test_sampler _test_sampler _test_sampler num_workers= _test_batch_sampler num_workers= multiprocessing_context= spawn _test_batch_sampler kwargs batches = using regular iterable i range batches append tuple range i i + batches append tuple range i + i + dl = _get_data_loader dataset batch_sampler=batches kwargs assertEqual len dl i input _target enumerate dl i == offset = i assertEqual len input assertEqual input data offset offset + offset = i assertEqual len input assertEqual input data offset offset + test_batch_sampler _test_batch_sampler _test_batch_sampler num_workers= _test_batch_sampler num_workers= multiprocessing_context= spawn unittest skipIf TEST_CUDA CUDA unavailable test_shuffle_pin_memory loader = _get_data_loader dataset batch_size= shuffle=True num_workers= pin_memory=True input target loader assertTrue input is_pinned assertTrue target is_pinned unittest skipIf TEST_NUMPY numpy unavailable test_numpy numpy np TestDataset torch utils data Dataset __getitem__ i np ones i __len__ loader = _get_data_loader TestDataset batch_size= batch = next iter loader assertIsInstance batch torch DoubleTensor assertEqual batch size torch Size unittest skipIf TEST_NUMPY numpy unavailable test_numpy_gen_state torch utils data _utils worker _generate_state Using NumPy generated states reference test ` _generate_state ` having same result Test case worker_id base_seed expected_state test_cases = worker_id base_seed exp test_cases assertEqual exp _generate_state base_seed worker_id test_error _test_error _get_data_loader ErrorDataset batch_size= shuffle=True test_error_workers _test_error _get_data_loader ErrorDataset batch_size= shuffle=True num_workers= unittest skipIf IS_WINDOWS FIXME stuck test test_partial_workers r Check workers exit even iterator exhausted TEST_CUDA pin_memory_configs = True False pin_memory_configs = False pin_memory pin_memory_configs loader = iter _get_data_loader dataset batch_size= num_workers= pin_memory=pin_memory workers = loader _workers pin_memory pin_memory_thread = loader _pin_memory_thread i _ enumerate loader i == break assert i == del loader w workers w join JOIN_TIMEOUT assertFalse w is_alive subprocess terminated pin_memory pin_memory_thread join JOIN_TIMEOUT assertFalse pin_memory_thread is_alive Takes min finish see https github com pytorch pytorch issues unittest skipIf HAS_PSUTIL psutil found slowTest test_proper_exit There might ConnectionResetError leaked semaphore warning due dirty process exit they all safe ignore TODO test case where pin_memory_thread triggers error fatal signal I haven t found out how properly do is_iterable_dataset use_workers pin_memory hold_iter_reference itertools product True False repeat= ` hold_iter_reference ` specifies whether we hold reference iterator This interesting because Python error traces holds reference frames which hold references all local variables including iterator then iterator dtor may called before process end It important see processes still exit both cases pin_memory TEST_CUDA IS_WINDOWS This test runs subprocess which can only initialize CUDA spawn DataLoader pin_memory=True initializes CUDA when its iterator constructed For windows pin_memory sometimes causes CUDA oom continue ` exit_method ` controls way loader process ends - ` _kill ` means ` ` killed OS - ` _error ` means ` ` raises error - ` None ` means no error happens In all cases all processes should end properly use_workers TODO Fix test loader_kill would cause running out shared memory Killing loader process would prevent DataLoader iterator clean up all queues worker processes exit_methods = None loader_error worker_error worker_kill persistent_workers = persistent_workers exit_methods = None loader_error loader_kill persistent_workers = False exit_method exit_methods exit_method == worker_kill FIXME This sometimes hangs See continue desc = desc append f is_iterable_dataset= is_iterable_dataset desc append f use_workers= use_workers desc append f pin_memory= pin_memory desc append f hold_iter_reference= hold_iter_reference desc append f exit_method= exit_method desc = test_proper_exit + join desc Event loader process uses signal testing process various things setup including worker pids specified ` worker_pids ` array loader_setup_event = mp Event Event process has finished setting up loader process can now proceed trigger error events finish normally tester_setup_event = mp Event loader_p = ErrorTrackingProcess target=_test_proper_exit args= is_iterable_dataset use_workers pin_memory exit_method hold_iter_reference loader_setup_event tester_setup_event persistent_workers disable_stderr=False loader_p start loader_psutil_p = psutil Process loader_p pid Wait loader process set everything up e g starting workers loader_setup_event wait timeout=JOIN_TIMEOUT loader_setup_event is_set fail_msg = desc + loader process failed setup within given time loader_p exception None fail_msg += f had exception loader_p exception loader_p is_alive fail_msg += f exited code loader_p exitcode had no exception fail_msg += still alive loader_p is_alive may kill process needs run after above lines loader_p print_traces_of_all_threads fail fail_msg We certain workers have started now worker_psutil_ps = loader_psutil_p children fail reason report_psutil_attrs = pid name cpu_times io_counters memory_full_info num_ctx_switches open_files threads status nice ionice reason None err_msg = desc err_msg = f desc reason err_msg += \nLoader info \n\t loader_psutil_p is_running err_msg += str loader_psutil_p as_dict attrs=report_psutil_attrs may kill process needs run after above line loader_p print_traces_of_all_threads err_msg += f exited code loader_p exitcode use_workers err_msg += \nWorker s info idx worker_psutil_p enumerate worker_psutil_ps err_msg += f \n\tWorker idx \n\t\t worker_psutil_p is_running err_msg += str worker_psutil_p as_dict attrs=report_psutil_attrs may kill process needs run after above line print_traces_of_all_threads worker_psutil_p pid err_msg += exited unknown code fail err_msg tester_setup_event set try loader_p join JOIN_TIMEOUT + MP_STATUS_CHECK_INTERVAL loader_p is_alive fail_reason = loader process did terminate loader_p exception None fail fail_reason + f had exception loader_p exception fail fail_reason + had no exception _ alive = psutil wait_procs worker_psutil_ps timeout= MP_STATUS_CHECK_INTERVAL + JOIN_TIMEOUT len alive fail worker process pid s did terminate format join str p pid p alive exit_method None loader_p exitcode = fail f loader process had nonzero exitcode loader_p exitcode loader_p exitcode == fail loader process had zero exitcode exit_method == loader_error isinstance loader_p exception RuntimeError Loader error str loader_p exception fail f loader process did raise expected exception had loader_p exception exit_method == worker_kill isinstance loader_p exception RuntimeError DataLoader worker pid str loader_p exception fail f loader process did raise expected exception had loader_p exception isinstance loader_p exception ConnectionRefusedError Sometimes when worker being killed freeing its resources unpickling loader process will met ` ConnectionRefusedError ` can open socket receive resource In such cases worker may have fully exited loader can t know via ` is_alive ` check ` SIGCHLD ` handler So we permit allowed error well After all we happy long terminates pass fail f loader process did raise expected exception had loader_p exception exit_method == worker_error isinstance loader_p exception RuntimeError Worker error str loader_p exception fail f loader process did raise expected exception had loader_p exception finally loader_p terminate test_len check_len dl expected assertEqual len dl expected n = _ dl n += assertEqual n expected check_len dataset check_len _get_data_loader dataset batch_size= check_len _get_data_loader dataset batch_size= test_iterabledataset_len IterableDataset torch utils data IterableDataset __len__ __iter__ iter range iterable_loader = DataLoader IterableDataset batch_size= assertEqual len iterable_loader iterable_loader = DataLoader IterableDataset batch_size= drop_last=True assertEqual len iterable_loader iterable_loader = DataLoader IterableDataset batch_size= assertEqual len iterable_loader iterable_loader = DataLoader IterableDataset batch_size= drop_last=True assertEqual len iterable_loader iterable_loader = DataLoader IterableDataset batch_size= assertEqual len iterable_loader iterable_loader = DataLoader IterableDataset batch_size= drop_last=True assertEqual len iterable_loader unittest skipIf TEST_NUMPY numpy unavailable test_numpy_scalars numpy np ScalarDataset torch utils data Dataset __init__ dtype dtype = dtype __getitem__ i dtype __len__ dtypes = np float torch DoubleTensor np float torch FloatTensor np float torch HalfTensor np int torch LongTensor np int torch IntTensor np int torch ShortTensor np int torch CharTensor np uint torch ByteTensor dt tt dtypes items dset = ScalarDataset dt loader = _get_data_loader dset batch_size= batch = next iter loader assertIsInstance batch tt test_default_convert_mapping_keep_type data = CustomDict b converted = dataloader default_convert data assertEqual converted data test_default_convert_sequence_keep_type data = CustomList converted = dataloader default_convert data assertEqual converted data test_default_convert_sequence_dont_keep_type data = range converted = dataloader default_convert data assertEqual converted test_default_collate_dtype arr = - collated = dataloader default_collate arr assertEqual collated torch tensor arr assertEqual collated dtype torch int arr = - collated = dataloader default_collate arr assertEqual collated torch tensor arr dtype=torch float arr = True False collated = dataloader default_collate arr assertEqual collated torch tensor arr assertEqual collated dtype torch bool Should no-op arr = b c assertEqual arr dataloader default_collate arr test_default_collate_mapping_keep_type batch = CustomDict b CustomDict b collated = dataloader default_collate batch expected = CustomDict torch tensor b torch tensor assertEqual collated expected test_default_collate_sequence_keep_type batch = CustomList CustomList collated = dataloader default_collate batch expected = CustomList torch tensor torch tensor torch tensor assertEqual collated expected test_default_collate_sequence_dont_keep_type batch = range range collated = dataloader default_collate batch assertEqual collated torch tensor torch tensor unittest skipIf TEST_NUMPY numpy unavailable test_default_collate_bad_numpy_types numpy np Should no-op arr = np array b c assertEqual arr dataloader default_collate arr arr = np array b c assertRaises TypeError lambda dataloader default_collate arr arr = np array object object object assertRaises TypeError lambda dataloader default_collate arr arr = np array object object object assertRaises TypeError lambda dataloader default_collate arr unittest skipIf TEST_NUMPY numpy unavailable test_default_collate_numpy_memmap numpy np tempfile TemporaryFile f arr = np array arr_memmap = np memmap f dtype=arr dtype mode= w+ shape=arr shape arr_memmap = arr arr_new = np memmap f dtype=arr dtype mode= r shape=arr shape tensor = dataloader default_collate list arr_new assertTrue tensor == tensor new_tensor all item test_default_collate_bad_sequence_type batch = X X X assertRaises RuntimeError lambda dataloader default_collate batch assertRaises RuntimeError lambda dataloader default_collate batch - unittest skipIf TEST_NUMPY numpy unavailable test_default_collate_shared_tensor numpy np t_in = torch zeros n_in = np zeros assertEqual t_in is_shared False assertEqual dataloader default_collate t_in is_shared False assertEqual dataloader default_collate n_in is_shared False FIXME fix following hack makes ` default_collate ` believe worker process since tests ` get_worker_info = None ` even though old = _utils worker _worker_info try _utils worker _worker_info = x assertEqual dataloader default_collate t_in is_shared True assertEqual dataloader default_collate n_in is_shared True finally _utils worker _worker_info = old test_excessive_thread_creation_warning assertWarnsRegex UserWarning r excessive worker creation might get DataLoader running slow even freeze dataloader = DataLoader dataset batch_size= num_workers= TestDataLoaderDeviceType TestCase parametrize context ctx ctx supported_multiprocessing_contexts ctx None unittest skipIf TEST_CUDA_IPC CUDA IPC available test_nested_tensor_multiprocessing device context The fork multiprocessing context doesn t work CUDA so skip cuda device context == fork skipTest f context multiprocessing context supported device dataset = torch nested nested_tensor torch randn device=device _ range pin_memory_settings = False device == cpu torch cuda is_available pin_memory_settings append True pin_memory pin_memory_settings loader = torch utils data DataLoader dataset batch_size= num_workers= collate_fn=_clone_collate pin_memory=pin_memory multiprocessing_context=context i batch enumerate loader assertEqual batch dataset i Error case default collate_fn doesn t currently support batches nested tensors Following current semantics we d need stack them which isn t possible atm assertRaisesRegex RuntimeError currently supported default collate_fn loader = torch utils data DataLoader dataset batch_size= num_workers= multiprocessing_context=context next iter loader parametrize context ctx ctx supported_multiprocessing_contexts ctx None unittest skipIf TEST_CUDA_IPC CUDA IPC available test_sparse_tensor_multiprocessing device context The fork multiprocessing context doesn t work CUDA so skip cuda device context == fork skipTest f context multiprocessing context supported device dataset = torch randn to_sparse device _ range pin_memory_settings = False device == cpu torch cuda is_available pin_memory_settings append True pin_memory pin_memory_settings loader = torch utils data DataLoader dataset batch_size= num_workers= collate_fn=_sparse_coo_collate pin_memory=pin_memory multiprocessing_context=context i batch enumerate loader assertEqual batch dataset i IntegrationTestDataLoaderDataPipe TestCase r Verify behavior certain ` ` DataPipes ` ` ` ` DataLoader ` ` test_shuffler_iterdatapipe r Verify ` ` IterDataPipe shuffle ` ` controlled ` ` DataLoader ` ` generate different seeds deterministically per epoch exp = list range _create_dp buffer_size input_ds = dp iter IterableWrapper exp input_ds shuffle buffer_size=buffer_size sharding_filter bs Test Deterministic num_workers pw itertools product True False num_workers == pw continue shuffle_dp = _create_dp bs mp_ctx = spawn num_workers None dl = DataLoader shuffle_dp num_workers=num_workers shuffle=True multiprocessing_context=mp_ctx persistent_workers=pw No seed dl_res_ns = list dl assertEqual sorted dl_res_ns exp Same seeds dl_res = _epoch range torch manual_seed dl_res append list dl assertEqual dl_res dl_res assertEqual sorted dl_res exp Different seeds torch manual_seed dl_res append list dl assertEqual len dl_res len dl_res assertNotEqual dl_res dl_res assertEqual sorted dl_res sorted dl_res dl _iterator None dl _iterator _shutdown_workers dl _iterator = None del dl StringDataset Dataset __init__ - None s = __len__ len s __getitem__ ndx s ndx ndx unittest skipIf TEST_WITH_TSAN Fails TSAN following error starting new threads after multi-threaded fork supported Dying set die_after_fork= override TestStringDataLoader TestCase setUp super setUp dataset = StringDataset unittest skipIf TEST_CUDA CUDA unavailable test_shuffle_pin_memory loader = DataLoader dataset batch_size= shuffle=True num_workers= pin_memory=True s n loader assertIsInstance s str assertTrue n is_pinned DictDataset Dataset __len__ __getitem__ ndx a_tensor torch empty fill_ ndx another_dict a_number ndx unittest skipIf TEST_WITH_TSAN Fails TSAN following error starting new threads after multi-threaded fork supported Dying set die_after_fork= override TestDictDataLoader TestCase setUp super setUp dataset = DictDataset test_sequential_batch persistent_workers False True persistent_workers loader = DataLoader dataset batch_size= shuffle=False persistent_workers=persistent_workers num_workers= loader = DataLoader dataset batch_size= shuffle=False persistent_workers=persistent_workers batch_size = loader batch_size i sample enumerate loader idx = i batch_size assertEqual set sample keys a_tensor another_dict assertEqual set sample another_dict keys a_number t = sample a_tensor assertEqual t size torch Size batch_size assertTrue t == idx all assertTrue t == idx + all n = sample another_dict a_number assertEqual n size torch Size batch_size assertEqual n idx assertEqual n idx + unittest skipIf TEST_CUDA CUDA unavailable test_pin_memory loader = DataLoader dataset batch_size= pin_memory=True sample loader assertTrue sample a_tensor is_pinned assertTrue sample another_dict a_number is_pinned skipIfXpu unittest skipIf TEST_CUDA Test when CUDA available test_pin_memory_no_cuda loader = DataLoader dataset batch_size= pin_memory=True sample loader assertFalse sample a_tensor is_pinned assertFalse sample another_dict a_number is_pinned unittest skipIf TEST_CUDA CUDA unavailable test_pin_memory_device loader = DataLoader dataset batch_size= pin_memory=True pin_memory_device= cuda sample loader assertTrue sample a_tensor is_pinned assertTrue sample another_dict a_number is_pinned unittest skipIf TEST_CUDA CUDA unavailable test_pin_memory_with_only_device loader = DataLoader dataset batch_size= pin_memory_device= cuda sample loader assertFalse sample a_tensor is_pinned assertFalse sample another_dict a_number is_pinned DummyDataset torch utils data Dataset __init__ - None data = list range __len__ len data __getitem__ idx torch is_tensor idx idx = idx tolist The persistent workers always maintain original dataset through dataloader lifetime so attributes will remain same first time workers where spawned dataloader iteration assert start == data idx unittest skipIf TEST_WITH_TSAN Fails TSAN following error starting new threads after multi-threaded fork supported Dying set die_after_fork= override TestDataLoaderPersistentWorkers TestDataLoader setUp super setUp persistent_workers = True unittest skipIf IS_SANDCASTLE subprocess doesn t work FB internal CI unittest skipIf IS_WINDOWS No resource module Windows test_fd_limit_exceeded See NOTE DataLoader Linux open files limit subprocess subprocess check_output sys executable -c \ torch resource torch utils data DataLoader IterableDataset RandomDataset IterableDataset __init__ len size super RandomDataset __init__ len = len size = size __iter__ __next__ len = raise StopIteration len -= torch randn size try keep_fds_alive = resource setrlimit resource RLIMIT_NOFILE random_t DataLoader RandomDataset multiprocessing_context= fork num_workers= persistent_workers=True random_t max dim= keep_fds_alive append random_t except RuntimeError e assert ulimit -n str e assert set_sharing_strategy str e test_dataset_not_reset dataset = DummyDataset pin_memory_configs = False TEST_CUDA pin_memory_configs append True pin_memory pin_memory_configs dataloader = _get_data_loader dataset num_workers= pin_memory=pin_memory dataset start = i range _ dataloader pass Changing start value here doesn t have any effect dataset cached workers since they recreated between epochs can cache values safely dataset start = i unittest skipIf IS_SANDCASTLE subprocess doesn t work FB internal CI unittest skipIf IS_WINDOWS Needs fork test_early_exit subprocess proc = subprocess check_output sys executable -c \ torch torch utils data DataLoader IterableDataset RandomDataset IterableDataset __init__ len size super RandomDataset __init__ len = len size = size __iter__ __next__ len = raise StopIteration len -= torch randn size __name__ == __main__ dl = DataLoader RandomDataset batch_size= num_workers= pin_memory=True persistent_workers=True multiprocessing_context= fork _ dl break NamedTupleDataset Dataset collections namedtuple Batch = namedtuple Batch data label random_tensor Data = namedtuple Data positive negative __len__ __getitem__ ndx Batch data=self Data positive=ndx negative=-ndx label=str ndx random_tensor=torch randn unittest skipIf TEST_WITH_TSAN Fails TSAN following error starting new threads after multi-threaded fork supported Dying set die_after_fork= override TestNamedTupleDataLoader TestCase setUp super setUp dataset = NamedTupleDataset test_dataloader_with_namedtuple auto-collation loader = DataLoader dataset batch_size= pin_memory=TEST_CUDA batch loader assertIsInstance batch NamedTupleDataset Batch assertEqual batch random_tensor is_pinned TEST_CUDA assertIsInstance batch data NamedTupleDataset Data assertIsInstance batch data positive torch Tensor assertEqual batch data positive is_pinned TEST_CUDA no auto-collation loader = DataLoader dataset batch_size=None pin_memory=TEST_CUDA batch loader assertIsInstance batch NamedTupleDataset Batch assertEqual batch random_tensor is_pinned TEST_CUDA assertIsInstance batch data NamedTupleDataset Data assertNotIsInstance batch data positive torch Tensor SimpleCustomBatch __init__ data transposed_data = list zip data inp = torch stack transposed_data tgt = torch stack transposed_data pin_memory inp = inp pin_memory tgt = tgt pin_memory is_pinned inp is_pinned tgt is_pinned Workaround https github com pytorch pytorch issues Classes ` __main__ ` can correctly unpickled spawned module See https docs python org library multiprocessing html#multiprocessing-programming self_module = __import__ os path splitext os path basename __file__ collate_wrapper batch self_module SimpleCustomBatch batch collate_into_packed_sequence batch data = torch stack sample sample batch t b = data size lengths = torch randint t size= b dtype=torch int torch nn utils rnn pack_padded_sequence data lengths enforce_sorted=False collate_into_packed_sequence_batch_first batch data = torch stack sample sample batch b t = data size lengths = torch randint t size= b dtype=torch int torch nn utils rnn pack_padded_sequence data lengths batch_first=True enforce_sorted=False unittest skipIf TEST_WITH_TSAN Fails TSAN following error starting new threads after multi-threaded fork supported Dying set die_after_fork= override TestCustomPinFn TestCase setUp super setUp inps = torch arange dtype=torch float view tgts = torch arange dtype=torch float view dataset = TensorDataset inps tgts unittest skipIf TEST_CUDA CUDA unavailable test_custom_batch_pin test_cases = collate_wrapper self_module SimpleCustomBatch collate_into_packed_sequence torch nn utils rnn PackedSequence collate_into_packed_sequence_batch_first torch nn utils rnn PackedSequence collate_fn elem_cls test_cases loader = DataLoader dataset batch_size= collate_fn=collate_fn pin_memory=True sample loader assertIsInstance sample elem_cls assertTrue sample is_pinned unittest skipIf TEST_CUDA CUDA unavailable test_custom_batch_pin_worker test_cases = collate_wrapper self_module SimpleCustomBatch collate_into_packed_sequence torch nn utils rnn PackedSequence collate_into_packed_sequence_batch_first torch nn utils rnn PackedSequence collate_fn elem_cls test_cases loader = DataLoader dataset batch_size= collate_fn=collate_fn pin_memory=True num_workers= sample loader assertIsInstance sample elem_cls assertTrue sample is_pinned TestWorkerQueueDataset Dataset __init__ data data = data worker_id = None worker_init_fn worker_id worker_id = worker_id __getitem__ item worker_id data item __len__ len data unittest skipIf TEST_WITH_TSAN Fails TSAN following error starting new threads after multi-threaded fork supported Dying set die_after_fork= override TestIndividualWorkerQueue TestCase setUp super setUp dataset = TestWorkerQueueDataset list range _run_ind_worker_queue_test batch_size num_workers loader = DataLoader dataset batch_size=batch_size shuffle=False num_workers=num_workers timeout= worker_init_fn=self dataset worker_init_fn current_worker_idx = i worker_ids sample enumerate loader assertEqual worker_ids tolist current_worker_idx batch_size assertEqual sample tolist list range i batch_size i + batch_size current_worker_idx += current_worker_idx == num_workers current_worker_idx = unittest skipIf IS_WINDOWS IS_MACOS Flaky Windows MacOS https github com pytorch pytorch issues test_ind_worker_queue max_num_workers = None hasattr os sched_getaffinity try max_num_workers = len os sched_getaffinity except Exception pass max_num_workers None cpu_count = os cpu_count cpu_count None Use half number CPUs max_num_workers = cpu_count max_num_workers None max_num_workers = batch_size num_workers range min max_num_workers _run_ind_worker_queue_test batch_size=batch_size num_workers=num_workers + SetAffinityDataset IterableDataset __iter__ torch randperm after = os sched_getaffinity iter after unittest skipIf hasattr os sched_setaffinity os sched_setaffinity available TestSetAffinity TestCase test_set_affinity_in_worker_init Query current affinity mask avoid setting disallowed one old_affinity = os sched_getaffinity old_affinity skipTest No affinity information Choose any expected_affinity = list old_affinity - worker_set_affinity _ os sched_setaffinity expected_affinity dataset = SetAffinityDataset dataloader = torch utils data DataLoader dataset num_workers= worker_init_fn=worker_set_affinity sample dataloader assertEqual sample expected_affinity ConvDataset Dataset __init__ - None x = torch ones Call convolution parent process __len__ __getitem__ index torch nn functional conv d x torch ones unittest skipIf IS_WINDOWS Needs fork TestConvAfterFork TestCase Tests crash reported https github com pytorch pytorch issues test_conv_after_fork loader = DataLoader ConvDataset num_workers= x loader assertEqual x shape TestSlowIndexDataset Dataset __init__ end int slow_index int end = end slow_index = slow_index _worker_id = None __getitem__ idx _worker_id worker_info = torch utils data get_worker_info _worker_id = worker_info id idx == slow_index time sleep _worker_id idx __len__ end TestSlowIterableDataset IterableDataset __init__ start int end int start = start end = end mid = math ceil end - start give_data worker_id iter_start iter_end i range iter_start iter_end i == mid time sleep yield worker_id i __iter__ worker_info = torch utils data get_worker_info per_worker = int math ceil end - start float worker_info num_workers worker_id = worker_info id iter_start = start + worker_id per_worker iter_end = min iter_start + per_worker end give_data worker_id iter_start iter_end TestOutOfOrderDataLoader TestCase test_in_order_index_ds dataset = TestSlowIndexDataset end= slow_index= dataloader = torch utils data DataLoader dataset num_workers= in_order=True expected_worker_ids = expected_data = outputs = list dataloader worker_ids = o o outputs data = o o outputs assertEqual expected_worker_ids worker_ids assertEqual expected_data data test_out_of_order_index_ds dataset = TestSlowIndexDataset end= slow_index= dataloader = torch utils data DataLoader dataset num_workers= prefetch_factor= in_order=False worker_id = gets stuck also has s queue due prefetch_factor being makes test more deterministic will last elements expected_worker_ids = expected_data = outputs = list dataloader worker_ids = o item o outputs data = o item o outputs assertEqual expected_worker_ids worker_ids assertNotEqual data list range assertEqual expected_data data test_in_order_iterable_ds dataset = TestSlowIterableDataset start= end= dataloader = torch utils data DataLoader dataset num_workers= in_order=True expected_worker_ids = expected_data = outputs = list dataloader worker_ids = o o outputs data = o o outputs assertEqual expected_worker_ids worker_ids assertEqual expected_data data test_out_of_order_iterable_ds dataset = TestSlowIterableDataset start= end= dataloader = torch utils data DataLoader dataset num_workers= in_order=False worker has worker has index slow so expect all worker before worker expected_worker_ids = expected_data = outputs = list dataloader worker_ids = o o outputs data = o o outputs assertEqual expected_worker_ids worker_ids assertEqual sum worker_ids assertNotEqual data assertEqual expected_data data instantiate_device_type_tests TestDataLoaderDeviceType globals __name__ == __main__ run_tests