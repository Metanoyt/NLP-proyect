mypy allow-untyped-defs r Functional interface math torch Tensor adadelta adadelta type ignore attr-defined noqa F adagrad _make_sparse adagrad type ignore attr-defined noqa F adam adam type ignore attr-defined noqa F adamax adamax type ignore attr-defined noqa F adamw adamw type ignore attr-defined noqa F asgd asgd type ignore attr-defined noqa F nadam nadam type ignore attr-defined noqa F radam radam type ignore attr-defined noqa F rmsprop rmsprop type ignore attr-defined noqa F rprop rprop type ignore attr-defined noqa F sgd sgd type ignore attr-defined noqa F TODO use foreach API optim _functional do all computation sparse_adam params list Tensor grads list Tensor exp_avgs list Tensor exp_avg_sqs list Tensor state_steps list int eps float beta float beta float lr float maximize bool r Functional API performs Sparse Adam algorithm computation See ` ~torch optim SparseAdam ` details i param enumerate params grad = grads i grad = grad maximize -grad grad = grad coalesce update non-linear so indices must unique grad_indices = grad _indices grad_values = grad _values grad_values numel == Skip update empty grad continue size = grad size exp_avg = exp_avgs i exp_avg_sq = exp_avg_sqs i step = state_steps i make_sparse values constructor = grad new grad_indices dim == values dim == constructor resize_as_ grad constructor grad_indices values size Decay first second moment running average coefficient old - b old + - b new == old += - b new - old old_exp_avg_values = exp_avg sparse_mask grad _values exp_avg_update_values = grad_values sub old_exp_avg_values mul_ - beta exp_avg add_ make_sparse exp_avg_update_values old_exp_avg_sq_values = exp_avg_sq sparse_mask grad _values exp_avg_sq_update_values = grad_values pow sub_ old_exp_avg_sq_values mul_ - beta exp_avg_sq add_ make_sparse exp_avg_sq_update_values Dense addition again intended avoiding another sparse_mask numer = exp_avg_update_values add_ old_exp_avg_values exp_avg_sq_update_values add_ old_exp_avg_sq_values denom = exp_avg_sq_update_values sqrt_ add_ eps del exp_avg_update_values exp_avg_sq_update_values bias_correction = - beta step bias_correction = - beta step step_size = lr math sqrt bias_correction bias_correction param add_ make_sparse -step_size numer div_ denom