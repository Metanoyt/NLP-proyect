mypy allow-untyped-defs contextlib warnings weakref abc ABC abstractmethod collections abc Callable contextlib AbstractContextManager typing Any Optional Union torch torch fx traceback fx_traceback torch utils _pytree pytree torch _C _functionalization_reapply_views_tls _reapply_views torch _ops _get_dispatch_mode_pre_dispatch torch _subclasses meta_utils is_sparse_any torch utils _python_dispatch _detect_infra_mode _disable_infra_mode return_and_correct_aliasing TorchDispatchMode not_implemented_log = torch _logging getArtifactLogger __name__ not_implemented NOTE Some special handling tensor conversion during export needed Normally when tracing through model tensor maybe-aliasing relationship between input output tensors will baked into graph For example we got tensor device cpu call tensor cpu will become no-op graph For whole graph capture sound so we need do something different Instead export we will try preserve tensor conversion forcing non-semantic-breaking aten _to_copy operator traced graph subsequently banning mutations all such converted tensors In addition patching method call functionalization we will have patch other similar methods like float cpu because they intentionally don t fall back methods have same behavior according pytorch document https pytorch org docs stable generated torch Tensor float html thus we simply force them go through call _conversion_method_template extra_kwargs _ args kwargs args kwargs extra_kwargs _ FunctionalTensor torch Tensor Functional tensors represent tensors will remove mutations program If you perform mutable operation functional tensor will re-dispatch functional variant operation Historically functionalization implemented C++ dispatcher This lightweight python shim around C++ functionalization logic FunctionalTensor required used corresponding FunctionalTensormode active because relies using mode dispatch which can properly handle factory functions elem torch Tensor Indicates our torch_dispatch dispatching infra infra mode lower dispatching precedence _mode_key = torch _C _TorchDispatchModeKey FUNCTIONAL Note The reason we add these extra keys our FunctionalTensor subclass mirror behavior C++ functionalization we can choose change later long doesn t break anything FunctionalTensorWrapper copies all dispatch keys inner tensor wrapper excluding functorch python dispatch keys Here I m trying reuse keyset functorch wrapper subclasses copy except they don t include ZeroTensor so I m manually adding _extra_dispatch_keys = torch _C _additional_keys_to_prop_for_wrapper_tensors add torch _C DispatchKey ZeroTensor These all aten ops correspond metadata queries We want FunctionalTensor able handle them directly metadata_fns = torch ops aten is_contiguous default type ignore has-type torch ops aten is_contiguous memory_format type ignore has-type torch ops aten is_strides_like_format default type ignore has-type torch ops aten is_non_overlapping_and_dense default type ignore has-type torch ops aten size default type ignore has-type torch ops aten sym_size default type ignore has-type torch ops aten stride default type ignore has-type torch ops aten sym_stride default type ignore has-type torch ops aten storage_offset default type ignore has-type torch ops aten sym_storage_offset default type ignore has-type torch ops aten numel default type ignore has-type torch ops aten sym_numel default type ignore has-type torch ops aten dim default type ignore has-type torch ops prim device default type ignore has-type Used auto_functionalize determine base tensors during inference mode _inference_mode_base Optional FunctionalTensor = None __new__ cls elem mode assert torch _is_functional_tensor elem In general we d like our functional tensor subclass only charge functionalization defer inner subclass all other functionality Example If our inner tensor ZeroTensor we would want defer running ZeroTensor fallback until after we redispatch our inner ZeroTensor However there few keys we need mirror between inner outer tensors Conjugate Negative Why These keys used test metadata queries like ` is_conj ` ` is_neg ` We need calls is_conj same thing outer inner tensors Because user code framework code branches like so needs do same thing when sees outer FunctionalTensor x is_conj view_as_real x resolve_conj view_as_real x extra_dispatch_keys = FunctionalTensor _extra_dispatch_keys torch _C _dispatch_keys elem out = torch Tensor _make_wrapper_subclass TODO right now _make_wrapper_subclass s dynamic shape interaction great Calling overload has kwargs causes us go down first overload path which will always specialize sizes We should probably eventually fix so first overload can just handle dynamic shapes cls elem shape sizes elem stride is_sparse_any elem None strides elem storage_offset is_sparse_any elem None storage_offset None memory_format elem dtype dtype elem layout layout elem device device False pin_memory elem requires_grad requires_grad None dispatch_sizes_strides_policy False dispatch_device False dispatch_layout extra_dispatch_keys _extra_dispatch_keys torch _C _set_throw_on_mutable_data_ptr out out elem = elem torch _export config enable_auto_functionalized_v _for_export torch is_inference_mode_enabled torch _inductor config enable_auto_functionalized_v out is_base_tensor out _inference_mode_base = None This assumes FunctionalTensor elem does change its storage after point Otherwise would invalid mode _storage_to_base out elem untyped_storage = out out _inference_mode_base = mode _storage_to_base out elem untyped_storage assert out _inference_mode_base None out __torch_dispatch__ func types args= kwargs=None type ignore override unrecognized_types = t t types t torch Tensor torch _subclasses FakeTensor FunctionalTensor unrecognized_types not_implemented_log debug FunctionalTensor unrecognized subclass es s unrecognized_types NotImplemented kwargs None kwargs = FunctionalTensor needs plumb all metadata requests inner tensor In theory we don t have do - we want service metadata requests here we need carefully make sure all metadata accurate including metadata mutations func FunctionalTensor metadata_fns All metadata accesses should plumbed inner tensor way we don t have worry about problem keeping metadata sync between wrapper inner tensor This also alleviates us having manually handle metadata mutations wrapper assert len kwargs == func torch ops aten is_strides_like_format default torch ops aten is_contiguous memory_format assert len args == isinstance args FunctionalTensor func torch _from_functional_tensor args elem args assert len args == isinstance args FunctionalTensor func torch _from_functional_tensor args elem Originally I tried implement my subclass without giving torch_dispatch I gave up - _make_wrapper_subclass requires __torch_dispatch__ - If we want use _make_subclass we have problem subclass will share TensorImpl inner tensor which type FunctionalTensorWrapper We explicitly do want our wrapper FunctionalTensorWrapper - If we use default tensor __new__ we have another problem returns inner_tensor alias which causes every subclass created above autograd have autograd view metadata addition also being FunctionalTensorWrapper raise RuntimeError Attempting use FunctionalTensor its own Instead please use corresponding FunctionalTensorMode __repr__ - str type ignore override f FunctionalTensor repr elem staticmethod to_functional x We will do wrapping user assert torch _is_functional_tensor x The only autograd metadata we care about FunctionalTensor - requires_grad so autograd runs - is_leaf so mutations graph inputs leaves allowed autograd engine handled FunctionalTensor to_functional x_functional = torch _to_functional_tensor x Technically FunctionalTensormode here unnecessary avoids spurious NotImplemented logs during ` ProxyTorchDispatchMode ` tracing _mirror_autograd_meta_to queries tensor sizes otherwise sym_size call will go proxy mode before hitting FunctionalTensor __torch_dispatch__ functional_mode = _detect_infra_mode torch _C _TorchDispatchModeKey FUNCTIONAL assert functional_mode None functional_mode torch _mirror_autograd_meta_to x x_functional type ignore attr-defined out = FunctionalTensor x_functional functional_mode torch _mirror_autograd_meta_to x_functional out type ignore attr-defined out from_functional torch _sync torch _from_functional_tensor elem is_base_tensor - bool torch _is_functional_tensor_base elem replace_ output - None torch _functionalize_replace elem output commit_update - None torch _functionalize_commit_update elem sync - None torch _functionalize_sync elem mark_mutation_hidden_from_autograd - None torch _functionalize_mark_mutation_hidden_from_autograd elem tolist - Any elem dim == elem item elem dim == elem item elem elem elem tolist elem elem args kwargs _detect_infra_mode torch _C _TorchDispatchModeKey FUNCTIONAL export torch ops aten _assert_tensor_metadata dtype=self dtype device=self device layout=self layout pyrefly ignore not-iterable super args kwargs cuda device=None args kwargs device = device torch cuda current_device len args device args kwargs device=device kwargs char = _conversion_method_template dtype=torch int cpu = _conversion_method_template device=torch device cpu bfloat = _conversion_method_template dtype=torch bfloat byte = _conversion_method_template dtype=torch uint double = _conversion_method_template dtype=torch float float = _conversion_method_template dtype=torch float bool = _conversion_method_template dtype=torch bool half = _conversion_method_template dtype=torch float int = _conversion_method_template dtype=torch int long = _conversion_method_template dtype=torch int TODO sparse-team fixes can we do without relay to_dense type ignore override elem to_dense property layout type ignore override elem layout __bool__ bool item FunctionalTensorMode TorchDispatchMode __init__ pre_dispatch=False export=False _allow_token_discovery=False super __init__ export = export is_on_stack = False enter_stack = Indicates our torch_dispatch dispatching infra infra mode lower dispatching precedence _mode_key = torch _C _TorchDispatchModeKey FUNCTIONAL pre_dispatch = pre_dispatch This will turned off later pre-dispatch functionalization _dispatch_key = torch _C DispatchKey PreDispatch pre_dispatch None type ignore attr-defined Map effect type ex _EffectType ORDERED token The tokens help keep track ordering between side effectful operations _tokens dict Any torch Tensor = Filled after forward tracing _tokens_forward_output dict Any torch Tensor = Functionalization runs twice AOTAutograd once ` run_functionalized_fw_and_collect_metadata ` collect metadata see which tensors need functionalized discover how many tokens we need another time ` make_fx ` which does actual tracing replace ops their functional variants handling side-effectful ops In second stage there should no token discovery This flag distinguishes between two stages _allow_token_discovery = _allow_token_discovery _storage_to_base weakref WeakKeyDictionary torch storage UntypedStorage Optional FunctionalTensor = weakref WeakKeyDictionary No-op FunctionalTensorMode already use __enter__ _get_prev_mode _dispatch_key == torch _C DispatchKey PreDispatch _get_dispatch_mode_pre_dispatch torch _C _TorchDispatchModeKey FUNCTIONAL torch _C _get_dispatch_mode torch _C _TorchDispatchModeKey FUNCTIONAL _get_prev_mode None enter_stack append True super __enter__ enter_stack append False __exit__ b c is_on_stack = enter_stack pop is_on_stack super __exit__ b c __torch_dispatch__ func types args= kwargs=None kwargs None kwargs = unrecognized_types = t t types issubclass t torch _subclasses FakeTensor t torch Tensor FunctionalTensor unrecognized_types not_implemented_log debug FunctionalTensor unrecognized subclass es s unrecognized_types NotImplemented _can_decompose func See https github com pytorch pytorch pull #issuecomment- Never decompose dropout export export func torch ops aten dropout default False We unconditionally decompose ops maybe aliasing mutating ops torch _decomp _should_decompose_because_unsafe_op _should_decompose_because_unsafe_op func True we unconditionally decompose maybe-aliasing maybe-mutating ops because we must know statically op mutates aliasing order functionalize properly mutating ops have CompositeImplicit decomps we choose decompose them today In theory we could walk back avoid decomposing them later we need alias_info_present = any arg alias_info arg func _schema arguments alias_info_present func _schema is_mutable True If we here means we seeing functional composite op For pre-dispatch IR we don t want decompose op For post-dispatch IR we do want decompose op fine decompose here even you want preserve CIA post-dispatch export because we already override decompose behaviour so will do right thing export pre_dispatch If CIA custom op we warn we assuming op indeed functional func namespace aten prim func _can_decompose warnings warn f At pre-dispatch tracing we assume any custom op marked f CompositeImplicitAutograd have functional schema safe decompose f Found func one such op stacklevel= False True normal torch compile IR we decompose functional composite ops True func FunctionalTensor metadata_fns _can_decompose func Not all funcs __torch_dispatch__ actual dispatcher ops e g prim device torch _C _dispatch_has_kernel func name r = func decompose args kwargs r NotImplemented r wrap x Only wrap our outputs subclasses inner functionalization call also wrapped outputs into FunctionalTensorWrappers When can happen e g ` torch div ` assert isinstance x FunctionalTensor isinstance x torch Tensor torch _is_functional_tensor x FunctionalTensor x x unwrap x x elem torch _higher_order_ops auto_functionalize can_auto_functionalize do_auto_functionalize do_auto_functionalize_v can_auto_functionalize func torch _C _dispatch_has_kernel_for_dispatch_key func name torch _C DispatchKey Functionalize torch _export config export_config torch _inductor config inductor_config torch compiler is_exporting export_config enable_auto_functionalized_v _for_export do_auto_functionalize_v func args kwargs do_auto_functionalize func args kwargs inductor_config enable_auto_functionalized_v do_auto_functionalize_v func args kwargs do_auto_functionalize func args kwargs torch _higher_order_ops effects handle_effects has_effects has_effects func args kwargs assert torch _C _dispatch_has_kernel_for_dispatch_key func name torch _C DispatchKey Functionalize handle_effects _allow_token_discovery _tokens func args kwargs args_unwrapped kwargs_unwrapped = pytree tree_map_only FunctionalTensor unwrap args kwargs Expectation functionalization should already enabled above our mode Why would bad when we FunctionalTensor here we don t want functionalization run above mode further wrap output another C++ FunctionalTensorWrapper is_included = torch _C _dispatch_tls_is_dispatch_key_included torch _C DispatchKey Functionalize is_excluded = torch _C _dispatch_tls_is_dispatch_key_excluded torch _C DispatchKey Functionalize assert is_excluded is_included include_to_set = torch _C _dispatch_tls_local_include_set &#124; torch _C DispatchKeySet torch _C DispatchKey Functionalize exclude_to_set = torch _C _dispatch_tls_local_exclude_set remove torch _C DispatchKey Functionalize - FunctionalTensor _extra_dispatch_keys All we want do here reuse existing C++ functionalization logic This requires swizzling our TLS dispatch keys so Functionalize key active torch _C _ForceDispatchKeyGuard include_to_set exclude_to_set try By default python functionalization AOTAutograd we reapply views old_apply_views = torch _functionalize_enable_reapply_views True type ignore attr-defined Sometimes these functions cannot directly dispatched functionalize key because args sometimes functional tensors some reason func FunctionalTensor metadata_fns outs_unwrapped = func args_unwrapped kwargs_unwrapped outs_wrapped = pytree tree_map_only torch Tensor wrap outs_unwrapped Note Functionalization View Replay Annotation When functionalization encounters mutation handles aliases lazily regenerating aliases first time they next used This problem when plumbing user annotations during tracing We want view ops view replay have same annotation user specified original views But view replay functionalization happens next time alias used e g second_op alias_with_pending_mutation so when we regenerate views before calling into second_op those views will end up getting metadata second_op Instead we need remember node metadata original views ensure node metadata globally set when we lazily perform view replay The globally set metadata will used populate fx node created replayed operation m = torch _C _get_dispatch_mode torch _C _TorchDispatchModeKey PROXY pytree tree_leaves args kwargs isinstance FunctionalTensor continue curr_node = m tracer tensor_tracker torch _from_functional_tensor elem proxy node fx_traceback set_current_replay_node curr_node torch _sync When we dispatch C++ functionalization kernel we might need jump back PreDispatch mode stack afterwards handle any other PreDispatch modes underneath FunctionalTensorMode If we call func directly we would need exclude PreDispatch TLS order avoid infinite looping would prevent us coming back PreDispatch later outs_unwrapped = func _op_dk torch _C DispatchKey Functionalize args_unwrapped kwargs_unwrapped export func torch ops aten dropout default torch _freeze_functional_tensor outs_unwrapped type ignore attr-defined outs_wrapped = pytree tree_map_only torch Tensor wrap outs_unwrapped finally torch _disable_functionalization torch _functionalize_enable_reapply_views old_apply_views type ignore attr-defined is_included = torch _C _dispatch_tls_is_dispatch_key_included torch _C DispatchKey Functionalize is_excluded = torch _C _dispatch_tls_is_dispatch_key_excluded torch _C DispatchKey Functionalize assert is_excluded is_included If no outputs our functional subclass then don t try fix up aliasing any isinstance x FunctionalTensor x pytree tree_leaves outs_wrapped Since lift_fresh lifts its argument into functional tensor we can skip aliasing correction step Otherwise we would setting storage lifted tensor unlifted tensor Ref https github com pytorch pytorch issues func torch ops aten lift_fresh default outs_wrapped metadata mutations need manually mutate metadata FunctionalTensor wrapper torch Tag inplace_view func tags func torch ops aten set_ source_Tensor torch utils _mode_utils no_dispatch func args kwargs Wrapper tensor subclasses do have correct aliasing info Use util manually correct output aliasing inplace ops like ` aten add_ ` expected inputs directly instead creating fresh tensor objects Use util figure out right thing If none our inputs wrapped then we have no FunctionalTensor outputs we need fix up storages return_and_correct_aliasing func args kwargs outs_wrapped classmethod is_infra_mode cls - bool True contextlib contextmanager disable_functional_mode _disable_infra_mode torch _C _TorchDispatchModeKey FUNCTIONAL This similar torch func functionalize - It uses FunctionalTensorMode FunctionalTensor python subclass One important advantage using mode will let us run functionalization underneath __torch_dispatch__ which we need AOTAutograd - Doing so means does automatically compose other functorch transforms since these transforms always run above __torch_dispatch__ That s why util lives here functorch dispatch_functionalize func mode FunctionalTensorMode = FunctionalTensorMode TODO pull these aot autograd to_fun t isinstance t torch Tensor FunctionalTensor to_functional t t from_fun t isinstance t FunctionalTensor quick sanity assert isinstance t torch Tensor assert torch _is_functional_tensor t t torch _sync t torch _from_functional_tensor t elem inner args kwargs disable_above = torch _C _ExcludeDispatchKeyGuard torch _C DispatchKeySet torch _C DispatchKey Functionalize disable_above mode func_args = pytree tree_map_only torch Tensor to_fun args func_kwargs = pytree tree_map_only torch Tensor to_fun kwargs func_outputs = func func_args func_kwargs outputs = pytree tree_map_only FunctionalTensor from_fun func_outputs outputs inner BaseFunctionalizeAPI ABC abstractmethod wrap_tensors args tuple Any - tuple Any pass abstractmethod unwrap_tensors args Union torch Tensor tuple torch Tensor - Any pass abstractmethod functionalize inner_f Callable - Callable pass abstractmethod redispatch_to_next - AbstractContextManager pass abstractmethod replace input_tensor output_tensor - None pass abstractmethod commit_update tensor - None pass abstractmethod sync tensor - None pass abstractmethod mark_mutation_hidden_from_autograd tensor - None pass PythonFunctionalizeAPI BaseFunctionalizeAPI __init__ mode Optional FunctionalTensorMode = None pre_dispatch bool = False - None super __init__ mode = mode mode FunctionalTensorMode pre_dispatch = pre_dispatch wrap_tensors args tuple Any - tuple Any mode torch utils _pytree tree_map_only torch Tensor FunctionalTensor to_functional args unwrap_tensors args Union torch Tensor tuple torch Tensor list torch Tensor - Any torch utils _pytree tree_map_only FunctionalTensor FunctionalTensor from_functional args functionalize inner_f Callable - Callable dispatch_functionalize inner_f mode redispatch_to_next - AbstractContextManager NOTE We don t do anything here because time we exercise path we would have already popped FunctionalTensorMode mode stack Since FunctionalTensorMode now stateful better explicitly pass correct mode directly instead globally setting contextlib nullcontext replace input_tensor output_tensor - None assert isinstance input_tensor FunctionalTensor assert isinstance output_tensor FunctionalTensor input_tensor replace_ output_tensor commit_update tensor - None assert isinstance tensor FunctionalTensor tensor commit_update sync tensor - None assert isinstance tensor FunctionalTensor tensor sync mark_mutation_hidden_from_autograd tensor - None assert isinstance tensor FunctionalTensor tensor mark_mutation_hidden_from_autograd CppFunctionalizeAPI BaseFunctionalizeAPI wrap_tensors args tuple Any - tuple Any torch _functorch eager_transforms _wrap_all_tensors_to_functional _wrap_all_tensors_to_functional args level= unwrap_tensors args Union torch Tensor tuple torch Tensor - Union torch Tensor tuple torch Tensor torch _functorch eager_transforms _unwrap_all_tensors_from_functional _unwrap_all_tensors_from_functional args reapply_views=_reapply_views functionalize inner_f Callable - Callable torch func functionalize inner_f redispatch_to_next - AbstractContextManager torch _C _ExcludeDispatchKeyGuard torch _C DispatchKeySet torch _C DispatchKey Functionalize replace input_tensor output_tensor - None torch _functionalize_replace input_tensor output_tensor commit_update tensor - None torch _functionalize_commit_update tensor sync tensor - None torch _functionalize_sync tensor mark_mutation_hidden_from_autograd tensor - None torch _functionalize_mark_mutation_hidden_from_autograd tensor FunctorchFunctionalizeAPI BaseFunctionalizeAPI __init__ interpreter interpreter = interpreter wrap_tensors args tuple Any - tuple Any torch _functorch eager_transforms _wrap_all_tensors_to_functional _wrap_all_tensors_to_functional args level=self interpreter level unwrap_tensors args Union torch Tensor tuple torch Tensor - Union torch Tensor tuple torch Tensor torch _functorch eager_transforms _unwrap_all_tensors_from_functional _unwrap_all_tensors_from_functional args reapply_views=self interpreter functionalize_add_back_views functionalize inner_f Callable - Callable torch func functionalize inner_f remove= mutations_and_views interpreter functionalize_add_back_views mutations redispatch_to_next - AbstractContextManager interpreter lower replace input_tensor output_tensor - None torch _functionalize_replace input_tensor output_tensor commit_update tensor - None torch _functionalize_commit_update tensor sync tensor - None torch _functionalize_sync tensor mark_mutation_hidden_from_autograd tensor - None torch _functionalize_mark_mutation_hidden_from_autograd tensor mb_unwrap_functional_tensor tensor torch Tensor isinstance tensor FunctionalTensor torch _from_functional_tensor tensor elem tensor