mypy allow-untyped-defs io collections abc Callable typing Any cast torch torch distributed dist torch _utils _get_device_module torch distributed _shard metadata ShardMetadata torch distributed _shard sharded_tensor ShardedTensor torch distributed tensor DTensor torch distributed tensor _utils compute_local_shape_and_global_offset metadata BytesStorageMetadata ChunkStorageMetadata MetadataIndex STATE_DICT_TYPE STORAGE_TYPES TensorProperties TensorStorageMetadata planner LoadItemType ReadItem SavePlan TensorWriteData WriteItem WriteItemType resharding _check_shard_metadata_pair_overlap _shards_get_overlap_region_wrt_saved_tensor __all__ list str = create_read_items_for_chunk_list _compare_save_plans plan SavePlan other_plan SavePlan - bool Compare two Save plans True they equal Args plan SavePlan First SavePlan compare other_plan SavePlan Second SavePlan compare Returns True two plans equal False otherwise plan usable = other_plan usable False Both plans should have same number items len plan items = len other_plan items False Both plans should have same write items plan_item other_plan_item zip plan items other_plan items Write item type should same plan_item type = other_plan_item type False plan_metadata_index = plan_item index other_plan_metadata_index = other_plan_item index Write item metadata_index should same plan_metadata_index fqn = other_plan_metadata_index fqn plan_metadata_index offset = other_plan_metadata_index offset plan_metadata_index index = other_plan_metadata_index index False Write item tensor_data should present both write items plans exists either them tensor_data = plan_item tensor_data other_tensor_data = other_plan_item tensor_data tensor_data other_tensor_data tensor_data other_tensor_data False tensor_data other_tensor_data Write item tensor_data size should same tensor_data size = other_tensor_data size False Write item tensor_data chunk should present both write items exists either them chunk = tensor_data chunk other_chunk = other_tensor_data chunk chunk other_chunk chunk other_chunk False Write item tensor_data chunk offsets sizes should same chunk other_chunk chunk offsets = other_chunk offsets chunk sizes = other_chunk sizes False True _contains_usable_plan delta_plans list SavePlan - bool Check any delta plan usable indicating plan has changed Args delta_plans List SavePlan A list delta plans check Returns True any delta plan usable False otherwise any delta_plan delta_plan usable delta_plan delta_plans _merge_delta_local_plans cached_plans list SavePlan delta_plans list SavePlan - list SavePlan Merge list delta plans into single plan Args cached_plans List SavePlan A list cached plans delta_plans List SavePlan A list delta plans merge It can contain empty plans Returns A single merged plan If delta plan usable use cached plan Otherwise use delta plan merged_plans = cached_plan delta_plan zip cached_plans delta_plans delta_plan delta_plan usable merged_plans append cached_plan merged_plans append delta_plan merged_plans _create_chunk_from_tensor tensor torch Tensor - ChunkStorageMetadata ChunkStorageMetadata offsets=torch Size len tensor size sizes=tensor size _chunk_for_shard shard_md ShardMetadata - ChunkStorageMetadata ChunkStorageMetadata offsets=torch Size shard_md shard_offsets sizes=torch Size shard_md shard_sizes _sharded_tensor_metadata sharded_tensor ShardedTensor shard_md ShardMetadata - TensorWriteData shard_properties = sharded_tensor metadata tensor_properties properties = TensorProperties dtype=shard_properties dtype layout=shard_properties layout requires_grad=shard_properties requires_grad memory_format=shard_properties memory_format pin_memory=shard_properties pin_memory TensorWriteData chunk=_chunk_for_shard shard_md properties=properties size=sharded_tensor metadata size _create_write_items_for_dtensor fqn str tensor DTensor - WriteItem sizes offsets = compute_local_shape_and_global_offset tensor shape tensor device_mesh tensor placements sizes offsets = torch Size sizes torch Size offsets WriteItem index=MetadataIndex fqn offsets type=WriteItemType SHARD tensor_data=TensorWriteData chunk=ChunkStorageMetadata offsets=offsets sizes=sizes properties=TensorProperties create_from_tensor tensor to_local size=tensor size _create_write_item_for_shard fqn str sharded_tensor ShardedTensor shard_md ShardMetadata - WriteItem offsets = torch Size shard_md shard_offsets WriteItem index=MetadataIndex fqn offsets type=WriteItemType SHARD tensor_data=_sharded_tensor_metadata sharded_tensor shard_md _create_write_item_for_tensor fqn str tensor torch Tensor - WriteItem offsets = torch Size len tensor size WriteItem index=MetadataIndex fqn offsets type=WriteItemType TENSOR tensor_data=TensorWriteData chunk=ChunkStorageMetadata offsets=offsets sizes=tensor size properties=TensorProperties create_from_tensor tensor size=tensor size _create_write_item_for_bytesio fqn str bytes Any WriteItem index=MetadataIndex fqn type=WriteItemType BYTE_IO _create_read_item_for_byteio dest_index dest_offset storage_index storage_offset length ReadItem type=LoadItemType BYTE_IO dest_index=dest_index dest_offsets=torch Size dest_offset storage_index=storage_index storage_offsets=torch Size storage_offset lengths=torch Size length _create_read_item_for_tensor dest_index dest_offsets storage_index storage_offsets lengths ReadItem type=LoadItemType TENSOR dest_index=dest_index dest_offsets=torch Size dest_offsets storage_index=storage_index storage_offsets=torch Size storage_offsets lengths=torch Size lengths create_read_items_for_chunk_list fqn str checkpoint_md TensorStorageMetadata local_chunks list ChunkStorageMetadata - list ReadItem Create list ` ` ReadItem ` ` based checkpoint local chunks This applies resharding algorithm computes reads needed satisfy ` ` local_chunks ` ` checkpoint described ` ` checkpoint_md ` ` Args fqn str The state_dict FQN pass ` ` ReadItem ` ` checkpoint_md TensorStorageMetadata metadata given tensor checkpoint local_chunks List ChunkStorageMetadata Local chunks needs loaded Returns A list ` ` ReadItem ` ` will satisfy all input chunks read_items = naive quadratic algo can optimized later idx shard enumerate local_chunks storage_idx storage_md enumerate checkpoint_md chunks _check_shard_metadata_pair_overlap shard storage_md continue storage_offsets = dest_offsets = lengths = _dim offset_for_saved_tensor offset_for_current_tensor length _shards_get_overlap_region_wrt_saved_tensor saved_shard=storage_md current_shard=shard storage_offsets append offset_for_saved_tensor dest_offsets append offset_for_current_tensor lengths append length read_items append _create_read_item_for_tensor dest_index=MetadataIndex fqn shard offsets idx dest_offsets=dest_offsets storage_index=MetadataIndex fqn storage_md offsets storage_idx storage_offsets=storage_offsets lengths=lengths read_items _create_default_metadata_only_plan state_dict STATE_DICT_TYPE - SavePlan requests = fqn obj state_dict items isinstance obj DTensor requests append _create_write_items_for_dtensor fqn obj isinstance obj ShardedTensor requests extend _create_write_item_for_shard fqn obj shard_md shard_md obj metadata shards_metadata isinstance obj torch Tensor requests append _create_write_item_for_tensor fqn obj requests append _create_write_item_for_bytesio fqn obj SavePlan requests _create_write_items fqn str object Any - list WriteItem hasattr object __create_write_items__ DTensor implements _Checkpointable object __create_write_items__ fqn object isinstance object ShardedTensor _create_write_item_for_shard fqn object shard metadata shard object local_shards isinstance object torch Tensor _create_write_item_for_tensor fqn object _create_write_item_for_bytesio fqn object _create_chunk_from_dtensor tensor DTensor - ChunkStorageMetadata sizes offsets = compute_local_shape_and_global_offset tensor shape tensor device_mesh tensor placements sizes offsets = torch Size sizes torch Size offsets ChunkStorageMetadata offsets=offsets sizes=sizes _create_chunk_list tensor torch Tensor - list ChunkStorageMetadata hasattr tensor __create_chunk_list__ DTensor implements _Checkpointable local_chunks = tensor __create_chunk_list__ type ignore attr-defined isinstance tensor ShardedTensor local_chunks = _chunk_for_shard shard metadata shard tensor local_shards isinstance tensor torch Tensor local_chunks = _create_chunk_from_tensor tensor raise ValueError Unsupported Type expecting one Tensor DTensor ShardedTensor f got type tensor local_chunks _create_read_items fqn str md STORAGE_TYPES obj Any - list ReadItem isinstance md BytesStorageMetadata try local_chunks = _create_chunk_list obj except ValueError ex raise ValueError f Invalid checkpoint metadata fqn + f expected BytesStorageMetadata found type md ex create_read_items_for_chunk_list fqn md local_chunks _create_read_item_for_byteio dest_index=MetadataIndex fqn dest_offset= storage_index=MetadataIndex fqn storage_offset= length= _init_state_dict state_dict dict str Any - Any Initializes meta tensor meta tensor DTensor torch Tensor dtensor_func value DTensor device = getattr value device None device == torch device meta device_type = dist distributed_c d _get_pg_default_device type device = cast torch device _get_device_module device_type current_device new_local_tensor = torch empty_like value to_local device=device We need pass shape stride explicitly since DTensor might sharded unevenly dtensor = DTensor from_local new_local_tensor device_mesh=value device_mesh placements=value placements shape=value size stride=value stride dtensor value sharded_tensor_func value Any device = getattr value device None device == torch device meta raise RuntimeError f Found unsupported type type value meta device loading value tensor_func value torch Tensor device = getattr value device None device == torch device meta device_type = dist distributed_c d _get_pg_default_device type device = cast torch device _get_device_module device_type current_device tensor = torch empty_like value device=device tensor value _iterate_state_dict state_dict dtensor_func sharded_tensor_func tensor_func _iterate_state_dict iter_object Any dtensor_func Callable sharded_tensor_func Callable tensor_func Callable Iterate through state dict applying given functions each tensor type update state dict place Args iter_object Any target state_dict sharded_tensor_func Callable function apply ShardedTensor dtensor_func Callable function apply DTensor tensor_func Callable function apply Tensor TODO let state_dict_util _iterate_state_dict support place option so we don t need have two versions _iterate_state_dict isinstance iter_object DTensor dtensor_func iter_object isinstance iter_object ShardedTensor sharded_tensor_func iter_object isinstance iter_object torch Tensor tensor_func iter_object isinstance iter_object int float str bytes io BytesIO iter_object None iter_object isinstance iter_object dict key value iter_object items iter_object key = _iterate_state_dict value dtensor_func sharded_tensor_func tensor_func iter_object isinstance iter_object list tuple ret = _iterate_state_dict v dtensor_func sharded_tensor_func tensor_func v iter_object isinstance iter_object tuple ret = tuple ret type ignore assignment ret