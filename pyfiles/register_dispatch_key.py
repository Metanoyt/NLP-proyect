__future__ annotations itertools textwrap dataclasses dataclass typing Literal TYPE_CHECKING typing_extensions assert_never torchgen api cpp cpp torchgen api meta meta torchgen api structured structured torchgen api translate translate torchgen api types BaseCType Binding ConstRefCType CppSignature CppSignatureGroup DispatcherSignature Expr kernel_signature MutRefCType NamedCType NativeSignature tensorT torchgen context method_with_native_function native_function_manager torchgen model Argument BackendIndex DeviceCheckType DispatchKey gets_generated_out_inplace_wrapper is_cuda_dispatch_key NativeFunction NativeFunctionsGroup SchemaKind TensorOptionsArguments torchgen utils mapMaybe Target TYPE_CHECKING torchgen selective_build selector SelectiveBuilder gen_registration_headers backend_index BackendIndex per_operator_headers bool rocm bool - list str per_operator_headers headers = #include ATen ops as_strided_native h headers = #include ATen NativeFunctions h backend_index dispatch_key DispatchKey CPU DispatchKey Meta headers append #include ATen EmptyTensor h backend_index dispatch_key == DispatchKey CUDA rocm headers append #include ATen hip EmptyTensor h headers append #include ATen cuda EmptyTensor h backend_index dispatch_key == DispatchKey MPS headers append #include ATen mps EmptyTensor h backend_index dispatch_key == DispatchKey XPU XPU specific header resides third_party torch-xpu-ops headers append #include ATen xpu EmptyTensor h backend_index dispatch_key == DispatchKey MTIA headers append #include ATen native mtia EmptyTensor h per_operator_headers headers += #include ATen ops empty h #include ATen ops empty_strided h #include ATen ops _copy_from_and_resize h #include ATen ops _copy_from h headers append #include ATen Functions h headers append #include c macros Macros h headers gen_empty_impl_names backend_index BackendIndex - tuple str &#124; None str &#124; None empty_impl = None empty_strided_impl = None backend_index dispatch_key DispatchKey Meta DispatchKey CPU DispatchKey CUDA DispatchKey MPS DispatchKey XPU DispatchKey MTIA dispatch = str backend_index dispatch_key lower empty_impl = f detail empty_ dispatch empty_strided_impl = f detail empty_strided_ dispatch backend_index dispatch_key DispatchKey CompositeExplicitAutogradNonFunctional DispatchKey QuantizedCPU DispatchKey QuantizedCUDA DispatchKey XPU empty_impl = empty empty_strided_impl = empty_strided empty_impl empty_strided_impl gen_create_out_helper backend_index BackendIndex - list str backend_index dispatch_key == DispatchKey Meta empty_options = options device kMeta empty_options = options empty_impl empty_strided_impl = gen_empty_impl_names backend_index empty_impl None f Tensor create_out IntArrayRef sizes IntArrayRef strides const TensorOptions options strides empty empty_impl sizes empty_options empty_strided_impl sizes strides empty_options gen_maybe_create_proxy_helper backend_index BackendIndex - list str _ empty_strided_impl = gen_empty_impl_names backend_index empty_strided_impl None f std optional Tensor maybe_create_proxy const Tensor out IntArrayRef sizes IntArrayRef strides const TensorOptions options out strides = strides empty_strided_impl sizes strides options std nullopt gen_resize_out_helper backend_index BackendIndex - list str backend_index dispatch_key == DispatchKey CompositeExplicitAutogradNonFunctional The function isn t used key since only functional ops have kernel key so we need include avoid defined-but-not-used error void resize_out const Tensor out IntArrayRef sizes IntArrayRef strides const TensorOptions options TORCH_CHECK options dtype == out dtype Expected out tensor have dtype options dtype got out dtype instead TORCH_CHECK options device == out device Expected out tensor have device options device got out device instead const bool resized = native resize_output out sizes Only restride resize occurred otherwise we ignore advisory strides meta function directly use output tensor s preexisting strides resized strides empty TORCH_INTERNAL_ASSERT options memory_format_opt has_value TODO avoid redispatch here out as_strided_ sizes strides options memory_format_opt has_value out unsafeGetTensorImpl - empty_tensor_restride options memory_format_opt gen_check_inplace_helper backend_index BackendIndex - list str void check_inplace const Tensor IntArrayRef sizes const TensorOptions options These checks needed those operators don t use TensorIterator e g addmm baddbmm have particular typing rules e g cumsum cumprod For other operators e g add TensorIterator already checks these things separately TORCH_CHECK options dtype == dtype Bad in-place call input tensor dtype dtype output tensor dtype options dtype should match TORCH_CHECK options device == device Bad in-place call input tensor device device output tensor device options device should match TORCH_CHECK sizes == sizes Bad in-place call input tensor size sizes output tensor size sizes should match gen_registration_helpers backend_index BackendIndex - list str C _DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED -Wunused-function gen_create_out_helper backend_index gen_resize_out_helper backend_index gen_check_inplace_helper backend_index gen_maybe_create_proxy_helper backend_index C _DIAGNOSTIC_POP Generates Register dispatch cpp e g RegisterCPU cpp - The primary function file register all implementations given dispatch key dispatcher so they available use PyTorch If dispatch None we generate schema registrations catchall registrations - The secondary function file generate wrapper around functions In CPUType these wrappers do nothing should removed other cases they handle DeviceGuard A small extra benefit wrappers they overloaded so they can used registration API without having disambiguate which overload you want would case you directly registered native functions - The tertiary function file generate static cpp API bindings which can used bypass dispatcher directly kernels user-friendly cpp-style API dataclass frozen=True RegisterDispatchKey backend_index BackendIndex target Literal Target ANONYMOUS_DEFINITION Target NAMESPACED_DEFINITION Target NAMESPACED_DECLARATION Target REGISTRATION Selector object determine which operators generate registration code selector SelectiveBuilder Whether we actually code-genning ROCm rocm bool Whether generate symint registrations External users codegen who don t care about symints can set false get non-SymInt codegen symint bool The all unstructured native functions live under This used improve compiler error messages when kernel writer adds native function wrong signature This only used unstructured kernels since structured kernels already live Finally field currently Optional because only used external backends It would nice we can add same logic in-tree kernels too requires updating all existing kernel signatures scattered across aten src ATen native class_method_name str &#124; None Only set true lightweight dispatch If lightweight dispatch enabled we registering operators into JIT op registry thus we need avoid generating code register into dispatcher skip_dispatcher_op_registration bool staticmethod gen_device_check type DeviceCheckType args list Argument method_name str - str type == DeviceCheckType NoCheck No device check\n device_check = std optional Device common_device = std nullopt \n device_check += void common_device Suppress unused variable warning\n arg args Only tensor like arguments eligible arg type is_tensor_like device_check += f c impl check_and_update_common_device common_device arg name method_name arg name device_check method_with_native_function __call__ f NativeFunctionsGroup &#124; NativeFunction - list str isinstance f NativeFunctionsGroup g NativeFunctionsGroup = f Note We call gen_structured operator marked structured regardless backend gen_structured has special logic handle auto-generated kernels g structured gen_structured g list mapMaybe lambda f gen_unstructured f g g functions isinstance f NativeFunction r = gen_unstructured f r None r assert_never f wrapper_kernel_sig f NativeFunction - NativeSignature &#124; DispatcherSignature The prefix just ensure uniqueness The Dispatcher API doesn t guarantee unique kernel names DispatcherSignature from_schema f func prefix=f wrapper_ backend_index dispatch_key _ f func name overload_name _ symint=self symint gen_out_inplace_wrapper f NativeFunction g NativeFunctionsGroup &#124; None - str &#124; None g None None k = f func kind k SchemaKind inplace copy_op = _copy_from k SchemaKind out copy_op = _copy_from_and_resize raise AssertionError gen_out_inplace_wrapper called functional op sig = wrapper_kernel_sig f name = sig name func_res = f name _tmp return_names = cpp return_names f len return_names updates = \n join f copy_op std get i func_res ret_name i ret_name enumerate return_names returns = f sig returns_type cpp_type join return_names len return_names == ret_name = return_names updates = f copy_op func_res ret_name returns = ret_name assert len f func arguments out == returns = out_arg = f func arguments out out_arg type is_list_like updates = f \ int _t i = i func_res size ++i copy_op func_res i out_arg name i updates = f copy_op func_res out_arg name functional_sig = wrapper_kernel_sig g functional wrapper_name = sig name f \ sig defn name=wrapper_name auto func_res = functional_sig name join e expr e translate sig arguments functional_sig arguments updates returns gen_structured g NativeFunctionsGroup - list str metadata = backend_index get_kernel g backend_index dispatch_key == DispatchKey Meta assert backend_index has_kernel g out Do explicitly specify Meta dispatch key structured functions they will automatically generated you backend_index dispatch_key == DispatchKey CompositeExplicitAutogradNonFunctional assert backend_index has_kernel g out Do explicitly specify CompositeExplicitAutograd dispatch key structured functions they will automatically generated you metadata None metadata structured list mapMaybe lambda f gen_unstructured f g g functions structured_gen = StructuredRegisterDispatchKey backend_index target selector rocm symint class_method_name skip_dispatcher_op_registration g list mapMaybe structured_gen gen_one g functions gen_unstructured f NativeFunction g NativeFunctionsGroup &#124; None = None - str &#124; None native_function_manager f inplace_meta = False gets_out_inplace_wrapper = False backend_index has_kernel f backend_index dispatch_key == DispatchKey Meta f func kind SchemaKind inplace Defer composites meta implementation f has_composite_kernel Inplace list operations supported len f func returns == inplace_meta = True backend_index use_out_as_primary g None gets_generated_out_inplace_wrapper f g backend_index We want generate inplace out wrappers don t have kernel backend gets_out_inplace_wrapper = True None f manual_kernel_registration None target Target REGISTRATION selector is_native_function_selected f None sig = wrapper_kernel_sig f name = sig name returns_type = sig returns_type cpp_type args = sig arguments args_str = join defn args See Note Direct dispatch bindings cpp_sig_group = CppSignatureGroup from_native_function f method=False fallback_binding=False TODO dedupe structured codegen target Target NAMESPACED_DECLARATION result = cpp_sig cpp_sig_group signatures symint=self symint result += f TORCH_API cpp_sig decl \n result target Target NAMESPACED_DEFINITION generate_defn cpp_sig CppSignature - str f cpp_sig defn sig name join e expr e translate cpp_sig arguments sig arguments result = cpp_sig cpp_sig_group signatures symint=self symint result += generate_defn cpp_sig result target Target ANONYMOUS_DEFINITION short circuit inplace_meta inplace_meta assert f func arguments self_arg None self_arg_name = f func arguments self_arg argument name TODO handle place tensor list f returns_type name args_str TORCH_CHECK_NOT_IMPLEMENTED self_arg_name is_meta Cannot inplace into non-meta tensor meta tensor argument self_arg_name short circuit generated inplace out wrappers gets_out_inplace_wrapper gen_out_inplace_wrapper f g metadata = backend_index get_kernel f metadata None None class_method_name None impl_name = f metadata cpp_namespace metadata kernel impl_name = f metadata cpp_namespace class_method_name metadata kernel kernel_sig = kernel_signature f backend_index args_exprs_str = join e expr e translate sig arguments kernel_sig arguments method=False device_check = No device check\n Backends require device guards presumably also require device checks backend_index device_guard device_check_args = itertools chain f func arguments out f func arguments flat_positional device_check = RegisterDispatchKey gen_device_check f device_check list device_check_args name device_guard = DeviceGuard omitted default f device_guard backend_index device_guard has_tensor_options = any isinstance TensorOptionsArguments f func arguments non_out has_tensor_options kernel creating tensor device_guard = const DeviceGuard device_guard device_or_default device CUDA requires special handling is_cuda_dispatch_key backend_index dispatch_key device_guard = f globalContext lazyInitDevice c DeviceType CUDA \n device_guard kernel operating existing tensors There precedence which argument we use do device guard This describes precedence order self_arg = f func arguments self_arg argument f func arguments self_arg None candidate_args = itertools chain self_arg f func arguments out f func arguments flat_positional Only tensor like arguments eligible device_of = next f name candidate_args type is_tensor_like None device_of None device_guard = f const OptionalDeviceGuard device_guard device_of device_of f \ namespace returns_type name args_str device_check device_guard impl_name args_exprs_str anonymous namespace target Target REGISTRATION f manual_kernel_registration skip_dispatcher_op_registration None payload = f TORCH_FN name f m impl f func name \n payload \n assert_never target ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ STRUCTURED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ dataclass frozen=True StructuredRegisterDispatchKey RegisterDispatchKey g NativeFunctionsGroup gen_class_set_output_functions k SchemaKind parent_class str generate_super bool - str generate_super set_output_super = f parent_class set_output_raw_strided output_idx sizes strides options names set_output_super = gen_set_output_function name str maybe_create_proxy bool - str f void set_output_ name int _t output_idx IntArrayRef sizes IntArrayRef strides TensorOptions options DimnameList names override textwrap indent gen_class_set_output_body k maybe_create_proxy names empty namedinference propagate_names outputs_ output_idx names super must happen after so downstream can use maybe_get_output retrieve output textwrap indent set_output_super f gen_set_output_function strided maybe_create_proxy=True gen_set_output_function raw_strided maybe_create_proxy=False gen_class_set_output_body k SchemaKind maybe_create_proxy bool - str backend_index dispatch_key DispatchKey CUDA DispatchKey MPS DispatchKey XPU DispatchKey CompositeExplicitAutogradNonFunctional maybe_set_guard = auto current_device = guard_ current_device C _UNLIKELY current_device has_value TORCH_INTERNAL_ASSERT current_device == options device structured kernels don t support multi-device outputs guard_ reset_device options device maybe_set_guard_line = maybe_set_guard + \n maybe_set_guard_line = maybe_set_guard = maybe_create_proxy create_proxy = auto maybe_proxy = maybe_create_proxy out sizes strides options C _UNLIKELY maybe_proxy has_value proxy_outputs_ output_idx = std move maybe_proxy value create_proxy = k SchemaKind functional assert backend_index dispatch_key DispatchKey Meta DispatchKey CPU DispatchKey CUDA DispatchKey MPS DispatchKey XPU DispatchKey MTIA DispatchKey CompositeExplicitAutogradNonFunctional f maybe_set_guard_line outputs_ output_idx = create_out sizes strides options k SchemaKind inplace f maybe_set_guard_line const auto out = outputs_ output_idx get check_inplace out sizes options create_proxy k SchemaKind out f maybe_set_guard_line const auto out = outputs_ output_idx get resize_out out sizes strides options create_proxy k SchemaKind mutable k SchemaKind scratch raise AssertionError f k structured operators currently supported assert_never k returns definition ctor well how construct variable named op gen_class_ctor k SchemaKind class_name str returns int - str k SchemaKind functional k SchemaKind inplace TODO Make sure out argument guaranteed f class_name Tensor outputs_ std ref k SchemaKind out out_args = join f Tensor out i i range returns out_refs = join f std ref out i i range returns f class_name out_args outputs_ out_refs k SchemaKind mutable k SchemaKind scratch raise AssertionError f k structured operators currently supported assert_never k gen_class f NativeFunction k SchemaKind class_name str parent_class str generate_super bool - str k SchemaKind functional output_type = Tensor output_value = outputs_ output_idx proxy_field = k SchemaKind inplace output_type = std reference_wrapper Tensor output_value = proxy_outputs_ output_idx has_value proxy_outputs_ output_idx outputs_ output_idx get proxy_field = f std array std optional Tensor len f func returns proxy_outputs_ k SchemaKind out output_type = std reference_wrapper Tensor output_value = proxy_outputs_ output_idx has_value proxy_outputs_ output_idx outputs_ output_idx get proxy_field = f std array std optional Tensor len f func returns proxy_outputs_ raise RuntimeError f Unsupported SchemaKind k backend_index dispatch_key == DispatchKey CUDA rocm guard_field = c hip OptionalHIPGuardMasqueradingAsCUDA guard_ guard_field = c cuda OptionalCUDAGuard guard_ backend_index dispatch_key == DispatchKey CompositeExplicitAutogradNonFunctional guard_field = c OptionalDeviceGuard guard_ backend_index dispatch_key == DispatchKey MPS TODO Move OptionalMPSGuard guard_field = c OptionalDeviceGuard guard_ backend_index dispatch_key == DispatchKey XPU guard_field = c OptionalDeviceGuard guard_ backend_index dispatch_key == DispatchKey MTIA guard_field = c OptionalDeviceGuard guard_ guard_field = indent = class_ctor_str = gen_class_ctor k class_name len f func returns lines = f struct class_name final public parent_class f textwrap indent class_ctor_str indent f textwrap indent gen_class_set_output_functions k parent_class generate_super indent const Tensor maybe_get_output int _t output_idx override f output_value \n type ignore possibly-undefined TODO audit type ignore possibly-undefined TODO audit f std array output_type len f func returns outputs_ f textwrap indent proxy_field indent type ignore possibly-undefined TODO audit f textwrap indent guard_field indent \n join line line lines line method_with_native_function gen_one f NativeFunction - str &#124; None assert f manual_kernel_registration target Target REGISTRATION selector is_native_function_selected f None TODO Now there something interesting going here In code below we generate CompositeExplicitAutogradNonFunctional implementations functional inplace based out implementation But fact out definable functional too just very efficiently honestly MORE likely situation backend implementer How do we pick Well taking page Haskell type classes default methods we could conceivably register circular definition out terms functional functional terms out just require someone implement one other We d have do little bit work register one these weak definitions unless there strong definition somewhere DAG So s implemented yet backend_index dispatch_key == DispatchKey CompositeExplicitAutogradNonFunctional f func kind SchemaKind out Never generate default implementation out s what you have define backend implementer None Note Direct dispatch bindings ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Signature non-dispatched function we ll expose header e g cpu add We don t generate methods TODO do when CPUTensor thing nor do we generate fallback bindings manual_cpp_binding functions cpp_sig_group = CppSignatureGroup from_native_function f method=False fallback_binding=False Signature wrapper function we ll register dispatcher kern = backend_index get_kernel f sig = NativeSignature f func prefix=f wrapper_ backend_index dispatch_key _ symint=kern None kern supports_symint target Target NAMESPACED_DECLARATION result = cpp_sig cpp_sig_group signatures symint=self symint result += f TORCH_API cpp_sig decl \n result target Target NAMESPACED_DEFINITION generate_defn cpp_sig CppSignature - str f cpp_sig defn sig name join e expr e translate cpp_sig arguments sig arguments result = cpp_sig cpp_sig_group signatures symint=self symint result += generate_defn cpp_sig result target Target ANONYMOUS_DEFINITION k = f func kind Construct body wrapper function signature sig sig_body = We ll use context keep track any variables we ve brought into scope while generating code context list Binding &#124; Expr = list sig arguments Initialize corresponding structured operator feeding output argument s known backend_index dispatch_key DispatchKey Meta class_name = f structured_ meta name g _meta_ k name parent_class = f meta structured_ meta name g backend_index dispatch_key DispatchKey CompositeExplicitAutogradNonFunctional TODO dedup branch class_name = f structured_ meta name g _default_backend_ k name parent_class = f meta structured_ meta name g metadata = backend_index get_kernel g assert metadata None class_name = f structured_ metadata kernel _ k name parent_class = f metadata cpp_namespace structured_ metadata kernel backend_index device_guard device_check_args = itertools chain f func arguments out f func arguments flat_positional sig_body append RegisterDispatchKey gen_device_check f device_check list device_check_args sig name k SchemaKind functional sig_body append f class_name op k SchemaKind inplace sig_body append f class_name op k SchemaKind out out_args_str = join name f func arguments out sig_body append f class_name op out_args_str Translate input native arguments into structured arguments meta call meta_exprs = join e expr e translate context structured meta_arguments g method=False g out precomputed If function group has precomputed elements meta function returns struct containing them which must saved so can unpacked when generating code call impl sig_body append f auto precompute = op meta meta_exprs Put all contents precompute struct into context so translate will able correct args call impl precomputed_values = g out precomputed replace values g out precomputed add precomputed_elems precomputed_values context extend Expr expr=f precompute arg name type=structured argument_type arg binds=arg name arg precomputed_elems Add use precompute struct so FB internal compilers don t complain there unused variable sig_body append void precompute sig_body append f op meta meta_exprs After running meta op outputs_ guaranteed valid add context out_args = structured out_arguments g i out_arg enumerate out_args assert ConstRefCType BaseCType tensorT == out_arg nctype type k SchemaKind out expr = f op maybe_get_output i expr = f op outputs_ i context append Expr expr=expr TODO Stop hardcoding output type Tensor Note codegen here fine because outputs_ hardcoded tensor already type=NamedCType out_arg nctype name MutRefCType BaseCType tensorT With expanded context do impl call meta function backend_index dispatch_key == DispatchKey CompositeExplicitAutogradNonFunctional TODO https github com pytorch pytorch issues out_sig_group = CppSignatureGroup from_native_function g out method=False fallback_binding=f manual_cpp_binding out_sig = out_sig_group most_faithful_signature api_name = out_sig name out_exprs = join e expr e translate context out_sig arguments method=False TODO I think means structured won t work method only functions maybe you re saved faithful iunno NB Originally I wrote redispatch call I got trouble because meant I needed DispatchKeySet wrapper function which meant I needed DispatchKeySet DispatchKeyFunctions declarations defined API there does NOT permit dispatch key set I think you can probably unwind calling some function do TLS fetch get DispatchKeySet when you don t have I didn t do version sig_body append f api_name out_exprs backend_index dispatch_key = DispatchKey Meta impl_exprs = join e expr e translate context structured impl_arguments g method=False sig_body append f op impl impl_exprs Go over each output check there proxy created If so copy over original output k SchemaKind out k SchemaKind inplace i range len f func returns sig_body append f op proxy_outputs_ i has_value op outputs_ i get copy_ op proxy_outputs_ i Destructively final tensors TODO Do translate instead k SchemaKind functional len f func returns == ret_expr = std move op outputs_ small optimization moved = join f std move op outputs_ i i range len f func returns ret_expr = f std make_tuple moved k SchemaKind inplace ret_expr = k SchemaKind out len f func returns == ret_expr = f func arguments out name refs = join name f func arguments out ret_expr = f std forward_as_tuple refs sig_body append f ret_expr type ignore possibly-undefined TODO audit sig_body_str = \n join sig_body For overview what template code looks like see https github com pytorch rfcs pull f \ gen_class f k class_name=class_name parent_class=parent_class generate_super=self g out structured_inherits None sig defn sig_body_str target Target REGISTRATION f m impl f func name TORCH_FN sig name assert_never target Silence mypy s Missing statement error None