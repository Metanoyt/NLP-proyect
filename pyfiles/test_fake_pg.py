Owner s oncall distributed sys unittest torch torch distributed dist torch distributed _functional_collectives funcol torch nn nn torch _C _distributed_c d FakeProcessGroup torch distributed device_mesh init_device_mesh torch distributed fsdp FullyShardedDataParallel FSDP torch distributed tensor DeviceMesh Shard torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel torch fx experimental proxy_tensor make_fx torch testing FileCheck torch testing _internal common_distributed HAS_ACCELERATOR torch testing _internal common_fsdp get_devtype torch testing _internal common_utils run_tests skipIfHpu TestCase torch testing _internal distributed _tensor common_dtensor MLPModule torch testing _internal distributed fake_pg FakeStore torch utils _python_dispatch TorchDispatchMode dist is_available print Distributed available skipping tests file=sys stderr sys exit device_type = get_devtype type TestFakePG TestCase tearDown super tearDown try dist destroy_process_group except AssertionError pass test_all_reduce dist init_process_group backend= fake rank= world_size= output = torch ones dist get_rank dist all_reduce output assertEqual tuple output shape test_allgather dist init_process_group backend= fake rank= world_size= input_tensor = torch ones dist get_rank output_tensors = torch empty_like input_tensor _ range dist all_gather output_tensors input_tensor out_tensor output_tensors assertEqual tuple out_tensor shape test_reduce_scatter store = FakeStore dist init_process_group backend= fake rank= world_size= store=store to_reduce_scatter = torch ones rank rank range output_tensor = torch empty dist reduce_scatter output_tensor to_reduce_scatter assertEqual tuple output_tensor shape unittest skipIf HAS_ACCELERATOR No accelerator test_construct_fsdp store = FakeStore dist init_process_group backend= fake rank= world_size= store=store FSDP nn Linear device=device_type skipIfHpu unittest skipIf HAS_ACCELERATOR No accelerator test_fsdp_fake_e e store = dist HashStore dist init_process_group backend= fake rank= world_size= store=store my_module = nn Sequential nn Linear device=device_type nn ReLU nn Linear device=device_type sharded_module = FSDP my_module use_orig_params=True optim = torch optim Adam sharded_module parameters lr= input = torch randn x = sharded_module input loss = x sum loss backward optim step skipIfHpu unittest skipIf HAS_ACCELERATOR No accelerator test_fake_pg_tracing store = dist HashStore dist init_process_group backend= fake rank= world_size= store=store default_pg = dist distributed_c d _get_default_group allgather_fn tensor funcol all_gather_tensor tensor default_pg gm = make_fx allgather_fn torch randn device=device_type FileCheck check all_gather check wait_tensor run str gm graph test_broadcast dist init_process_group backend= fake rank= world_size= src == rank output = torch ones dist broadcast output src= assertEqual tuple output shape src = rank output = torch ones dist broadcast output src= assertEqual tuple output shape test_scatter store = FakeStore dist init_process_group backend= fake rank= world_size= store=store src == rank output = torch ones to_scatter = torch ones rank rank range dist scatter output to_scatter assertEqual tuple output shape src = rank output = torch ones dist scatter output None src= assertEqual tuple output shape test_alltoall store = FakeStore dist init_process_group backend= fake rank= world_size= store=store output_list = torch ones _ range input_list = torch ones _ range dist all_to_all output_list input_list assertEqual len output_list output output_list assertEqual tuple output shape test_alltoall_base store = FakeStore dist init_process_group backend= fake rank= world_size= store=store out_tensor = torch ones in_tensor = torch ones output_split = input_split = dist all_to_all_single out_tensor in_tensor output_split input_split assertEqual tuple out_tensor shape test_send store = FakeStore dist init_process_group backend= fake rank= world_size= store=store tensor = torch ones dist send tensor assertEqual tuple tensor shape test_recv store = FakeStore dist init_process_group backend= fake rank= world_size= store=store output = torch ones dist recv output assertEqual tuple output shape skipIfHpu unittest skipIf HAS_ACCELERATOR No accelerator test_fsdp_tp_fake_e e world_size = tp_size = store = dist HashStore dist init_process_group backend= fake rank= world_size=world_size store=store device_mesh = DeviceMesh device_type torch arange world_size view - tp_size device_mesh = init_device_mesh device_type world_size tp_size tp_size mesh_dim_names= dp tp sequence_parallelize_plan = net ColwiseParallel input_layouts=Shard net RowwiseParallel output_layouts=Shard pairwise_parallelize_plan = net ColwiseParallel net RowwiseParallel parallel_plan sequence_parallelize_plan pairwise_parallelize_plan my_module = parallelize_module MLPModule device=device_type device_mesh tp parallel_plan sharded_module = FSDP my_module use_orig_params=True device_mesh=device_mesh dp optim = torch optim Adam sharded_module parameters lr= i range dp_rank = dist get_rank torch manual_seed i + dp_rank input = torch randn device=f device_type dp_rank x = sharded_module input loss = x sum loss backward optim step test_error_on_collective torch testing _internal distributed fake_pg FakeStore Test error_on_collective=False default behavior store = FakeStore dist init_process_group backend= fake rank= world_size= store=store These should work normally tensor = torch ones dist all_reduce tensor assertEqual tuple tensor shape dist destroy_process_group Test error_on_collective=True torch _C _distributed_c d FakeProcessGroup options = FakeProcessGroup Options options error_on_collective = True store = FakeStore dist init_process_group backend= fake rank= world_size= store=store pg_options=options These should now raise errors tensor = torch ones assertRaisesRegex RuntimeError FakeProcessGroup collective operation error dist all_reduce tensor assertRaisesRegex RuntimeError FakeProcessGroup collective operation error output_tensors = torch empty_like tensor _ range dist all_gather output_tensors tensor assertRaisesRegex RuntimeError FakeProcessGroup collective operation error dist broadcast tensor src= assertRaisesRegex RuntimeError FakeProcessGroup collective operation error dist barrier test_fake_process_group_direct_usage_error SimpleTensorMode TorchDispatchMode __torch_dispatch__ func types args= kwargs=None kwargs None kwargs = func args kwargs assertRaisesRegex TypeError r No constructor defined fake_pg = FakeProcessGroup rank= world_size= SimpleTensorMode tensor = torch tensor dist all_reduce tensor group=fake_pg test_fake_process_group_proper_usage_dispatch SimpleTensorMode TorchDispatchMode __init__ ops = __torch_dispatch__ func types args= kwargs=None ops append str func kwargs None kwargs = func args kwargs fake_store = FakeStore dist init_process_group fake store=fake_store rank= world_size= SimpleTensorMode mode tensor = torch tensor dist all_reduce tensor op_names = str op op mode ops assertIn aten lift_fresh default op_names assertIn c d allreduce_ default op_names __name__ == __main__ run_tests