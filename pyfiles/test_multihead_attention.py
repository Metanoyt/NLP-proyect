Owner s module nn contextlib random unittest unittest mock mock torch torch nn nn torch nn MultiheadAttention torch testing _internal common_device_type dtypes instantiate_device_type_tests onlyOn torch testing _internal common_nn NNTestCase torch testing _internal common_utils instantiate_parametrized_tests parametrize parametrize_test run_tests TEST_CUDA TEST_NUMPY TEST_WITH_CROSSREF TEST_NUMPY numpy np WARNING If you add new top-level test case file you MUST update test run_test py list otherwise will NOT run CI TestMultiheadAttentionNN NNTestCase TEST_CUDA _do_cuda_memory_leak_check = True _do_cuda_non_default_stream = True unittest skipIf TEST_NUMPY numpy found parametrize_test average_attn_weights True False test_multihead_attention average_attn_weights _scaled_dot_attn_ref Q K V dims unseen_mask=None key_padding_mask=None average_attn_weights=average_attn_weights Numpy-based reference implementation scaled dot attention testing QKT = _batchmatmul Q np transpose K axes= np sqrt dims dtype=np float divide sqrt d_head b b s s = QKT shape unseen_mask None key_padding_mask None assert s == s i range b j range b m range s n range s unseen_mask None unseen_mask m n == QKT i j m n = -np inf key_padding_mask None key_padding_mask i n QKT i j m n = -np inf reference = _softmax QKT ref_attn_weight = reference average_attn_weights ref_attn_weight = np sum ref_attn_weight axis= b reference = _batchmatmul reference V reference ref_attn_weight _batchmatmul b batchmatmul over dim matrix Numpy-based batch matrix multiply over dim matrix assert shape == b shape assert shape == b shape retval = np zeros shape shape shape b shape dtype=np float i range shape j range shape retval i j = np matmul i j b i j retval _softmax x softmax over dim matrix Numpy-based reference softmax over dim matrix np seterr invalid= ignore output = np zeros x shape dtype=np float i range x shape j range x shape k range x shape x_curr = x i j k e_x = np exp x_curr - np amax x_curr output i j k = e_x np sum e_x output _split_heads_ref X dims nheads d_head X_split = np reshape X dims + nheads d_head X_split_transposed = np transpose X_split reference = np reshape X_split_transposed dims nheads dims d_head reference _combine_heads_ref X dims nheads d_head X_transposed = np transpose X reference = np reshape X_transposed dims + nheads d_head reference _fc X X_weight X_bias X_fc_b = X_bias detach numpy X_fc_w = X_weight detach numpy np matmul X np transpose X_fc_w + X_fc_b _create_src_lengths_mask batch_size src_lengths Generate boolean mask prevent attention beyond end source Inputs batch_size int src_lengths batch_size sentence lengths Outputs batch_size max_src_len max_srclen = src_lengths max src_indices = torch arange max_srclen unsqueeze src_lengths src_indices = src_indices expand batch_size max_srclen src_lengths = src_lengths unsqueeze dim= expand batch_size max_srclen returns batch_size max_seq_len src_indices src_lengths int detach _multihead_attn_test_helper add_key_padding_mask=False add_bias_kv=False add_zero_attn=False saved_kv=False same_embed_dim=False average_attn_weights=average_attn_weights _ range batch_sz seq_len = random randint r range d_head = random randint nheads = random randint d_model = d_head nheads same_embed_dim kv_dim = d_model kv_dim = random randint dims = batch_sz seq_len kv_dim saved_k = None saved_k_tensor = None saved_v = None saved_v_tensor = None saved_kv saved_k = np random rand batch_sz nheads seq_len d_head saved_k_tensor = torch from_numpy saved_k torch get_default_dtype saved_v = np random rand batch_sz nheads seq_len d_head saved_v_tensor = torch from_numpy saved_v torch get_default_dtype key_padding_mask = None key_padding_mask_tensor = None add_key_padding_mask seq_mask = np random randint seq_len key_padding_mask = np repeat seq_mask batch_sz axis= == key_padding_mask_tensor = torch from_numpy key_padding_mask decoder_state = np random rand batch_sz d_model K = np random rand dims V = K Q = np expand_dims decoder_state attn_mask = np random randint size= seq_len attn_mask_tensor = torch from_numpy attn_mask float attn_mask_tensor masked_fill_ attn_mask_tensor == float -inf attn_mask_tensor masked_fill_ attn_mask_tensor float decoder_state_tensor = torch from_numpy decoder_state torch get_default_dtype source_hid_tensor = torch from_numpy K torch get_default_dtype transpose multihead_attn_module = MultiheadAttention d_model nheads add_bias_kv=add_bias_kv add_zero_attn=add_zero_attn kdim=kv_dim vdim=kv_dim add_bias_kv bias_k = multihead_attn_module bias_k detach numpy bias_v = multihead_attn_module bias_v detach numpy bias_k = None bias_v = None _Q = decoder_state_tensor unsqueeze transpose _V = source_hid_tensor _K = source_hid_tensor multihead_attn_module _qkv_same_embed_dim result result_weight = torch nn functional multi_head_attention_forward _Q _K _V d_model nheads multihead_attn_module in_proj_weight multihead_attn_module in_proj_bias multihead_attn_module bias_k multihead_attn_module bias_v multihead_attn_module add_zero_attn multihead_attn_module dropout multihead_attn_module out_proj weight multihead_attn_module out_proj bias multihead_attn_module training key_padding_mask_tensor True attn_mask_tensor static_k=saved_k_tensor static_v=saved_v_tensor average_attn_weights=average_attn_weights is_causal=False result result_weight = torch nn functional multi_head_attention_forward _Q _K _V d_model nheads None multihead_attn_module in_proj_bias multihead_attn_module bias_k multihead_attn_module bias_v multihead_attn_module add_zero_attn multihead_attn_module dropout multihead_attn_module out_proj weight multihead_attn_module out_proj bias multihead_attn_module training key_padding_mask_tensor True attn_mask_tensor True multihead_attn_module q_proj_weight multihead_attn_module k_proj_weight multihead_attn_module v_proj_weight static_k=saved_k_tensor static_v=saved_v_tensor average_attn_weights=average_attn_weights is_causal=False result = result squeeze detach numpy multihead_attn_module _qkv_same_embed_dim q_proj_weight = multihead_attn_module in_proj_weight d_model k_proj_weight = multihead_attn_module in_proj_weight d_model d_model v_proj_weight = multihead_attn_module in_proj_weight d_model q_proj_weight = multihead_attn_module q_proj_weight k_proj_weight = multihead_attn_module k_proj_weight v_proj_weight = multihead_attn_module v_proj_weight Q_fc = _fc Q q_proj_weight multihead_attn_module in_proj_bias d_model K_fc = _fc K k_proj_weight multihead_attn_module in_proj_bias d_model d_model V_fc = _fc V v_proj_weight multihead_attn_module in_proj_bias d_model add_bias_kv K_fc = np concatenate K_fc np repeat bias_k K_fc shape axis= axis= V_fc = np concatenate V_fc np repeat bias_v V_fc shape axis= axis= attn_mask None attn_mask = np concatenate attn_mask np ones axis= key_padding_mask None key_padding_mask = np concatenate key_padding_mask np full batch_sz False dtype=bool axis= dims += Q_split = _split_heads_ref Q_fc batch_sz d_model nheads d_head saved_k None K_split = np reshape saved_k dims nheads dims d_head K_split = _split_heads_ref K_fc dims nheads d_head saved_v None V_split = np reshape saved_v dims nheads dims d_head V_split = _split_heads_ref V_fc dims nheads d_head add_zero_attn dims += K_split = np concatenate K_split np zeros K_split shape K_split shape K_split shape axis= V_split = np concatenate V_split np zeros V_split shape V_split shape V_split shape axis= attn_mask None attn_mask = np concatenate attn_mask np ones axis= key_padding_mask None key_padding_mask = np concatenate key_padding_mask np full batch_sz False dtype=bool axis= attn_heads ref_attn_weight = _scaled_dot_attn_ref Q=Q_split K=K_split V=V_split dims=Q_split shape unseen_mask=attn_mask key_padding_mask=key_padding_mask combined_attn_heads = _combine_heads_ref X=attn_heads dims= batch_sz nheads=nheads d_head=d_head reference = _fc combined_attn_heads multihead_attn_module out_proj weight multihead_attn_module out_proj bias reference = np squeeze reference axis= result = reference assertEqual tuple result shape batch_sz d_model np testing assert_allclose result reference atol= e- result_weight = ref_attn_weight result_weight = result_weight detach numpy assertEqual tuple result_weight shape tuple ref_attn_weight shape np testing assert_allclose result_weight ref_attn_weight atol= e- test_multihead_attn_add_bias_kv _multihead_attn_test_helper add_bias_kv=True test_multihead_attn_add_zero_attn _multihead_attn_test_helper add_zero_attn=True test_multihead_attn_no_masking _multihead_attn_test_helper test_multihead_attn_key_padding_mask _multihead_attn_test_helper add_key_padding_mask=True test_multihead_attn_saved_kv _multihead_attn_test_helper saved_kv=True test_multihead_attn_add_bias_kv_zero_attn _multihead_attn_test_helper add_key_padding_mask=True add_bias_kv=True add_zero_attn=True test_multihead_attn_all_arguments _multihead_attn_test_helper add_key_padding_mask=True add_zero_attn=True saved_kv=True test_multihead_attn_all_arguments _multihead_attn_test_helper add_key_padding_mask=True add_bias_kv=True add_zero_attn=True saved_kv=True test_multihead_attn_all_arguments _multihead_attn_test_helper add_key_padding_mask=True add_zero_attn=True saved_kv=True same_embed_dim=True test_multihead_attn_add_zero_attn Test MultiheadAttention add_zero_attn test_multihead_attn_add_bias_kv Test MultiheadAttention add_bias_kv test_multihead_attn_no_masking Test MultiheadAttention without masking test_multihead_attn_key_padding_mask Test MultiheadAttention src lengths test_multihead_attn_saved_kv Test MultiheadAttention static kv test_multihead_attn_add_bias_kv_zero_attn Test MultiheadAttention bias_kv zero_attn test_multihead_attn_all_arguments Test MultiheadAttention all argument assertRaisesRegex AssertionError bias cannot added static key test_multihead_attn_all_arguments Test MultiheadAttention all argument test_multihead_attn_all_arguments Test MultiheadAttention all argument test_multihead_attn_ d_attn_mask embed_dim = num_heads = batch_size = src_len = tgt_len = query = torch rand batch_size tgt_len embed_dim N T D key = torch rand batch_size src_len embed_dim N S D value = key N S D attn_mask = torch randint batch_size tgt_len src_len float N T S attn_mask = attn_mask masked_fill attn_mask == float -inf masked_fill attn_mask == mta_model = torch nn MultiheadAttention embed_dim num_heads Generate D results attn_mask_ d = torch repeat_interleave attn_mask num_heads dim= N H T S output_ d = mta_model query transpose key transpose value transpose attn_mask=attn_mask_ d output_ d = output_ d transpose N T D i range batch_size output_ d = mta_model query i unsqueeze transpose key i unsqueeze transpose value i unsqueeze transpose attn_mask=attn_mask i output_ d shape T D assertEqual output_ d i unsqueeze transpose output_ d test_multihead_attn_no_bias embed_dim = num_heads = mha = torch nn MultiheadAttention embed_dim num_heads bias=False Verify bias=False applies both out projection layers assertIsNone mha in_proj_bias assertIsNone mha out_proj bias _test_multihead_attn_invalid_shape_impl mha Batched D query cases query = torch randn key = torch randn value = torch randn msg = expected ` key ` ` value ` -D found -D -D tensors respectively D query D key D value assertRaisesRegex AssertionError msg mha query torch randn value msg = expected ` key ` ` value ` -D found -D -D tensors respectively D query D key D value assertRaisesRegex AssertionError msg mha query key torch randn msg = expected ` key_padding_mask ` ` None ` -D found -D tensor instead D query D key D value D key_padding_mask assertRaisesRegex AssertionError msg mha query key value key_padding_mask=torch tensor False False True True dtype=torch bool msg = expected ` attn_mask ` ` None ` -D -D found -D tensor instead D query D key D value D attn_mask assertRaisesRegex AssertionError msg mha query key value attn_mask=torch tensor False False True True dtype=torch bool Unbatched D query cases query = torch randn key = torch randn value = torch randn msg = expected ` key ` ` value ` -D found -D -D tensors respectively D query D key D value assertRaisesRegex AssertionError msg mha query torch randn value msg = expected ` key ` ` value ` -D found -D -D tensors respectively D query D key D value assertRaisesRegex AssertionError msg mha query key torch randn msg = expected ` key_padding_mask ` ` None ` -D found -D tensor instead D query D key D value D key_padding_mask assertRaisesRegex AssertionError msg mha query key value key_padding_mask=torch tensor False False True True dtype=torch bool msg = expected ` attn_mask ` ` None ` -D -D found -D tensor instead D query D key D value D attn_mask assertRaisesRegex AssertionError msg mha query key value attn_mask=torch tensor False False True True dtype=torch bool msg = r Expected ` attn_mask ` shape \ \ D query D key D value D incorrect attn_mask assertRaisesRegex AssertionError msg mha query key value attn_mask=torch randn bernoulli_ torch bool test_multihead_attn_invalid_shape mha = torch nn MultiheadAttention _test_multihead_attn_invalid_shape_impl mha Give test chance hit fast path Right now won t gating may less restricted future torch no_grad _test_multihead_attn_invalid_shape_impl mha eval torch no_grad test_multihead_attn_fast_path_invalid_shape mha = torch nn MultiheadAttention batch_first=True eval Batched D query cases query = torch randn key = torch randn value = torch randn Currently case will just go slow path get usual message because fails requirement batched msg = expected ` key ` ` value ` -D found -D -D tensors respectively D query D key D value assertRaisesRegex AssertionError msg mha query torch randn value need_weights=False Currently case will just go slow path get usual message because fails requirement batched msg = expected ` key ` ` value ` -D found -D -D tensors respectively D query D key D value assertRaisesRegex AssertionError msg mha query key torch randn need_weights=False msg = expected ` key_padding_mask ` ` None ` -D found -D tensor instead D query D key D value D key_padding_mask assertRaisesRegex AssertionError msg mha query key value key_padding_mask=torch tensor False True True dtype=torch bool need_weights=False msg = expected ` attn_mask ` ` None ` -D -D found -D tensor instead D query D key D value D attn_mask assertRaisesRegex AssertionError msg mha query key value attn_mask=torch tensor False True True dtype=torch bool need_weights=False Unbatched D query cases NOTE error messages same regular path because fast path doesn t support D query = torch randn key = torch randn value = torch randn msg = expected ` key ` ` value ` -D found -D -D tensors respectively D query D key D value assertRaisesRegex AssertionError msg mha query torch randn value msg = expected ` key ` ` value ` -D found -D -D tensors respectively D query D key D value assertRaisesRegex AssertionError msg mha query key torch randn msg = expected ` key_padding_mask ` ` None ` -D found -D tensor instead D query D key D value D key_padding_mask assertRaisesRegex AssertionError msg mha query key value key_padding_mask=torch tensor False False True True dtype=torch bool msg = expected ` attn_mask ` ` None ` -D -D found -D tensor instead D query D key D value D attn_mask assertRaisesRegex AssertionError msg mha query key value attn_mask=torch tensor False False True True dtype=torch bool msg = r Expected ` attn_mask ` shape \ \ D query D key D value D incorrect attn_mask assertRaisesRegex AssertionError msg mha query key value attn_mask=torch randn bernoulli_ torch bool test_multihead_attn_nested_tensor_outside_fast_path mha = torch nn MultiheadAttention batch_first=True eval nt = torch nested nested_tensor torch randn One tested platform linux-bionic-py -clang has torch_function one more these Take advantage test torch_function bailout has_torch_func = torch overrides has_torch_function nt mha in_proj_weight mha in_proj_bias mha out_proj weight mha out_proj bias has_torch_func msg = MultiheadAttention does support NestedTensor argument has_torch_function msg = MultiheadAttention does support NestedTensor outside its fast path grad + enabled biases requires_grad assertRaisesRegex AssertionError msg mha nt nt nt has_torch_func Just give up they re all going fail same message torch no_grad mha nt nt nt torch inference_mode mha nt nt nt nt = torch nested nested_tensor torch randn requires_grad=False nt requires_grad = False assertRaisesRegex AssertionError msg mha nt nt nt mha in_proj_weight requires_grad = False mha in_proj_bias requires_grad = False mha out_proj weight requires_grad = False mha out_proj bias requires_grad = False mha nt nt nt TestMultiheadAttentionNNDeviceType NNTestCase test_multihead_self_attn_two_masks_fast_path device Multihead self-attention should give same result fast path BetterTransformer slow path when both attention mask mask type key padding mask mask type provided torch no_grad embed_dim = num_heads = batch_size = src_len = query = value = key = torch rand batch_size src_len embed_dim device Create masks two different types attn_mask = torch randint src_len src_len bool device key_padding_mask = torch randint batch_size src_len bool device We ll need expanded versions masks masking out outputs below attn_mask_expanded = attn_mask reshape src_len src_len expand batch_size num_heads src_len src_len key_padding_mask_expanded = key_padding_mask reshape batch_size src_len expand batch_size num_heads src_len src_len merged_mask = attn_mask_expanded logical_or key_padding_mask_expanded Compute attention fast path mta_model = torch nn MultiheadAttention embed_dim num_heads batch_first=True device=device mta_model training = False result_fast_path _ = mta_model query key value attn_mask=attn_mask key_padding_mask=key_padding_mask Compute attention slow path result_ref _ = torch nn functional multi_head_attention_forward query transpose key transpose value transpose embed_dim num_heads mta_model in_proj_weight mta_model in_proj_bias mta_model bias_k mta_model bias_v mta_model add_zero_attn mta_model dropout mta_model out_proj weight mta_model out_proj bias training=mta_model training key_padding_mask=key_padding_mask need_weights=False attn_mask=attn_mask use_separate_proj_weight=False q_proj_weight=mta_model q_proj_weight k_proj_weight=mta_model k_proj_weight v_proj_weight=mta_model v_proj_weight average_attn_weights=False result_ref = result_ref transpose Convert batch-first Rows which completely masked out nan we need exclude them comparison mask_out = merged_mask all - keepdim=True expand batch_size src_len embed_dim result_fast_path_masked = result_fast_path masked_fill mask_out result_ref_masked = result_ref masked_fill mask_out assertEqual result_fast_path_masked result_ref_masked torch no_grad unittest skipIf TEST_WITH_CROSSREF CrossRef turns TorchFunctionMode so disables fastpath test_multihead_self_attn_two_masks_fast_path_mock device Multihead self-attention should take fast path when both attention mask mask type key padding mask mask type provided same time CPU CUDA PrivateUse device = device rstrip device cpu cuda xpu torch _C _get_privateuse _backend_name skipTest Fastpath only runs CPU CUDA XPU PrivateUse torch autocast device_type=device enabled=False embed_dim = num_heads = batch_size = src_len = query = value = key = torch rand batch_size src_len embed_dim device Create masks two different types attn_mask = torch randint src_len src_len bool device key_padding_mask = torch randint batch_size src_len bool device mock patch torch _native_multi_head_attention new=mock MagicMock return_value= torch Tensor torch Tensor fastpath_mock Compute attention fast path mta_model = torch nn MultiheadAttention embed_dim num_heads batch_first=True device=device eval mta_model training = False mta_model query key value attn_mask=attn_mask key_padding_mask=key_padding_mask If mock called fastpath taken assertTrue fastpath_mock called onlyOn cuda xpu torch _C _get_privateuse _backend_name dtypes torch half torch float torch double test_multihead_attention_dtype device dtype embed_dim = num_heads = sl = bs = model = nn MultiheadAttention embed_dim num_heads device dtype q = torch randn sl bs embed_dim device=device dtype=dtype k = torch randn sl bs embed_dim device=device dtype=dtype v = torch randn sl bs embed_dim device=device dtype=dtype out = model q k v assertEqual q size out size assertEqual dtype out dtype onlyOn cuda xpu torch _C _get_privateuse _backend_name dtypes torch half torch float torch double test_multihead_attention_dtype_batch_first device dtype embed_dim = num_heads = sl = bs = With batch_first=True we have possibility hitting native fast path we call eval enable inference mode Test both paths training True False model = nn MultiheadAttention embed_dim num_heads batch_first=True device dtype training model = model eval cm = torch no_grad cm = contextlib nullcontext cm q = torch randn bs sl embed_dim device=device dtype=dtype k = torch randn bs sl embed_dim device=device dtype=dtype v = torch randn bs sl embed_dim device=device dtype=dtype fast path currently doesn t support weights out = model q k v need_weights=False assertEqual q size out size assertEqual dtype out dtype dtypes torch double torch no_grad test_multihead_attn_fast_path_query_and_bias_have_different_dtypes device dtype mha = torch nn MultiheadAttention batch_first=True dtype=dtype device=device eval mha in_proj_bias = torch nn Parameter mha in_proj_bias torch half device query = torch randn dtype=dtype device=device mha query query query dtypes torch double torch no_grad test_multihead_attn_fast_path_small_test device dtype mha = torch nn MultiheadAttention batch_first=True dtype=dtype device=device eval query = torch randn dtype=dtype device=device mha query query query dtypes torch double test_fast_path_check_with_mask_does_not_break_in_compile device dtype Test TransformerEncoder fast path determination src_key_padding_mask set Specifically ensure mask left-align check doesn t fail torch compile See https github com pytorch pytorch issues layer = nn TransformerEncoderLayer d_model= nhead= batch_first=True dropout= device=device dtype=dtype encoder = nn TransformerEncoder layer num_layers= eval encoder = torch compile encoder fullgraph=True x = torch randn dtype=dtype device=device pad_mask = torch rand device=device pad_mask = True encoder x mask=None src_key_padding_mask=pad_mask dtypes torch double torch no_grad test_multihead_attn_in_proj_bias_none device dtype mha = torch nn MultiheadAttention bias=False dtype=dtype device=device query = torch rand dtype=dtype device=device mha query query query dtypes torch double torch no_grad test_multihead_attn_in_proj_weight_none device dtype Setting kdim == vdim == means vdim = embed_dim will cause logic use per-input project weights thereby forcing in_proj_weight = None mha = torch nn MultiheadAttention vdim= kdim= dtype=dtype device=device query = torch rand dtype=dtype device=device key = torch rand dtype=dtype device=device mha query key key instantiate_device_type_tests TestMultiheadAttentionNNDeviceType globals instantiate_parametrized_tests TestMultiheadAttentionNN __name__ == __main__ run_tests