mypy allow-untyped-defs collections abc Callable dataclasses dataclass functools partial typing Any Optional torch torch _export utils _disable_aten_to_metadata_assertions torch _higher_order_ops out_dtype out_dtype torch ao quantization fx _decomposed quantized_decomposed_lib noqa F torch ao quantization pt e export_utils _WrapperModule torch ao quantization pt e utils _get_aten_graph_module_for_pattern _replace_literals_with_existing_placeholders _replace_literals_with_new_placeholders remove_tensor_overload_for_qdq_ops torch fx GraphModule torch fx subgraph_rewriter replace_pattern __all__ = reference_representation_rewrite _qdq_quantized_linear x_i x_scale x_zero_point x_quant_min x_quant_max weight_i weight_scale weight_zero_point weight_quant_min weight_quant_max bias_fp out_scale out_zero_point out_quant_min out_quant_max x_fp = torch ops quantized_decomposed dequantize_per_tensor x_i x_scale x_zero_point x_quant_min x_quant_max torch int weight_fp = torch ops quantized_decomposed dequantize_per_tensor weight_i weight_scale weight_zero_point weight_quant_min weight_quant_max torch int out_fp = torch ops aten linear default x_fp weight_fp bias_fp out_i = torch ops quantized_decomposed quantize_per_tensor out_fp out_scale out_zero_point out_quant_min out_quant_max torch int out_i _reference_quantized_linear x_i x_scale x_zero_point x_quant_min x_quant_max weight_i weight_scale weight_zero_point weight_quant_min weight_quant_max bias_fp out_scale out_zero_point out_quant_min out_quant_max without using quant_min max clamp traced graph will have quant_mi max args This results failure match pattern Therefore we call torch ops aten clamp here x_i = torch ops aten clamp x_i x_quant_min x_quant_max weight_i = torch ops aten clamp weight_i weight_quant_min weight_quant_max x_i = x_i torch int weight_i = weight_i torch int always set bias None so same representation can work case no matter bias_scale == x_scale weight_scale acc_i = out_dtype torch ops aten linear default torch int x_i - x_zero_point weight_i - weight_zero_point None TODO change mul Scalar Note we quantizing bias these scales without signal user might OK bias_scale = x_scale weight_scale bias_i = out_dtype torch ops aten div Tensor torch int bias_fp bias_scale acc_i = acc_i + bias_i TODO change mul Scalar when we make x_scale weight_scale etc Scalar values acc_i = out_dtype torch ops aten mul Tensor torch int acc_i x_scale weight_scale out_scale + out_zero_point out_i = torch ops aten clamp acc_i out_quant_min out_quant_max torch int out_i _qdq_dynamic_quantized_linear x_fp x_quant_min x_quant_max x_eps weight_i weight_scale weight_zero_point weight_quant_min weight_quant_max bias_fp x_scale x_zero_point = torch ops quantized_decomposed choose_qparams x_fp x_quant_min x_quant_max x_eps torch int x_i = torch ops quantized_decomposed quantize_per_tensor x_fp x_scale x_zero_point x_quant_min x_quant_max torch int x_fp = torch ops quantized_decomposed dequantize_per_tensor x_i x_scale x_zero_point x_quant_min x_quant_max torch int weight_fp = torch ops quantized_decomposed dequantize_per_tensor weight_i weight_scale weight_zero_point weight_quant_min weight_quant_max torch int out_fp = torch ops aten linear default x_fp weight_fp bias_fp out_fp _reference_dynamic_quantized_linear x_fp x_quant_min x_quant_max x_eps weight_i weight_scale weight_zero_point weight_quant_min weight_quant_max bias_fp x_scale x_zero_point = torch ops quantized_decomposed choose_qparams x_fp x_quant_min x_quant_max x_eps torch int decomposed representation quantize_per_tensor TODO use out_dtype mul here when op ready x_fp = x_fp x_scale fp round modes might different here pytorch rounding even which also common most backends x_fp = torch round x_fp fp x_i = x_fp dtype=torch int int x_i = x_i + x_zero_point int clamp works fp int int dtypes x_i = torch clamp x_i x_quant_min x_quant_max int x_i = x_i dtype=torch int weight_i = torch ops aten clamp weight_i weight_quant_min weight_quant_max x_i = x_i torch int weight_i = weight_i torch int always set bias None so same representation can work case no matter bias_scale == x_scale weight_scale acc_i = out_dtype torch ops aten linear default torch int x_i - x_zero_point weight_i - weight_zero_point None bias_scale = x_scale weight_scale bias_i = out_dtype torch ops aten div Tensor torch int bias_fp bias_scale acc_i = acc_i + bias_i out_fp = acc_i x_scale weight_scale out_fp _qdq_quantized_conv d x_i x_scale x_zero_point x_quant_min x_quant_max weight_i weight_scale weight_zero_point weight_quant_min weight_quant_max bias_fp out_scale out_zero_point out_quant_min out_quant_max stride = padding = dilation = transposed = False output_padding = groups = x_fp = torch ops quantized_decomposed dequantize_per_tensor x_i x_scale x_zero_point x_quant_min x_quant_max torch int weight_fp = torch ops quantized_decomposed dequantize_per_tensor weight_i weight_scale weight_zero_point weight_quant_min weight_quant_max torch int out_fp = torch ops aten convolution default x_fp weight_fp bias_fp stride padding dilation transposed output_padding groups out_i = torch ops quantized_decomposed quantize_per_tensor out_fp out_scale out_zero_point out_quant_min out_quant_max torch int out_i _reference_quantized_conv d x_i x_scale x_zero_point x_quant_min x_quant_max weight_i weight_scale weight_zero_point weight_quant_min weight_quant_max bias_fp out_scale out_zero_point out_quant_min out_quant_max stride = padding = dilation = transposed = False output_padding = groups = without using quant_min max clamp traced graph will have quant_mi max args This results failure match pattern Therefore we call torch ops aten clamp here x_i = torch ops aten clamp x_i x_quant_min x_quant_max weight_i = torch ops aten clamp weight_i weight_quant_min weight_quant_max x_i = x_i torch int weight_i = weight_i torch int always set bias None so same representation can work case no matter bias_scale == x_scale weight_scale acc_i = out_dtype torch ops aten convolution default torch int x_i - x_zero_point weight_i - weight_zero_point None stride padding dilation transposed output_padding groups Note we quantizing bias these scales without signal user might OK bias_scale = x_scale weight_scale bias quantization int uses bias_scale = x_scale weight_scale due Take linear calculation example Out_ i j _fp = Sum_ over k X_ i k _fp W_ i k _fp + bias_ i _fp Represent X W fp their dequant transforms A_fp = A_q - A_zero_point A_scale Out_ i j _fp = Sum_ over k X_ i k _fp - X_zp X_scale W_ i k _fp - W_zp W_scale + bias_ i _fp Factor out X_scale W_scale Out_ i j _fp = X_scale W_scale Sum_ over k X_ i k _fp - X_zp W_ i k _fp - W_zp + bias_ i _fp In order addition bias_ i _fp inside we must do Out_ i j _fp = X_scale W_scale Sum_ over k X_ i k _fp - X_zp W_ i k _fp - W_zp + X_scale W_scale bias_ i _fp W_scale noqa B Note we had multiply bias_fp X_scale W_scale = bias_scale Thus bias quantization int must X_scale W_scale bias_i = out_dtype torch ops aten div Tensor torch int bias_fp bias_scale Unsqueeze match broadcast dims Unfortnuately I cannot do bias_i unsqueeze due literal matching nightmare graph pattern replacement bias_i = bias_i unsqueeze - bias_i = bias_i unsqueeze - acc_i = acc_i + bias_i TODO change mul Scalar when we make x_scale weight_scale etc Scalar values acc_i = out_dtype torch ops aten mul Tensor torch int acc_i x_scale weight_scale out_scale + out_zero_point out_i = torch ops aten clamp acc_i out_quant_min out_quant_max torch int out_i _qdq_quantized_add_relu x_i x_scale x_zero_point y_i y_scale y_zero_point out_scale out_zero_point quant_min quant_max x_fp = torch ops quantized_decomposed dequantize_per_tensor x_i x_scale x_zero_point quant_min quant_max torch int y_fp = torch ops quantized_decomposed dequantize_per_tensor y_i y_scale y_zero_point quant_min quant_max torch int out_fp = x_fp + y_fp out_fp = torch ops aten relu out_fp out_i = torch ops quantized_decomposed quantize_per_tensor out_fp out_scale out_zero_point quant_min quant_max torch int out_i _reference_quantized_add_relu x_i x_scale x_zero_point y_i y_scale y_zero_point out_scale out_zero_point quant_min quant_max See comments ` _reference_quantized_add ` more information how derive formula out_i based x_i y_i x_i = x_i torch int y_i = y_i torch int TODO change mul Scalar x_i = out_dtype torch ops aten mul Tensor torch int x_i - x_zero_point x_scale out_scale y_i = out_dtype torch ops aten mul Tensor torch int y_i - y_zero_point y_scale out_scale out_i = x_i + y_i + out_zero_point out_i = torch ops aten clamp out_i out_zero_point out_i = torch ops aten clamp out_i out_zero_point quant_max torch int out_i _qdq_quantized_add x_i x_scale x_zero_point y_i y_scale y_zero_point out_scale out_zero_point quant_min quant_max x_fp = torch ops quantized_decomposed dequantize_per_tensor x_i x_scale x_zero_point quant_min quant_max torch int y_fp = torch ops quantized_decomposed dequantize_per_tensor y_i y_scale y_zero_point quant_min quant_max torch int out_fp = x_fp + y_fp out_i = torch ops quantized_decomposed quantize_per_tensor out_fp out_scale out_zero_point quant_min quant_max torch int out_i _reference_quantized_add x_i x_scale x_zero_point y_i y_scale y_zero_point out_scale out_zero_point quant_min quant_max How Derive formula out_i based x_i y_i since quantized add takes x_i y_i their quantization parameters produce out_i out_i quantized output we can write down formula first out_i = out_f out_scale + out_zero_point then out_fp computed x_f + y_f x_fp y_fp dequantized x_i y_i out_f = x_f + y_f x_fp = x_i - x_zero_point x_scale y_fp = y_i - y_zero_point y_scale applying above formula out_i equation we can get following out_i = out_fp out_scale + out_zero_point = x_f + y_f out_scale + out_zero_point applying substitute out_fp x_fp + y_fp = x_i - x_zero_point x_scale + y_i - y_zero_point y_scale out_scale + out_zero_point apply x_i = x_i torch int y_i = y_i torch int TODO use out_dtype op x_i = torch round x_scale out_scale x_i - x_zero_point torch int y_i = torch round y_scale out_scale y_i - y_zero_point torch int out_i = x_i + y_i + out_zero_point quant_min = - quant_max = out_i = torch ops aten clamp out_i quant_min quant_max torch int out_i _qdq_quantized_max_pool d x_i x_scale x_zero_point x_quant_min x_quant_max out_scale out_zero_point out_quant_min out_quant_max kernel_size = stride = padding = dilation = ceil_mode = False x_fp = torch ops quantized_decomposed dequantize_per_tensor x_i x_scale x_zero_point x_quant_min x_quant_max torch int out_fp _ = torch ops aten max_pool d_with_indices default x_fp kernel_size stride padding dilation ceil_mode out_i = torch ops quantized_decomposed quantize_per_tensor out_fp out_scale out_zero_point out_quant_min out_quant_max torch int out_i _reference_quantized_max_pool d x_i x_scale x_zero_point x_quant_min x_quant_max out_scale out_zero_point out_quant_min out_quant_max kernel_size = stride = padding = dilation = ceil_mode = False preserve x_quant_min x_quant_max graph pattern matching x_i = torch clamp x_i x_quant_min x_quant_max x_i = x_i torch int out_i _ = torch ops aten max_pool d_with_indices default x_i - x_zero_point kernel_size stride padding dilation ceil_mode out_fp = out_i x_scale out_scale + out_zero_point out_fp = torch clamp out_fp out_quant_min out_quant_max out_i = out_fp torch int out_i _quantize_per_tensor_int x_fp scale zero_point quant_min quant_max x = torch ops quantized_decomposed quantize_per_tensor x_fp scale zero_point quant_min quant_max torch int x _reference_quantize_per_tensor_int x_fp scale zero_point quant_min quant_max TODO use out_dtype mul here when op ready x = x_fp scale fp round modes might different here pytorch rounding even which also common most backends x = torch round x fp x = x dtype=torch int int x = x + zero_point int clamp works fp int int dtypes x = torch clamp x quant_min quant_max int x = x dtype=torch int x _dequantize_per_tensor_int x_i scale zero_point quant_min quant_max x_fp = torch ops quantized_decomposed dequantize_per_tensor x_i scale zero_point quant_min quant_max torch int x_fp _reference_dequantize_per_tensor_int x_i scale zero_point quant_min quant_max without using quant_min max clamp traced graph will have quant_mi max args This results failure match pattern Therefore we call torch ops aten clamp here x_i = torch ops aten clamp x_i quant_min quant_max TODO use out_dtype op note x_i torch int does work here TODO debug implementation later when torchdynamo time out issue resolved x_i torch float - zero_point scale dtype=torch float _quantize_per_channel_int x_fp scales zero_points ch_axis quant_min quant_max out_i = torch ops quantized_decomposed quantize_per_channel x_fp scales zero_points ch_axis quant_min quant_max torch int out_i _reference_quantize_per_channel_int x_fp scales zero_points ch_axis quant_min quant_max x_fp = torch transpose x_fp ch_axis - out_i = torch ops aten clamp torch round x_fp scales torch int + zero_points quant_min quant_max out_i = torch transpose out_i ch_axis - out_i torch int _dequantize_per_channel_int x_i scales zero_points ch_axis quant_min quant_max following will replaced placeholders out_fp = torch ops quantized_decomposed dequantize_per_channel x_i scales zero_points ch_axis quant_min quant_max torch int out_fp _reference_dequantize_per_channel_int x_i scales zero_points ch_axis quant_min quant_max following will replaced placeholders order preserve quant_min quant_max args pattern matching e g matching int quantized ops we call torch ops aten clamp here x_i = torch ops aten clamp x_i quant_min quant_max x_i = torch transpose x_i ch_axis - x_i = x_i torch int out_fp = x_i - zero_points torch float scales out_fp = torch transpose out_fp ch_axis - out_fp _replace_ph_qdq_per_channel_replacement gm torch fx GraphModule _replace_literals_with_existing_placeholders gm exclude_literals= - literal_to_ph_idx= - dataclass _RewriteInfo Data needed rewrite includes example inputs pattern replacement functions post transformation functions exported pattern replacement GraphModule example inputs used exporting pattern into GraphModule example_inputs tuple Any pattern Callable replacement Callable post transformation exported pattern replacement GraphModule pattern_post_trans Optional Callable GraphModule GraphModule = None replacement_post_trans Optional Callable GraphModule GraphModule = None reference_representation_rewrite model GraphModule - GraphModule _QUANTIZED_LINEAR_EXAMPLE_INPUTS = torch randint - dtype=torch int torch randn dtype=torch float torch zeros dtype=torch int torch tensor - dtype=torch int torch tensor dtype=torch int torch randint - dtype=torch int torch randn dtype=torch float torch zeros dtype=torch int torch tensor - dtype=torch int torch tensor dtype=torch int torch randn dtype=torch float torch randn dtype=torch float torch zeros dtype=torch int torch tensor - dtype=torch int torch tensor dtype=torch int _DYNAMIC_QUANTIZED_LINEAR_EXAMPLE_INPUTS = torch randn dtype=torch float - torch finfo torch float eps torch randint - dtype=torch int torch randn dtype=torch float torch zeros dtype=torch int torch tensor - dtype=torch int torch tensor dtype=torch int torch randn dtype=torch float _QUANTIZED_CONV d_EXAMPLE_INPUTS = torch randint - dtype=torch int torch randn dtype=torch float torch zeros dtype=torch int torch tensor - dtype=torch int torch tensor dtype=torch int torch randint - dtype=torch int torch randn dtype=torch float torch zeros dtype=torch int torch tensor - dtype=torch int torch tensor dtype=torch int torch randn dtype=torch float torch randn dtype=torch float torch zeros dtype=torch int torch tensor - dtype=torch int torch tensor dtype=torch int _QUANTIZED_ADD_OR_ADD_RELU_EXAMPLE_INPUTS = torch randint - dtype=torch int torch randn dtype=torch float torch zeros dtype=torch int torch randint - dtype=torch int torch randn dtype=torch float torch zeros dtype=torch int torch randn dtype=torch float torch zeros dtype=torch int torch tensor - dtype=torch int torch tensor dtype=torch int _QUANTIZED_MAX_POOL D_EXAMPLE_INPUTS = torch randint - dtype=torch int torch randn dtype=torch float torch zeros dtype=torch int torch tensor - dtype=torch int torch tensor dtype=torch int torch randn dtype=torch float torch zeros dtype=torch int torch tensor - dtype=torch int torch tensor dtype=torch int _QUANTIZE_PER_TENSOR_INT _EXAMPLE_INPUTS = torch randn dtype=torch float torch randn dtype=torch float torch zeros dtype=torch int torch tensor - dtype=torch int torch tensor dtype=torch int _DEQUANTIZE_PER_TENSOR_INT _EXAMPLE_INPUTS = torch randint - dtype=torch int torch randn dtype=torch float torch zeros dtype=torch int torch tensor - dtype=torch int torch tensor dtype=torch int _QUANTIZE_PER_CHANNEL_INT _EXAMPLE_INPUTS = torch randn dtype=torch float torch randn dtype=torch float torch zeros dtype=torch int - _DEQUANTIZE_PER_CHANNEL_INT _EXAMPLE_INPUTS = torch randint - dtype=torch int torch randn dtype=torch float torch zeros dtype=torch int - _REWRITE_INFO_LIST = _RewriteInfo _DYNAMIC_QUANTIZED_LINEAR_EXAMPLE_INPUTS _WrapperModule _qdq_dynamic_quantized_linear _WrapperModule _reference_dynamic_quantized_linear partial _replace_literals_with_existing_placeholders literal_to_ph_idx= - torch finfo torch float eps partial _replace_literals_with_existing_placeholders literal_to_ph_idx= - torch finfo torch float eps _RewriteInfo _QUANTIZED_LINEAR_EXAMPLE_INPUTS _WrapperModule _qdq_quantized_linear _WrapperModule _reference_quantized_linear _replace_literals_with_new_placeholders _replace_literals_with_new_placeholders _RewriteInfo _QUANTIZED_CONV d_EXAMPLE_INPUTS _WrapperModule _qdq_quantized_conv d _WrapperModule _reference_quantized_conv d partial _replace_literals_with_new_placeholders exclude_literals= - partial _replace_literals_with_new_placeholders exclude_literals= - _RewriteInfo _QUANTIZED_ADD_OR_ADD_RELU_EXAMPLE_INPUTS _WrapperModule _qdq_quantized_add_relu _WrapperModule _reference_quantized_add_relu _RewriteInfo _QUANTIZED_ADD_OR_ADD_RELU_EXAMPLE_INPUTS _WrapperModule _qdq_quantized_add _WrapperModule _reference_quantized_add _RewriteInfo _QUANTIZED_MAX_POOL D_EXAMPLE_INPUTS _WrapperModule _qdq_quantized_max_pool d _WrapperModule _reference_quantized_max_pool d _replace_literals_with_new_placeholders _replace_literals_with_new_placeholders _RewriteInfo _QUANTIZE_PER_TENSOR_INT _EXAMPLE_INPUTS _WrapperModule _quantize_per_tensor_int _WrapperModule _reference_quantize_per_tensor_int _RewriteInfo _DEQUANTIZE_PER_TENSOR_INT _EXAMPLE_INPUTS _WrapperModule _dequantize_per_tensor_int _WrapperModule _reference_dequantize_per_tensor_int _RewriteInfo _QUANTIZE_PER_CHANNEL_INT _EXAMPLE_INPUTS _WrapperModule _quantize_per_channel_int _WrapperModule _reference_quantize_per_channel_int _replace_ph_qdq_per_channel_replacement _replace_ph_qdq_per_channel_replacement _RewriteInfo _DEQUANTIZE_PER_CHANNEL_INT _EXAMPLE_INPUTS _WrapperModule _dequantize_per_channel_int _WrapperModule _reference_dequantize_per_channel_int _replace_ph_qdq_per_channel_replacement _replace_ph_qdq_per_channel_replacement remove_tensor_overload_for_qdq_ops model _disable_aten_to_metadata_assertions rewrite_info _REWRITE_INFO_LIST example_inputs = rewrite_info example_inputs pattern = rewrite_info pattern replacement = rewrite_info replacement pattern_post_trans = rewrite_info pattern_post_trans replacement_post_trans = rewrite_info replacement_post_trans pattern = _get_aten_graph_module_for_pattern pattern example_inputs type ignore arg-type assignment remove_tensor_overload_for_qdq_ops pattern type ignore arg-type replacement = _get_aten_graph_module_for_pattern type ignore assignment replacement example_inputs type ignore arg-type remove_tensor_overload_for_qdq_ops replacement type ignore arg-type pattern_post_trans pattern = pattern_post_trans pattern replacement_post_trans replacement = replacement_post_trans replacement pattern recompile type ignore attr-defined replacement recompile type ignore attr-defined replace_pattern model pattern replacement model