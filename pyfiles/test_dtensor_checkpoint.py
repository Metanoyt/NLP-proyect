Owner s oncall distributed typing Union torch torch distributed dist torch distributed checkpoint dist_cp torch distributed tensor DeviceMesh distribute_tensor DTensor Replicate Shard zeros torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor DTensorTestBase skip_if_lt_x_gpu with_comms torch testing _internal distributed checkpoint_utils with_temp_dir SUBMESH_TENSOR_SIZE = MyTestModule torch nn Module __init__ sdt DTensor rdt DTensor submesh_sdt DTensor submesh_rdt DTensor extra_state int = extra_state_tensor torch Tensor = torch zeros - None super __init__ sdt = torch nn Parameter sdt rdt = torch nn Parameter rdt submesh_sdt = torch nn Parameter submesh_sdt submesh_rdt = torch nn Parameter submesh_rdt _extra_state = extra_state _extra_state_tensor = extra_state_tensor property extra_state - int _extra_state extra_state setter extra_state new_extra_state int - None _extra_state = new_extra_state property extra_state_tensor - torch Tensor _extra_state_tensor extra_state_tensor setter extra_state_tensor new_extra_state_tensor torch Tensor - None _extra_state_tensor = new_extra_state_tensor get_extra_state - dict str Union int torch _tensor Tensor extra_state _extra_state extra_state_tensor _extra_state_tensor set_extra_state state dict str Union int torch _tensor Tensor - None _extra_state = state extra_state pyre-ignore _extra_state_tensor = state extra_state_tensor pyre-ignore DTensorPlanner DTensorTestBase create_dtensor_model tensor_to_shard torch tensor tensor_to_replicate torch tensor - torch nn Module mesh = DeviceMesh device_type=self device_type mesh=range dist get_world_size sharded_dt = distribute_tensor tensor_to_shard mesh placements= Shard replicated_dt = distribute_tensor tensor_to_replicate mesh placements= Replicate Only even rank will part mesh submesh = DeviceMesh device_type=self device_type mesh= i i range dist get_world_size i == submesh_tensor_size = SUBMESH_TENSOR_SIZE submesh_sharded_dt = zeros submesh_tensor_size device_mesh=submesh placements= Shard submesh_replicated_dt = zeros submesh_tensor_size device_mesh=submesh placements= Replicate model = MyTestModule sharded_dt replicated_dt submesh_sharded_dt submesh_replicated_dt device_type model sharded_dt replicated_dt with_comms with_temp_dir skip_if_lt_x_gpu test_distributed_tensor_planner - None CHECKPOINT_DIR = temp_dir local_tensor = torch arange dtype=torch float local_tensor_ = torch arange dtype=torch float model sharded_dt replicated_dt = create_dtensor_model local_tensor local_tensor_ state_dict = model state_dict When model initialized state_dict rank follows when there GPUs rank OrderedDict rdt DTensor local_tensor=tensor device=f device_type device_mesh=DeviceMesh placements= Replicate sdt DTensor local_tensor=tensor device=f device_type device_mesh=DeviceMesh placements= Shard dim= submesh_sdt DTensor local_tensor=tensor device=f device_type device_mesh=DeviceMesh placements= Shard dim= submesh_rdt DTensor local_tensor=tensor device=f device_type device_mesh=DeviceMesh placements= Replicate _extra_state extra_state extra_state_tensor tensor dist_cp save state_dict=state_dict storage_writer=dist_cp FileSystemWriter path=CHECKPOINT_DIR planner=dist_cp DefaultSavePlanner model _ _ = create_dtensor_model local_tensor local_tensor_ state_dict = model state_dict When model re-initialized we have changed params state_dict The updated values follows when there GPUs rank OrderedDict rdt DTensor local_tensor=tensor device=f device_type device_mesh=DeviceMesh placements= Replicate sdt DTensor local_tensor=tensor device=f device_type device_mesh=DeviceMesh placements= Shard dim= submesh_sdt DTensor local_tensor=tensor device=f device_type device_mesh=DeviceMesh placements= Shard dim= submesh_rdt DTensor local_tensor=tensor device=f device_type device_mesh=DeviceMesh placements= Replicate _extra_state extra_state extra_state_tensor tensor dist_cp load state_dict=state_dict storage_reader=dist_cp FileSystemReader CHECKPOINT_DIR planner=dist_cp DefaultLoadPlanner After loading model checkpoint we want make sure values state_dict match values originally saved checkpoint k v state_dict items k == sdt assertEqual sharded_dt to_local v to_local k == rdt assertEqual replicated_dt to_local v to_local k == submesh_sdt rank == shard_size = int SUBMESH_TENSOR_SIZE v device_mesh size assertEqual v to_local size torch Size shard_size assertEqual v to_local torch zeros shard_size assertEqual v to_local size torch Size assertEqual v to_local torch tensor k == submesh_rdt rank == shard_size = SUBMESH_TENSOR_SIZE assertEqual v to_local size torch Size shard_size assertEqual v to_local torch zeros shard_size assertEqual v to_local size torch Size assertEqual v to_local torch tensor k == _extra_state assertEqual v extra_state assertEqual torch tensor v extra_state_tensor __name__ == __main__ run_tests