Owner s oncall distributed contextlib copy functools itertools unittest collections defaultdict collections abc Iterable typing Union torch torch distributed dist torch nn nn torch distributed _composable checkpoint torch distributed _composable replicate_with_fsdp replicate torch distributed algorithms _checkpoint checkpoint_wrapper _CHECKPOINT_PREFIX apply_activation_checkpointing torch distributed device_mesh DeviceMesh torch distributed fsdp CPUOffloadPolicy FSDPModule OffloadPolicy register_fsdp_forward_method torch distributed tensor DTensor init_device_mesh torch distributed tensor debug CommDebugMode torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp check_sharded_parity compiled_fsdp_test FSDPTest FSDPTestMultiThread MLP MLPStack patch_all_gather patch_reduce_scatter torch testing _internal common_utils get_cycles_per_ms run_tests TEST_HPU wrapSwapTensorsTest torch testing _internal distributed _tensor common_dtensor ModelArgs Transformer TransformerBlock c d_ops = torch ops c d funcol = torch ops c d_functional torch testing _internal common_fsdp get_devtype device_type = torch device get_devtype TestReplicateForwardInputs FSDPTestMultiThread property world_size - int skip_if_lt_x_gpu test_root_move_forward_input_to_device device = torch device device_type type ParamlessModule nn Module forward x torch Tensor ys tuple torch Tensor Check Replicate moved inputs GPU including recursing into tuple data structure assert x device == device f Expects device got x device assert ys device == device f Expects device got ys device assert ys device == device f Expects device got ys device y = ys + ys x + y + model = ParamlessModule device replicate model device x = torch randn ys = torch randn torch randn assertEqual x device torch device cpu assertEqual ys device torch device cpu assertEqual ys device torch device cpu model x ys TestReplicateRegisteredParams FSDPTestMultiThread property world_size - int skip_if_lt_x_gpu test_param_registration_after_forward Tests parameter registration after forward device = torch device device_type type Single Replicate group reshard_after_forward True False None torch manual_seed model = MLP device Since seed per process per thread we broadcast ensure same parameters across ranks param model parameters dist broadcast param src= ref_model = copy deepcopy model replicate model reshard_after_forward=reshard_after_forward root only inp = torch randn device=device_type type _assert_dtensor_params model parameters _assert_same_params model parameters ref_model parameters model inp reshard_after_forward _assert_dtensor_params model parameters _assert_tensor_params model parameters _assert_same_params model parameters ref_model parameters model reshard however we can manually reshard _assert_dtensor_params model parameters _assert_same_params model parameters ref_model parameters Multiple Replicate groups reshard_after_forward True False None torch manual_seed model = nn Sequential MLP device MLP device param model parameters dist broadcast param src= ref_model = copy deepcopy model replicate model in_proj reshard_after_forward=reshard_after_forward replicate model out_proj reshard_after_forward=reshard_after_forward replicate model reshard_after_forward=reshard_after_forward _assert_dtensor_params model parameters _assert_same_params model parameters ref_model parameters model inp non_root_params = list model in_proj parameters + list model out_proj parameters root_params = list set model parameters - set non_root_params reshard_after_forward None _assert_dtensor_params non_root_params _assert_tensor_params root_params reshard_after_forward _assert_dtensor_params non_root_params _assert_dtensor_params root_params _assert_tensor_params non_root_params _assert_tensor_params root_params _assert_same_params model parameters ref_model parameters module model modules isinstance module FSDPModule module reshard however we can manually reshard _assert_dtensor_params model parameters _assert_same_params model parameters ref_model parameters skip_if_lt_x_gpu test_param_registration_after_backward Tests parameter registration after backward device = torch device device_type type Single Replicate group reshard_after_forward True False model = MLP device replicate model reshard_after_forward=reshard_after_forward root only inp = torch randn device=device_type type _assert_dtensor_params model parameters model inp sum backward _assert_dtensor_params model parameters Multiple Replicate groups reshard_after_forward True False model = MLP device replicate model in_proj reshard_after_forward=reshard_after_forward replicate model out_proj reshard_after_forward=reshard_after_forward replicate model reshard_after_forward=reshard_after_forward _assert_dtensor_params model parameters model inp sum backward _assert_dtensor_params model parameters _assert_tensor_params params Iterable nn Parameter need iterate over list multiple times params = list params assertGreater len params param params assertNotIsInstance param DTensor assertIsInstance param torch Tensor _assert_dtensor_params params Iterable nn Parameter params = list params assertGreater len params param params assertIsInstance param DTensor _assert_same_params params Iterable nn Parameter ref_params Iterable nn Parameter params ref_params = list params list ref_params assertEqual len params len ref_params param ref_param zip params ref_params isinstance param DTensor param = param full_tensor assertEqual param shape ref_param shape assertEqual param ref_param TestReplicateCastAfterInit FSDPTestMultiThread property world_size - int skip_if_lt_x_gpu wrapSwapTensorsTest True test_to_float _after_init Tests user can cast module float after init NOTE Test fp instead lower precision dtype like bf better numerics The important part changing dtype torch manual_seed mlp_dim device dtype = device_type torch float model = MLP mlp_dim device=device param model parameters dist broadcast param src= ref_model = copy deepcopy model dtype ref_optim = torch optim Adam ref_model parameters lr= e- module model in_proj model out_proj model replicate module model dtype param model parameters assertEqual param dtype dtype assertEqual param to_local dtype dtype assertEqual param _spec tensor_meta dtype dtype optim = torch optim Adam model parameters lr= e- foreach=True check_sharded_parity ref_model model torch manual_seed + rank + inp = torch randn mlp_dim device=device_type type dtype=dtype iter_idx range losses list torch Tensor = _model ref_model model losses append _model inp sum losses - backward param ref_model parameters param grad None dist all_reduce param grad param grad div_ world_size assertEqual losses losses check_sharded_parity ref_model model param model parameters assertEqual param dtype dtype assertEqual param to_local dtype dtype assertEqual param _spec tensor_meta dtype dtype assertEqual param grad dtype dtype assertEqual param grad to_local dtype dtype assertEqual param grad _spec tensor_meta dtype dtype _optim ref_optim optim _optim step _optim zero_grad set_to_none= iter_idx == TestReplicate DTrainingCore FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_train_parity_single_group Tests train parity DDP single FSDP group when sharding parameters dim- run_subtests lin_shapes use_shard_placement_fn False _test_train_parity_single_group _test_train_parity_single_group lin_shapes list tuple int int use_shard_placement_fn bool torch manual_seed model = nn Sequential nn Linear lin_shapes nn ReLU nn Linear lin_shapes ref_model = copy deepcopy model device_type ref_optim = torch optim Adam ref_model parameters lr= e- replicate model optim = torch optim Adam model parameters lr= e- torch manual_seed + rank + inp = torch randn lin_shapes device=device_type type iter_idx range losses list torch Tensor = _model ref_model model losses append _model inp sum losses - backward param ref_model parameters param grad None dist all_reduce param grad param grad div_ world_size _optim ref_optim optim _optim zero_grad set_to_none= iter_idx == _optim step assertEqual losses losses skip_if_lt_x_gpu unittest skipIf TEST_HPU Sleep kernel supported HPU compiled_fsdp_test compile_compute_on_module=Transformer test_train_parity_multi_groups Tests train parity against DDP when using multiple parameter groups communication communication computation overlap plus memory reduction run_subtests reshard_after_forward True False test_device_type device_type type offload_policy OffloadPolicy delay_after_forward False True delay_before_all_gather False True delay_before_reduce_scatter False True delay_before_optim False True unshard_async_op False _test_train_parity_multi_group skip_if_lt_x_gpu unittest skipIf TEST_HPU sleep kernel supported HPU test_train_parity_multi_group_cpu_offload_eager Tests train parity when using multiple parameter groups communication CPU offloading run_subtests reshard_after_forward True save CI time offload_policy CPUOffloadPolicy pin_memory=True CPUOffloadPolicy pin_memory=False test_device_type device_type type delay_after_forward False True delay_before_all_gather False True delay_before_reduce_scatter False True delay_before_optim False True unshard_async_op False _test_train_parity_multi_group _test_train_parity_multi_group reshard_after_forward Union bool int offload_policy OffloadPolicy test_device_type str delay_after_forward bool delay_before_all_gather bool delay_before_reduce_scatter bool delay_before_optim bool unshard_async_op bool Only test individual delays all four delays save test time delay_after_forward + delay_before_all_gather + delay_before_reduce_scatter + delay_before_optim assert test_device_type cuda hpu xpu cpu f test_device_type torch manual_seed vocab_size = model_args = ModelArgs n_layers= n_heads= vocab_size=vocab_size max_seq_len= dropout_p= model = Transformer model_args ref_model = copy deepcopy model device_type ref_optim = torch optim Adam ref_model parameters lr= e- mesh = init_device_mesh test_device_type world_size mesh_dim_names= replicate shard fully_shard_fn = functools partial replicate device_mesh=mesh reshard_after_forward=reshard_after_forward offload_policy=offload_policy module model modules isinstance module TransformerBlock fully_shard_fn module fully_shard_fn model unshard_async_op model _set_unshard_async_op unshard_async_op optim = torch optim Adam model parameters lr= e- delay_in_ms = orig_all_gather = dist all_gather_into_tensor orig_reduce_scatter = dist reduce_scatter_tensor delayed_all_gather args kwargs torch get_device_module device_type _sleep int delay_in_ms get_cycles_per_ms orig_all_gather args kwargs delayed_reduce_scatter args kwargs torch get_device_module device_type _sleep int delay_in_ms get_cycles_per_ms orig_reduce_scatter args kwargs torch manual_seed + rank + patch_all_gather_ctx = patch_all_gather delayed_all_gather delay_before_all_gather contextlib nullcontext patch_reduce_scatter_ctx = patch_reduce_scatter delayed_reduce_scatter delay_before_reduce_scatter contextlib nullcontext patch_all_gather_ctx patch_reduce_scatter_ctx iter_idx range inp = torch randint vocab_size device=device_type losses list torch Tensor = _model _optim ref_model ref_optim model optim losses append _model inp sum _model model delay_after_forward torch get_device_module device_type _sleep int delay_in_ms get_cycles_per_ms losses - backward _model model delay_before_optim torch get_device_module device_type _sleep int delay_in_ms get_cycles_per_ms param ref_model parameters param grad None dist all_reduce param grad param grad div_ world_size _optim ref_optim optim _optim zero_grad set_to_none= iter_idx == _optim step assertEqual losses losses skip_if_lt_x_gpu test_non_root_forward_backward Tests running forward backward through root then through non-root The non-root needs synchronize streams queue callback torch manual_seed lin_dim = model = nn Sequential MLP lin_dim torch device cpu _ range ref_model = copy deepcopy model device_type ref_optim = torch optim Adam ref_model parameters lr= e- mlp model replicate mlp replicate model optim = torch optim Adam model parameters lr= e- foreach=True torch manual_seed + rank inp = torch randn lin_dim device=device_type ref_root_loss = ref_model inp sum ref_root_loss backward param ref_model parameters dist all_reduce param grad param grad detach div_ world_size ref_optim step ref_optim zero_grad ref_nonroot_loss = ref_model inp sum ref_nonroot_loss backward param ref_model parameters param grad None dist all_reduce param grad param grad detach div_ world_size ref_optim step root_loss = model inp sum root_loss backward torch get_device_module device_type _sleep int get_cycles_per_ms optim step optim zero_grad nonroot_loss = model inp sum nonroot_loss backward optim step assertEqual ref_root_loss root_loss assertEqual ref_nonroot_loss nonroot_loss assertEqual ref_model inp sum model inp sum skip_if_lt_x_gpu test_multi_forward_module Tests parity when running module participates multiple times forward run_subtests reshard_after_forward True False _test_multi_forward_module _test_multi_forward_module reshard_after_forward Union bool int MultiForwardModule nn Module __init__ device torch device super __init__ inner = nn Linear device=device outer = nn Linear device=device forward x i = inner x j = inner x outer i + j torch manual_seed model = MultiForwardModule device=device_type type ref_model = copy deepcopy model device_type ref_optim = torch optim Adam ref_model parameters lr= e- replicate model inner replicate model optim = torch optim Adam model parameters lr= e- torch manual_seed + rank inp = torch randn device=device_type type iter_idx range losses list torch Tensor = _model ref_model model losses append _model inp sum losses - backward param ref_model parameters param grad None dist all_reduce param grad param grad div_ world_size _optim ref_optim optim _optim zero_grad set_to_none= iter_idx == _optim step assertEqual losses losses skip_if_lt_x_gpu test_explicit_prefetching torch manual_seed model_args = ModelArgs n_layers= dropout_p= model = Transformer model_args ref_model = copy deepcopy model device_type ref_optim = torch optim AdamW ref_model parameters lr= e- layer itertools chain model layers model replicate layer optim = torch optim AdamW model parameters lr= e- num_to_forward_prefetch = num_to_backward_prefetch = i layer enumerate model layers i = len model layers - num_to_forward_prefetch break layers_to_prefetch = model layers i + j j range num_to_forward_prefetch + layer set_modules_to_forward_prefetch layers_to_prefetch i layer enumerate model layers i num_to_backward_prefetch continue layers_to_prefetch = model layers i - j j range num_to_backward_prefetch + layer set_modules_to_backward_prefetch layers_to_prefetch torch manual_seed + rank inp = torch randint model_args vocab_size device=device_type type _ range losses list torch Tensor = _model ref_model model losses append _model inp sum losses - backward param ref_model parameters param grad None dist all_reduce param grad param grad div_ world_size _optim ref_optim optim _optim zero_grad _optim step assertEqual losses losses skip_if_lt_x_gpu unittest skipIf TEST_HPU Sleep supported HPU test_post_optim_event torch manual_seed model_args = ModelArgs dropout_p= model = Transformer model_args ref_model = copy deepcopy model device_type type ref_optim = torch optim AdamW ref_model parameters lr= e- layer itertools chain model layers model replicate layer optim = torch optim AdamW model parameters lr= e- step_post_hook fsdp_module FSDPModule opt torch optim Optimizer args kwargs - None post_optim_event = torch get_device_module device_type current_stream record_event fsdp_module set_post_optim_event post_optim_event optim register_step_post_hook functools partial step_post_hook model torch manual_seed + rank inp = torch randint model_args vocab_size device=device_type type Track all losses check equality end avoid CPU sync point after each iteration ref_losses list torch Tensor = losses list torch Tensor = _ range ref_optim zero_grad ref_losses append ref_model inp sum ref_losses - backward param ref_model parameters param grad None dist all_reduce param grad param grad div_ world_size ref_optim step _ range optim zero_grad losses append model inp sum losses - backward optim step Sleep after optimizer step allow CPU run ahead into next iteration s forward exercising post-optim stream sync torch get_device_module device_type _sleep int get_cycles_per_ms ref_loss loss zip ref_losses losses assertEqual ref_loss loss TestReplicateTrainingCompose FSDPTest property world_size - int Since these tests run larger transformer model they may see some numeric drift GPUs min torch get_device_module device_type device_count skip_if_lt_x_gpu compiled_fsdp_test compile_compute_on_module=Transformer test_train_parity_with_activation_checkpointing Tests train parity against DDP when composing activation checkpointing run_subtests reshard_after_forward True False checkpoint_impl composable utils wrapper module_grouping block mem_eff mem_eff_weight_tied test_device_type device_type type _test_train_parity_with_activation_checkpointing _test_train_parity_with_activation_checkpointing reshard_after_forward Union bool int checkpoint_impl str module_grouping str test_device_type str assert checkpoint_impl composable utils wrapper testing_compile = replicate = torch distributed _composable replicate_with_fsdp testing_compile checkpoint_impl == composable torch manual_seed vocab_size = torch device device_type model_args = ModelArgs n_layers= n_heads= vocab_size=vocab_size max_seq_len= dropout_p= checkpoint_activations= checkpoint_impl == utils For mem-efficient module grouping we separate embeddings output projection which does support weight tying weight_tying=module_grouping = mem_eff model = Transformer model_args ref_model = copy deepcopy model device_type ref_optim = torch optim Adam ref_model parameters lr= e- Apply activation checkpointing prefixes_to_ignore = checkpoint_impl == wrapper prefixes_to_ignore = _CHECKPOINT_PREFIX apply_activation_checkpointing model check_fn=lambda m isinstance m TransformerBlock checkpoint_impl == composable module model modules isinstance module TransformerBlock checkpoint module Apply Replicate device_mesh = init_device_mesh test_device_type world_size mesh_dim_names= replicate shard fsdp_kwargs = reshard_after_forward reshard_after_forward device_mesh device_mesh module_grouping == mem_eff assert model_args n_layers == replicate model layers fsdp_kwargs replicate model layers model layers fsdp_kwargs replicate model tok_embeddings model pos_embeddings fsdp_kwargs Embedding weights needed embedding backward model tok_embeddings set_unshard_in_backward False replicate model norm model output fsdp_kwargs module_grouping == mem_eff_weight_tied replicate model tok_embeddings model output fsdp_kwargs layer model layers replicate layer fsdp_kwargs module_grouping == block layer model layers replicate layer fsdp_kwargs raise NotImplementedError f Unknown module grouping module_grouping replicate model fsdp_kwargs optim = torch optim Adam model parameters lr= e- torch manual_seed + rank Reuse same input across iterations avoid loss explosion trying learn random inputs inp = torch randint vocab_size device=device_type type check_sharded_parity ref_model model prefixes_to_ignore=prefixes_to_ignore iter_idx range losses list torch Tensor = _model ref_model model torch manual_seed iter_idx + dropout determinism losses append _model inp sum losses - backward param ref_model parameters param grad None dist all_reduce param grad param grad div_ world_size testing_compile check_sharded_parity ref_model model prefixes_to_ignore=prefixes_to_ignore assertEqual losses losses _optim ref_optim optim _optim step _optim zero_grad set_to_none= iter_idx == testing_compile check_sharded_parity ref_model model prefixes_to_ignore=prefixes_to_ignore TestReplicateSharedParams FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_train_parity_with_shared_params run_subtests reshard_after_forward False True use_activation_checkpointing False True _test_train_shared_params _test_train_shared_params reshard_after_forward bool use_activation_checkpointing bool torch manual_seed model_args = ModelArgs n_layers= dropout_p= weight_tying=True model = Transformer model_args ref_model = copy deepcopy model device_type ref_optim = torch optim Adam ref_model parameters lr= e- module model modules isinstance module TransformerBlock use_activation_checkpointing checkpoint module replicate module reshard_after_forward=reshard_after_forward replicate model reshard_after_forward=reshard_after_forward optim = torch optim Adam model parameters lr= e- torch manual_seed + rank + iter_idx range inp = torch randint model_args vocab_size device=device_type type losses list torch Tensor = _model ref_model model losses append _model inp sum losses - backward param ref_model parameters param grad None dist all_reduce param grad param grad div_ world_size _optim ref_optim optim _optim zero_grad set_to_none= iter_idx == _optim step assertEqual losses losses TestReplicateGradientAccumulation FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_gradient_accumulation Tests gradient accumulation without gradient reduction without resharding after backward shard_size replicate_size = world_size meshes = init_device_mesh device_type type replicate_size shard_size mesh_dim_names= replicate shard run_subtests mesh meshes reshard_after_forward True False all disable reduce-scatter all modules root_only disable reduce-scatter root s linear only some_mlps disable reduce-scatter some MLPs mode all root_only some_mlps reshard_after_backward False True offload_policy OffloadPolicy CPUOffloadPolicy For HSDP only ` True ` reduce-scatter only no all-reduce each microbatch until last microbatch ` False ` neither reduce-scatter nor all-reduce each microbatch until last microbatch reduce_scatter_only False True _test_gradient_accumulation _test_gradient_accumulation mesh DeviceMesh reshard_after_forward Union bool int mode str reshard_after_backward bool offload_policy OffloadPolicy reduce_scatter_only bool HSDP reshard_after_backward reshard_after_forward False mode == some_mlps isinstance offload_policy CPUOffloadPolicy reshard_after_forward True mesh ndim = may eventually need change once decision device mesh made skip since common applicable torch manual_seed batch_size lin_dim num_mlps num_microbatches = mode == some_mlps num_mlps_to_disable_reduce_scatter = modules = nn Linear lin_dim lin_dim modules extend MLP lin_dim _ range num_mlps model = nn Sequential modules ref_model = copy deepcopy model device_type replicate_fn = functools partial replicate device_mesh=mesh reshard_after_forward=reshard_after_forward offload_policy=offload_policy mlp model replicate_fn mlp replicate_fn model root gets st linear ref_optim = torch optim Adam ref_model parameters lr= e- optim = torch optim Adam model parameters lr= e- set_grad_sync_flag module nn Module is_last_microbatch bool recurse bool = True reduce_scatter_only module set_requires_all_reduce is_last_microbatch recurse=recurse module set_requires_gradient_sync is_last_microbatch recurse=recurse set_backward_flags _model nn Module is_last_microbatch bool mode == all set_grad_sync_flag _model is_last_microbatch reshard_after_backward _model set_reshard_after_backward is_last_microbatch mode == some_mlps mlp model + num_mlps_to_disable_reduce_scatter set_grad_sync_flag mlp is_last_microbatch reshard_after_backward mlp set_reshard_after_backward is_last_microbatch mode == root_only set_grad_sync_flag model is_last_microbatch recurse=False reshard_after_backward model set_reshard_after_backward is_last_microbatch recurse=False torch manual_seed + rank + iter_idx range comm_count_list = microbatch_idx range num_microbatches is_last_microbatch = microbatch_idx == num_microbatches - set_backward_flags model is_last_microbatch inp = torch randn batch_size lin_dim device=device_type type losses list torch Tensor = _model ref_model model CommDebugMode comm_mode losses append _model inp sum losses - backward comm_count_list append comm_mode get_comm_counts assertEqual losses losses comm_counts = defaultdict int comm_count_dict comm_count_list collective count comm_count_dict items comm_counts collective += count all_gather_count = comm_counts c d_ops _allgather_base_ reduce_scatter_count = comm_counts c d_ops _reduce_scatter_base_ all_reduce_count = comm_counts c d_ops allreduce_ Expect one reduce-scatter per MLP plus one root s linear last microbatch expected_reduce_scatter_count = expected_all_reduce_count = num_mlps + mode == some_mlps Expect additional reduce-scatters non-disabled MLPs root s linear expected_all_reduce_count += num_mlps - num_mlps_to_disable_reduce_scatter + num_microbatches - mode == root_only Expect additional reduce-scatters all MLPs expected_all_reduce_count += num_mlps num_microbatches - assertEqual reduce_scatter_count expected_reduce_scatter_count assertEqual all_reduce_count expected_all_reduce_count Expect one all-gather per MLP plus one root s linear first microbatch s forward expected_all_gather_count = assertEqual all_gather_count expected_all_gather_count param ref_model parameters param grad None dist all_reduce param grad op=dist ReduceOp AVG check_sharded_parity ref_model model _optim optim ref_optim _optim step When ` set_to_none=False ` we exercising mixing gradient accumulation without communication _optim zero_grad set_to_none= iter_idx skip_if_lt_x_gpu test_ f b_microbatching run_subtests use_explicit_unshard False True reshard_after_backward False True _test_ f b_microbatching _test_ f b_microbatching use_explicit_unshard bool reshard_after_backward bool torch manual_seed model_args = ModelArgs dropout_p= model = Transformer model_args ref_model = copy deepcopy model device_type ref_optim = torch optim AdamW ref_model parameters lr= e- module model modules isinstance module TransformerBlock replicate module reshard_after_forward=False replicate model reshard_after_forward=False optim = torch optim AdamW model parameters lr= e- num_microbatches = local_batch_size = torch manual_seed + rank + inps = torch randint model_args vocab_size local_batch_size device=device_type type _ range num_microbatches Before pipelining we may prefer issue all all-gathers ahead time increase overlap opportunity no difference parameter memory usage since we do reshard after forward use_explicit_unshard module model modules isinstance module FSDPModule module unshard async_op=True Emulate f b pipeline schedule only reduce gradients last microbatch losses list torch Tensor = ref_losses list torch Tensor = inp_idx inp enumerate inps is_last_microbatch = inp_idx == num_microbatches - model set_requires_gradient_sync is_last_microbatch model set_is_last_backward is_last_microbatch reshard_after_backward model set_reshard_after_backward is_last_microbatch losses append model inp sum losses - backward ref_losses append ref_model inp sum ref_losses - backward param ref_model parameters dist all_reduce param grad op=dist ReduceOp AVG loss ref_loss zip losses ref_losses assertEqual loss ref_loss optim step ref_optim step check_sharded_parity ref_model model TestReplicateCustomForwardMethod FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu test_register_fsdp_forward_method VisionTransformer nn Module __init__ - None super __init__ patch_proj = nn Conv d kernel_size= stride= forward_features imgs torch Tensor - torch Tensor patch_proj imgs flatten transpose forward imgs torch Tensor - torch Tensor forward_features imgs sum dim= Model nn Module __init__ - None super __init__ vit projector = VisionTransformer nn Linear forward imgs torch Tensor - torch Tensor Run ` vit forward_features ` which ` forward ` patch_embeddings = vit forward_features imgs projector patch_embeddings torch manual_seed model = Model ref_model = copy deepcopy model device_type replicate model vit replicate model projector replicate model register_fsdp_forward_method model vit forward_features torch manual_seed + rank + inp = torch randn device=device_type type ref_loss = ref_model inp sum loss = model inp sum assertEqual ref_loss loss ref_loss backward loss backward param ref_model parameters dist all_reduce param grad op=dist ReduceOp AVG check_sharded_parity ref_model model TestReplicateTPTraining FSDPTest property world_size - int min torch get_device_module device_type device_count init_global_mesh - DeviceMesh init_device_mesh device_type type mesh_dim_names= dp_replicate dp_shard tp skip_if_lt_x_gpu test_replicate_tp global_mesh = init_global_mesh run_subtests reshard_after_forward False True use_activation_checkpointing False True mlp_dim foreach False functools partial _test_replicate_tp global_mesh _test_replicate_tp global_mesh DeviceMesh reshard_after_forward bool use_activation_checkpointing bool mlp_dim int foreach bool dp_mesh tp_mesh = global_mesh dp_replicate dp_shard global_mesh tp dp_pg = dp_mesh _flatten get_group used ` replicate ` torch manual_seed model = MLPStack mlp_dim ref_model = copy deepcopy model device_type ref_optim = torch optim Adam ref_model parameters lr= e- foreach=foreach parallelize_plan = Pass ` use_local_output=False ` keep DTensor preserve uneven activation dims in_proj ColwiseParallel use_local_output=False out_proj RowwiseParallel use_local_output=False in_proj ColwiseParallel use_local_output=False out_proj RowwiseParallel use_local_output=False in_proj ColwiseParallel use_local_output=False out_proj RowwiseParallel model = parallelize_module model tp_mesh parallelize_plan module model isinstance module nn LayerNorm continue use_activation_checkpointing checkpoint module replicate module device_mesh=dp_mesh replicate model device_mesh=dp_mesh Checking parameters match orig model critical validate full_tensor correctly replicates strided-sharded layers ref_p p zip ref_model parameters model parameters assertIsInstance p DTensor assertEqual ref_p p full_tensor optim = torch optim Adam model parameters lr= e- foreach=foreach torch manual_seed + dp_pg rank + device = device_type iter_idx range inp = torch randn mlp_dim device=device losses list torch Tensor = _model ref_model model losses append _model inp sum losses - backward param ref_model parameters param grad None dist all_reduce param grad op=dist ReduceOp AVG _optim ref_optim optim _optim zero_grad set_to_none= iter_idx == _optim step assertEqual losses losses check_sharded_parity ref_model model _ p model named_parameters assertIsInstance p DTensor assertEqual p device_mesh ndim assertEqual len p placements assertEqual p device_mesh mesh_dim_names dp_replicate dp_shard tp __name__ == __main__ run_tests