Copyright c Meta Platforms Inc affiliates Owner s oncall distributed itertools random unittest typing Any Callable ClassVar Optional torch torch distributed dist torch distributed distributed_c d c d torch nn functional F torch Tensor torch distributed device_mesh init_device_mesh torch distributed tensor DeviceMesh torch distributed tensor debug CommDebugMode torch distributed tensor experimental _attention _CausalBehavior _context_parallel_shard _ContextParallel _cp_options _disable_context_parallel_dispatcher _enable_context_parallel_dispatcher _HeadTailLoadBalancer _is_causal_behavior _LoadBalancer _PerDocumentHeadTailLoadBalancer _PTRRLoadBalancer _RotateMethod context_parallel context_parallel_unshard set_rotate_method torch distributed tensor experimental _context_parallel _cp_custom_ops flex_cp_allgather torch distributed tensor parallel parallelize_module torch nn attention sdpa_kernel SDPBackend torch nn attention flex_attention _mask_mod_signature AuxOutput AuxRequest BlockMask create_block_mask flex_attention torch testing _internal common_cuda PLATFORM_SUPPORTS_CUDNN_ATTENTION PLATFORM_SUPPORTS_FLASH_ATTENTION PLATFORM_SUPPORTS_FUSED_ATTENTION PLATFORM_SUPPORTS_MEM_EFF_ATTENTION torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_utils run_tests skipIfRocm torch testing _internal distributed _tensor common_dtensor create_local_tensor_test_class DTensorTestBase map_local_tensor_for_rank with_comms c d_functional = torch ops c d_functional backends = PLATFORM_SUPPORTS_FLASH_ATTENTION backends append SDPBackend FLASH_ATTENTION PLATFORM_SUPPORTS_MEM_EFF_ATTENTION backends append SDPBackend EFFICIENT_ATTENTION PLATFORM_SUPPORTS_CUDNN_ATTENTION backends append SDPBackend CUDNN_ATTENTION rotater_enum_to_str = _RotateMethod ALL_GATHER allgather _RotateMethod ALL_TO_ALL alltoall mapping _RotateMethod enum string SDPAWrapper torch nn Module __init__ compiled bool backend SDPBackend - None super __init__ compiled sdpa = torch compile F scaled_dot_product_attention fullgraph=True backend= aot_eager sdpa = F scaled_dot_product_attention backend = backend forward args object kwargs object - torch Tensor sdpa_kernel backend sdpa args kwargs RingAttentionTest DTensorTestBase property world_size - int torch cuda device_count property destroy_pg_upon_exit - bool False skip_if_lt_x_gpu skipIfRocm Missing _c d_functional_autograd all_to_all_single unittest skipIf PLATFORM_SUPPORTS_FUSED_ATTENTION Does support flash nor efficient attention with_comms test_ring_attention_sdpa - None run_subtests is_causal True False compiled True False backend backends load_balance True False rotater _RotateMethod ALL_TO_ALL _RotateMethod ALL_GATHER test_forward_only True False use_context True False _test_ring_attention_sdpa _ring_attention_sdpa cp_q torch Tensor cp_k torch Tensor cp_v torch Tensor fn_eval Callable mesh DeviceMesh seq_dim int is_causal bool compiled bool backend SDPBackend rotater _RotateMethod test_forward_only bool load_balance bool use_context bool - tuple torch Tensor torch Tensor torch Tensor torch Tensor use_context cp_plan = _ContextParallel seq_dim=seq_dim attention_type=_ContextParallel AttentionType SDPA attention = SDPAWrapper compiled=compiled backend=backend attention = parallelize_module attention mesh cp_plan load_balance seq_len = cp_q size seq_dim load_balancer = _HeadTailLoadBalancer seq_len mesh size cp_q device load_balancer = None cp_q cp_k cp_v = _context_parallel_shard mesh cp_q cp_k cp_v seq_dim load_balancer=load_balancer _enable_context_parallel_dispatcher Theoretically context_parallel should used shard parameters because when require_grad True resize_ allowed But requires_grad cp_q cp_k cp_v False now So we can just use context_parallel shard q k v In reality context_parallel should used shard input In reality context_parallel should only used shard model inputs batch _cp_options enable_load_balance = load_balance cp_context = context_parallel mesh buffers= cp_q cp_k cp_v buffer_seq_dims= seq_dim cp_context __enter__ NOTE This demonstrates monkey patching fully reliable If we use SDPAWrapper directly monkey patching dispatch mode does function correctly To ensure proper behavior F scaled_dot_product_attention must referenced within context_parallel scope attention = F scaled_dot_product_attention compiled attention = torch compile attention fullgraph=True backend= aot_eager target cp_q cp_k cp_v target requires_grad = True CommDebugMode comm_mode sdpa_kernel backend cp_out = fn_eval attention cp_q cp_k cp_v is_causal=is_causal compiled rotater == _RotateMethod ALL_TO_ALL Compiler CommDebugMode do work well together expect_all all_count = world_size - test_forward_only world_size - assertDictEqual comm_mode get_comm_counts c d_functional all_to_all_single expect_all all_count cp_dq cp_dk cp_dv = cp_q grad cp_k grad cp_v grad target cp_q cp_k cp_v target requires_grad = False use_context _disable_context_parallel_dispatcher cp_context __exit__ None None None cp_out cp_dq cp_dk cp_dv _test_ring_attention_sdpa is_causal bool compiled bool backend SDPBackend load_balance bool rotater _RotateMethod test_forward_only bool use_context bool - None fn_eval fn args kwargs test_forward_only torch no_grad fn args kwargs out = fn args kwargs out sum backward out load_balance is_causal set_rotate_method rotater_enum_to_str rotater assertEqual _cp_options rotate_method rotater device_mesh = DeviceMesh device_type torch arange world_size dtype = torch bfloat bs = seq_length = seq_dim = dim = nheads = torch manual_seed dtype = torch bfloat backend == SDPBackend FLASH_ATTENTION backend == SDPBackend CUDNN_ATTENTION torch float q k v = torch rand bs nheads seq_length world_size dim device=self device_type dtype=dtype requires_grad=True _ range Ensure all ranks have same initialization data torch no_grad dist broadcast q src= dist broadcast k src= dist broadcast v src= sdpa_kernel backend out = fn_eval F scaled_dot_product_attention q k v is_causal=is_causal cp_q cp_k cp_v = target detach clone target q k v cp_out cp_dq cp_dk cp_dv = _ring_attention_sdpa cp_q cp_k cp_v fn_eval=fn_eval mesh=device_mesh seq_dim=seq_dim is_causal=is_causal compiled=compiled backend=backend rotater=rotater test_forward_only=test_forward_only load_balance=load_balance use_context=use_context Due numerical error we need choose different atol different attention kernels cp_out = context_parallel_unshard device_mesh cp_out seq_dim atol = e- backend == SDPBackend EFFICIENT_ATTENTION e- world_size rtol = e- backend == SDPBackend EFFICIENT_ATTENTION e- world_size torch testing assert_close out cp_out atol=atol rtol=rtol test_forward_only cp_dq cp_dk cp_dv = context_parallel_unshard device_mesh cp_dq cp_dk cp_dv seq_dim torch testing assert_close q grad cp_dq atol=atol rtol=rtol torch testing assert_close k grad cp_dk atol=atol rtol=rtol torch testing assert_close v grad cp_dv atol=atol rtol=rtol test_is_causal_behavior - None _cp_options enable_load_balance = False assertEqual _is_causal_behavior rank= world_size= i= is_causal=False _CausalBehavior NOT_IS_CAUSAL ranks = _CausalBehavior IS_CAUSAL _CausalBehavior SKIP _CausalBehavior IS_CAUSAL _CausalBehavior NOT_IS_CAUSAL rank iters enumerate ranks i behavior enumerate iters assertEqual _is_causal_behavior rank=rank world_size= i=i is_causal=True behavior _cp_options enable_load_balance = True ranks = _CausalBehavior IS_CAUSAL _CausalBehavior NOT_IS_CAUSAL _CausalBehavior IS_CAUSAL _CausalBehavior NOT_IS_CAUSAL rank iters enumerate ranks i behavior enumerate iters assertEqual _is_causal_behavior rank=rank world_size= i=i is_causal=True behavior Compile flex_attention function compiled_flex_attention = torch compile flex_attention dynamic=False fullgraph=True compiled_create_block_mask = torch compile create_block_mask dynamic=False fullgraph=True causal_mask b h q_idx kv_idx q_idx = kv_idx copied https github com meta-pytorch attention-gym blob main attn_gym masks document_mask py generate_random_lengths total_length num_documents - list int Initialize all lengths ensure each document has least one token lengths = num_documents remaining_length = total_length - num_documents Randomly distribute remaining length _ range remaining_length index = random randint num_documents - lengths index += lengths generate_random_lengths_in_chunks total_length num_documents chunk_size - list int Generate list random document lengths so each document contains some number chunks size ` chunk_size ` This means each document s length must multiple ` chunk_size ` Besides lengths all documents sum up ` total_length ` num_chunks = total_length chunk_size assert total_length chunk_size == num_chunks = num_documents num_chunks_per_document = num_documents remaining_chunks = num_chunks - num_documents Randomly distribute remaining chunks _ range remaining_chunks index = random randint num_documents - document_id num_chunks_per_document index += num_chunks chunk_size num_chunks num_chunks_per_document length_to_offsets lengths list list int device str &#124; torch device - Tensor Converts list lengths list offsets Args lengths A list lengths offsets = + lengths_in_batch lengths_in_batch lengths offsets = torch tensor offsets device=device dtype=torch int offsets = torch cumsum offsets dim=- offsets _offsets_to_doc_ids_tensor offsets doc_ids = device = offsets device batch_idx range offsets size counts = offsets batch_idx - offsets batch_idx - doc_id = torch repeat_interleave torch arange len counts device=device dtype=torch int counts doc_ids append doc_id torch stack doc_ids generate_doc_mask_mod mask_mod _mask_mod_signature offsets Tensor - _mask_mod_signature Generates mask mods apply inputs flex attention sequence stacked format Args mask_mod The mask mod apply documents offsets This tensor should shape num_documents + should contain cumulative counts document tokens e g you have documents length then offsets = Note What sequence stacked format When assembling batches inputs we take multiple sequences stack them together form large sequence We then use masking ensure attention scores only applied tokens within same document document_id = _offsets_to_doc_ids_tensor offsets doc_mask_mod b h q_idx kv_idx same_doc = document_id b q_idx == document_id b kv_idx q_logical = q_idx - offsets b document_id b q_idx kv_logical = kv_idx - offsets b document_id b kv_idx inner_mask = mask_mod b h q_logical kv_logical same_doc inner_mask doc_mask_mod FlexAttentionWrapper torch nn Module _flex_attn ClassVar Callable = torch compile flex_attention __init__ - None super __init__ forward args object kwargs object - torch Tensor &#124; tuple torch Tensor torch Tensor tuple torch Tensor AuxOutput FlexAttentionWrapper _flex_attn args kwargs CPFlexAttentionTest DTensorTestBase property world_size - int _test_cp_flex_attention qkv_size int B int = block_mask lb_type str document_lengths Optional list list int = None - None torch use_deterministic_algorithms True torch cuda manual_seed dtype = torch float bs = B B dim = nheads = seq_dim = lb = _get_load_balancer lb_type seq_length qkv_size document_lengths document_lengths block_mask block_mask qkv = torch rand bs nheads qkv_size dim device=self device_type dtype=dtype requires_grad=True _ range expect_out expect_aux = compiled_flex_attention qkv block_mask=block_mask return_aux=AuxRequest lse=True expect_out sum backward Prepare required global vars CP+Flex device_mesh = init_device_mesh device_type=self device_type mesh_shape= world_size mesh_dim_names= cp flex_attention_wrapper_module = FlexAttentionWrapper cp_plan = _ContextParallel seq_dim=seq_dim attention_type=_ContextParallel AttentionType FLEX parallelize_module flex_attention_wrapper_module device_mesh cp_plan cp_qkv cp_block_mask = _context_parallel_shard device_mesh t detach clone t qkv + block_mask seq_dim load_balancer=lb t cp_qkv t requires_grad = True cp_out cp_aux = flex_attention_wrapper_module cp_qkv block_mask=cp_block_mask return_aux=AuxRequest lse=True backward run cp_out sum backward atol = e- rtol = e- unshard output cp_out cp_lse = context_parallel_unshard device_mesh buffers= cp_out cp_aux lse seq_dims= seq_dim load_balancer=lb torch testing assert_close cp_out expect_out atol=atol rtol=rtol torch testing assert_close cp_lse expect_aux lse atol=atol rtol=rtol unshard gradient cp_qkv_grad = context_parallel_unshard device_mesh buffers= t grad t cp_qkv seq_dims= seq_dim load_balancer=lb qkv_grad = t grad t qkv grad cp_grad zip qkv_grad cp_qkv_grad torch testing assert_close grad cp_grad atol=atol rtol=rtol _get_load_balancer lb_type str kwargs dict str Any - Optional _LoadBalancer seq_length = kwargs seq_length document_lengths = kwargs document_lengths block_mask = kwargs block_mask generate load balancer lb_type == None load_balancer = None no load-balance lb_type == _HeadTailLoadBalancer assert isinstance seq_length int load_balancer = _HeadTailLoadBalancer seq_length world_size torch device device_type lb_type == _PerDocumentHeadTailLoadBalancer assert isinstance document_lengths list load_balancer = _PerDocumentHeadTailLoadBalancer document_lengths world_size torch device device_type lb_type == _PTRRLoadBalancer assert isinstance block_mask BlockMask load_balancer = _PTRRLoadBalancer block_mask world_size raise ValueError f load_balancer type lb_type supported load_balancer skip_if_lt_x_gpu with_comms unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Does support flash attention test_cp_flex_attention_causal_mask - None seq_length_list = world_size load_balance_type_list = None _HeadTailLoadBalancer _PTRRLoadBalancer NOTE Each seq_len load_balance_type tuple introduces create_block_mask compilations single-rank flex_attention CP flex_attention In order avoid exceeds_recompile_limit error we need increase cache_size_limit num_of_sub_test_runs which will total number compilations our test case torch _dynamo config cache_size_limit = len seq_length_list + + len load_balance_type_list qkv_size lb_type itertools product seq_length_list load_balance_type_list block_mask = compiled_create_block_mask causal_mask B= H= Q_LEN=qkv_size KV_LEN=qkv_size device=self device_type _test_cp_flex_attention qkv_size=qkv_size block_mask=block_mask lb_type=lb_type NOTE Context Parallel should used small attentions block_size qkv_size = world_size block_mask = compiled_create_block_mask causal_mask B= H= Q_LEN=qkv_size KV_LEN=qkv_size device=self device_type lb_type None _HeadTailLoadBalancer assertRaisesRegex NotImplementedError f Q_LEN qkv_size divisible _test_cp_flex_attention qkv_size=qkv_size block_mask=block_mask lb_type=lb_type lb_type _PTRRLoadBalancer assertRaisesRegex NotImplementedError must divisible group_size _test_cp_flex_attention qkv_size=qkv_size block_mask=block_mask lb_type=lb_type TODO merge above test skip_if_lt_x_gpu with_comms unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Does support flash attention test_cp_flex_attention_document_mask - None random seed parameters testing doc_count = batch_size_list = max_seq_len_list = world_size world_size NOTE Mismatched elements load_balance_type = None _HeadTailLoadBalancer _PerDocumentHeadTailLoadBalancer _PTRRLoadBalancer NOTE Each batch_size seq_len load_balance_type tuple introduces create_block_mask compilations single-rank flex_attention CP flex_attention In order avoid exceeds_recompile_limit error we need increase cache_size_limit num_of_sub_test_runs which will total number compilations our test case torch _dynamo config cache_size_limit = len batch_size_list len max_seq_len_list len load_balance_type TODO change for-loop run_subtests Use for-loop instead run_subtests because we need initialize mask each subtest This can baked into _test_cp_flex_attention str argument denoting mask type batch_size max_seq_len lb_type itertools product batch_size_list max_seq_len_list load_balance_type initialize document mask lengths = generate_random_lengths_in_chunks max_seq_len doc_count chunk_size= world_size lb_type == _PerDocumentHeadTailLoadBalancer generate_random_lengths max_seq_len doc_count _ range batch_size offsets = length_to_offsets lengths device_type document_causal_mask = generate_doc_mask_mod causal_mask offsets block_mask = compiled_create_block_mask document_causal_mask B=batch_size H= Q_LEN=max_seq_len KV_LEN=max_seq_len device=self device_type _test_cp_flex_attention qkv_size=max_seq_len B=batch_size lb_type=lb_type block_mask=block_mask document_lengths=lengths TestCPCustomOps DTensorTestBase property world_size - int skip_if_lt_x_gpu with_comms test_flex_cp_custom_op - None mesh = init_device_mesh device_type=self device_type mesh_shape= world_size mesh_dim_names= cp examples_k_v = torch randn device=self device_type torch randn device=self device_type c d _get_process_group_name mesh get_group torch randn device=self device_type requires_grad=True torch randn device=self device_type requires_grad=True c d _get_process_group_name mesh get_group example examples_k_v torch library opcheck flex_cp_allgather example TestSharding DTensorTestBase property world_size - int skip_if_lt_x_gpu with_comms test_context_parallel_shard - None B = seq_len = device_mesh = init_device_mesh mesh_shape= mesh_dim_names= cp device_type=self device_type freqs_cis = torch arange seq_len device=self device_type q = torch ones B seq_len device=self device_type reshape B seq_len k = torch ones B seq_len device=self device_type reshape B seq_len v = torch ones B seq_len device=self device_type reshape B seq_len load_balancer = _HeadTailLoadBalancer seq_len world_size torch device device_type freqs_cis_shard q_shard k_shard v_shard = _context_parallel_shard device_mesh freqs_cis q k v load_balancer=load_balancer assertEqual freqs_cis_shard size seq_len chunks = freqs_cis chunk world_size assertEqual freqs_cis_shard map_local_tensor_for_rank chunks rank lambda chunks rank torch cat chunks rank chunks world_size - rank - dim= RingAttentionTestWithLocalTensor = create_local_tensor_test_class RingAttentionTest skipped_tests= Need make attention implementation local tensor friendly e g rewrite rank local logic test_ring_attention_sdpa CPFlexAttentionTestWithLocalTensor = create_local_tensor_test_class CPFlexAttentionTest skipped_tests= Missing support batched tensors test_cp_flex_attention_causal_mask test_cp_flex_attention_document_mask TestCPCustomOpsWithLocalTensor = create_local_tensor_test_class TestCPCustomOps skipped_tests= Missing support fake tensors test_flex_cp_custom_op TestShardingWithLocalTensor = create_local_tensor_test_class TestSharding __name__ == __main__ run_tests