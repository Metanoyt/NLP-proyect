mypy ignore-errors math copy copy dataclasses dataclass functools partial typing Optional torch torch fx experimental symbolic_shapes is_nested_int torch testing _internal common_methods_invocations op_db torch testing _internal opinfo core BinaryUfuncInfo ReductionOpInfo SampleInput UnaryUfuncInfo torch utils _pytree tree_flatten tree_map dataclass ExtraOpData Contains info top typical OpInfo data useful NJT test generation The process converts standard op_db - NJT-compatible op_db will attach data onto each associated OpInfo entry Indicates whether associated op view op is_view bool = False Specifies names any dim-related args op takes This useful NJT tests because there often asymmetry across supported set dims op may make sense operate over batch dim ragged dim example The length list should match number relevant overloads Each list item outer list should specify dim argnames Ellipses should used indicate multi-dim support given overload For example squeeze has both dim multi-dim overload where argname each simply dim Its entry should dim dim If no overload op accepts dim-related args should None dim_args list list str = None Helper function extract names dim-related args Returns tuple single dim argname available dim list argname available If op doesn t support dim-related args all OR op only has overloads multiple dim args e g transpose then returns None None get_dim_argnames - tuple Optional str Optional str dim_args None None None name dim arg supports single dim single_dim_argname = None name dim arg supports list dims dimlist_argname = None overload dim_args only consider overloads single dim-related arg len overload = continue overload endswith dimlist_argname = overload replace single_dim_argname None single_dim_argname = dimlist_argname single_dim_argname = overload single_dim_argname dimlist_argname Mapping OpInfo full names - extra data tack onto OpInfo entry use test generation extra_op_data = _segment_reduce lengths ExtraOpData dim_args= axis _segment_reduce offsets ExtraOpData dim_args= axis all ExtraOpData dim_args= dim dim argmax ExtraOpData dim_args= dim argmin ExtraOpData dim_args= dim amax ExtraOpData dim_args= dim amin ExtraOpData dim_args= dim any ExtraOpData dim_args= dim dim argsort ExtraOpData dim_args= dim broadcast_to ExtraOpData is_view=True cat ExtraOpData dim_args= dim chunk ExtraOpData is_view=True dim_args= dim conj ExtraOpData is_view=True contiguous ExtraOpData is_view=True count_nonzero ExtraOpData dim_args= dim dim cummax ExtraOpData dim_args= dim cummin ExtraOpData dim_args= dim cumprod ExtraOpData dim_args= dim cumsum ExtraOpData dim_args= dim cumulative_trapezoid ExtraOpData dim_args= dim diag_embed ExtraOpData dim_args= dim dim diagonal ExtraOpData is_view=True dim_args= dim dim diagonal_copy ExtraOpData dim_args= dim dim diagonal_scatter ExtraOpData dim_args= dim dim diff ExtraOpData dim_args= dim expand ExtraOpData is_view=True expand_as ExtraOpData is_view=True fft fft ExtraOpData dim_args= dim fft hfft ExtraOpData dim_args= dim fft ifft ExtraOpData dim_args= dim fft ihfft ExtraOpData dim_args= dim fft irfft ExtraOpData dim_args= dim fft rfft ExtraOpData dim_args= dim flatten ExtraOpData is_view=True dim_args= start_dim end_dim flip ExtraOpData dim_args= dims gather ExtraOpData dim_args= dim hash_tensor ExtraOpData dim_args= dim imag ExtraOpData is_view=True index_add ExtraOpData dim_args= dim index_copy ExtraOpData dim_args= dim index_fill ExtraOpData dim_args= dim index_reduce amax ExtraOpData dim_args= dim index_reduce amin ExtraOpData dim_args= dim index_reduce mean ExtraOpData dim_args= dim index_reduce prod ExtraOpData dim_args= dim index_select ExtraOpData dim_args= dim kthvalue ExtraOpData dim_args= dim linalg cross ExtraOpData dim_args= dim linalg diagonal ExtraOpData is_view=True dim_args= dim dim linalg tensorsolve ExtraOpData dim_args= dims linalg vecdot ExtraOpData dim_args= dim linalg vector_norm ExtraOpData dim_args= dim log_softmax ExtraOpData dim_args= dim logcumsumexp ExtraOpData dim_args= dim masked amax ExtraOpData dim_args= dim masked amin ExtraOpData dim_args= dim masked argmax ExtraOpData dim_args= dim masked argmin ExtraOpData dim_args= dim masked logsumexp ExtraOpData dim_args= dim masked mean ExtraOpData dim_args= dim masked norm ExtraOpData dim_args= dim masked prod ExtraOpData dim_args= dim masked std ExtraOpData dim_args= dim masked sum ExtraOpData dim_args= dim masked var ExtraOpData dim_args= dim max reduction_with_dim ExtraOpData dim_args= dim median ExtraOpData dim_args= dim mean ExtraOpData dim_args= dim min reduction_with_dim ExtraOpData dim_args= dim mode ExtraOpData dim_args= dim movedim ExtraOpData dim_args= source destination source destination nanmean ExtraOpData dim_args= dim nanmedian ExtraOpData dim_args= dim nansum ExtraOpData dim_args= dim narrow ExtraOpData is_view=True dim_args= dim narrow_copy ExtraOpData dim_args= dim nn functional cosine_similarity ExtraOpData dim_args= dim nn functional glu ExtraOpData dim_args= dim permute ExtraOpData is_view=True dim_args= dims positive ExtraOpData is_view=True prod ExtraOpData dim_args= dim ravel ExtraOpData is_view=True real ExtraOpData is_view=True renorm ExtraOpData dim_args= dim reshape ExtraOpData is_view=True reshape_as ExtraOpData is_view=True roll ExtraOpData dim_args= dims rot ExtraOpData dim_args= dims scatter ExtraOpData dim_args= dim scatter_add ExtraOpData dim_args= dim scatter_reduce amax ExtraOpData dim_args= dim scatter_reduce amin ExtraOpData dim_args= dim scatter_reduce mean ExtraOpData dim_args= dim scatter_reduce prod ExtraOpData dim_args= dim scatter_reduce sum ExtraOpData dim_args= dim select ExtraOpData is_view=True dim_args= dim select_scatter ExtraOpData dim_args= dim slice ExtraOpData is_view=True dim_args= dim slice_scatter ExtraOpData dim_args= dim softmax ExtraOpData dim_args= dim sort ExtraOpData dim_args= dim split ExtraOpData is_view=True dim_args= dim split_with_sizes ExtraOpData is_view=True dim_args= dim split_with_sizes_copy ExtraOpData dim_args= dim squeeze ExtraOpData is_view=True dim_args= dim dim squeeze_copy ExtraOpData dim_args= dim dim stack ExtraOpData dim_args= dim std ExtraOpData dim_args= dim std unbiased ExtraOpData dim_args= dim sum ExtraOpData dim_args= dim t ExtraOpData is_view=True tensor_split ExtraOpData is_view=True dim_args= dim tensordot ExtraOpData dim_args= dims tile ExtraOpData dim_args= dims topk ExtraOpData dim_args= dim transpose ExtraOpData is_view=True dim_args= dim dim transpose_copy ExtraOpData dim_args= dim dim trapezoid ExtraOpData dim_args= dim trapz ExtraOpData dim_args= dim unbind ExtraOpData is_view=True dim_args= dim unflatten ExtraOpData is_view=True dim_args= dim unfold ExtraOpData is_view=True dim_args= dimension unfold_copy ExtraOpData dim_args= dimension unsafe_chunk ExtraOpData dim_args= dim unsafe_split ExtraOpData dim_args= dim unsqueeze ExtraOpData is_view=True dim_args= dim unsqueeze_copy ExtraOpData dim_args= dim var ExtraOpData dim_args= dim var unbiased ExtraOpData dim_args= dim view ExtraOpData is_view=True view_as ExtraOpData is_view=True view_as_complex ExtraOpData is_view=True view_as_real ExtraOpData is_view=True random integer used sizes _rnd torch randint item _raggedness_matches nt nt nt is_nested nt is_nested nt _ragged_idx == nt _ragged_idx nt shape nt _ragged_idx == nt shape nt _ragged_idx Helper function avoid reusing exact same tensor NJT across SampleInputs causes autograd problems _clone t requires_grad = t requires_grad t detach clone requires_grad_ requires_grad Helper function update sample new kwargs name _update_sample sample new_kwargs all_kwargs = dict sample kwargs all_kwargs update new_kwargs full_name = join sample name f k = v k v new_kwargs items SampleInput _clone sample input args=sample args kwargs=all_kwargs name=full_name Generates random NT dims should something like None None indicating random ragged structure should used random_nt_from_dims dims device=None dtype=None layout=torch strided requires_grad=False sizes = d d None _rnd d dims d range dims torch nested nested_tensor torch randn size size sizes device=device dtype=dtype layout=layout requires_grad=requires_grad Helper function get reasonable string representation NJT use SampleInput names _describe_njt njt - str contig_type = _contig njt is_contiguous _noncontig njt _lengths None njt _offsets None contig_type += _holes njt _ragged_idx = contig_type += _transposed cached_data = _without_seqlen_cache njt _max_seqlen_tensor None cached_data = _with_seqlen_cache f njt dim D contig_type cached_data Helper function get reasonable string representation given dim wrt NJT _describe_dim njt dim dim == batch_dim dim == njt _ragged_idx ragged_dim normal_dim Helper function generating comprehensive set NJT sample inputs _sample_njts device dtype requires_grad=False dims=None dims None dims = isinstance dims list tuple dims = dims contiguous NJTs dim dims min max seqlen cached shape = _rnd None _rnd _ range dim - nt = random_nt_from_dims shape device=device dtype=dtype requires_grad=requires_grad layout=torch jagged yield nt without min max seqlen cached values = _clone nt values offsets = _clone nt offsets yield torch nested nested_tensor_from_jagged values offsets requires_grad_ requires_grad non-contiguous transposed NJT possible D dim yield nt transpose - nt _ragged_idx non-contiguous holes NJT values = _clone nt values offsets = _clone nt offsets subtract cause holes lengths = _clone offsets diff - yield torch nested nested_tensor_from_jagged values=values offsets=offsets lengths=lengths requires_grad_ requires_grad Computes unbind-based reference given OpInfo given SampleInput This reference unbinds input NJT invokes op each components optionally wrapping result NJT unbind_reference op sample wrap_output_as_njt=True first NJT arglist determines expected ragged structure nt_inp = sample input sample input is_nested TODO look kwargs too next sample args is_nested out_ref_components = i range nt_inp shape _slice_input t i=i inp=nt_inp any NJT same ragged structure input should sliced pass reference isinstance t torch Tensor _raggedness_matches t inp t i allow SampleInput tell us how slice ref calculation isinstance t torch Tensor hasattr t _batch_dim bdim = t _batch_dim type ignore attr t shape bdim == t t select bdim i t inp = _slice_input sample input args = tree_map _slice_input sample args kwargs = tree_map _slice_input sample kwargs Handle indices index_put index_put op full_name indices kwargs len kwargs indices If after unrolling we still have indices left use them kwargs indices = t i t kwargs indices If no indices left create them so they match NJT implementation sequence_put = kwargs indices tolist i sequence_put kwargs indices = torch tensor list range inp shape dtype=torch int device=kwargs indices device kwargs indices = torch tensor dtype=torch int device=kwargs indices device torch nested _internal ops _outer_to_inner_dim Need adjust dims apply NJT component op _extra_op_data dim_args None get all possible dim-related argnames could encountered op argnames = tree_map lambda replace tree_flatten op _extra_op_data dim_args all dim-related args present convert outer - inner dim space argname argnames kwargs allow SampleInput tell us how canonicalize dim kwargs ndim = nt_inp _ndim hasattr nt_inp _ndim nt_inp dim kwargs argname = _outer_to_inner_dim ndim kwargs argname nt_inp _ragged_idx canonicalize=True out_ref_component = op op inp args kwargs out_ref_components append out_ref_component wrap_output_as_njt handle list tuple outputs len out_ref_components isinstance out_ref_components list tuple num_returns = len out_ref_components ensure we get same number returns each invocation assert all len o == num_returns o out_ref_components construct NJTs same index returns each invocation njt_returns = torch nested as_nested_tensor o r o out_ref_components layout=torch jagged r range num_returns type out_ref_components njt_returns torch nested as_nested_tensor out_ref_components layout=torch jagged out_ref_components Computes reference value non-reduction unary op dim-wise application unary_dimwise_reference op sample batchwise_reference=None extract info about dim args op supports assert op _extra_op_data dim_args None single_dim_argname dimlist_argname = op _extra_op_data get_dim_argnames only support single non-list dim arg now assert dimlist_argname None assert single_dim_argname None sample kwargs single_dim_argname == unbind reference won t work batch-wise operation handle case here assert batchwise_reference None batchwise_reference op sample unbind_reference op sample Computes reference value reduction op reduction_reference op sample assert sample input is_nested extract info about dim args op supports assert op _extra_op_data dim_args None single_dim_argname dimlist_argname = op _extra_op_data get_dim_argnames assert single_dim_argname None dim = sample kwargs get dimlist_argname sample kwargs get single_dim_argname None keepdim = sample kwargs get keepdim False assert dim = reductions over just batch dim supported isinstance dim tuple list reduce_on_ragged = sample input _ragged_idx dim reduce_on_batch = dim reduce_on_ragged = sample input _ragged_idx == dim reduce_on_batch = dim == dim None calculate reference value running reduction values buffer op op sample input values sample args sample kwargs reduce_on_ragged reduce_on_batch run reference directly buffer dims converted inner space torch nested _internal ops _outer_to_inner_dim ref_kwargs = dict sample kwargs assert dimlist_argname None ref_kwargs dimlist_argname = _outer_to_inner_dim sample input dim dim sample input _ragged_idx canonicalize=True out = op op sample input values sample args ref_kwargs keepdim isinstance out tuple list some ops multiple things unsqueeze all them out = type out o unsqueeze o out out = out unsqueeze out reduce_on_ragged reduce_on_batch calculate reference value running unbind reference stacking out_ref_components = unbind_reference op sample wrap_output_as_njt=False len out_ref_components isinstance out_ref_components tuple list some ops multiple things stack all them num_returns = len out_ref_components ensure we get same number returns each invocation assert all len o == num_returns o out_ref_components stack same index returns each invocation stacked_returns = torch stack o r o out_ref_components dim= r range num_returns type out_ref_components stacked_returns torch stack out_ref_components dim= unbind reference works other reductions unbind_reference op sample sample_inputs_elementwise_njt_unary op_info device dtype requires_grad op_kwargs=None kwargs op_kwargs op_kwargs = njt _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= yield SampleInput njt kwargs=dict op_kwargs name=_describe_njt njt sample_inputs_elementwise_njt_binary op_info device dtype requires_grad op_kwargs=None kwargs op_kwargs op_kwargs = njt _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= njt_desc = _describe_njt njt njt = torch randn_like njt yield SampleInput _clone njt args= njt kwargs=dict op_kwargs name=f njt_desc NT NT broadcasting case B j B dense_shape = list njt shape dense_shape njt _ragged_idx = t = torch randn dense_shape device=device dtype=dtype requires_grad=requires_grad t = _clone t used slicing unbind_reference t _batch_dim = t _batch_dim = NT T yield SampleInput _clone njt args= t kwargs=dict op_kwargs name=f njt_desc NT T broadcasting over ragged T NT yield SampleInput t args= _clone njt kwargs=dict op_kwargs name=f njt_desc T NT broadcasting over ragged broadcasting case B j t = torch randn _ range njt dim device=device dtype=dtype requires_grad=requires_grad t = _clone t used slicing unbind_reference t _batch_dim = t _batch_dim = NT T yield SampleInput _clone njt args= t kwargs=dict op_kwargs name=f njt_desc NT T broadcasting all s T NT yield SampleInput t args= _clone njt kwargs=dict op_kwargs name=f njt_desc T NT broadcasting all s broadcasting case B j njt dim njt _ragged_idx + t = torch randn njt shape njt _ragged_idx + device=device dtype=dtype requires_grad=requires_grad NT T yield SampleInput _clone njt args= _clone t kwargs=dict op_kwargs name=f njt_desc NT T broadcasting normal dims T NT yield SampleInput _clone t args= _clone njt kwargs=dict op_kwargs name=f njt_desc T NT broadcasting normal dims broadcasting case B j scalar t = torch randn device=device dtype=dtype requires_grad=requires_grad NT T yield SampleInput _clone njt args= _clone t kwargs=dict op_kwargs name=f njt_desc NT T broadcasting scalar T NT yield SampleInput _clone t args= _clone njt kwargs=dict op_kwargs name=f njt_desc T NT broadcasting scalar mixed broadcasting case B j B D B = D = njt = random_nt_from_dims B None device=device dtype=dtype requires_grad=requires_grad layout=torch jagged njt_desc = _describe_njt njt t = torch randn B D device=device dtype=dtype requires_grad=requires_grad t = _clone t used slicing unbind_reference t _batch_dim = t _batch_dim = NT T yield SampleInput _clone njt args= t kwargs=dict op_kwargs name=f njt_desc NT T mixed broadcasting T NT yield SampleInput t args= _clone njt kwargs=dict op_kwargs name=f njt_desc T NT mixed broadcasting sample_inputs_njt_reduction op_info device dtype requires_grad supports_keepdim=True op_kwargs=None kwargs op_kwargs op_kwargs = extract info about dim args op supports assert op_info _extra_op_data dim_args None single_dim_argname dimlist_argname = op_info _extra_op_data get_dim_argnames assert single_dim_argname None supports_dimlist = dimlist_argname None njt _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= njt_desc = _describe_njt njt keepdim_values = False True supports_keepdim None keepdim keepdim_values keepdim_suffix = f keepdim= keepdim supports_keepdim single dim-wise reduction includes reduction over ragged dim NB reduction over batch dim supported TODO Cover set error inputs dim range njt dim dim_desc = normal dim = njt _ragged_idx ragged yield SampleInput _clone njt kwargs= op_kwargs single_dim_argname dim keepdim keepdim supports_keepdim name=f njt_desc dim_desc dim reduction keepdim_suffix supports_dimlist reduce both batch ragged dims yield SampleInput _clone njt kwargs= op_kwargs dimlist_argname njt _ragged_idx keepdim keepdim supports_keepdim name=f njt_desc batch+ragged reduction keepdim_suffix reduce batch ragged other dims other_dim range njt _ragged_idx + njt dim yield SampleInput _clone njt kwargs= op_kwargs dimlist_argname njt _ragged_idx other_dim keepdim keepdim supports_keepdim name= f njt_desc batch+ragged+dim= other_dim f reduction keepdim_suffix reduce two non-ragged non-batch dims njt dim njt _ragged_idx == yield SampleInput _clone njt kwargs= op_kwargs dimlist_argname njt dim - njt dim - keepdim keepdim supports_keepdim name=f njt_desc two normal dim reduction keepdim_suffix full reduction specifying all dims yield SampleInput _clone njt kwargs= op_kwargs dimlist_argname list range njt dim keepdim keepdim supports_keepdim name=f njt_desc all dim reduction keepdim_suffix TODO Reducing ragged dim non-batch dim supported cover set error inputs full reduction yield SampleInput _clone njt kwargs=dict op_kwargs name=f njt_desc full reduction keepdim= keepdim unsupported_sample_inputs_func op_name _f op_info device dtype requires_grad op_name=op_name kwargs raise RuntimeError f OpInfo op_name does support NJT Support can added modifying torch testing _internal opinfo definitions nested py _f unsupported_reference op_name _f op sample raise RuntimeError f OpInfo op_name does define ref function Support can added modifying torch testing _internal opinfo definitions nested py _f === BEGIN OP-SPECIFIC SAMPLE INPUTS FUNCS REFERENCES === sample_inputs_unary_dimwise op_info device dtype requires_grad op_kwargs=None kwargs op_kwargs None op_kwargs = only support single non-list dim arg now assert op_info _extra_op_data None single_dim_argname dimlist_argname = op_info _extra_op_data get_dim_argnames assert single_dim_argname None assert dimlist_argname None njt _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= dim range njt dim kwargs = single_dim_argname dim kwargs update op_kwargs yield SampleInput _clone njt kwargs=kwargs name=f _describe_njt njt _describe_dim njt dim batchwise_reference_chunk op sample reference chunk over dim= B = sample input size num_chunks = sample kwargs chunks chunk_size = math ceil B num_chunks num_full_chunks = B chunk_size chunk_sizes = chunk_size _ range num_full_chunks B chunk_size = final chunk contains leftovers chunk_sizes append B chunk_size split unbound components into chunks according calculated sizes components = list sample input unbind start = chunks = chunk_size chunk_sizes chunks append components start start + chunk_size start += chunk_size rejoin into NJT outputs torch nested as_nested_tensor lst layout=torch jagged lst chunks batchwise_reference_narrow op sample TODO write raise NotImplementedError batchwise_reference_select op sample reference select over dim= sample input unbind sample kwargs index batchwise_reference_split op sample TODO write raise NotImplementedError batchwise_reference_split_with_sizes op sample TODO write raise NotImplementedError batchwise_reference_unflatten op sample TODO write raise NotImplementedError batchwise_reference_unsqueeze op sample raise ValueError unsqueeze intended operate batch dim sample_inputs_clone op_info device dtype requires_grad kwargs non-contiguous NJTs njt _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= yield SampleInput njt name=_describe_njt njt memory_format torch contiguous_format torch preserve_format construct non-contiguous holes NJT values = torch randn device=device dtype=dtype requires_grad=requires_grad offsets = torch tensor device=device dtype=torch int lengths = torch tensor device=device dtype=torch int njt = torch nested nested_tensor_from_jagged values offsets=offsets lengths=lengths njt_desc = _describe_njt njt yield SampleInput njt kwargs= memory_format memory_format name=f njt_desc memory_format sample_inputs_fill op_info device dtype requires_grad kwargs scalar case unary_func = partial sample_inputs_elementwise_njt_unary op_kwargs= value yield unary_func op_info device dtype requires_grad TODO add Tensor case sample_inputs_mvl_gamma p partial sample_inputs_elementwise_njt_unary op_kwargs= p p sample_inputs_polygamma_n n partial sample_inputs_elementwise_njt_unary op_kwargs= n n sample_inputs_special_polygamma_n n partial sample_inputs_elementwise_njt_unary op_kwargs= n n sample_inputs_to op_info device dtype requires_grad op_kwargs=None kwargs njt _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= other_dtypes = d d torch float torch half torch double d dtype other_dtype other_dtypes sample_name = f njt dim D dtype - other_dtype yield SampleInput _clone njt kwargs= dtype dtype name=sample_name only include device transfer CUDA inputs cuda device other_device = cpu sample_name = f _describe_njt njt device - other_device yield SampleInput _clone njt kwargs= device other_device name=sample_name sample_inputs_bmm op_info device dtype requires_grad op_kwargs=None kwargs njt_ d _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= B j D x B D E = B j E njt_ d _ragged_idx == B D = njt_ d shape njt_ d shape - E = D + other = torch randn B D E device=device dtype=dtype used slicing unbind_reference other _batch_dim = njt_desc = _describe_njt njt_ d yield SampleInput _clone njt_ d kwargs= mat other name=f njt_desc B j D x B D E TODO need factory functions B D j x B j E = B D E reference_bmm op sample unbind reduces dim bmm requires D so use matmul reference matmul_op = copy op matmul_op op = torch matmul change arg name mat - other modified_sample = copy sample other = modified_sample kwargs mat del modified_sample kwargs mat modified_sample kwargs other = other unbind_reference matmul_op modified_sample sample_inputs_chunk op_info device dtype requires_grad kwargs sample_input sample_inputs_unary_dimwise op_info device dtype requires_grad kwargs ragged dim chunking test single chunks value sample_input kwargs dim == sample_input input _ragged_idx yield _update_sample sample_input chunks other dim chunking test different chunks values D = sample_input input size sample_input kwargs dim chunks D D - D yield _update_sample sample_input chunks chunks sample_inputs_matmul op_info device dtype requires_grad op_kwargs=None kwargs also run bmm samples through sample_input sample_inputs_bmm op_info device dtype requires_grad change arg name mat - other other = sample_input kwargs mat del sample_input kwargs mat sample_input kwargs other = other yield sample_input D cases covered bmm njt_ d _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= B j D x D E = B j E njt_ d _ragged_idx == D = njt_ d shape - E = D + njt_desc = _describe_njt njt_ d yield SampleInput _clone njt_ d kwargs= other torch randn D E device=device dtype=dtype name=f njt_desc B j D x D E D cases njt_ d _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= B j D E x E F = B j D F njt_ d _ragged_idx == E = njt_ d shape - F = E + njt_desc = _describe_njt njt_ d yield SampleInput _clone njt_ d kwargs= other torch randn E F device=device dtype=dtype name=f njt_desc B j D E x E F Dense x NJT cases njt_ d _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= B F E x B E j = B F j njt_ d _ragged_idx == B = njt_ d shape E = njt_ d shape F = E + njt_desc = _describe_njt njt_ d dense_t = torch randn B F E device=device dtype=dtype requires_grad=requires_grad dense_t _batch_dim = unbind_reference yield SampleInput dense_t args= _clone njt_ d name=f njt_desc B F E x B E j NJT x NJT = Dense case njt_ d _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= B E j x B j F = B E F njt_ d _ragged_idx == njt_ d is_contiguous B E _ = njt_ d shape sum_j = len njt_ d values other_cont = torch randn sum_j E + device=device dtype=dtype requires_grad=requires_grad other_njt = torch nested nested_tensor_from_jagged other_cont njt_ d offsets lengths=njt_ d _lengths njt_desc = _describe_njt njt_ d yield SampleInput _clone njt_ d kwargs= other _clone other_njt name=f njt_desc B E j x B j F TODO need factory functions B j D E x B j E F = B j D F sample_inputs_masked_select op_info device dtype requires_grad op_kwargs=None kwargs njt _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= yield SampleInput njt kwargs= mask torch randn_like njt requires_grad=False name=_describe_njt njt sample_inputs_narrow op_info device dtype requires_grad kwargs sample_input sample_inputs_unary_dimwise op_info device dtype requires_grad kwargs ragged dim narrowing test single start length value sample_input kwargs dim == sample_input input _ragged_idx yield _update_sample sample_input start length other dim narrowing test different start length values D = sample_input input size sample_input kwargs dim start length D D - D - D - yield _update_sample sample_input start start length length sample_inputs_nn_functional_embedding op_info device dtype requires_grad kwargs indices = torch nested nested_tensor torch tensor torch tensor torch tensor layout=torch jagged dtype=torch int device=device NUM_EMBEDDINGS = EMBEDDING_DIM = weight = torch randn NUM_EMBEDDINGS EMBEDDING_DIM device=device dtype=dtype NB OpInfo entry embedding_bag expects weight first so gradients can checked yield SampleInput _clone weight requires_grad_ args= indices yield SampleInput _clone weight requires_grad_ args= indices kwargs= padding_idx sample_inputs_index_put op_info device dtype requires_grad op_kwargs=None kwargs njt _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= dim range njt dim indices = torch tensor list range njt size device=njt device torch tensor njt size device=njt device _ range dim - njt_desc = _describe_njt njt yield SampleInput _clone njt kwargs= indices indices values torch tensor device=njt device name=f njt_desc up dim dim - Non-cont NJT completeness offsets = torch tensor device=device lengths = torch tensor device=device indices = torch tensor device=device torch tensor device=device torch tensor device=device = torch nested nested_tensor_from_jagged torch zeros device=device offsets lengths requires_grad_ requires_grad njt_desc = _describe_njt yield SampleInput _clone kwargs= indices indices values torch tensor device=a device name=f njt_desc all dims sample_inputs_nn_functional_embedding_bag op_info device dtype requires_grad kwargs generate_per_sample_weight True False mode sum mean max per_sample_weights only supported mode= sum mode = sum generate_per_sample_weight continue NUM_EMBEDDINGS = EMBEDDING_DIM = weight = torch randn NUM_EMBEDDINGS EMBEDDING_DIM dtype=dtype device=device njt = torch nested nested_tensor torch randint NUM_EMBEDDINGS size= torch randint NUM_EMBEDDINGS size= torch randint NUM_EMBEDDINGS size= layout=torch jagged dtype=torch int device=device per_sample_weights = None generate_per_sample_weight per_sample_weights = torch randn_like njt dtype=dtype NB OpInfo entry embedding_bag expects weight first so gradients can checked yield SampleInput weight args= njt kwargs= mode mode per_sample_weights per_sample_weights reference_nn_functional_embedding_bag op sample run reference single bag time new_kwargs = dict sample kwargs new_kwargs update offsets torch tensor dtype=torch int device=sample input device flip input weight back what unbind_reference expects sample = SampleInput sample args args= sample input kwargs=new_kwargs old_op = op op op op = torch nn functional embedding_bag output = unbind_reference op sample wrap_output_as_njt=False op op = old_op concat bag outputs get final output torch cat output dim= sample_inputs_nn_functional_linear op_info device dtype requires_grad kwargs njt _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= projection over ragged dim currently supported is_nested_int njt size - continue bias NUM_OUTPUT = weight = torch randn NUM_OUTPUT njt size - device=device dtype=dtype requires_grad=requires_grad bias = torch randn NUM_OUTPUT device=device dtype=dtype requires_grad=requires_grad yield SampleInput _clone njt kwargs= weight _clone weight bias _clone bias name=f _describe_njt njt bias without bias yield SampleInput _clone njt kwargs= weight _clone weight name=f _describe_njt njt without bias sample_inputs_nn_functional_prelu op_info device dtype requires_grad kwargs njt _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= Second dim interpreted number channels should non-ragged now num_channels = njt size is_nested_int num_channels continue D weight weight = torch randn num_channels device=device dtype=dtype requires_grad=requires_grad yield SampleInput _clone njt kwargs= weight _clone weight name=f _describe_njt njt D weight scalar tensor weight yield SampleInput _clone njt kwargs= weight torch tensor device=device dtype=dtype name=f _describe_njt njt scalar tensor weight sample_inputs_nn_functional_rms_norm op_info device dtype requires_grad kwargs njt _sample_njts device=device dtype=dtype requires_grad=requires_grad dims= normalize over non-ragged dims start_dim range njt dim start_dim = njt _ragged_idx continue normalized_shape = njt shape start_dim weight = torch randn normalized_shape device=device dtype=dtype requires_grad=requires_grad yield SampleInput _clone njt kwargs= normalized_shape normalized_shape weight weight name=f _describe_njt njt sample_inputs_nn_functional_threshold = partial sample_inputs_elementwise_njt_unary op_kwargs= threshold float fromhex x ap- value - sample_inputs_select op_info device dtype requires_grad kwargs sample_input sample_inputs_unary_dimwise op_info device dtype requires_grad kwargs ragged dim chunking test single index sample_input kwargs dim == sample_input input _ragged_idx yield _update_sample sample_input index other dim chunking test different indices D = sample_input input size sample_input kwargs dim index D D - yield _update_sample sample_input index index sample_inputs_split op_info device dtype requires_grad kwargs sample_input sample_inputs_unary_dimwise op_info device dtype requires_grad kwargs ragged dim chunking test single split size sample_input kwargs dim == sample_input input _ragged_idx yield _update_sample sample_input split_size_or_sections other dim chunking test different split sizes D = sample_input input size sample_input kwargs dim split_size D D - D yield _update_sample sample_input split_size_or_sections split_size sample_inputs_split_with_sizes op_info device dtype requires_grad kwargs sample_input sample_inputs_unary_dimwise op_info device dtype requires_grad kwargs It will never make sense operate ragged dim TODO Handle error_inputs sample_input kwargs dim == sample_input input _ragged_idx continue D = sample_input input size sample_input kwargs dim splits should add up D split = torch randint D - size= item split = D - split yield _update_sample sample_input split_sizes split split sample_inputs_squeeze op_info device dtype requires_grad kwargs squeeze-specific NJT generator need ensure there some s shape _get_njts njt = random_nt_from_dims None device=device dtype=dtype requires_grad=requires_grad layout=torch jagged yield njt without min max seqlen cached values = njt values detach clone offsets = njt offsets detach clone yield torch nested nested_tensor_from_jagged values offsets non-contiguous transposed yield njt transpose non-contiguous holes values = njt values detach clone offsets = njt offsets detach clone subtract cause holes lengths = offsets diff - detach clone yield torch nested nested_tensor_from_jagged values=values offsets=offsets lengths=lengths njt _get_njts single dim operation dim range njt dim Operation batch ragged dim never expected work TODO Handle these via error_inputs dim == dim == njt _ragged_idx continue yield SampleInput _clone njt kwargs= dim dim name=f _describe_njt njt _describe_dim njt dim multiple dim operation pass no args yield SampleInput _clone njt kwargs= dim dim name=f _describe_njt njt multiple dims sample_inputs_unflatten op_info device dtype requires_grad kwargs sample_input sample_inputs_unary_dimwise op_info device dtype requires_grad kwargs It will never make sense operate ragged dim TODO Handle error_inputs sample_input kwargs dim == sample_input input _ragged_idx continue D = sample_input input size sample_input kwargs dim sizes should multiply D yield _update_sample sample_input sizes D yield _update_sample sample_input sizes D D == yield _update_sample sample_input sizes D yield _update_sample sample_input sizes D sample_inputs_unsqueeze op_info device dtype requires_grad kwargs sample_input sample_inputs_unary_dimwise op_info device dtype requires_grad kwargs yield sample_input last_dim_sample = _update_sample sample_input dim - last_dim_sample name = f _describe_njt last_dim_sample input add dim end Tell unbind reference how canonicalize dim kwargs This necessary because unsqueeze allows dim after last dim indicate unsqueeze end last_dim_sample input _ndim = last_dim_sample input dim + yield last_dim_sample sample_inputs_where op_info device dtype requires_grad kwargs sample sample_inputs_elementwise_njt_binary op_info device dtype requires_grad kwargs other = sample args sample args = sample kwargs other = other sample kwargs condition = sample input sample name = sample name replace NT yield sample === END OP-SPECIFIC SAMPLE INPUTS FUNCS REFERENCES === Mapping OpInfo full names - sample_inputs_funcs which define set sample inputs involving NJTs pass op Full name consists OpInfo s name variant name separated period e g special polygamma special_polygamma_n_ These necessary specify they cannot auto-generated some reason Try keep these sorted alphabetical order njt_sample_inputs = bmm sample_inputs_bmm chunk sample_inputs_chunk clone sample_inputs_clone count_nonzero partial sample_inputs_njt_reduction supports_keepdim=False fill sample_inputs_fill f mvlgamma mvlgamma_p_ p sample_inputs_mvl_gamma p= p nn functional embedding sample_inputs_nn_functional_embedding nn functional embedding_bag sample_inputs_nn_functional_embedding_bag nn functional linear sample_inputs_nn_functional_linear nn functional prelu sample_inputs_nn_functional_prelu nn functional rms_norm sample_inputs_nn_functional_rms_norm nn functional threshold sample_inputs_nn_functional_threshold f polygamma polygamma_n_ n sample_inputs_polygamma_n n=n n range special polygamma special_polygamma_n_ sample_inputs_special_polygamma_n n= sample_inputs_to matmul sample_inputs_matmul masked_select sample_inputs_masked_select narrow sample_inputs_narrow index_put sample_inputs_index_put these two don t have ReductionOpInfo entries max reduction_with_dim sample_inputs_njt_reduction min reduction_with_dim sample_inputs_njt_reduction select sample_inputs_select split sample_inputs_split split_with_sizes sample_inputs_split_with_sizes squeeze sample_inputs_squeeze unflatten sample_inputs_unflatten unsqueeze sample_inputs_unsqueeze where sample_inputs_where njt_references = bmm reference_bmm chunk partial unary_dimwise_reference batchwise_reference=batchwise_reference_chunk count_nonzero reduction_reference these two don t have ReductionOpInfo entries max reduction_with_dim reduction_reference min reduction_with_dim reduction_reference narrow partial unary_dimwise_reference batchwise_reference=batchwise_reference_narrow select partial unary_dimwise_reference batchwise_reference=batchwise_reference_select split partial unary_dimwise_reference batchwise_reference=batchwise_reference_split split_with_sizes partial unary_dimwise_reference batchwise_reference=batchwise_reference_split_with_sizes squeeze unbind_reference nn functional embedding_bag reference_nn_functional_embedding_bag unflatten partial unary_dimwise_reference batchwise_reference=batchwise_reference_unflatten unsqueeze partial unary_dimwise_reference batchwise_reference=batchwise_reference_unsqueeze Translates OpInfo entry one operates NJTs translate_opinfo op new_op = copy op new_op supports_njt = True add some extra info use generating tests right subset ops new_op _extra_op_data = extra_op_data get op full_name ExtraOpData op full_name njt_sample_inputs new_op sample_inputs_func = njt_sample_inputs op full_name new_op ref = njt_references get op full_name unbind_reference isinstance op UnaryUfuncInfo new_op sample_inputs_func = partial sample_inputs_elementwise_njt_unary op_kwargs=None new_op ref = unbind_reference isinstance op BinaryUfuncInfo new_op sample_inputs_func = partial sample_inputs_elementwise_njt_binary op_kwargs=None new_op ref = unbind_reference isinstance op ReductionOpInfo new_op sample_inputs_func = partial sample_inputs_njt_reduction op_kwargs=None new_op ref = reduction_reference TODO Translate rest OpInfos new_op sample_inputs_func = unsupported_sample_inputs_func op full_name new_op ref = unsupported_reference op full_name new_op supports_njt = False new_op njt_op_db = translate_opinfo op op op_db