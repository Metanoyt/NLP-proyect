Owner s module c d gc re threading unittest datetime timedelta typing Optional torch torch distributed dist torch distributed _functional_collectives funcol torch _C FileCheck torch _inductor utils fresh_cache run_and_get_code run_and_get_triton_code torch distributed _functional_collectives all_gather_into_tensor_coalesced all_gather_tensor all_reduce all_reduce_coalesced all_to_all_single AsyncCollectiveTensor reduce_scatter_tensor reduce_scatter_tensor_coalesced torch testing _internal common_cuda PLATFORM_SUPPORTS_FP torch testing _internal common_device_type e m _type torch testing _internal common_distributed MultiProcessTestCase requires_accelerator_dist_backend skip_if_lt_x_gpu torch testing _internal common_utils type ignore attr-defined run_tests TestCase torch testing _internal distributed fake_pg FakeStore torch testing _internal inductor_utils HAS_GPU load_test_module name sys importlib machinery SourceFileLoader pathlib Path unittest mock testdir = Path __file__ absolute parent parent mock patch sys path sys path str testdir SourceFileLoader name str testdir f name replace py load_module AOTIRunnerUtil = load_test_module inductor test_aot_inductor_utils AOTIRunnerUtil sys dist is_available print distributed package available skipping tests file=sys stderr sys exit requires_accelerator_dist_backend nccl xccl TestWithNCCL MultiProcessTestCase setUp - None super setUp _spawn_processes property world_size - int property ranks - list int list range world_size property device - torch device torch device rank _init_process_group - None torch accelerator set_device_index rank store = dist FileStore file_name world_size backend = dist get_default_backend_for_device device type dist init_process_group backend=backend world_size=self world_size rank=self rank store=store torch _C _distributed_c d _register_process_group default dist group WORLD skip_if_lt_x_gpu test_all_reduce_single - None _init_process_group input = torch full float rank device=self device output = torch ops _c d_functional all_reduce input avg default output = torch ops _c d_functional wait_tensor output assert id output = id input expect = sum ranks world_size assert output eq expect all Test Python API AsyncCollectiveTensor output = all_reduce input avg default assert isinstance output AsyncCollectiveTensor assert output completed assert output eq expect all assert output completed skip_if_lt_x_gpu test_all_reduce_single_ - None _init_process_group input = torch full float rank device=self device output = torch ops _c d_functional all_reduce_ input avg default output = torch ops _c d_functional wait_tensor output assert id output == id input expect = sum ranks world_size assert output eq expect all skip_if_lt_x_gpu test_all_reduce_coalesced - None _init_process_group inputs = torch full i i float rank i device=self device i range outputs = torch ops _c d_functional all_reduce_coalesced inputs avg default i output input enumerate zip outputs inputs output = torch ops _c d_functional wait_tensor output assert id output = id input assert output eq sum ranks world_size i all Test Python API AsyncCollectiveTensor outputs = all_reduce_coalesced inputs avg default i output input enumerate zip outputs inputs assert output completed assert output eq sum ranks world_size i all assert output completed skip_if_lt_x_gpu test_all_reduce_coalesced_ - None _init_process_group inputs = torch full i i float rank i device=self device i range outputs = torch ops _c d_functional all_reduce_coalesced_ inputs avg default i output input enumerate zip outputs inputs output = torch ops _c d_functional wait_tensor output assert id output == id input assert output eq sum ranks world_size i all skip_if_lt_x_gpu test_all_gather_into_tensor_single - None _init_process_group input = torch full float rank device=self device output = torch ops _c d_functional all_gather_into_tensor input world_size default output = torch ops _c d_functional wait_tensor output expect = torch cat torch full float rank device=self device rank ranks assert torch allclose output expect assert output eq expect all Test out-variant all_gather_into_tensor output = torch empty expect shape device=self device output = torch ops _c d_functional all_gather_into_tensor_out input world_size default out=output output = torch ops _c d_functional wait_tensor output assert torch allclose output expect assert output eq expect all Test Python API AsyncCollectiveTensor output = all_gather_tensor input default assert isinstance output AsyncCollectiveTensor assert output completed assert output eq expect all assert output completed https github com pytorch pytorch issues skip_if_lt_x_gpu test_functional_collectives_inference_mode - None _init_process_group torch inference_mode input = torch full float rank device=self device out = funcol all_gather_tensor input gather_dim= group=torch distributed group WORLD out = out dtype=torch bfloat tests call properly triggered wait AsyncCollectiveTensor assertTrue type out torch Tensor assertEqual out torch tensor device=self device dtype=torch bfloat unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu https github com pytorch pytorch issues test_inductor_dtypeview_memory_leak _init_process_group func arg torch Tensor - torch Tensor ag = torch ops _c d_functional all_gather_into_tensor default arg world_size default ag _view = torch ops aten view dtype ag torch int funcol wait_tensor ag _view arg = torch full float rank device=self device dtype=torch float compiled = torch compile func mem_usage = check aten view dtype compiled aten view dtype code = run_and_get_triton_code compiled arg FileCheck check torch ops _c d_functional wait_tensor default aten view dtype run code check memory leak i range mem_usage i = torch accelerator max_memory_allocated compiled arg assert mem_usage == mem_usage skip_if_lt_x_gpu test_all_gather_into_tensor_coalesced - None _init_process_group inputs = torch full float rank i device=self device i range outputs = torch ops _c d_functional all_gather_into_tensor_coalesced inputs world_size default expect = torch cat torch full float rank i device=self device rank ranks i range i output enumerate outputs output = torch ops _c d_functional wait_tensor output assert output eq expect i all Test Python API AsyncCollectiveTensor outputs = all_gather_into_tensor_coalesced inputs default i output enumerate outputs assert output completed assert output eq expect i all assert output completed skip_if_lt_x_gpu test_reduce_scatter_tensor_single - None _init_process_group input = torch tensor ranks device=self device output = torch ops _c d_functional reduce_scatter_tensor input avg world_size default output = torch ops _c d_functional wait_tensor output assert output eq rank all Test Python API AsyncCollectiveTensor output = reduce_scatter_tensor input avg default assert isinstance output AsyncCollectiveTensor assert output completed assert output eq rank all assert output completed skip_if_lt_x_gpu test_reduce_scatter_tensor_coalesced - None _init_process_group inputs = torch tensor ranks device=self device i i range outputs = torch ops _c d_functional reduce_scatter_tensor_coalesced inputs avg world_size default i output enumerate outputs output = torch ops _c d_functional wait_tensor output assert output eq rank i all Test Python API AsyncCollectiveTensor outputs = reduce_scatter_tensor_coalesced inputs avg default i output enumerate outputs assert output completed assert output eq rank i all assert output completed skip_if_lt_x_gpu test_all_to_all_single - None _init_process_group torch accelerator set_device_index rank torch manual_seed send_sz_matrix = torch randint world_size world_size input_split_sizes = send_sz_matrix rank tolist output_split_sizes = send_sz_matrix rank tolist input = torch full sum input_split_sizes float rank device type output = torch ops _c d_functional all_to_all_single input output_split_sizes input_split_sizes default output = torch ops _c d_functional wait_tensor output expect = torch cat torch full sz float rank device type rank sz enumerate output_split_sizes assert output eq expect all Test Python API AsyncCollectiveTensor output = all_to_all_single input output_split_sizes input_split_sizes default assert output completed assert output eq expect all assert output completed skip_if_lt_x_gpu test_broadcast - None _init_process_group input = torch full float rank device=self device output = torch ops _c d_functional broadcast input default output = torch ops _c d_functional wait_tensor output assert id output = id input expect = assert output eq expect all Test Python API AsyncCollectiveTensor output = funcol broadcast input default assert isinstance output AsyncCollectiveTensor assert output completed assert output eq expect all assert output completed skip_if_lt_x_gpu test_wait_tensor - None _init_process_group input = torch full float rank device=self device assertEqual torch _C _distributed_c d _get_work_registry_size output = torch ops _c d_functional all_reduce input avg default assertEqual torch _C _distributed_c d _get_work_registry_size torch ops _c d_functional wait_tensor output ` wait_tensor output ` will pop work work registry immediately assertEqual torch _C _distributed_c d _get_work_registry_size skip_if_lt_x_gpu test_unwaited - None Verify process can terminate gracefully even unwaited tensors _init_process_group input = torch full float rank device=self device assertEqual torch _C _distributed_c d _get_work_registry_size torch ops _c d_functional all_reduce input avg default assertEqual torch _C _distributed_c d _get_work_registry_size unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch skip_if_lt_x_gpu fresh_cache test_threading _init_process_group device = device func arg torch Tensor - torch Tensor buf = arg + ar = funcol all_reduce buf avg ar = funcol wait_tensor ar ar + arg = torch rand device=device func arg compiled = torch compile func fullgraph=True code = run_and_get_triton_code compiled arg FileCheck check all_reduce_ default buf avg run code Unless explicitly specified e g custom runtime process group registry shared among all threads process Here we verify process group registered main thread can resolved different thread TestThread threading Thread run exc = None try func arg compiled arg except BaseException exc noqa B exc = exc join threading Thread join exc raise exc t = TestThread t start t join unittest skipIf PLATFORM_SUPPORTS_FP _scaled_mm currently only supports sm = cuda gfx ROCm skip_if_lt_x_gpu fresh_cache test_fixed_striding _init_process_group scale t scale = torch finfo e m _type max t abs amax dim=- keepdim=True float t = t mul scale e m _type t scale fp _rowwise_backward in_ w out_grad out_grad_fp scale_out_grad = scale out_grad w_fp scale_w = scale w t contiguous out_grad_fp = funcol all_gather_tensor out_grad_fp gather_dim= group=torch distributed group WORLD scale_out_grad = funcol all_gather_tensor scale_out_grad gather_dim= group=torch distributed group WORLD in_grad = torch _scaled_mm out_grad_fp w_fp t scale_a=scale_out_grad scale_b=scale_w t out_dtype=torch bfloat out_grad = funcol all_gather_tensor out_grad t contiguous gather_dim= group=torch distributed group WORLD w_grad = out_grad in_ in_grad w_grad m n k = in_ = torch randn m k device=self device type dtype=torch bfloat w = torch randn n k device=self device type dtype=torch bfloat out_grad = torch randn m n device=self device type dtype=torch bfloat eager_in_grad eager_w_grad = fp _rowwise_backward in_ w out_grad compile_in_grad compile_w_grad = torch compile fp _rowwise_backward in_ w out_grad assertTrue torch allclose compile_w_grad eager_w_grad dummy_init_pg - None dist is_initialized dist init_process_group backend= gloo rank= world_size= store=dist HashStore _DummyWork dist Work __init__ pg ProcessGroupDummy - None super __init__ pg = pg wait timeout Optional timedelta = None - bool pg waits += True __del__ pg dels += ProcessGroupDummy dist ProcessGroup This process group discards all data passed returns success This intended rare cases where we want discard certain operations without modifying underlying library This PG only supports world_size __init__ - None super __init__ _group_name = dummy dummy waits = dels = broadcast tensor_list list torch Tensor opts object - dist Work _DummyWork allgather_into_tensor_coalesced output_lists list torch Tensor input_list list torch Tensor opts object - dist Work _DummyWork allreduce tensors list torch Tensor opts object - dist Work _DummyWork reduce_scatter_tensor_coalesced outputTensors list torch Tensor inputTensors list torch Tensor opts object - dist Work _DummyWork property group_name - str _group_name None raise ValueError ProcessGroup name set _group_name _set_group_name name str - None _group_name = name register - dist ProcessGroup create_pg prefix_store dist PrefixStore rank int world_size int timeout float - dist ProcessGroup dist Backend register_backend group_name create_pg devices= cpu dist new_group ranks= backend=self group_name group_desc=self group_name timeout=timedelta seconds= timeout isn t used PyWorkTest TestCase Native functional collectives have some interesting interactions PyProcessGroup due Python reference counting pybind trampoline classes C++ types This validates PyProcessGroup PyWork aren t getting prematurely freed test_wait_tensor - None wait_called = False MyWork dist Work wait _ nonlocal wait_called wait_called = True check registration implicit unregistration tensor = torch rand work = MyWork torch _C _distributed_c d _register_work tensor work Force GC collection MyWork object we re doing correct reference counting we ll deadlock wait_tensor del work gc collect torch ops _c d_functional wait_tensor tensor assertTrue wait_called test_collectives - None dummy_init_pg pg = ProcessGroupDummy register x = torch rand x = funcol all_reduce x sum group=pg gc collect assertEqual pg dels x wait assertEqual pg waits assertEqual pg dels x = torch rand x = funcol broadcast x group=pg gc collect assertEqual pg dels x wait assertEqual pg waits assertEqual pg dels x = torch rand x = funcol all_gather_tensor x group=pg gc collect assertEqual pg dels x wait assertEqual pg waits assertEqual pg dels x = torch rand x = funcol reduce_scatter_tensor x sum group=pg gc collect assertEqual pg dels x wait assertEqual pg waits assertEqual pg dels find_buffer_assignments code pattern = r buf \d+ = empty_strided_ matches = re finditer pattern code tuple f buf match group match matches CompileTestCPU TestCase setUp super setUp dist is_initialized rank = world_size = store = FakeStore dist init_process_group backend= fake world_size=self world_size rank=self rank store=store tearDown dist destroy_process_group fresh_cache _test_inductor_all_reduce_cpu cpp_wrapper=False func arg torch Tensor - torch Tensor buf = arg + ar = funcol all_reduce buf avg ar = funcol wait_tensor ar ar arg = torch rand device= cpu torch _inductor config patch cpp_wrapper cpp_wrapper compiled = torch compile func _ code = run_and_get_code compiled arg include_ops = aoti_torch_cpu__c d_functional_all_reduce_ aoti_torch_cpu__c d_functional_wait_tensor cpp_wrapper torch ops _c d_functional all_reduce_ default torch ops _c d_functional wait_tensor default op include_ops assertIn op code Test aoti AOTIRunnerUtil run func arg torch cpu synchronize test_inductor_all_reduce_cpu _test_inductor_all_reduce_cpu cpp_wrapper=False _test_inductor_all_reduce_cpu cpp_wrapper=True CompileTest TestCase setUp super setUp rank = world_size = torch accelerator set_device_index device = torch accelerator current_accelerator store = FakeStore dist init_process_group backend= fake world_size=self world_size rank=self rank store=store tearDown dist destroy_process_group unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch fresh_cache test_inductor_all_reduce_single func arg torch Tensor - torch Tensor buf = arg + Expect in-place inductor allocated buf ar = funcol all_reduce buf avg ar = funcol wait_tensor ar Expect no in-place graph input ar = funcol all_reduce arg avg ar = funcol wait_tensor ar ar ar arg = torch rand device=self device compiled = torch compile func code = run_and_get_triton_code compiled arg buf buf = find_buffer_assignments code FileCheck check f buf = empty check f buf = empty Expect in-place inductor allocated buf check f torch ops _c d_functional all_reduce_ default buf check f torch ops _c d_functional wait_tensor default buf Expect no in-place graph input check f torch ops _c d_functional all_reduce_ default buf check f torch ops _c d_functional wait_tensor default buf Expect no extra copy check f buf buf run code Check tensor wait_tensor used anywhere assert = torch ops _c d_functional wait_tensor default code torch _inductor config patch cpp_wrapper True code = run_and_get_triton_code compiled arg Check tensors all_reduce wait_tensor used anywhere checking they explicitly deleted calling aoti_torch_delete_tensor_object FileCheck check_not all_reduce must have been rewritten into all_reduce_ aoti_torch_cpu__c d_functional_all_reduce buf check_count aoti_torch_delete_tensor_object buf run code Test aoti AOTIRunnerUtil run func arg torch accelerator synchronize unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch fresh_cache test_inductor_all_reduce_coalesced func args list torch Tensor - torch Tensor bufs = arg + arg args Expect in-place inductor allocated buf ar = funcol all_reduce_coalesced bufs avg ar = funcol wait_tensor out out ar Expect no in-place graph input ar = funcol all_reduce_coalesced args avg ar = funcol wait_tensor out out ar ar ar args = torch rand device=self device type _ range compiled = torch compile func code = run_and_get_triton_code compiled args buf buf buf buf = find_buffer_assignments code FileCheck check f buf = empty check f buf = empty check f buf = empty check f buf = empty Expect in-place inductor allocated buf check f torch ops _c d_functional all_reduce_coalesced_ default buf buf Expect no in-place graph input buf buf clones check f torch ops _c d_functional all_reduce_coalesced_ default buf buf check f torch ops _c d_functional wait_tensor default buf check f torch ops _c d_functional wait_tensor default buf check f torch ops _c d_functional wait_tensor default buf check f torch ops _c d_functional wait_tensor default buf Expect no extra copy check f buf buf buf buf run code assert = torch ops _c d_functional wait_tensor default code Test aoti out = AOTIRunnerUtil run func args noqa F torch accelerator synchronize unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch fresh_cache test_inductor_inplace_op_on_view func arg torch Tensor - torch Tensor buf = arg + ar = funcol all_reduce buf avg ar = funcol wait_tensor ar ar arg = torch rand device=self device type compiled = torch compile func code = run_and_get_triton_code compiled arg buf = find_buffer_assignments code FileCheck check f buf = empty We always call contiguous input all_reduce_ so input will view anymore check f torch ops _c d_functional all_reduce_ default buf check f torch ops _c d_functional wait_tensor default buf check f buf run code unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch fresh_cache test_inductor_all_reduce_non_contig_input func arg torch Tensor - torch Tensor ar = funcol all_reduce arg avg ar = funcol wait_tensor ar Expect allocation ar arg = torch rand device=self device type T compiled = torch compile func code = run_and_get_triton_code compiled arg clone induced non contig input assert torch ops _c d_functional wait_tensor default code func arg torch Tensor - torch Tensor torch ops _c d_functional all_reduce_ arg avg arg compiled = torch compile func code = run_and_get_triton_code compiled arg clone induced non contig input assert torch ops _c d_functional wait_tensor default code unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch unittest skipIf torch _inductor config triton native_matmul no extern_kernels mm fresh_cache test_inductor_reuse_buffer_after_inplace_collective func arg torch Tensor - torch Tensor Expect allocation buf = arg + ar = funcol all_reduce buf avg ar = funcol wait_tensor ar Expect allocation buf = torch mm arg ar Expect buf reused buf = torch mm arg buf buf buf arg = torch rand device=self device type compiled = torch compile func code = run_and_get_triton_code compiled arg buf buf = find_buffer_assignments code FileCheck Expect allocation check f buf = empty check f torch ops _c d_functional all_reduce_ default buf check f torch ops _c d_functional wait_tensor default buf Expect allocation check f buf = empty check f extern_kernels mm arg _ buf out= buf Expect buf reused check f buf = buf del buf reuse check f extern_kernels mm arg _ buf out=buf Expect no extra copy check f buf buf run code assert = torch ops _c d_functional wait_tensor default code unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch fresh_cache test_inductor_all_gather_into_tensor_single func arg torch Tensor - torch Tensor ag = funcol all_gather_tensor arg ag = funcol wait_tensor ag ag arg = torch rand device=self device type compiled = torch compile func code = run_and_get_triton_code compiled arg FileCheck check buf = torch ops _c d_functional all_gather_into_tensor default arg _ check torch ops _c d_functional wait_tensor default buf Expect no extra copy check buf run code assert = torch ops _c d_functional wait_tensor default code Test aoti AOTIRunnerUtil run func arg torch accelerator synchronize unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch fresh_cache test_inductor_all_gather_into_tensor_coalesced func args list torch Tensor - torch Tensor ag = funcol all_gather_into_tensor_coalesced args ag = funcol wait_tensor out out ag ag args = torch rand device=self device type _ range compiled = torch compile func code = run_and_get_triton_code compiled args FileCheck check buf = torch ops _c d_functional all_gather_into_tensor_coalesced default arg _ arg _ arg _ arg _ check buf = buf check buf = buf check buf = buf check buf = buf check torch ops _c d_functional wait_tensor default buf check torch ops _c d_functional wait_tensor default buf check torch ops _c d_functional wait_tensor default buf check torch ops _c d_functional wait_tensor default buf Expect no extra copy check buf buf buf buf run code Test aoti out = AOTIRunnerUtil run func args noqa F torch accelerator synchronize unittest skipIf HAS_GPU This GPU test fresh_cache test_wait_tensor func arg torch Tensor - torch Tensor t = torch ops _c d_functional all_reduce arg avg funcol wait_tensor t Test aoti arg = torch rand device=self device type compiled = torch compile func code = run_and_get_triton_code compiled arg FileCheck check torch ops _c d_functional wait_tensor default buf check buf run code Test aoti AOTIRunnerUtil run func arg torch accelerator synchronize unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch fresh_cache test_inductor_reduce_scatter_tensor_single func arg torch Tensor - torch Tensor rs = funcol reduce_scatter_tensor arg avg rs = funcol wait_tensor rs rs arg = torch rand device=self device type compiled = torch compile func code = run_and_get_triton_code compiled arg FileCheck check buf = torch ops _c d_functional reduce_scatter_tensor default arg _ check torch ops _c d_functional wait_tensor default buf Expect no extra copy check buf run code Test aoti AOTIRunnerUtil run func arg torch accelerator synchronize unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch fresh_cache test_inductor_reduce_scatter_tensor_coalesced func args list torch Tensor - torch Tensor rs = funcol reduce_scatter_tensor_coalesced args avg len args rs = funcol wait_tensor out out rs rs args = torch rand device=self device type _ range compiled = torch compile func code = run_and_get_triton_code compiled args FileCheck check buf = torch ops _c d_functional reduce_scatter_tensor_coalesced default arg _ arg _ arg _ arg _ check buf = buf check buf = buf check buf = buf check buf = buf check torch ops _c d_functional wait_tensor default buf check torch ops _c d_functional wait_tensor default buf check torch ops _c d_functional wait_tensor default buf check torch ops _c d_functional wait_tensor default buf Expect no extra copy check buf buf buf buf run code Test aoti AOTIRunnerUtil run func args torch accelerator synchronize unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch fresh_cache test_inductor_all_to_all_single func input torch Tensor output_split_sizes torch Tensor input_split_sizes torch Tensor - torch Tensor output = funcol all_to_all_single input output_split_sizes tolist input_split_sizes tolist funcol wait_tensor output torch manual_seed send_sz_matrix = torch randint world_size world_size input_split_sizes = send_sz_matrix rank output_split_sizes = send_sz_matrix rank contiguous input = torch full input_split_sizes sum item float rank device type torch _dynamo config patch dynamic_shapes=True capture_dynamic_output_shape_ops=True capture_scalar_outputs=True compiled = torch compile func dynamic=True code = run_and_get_triton_code compiled input output_split_sizes input_split_sizes FileCheck check_regex torch ops _c d_functional all_to_all_single default\\ arg\\d+_\\d+ \\ u\\d+ u\\d+\\ \\ u\\d+ u\\d+\\ check torch ops _c d_functional wait_tensor default run code unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch fresh_cache test_inductor_broadcast func arg torch Tensor - torch Tensor buf = arg + Expect in-place inductor allocated buf br = funcol broadcast buf br = funcol wait_tensor br Expect no in-place graph input br = funcol broadcast arg br = funcol wait_tensor br br br arg = torch rand device=self device type compiled = torch compile func code = run_and_get_triton_code compiled arg buf buf = find_buffer_assignments code FileCheck check f buf = empty check f buf = buf check f buf = empty Expect in-place inductor allocated buf check torch ops _c d_functional broadcast_ default buf check torch ops _c d_functional wait_tensor default buf Expect no in-place graph input buf clone check f torch ops _c d_functional broadcast_ default buf check f torch ops _c d_functional wait_tensor default buf Expect no extra copy check f buf buf run code Test aoti AOTIRunnerUtil run func arg torch accelerator synchronize unittest skipIf HAS_GPU Inductor+gpu needs triton recent GPU arch fresh_cache test_ranks_and_tag func arg torch Tensor - torch Tensor buf = arg + Expect in-place inductor allocated buf ar = funcol all_reduce buf avg ar = funcol wait_tensor ar Expect no in-place graph input ar = funcol all_reduce arg avg ar = funcol wait_tensor ar ar ar arg = torch rand device=self device type compiled = torch compile func fullgraph=True code = run_and_get_triton_code compiled arg FileCheck check all_reduce_ default buf avg run code __name__ == __main__ run_tests