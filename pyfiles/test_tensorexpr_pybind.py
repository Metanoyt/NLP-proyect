Owner s NNC torch numpy np torch _C _te te torch testing _internal common_utils run_tests torch testing _internal jit_utils JitTestCase unittest LLVM_ENABLED = torch _C _llvm_enabled construct_adder n int dtype=torch float A = te BufHandle A n dtype B = te BufHandle B n dtype compute i A load i + B load i C = te Compute C n compute loopnest = te LoopNest C loopnest prepare_for_codegen stmt = te simplify loopnest root_stmt te construct_codegen ir_eval stmt A B C TestTensorExprPyBind JitTestCase test_simple_sum n = cg = construct_adder n tA = torch randn n tB = torch randn n tC = torch empty n cg call tA tB tC torch testing assert_close tA + tB tC test_call_raw n = cg = construct_adder n dtype=torch float tA = torch randn n dtype=torch float tB = torch randn n dtype=torch float tC = torch empty n dtype=torch float cg call_raw tA data_ptr tB data_ptr tC data_ptr torch testing assert_close tA + tB tC test_external_calls dtype = torch float A = te BufHandle A dtype B = te BufHandle B dtype C = te BufHandle C dtype s = te ExternalCall C nnc_aten_matmul A B loopnest = te LoopNest s C loopnest prepare_for_codegen codegen = te construct_codegen ir_eval s A B C tA = torch ones tB = torch ones tC = torch empty codegen call tA tB tC torch testing assert_close torch matmul tA tB tC test_dynamic_shape dN = te VarHandle torch int A = te BufHandle dN torch float B = te BufHandle dN torch float compute i A load i - B load i C = te Compute C dN compute loopnest = te LoopNest C loopnest prepare_for_codegen cg = te construct_codegen ir_eval loopnest simplify A B C dN test_with_shape n tA = torch randn n dtype=torch double tB = torch randn n dtype=torch double tC = torch empty n dtype=torch double cg call tA tB tC n torch testing assert_close tA - tB tC test_with_shape test_with_shape test_dynamic_shape_ d dN = te VarHandle torch int dM = te VarHandle torch int A = te BufHandle dN dM torch float B = te BufHandle dN dM torch float compute i j A load i j - B load i j C = te Compute C dN dM compute loopnest = te LoopNest C loopnest prepare_for_codegen cg = te construct_codegen ir_eval loopnest simplify A B C dN dM test_with_shape n m tA = torch randn n m dtype=torch double tB = torch randn n m dtype=torch double tC = torch empty n m dtype=torch double cg call tA tB tC n m torch testing assert_close tA - tB tC test_with_shape test_with_shape test_dtype_error te BufHandle torch float ok assertRaises TypeError lambda te BufHandle float unittest skipIf LLVM_ENABLED LLVM backend enabled test_kernel_with_tensor_inputs f b c + b + c device size = cpu x = torch rand size device=device y = torch rand size device=device z = torch rand size device=device graph_str = graph Float strides= requires_grad= device=cpu b Float strides= requires_grad= device=cpu c Float strides= requires_grad= device=cpu int = prim Constant value= Float strides= requires_grad= device=cpu = aten add b Float strides= requires_grad= device=cpu = aten add c graph = torch _C parse_ir graph_str kernel = te TensorExprKernel graph res = kernel run x y z res = kernel fallback x y z correct = f x y z np testing assert_allclose res numpy correct numpy atol= e- np testing assert_allclose res numpy correct numpy atol= e- unittest skipIf LLVM_ENABLED LLVM backend enabled test_kernel_with_scalar_inputs f b c + b + c x = torch tensor dtype=torch float device= cpu y = torch tensor dtype=torch float device= cpu z = torch tensor dtype=torch float device= cpu graph_str = graph Float requires_grad= device=cpu b Float requires_grad= device=cpu c Float requires_grad= device=cpu int = prim Constant value= Float requires_grad= device=cpu = aten add b Float requires_grad= device=cpu = aten add c graph = torch _C parse_ir graph_str kernel = te TensorExprKernel graph res = kernel run x y z res = kernel fallback x y z correct = f x y z np testing assert_allclose res numpy correct numpy atol= e- np testing assert_allclose res numpy correct numpy atol= e- unittest skipIf LLVM_ENABLED LLVM backend enabled test_kernel_shape_prop device size = cpu x = torch rand size device=device y = torch rand size device=device graph_str = graph Tensor b Tensor c Tensor = aten mul b c graph = torch _C parse_ir graph_str exception_thrown = False try kernel = te TensorExprKernel graph except RuntimeError Graph doesn t have shape info inputs = compilation should fail exception_thrown = True assert exception_thrown Inject shape info try compiling again example_inputs = torch rand torch rand torch _C _te annotate_input_shapes graph example_inputs torch _C _jit_pass_propagate_shapes_on_graph graph Now compilation should pass kernel = te TensorExprKernel graph res = kernel run x y correct = torch mul x y np testing assert_allclose res numpy correct numpy atol= e- unittest skipIf LLVM_ENABLED LLVM backend enabled test_kernel_shape_prop_module TestModule torch nn Module forward x y x x + y graph = torch jit script TestModule graph Try compiling graph as-is It should fail because doesn t have shape info exception_thrown = False try kernel = te TensorExprKernel graph except RuntimeError exception_thrown = True assert exception_thrown Try injecting shape info graph inputs example_inputs = torch rand torch rand exception_thrown = False try torch _C _te annotate_input_shapes graph example_inputs except RuntimeError Graph has argument which we can t set shapes exception_thrown = True assert exception_thrown Remove argument try annotating shapes one more time torch _C _te remove_unused_self_argument graph Inject shape info try compiling again torch _C _te annotate_input_shapes graph example_inputs torch _C _jit_pass_propagate_shapes_on_graph graph Now compilation should pass kernel = te TensorExprKernel graph device size = cpu x = torch rand size device=device y = torch rand size device=device res = kernel run x y correct = TestModule forward x y np testing assert_allclose res numpy correct numpy atol= e- unittest skipIf LLVM_ENABLED LLVM backend enabled test_kernel_with_t f t device size = cpu x = torch rand size device=device graph_str = graph Float strides= requires_grad= device=cpu Float strides= requires_grad= device=cpu = aten t graph = torch _C parse_ir graph_str kernel = te TensorExprKernel graph res = kernel run x res = kernel fallback x correct = f x np testing assert_allclose res numpy correct numpy atol= e- np testing assert_allclose res numpy correct numpy atol= e- unittest skipIf LLVM_ENABLED LLVM backend enabled test_kernel_with_transpose f transpose - - device size = cpu x = torch rand size device=device graph_str = graph Float strides= requires_grad= device=cpu int = prim Constant value=- int = prim Constant value=- Float strides= requires_grad= device=cpu = aten transpose graph = torch _C parse_ir graph_str kernel = te TensorExprKernel graph res = kernel run x res = kernel fallback x correct = f x np testing assert_allclose res numpy correct numpy atol= e- np testing assert_allclose res numpy correct numpy atol= e- unittest skipIf LLVM_ENABLED LLVM backend enabled test_kernel_with_permute f permute device size = cpu x = torch rand size device=device graph_str = graph Float strides= requires_grad= device=cpu int = prim Constant value= int = prim Constant value= int = prim Constant value= int = prim ListConstruct Float strides= requires_grad= device=cpu = aten permute graph = torch _C parse_ir graph_str kernel = te TensorExprKernel graph res = kernel run x res = kernel fallback x correct = f x np testing assert_allclose res numpy correct numpy atol= e- np testing assert_allclose res numpy correct numpy atol= e- unittest skipIf LLVM_ENABLED LLVM backend enabled test_kernel_with_custom_lowering f nan_to_num device = cpu x = torch ones device=device x = x = torch nan graph_str = graph x Float strides= requires_grad= device=cpu none NoneType = prim Constant y Float strides= requires_grad= device=cpu = aten nan_to_num x none none none y graph = torch _C parse_ir graph_str my_custom_lowering inputs out_shape out_stride out_type device compute idxs load = inputs as_buf load idxs te ifThenElse te ExprHandle isnan load te ExprHandle float load te Compute custom_nan_to_num out_shape compute kernel = te TensorExprKernel graph aten nan_to_num my_custom_lowering res = kernel run x res = kernel fallback x correct = f x np testing assert_allclose res numpy correct numpy atol= e- np testing assert_allclose res numpy correct numpy atol= e- unittest skipIf LLVM_ENABLED LLVM backend enabled test_kernel_with_expand f expand device = cpu x = torch rand device=device graph_str = graph Float strides= requires_grad= device=cpu int = prim Constant value= int = prim Constant value= int = prim Constant value= int = prim ListConstruct bool = prim Constant value= Float strides= requires_grad= device=cpu = aten expand graph = torch _C parse_ir graph_str kernel = te TensorExprKernel graph res = kernel run x res = kernel fallback x correct = f x np testing assert_allclose res numpy correct numpy atol= e- np testing assert_allclose res numpy correct numpy atol= e- unittest skipIf LLVM_ENABLED LLVM backend enabled test_alloc_in_loop tmp b = te BufHandle name torch float name tmp b body = te Block tmp store load b store tmp load _ range i = te VarHandle i torch int body = te For make i body nest = te LoopNest body b nest prepare_for_codegen f = te construct_codegen llvm nest simplify b ta tb = torch ones _ range f call ta data_ptr tb data_ptr TestExprHandlePyBind JitTestCase test_unary_ops unary_operators = torch sin torch _C _te sin torch cos torch _C _te cos torch tan torch _C _te tan torch asin torch _C _te asin torch acos torch _C _te acos torch atan torch _C _te atan torch sinh torch _C _te sinh torch cosh torch _C _te cosh torch tanh torch _C _te tanh torch sigmoid torch _C _te sigmoid torch exp torch _C _te exp torch expm torch _C _te expm torch abs torch _C _te abs torch log torch _C _te log torch log torch _C _te log torch log torch _C _te log torch log p torch _C _te log p torch erf torch _C _te erf torch erfc torch _C _te erfc torch sqrt torch _C _te sqrt torch rsqrt torch _C _te rsqrt torch ceil torch _C _te ceil torch floor torch _C _te floor torch round torch _C _te round torch trunc torch _C _te trunc torch lgamma torch _C _te lgamma torch frac torch _C _te frac construct_te_fn op n int dtype=torch float A = torch _C _te BufHandle A n dtype compute i op A load i C = te Compute C n compute loopnest = te LoopNest C loopnest prepare_for_codegen stmt = te simplify loopnest root_stmt te construct_codegen ir_eval stmt A C n = = torch rand n torch_op te_op unary_operators items ref = torch_op te_fn = construct_te_fn te_op n torch float res = torch empty n te_fn call res assert torch allclose ref res atol= e- rtol= e- __name__ == __main__ run_tests