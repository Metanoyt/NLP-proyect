mypy allow-untyped-defs warnings weakref collections abc Callable typing Optional torch torch autograd graph register_multi_grad_hook torch nn modules module register_module_forward_hook register_module_forward_pre_hook torch utils _pytree tree_flatten __all__ = ModTracker ModTracker ` ` ModTracker ` ` context manager tracks nn Module hierarchy during execution so other system can query which Module currently being executed its backward being executed You can access ` ` parents ` ` attribute context manager get set all Modules currently being executed via their fqn fully qualified name also used key within state_dict You can access ` ` is_bw ` ` attribute know you currently running backward Note ` ` parents ` ` never empty always contains Global key The ` ` is_bw ` ` flag will remain ` ` True ` ` after forward until another Module executed If you need more accurate please submit issue requesting Adding map fqn module instance possible done yet please submit issue requesting you need Example usage code-block python mod = torch nn Linear ModTracker tracker Access anything during forward pass my_linear m m bias print f Current modules tracker parents torch mm m m t + bias torch nn functional linear = my_linear mod torch rand parents set str A Set containing fqn each module currently running their forward __init__ parents = Global _active_module_cnt = _known_modules weakref WeakKeyDictionary = weakref WeakKeyDictionary _seen_modules weakref WeakSet = weakref WeakSet _has_callback = False _post_bw_callbacks_to_enqueue list Callable = _user_pre_fw_hook = None _user_post_fw_hook = None _user_pre_bw_hook = None _user_post_bw_hook = None _maybe_set_engine_callback This assumes no concurrent calls backward _has_callback post_bw_callback reversed _post_bw_callbacks_to_enqueue torch autograd Variable _execution_engine queue_callback post_bw_callback _post_bw_callbacks_to_enqueue clear callback parents = Global _has_callback = False torch autograd Variable _execution_engine queue_callback callback _has_callback = True property is_bw A boolean marking currently running during backward pass torch _C _current_graph_task_id = - get_known_fqn mod Return fqn given module known ` ` ModTracker ` ` otherwise ` ` None ` ` _known_modules get mod None register_user_hooks pre_fw_hook Optional Callable = None post_fw_hook Optional Callable = None pre_bw_hook Optional Callable = None post_bw_hook Optional Callable = None Registers user-specified hooks called before after forward backward pass each module tracked ` ` ModTracker ` ` One more can ` ` None ` ` Args pre_fw_hook Callable optional A hook called before forward pass module It should have following signature pre_fw_hook module input - None post_fw_hook Callable optional A hook called after forward pass module It should have following signature post_fw_hook module input output - None pre_bw_hook Callable optional A multi-grad hook called all outputs module require gradients It should have following signature pre_bw_hook module grad_output - None post_bw_hook Callable optional A multi-grad hook called all inputs module require gradients It should have following signature post_bw_hook module grad_input - None Raises AssertionError If new hook provided when one already registered Note If module alive during backward pass pre_bw_hook post_bw_hook will will receive None module argument The module fqn will present ` ` parents ` ` attribute when each hooks called Hooks intended used markers only modify inputs outputs set_hook hook user_hook hook_name hook None user_hook None raise AssertionError f Only one hook_name can registered time f Clear existing hook calling ` ` clear_user_hooks ` ` before registering new one hook _user_pre_fw_hook = set_hook pre_fw_hook _user_pre_fw_hook pre_fw_hook _user_post_fw_hook = set_hook post_fw_hook _user_post_fw_hook post_fw_hook _user_pre_bw_hook = set_hook pre_bw_hook _user_pre_bw_hook pre_bw_hook _user_post_bw_hook = set_hook post_bw_hook _user_post_bw_hook post_bw_hook clear_user_hooks Clears user specified hooks registered ` ` register_user_hooks ` ` _user_pre_fw_hook = None _user_post_fw_hook = None _user_pre_bw_hook = None _user_post_bw_hook = None _get_mod_name mod mod _known_modules _known_modules mod = type mod __name__ mod_name = _known_modules mod mod _seen_modules name submod mod named_children _known_modules submod = f mod_name name _get_mod_name submod _seen_modules add mod mod_name _get_append_fn w_mod name is_bw fn args is_bw _maybe_set_engine_callback name parents is_bw custom_formatwarning msg category filename lineno line=None f filename lineno category __name__ msg \n pyrefly ignore bad-assignment warnings formatwarning = custom_formatwarning warnings warn The module hierarchy tracking maybe messed up Please file bug PyTorch case stacklevel= name parents _active_module_cnt name = parents add name _active_module_cnt name += _user_pre_bw_hook None is_bw _user_pre_bw_hook w_mod args fn _get_pop_fn w_mod name is_bw fn args _user_post_bw_hook None is_bw _user_post_bw_hook w_mod args name parents _active_module_cnt name -= _active_module_cnt name == parents remove name is_bw Due some input output requiring gradients we cannot enforce proper nesting backward raise RuntimeError The Module hierarchy tracking wrong Report bug PyTorch fn _fw_pre_hook mod input name = _get_mod_name mod w_mod = weakref ref mod _get_append_fn w_mod name False _user_pre_fw_hook None _user_pre_fw_hook mod input args _ = tree_flatten input tensors = args isinstance torch Tensor requires_grad is_bw tensors register_multi_grad_hook tensors _get_pop_fn w_mod name True _post_bw_callbacks_to_enqueue append _get_pop_fn w_mod name True _fw_post_hook mod input output name = _get_mod_name mod w_mod = weakref ref mod _user_post_fw_hook None _user_post_fw_hook mod input output _get_pop_fn w_mod name False args _ = tree_flatten output tensors = args isinstance torch Tensor requires_grad is_bw tensors register_multi_grad_hook tensors _get_append_fn w_mod name True mode= any __enter__ _fw_pre_handle = register_module_forward_pre_hook _fw_pre_hook _fw_post_handle = register_module_forward_hook _fw_post_hook always_call=True __exit__ args _fw_pre_handle remove _fw_post_handle remove