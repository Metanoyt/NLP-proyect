mypy allow-untyped-decorators __future__ annotations atexit contextlib dataclasses functools logging math os threading traceback types typing weakref collections defaultdict dataclasses dataclass typing Any cast Literal Optional TYPE_CHECKING TypeGuard TypeVar Union typing_extensions Self weakref ReferenceType torch torch _library utils library_utils torch SymBool SymFloat SymInt Tensor torch _C _functorch is_functorch_wrapped_tensor is_legacy_batchedtensor torch _library fake_class_registry FakeScriptObject torch _library fake_profile MissingOpProfile torch _logging dtrace_structured torch _prims_common suggest_memory_format torch _subclasses meta_utils assert_eq assert_metadata_eq is_sparse_any is_sparse_compressed MetaConverter torch _utils render_call torch fx immutable_collections immutable_dict torch fx operator_schemas normalize_function torch multiprocessing reductions StorageWeakRef torch overrides TorchFunctionMode torch types IntLikeType py_sym_types torch utils _mode_utils no_dispatch torch utils _python_dispatch is_traceable_wrapper_subclass TorchDispatchMode torch utils _pytree KeyPath keystr PyTree tree_map tree_map_ TreeSpec torch utils _stats count torch utils _traceback CapturedTraceback _fake_tensor_utils _CacheKeyState _PySymInputStub _SymIntOutputStub TYPE_CHECKING collections abc Callable Generator Iterable Mapping Sequence types TracebackType torch _guards Source torch _ops OpOverload torch fx experimental symbolic_shapes ShapeEnv SymbolicContext log = logging getLogger __name__ hc_log = torch _logging getArtifactLogger __name__ hierarchical_compile TODO Hack unblock https github com pytorch pytorch pull Proper fix tracked https github com pytorch pytorch issues try not_implemented_log = torch _logging getArtifactLogger __name__ not_implemented except ValueError e not_implemented registered str e not_implemented_log = logging getLogger __name__ + not_implemented raise e DimList = list pytree = torch utils _pytree T = TypeVar T aten = torch _ops ops aten CONSTANT_NUMEL_LIMIT = RECURSION_COUNT = Small helper increments recursion count resets when object goes out scope Useful you don t want increase indentation which what context manager would do IncrementRecursionCount __init__ - None global RECURSION_COUNT RECURSION_COUNT += __del__ - None global RECURSION_COUNT RECURSION_COUNT -= dataclass UnsupportedFakeTensorException RuntimeError reason str dataclass DynamicOutputShapeException RuntimeError func OpOverload dataclass DataDependentOutputException RuntimeError func OpOverload dataclass UnsupportedOperatorException RuntimeError func OpOverload dataclass UnsupportedMutationAliasingException RuntimeError reason str dataclass MetadataMismatchError RuntimeError reason str FakeTensorTLS threading local Default None otherwise ll used override _all_ ` FakeTensorMode allow_non_fake_inputs ` thread allow_non_fake_inputs_override Optional bool non_strict_export_fake_tensor_tracker weakref WeakSet __init__ - None allow_non_fake_inputs_override = None non_strict_export_fake_tensor_tracker = weakref WeakSet fake_tensor_tls = FakeTensorTLS ordered_set items T - dict T Literal True dict fromkeys items True contextlib contextmanager unset_fake_temporarily - Generator Optional TorchDispatchMode None None old = torch _C _unset_dispatch_mode torch _C _TorchDispatchModeKey FAKE try yield old finally old None torch _C _set_dispatch_mode old contextlib contextmanager disable_fake_tensor_cache fake_mode FakeTensorMode - Generator None None None old_value bool = fake_mode cache_enabled try fake_mode cache_enabled = False yield finally fake_mode cache_enabled = old_value get_plain_tensors subclass Tensor out list Union Tensor int SymInt - list Union Tensor int SymInt This function used Runtime do add redundant asserts todo = subclass while todo curr = todo pop is_traceable_wrapper_subclass curr out append curr continue inner_keys _ = curr __tensor_flatten__ todo extend getattr curr key key reversed inner_keys out is_fake x object - TypeGuard Tensor torch _subclasses functional_tensor FunctionalTensor isinstance x FakeTensor True is_traceable_wrapper_subclass x attrs _ = type x __tensor_flatten__ x flattened_tensors = getattr x attr attr attrs all_fake = all is_fake x x flattened_tensors any_fake = any is_fake x x flattened_tensors assert all_fake == any_fake got mixed fake real tensors all_fake isinstance x FunctionalTensor is_fake x elem isinstance x Tensor torch _is_functional_tensor x reapply_views = torch _C _functionalization_reapply_views_tls unwrapped = torch _C _functorch _unwrap_functional_tensor x reapply_views is_fake unwrapped isinstance x Tensor is_functorch_wrapped_tensor x unwrapped = torch _C _functorch get_unwrapped x is_fake unwrapped False maybe_get_fake_mode t object - Optional FakeTensorMode torch _subclasses functional_tensor FunctionalTensor isinstance t FakeTensor t fake_mode is_traceable_wrapper_subclass t inner_tensor_names _ = t __tensor_flatten__ modes = maybe_get_fake_mode getattr t t_name t_name inner_tensor_names m = modes assert all m x x modes m isinstance t FunctionalTensor maybe_get_fake_mode t elem isinstance t Tensor torch _is_functional_tensor t reapply_views = torch _C _functionalization_reapply_views_tls unwrapped = torch _C _functorch _unwrap_functional_tensor t reapply_views maybe_get_fake_mode unwrapped isinstance t Tensor is_functorch_wrapped_tensor t unwrapped = torch _C _functorch get_unwrapped t maybe_get_fake_mode unwrapped None functools cache get_schema_info func OpOverload - torch _C _SchemaInfo torch _C _SchemaInfo func _schema many decompositions registered torch _prims do moment model aliasing strides so incremental step just enable decompositions torch _decomp decompositions py decomps used aot autograd tracing so we would like unify their implementation add additional testing them functools cache torch_decomp_decompositions func OpOverload - bool torch _decomp decomposition_table decompositions = torch _decomp decompositions Note function decomposition table might different one module because difference out handling aten API torch public API decomposition_table func __module__ startswith torch _decomp decomposition_table func __name__ dir decompositions tree_flatten_only ty type T tree PyTree - list T flat_vals = pytree tree_leaves tree elem elem flat_vals isinstance elem ty _is_plain_tensor t object - bool type t Tensor t layout == torch strided t is_sparse t is_nested is_functorch_wrapped_tensor t is_legacy_batchedtensor t torch _is_functional_tensor t Similar ` MetaConverter ` converting multiple tensors into fake tensors which share same view storage structure Like ` MetaConverter ` uses ` WeakIdRef ` hold weak reference all memoized tensors FakeTensorConverter property tensor_memo - weakref WeakValueDictionary valid until py weakref WeakValueDictionary torch _subclasses meta_utils MetaTensorId Optional FakeTensor meta_converter tensor_memo meta_converter MetaConverter constant_storage_mapping dict StorageWeakRef list ReferenceType export bool __init__ copy_data bool = False export bool = False - None meta_converter = MetaConverter copy_data=copy_data export = export map storage corresponding constant tensors constant_storage_mapping = add_constant_storage_mapping fake_tensor FakeTensor - None when you have constant aliased tensor const_tensor add_ torch rand all aliases must become no longer const assert isinstance fake_tensor FakeTensor fake_tensor constant None weak_st = StorageWeakRef fake_tensor constant _typed_storage we need map weak storage all its corresponding constant tensors python doesn t have weak value equivalent defaultdict list so we using WeakValueDictionary one weak_st constant_storage_mapping constant_storage_mapping weak_st = constant_storage_mapping weak_st append weakref ref fake_tensor invalidate_constant_aliases tensor Tensor - None assert isinstance tensor FakeTensor weak_st = StorageWeakRef tensor _typed_storage weak_st constant_storage_mapping weak_tensor_ref constant_storage_mapping weak_st ten = weak_tensor_ref ten None ten _fix_weakref ten constant = None del constant_storage_mapping weak_st _get_memo t Tensor - Optional FakeTensor tid = meta_converter describer lookup_tensor get t tid None None tensor_memo get tid set_tensor_memo t Tensor v FakeTensor - None tid = meta_converter describer get_tensor_id t meta_converter tensor_memo tid = v You can have real tensor you need convert into fake tensor If you have meta tensor already call from_meta_and_device You re allowed pass meta tensor turned into fake tensor although odd thing do can occur you re doing cross ref testing inner test already operating meta tensors from_real_tensor fake_mode FakeTensorMode t Tensor make_constant bool = False shape_env Optional ShapeEnv = None source Optional Source = None symbolic_context Optional SymbolicContext = None trace bool = True - FakeTensor see note Tensor Fakification Symbol Caching symbolic_context source shape_env tracing_context = torch _guards TracingContext try_get t tracing_context tensor_to_context symbolic_context = tracing_context tensor_to_context t torch fx experimental symbolic_shapes StatefulSymbolicContext assert isinstance symbolic_context StatefulSymbolicContext source = symbolic_context tensor_source maybe_memo = _get_memo t maybe_memo None maybe_memo yet supported metatensors t is_quantized raise UnsupportedFakeTensorException quantized nyi meta tensors type t torch nn Parameter assert make_constant constant = t make_constant None This callback used both subclass inner tensors Require caller explicitly specify device case outer inner tensors have different devices mk_fake_tensor make_meta_t Callable object device Union torch device str - FakeTensor NB don t use in_kernel_invocation_manager ensure FakeTensor can internally do constant computation necessary Invocation manager more correct works more operators make_meta_t invariant make_meta_t only calls factories which strictly necessary use invocation manager I think no_dispatch FakeTensor fake_mode pyrefly ignore bad-argument-type make_meta_t pyrefly ignore bad-argument-type device TODO callback might used recursive contexts which case using t wrong BUG constant=constant out = meta_converter t shape_env=shape_env callback=mk_fake_tensor source=source symbolic_context=symbolic_context trace=trace out NotImplemented raise UnsupportedFakeTensorException meta converter nyi torch _dynamo source RandomValueSource value = None export _is_plain_tensor t mostly we want know item works t dim == t device type == cpu All integer types fair game because signed overflow UB even int can overflow since integers Python arbitrary precision But only float OK float because switching between float float changes semantics observable way without hitting UB t dtype torch int torch int torch int torch int torch float source None Impede setting up item things coming random These real item calls instead UnspecializedPythonVariable unsafely pretending int tensor which can sometimes implicitly cause item call The problem pretty unsound there s no reason substituting int Tensor going give same results Today you mostly get around typically having capture_scalar_outputs graph breaking when someone tries use unspec variable int-y context But allowing through here would break So don t Once random values setup represented SymNodeVariable condition can removed To check you ve done right good test PYTORCH_TEST_WITH_DYNAMO= python test test_reductions py -k TestReductionsCPU test_dim_reduction_fns_fn_name_amax_cpu_bfloat isinstance source RandomValueSource In Dynamo shape_env never none even static shapes However FakeTensorMode can used hand some cases ShapeEnv allocated shape_env None torch _dynamo source CallMethodItemSource FloatTensorSource torch fx experimental symbolic_shapes DimDynamic no_dispatch value = t item math isnan value math isinf value Peephole strip out unnecessary torch as_tensor x item isinstance source FloatTensorSource item_source = source base item_source = CallMethodItemSource source symbol = shape_env create_unspecified_symbol value source=item_source dynamic_dim=DimDynamic DYNAMIC symbolic_context=symbolic_context NB reusing item_memo here ensures we invalidate mutation t dtype == torch int out item_memo = shape_env create_symintnode symbol hint=value source=item_source t dtype == torch float out item_memo = shape_env create_symfloatnode symbol hint=value source=item_source make_constant add_constant_storage_mapping out NB meta_converter set memo out If you specify device MUST meta tensor from_meta_and_device fake_mode FakeTensorMode t Tensor device torch device pytype Optional type torch Tensor = None dispatch_keys Optional torch DispatchKeySet = None - FakeTensor assert t device type == meta f tensor s device must ` meta ` got t device type instead This bit abusive real tensor whatever meta tensor should fresh so there s no way get wrong maybe_memo = _get_memo t maybe_memo None maybe_memo out = FakeTensor fake_mode t device pytype=pytype dispatch_keys=dispatch_keys set_tensor_memo t out out functools cache init_gpu_context device torch device - None Backward will error cuda Fake Tensors no cuda tensors have been initialized first torch cuda is_available torch xpu is_available torch empty device=device torch version hip None torch zeros device=device contextlib contextmanager in_kernel_invocation_manager fake_mode FakeTensorMode - Generator None None None See note Fake Tensor Dispatch Keys prev_in_kernel = fake_mode in_kernel_invocation meta_in_tls = torch _C _meta_in_tls_dispatch_include assert meta_in_tls == prev_in_kernel f meta_in_tls prev_in_kernel torch _C _DisableTorchDispatch fake_mode in_kernel_invocation = True Unfortunately _set_meta_in_tls_dispatch_include False can leave ` Dense ` turned because s implied ` Meta ` torch _C _PreserveDispatchKeyGuard torch _C _set_meta_in_tls_dispatch_include True try yield finally fake_mode in_kernel_invocation = prev_in_kernel torch _C _set_meta_in_tls_dispatch_include prev_in_kernel Return function allows Python numbers bind Tensors should_allow_numbers_as_tensors func OpOverload - bool torch _C _should_allow_numbers_as_tensors func name split - split FakeTensorConfig debug = os environ get TORCH_FAKE_TENSOR_DEBUG == This memorizes unbacked SymInt SymFloats representing quantities like number nonzero elements tensor learning rate There one instance descriptor per particular quantity memoize Memoization helpful you do something like x mask y mask mask nonzero gets repeatedly called should give consistent unbacked SymInt It needs invalidated same way constant Making descriptor may seem overly fancy actually s most convenient way ensure access FakeTensor during access which required testing version counter epoch validity SymNumberMemoDescriptor _name str By default SymInts memo invalidated across versions epochs nested_ints however preserved across epochs across versions Preserving across versions okay nested int since association nested int agnostic underlying data nested ints shared across multiple distinct tensors _is_nested_int bool __init__ is_nested_int bool = False - None _is_nested_int = is_nested_int __set_name__ owner str name str - None _name = name _memo obj FakeTensor - str f _ _name _memo_vc obj FakeTensor - str f _ _name _vc When we retrace we need invalidate all memos so we can accurately identify first time unbacked SymInts allocated This only relevant inputs intermediates they will get fresh fake tensors so you won t have memo anyway _memo_epoch obj FakeTensor - str f _ _name _epoch __get__ obj FakeTensor objtype Optional type FakeTensor = None - Optional Union torch SymInt torch SymFloat r = getattr obj _memo obj None None If backed s ok preserve memo since we know won t renumber isinstance r torch SymFloat r node hint None r Version counter based tracking isn t sound s close enough _is_nested_int getattr obj _memo_vc obj = obj _version _is_nested_int getattr obj _memo_epoch obj = obj fake_mode epoch setattr obj _memo obj None None r __set__ obj FakeTensor value Optional Union torch SymInt torch SymFloat - None value None setattr obj _memo obj None setattr obj _memo_vc obj None setattr obj _memo_epoch obj None obj is_inference _is_nested_int setattr obj _memo obj value _is_nested_int setattr obj _memo_vc obj obj _version setattr obj _memo_epoch obj obj fake_mode epoch FakeTensor Tensor Meta tensors give you ability run PyTorch code without having actually do computation through tensors allocated ` meta ` device Because device ` meta ` meta tensors do model device propagation FakeTensor extends MetaTensors also carry additional ` fake_device ` which tracks devices would have been used fake_device torch device fake_mode FakeTensorMode constant Optional Tensor real_tensor Optional Tensor TODO Generalize needed e g into trie memos you do something like x item x fresh each time so memo mechanism here won t work nonzero_memo = SymNumberMemoDescriptor item_memo = SymNumberMemoDescriptor unique_memo = SymNumberMemoDescriptor unique_consecutive_memo = SymNumberMemoDescriptor We expect nested_int_memo None when offsets graph intermediate input has never been associated nested int nested_int_memo = SymNumberMemoDescriptor is_nested_int=True FakeTensor doesn t fully emulate original tensor s Python type dispatch key set therefore sometimes we want track them separately pytype Optional type Tensor dispatch_keys Optional torch DispatchKeySet Indicates our torch_dispatch dispatching infra infra mode lower dispatching precedence _mode_key = torch _C _TorchDispatchModeKey FAKE property pyrefly ignore bad-override device - torch device fake_mode in_kernel_invocation torch device meta fake_device device setter device _ torch device - None raise NotImplementedError Note Fake Tensor Dispatch Keys In order model behavior device-specific autocast autograd logic we update dispatch keys FakeTensors reflect their fake device This includes BackendComponent DispatchKey Meta - DispatchKey CUDA also BackendComponent related Autocast Autograd keys __torch_dispatch__ sits below Autocast Autograd only invoked when we kernel BackendComponent Then we add Meta thread-local dispatch include set hit meta kernel instead kernel BackendComponent fake device The ` device_for_backend_keys ` does below NOTE probably will do right thing backends have dispatch keys which higher than meta key https github com pytorch pytorch blob main c core DispatchKey h#L We don t support named tensors graph break property pyrefly ignore bad-override names - list str raise UnsupportedFakeTensorException torch compile doesn t support named tensors names setter names _ list str - None raise NotImplementedError staticmethod __new__ cls fake_mode FakeTensorMode elem Tensor device torch device constant Optional Tensor = None real_tensor Optional Tensor = None pytype Optional type Tensor = None dispatch_keys Optional torch DispatchKeySet = None - Self = Tensor _make_subclass cls elem elem requires_grad dispatch_device=True device_for_backend_keys=device fake_mode _allow_unsafe_data_ptr_access torch _C _set_throw_on_mutable_data_ptr torch _C _set_warn_deprecated_on_mutable_data_ptr assert elem device type == meta elem device type device = device isinstance device torch device torch device device NB fine little confusing device meta we faking meta tensor case However often indicates some sort confusion e g you accidentally passed meta tensor when you should have passed real tensor So default we disallow meta you working situation where helpful e g crossref testing you can turn back fake_mode allow_meta assert device type = meta normalize device device type cuda xpu init_gpu_context device device type cuda hpu xpu mps torch _C _get_privateuse _backend_name device index None device type = mps getattr torch device type is_initialized device = torch device f device type getattr torch device type current_device device = torch device f device type pyrefly ignore read-only fake_device = device fake_mode = fake_mode constant = constant pytype = pytype dispatch_keys = dispatch_keys assert isinstance real_tensor FakeTensor real_tensor = real_tensor nonzero_memo = None item_memo = None unique_memo = None unique_consecutive_memo = None nested_int_memo = None FakeTensorConfig debug _debug_trace = CapturedTraceback extract type ignore attr-defined In some circumstances conventional Tensor constructor will get rewritten call into FakeTensor We must provide __init__ method can accept Python interpreters initialization such situation we must also able handle direct fake tensor construction via FakeTensor In particular __init__ call will look funny following case FakeTensorMode x = Tensor desugars into FakeTensorMode x = Tensor __new__ NB x fake tensor because mode x __init__ normal fake tensor args __init__ args object kwargs object - None super __init__ torch compiler is_exporting torch _export config detect_non_strict_fake_tensor_leaks fake_tensor_tls non_strict_export_fake_tensor_tracker add staticmethod from_tensor t Tensor fake_mode FakeTensorMode - FakeTensor fake_mode from_tensor t classmethod count __torch_dispatch__ type ignore override TODO cls func OpOverload types Sequence type args Sequence object = kwargs Mapping str object = immutable_dict - object need handle here avoid infinite recursion see in_kernel_invocation func torch ops prim device default assert len args == isinstance args FakeTensor args fake_mode in_kernel_invocation torch device meta args fake_device handler must done inside FakeTensor subclass mode because we can end up dispatching here when we have fake tensor symbolic sizes running under in_kernel_invocation_manager The subclass asked handle query because size sym_size called we unable serve directly because there symbolic sizes The use in_kernel_invocation_manager means s incorrect activate mode actually handle caused https github com pytorch pytorch issues handler = _DISPATCH_META_HANDLERS get func handler args Because fake mode can NotImplemented sees subclass doesn t know how deal test here important because next dispatch after fake mode will attempt use subclasses tensors dispatch any FakeTensor arguments will considered eligible unrecognized_types = t t types issubclass t FakeTensor t Tensor unrecognized_types not_implemented_log debug FakeTensor unrecognized subclass es s unrecognized_types NotImplemented fake_mode = None arg pytree arg_tree_leaves args kwargs isinstance arg FakeTensor fake_mode = arg fake_mode break assert fake_mode None If fake mode already active don t try reapply NotImplemented right thing here because typical situation can occur ProxyTensorMode returned NotImplemented because implemented subclass we may have unluckily attempted hit FakeTensor s dispatch first NotImplemented lets us keep chaining until we find actual subclass maybe_cur_fake_mode = torch _C _get_dispatch_mode torch _C _TorchDispatchModeKey FAKE maybe_cur_fake_mode not_implemented_log debug FakeTensor mode already active s s fake_mode maybe_cur_fake_mode NotImplemented assert fake_mode in_kernel_invocation fake_mode func args kwargs staticmethod _find_common_device func OpOverload flat_args Sequence object - tuple torch device bool Returns common_device has_scalar_only_inputs cpu - zero-dim tensors can called cuda kernels so overwrite common_device only existing device comes cpu zero-dim tensor common_device = None has_scalar_only_inputs = False is_cpu_zero_dim = None list ops which can have args tensor tensorList mixed device mixed_device_fns = ordered_set aten _foreach_copy default list ops using zero dim cpu tensor logic align eager mode bypass_zero_dim_cpu_tensor_check_ops = ordered_set aten nextafter default check_cpu_device device torch device - bool device type == cpu cpu_zero_dim t Tensor - bool check_cpu_device t device t dim == merge_devices t object - None nonlocal common_device nonlocal is_cpu_zero_dim isinstance t FakeTensor common_device None common_device = t device is_cpu_zero_dim = cpu_zero_dim t t_is_cpu_zero_dim = cpu_zero_dim t t device == common_device is_cpu_zero_dim is_cpu_zero_dim = t_is_cpu_zero_dim is_bypass_zero_dim_cpu_tensor_check_op = func bypass_zero_dim_cpu_tensor_check_ops mismatching devices current tensor cpu dim defer existing device t_is_cpu_zero_dim is_bypass_zero_dim_cpu_tensor_check_op current device cpu dim tensor overwrite is_cpu_zero_dim is_bypass_zero_dim_cpu_tensor_check_op common_device = t device is_cpu_zero_dim = t_is_cpu_zero_dim still device mismatches we will check ops which can work different devices ex _foreach_copy one device must cpu case we will here without throwing error func mixed_device_fns any map check_cpu_device common_device t device prefer_device_type set prefer device type over others prefer_device_type = torch _functorch config fake_tensor_prefer_device_type prefer_device_type None common_has_preferred = prefer_device_type common_device type t_has_preferred = prefer_device_type t device type common_has_preferred t_has_preferred Switch preferred device type common_device = t device is_cpu_zero_dim = t_is_cpu_zero_dim common_has_preferred t_has_preferred Keep existing preferred device type mismatching devices non-zero dim tensors throw This might valid behavior need explicitly modeled e g reshape_as raise RuntimeError f Unhandled FakeTensor Device Propagation func found two different devices common_device t device arg flat_args merge_devices arg some functions allow Python numbers bind Tensors we have failed find device we re running one these operators we must have scalar only inputs should_allow_numbers_as_tensors func common_device None ops scalar only inputs always have result cpu has_scalar_only_inputs = True common_device = torch device cpu assert common_device None f Could find common device func common_device has_scalar_only_inputs get_nested_int coeff Union int torch SymInt = - torch SymInt nested_int_memo None nested_int_memo = fake_mode create_symbolic_nested_int nt_tensor_id=None assert isinstance nested_int_memo torch SymInt nested_int_memo coeff Similar FunctionalTensor tolist tolist - Any dim == item dim == elem item elem elem tolist elem _MetadataIntLike = Union IntLikeType _PySymInputStub _SymIntOutputStub dataclass slots=True TensorMetadata The Tensor metadata relevant hashing FakeTensors when caching dtype torch dtype shape tuple _MetadataIntLike stride tuple _MetadataIntLike device torch device layout torch layout memory_format Optional torch memory_format storage_offset _MetadataIntLike storage_bytes Optional _MetadataIntLike requires_grad bool is_quantized bool is_conj bool is_neg bool is_inference bool is_sparse bool read sparse COO is_coalesced Optional bool dense_dim Optional int sparse_dim Optional int _flatten_into result list object mode FakeTensorMode state _CacheKeyState - None Flatten TensorMetadata out into ` result ` Make sure call state convert_sym_int any SymInts field dataclasses fields value = getattr field name isinstance value tuple list torch Size This will recursively flatten iterable calling convert_sym_int necessary id_hashed_objects list object = mode _prep_args_for_hash result value state id_hashed_objects id_hashed_objects clear isinstance value SymInt state convert_sym_int result value result append value extract_tensor_metadata t Tensor - TensorMetadata Extract TensorMetadata tensor memory_format = suggest_memory_format t Don t call is_contiguous Tensor which has symbolic sizes things will go badly guards will messed up t _has_symbolic_sizes_strides is_sparse_any t t is_contiguous memory_format=memory_format memory_format = None type ignore assignment storage_offset = t storage_offset TensorMetadata t dtype t shape t stride t layout == torch strided t device t layout memory_format storage_offset Only set storage_bytes tensors have storage sparse t untyped_storage nbytes is_sparse_any t None t requires_grad t is_quantized t is_conj t is_neg t is_inference t is_sparse t is_coalesced t is_sparse None t dense_dim is_sparse_any t None t sparse_dim is_sparse_any t None dataclass slots=True _DispatchCacheKey Key FakeTensor dispatch cache key tuple object hashvalue int __init__ tup tuple object - None key = tup hashvalue = hash tup __eq__ other object - bool isinstance other _DispatchCacheKey key == other key __hash__ - int hashvalue strip_shape_env - None We need strip ShapeEnv any values before we store cache so cache doesn t keep our ShapeEnvs alive v key isinstance v _PySymInputStub v strip_shape_env Default value constant_value _DispatchCacheEntryOutputInfo This only checking differentiates None SingletonConstant pass dataclass frozen=True slots=True _DispatchCacheEntryOutputInfo Entry type FakeTensor dispatch cache output Accounts three possibilities The op inplace hit means we need alias argument given index We need synthesize new FakeTensor given tensor metadata For view ops we further capture index arg alias tensor related fields None then constant value e g None integer inplace_idx Optional int metadata Optional TensorMetadata view_idx Optional int constant_value Optional Any = SingletonConstant dataclass frozen=True slots=True _DispatchCacheValidEntry Entry type FakeTensor dispatch cache It supports two types outputs tensor tuple tensors is_output_tuple flag helps differentiating type output_infos tuple _DispatchCacheEntryOutputInfo is_output_tuple bool = False dataclass frozen=True slots=True _DispatchCacheBypassEntry Entry type negative cache entry reason str TYPE_CHECKING _DispatchCacheEntry = Union _DispatchCacheValidEntry _DispatchCacheBypassEntry dataclass frozen=True slots=True _BypassDispatchCache Exception Signals cases should skip FakeTensor caching reason str dataclass frozen=True slots=True DispatchCacheInfo Information about state FakeTensor dispatch cache hits int misses int bypasses dict str int size int We keep one instantiation ` fake_tensor_converter ` active duration ` FakeTensorMode ` This allows accurate storage aliasing across invocation different operators While will keep all freshly allocated tensors alive during ` FakeTensorMode ` there will no new allocations Tensors which have non-meta storage so memory should significantly increase FakeTensorMode TorchDispatchMode cache dict _DispatchCacheKey _DispatchCacheEntry = cache_hits int = cache_misses int = cache_bypasses dict str int = defaultdict int Every time you retrace using same fake tensor mode you should advance epoch so we don t reuse unbacked memos epoch int = in_kernel_invocation bool = False static_shapes bool shape_env Optional ShapeEnv _stack Optional str allow_meta bool NestedTensor uses tensor_id_counter uniquely identify offsets This counter incremented when offsets used create NJT first time To avoid mutating eager state we construct NJT during tracing we maintain separate counter FakeTensorMode The initial count set current eager tensor_id_counter value upon initialization every time you retrace using same fake tensor mode you should reset counter initial count nt_tensor_id_counter int = - nt_tensor_id_initial_count int = - __init__ allow_fallback_kernels bool = True allow_non_fake_inputs bool = False shape_env Optional ShapeEnv = None static_shapes Optional bool = None TODO This temporary measure see https github com pytorch pytorch pull #discussion_r We re currently solely using impede population item_memo d scalar tensor inputs when export because causes things used deferred runtime asserts turn into guards then guards just lost We can potentially fix ensuring guards also get put graph pending rework how deferred runtime asserts export Once s done we can remove export bool = False - None log debug create_mode x x id super __init__ allow_fallback_kernels = allow_fallback_kernels torch _dynamo config torch _functorch config propagate_real_tensors = torch _functorch config fake_tensor_propagate_real_tensors fake_tensor_converter = FakeTensorConverter copy_data=self propagate_real_tensors export=export static_shapes None static_shapes = static_shapes static_shapes = shape_env None This temporarily patched True Dynamo grandfather some places where we unconditionally allow scalar outputs TO BE REMOVED allow_scalar_outputs = False _allow_unsafe_data_ptr_access = torch _functorch config fake_tensor_allow_unsafe_data_ptr_access allow_meta = torch _functorch config fake_tensor_allow_meta cache_enabled bool = torch _dynamo config fake_tensor_cache_enabled propagate_real_tensors cache_crosscheck_enabled = torch _dynamo config fake_tensor_cache_crosscheck_enabled A flag controls whether we want invoke ops mix real weights global variables fake inputs allow_non_fake_inputs = allow_non_fake_inputs in_kernel_invocation when FakeTensor invoked user code device should fake_device tensor so code such ` x is_cuda ` torch zeros device=x device continues execute FakeTensor real However within kernel execution we ` Meta ` device because all computation within kernels should behave Tensors meta devices Kernels should allocate new tensors meta devices checks like ` is_meta ` should true within python refs we always real device defining device property in_kernel_invocation = False True we enter ed actually enabled fake tensor mode false no-op Not thread safe neither in_kernel_invocation If another fake mode already active when we enter we also stash here That way when we exit we know re-enable previous fake mode enter_stack list tuple bool Optional TorchDispatchMode Optional bool = shape_env = shape_env _stack_trace = traceback extract_stack _stack = None Indicates our torch_dispatch dispatching infra infra mode lower dispatching precedence _mode_key = torch _C _TorchDispatchModeKey FAKE torch nested _internal nested_tensor nt_tensor_id_initial_count = torch nested _internal nested_tensor _tensor_id_counter nt_tensor_id_counter = nt_tensor_id_initial_count reset_nt_tensor_id_counter - None nt_tensor_id_counter = nt_tensor_id_initial_count Typically there only one fake tensor mode you test doing isinstance test However some situations there might TWO fake tensor modes The canonical example exporting fake model there outer fake mode created user inner fake mode created Dynamo The two phase process required because outer fake mode typically won t have ShapeEnv even user interested exporting dynamic shapes so inner fake mode will actually have ShapeEnv swap symbolic sizes In case s insufficient test only one FakeTensor you need distinguish between our fake tensor other fake tensors That s what function does is_our_fake t object - TypeGuard FakeTensor isinstance t FakeTensor t fake_mode If we should avoid device init This changes behavior various APIs - We avoid constant-prop Tensors ops move them another device - We change torch tensor ctor contract never materialize tensors device see NOTE torch tensor lift_fresh device movement property avoid_device_init - bool torch xpu _is_compiled assert torch cuda _is_compiled torch xpu is_available torch cuda is_available hasattr torch hpu torch hpu is_available property stack - str _stack None _stack = join traceback format_list _stack_trace _stack count pyrefly ignore bad-override __torch_dispatch__ func OpOverload types Sequence type args Sequence object = kwargs Mapping str object = immutable_dict - object FakeTensorMode should set when we re inside assert torch _C _get_dispatch_mode torch _C _TorchDispatchModeKey FAKE None func try dispatch func types args kwargs except TypeError log exception fake tensor raised TypeError raise No-op FakeTensorMode already use __enter__ - Self torch nested _internal nested_tensor prev_only_lift_cpu_tensors = None avoid_device_init See NOTE torch tensor lift_fresh device movement prev_only_lift_cpu_tensors = torch _C _only_lift_cpu_tensors torch _C _set_only_lift_cpu_tensors True In case CPU-only build cuda device unavailable we patch cuda device guard use NoOpDeviceGuardImpl This enables us trace over cuda kernels under FakeTensorMode torch _C _ensureCUDADeviceGuardSet maybe_prev_fake_mode = torch _C _unset_dispatch_mode _mode_key maybe_prev_fake_mode enter_stack append True maybe_prev_fake_mode prev_only_lift_cpu_tensors super __enter__ no-op still need re-set fake mode though since we unset torch _C _set_dispatch_mode enter_stack append False None prev_only_lift_cpu_tensors __exit__ Optional type BaseException b Optional BaseException c Optional TracebackType - None live maybe_prev_fake_mode maybe_prev_only_lift_cpu_tensors = enter_stack pop live super __exit__ b c Re-enable previous fake mode there one maybe_prev_fake_mode None torch _C _set_dispatch_mode maybe_prev_fake_mode maybe_prev_only_lift_cpu_tensors None torch _C _set_only_lift_cpu_tensors maybe_prev_only_lift_cpu_tensors classmethod is_infra_mode cls - bool True classmethod cache_info cls - DispatchCacheInfo Query state dispatch cache DispatchCacheInfo FakeTensorMode cache_hits FakeTensorMode cache_misses dict FakeTensorMode cache_bypasses len FakeTensorMode cache classmethod cache_clear cls - None Clear dispatch cache cls cache_hits = cls cache_misses = cls cache_bypasses clear cls cache clear _cached_dispatch_impl func OpOverload types Sequence type args Sequence object kwargs Mapping str object - object Lookup cache entry given arguments If none exists dispatch cache result result eligible caching state = None key = None try state = _CacheKeyState shape_env key = _cache_key state func args kwargs except _BypassDispatchCache e We couldn t create cache key all isinstance func torch _ops HigherOrderOperator func name == invoke_subgraph hc_log debug Fake tensor cache failed identifier = s reason = s args e reason FakeTensorMode cache_bypasses e reason += key None Do dispatch outside above except handler so generates its own exception there won t __context__ caused caching mechanism pyrefly ignore bad-argument-type _dispatch_impl func types args kwargs assert state None state cache_on_shape_env assert state shape_env None cache = state shape_env fake_tensor_cache set_cache_key = _set_cache_key_for_shape_env cache = FakeTensorMode cache set_cache_key = _set_cache_key entry = cache get key None entry None isinstance entry _DispatchCacheBypassEntry This represents negative cache entry - we already saw output uncachable Compute first principals FakeTensorMode cache_bypasses entry reason += pyrefly ignore bad-argument-type _dispatch_impl func types args kwargs We have cache entry pyrefly ignore bad-argument-type output = _output_from_cache_entry state entry key func args FakeTensorMode cache_hits += cache_crosscheck_enabled For debugging testing Validate output synthesized cache matches output created normal dispatch disable_fake_tensor_cache pyrefly ignore bad-argument-type _crosscheck_cache_output output func types args kwargs output We don t have cache entry pyrefly ignore bad-argument-type output = _dispatch_impl func types args kwargs try pyrefly ignore bad-argument-type entry = _make_cache_entry state key func args kwargs output except _BypassDispatchCache e We ran extra checks cache key determined s no good Record reason mark so we don t bother validating again isinstance func torch _ops HigherOrderOperator func name == invoke_subgraph hc_log debug Fake tensor cache failed identifier = s reason = s args e reason FakeTensorMode cache_bypasses e reason += set_cache_key cache key _DispatchCacheBypassEntry e reason output set_cache_key cache key entry FakeTensorMode cache_misses += output _cache_key state _CacheKeyState func OpOverload args Sequence object kwargs Mapping str object - _DispatchCacheKey Create cache key given dispatch args Raises _BypassDispatchCache any situation precludes caching is_tracing = torch fx experimental proxy_tensor get_proxy_mode None key_values = func Capture default_dtype mode since can affect output tensor e g when operating constant float values torch get_default_dtype Capture current device support e g cache tensor creation where there isn t necessarily tensor take device torch _C _get_default_device We want create tensors cached metadata only when inference mode same torch is_inference_mode_enabled Shape env settings could affect behavior One example seen wild Disallowing dynamic shapes can introduce DynamicOutputShapeException where wasn t seen previous instance same op shape_env settings shape_env None ProxyTorchDispatchMode needs track how SymNodes constructed so we need handle things little different depending whether we re tracing is_tracing state known_symbols If there symbols then include epoch - really more Shape env var which lives FakeTensorMode pyrefly ignore bad-argument-type key_values append epoch Collect id_hashed objects attach weakref finalize later id_hashed_objects list object = Translate any FakeTensor args metadata args pyrefly ignore bad-argument-type _prep_args_for_hash key_values args state id_hashed_objects kwargs pyrefly ignore bad-argument-type _prep_args_for_hash key_values kwargs state id_hashed_objects key = _DispatchCacheKey tuple key_values id_hashed_obj id_hashed_objects weakref finalize id_hashed_obj functools partial evict_fake_tensor_cache_key key=key id_hashed_objects clear key _validate_cache_key func OpOverload args Sequence object kwargs Mapping str object - None Validate cache key generated _cache_key will reasonable torch _higher_order_ops utils registered_hop_fake_fns For hops we perform validity check _make_cache_entry because we need have output tensor isinstance func torch _ops HigherOrderOperator func registered_hop_fake_fns Avoid caching any ops would require more sophisticated caching implementation e g data dependent ops ops modify inputs torch Tag data_dependent_output func tags raise _BypassDispatchCache data dependent output torch Tag dynamic_output_shape func tags func aten index Tensor _ new_kwargs = normalize_function type ignore misc func args=args type ignore arg-type kwargs=kwargs type ignore arg-type normalize_to_only_use_kwargs=True index new_kwargs indices index calls nonzero bool int tensors therefore has dynamic shape output For other dtypes output shape depends input shape data isinstance index torch Tensor index dtype torch bool torch int raise _BypassDispatchCache dynamic output shape raise _BypassDispatchCache dynamic output shape torch Tag inplace_view func tags raise _BypassDispatchCache inplace view func aten _unsafe_view default raise _BypassDispatchCache unsafe view func lift_fns raise _BypassDispatchCache lift func name == inductor resize_storage_bytes_ raise _BypassDispatchCache inductor resize_storage_bytes_ torch _library utils is_builtin func raise _BypassDispatchCache non-builtin In order handle storage aliasing we need establish alias any view op cache hit But CompositeImplicitAutograd ops may may alias input so just punt caching these func is_view torch _C _dispatch_has_kernel_for_dispatch_key func name torch _C DispatchKey CompositeImplicitAutograd raise _BypassDispatchCache CompositeImplicitAutograd _prep_args_for_hash result list object args Union Mapping str object Sequence object Iterable object state _CacheKeyState id_hashed_objects list object - None Translate provided args into form suitable caching FakeTensor dispatch i e convert unhashable types like lists dicts into tuples convert FakeTensors into metadata Raises _BypassDispatchCache signal unsupported cases should bypass caching torch _higher_order_ops auto_functionalize FunctionalCallableWithEpilogue torch _higher_order_ops utils FunctionalizeCtxWrapper isinstance args list tuple dict result append type args result append f length_ len args isinstance args dict _prep_args_for_hash result args keys state id_hashed_objects _prep_args_for_hash result args values state id_hashed_objects arg args isinstance arg FakeTensor is_our_fake arg raise _BypassDispatchCache our fake arg constant None raise _BypassDispatchCache constant attribute is_sparse_any arg raise _BypassDispatchCache f arg layout tensor metadata = extract_tensor_metadata arg metadata _flatten_into result state isinstance arg Tensor raise _BypassDispatchCache non-fake tensor isinstance arg SymInt state convert_sym_int result arg isinstance arg SymBool SymFloat raise _BypassDispatchCache symbolic shape isinstance arg list tuple dict _prep_args_for_hash result arg state id_hashed_objects isinstance arg types FunctionType raise _BypassDispatchCache function argument isinstance arg torch fx GraphModule This used invoke_subgraph where id graph_module allows us cache fake outputs result append type arg result append id arg id_hashed_objects append arg isinstance arg FunctionalizeCtxWrapper Special case AOT Dispatcher first pass where fake tensor called functional wrapper subgraph result append hash arg functional wrapper destroyed after fake tensor prop We need put finalizer subgraph id_hashed_objects append arg subgraph isinstance arg FunctionalCallableWithEpilogue result append type arg result append hash arg id_hashed_objects append arg orig_callable It s important capture type arg since e g hash same value can produce different dtypes output tensor result append type arg result append arg _validate_output_for_cache_entry state _CacheKeyState key _DispatchCacheKey func OpOverload args Sequence object kwargs Mapping str object output Optional FakeTensor - None Is even possible According signature can None ` int ` So either signature lie part line unnecessary isinstance output int type None Check symbolic content should bypass caching - raises _BypassDispatchCache necessary _validate_symbolic_output_for_caching state output Some ops tuples Tensors s rare so avoid complexity caching other types isinstance output FakeTensor raise _BypassDispatchCache non-FakeTensor output Avoid caching FakeTensors constants attached since those can invalidated output constant None raise _BypassDispatchCache constant attribute TODO support caching sparse outputs output is_sparse raise _BypassDispatchCache sparse output is_sparse_compressed output raise _BypassDispatchCache sparse compressed output Can in-place op really reference kwarg If so then we need extend implementation handle kval kwargs values id kval == id output raise _BypassDispatchCache kwarg aliases output _get_output_info_for_cache_entry state _CacheKeyState key _DispatchCacheKey func OpOverload args Sequence object kwargs Mapping str object output FakeTensor - _DispatchCacheEntryOutputInfo isinstance output int torch SymInt type None _DispatchCacheEntryOutputInfo inplace_idx=None metadata=None view_idx=None constant_value=output If in-place op entry records which input arg aliased idx range len args id args idx == id output _DispatchCacheEntryOutputInfo inplace_idx=idx metadata=None view_idx=None Otherwise create entry records output tensor s metadata view_idx = None isinstance func torch _ops OpOverload func is_view idxs = i i t enumerate args isinstance t Tensor assert len idxs == view_idx = idxs metadata = extract_tensor_metadata output metadata shape = tuple state convert_output v v metadata shape metadata stride = tuple state convert_output v v metadata stride metadata storage_offset = state convert_output metadata storage_offset metadata storage_bytes = None metadata storage_bytes None state convert_output metadata storage_bytes entry = _DispatchCacheEntryOutputInfo inplace_idx=None metadata=metadata view_idx=view_idx N B Some checks bypassing cache would performed output tensor synthesized cached metadata As optimization we can synthesize tensor here do checks instance This approach keeps more frequent cache-hit path lightweight possible entry_for_synth_output = _DispatchCacheValidEntry output_infos= entry is_output_tuple=False torch fx experimental symbolic_shapes GuardOnDataDependentSymNode try synth_output = _output_from_cache_entry state entry_for_synth_output key func args except GuardOnDataDependentSymNode This should probably never really happen If does means although original call didn t get data-dependent error when we tried reconstruct output we did - s almost certainly bug raise _BypassDispatchCache data dependent symnode None Make sure dispatch_key_set synthesized output tensor will same synth_key_set = torch _C _dispatch_key_set synth_output key_set = torch _C _dispatch_key_set output synth_key_set = key_set raise _BypassDispatchCache dispatch_key_set mismatch entry _make_cache_entry state _CacheKeyState key _DispatchCacheKey func OpOverload args Sequence object kwargs Mapping str object output Optional FakeTensor - _DispatchCacheValidEntry Make cache entry object given output Tensor Raises _BypassDispatchCache output tensor has characteristics prevent caching torch _higher_order_ops utils registered_hop_fake_fns torch fx experimental symbolic_shapes has_free_unbacked_symbols _validate_cache_key func args kwargs For hops lets look output tensor find any unbacked symints If there none then we rely existing checks validate caching NB Note HOPs sta alive till FakeTensor functional once they support mutations we will have revisit logic isinstance func torch _ops HigherOrderOperator func registered_hop_fake_fns assert isinstance output tuple non_cacheable = any isinstance o torch Tensor torch SymInt has_free_unbacked_symbols o o output non_cacheable raise _BypassDispatchCache f unbacked symbol HOP func output isinstance output int torch SymInt type None output_info = _DispatchCacheEntryOutputInfo inplace_idx=None metadata=None view_idx=None constant_value=output _DispatchCacheValidEntry output_infos= output_info is_output_tuple=False isinstance output tuple out_element output _validate_output_for_cache_entry state key pyrefly ignore bad-argument-type func args kwargs out_element _validate_output_for_cache_entry state key pyrefly ignore bad-argument-type func args kwargs output isinstance output tuple output_infos = _get_output_info_for_cache_entry state key pyrefly ignore bad-argument-type func args kwargs out_elem out_elem output _DispatchCacheValidEntry pyrefly ignore bad-argument-type output_infos=tuple output_infos is_output_tuple=True output_info = _get_output_info_for_cache_entry state key pyrefly ignore bad-argument-type func args kwargs output _DispatchCacheValidEntry output_infos= output_info is_output_tuple=False _get_output_tensor_from_cache_entry state _CacheKeyState entry _DispatchCacheEntryOutputInfo key _DispatchCacheKey func OpOverload args Sequence object - Optional FakeTensor entry inplace_idx None entry metadata None entry view_idx None assert entry constant_value SingletonConstant entry constant_value entry inplace_idx None This in-place op aliased arg inplace_arg = args entry inplace_idx assert isinstance inplace_arg FakeTensor inplace_arg Synthesize new FakeTensor cached metadata metadata = entry metadata metadata None None assert is_sparse_any metadata check_value value _MetadataIntLike state _CacheKeyState - Union IntLikeType isinstance value _SymIntOutputStub assert state shape_env None value extract key state shape_env assert isinstance value _PySymInputStub value shape = tuple check_value v state v metadata shape stride = tuple check_value v state v metadata stride storage_offset = check_value metadata storage_offset state metadata storage_bytes None check_value metadata storage_bytes state maybe_suppress Callable typing ContextManager = contextlib nullcontext shape_env None maybe_suppress = shape_env suppress_guards in_kernel_invocation_manager maybe_suppress empty = torch empty_strided shape stride dtype=metadata dtype layout=metadata layout device= meta requires_grad=metadata requires_grad metadata is_conj torch _C _set_conj empty True metadata is_neg torch _C _set_neg empty True isinstance func torch _ops OpOverload func is_view For view ops storage should same tensor input view_arg = args cast int entry view_idx assert isinstance view_arg FakeTensor storage = view_arg untyped_storage in_kernel_invocation_manager maybe_suppress empty set_ storage storage_offset shape stride FakeTensor empty metadata device _output_from_cache_entry state _CacheKeyState entry _DispatchCacheValidEntry key _DispatchCacheKey func OpOverload args Sequence object - Union Optional FakeTensor tuple Optional FakeTensor Create new FakeTensor cache entry entry is_output_tuple outputs = _get_output_tensor_from_cache_entry state output_info key func args output_info entry output_infos tuple outputs _get_output_tensor_from_cache_entry state entry output_infos key func args _crosscheck_cache_output output Union Optional FakeTensor tuple Optional FakeTensor func OpOverload types Sequence type args Sequence object kwargs Mapping str object - None Helper validate output synthesized cache matches output created normal dispatch assert_helper Any b Any - None isinstance tuple assert isinstance b tuple assert len == len b l r zip b assert_helper l r isinstance int assert isinstance b int == b None assert b None isinstance py_sym_types assert type type b node b node isinstance torch Tensor assert isinstance b torch Tensor assert_metadata_eq assert_eq b raise RuntimeError f Unsupported type type try true_output = _dispatch_impl func types args kwargs except Exception e raise RuntimeError f FakeTensor cache crosscheck failure func= func f args= args kwargs= kwargs Dispatch raised= e e try assert_helper true_output output except Exception e raise RuntimeError f FakeTensor cache crosscheck failure func= func f args= args kwargs= kwargs e dispatch func OpOverload types Sequence type args Sequence object = kwargs Mapping str object = immutable_dict - object kwargs = kwargs no_dispatch log debug s s s func args kwargs func _DISPATCH_META_HANDLERS _DISPATCH_META_HANDLERS func args log getEffectiveLevel = logging DEBUG log debug sFakeTensorMode __torch_dispatch__ s RECURSION_COUNT func NOTE incr intentionally unused RAII pattern incr = IncrementRecursionCount noqa F Some attribute queries can serviced directly See Note is_coalesced dispatched func _DISPATCH_HANDLE_DIRECTLY NB no_dispatch ok here too func very simple in_kernel_invocation_manager func args kwargs cache_enabled _cached_dispatch_impl func types args kwargs _dispatch_impl func types args kwargs _maybe_infer_fake func OpOverload path KeyPath fake object real object - tuple Optional object bool Helper cross-check fake real output properties values create new fake vals mismatched Returns tuple object boolean whether overwrriten sympy torch _subclasses fake_utils _check_fake_real_tensors _check_fake_real_vals fake Any real Any - None use real values + ShapeEnv check mismatches between potentially symbolic values isinstance fake SymInt SymFloat symbolic expression ask ShapeEnv substitute known backed unbacked values assert shape_env None fake node expr free_symbols - shape_env var_to_val keys - shape_env unbacked_var_to_val keys shape_env _maybe_evaluate_static sympy Eq fake node expr real compute_hint=True sympy S true raise MetadataMismatchError f mismatch between fake value fake real value real isinstance fake int float bool concrete value check direct equality fake = real raise MetadataMismatchError f mismatch between fake value fake real value real isinstance fake torch Tensor try _check_fake_real_tensors real type ignore arg-type fake type ignore arg-type context= Real tensor propagation found sizes=False manual check below strides=False skip strides storage_offset=True requires_grad=False issues FakeTensorConverter preserving requires_grad except MetadataMismatchError exc torch _functorch config generate_fake_kernels_from_real_mismatches dtrace_structured mismatched_fake_kernel metadata_fn=lambda op str func reason exc reason noqa F _infer_fake_from_real_tensor func real True type ignore arg-type raise MetadataMismatchError f Real tensor propagation found metadata mismatch between f fake tensor fake real tensor real f output keystr path func func exc j s_fake s_real enumerate zip fake size real size type ignore attr-defined try _check_fake_real_vals s_fake s_real except MetadataMismatchError exc torch _functorch config generate_fake_kernels_from_real_mismatches dtrace_structured mismatched_fake_kernel metadata_fn=lambda op str func reason exc reason noqa F _infer_fake_from_real_tensor func real True type ignore arg-type raise MetadataMismatchError f Real tensor propagation found output size mismatch between f fake shape s_fake real shape s_real f output keystr path size j func func exc fake None real None torch _functorch config generate_fake_kernels_from_real_mismatches dtrace_structured mismatched_fake_kernel metadata_fn=lambda op str func reason f mismatch between fake value fake real value real noqa F _infer_fake_from_real_tensor func real True type ignore arg-type raise MetadataMismatchError f Real tensor propagation found metadata mismatch between f fake tensor fake real tensor real f output keystr path func func try _check_fake_real_vals fake real except MetadataMismatchError exc raise MetadataMismatchError f Real tensor propagation found output value mismatch between f fake output value fake real output value real f output keystr path func func exc fake False _maybe_infer_fake_kernel_from_pytree_out func OpOverload fake_in object real_in object fake_out object real_out object - Optional object Helper cross-check fake real output properties values create new fake vals mismatched kernel level Means handles pytree outputs checks aliasing torch _subclasses fake_utils _check_alias_info we might have clear pending unbacked symbols we override kernel pending_unbacked = None shape_env pending_unbacked = list shape_env pending_fresh_unbacked_symbols _clear_pending_unbacked - None shape_env pending_fresh_unbacked_symbols = list type ignore union-attr set shape_env pending_fresh_unbacked_symbols difference type ignore union-attr pending_unbacked type ignore arg-type fake_paths_leaves fake_spec = pytree tree_flatten_with_path fake_out real_leaves _ = pytree tree_flatten real_out try catch aliasing mismatches between fake real tensors _check_alias_info Real tensor propagation found real_out real_in fake_out fake_in except MetadataMismatchError exc mismatch found optionally infer fake kernel torch _functorch config generate_fake_kernels_from_real_mismatches dtrace_structured mismatched_fake_kernel metadata_fn=lambda op str func reason f Mismatched aliasing spec between fake kernel real kernel exc reason noqa F aliasing mismatches found s likely fake tensor impl incorrectly aliasing since we don t support aliasing custom ops case we can default inferring non-aliasing fake kernels real outputs _clear_pending_unbacked tree_map lambda x _infer_fake_from_real_tensor func x real_out raise MetadataMismatchError f Real tensor propagation found aliasing mismatch between f fake output fake_out real output real_out f func func exc no errors raised run cross checks fake real tensors optionally overriding individual fake tensors individual meta kernel output incorrect fake_leaves overrides = zip _maybe_infer_fake func _fake_path _fake_out _real_out _fake_path _fake_out _real_out zip fake_paths_leaves real_leaves any overrides pending_unbacked only keep new pending unbacked symbols _clear_pending_unbacked pytree tree_unflatten fake_leaves fake_spec _dispatch_impl func OpOverload types Sequence type args Sequence object kwargs Mapping str object - Optional FakeTensor torch _higher_order_ops utils registered_hop_fake_fns flat_args args_spec = pytree tree_flatten args kwargs DO NOT PUT LOGIC BEFORE UNRECOGNIZED TYPE CHECKING We must throw NotImplemented case unrecognized types handle subclasses Throwing exception will pass control next __torch_dispatch__ See subclass inputs below NB If you re seeing mysterious infinite loop involving fake tensor might related line Though I m sure how you ll know read comment line won t show up stack trace has_unrecognized_types = _check_for_subclass flat_args has_unrecognized_types unrecognized_types = type x x flat_args _check_for_subclass_arg x not_implemented_log debug FakeTensorMode unrecognized subclass es s unrecognized_types NotImplemented flat_arg_fake_tensors = t t flat_args is_our_fake t has_symbolic_sizes = any i _has_symbolic_sizes_strides i flat_arg_fake_tensors any isinstance SymInt flat_args converter = fake_tensor_converter is_lift_func = func lift_fns If we trying avoid device init then we need avoid constant prop constant tensors ops change devices avoiding_device_init = False avoid_device_init func torch ops aten _to_copy default device kwargs kwargs device type = cpu type ignore attr-defined avoiding_device_init = True func torch ops prims device_put default avoiding_device_init = True skip const prop aten _to_copy input tensor meta device destination device unavailable captured ` avoiding_device_init ` device_conversion_skip_const_prop = func torch ops aten _to_copy default isinstance args torch Tensor args device type == meta avoiding_device_init To constant propagate through these functions If lift due torch tensor call input tensor guaranteed constant so we keep copy original argument along so we can query we re asked item some later point Note you can always call lift fn manually so we do have check there any fake tensors Some functions allow Python numbers bind Tensors e g torch div is_lift_func flat_arg_fake_tensors should_allow_numbers_as_tensors func has_symbolic_sizes flat_arg_fake_tensors device_conversion_skip_const_prop assert all t constant None t flat_arg_fake_tensors f func should have fake inputs without constants const_flat_args = constant is_our_fake flat_args const_args const_kwargs = pytree tree_unflatten const_flat_args args_spec out = func const_args const_kwargs type out Tensor may_turn_const out NB in_kernel_invocation_manager because we re doing real compute here NB no_dispatch here VERY DANGEROUS like segfault dangerous actually wrapper subclass tensor therefore exact type test above no_dispatch out = out clone converter from_real_tensor out make_constant=True we dispatch mode we will enter function even inputs FakeTensors For now throw any non-Fake Tensor inputs just support constructors generated torch tensor which does use dispatcher allow wrapper subclasses wrap new tensor is_lift_func assert len kwargs == len args == f args kwargs type args Tensor converter from_real_tensor args Recompute flat_arg_fake_tensors here again case some inputs real tensors fakified validate_and_convert_non_fake_tensors flat_args flat_arg_fake_tensors = validate_and_convert_non_fake_tensors func converter flat_args args_spec del args kwargs Invalidated The current constant handling only support tracing systems aot autograd torchdynamo where each operation run consecutively Because each operation run order we can trace out support sequences like x = torch tensor y = x add_ Whenever constant written inputs cannot evaluated statically such random_ we invalidate all constants alias input We will rely functionalization use fake tensors constants persistent objects FX Graph We dispatch size stride numel FakeTensor its constant so bail inplace_view all_constant = all e constant None e flat_arg_fake_tensors isinstance func torch _ops OpOverload torch Tag nondeterministic_seeded func tags torch Tag inplace_view func tags all_constant len flat_arg_fake_tensors = has_symbolic_sizes avoiding_device_init func aten _nested_tensor_from_tensor_list default const_flat_args = constant is_our_fake flat_args const_args const_kwargs = pytree tree_unflatten const_flat_args args_spec NB in_kernel_invocation_manager we want do REAL compute no_dispatch out = func const_args const_kwargs flat_out = pytree tree_leaves out flat_out_tensors = t t flat_out isinstance t Tensor all_constant = all may_turn_const t t flat_out_tensors all_constant pytree tree_map_only Tensor lambda t converter from_real_tensor t make_constant=True out we weren t able turn outputs constants so invalidate all constants might aliases outputs ten flat_out_tensors converter invalidate_constant_aliases ten we falling through running non constant tensors any input constant written must invalidated args kwargs = pytree tree_unflatten flat_args args_spec isinstance func torch _ops HigherOrderOperator func registered_hop_fake_fns Reenable fake tensor mode registered fake function maybe_ignore_fresh_unbacked_symbols = contextlib nullcontext shape_env None shape_env ignore_fresh_unbacked_symbols maybe_ignore_fresh_unbacked_symbols pyrefly ignore index-error registered_hop_fake_fns func args kwargs invalidate_written_to_constants func flat_arg_fake_tensors args kwargs maybe_to_real_tensor t T - Optional Union T Tensor torch _C ScriptObject isinstance t FakeTensor t real_tensor isinstance t py_sym_types assert shape_env None t node pytype t node expr xreplace shape_env var_to_val xreplace shape_env unbacked_var_to_val isinstance t FakeScriptObject t real_obj t torch fx experimental symbolic_shapes compute_unbacked_bindings free_unbacked_symbols nil = object real_out = nil propagate_real_tensors all e real_tensor None e flat_arg_fake_tensors any isinstance py_sym_types syms = free_unbacked_symbols shape_env None any s shape_env unbacked_var_to_val s syms flat_args log debug propagate_real_tensors s func real_flat_args = maybe_to_real_tensor flat_args real_args real_kwargs = pytree tree_unflatten real_flat_args args_spec is_builtin = library_utils is_builtin func is_builtin mutation_checker = library_utils MutationChecker func real_flat_args args_spec try real_out = func real_args real_kwargs except ZeroDivisionError exc we shouldn t broadly catch all errors here some come real-kernel mutation aliasing checks we want run add more exception types needed log debug noqa G real-tensor fallback failed s s silently ignoring func exc is_builtin mutation_checker check type ignore possibly-undefined library_utils check_aliasing_constraint func _name flat_args real_out propagate_real_tensors This can happen occasionally legitimately specifically when you inside meta data dependent operation you create tensor unbacked SymInt point time we don t know what unbacked SymInt we will know later However there s bug condition above condition will also trigger log debug SKIPPED propagate_real_tensors s s s s func flat_arg_fake_tensors flat_args shape_env unbacked_var_to_val shape_env None maybe_propagate_real_tensors fake_out T - T sympy log debug maybe_propagate_real_tensors s func go t object real_t Tensor - None isinstance t FakeTensor NB unconditionally overwrite log debug maybe_propagate_real_tensors s - s id t id real_t t real_tensor = real_t s real_s zip t size real_t size go s real_s type ignore arg-type s real_s zip t stride real_t stride go s real_s type ignore arg-type go t storage_offset real_t storage_offset type ignore arg-type isinstance t py_sym_types free_unbacked_symbols t isinstance t node expr sympy Symbol assert shape_env None shape_env set_unbacked_var_to_val t node expr real_t isinstance s = t node expr sympy Eq isinstance s lhs sympy Symbol s rhs == assert shape_env None shape_env set_unbacked_var_to_val s int real_t real_out nil cross check fake real outputs optionally override fake kernel mismatches torch _functorch config generate_fake_kernels_from_real_mismatches _maybe_infer_fake_kernel_from_pytree_out func args kwargs real_args real_kwargs fake_out real_out can override output only when flag True fake_out = _maybe_infer_fake_kernel_from_pytree_out type ignore assignment func args kwargs real_args real_kwargs fake_out real_out populate unbacked_var_to_val isinstance fake_out Tensor isinstance real_out Tensor type fake_out type real_out This can happen when decompositions have different types e g namedtuple vs tuple vs list tree_map_ go tuple pytree tree_flatten fake_out tuple pytree tree_flatten real_out tree_map_ go fake_out real_out If data-dependent op used decomposition we may need get unbacked settings early TODO Is really needed compute_unbacked_bindings shape_env fake_out peek=True pyrefly ignore bad-return fake_out Try fastpath has_symbolic_sizes fast_impl = get_fast_op_impls get func fast_impl None maybe_propagate_real_tensors fast_impl args kwargs If there s Python meta prefer over decomposition torch _decomp meta_table func meta_table cpp_meta_supports_symint func has_symbolic_sizes func _unbacked_special_fake_handling_ops torch _decomp decomposition_table Prefer Python decompositions over C++ ones func decomposition_table has_symbolic_sizes TODO Remove these exclusions so we can remove leg entirely torch_decomp_decompositions func all is_sparse_any e e flat_arg_fake_tensors maybe_propagate_real_tensors decomposition_table func args kwargs Decomposes CompositeImplicitAutograd ops r = func decompose args kwargs r NotImplemented maybe_propagate_real_tensors r prims already wrap FakeTensor inputs FakeTensor outputs do device logic we dont need do anything run them ensure Meta kernels dispatched see Fake Tensor Dispatch Keys TODO - we should use prim aten impl TODO - fix prims complex ops prims func _schema name hasattr func prim_meta_impl stride_incorrect_op func maybe_propagate_real_tensors func prim_meta_impl args kwargs profiles = torch _dynamo config _custom_ops_profile profiles None func profiles data profiles generic_fake_kernel func args kwargs propagate_real_tensors real_out nil library_utils is_builtin func shape_env None Automatically infer Fake kernel there isn t one library_utils has_fake_kernel func result = inferred_fake_kernel_from_real_out func real_out dtrace_structured missing_fake_kernel metadata_fn=lambda op str func maybe_propagate_real_tensors result Users can register FakeTensor rules custom operators Call them they exist maybe_fake_impl = torch _library simple_registry singleton find func name fake_impl kernel maybe_fake_impl try ctx = torch _library fake_impl FakeImplCtx func torch _library fake_impl set_ctx_getter lambda ctx result = maybe_fake_impl args kwargs maybe_propagate_real_tensors result except MissingOpProfile e If we have fake kernel registered generated OpProfiles there doesn t exist profile existing inputs we propagate_real_tensors real_out nil library_utils is_builtin func shape_env None result = inferred_fake_kernel_from_real_out func real_out dtrace_structured missing_fake_kernel metadata_fn=lambda op str func maybe_propagate_real_tensors result raise e special handling funcs registered through ` register_op_impl ` e g manipulating args constructor calls construct meta tensors then afterwards wrapping them FakeTensor run_impl_check op_impl op_implementations_checks run_impl_check func op_impl_out = op_impl func args kwargs op_impl_out NotImplemented maybe_propagate_real_tensors op_impl_out maybe_run_unsafe_fallback error Optional RuntimeError = None - Optional FakeTensor We infer meta custom ops None just None custom ops allowed mutate metadata their inputs so safe torch _library utils can_generate_trivial_fake_impl func None no meta kernel registered fallback kernel device has_symbolic_sizes can_run_unsafe_fallback func raise UnsupportedOperatorException func error None error = UnsupportedOperatorException func run_fallback_kernel func flat_args args_spec error Optimization If there no Meta kernel takes surprisingly long amount time catch NotImplementedError so we check here has_meta func fallback = maybe_run_unsafe_fallback maybe_propagate_real_tensors fallback run kernel registered meta func which include python meta registrations prims decomps c++ meta fns structured kernels It s possible kernel will NotImplementedError try in_kernel_invocation_manager r = func args kwargs except NotImplementedError not_implemented_error maybe_run_unsafe_fallback not_implemented_error except Exception log exception failed while attempting run meta s func raise maybe_propagate_real_tensors wrap_meta_outputs_with_default_device_logic r func flat_args device=kwargs get device WARNING DO NOT add any additional namespaces operators here they refer operators outside pytorch pytorch library Any pre-existing things here either pytorch pytorch library have been grandfathered The fallback does always work MAY CRASH emit unreadable error messages so should allowed default _can_run_unsafe_fallback_allowed_namespaces = ordered_set debugprims prims aten xla vision torchtext torchaudio quantized can_run_unsafe_fallback func OpOverload - bool allow_fallback_kernels False It s OK try fallback built-in ops e g aten prims because we control test these fallback leads unexpected behavior user-defined custom ops func namespace _can_run_unsafe_fallback_allowed_namespaces func name == fbgemm gmm validate_and_convert_non_fake_tensors func OpOverload converter FakeTensorConverter flat_args Sequence object args_spec TreeSpec - tuple list object list FakeTensor Checks list tensors fake tensors If try convert them fake tensors Returns original args kwargs flattened list args kwargs fake tensors flat_arg_fake_tensors list FakeTensor = validate x T - Union T FakeTensor isinstance x Tensor x nonlocal flat_arg_fake_tensors is_our_fake x hasattr func tags torch Tag inplace_view func tags args kwargs = pytree tree_unflatten flat_args args_spec raise AssertionError f Can t call metadata mutating ops non-Fake Tensor inputs Found render_call func args kwargs allow_non_fake_inputs = allow_non_fake_inputs fake_tensor_tls allow_non_fake_inputs_override None fake_tensor_tls allow_non_fake_inputs_override allow_non_fake_inputs isinstance x FakeTensor x fake_mode raise AssertionError Mixing fake modes NYI args kwargs = pytree tree_unflatten flat_args args_spec raise AssertionError f Please convert all Tensors FakeTensors first instantiate FakeTensorMode f allow_non_fake_inputs Found render_call func args kwargs out = converter from_real_tensor x out = x flat_arg_fake_tensors append out out validated_args = validate flat_args validated_args flat_arg_fake_tensors wrap_meta_outputs_with_default_device_logic r object func OpOverload flat_args Sequence object device torch device - PyTree converter = fake_tensor_converter Lazily initialized case there no tensor returns common_device = None has_scalar_only_inputs = False wrap e T - Union T FakeTensor nonlocal common_device nonlocal has_scalar_only_inputs isinstance e Tensor e common_device None common_device has_scalar_only_inputs = FakeTensor _find_common_device func flat_args is_our_fake = is_our_fake e is_our_fake torch _check e device == common_device lambda f FakeTensor wrapped wrong device found e device expected common_device cast T e converter None has_scalar_only_inputs Under FakeTensorMode op accepts scalar only inputs such aten add sub mul div returns real scalar tensor CPU See TensorMeta _prims __init__ py details We thus directly convert real tensor fake tensor converter from_real_tensor e converter from_meta_and_device e device common_device pyrefly ignore bad-return e tree_map wrap r create_symbolic_nested_int nt_tensor_id Optional int = None - torch SymInt See Note Creating symbolic nested int Returned nested int always has coeff= multiply result coeff needed torch nested _internal nested_tensor torch nested _internal nested_int NestedIntNode nt_tensor_id None nt_tensor_id = nt_tensor_id_counter assert enter_stack should only called while FakeTensorMode active nt_tensor_id_counter += hint = torch SymInt NestedIntNode nt_tensor_id src = torch _dynamo source EphemeralSource intermediate_offsets_or_lengths assert shape_env None ret = shape_env create_symintnode sym=self shape_env create_symbol val=hint source=src hint=hint source=src ret _cpp_meta_supports_symint = ordered_set aten empty memory_format aten empty_strided default aten as_strided_scatter default aten as_strided default aten as_strided_ default aten zeros default aten detach default aten view_as_real default aten view_as_complex default aten set_ source_Storage_storage_offset aten _sparse_coo_tensor_with_dims_and_tensors default _unbacked_special_fake_handling_ops = ordered_set aten view default aten _unsafe_view default aten slice Tensor cpp_meta_supports_symint func OpOverload - bool torch Tag view_copy func tags True func _cpp_meta_supports_symint lift_fns = ordered_set aten lift_fresh default aten lift_fresh_copy default may_turn_const t Tensor - bool t numel = CONSTANT_NUMEL_LIMIT is_sparse_any t is_our_fake t t device type = meta invalidate_written_to_constants func OpOverload flat_arg_fake_tensors Sequence FakeTensor args Sequence object kwargs Mapping str object - None any_constant = any e constant None e flat_arg_fake_tensors schema_info = get_schema_info func any_constant schema_info is_mutable _ new_kwargs = normalize_function type ignore misc func args=args type ignore arg-type kwargs=kwargs type ignore arg-type normalize_to_only_use_kwargs=True k v new_kwargs items k = k k = input schema_info has_argument k is_our_fake v schema_info is_mutable k v constant None fake_tensor_converter invalidate_constant_aliases v constant from_tensor tensor Tensor static_shapes Optional bool = None source Optional Source = None symbolic_context Optional SymbolicContext = None trace bool = True - FakeTensor shape_env Optional ShapeEnv = shape_env static_shapes None static_shapes = static_shapes static_shapes assert symbolic_context None cannot set both static_shapes symbolic_context shape_env = None fake_tensor_converter from_real_tensor tensor shape_env=shape_env source=source symbolic_context=symbolic_context trace=trace _StoragePointer = object _validate_symbolic_output_for_caching state _CacheKeyState output FakeTensor - None Validate symbolic content output raise _BypassDispatchCache caching should bypassed Args state Cache key state containing known symbols output Output validate proxy_mode_active Whether PROXY dispatch mode currently active Raises _BypassDispatchCache If output contains symbolic content prevents caching Details If our output contains any symbols didn t appear input then we need bypass Usually will unbacked symbols which can t properly reconstructed there could weird cases where backed symbols spontaneously appear non-input state If we re proxy symbol tracing output contains ANY symbols then we need bypass The problem ProxyTorchDispatchMode relies SymNode object identity being able see construction SymNodes We could improve proxy tracing case few ways If output SymNodes directly copied inputs then actually fine - they re already tracked This would probably biggest bang buck If output tensors all direct copies inputs then also fine - since they re inputs they must tracked We already compute we just don t plumb around enough If output SymNodes already tracked proxy then also actually fine - they re properly tracked This probably wouldn t common since most outputs we use torch empty_strided recompute strides We could use proxy track how SymNodes computed when using cache we could replay them properly teach proxy how build them torch fx experimental symbolic_shapes _iterate_exprs _iterate_nodes is_tracing = torch fx experimental proxy_tensor get_proxy_mode None is_tracing Check SymNode types PROXY mode - should bypass caching regardless whether symbols known _ _iterate_nodes output raise _BypassDispatchCache Proxy mode SymNode output Check unrepresented symbols tensor expressions s _iterate_exprs output symbol s free_symbols symbol state known_symbols raise _BypassDispatchCache unrepresented symbol output NB returns fake tensors run_fallback_kernel fake_mode FakeTensorMode func OpOverload flat_args Sequence object args_spec PyTree orig_not_implemented_exception RuntimeError - FakeTensor these should all supported just safe avoid fallback operators which inplace modify metadata because input fake tensors would umodified torch Tag inplace_view func tags raise orig_not_implemented_exception inp_impls = Don t use in_kernel_invocation_manager fake_mode we want do REAL compute meta device no_dispatch to_real_tensor e T - Union T Tensor fake_mode is_our_fake e out = torch zeros_like e device=e fake_device e is_sparse out _coalesced_ e is_coalesced inp_impls id out = e out e flat_args = to_real_tensor flat_args args kwargs = pytree tree_unflatten flat_args args_spec r = func args kwargs storages set _StoragePointer = set e flat_args isinstance e Tensor is_sparse_any e storages add e _typed_storage _cdata TODO also check metadata change inputs proper aliasing metadata relationship between outputs inputs will set up bc conversion device unless we can reuse input impl map_out e T - Union T FakeTensor id e inp_impls isinstance e Tensor is_sparse_any e e _typed_storage _cdata storages raise orig_not_implemented_exception isinstance e Tensor id e inp_impls inp_impls id e fake_mode fake_tensor_converter from_real_tensor fake_mode e e pytree tree_map map_out r _set_cache_key_for_shape_env cache dict _DispatchCacheKey _DispatchCacheEntry key _DispatchCacheKey entry _DispatchCacheEntry - None key strip_shape_env cache key = entry _set_cache_key cache dict _DispatchCacheKey _DispatchCacheEntry key _DispatchCacheKey entry _DispatchCacheEntry - None cache key = entry Just use allow copying module fake tensors does apply elsewhere FakeCopyMode TorchFunctionMode __init__ fake_mode FakeTensorMode - None fake_mode = fake_mode __torch_function__ func OpOverload types Sequence type args Sequence object = kwargs Optional Mapping str object = None - FakeTensor kwargs = kwargs kwargs clone will get called Parameter deepcopy func torch _C TensorBase clone assert isinstance args Tensor func fake_mode from_tensor args static_shapes=True kwargs func Tensor __deepcopy__ assert len args == len kwargs == tensor = cast Tensor args memo = cast dict int FakeTensor args id tensor memo memo id tensor out = fake_mode from_tensor tensor static_shapes=True memo id tensor = out out torch _C DisableTorchFunctionSubclass func args kwargs _device_handler args Sequence object - torch device NB Don t use is_our_fake just serve fake information Notice we don t use we use args fake_mode because they may same It would also possible NotImplemented here which case FakeTensor handler args would handle we re being nice short-circuiting quickly assert len args == isinstance args FakeTensor args fake_mode in_kernel_invocation torch device meta args fake_device subclass inputs Suppose we enable fake tensor mode This means fake tensor mode will run first But what we do operation involves tensor subclass will desugar into normal tensor operations Without returning NotImplemented fake tensor mode will run first decide conversion made since there non fake tensor argument report error converting non fake tensor supported What we actually wanted happen give subclass chance figure out what wants before erroring out Returning NotImplemented here allows _check_for_subclass flat_args Sequence object - bool any _check_for_subclass_arg x x flat_args _check_for_subclass_arg x object - bool isinstance x FakeTensor isinstance x Tensor type x Tensor type x torch nn Parameter _DISPATCH_META_HANDLERS = torch ops prim device default _device_handler torch ops aten size default lambda args tuple int s s cast Tensor args size torch ops aten stride default lambda args tuple int s s cast Tensor args stride torch ops aten storage_offset default lambda args int cast Tensor args storage_offset _DISPATCH_HANDLE_DIRECTLY = ordered_set torch ops aten is_coalesced default torch ops aten dense_dim default torch ops aten sparse_dim default _RecordFunction doesn t support __eq__ so make sure attempt cache torch ops profiler _record_function_exit _RecordFunction torch _subclasses fake_impls noqa F _device_not_kwarg_ops _is_tensor_constructor _like_tensor_constructors contains_tensor_types get_fast_op_impls has_meta op_implementations_checks stride_incorrect_op evict_fake_tensor_cache_key key _DispatchCacheKey - None key FakeTensorMode cache FakeTensorMode cache pop key atexit register dump_cache_stats - None log info FakeTensor cache stats log info cache_hits s FakeTensorMode cache_hits log info cache_misses s FakeTensorMode cache_misses bypasses = FakeTensorMode cache_bypasses bypasses log info cache_bypasses width = max len k k bypasses k v sorted bypasses items key=lambda i -i log info - s s width + f k v _infer_fake_from_real_tensor mode FakeTensorMode op torch _ops OpOverload real_out torch Tensor - torch Tensor unsupported reason str - None raise RuntimeError f propagate_real_tensors we cannot infer Fake kernel f meta kernel operator op _name because reason f Please use torch library register_fake add Fake kernel real_out storage_offset = unsupported f has non-zero storage offset real_out storage_offset Since PT rank specialized there s no such thing symbolic output rank So we can assume fake tensor has same number dimensions real tensor output We shouldn t assume Fake sizes strides exactly what we see real tensor output perhaps we should give users lever toggle This because there s good amount operators outputs data-dependent output shape So we infer output sizes all unbacked symints fake_shape = torch _library fake_impl allocate_size mode shape_env _ range real_out dim We infer what strides We had couple options - assume strides computable sizes - use new fresh unbacked symints strides This doesn t work well PT doesn t support unbacked symint strides well - use real strides This can only used we assume strides static We went first option fake_strides = - real_out dim strides = s idx idx s enumerate real_out stride strides sort key=lambda x x -x expected = fake_stride = expected s idx strides s = expected unsupported f dense memory sizes real_out shape strides real_out stride fake_strides idx = fake_stride expected = expected real_out shape idx fake_stride = fake_stride fake_shape idx mode torch empty_strided fake_shape fake_strides device=real_out device dtype=real_out dtype layout=real_out layout inferred_fake_kernel_from_real_out mode FakeTensorMode op torch _ops OpOverload real_out Any - Any assert mode shape_env None Only support operators have all Tensor outputs This general limitation custom ops we impose PT avoid baking non-symbolic float int outputs into graph real_flat_out spec = pytree tree_flatten real_out all isinstance t torch Tensor t real_flat_out raise RuntimeError f propagate_real_tensors we don t support operators f non-Tensors Got op _schema fake_flat_out = _infer_fake_from_real_tensor mode op t t real_flat_out pytree tree_unflatten fake_flat_out spec