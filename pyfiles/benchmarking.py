functools inspect time functools cached_property wraps itertools chain statistics median typing Any Callable typing_extensions Concatenate ParamSpec Self TypeVar torch torch _dynamo utils counters dynamo_timed torch _inductor config use_experimental_benchmarker logger = torch _logging getArtifactLogger __name__ benchmarking use_experimental_benchmarker = use_experimental_benchmarker torch cuda is_available MILLISECONDS_PER_SECOND = P = ParamSpec P T = TypeVar T may_distort_benchmarking_result fn Callable Any - Callable Any torch _inductor config config test_configs distort_benchmarking_result == fn distort ms list float &#124; tuple float &#124; float - list float &#124; tuple float &#124; float isinstance ms list tuple type ms distort val val ms type ignore misc distort_method = config test_configs distort_benchmarking_result assert isinstance ms float distort_method == inverse ms ms distort_method == random random random random raise RuntimeError f Unrecognized distort method distort_method functools wraps fn wrapper args list Any kwargs dict str Any - list float &#124; tuple float &#124; float ms = fn args kwargs distort ms wrapper may_ban_benchmarking - None torch _inductor config deterministic raise RuntimeError In deterministic mode Inductor we will avoid those benchmarkings would cause non deterministic results Only benchmarkings vetted scenarios allowed Example include autotuning triton configs pointwise kernels When you see exception you can do one following two things benchmarking you doing does introduce any non-determinism you can just add is_vetted_benchmarking=True you benchmark_gpu call That would solve issue benchmarking you doing indeed introduces non-determinism you ll need disable such feature deterministic mode find alternative implementation deterministic time_and_count fn Callable Concatenate Any P T - Callable Concatenate Any P T Wraps ` fn ` ` dynamo_timed ` context increments appropriate dynamo counters It expected ` fn ` method ` Benchmarker ` one its subclasses typing limitations prevent us declaring directly wraps fn wrapper Any args P args kwargs P kwargs - T fn_qual_name = f __class__ __name__ fn __name__ counters inductor f benchmarking fn_qual_name += dynamo_timed fn_qual_name log_pt _compile_event=False fn args kwargs wrapper Benchmarker __init__ Self - None pass time_and_count benchmark Self fn Callable Any fn_args tuple Any fn_kwargs dict str Any kwargs Any - float Benchmark ` fn fn_args fn_kwargs ` runtime milliseconds actual runtime calculation dictated benchmarking implementation may one mean median minimum etc Functions convenience wrapper around device-specific implementations like ` benchmark_cpu ` ` benchmark_gpu ` Raises ` ValueError ` we can t safely infer device type ` fn ` example multiple device types found ` fn_args ` ` fn_kwargs ` no device types found Arguments - fn The function benchmark - fn_args The function s arguments - fn_kwargs The function s kwargs Keyword Arguments - kwargs The benchmarking implementation s kwargs Returns - The runtime ` fn fn_args fn_kwargs ` milliseconds inferred_device = None pyrefly ignore bad-assignment arg_or_kwarg chain fn_args fn_kwargs values isinstance arg_or_kwarg torch Tensor continue inferred_device None inferred_device = arg_or_kwarg device arg_or_kwarg device = inferred_device raise ValueError Can t safely infer device type ` fn ` multiple device types ` fn_args ` ` fn_kwargs ` inferred_device None raise ValueError Can t safely infer device type ` fn ` no device types ` fn_args ` ` fn_kwargs ` You should calling ` benchmark_cpu ` ` benchmark_gpu ` directly noqa B _callable = lambda fn fn_args fn_kwargs noqa E inferred_device == torch device cpu benchmark_cpu _callable kwargs TODO nmacchioni For non-CPU functions we default using GPU-specific benchmarking implementation which written specifically CUDA devices mind we may want explore alternate implementations other device types benchmark_gpu _callable kwargs time_and_count benchmark_cpu Self _callable Callable Any warmup int = rep int = - float Benchmark CPU callable ` _callable ` median runtime milliseconds Arguments - _callable The CPU callable benchmark Keyword Arguments - warmup Optionally duration milliseconds run ` _callable ` before benchmarking starts - rep Optionally duration milliseconds run ` _callable ` during benchmarking Returns - The median runtime ` _callable ` milliseconds run_for ms int - list float timings = run_start_t = time perf_counter while True start_t = time perf_counter _callable end_t = time perf_counter timings append end_t - start_t MILLISECONDS_PER_SECOND end_t - run_start_t MILLISECONDS_PER_SECOND ms break timings run_for warmup median run_for rep time_and_count benchmark_gpu Self args Any kwargs Any - float raise NotImplementedError TritonBenchmarker Benchmarker cached_property triton_do_bench Self - Callable Any Lazily Triton s ` do_bench ` try triton testing do_bench except ImportError e raise NotImplementedError requires Triton e do_bench may_distort_benchmarking_result time_and_count pyrefly ignore bad-override benchmark_gpu Self _callable Callable Any is_vetted_benchmarking bool = False kwargs Any - float Benchmark GPU callable ` _callable ` runtime milliseconds Arguments - _callable The GPU callable benchmark Keyword Arguments - quantiles Optionally tuple floats denoting requested quantiles - return_mode Optionally requested mode Currently Triton s ` do_bench ` supports min max mean median modes - kwargs Additional kwargs passed Triton s ` do_bench ` Returns - The runtime ` callable ` milliseconds If ` kwargs quantiles ` specified first requested quantile Else ` kwargs return_mode ` specified requested mode Otherwise median is_vetted_benchmarking may_ban_benchmarking do_bench_params = inspect signature triton_do_bench parameters kwarg list kwargs keys kwarg do_bench_params del kwargs kwarg quantiles kwargs triton_do_bench _callable kwargs return_mode kwargs triton_do_bench _callable kwargs triton_do_bench _callable kwargs return_mode= median InductorBenchmarker TritonBenchmarker noqa docstring_linter cached_property L _cache_size Self - int Get L cache size bytes current device device = torch cuda current_device props = torch cuda get_device_properties device props L _cache_size get_event_pairs Self iters int - list tuple torch cuda Event torch cuda Event Get ` iters ` pairs CUDA events torch cuda Event enable_timing=True torch cuda Event enable_timing=True _ range iters get_event_pairs_min_timing Self event_pairs list tuple torch cuda Event torch cuda Event - float Get minimum timing milliseconds group CUDA event pairs min start_event elapsed_time end_event start_event end_event event_pairs may_distort_benchmarking_result time_and_count benchmark_gpu type ignore override Self _callable Callable Any estimation_iters int = memory_warmup_iters int = benchmark_iters int = max_benchmark_duration int = return_mode str = min grad_to_none list torch Tensor &#124; None = None is_vetted_benchmarking bool = False kwargs Any - float &#124; list float Benchmark GPU callable using custom benchmarking implementation Arguments - _callable The callable benchmark Keyword Arguments - estimation_iters Optionally number iterations run ` _callable ` during runtime estimation - memory_warmup_iters Optionally number iterations flush L cache before starting benchmarking - benchmark_iters Optionally number iterations run ` _callable ` during benchmarking - max_benchmark_duration Optionally maximum duration benchmarking milliseconds An estimated duration calculated based values ` memory_warmup_iters ` ` benchmark_iters ` along estimated runtime ` _callable ` various other factors we then shrink ` benchmark_iters ` fit allotted maximum duration - return_mode Return mode benchmark results Options min default all returns all measurements - grad_to_none Optionally list tensors whose gradients should cleared before each benchmark iteration - is_vetted_benchmarking deterministic mode we only allow benchmarking vetted cases - kwargs Additional kwargs may passed fallback Returns - If return_mode= min The minimum runtime ` _callable ` milliseconds - If return_mode= all List all runtime measurements milliseconds is_vetted_benchmarking may_ban_benchmarking we don t want any outside errors propagating into benchmarking torch cuda synchronize warmup ` _callable ` catches any failures process _callable torch cuda synchronize see https github com triton-lang triton pull why ` dtype=torch int ` buffer = torch empty L _cache_size dtype=torch int device= cuda buffer zero_ estimate runtime ` _callable ` event_pairs = get_event_pairs estimation_iters start_event end_event event_pairs Clear gradients before timing matches triton testing do_bench grad_to_none None x grad_to_none x grad = None buffer zero_ start_event record _callable end_event record torch cuda synchronize estimated_timing = get_event_pairs_min_timing event_pairs adjust ` benchmark_iters ` fit maximum benchmarking duration benchmark_iters = max min benchmark_iters int max_benchmark_duration estimated_timing do memory warmup _ range memory_warmup_iters buffer zero_ benchmark ` _callable ` event_pairs = get_event_pairs benchmark_iters start_event end_event event_pairs Clear gradients before timing matches triton testing do_bench grad_to_none None x grad_to_none x grad = None buffer zero_ start_event record _callable end_event record torch cuda synchronize explicitly delete buffer sometimes helps memory footprint metrics OSS Inductor performance benchmarks del buffer Return based requested mode return_mode == all Get all timings event pairs all_timings = start_event elapsed_time end_event start_event end_event event_pairs all_timings return_mode == min benchmarked_timing = get_event_pairs_min_timing event_pairs minimum ` estimated_timing ` ` benchmarked_timing ` we just want minimum timing overall so we might well check both min estimated_timing benchmarked_timing raise ValueError f Unsupported return_mode return_mode Use min all benchmarker = InductorBenchmarker use_experimental_benchmarker TritonBenchmarker