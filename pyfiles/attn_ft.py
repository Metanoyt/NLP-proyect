Copyright c Facebook Inc its affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree math torch functorch dim cat dimlists dims torch nn Linear nn Linear forward input ci co = dims b = dimlists result = input b ci weight co ci sum ci + bias co result order b co BertSelfAttention nn Module __init__ hidden_size num_attention_heads attention_probs_dropout_prob position_embedding_type=None max_position_embeddings=None linear=Linear super __init__ hidden_size num_attention_heads = raise ValueError f The hidden size hidden_size multiple number attention f heads num_attention_heads num_attention_heads = num_attention_heads attention_head_size = int hidden_size num_attention_heads all_head_size = num_attention_heads attention_head_size query = linear hidden_size all_head_size key = linear hidden_size all_head_size value = linear hidden_size all_head_size dropout_prob = attention_probs_dropout_prob position_embedding_type = position_embedding_type position_embedding_type None assert max_position_embeddings None max_position_embeddings = max_position_embeddings distance_embedding = nn Embedding max_position_embeddings - attention_head_size forward hidden_states past_key_value=None first run encoding linear layers q k v normally meaning linear layer well understood so no need use explicit dimensions q = query hidden_states k = key hidden_states v = value hidden_states introduce values represent each dimension dimensions first because they actual python values introduced here batch query_sequence key_sequence heads features = dims heads size = num_attention_heads bind positional dimensions k q v against our values sizes each dimension determined binding when dimension used twice e g batch its size against both uses checked consistency The group heads features splits apart single positional dimension into two dimensions Since heads size features size == q size we specified heads size features size inferred here q = q batch query_sequence heads features k = k batch key_sequence heads features v = v batch key_sequence heads features option allows model attend just elements current sequence previous elements well additional tokens past_key_value None extended_key_sequence = dims key_past = past_key_value batch heads key_sequence features value_past = past_key_value batch heads key_sequence features cat introduces new dimension extended_key_sequence because twice long original key_sequence k = cat key_past k key_sequence extended_key_sequence v = cat value_past v key_sequence extended_key_sequence rest function we will just use extended_key_sequence lieu key_sequence key_sequence = extended_key_sequence Take dot product between query key get raw attention scores The actual outer-product summation explicitly represented here like einsum will pattern matched efficient matrix multiply op attention_scores = q k sum features math sqrt features size relative positional embeddings gave unique embedding based distance between key value tokens sequence e g - - - - - - position_embedding_type None value dimension object when used tensor indices along its dimension so we can directly subtract two dimensions get D tensor query_sequence x key_sequence distance between them distance = query_sequence - key_sequence assert key_sequence size = max_position_embeddings we can then use indirect index into embedding table values look up features index just ` gather ` primitive op The resulting tensor will have all dimensions embedding_idx query_sequence x key_sequence plus all dimensions ` embed ` indirectly accessed ` embedding_range ` form indirect indexing more straightforward than either advanced indexing torch gather which both have lot dependencies positions indexing tensors positional_embedding = distance_embedding weight max_position_embeddings - + distance features position_embedding_type == relative_key these einsum ops positional code because they easy fit existing matmul operators even though they degenerate matmuls relative_position_scores = q positional_embedding sum features attention_scores = attention_scores + relative_position_scores position_embedding_type == relative_key_query relative_position_scores_query = q positional_embedding sum features relative_position_scores_key = k positional_embedding sum features attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key attention_probs = attention_scores Normalize attention scores probabilities attention_probs = torch softmax attention_scores dim=key_sequence This actually dropping out entire tokens attend which might seem bit unusual taken original Transformer paper attention_probs = torch nn functional dropout attention_probs p=self dropout_prob similarly we can replace matmul direct listing outer product which makes clear we weighting values v across all keys attention scores context_layer = attention_probs v sum key_sequence finally we convert back standard tensor describing layout dimensions working reverse with_dims heads features group flattens dimensions into single one context_layer order batch query_sequence heads features