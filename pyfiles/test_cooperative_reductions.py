Owner s module inductor unittest typing Any sympy torch torch _inductor torch _inductor config torch _inductor choices InductorChoices torch _inductor codegen simd_kernel_features SIMDKernelFeatures torch _inductor codegen triton FixedTritonConfig TritonKernel torch _inductor test_case TestCase torch _inductor utils run_and_get_code torch testing assert_close torch testing _internal common_cuda IS_SM torch testing _internal common_utils instantiate_parametrized_tests parametrize torch testing _internal inductor_utils GPU_TYPE HAS_GPU TestingHeuristics InductorChoices __init__ cooperative bool persistent bool cfg dict str int super __init__ cooperative = cooperative persistent = persistent cfg = cfg call_count = triton_kernel_kwargs kernel_cls type TritonKernel features SIMDKernelFeatures groups list sympy Expr kernel_kwargs dict str Any - dict str Any call_count += kernel_kwargs override_cooperative_reduction cooperative override_persistent_reduction persistent fixed_config FixedTritonConfig cfg config patch triton cooperative_reductions True triton force_cooperative_reductions True instantiate_parametrized_tests CooperativeReductionTests TestCase setUp super setUp torch _inductor metrics generated_kernel_count = torch _dynamo reset run_and_check fn args dtype=None expect_kernel_count= Define fixed tolerances RTOL = e- ATOL = e- calculate reference value higher precision when input dtype float ref_dtype = dtype dtype == torch float ref_dtype = torch float Cast determined reference dtype args_ref = tensor ref_dtype tensor args Calculate expected output raw_expected = fn args_ref isinstance raw_expected tuple list If s tuple list apply dtype each tensor within Also handle cases where dtype might provided e g bool reductions dtype None expected = type raw_expected t dtype isinstance t torch Tensor t t raw_expected expected = type raw_expected t torch float isinstance t torch Tensor t t raw_expected If s single tensor dtype None expected = raw_expected dtype expected = raw_expected torch float fn_compiled = torch compile fn fullgraph=True result source_code = run_and_get_code fn_compiled args For comparison ensure result also tuple list expected isinstance expected tuple list isinstance result torch Tensor result = result isinstance result type expected result = type expected result dtype None result = type result t dtype isinstance t torch Tensor t t result result = type result t torch float isinstance t torch Tensor t t result dtype None isinstance result torch Tensor result = result dtype isinstance result torch Tensor result = result torch float Apply assert_close fixed tolerances tensor comparisons isinstance result torch Tensor isinstance expected torch Tensor assert_close result expected rtol=RTOL atol=ATOL isinstance result tuple list isinstance expected tuple list Iterate through elements comparison r_item e_item zip result expected isinstance r_item torch Tensor isinstance e_item torch Tensor assert_close r_item e_item rtol=RTOL atol=ATOL Fallback assertEqual non-tensor elements e g bool int assertEqual r_item e_item Fallback assertEqual other types handled assert_close assertEqual result expected triton_heuristics fixed_config source_code assertIn cooperative_reduction_grid source_code assertIn triton_heuristics cooperative_reduction source_code async_compile multi_kernel source_code assertEqual torch _inductor metrics generated_kernel_count expect_kernel_count source_code parametrize name sum mean prod amin amax min max var_mean std softmax parametrize dtype torch float torch float torch float test_reduction_fns name dtype IS_SM dtype == torch float name std var_mean raise unittest SkipTest Timeouts SM fn x y reduction_fn x + y dim=- reduction_fn = getattr torch name args = torch randn device=GPU_TYPE dtype=dtype _ range run_and_check fn args dtype test_bool_reduction_fns fn x y torch any x == y torch all x == y torch any x = y torch all x = y torch any x y torch all x y args = torch randn device=GPU_TYPE _ range source_code = run_and_check fn args async_compile multi_kernel source_code before after = source_code split triton_helpers x_grid_barrier assertEqual before count rsplit_id == assertEqual after count rsplit_id == parametrize bs parametrize count + - test_non_power_of_ bs count fn x x mean x std + x min args = torch randn bs count device=GPU_TYPE run_and_check fn args test_chained_reductions fn x _ range x = x + torch softmax x x args = torch randn device=GPU_TYPE source_code = run_and_check fn args async_compile multi_kernel source_code With online softmax computation max sum done jointly they share single barrier call XPU doesn t support online softmax yet expected_num_barrier = config online_softmax GPU_TYPE = xpu assertEqual source_code count triton_helpers x_grid_barrier expected_num_barrier assertEqual source_code count f empty_strided_ GPU_TYPE test_reduce_split fn b = torch linalg vector_norm b = torch sum b dim= b inps = torch rand device=GPU_TYPE torch rand device=GPU_TYPE run_and_check fn inps expect_kernel_count= config patch triton persistent_reductions config triton persistent_reductions NoPersistCooperativeReductionTests CooperativeReductionTests pass config patch triton multi_kernel int config triton multi_kernel MultiKernelCooperativeReductionTests CooperativeReductionTests pass config patch triton cooperative_reductions True instantiate_parametrized_tests TestFixedConfigs TestCase _check fn args persistent=False cooperative=True cfg expected = fn args heuristic = TestingHeuristics persistent=persistent cooperative=cooperative cfg=cfg torch _inductor virtualized V set_choices_handler heuristic result source_code = run_and_get_code torch compile fn fullgraph=True args assertEqual result expected assertEqual heuristic call_count assertIn triton_heuristics fixed_config source_code parametrize persistent cooperative cfg False False XBLOCK R _BLOCK False False XBLOCK R _BLOCK True False XBLOCK True False XBLOCK False True XBLOCK R _BLOCK RSPLIT False True XBLOCK R _BLOCK RSPLIT True True XBLOCK RSPLIT True True XBLOCK RSPLIT False True XBLOCK R _BLOCK RSPLIT False True XBLOCK R _BLOCK RSPLIT True True XBLOCK RSPLIT True True XBLOCK RSPLIT test_fixed_configs persistent cooperative cfg fn x torch softmax x + dim=- + x args = torch randn device=GPU_TYPE _check fn args persistent=persistent cooperative=cooperative cfg=cfg parametrize persistent x r rsplit False False False False False True True True True True test_welford_non_power_of_ _rsplit persistent x r rsplit fn x torch var_mean x dim=- cfg = XBLOCK RSPLIT rsplit num_warps persistent cfg R _BLOCK = args = torch randn x r device=GPU_TYPE _check fn args persistent=persistent cfg=cfg parametrize persistent True False test_min_max_non_power_of_ _rsplit persistent fn x torch amin x dim=- torch amax x dim=- torch argmin x dim=- torch argmax x dim=- cfg = XBLOCK RSPLIT num_warps persistent cfg R _BLOCK = args = torch stack torch arange device=GPU_TYPE -torch arange device=GPU_TYPE _check fn args persistent=persistent cfg=cfg args = torch stack torch tensor + float inf device=GPU_TYPE dtype=torch float torch tensor + -float inf device=GPU_TYPE dtype=torch float _check fn args persistent=persistent cfg=cfg parametrize persistent False True parametrize rsplit test_fixed_config_with_larger_xblock_than_xnumel persistent rsplit fn x y torch any x == y torch all x == y torch any x = y torch all x = y torch mean x + y cfg = XBLOCK RSPLIT rsplit num_warps num_stages persistent cfg R _BLOCK = args = torch randn device=GPU_TYPE _ range _check fn args persistent=persistent cfg=cfg __name__ == __main__ torch _dynamo test_case run_tests HAS_GPU run_tests needs= filelock