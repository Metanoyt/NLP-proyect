Copyright c Meta Platforms Inc affiliates Owner s oncall distributed collections abc Callable Sequence typing Any Optional unittest skip torch torch utils _pytree pytree torch Tensor torch distributed tensor DeviceMesh distribute_tensor DTensor Partial Placement Replicate Shard torch distributed tensor debug CommDebugMode torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor DTensorOpTestBase skip_unless_torch_gpu no_op None deepcopy_convert_to_dtensor val Any device_mesh DeviceMesh placements Sequence Placement - Any Recursively convert over Sequence Dict types Tensors into DTensors param device_mesh DeviceMesh use param placements Placement list use transformed structure f x isinstance x Tensor isinstance x DTensor distribute_tensor x device_mesh=device_mesh placements=placements x pytree tree_map f val deepcopy_convert_from_dtensor val Any - Any Recursive convert any DTensor local Tensor param val structure coerce coerced structure f x isinstance x DTensor x full_tensor x pytree tree_map f val DistElementwiseOpsTest DTensorOpTestBase _compare_pairwise_ops device_mesh DeviceMesh placements Sequence Placement op Callable pre_op_fn Optional Callable = None args Sequence Any = kwargs Optional dict str Any = None pre_op_fn None pre_op_fn = no_op kwargs kwargs = dargs = deepcopy_convert_to_dtensor args device_mesh=device_mesh placements=placements dkwargs = deepcopy_convert_to_dtensor kwargs device_mesh=device_mesh placements=placements pre_op_fn run reference first case call broken s better debug incorrect call point reference_result = op args kwargs pre_op_fn dist_result = op dargs dkwargs collected_result = deepcopy_convert_from_dtensor dist_result assertEqualOnRank reference_result collected_result TODO We need add CPU tests ops future _run_sharded_elementwise_ops device_mesh DeviceMesh placements Sequence Placement pre_op_fn Optional Callable = None input_size Sequence int op Callable kwargs pre_op_fn None pre_op_fn = no_op input_tensor = torch randn input_size device=self device_type requires_grad=True _compare_pairwise_ops device_mesh=device_mesh placements=placements pre_op_fn=pre_op_fn op=op args= input_tensor kwargs=kwargs test_partial_add device_mesh = build_device_mesh d_ = DTensor from_local torch rand device_mesh Partial d_ = DTensor from_local torch rand device_mesh Partial d_ = d_ + d_ assertTrue d_ _spec placements is_partial test_activations device_mesh = build_device_mesh _run_sharded_elementwise_ops device_mesh=device_mesh placements= Shard input_size= op=torch nn functional gelu _run_sharded_elementwise_ops device_mesh=device_mesh placements= Replicate input_size= op=torch nn functional gelu _run_sharded_elementwise_ops device_mesh=device_mesh placements= Shard input_size= op=torch nn functional relu _run_sharded_elementwise_ops device_mesh=device_mesh placements= Replicate input_size= op=torch nn functional relu _run_sharded_elementwise_ops device_mesh=device_mesh placements= Shard input_size= op=torch sigmoid _run_sharded_elementwise_ops device_mesh=device_mesh placements= Replicate input_size= op=torch sigmoid skip testing RNG based ops broken https github com pytorch PiPPy issues test_dropout device_mesh = build_device_mesh _reset_random_seed torch manual_seed rank + _run_sharded_elementwise_ops device_mesh=device_mesh placements= Shard input_size= op=torch nn functional dropout pre_op_fn=_reset_random_seed p= training=False _run_sharded_elementwise_ops device_mesh=device_mesh placements= Shard input_size= op=torch nn functional dropout pre_op_fn=_reset_random_seed p= training=True skip_unless_torch_gpu test_dropout_backward device_mesh = build_device_mesh placements = Shard input_size = grad_output = torch rand input_size device=self device_type requires_grad=True mask = torch rand input_size device=self device_type requires_grad=False _compare_pairwise_ops device_mesh=device_mesh placements=placements op=torch ops aten native_dropout_backward kwargs=dict grad_output=grad_output mask=mask scale= test_dropout_errors device_mesh = build_device_mesh assertRaisesRegex RuntimeError supported _run_sharded_elementwise_ops device_mesh=device_mesh placements= Partial sum input_size= op=torch nn functional dropout test_mul_out device_mesh = build_device_mesh torch manual_seed rank shard_spec = Shard input_size = input_tensor = torch randn input_size device=self device_type dtensor = DTensor from_local input_tensor device_mesh shard_spec other_tensor = torch randn input_size device=self device_type other_dtensor = DTensor from_local other_tensor device_mesh shard_spec output_tensor = torch randn input_size device=self device_type output_dtensor = DTensor from_local output_tensor device_mesh shard_spec dt = torch mul dtensor other_dtensor out=output_dtensor expected = torch mul input_tensor other_tensor out=output_tensor assertEqual input_tensor dtensor to_local assertEqual expected dt to_local test_mul_partial we only test partial behavior mul op other placement behaviors should well tested test_dtensor_ops py device_mesh = build_device_mesh comm_mode = CommDebugMode simple test partial partial d_ = DTensor from_local torch ones device_mesh Partial d_ = DTensor from_local torch ones device_mesh Partial comm_mode d_ = d_ d_ comm_counts = comm_mode get_total_counts assertEqual comm_counts assertTrue isinstance d_ DTensor assertEqual d_ placements Partial assertEqual d_ to_local torch ones world_size test partial input DTensor scalar replicate input input = torch full device=self device_type test different types other inputs other_inps = scalar torch tensor device=self device_type scalar tensor torch full device=self device_type tensor partial_op sum avg expected_p_out = input world_size partial_op == sum input d_input = DTensor from_local input device_mesh Partial partial_op other_inp other_inps isinstance other_inp Tensor other_inp numel d_other = distribute_tensor other_inp device_mesh Replicate d_other = other_inp comm_mode z = d_input d_other comm_counts = comm_mode get_total_counts assertEqual comm_counts assertTrue isinstance z DTensor assertEqual z placements Partial partial_op assertEqual z full_tensor expected_p_out test other partial assert partial getting propagated d_input = DTensor from_local input device_mesh Partial max d_other = distribute_tensor torch ones device_mesh Replicate z = d_input d_other assertEqual z placements Replicate assertEqual z to_local input __name__ == __main__ run_tests