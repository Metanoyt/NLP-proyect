argparse datetime logging re sys collections defaultdict torch torch _C parse_schema Tag FORMAT = levelname s asctime s filename s lineno s message s log = logging getLogger log log setLevel logging INFO handler = logging StreamHandler formatter = logging Formatter FORMAT handler setFormatter formatter log addHandler handler log propagate = False Avoid double logging root logger has handlers How run test locally Have two virtual environments eg conda env one without PyTorch installed venv_nightly one your local changes venv_yours In venv_nightly First ensure Pytorch uninstalled all prereqs installed Install torch nightly build ` pip install -- pre torch -f https download pytorch org whl nightly cpu torch_nightly html ` Generate original schemas ` python test forward_backward_compatibility dump_all_function_schemas py -- filename nightly_schemas txt ` Now venv_yours Run test ` python test forward_backward_compatibility check_forward_backward_compatibility py -- existing-schemas nightly_schemas txt ` The date specifies how long allowlist exclusion should apply Note core ATen opset https pytorch org docs stable torch compiler_ir html#core-aten-ir guaranteed BC based policy https dev-discuss pytorch org t core-aten-opset-backward-forward-compatibility-policy hence allowlist does apply date always arbitrarily far core ATen ops - If we NEVER give BC guarantee operator you can put date arbitrarily far future - Otherwise pick date far enough future you believe you can land your diff before then Allowlist entries can removed after date listed them passes Allowlist item format function name regex date until which allowlist entry valid optional default None function argument regex optional default False If True tells us you NOT core ATen op See Note Op removal core ATen detection NB function name DOES NOT include overload name ALLOW_LIST = c _experimental datetime date Internal static datetime date prim ModuleDictIndex datetime date prim MKLDNNRelu datetime date prim MKLDNNRelu _ datetime date prim is_ort datetime date prim Concat datetime date aten _NestedTensor_GeneralizedBMM datetime date Internal profiler-specific ops profiler _call_end_callbacks_on_jit_fut datetime date profiler _record_function_enter datetime date aten _cholesky_helper datetime date aten _cslt_sparse_mm datetime date aten _lstsq_helper datetime date aten _syevd_helper datetime date aten _linalg_solve_out_helper_ datetime date aten select_backward datetime date aten lstsq datetime date aten lstsq X datetime date aten slice_backward datetime date aten diagonal_backward datetime date aten rowwise_prune datetime date aten eig datetime date aten eig e datetime date aten adaptive_avg_pool d_backward datetime date aten _embedding_bag_dense_backward datetime date aten matrix_rank datetime date aten matrix_rank tol datetime date aten randperm datetime date aten solve datetime date aten solve solution datetime date aten _solve_helper datetime date aten _convolution_nogroup datetime date aten miopen_convolution_backward datetime date aten miopen_convolution_backward_bias datetime date aten miopen_convolution_backward_input datetime date aten miopen_convolution_backward_weight datetime date aten miopen_convolution_transpose_backward datetime date aten miopen_convolution_transpose_backward_input datetime date aten miopen_convolution_transpose_backward_weight datetime date aten miopen_depthwise_convolution_backward datetime date aten miopen_depthwise_convolution_backward_input datetime date aten miopen_depthwise_convolution_backward_weight datetime date aten _nested_tensor datetime date prepacked unpack_prepacked_sizes_conv d datetime date prepacked unpack_prepacked_sizes_linear datetime date aten _symeig_helper datetime date aten symeig datetime date aten symeig e datetime date aten native_multi_head_self_attention datetime date aten _native_multi_head_self_attention datetime date aten grid_sampler_ d_backward datetime date aten _transform_bias_rescale_qkv datetime date prim infer_squeeze_size dim datetime date prim infer_squeeze_size datetime date aten _weight_norm_cuda_interface datetime date aten _weight_norm_cuda_interface_backward datetime date aten empty SymInt datetime date nested tensor temporary auxiliary ops aten _reshape_nested datetime date aten _reshape_nested_backward datetime date aten mps_linear datetime date aten _mps_linear datetime date aten _mps_max_pool d datetime date aten _mps_max_pool d out datetime date aten mps_max_pool d_backward datetime date aten mps_max_pool d_backward out datetime date TODO FIXME prims shouldn t checked prims datetime date aten _scaled_dot_product_cudnn_attention datetime date BetterTransformer internal operators aten _transformer_decoder_only_layer_fwd datetime date aten _native_decoder_only_multi_head_attention datetime date These ops moved python under c d_functional namespace aten wait_tensor datetime date aten reduce_scatter_tensor datetime date aten all_gather_into_tensor datetime date aten all_reduce datetime date These ops defined torch csrc distributed c d Ops cpp TODO add back restriction when c d ops can exported c d datetime date Previously MPS_only did support backward aten _fused_rms_norm datetime date ALLOW_LIST_COMPILED = re compile item item re compile item len item item None None item len item False item ALLOW_LIST item = datetime date today allow_listed schema item ALLOW_LIST_COMPILED item search str schema len item item None arguments regex present use bool item search str schema item True item False None The nightly will fail parse newly added syntax schema declarations Add new schemas will fail nightly here dont_parse_list = _TorchScriptTesting datetime date test_backend datetime date dist_c d datetime date __backends__ nnc datetime date has_valid_upgraders schema version_map we want parse through map find schema has valid upgraders Since version map has entry each overload we need do some ugly parsing name operator schema_name = schema name schema_name version_map False entries = version_map schema_name possible_overloads = possible_schemas = key upgrader_schema_entries entries items possible_overloads append key possible_schemas extend upgrader_schema_entries let s make sure existing schema part possible schemas old_schema possible_schemas old_schema == schema True False dont_parse schema_line item dont_parse_list item datetime date today continue regexp = re compile item regexp search schema_line True False load_schemas_to_dict new_schemas = torch _C _jit_get_all_schemas new_schemas += torch _C _jit_get_custom_class_schemas new_schema_dict = defaultdict list s new_schemas new_schema_dict s name append s new_schema_dict process_version_map version_map version map maps full schema name list upgraders Since we only have name schema aka no overload we want first process map make key lookup easier After will Dict schema_name Dict overload List schema output = defaultdict dict key entries version_map items operator_name = key split schema_entries = parse_schema entry old_schema entry entries output operator_name key = schema_entries output is_core_aten_op schema - bool Check schema core ATen op schema name False res = torch _C _get_operation_overload schema name schema overload_name res None Note Op removal core ATen detection If core ATen op has been removed we cannot sure whether previously core ATen op via checking tags way Conservatively assume you ARE core ATen op case This means deleting core ATen op will still caught But you re deleting operator core ATen op add allow_list you would need additionally specify flag ALLOW_LIST tell us you core ATen op See comment block above ALLOW_LIST more info See https github com pytorch pytorch issues True _ _ tags = res Tag core tags check_bc existing_schemas new_schema_dict = load_schemas_to_dict version_map = process_version_map torch _C _get_operator_version_map is_bc = True broken_ops = existing_schema existing_schemas is_allow_list trust_not_core_aten = allow_listed existing_schema is_allow_list trust_not_core_aten is_core_aten_op existing_schema log info schema s found allowlist skipping existing_schema continue log info schema s found allowlist core ATen op checking BC NOTE If you have removed operator we will conservatively assume core ATen op If operator you removed core ATen op please specify ALLOW_LIST entry see comment block top ALLOW_LIST more info existing_schema has_valid_upgraders existing_schema version_map is_core_aten_op existing_schema log info schema s has valid upgrader skipping existing_schema continue log info schema s has valid upgrader core ATen op checking BC log debug processing existing schema s existing_schema matching_new_schemas = new_schema_dict get existing_schema name found = False matching_new_schema matching_new_schemas matching_new_schema is_backward_compatible_with existing_schema found = True break found log warning Can NOT find backward compatible schemas after changes schema s following candidates \n \n s\n str existing_schema \n\t join str s s matching_new_schemas TODO Print out more details about why candidates don t match broken_ops append str existing_schema is_bc = False is_bc log info Found backward compatible schemas all existing schemas log warning The PR introducing backward incompatible changes operator library Please contact PyTorch team confirm whether change wanted \n\nBroken ops \n\t s\n \n\t join broken_ops is_bc check_fc existing_schemas new_schema_dict = load_schemas_to_dict is_fc = True broken_ops = existing_schema existing_schemas is_allow_list _ = allow_listed existing_schema is_allow_list log info schema s found allowlist skipping existing_schema continue log info processing existing schema s existing_schema matching_new_schemas = new_schema_dict get existing_schema name found = False possible_failure_reasons = matching_new_schema matching_new_schemas is_compatible reason = matching_new_schema check_forward_compatible_with existing_schema is_compatible found = True break reason = possible_failure_reasons append reason found log warning Can NOT find forward compatible schemas after changes schema s following candidates \n \n\t s\n str existing_schema \n\t join str s s matching_new_schemas log warning Refer following reasons failure find FC schema \n \n s\n \n\t join str r r possible_failure_reasons broken_ops append str existing_schema is_fc = False is_fc log info Found forward compatible schemas all existing schemas log warning The PR introducing potentially forward incompatible changes operator library Please contact PyTorch team confirm whether change wanted \n\nBroken ops \n\t s\n \n\t join broken_ops __name__ == __main__ parser = argparse ArgumentParser description= Process some integers parser add_argument -- existing-schemas help= filename load existing schemas type=str default= schemas txt args = parser parse_args existing_schema_dict = slist = open args existing_schemas f while True line = f readline line break dont_parse line strip log info Not parsing schema line s line strip continue s = parse_schema line strip slist append s TODO case there FC breaking changes we just warn now until there policy check_fc slist check_bc slist sys exit