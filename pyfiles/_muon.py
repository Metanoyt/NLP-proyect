mypy allow-untyped-defs mypy disable-error-code=arg-type Implementation Muon optimizer math collections abc MutableMapping typing Optional torch torch Tensor optimizer _disable_dynamo_if_unsupported _params_doc _to_scalar Optimizer ParamsT __all__ = Muon Constants Keller Jordan s Muon post https kellerjordan github io posts muon github permlink https github com KellerJordan Muon blob f b e b d d d fe d f abcbcbd muon py#L EPS = e- DEFAULT_A = DEFAULT_B = - DEFAULT_C = DEFAULT_NS_STEPS = _zeropower_via_newtonschulz grad Tensor ns_coefficients tuple float float float ns_steps int eps float - Tensor Newton-Schulz iteration compute zeroth power orthogonalization G We opt use quintic iteration whose coefficients selected maximize slope zero For purpose minimizing steps turns out empirically effective keep increasing slope zero even beyond point where iteration no longer converges all way one everywhere interval This iteration therefore does produce UV^T rather something like US V^T where S diagonal S_ ii ~ Uniform which turns out hurt model performance all relative UV^T where USV^T = G SVD Implementation reference https github com KellerJordan Muon blob master muon py suggestions jxbz leloykun YouJiacheng ns_steps = raise ValueError Number steps must less than computational efficiency len grad shape = raise ValueError Input tensor gradient must D matrix len ns_coefficients = raise ValueError Coefficients must tuple exactly values b c = ns_coefficients ortho_grad = grad bfloat grad size grad size ortho_grad = ortho_grad T Ensure spectral norm most ortho_grad div_ ortho_grad norm clamp min=eps Perform NS iterations _ range ns_steps gram_matrix = ortho_grad ortho_grad T gram_update = torch addmm gram_matrix gram_matrix gram_matrix beta=b alpha=c ortho_grad = torch addmm ortho_grad gram_update ortho_grad beta=a grad size grad size ortho_grad = ortho_grad T ortho_grad _adjust_lr lr float adjust_lr_fn Optional str param_shape torch Size - float Default learning rate adjustment used Muon A B = param_shape adjust_lr_fn None adjust_lr_fn == original pyrefly ignore no-matching-overload adjusted_ratio = math sqrt max A B adjust_lr_fn == match_rms_adamw adjusted_ratio = math sqrt max A B adjusted_ratio = lr adjusted_ratio Muon Optimizer __init__ params ParamsT lr float = e- weight_decay float = momentum float = nesterov bool = True ns_coefficients tuple float float float = DEFAULT_A DEFAULT_B DEFAULT_C eps float = EPS ns_steps int = DEFAULT_NS_STEPS adjust_lr_fn Optional str = None - None isinstance lr Tensor lr numel = raise ValueError Tensor lr must -element = lr raise ValueError f Learning rate should = lr = momentum raise ValueError f momentum should = momentum = weight_decay raise ValueError f weight decay should = weight_decay adjust_lr_fn None adjust_lr_fn original match_rms_adamw raise ValueError f Adjust learning rate function adjust_lr_fn supported defaults = lr lr weight_decay weight_decay momentum momentum nesterov nesterov ns_coefficients ns_coefficients eps eps ns_steps ns_steps adjust_lr_fn adjust_lr_fn super __init__ params defaults group param_groups p group params p ndim = raise ValueError f Muon only supports D parameters whereas we found parameter size p size _init_group group MutableMapping params_with_grad list Tensor grads list Tensor muon_momentum_bufs list Tensor p group params p grad None continue torch is_complex p raise RuntimeError Muon does support complex parameters p grad is_sparse raise RuntimeError Muon does support sparse gradients params_with_grad append p grads append p grad state = state p momentum_buffer state state momentum_buffer = torch zeros_like p grad memory_format=torch preserve_format muon_momentum_bufs append state momentum_buffer False has_complex torch no_grad step closure=None Performs single optimization step loss = None closure None torch enable_grad loss = closure group param_groups lr = group lr weight_decay = group weight_decay momentum = group momentum params_with_grad list Tensor = grads list Tensor = muon_momentum_bufs list Tensor = has_complex = _init_group group params_with_grad grads muon_momentum_bufs muon params_with_grad grads muon_momentum_bufs lr=lr weight_decay=weight_decay momentum=momentum nesterov=group nesterov ns_coefficients=group ns_coefficients eps=group eps ns_steps=group ns_steps adjust_lr_fn=group adjust_lr_fn has_complex=has_complex loss Muon __doc__ = r Implements Muon algorithm math \begin aligned \rule mm pt \\ \textbf input \gamma \text lr \ \lambda \text weight decay \ \mu \text momentum \ \textit nesterov \in\ True False\ \\ \hspace mm b c \ \text NS coefficients \ \varepsilon \text epsilon \ k \text NS steps \ \theta_ \text params \ f \theta \text objective \\ \textbf initialize B_ \leftarrow \text momentum buffer \\ - ex \rule mm pt \\ \textbf \ t= \ \textbf \ \ldots\ \textbf do \\ ex \hspace mm g_t \leftarrow \nabla_ \theta f_t \theta_ t- \\ ex \hspace mm B_t \leftarrow \mu B_ t- + g_t \\ ex \hspace mm \widetilde B _t \leftarrow \begin cases g_t + \mu B_t \text nesterov =True \\ B_t \text nesterov =False \end cases \\ ex \hspace mm O_t \leftarrow \mathrm NS ^ b c _ k \ \big \widetilde B _t \ \varepsilon\big \\ ex \hspace mm \theta_t \leftarrow \theta_ t- - \gamma\ \lambda\ \theta_ t- \quad\text decoupled weight decay \\ ex \hspace mm \gamma \leftarrow \mathrm AdjustLR \ \big \gamma \ \mathrm shape \ \big \theta_t \big \big \\ ex \hspace mm \theta_t \leftarrow \theta_t - \gamma\ O_t \\ \rule mm pt \\ - ex \mathbf \ \theta_t \\ - ex \rule mm pt s \end aligned Here math ` \mathrm NS ^ b c _ k \cdot \varepsilon ` denotes math ` k ` iterations Newton – Schulz orthogonalization operator parameterized coefficients math ` b c ` numerical stabilization math ` \varepsilon ` The purpose math ` \mathrm AdjustLR \ \big \gamma \ \mathrm shape \ \big \theta_t \big \big ` make orthogonalized update have consistent math ` RMS ` across rectangular matrices Keller s original implementation scales update math ` \sqrt \max\ \left \frac A B \right ` where math ` A ` math ` B ` dimension matrix being optimized Moonshot s implementation also focuses matching math ` RMS ` AdamW The adjustment computed math ` \gamma \leftarrow \gamma\ \sqrt \max\ \left A B \right ` The method adopted ` Muon Scalable LLM Training ` _ Research results show adjustment Muon can directly reuse learning rate weight decay tuned AdamW We provide two options learning rate adjustment original which follows Keller s implementation match_rms_adamw which refers Moonshot s implementation This gives users flexibility choose between two If ` adjust_lr_fn ` specified default original For further details regarding algorithm we refer ` Muon An optimizer hidden layers neural networks ` _ ` Muon Scalable LLM Training ` _ + rf Args _params_doc Note Muon optimizer D parameters neural network hidden layers Other parameters such bias embedding should optimized standard method such AdamW lr float Tensor optional learning rate default e- weight_decay float optional weight decay L penalty default momentum float optional momentum factor default nesterov bool optional enables Nesterov momentum Only applicable when momentum non-zero ns_coefficients tuple three floats optional coefficients \ b c\ Newton – Schulz orthogonalization polynomial default DEFAULT_A DEFAULT_B DEFAULT_C eps float optional term added denominator numerical stability default EPS ns_steps int optional number Newton – Schulz iteration steps default DEFAULT_NS_STEPS adjust_lr_fn str optional function adjust learning rate One original match_rms_adamw If specified we will default use original default None _Muon\ An optimizer hidden layers neural networks https kellerjordan github io posts muon _Muon Scalable LLM Training https arxiv org pdf _single_tensor_muon params list Tensor grads list Tensor muon_momentum_bufs list Tensor lr float weight_decay float momentum float nesterov bool ns_coefficients tuple float float float ns_steps int eps float adjust_lr_fn Optional str has_complex bool - None lr = _to_scalar lr has_complex raise ValueError Complex parameters supported i param enumerate params grad = grads i grad ndim = raise ValueError Param gradient must D matrix buf = muon_momentum_bufs i buf lerp_ grad - momentum update = grad lerp buf momentum nesterov buf update = _zeropower_via_newtonschulz update ns_coefficients ns_steps eps adjusted_lr = _adjust_lr lr adjust_lr_fn param shape param mul_ - lr weight_decay param add_ update alpha=-adjusted_lr _disable_dynamo_if_unsupported single_tensor_fn=_single_tensor_muon muon params list Tensor grads list Tensor muon_momentum_bufs list Tensor foreach Optional bool = None lr float weight_decay float momentum float nesterov bool ns_coefficients tuple float float float ns_steps int eps float adjust_lr_fn Optional str has_complex bool r Functional API performs Muon algorithm computation See ` ~torch optim Muon ` details foreach None foreach raise RuntimeError Foreach supported Muon yet func = _single_tensor_muon func params grads muon_momentum_bufs lr=lr weight_decay=weight_decay momentum=momentum nesterov=nesterov ns_coefficients=ns_coefficients ns_steps=ns_steps eps=eps adjust_lr_fn=adjust_lr_fn has_complex=has_complex