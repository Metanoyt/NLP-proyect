Owner s NNC ruff noqa F numpy np torch torch nn functional F torch nn unittest itertools torch testing _internal common_utils suppress_warnings num_profiled_runs run_tests skipIfTorchDynamo torch testing _internal jit_utils JitTestCase TensorExprTestOptions LLVM_ENABLED = torch _C _llvm_enabled BaseTestClass JitTestCase setUp super setUp tensorexpr_options = TensorExprTestOptions devices = cpu torch cuda is_available cpu cuda dtypes = torch float torch bfloat LLVM_ENABLED torch float tearDown tensorexpr_options restore super tearDown assertLastGraphAllFused assertAllFused torch jit last_executed_optimized_graph warmup_and_run_forward f args _ range torch _C _jit_get_num_profiled_runs + results = f args results skipIfTorchDynamo TestTensorExprFuser BaseTestClass test_easy easy x y aaa = torch add x y aaa traced = torch jit trace easy torch rand torch rand = torch rand b = torch rand x = warmup_and_run_forward traced b assertLastGraphAllFused np testing assert_allclose numpy + b numpy x numpy test_three_arg easy x y z aaa = torch add x y bbb = torch add aaa z bbb traced = torch jit trace easy torch rand torch rand torch rand = torch rand b = torch rand c = torch rand x = warmup_and_run_forward traced b c assertLastGraphAllFused npr = numpy + b numpy + c numpy np testing assert_allclose npr x numpy test_four_arg run_addcmul x y z w c = torch addcmul torch add x y z w c dev devices rand_a = torch rand dtype=torch float device=dev rand_b = torch rand dtype=torch float device=dev rand_c = torch rand dtype=torch float device=dev rand_d = torch rand dtype=torch float device=dev traced = torch jit trace run_addcmul torch zeros dtype=torch float device=dev torch zeros dtype=torch float device=dev torch zeros dtype=torch float device=dev torch zeros dtype=torch float device=dev x = warmup_and_run_forward traced rand_a rand_b rand_c rand_d assertLastGraphAllFused y = run_addcmul rand_a rand_b rand_c rand_d np testing assert_allclose x cpu numpy y cpu numpy atol= e- test_three_arg device devices test x y z aaa = torch add x y bbb = torch add aaa z bbb M = N = traced = torch jit trace test torch rand M N device=device torch rand M N device=device torch rand M N device=device = torch rand M N device=device b = torch rand M N device=device c = torch rand M N device=device x = traced b c x = warmup_and_run_forward traced b c assertLastGraphAllFused npr = cpu numpy + b cpu numpy + c cpu numpy np testing assert_allclose npr x cpu numpy test_broadcast device devices test_body M N L K test x y z v = torch add x y v = torch add v z v a_shape = M N b_shape = L M c_shape = K L traced = torch jit trace test torch rand a_shape device=device torch rand b_shape device=device torch rand c_shape device=device = torch rand a_shape device=device b = torch rand b_shape device=device c = torch rand c_shape device=device x = warmup_and_run_forward traced b c assertLastGraphAllFused npr = cpu numpy + b cpu numpy + c cpu numpy np testing assert_allclose npr x cpu numpy test_configs = test_config test_configs test_body test_config test_all_combos easy x y z = torch add x y b = torch add z c = torch add x b d = torch add c d np_easy x y z = x + y b = + z c = x + b d = c + d traced = torch jit trace easy torch rand torch rand torch rand = torch rand b = torch rand c = torch rand x = warmup_and_run_forward traced b c assertLastGraphAllFused npr = np_easy numpy b numpy c numpy np testing assert_allclose npr x numpy test_rank_two easy x y z = torch add x y b = torch add z c = torch add x b d = torch add c d np_easy x y z = x + y b = + z c = x + b d = c + d shape = traced = torch jit trace easy torch rand shape torch rand shape torch rand shape = torch rand shape b = torch rand shape c = torch rand shape x = warmup_and_run_forward traced b c assertLastGraphAllFused npr = np_easy numpy b numpy c numpy np testing assert_allclose npr x numpy test_broadcast easy x y z = torch add x y b = torch add z b np_easy x y z = x + y b = + z b N = traced = torch jit trace easy torch rand N N torch rand N torch rand N N = torch rand N N b = torch rand N c = torch rand N N x = warmup_and_run_forward traced b c assertLastGraphAllFused npr = np_easy numpy b numpy c numpy np testing assert_allclose npr x numpy test_broadcast_ zero = torch tensor dtype=torch float foo x y z aaa = torch add x y bbb = torch add zero aaa torch add bbb z foo_np x y z = x + y b = zero numpy + b + z x = torch rand y = torch ones z = torch rand traced = torch jit trace foo x y z r = warmup_and_run_forward traced x y z assertLastGraphAllFused rnp = foo_np x numpy y numpy z numpy np testing assert_allclose r rnp test_broadcast_big zero = torch tensor dtype=torch float foo x y z aaa = torch add x y bbb = torch add zero aaa torch add bbb z foo_np x y z = x + y b = zero numpy + b + z x = torch rand y = torch ones z = torch rand traced = torch jit trace foo x y z r = warmup_and_run_forward traced x y z assertLastGraphAllFused rnp = foo_np x numpy y numpy z numpy np testing assert_allclose r rnp test_alpha alpha x aaa = torch add x x alpha= aaa traced = torch jit trace alpha torch tensor = torch tensor x = traced np testing assert_allclose numpy + numpy x numpy suppress_warnings test_constant constant x bbb = torch tensor aaa = torch add x bbb aaa traced = torch jit trace constant torch tensor = torch tensor x = warmup_and_run_forward traced assertLastGraphAllFused np testing assert_allclose numpy + x numpy test_add_sub easy x y z aaa = torch add x y bbb = torch sub aaa z bbb traced = torch jit trace easy torch rand torch rand torch rand = torch rand b = torch rand c = torch rand x = warmup_and_run_forward traced b c assertLastGraphAllFused np testing assert_allclose numpy + b numpy - c numpy x numpy test_promotion easy x y aaa = torch add x y aaa traced = torch jit trace easy torch zeros dtype=torch int torch rand dtype=torch float = torch zeros dtype=torch int b = torch rand dtype=torch float x = warmup_and_run_forward traced b assertLastGraphAllFused np testing assert_allclose numpy + b numpy x numpy test_double TENSOR_LEN = easy x y aaa = torch add x y bbb = torch mul aaa y bbb traced = torch jit trace easy torch rand TENSOR_LEN dtype=torch float torch full TENSOR_LEN dtype=torch float = torch rand TENSOR_LEN dtype=torch double b = torch full TENSOR_LEN dtype=torch double x = warmup_and_run_forward traced b assertLastGraphAllFused np testing assert_allclose numpy + b numpy b numpy x numpy test_short TENSOR_LEN = easy x y aaa = torch add x y bbb = torch mul aaa y bbb traced = torch jit trace easy torch randint TENSOR_LEN TENSOR_LEN dtype=torch int torch randint TENSOR_LEN TENSOR_LEN dtype=torch int = torch randint TENSOR_LEN TENSOR_LEN dtype=torch int b = torch randint TENSOR_LEN TENSOR_LEN dtype=torch int x = warmup_and_run_forward traced b assertLastGraphAllFused np testing assert_allclose numpy + b numpy b numpy x numpy test_char TENSOR_LEN = easy x y aaa = torch add x y bbb = torch mul aaa y bbb traced = torch jit trace easy torch randint TENSOR_LEN TENSOR_LEN dtype=torch int torch randint TENSOR_LEN TENSOR_LEN dtype=torch int = torch randint TENSOR_LEN TENSOR_LEN dtype=torch int b = torch randint TENSOR_LEN TENSOR_LEN dtype=torch int x = warmup_and_run_forward traced b assertLastGraphAllFused np testing assert_allclose numpy + b numpy b numpy x numpy test_int _promotion TENSOR_LEN = easy x y aaa = torch add x y bbb = torch mul aaa y bbb traced = torch jit trace easy torch randint TENSOR_LEN TENSOR_LEN dtype=torch int torch randint TENSOR_LEN TENSOR_LEN dtype=torch int = torch randint TENSOR_LEN TENSOR_LEN dtype=torch int b = torch randint TENSOR_LEN TENSOR_LEN dtype=torch int x = warmup_and_run_forward traced b assertLastGraphAllFused np testing assert_allclose numpy + b numpy b numpy x numpy test_eq easy x y c = torch eq x y c traced = torch jit trace easy torch zeros torch zeros = torch zeros dtype=torch int b = torch zeros dtype=torch int x = warmup_and_run_forward traced b assertLastGraphAllFused np testing assert_allclose np ones x numpy test_ne easy x y c = torch ne x y c traced = torch jit trace easy torch zeros torch zeros = torch zeros dtype=torch int b = torch ones dtype=torch int x = warmup_and_run_forward traced b assertLastGraphAllFused np testing assert_allclose np ones x numpy test_ge easy x y c = torch ge x y c traced = torch jit trace easy torch zeros torch zeros aa = np empty dtype=np int aa fill = torch from_numpy aa b = torch zeros dtype=torch int x = warmup_and_run_forward traced b assertLastGraphAllFused np testing assert_allclose np ones x numpy test_gt easy x y c = torch gt x y c traced = torch jit trace easy torch zeros torch zeros = torch ones dtype=torch int b = torch zeros dtype=torch int x = warmup_and_run_forward traced b assertLastGraphAllFused np testing assert_allclose np ones x numpy test_le easy x y c = torch le x y c traced = torch jit trace easy torch zeros torch zeros aa = np empty dtype=np int aa fill = torch from_numpy aa b = torch zeros dtype=torch int x = warmup_and_run_forward traced b assertLastGraphAllFused np testing assert_allclose np zeros x numpy test_lt easy x y c = torch lt x y c dev devices traced = torch jit trace easy torch zeros device=dev torch zeros device=dev = torch ones dtype=torch int device=dev b = torch zeros dtype=torch int device=dev x = warmup_and_run_forward traced b assertLastGraphAllFused np testing assert_allclose np zeros x cpu numpy suppress_warnings test_min_max test x y torch max torch min x y torch tensor traced = torch jit trace test torch zeros torch zeros = torch rand b = torch rand np testing assert_allclose warmup_and_run_forward traced b np maximum np minimum numpy b numpy assertLastGraphAllFused test_min_max_reduction test x torch min x + torch max x traced = torch jit trace test torch zeros = torch rand np testing assert_allclose warmup_and_run_forward traced np amin numpy + np amax numpy assertLastGraphAllFused test_min_max_reduction test x x min + x max traced = torch jit trace test torch zeros = torch rand np testing assert_allclose warmup_and_run_forward traced np amin numpy + np amax numpy assertLastGraphAllFused test_min_max_reduction_dim test x torch min x + torch max x traced = torch jit trace test torch zeros = torch rand np testing assert_allclose warmup_and_run_forward traced np amin numpy axis= + np amax numpy axis= assertLastGraphAllFused test_min_max_reduction_dim _ test x torch min x x traced = torch jit trace test torch zeros = torch rand np testing assert_allclose warmup_and_run_forward traced np amin numpy axis= assertLastGraphAllFused test_clamp test x torch clamp x + dev devices traced = torch jit trace test torch zeros device=dev = torch rand device=dev - = cpu numpy np testing assert_allclose warmup_and_run_forward traced cpu np clip + assertLastGraphAllFused test_relu test x torch clamp F relu x dev devices traced = torch jit trace test torch zeros device=dev = torch rand device=dev - = cpu numpy np testing assert_allclose warmup_and_run_forward traced cpu np clip np maximum assertLastGraphAllFused test_reps easy x y c = torch add x y c traced = torch jit trace easy torch rand torch rand _ range = torch ones b = torch zeros x = warmup_and_run_forward traced b np testing assert_allclose np ones x numpy test_add_const_rhs test x x + traced = torch jit trace test torch rand x = torch rand y = warmup_and_run_forward traced x assertLastGraphAllFused np testing assert_allclose x numpy + y numpy test_int_output test x y z x y z xs = torch rand + torch int i range x y z = xs xn yn zn = t numpy t xs traced = torch jit trace test x y z res = warmup_and_run_forward traced x y z assertLastGraphAllFused np testing assert_allclose xn yn zn res numpy test_binary_ops test_atan x y c = torch atan torch add x y y c test_gt x y c = torch gt torch add x y y c test_ge x y c = torch ge torch add x y y c test_lt x y c = torch lt torch add x y y c test_le x y c = torch le torch add x y y c test_lerp x y c = torch lerp torch add x x c test_mul x y c = torch mul torch add x y y c test_ne x y c = torch ne torch add x y y c test_div x y c = torch div torch add x y c test_eq x y c = torch eq torch add x y y c test_fmod x y c = torch fmod torch add x y c test_sub x y c = torch sub torch add x y x c test_remainder x y c = torch remainder torch add x y c test_pow x y c = torch pow torch add x y c test_type_as x y x type_as torch add x y cmp_fns = test_gt test_ge test_lt test_le test_ne test_eq non_cmp_fns = test_atan test_lerp test_mul test_div test_fmod test_sub test_remainder test_pow test_type_as all_test_fns = cmp_fns union non_cmp_fns fn_dev_dtype = itertools product all_test_fns devices dtypes torch_fn dev data_type fn_dev_dtype torch_fn test_lerp data_type torch bfloat continue rand_a = torch rand dtype=data_type device=dev rand_b = torch rand dtype=data_type device=dev = torch rand dtype=data_type device=dev = torch rand dtype=data_type device=dev traced = torch jit trace torch_fn x = warmup_and_run_forward traced rand_a rand_b assertLastGraphAllFused _atol = e- _rtol = e- data_type torch bfloat Compared aten logic NNC could save additional BF Fp conversion Take d = + b - c example aten logic follows operator level tmp = to_bf to_fp + to_fp b d = to_bf to_fp tmp + to_fp c But NNC could fuse compression remove redundant conversions The final statement follows d = to_bf to_fp + to_fp b + to_fp c Hence we simulate NNC computation feeding fp tensors converting result tensor back bf The simulation could avoid numeric deviation simplify result comparison y = warmup_and_run_forward traced rand_a float rand_b float torch_fn cmp_fns y = y bfloat _atol = e- y = torch_fn rand_a rand_b assertEqual x cpu y cpu atol=_atol rtol=_rtol test_unary_ops test_cast_float x y c = torch ops aten _cast_Float torch add x y c test_round x y c = torch round torch add x y c test_sin x y c = torch sin torch add x y c test_asin x y c = torch asin torch add x y c test_sinh x y c = torch sinh torch add x y c test_cos x y c = torch cos torch add x y c test_acos x y c = torch acos torch add x y c test_cosh x y c = torch cosh torch add x y c test_tan x y c = torch tan torch add x y c test_atan x y c = torch atan torch add x y c test_tanh x y c = torch tanh torch add x y c test_sqrt x y c = torch sqrt torch add x y c test_rsqrt x y c = torch rsqrt torch add x y c test_floor x y c = torch floor torch add x y c test_ceil x y c = torch ceil torch add x y c test_trunc x y c = torch trunc torch add x y c test_abs x y c = torch abs torch add x y c test_log x y c = torch log torch add x y c test_log x y c = torch log torch add x y c test_log x y c = torch log torch add x y c test_log p x y c = torch log p torch add x y c test_rqrt x y c = torch rsqrt torch add x y c test_erf x y c = torch erf torch add x y c test_exp x y c = torch exp torch add x y c test_expm x y c = torch expm torch add x y c test_erfc x y c = torch erfc torch add x y c test_frac x y c = torch frac torch add x y c test_lgamma x y c = torch lgamma torch add x y c test_sigmoid x y c = torch sigmoid torch add x y c test_reciprocal x y c = torch reciprocal torch add x y c test_neg x y c = torch neg torch add x y c test_relu x y c = torch relu torch add x y c test_hardtanh x y c = F hardtanh torch add x y - c test_threshold x y c = F threshold torch add x y c gpu_only_fns = test_erf test_erfc fns = test_round test_sin test_asin test_sinh test_cos test_acos test_cosh test_tan test_atan test_sqrt test_floor test_ceil test_trunc test_abs test_log test_log test_log test_log p test_rsqrt test_exp test_expm test_frac test_lgamma test_reciprocal test_neg test_threshold test_relu test_tanh test_hardtanh test_sigmoid fn_dev_dtype = itertools product gpu_only_fns union fns devices dtypes torch manual_seed torch_fn dev data_type fn_dev_dtype torch_fn == test_lgamma dev == cuda lgamma_cuda does support BF continue rand_a = torch rand dtype=data_type device=dev rand_b = torch rand dtype=data_type device=dev ins = torch rand dtype=data_type device=dev cc = np empty dtype=np float cc fill np nan nans = torch from_numpy cc dev traced = torch jit trace torch_fn ins ins x = warmup_and_run_forward traced rand_a rand_b assertLastGraphAllFused _atol = e- data_type torch bfloat e- _rtol = e- data_type torch bfloat torch_fn gpu_only_fns y = warmup_and_run_forward traced rand_a float rand_b float y = y bfloat y = torch_fn rand_a rand_b assertEqual x cpu y cpu atol=_atol rtol=_rtol nans TODO reenable Currently all tests fail traced = torch jit trace torch_fn ins ins x = warmup_and_run_forward traced rand_a rand_b y = torch_fn nans rand_b try np testing assert_allclose x cpu numpy y cpu numpy print Succeeded dev= dev function= torch_fn except AssertionError Print extra info before exiting print Failed dev= dev function= torch_fn np testing assert_allclose x cpu numpy y cpu numpy test_round_ round x torch round x data_type torch float torch double = torch tensor data_type traced = torch jit trace round x = warmup_and_run_forward traced assertLastGraphAllFused y = round x assertEqual x y test_rand_like N = run_rand_like x y torch rand_like torch add x y device devices x = torch rand N device=device traced = torch jit trace run_rand_like x x check_trace=False data_type dtypes _x = x dtype=data_type x_v = warmup_and_run_forward traced _x _x assertLastGraphAllFused x_np = x cpu numpy x _mean = np mean x_np x _mean = np mean x_np x _mean = np mean x_np np testing assert_allclose x _mean rtol= e- np testing assert_allclose x _mean rtol= e- np testing assert_allclose x _mean rtol= e- test_nans test_max x y torch max x y test_min x y torch min x y tmax = torch jit trace test_max torch rand torch rand tmin = torch jit trace test_min torch rand torch rand data_type dtypes x = torch tensor np nan dtype=data_type y = torch tensor dtype=data_type assert np isnan warmup_and_run_forward tmin x y float item assert np isnan warmup_and_run_forward tmin y x float item assertLastGraphAllFused assert np isnan warmup_and_run_forward tmax x y float item assert np isnan warmup_and_run_forward tmax y x float item assertLastGraphAllFused test_double_intrinsics do_pow x torch pow x device devices x = torch rand dtype=torch double device=device traced = torch jit trace do_pow x x = warmup_and_run_forward traced x assertLastGraphAllFused test_remainder run_remainder x y c = torch remainder torch add x y x c data_type dtypes = torch rand dtype=data_type b = torch rand dtype=data_type zeros = torch zeros dtype=data_type cc = np array dtype=float cc fill np nan nans = torch from_numpy cc dtype=data_type random floats zeros = torch zeros dtype=data_type zeros = torch zeros dtype=data_type traced = torch jit trace run_remainder zeros zeros x = warmup_and_run_forward traced b assertLastGraphAllFused y = run_remainder b data_type torch bfloat assertEqual x y atol= e- rtol= e- assertEqual x y div traced = torch jit trace run_remainder zeros zeros x = warmup_and_run_forward traced zeros assertLastGraphAllFused y = run_remainder zeros assertEqual x y numerators denominatos nan traced = torch jit trace run_remainder zeros zeros x = warmup_and_run_forward traced nans assertLastGraphAllFused y = run_remainder nans assertEqual x y test_multioutput easy x b = x + c = b + b b c traced = torch jit trace easy torch zeros = torch zeros b c = warmup_and_run_forward traced assertLastGraphAllFused bp = numpy + cp = bp + bp np testing assert_allclose b numpy bp np testing assert_allclose c numpy cp test_chunk easy x y = x + aaa bbb = torch chunk y aaa + bbb data_type dtypes trace_input = torch zeros dtype=data_type traced = torch jit trace easy trace_input = torch zeros dtype=data_type x = warmup_and_run_forward traced assertLastGraphAllFused npr = float numpy npr = npr + npr_a npr_b = np array_split npr np testing assert_allclose npr_a + npr_b x float numpy test_cat device devices _dim = foo args args_ = v + i i v enumerate args v = torch cat args_ dim=_dim v v data_type dtypes M = Ns = values = torch zeros M N dtype=data_type device=device N Ns traced = torch jit trace foo values x = warmup_and_run_forward traced values assertLastGraphAllFused ref = foo values np testing assert_allclose ref cpu float numpy x cpu float numpy Test channels-last _cur_dim range _dim = _cur_dim values = torch randn device=device memory_format=torch channels_last _ range traced = torch jit trace foo values x = warmup_and_run_forward traced values assertLastGraphAllFused ref = foo values assertEqual ref x This test checks we correctly handle fusion group just aten cat Note test only makes sense min_fusion_group= otherwise no fusion groups would formed all TODO Fix re-enable test unittest skip cat broken fusion group inlining disabled test_cat_only device devices foo args args_ = v + i i v enumerate args v = torch cat args_ dim= v M = Ns = values = torch zeros M N device=device N Ns traced = torch jit trace foo values x = warmup_and_run_forward traced values assertLastGraphAllFused ref = foo values np testing assert_allclose ref cpu numpy x cpu numpy test_cat_negative_dim device devices foo args v = torch cat args dim=- v v M = Ns = values = torch randn M N device=device N Ns traced = torch jit trace foo values x = warmup_and_run_forward traced values assertLastGraphAllFused ref = foo values np testing assert_allclose ref cpu numpy x cpu numpy test_cat_promote_inputs device devices foo args v = torch cat args dim= v v M = Ns = dtypes = torch half torch float torch double values = torch randn M N device=device dtype=dt N dt zip Ns dtypes traced = torch jit trace foo values x = warmup_and_run_forward traced values assertLastGraphAllFused ref = foo values np testing assert_allclose ref cpu numpy x cpu numpy test_cat_empty_tensors device devices foo args v = torch cat args dim= v v M = Ns = empty = torch tensor device=device dtype=torch double values = empty + torch randn M N device=device N Ns traced = torch jit trace foo values x = warmup_and_run_forward traced values assertLastGraphAllFused ref = foo values np testing assert_allclose ref cpu numpy x cpu numpy now test only empty tensors values = empty i range traced = torch jit trace foo values x = warmup_and_run_forward traced values assertLastGraphAllFused ref = foo values np testing assert_allclose ref cpu numpy x cpu numpy test_cat_with_constant_dim device devices foo args v = torch cat args dim= v = torch cat v dim= v v empty = torch tensor device=device dtype=torch float inputs = empty + torch randn device=device torch randn device=device traced = torch jit trace foo inputs x = warmup_and_run_forward traced inputs assertLastGraphAllFused ref = foo inputs np testing assert_allclose ref cpu numpy x cpu numpy test_scalar torch jit script test_float x torch Tensor y torch Tensor z torch Tensor float b float - torch Tensor torch add torch add x y alpha=a z alpha=b torch jit script test_int x torch Tensor y torch Tensor z torch Tensor int b int - torch Tensor torch add torch add x y alpha=a z alpha=b test test_float test_int data_type dtypes x y z = torch rand dtype=data_type i range b = test x y z b r = test x y z b assertEqual r x + y + z b test_loop torch jit script test x torch Tensor y torch Tensor z int - torch Tensor b = y _ range z = x + y b = b + y b x y z = torch zeros torch ones test x y z r = test x y z test_slice easy x y = x b = y + b traced = torch jit trace easy torch ones torch zeros = torch ones x = traced npr = npr = npr + npr np testing assert_allclose npr numpy x numpy test_unsqueeze N= easy x y = torch unsqueeze x b = torch unsqueeze y + b traced = torch jit trace easy torch ones N N torch zeros N N = torch rand N N x = traced npr = np expand_dims npr = npr + npr np testing assert_allclose npr x numpy _test_softmax device test_softmax x y = F softmax x dim= dtype=torch float b = F softmax y dim= dtype=torch float c = F softmax x dim= dtype=torch float d = F softmax y dim= dtype=torch float + b + c + d test_softmax_neg_index x y = F softmax x dim=- dtype=torch float b = F softmax y dim=- dtype=torch float c = F softmax x dim=- dtype=torch float d = F softmax y dim=- dtype=torch float + b + c + d test_log_softmax x y = F log_softmax x dim= dtype=torch float b = F log_softmax y dim= dtype=torch float c = F log_softmax x dim= dtype=torch float d = F log_softmax y dim= dtype=torch float + b + c + d test test_softmax test_log_softmax test_softmax_neg_index data_type dtypes old = torch _C _jit_set_texpr_reductions_enabled True traced_input = torch randn dtype=data_type device=device traced = torch jit trace test traced_input traced_input inp = torch randn dtype=data_type device=device res = traced inp inp Use eager mode reference ref = test inp inp np testing assert_allclose ref res cpu numpy rtol= e- atol= e- torch _C _jit_set_texpr_reductions_enabled old test_softmax_cpu _test_softmax cpu unittest skipIf torch cuda is_available requires CUDA unittest skip global allocs supported yet test_softmax_cuda _test_softmax cuda test_half_gelu devices = cuda torch cuda is_available torch jit script bias_gelu bias y x = bias + y x + torch erf x device devices = torch rand dtype=torch half device=device b = torch rand dtype=torch half device=device traced = torch jit trace bias_gelu b x = warmup_and_run_forward traced b assertLastGraphAllFused test_half_bn_relu devices = cuda torch cuda is_available foo b c y = torch nn functional batch_norm b c z = y relu z device devices = torch rand dtype=torch half device=device b = torch rand dtype=torch half device=device c = torch rand dtype=torch half device=device traced = torch jit trace foo b c print traced graph x = warmup_and_run_forward traced b c assertLastGraphAllFused test_exp_pow torch jit script do_exp x y z x y torch pow z device devices x = torch rand dtype=torch double device=device y = torch rand dtype=torch double device=device z = torch rand dtype=torch double device=device traced = torch jit trace do_exp x y z x = warmup_and_run_forward traced x y z assertLastGraphAllFused test_sin_pow test x torch sin torch pow x data_type shape itertools product dtypes x = torch rand shape dtype=data_type scripted = torch jit script test out = warmup_and_run_forward scripted x assertLastGraphAllFused assertEqual out test x test_transpose torch jit script test x y z x transpose + y + z x = torch rand y = torch rand z = torch rand ref = test x y z res = test x y z np testing assert_allclose ref numpy res numpy test_sliced_stride torch jit script test x y z x + y + z x = torch rand y = torch rand z = torch rand ref = test x y z res = test x y z np testing assert_allclose ref numpy res numpy unittest skip dynamic shapes quite there yet unittest skipIf torch cuda is_available requires CUDA test_dynamic_shape num_profiled_runs torch jit script test x y z x y z x y z = torch rand cuda _ range ref = test x y z _ = test torch rand cuda _ range res = test x y z np testing assert_allclose ref cpu numpy res cpu numpy A wild broadcast appears x = torch rand cuda y = torch rand cuda z = torch rand cuda res = test x y z xn yn zn = t cpu numpy t x y z np testing assert_allclose res cpu numpy xn yn zn Mismatched shapes shouldn t reach codegen x = torch rand cuda y = torch rand cuda z = torch rand cuda try res = test x y z except RuntimeError e assert The size tensor must match e args Changing static dimension fails guards x y z = torch rand cuda _ range xn yn zn = t cpu numpy t x y z res = test x y z print test graph_for x y z np testing assert_allclose res cpu numpy xn yn zn unittest skipIf torch cuda is_available requires CUDA test_guard_fails torch jit script test x y z x y z r = test torch rand cuda _ range r = test torch rand cuda _ range r = test torch rand cuda _ range r = test torch rand cuda _ range test_bitwise_ops run_and x y x x y run_or x y x x &#124; y run_xor x y x ^ x ^ y run_lshift x y x x y run_rshift x y x x y fns = run_and run_or run_xor run_lshift run_rshift device devices fn fns = torch ones dtype=torch int device=device b = torch zeros dtype=torch int device=device inp = torch ones dtype=torch int device=device traced = torch jit trace fn inp inp x = warmup_and_run_forward traced b assertLastGraphAllFused y = fn b np testing assert_allclose x cpu numpy y cpu numpy test_where run_where x y torch where torch gt x y x y data_type dtypes = torch rand dtype=data_type b = torch rand dtype=data_type zeros = torch zeros dtype=data_type traced = torch jit trace run_where zeros zeros x = warmup_and_run_forward traced b assertLastGraphAllFused y = run_where b np testing assert_allclose x float numpy y float numpy test_multi_rand device devices test x y = torch rand_like x x + y - y - x _atol = e- _rtol = e- data_type dtypes data_type torch bfloat _atol = e- = torch rand dtype=data_type device=device scripted = torch jit script test out = warmup_and_run_forward scripted assertLastGraphAllFused assert torch allclose out atol=_atol rtol=_rtol test_mask test x x unsqueeze == d devices data_type dtypes x = torch rand dtype=data_type device=d scripted = torch jit script test out = warmup_and_run_forward scripted x assertLastGraphAllFused assert torch equal out test x test_simple_add val = torch _C _jit_get_te_generate_block_code torch _C _jit_set_te_generate_block_code True fall_bk = torch _C _jit_texpr_fallback_allowed torch _C _jit_texpr_set_fallback_allowed True simple b torch add b = torch ones b = torch ones traced = torch jit trace simple torch ones torch ones f = traced b f_test = np full dtype=float np testing assert_allclose f numpy f_test torch _C _jit_set_te_generate_block_code val torch _C _jit_texpr_set_fallback_allowed fall_bk test_strided_output_preserved foo b + b - smaller easier debug example x = torch arange x = torch as_strided x total = i range j range x i j = total total += foo_script = torch jit script foo foo_script x x foo_script x x out_s = foo_script x x out_eager = foo x x assertEqual out_s out_eager assertEqual out_s stride out_eager stride assertLastGraphAllFused more dims N C H W = x = torch rand N C H W memory_format=torch channels_last foo_script = torch jit script foo foo_script x x foo_script x x out_s = foo_script x x out_eager = foo x x assertEqual out_s out_eager assertEqual out_s stride out_eager stride assertLastGraphAllFused test_alias_analysis_module AliasModule nn Module __init__ - None super __init__ torch manual_seed = torch randn b = torch randn c = torch randn forward x y z z = z + b add_ y w = z + z = w + x z x = torch randn getModule script am = AliasModule script torch jit script am am am = getModule False am_s = getModule True ref = am x x x test = am_s x x x torch testing assert_close ref test Now do aliasing am = am b ref = am x x x am_s = am_s b test = am_s x x x torch testing assert_close ref test test_alias_analysis_inputs AliasModule nn Module __init__ - None super __init__ torch manual_seed = torch randn b = torch randn c = torch randn forward x y z x add_ y w = z + z = w + x z getModule script am = AliasModule script torch jit script am am am = getModule False am_s = getModule True torch manual_seed x = torch randn ref = am x x x torch manual_seed x = torch randn test = am_s x x x torch testing assert_close ref test test_alias_analysis_input_and_module AliasModule nn Module __init__ - None super __init__ torch manual_seed = torch randn b = torch randn c = torch randn forward x y z x add_ y w = z + b z = w + x z getModule script am = AliasModule script torch jit script am am am = getModule False am_s = getModule True torch manual_seed x = torch randn am b = x ref = am x x x torch manual_seed x = torch randn am_s b = x test = am_s x x x torch testing assert_close ref test test_multiple_outputs device devices A bug reported internally similar one reported foo b c t_next = c + t = t_next b t = torch unsqueeze t_next t = t t t t_next data_type dtypes = torch rand dtype=data_type device=device b = torch rand dtype=data_type device=device as_strided c = torch ones dtype=torch int device=device traced = torch jit trace foo b c ref = foo b c exp = traced b c exp = traced b c assertEqual ref exp test_propagated_mem_layout foo b c t_next = c + t = t_next b t = t t foo_multi_outputs b c t_next = c + t = b t_next t = t t t t_next foo_multi_outputs_i_nhwc_o_nchw b c t_next = c + t = b t_next t = t t = t memory_format=torch contiguous_format t t t t_next run_foo_case foo b c traced_contiguous = torch jit trace foo b c ref = foo b c exp = traced_contiguous b c exp = traced_contiguous b c assertEqual ref exp mem_layouts = list itertools product torch contiguous_format torch channels_last repeat= shapes = permutes = funcs = foo foo_multi_outputs foo_multi_outputs_i_nhwc_o_nchw configs = itertools product funcs shapes mem_layouts permutes strategy STATIC DYNAMIC old_strategy = torch jit set_fusion_strategy strategy _func _shape _mem_layouts _permute configs = torch rand _shape dtype=torch float memory_format=_mem_layouts b = torch rand _shape dtype=torch float memory_format=_mem_layouts c = torch rand _shape dtype=torch float memory_format=_mem_layouts run_foo_case _func b c = permute dims=_permute b = b permute dims=_permute c = c permute dims=_permute run_foo_case _func b c torch jit set_fusion_strategy old_strategy __name__ == __main__ run_tests