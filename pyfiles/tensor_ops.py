mypy allow-untyped-defs copy torch torch distributed _shard common_op_utils _register_default_op torch distributed _shard sharded_tensor _sharded_op_impl Shard ShardedTensor _common _register_sharded_op_on_local_shards Tensor properties access _register_default_op torch Tensor shape __get__ _sharded_op_impl type ignore attr-defined _register_default_op torch Tensor dtype __get__ _sharded_op_impl type ignore attr-defined _register_default_op torch Tensor layout __get__ _sharded_op_impl type ignore attr-defined _register_default_op torch Tensor size _sharded_op_impl _register_default_op torch Tensor dim _sharded_op_impl _register_default_op torch Tensor ndim __get__ _sharded_op_impl type ignore attr-defined _register_default_op torch Tensor is_contiguous _sharded_op_impl _register_default_op torch Tensor contiguous _sharded_op_impl _register_default_op torch Tensor is_floating_point _sharded_op_impl __reduce_ex__ dispatch get_state set_state _register_default_op torch Tensor __reduce_ex__ _sharded_op_impl autograd related properties _register_default_op torch Tensor requires_grad __get__ _sharded_op_impl type ignore attr-defined TODO set grad ShardedTensor consists all local grads _register_default_op torch Tensor grad __get__ _sharded_op_impl type ignore union-attr _register_default_op torch Tensor grad_fn __get__ _sharded_op_impl type ignore union-attr _register_default_op torch Tensor is_leaf __get__ _sharded_op_impl type ignore attr-defined device property ambiguous global prospective ShardedTensor device consists multiple devices might even across hosts We choose current device local tensor represent device property each rank _sharded_op_impl torch Tensor device __get__ tensor_device types args= kwargs=None pg=None pyrefly ignore index-error self_st = args Validate types isinstance self_st ShardedTensor raise TypeError input needs ShardedTensor dev torch device self_st _local_shards dev = self_st _local_shards tensor device pg pg _get_backend_name == gloo dev = torch device cpu dev = torch device torch cuda current_device dev _sharded_op_impl torch Tensor is_meta __get__ type ignore attr-defined st_is_meta types args= kwargs=None pg=None pyrefly ignore index-error args local_tensor is_meta sharded_type_as_check args kwargs Perform extra checks sharded_type_as op such input needs either Tensor ShardedTensor Args same ` ` torch Tensor type_as ` ` Return None len args raise ValueError Needs give tensor cast type isinstance args torch Tensor isinstance args ShardedTensor raise ValueError Needs give Tensor ShardedTensor cast type same_dtype args kwargs When dtype same original ShardedTensor Args same ` ` torch Tensor type_as ` ` Return bool Whether early args dtype == args dtype sharded_type_as args kwargs pg Handles ` ` __torch_function__ ` ` dispatch ` ` torch Tensor type_as ` ` op Args same ` ` torch Tensor type_as ` ` Return new_local_shards List Shard Local shards new sharded tensor st_meta ShardedTensorMetadata Metadata new sharded tensor st = args tensor = args isinstance tensor ShardedTensor tensor = tensor local_tensor new_local_shards = Shard shard tensor type_as tensor shard metadata shard st local_shards st_meta = copy deepcopy st _metadata st_meta tensor_properties dtype = tensor dtype new_local_shards st_meta _register_sharded_op_on_local_shards torch Tensor type_as early_stop_func=same_dtype extra_check=sharded_type_as_check customized_func=sharded_type_as sharded_deepcopy args kwargs pg NOTE we directly implement deepcopy magic method instead using default tensor __deepcopy__ implement clone This because default tensor deepcopy copies every attribute process_group ShardedTensor cannot deep copied self_st = args new_local_shards = copy deepcopy self_st local_shards new_metadata = copy deepcopy self_st metadata new_local_shards new_metadata _register_sharded_op_on_local_shards torch Tensor __deepcopy__ customized_func=sharded_deepcopy _sharded_op_impl torch Tensor copy_ sharded_inplace_copy types args kwargs pg NOTE inplace op don t need rewrap kwargs = kwargs None kwargs self_st = args new_st = args nonblocking = kwargs get non_blocking False local_shard new_shard zip self_st local_shards new_st local_shards local_shard metadata = new_shard metadata raise RuntimeError inplace copy can only happen between two ShardedTensor same metadata local_shard new_shard zip self_st local_shards new_st local_shards local_shard tensor copy_ new_shard tensor nonblocking self_st sharded_clone args kwargs pg self_st = args desire_memory_format = kwargs get memory_format None desire_memory_format desire_memory_format = torch preserve_format raise RuntimeError Only support torch preserve_format ShardedTensor cloned_local_shards = Shard local_shard tensor clone memory_format=desire_memory_format metadata=copy deepcopy local_shard metadata local_shard self_st local_shards new_metadata = copy deepcopy self_st metadata cloned_local_shards new_metadata _register_sharded_op_on_local_shards torch Tensor clone customized_func=sharded_clone sharded_detach args kwargs pg self_st = args detached_local_shards = Shard local_shard tensor detach metadata=copy deepcopy local_shard metadata local_shard self_st local_shards new_metadata = copy deepcopy self_st metadata new_metadata tensor_properties requires_grad = False detached_local_shards new_metadata _register_sharded_op_on_local_shards torch Tensor detach customized_func=sharded_detach _sharded_op_impl torch Tensor requires_grad_ tensor_requires_grad_set types args= kwargs=None pg=None pyrefly ignore index-error self_st = args Validate types isinstance self_st ShardedTensor raise TypeError input needs ShardedTensor kwargs None kwargs = requires_grad = args len args kwargs get requires_grad True requires_grad == self_st requires_grad self_st local_shard self_st local_shards local_shard tensor requires_grad_ requires_grad update wrapper property torch _C DisableTorchFunctionSubclass self_st requires_grad_ requires_grad update metadata meanwhile self_st _metadata tensor_properties requires_grad = requires_grad self_st