mypy allow-untyped-defs Triton Implementation flex_attention Kernel __future__ annotations logging math collections abc Sequence dataclasses dataclass typing Any Optional TYPE_CHECKING Union sympy torch torch _inductor virtualized V ir ComputedBuffer ExternKernel FixedLayout TensorBox lowering empty empty_strided lowerings register_lowering select_algorithm autotune_select_algorithm SymbolicGridFn TritonTemplate common build_subgraph_buffer create_indices_fake create_num_blocks_fake_generator create_placeholder freeze_irnodes get_fwd_subgraph_outputs infer_dense_strides load_flex_template maybe_realize set_head_dim_values SubgraphResults flex_cpu lower_cpu flex_decoding _use_flex_decoding create_flex_decoding_kernel flex_flash_attention _use_flex_flash_attention create_flex_flash_attention_kernel TYPE_CHECKING template_heuristics triton FlexBwDConfig FlexConfig log = logging getLogger __name__ aten = torch ops aten Expr = sympy Expr SymbolicGridFn flex_attention_grid batch_size q_heads num_queries d_model meta cdiv How kernel parallelized We create grid ceil_div n_queries query_block_size batch_size num_heads Each block responsible iterating over blocks keys values calculating final attention output cdiv num_queries meta BLOCK_M batch_size q_heads get_float _precision torch backends cuda matmul fp _precision == ieee torch backends cuda matmul fp _precision = none torch get_float _matmul_precision == highest torch version hip torch mtia is_available ieee tf flex_attention_template = TritonTemplate name= flex_attention grid=flex_attention_grid source=load_flex_template flex_attention + load_flex_template utilities + load_flex_template common register_lowering torch ops higher_order flex_attention type_promotion_kind=None flex_attention query key value subgraph block_mask scale kernel_options score_mod_other_buffers mask_mod_other_buffers The main lowering flex_attention hop This can currently lower one templates Base Triton Template Flex Decode Triton Template Cpu specific CPP template query get_device type == cpu lower_cpu query key value subgraph block_mask scale kernel_options score_mod_other_buffers mask_mod_other_buffers below cuda path device cpu tl dot does support embedding size less than small_dqk = V graph sizevars evaluate_expr sympy Lt query get_size - small_dv = V graph sizevars evaluate_expr sympy Lt value get_size - small_dqk small_dv raise NotImplementedError f NYI embedding dimension query key value must f least got E= query get_size - Ev= value get_size - _ q_length _ kv_length kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices q_num_blocks q_indices full_q_num_blocks full_q_indices SPARSE_Q_BLOCK_SIZE SPARSE_KV_BLOCK_SIZE mask_graph = block_mask placeholder_inps = create_placeholder name dtype query get_device name dtype score query get_dtype b torch int h torch int m torch int n torch int subgraph_buffer = build_subgraph_buffer placeholder_inps + list score_mod_other_buffers subgraph freeze_irnodes subgraph_buffer mask_graph_placeholder_inps = create_placeholder name dtype query get_device name dtype b torch int h torch int m torch int n torch int mask_graph_buffer = build_subgraph_buffer mask_graph_placeholder_inps + list mask_mod_other_buffers mask_graph freeze_irnodes mask_graph_buffer kernel_options = dict kernel_options Mark symbols custom kernel options static shapes add guards kernel_options = k V graph sizevars guard_int v isinstance v sympy Symbol v k v kernel_options items kernel_options setdefault FLOAT _PRECISION get_float _precision enable_gqa = V graph sizevars evaluate_expr sympy Ne query get_size key get_size _use_flex_decoding query kv_indices value kernel_options enable_gqa create_flex_decoding_kernel query key value block_mask scale kernel_options subgraph_buffer mask_graph_buffer score_mod_other_buffers mask_mod_other_buffers query key value kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices q_num_blocks q_indices full_q_num_blocks full_q_indices = maybe_realize query key value kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices q_num_blocks q_indices full_q_num_blocks full_q_indices _use_flex_flash_attention subgraph mask_graph kernel_options num_score_mod_placeholders=len placeholder_inps create_flex_flash_attention_kernel query key value block_mask scale kernel_options subgraph_buffer mask_graph_buffer score_mod_other_buffers mask_mod_other_buffers kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices mask_graph=mask_graph subgraph=subgraph score_mod_other_buffers = maybe_realize score_mod_other_buffers mask_mod_other_buffers = maybe_realize mask_mod_other_buffers freeze_irnodes score_mod_other_buffers freeze_irnodes mask_mod_other_buffers Bq Hq seq_len_q qk_head_dim = query get_size Bkv Hkv seq_len_kv v_head_dim = value get_size assert V graph sizevars evaluate_expr sympy Eq Bq Bkv &#124; sympy Eq Bkv f Bq Bkv must broadcastable Got Bq= Bq Bkv= Bkv assert V graph sizevars evaluate_expr sympy Gt seq_len_q Query length must greater than assert V graph sizevars evaluate_expr sympy Gt seq_len_kv Key length must greater than B = Bq seq_len_q = seq_len_kv = kernel_options setdefault IS_DIVISIBLE False kernel_options setdefault IS_DIVISIBLE True NB okay v_head_dim different We using these match fill order output q_strides = query get_stride Construct output layout strides matching query out_size = B Hq seq_len_q v_head_dim out_strides = infer_dense_strides out_size q_strides layout = FixedLayout query get_device query get_dtype B Hq seq_len_q v_head_dim stride= sympy sympify s s out_strides see NOTE TritonTemplates multiple outputs logsumexp_shape = B Hq seq_len_q logsumexp = empty_strided logsumexp_shape None dtype=torch float The logsumexp always stored fp regardless input dtype device=query get_device max_scores = empty_strided logsumexp_shape Same shape logsumexp None dtype=torch float The max scores always stored fp regardless input dtype device=query get_device kernel_options setdefault SM_SCALE scale Determine GQA broadcast factor gqa_shared_heads = Hq Hkv kernel_options setdefault GQA_SHARED_HEADS gqa_shared_heads Inside Triton kernel only apply partial masking partial blocks computed full_kv_num_blocks None partial blocks computed has_full_blocks = full_kv_num_blocks None kernel_options setdefault HAS_FULL_BLOCKS has_full_blocks has_full_blocks full_kv_num_blocks full_kv_indices = empty device=query get_device _ range set_head_dim_values kernel_options qk_head_dim v_head_dim V graph sizevars choices list Any = dtype = query get_dtype head_dim = V graph sizevars guard_int query get_size - configs list FlexConfig = V choices get_flex_attention_fwd_configs head_dim dtype query get_device type Mark SPARSE_KV_BLOCK_SIZE SPARSE_Q_BLOCK_SIZE static shapes add guards SPARSE_KV_BLOCK_SIZE = V graph sizevars guard_int SPARSE_KV_BLOCK_SIZE SPARSE_Q_BLOCK_SIZE = V graph sizevars guard_int SPARSE_Q_BLOCK_SIZE Note we don t need pass captured buffers explicitly because they re implicitly added score_mod function We do need explicitly pass autotuning though original_kernel_options = kernel_options copy Default config warp specialization num_consumer_groups num_buffers_warp_spec = conf configs cur_kernel_options = original_kernel_options copy Performance tuning Triton parameters Remove prefix forward kernels options delete backward kernel options k list cur_kernel_options keys k startswith fwd_ v = cur_kernel_options pop k cur_kernel_options k = v k startswith bwd_ cur_kernel_options pop k cur_kernel_options setdefault num_stages conf num_stages cur_kernel_options setdefault num_warps conf num_warps cur_kernel_options get num_consumer_groups False cur_kernel_options setdefault num_consumer_groups num_consumer_groups cur_kernel_options setdefault num_buffers_warp_spec num_buffers_warp_spec USE TMA = false default cur_kernel_options setdefault USE_TMA False cur_kernel_options setdefault BLOCK_M conf block_m cur_kernel_options setdefault BLOCK_N conf block_n Blocksparse options cur_kernel_options setdefault SPARSE_Q_BLOCK_SIZE SPARSE_Q_BLOCK_SIZE cur_kernel_options setdefault SPARSE_KV_BLOCK_SIZE SPARSE_KV_BLOCK_SIZE cur_kernel_options SPARSE_KV_BLOCK_SIZE cur_kernel_options BLOCK_N = cur_kernel_options SPARSE_Q_BLOCK_SIZE cur_kernel_options BLOCK_M = len configs == raise ValueError f Q KV block size must divisible BLOCK_M BLOCK_N We f got Q_BLOCK_SIZE= cur_kernel_options SPARSE_Q_BLOCK_SIZE f KV_BLOCK_SIZE= cur_kernel_options SPARSE_KV_BLOCK_SIZE continue ROCm specific kernargs attrib kpack matrix_instr_nonkdim waves_per_eu hasattr conf attrib cur_kernel_options attrib = getattr conf attrib error = flex_attention_template maybe_append_choice choices=choices input_nodes= query key value logsumexp max_scores kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices layout=layout subgraphs= subgraph_buffer mask_graph_buffer mutated_inputs= logsumexp max_scores call_sizes=query get_size cur_kernel_options error None len configs == raise error inputs_for_autotuning = query key value logsumexp max_scores kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices + list score_mod_other_buffers + list mask_mod_other_buffers input_gen_fns = create_num_blocks_fake_generator kv_indices create_indices_fake create_num_blocks_fake_generator full_kv_indices create_indices_fake out = autotune_select_algorithm flex_attention choices Need filter out symbols since there invariant all input_nodes type IRNode x x inputs_for_autotuning isinstance x torch _inductor ir IRNode layout input_gen_fns=input_gen_fns need subgraph inputs outputs analyze all symints used flex attention out data data subgraph_inps = list score_mod_other_buffers + list mask_mod_other_buffers out data data subgraph_outs = get_fwd_subgraph_outputs subgraph_buffer mask_graph_buffer out logsumexp max_scores ---------------------------- Backward HOP Implementation ---------------------------- SymbolicGridFn flex_attention_backward_grid batch_size q_heads num_queries d_model kv_heads num_key_value meta cdiv How kernel parallelized We create grid ceil_div n_queries query_block_size heads_ratio + ceil_div n_kv kv_block_size batch_size kv_heads Currently only parallelizing over batch kv_heads we can want parallelize over ceil_div q_heads kv_heads num_key_value key_value_block_size To do will either require atomic updates some grad values have two pass kernel design cdiv num_queries meta BLOCK_M q_heads kv_heads + cdiv num_key_value meta BLOCK_N batch_size kv_heads flex_attention_backward_template = TritonTemplate name= flex_attention_backward grid=flex_attention_backward_grid source=load_flex_template flex_backwards + load_flex_template utilities validate_joint_graph joint_graph torch fx Graph We do some pre lowering graph checks order raise nicer error messages node joint_graph nodes node op == call_function node target torch ops flex_lib zeros_and_scatter default user node users user op = output raise NotImplementedError Using multiple indexing operations same tensor requires gradients score_mod function currently supported This typically happens when indexing same tensor multiple times like \n\n score_mod score b h q_idx kv_idx \n score + bias q_idx + bias kv_idx bias used twice \n\n A valid workaround clone tensors will indexed multiple times For example \n\n bias = bias clone \n score_mod score b h q_idx kv_idx \n score + bias q_idx + bias kv_idx \n\n Note solution will use additional memory dataclass frozen=True JointOutputResult Results processing joint outputs grad_input ComputedBuffer captured_grads_compute list ComputedBuffer captured_grads list Optional TensorBox mutated_grads list TensorBox process_joint_outputs all_joint_outputs SubgraphResults num_placeholders int - JointOutputResult Process joint outputs extract various buffers needed lowering Args all_joint_outputs List all outputs build_subgraphs num_placeholders The number placeholder inputs used skip over unused backward compute buffers Returns JointOutputResult containing processed buffers gradients assert isinstance all_joint_outputs list assert all_joint_outputs None joint_subgraph_buffer None - bug joint_buffer = all_joint_outputs other_grads = all_joint_outputs num_placeholders - outer_grads has structure Len other_buffer_grads buffer doesn t require grad than will None We only grab buffers require grad inlining into kernel grads_compute = buf buf other_grads buf None get_out buf buf None None assert isinstance buf ComputedBuffer assert buf name None TensorBox create V graph get_buffer buf name grads_out = get_out x x other_grads mutated_grads = buf buf grads_out buf None JointOutputResult grad_input=joint_buffer captured_grads_compute=grads_compute captured_grads=grads_out mutated_grads=mutated_grads TODO We probably also need layout constraint register_lowering torch ops higher_order flex_attention_backward type_promotion_kind=None flex_attention_backward args kwargs Lowering flex_attention_backward op triton query key value out logsumexp grad_out grad_logsumexp fw_graph joint_graph block_mask scale kernel_options score_mod_other_buffers mask_mod_other_buffers = args _ q_length _ kv_length kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices q_num_blocks q_indices full_q_num_blocks full_q_indices SPARSE_Q_BLOCK_SIZE SPARSE_KV_BLOCK_SIZE mask_graph = block_mask query key value logsumexp grad_out kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices q_num_blocks q_indices full_q_num_blocks full_q_indices = maybe_realize query key value logsumexp grad_out kv_num_blocks kv_indices full_kv_num_blocks full_kv_indices q_num_blocks q_indices full_q_num_blocks full_q_indices device = query get_device dtype = query get_dtype Bq Hq seq_len_q qk_head_dim = query get_size Bkv Hkv seq_len_kv v_head_dim = value get_size assert V graph sizevars evaluate_expr sympy Eq Bq Bkv &#124; sympy Eq Bkv f Bq Bkv must broadcastable Got Bq= Bq Bkv= Bkv kernel_options = dict kernel_options Mark symbols custom kernel options static shapes add guards kernel_options = k V graph sizevars guard_int v isinstance v sympy Symbol v k v kernel_options items kernel_options setdefault FLOAT _PRECISION get_float _precision seq_q_divisible = V graph sizevars statically_known_true seq_len_q == seq_kv_divisible = V graph sizevars statically_known_true seq_len_kv == seq_q_divisible seq_kv_divisible kernel_options setdefault IS_DIVISIBLE True kernel_options setdefault IS_DIVISIBLE False fwd_placeholder_inps = create_placeholder name dtype device name dtype score dtype b torch int h torch int m torch int n torch int fw_subgraph_buffer = build_subgraph_buffer fwd_placeholder_inps + list score_mod_other_buffers fw_graph freeze_irnodes fw_subgraph_buffer joint_placeholder_inps = fwd_placeholder_inps + create_placeholder grad_score_mod dtype device Sometimes we have weird unused nodes here joint_graph graph_module graph eliminate_dead_code It hard raise nice errors some joint graphs during subgraph lowering This lets us do some checks before attempting lower validate_joint_graph joint_graph graph_module graph all_joint_outputs = build_subgraph_buffer joint_placeholder_inps + list score_mod_other_buffers joint_graph freeze_irnodes all_joint_outputs joint_outputs = process_joint_outputs all_joint_outputs len joint_placeholder_inps mask_graph_placeholder_inps = create_placeholder name dtype query get_device name dtype b torch int h torch int m torch int n torch int mask_graph_buffer = build_subgraph_buffer mask_graph_placeholder_inps + list mask_mod_other_buffers mask_graph freeze_irnodes mask_graph_buffer Construct layout stride order matching K key_size = Bq Hkv seq_len_kv qk_head_dim key_strides = infer_dense_strides key_size key get_stride layout_broadcasted_k = FixedLayout key get_device key get_dtype key_size stride= sympy sympify s s key_strides Create delta which will needed bwd s kernel grad_lse_exp = lowerings aten mul grad_logsumexp math log mul_delta = lowerings aten mul out grad_out delta = lowerings aten sum mul_delta axis=- delta = lowerings aten sub delta grad_lse_exp delta = ExternKernel require_contiguous delta grad_lse_exp delta = maybe_realize grad_lse_exp delta see NOTE TritonTemplates multiple outputs query_size = Bq Hq seq_len_q qk_head_dim grad_query_strides = infer_dense_strides query_size query get_stride grad_query = empty_strided query_size stride= sympy sympify s s grad_query_strides dtype=query get_dtype device=query get_device Construct output layout stride order matching value value_size = Bq Hkv seq_len_kv v_head_dim value_strides = infer_dense_strides value_size value get_stride broadcasted_grad_value = empty_strided value_size stride= sympy sympify s s value_strides dtype=value get_dtype device=value get_device kernel_options setdefault SM_SCALE scale Determine GQA factor gqa_shared_heads = Hq Hkv kernel_options setdefault GQA_SHARED_HEADS gqa_shared_heads Inside Triton kernel only apply partial masking partial blocks computed full_kv_num_blocks torch zeros partial blocks computed has_full_blocks = full_kv_num_blocks None kernel_options setdefault HAS_FULL_BLOCKS has_full_blocks has_full_blocks full_kv_num_blocks full_kv_indices full_q_num_blocks full_q_indices = empty device=query get_device _ range set_head_dim_values kernel_options qk_head_dim v_head_dim V graph sizevars SPARSE_Q_BLOCK_SIZE = V graph sizevars guard_int SPARSE_Q_BLOCK_SIZE SPARSE_KV_BLOCK_SIZE = V graph sizevars guard_int SPARSE_KV_BLOCK_SIZE choices list Any = dtype = query get_dtype head_dim = V graph sizevars guard_int query get_size - configs list FlexBwDConfig = V choices get_flex_attention_bwd_configs head_dim dtype query get_device type Default config warp specialization num_consumer_groups num_buffers_warp_spec = original_kernel_options = kernel_options copy conf configs SPARSE_KV_BLOCK_SIZE conf block_n = SPARSE_Q_BLOCK_SIZE conf block_m = SPARSE_KV_BLOCK_SIZE conf block_n = SPARSE_Q_BLOCK_SIZE conf block_m = continue Performance tuning Triton heuristics cur_kernel_options = original_kernel_options copy Remove prefix backward kernels options delete forward kernel options k list cur_kernel_options keys k startswith bwd_ v = cur_kernel_options pop k cur_kernel_options k = v k startswith fwd_ cur_kernel_options pop k cur_kernel_options setdefault num_warps conf num_warps cur_kernel_options setdefault num_stages conf num_stages cur_kernel_options get num_consumer_groups False cur_kernel_options setdefault num_consumer_groups num_consumer_groups cur_kernel_options setdefault num_buffers_warp_spec num_buffers_warp_spec cur_kernel_options setdefault BLOCK_M conf block_m cur_kernel_options setdefault BLOCK_N conf block_n cur_kernel_options setdefault BLOCK_M conf block_m cur_kernel_options setdefault BLOCK_N conf block_n Blocksparse options cur_kernel_options setdefault SPARSE_Q_BLOCK_SIZE SPARSE_Q_BLOCK_SIZE cur_kernel_options setdefault SPARSE_KV_BLOCK_SIZE SPARSE_KV_BLOCK_SIZE ROCm specific kernargs attrib kpack matrix_instr_nonkdim waves_per_eu hasattr conf attrib cur_kernel_options attrib = getattr conf attrib flex_attention_backward_template maybe_append_choice choices=choices input_nodes= query key value logsumexp delta grad_out grad_query broadcasted_grad_value kv_num_blocks kv_indices q_num_blocks q_indices full_kv_num_blocks full_kv_indices full_q_num_blocks full_q_indices layout=layout_broadcasted_k We use store_output only grad_key subgraphs= fw_subgraph_buffer joint_outputs grad_input mask_graph_buffer joint_outputs captured_grads_compute mutated_inputs= grad_query broadcasted_grad_value joint_outputs mutated_grads call_sizes=query get_size + key get_size cur_kernel_options inputs_for_autotuning = pyrefly ignore unsupported-operation query key value logsumexp delta grad_out grad_query broadcasted_grad_value kv_num_blocks kv_indices q_num_blocks q_indices full_kv_num_blocks full_kv_indices full_q_num_blocks full_q_indices + list score_mod_other_buffers + list mask_mod_other_buffers + joint_outputs mutated_grads input_gen_fns = create_num_blocks_fake_generator kv_indices kv_num_blocks create_indices_fake create_num_blocks_fake_generator q_indices q_num_blocks create_indices_fake create_num_blocks_fake_generator full_kv_indices full_kv_num_blocks create_indices_fake create_num_blocks_fake_generator full_q_indices full_q_num_blocks create_indices_fake broadcasted_grad_key = autotune_select_algorithm flex_attention_backward choices x x inputs_for_autotuning isinstance x torch _inductor ir IRNode layout_broadcasted_k input_gen_fns=input_gen_fns Bq Hkv seq_len_kv k_head_dim need subgraph inputs outputs analyze all symints used flex attention broadcasted_grad_key data data subgraph_inps = list score_mod_other_buffers + list mask_mod_other_buffers broadcasted_grad_key data data subgraph_outs = get_bwd_subgraph_outputs fw_subgraph_buffer mask_graph_buffer joint_outputs V graph sizevars evaluate_expr sympy Eq Bq Bkv grad_key = broadcasted_grad_key grad_value = broadcasted_grad_value assert V graph sizevars evaluate_expr sympy Gt Bq sympy Eq Bkv f Bq Bkv must broadcastable f Got Bq= V graph sizevars evaluate_expr Bq f Bkv= V graph sizevars evaluate_expr Bkv grad_key = lowerings aten sum broadcasted_grad_key axis= keepdims=True grad_value = lowerings aten sum broadcasted_grad_value axis= keepdims=True grad_query grad_key grad_value tuple joint_outputs captured_grads get_bwd_subgraph_outputs subgraph_buffer SubgraphResults mask_graph_buffer SubgraphResults joint_outputs JointOutputResult - list Optional Union ComputedBuffer TensorBox subgraph_buffer = pyrefly ignore bad-assignment subgraph_buffer isinstance subgraph_buffer Sequence subgraph_buffer mask_graph_buffer = pyrefly ignore bad-assignment mask_graph_buffer isinstance mask_graph_buffer Sequence mask_graph_buffer joint_output_buffers = joint_outputs grad_input joint_outputs captured_grads_compute joint_outputs captured_grads joint_outputs mutated_grads pyrefly ignore not-iterable subgraph_buffer mask_graph_buffer joint_output_buffers