This module implements graph deduplication functionality TorchDynamo s optimization pipeline Graph deduplication identifies identical subgraphs computational graph merges them reduce redundancy improve performance The process involves analyzing regions graph identifying structurally equivalent regions replacing them single shared implementation This optimization particularly effective models repeated patterns similar computational structures across different parts network logging operator collections defaultdict deque collections abc Generator Iterable typing Optional torch torch fx torch _dynamo config torch multiprocessing reductions StorageWeakRef torch utils _ordered_set OrderedSet graph_region_tracker Node Region graph_utils _detect_cycles _get_flat_args _get_flat_args_unique Represents index into region select node then index into node s flattened arguments UsageIndex = tuple int int log = logging getLogger __name__ last_node_to_additional_deps Optional dict Node OrderedSet Node = None apply_graph_deduplication output_graph - dict str torch fx GraphModule type ignore no-untyped-def This main entry point applying graph deduplication pass \ Deduplication occurs two phases Subgraph creation Subgraph creation works taking one representative region each region \ group creating subgraph which will then used replace all regions \ group This implemented first copying all nodes region new \ subgraph then finding all inputs which within region creating placeholders \ them For outputs all regions region group need scanned ensure \ largest set outputs found then output node created which returns \ tuple all outputs Graph replacement To replace each region extracted subgraph node index region \ argument index within node s flattened args kwargs recorded once during \ subgraph creation This allows us determine which external region nodes \ which order these nodes passed inputs For outputs getitem nodes created \ each output all nodes region external outputs replaced proper \ getitem node Finally all original nodes erased there should no uses these \ left graph The deduplication mutates output_graph argument place Returns mapping nodes their subgraph output replacement node remap outputs when they created output_graph duplicated_region_groups = output_graph region_tracker get_identical_regions output_graph graph node_to_mutated_arg_positions = output_graph region_tracker node_to_mutated_arg_positions node_to_additional_deps = _populate_additional_deps output_graph graph output_graph region_tracker node_to_mutated_arg_positions sub_gms dict str torch fx GraphModule = region_group duplicated_region_groups inds_with_external_users = _get_all_output_indices region_group region = region_group subgraph external_node_usages node_usage_to_tuple_elems ind_to_tuple_spec = _create_subgraph region inds_with_external_users Ignore regions no args now could they possibly evaluated compile time list external_node_usages continue sub_gm = torch fx GraphModule output_graph nn_modules subgraph subgraph_name = output_graph install_subgraph subgraph sub_gm sub_gms subgraph_name = sub_gm output_graph graph inserting_before get_subgraph_node = output_graph graph create_node get_attr subgraph_name region region_group _replace_region_with_subgraph output_graph graph region get_subgraph_node external_node_usages node_usage_to_tuple_elems ind_to_tuple_spec inds_with_external_users subgraph_name node_to_additional_deps node_to_mutated_arg_positions This expose updated node_to_additional_deps tests global last_node_to_additional_deps last_node_to_additional_deps = node_to_additional_deps _stable_topological_sort output_graph graph node_to_additional_deps sub_gms _replace_region_with_subgraph graph torch fx Graph region Region get_subgraph_node Node external_node_usages Iterable OrderedSet UsageIndex node_usage_to_tuple_elems dict UsageIndex OrderedSet int ind_to_tuple_spec dict int dict tuple int int inds_with_external_users list int subgraph_name str node_to_additional_deps dict Node OrderedSet Node node_to_mutated_arg_positions dict Node OrderedSet int - None sub_args = flattened_getitem_nodes OrderedSet Node = OrderedSet usages external_node_usages usage = next iter usages node_ind usage_ind = usage node = region node_ind flattened_args_kwargs = _get_flat_args node user_ind node_usage_ind usages user = region user_ind user node_to_mutated_arg_positions node_usage_ind node_to_mutated_arg_positions user log debug NYI Failed substitute region s due mutation region usage node_usage_to_tuple_elems tuple_elems = region i i node_usage_to_tuple_elems usage flattened_getitem_nodes update tuple_elems sub_args extend tuple_elems sub_args append flattened_args_kwargs usage_ind Input Output aliasing supported HOPs today Note we should use nodes original graph region here because we use original traced example values check _has_aliasing region sub_args inds_with_external_users flattened_getitem_nodes invoke_args = get_subgraph_node subgraph_name sub_args invoke_subgraph_node = graph create_node call_function torch ops higher_order invoke_subgraph invoke_args type ignore arg-type ind = flattened_output_nodes OrderedSet Node = OrderedSet external_user_ind inds_with_external_users node = region external_user_ind _is_tuple_node node tuple_spec = ind_to_tuple_spec external_user_ind flattened_output_nodes update _replace_tuple_outputs node ind tuple_spec invoke_subgraph_node graph ind += len tuple_spec subgraph_output = graph create_node call_function operator getitem invoke_subgraph_node ind node replace_all_uses_with subgraph_output propagate_meta=True ind += Erase reverse topological order node reversed region node flattened_getitem_nodes Don t erase these since they will still used continue node flattened_output_nodes graph erase_node node Remove any nodes additional deps This safe we ve guaranteed there no input mutation so all additional deps will internal subgraph node_to_additional_deps pop node None deps node_to_additional_deps values try deps remove node deps add invoke_subgraph_node except KeyError pass config graph_deduplication_lint print _detect_cycles graph node_to_additional_deps _stable_topological_sort graph node_to_additional_deps graph lint _get_external_inputs region Region - dict Node OrderedSet UsageIndex external_node_to_usages = defaultdict Node OrderedSet UsageIndex OrderedSet region_unique = set region node_ind node enumerate region flattened_args_kwargs = _get_flat_args node arg_ind in_node enumerate flattened_args_kwargs isinstance in_node Node in_node region_unique in_node may occur multiple nodes flat_args track so we can check arg mutated Previously we only needed track one occurrence able map node placeholder external_node_to_usages in_node add node_ind arg_ind external_node_to_usages _get_all_output_indices regions list Region - list int Scan all regions get set all possible output nodes indices region perhaps we can record information during region creation more efficiency inds_with_external_users set int = set region regions _get_inds_with_external_users region inds_with_external_users sorted inds_with_external_users _get_inds_with_external_users region Region inds_unique set int - None ind node enumerate region user node users user region ind inds_unique inds_unique add ind _create_subgraph region Region inds_with_external_users list int - tuple torch fx Graph list OrderedSet UsageIndex dict UsageIndex OrderedSet int dict int dict tuple int int subgraph torch fx Graph = torch fx Graph external_input_to_usages = _get_external_inputs region external_node_usages = list OrderedSet UsageIndex region_to_subgraph_node = flattened_getitem_nodes OrderedSet Node = OrderedSet node_usage_to_tuple_elems dict UsageIndex OrderedSet int = node usage_indices external_input_to_usages items We don t handle tuples inputs today _is_tuple_node node If node tuple we will possibly create multiple placeholders them track which nodes we won t copy into subgraph because they flattened away Later when replacing each region subgraph we will create getitem node externally which will perform flattening outer nodes flattened_node_indices = _get_flattened_node_indices node region ind flattened_node_indices placeholder = subgraph placeholder f supgraph_input_ node name _flattened_ ind region_to_subgraph_node region ind = placeholder flattened_getitem_nodes add region ind node_usage_to_tuple_elems next iter usage_indices = flattened_node_indices placeholder = subgraph placeholder f subgraph_input_ node name region_to_subgraph_node node = placeholder external_node_usages append usage_indices map_arg node Node - Node node region_to_subgraph_node region_to_subgraph_node node node copy_to_subgraph node Node - Node subgraph_node = subgraph node_copy node lambda old map_arg old region_to_subgraph_node node = subgraph_node subgraph_node output_list = ind_to_tuple_spec = ind node enumerate region node flattened_getitem_nodes subgraph_node = copy_to_subgraph node ind inds_with_external_users flatten tuple outputs generating getitem node tree _is_tuple_node node getitem_nodes ind_to_tuple_spec ind = _create_getitem_nodes node subgraph_node subgraph output_list extend getitem_nodes output_list append subgraph_node subgraph output tuple output_list subgraph external_node_usages node_usage_to_tuple_elems ind_to_tuple_spec _stable_topological_sort_impl graph torch fx Graph node_to_additional_deps dict Node OrderedSet Node do_sort bool = True - bool Nodes exactly one these four collections - Nodes ` pending ` waiting processed reverse order pending = list reversed graph nodes - Nodes ` ready ` have been processed already correct order ready = OrderedSet Node - ` waiting ` mapping dependency nodes which depend dependency waiting = defaultdict list - ` outputs ` always end graph outputs = OrderedSet Node The cursor indicates last processed node so we can add new nodes after cursor = None while pending node = pending pop node target == output outputs add node assert node users output nodes should have no users continue waiting_for = x x _get_flat_args_unique node node_to_additional_deps x ready waiting_for We have unprocessed input nodes Might well wait last arg so already sorted list will only recheck node once waiting waiting_for - append node ready add node cursor cursor next node do_sort cursor append node cursor = node Mark nodes have been waiting node finish ready check again pending extend reversed waiting pop node ready update outputs waiting len ready == len graph nodes _stable_topological_sort graph torch fx Graph node_to_additional_deps dict Node OrderedSet Node - None assert _stable_topological_sort_impl graph node_to_additional_deps _has_cycle graph torch fx Graph node_to_additional_deps dict Node OrderedSet Node - bool _stable_topological_sort_impl graph node_to_additional_deps do_sort=False _populate_additional_deps graph torch fx Graph node_to_mutated_arg_positions dict Node OrderedSet int - dict Node OrderedSet Node node_to_additional_deps dict Node OrderedSet Node = defaultdict OrderedSet _add_mutation_dependencies node_to_mutated_arg_positions node_to_additional_deps _add_global_state_dependencies graph node_to_additional_deps node_to_additional_deps _add_global_state_dependencies graph torch fx Graph node_to_additional_deps dict Node OrderedSet Node - None torch amp all_nodes = list graph nodes These targets nodes which need stay same relative place graph global_state_targets = torch amp _enter_autocast torch amp _exit_autocast all_nodes_dep_on list Node = prev_cur_nodes all_nodes list Node - Generator tuple list Node Node None None prev_nodes list Node = next_nodes = list reversed all_nodes while next_nodes cur_node = next_nodes pop yield prev_nodes cur_node prev_nodes append cur_node prev_nodes cur_node prev_cur_nodes all_nodes args_unique = _get_flat_args_unique cur_node new_deps = n n all_nodes_dep_on n args_unique new_deps additional_deps = node_to_additional_deps cur_node additional_deps update new_deps cur_node target global_state_targets additional_deps = node_to_additional_deps cur_node additional_deps update n n prev_nodes n args_unique all_nodes_dep_on append cur_node _add_mutation_dependencies node_to_mutated_arg_positions dict Node OrderedSet int node_to_additional_deps dict Node OrderedSet Node - None node indices node_to_mutated_arg_positions items flat_args_kwargs = _get_flat_args node all mutated args add dependency usages which occur after node ensure node will always ordered before them also add node dependency usages which occur before node ensure node ordered after them index indices mutated_arg = flat_args_kwargs index user mutated_arg users user node continue pyrefly ignore unsupported-operation user node node_to_additional_deps node add user pyrefly ignore unsupported-operation user node node_to_additional_deps user add node _has_aliasing region Region inputs list Node inds_with_external_users list int flattened_getitem_nodes OrderedSet Node - bool input_storages dict StorageWeakRef Node = dict node inputs node flattened_getitem_nodes continue example_value = node meta example_value isinstance example_value torch Tensor storage = StorageWeakRef example_value _typed_storage storage input_storages input-input aliasing log debug NYI Failed substitute region s due input-output aliasing detected nodes s s region input_storages storage node True input_storages storage = node output_storages dict StorageWeakRef Node = dict i inds_with_external_users out_node = region i out_node flattened_getitem_nodes continue out_node example_value = out_node meta example_value assert isinstance example_value list isinstance example_value torch Tensor storage = StorageWeakRef example_value _typed_storage storage output_storages output-output aliasing log debug NYI Failed substitute region s due output-output aliasing detected nodes s s region output_storages storage out_node True output_storages storage = out_node intersected_storages = input_storages keys output_storages keys len intersected_storages input-output aliasing aliased = input_storages s output_storages s s intersected_storages aliased = join f i o i o aliased log debug NYI Failed substitute region s due input-output aliasing detected nodes s region aliased True False _is_tuple_node node Node - bool isinstance node meta example_value tuple _get_children_getitems node Node - Generator Node None None user node users user target operator getitem isinstance user args int yield user _get_flattened_node_indices node Node region Region - OrderedSet int Returns ordered set indices each representing node region which will flattened flattened_node_to_ind = n i i n enumerate region node_indices OrderedSet int = OrderedSet queue = deque _get_children_getitems node while queue cur_node = queue popleft any user region user cur_node users node_indices add flattened_node_to_ind cur_node child _get_children_getitems cur_node queue append child node_indices _create_getitem_nodes node Node subgraph_tuple_node Node subgraph torch fx Graph - tuple list Node dict tuple int int tup = node meta example_value assert isinstance tup tuple _get_getitem_children expects tuple getitem_nodes list Node = queue = deque e i subgraph_tuple_node i e enumerate tup path_to_output_index = while queue cur_elem path parent = queue popleft subgraph inserting_after parent new_getitem_node = subgraph create_node call_function operator getitem parent path - new_getitem_node meta example_value = cur_elem path_to_output_index path = len getitem_nodes getitem_nodes append new_getitem_node isinstance cur_elem tuple queue extend e path + i new_getitem_node i e enumerate cur_elem type ignore arg-type misc getitem_nodes path_to_output_index type ignore return-value _replace_tuple_outputs node Node output_index int tuple_spec dict tuple int int invoke_subgraph_node Node graph torch fx Graph - OrderedSet Node assert _is_tuple_node node _replace_tuple_outputs expects tuple node queue = deque c c args c _get_children_getitems node erased_nodes OrderedSet Node = OrderedSet while queue cur_node path = queue pop c _get_children_getitems cur_node queue append c path + c args type ignore return-value arg-type graph inserting_after invoke_subgraph_node subgraph_output = graph create_node call_function operator getitem invoke_subgraph_node output_index + tuple_spec path type ignore index cur_node replace_all_uses_with subgraph_output propagate_meta=True graph erase_node cur_node erased_nodes add cur_node graph erase_node node erased_nodes add node erased_nodes