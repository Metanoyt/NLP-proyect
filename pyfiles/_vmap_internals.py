mypy allow-untyped-defs functools collections abc Callable typing Any Optional Union typing_extensions deprecated torch torch Tensor torch utils _pytree _broadcast_to_and_flatten tree_flatten tree_unflatten in_dims_t = Union int tuple out_dims_t = Union int tuple int Checks all args-to-be-batched have same batch dim size _validate_and_get_batch_size flat_in_dims list Optional int flat_args list - int batch_sizes = arg size in_dim in_dim arg zip flat_in_dims flat_args in_dim None batch_sizes any size = batch_sizes size batch_sizes raise ValueError f vmap Expected all tensors have same size mapped f dimension got sizes batch_sizes mapped dimension batch_sizes _num_outputs batched_outputs Union Tensor tuple Tensor - int isinstance batched_outputs tuple len batched_outputs If value tuple check has length ` num_elements ` If value tuple make tuple ` value ` repeated ` num_elements ` times _as_tuple value Any num_elements int error_message_lambda Callable str - tuple isinstance value tuple value num_elements len value = num_elements raise ValueError error_message_lambda value Creates BatchedTensors every Tensor arg should batched Returns potentially batched arguments batch_size _create_batched_inputs in_dims in_dims_t args tuple vmap_level int func Callable - tuple tuple int isinstance in_dims int isinstance in_dims tuple raise ValueError f vmap _get_name func in_dims= in_dims inputs f expected ` in_dims ` int potentially nested tuple f matching structure inputs got type in_dims len args == raise ValueError f vmap _get_name func inputs got no inputs Maybe you forgot add f inputs you trying vmap over function no inputs f The latter unsupported flat_args args_spec = tree_flatten args flat_in_dims = _broadcast_to_and_flatten in_dims args_spec flat_in_dims None raise ValueError f vmap _get_name func in_dims= in_dims inputs f in_dims compatible structure ` inputs ` f in_dims has structure tree_flatten in_dims inputs f has structure args_spec arg in_dim zip flat_args flat_in_dims isinstance in_dim int in_dim None raise ValueError f vmap _get_name func in_dims= in_dims inputs f Got in_dim= in_dim input in_dim must either f integer dimension None isinstance in_dim int isinstance arg Tensor raise ValueError f vmap _get_name func in_dims= in_dims inputs f Got in_dim= in_dim input input type f type arg We cannot vmap over non-Tensor arguments f please use None respective in_dim in_dim None in_dim in_dim = arg dim raise ValueError f vmap _get_name func in_dims= in_dims inputs f Got in_dim= in_dim some input input Tensor f dimensionality arg dim so expected in_dim satisfy f = in_dim arg dim batch_size = _validate_and_get_batch_size flat_in_dims flat_args See NOTE Ignored _remove_batch_dim _add_batch_dim batched_inputs = arg in_dim None torch _add_batch_dim arg in_dim vmap_level in_dim arg zip flat_in_dims flat_args tree_unflatten batched_inputs args_spec batch_size Undos batching any batch dimensions associated ` vmap_level ` _unwrap_batched batched_outputs Union Tensor tuple Tensor out_dims out_dims_t vmap_level int batch_size int func Callable allow_none_pass_through bool = False - tuple num_outputs = _num_outputs batched_outputs out_dims_as_tuple = _as_tuple out_dims num_outputs lambda f vmap _get_name func out_dims= out_dims ` out_dims ` must f have one dim per output got num_outputs outputs _get_name func NOTE Ignored _remove_batch_dim _add_batch_dim There something wrong our type bindings functions begin _ see isinstance batched_outputs Tensor out_dim = out_dims_as_tuple torch _remove_batch_dim batched_outputs vmap_level batch_size out_dim type ignore return-value allow_none_pass_through tuple torch _remove_batch_dim out vmap_level batch_size out_dim out None None out out_dim zip batched_outputs out_dims_as_tuple tuple torch _remove_batch_dim out vmap_level batch_size out_dim out out_dim zip batched_outputs out_dims_as_tuple Checks ` fn ` returned one more Tensors nothing NB A python function multiple arguments returns single tuple so we effectively checking ` outputs ` single Tensor tuple Tensors _validate_outputs outputs Any func Callable - None isinstance outputs Tensor isinstance outputs tuple raise ValueError f vmap _get_name func ` _get_name func ` must only f Tensors got type type outputs idx output enumerate outputs isinstance output Tensor continue raise ValueError f vmap _get_name func ` _get_name func ` must only f Tensors got type type output idx _check_out_dims_is_int_or_int_tuple out_dims out_dims_t func Callable - None isinstance out_dims int isinstance out_dims tuple all isinstance out_dim int out_dim out_dims raise ValueError f vmap _get_name func out_dims= out_dims ` out_dims ` must f int tuple int representing where outputs f vmapped dimension should appear _get_name func Callable hasattr func __name__ func __name__ Not all callables have __name__ fact only static functions methods do A callable created via functools partial nn Module name some examples don t have __name__ repr func vmap func inputs wraps all Tensor inputs batched BatchedTensors sends those into func then unwraps output BatchedTensors Operations BatchedTensors perform batched operations user asking deprecated Please use ` torch vmap ` instead ` torch _vmap_internals vmap ` category=FutureWarning vmap func Callable in_dims in_dims_t = out_dims out_dims_t = - Callable Please use torch vmap instead API _vmap func in_dims out_dims A version vmap without initial experimental prototype warning _vmap func Callable in_dims in_dims_t = out_dims out_dims_t = allow_none_pass_through bool = False - Callable The ` allow_none_pass_through ` argument temporary workaround may removed Currently enables us wrap call ` autograd grad ` autograd engine which may None any inputs unused See issue discussing https github com pytorch functorch issues functools wraps func wrapped args _check_out_dims_is_int_or_int_tuple out_dims func vmap_level = torch _C _vmapmode_increment_nesting try batched_inputs batch_size = _create_batched_inputs in_dims args vmap_level func batched_outputs = func batched_inputs allow_none_pass_through _validate_outputs batched_outputs func _unwrap_batched batched_outputs out_dims vmap_level batch_size func allow_none_pass_through=allow_none_pass_through finally torch _C _vmapmode_decrement_nesting wrapped