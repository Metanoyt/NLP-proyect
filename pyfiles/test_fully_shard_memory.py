Owner s oncall distributed functools gc unittest torch torch distributed fsdp CPUOffloadPolicy fully_shard OffloadPolicy torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest get_devtype torch testing _internal common_utils run_tests TEST_CUDA TEST_HPU TEST_XPU torch testing _internal distributed _tensor common_dtensor ModelArgs Transformer TransformerBlock device_type = torch device get_devtype TestFullyShardMemory FSDPTest property world_size - int min torch get_device_module device_type device_count skip_if_lt_x_gpu unittest skipIf TEST_HPU empty_cache supported hpu test_fully_shard_training_memory run_subtests reshard_after_forward True False use_cpu_offload True False run_optim_in_backward True False _test_fully_shard_training_memory _test_fully_shard_training_memory reshard_after_forward bool use_cpu_offload bool run_optim_in_backward bool CPU offloading typically memory savings so we expect users want reshard after forward reshard_after_forward use_cpu_offload Optimizer backward frees sharded gradient GPU memory early memory savings so we expect users want reshard after forward plus has no real effect CPU offloading run_optim_in_backward reshard_after_forward use_cpu_offload skip since common use case assert world_size == f Requires world size since some values hard coded world_size torch manual_seed Pre-run linear forward gemm bias backward gemm allocate cuBLAS workspaces before measuring memory usage since workspace size can differ between hardwares lin = torch nn Linear device=device_type NOTE before https github com pytorch pytorch pull input shape so forward gemm used cublaslt backward used cublas With aforementioned PR shape cublas path used both forward backward altering peak memory usage accounting cublaslt Here we change input shape swaps cublas cublaslt selection forward backward does affect peak memory usage stored ` base_mem_mb ` Reasons flip before PR no Lt addmm when mat has nrows ncols = after PR no Lt addmm when either mat mat have nrows ncols = since input preparation can swap matrices based output row- col-majorness inp = torch randn device=device_type lin inp sum backward torch get_device_module device_type empty_cache base_mem_mb = _get_peak_active_memory_mb vocab_size = model_args = ModelArgs vocab_size=vocab_size n_layers= dim= n_heads= weight_tying=False model = Transformer model_args model_unsharded_numel = sum p numel p model parameters model_sharded_numel = model_unsharded_numel + max_unsharded_numel = sum p numel p model layers parameters i e block unsharded numel non_block_numel = round sum p numel p model tok_embeddings parameters + sum p numel p model pos_embeddings parameters + sum p numel p model norm parameters + sum p numel p model output parameters fully_shard_fn = functools partial fully_shard reshard_after_forward=reshard_after_forward offload_policy=CPUOffloadPolicy use_cpu_offload OffloadPolicy module model modules isinstance module TransformerBlock fully_shard_fn module fully_shard_fn model Do use foreach since intermediates increase peak memory optim_kwargs = lr e- foreach False run_optim_in_backward _register_optim_in_backward model optim_kwargs optim = torch optim Adam model parameters lr= e- foreach=False Init Each module moved GPU before sharding parameters peak_mem_mb = _get_peak_active_memory_mb curr_mem_mb = _get_curr_active_memory_mb Allow some buffer peak memory since original parameters freed until ` fully_shard ` call returns buffer_mb = use_cpu_offload Parameters offloaded after sharding init_mem_mb = max_unsharded_numel e init_mem_mb = model_sharded_numel + max_unsharded_numel e assertLessEqual peak_mem_mb - base_mem_mb init_mem_mb + buffer_mb assertLessEqual curr_mem_mb - base_mem_mb init_mem_mb Use small input minimize activation memory usage inp = torch randint vocab_size device=device_type type Forward loss = model inp mem_mb = _get_peak_active_memory_mb Allow some buffer fragmentation activations where number kept much smaller than actual memory usage which order - + MB buffer_mb = The default workspace hipblaslt larger than cublas cublaslt which requires slight increase buffer value buffer_mb = torch version cuda reshard_after_forward x max unsharded block parameters current all-gather + copy-out next all-gather non-block parameters other expected_mem_mb = max_unsharded_numel + non_block_numel e + buffer_mb use_cpu_offload Sharded parameters expected_mem_mb += model_sharded_numel e assert use_cpu_offload Sharded parameters unsharded parameters x max unsharded block parameters copy-out other peak end forward expected_mem_mb = model_sharded_numel + model_unsharded_numel + max_unsharded_numel e + buffer_mb assertLessEqual mem_mb - base_mem_mb expected_mem_mb Backward loss sum backward mem_mb = _get_peak_active_memory_mb reshard_after_forward x max unsharded block parameters all-gather + copy-out x max unsharded block gradients gradients reduce-scatter input non-block parameters other NOTE Reduce-scatter output counted part x sharded gradients below since gradients view into output expected_mem_mb = max_unsharded_numel + non_block_numel e + buffer_mb use_cpu_offload run_optim_in_backward x sharded parameters expected_mem_mb += model_sharded_numel e- x sharded block gradients expected_mem_mb += max_unsharded_numel world_size e- x sharded parameters gradients expected_mem_mb += model_sharded_numel e assert use_cpu_offload Sharded parameters unsharded parameters x max unsharded block parameters reduce-scatter input output other peak beginning backward expected_mem_mb = model_sharded_numel + model_unsharded_numel + max_unsharded_numel e + buffer_mb assertLessEqual mem_mb - base_mem_mb expected_mem_mb del loss torch get_device_module device_type reset_peak_memory_stats Optimizer step unsharded parameters gradients freed run_optim_in_backward optim step mem_mb = _get_peak_active_memory_mb expected_mem_mb = buffer_mb use_cpu_offload x sharded parameters x sharded optimizer states expected_mem_mb += model_sharded_numel e run_optim_in_backward x sharded gradients expected_mem_mb += model_sharded_numel e assertLessEqual mem_mb - base_mem_mb expected_mem_mb Zero grad sharded gradients freed run_optim_in_backward optim zero_grad torch get_device_module device_type reset_peak_memory_stats reset after freeing mem_mb = _get_peak_active_memory_mb expected_mem_mb = use_cpu_offload x sharded parameters expected_mem_mb += model_sharded_numel e + buffer_mb x sharded optimizer states expected_mem_mb += model_sharded_numel e + buffer_mb assertLessEqual mem_mb - base_mem_mb expected_mem_mb skip_if_lt_x_gpu test_fully_shard_del_memory base_mem_mb = _get_peak_active_memory_mb vocab_size = model_args = ModelArgs vocab_size=vocab_size n_layers= dim= n_heads= weight_tying=False model = Transformer model_args Initializing model CPU should change GPU memory usage post_model_init_mem_mb = _get_peak_active_memory_mb assertEqual base_mem_mb post_model_init_mem_mb module model modules isinstance module TransformerBlock fully_shard module fully_shard model unsharded_numel = sum p numel p model parameters sharded_numel = unsharded_numel world_size buffer_mb = mem_mb = _get_curr_active_memory_mb expected_mb = sharded_numel e + buffer_mb assertLessEqual mem_mb - base_mem_mb expected_mb Deleting model should free all FSDP-managed GPU memory del model Manually call garbage collection since there ref cycles FSDP gc collect mem_mb = _get_curr_active_memory_mb assertEqual mem_mb base_mem_mb _get_peak_active_memory_mb - int mem_stats = torch get_device_module device_type memory_stats TEST_CUDA TEST_XPU round mem_stats active_bytes all peak e TEST_HPU round mem_stats MaxInUse e _get_curr_active_memory_mb - int mem_stats = torch get_device_module device_type memory_stats TEST_CUDA TEST_XPU round mem_stats active_bytes all current e TEST_HPU round mem_stats InUse e _register_optim_in_backward model torch nn Module optim_kwargs - None param_to_optim = param model parameters param_to_optim param = torch optim AdamW param optim_kwargs optim_hook param torch nn Parameter - None param_to_optim param step param_to_optim param zero_grad param model parameters param register_post_accumulate_grad_hook optim_hook __name__ == __main__ run_tests