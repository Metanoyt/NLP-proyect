Owner s oncall distributed os shutil sys tempfile torch torch distributed dist torch distributed _shard sharded_tensor torch distributed _shard sharded_tensor ShardedTensor state_dict_hook torch distributed _shard sharding_spec ChunkShardingSpec EnumerableShardingSpec ShardingSpec ShardMetadata torch distributed checkpoint FileSystemReader FileSystemWriter load_state_dict save_state_dict torch distributed checkpoint _extension ZStandard torch distributed checkpoint default_planner DefaultSavePlanner torch testing _internal common_distributed requires_accelerator_dist_backend skip_if_lt_x_gpu torch testing _internal common_utils instantiate_parametrized_tests parametrize run_tests TEST_WITH_DEV_DBG_ASAN TestCase torch testing _internal distributed _shard sharded_tensor ShardedTensorTestBase with_comms torch testing _internal distributed _shard sharded_tensor _test_st_common MyShardedModel torch testing _internal distributed checkpoint_utils get_test_extension_registry Rot Example with_temp_dir device_type = acc type acc = torch accelerator current_accelerator cpu TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit assert_state_dict_equal TestCase state_dict_ dict str torch Tensor state_dict_ dict str torch Tensor - bool assertEqual len state_dict_ len state_dict_ state_dict must same size assertEqual set state_dict_ keys set state_dict_ keys state_dict keys do match key value_ state_dict_ items value_ = state_dict_ key isinstance value_ ShardedTensor local_shard_ local_shard_ zip value_ local_shards value_ local_shards assertTrue torch equal local_shard_ tensor local_shard_ tensor f Key key s shard does match isinstance value_ torch Tensor assertTrue torch equal value_ value_ f Key key s tensor does match True MyTestModule torch nn Module __init__ - None super __init__ linear_ = torch nn Linear linear_ = torch nn Linear emb = torch nn EmbeddingBag The ShardedModels borrowed test distributed _sharded_tensor test_sharded_tensor py MyShardedModel torch nn Module __init__ spec ShardingSpec - None super __init__ sharded_tensor ShardedTensor = sharded_tensor rand spec init_rrefs=False TestDistributedStateDictSaveLoad TestCase test_read_write_only_tensor - None tempfile TemporaryDirectory path state_dict_to_save = MyTestModule state_dict fs_writer = FileSystemWriter path=path save_state_dict state_dict=state_dict_to_save storage_writer=fs_writer no_dist=True state_dict_to_load_to = MyTestModule state_dict assertRaises AssertionError assert_state_dict_equal state_dict_to_load_to state_dict_to_save Load file without any resharding fs_reader = FileSystemReader path=path load_state_dict state_dict=state_dict_to_load_to storage_reader=fs_reader no_dist=True assert_state_dict_equal state_dict_to_load_to state_dict_to_save tempfile TemporaryDirectory path state_dict_to_save = MyTestModule state_dict fs_writer = FileSystemWriter path=path single_file_per_rank=True save_state_dict state_dict=state_dict_to_save storage_writer=fs_writer no_dist=True state_dict_to_load_to = MyTestModule state_dict assertRaises AssertionError assert_state_dict_equal state_dict_to_load_to state_dict_to_save Load file without any resharding fs_reader = FileSystemReader path=path load_state_dict state_dict=state_dict_to_load_to storage_reader=fs_reader no_dist=True assert_state_dict_equal state_dict_to_load_to state_dict_to_save TestDistributedStateDictSaveLoadWithSharedTensor ShardedTensorTestBase property world_size - int with_comms init_rpc=False skip_if_lt_x_gpu requires_accelerator_dist_backend parametrize extensions None Rot Example ZStandard test_read_write_shard_tensor extensions - None paths = tempfile mkdtemp dist broadcast_object_list paths path = paths pyre-fixme Unexpected keyword argument ` dim ` call ` dist _sharding_spec api ChunkShardingSpec __init__ ` spec = ChunkShardingSpec dim= placements= f rank device_type f rank device_type model_to_save = MyShardedModel spec init_rrefs=False Test save model_to_save _register_state_dict_hook state_dict_hook state_dict_to_save = model_to_save state_dict fs_writer = FileSystemWriter path=path _extensions=extensions save_state_dict state_dict=state_dict_to_save storage_writer=fs_writer dist barrier Create new model model_to_load = MyShardedModel spec init_rrefs=False This correct hook loading state dict model_to_load _register_load_state_dict_pre_hook pre_load_state_dict_hook True model_to_load _register_state_dict_hook state_dict_hook state_dict_to_load_to = model_to_load state_dict dist barrier assertRaises AssertionError assert_state_dict_equal state_dict_to_load_to state_dict_to_save Test load fs_reader = FileSystemReader path=path _extension_registry=get_test_extension_registry load_state_dict state_dict=state_dict_to_load_to storage_reader=fs_reader assert_state_dict_equal state_dict_to_load_to state_dict_to_save dist barrier TestDistributedReshardOnLoad ShardedTensorTestBase property world_size - int get_file_path - str paths = tempfile mkdtemp dist get_rank == None dist broadcast_object_list paths paths load_tensor tensor ShardedTensor - torch Tensor res = torch zeros tensor shape device=f device_type dist get_rank == None tensor gather out=res res with_comms init_rpc=False skip_if_lt_x_gpu requires_accelerator_dist_backend test_load_with_different_shard_plan - None path = get_file_path We hardcode assumption how many shards around assertEqual world_size dist get_world_size specs = pyre-fixme Unexpected keyword argument ` dim ` call ` dist _sharding_spec api ChunkShardingSpec __init__ ` ChunkShardingSpec dim= placements= f rank device_type f rank device_type pyre-fixme Unexpected keyword argument ` dim ` call ` dist _sharding_spec api ChunkShardingSpec __init__ ` ChunkShardingSpec dim= placements= f rank device_type f rank device_type f rank device_type f rank device_type This requires tensors EnumerableShardingSpec shards= ShardMetadata shard_offsets= shard_sizes= placement=f rank device_type ShardMetadata shard_offsets= shard_sizes= placement=f rank device_type ShardMetadata shard_offsets= shard_sizes= placement=f rank device_type ShardMetadata shard_offsets= shard_sizes= placement=f rank device_type ShardMetadata shard_offsets= shard_sizes= placement=f rank device_type This requires tensors EnumerableShardingSpec shards= ShardMetadata shard_offsets= shard_sizes= placement=f rank device_type ShardMetadata shard_offsets= shard_sizes= placement=f rank device_type s specs s specs s == s continue dist get_rank == shutil rmtree path ignore_errors=True os makedirs path dist barrier model_to_save = MyShardedModel s model_to_save _register_state_dict_hook state_dict_hook state_dict_to_save = model_to_save state_dict fs_writer = FileSystemWriter path=path save_state_dict state_dict=state_dict_to_save storage_writer=fs_writer dist barrier model_to_load = MyShardedModel s model_to_load _register_state_dict_hook state_dict_hook state_dict_to_load_to = model_to_load state_dict dist barrier fs_reader = FileSystemReader path=path load_state_dict state_dict=state_dict_to_load_to storage_reader=fs_reader dist barrier store_tensor = load_tensor model_to_save sharded_tensor dist barrier load_tensor = load_tensor model_to_load sharded_tensor dist get_rank == assertTrue torch allclose store_tensor load_tensor msg=f s vs s with_comms init_rpc=False skip_if_lt_x_gpu requires_accelerator_dist_backend test_load_rowwise_to_colwise - None path = get_file_path assertEqual world_size dist get_world_size pyre-fixme Unexpected keyword argument ` dim ` call ` dist _sharding_spec api ChunkShardingSpec __init__ ` src_spec = ChunkShardingSpec dim= placements= f rank device_type f rank device_type pyre-fixme Unexpected keyword argument ` dim ` call ` dist _sharding_spec api ChunkShardingSpec __init__ ` dst_spec = ChunkShardingSpec dim= placements= f rank device_type f rank device_type dist get_rank == shutil rmtree path ignore_errors=True os makedirs path model_to_save = MyShardedModel src_spec dist get_rank model_to_save _register_state_dict_hook state_dict_hook state_dict_to_save = model_to_save state_dict fs_writer = FileSystemWriter path=path save_state_dict state_dict=state_dict_to_save storage_writer=fs_writer model_to_load = MyShardedModel dst_spec dist get_rank model_to_load _register_state_dict_hook state_dict_hook state_dict_to_load_to = model_to_load state_dict fs_reader = FileSystemReader path=path load_state_dict state_dict=state_dict_to_load_to storage_reader=fs_reader We can t use torch allclose since each ST has different sharding spec store_tensor = load_tensor model_to_save sharded_tensor load_tensor = load_tensor model_to_load sharded_tensor dist get_rank == assertTrue torch allclose store_tensor load_tensor with_comms init_rpc=False skip_if_lt_x_gpu requires_accelerator_dist_backend test_save_load_bytes - None path = get_file_path state_dict_to_save = bytes bytes string fs_writer = FileSystemWriter path=path save_state_dict state_dict=state_dict_to_save storage_writer=fs_writer state_dict_to_load = bytes bytes other fs_reader = FileSystemReader path=path load_state_dict state_dict=state_dict_to_load storage_reader=fs_reader assertEqual state_dict_to_load bytes assertEqual string state_dict_to_load bytes with_comms init_rpc=False skip_if_lt_x_gpu requires_accelerator_dist_backend test_switch_between_sharded_tensor_to_tensor - None path = get_file_path tensor_size = specs = ChunkShardingSpec dim= placements= f rank device_type f rank device_type ChunkShardingSpec dim= placements= f rank device_type f rank device_type f rank device_type f rank device_type EnumerableShardingSpec shards= ShardMetadata shard_offsets= shard_sizes= placement=f rank device_type ShardMetadata shard_offsets= shard_sizes= tensor_size - placement=f rank device_type EnumerableShardingSpec shards= ShardMetadata shard_offsets= shard_sizes= placement=f rank device_type ShardMetadata shard_offsets= shard_sizes= tensor_size - placement=f rank device_type save_spec specs load_spec specs save_dict = sharded sharded_tensor rand save_spec tensor_size replicated torch rand tensor_size device=self rank fs_writer = FileSystemWriter path=path save_state_dict state_dict=save_dict storage_writer=fs_writer Freaky Friday tensors load_dict = sharded torch zeros tensor_size device=self rank replicated sharded_tensor zeros load_spec tensor_size fs_reader = FileSystemReader path=path load_state_dict state_dict=load_dict storage_reader=fs_reader save_dict_sharded = load_tensor save_dict sharded load_dict_replicated = load_tensor load_dict replicated dist get_rank == assertTrue torch allclose save_dict_sharded load_dict sharded f save-spec save_spec load-spec load_spec assertTrue torch allclose save_dict replicated load_dict_replicated f save-spec save_spec load-spec load_spec TestDistributedStateDictSaveLoadWithCaching ShardedTensorTestBase property world_size - int with_comms init_rpc=False skip_if_lt_x_gpu requires_accelerator_dist_backend with_temp_dir test_read_write_shard_tensor - None pyre-fixme Unexpected keyword argument ` dim ` call ` dist _sharding_spec api ChunkShardingSpec __init__ ` spec = ChunkShardingSpec dim= placements= f rank device_type f rank device_type model_to_save = MyShardedModel spec init_rrefs=False Test save model_to_save _register_state_dict_hook state_dict_hook state_dict_to_save = model_to_save state_dict fs_writer = FileSystemWriter path=self temp_dir save_state_dict state_dict=state_dict_to_save storage_writer=fs_writer planner=DefaultSavePlanner enable_plan_caching=True dist barrier Create new model model_to_load = MyShardedModel spec init_rrefs=False This correct hook loading state dict model_to_load _register_load_state_dict_pre_hook pre_load_state_dict_hook True model_to_load _register_state_dict_hook state_dict_hook state_dict_to_load_to = model_to_load state_dict dist barrier assertRaises AssertionError assert_state_dict_equal state_dict_to_load_to state_dict_to_save Test load fs_reader = FileSystemReader path=self temp_dir _extension_registry=get_test_extension_registry load_state_dict state_dict=state_dict_to_load_to storage_reader=fs_reader assert_state_dict_equal state_dict_to_load_to state_dict_to_save dist barrier Save Attempt save_state_dict state_dict=state_dict_to_save storage_writer=fs_writer planner=DefaultSavePlanner enable_plan_caching=True dist barrier Create new model model_to_load = MyShardedModel spec init_rrefs=False This correct hook loading state dict model_to_load _register_load_state_dict_pre_hook pre_load_state_dict_hook True model_to_load _register_state_dict_hook state_dict_hook state_dict_to_load_to = model_to_load state_dict dist barrier assertRaises AssertionError assert_state_dict_equal state_dict_to_load_to state_dict_to_save Test load fs_reader = FileSystemReader path=self temp_dir _extension_registry=get_test_extension_registry load_state_dict state_dict=state_dict_to_load_to storage_reader=fs_reader assert_state_dict_equal state_dict_to_load_to state_dict_to_save dist barrier instantiate_parametrized_tests TestDistributedStateDictSaveLoadWithSharedTensor __name__ == __main__ run_tests