mypy allow-untyped-defs functools typing Any Optional TYPE_CHECKING torch torch ao quantization observer HistogramObserver PerChannelMinMaxObserver torch ao quantization quantizer quantizer QuantizationSpec torch ao quantization quantizer x _inductor_quantizer _is_any_annotated FilterFn int _in_int _out_ops X InductorQuantizer torch ao quantization quantizer xnnpack_quantizer_utils QuantizationConfig torch fx Node TYPE_CHECKING torch ao quantization qconfig _ObserverOrFakeQuantizeConstructor __all__ = XPUInductorQuantizer get_default_xpu_inductor_quantization_config functools lru_cache get_default_xpu_inductor_quantization_config extra_args dict str Any = eps - act_observer_or_fake_quant_ctr = HistogramObserver act_quantization_spec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_tensor_affine is_dynamic=False observer_or_fake_quant_ctr=act_observer_or_fake_quant_ctr with_args extra_args weight_observer_or_fake_quant_ctr _ObserverOrFakeQuantizeConstructor = PerChannelMinMaxObserver weight_quantization_spec = QuantizationSpec dtype=torch int quant_min=- quant_max= qscheme=torch per_channel_symmetric ch_axis= corresponding weight shape = oc ic kh kw conv is_dynamic=False observer_or_fake_quant_ctr=weight_observer_or_fake_quant_ctr with_args extra_args bias_quantization_spec = None will use placeholder observer default quantization_config = QuantizationConfig act_quantization_spec act_quantization_spec weight_quantization_spec bias_quantization_spec False quantization_config XPUInductorQuantizer X InductorQuantizer XPUInductorQuantizer designed facilitate quantization capability Intel GPU backend The highly reuses existing implementation X InductorQuantizer both intended take advantage optimized kernels oneDNN library __init__ - None super __init__ Following annotate_xx overrides impls base no XPU implementation these operators currently We would gradually enable XPU implementation remove following overrides We keep annotate methods make function body empty aiming let ` _generate_qdq_quantized_model ` generate qdq around op graph execute fp dtype unsupported operators _annotate_qat_conv d_fusion_pattern model torch fx GraphModule quantization_config Optional QuantizationConfig filter_fn Optional FilterFn = None pass _annotate_maxpool d node Node quantization_config Optional QuantizationConfig - None Here we skip annotate logic maxpool XPU backend quantized max_pool d only implemented CPU _annotate_output_for_int _in_int _out_pattern node Node - None node target int _in_int _out_ops _is_any_annotated node node target torch ops aten max_pool d default input_node = node all_input_nodes _annotate_output_share_observer_as_input input_node node