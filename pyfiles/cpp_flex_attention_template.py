mypy allow-untyped-defs contextlib logging re typing Optional unittest mock patch sympy torch torch utils utils _ordered_set OrderedSet ir ir TensorBox select_algorithm DataProcessorTemplateWrapper utils parallel_num_threads virtualized V cpp_template CppTemplate cpp_utils GemmBlocking log = logging getLogger __name__ TODO reuse cpp codegen generate below pointwise reduction kernels SOFTMAX_FUSIONS = r out = exp - val val = sum out template typename T typename T inline void kernel_name _exp_reduce_sum_fusion_kernel T const int size T out T val auto vec_size = vec Vectorized T size auto vec_max = vec Vectorized T val T tmp_sum = auto vec_tmp_sum = vec Vectorized T tmp_sum long i = i vec_size size vec_size i += vec_size auto tmp = vec Vectorized T loadu + i auto tmp = tmp - vec_max auto tmp = tmp exp_u vec_tmp_sum += tmp native _store out + i tmp tmp_sum = vec vec_reduce_all T vec Vectorized T x vec Vectorized T y x + y vec_tmp_sum long i = vec_size size vec_size i size i++ auto tmp = i auto tmp = tmp - val auto tmp = exp tmp tmp_sum += tmp out i = tmp val = tmp_sum out = scale max = max out template typename scalar_t inline void kernel_name _mul_reduce_max_fusion_kernel const scalar_t const scalar_t scale const int size scalar_t out scalar_t max auto vec_size = vec Vectorized scalar_t size auto vec_scale = vec Vectorized scalar_t scale scalar_t tmp_max = -std numeric_limits scalar_t infinity auto vec_tmp_max = vec Vectorized scalar_t tmp_max long i = i vec_size size vec_size i += vec_size auto tmp = vec Vectorized scalar_t loadu + i auto tmp = tmp vec_scale vec_tmp_max = vec maximum vec_tmp_max tmp native _store out + i tmp long i = vec_size size vec_size i size i++ auto tmp = i auto tmp = tmp scale tmp_max = std max tmp_max tmp out i = tmp max = std max tmp_max vec vec_reduce_all scalar_t vec Vectorized scalar_t x vec Vectorized scalar_t y vec maximum x y vec_tmp_max template typename scalar_t static inline scalar_t kernel_name _conditional_data_ptr scalar_t ptr scalar_t ptr TORCH_CHECK ptr == nullptr ptr template typename scalar_t typename std enable_if_t c is_reduced_floating_point_v scalar_t int = static inline scalar_t kernel_name _conditional_data_ptr float ptr scalar_t ptr ptr template typename scalar_t inline void kernel_name _fill_stub scalar_t data scalar_t val int _t size using Vec = vec Vectorized scalar_t Vec data_vec = Vec val int _t d = d size - size Vec size d += Vec size data_vec store data + d #if defined _MSC_VER defined COMPILING_FOR_MIN_SIZE pragma unroll #endif d size d++ data d = val out = scale template typename scalar_t inline void kernel_name _mul_scale_kernel scalar_t scalar_t scale int _t size auto vec_size = vec Vectorized scalar_t size auto vec_scale = vec Vectorized scalar_t scale int _t i = i vec_size size vec_size i += vec_size auto tmp = vec Vectorized scalar_t loadu + i auto tmp = tmp vec_scale native _store + i tmp int _t i = vec_size size vec_size i size i++ auto tmp = i auto tmp = tmp scale i = tmp BRGEMM_PACK_FUNCTIONS = r template typename scalar_t inline void kernel_name _copy_value_with_pad const scalar_t value_ptr scalar_t dst_ptr int _t rows int _t cols int _t prows int _t pcols int _t ldi auto vec_size = vec Vectorized scalar_t size int _t i = i rows i++ int _t j = j cols - cols vec_size j += vec_size auto vec_v = vec Vectorized scalar_t loadu value_ptr + i ldi + j vec_v store dst_ptr + i pcols + j j cols auto vec_v = vec Vectorized scalar_t loadu value_ptr + i ldi + j cols - j vec_v store dst_ptr + i pcols + j cols - j col padding auto psize = pcols - cols psize auto zero_vec = vec Vectorized scalar_t int _t pj = pj psize - psize vec_size pj += vec_size zero_vec store dst_ptr + i pcols + cols + pj pj psize zero_vec store dst_ptr + i pcols + cols + pj psize - pj row padding i prows i++ auto zero_vec = vec Vectorized scalar_t int _t j = j pcols - pcols vec_size j += vec_size zero_vec store dst_ptr + i pcols + j j pcols zero_vec store dst_ptr + i pcols + j pcols - j MICRO_GEMM_TEMPLATE = r GEMM_DEFINE ALLOCATE_BUFFER = r int _t buffer_name _dtype_itemsize = c is_reduced_floating_point_v buffer_dtype auto buffer_name _allocator = getCPUAllocator auto buffer_name _work_data = buffer_name _allocator allocate buffer_size buffer_name _dtype_itemsize void buffer_name _data_ptr = buffer_name _work_data get buffer_dtype buffer_name = buffer_dtype buffer_name _data_ptr FLEX_ATTENTION_TEMPLATE = r template header getvalue #include ATen native cpu utils h #include ATen native CPUBlas h #include ATen Context h template codegen_micro_gemm kernel kernel_name template codegen_softmax_fusion kernel kernel_name template codegen_brgemm_pack_function kernel kernel_name - set kernel_args = query query key key value value kv_num_blocks kv_num_blocks kv_indices kv_indices full_kv_num_blocks full_kv_num_blocks full_kv_indices full_kv_indices - set kernel_args = template update_kernel_args kernel_args extern C kernel def_kernel inputs=kernel_args outputs= output output extra_sizevars=template extra_sizevars kernel maybe_codegen_profile int _t qBlockSize = qBlockSize int _t kvBlockSize = kvBlockSize int _t num_thread = num_thread dtypes kernel internal buffers using scalar_t = kernel dtype query constexpr bool is_reduced_type = c is_reduced_floating_point_v scalar_t using accum_t = opmath_type kernel dtype query using Vec = vec Vectorized accum_t accum_t scaling_factor = scale int _t batchSize = kernel size query int _t qSize = kernel size query int _t num_head = kernel size query int _t headSize = kernel size query int _t batchSize_k = kernel size key int _t num_head_k = kernel size key int _t headSize_v = kernel size value bool is_broadcast_bs_kv = batchSize = batchSize_k bool is_broadcast_head_kv = num_head = num_head_k int _t gqa_shards = num_head num_head_k int _t bs_shards = batchSize batchSize_k int _t batchSize_kvi = kernel size kv_indices int _t num_head_kvi = kernel size kv_indices int _t block_num_kvi = kernel size kv_indices bool is_broadcast_bs_kvi = batchSize = batchSize_kvi bool is_broadcast_head_kvi = num_head = num_head_kvi int _t gqa_shards_kvi = num_head num_head_kvi int _t bs_shards_kvi = batchSize batchSize_kvi int _t kviStrideB = kernel stride kv_indices int _t kviStrideH = kernel stride kv_indices int _t kviStrideQ = kernel stride kv_indices int _t num_kviStrideB = kernel stride kv_num_blocks int _t num_kviStrideH = kernel stride kv_num_blocks - has_full_kv_block int _t full_kviStrideB = kernel stride full_kv_indices int _t full_kviStrideH = kernel stride full_kv_indices int _t full_kviStrideQ = kernel stride full_kv_indices int _t full_num_kviStrideB = kernel stride full_kv_num_blocks int _t full_num_kviStrideH = kernel stride full_kv_num_blocks auto full_kv_indices_data = full_kv_indices auto full_kv_num_blocks_data = full_kv_num_blocks - endif auto kv_num_blocks_data = kv_num_blocks auto kv_indices_data = kv_indices Strides int _t qStrideB = kernel stride query int _t qStrideM = kernel stride query int _t qStrideH = kernel stride query int _t kStrideB = kernel stride key int _t kStrideN = kernel stride key int _t kStrideH = kernel stride key int _t vStrideB = kernel stride value int _t vStrideN = kernel stride value int _t vStrideH = kernel stride value int _t oStrideB = kernel stride output int _t oStrideM = kernel stride output int _t oStrideH = kernel stride output int _t kvSize = kernel size key int _t qSplitSize = qBlockSize int _t kvSplitSize = kvBlockSize qSplitSize = qSplitSize qSize qSize qSplitSize kvSplitSize = kvSplitSize kvSize kvSize kvSplitSize int _t qSlice = qSize + qSplitSize - qSplitSize int _t kvSlice = kvSize + kvSplitSize - kvSplitSize int _t kvTail = kvSize - kvSplitSize + bool need_pack = false Whether pack needed BFloat Half is_reduced_type check platform ability need_pack = std is_same_v scalar_t BFloat native cpublas could_pack kBFloat native cpublas could_pack kHalf need_pack When number gemm greater than number pack pack overhead can overlapped int _t thresh_size = need_pack = kvSize = thresh_size qSize = thresh_size need_pack double pack_size = batchSize num_head kvSize headSize double qs_per_thread = batchSize num_head qSlice + num_thread - num_thread double gemm_size_per_thread = qs_per_thread qSplitSize kvSize headSize need_pack = gemm_size_per_thread pack_size = Pad needed packing when K even bool headSize_even = headSize == int _t eheadSize = need_pack headSize_even headSize + headSize int _t ekvSplitSize = need_pack kvSplitSize = kvSplitSize + kvSplitSize int _t ekvTail = need_pack kvTail = kvTail + kvTail int _t kv_padding_size = kvSize - kvSplitSize ekvSplitSize + ekvTail Allocate per thread temp buf accumulate type int _t _size_per_thread = qk qSplitSize kvSplitSize + qk_max qSplitSize + qk_sum qSplitSize + dst qSplitSize headSize_v Inputs outputs buffers const scalar_t q_data = query const scalar_t k_data = key const scalar_t v_data = value scalar_t out_data = output Buffers store accum results padding query transpose packing key value template codegen_allocate_buffer buf_data accum_t num_thread _size_per_thread template codegen_allocate_buffer buf_reduced_data scalar_t num_thread qSplitSize ekvSplitSize template codegen_allocate_buffer key_reorder_ptr scalar_t batchSize_k num_head_k eheadSize kvSize template codegen_allocate_buffer value_reorder_ptr scalar_t batchSize_k num_head_k kv_padding_size headSize_v template codegen_allocate_buffer transpose_buffer_ptr scalar_t num_thread kvSplitSize headSize template codegen_allocate_buffer query_padding_ptr scalar_t num_thread qSplitSize eheadSize need_pack Pack K V parallel_for batchSize_k num_head_k kvSlice int _t begin int _t end int ompIdx = get_thread_num int _t i = j = l = n = scalar_t transpose_ptr = need_pack transpose_buffer_ptr + ompIdx kvSplitSize headSize nullptr native data_index_init begin i batchSize_k j num_head_k l kvSlice maybe_unused auto z c irange begin end n = l kvSplitSize int _t cur_kvSplitSize = std min kvSplitSize kvSize - n auto k_addr = k_data + i kStrideB + j kStrideH + n kStrideN auto v_addr = v_data + i vStrideB + j vStrideH + n vStrideN transpose cur_kvSplitSize headSize - headSize cur_kvSplitSize native utils transpose uint _t cur_kvSplitSize headSize src_ptr reinterpret_cast const uint _t k_addr ld_src kStrideN dst reinterpret_cast uint _t transpose_ptr ld_dst cur_kvSplitSize Pack headSize cur_kvSplitSize vec pack_vnni src reinterpret_cast const uint _t transpose_ptr dst reinterpret_cast uint _t key_reorder_ptr + i num_head_k eheadSize kvSize + j eheadSize kvSize + n eheadSize ld_src cur_kvSplitSize K headSize N cur_kvSplitSize Pack cur_kvSplitSize headSize_v vec pack_vnni src reinterpret_cast const uint _t v_addr dst reinterpret_cast uint _t value_reorder_ptr + i num_head_k kv_padding_size headSize_v + j kv_padding_size headSize_v + n headSize_v ld_src vStrideN K cur_kvSplitSize N headSize_v Move next query native data_index_step i batchSize_k j num_head_k l kvSlice Attention loop below parallel_for batchSize num_head qSlice int _t begin int _t end int _t i = j = k = native data_index_init begin i batchSize j num_head k qSlice int ompIdx = get_thread_num accum_t buf_ptr = buf_data + ompIdx _size_per_thread accum_t qk_data = buf_ptr accum_t qk_max_data = qk_data + qSplitSize kvSplitSize accum_t qk_sum_data = qk_max_data + qSplitSize accum_t dst_data = qk_sum_data + qSplitSize scalar_t qk_reduced_data = is_reduced_type buf_reduced_data + ompIdx qSplitSize ekvSplitSize nullptr scalar_t query_t_padding_ptr = headSize_even need_pack query_padding_ptr + ompIdx qSplitSize eheadSize nullptr maybe_unused auto z c irange begin end auto i_kvi = is_broadcast_bs_kvi i bs_shards_kvi i auto j_kvi = is_broadcast_head_kvi j gqa_shards_kvi j auto kv_logical_num_data = kv_num_blocks_data + i_kvi num_kviStrideB + j_kvi num_kviStrideH + k int kv_indice_num = kv_logical_num_data std vector int kv_indice_list kv_indice_num int kv_i = kv_i kv_indice_num kv_i++ auto kv_logical_data = kv_indices_data + i_kvi kviStrideB + j_kvi kviStrideH + k kviStrideQ + kv_i kv_indice_list kv_i = kv_logical_data bool is_skip_kv = kv_indice_num false true - has_full_kv_block auto full_kv_logical_num_data = full_kv_num_blocks_data + i_kvi num_kviStrideB + j_kvi num_kviStrideH + k int full_kv_indice_num = full_kv_logical_num_data std vector int full_kv_indice_list full_kv_indice_num int kv_i = kv_i full_kv_indice_num kv_i++ auto full_kv_logical_data = full_kv_indices_data + i_kvi full_kviStrideB + j_kvi full_kviStrideH + k full_kviStrideQ + kv_i full_kv_indice_list kv_i = full_kv_logical_data is_skip_kv = kv_indice_num + full_kv_indice_num false true - endif int _t m = k qSplitSize int _t cur_qSplitSize = std min qSplitSize qSize - m is_skip_kv Initialize max sum kernel kernel_name _fill_stub qk_max_data -std numeric_limits accum_t infinity cur_qSplitSize kernel kernel_name _fill_stub qk_sum_data static_cast accum_t cur_qSplitSize headSize_even need_pack Pad query headSize even kernel kernel_name _copy_value_with_pad scalar_t q_data + i qStrideB + j qStrideH + m qStrideM query_t_padding_ptr cur_qSplitSize headSize cur_qSplitSize eheadSize qStrideM - has_full_kv_block int _t n_idx = n_idx kv_indice_num + full_kv_indice_num n_idx += auto n = n_idx kv_indice_num kv_indice_list n_idx kvSplitSize full_kv_indice_list n_idx - kv_indice_num kvSplitSize - int _t n_idx = n_idx kv_indice_num n_idx += auto n = kv_indice_list n_idx kvSplitSize - endif auto cur_n = n kvSplitSize int _t cur_kvSplitSize = std min kvSplitSize kvSize - n int _t cur_ekvSplitSize = need_pack cur_kvSplitSize = cur_kvSplitSize + cur_kvSplitSize Calculate scale q k T auto i_kv = is_broadcast_bs_kv i bs_shards i auto j_kv = is_broadcast_head_kv j gqa_shards j need_pack auto k_addr = k_data + i_kv kStrideB + j_kv kStrideH + n kStrideN kernel kernel_name _kernel_micro_gemm_transpose_b static_cast bool false q_data + i qStrideB + j qStrideH + m qStrideM k_addr qk_data cur_qSplitSize cur_kvSplitSize headSize qStrideM kStrideN cur_kvSplitSize native cpublas brgemm cur_qSplitSize cur_kvSplitSize eheadSize headSize_even qStrideM eheadSize cur_kvSplitSize cur_kvSplitSize false headSize_even query_t_padding_ptr q_data + i qStrideB + j qStrideH + m qStrideM key_reorder_ptr + i_kv num_head_k eheadSize kvSize + j_kv eheadSize kvSize + n eheadSize qk_data need_pack kernel kernel_name _mul_scale_kernel accum_t qk_data scaling_factor cur_qSplitSize cur_kvSplitSize - score_mod mask_mod TODO reduce number calls q_idx kv_idx initialization std vector int _t q_idx cur_qSplitSize int _t i = i cur_qSplitSize ++i q_idx i = m + i std vector int _t kv_idx cur_kvSplitSize int _t i = i cur_kvSplitSize ++i kv_idx i = n + i std vector int _t b_idx = i std vector int _t h_idx = j accum_t in_ptr = qk_data auto in_ptr = b_idx data auto in_ptr = h_idx data auto in_ptr = q_idx data auto in_ptr = kv_idx data apply score mod function template generate_other_buffer score_others len_score_other kernel args accum_t out_ptr score_buf_idx = in_ptr template modification score_mod score_buf_name score_buf_idx &#124; indent false std find kv_indice_list begin kv_indice_list end cur_n = kv_indice_list end Apply block mask fill unused -inf template generate_other_buffer mask_others - len_mask_other kernel args accum_t out_ptr mask_buf_idx = in_ptr template modification mask_mod mask_buf_name mask_buf_idx &#124; indent false - endif Update coefficients Softmax accum_t tmp_max = tmp_sum = exp_tmp = int _t row = row cur_qSplitSize ++row apply scaling factor max per row fusion kernel kernel_name _mul_reduce_max_fusion_kernel qk_data + row cur_kvSplitSize static_cast accum_t cur_kvSplitSize qk_data + row cur_kvSplitSize tmp_max tmp_max = qk_max_data row tmp_max qk_max_data row tmp_max tmp_max == -std numeric_limits accum_t infinity avoid ` nan = exp f -inf - -inf ` kernel kernel_name _fill_stub kernel kernel_name _conditional_data_ptr qk_data qk_reduced_data + row cur_ekvSplitSize static_cast scalar_t cur_kvSplitSize tmp_sum = tmp_max qk - exp qk - max sum per row kernel kernel_name _exp_reduce_sum_fusion_kernel qk_data + row cur_kvSplitSize cur_kvSplitSize kernel kernel_name _conditional_data_ptr qk_data qk_reduced_data + row cur_ekvSplitSize tmp_sum exp_tmp - exp max row - max exp_tmp = std exp qk_max_data row - tmp_max sum row - sum + exp_tmp sum row qk_sum_data row = tmp_sum + exp_tmp qk_sum_data row max row - max qk_max_data row = tmp_max dst - dst exp_tmp n_idx vec map accum_t exp_tmp Vec x x Vec exp_tmp dst_data + row headSize_v dst_data + row headSize_v headSize_v need_pack cur_kvSplitSize = Pad qSplitSize cur_kvSplitSize - qSplitSize cur_kvSplitSize + qk_reduced_data + row + cur_kvSplitSize + cur_kvSplitSize = scalar_t Calculate Softmax q k T v need_pack auto v_addr = v_data + i_kv vStrideB + j_kv vStrideH + n vStrideN Fallback Half brgemm slower than micro gemm std is_same_v scalar_t Half native cpublas brgemm cur_qSplitSize headSize_v cur_ekvSplitSize cur_ekvSplitSize vStrideN headSize_v n_idx kernel kernel_name _conditional_data_ptr qk_data qk_reduced_data v_addr dst_data need_pack n_idx kernel kernel_name _kernel_micro_gemm static_cast bool true kernel kernel_name _conditional_data_ptr qk_data qk_reduced_data v_addr dst_data cur_qSplitSize headSize_v cur_ekvSplitSize cur_ekvSplitSize vStrideN headSize_v kernel kernel_name _kernel_micro_gemm static_cast bool false kernel kernel_name _conditional_data_ptr qk_data qk_reduced_data v_addr dst_data cur_qSplitSize headSize_v cur_ekvSplitSize cur_ekvSplitSize vStrideN headSize_v int _t psize = n kvSplitSize ekvSplitSize native cpublas brgemm cur_qSplitSize headSize_v cur_ekvSplitSize cur_ekvSplitSize headSize_v headSize_v n_idx qk_reduced_data value_reorder_ptr + i_kv num_head_k kv_padding_size headSize_v + j_kv kv_padding_size headSize_v + psize headSize_v dst_data need_pack dst - dst sum row reorder MHA output strides int _t row = row cur_qSplitSize ++row Row sums full masked out rows we set them order avoid NaNs output instead set fully masked out rows qk_max_data row = qk_max_data row == -std numeric_limits accum_t infinity qk_max_data row qk_sum_data row = qk_sum_data row == qk_sum_data row accum_t sum_reciprocal = qk_sum_data row vec map scalar_t sum_reciprocal is_skip_kv Vec x is_skip_kv Vec x Vec sum_reciprocal out_data + i oStrideB + j oStrideH + m oStrideM + row oStrideM dst_data + row headSize_v headSize_v Move next query native data_index_step i batchSize j num_head k qSlice native cpublas brgemm_release need_pack CppFlexAttentionTemplate CppTemplate __init__ input_nodes layout ir Layout scale score_mod mask_mod kv_block_size q_block_size has_other_buffer no_full_kv_block fake_buffers len_score_other len_mask_other kernel_input_name_to_buffer block_vars - None assert layout dtype torch float torch bfloat torch float super __init__ flex_attention input_nodes layout parallel_num_threads scale = scale score_mod = score_mod mask_mod = mask_mod score_buf_name = V graph register_buffer score_mod score_mod None mask_buf_name = V graph register_buffer mask_mod mask_mod None get_idx buf_name match = re search r \d+ buf_name assert match f incorrect score buf name buf_name match group score_buf_idx = get_idx score_buf_name score_buf_name None mask_buf_idx = get_idx mask_buf_name mask_buf_name None kv_block_size = kv_block_size q_block_size = q_block_size has_other_buffer = has_other_buffer no_full_kv_block = no_full_kv_block other_buffer_input_offset = no_full_kv_block other_buffer_input_offset = fake_buffers = fake_buffers len_score_other = len_score_other len_mask_other = len_mask_other kernel_input_name_to_buffer = kernel_input_name_to_buffer block_vars = block_vars extra_sizevars = list OrderedSet val val kernel_input_name_to_buffer values isinstance val sympy Symbol other_buf_start_idx = score_mod_other_buffers = input_nodes other_buf_start_idx + other_buffer_input_offset other_buf_start_idx + other_buffer_input_offset + len_score_other has_other_buffer None mask_mod_other_buffers = input_nodes other_buf_start_idx + other_buffer_input_offset + len_score_other has_other_buffer None other_ptr_data = type ignore var-annotated update_kernel_args kernel_args kernel_args update key value key value kernel_input_name_to_buffer items isinstance value sympy Symbol kernel_args generate_other_buffer buf_list start_offset len_attr kernel_args kernel_input_name_to_buffer_name = key value isinstance value sympy Symbol value get_name key value kernel_input_name_to_buffer items get_arg name kernel_input_name_to_buffer_name get name get_arg_name name isinstance get_arg name sympy Symbol kernel_args sizevars get get_arg name kernel_args input_buffers get get_arg name has_other_buffer start_offset == - start_offset = len_score_other length = getattr len_attr i range length pointer = f in_ptr other_buf_start_idx + start_offset + i buffer_key = f buf_list _ i pointer other_ptr_data other_ptr_data pointer = get_arg_name buffer_key get_arg buffer_key \n join f auto ptr = name ptr name _ other_ptr_data items modification subgraph_buffer output_name output_idx assert isinstance subgraph_buffer ir ComputedBuffer subgraph_buffer_data = subgraph_buffer data loop_body LoopBody utils sympy_index_symbol_with_prefix SymT virtualized V cpp CppKernelProxy KernelGroup ParallelDepth kernel_group = KernelGroup kernel_input_args = score in_ptr b in_ptr h in_ptr q_idx in_ptr kv_idx in_ptr has_other_buffer kernel_input_args update arg ptr ptr _ arg other_ptr_data items kernel_output_args = output_name f out_ptr output_idx args = kernel_group args name inp kernel_input_args items args input_buffers name = inp name inp kernel_output_args items args output_buffers name = inp name extra_sizevars args sizevars name = f k name kernel_group args = args cpp_kernel_proxy = CppKernelProxy kernel_group bodies = var_sizes_list = var_sizes = tuple subgraph_buffer get_size var_ranges = sympy_index_symbol_with_prefix SymT INDEX i sz i sz enumerate var_sizes dst_layout = subgraph_buffer get_layout output_index = dst_layout make_indexer var_ranges keys fn args V ops store output_name output_index subgraph_buffer_data make_loader args value body = LoopBody fn list var_ranges keys var_ranges list var_ranges keys tuple loop_body MemoryUsageType assert all mem buffer_name kernel_group args input_buffers mem body memory_usage MemoryUsageType LOAD All buffers score mask subgraph should kernel_group args input_buffers bodies append body var_sizes_list append var_sizes cpp_kernel_proxy codegen_loop_bodies bodies var_sizes_list max_parallel_depth ParallelDepth parallel_depth= start_depth= This loop parallelized since outermost loop patch object cpp_kernel_proxy loop_nest max_parallel_depth max_parallel_depth kernel_group finalize_kernel cpp_kernel_proxy output_code = kernel_group loops_code getvalue var_q_symbol var_kv_symbol = block_vars See Note Handle case where split sizes statically known We don t know value qBlockSize rkvBlockSize during compilation time thus we ve represented them symbols We change symbol strings back cur_qSplitSize cur_kvSplitSize generated code thus they ll filled real value during runtime var_q_symbol kernel_group args sizevars output_code = output_code replace kernel_group args sizevars var_q_symbol cur_qSplitSize var_kv_symbol kernel_group args sizevars output_code = output_code replace kernel_group args sizevars var_kv_symbol cur_kvSplitSize output_code staticmethod add_choices choices input_nodes layout scale score_mod mask_mod kv_block_size q_block_size has_other_buffer no_full_kv_block fake_buffers len_score_other len_mask_other kernel_input_name_to_buffer block_vars preprocessor input_nodes layout input_nodes layout postprocessor output output template = DataProcessorTemplateWrapper CppFlexAttentionTemplate preprocessor postprocessor input_nodes=input_nodes layout=layout scale=scale score_mod=score_mod mask_mod=mask_mod kv_block_size=kv_block_size q_block_size=q_block_size has_other_buffer=has_other_buffer no_full_kv_block=no_full_kv_block fake_buffers=fake_buffers len_score_other=len_score_other len_mask_other=len_mask_other kernel_input_name_to_buffer=kernel_input_name_to_buffer block_vars=block_vars template maybe_append_choice choices template apply_score_mod score b h q_idx kv_idx score_mod graph_module score b h q_idx kv_idx item render type ignore override kernel template_buffer_node Optional ir CppTemplateBuffer = None epilogue_nodes Optional list ir IRNode = None kwargs - str epilogue_nodes None epilogue_nodes = raise NotImplementedError Unsupported ` epilogue_nodes ` CppFlexAttentionTemplate Query Batch x Num_heads x Q_seq_len x Dim_per_head - Batch x Q_seq_len x Num_heads x Dim_per_head Key Batch x Num_heads x KV_seq_len x Dim_per_head - Batch x KV_seq_len x Num_heads x Dim_per_head Value Batch x Num_heads x KV_seq_len x Dim_per_head - Batch x KV_seq_len x Num_heads x Dim_per_head query = kernel permute input_nodes key = kernel permute input_nodes value = kernel permute input_nodes accumulate_dtype = torch float input_dtype = query layout dtype num_threads = parallel_num_threads assert isinstance output_node ir IRNode buf_out ir IRNode = TensorBox create output_node template_buffer_node None buf_out = template_buffer_node options = dict query=query key=key value=value kv_num_blocks=self input_nodes kv_indices=self input_nodes full_kv_num_blocks= input_nodes no_full_kv_block None full_kv_indices=self input_nodes no_full_kv_block None score_mod_other_buffers=self score_mod_other_buffers mask_mod_other_buffers=self mask_mod_other_buffers scale=self scale has_full_kv_block=not no_full_kv_block accumulate_dtype=self accumulate_dtype query_dtype=self input_dtype kvBlockSize=self kv_block_size qBlockSize=self q_block_size template=self output=buf_out kernel=kernel num_thread=num_threads score_mod=self score_mod mask_mod=self mask_mod score_buf_name=self score_buf_name mask_buf_name=self mask_buf_name score_buf_idx=self score_buf_idx mask_buf_idx=self mask_buf_idx contextlib ExitStack stack buf fake_buffers stack enter_context patch object V graph get_dtype _fake_get_dtype buf _template_from_string FLEX_ATTENTION_TEMPLATE render options codegen_softmax_fusion kernel_name str TODO use inductor IR rewrite those fusions _template_from_string SOFTMAX_FUSIONS render dict kernel_name=kernel_name codegen_brgemm_pack_function kernel_name str TODO make them general common bmm templates _template_from_string BRGEMM_PACK_FUNCTIONS render dict kernel_name=kernel_name codegen_allocate_buffer buffer_name str buffer_dtype buffer_size _template_from_string ALLOCATE_BUFFER render dict buffer_name=buffer_name buffer_dtype=buffer_dtype buffer_size=buffer_size micro_gemm_define kernel_name str torch _inductor codegen cpp_gemm_template CppTemplateKernel parallel_num_threads torch _inductor codegen cpp_micro_gemm CppMicroGemmFP Vec torch _inductor virtualized V micro_gemm_trans = CppMicroGemmFP Vec kernel_name + _kernel_micro_gemm_transpose_b input_dtype input_dtype accumulate_dtype accumulate_dtype GemmBlocking True True micro_gemm = CppMicroGemmFP Vec kernel_name + _kernel_micro_gemm input_dtype input_dtype accumulate_dtype accumulate_dtype GemmBlocking True False V set_graph_handler V graph kernel = CppTemplateKernel cpp_micro_gemm parallel_num_threads code_trans = micro_gemm_trans codegen_define kernel code = micro_gemm codegen_define kernel code + code_trans codegen_micro_gemm kernel_name str micro_gemm = micro_gemm_define kernel_name GEMM_SOURCE_CODE = MICRO_GEMM_TEMPLATE replace GEMM_DEFINE micro_gemm _template_from_string GEMM_SOURCE_CODE render