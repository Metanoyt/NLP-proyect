ast contextlib inspect threading collections abc Callable Generator Iterable typing Any Optional Union torch utils _exposed_in exposed_in custom_ops custom_op CustomOpDef infer_schema infer_schema triton_ops_to_kernels dict str list object = get_triton_kernels_for_op name str - list object triton_ops_to_kernels get name get_inner_triton_kernels fn Callable Any - list object Inspect source arbitrary callable passed torch _library triton_op grab all triton kernels wrapped inside TODO This check best effort It does handle case where triton kernel hidden behind recursive function calls find_triton_kernels fn Callable Any - list object try source = inspect getsource fn except OSError TypeError Source code available torch _inductor utils IndentedBuffer buffer = IndentedBuffer buffer splice source strip=True tree = ast parse buffer getrawvalue Visitor collect function calls triton kernels Visitor ast NodeVisitor __init__ - None triton_kernels list Any = visit_Call node ast Call - None triton_func_names = capture_triton wrap_triton isinstance node func ast Attribute attr = node func isinstance attr value ast Attribute isinstance attr value value ast Name attr value value id == torch attr value attr == _library attr attr triton_func_names node args isinstance node args ast Name triton_kernels append node args id Catch capture_triton wrap_triton s been imported directly isinstance node func ast Name node func id triton_func_names node args isinstance node args ast Name triton_kernels append node args id generic_visit node collector = Visitor collector visit tree closure_vars = inspect getclosurevars fn resolved = First resolve triton kernel names name collector triton_kernels name closure_vars nonlocals resolved append closure_vars nonlocals name name closure_vars globals resolved append closure_vars globals name name closure_vars builtins resolved append closure_vars builtins name resolved find_triton_kernels fn exposed_in torch library triton_op name str fn Optional Callable = None mutates_args Union str Iterable str schema Optional str = None - Callable Create custom operator whose implementation backed + triton kernels This more structured way using triton kernels PyTorch Prefer using triton kernels no ` ` torch library ` ` custom operator wrappers like func ` torch library custom_op ` func ` torch library triton_op ` because simpler only use func ` torch library custom_op ` func ` torch library triton_op ` you want create operator behaves like PyTorch built-in operators For example you may use ` ` torch library ` ` wrapper API define behavior triton kernel when passed tensor subclass under TorchDispatchMode Use func ` torch library triton_op ` instead func ` torch library custom_op ` when implementation consists + triton kernels func ` torch library custom_op ` treats custom operators opaque func ` torch compile ` func ` torch export export ` will never trace into them ` ` triton_op ` ` makes implementation visible these subsystems allowing them optimize triton kernel s Note ` ` fn ` ` must only consist calls PyTorch-understood operators triton kernels Any triton kernels called inside ` ` fn ` ` must wrapped call func ` torch library wrap_triton ` Args name str A name custom op looks like namespace name e g mylib my_linear The name used op s stable identifier PyTorch subsystems e g torch export FX graphs To avoid name collisions please use your project name namespace e g all custom ops pytorch fbgemm use fbgemm namespace mutates_args Iterable str unknown The names args function mutates This MUST accurate otherwise behavior undefined If unknown pessimistically assumes all inputs operator being mutated schema None &#124; str A schema string operator If None recommended we ll infer schema operator its type annotations We recommend letting us infer schema unless you have specific reason Example Tensor x int y - Tensor Tensor Example xdoctest +REQUIRES env TORCH_DOCTEST_CUDA torch torch library triton_op wrap_triton triton triton language tl triton jit add_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask triton_op mylib add mutates_args= add x torch Tensor y torch Tensor - torch Tensor output = torch empty_like x n_elements = output numel grid meta triton cdiv n_elements meta BLOCK_SIZE NB we need wrap triton kernel call wrap_triton wrap_triton add_kernel grid x y output n_elements output torch compile f x y add x y x = torch randn device= cuda y = torch randn device= cuda z = f x y assert torch allclose z x + y dec fn Callable object - CustomOpDef backend_fn args kwargs type ignore no-untyped-def Optimization we re passing regular Tensors into triton kernel so no need go through HOP dispatch set_wrap_triton_enabled False fn args kwargs result = custom_op name backend_fn mutates_args=mutates_args schema=infer_schema fn mutates_args=mutates_args _subclasses functional_tensor FunctionalTensorMode We require user pass us function make_fx traceable so we can just register Fake meta kernel result register_fake fn We decompose operator when FunctionalTensorMode active The goal decompose operator AOTDispatcher - With torch compile means backend usually Inductor can see call triton kernel s so can directly optimize them inlining them into lowering process functional_decomp type ignore no-untyped-def mode op types args kwargs NOTE Export custom triton op For torch export strict non-strict we don t do functional decomposition Instead we preserve custom triton ops custom ops This because we want exported program high-level serializable If we decompose custom op functional hop make node exported program we need figure out ways serializing hop its arguments which can triton jited functions triton dtypes This undesirable because - can tedious maintain layer serializes jited function e g string dtypes - exported program will contain implementation detail e g triton source code specific backend GPU which probably wrong level abstraction - changes triton serialization logic triton arguments can BC breaking In short term we expect users have separate aot_compile stage compiles exported program into Cubin file same machine users call export which does autotuning removes triton dependency serve model Cubin This guarantees triton changes won t break BC In long term we may export multiple cubins triton op directly torch export _trace custom_triton_ops_decomposition_disabled custom_triton_ops_decomposition_disabled mode __torch_dispatch__ op types args kwargs TODO https github com pytorch pytorch issues We should deduplicate unrecognized_types logic torch _subclasses unrecognized_types = t t types issubclass t torch _subclasses FakeTensor t torch Tensor torch _subclasses functional_tensor FunctionalTensor unrecognized_types NotImplemented mode fn args kwargs triton_kernels = get_inner_triton_kernels fn triton_ops_to_kernels name = triton_kernels result register_torch_dispatch FunctionalTensorMode functional_decomp result fn None dec dec fn wrap_triton_enabled = threading local wrap_triton_enabled_default = True contextlib contextmanager set_wrap_triton_enabled enabled bool - Generator None None None If triton kernels annotated wrap_triton should dispatch via HOP go straight triton kernel execution We have switch because eager-mode performance HOP dispatch slow enough matter ~ ms we know wrap_triton isn t necessary some situations eager-mode regular Tensors try prev = is_wrap_triton_enabled wrap_triton_enabled value = enabled yield finally wrap_triton_enabled value = prev is_wrap_triton_enabled - bool getattr wrap_triton_enabled value wrap_triton_enabled_default capture_triton triton_kernel Callable - Any This API has been renamed wrap_triton wrap_triton triton_kernel exposed_in torch library wrap_triton triton_kernel Callable - Any Allows capture triton kernel into graph via make_fx non-strict ` ` torch export ` ` These technologies perform Dispatcher-based tracing via ` ` __torch_dispatch__ ` ` cannot see calls raw triton kernels The ` ` wrap_triton ` ` API wraps triton kernel into callable can actually traced into graph Please use API together func ` torch library triton_op ` Examples xdoctest +SKIP torch triton triton language tl torch fx experimental proxy_tensor make_fx torch library wrap_triton triton jit add_kernel in_ptr in_ptr out_ptr n_elements BLOCK_SIZE tl constexpr pid = tl program_id axis= block_start = pid BLOCK_SIZE offsets = block_start + tl arange BLOCK_SIZE mask = offsets n_elements x = tl load in_ptr + offsets mask=mask y = tl load in_ptr + offsets mask=mask output = x + y tl store out_ptr + offsets output mask=mask add x y output = torch empty_like x n_elements = output numel grid_fn meta triton cdiv n_elements meta BLOCK_SIZE wrap_triton add_kernel grid_fn x y output n_elements output x = torch randn device= cuda y = torch randn device= cuda gm = make_fx add x y print gm code forward x_ y_ empty_like = torch ops aten empty_like default x_ pin_memory = False triton_kernel_wrapper_mutation_proxy = triton_kernel_wrapper_mutation kernel_idx = constant_args_idx = grid = kwargs = in_ptr x_ in_ptr y_ out_ptr empty_like n_elements BLOCK_SIZE empty_like triton runtime autotuner Autotuner triton runtime jit JITFunction torch _higher_order_ops triton_kernel_wrap TraceableTritonKernelWrapper isinstance triton_kernel JITFunction Autotuner raise RuntimeError wrap_triton only works functions annotated triton jit triton autotune is_wrap_triton_enabled triton_kernel TraceableTritonKernelWrapper triton_kernel None None