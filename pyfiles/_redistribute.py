mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates contextlib dataclasses itertools logging weakref collections defaultdict collections abc Sequence functools cache typing cast NamedTuple Optional torch torch distributed _functional_collectives funcol torch distributed tensor _api dtensor torch distributed _functional_collectives _are_we_tracing torch distributed tensor _dtensor_spec DTensorSpec ShardOrder ShardOrderEntry TensorMeta torch distributed tensor device_mesh DeviceMesh torch distributed tensor placement_types Partial Placement Replicate Shard torch utils _debug_mode get_active_debug_mode logger = logging getLogger __name__ _TransformInfo NamedTuple mesh_dim int src_dst_placements tuple Placement Placement logical_shape mesh dimension logical_shape list int Global cache DTensorRedistributePlanner instances _planner_cache dict tuple weakref ReferenceType int DTensorRedistributePlanner = get_redistribute_planner device_mesh DeviceMesh tensor_dimension int - DTensorRedistributePlanner Factory function get create DTensorRedistributePlanner instance This function provides transparent caching planner instances based device_mesh tensor_dimension Multiple calls same parameters will same cached instance better performance Args device_mesh The device mesh planner tensor_dimension Number tensor dimensions Returns A DTensorRedistributePlanner instance potentially cached cache_key = weakref ref device_mesh tensor_dimension cache_key _planner_cache planner = DTensorRedistributePlanner device_mesh tensor_dimension _planner_cache cache_key = planner _planner_cache cache_key clear_redistribute_planner_cache - None Clear cache DTensorRedistributePlanner instances _planner_cache clear DTensorRedistributePlanner This used plan collective calls transform local shard DTensor its current spec target spec Suppose there N tensor dimensions M mesh dimensions total possible state size will N+ M M Note Use get_redistribute_planner factory function instead direct instantiation automatic caching dataclasses dataclass frozen=True slots=True DistState placements tuple Placement tensor_dim_to_mesh_dim ShardOrder _hash Optional int = dataclasses field default=None init=False repr=False compare=False __str__ DTensorSpec format_shard_order_str placements tensor_dim_to_mesh_dim __repr__ __str__ __post_init__ precompute hash after all attributes set object __setattr__ _hash _compute_hash __hash__ - int _hash _hash None _compute_hash _compute_hash - int hash placements tensor_dim_to_mesh_dim __eq__ other object - bool isinstance other DTensorRedistributePlanner DistState False _hash = other _hash False placements tensor_dim_to_mesh_dim == other placements other tensor_dim_to_mesh_dim _to_tuple x Convert nested list structure nested tuple structure isinstance x list &#124; tuple tuple _to_tuple item item x x staticmethod _dict_to_ShardOrder x dict int list int - ShardOrder Convert dict ShardOrder tuple ShardOrderEntry tensor_dim=key mesh_dims=tuple value key value sorted x items value staticmethod _ShardOrder_to_dict x ShardOrder - dict int list int Convert ShardOrder dict tensor dim key tensor_mesh_dim_dict = defaultdict list entry x tensor_mesh_dim_dict entry tensor_dim = list entry mesh_dims tensor_mesh_dim_dict staticmethod stringify_transform_infos mesh DeviceMesh transform_infos Sequence _TransformInfo src_placement tuple Placement src_shard_order Optional ShardOrder = None - str Generate string representation sequence state transitions placements shard orders described given transform_info Args mesh The DeviceMesh used redistribution transform_infos A sequence _TransformInfo objects describing each transformation step src_placement The initial tuple Placement objects src_shard_order Optional The initial ShardOrder representing mapping tensor dimensions mesh dimensions If None default shard order computed src_placement mesh Returns A string showing sequence DistState transitions separated - assert len src_placement == mesh ndim src_shard_order None src_shard_order = DTensorSpec compute_default_shard_order src_placement cur_placement = list src_placement shard_order_dict = DTensorRedistributePlanner _ShardOrder_to_dict src_shard_order cur_state = DTensorRedistributePlanner DistState tuple cur_placement src_shard_order state_list = cur_state transform_info transform_infos src_dim_placement dst_dim_placement = transform_info src_dst_placements src_dim_placement is_shard src_dim = src_dim_placement dim type ignore attr-defined assert src_dim shard_order_dict len shard_order_dict src_dim shard_order_dict src_dim pop dst_dim_placement is_shard dst_dim = dst_dim_placement dim type ignore attr-defined dst_dim shard_order_dict shard_order_dict dst_dim = shard_order_dict dst_dim append transform_info mesh_dim cur_placement transform_info mesh_dim = dst_dim_placement new_state = DTensorRedistributePlanner DistState tuple cur_placement DTensorRedistributePlanner _dict_to_ShardOrder shard_order_dict state_list append new_state - join str s s state_list __init__ device_mesh DeviceMesh tensor_dimension int - None Initialize DTensorRedistributePlanner Args device_mesh The device mesh planner tensor_dimension Number tensor dimensions device_mesh = device_mesh coordinate = device_mesh get_coordinate assert coordinate None tensor_dimension = tensor_dimension setup_collective_cost setup_collective_cost all_reduce_cost int = all_to_all_cost int = all_gather_cost int = reduce_scatter_cost int = chunk_cost int = - None Set up cost weights different collective operations those can turned handler considering tensor dim size all_reduce_cost = all_reduce_cost all_to_all_cost = all_to_all_cost all_gather_cost = all_gather_cost reduce_scatter = reduce_scatter_cost chunk_cost = chunk_cost get_next_state placements tuple Placement tensor_mesh_dim_tuple ShardOrder - dict DTensorRedistributePlanner DistState int We map tensor dimensions device mesh axes similar JAX-style sharding representation Notation S tensor_dim list_of_device_dims means tensor dimension tensor_dim sharded listed device mesh axes where list_of_device_dims sorted device order To generalize arbitrary dimensionality we use following notation S x tensor dimension sharded device mesh axes x variadic possibly empty R replicated listed device mesh axes possibly empty P partial listed device mesh axes possibly empty The ellipsis denotes variadic wildcard i e zero more device mesh axes Below possible transitions one sharding state another We use ` S ` Shard ` R ` Replicate ` P ` Partial Case Shard - Shard b use all-to-all applies S x - S b x S x y S b z k - S x S b z k y where device order y device order z k Case Shard - Replicate use all-gather applies S x y z - S x y Case Partial - Replicate use all-reduce applies P x y - P y P x Note case can disabled because all-reduce technically primitive since combines reduce-scatter + all-gather Case Replicate - Shard use chunk applies S z - S z y ` ` can any tensor dim Note y must after z Case Partial - Shard use reduce-scatter applies P x y - P x S y P x y - P y S x Case Replicate - Partial local math op applies R - P x NB Device order Partial placement doesn t take impact We should able operate any Partial mesh dim list DistState cost all_next_state dict DTensorRedistributePlanner DistState int = tensor_mesh_dim_dict = DTensorRedistributePlanner _ShardOrder_to_dict tensor_mesh_dim_tuple ###################################################################### handle case Shard - Shard b For S S b only last device order S S b can interchangeably convert sparse tuple entry tensor_mesh_dim_tuple src_tensor_dim = entry tensor_dim dst_tensor_dim range tensor_dimension src_tensor_dim == dst_tensor_dim continue try move last sharded device dim Shard src_tensor_dim Shard dst_tensor_dim move_mesh_dim = tensor_mesh_dim_dict src_tensor_dim pop tensor_mesh_dim_dict dst_tensor_dim append move_mesh_dim new_placements = list placements new_placements move_mesh_dim = Shard dst_tensor_dim dist_state = DistState _to_tuple new_placements DTensorRedistributePlanner _dict_to_ShardOrder tensor_mesh_dim_dict all_next_state dist_state = all_to_all_cost reset content next iteration tensor_mesh_dim_dict src_tensor_dim append move_mesh_dim tensor_mesh_dim_dict dst_tensor_dim pop TODO zpcore support discovering submesh prevent padding when tensor dim divisible mesh dim ###################################################################### handle case Shard - Replicate entry tensor_mesh_dim_tuple src_tensor_dim = entry tensor_dim move_mesh_dim = tensor_mesh_dim_dict src_tensor_dim pop new_placements = list placements new_placements move_mesh_dim = Replicate dist_state = DistState _to_tuple new_placements DTensorRedistributePlanner _dict_to_ShardOrder tensor_mesh_dim_dict tensor_mesh_dim_dict src_tensor_dim append move_mesh_dim all_next_state dist_state = all_gather_cost ###################################################################### handle case Partial - Replicate src_mesh_dim placement enumerate placements isinstance placement Partial continue new_placements = list placements new_placements src_mesh_dim = Replicate dist_state = DistState _to_tuple new_placements tensor_mesh_dim_tuple all_next_state dist_state = all_reduce_cost ###################################################################### handle case Replicate - Shard mesh_dim placement enumerate placements isinstance placement Replicate continue dst_tensor_dim range tensor_dimension try convert placement mesh_dim Shard dst_tensor_dim new_placements = list placements new_placements mesh_dim = Shard dst_tensor_dim tensor_mesh_dim_dict dst_tensor_dim append mesh_dim dist_state = DistState _to_tuple new_placements DTensorRedistributePlanner _dict_to_ShardOrder tensor_mesh_dim_dict all_next_state dist_state = chunk_cost tensor_mesh_dim_dict dst_tensor_dim pop ###################################################################### handle case Partial - Shard mesh_dim placement enumerate placements isinstance placement Partial continue dst_tensor_dim range tensor_dimension try convert placement mesh_dim Shard dst_tensor_dim new_placements = list placements new_placements mesh_dim = Shard dst_tensor_dim tensor_mesh_dim_dict dst_tensor_dim append mesh_dim dist_state = DistState _to_tuple new_placements DTensorRedistributePlanner _dict_to_ShardOrder tensor_mesh_dim_dict all_next_state dist_state = reduce_scatter tensor_mesh_dim_dict dst_tensor_dim pop ###################################################################### handle case Replicate - Partial default partial sum mesh_dim placement enumerate placements isinstance placement Replicate continue new_placements = list placements new_placements mesh_dim = Partial dist_state = DistState _to_tuple new_placements tensor_mesh_dim_tuple all_next_state dist_state = chunk_cost all_next_state TODO zpcore dst_state contains special placement like ` _MaskPartial ` we will never reach state Need support case find_min_cost_path src_state DistState dst_state DistState - list DTensorRedistributePlanner DistState Find min cost path src_state dst_state using Dijkstra s algorithm Args src_state The source state dst_state The destination state Returns A list states representing min cost path src_state dst_state heapq priority queue cost counter state path Dijkstra s algorithm use counter break ties avoid comparing DistState objects counter = pq list tuple int int DTensorRedistributePlanner DistState list DTensorRedistributePlanner DistState = counter src_state src_state visited = set while pq cost _ current_state path = heapq heappop pq current_state == dst_state path current_state visited continue visited add current_state get all possible next states their costs next_states = get_next_state current_state placements current_state tensor_dim_to_mesh_dim next_state transition_cost next_states items next_state visited new_cost = cost + transition_cost new_path = path + next_state counter += heapq heappush pq new_cost counter next_state new_path raise AssertionError f No path found src_state src_state dst_state dst_state get_logical_shape src_state DTensorRedistributePlanner DistState mesh_dim int full_tensor_shape tuple int - list int new_logical_shape = list full_tensor_shape assert coordinate None entry src_state tensor_dim_to_mesh_dim tensor_dim = entry tensor_dim mesh_dims = entry mesh_dims assert len mesh_dims mdim mesh_dims mdim == mesh_dim continue new_size = Shard local_shard_size_and_offset new_logical_shape tensor_dim device_mesh size mesh_dim=mdim coordinate mdim new_logical_shape tensor_dim = new_size new_logical_shape generate_graph_based_transform_infos src_spec DTensorSpec dst_spec DTensorSpec full_tensor_shape tuple int - list _TransformInfo assert src_spec shard_order None dst_spec shard_order None src_state = DistState src_spec placements src_spec shard_order dst_state = DistState dst_spec placements dst_spec shard_order transform_infos list _TransformInfo = state_path = find_min_cost_path src_state dst_state cur_state nxt_state itertools pairwise state_path find mesh_dim different between cur_state nxt_state cur_state placements = nxt_state placements update_mesh_dim = - mesh_dim cur_placement nxt_placement enumerate zip cur_state placements nxt_state placements cur_placement = nxt_placement update_mesh_dim = - raise AssertionError Multiple mesh_dims different between cur_state nxt_state update_mesh_dim = mesh_dim logical_shape = get_logical_shape cur_state mesh_dim full_tensor_shape transform_infos append _TransformInfo mesh_dim=update_mesh_dim src_dst_placements= cur_placement nxt_placement logical_shape=logical_shape transform_infos generate_greedy_transform_infos src_spec DTensorSpec dst_spec DTensorSpec - list _TransformInfo Generate transform infos source placements target placements To transform source target placement might have multiple steps i e might decompose Si - Sj into Si - R - Sj This would detect there re mis-aligned nested shardings between src dst placements E g Suppose redistribution perform Shard Shard - Replicate Shard case Shard - Shard mesh dimension actually needs resharding because former nested-sharding tensor already already sharded dimension whereas latter first sharding tensor dimension logical shape records logic tensor shape mesh dimension useful ensure uneven sharding gets correct output shape assert coordinate None initial_logical_shape = list src_spec shape mesh_dims_to_logical_shape = initial_logical_shape transform_infos list _TransformInfo = device_mesh ndim == device_mesh D redistribute simple direct transformation transform_infos append _TransformInfo mesh_dim= src_dst_placements= src_spec placements dst_spec placements logical_shape=initial_logical_shape transform_infos Handle multi-dim device mesh placement redistribution First we need build logical shape each mesh dim correct allgather uneven shards each mesh dim dynamic padding i src enumerate src_spec placements current_logical_shape = mesh_dims_to_logical_shape i isinstance src Shard i device_mesh ndim - calculate save logical shape sharding mesh_dim_size = device_mesh size mesh_dim=i local_shard_size _ = src _local_shard_size_and_offset current_logical_shape src dim mesh_dim_size coordinate i new_logical_shape = list current_logical_shape new_logical_shape src dim = local_shard_size mesh_dims_to_logical_shape append new_logical_shape mesh_dims_to_logical_shape append current_logical_shape Next we need derive transform infos src dst placements here we use greedy search step step state transformations current_placements = list src_spec placements target_placements = list dst_spec placements src_spec num_shards If src_spec have sharding could potentially have sharding misaligned dst_spec common case nested sharding i e S S - R S In those cases we first traverse inner placement outer placement detect misaligned shardings properly replicate nested sharding first mesh_dim reversed range len current_placements current = current_placements mesh_dim target = target_placements mesh_dim If target Shard we can directly redistribute since we traversing inner outer placements here isinstance target Shard If target Shard check nested sharding tensor dim BEFORE current mesh_dim shard_dim = target dim current_mesh_sharding target_mesh_sharding = i s p enumerate zip current_placements target_placements i = mesh_dim break s is_shard shard_dim current_mesh_sharding append i p is_shard shard_dim target_mesh_sharding append i current_mesh_sharding = target_mesh_sharding current target_placements have misaligned sharding tensor dim BEFORE current mesh_dim we need replicate tensor mesh dim first clear nested sharding target = Replicate current = target transform_infos append _TransformInfo mesh_dim=mesh_dim src_dst_placements= current target logical_shape=mesh_dims_to_logical_shape mesh_dim current_placements mesh_dim = target We always traverse outer placement inner placement collect remaining needed transform infos i e replication nested sharding might need further perform resharding Shard again mesh_dim current target enumerate zip current_placements target_placements current = target transform_infos append _TransformInfo mesh_dim=mesh_dim src_dst_placements= current target logical_shape=mesh_dims_to_logical_shape mesh_dim current_placements mesh_dim = target transform_infos _gen_transform_infos_non_cached src_spec DTensorSpec dst_spec DTensorSpec use_graph_based_transform Optional bool = None - list _TransformInfo transform_infos list _TransformInfo = device_mesh = src_spec device_mesh src_shard_order = src_spec shard_order dst_shard_order = dst_spec shard_order DTensorSpec should automatically generate shard_order can no shard assert src_shard_order None dst_shard_order None use_graph_based_transform None all DTensorSpec is_default_device_order order order src_shard_order dst_shard_order use_graph_based_transform = False switch graph search algorithm device order default use_graph_based_transform = True drp = get_redistribute_planner device_mesh len src_spec shape use_graph_based_transform transform_infos = drp generate_graph_based_transform_infos src_spec dst_spec src_spec shape transform_infos = drp generate_greedy_transform_infos src_spec dst_spec transform_infos cache _gen_transform_infos src_spec DTensorSpec dst_spec DTensorSpec use_graph_based_transform Optional bool = None - list _TransformInfo _gen_transform_infos_non_cached src_spec dst_spec use_graph_based_transform redistribute_local_tensor local_tensor torch Tensor current_spec DTensorSpec target_spec DTensorSpec async_op bool = False is_backward bool = False use_graph_based_transform Optional bool = None - torch Tensor This redistribute local tensor torch Tensor current DTensorSpec target DTensorSpec which involves necessary collective calls transform local shard DTensor its current spec target spec current_spec mesh = target_spec mesh TODO alltoall permute reshuffling change device_mesh they same raise NotImplementedError Cross device mesh comm supported yet new_local_tensor = local_tensor device_mesh = current_spec mesh my_coordinate = device_mesh get_coordinate my_coordinate None rank part mesh we skip redistribute simply local_tensor which should empty tensor local_tensor _are_we_tracing transform_infos = _gen_transform_infos_non_cached current_spec target_spec use_graph_based_transform transform_infos = _gen_transform_infos current_spec target_spec use_graph_based_transform debug_mode = get_active_debug_mode redistribute_context = debug_mode record_redistribute_calls type ignore union-attr local_tensor current_spec placements target_spec placements DTensorRedistributePlanner stringify_transform_infos device_mesh transform_infos current_spec placements current_spec shard_order debug_mode None contextlib nullcontext redistribute_context transform_info transform_infos i = transform_info mesh_dim current target = transform_info src_dst_placements num_chunks = device_mesh size mesh_dim=i current == target short cut just use original local tensor new_local_tensor = local_tensor continue num_chunks == short cut there s only one shard we don t need do any collective comm just use original local tensor new_local_tensor = local_tensor continue target is_replicate Case target Replicate current is_partial partial_spec = cast Partial current new_local_tensor = partial_spec _reduce_value local_tensor device_mesh i current is_shard current_placement = cast Shard current new_local_tensor = current_placement _to_replicate_tensor local_tensor device_mesh i transform_info logical_shape raise RuntimeError f redistribute current target supported yet target is_shard Case target Shard target_placement = cast Shard target current is_partial partial_spec = cast Partial current new_local_tensor = partial_spec _reduce_shard_value local_tensor device_mesh i target_placement current is_replicate split tensor corresponding cloned local shard new_local_tensor = target_placement _replicate_to_shard local_tensor device_mesh i my_coordinate i assert current is_shard f Current placement should shard found current shard_spec = cast Shard current shard_spec dim = target_placement dim new_local_tensor = shard_spec _to_new_shard_dim local_tensor device_mesh i transform_info logical_shape target_placement dim target is_partial current is_replicate partial_spec = cast Partial target skip replicate partial transformation when we backward pass In case we keep grad replicate because we don t want convert replicated gradients back partial although s logically conform same layout converting gradients back partial actually useless you would have do reduce later which would more expensive than keeping replicate For reason we keep replicate grad here new_local_tensor = partial_spec _partition_value local_tensor device_mesh i is_backward local_tensor current is_shard is_backward raise RuntimeError f redistribute current target supported yet backward shard - partial we just need convert shard replicate current_placement = cast Shard current new_local_tensor = current_placement _to_replicate_tensor local_tensor device_mesh i transform_info logical_shape partial - partial no op should never hit new_local_tensor = local_tensor async_op isinstance new_local_tensor funcol AsyncCollectiveTensor new_local_tensor = new_local_tensor wait local_tensor = new_local_tensor new_local_tensor Redistribute torch autograd Function staticmethod forward type ignore override pyre-fixme Parameter must annotated ctx input dtensor DTensor device_mesh DeviceMesh placements tuple Placement async_op bool = False forward_dtype Optional torch dtype = None backward_dtype Optional torch dtype = None ctx async_op = async_op ctx backward_dtype = backward_dtype ctx original_dtype = input _local_tensor dtype forward_dtype None forward_dtype = input _local_tensor dtype local_tensor = input _local_tensor dtype=forward_dtype current_spec = DTensorSpec mesh=device_mesh placements=input _spec placements tensor_meta=TensorMeta shape=input shape stride=input stride dtype=forward_dtype local_tensor = input _local_tensor current_spec = input _spec ctx current_spec = current_spec current_spec placements = placements target_spec = DTensorSpec device_mesh placements tensor_meta=current_spec tensor_meta output = redistribute_local_tensor local_tensor current_spec target_spec async_op=async_op use same local tensor placements same output = local_tensor target_spec = current_spec pyrefly ignore bad-argument-type dtensor DTensor pyrefly ignore bad-argument-count output target_spec pyrefly ignore unexpected-keyword requires_grad=input requires_grad staticmethod backward ctx grad_output dtensor DTensor type ignore override previous_spec = ctx current_spec async_op = ctx async_op backward_dtype = ctx backward_dtype ctx original_dtype backward_dtype = grad_output _local_tensor dtype local_tensor = grad_output _local_tensor dtype=backward_dtype current_spec = DTensorSpec mesh=grad_output _spec device_mesh placements=grad_output _spec placements tensor_meta=TensorMeta shape=grad_output shape stride=grad_output stride dtype=backward_dtype previous_spec = DTensorSpec mesh=previous_spec device_mesh placements=previous_spec placements tensor_meta=current_spec tensor_meta local_tensor = grad_output _local_tensor current_spec = grad_output _spec output = redistribute_local_tensor local_tensor current_spec previous_spec async_op=async_op is_backward=True output dtype = ctx original_dtype output = output ctx original_dtype normalize target placement replicate partial normalized_placements list Placement = previous_placement previous_spec placements previous_placement is_partial keep target placement replicate instead partial case normalized_placements append Replicate normalized_placements append previous_placement spec = DTensorSpec previous_spec device_mesh tuple normalized_placements tensor_meta=TensorMeta shape=grad_output shape stride=grad_output stride dtype=output dtype pyrefly ignore bad-argument-type output_dtensor = dtensor DTensor pyrefly ignore bad-argument-count output spec pyrefly ignore unexpected-keyword requires_grad=grad_output requires_grad output_dtensor None None None None None