Owner s oncall distributed contextlib copy torch torch distributed checkpoint dcp torch nn nn torch distributed checkpoint state_dict get_model_state_dict get_optimizer_state_dict StateDictOptions torch distributed device_mesh init_device_mesh torch distributed fsdp fully_shard FullyShardedDataParallel FSDP StateDictType torch distributed fsdp wrap always_wrap_policy torch distributed tensor DTensor torch distributed tensor experimental implicit_replication torch distributed tensor parallel ColwiseParallel parallelize_module RowwiseParallel torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest MLP torch testing _internal common_utils run_tests torch testing _internal distributed checkpoint_utils with_temp_dir torch utils _pytree tree_all_only device_type = acc type acc = torch accelerator current_accelerator cpu TestFullyShardWithDistributedStateDict FSDPTest property world_size - int min torch accelerator device_count _get_base_model mlp_dim int = base_model = nn Sequential MLP mlp_dim nn Sequential MLP mlp_dim nn Linear mlp_dim mlp_dim MLP mlp_dim base_model skip_if_lt_x_gpu test_ d_fsdp_get_model_state_dict run_subtests mlp_dim _test_ d_fsdp_get_model_state_dict _test_ d_fsdp_get_model_state_dict mlp_dim int Test model state_dict distributed_state_dict parity base_model = _get_base_model mlp_dim Default ` reshard_after_forward=True ` model = copy deepcopy base_model module model fully_shard module fully_shard model osd original state dict dsd distributed state dict osd = model state_dict dsd = get_model_state_dict model assertEqual osd dsd Check ` reshard_after_forward=False ` after forward model = copy deepcopy base_model module model fully_shard module reshard_after_forward=False fully_shard model reshard_after_forward=False inp = torch randn mlp_dim device=device_type model inp parameters resharded after forward Check state dict hooks reshard osd_ = model state_dict dsd_ = get_model_state_dict model assertEqual osd_ dsd_ skip_if_lt_x_gpu test_ d_fsdp_cpu_offload_full_model_state_dict Test full_state_dict cpu_offload works FSDP state_dict orig_model = _get_base_model fsdp_model = copy deepcopy orig_model module fsdp_model fully_shard module fully_shard fsdp_model osd = orig_model state_dict dsd = get_model_state_dict fsdp_model options=StateDictOptions full_state_dict=True cpu_offload=True cpu_device = torch device cpu is_cpu v isinstance v DTensor v device == torch device cpu v device == cpu_device rank == assertEqual osd dsd assertTrue tree_all_only torch Tensor DTensor is_cpu osd assertEqual dsd skip_if_lt_x_gpu test_save_with_fsdp _and_load_with_fsdp run_subtests state_dict_type StateDictType FULL_STATE_DICT StateDictType SHARDED_STATE_DICT _test_save_with_fsdp _and_load_with_fsdp skip_if_lt_x_gpu with_temp_dir _test_save_with_fsdp _and_load_with_fsdp state_dict_type StateDictType Test we can save model FSDP load FSDP Save state dict model wrapped FSDP fsdp _model = FSDP _get_base_model device_type use_orig_params=True auto_wrap_policy=always_wrap_policy fsdp _optim = torch optim AdamW fsdp _model parameters lr= fsdp _model torch randn device=self rank sum backward fsdp _optim step FSDP state_dict_type fsdp _model state_dict_type fsdp _state_dict = model fsdp _model state_dict optim FSDP sharded_optim_state_dict fsdp _model fsdp _optim dcp save fsdp _state_dict checkpoint_id=self temp_dir fsdp _full_msd = get_model_state_dict fsdp _model options=StateDictOptions full_state_dict=True cpu_offload=True fsdp _full_osd = get_optimizer_state_dict fsdp _model fsdp _optim options=StateDictOptions full_state_dict=True cpu_offload=True Load state dict into model FSDP applied fsdp _model = _get_base_model module fsdp _model fully_shard module fully_shard fsdp _model fsdp _optim = torch optim AdamW fsdp _model parameters lr= fsdp _state_dict = model get_model_state_dict fsdp _model optim get_optimizer_state_dict fsdp _model fsdp _optim dcp load fsdp _state_dict checkpoint_id=self temp_dir fsdp _model load_state_dict fsdp _state_dict model fsdp _optim load_state_dict fsdp _state_dict optim fsdp _full_msd = get_model_state_dict fsdp _model options=StateDictOptions full_state_dict=True cpu_offload=True fsdp _full_osd = get_optimizer_state_dict fsdp _model fsdp _optim options=StateDictOptions full_state_dict=True cpu_offload=True Compare full state dict make sure they same assertEqual fsdp _full_msd fsdp _full_msd assertEqual fsdp _full_osd fsdp _full_osd skip_if_lt_x_gpu with_temp_dir test_save_with_fsdp _and_load_with_fsdp _tp Test we can save model FSDP load FSDP + TP d mesh _get_base_model mlp_dim int = base_model = nn Sequential MLP mlp_dim MLP mlp_dim MLP mlp_dim base_model init device mesh dp_size = global_mesh = init_device_mesh device_type dp_size world_size dp_size mesh_dim_names= dp tp dp_mesh tp_mesh = global_mesh dp global_mesh tp Save state dict original model base_model = _get_base_model device_type base_optim = torch optim AdamW base_model parameters lr= Save state dict model wrapped FSDP fsdp _model = FSDP copy deepcopy base_model device_mesh=global_mesh use_orig_params=True auto_wrap_policy=always_wrap_policy fsdp _optim = torch optim AdamW fsdp _model parameters lr= one-step training modify state dict inp = torch randn device=self rank base_model inp sum backward base_optim step fsdp _model inp sum backward fsdp _optim step obtain full state dict base_msd = get_model_state_dict base_model options=StateDictOptions full_state_dict=True cpu_offload=True base_osd = get_optimizer_state_dict base_model base_optim options=StateDictOptions full_state_dict=True cpu_offload=True obtain sharded state dict fsdp _msd = get_model_state_dict fsdp _model options=StateDictOptions full_state_dict=False fsdp _osd = get_optimizer_state_dict fsdp _model fsdp _optim options=StateDictOptions full_state_dict=False save state dict temp dir source_state_dict = model_full base_msd optim_full base_osd model_sharded fsdp _msd optim_sharded fsdp _osd dcp save source_state_dict checkpoint_id=self temp_dir FSDP + TP fsdp _tp_model = _get_base_model fsdp _tp_model = parallelize_module fsdp _tp_model device_mesh=tp_mesh parallelize_plan= in_proj ColwiseParallel out_proj RowwiseParallel in_proj ColwiseParallel out_proj RowwiseParallel in_proj ColwiseParallel out_proj RowwiseParallel module fsdp _tp_model fully_shard module mesh=dp_mesh fully_shard fsdp _tp_model mesh=dp_mesh fsdp _tp_optim = torch optim AdamW fsdp _tp_model parameters lr= Load state dict into model FSDP + TP applied src_state_dict_type full sharded msd_name = f model_ src_state_dict_type osd_name = f optim_ src_state_dict_type fsdp _tp_state_dict = msd_name get_model_state_dict fsdp _tp_model osd_name get_optimizer_state_dict fsdp _tp_model fsdp _tp_optim load state dict temp dir dcp load fsdp _tp_state_dict checkpoint_id=self temp_dir fsdp _tp_model load_state_dict fsdp _tp_state_dict msd_name fsdp _tp_optim load_state_dict fsdp _tp_state_dict osd_name fsdp _tp_full_msd = get_model_state_dict fsdp _tp_model options=StateDictOptions full_state_dict=True cpu_offload=True fsdp _tp_full_osd = get_optimizer_state_dict fsdp _tp_model fsdp _tp_optim options=StateDictOptions full_state_dict=True cpu_offload=True Compare full state dict make sure they same assertEqual base_msd fsdp _tp_full_msd assertEqual base_osd fsdp _tp_full_osd skip_if_lt_x_gpu with_temp_dir test_save_with_tp_and_load_with_fsdp _tp Test we can save model TP load FSDP + TP d mesh _get_base_model mlp_dim int = base_model = nn Sequential MLP mlp_dim MLP mlp_dim MLP mlp_dim base_model tp_parallelize_plan = in_proj ColwiseParallel out_proj RowwiseParallel in_proj ColwiseParallel out_proj RowwiseParallel in_proj ColwiseParallel out_proj RowwiseParallel init device mesh dp_size = global_mesh_ d = init_device_mesh device_type world_size mesh_dim_names= tp global_mesh_ d = init_device_mesh device_type dp_size world_size dp_size mesh_dim_names= dp tp dp_mesh tp_mesh = global_mesh_ d dp global_mesh_ d tp Save state dict original model base_model = _get_base_model device_type base_optim = torch optim AdamW base_model parameters lr= Save state dict TP model tp_model = copy deepcopy base_model tp_model = parallelize_module tp_model device_mesh=global_mesh_ d parallelize_plan=tp_parallelize_plan tp_model_optim = torch optim AdamW tp_model parameters lr= one-step training modify state dict inp = torch randn device=self rank base_model inp sum backward base_optim step tp_model inp sum backward tp_model_optim step obtain full state dict base_msd = get_model_state_dict base_model options=StateDictOptions full_state_dict=True cpu_offload=True base_osd = get_optimizer_state_dict base_model base_optim options=StateDictOptions full_state_dict=True cpu_offload=True obtain sharded state dict tp_msd = get_model_state_dict tp_model options=StateDictOptions full_state_dict=False tp_osd = get_optimizer_state_dict tp_model tp_model_optim options=StateDictOptions full_state_dict=False save state dict temp dir source_state_dict = model_full base_msd optim_full base_osd model_sharded tp_msd optim_sharded tp_osd dcp save source_state_dict checkpoint_id=self temp_dir FSDP + TP fsdp _tp_model = _get_base_model fsdp _tp_model = parallelize_module fsdp _tp_model device_mesh=tp_mesh parallelize_plan=tp_parallelize_plan module fsdp _tp_model fully_shard module mesh=dp_mesh fully_shard fsdp _tp_model mesh=dp_mesh fsdp _tp_optim = torch optim AdamW fsdp _tp_model parameters lr= Load state dict into model FSDP + TP applied src_state_dict_type full sharded msd_name = f model_ src_state_dict_type osd_name = f optim_ src_state_dict_type fsdp _tp_state_dict = msd_name get_model_state_dict fsdp _tp_model osd_name get_optimizer_state_dict fsdp _tp_model fsdp _tp_optim load state dict temp dir dcp load fsdp _tp_state_dict checkpoint_id=self temp_dir fsdp _tp_model load_state_dict fsdp _tp_state_dict msd_name fsdp _tp_optim load_state_dict fsdp _tp_state_dict osd_name fsdp _tp_full_msd = get_model_state_dict fsdp _tp_model options=StateDictOptions full_state_dict=True cpu_offload=True fsdp _tp_full_osd = get_optimizer_state_dict fsdp _tp_model fsdp _tp_optim options=StateDictOptions full_state_dict=True cpu_offload=True Compare full state dict make sure they same assertEqual base_msd fsdp _tp_full_msd assertEqual base_osd fsdp _tp_full_osd skip_if_lt_x_gpu test_save_with_fsdp _tp_and_load_with_tp run_subtests allow_implicit_replication True False _test_save_with_fsdp _tp_and_load_with_tp skip_if_lt_x_gpu with_temp_dir _test_save_with_fsdp _tp_and_load_with_tp allow_implicit_replication bool Test we can save model FSDP + TP d mesh load TP mlp_dim = _get_base_model mlp_dim dim_multiplier= helps make easier hit corner cases uneven sharding e g out dim both= means unevenness easier hit depending row col sharding base_model = nn Sequential MLP mlp_dim dim_multiplier= MLP mlp_dim dim_multiplier= MLP mlp_dim dim_multiplier= base_model cm = implicit_replication allow_implicit_replication contextlib nullcontext Must set use_local_output=False order test uneven-sharding case see https github com pytorch pytorch issues tp_parallelize_plan = in_proj ColwiseParallel use_local_output=False out_proj RowwiseParallel use_local_output=False in_proj ColwiseParallel use_local_output=False out_proj RowwiseParallel use_local_output=False in_proj ColwiseParallel use_local_output=False out_proj RowwiseParallel use_local_output=False allow_implicit_replication intentionally pop plans some tp layers so model fully tensor parallelized tp_parallelize_plan pop in_proj tp_parallelize_plan pop out_proj cm init device mesh dp_size = global_mesh_ d = init_device_mesh device_type world_size mesh_dim_names= tp global_mesh_ d = init_device_mesh device_type dp_size world_size dp_size mesh_dim_names= dp tp dp_mesh tp_mesh = global_mesh_ d dp global_mesh_ d tp save_full_state_dict True False Save state dict original model base_model = _get_base_model mlp_dim device_type base_optim = torch optim AdamW base_model parameters lr= Save state dict FSDP + TP model fsdp _tp_model = copy deepcopy base_model fsdp _tp_model = parallelize_module fsdp _tp_model device_mesh=tp_mesh parallelize_plan=tp_parallelize_plan module fsdp _tp_model fully_shard module mesh=dp_mesh fully_shard fsdp _tp_model mesh=dp_mesh fsdp _tp_optim = torch optim AdamW fsdp _tp_model parameters lr= one-step training modify state dict inp = torch randn mlp_dim device=self rank base_model inp sum backward base_optim step fsdp _tp_model inp sum backward fsdp _tp_optim step obtain unsharded state dict base_msd = get_model_state_dict base_model options=StateDictOptions full_state_dict=True cpu_offload=True base_osd = get_optimizer_state_dict base_model base_optim options=StateDictOptions full_state_dict=True cpu_offload=True obtain FSDP + TP state dict fsdp _tp_msd = get_model_state_dict fsdp _tp_model options=StateDictOptions full_state_dict=save_full_state_dict fsdp _tp_osd = get_optimizer_state_dict fsdp _tp_model fsdp _tp_optim options=StateDictOptions full_state_dict=save_full_state_dict fsdp _tp_state_dict = model fsdp _tp_msd optim fsdp _tp_osd dcp save fsdp _tp_state_dict checkpoint_id=self temp_dir fsdp _tp_full_msd = get_model_state_dict fsdp _tp_model options=StateDictOptions full_state_dict=True cpu_offload=True fsdp _tp_full_osd = get_optimizer_state_dict fsdp _tp_model fsdp _tp_optim options=StateDictOptions full_state_dict=True cpu_offload=True Load state dict into model TP applied tp_model = _get_base_model mlp_dim tp_model = parallelize_module tp_model device_mesh=global_mesh_ d parallelize_plan=tp_parallelize_plan tp_optim = torch optim AdamW tp_model parameters lr= tp_state_dict = model get_model_state_dict tp_model optim get_optimizer_state_dict tp_model tp_optim dcp load tp_state_dict checkpoint_id=self temp_dir tp_model load_state_dict tp_state_dict model tp_optim load_state_dict tp_state_dict optim tp_full_msd = get_model_state_dict tp_model options=StateDictOptions full_state_dict=True cpu_offload=True tp_full_osd = get_optimizer_state_dict tp_model tp_optim options=StateDictOptions full_state_dict=True cpu_offload=True Compare full state dict make sure they same assertEqual base_msd tp_full_msd assertEqual base_osd tp_full_osd assertEqual fsdp _tp_full_msd tp_full_msd assertEqual fsdp _tp_full_osd tp_full_osd __name__ == __main__ run_tests