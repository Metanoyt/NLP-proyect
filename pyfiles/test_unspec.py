Owner s module dynamo math random unittest numpy np torch torch _dynamo test_case torch _dynamo testing torch nn functional F torch _dynamo comptime comptime torch _dynamo testing CompileCounter CompileCounterWithBackend same torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_utils requires_cuda skipIfWindows torch testing _internal logging_utils logs_to_string The intention test file you should put test cases specifically assume_static_by_default=False aka you want YOLO make everything dynamic possible If you want test more normal situation where you assume static default put regular test file test_dynamic_shapes will cover both YOLO non-YOLO cases torch _dynamo config patch assume_static_by_default=False UnspecTests torch _dynamo test_case TestCase test_numpy_correctness fn x y z xy = x + y y False np_x = x numpy np_y = y numpy x x z z np_y sum b xy c np_y d np_x sum e np_x + np_y x + np_y sum + z x = torch tensor dtype=torch float y = torch ones dtype=torch int z = np int res = fn x y z cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts res = opt_fn x y z assertEqual res res test_no_recompilations no recompilations passing different numpy int values fn x y x + b y x = torch tensor dtype=torch float cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts i range opt_fn x np int i assertEqual cnts frame_count assertEqual cnts op_count requires_cuda test_no_recompilations_with_efficient_attention fn q k v attn_mask torch nn attention sdpa_kernel SDPBackend torch nn functional scaled_dot_product_attention sdpa_kernel backends= SDPBackend EFFICIENT_ATTENTION scaled_dot_product_attention q k v attn_mask=attn_mask scale= make_q_k_v_mask batch num_heads head_dim seq_len_kv collections namedtuple functools partial dtype = torch float device = cuda make_tensor = partial torch rand device=device dtype=dtype requires_grad=True seq_len_q = SdpaShape = namedtuple Sdpa_Shape batch num_heads seq_len head_dim query = make_tensor SdpaShape batch num_heads seq_len_q head_dim kv_shape = SdpaShape batch num_heads seq_len_kv head_dim key value = make_tensor kv_shape make_tensor kv_shape mask = torch randn batch num_heads seq_len_q seq_len_kv device=device dtype=dtype query key value mask cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts q k v mask = make_q_k_v_mask opt_fn q k v mask q k v mask = make_q_k_v_mask opt_fn q k v mask assertEqual cnts frame_count unittest expectedFailure array scalars decay D arrays test_builtin_max_min test unspecialized primitive max min fn x y z z + max x y min x - y x = np int y = z = torch tensor dtype=torch float res = fn x y z cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts res = opt_fn x y z assertTrue same res res relax_numpy_equality=True test_feed_random_values_into_graph_only fn shape torch manual_seed x = torch randn shape device= cpu random randint x shape = random seed res = fn shape cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts Especially internal before resetting seed first shake out any rng calls occur compile e g result some module initializations opt_fn shape random seed res = opt_fn shape assertTrue same res res test_random_values_with_graph_break fn x r = random random y = x + random uniform y sum item r = random randint no graph output frame y sum item y + r r x = torch tensor random seed res = fn x cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts Especially internal before resetting seed first shake out any rng calls occur compile e g result some module initializations opt_fn x random seed res = opt_fn x assertTrue same res res Really annoying intersection specialization RandomValueSource If we get RandomValueSource single element tensor we should ConstantVariable like other unspects we do we break bytecode assumptions guards will work we will referring name source there If we call item take wrapped_value out where we do wrapped_value = wrapped_value item where we send unspec down wrap_fx_proxy test passes then some models fail missing codegen tx output random_values_var If we let tensor value go into wrap test fails The real solution here rewrite RandomValueSource all codegen does ground up test_multiple_consecutive_random_calls_before_graph fn x dim = random randrange start= stop= dim = random randrange start= stop= dim = random randrange start= stop= y = torch rand dim dim dim x + y x = torch tensor random seed res = fn x cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts Especially internal before resetting seed first shake out any rng calls occur compile e g result some module initializations opt_fn x random seed res = opt_fn x assertTrue same res res test_compiled_random_calls_are_random For compiled functions random calls should different values every iteration https github com pytorch pytorch issues torch compile backend= eager fullgraph=True fn x x + random uniform res = _ range res append fn torch ones i range assertFalse same res i - res i test_random_call_with_while_loop fn x dim = random randrange start= stop= dim = dim while dim == dim dim = random randrange start= stop= x x = torch randn random seed res = fn x opt_fn = torch compile fn backend= eager Especially internal before resetting seed first shake out any rng calls occur compile e g result some module initializations opt_fn x random seed res = opt_fn x assertTrue same res res random seed res = fn x random seed res = opt_fn x assertTrue same res res test_random_object test argument passing mutation reconstruction state correctness fn x rand r = random randint r = rand randint rand = random Random r = rand randint y = x + r + r + r y rand rand inp = torch randn opt_fn = torch compile fn backend= eager fullgraph=True random seed y_ rand _ rand _ = fn inp random Random state_ = random getstate Especially internal before resetting seed first shake out any rng calls occur compile e g result some module initializations opt_fn inp random Random random seed y_ rand _ rand _ = opt_fn inp random Random state_ = random getstate assertEqual y_ y_ assertEqual state_ state_ assertEqual rand _ getstate rand _ getstate assertEqual rand _ getstate rand _ getstate test_random_object_methods fn x rand rand rand rand seed rand = random Random rand setstate rand getstate r = rand random r = rand randint r = rand randrange r = rand uniform x + r + r + r + r inp = torch randn opt_fn = torch compile fn backend= eager fullgraph=True rand _ = random Random rand _ = random Random rand _ = random Random rand _ = random Random rand _ = random Random rand _ = random Random y = fn inp rand _ rand _ rand _ y = opt_fn inp rand _ rand _ rand _ assertEqual y y assertEqual rand _ getstate rand _ getstate assertEqual rand _ getstate rand _ getstate assertEqual rand _ getstate rand _ getstate test_random_object_overridden_methods these will result graph breaks we shouldn t crash get_rng rand = random Random rand = random Random orig_random = rand random custom_random orig_random orig_getstate = rand getstate custom_getstate orig_getstate rand random = custom_random rand getstate = custom_getstate rand rand fn x rand rand r = rand random rand = random Random rand setstate rand getstate r = rand random x + r + r inp = torch randn opt_fn = torch compile fn backend= eager y = fn inp get_rng y = opt_fn inp get_rng assertEqual y y test_builtin_getitem builtin getitem args python list args unspec fn x idx torch zeros idx x idx x idx x = list range ref = fn x unspecialized cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts res = opt_fn x assertTrue same ref res test_use_and_specialize cnt = CompileCounter torch compile backend=cnt fullgraph=True dynamic=True fn x y x = x + y y == x - x + assertTrue same fn torch tensor assertTrue same fn torch tensor assertTrue same fn torch tensor assertTrue same fn torch tensor assertEqual cnt frame_count test_no_recompiles cnt = CompileCounter torch compile backend=cnt fullgraph=True dynamic=True fn x y x + y assertTrue same fn torch tensor assertTrue same fn torch tensor assertTrue same fn torch tensor assertTrue same fn torch tensor assertEqual cnt frame_count assertEqual cnt op_count test_no_recompiles_prod_backward https github com pytorch pytorch issues cnt = CompileCounter torch compile backend=cnt fullgraph=True dynamic=True fn t torch prod t keepdim=True input_shapes = s input_shapes t = torch randn s requires_grad=True h_result = fn t grad = torch ones_like h_result h_result backward grad assertEqual cnt frame_count assertEqual cnt op_count test_unspec_float_precision fn image scale_factor image = torch nn functional interpolate image None size=None scale_factor=scale_factor mode= bilinear recompute_scale_factor=True align_corners=False image shape x = torch rand scale_factor = ref = fn x scale_factor cnts = torch _dynamo testing CompileCounter opt_fn = torch compile fn backend=cnts res = opt_fn x scale_factor assertTrue same ref res unittest expectedFailure fails long numpy scalars D arrays test_specializing_numpy_float_in_control_flow np float unspecialized default should specialized when used control flow fn x y y x + x - x = torch rand opt_fn = torch compile fn backend= eager fullgraph=True t np float np float np float y = t ref = fn x y res = opt_fn x y assertTrue same ref res test_mark_static_inside fn x torch _dynamo mark_static x comptime assert_static x size x + opt_fn = torch compile fn dynamic=True fullgraph=True opt_fn torch randn test_shape_graph_break torch _dynamo comptime comptime fn x x_shape = x size comptime graph_break x + torch randn x_shape x = torch randn opt_fn = torch compile fn backend= eager opt_fn x test_isinstance_symint fn x assert isinstance x size int x x = torch randn opt_fn = torch compile fn backend= eager opt_fn x y = torch randn torch _dynamo mark_dynamic y opt_fn y test_mark_ _dynamic fn x x x = torch randn torch _dynamo mark_dynamic x opt_fn = torch compile fn backend= eager This will fail compile generic kernel we should complain about mark dynamic will try its best specialization allowed opt_fn x test_conv d_symint_padding kernel = torch randn func x padding = math ceil kernel shape - + x shape - - out = F conv d x kernel padding=padding stride= out opt_func = torch compile func x = torch randn opt_func x passes x = torch randn opt_func x crashes torch _dynamo config patch assume_static_by_default True test_propagate_dynamic_dim x = torch randn torch _dynamo mark_dynamic x torch compile fn x y = x comptime graph_break z = y z z = fn x assertEqual z _dynamo_weak_dynamic_indices test_rshift_dynamic shift_right tensor torch Tensor - torch Tensor tensor torch long opt_fn = torch compile shift_right fullgraph=True dynamic=True sample_input = torch tensor dtype=torch uint opt_fn sample_input torch _dynamo config patch capture_scalar_outputs=True test_symfloat_to_tensor f v torch tensor v item f v torch tensor v item f v torch tensor v item f v torch tensor v item optimize = torch compile backend= aot_eager fullgraph=True r = torch randn assertEqual f r optimize f r assertEqual f r optimize f r assertEqual f r optimize f r assertEqual f r optimize f r skipIfWindows msg= AssertionError The values attribute dtype do match torch int = torch int test_to_tensor f = np random uniform low=- high= size= torch tensor dtype=torch float device= cpu f = torch tensor torch tensor f = torch tensor torch tensor f = torch tensor b = torch tensor torch tensor b f = np array torch tensor optimize = torch compile backend= aot_eager fullgraph=True assertEqual f shape optimize f shape assertEqual f optimize f assertEqual f optimize f assertEqual f optimize f assertEqual f optimize f test_sym_int_conversion f x y = x size x int y == opt_fn = torch compile f backend= eager fullgraph=True x = torch randn opt_fn x test_sum_dimlist_spec fn inputs dim torch sum inputs dim inputs = torch randn dim = - compl_fn = torch compile fn dynamic=True backend= eager fullgraph=True assertEqual compl_fn inputs dim fn inputs dim torch _dynamo config patch capture_scalar_outputs=True test_item_max fn x torch ones max x item x = torch tensor y = torch tensor compl_fn = torch compile fn backend= eager fullgraph=True assertEqual fn x compl_fn x assertEqual fn y compl_fn y https github com pytorch pytorch issues test_argmin_coerces_symint_to_intlist_spec fn x dim python arg parser coerces dim into vector int torch amin x dim=dim keepdim=True x = torch randn dim = compl_fn = torch compile fn dynamic=True backend= eager fullgraph=True assertEqual compl_fn x dim fn x dim test_exponential fn inputs op_inputs_dict res = inputs exponential_ op_inputs_dict res inputs = torch randn op_inputs_dict = lambd generator None compl_fn = torch compile fn dynamic=True backend= eager fullgraph=True assertEqual compl_fn inputs op_inputs_dict fn inputs op_inputs_dict test_symbol_guard_limit_before_specialize cnts = torch _dynamo testing CompileCounter torch compile backend=cnts dynamic=True fn x torch _check x size = torch _check x size = torch _check x size = torch _check x size = x + Control test fn torch randn fn torch randn fn torch randn assertExpectedInline cnts frame_count cnts frame_count = torch _dynamo reset torch fx experimental _config patch symbol_guard_limit_before_specialize= fn torch randn fn torch randn fn torch randn assertExpectedInline cnts frame_count test_defaults g x i= comptime assert_static i x i fn x g x inputs = torch randn compl_fn = torch compile fn dynamic=True backend= eager assertEqual compl_fn inputs fn inputs torch _dynamo config patch specialize_float=False test_symfloat_no_replacement See https github com pytorch pytorch pull more context The high level idea we don t want set replacement where symbol both right left side otherwise we ll end up infinite _find recursion fn t m t m is_integer t t = torch tensor compl_fn = torch compile fn dynamic=True backend= eager assertEqual fn t compl_fn t torch _dynamo config patch specialize_float=False test_unspec_roundtrip_float_input f x y y == x + x + y x y cf = torch compile backend= eager fullgraph=True f x = y = assertAlmostEqual f x y cf x y torch _dynamo config patch specialize_float=False assume_static_by_default=True test_unspec_float_input cnts = torch _dynamo testing CompileCounter f x y y == x + x + y cf = torch compile backend=cnts fullgraph=True f x = torch randn assertEqual f x cf x assertEqual f x cf x automatic dynamic kicks here assertEqual f x cf x assertExpectedInline cnts frame_count no recompile assertEqual f x cf x assertExpectedInline cnts frame_count guard worked assertEqual f x math nan cf x math nan assertExpectedInline cnts frame_count nan always recompiles torch _dynamo config patch specialize_float=False capture_scalar_outputs=True test_unspecialized_float_multiply_precision dtypes = torch bfloat torch float torch float torch float dtype dtypes fn x y x y cnt = CompileCounterWithBackend aot_eager fn_opt = torch compile fn backend=cnt x = torch randn dtype=dtype requires_grad=True y = y = y = assertEqual fn_opt x y fn x y assertEqual fn_opt x y fn x y assertEqual fn_opt x y fn x y assertEqual cnt frame_count torch _dynamo config patch capture_scalar_outputs=True test_tensorfiy_python_scalars_ torch compile backend= aot_eager f x y = x sum x + y item dtypes = torch bfloat torch float torch float torch float dtype dtypes x = torch ones dtype=dtype assertEqual f x x + x sum item torch _dynamo config patch capture_scalar_outputs=True test_tensorfiy_python_scalars_ torch compile backend= aot_eager f x x item x item torch ones dtype=torch float x = torch tensor e dtype=torch float assertEqual f x x item x item torch ones dtype=torch float torch _dynamo config patch capture_scalar_outputs=True test_tensorfiy_python_scalars_ torch compile backend= aot_eager f x y = x item y torch tensor dtype=torch float finfo_float = torch finfo torch float x = torch tensor finfo_float max dtype=torch float assertEqual f x x item torch tensor dtype=torch float torch _dynamo config patch specialize_float=False assume_static_by_default=False test_unspec_float_input_f cnts = torch _dynamo testing CompileCounter f x y x + y cf = torch compile backend=cnts fullgraph=True f x = torch zeros dtype=torch float digits precision so unrepresentable float flt = assertEqual f x flt cf x flt torch _dynamo config patch specialize_float=False assume_static_by_default=True test_unspec_float_output cnts = torch _dynamo testing CompileCounter f x y x + y cf = torch compile backend=cnts fullgraph=True f x = torch randn assertEqual f x cf x assertEqual f x cf x assertEqual f x cf x torch _dynamo config patch capture_scalar_outputs=True test_data_dependent_evaluate_expr_graph_break cnts = torch _dynamo testing CompileCounter To ensure continuation frame compiled have write test function funny way See https github com pytorch pytorch issues test y y True False torch compile backend=cnts fn x x = x + y = x item test y x x x = torch tensor fn x assertExpectedInline cnts frame_count assertExpectedInline cnts op_count test_prune_torch_check log_stream ctx = logs_to_string torch _dynamo output_graph graph_code torch compile fullgraph=True dynamic=True backend= eager f x y torch _check y + == torch _check x size == ctx f torch randn out = \n join log_stream getvalue strip split \n strip assertExpectedInline out \ forward torch _dynamo config patch capture_scalar_outputs=True test_split_aot_autograd torch compile backend= aot_eager fullgraph=True f x i y z = i tolist torch split x y z print f torch randn requires_grad=True torch tensor test_bool_tensor_ctor cnts = torch _dynamo testing CompileCounter torch compile backend=cnts dynamic=True fullgraph=True f x y = torch empty x size torch tensor y numel == assertTrue f torch empty item assertFalse f torch empty item torch _dynamo config patch error_on_recompile=True test_mark_unbacked TestModel torch nn Module __init__ super __init__ forward x torch Tensor val int - torch Tensor x main_model = TestModel opt_model = torch compile main_model mode= max-autotune dynamic=True x = torch rand x = torch rand torch _dynamo decorators mark_unbacked x o _ref = main_model x o = opt_model x assertEqual o _ref o o _ _ref = main_model x o _ = opt_model x assertEqual o _ _ref o _ torch _dynamo config patch error_on_recompile=True test_mark_unbacked_hint_consistency torch fx experimental symbolic_shapes guard_size_oblivious x = torch randn torch _dynamo decorators mark_unbacked x torch compile f x guard_size_oblivious x size = x + x + assertEqual f x x + torch _dynamo config patch error_on_recompile=True test_mark_unbacked_channels_last TestModel torch nn Module __init__ super __init__ forward x torch Tensor val int - torch Tensor x main_model = TestModel opt_model = torch compile main_model mode= max-autotune dynamic=True x = torch rand memory_format=torch channels_last x = torch rand memory_format=torch channels_last torch _dynamo decorators mark_unbacked x o _ref = main_model x o = opt_model x assertEqual o _ref o o _ _ref = main_model x o _ = opt_model x assertEqual o _ _ref o _ UnspecTestsDevice torch _dynamo test_case TestCase test_builtin_functions_on_device device fn x scaler m = torch nn ReLU m device y = m x scaler y x = torch randn device=device scaler = unspecialized ref = fn x scaler cnts = torch _dynamo testing CompileCounter opt_fn = torch _dynamo optimize cnts fn res = opt_fn x scaler assertTrue same ref res assertEqual ref device res device devices = cuda hpu xpu instantiate_device_type_tests UnspecTestsDevice globals only_for=devices allow_xpu=True __name__ == __main__ torch _dynamo test_case run_tests run_tests