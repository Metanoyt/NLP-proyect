Copyright c Meta Platforms Inc affiliates Owner s oncall distributed contextlib nullcontext torch torch distributed dist torch distributed _local_tensor local_tensor_mode LocalTensor LocalTensorMode torch distributed tensor DeviceMesh distribute_tensor init_device_mesh Partial Replicate Shard torch testing _internal common_utils run_tests TestCase LocalTensorTestBase TestCase assertEqual lhs rhs kwargs mode = local_tensor_mode nullcontext mode None mode disable isinstance lhs LocalTensor isinstance rhs LocalTensor assert isinstance lhs LocalTensor isinstance rhs LocalTensor super assertEqual lhs _ranks rhs _ranks r lhs _ranks super assertEqual lhs _local_tensors r rhs _local_tensors r lambda m f rank r m isinstance lhs LocalTensor isinstance rhs LocalTensor lhs rhs = lhs rhs isinstance lhs LocalTensor rhs lhs r lhs _ranks super assertEqual lhs _local_tensors r rhs lambda m f rank r m super assertEqual lhs rhs kwargs property world_size raise NotImplementedError override world-size your subclass build_device_mesh - DeviceMesh init_device_mesh cpu world_size setUp super setUp torch distributed init_process_group TODO test other ranks too fake rank= world_size=self world_size tearDown super tearDown try dist destroy_process_group except AssertionError pass TestLocalTensorWorld LocalTensorTestBase world_size = test_local_tensor_dtype_consistency Test LocalTensor enforces dtype consistency device = torch device cpu shape = inconsistent_tensors = torch randn shape dtype=torch float device=device torch randn shape dtype=torch float device=device Different dtype assertRaises AssertionError LocalTensor inconsistent_tensors test_local_tensor_creation_fails_with_grad_tensors Test LocalTensor creation fails when local tensors have requires_grad=True device = torch device cpu shape = dtype = torch float Create sample local tensors different ranks local_tensors = torch randn shape dtype=dtype device=device requires_grad=True torch randn shape dtype=dtype device=device requires_grad=True assertRaises AssertionError LocalTensor local_tensors TODO test flatten unflatten test_basic_arithmetic_operations Test basic arithmetic operations LocalTensors device = torch device cpu shape = dtype = torch float Create identical local tensors consistency tests base_tensor = torch randn shape dtype=dtype device=device identical_local_tensors = base_tensor clone base_tensor clone lt = LocalTensor identical_local_tensors lt = LocalTensor identical_local_tensors Test addition result_add = lt + lt assertIsInstance result_add LocalTensor assertEqual len result_add _local_tensors Verify operation applied each local tensor rank identical_local_tensors keys expected = identical_local_tensors rank + identical_local_tensors rank assertEqual result_add _local_tensors rank expected Test multiplication result_mul = lt assertIsInstance result_mul LocalTensor rank identical_local_tensors keys expected = identical_local_tensors rank assertEqual result_mul _local_tensors rank expected TODO consider op-info test we don t actually need cover all ops will help make sure views more exotic things done correctly standard subclass style test_mixed_operations_with_regular_tensors Test operations between LocalTensors regular tensors device = torch device cpu shape = dtype = torch float Create identical local tensors consistency tests base_tensor = torch randn shape dtype=dtype device=device identical_local_tensors = base_tensor clone base_tensor clone lt = LocalTensor identical_local_tensors regular_tensor = torch ones_like identical_local_tensors Test LocalTensor + regular tensor result = lt + regular_tensor assertIsInstance result LocalTensor rank identical_local_tensors keys expected = identical_local_tensors rank + regular_tensor assertEqual result _local_tensors rank expected test_local_tensor_mode Test LocalTensorMode functionality device = torch device cpu shape = dtype = torch float Create identical local tensors consistency tests base_tensor = torch randn shape dtype=dtype device=device identical_local_tensors = base_tensor clone base_tensor clone lt = LocalTensor identical_local_tensors LocalTensorMode lt _ranks result = lt + assertIsInstance result LocalTensor regular = torch ones regular_result = regular + assertIsInstance regular LocalTensor assertIsInstance regular_result LocalTensor test_empty_local_tensors Test behavior empty local tensors dict TODO raise better error here assertRaises StopIteration next empty iterator LocalTensor test_collectives_within_local_tensor_mode Test collective operations work within LocalTensorMode context test_tensors = torch tensor torch tensor lt = LocalTensor test_tensors fake_pg = torch distributed distributed_c d _get_default_group LocalTensorMode lt _ranks Test all_reduce within mode lt_sum = LocalTensor k v clone k v test_tensors items dist all_reduce lt_sum group=fake_pg expected_sum = torch tensor rank test_tensors keys assertEqual lt_sum _local_tensors rank expected_sum Test broadcast within mode lt_broadcast = LocalTensor k v clone k v test_tensors items dist broadcast lt_broadcast src= group=fake_pg rank test_tensors keys assertEqual lt_broadcast _local_tensors rank test_tensors Test regular operations still work result = lt + assertIsInstance result LocalTensor test_scalar_mul_reduction_bug LocalTensorMode world_size mesh = build_device_mesh tensor = torch tensor float dt = distribute_tensor tensor device_mesh=mesh placements= Shard y = dt sum noqa F tensor = torch arange reshape float requires_grad_ dt = distribute_tensor tensor device_mesh=mesh placements= Shard print dt sum dt sum dt sum test_uneven_sharding_mean_bug LocalTensorMode world_size mesh = build_device_mesh tensor = torch arange reshape - float dt = distribute_tensor tensor device_mesh=mesh placements= Shard mean = dt mean assertEqual mean placements Replicate full = mean full_tensor assertEqual tensor mean full test_uneven_sharding_prod LocalTensorMode world_size mesh = build_device_mesh tensor = torch arange + reshape - float dt = distribute_tensor tensor device_mesh=mesh placements= Shard x = dt prod full = x full_tensor assertEqual tensor prod full test_even_sharding_mean_is_partial LocalTensorMode world_size mesh = build_device_mesh tensor = torch arange reshape float dt = distribute_tensor tensor device_mesh=mesh placements= Shard mean = dt mean full = mean full_tensor assertEqual tensor mean full assertEqual mean placements Partial avg TestLocalTensorWorld LocalTensorTestBase world_size = test_collective_reduction_operations Test different reduction operations all_reduce Create different tensors each rank simple values testing test_tensors = torch tensor torch tensor torch tensor fake_pg = torch distributed distributed_c d _get_default_group Test SUM reduction lt_sum = LocalTensor k v clone k v test_tensors items dist all_reduce lt_sum op=dist ReduceOp SUM group=fake_pg expected_sum = torch tensor Sum all tensors rank test_tensors keys assertEqual lt_sum _local_tensors rank expected_sum Test MAX reduction lt_max = LocalTensor k v clone k v test_tensors items dist all_reduce lt_max op=dist ReduceOp MAX group=fake_pg expected_max = torch tensor Max across all tensors rank test_tensors keys assertEqual lt_max _local_tensors rank expected_max Test MIN reduction lt_min = LocalTensor k v clone k v test_tensors items dist all_reduce lt_min op=dist ReduceOp MIN group=fake_pg expected_min = torch tensor Min across all tensors rank test_tensors keys assertEqual lt_min _local_tensors rank expected_min test_all_reduce_collective Test all_reduce collective operation works correctly LocalTensor Create different tensors each rank different_tensors = torch tensor torch tensor torch tensor fake_pg = torch distributed distributed_c d _get_default_group Test all_reduce SUM default lt_sum = LocalTensor k v clone k v different_tensors items lt_sum = lt_sum + dist all_reduce lt_sum group=fake_pg Verify all ranks have sum all tensors after adding each expected_sum = torch tensor rank different_tensors keys assertEqual lt_sum _local_tensors rank expected_sum test_broadcast_collective Test broadcast collective operation works correctly LocalTensor Create different tensors each rank different_tensors = torch tensor torch tensor torch tensor fake_pg = torch distributed distributed_c d _get_default_group Test broadcast rank lt_broadcast = LocalTensor k v clone k v different_tensors items dist broadcast lt_broadcast src= group=fake_pg Verify all ranks have rank s original tensor expected_broadcast = different_tensors rank different_tensors keys assertEqual lt_broadcast _local_tensors rank expected_broadcast test_all_gather_collective Test all_gather collective operation works correctly LocalTensor Create different tensors each rank different_tensors = torch tensor torch tensor torch tensor fake_pg = torch distributed distributed_c d _get_default_group Test all_gather lt_gather = LocalTensor different_tensors tensor_list = torch zeros_like lt_gather _ range dist all_gather tensor_list lt_gather group=fake_pg Verify each position tensor_list contains corresponding rank s tensor assertEqual tensor_list different_tensors assertEqual tensor_list different_tensors assertEqual tensor_list different_tensors TestLocalTensorWorld LocalTensorTestBase world_size = test_dtensor_cat LocalTensorMode world_size device_mesh = build_device_mesh t = torch arange view float d = distribute_tensor t device_mesh Replicate t = torch arange + view float d = distribute_tensor t device_mesh Shard local_res = torch cat t t dim=- dist_res = torch cat d d dim=- full_tensor = dist_res full_tensor assertEqual full_tensor local_res TestLocalTensorWorld LocalTensorTestBase world_size = test_dtensor_addmm LocalTensorMode world_size device_mesh = build_device_mesh shard_spec = Shard replica_spec = Replicate tensor_to_shard = torch randn mat = distribute_tensor tensor_to_shard device_mesh shard_spec tensor_to_replicate = torch randn mat = distribute_tensor tensor_to_replicate device_mesh replica_spec input_tensor = torch randn input = distribute_tensor input_tensor device_mesh replica_spec dist_res = torch addmm input mat mat local_res = torch addmm input_tensor tensor_to_shard tensor_to_replicate full_tensor = dist_res full_tensor assertEqual full_tensor local_res __name__ == __main__ run_tests