Copyright c Facebook Inc its affiliates Licensed under Apache License Version License you may use file except compliance License You may obtain copy License http www apache org licenses LICENSE- Unless required applicable law agreed writing software distributed under License distributed AS IS BASIS WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND either express implied See License specific language governing permissions limitations under License These Omniglot loaders Jackie Loong s PyTorch MAML implementation https github com dragen MAML-Pytorch https github com dragen MAML-Pytorch blob master omniglot py https github com dragen MAML-Pytorch blob master omniglotNShot py errno os os path numpy np PIL Image torchvision transforms torch torch utils data data Omniglot data Dataset urls = https github com brendenlake omniglot raw master python images_background zip https github com brendenlake omniglot raw master python images_evaluation zip raw_folder = raw processed_folder = processed training_file = training pt test_file = test pt The items filename category The index all categories can found idx_classes Args - root directory where dataset will stored - transform how transform input - target_transform how transform target - download need download dataset __init__ root transform=None target_transform=None download=False root = root transform = transform target_transform = target_transform _check_exists download download raise RuntimeError Dataset found + You can use download=True download all_items = find_classes os path join root processed_folder idx_classes = index_classes all_items __getitem__ index filename = all_items index img = str join all_items index filename target = idx_classes all_items index transform None img = transform img target_transform None target = target_transform target img target __len__ len all_items _check_exists os path exists os path join root processed_folder images_evaluation os path exists os path join root processed_folder images_background download urllib zipfile _check_exists download files try os makedirs os path join root raw_folder os makedirs os path join root processed_folder except OSError e e errno == errno EEXIST pass raise url urls print == Downloading + url data = urllib request urlopen url filename = url rpartition file_path = os path join root raw_folder filename open file_path wb f f write data read file_processed = os path join root processed_folder print == Unzip + file_path + + file_processed zip_ref = zipfile ZipFile file_path r zip_ref extractall file_processed zip_ref close print Download finished find_classes root_dir retour = root dirs files os walk root_dir f files f endswith png r = root split lr = len r retour append f r lr - + + r lr - root print f == Found len retour items retour index_classes items idx = i items i idx idx i = len idx print f == Found len idx classes idx OmniglotNShot __init__ root batchsz n_way k_shot k_query imgsz device=None Different mnistNShot param root param batchsz task num param n_way param k_shot param k_query param imgsz resize = imgsz device = device os path isfile os path join root omniglot npy root data npy does exist just download x = Omniglot root download=True transform=transforms Compose lambda x Image open x convert L lambda x x resize imgsz imgsz lambda x np reshape x imgsz imgsz lambda x np transpose x lambda x x temp = label img img imgs label img img total label img label x label temp keys temp label append img temp label = img x = label imgs temp items labels info deserted each label contains imgs x append np array imgs different may have different number imgs x = np array x astype np float imgs classes total each character contains imgs print data shape x shape temp = Free memory save all dataset into npy file np save os path join root omniglot npy x print write into omniglot npy data npy exists just load x = np load os path join root omniglot npy print load omniglot npy TODO can shuffle here we must keep training test set distinct x_train x_test = x x normalization batchsz = batchsz n_cls = x shape n_way = n_way n way k_shot = k_shot k shot k_query = k_query k query assert k_shot + k_query = save pointer current read batch total cache indexes = train test datasets = train x_train test x_test original data cached print DB train x_train shape test x_test shape datasets_cache = train load_data_cache datasets train current epoch data cached test load_data_cache datasets test normalization Normalizes our data have mean sdt mean = np mean x_train std = np std x_train max = np max x_train min = np min x_train print before norm mean mean max max min min std std x_train = x_train - mean std x_test = x_test - mean std mean = np mean x_train std = np std x_train max = np max x_train min = np min x_train print after norm mean mean max max min min std std load_data_cache data_pack Collects several batches data N-shot learning param data_pack cls_num A list support_set_x support_set_y target_x target_y ready fed our networks take way shot example setsz = k_shot n_way querysz = k_query n_way data_cache = print preload next caches batchsz batch sample range num episodes x_spts y_spts x_qrys y_qrys = i range batchsz one batch means one set x_spt y_spt x_qry y_qry = selected_cls = np random choice data_pack shape n_way False j cur_class enumerate selected_cls selected_img = np random choice k_shot + k_query False meta-training meta-test x_spt append data_pack cur_class selected_img k_shot x_qry append data_pack cur_class selected_img k_shot y_spt append j _ range k_shot y_qry append j _ range k_query shuffle inside batch perm = np random permutation n_way k_shot x_spt = np array x_spt reshape n_way k_shot resize resize perm y_spt = np array y_spt reshape n_way k_shot perm perm = np random permutation n_way k_query x_qry = np array x_qry reshape n_way k_query resize resize perm y_qry = np array y_qry reshape n_way k_query perm append sptsz = b setsz x_spts append x_spt y_spts append y_spt x_qrys append x_qry y_qrys append y_qry b setsz x_spts = np array x_spts astype np float reshape batchsz setsz resize resize y_spts = np array y_spts astype int reshape batchsz setsz b qrysz x_qrys = np array x_qrys astype np float reshape batchsz querysz resize resize y_qrys = np array y_qrys astype int reshape batchsz querysz x_spts y_spts x_qrys y_qrys = torch from_numpy z device z x_spts y_spts x_qrys y_qrys data_cache append x_spts y_spts x_qrys y_qrys data_cache next mode= train Gets next batch dataset name param mode The name splitting one train val test update cache indexes larger cached num indexes mode = len datasets_cache mode indexes mode = datasets_cache mode = load_data_cache datasets mode next_batch = datasets_cache mode indexes mode indexes mode += next_batch