__future__ annotations inspect typing Any torch try triton except ImportError triton = None triton None triton language tl triton Config triton compiler CompiledKernel triton runtime autotuner OutOfResources triton runtime jit JITFunction KernelInterface try triton runtime autotuner PTXASError except ImportError PTXASError Exception type ignore no-redef pass try triton compiler compiler ASTSource except ImportError ASTSource = None try triton backends compiler GPUTarget except ImportError GPUTarget backend str arch int &#124; str warp_size int - Any torch version hip backend arch warp_size backend arch In latest triton math functions shuffled around into different modules https github com triton-lang triton pull try triton language extra libdevice libdevice = tl extra libdevice noqa F math = tl math except ImportError hasattr tl extra cuda hasattr tl extra cuda libdevice libdevice = tl extra cuda libdevice math = tl math hasattr tl extra intel hasattr tl extra intel libdevice libdevice = tl extra intel libdevice math = tl math libdevice = tl math math = tl try triton language standard _log except ImportError _log x Any - Any raise NotImplementedError _triton_config_has param_name str - bool hasattr triton Config False hasattr triton Config __init__ False param_name inspect signature triton Config __init__ parameters HAS_WARP_SPEC = hasattr tl async_task _triton_config_has num_consumer_groups _triton_config_has num_buffers_warp_spec try triton knobs except ImportError knobs = None try triton runtime cache triton_key type ignore attr-defined except ImportError triton compiler compiler triton_key type ignore attr-defined no-redef builtins_use_semantic_kwarg = _semantic inspect signature triton language core view parameters HAS_TRITON = True _raise_error args Any kwargs Any - Any raise RuntimeError triton package installed OutOfResources Exception type ignore no-redef pass PTXASError Exception type ignore no-redef pass Config = object CompiledKernel = object KernelInterface = object ASTSource = None GPUTarget = None _log = _raise_error libdevice = None math = None knobs = None builtins_use_semantic_kwarg = False triton type ignore no-redef staticmethod jit args Any kwargs Any - Any _raise_error tl type ignore no-redef staticmethod constexpr val Any - Any val tensor = Any dtype = Any JITFunction type ignore no-redef pass HAS_WARP_SPEC = False triton_key = _raise_error HAS_TRITON = False cc_warp_size cc str &#124; int - int torch version hip cc_str = str cc gfx cc_str gfx cc_str try autograd_profiler = torch autograd profiler except AttributeError Compile workers only have mock version torch autograd_profiler type ignore no-redef _is_profiler_enabled = False __all__ = Config CompiledKernel OutOfResources KernelInterface PTXASError ASTSource GPUTarget tl _log libdevice math triton cc_warp_size knobs triton_key