Owner s oncall distributed copy json logging math operator os pickle random sys tempfile time datetime timedelta functools reduce itertools groupby torch torch distributed c d c d is_available c d is_gloo_available print c d GLOO available skipping tests file=sys stderr sys exit test_c d_common test_c d_common FFTModel gpus_for_rank LOOPBACK ModuleForDdpCommHook SparseGradientModule Task torch distributed dist torch nn functional F torch testing _internal common_utils common torch nn torch distributed _shard sharded_tensor init_from_local_shards Shard ShardedTensor ShardMetadata torch nn parallel DistributedDataParallel torch testing _internal common_distributed create_device MultiProcessTestCase requires_gloo simple_sparse_reduce_tests skip_if_lt_x_gpu skip_if_win verify_ddp_error_logged torch testing _internal common_utils MI _ARCH retry_on_connect_failures run_tests skip_but_pass_in_sandcastle skipIfRocm skipIfRocmArch TestCase simple_reduce_tests rank world_size tests = c d ReduceOp SUM torch tensor rank + torch tensor float world_size world_size + c d ReduceOp PRODUCT torch tensor rank + torch tensor float math factorial world_size c d ReduceOp MIN torch tensor rank + torch tensor c d ReduceOp MAX torch tensor rank + torch tensor float world_size c d ReduceOp AVG torch tensor rank + torch tensor float world_size + Generate tests BAND The bit set changes every iteration check output changes accordingly i range vin = rank &#124; i vout = i tests append c d ReduceOp BAND torch tensor vin dtype=torch int torch tensor vout dtype=torch int Generate tests BOR These emulate larger world size per iteration having every rank contribute multiple values pre-OR ed i range vin = reduce operator or_ rank i + j j range i vout = reduce operator or_ range world_size i tests append c d ReduceOp BOR torch tensor vin dtype=torch int torch tensor vout dtype=torch int Generate tests XOR These emulate larger world size per iteration having every rank contribute multiple values pre-XOR ed i range vin = reduce operator xor rank i + j j range i vout = reduce operator xor range world_size i tests append c d ReduceOp BXOR torch tensor vin dtype=torch int torch tensor vout dtype=torch int Extend tests cfloat dtype tests extend c d ReduceOp SUM torch tensor complex rank + rank + dtype=torch cfloat torch tensor complex world_size world_size + world_size world_size + dtype=torch cfloat c d ReduceOp AVG torch tensor complex rank + rank + dtype=torch cfloat torch tensor complex float world_size + float world_size + dtype=torch cfloat tests simple_coalesced_reduce_tests rank world_size c d ReduceOp SUM torch tensor rank + torch tensor rank + torch tensor float world_size world_size + torch tensor float world_size world_size + world_size + c d ReduceOp PRODUCT torch tensor rank + torch tensor rank + torch tensor float math factorial world_size torch tensor float math factorial world_size + c d ReduceOp MIN torch tensor rank + x x torch tensor torch tensor c d ReduceOp MAX torch tensor rank + x x torch tensor float world_size torch tensor world_size + simple_multi_input_reduce_tests rank world_size c d ReduceOp SUM torch tensor rank + torch tensor rank + torch tensor float world_size world_size - c d ReduceOp PRODUCT torch tensor rank + torch tensor rank + torch tensor float math factorial world_size c d ReduceOp MIN torch tensor rank + torch tensor rank + torch tensor c d ReduceOp MAX torch tensor rank + torch tensor rank + torch tensor world_size RendezvousTCPTest TestCase retry_on_connect_failures test_tcp_init rendezvous_iterator = dist rendezvous tcp rank= world_size= store rank world_size = next rendezvous_iterator assertEqual rank assertEqual world_size port number should get assigned assertNotEqual store port RendezvousEnvTest TestCase requires_gloo retry_on_connect_failures test_logging_init os environ WORLD_SIZE = os environ MASTER_ADDR = os environ MASTER_PORT = str common find_free_port os environ RANK = previous_handlers = logging root handlers c d init_process_group backend= gloo init_method= env current_handlers = logging root handlers assertEqual len previous_handlers len current_handlers current previous zip current_handlers previous_handlers assertEqual current previous c d destroy_process_group TimeoutTest test_c d_common AbstractTimeoutTest TestCase requires_gloo retry_on_connect_failures test_default_store_timeout_gloo _test_default_store_timeout gloo ProcessGroupGlooTest MultiProcessTestCase lazy_init = False _create_process_group_gloo store rank world_size opts pg = c d ProcessGroupGloo store rank world_size opts dist barrier group=pg pg setUp super setUp _spawn_processes opts threads= group_name= opts = c d ProcessGroupGloo _Options opts _timeout = opts _devices = create_device interface=LOOPBACK lazy_init=self lazy_init opts _threads = threads opts group_name = group_name opts requires_gloo test_multi_device_constructor store = c d FileStore file_name world_size opts = c d ProcessGroupGloo _Options opts _timeout = opts _devices = create_device interface=LOOPBACK lazy_init=self lazy_init create_device interface=LOOPBACK lazy_init=self lazy_init pg = _create_process_group_gloo store rank world_size opts Execute x number operations ensure we use every device fut pg allreduce torch ones i + get_future i range fut wait requires_gloo test_empty_tensors store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts xs = torch FloatTensor fut = pg broadcast xs get_future fut wait output = fut value assertEqual output numel assertEqual xs output requires_gloo test_broadcast_checks store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts t = torch zeros dtype=torch float t = torch zeros dtype=torch float t = torch zeros dtype=torch float assertRaisesRegex RuntimeError invalid root rank opts = c d BroadcastOptions opts rootRank = - opts rootTensor = pg broadcast t opts assertRaisesRegex RuntimeError invalid root rank opts = c d BroadcastOptions opts rootRank = world_size opts rootTensor = pg broadcast t opts assertRaisesRegex RuntimeError invalid root tensor opts = c d BroadcastOptions opts rootRank = rank opts rootTensor = - pg broadcast t opts assertRaisesRegex RuntimeError invalid root tensor opts = c d BroadcastOptions opts rootRank = rank opts rootTensor = pg broadcast t opts assertRaisesRegex RuntimeError invalid root tensor opts = c d BroadcastOptions opts rootRank = rank opts rootTensor = pg broadcast opts assertRaisesRegex RuntimeError invalid tensor type opts = c d BroadcastOptions opts rootRank = rank opts rootTensor = pg broadcast t t opts assertRaisesRegex RuntimeError invalid tensor size opts = c d BroadcastOptions opts rootRank = rank opts rootTensor = pg broadcast t t opts _test_broadcast_basics fn store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts broadcast xs rootRank rootTensor opts = c d BroadcastOptions opts rootRank = rootRank opts rootTensor = rootTensor fut = pg broadcast xs opts get_future fut wait fut value Every rank root once i range world_size Run input tensor x = fn torch tensor rank output = broadcast x i assertEqual torch tensor i output Run input tensors num = j range num xs = fn torch tensor rank num + fn torch tensor rank num + output = broadcast xs i j assertEqual torch tensor i num + j dtype=torch float output assertEqual torch tensor i num + j dtype=torch float output Run input tensor cfloat dtype x = fn torch tensor complex rank rank dtype=torch cfloat output = broadcast x i assertEqual torch tensor complex i i dtype=torch cfloat output Test overloaded convenience function x = torch tensor rank + fut = pg broadcast x root= get_future fut wait result = fut value assertEqual torch tensor result requires_gloo test_broadcast_basics _test_broadcast_basics lambda t t clone skip_if_lt_x_gpu requires_gloo test_broadcast_basics_cuda _test_broadcast_basics lambda t t clone cuda _test_broadcast_stress inputs store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts threads= work_handles = pg broadcast inputs i root= i world_size i range len inputs i work_handle enumerate work_handles work_handle wait assertEqual torch tensor i world_size + i world_size inputs i msg= f Mismatch iteration i d requires_gloo test_broadcast_stress inputs = torch tensor i world_size + rank i range _test_broadcast_stress inputs skip_if_lt_x_gpu requires_gloo skipIfRocm test_broadcast_stress_cuda inputs = torch tensor i world_size + rank cuda i range _test_broadcast_stress inputs requires_gloo test_allreduce_checks store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts t = torch zeros dtype=torch float t = torch zeros dtype=torch float t = torch zeros dtype=torch float assertRaisesRegex RuntimeError requires non-empty tensor list opts = c d AllreduceOptions pg allreduce opts assertRaisesRegex RuntimeError invalid tensor type opts = c d AllreduceOptions pg allreduce t t opts assertRaisesRegex RuntimeError invalid tensor size opts = c d AllreduceOptions pg allreduce t t opts requires_gloo test_allreduce_op_timeout store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts opts = c d AllreduceOptions opts timeout = timedelta milliseconds= rank == t = torch zeros dtype=torch float assertRaisesRegex RuntimeError Timed out waiting ms pg allreduce t opts wait requires_gloo test_allreduce_overall_timeout store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts pg set_timeout timedelta milliseconds= rank == t = torch zeros dtype=torch float assertRaisesRegex RuntimeError Timed out waiting ms pg allreduce t wait _test_allreduce_basics fn store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts Single input tests tests = simple_reduce_tests rank world_size op input expected tests opts = c d AllreduceOptions opts reduceOp = op tensor = fn input fut = pg allreduce tensor opts get_future fut wait result = fut value assertEqual expected result Multi input tests tests = simple_multi_input_reduce_tests rank world_size op inputs output tests opts = c d AllreduceOptions opts reduceOp = op tensors = fn input input inputs fut = pg allreduce tensors opts get_future fut wait result = fut value tensor result assertEqual output tensor Test overloaded convenience function defaults using sum x = fn torch tensor rank + fut = pg allreduce x get_future fut wait result = fut value assertEqual torch tensor float world_size world_size + result Test fp numerical correctness all-reduce SUM torch manual_seed rank TODO when create larger sizes tensors numerical instability will observed We need investigate root cause ensure fixed tensor = torch rand dtype=torch float - world_size opts = c d AllreduceOptions tensor = tensor torch float output = torch zeros_like tensor _ range world_size allgather all local tensors first then sum up fut = pg allgather output tensor get_future fut wait ag_result = fut value total = torch stack ag_result dim= sum dim= result fp all-reduce fut = pg allreduce tensor opts get_future fut wait result_fp = fut value float has only ~ bits mantissa sensitive accumulation order rounding errors so we use larger tolerance assertEqual total result_fp rtol= e- atol= e- requires_gloo test_allreduce_basics _test_allreduce_basics lambda t t clone skip_if_lt_x_gpu requires_gloo test_allreduce_basics_cuda _test_allreduce_basics lambda t t clone cuda _test_allreduce_stress inputs store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts threads= future_handles = pg allreduce inputs i get_future i range len inputs i future_handle enumerate future_handles future_handle wait assertEqual torch tensor i world_size + world_size world_size - future_handle value msg= f Mismatch iteration i d requires_gloo test_allreduce_stress inputs = torch tensor i + rank i range _test_allreduce_stress inputs skip_if_lt_x_gpu requires_gloo skipIfRocm test_allreduce_stress_cuda inputs = torch tensor i + rank cuda i range _test_allreduce_stress inputs requires_gloo test_allreduce_coalesced_checks store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts t = torch zeros dtype=torch float t = torch zeros dtype=torch float t = torch sparse_coo_tensor size= assertRaisesRegex RuntimeError requires non-empty tensor list opts = c d AllreduceCoalescedOptions pg allreduce_coalesced opts assertRaisesRegex RuntimeError tensors must all have same type opts = c d AllreduceCoalescedOptions pg allreduce_coalesced t t opts assertRaisesRegex RuntimeError invalid tensor layout index opts = c d AllreduceCoalescedOptions pg allreduce_coalesced t t opts assertRaisesRegex RuntimeError unsupported layout opts = c d AllreduceCoalescedOptions pg allreduce_coalesced t t clone opts skip_if_lt_x_gpu requires_gloo test_allreduce_coalesced_checks_cuda store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts t = torch zeros dtype=torch float assertRaisesRegex RuntimeError unsupported device type opts = c d AllreduceCoalescedOptions pg allreduce_coalesced t cuda t cuda opts _test_allreduce_coalesced_basics fn store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts test_cases = simple_coalesced_reduce_tests rank world_size op inputs outputs test_cases opts = c d AllreduceCoalescedOptions opts reduceOp = op tensors = fn x x inputs fut = pg allreduce_coalesced tensors opts get_future fut wait result = fut value result_tensor expected zip result outputs assertEqual result_tensor expected requires_gloo test_allreduce_coalesced_basics _test_allreduce_coalesced_basics lambda t t clone _expected_output i ws = world_size torch tensor i ws + ws ws - _test_allreduce_coalesced_stress inputs store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts threads= future_handles = pg allreduce_coalesced input get_future input inputs i future_handle enumerate future_handles future_handle wait result = future_handle value assertEqual _expected_output i result msg=f Mismatch iteration i requires_gloo test_allreduce_coalesced_stress inputs = torch tensor i + rank i range _test_allreduce_coalesced_stress inputs requires_gloo test_allreduce_coalesced_async store = c d FileStore file_name world_size c d init_process_group backend= gloo rank=self rank world_size=self world_size store=store xs = torch tensor i + rank i range futs = c d all_reduce_coalesced x async_op=True x xs torch futures wait_all futs i fut enumerate futs assertEqual _expected_output i fut wait msg=f Mismatch iteration i requires_gloo test_sparse_allreduce_checks store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts t = torch zeros t = torch sparse_coo_tensor size= t = torch sparse_coo_tensor size= assertRaisesRegex RuntimeError requires non-empty tensor list opts = c d AllreduceOptions pg allreduce opts assertRaisesRegex RuntimeError invalid tensor layout opts = c d AllreduceOptions pg allreduce t t opts assertRaisesRegex RuntimeError invalid tensor size opts = c d AllreduceOptions pg allreduce t t opts Sparse allreduce only works c d ReduceOp SUM op c d ReduceOp PRODUCT c d ReduceOp MIN c d ReduceOp MAX assertRaisesRegex RuntimeError unsupported reduction operation opts = c d AllreduceOptions opts reduceOp = op pg allreduce t opts _test_sparse_allreduce_basics fn store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts num_inputs_per_rank tests = simple_sparse_reduce_tests rank world_size num_inputs=num_inputs_per_rank inputs outputs tests tensors = fn input input inputs fut = pg allreduce tensors get_future fut wait result = fut value assertEqual tensors outputs assertEqual result outputs requires_gloo test_sparse_allreduce_basics _test_sparse_allreduce_basics lambda t t skip_if_lt_x_gpu requires_gloo test_sparse_allreduce_basics_cuda _test_sparse_allreduce_basics lambda t t clone cuda skip_if_lt_x_gpu requires_gloo test_sparse_allreduce_cuda_dispatched store = c d FileStore file_name world_size dist init_process_group backend= gloo store=store rank=self rank world_size=self world_size tests = simple_sparse_reduce_tests rank world_size num_inputs= inputs outputs tests tensors = inputs - clone cuda work = dist all_reduce tensors async_op=True work wait assertEqual tensors outputs requires_gloo test_allgather_into_tensor_coalesced store = c d FileStore file_name world_size dist init_process_group backend= gloo store=store rank=self rank world_size=self world_size torch manual_seed in_shapes = out_shapes = s world_size + s s in_shapes outputs = torch empty s s out_shapes inputs = torch rand s s in_shapes work = dist group WORLD allgather_into_tensor_coalesced outputs inputs work wait output input zip outputs inputs expect = torch cat input world_size assertTrue torch allclose output expect requires_gloo test_reduce_scatter store = c d FileStore file_name world_size dist init_process_group backend= gloo store=store rank=self rank world_size=self world_size torch manual_seed variable size per rank inputs = torch rand i i range world_size output = torch empty rank work = dist reduce_scatter output inputs async_op=True work wait expect = inputs rank world_size assertTrue torch allclose output expect requires_gloo test_reduce_scatter_tensor store = c d FileStore file_name world_size dist init_process_group backend= gloo store=store rank=self rank world_size=self world_size torch manual_seed out_shape = in_shape = out_shape world_size + out_shape output = torch empty out_shape input = torch rand in_shape work = dist reduce_scatter_tensor output input async_op=True work wait expect = input view world_size out_shape chunk world_size rank world_size assertTrue torch allclose output expect requires_gloo test_reduce_scatter_tensor_coalesced store = c d FileStore file_name world_size dist init_process_group backend= gloo store=store rank=self rank world_size=self world_size torch manual_seed out_shapes = in_shapes = s world_size + s s out_shapes outputs = torch empty s s out_shapes inputs = torch rand s s in_shapes work = dist group WORLD reduce_scatter_tensor_coalesced outputs inputs work wait output input zip outputs inputs expect = input view world_size output shape chunk world_size rank world_size assertTrue torch allclose output expect requires_gloo test_scatter_checks store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts t = torch zeros dtype=torch float t = torch zeros dtype=torch float t = torch zeros dtype=torch float assertRaisesRegex RuntimeError invalid root rank opts = c d ScatterOptions opts rootRank = - pg scatter t opts assertRaisesRegex RuntimeError invalid root rank opts = c d ScatterOptions opts rootRank = world_size pg scatter t opts assertRaisesRegex RuntimeError requires single-element output tensor list opts = c d ScatterOptions opts rootRank = pg scatter opts assertRaisesRegex RuntimeError requires single-element output tensor list opts = c d ScatterOptions opts rootRank = pg scatter t t opts assertRaisesRegex RuntimeError requires single-element input list opts = c d ScatterOptions opts rootRank = rank pg scatter t opts assertRaisesRegex RuntimeError requires single-element input list opts = c d ScatterOptions opts rootRank = rank pg scatter t t world_size t world_size opts desired_list_size = world_size incorrect_list_size = world_size - err_str = Incorrect input list size Input list size should assertRaisesRegex RuntimeError err_str format incorrect_list_size desired_list_size opts = c d ScatterOptions opts rootRank = rank pg scatter t t incorrect_list_size opts incorrect_list_size = world_size + assertRaisesRegex RuntimeError err_str format incorrect_list_size desired_list_size opts = c d ScatterOptions opts rootRank = rank pg scatter t t incorrect_list_size opts assertRaisesRegex RuntimeError invalid tensor type opts = c d ScatterOptions opts rootRank = rank pg scatter t t world_size opts assertRaisesRegex RuntimeError invalid tensor size opts = c d ScatterOptions opts rootRank = rank pg scatter t t world_size opts assertRaisesRegex RuntimeError requires empty input non-root opts = c d ScatterOptions opts rootRank = rank + world_size pg scatter t t world_size opts _test_scatter_basics fn store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts Preallocate tensors input output input = fn torch tensor rank _ range world_size outputs = fn torch tensor - _ range world_size Take turns being scatter root accumulate work items futures = i range world_size opts = c d ScatterOptions opts rootRank = i i == rank futures append pg scatter outputs i input opts get_future futures append pg scatter outputs i opts get_future Wait work complete i range world_size futures i wait result = futures i value assertEqual torch tensor i result requires_gloo test_scatter_basics _test_scatter_basics lambda t t clone skip_if_lt_x_gpu requires_gloo test_scatter_basics_cuda _test_scatter_basics lambda t t clone cuda _test_scatter_stress inputs fn store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts threads= outputs = fn torch tensor - _ range world_size _ range len inputs future_handles = i range len inputs root range world_size opts = c d ScatterOptions opts rootRank = root root == rank fut = pg scatter outputs i root fn e e inputs i opts get_future fut = pg scatter outputs i root opts get_future future_handles append fut i future_handle enumerate future_handles future_handle wait iter = i world_size root = i world_size result = future_handle value assertEqual torch tensor iter + root result msg= f Mismatch iteration iter d rank root d requires_gloo test_set_gloo_pg_timeout store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts pg allreduce torch rand assertEqual pg options _timeout timedelta seconds= pg _set_default_timeout timedelta seconds= assertEqual pg options _timeout timedelta seconds= requires_gloo test_scatter_stress inputs = torch tensor i + rank _ range world_size i range _test_scatter_stress inputs lambda t t clone skip_but_pass_in_sandcastle Test flaky see https github com pytorch pytorch issues skip_if_lt_x_gpu requires_gloo skipIfRocm test_scatter_stress_cuda inputs = torch tensor i + rank _ range world_size i range _test_scatter_stress inputs lambda t t clone cuda requires_gloo test_gather_checks store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts t = torch zeros dtype=torch float t = torch zeros dtype=torch float t = torch zeros dtype=torch float assertRaisesRegex RuntimeError invalid root rank opts = c d GatherOptions opts rootRank = - pg gather t opts assertRaisesRegex RuntimeError invalid root rank opts = c d GatherOptions opts rootRank = world_size pg gather t opts assertRaisesRegex RuntimeError requires single-element input tensor list opts = c d GatherOptions opts rootRank = pg gather opts assertRaisesRegex RuntimeError requires single-element input tensor list opts = c d GatherOptions opts rootRank = pg gather t t opts assertRaisesRegex RuntimeError requires single-element output list opts = c d GatherOptions opts rootRank = rank pg gather t opts assertRaisesRegex RuntimeError requires single-element output list opts = c d GatherOptions opts rootRank = rank pg gather t world_size t world_size t opts desired_list_size = world_size incorrect_list_size = world_size - err_str = Incorrect output list size Output list size should assertRaisesRegex RuntimeError err_str format incorrect_list_size desired_list_size opts = c d GatherOptions opts rootRank = rank pg gather t incorrect_list_size t opts incorrect_list_size = world_size + assertRaisesRegex RuntimeError err_str format incorrect_list_size desired_list_size opts = c d GatherOptions opts rootRank = rank pg gather t incorrect_list_size t opts assertRaisesRegex RuntimeError invalid tensor type opts = c d GatherOptions opts rootRank = rank pg gather t world_size t opts assertRaisesRegex RuntimeError invalid tensor size opts = c d GatherOptions opts rootRank = rank pg gather t world_size t opts assertRaisesRegex RuntimeError requires empty output non-root opts = c d GatherOptions opts rootRank = rank + world_size pg gather t world_size t opts _test_gather_basics fn store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts Preallocate tensors input output input = fn torch tensor rank outputs = fn torch tensor - _ range world_size Take turns being gather root accumulate work items futures = i range world_size opts = c d GatherOptions opts rootRank = i i == rank futures append pg gather outputs input opts get_future futures append pg gather input opts get_future Wait work complete expected = fn torch tensor rank rank range world_size i range world_size futures i wait result = futures i value i == rank assertEqual expected result requires_gloo test_gather_basics _test_gather_basics lambda t t clone skip_if_lt_x_gpu requires_gloo test_gather_basics_cuda _test_gather_basics lambda t t clone cuda requires_gloo test_gather_noncontiguous_input Take column D tensor such memory dense _test_gather_basics lambda t t expand tril contiguous _test_gather_stress inputs fn store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts threads= future_handles = outputs = fn torch tensor - _ range world_size _ range len inputs expected_outputs = torch tensor i + j j range world_size i range len inputs i range len inputs root range world_size opts = c d GatherOptions opts rootRank = root root == rank fut = pg gather outputs i fn inputs i opts get_future fut = pg gather fn inputs i opts get_future future_handles append fut i future_handle enumerate future_handles future_handle wait iter = i world_size root = i world_size root == rank result = future_handle value assertEqual expected_outputs iter result msg= f Mismatch iteration iter d root root d requires_gloo test_gather_stress inputs = torch tensor i + rank i range _test_gather_stress inputs lambda t t clone skip_if_lt_x_gpu skipIfRocmArch MI _ARCH requires_gloo test_gather_stress_cuda inputs = torch tensor i + rank cuda i range _test_gather_stress inputs lambda t t clone cuda requires_gloo test_allgather_checks store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts t = torch zeros dtype=torch float t = torch zeros dtype=torch float t = torch zeros dtype=torch float assertRaisesRegex RuntimeError requires non-empty input tensor list pg allgather assertRaisesRegex RuntimeError requires input output tensor lists have same length pg allgather t assertRaisesRegex RuntimeError requires input output tensor lists have same length pg allgather t world_size t world_size t assertRaisesRegex RuntimeError invalid output tensor list pg allgather t world_size - t assertRaisesRegex RuntimeError invalid output tensor list pg allgather t world_size + t assertRaisesRegex RuntimeError invalid tensor type pg allgather t t world_size t t world_size t t assertRaisesRegex RuntimeError invalid tensor size pg allgather t t world_size t t world_size t t assertRaisesRegex RuntimeError invalid tensor type pg allgather t t world_size world_size t assertRaisesRegex RuntimeError invalid tensor size pg allgather t t world_size world_size t _test_allgather_basics fn store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts Run N input tensor per rank n input = fn torch tensor n rank + i i range n output = fn torch tensor - _ range n world_size _ range n expected_output = fn torch tensor i i range n world_size _ range n fut = pg allgather output input get_future fut wait result = fut value n == result = result assertEqual expected_output result requires_gloo test_allgather_basics _test_allgather_basics lambda t t clone skip_if_lt_x_gpu requires_gloo test_allgather_basics_cuda _test_allgather_basics lambda t t clone cuda requires_gloo test_allgather_noncontiguous_input Take column D tensor such memory dense _test_allgather_basics lambda t t expand tril contiguous requires_gloo test_allgather_inference_mode torch inference_mode _test_allgather_basics lambda t t clone _test_allgather_stress inputs fn store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts threads= future_handles = outputs = fn torch tensor - _ range world_size _ range len inputs expected_outputs = torch tensor i + j j range world_size i range len inputs input_holder = i range len inputs Note works around data race discussed https github com pytorch pytorch issues we should actually able pass list directly into allgather when race fixed input_holder i = fn inputs i fut = pg allgather outputs i input_holder i get_future future_handles append fut i future_handle enumerate future_handles future_handle wait result = future_handle value assertEqual expected_outputs i result msg= f Mismatch iteration i d requires_gloo test_allgather_stress inputs = torch tensor i + rank i range _test_allgather_stress inputs lambda t t clone skip_if_lt_x_gpu requires_gloo skipIfRocm test_allgather_stress_cuda inputs = torch tensor i + rank cuda i range _test_allgather_stress inputs lambda t t clone cuda requires_gloo test_allgather_coalesced_checks store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts dummy_input = torch zeros dtype=torch float dummy_output_lists = torch zeros dtype=torch float _ range world_size One output tensors does match input list dummy_output_lists = torch zeros dtype=torch float assertRaisesRegex RuntimeError invalid size output tensor index c d all_gather_coalesced dummy_output_lists dummy_input pg One output tensors does match input list dummy_output_lists = torch zeros dtype=torch float assertRaisesRegex RuntimeError invalid tensor type index c d all_gather_coalesced dummy_output_lists dummy_input pg Output lists have too many elements dummy_output_lists = torch zeros dtype=torch float _ range world_size + assertRaisesRegex RuntimeError output lists should equal world size c d all_gather_coalesced dummy_output_lists dummy_input pg Output list lists dummy_output_lists = torch zeros dtype=torch float assertRaisesRegex TypeError Invalid function argument output_tensor_lists c d all_gather_coalesced dummy_output_lists dummy_input pg requires_gloo test_allgather_coalesced_async store = c d FileStore file_name world_size c d init_process_group backend= gloo rank=self rank world_size=self world_size store=store xxs = torch tensor i + rank i range yys = torch zeros_like x x xx _ range world_size xx xxs futs = c d all_gather_coalesced yy xx async_op=True xx yy zip xxs yys expected outputs zzs = torch tensor i + r r range world_size i range torch futures wait_all futs yy zz zip yys zzs one iteration y_out z_out zip yy zz one output tensor list y z zip y_out z_out one tensor output tensor list assertEqual y z Added address https github com pytorch pytorch issues In failed tests all assertEqual passed all processes However one processes didn t call ProcessGroupGloo destructor before exiting program This surprising only guarantee Python makes garbage collection MAY happen before program exits If GC didn t happen two threads ProcessGroup might destructed before joined FIXME s still unclear why only test require explicit destroy_process_group c d destroy_process_group requires_gloo test_reduce_checks store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts t = torch zeros dtype=torch float assertRaisesRegex RuntimeError invalid root rank opts = c d ReduceOptions opts rootRank = - opts rootTensor = pg reduce t opts assertRaisesRegex RuntimeError invalid root rank opts = c d ReduceOptions opts rootRank = world_size opts rootTensor = pg reduce t opts assertRaisesRegex RuntimeError invalid root tensor opts = c d ReduceOptions opts rootRank = rank opts rootTensor = pg reduce t opts assertRaisesRegex RuntimeError requires single-element tensor list opts = c d ReduceOptions opts rootRank = rank opts rootTensor = pg reduce t t opts _test_reduce_basics fn store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts op input output simple_reduce_tests rank world_size root range world_size opts = c d ReduceOptions opts reduceOp = op opts rootRank = root tmp = fn input fut = pg reduce tmp opts get_future fut wait result = fut value root == rank assertEqual output result requires_gloo test_reduce_basics _test_reduce_basics lambda t t clone skip_if_lt_x_gpu requires_gloo test_reduce_basics_cuda _test_reduce_basics lambda t t clone cuda _test_reduce_stress inputs store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts threads= future_handles = outputs = i range len inputs root range world_size opts = c d ReduceOptions opts rootRank = root tmp = inputs i clone outputs append tmp fut = pg reduce tmp opts get_future future_handles append fut i future_handle enumerate future_handles future_handle wait result = future_handle value iter = i world_size root = i world_size root == rank assertEqual torch tensor iter world_size + world_size world_size - result msg= f Mismatch iteration iter d root rank root d requires_gloo test_reduce_stress inputs = torch tensor i + rank i range _test_reduce_stress inputs skip_if_lt_x_gpu requires_gloo skipIfRocm test_reduce_stress_cuda inputs = torch tensor i + rank cuda i range _test_reduce_stress inputs requires_gloo test_send_recv_all_to_all store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts Preallocate tensors input output inputs = torch tensor rank _ range world_size outputs = torch tensor - _ range world_size Issue sends send_work = i range world_size i == rank continue send_work append pg send inputs i i Issue recvs recv_work = i range world_size i == rank continue recv_work append pg recv outputs i i Wait sends complete work send_work work wait assertTrue work is_completed Wait recvs complete work recv_work work wait assertTrue work is_completed Test every output other than our own contains respective rank i range world_size i == rank continue assertEqual torch tensor i outputs i requires_gloo test_barrier_implies_wait store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts Kick off allreduce operations size = num = tensors = torch full size float i i range num tensor tensors Note leak returned work handle pg allreduce tensor Barrier should ensure all previous work has completed pg barrier get_future wait i tensor enumerate tensors assertEqual torch full size float i world_size tensor skip_if_lt_x_gpu requires_gloo skipIfRocm test_block_current_stream_cuda store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts t = torch zeros device= cuda work = pg allreduce t work block_current_stream torch cuda current_stream synchronize work wait requires_gloo test_send_recv_complex store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts Generate same random tensor torch manual_seed send_tensor = torch rand dtype=torch cfloat rank == pg send send_tensor wait rank == recv_tensor = torch rand dtype=torch cfloat pg recv recv_tensor wait assertEqual send_tensor recv_tensor DistributedDataParallelTest test_c d_common CommonDistributedDataParallelTest MultiProcessTestCase setUp super setUp _spawn_processes _get_process_group store = _get_store c d init_process_group backend= gloo store=store rank=self rank world_size=self world_size c d distributed_c d _get_default_group _test_gloo_backend devices device_ids multi_device=False gradient_as_bucket_view=False store = c d FileStore file_name world_size c d init_process_group backend= gloo store=store rank=self rank world_size=self world_size process_group = c d distributed_c d _get_default_group device = devices - backend = process_group _get_backend device backend create_device interface=LOOPBACK _test_ddp_with_process_group process_group devices device_ids multi_device gradient_as_bucket_view requires_gloo test_gloo_backend_cpu_module _test_gloo_backend torch device cpu None requires_gloo test_gloo_backend_cpu_module_grad_is_view _test_gloo_backend torch device cpu None gradient_as_bucket_view=True requires_gloo skip_if_lt_x_gpu test_gloo_backend_ gpu_module_device_ids_integer_list int_devices = gpus_for_rank world_size rank devices = torch device cuda + str i i int_devices _test_gloo_backend devices int_devices requires_gloo skip_if_lt_x_gpu test_gloo_backend_ gpu_module_device_ids_torch_device_list int_devices = gpus_for_rank world_size rank devices = torch device cuda + str i i int_devices _test_gloo_backend devices devices requires_gloo skip_if_lt_x_gpu test_gloo_backend_ gpu_module int_devices = gpus_for_rank world_size rank devices = torch device cuda + str i i int_devices _test_gloo_backend devices None multi_device=True requires_gloo skip_if_lt_x_gpu test_gloo_backend_ gpu_module int_devices = gpus_for_rank world_size rank devices = torch device cuda + str i i int_devices _test_gloo_backend devices None multi_device=True _test_global_local_unused_params_grad gradient_as_bucket_view=False static_graph=False By simulating multi-task training test make sure DDP does touch grad globally unused parameters DDP does update grad locally unused parameters GlobalLocalUnusedParamModule nn Module __init__ - None super __init__ t = Task t = Task task_unused = Task task_parameters t p t p task_unused p forward x rank t x rank == t x run_and_verify_grad model Run forward output = model rank The grads all parameters should None point t _p t _p task_unused_p = model module task_parameters assertIsNone t _p grad assertIsNone t _p grad assertIsNone task_unused_p grad Run backward output mean backward Now locally unused parameter should have grad updated all ranks However globally unused parameter should still have None grad assertIsNotNone t _p grad assertIsNotNone t _p grad assertIsNone task_unused_p grad process_group = _get_process_group Test CPU cpu_model = DistributedDataParallel GlobalLocalUnusedParamModule cpu process_group=process_group find_unused_parameters=True gradient_as_bucket_view=gradient_as_bucket_view static_graph=static_graph run_and_verify_grad cpu_model Test GPU device_id = gpus_for_rank world_size rank gpu_model = DistributedDataParallel GlobalLocalUnusedParamModule device_id device_ids= device_id process_group=process_group find_unused_parameters=True gradient_as_bucket_view=gradient_as_bucket_view static_graph=static_graph run_and_verify_grad gpu_model requires_gloo skip_if_lt_x_gpu test_global_local_unused_params_grad _test_global_local_unused_params_grad requires_gloo skip_if_lt_x_gpu test_global_local_unused_params_grad_with_grad_is_view _test_global_local_unused_params_grad gradient_as_bucket_view=True requires_gloo skip_if_lt_x_gpu test_global_local_unused_params_grad_with_static_graph _test_global_local_unused_params_grad static_graph=True requires_gloo skip_if_lt_x_gpu test_find_unused_parameters_when_unused_parameters_empty An empty unused_parameters array does imply find_unused_parameters = false This test makes sure DDP allreduces unused parameters accordingly where forward pass some process uses all parameters This unit test creates module uses all parameters rank = has unused parameters other ranks FindUnusedParamModule nn Module __init__ - None super __init__ t = Task t = Task task_parameters t p t p forward x rank t t x rank == t x run_and_verify_grad model Run forward output = model rank The grads all parameters should None point assertIsNone t_p grad t_p model module task_parameters Run backward output mean backward Now locally unused parameter should have grad updated all ranks assertIsNotNone t_p grad t_p model module task_parameters process_group = _get_process_group Test CPU cpu_model = DistributedDataParallel FindUnusedParamModule cpu process_group=process_group find_unused_parameters=True run_and_verify_grad cpu_model Test GPU device_id = gpus_for_rank world_size rank gpu_model = DistributedDataParallel FindUnusedParamModule device_id device_ids= device_id process_group=process_group find_unused_parameters=True run_and_verify_grad gpu_model requires_gloo test_ignored_output Test output model can ignored there no implicit requirement ` backward ` gets called process_group = _get_process_group IgnoredOutput nn Module __init__ - None super __init__ fc = nn Linear bias=False fc = nn Linear bias=False relu = nn ReLU forward x x = relu fc x x = relu fc x F softmax x dim= model = DistributedDataParallel IgnoredOutput float process_group=process_group batch_size = criterion = nn CrossEntropyLoss input = torch rand batch_size dtype=torch float target = torch LongTensor random randrange _ range batch_size Run few iterations where we ignore output _ range output = model input del output Run few iterations where we use output _ range output = model input loss = criterion output target loss backward requires_gloo test_ignored_output_with_unused_parameters Test output model can ignored there no implicit requirement ` backward ` gets called all model parameters participated computing model output process_group = _get_process_group IgnoredOutputWithUnusedParameters nn Module __init__ - None super __init__ fc = nn Linear bias=False fc = nn Linear bias=False fc = nn Linear bias=False relu = nn ReLU forward x x = relu fc x x = relu fc x F softmax x dim= model = DistributedDataParallel IgnoredOutputWithUnusedParameters float process_group=process_group find_unused_parameters=True batch_size = criterion = nn CrossEntropyLoss input = torch rand batch_size dtype=torch float target = torch LongTensor random randrange _ range batch_size Run few iterations where we ignore output _ range output = model input del output Run few iterations where we use output _ range output = model input loss = criterion output target loss backward requires_gloo skip_if_lt_x_gpu test_ignored_sharded_tensor MyModule nn Module __init__ shard_tensor ShardedTensor - None super __init__ fc = nn Linear bias=False st = nn Parameter shard_tensor relu = nn ReLU forward x x = relu fc x F softmax x dim= pg = dist init_process_group gloo init_method=f file file_name world_size=self world_size rank=self rank device = torch device f cuda rank local_shard_metadata = ShardMetadata shard_offsets= rank shard_sizes= placement=f rank rank cuda rank local_shards = Shard torch randn device=device local_shard_metadata st = init_from_local_shards local_shards m = MyModule st DistributedDataParallel _set_params_and_buffers_to_ignore_for_model module=m params_and_buffers_to_ignore= st test make DDP constructor will fail when module includes ShardedTensor when ignored DistributedDataParallel m device_ids= device device type == gpu None process_group=pg gradient_as_bucket_view=True broadcast_buffers=False static_graph=True _run_and_verify_sparse_gradients vanilla_model ddp_model mult = batch_size = mult world_size criterion = nn CrossEntropyLoss input = torch randint batch_size target = torch randint batch_size Run entire batch against single process version criterion vanilla_model input target backward Run partial batch against multi process version partial_input = input split mult rank partial_target = target split mult rank criterion ddp_model partial_input partial_target backward Check gradients sparse identical vanilla_parameter = next vanilla_model parameters ddp_parameter = next ddp_model parameters assertEqual vanilla_parameter grad coalesce ddp_parameter grad coalesce requires_gloo skip_if_lt_x_gpu test_save_load_checkpoint dist init_process_group gloo init_method=f file file_name world_size=self world_size rank=self rank TestModel nn Module __init__ - None super __init__ fc = nn Linear bias=False fc = nn Linear bias=False relu = nn ReLU forward x x = relu fc x x = relu fc x F softmax x dim= train_loop model optimizer iterations _ range iterations optimizer zero_grad output = model input loss = criterion output target loss backward optimizer step device_id = gpus_for_rank world_size rank model_withload = TestModel float device_id model_withoutload = TestModel float device_id ddp_withload = DistributedDataParallel model_withload device_ids= device_id ddp_withoutload = DistributedDataParallel model_withoutload device_ids= device_id ensure all three models start same set parameters By default they randomized construction p ddp_withload parameters torch no_grad p zero_ p model_withload parameters torch no_grad p zero_ p ddp_withoutload parameters torch no_grad p zero_ batch_size = criterion = nn CrossEntropyLoss optimizer_withload = torch optim SGD ddp_withload parameters lr= optimizer_non_ddp_withload = torch optim SGD model_withload parameters lr= optimizer_withoutload = torch optim SGD ddp_withoutload parameters lr= input = torch rand batch_size dtype=torch float device_id target = torch LongTensor random randrange _ range batch_size device_id run model iterations checkpoint middle train_loop ddp_withload optimizer_withload zero out parameters both DDP non-DDP models reload them DDP state dict checkpoint_path = tempfile gettempdir + model checkpoint rank == torch save ddp_withload state_dict checkpoint_path dist barrier map_location = cuda f cuda rank d ddp_state_dict = torch load checkpoint_path map_location=map_location model ddp_withload model_withload p model parameters torch no_grad p zero_ ddp_withload load_state_dict ddp_state_dict non-DDP model needs first remove prefix module DDP state dict torch nn modules utils consume_prefix_in_state_dict_if_present ddp_state_dict module model_withload load_state_dict ddp_state_dict train_loop ddp_withload optimizer_withload train_loop model_withload optimizer_non_ddp_withload re-run model same inputs iterations no checkpoint train_loop ddp_withoutload optimizer_withoutload p_withload p_withoutload p_non_ddp_withload zip ddp_withload parameters ddp_withoutload parameters model_withload parameters assertEqual p_withload p_withoutload assertEqual p_non_ddp_withload p_withoutload _test_sparse_gradients gradient_as_bucket_view=False process_group = _get_process_group Ensure initialized weights inputs identical across processes torch manual_seed vanilla_model = SparseGradientModule ddp_model = DistributedDataParallel copy deepcopy vanilla_model process_group=process_group gradient_as_bucket_view=gradient_as_bucket_view _run_and_verify_sparse_gradients vanilla_model ddp_model requires_gloo test_sparse_gradients _test_sparse_gradients requires_gloo test_sparse_gradients_grad_is_view _test_sparse_gradients gradient_as_bucket_view=True requires_gloo test_ddp_comm_hook_future_passing_cpu This unit test verifies whether Future object passed properly The callback function creates Future object sets value store = c d FileStore file_name world_size noqa F process_group = _get_process_group Test CPU cpu_model = DistributedDataParallel ModuleForDdpCommHook cpu process_group=process_group Register DDP Communication Hook cpu_model register_comm_hook None _simple_hook check whether grads equal what then callback returns without comm_hook result would torch ones _run_and_verify_hook cpu_model torch ones _gpu_model_with_ddp_comm_hook process_group hook=None gradient_as_bucket_view=False state=None device_id = gpus_for_rank world_size rank gpu_model = DistributedDataParallel ModuleForDdpCommHook device_id device_ids= device_id process_group=process_group gradient_as_bucket_view=gradient_as_bucket_view Register DDP communication hook any hook None gpu_model register_comm_hook state hook gpu_model requires_gloo skip_if_lt_x_gpu test_ddp_comm_hook_future_passing_gpu_gloo This unit test verifies whether Future object passed properly using gloo backend The hook callback function creates Future object sets value process_group = _get_process_group Get GPU model simple_hook registered gpu_model = _gpu_model_with_ddp_comm_hook process_group _simple_hook check whether grads equal what simple_hook s then callback returns without comm_hook result would torch ones _run_and_verify_hook gpu_model torch ones requires_gloo test_ddp_invalid_comm_hook_init This unit test makes sure register_comm_hook properly checks format hook defined user The Python hook must callable This test also checks whether bucket annotation checked properly defined process_group = _get_process_group model = DistributedDataParallel ModuleForDdpCommHook process_group=process_group assertRaisesRegex TypeError Communication hook must callable model register_comm_hook state=None hook= assertRaisesRegex ValueError bucket annotation should dist GradBucket comm_hook state object bucket int - torch futures Future torch Tensor torch futures Future model register_comm_hook state=None hook=comm_hook requires_gloo test_ddp_invalid_comm_hook_return_type This test checks whether annotation checked properly defined It also checks whether internal error thrown type incorrect user hasn t specified any type annotation process_group = _get_process_group model = DistributedDataParallel ModuleForDdpCommHook process_group=process_group expected_err = Communication hook annotation should torch futures Future assertRaisesRegex ValueError expected_err comm_hook state object bucket dist GradBucket - int torch futures Future model register_comm_hook state=None hook=comm_hook verify_ddp_error_logged model expected_err assertRaisesRegex RuntimeError callback must torch futures Future object got comm_hook state object bucket dist GradBucket model register_comm_hook state=None hook=comm_hook Run forward output = model rank Run backward output mean backward requires_gloo test_ddp_comm_hook_register_just_once DDP communication hook can only registered once This test validates whether error thrown properly when register_comm_hook called more than once process_group = _get_process_group model = DistributedDataParallel ModuleForDdpCommHook process_group=process_group dummy_hook state bucket fut = torch futures Future fut set_result bucket buffer fut model register_comm_hook None dummy_hook assertRaisesRegex RuntimeError register_comm_hook register_builtin_comm_hook can only called once model register_comm_hook None dummy_hook requires_gloo test_ddp_comm_hook_sparse_gradients Runs test_sparse_gradients unit test DDP communication hook We define simple hook does allreduce works gloo backend test process_group = _get_process_group Ensure initialized weights inputs identical across processes torch manual_seed vanilla_model = SparseGradientModule ddp_model = DistributedDataParallel copy deepcopy vanilla_model process_group=process_group allreduce_hook_gloo state object bucket dist GradBucket - torch futures Future torch Tensor div_by_world_size fut Divide result world_size fut wait world_size Prepare allreduced grad bucket tensors running async work fut = process_group allreduce bucket buffer get_future fut then div_by_world_size ddp_model register_comm_hook None allreduce_hook_gloo _run_and_verify_sparse_gradients vanilla_model ddp_model requires_gloo test_ddp_complex_params process_group = _get_process_group N C H W = ddp_model = DistributedDataParallel FFTModel hin=H win=W n_features=C process_group=process_group optimizer = torch optim Adam ddp_model parameters lr= inp = torch ones N C H W dtype=torch float train step out = ddp_model inp loss = torch sum out loss backward optimizer step ReducerModule nn Module __init__ - None super __init__ fc = nn Linear bias=False fc = nn Linear bias=False fc = nn Linear bias=False relu = nn ReLU forward x use_fc =True x = relu fc x float x = relu fc x float use_fc x = fc x float F softmax x dim= ReducerTest TestCase setUp file = tempfile NamedTemporaryFile delete=False world_size = store = c d FileStore file name world_size c d init_process_group backend= gloo store=self store rank= world_size=world_size process_group = c d distributed_c d _get_default_group tearDown c d destroy_process_group try os remove file name except OSError e print str e requires_gloo test_single_dtype_single_bucket model = ReducerModule parameters = list model parameters buckets = list range len parameters dist Reducer parameters buckets dist _DEFAULT_FIRST_BUCKET_BYTES process_group _create_mixed_precision_model model = ReducerModule model float model fc double model requires_gloo test_multi_dtype_single_bucket model = _create_mixed_precision_model Raise there multiple types per bucket In case we create one bucket all parameters assertRaises RuntimeError parameters = list model parameters buckets = list range len parameters dist Reducer parameters buckets dist _DEFAULT_FIRST_BUCKET_BYTES process_group requires_gloo test_multi_dtype_multi_bucket model = _create_mixed_precision_model parameters = list model parameters group_by_dtype = groupby range len parameters key=lambda i parameters i dtype buckets = list indices _ indices group_by_dtype dist Reducer parameters buckets dist _DEFAULT_FIRST_BUCKET_BYTES _ buckets process_group _create_reducer_for_models models find_unused_parameters=False assertEqual len models parameters = list models parameters group_by_dtype = groupby range len parameters key=lambda i parameters i dtype buckets = list indices _ indices group_by_dtype dist Reducer parameters buckets dist _DEFAULT_FIRST_BUCKET_BYTES _ range len buckets process_group find_unused_parameters=find_unused_parameters requires_gloo test_forward_backward batch_size = model = _create_mixed_precision_model reducer = _create_reducer_for_models model reducer prepare_for_forward loss = nn CrossEntropyLoss input = torch rand batch_size dtype=torch double target = torch LongTensor random randrange _ range batch_size output = loss model input target reducer prepare_for_backward output output backward requires_gloo test_forward_backward_unused_parameters batch_size = model = _create_mixed_precision_model reducer = _create_reducer_for_models model find_unused_parameters=True reducer prepare_for_forward loss = nn CrossEntropyLoss input = torch rand batch_size dtype=torch double target = torch LongTensor random randrange _ range batch_size output = loss model input use_fc =False target Check grad fc set assertEqual None model fc weight grad Compute accumulate gradients reducer prepare_for_backward output output backward The reducer will have marked grad fc ready because doesn t show up autograd graph ` output ` Since fc weight considered being globally unused will kept untouched None assertEqual None model fc weight grad requires_gloo test_forward_backward_optimizer batch_size = model = _create_mixed_precision_model reducer = _create_reducer_for_models model find_unused_parameters=True reducer prepare_for_forward loss = nn CrossEntropyLoss optimizer = torch optim Adam model parameters i range input = torch rand batch_size dtype=torch double target = torch LongTensor random randrange _ range batch_size The ` zero_grad ` function calls ` detach_ ` ` zero_ ` grad tensors model parameters If we tried set grad tensors view reducer s bucket tensors would blow up optimizer zero_grad Unused parameter only first iteration output = loss model input use_fc = i target reducer prepare_for_backward output output backward optimizer step skip_if_win ProcessGroupGlooLazyInitTest ProcessGroupGlooTest lazy_init = True setUp os environ TORCH_GLOO_LAZY_INIT = super setUp tearDown - None del os environ TORCH_GLOO_LAZY_INIT super tearDown ProcessGroupGlooFRTest ProcessGroupGlooTest setUp os environ TORCH_FR_BUFFER_SIZE = super setUp tearDown - None del os environ TORCH_FR_BUFFER_SIZE super tearDown _verify_trace t is_json ver = t version assertEqual ver pg_config = t pg_config assertEqual len pg_config default_pg_info = pg_config assertIn name default_pg_info assertIn desc default_pg_info assertIn ranks default_pg_info pg_status = t pg_status assertEqual len pg_status assertEqual str pg_status last_enqueued_collective assertEqual str pg_status last_completed_collective assertEqual str pg_status last_started_collective - global_ranks = pg_config ranks assertEqual len json loads global_ranks world_size assertEqual len t entries t = t entries last = t - assertEqual last process_group No event recorded Gloo assertEqual last state scheduled we don t collect stack traces JSON moment is_json assertIn test_c d_gloo py str last frames assertEqual last input_sizes assertEqual last input_dtypes Float assertEqual last output_sizes assertEqual last output_dtypes Float assertEqual last collective_seq_id TODO Needs verification assertEqual last timeout_ms assertTrue duration_ms last requires_gloo test_short_json store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts group_name= = torch full float rank _ range f = pg allreduce f wait time sleep t = json loads torch _C _distributed_c d _dump_fr_trace_json includeCollectives=True _verify_trace t True requires_gloo test_short_pickle store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts group_name= = torch full float rank _ range f = pg allreduce f wait time sleep t = pickle loads torch _C _distributed_c d _dump_fr_trace includeCollectives=True _verify_trace t is_json=False requires_gloo test_long store = c d FileStore file_name world_size pg = _create_process_group_gloo store rank world_size opts group_name= = torch full float rank _ range test some other primitives make sure their strings valid xs = torch ones pg broadcast xs wait pg allreduce xs wait pg reduce xs wait ys = torch empty _ range world_size pg allgather ys xs wait pg reduce_scatter xs ys wait f = pg allreduce f wait t = pickle loads torch _C _distributed_c d _dump_fr_trace t = t entries assertEqual len t first = t last = t - assertEqual last profiling_name gloo all_reduce assertEqual last state scheduled assertIn test_c d_gloo py str last frames assertEqual last input_sizes assertEqual last input_dtypes Float assertEqual last output_sizes assertEqual last output_dtypes Float assertEqual last timeout_ms assertEqual last collective_seq_id - first collective_seq_id CommTest test_c d_common AbstractCommTest MultiProcessTestCase property device cpu setUp super setUp _spawn_processes tearDown super tearDown try os remove file_name except OSError pass _test_broadcast_coalesced process_group device root_rank half = torch float No support float CPU tensors device == torch device cpu half = torch float target = torch arange dtype=half device=device chunk target += torch arange dtype=torch float device=device chunk target += torch arange dtype=half device=device chunk target += torch arange dtype=torch float device=device chunk target += torch arange dtype=half device=device chunk target += torch arange dtype=torch float device=device chunk The tensors pass broadcast identical target only process root broadcast rank == root_rank tensors = tensor clone tensor target tensors = torch zeros_like tensor tensor target rank = root_rank assertNotEqual tensors target c d _broadcast_coalesced process_group tensors buffer_size= src=root_rank rank = root_rank assertEqual tensors target requires_gloo skip_if_lt_x_gpu test_broadcast_coalesced_gloo_cuda store = c d FileStore file_name world_size c d init_process_group backend= gloo store=store rank=self rank world_size=self world_size process_group = c d distributed_c d _get_default_group device = torch device f cuda rank d backend = process_group _get_backend device backend create_device interface=LOOPBACK ranks = list range world_size root_rank ranks _test_broadcast_coalesced process_group device root_rank requires_gloo test_broadcast_coalesced_gloo_cpu store = c d FileStore file_name world_size c d init_process_group backend= gloo store=store rank=self rank world_size=self world_size process_group = c d distributed_c d _get_default_group device = torch device cpu backend = process_group _get_backend device backend create_device interface=LOOPBACK ranks = list range world_size root_rank ranks _test_broadcast_coalesced process_group device root_rank requires_gloo skip_if_lt_x_gpu test_sequence_num_set_default_pg_gloo _test_sequence_num_set_default_pg backend= gloo requires_gloo skip_if_lt_x_gpu test_sequence_num_set_gloo_new_group _test_sequence_num_set_new_group backend= gloo skip_if_lt_x_gpu requires_gloo test_sequence_num_incremented_gloo_default _test_sequence_num_incremented_default_group gloo skip_if_lt_x_gpu requires_gloo test_sequence_num_incremented_gloo_subgroup world_size skip_but_pass_in_sandcastle Test requires world_size least _test_sequence_num_incremented_subgroup gloo skip_if_lt_x_gpu requires_gloo test_gloo_warn_not_in_group _test_warn_not_in_group backend= gloo skip_if_lt_x_gpu requires_gloo test_gloo_rank_membership _test_rank_membership backend= gloo skip_if_lt_x_gpu requires_gloo test_tensor_dtype_mismatch _test_tensor_dtype_mismatch backend= gloo skip_if_lt_x_gpu requires_gloo test_tensor_dtype_complex _test_tensor_dtype_complex backend= gloo requires_gloo test_bool_tensors _test_bool_tensors backend= gloo GlooProcessGroupWithDispatchedCollectivesTests test_c d_common ProcessGroupWithDispatchedCollectivesTests requires_gloo test_collectives _test_collectives backend= gloo requires_gloo test_allreduce_coalesced _test_allreduce_coalesced backend= gloo requires_gloo test_all_to_all_single _test_all_to_all_single backend= gloo requires_gloo test_allgather_coalesced store = dist FileStore file_name world_size dist init_process_group gloo world_size=self world_size rank=self rank store=store input_tensor = torch ones dtype=torch float output_tensor_list = torch zeros_like input_tensor dist all_gather_coalesced output_tensor_list input_tensor assertEqual output_tensor_list input_tensor requires_gloo test_monitored_barrier store = dist FileStore file_name world_size dist init_process_group gloo world_size=self world_size rank=self rank store=store dist monitored_barrier LargeCommTest test_c d_common AbstractLargeCommTest MultiProcessTestCase setUp super setUp _spawn_processes tearDown super tearDown try os remove file_name except OSError pass property device torch device cpu requires_gloo test_new_group_local_sync _test_new_group_local_sync backend= gloo requires_gloo test_new_group_local_sync_sanity_check _test_new_group_local_sync_sanity_check backend= gloo requires_gloo test_new_group_local_sync_duplicate_pg _test_new_group_local_sync_duplicate_pg backend= gloo __name__ == __main__ assert torch cuda _initialized test_distributed must have initialized CUDA context main process run_tests