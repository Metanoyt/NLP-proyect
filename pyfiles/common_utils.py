mypy allow-untyped-defs r Importing file must initialize CUDA context test_distributed relies assumption properly run This means when imported no CUDA calls shall made including torch cuda device_count etc torch testing _internal common_cuda py can freely initialize CUDA context when imported argparse contextlib copy ctypes errno functools gc hashlib inspect io json logging math operator os pathlib platform random re shutil signal socket subprocess sys tempfile threading time types unittest warnings collections abc Mapping Sequence contextlib closing contextmanager copy deepcopy dataclasses dataclass enum Enum functools partial wraps itertools product chain pathlib Path statistics mean typing Any Optional TypeVar Union collections abc Callable collections abc Iterable Iterator unittest mock MagicMock expecttest numpy np __main__ type ignore torch torch backends cudnn torch backends mkl torch backends mps torch backends xnnpack torch cuda torch Tensor torch _C ScriptDict ScriptList type ignore attr-defined torch _utils_internal get_writable_path torch _logging scribe open_source_signpost torch nn ModuleDict ModuleList ParameterDict ParameterList Sequential torch onnx register_custom_op_symbolic unregister_custom_op_symbolic torch testing make_tensor torch testing _comparison BooleanPair NonePair NumberPair Pair TensorLikePair torch testing _comparison not_close_error_metas torch testing _internal common_dtype get_all_dtypes torch utils _import_utils _check_module_exists torch utils _pytree pytree torch utils cpp_extension try pytest type ignore import-not-found has_pytest = True except ImportError has_pytest = False SEED = MI _ARCH = gfx MI _ARCH = gfx MI _ARCH = gfx NAVI_ARCH = gfx gfx gfx gfx gfx NAVI _ARCH = gfx gfx NAVI _ARCH = gfx gfx ProfilingMode Enum LEGACY = SIMPLE = PROFILING = Set parse_cmd_line_args called CI_FUNCTORCH_ROOT = CI_PT_ROOT = CI_TEST_PREFIX = DISABLED_TESTS_FILE = GRAPH_EXECUTOR Optional ProfilingMode = None LOG_SUFFIX = PYTEST_SINGLE_TEST = REPEAT_COUNT = RERUN_DISABLED_TESTS = False RUN_PARALLEL = SHOWLOCALS = False SLOW_TESTS_FILE = TEST_BAILOUTS = False TEST_DISCOVER = False TEST_IN_SUBPROCESS = False TEST_SAVE_XML = UNITTEST_ARGS list str = USE_PYTEST = False is_navi _arch torch cuda is_available prop = torch cuda get_device_properties gfx_arch = prop gcnArchName split gfx_arch NAVI _ARCH True False freeze_rng_state args kwargs torch testing _utils freeze_rng_state args kwargs Class keep track test flags configurable environment variables Flags set here intended read-only should modified after definition TODO Expand handle arbitrary settings addition boolean flags TestEnvironment Set env vars set repro command output test failure Specifically includes env vars set non-default values implied Maps env var name - value int repro_env_vars dict = Defines flag usable throughout test suite determining its value querying specified environment variable Args name str The name flag A global variable name will set convenient access throughout test suite env_var str The name primary environment variable which determine value flag If None environment variable unset default value will used unless otherwise implied see implied_by_fn Default None default bool The default value use flag unset environment variable unimplied Default False include_in_repro bool Indicates whether flag should included repro command output test failure i e whether possibly relevant reproducing test failure Default True enabled_fn Callable Callable returning whether flag should enabled given environment variable value default value Default Lambda requiring disable default OR enable off default implied_by_fn Callable Thunk returning bool imply flag enabled something outside its primary environment variable setting For example can useful value another environment variable implies flag enabled Default Lambda returning False indicate no implications staticmethod def_flag name env_var=None default=False include_in_repro=True enabled_fn=lambda env_var_val default env_var_val = default env_var_val == implied_by_fn=lambda False enabled = default env_var_val = None env_var None env_var_val = os getenv env_var enabled = enabled_fn env_var_val default implied = implied_by_fn enabled = enabled implied include_in_repro env_var None enabled = default implied TestEnvironment repro_env_vars env_var = env_var_val export flag globally convenience assert name globals f duplicate definition flag name globals name = enabled enabled Defines setting usable throughout test suite determining its value querying specified environment variable This differs flag s restricted boolean value Args name str The name setting A global variable name will set convenient access throughout test suite env_var str The name primary environment variable which determine value setting If None environment variable unset default value will used Default None default Any The default value use setting unset environment variable Default None include_in_repro bool Indicates whether setting should included repro command output test failure i e whether possibly relevant reproducing test failure Default True parse_fn Callable Callable parsing env var string Default value just uses string itself staticmethod def_setting name env_var=None default=None include_in_repro=True parse_fn=lambda maybe_val_str maybe_val_str value = default env_var None os getenv env_var value = parse_fn value include_in_repro value = default TestEnvironment repro_env_vars env_var = value export setting globally convenience assert name globals f duplicate definition setting name globals name = value value Returns string prefix usable set environment variables any test settings should explicitly set match instantiation test suite Example PYTORCH_TEST_WITH_ASAN= PYTORCH_TEST_WITH_ROCM= staticmethod repro_env_var_prefix - str join f env_var = value env_var value TestEnvironment repro_env_vars items log = logging getLogger __name__ torch backends disable_global_flags FILE_SCHEMA = file sys platform == win FILE_SCHEMA = file NB This flag differs semantically others setting env var any non-empty value will cause true CI= CI= true CI= etc all set flag true CI= unset CI set flag false GitHub sets value CI= true enable IS_CI bool = TestEnvironment def_flag IS_CI env_var= CI include_in_repro=False enabled_fn=lambda env_var_value _ bool env_var_value IS_SANDCASTLE bool = TestEnvironment def_flag IS_SANDCASTLE env_var= SANDCASTLE implied_by_fn=lambda os getenv TW_JOB_USER == sandcastle include_in_repro=False IN_RE_WORKER bool = os environ get INSIDE_RE_WORKER None _is_fbcode_default = hasattr torch _utils_internal IS_FBSOURCE torch _utils_internal IS_FBSOURCE IS_FBCODE bool = TestEnvironment def_flag IS_FBCODE env_var= PYTORCH_TEST_FBCODE default=_is_fbcode_default include_in_repro=False IS_REMOTE_GPU bool = TestEnvironment def_flag IS_REMOTE_GPU env_var= PYTORCH_TEST_REMOTE_GPU include_in_repro=False DISABLE_RUNNING_SCRIPT_CHK bool = TestEnvironment def_flag DISABLE_RUNNING_SCRIPT_CHK env_var= PYTORCH_DISABLE_RUNNING_SCRIPT_CHK include_in_repro=False NB enabled default unless fbcode context PRINT_REPRO_ON_FAILURE bool = TestEnvironment def_flag PRINT_REPRO_ON_FAILURE env_var= PYTORCH_PRINT_REPRO_ON_FAILURE default= IS_FBCODE include_in_repro=False possibly restrict OpInfo tests single sample input OPINFO_SAMPLE_INPUT_INDEX Optional int = TestEnvironment def_setting OPINFO_SAMPLE_INPUT_INDEX env_var= PYTORCH_OPINFO_SAMPLE_INPUT_INDEX default=None Don t include env var value repro command because info will queried tracked sample input instead include_in_repro=False parse_fn=lambda val None val None int val DEFAULT_DISABLED_TESTS_FILE = pytorch-disabled-tests json DEFAULT_SLOW_TESTS_FILE = slow_tests json disabled_tests_dict = slow_tests_dict = maybe_load_json filename os path isfile filename open filename fp json load fp log warning Attempted load json file s does exist filename set them here case tests running subprocess doesn t call run_tests os getenv SLOW_TESTS_FILE slow_tests_dict = maybe_load_json os getenv SLOW_TESTS_FILE os getenv DISABLED_TESTS_FILE disabled_tests_dict = maybe_load_json os getenv DISABLED_TESTS_FILE NATIVE_DEVICES = cpu cuda xpu meta mps torch _C _get_privateuse _backend_name used managing devices testing torch profiler UTs now cpu cuda xpu added testing torch profiler UTs DEVICE_LIST_SUPPORT_PROFILING_TEST = cpu cuda xpu ALLOW_XPU_PROFILING_TEST = True check_names = orin concord galen xavier nano jetson tegra thor IS_JETSON = any name platform platform name check_names gcIfJetson fn Irregular Jetson host device memory setup requires cleanup avoid tests being killed functools wraps fn wrapper args kwargs IS_JETSON gc collect torch cuda empty_cache fn args kwargs wrapper Tries extract current test function crawling stack If unsuccessful None extract_test_fn - Optional Callable try stack = inspect stack frame_info stack frame = frame_info frame frame f_locals continue self_val = frame f_locals isinstance self_val unittest TestCase test_id = self_val id _ cls_name test_name = test_id rsplit cls_name == type self_val __name__ test_name startswith test test_fn = getattr self_val test_name __func__ test_fn except Exception pass None Contains tracked input data useful debugging purposes dataclass TrackedInput index int val Any type_desc str Attempt pull out tracked input information test function A TrackedInputIter used insert information get_tracked_input - Optional TrackedInput test_fn = extract_test_fn test_fn None None getattr test_fn tracked_input None clear_tracked_input - None test_fn = extract_test_fn test_fn None hasattr test_fn tracked_input test_fn tracked_input = None type ignore attr-defined Wraps iterator tracks most recent value iterator produces debugging purposes Tracked values stored test function TrackedInputIter __init__ child_iter input_type_desc item_callback=None track_callback=None set_seed=True restrict_to_index=None child_iter = enumerate child_iter Input type describes things we re tracking e g sample input error input input_type_desc = input_type_desc NB The two types callbacks below exist because thing we want track isn t always same thing we want returned iterator An example ErrorInput which we want returned iterator which contains SampleInput we want track Item callback run each iterated thing index get thing item_callback = item_callback item_callback None item_callback = lambda x i x Track callback run each iterated thing get thing track track_callback = track_callback track_callback None track_callback = lambda x x test_fn = extract_test_fn Indicates whether random seed should set before each call iterator set_seed = set_seed Indicates iteration should restricted only provided index If None no restriction done restrict_to_index = restrict_to_index __iter__ __next__ while True set_seed use test-name-specific hash seed possible seed = int from_bytes hashlib sha test_fn __qualname__ encode utf- digest little test_fn None SEED set_rng_seed seed allow StopIteration bubble up input_idx input_val = next child_iter restrict_to_index None input_idx == restrict_to_index break _set_tracked_input TrackedInput index=input_idx val=self track_callback input_val type_desc=self input_type_desc item_callback input_val input_idx _set_tracked_input tracked_input TrackedInput test_fn None hasattr test_fn tracked_input test_fn tracked_input = tracked_input type ignore attr-defined _TestParametrizer Decorator parametrizing test function yielding set new tests spawned original generic test each specialized specific set test inputs For example parametrizing test across set ops will result test function per op The decision how parametrize what parametrize over intended implemented each derived In details decorator adds parametrize_fn property test function This function intended called later one Device-specific test instantiation via instantiate_device_type_tests Note case there no need explicitly parametrize over device type handled separately Device-agnostic parametrized test instantiation via instantiate_parametrized_tests If decorator applied test function already has parametrize_fn property new composite parametrize_fn will created generates tests product parameters generated old new parametrize_fns This allows convenient composability decorators _parametrize_test test generic_cls device_cls Parametrizes given test function across whatever dimension specified derived Tests can parametrized over any arbitrary dimension combination dimensions such all ops all modules all ops + their associated dtypes Args test fn Test function parametrize over generic_cls Generic test object containing tests e g TestFoo device_cls Device-specialized test object e g TestFooCPU set None tests part device-specific set Returns Generator object returning -tuples test fn Parametrized test function must support device arg args any params test_name str Parametrized suffix test e g opname_int will appended base name test param_kwargs dict Param kwargs pass test e g op add dtype torch int decorator_fn callable Callable Dict List list decorators apply given param_kwargs raise NotImplementedError __call__ fn hasattr fn parametrize_fn Do composition product args old_parametrize_fn = fn parametrize_fn new_parametrize_fn = _parametrize_test fn parametrize_fn = compose_parametrize_fns old_parametrize_fn new_parametrize_fn fn parametrize_fn = _parametrize_test fn compose_parametrize_fns old_parametrize_fn new_parametrize_fn Returns parametrize_fn parametrizes over product parameters handled given parametrize_fns Each given parametrize_fn should each have signature f test generic_cls device_cls The test names will combination names produced parametrize_fns new_name _ old_name order This order done match intuition constructed names when composing multiple decorators names will built top bottom order when stacking parametrization decorators Args old_parametrize_fn callable - First parametrize_fn compose new_parametrize_fn callable - Second parametrize_fn compose composite_fn test generic_cls device_cls old_parametrize_fn=old_parametrize_fn new_parametrize_fn=new_parametrize_fn old_tests = list old_parametrize_fn test generic_cls device_cls old_test old_test_name old_param_kwargs old_dec_fn old_tests new_test new_test_name new_param_kwargs new_dec_fn \ new_parametrize_fn old_test generic_cls device_cls redundant_params = set old_param_kwargs keys intersection new_param_kwargs keys redundant_params raise RuntimeError Parametrization over same parameter multiple parametrization f decorators supported For test test __name__ following parameters f handled multiple times redundant_params full_param_kwargs = old_param_kwargs new_param_kwargs merged_test_name = format new_test_name _ old_test_name = new_test_name = old_test_name merged_decorator_fn param_kwargs old_dec_fn=old_dec_fn new_dec_fn=new_dec_fn list old_dec_fn param_kwargs + list new_dec_fn param_kwargs yield new_test merged_test_name full_param_kwargs merged_decorator_fn composite_fn instantiate_parametrized_tests generic_cls Instantiates tests have been decorated parametrize_fn This generally performed decorator subclass _TestParametrizer The generic test will replaced test parametrized tests specialized names This should used instead instantiate_device_type_tests test contains device-agnostic tests You can also use decorator E g ` ` ` instantiate_parametrized_tests TestFoo TestCase ` ` ` Args generic_cls Generic test object containing tests e g TestFoo attr_name tuple dir generic_cls class_attr = getattr generic_cls attr_name hasattr class_attr parametrize_fn continue Remove generic test test delattr generic_cls attr_name Add parametrized tests test instantiate_test_helper cls name test param_kwargs wraps test instantiated_test param_kwargs=param_kwargs test param_kwargs assert hasattr generic_cls name f Redefinition test name setattr generic_cls name instantiated_test test test_suffix param_kwargs decorator_fn class_attr parametrize_fn class_attr generic_cls=generic_cls device_cls=None full_name = f test __name__ _ test_suffix Apply decorators based full param kwargs decorator decorator_fn param_kwargs test = decorator test instantiate_test_helper cls=generic_cls name=full_name test=test param_kwargs=param_kwargs generic_cls subtest Explicit subtest case use test parametrization Allows explicit naming individual subtest cases well applying decorators parametrized test Args arg_values iterable Iterable arg values e g range tuples arg values e g name str Optional name use test decorators iterable Iterable decorators apply generated test __slots__ = arg_values name decorators __init__ arg_values name=None decorators=None arg_values = arg_values name = name decorators = decorators decorators parametrize _TestParametrizer Decorator applying generic test parametrizations The interface decorator modeled after ` pytest mark parametrize ` Basic usage between decorator pytest s identical The first argument should string containing comma-separated names parameters test second argument should iterable returning values tuples values case multiple parameters Beyond basic usage decorator provides some additional functionality pytest does Parametrized tests end up generated test functions unittest test classes Since differs how pytest works decorator takes additional responsibility naming these test functions The default test names consists test s base name followed each parameter name + value e g test_bar_x_ _y_foo custom names can defined using ` name_fn ` ` subtest ` structure see below The decorator specially handles parameter values type ` subtest ` which allows more fine-grained control over both test naming test execution In particular can used tag subtests explicit test names apply arbitrary decorators see examples below Examples parametrize x range test_foo x parametrize x y foo bar baz test_bar x y parametrize x y foo bar baz name_fn=lambda x y _ format x y test_bar_custom_names x y parametrize x y subtest name= double subtest name= triple decorators= unittest expectedFailure subtest name= quadruple test_baz x y To actually instantiate parametrized tests one instantiate_parametrized_tests instantiate_device_type_tests should called The former intended test classes contain device-agnostic tests while latter should used test classes contain device-specific tests Both support arbitrary parametrizations using decorator Args arg_str str String arg names separate commas e g x y arg_values iterable Iterable arg values e g range tuples arg values e g name_fn Callable Optional function takes parameters returns subtest name __init__ arg_str arg_values name_fn=None arg_names list str = s strip s arg_str split s = arg_values = arg_values name_fn = name_fn _formatted_str_repr idx name value Returns string representation given arg suitable use test function names isinstance value torch dtype dtype_name value isinstance value torch device str value Can t use isinstance would cause circular type value __name__ OpInfo ModuleInfo value formatted_name isinstance value int float str f name _ str value replace _ f name idx _default_subtest_name idx values _ join _formatted_str_repr idx v v zip arg_names values strict=True _get_subtest_name idx values explicit_name=None explicit_name subtest_name = explicit_name name_fn subtest_name = name_fn values subtest_name = _default_subtest_name idx values subtest_name _parametrize_test test generic_cls device_cls len arg_names == No additional parameters needed test test_name = yield test test_name lambda _ Each values item expected either A tuple values one each arg For single arg single item expected A subtest instance arg_values matching previous values = check_exhausted_iterator = object idx values enumerate arg_values maybe_name = None decorators list Any = isinstance values subtest sub = values values = sub arg_values maybe_name = sub name wraps test test_wrapper args kwargs test args kwargs decorators = sub decorators gen_test = test_wrapper gen_test = test values = list values len arg_names values type ignore call-overload len values = len arg_names raise RuntimeError f Expected values == arg names got len values f values len arg_names names test test __name__ param_kwargs = dict zip arg_names values strict=True test_name = _get_subtest_name idx values explicit_name=maybe_name decorator_fn _ decorators=decorators decorators yield gen_test test_name param_kwargs decorator_fn values check_exhausted_iterator raise ValueError f test An empty arg_values passed parametrize Note may result reuse generator reparametrize _TestParametrizer Decorator adjusting way existing parametrizer operates This runs given adapter_fn each parametrization produced given parametrizer allowing on-the-fly parametrization more flexible than default product-based composition occurs when stacking parametrization decorators If adapter_fn returns None given test parametrization parametrization will excluded Otherwise s expected adapter_fn returns iterable modified parametrizations tweaked test names parameter kwargs Examples include_is_even_arg test_name param_kwargs x = param_kwargs x is_even = x == new_param_kwargs = dict param_kwargs new_param_kwargs is_even = is_even is_even_suffix = _even is_even _odd new_test_name = f test_name is_even_suffix yield new_test_name new_param_kwargs reparametrize parametrize x range include_is_even_arg test_foo x is_even exclude_odds test_name param_kwargs x = param_kwargs x is_even = x == yield None is_even test_name param_kwargs reparametrize parametrize x range exclude_odds test_bar x __init__ parametrizer adapter_fn parametrizer = parametrizer adapter_fn = adapter_fn _parametrize_test test generic_cls device_cls gen_test test_name param_kwargs decorator_fn \ parametrizer _parametrize_test test generic_cls device_cls adapted = adapter_fn test_name param_kwargs adapted None adapted_item adapted adapted_item None new_test_name new_param_kwargs = adapted_item yield gen_test new_test_name new_param_kwargs decorator_fn decorateIf _TestParametrizer Decorator applying parameter-specific conditional decoration Composes other test parametrizers e g modules ops parametrize etc Examples decorateIf unittest skip lambda params params x == parametrize x range test_foo x parametrize x y foo bar baz decorateIf unittest expectedFailure lambda params params x == params y == baz test_bar x y decorateIf unittest expectedFailure lambda params params op name == add params dtype == torch float ops op_db test_op_foo device dtype op decorateIf unittest skip lambda params params module_info module_cls torch nn Linear \ params device == cpu modules module_db test_module_foo device dtype module_info Args decorator Test decorator apply predicate satisfied predicate_fn Callable Function taking dict params returning boolean indicating whether decorator should applied __init__ decorator predicate_fn decorator = decorator predicate_fn = predicate_fn _parametrize_test test generic_cls device_cls Leave test as-is appropriate decorator_fn decorator_fn params decorator=self decorator predicate_fn=self predicate_fn predicate_fn params decorator wraps test test_wrapper args kwargs test args kwargs test_name = yield test_wrapper test_name decorator_fn cppProfilingFlagsToProfilingMode old_prof_exec_state = torch _C _jit_set_profiling_executor True old_prof_mode_state = torch _C _get_graph_executor_optimize True torch _C _jit_set_profiling_executor old_prof_exec_state torch _C _get_graph_executor_optimize old_prof_mode_state old_prof_exec_state old_prof_mode_state ProfilingMode PROFILING ProfilingMode SIMPLE ProfilingMode LEGACY contextmanager enable_profiling_mode_for_profiling_tests old_prof_exec_state = False old_prof_mode_state = False assert GRAPH_EXECUTOR GRAPH_EXECUTOR == ProfilingMode PROFILING old_prof_exec_state = torch _C _jit_set_profiling_executor True old_prof_mode_state = torch _C _get_graph_executor_optimize True try yield finally GRAPH_EXECUTOR == ProfilingMode PROFILING torch _C _jit_set_profiling_executor old_prof_exec_state torch _C _get_graph_executor_optimize old_prof_mode_state contextmanager enable_profiling_mode old_prof_exec_state = torch _C _jit_set_profiling_executor True old_prof_mode_state = torch _C _get_graph_executor_optimize True try yield finally torch _C _jit_set_profiling_executor old_prof_exec_state torch _C _get_graph_executor_optimize old_prof_mode_state contextmanager num_profiled_runs num_runs old_num_runs = torch _C _jit_set_num_profiled_runs num_runs try yield finally torch _C _jit_set_num_profiled_runs old_num_runs func_call = torch _C ScriptFunction __call__ meth_call = torch _C ScriptMethod __call__ prof_callable callable args kwargs profile_and_replay kwargs del kwargs profile_and_replay assert GRAPH_EXECUTOR GRAPH_EXECUTOR == ProfilingMode PROFILING enable_profiling_mode_for_profiling_tests callable args kwargs callable args kwargs callable args kwargs raise_on_run_directly file_to_call raise RuntimeError This test file meant run directly f use \n\n\tpython file_to_call TESTNAME\n\n instead prof_func_call args kwargs prof_callable func_call args kwargs prof_meth_call args kwargs prof_callable meth_call args kwargs torch _C ScriptFunction __call__ = prof_func_call type ignore method-assign torch _C ScriptMethod __call__ = prof_meth_call type ignore method-assign _get_test_report_path allow users override test file location We need because distributed tests run same test file multiple times different configurations override = os environ get TEST_REPORT_SOURCE_OVERRIDE test_source = override override None python-unittest os path join test-reports test_source parse_cmd_line_args global CI_FUNCTORCH_ROOT global CI_PT_ROOT global CI_TEST_PREFIX global DISABLED_TESTS_FILE global GRAPH_EXECUTOR global LOG_SUFFIX global PYTEST_SINGLE_TEST global REPEAT_COUNT global RERUN_DISABLED_TESTS global RUN_PARALLEL global SHOWLOCALS global SLOW_TESTS_FILE global TEST_BAILOUTS global TEST_DISCOVER global TEST_IN_SUBPROCESS global TEST_SAVE_XML global UNITTEST_ARGS global USE_PYTEST is_running_via_run_test = run_test py getattr __main__ __file__ parser = argparse ArgumentParser add_help=not is_running_via_run_test allow_abbrev=False parser add_argument -- subprocess action= store_true help= whether run each test subprocess parser add_argument -- accept action= store_true parser add_argument -- jit-executor -- jit_executor type=str parser add_argument -- repeat type=int default= parser add_argument -- test-bailouts -- test_bailouts action= store_true parser add_argument -- use-pytest action= store_true parser add_argument -- save-xml nargs= type=str const=_get_test_report_path default=_get_test_report_path IS_CI None parser add_argument -- discover-tests action= store_true parser add_argument -- log-suffix type=str default= parser add_argument -- run-parallel type=int default= parser add_argument -- import-slow-tests type=str nargs= const=DEFAULT_SLOW_TESTS_FILE parser add_argument -- import-disabled-tests type=str nargs= const=DEFAULT_DISABLED_TESTS_FILE parser add_argument -- rerun-disabled-tests action= store_true parser add_argument -- pytest-single-test type=str nargs= parser add_argument -- showlocals action=argparse BooleanOptionalAction default=False Only run when -h -- help flag active display both unittest parser help messages run_unittest_help argv unittest main argv=argv -h sys argv -- help sys argv help_thread = threading Thread target=run_unittest_help args= sys argv help_thread start help_thread join args remaining = parser parse_known_args args jit_executor == legacy GRAPH_EXECUTOR = ProfilingMode LEGACY args jit_executor == profiling GRAPH_EXECUTOR = ProfilingMode PROFILING args jit_executor == simple GRAPH_EXECUTOR = ProfilingMode SIMPLE infer flags based default settings GRAPH_EXECUTOR = cppProfilingFlagsToProfilingMode RERUN_DISABLED_TESTS = args rerun_disabled_tests SLOW_TESTS_FILE = args import_slow_tests DISABLED_TESTS_FILE = args import_disabled_tests LOG_SUFFIX = args log_suffix RUN_PARALLEL = args run_parallel TEST_BAILOUTS = args test_bailouts USE_PYTEST = args use_pytest PYTEST_SINGLE_TEST = args pytest_single_test TEST_DISCOVER = args discover_tests TEST_IN_SUBPROCESS = args subprocess TEST_SAVE_XML = args save_xml REPEAT_COUNT = args repeat SHOWLOCALS = args showlocals getattr expecttest ACCEPT False expecttest ACCEPT = args accept UNITTEST_ARGS = sys argv + remaining set_rng_seed CI Prefix path used only CI environment CI_TEST_PREFIX = str Path os getcwd CI_PT_ROOT = str Path os getcwd parent CI_FUNCTORCH_ROOT = str os path join Path os getcwd parent functorch wait_for_process p timeout=None try p wait timeout=timeout except KeyboardInterrupt Give ` p ` chance handle KeyboardInterrupt Without ` pytest ` can t print errors collected so far upon KeyboardInterrupt exit_status = p wait timeout= exit_status None exit_status p kill raise except subprocess TimeoutExpired send SIGINT give pytest chance make xml p send_signal signal SIGINT exit_status = None try exit_status = p wait timeout= try handle case where p wait timeout= times out well otherwise wait call finally block can potentially hang except subprocess TimeoutExpired pass exit_status None exit_status p kill raise except noqa B E copied python core library p kill raise finally Always call p wait ensure exit p wait shell command cwd=None env=None stdout=None stderr=None timeout=None sys stdout flush sys stderr flush The following cool snippet copied Py core library subprocess call only ` except KeyboardInterrupt ` block added SIGINT handling In Py subprocess Popen doesn t context manager so we do ` p wait ` ` final ` block code portable https github com python cpython blob b c af fbe fb d cea f Lib subprocess py#L -L assert isinstance command str Command shell should list tuple tokens p = subprocess Popen command universal_newlines=True cwd=cwd env=env stdout=stdout stderr=stderr wait_for_process p timeout=timeout retry_shell command cwd=None env=None stdout=None stderr=None timeout=None retries= was_rerun=False - tuple int bool Returns exicode + whether rerun assert retries = f Expecting non negative number number retries got retries try exit_code = shell command cwd=cwd env=env stdout=stdout stderr=stderr timeout=timeout exit_code == retries == exit_code was_rerun print f Got exit code exit_code retrying retries left= retries file=stdout flush=True except subprocess TimeoutExpired retries == print f Command took timeout min returning file=stdout flush=True was_rerun print f Command took timeout min retrying retries left= retries file=stdout flush=True retry_shell command cwd=cwd env=env stdout=stdout stderr=stderr timeout=timeout retries=retries - was_rerun=True discover_test_cases_recursively suite_or_case isinstance suite_or_case unittest TestCase suite_or_case rc = element suite_or_case print element rc extend discover_test_cases_recursively element rc get_test_names test_cases join case id split - case test_cases _print_test_names suite = unittest TestLoader loadTestsFromModule __main__ test_cases = discover_test_cases_recursively suite name get_test_names test_cases print name chunk_list lst nchunks lst i nchunks i range nchunks sanitize filename e g distributed pipeline sync skip test_api py - distributed pipeline sync skip test_api sanitize_test_filename filename inspect getfile returns absolute path some CI jobs converting relative path needed filename startswith CI_TEST_PREFIX filename = filename len CI_TEST_PREFIX + strip_py = re sub r py$ filename re sub r strip_py lint_test_case_extension suite succeed = True test_case_or_suite suite test_case = test_case_or_suite isinstance test_case_or_suite unittest TestSuite first_test = test_case_or_suite _tests len test_case_or_suite _tests None first_test None isinstance first_test unittest TestSuite succeed lint_test_case_extension test_case_or_suite test_case = first_test test_case None isinstance test_case TestCase test_class = test_case id split split err = This test should extend torch testing _internal common_utils TestCase doesn t print f test_class - failed err succeed = False succeed get_report_path argv=None pytest=False argv None argv = UNITTEST_ARGS test_filename = sanitize_test_filename argv test_report_path = TEST_SAVE_XML + LOG_SUFFIX test_report_path = os path join test_report_path test_filename pytest test_report_path = test_report_path replace python-unittest python-pytest os makedirs test_report_path exist_ok=True test_report_path = os path join test_report_path f test_filename - os urandom hex xml test_report_path os makedirs test_report_path exist_ok=True test_report_path sanitize_pytest_xml xml_file str pytext xml different unittext xml function makes pytest xml more similar unittest xml consider somehow modifying XML logger conftest do instead xml etree ElementTree ET tree = ET parse xml_file testcase tree iter testcase full_classname = testcase attrib get classname full_classname None continue The test prefix optional regex_result = re search r ^ test\ P file \ P classname ^\ $ full_classname regex_result None continue classname = regex_result group classname file = regex_result group file replace testcase set classname classname testcase set file f file py tree write xml_file get_pytest_test_cases argv list str - list str TestCollectorPlugin __init__ - None tests list Any = pytest_collection_finish session item session items tests append session config cwd_relative_nodeid item nodeid test_collector_plugin = TestCollectorPlugin pytest pytest main arg arg argv arg = -vv + -- collect-only -qq -- use-main-module plugins= test_collector_plugin test_collector_plugin tests run_tests argv=None parse_cmd_line_args argv None argv = UNITTEST_ARGS test files SLOW_TESTS_FILE os path exists SLOW_TESTS_FILE open SLOW_TESTS_FILE fp global slow_tests_dict slow_tests_dict = json load fp use env vars so pytest-xdist subprocesses can still access them os environ SLOW_TESTS_FILE = SLOW_TESTS_FILE warnings warn f slow test file provided found SLOW_TESTS_FILE stacklevel= DISABLED_TESTS_FILE os path exists DISABLED_TESTS_FILE open DISABLED_TESTS_FILE fp global disabled_tests_dict disabled_tests_dict = json load fp os environ DISABLED_TESTS_FILE = DISABLED_TESTS_FILE warnings warn f disabled test file provided found DISABLED_TESTS_FILE stacklevel= Determine test launch mechanism TEST_DISCOVER _print_test_names Before running tests lint check every test extends TestCase suite = unittest TestLoader loadTestsFromModule __main__ lint_test_case_extension suite sys exit SHOWLOCALS argv = argv -- showlocals -- tb=long -- color=yes USE_PYTEST -- locals argv TEST_IN_SUBPROCESS other_args = DISABLED_TESTS_FILE other_args append -- import-disabled-tests SLOW_TESTS_FILE other_args append -- import-slow-tests USE_PYTEST other_args append -- use-pytest RERUN_DISABLED_TESTS other_args append -- rerun-disabled-tests TEST_SAVE_XML other_args += -- save-xml TEST_SAVE_XML test_cases = get_pytest_test_cases argv USE_PYTEST case id split case discover_test_cases_recursively suite failed_tests = test_case_full_name test_cases cmd = sys executable + argv + other_args + argv + -- pytest-single-test USE_PYTEST + test_case_full_name string_cmd = join cmd timeout = None RERUN_DISABLED_TESTS exitcode _ = retry_shell cmd timeout=timeout retries= RERUN_DISABLED_TESTS exitcode = This sort hacky add relevant env variables distributed tests TestDistBackendWithSpawn test_case_full_name backend = os environ get BACKEND world_size = os environ get WORLD_SIZE env_prefix = f BACKEND= backend WORLD_SIZE= world_size string_cmd = env_prefix + + string_cmd Log command reproduce failure print f Test exited non-zero exitcode exitcode Command reproduce string_cmd failed_tests append test_case_full_name assert len failed_tests == unit test s failed \n\t format len failed_tests \n\t join failed_tests RUN_PARALLEL test_cases = discover_test_cases_recursively suite test_batches = chunk_list get_test_names test_cases RUN_PARALLEL processes = i range RUN_PARALLEL command = sys executable + argv + f -- log-suffix=-shard- i + + test_batches i processes append subprocess Popen command universal_newlines=True failed = False p processes failed &#124; = wait_for_process p = assert failed Some test shards have failed USE_PYTEST pytest_args = argv + -- use-main-module test_report_path = TEST_SAVE_XML test_report_path = get_report_path pytest=True print f Test results will stored test_report_path pytest_args append f -- junit-xml-reruns= test_report_path PYTEST_SINGLE_TEST pytest_args = PYTEST_SINGLE_TEST + pytest_args pytest os environ NO_COLOR = exit_code = pytest main args=pytest_args TEST_SAVE_XML sanitize_pytest_xml test_report_path exitcode means no tests found which happens since some test configs don t run tests certain files sys exit exit_code == exit_code TEST_SAVE_XML here so non-CI doesn t need xmlrunner installed xmlrunner type ignore xmlrunner result _XMLTestResult type ignore XMLTestResultVerbose _XMLTestResult Adding verbosity test outputs default test summary prints skip we want also print skip reason GH issue https github com pytorch pytorch issues This works unittest_xml_reporting = = latest moment __init__ args kwargs super __init__ args kwargs addSkip test reason super addSkip test reason c callback __closure__ isinstance c cell_contents str c cell_contents == skip message printed test summary stands ` verbose_str ` captured closure c cell_contents = f skip reason printErrors - None super printErrors printErrorList XPASS unexpectedSuccesses test_report_path = get_report_path verbose = -- verbose argv -v argv verbose print f Test results will stored test_report_path unittest main argv=argv testRunner=xmlrunner XMLTestRunner output=test_report_path verbosity= verbose resultclass=XMLTestResultVerbose REPEAT_COUNT _ range REPEAT_COUNT unittest main exit=False argv=argv result wasSuccessful sys exit - unittest main argv=argv IS_LINUX = sys platform == linux IS_WINDOWS = sys platform == win IS_MACOS = sys platform == darwin IS_PPC = platform machine == ppc le IS_X = platform machine x _ i IS_ARM = platform machine arm aarch IS_S X = platform machine == s x is_avx _vnni_supported sys platform = linux False open proc cpuinfo encoding= ascii f lines = f read vnni lines IS_AVX _VNNI_SUPPORTED = is_avx _vnni_supported IS_WINDOWS contextmanager TemporaryFileName args kwargs Ideally we would like have manually delete file NamedTemporaryFile opens file cannot opened multiple times Windows To support Windows close file after creation try remove manually delete kwargs kwargs delete False raise UserWarning only TemporaryFileName delete=False supported Windows kwargs delete = False f = tempfile NamedTemporaryFile args kwargs try f close yield f name finally os unlink f name contextmanager noqa T TemporaryFileName args kwargs tempfile NamedTemporaryFile args kwargs f yield f name IS_WINDOWS contextmanager TemporaryDirectoryName suffix=None On Windows directory created TemporaryDirectory likely removed prematurely so we first create directory using mkdtemp then remove manually try dir_name = tempfile mkdtemp suffix=suffix yield dir_name finally shutil rmtree dir_name contextmanager noqa T TemporaryDirectoryName suffix=None tempfile TemporaryDirectory suffix=suffix d yield d is_privateuse _backend_available privateuse _backend_name = torch _C _get_privateuse _backend_name privateuse _backend_module = getattr torch privateuse _backend_name None is_available = getattr privateuse _backend_module is_available None is_available IS_FILESYSTEM_UTF _ENCODING = sys getfilesystemencoding == utf- TEST_NUMPY = _check_module_exists numpy TEST_FAIRSEQ = _check_module_exists fairseq TEST_SCIPY = _check_module_exists scipy TEST_MKL = torch backends mkl is_available TEST_ACL = torch backends mkldnn is_available torch ops mkldnn _is_mkldnn_acl_supported TEST_MPS = torch backends mps is_available MACOS_VERSION = float join platform mac_ver split - TEST_XPU = torch xpu is_available TEST_HPU = bool hasattr torch hpu torch hpu is_available TEST_CUDA = torch cuda is_available custom_device_mod = getattr torch torch _C _get_privateuse _backend_name None TEST_PRIVATEUSE = is_privateuse _backend_available TEST_PRIVATEUSE _DEVICE_TYPE = torch _C _get_privateuse _backend_name TEST_NUMBA = _check_module_exists numba TEST_TRANSFORMERS = _check_module_exists transformers TEST_DILL = _check_module_exists dill TEST_LIBROSA = _check_module_exists librosa IS_ARM TEST_OPT_EINSUM = _check_module_exists opt_einsum TEST_Z = _check_module_exists z split_if_not_empty x str x split len x = NOTEST_CPU = cpu split_if_not_empty os getenv PYTORCH_TESTING_DEVICE_EXCEPT_FOR skipIfNoDill = unittest skipIf TEST_DILL no dill NO_MULTIPROCESSING_SPAWN bool = False TEST_WITH_ASAN bool = TestEnvironment def_flag TEST_WITH_ASAN env_var= PYTORCH_TEST_WITH_ASAN TEST_WITH_DEV_DBG_ASAN bool = TestEnvironment def_flag TEST_WITH_DEV_DBG_ASAN env_var= PYTORCH_TEST_WITH_DEV_DBG_ASAN TEST_WITH_TSAN bool = TestEnvironment def_flag TEST_WITH_TSAN env_var= PYTORCH_TEST_WITH_TSAN TEST_WITH_UBSAN bool = TestEnvironment def_flag TEST_WITH_UBSAN env_var= PYTORCH_TEST_WITH_UBSAN TEST_WITH_ROCM bool = TestEnvironment def_flag TEST_WITH_ROCM env_var= PYTORCH_TEST_WITH_ROCM TEST_WITH_MTIA bool = TestEnvironment def_flag TEST_WITH_MTIA env_var= PYTORCH_TEST_WITH_MTIA TODO Remove PYTORCH_MIOPEN_SUGGEST_NHWC once ROCm officially supports NHWC MIOpen See TEST_WITH_MIOPEN_SUGGEST_NHWC = os getenv PYTORCH_MIOPEN_SUGGEST_NHWC == Enables tests slow run disabled default TEST_WITH_SLOW bool = TestEnvironment def_flag TEST_WITH_SLOW env_var= PYTORCH_TEST_WITH_SLOW Disables non-slow tests these tests enabled default This usually used conjunction TEST_WITH_SLOW run only slow tests I could have done enum felt little awkward TEST_SKIP_FAST bool = TestEnvironment def_flag TEST_SKIP_FAST env_var= PYTORCH_TEST_SKIP_FAST Enables crossref tests addition standard tests which being run crossref tests work installing torch function mode runs extra compute alongside regular computation happens test After both computations done we cross-reference them thus name check correction before throwing out extra compute proceeding we had before By default we don t run these tests TEST_WITH_CROSSREF bool = TestEnvironment def_flag TEST_WITH_CROSSREF env_var= PYTORCH_TEST_WITH_CROSSREF TEST_SKIP_CUDAGRAPH bool = TestEnvironment def_flag TEST_SKIP_CUDAGRAPH env_var= PYTORCH_TEST_SKIP_CUDAGRAPH TEST_CUDA_GRAPH = TEST_CUDA TEST_SKIP_CUDAGRAPH torch version cuda torch version hip float join torch version hip split = TEST_CUDA_CUDSS = TEST_CUDA torch version cuda int torch version cuda split = TEST_CUDA_PYTHON_BINDINGS = _check_module_exists cuda bindings torch version cuda int torch version cuda split = TEST_CUDA_PYTHON_BINDINGS cuda_python_error_check function_call_output Makes calls cuda-python s cuda runtime functions more pythonic throwing exception they status which cudaSuccess cuda bindings type ignore error others = function_call_output error = cuda bindings runtime cudaError_t cudaSuccess raise ValueError f CUDA failure error tuple others cuda_python_error_check = None type ignore assignment allocator_option_enabled_fn allocator_config _ option allocator_config None False allocator_config = allocator_config split allocator_config allocator_config mapping = dict var split var allocator_config option mapping mapping option == True True False EXPANDABLE_SEGMENTS bool = TestEnvironment def_flag EXPANDABLE_SEGMENTS env_var= PYTORCH_CUDA_ALLOC_CONF enabled_fn=functools partial allocator_option_enabled_fn option= expandable_segments TEST_CUDA NUM_PARALLEL_PROCS os environ num_procs = int os getenv NUM_PARALLEL_PROCS gb_available = torch cuda mem_get_info other libraries take up about little under GB space per process torch cuda set_per_process_memory_fraction round gb_available - num_procs gb_available num_procs requires_cuda = unittest skipUnless torch cuda is_available Requires CUDA skipIfCrossRef fn wraps fn wrapper args kwargs TEST_WITH_CROSSREF raise unittest SkipTest test doesn t currently crossref fn args kwargs wrapper CrossRefMode torch overrides TorchFunctionMode __torch_function__ func types args= kwargs=None kwargs = kwargs r = func args kwargs r Run PyTorch tests TorchDynamo TEST_WITH_TORCHINDUCTOR bool = TestEnvironment def_flag TEST_WITH_TORCHINDUCTOR env_var= PYTORCH_TEST_WITH_INDUCTOR AOT_EAGER tested ci useful debugging TEST_WITH_AOT_EAGER bool = TestEnvironment def_flag TEST_WITH_AOT_EAGER env_var= PYTORCH_TEST_WITH_AOT_EAGER TEST_WITH_TORCHDYNAMO bool = TestEnvironment def_flag TEST_WITH_TORCHDYNAMO env_var= PYTORCH_TEST_WITH_DYNAMO implied_by_fn=lambda TEST_WITH_TORCHINDUCTOR TEST_WITH_AOT_EAGER TEST_WITHOUT_COMPILED_AUTOGRAD bool = TestEnvironment def_flag TEST_WITHOUT_COMPILED_AUTOGRAD env_var= PYTORCH_TEST_WITHOUT_COMPILED_AUTOGRAD TEST_WITH_TORCHDYNAMO torch _dynamo Do spend time helper functions called different inputs torch _dynamo config accumulated_recompile_limit = Do log compilation metrics unit tests torch _dynamo config log_compilation_metrics = False Silence guard performance warnings torch _dynamo config issue_ _ _ _warning = False TEST_WITH_TORCHINDUCTOR torch _inductor config torch _inductor config fallback_random = True only dynamo now torch _dynamo config compiled_autograd = TEST_WITHOUT_COMPILED_AUTOGRAD seems like only used test torch_np xpassIfTorchDynamo_np func numpy + causing issues TEST_WITH_TORCHDYNAMO np __version__ == unittest skip skipping numpy + dynamo-wrapped test func func TEST_WITH_TORCHDYNAMO unittest expectedFailure func xfailIfACL func unittest expectedFailure func TEST_ACL func xfailIfTorchDynamo func unittest expectedFailure func TEST_WITH_TORCHDYNAMO func xfailIfPy Plus func unittest expectedFailure func sys version_info = func xfailIfLinux func unittest expectedFailure func IS_LINUX TEST_WITH_ROCM IS_FBCODE func skipIfTorchDynamo msg= test doesn t currently work dynamo Usage skipIfTorchDynamo msg test_blah assert isinstance msg str Are you using skipIfTorchDynamo correctly decorator fn isinstance fn type wraps fn wrapper args kwargs TEST_WITH_TORCHDYNAMO raise unittest SkipTest msg fn args kwargs wrapper assert isinstance fn type TEST_WITH_TORCHDYNAMO fn __unittest_skip__ = True type ignore attr-defined fn __unittest_skip_why__ = msg type ignore attr-defined fn decorator skipIfTorchInductor msg= test doesn t currently work torchinductor condition=TEST_WITH_TORCHINDUCTOR decorator fn isinstance fn type wraps fn wrapper args kwargs condition raise unittest SkipTest msg fn args kwargs wrapper assert isinstance fn type condition fn __unittest_skip__ = True type ignore attr-defined fn __unittest_skip_why__ = msg type ignore attr-defined fn decorator runWithoutCompiledAutograd msg= test doesn t currently work compiled autograd Usage runWithoutCompiledAutograd msg test_blah assert isinstance msg str decorator func wraps func wrapper args kwargs torch _dynamo compiled_autograd _disable func args kwargs wrapper decorator serialTest condition=True Decorator running tests serially Requires pytest If one apply decorator directly condition will callable And test will essentially essentially skipped which undesirable assert type condition bool decorator fn has_pytest condition pytest mark serial fn fn decorator unMarkDynamoStrictTest cls=None decorator cls cls dynamo_strict = False cls cls None decorator decorator cls markDynamoStrictTest cls_or_func=None nopython=False Marks test strict In strict mode we reset before after test run without suppress errors Args - nopython we should run torch _dynamo optimize nopython= True False decorator cls_or_func inspect isclass cls_or_func cls_or_func dynamo_strict = True cls_or_func dynamo_strict_nopython = nopython cls_or_func fn = cls_or_func wraps fn wrapper args kwargs torch _dynamo reset unittest mock patch torch _dynamo config suppress_errors False fn args kwargs torch _dynamo reset wrapper cls_or_func None decorator decorator cls_or_func skipRocmIfTorchInductor msg= test doesn t currently work torchinductor ROCm stack skipIfTorchInductor msg=msg condition=TEST_WITH_ROCM TEST_WITH_TORCHINDUCTOR skipIfLegacyJitExecutor msg= test doesn t currently work legacy JIT executor decorator fn isinstance fn type wraps fn wrapper args kwargs assert GRAPH_EXECUTOR GRAPH_EXECUTOR == ProfilingMode LEGACY raise unittest SkipTest msg fn args kwargs wrapper assert isinstance fn type GRAPH_EXECUTOR == ProfilingMode LEGACY fn __unittest_skip__ = True type ignore attr-defined fn __unittest_skip_why__ = msg type ignore attr-defined fn decorator make_dynamo_test fn Optional Callable Any = None - Callable Any Decorator function create dynamo test case A function annotate decorator takes input unittest object torch _dynamo testing CompileCounter reset optimize_assert fn None lambda fn make_dynamo_test fn standard_test Any fn Callable Any kwargs - None dummy - None fn kwargs actual = CompileCounter dummy reset opt_fn = optimize_assert actual dummy opt_fn reset functools wraps fn test_fn Any kwargs - None standard_test fn=fn kwargs=kwargs test_fn Run PyTorch tests translation validation TEST_WITH_TV = os getenv PYTORCH_TEST_WITH_TV == TEST_WITH_TV torch fx experimental _config translation_validation = True Determine whether enable cuda memory leak check CUDA mem leak check expensive thus we don t want execute every test case configuration If True then CUDA memory leak checks skipped If false then CUDA memory leak checks performed See https github com pytorch pytorch pull #issuecomment- TEST_CUDA_MEM_LEAK_CHECK bool = TestEnvironment def_flag TEST_CUDA_MEM_LEAK_CHECK env_var= PYTORCH_TEST_CUDA_MEM_LEAK_CHECK Dict NumPy dtype - torch dtype when correspondence exists numpy_to_torch_dtype_dict = np bool_ torch bool np uint torch uint np uint torch uint np uint torch uint np uint torch uint np int torch int np int torch int np int torch int np int torch int np float torch float np float torch float np float torch float np complex torch complex np complex torch complex numpy dtypes like np float instances rather classes This leads rather absurd cases like np float = np dtype float np float == np dtype float type Especially when checking against reference we can t sure which variant we get so we simply try both numpy_to_torch_dtype np_dtype try numpy_to_torch_dtype_dict np_dtype except KeyError numpy_to_torch_dtype_dict np_dtype type has_corresponding_torch_dtype np_dtype try numpy_to_torch_dtype np_dtype True except KeyError False IS_WINDOWS Size ` np intc ` platform defined It returned functions like ` bitwise_not ` On Windows ` int ` -bit https docs microsoft com en-us cpp cpp data-type-ranges view=msvc- numpy_to_torch_dtype_dict np intc = torch int Dict torch dtype - NumPy dtype torch_to_numpy_dtype_dict = value key key value numpy_to_torch_dtype_dict items torch_to_numpy_dtype_dict update torch bfloat np float torch complex np complex skipIfNNModuleInlined msg= test doesn t currently work nn module inlining condition=torch _dynamo config inline_inbuilt_nn_modules decorator fn isinstance fn type wraps fn wrapper args kwargs condition raise unittest SkipTest msg fn args kwargs wrapper assert isinstance fn type condition fn __unittest_skip__ = True type ignore attr-defined fn __unittest_skip_why__ = msg type ignore attr-defined fn decorator skipIfRocm func=None msg= test doesn t currently work ROCm stack dec_fn fn reason = f skipIfRocm msg wraps fn wrapper args kwargs TEST_WITH_ROCM raise unittest SkipTest reason fn args kwargs wrapper func dec_fn func dec_fn getRocmArchName device_index int = torch cuda get_device_properties device_index gcnArchName isRocmArchAnyOf arch tuple str rocmArch = getRocmArchName any x rocmArch x arch skipIfRocmArch arch tuple str dec_fn fn wraps fn wrap_fn args kwargs TEST_WITH_ROCM isRocmArchAnyOf arch reason = f skipIfRocm test skipped arch raise unittest SkipTest reason fn args kwargs wrap_fn dec_fn runOnRocm fn wraps fn wrapper args kwargs TEST_WITH_ROCM fn args kwargs raise unittest SkipTest test currently only works ROCm stack wrapper runOnRocmArch arch tuple str dec_fn fn wraps fn wrap_fn args kwargs TEST_WITH_ROCM isRocmArchAnyOf arch reason = f skipIfRocm test only runs arch raise unittest SkipTest reason fn args kwargs wrap_fn dec_fn xfailIfS X func unittest expectedFailure func IS_S X func xfailIf condition wrapper func condition unittest expectedFailure func func wrapper skipIfXpu func=None msg= test doesn t currently work XPU stack dec_fn fn reason = f skipIfXpu msg wraps fn wrapper args kwargs TEST_XPU raise unittest SkipTest reason fn args kwargs wrapper func dec_fn func dec_fn skipIfMPS fn wraps fn wrapper args kwargs TEST_MPS raise unittest SkipTest test doesn t currently work MPS fn args kwargs wrapper skipIfHpu fn wraps fn wrapper args kwargs TEST_HPU raise unittest SkipTest test doesn t currently work HPU fn args kwargs wrapper getRocmVersion - tuple int int torch testing _internal common_cuda _get_torch_rocm_version rocm_version = _get_torch_rocm_version rocm_version rocm_version Skips test CUDA ROCm available its version lower than requested skipIfRocmVersionLessThan version=None dec_fn fn wraps fn wrap_fn args kwargs TEST_WITH_ROCM rocm_version_tuple = getRocmVersion rocm_version_tuple None version None rocm_version_tuple tuple version reason = f ROCm rocm_version_tuple available version required raise unittest SkipTest reason fn args kwargs wrap_fn dec_fn skipIfNotMiopenSuggestNHWC fn wraps fn wrapper args kwargs TEST_WITH_MIOPEN_SUGGEST_NHWC raise unittest SkipTest test doesn t currently work without MIOpen NHWC activation fn args kwargs wrapper skipIfWindows func=None msg= test doesn t currently work Windows stack dec_fn fn reason = f skipIfWindows msg wraps fn wrapper args kwargs IS_WINDOWS noqa F raise unittest SkipTest reason fn args kwargs wrapper func dec_fn func dec_fn skipIfWindowsXPU func=None msg= test doesn t currently work Windows stack dec_fn fn reason = f skipIfWindowsXPU msg wraps fn wrapper args kwargs IS_WINDOWS torch xpu is_available noqa F raise unittest SkipTest reason fn args kwargs wrapper func dec_fn func dec_fn requires_cuda_p p_access cuda_p p_access_available = torch cuda is_available torch cuda get_device_capability = torch cuda device_count = num_devices = torch cuda device_count i range num_devices - j range i + num_devices torch cuda can_device_access_peer i j cuda_p p_access_available = False break cuda_p p_access_available break skip_but_pass_in_sandcastle_if cuda_p p_access_available cuda p p access available Reverts linalg backend back default make sure potential failures one test do affect other tests setLinalgBackendsToDefaultFinally fn wraps fn _fn args kwargs _preferred_backend = torch backends cuda preferred_linalg_library try fn args kwargs finally torch backends cuda preferred_linalg_library _preferred_backend _fn Reverts blas backend back default make sure potential failures one test do affect other tests setBlasBackendsToDefaultFinally fn wraps fn _fn args kwargs _preferred_backend = torch backends cuda preferred_blas_library try fn args kwargs finally torch backends cuda preferred_blas_library _preferred_backend _fn Context manager setting deterministic flag automatically resetting its original value DeterministicGuard __init__ deterministic warn_only=False fill_uninitialized_memory=True deterministic = deterministic warn_only = warn_only fill_uninitialized_memory = fill_uninitialized_memory classmethod _current_state cls cls torch are_deterministic_algorithms_enabled warn_only=torch is_deterministic_algorithms_warn_only_enabled fill_uninitialized_memory=torch utils deterministic fill_uninitialized_memory type ignore attr-defined _update torch use_deterministic_algorithms deterministic warn_only=self warn_only torch utils deterministic fill_uninitialized_memory = fill_uninitialized_memory type ignore attr-defined __enter__ _restore = _current_state _update __exit__ exception_type exception_value traceback _restore _update AlwaysWarnTypedStorageRemoval __init__ always_warn assert isinstance always_warn bool always_warn = always_warn __enter__ always_warn_restore = torch storage _get_always_warn_typed_storage_removal torch storage _set_always_warn_typed_storage_removal always_warn __exit__ exception_type exception_value traceback torch storage _set_always_warn_typed_storage_removal always_warn_restore Context manager setting cuda sync debug mode reset original value we exposing core because sync debug mode global thus thread safe CudaSyncGuard __init__ sync_debug_mode mode = sync_debug_mode __enter__ debug_mode_restore = torch cuda get_sync_debug_mode torch cuda set_sync_debug_mode mode __exit__ exception_type exception_value traceback torch cuda set_sync_debug_mode debug_mode_restore Context manager setting torch __future__ set_swap_module_params_on_conversion automatically resetting its original value SwapTensorsGuard __init__ use_swap_tensors use_swap_tensors = use_swap_tensors __enter__ swap_tensors_restore = torch __future__ get_swap_module_params_on_conversion use_swap_tensors None torch __future__ set_swap_module_params_on_conversion use_swap_tensors __exit__ exception_type exception_value traceback torch __future__ set_swap_module_params_on_conversion swap_tensors_restore This decorator can used API tests call torch use_deterministic_algorithms When test finished will restore previous deterministic flag setting If CUDA = will set environment variable CUBLAS_WORKSPACE_CONFIG= so error associated setting thrown during test unless test changes variable purpose The previous CUBLAS_WORKSPACE_CONFIG setting will also restored once test finished Note test requires CUDA actually register changed CUBLAS_WORKSPACE_CONFIG variable new subprocess must created because CUDA only checks variable when runtime initializes Tests can run inside subprocess like so subprocess sys os script = Test code should go here try subprocess check_output sys executable -c script stderr=subprocess STDOUT cwd=os path dirname os path realpath __file__ env=os environ copy except subprocess CalledProcessError e error_message = e output decode utf- Handle exceptions raised subprocess here wrapDeterministicFlagAPITest fn wraps fn wrapper args kwargs DeterministicGuard torch are_deterministic_algorithms_enabled warn_only=torch is_deterministic_algorithms_warn_only_enabled CuBLASConfigGuard cublas_var_name = CUBLAS_WORKSPACE_CONFIG __enter__ cublas_config_restore = os environ get cublas_var_name os environ cublas_var_name = __exit__ exception_type exception_value traceback cur_cublas_config = os environ get cublas_var_name cublas_config_restore None cur_cublas_config None del os environ cublas_var_name os environ cublas_var_name = cublas_config_restore CuBLASConfigGuard fn args kwargs wrapper This decorator can used API tests want safely call torch __future__ set_swap_module_params_on_conversion ` swap ` can set True False None where None indicates context manager does set flag When test finished will restore previous swap flag setting wrapSwapTensorsTest swap=None dec_fn fn wraps fn wrapper args kwargs SwapTensorsGuard swap fn args kwargs wrapper dec_fn test parametrizer swapping swap _TestParametrizer __init__ swap_values super __init__ swap_values = swap_values _parametrize_test test generic_cls device_cls swap swap_values yield wrapSwapTensorsTest swap test f swap_ swap lambda _ skipIfCompiledWithoutNumpy fn Even numpy module present ` USE_NUMPY= ` used during build numpy tests will fail numpy_support = TEST_NUMPY numpy_support try The numpy module present verify PyTorch compiled numpy support torch from_numpy np array except RuntimeError numpy_support = False wraps fn wrapper args kwargs numpy_support raise unittest SkipTest PyTorch compiled without numpy support fn args kwargs wrapper _test_function fn device run_test_function fn device run_test_function skipIfNoXNNPACK fn wraps fn wrapper args kwargs torch backends xnnpack enabled type ignore attr-defined raise unittest SkipTest XNNPACK must enabled these tests Please build USE_XNNPACK= fn args kwargs wrapper skipIfNoLapack fn wraps fn wrapper args kwargs torch _C has_lapack raise unittest SkipTest PyTorch compiled without Lapack fn args kwargs wrapper skipIfNotRegistered op_name message Wraps decorator hide ` core ` Args op_name Check op registered ` core _REGISTERED_OPERATORS ` message message fail Usage skipIfNotRegistered MyOp MyOp linked This will check MyOp caffe python core unittest skip Pytorch compiled without Caffe skipIfNoSciPy fn wraps fn wrapper args kwargs TEST_SCIPY raise unittest SkipTest test require SciPy SciPy found fn args kwargs wrapper skip_if_pytest fn wraps fn wrapped args kwargs PYTEST_CURRENT_TEST os environ raise unittest SkipTest does work under pytest fn args kwargs wrapped skipIfNoXPU fn wraps fn wrapper args kwargs TEST_XPU raise unittest SkipTest test required PyTorched compiled XPU fn args kwargs wrapper slowTest fn wraps fn wrapper args kwargs TEST_WITH_SLOW raise unittest SkipTest test slow run PYTORCH_TEST_WITH_SLOW enable test fn args kwargs wrapper __dict__ slow_test = True wrapper slowTestIf condition slowTest condition lambda fn fn skipCUDAMemoryLeakCheckIf condition dec fn getattr fn _do_cuda_memory_leak_check True current True fn _do_cuda_memory_leak_check = condition fn dec skipCUDANonDefaultStreamIf condition dec fn getattr fn _do_cuda_non_default_stream True current True fn _do_cuda_non_default_stream = condition fn dec suppress_warnings fn wraps fn wrapper args kwargs warnings catch_warnings warnings simplefilter ignore fn args kwargs wrapper to_gpu obj type_map=None type_map None type_map = isinstance obj torch Tensor assert obj is_leaf t = type_map get obj dtype obj dtype torch no_grad res = obj dtype=t device= cuda copy=True res requires_grad = obj requires_grad res torch is_storage obj obj new resize_ obj size copy_ obj type ignore attr-defined union-attr isinstance obj list to_gpu o type_map o obj isinstance obj tuple tuple to_gpu o type_map o obj deepcopy obj get_function_arglist func inspect getfullargspec func args set_rng_seed seed=None seed None seed = SEED torch manual_seed seed random seed seed TEST_NUMPY np random seed seed contextlib contextmanager set_default_dtype dtype saved_dtype = torch get_default_dtype torch set_default_dtype dtype try yield finally torch set_default_dtype saved_dtype contextlib contextmanager set_default_tensor_type tensor_type saved_tensor_type = torch tensor type torch set_default_tensor_type tensor_type try yield finally torch set_default_tensor_type saved_tensor_type iter_indices tensor tensor dim == range tensor dim == range tensor size product range s s tensor size is_iterable obj try iter obj True except TypeError False is_iterable_of_tensors iterable include_empty=False Returns True iterable iterable tensors False o w If iterable empty value attr ` include_empty ` Tensor itself iterable so we check first isinstance iterable torch Tensor False try len iterable == include_empty t iter iterable isinstance t torch Tensor False except TypeError False True CudaNonDefaultStream __enter__ Before starting CUDA test save currently active streams all CUDA devices set new non default streams all CUDA devices ensure CUDA tests do use default stream mistake beforeDevice = torch cuda current_device beforeStreams = d range torch cuda device_count beforeStreams append torch cuda current_stream d deviceStream = torch cuda Stream device=d beforeStreams - synchronize torch _C _cuda_setStream stream_id=deviceStream stream_id device_index=deviceStream device_index device_type=deviceStream device_type torch _C _cuda_setDevice beforeDevice __exit__ exc_type exc_value traceback After completing CUDA test load previously active streams all CUDA devices beforeDevice = torch cuda current_device d range torch cuda device_count torch _C _cuda_setStream stream_id=self beforeStreams d stream_id device_index=self beforeStreams d device_index device_type=self beforeStreams d device_type torch _C _cuda_setDevice beforeDevice CudaMemoryLeakCheck __init__ testcase name=None name = testcase id name None name testcase = testcase initialize context RNG prevent false positive detections when test first initialize those torch testing _internal common_cuda initialize_cuda_context_rng initialize_cuda_context_rng Stores CUDA memory data provided PyTorch s caching allocator CUDA driver NOTE The undocumented torch cuda mem_get_info returns #free bytes #total bytes available GPU __enter__ caching_allocator_befores = driver_befores = Performs gc required required any CUDA memory held num_devices = torch cuda device_count i range num_devices caching_allocator_mem_allocated = torch cuda memory_allocated i NOTE gc based exclusively caching allocator memory because driver will always have some bytes use context size caching_allocator_mem_allocated gc collect torch _C _cuda_clearCublasWorkspaces torch cuda empty_cache break Acquires caching allocator driver statistics before test run i range num_devices caching_allocator_befores append torch cuda memory_allocated i bytes_free bytes_total = torch cuda mem_get_info i driver_mem_allocated = bytes_total - bytes_free driver_befores append driver_mem_allocated __exit__ exc_type exc_value traceback Don t check leaks exception thrown exc_type None Compares caching allocator before after statistics An increase allocated memory discrepancy indicating possible memory leak discrepancy_detected = False num_devices = torch cuda device_count i range num_devices avoid counting cublasWorkspace allocations torch _C _cuda_clearCublasWorkspaces caching_allocator_mem_allocated = torch cuda memory_allocated i caching_allocator_mem_allocated caching_allocator_befores i discrepancy_detected = True break Short-circuits no discrepancy detected discrepancy_detected Validates discrepancy persists after garbage collection confirmed driver API NOTE driver API iscrepancies alone ignored because jiterator some tests may permanently increase CUDA context size will appear driver memory leak expected behavior GCs clears cache gc collect torch cuda empty_cache i range num_devices discrepancy_detected = True Query memory multiple items ensure leak transient _ range caching_allocator_mem_allocated = torch cuda memory_allocated i bytes_free bytes_total = torch cuda mem_get_info i driver_mem_allocated = bytes_total - bytes_free caching_allocator_discrepancy = False driver_discrepancy = False caching_allocator_mem_allocated caching_allocator_befores i caching_allocator_discrepancy = True driver_mem_allocated driver_befores i driver_discrepancy = True caching_allocator_discrepancy driver_discrepancy Leak false positive exit loop discrepancy_detected = False break discrepancy_detected continue caching_allocator_discrepancy driver_discrepancy type ignore possibly-undefined Just raises warning leak validated driver API NOTE may problem how caching allocator collects its statistics leak too small trigger allocation additional block memory CUDA driver msg = CUDA caching allocator reports memory leak type ignore possibly-undefined f verified driver API name f Caching allocator allocated memory caching_allocator_befores i f now reported caching_allocator_mem_allocated type ignore possibly-undefined f device i f CUDA driver allocated memory driver_befores i now driver_mem_allocated type ignore possibly-undefined warnings warn msg stacklevel= caching_allocator_discrepancy driver_discrepancy type ignore possibly-undefined A caching allocator discrepancy validated driver API failure except ROCm see below msg = f CUDA driver API confirmed leak name type ignore possibly-undefined f Caching allocator allocated memory caching_allocator_befores i f now reported caching_allocator_mem_allocated type ignore possibly-undefined f device i f CUDA driver allocated memory driver_befores i now driver_mem_allocated type ignore possibly-undefined raise RuntimeError msg contextmanager skip_exception_type exc_type try yield except exc_type e raise unittest SkipTest f implemented e e contextmanager print_repro_on_failure repro_parts try yield except unittest SkipTest raise except Exception e Get index sample input failed test possible sample_isolation_prefix = tracked_input = getattr e _tracked_input None tracked_input None sample_isolation_prefix = f PYTORCH_OPINFO_SAMPLE_INPUT_INDEX= tracked_input index repro_str = join filter None sample_isolation_prefix repro_parts open_source_signpost subsystem= test_repros name= test_failure parameters=json dumps repro join filter None sample_isolation_prefix repro_parts repro_msg = f To execute test run following base repo dir repro_str This message can suppressed setting PYTORCH_PRINT_REPRO_ON_FAILURE= NB Hacking exception args cleanest way I ve found append failure reproduction info without poisoning stack trace len e args = e args = f e args \n repro_msg e args raise min_satisfying_examples setting has been deprecated hypothesis removed hypothesis x try hypothesis settings args kwargs min_satisfying_examples kwargs hypothesis version __version_info__ = kwargs pop min_satisfying_examples hypothesis settings args kwargs hypothesis settings register_profile pytorch_ci settings derandomize=True suppress_health_check= hypothesis HealthCheck too_slow database=None max_examples= verbosity=hypothesis Verbosity normal hypothesis settings register_profile dev settings suppress_health_check= hypothesis HealthCheck too_slow database=None max_examples= verbosity=hypothesis Verbosity normal hypothesis settings register_profile debug settings suppress_health_check= hypothesis HealthCheck too_slow database=None max_examples= verbosity=hypothesis Verbosity verbose hypothesis settings load_profile pytorch_ci IS_CI os getenv PYTORCH_HYPOTHESIS_PROFILE dev except ImportError warnings warn Fail hypothesis common_utils tests derandomized ImportWarning stacklevel= Used check_if_enable see test method should disabled issue sanitizes test method name appended suffixes dtypes parametrization e g issue title DISABLED test_bitwise_ops __main__ TestBinaryUfuncs should disabled ALL parametrized test_bitwise_ops tests such test_bitwise_ops_cuda_int remove_device_and_dtype_suffixes test_name str - str statement localized avoid circular dependency issues common_device_type py torch testing _internal common_device_type get_device_type_test_bases device_suffixes = x device_type x get_device_type_test_bases dtype_suffixes = str dt len torch dt get_all_dtypes test_name_chunks = test_name split _ len test_name_chunks test_name_chunks - dtype_suffixes len test_name_chunks test_name_chunks - device_suffixes _ join test_name_chunks - _ join test_name_chunks - test_name check_if_enable test unittest TestCase classname = str test __class__ split split - sanitized_testname = remove_device_and_dtype_suffixes test _testMethodName matches_test target str target_test_parts = target split len target_test_parts poorly formed target test name False target_testname = target_test_parts target_classname = target_test_parts - split - test method name its sanitized version exactly matches disabled test method name AND allow non-parametrized suite names disable parametrized ones TestSuite disables TestSuiteCPU classname startswith target_classname target_testname test _testMethodName sanitized_testname any matches_test x x slow_tests_dict keys getattr test test _testMethodName __dict__ slow_test = True TEST_WITH_SLOW raise unittest SkipTest test slow run PYTORCH_TEST_WITH_SLOW enable test IS_SANDCASTLE should_skip = False skip_msg = disabled_test issue_url platforms disabled_tests_dict items matches_test disabled_test platform_to_conditional dict = mac IS_MACOS macos IS_MACOS win IS_WINDOWS windows IS_WINDOWS linux IS_LINUX rocm TEST_WITH_ROCM xpu TEST_XPU asan TEST_WITH_ASAN dynamo TEST_WITH_TORCHDYNAMO dynamo_wrapped TEST_WITH_TORCHDYNAMO inductor TEST_WITH_TORCHINDUCTOR slow TEST_WITH_SLOW invalid_platforms = list filter lambda p p platform_to_conditional platforms len invalid_platforms invalid_plats_str = join invalid_platforms valid_plats = join platform_to_conditional keys print f Test disabled_test disabled some unrecognized f platforms invalid_plats_str Please edit issue issue_url fix platforms assigned flaky test changing Platforms comma separated f subset following leave blank match all platforms valid_plats Sanitize platforms list so we continue disable test any valid platforms given platforms = list filter lambda p p platform_to_conditional platforms platforms == any platform_to_conditional platform platform platforms should_skip = True skip_msg = f Test disabled because issue exists disabling issue_url \ f all platforms == platform s join platforms \ If you re seeing your local machine would like enable test \ please make sure CI set you using flag -- import-disabled-tests break should_skip RERUN_DISABLED_TESTS Skip disabled test when running under -- rerun-disabled-tests verification mode raise unittest SkipTest skip_msg should_skip RERUN_DISABLED_TESTS Probably test has disable issue platform skip_msg = Test enabled -- rerun-disabled-tests verification mode set so only \ disabled tests run raise unittest SkipTest skip_msg TEST_SKIP_FAST hasattr test test _testMethodName getattr test test _testMethodName __dict__ get slow_test False raise unittest SkipTest test fast we disabled PYTORCH_TEST_SKIP_FAST ` TestCase assertEqual ` very permissive coerced inputs into format could compared This very convenient when writing tests so much while reviewing them By default comparison ` Pair ` framework ` torch testing _comparison are_equal ` used example public testing function ` torch testing assert_close ` more strict In order use same framework thus reduce divergence between internal external comparison logic much possible we define some relaxed pairs here They only change supported inputs comparison logic same TODO Revisit relaxed pairs check how much work fix tests would fail without relaxation RelaxedBooleanPair BooleanPair Pair boolean-like inputs In contrast builtin ` BooleanPair ` also supports one input being number single element tensor-like _supported_number_types = NumberPair _supported_types _process_inputs actual expected id We require only one inputs inputs boolean other can also boolean number single element tensor array whereas default BooleanPair both inputs have booleans tensor_or_array_types tuple type = torch Tensor np ndarray other_supported_types = _supported_types _supported_number_types tensor_or_array_types isinstance actual _supported_types isinstance expected other_supported_types isinstance expected _supported_types isinstance actual other_supported_types _inputs_not_supported _to_bool input id=id input actual expected _to_bool bool_like id isinstance bool_like np number bool bool_like item type bool_like _supported_number_types bool bool_like isinstance bool_like torch Tensor np ndarray numel = bool_like numel isinstance bool_like torch Tensor bool_like size numel _fail ValueError f Only single element tensor-likes can compared against boolean f Got numel elements instead id=id bool bool_like item super _to_bool bool_like id=id RelaxedNumberPair NumberPair Pair number-like inputs In contrast builtin ` NumberPair ` also supports one input being single element tensor-like ` enum Enum ` D Type checks disabled meaning comparing succeeds even when ` ` check_dtype=True ` ` passed In addition uses looser default tolerances ` float ` ` complex ` inputs Also supports overriding absolute relative tolerance through ` ` precisionOverride ` ` ` ` toleranceOverride ` ` decorators _TYPE_TO_DTYPE = int torch int float torch float complex torch complex __init__ actual expected rtol_override= atol_override= check_dtype=None other_parameters - None super __init__ actual expected check_dtype=False other_parameters rtol = max rtol rtol_override atol = max atol atol_override _process_inputs actual expected id We require only one inputs inputs number other can also number single element tensor array whereas default NumberPair both inputs have numbers tensor_or_array_types tuple type = torch Tensor np ndarray other_supported_types = _supported_types tensor_or_array_types isinstance actual _supported_types isinstance expected other_supported_types isinstance expected _supported_types isinstance actual other_supported_types _inputs_not_supported _to_number input id=id input actual expected _to_number number_like id isinstance number_like torch Tensor np ndarray numel = number_like numel isinstance number_like torch Tensor number_like size numel _fail ValueError f Only single element tensor-likes can compared against number f Got numel elements instead id=id number = number_like item isinstance number bool number = int number number isinstance number_like Enum int number_like type ignore call-overload number = super _to_number number_like id=id type number _TYPE_TO_DTYPE keys _inputs_not_supported number TensorOrArrayPair TensorLikePair Pair tensor-like inputs On one hand stricter than builtin ` TensorLikePair ` since only allows instances ` torch Tensor ` ` numpy ndarray ` rather than allowing any tensor-like than can converted into tensor On other hand looser since converts all inputs into tensors no regard their relationship e g comparing ` torch Tensor ` ` numpy ndarray ` fine In addition supports overriding absolute relative tolerance through ` ` precisionOverride ` ` ` ` toleranceOverride ` ` decorators __init__ actual expected rtol_override= atol_override= other_parameters super __init__ actual expected other_parameters rtol = max rtol rtol_override atol = max atol atol_override _process_inputs actual expected id allow_subclasses _check_inputs_isinstance actual expected cls= torch Tensor np ndarray actual expected = _to_tensor input input actual expected tensor actual expected _check_supported tensor id=id actual expected TypedStoragePair TensorLikePair Pair ` torch storage TypedStorage ` inputs __init__ actual expected rtol_override= atol_override= other_parameters _check_inputs_isinstance actual expected cls=torch storage TypedStorage super __init__ actual expected other_parameters rtol = max rtol rtol_override atol = max atol atol_override _to_tensor typed_storage torch tensor typed_storage _untyped_storage dtype= torch quint torch uint torch quint x torch uint torch quint x torch uint torch qint torch int torch qint torch int get typed_storage dtype typed_storage dtype device=typed_storage device UnittestPair Pair Fallback ABC pair handles non-numeric inputs To avoid recreating mismatch messages meth ` unittest TestCase assertEqual ` pair simply wraps order use ` Pair ` framework func ` are_equal ` Define attr ` UnittestPair CLS ` subclass indicate which es inputs pair should support CLS Union type tuple type TYPE_NAME Optional str = None __init__ actual expected other_parameters _check_inputs_isinstance actual expected cls=self CLS super __init__ actual expected other_parameters compare test_case = unittest TestCase try test_case assertEqual actual expected except test_case failureException error msg = str error type_name = TYPE_NAME CLS isinstance CLS type CLS __name__ _fail AssertionError f type_name title comparison failed msg StringPair UnittestPair CLS = str bytes TYPE_NAME = string SetPair UnittestPair CLS = set TypePair UnittestPair CLS = type ObjectPair UnittestPair CLS = object This implements variant assertRaises assertRaisesRegex where we first test exception NotImplementedError so just skip test instead failing This implemented inheriting private implementation assertRaises unittest case slightly tweaking new behavior The year private hierarchy hasn t changed since seems low risk inherit AssertRaisesContextIgnoreNotImplementedError unittest case _AssertRaisesContext __exit__ exc_type exc_value tb exc_type None issubclass exc_type NotImplementedError test_case skipTest f not_implemented exc_value type ignore attr-defined super __exit__ exc_type exc_value tb contextmanager set_warn_always_context new_val bool old_val = torch is_warn_always_enabled torch set_warn_always new_val try yield finally torch set_warn_always old_val NoTest causes pytest recognize test __test__ = False TestCase expecttest TestCase NOTE precision lets classes generated tests set minimum atol values when comparing tensors Used precisionOverride toleranceOverride example NOTE rel_tol lets classes generated tests set minimum rtol values when comparing tensors Used toleranceOverride example _precision float = _rel_tol float = Toggles whether assert ` torch get_default_dtype ` returns ` torch float ` when ` setUp ` ` tearDown ` called _default_dtype_check_enabled bool = False Always use difflib print diffs multi line equality Undocumented feature unittest _diffThreshold = sys maxsize maxDiff = None checker early terminate test suite unrecoverable failure occurs _should_stop_test_suite torch cuda is_initialized CUDA device side error will cause subsequence test cases fail stop entire test suite catches RuntimeError during torch cuda synchronize try torch cuda synchronize except RuntimeError rte print TEST SUITE EARLY TERMINATION due torch cuda synchronize failure file=sys stderr print str rte file=sys stderr True False False property precision - float _precision precision setter precision prec float - None _precision = prec property rel_tol - float _rel_tol rel_tol setter rel_tol prec float - None _rel_tol = prec _do_cuda_memory_leak_check = False _do_cuda_non_default_stream = False When True test case raises NotImplementedError instead failing test skip instead _ignore_not_implemented_error = False __init__ method_name= runTest methodName= runTest methodName correct naming unittest testslide uses keyword arguments So we need use both break BC support testslide methodName = runTest method_name = methodName super __init__ method_name test_method = getattr method_name None test_method None Wraps tested method we should do CUDA memory check TEST_CUDA_MEM_LEAK_CHECK _do_cuda_memory_leak_check = getattr test_method _do_cuda_memory_leak_check True FIXME figure out flaky - anti-leaks windows See _do_cuda_memory_leak_check IS_WINDOWS wrap_with_cuda_policy method_name assertLeaksNoCudaTensors Wraps tested method we should enforce non default CUDA stream _do_cuda_non_default_stream = getattr test_method _do_cuda_non_default_stream True _do_cuda_non_default_stream IS_WINDOWS wrap_with_cuda_policy method_name enforceNonDefaultStream _ignore_not_implemented_error wrap_with_policy method_name lambda skip_exception_type NotImplementedError PRINT_REPRO_ON_FAILURE try _get_rel_test_path abs_test_path Attempt get relative path based test dir In CI working dir guaranteed base repo dir so we can t just compute relative path parts = Path abs_test_path parts i part enumerate parts part == test base_dir = os path join parts i i os path relpath abs_test_path start=base_dir Can t determine containing dir just test filename The path isn t strictly correct s arguably better than nothing os path split abs_test_path abs_test_path = inspect getfile type test_filename = _get_rel_test_path abs_test_path class_name = type __name__ test_run_cmd = f python test_filename class_name method_name env_var_prefix = TestEnvironment repro_env_var_prefix repro_parts = env_var_prefix test_run_cmd wrap_with_policy method_name lambda repro_parts=repro_parts print_repro_on_failure repro_parts except Exception e Don t fail entirely we can t get test filename log info could print repro string extra=str e type ignore arg-type assertLeaksNoCudaTensors name=None name = id name None name CudaMemoryLeakCheck name enforceNonDefaultStream CudaNonDefaultStream _remove_ansi_escape input -bit C ANSI sequences ansi_escape = re compile r \x B ESC -bit C Fe except CSI -Z\\-_ &#124; CSI followed control sequence \ - Parameter bytes - Intermediate bytes -~ Final byte re VERBOSE ansi_escape sub input remove_comment_lines input_string lines = input_string split \n filtered_lines = line line lines line strip startswith \n join filtered_lines remove_empty_lines input_string lines = input_string split \n filtered_lines = line line lines line strip = \n join filtered_lines ignore comments will ignore lines starts after being stripped assertExpectedInline actual expect skip= ignore_comments=False ignore_empty_lines=False actual = actual isinstance actual str str actual actual = _remove_ansi_escape actual expect = _remove_ansi_escape expect ignore_comments actual = remove_comment_lines actual expect = remove_comment_lines expect ignore_empty_lines actual = remove_empty_lines actual expect = remove_empty_lines expect super assertExpectedInline actual isinstance actual str str actual expect skip + Munges exceptions internally contain stack traces using munge_exc assertExpectedInlineMunged exc_type callable expect skip= suppress_suffix=True post_munge=None try callable except exc_type e munged = munge_exc e suppress_suffix=suppress_suffix skip=skip + post_munge munged = post_munge munged assertExpectedInline munged expect skip=skip + fail msg= Did raise when expected assertLogs logger=None level=None logger None logger = logging getLogger torch super assertLogs logger level assertNoLogs logger=None level=None logger None logger = logging getLogger torch super assertNoLogs logger level wrap_with_cuda_policy method_name policy test_method = getattr method_name below may initialize CUDA context so we do only _do_cuda_memory_leak_check _do_cuda_non_default_stream True TODO sure looks like we unconditionally initialize context here -- ezyang torch testing _internal common_cuda TEST_CUDA fullname = id lower class_name method_name TEST_CUDA gpu fullname cuda fullname setattr method_name wrap_method_with_policy test_method policy wrap_with_policy method_name policy test_method = getattr method_name setattr method_name wrap_method_with_policy test_method policy A policy zero-argument function returns context manager We don t take context manager directly may necessary construct once per test method wrap_method_with_policy method policy Assumes ` method ` tested function ` ` NOTE Python Exceptions e g unittest Skip keeps objects scope alive so cannot done setUp tearDown because tearDown run unconditionally no matter whether test passes For same reason we can t wrap ` method ` call try-finally always do check wraps method wrapper args kwargs policy method args kwargs types MethodType wrapper wrap_with_cuda_memory_check method wrap_method_with_policy method assertLeaksNoCudaTensors _dynamo_test_key f __class__ __name__ _testMethodName compile_fn fn backend nopython Allows subclasses control compilation torch _dynamo optimize backend nopython=nopython fn _run_custom result=None using_unittest = isinstance result unittest TestResult super_run = super run test_cls = super_run __self__ type ignore attr-defined Are we compiling compiled = TEST_WITH_TORCHDYNAMO TEST_WITH_AOT_EAGER TEST_WITH_TORCHINDUCTOR Is strict compiling strict_default = False should_reset_dynamo = False We disable size_asserts test_ops since some tests fail due mismatch strides returned eager v s meta kernels Only some ops has problem since tests test_op py parametrized s hard do specifically affected ops It s big deal since these problems captured test_torchinductor_opinfo py well should_disable_size_asserts = False compiled try path = inspect getfile type test_cls full_path = os path abspath path match = re match r test py full_path match None filename = match group TEST_WITH_TORCHINDUCTOR dynamo_test_failures FIXME_inductor_non_strict strict_default = filename FIXME_inductor_non_strict should_reset_dynamo = True filename == test_ops should_disable_size_asserts = True strict_default = True inspect getfile can fail these except OSError TypeError pass STRICT_DEFAULT os environ os environ STRICT_DEFAULT == strict_default = True strict_mode = False compiled test_method = getattr _testMethodName hasattr test_method dynamo_strict strict_mode = test_method dynamo_strict hasattr test_cls dynamo_strict strict_mode = test_cls dynamo_strict strict_mode = strict_default nopython = getattr test_cls dynamo_strict_nopython False compiled strict_mode should_reset_dynamo torch _dynamo reset torch compiler set_stance default TODO Remove grandfathered because we suppressed errors test suite previously When strict mode False suppress_errors True compiled suppress_errors = strict_mode suppress_errors = torch _dynamo config suppress_errors maybe_disable_size_asserts = torch _inductor config patch size_asserts=False should_disable_size_asserts contextlib nullcontext unittest mock patch torch _dynamo config suppress_errors suppress_errors maybe_disable_size_asserts TEST_WITH_AOT_EAGER super_run = compile_fn super_run aot_eager_decomp_partition nopython TEST_WITH_TORCHDYNAMO TEST_WITH_TORCHINDUCTOR TEST_WITH_TORCHINDUCTOR super_run = compile_fn super_run inductor nopython Assume eager-generated GraphModules will error out If we do probably Dynamo bug super_run = compile_fn super_run eager_noexcept nopython key = _dynamo_test_key expect_failure f file_name wraps f wrapper args kwargs try f args kwargs except BaseException e noqa B skipTest e raise RuntimeError f Unexpected success please remove ` file_name ` wrapper TEST_WITH_TORCHINDUCTOR subdir = test inductor_expected_failures dynamo_test_failures inductor_expected_failures expected_failures subdir = test dynamo_expected_failures dynamo_test_failures dynamo_expected_failures expected_failures key expected_failures method = getattr _testMethodName file_name = os path join subdir key setattr _testMethodName expect_failure method file_name ignore_failure f file_name wraps f wrapper args kwargs try f args kwargs except BaseException e noqa B skipTest e method = getattr _testMethodName getattr method __unittest_expecting_failure__ False skipTest unexpected success skipTest f This test passed maybe we can remove ` file_name ` wrapper TEST_WITH_TORCHINDUCTOR subdir = test inductor_skips dynamo_test_failures inductor_skips skips subdir = test dynamo_skips dynamo_test_failures dynamo_skips skips key skips method = getattr _testMethodName file_name = os path join subdir key setattr _testMethodName ignore_failure method file_name dynamo_test_failures compiled_autograd_skips torch _dynamo config compiled_autograd key compiled_autograd_skips Still run test compiled autograd disabled super_run = runWithoutCompiledAutograd super_run super_run result=result strict_mode should_reset_dynamo torch _dynamo reset torch _dynamo config compiled_autograd torch _dynamo compiled_autograd reset Early terminate test necessary If using pytest use -x flag instead using_unittest _should_stop_test_suite result wasSuccessful case = TestCase TEST_SAVE_XML None This big hacky XMLRunner modifies expected type TestCase TestInfo Create dummy TestInfo record results correctly xmlrunner result _TestInfo type ignore case = _TestInfo result case case output = _TestInfo ERROR type ignore attr-defined case elapsed_time = type ignore attr-defined case test_description = TestSuiteEarlyFailure type ignore attr-defined This shouldn t really happen does add fake failure For more details see https github com pytorch pytorch issues result failures append case TestSuite execution aborted early assert result wasSuccessful False result stop run result=None contextlib ExitStack stack TEST_WITH_CROSSREF stack enter_context CrossRefMode _run_custom result=result setUp check_if_enable set_rng_seed Save global check sparse tensor invariants state can restored tearDown _check_invariants = torch sparse check_sparse_tensor_invariants is_enabled Enable invariant checks all sparse tensors constructions including unsafe ones If desired some test case use check_invariants=False optional argument sparse tensor constructors torch sparse check_sparse_tensor_invariants False decorator disable invariant checks torch sparse check_sparse_tensor_invariants enable _default_dtype_check_enabled assert torch get_default_dtype == torch float attempt reset some global state end test _prev_grad_state = torch is_grad_enabled tearDown There exists test cases override TestCase setUp definition so we cannot assume _check_invariants attribute defined general hasattr _check_invariants Restore global check sparse tensor invariants state _check_invariants torch sparse check_sparse_tensor_invariants enable torch sparse check_sparse_tensor_invariants disable _default_dtype_check_enabled assert torch get_default_dtype == torch float attribute may defined per above hasattr _prev_grad_state torch set_grad_enabled _prev_grad_state staticmethod _make_crow_indices n_rows n_cols nnz device dtype random=True Return crow_indices CSR tensor size n_rows n_cols number specified elements nnz If random True column counts rows random order Otherwise column counts rows defined used sampling method Sampling method --------------- The used sampling method introduced https pearu github io csr_sampling html here we give only overall description method Notice crow_indices can defined cumsum counts where counts sequence non-negative integers satisfying following conditions len counts == n_rows + counts max = n_cols while counts i + interpreted number specified elements i-th row The used sampling method aims increasing diversity CSR samples CSR sample should contain i rows all filled ii rows no elements all iii rows partially filled At same time given total number specified elements nnz there should minimal preference rows given number elements To achieve sampling method built-up using sawteeth model counts In simplest case we would have counts = arange n_rows + n_cols + has equal number all possible column counts per row This formula can used only specific input values n_rows n_cols nnz To generalize model any combinations inputs counts model above extended incomplete sawtooth right lower rectangular parts will guarantee counts sum == nnz any combination n_rows n_cols nnz Basically we ll find maximal window n_rows + n_cols + -grid able hold sequence sawteeth so-called final correction while external part window filled counts meet nnz constraint exactly assert = nnz = n_rows n_cols nnz n_rows n_cols sawteeth n m total number counts sequence sawteeth where n m define window n_rows+ n_cols+ rectangle where sequence sawteeth perfectly fit M = n_cols - m n_cols - m + K = n_rows - n n_cols - m + M n_rows - n n_cols - m + + K K - Different original method description here counts has leading required crow_indices counts = torch zeros n_rows + dtype=dtype device=torch device cpu n = m = N = sawteeth n m N nnz = max N n_cols determine width sawteeth window We use bisection solve N n == nnz - n n_cols max N n n_cols n n_left = n n_right = n_rows - N_right = sawteeth n_right m while n_right - n_left n_middle = n_left + n_right N_middle = sawteeth n_middle m N_middle == nnz - n_middle n_cols max N_middle n_cols n_right N_right = n_middle N_middle n_left = n_middle n N = n_right N_right fill right rectangle counts assert n counts -n fill_ n_cols N nnz - n n_cols = max N n_rows - n determine height sawteeth window We use bisection solve N n m == nnz - n n_cols - m n_rows - n max N n m n_rows - n m m_left = m m_right = n_cols - N_right = sawteeth n m_right while m_right - m_left m_middle = m_left + m_right N_middle = sawteeth n m_middle N_middle == nnz - n n_cols - m_middle n_rows - n max N_middle n_rows - n m_right N_right = m_middle N_middle m_left = m_middle m N = m_right N_right fill bottom rectangle counts assert m counts n_rows - n + fill_ m N fill sawteeth window counts q r = divmod nnz - n n_cols - m n_rows - n n_cols - m n_cols - m + p = + q n_cols - m + k = math isqrt r k k + r k -= corr = r - k k + assert p m full sawteeth never top bottom rectangle sequence full sawteeth counts p = torch arange p - dtype=dtype device=counts device n_cols - m + incomplete sawtooth counts p p + k + += torch arange k + dtype=dtype device=counts device given input does support sawteeth p = corr = nnz - n n_cols - m n_rows - n correction will guarantee counts sum == nnz counts p += corr random randomize crow_indices shuffling sawteeth sequence perm = torch randperm n_rows device=counts device counts = counts perm compute crow_indices crow_indices = counts crow_indices cumsum_ dim= crow_indices device=device genSparseCompressedTensor size nnz layout device dtype index_dtype blocksize= dense_dims= operator mul functools reduce sparse_dim = assert all size d d range len size nnz == invalid arguments assert len size = sparse_dim blocksize assert len blocksize == size blocksize assert size - - dense_dims blocksize == size blocksize assert size - - dense_dims blocksize == size blocksize blocksize blocksize = blocksize blocksize = blocksize = size = tuple size dense_size = size len size - dense_dims random_sparse_compressed n_compressed_dims n_plain_dims nnz compressed_indices = _make_crow_indices n_compressed_dims n_plain_dims nnz device=device dtype=index_dtype plain_indices = torch zeros nnz dtype=index_dtype device=device i range n_compressed_dims count = compressed_indices i + - compressed_indices i plain_indices compressed_indices i compressed_indices i + _ = torch sort torch randperm n_plain_dims dtype=index_dtype device=device count low = - dtype = torch uint high = dtype = torch uint values = make_tensor nnz + blocksize + dense_size device=device dtype=dtype low=low high=high values compressed_indices plain_indices batch_shape = size - - dense_dims n_batch = reduce mul batch_shape layout torch sparse_csr torch sparse_bsr n_compressed_dims n_plain_dims = size - - dense_dims blocksize size - - dense_dims blocksize n_compressed_dims n_plain_dims = size - - dense_dims blocksize size - - dense_dims blocksize blocknnz = nnz blocksize blocksize sparse_tensors = random_sparse_compressed n_compressed_dims n_plain_dims blocknnz _ range n_batch sparse_tensors_it = map list zip sparse_tensors strict=True values = torch stack next sparse_tensors_it reshape batch_shape blocknnz blocksize dense_size compressed_indices = torch stack next sparse_tensors_it reshape batch_shape - plain_indices = torch stack next sparse_tensors_it reshape batch_shape - torch sparse_compressed_tensor compressed_indices plain_indices values size=size dtype=dtype layout=layout device=device genSparseCSRTensor size nnz device dtype index_dtype dense_dims= genSparseCompressedTensor size nnz layout=torch sparse_csr device=device dtype=dtype index_dtype=index_dtype blocksize= dense_dims=dense_dims genSparseCSCTensor size nnz device dtype index_dtype dense_dims= genSparseCompressedTensor size nnz layout=torch sparse_csc device=device dtype=dtype index_dtype=index_dtype blocksize= dense_dims= genSparseBSRTensor size blocksize nnz device dtype index_dtype dense_dims= assert len blocksize == genSparseCompressedTensor size nnz layout=torch sparse_bsr device=device dtype=dtype index_dtype=index_dtype blocksize=blocksize dense_dims=dense_dims genSparseBSCTensor size blocksize nnz device dtype index_dtype dense_dims= assert len blocksize == genSparseCompressedTensor size nnz layout=torch sparse_bsc device=device dtype=dtype index_dtype=index_dtype blocksize=blocksize dense_dims=dense_dims genSparseTensor size sparse_dim nnz is_uncoalesced device dtype Assert given impossible combination where sparse dims have empty numel nnz makes indices containing values assert all size d d range sparse_dim nnz == invalid arguments v_size = nnz + list size sparse_dim v = make_tensor v_size device=device dtype=dtype low=- high= i = torch rand sparse_dim nnz device=device i mul_ torch tensor size sparse_dim unsqueeze i i = i torch long is_uncoalesced i = i nnz i = i nnz + i = torch cat i i x = torch sparse_coo_tensor i v torch Size size dtype=dtype device=device is_uncoalesced x = x coalesce FIXME ` x ` sparse view ` v ` Currently rebase_history sparse views implemented so workaround needed inplace operations done ` x ` e g copy_ Remove after implementing something equivalent CopySlice sparse views NOTE We do clone after detach here because we need able change size storage x afterwards x = x detach clone _coalesced_ False x x _indices clone x _values clone generate_simple_inputs layout device=None dtype=None index_dtype=None pin_memory=None members_pin_memory=None enable_batch=True enable_hybrid=True enable_zero_sized=True enable_non_contiguous_indices=True enable_non_contiguous_values=True enable_batch_variable_nse=False output_tensor=True patterns=None Generator simple inputs tensor constructors given layout The generated tensor inputs have following properties - tensor shapes minimal trivial - tensor values sorted sequences COO CSR formats e g - generated tensors represent same mathematical tensor all layouts - generated tensors include regular zero-sized optionally batched hybrid tensors - generated tensors include contiguous non-contiguous tensors both indices values If output_tensor True yield tensors given layout Otherwise yield inputs corresponding tensor constructors - sparse compressed input defined compressed_indices plain_indices values dict size=expected_size_from_shape_inference device=device dtype=dtype pin_memory=pin_memory - sparse COO input defined indices values dict size=expected_size_from_shape_inference device=device dtype=dtype pin_memory=pin_memory - strided input defined values dict device=device dtype=dtype index_dtype None index_dtype = torch int is_compressed_sparse_layout = layout torch sparse_csr torch sparse_csc torch sparse_bsr torch sparse_bsc output_tensor args kwargs generate_simple_inputs layout device=device dtype=dtype index_dtype=index_dtype pin_memory=pin_memory enable_batch=enable_batch enable_hybrid=enable_hybrid enable_zero_sized=enable_zero_sized enable_non_contiguous_indices=enable_non_contiguous_indices enable_non_contiguous_values=enable_non_contiguous_values enable_batch_variable_nse=enable_batch_variable_nse output_tensor=False members_pin_memory args = tuple pin_memory args layout torch strided assert len args == size = kwargs pop size None ensure zero-sized tensor has desired shape assert size None pin_memory yield args reshape size pin_memory yield args reshape size layout torch sparse_coo yield torch sparse_coo_tensor args kwargs is_compressed_sparse_layout kwargs update layout=layout yield torch sparse_compressed_tensor args kwargs assert unreachable get_blockpattern pattern blocksize basesize = pattern shape assert basesize blocksize == basesize blocksize assert basesize blocksize == basesize blocksize blockpattern = pattern reshape - blocksize basesize blocksize blocksize transpose - - any - any - block_ids = torch arange blockpattern numel + reshape blockpattern shape blockpattern = block_ids get_sparse_data pattern basesize = pattern shape assert len basesize == basesize pattern expected matrix We cannot use ` torch sparse_xyz_tensor pattern ` compute sparse layout indices values because generate_simple_inputs used generate inputs test ` torch sparse_xyz_tensor ` factory functions so we ll compute indices values independently factory functions indices = torch where pattern = coo_indices = torch stack indices crow_indices = torch zeros basesize + dtype=torch int crow_indices = torch cumsum coo_indices bincount minlength=basesize col_indices = coo_indices strided_values = torch zeros basesize dtype=torch int property ` values == range +nnz ` used get_sparse_data_with_block relate BSR BSC values so don t change following line values = torch arange + len indices dtype=torch int strided_values indices = values indices_T = torch where pattern transpose = coo_indices_T = torch stack indices_T ccol_indices = torch zeros basesize + dtype=torch int ccol_indices = torch cumsum coo_indices_T bincount minlength=basesize row_indices = coo_indices_T csc_values = strided_values transpose indices_T torch sparse_coo coo_indices values torch sparse_csr crow_indices col_indices values torch sparse_csc ccol_indices row_indices csc_values torch strided strided_values get_sparse_data_with_block pattern blocksize nonblock_data = get_sparse_data pattern blockpattern = get_blockpattern pattern blocksize block_data = get_sparse_data blockpattern strided_values = nonblock_data torch strided block_indices = block_data torch sparse_coo bsr_values = torch stack strided_values bi blocksize bi + blocksize bj blocksize bj + blocksize bi bj block_indices transpose here we use property ` values == range +nnz ` ` values ` relation ` csc_values ` see get_sparse_data get BSC blocks via reordering BSR blocks bsc_values = bsr_values block_data torch sparse_csc - torch sparse_bsr block_data torch sparse_csr bsr_values torch sparse_bsc block_data torch sparse_csc bsc_values nonblock_data get_batch_sparse_data pattern blocksize size = pattern shape len size = non-batch get_sparse_data_with_block pattern blocksize batch data created recursively batch_data = type ignore var-annotated i item enumerate pattern layout d get_batch_sparse_data item blocksize items target = batch_data get layout layout torch sparse_coo batch COO means COO leading sparse dimensions interpreted batch dimensions ext_coo_indices = torch cat torch full len d i dtype=torch int d target None target = batch_data layout = ext_coo_indices d target set_ torch cat target ext_coo_indices type ignore call-overload target set_ torch cat target d target None target = batch_data layout = tuple d j unsqueeze j range len d j range len d target j set_ torch cat target j d j unsqueeze type ignore call-overload batch_data generate_values base densesize Generates tensor shape densesize values equal base + i_ ^ + + i_d ^ d - indices i_ i_d = i_j densesize j any = j = len densesize This mapping produces unique values long densesize i all i range len densesize densesize base isinstance base int base ndim torch stack generate_values b densesize b base base == torch zeros densesize dtype=torch int r = torch arange densesize dtype=torch int i d enumerate densesize y = torch arange d dtype=torch int i + r = r None + y None r add_ base r patterns None A pattern -tuple following items - list integers depth two more The integers define sparsity patterns generated inputs zero values correspond unspecified elements blocks non-zero values specified elements For debugging convenience elements same value typically belong same block However hard requirement long shape pattern divides block sizes pattern will valid one If depth list larger than two inputs batch dimensions will generated - list -tuples block sizes used generate BSR BSC tensors various block size parameters - list tuples dense dimensions used generate hybrid tensors various dense dimensions patterns = simple x tensor non-hybrid hybrid dense dimensions x batch x tensors non-hybrid hybrid dense dimensions tensor non-trivial blocksize batch tensor variable NSE Requires https github com pytorch pytorch pull similar enable_batch_variable_nse non_contiguous_copy t dim=- offset= copy t non-contiguous along given dimension given storage offset assertTrue t is_contiguous dim dim = dim + t ndim assert dim = dim t ndim step = max offset + tmp = torch zeros t shape dim t shape dim step t shape dim + dtype=t dtype device=t device dim_slices = slice None dim slice offset None step r = tmp dim_slices copy_ t assertFalse r is_contiguous assertEqual t r r main loop method pattern blocksizes densesizes patterns enable_hybrid densesizes = s s densesizes s densesizes blocksizes continue pattern = torch tensor pattern dtype=torch int enable_batch pattern ndim continue blocksize blocksizes data = get_batch_sparse_data pattern blocksize layout densesize densesizes indices = device=device dtype=index_dtype data - values = generate_values data - densesize device=device dtype=dtype kwargs = dict device=device dtype=dtype size=pattern shape + densesize pin_memory None kwargs update pin_memory=pin_memory yield indices values kwargs copy enable_non_contiguous_indices pattern ndim sparse compressed indices can sliced only along batch dimensions dim offset - indices_copy = non_contiguous_copy dim=dim offset=offset indices yield indices_copy values kwargs copy enable_non_contiguous_values values_copy = non_contiguous_copy values dim=- offset= yield indices_copy values_copy kwargs copy enable_non_contiguous_values values_copy = non_contiguous_copy values dim=- offset= yield indices values_copy kwargs copy zero-sized tensor inputs non-batch non-hybrid hybrid enable_zero_sized basesize blocksizes densesizes enable_hybrid blocksize blocksizes densesize densesizes type ignore attr-defined layout == torch strided indices = type ignore assignment values = torch empty basesize + densesize device=device dtype=dtype layout == torch sparse_coo indices = torch empty len basesize device=device dtype=index_dtype type ignore assignment values = torch empty densesize device=device dtype=dtype layout == torch sparse_csr crow_indices = torch tensor basesize + device=device dtype=index_dtype col_indices = torch empty device=device dtype=index_dtype indices = crow_indices col_indices type ignore assignment values = torch empty densesize device=device dtype=dtype layout == torch sparse_csc ccol_indices = torch tensor basesize + device=device dtype=index_dtype row_indices = torch empty device=device dtype=index_dtype indices = ccol_indices row_indices type ignore assignment values = torch empty densesize device=device dtype=dtype layout == torch sparse_bsr crow_indices = torch tensor basesize blocksize + device=device dtype=index_dtype col_indices = torch empty device=device dtype=index_dtype indices = crow_indices col_indices type ignore assignment values = torch empty blocksize densesize device=device dtype=dtype layout == torch sparse_bsc ccol_indices = torch tensor basesize blocksize + device=device dtype=index_dtype row_indices = torch empty device=device dtype=index_dtype indices = ccol_indices row_indices type ignore assignment values = torch empty blocksize densesize device=device dtype=dtype assert unreachable kwargs = dict device=device dtype=dtype size=basesize + densesize pin_memory None kwargs update pin_memory=pin_memory yield indices values kwargs safeToDense t coalesce only implemented COO t layout == torch sparse_coo t = t coalesce t to_dense Compares torch function reference function given sample input object SampleInput Note only values compared type comparison done here compare_with_reference torch_fn ref_fn sample_input kwargs numpy_sample = sample_input numpy n_inp n_args n_kwargs = numpy_sample input numpy_sample args numpy_sample kwargs t_inp t_args t_kwargs = sample_input input sample_input args sample_input kwargs actual = torch_fn t_inp t_args t_kwargs expected = ref_fn n_inp n_args n_kwargs assertEqual actual expected exact_device=False kwargs Compares given Torch NumPy functions given tensor-like object NOTE both torch_fn np_fn should functions take single tensor array If torch NumPy function require additional arguments then wrap function lambda pass partial function TODO add args kwargs passing assertEqual e g rtol atol compare_with_numpy torch_fn np_fn tensor_like device=None dtype=None kwargs assert TEST_NUMPY isinstance tensor_like torch Tensor assert device None assert dtype None t_cpu = tensor_like detach cpu t_cpu dtype torch bfloat t_cpu = t_cpu float = t_cpu numpy t = tensor_like d = copy copy torch_to_numpy_dtype_dict d torch bfloat = np float = np array tensor_like dtype=d dtype t = torch tensor tensor_like device=device dtype=dtype np_result = np_fn torch_result = torch_fn t cpu Converts arrays tensors isinstance np_result np ndarray try np_result = torch from_numpy np_result except Exception NOTE copying array before conversion necessary when example array has negative strides np_result = torch from_numpy np_result copy t dtype torch bfloat torch_result dtype torch bfloat np_result dtype torch float torch_result = torch_result torch float assertEqual np_result torch_result kwargs assertEqualIgnoreType args kwargs - None If you seeing function used means test written wrongly deserves detailed investigation assertEqual args exact_dtype=False kwargs assertEqualBroadcasting x y args kwargs - None r Tests tensor x equals y y broadcast x shape isinstance y Iterable int float etc different shape tensors y = torch ones_like x y isinstance y torch Tensor iterable tensor y = torch ones_like x torch tensor y assertEqual x y args kwargs assertEqual x y msg Optional Union str Callable str str = None atol Optional float = None rtol Optional float = None equal_nan=True exact_dtype=True TODO default True exact_device=False exact_layout=False exact_stride=False exact_is_coalesced=False Hide function ` pytest ` s traceback __tracebackhide__ = True numpy s dtypes superset what PyTorch supports In case we encounter unsupported dtype we fall back elementwise comparison Note has happen here example ` TensorOrArrayPair ` since stage we can no longer split array into its elements perform multiple comparisons any isinstance input np ndarray has_corresponding_torch_dtype input dtype input x y to_list input input tolist isinstance input torch Tensor np ndarray list input x = to_list x y = to_list y When comparing sequence numbers tensor we need convert sequence tensor here Otherwise pair origination ` are_equal ` will fail because sequence recognized container should checked elementwise while tensor isinstance x torch Tensor isinstance y Sequence y = torch as_tensor y dtype=x dtype device=x device isinstance x Sequence isinstance y torch Tensor x = torch as_tensor x dtype=y dtype device=y device unbind NSTs compare them don t do NJTs isinstance x torch Tensor x is_nested x layout == torch strided x = x unbind isinstance y torch Tensor y is_nested y layout == torch strided y = y unbind error_metas = not_close_error_metas x y pair_types= NonePair RelaxedBooleanPair RelaxedNumberPair TensorOrArrayPair TypedStoragePair StringPair SetPair TypePair ObjectPair sequence_types= Sequence Sequential ModuleList ParameterList ScriptList torch utils data dataset Subset mapping_types= Mapping ModuleDict ParameterDict ScriptDict rtol=rtol rtol_override=self rel_tol atol=atol atol_override=self precision equal_nan=equal_nan check_device=exact_device check_dtype=exact_dtype check_layout=exact_layout check_stride=exact_stride check_is_coalesced=exact_is_coalesced error_metas See ErrorMeta Cycles error_metas = error_metas type ignore list-item TODO compose all metas into one AssertionError raise error_metas pop to_error type ignore index This emulates unittest TestCase s behavior custom message passed TestCase longMessage https docs python org library unittest html#unittest TestCase longMessage True default lambda generated_msg f generated_msg \n msg isinstance msg str longMessage msg assertNotEqual x y msg Optional str = None type ignore override atol Optional float = None rtol Optional float = None kwargs - None assertRaises AssertionError msg=msg assertEqual x y msg atol=atol rtol=rtol kwargs assertEqualTypeString x y - None This API used simulate deprecated x type y type assertEqual x device y device assertEqual x dtype y dtype assertEqual x is_sparse y is_sparse assertObjectIn obj Any iterable Iterable Any - None elem iterable id obj == id elem raise AssertionError object found iterable Reimplemented provide special behavior when _ignore_not_implemented_error True assertRaises expected_exception args kwargs _ignore_not_implemented_error context Optional AssertRaisesContextIgnoreNotImplementedError = \ AssertRaisesContextIgnoreNotImplementedError expected_exception type ignore call-arg try context handle assertRaises args kwargs type ignore union-attr arg-type finally see https bugs python org issue context = None super assertRaises expected_exception args kwargs Reimplemented provide special behavior when _ignore_not_implemented_error True assertRaisesRegex expected_exception expected_regex args kwargs Verifies exception type expected_exception message matching regular expression defined expected_regex thrown If test instantiated non-native device type like XLA then message validated Checks whether test instantiated device type testing test has defined device_type attribute so tests whether instantiated device type native hasattr device_type device_type NATIVE_DEVICES device_type = mps type ignore attr-defined empty string matches any string expected_regex = _ignore_not_implemented_error context = AssertRaisesContextIgnoreNotImplementedError type ignore call-arg expected_exception expected_regex context handle assertRaisesRegex args kwargs type ignore attr-defined arg-type super assertRaisesRegex expected_exception expected_regex args kwargs Verifies no unraisable exceptions raised callable Unlike regular exceptions these do actually propagate caller suppressed We must test them specially assertNoUnraisable callable args kwargs raised = None record_unraisable unraisable nonlocal raised raised = unraisable Disable GC when running callable prevent spurious flakiness unlucky GCs inside callable prev = gc isenabled gc disable try unittest mock patch sys unraisablehook record_unraisable callable args kwargs finally prev gc enable assertIsNone raised TODO Support context manager interface NB The kwargs forwarding callable robs subname parameter If you need manually apply your callable lambda instead assertExpectedRaises exc_type callable args kwargs subname = None subname kwargs subname = kwargs subname del kwargs subname try callable args kwargs except exc_type e assertExpected str e subname Don t put try block AssertionError will catch fail msg= Did raise when expected assertNotWarn callable msg= r Test attr ` callable ` does raise warning warnings catch_warnings record=True ws warnings simplefilter always allow any warning raised set_warn_always_context True callable assertTrue len ws == msg contextmanager assertWarnsOnceRegex category regex= Context manager code must always warn This filters expected warnings test fails expected warning caught It uses set_warn_always force TORCH_WARN_ONCE behave like TORCH_WARN pattern = re compile regex warnings catch_warnings record=True ws warnings simplefilter always allow any warning raised set_warn_always_context True yield len ws == fail no warning caught assertTrue any type w message category w ws assertTrue any re match pattern str w message w ws f pattern w message w ws type w message category assertExpected s subname=None r Test string matches recorded contents file derived name test subname This file placed expect directory same directory test script You can automatically update recorded test output using -- accept If you call multiple times single function you must give unique subname each time isinstance s str raise TypeError assertExpected strings only remove_prefix text prefix text startswith prefix text len prefix text NB we take __file__ module defined test so we place expect directory where test script lives NOT where test common_utils py lives This doesn t matter PyTorch where all test scripts same directory test common_utils py matters onnx-pytorch module_id = __class__ __module__ munged_id = remove_prefix id module_id + test_file = os path realpath sys modules module_id __file__ type ignore type-var expected_file = os path join os path dirname test_file type ignore type-var arg-type expect munged_id subname_output = subname expected_file += - + subname subname_output = f subname expected_file += expect expected = None accept_output update_type print f Accepting update_type munged_id subname_output \n\n s open expected_file w f Adjust producer_version leave s unmodified s_tag = re sub r producer_version - r \ CURRENT_VERSION s f write s_tag try open expected_file f expected = f read except OSError e e errno = errno ENOENT raise expecttest ACCEPT accept_output output raise RuntimeError f I got output munged_id subname_output \n\n s \n\n No expect file exists accept current output run \n f python __main__ __file__ munged_id -- accept None hack JIT tests IS_WINDOWS expected = re sub r CppOp\ + \ CppOp expected s = re sub r CppOp\ + \ CppOp s Adjust producer_version expected = expected replace producer_version CURRENT_VERSION f producer_version torch onnx producer_version expecttest ACCEPT expected = s accept_output updated output hasattr assertMultiLineEqual Python only NB Python considers lhs old rhs new assertMultiLineEqual expected s assertEqual s expected assertExpectedStripMangled s subname=None s = re sub r __torch__ ^ + s assertExpected s subname assertGreaterAlmostEqual first second places=None msg=None delta=None Assert ` ` first ` ` greater than almost equal ` ` second ` ` The equality ` ` first ` ` ` ` second ` ` determined similar way ` ` assertAlmostEqual ` ` function standard library delta None places None raise TypeError specify delta places both first = second diff = second - first delta None diff = delta standardMsg = f first greater than equal second within delta delta places None places = round diff places == standardMsg = f first greater than equal second within places places msg = _formatMessage msg standardMsg raise failureException msg assertAtenOp onnx_model operator overload_name= all_aten_nodes = p p onnx_model graph node p op_type == ATen p domain == org pytorch aten assertTrue all_aten_nodes op all_aten_nodes attrs = attr name attr s decode attr op attribute attrs get operator == operator break assertEqual attrs operator operator type ignore possibly-undefined assertEqual attrs get overload_name overload_name check_nondeterministic_alert fn caller_name should_alert=True Checks operation produces nondeterministic alert when expected while ` torch use_deterministic_algorithms True ` set Args fn callable Function check nondeterministic alert caller_name str Name operation produces nondeterministic alert This name expected appear beginning error warning message should_alert bool optional If True then check will only pass calling ` fn ` produces nondeterministic error warning expected message If False then check will only pass calling ` fn ` does produce error Default ` True ` alert_message = ^ + caller_name + does have deterministic implementation you set Check errors thrown correctly DeterministicGuard True should_alert assertRaisesRegex RuntimeError alert_message msg= expected non-deterministic error raised fn If nondeterministic error expected make sure raised try fn except RuntimeError e does have deterministic implementation str e fail did expect non-deterministic error message + got one anyway + str e + Reraise exceptions unrelated nondeterminism raise Check warnings thrown correctly DeterministicGuard True warn_only=True should_alert assertWarnsRegex UserWarning alert_message fn warnings catch_warnings record=True w warnings simplefilter always fn warning w isinstance warning UserWarning assertTrue re search alert_message str warning None run code subprocess capture exceptions staticmethod run_process_no_exception code env=None subprocess popen = subprocess Popen sys executable -c code stdout=subprocess PIPE stderr=subprocess PIPE env=env stdout stderr = popen communicate stdout stderr returns captured stderr staticmethod runWithPytorchAPIUsageStderr code env = os environ copy env PYTORCH_API_USAGE_STDERR = remove CI flag since wrapped test process CI flag should set parent process only env pop CI None env pop TEST_SHOWLOCALS None _stdout stderr = TestCase run_process_no_exception code env=env stderr decode ascii _attempt_load_from_subprocess file pathlib Path import_string str expected_failure_message Optional str = None - None Attempts weights_only ` torch load ` subprocess This used test weights_only ` torch load ` works expected without global imports Args file pathlib Path The path checkpoint load import_string str string add script exected_failure_message str optional The expected failure message checkpoint fails load If None test will pass script = f torch import_string torch load r file weights_only=True cm = assertRaisesRegex RuntimeError re escape expected_failure_message expected_failure_message contextlib nullcontext cm try subprocess check_output sys executable -c script On Windows opening subprocess default CWD makes ` torch ` fail so just set CWD script s directory cwd=os path dirname os path realpath __file__ stderr=subprocess STDOUT except subprocess CalledProcessError e raise RuntimeError e output decode utf- None TestCaseBase TestCase Calls super dynamically created classes bit odd See https github com pytorch pytorch pull more info Subclassing then calling super TestCaseBase will run TestCase s setUp tearDown etc functions pass download_file url binary=True urllib parse urlsplit urllib request error filename = os path basename urlsplit url data_dir = get_writable_path os path join os path dirname __file__ data path = os path join data_dir filename os path exists path path try data = request urlopen url timeout= read open path wb binary w f f write data path except error URLError e msg = f could download test file url warnings warn msg RuntimeWarning stacklevel= raise unittest SkipTest msg e find_free_port Finds available port returns port number NOTE If function being used allocate port Store indirectly via init_process_group init_rpc should used conjunction ` retry_on_connect_failures ` decorator there potential race condition where allocated port may become unavailable before can used closing socket socket socket AF_INET socket SOCK_STREAM sock sock setsockopt socket SOL_SOCKET socket SO_REUSEADDR sock bind localhost _ port = sock getsockname port Errors we can get c d initialization which we should retry tests ADDRESS_IN_USE = Address already use CONNECT_TIMEOUT = connect timed out retry_on_connect_failures func=None connect_errors= ADDRESS_IN_USE Reruns test test returns RuntimeError exception contains one strings connect_errors This block executed when using function decorator arguments func None partial retry_on_connect_failures connect_errors=connect_errors wraps func wrapper args kwargs n_retries = tries_remaining = n_retries while True try func args kwargs except RuntimeError error any connect_error str error connect_error connect_errors tries_remaining -= tries_remaining == raise RuntimeError f Failing after n_retries retries error str error error time sleep random random continue raise wrapper Decorator retry upon certain Exceptions retry ExceptionToCheck tries= delay= skip_after_retries=False deco_retry f wraps f f_retry args kwargs mtries mdelay = tries delay while mtries try f args kwargs except ExceptionToCheck e msg = f e Retrying mdelay d seconds print msg time sleep mdelay mtries -= try f args kwargs except ExceptionToCheck e raise unittest SkipTest f Skipping after tries consecutive str e e skip_after_retries e f_retry true decorator deco_retry FIXME modernize these consistent make_tensor review including them torch testing Methods matrix generation random_square_matrix_of_rank l rank dtype=torch double device= cpu assert rank = l A = torch randn l l dtype=dtype device=device u s vh = torch linalg svd A full_matrices=False i range l i = rank s i = s i == s i = u s dtype unsqueeze - vh random_well_conditioned_matrix shape dtype device mean= sigma= Returns random rectangular matrix batch matrices singular values sampled Gaussian mean ` mean ` standard deviation ` sigma ` The smaller ` sigma ` better conditioned output matrix primitive_dtype = torch float torch float torch double torch double torch cfloat torch float torch cdouble torch double x = torch rand shape dtype=dtype device=device m = x size - n = x size - u _ vh = torch linalg svd x full_matrices=False s = torch randn shape - + min m n dtype=primitive_dtype dtype device=device sigma + mean \ sort - descending=True values dtype u s unsqueeze - vh Returns noncontiguous tensor same shape values t The noncontiguous tensor constructed such elements innermost dimension separated zeros whenever possible nans TODO consider more complicated noncontiguity schemes noncontiguous_like t Short-circuits t already noncontiguous t is_contiguous t Choose weird value won t accessed t dtype is_floating_point t dtype is_complex value = math nan t dtype == torch bool value = True value = result = t new_empty t shape + result = value result = t detach result = result result requires_grad_ t requires_grad result TODO remove prefer make_symmetric_matrices below random_symmetric_matrix l batches kwargs dtype = kwargs get dtype torch double device = kwargs get device cpu A = torch randn batches + l l dtype=dtype device=device A = A + A mT div_ A Creates symmetric matrix batch symmetric matrices Shape must square matrix batch square matrices make_symmetric_matrices shape device dtype assert shape - == shape - t = make_tensor shape device=device dtype=dtype t = t + t mT div_ t random_hermitian_matrix l batches kwargs dtype = kwargs get dtype torch double device = kwargs get device cpu A = torch randn batches + l l dtype=dtype device=device A = A + A mH div_ A random_symmetric_psd_matrix l batches kwargs Returns batch random symmetric positive-semi-definite matrices The shape result batch_dims + matrix_size matrix_size The following example creates tensor size x x x xdoctest +SKIP undefined variables matrices = random_symmetric_psd_matrix dtype=dtype device=device dtype = kwargs get dtype torch double device = kwargs get device cpu A = torch randn batches + l l dtype=dtype device=device A A mT random_hermitian_psd_matrix matrix_size batch_dims dtype=torch double device= cpu Returns batch random Hermitian positive-semi-definite matrices The shape result batch_dims + matrix_size matrix_size The following example creates tensor size x x x xdoctest +SKIP undefined variables matrices = random_hermitian_psd_matrix dtype=dtype device=device A = torch randn batch_dims + matrix_size matrix_size dtype=dtype device=device A A mH TODO remove prefer make_symmetric_pd_matrices below random_symmetric_pd_matrix matrix_size batch_dims kwargs dtype = kwargs get dtype torch double device = kwargs get device cpu A = torch randn batch_dims + matrix_size matrix_size dtype=dtype device=device torch matmul A A mT \ + torch eye matrix_size dtype=dtype device=device e- Creates symmetric positive-definite matrix batch such matrices make_symmetric_pd_matrices shape device dtype assert shape - == shape - t = make_tensor shape device=device dtype=dtype i = torch eye shape - device=device dtype=dtype e- t t mT + i random_hermitian_pd_matrix matrix_size batch_dims dtype device Returns batch random Hermitian positive-definite matrices The shape result batch_dims + matrix_size matrix_size The following example creates tensor size x x x xdoctest +SKIP undefined variables matrices = random_hermitian_pd_matrix dtype=dtype device=device A = torch randn batch_dims + matrix_size matrix_size dtype=dtype device=device A A mH + torch eye matrix_size dtype=dtype device=device Creates full rank matrix distinct singular values batch such matrices make_fullrank_matrices_with_distinct_singular_values shape device dtype requires_grad=False torch no_grad t = make_tensor shape device=device dtype=dtype u _ vh = torch linalg svd t full_matrices=False real_dtype = t real dtype t dtype is_complex t dtype k = min shape - shape - We choose singular values around one This make matrix well conditioned s = k+ s = torch arange k + dtype=real_dtype device=device s = - - ^k k+ s = - + s so singular values range This gives condition number which should good enough s reciprocal_ add_ Note singular values need ordered SVD so we don t need need sort S x = u s u dtype vh x requires_grad_ requires_grad x random_matrix rows columns batch_dims kwargs Return rectangular matrix batches rectangular matrices Parameters dtype - data type device - device kind singular - when True output will singular dtype = kwargs get dtype torch double device = kwargs get device cpu silent = kwargs get silent False singular = kwargs get singular False silent torch _C has_lapack torch ones rows columns dtype=dtype device=device A = torch randn batch_dims + rows columns dtype=dtype device=device A numel == A u _ vh = torch linalg svd A full_matrices=False k = min rows columns s = torch linspace k + k dtype=dtype device=device singular make matrix singular s k - = k increase order singularity so pivoting LU factorization will non-trivial s = u s unsqueeze - vh random_lowrank_matrix rank rows columns batch_dims kwargs Return rectangular matrix batches rectangular matrices given rank B = random_matrix rows rank batch_dims kwargs C = random_matrix rank columns batch_dims kwargs B matmul C _generate_indices_prefer_all_rows rows int cols int num_indices int - torch Tensor Generate indices row x cols matrix preferring least one index per row possible indices = type ignore var-annotated n_per_row = math ceil num_indices rows col_indices = list range cols r range rows Note can yield overlapping indices indices extend r c c random choices col_indices k=n_per_row torch tensor indices num_indices random_sparse_matrix rows columns density= kwargs Return rectangular random sparse matrix within given density The density result approaches given density size matrix increased relatively small value density specified higher than min rows columns rows columns non-singular matrices dtype = kwargs get dtype torch double device = kwargs get device cpu nonzero_elements = max min rows columns int rows columns density indices = _generate_indices_prefer_all_rows rows columns nonzero_elements values = torch randn nonzero_elements dtype=dtype device=device ensure diagonal dominates values = torch tensor -float i - j i j indices dtype=dtype device=device exp A = torch sparse_coo_tensor indices t values rows columns device=device A coalesce random_sparse_pd_matrix matrix_size density= kwargs Return random sparse positive-definite matrix given density The eigenvalues matrix defined arange matrix_size+ matrix_size Algorithm A = diag arange matrix_size+ matrix_size while A density smaller than required choose random i j range matrix_size theta pi R = rotation matrix i j theta A = R^T A R math torch = kwargs get torch globals torch dtype = kwargs get dtype torch double device = kwargs get device cpu data = i i float i + matrix_size i range matrix_size multiply data N i j cs sn left=True k range N left ik jk = k i k j ik jk = i k j k aik ajk = data get ik data get jk aik ajk = cs aik + sn ajk -sn aik + cs ajk aik data ik = aik data pop ik None ajk data jk = ajk data pop jk None target_nnz = density matrix_size matrix_size while len data target_nnz i = random randint matrix_size - j = random randint matrix_size - i = j theta = random uniform math pi cs = math cos theta sn = math sin theta multiply data matrix_size i j cs sn left=True multiply data matrix_size i j cs sn left=False icoords jcoords values = i j v sorted data items icoords append i jcoords append j values append v indices_tensor = torch tensor icoords jcoords torch sparse_coo_tensor indices_tensor values matrix_size matrix_size dtype=dtype device=device FIXME remove updating test suites using do_test_dtypes dtypes layout device dtype dtypes dtype = torch float out = torch zeros dtype=dtype layout=layout device=device assertIs dtype out dtype assertIs layout out layout assertEqual device out device FIXME remove updating test suites using do_test_empty_full dtypes layout device shape = torch Size check_value tensor dtype layout device value requires_grad assertEqual shape tensor shape assertIs dtype tensor dtype assertIs layout tensor layout assertEqual tensor requires_grad requires_grad tensor is_cuda device None assertEqual device tensor device value None fill = tensor new shape fill_ value assertEqual tensor fill get_int _dtype dtype module = join str dtype split - module torch int operator attrgetter module torch int default_dtype = torch get_default_dtype check_value torch empty shape default_dtype torch strided - None False check_value torch full shape - default_dtype torch strided - None False dtype dtypes rg dtype is_floating_point False int _dtype = get_int _dtype dtype v = torch empty shape dtype=dtype device=device layout=layout requires_grad=rg check_value v dtype layout device None rg out = v new check_value torch empty shape out=out device=device layout=layout requires_grad=rg dtype layout device None rg check_value v new_empty shape dtype layout device None False check_value v new_empty shape dtype=int _dtype device=device requires_grad=False int _dtype layout device None False check_value torch empty_like v dtype layout device None False check_value torch empty_like v dtype=int _dtype layout=layout device=device requires_grad=False int _dtype layout device None False dtype torch float layout = torch sparse_coo fv = v = torch full shape fv dtype=dtype layout=layout device=device requires_grad=rg check_value v dtype layout device fv rg check_value v new_full shape fv + dtype layout device fv + False out = v new check_value torch full shape fv + out=out device=device layout=layout requires_grad=rg dtype layout device fv + rg check_value v new_full shape fv + dtype=int _dtype device=device requires_grad=False int _dtype layout device fv + False check_value torch full_like v fv + dtype layout device fv + False check_value torch full_like v fv + dtype=int _dtype layout=layout device=device requires_grad=False int _dtype layout device fv + False FIXME improve load_tests documentation here running_script_path = None type ignore var-annotated set_running_script_path global running_script_path try running_file = os path abspath os path realpath sys argv running_file endswith py skip running file script running_script_path = running_file except Exception pass check_test_defined_in_running_script test_case running_script_path None test_case_class_file = os path abspath os path realpath inspect getfile test_case __class__ assert test_case_class_file == running_script_path f Class loaded TestCase test_case id \ f defined running script running_script_path test_case_class_file Did you \ accidentally unittest TestCase another file load_tests loader tests pattern set_running_script_path test_suite = unittest TestSuite test_group tests DISABLE_RUNNING_SCRIPT_CHK test test_group check_test_defined_in_running_script test test_group _tests test_suite addTest test_group test_suite FIXME document move test_serialization BytesIOContext io BytesIO __enter__ __exit__ args pass Tentative value nondet_tol gradcheck when backward implementation relies nondeterministic operations i e those listed here https pytorch org docs stable generated torch use_deterministic_algorithms html For more information see https github com pytorch pytorch issues GRADCHECK_NONDET_TOL = e- TEST_WITH_SLOW_GRADCHECK bool = TestEnvironment def_flag TEST_WITH_SLOW_GRADCHECK env_var= PYTORCH_TEST_WITH_SLOW_GRADCHECK skipIfSlowGradcheckEnv = unittest skipIf TEST_WITH_SLOW_GRADCHECK Tests don t use gradcheck don t need run slow_gradcheck CI gradcheck fn inputs kwargs Wrapper around gradcheck enables certain keys default Use testing-internal gradcheck instead autograd gradcheck so new features like vmap forward-mode AD tested default We create wrapper because we d like keep new checks disabled default public-facing api avoid breaking user code All PyTorch devs doing testing should use wrapper instead autograd gradcheck default_values = check_batched_grad True fast_mode True TEST_WITH_SLOW_GRADCHECK default_values fast_mode = False key value default_values items default value override values explicitly set None k = kwargs get key kwargs key = k k None value torch autograd gradcheck fn inputs kwargs gradgradcheck fn inputs grad_outputs=None kwargs Wrapper around gradgradcheck enables certain keys default See gradcheck above explanation why we need something like All PyTorch devs doing testing should use wrapper instead autograd gradgradcheck default_values = check_batched_grad True fast_mode True TEST_WITH_SLOW_GRADCHECK default_values fast_mode = False key value default_values items default value override values explicitly set None k = kwargs get key kwargs key = k k None value torch autograd gradgradcheck fn inputs grad_outputs kwargs _assertGradAndGradgradChecks test_case apply_fn inputs kwargs call assert function rather than returning bool since s nicer we get whether failed gradcheck gradgradcheck test_case assertTrue gradcheck apply_fn inputs kwargs test_case assertTrue gradgradcheck apply_fn inputs kwargs contextmanager set_cwd path str - Iterator None old_cwd = os getcwd try os chdir path yield finally os chdir old_cwd FIXME delete Using toleranceOverride specific your test recommended way doing These just some values worked test_nn dtype prec_DONTUSE = torch float e- torch double e- torch half e- torch bfloat e- FIXME move test_sparse sparse utils This wrapper wraps test run test twice one coalesced=True another coalesced=False coalesced uncoalesced sparse tensors coalescedonoff f wraps f wrapped args kwargs f args kwargs coalesced=True f args kwargs coalesced=False wrapped is_coalesced_indices s indices = s _indices hash_coeffs = + s shape s sparse_dim - - hash_indices = torch tensor hash_coeffs device=s device cumprod - flip - s sparse_dim hash_indices unsqueeze_ - hash_indices = indices hash_indices sum hash_indices = indices hash_indices check indices sorted res = torch allclose hash_indices hash_indices sort check there no repeated indices res = res torch allclose hash_indices hash_indices unique res contextlib contextmanager disable_gc gc isenabled try gc disable yield finally gc enable yield find_library_location lib_name str - Path shared library file installed folder exist file build folder torch_root = Path torch __file__ resolve parent path = torch_root lib lib_name os path exists path path torch_root = Path __file__ resolve parents torch_root build lib lib_name skip_but_pass_in_sandcastle reason Similar unittest skip however sandcastle environment just passes test instead avoid creating tasks complaining about tests skipping continuously decorator func IS_SANDCASTLE func __unittest_skip__ = True func __unittest_skip_why__ = reason func wraps func wrapper args kwargs print f Skipping func __name__ sandcastle following reason reason file=sys stderr wrapper decorator mock_wrapper method Returns function calls real implementation method addition passing args mock object mock = MagicMock wraps method wrapper args kwargs mock args kwargs method args kwargs wrapper mock = mock type ignore attr-defined wrapper get_tensors_from args kwargs Returns set all Tensor objects given args kwargs set arg arg args isinstance arg Tensor + v v kwargs values isinstance v Tensor Returns scalar tensor representation list integer byte values bytes_to_scalar byte_list list int dtype torch dtype device torch device dtype_to_ctype dict torch dtype Any = torch int ctypes c_int torch uint ctypes c_uint torch uint ctypes c_uint torch uint ctypes c_uint torch uint ctypes c_uint torch int ctypes c_int torch int ctypes c_int torch int ctypes c_int torch bool ctypes c_bool torch float ctypes c_float torch complex ctypes c_float torch float ctypes c_double torch complex ctypes c_double ctype = dtype_to_ctype dtype num_bytes = ctypes sizeof ctype check_bytes byte_list byte byte_list assert = byte = dtype is_complex assert len byte_list == num_bytes check_bytes byte_list real = ctype from_buffer ctypes c_byte num_bytes byte_list num_bytes value imag = ctype from_buffer ctypes c_byte num_bytes byte_list num_bytes value res = real + j imag assert len byte_list == num_bytes check_bytes byte_list res = ctype from_buffer ctypes c_byte num_bytes byte_list value torch tensor res device=device dtype=dtype copy_func f Based http stackoverflow com Glenn Maynard g = types FunctionType f __code__ f __globals__ name=f __name__ argdefs=f __defaults__ closure=f __closure__ g = functools update_wrapper g f g __kwdefaults__ = f __kwdefaults__ type ignore attr-defined g xfail_inherited_tests tests Given list test names which defined superclass decorates mark them expected failure This useful you doing poor man s parameterized tests subclassing generic test deco cls t tests NB expectedFailure operates mutating method question which why you have copy function first setattr cls t unittest expectedFailure copy_func getattr cls t cls deco skip_but_pass_in_sandcastle_if condition reason Similar unittest skipIf however sandcastle environment just passes test instead avoid creating tasks complaining about tests skipping continuously decorator func condition IS_SANDCASTLE wraps func wrapper args kwargs print f Skipping func __name__ sandcastle following reason reason file=sys stderr wrapper func __unittest_skip__ = True func __unittest_skip_why__ = reason func decorator dtype_name dtype Returns pretty name dtype e g torch int - int str dtype split functools lru_cache get_cycles_per_ms - float Measure approximate number cycles per millisecond torch cuda _sleep measure - float start = torch cuda Event enable_timing=True end = torch cuda Event enable_timing=True start record torch cuda _sleep end record end synchronize cycles_per_ms = start elapsed_time end cycles_per_ms Get values remove max min avg This avoid system disturbance skew results e g very first cuda call likely does bunch init which takes much longer than subsequent calls Tested both Tesla V Quadro GP Titan RTX RTX GPUs seems stable values Therefore we enable caching using lru_cache decorator above num = vals = measure _ range num vals = sorted vals mean vals num - OpInfo utils T = TypeVar T first_sample unittest TestCase samples Iterable T - T Returns first sample iterable samples like those returned OpInfo The test will skipped no samples available try next iter samples except StopIteration e raise unittest SkipTest Skipped Need least sample input e helper method recursively clone tensor-type input operators tested OpInfo clone_input_helper input isinstance input torch Tensor torch clone input isinstance input Sequence tuple map clone_input_helper input input contextmanager custom_op opname symbolic_fn opset_version Context manager decorator test ONNX export custom operator try register_custom_op_symbolic opname symbolic_fn opset_version yield finally unregister_custom_op_symbolic opname opset_version outs_and_grads fn graph_inps inps outs = fn graph_inps out pytree tree_leaves outs isinstance out torch Tensor out requires_grad out sum backward retain_graph=True grads = inp grad inp pytree tree_leaves inps isinstance inp torch Tensor inp pytree tree_leaves inps isinstance inp torch Tensor inp grad = None outs grads compare_equal_outs_and_grads test m m inps r g = outs_and_grads m inps inps r g = outs_and_grads m inps inps test assertEqual r r test assertEqual g g TestGradients TestCase exact_dtype = True Copies inputs inplace operations avoid inplace modifications leaves requiring gradient _get_safe_inplace inplace_variant wraps inplace_variant _fn t args kwargs inplace_variant t clone args kwargs _fn _check_helper device dtype op variant check check_forward_ad=False check_backward_ad=True check_batched_grad=None check_batched_forward_grad=False assert check gradcheck bwgrad_bwgrad fwgrad_bwgrad NB check_backward_ad does affect gradgradcheck always True variant None skipTest Skipped Variant implemented op supports_dtype dtype torch device device type skipTest f Skipped op name does support dtype str dtype is_inplace variant hasattr variant __wrapped__ variant __wrapped__ op get_inplace variant op get_inplace include_conjugated_inputs = op test_conjugated_samples dtype is_complex samples = op sample_inputs device dtype requires_grad=True include_conjugated_inputs=include_conjugated_inputs small_inputs_only=TEST_WITH_SLOW_GRADCHECK sample samples sample broadcasts_input is_inplace variant continue Gradcheck expects tensors its input autograd actually supports tensorlists tensors passed kwargs The following creates function accepts just tensors require grad varargs then recomposes them back into original input Creates gradcheck inputs identifying tensors requiring grad all_args = None is_iterable_of_tensors sample input all_args = chain sample input sample args sample kwargs values all_args = tuple chain sample input sample args sample kwargs values type ignore assignment gradcheck_args = tuple x x all_args isinstance x torch Tensor x requires_grad type ignore union-attr Verifies sample input tensors should have no grad This may happen same tensor used two different SampleInputs t gradcheck_args assertIsNone t grad A sampled input has gradient before running autograd This usually means least one input tensor reused across different SampleInputs Please create new tensor each SampleInput _input_recomposition_helper inputs inp input_idx is_iterable_of_tensors inp tensor_list = x inp isinstance x torch Tensor x requires_grad tensor_list append inputs input_idx input_idx = input_idx + tensor_list append x tensor_list input_idx isinstance inp torch Tensor inp requires_grad inputs input_idx input_idx + inp input_idx fn inputs Puts inputs back into sample properly positional_args = input_idx = inp input_idx = _input_recomposition_helper inputs sample input input_idx positional_args append inp x sample args inp input_idx = _input_recomposition_helper inputs x input_idx positional_args append inp Recreates kwargs kwargs = k v sample kwargs items inp input_idx = _input_recomposition_helper inputs v input_idx kwargs k = inp output = op gradcheck_wrapper variant positional_args kwargs sample output_process_fn_grad None sample output_process_fn_grad output output check == gradcheck check_batched_grad None check_batched_grad = op check_batched_grad assertTrue gradcheck fn gradcheck_args check_batched_grad=check_batched_grad check_grad_dtypes=True nondet_tol=op gradcheck_nondet_tol fast_mode=op gradcheck_fast_mode check_forward_ad=check_forward_ad check_backward_ad=check_backward_ad check_undefined_grad=True check_batched_forward_grad=check_batched_forward_grad check bwgrad_bwgrad fwgrad_bwgrad gradgrad check assertFalse check_forward_ad msg= Cannot run forward AD check gradgradcheck gen_non_contig_grad_outputs False True kwargs = gen_non_contig_grad_outputs gen_non_contig_grad_outputs check_batched_grad op check_batched_gradgrad check_grad_dtypes True nondet_tol op gradcheck_nondet_tol fast_mode op gradcheck_fast_mode check == fwgrad_bwgrad kwargs check_fwd_over_rev = True kwargs check_rev_over_rev = False kwargs check_batched_grad = False kwargs check_undefined_grad = False assertTrue gradgradcheck fn gradcheck_args kwargs assertTrue False msg= Unknown check requested _grad_test_helper device dtype op variant check_forward_ad=False check_backward_ad=True check_batched_grad=None check_batched_forward_grad=False _check_helper device dtype op variant gradcheck check_forward_ad=check_forward_ad check_backward_ad=check_backward_ad check_batched_grad=check_batched_grad check_batched_forward_grad=check_batched_forward_grad _skip_helper op device dtype dtype op supported_backward_dtypes torch device device type skipTest Skipped Op doesn t support autograd dtype op supports_autograd op supports_forward_ad skipTest Skipped autograd supported make_lazy_class cls lazy_init cb _cb = cb _value = None cls __init__ = lazy_init basename add sub mul truediv floordiv mod divmod pow lshift rshift xor neg pos abs invert eq ne lt le gt ge bool int index name = f __ basename __ inner_wrapper name use_operator = basename bool int wrapped args kwargs _cb None _value = _cb _cb = None use_operator getattr _value name args kwargs getattr operator name _value args kwargs wrapped setattr cls name inner_wrapper name cls Base TestCase NT tests used define common helpers etc NestedTensorTestCase TestCase assertEqualIgnoringNestedInts b unbinding NJTs allows us compare them essentially equal without caring about exact nested int comparison _unbind_njts x isinstance x torch Tensor x is_nested x layout == torch jagged x unbind x assertEqual pytree tree_map _unbind_njts pytree tree_map _unbind_njts b assertEqualNoncontigAware b assertEqual doesn t take into account lengths so hack around comparing unbound components shapes assertEqualIgnoringNestedInts b _get_njt_shapes x x shape isinstance x torch Tensor x is_nested None a_shapes = pytree tree_map _get_njt_shapes b_shapes = pytree tree_map _get_njt_shapes b assertEqual a_shapes b_shapes contextlib contextmanager branch_nested_state Context manager branch restore nested tensor state nested_tensor_module = torch nested _internal nested_tensor original_tensor_symint_registry = nested_tensor_module _tensor_symint_registry copy original_tensor_id_counter = nested_tensor_module _tensor_id_counter try yield finally nested_tensor_module _tensor_id_counter = original_tensor_id_counter nested_tensor_module _tensor_symint_registry = original_tensor_symint_registry make_lazy_class LazyVal pass munge_exc e suppress_suffix=True suppress_prefix=True file=None skip= torch _dynamo trace_rules _as_posix_path file None file = inspect stack + skip filename skip one frame file = _as_posix_path file s = _as_posix_path str e Remove everything looks like stack frames NOT file repl_frame m m group = file Don t accept top-level even script these will wobble depending how testing script invoked m group == module m group s = re sub r File ^ + line \d+ + \n +\n + ~^ + \n + repl_frame s s = re sub r line \d+ line N s s = re sub r py \d+ py N s s = re sub r https a-zA-Z - _ - + r https \ s s = re sub file _as_posix_path os path basename file s s = re sub _as_posix_path os path join os path dirname torch __file__ s suppress_suffix s = re sub r \n Set TORCH_LOGS + s flags=re DOTALL s = re sub r \n You can suppress exception + s flags=re DOTALL s = re sub r \n Set TORCHDYNAMO_VERBOSE= + s flags=re DOTALL suppress_prefix s = re sub r Cannot export model +\n\n s s = re sub r +$ s flags=re MULTILINE s contextmanager check_leaked_tensors limit= matched_type=torch Tensor Wrap around operations you want ensure leaking tensor memory This code intentionally ignores other reference cycles which can benign which we have plenty pytorch code It focuses any reference cycles directly indirectly result holding Tensor alive since likely more serious leak than typical python refcycles limit specifies how many tensors dump debug graphs default= match_obj obj isinstance obj matched_type try gc collect gc set_debug gc DEBUG_SAVEALL garbage_objs = type ignore var-annotated run user code after cleaning any existing refcycles then check new ones also allow usercode check garbage objs e g assertion after exiting ctxmgr yield garbage_objs gc collect garbage_objs extend filter match_obj gc garbage num_garbage_objs = len garbage_objs num_garbage_objs warnings warn f num_garbage_objs tensors found garbage Did you introduce reference cycle stacklevel= try objgraph type ignore import-not-found import-untyped warnings warn f Dumping first limit objgraphs leaked matched_type s rendered png stacklevel= g garbage_objs limit objgraph show_backrefs g max_depth= except ImportError warnings warn ` pip install objgraph ` enable memory leak debugging stacklevel= finally gc set_debug remove_cpp_extensions_build_root Removes default root folder under which extensions built default_build_root = cpp_extension get_default_build_root os path exists default_build_root IS_WINDOWS rmtree returns permission error WinError Access denied Windows workaround subprocess run rm -rf default_build_root stdout=subprocess PIPE shutil rmtree default_build_root ignore_errors=True install_cpp_extension extension_root Wipe build install dirs they exist build_dir = os path join extension_root build install_dir = os path join extension_root install d build_dir install_dir os path exists d shutil rmtree d Build extension cmd = sys executable -m pip install extension_root -v -- no-build-isolation -- root install_dir return_code = shell cmd cwd=extension_root env=os environ return_code = raise RuntimeError f build failed cpp extension extension_root mod_install_dir = None install directory one named site-packages root directories _ os walk install_dir directory directories -packages directory mod_install_dir = os path join root directory mod_install_dir None raise RuntimeError f installation failed cpp extension extension_root mod_install_dir sys path sys path insert mod_install_dir Decorator provide helper load inline extensions temp directory scoped_load_inline func wraps func wrapper args kwargs load_inline args kwargs IS_WINDOWS TODO xmfan even using TemporaryDirectoryName will result permission error cpp_extension load_inline args kwargs assert build_directory kwargs TemporaryDirectoryName temp_dir_name kwargs get verbose False print f Using temporary extension directory temp_dir_name file=sys stderr kwargs build_directory = temp_dir_name cpp_extension load_inline args kwargs func args load_inline=load_inline kwargs wrapper recover_orig_fp _precision fn contextlib contextmanager recover old_mkldnn_conv_p = torch backends mkldnn conv fp _precision type ignore attr-defined old_mkldnn_rnn_p = torch backends mkldnn rnn fp _precision type ignore attr-defined old_mkldnn_matmul_p = torch backends mkldnn matmul fp _precision type ignore attr-defined old_cudnn_conv_p = torch backends cudnn conv fp _precision type ignore attr-defined old_cudnn_rnn_p = torch backends cudnn rnn fp _precision type ignore attr-defined old_cuda_matmul_p = torch backends cuda matmul fp _precision try yield finally torch backends mkldnn conv fp _precision = old_mkldnn_conv_p type ignore attr-defined torch backends mkldnn rnn fp _precision = old_mkldnn_rnn_p type ignore attr-defined torch backends mkldnn matmul fp _precision = old_mkldnn_matmul_p type ignore attr-defined torch backends cudnn conv fp _precision = old_cudnn_conv_p type ignore attr-defined torch backends cudnn rnn fp _precision = old_cudnn_rnn_p type ignore attr-defined torch backends cuda matmul fp _precision = old_cuda_matmul_p recover fn skipIfPythonVersionMismatch predicate vi = sys version_info dec_fn fn wraps fn wrap_fn args kwargs predicate vi major vi minor vi micro fn args kwargs raise unittest SkipTest Python version mismatch wrap_fn dec_fn Decorator patch multiple test members duration subtest patch_test_members updates dict str Any decorator test_func wraps test_func wrapper args kwargs Store original values specified members original_values = member getattr member member updates Update members before running subtest member value updates items setattr member value Run test function allowing subtests run try test_func args kwargs finally Restore original values specified members after subtest finishes member original_value original_values items setattr member original_value wrapper decorator