Owner s module cpp math pathlib Path torch torch testing _internal common_device_type deviceCountAtLeast instantiate_device_type_tests onlyCPU onlyCUDA torch testing _internal common_utils install_cpp_extension IS_WINDOWS run_tests TestCase xfailIfTorchDynamo TODO Fix error Windows LINK error LNK unresolved external symbol PyInit__C IS_WINDOWS TestLibtorchAgnostic TestCase classmethod setUpClass cls try libtorch_agnostic noqa F except Exception install_cpp_extension extension_root=Path __file__ parent parent onlyCPU test_slow_sgd device libtorch_agnostic param = torch rand device=device grad = torch rand_like param weight_decay = lr = maximize = False new_param = libtorch_agnostic ops sgd_out_of_place param grad weight_decay lr maximize torch _fused_sgd_ param grad weight_decay=weight_decay momentum= lr=lr dampening= nesterov=False maximize=maximize is_first_step=False assertEqual new_param param onlyCUDA test_identity_does_not_hog_memory device libtorch_agnostic _run_identity prior_mem t = torch rand device=device assertGreater torch cuda memory_allocated device prior_mem identi_t = libtorch_agnostic ops identity t assert identi_t t init_mem = torch cuda memory_allocated device _ range _run_identity init_mem curr_mem = torch cuda memory_allocated device assertEqual curr_mem init_mem test_exp_neg_is_leaf device libtorch_agnostic t = torch rand device=device t = torch rand device=device t = torch rand device=device exp neg is_leaf = libtorch_agnostic ops exp_neg_is_leaf t t t assertEqual exp torch exp t assertEqual neg torch neg t assertEqual is_leaf t is_leaf test_my_abs device libtorch_agnostic t = torch rand device=device - res = libtorch_agnostic ops my_abs t assertEqual res torch abs t _make_cuda_tensors prior_mem cuda_t = libtorch_agnostic ops my_abs t assertGreater torch cuda memory_allocated device prior_mem assertEqual cuda_t torch abs t t is_cuda init_mem = torch cuda memory_allocated device _ range _make_cuda_tensors init_mem curr_mem = torch cuda memory_allocated device assertEqual curr_mem init_mem test_neg_exp device libtorch_agnostic t = torch rand device=device - res = libtorch_agnostic ops neg_exp t assertEqual res torch neg torch exp t _make_cuda_tensors prior_mem cuda_res = libtorch_agnostic ops neg_exp t assertGreater torch cuda memory_allocated device prior_mem assertEqual cuda_res torch neg torch exp t t is_cuda init_mem = torch cuda memory_allocated device _ range _make_cuda_tensors init_mem curr_mem = torch cuda memory_allocated device assertEqual curr_mem init_mem test_divide_neg_exp device libtorch_agnostic t = torch zeros device=device - res = libtorch_agnostic ops divide_neg_exp t assertEqual res torch neg t torch exp t _make_cuda_tensors prior_mem cuda_res = libtorch_agnostic ops divide_neg_exp t assertGreater torch cuda memory_allocated device prior_mem assertEqual cuda_res torch neg t torch exp t t is_cuda init_mem = torch cuda memory_allocated device _ range _make_cuda_tensors init_mem curr_mem = torch cuda memory_allocated device assertEqual curr_mem init_mem test_is_contiguous device libtorch_agnostic t = torch rand device=device assertTrue libtorch_agnostic ops is_contiguous t assertFalse libtorch_agnostic ops is_contiguous t transpose TODO Debug torch _dynamo exc TorchRuntimeError Dynamo failed run FX node fake tensors call_function libtorch_agnostic my_ones_like default FakeTensor size= cpu got AssertionError tensor s device must ` meta ` got cpu instead xfailIfTorchDynamo test_my_ones_like device libtorch_agnostic t = torch rand device=device - cpu_t = libtorch_agnostic ops my_ones_like t cpu assertEqual cpu_t torch ones_like t device= cpu _make_cuda_tensors prior_mem cuda_t = libtorch_agnostic ops my_ones_like t device assertGreater torch cuda memory_allocated device prior_mem assertEqual cuda_t torch ones_like t device=device t is_cuda init_mem = torch cuda memory_allocated device _ range _make_cuda_tensors init_mem curr_mem = torch cuda memory_allocated device assertEqual curr_mem init_mem test_my_transpose device libtorch_agnostic t = torch rand device=device out = libtorch_agnostic ops my_transpose t assertEqual out torch transpose t assertRaisesRegex RuntimeError API call failed libtorch_agnostic ops my_transpose t test_my_empty_like device libtorch_agnostic deterministic = torch are_deterministic_algorithms_enabled try set use_deterministic_algorithms fill uninitialized memory torch use_deterministic_algorithms True t = torch rand device=device out = libtorch_agnostic ops my_empty_like t assertTrue id out = id t assertEqual out torch empty_like t finally torch use_deterministic_algorithms deterministic onlyCPU test_my_zero_ device libtorch_agnostic t = torch rand device=device out = libtorch_agnostic ops my_zero_ t assertEqual id out id t assertEqual out torch zeros_like t test_my_amax device libtorch_agnostic t = torch rand device=device out = libtorch_agnostic ops my_amax t assertEqual out torch amax t test_my_amax_vec device libtorch_agnostic t = torch rand device=device out = libtorch_agnostic ops my_amax_vec t assertEqual out torch amax t test_my_is_cpu device libtorch_agnostic t = torch rand device=device out = libtorch_agnostic ops my_is_cpu t assertEqual out t is_cpu test_fill_infinity device libtorch_agnostic t = torch rand device=device out = libtorch_agnostic ops fill_infinity t assertEqual id out id t expected = torch full_like t math inf assertEqual out expected onlyCPU test_default_constructor libtorch_agnostic defined_tensor_is_defined = libtorch_agnostic ops test_default_constructor True assertTrue defined_tensor_is_defined undefined_tensor_is_defined = libtorch_agnostic ops test_default_constructor False assertFalse undefined_tensor_is_defined test_my_pad device libtorch_agnostic t = torch rand device=device out = libtorch_agnostic ops my_pad t expected = torch nn functional pad t constant assertEqual out expected test_my_narrow device libtorch_agnostic t = torch randn device=device dim = start = length = out = libtorch_agnostic ops my_narrow t dim start length expected = torch narrow t dim start length assertEqual out expected onlyCUDA deviceCountAtLeast test_device_guard device libtorch_agnostic device_index = out = libtorch_agnostic ops test_device_guard device_index assertEqual out device_index onlyCUDA deviceCountAtLeast test_device_guard_set_index device libtorch_agnostic This test creates DeviceGuard index then sets index returns current device should out = libtorch_agnostic ops test_device_guard_set_index assertEqual out onlyCUDA test_stream device libtorch_agnostic stream = torch cuda Stream device = torch cuda current_device stream expected_stream_id = torch cuda current_stream stream_id stream_id = libtorch_agnostic ops test_stream device assertEqual stream_id expected_stream_id onlyCUDA deviceCountAtLeast test_get_current_device_index device libtorch_agnostic prev_device = torch cuda current_device try expected_device = torch cuda set_device expected_device current_device = libtorch_agnostic ops test_get_current_device_index assertEqual current_device expected_device finally torch cuda set_device prev_device test_my_new_empty_dtype_variant device libtorch_agnostic deterministic = torch are_deterministic_algorithms_enabled try set use_deterministic_algorithms fill uninitialized memory torch use_deterministic_algorithms True t = torch randn device=device out = libtorch_agnostic ops my_new_empty_dtype_variant t ref_out = t new_empty dtype=torch bfloat assertEqual out ref_out exact_device=True finally torch use_deterministic_algorithms deterministic test_my_new_zeros_dtype_variant device libtorch_agnostic t = torch randn device=device out = libtorch_agnostic ops my_new_zeros_dtype_variant t ref_out = t new_zeros dtype=torch float assertEqual out ref_out exact_device=True test_my_copy_ device libtorch_agnostic dst = torch empty device=device src = torch randn device=device result = libtorch_agnostic ops my_copy_ dst src False expected = src assertEqual result expected assertEqual result data_ptr dst data_ptr test_my_clone device libtorch_agnostic t = torch randn device=device result = libtorch_agnostic ops my_clone t expected = t clone assertEqual result expected assertNotEqual result data_ptr expected data_ptr assertEqual result stride expected stride instantiate_device_type_tests TestLibtorchAgnostic globals except_for=None __name__ == __main__ run_tests