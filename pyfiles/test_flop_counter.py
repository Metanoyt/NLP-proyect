Owner s module unknown ruff noqa F functools unittest torch torch nn functional F torch utils flop_counter torch _subclasses fake_tensor FakeTensorMode torch testing _internal common_cuda PLATFORM_SUPPORTS_CUDNN_ATTENTION PLATFORM_SUPPORTS_FLASH_ATTENTION PLATFORM_SUPPORTS_FP PLATFORM_SUPPORTS_MEM_EFF_ATTENTION torch testing _internal common_device_type e m _type torch testing _internal common_utils run_tests TEST_WITH_TORCHDYNAMO TestCase try torchvision models torchvision_models HAS_TORCHVISION = True except ImportError HAS_TORCHVISION = False skipIfNoTorchVision = unittest skipIf HAS_TORCHVISION no torchvision HAS_CUDA = torch cuda is_available FlopCounterMode args kwargs torch utils flop_counter FlopCounterMode args kwargs display=False get_total_flops mode str sum v _ v mode flop_counts Global items T shape requires_grad=False torch randn shape requires_grad=requires_grad unittest skipIf TEST_WITH_TORCHDYNAMO torchdynamo doesn t work __torch_dispatch__ right now TestFlopCounter TestCase test_flop_counter_variety mod = torch nn Linear FlopCounterMode mode torch mm T T torch addmm T T T beta= alpha= torch matmul T T torch einsum ab bc- ac T T mod T assertExpectedInline get_total_flops mode test_op FlopCounterMode mode torch mm T T = assertExpectedInline get_total_flops mode mode torch bmm T T = assertExpectedInline get_total_flops mode mode torch addmm T T T torch addmm T T T torch addmm T T T = assertExpectedInline get_total_flops mode mode torch baddbmm T T T = assertExpectedInline get_total_flops mode mode torch conv d T T padding= out_image_size = kernel_size = c_out = c_in = out_image_size kernel_size c_out c_in NB I don t think properly accounts padding assertExpectedInline get_total_flops mode mode torch conv d T T padding= out_image_size = kernel_size = c_out = c_in = out_image_size kernel_size c_out c_in NB I don t think properly accounts padding assertExpectedInline get_total_flops mode test_backward FlopCounterMode mode = T requires_grad=True = torch mm T = unsqueeze expand = torch bmm T sum backward assertExpectedInline get_total_flops mode test_backward_reset FlopCounterMode mode = T requires_grad=True mm t sum backward mm t sum backward assertExpectedInline get_total_flops mode test_torchscript foo x torch mm x x FlopCounterMode mode foo T unscripted_flops = get_total_flops mode ts_foo = torch jit script foo mode ts_foo T assertEqual unscripted_flops get_total_flops mode test_autograd_op _CustomOp torch autograd Function staticmethod forward ctx input torch Tensor - torch Tensor torch mm input input staticmethod backward ctx grad_output torch Tensor - torch Tensor torch mm grad_output grad_output + torch mm grad_output grad_output = T requires_grad=True FlopCounterMode mode = _CustomOp apply sum backward assertExpectedInline get_total_flops mode test_conv_backwards_as_decomposition conv backwards decomposition conv forwards onlyConvs torch autograd Function staticmethod forward inp weight transposed transposed F conv d inp weight F conv_transpose d inp weight staticmethod setup_context ctx inputs output inp weight transposed = inputs ctx save_for_backward inp weight ctx transposed = transposed staticmethod backward ctx grad_out inp weight = ctx saved_tensors ctx transposed grad_inp = F conv_transpose d grad_out weight grad_weight = F conv d inp grad_out grad_inp grad_weight None grad_inp = F conv d grad_out weight grad_weight = F conv d grad_out transpose inp transpose grad_inp grad_weight transpose None torch func grad x = torch randn dtype=torch float weight = torch randn dtype=torch float boring_conv x weight transposed transposed F conv d x weight pow sum F conv_transpose d x weight pow sum only_convs x weight transposed onlyConvs apply x weight transposed pow sum boring_grads = grad boring_conv argnums= x weight True fun_grads = grad only_convs argnums= x weight True assertEqual boring_grads fun_grads test_convs assert_equivalence f expected_forward=None FlopCounterMode mode f conv_forward_flops = mode get_flop_counts Global torch ops aten convolution conv_backward_flops = mode get_flop_counts Global torch ops aten convolution_backward assertEqual conv_forward_flops conv_backward_flops expected_forward None assertEqual conv_forward_flops expected_forward x = torch rand requires_grad=True weight = torch randn requires_grad=True assert_equivalence lambda F conv_transpose d x weight sum backward x = torch rand requires_grad=True weight = torch randn requires_grad=True assert_equivalence lambda F conv d x weight sum backward in_channels out_channels groups x = torch rand in_channels requires_grad=True weight = torch randn out_channels in_channels requires_grad=True assert_equivalence lambda F conv d x weight sum backward transposed_weight = torch randn in_channels out_channels requires_grad=True assert_equivalence lambda F conv_transpose d x transposed_weight sum backward skipIfNoTorchVision test_module resnet = torchvision_models resnet FlopCounterMode resnet mode = T requires_grad=True resnet sum backward assertExpectedInline get_total_flops mode layer _conv_flops = mode flop_counts ResNet layer torch ops aten convolution layer _conv_back_flops = mode flop_counts ResNet layer torch ops aten convolution_backward assertExpectedInline str layer _conv_flops assertExpectedInline str layer _conv_back_flops test_conv_transpose_loop x = torch rand model = torch nn ConvTranspose d stride= FlopCounterMode mode _ range out = model x out sum backward assertExpectedInline str mode get_total_flops test_custom mode = FlopCounterMode custom_mapping= torch ops aten add lambda args out_shape mode = T + assertExpectedInline get_total_flops mode count args out_val out_val numel count _get_raw = True mode = FlopCounterMode custom_mapping= torch ops aten add count mode = T + assertExpectedInline get_total_flops mode test_noop FlopCounterMode mode T cos unittest skipIf HAS_CUDA CUDA available unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION PLATFORM_SUPPORTS_MEM_EFF_ATTENTION PLATFORM_SUPPORTS_CUDNN_ATTENTION Does support all SDPA backends pre-SM hardware CUDA test_sdpa batch_size = n_heads = seq_len_q = seq_len_k = head_dim = head_dim_v = dtype = torch float torch manual_seed get_flops batch_size n_heads seq_len_q seq_len_k head_dim head_dim_v dtype backend with_backward=False query = torch randn batch_size n_heads seq_len_q head_dim device= cuda dtype=dtype requires_grad=True key = torch randn batch_size n_heads seq_len_k head_dim device= cuda dtype=dtype requires_grad=True value = torch randn batch_size n_heads seq_len_k head_dim_v device= cuda dtype=dtype requires_grad=True backend == math backend = torch backends cuda sdp_kernel enable_flash=False enable_math=True enable_mem_efficient=False enable_cudnn=False backend == flash backend = torch backends cuda sdp_kernel enable_flash=True enable_math=False enable_mem_efficient=False enable_cudnn=False backend == mem_efficient backend = torch backends cuda sdp_kernel enable_flash=False enable_math=False enable_mem_efficient=True enable_cudnn=False backend == cudnn backend = torch backends cuda sdp_kernel enable_flash=False enable_math=False enable_mem_efficient=False enable_cudnn=True mode = FlopCounterMode backend mode out = F scaled_dot_product_attention query key value dropout_p= is_causal=True with_backward out sum backward int get_total_flops mode Sets seq_len_q == seq_len_k dim_q == dim_v run_uniform_flops = functools partial get_flops batch_size n_heads seq_len_q seq_len_q head_dim head_dim dtype flops = run_uniform_flops backend with_backward=False backend math flash mem_efficient cudnn flops_fw_math flops_fw_flash flops_fw_efficient flops_fw_cudnn = flops assertEqual flops_fw_math flops_fw_flash assertEqual flops_fw_math flops_fw_efficient assertEqual flops_fw_math flops_fw_cudnn assertExpectedInline str flops_fw_math flops = run_uniform_flops backend with_backward=True backend math flash mem_efficient cudnn flops_fw_bw_math flops_fw_bw_flash flops_fw_bw_efficient flops_fw_bw_cudnn = flops assertEqual flops_fw_math flops_fw_bw_math assertEqual flops_fw_math flops_fw_bw_flash assertEqual flops_fw_bw_flash flops_fw_bw_efficient assertEqual flops_fw_bw_flash flops_fw_bw_cudnn run_nonuniform_flops = functools partial get_flops batch_size n_heads seq_len_q seq_len_k head_dim head_dim_v dtype Flash does support non-uniform attention i e seq_len_q = seq_len_k dim_q = dim_v non_uniform_backends = math mem_efficient flops = run_nonuniform_flops backend with_backward=False backend non_uniform_backends flops_fw_math flops_fw_efficient = flops assertEqual flops_fw_math flops_fw_efficient assertExpectedInline str flops_fw_math flops = run_nonuniform_flops backend with_backward=True backend non_uniform_backends flops_fw_bw_math flops_fw_bw_efficient = flops assertExpectedInline str flops_fw_bw_math assertExpectedInline str flops_fw_bw_efficient unittest skipIf HAS_CUDA CUDA available unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION PLATFORM_SUPPORTS_MEM_EFF_ATTENTION Does support all SDPA backends pre-SM hardware CUDA test_sdpa_nested_tensor get_flops q k v backend with_backward=False mode = FlopCounterMode backend == math backend = torch backends cuda sdp_kernel enable_flash=False enable_math=True enable_mem_efficient=False enable_cudnn=False backend == flash backend = torch backends cuda sdp_kernel enable_flash=True enable_math=False enable_mem_efficient=False enable_cudnn=False backend == mem_efficient backend = torch backends cuda sdp_kernel enable_flash=False enable_math=False enable_mem_efficient=True enable_cudnn=False backend mode out = F scaled_dot_product_attention q k v dropout_p= is_causal=True with_backward out is_nested out values sum backward out sum backward int get_total_flops mode get_nested_inputs batch_size n_heads max_seq_len_q max_seq_len_k head_dim head_dim_v dtype q_lengths = torch tensor max_seq_len_q max_seq_len_q max_seq_len_q max_seq_len_q k_lengths = torch tensor max_seq_len_k max_seq_len_k max_seq_len_k max_seq_len_k q_offsets k_offsets = torch cat torch tensor torch cumsum lengths dim= cuda lengths q_lengths k_lengths q_values = torch randn q_offsets - head_dim n_heads dtype=dtype requires_grad=True device= cuda k_values = torch randn k_offsets - head_dim n_heads dtype=dtype requires_grad=True device= cuda v_values = torch randn k_offsets - head_dim_v n_heads dtype=dtype requires_grad=True device= cuda q = torch nested nested_tensor_from_jagged q_values q_offsets k = torch nested nested_tensor_from_jagged k_values k_offsets v = torch nested nested_tensor_from_jagged v_values k_offsets q = q view batch_size - n_heads head_dim transpose k = k view batch_size - n_heads head_dim transpose v = v view batch_size - n_heads head_dim_v transpose q k v get_dense_flops q k v backend with_backward=False split_tensor x y unsqueeze transpose detach requires_grad_ True y x transpose unbind q_tensors = split_tensor q k_tensors = split_tensor k v_tensors = split_tensor v flops = q_i k_i v_i zip q_tensors k_tensors v_tensors flops += get_flops q_i k_i v_i backend=backend with_backward=with_backward flops uniform_config = batch_size n_heads max_seq_len_q max_seq_len_k head_dim head_dim_v dtype torch float max_seq_len_q = max_seq_len_k doesn t work flash attention dense tensors differing_config = batch_size n_heads max_seq_len_q max_seq_len_k head_dim head_dim_v dtype torch float assertEqual get_dense_flops get_nested_inputs uniform_config backend= flash with_backward=False get_flops get_nested_inputs uniform_config backend= flash with_backward=False assertEqual get_dense_flops get_nested_inputs uniform_config backend= mem_efficient with_backward=False get_flops get_nested_inputs uniform_config backend= mem_efficient with_backward=False assertEqual get_dense_flops get_nested_inputs differing_config backend= mem_efficient with_backward=False get_flops get_nested_inputs differing_config backend= mem_efficient with_backward=False assertEqual get_dense_flops get_nested_inputs uniform_config backend= flash with_backward=True get_flops get_nested_inputs uniform_config backend= flash with_backward=True assertEqual get_dense_flops get_nested_inputs uniform_config backend= mem_efficient with_backward=True get_flops get_nested_inputs uniform_config backend= mem_efficient with_backward=True assertEqual get_dense_flops get_nested_inputs differing_config backend= mem_efficient with_backward=True get_flops get_nested_inputs differing_config backend= mem_efficient with_backward=True unittest skipIf HAS_CUDA CUDA available unittest skipIf PLATFORM_SUPPORTS_FLASH_ATTENTION Does support all SDPA backends pre-SM hardware CUDA test_nested_attention_fake_tensors x = torch randn device= cuda dtype=torch bfloat offsets = torch tensor device= cuda max_seqlen = FakeTensorMode fake_mode fake_x = fake_mode from_tensor x fake_offsets = fake_mode from_tensor offsets FlopCounterMode fake_flop_counter_mode torch ops aten _flash_attention_forward fake_x fake_x fake_x fake_offsets fake_offsets max_seqlen max_seqlen False False dense_x = torch randn dtype=torch bfloat device= cuda transpose FlopCounterMode real_flop_counter_mode torch ops aten _flash_attention_forward dense_x dense_x dense_x None None max_seqlen max_seqlen False False assertEqual int get_total_flops fake_flop_counter_mode int get_total_flops real_flop_counter_mode test_addmm_out f x y = torch zeros torch mm x x out=y FlopCounterMode mode f torch randn assertExpectedInline get_total_flops mode test_hook_registration model = torch nn Linear x = torch randn FlopCounterMode mode assertEqual len torch nn modules module _global_forward_pre_hooks assertEqual len torch nn modules module _global_forward_hooks model x sum backward assertEqual len torch nn modules module _global_forward_pre_hooks assertEqual len torch nn modules module _global_forward_hooks test_pytrees Foo torch nn Module forward x x = x relu_ torch mm x x Mod torch nn Module __init__ - None super __init__ = Foo b = Foo forward x b x mod = Mod FlopCounterMode mode mod torch randn requires_grad=True clone sum backward assertExpectedInline mode flop_counts Mod torch ops aten mm Mod torch nn Module forward x torch mm x x mod = Mod FlopCounterMode mode mod torch randn requires_grad=True sum backward assertExpectedInline mode flop_counts Mod torch ops aten mm test_warning mod = torch nn Linear assertWarnsRegex UserWarning needed FlopCounterMode mod test_custom_op torch utils flop_counter FlopCounterMode register_flop_formula torch library custom_op mylib foo mutates_args= foo x torch Tensor - torch Tensor x sin called = assertRaisesRegex ValueError expected each target OpOverloadPacket register_flop_formula torch ops mylib foo default lambda x x register_flop_formula torch ops mylib foo formula args kwargs nonlocal called called += x = torch randn FlopCounterMode display=False mode y = foo x assertEqual called assertExpectedInline get_total_flops mode skipIfNoTorchVision test_inference_mode get_flops model FlopCounterMode model mode = T model sum mode resnet = torchvision_models resnet mode_standard = get_flops resnet torch inference_mode mode_inference = get_flops resnet assertEqual get_total_flops mode_standard get_total_flops mode_inference layer _conv_flops_standard = mode_standard flop_counts ResNet layer torch ops aten convolution layer _conv_flops_inference = mode_inference flop_counts ResNet layer torch ops aten convolution assertEqual layer _conv_flops_standard layer _conv_flops_inference unittest skipIf HAS_CUDA CUDA available unittest skipIf PLATFORM_SUPPORTS_FP FP only supported H + SM MI + devices test_scaled_mm dtype = e m _type FlopCounterMode mode torch _scaled_mm torch randn device= cuda dtype torch randn device= cuda dtype t scale_a=torch ones device= cuda scale_b=torch ones device= cuda out_dtype=torch bfloat assertExpectedInline get_total_flops mode __name__ == __main__ run_tests