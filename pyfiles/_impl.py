Implementations ONNX operators native Torch ops NOTE Fake implementations Refer https docs pytorch org docs stable library html#torch library register_fake more details how create fake kernels flake noqa B math collections abc Callable typing Optional TypeVar typing_extensions ParamSpec torch torch onnx ops _dtype_mappings Use ParamSpec better type preservation instead bound Callable TypeVar _P = ParamSpec _P _R = TypeVar _R ONNX ATen decomp table ONNX_ATEN_DECOMP_TABLE dict torch _ops OpOverload Callable = _ATTENTION_ _ALLOWED_INTERMEDIATE_PRECISIONS = frozenset FLOAT FLOAT DOUBLE BFLOAT _onnx_op op_type str opset_version int fake_impl Callable _P _R - Callable Callable _P _R Callable _P _R Decorator register ONNX operator custom implementation decorator func Callable _P _R - Callable _P _R overload = f opset opset_version torch_op = torch library custom_op f onnx op_type overload mutates_args= func ONNX_ATEN_DECOMP_TABLE getattr getattr torch ops onnx op_type overload = func type ignore assignment torch_op register_fake fake_impl torch_op type ignore return-value decorator _rotary_embedding_ _fake_impl x torch Tensor cos_cache torch Tensor sin_cache torch Tensor position_ids Optional torch Tensor = None interleaved bool = False num_heads int = rotary_embedding_dim int = - torch Tensor Fake implementation RotaryEmbedding- torch compile purposes x clone _onnx_op RotaryEmbedding _rotary_embedding_ _fake_impl rotary_embedding_ x torch Tensor cos_cache torch Tensor sin_cache torch Tensor position_ids Optional torch Tensor = None interleaved bool = False num_heads int = rotary_embedding_dim int = - torch Tensor RotaryEmbedding- https onnx ai onnx operators onnx__RotaryEmbedding html#rotaryembedding- x has shape batch_size num_heads sequence_length head_size batch_size sequence_length hidden_size input_shape = x shape input_rank = len input_shape batch_size = input_shape sequence_length = input_shape - Validate position_ids caches match x position_ids None torch _check position_ids dim == lambda f position_ids must D when provided Received shape position_ids shape torch _check position_ids shape == batch_size lambda f position_ids first dim batch must match x shape batch_size Received position_ids shape torch _check position_ids shape == sequence_length lambda f position_ids second dim sequence must match x shape - sequence_length Received position_ids shape torch _check cos_cache dim == sin_cache dim == lambda cos_cache sin_cache must D when position_ids provided f Received cos_cache shape cos_cache shape sin_cache shape sin_cache shape torch _check cos_cache dim == sin_cache dim == lambda cos_cache sin_cache must D when position_ids provided f Received cos_cache shape cos_cache shape sin_cache shape sin_cache shape First ensure x has shape batch_size num_heads seq_len head_size So rotation logic can shared reshaped D inputs input_rank == Reshape batch_size num_heads seq_len head_size batch_size seq_len num_heads head_size x = torch permute x input_rank == torch _check num_heads = lambda f num_heads must provided D inputs Received input tensor shape input_shape hidden_size = input_shape head_size = hidden_size num_heads new_shape = batch_size sequence_length num_heads head_size x = torch reshape x new_shape torch _check len x shape == lambda x should D tensor now head_size = x shape Fully partially perform rotation x based rotary_embedding_dim attribute rotary_embedding_dim == If rotary_embedding_dim provided perform full rotation using head_size rotary_embedding_dim = head_size x_rotate = x rotary_embedding_dim x_not_rotate = x rotary_embedding_dim rotary_embedding_dim_half = rotary_embedding_dim Retrieve sin cos caches using position ids position_ids None cos = cos_cache position_ids Shape batch_size sequence_length head_size sin = sin_cache position_ids Shape batch_size sequence_length head_size cos = cos_cache Shape batch_size sequence_length rotary_embedding_dim sin = sin_cache Shape batch_size sequence_length rotary_embedding_dim torch _check cos shape == batch_size cos shape == sequence_length lambda f cos has shape cos shape expected batch= batch_size seq= sequence_length torch _check sin shape == batch_size sin shape == sequence_length lambda f sin has shape sin shape expected batch= batch_size seq= sequence_length torch _check cos shape - == rotary_embedding_dim_half lambda f Last dimension cos cache cos shape - should match rotary_embedding_dim rotary_embedding_dim_half torch _check sin shape - == rotary_embedding_dim_half lambda f Last dimension sin cache sin shape - should match rotary_embedding_dim rotary_embedding_dim_half cos = torch unsqueeze cos Shape batch_size sequence_length rotary_embedding_dim sin = torch unsqueeze sin Shape batch_size sequence_length rotary_embedding_dim Either divide x halves interleave based interleaved attribute interleaved x = x_rotate x = x_rotate x x = torch chunk x_rotate dim=- Calculate real imaginary values real = cos x - sin x imag = sin x + cos x Inserted rotated embeddings back original x interleaved x_rotate = real x_rotate = imag real = torch unsqueeze real - imag = torch unsqueeze imag - x_rotate_concat = torch cat real imag dim=- x_rotate = torch reshape x_rotate_concat x_rotate shape x_rotate = torch cat real imag dim=- output = torch cat x_rotate x_not_rotate dim=- input_rank == torch reshape output input_shape Return dimensions original order torch permute output _get_scale_factor scale Optional float head_size int - float Get scale factor attention computation scale scale None math sqrt head_size _reshape_ d_to_ d tensor torch Tensor batch_size int num_heads int - torch Tensor Reshape D tensor D multi-head attention sequence_length hidden_size = tensor shape tensor shape head_size = hidden_size num_heads tensor view batch_size sequence_length num_heads head_size transpose contiguous _get_qk_output_for_aten_spda Q torch Tensor K torch Tensor current_q_num_heads int current_kv_num_heads int scale Optional float qk_matmul_output_mode int - torch Tensor Get QK output tensor based specified mode qk_matmul_output_mode == _compute_qk_output_for_mode_ Q K current_q_num_heads current_kv_num_heads scale For other modes zero tensor correct shape torch zeros_like torch matmul Q K transpose - - _validate_gqa_configuration current_q_num_heads int current_kv_num_heads int - None Validate Group Query Attention configuration torch _check current_q_num_heads current_kv_num_heads == lambda f q_num_heads current_q_num_heads must divisible kv_num_heads current_kv_num_heads GQA _compute_qk_output_for_mode_ Q torch Tensor K torch Tensor current_q_num_heads int current_kv_num_heads int scale Optional float - torch Tensor Helper function compute QK output qk_matmul_output_mode == Handle GQA manually QK output K_for_qk = K current_q_num_heads = current_kv_num_heads repeat_factor = current_q_num_heads current_kv_num_heads K_for_qk = K repeat_interleave repeat_factor dim= scale_factor = _get_scale_factor scale Q shape Scale both Q K sqrt scale_factor numerical stability sqrt_scale = math sqrt scale_factor Q_scaled = Q sqrt_scale K_scaled = K_for_qk sqrt_scale torch matmul Q_scaled K_scaled transpose - - _attention_ _fake_impl Q torch Tensor K torch Tensor V torch Tensor attn_mask Optional torch Tensor = None past_key Optional torch Tensor = None past_value Optional torch Tensor = None is_causal bool = False kv_num_heads int = q_num_heads int = qk_matmul_output_mode int = scale Optional float = None softcap float = softmax_precision Optional int = None - tuple torch Tensor torch Tensor torch Tensor torch Tensor Fake implementation Attention- torch compile purposes batch_size = Q shape Handle D vs D input shapes len Q shape == D input batch_size sequence_length hidden_size q_sequence_length = Q shape output_shape = Q shape Same shape Q D output For present_key present_value we need D shapes past_key None present_key_shape = batch_size kv_num_heads past_key shape + K shape Combined sequence length K shape kv_num_heads head_size present_key_shape = batch_size kv_num_heads K shape sequence_length K shape kv_num_heads head_size present_value_shape = present_key_shape Same shape present_key QK output shape D input reshaped D internally qk_output_shape = batch_size q_num_heads q_sequence_length present_key_shape kv_sequence_length D input batch_size num_heads sequence_length head_size q_sequence_length = Q shape Same shape Q D output output_shape = Q shape type ignore assignment Handle past key value concatenation past_key None present_key_shape = K shape batch_size K shape num_heads past_key shape + K shape Combined sequence length K shape head_size present_key_shape = K shape type ignore assignment present_value_shape = present_key_shape Same shape present_key QK output shape qk_output_shape = Q shape batch_size Q shape q_num_heads Q shape q_sequence_length present_key_shape kv_sequence_length Create fake tensors correct shapes dtypes output = torch empty output_shape dtype=Q dtype device=Q device present_key = torch empty present_key_shape dtype=K dtype device=K device present_value = torch empty present_value_shape dtype=V dtype device=V device qk_output = torch empty qk_output_shape dtype=Q dtype device=Q device output present_key present_value qk_output _onnx_op Attention _attention_ _fake_impl attention_ Q torch Tensor K torch Tensor V torch Tensor attn_mask Optional torch Tensor = None past_key Optional torch Tensor = None past_value Optional torch Tensor = None is_causal bool = False kv_num_heads int = q_num_heads int = qk_matmul_output_mode int = scale Optional float = None softcap float = softmax_precision Optional int = None - tuple torch Tensor torch Tensor torch Tensor torch Tensor Attention- https onnx ai onnx operators onnx__Attention html#attention- num_head_dim sequence_dim head_dim = Store original input shape determine output shape input_shape_len = len Q shape batch_size = Q shape Reshape D inputs D format len Q shape == torch _check q_num_heads = kv_num_heads = lambda q_num_heads kv_num_heads must provided D inputs q_sequence_length = Q shape Q = _reshape_ d_to_ d Q batch_size q_num_heads K = _reshape_ d_to_ d K batch_size kv_num_heads V = _reshape_ d_to_ d V batch_size kv_num_heads torch _check len Q shape == len K shape == len V shape == lambda Q K V should D tensors now Calculate scale factor provided q_head_size = Q shape head_dim scale = _get_scale_factor scale q_head_size Handle past key value caches present_key = torch cat past_key K dim=sequence_dim past_key None K clone present_value = torch cat past_value V dim=sequence_dim past_value None V clone Update K V include past states K V = present_key present_value Get current dimensions current_q_num_heads = Q shape num_head_dim current_kv_num_heads = K shape num_head_dim q_sequence_length = Q shape sequence_dim kv_sequence_length = K shape sequence_dim Check we can use optimized scaled_dot_product_attention most optimized can_use_sdpa = softcap == No softcap qk_matmul_output_mode == Default QK output mode softmax_precision None No custom softmax precision attn_mask None attn_mask dtype == torch bool _validate_gqa_configuration current_q_num_heads current_kv_num_heads can_use_sdpa Use PyTorch s optimized scaled_dot_product_attention output = torch nn functional scaled_dot_product_attention Q K V attn_mask=attn_mask dropout_p= is_causal=is_causal scale=scale enable_gqa=bool current_q_num_heads = current_kv_num_heads Ensure enable_gqa SymBool qk_output = _get_qk_output_for_aten_spda Q K current_q_num_heads current_kv_num_heads scale qk_matmul_output_mode Fallback manual implementation complex cases Handle Group Query Attention GQA Multi-Query Attention MQA current_q_num_heads = current_kv_num_heads repeat_factor = current_q_num_heads current_kv_num_heads K = K repeat_interleave repeat_factor dim=num_head_dim V = V repeat_interleave repeat_factor dim=num_head_dim Create attention bias attn_bias = torch zeros q_sequence_length kv_sequence_length dtype=Q dtype device=Q device Apply causal masking is_causal torch _check attn_mask None lambda Cannot use both is_causal attn_mask causal_mask = torch tril torch ones q_sequence_length kv_sequence_length dtype=torch bool device=Q device attn_bias = attn_bias masked_fill ~causal_mask float -inf Apply attention mask attn_mask None attn_mask dtype == torch bool Boolean mask True means participate attention attn_bias = attn_bias masked_fill ~attn_mask float -inf Float mask added attention scores attn_bias = attn_bias + attn_mask Apply scaling factor scale_factor = _get_scale_factor scale Q shape Scale both Q K sqrt scale_factor numerical stability sqrt_scale = math sqrt scale_factor Q_scaled = Q sqrt_scale K_scaled = K sqrt_scale Compute Q K^T qk_matmul_output = torch matmul Q_scaled K_scaled transpose - - Initialize QK output based mode qk_output = qk_matmul_output Default case mode Add attention bias qk_with_bias = qk_matmul_output + attn_bias qk_matmul_output_mode == qk_output = qk_with_bias Apply softcap provided softcap qk_with_bias = softcap torch tanh qk_with_bias softcap qk_matmul_output_mode == qk_output = qk_with_bias Apply softmax optional precision casting softmax_precision None Map ONNX data type torch dtype softmax_precision _ATTENTION_ _ALLOWED_INTERMEDIATE_PRECISIONS original_dtype = qk_with_bias dtype qk_with_bias = qk_with_bias _dtype_mappings ONNX_DTYPE_TO_TORCH_DTYPE softmax_precision qk_softmax = torch softmax qk_with_bias dim=- qk_softmax = qk_softmax original_dtype qk_softmax = torch softmax qk_with_bias dim=- qk_softmax = torch softmax qk_with_bias dim=- qk_matmul_output_mode == qk_output = qk_softmax Compute attention output output = torch matmul qk_softmax V Reshape output back D input D input_shape_len == output batch_size q_num_heads q_sequence_length v_head_size - batch_size q_sequence_length hidden_size output = output transpose contiguous view batch_size q_sequence_length - output present_key present_value qk_output