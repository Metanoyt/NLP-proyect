Owner s oncall distributed Copyright c Meta Platforms Inc affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree unittest copy deepcopy torch torch nn nn torch distributed optim _apply_optimizer_in_backward _get_in_backward_optimizers TODO rohan-varma Add FSDP DDP tests once supported _validate_params params_list fn ref_params = params_list param_list params_list p p zip ref_params param_list fn p p ApplyOverlappedOptimizerTest unittest TestCase _run_training_loop_and_validate inp models optimizers i range model models model inp sum backward opt optimizers opt step subTest i _validate_params model parameters model models torch testing assert_close opt optimizers opt zero_grad set_to_none=True _test_apply_optimizer_in_backward share_params - None weight_optimizer_kwargs = lr bias_optimizer_kwargs = lr model = nn Sequential nn Linear nn Linear share_params model weight = model weight Use different optimizers weights biases weights = m weight m model biases = m bias m model optim_weight = torch optim SGD weights weight_optimizer_kwargs optim_bias = torch optim SGD biases bias_optimizer_kwargs model_with_opt_in_bwd = deepcopy model Apply different optimizer backwards weights biases _apply_optimizer_in_backward torch optim SGD m weight m model_with_opt_in_bwd optimizer_kwargs=weight_optimizer_kwargs _apply_optimizer_in_backward torch optim SGD m bias m model_with_opt_in_bwd optimizer_kwargs=bias_optimizer_kwargs _validate_params model parameters model_with_opt_in_bwd parameters torch testing assert_close _run_training_loop_and_validate torch randn model model_with_opt_in_bwd optim_weight optim_bias test_apply_optimizer_in_backward - None _test_apply_optimizer_in_backward share_params=False test_apply_optimizer_in_backward_shared_params - None _test_apply_optimizer_in_backward share_params=True test_no_register_hook model_with_hook = nn Sequential nn Linear nn Linear initial_model = deepcopy model_with_hook model_no_hook = deepcopy model_with_hook _apply_optimizer_in_backward torch optim SGD model_with_hook parameters optimizer_kwargs= lr _apply_optimizer_in_backward torch optim SGD model_no_hook parameters optimizer_kwargs= lr register_hook=False inp = torch randn model_with_hook inp sum backward model_no_hook inp sum backward p p zip model_with_hook parameters initial_model parameters assertRaises AssertionError torch testing assert_close p p p p zip model_no_hook parameters initial_model parameters torch testing assert_close p p test_multiple_optim_for_params - None model = nn Sequential nn Linear nn Linear opt_ _kwargs = lr opt_ _kwargs = lr opt_ = torch optim SGD model parameters opt_ _kwargs opt_ = torch optim SGD model parameters opt_ _kwargs model_with_opt_in_bwd = deepcopy model _apply_optimizer_in_backward torch optim SGD model_with_opt_in_bwd parameters optimizer_kwargs=opt_ _kwargs _apply_optimizer_in_backward torch optim SGD model_with_opt_in_bwd parameters optimizer_kwargs=opt_ _kwargs _run_training_loop_and_validate torch randn model model_with_opt_in_bwd opt_ opt_ test_get_optimizers_in_backward Create simple test model TestModel torch nn Module __init__ - None super __init__ linear = torch nn Linear linear = torch nn Linear model = TestModel Apply optimizers backward _apply_optimizer_in_backward torch optim SGD model parameters lr in_backward_optims = _get_in_backward_optimizers model assertEqual len list model parameters len in_backward_optims result = set in_backward_optims expected = optim p model parameters optim p _in_backward_optimizers assertEqual result expected