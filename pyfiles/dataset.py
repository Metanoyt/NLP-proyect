mypy allow-untyped-defs bisect itertools math warnings collections abc Sequence UP wants Iterable imported collections abc needs stay typing now due BC concerns In particular several internal targets fail typecheck TypeError Cannot create consistent method resolution order MRO bases Iterable Generic typing cast Generic Iterable Optional TypeVar Union noqa UP typing_extensions deprecated No default_generator torch __init__ pyi torch default_generator Generator randperm Tensor __all__ = Dataset IterableDataset TensorDataset StackDataset ConcatDataset ChainDataset Subset random_split _T = TypeVar _T _T_co = TypeVar _T_co covariant=True _T_dict = dict str _T_co _T_tuple = tuple _T_co _T_stack = TypeVar _T_stack _T_tuple _T_dict Dataset Generic _T_co r An abstract representing ` Dataset ` All datasets represent map keys data samples should subclass All subclasses should overwrite meth ` __getitem__ ` supporting fetching data sample given key Subclasses could also optionally overwrite meth ` __len__ ` which expected size dataset many ` ~torch utils data Sampler ` implementations default options ` ~torch utils data DataLoader ` Subclasses could also optionally implement meth ` __getitems__ ` speedup batched samples loading This method accepts list indices samples batch returns list samples note ` ~torch utils data DataLoader ` default constructs index sampler yields integral indices To make work map-style dataset non-integral indices keys custom sampler must provided __getitem__ index - _T_co raise NotImplementedError Subclasses Dataset should implement __getitem__ __getitems__ indices List - List _T_co Not implemented prevent false-positives fetcher check torch utils data _utils fetch _MapDatasetFetcher __add__ other Dataset _T_co - ConcatDataset _T_co ConcatDataset other No ` __len__ ` default See NOTE Lack Default ` __len__ ` Python Abstract Base Classes pytorch torch utils data sampler py IterableDataset Dataset _T_co Iterable _T_co r An iterable Dataset All datasets represent iterable data samples should subclass Such form datasets particularly useful when data come stream All subclasses should overwrite meth ` __iter__ ` which would iterator samples dataset When subclass used ` ~torch utils data DataLoader ` each item dataset will yielded ` ~torch utils data DataLoader ` iterator When attr ` num_workers ` each worker process will have different copy dataset object so often desired configure each copy independently avoid having duplicate data returned workers func ` ~torch utils data get_worker_info ` when called worker process returns information about worker It can used either dataset s meth ` __iter__ ` method ` ~torch utils data DataLoader ` s attr ` worker_init_fn ` option modify each copy s behavior Example splitting workload across all workers meth ` __iter__ ` xdoctest +REQUIRES env TORCH_DOCTEST_DATALOADER xdoctest +SKIP Fails MacOS MyIterableDataset torch utils data IterableDataset __init__ start end super MyIterableDataset __init__ assert end start example only works end = start start = start end = end __iter__ worker_info = torch utils data get_worker_info worker_info None single-process data loading full iterator iter_start = start iter_end = end worker process split workload per_worker = int math ceil end - start float worker_info num_workers worker_id = worker_info id iter_start = start + worker_id per_worker iter_end = min iter_start + per_worker end iter range iter_start iter_end should give same set data range i e ds = MyIterableDataset start= end= Single-process loading print list torch utils data DataLoader ds num_workers= tensor tensor tensor tensor xdoctest +REQUIRES POSIX Multi-process loading two worker processes Worker fetched Worker fetched xdoctest +IGNORE_WANT non deterministic print list torch utils data DataLoader ds num_workers= tensor tensor tensor tensor With even more workers xdoctest +IGNORE_WANT non deterministic print list torch utils data DataLoader ds num_workers= tensor tensor tensor tensor Example splitting workload across all workers using attr ` worker_init_fn ` xdoctest +REQUIRES env TORCH_DOCTEST_DATALOADER MyIterableDataset torch utils data IterableDataset __init__ start end super MyIterableDataset __init__ assert end start example only works end = start start = start end = end __iter__ iter range start end should give same set data range i e ds = MyIterableDataset start= end= Single-process loading print list torch utils data DataLoader ds num_workers= Directly doing multi-process loading yields duplicate data print list torch utils data DataLoader ds num_workers= Define ` worker_init_fn ` configures each dataset copy differently worker_init_fn worker_id worker_info = torch utils data get_worker_info dataset = worker_info dataset dataset copy worker process overall_start = dataset start overall_end = dataset end configure dataset only process split workload per_worker = int math ceil overall_end - overall_start float worker_info num_workers worker_id = worker_info id dataset start = overall_start + worker_id per_worker dataset end = min dataset start + per_worker overall_end Mult-process loading custom ` worker_init_fn ` Worker fetched Worker fetched print list torch utils data DataLoader ds num_workers= worker_init_fn=worker_init_fn With even more workers print list torch utils data DataLoader ds num_workers= worker_init_fn=worker_init_fn __add__ other Dataset _T_co ChainDataset other No ` __len__ ` default Subclasses raise ` TypeError ` when needed See NOTE Lack Default ` __len__ ` Python Abstract Base Classes TensorDataset Dataset tuple Tensor r Dataset wrapping tensors Each sample will retrieved indexing tensors along first dimension Args tensors Tensor tensors have same size first dimension tensors tuple Tensor __init__ tensors Tensor - None all tensors size = tensor size tensor tensors raise AssertionError Size mismatch between tensors tensors = tensors __getitem__ index tuple tensor index tensor tensors __len__ tensors size StackDataset Dataset _T_stack r Dataset stacking multiple datasets This useful assemble different parts complex input data given datasets Example xdoctest +SKIP images = ImageDataset texts = TextDataset tuple_stack = StackDataset images texts tuple_stack == images texts dict_stack = StackDataset image=images text=texts dict_stack == image images text texts Args args Dataset Datasets stacking returned tuple kwargs Dataset Datasets stacking returned dict datasets Union tuple dict __init__ args Dataset _T_co kwargs Dataset _T_co - None args kwargs raise ValueError Supported either ` ` tuple ` ` - via ` ` args ` ` ` ` dict ` ` - via ` ` kwargs ` ` like input output both types given _length = len args type ignore arg-type any _length = len dataset dataset args type ignore arg-type raise ValueError Size mismatch between datasets datasets = args kwargs tmp = list kwargs values _length = len tmp type ignore arg-type any _length = len dataset dataset tmp type ignore arg-type raise ValueError Size mismatch between datasets datasets = kwargs raise ValueError At least one dataset should passed __getitem__ index isinstance datasets dict k dataset index k dataset datasets items tuple dataset index dataset datasets __getitems__ indices list add batched sampling support when parent datasets supports isinstance datasets dict dict_batch list _T_dict = _ indices k dataset datasets items callable getattr dataset __getitems__ None items = dataset __getitems__ indices type ignore attr-defined len items = len indices raise ValueError Nested dataset s output size mismatch f Expected len indices got len items data d_sample zip items dict_batch strict=True d_sample k = data idx d_sample zip indices dict_batch strict=True d_sample k = dataset idx dict_batch tuple data list_batch list list = _ indices dataset datasets callable getattr dataset __getitems__ None items = dataset __getitems__ indices type ignore attr-defined len items = len indices raise ValueError Nested dataset s output size mismatch f Expected len indices got len items data t_sample zip items list_batch strict=True t_sample append data idx t_sample zip indices list_batch strict=True t_sample append dataset idx tuple_batch list _T_tuple = tuple sample sample list_batch tuple_batch __len__ _length ConcatDataset Dataset _T_co r Dataset concatenation multiple datasets This useful assemble different existing datasets Args datasets sequence List datasets concatenated datasets list Dataset _T_co cumulative_sizes list int staticmethod cumsum sequence r s = e sequence l = len e r append l + s s += l r __init__ datasets Iterable Dataset - None super __init__ datasets = list datasets len datasets == raise AssertionError datasets should empty iterable d datasets isinstance d IterableDataset raise AssertionError ConcatDataset does support IterableDataset cumulative_sizes = cumsum datasets __len__ cumulative_sizes - __getitem__ idx idx -idx len raise ValueError absolute value index should exceed dataset length idx = len + idx dataset_idx = bisect bisect_right cumulative_sizes idx dataset_idx == sample_idx = idx sample_idx = idx - cumulative_sizes dataset_idx - datasets dataset_idx sample_idx property deprecated ` cummulative_sizes ` attribute renamed ` cumulative_sizes ` category=FutureWarning cummulative_sizes cumulative_sizes ChainDataset IterableDataset r Dataset chaining multiple ` IterableDataset ` s This useful assemble different existing dataset streams The chaining operation done on-the-fly so concatenating large-scale datasets will efficient Args datasets iterable IterableDataset datasets chained together __init__ datasets Iterable Dataset - None super __init__ datasets = datasets __iter__ d datasets isinstance d IterableDataset raise AssertionError ChainDataset only supports IterableDataset yield d __len__ total = d datasets isinstance d IterableDataset raise AssertionError ChainDataset only supports IterableDataset total += len d type ignore arg-type total Subset Dataset _T_co r Subset dataset specified indices Args dataset Dataset The whole Dataset indices sequence Indices whole set selected subset dataset Dataset _T_co indices Sequence int __init__ dataset Dataset _T_co indices Sequence int - None dataset = dataset indices = indices __getitem__ idx isinstance idx list dataset indices i i idx dataset indices idx __getitems__ indices list int - list _T_co add batched sampling support when parent dataset supports see torch utils data _utils fetch _MapDatasetFetcher callable getattr dataset __getitems__ None dataset __getitems__ indices idx idx indices type ignore attr-defined dataset indices idx idx indices __len__ len indices random_split dataset Dataset _T lengths Sequence Union int float generator Optional Generator = default_generator - list Subset _T r Randomly split dataset into non-overlapping new datasets given lengths If list fractions sum up given lengths will computed automatically floor frac len dataset each fraction provided After computing lengths there any remainders count will distributed round-robin fashion lengths until there no remainders left Optionally fix generator reproducible results e g Example xdoctest +SKIP generator = torch Generator manual_seed generator = torch Generator manual_seed random_split range generator=generator random_split range generator=generator Args dataset Dataset Dataset split lengths sequence lengths fractions splits produced generator Generator Generator used random permutation math isclose sum lengths sum lengths = subset_lengths list int = i frac enumerate lengths frac frac raise ValueError f Fraction index i between n_items_in_split = math floor len dataset frac type ignore arg-type subset_lengths append n_items_in_split remainder = len dataset - sum subset_lengths type ignore arg-type add all lengths round-robin fashion until remainder i range remainder idx_to_add_at = i len subset_lengths subset_lengths idx_to_add_at += lengths = subset_lengths i length enumerate lengths length == warnings warn f Length split index i f This might result empty dataset stacklevel= Cannot verify dataset Sized sum lengths = len dataset type ignore arg-type raise ValueError Sum input lengths does equal length input dataset indices = randperm sum lengths generator=generator tolist type ignore arg-type call-overload lengths = cast Sequence int lengths Subset dataset indices offset - length offset offset length zip itertools accumulate lengths lengths strict=True