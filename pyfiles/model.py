__future__ annotations dataclasses itertools re dataclasses dataclass enum auto Enum typing Optional TYPE_CHECKING typing_extensions assert_never torchgen utils NamespaceHelper OrderedSet TYPE_CHECKING collections abc Callable Iterator Sequence ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ DATA MODEL ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Some general principles our data model - Stop using C++ data types internal data representation format Instead internal data structures centered around JIT schema representation This avoid big problem old codegen where we read all types native_functions yaml then immediately had retranslate them into C++ types - More semantic data representation Instead representing everything dicts strings we define dataclasses every interesting entity code generation has deal These dataclasses have strong semantic invariants example we generally require them roundtrip losslessly into form they parsed These structures immutable you re expected populate information once during construction Represent source location used better error reporting dataclass frozen=True Location file str line int __str__ - str f file line Valid values variants field native_functions yaml Variant Enum function = auto method = auto Default kernel namespace DEFAULT_KERNEL_NAMESPACE = native NOTE Keep list sync ` DispatchKey ` c core DispatchKey h BACKEND_COMPONENTS = CPU CUDA HIP XLA MTIA MPS IPU XPU HPU VE Lazy Meta PrivateUse PrivateUse PrivateUse FUNCTIONALITY_KEYS = Quantized Sparse SparseCsr NestedTensor Autograd This list guards dispatches can used derivatives yaml For now we omit AutogradFunctionality AutogradOther AUTOGRAD_KEYS = AutogradNestedTensor + Autograd + component component BACKEND_COMPONENTS FRAGMENT_NAMESPACES = quantized quantized_decomposed This doesn t have sync header only needs contain entries we actually use codegen want pyi entries DispatchKey Enum Undefined = CatchAll = Undefined FPGA = auto MAIA = auto Vulkan = auto Metal = auto MKLDNN = auto OpenGL = auto OpenCL = auto IDEEP = auto CustomRNGKeyId = auto MkldnnCPU = auto Sparse = auto SparseCsr = auto NestedTensor = auto Dense = auto PythonTLSSnapshot = auto PreDispatch = auto PythonDispatcher = auto Python = auto FuncTorchDynamicLayerBackMode = auto ZeroTensor = auto Conjugate = auto Negative = auto BackendSelect = auto Named = auto AutogradOther = auto AutogradFunctionality = auto AutogradNestedTensor = auto Tracer = auto Autocast = auto AutocastCPU = auto AutocastCUDA = auto Batched = auto VmapMode = auto FuncTorchGradWrapper = auto FuncTorchBatched = auto BatchedNestedTensor = auto FuncTorchVmapMode = auto FuncTorchDynamicLayerFrontMode = auto Functionalize = auto TESTING_ONLY_GenericWrapper = auto TESTING_ONLY_GenericMode = auto ADInplaceOrView = auto Autograd = auto CompositeImplicitAutograd = auto CompositeImplicitAutogradNestedTensor = auto CompositeExplicitAutograd = auto CompositeExplicitAutogradNonFunctional = auto FuncTorchBatchedDecomposition = auto BEGIN autogenerated CPU = auto CUDA = auto HIP = auto XLA = auto MTIA = auto MPS = auto IPU = auto XPU = auto HPU = auto VE = auto Lazy = auto Meta = auto PrivateUse = auto PrivateUse = auto PrivateUse = auto QuantizedCPU = auto QuantizedCUDA = auto QuantizedHIP = auto QuantizedXLA = auto QuantizedMTIA = auto QuantizedMPS = auto QuantizedIPU = auto QuantizedXPU = auto QuantizedHPU = auto QuantizedVE = auto QuantizedLazy = auto QuantizedMeta = auto QuantizedPrivateUse = auto QuantizedPrivateUse = auto QuantizedPrivateUse = auto SparseCPU = auto SparseCUDA = auto SparseHIP = auto SparseXLA = auto SparseMTIA = auto SparseMPS = auto SparseIPU = auto SparseXPU = auto SparseHPU = auto SparseVE = auto SparseLazy = auto SparseMeta = auto SparsePrivateUse = auto SparsePrivateUse = auto SparsePrivateUse = auto SparseCsrCPU = auto SparseCsrCUDA = auto SparseCsrHIP = auto SparseCsrXLA = auto SparseCsrMTIA = auto SparseCsrMPS = auto SparseCsrIPU = auto SparseCsrXPU = auto SparseCsrHPU = auto SparseCsrVE = auto SparseCsrLazy = auto SparseCsrMeta = auto SparseCsrPrivateUse = auto SparseCsrPrivateUse = auto SparseCsrPrivateUse = auto NestedTensorCPU = auto NestedTensorCUDA = auto NestedTensorHIP = auto NestedTensorXLA = auto NestedTensorMTIA = auto NestedTensorMPS = auto NestedTensorIPU = auto NestedTensorXPU = auto NestedTensorHPU = auto NestedTensorVE = auto NestedTensorLazy = auto NestedTensorMeta = auto NestedTensorPrivateUse = auto NestedTensorPrivateUse = auto NestedTensorPrivateUse = auto AutogradCPU = auto AutogradCUDA = auto AutogradHIP = auto AutogradXLA = auto AutogradMTIA = auto AutogradMPS = auto AutogradIPU = auto AutogradXPU = auto AutogradHPU = auto AutogradVE = auto AutogradLazy = auto AutogradMeta = auto AutogradPrivateUse = auto AutogradPrivateUse = auto AutogradPrivateUse = auto END autogenerated __str__ - str name lower - str str lower staticmethod parse value str - DispatchKey k v DispatchKey __members__ items k == value v raise AssertionError f unknown dispatch key value _TorchDispatchModeKey Enum FAKE = auto PROXY = auto FUNCTIONAL = auto codegen_per_backend_entries - str r list str = fk FUNCTIONALITY_KEYS r extend f fk bc = auto bc BACKEND_COMPONENTS \n join r fk FUNCTIONALITY_KEYS bc BACKEND_COMPONENTS hasattr DispatchKey fk + bc r = codegen_per_backend_entries print r raise RuntimeError f Missing fk bc DispatchKey enum Here autogenerated list we expect have \n\n r STRUCTURED_DISPATCH_KEYS = DispatchKey MPS DispatchKey CUDA DispatchKey CPU DispatchKey XPU DispatchKey MTIA UFUNC_DISPATCH_KEYS = DispatchKey CUDA DispatchKey CPU Set supported dispatch keys dispatch_keys = DispatchKey CPU DispatchKey SparseCPU DispatchKey SparseCsrCPU DispatchKey MkldnnCPU DispatchKey CUDA DispatchKey MPS DispatchKey XPU DispatchKey SparseXPU DispatchKey SparseCsrXPU DispatchKey SparseCUDA DispatchKey SparseCsrCUDA DispatchKey SparseMPS DispatchKey SparseCsrMPS DispatchKey QuantizedCPU DispatchKey QuantizedCUDA DispatchKey CompositeImplicitAutograd DispatchKey CompositeImplicitAutogradNestedTensor DispatchKey CompositeExplicitAutograd DispatchKey CompositeExplicitAutogradNonFunctional DispatchKey NestedTensorCPU DispatchKey NestedTensorCUDA DispatchKey NestedTensorXPU DispatchKey NestedTensorHPU Meta magic key automatically generated structured kernels DispatchKey Meta DispatchKey SparseMeta DispatchKey SparseCsrMeta DispatchKey QuantizedMeta DispatchKey NestedTensorMeta DispatchKey ZeroTensor DispatchKey MTIA Dispatch keys support all backends These codegen slightly differently then backend specific keys is_generic_dispatch_key dk DispatchKey - bool dk DispatchKey CompositeExplicitAutograd DispatchKey CompositeExplicitAutogradNonFunctional DispatchKey CompositeImplicitAutograd DispatchKey CompositeImplicitAutogradNestedTensor CUDA specific dispatch keys is_cuda_dispatch_key dk DispatchKey - bool dk DispatchKey CUDA DispatchKey QuantizedCUDA DispatchKey SparseCUDA DispatchKey SparseCsrCUDA DispatchKey NestedTensorCUDA DispatchKey AutogradCUDA XPU specific dispatcy keys is_xpu_dispatch_key dk DispatchKey - bool dk DispatchKey XPU DispatchKey QuantizedXPU DispatchKey SparseXPU DispatchKey SparseCsrXPU DispatchKey NestedTensorXPU DispatchKey AutogradXPU Structured kernel generation only supported certain key types otherwise use old-style is_structured_dispatch_key dk DispatchKey - bool dk STRUCTURED_DISPATCH_KEYS is_ufunc_dispatch_key dk DispatchKey - bool For now ufunc dispatch keys coincide structured keys dk UFUNC_DISPATCH_KEYS dispatch_device_map = is_cuda_dispatch_key cuda is_xpu_dispatch_key xpu This oddly named ScalarType DType symmetry C++ ScalarType Enum Byte = auto Char = auto Short = auto Int = auto Long = auto Half = auto Float = auto Double = auto ComplexHalf = auto ComplexFloat = auto ComplexDouble = auto Bool = auto BFloat = auto Float _e m = auto Float _e m fnuz = auto Float _e m fn = auto Float _e m fnuz = auto Float _e m fnu = auto __str__ - str name staticmethod maybe_parse value str - ScalarType &#124; None k v ScalarType __members__ items k == value v None staticmethod parse value str - ScalarType mb_r = ScalarType maybe_parse value assert mb_r None f unknown dtype value mb_r staticmethod parse_set values str - OrderedSet ScalarType dtypes OrderedSet ScalarType = OrderedSet value values split value DTYPE_CLASSES dtypes update DTYPE_CLASSES value dtypes add ScalarType parse value dtypes DTYPE_CLASSES dict str OrderedSet ScalarType = NB Integral doesn t include boolean DTYPE_CLASSES Integral = OrderedSet ScalarType Byte ScalarType Char ScalarType Int ScalarType Long ScalarType Short NB Floating doesn t include low precision types DTYPE_CLASSES Floating = OrderedSet ScalarType Float ScalarType Double DTYPE_CLASSES Complex = OrderedSet ScalarType ComplexFloat ScalarType ComplexDouble DTYPE_CLASSES All = DTYPE_CLASSES Integral &#124; DTYPE_CLASSES Floating DTYPE_CLASSES AllAndComplex = DTYPE_CLASSES All &#124; DTYPE_CLASSES Complex DTYPE_CLASSES FloatingAndComplex = DTYPE_CLASSES Floating &#124; DTYPE_CLASSES Complex Represents valid entries ufunc_inner_loop native_functions yaml NB you add new UfuncKey you will teach torchgen dest ufunc how process Most logic will ignore keys they don t understand so your new key will get silently ignored until you hook logic deal UfuncKey Enum These low level keys represent exactly one particular instantiation kernel produced codegen CUDAFunctor = auto CUDAFunctorOnOther = auto CUDAFunctorOnSelf = auto CPUScalar = auto CPUVector = auto These ones users will usually specify implicitly fill low level keys ScalarOnly = auto CUDA CPUScalar Generic = auto CUDA CPU __str__ - str name staticmethod parse value str - UfuncKey k v UfuncKey __members__ items k == value v raise AssertionError f unknown ufunc key value DeviceCheckType Enum NoCheck = ExactSame = ViewSchemaKind Enum aliasing = auto aliasing_inplace = auto non_aliasing = auto The basic input code generation native_functions yaml The name native BTW comes distinction between native functions legacy TH functions The legacy TH functions gone native descriptor has stuck NativeFunction models single entry native_functions yaml Its fields roughly correspond what you would see YAML itself after canonicalization parsing has occurred You can see some overall design patterns how we setup dataclasses we will defer complete discussion FunctionSchema dataclass frozen=True NativeFunction The namespace operator For example we have add then namespace would This enables ops registered through same DSL custom namespace If specified default namespace would namespace str The function schema operator question This schema has been parsed see FunctionSchema more about its structure This type quoted we forward referencing type defined later file I opted ordering classes expository clarity func FunctionSchema Whether generate mutable tensor arguments like regular ones use_const_ref_for_mutable_tensors bool Whether omit automatic generation DeviceGuard device_guard bool How emit automatic generation device check device_check DeviceCheckType What python module put function python_module str &#124; None TODO figure out what does category_override str &#124; None If no variants specified native_functions yaml assumed function variants set Variant Whether we should skip generating registrations kernel This bit double-edged sword manual registrations don t participate codegen-based selective build manual_kernel_registration bool Whether skip generating TensorMethod Functions bindings kernel Technically doesn t actually skip generating binding instead binding gets generated __dispatch_ funcname so you can make use normal binding you need manual_cpp_binding bool The location YAML file native function entry defined This conveniently reporting error messages loc Location A list operators expected auto-generated NativeFunction Note This list isn t actually directly used codegen generate anything Instead codegen figures out what operators generate purely based off function schema uses autogen declarations error check We expect every NativeFunction gets auto-generated explicitly called out native_functions yaml autogen list OperatorName If non-empty kernel subject ufunc codegen Sorted ufunc_key ufunc_inner_loop dict UfuncKey UfuncInnerLoop Whether out functions structured kernel Structured kernels defined little differently normal kernels particular their shape checking logic defined separately kernel Only out functions can structured other functions delegate out function using structured_delegate keyword Every structured kernel must have least out functional variant structured bool Whether non-out function structured kernel defined terms out kernel referenced string here structured_delegate OperatorName &#124; None Only valid structured kernels Specifies alternative what inherit when defining meta structured operator This will usually TensorIteratorBase This also changes semantics set_output call parent structured_inherits str &#124; None Structured kernels can declare elements precomputed These elements returned meta function one struct passed impl function lieu certain kernel arguments these precomputed elements supersede Information about names types these precomputed elements how they correspond kernel arguments stored member applicable precomputed Precompute &#124; None Argument names whose default should excluded C++ interface Intended resolving overload ambiguities between signatures cpp_no_default_args set str Note Abstract ATen methods ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ An abstract ATen method one whose dispatch differs between types These implemented derived types standard throwing definition Type A concrete ATen method one which has same dispatch all types we just implement base Type This exposed Declarations yaml via field named abstract is_abstract bool Whether NativeFunction contains backend-agnostic kernel has_composite_implicit_autograd_kernel bool has_composite_implicit_autograd_nested_tensor_kernel bool has_composite_explicit_autograd_kernel bool has_composite_explicit_autograd_non_functional_kernel bool Tags used describe semantic information about groups operators That aren t easily inferable directly operator s schema tags set str NB The benefit defining dataclass we automatically get constructor defined all fields we specify No need explicitly write out We parse both NativeFunction + backend-specific information about which stored corresponding BackendIndex staticmethod from_yaml ei dict str object loc Location valid_tags set str ignore_keys set DispatchKey &#124; None = None - tuple NativeFunction dict DispatchKey dict OperatorName BackendMetadata Parse NativeFunction dictionary directly parsed native_functions yaml e = ei copy funcs = e pop func assert isinstance funcs str f str funcs only support one level namespace E g aten add namespace_helper = NamespaceHelper from_namespaced_entity namespaced_entity=funcs max_level= namespace = namespace_helper get_cpp_namespace default= aten func = FunctionSchema parse namespace_helper entity_name cpp_no_default_args_list = e pop cpp_no_default_args assert isinstance cpp_no_default_args_list list cpp_no_default_args = set cpp_no_default_args_list use_const_ref_for_mutable_tensors = e pop use_const_ref_for_mutable_tensors False assert isinstance use_const_ref_for_mutable_tensors bool use_const_ref_for_mutable_tensors assert func arguments out see https github com pytorch pytorch issues variants_s = e pop variants function assert isinstance variants_s str variants set Variant = set v variants_s split v == function variants add Variant function v == method variants add Variant method raise AssertionError f illegal variant v manual_kernel_registration = e pop manual_kernel_registration False assert isinstance manual_kernel_registration bool f bool manual_kernel_registration manual_cpp_binding = e pop manual_cpp_binding False assert isinstance manual_cpp_binding bool f bool manual_cpp_binding device_guard = e pop device_guard True assert isinstance device_guard bool f bool device_guard device_check_s = e pop device_check None assert device_check_s None isinstance device_check_s str f str device_check_s assert device_check_s None device_check_s DeviceCheckType __members__ f illegal device_check device_check_s device_check DeviceCheckType device_check_s None device_check = DeviceCheckType ExactSame device_check = DeviceCheckType device_check_s structured = e pop structured False assert isinstance structured bool f bool structured structured_delegate_s = e pop structured_delegate None assert structured_delegate_s None isinstance structured_delegate_s str f str structured_delegate_s assert structured_delegate_s None structured_delegate_s namespace supported structured delegate using same namespace native function structured_delegate OperatorName &#124; None = None structured_delegate_s None structured_delegate = OperatorName parse structured_delegate_s structured_inherits = e pop structured_inherits None assert structured_inherits None isinstance structured_inherits str f str structured_inherits assert structured_inherits None structured_inherits namespace supported structured inherits using same namespace native function python_module = e pop python_module None assert python_module None isinstance python_module str f str python_module assert python_module None Variant method variants functions modules cannot methods category_override = e pop category_override None assert category_override None isinstance category_override str f str category_override precomputed_dict = e pop precomputed None assert precomputed_dict None structured True precomputed = Precompute parse precomputed_dict precomputed_dict None tags_inp = e pop tags isinstance tags_inp str tags_inp = tags_inp assert isinstance tags_inp list All aten ops generated torchgen receive pt _compliant tag namespace == aten pt _compliant_tag valid_tags tags_inp append pt _compliant_tag tags set str = set t tags_inp assert len valid_tags TODO verify tag valid has entry tags yaml t valid_tags tags add t raise AssertionError f illegal tag t torchgen api cpp raw_dispatch = e pop dispatch None assert raw_dispatch None isinstance raw_dispatch dict e dispatch dict DispatchKey BackendMetadata = num_dispatch_keys int = raw_dispatch None assert manual_kernel_registration cannot specify both manual_kernel_registration dispatch manual registration dispatch has no effect redundant_composite_implicit_autograd = False ks v raw_dispatch items ks == __line__ continue worth tracking line numbers dispatch entries assert isinstance ks str f illegal dispatch key ks raw_dispatch assert isinstance v str f illegal dispatch value v raw_dispatch k ks split dispatch_key = DispatchKey parse k strip num_dispatch_keys += ignore_keys dispatch_key ignore_keys continue assert dispatch_key dispatch_keys f Dispatch key dispatch_key kernel v supported dispatch key We only allow most levels namespace kernels We will append native custom kernel namespace namespace_helper = NamespaceHelper from_namespaced_entity v max_level= kernel_namespace = namespace_helper get_cpp_namespace default= Why structured included External backends e g XLA opt into which ops structured independently which in-tree ops structured dispatch dispatch_key = BackendMetadata kernel=namespace_helper entity_name structured=structured is_structured_dispatch_key dispatch_key cpp_namespace= kernel_namespace + native dispatch_key DispatchKey CompositeImplicitAutograd v == cpp name func redundant_composite_implicit_autograd = True We count number dispatch keys which have been ignored prevent dispatch table which all backend keys ignored necessarily kept remaining compositeimplicit being treated redundant assert num_dispatch_keys == redundant_composite_implicit_autograd unnecessary dispatch table function just delete dispatch key entirely function structured delegate deleting dispatch table NOT semantics preserving assert structured_delegate dispatch keys = DispatchKey CompositeImplicitAutograd dispatch DispatchKey CompositeImplicitAutograd supports_symint num_dispatch_keys = f unexpected name singleton CompositeImplicitAutograd dispatch entry expected cpp name func f got dispatch DispatchKey CompositeImplicitAutograd Rename your implementation expected name then delete dispatch table structured structured_delegate None name = str func name name assert name startswith new_ name endswith _like TODO maybe s better test func arguments tensor_options func arguments has_tensor_arg f expected name have CompositeExplicitAutograd dispatch entry there no dispatch table Factory functions should have implicit dispatch they should decomposed __torch_dispatch__ dispatch DispatchKey CompositeImplicitAutograd = BackendMetadata cpp name func structured=False cpp_namespace=DEFAULT_KERNEL_NAMESPACE composites_in_dispatch = d d dispatch d == DispatchKey CompositeExplicitAutograd d == DispatchKey CompositeExplicitAutogradNonFunctional d == DispatchKey CompositeImplicitAutograd d == DispatchKey CompositeImplicitAutogradNestedTensor assert len composites_in_dispatch = len composites_in_dispatch == DispatchKey CompositeExplicitAutogradNonFunctional composites_in_dispatch DispatchKey CompositeImplicitAutogradNestedTensor composites_in_dispatch cannot specify more than one CompositeExplicitAutograd CompositeExplicitAutogradNonFunctional CompositeImplicitAutograd single kernel each strictly subsumes other If you wanted provide explicit autograd implementation specify CompositeExplicitAutograd otherwise specify CompositeImplicitAutograd only autogen_str = e pop autogen assert isinstance autogen_str str autogen = autogen_str == OperatorName parse x x autogen_str split raw_ufunc_inner_loop = e pop ufunc_inner_loop ufunc_inner_loop = isinstance raw_ufunc_inner_loop str ufunc_inner_loop UfuncKey Generic = UfuncInnerLoop parse raw_ufunc_inner_loop UfuncKey Generic isinstance raw_ufunc_inner_loop dict k vo raw_ufunc_inner_loop items k == __line__ continue assert isinstance k str f ufunc_inner_loop key str k assert isinstance vo str f ufunc_inner_loop value str v ufunc_key = UfuncKey parse k ufunc_inner_loop ufunc_key = UfuncInnerLoop parse vo ufunc_key raise AssertionError f ufunc_inner_loop str dict raw_ufunc_inner_loop Program BackendIndex implicit dispatch entry ufunc ufunc_inner_loop assert structured ufunc must structured Delay ufunc here avoid circular issue See https github com pytorch pytorch issues torchgen api ufunc ufunc dispatch_key UFUNC_DISPATCH_KEYS assert dispatch_key dispatch f ufunc should have explicit dispatch entry dispatch_key dispatch dispatch_key = BackendMetadata kernel=ufunc schema_kernel_name func dispatch_key structured=True cpp_namespace=DEFAULT_KERNEL_NAMESPACE structured_delegate Structured functions MUST have dispatch table is_abstract = True is_abstract = dispatch keys = DispatchKey CompositeImplicitAutograd dispatch keys = DispatchKey CompositeImplicitAutogradNestedTensor dispatch keys = DispatchKey CompositeImplicitAutograd DispatchKey CompositeImplicitAutogradNestedTensor has_composite_implicit_autograd_kernel = DispatchKey CompositeImplicitAutograd dispatch has_composite_implicit_autograd_nested_tensor_kernel = DispatchKey CompositeImplicitAutogradNestedTensor dispatch has_composite_explicit_autograd_kernel = DispatchKey CompositeExplicitAutograd dispatch has_composite_explicit_autograd_non_functional_kernel = DispatchKey CompositeExplicitAutogradNonFunctional dispatch We aren t going store dispatch metadata inline NativeFunctions instead separately indexed backend so other backends can add more dispatch entries after fact Reindex individual metadata OperatorName backend_metadata = k func name v k v dispatch items don t care exists make easier use function other yaml parsers aren t setting __line__ dict e pop __line__ None assert e f leftover entries e Asserts we can t do post_init because they rely backend-specific info structured_delegate None key STRUCTURED_DISPATCH_KEYS assert key dispatch f structured_delegate then must have key dispatch dictionary delegated NativeFunction func=func use_const_ref_for_mutable_tensors=use_const_ref_for_mutable_tensors variants=variants structured=structured structured_delegate=structured_delegate structured_inherits=structured_inherits precomputed=precomputed autogen=autogen ufunc_inner_loop=ufunc_inner_loop manual_kernel_registration=manual_kernel_registration manual_cpp_binding=manual_cpp_binding python_module=python_module category_override=category_override device_guard=device_guard device_check=device_check loc=loc cpp_no_default_args=cpp_no_default_args is_abstract=is_abstract has_composite_implicit_autograd_kernel=has_composite_implicit_autograd_kernel has_composite_implicit_autograd_nested_tensor_kernel=has_composite_implicit_autograd_nested_tensor_kernel has_composite_explicit_autograd_kernel=has_composite_explicit_autograd_kernel has_composite_explicit_autograd_non_functional_kernel=has_composite_explicit_autograd_non_functional_kernel tags=tags namespace=namespace backend_metadata validate_unstructured - None TODO probably better accumulate these errors report them all once assert structured This function structured there no valid functional variant assert structured_delegate This function delegates another structured out function no valid function found delegate may exist has wrong type __post_init__ functions dataclasses can used do extra validation after construction Notice we don t do any type validation here In fact we rely exclusively mypy check you ve done types correctly Validation nontrivial invariants cannot conveniently encoded type system __post_init__ - None func arguments out assert variants == Variant function Native functions out arguments MUST declared only function variant e g variants function otherwise you will tickle Python argument binding bug which usually manifests itself result variable being undefined structured assert func kind == SchemaKind out Put structured field out= variant function did you mean structured_delegate assert device_guard device_guard False respected structured kernels structured_delegate assert func kind = SchemaKind out structured_delegate field allowed out= functions did you mean structured assert device_guard device_guard False respected structured kernels Technically asserts above assert impossible happen assert structured structured_delegate Cannot have both structured structured_delegate function defaulted_arguments = name func schema_order_arguments default None invalid_args = set difference cpp_no_default_args defaulted_arguments assert len invalid_args == f Invalid cpp_no_default_args invalid_args structured_inherits None assert structured structured_inherits must also imply structured True str func name startswith _foreach assert device_check == DeviceCheckType NoCheck foreach kernels fall back slow path when tensor different devices device_check allowed enabled NB your function accidentally has rand dropout its name actually random feel free amend special case rand str func name dropout str func name any dropout arg name arg func arguments flat_all Backwards dropout typically deterministic backward str func name str func name name _cudnn_init_dropout_state func arguments has_generator_arg assert nondeterministic_seeded tags str func name property has_composite_kernel - bool has_composite_implicit_autograd_kernel has_composite_explicit_autograd_kernel has_composite_explicit_autograd_non_functional_kernel has_composite_implicit_autograd_kernel has_composite_implicit_autograd_nested_tensor_kernel property is_view_op - bool rets = func returns is_non_mutating_view = len rets any r annotation None r annotation is_write r rets See Note resize_ Functionalization more dtails is_inplace_view = inplace_view tags str func name = resize_ str func name = resize_as_ is_wildcard_view = any inp annotation None inp annotation alias_set_after inp func schema_order_arguments is_non_mutating_view is_inplace_view is_wildcard_view property view_schema_kind - ViewSchemaKind is_view_op func name name inplace assert inplace_view tags ViewSchemaKind aliasing_inplace is_view_op ViewSchemaKind aliasing ViewSchemaKind non_aliasing property root_name - str func name name base property part_of_structured_group - bool structured structured_delegate None SchemaKind Enum functional = auto inplace = auto out = auto mutable = auto scratch = auto A structured kernel guaranteed have functional out variant optionally inplace variant NB we create NativeFunctionsGroup even function actually annotated structured Test structured boolean see actually structured dataclass frozen=True NativeFunctionsGroup functional NativeFunction inplace NativeFunction &#124; None mutable NativeFunction &#124; None out NativeFunction property structured - bool Whether operator has meta function This information backend-agnostic out structured __post_init__ - None test_sig FunctionSchema = functional func signature f functions test_sig = f func signature raise AssertionError NativeFunctionsGroup constructed two NativeFunctions f don t have matching signatures test_sig = f func signature structured = f part_of_structured_group raise AssertionError NativeFunctionsGroup constructed structured unstructured f functions out func name f func name assert functional func kind == SchemaKind functional assert out func kind == SchemaKind out assert functional namespace == out namespace inplace None assert inplace func kind == SchemaKind inplace assert inplace namespace == functional namespace mutable None assert mutable func kind == SchemaKind mutable assert mutable namespace == functional namespace See Note Overload Ambiguity With Functional Variants assert functional func name name functional_overload structured For now structured composite kernels supported need some design work figure out how make composite case work assert out has_composite_implicit_autograd_kernel out has_composite_implicit_autograd_nested_tensor_kernel assert functional structured_delegate == out func name f functional func name delegates functional structured_delegate f its actual delegate out func name inplace None assert inplace structured_delegate == out func name generated_fns = sorted str f func name f functions generated f tags generated_fns_str = join str x x generated_fns expected_generated_fns set str = set f functions expected_generated_fns update str op op f autogen expected_generated_fns_str = join str x x sorted expected_generated_fns len expected_generated_fns == len generated_fns raise RuntimeError f The codegen expects able generate generated_fns_str In order generate them however we expect them called out explicitly yaml f Please add autogen generated_fns_str line entry str f func name expected_generated_fns_str = generated_fns_str raise RuntimeError f The codegen expects able generate generated_fns_str f To do so expects line autogen generated_fns_str f Instead found autogen expected_generated_fns_str signature - FunctionSchema out func signature functions - Iterator NativeFunction yield functional yield out inplace None yield inplace mutable None yield mutable property root_name - str functional root_name staticmethod from_dict d dict SchemaKind NativeFunction - NativeFunctionsGroup &#124; None assert d len d == None d = dict d non-destructive updates please functional = d pop SchemaKind functional None inplace = d pop SchemaKind inplace None mutable = d pop SchemaKind mutable None out = d pop SchemaKind out None assert d assert functional None There few operators which only have functional inplace variants these don t count structured our purposes here out None None assuming all variants have same namespace NativeFunctionsGroup functional=functional inplace=inplace mutable=mutable out=out dataclass frozen=True BackendMetadata The name backend kernel given operator in-tree backends These names come directly dispatch field native_functions yaml The dispatch entry optional case equivalent having written dispatch CompositeImplicitAutograd $ operator_name kernel str Whether operator has structured kernel implemented particular backend For in-tree backends they all have same value structured- listed native_functions yaml However external backends like XLA can indendently toggle which ops structured structured bool The namespace kernels default value DEFAULT_KERNEL_NAMESPACE cpp_namespace str supports_symint - bool _symint kernel dataclass frozen=True UfuncInnerLoop name str supported_dtypes OrderedSet ScalarType key stored here because affects semantics name so its helpful have them together further processing ufunc_key UfuncKey staticmethod parse value str ufunc_key UfuncKey - UfuncInnerLoop name supported_dtypes_str = value split assert supported_dtypes_str == assert supported_dtypes_str - == supported_dtypes OrderedSet ScalarType = OrderedSet k supported_dtypes_str - split supported_dtypes &#124; = ScalarType parse_set k UfuncInnerLoop name=name supported_dtypes=supported_dtypes ufunc_key=ufunc_key BackendIndex represents backend The BackendIndex encodes per-operator information potentially different each backend The most obvious example name kernel dispatch entry native_functions yaml However there can other examples different backends having different information External backends can choose opt their kernels structured independently in-tree backends which means information isn t inherently tied NativeFunction- s different per backend dataclass frozen=True BackendIndex dispatch_key DispatchKey Mainly important structured kernels determines which variant operator group used implement others All in-tree ops use out kernels while XLA uses functional kernels use_out_as_primary bool Whether backend requires device guard device checks For in-tree backends currently just CUDA HIP For out-of-tree backends currently just Intel XPU device_guard bool Whether backend in-tree CPU CUDA out-of-tree XLA external bool Other backend-specific information per-operator basis index dict OperatorName BackendMetadata staticmethod grow_index parent_index dict DispatchKey dict OperatorName BackendMetadata child_index dict DispatchKey dict OperatorName BackendMetadata - None k v child_index items op_name metadata v items assert op_name parent_index k f duplicate operator op_name dispatch key k parent_index k op_name = metadata primary g NativeFunctionsGroup - NativeFunction use_out_as_primary g out g functional has_kernel g NativeFunction &#124; NativeFunctionsGroup - bool m = get_kernel g m None get_kernel g NativeFunction &#124; NativeFunctionsGroup - BackendMetadata &#124; None isinstance g NativeFunction f = g isinstance g NativeFunctionsGroup f = primary g assert_never g f func name index None index f func name native_function_class_name - str &#124; None external f str dispatch_key NativeFunctions TODO This discrepancy isn t required we could also generated in-tree kernels It ll just require carefully updating every kernel definition + callsite every in-tree aten kernel None The function schema undoubtedly most important data structure all codegen defines type signature operators most code generation we do type directed e g look types decide what do Think about how we code generate C++ function stubs We will also see general structure how we model data code generation A few notable properties point out ahead time - These dataclasses lossless representation strings they parsed In fact we assert given information stored dataclass we can exactly reconstruct string we parsed assert inside parse definition There few reasons - If you find difficult reconstruct string given dataclass clue you data representation wrong - It helps ensure all relevant information present dataclass so downstream users aren t tempted reparse original string get some information omitted - It forces you represent data in-memory same way recorded textually which makes dataclasses easier understand someone who familiar textual format As tradeoff means you have model syntax even when inconvenient But maybe means syntax bad If you don t understand internal representation go look printing code see how maps onto surface syntax - It makes easy test parsing code parsing code inconsistent string code will fail early loudly As tradeoff makes parsing code bit brittle particular trivial whitespace changes you likely trigger assert error In general try make __str__ code simple possible even cost more complex parsing logic Additionally try minimize redundancy data representation Precomputed fields OK though they defined simple function canonical representation question - These dataclasses all frozen once constructed their values never change This makes easy tell where any given data came just look constructor As tradeoff you can t easily decorate schema extra information post-facto analysis We impose restriction make these structures more understandable dataclass frozen=True FunctionSchema The name operator function schema describes name OperatorName arguments Arguments TODO Need handle collisions argument names some point returns tuple Return property is_mutable - bool is_write arg Argument - bool arg annotation None False arg annotation is_write Corresponds torch _C _FunctionSchema is_mutable See aten src ATen core function_schema h keep these sync any is_write arguments flat_all schema_order_arguments - Iterator Argument itertools chain arguments flat_positional arguments flat_kwarg_only arguments out decl_re = re compile r P name ^\ + \ P args \ - P returns staticmethod parse func str - FunctionSchema We should probably get proper parser here decls = FunctionSchema decl_re findall func assert len decls == f Invalid function schema func ops args return_decl = decls name = OperatorName parse ops arguments = Arguments parse args returns = parse_returns return_decl r = FunctionSchema name=name arguments=arguments returns=returns assert str r == func f str r = func r returns_are_aliased - bool We assert earlier schemas can t have mix aliased non-aliased returns any r r returns r annotation None r annotation is_write __post_init__ - None arg ret zip arguments out returns assert arg annotation == ret annotation Out arguments must have matching Tensor furthermore ith-argument needs correspond ith We also enforce you have any mutable positional args then they returned This makes easier group these functions properly their functional out= counterparts arguments post_self_positional_mutable assert any annotation == r annotation r returns f If you have schema mutable positional args we expect them returned schema str Invariant we expect out arguments appear keyword arguments schema This means all mutable returns should aliased keyword argument except which we explicitly don t treat out argument because its use methods See Note is_out_fn out_and_self = list arguments out + arg arg arguments flat_positional arg name == mutable_returns = ret ret returns ret annotation None ret annotation is_write immutable_returns = ret ret returns ret annotation None ret annotation is_write Some assertions We don t want any functions type - Tensor Tensor because It s more annoying handle properly It s unnecessary - you can t method-chain first mutated output because s part tuple Instead we expect argument returned assert len mutable_returns == len immutable_returns == f NativeFunctions must have either only mutable returns only immutable returns Found str ret mutable_returns assert any ret annotation == arg annotation arg out_and_self All mutable returns must aliased either keyword argument Did you forget mark out argument keyword-only arguments out out= ops their mutable inputs only really useful method chaining And method chaining only really useful thing you re returning plain Tensor So ideally we d enforce out= ops single plain mutable tensor should tensor all other types out= op schemas should void There bunch existing out= ops tuples tensors though so we re stuck allowing any type = BaseType BaseTy Tensor arguments out assert len returns == out= ops accept tensor lists out arguments expected have no type since you can t do method chaining them mutable keyword arguments whose name has _scratch_ prefix scratch tensors memory planning should returned assert len arg arg arguments out arg name startswith _scratch_ == len returns Must many arguments there out arguments no all name name inplace self_a = arguments self_arg assert self_a self_a argument annotation self_a argument annotation is_write self_a argument type == BaseType BaseTy Tensor All inplace ops ordinary ` Tensor ` argument should allow method chaining assert len returns == returns annotation == self_a argument annotation You can t method chain non-tensor arguments though like list Tensor so all other cases we expect type none assert len returns == arguments tensor_options None assert kind == SchemaKind functional Found operator functional out variant has tensor options arguments This allowed- tensor options arguments only allowed factory functions f schema str is_functional_fn assert kind == SchemaKind functional Found operator functional its overload contains string functional This special keyword codegen please use different overload name f schema str is_functional_fn - bool functional name overload_name is_out_fn - bool Note is_out_fn out functions variants which take explicit out= argument populate into We need know schema corresponds out function several reasons - They codegen differently C++ API - codegen add_out rather than add - out argument moved front C++ argument list out functions DEFINED any function keyword-only argument mutable In principle could lead false positive you define function mutates kwarg only argument isn t true output function A more robust definition would work case would also look - The output types Out functions take arguments they mutate then them again sort definitionally what makes something out function Historically we DO check consistency - Correspondence pure variant An out function should have signature equivalent its pure variant just extra kwargs output elements This difficult actually check historically we only do check tools bool arguments out kind - SchemaKind What kind schema A functional schema one returns newly allocated output inplace schema modifies argument inplace out schema writes result into explicitly provided out argument is_out = bool arguments out is_scratch = bool arg arg arguments out arg name startswith _scratch_ is_inplace = name name inplace is_mutable = any annotation None annotation is_write arguments post_self_positional assert is_out is_inplace out= inplace schemas can also have post_self_positional mutable args we give precedence out= inplace when deciding schema kind Tradeoff we probably don t want have teach codegen looks inplace ops also worry about mutable post_self_positional arguments seems like much bigger lift classify them has having new schema kind The number ops fit strange category small enough we can probably manually write code them instead forcing codegen handle them is_inplace SchemaKind inplace is_scratch assert is_out invariant all scratch operators expected out= operators too SchemaKind scratch is_out assert is_scratch We should categorize scratch op out variant Check order statements expected noqa B SchemaKind out is_mutable SchemaKind mutable SchemaKind functional For every - If aliases input we input name - Otherwise we None If names enforced consistent aliasing information then we wouldn t need aliased_return_names - list str &#124; None outs list str &#124; None = r returns aliased_args = arguments flat_all annotation None annotation == r annotation len aliased_args == outs append None len aliased_args == outs append aliased_args name aliased_names = join name aliased_args raise AssertionError f Found r name aliases multiple inputs aliased_names outs signature strip_default bool = False strip_view_copy_name bool = False keep_return_names bool = False - FunctionSchema Certain schemas related they simply inplace out functional versions same function This method factors these schemas into core functional signature which equal across all versions Here what normalization happens schema convert signature - The overload name stripped name retained since expresses semantic content about what function does - Inplace set False - Out arguments stripped - Mutable post_self_positional args converted returns - Mutability annotations stripped sound because you cannot overload mutability annotation - Return names stripped since they overloadable some variants have names some - TensorOptions dropped because out= variants factory functions don t include them we want able pair up factory functions their out variants Finally we want able pair up related view their corresponding view_copy operators We do optionally stripping trailing _copy base name Example mutable op before after f func Mutable operator _fused_moving_avg_obs_fq_helper Tensor Tensor observer_on Tensor fake_quant_on Tensor running_min Tensor b running_max Tensor c scale Tensor d zero_point float averaging_const int quant_min int quant_max int ch_axis bool per_row_fake_quant=False bool symmetric_quant=False - Tensor output Tensor mask noqa B f func Corresponding functional operator _fused_moving_avg_obs_fq_helper functional Tensor Tensor observer_on Tensor fake_quant_on Tensor running_min Tensor running_max Tensor scale Tensor zero_point float averaging_const int quant_min int quant_max int ch_axis bool per_row_fake_quant=False bool symmetric_quant=False - Tensor output Tensor mask Tensor running_min_out Tensor running_max_out Tensor scale_out Tensor zero_point_out noqa B f func signature output _fused_moving_avg_obs_fq_helper Tensor Tensor observer_on Tensor fake_quant_on Tensor running_min Tensor running_max Tensor scale Tensor zero_point float averaging_const int quant_min int quant_max int ch_axis bool per_row_fake_quant=False bool symmetric_quant=False - Tensor Tensor Tensor Tensor Tensor Tensor noqa B strip_ret_annotation r Return - Return Return name=r name keep_return_names None type=r type annotation=None base_name = name name base strip_view_copy_name base_name endswith _copy base_name = base_name replace _copy base_name endswith _scatter base_name = base_name replace scatter inverse find mutable inputs originally returned convert them returns returns_from_mutable_inputs = tuple When we re grouping functions we strip names when we re generating actual functional variants then we follow convention what name returns Return name=f name _out keep_return_names None type=a type annotation=None itertools chain Order important here otherwise e g inplace mutable args out= mutable args won t have same signature arguments self_arg argument arguments self_arg None arguments out arguments post_self_positional annotation None annotation is_write any annotation == r annotation r returns original_returns = tuple map strip_ret_annotation returns Ordering important here We expect mutable input returns come last returns = original_returns + returns_from_mutable_inputs args_sig = arguments signature strip_default=strip_default See Note bernoulli p schema str name == bernoulli p args_sig = Arguments parse str args_sig replace float p float p= FunctionSchema name=OperatorName name=BaseOperatorName base=base_name inplace=False dunder_method=self name name dunder_method overload_name= stripped arguments=args_sig returns=returns view_signature - FunctionSchema signature strip_view_copy_name=True with_name name OperatorName - FunctionSchema FunctionSchema name=name arguments=self arguments returns=self returns property modifies_arguments - bool kind SchemaKind inplace SchemaKind out SchemaKind mutable has_symint - bool arguments has_symint_arg __str__ - str all_arguments_str = str arguments len returns == returns = str returns omit parentheses returns = + join map str returns + f name all_arguments_str - returns Here rest data model described more briefly Simplified version what actually shows up built-ins Look alias_info h expanded syntax If you need structure you also need make structure recursive so can lined up type components too For primitives isn t really necessary dataclass frozen=True Annotation Typically only has one element Not actually set so we can conveniently assume canonically ordered alias_set tuple str is_write bool alias_set_after tuple str staticmethod parse ann str - Annotation TODO implement proper parser gets more ugly Regex Explanation Example - &#124; b Group alias before optional &#124; required Matches first character example Group optional alias set after optional &#124; matches empty string example Group optional write flag matches example Group optional section containing arrow matches - &#124; b example Group optional alias after set supports wildcard matches &#124; b example Group optional sub-section alias after set matches &#124; b example m = re match r ^ a-z \ &#124; a-z - \ &#124; a-z \ &#124; a-z $ ann assert m None f unrecognized alias annotation ann before_alias = m group + m group m group alias_set = tuple before_alias split &#124; is_write = m group == assert is_write len alias_set f alias set larger than mutable got ann instead after_set = tuple m group split &#124; m group assert len before_alias len after_set f before alias set after alias set cannot larger than same time got ann instead r = Annotation alias_set=alias_set is_write=is_write alias_set_after=after_set assert str r == ann f r = ann r __str__ - str alias_set = &#124; join alias_set is_write alias_set = f alias_set alias_set_after = &#124; join alias_set_after alias_set_after alias_set = f alias_set - alias_set_after alias_set The base type system This also loosely modeled off jit_type h we ve simplified hierarchy focus aspects type system matter code generation example there s no SingleElementType subclass anymore You never actually construct Type usually s going one subclasses If Python had ADTs would one dataclass frozen=True Type staticmethod parse t str - Type r = Type _parse t assert str r == t f r = t r staticmethod _parse t str - Type m = re match r ^ + \ $ t m None OptionalType Type parse m group m = re match r ^ + \ - + \ $ t m None size = int m group m group None None ListType elem=Type parse m group size=size __torch__ torch classes prefix custom m = re match r ^__torch__\ torch\ classes\ a-zA-Z - _ + $ t m None CustomClassType m group try BaseType BaseTy t except KeyError e raise RuntimeError f unrecognized type t e __str__ - str raise NotImplementedError WARNING These concepts very well-defined For example int nullable How about int They defined so we can conveniently generate legacy Declarations yaml really we should probably just remove these some point is_base_ty_like base_ty BaseTy - bool raise NotImplementedError is_tensor_like - bool is_base_ty_like BaseTy Tensor is_generator_like - bool is_base_ty_like BaseTy Generator is_symint_like - bool is_base_ty_like BaseTy SymInt is_nullable - bool raise NotImplementedError is_list_like - ListType &#124; None raise NotImplementedError Base types simple atomic types no further structure BaseTy Enum Generator = auto ScalarType = auto Tensor = auto int = auto Dimname = auto DimVector = auto float = auto str = auto bool = auto Layout = auto Device = auto DeviceIndex = auto Scalar = auto MemoryFormat = auto QScheme = auto Storage = auto Stream = auto SymInt = auto SymBool = auto GraphModule = auto dataclass frozen=True BaseType Type name BaseTy __str__ - str f name name is_base_ty_like base_ty BaseTy - bool name == base_ty is_nullable - bool False is_list_like - ListType &#124; None None is_symint_like - bool name == BaseTy SymInt Optional types may specified may also validly given None dataclass frozen=True OptionalType Type elem Type __str__ - str f elem is_base_ty_like base_ty BaseTy - bool elem is_base_ty_like base_ty is_symint_like - bool elem is_symint_like is_nullable - bool True is_list_like - ListType &#124; None elem is_list_like A type representing PyTorch custom dataclass frozen=True CustomClassType Type class_name str __str__ - str Return name will prefix __torch__ torch classes f __torch__ torch classes class_name is_base_ty_like base_ty BaseTy - bool False is_symint_like - bool False is_nullable - bool Assume custom nullable False is_list_like - ListType &#124; None None List types specify we may have multiples element We also support explicit sizes list types these have some nontrivial semantics However C++ API purposes explicit sizes mostly erased type system DANGER WILL ROBINSON C++ elaboration depends elem type e g int elaborates differently than bool dataclass frozen=True ListType Type elem Type size int &#124; None __str__ - str size = f size size f elem size is_base_ty_like base_ty BaseTy - bool elem is_base_ty_like base_ty is_symint_like - bool elem is_symint_like is_nullable - bool elem is_nullable is_list_like - ListType &#124; None dataclass frozen=True Argument NB I didn t put kwarg_only boolean field here unlike c Argument so printing works correctly name str type Type default str &#124; None The semantics annotation field little strange Alias annotations parametrize Tensors since Tensors only things can alias This motivates why I write Tensor example Tensor because describes aliasing tensor which may optional i e alias annotation should bind first Tensor before optional postfix annotation However despite being property Tensor we c Argument store annotation top level Argument rather than inside embedded Tensor type In C++ version we then go through great lengths mimic type structure annotation structure so we can correlate annotations types Now turns out all applications code generation structure annotated types very simple So we just hard code here But we ever do get anything more complex model will have change annotation Annotation &#124; None property alias_info - Annotation &#124; None annotation staticmethod parse arg str - Argument name str default str &#124; None assert arg f illegal argument arg = arg assert arg count = == f illegal argument default value arg type_and_annot_and_name default = arg split = type_and_annot name = type_and_annot_and_name rsplit name_and_default = f name = default type_and_annot name_and_default = arg rsplit name = name_and_default default = None TODO deduplicate annotation matching Return match = re match r Tensor\ + \ type_and_annot annotation Annotation &#124; None match If you update make sure __str__ still works too assert match group unrecognized alias analysis form Tensor type_s = Tensor + match group annotation = Annotation parse match group type_s = type_and_annot annotation = None type = Type parse type_s r = Argument name=name type=type default=default annotation=annotation assert str r == arg f str r = arg r property is_write - bool annotation None annotation is_write __str__ - str type = f type annotation assert type Tensor Tensor Tensor type = type replace Tensor f Tensor annotation name None type mb_default = default mb_default = f = default f type name mb_default dataclass frozen=True Return name str &#124; None type Type annotation Annotation &#124; None property alias_info - Annotation &#124; None annotation staticmethod parse arg str - Return name str &#124; None arg type_and_annot name = arg rsplit type_and_annot = arg name = None match = re match r Tensor\ + \ type_and_annot annotation Annotation &#124; None match If you update make sure __str__ still works too assert match group unrecognized alias analysis form Tensor type_s = Tensor + match group annotation = Annotation parse match group type_s = type_and_annot annotation = None type = Type parse type_s r = Return name=name type=type annotation=annotation assert str r == arg f str r = arg r property is_write - bool annotation None annotation is_write __str__ - str type = f type annotation assert type Tensor Tensor Tensor type = type replace Tensor f Tensor annotation name None type f type name Represents argument functions may methods dataclass frozen=True SelfArgument argument Argument Bundle arguments represent TensorOptions This mostly relevant public C++ API we bake into core data model because other APIs often have interact dataclass frozen=True TensorOptionsArguments dtype Argument layout Argument device Argument pin_memory Argument all - Sequence Argument dtype layout device pin_memory dataclass frozen=True Arguments pre_self_positional usually empty notably non-empty where where condition argument comes before argument pre_self_positional tuple Argument self_arg SelfArgument &#124; None post_self_positional tuple Argument pre_tensor_options_kwarg_only tuple Argument tensor_options TensorOptionsArguments &#124; None post_tensor_options typically memory format which should part tensor options isn t right now usually placed after tensor options arguments post_tensor_options_kwarg_only tuple Argument Unlike previous codegen we have factored out out arguments canonical representation removing them kwarg arguments This choice justified numerous downstream transformations which treat out arguments specially additionally you can see canonicity violated out tuple Argument these also kwarg-only property flat_non_out - Sequence Argument ret list Argument = ret extend flat_positional ret extend flat_kwarg_only ret property flat_positional - Sequence Argument ret list Argument = ret extend pre_self_positional self_arg None ret append self_arg argument ret extend post_self_positional ret property post_self_positional_mutable - Sequence Argument post_self_positional is_write NB doesn t contain out arguments property flat_kwarg_only - Sequence Argument ret list Argument = ret extend pre_tensor_options_kwarg_only tensor_options None ret extend tensor_options all ret extend post_tensor_options_kwarg_only ret property flat_all - Sequence Argument ret list Argument = ret extend flat_positional ret extend flat_kwarg_only ret extend out ret property non_out - Sequence Argument &#124; SelfArgument &#124; TensorOptionsArguments ret list Argument &#124; SelfArgument &#124; TensorOptionsArguments = ret extend positional ret extend kwarg_only ret property positional - Sequence Argument &#124; SelfArgument ret list Argument &#124; SelfArgument = ret extend pre_self_positional self_arg None ret append self_arg ret extend post_self_positional ret property kwarg_only - Sequence Argument &#124; TensorOptionsArguments ret list Argument &#124; TensorOptionsArguments = ret extend pre_tensor_options_kwarg_only tensor_options None ret append tensor_options ret extend post_tensor_options_kwarg_only ret property all - Sequence Argument &#124; SelfArgument &#124; TensorOptionsArguments ret list Argument &#124; SelfArgument &#124; TensorOptionsArguments = ret extend positional ret extend kwarg_only ret extend out ret mutable_arg_names - list str name flat_all annotation None annotation is_write has_tensor_arg - bool any type is_tensor_like flat_non_out has_symint_arg - bool any type is_symint_like flat_non_out has_generator_arg - bool any type is_generator_like flat_non_out signature strip_default bool = False - Arguments dataclasses replace could used here less type safe so now I ve opted type everything out strip_arg_annotation Argument - Argument Argument name=a name type=a type default=a default strip_default None annotation=None Arguments pre_self_positional=tuple map strip_arg_annotation pre_self_positional self_arg= SelfArgument strip_arg_annotation self_arg argument self_arg None None post_self_positional=tuple map strip_arg_annotation post_self_positional Since TensorOptions dropped post_tensor_options_kwargs converted pre_tensor_options_kwargs pre_tensor_options_kwarg_only=tuple map strip_arg_annotation pre_tensor_options_kwarg_only + tuple map strip_arg_annotation post_tensor_options_kwarg_only TensorOptions dropped signature so we can pair factory functions their out= variants tensor_options=None post_tensor_options_kwarg_only= out arguments dropped signature out= remove_self_annotation - Arguments assert self_arg None dataclasses replace self_arg=SelfArgument dataclasses replace self_arg argument annotation=None with_out_args outs list Argument - Arguments assert len out == dataclasses replace out=tuple outs staticmethod _preparse args str - tuple list Argument list Argument list Argument positional list Argument = kwarg_only list Argument = out list Argument = arguments_acc = positional TODO Use real parser here will get bamboozled signatures contain things like std array bool note space arg args split arg continue arg == assert arguments_acc positional invalid syntax kwarg-only specifier can only occur once arguments_acc = kwarg_only continue parg = Argument parse arg Currently we rely directly invariant there NO kwarg-only mutating arguments If you want relax we will need more semantic way matching takes into account arguments In case you will have manage out computation level up FunctionSchema See Note is_out_fn parg annotation None parg annotation is_write arguments_acc positional pass do nothing arguments_acc kwarg_only arguments_acc = out assert arguments_acc out arguments_acc append parg positional kwarg_only out staticmethod parse args str - Arguments Input int x int y int z We do two phases First we parse into three main categories positional kwarg_only out Then we reparse positional kwarg_only separate out argument tensor options arguments positional kwarg_only out = Arguments _preparse args Split argument self_ix = None i enumerate positional name == self_ix = i break pre_self_positional list Argument self_arg SelfArgument &#124; None post_self_positional list Argument self_ix None pre_self_positional = positional self_ix self_arg = SelfArgument positional self_ix post_self_positional = positional self_ix + pre_self_positional = self_arg = None post_self_positional = positional Group tensor options arguments pre_tensor_options_kwarg_only list Argument = tensor_options TensorOptionsArguments &#124; None = None post_tensor_options_kwarg_only list Argument = kwarg_only_acc = pre_tensor_options_kwarg_only pred name str ty Type - Callable Argument bool lambda name == name type ty OptionalType ty predicates = order matters pred dtype Type parse ScalarType pred layout Type parse Layout pred device Type parse Device pred pin_memory Type parse bool i = while i len kwarg_only If there enough space i = len kwarg_only - len predicates And next len predicates arguments look like TensorOptions arguments all p p zip predicates kwarg_only i i + len predicates assert kwarg_only_acc pre_tensor_options_kwarg_only Group them together one argument tensor_options = TensorOptionsArguments dtype=kwarg_only i layout=kwarg_only i + device=kwarg_only i + pin_memory=kwarg_only i + i += len predicates kwarg_only_acc = post_tensor_options_kwarg_only continue kwarg_only_acc append kwarg_only i i += Arguments pre_self_positional=tuple pre_self_positional self_arg=self_arg post_self_positional=tuple post_self_positional pre_tensor_options_kwarg_only=tuple pre_tensor_options_kwarg_only tensor_options=tensor_options post_tensor_options_kwarg_only=tuple post_tensor_options_kwarg_only out=tuple out __str__ - str all_arguments list str = all_arguments extend map str flat_positional flat_kwarg_only out all_arguments append all_arguments extend map str flat_kwarg_only all_arguments extend map str out join all_arguments __post_init__ - None TODO These invariants weirdly asymmetric TODO Fancier types self_arg None assert pre_self_positional tensor_options None assert post_tensor_options_kwarg_only We don t allow any following have argument annotations keep things simple mutable_pre_self_positionals = pre_self_positional annotation None annotation is_write assert len mutable_pre_self_positionals == mutable pre_self_positional arguments currently supported schema Names validly __iXXX__ indicating inplace operations Taken https www python org dev peps pep- #new-methods NB PyTorch hasn t actually implemented all these AUGMENTED_ASSIGNMENT_NAMES = add sub mul div mod pow lshift rshift xor A BaseOperatorName what we think operator name without overload name Unusually we don t represent just string instead we directly represent few important semantic bits information we derive string namely whether s inplace add_ whether s double-underscore method __add__ dataclass frozen=True BaseOperatorName base str inplace bool dunder_method bool Note Overload Ambiguity With Functional Variants A handful operators have both mutable functional variant native_batch_norm good example although isn t case today For those operators mutable functional variant take same set arguments have different alias annotations makes ambiguous when you try resolve OverloadPacket into overload given set input arguments So instead making functional variant case real overload e g native_batch_norm mutable variant native_batch_norm functional functional variant we make new base operator native_batch_norm_functional functional variant In ideal world we would probably invert so operators native_batch_norm mutable mutable variant native_batch_norm functional variant Doing BC-breaking though so we re stuck above modeling functional_overload bool = False NB We don t officially support namespace FunctionSchema we treat prefix part base operator name __str__ consume The canonical input rest infra will contain namespace we have usecase ExecuTorch where we want support BaseOperatorName namespace namespace Optional str = None staticmethod parse op str - BaseOperatorName assert op = assert op endswith _out _out suffix reserved permitted operator names did you mean specify out overload name instead Extract namespace out Base operator name may may contain namespace E g aten __lshift__ valid base operator name __lshift__ also valid We want split namespace out base operator name match = re match r ^ $ op namespace = match group match op_without_ns = match group match op m = re match r ^__ ^_ + __$ op_without_ns m None dunder_method = True base = m group any base == f i n n AUGMENTED_ASSIGNMENT_NAMES inplace = True base = base inplace = False temporary intrinsically true has been historically true dunder methods we support we ever got say __int__ would wrong assert base = i dunder_method = False base = op_without_ns base - == _ inplace = True base = base - inplace = False See Note Overload Ambiguity With Functional Variants functional_suffix = _functional base endswith functional_suffix functional_overload = True base = base -len functional_suffix This seems complicated unnecessary so banning dunder methods now ops have functional + mutable variant like native_batch_norm assert dunder_method inplace functional_overload = False r = BaseOperatorName base=base inplace=inplace dunder_method=dunder_method functional_overload=functional_overload namespace=namespace assert str r == op f str r = op r __str__ - str namespace_prefix = f namespace namespace dunder_method i = i inplace f namespace_prefix __ i base __ i = _ inplace _functional functional_overload f namespace_prefix base i Operator name base operator name along typically user visible overload string dataclass frozen=True OperatorName name BaseOperatorName overload_name str staticmethod parse op_name str - OperatorName op_name name overload_name = op_name split name = op_name overload_name = r = OperatorName name=BaseOperatorName parse name overload_name=overload_name assert str r == op_name f str r = op_name r __str__ - str overload_name f name overload_name f name NB This must synchronized naming scheme aten src ATen templates Operators h Given function schema aten op overload If there no overload name returns f op If there overload name returns f op _ overload unambiguous_name - str overload_name f name _ overload_name f name remove_inplace - OperatorName OperatorName name=BaseOperatorName base=self name base inplace=False dunder_method=self name dunder_method overload_name=self overload_name with_overload overload str - OperatorName OperatorName name=BaseOperatorName base=self name base inplace=False dunder_method=self name dunder_method overload_name=overload gets_generated_out_inplace_wrapper f NativeFunction g NativeFunctionsGroup b BackendIndex - bool f func kind SchemaKind functional b has_kernel f b has_kernel g functional NativeFunction objects views f is_view_op returns True added into ` NativeFunctionsViewGroup ` which we can use easily access generated optional view_copy NativeFunction It s convenient group them together so we pair them up NativeFunctionsViewGroup See Note Codegen d view _copy Operators One property representation order view-like op part NativeFunctionsViewGroup aliasing version view op must exist There s one case where doesn t happen we have non-aliasing ` narrow_copy out ` op don t have corresponding aliasing ` narrow out ` op This means ` narrow_copy out ` won t appear NativeFunctionsViewGroup dataclass frozen=True NativeFunctionsViewGroup view NativeFunction Note view _copy operator optional because we currently don t generate copy variants all view ops Notably we don t generate them CompositeImplicitAutograd views we already get them free through decomposition view_copy NativeFunction &#124; None view_inplace ops also optional every view_inplace op should have out-of-place variant view_inplace NativeFunction &#124; None __post_init__ - None assert view is_view_op view_copy None assert gets_generated_view_copy view f str view func name appears new operator aliases its inputs The codegen expects you add corresponding operator native_functions yaml f get_view_copy_name view s See Note view_copy NativeFunctions details assert view_copy func name name base endswith _copy _scatter assert view func signature == view_copy func signature strip_view_copy_name=True assert view_copy view_copy tags f str view_copy func name str view tags appears view_copy operator The codegen expects view_copy operators annotated view_copy tag native_functions yaml See Note view_copy NativeFunction details view_inplace None assert view func signature == view_inplace func signature view has_composite_implicit_autograd_kernel view_inplace None assert view_inplace has_composite_implicit_autograd_kernel f str view func name str view_inplace func name must either both have CompositeImplicitAutograd kernels both have composite kernels view has_composite_implicit_autograd_nested_tensor_kernel view_inplace None assert view_inplace has_composite_implicit_autograd_nested_tensor_kernel f str view func name str view_inplace func name must either both have CompositeImplicitAutogradNestedTensor kernels both have composite kernels functions include_copy bool = True - Iterator NativeFunction yield view view_inplace None yield view_inplace view_copy None include_copy yield view_copy property root_name - str view root_name property composite - bool We currently assert group consistent If view op composite then its view_inplace op too view has_composite_implicit_autograd_kernel gets_generated_view_copy f NativeFunction - bool Only aliasing view operators get copy variant f is_view_op False We don t need bother generating copy variants CompositeImplicitAutograd ops because we can let them decompose into base view ops f has_composite_implicit_autograd_kernel False We also don t need generate copy variants inplace views inplace_view f tags False Assume ops ending _inverse have manually-defined copy variants e g slice_inverse has copy variant slice_scatter We -could- probably generate these well codegen will slightly different hand-writing these few kernels keeps codegen complexity lower f func name name base endswith _inverse False True Given NativeFunction corresponds view op returns OperatorName corresponding copy variant op get_view_copy_name f NativeFunction - OperatorName Right now when asking view op s corresponding view_copy name we assert sanity op allowed have generated view_copy variant We can do because gets_generated_view_copy tell us which ops get generated view_copy op However narrow_copy already exists op directly native_functions yaml I m hardcoding narrow_copy here now maintain assert But we could also just get rid assert list_of_ops_with_explicit_view_copy_operators = narrow str f func name list_of_ops_with_explicit_view_copy_operators assert gets_generated_view_copy f base_name = f f func name name base _copy view_copy_name = OperatorName name=BaseOperatorName base=base_name inplace=False dunder_method=f func name name dunder_method overload_name=f func name overload_name view_copy_name Helper functions parsing argument lists both inputs returns parse_returns return_decl str - tuple Return Input Output return_decl == return_decl == return_decl - == return_decl = return_decl - tuple Return parse arg arg return_decl split A Precompute instance consists map kernel argument name list Argument instances should replace kernel argument impl function dataclass frozen=True Precompute A map kernel argument name - list precomputed elements replaces supersedes replace dict str list Argument List precomputed args added without replacement add list Argument staticmethod parse src object - Precompute assert isinstance src list src list strings format kernel param name - replacement decl replacement decl add decl add decl The last line optional contains precomputed parameters added without replacement The other lines parsed get names which precomputed elements should replace which kernel arguments add_args = - src - add_list = src - split add_args = Argument parse name strip name add_list src = src - replace = raw_replace_item src assert isinstance raw_replace_item str assert - raw_replace_item precomputed parameters without replacement allowed only last line arg with_list_raw = raw_replace_item split - assert arg f illegal kernel param name arg precomputed parameters with_list = with_list_raw split with_list_args = Argument parse name strip name with_list replace arg = with_list_args r = Precompute replace=replace add=add_args assert r to_list == src r to_list = src r __post_init__ - None template parameters upper so these same then ambiguous add assert name upper = name args replace values args assert name upper = name to_list - list str replace_list = kernel_param replacement_params replace items replacements = join str param param replacement_params replace_list append f kernel_param - replacements replace_list