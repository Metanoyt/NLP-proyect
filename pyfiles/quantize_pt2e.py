typing_extensions torch torch _export passes constant_folding constant_fold torch ao quantization pt e duplicate_dq_pass DuplicateDQPass torch ao quantization pt e port_metadata_pass PortNodeMetaForQDQ torch ao quantization quantizer noqa F DerivedQuantizationSpec FixedQParamsQuantizationSpec QuantizationAnnotation QuantizationSpec QuantizationSpecBase Quantizer SharedQuantizationSpec torch fx GraphModule Node torch fx passes infra pass_manager PassManager pt e prepare prepare pt e qat_utils _fold_conv_bn_qat _fuse_conv_bn_qat pt e representation reference_representation_rewrite pt e utils _disallow_eval_train _fuse_conv_bn_ _get_node_name_to_scope quantize_fx _convert_to_reference_decomposed_fx utils DEPRECATION_WARNING __all__ = prepare_pt e prepare_qat_pt e convert_pt e typing_extensions deprecated DEPRECATION_WARNING prepare_pt e model GraphModule quantizer Quantizer - GraphModule Prepare model post training quantization Args ` model ` torch fx GraphModule model captured ` torch export export_for_training ` API ` quantizer ` A backend specific quantizer conveys how user want model quantized Tutorial how write quantizer can found here https pytorch org tutorials prototype pt e_quantizer html Return A GraphModule observer based quantizer annotation ready calibration Example torch torch ao quantization quantize_pt e prepare_pt e torch ao quantization quantizer XNNPACKQuantizer get_symmetric_quantization_config M torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x initialize floating point model float_model = M eval define calibration function calibrate model data_loader model eval torch no_grad image target data_loader model image Step program capture NOTE API will updated torch export API future captured result should mostly stay same m = torch export export_for_training m example_inputs module we get model aten ops Step quantization backend developer will write their own Quantizer expose methods allow users express how they want model quantized quantizer = XNNPACKQuantizer set_global get_symmetric_quantization_config m = prepare_pt e m quantizer run calibration calibrate m sample_inference_data torch _C _log_api_usage_once quantization_api quantize_pt e prepare_pt e original_graph_meta = model meta node_name_to_scope = _get_node_name_to_scope model TODO check qconfig_mapping make sure conv bn both configured quantized before fusion TODO maybe rewrite subgraph_rewriter _fuse_conv_bn_ model model = quantizer transform_for_annotation model quantizer annotate model quantizer validate model model = prepare model node_name_to_scope is_qat=False obs_or_fq_callback=quantizer prepare_obs_or_fq_callback model meta update original_graph_meta model = _disallow_eval_train model model typing_extensions deprecated DEPRECATION_WARNING prepare_qat_pt e model GraphModule quantizer Quantizer - GraphModule Prepare model quantization aware training Args ` model ` torch fx GraphModule see func ` ~torch ao quantization quantize_pt e prepare_pt e ` ` quantizer ` see func ` ~torch ao quantization quantize_pt e prepare_pt e ` Return A GraphModule fake quant modules based quantizer annotation ready quantization aware training Example torch torch ao quantization quantize_pt e prepare_qat_pt e torch ao quantization quantizer XNNPACKQuantizer get_symmetric_quantization_config M torch nn Module __init__ - None super __init__ linear = torch nn Linear forward x linear x initialize floating point model float_model = M eval define training loop quantization aware training train_loop model train_data model train image target data_loader Step program capture NOTE API will updated torch export API future captured result should mostly stay same m = torch export export_for_training m example_inputs module we get model aten ops Step quantization backend developer will write their own Quantizer expose methods allow users express how they want model quantized quantizer = XNNPACKQuantizer set_global get_symmetric_quantization_config m = prepare_qat_pt e m quantizer run quantization aware training train_loop prepared_model train_loop torch _C _log_api_usage_once quantization_api quantize_pt e prepare_qat_pt e original_graph_meta = model meta node_name_to_scope = _get_node_name_to_scope model model = quantizer transform_for_annotation model quantizer annotate model quantizer validate model Perform fusion after annotate avoid quantizing ops new subgraph don t need quantized TODO only fuse conv bn both configured quantized _fuse_conv_bn_qat model model = prepare model node_name_to_scope is_qat=True obs_or_fq_callback=quantizer prepare_obs_or_fq_callback model meta update original_graph_meta model = _disallow_eval_train model model _QUANT_OPS = torch ops quantized_decomposed quantize_per_tensor default torch ops quantized_decomposed quantize_per_tensor tensor torch ops quantized_decomposed quantize_per_channel default torch ops pt e_quant quantize_affine _quant_node_constraint n Node - bool If there any pure ops between get_attr quantize op they will const propagated e g get_attr weight - transpose - quantize - dequantize Note dequantize op going constant propagated This filter added because we don t want constant fold things related quantization n op == call_function n target _QUANT_OPS typing_extensions deprecated DEPRECATION_WARNING convert_pt e model GraphModule use_reference_representation bool = False fold_quantize bool = True - GraphModule Convert calibrated trained model quantized model Args ` model ` torch fx GraphModule calibrated trained model ` use_reference_representation ` bool boolean flag indicate whether produce reference representation ` fold_quantize ` bool boolean flag whether fold quantize op Returns quantized model either q dq representation reference representation Example prepared_model model produced ` prepare_pt e ` ` prepare_qat_pt e ` calibration training ` convert_pt e ` produces quantized model represents quantized computation quantize dequantize ops fp ops default Please refer https pytorch org tutorials prototype pt e_quant_ptq_static html#convert-the-calibrated-model-to-a-quantized-model detailed explanation output quantized model quantized_model = convert_pt e prepared_model torch _C _log_api_usage_once quantization_api quantize_pt e convert_pt e isinstance use_reference_representation bool raise ValueError Unexpected argument type ` use_reference_representation ` f please make sure you intend pass argument use_reference_representation convert_pt e original_graph_meta = model meta model = _convert_to_reference_decomposed_fx model model = _fold_conv_bn_qat model pm = PassManager DuplicateDQPass model = pm model graph_module pm = PassManager PortNodeMetaForQDQ model = pm model graph_module fold_quantize constant_fold model _quant_node_constraint use_reference_representation model = reference_representation_rewrite model model meta update original_graph_meta model = _disallow_eval_train model model