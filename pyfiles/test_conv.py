Owner s module intel copy itertools math unittest itertools product torch torch backends cudnn cudnn torch nn nn torch nn functional F torch _C _dynamo guards assert_size_stride torch testing make_tensor torch testing _internal common_device_type dtypes instantiate_device_type_tests onlyXPU torch testing _internal common_dtype floating_types_and torch testing _internal common_nn _test_module_empty_input NNTestCase torch testing _internal common_utils dtype prec_DONTUSE gradcheck gradgradcheck parametrize parametrize_test run_tests set_default_dtype TEST_SCIPY TEST_WITH_ROCM AMPERE_OR_ROCM = TEST_WITH_ROCM torch cuda is_tf _supported TEST_SCIPY scipy ndimage scipy signal TestConvolutionNNDeviceType NNTestCase run_conv_double_back_test kern stride padding chan_in chan_out batch_size inp_size dilation no_weight groups= use_xpu=False use_bias=True dtype=torch double device = torch device xpu use_xpu cpu x = torch randn batch_size chan_in inp_size inp_size device=device dtype=dtype requires_grad=True weight = torch randn chan_out chan_in groups kern kern device=device dtype=dtype requires_grad=not no_weight use_bias bias = torch randn chan_out device=device dtype=dtype requires_grad=True bias = None func inputs use_bias lx lweight lbias = inputs lx lweight = inputs lbias = None out = F conv d lx lweight lbias stride padding dilation groups out use_bias inputs = x weight bias inputs = x weight dummy_out = func inputs grad_y = torch randn_like dummy_out device=device dtype=dtype requires_grad=True dtype == torch float g = torch autograd grad dummy_out sum x create_graph=True g requires_grad gradgradcheck func inputs grad_y dtypes floating_types_and torch half torch bfloat test_Conv d_large_workspace device dtype sizes = run_test benchmark conv = torch nn Conv d kernel_size= padding= device dtype size sizes x = torch randn size device=device dtype=dtype out = conv x detach clone requires_grad_ out backward torch ones_like out run_test benchmark=False run_test benchmark=True dtypes torch half torch float test_ConvTranspose d_large_output_padding device dtype net = torch nn ConvTranspose d kernel_size= stride= padding= output_padding= device=device dtype=dtype net = torch nn ConvTranspose d kernel_size= stride= padding= output_padding= device=device dtype=dtype net = torch nn ConvTranspose d kernel_size= stride= padding= output_padding= device=device dtype=dtype x = torch rand device=device dtype=dtype requires_grad=True x = net x x = net x x = net x x backward torch randn_like x dtypes torch float torch double torch half test_Conv d_depthwise_naive_groups device dtype dtype == torch half xpu device skipTest The accuracy issue dtype fp would fixed oneDNN v depth_multiplier m = nn Conv d depth_multiplier kernel_size= groups= device dtype i = torch randn device=device dtype=dtype div_ requires_grad_ output = m i grad_output = torch randn depth_multiplier device=device dtype=dtype output backward grad_output offset = depth_multiplier m = nn Conv d depth_multiplier kernel_size= device dtype m weight data = m weight data offset clone m bias data = m bias data offset clone i = i detach clone requires_grad_ output = m i output backward grad_output offset contiguous m = nn Conv d depth_multiplier kernel_size= device dtype m weight data copy_ m weight data offset m bias data copy_ m bias data offset i = i detach clone requires_grad_ output = m i output backward grad_output offset contiguous assertEqual output torch cat output output atol=dtype prec_DONTUSE dtype rtol= assertEqual i grad data torch cat i grad data i grad data atol=dtype prec_DONTUSE dtype rtol= assertEqual m bias grad data torch cat m bias grad data m bias grad data atol=dtype prec_DONTUSE dtype rtol= assertEqual m weight grad data torch cat m weight grad data m weight grad data atol=dtype prec_DONTUSE dtype rtol= dtypes torch float torch double torch half test_Conv d_depthwise_naive_groups device dtype dtype == torch half xpu device skipTest The accuracy issue dtype fp would fixed oneDNN v depth_multiplier m = nn Conv d depth_multiplier kernel_size= groups= device dtype i = torch randn device=device dtype=dtype div_ requires_grad_ output = m i grad_output = torch randn depth_multiplier device=device dtype=dtype output backward grad_output offset = depth_multiplier m = nn Conv d depth_multiplier kernel_size= device dtype m weight data = m weight data offset clone m bias data = m bias data offset clone i = i detach clone requires_grad_ output = m i output backward grad_output offset contiguous m = nn Conv d depth_multiplier kernel_size= device dtype m weight data copy_ m weight data offset m bias data copy_ m bias data offset i = i detach clone requires_grad_ output = m i output backward grad_output offset contiguous atol rtol = e- e- assertEqual output torch cat output output atol=atol rtol=rtol assertEqual i grad data torch cat i grad data i grad data atol=dtype prec_DONTUSE dtype rtol= assertEqual m bias grad data torch cat m bias grad data m bias grad data atol=dtype prec_DONTUSE dtype rtol= assertEqual m weight grad data torch cat m weight grad data m weight grad data atol=atol rtol=rtol dtypes torch float torch double torch half test_noncontig_conv_grad device dtype module = nn Conv d kernel_size= padding= device dtype input = torch randn dtype=dtype device=device requires_grad=True output = module input grad = torch randn dtype=dtype device=device assert grad is_contiguous output backward grad retain_graph=True assertIsNotNone input grad result = input grad data clone input grad data zero_ output backward grad contiguous assertEqual result input grad data atol=dtype prec_DONTUSE dtype rtol= dtypes torch double test_conv_double_backward device dtype torch backends cudnn flags enabled=True deterministic=True batch_size = kern inp_size dilations stride padding chan_in chan_out dilation product dilations no_weight = stride == result = run_conv_double_back_test kern stride padding chan_in chan_out batch_size inp_size dilation no_weight use_xpu=True dtype=dtype assertTrue result Conv double backward test failed test_conv_double_backward_no_bias kern stride = chan_in chan_out = batch_size inp_size = padding dilation = no_weight use_bias = False True result = run_conv_double_back_test kern stride padding chan_in chan_out batch_size inp_size dilation no_weight use_bias=use_bias assertTrue result Conv double backward test failed test_conv_double_backward_groups kern stride padding = chan_in chan_out = batch_size inp_size dilation = no_weight = False groups = result = run_conv_double_back_test kern stride padding chan_in groups chan_out groups batch_size inp_size dilation no_weight groups=groups assertTrue result Conv double backward test failed test_conv_double_backward_stride batch_size = kern inp_size dilations stride padding chan_in chan_out dilation product dilations no_weight = False run_conv_double_back_test kern stride padding chan_in chan_out batch_size inp_size dilation no_weight dtypes torch float test_conv d_same_padding device dtype test_args = range range in_size k_size dilation stride itertools product test_args x = torch rand in_size device=device dtype=dtype y = torch rand k_size device=device dtype=dtype z = F conv d x y padding= same dilation=dilation stride=stride assertEqual z size int math ceil in_size stride x = torch rand device=device dtype=dtype y = torch rand device=device dtype=dtype expect = F conv d x y padding= actual = F conv d x y padding= same assertEqual expect actual x = torch rand device=device dtype=dtype y = torch rand device=device dtype=dtype expect = F conv d x y padding= dilation= actual = F conv d x y padding= same dilation= assertEqual expect actual expect = F conv d x y padding= dilation= actual = F conv d x y padding= same dilation= assertEqual expect actual dtypes torch float test_conv d_same_padding device dtype rtol atol = None None x = torch rand device=device dtype=dtype y = torch rand device=device dtype=dtype expect = F conv d x y padding= actual = F conv d x y padding= same assertEqual expect actual rtol=rtol atol=atol expect = F conv d x y padding= dilation= actual = F conv d x y padding= same dilation= assertEqual expect actual rtol=rtol atol=atol y = torch rand device=device dtype=dtype expect = F conv d x y padding= dilation= actual = F conv d x y padding= same dilation= assertEqual expect actual rtol=rtol atol=atol dtypes torch float test_conv d_valid_padding device dtype x = torch rand device=device dtype=dtype y = torch rand device=device dtype=dtype expect = F conv d x y actual = F conv d x y padding= valid assertEqual expect actual dtypes torch float test_conv d_valid_padding device dtype x = torch rand device=device dtype=dtype y = torch rand device=device dtype=dtype expect = F conv d x y actual = F conv d x y padding= valid assertEqual expect actual dtypes torch float test_conv d_valid_padding device dtype x = torch rand dtype=dtype device=device y = torch rand dtype=dtype device=device expect = F conv d x y actual = F conv d x y padding= valid assertEqual expect actual dtypes torch float test_conv d_same_padding_backward device dtype x = torch rand dtype=dtype device=device requires_grad=True y = torch rand dtype=dtype device=device requires_grad=True z = F conv d x y padding= dilation= z sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None z = F conv d x y padding= same dilation= z sum abs backward assertEqual gx_expect x grad assertEqual gy_expect y grad x grad y grad = None None z = F conv d x y padding= z sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None z = F conv d x y padding= same z sum abs backward assertEqual gx_expect x grad assertEqual gy_expect y grad dtypes torch float test_conv d_same_padding_backward device dtype x = torch rand device=device dtype=dtype requires_grad=True y = torch rand device=device dtype=dtype requires_grad=True z = F conv d x y padding= dilation= z sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None z = F conv d x y padding= same dilation= z sum abs backward assertEqual gx_expect x grad assertEqual gy_expect y grad x grad y grad = None None y = torch rand device=device dtype=dtype requires_grad=True z = F conv d x y padding= z sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None z = F conv d x y padding= same z sum abs backward assertEqual gx_expect x grad assertEqual gy_expect y grad dtypes torch double test_conv d_same_padding_backward device dtype x = torch rand dtype=dtype device=device requires_grad=True y = torch rand dtype=dtype device=device requires_grad=True z = F conv d x y padding= dilation= z sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None z = F conv d x y padding= same dilation= z sum abs backward assertEqual gx_expect x grad assertEqual gy_expect y grad x grad y grad = None None gradcheck lambda x y F conv d x y padding= same dilation= x y check_forward_ad=True nondet_tol= e- gradgradcheck lambda x y F conv d x y padding= same dilation= x y check_fwd_over_rev=True y = torch rand dtype=dtype device=device requires_grad=True z = F conv d x y padding= z sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None z = F conv d x y padding= same z sum abs backward assertEqual gx_expect x grad assertEqual gy_expect y grad gradcheck lambda x y F conv d x y padding= same x y check_forward_ad=True nondet_tol= e- gradgradcheck lambda x y F conv d x y padding= same x y check_fwd_over_rev=True dtypes torch float test_conv d_valid_padding_backward device dtype x = torch rand dtype=dtype device=device requires_grad=True y = torch rand dtype=dtype device=device requires_grad=True F conv d x y padding= sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None F conv d x y padding= valid sum abs backward gx_actual gy_actual = x grad y grad assertEqual gx_expect gx_actual assertEqual gy_expect gy_actual unittest skipIf TEST_SCIPY Scipy required test dtypes torch float parametrize_test mode valid same test_conv d_vs_scipy device dtype mode t = make_tensor device=device dtype=dtype feat_dim = t shape weight_even = make_tensor device=device dtype=dtype weight_odd = make_tensor device=device dtype=dtype _test t weight mode t_a = t view - cpu numpy w_a = weight view - cpu numpy expected = scipy signal convolve t_a w_a mode=mode kwargs = padding mode mode == same p = weight shape t = torch nn functional pad t p p kwargs pop padding weight_flipped = torch flip weight actual = torch nn functional conv d t weight_flipped kwargs squeeze mode == same actual = actual feat_dim assertEqual actual expected atol= e- rtol= e- set_default_dtype torch float _test t weight_even mode _test t weight_odd mode unittest skipIf TEST_SCIPY Scipy required test dtypes torch float parametrize_test mode valid same test_conv d_vs_scipy device dtype mode t = make_tensor device=device dtype=dtype weight_even = make_tensor device=device dtype=dtype weight_odd = make_tensor device=device dtype=dtype _test t weight mode t_a = t squeeze cpu numpy w_a = weight squeeze squeeze cpu numpy expected = scipy signal convolve d t_a w_a mode=mode kwargs = padding mode mode == same left_right_pad = weight shape top_bottom_pad = weight shape p = left_right_pad left_right_pad top_bottom_pad top_bottom_pad t = torch nn functional pad t p kwargs pop padding weight_flipped = torch flip weight actual = torch nn functional conv d t weight_flipped kwargs squeeze mode == same actual = actual assertEqual actual expected rtol= e- atol= e- set_default_dtype torch float _test t weight_even mode _test t weight_odd mode unittest skipIf TEST_SCIPY Scipy required test dtypes torch float parametrize_test mode valid same test_conv d_vs_scipy device dtype mode t = make_tensor device=device dtype=dtype weight_even = make_tensor device=device dtype=dtype weight_odd = make_tensor device=device dtype=dtype _test t weight mode t_a = t squeeze cpu numpy w_a = weight squeeze squeeze cpu numpy expected = scipy signal convolve t_a w_a mode=mode kwargs = padding mode mode == same left_right_pad = weight shape top_bottom_pad = weight shape front_back_pad = weight shape p = left_right_pad left_right_pad top_bottom_pad top_bottom_pad front_back_pad front_back_pad t = torch nn functional pad t p kwargs pop padding weight_flipped = torch flip weight actual = torch nn functional conv d t weight_flipped kwargs squeeze mode == same actual = actual assertEqual actual expected rtol= e- atol= e- set_default_dtype torch float _test t weight_even mode _test t weight_odd mode dtypes torch float test_conv d_valid_padding_backward device dtype x = torch rand device=device dtype=dtype requires_grad=True y = torch rand device=device dtype=dtype requires_grad=True F conv d x y padding= sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None F conv d x y padding= valid sum abs backward gx_actual gy_actual = x grad y grad assertEqual gx_expect gx_actual assertEqual gy_expect gy_actual dtypes torch double test_conv d_valid_padding_backward device dtype x = torch rand dtype=dtype device=device requires_grad=True y = torch rand dtype=dtype device=device requires_grad=True F conv d x y padding= sum abs backward gx_expect gy_expect = x grad y grad x grad y grad = None None F conv d x y padding= valid sum abs backward gx_actual gy_actual = x grad y grad assertEqual gx_expect gx_actual assertEqual gy_expect gy_actual gradcheck lambda x y F conv d x y padding= valid x y check_forward_ad=True gradgradcheck lambda x y F conv d x y padding= valid x y check_fwd_over_rev=True parametrize_test N range name_fn=lambda N f ConvTranspose N d test_conv_transpose_with_output_size_and_no_batch_dim device N inp = torch randn N == device=device output_size = N == ConvTransposeNd = getattr nn f ConvTranspose N d m = ConvTransposeNd kernel_size= stride= padding= bias=False device=device output = m inp output_size=output_size assertEqual output shape output_size dtypes torch float test_conv_empty_channel device dtype in_channels = mod = torch nn Conv d in_channels stride= dtype=dtype device inp = torch randn device=device dtype=dtype _test_module_empty_input mod inp check_size=False assertRaisesRegex RuntimeError Given groups= weight inp = torch randn device=device dtype=dtype mod inp mod = torch nn Conv d in_channels stride= dtype=dtype device inp = torch randn device=device dtype=dtype _test_module_empty_input mod inp check_size=False assertRaisesRegex RuntimeError Given groups= weight inp = torch randn device=device dtype=dtype mod inp mod = torch nn Conv d in_channels stride= dtype=dtype device inp = torch randn device=device dtype=dtype _test_module_empty_input mod inp check_size=False assertRaisesRegex RuntimeError Given groups= weight inp = torch randn device=device dtype=dtype mod inp test_group_conv_empty device mod = torch nn Conv d stride= kernel_size= padding= groups= device inp = torch randn device=device _test_module_empty_input mod inp check_size=False test_group_convTranspose_empty device mod = torch nn ConvTranspose d stride= kernel_size= padding= groups= device inp = torch randn device=device _test_module_empty_input mod inp check_size=False test_convTranspose_empty device mod = torch nn ConvTranspose d stride= kernel_size= padding= device inp = torch randn device=device _test_module_empty_input mod inp check_size=False test_conv_large_nosplit device dtype = torch half conv = nn Conv d device dtype input_large = torch randn dtype=dtype device=device conv input_large conv = torch nn Conv d device dtype input_large = torch randn dtype=dtype device=device conv input_large test_conv_noncontig_weights device dim grouped False True nc = groups = grouped w = torch randn dim device=device w = w expand nc int nc groups + list w shape w = w detach requires_grad_ x = torch randn nc + dim device=device requires_grad=True y = getattr F f conv dim d x w groups=groups y sum backward y = getattr F f conv_transpose dim d x w groups=groups y sum backward test_conv_noncontig_weights_and_bias device bias True False conv = nn Conv d kernel_size= stride= padding= bias=bias device torch float input_nc = torch randn device=device dtype=torch float input_c = input_nc contiguous weight_nc = torch randn device=device dtype=torch float conv weight = nn Parameter weight_nc weight_c = conv weight contiguous bias bias_nc = torch randn device=device dtype=torch float conv bias = nn Parameter bias_nc bias_c = conv bias contiguous out = conv input_nc conv weight = nn Parameter weight_c bias conv bias = nn Parameter bias_c out = conv input_c assertEqual out out test_conv_transposed_large device dtype = torch half device_type == cuda torch float conv = nn ConvTranspose d bias=False device dtype input_large = torch randn dtype=dtype device=device ret = conv input_large maxdiff = ret narrow - conv input_large narrow abs_ max item maxdiff = ret narrow - conv input_large narrow abs_ max item maxdiff = ret narrow - conv input_large narrow abs_ max item maxdiff = ret narrow - conv input_large narrow abs_ max item assertEqual maxdiff assertEqual maxdiff assertEqual maxdiff assertEqual maxdiff test_conv_large device dtype = torch half device_type == cuda torch float conv = nn Conv d bias=False device dtype input_large = torch randn dtype=dtype device=device ret = conv input_large assertEqual ret conv input_large assertEqual ret conv input_large assertEqual ret conv input_large conv zero_grad ret view - max dim= values sum backward del ret grad = conv weight grad detach clone conv zero_grad conv input_large view - max dim= values sum backward conv input_large view - max dim= values sum backward conv input_large view - max dim= values sum backward grad = conv weight grad detach clone scale = grad abs mean grad = grad scale grad = grad scale assertEqual grad grad atol= e- rtol= e- test_Conv d_size_ _kernel device x_cpu = torch randn conv_cpu = torch nn Conv d kernel_size= y_cpu = conv_cpu x_cpu y = torch rand_like y_cpu y_cpu backward y cudnn flags enabled=False conv_cuda = torch nn Conv d kernel_size= device conv_cuda bias data copy_ conv_cpu bias data conv_cuda weight data copy_ conv_cpu weight data y_cuda = conv_cuda x_cpu device y_cuda backward y device assertEqual y_cpu y_cuda atol= e- rtol= exact_device=False assertEqual conv_cpu bias grad data conv_cuda bias grad data atol= e- rtol= exact_device=False assertEqual conv_cpu weight grad data conv_cuda weight grad data atol= e- rtol= exact_device=False test_ConvTranspose d_size_ _kernel device x_cpu = torch randn conv_cpu = torch nn ConvTranspose d kernel_size= y_cpu = conv_cpu x_cpu y = torch rand_like y_cpu y_cpu backward y conv_cuda = torch nn ConvTranspose d kernel_size= device conv_cuda bias data copy_ conv_cpu bias data conv_cuda weight data copy_ conv_cpu weight data y_cuda = conv_cuda x_cpu device y_cuda backward y device assertEqual y_cpu y_cuda atol= e- rtol= exact_device=False assertEqual conv_cpu bias grad data conv_cuda bias grad data atol= e- rtol= exact_device=False assertEqual conv_cpu weight grad data conv_cuda weight grad data atol= e- rtol= exact_device=False test_ConvTranspose d_size_ _kernel device set_default_dtype torch double x_cpu = torch randn conv_cpu = torch nn ConvTranspose d kernel_size= y_cpu = conv_cpu x_cpu y = torch rand_like y_cpu y_cpu backward y conv_cuda = torch nn ConvTranspose d kernel_size= device conv_cuda bias data copy_ conv_cpu bias data conv_cuda weight data copy_ conv_cpu weight data y_cuda = conv_cuda x_cpu device y_cuda backward y device assertEqual y_cpu y_cuda atol= e- rtol= exact_device=False assertEqual conv_cpu bias grad data conv_cuda bias grad data atol= e- rtol= exact_device=False assertEqual conv_cpu weight grad data conv_cuda weight grad data atol= e- rtol= exact_device=False dtypes torch float test_Conv d_naive_groups device dtype m = nn Conv d kernel_size= groups= device dtype i = torch randn device=device dtype=dtype requires_grad=True output = m i grad_output = torch randn device=device dtype=dtype output backward grad_output m = nn Conv d kernel_size= device dtype m weight data copy_ m weight data m bias data copy_ m bias data i = i data contiguous requires_grad_ True output = m i output backward grad_output contiguous m = nn Conv d kernel_size= device dtype m weight data copy_ m weight data m bias data copy_ m bias data i = i data contiguous requires_grad_ True output = m i output backward grad_output contiguous assertEqual output torch cat output output assertEqual i grad data torch cat i grad data i grad data atol=dtype prec_DONTUSE dtype rtol= assertEqual m bias grad data torch cat m bias grad data m bias grad data atol=dtype prec_DONTUSE dtype rtol= assertEqual m weight grad data torch cat m weight grad data m weight grad data atol=dtype prec_DONTUSE dtype rtol= dtypes torch double test_Conv d_backward_depthwise device dtype x = torch randn device=device dtype=dtype requires_grad=True weight = torch randn device=device dtype=dtype requires_grad=True conv d_depthwise x weight torch nn functional conv d x weight bias=None stride= groups= torch autograd gradcheck conv d_depthwise x weight dtypes torch half torch float test_conv_cudnn_nhwc device dtype helper n c h w out_channels kernel_size groups input = torch randint - n c h w dtype=dtype device=device memory_format=torch channels_last input requires_grad_ conv = nn Conv d c out_channels kernel_size groups=groups device=device dtype=dtype memory_format=torch channels_last p conv parameters p data = torch randint_like p - ref_input = input detach clone contiguous double requires_grad_ ref_conv = nn Conv d c out_channels kernel_size groups=groups ref_conv load_state_dict conv state_dict ref_conv = ref_conv device=device dtype=torch double memory_format=torch contiguous_format out = conv input ref_out = ref_conv ref_input grad = torch randint_like out - ref_grad = grad detach clone double contiguous out backward grad ref_out backward ref_grad assertTrue out is_contiguous memory_format=torch channels_last assertTrue input grad is_contiguous memory_format=torch channels_last assertTrue conv weight grad is_contiguous memory_format=torch channels_last assertTrue ref_out is_contiguous assertTrue ref_input grad is_contiguous assertTrue ref_conv weight grad is_contiguous assertEqual out ref_out exact_dtype=False assertEqual conv weight grad ref_conv weight grad exact_dtype=False assertEqual conv bias grad ref_conv bias grad exact_dtype=False assertEqual input grad ref_input grad exact_dtype=False helper out_channels= kernel_size= groups= helper out_channels= kernel_size= groups= helper out_channels= kernel_size= groups= helper out_channels= kernel_size= groups= dtypes torch half torch float test_conv_cudnn_ndhwc device dtype helper n c d h w out_channels kernel_size groups input = torch randint - n c d h w dtype=dtype device=device memory_format=torch channels_last_ d input requires_grad_ conv = nn Conv d c out_channels kernel_size groups=groups device=device dtype=dtype memory_format=torch channels_last_ d p conv parameters p data = torch randint_like p - ref_input = input detach clone contiguous double requires_grad_ ref_conv = nn Conv d c out_channels kernel_size groups=groups ref_conv load_state_dict conv state_dict ref_conv = ref_conv device=device dtype=torch double memory_format=torch contiguous_format out = conv input ref_out = ref_conv ref_input grad = torch randint_like out - ref_grad = grad detach clone double contiguous out backward grad ref_out backward ref_grad assertTrue out is_contiguous memory_format=torch channels_last_ d assertTrue input grad is_contiguous memory_format=torch channels_last_ d assertTrue conv weight grad is_contiguous memory_format=torch channels_last_ d assertTrue ref_out is_contiguous assertTrue ref_input grad is_contiguous assertTrue ref_conv weight grad is_contiguous assertEqual out ref_out exact_dtype=False assertEqual conv weight grad ref_conv weight grad exact_dtype=False assertEqual conv bias grad ref_conv bias grad exact_dtype=False assertEqual input grad ref_input grad exact_dtype=False helper out_channels= kernel_size= groups= helper out_channels= kernel_size= groups= helper out_channels= kernel_size= groups= helper out_channels= kernel_size= groups= _run_conv layer device inp grad ref_conv ref_input ref_out input_format weight_format grad_format output_format conv = layer inp size grad size ref_conv weight size float device conv load_state_dict ref_conv state_dict weight_data = conv weight detach clone contiguous memory_format=weight_format conv weight data = weight_data resize_ weight_data size memory_format=weight_format input = inp clone contiguous memory_format=input_format input resize_ input size memory_format=input_format input = input requires_grad_ grad = grad contiguous memory_format=grad_format grad resize_ grad size memory_format=grad_format out = conv input out backward grad assertTrue out is_contiguous memory_format=output_format assertEqual out ref_out assertEqual conv weight grad ref_conv weight grad assertEqual conv bias grad ref_conv bias grad assertEqual input grad ref_input grad _test_conv_cudnn_nhwc_nchw layer n c h w k filter_size device data = torch randint n c h w dtype=torch float device=device ref_input = data clone contiguous requires_grad_ True ref_conv = layer c k filter_size float device ref_out = ref_conv ref_input grad = torch randint ref_out size dtype=torch float device=device ref_out backward grad w_f torch contiguous_format torch channels_last g_f torch contiguous_format torch channels_last input_format torch contiguous_format torch channels_last output_format = torch contiguous_format input_format == torch channels_last output_format = torch channels_last w_f == torch channels_last output_format = torch channels_last _run_conv layer device data grad ref_conv ref_input ref_out input_format w_f g_f output_format dtypes torch float torch double test_conv_cudnn_nhwc_support device dtype input = torch randn dtype=dtype device=device requires_grad=True weight = torch randn dtype=dtype device=device requires_grad=True weight = weight memory_format=torch channels_last o = torch conv d input weight None assertTrue o is_contiguous memory_format=torch channels_last o sum backward dtypes torch float test_conv d_no_grad device dtype batch groups input = torch rand batch groups dtype=dtype device=device m = nn Conv d groups kernel_size= groups=groups dtype=dtype device=device torch no_grad output_ng = m input output = m input assertEqual output output_ng rtol= e- atol= e- unittest skipIf torch xpu device_count only one GPU detected dtypes torch double torch float torch half test_conv d_on_multi_device dtype input = torch randn dtype=dtype requires_grad=True conv = torch nn Conv d kernel_size= padding= dtype=dtype output_grad = torch randn dtype=dtype input_ = input device= xpu conv_ = copy deepcopy conv device= xpu output_ = conv_ input_ input_ = input device= xpu conv_ = copy deepcopy conv device= xpu output_ = conv_ input_ assertEqual output_ cpu output_ cpu output_grad_ = output_grad device= xpu output_ backward output_grad_ output_grad_ = output_grad device= xpu output_ backward output_grad_ assertEqual output_grad_ cpu output_grad_ cpu test_conv_double_backward_strided_with_ D_input_and_weight device input = torch randn device=device weight = torch randn device=device bias = torch randn device=device stride = padding = dilation = transposed = False output_padding = groups = output = torch ops aten convolution input weight bias stride padding dilation transposed output_padding groups ggI = torch randn input shape device=device ggW = torch randn weight shape device=device ggB = torch randn bias shape device=device gO = torch randn output shape device=device output_mask = True True True grad_grad_output grad_input grad_weight = torch ops aten _convolution_double_backward ggI ggW ggB gO weight input stride padding dilation transposed output_padding groups output_mask assertEqual grad_grad_output shape gO shape assertEqual grad_input shape input shape assertEqual grad_weight shape weight shape onlyXPU dtypes torch float torch bfloat torch float torch float test_channels_last_ouput_stride device dtype input = torch randn device=device dtype=dtype requires_grad=True weight = torch randn device=device dtype=dtype requires_grad=True input = input memory_format=torch channels_last weight = weight memory_format=torch channels_last out = torch conv d input weight None input NHWC output NHWC assert_size_stride out onlyXPU test_onednn_allow_tf _get_set torch backends mkldnn flags enabled=None deterministic=None allow_tf =False assertFalse torch backends mkldnn allow_tf torch backends mkldnn flags enabled=None deterministic=None allow_tf =True assertTrue torch backends mkldnn allow_tf instantiate_device_type_tests TestConvolutionNNDeviceType globals only_for= xpu allow_xpu=True __name__ == __main__ run_tests