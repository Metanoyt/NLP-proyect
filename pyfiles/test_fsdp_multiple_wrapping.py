Owner s oncall distributed sys torch torch distributed dist torch distributed fsdp FullyShardedDataParallel FSDP torch nn Linear Module Sequential torch optim SGD torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp FSDPTest get_devtype torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN device_type = torch device get_devtype dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit InnerModel Module __init__ device super __init__ layers = Sequential FSDP Linear device_id=device_type type forward x layers x TestMultipleWrapping FSDPTest skip_if_lt_x_gpu test_multiple_wrapping device This test simulates wrapping module after training run inference This required cases where later session model wrapped again FSDP contains nested FSDP wrappers within module inner_model = InnerModel device model = FSDP inner_model device_type type optim = SGD model parameters lr= _ range input = torch rand dtype=torch float device_type type input requires_grad = True output = model input output sum backward optim step optim zero_grad input = torch rand dtype=torch float device_type type output = model input second time rewrap inner model rewrapped_model = FSDP inner_model device_id=device rewrapped_model = FSDP inner_model device_type type rewrapped_output = rewrapped_model input assertEqual output rewrapped_output devices = cuda hpu xpu instantiate_device_type_tests TestMultipleWrapping globals only_for=devices allow_xpu=True __name__ == __main__ run_tests