Owner s module multiprocessing ruff noqa F contextlib copy gc os sys time unittest sys platform torch torch cuda torch multiprocessing mp torch utils hooks torch nn Parameter torch testing _internal common_cuda IS_JETSON torch testing _internal common_utils IS_MACOS IS_WINDOWS load_tests run_tests slowTest TEST_WITH_ASAN TEST_WITH_ROCM TEST_WITH_TSAN TestCase load_tests common_utils used automatically filter tests sharding sandcastle This line silences flake warnings load_tests = load_tests noqa PLW TEST_REPEATS = HAS_SHM_FILES = os path isdir dev shm MAX_WAITING_TIME_IN_SECONDS = TEST_CUDA_IPC = torch cuda is_available sys platform = darwin sys platform = win IS_JETSON TEST_WITH_ROCM https github com pytorch pytorch issues TEST_MULTIGPU = TEST_CUDA_IPC torch cuda device_count SubProcess mp Process __init__ tensor super __init__ tensor = tensor daemon = True run tensor add_ _test_cuda_ipc_deadlock_actor queue iterations _ range iterations queue empty queue get time sleep _test_cuda_ipc_deadlock_learner queue iterations net = torch nn LSTM cuda _ range iterations queue full queue put copy deepcopy net state_dict time sleep simple_fill queue event data = queue get data = event set simple_pool_fill tensor tensor fill_ tensor add send_tensor queue event device dtype t = torch ones device=device dtype=dtype queue put t queue put t event wait send_and_delete_tensors queue event device dtype count size= i range count t = torch full size i device=device dtype=dtype queue put t del t event wait send_tensor_with_untyped_storage queue event tensors = torch ones device= cuda chunk dim= specs = tensor tensors storage = tensor untyped_storage storage_device storage_handle storage_size_bytes storage_offset_bytes ref_counter_handle ref_counter_offset event_handle event_sync_required = storage _share_cuda_ specs append tensor_cls type tensor tensor_size tensor shape tensor_stride tensor stride tensor_offset tensor storage_offset dtype tensor dtype requires_grad tensor requires_grad storage_cls type storage storage_device storage_device storage_handle storage_handle storage_size_bytes storage_size_bytes storage_offset_bytes storage_offset_bytes ref_counter_handle ref_counter_handle ref_counter_offset ref_counter_offset event_handle event_handle event_sync_required event_sync_required queue put specs event wait receive_and_send_sum queue out_queue event device dtype count size= s = torch full size device=device dtype=dtype _ range count t = queue get s += t out_queue put s event wait receive_and_send queue out_queue event count _ range count t = queue get out_queue put t clone event wait sum_tensors inq outq torch cuda device tensors = inq get tensor tensors outq put tensor sum item tensor get_device tensor numel tensor storage size queue_get_exception inqueue outqueue os close hide expected error message try torch zeros cuda except Exception e outqueue put e outqueue put no exception Multiply two separate stream cuda_multiply_two queue ready done ready set torch cuda stream torch cuda Stream cuda_event tensor = queue get cuda_event wait tensor mul_ cuda_event record done set del cuda_event requires_grad_variable_sharing queue ready var = queue get ready set queue put var requires_grad integer_parameter_serialization iparam iparam + autograd_sharing queue ready master_modified device is_parameter var = queue get ready set master_modified wait expected_var = torch arange device=device view expected_var = is_ok = var data equal expected_var var data = torch ones device=device is_ok = var grad None is_ok = var _backward_hooks is_parameter is_ok = type var Parameter is_ok = type var torch Tensor var _grad = torch ones device=device queue put is_ok mixed_type_producer queue event _ range float_tensor = torch ones float cuda byte_tensor = torch zeros byte cuda queue put float_tensor queue put byte_tensor event wait event clear simple_autograd_function a= torch rand requires_grad_ True mean backward contextlib contextmanager fs_sharing prev_strategy = mp get_sharing_strategy mp set_sharing_strategy file_system try yield finally mp set_sharing_strategy prev_strategy leak_checker __init__ test_case checked_pids = os getpid test_case = test_case __enter__ next_fds = _get_next_fds __exit__ args torch cuda is_available torch cuda ipc_collect args None Check th available file-descriptor end test no more than higher than th available start This attempts catch file descriptor leaks allows one-off initialization may use up file descriptor TODO Disabled because check too flaky available_fds = _get_next_fds test_case assertLessEqual available_fds - - next_fds - test_case assertFalse has_shm_files False check_pid pid checked_pids append pid _get_next_fds n= dup uses lowest-numbered unused descriptor new descriptor fds = os dup i range n fd fds os close fd fds has_shm_files wait=True HAS_SHM_FILES False result = _has_shm_files result mp get_sharing_strategy = file_system wait result total_waiting_time = waiting_time = while total_waiting_time = MAX_WAITING_TIME_IN_SECONDS result time sleep waiting_time total_waiting_time += waiting_time result = _has_shm_files result _has_shm_files gc collect names = torch_ + str pid pid checked_pids filename os listdir dev shm name names filename startswith name True False unittest skipIf TEST_WITH_TSAN TSAN fork-safe since we re forking multi-threaded environment TestMultiprocessing TestCase tearDown This will keep tests isolated each-other torch cuda is_available torch cuda ipc_collect _test_sharing ctx=mp device= cpu dtype=torch float repeat= test_fill x = torch zeros device dtype q = ctx Queue e = ctx Event data = x x q put data p = ctx Process target=simple_fill args= q e p daemon = True lc check_pid p pid p start total_waiting_time = waiting_time = is_set = False Once child process done will set event notify parent accordingly while total_waiting_time = MAX_WAITING_TIME_IN_SECONDS is_set time sleep waiting_time total_waiting_time += waiting_time is_set = e is_set assertTrue is_set device = meta assertTrue data eq all assertTrue data eq all p join assertFalse p is_alive test_receive q = ctx Queue e = ctx Event p = ctx Process target=send_tensor args= q e device dtype p daemon = True lc check_pid p pid p start t = q get t = q get device == meta assertEqual t size t size assertTrue t eq all s = t storage s = t storage assertEqual type s type s assertEqual s data_ptr s data_ptr device == meta assertEqual s size s size assertEqual s s We need delete tensors allow producer child process collect them properly del t t Mark event done join process e set p join assertFalse p is_alive leak_checker lc _ range repeat test_fill test_receive _test_preserve_sharing ctx=mp repeat= do_test x = torch randn data = x storage x x x q = ctx Queue q put data new_data = q get timeout= assertEqual new_data data atol= rtol= storage_cdata = data _cdata assertEqual new_data _cdata storage_cdata t new_data assertEqual t storage _cdata storage_cdata leak_checker _ range repeat do_test _test_pool ctx=mp repeat= do_test p = ctx Pool proc p _pool lc check_pid proc pid buffers = torch zeros i range results = p map simple_pool_fill buffers assertEqual len results len buffers r results assertEqual r torch ones atol= rtol= b buffers assertEqual b torch ones atol= rtol= p close p join leak_checker lc _ range repeat do_test unittest skipIf platform == darwin file descriptor strategy supported macOS unittest skipIf TEST_WITH_ASAN seems hang ASAN see https github com pytorch pytorch issues test_fd_sharing _test_sharing repeat=TEST_REPEATS unittest skipIf platform == darwin file descriptor strategy supported macOS test_fd_preserve_sharing _test_preserve_sharing repeat=TEST_REPEATS unittest skipIf platform == darwin file descriptor strategy supported macOS test_fd_pool _test_pool repeat=TEST_REPEATS unittest skipIf TEST_WITH_ASAN seems hang ASAN see https github com pytorch pytorch issues test_fs_sharing fs_sharing The test works very slow MacOS see https github com pytorch pytorch pull so run only once there The delay waiting child process terminate join repeat = IS_MACOS TEST_REPEATS _test_sharing repeat=repeat test_fs_preserve_sharing fs_sharing _test_preserve_sharing repeat=TEST_REPEATS test_fs_pool fs_sharing _test_pool repeat=TEST_REPEATS unittest skipIf HAS_SHM_FILES don t how check shm files exist test_fs queue_put x = torch DoubleStorage q = mp Queue assertFalse lc has_shm_files q put x time sleep queue serializes asynchronously assertTrue lc has_shm_files wait=False q get fs_sharing leak_checker lc _ range TEST_REPEATS queue_put test_inherit_tensor t = torch zeros p = SubProcess t share_memory_ p start p join p exitcode None print test_inherit_tensor SubProcess too slow assertEqual t torch ones atol= rtol= unittest skipIf IS_WINDOWS Test needs use fork multiprocessing test_autograd_errors ctx = mp get_context fork simple_autograd_function Autograd only uses thread when GPUs involved torch cuda is_available torch backends mps is_available torch xpu is_available assertRaisesRegex RuntimeError r Unable handle autograd ctx Pool pool pool map simple_autograd_function ctx Pool pool pool map simple_autograd_function test_autograd_fine_with_spawn ctx = mp get_context spawn simple_autograd_function ctx Pool pool pool map simple_autograd_function unittest skipIf TEST_CUDA_IPC CUDA IPC available test_cuda_simple torch cuda FloatTensor initialize CUDA outside leak checker _test_sharing mp get_context spawn cuda torch float unittest skipIf TEST_CUDA_IPC CUDA IPC available test_cuda_memory_allocation ctx = mp get_context spawn q = ctx Queue e = ctx Event p = ctx Process target=send_and_delete_tensors args= q e cuda torch int p start t = _ range t append q get assertEqual t torch full dtype=torch int del t e set p join unittest skipIf TEST_CUDA_IPC CUDA IPC available test_cuda_ipc_deadlock ctx = mp get_context spawn queue = ctx Queue processes = dict a=ctx Process target=_test_cuda_ipc_deadlock_actor args= queue l=ctx Process target=_test_cuda_ipc_deadlock_learner args= queue p processes values p start p processes values p join p processes values assertFalse p is_alive slowTest unittest skipIf TEST_CUDA_IPC CUDA IPC available test_cuda_send_many name=None size= count= ctx = mp get_context spawn q = ctx Queue q = ctx Queue q = ctx Queue e = ctx Event e = ctx Event e = ctx Event p = ctx Process target=send_and_delete_tensors args= q e cuda torch long count size p = ctx Process target=receive_and_send args= q q e count p = ctx Process target=receive_and_send_sum args= q q e cuda torch long count size p start p start p start result = q get assertEqual result int count count - del result e set e set e set p join p join p join unittest skipIf TEST_CUDA_IPC CUDA IPC available unittest skipIf TEST_MULTIGPU found only GPU test_cuda_small_tensors Check multiple small tensors which will likely use same underlying cached allocation ctx = mp get_context spawn tensors = i range device = i tensors += torch arange i i + cuda device inq = ctx Queue outq = ctx Queue inq put tensors p = ctx Process target=sum_tensors args= inq outq p start results = _ range results append outq get p join i _tensor enumerate tensors v device tensor_size storage_size = results i assertEqual v torch arange i i + sum assertEqual device i assertEqual tensor_size You might think should case s After data CUDA caching allocator goes through IPC size storage size cached cudaMalloc entire memory block storage just storage See Note CUDA IPC caching allocator more info assertEqual storage_size Collect current process producer files make sure nothing holds ref sent tensors del _tensor del tensors We need collect CUDA MP implementation holds one shared memory file performance reason torch cuda ipc_collect unittest skipIf IS_WINDOWS applicable Windows only fails fork unittest skipIf torch cuda is_available CUDA available test_cuda_bad_call Initialize CUDA t = torch zeros cuda cpu inq = mp Queue outq = mp Queue p = mp Process target=queue_get_exception args= inq outq p start inq put t p join assertIsInstance outq get RuntimeError unittest skipIf IS_WINDOWS applicable Windows only fails fork unittest skipIf torch cuda is_available CUDA available test_wrong_cuda_fork stderr = TestCase runWithPytorchAPIUsageStderr \ torch torch multiprocessing Process run rank torch cuda set_device rank __name__ == __main__ size = processes = rank range size would work fine without line below x = torch rand cuda p = Process target=run args= rank p start processes append p p processes p join assertRegex stderr Cannot re-initialize CUDA forked subprocess unittest skipIf TEST_CUDA_IPC CUDA IPC available test_rebuild_cuda_tensor ctx = mp get_context spawn queue = ctx Queue event = ctx Event proc = ctx Process target=send_tensor_with_untyped_storage args= queue event proc start specs = queue get tensors = spec specs tensors append mp reductions rebuild_cuda_tensor spec assertEqual tensors del tensors spec event set unittest skipIf TEST_CUDA_IPC CUDA IPC available test_event ctx = mp get_context spawn queue = ctx Queue ready = ctx Event done = ctx Event p = ctx Process target=cuda_multiply_two args= queue ready done p start ready wait torch cuda stream torch cuda Stream tensor = torch cuda FloatTensor Use sleep kernel test events Without event multiply happens before add event = torch cuda Event interprocess=True torch cuda _sleep about ms tensor add_ event record queue put event tensor done wait must wait until subprocess records event event synchronize assertEqual list tensor p join staticmethod _test_event_multiprocess_child event p c c p c p put notify parent child ready p c get wait record parent event synchronize c p put notify parent synchronization done unittest skipIf TEST_CUDA_IPC CUDA IPC available test_event_multiprocess event = torch cuda Event enable_timing=False interprocess=True assertTrue event query ctx = mp get_context spawn p c = ctx SimpleQueue c p = ctx SimpleQueue p = ctx Process target=TestMultiprocessing _test_event_multiprocess_child args= event p c c p p start c p get wait until child process ready torch cuda _sleep spin about ms event record p c put notify child event recorded assertFalse event query c p get wait synchronization child assertTrue event query p join unittest skipIf TEST_CUDA_IPC CUDA IPC available unittest skipIf TEST_MULTIGPU found only GPU test_event_handle_multi_gpu d = torch device cuda d = torch device cuda torch cuda device d e = torch cuda Event enable_timing=False interprocess=True torch cuda device d create handle different device un-recorded event e ipc_handle torch cuda device d e = torch cuda Event enable_timing=False interprocess=True stream = torch cuda Stream torch cuda _sleep spin about ms e record stream torch cuda device d create handle different device recorded event e ipc_handle staticmethod _test_event_handle_importer_consumer handle p c c p e = torch cuda Event from_ipc_handle handle c p put notify parent child ready p c get wait record parent e synchronize c p put notify synchronization done child p c get wait parent finish before destructing child event unittest skipIf TEST_CUDA_IPC CUDA IPC available test_event_handle_importer e = torch cuda Event enable_timing=False interprocess=True assertTrue e query ctx = mp get_context spawn p c = ctx SimpleQueue c p = ctx SimpleQueue p = ctx Process target=TestMultiprocessing _test_event_handle_importer_consumer args= e ipc_handle p c c p p start c p get wait child become ready torch cuda _sleep spin about ms e record p c put notify child event recorded assertFalse e query c p get wait synchronization child assertTrue e query p c put notify child parent done p join staticmethod _test_event_handle_exporter_consumer handle p c c p stream = torch cuda Stream torch cuda stream stream e = torch cuda Event from_ipc_handle torch cuda current_device handle torch cuda _sleep spin about ms e record c p put wait parent process finished synchronization before destructing e p c get unittest skipIf TEST_CUDA_IPC CUDA IPC available test_event_handle_exporter e = torch cuda Event enable_timing=False interprocess=True ctx = mp get_context spawn p c = ctx SimpleQueue c p = ctx SimpleQueue p = ctx Process target=TestMultiprocessing _test_event_handle_exporter_consumer args= e ipc_handle p c c p p start wait event child process recorded c p get assertFalse e query e synchronize assertTrue e query p c put p join _test_empty_tensor_sharing dtype device q = mp Queue empty = torch tensor dtype=dtype device=device q put empty out = q get timeout= assertEqual out empty test_empty_tensor_sharing _test_empty_tensor_sharing torch float torch device cpu _test_empty_tensor_sharing torch int torch device cpu unittest skipIf torch cuda is_available CUDA available test_empty_tensor_sharing_cuda _test_empty_tensor_sharing torch float torch device cuda _test_empty_tensor_sharing torch int torch device cuda test_empty_tensor_sharing_meta _test_empty_tensor_sharing torch float torch device meta _test_empty_tensor_sharing torch int torch device meta test_tensor_sharing_meta dtype = torch float device = torch device meta q = mp Queue empty = torch tensor dtype=dtype device=device q put empty out = q get timeout= assertEqual out empty test_meta_simple _test_sharing mp get_context spawn meta torch float _test_autograd_sharing var ctx=mp is_parameter=False device = cuda var is_cuda cpu ready = ctx Event master_modified = ctx Event queue = ctx Queue p = ctx Process target=autograd_sharing args= queue ready master_modified device is_parameter p daemon = True p start This would cause error we tried serialize hooks because s closure pickle doesn t support closures torch utils hooks unserializable_hook hook unused pass var requires_grad var register_hook hook var _grad = torch zeros device=device queue put var ready wait var data = var grad data = torch ones device=device master_modified set worker_ok = queue get assertTrue worker_ok assertEqual var data torch ones device=device assertEqual var grad data torch ones device=device p join assertFalse p is_alive Check sharing cudaMalloc allocation different types storage Issue _test_mixed_types_cuda_sharing ctx=mp all_ones = torch ones float all_zeros = torch zeros byte queue = ctx Queue event = ctx Event p = ctx Process target=mixed_type_producer args= queue event p start _ range float_tensor = queue get byte_tensor = queue get assertEqual float_tensor all_ones assertEqual byte_tensor all_zeros del float_tensor byte_tensor event set time sleep p join unittest skipIf TEST_WITH_ASAN non-deterministically hangs ASAN https github com pytorch pytorch issues test_variable_sharing requires_grad True False var = torch arange view requires_grad_ requires_grad _test_autograd_sharing var See https github com pytorch pytorch issues unittest skipIf TEST_WITH_ASAN non-deterministically hangs ASAN test_leaf_variable_sharing devices = cpu torch cuda is_available TEST_CUDA_IPC devices append cuda device devices requires_grad True False var = torch arange device=device view requires_grad_ requires_grad assertTrue var is_leaf ctx = mp get_context spawn device == cuda mp ready = ctx Event queue = ctx Queue p = ctx Process target=requires_grad_variable_sharing args= queue ready p daemon = True p start queue put var ready wait worker_requires_grad = queue get assertTrue worker_requires_grad == requires_grad test_non_leaf_variable_sharing devices = cpu torch cuda is_available cpu cuda device devices var = torch arange device=device view requires_grad_ True var = var Don t use regular Queue uses background thread which means we can t catch exceptions queue = mp SimpleQueue assertRaisesRegex RuntimeError r requires_grad lambda queue put var unittest skipIf TEST_CUDA_IPC CUDA IPC available test_cuda_variable_sharing requires_grad True False var = torch arange device= cuda view requires_grad_ requires_grad _test_autograd_sharing var mp get_context spawn unittest skipIf TEST_CUDA_IPC CUDA IPC available test_mixed_types_cuda_sharing _test_mixed_types_cuda_sharing mp get_context spawn test_parameter_sharing param = Parameter torch arange view _test_autograd_sharing param is_parameter=True unittest skipIf TEST_CUDA_IPC CUDA IPC available test_cuda_parameter_sharing param = Parameter torch arange device= cuda view _test_autograd_sharing param mp get_context spawn is_parameter=True test_integer_parameter_serialization_cpu _test_integer_parameter_serialization device= cpu unittest skipIf TEST_CUDA_IPC CUDA IPC available test_integer_parameter_serialization_cuda _test_integer_parameter_serialization device= cuda _test_integer_parameter_serialization device param = torch nn Parameter torch tensor dtype=torch int device=device requires_grad=False ctx = mp get_context spawn p = ctx Process target=integer_parameter_serialization args= param p start p join assertEqual p exitcode msg=f Failed serialize successfully device device test_empty_shared t = torch tensor t share_memory_ _test_is_shared t = torch randn assertFalse t is_shared t share_memory_ assertTrue t is_shared unittest skipIf platform == darwin file descriptor strategy supported macOS test_is_shared _test_is_shared test_fs_is_shared fs_sharing _test_is_shared unittest skipIf torch cuda is_available CUDA available test_is_shared_cuda t = torch randn cuda assertTrue t is_shared unittest skipIf sys platform = linux Only runs Linux requires prctl test_set_thread_name name = test name mp _set_thread_name name assertEqual mp _get_thread_name name __name__ == __main__ run_tests