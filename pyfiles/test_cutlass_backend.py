Owner s module inductor itertools logging math os re sysconfig time unittest unittest mock mock collections abc Callable enum Enum pathlib Path typing Optional torch _dynamo exc BackendCompilerFailed torch _inductor codegen cuda serialization get_cutlass_operation_serializer torch _inductor utils clear_caches torch export Dim torch testing _internal logging_utils log_settings torch utils _pytree pytree try test_aot_inductor_utils AOTIRunnerUtil except ImportError test_aot_inductor_utils AOTIRunnerUtil torch torch _inductor codecache torch version torch _dynamo config dynamo_config torch _dynamo utils counters torch _inductor config torch _inductor codegen cuda cuda_kernel CUDATemplateCaller torch _inductor codegen cuda cutlass_utils _gen_ops_cached get_max_alignment torch _inductor exc InductorError torch _inductor ir FixedLayout torch _inductor select_algorithm NoValidChoicesError torch _inductor test_case run_tests TestCase torch _inductor utils fresh_cache torch sparse SparseSemiStructuredTensor to_sparse_semi_structured torch testing FileCheck torch testing _internal common_cuda PLATFORM_SUPPORTS_FP SM OrLater SM OrLater torch testing _internal common_utils IN_RE_WORKER instantiate_parametrized_tests IS_FBCODE parametrize torch testing _internal inductor_utils _quantize_rowwise _quantize_tensorwise HAS_CPU HAS_CUDA_AND_TRITON torch set_float _matmul_precision high HAS_CUDA_AND_TRITON torch cuda memory _set_allocator_settings expandable_segments False log = logging getLogger __name__ _get_path_without_sccache - str Get PATH environment variable without sccache path_envs = os environ get PATH split path_envs = env env path_envs opt cache bin env join path_envs _check_if_instances_equal op op - bool Utility function check two instances equal cutlass uses list tuple inconsistently isinstance op list &#124; tuple tuple op == tuple op type op type op False some classes have __eq__ defined they may insufficient op __class__ __dict__ get __eq__ op = op False isinstance op Enum op value == op value hasattr op __dict__ key value op __dict__ items key op __dict__ False _check_if_instances_equal value op __dict__ key False True un_ops_under_test = torch relu torch tanh torch exp torch sigmoid bin_ops_under_test = torch add torch mul torch sub torch div evt_all_ops = parametrize op un_ops_under_test + bin_ops_under_test name_fn=lambda f f __name__ evt_un_ops = parametrize op un_ops_under_test name_fn=lambda f f __name__ evt_bin_ops = parametrize op bin_ops_under_test name_fn=lambda f f __name__ evt_all_shapes = parametrize shape itertools product repeat= gen_args op shape dtype=torch float op bin_ops_under_test torch rand shape device= cuda dtype=dtype use_evt_config = config patch max_autotune True max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs benchmark_epilogue_fusion False EVT doesn t support benchmark fusion yet cuda cutlass_tma_only True cuda cutlass_epilogue_fusion_enabled True fp _config = config patch max_autotune True max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs benchmark_epilogue_fusion False EVT doesn t support benchmark fusion yet cuda cutlass_tma_only True select_no_algorithm args kwargs Utility function skip precompilation autotuning raise NoValidChoicesError instantiate_parametrized_tests TestCutlassBackend TestCase setUp HAS_CUDA_AND_TRITON skipTest CUDA triton available torch version hip skipTest CUTLASS backend supported HIP The new inductor cache refresh mechanism introduced https github com pytorch pytorch pull interacts badly persistent subprocesses during autotuning So we need disable automatic cache refresh before calling setUp parent old_disable_fresh_cache_envvar = os environ get INDUCTOR_TEST_DISABLE_FRESH_CACHE try os environ INDUCTOR_TEST_DISABLE_FRESH_CACHE = super setUp finally os environ INDUCTOR_TEST_DISABLE_FRESH_CACHE = old_disable_fresh_cache_envvar torch random manual_seed tearDown super tearDown clear_caches run_evt_test model op shape num_fusions= M N = shape = torch ones M N cuda half b = torch ones N N cuda half t extra_args = gen_args op M N model = model cuda result = torch compile model b extra_args ref_result = model b extra_args assertEqual torch _dynamo utils counters inductor cuda_epilogue_fusion_counter num_fusions torch testing assert_close result ref_result test_check_paths cutlass_mock_imports_path = os path join os path dirname torch __file__ _inductor codegen cuda cutlass_lib_extensions cutlass_mock_imports cutlass_mock_cuda_path = os path join cutlass_mock_imports_path cuda cutlass_mock_pydot_path = os path join cutlass_mock_imports_path pydot cutlass_mock_scipy_path = os path join cutlass_mock_imports_path scipy assertTrue os path exists cutlass_mock_imports_path assertTrue os path exists cutlass_mock_cuda_path assertTrue os path exists cutlass_mock_pydot_path assertTrue os path exists cutlass_mock_scipy_path unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_max_autotune_cutlass_threshold Make sure Cutlass GEMM threshold works intended mm b b = torch randn cuda half b = torch randn cuda half t config patch max_autotune True max_autotune_gemm_backends CUTLASS compile_threads cuda cutlass_backend_min_gemm_size cuda cutlass_max_profiling_configs mock patch torch _inductor kernel mm autotune_select_algorithm wraps=select_no_algorithm sa assertRaisesRegex InductorError r NoValidChoicesError _ = torch compile mm dynamic=False b args _ = sa call_args _ choices _ __ = args assertEqual choices mock patch dict os environ PATH _get_path_without_sccache test_import_cutlass torch _inductor codegen cuda cutlass_utils try_import_cutlass assertTrue try_import_cutlass cutlass_cppgen type ignore import-not-found noqa F cutlass_library noqa F test_cutlass_key torch _inductor codegen cuda cutlass_utils try_import_cutlass assertTrue try_import_cutlass torch _inductor codecache cutlass_key assertIsNotNone cutlass_key unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_cutlass_backend_subproc_mm Test autotune_in_subproc works mm NOTE Shape like M N K = would get filtered out due alignment mismatch M N K = = torch randn M K cuda half b = torch randn N K cuda half t config patch max_autotune True autotune_in_subproc True max_autotune_gemm_backends CUTLASS compile_threads cuda cutlass_max_profiling_configs Y_compiled = torch compile torch mm b Y = torch mm b torch testing assert_close Y_compiled Y unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache parametrize dtype torch float torch bfloat test_cutlass_backend_subproc_addmm dtype Test autotune_in_subproc works addmm M N K = dtype = torch float = torch randn M K dtype=dtype cuda b = torch randn N K dtype=dtype cuda t x_shapes = M N M N N alpha = beta = config patch max_autotune True autotune_in_subproc True max_autotune_gemm_backends CUTLASS compile_threads cuda cutlass_max_profiling_configs x_shape x_shapes torch _dynamo reset clear_caches x = torch randn x_shape cuda dtype Y_compiled = torch compile torch addmm x b alpha=alpha beta=beta Y = torch addmm x b alpha=alpha beta=beta torch testing assert_close Y_compiled Y unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_cutlass_backend_subproc_bmm Test autotune_in_subproc works bmm B M N K = = torch randn B M K cuda half b = torch randn B N K cuda half permute config patch max_autotune True autotune_in_subproc True max_autotune_gemm_backends CUTLASS compile_threads cuda cutlass_max_profiling_configs Y_compiled = torch compile torch bmm b Y = torch bmm b torch testing assert_close Y_compiled Y unittest skipIf SM OrLater need sm_ parametrize dynamic False True mock patch dict os environ PATH _get_path_without_sccache test_diff_matmul_share_same_kernel dynamic max_autotune_gemm_backends = CUTLASS MyModel torch nn Module __init__ super __init__ forward b c ab = b ac = c ab ac model = MyModel = torch randn cuda half b = torch randn cuda half t c = torch randn cuda half t config patch max_autotune True autotune_in_subproc True max_autotune_gemm_backends max_autotune_gemm_backends cuda cutlass_max_profiling_configs torch _inductor utils run_and_get_code compiled = torch compile model dynamic=dynamic expected = model b c actual codes = run_and_get_code compiled b c torch testing assert_close actual expected pattern = r cutlass_ \w +\ cutlass_ \w + match = re search pattern codes assertTrue match None cutlass_kernel = match group FileCheck check_count cutlass_kernel run codes unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_number_mm_precompiles torch _dynamo utils counters clear max_autotune_gemm_backends = CUTLASS MyModel torch nn Module __init__ super __init__ forward b c ab = b ab model = MyModel = torch randn cuda half b = torch randn cuda half t c = torch randn cuda half t config patch max_autotune True autotune_in_subproc True max_autotune_gemm_backends max_autotune_gemm_backends cuda cutlass_max_profiling_configs cuda cutlass_max_profiling_swizzle_options guarantees choices fx_graph_cache False fx_graph_remote_cache False autotune_local_cache False torch _inductor utils run_and_get_code compiled = torch compile model dynamic=True expected = model b c actual codes = run_and_get_code compiled b c torch testing assert_close actual expected assertTrue re search r cutlass_ cutlass_ codes Verifies expected number precompilations assertEqual torch _dynamo utils counters inductor select_algorithm_num_precompiles NOTE right now tuned_mm doesn t support cutlass x which used A unittest skipIf SM OrLater need sm_ parametrize dynamic False True parametrize use_aoti False True parametrize dtype torch float torch bfloat mock patch dict os environ PATH _get_path_without_sccache test_max_autotune_cutlass_backend_regular_mm dynamic bool max_autotune_gemm_backends str = CUTLASS use_aoti bool = False dtype torch dtype = torch float Main test mm M N K shapes = M N K shapes = shapes dynamic shapes MyModel torch nn Module forward b b model = MyModel cuda inputs = torch randn M K cuda dtype torch randn K N cuda dtype M N K shapes dynamic_shapes = Dim DYNAMIC Dim DYNAMIC b Dim DYNAMIC Dim DYNAMIC dynamic None config patch max_autotune True max_autotune_gemm_backends max_autotune_gemm_backends cuda cutlass_max_profiling_configs dynamo_config patch error_on_recompile dynamic expected = model input input inputs use_aoti actual = AOTIRunnerUtil run_multiple model inputs dynamic_shapes=dynamic_shapes compiled_model = torch compile model dynamic=True actual = compiled_model input input inputs torch testing assert_close actual expected unittest skipIf SM OrLater need sm_ parametrize dynamic False True parametrize use_aoti False True parametrize dtype torch float _e m fn mock patch dict os environ PATH _get_path_without_sccache test_max_autotune_cutlass_backend_fp _scaled_mm dynamic bool max_autotune_gemm_backends str = CUTLASS use_aoti bool = False dtype torch dtype = torch float Main test mm M N K shapes = M N K shapes = shapes dynamic shapes inputs = shape shapes M N K = shape output_dtype = torch bfloat device = cuda x = torch randn M K dtype=output_dtype device=device w = torch randn N K dtype=output_dtype device=device quantize weight prior inference w_fp w_inverse_scale = _quantize_rowwise w dtype w_t_fp = w_fp t w_inverse_scale = w_inverse_scale t scale_b should N quantize input x x_fp x_inverse_scale = _quantize_rowwise x dtype inputs append x_fp x_inverse_scale w_t_fp w_inverse_scale MyModel torch nn Module forward x_fp x_inverse_scale w_t_fp w_inverse_scale y = torch _scaled_mm x_fp w_t_fp x_inverse_scale w_inverse_scale None out_dtype=torch bfloat use_fast_accum=False y dynamic_shapes = x_fp Dim DYNAMIC Dim DYNAMIC x_inverse_scale Dim DYNAMIC w_t_fp Dim DYNAMIC Dim DYNAMIC w_inverse_scale Dim DYNAMIC dynamic None model = MyModel cuda config patch max_autotune True max_autotune_gemm_backends max_autotune_gemm_backends cuda cutlass_max_profiling_configs benchmark_epilogue_fusion False EVT doesn t support benchmark fusion yet cuda cutlass_tma_only True dynamo_config patch error_on_recompile dynamic expected = model input input inputs use_aoti actual = AOTIRunnerUtil run_multiple model inputs dynamic_shapes=dynamic_shapes compiled_model = torch compile model dynamic=True actual = compiled_model input input inputs torch testing assert_close actual expected rtol= e- atol= unittest skipIf SM OrLater need sm_ parametrize dynamic False True parametrize use_aoti False True parametrize dtype torch float torch bfloat mock patch dict os environ PATH _get_path_without_sccache test_max_autotune_cutlass_backend_addmm dynamic bool max_autotune_gemm_backends str = CUTLASS use_aoti bool = False dtype torch dtype = torch float Main test addmm MyModel torch nn Module forward x b torch addmm x b model = MyModel cuda M N K shapes = shapes = shapes dynamic shapes x_shapes = lambda M N M N lambda M N M lambda M N N lambda M N N x_shape x_shapes torch _dynamo reset clear_caches inputs = torch randn x_shape M N cuda dtype torch randn M K cuda dtype torch randn N K cuda dtype t M N K shapes dynamic_shapes = x i v i v enumerate x_shape Dim DYNAMIC Dim DYNAMIC v = Dim DYNAMIC Dim DYNAMIC b Dim DYNAMIC Dim DYNAMIC dynamic None config patch max_autotune True max_autotune_gemm_backends max_autotune_gemm_backends cuda cutlass_max_profiling_configs dynamo_config patch error_on_recompile dynamic expected = model input input inputs use_aoti actual = AOTIRunnerUtil run_multiple model inputs dynamic_shapes=dynamic_shapes compiled_model = torch compile model dynamic=dynamic actual = compiled_model input input inputs torch testing assert_close actual expected unittest skipIf SM OrLater need sm_ parametrize dynamic False True parametrize use_aoti False True parametrize dtype torch float torch bfloat parametrize use_expand False True mock patch dict os environ PATH _get_path_without_sccache test_max_autotune_cutlass_backend_bmm dynamic bool use_aoti bool = False max_autotune_gemm_backends str = CUTLASS dtype torch dtype = torch float use_expand bool = False Main test bmm MyModel torch nn Module forward b torch bmm b model = MyModel cuda B M N K shapes = shapes = shapes dynamic shapes inputs = B M N K shapes use_expand Create A using unsqueeze expand A = torch randn M K cuda dtype unsqueeze expand B - - Original method A = torch randn B M K cuda dtype B_tensor = torch randn B N K cuda dtype permute inputs append A B_tensor dynamic_shapes = Dim DYNAMIC Dim DYNAMIC Dim DYNAMIC b Dim DYNAMIC Dim DYNAMIC Dim DYNAMIC dynamic None config patch max_autotune True max_autotune_gemm_backends max_autotune_gemm_backends cuda cutlass_max_profiling_configs expected = model input input inputs use_aoti actual = AOTIRunnerUtil run_multiple model inputs dynamic_shapes=dynamic_shapes compiled_model = torch compile model dynamic=dynamic actual = compiled_model input input inputs torch testing assert_close actual expected unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_max_autotune_cutlass_backend_regular_mm_streamk dynamic bool = False max_autotune_gemm_backends str = CUTLASS Make sure autotuning mm sub processes work without crashes compiled_model = torch compile torch mm dynamic=dynamic config patch max_autotune True autotune_in_subproc True max_autotune_gemm_backends max_autotune_gemm_backends cuda cutlass_max_profiling_configs cuda cutlass_op_allowlist_regex stream_k only stream-k GEMM Kernels M K N = torch randn M K cuda half b = torch randn N K cuda half t Y_compiled = compiled_model b Y = torch mm b we need relaxed numerical limits due sheer size matmuls involved Many small addition differences add up torch testing assert_close Y_compiled Y atol= rtol= unittest skipIf SM OrLater need sm_ test_streamk_with_dynamic Test streamk dynamic=True Streamk should filtered out Problem streamk can have different workspace depending shape Without correct workspace kernel will fail runtime = torch randn cuda half b = torch randn cuda half t config patch max_autotune True max_autotune_gemm_backends CUTLASS cuda cutlass_op_allowlist_regex stream_k only stream-k GEMM Kernels assertRaisesRegex InductorError r NoValidChoicesError _ = torch compile torch mm dynamic=True b unittest skipIf SM OrLater need sm_ test_streamk_with_static Test streamk dynamic=False Streamk should work shapes = compiled_model = torch compile torch mm dynamic=False shape shapes M N K = shape = torch randn M K cuda half b = torch randn N K cuda half t config patch max_autotune True max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs cuda cutlass_op_allowlist_regex stream_k only stream-k GEMM Kernels _ = compiled_model b _test_max_autotune_cutlass_backend_epilogue_fusion dynamic bool = False max_autotune_gemm_backends str = CUTLASS fp =True expected_fuse_count= mm Optional Callable torch Tensor torch Tensor torch Tensor = None batch_size Optional int = None Note The ops available also depend alignment shapes so these shapes don t all align least elements can happen no Cutlass x op available allows fusions batch_size None = torch randn cuda b = torch randn cuda t = torch randn batch_size cuda b = torch randn batch_size cuda permute fp = half b = b half config patch max_autotune True autotune_in_subproc True max_autotune_gemm_backends max_autotune_gemm_backends cuda cutlass_max_profiling_configs cuda version required enable Kernels we need counters inductor cuda_epilogue_fusion_counter = assert mm None Y_compiled = torch compile mm dynamic=dynamic b Y = mm b actual_count = counters inductor cuda_epilogue_fusion_counter assert actual_count == expected_fuse_count f Expected fuse count expected_fuse_count got actual_count torch testing assert_close Y_compiled Y atol= e- rtol= e- unittest skipIf SM OrLater need sm_ test_max_autotune_cutlass_backend_simple_fusion_fp _fp acc mm b b _test_max_autotune_cutlass_backend_epilogue_fusion fp =True expected_fuse_count= mm=mm unittest skipIf SM OrLater need sm_ test_max_autotune_cutlass_backend_chained_fusion_fp _fp acc mm b b - _test_max_autotune_cutlass_backend_epilogue_fusion fp =True expected_fuse_count= mm=mm unittest skipIf SM OrLater need sm_ test_max_autotune_cutlass_backend_relu_fusion_fp _fp acc mm b torch nn functional relu b - The pointwise ops seem pre-fused into single Pointwise _test_max_autotune_cutlass_backend_epilogue_fusion fp =True expected_fuse_count= mm=mm unittest skipIf SM OrLater need sm_ test_max_autotune_cutlass_backend_relu _fusion_fp _fp acc mm b torch clamp torch nn functional relu b max= The pointwise ops seem pre-fused into single Pointwise _test_max_autotune_cutlass_backend_epilogue_fusion fp =True expected_fuse_count= mm=mm unittest skipIf SM OrLater need sm_ test_max_autotune_cutlass_backend_no_fusion_dtype_mismatch mm b should fused since output dtype different matmul dtype b torch float _test_max_autotune_cutlass_backend_epilogue_fusion fp =True expected_fuse_count= mm=mm unittest skipIf SM OrLater need sm_ test_max_autotune_cutlass_backend_shape_dependent_normalization_fusion mm b b b size _test_max_autotune_cutlass_backend_epilogue_fusion fp =True expected_fuse_count= mm=mm TODO Enable dynamic test cases when dynamic support added unittest skipIf SM OrLater need sm_ parametrize dynamic False mock patch dict os environ PATH _get_path_without_sccache test_max_autotune_cutlass_backend_int_mm dynamic bool max_autotune_gemm_backends str = CUTLASS Make sure autotuning mm sub processes work without crashes mm b torch _int_mm b CUTLASS only supports row-major column-major combination layouts operation thus transpose tensor b other side Triton moment doesn t support combination so s excluded test Also CUTLASS alignment requirements number columns both tensors has divisible = torch randint dtype=torch int cuda b = torch randint dtype=torch int cuda T config patch max_autotune True autotune_in_subproc True max_autotune_gemm_backends max_autotune_gemm_backends cuda cutlass_max_profiling_configs Y_compiled = torch compile mm dynamic=dynamic b Y = mm b torch testing assert_close Y_compiled Y mock patch dict os environ PATH _get_path_without_sccache unittest skipIf SM OrLater need sm_ test_force_cutlass_backend_aoti_dynamic MyModel torch nn Module forward x w x w config patch max_autotune True autotune_in_subproc False max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs model = MyModel M N K = dynamic_shapes = x M K w K N x = torch randn M K cuda half w = torch randn N K cuda half t actual = AOTIRunnerUtil run model x w dynamic_shapes=dynamic_shapes expected = model x w torch testing assert_close expected actual mock patch dict os environ PATH _get_path_without_sccache unittest skipIf SM OrLater need sm_ test_force_cutlass_backend_aoti_cexpr_codegen MyModel torch nn Module forward x w x x = x shape x = x reshape x x x = x contiguous x = x as_strided x size x stride x w config patch max_autotune True autotune_in_subproc False max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs model = MyModel M N K = dynamic_shapes = x Dim DYNAMIC w None x = torch randn M K cuda half w = torch randn N K cuda half t actual = AOTIRunnerUtil run model x w dynamic_shapes=dynamic_shapes expected = model x w torch testing assert_close expected actual mock patch dict os environ PATH _get_path_without_sccache unittest skipIf SM OrLater need sm_ test_aoti_workspace_ptr MyModel torch nn Module forward x w x w config patch max_autotune True autotune_in_subproc False max_autotune_gemm_backends CUTLASS cuda cutlass_op_allowlist_regex x x stream_k_warpspecialized_cooperative_epi_nosmem cuda cutlass_max_profiling_configs model = MyModel M N K = _ x = torch randn M K cuda half w = torch randn N K cuda half t actual = AOTIRunnerUtil run model x w expected = model x w torch testing assert_close expected actual atol= rtol= TODO Enable dynamic test cases when dynamic support added unittest skipIf SM OrLater SM OrLater need sm_ x exactly parametrize dynamic False mock patch dict os environ PATH _get_path_without_sccache test_max_autotune_cutlass_backend_sparse_semi_structured_mm dynamic bool Make sure autotuning mm sub processes work without crashes SparseSemiStructuredTensor _FORCE_CUTLASS = True mm b torch mm b m n k = mask = torch tensor tile m k cuda half = torch rand m k cuda half mask a_sparse = to_sparse_semi_structured b = torch rand k n cuda half config patch max_autotune True autotune_in_subproc True max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs autotune_local_cache True Y_compiled = torch compile mm dynamic=dynamic a_sparse b Y = mm b torch testing assert_close Y_compiled Y cache = torch _inductor codecache LocalCache lookup sparse_semi_structured_mm assert cache None high = cache f cuda torch float m k k f cuda torch int m k k f cuda torch float k n n high cutlass_kernels_count = kernel duration high items kernel startswith cutlass_gemm math isinf duration cutlass_kernels_count += assert cutlass_kernels_count unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_cutlass_backend_op_denylist my_addmm x b alpha beta torch addmm x b alpha=beta beta=alpha x = torch randn cuda half = torch randn cuda half b = torch randn cuda half t fresh_cache config patch max_autotune True max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs cuda cutlass_op_allowlist_regex cuda cutlass_op_denylist_regex pingpong mock patch torch _inductor kernel mm autotune_select_algorithm wraps=select_no_algorithm sa assertRaisesRegex InductorError r NoValidChoicesError torch compile my_addmm dynamic=False x b args _ = sa call_args op_name choices _ __ = args assert op_name == addmm cuda_template_count = choice choices isinstance choice CUDATemplateCaller choice_info = choice info_dict op_conf_name = choice_info get op_conf_name assert isinstance op_conf_name str assert pingpong op_conf_name All pingpong Kernels should have been filtered cuda_template_count += assert cuda_template_count No CUDATemplateCaller choices unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_cutlass_backend_op_allowlist addmm x b alpha beta torch addmm x b alpha=alpha beta=beta x = torch randn cuda half = torch randn cuda half b = torch randn cuda half t fresh_cache config patch max_autotune True max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs cuda cutlass_op_allowlist_regex pingpong cuda cutlass_op_denylist_regex None mock patch torch _inductor kernel mm autotune_select_algorithm wraps=select_no_algorithm sa assertRaisesRegex InductorError r NoValidChoicesError torch compile addmm dynamic=False x b args _ = sa call_args op_name choices _ __ = args assert op_name == addmm cuda_template_count = choice choices isinstance choice CUDATemplateCaller choice_info = choice info_dict op_conf_name = choice_info get op_conf_name assert isinstance op_conf_name str assert pingpong op_conf_name Only pingpong Kernels should have been allowed cuda_template_count += assert cuda_template_count No CUDATemplateCaller choices unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_cutlass_backend_fp _scaled_mm_fast_accum_filtering float _dtype = torch float _e m fn Only bf output type supported row-wise scaling fp output_dtype torch dtype = torch bfloat device = cuda M K N = Matmul Y = X M K x W N K x = torch randn M K dtype=output_dtype device=device w = torch randn N K dtype=output_dtype device=device bias = None quantize weight prior inference w_fp w_inverse_scale = _quantize_rowwise w float _dtype w_t_fp = w_fp t w_inverse_scale = w_inverse_scale t scale_b should N quantize input x x_fp x_inverse_scale = _quantize_rowwise x float _dtype linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias use_fast_accum y = torch _scaled_mm x_fp w_t_fp x_inverse_scale w_inverse_scale bias out_dtype=output_dtype use_fast_accum=use_fast_accum y linear_compiled = torch compile linear backend= inductor run_test use_fast_accum fresh_cache config patch max_autotune True max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs mock patch torch _inductor kernel mm autotune_select_algorithm wraps=select_no_algorithm sa assertRaisesRegex InductorError r NoValidChoicesError linear_compiled x_fp x_inverse_scale w_t_fp w_inverse_scale bias use_fast_accum args _ = sa call_args _ choices _ _ = args cuda_template_count = choice choices isinstance choice CUDATemplateCaller choice_info = choice info_dict op_conf_name = choice_info get op_conf_name assert isinstance op_conf_name str use_fast_accum assert fastaccum op_conf_name Only fastaccum Kernels should have been allowed assert fastaccum op_conf_name fastaccum Kernels should have been filtered cuda_template_count += assert cuda_template_count No CUDATemplateCaller choices run_test True run_test False unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_cutlass_backend_shape_coverage_mm Checks cutlass backend produces some ops variety shapes This test doesn t compile check correctness ops NOTE K has even inputs = torch randn cuda half torch randn cuda half torch randn cuda half torch randn cuda half torch randn cuda half torch randn cuda half torch randn cuda half torch randn cuda half torch randn cuda half torch randn cuda half fresh_cache config patch max_autotune True max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs mock patch torch _inductor kernel mm autotune_select_algorithm wraps=select_no_algorithm sa input inputs A B = input M K = A shape _ N = B shape assertRaisesRegex InductorError r NoValidChoicesError torch compile torch mm dynamic=False input assertTrue sa called f autotune_select_algorithm called shape M= M N= N K= K args _ = sa call_args op_name choices _ __ = args assert op_name == mm cuda_template_count = choice choices isinstance choice CUDATemplateCaller choice_info = choice info_dict op_conf_name = choice_info get op_conf_name assert isinstance op_conf_name str cuda_template_count += assertGreater cuda_template_count No CUDATemplateCaller choices found matmul shape f M= M N= N K= K unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_get_max_alignment l = FixedLayout torch device cpu torch half size= stride= m = get_max_alignment l assertEqual m Wrong max alignment Should have been simple contiguous case l _ = FixedLayout torch device cpu torch half size= stride= m _ = get_max_alignment l _ assertEqual m _ Wrong max alignment Should have been Did deal strides correctly l = FixedLayout torch device cpu torch half size= stride= m = get_max_alignment l assertEqual m Wrong max alignment Should have been Did take stride into account correctly l = FixedLayout torch device cpu torch half size= stride= offset= m = get_max_alignment l assertEqual m Wrong max alignment Should have been due choice offset l = FixedLayout torch device cpu torch half size= stride= offset= m = get_max_alignment l assertEqual m Wrong max alignment Should have been l = FixedLayout torch device cpu torch float size= stride= offset= m = get_max_alignment l assertEqual m Wrong max alignment Should have been due float dtype unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_standalone_runner max_autotune_gemm_backends = CUTLASS = torch randn cuda half b = torch randn cuda half t config patch max_autotune True max_autotune_gemm_backends max_autotune_gemm_backends cuda cutlass_max_profiling_configs cuda generate_test_runner True put standalone runner generated code tempfile NamedTemporaryFile torch _inductor codegen cuda cutlass_utils cuda_standalone_runner_compile_command CUDACompileSourceCapturingContext Run compilation check results just case save CUTLASS-based generated code CUDACompileSourceCapturingContext ctx compiled = torch compile torch mm dynamic=False expected = torch mm b actual = compiled b torch testing assert_close actual expected sources = ctx sources assert len sources = Get names temporary source executable files cu_file = NamedTemporaryFile w suffix= cu delete=False cu_file close exe_file = NamedTemporaryFile w suffix= delete=False exe_file close Save generated code into cu file open cu_file name w file file write sources Get command compile cu file run compilation command = cuda_standalone_runner_compile_command Path cu_file name Path exe_file name IS_FBCODE hack bypass following error error while loading shared libraries IX invalid mode dlopen Invalid argument platform_path = sysconfig get_config_var LIBDIR cuda_path = os path realpath os path join platform_path libcuda so command = command replace -lcuda f -L cuda_path repro_message = f Reproduce command \n f exe_file name exe_file name \n f cu_file name cu_file name \n retcode = os system command assertEqual retcode repro_message Run executable generated IS_FBCODE IN_RE_WORKER retcode = os system exe_file name assertEqual retcode repro_message Remove temporary files os remove cu_file name os remove exe_file name unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_cutlass_backend_integration Test cutlass backend can autotune other backends mm b b = torch randn cuda half b = torch randn cuda half t config patch max_autotune True max_autotune_gemm_backends ATEN TRITON CUTLASS cuda cutlass_max_profiling_configs needed log searching fx_graph_cache False fx_graph_remote_cache False log_settings +inductor assertLogs logger= torch _inductor codegen cuda level=logging DEBUG test_log Y_compiled = torch compile mm dynamic=False b Y = mm b torch testing assert_close Y_compiled Y output = \n join record getMessage record test_log records match = re search r Got cutlass configs total number ops \d+ output assert match Expect find cutlass configs log num_ops = int match group assertTrue num_ops The number ops should greater than unittest skipIf SM OrLater need sm_ test_maybe_append_choice_caching Test maybe_append_choice s caching leads correct results shorter maybe_append_choice time NUM_ITERATIONS = TestModule torch nn Module forward A B _ range NUM_ITERATIONS A = A B A model = TestModule cuda A = torch randn dtype=torch bfloat device= cuda B = torch randn dtype=torch bfloat device= cuda t expected = model A B Track render calls torch _inductor codegen cuda gemm_template CUTLASSGemmTemplate original_render = CUTLASSGemmTemplate render render_call_count = counting_render args kwargs nonlocal render_call_count render_call_count += original_render args kwargs mock patch object CUTLASSGemmTemplate render counting_render config patch max_autotune True max_autotune_gemm_backends CUTLASS fx_graph_cache False fx_graph_remote_cache False cuda enable_caching_codegen True cuda cutlass_max_profiling_configs compiled_model = torch compile model fullgraph=True actual = compiled_model A B torch testing assert_close actual expected Check render call count render called uniquely each codegen each finalized codegen assertEqual render_call_count NUM_ITERATIONS + unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_multiple_mm Test multiple matrix multiplications different shapes single nn Module MultipleMMModel torch nn Module forward b c d First mm shape - mm = b Second mm shape - mm = c d mm mm model = MultipleMMModel cuda Create tensors different shapes = torch randn cuda half b = torch randn cuda half t c = torch randn cuda half d = torch randn cuda half t Track render calls torch _inductor codegen cuda gemm_template CUTLASSGemmTemplate original_render = CUTLASSGemmTemplate render render_call_count = counting_render args kwargs nonlocal render_call_count render_call_count += original_render args kwargs mock patch object CUTLASSGemmTemplate render counting_render config patch max_autotune True max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs fx_graph_cache False fx_graph_remote_cache False cuda enable_caching_codegen True Get expected results expected = model b c d Compile run compiled_model = torch compile model actual = compiled_model b c d Verify results torch testing assert_close actual expected num_matmuls = assertEqual render_call_count num_matmuls + num_matmuls unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_multiple_mm_with_dynamic_shape Test multiple matrix multiplications where one has dynamic shapes MultipleMMDynamicModel torch nn Module __init__ - None super __init__ c = torch randn cuda half d = torch randn cuda half t forward b dynamic shape matmul mm = b static shape matmul mm = c d mm mm model = MultipleMMDynamicModel cuda Create tensors different shapes = torch randn cuda half b = torch randn cuda half t Track render calls torch _inductor codegen cuda gemm_template CUTLASSGemmTemplate original_render = CUTLASSGemmTemplate render render_call_count = counting_render args kwargs nonlocal render_call_count render_call_count += original_render args kwargs mock patch object CUTLASSGemmTemplate render counting_render config patch max_autotune True max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs fx_graph_cache False fx_graph_remote_cache False cuda enable_caching_codegen True Get expected results expected = model b Compile run compiled_model = torch compile model dynamic=True actual = compiled_model b Verify results torch testing assert_close actual expected num_matmuls = assertEqual render_call_count num_matmuls + num_matmuls unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_cutlass_backend_matmul_same_tensor max_autotune_gemm_backends = CUTLASS M = A = torch randn M M cuda half config patch max_autotune True max_autotune_gemm_backends max_autotune_gemm_backends cuda cutlass_max_profiling_configs compiled = torch compile torch mm torch testing assert_close A A t compiled A A t unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_cutlass_backend_matmul_nonzero_offset max_autotune_gemm_backends = CUTLASS M = A = torch randn M M - cuda half config patch max_autotune True max_autotune_gemm_backends max_autotune_gemm_backends cuda cutlass_max_profiling_configs compiled = torch compile torch mm torch testing assert_close A A t compiled A A t unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_flexible_layout TestModel torch nn Module forward B A = torch zeros_like B A B t M = B = torch randn M M cuda half model = TestModel cuda config patch max_autotune True max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs _ = torch compile model B unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache use_evt_config test_evt_flexible_layout TestModel torch nn Module forward B A = torch zeros_like B A B t relu M = B = torch randn M M cuda half model = TestModel cuda half config patch max_autotune True max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs _ = torch compile model B assertEqual torch _dynamo utils counters inductor cuda_epilogue_fusion_counter unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache test_filtered_ops_cache TestModel torch nn Module forward B A = torch zeros_like B _ range A = A B t A M = B = torch randn M M cuda half model = TestModel cuda start_time = time time config patch max_autotune True max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs _ = torch compile model B assertTrue time time - start_time unittest skipIf SM OrLater need sm_ mock patch dict os environ PATH _get_path_without_sccache parametrize use_aoti False True test_compilation_time use_aoti M = A = torch randn M M cuda half B = torch randn M M cuda half t MyModel torch nn Module forward b b model = MyModel cuda expected = model A B start_time = time time config patch max_autotune True max_autotune_gemm_backends CUTLASS cuda cutlass_max_profiling_configs use_aoti actual = AOTIRunnerUtil run model A B actual = torch compile model fullgraph=True A B torch testing assert_close actual expected assertTrue time time - start_time unittest skipIf SM OrLater need sm_ use_evt_config evt_all_ops evt_all_shapes test_evt_fusions_basic op shape TestModel torch nn Module forward b extra_args res = b relu add extra activation hit addmm path op res extra_args run_evt_test TestModel op shape unittest skipIf SM OrLater need sm_ use_evt_config evt_bin_ops test_evt_broadcasting op TestModel torch nn Module forward b extra_args acc = b acc op acc relu extra_args M = N = = torch ones M N cuda half b = torch ones N N cuda half t extra_args = gen_args op M N model = TestModel cuda result = torch compile model b extra_args ref_result = model b extra_args assertEqual torch _dynamo utils counters inductor cuda_epilogue_fusion_counter torch testing assert_close result ref_result unittest skipIf SM OrLater need sm_ use_evt_config evt_un_ops test_evt_activations op TestModel torch nn Module forward b extra_args acc = b acc op acc extra_args M = N = = torch ones M N cuda half b = torch ones N N cuda half t extra_args = gen_args op M N model = TestModel cuda result = torch compile model b extra_args ref_result = model b extra_args assertEqual torch _dynamo utils counters inductor cuda_epilogue_fusion_counter torch testing assert_close result ref_result unittest skipIf SM OrLater need sm_ use_evt_config evt_all_ops test_evt_mixed_dtypes op M = N = fp _tensor = torch ones M N cuda float TestModel torch nn Module forward b extra_args acc = b out = op acc relu extra_args out = torch add out fp _tensor out model = TestModel cuda = torch ones M N cuda half b = torch ones N N cuda half t extra_args = gen_args op M N dtype=torch float baseline cutlass kernel + triton matches expected casting behavior config patch cuda cutlass_epilogue_fusion_enabled False ref_result = torch compile model b extra_args assertEqual torch _dynamo utils counters inductor cuda_epilogue_fusion_counter torch _dynamo reset result = torch compile model b extra_args assertEqual torch _dynamo utils counters inductor cuda_epilogue_fusion_counter torch testing assert_close result ref_result unittest skipIf SM OrLater need sm_ use_evt_config evt_all_ops test_evt_multi_op op TestModel torch nn Module forward b extra_args acc = b torch add op acc relu extra_args relu acc run_evt_test TestModel op unittest skipIf SM OrLater need sm_ use_evt_config evt_all_ops test_evt_reuse_matmul_input op TestModel torch nn Module forward b extra_args acc = b torch add op acc relu extra_args relu run_evt_test TestModel op shape needs square unittest skipIf SM OrLater need sm_ use_evt_config evt_all_ops parametrize dynamic False True To drastically increase test time we only test dynamic test test_evt_multi_output op dynamic TestModel torch nn Module forward b extra_args acc = b z = acc relu z = op z extra_args y = z + z z y M = N = shapes = dynamic i shape enumerate shapes M N = shape = torch ones M N cuda half b = torch ones N N cuda half t extra_args = gen_args op M N model = TestModel cuda result = torch compile model b extra_args ref_result = model b extra_args assertEqual torch _dynamo utils counters inductor cuda_epilogue_fusion_counter i + torch testing assert_close result ref_result unittest skipIf SM OrLater need sm_ use_evt_config test_evt_return_accumulator op = torch add TestModel torch nn Module forward b extra_args acc = b acc op acc relu extra_args M = N = = torch ones M N cuda half b = torch ones N N cuda half t extra_args = gen_args op M N model = TestModel cuda result = torch compile model b extra_args ref_result = model b extra_args assertEqual torch _dynamo utils counters inductor cuda_epilogue_fusion_counter torch testing assert_close result ref_result mock patch dict os environ PATH _get_path_without_sccache parametrize arch parametrize cuda_version test_gemm_operation_serialization arch str cuda_version str Testing serialization GEMM operations generated CUTLASS This should cover GroupedGemmOperation well full_ops = _gen_ops_cached arch cuda_version ops = pytree tree_flatten full_ops sanity check assertGreater len ops Too few ops generated test configuration name unique op_config_names = op configuration_name op ops assertEqual len op_config_names len set op_config_names serializer = get_cutlass_operation_serializer assertIsNotNone serializer serialized_ops = serializer serialize op op ops deserialized_ops = serializer deserialize serialized_op serialized_op serialized_ops op deserialized_op zip ops deserialized_ops strict=False assertTrue _check_if_instances_equal op deserialized_op unittest skipIf PLATFORM_SUPPORTS_FP FP only supported H + unittest skipIf SM OrLater need sm_ fp _config parametrize float _dtype torch float _e m fn parametrize shape parametrize has_bias False True parametrize use_fast_accum False True parametrize input_dtype torch bfloat torch float test_fp _rowwise_scaling float _dtype torch dtype shape tuple int int int has_bias bool use_fast_accum bool input_dtype torch dtype Only bf output type supported row-wise scaling fp output_dtype torch dtype = torch bfloat device = cuda M K N = shape Matmul Y = X M K x W N K x = torch randn M K dtype=input_dtype device=device w = torch randn N K dtype=input_dtype device=device bias = None has_bias bias = torch randn N device=device dtype=input_dtype torch bfloat quantize weight prior inference w_fp w_inverse_scale = _quantize_rowwise w float _dtype w_t_fp = w_fp t w_inverse_scale = w_inverse_scale t scale_b should N quantize input x x_fp x_inverse_scale = _quantize_rowwise x float _dtype linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias y = torch _scaled_mm x_fp w_t_fp x_inverse_scale w_inverse_scale bias out_dtype=output_dtype use_fast_accum=use_fast_accum y y_eager = linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias linear_compiled = torch compile linear backend= inductor y_compiled = linear_compiled x_fp x_inverse_scale w_t_fp w_inverse_scale bias assertEqual y_eager dtype output_dtype assertEqual y_compiled dtype output_dtype torch testing assert_close y_eager y_compiled rtol= e- atol= unittest skipIf PLATFORM_SUPPORTS_FP FP only supported H + unittest skipIf SM OrLater need sm_ fp _config parametrize float _dtype torch float _e m fn parametrize shape parametrize use_fast_accum True parametrize use_aoti False True parametrize dynamic False True test_fp _rowwise_scaling_multiple_linear float _dtype torch dtype shape tuple int int use_fast_accum bool use_aoti bool = False dynamic bool = False This test meant simulate more realistic scenario dynamic use_aoti skipTest Accuracy issues when both AOTI dynamic enabled Only bf output type supported row-wise scaling fp output_dtype torch dtype = torch bfloat device = cuda M N = shape Matmul Y = X M K x W N K x = torch randn M N dtype=output_dtype device=device w = torch randn N N dtype=output_dtype device=device w = torch randn N N dtype=output_dtype device=device TestModule torch nn Module __init__ w w float _dtype super __init__ w _fp w _inverse_scale = _quantize_rowwise w float _dtype w _fp w _inverse_scale = _quantize_rowwise w float _dtype w _t_fp = w _fp t w _t_fp = w _fp t float _dtype = float _dtype forward x x_fp x_inverse_scale = _quantize_rowwise x float _dtype y = torch _scaled_mm x_fp w _t_fp x_inverse_scale view - w _inverse_scale view - out_dtype=output_dtype use_fast_accum=use_fast_accum y _fp y _inverse_scale = _quantize_rowwise y float _dtype y = torch _scaled_mm y _fp w _t_fp y _inverse_scale view - w _inverse_scale view - out_dtype=output_dtype use_fast_accum=use_fast_accum y model = TestModule w w float _dtype cuda dynamic_shapes = x Dim DYNAMIC Dim DYNAMIC dynamic None expected = model x use_aoti actual = AOTIRunnerUtil run model x dynamic_shapes=dynamic_shapes compiled_model = torch compile model fullgraph=True dynamic=dynamic actual = compiled_model x torch testing assert_close expected actual rtol= e- atol= unittest skipIf PLATFORM_SUPPORTS_FP FP only supported H + unittest skipIf SM OrLater need sm_ fp _config parametrize float _dtype torch float _e m fn parametrize shape parametrize has_bias False True parametrize use_fast_accum False parametrize input_dtype torch bfloat torch float test_fp _tensorwise_scaling float _dtype torch dtype shape tuple int int int has_bias bool use_fast_accum bool input_dtype torch dtype device = cuda M K N = shape Matmul Y = X M K x W N K output_dtype = input_dtype input output dtypes _scaled_mm do need same typically model they x = torch randn M K dtype=input_dtype device=device w = torch randn N K dtype=input_dtype device=device bias = None has_bias bias = torch randn N device=device dtype=input_dtype quantize weight prior inference w_fp w_inverse_scale = _quantize_tensorwise w float _dtype w_t_fp = w_fp t quantize input x x_fp x_inverse_scale = _quantize_tensorwise x float _dtype linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias y = torch _scaled_mm x_fp w_t_fp x_inverse_scale w_inverse_scale bias out_dtype=output_dtype use_fast_accum=use_fast_accum y y_eager = linear x_fp x_inverse_scale w_t_fp w_inverse_scale bias linear_compiled = torch compile linear backend= inductor mode= max-autotune y_compiled = linear_compiled x_fp x_inverse_scale w_t_fp w_inverse_scale bias assertEqual y_eager dtype output_dtype assertEqual y_compiled dtype output_dtype depending kernel config BLOCK_M size etc selected during Inductor autotuning compiled case results can different because way blocks results accumulated float addition associative so setting small absolute tolerance these tests torch testing assert_close y_eager y_compiled rtol= e- atol= unittest skipIf SM OrLater need sm_ test_config_number_post_filtering - None Test cutlass backend produces same number configs after filtering regardless layout dtype layouts = rr rc cr cc dtypes = torch float torch bfloat config_counts = layout layouts dtype dtypes = torch randn dtype=dtype cuda b = torch randn dtype=dtype cuda layout == c = t layout == c b = b t config patch max_autotune True max_autotune_gemm_backends CUTLASS needed log searching force_disable_caches True cuda cutlass_max_profiling_swizzle_options mock patch torch _inductor kernel mm autotune_select_algorithm wraps=select_no_algorithm sa assertRaisesRegex BackendCompilerFailed r NoValidChoicesError _ = torch compile torch mm dynamic=False b args _ = sa call_args _ choices _ __ = args config_counts layout dtype = len choices Check all config counts equal all_counts = list config_counts values assertTrue len set all_counts == f Config counts should equal across all layout dtype combinations f Got counts config_counts __name__ == __main__ torch _inductor utils is_big_gpu Set env make work CI HAS_CUDA_AND_TRITON HAS_CPU is_big_gpu run_tests