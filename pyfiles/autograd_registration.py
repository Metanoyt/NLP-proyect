mypy ignore-errors contextlib torch torch utils _pytree pytree contextlib contextmanager set_autograd_fallback_mode mode prev = torch _C _get_autograd_fallback_mode try torch _C _set_autograd_fallback_mode mode yield finally torch _C _set_autograd_fallback_mode prev autograd_registration_check op args kwargs Check autograd registered correctly operator Operators should have autograd support registered directly autograd dispatch key An incorrect registration may lead unexpected silent incorrectness Note check won t catch all problems will catch most common ones Example usage x = torch randn requires_grad=True autograd_registration_check torch ops aten sin default x Here some best practices you do find your autograd registered incorrectly - If operator composite i e consists other PyTorch ops you wish operator decompose get autograd support way then please register implementation DispatchKey CompositeImplicitAutograd - If you re adding autograd formula operator correct thing do register autograd Function DispatchKey Autograd preferred one DispatchKey Autograd BACKEND keys It NOT OK register autograd Function backend e g CPU CUDA key - If your operator non-differentiable then you should register implementation Autograd key uses AutoDispatchBelowAutograd re-invokes operator assert isinstance op torch _ops OpOverload Implementation details ----------------------------------------------- If operator doesn t have autograd kernel autograd key operator does inputs as-is then all outputs should have requires_grad=False before we apply special behaviors our default autograd fallback The default autograd fallback may set requires_grad=True output tensors certain modes so when they backpropped through they raise error Our strategy detecting operator doesn t have autograd kernel autograd key - set autograd fallback mode nothing so does change required-gradness outputs - run operator - Check any outputs operator inputs require grad This would only happen user calls regular PyTorch operations their backend key op should instead CompositeImplicitAutograd op user invokes autograd Function backend key Note s already likely bug operator directly returns input output because custom ops don t have good way constructing true in-place out variants we defer responsibility different test schema_check flat_args = pytree arg_tree_leaves args kwargs all_tensors = arg arg flat_args isinstance arg torch Tensor any t requires_grad t all_tensors raise RuntimeError autograd_registration_check no inputs have requires_grad=True so we unable actually perform test Please pass inputs do require grad Determine which AutogradBACKEND key check all_device_types = arg device type arg all_tensors all_device_types issubset cpu cuda xpu Don t want support other keys yet raise NotImplementedError f autograd_registration_check NYI devices other than CPU CUDA XPU got all_device_types cuda all_device_types key = AutogradCUDA cpu all_device_types key = AutogradCPU xpu all_device_types key = AutogradXPU torch _C _dispatch_has_kernel_for_dispatch_key op name key torch _C _dispatch_has_kernel_for_dispatch_key op name Autograd torch _C _dispatch_has_kernel_for_dispatch_key op name CompositeImplicitAutograd At point we know operator doesn t have kernel registered autograd key Let s proceed our test set_autograd_fallback_mode nothing all_outs = op args kwargs inp_ids = id arg arg flat_args not_an_input_and_requires_grad tensor tensor requires_grad False id tensor inp_ids False True pytree tree_any_only torch Tensor not_an_input_and_requires_grad all_outs raise AssertionError f op name least one output operator has requires_grad=True f operator does have autograd kernel defined autograd f key e g DispatchKey Autograd This could mean you have f incorrectly registered autograd kernel non-Autograd DispatchKey f which may lead silently incorrect results If your operator consists f regular PyTorch operations consider using operator all f registering your operator CompositeImplicitAutograd If you have f autograd Function registered backend CPU CUDA XPU key correct f location Autograd key