TorchScript This module contains functionality support JIT s scripting frontend notably - torch jit script This intended imported directly please use exposed functionalities ` torch jit ` collections copy enum functools inspect pickle warnings collections abc Callable typing Any Union typing_extensions deprecated torch torch _jit_internal _jit_internal torch _classes classes torch _jit_internal _get_model_id _qualified_name torch _utils_internal log_torchscript_usage torch jit _builtins _register_builtin torch jit _fuser _graph_for _script_method_graph_for torch jit _monkeytype_config JitTypeTraceConfig JitTypeTraceStore monkeytype_trace torch jit _recursive _compile_and_register_class infer_methods_to_compile ScriptMethodStub wrap_cpp_module torch jit _state _enabled _set_jit_function_cache _set_jit_overload_cache _try_get_jit_cached_function _try_get_jit_cached_overloads torch jit frontend get_default_args get_jit_class_def get_jit_def torch nn Module torch overrides has_torch_function has_torch_function_unary has_torch_function_variadic torch package PackageExporter PackageImporter torch utils set_module _serialization validate_map_location type_trace_db = JitTypeTraceStore DB hold all call traces MonkeyType torch _C ScriptMethod graph_for = _script_method_graph_for type ignore attr-defined torch _C ScriptFunction graph_for = _graph_for type ignore attr-defined ScriptFunction = torch _C ScriptFunction ScriptFunction __doc__ = Functionally equivalent ` ScriptModule ` represents single function does have any attributes Parameters ScriptFunction __name__ = ScriptFunction ScriptFunction __qualname__ = torch jit ScriptFunction set_module ScriptFunction torch jit Throws error jit function pickled Helps avoid Python crashes Python versions + when protocol given argument _reduce cls raise pickle PickleError ScriptFunction cannot pickled ScriptFunction __reduce__ = _reduce type ignore assignment _enabled Attribute = collections namedtuple Attribute value type Attribute value type type ignore no-redef value Attribute __doc__ = This method pass-through function returns ` value ` mostly used indicate TorchScript compiler left-hand side expression instance attribute type ` type ` Note ` torch jit Attribute ` should only used ` __init__ ` method ` jit ScriptModule ` subclasses Though TorchScript can infer correct type most Python expressions there some cases where type inference can wrong including - Empty containers like ` ` ` ` which TorchScript assumes container ` Tensor ` - Optional types like ` Optional T ` assigned valid value type ` T ` TorchScript would assume type ` T ` rather than ` Optional T ` In eager mode simply pass-through function returns ` value ` without other implications Example testcode torch typing Dict AttributeModule torch jit ScriptModule __init__ - None super __init__ foo = torch jit Attribute float we should able use foo float here assert foo names_ages = torch jit Attribute Dict str int names_ages someone = assert isinstance names_ages someone int m = AttributeModule m will contain two attributes foo type float names_ages type Dict str int testcleanup del AttributeModule del m Note s now preferred instead use type annotations instead ` torch jit Attribute ` testcode torch typing Dict AttributeModule torch nn Module names Dict str int __init__ - None super __init__ names = m = AttributeModule testcleanup del AttributeModule del m Args value An initial value assigned attribute type A Python type Returns Returns ` value ` _get_type_trace_db This private API Use external purposes discouraged type_trace_db Gets function name method type _get_function_from_type cls name getattr cls name None ScriptClasses must new-style classes because we construct them using their __new__ method _is_new_style_class cls hasattr cls __class__ __dict__ dir cls hasattr cls __slots__ These OrderedDictWrapper classes replace actual OrderedDicts module versions get set properties inside Module This allows us reuse most nn Module while still storing data C++ Each OrderedDict needs support x view x view view name = view values del view name view items view keys len view OrderedDictWrapper __init__ _c _c = _c keys k k v items values v k v items __len__ len values __delitem__ k raise RuntimeError cannot delete methods parameters script module items _c items __setitem__ k v k raise RuntimeError f Can t add new parameter after ScriptModule construction Tried add k _c setattr k v __contains__ k _c contains k __getitem__ k k raise KeyError k _c getattr k OrderedModuleDict OrderedDictWrapper __init__ module python_dict super __init__ torch _C ModuleDict module contains _both_ script modules non-script python-only modules because script modules subclassed python C++ Module will hold references them ensure you always get same python value here we store python dict well _python_modules = python_dict items r = _python_modules items r __contains__ k k _python_modules __setitem__ k v Cases where sub-module can re-assigned after ScriptModule construction If attr module interface type s guaranteed module inlined graph so s safe swap new ScriptModule new value ScriptModule same JIT type IR won t change s legit swap new module In these two cases we allow swapping new scripted module update corresponding python module dict keep sync Note value swapped has ScriptModule instead nn Module otherwise s illegal we throw error isinstance v ScriptModule _c setattr k v _python_modules k = v raise RuntimeError Cannot re-assign modules ScriptModule non-scripted f module tried replace existing module k v __getitem__ k _python_modules k For each user-defined subclasses ScriptModule meta-class finds all methods annotated script_method ScriptModule removes them attributes puts wrapper around s __init__ method recursively compile all script_methods module after original __init__ has run This has occur after user-defined __init__ so submodules parameters initialized _before_ script compiler resolve references ` param ` ` module ` ScriptMeta type __init__ cls name bases attrs noqa B Aggregate all ScriptMethods constants superclasses cls _methods dict str Any = cls _constants_set = set getattr cls __constants__ base reversed bases k v getattr base _methods items cls _methods k = v base_constants set = getattr base _constants_set set cls _constants_set = cls _constants_set union base_constants find all script methods current k v sorted attrs items isinstance v ScriptMethodStub delattr cls k cls _methods v original_method __name__ = v getattr cls _disable_script_meta False We leave built-in ScriptModule types alone since metaclass only compiling user classes inherit ScriptModule super __init__ name bases attrs original_init = getattr cls __init__ lambda None functools wraps original_init init_then_script args kwargs num_methods = len cls _methods original_init args kwargs added_methods_in_init = len cls _methods num_methods type cls make_stubs module cls = type module hasattr cls _methods v k v sorted cls _methods items infer_methods_to_compile module __dict__ _actual_script_module = torch jit _recursive create_script_module make_stubs share_types=not added_methods_in_init Delete Python attributes now shadow ScriptModule ones so __getattr__ __setattr__ will properly find scripted versions concrete_type = _actual_script_module _concrete_type name concrete_type get_attributes delattr name name _ concrete_type get_modules delattr name name _parameters _buffers _modules delattr name cls __init__ = init_then_script type ignore misc super __init__ name bases attrs _CachedForward __get__ obj cls __getattr__ forward type ignore attr-defined ScriptWarning Warning pass script_method fn _enabled fn NOTE we need traverse two frames here because meta-class frame ScriptModule will present opposed invoking script function invoking define CompilationUnit The stack will look like createResolutionCallback script_method ScriptModule metaclass frame Surrounding scope createResolutionCallback internally adds get us scope function calling function Adding gets us proper surrounding scope _rcb = _jit_internal createResolutionCallbackFromFrame frames_up= ast = get_jit_def fn fn __name__ self_name= ScriptModule ScriptMethodStub _rcb ast fn ConstMap __init__ const_mapping const_mapping = const_mapping __getattr__ attr const_mapping attr unpackage_script_module importer PackageImporter script_module_id str - torch nn Module Call ` ` torch package PackageImporter ` ` s Pickler s ` ` persistent_load ` ` function Performs work loading returning ScriptModule ` ` torch package ` ` archive isinstance importer zip_reader torch _C PyTorchFileReader raise RuntimeError Loading ScriptObjects PackageImporter created directory supported Use package archive file instead cu = torch _C CompilationUnit cpp_module = torch _C _import_ir_module_from_package cu importer zip_reader importer storage_context validate_map_location importer last_map_location script_module_id wrap_cpp_module cpp_module _enabled _magic_methods = __iter__ __len__ __neg__ __mul__ __contains__ __add__ __sub__ __pow__ __truediv__ __mod__ __ne__ __eq__ __lt__ __gt__ __le__ __ge__ __and__ __or__ __xor__ __getitem__ __setitem__ __call__ __int__ __float__ __bool__ __str__ __enter__ __exit__ RecursiveScriptClass Wrapper TorchScript instance use Python An analogue RecursiveScriptModule regular objects modules This wrapper around torch _C ScriptObject represents instance TorchScript allows used Python Attributes _c torch _C ScriptObject The C++ object which attribute lookups method calls forwarded _props Dict str property A dictionary properties fetched _c exposed wrppaer __init__ cpp_class super __init__ __dict__ _initializing = True _c = cpp_class Add wrapped object s properties instance _props = prop name property prop getter prop setter prop _c _properties __dict__ _initializing = False __getattr__ attr __dict__ get _initializing super __getattr__ attr type ignore misc attr _props _props attr fget type ignore call-arg misc getattr _c attr __setattr__ attr value __dict__ get _initializing super __setattr__ attr value attr _props _props attr fset value type ignore call-arg misc setattr _c attr value Delegate calls magic methods like __len__ C++ module backing RecursiveScriptClass forward_magic_method method_name args kwargs _c _has_method method_name raise TypeError self_method = __getattr__ method_name self_method args kwargs __getstate__ raise pickle PickleError ScriptClasses cannot pickled __iadd__ other _c _has_method __iadd__ forward_magic_method __iadd__ other forward_magic_method __add__ other method_name _magic_methods method_template args kwargs forward_magic_method method_name args kwargs setattr RecursiveScriptClass method_name method_template Python non-data descriptor causes first access ScriptModule s forward look up forward method stash objects dict Due standard rules attribute lookup subsequent lookups will just directly previously looked up method This necessary because nn Module defines forward method If we did nothing __getattr__ would called Instead we d get nn Module forward which always throws exception ScriptModule Module metaclass=ScriptMeta r Wrapper C++ torch jit Module methods attributes parameters A wrapper around C++ ` ` torch jit Module ` ` ` ` ScriptModule ` ` \s contain methods attributes parameters constants These can accessed same way normal ` ` nn Module ` ` __jit_unused_properties__ = code code_with_constants graph inlined_graph original_name __init__ - None super __init__ forward Callable Any = _CachedForward type ignore assignment __getattr__ attr _actual_script_module __dict__ super __getattr__ attr getattr _actual_script_module attr __setattr__ attr value _actual_script_module __dict__ Unwrap torch jit Attribute into regular setattr + record provided type __annotations__ This ensures we use attr again ` __init__ ` will look like actual value instance Attribute pyrefly ignore invalid-argument isinstance value Attribute NB Ensure we set __annotations__ specific question superclass which would wrong wrong wrong See also https github com pytorch pytorch issues __annotations__ __class__ __dict__ __class__ __annotations__ = __annotations__ attr = value type value = value value super __setattr__ attr value setattr _actual_script_module attr value define src _actual_script_module __dict__ If we have completed initialization just defer backing RecursiveScriptModule eagerly compile provided source _actual_script_module define src Otherwise we still object s __init__ In case add ` src ` stub compiled We use frames_up= get proper surrounding scope The stack will look like createResolutionCallback define surrounding scope createResolutionCallback internally adds get us our frame then we add get proper surrounding scope rcb = _jit_internal createResolutionCallbackFromFrame frames_up= ast = torch _C _parse_source_def src _methods ast name name = ScriptMethodStub rcb ast None _replicate_for_data_parallel _actual_script_module _replicate_for_data_parallel __reduce_package__ exporter PackageExporter Save ScriptModule inside ` ` torch package ` ` archive Called ` ` torch package PackageExporter ` ` s Pickler s ` ` persistent_id ` ` when saving TorchScript objects Performs act saving ScriptModule inside ` ` torch package ` ` archive Returns method load ScriptModule ` ` torch package PackageImporter ` ` s Pickler s ` ` persistent_load ` ` function script_module_id = exporter get_unique_id exporter script_module_serializer serialize _c int script_module_id unpackage_script_module script_module_id RecursiveScriptModule ScriptModule XXX RecursiveScriptModule inherits ScriptModule sole reason retains existing isinstance ScriptModule behavior r Retain existing isinstance ScriptModule behavior The core data structure TorchScript ` ` ScriptModule ` ` It analogue torch s ` ` nn Module ` ` represents entire model tree submodules Like normal modules each individual module ` ` ScriptModule ` ` can have submodules parameters methods In ` ` nn Module ` ` \s methods implemented Python functions ` ` ScriptModule ` ` \s methods implemented TorchScript functions statically-typed subset Python contains all PyTorch s built-in Tensor operations This difference allows your ` ` ScriptModule ` ` \s code run without need Python interpreter ` ` ScriptModule ` ` \s should created manually instead use either func ` tracing torch jit trace ` func ` scripting torch jit script ` Tracing scripting can applied incrementally ref ` composed necessary Types ` Tracing records tensor operations executed set example inputs uses these operations construct computation graph You can use full dynamic behavior Python tracing values other than Tensors control flow aren t captured graph Scripting inspects Python code model compiles TorchScript Scripting allows use many ` types ` _ values supports dynamic control flow Many all features Python supported compiler so changes source code may necessary _disable_script_meta = True __init__ cpp_module __dict__ _initializing = True _c = cpp_module super __init__ Delete training attribute set up ` Module __init__ ` It will get set underlying cpp module so we delete here avoid version shadowing cpp module version delattr training staticmethod _construct cpp_module init_fn Construct RecursiveScriptModule s ready use PyTorch code should use construct RecursiveScriptModule instead instead calling ` __init__ ` directly makes sure object properly finalized future we may take control how RecursiveScriptModule instance created Args cpp_module The C++ Module will hold actual state RecursiveScriptModule instance init_fn Lambda initializes RecursiveScriptModule passed script_module = RecursiveScriptModule cpp_module init_fn script_module Finalize ScriptModule replace nn Module state our custom implementations flip _initializing bit pyrefly ignore missing-attribute RecursiveScriptModule _finalize_scriptmodule script_module script_module staticmethod _finalize_scriptmodule script_module script_module _parameters = OrderedDictWrapper torch _C ParameterDict script_module _c script_module _buffers = OrderedDictWrapper torch _C BufferDict script_module _c script_module _modules = OrderedModuleDict script_module _c script_module _modules script_module _initializing = False _reconstruct cpp_module Re-construct instance RecursiveScriptModule using instance C++ module Args cpp_module The C++ module RecursiveScriptModule will rebuilt around __init__ cpp_module type ignore misc Copy concrete type C++ module ScriptModule _concrete_type = torch _C ConcreteModuleType from_jit_type _c _type Copy submodules C++ module ScriptModule modules = name cpp_module torch _C ModuleDict _c items modules name = wrap_cpp_module cpp_module _modules = OrderedModuleDict _c modules type ignore assignment Copy parameters buffers _parameters = OrderedDictWrapper torch _C ParameterDict _c type ignore assignment _buffers = OrderedDictWrapper torch _C BufferDict _c type ignore assignment Get rid functions old C++ module __dict__ = k v k v __dict__ items isinstance v torch _C ScriptMethod __dict__ _initializing = False property graph r Return string representation internal graph ` ` forward ` ` method _c _get_method forward graph property inlined_graph r Return string representation internal graph ` ` forward ` ` method This graph will preprocessed inline all function method calls forward inlined_graph type ignore attr-defined property code r Return pretty-printed representation valid Python syntax internal graph ` ` forward ` ` method forward code type ignore attr-defined property code_with_constants r Return tuple Returns tuple pretty-printed representation valid Python syntax internal graph ` ` forward ` ` method See ` code ` ConstMap following CONSTANT cN format output The indices output keys underlying constant s values r = forward code_with_constants type ignore attr-defined r ConstMap r save f kwargs r Save file-like object save f _extra_files= See func ` torch jit save torch jit save ` which accepts file-like object This function torch save converts object string treating path DO NOT confuse these two functions when comes f parameter functionality _c save str f kwargs deprecated Lite Interpreter deprecated Please consider switching ExecuTorch \ https docs pytorch org executorch stable getting-started html _save_for_lite_interpreter args kwargs r Add update bytecode session script model _save_for_lite_interpreter f The updated model used lite interpreter mobile applications Args f string containing file name _extra_files Map filename contents which will stored part f warnings warn Lite Interpreter deprecated Please consider switching ExecuTorch \ https docs pytorch org executorch stable getting-started html DeprecationWarning stacklevel= _c _save_for_mobile args kwargs deprecated Lite Interpreter deprecated Please consider switching ExecuTorch \ https docs pytorch org executorch stable getting-started html _save_to_buffer_for_lite_interpreter args kwargs warnings warn Lite Interpreter deprecated Please consider switching ExecuTorch \ https docs pytorch org executorch stable getting-started html DeprecationWarning stacklevel= _c _save_to_buffer_for_mobile args kwargs save_to_buffer args kwargs _c save_to_buffer args kwargs get_debug_state args kwargs _c get_debug_state extra_repr f original_name= original_name graph_for args kwargs forward graph_for args kwargs type ignore attr-defined property original_name type str _c _type name str _c _type name define src We use frames_up= get proper surrounding scope The stack will look like createResolutionCallback define surrounding scope createResolutionCallback internally adds get us our frame then we add get proper surrounding scope rcb = _jit_internal createResolutionCallbackFromFrame frames_up= _c _define _concrete_type src rcb __getattr__ attr _initializing __dict__ raise RuntimeError ScriptModule has been initialized did you forget call super s init _initializing super __getattr__ attr _modules check before hasattr since modules included attributes _c we want get python wrapper _modules instead raw _c object attr _modules _modules attr _c hasattr attr _c getattr attr _c _has_method attr script_method = _c _get_method attr cache method so future calls do go through __getattr__ improve invocation performance __dict__ attr = script_method script_method super __getattr__ attr __setattr__ attr value _initializing super __setattr__ attr value attr _modules _modules attr = value _c hasattr attr _c setattr attr value hasattr _concrete_type attr _concrete_type get_constants keys TODO we don t have _concrete_type set after load general we lose constant information We should encode constants type attributes something so persists across save load raise AttributeError f Cannot mutate TorchScript constant value attr Value value We allow setting Python attributes ScriptModule when people want stash some convenience info TODO s possible following confusing s = torch jit script s python_attr = s save --- doesn t have ` python_attr ` It s fairly trivial save enough info warn case super __setattr__ attr value __copy__ torch jit _recursive wrap_cpp_module copy copy _c __deepcopy__ memo torch jit _recursive wrap_cpp_module copy deepcopy _c memo Python magic methods do method lookups object s type instead looking up method defines instance In order continue expose magic methods builtin-containers ModuleList Sequential ModuleDict Python we define magic methods here shim correct attribute forward_magic_method method_name args kwargs self_method = getattr method_name getattr self_method __func__ None == getattr RecursiveScriptModule method_name raise NotImplementedError self_method args kwargs __iter__ forward_magic_method __iter__ __getitem__ idx forward_magic_method __getitem__ idx __len__ forward_magic_method __len__ __contains__ key forward_magic_method __contains__ key dir defined base nn Module so instead throwing overridden we call into nn Module __dir__ method __dir__ self_method = __dir__ self_method __func__ type ignore attr-defined _get_function_from_type RecursiveScriptModule __dir__ super __dir__ self_method resolve bool value Python looks __bool__ defined then __iter__ defined then returns true classes Since __iter__ throws isn t overridden we define __bool__ preserve default behavior __bool__ self_method = __bool__ self_method __func__ type ignore attr-defined _get_function_from_type RecursiveScriptModule __bool__ True self_method _replicate_for_data_parallel we have initialize ScriptModule properly so works pybind init_fn script_module Don t do anything here we ll initialize ScriptModule below pyrefly ignore missing-attribute RecursiveScriptModule _construct _c _replicate_for_data_parallel init_fn Need copy all RecursiveScriptModule methods ScriptModule This because ` super foo ` does use ` __getattr__ ` look up ` foo ` So we need make each method available ScriptModule manually pyrefly ignore missing-attribute name item RecursiveScriptModule __dict__ items callable item isinstance item property continue name startswith __ hasattr ScriptModule name continue We can copy over implementation wholesale because besides ` super ` thing above ScriptModule behaves exactly like RecursiveScriptModule setattr ScriptModule name item _get_methods cls inspect In Python unbound methods functions Python they methods inspect getmembers cls predicate=lambda x inspect isfunction x inspect ismethod x _compiled_methods_allowlist = forward register_buffer register_parameter register_module add_module _apply apply cuda cpu type float double half state_dict _save_to_state_dict load_state_dict _load_from_state_dict _named_members parameters named_parameters buffers named_buffers children named_children modules named_modules zero_grad share_memory _get_name extra_repr _slow_forward _tracing_name eval train get_extra_state set_extra_state _make_fail name fail args kwargs raise RuntimeError name + supported ScriptModules fail name method _get_methods torch nn Module name startswith __ name endswith _call_impl continue pyrefly ignore missing-attribute name RecursiveScriptModule __dict__ name _compiled_methods_allowlist setattr RecursiveScriptModule method __name__ _make_fail name TODO MAKE SURE THAT DISABLING WORKS RecursiveScriptClass type ignore no-redef pass ScriptModule torch nn Module type ignore no-redef __init__ arg=None super __init__ RecursiveScriptModule ScriptModule type ignore no-redef __init__ arg=None super __init__ call_prepare_scriptable_func_impl obj memo isinstance obj torch nn Module obj obj_id = id obj If obj_id memo obj has already been prepared being prepared another call up stack obj_id memo memo id obj obj = pyrefly ignore not-callable obj __prepare_scriptable__ hasattr obj __prepare_scriptable__ obj type ignore operator Record obj memo avoid infinite recursion case cycles module hierarchy when recursing below memo obj_id = obj new_obj_dict = name sub_module obj __dict__ items name == _modules k v sub_module items sub_module k = call_prepare_scriptable_func_impl v memo new_obj_dict name = sub_module isinstance sub_module torch nn Module isinstance sub_module ScriptModule new_obj_dict name = call_prepare_scriptable_func_impl sub_module memo new_obj_dict name = sub_module v new_obj_dict values obj __dict__ name = v obj call_prepare_scriptable_func obj memo dict int torch nn Module = call_prepare_scriptable_func_impl obj memo create_script_dict obj Create ` ` torch _C ScriptDict ` ` instance data ` ` obj ` ` Args obj dict The Python dictionary used initialize ` ` ScriptDict ` ` returned function Returns An instance ` ` torch _C ScriptDict ` ` has same data ` ` obj ` ` can passed between Python TorchScript reference semantics zero copy overhead torch _C ScriptDict obj type ignore attr-defined create_script_list obj type_hint=None Create ` ` torch _C ScriptList ` ` instance data ` ` obj ` ` Args obj dict The Python list used initialize ` ` ScriptList ` ` returned function Returns An instance ` ` torch _C ScriptList ` ` has same data ` ` obj ` ` can passed between Python TorchScript reference semantics zero copy overhead torch _C ScriptList obj type ignore attr-defined _TOPLEVEL bool = True _script_impl obj optimize=None _frames_up= _rcb=None example_inputs Union list tuple dict Callable list tuple None = None global type_trace_db optimize None warnings warn ` optimize ` deprecated has no effect Use ` torch jit optimized_execution ` instead FutureWarning stacklevel= No-op modules functions instances already scripted isinstance obj RecursiveScriptClass obj isinstance obj ScriptModule obj isinstance obj ScriptFunction obj example_inputs If MonkeyType installed enable profile directed type annotation Check example_inputs defined generate call traces method running eager mode version method provide example inputs This logs all traces type_trace_db type_trace_db = JitTypeTraceStore monkeytype_trace pyrefly ignore bad-argument-count monkeytype_config = JitTypeTraceConfig type_trace_db monkeytype_trace monkeytype_config isinstance example_inputs dict If obj nn Module then each method executed arguments provided example inputs example inputs here will type Dict method arguments This used infer type annotations those methods which called directly under hood monkeytype module example_input example_inputs items example example_input module example isinstance example_inputs list examples example_inputs obj examples raise ValueError Error Unable infer types Please format inputs type ` List Tuple ` ` Dict Callable List Tuple ` run MonkeyType warnings warn Warning monkeytype installed Please install https github com Instagram MonkeyType enable Profile-Directed Typing TorchScript Refer https github com Instagram MonkeyType blob master README rst install MonkeyType stacklevel= isinstance obj torch nn Module obj = call_prepare_scriptable_func obj torch jit _recursive create_script_module obj torch jit _recursive infer_methods_to_compile obj = obj __prepare_scriptable__ hasattr obj __prepare_scriptable__ obj type ignore operator isinstance obj dict create_script_dict obj isinstance obj list create_script_list obj inspect isclass obj qualified_name = _qualified_name obj If type ` nn Module ` subclass they probably meant pass instance instead Module issubclass obj torch nn Module raise RuntimeError f Type obj cannot compiled since inherits nn Module pass instance instead Enums automatically usable TorchScript explicitly scripting necessary harmful either issubclass obj enum Enum obj _is_new_style_class obj raise RuntimeError TorchScript classes must new-style classes Please inherit object len obj mro raise RuntimeError TorchScript classes does support inheritance yet Please directly inherit object _rcb None _rcb = _jit_internal createResolutionCallbackFromFrame _frames_up + _compile_and_register_class obj _rcb qualified_name obj inspect isfunction obj inspect ismethod obj qualified_name = _qualified_name obj decorated fn we need underlying fn its rcb hasattr obj __script_if_tracing_wrapper obj = obj __original_fn type ignore union-attr _rcb = _jit_internal createResolutionCallbackFromClosure obj some functions explicitly marked supported script mode hasattr obj __script_unsupported raise RuntimeError TorchScript error + obj __script_unsupported _check_directly_compile_overloaded obj maybe_already_compiled_fn = _try_get_jit_cached_function obj maybe_already_compiled_fn maybe_already_compiled_fn _torchdynamo_inline = obj type ignore attr-defined maybe_already_compiled_fn ast = get_jit_def obj obj __name__ _rcb None _rcb = _jit_internal createResolutionCallbackFromClosure obj fn = torch _C _jit_script_compile qualified_name ast _rcb get_default_args obj Forward docstrings fn __doc__ = obj __doc__ fn __name__ = ScriptFunction fn __qualname__ = torch jit ScriptFunction Allow torch compile inline fn _torchdynamo_inline = obj type ignore attr-defined _set_jit_function_cache obj fn fn torch jit _recursive create_script_class obj script obj optimize=None _frames_up= _rcb=None example_inputs Union list tuple dict Callable list tuple None = None r Script function Scripting function ` ` nn Module ` ` will inspect source code compile TorchScript code using TorchScript compiler ` ScriptModule ` ` ScriptFunction ` TorchScript itself subset Python language so all features Python work we provide enough functionality compute tensors do control-dependent operations For complete guide see ref ` language-reference ` Scripting dictionary list copies data inside into TorchScript instance than can subsequently passed reference between Python TorchScript zero copy overhead ` ` torch jit script ` ` can used function modules functions dictionaries lists decorator ` ` torch jit script ` ` torchscript-classes functions Args obj Callable nn Module The ` ` nn Module ` ` function type dictionary list compile example_inputs Union List Tuple Dict Callable List Tuple None Provide example inputs annotate arguments function ` ` nn Module ` ` Returns If ` ` obj ` ` ` ` nn Module ` ` ` ` script ` ` returns ` ScriptModule ` object The returned ` ScriptModule ` will have same set sub-modules parameters original ` ` nn Module ` ` If ` ` obj ` ` standalone function ` ScriptFunction ` will returned If ` ` obj ` ` ` ` dict ` ` then ` ` script ` ` returns instance ` torch _C ScriptDict ` If ` ` obj ` ` ` ` list ` ` then ` ` script ` ` returns instance ` torch _C ScriptList ` Scripting function The ` ` torch jit script ` ` decorator will construct ` ScriptFunction ` compiling body function Example scripting function testcode torch torch jit script foo x y x max y max r = x r = y r print type foo torch jit ScriptFunction See compiled graph Python code print foo code Call function using TorchScript interpreter foo torch ones torch ones testoutput hide Scripting function using example_inputs Example inputs can used annotate function arguments Example annotating function before scripting testcode torch test_sum b + b Annotate arguments int scripted_fn = torch jit script test_sum example_inputs= print type scripted_fn torch jit ScriptFunction See compiled graph Python code print scripted_fn code Call function using TorchScript interpreter scripted_fn testoutput hide Scripting nn Module Scripting ` ` nn Module ` ` default will compile ` ` forward ` ` method recursively compile any methods submodules functions called ` ` forward ` ` If ` ` nn Module ` ` only uses features supported TorchScript no changes original module code should necessary ` ` script ` ` will construct ` ScriptModule ` has copies attributes parameters methods original module Example scripting simple module Parameter testcode torch MyModule torch nn Module __init__ N M super __init__ This parameter will copied new ScriptModule weight = torch nn Parameter torch rand N M When submodule used will compiled linear = torch nn Linear N M forward input output = weight mv input This calls ` forward ` method ` nn Linear ` module which will cause ` linear ` submodule compiled ` ScriptModule ` here output = linear output output scripted_module = torch jit script MyModule Example scripting module traced submodules testcode torch torch nn nn torch nn functional F MyModule nn Module __init__ - None super __init__ torch jit trace produces ScriptModule s conv conv conv = torch jit trace nn Conv d torch rand conv = torch jit trace nn Conv d torch rand forward input input = F relu conv input input = F relu conv input input scripted_module = torch jit script MyModule To compile method other than ` ` forward ` ` recursively compile anything calls add func ` torch jit export torch jit export ` decorator method To opt out compilation use func ` torch jit ignore torch jit ignore ` func ` torch jit unused torch jit unused ` Example exported ignored method module torch torch nn nn MyModule nn Module __init__ - None super __init__ torch jit export some_entry_point input input + torch jit ignore python_only_fn input This function won t compiled so any Python APIs can used pdb pdb set_trace forward input training python_only_fn input input scripted_module = torch jit script MyModule print scripted_module some_entry_point torch randn print scripted_module torch randn Example Annotating forward nn Module using example_inputs torch torch nn nn typing NamedTuple MyModule NamedTuple result List int TestNNModule torch nn Module forward - MyModule result = MyModule result=a result pdt_model = TestNNModule Runs pdt_model eager model inputs provided annotates arguments forward scripted_model = torch jit script pdt_model example_inputs= pdt_model Run scripted_model actual inputs print scripted_model _enabled obj try global _TOPLEVEL prev = _TOPLEVEL _TOPLEVEL = False ret = _script_impl obj=obj optimize=optimize _frames_up=_frames_up + _rcb=_rcb example_inputs=example_inputs prev log_torchscript_usage script model_id=_get_model_id ret ret finally _TOPLEVEL = prev overloads registered _jit_internal compiled here so _overload can used nn functional py without cycle _check_overload_defaults impl_defaults overload_defaults loc name overload_value overload_defaults items name impl_defaults impl_defaults name = overload_value raise torch jit frontend FrontendError loc Default parameters overloads do affect runtime so they must equal default parameter implementation function Found f parameter name _compile_function_with_overload overload_fn qual_name impl_fn overload_decl = get_jit_def overload_fn overload_fn __name__ decl overload_signature = torch jit annotations get_signature overload_fn None None inspect ismethod overload_fn impl_ast = get_jit_def impl_fn impl_fn __name__ overload_defaults = get_default_args overload_fn implementation_defaults = get_default_args impl_fn _rcb = _jit_internal createResolutionCallbackFromClosure impl_fn _check_overload_defaults implementation_defaults overload_defaults overload_decl range fn = torch _C _jit_script_compile_overload qual_name overload_decl impl_ast _rcb implementation_defaults overload_signature fn _get_overloads obj check cached compiled fns existing_compiled_fns = _try_get_jit_cached_overloads obj qual_name = _qualified_name obj uncompiled_overloads = _jit_internal _get_fn_overloads qual_name uncompiled_overloads None existing_compiled_fns obj uncompiled_overloads raise RuntimeError _jit_internal get_overload_no_implementation_error_message function obj compiled_fns = _compile_function_with_overload overload_fn qual_name obj overload_fn uncompiled_overloads existing_compiled_fns compiled_fns = existing_compiled_fns + compiled_fns cache compilation remove information stored do compilation _set_jit_overload_cache obj compiled_fns _jit_internal _clear_fn_overloads qual_name compiled_fns _check_directly_compile_overloaded obj qual_name = _qualified_name obj _jit_internal _get_fn_overloads qual_name _try_get_jit_cached_overloads obj raise RuntimeError f Function qual_name cannot directly compiled because overloaded It must used context function where its inputs can determine which overload call interface obj r Decorate annotate classes modules different types This decorator can used define interface can used annotate classes modules different types This can used annotate submodule attribute could have different types implement same interface which could swapped runtime store list modules classes varying types It sometimes used implement Callables - functions modules implement interface whose implementations differ which can swapped out Example testcode torch typing List torch jit interface InterfaceType run x torch Tensor - torch Tensor pass implements InterfaceType torch jit script Impl run x torch Tensor - torch Tensor x relu Impl torch nn Module __init__ - None super __init__ val = torch rand torch jit export run x torch Tensor - torch Tensor x + val user_fn impls List InterfaceType idx int val torch Tensor - torch Tensor impls idx run val user_fn_jit = torch jit script user_fn impls = Impl torch jit script Impl val = torch rand user_fn_jit impls val user_fn_jit impls val inspect isclass obj raise RuntimeError interface must applied _is_new_style_class obj raise RuntimeError TorchScript interfaces must inherit object Expected MRO User module torch nn modules module Module object is_module_interface = issubclass obj torch nn Module len obj mro == is_module_interface len obj mro raise RuntimeError TorchScript interface does support inheritance yet Please directly inherit object nn Module qualified_name = _qualified_name obj rcb = _jit_internal createResolutionCallbackFromFrame type ` nn Module ` subclass generate module interface type instead interface type module interface type only compiles user provided methods part interface ast = get_jit_class_def obj obj __name__ mangled_classname = torch _C _jit_script_interface_compile qualified_name ast rcb is_module_interface obj __torch_script_interface__ = mangled_classname obj _recursive_compile_class obj loc _qual_name = _qualified_name obj We re starting new compilation so update error call stack case fails error_stack = torch _C CallStack _qual_name loc noqa F rcb = _jit_internal createResolutionCallbackForClassMethods obj _compile_and_register_class obj rcb _qual_name CompilationUnit = torch _C CompilationUnit set_module CompilationUnit torch jit pad s str padding int offset int = char str = padding = len s padding -= len s join char _ range padding + offset + s _ScriptProfileColumn __init__ header str alignment int = offset int = header = header alignment = alignment offset = offset rows dict int Any = add_row lineno int value Any rows lineno = value materialize max_length = len header rows list tuple int str = key value rows items cell = str value rows append key cell max_length = max len cell max_length alignment padding = max_length + alignment padding -= padding alignment padding = rows = key pad cell padding offset key cell rows pad header padding offset rows _ScriptProfileTable __init__ cols list _ScriptProfileColumn source_range list int cols = cols source_range = source_range dump_string outputs list str = cells list tuple str dict int str = header_buffer = col cols header rows = col materialize header_buffer += header cells append header dict rows outputs append header_buffer outputs append pad len header_buffer = line source_range row_buffer = header rows cells cell = rows get line cell None row_buffer += pad len header row_buffer += cell outputs append row_buffer \n join outputs _ScriptProfile __init__ - None profile = classes profiling _ScriptProfile enable profile enable disable profile disable dump_string - str outputs list str = source_stats profile _dump_stats source_ref = source_stats source source_lines = source_ref text splitlines dedent = min len line - len line lstrip line source_lines source_lines = line dedent line source_lines start_line = source_ref starting_lineno end_line = start_line + len source_lines source_range = range start_line end_line lineno = _ScriptProfileColumn Line hits = _ScriptProfileColumn Hits time_ns = _ScriptProfileColumn Time ns line_contents = _ScriptProfileColumn Line Contents stats = source_stats line_map line source_range lineno add_row line line line_contents add_row line source_lines line - start_line stat = stats get line stat None hits add_row line stat count time_ns add_row line stat duration_ns table = _ScriptProfileTable lineno hits time_ns line_contents list source_range outputs append table dump_string \n\n join outputs dump print dump_string _unwrap_optional x assert x None Unwrapping null optional x _register_builtin _unwrap_optional aten _unwrap_optional _register_builtin _jit_internal is_scripting aten is_scripting _register_builtin has_torch_function aten has_torch_function _register_builtin has_torch_function_unary aten has_torch_function _register_builtin has_torch_function_variadic aten has_torch_function