mypy allow-untyped-defs typing Optional Union torch _is_tensorpipe_available constants rpc_contants DeviceType = Union int str torch device __all__ = TensorPipeRpcBackendOptions _to_device device DeviceType - torch device device = torch device device device type = cuda raise ValueError ` set_devices ` expect list CUDA devices got f device type device type device _to_device_map device_map dict DeviceType DeviceType - dict torch device torch device full_device_map dict torch device torch device = reverse_map dict torch device torch device = k v device_map items k v = torch device k torch device v v reverse_map raise ValueError ` device_map ` only supports -to- mapping f trying map k reverse_map v v full_device_map k = v reverse_map v = k full_device_map _to_device_list devices list DeviceType - list torch device list map _to_device devices _is_tensorpipe_available type ignore has-type torch _C _distributed_rpc _TensorPipeRpcBackendOptionsBase _TensorPipeRpcBackendOptionsBase = object type ignore assignment misc pyrefly ignore invalid-inheritance TensorPipeRpcBackendOptions _TensorPipeRpcBackendOptionsBase r The backend options ` ~torch distributed rpc TensorPipeAgent ` derived ` ~torch distributed rpc RpcBackendOptions ` Args num_worker_threads int optional The number threads thread-pool used ` ~torch distributed rpc TensorPipeAgent ` execute requests default rpc_timeout float optional The default timeout seconds RPC requests default seconds If RPC has completed timeframe exception indicating so will raised Callers can override timeout individual RPCs meth ` ~torch distributed rpc rpc_sync ` meth ` ~torch distributed rpc rpc_async ` necessary init_method str optional The URL initialize distributed store used rendezvous It takes any value accepted same argument meth ` ~torch distributed init_process_group ` default ` ` env ` ` device_maps Dict str Dict optional Device placement mappings worker callee Key callee worker name value dictionary ` ` Dict ` ` ` ` int ` ` ` ` str ` ` ` ` torch device ` ` maps worker s devices callee worker s devices default ` ` None ` ` devices List int str ` ` torch device ` ` optional all local CUDA devices used RPC agent By Default will initialized all local devices its own ` ` device_maps ` ` corresponding devices its peers ` ` device_maps ` ` When processing CUDA RPC requests agent will properly synchronize CUDA streams all devices ` ` List ` ` __init__ num_worker_threads int = rpc_contants DEFAULT_NUM_WORKER_THREADS rpc_timeout float = rpc_contants DEFAULT_RPC_TIMEOUT_SEC init_method str = rpc_contants DEFAULT_INIT_METHOD device_maps Optional dict str dict DeviceType DeviceType = None devices Optional list DeviceType = None _transports Optional list = None _channels Optional list = None full_device_maps = device_maps None k _to_device_map v k v device_maps items full_device_list = devices None _to_device_list devices super __init__ num_worker_threads _transports _channels rpc_timeout init_method full_device_maps full_device_list set_device_map str device_map dict DeviceType DeviceType r Set device mapping between each RPC caller callee pair This function can called multiple times incrementally add device placement configurations Args str Callee name device_map Dict int str torch device Device placement mappings worker callee This map must invertible Example xdoctest +SKIP distributed both workers add x y print x tensor device= cuda x + y x + y worker options = TensorPipeRpcBackendOptions num_worker_threads= device_maps= worker maps worker s cuda worker s cuda options set_device_map worker maps worker s cuda worker s cuda rpc init_rpc worker rank= world_size= backend=rpc BackendType TENSORPIPE rpc_backend_options=options x = torch ones rets = rpc rpc_sync worker add args= x The first argument will moved cuda worker When sending value back will follow invert device map hence will moved back cuda cuda worker print rets tensor device= cuda print rets tensor device= cuda full_device_map = _to_device_map device_map curr_device_maps = super device_maps curr_device_maps k v full_device_map items k curr_device_maps v = curr_device_maps k raise ValueError ` set_device_map ` only supports -to- mapping trying f map k v curr_device_maps k super _set_device_map full_device_map set_devices devices list DeviceType r Set local devices used TensorPipe RPC agent When processing CUDA RPC requests TensorPipe RPC agent will properly synchronize CUDA streams all devices ` ` List ` ` Args devices List int str torch device local devices used TensorPipe RPC agent devices = _to_device_list devices