Owner s oncall distributed copy copy torch torch distributed _tools mod_tracker ModTracker torch testing _internal common_utils run_tests TestCase xfailIfTorchDynamo torch utils checkpoint checkpoint TestModTracker TestCase https github com pytorch pytorch issues xfailIfTorchDynamo test_module_hierarchy seen_fw = seen_bw = Foo torch nn Module forward x x = x relu_ seen_fw append copy tracker parents tracker is_bw x register_hook lambda grad seen_bw append copy tracker parents tracker is_bw torch mm x x Mod torch nn Module __init__ - None super __init__ = Foo b = torch nn ModuleDict nest Foo c = torch nn ModuleList Foo forward x x = c x b nest x mod = Mod ModTracker tracker mod torch randn requires_grad=True clone sum backward mod torch randn requires_grad=True clone sum backward assertEqual seen_fw Global Mod Mod c False Global Mod Mod False Global Mod Mod b nest False Global Mod Mod c False Global Mod Mod False Global Mod Mod b nest False assertEqual seen_bw Global Mod Mod b nest True Global Mod Mod True Global Mod Mod c True Global Mod Mod b nest True Global Mod Mod True Global Mod Mod c True test_bw_detection mod = torch nn Linear ModTracker tracker mod torch rand requires_grad=True sum backward assertFalse tracker is_bw assertEqual tracker parents Global xfailIfTorchDynamo test_user_hooks Bar torch nn Module __init__ - None super __init__ foo = torch nn Linear forward x foo x relu_ mt = ModTracker test_op = hook mod hook_name mfqn = mt get_known_fqn mod mod None None test_op append hook_name mfqn mfqn mt parents mt is_bw mod = Bar mt register_user_hooks lambda m inp hook m pre_fw lambda m inp op hook m post_fw lambda m gop hook m pre_bw lambda m ginp hook m post_bw mt mod torch rand requires_grad=True sum backward expected_op = pre_fw Bar True False pre_fw Bar foo True False post_fw Bar foo True False post_fw Bar True False pre_bw Bar True True pre_bw Bar foo True True post_bw Bar True True post_bw Bar foo True True assertEqual test_op expected_op assertRaises AssertionError mt register_user_hooks lambda x y x None None None test_op clear mt loss = mod torch rand requires_grad=True sum del mod loss backward expected_op = pre_fw Bar True False pre_fw Bar foo True False post_fw Bar foo True False post_fw Bar True False pre_bw None False True pre_bw None False True post_bw None False True post_bw None False True assertEqual test_op expected_op xfailIfTorchDynamo test_ac Foo torch nn Module __init__ n_layers int dim int use_ac bool = False super __init__ linears = torch nn ModuleList use_ac = use_ac _ range n_layers linears append torch nn Linear dim dim forward x i block enumerate linears i = use_ac x = checkpoint block x preserve_rng_state=True use_reentrant=False x = block x assert x None x = torch nn functional relu x x bsz = dim = n_layers = test_op = hook mod mt hook_name mfqn = mt get_known_fqn mod mod None None test_op append hook_name mfqn mfqn mt parents mt is_bw mt = ModTracker mt register_user_hooks lambda m i hook m mt pre_fw lambda m i o hook m mt post_fw lambda m go hook m mt pre_bw lambda m gi hook m mt post_bw model = Foo n_layers dim True x = torch randn bsz dim mt model x sum backward expected_op = pre_fw Foo True False pre_fw Foo linears True False post_fw Foo linears True False pre_fw Foo linears True False post_fw Foo linears True False post_fw Foo True False pre_bw Foo True True pre_bw Foo linears True True pre_fw Foo linears True True post_fw Foo linears True True post_bw Foo linears True True pre_bw Foo linears True True post_bw Foo linears True True post_bw Foo True True assertEqual test_op expected_op __name__ == __main__ run_tests