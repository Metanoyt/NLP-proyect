mypy allow-untyped-defs If you need modify file make test pass please also apply same edits accordingly https github com pytorch examples blob master distributed rpc rl main py https pytorch org tutorials intermediate rpc_tutorial html numpy np torch torch distributed rpc rpc torch nn nn torch nn functional F torch optim optim torch distributed rpc remote rpc_async rpc_sync RRef torch distributions Categorical torch testing _internal dist_utils dist_init worker_name torch testing _internal distributed rpc rpc_agent_test_fixture RpcAgentTestFixture TOTAL_EPISODE_STEP = GAMMA = SEED = _call_method method rref args kwargs r helper function call method given RRef method rref local_value args kwargs _remote_method method rref args kwargs r helper function run method owner rref fetch back result using RPC args = method rref + list args rpc_sync rref owner _call_method args=args kwargs=kwargs Policy nn Module r Borrowing ` ` Policy ` ` Reinforcement Learning example Copying code make these two examples independent See https github com pytorch examples tree master reinforcement_learning __init__ - None super __init__ affine = nn Linear dropout = nn Dropout p= affine = nn Linear saved_log_probs = rewards = forward x x = affine x x = dropout x x = F relu x action_scores = affine x F softmax action_scores dim= DummyEnv r A dummy environment implements required subset OpenAI gym interface It exists only avoid dependency gym running tests file It designed run set max number iterations returning random states rewards each step __init__ state_dim= num_iters= reward_threshold= state_dim = state_dim num_iters = num_iters iter = reward_threshold = reward_threshold seed manual_seed torch manual_seed manual_seed reset iter = torch randn state_dim step action iter += state = torch randn state_dim reward = torch rand item reward_threshold done = iter = num_iters info = state reward done info Observer r An observer has exclusive access its own environment Each observer captures state its environment send state agent select action Then observer applies action its environment reports reward agent __init__ - None id = rpc get_worker_info id env = DummyEnv env seed SEED run_episode agent_rref n_steps r Run one episode n_steps Arguments agent_rref RRef RRef referencing agent object n_steps int number steps episode state _ep_reward = env reset _ range n_steps send state agent get action action = _remote_method Agent select_action agent_rref id state apply action environment get reward state reward done _ = env step action report reward agent training purpose _remote_method Agent report_reward agent_rref id reward done break Agent __init__ world_size ob_rrefs = agent_rref = RRef rewards = saved_log_probs = policy = Policy optimizer = optim Adam policy parameters lr= e- eps = np finfo np float eps item running_reward = reward_threshold = DummyEnv reward_threshold ob_rank range world_size ob_info = rpc get_worker_info worker_name ob_rank ob_rrefs append remote ob_info Observer rewards ob_info id = saved_log_probs ob_info id = select_action ob_id state r This function mostly borrowed Reinforcement Learning example See https github com pytorch examples tree master reinforcement_learning The main difference instead keeping all probs one list agent keeps probs dictionary one key per observer NB no need enforce thread-safety here GIL will serialize executions probs = policy state unsqueeze m = Categorical probs action = m sample saved_log_probs ob_id append m log_prob action action item report_reward ob_id reward r Observers call function report rewards rewards ob_id append reward run_episode n_steps= r Run one episode The agent will tell each observer run n_steps make async RPC kick off episode all observers futs = rpc_async ob_rref owner _call_method args= Observer run_episode ob_rref agent_rref n_steps ob_rref ob_rrefs wait until all observers have finished episode fut futs fut wait finish_episode r This function mostly borrowed Reinforcement Learning example See https github com pytorch examples tree master reinforcement_learning The main difference joins all probs rewards different observers into one list uses minimum observer rewards reward current episode joins probs rewards different observers into lists R probs rewards = ob_id rewards probs extend saved_log_probs ob_id rewards extend rewards ob_id use minimum observer reward calculate running reward min_reward = min sum rewards ob_id ob_id rewards running_reward = min_reward + - running_reward clear saved probs rewards ob_id rewards rewards ob_id = saved_log_probs ob_id = policy_loss returns = r rewards - R = r + GAMMA R returns insert R returns = torch tensor returns returns = returns - returns mean returns std + eps log_prob R zip probs returns strict=True policy_loss append -log_prob R optimizer zero_grad policy_loss = torch cat policy_loss sum policy_loss backward optimizer step min_reward run_agent agent n_steps while True agent run_episode n_steps=n_steps agent finish_episode agent running_reward agent reward_threshold print f Solved Running reward now agent running_reward break ReinforcementLearningRpcTest RpcAgentTestFixture dist_init setup_rpc=False test_rl_rpc rank == Rank agent rpc init_rpc name=worker_name rank backend=self rpc_backend rank=self rank world_size=self world_size rpc_backend_options=self rpc_backend_options agent = Agent world_size run_agent agent n_steps=int TOTAL_EPISODE_STEP world_size - Ensure training run We don t really care about whether task learned since purpose test check API calls assertGreater agent running_reward Other ranks observers passively wait instructions agent rpc init_rpc name=worker_name rank backend=self rpc_backend rank=self rank world_size=self world_size rpc_backend_options=self rpc_backend_options rpc shutdown