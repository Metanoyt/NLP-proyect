Owner s oncall pt dataclasses functools torch torch nn torch _dynamo compiled_autograd torch _dynamo test_case run_tests TestCase torch _dynamo testing CompileCounter torch testing _internal common_utils IS_MACOS skipIfXpu torch testing _internal inductor_utils GPU_TYPE HAS_CPU requires_gpu Fake distributed WORLD_SIZE = init_fake_distributed device= cpu torch no_grad all_gather t torch cat t WORLD_SIZE torch no_grad reduce_scatter t clone since reduce_scatter input output should aliases t narrow t size WORLD_SIZE clone fw_pre_hook mod inp mod unsharded_weight untyped_storage resize_ mod unsharded_weight nelement mod unsharded_weight element_size torch no_grad torch autograd _unsafe_preserve_version_counter mod unsharded_weight torch ops fsdp copy_ mod unsharded_weight all_gather mod sharded_weight mod _parameters weight = mod unsharded_weight Forward mod sharded_weight = local_shard always Before mod weight = local_shard mod unsharded_weight = zero-sized allgather After mod weight = local_shard mod unsharded_weight = zero-sized allgather fw_post_hook mod inp out mod _parameters weight = mod sharded_weight mod unsharded_weight untyped_storage resize_ bw_pre_hook mod gO mod unsharded_weight untyped_storage resize_ mod unsharded_weight nelement mod unsharded_weight element_size torch no_grad torch autograd _unsafe_preserve_version_counter mod unsharded_weight torch ops fsdp copy_ mod unsharded_weight all_gather mod sharded_weight mod _parameters weight = mod unsharded_weight Backward mod sharded_weight = local_shard always Before mod weight = local_shard mod unsharded_weight = zero-sized allgather After mod weight = local_shard mod unsharded_weight = zero-sized allgather bw_post_hook mod gI gO grad = mod weight grad new_grad = reduce_scatter grad mod _parameters weight = mod sharded_weight mod weight grad = new_grad mod unsharded_weight untyped_storage resize_ torch manual_seed m = nn Linear bias=False device=device Mimics eager st iteration m sharded_weight = nn Parameter reduce_scatter m weight m unsharded_weight = nn Parameter all_gather m sharded_weight m unsharded_weight untyped_storage resize_ m register_full_backward_pre_hook bw_pre_hook m register_full_backward_hook bw_post_hook m register_forward_pre_hook fw_pre_hook m register_forward_hook fw_post_hook m torch rand requires_grad=True device=device init_module_bw_hooks allow_eager bw_pre_hook mod gO assert allow_eager torch _dynamo is_compiling assert mod weight size == mod hook_count_pre add_ torch sin gO + bw_post_hook mod gI gO assert allow_eager torch _dynamo is_compiling assert mod weight size == mod hook_count_post add_ torch sin gI + torch manual_seed m = nn Linear m hook_count_pre = torch tensor m hook_count_post = torch tensor m register_full_backward_pre_hook bw_pre_hook m register_full_backward_hook bw_post_hook m torch rand requires_grad=True steps m inp _ range out = m inp out sum backward out DistributedPatternTests TestCase test_intermediate_hook_with_closure dataclasses dataclass CustomObj val torch Tensor fn x obj y = x sin closure_var = y + y register_hook lambda grad grad + obj val + closure_var z = y sin z opt = torch compile fn fullgraph=True obj = CustomObj torch tensor obj = CustomObj torch tensor x = torch ones requires_grad=True x = torch ones requires_grad=True x = torch ones requires_grad=True x = torch ones requires_grad=True fn x obj sum backward fn x obj sum backward compiled_autograd _enable functools partial torch compile fullgraph=True opt x obj sum backward opt x obj sum backward assertEqual x grad x grad assertEqual x grad x grad test_intermediate_hook_with_nested_closure dataclasses dataclass CustomObj val torch Tensor fn x obj run y = x sin closure_var = y + y register_hook lambda grad grad + obj val + closure_var z = y sin z run opt = torch compile fn fullgraph=True obj = CustomObj torch tensor obj = CustomObj torch tensor x = torch ones requires_grad=True x = torch ones requires_grad=True x = torch ones requires_grad=True x = torch ones requires_grad=True fn x obj sum backward fn x obj sum backward compiled_autograd _enable functools partial torch compile fullgraph=True opt x obj sum backward opt x obj sum backward assertEqual x grad x grad assertEqual x grad x grad torch no_grad _test_storage_resize_zero device torch compile fullgraph=True fn x y = torch sin x x untyped_storage resize_ torch cos y x = torch randn device=device expected = torch cos torch sin x y = fn x assertEqual y expected assertEqual x untyped_storage size test_storage_resize_zero_cpu _test_storage_resize_zero cpu requires_gpu test_storage_resize_zero_gpu _test_storage_resize_zero GPU_TYPE torch no_grad _test_storage_resize_nonzero device torch compile fullgraph=True fn x out y = torch sin x assert out untyped_storage size == out untyped_storage resize_ x untyped_storage size out copy_ y cos x = torch randn device=device out = torch randn device=device expected = torch cos torch sin x out untyped_storage resize_ fn x out assertEqual out untyped_storage size x untyped_storage size assertEqual out expected test_storage_resize_nonzero_cpu _test_storage_resize_nonzero cpu requires_gpu test_storage_resize_nonzero_gpu _test_storage_resize_nonzero GPU_TYPE torch no_grad test_unsafe_set_version_counter cnt = CompileCounter torch compile backend=cnt fullgraph=True fn w x x = x sin v = w _version w copy_ x + torch _C _autograd _unsafe_set_version_counter w v w v v w = torch randn i range v w fill_ i bump w _version assertEqual w _version v x = torch randn w v = fn w x assertIs w w assertEqual w x sin + assertEqual v v assertEqual w _version v assertEqual cnt frame_count test_unsafe_set_version_counter torch compile backend= inductor fullgraph=True fn w x r = w sin torch no_grad v = w _version w copy_ x torch _C _autograd _unsafe_set_version_counter w v r w = torch randn requires_grad=True x = torch randn expected_r = w detach sin r = fn w x r backward assertEqual r expected_r assertEqual w x assertEqual w grad x cos torch no_grad test_unsafe_preserve_version_counter torch compile backend= eager fullgraph=True fn w x x = x sin torch autograd _unsafe_preserve_version_counter w w copy_ x + w w = torch randn fill_ fill_ x = torch randn v = w _version w = fn w x v = w _version assertIs w w assertEqual w x sin + assertEqual v v test_unsafe_preserve_version_counter torch compile backend= inductor fullgraph=True fn w x r = w sin torch no_grad torch autograd _unsafe_preserve_version_counter w w copy_ x r w = torch randn requires_grad=True x = torch randn expected_r = w detach sin r = fn w x r backward assertEqual r expected_r assertEqual w x assertEqual w grad x cos test_module_backward_hooks_eager m inp = init_module_bw_hooks True out = steps m inp m inp = init_module_bw_hooks False fw_cnt = CompileCounter bw_cnt = CompileCounter compiled_autograd _enable torch compile backend=bw_cnt fullgraph=True m = torch compile m backend=fw_cnt fullgraph=True out = steps m inp assertEqual m hook_count_pre m hook_count_pre assertEqual m hook_count_post m hook_count_post assertEqual out out assertEqual inp grad inp grad assertEqual m weight grad m weight grad assertEqual m bias grad m bias grad assertEqual fw_cnt frame_count assertEqual fw_cnt op_count assertEqual bw_cnt frame_count grad=None grad =None assertEqual bw_cnt op_count Number ops Dynamo-produced graphs test_module_backward_hooks_aot m inp = init_module_bw_hooks True out = steps m inp m inp = init_module_bw_hooks True m = torch compile m backend= aot_eager fullgraph=True compiled_autograd _enable lambda gm gm out = steps m inp assertEqual m hook_count_pre m hook_count_pre assertEqual m hook_count_post m hook_count_post assertEqual out out assertEqual inp grad inp grad assertEqual m weight grad m weight grad assertEqual m bias grad m bias grad test_module_backward_hooks_inductor m inp = init_module_bw_hooks True out = steps m inp m inp = init_module_bw_hooks False m = torch compile m fullgraph=True compiled_autograd _enable torch compile fullgraph=True out = steps m inp assertEqual m hook_count_pre m hook_count_pre assertEqual m hook_count_post m hook_count_post assertEqual out out assertEqual inp grad inp grad assertEqual m weight grad m weight grad assertEqual m bias grad m bias grad test_module_backward_hooks_multi_layers inp = init_module_bw_hooks True b _ = init_module_bw_hooks True out = steps torch nn Sequential b inp inp = init_module_bw_hooks False b _ = init_module_bw_hooks False compiled_autograd _enable torch compile fullgraph=True out = steps torch compile torch nn Sequential b fullgraph=True inp assertEqual hook_count_pre hook_count_pre assertEqual hook_count_post hook_count_post assertEqual b hook_count_pre b hook_count_pre assertEqual b hook_count_post b hook_count_post assertEqual out out assertEqual inp grad inp grad assertEqual weight grad weight grad assertEqual bias grad bias grad assertEqual b weight grad b weight grad assertEqual b bias grad b bias grad TODO jansel support bw hooks graph break _assert_same_grad b assertEqual type type b assertEqual b assertEqual grad b grad assertEqual requires_grad b requires_grad test_nn_param_return fn x p = torch nn Parameter x p p sin opt = torch compile fn fullgraph=True x = torch randn x = x clone p r = fn x r sum backward p r = opt x r sum backward _assert_same_grad r r _assert_same_grad p p test_nn_param_return fn x p = torch nn Parameter x requires_grad=False p x + opt = torch compile fn fullgraph=True x = torch randn x = x clone p r = fn x p r = opt x _assert_same_grad r r _assert_same_grad p p torch _dynamo config patch graph_break_on_nn_param_ctor False test_nn_param_return fn x p = torch nn Parameter x + p p sin opt = torch compile fn fullgraph=True x = torch randn x = x clone p r = fn x r sum backward p r = opt x r sum backward _assert_same_grad r r _assert_same_grad p p torch _dynamo config patch graph_break_on_nn_param_ctor False test_nn_param_return fn x p = torch nn Parameter x + requires_grad=False p x + opt = torch compile fn fullgraph=True x = torch randn x = x clone p r = fn x p r = opt x _assert_same_grad r r _assert_same_grad p p torch _functorch config patch recompute_views=True test_fake_distributed_aot_eager m inp = init_fake_distributed out = steps m inp m inp = init_fake_distributed m = torch compile m backend= aot_eager fullgraph=True bw_cnt = CompileCounter compiled_autograd _enable torch compile backend=bw_cnt fullgraph=True out = steps m inp _assert_same_grad m weight m weight _assert_same_grad inp inp _assert_same_grad out out Recompile grad==None grad =None assertEqual bw_cnt frame_count skipIfXpu requires_gpu torch _functorch config patch recompute_views=True test_fake_distributed_inductor m inp = init_fake_distributed GPU_TYPE out = steps m inp m inp = init_fake_distributed GPU_TYPE m = torch compile m fullgraph=True compiled_autograd _enable torch compile fullgraph=True out = steps m inp _assert_same_grad m weight m weight _assert_same_grad inp inp _assert_same_grad out out __name__ == __main__ HAS_CPU IS_MACOS run_tests needs= filelock