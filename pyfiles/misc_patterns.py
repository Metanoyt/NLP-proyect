mypy allow-untyped-defs functools torch torch _dynamo utils counters torch _ops OpOverload OpOverloadPacket torch utils _ordered_set OrderedSet pattern_matcher fwd_only register_replacement aten = torch ops aten functools cache _misc_patterns_init joint_graph patterns joint_graph_patterns post_grad pass_patterns post_grad_patterns_all post_grad_patterns = post_grad_patterns_all medium priority torch cuda is_available workaround https github com pytorch pytorch issues device = cuda device = cpu These patterns do things Since we know index completely unique we can codegen using stores instead atomic adds which quite bit faster Also since we guaranteed they completely within bounds we can use unsafe indexing skip debug asserts randperm_index_add_pattern x y index = torch randperm x shape device=x device y shape torch index_add x dim= source=y index=index index randperm_index_add_replacement x y index = torch randperm x shape device=x device y shape torch ops aten _unsafe_index_put x index aten _unsafe_index x index + y accumulate=False index register_replacement pyrefly ignore bad-argument-type randperm_index_add_pattern pyrefly ignore bad-argument-type randperm_index_add_replacement torch empty device=device torch empty device=device pyrefly ignore bad-argument-type fwd_only pyrefly ignore bad-argument-type post_grad_patterns joint_graph_patterns randperm_index_pattern x slice_shape index = torch randperm x shape device=x device slice_shape torch ops aten index x index index randperm_index_replacement x slice_shape index = torch randperm x shape device=x device slice_shape torch ops aten _unsafe_index x index index register_replacement pyrefly ignore bad-argument-type randperm_index_pattern pyrefly ignore bad-argument-type randperm_index_replacement torch empty device=device pyrefly ignore bad-argument-type fwd_only pyrefly ignore bad-argument-type post_grad_patterns joint_graph_patterns scalar_workaround= slice_shape NumpyCompatNormalization numpy_compat dict str tuple str = dim axis keepdim keepdims input x x other x inverse_mapping dict str str cache dict torch fx graph Target OrderedSet str __init__ - None cache = callable - tuple replaceable args e g axis inverse_mapping = actual_kwarg numpy_kwargs numpy_compat items numpy_kwarg numpy_kwargs assert numpy_kwarg inverse_mapping inverse_mapping numpy_kwarg = actual_kwarg __call__ graph torch fx Graph node graph nodes node op = call_function continue isinstance node target OpOverload OpOverloadPacket only applies torch ops e g torch stack axis= works torch ops aten stack axis= doesn t continue kwargs = node kwargs node target cache replaceable_kwargs = cache node target signatures = torch fx operator_schemas get_signature_for_torch_op node target signatures = signatures None signatures replaceable_kwargs = OrderedSet sig signatures param_name sig parameters keys param_name numpy_compat replaceable_kwargs update numpy_compat param_name cache node target = replaceable_kwargs replaceable_kwargs continue new_kwargs = kwargs_changed = False k v kwargs items k replaceable_kwargs kwargs_changed = True new_kwargs inverse_mapping k = v new_kwargs k = v kwargs_changed node kwargs = torch fx immutable_collections immutable_dict new_kwargs counters inductor numpy_compat_normalization += numpy_compat_normalization = NumpyCompatNormalization