functools partial typing no_type_check Optional torch torch distributed _functional_collectives AsyncCollectiveTensor torch distributed tensor DTensor torch distributed tensor _dtensor_spec DTensorSpec no_type_check sync_grad_hook grad device_handle=None compute_stream=None isinstance grad AsyncCollectiveTensor compute_stream None device_handle stream compute_stream grad = grad wait grad = grad wait grad _flatten_tensor tensor torch Tensor - tuple torch Tensor Optional DTensorSpec isinstance tensor DTensor tensor _local_tensor requires_grad_ tensor _local_tensor tensor _spec tensor None no_type_check _unflatten_tensor tensor spec device_handle=None compute_stream=None unflatten would mainly called every time FSDP allgather parameters result = DTensor from_local tensor spec mesh spec placements run_check=False shape=spec shape stride=spec stride tensor requires_grad only register hook tensor requires grad tensor register_hook partial sync_grad_hook device_handle=device_handle compute_stream=compute_stream result