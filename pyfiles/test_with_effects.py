Owner s module functorch ruff noqa F flake noqa B unittest collections deque functools partial typing TYPE_CHECKING torch torch _dynamo torch _functorch torch _inductor torch _inductor decomposition functorch compile aot_function default_decompositions min_cut_rematerialization_partition nop torch _functorch aot_autograd aot_export_module torch _higher_order_ops effects with_effects torch _higher_order_ops torchbind enable_torchbind_tracing torch fx experimental proxy_tensor make_fx torch testing FileCheck torch testing _internal common_cuda _get_torch_cuda_version SM OrLater SM OrLater torch testing _internal common_quantization skipIfNoDynamoSupport torch testing _internal common_utils IS_WINDOWS run_tests skipIfTorchDynamo TEST_CUDA TEST_WITH_ROCM TestCase torch testing _internal torchbind_impls init_torchbind_implementations TYPE_CHECKING torch utils hooks RemovableHandle torch testing _internal two_tensor TwoTensor extract_graph fx_g _ graph_cell graph_cell = fx_g fx_g get_fw_bw_graph f inps partitioner=min_cut_rematerialization_partition dynamic=False fw_graph_cell = None bw_graph_cell = None requires_grad = False fn_req_grad t nonlocal requires_grad requires_grad = requires_grad t requires_grad t torch utils _pytree tree_map_only torch Tensor fn_req_grad inps out = aot_function f fw_compiler=partial extract_graph graph_cell=fw_graph_cell bw_compiler= partial extract_graph graph_cell=bw_graph_cell requires_grad nop partition_fn=partitioner decompositions=default_decompositions dynamic=dynamic inps requires_grad out sum backward fw_graph_cell bw_graph_cell make_inputs_non_leaves inps torch utils _pytree tree_map_only torch Tensor lambda t t add inps unittest skipIf torch _dynamo is_dynamo_supported dynamo isn t support TestWithEffects TestCase setUp init_torchbind_implementations test_print M torch nn Module forward x torch ops aten _print moo res = x + x torch ops aten _print moo res inputs = torch randn Without functionalization print should just appear graph directly gm = make_fx M inputs FileCheck check_count torch ops aten _print default exactly=True run gm code With functionalization should appear wrapped with_effects gm gs = aot_export_module M inputs trace_joint=False assertExpectedInline str gm code strip \ forward arg _ arg _ with_effects = torch ops higher_order with_effects arg _ torch ops aten _print default moo arg _ = None getitem = with_effects with_effects = None add = torch ops aten add Tensor arg _ arg _ arg _ = None with_effects_ = torch ops higher_order with_effects getitem torch ops aten _print default moo getitem = None getitem_ = with_effects_ with_effects_ = None getitem_ add assertEqual len gs input_tokens assertEqual len gs output_tokens torch _functorch config patch unlift_effect_tokens=True gm gs = aot_export_module M inputs trace_joint=False assertExpectedInline str gm code strip \ forward arg _ _make_token_default = torch ops prims _make_token default with_effects = torch ops higher_order with_effects _make_token_default torch ops aten _print default moo _make_token_default = None getitem = with_effects with_effects = None add = torch ops aten add Tensor arg _ arg _ arg _ = None with_effects_ = torch ops higher_order with_effects getitem torch ops aten _print default moo getitem = None getitem_ = with_effects_ with_effects_ = None _sink_tokens_default = torch ops prims _sink_tokens default getitem_ getitem_ = _sink_tokens_default = None add noqa B test_torchbind_custom_op M torch nn Module __init__ - None super __init__ attr = torch classes _TorchScriptTesting _Foo forward x x + torch ops _TorchScriptTesting takes_foo attr x enable_torchbind_tracing gm gs = aot_export_module M torch ones trace_joint=False assertExpectedInline str gm code strip \ forward arg _ arg _ _torchbind_obj = _torchbind_obj with_effects = torch ops higher_order with_effects arg _ torch ops _TorchScriptTesting takes_foo default _torchbind_obj arg _ arg _ = _torchbind_obj = None getitem = with_effects getitem_ = with_effects with_effects = None add = torch ops aten add Tensor arg _ getitem_ arg _ = getitem_ = None getitem add noqa B assertEqual len gs input_tokens assertEqual len gs output_tokens test_print_with_buffer_mutations M torch nn Module __init__ - None super __init__ buf = torch nn Buffer torch ones forward x torch ops aten _print moo res = x + x buf add_ res res = buf + x torch ops aten _print moo res inputs = torch randn With functionalization should appear wrapped with_effects gm gs = aot_export_module M inputs trace_joint=False assertExpectedInline str gm code strip \ forward arg _ arg _ arg _ with_effects = torch ops higher_order with_effects arg _ torch ops aten _print default moo arg _ = None getitem = with_effects with_effects = None add = torch ops aten add Tensor arg _ arg _ add_ = torch ops aten add Tensor arg _ add arg _ = add = None add_ = torch ops aten add Tensor add_ arg _ arg _ = None with_effects_ = torch ops higher_order with_effects getitem torch ops aten _print default moo getitem = None getitem_ = with_effects_ with_effects_ = None getitem_ add_ add_ assertEqual len gs input_tokens assertEqual len gs output_tokens assertEqual len gs buffers_to_mutate test_print_with_input_mutations M torch nn Module __init__ - None super __init__ forward x torch ops aten _print moo res = x + x x add_ res res = x + x torch ops aten _print moo res inputs = torch randn With functionalization should appear wrapped with_effects gm gs = aot_export_module M inputs trace_joint=False assertEqual len gs input_tokens assertEqual len gs output_tokens assertEqual len gs user_inputs_to_mutate test_alias_op f token x token out = with_effects token torch ops aten absolute_ default x token out assertRaisesRegex AssertionError r Ops aliasing supported make_fx f torch tensor torch tensor test_compile_aot_eager f x torch ops aten _print moo res = x + x torch ops aten _print moo res inputs = torch randn res = torch compile f backend= aot_eager inputs assertTrue torch allclose res f inputs unittest skipIf IS_WINDOWS triton unittest skipIf SM OrLater triton test_compile_inductor f x torch ops aten _print moo res = x + x torch ops aten _print moo res inputs = torch randn res = torch compile f backend= inductor inputs assertTrue torch allclose res f inputs unittest skipIf IS_WINDOWS Skipped Windows skipIfNoDynamoSupport test_compile_inductor_external_op_return_none torch library _scoped_library mylib FRAGMENT lib torch library define mylib inplace_add Tensor input Tensor output - lib=lib inplace_add input torch Tensor output torch Tensor - None assert input device == output device output add_ input lib impl inplace_add inplace_add CompositeExplicitAutograd f x out = torch empty out = torch zeros_like out torch ops mylib inplace_add x out out inputs = torch randn res = torch compile f backend= inductor inputs assertTrue torch allclose res f inputs test_compile_aot_eager_requires_grad f x torch ops aten _print moo res = x + x torch ops aten _print moo res inputs = torch randn requires_grad=True res = torch compile f backend= aot_eager inputs assertTrue torch allclose res f inputs res sum backward unittest skipIf IS_WINDOWS triton unittest skipIf TEST_WITH_ROCM triton unittest skipIf SM OrLater triton unittest skipIf _get_torch_cuda_version = triton unittest skipIf TEST_CUDA triton skipIfNoDynamoSupport test_register_effectful_custom_op torch library _scoped_library mylib FRAGMENT lib torch _dynamo config capture_scalar_outputs = True torch _dynamo config capture_dynamic_output_shape_ops = True torch library define mylib record_scalar_tensor Tensor x str prefix - lib=lib global variable store recorded tensor prefix recorded_dict = Pytorch custorm op implementation torch library impl mylib record_scalar_tensor CompositeExplicitAutograd lib=lib record_scalar_tensor x prefix recorded_dict prefix = x clone Meta function custom op torch library register_fake mylib record_scalar_tensor lib=lib record_scalar_tensor_meta x prefix torch _higher_order_ops effects _EffectType _register_effectful_op _register_effectful_op torch ops mylib record_scalar_tensor default _EffectType ORDERED my_config = my_config MockModule = mean my_config MockModule linear = mean my_config MockModule relu = mean MyLinear torch nn Module __init__ in_features out_features super __init__ weight = torch nn Parameter torch randn out_features in_features requires_grad=True bias = torch nn Parameter torch randn out_features requires_grad=True forward x torch nn functional linear x weight bias MockModule torch nn Module __init__ - None super __init__ linear = MyLinear register_buffer buf torch randn requires_grad=True forward x torch nn functional relu linear x + buf forward_hook module torch nn Module inputs torch Tensor output torch Tensor prefix str aggregate_method str - torch Tensor aggregate_method == mean torch ops mylib record_scalar_tensor output mean prefix aggregate_method == max torch ops mylib record_scalar_tensor output max prefix demo purpose using min torch ops mylib record_scalar_tensor output sum prefix output add_hooks module config handles list RemovableHandle = q = deque module __class__ __name__ module while q name m = q pop children = name + + n y n y m named_children q extend children aggregate_method = config get name mean prefix = name + + aggregate_method handle = m register_forward_hook partial forward_hook prefix=prefix aggregate_method=aggregate_method handle handles append handle handles x = torch randn device= cuda mod = MockModule cuda add_hooks mod my_config opt_mod = torch compile backend= inductor mod y = opt_mod x assertTrue torch allclose y mod x Ensure works well backward y sum backward Ensure grad existing assertTrue isinstance opt_mod linear weight grad torch Tensor assertEqual len recorded_dict assertTrue MockModule linear mean recorded_dict assertTrue MockModule mean recorded_dict skipIfNoDynamoSupport test_effectful_custom_op_with_subclasses torch library _scoped_library _mylib FRAGMENT lib lib define zoo Tensor x - Tensor lib define zoo Tensor x - Tensor d = fw bw reset_counter d fw = d bw = assert_counter fw bw assertEqual d fw fw assertEqual d bw bw foo_impl d fw = d fw + clone foo_meta clone foo _impl x d bw = d bw + x clone foo _meta clone backend CPU CUDA lib impl zoo foo_impl backend lib impl zoo foo _impl backend lib impl zoo foo_meta Meta lib impl zoo foo _meta Meta foo_bwd ctx grad torch ops _mylib zoo grad grad clone torch library register_autograd _mylib zoo foo_bwd lib=lib torch _higher_order_ops effects _EffectType _register_effectful_op _register_effectful_op torch ops _mylib zoo default _EffectType ORDERED _register_effectful_op torch ops _mylib zoo default _EffectType ORDERED fn x y torch ops _mylib zoo x + y ins_sc TwoTensor torch tensor torch tensor torch tensor ins_dense torch tensor torch tensor ins_fn expected_fw_count zip ins_sc ins_dense reset_counter ref_out = fn ins_fn assert_counter expected_fw_count compiled_fn = torch compile fn backend= aot_eager out = compiled_fn ins_fn reset_counter out = compiled_fn ins_fn assert_counter expected_fw_count assertEqual ref_out out ins_dense_req_grad torch tensor requires_grad=True torch tensor requires_grad=True ins_sc_req_grad TwoTensor torch tensor requires_grad=True torch tensor requires_grad=True TwoTensor torch tensor requires_grad=True torch tensor requires_grad=True ins_fn_req_grad expected_fw_count expected_fw_count_after_bw expected_bw_count_after_bw zip ins_dense_req_grad ins_sc_req_grad ref_ins = ins_fn_req_grad reset_counter ref_out = fn ref_ins assert_counter expected_fw_count ref_out sum backward assert_counter expected_fw_count_after_bw expected_bw_count_after_bw compiled_fn = torch compile fn fullgraph=True ins = ins_fn_req_grad out = compiled_fn ins reset_counter out = compiled_fn ins assert_counter expected_fw_count assertEqual ref_out out out sum backward assert_counter expected_fw_count_after_bw expected_bw_count_after_bw assertEqual ref_ins grad ins grad assertEqual ref_ins grad ins grad fw_graph bw_graph = get_fw_bw_graph fn ins_sc_req_grad assertExpectedInline fw_graph code strip \ forward primals_ primals_ primals_ primals_ primals_ with_effects = torch ops higher_order with_effects primals_ torch ops _mylib zoo default primals_ primals_ = primals_ = None getitem = with_effects getitem_ = with_effects with_effects = None with_effects_ = torch ops higher_order with_effects getitem torch ops _mylib zoo default primals_ getitem = primals_ = None getitem_ = with_effects_ getitem_ = with_effects_ with_effects_ = None add = torch ops aten add Tensor getitem_ primals_ getitem_ = primals_ = None add_ = torch ops aten add Tensor getitem_ primals_ getitem_ = primals_ = None getitem_ add add_ assertExpectedInline bw_graph code strip \ forward tangents_ tangents_ tangents_token with_effects_ = torch ops higher_order with_effects tangents_token torch ops _mylib zoo default tangents_ tangents_token = None getitem_ = with_effects_ with_effects_ = None with_effects_ = torch ops higher_order with_effects getitem_ torch ops _mylib zoo default tangents_ getitem_ = None getitem_ = with_effects_ with_effects_ = None clone = torch ops aten clone default tangents_ clone_ = torch ops aten clone default tangents_ clone clone_ tangents_ tangents_ getitem_ test_effects_and_input_mutation_return fn b torch ops aten _print effect torch sin out=b inp = torch randn torch ones ref_out = fn inp out = torch compile fn fullgraph=True inp assertEqual ref_out out fw_graph bw_graph = get_fw_bw_graph fn inp assertExpectedInline fw_graph code strip \ forward arg _ arg _ arg _ with_effects = torch ops higher_order with_effects arg _ torch ops aten _print default effect arg _ = None getitem = with_effects with_effects = None sin = torch ops aten sin default arg _ arg _ = None getitem sin sin test_effects_and_input_output_view_simple fn view - inp = torch ones requires_grad=False add ref_out = fn inp out = torch compile fn fullgraph=True inp assertEqual ref_out out inp = torch ones requires_grad=True add ref_out = fn inp out = torch compile fn fullgraph=True inp assertEqual ref_out out fw_graph bw_graph = get_fw_bw_graph fn inp assertExpectedInline fw_graph code strip \ forward arg _ view = torch ops aten view default arg _ - arg _ = None view test_effects_and_aliased_outputs fn b = mul torch ops aten _print effect c = b view - b c f_compiled = aot_function fn nop req_grad True False inp = torch ones requires_grad=req_grad out_ref = fn inp out_test = f_compiled inp assertEqual out_ref out_test assertEqual out_ref out_test Try mutating one outputs which aliased out_ref mul_ out_test mul_ Assert aliasing relationship preserved assertEqual out_ref out_test assertEqual out_ref out_test test_effects_and_input_mutation_is_output fn mul_ torch ops aten _print effect inp = make_inputs_non_leaves torch ones requires_grad=True ref_out = fn inp out = torch compile fn backend= aot_eager fullgraph=True inp assertEqual ref_out out inp = torch ones requires_grad=False ref_out = fn inp out = torch compile fn backend= aot_eager fullgraph=True inp assertEqual ref_out out fw_graph bw_graph = get_fw_bw_graph fn inp assertExpectedInline fw_graph code strip \ forward arg _ arg _ mul = torch ops aten mul Tensor arg _ arg _ = None with_effects = torch ops higher_order with_effects arg _ torch ops aten _print default effect arg _ = None getitem = with_effects with_effects = None getitem mul mul skipIfTorchDynamo test_effectful_op_in_backward torch library _scoped_library _mylib FRAGMENT lib lib define foo Tensor x - Tensor foo_impl clone foo_bwd ctx grad torch ops _mylib foo grad backend CPU CUDA Meta lib impl foo foo_impl backend torch library register_autograd _mylib foo foo_bwd lib=lib torch _higher_order_ops effects _deregister_effectful_op _EffectType _register_effectful_op _register_effectful_op torch ops _mylib foo default _EffectType ORDERED try fn x y torch ops _mylib foo x + y ins_dense_req_grad torch tensor requires_grad=True torch tensor requires_grad=True ins_sc_req_grad TwoTensor torch tensor requires_grad=True torch tensor requires_grad=True torch tensor requires_grad=True i ins_fn enumerate ins_dense_req_grad ins_sc_req_grad ref_ins = ins_fn ref_out = fn ref_ins ref_out sum backward compiled_fn = torch compile fn backend= inductor fullgraph=True ins = ins_fn out = compiled_fn ins assertEqual ref_out out out sum backward assertEqual ref_ins grad ins grad assertEqual ref_ins grad ins grad fw_graph bw_graph = get_fw_bw_graph fn ins i == assertExpectedInline fw_graph code strip \ forward primals_ primals_ primals_ with_effects = torch ops higher_order with_effects primals_ torch ops _mylib foo default primals_ primals_ = primals_ = None getitem = with_effects getitem_ = with_effects with_effects = None add = torch ops aten add Tensor getitem_ primals_ getitem_ = primals_ = None getitem add assertExpectedInline bw_graph code strip \ forward tangents_ tangents_token with_effects_ = torch ops higher_order with_effects tangents_token torch ops _mylib foo default tangents_ tangents_token = None getitem_ = with_effects_ getitem_ = with_effects_ with_effects_ = None getitem_ tangents_ getitem_ i == assertExpectedInline fw_graph code strip \ forward primals_ primals_ primals_ primals_ with_effects = torch ops higher_order with_effects primals_ torch ops _mylib foo default primals_ primals_ = primals_ = None getitem = with_effects getitem_ = with_effects with_effects = None with_effects_ = torch ops higher_order with_effects getitem torch ops _mylib foo default primals_ getitem = primals_ = None getitem_ = with_effects_ getitem_ = with_effects_ with_effects_ = None add = torch ops aten add Tensor getitem_ primals_ getitem_ = None add_ = torch ops aten add Tensor getitem_ primals_ getitem_ = primals_ = None getitem_ add add_ assertExpectedInline bw_graph code strip \ forward tangents_ tangents_ tangents_token with_effects_ = torch ops higher_order with_effects tangents_token torch ops _mylib foo default tangents_ tangents_token = None getitem_ = with_effects_ getitem_ = with_effects_ with_effects_ = None with_effects_ = torch ops higher_order with_effects getitem_ torch ops _mylib foo default tangents_ getitem_ = None getitem_ = with_effects_ getitem_ = with_effects_ with_effects_ = None getitem_ getitem_ tangents_ tangents_ getitem_ raise NotImplementedError finally _deregister_effectful_op torch ops _mylib foo default skipIfNoDynamoSupport test_regular_effectful_op_only_in_backward torch _higher_order_ops effects _deregister_effectful_op _EffectType _register_effectful_op _register_effectful_op torch ops aten cos default _EffectType ORDERED try fn x x sin inps_fn torch tensor requires_grad=True torch compile fn backend= inductor fullgraph=True inps_fn fw_graph bw_graph = get_fw_bw_graph fn inps_fn assertExpectedInline fw_graph code strip \ forward primals_ sin = torch ops aten sin default primals_ sin primals_ assertExpectedInline bw_graph code strip \ forward primals_ tangents_ tangents_token with_effects = torch ops higher_order with_effects tangents_token torch ops aten cos default primals_ tangents_token = primals_ = None getitem = with_effects getitem_ = with_effects with_effects = None mul = torch ops aten mul Tensor tangents_ getitem_ tangents_ = getitem_ = None mul getitem inps_fn_sc TwoTensor torch tensor requires_grad=True torch tensor requires_grad=True torch compile fn backend= inductor fullgraph=True inps_fn_sc fw_graph bw_graph = get_fw_bw_graph fn inps_fn_sc assertExpectedInline fw_graph code strip \ forward primals_ primals_ sin = torch ops aten sin default primals_ sin_ = torch ops aten sin default primals_ sin sin_ primals_ primals_ assertExpectedInline bw_graph code strip \ forward primals_ primals_ tangents_ tangents_ tangents_token with_effects = torch ops higher_order with_effects tangents_token torch ops aten cos default primals_ tangents_token = primals_ = None getitem = with_effects getitem_ = with_effects with_effects = None with_effects_ = torch ops higher_order with_effects getitem torch ops aten cos default primals_ getitem = primals_ = None getitem_ = with_effects_ getitem_ = with_effects_ with_effects_ = None mul = torch ops aten mul Tensor tangents_ getitem_ tangents_ = getitem_ = None mul_ = torch ops aten mul Tensor tangents_ getitem_ tangents_ = getitem_ = None mul mul_ getitem_ finally _deregister_effectful_op torch ops aten cos default skipIfNoDynamoSupport test_regular_effectful_op_in_forward_and_backward torch _higher_order_ops effects _deregister_effectful_op _EffectType _register_effectful_op _register_effectful_op torch ops aten cos default _EffectType ORDERED try fn x x = x cos x sin inps = torch tensor requires_grad=True torch compile fn backend= inductor fullgraph=True inps fw_graph bw_graph = get_fw_bw_graph fn inps assertExpectedInline fw_graph code strip \ forward primals_ primals_ with_effects = torch ops higher_order with_effects primals_ torch ops aten cos default primals_ primals_ = None getitem = with_effects getitem_ = with_effects with_effects = None sin = torch ops aten sin default getitem_ getitem sin primals_ getitem_ assertExpectedInline bw_graph code strip \ forward primals_ getitem_ tangents_ tangents_token with_effects_ = torch ops higher_order with_effects tangents_token torch ops aten cos default getitem_ tangents_token = getitem_ = None getitem_ = with_effects_ getitem_ = with_effects_ with_effects_ = None mul = torch ops aten mul Tensor tangents_ getitem_ tangents_ = getitem_ = None sin_ = torch ops aten sin default primals_ primals_ = None neg = torch ops aten neg default sin_ sin_ = None mul_ = torch ops aten mul Tensor mul neg mul = neg = None mul_ getitem_ finally _deregister_effectful_op torch ops aten cos default __name__ == __main__ run_tests