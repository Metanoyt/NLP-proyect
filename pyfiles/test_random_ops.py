Copyright c Meta Platforms Inc affiliates Owner s oncall distributed itertools torch torch distributed _functional_collectives funcol torch distributed tensor _random random torch distributed device_mesh init_device_mesh torch distributed distributed_c d broadcast_object_list torch distributed fsdp fully_shard torch distributed tensor DeviceMesh distribute_tensor DTensor Replicate Shard torch distributed tensor _random is_rng_supported_mesh manual_seed OffsetBasedRNGTracker torch distributed tensor _utils compute_local_shape_and_global_offset torch distributed tensor debug CommDebugMode torch distributed tensor parallel ColwiseParallel parallelize_module torch testing _internal common_utils run_tests torch testing _internal distributed _tensor common_dtensor DTensorTestBase skip_if_lt_x_gpu skip_unless_torch_gpu with_comms torch utils _typing_utils not_none get_generator_seed_for_device_type device_type str - int device_module = torch get_device_module device_type device_module get_rng_state view torch int item DistTensorRandomInitTest DTensorTestBase _run_init_op init_op args kwargs device_mesh = build_device_mesh shard_spec = Shard input_size = NOTE currently random initialization gpu device has different behavior other devices Unify test once behavior unified is_rng_supported_mesh device_mesh input_tensor = torch randn input_size device=self device_type dtensor = DTensor from_local input_tensor device_mesh shard_spec local_tensor_clone = torch clone input_tensor torch manual_seed rank local_tensor_clone = init_op local_tensor_clone args kwargs torch manual_seed rank dtensor = init_op dtensor args kwargs assertEqual local_tensor_clone dtensor to_local create DTensor Tensor _tensor = torch empty input_size device=self device_type dtensor = distribute_tensor _tensor device_mesh Shard DTensor random init dtensor = init_op dtensor args kwargs local_tensor = dtensor to_local compare local tensors other ranks other_rank range world_size rank = other_rank slice_idx = slice input_size slice other_rank input_size other_rank + input_size other rank should have different local tensor assertNotEqual dtensor full_tensor slice_idx local_tensor with_comms test_init_ops _run_init_op torch nn init kaiming_uniform_ a= mode= fan_in nonlinearity= leaky_relu _run_init_op torch nn init normal_ mean= std= _run_init_op torch nn init uniform_ a= b= dtype torch float torch float _run_init_op torch rand_like dtype=dtype _run_init_op torch randn_like dtype=dtype _run_init_op torch randint_like low= high= dtype=dtype with_comms skip_if_lt_x_gpu test_init_with_user_generator device_mesh = build_device_mesh torch manual_seed rng = torch Generator device=self device_type manual_seed t = torch distributed tensor empty device_mesh=device_mesh placements= Shard t = torch distributed tensor empty device_mesh=device_mesh placements= Shard i range run second time make sure ` rng ` s offset-state advancing second usage torch nn init uniform_ t torch nn init uniform_ t rng assertEqual t full_tensor t full_tensor f Failed i= ensure we do cache seed first time we see DTensor behavior change DTensor used cache generator state modify original generator now modifies original generator instead torch manual_seed rng manual_seed torch nn init uniform_ t torch nn init uniform_ t rng assertEqual t full_tensor t full_tensor with_comms skip_if_lt_x_gpu test_meta_tensor_init test suite sets each rank s seed same value The DTensor random ops will use same generator default one device Note behavior changed now guideline set same RNG seed all SPMD ranks torch get_device_module device_type manual_seed device_mesh = DeviceMesh device_type torch arange world_size size = meta_dtensor = distribute_tensor torch empty size device= meta device_mesh Replicate tensor slice current rank self_slice = slice rank rank + Test enable distribute region RNG default assertTrue meta_dtensor is_meta Tensor meta init dtensor = torch empty_like meta_dtensor device=self device_type dtensor uniform_ check ` distribute_region_enabled ` set True default assertTrue random _rng_tracker distribute_region_enabled allgather local tensors gathered_local_tensors = funcol all_gather_tensor dtensor to_local gather_dim= group= device_mesh compare local tensors other ranks other_rank range world_size RNG result each rank same because they re replicated rank = other_rank other rank should have identical local tensor other_slice = slice other_rank other_rank + assertEqual gathered_local_tensors self_slice gathered_local_tensors other_slice Test disable distribute region RNG assertTrue meta_dtensor is_meta Tensor meta init dtensor = torch empty_like meta_dtensor device=self device_type random _rng_tracker distribute_region_enabled = False dtensor uniform_ check ` distribute_region_enabled ` set False assertTrue random _rng_tracker distribute_region_enabled allgather local tensors local_tensor = funcol all_gather_tensor dtensor to_local gather_dim= group= device_mesh compare local tensors other ranks other_rank range world_size RNG result each rank same even without help DTensor s RNG infra since default RNG same across ranks rank = other_rank other_slice = slice other_rank other_rank + assertEqual local_tensor self_slice local_tensor other_slice with_comms skip_unless_torch_gpu test_tp_model_meta_init initialize -d device mesh TP tp_mesh = init_device_mesh device_type mesh_shape= world_size model meta init torch device meta model = torch nn Linear world_size world_size bias=False assertEqual model weight device torch device meta parallelize_module model tp_mesh ColwiseParallel random _rng_tracker None random _rng_tracker distribute_region_enabled = True assertEqual model weight device torch device meta actual initialization device = torch device device_type torch get_device_module device_type current_device model to_empty device=device model reset_parameters assertTrue random _rng_tracker None isinstance random _rng_tracker OffsetBasedRNGTracker assertEqual model weight device device assert isinstance model weight DTensor gather all shards compare initialization results WORLD = torch distributed group WORLD assert WORLD None weight_local = model weight to_local weight_gather = funcol all_gather_tensor weight_local gather_dim= group=WORLD verify weights initialized differently all ranks other_rank range world_size rank = other_rank assertNotEqual weight_local weight_gather other_rank other_rank + with_comms skip_if_lt_x_gpu test_fsdp_tp_model_meta_init initialize -d device mesh global_mesh = init_device_mesh device_type mesh_shape= world_size mesh_dim_names= dp tp dp_mesh tp_mesh = global_mesh dp global_mesh tp model meta init torch device meta model = torch nn Linear world_size world_size bias=False assertEqual model weight device torch device meta parallelize_module model tp_mesh ColwiseParallel random _rng_tracker None random _rng_tracker distribute_region_enabled = True fully_shard model mesh=dp_mesh assertEqual model weight device torch device meta actual initialization device = torch device device_type torch get_device_module device_type current_device model to_empty device=device model reset_parameters assertTrue random _rng_tracker None isinstance random _rng_tracker OffsetBasedRNGTracker assertEqual model weight device device assert isinstance model weight DTensor gather all shards compare initialization results WORLD = torch distributed group WORLD assert WORLD None weight_local = model weight to_local weight_gather = funcol all_gather_tensor weight_local gather_dim= group=WORLD verify weights initialized differently all ranks other_rank range world_size rank = other_rank assertNotEqual weight_local weight_gather other_rank other_rank + DistTensorRandomOpTest DTensorTestBase with_comms skip_unless_torch_gpu test_rng_tracker_init torch manual_seed rank object_list = torch initial_seed broadcast_object_list object_list seed_from_rank_ = int object_list device_mesh = DeviceMesh device_type torch arange world_size seed synchronization now does NOT happen after first ` distribute_tensor ` call dt = distribute_tensor torch empty world_size device=self device_type device_mesh Shard assertTrue random _rng_tracker None seed synchronization only happens after ` manual_seed ` first DTensor random op call dt uniform_ We do maintain copy seed dtensor we do mutate global rng state since we now always pull fresh local device generator assertEqual seed_from_rank_ get_generator_seed_for_device_type device_type with_comms skip_unless_torch_gpu test_manual_seed device_mesh = DeviceMesh device_type torch arange world_size case calling ` ` torch distributed tensor _random manual_seed ` ` no seed synchronization should happen since we fully trust users input will override value comm_mode = CommDebugMode comm_mode Test set different seed different ranks RNG tracker should initialized until DTensor ` ` manual_seed ` ` called assertTrue random _rng_tracker None manual_seed rank device_mesh RNG tracker should already initialized assertTrue random _rng_tracker None assertEqual rank get_generator_seed_for_device_type device_type Test set same seed different ranks manual_seed device_mesh assertEqual get_generator_seed_for_device_type device_type assertEqual comm_mode get_total_counts with_comms skip_unless_torch_gpu test_manual_seed_submesh current rank part mesh single_rank_device_mesh = DeviceMesh device_type rank + world_size assertRaisesRegex RuntimeError manual_seed requires current rank part device mesh manual_seed rank single_rank_device_mesh with_comms skip_unless_torch_gpu test_pipeline_parallel_manual_seed This test verify ` manual_seed ` API works expected pipeline parallel setting world_mesh = init_device_mesh device_type world_size mesh_dim_names= pp spmd pp_mesh = world_mesh pp pp_rank = pp_mesh get_local_rank rank = rank = spmd_mesh = world_mesh spmd set seed each pipeline stage + pp_rank manual_seed + pp_rank spmd_mesh dtensor no longer stores copy seed mutates device s generator so we can check assertEqual + pp_rank get_generator_seed_for_device_type device_type mimic initializing model weight sharded SPMD mesh spmd_dtensor = torch distributed tensor ones spmd_mesh size device_mesh=spmd_mesh placements= Shard torch nn init normal_ spmd_dtensor gather all shards compare initialization results WORLD = torch distributed group WORLD assert WORLD None tensor_gather = funcol all_gather_tensor spmd_dtensor to_local gather_dim= group=WORLD verify weights initialized differently all ranks other_rank range world_size rank = other_rank assertNotEqual spmd_dtensor to_local tensor_gather other_rank other_rank + with_comms skip_unless_torch_gpu test_deterministic_dropout_ d test suite sets each rank s seed same value actual execution default random seed will different random value The DTensor random ops will use same random seed even though torch random generator keeps different seeds ranks torch manual_seed rank TODO add test before after enabling distribute region device_mesh = DeviceMesh device_type torch arange world_size size = dtensor = distribute_tensor torch empty size device=self device_type device_mesh Shard random op call shifts offset dtensor uniform_ dtensor now replicate all ranks dtensor = dtensor redistribute device_mesh Replicate dropout = torch nn Dropout p= dtensor = dropout dtensor allgather local tensors local_tensor = funcol all_gather_tensor dtensor to_local gather_dim= group= device_mesh compare local tensors other ranks self_slice = slice rank rank + other_rank range world_size rank = other_rank other rank should have identical local tensor other_slice = slice other_rank other_rank + assertEqual local_tensor self_slice local_tensor other_slice with_comms skip_unless_torch_gpu test_deterministic_rand_ d device_mesh = DeviceMesh device_type torch arange world_size size = world_size fn torch distributed tensor rand torch distributed tensor randn dtensor = fn size device_mesh=device_mesh placements= Shard local_tensor = funcol all_gather_tensor dtensor to_local gather_dim= group= device_mesh compare local tensors other ranks self_slice = slice rank rank + other_rank range world_size rank = other_rank other rank should have different local tensor shard placement other_slice = slice other_rank other_rank + assertNotEqual local_tensor self_slice local_tensor other_slice we should set manual seed same value all SPMD ranks torch manual_seed dtensor = fn size device_mesh=device_mesh placements= Replicate local_tensor = funcol all_gather_tensor dtensor to_local gather_dim= group= device_mesh compare local tensors other ranks self_slice = slice rank rank + other_rank range world_size rank = other_rank other rank should have identical local tensor replicate placement other_slice = slice other_rank other_rank + assertEqual local_tensor self_slice local_tensor other_slice with_comms skip_if_lt_x_gpu test_deterministic_uniform_ d mesh = torch arange world_size reshape device_mesh = DeviceMesh device_type mesh dtensor = distribute_tensor torch empty world_size _ mesh size device=self device_type device_mesh Replicate Replicate placements_list = list placements should enough cover Shard Shard Shard Shard Shard Replicate Replicate Shard Shard Replicate Replicate Shard Replicate Replicate shard_index_list = coordinate = device_mesh get_coordinate assert coordinate None placements shard_index zip placements_list shard_index_list dtensor = dtensor redistribute device_mesh placements random op call dtensor uniform_ check shard information correct shard_coord = coordinate mesh_dim mesh_dim = mesh_dim dtensor _spec dim_map shard_size = device_mesh size mesh_dim mesh_dim = mesh_dim dtensor _spec dim_map shard_linear_idx = random _rng_tracker _calc_shard_linear_idx shard_coord shard_size assertEqual shard_linear_idx shard_index rank compute local size offset _ local_shard_offset = compute_local_shape_and_global_offset dtensor shape device_mesh placements get local shard size local shard offset each shard local_shard_list_on_dim i has list all shards dim tuple local_shard_offset local_shard_size dtensor_shape = dtensor shape local_shard_list_on_dim list list tuple int int = l l dtensor_shape idx placement enumerate placements isinstance placement Shard mesh_dim_size = device_mesh size idx shard_dim = placement dim local_shard_list_on_dim shard_dim = shard_idx_on_dim range mesh_dim_size shard_size shard_offset = placement _local_shard_size_and_offset dtensor_shape shard_dim mesh_dim_size shard_idx_on_dim local_shard_list_on_dim shard_dim append not_none shard_offset shard_size local_shard_comb = itertools product local_shard_list_on_dim local shard local_tensor = dtensor to_local allgather local tensors full_tensor = dtensor full_tensor compare local tensor each other shard other_local_shard local_shard_comb other_local_shard_offset _ = zip other_local_shard slice_idx = slice offset offset + size offset size other_local_shard local_shard_offset == other_local_shard_offset assertEqual full_tensor tuple slice_idx local_tensor assertNotEqual full_tensor tuple slice_idx local_tensor DistTensorRandomOpsTest D DTensorTestBase property world_size skip_if_lt_x_gpu with_comms test_hsdp_tp_model_meta_init initialize -d device mesh global_mesh = init_device_mesh device_type mesh_shape= world_size mesh_dim_names= dp_replicate dp_shard tp tp_mesh = global_mesh tp dp_mesh = global_mesh dp_replicate dp_shard model meta init torch device meta model = torch nn Linear world_size world_size bias=False assertEqual model weight device torch device meta parallelize_module model tp_mesh ColwiseParallel random _rng_tracker None random _rng_tracker distribute_region_enabled = True fully_shard model mesh=dp_mesh assertEqual model weight device torch device meta actual initialization device = torch device device_type torch get_device_module device_type current_device model to_empty device=device model reset_parameters assertTrue random _rng_tracker None isinstance random _rng_tracker OffsetBasedRNGTracker assertEqual model weight device device assert isinstance model weight DTensor gather all shards compare initialization results WORLD = torch distributed group WORLD assert WORLD None weight_local = model weight to_local weight_gather = funcol all_gather_tensor weight_local gather_dim= group=WORLD verify weights initialized differently all ranks shard_dim_ _len = world_size other_rank range world_size other_rank_dim_ _start = other_rank shard_dim_ _len other_rank_dim_ _end = other_rank_dim_ _start + shard_dim_ _len rank = other_rank assertNotEqual weight_local weight_gather other_rank_dim_ _start other_rank_dim_ _end assertEqual weight_local weight_gather other_rank_dim_ _start other_rank_dim_ _end __name__ == __main__ run_tests