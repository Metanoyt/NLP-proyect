mypy allow-untyped-defs Copyright c Meta Platforms Inc affiliates All rights reserved This source code licensed under BSD-style license found LICENSE file root directory source tree typing Any torch torch distributed checkpoint metadata ChunkStorageMetadata MetadataIndex TensorProperties TensorStorageMetadata torch distributed checkpoint planner TensorWriteData WriteItem WriteItemType aten = torch ops aten LocalShardsWrapper torch Tensor A wrapper hold local shards DTensor This used largely checkpointing purposes implicitly subtypes _Checkpointable protocol __slots__ = _local_shards _storage_meta _local_shards list torch Tensor _storage_meta TensorStorageMetadata staticmethod __new__ cls local_shards list torch Tensor local_offsets list tuple int - LocalShardsWrapper assert all tensor device == local_shards device tensor local_shards empty shard we create empty tensor len local_shards == r = torch Tensor _make_wrapper_subclass cls torch Size r _local_shards = r _storage_meta = TensorStorageMetadata properties=TensorProperties size=torch Size chunks= ChunkStorageMetadata offsets=torch Size sizes=torch Size r we calculate total tensor size concat second tensor dimension cat_tensor_shape = list local_shards size len local_shards local_shards ndim == column-wise sharding shard local_shards cat_tensor_shape += shard size cases sharding optimizer rowwise we calculate total tensor size concat first tensor dimension len local_shards local_shards ndim == column-wise sharding shard local_shards cat_tensor_shape += shard size wrapper_properties = TensorProperties create_from_tensor local_shards wrapper_shape = torch Size cat_tensor_shape chunks_meta = ChunkStorageMetadata offsets=torch Size offset sizes=shard size shard offset zip local_shards local_offsets r = torch Tensor _make_wrapper_subclass cls torch Size cat_tensor_shape r _local_shards = local_shards r _storage_meta = TensorStorageMetadata properties=wrapper_properties size=wrapper_shape chunks=chunks_meta r necessary ops dispatching subclass its local shards classmethod __torch_dispatch__ cls func types args= kwargs=None type ignore override kwargs = kwargs dispatcher = torch ops _c d_functional all_gather_into_tensor default cls handle_all_gather_into_tensor torch ops _c d_functional wait_tensor default cls handle_wait_tensor aten _to_copy default cls handle_to_copy aten view default cls handle_view aten equal default cls handle_equal aten detach default cls handle_detach aten clone default cls handle_clone aten new_empty default cls handle_new_empty func dispatcher dispatcher func args kwargs raise NotImplementedError f func supported LocalShardsWrapper staticmethod handle_all_gather_into_tensor args kwargs - torch Tensor dim = args local_sizes cat_tensor = torch cat t view - t args local_shards dim= view - dim torch ops _c d_functional all_gather_into_tensor default cat_tensor args kwargs staticmethod handle_wait_tensor args kwargs - torch Tensor torch ops _c d_functional wait_tensor args staticmethod handle_to_copy args kwargs - torch Tensor res_shards_list = aten _to_copy default shard args kwargs shard args local_shards LocalShardsWrapper res_shards_list args local_offsets staticmethod handle_view args kwargs - LocalShardsWrapper view_shape = args res_shards_list = len args local_shards args local_shards ndim == assert args storage_metadata size == view_shape args storage_metadata size == view_shape This accounts DTensor quirk when multiple shards present rank DTensor init calls view_as global tensor shape will fail because view shape applicable individual shards res_shards_list = aten view default shard shard shape kwargs shard args local_shards args local_shards ndim == assert args storage_metadata size == view_shape This case optimizer sharding regardless sharding type optimizer state row wise sharded res_shards_list = aten view default shard shard shape kwargs shard args local_shards raise NotImplementedError No support view tensors ndim view called per shard res_shards_list = aten view default shard args kwargs shard args local_shards LocalShardsWrapper res_shards_list args local_offsets staticmethod handle_equal args kwargs - bool LocalShardsWrapper equal impl also checks equality storage metadata order shards b = args args len local_shards = len b local_shards False all aten equal default x y x y zip local_shards b local_shards False storage_metadata = b storage_metadata False True staticmethod handle_detach args kwargs - LocalShardsWrapper self_ls = args deatched_local_shards = aten detach default shard shard self_ls local_shards self_ls _local_shards = deatched_local_shards self_ls _storage_meta properties requires_grad = False self_ls staticmethod handle_clone args kwargs - LocalShardsWrapper self_ls = args desired_memory_format = kwargs get memory_format None desired_memory_format desired_memory_format = torch preserve_format raise NotImplementedError f desired_memory_format supported LocalShardsWrapper cloned_local_shards = shard clone memory_format=desired_memory_format shard self_ls _local_shards LocalShardsWrapper cloned_local_shards self_ls local_offsets staticmethod handle_new_empty args kwargs - LocalShardsWrapper self_ls = args LocalShardsWrapper torch empty_like shard shard self_ls _local_shards self_ls local_offsets property device - torch _C device type ignore override _local_shards device _local_shards torch device meta property is_meta - bool type ignore override _local_shards is_meta _local_shards True is_pinned - bool type ignore override _storage_meta properties pin_memory requires_grad_ requires_grad bool = True - LocalShardsWrapper _storage_meta properties requires_grad = requires_grad shard requires_grad_ requires_grad shard _local_shards local_shards - list torch Tensor Returns list ` torch Tensor corresponding local shards rank Returns empty list current rank does host any shards Tensor _local_shards local_sizes - list torch Size Returns list ` torch Size corresponding local sizes shards rank Returns empty list current rank does host any shards Tensor chunk sizes chunk _storage_meta chunks local_offsets - list torch Size Returns list ` torch Size corresponding local offsets shards rank Returns empty list current rank does host any shards Tensor chunk offsets chunk _storage_meta chunks property local_chunks - list ChunkStorageMetadata Returns ` list ChunkStorageMetadata ` object corresponding metadata each tensor shard _storage_meta chunks storage_metadata - TensorStorageMetadata Returns ` TensorStorageMetadata ` object corresponding metadata local tensor current rank _storage_meta is_empty_shard - bool Returns ` bool ` object indicating local tensor current rank empty tensor _storage_meta size == _storage_meta size == __create_write_items__ fqn str object Any - list WriteItem For compatibility DCP we support creation WriteItems such they can saved properly WriteItem index=MetadataIndex fqn chunks offsets type=WriteItemType SHARD tensor_data=TensorWriteData chunk=ChunkStorageMetadata offsets=chunks offsets sizes=chunks sizes properties=self _storage_meta properties size=object size tensor chunks zip local_shards local_chunks __create_chunk_list__ - list ChunkStorageMetadata For compatibility DCP we support creation chunk lists such they can saved properly _storage_meta chunks __get_tensor_shard__ index MetadataIndex - torch Tensor For compatibility DCP we support finding shard based index Return torch Tensor shard based MetadataIndex Fast lookup path index index None len _local_shards index index _storage_meta chunks index index offsets == index offset _local_shards index index index offset None shard chunk zip _local_shards _storage_meta chunks chunk offsets == index offset shard Empty shard case len _local_shards == _storage_meta chunks sizes == torch Size torch empty raise ValueError f Could find shard index offset FQN index fqn _get_tensor_size_bytes - int object_size = shard local_shards object_size += shard nelement shard element_size object_size __hash__ - int id __repr__ - str type ignore override f LocalShardsWrapper _local_shards _storage_meta __str__ - str f LocalShardsWrapper _local_shards _storage_meta