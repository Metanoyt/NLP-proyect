mypy allow-untyped-defs atexit functools logging os shutil sys time dataclasses dataclass pathlib Path typing Any Optional typing_extensions TypeIs sympy torch torch _inductor runtime runtime_utils dynamo_timed torch _inductor utils clear_on_fresh_cache torch utils _ordered_set OrderedSet config ir Layout runtime runtime_utils cache_dir virtualized V cpp_utils DTYPE_TO_CPP cuda_env get_cuda_arch get_cuda_version log = logging getLogger __name__ CUTLASS_OPERATION_KIND str = gemm ACCUMULATOR_DTYPES OrderedSet torch dtype = OrderedSet torch float torch int XW_DTYPES OrderedSet torch dtype = OrderedSet torch half torch bfloat torch float _e m fn torch int atexit register move_cutlass_compiled_cache - None Move CUTLASS compiled cache file cache directory exists try_import_cutlass cache_info currsize cutlass_cppgen type ignore import-not-found Check CACHE_FILE attribute exists cutlass_cppgen file exists hasattr cutlass_cppgen CACHE_FILE os path exists cutlass_cppgen CACHE_FILE try filename = os path basename cutlass_cppgen CACHE_FILE shutil move cutlass_cppgen CACHE_FILE os path join cache_dir filename log debug Moved CUTLASS compiled cache file s cache_dir except OSError log warning Failed move CUTLASS compiled cache file exc_info=True _rename_cutlass_import content str cutlass_modules list str - str cutlass_module cutlass_modules content = content replace f cutlass_module f cutlass_library cutlass_module content functools cache try_import_cutlass - bool We want support three ways passing CUTLASS fbcode handled internal build system User specifies cutlass_dir The default third_party cutlass which directory when developers build source config is_fbcode try cutlass_cppgen type ignore import-not-found noqa F cutlass_library type ignore import-not-found except ImportError e log warning noqa G Failed CUTLASS packages fbcode s ignoring CUTLASS backend str e False True Copy CUTLASS python scripts temp dir add temp dir Python search path This temporary hack avoid CUTLASS module naming conflicts TODO ipiszy remove hack when CUTLASS solves Python scripts packaging structure issues TODO mlazos epilogue visitor tree currently lives python cutlass will moved python cutlass_library future later path_join path path os path abspath os path join path path contains both cutlass cutlass_library we need cutlass eVT cutlass_python_path = path_join config cuda cutlass_dir python torch_root = os path abspath os path dirname torch __file__ mock_src_path = os path join torch_root _inductor codegen cuda cutlass_lib_extensions cutlass_mock_imports cutlass_library_src_path = path_join cutlass_python_path cutlass_library cutlass_cppgen_src_path = path_join cutlass_python_path cutlass_cppgen pycute_src_path = path_join cutlass_python_path pycute tmp_cutlass_full_path = os path abspath os path join cache_dir torch_cutlass dst_link_library = path_join tmp_cutlass_full_path cutlass_library dst_link_cutlass_cppgen = path_join tmp_cutlass_full_path cutlass_cppgen dst_link_pycute = path_join tmp_cutlass_full_path pycute mock modules cutlass mock_modules = cuda scipy pydot os path isdir cutlass_python_path tmp_cutlass_full_path sys path link_and_append dst_link src_path parent_dir os path lexists dst_link assert os path islink dst_link f dst_link symlink Try remove dst_link manually try again assert os path realpath os readlink dst_link == os path realpath src_path f Symlink dst_link does point src_path os makedirs parent_dir exist_ok=True os symlink src_path dst_link parent_dir sys path sys path append parent_dir link_and_append dst_link_library cutlass_library_src_path tmp_cutlass_full_path link_and_append dst_link_cutlass_cppgen cutlass_cppgen_src_path tmp_cutlass_full_path link_and_append dst_link_pycute pycute_src_path tmp_cutlass_full_path module mock_modules link_and_append path_join tmp_cutlass_full_path module dst_link path_join mock_src_path module src_path tmp_cutlass_full_path parent try cutlass_cppgen type ignore import-not-found noqa F F cutlass_library generator noqa F cutlass_library library noqa F cutlass_library manifest noqa F pycute type ignore import-not-found noqa F True except ImportError e log debug noqa G Failed CUTLASS packages s ignoring CUTLASS backend str e log debug Failed CUTLASS packages CUTLASS repo does exist s cutlass_python_path False functools lru_cache _normalize_cuda_arch arch str - str int arch = log warning Detected CUDA architecture = s We will generate operations GenerateSM available GenerateSM Please file issue any problems feedback arch int arch = int arch = int arch = int arch = int arch = raise NotImplementedError f Unsupported cuda arch arch dataclass CUTLASSArgs CUTLASS args used initialize CUTLASS Manifest architectures Optional str = None cuda_version Optional str = None instantiation_level Optional str = None operations Optional str = None build_dir = curr_build_dir = generator_target = kernels = all ignore_kernels = exclude_kernels = TODO these three look dead kernel_filter_file None = None selected_kernel_list None = None interface_dir None = None filter_by_cc = True disable_full_archs_compilation = False __post_init__ architectures None cuda_version None raise RuntimeError f architectures= cuda_version= None architectures = _normalize_cuda_arch architectures clear_on_fresh_cache functools cache _gen_ops_cached arch version - dict Any Any Note Cache needs specific cuda architecture version Import cutlass python scripts assert try_import_cutlass cutlass_library generator cutlass_generator cutlass_library manifest cutlass_manifest arch None version None log error Cannot detect cuda arch s cuda version s Will discard all cutlass ops Please consider setting _inductor cuda arch _inductor cuda version configs arch version arch = _normalize_cuda_arch arch instantiation_level str = config cuda cutlass_instantiation_level args = CUTLASSArgs architectures=arch cuda_version=version instantiation_level=instantiation_level operations=CUTLASS_OPERATION_KIND manifest = cutlass_manifest Manifest args start_time = time time arch == hasattr cutlass_generator GenerateSM cutlass_generator GenerateSM manifest args cuda_version cutlass_generator GenerateSM manifest args cuda_version try func = getattr cutlass_generator GenerateSM + arch func manifest args cuda_version except AttributeError e raise NotImplementedError Arch + arch + supported current cutlass lib e log info CUTLASS library generated dict d operation kinds f seconds len manifest operations time time - start_time manifest operations gen_ops - dict Any Any Generates all supported CUTLASS operations dynamo_timed cutlass_utils gen_ops arch = get_cuda_arch version = get_cuda_version _gen_ops_cached arch version DTYPE_TO_CUTLASS_TYPE = DTYPE_TO_CPP torch float __half torch bfloat __nv_bfloat torch float _e m fn __nv_fp _e m functools lru_cache torch_dtype_to_cutlass_type torch_dtype torch dtype - cutlass_library library DataType type ignore name-defined noqa F Import cutlass python scripts assert try_import_cutlass cutlass_library type ignore torch_dtype == torch float cutlass_library library DataType f torch_dtype == torch half cutlass_library library DataType f torch_dtype == torch bfloat cutlass_library library DataType bf raise NotImplementedError f Unsupported data type torch_dtype= functools lru_cache dtype_match torch_dtype Optional torch dtype cutlass_dtype cutlass_library library DataType type ignore name-defined noqa F - bool Import cutlass python scripts assert try_import_cutlass cutlass_library torch_dtype == torch float cutlass_dtype == cutlass_library library DataType f cutlass_dtype == cutlass_library library DataType tf torch_dtype == torch half cutlass_dtype == cutlass_library library DataType f torch_dtype == torch bfloat cutlass_dtype == cutlass_library library DataType bf torch_dtype == torch int cutlass_dtype == cutlass_library library DataType s torch_dtype == torch uint cutlass_dtype == cutlass_library library DataType u torch_dtype == torch int cutlass_dtype == cutlass_library library DataType s torch_dtype == torch float _e m fn cutlass_dtype == cutlass_library library DataType e m False get_accumulator_dtype input_torch_dtypes list torch dtype - Optional torch dtype Given pair input torch dtypes returns inferred accumulator torch dtype assert OrderedSet input_torch_dtypes = XW_DTYPES f input_torch_dtypes= supported len input_torch_dtypes = None torch_dtype = None input_torch_dtypes == input_torch_dtypes torch_dtype = input_torch_dtypes size = torch tensor dtype=input_torch_dtypes element_size size = torch tensor dtype=input_torch_dtypes element_size size size dtype dtype = input_torch_dtypes dtype dtype = input_torch_dtypes dtype torch half torch bfloat dtype torch int torch uint torch_dtype = dtype torch_dtype torch float torch bfloat torch float torch float _e m fn accumulator_dtype = torch float torch_dtype == torch int accumulator_dtype = torch int raise NotImplementedError f Unsupported data types input_torch_dtypes= assert accumulator_dtype ACCUMULATOR_DTYPES f accumulator_dtype= supported accumulator_dtype functools lru_cache get_alignments torch_dtype torch dtype - list int Returns all possible valid CUTLASS alignments terms number elements given dtype CUTLASS gemm conv SM APIs support bytes max alignment bytes min alignment torch_dtype torch half torch bfloat torch_dtype == torch float torch_dtype torch uint torch int torch float _e m fn torch_dtype == torch int raise NotImplementedError f unsupported torch_dtype= alignments get_max_alignment inductor_layout Layout - int Returns max alignment terms number elements given Inductor Layout dtype = inductor_layout dtype size = inductor_layout size offset = inductor_layout offset is_static_int number object - TypeIs int &#124; sympy Integer isinstance number int &#124; sympy Integer a_factor_of x alignment is_static_int x is_static_int alignment x alignment == rem = sympy Mod x alignment V graph sizevars evaluate_expr sympy Eq rem try contiguous_dim = inductor_layout stride index except ValueError No dim stride found alignments = get_alignments dtype alignment alignments a_factor_of size contiguous_dim alignment a_factor_of offset alignment continue all dim == contiguous_dim a_factor_of inductor_layout stride dim alignment dim range len size alignment CUDACompileSourceCapturingContext Helper Benchmarking Testing CUTLASS Kernels isolation Can used capture sourcecode passed CUDACodeCache compile __init__ sources = _compile_patch = None __enter__ args kwargs unittest mock mock torch _inductor codecache _compile_method_orig = torch _inductor codecache CUDACodeCache compile my_compile source_code dst_file_ext extra_args Optional list str = None sources append source_code _compile_method_orig source_code dst_file_ext pyrefly ignore bad-assignment _compile_patch = mock patch torch _inductor codecache CUDACodeCache compile my_compile _compile_patch __enter__ args kwargs type ignore union-attr __exit__ args kwargs _compile_patch __exit__ args kwargs type ignore union-attr cuda_standalone_runner_compile_command srcpath Path exepath Path returns command string compile captured CUDA GEMM Kernel source standalone executable s ready run Passes correct preprocessor define nvcc ensure standalone runner enabled torch _inductor codecache cuda_compile_command extra_args = -DGENERATE_STANDALONE_RUNNER= -DCUTLASS_DEBUG_TRACE_LEVEL= compile_command = cuda_compile_command str srcpath str exepath exe extra_args=extra_args compile_command