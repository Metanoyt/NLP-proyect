mypy allow-untyped-defs contextlib typing Any Optional Union typing_extensions deprecated torch torch Tensor torch nn utils _named_member_accessor NamedMemberAccessor __all__ = functional_call _untie_named_tensors_map module torch nn Module parameters_and_buffers dict str Tensor - dict str Tensor Unties all tied tensors module parameters_and_buffers This function returns new untied_parameters_and_buffers dictionary leave original untied_parameters_and_buffers dictionary unchanged It adds new missing keys tied tensors module untied_parameters_and_buffers The value new key user-given value original parameters_and_buffers dictionary If there more than one user-given values same tied tensor will raise error For example module has two tied weights foo tied_foo user passes foo foo_value will foo foo_value tied_foo foo_value If user passes foo foo_value tied_foo tied_foo_value will raise error If user passes foo foo_value tied_foo foo_value will raise error Args module torch nn Module module determine which tensors tied parameters_and_buffers Dict str Tensor map name tensor reparamaterizing module Returns A new untied version parameters_and_buffers dictionary Raises ValueError there more than one user-given values same tied tensor A map name tensor all tensors including tied ones module all_named_tensors dict str Tensor = all_named_tensors update module named_parameters remove_duplicate=False all_named_tensors update module named_buffers remove_duplicate=False A map tensor set all_tied_names all tensor names module tensor_to_tied_names_map dict Tensor set str = name tensor all_named_tensors items tensor tensor_to_tied_names_map tensor_to_tied_names_map tensor = set tensor_to_tied_names_map tensor add name A map tied_name set all_tied_names all tensor names module If name tied will map tied_names_map dict str set str = tied_names tensor_to_tied_names_map values len tied_names tied_name tied_names tied_names_map tied_name = tied_names Make sure user didn t pass multiple values same tied tensor given_names = set parameters_and_buffers keys same given_names intersection tied_names_map keys dynamo can t handle given_names_for_tied_tensors set str = set name given_names name tied_names_map given_names_for_tied_tensors add name given_name given_names_for_tied_tensors tied_names = tied_names_map given_name Detect there multiple keys present same tied tensor len tied_names intersection given_names_for_tied_tensors Only raise error user passed multiple values same tied tensor If all given values same don t raise len parameters_and_buffers tied_name tied_name tied_names = raise ValueError f functional_call got multiple values keys sorted tied_names f which tied Consider using tie_weights=False Untie given named tensor map Make copy modifying original dict untied_parameters_and_buffers = parameters_and_buffers copy given_name given_names_for_tied_tensors tied_name tied_names_map given_name untied_parameters_and_buffers tied_name = parameters_and_buffers given_name untied_parameters_and_buffers contextlib contextmanager _reparametrize_module module torch nn Module parameters_and_buffers dict str Tensor tie_weights bool = False strict bool = False stack_weights bool = False tie_weights untied_parameters_and_buffers = _untie_named_tensors_map module parameters_and_buffers untied_parameters_and_buffers = parameters_and_buffers accessor = NamedMemberAccessor module strict missing_keys unexpected_keys = accessor check_keys untied_parameters_and_buffers error_msgs = len unexpected_keys error_msgs append f Unexpected key s join map repr unexpected_keys len missing_keys error_msgs append f Missing key s join map repr missing_keys len error_msgs raise RuntimeError Error s reparametrizing \n\t format module _get_name \n\t join error_msgs orig_parameters_and_buffers dict str Tensor = try orig_parameters_and_buffers _ = accessor swap_tensors_dict untied_parameters_and_buffers allow_missing=True yield finally stack_weights When stacking enabled we will restore weights LIFO order orig_parameters_and_buffers = dict reversed orig_parameters_and_buffers items new_parameters_and_buffers _ = accessor swap_tensors_dict orig_parameters_and_buffers allow_missing=True Sometimes module completely stateless has some in-place modifications _parameters _buffers dictionaries Write changed parameters buffers back original dict parameters_and_buffers update k new_parameters_and_buffers k k parameters_and_buffers k new_parameters_and_buffers deprecated ` torch nn utils stateless functional_call ` deprecated PyTorch will removed future version PyTorch Please use ` torch func functional_call ` instead which drop-in replacement category=FutureWarning functional_call module torch nn Module parameters_and_buffers dict str Tensor args Optional Union Any tuple = None kwargs Optional dict str Any = None tie_weights bool = True strict bool = False r Perform functional call module replacing module parameters buffers provided ones warning This API deprecated PyTorch will removed future version PyTorch Please use func ` torch func functional_call ` instead which drop-in replacement API note If module has active parametrizations passing value attr ` parameters_and_buffers ` argument name set regular parameter name will completely disable parametrization If you want apply parametrization function value passed please set key ` ` submodule_name parametrizations parameter_name original ` ` note If module performs in-place operations parameters buffers these will reflected ` parameters_and_buffers ` input Example = foo torch zeros xdoctest +SKIP mod = Foo does foo = foo + print mod foo tensor functional_call mod torch ones print mod foo tensor print foo tensor note If module has tied weights whether functional_call respects tying determined tie_weights flag Example = foo torch zeros xdoctest +SKIP mod = Foo has both foo foo_tied which tied Returns x + foo + foo_tied print mod foo tensor mod torch zeros tensor functional_call mod torch zeros tensor since will change foo_tied too functional_call mod torch zeros tie_weights=False tensor -- foo_tied updated new_a = foo torch zeros foo_tied torch zeros functional_call mod new_a torch zeros tensor Args module torch nn Module module call parameters_and_buffers dict str Tensor parameters will used module call args Any tuple arguments passed module call If tuple considered single argument kwargs dict keyword arguments passed module call tie_weights bool optional If True then parameters buffers tied original model will treated tied reparamaterized version Therefore True different values passed tied parameters buffers will error If False will respect originally tied parameters buffers unless values passed both weights same Default True strict bool optional If True then parameters buffers passed must match parameters buffers original module Therefore True there any missing unexpected keys will error Default False Returns Any result calling ` ` module ` ` _functional_call module parameters_and_buffers args kwargs tie_weights=tie_weights strict=strict _functional_call module torch nn Module parameters_and_buffers dict str Tensor args Optional Union Any tuple = None kwargs Optional dict str Any = None tie_weights bool = True strict bool = False TODO allow kwargs such unsafe others parametrization torch jit is_tracing torch jit is_scripting isinstance module torch jit RecursiveScriptModule torch jit ScriptModule torch jit ScriptFunction raise RuntimeError The stateless API can t used Jitted modules isinstance module torch nn DataParallel raise RuntimeError The stateless API can t used nn DataParallel module kwargs None kwargs = args None args = isinstance args tuple args = args _reparametrize_module module parameters_and_buffers tie_weights=tie_weights strict=strict module args kwargs