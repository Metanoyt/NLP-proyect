Owner s oncall profiler functools gc itertools textwrap unittest collections abc Callable Iterator typing Optional torch torch _C _profiler _EventType _TensorMetadata torch profiler _memory_profiler _utils torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_utils ALLOW_XPU_PROFILING_TEST DEVICE_LIST_SUPPORT_PROFILING_TEST run_tests skipIfTorchDynamo TestCase torch utils _pytree pytree profile = functools partial torch profiler profile record_shapes=True profile_memory=True with_stack=True skipIfTorchDynamo TorchDynamo removes profiler altogether TestMemoryProfiler TestCase test_config_check - None torch profiler profile prof pass pattern = r record_shapes=True profile_memory=True with_stack=True assertRaisesRegex ValueError pattern prof _memory_profile torch profiler profile record_shapes=True with_stack=True prof pass pattern = r ^profile_memory=True required memory profiling\ $ assertRaisesRegex ValueError pattern prof _memory_profile profile prof pass assertIsInstance prof _memory_profile _memory_profiler MemoryProfile ScaleLayer torch nn Module __init__ - None super __init__ scale = torch nn Parameter torch rand requires_grad=True forward x torch Tensor - torch Tensor x scale LazyLinear torch nn Module __init__ in_features int out_features int super __init__ in_features = in_features out_features = out_features forward x - torch Tensor getattr weight None None weight = torch nn Parameter torch empty out_features in_features bias = torch nn Parameter torch empty out_features torch nn functional linear x weight bias RecordInputOutputDispatchMode torch utils _python_dispatch TorchDispatchMode __init__ - None results = mark_region name str results append name staticmethod flat_ids args flat_args = pytree tree_leaves args tuple t _cdata t storage data_ptr t flat_args isinstance t torch Tensor t storage __torch_dispatch__ func types args= kwargs=None args = args kwargs = kwargs flat_inputs = flat_ids args + flat_ids kwargs out = func args kwargs flat_outputs = flat_ids out flat_inputs flat_outputs _record_function_enter func name results append func name flat_inputs flat_outputs out skipIfTorchDynamo TorchDynamo changes Python calls memory profiling relies TestIdentifyGradients TestCase gradient_detected prof torch profiler profile ctx _EventType grad_tensor torch Tensor parameter Optional torch Tensor = None - None This exhaustive check purpose unit testing sufficient key_matches_tensor key tensor - bool Vacuous case tensor None True key None False tensor storage data_ptr == key storage ptr tree = prof profiler kineto_results experimental_event_tree node _utils traverse_dfs tree p_key p_grad_key _memory_profiler extract_gradients node node tag == ctx key_matches_tensor p_grad_key grad_tensor parameter None True Don t need check parameter we re done p_key None For complex workflow gradient could correspond different parameters different points trace However will happen relatively simple cases tested here so ` extract_gradients ` identifies parameter corresponding particular gradient must one we expect assertTrue key_matches_tensor p_key parameter True False assertGradientDetected name str args kwargs - None assertTrue gradient_detected args kwargs f Failed identify gradient ` name ` profile assertOnlyGradients prof torch profiler profile tensors Iterator torch Tensor - None allowed_set = t storage data_ptr t tensors tree = prof profiler kineto_results experimental_event_tree node _utils traverse_dfs tree _ p_grad_key _memory_profiler extract_gradients node assertTrue p_grad_key storage ptr allowed_set f Tensor wrongly marked gradient node name p_grad_key test_extract_gradients_low_level - None x = torch ones w = torch ones requires_grad=True w = torch ones requires_grad=True check cold_start bool assertEqual w grad None cold_start assertEqual w grad None cold_start profile prof z = x expand w z w sum backward Gradient detection through op inspection does provide reference parameter corresponding gradient assertGradientDetected w prof _EventType TorchOp w grad assertGradientDetected w prof _EventType TorchOp w grad assertOnlyGradients prof w grad w grad check cold_start=True check cold_start=False test_extract_gradients_from_module - None model = torch nn Sequential torch nn Linear ScaleLayer named_parameters = dict model named_parameters assertEqual len named_parameters assert_only_gradients prof torch profiler profile gradients = tuple i grad i named_parameters values assertFalse any i None i gradients assertOnlyGradients prof gradients check cold_start bool x = torch ones profile prof model x sum backward name p named_parameters items The first time we run module none ` grad ` fields have been initialized This fine case we can detect everything we need profiled section assertNotEqual gradient_detected prof _EventType PyCall p grad p cold_start name Op based detection should still identify gradients assertGradientDetected name prof _EventType TorchOp p grad assert_only_gradients prof We can detect gradients even when ` backward ` called profile prof model torch ones name p named_parameters items assertGradientDetected name prof _EventType PyCall p grad p assertFalse gradient_detected prof _EventType TorchOp p grad name assert_only_gradients prof check cold_start=True check cold_start=False _test_extract_gradients_from_optimizer set_to_none bool - None x = torch ones w = torch ones requires_grad=True w = torch ones requires_grad=True optimizer = torch optim SGD w w lr= momentum= check cold_start bool assertEqual w grad None cold_start assertEqual w grad None cold_start profile prof optimizer zero_grad set_to_none=set_to_none z = x expand w z w sum backward optimizer step Optimizer instrumentation runs late step so we can detect gradients both cold warm start assertGradientDetected w prof _EventType PyCall w grad w assertGradientDetected w prof _EventType PyCall w grad w assertGradientDetected w prof _EventType TorchOp w grad assertGradientDetected w prof _EventType TorchOp w grad assertOnlyGradients prof w grad w grad profile prof _ range optimizer zero_grad set_to_none=set_to_none z = x expand w z w sum backward optimizer step Inspected state cached so we replace gradients case ` set_to_none=True ` our python instrumentation will see them TODO robieta Should ` step ` excluded caching assertNotEqual gradient_detected prof _EventType PyCall w grad w set_to_none assertNotEqual gradient_detected prof _EventType PyCall w grad w set_to_none set_to_none assertRaisesRegex AssertionError Tensor wrongly marked assertOnlyGradients prof w grad w grad check cold_start=True check cold_start=False test_extract_gradients_from_optimizer - None _test_extract_gradients_from_optimizer set_to_none=False test_extract_gradients_from_optimizer_set_to_none - None _test_extract_gradients_from_optimizer set_to_none=True test_extract_gradients_from_module_and_optimizer - None Module optimizer thoroughly tested individually should additive Thus we can manage lightweight check they don t interact adversely model = torch nn Sequential torch nn Linear ScaleLayer optimizer = torch optim SGD model parameters lr= momentum= profile prof model torch ones sum backward optimizer step assertGradientDetected weight prof _EventType PyCall model weight grad model weight skipIfTorchDynamo TorchDynamo removes profiler altogether TestDataFlow TestCase setUp - None super setUp maxDiff = None staticmethod formatSchemas prof torch profiler profile indent int = - tuple tuple str tuple bool tree = prof profiler kineto_results experimental_event_tree out list tuple str tuple bool = node _utils traverse_dfs tree node tag == _EventType TorchOp e = node extra_fields schemas = _memory_profiler SchemaMatcher match_schemas e name = node name len schemas == name = f name schemas overload_name len schemas name = f name join s overload_name s schemas out append name _memory_profiler SchemaMatcher inputs_are_mutable e tuple out staticmethod _run_and_format_data_flow inputs dict str torch Tensor f Callable Optional dict str torch Tensor indent int = - str profile prof outputs = f inputs gc collect memory_profile = prof _memory_profile graph = memory_profile _data_flow_graph storage_to_id = key storage ptr key id key graph _active_version lines list str = name t chain inputs items outputs items lines append f name + T storage_to_id t storage data_ptr t grad None grad_id = storage_to_id t grad storage data_ptr lines append f name + grad T grad_id lines lines append node graph flow_nodes destroyed = k k v node _edges items v is_deletion inputs list str = key _ v node inputs items inputs append f T key id v v key destroyed outputs = f T key id v v key v node outputs items inputs outputs event_name = node _event name replace torch autograd lines append f event_name join inputs - join outputs textwrap indent \n join l rstrip l lines indent test_match_schemas - None profile prof x = torch ones mul add_ _ = torch sin x out=torch empty_like x assertEqual formatSchemas prof aten ones False aten empty memory_format False fill_ Scalar Tensor Scalar value - Tensor aten fill_ Scalar True False aten mul Tensor False False aten dtype False aten _to_copy False aten empty_strided False copy_ Tensor Tensor src bool non_blocking=False - Tensor aten copy_ True False False add_ Tensor Tensor Tensor other Scalar alpha= - Tensor aten add_ Tensor True False False aten dtype False aten _to_copy False aten empty_strided False copy_ Tensor Tensor src bool non_blocking=False - Tensor aten copy_ True False False aten empty_like False aten empty_strided False sin out Tensor Tensor out - Tensor aten sin out False True test_match_schemas_backward - None x = torch ones w = torch ones requires_grad=True profile prof torch mul x w backward assertEqual formatSchemas prof aten mul Tensor False False aten ones_like False aten empty_like False aten empty_strided False fill_ Scalar Tensor Scalar value - Tensor aten fill_ Scalar True False autograd engine evaluate_function MulBackward MulBackward None aten mul Tensor False False autograd engine evaluate_function torch autograd AccumulateGrad torch autograd AccumulateGrad None aten detach False detach None test_match_schemas_tensorlist - None x = torch ones y = torch ones profile prof torch cat x y axis= assertEqual formatSchemas prof aten cat False False test_data_flow_graph_with_annotations - None f x y torch _C _jit_get_schemas_for_operator will reject any name missing namespace denoted presence We want check we skip both annotations which have no schema empty tuple SchemaMatcher lookup_schemas annotations which cannot have schema None SchemaMatcher lookup_schemas torch profiler record_function Namespaced Annotation torch profiler record_function My Annotation x zero_ y zero_ x torch ones_like x y torch zeros_like y inputs = x torch ones y torch ones assertExpectedInline _run_and_format_data_flow inputs f \ x T y T x T y T aten zero_ T v - T v aten zero_ T v - T v aten ones_like T v - T v aten zeros_like T v - T v test_data_flow_graph_non_op_allocations - None f x x mul The python arg parser will convert python scalar ` ` Tensor pass ` aten mul ` As result there no op owns allocation The Tensor deletions also do happen op they collected result Python objects going out scope assertExpectedInline _run_and_format_data_flow x torch ones f \ x T memory - T v aten mul T v T v - memory T v - test_data_flow_graph_simple - None inputs = x torch ones y torch ones requires_grad=True f x y z = x mul y z z view_as z f x y noqa F torch no_grad f x y assertExpectedInline _run_and_format_data_flow inputs f \ x T y T z T aten mul T v T v - T v aten view_as T v - Out place identical regardless Autograd assertExpectedInline _run_and_format_data_flow inputs f \ x T y T z T aten mul T v T v - T v aten view_as T v - test_data_flow_graph_simple_inplace - None inputs = x torch ones y torch ones requires_grad=True f x y x mul_ y f x y torch no_grad f x y When Autograd enabled second Tensor ` T ` created store values T v which needed backwards assertExpectedInline _run_and_format_data_flow inputs f \ x T y T aten mul_ T v T v - T v T v assertExpectedInline _run_and_format_data_flow inputs f \ x T y T aten mul_ T v T v - T v test_data_flow_graph_simple_backward - None inputs = x torch ones w torch ones requires_grad=True assertExpectedInline _run_and_format_data_flow inputs lambda x w x w sin backward \ x T w T w grad T aten mul T v T v - T v aten sin T v - T v aten ones_like T v - T v SinBackward T v T v - T v memory T v - MulBackward T v T v - T v memory T v - AccumulateGrad T v - memory T v - memory T v - test_data_flow_graph_complicated - None f x = torch ones y = x mul add_ z = torch sin y out=torch empty_like y x x y y z z T ` ` ` mul ` The Python arg parser automatically converts Scalar arguments Tensors The same true ` T ` ` add_ ` assertExpectedInline _run_and_format_data_flow f \ x T y T z T aten ones - T v memory - T v aten mul T v T v - T v memory T v - memory - T v aten add_ T v T v - T v memory T v - aten empty_like T v - T v aten sin T v T v - T v profile prof f ` aten mul ` creates temporary Tensor T which why output has ID three rather than two mul_node = prof _memory_profile _data_flow_graph flow_nodes assertEqual mul_node _event name aten mul assertEqual len mul_node intermediates assertEqual mul_node intermediates id test_data_flow_graph_stacked - None inputs = x torch ones w torch ones requires_grad=True w torch ones requires_grad=True f x w w x mul w relu mul w relu sum f_fwd kwargs torch no_grad loss f kwargs f_fwd_bwd kwargs loss = f kwargs loss backward loss loss assertExpectedInline _run_and_format_data_flow inputs f_fwd \ x T w T w T loss T aten mul T v T v - T v aten relu T v - T v memory T v - aten mul T v T v - T v memory T v - aten relu T v - T v memory T v - aten sum T v - T v memory T v - assertExpectedInline _run_and_format_data_flow inputs f_fwd_bwd \ x T w T w grad T w T w grad T loss T aten mul T v T v - T v aten relu T v - T v memory T v - aten mul T v T v - T v aten relu T v - T v memory T v - aten sum T v - T v aten ones_like T v - T v SumBackward T v - ReluBackward T v T v - T v memory T v - MulBackward T v T v T v - T v T v aten sum T v - T v memory T v - memory T v - AccumulateGrad T v - ReluBackward T v T v - T v memory T v - memory T v - MulBackward T v T v - T v aten sum T v - T v memory T v - memory T v - AccumulateGrad T v - memory T v - Second time grads already initialized assertExpectedInline _run_and_format_data_flow inputs f_fwd_bwd \ x T w T w grad T w T w grad T loss T aten mul T v T v - T v aten relu T v - T v memory T v - aten mul T v T v - T v aten relu T v - T v memory T v - aten sum T v - T v aten ones_like T v - T v SumBackward T v - ReluBackward T v T v - T v memory T v - MulBackward T v T v T v - T v T v aten sum T v - T v memory T v - memory T v - AccumulateGrad T v T v - T v ReluBackward T v T v - T v memory T v - memory T v - MulBackward T v T v - T v aten sum T v - T v memory T v - memory T v - AccumulateGrad T v T v - T v memory T v - x = torch ones w = torch ones requires_grad=True w = torch ones requires_grad=True profile prof_no_grad torch no_grad x mul w relu mul w relu sum TODO one ` logsumexp dim= ` assertExpectedInline _format_graph prof_no_grad \ aten mul T v T v - T v aten relu T v - T v memory T v - aten mul T v T v - T v memory T v - aten relu T v - T v memory T v - aten sum T v - T v memory T v - memory T v - profile prof_grad loss = x mul w relu mul w relu sum loss backward assertExpectedInline _format_graph prof_grad \ aten mul T v T v - T v aten relu T v - T v memory T v - aten mul T v T v - T v aten relu T v - T v memory T v - aten sum T v - T v aten ones_like T v - T v SumBackward T v - T v ReluBackward T v T v - T v T v memory T v - MulBackward T v T v T v - T v T v T v aten sum T v - T v memory T v - memory T v - AccumulateGrad T v - T v ReluBackward T v T v - T v T v memory T v - memory T v - MulBackward T v T v - T v T v aten sum T v - T v memory T v - memory T v - AccumulateGrad T v - T v memory T v - Second time grads already initialized profile prof_grad loss = x mul w relu mul w relu sum loss backward assertExpectedInline _format_graph prof_grad \ aten mul T v T v - T v aten relu T v - T v memory T v - aten mul T v T v - T v aten relu T v - T v memory T v - aten sum T v - T v aten ones_like T v - T v SumBackward T v - T v ReluBackward T v T v - T v T v memory T v - MulBackward T v T v T v - T v T v T v aten sum T v - T v memory T v - memory T v - AccumulateGrad T v T v - T v ReluBackward T v T v - T v T v memory T v - memory T v - MulBackward T v T v - T v T v aten sum T v - T v memory T v - memory T v - AccumulateGrad T v T v - T v memory T v - skipIfTorchDynamo TorchDynamo changes Python calls memory profiling relies TestMemoryProfilerE E TestCase staticmethod _lookup_tensor_categories t torch Tensor memory_profile _memory_profiler MemoryProfile - dict _memory_profiler TensorAndID Optional _memory_profiler Category storage = t storage storage None raise ValueError Cannot look up uninitialized Tensor snapshot = memory_profile _category_snapshot ids = key storage allocation_id key _ snapshot key storage ptr == storage data_ptr key device == storage device key version category key version category memory_profile _category_snapshot items If Tensor live we want most recent ID key storage allocation_id == max ids &#124; - _run_and_check_parameters_and_gradients inner_fn model grads_none bool = False profile prof inner_fn memory_profile = prof _memory_profile assert_category t torch Tensor category _memory_profiler Category should_be_none bool = False should_be_none assert t None tensor should None assertIsNotNone t categories = _lookup_tensor_categories t memory_profile assertGreater len categories assertTrue all c == category c categories values categories p model parameters assert_category p _memory_profiler Category PARAMETER assert_category p grad _memory_profiler Category GRADIENT grads_none Rely internal asserts _ = memory_profile timeline _run_and_format_categories fn indent= Generate summary assigned categories expecttest Use ` __torch_dispatch__ ` collect ground truth RecordInputOutputDispatchMode record_ops profile prof fn lambda name record_ops mark_region f -- name ljust - memory_profile = prof _memory_profile ptr_pair_to_key dict tuple int int _memory_profiler TensorKey = snapshot = memory_profile _category_snapshot Build map observed live Tensors memory profiler s TensorKey representation op memory_profile _op_tree dfs op typed == _EventType TorchOp inputs = pytree tree_leaves op typed inputs t i i inputs isinstance i _TensorMetadata key = _memory_profiler TensorKey from_tensor t key ptr_pair_to_key t impl_ptr t storage_data_ptr = key format_categories ptr_pair int target_key = ptr_pair_to_key get ptr_pair target_key None matches = tuple version category name category key version category snapshot items key == target_key assert matches Failed lookup Tensor Deduplicate version bumps which don t change category categories = matches _ category matches category = categories - categories append category f target_key storage allocation_id join categories out list str = name inputs outputs record_ops results inputs outputs PyTorch ops inputs_str = join format_categories i i inputs outputs_str = join format_categories i i outputs out append f name inputs_str - outputs_str Marked regions out append f \n name textwrap indent \n join out indent test_parameters_and_gradients model = torch nn Sequential torch nn Linear ScaleLayer torch nn Linear ScaleLayer optimizer = torch optim SGD model parameters lr= fwd_only _ = model torch ones fwd_bwd_step optimizer zero_grad y = model torch ones torch nn functional mse_loss y torch rand backward optimizer step If we profile first step then gradients will have been created when we call ` model forward ` so we don t call ` backward ` then gradients never created _run_and_check_parameters_and_gradients inner_fn=fwd_only model=model grads_none=True On first step we must rely ` AccumulateGrad ` since gradients did exist when ` model forward ` called assertTrue all p grad None p model parameters _run_and_check_parameters_and_gradients inner_fn=fwd_bwd_step model=model After one step python tracer will also flag gradients assertTrue any p grad None p model parameters _run_and_check_parameters_and_gradients inner_fn=fwd_bwd_step model=model The parameter gradients used we still detect them python tracer _run_and_check_parameters_and_gradients inner_fn=fwd_only model=model test_parameters_and_gradients_set_to_none model = torch nn Sequential torch nn Linear torch nn Linear optimizer = torch optim SGD model parameters lr= fwd_bwd_step _ range zero grads start so gradients still live checked optimizer zero_grad set_to_none=True y = model torch ones torch nn functional mse_loss y torch rand backward optimizer step fwd_bwd_step assertTrue any p grad None p model parameters _run_and_check_parameters_and_gradients inner_fn=fwd_bwd_step model=model optimizer zero_grad set_to_none=True assertTrue all p grad None p model parameters _run_and_check_parameters_and_gradients inner_fn=fwd_bwd_step model=model test_inputs_fwd model = torch nn Sequential torch nn Linear torch nn Linear inputs = torch ones _ range profile prof Inputs which allocated before profiling began x inputs _ = model x Inputs which allocated after profiling began _ range x = torch ones inputs append x _ = model x memory_profile = prof _memory_profile x inputs categories = _lookup_tensor_categories x memory_profile assertGreater len categories assertTrue all i == _memory_profiler Category INPUT i categories values categories snapshot = memory_profile _category_snapshot assertTrue _memory_profiler Category INPUT snapshot values test_inputs_fwd_lazy model = torch nn Sequential LazyLinear LazyLinear inputs = torch ones _ range profile prof Inputs which allocated before profiling began x inputs _ = model x Inputs which allocated after profiling began _ range x = torch ones inputs append x _ = model x For now we can t make any meaningful statements without backward pass Here we simply ensure passes don t generate false positive category classifications memory_profile = prof _memory_profile x inputs categories = _lookup_tensor_categories x memory_profile assertGreater len categories assertTrue all i None i categories values categories snapshot = memory_profile _category_snapshot assertFalse _memory_profiler Category INPUT snapshot values test_inputs_fwd_bwd model = torch nn Sequential torch nn Linear torch nn Linear optimizer = torch optim SGD model parameters lr= inputs_targets = torch ones torch rand _ range fwd_bwd_step x targets y = model x torch nn functional mse_loss y targets backward optimizer step optimizer zero_grad profile prof Inputs which allocated before profiling began x targets inputs_targets fwd_bwd_step x targets Inputs which allocated after profiling began _ range x = torch ones targets = torch rand inputs_targets append x targets fwd_bwd_step x targets memory_profile = prof _memory_profile check t categories = _lookup_tensor_categories t memory_profile assertGreater len categories assertTrue all i == _memory_profiler Category INPUT i categories values x targets inputs_targets check x check targets test_lazily_initialized - None model = torch nn Sequential torch nn Linear torch nn ReLU LazyLinear torch nn ReLU torch nn Linear assertEqual len list model parameters inner_fn y = model torch ones optimizer = torch optim SGD model parameters lr= optimizer zero_grad torch nn functional mse_loss y torch rand backward optimizer step _run_and_check_parameters_and_gradients inner_fn=inner_fn model=model assertEqual len list model parameters test_manual_optimizer_step - None model = torch nn Sequential torch nn Linear torch nn Linear inner_fn y = model torch ones torch nn functional mse_loss y torch rand backward torch no_grad p model parameters grad = p grad assertIsNotNone grad p add_ grad alpha=- _run_and_check_parameters_and_gradients inner_fn=inner_fn model=model test_categories_e e_simple_fwd - None w = torch ones requires_grad=True w = torch ones requires_grad=True step_fn _ x = torch ones noqa F y = torch cat x w x w dim= noqa F NOTE We expect all unknown categories This simply sanity check ensure we do over-label assertExpectedInline _run_and_format_categories step_fn \ aten ones - aten mul Tensor - aten mul Tensor - aten cat - test_categories_e e_simple_fwd_bwd - None w = torch ones requires_grad=True w = torch ones requires_grad=True step_fn mark_region x = torch ones targets = torch ones mark_region Forward loss y = torch cat x w x w dim= loss = torch nn functional binary_cross_entropy_with_logits y targets mark_region Backward loss backward assertExpectedInline _run_and_format_categories step_fn \ aten ones - INPUT aten ones - INPUT -- Forward loss --------------------------------------------------------------------------------------- aten mul Tensor INPUT INPUT - INPUT aten mul Tensor INPUT INPUT - INPUT aten cat INPUT INPUT - INPUT aten binary_cross_entropy_with_logits INPUT INPUT - INPUT -- Backward --------------------------------------------------------------------------------------------- aten ones_like INPUT - INPUT aten sigmoid INPUT - TEMPORARY aten sub Tensor TEMPORARY INPUT - TEMPORARY aten mul Tensor TEMPORARY INPUT - AUTOGRAD_DETAIL aten div_ Scalar AUTOGRAD_DETAIL - AUTOGRAD_DETAIL aten slice Tensor AUTOGRAD_DETAIL - AUTOGRAD_DETAIL aten slice Tensor AUTOGRAD_DETAIL - AUTOGRAD_DETAIL aten mul Tensor AUTOGRAD_DETAIL INPUT - AUTOGRAD_DETAIL aten sum dim_IntList AUTOGRAD_DETAIL - GRADIENT aten view GRADIENT - GRADIENT aten detach GRADIENT - aten mul Tensor AUTOGRAD_DETAIL INPUT - AUTOGRAD_DETAIL aten sum dim_IntList AUTOGRAD_DETAIL - GRADIENT aten view GRADIENT - GRADIENT aten detach GRADIENT - test_categories_e e_simple_fwd_bwd_step - None w = torch ones requires_grad=True w = torch ones requires_grad=True optimizer = torch optim SGD w w lr= step_fn mark_region x = torch ones targets = torch ones mark_region Forward loss y = torch cat x w x w dim= loss = torch nn functional binary_cross_entropy_with_logits y targets mark_region Backward loss backward mark_region Optimizer optimizer step optimizer zero_grad assertExpectedInline _run_and_format_categories step_fn \ aten ones - INPUT aten ones - INPUT -- Forward loss --------------------------------------------------------------------------------------- aten mul Tensor INPUT PARAMETER - ACTIVATION aten mul Tensor INPUT PARAMETER - ACTIVATION aten cat ACTIVATION ACTIVATION - ACTIVATION aten binary_cross_entropy_with_logits ACTIVATION INPUT - ACTIVATION -- Backward --------------------------------------------------------------------------------------------- aten ones_like ACTIVATION - ACTIVATION aten sigmoid ACTIVATION - TEMPORARY aten sub Tensor TEMPORARY INPUT - TEMPORARY aten mul Tensor TEMPORARY ACTIVATION - AUTOGRAD_DETAIL aten div_ Scalar AUTOGRAD_DETAIL - AUTOGRAD_DETAIL aten slice Tensor AUTOGRAD_DETAIL - AUTOGRAD_DETAIL aten slice Tensor AUTOGRAD_DETAIL - AUTOGRAD_DETAIL aten mul Tensor AUTOGRAD_DETAIL INPUT - AUTOGRAD_DETAIL aten sum dim_IntList AUTOGRAD_DETAIL - GRADIENT aten view GRADIENT - GRADIENT aten detach GRADIENT - GRADIENT aten mul Tensor AUTOGRAD_DETAIL INPUT - AUTOGRAD_DETAIL aten sum dim_IntList AUTOGRAD_DETAIL - GRADIENT aten view GRADIENT - GRADIENT aten detach GRADIENT - GRADIENT -- Optimizer -------------------------------------------------------------------------------------------- aten add_ Tensor PARAMETER GRADIENT - PARAMETER aten add_ Tensor PARAMETER GRADIENT - PARAMETER test_categories_e e_simple_module_fwd - None model = torch nn Linear bias=True assertExpectedInline _run_and_format_categories lambda _ model torch ones \ aten ones - INPUT aten t PARAMETER - PARAMETER aten addmm PARAMETER INPUT PARAMETER - ACTIVATION test_categories_e e_simple_module_fwd_bwd - None model = torch nn Linear bias=True step_fn mark_region mark_region Forward loss loss = model torch ones sum mark_region Backward loss backward assertExpectedInline _run_and_format_categories step_fn \ -- Forward loss --------------------------------------------------------------------------------------- aten ones - INPUT aten t PARAMETER - PARAMETER aten addmm PARAMETER INPUT PARAMETER - ACTIVATION aten sum ACTIVATION - ACTIVATION -- Backward --------------------------------------------------------------------------------------------- aten ones_like ACTIVATION - ACTIVATION aten expand ACTIVATION - ACTIVATION aten t ACTIVATION - ACTIVATION aten mm ACTIVATION INPUT - GRADIENT aten t GRADIENT - GRADIENT aten sum dim_IntList ACTIVATION - GRADIENT aten view GRADIENT - GRADIENT aten detach GRADIENT - aten t GRADIENT - GRADIENT aten detach GRADIENT - test_categories_e e_simple_module_fwd_bwd_step - None model = torch nn Linear bias=True optimizer = torch optim SGD model parameters lr= momentum= step_fn mark_region mark_region Forward loss loss = model torch ones sum mark_region Backward loss backward mark_region Optimizer optimizer step optimizer zero_grad assertExpectedInline _run_and_format_categories step_fn \ -- Forward loss --------------------------------------------------------------------------------------- aten ones - INPUT aten t PARAMETER - PARAMETER aten addmm PARAMETER INPUT PARAMETER - ACTIVATION aten sum ACTIVATION - ACTIVATION -- Backward --------------------------------------------------------------------------------------------- aten ones_like ACTIVATION - ACTIVATION aten expand ACTIVATION - ACTIVATION aten t ACTIVATION - ACTIVATION aten mm ACTIVATION INPUT - GRADIENT aten t GRADIENT - GRADIENT aten sum dim_IntList ACTIVATION - GRADIENT aten view GRADIENT - GRADIENT aten detach GRADIENT - GRADIENT aten t GRADIENT - GRADIENT aten detach GRADIENT - GRADIENT -- Optimizer -------------------------------------------------------------------------------------------- aten detach GRADIENT - GRADIENT aten clone GRADIENT - OPTIMIZER_STATE aten add_ Tensor PARAMETER OPTIMIZER_STATE - PARAMETER aten detach GRADIENT - GRADIENT aten clone GRADIENT - OPTIMIZER_STATE aten add_ Tensor PARAMETER OPTIMIZER_STATE - PARAMETER test_categories_e e_sequential_fwd - None model = torch nn Sequential torch nn Linear bias=True torch nn ReLU torch nn Linear bias=False torch nn Softmax dim= assertExpectedInline _run_and_format_categories lambda _ model torch ones \ aten ones - INPUT aten t PARAMETER - PARAMETER aten addmm PARAMETER INPUT PARAMETER - ACTIVATION aten relu ACTIVATION - ACTIVATION aten detach ACTIVATION - aten t PARAMETER - PARAMETER aten mm ACTIVATION PARAMETER - ACTIVATION aten _softmax ACTIVATION - ACTIVATION aten detach ACTIVATION - test_categories_e e_sequential_fwd_bwd - None model = torch nn Sequential torch nn Linear bias=True torch nn ReLU torch nn Linear bias=False torch nn Softmax dim= step_fn mark_region x = torch ones targets = torch ones mark_region Forward y = model x mark_region Loss loss = torch sum y - targets mean mark_region Backward loss backward assertExpectedInline _run_and_format_categories step_fn \ aten ones - INPUT aten ones - INPUT -- Forward ---------------------------------------------------------------------------------------------- aten t PARAMETER - PARAMETER aten addmm PARAMETER INPUT PARAMETER - ACTIVATION aten relu ACTIVATION - ACTIVATION aten detach ACTIVATION - ACTIVATION aten t PARAMETER - PARAMETER aten mm ACTIVATION PARAMETER - ACTIVATION aten _softmax ACTIVATION - ACTIVATION aten detach ACTIVATION - ACTIVATION -- Loss ------------------------------------------------------------------------------------------------- aten sub Tensor ACTIVATION INPUT - ACTIVATION aten pow Tensor_Scalar ACTIVATION - ACTIVATION aten sum ACTIVATION - ACTIVATION aten mean ACTIVATION - ACTIVATION -- Backward --------------------------------------------------------------------------------------------- aten ones_like ACTIVATION - ACTIVATION aten expand ACTIVATION - ACTIVATION aten div Scalar ACTIVATION - AUTOGRAD_DETAIL aten expand AUTOGRAD_DETAIL - AUTOGRAD_DETAIL aten pow Tensor_Scalar ACTIVATION - TEMPORARY aten mul Scalar TEMPORARY - TEMPORARY aten mul Tensor AUTOGRAD_DETAIL TEMPORARY - AUTOGRAD_DETAIL aten detach ACTIVATION - ACTIVATION aten _softmax_backward_data AUTOGRAD_DETAIL ACTIVATION - AUTOGRAD_DETAIL aten t AUTOGRAD_DETAIL - AUTOGRAD_DETAIL aten mm AUTOGRAD_DETAIL ACTIVATION - GRADIENT aten t GRADIENT - GRADIENT aten t PARAMETER - PARAMETER aten mm AUTOGRAD_DETAIL PARAMETER - AUTOGRAD_DETAIL aten t GRADIENT - GRADIENT aten detach GRADIENT - aten detach ACTIVATION - ACTIVATION aten threshold_backward AUTOGRAD_DETAIL ACTIVATION - AUTOGRAD_DETAIL aten t AUTOGRAD_DETAIL - AUTOGRAD_DETAIL aten mm AUTOGRAD_DETAIL INPUT - GRADIENT aten t GRADIENT - GRADIENT aten sum dim_IntList AUTOGRAD_DETAIL - GRADIENT aten view GRADIENT - GRADIENT aten detach GRADIENT - aten t GRADIENT - GRADIENT aten detach GRADIENT - test_memory_timeline - None model = torch nn Sequential torch nn Linear bias=True torch nn ReLU torch nn Linear bias=False torch nn Softmax dim= optimizer = torch optim Adam model parameters lr= profile prof x = torch ones targets = torch ones y = model x loss = torch nn functional mse_loss y targets loss backward optimizer step optimizer zero_grad memory_profile = prof _memory_profile timeline = memory_profile timeline times = tuple t t _ _ _ timeline assertTrue all t = t t t pairwise times times assertTrue all t == - action == _memory_profiler Action PREEXISTING t t action _ _ timeline category_name category category name category format_action action key version category = memory_profile _categories get key version action == _memory_profiler Action INCREMENT_VERSION new_category = memory_profile _categories get key version + category = new_category f category_name category - category_name new_category category_name category format_size size int size f size f kB f size kB We generate sequential IDs Tensors however platforms vary slightly exact computation executed If results tensor creation IDs will shifted unit test will fail Even though behavior we re testing unchanged To correct we assign sequential numbers tensors which actually tested effectively suppressing extraneous implementation details id_map = id_for_testing key id_map setdefault key storage allocation_id len id_map lines = f action name lower format_action action key version f id_for_testing key v version format_size size _ action key version size prof _memory_profile timeline We generally don t care about tiny allocations during memory profiling they add lot noise unit test size isinstance key _memory_profiler TensorKey assertExpectedInline textwrap indent \n join lines \ preexisting PARAMETER v kB preexisting PARAMETER v kB preexisting PARAMETER v kB create INPUT v kB create INPUT v kB create ACTIVATION v kB create ACTIVATION v kB destroy ACTIVATION v kB create ACTIVATION v kB create ACTIVATION v kB destroy ACTIVATION v kB create ACTIVATION v kB create TEMPORARY v kB destroy TEMPORARY v kB create AUTOGRAD_DETAIL v kB create AUTOGRAD_DETAIL v kB destroy AUTOGRAD_DETAIL v kB create GRADIENT v kB create AUTOGRAD_DETAIL v kB destroy AUTOGRAD_DETAIL v kB create AUTOGRAD_DETAIL v kB destroy AUTOGRAD_DETAIL v kB destroy ACTIVATION v kB create GRADIENT v kB create GRADIENT v kB destroy AUTOGRAD_DETAIL v kB create OPTIMIZER_STATE v kB create OPTIMIZER_STATE v kB create OPTIMIZER_STATE v kB create OPTIMIZER_STATE v kB create OPTIMIZER_STATE v kB create OPTIMIZER_STATE v kB increment_version OPTIMIZER_STATE v kB increment_version OPTIMIZER_STATE v kB increment_version OPTIMIZER_STATE v kB create v kB create v kB destroy v kB increment_version v kB increment_version PARAMETER v kB increment_version OPTIMIZER_STATE v kB increment_version OPTIMIZER_STATE v kB increment_version OPTIMIZER_STATE v kB create v kB create v kB destroy v kB increment_version v kB destroy v kB increment_version PARAMETER v kB increment_version OPTIMIZER_STATE v kB increment_version OPTIMIZER_STATE v kB increment_version OPTIMIZER_STATE v kB create v kB create v kB destroy v kB increment_version v kB destroy v kB increment_version PARAMETER v kB destroy v kB destroy GRADIENT v kB destroy GRADIENT v kB destroy GRADIENT v kB skipIfTorchDynamo TorchDynamo changes Python calls memory profiling relies TestMemoryProfilerTimeline TestCase unittest skipIf torch xpu is_available The XPU Profiler will cover case now Will support next period test_memory_timeline_no_id device - None On CPU default behavior simply forward malloc That means when we free ` x ` allocator doesn t actually know how many bytes allocation thus there s no point calling ` c reportMemoryUsageToProfiler ` So order test memory profiler processes case correctly we need use device where we do always keep record x = torch ones device=device profile prof We never see ` x ` used so we don t know storage Tensor we do still see free event del x For empty we see allocation free any use So also cannot identified Tensor y = torch empty del y z = torch empty z view_as z Show ` z ` profiler del z memory_profile = prof _memory_profile expected = x _memory_profiler Action PREEXISTING _memory_profiler Action DESTROY y _memory_profiler Action CREATE _memory_profiler Action DESTROY z _memory_profiler Action CREATE _memory_profiler Action DESTROY actual = action size _ action _ size memory_profile timeline See above device == cpu expected = expected event expected assertTrue event actual f event event found actual assertEqual actual expected f expected does match actual actual instantiate_device_type_tests TestMemoryProfilerTimeline globals only_for=DEVICE_LIST_SUPPORT_PROFILING_TEST allow_xpu=ALLOW_XPU_PROFILING_TEST __name__ == __main__ run_tests