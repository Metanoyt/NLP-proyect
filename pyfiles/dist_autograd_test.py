mypy allow-untyped-defs torch torch distributed autograd dist_autograd torch distributed rpc rpc torch Tensor torch distributed rpc rpc_async torch testing FileCheck torch testing _internal dist_utils dist_init worker_name torch testing _internal distributed rpc rpc_agent_test_fixture RpcAgentTestFixture torch jit script local_add t t torch add t t torch jit script remote_add t t dst str noqa E rpc_async dst local_add t t wait torch jit script fork_add t t dst str fut = torch jit _fork remote_add t t dst torch jit _wait fut JitDistAutogradTest RpcAgentTestFixture dist_init test_get_gradients torch jit script dist_get_gradients context_id int - dict Tensor Tensor dist_autograd get_gradients context_id FileCheck check get_gradients run str dist_get_gradients graph dist_autograd context context_id t = torch rand requires_grad=True t = torch rand requires_grad=True t = torch add t t dist_autograd backward context_id t sum grads = dist_get_gradients context_id assertEqual len grads assertIn t grads assertIn t grads assertEqual torch ones grads t assertEqual torch ones grads t dist_init test_dist_backward rank = torch jit script dist_backward_script context_id int loss torch Tensor dist_autograd backward context_id loss FileCheck check dist_backward run str dist_backward_script graph dist_autograd context context_id t = torch rand requires_grad=True t = torch rand requires_grad=True dst_worker_name = worker_name rank + world_size loss = rpc rpc_sync dst_worker_name torch add args= t t sum dist_backward_script context_id loss dist_init test_jit_fork_within_context dist_autograd context context_id t = torch rand requires_grad=True t = torch rand requires_grad=True dst_worker_name = worker_name rank + world_size res = fork_add t t dst_worker_name loss = res sum dist_autograd backward context_id loss grads = dist_autograd get_gradients context_id assertEqual len grads assertIn t grads assertIn t grads dist_init test_restore_context_after_swtich_to_jit_thread rank = torch jit script forward_script context_id int dst_worker_name str t Tensor t Tensor - tuple Tensor Tensor res _fut = rpc rpc_async dst_worker_name local_add t t res = res _fut wait After script runs new JIT thread loss = res sum SendRpcBackward attached since DistAutogradContext lost here res _fut = rpc rpc_async dst_worker_name local_add t t res = res _fut wait loss = res sum loss loss dist_autograd context context_id t = torch ones requires_grad=True t = torch ones requires_grad=True dst_worker_name = worker_name rank + world_size loss loss = forward_script context_id dst_worker_name t t dist_autograd backward context_id loss loss grad grad = dist_autograd get_gradients context_id assertEqual grad grad