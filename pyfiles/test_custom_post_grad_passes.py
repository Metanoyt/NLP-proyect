Owner s module inductor contextlib operator collections defaultdict torch torch _inductor pattern_matcher pattern_matcher torch fx fx torch _dynamo utils counters torch _inductor config torch _inductor codegen common get_custom_backend_pass_for_device torch _inductor custom_graph_pass CustomGraphModulePass CustomGraphPass get_hash_for_files torch _inductor lowering lowerings L torch _inductor pattern_matcher Arg CallFunction PatternMatcherPass torch _inductor test_case run_tests TestCase torch testing _internal common_utils IS_LINUX torch testing _internal inductor_utils HAS_CPU patch_inductor_backend config patch freezing True TestCustomPassBase TestCase _clone_inputs inputs clone x isinstance x torch Tensor x x clone tuple clone x x inputs _test_common mod inputs matcher_count matcher_nodes atol= e- rtol= e- counters clear maybe_autocast = contextlib nullcontext torch no_grad maybe_autocast clone_inputs = _clone_inputs inputs expected = mod inputs actual = torch compile mod clone_inputs torch testing assert_close actual expected atol=atol rtol=rtol assertEqual counters inductor pattern_matcher_count matcher_count assertEqual counters inductor pattern_matcher_nodes matcher_nodes aten = torch ops aten mkldnn = torch ops mkldnn change_cos_pass graph node graph nodes node op == call_function node target == aten cos default node target = aten sin default ChangeCosCustomPass CustomGraphPass __init__ - None super __init__ __call__ g torch fx graph Graph change_cos_pass g uuid - bytes get_hash_for_files __file__ TestPostGradCustomPrePostPass TestCustomPassBase mkldnn fusion s pattern_matcher torch _inductor fx_passes mkldnn_fusion py apply custom post_grad_passes _register_mkldnn_conv_relu_fusion custom_pass_dict pattern _mkldnn_conv_relu_pattern CallFunction aten relu CallFunction mkldnn _convolution_pointwise default Arg Arg Arg Arg Arg Arg Arg Arg Arg Arg _users= utils pattern matcher registration _register_fusion_lowering pattern custom_pass_dict dummy_check m True register_custom_lowering_pattern pattern extra_check custom_pass_dict pattern_matcher register_lowering_pattern pattern extra_check pass_dict=custom_pass_dict register_custom_lowering_pattern pattern dummy_check custom_pass_dict fn match args kwargs computation_args = list args - + relu L mkldnn _convolution_pointwise default computation_args fn _register_fusion_lowering _mkldnn_conv_relu_pattern custom_pass_dict custom post grad pass _CustomPass PatternMatcherPass CustomGraphPass __init__ - None super __init__ __call__ g torch fx graph Graph apply g uuid - bytes get_hash_for_files __file__ case model _ConvReLU torch nn Module __init__ ic oc super __init__ conv = torch nn Conv d ic oc kernel_size= stride= padding= forward x x = conv x x relu test_custom_joint_pass_pre config patch joint_custom_pre_pass=ChangeCosCustomPass g x x sin sin sin f x x cos cos cos x = torch randn dtype=torch float torch testing assert_close torch compile f x g x test_custom_joint_pass_post config patch joint_custom_post_pass=ChangeCosCustomPass g x x sin sin sin f x x cos cos cos x = torch randn dtype=torch float torch testing assert_close torch compile f x g x test_custom_pre_pass config patch leave custom pass only post_grad_passes pattern_matcher=False post_grad_custom_pre_pass=self _CustomPass define pattern match custom post grad opt pass post_grad_custom_post_pass=None init mkldnn fusion custom_matcher _register_mkldnn_conv_relu_fusion config post_grad_custom_pre_pass mod = _ConvReLU eval x = torch randn dtype=torch float match_count = match_nodes = other_match_count = conv prepack weight other_match_nodes = conv prepack weight _test_common mod x match_count + other_match_count match_nodes + other_match_nodes test_custom_post_pass config patch leave custom pass only post_grad_passes pattern_matcher=False define pattern match custom post grad opt pass post_grad_custom_pre_pass=None post_grad_custom_post_pass=self _CustomPass init mkldnn fusion custom_matcher _register_mkldnn_conv_relu_fusion config post_grad_custom_post_pass mod = _ConvReLU eval x = torch randn dtype=torch float match_count = match_nodes = other_match_count = conv prepack weight other_match_nodes = conv prepack weight _test_common mod x match_count + other_match_count match_nodes + other_match_nodes test_custom_pre_grad_pass saved_graph = None merge_mm_shared_rhs graph fx Graph Bad POC merging mm shared RHS i e mm x W mm x W = mm cat x x W split Isn t actually safe couple reasons For example doesn t handle case where LHS inputs depend each other saved_graph = graph matmuls = n n graph nodes n target == torch mm rhs_vals = defaultdict set m matmuls rhs_vals m args add m order = n idx idx n enumerate graph nodes rhs matmuls rhs_vals items len matmuls == continue matmuls = sorted matmuls key=lambda x order x graph inserting_before matmuls lhs_vals = m args m matmuls new_cat = graph create_node call_function torch cat args= lhs_vals new_mm = graph create_node call_function torch mm args= new_cat rhs split_vals = graph create_node call_function torch split args= new_mm l meta example_value shape l lhs_vals idx m enumerate matmuls m target = operator getitem m args = split_vals idx config patch pre_grad_custom_pass=merge_mm_shared_rhs inner_test torch compile f W nested_seqs outs = torch mm s W s nested_seqs outs W = torch randn dtype=torch bfloat nested_seqs = torch randn l dtype=torch bfloat l f W nested_seqs assert saved_graph None matmuls = n n saved_graph nodes n target == torch mm assert len matmuls == inner_test test_custom_backend_pass CustomBackendPass CustomGraphModulePass __init__ existing_pass CustomGraphModulePass = None super __init__ existing_pass = existing_pass __call__ gm fx GraphModule - None existing_pass existing_pass gm change_cos_pass gm graph uuid - bytes get_hash_for_files __file__ custom_backend_pass = CustomBackendPass get_custom_backend_pass_for_device cpu patch_inductor_backend cpu custom_pass=custom_backend_pass g x x sin sin sin f x x cos cos cos x = torch randn dtype=torch float torch testing assert_close torch compile f x g x __name__ == __main__ IS_LINUX HAS_CPU torch backends mkldnn is_available run_tests