Owner s module functorch unittest torch torch _dynamo torch _inductor torch _inductor decomposition torch _higher_order_ops out_dtype out_dtype torch fx experimental proxy_tensor make_fx torch testing _internal common_utils run_tests TestCase IS_WINDOWS TEST_WITH_ROCM IS_FBCODE IS_REMOTE_GPU TEST_CUDA torch testing _internal common_quantization skipIfNoDynamoSupport torch testing FileCheck torch testing _internal common_cuda SM OrLater _get_torch_cuda_version unittest skipIf torch _dynamo is_dynamo_supported dynamo isn t support TestOutDtypeOp TestCase test_out_dtype_make_fx M torch nn Module __init__ weight super __init__ weight = weight forward x out_dtype torch ops aten mm default torch int x weight weight = torch randint - dtype=torch int m = M weight x = torch randint - dtype=torch int gm = make_fx m x assertTrue torch allclose m x gm x gm = make_fx torch func functionalize M weight x assertTrue torch allclose m x gm x FileCheck check torch ops higher_order out_dtype check aten mm default run gm code assertTrue torch allclose m x gm x node gm graph nodes node op == call_function node target out_dtype Result node should int assertTrue node meta val dtype torch int Argument node should int assertTrue node args meta val dtype torch int test_out_dtype_op_functional M torch nn Module __init__ weight super __init__ weight = weight forward x out_dtype torch ops aten mm default torch int x weight weight = torch randint - dtype=torch int m = M weight x = torch randint - dtype=torch int ep = torch export export m x strict=True FileCheck check torch ops higher_order out_dtype check aten mm default run ep graph_module code assertTrue torch allclose m x ep module x node ep graph nodes node op == call_function node target out_dtype Result node should int assertTrue node meta val dtype torch int Argument node should int assertTrue node args meta val dtype torch int test_out_dtype_mm_numerical M torch nn Module __init__ weight super __init__ weight = weight forward x out_dtype torch ops aten mm default torch int x weight weight = torch randint - dtype=torch int m = M weight x = torch randint - dtype=torch int gm = make_fx m x x_casted = x torch int weight_casted = weight torch int numerical_res = torch ops aten mm default x_casted weight_casted assertTrue torch allclose numerical_res gm x test_out_dtype_dynamo f x y out_dtype torch ops aten mul Scalar torch int x y inp = torch randint - dtype=torch int compiled = torch compile f backend= eager fullgraph=True assertTrue torch allclose f inp compiled inp test_out_dtype_mul_scalar_numerical f x y out_dtype torch ops aten mul Scalar torch int x y inp = torch randint - dtype=torch int gm = make_fx f inp numerical_res = torch ops aten mul Scalar inp dtype=torch int assertTrue torch allclose numerical_res gm inp test_out_dtype_non_functional M torch nn Module forward x y out_dtype torch ops aten add_ Tensor torch int x y assertRaisesRegex ValueError out_dtype s first argument needs functional operator _ = torch export export M torch randint - dtype=torch int torch randint - dtype=torch int strict=True test_out_dtype_non_op_overload f x y out_dtype torch add torch int x y assertRaisesRegex ValueError out_dtype s first argument must OpOverload f torch randint - dtype=torch int torch randint - dtype=torch int test_out_dtype_no_autograd f x y out_dtype torch ops aten mm default torch int x y inp = torch randn requires_grad=True torch randn requires_grad=True error delayed f inp torch no_grad f inp assertRaisesRegex RuntimeError does require grad does have grad_fn out = f inp loss = out - torch ones out shape loss backward unittest skipIf IS_WINDOWS _int_mm unavailable unittest skipIf TEST_WITH_ROCM _int_mm unavailable unittest skipIf SM OrLater _int_mm unavailable unittest skipIf IS_FBCODE IS_REMOTE_GPU cublas runtime error unittest skipIf _get_torch_cuda_version = _int_mm unavailable unittest skipIf TEST_CUDA _int_mm unavailable skipIfNoDynamoSupport test_out_dtype_inductor_decomp - None func x w out_dtype torch ops aten mm default torch int x w w = torch randint - dtype=torch int device= cuda x = torch randint - dtype=torch int device= cuda ref = torch _int_mm x w test_out = func x w func_comp = torch compile func fullgraph=True mode= max-autotune test_out_c = func_comp x w assertTrue torch allclose ref test_out assertTrue torch allclose ref test_out_c unittest skipIf TEST_CUDA cuda only test_out_dtype_inductor_decomp_trace - None func x w out_dtype torch ops aten mm default torch int x w w = torch randint - dtype=torch int device= cuda x = torch randint - dtype=torch int device= cuda Check make_fx inductor decomps produces _int_mm decomp_table = torch _inductor decomposition select_decomp_table gm = make_fx func decomp_table tracing_mode= symbolic x w assertExpectedInline gm code strip \ forward x_ w_ _int_mm = torch ops aten _int_mm default x_ w_ x_ = w_ = None _int_mm unittest skipIf TEST_CUDA cuda only test_out_dtype_int_mm_default_trace - None func x w out_dtype torch ops aten mm default torch int x w w = torch randint - dtype=torch int device= cuda x = torch randint - dtype=torch int device= cuda By default out_dtype preserved trace gm = make_fx func tracing_mode= symbolic x w assertExpectedInline gm code strip \ forward x_ w_ out_dtype = torch ops higher_order out_dtype torch ops aten mm default torch int x_ w_ x_ = w_ = None out_dtype test_out_dtype_wrong_output - None multiple_out x out_dtype torch ops aten topk default torch int x inp = torch randn assertRaisesRegex ValueError out_dtype s can only apply ops single tensor multiple_out inp singleton_list_out x out_dtype torch ops aten split_copy Tensor torch int x assertRaisesRegex ValueError out_dtype s can only apply ops single tensor singleton_list_out inp __name__ == __main__ run_tests