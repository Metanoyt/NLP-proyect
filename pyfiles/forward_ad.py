mypy allow-untyped-defs os typing Any NamedTuple Optional torch grad_mode _DecoratorContextManager __all__ = UnpackedDualTensor enter_dual_level exit_dual_level make_dual unpack_dual dual_level Global variable used make python API simpler use _current_level = - enter_dual_level r Enter new forward grad level This level can used make unpack dual Tensors compute forward gradients This function also updates current level used default other functions API global _current_level new_level = torch _C _enter_dual_level new_level = _current_level + raise RuntimeError Entering new forward AD level current level valid Make sure you did modified directly _current_level = new_level new_level exit_dual_level level=None r Exit forward grad level This function deletes all gradients associated level Only deleting latest entered level allowed This function also updates current level used default other functions API global _current_level level None level = _current_level level = _current_level raise RuntimeError Trying exit forward AD level last one created This supported torch _C _exit_dual_level level=level _current_level = level - _maybe_load_decompositions os environ get PYTORCH_JIT == __debug__ torch _decomp decompositions_for_jvp noqa F make_dual tensor tangent level=None r Associate tensor value its tangent create dual tensor forward AD gradient computation The result new tensor aliased attr ` tensor ` attr ` tangent ` embedded attribute as-is has same storage layout copied otherwise The tangent attribute can recovered func ` unpack_dual ` This function backward differentiable Given function ` f ` whose jacobian ` J ` allows one compute Jacobian-vector product ` jvp ` between ` J ` given vector ` v ` follows Example xdoctest +SKIP Undefined variables dual_level inp = make_dual x v out = f inp y jvp = unpack_dual out Please see ` forward-mode AD tutorial https pytorch org tutorials intermediate forward_ad_usage html ` __ detailed steps how use API See NOTE forward-mode AD decompositions mechanism Import torch _decomp decompositions_for_jvp register decompositions jvp jit registry FIXME We specify __debug__ must True because python run -OO -O flags i e __debug__ False we encounter following error Return value annotated having type Tuple NoneType NoneType actually type Tuple Tensor Tensor File torch _decomp __init__ py line buffer = z min - torch log p z buffer ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ --- HERE _maybe_load_decompositions level None level = _current_level level raise RuntimeError Trying create dual Tensor forward AD no level exists make sure enter_dual_level first tensor is_floating_point tensor is_complex raise ValueError f Expected primal floating point complex got tensor dtype tangent is_floating_point tangent is_complex raise ValueError f Expected tangent floating point complex got tangent dtype torch _VF _make_dual tensor tangent level=level UnpackedDualTensor NamedTuple r Namedtuple returned func ` unpack_dual ` containing primal tangent components dual tensor See func ` unpack_dual ` more details primal torch Tensor tangent Optional torch Tensor unpack_dual tensor level=None r Unpack dual tensor get both its Tensor value its forward AD gradient The result namedtuple ` ` primal tangent ` ` where ` ` primal ` ` view attr ` tensor ` s primal ` ` tangent ` ` attr ` tensor ` s tangent as-is Neither these tensors can dual tensor level attr ` level ` This function backward differentiable Example xdoctest +SKIP Undefined variables dual_level inp = make_dual x x_t out = f inp y jvp = unpack_dual out jvp = unpack_dual out tangent Please see ` forward-mode AD tutorial https pytorch org tutorials intermediate forward_ad_usage html ` __ detailed steps how use API level None level = _current_level level UnpackedDualTensor tensor None primal dual = torch _VF _unpack_dual tensor level=level UnpackedDualTensor primal dual dual_level _DecoratorContextManager r Context-manager forward AD where all forward AD computation must occur within ` ` dual_level ` ` context Note The ` ` dual_level ` ` context appropriately enters exit dual level controls current forward AD level which used default other functions API We currently don t plan support nested ` ` dual_level ` ` contexts however so only single forward AD level supported To compute higher-order forward grads one can use func ` torch func jvp ` Example xdoctest +SKIP Undefined variables x = torch tensor x_t = torch tensor dual_level inp = make_dual x x_t Do computations inp out = your_fn inp _ grad = unpack_dual out grad None False After exiting level grad deleted _ grad_after = unpack_dual out grad None True Please see ` forward-mode AD tutorial https pytorch org tutorials intermediate forward_ad_usage html ` __ detailed steps how use API __enter__ enter_dual_level __exit__ exc_type Any exc_value Any traceback Any - None exit_dual_level Private helper functions _is_fwd_grad_enabled = torch _C _is_fwd_grad_enabled Private helper function enable disable fwd grad If you re user want use please file issue discuss use case _set_fwd_grad_enabled _DecoratorContextManager __init__ mode bool - None prev = _is_fwd_grad_enabled torch _C _set_fwd_grad_enabled mode __enter__ - None pass __exit__ exc_type Any exc_value Any traceback Any - None torch _C _set_fwd_grad_enabled prev