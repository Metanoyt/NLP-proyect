mypy allow-untyped-defs copy inspect math warnings collections abc Sequence itertools chain typing Any Optional sympy torch torch utils _pytree pytree torch _export non_strict_utils _enter_enable_graph_inputs_of_type_nn_module _exit_enable_graph_inputs_of_type_nn_module _get_graph_inputs_of_type_nn_module torch _export passes add_runtime_assertions_for_constraints_pass _convert_range_to_int torch _export utils _check_input_constraints_for_graph torch export unflatten _assign_attr _AttrKind torch fx experimental proxy_tensor _pytree_subclasses_that_lose_info torch fx graph _PyTreeCodeGen _PyTreeInfo torch fx traceback NodeSource NodeSourceAction torch utils _sympy solve try_solve torch utils _sympy value_ranges ValueRanges _remove_effect_tokens_pass _remove_effect_tokens _tree_utils reorder_kwargs exported_program ExportedProgram ExportGraphSignature InputKind OutputKind eq_spec pytree TreeSpec other pytree TreeSpec - bool Refinement TreeSpec __eq__ where e g torch Size matches tuple See _pytree_subclasses_that_lose_info proxy_tensor py more details _normalize_type t str _pytree_subclasses_that_lose_info get t t _match_normalized_structure b b True _normalize_type type = _normalize_type b type False type dict b type dict case dict context list keys we allow keys any order set context = set b context False context = b context False num_children = b num_children False all _match_normalized_structure b b zip children b children _match_normalized_structure other _check_inputs_match args kwargs in_spec pytree TreeSpec - list reordered_kwargs = reorder_kwargs kwargs in_spec flat_args_with_path received_spec = pytree tree_flatten_with_path args reordered_kwargs eq_spec received_spec in_spec raise ValueError noqa B Trying flatten user inputs exported input tree spec \n f in_spec \n actually got inputs tree spec \n f received_spec \n Please check inputs have same number type args kwargs ones you used when tracing flat_args_with_path _force_ep_signature_match ep_guards_code list str input_paths TODO tmanlaibaatar This band-aid solution export new tracer replacing shape env sources flat_args The real fix should replacing shape env sources original user sources quite involved because you need carefully construct new sources using dynamo replace all instances inside shape env But lot easier manipulate after we turn them into strings only time we use these guards during retracing running exported program so probably ok have useful guards ep now name_mapping = idx path enumerate input_paths name_mapping f L flat_args idx = f L pytree keystr path new_guards_code = guard ep_guards_code old_name new_name name_mapping items guard = guard replace old_name new_name new_guards_code append guard new_guards_code _force_gm_signature_match ep_guards_code list str signature The signature originally exported module may match signature unlifted graph module extracted exported program The guards code extracted exported program based former generated guards fn based latter thus we need reconcile any such diff re Handle case where signatures may differ var args orig_arg_names = set g ep_guards_code match substrings form L name number orig_arg_names update re findall r L\ \ ^\ + \ \ \ - + \ g sig_arg_names = set n signature parameters match substrings form name _ number sig_arg_names update re findall r + _ - + n replace L name number L name _ number new_guards_code = ep_guards_code match orig_arg_names match sig_arg_names base idx = match new_guards_code = g replace f L base idx f L base _ idx g new_guards_code new_guards_code _convert_guards_code_to_fn guards_code list str paths_of_placeholders list pytree KeyPath Generates Python code given guards code paths placeholders We assume based source information - tracer generates guards code - input spec generates paths placeholders Example Suppose we given guards code L z k size == we given z k path placeholder Then we will generate ` ` ` torch _assert args size == Guard failed z k size == ` ` ` FAQ Why do we generate code based flattened args instead original unflattened inputs Because would require inserting additional pytree unflatten call our graph FAQ Why do we emit RuntimeError guard failure we used Because inconvenient get used AssertionError instead ast torch fx experimental symbolic_shapes SYMPY_INTERP actual_guards_code = shadow_guards_code = c guards_code s = c c idx path enumerate paths_of_placeholders e g replace L z k args Python code actual = replace L + pytree keystr path f args idx e g replace L z k z k error message shadow s = s replace L + pytree keystr path path key + pytree keystr path type ignore attr-defined actual_guards_code append shadow_guards_code append s replace \n generate function code str code_str = \ndef _ args \n actual shadow zip actual_guards_code shadow_guards_code printing guards code may potentially introduce redundant parens we can normalize them out readability parsing unparsing NOTE necessary correctness just deemed desirable _shadow = ast unparse ast parse shadow mode= eval actual code shadow error message code_str += f torch _assert actual Guard failed _shadow \n code_str += return\n populate namespace sympy globals materialize function named ` _ ` namespace = SYMPY_INTERP exec code_str namespace create module whose forward materialized function NOTE we want Dynamo trace through module repopulate guards otherwise we would lose them when retracing NOTE calling module will side effect no users so must marked impure avoid being cleaned up DCE guards_fn = GuardsFn guards_fn forward = torch _dynamo dont_skip_tracing namespace _ type ignore call-overload method-assign guards_fn _is_impure = True type ignore assignment guards_fn torch _dynamo disable _check_input_constraints_for_module args kwargs flat_args_with_path = _check_inputs_match args kwargs _in_spec _check_input_constraints_for_graph graph find_nodes op= placeholder flat_args_with_path range_constraints _check_input_constraints_pre_hook args kwargs preserve current behavior clients do want any validation validate_inputs when guards function exists assume graph does calls so we do need check input constraints we still want check inputs match otherwise we d get obscure pytree errors hasattr _guards_fn _check_inputs_match args kwargs _in_spec NOTE some reason Dynamo tracing into we should see why put compile right place Until then we can skip input constraint checks torch compiler is_dynamo_compiling _check_input_constraints_for_module args kwargs _unlift_inputs_as_getattr gm torch fx GraphModule lifted_inputs Sequence Optional str - tuple dict str torch fx Node dict str torch fx Node Unlift inputs referring params buffers constants getattr nodes graph unlifted_name_to_node = input_name_to_node = placeholder_nodes = node node gm graph nodes node op == placeholder assert len lifted_inputs == len placeholder_nodes input_node lifted_node zip placeholder_nodes lifted_inputs lifted_node None input_name_to_node input_node name = input_node gm graph inserting_after input_node It fine ignore warning because guaranteed we will populate attr later warnings catch_warnings warnings simplefilter ignore getattr_node = gm graph get_attr lifted_node input_node replace_all_uses_with getattr_node metadata = input_node meta gm graph erase_node input_node getattr_node meta = metadata getattr_node meta from_node = NodeSource input_node ExportedProgram module unlift NodeSourceAction CREATE NodeSourceAction REPLACE unlifted_name_to_node lifted_node = getattr_node unlifted_name_to_node input_name_to_node _insert_copy_for_mutations gm torch fx GraphModule mutated_outputs Sequence Optional str unlifted_name_to_node dict str torch fx Node input_name_to_node dict str torch fx Node - None Find all buffers inputs mutated insert copy_ operators reflect mutations output_node = gm graph output_node outputs = pytree tree_flatten output_node args assert len outputs == len mutated_outputs user_output_nodes = return_nodes_to_copy = return_node mutated_node_name zip outputs mutated_outputs mutated_node_name None user_output_nodes append return_node continue mutated_node_name unlifted_name_to_node mutated_node = unlifted_name_to_node mutated_node_name mutated_node_name input_name_to_node mutated_node = input_name_to_node mutated_node_name raise RuntimeError f Could find mutated_node_name either buffer input nodes gm graph inserting_before output_node copy_node = gm graph call_function torch ops aten copy_ default mutated_node return_node return_nodes_to_copy return_node = copy_node output_args = tuple return_nodes_to_copy get node node node user_output_nodes gm graph inserting_before output_node Only user outputs new_output = gm graph output output_args output_node replace_all_uses_with new_output gm graph erase_node output_node new_output name = output_node name new_output meta update output_node meta new_output meta from_node = NodeSource output_node ExportedProgram module unlift NodeSourceAction CREATE NodeSourceAction REPLACE _get_codegen in_spec pytree TreeSpec out_spec Optional pytree TreeSpec forward_arg_names Optional list str = None - _PyTreeCodeGen Create codegen graph module based out specs forward_arg_names names = forward_arg_names in_spec type tuple in_spec num_children == in_spec child type tuple in_spec child type dict in_spec contains args tuple kwargs dict names = f arg_ i i range in_spec child num_children add kwarg names names extend in_spec child context names = f arg_ i i range in_spec num_children _PyTreeCodeGen _PyTreeInfo names in_spec out_spec _unlift gm torch fx GraphModule lifted_inputs Sequence Optional str mutated_outputs Sequence Optional str in_spec pytree TreeSpec out_spec Optional pytree TreeSpec forward_arg_names Optional list str = None Args lifted_inputs A list matching graph module s input nodes For input node referring lifted parameter buffer list will contain fqn corresponding attribute Otherwise list will contain None This used unlift lifted parameters get_attr nodes mutated_outputs A list matching graph module s output nodes For output node referring mutated buffer user input list will contain name corresponding buffer user input needs mutated Otherwise list will contain None This used re-insert inplace copy_ operator copy mutated values back original node unlifted_name_to_node input_name_to_node = _unlift_inputs_as_getattr gm lifted_inputs _insert_copy_for_mutations gm mutated_outputs unlifted_name_to_node input_name_to_node gm graph _codegen = _get_codegen in_spec out_spec forward_arg_names gm graph lint gm recompile gm _register_attrs_to_new_gm new_gm torch fx GraphModule graph_signature ExportGraphSignature state_dict dict str Any constants dict str Any - None non_persistent_buffers = set graph_signature non_persistent_buffers name graph_signature buffers name non_persistent_buffers persistent = False value = constants name persistent = True value = state_dict name _assign_attr value new_gm name attr_kind=_AttrKind BUFFER persistent=persistent name graph_signature parameters value = state_dict name _assign_attr value new_gm name attr_kind=_AttrKind PARAMETER Technically doesn t account aliased multiple constants ok because we have separate pass later stack populates final gm name chain graph_signature lifted_custom_objs graph_signature lifted_tensor_constants value = constants name _assign_attr value new_gm name attr_kind=_AttrKind CONSTANT _StatefulGraphModuleFactory type Metaclass ensures private constructor _StatefulGraphModule __call__ cls args kwargs raise TypeError f cls __module__ cls __qualname__ has no public constructor _create cls root graph range_constraints=None super __call__ root graph range_constraints=range_constraints _StatefulGraphModule torch fx GraphModule metaclass=_StatefulGraphModuleFactory __init__ root graph range_constraints=None super __init__ root graph Need fix up non-persistent buffers range_constraints = range_constraints validate_inputs = True _create_stateful_graph_module plain_graph_module torch fx GraphModule range_constraints ep ExportedProgram - _StatefulGraphModule stateful_gm = _StatefulGraphModule _create plain_graph_module plain_graph_module graph range_constraints=range_constraints module_types = _get_graph_inputs_of_type_nn_module ep example_inputs stateful_gm register_forward_pre_hook lambda args kwargs _enter_enable_graph_inputs_of_type_nn_module module_types stateful_gm register_forward_pre_hook _check_input_constraints_pre_hook with_kwargs=True stateful_gm register_forward_hook lambda args kwargs _exit_enable_graph_inputs_of_type_nn_module module_types always_call=True When we have constant has requires_grad=True we need detach when we unlift tensors require gradients should registered via parameters But problematic when we have aliasing two constants because when we call detach they will become different tensors This dict keeps track logic original_tensor_to_detached_tensor = Fix up lifted tensor constants fx GraphModule constructor silently turns constant attribute plain_graph_module into buffer stateful_gm creates inconsistency graph_signature We fix de-registering these buffers lifted_tensor_constants call _assign_attr attr_kind=CONSTANT register them constants constant_fqn ep graph_signature lifted_tensor_constants Sometimes constant can require gradient probably bug user code e g ` const = torch randn requires_grad=True ` We call detach constant_val since they re tensor constants we don t need compute their gradients anyway Users should properly register parameter they want require gradient buffer = stateful_gm get_buffer constant_fqn buffer requires_grad warnings warn f A model attribute ` constant_fqn ` requires gradient f s properly registered parameter f torch export will detach treat constant tensor f please register parameter instead stacklevel= detached_buffer = buffer detach original_tensor_to_detached_tensor buffer = detached_buffer buffer = detached_buffer prefix field = constant_fqn rsplit submod = torch fx graph_module _get_attr_via_attr_list stateful_gm prefix delattr submod field _assign_attr buffer stateful_gm constant_fqn attr_kind=_AttrKind CONSTANT Constants preserved well when we create new GraphModule unlike param buffers const_name value ep constants items torch fx graph_module _has_attr stateful_gm const_name isinstance value torch Tensor value requires_grad warnings warn f A model attribute ` const_name ` requires gradient f s properly registered parameter f torch export will detach treat constant tensor f please register parameter instead stacklevel= value original_tensor_to_detached_tensor value = original_tensor_to_detached_tensor value detached_value = value detach original_tensor_to_detached_tensor value = detached_value value = detached_value _assign_attr value stateful_gm const_name attr_kind=_AttrKind CONSTANT Fix up non-persistent buffers torch fx does distinguish between persistent non-persistent buffers so we must restore distinction here buffer ep graph_signature non_persistent_buffers _assign_attr plain_graph_module get_buffer buffer stateful_gm buffer attr_kind=_AttrKind BUFFER persistent=False stateful_gm _get_input_paths example_inputs signature Generate paths placeholders needed generating guards function NOTE Here we make use example inputs used export well signature unlifted graph module preserved export args kwargs = example_inputs binded = signature bind args kwargs binded apply_defaults ctx = binded arguments flat_example_inputs_with_paths = pytree tree_leaves_with_path ctx path path _ flat_example_inputs_with_paths _replace_sources result_str str flat_input_paths list Any Given user specified input paths maybe fix up guard string reflect user path instead tracer path name_mapping = idx path enumerate flat_input_paths name_mapping f L flat_args idx = f L pytree keystr path replace = result_str key val name_mapping items replace = replace replace key val replace _get_input_guards_for_graph placeholders list torch fx Node range_constraints dict sympy Symbol ValueRanges paths_for_placeholders list pytree KeyPath Guards generated tracer include conditions observed code do include some additional checks we typically do export For example when dynamic shapes get specialized specified within range specified some equational relation corresponding input invalidation done within pre_hook specifically ` _check_input_constraints_for_graph ` Here we generate guards corresponding checks happen ` _check_input_constraints_for_graph ` add them guards already generated tracer In future may worthwhile separate them so we can allow clients turn off one other Looking you AOTI NOTE We should eventually reconcile logic ` build_guards ` used AOT Precompile deferred_expressions = new_guards_code = sources dict sympy Expr str = handle_symint expr src len expr free_symbols == complex equations e g involving derived dims need handled later since we may have enough information just we passing through placeholders order deferred_expressions append src expr expr sources expressions appear multiple sources should force inputs corresponding those sources equal e g x shape == y shape orig_src = sources expr new_guards_code append f src == orig_src sources expr = src process value ranges elsewhere export min_val max_val = _convert_range_to_int range_constraints expr min_val new_guards_code append f src = min_val max_val math inf new_guards_code append f src = max_val placeholder path zip placeholders paths_for_placeholders src = L + pytree keystr path meta = placeholder meta val specializations isinstance meta int new_guards_code append f src == meta isinstance meta float meta == math inf new_guards_code append f src == math inf meta == -math inf new_guards_code append f src == -math inf new_guards_code append f src == meta isinstance meta str new_guards_code append f src == meta range constraints equalities isinstance meta torch SymInt meta node expr range_constraints handle_symint meta node expr src isinstance meta torch Tensor i dim enumerate meta shape src = L + pytree keystr path + f size i isinstance dim int specializations new_guards_code append f src == dim isinstance dim torch SymInt dim node expr range_constraints range constraints equalities handle_symint dim node expr src unification_map dict sympy Symbol sympy Expr = py_printer = torch utils _sympy printers PythonPrinter process complex equations e g involving derived dims src expr deferred_expressions we know only symbol expr see check above symbol = next iter expr free_symbols symbol sources s already known directly sourced inputs e g z shape we do need do anything further assume we have already processed constraints s above continue otherwise s has some hidden source like dim example src = y shape expr = s + symbol unification_map suppose we already know s = x shape so we can emit guard x shape + = y shape substitution = expr subs unification_map new_guards_code append py_printer doprint sympy Eq substitution sympy Symbol src we do yet know what s given s + = y shape we can solve s now knowing s = y shape - solution = try_solve sympy Eq expr sympy Symbol src symbol solution None definition = solution unification_map symbol = definition new_guards_code _ok_to_generate_guards_fn patterns = executorch modai on_device_ai torchao force check_guards=False files matching ` patterns ` because they have too many calls module do like any call modules graph TODO fix these files handle guard fns frame = inspect currentframe while frame None any path frame f_code co_filename path patterns False frame = frame f_back True _unlift_exported_program_lifted_states ep ExportedProgram check_guards=True - torch fx GraphModule check_guards = check_guards _ok_to_generate_guards_fn TODO T ep verifiers dialect = TRAINING ep = _remove_effect_tokens ep new_gm = torch fx GraphModule ep graph_module copy deepcopy ep graph _register_attrs_to_new_gm new_gm ep graph_signature ep state_dict ep constants forward_arg_names = sig forward_arg_names sig = ep module_call_graph signature None lifted_inputs list Optional str = in_spec target in_spec kind InputKind BUFFER InputKind CONSTANT_TENSOR InputKind PARAMETER InputKind CUSTOM_OBJ None in_spec ep graph_signature input_specs mutated_outputs list Optional str = out_spec target out_spec kind OutputKind BUFFER_MUTATION OutputKind USER_INPUT_MUTATION OutputKind PARAMETER_MUTATION None out_spec ep graph_signature output_specs source_node_dict = node name node node ep graph nodes node op = placeholder placeholder node name might change after deepcopy placeholder_source_node_dict = node target node node ep graph nodes node op == placeholder node new_gm graph nodes source_node = None node op == placeholder source_node = placeholder_source_node_dict get node target source_node = source_node_dict get node name node meta from_node = NodeSource source_node ExportedProgram module NodeSourceAction CREATE assert ep call_spec in_spec None new_gm = _unlift new_gm lifted_inputs mutated_outputs ep call_spec in_spec ep call_spec out_spec forward_arg_names=forward_arg_names unlift_gm = _create_stateful_graph_module new_gm ep range_constraints ep unlift_gm meta update ep graph_module meta create _guards_fn submodule insert call after placeholders graph = unlift_gm graph placeholders = graph find_nodes op= placeholder check_guards placeholders ep example_inputs sig = inspect signature unlift_gm forward input_paths = _get_input_paths ep example_inputs sig TODO tmanlaibaatar This band-aid solution export new tracer replacing shape env sources flat_args The real fix should replacing shape env sources original user sources quite involved because you need carefully construct new sources using dynamo replace all instances inside shape env But lot easier manipulate after we turn them into strings only time we use these guards during retracing running exported program so probably ok have useful guards ep now ep_guards = guard ep _guards_code ep_guards append _replace_sources guard input_paths guards_code = _get_input_guards_for_graph placeholders ep range_constraints input_paths ep_guards_code = _force_ep_signature_match ep _guards_code input_paths ep_guards_code = _force_gm_signature_match ep_guards_code sig guards_code extend ep_guards_code unlift_gm _guards_fn = _convert_guards_code_to_fn guards_code input_paths root_nn_module_stack = torch fx _utils first_call_function_nn_module_stack graph graph inserting_after placeholders - node = graph call_module _guards_fn tuple placeholders node meta nn_module_stack = root_nn_module_stack unlift_gm recompile unlift_gm GuardsFn torch nn Module Module guard functions forward args pass