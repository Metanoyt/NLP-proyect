mypy allow-untyped-defs functools logging typing Any Optional Union torch torch _dynamo utils counters torch _inductor autoheuristic autoheuristic AutoHeuristicSelectAlgorithm torch _inductor autoheuristic autoheuristic_utils AHContext context_add_strides context_add_using_tf mm_operations torch _inductor codegen cpp_gemm_template CppGemmTemplate torch _inductor remote_gemm_autotune_cache gen_best_config torch _inductor virtualized ops V torch fx experimental proxy_tensor make_fx torch nn functional ScalingType type ignore attr-defined torch torch_version TorchVersion config inductor_config codegen cuda gemm_template CUTLASS xGemmTemplate CUTLASS xGemmTemplate codegen rocm ck_tile_universal_gemm_template CKTileGemmTemplate codegen rocm ck_universal_gemm_template CKGemmTemplate codegen subgraph SubgraphChoiceCaller SubgraphTemplate ir Buffer ChoiceCaller is_triton Layout kernel_inputs MMKernelInputs lowering lowerings make_pointwise make_reduction register_lowering transform_args select_algorithm autotune_select_algorithm ExternKernelChoice KernelTemplate realize_inputs TritonTemplate utils _use_cutlass_for_op ceildiv use_aten_gemm_kernels use_ck_gemm_template use_ck_tile_gemm_template use_cpp_gemm_template use_cutlass_template use_decompose_k_choice use_triton_blackwell_tma_template use_triton_template use_triton_tma_template mm_common _is_static_problem mm_args mm_grid persistent_mm_grid use_native_matmul try triton triton_version = TorchVersion triton __version__ has_triton = True except ImportError triton_version = TorchVersion has_triton = False log = logging getLogger __name__ aten = torch ops aten prims = torch ops prims mm_template = TritonTemplate name= mm grid=mm_grid source= r def_kernel A B M = size A N = size B K = size A M N == early exit due zero-size input s stride_am = stride A stride_ak = stride A stride_bk = stride B stride_bn = stride B based triton ops matmul pid = tl program_id INDEX_DTYPE grid_m = M + BLOCK_M - BLOCK_M grid_n = N + BLOCK_N - BLOCK_N re-order program ID better L performance width = GROUP_M grid_n group_id = pid width group_size = min grid_m - group_id GROUP_M GROUP_M pid_m = group_id GROUP_M + pid group_size pid_n = pid width group_size tl assume pid_m = tl assume pid_n = rm = pid_m BLOCK_M + tl arange BLOCK_M rn = pid_n BLOCK_N + tl arange BLOCK_N stride_am == stride_ak == M stride_am == K stride_ak == M = BLOCK_M K offs_a_m = tl max_contiguous tl multiple_of rm M BLOCK_M BLOCK_M offs_a_m = rm M stride_bk == stride_bn == K stride_bk == N stride_bn == N = BLOCK_N K offs_b_n = tl max_contiguous tl multiple_of rn N BLOCK_N BLOCK_N offs_b_n = rn N offs_k = tl arange BLOCK_K acc = tl zeros BLOCK_M BLOCK_N dtype=ACC_TYPE k_idx range tl cdiv K BLOCK_K EVEN_K a_mask = offs_k None K - k_idx BLOCK_K b_mask = offs_k None K - k_idx BLOCK_K endif a_k_idx_vals = offs_k None + k_idx BLOCK_K b_k_idx_vals = offs_k None + k_idx BLOCK_K idx_m = offs_a_m None idx_n = a_k_idx_vals load_input A idx_m idx_n mask=None EVEN_K a_mask indent_width= index_shape= BLOCK_M BLOCK_K idx_m = b_k_idx_vals idx_n = offs_b_n None load_input B b idx_m idx_n mask=None EVEN_K b_mask indent_width= index_shape= BLOCK_K BLOCK_N USE_FAST_ACCUM acc = tl dot b acc allow_tf =ALLOW_TF out_dtype=ACC_TYPE acc += tl dot b allow_tf =ALLOW_TF out_dtype=ACC_TYPE endif rematerialize rm rn save registers rm = pid_m BLOCK_M + tl arange BLOCK_M rn = pid_n BLOCK_N + tl arange BLOCK_N idx_m = rm None idx_n = rn None mask = idx_m M idx_n N inductor generates suffix store_output idx_m idx_n acc mask val_shape= BLOCK_M BLOCK_N torch version hip None triton_version = FIXME To get around rocm failures like https github com pytorch pytorch actions runs job The only difference between two templates M = BLOCK_M N = BLOCK_N checking See more details https github com pytorch pytorch pull r def_kernel A B M = size A N = size B K = size A M N == early exit due zero-size input s stride_am = stride A stride_ak = stride A stride_bk = stride B stride_bn = stride B based triton ops matmul pid = tl program_id INDEX_DTYPE grid_m = M + BLOCK_M - BLOCK_M grid_n = N + BLOCK_N - BLOCK_N re-order program ID better L performance width = GROUP_M grid_n group_id = pid width group_size = min grid_m - group_id GROUP_M GROUP_M pid_m = group_id GROUP_M + pid group_size pid_n = pid width group_size tl assume pid_m = tl assume pid_n = rm = pid_m BLOCK_M + tl arange BLOCK_M rn = pid_n BLOCK_N + tl arange BLOCK_N stride_am == stride_ak == M stride_am == K stride_ak == offs_a_m = tl max_contiguous tl multiple_of rm M BLOCK_M BLOCK_M offs_a_m = rm M stride_bk == stride_bn == K stride_bk == N stride_bn == offs_b_n = tl max_contiguous tl multiple_of rn N BLOCK_N BLOCK_N offs_b_n = rn N offs_k = tl arange BLOCK_K acc = tl zeros BLOCK_M BLOCK_N dtype=ACC_TYPE k_idx range tl cdiv K BLOCK_K EVEN_K a_mask = offs_k None K - k_idx BLOCK_K b_mask = offs_k None K - k_idx BLOCK_K endif a_k_idx_vals = offs_k None + k_idx BLOCK_K b_k_idx_vals = offs_k None + k_idx BLOCK_K idx_m = offs_a_m None idx_n = a_k_idx_vals load_input A idx_m idx_n mask=None EVEN_K a_mask indent_width= index_shape= BLOCK_M BLOCK_K idx_m = b_k_idx_vals idx_n = offs_b_n None load_input B b idx_m idx_n mask=None EVEN_K b_mask indent_width= index_shape= BLOCK_K BLOCK_N USE_FAST_ACCUM acc = tl dot b acc allow_tf =ALLOW_TF out_dtype=ACC_TYPE acc += tl dot b allow_tf =ALLOW_TF out_dtype=ACC_TYPE endif rematerialize rm rn save registers rm = pid_m BLOCK_M + tl arange BLOCK_M rn = pid_n BLOCK_N + tl arange BLOCK_N idx_m = rm None idx_n = rn None mask = idx_m M idx_n N inductor generates suffix store_output idx_m idx_n acc mask val_shape= BLOCK_M BLOCK_N cache_codegen_enabled_for_template=True prologue_loads_all_inputs=True persistent_tma_mm_template = TritonTemplate name= mm_persistent_tma grid=persistent_mm_grid source=r def_kernel A B M = size A N = size B K = size A M N == early exit due zero-size input s start_pid = tl program_id INDEX_DTYPE grid_m = tl cdiv M BLOCK_M grid_n = tl cdiv N BLOCK_N k_tiles = tl cdiv K BLOCK_K num_tiles = grid_m grid_n tiles_per_SM = num_tiles NUM_SMS start_pid num_tiles NUM_SMS tiles_per_SM += tile_id = start_pid - NUM_SMS ki = - width = GROUP_M grid_n rk_for_mask = tl arange BLOCK_K acc = tl zeros BLOCK_M BLOCK_N dtype=ACC_TYPE - TMA_EXPERIMENTAL_API workspace_base = ws_ptr + start_pid TMA_SIZE a_desc_ptr = workspace_base b_desc_ptr = workspace_base + TMA_SIZE triton language extra cuda experimental_device_tensormap_create d desc_ptr=a_desc_ptr global_address=A load_size= BLOCK_M BLOCK_K A_ROW_MAJOR BLOCK_K BLOCK_M global_size= M K A_ROW_MAJOR K M element_ty=A dtype element_ty triton language extra cuda experimental_device_tensormap_create d desc_ptr=b_desc_ptr global_address=B load_size= BLOCK_K BLOCK_N B_ROW_MAJOR BLOCK_N BLOCK_K global_size= K N B_ROW_MAJOR N K element_ty=B dtype element_ty tl extra cuda experimental_tensormap_fenceproxy_acquire a_desc_ptr tl extra cuda experimental_tensormap_fenceproxy_acquire b_desc_ptr - stride_am = stride A stride_ak = stride A stride_bk = stride B stride_bn = stride B a_desc = triton language make_tensor_descriptor base=A shape= M K A_ROW_MAJOR K M strides= stride_am A_ROW_MAJOR stride_ak block_shape= BLOCK_M BLOCK_K A_ROW_MAJOR BLOCK_K BLOCK_M b_desc = triton language make_tensor_descriptor base=B shape= K N B_ROW_MAJOR N K strides= stride_bk B_ROW_MAJOR stride_bn block_shape= BLOCK_K BLOCK_N B_ROW_MAJOR BLOCK_N BLOCK_K - endif pid_m = pid_n = rm = rn = _ range k_tiles tiles_per_SM ki = tl where ki == k_tiles - ki + ki == tile_id += NUM_SMS re-order program ID better L performance group_id = tile_id width group_size = min grid_m - group_id GROUP_M GROUP_M pid_m = group_id GROUP_M + tile_id group_size pid_n = tile_id width group_size rm = pid_m BLOCK_M rn = pid_n BLOCK_N rk = ki BLOCK_K - TMA_EXPERIMENTAL_API = tl _experimental_descriptor_load a_desc_ptr rm rk A_ROW_MAJOR rk rm BLOCK_M BLOCK_K A_ROW_MAJOR BLOCK_K BLOCK_M A dtype element_ty b = tl _experimental_descriptor_load b_desc_ptr rk rn B_ROW_MAJOR rn rk BLOCK_K BLOCK_N B_ROW_MAJOR BLOCK_N BLOCK_K B dtype element_ty - = tl load_tensor_descriptor a_desc rm rk A_ROW_MAJOR rk rm b = tl load_tensor_descriptor b_desc rk rn B_ROW_MAJOR rn rk - endif acc += tl dot A_ROW_MAJOR T b B_ROW_MAJOR b T allow_tf =ALLOW_TF ki == k_tiles - inductor generates suffix - TMA_EXPERIMENTAL_API rematerialize rm rn save registers rcm = rm + tl arange BLOCK_M rcn = rn + tl arange BLOCK_N idx_m = rcm None idx_n = rcn None mask = idx_m M idx_n N store_output idx_m idx_n acc mask indent_width= val_shape= BLOCK_M BLOCK_N - store_output rm rn acc indent_width= val_shape= BLOCK_M BLOCK_N block_indexing=True - endif acc = tl zeros BLOCK_M BLOCK_N dtype=ACC_TYPE load_scales = r triton jit load_scales scale_ptr SCALE_RECIPE tl constexpr SCALE_RECIPE == tl load scale_ptr For tensor-wise scaling we ll load scalar values scale_ptr For all other scaling recipes we ll pointers apply_scaling = r triton jit apply_scaling accumulator a_scale b_scale SCALE_RECIPE_A tl constexpr SCALE_RECIPE_B tl constexpr offs_cm offs_cn M N stride_a_scale_m stride_b_scale_n SCALE_RECIPE_A == SCALE_RECIPE_B == ScalingType RowWise ScalingType RowWise For row-wise scaling we need load scales each row column a_scales = tl load a_scale + offs_cm stride_a_scale_m mask=offs_cm M other= b_scales = tl load b_scale + offs_cn stride_b_scale_n mask=offs_cn N other= acc_scale = a_scales None b_scales None ScalingType TensorWise ScalingType TensorWise For per-tensor scaling we can directly use loaded scalar values acc_scale = a_scale b_scale accumulator acc_scale scaled_mm_device_tma_epilogue_scaling = r def_kernel A B A_inverse_scale B_inverse_scale M = size A N = size B K = size A M N == early exit due zero-size input s stride_am = stride A stride_ak = stride A stride_bk = stride B stride_bn = stride B SCALE_RECIPE_A == ScalingType RowWise stride_a_scale_m = stride_a_scale_m = SCALE_RECIPE_B == ScalingType RowWise stride_b_scale_n = stride_b_scale_n = start_pid = tl program_id axis= INDEX_DTYPE num_pid_m = tl cdiv M BLOCK_M num_pid_n = tl cdiv N BLOCK_N k_tiles = tl cdiv K BLOCK_K num_tiles = num_pid_m num_pid_n - TMA_EXPERIMENTAL_API workspace_base = ws_ptr + start_pid TMA_SIZE a_desc_ptr = workspace_base b_desc_ptr = workspace_base + TMA_SIZE triton language extra cuda experimental_device_tensormap_create d desc_ptr=a_desc_ptr global_address=A load_size= BLOCK_M BLOCK_K global_size= M K element_ty=A dtype element_ty triton language extra cuda experimental_device_tensormap_create d desc_ptr=b_desc_ptr global_address=B load_size= BLOCK_N BLOCK_K global_size= N K element_ty=B dtype element_ty tl extra cuda experimental_tensormap_fenceproxy_acquire a_desc_ptr tl extra cuda experimental_tensormap_fenceproxy_acquire b_desc_ptr - stride_am = stride A stride_bn = stride B a_desc = triton language make_tensor_descriptor base=A shape= M K strides= stride_am block_shape= BLOCK_M BLOCK_K b_desc = triton language make_tensor_descriptor base=B shape= N K strides= stride_bn block_shape= BLOCK_N BLOCK_K - endif tiles_per_SM = num_tiles NUM_SMS start_pid num_tiles NUM_SMS tiles_per_SM += tile_id = start_pid - NUM_SMS ki = - pid_m = pid_n = offs_am = offs_bn = num_pid_in_group = GROUP_M num_pid_n accumulator = tl zeros BLOCK_M BLOCK_N dtype=ACC_TYPE a_scale = load_scales A_inverse_scale SCALE_RECIPE_A b_scale = load_scales B_inverse_scale SCALE_RECIPE_B _ range k_tiles tiles_per_SM ki = tl where ki == k_tiles - ki + ki == tile_id += NUM_SMS group_id = tile_id num_pid_in_group first_pid_m = group_id GROUP_M group_size_m = min num_pid_m - first_pid_m GROUP_M pid_m = first_pid_m + tile_id group_size_m pid_n = tile_id num_pid_in_group group_size_m offs_am = pid_m BLOCK_M offs_bn = pid_n BLOCK_N offs_k = ki BLOCK_K - TMA_EXPERIMENTAL_API = tl _experimental_descriptor_load a_desc_ptr offs_am offs_k BLOCK_M BLOCK_K A dtype element_ty b = tl _experimental_descriptor_load b_desc_ptr offs_bn offs_k BLOCK_N BLOCK_K B dtype element_ty - = tl load_tensor_descriptor a_desc offs_am offs_k b = tl load_tensor_descriptor b_desc offs_bn offs_k - endif USE_FAST_ACCUM accumulator = tl dot b T accumulator accumulator += tl dot b T ki == k_tiles - Apply inverse scaling offs_cm = offs_am + tl arange BLOCK_M offs_cn = offs_bn + tl arange BLOCK_N Apply scaling accumulator = apply_scaling accumulator a_scale b_scale SCALE_RECIPE_A SCALE_RECIPE_B offs_cm offs_cn M N stride_a_scale_m stride_b_scale_n inductor generates suffix - TMA_EXPERIMENTAL_API idx_m = offs_cm None idx_n = offs_cn None mask = idx_m M idx_n N store_output idx_m idx_n accumulator mask indent_width= val_shape= BLOCK_M BLOCK_N - store_output offs_am offs_bn accumulator indent_width= val_shape= BLOCK_M BLOCK_N block_indexing=True - endif accumulator = tl zeros BLOCK_M BLOCK_N dtype=tl float scaled_mm_device_tma_epilogue_scaling_template = TritonTemplate name= scaled_mm_device_tma_epilogue_scaling grid=persistent_mm_grid source=scaled_mm_device_tma_epilogue_scaling + load_scales + apply_scaling blockwise xTILESIZE_scaling = r triton jit blockwise xTILESIZE_scaling pid scale ki lhs_size lhs_blocks k_blocks BLOCK_lhs tl constexpr BLOCK_K tl constexpr MIN_BLOCK_TILE_K tl constexpr TILE_SIZE tl constexpr row_offs_scale = pid BLOCK_lhs + tl arange BLOCK_lhs col_offs_scale = ki tl cdiv BLOCK_K TILE_SIZE + tl arange BLOCK_K + TILE_SIZE - TILE_SIZE ptrs = scale + row_offs_scale None k_blocks + col_offs_scale None mask = row_offs_scale None lhs_size col_offs_scale None k_blocks scale_block = tl load ptrs mask=mask other= scale_expanded = scale_block None scale_expanded = tl broadcast_to scale_expanded BLOCK_lhs BLOCK_K + TILE_SIZE - TILE_SIZE MIN_BLOCK_TILE_K scale_expanded = scale_expanded reshape BLOCK_lhs BLOCK_K + TILE_SIZE - TILE_SIZE MIN_BLOCK_TILE_K scale_expanded blockwise x _scaling = r triton jit blockwise x _scaling pid scale ki lhs_blocks k_blocks BLOCK_lhs tl constexpr BLOCK_K tl constexpr MIN_BLOCK_TILE_lhs tl constexpr MIN_BLOCK_TILE_K tl constexpr row_offs_scale = pid tl cdiv BLOCK_lhs + tl arange BLOCK_lhs + - col_offs_scale = ki tl cdiv BLOCK_K + tl arange BLOCK_K + - ptrs = scale + row_offs_scale None k_blocks + col_offs_scale None mask = row_offs_scale None lhs_blocks col_offs_scale None k_blocks scale_block = tl load ptrs mask=mask other= scale_expanded = scale_block None None scale_expanded = tl broadcast_to scale_expanded BLOCK_lhs + - BLOCK_K + - MIN_BLOCK_TILE_lhs MIN_BLOCK_TILE_K scale_expanded = scale_expanded reshape BLOCK_lhs + - MIN_BLOCK_TILE_lhs BLOCK_K + - MIN_BLOCK_TILE_K scale_expanded scaled_mm_device_tma_main_loop_scaling = r def_kernel A B A_inverse_scale B_inverse_scale M = size A N = size B K = size A M N == early exit due zero-size input s stride_am = stride A stride_bn = stride B start_pid = tl program_id axis= INDEX_DTYPE num_pid_m = tl cdiv M BLOCK_M num_pid_n = tl cdiv N BLOCK_N k_tiles = tl cdiv K BLOCK_K num_tiles = num_pid_m num_pid_n a_desc = triton language make_tensor_descriptor base=A shape= M K strides= stride_am block_shape= BLOCK_M BLOCK_K b_desc = triton language make_tensor_descriptor base=B shape= N K strides= stride_bn block_shape= BLOCK_N BLOCK_K tiles_per_SM = num_tiles NUM_SMS start_pid num_tiles NUM_SMS tiles_per_SM += tile_id = start_pid - NUM_SMS ki = - pid_m = pid_n = offs_am = offs_bn = num_pid_in_group = GROUP_M num_pid_n accumulator = tl zeros BLOCK_M BLOCK_N dtype=ACC_TYPE a_scale = load_scales A_inverse_scale SCALE_RECIPE_A b_scale = load_scales B_inverse_scale SCALE_RECIPE_B _ range k_tiles tiles_per_SM ki = tl where ki == k_tiles - ki + ki == tile_id += NUM_SMS group_id = tile_id num_pid_in_group first_pid_m = group_id GROUP_M group_size_m = min num_pid_m - first_pid_m GROUP_M pid_m = first_pid_m + tile_id group_size_m pid_n = tile_id num_pid_in_group group_size_m offs_am = pid_m BLOCK_M offs_bn = pid_n BLOCK_N offs_k = ki BLOCK_K = tl load_tensor_descriptor a_desc offs_am offs_k b = tl load_tensor_descriptor b_desc offs_bn offs_k am_blocks = tl cdiv M TILE_SIZE_A ak_blocks = tl cdiv K TILE_SIZE_A bn_blocks = tl cdiv N TILE_SIZE_B bk_blocks = tl cdiv K TILE_SIZE_B - SCALE_RECIPE_A == ScalingType Blockwise x scale_a_block = blockwise x _scaling pid_m a_scale ki am_blocks ak_blocks BLOCK_M BLOCK_K MIN_BLOCK_TILE_AM MIN_BLOCK_TILE_AK - ScalingType Blockwise xTILESIZE scale_a_block = blockwise xTILESIZE_scaling pid_m a_scale ki M am_blocks ak_blocks BLOCK_M BLOCK_K MIN_BLOCK_TILE_AK TILE_SIZE_A - endif - SCALE_RECIPE_A == ScalingType Blockwise x scale_b_block = blockwise x _scaling pid_n b_scale ki bn_blocks bk_blocks BLOCK_N BLOCK_K MIN_BLOCK_TILE_BN MIN_BLOCK_TILE_BK - ScalingType Blockwise xTILESIZE scale_b_block = blockwise xTILESIZE_scaling pid_n b_scale ki N bn_blocks bk_blocks BLOCK_N BLOCK_K MIN_BLOCK_TILE_BK TILE_SIZE_B - endif a_scaled = scale_a_block b_scaled = b scale_b_block accumulator = tl dot a_scaled b_scaled T accumulator ki == k_tiles - offs_cm = offs_am + tl arange BLOCK_M offs_cn = offs_bn + tl arange BLOCK_N inductor generates suffix store_output offs_am offs_bn accumulator indent_width= val_shape= BLOCK_M BLOCK_N block_indexing=True accumulator = tl zeros BLOCK_M BLOCK_N dtype=tl float scaled_mm_device_tma_main_loop_scaling_template = TritonTemplate name= scaled_mm_device_tma_main_loop_scaling grid=persistent_mm_grid source=scaled_mm_device_tma_main_loop_scaling + load_scales + blockwise xTILESIZE_scaling + blockwise x _scaling _compute_blackwell_pid = r triton jit _compute_pid tile_id num_pid_in_group grid_m GROUP_M tl constexpr NUM_SMS tl constexpr group_id = tile_id num_pid_in_group first_pid_m = group_id GROUP_M GROUP_M = min grid_m - first_pid_m GROUP_M pid_m = first_pid_m + tile_id GROUP_M pid_n = tile_id num_pid_in_group GROUP_M pid_m pid_n _blackwell_ws_persistent_device_tma = r def_kernel A B M = size A N = size B K = size A M N == early exit due zero-size input s start_pid = tl program_id grid_m = tl cdiv M BLOCK_M grid_n = tl cdiv N BLOCK_N k_tiles = tl cdiv K BLOCK_K num_tiles = grid_m grid_n Note We require TMA_EXPERIMENTAL_API == False which we will check before invoking template stride_am = stride A stride_ak = stride A stride_bk = stride B stride_bn = stride B a_desc = triton language make_tensor_descriptor base=A shape= M K A_ROW_MAJOR K M strides= stride_am A_ROW_MAJOR stride_ak block_shape= BLOCK_M BLOCK_K A_ROW_MAJOR BLOCK_K BLOCK_M b_desc = triton language make_tensor_descriptor base=B shape= K N B_ROW_MAJOR N K strides= stride_bk B_ROW_MAJOR stride_bn block_shape= BLOCK_K BLOCK_N B_ROW_MAJOR BLOCK_N BLOCK_K tile_id_c used epilogue break dependency between prologue epilogue tile_id_c = start_pid - NUM_SMS num_pid_in_group = GROUP_M grid_n tile_id tl range start_pid num_tiles NUM_SMS flatten=FLATTEN warp_specialize=WARP_SPECIALIZE pid_m pid_n = _compute_pid tile_id num_pid_in_group grid_m GROUP_M NUM_SMS offs_am = pid_m BLOCK_M offs_bn = pid_n BLOCK_N accumulator = tl zeros BLOCK_M BLOCK_N dtype=tl float ki range k_tiles offs_k = ki BLOCK_K = tl load_tensor_descriptor a_desc offs_am offs_k A_ROW_MAJOR offs_k offs_am b = tl load_tensor_descriptor b_desc offs_k offs_bn B_ROW_MAJOR offs_bn offs_k accumulator += tl dot A_ROW_MAJOR T b B_ROW_MAJOR b T allow_tf =ALLOW_TF tile_id_c += NUM_SMS pid_m pid_n = _compute_pid tile_id_c num_pid_in_group grid_m GROUP_M NUM_SMS offs_cm = pid_m BLOCK_M offs_cn = pid_n BLOCK_N - EPILOGUE_SUBTILE tl static_assert BLOCK_N == acc = tl reshape accumulator BLOCK_M BLOCK_N acc = tl permute acc acc acc = tl split acc store_output offs_cm offs_cn acc indent_width= val_shape= BLOCK_M BLOCK_N block_indexing=True offs_cn = offs_cn + BLOCK_N store_output offs_cm offs_cn acc indent_width= val_shape= BLOCK_M BLOCK_N block_indexing=True - store_output offs_cm offs_cn accumulator indent_width= val_shape= BLOCK_M BLOCK_N block_indexing=True - endif blackwell_ws_persistent_device_tma_mm_template = TritonTemplate name= blackwell_ws_persistent_device_tma grid=persistent_mm_grid source=_blackwell_ws_persistent_device_tma + _compute_blackwell_pid prevent duplication registration extern functions functools cache lazy_register_extern_choice fn ExternKernelChoice fn aten_mm = ExternKernelChoice torch mm mm_out op_overload=aten mm out aten_mm_dtype = ExternKernelChoice torch mm _mm_dtype_out_cuda name= mm_dtype op_overload=aten mm dtype_out aten_addmm = ExternKernelChoice torch addmm addmm_out op_overload=aten addmm out aten__int_mm = ExternKernelChoice torch _int_mm _int_mm_out op_overload=aten _int_mm out aten__sparse_semi_structured_mm = ExternKernelChoice torch _sparse_semi_structured_mm _sparse_semi_structured_mm has_out_variant=False op_overload=aten _sparse_semi_structured_mm default aten__fp _mm = ExternKernelChoice torch _scaled_mm _scaled_mm_out op_overload=aten _scaled_mm out _is_int _mat mat mat get_dtype torch int torch uint bias_addmm inp mat mat out=None alpha= beta= Giving torch addmm D tensor calls different faster cublasLt kernel under hood There few shapes where slower they rare inp stride == inp size = inp size == torch addmm inp mat mat out=out alpha=alpha beta=beta torch addmm inp mat mat out=out alpha=alpha beta=beta check_supported_striding mat_a mat_b - None is_row_major stride - bool V graph sizevars statically_known_equals stride is_col_major stride - bool V graph sizevars statically_known_equals stride has_zero_dim size - bool bool V graph sizevars statically_known_equals size V graph sizevars statically_known_equals size Check mat_a stride requirements torch _check is_row_major mat_a get_stride has_zero_dim mat_a get_size lambda f mat_a must row_major got stride mat_a get_stride Check mat_b stride requirements torch _check is_col_major mat_b get_stride has_zero_dim mat_b get_size lambda f mat_b must col_major got stride mat_b get_stride aten_bias_addmm = ExternKernelChoice bias_addmm None decomposeK b k_splits m = shape n = b shape k = shape k_parts = k k_splits B = k_splits a_reshaped = torch permute reshape m B k_parts b_reshaped = b reshape B k_parts n result = torch bmm a_reshaped b_reshaped out_dtype=torch float reduced_buf = torch sum result reduced_buf dtype DecomposeKSugraphTemplate SubgraphTemplate __init__ super __init__ name= decompose_k generate type ignore override input_nodes list Buffer layout Layout k_split int - SubgraphChoiceCaller torch _dispatch python enable_python_dispatcher decomposition select_decomp_table name = f decompose_k_mm_ k_split _split description = f k_split= enable_python_dispatcher decompositions = select_decomp_table fn = make_fx functools partial decomposeK k_splits=k_split decompositions super generate name=name input_nodes=input_nodes layout=layout make_fx_graph=fn description=description decompose_k_subgraph_template = DecomposeKSugraphTemplate ContiguousTemplate SubgraphTemplate __init__ name str description str fn Any name = name description = description fn = fn super __init__ name=name generate type ignore override input_nodes list Buffer layout Layout - SubgraphChoiceCaller torch _dispatch python enable_python_dispatcher decomposition select_decomp_table enable_python_dispatcher decompositions = select_decomp_table fn = make_fx fn decompositions super generate name=self name input_nodes=input_nodes layout=layout make_fx_graph=fn description=self description contiguous_mm b torch mm b contiguous contiguous_addmm inp b torch addmm inp b contiguous mm_contiguous_subgraph_template = ContiguousTemplate contiguous_mm contiguous mm contiguous_mm addmm_contiguous_subgraph_template = ContiguousTemplate contiguous_addmm contiguous addmm contiguous_addmm register_lowering aten mm type_promotion_kind=None tuned_mm mat mat out_dtype=None layout=None Lowering autotuning aten mm different backends Aten Triton CUTLASS etc out_dtype None input_dtype = mat get_dtype torch _check mat get_dtype == input_dtype lambda input dtypes must same torch _check mat get_device type == cuda lambda out_dtype only supported CUDA torch _check out_dtype == input_dtype out_dtype == torch float input_dtype torch float torch bfloat lambda out_dtype must same input dtype fp fp bf inputs Lower matmul-related operations e g torch matmul torch bmm torch addmm into native matmul IR using ` ops dot ` When we see matmul pattern C y x = A y r B r x core idea emulate broadcasted multiply followed sum For example given ` C = torch matmul A B ` can rewritten Prod = A unsqueeze - B unsqueeze C = Prod sum dim= Instead explicitly using ` ops mul ` ` ops reduction sum ` we lower these into ` ops dot ` pointwise ` ops reduction dot ` These IR nodes semantically equivalent ` ops mul ` + ` ops reduction sum ` combination lowered ` tl dot ` during code generation phase use_native_matmul mat mat mat = lowerings aten unsqueeze mat - mat = lowerings aten unsqueeze mat args kwargs = transform_args args= mat mat kwargs= broadcast=True type_promotion_kind=None convert_input_to_bool=False Handles broadcasting arguments inductor_config triton codegen_upcast_to_fp mat dtype torch float torch bfloat _to_dtype x ops to_dtype x mat dtype use_compute_types=False args = make_pointwise _to_dtype x x args mul_pointwise = make_pointwise ops dot args dot_reduction = make_reduction dot mul_pointwise dot_reduction TODO coconutruben integrate into MMKernelInputs when all callsites use m n k layout mat mat = mm_args mat mat layout=layout out_dtype=out_dtype static_shape is_nonzero = _is_static_problem layout name = mm Create MMKernelInputs standard MM top kernel_inputs = MMKernelInputs mat mat out_dtype=out_dtype below getting overview logging info inductor mms counters aten_mm_info f aten mm_ m _ n _ k += log info Tuned aten mm m= s n= s k= s mat _dtype= s mat _dtype= s output_layout= s m n k mat get_dtype mat get_dtype layout choices list ChoiceCaller = static_shape is_nonzero = _is_static_problem layout aten_handler ExternKernelChoice = aten_mm aten_extra_kwargs dict str Any = out_dtype None aten_handler = aten_mm_dtype aten_extra_kwargs = out_dtype out_dtype templates_to_use list Union ExternKernelChoice KernelTemplate = kwarg_overrides dict str dict str Any = use_aten_gemm_kernels templates_to_use append aten_handler aten_extra_kwargs kwarg_overrides aten_handler uid = aten_extra_kwargs out_dtype None is_nonzero use_triton_template layout check_max_autotune=True use_decompose_k_choice m n k templates_to_use append decompose_k_subgraph_template Triton Templates typically perform very poorly large K Its highly unlikely we want use decompose_k then Triton will ever win To conservative we increase threshold N M is_exhaustive = inductor_config max_autotune_gemm_search_space == exhaustive is_exhaustive use_decompose_k_choice m n k threshold_multiple= templates_to_use append mm_template use_triton_tma_template mat mat output_layout=layout templates_to_use append persistent_tma_mm_template use_triton_blackwell_tma_template mat mat output_layout=layout templates_to_use append blackwell_ws_persistent_device_tma_mm_template templates_to_use append mm_contiguous_subgraph_template choices extend V choices get_template_configs kernel_inputs templates_to_use mm kwarg_overrides=kwarg_overrides out_dtype None is_nonzero use_cutlass_template layout m n k _use_cutlass_for_op mm CUTLASS xGemmTemplate add_cutlass_gemm_choices choices layout kernel_inputs nodes out_dtype None is_nonzero use_ck_gemm_template layout m n k CKGemmTemplate add_ck_gemm_choices choices layout kernel_inputs nodes out_dtype None is_nonzero use_ck_tile_gemm_template layout m n k CKTileGemmTemplate add_choices choices layout kernel_inputs nodes out_dtype None use_cpp_gemm_template layout mat mat CppGemmTemplate add_choices choices layout kernel_inputs nodes input_nodes = mat mat out_dtype None is_nonzero use_triton_template layout torch _inductor config run_autoheuristic name is_triton mat always_included = use_aten_gemm_kernels always_included append extern_mm num_choices_before_extra_configs = len choices choices extend V choices get_template_configs TODO coconutruben remove once we deprecate ah mm-extra hack keep ah functionality alive while we transition unified kwargs retrieval kernel_inputs mm_template mm-ah using AutoHeuristic ranking ah_choices = mm_autoheuristic mat mat m n k choices name input_nodes mm_operations None top_k= always_included=always_included torch _inductor config collect_autoheuristic name we collecting data we do want modify choices ah_choices None len ah_choices order which autoheuristic returns choices same order choices which affects things like epilogue fusion once epilogue fusion benchmarks choices sorted order I think we can just use order returned autoheuristic choices = choice choice choices choice ah_choices choices = choices num_choices_before_extra_configs out_dtype None k inductor_config external_matmul choices append lazy_register_extern_choice k bind kernel_inputs nodes layout best_config_future = None out_dtype None torch _inductor config remote_gemm_autotune_cache Purposely awaiting future here - kicks off best config lookup lowering time The future will awaited scheduling time select_algorithm py best_config_future = gen_best_config mat mat autotune_select_algorithm name choices kernel_inputs nodes layout best_config_future=best_config_future register_lowering aten _int_mm type_promotion_kind=None tuned_int_mm mat mat layout=None TODO coconutruben integrate into MMKernelInputs when all callsites use m n k layout mat mat = mm_args mat mat layout=layout out_dtype=torch int name = int_mm below getting overview logging info inductor mms counters aten_mm_info f aten _int_mm_ m _ n _ k += log info Tuned aten _int_mm m= s n= s k= s mat _dtype= s mat _dtype= s output_layout= s m n k mat get_dtype mat get_dtype layout static_shape is_nonzero = _is_static_problem layout use_cutlass = static_shape is_nonzero use_cutlass_template layout m n k choices list ChoiceCaller = Create MMKernelInputs Int MM kernel_inputs = MMKernelInputs mat mat out_dtype=torch int Collect all templates unified call templates_to_use list Union ExternKernelChoice KernelTemplate = use_aten_gemm_kernels templates_to_use append aten__int_mm is_nonzero use_triton_template layout enable_int =True check_max_autotune=False templates_to_use append mm_template Single unified call all templates choices extend V choices get_template_configs kernel_inputs templates_to_use name use_cutlass _use_cutlass_for_op name CUTLASS xGemmTemplate add_cutlass_gemm_choices choices layout kernel_inputs nodes fuseable=True non_fuseable=True autotune_select_algorithm name choices kernel_inputs nodes layout register_lowering aten addmm type_promotion_kind=None tuned_addmm inp mat mat alpha= beta= layout=None Lowering autotuning aten addmm different backends Aten Triton CUTLASS etc use_native_matmul mat mat beta == arg = arg = lowerings aten mul beta inp alpha == arg = arg = lowerings aten mul alpha lowerings aten mm mat mat lowerings aten add arg arg TODO coconutruben integrate into MMKernelInputs when all callsites use m n k layout mat mat inp_expanded = mm_args mat mat inp layout=layout static_shape is_nonzero = _is_static_problem layout name = addmm Create MMKernelInputs AddMM top kernel_inputs = MMKernelInputs inp_expanded mat mat scalars=dict alpha=alpha beta=beta choices list ChoiceCaller = below getting overview logging info inductor mms counters aten_mm_info f aten addmm_ m _ n _ k += log info Tuned aten addmm m= s n= s k= s mat _dtype= s mat _dtype= s output_layout= s m n k mat get_dtype mat get_dtype layout is_nonzero inductor_config max_autotune inductor_config max_autotune_gemm TODO coconutruben combine main flow addmm through subgraph something inp vs inp_expanded causes some slight numeric differences kernel_inputs = MMKernelInputs inp mat mat scalars=dict alpha=alpha beta=beta choices extend V choices get_template_configs kernel_inputs aten_addmm name autotune_select_algorithm name choices kernel_inputs nodes layout Collect all templates unified call templates_to_use list Union ExternKernelChoice KernelTemplate = use_aten_gemm_kernels templates_to_use extend aten_bias_addmm aten_addmm is_nonzero use_triton_template layout check_max_autotune=False templates_to_use append mm_template use_triton_tma_template mat mat output_layout=layout templates_to_use append persistent_tma_mm_template use_triton_blackwell_tma_template mat mat output_layout=layout templates_to_use append blackwell_ws_persistent_device_tma_mm_template templates_to_use append addmm_contiguous_subgraph_template Single unified call all templates choices extend V choices get_template_configs kernel_inputs templates_to_use name is_nonzero use_cutlass_template layout m n k _use_cutlass_for_op name CUTLASS xGemmTemplate add_cutlass_gemm_choices choices layout reorder here because CUTLASS expects x w bias torch bias x w kernel_inputs nodes reorder= alpha=alpha beta=beta is_nonzero use_ck_gemm_template layout m n k CKGemmTemplate add_ck_gemm_choices choices layout reorder here because CK expects x w bias torch bias x w kernel_inputs nodes reorder= alpha=alpha beta=beta input_reorder= use_cpp_gemm_template layout mat mat CppGemmTemplate add_choices choices layout kernel_inputs nodes alpha=alpha beta=beta has_bias=True autotune_select_algorithm name choices kernel_inputs nodes layout register_lowering aten _sparse_semi_structured_mm type_promotion_kind=None tuned_sparse_semi_structured_mm mat mat _meta mat out_dtype=None layout=None torch _inductor select_algorithm realize_inputs TODO coconturuben support V choices get_mm_configs sparse_semi_structured_mm mat mat _meta mat = realize_inputs mat mat _meta mat m k = mat get_size m _ = mat _meta get_size k n = mat get_size m = V graph sizevars check_equals_and_simplify m m k = V graph sizevars check_equals_and_simplify k k layout None torch _inductor ir FixedLayout layout = FixedLayout mat get_device out_dtype out_dtype mat get_dtype m n n assert out_dtype None out_dtype ignored layout specified choices = aten__sparse_semi_structured_mm bind mat mat _meta mat layout out_dtype=out_dtype use_aten_gemm_kernels m n = use_cutlass_template layout m n k _use_cutlass_for_op sparse_semi_structured_mm CUTLASS xGemmTemplate add_cutlass_gemm_choices choices layout mat mat mat _meta fuseable=True non_fuseable=True autotune_select_algorithm sparse_semi_structured_mm choices mat mat _meta mat layout scaling_pairs = ScalingType TensorWise ScalingType TensorWise ScalingType RowWise ScalingType RowWise ScalingType BlockWise x ScalingType BlockWise x epilogue_scaling_types = ScalingType TensorWise ScalingType RowWise main_loop_scaling_types = ScalingType BlockWise x ScalingType BlockWise x _is_tensorwise_scaling sz Any - bool len sz == all V graph sizevars statically_known_equals d d sz _is_rowwise_scaling sz Any transpose bool - bool idx = transpose - V graph sizevars statically_known_equals sz idx _is_blockwise xTILESIZE_scaling sz Any tensor_sz Any tile_size int - bool V graph sizevars statically_known_equals sz tensor_sz V graph sizevars statically_known_equals sz ceildiv tensor_sz tile_size _is_blockwise x _scaling sz Any tensor_sz Any - bool V graph sizevars statically_known_equals sz ceildiv tensor_sz V graph sizevars statically_known_equals sz ceildiv tensor_sz is_desired_scaling t Any scale_size torch Tensor scaling_type ScalingType transpose bool = False - bool match scaling_type case ScalingType TensorWise _is_tensorwise_scaling scale_size case ScalingType RowWise _is_rowwise_scaling scale_size transpose case ScalingType BlockWise x _is_blockwise xTILESIZE_scaling scale_size t get_size case ScalingType BlockWise x _is_blockwise x _scaling scale_size t get_size case _ raise AssertionError f Unsupported scaling type scaling_type get_tile_size scale_option - int match scale_option case ScalingType BlockWise x case ScalingType BlockWise x case _ raise AssertionError f Unsupported scaling type scale_option get_tile_size get_scaling_options mat_a Any mat_b Any scale_a_size torch Tensor scale_b_size torch Tensor - tuple ScalingType ScalingType scale_option_a scale_option_b scaling_pairs is_desired_scaling mat_a scale_a_size scale_option_a is_desired_scaling mat_b scale_b_size scale_option_b transpose=True scale_option_a scale_option_b raise AssertionError f Inductor Triton does support scale_a shape = scale_a_size scale_b shape = scale_b_size verify shapes supported least one existing pairing register_lowering aten _scaled_mm default type_promotion_kind=None type ignore misc tuned_scaled_mm mat_a mat_b scale_a scale_b bias=None scale_result=None out_dtype=None use_fast_accum=False layout=None Performs optimized matrix multiplication where scaling factors applied inputs output Args mat Tensor First input matrix mat Tensor Second input matrix scale Tensor Scale factor applied mat supports broadcasting scale Tensor Scale factor applied mat supports broadcasting bias Tensor optional Optional bias tensor add result layout Layout hint optimization Returns Tensor The result scaled matrix multiplication TODO coconutruben integrate into MMKernelInputs when all callsites use m n k layout mat_a mat_b = mm_args mat_a mat_b layout=layout out_dtype=out_dtype below getting overview logging info inductor mms counters aten_mm_info f aten _scaled_mm default_ m _ n _ k += log info Tuned aten _scaled_mm default m= s n= s k= s mat _dtype= s mat _dtype= s output_layout= s m n k mat_a get_dtype mat_b get_dtype layout name = scaled_mm check_supported_striding mat_a mat_b scale_a_real scale_b_real = realize_inputs scale_a scale_b input_nodes list Any bias input_nodes = mat_a mat_b scale_a_real scale_b_real bias_real = realize_inputs bias input_nodes = mat_a mat_b scale_a_real scale_b_real bias_real Create MMKernelInputs Scaled MM matrices indices kernel_inputs = MMKernelInputs input_nodes mat _idx= mat _idx= out_dtype=out_dtype choices list ChoiceCaller = Collect all templates unified call templates_to_use list Union ExternKernelChoice KernelTemplate = kwarg_overrides = use_aten_gemm_kernels templates_to_use append aten__fp _mm kwarg_overrides aten__fp _mm uid = dict out_dtype=out_dtype use_fast_accum=use_fast_accum _ is_nonzero = _is_static_problem layout We dont have triton lowerings MX variants yet scale_a dtype == torch float is_nonzero use_triton_template layout enable_float =True check_max_autotune=False overriders = dict USE_FAST_ACCUM=use_fast_accum TODO paulzhan There no template exists bias TMA Don t run tma template currently bias exist use_triton_tma_template mat_a mat_b output_layout=layout bias scale_a_size scale_b_size = scale_a_real shape scale_b_real shape scale_option_a scale_option_b = get_scaling_options mat_a mat_b scale_a_size scale_b_size overriders SCALE_RECIPE_A = scale_option_a value overriders SCALE_RECIPE_B = scale_option_b value scale_option_a epilogue_scaling_types scale_option_b epilogue_scaling_types templates_to_use append scaled_mm_device_tma_epilogue_scaling_template kwarg_overrides scaled_mm_device_tma_epilogue_scaling_template uid = overriders scale_option_a main_loop_scaling_types scale_option_b main_loop_scaling_types overriders TILE_SIZE_A = get_tile_size scale_option_a overriders TILE_SIZE_B = get_tile_size scale_option_b templates_to_use append scaled_mm_device_tma_main_loop_scaling_template kwarg_overrides scaled_mm_device_tma_main_loop_scaling_template uid = overriders raise AssertionError Inductor Triton does support scaling options present + both epilogue scaling main loop scaling use_triton_blackwell_tma_template mat_a mat_b output_layout=layout bias templates_to_use append blackwell_ws_persistent_device_tma_mm_template kwarg_overrides blackwell_ws_persistent_device_tma_mm_template uid = overriders templates_to_use append mm_template kwarg_overrides mm_template uid = overriders Single unified call all templates choices extend V choices get_template_configs kernel_inputs templates_to_use name kwarg_overrides=kwarg_overrides Early MX variants scale_a dtype = torch float autotune_select_algorithm name choices input_nodes layout is_nonzero use_cutlass_template layout m n k _use_cutlass_for_op name CUTLASS xGemmTemplate add_cutlass_gemm_choices choices layout kernel_inputs nodes type ignore arg-type use_fast_accum=use_fast_accum type ignore arg-type is_nonzero use_ck_gemm_template layout m n k CKGemmTemplate add_ck_gemm_choices choices layout kernel_inputs nodes autotune_select_algorithm name choices kernel_inputs nodes layout functools cache _is_sm x_or_older_gpu index Optional int - bool props = torch cuda get_device_properties index props major = dims_are_int dims all isinstance dim int dim dims mm_autoheuristic mat mat m n k choices name input_nodes ops precondition top_k Optional int = None always_included=None m n k = get_size_hints mat mat m n k dims_are_int m n k None mat _stride mat _stride = get_size_hints_strides mat mat get_context m k n mat mat mat _stride mat _stride context = AHContext context add_feature m m context add_feature k k context add_feature n n context add_feature mat _dtype mat layout dtype is_categorical=True context add_feature mat _dtype mat layout dtype is_categorical=True context_add_strides context mat mat _stride context_add_strides context mat mat _stride context add_feature mat _iscontig mat layout is_contiguous is_categorical=True context add_feature mat _iscontig mat layout is_contiguous is_categorical=True name == mm context_add_using_tf context mat layout dtype context fallback None context = get_context m k n mat mat mat _stride mat _stride autoheuristic = AutoHeuristicSelectAlgorithm fallback=fallback choices=choices input_nodes=input_nodes context=context name=name augment_context=ops precondition=precondition top_k None TODO there cleaner way ensure aten mm always included autoheuristic get_top_k_choices_caller top_k always_included=always_included autoheuristic get_choice_caller get_size_hints mat mat m n k isinstance m int isinstance k int m k = V graph sizevars size_hints mat get_size fallback=torch _inductor config unbacked_symint_fallback isinstance n int isinstance k int k n = V graph sizevars size_hints mat get_size fallback=torch _inductor config unbacked_symint_fallback m n k get_size_hints_strides mat mat mat _stride = mat layout stride mat _stride = mat layout stride strides = mat _stride mat _stride strides_hints = stride strides isinstance stride int stride = V graph sizevars size_hints stride fallback=torch _inductor config unbacked_symint_fallback strides_hints append stride strides_hints strides_hints