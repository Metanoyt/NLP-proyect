Owner s oncall distributed sys torch torch distributed fsdp _traversal_utils traversal_utils torch distributed dist torch distributed fsdp CPUOffload FullyShardedDataParallel FSDP MixedPrecision torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_distributed skip_if_lt_x_gpu torch testing _internal common_fsdp DEVICEInitMode FSDPInitMode FSDPTest get_devtype NestedWrappedModule torch testing _internal common_utils run_tests TEST_WITH_DEV_DBG_ASAN device_type = torch device get_devtype dist is_available print Distributed available skipping tests file=sys stderr sys exit TEST_WITH_DEV_DBG_ASAN print Skip dev-asan torch + multiprocessing spawn have known issues file=sys stderr sys exit TestPureFP FSDPTest skip_if_lt_x_gpu test_pure_fp _training Tests pure FP training including when parameter s dtype changed after FSDP initialization before training run_subtests cpu_offload CPUOffload offload_params=True CPUOffload offload_params=False _test_pure_fp _training _test_pure_fp _training cpu_offload CPUOffload _test_fsdp_parity NestedWrappedModule FSDPInitMode RECURSIVE device_init_mode=DEVICEInitMode DEVICE_BEFORE Run one iteration avoid NaN without gradient scaler num_iters= cpu_offload=cpu_offload use_pure_fp =True skip_if_lt_x_gpu test_fp _dtypes Tests both user-facing parameter gradient dtypes internal saved dtype attributes expected when using FP model possibly explicit mixed precision enabled run_subtests to_half_before_fsdp_init False True use_orig_params False True mixed_precision MixedPrecision MixedPrecision param_dtype=torch float reduce_dtype=torch float MixedPrecision param_dtype=torch float _test_fp _dtypes _test_fp _dtypes to_half_before_fsdp_init bool use_orig_params bool mixed_precision MixedPrecision model = NestedWrappedModule init process_group FSDPInitMode NO_FSDP DEVICEInitMode DEVICE_NEVER device_id device_type fsdp_kwargs = use_orig_params use_orig_params device_id device_type mixed_precision mixed_precision to_half_before_fsdp_init model = model half fsdp_model = FSDP model fsdp_kwargs to_half_before_fsdp_init fsdp_model = fsdp_model half param fsdp_model parameters assertEqual param dtype torch float inp = tuple t half torch is_tensor t t t fsdp_model module get_input device_type out = fsdp_model inp out sum backward Check handle dtype attributes handle traversal_utils _get_fsdp_handles fsdp_model assertEqual handle flat_param dtype torch float assertEqual handle flat_param grad dtype torch float assertEqual handle _orig_param_dtype torch float Specifying ` mixed_precision ` takes precedence over model dtype both ` param_dtype ` ` reduce_dtype ` mixed_precision param_dtype None assertEqual handle _fwd_bwd_param_dtype mixed_precision param_dtype assertEqual handle _fwd_bwd_param_dtype torch float mixed_precision reduce_dtype None assertEqual handle _reduce_dtype mixed_precision reduce_dtype mixed_precision reduce_dtype None mixed_precision param_dtype None Special case infer reduce dtype parameter dtype assertEqual handle _reduce_dtype mixed_precision param_dtype assertEqual handle _reduce_dtype torch float Check parameter gradient dtypes param fsdp_model parameters assertEqual param dtype torch float param grad None assertEqual param grad dtype torch float devices = cuda hpu xpu instantiate_device_type_tests TestPureFP globals only_for=devices allow_xpu=True __name__ == __main__ run_tests