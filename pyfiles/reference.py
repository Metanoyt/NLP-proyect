mypy allow-untyped-defs math operator typing Union sympy torch torch utils _sympy functions _keep_float BitwiseFn_bitwise_and BitwiseFn_bitwise_or FloatPow FloatTrueDiv FloorDiv IntTrueDiv Max Min Mod OpaqueUnaryFn_exp OpaqueUnaryFn_log OpaqueUnaryFn_log OpaqueUnaryFn_sqrt PowByNatural RoundDecimal RoundToInt ToFloat TruncToInt The sympy interpretation operators It will also sometimes work plain int float you do certain operations you will get out sympy Basic end If you want Python FX traceable interpretation check PythonReferenceAnalysis NB For magic methods needs use normal magic methods so test_magic_methods works ReferenceAnalysis staticmethod constant c dtype sympy sympify c staticmethod or_ b &#124; b staticmethod and_ b b staticmethod eq b isinstance sympy Expr isinstance b sympy Expr sympy Eq b == b classmethod ne cls b cls not_ cls eq b staticmethod lt b b staticmethod gt b b staticmethod le b = b staticmethod ge b = b staticmethod not_ isinstance bool raise AssertionError not_ needs sympy expr ~a staticmethod reciprocal x FloatTrueDiv x staticmethod square x PowByNatural x staticmethod trunc_to_int x dtype TruncToInt x staticmethod ceil_to_int x dtype sympy ceiling x staticmethod floor_to_int x dtype sympy floor x staticmethod floor x _keep_float sympy floor x staticmethod ceil x _keep_float sympy ceiling x staticmethod to_dtype x dtype dtype == torch float ToFloat x raise NotImplementedError f to_dtype dtype NYI staticmethod mod x y Mod x y staticmethod abs x abs x staticmethod neg x -x staticmethod truediv b FloatTrueDiv b staticmethod int_truediv b IntTrueDiv b staticmethod floordiv b FloorDiv b staticmethod truncdiv b raise NotImplementedError TODO truncdiv staticmethod add b _keep_float operator add b classmethod sym_sum cls args sympy Add args staticmethod mul b _keep_float operator mul b staticmethod sub b _keep_float operator sub b staticmethod exp x OpaqueUnaryFn_exp x staticmethod log x OpaqueUnaryFn_log x staticmethod log x OpaqueUnaryFn_log x staticmethod sqrt x OpaqueUnaryFn_sqrt x staticmethod pow b pyrefly ignore bad-argument-type _keep_float FloatPow b staticmethod pow_by_natural b PowByNatural b staticmethod minimum b Min b staticmethod maximum b Max b staticmethod round_to_int dtype RoundToInt staticmethod round_decimal b RoundDecimal b staticmethod bitwise_and b BitwiseFn_bitwise_and b staticmethod bitwise_or b BitwiseFn_bitwise_or b Unlike ReferenceAnalysis does NOT sympyify instead works plain Python types FX traceable Inheritance here purely code sharing TODO considering splitting out BaseReferenceAnalysis PythonReferenceAnalysis ReferenceAnalysis staticmethod constant c dtype dtype torch int int c dtype torch double float c dtype torch bool bool c raise AssertionError f unrecognized dtype dtype staticmethod not_ torch sym_not classmethod sym_sum cls args len args == len args == args acc = cls add args args i range len args acc = cls add acc args i acc staticmethod floordiv b b staticmethod mod x y x y staticmethod truncdiv b b staticmethod to_dtype x dtype dtype == torch float torch sym_float x raise NotImplementedError f to_dtype dtype NYI staticmethod exp x raise AssertionError exp valid shape sympy expr staticmethod log x raise AssertionError log valid shape sympy expr staticmethod log x torch _sym_log x type ignore attr-defined staticmethod sqrt x torch _sym_sqrt x type ignore attr-defined staticmethod minimum b torch sym_min b staticmethod maximum b torch sym_max b staticmethod floor_to_int x dtype math floor x staticmethod ceil_to_int x dtype math ceil x staticmethod floor x float math floor x staticmethod ceil x float math ceil x staticmethod truediv b b staticmethod pow b b staticmethod pow_by_natural b Pray safe_pow needed here lol In particular never participates VR low high ranges so overflow should unlikely b staticmethod round_to_int dtype round staticmethod round_decimal b round ndigits=b staticmethod bitwise_and b b staticmethod bitwise_or b &#124; b Like PythonReferenceAnalysis some export-unfriendly choices operators make things faster OptimizedPythonReferenceAnalysis PythonReferenceAnalysis staticmethod sym_sum args torch sym_sum args _to_dtype x torch Tensor dtype torch dtype - torch Tensor torch ops prims convert_element_type default x dtype Suppose we have some int float arguments This diagram commutes int float -- PythonReferenceAnalysis op -- int float &#124; &#124; &#124; &#124; torch tensor dtype=torch int torch float &#124; &#124; V V Tensor -- TensorReferenceAnalysis op -- Tensor NB int before after must representable int we will insert guards accordingly This guaranteed FX traceable OpOverloads only TensorReferenceAnalysis NB This actually dead because Proxy tracing factory function isn t traced correctly Here completeness staticmethod constant c dtype d Union int float bool dtype torch int d = int c dtype torch double d = float c dtype torch bool d = bool c raise AssertionError f unrecognized dtype dtype torch ops aten scalar_tensor default d dtype=dtype staticmethod or_ b torch ops aten logical_or default b staticmethod and_ b torch ops aten logical_and default b staticmethod bitwise_and b torch ops aten bitwise_and b staticmethod bitwise_or b torch ops aten bitwise_or b staticmethod eq b torch ops aten eq Tensor b classmethod ne cls b torch ops aten ne Tensor b staticmethod lt b torch ops aten lt Tensor b staticmethod gt b torch ops aten gt Tensor b staticmethod le b torch ops aten le Tensor b staticmethod ge b torch ops aten ge Tensor b staticmethod not_ torch ops aten logical_not default staticmethod reciprocal x torch ops aten reciprocal default x staticmethod square x TODO maybe composite implicit autograd doesn t work here torch ops aten square default x staticmethod trunc_to_int x dtype _to_dtype torch ops aten trunc default x dtype staticmethod ceil_to_int x dtype _to_dtype torch ops aten ceil default x dtype staticmethod floor_to_int x dtype _to_dtype torch ops aten floor default x dtype staticmethod floor x torch ops aten floor default x staticmethod ceil x torch ops aten ceil default x staticmethod to_dtype x dtype _to_dtype x dtype staticmethod mod x y TODO https github com pytorch pytorch pull raise NotImplementedError no C-style modulus operation available frontend atm staticmethod abs x torch ops aten abs default x staticmethod neg x torch ops aten neg default x staticmethod truediv b torch ops aten true_divide Tensor b staticmethod int_truediv b raise NotImplementedError Python int truediv difficult implement PyTorch atm TODO This wrong CPython has custom implementation true division results higher precision when floats sufficiently large Short term fix add guard here torch ops aten true_divide default _to_dtype torch float _to_dtype b torch float staticmethod floordiv b torch ops aten div Tensor_mode b rounding_mode= floor staticmethod truncdiv b raise NotImplementedError no C-style truncdiv operation available frontend atm staticmethod add b torch ops aten add Tensor b staticmethod mul b torch ops aten mul Tensor b staticmethod sub b torch ops aten sub Tensor b staticmethod exp x torch ops aten exp default x staticmethod log x torch ops aten log default x staticmethod log x torch ops aten log default x staticmethod sqrt x torch ops aten sqrt default x staticmethod sin x torch ops aten sin default x staticmethod cos x torch ops aten cos default x staticmethod tanh x torch ops aten tanh default x staticmethod sinh x torch ops aten sinh default x staticmethod cosh x torch ops aten cosh default x staticmethod tan x torch ops aten tan default x staticmethod acos x torch ops aten acos default x staticmethod atan x torch ops aten atan default x staticmethod asin x torch ops aten asin default x staticmethod pow b torch ops aten pow Tensor_Tensor b staticmethod pow_by_natural b NB pow handles int x int fine torch ops aten pow Tensor_Tensor b staticmethod minimum b torch ops aten minimum default b staticmethod maximum b torch ops aten maximum default b staticmethod round_to_int dtype torch ops aten round default staticmethod round_decimal b raise NotImplementedError round decimal doesn t support Tensor second argument atm torch ops aten round decimals b