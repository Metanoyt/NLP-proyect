Owner s module dynamo copy re unittest textwrap dedent unittest mock patch torch torch _dynamo torch _dynamo test_case torch _inductor test_case torch fx traceback fx_traceback torch utils _pytree pytree torch _dynamo testing CompileCounter CompileCounterWithBackend expectedFailureDynamic rand_strided torch _functorch aot_autograd _aot_export_function create_functional_call torch _guards CompileContext StorageOverlap TracingContext torch _subclasses fake_tensor FakeTensorMode torch fx experimental proxy_tensor make_fx torch profiler profile torch testing FileCheck torch testing _internal common_utils compare_equal_outs_and_grads maybe_dupe_op x y = x + z = x + x numel y y y z is_dynamic_shape_test test_name test_name endswith _dynamic_shapes aten = torch ops aten lib = torch library Library custom DEF noqa TOR lib define maybe_dupe_op Tensor - Tensor Tensor lib impl maybe_dupe_op maybe_dupe_op CPU lib impl maybe_dupe_op maybe_dupe_op Meta AotAutogradFallbackTests torch _inductor test_case TestCase test_LSTM https github com pytorch torchdynamo issues Repro torch nn Module __init__ - None super __init__ self_mod_model_lstm_lstm = torch nn LSTM num_layers= bidirectional=True forward permute torch Tensor self_mod_model_lstm_lstm = self_mod_model_lstm_lstm permute self_mod_model_lstm_lstm mod = Repro aot_mod = torch compile mod backend= aot_eager args = torch float cpu False args = rand_strided sh st dt dev requires_grad_ rg sh st dt dev rg args eager_result = mod args aot_result = aot_mod args assertTrue torch _dynamo testing same eager_result aot_result test_mutation https github com pytorch torchdynamo issues fn param y prev_grad = torch is_grad_enabled try torch set_grad_enabled False param add_ y finally torch set_grad_enabled prev_grad y y = torch randn x = torch nn Parameter torch randn aot_fn = torch compile fn backend= aot_eager This should error we mutated autograd leaf under no_grad mode aot_fn x y test_mutation fn _stack torch Tensor diagonal_chunked_attention_scores torch Tensor getitem = diagonal_chunked_attention_scores slice None None None slice None None None slice None None slice None None _stack slice None None None slice None - None slice None None None slice None None = getitem view = _stack view view x = torch randn torch Size y = torch randn torch Size aot_fn = torch compile fn backend= aot_eager aot_fn x y test_negative_testing_mutation fn _stack torch Tensor diagonal_chunked_attention_scores torch Tensor getitem = diagonal_chunked_attention_scores slice None None None slice None None None slice None None slice None None _stack = torch sin _stack _stack slice None None None slice None - None slice None None None slice None None = getitem view = _stack view view x = torch randn torch Size y = torch randn torch Size aot_fn = torch compile fn backend= aot_eager aot_fn x y test_negative_testing fn x y torch sin x add_ y y = torch randn x = torch randn aot_fn = torch compile fn backend= aot_eager aot_fn x y test_call_fn_with_non_const_inputs_aot_safe ModuleSpecialFwd torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= _conv_forward x conv _conv_forward x conv weight conv bias forward x _conv_forward x Init mod mod = ModuleSpecialFwd rx = torch randn Run real real = mod rx Run export graph _ = torch _dynamo export mod rx Run exported graph AOT assertTrue torch _dynamo testing same real graph rx aot_fn = torch compile graph backend= aot_eager aot_fn rx test_call_fn_with_non_const_inputs_aot_unsafe ModuleSpecialFwd torch nn Module _some_bad_fwd param y prev_grad = torch is_grad_enabled try torch set_grad_enabled False param add_ y finally torch set_grad_enabled prev_grad y forward x y _some_bad_fwd x y Init mod mod = ModuleSpecialFwd x = torch nn Parameter torch randn y = torch randn Run real real = mod x y Run export graph _ = torch _dynamo export mod x y Assert equal assertTrue torch _dynamo testing same real graph x y Run exported graph AOT aot_fn = torch compile graph backend= aot_eager This should error we mutated autograd leaf under no_grad mode aot_fn x y test_call_fn_with_non_const_inputs_aot_unsafe_control_flow ModuleSpecialFwd torch nn Module _some_bad_fwd param y y y + param param y forward x y = x y = _some_bad_fwd b = x + y b Init mod mod = ModuleSpecialFwd x = torch nn Parameter torch randn y = torch randn Run real real = mod x y Run through optimize our capturing fn gms = counter = CompileCounter capturing_fn gm inputs nonlocal gms gms append gm counter gm inputs optimized_mod = torch compile mod backend=capturing_fn Assert equal assertTrue torch _dynamo testing same real optimized_mod x y Uncomment reproduce commented out graphs below gm gms print GM CODE gm code assertEqual counter frame_count assertEqual counter op_count Graph forward x torch nn parameter Parameter y torch Tensor mul = x y x = y = None mul BREAK Graph forward y torch Tensor getitem = y y = None getitem_ = getitem getitem = None lt = getitem_ getitem_ = None lt BREAK Graph forward param torch Tensor y torch Tensor add = y + param y = param = None add BREAK Graph forward _stack torch Tensor x torch nn parameter Parameter y torch Tensor add = x + y x = y = None mul = _stack add _stack = add = None mul Run fn AOT torch _dynamo reset aot_fn = torch compile optimized_mod backend= aot_eager aot_fn x y Note Dynamo recompilation guarding invalid grad This test spiritual equivalent test_invalid_requires_grad_fake test_autodispatch py The point test invoke aot_autograd way would normally trigger assertion This what test_invalid_requires_grad_fake does However point test prove we do hit assertion dynamo recompiles correctly protects condition Subnote The reason us having test_invalid_requires_grad_fake utilizing fake tensors because dynamo sends fake tensors down aot_autograd patch torch _functorch config debug_assert True test_requires_grad_fake_via_dynamo_recompiles F torch nn Module forward x y x + y x = torch randn requires_grad=True y = torch randn requires_grad=True z = torch randn requires_grad=False cc = torch _dynamo testing CompileCounterWithBackend aot_eager failure_reason = None guard_fail_fn failure nonlocal failure_reason failure_reason = failure fxy = torch _dynamo optimize cc guard_fail_fn=guard_fail_fn F compare_equal_outs_and_grads F fxy x y compare_equal_outs_and_grads F fxy x z assertIn tensor y requires_grad mismatch expected requires_grad= failure_reason Reset failure reason failure_reason = None assertEqual cc frame_count torch _dynamo reset new backend cc = torch _dynamo testing CompileCounterWithBackend aot_eager fxz = torch _dynamo optimize cc guard_fail_fn=guard_fail_fn F compare_equal_outs_and_grads F fxz x z compare_equal_outs_and_grads F fxz x z assertEqual cc frame_count assertTrue failure_reason None test_double_backward_errors Remove test after we get double backward actually work grad_output torch tensor requires_grad=True None x = torch tensor requires_grad=True err = torch compile aot_autograd does currently support double backward The following cases should equivalent double backward entirely inside compiled function f x y = x sin exp gx = torch autograd grad y x create_graph=True grad_outputs=grad_output torch autograd grad gx x gx compiled_f = torch compile backend= aot_eager f f x assertRaisesRegex RuntimeError err compiled_f x second half double backward outside compiled function f x y = x sin exp gx = torch autograd grad y x create_graph=True grad_outputs=grad_output gx compiled_f = torch compile backend= aot_eager f gx = compiled_f x assertRaisesRegex RuntimeError err torch autograd grad gx x double backward entirely outside compiled function f x y = x sin exp y compiled_f = torch compile backend= aot_eager f y = compiled_f x gx = torch autograd grad y x create_graph=True grad_outputs=grad_output assertRaisesRegex RuntimeError err torch autograd grad gx x create_graph=False f x y = x sin exp y compiled_f = torch compile backend= aot_eager f x = torch tensor requires_grad=True y = compiled_f x gx = torch autograd grad y x create_graph=False grad_outputs=grad_output patch torch _functorch config debug_assert True test_arg_dupe_via_dynamo_recompiles F torch nn Module forward x y x = x trunc_ y = y trunc_ x + y x = torch randn requires_grad=True x x x x = x clone x clone x clone x clone y = torch randn requires_grad=True y y y = y clone y clone y clone cc = torch _dynamo testing CompileCounterWithBackend aot_eager failure_reason = None guard_fail_fn failure nonlocal failure_reason failure_reason = failure fxy = torch _dynamo optimize cc guard_fail_fn=guard_fail_fn F Note prevent recompilation between two calls we need clone x y each use fxy mutates input s metadata so otherwise dynamo will end up recompiling fxy x y fxy x y assertTrue failure_reason None Reset failure reason failure_reason = None assertEqual cc frame_count torch _dynamo reset new backend cc = torch _dynamo testing CompileCounterWithBackend aot_eager fxx = torch _dynamo optimize cc guard_fail_fn=guard_fail_fn F fxx x x fxx x y assertEqual cc frame_count assertIn x y failure_reason patch torch _functorch config debug_assert True test_arg_dupe_via_dynamo_recompiles_many_args_param_non_tensor_arg F torch nn Module __init__ - None super __init__ mean = torch nn Parameter torch randn forward b e f trunc_ b trunc_ + b + mean e f = torch randn requires_grad=True b = torch randn requires_grad=True = clone clone _ b = b clone b clone failure_reason = None guard_fail_fn failure nonlocal failure_reason failure_reason = failure assertTrue failure_reason None cc = torch _dynamo testing CompileCounterWithBackend aot_eager f = torch _dynamo optimize cc guard_fail_fn=guard_fail_fn F f f b assertEqual cc frame_count assertIn b failure_reason torch _dynamo reset cc = torch _dynamo testing CompileCounterWithBackend aot_eager c = torch randn requires_grad=True d = torch randn requires_grad=True c c = c clone c clone _ d = d clone d clone f = torch _dynamo optimize cc guard_fail_fn=guard_fail_fn F f c c f c d assertEqual cc frame_count assertIn b failure_reason patch torch _functorch config debug_assert True test_arg_dupe_via_dynamo_recompiles_many_with_global z = None F torch nn Module __init__ - None super __init__ mean = torch nn Parameter torch randn forward b e f trunc_ b trunc_ + b + z + mean e f = torch randn requires_grad=True b = torch randn requires_grad=True z = = clone clone _ b = b clone b clone failure_reason = None guard_fail_fn failure nonlocal failure_reason failure_reason = failure assertTrue failure_reason None cc = torch _dynamo testing CompileCounterWithBackend aot_eager f = torch _dynamo optimize cc guard_fail_fn=guard_fail_fn F f f b assertEqual cc frame_count assertIn b failure_reason patch torch _functorch config debug_assert True test_arg_dupe_via_dynamo_recompiles_many_args_param_non_tensor_arg_list F torch nn Module __init__ - None super __init__ mean = torch nn Parameter torch randn forward e f b trunc_ b trunc_ + b + mean e f = torch randn requires_grad=True b = torch randn requires_grad=True = clone clone _ b = b clone b clone failure_reason = None guard_fail_fn failure nonlocal failure_reason failure_reason = failure assertTrue failure_reason None cc = torch _dynamo testing CompileCounterWithBackend aot_eager f = torch _dynamo optimize cc guard_fail_fn=guard_fail_fn F f f b assertEqual cc frame_count assertIn b failure_reason torch _dynamo reset cc = torch _dynamo testing CompileCounterWithBackend aot_eager c = torch randn requires_grad=True d = torch randn requires_grad=True c c = c clone c clone _ d = d clone d clone f = torch _dynamo optimize cc guard_fail_fn=guard_fail_fn F f c c f c d assertEqual cc frame_count patch torch _functorch config debug_assert True test_arg_dupe_via_dynamo_recompiles_many_args_param F torch nn Module __init__ - None super __init__ mean = torch nn Parameter torch randn forward b trunc_ b trunc_ + b + mean = torch randn requires_grad=True b = torch randn requires_grad=True = clone clone _ b = b clone b clone failure_reason = None guard_fail_fn failure nonlocal failure_reason failure_reason = failure assertTrue failure_reason None cc = torch _dynamo testing CompileCounterWithBackend aot_eager f = torch _dynamo optimize cc guard_fail_fn=guard_fail_fn F f f b assertEqual cc frame_count assertIn b failure_reason torch _dynamo reset cc = torch _dynamo testing CompileCounterWithBackend aot_eager c = torch randn requires_grad=True d = torch randn requires_grad=True c c = c clone c clone _ d = d clone d clone f = torch _dynamo optimize cc guard_fail_fn=guard_fail_fn F f c c f c d assertEqual cc frame_count assertIn b failure_reason patch torch _functorch config debug_assert True test_arg_dupe_via_dynamo_recompiles_many_args F torch nn Module forward b c d trunc_ b trunc_ c trunc_ d trunc_ + b + c + d = torch randn requires_grad=True b = torch randn requires_grad=True = clone clone clone clone _ b b b = b clone b clone b clone b clone failure_reason = None guard_fail_fn failure nonlocal failure_reason failure_reason = failure assertTrue failure_reason None cc = torch _dynamo testing CompileCounterWithBackend aot_eager f = torch _dynamo optimize cc guard_fail_fn=guard_fail_fn F f f b b b assertEqual cc frame_count assertIn b failure_reason torch _dynamo reset cc = torch _dynamo testing CompileCounterWithBackend aot_eager c = torch randn requires_grad=True d = torch randn requires_grad=True c c = c clone c clone _ d = d clone d clone f = torch _dynamo optimize cc guard_fail_fn=guard_fail_fn F f b c c f b c d assertEqual cc frame_count assertIn c d failure_reason test_alias_inputs fn = torch tensor = b = squeeze = e pass = b ref_output = fn aot_fn = torch compile fn backend= aot_eager actual_output = aot_fn assertEqual ref_output actual_output test_grad_inputs_alias_inputs Test torch autograd Function staticmethod forward ctx x y ctx save_for_backward x y staticmethod backward ctx grad x = ctx saved_tensors x grad fn x y Test apply x y x = torch ones requires_grad=True y = torch ones requires_grad=True compiled_fn = torch compile fn backend= aot_eager out = compiled_fn x y out sum backward test_joint_custom_pass is_called = False joint_custom_pass joint_gm torch fx GraphModule joint_inputs nonlocal is_called is_called = True assertTrue isinstance joint_gm torch fx GraphModule assertTrue isinstance joint_inputs tuple first input list primals assertTrue isinstance joint_inputs list second input list tangents assertTrue isinstance joint_inputs list joint_gm M torch nn Module forward x x sin x = torch randn requires_grad=False compiled_fn = torch compile M backend= aot_eager torch _functorch config patch joint_custom_pass joint_custom_pass _ = compiled_fn x x doesn t require grad shouldn t trigger joint graph compiler assertFalse is_called y = torch randn requires_grad=True torch _functorch config patch joint_custom_pass joint_custom_pass out = compiled_fn y y requires grad should trigger joint graph compiler assertTrue is_called out sum backward expectedFailureDynamic https github com pytorch pytorch issues torch _dynamo config patch automatic_dynamic_shapes=False patch torch _functorch config debug_assert True test_multiple_aot_autograd_calls_dupe_args just dealing fact aot_module_simplified expects submods always tuples lists WrapperModule torch nn Module __init__ mod super __init__ mod = mod forward args out = mod args isinstance out list tuple out out compile_submod input_mod args functorch compile nop torch _functorch aot_autograd aot_module_simplified WrapperModule torch nn Module __init__ - None super __init__ original = input_mod submod = aot_module_simplified input_mod args nop forward args submod args WrapperModule test_compile fx_g example_inps split_gm = torch fx passes split_module split_module fx_g None lambda node mul str node submod_ _inps = split_gm submod_ example_inps split_gm submod_ = compile_submod WrapperModule split_gm submod_ example_inps split_gm submod_ = compile_submod WrapperModule split_gm submod_ submod_ _inps split_gm torch compile backend=test_compile f b c = torch ops custom maybe_dupe_op b mul_ c f torch ones f torch ones test_nn_parameter_construction https github com pytorch pytorch issues fn x y = x sin z = torch nn Parameter torch ones y + z x = torch rand opt_fn = torch compile fn backend= aot_eager assertTrue torch _dynamo testing same fn x opt_fn x test_aot_sequence_nr Model torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= same bias=True bn = torch nn BatchNorm d num_features= relu = torch nn ReLU fc = torch nn Linear in_features= out_features= loss_fn = torch nn L Loss forward x target y = x x = conv x x = bn x x = relu x x = x + y x = torch flatten x x = fc x output = loss_fn x target output mod = Model mod train x = torch rand requires_grad=True target = torch rand Use dynamo export get fx graph module g_mod _ = torch _dynamo export mod x target _prepare_model_args named_parameters = dict g_mod named_parameters remove_duplicate=False named_buffers = dict g_mod named_buffers remove_duplicate=False params_and_buffers = dict named_parameters dict named_buffers params_and_buffers_flat params_spec = pytree tree_flatten params_and_buffers params_len = len params_and_buffers_flat functional_call = create_functional_call g_mod params_spec params_len params_and_buffers_flat functional_call full_args fn_to_trace = _prepare_model_args param_and_buf_len = len full_args full_args extend x target aot_export requires graph mod input fwd graph returns full fwd bwd graph graph mod format torch enable_grad fx_traceback preserve_node_meta fx_g _ _ _ = _aot_export_function fn_to_trace full_args decompositions=None num_params_buffers=param_and_buf_len no_tangents=True Walk all nodes fx graph Write resulting ops table min_seq_nr = - seq_table = SeqNr &#124; OrigAten &#124; SrcFn &#124; FwdSrcFn\n node fx_g graph nodes call_ node op getitem str node target seq_nr = node meta get seq_nr - seq_nr continue min_seq_nr min_seq_nr = seq_nr source_fn_stack = node meta get source_fn_stack orig_aten = node meta get original_aten mod_name = len source_fn_stack mod_name = source_fn_stack - Make all seq_nr relative so starts seq_nr = seq_nr - min_seq_nr For backward nodes also test metadata corresponding forward node copied over fwd_source_fn_stack = node meta get fwd_source_fn_stack fwd_mod_name = len fwd_source_fn_stack fwd_mod_name = fwd_source_fn_stack - seq_table = seq_table + f seq_nr &#124; orig_aten &#124; mod_name &#124; fwd_mod_name \n maxDiff = None assertExpectedInline seq_table dedent \ SeqNr &#124; OrigAten &#124; SrcFn &#124; FwdSrcFn &#124; aten convolution default &#124; conv d &#124; &#124; aten add Tensor &#124; add_ &#124; &#124; aten _native_batch_norm_legit_functional default &#124; batch_norm &#124; &#124; aten relu default &#124; relu &#124; &#124; aten detach default &#124; relu &#124; &#124; aten add Tensor &#124; add &#124; &#124; aten view default &#124; flatten &#124; &#124; aten view default &#124; linear &#124; &#124; aten t default &#124; linear &#124; &#124; aten addmm default &#124; linear &#124; &#124; aten view default &#124; linear &#124; &#124; aten sub Tensor &#124; l _loss &#124; &#124; aten abs default &#124; l _loss &#124; &#124; aten mean default &#124; l _loss &#124; &#124; aten ones_like default &#124; &#124; l _loss &#124; aten expand default &#124; &#124; l _loss &#124; aten div Scalar &#124; &#124; l _loss &#124; aten sgn default &#124; &#124; l _loss &#124; aten mul Tensor &#124; &#124; l _loss &#124; aten view default &#124; &#124; linear &#124; aten t default &#124; &#124; linear &#124; aten mm default &#124; &#124; linear &#124; aten t default &#124; &#124; linear &#124; aten mm default &#124; &#124; linear &#124; aten t default &#124; &#124; linear &#124; aten sum dim_IntList &#124; &#124; linear &#124; aten view default &#124; &#124; linear &#124; aten t default &#124; &#124; linear &#124; aten view default &#124; &#124; linear &#124; aten view default &#124; &#124; flatten &#124; aten detach default &#124; &#124; relu &#124; aten threshold_backward default &#124; &#124; relu &#124; aten native_batch_norm_backward default &#124; &#124; batch_norm &#124; aten convolution_backward default &#124; &#124; conv d &#124; aten add Tensor &#124; &#124; l _loss test_split_with_sizes_aot_autograd_cleans_up_traceback_meta torch _functorch aot_autograd setup_stacktrace_preservation_hooks fn result split_sizes rs = torch ops aten split_with_sizes result split_sizes tolist rs example_inputs = torch randn requires_grad=True torch tensor outs = fn example_inputs setup_stacktrace_preservation_hooks out grad_fn out outs fx_traceback preserve_node_meta outs sum + outs sum + outs sum backward assertNotIn grad_fn_seq_nr fx_traceback current_meta assertNotIn in_grad_fn fx_traceback current_meta https github com pytorch pytorch issues test_aot_export_joint_simple_repro Mod torch nn Module __init__ args kwargs - None super __init__ args kwargs linear = torch nn Linear forward x linear x mini_backend gm sample_inputs torch _functorch aot_autograd aot_export_joint_simple fake_mode = torch _dynamo utils detect_fake_mode sample_inputs patch object fake_mode allow_non_fake_inputs True fake_mode aot_export_joint_simple gm sample_inputs trace_joint=False sample_inputs = torch rand model = Mod m_compiled = torch compile model backend=mini_backend out_ref = model sample_inputs out_test = m_compiled sample_inputs assertEqual out_ref out_test set donated_buffer=False due create_graph=True torch _functorch config patch donated_buffer False test_eager_sequence_nr Model torch nn Module __init__ - None super __init__ conv = torch nn Conv d in_channels= out_channels= kernel_size= stride= padding= same bias=True bn = torch nn BatchNorm d num_features= relu = torch nn ReLU fc = torch nn Linear in_features= out_features= loss_fn = torch nn L Loss forward x target y = x x = conv x x = bn x x = relu x x = x + y x = torch flatten x x = fc x output = loss_fn x target output grad_with_create_graph mod x target y = mod x target Set create_graph=True ensure sequence_nr backward ops continues count down gx = torch autograd grad y x create_graph=True grad_outputs=grad_output gx x = torch rand requires_grad=True target = torch rand mod = Model args = mod x target grad_output = torch tensor requires_grad=True compiled_f = torch compile backend= aot_eager grad_with_create_graph model_instance = compiled_f profile activities= torch profiler ProfilerActivity CPU record_shapes=True kineto_prof model_instance args bwd_set = set prof_str = SeqNr &#124; Thread &#124; FwdThread &#124; Name\n event kineto_prof events event sequence_nr = prof_str = prof_str + f event sequence_nr &#124; event thread f &#124; event fwd_thread &#124; event name &#124; \n re search r Backward event name bwd_set add event sequence_nr assertTrue len bwd_set test_aot_grad_mode_mutation compiler aot_eager inductor f x y = x x torch set_grad_enabled False y clone y f_compiled = torch compile f backend=compiler fullgraph=True torch set_grad_enabled True x = torch ones requires_grad=True y_ref = f x assertEqual torch is_grad_enabled False torch set_grad_enabled True y = f_compiled x assertEqual torch is_grad_enabled False torch set_grad_enabled True assertEqual y_ref y assertIsNone y_ref grad_fn assertIsNone y grad_fn assertIsNotNone y_ref grad_fn assertIsNotNone y grad_fn Check grad computed inputs given input same The tangent ` y ` which has grad_required=False irrelevant assertEqual sum y_ref grad_fn torch tensor - sum x x y grad_fn apply None torch tensor - x None test_aot_autograd_raises_invalid_leaf_set torch compile f x x set_ torch ones We still want make sure raises x = torch ones requires_grad=True assertRaisesRegex RuntimeError being used in-place operation f x test_aot_autograd_expand_mutation_functionalizes fn x y = x expand x shape y add_ y opt_fn = torch compile fn backend= aot_eager x = torch arange x_opt = x detach clone assertEqual fn x opt_fn x_opt assertEqual x x_opt test_aot_autograd_expand_mutation_backwards fn x z y = x expand x shape y mul_ ret = y z ret opt_fn = torch compile fn backend= aot_eager x = torch arange dtype=torch float z = x detach clone x_opt = x detach clone z_opt = x detach clone z requires_grad = True z_opt requires_grad = True res = fn x z opt_res = opt_fn x_opt z_opt assertEqual res opt_res res sum backward opt_res sum backward assertEqual x x_opt assertEqual z grad z_opt grad test_data_ptr_access_copy torch _functorch config _config _config patch fake_tensor_allow_unsafe_data_ptr_access=False FakeTensorMode x = torch randn y = copy copy x assertEqual y shape x shape test_data_ptr_access_fails_in_forward torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x - Tensor lib=lib torch library impl mylib foo CompositeImplicitAutograd lib=lib _ x x data_ptr x clone x = torch randn data_ptr_graph_input x r = torch ops mylib foo x r data_ptr_graph_intermediate x y = x clone r = torch ops mylib foo y r tests = data_ptr_graph_input data_ptr_graph_intermediate ctx assertRaisesRegex RuntimeError Cannot access data pointer f tests ctx make_fx f tracing_mode= fake x ctx make_fx f tracing_mode= symbolic x ctx torch compile f backend= eager fullgraph=True x test_data_ptr_access_fails_in_backward torch library _scoped_library mylib FRAGMENT lib torch library define mylib foo Tensor x - Tensor lib=lib backward_called = False Foo torch autograd Function staticmethod forward ctx x x clone staticmethod backward ctx grad nonlocal backward_called backward_called = True grad data_ptr grad clone torch library impl mylib foo CompositeImplicitAutograd lib=lib _ x Foo apply x f x torch ops mylib foo x x = torch randn requires_grad=True assertRaisesRegex RuntimeError Cannot access data pointer torch compile f backend= aot_eager fullgraph=True x assertTrue backward_called We don t know how catch multiple mutations same memory location unittest expectedFailure test_aot_autograd_expand_mutation_error fn x y = x expand x shape y add_ y opt_fn = torch compile fn backend= aot_eager x = torch arange x_opt = x detach clone assertRaises Exception fn x assertRaises Exception opt_fn x_opt torch _functorch config patch donated_buffer=True test_donated_buffer logger_name = torch _functorch _aot_autograd graph_compile torch compile relu x torch nn functional relu x assertLogs logger_name level= INFO captured relu torch rand requires_grad=True sum backward is_dynamic_shape_test _testMethodName extra symint exists expected_msg = bw_donated_idxs= expected_msg = bw_donated_idxs= le donated buffer relu FileCheck check expected_msg run \n join captured output torch _functorch config patch donated_buffer True test_donated_buffer logger_name = torch _functorch _aot_autograd graph_compile we will reuse graph g across f f torch compile g activation param torch matmul activation param f inp param param activation = inp + param g activation param inp = torch ones param = torch ones requires_grad=True param = torch ones requires_grad=True assertLogs logger_name level= INFO captured f inp param param sum backward FileCheck check bw_donated_idxs= run \n join captured output torch _functorch config patch donated_buffer True test_donated_buffer logger_name = torch _functorch _aot_autograd graph_compile we will reuse graph g across f f torch compile g activation param torch matmul activation param f inp param param exp saves output activation bw activation = torch exp inp + param g activation param inp = torch ones param = torch ones requires_grad=True param = torch ones requires_grad=True assertLogs logger_name level= INFO captured f inp param param sum backward FileCheck check bw_donated_idxs= run \n join captured output torch _functorch config patch donated_buffer True test_donated_buffer logger_name = torch _functorch _aot_autograd graph_compile Mod torch nn Module __init__ - None super __init__ param = torch nn Parameter torch zeros forward x torch Tensor - torch Tensor torch nn functional relu x + param mod = Mod mod = torch compile mod inp = torch ones requires_grad=True assertLogs logger_name level= INFO captured mod inp sum backward Forward graph primals_ num_users= = placeholder target=primals_ primals_ num_users= = placeholder target=primals_ relu num_users= = call_function target=torch ops aten relu default args = primals_ kwargs = add num_users= = call_function target=torch ops aten add Tensor args = relu primals_ kwargs = le num_users= = call_function target=torch ops aten le Scalar args = relu kwargs = add le ` le ` donated buffer FileCheck check bw_donated_idxs= run \n join captured output torch _functorch config patch donated_buffer True test_donated_buffer logger_name = torch _functorch _aot_autograd graph_compile torch compile f x z y = x view z = torch nn functional relu z torch mm y x + z inp = torch rand requires_grad=True torch rand requires_grad=True assertLogs logger_name level= INFO captured f inp sum backward Forward graph primals_ num_users= = placeholder target=primals_ primals_ num_users= = placeholder target=primals_ view num_users= = call_function target=torch ops aten view default args = primals_ kwargs = relu num_users= = call_function target=torch ops aten relu default args = primals_ kwargs = mm num_users= = call_function target=torch ops aten mm default args = view primals_ kwargs = add num_users= = call_function target=torch ops aten add Tensor args = mm relu kwargs = le num_users= = call_function target=torch ops aten le Scalar args = relu kwargs = add primals_ le ` le ` donated buffer primals_ FileCheck check bw_donated_idxs= run \n join captured output torch _functorch config patch donated_buffer True torch _dynamo config patch graph_break_on_nn_param_ctor False test_donated_buffer is_dynamic_shape_test _testMethodName parameters should dynamic shape torch _dynamo exc Unsupported Parameter python_constant SymNodeVariable constant logger_name = torch _functorch _aot_autograd graph_compile fn x p = torch nn Parameter x + p p sin opt = torch compile fn fullgraph=True x = torch randn assertLogs logger_name level= INFO captured p r = opt x r sum backward FileCheck check bw_donated_idxs= run \n join captured output torch _functorch config patch donated_buffer True test_donated_buffer_with_retain_or_create_graph Gives non-empty bw_donated_idxs Mod torch nn Module __init__ - None super __init__ param = torch nn Parameter torch zeros forward x torch nn functional relu x + param inp = torch randn requires_grad=True mod = torch compile Mod _ range mod inp sum backward torch _functorch config patch donated_buffer True test_donated_buffer_with_retain_or_create_graph Gives non-empty bw_donated_idxs Mod torch nn Module __init__ - None super __init__ param = torch nn Parameter torch zeros forward x torch nn functional relu x + param inp = torch randn requires_grad=True mod = torch compile Mod out = mod inp sum _ range out backward retain_graph=True out backward torch _functorch config patch donated_buffer True test_donated_buffer_with_retain_or_create_graph Gives non-empty bw_donated_idxs Mod torch nn Module __init__ - None super __init__ param = torch nn Parameter torch zeros forward x torch nn functional relu x + param inp = torch randn requires_grad=True mod = torch compile Mod mod inp sum backward create_graph=True out = mod inp sum _ range out backward retain_graph=True out backward test_autograd_function_tangent_mutation Foo torch autograd Function staticmethod forward ctx x x clone x clone staticmethod backward ctx grad grad grad copy_ grad f x Foo apply x x = torch randn requires_grad=True x_ref = x clone detach requires_grad_ out_ref = f x_ref out = torch compile f backend= aot_eager fullgraph=True x assertEqual out_ref out assertEqual x_ref x out + out sum backward out_ref + out_ref sum backward assertEqual x_ref grad x grad torch _functorch config patch donated_buffer True test_donated_buffer_with_retain_or_create_graph Gives non-empty bw_donated_idxs Mod torch nn Module __init__ - None super __init__ param = torch nn Parameter torch zeros forward x torch nn functional relu x + param inp = torch randn requires_grad=True mod = torch compile Mod mod inp sum backward out = mod inp sum assertRaisesRegex RuntimeError r This backward function compiled non-empty donated r buffers which requires create_graph=False retain_graph=False r Please keep backward\ create_graph=False retain_graph=False\ r across all backward\ \ function calls set r torch _functorch config donated_buffer=False disable r donated buffer out backward retain_graph=True _get_guard_failure_on_overlapping_view_inputs f argsfn argsfn Compile run f twice using arguments generated argsfn argsfn This function expects second argument set will trigger recompilation which shall returned end guard_failure = guard_fail_fn failure nonlocal guard_failure guard_failure append failure input = torch ones opt_input = input clone detach opt_f = torch _dynamo optimize aot_eager dynamic=True guard_fail_fn=guard_fail_fn f out = f argsfn input opt_out = opt_f argsfn opt_input assertEqual out opt_out out = f argsfn input opt_out = opt_f argsfn opt_input assertEqual out opt_out Check we only have one instance guard failure due overlapping state matching assertEqual len guard_failure guard_failure test_inputs_overlapping_with_mutation_recompile Check overlap guard actually fails when we run second time args have no storage overlap f args args add_ args overlapping_args x x x x non_overlapping_args x x x x guard_failure = _get_guard_failure_on_overlapping_view_inputs f overlapping_args non_overlapping_args assertExpectedInline guard_failure check_overlapping overlapping= args args non_overlapping= args test_different_inputs_overlapping_set_with_mutation Check overlap guard actually fails when we run second time arguments whose overlapping set superset set arguments used first time f b c d mul_ + b + c + d a_b_overlapping_args x x x x x a_b_c_overlapping_args x x x x x guard_failure = _get_guard_failure_on_overlapping_view_inputs f a_b_overlapping_args a_b_c_overlapping_args assertExpectedInline guard_failure check_overlapping overlapping= b non_overlapping= c d _test_no_storage_overlap_guards f argsfn Compile f aot_eager backend run argument set returned argsfn function Meanwhile keep track aotautograd_gurads so make sure no StorageOverlap guard added Compiler __init__ counter = CompileCounterWithBackend aot_eager __call__ args kwargs Instead checking here we need check afterwards since StorageOverlap guard only added later guards = TracingContext get guards_context aotautograd_guards counter args kwargs compiler = Compiler input = torch arange opt_input = input clone detach out = f argsfn input opt_out = torch compile f backend=compiler dynamic=True argsfn opt_input assertEqual out opt_out assertEqual compiler counter frame_count Check none AOTAutograd guards StorageOverlap guards g compiler guards assertNotIsInstance g StorageOverlap test_no_storage_overlap_guards_no_mutation f b + b overlapping_args input input input _test_no_storage_overlap_guards f overlapping_args test_no_storage_overlap_guards_no_aliasing f b add_ b add_ non_overlapping_args input input torch arange _test_no_storage_overlap_guards f non_overlapping_args test_inputs_overlapping_with_mutation_stress Stress test StorageOverlap guard Create non-overlapping tensor views extra one overlaps first them Then make sure none produced ShapeEnv guards came overlapping computation f args args add_ args overlapping_args input non-overlapping tensors size input split A tensor overlaps half tensors above input Compiler __init__ counter = CompileCounterWithBackend aot_eager __call__ args kwargs compile_context = CompileContext get counter args kwargs compiler = Compiler opt_f = torch compile f backend=compiler dynamic=True input = torch arange _ opt_input = input clone detach out = f overlapping_args input opt_out = opt_f overlapping_args opt_input assertEqual out opt_out Check none produced ShapeEnv guards came compute_overlapping_inputs function overlapping_computation_fn = compute_overlapping_inputs shape_env_guards = compiler compile_context shape_env_guards g shape_env_guards assertNotIn overlapping_computation_fn g Check we have no more than ShapeEnv guards Note arbitrary number So we might have change future However time change introduced went down assertLess len shape_env_guards See https github com pytorch pytorch issues test_aot_autograd_stride_reconstruction_on_zero_dim_dynamic_shaped_tensor - None repro sentinel torch Tensor skip_squeeze bool = False - torch Tensor x = torch unique torch ones x = torch reshape x skip_squeeze x = torch squeeze x -d tensor x sentinel Grad required trigger issue need replay stride sentinel = torch tensor requires_grad=True eager_sq = repro sentinel comp_aot_sq = torch compile repro backend= aot_eager fullgraph=True sentinel comp_ind_sq = torch compile repro backend= inductor fullgraph=True sentinel assertEqual eager_sq comp_aot_sq assertEqual eager_sq comp_ind_sq assertEqual eager_sq stride comp_ind_sq stride Now check semantics preserved when skipping squeeze eager_no_sq = repro sentinel skip_squeeze=True comp_aot_no_sq = torch compile repro backend= aot_eager fullgraph=True sentinel skip_squeeze=True comp_ind_no_sq = torch compile repro backend= inductor fullgraph=True sentinel skip_squeeze=True assertEqual eager_no_sq comp_aot_no_sq assertEqual eager_no_sq comp_ind_no_sq assertEqual eager_no_sq stride comp_ind_no_sq stride torch _dynamo config patch capture_scalar_outputs=True torch _dynamo config patch capture_dynamic_output_shape_ops=True test_unbacked_activation_specialized_in_inductor Test compilation unbacked operations like nonzero torch _dynamo reset fuzzed_program arg_ sentinel var_node_ = arg_ var_node_ = torch full - dtype=torch int var_node_ = torch full dtype=torch int var_node_ = torch ops aten add var_node_ var_node_ var_node_ = torch full - dtype=torch int var_node_ = torch ops aten mul var_node_ var_node_ var_node_ = torch full False dtype=torch bool var_node_ = torch nonzero var_node_ var_node_ = torch ops aten add var_node_ var_node_ var_node_ = torch ops aten div var_node_ var_node_ result = var_node_ sentinel result is_complex result = result real result sentinel = torch tensor requires_grad=True arg_ = torch randint dtype=torch int args = arg_ + sentinel result_original = fuzzed_program args compiled_program = torch compile fuzzed_program fullgraph=True dynamic=True result_compiled = compiled_program args assertTrue torch allclose result_original result_compiled __name__ == __main__ torch _dynamo test_case run_tests run_tests