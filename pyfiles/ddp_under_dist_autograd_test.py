mypy allow-untyped-defs contextlib enum logging os threading typing NamedTuple torch torch distributed dist torch distributed autograd dist_autograd torch nn nn torch distributed rpc torch distributed nn RemoteModule torch nn parallel DistributedDataParallel torch testing _internal common_distributed requires_gloo requires_nccl skip_if_lt_x_gpu skip_if_rocm_multiprocess torch testing _internal dist_utils dist_init INIT_METHOD_TEMPLATE torch testing _internal distributed rpc rpc_agent_test_fixture RpcAgentTestFixture NUM_EM_ROW = D_SPARSE = D_DENSE = D_HID = D_OUT = NUM_TRAINERS = Trainers + master + remote worker WORLD_SIZE = NUM_TRAINERS + TRAINER_RANKS = list range NUM_TRAINERS REMOTE_WORKER_RANK = TRAINER_RANKS - + MASTER_RANK = REMOTE_WORKER_RANK + DdpMode enum Enum Don t apply DDP NONE = enum auto Apply DDP top level nn Module OUTSIDE = enum auto Embed DDP inside top level nn Module INSIDE = enum auto init_logger logger = logging getLogger __name__ level = logging DEBUG debug os environ logging INFO logger setLevel level console = logging StreamHandler formatter = logging Formatter asctime s filename s lineno s levelname s p processName s t threadName s message s console setFormatter formatter console setLevel level add handlers logger logger addHandler console logger propagate = False logger gLogger = init_logger FeatureSet NamedTuple A feature set has types features dense_features torch Tensor sparse_features torch LongTensor values torch Tensor _call_method method rref args kwargs method rref local_value args kwargs _remote_method method rref args kwargs args_tup = tuple method rref + list args rpc rpc_sync rref owner _call_method args=args_tup kwargs=kwargs _remote_method_async method rref args kwargs args_tup = tuple method rref + list args rpc rpc_async rref owner _call_method args=args_tup kwargs=kwargs RemoteEM nn Module __init__ num_embeddings int embedding_dim int gLogger info Initing RemoteEM s s num_embeddings embedding_dim super __init__ init_em = embedding_dim em = nn EmbeddingBag num_embeddings embedding_dim _weight=torch tensor init_em num_embeddings forward input torch Tensor gLogger debug Running RemoteEM forward s input em input offsets=torch LongTensor range input shape Return linear module predefined parameters getLinear d_in d_out l = nn Linear d_in d_out bias=False w = torch ones d_out d_in w = - w requires_grad_ l weight data = w l RemoteNet nn Module __init__ d_in int d_out int gLogger info Initing RemoteNet s s d_in d_out super __init__ fc = getLinear d_in d_out relu = nn ReLU forward input torch Tensor gLogger debug Running RemoteNet forward s input relu fc input HybridModel nn Module __init__ remote_em_rref rpc RRef remote_net_rref rpc RRef process_group_for_ddp dist ProcessGroup = None super __init__ remote_em_rref = remote_em_rref remote_net_rref = remote_net_rref fc = getLinear D_DENSE D_DENSE fc = getLinear D_HID D_OUT non_ddp_params = tuple fc parameters + tuple fc parameters ddp_params = process_group_for_ddp None non_ddp_params ddp_params = tuple fc parameters tuple fc parameters gLogger info Use DDP second local net fc = DistributedDataParallel fc check_reduction=True process_group=process_group_for_ddp gLogger info HybridModel has s groups parameters len list parameters forward input FeatureSet gLogger debug Running HybridModel forward s input sparse = _remote_method RemoteEM forward remote_em_rref input sparse_features The same size mini batch assert sparse shape == input dense_features shape dense = fc input dense_features x = torch cat dense sparse gLogger debug Concatenated feature s x x = _remote_method RemoteNet forward remote_net_rref x fc x Trainer __init__ remote_em_rref rpc RRef remote_net_rref rpc RRef ddp_mode DdpMode rank int rank = rank trainer_group = dist new_group TRAINER_RANKS ddp_mode DdpMode INSIDE DdpMode OUTSIDE None remote_em_rref = remote_em_rref remote_net_rref = remote_net_rref hybrid_module = HybridModel remote_em_rref remote_net_rref trainer_group ddp_mode == DdpMode INSIDE None ddp_params non_ddp_params = hybrid_module ddp_params hybrid_module non_ddp_params ddp_mode == DdpMode OUTSIDE gLogger info Wrapping whole hybrid module into DDP ddp_params += non_ddp_params non_ddp_params = hybrid_module = DistributedDataParallel hybrid_module check_reduction=True process_group=self trainer_group gLogger info Succeeded creating HybridModel instance s ddp params s other local params len ddp_params len non_ddp_params destroy_pg trainer_group dist destroy_process_group trainer_group train_batch mini_batch FeatureSet trainer_has_less_inputs bool simulate_uneven_inputs bool grads_dict = None simulate_uneven_inputs input_batches = mini_batch Split into microbatches trim simulate uneven inputs dense_features = mini_batch dense_features sparse_features = mini_batch sparse_features values = mini_batch values dense_microbatch = torch split dense_features sparse_microbatch = torch split sparse_features values_microbatch = torch split values batches = d s v zip dense_microbatch sparse_microbatch values_microbatch strict=True feature_set = FeatureSet dense_features=d sparse_features=s values=v batches append feature_set trainer_has_less_inputs input_batches = batches len batches gLogger info Trainer reduced input patches s s simulate uneven inputs len batches len input_batches input_batches = batches hybrid_module join simulate_uneven_inputs contextlib nullcontext b input_batches dist_autograd context context_id output = hybrid_module forward b loss = output mini_batch values sum dist_autograd backward context_id loss grads_dict = dist_autograd get_gradients context_id gLogger info Loss s mini batch s Grads dict has s entries s loss mini_batch len grads_dict grads_dict tuple grads_dict param param ddp_params tuple grads_dict param param non_ddp_params get_training_examples n = training_examples = FeatureSet dense_features=torch zeros n D_DENSE sparse_features=torch zeros n dtype=torch long values=torch zeros n idx = Every example has another one has exactly same features opposite value Therefore their grads cancel each other all-reduce value - x - value value y value - value z training_examples dense_features idx = torch tensor x y training_examples sparse_features idx = z training_examples values idx = value idx += Split examples among NUM_TRAINERS trainers assert == n NUM_TRAINERS examples_per_trainer = int n NUM_TRAINERS FeatureSet dense_features=training_examples dense_features start start + examples_per_trainer sparse_features=training_examples sparse_features start start + examples_per_trainer values=training_examples values start start + examples_per_trainer start range n examples_per_trainer shutdown_signal = threading Condition set_shutdown_signal global shutdown_signal shutdown_signal shutdown_signal notify DdpUnderDistAutogradTest RpcAgentTestFixture property world_size - int WORLD_SIZE remote_worker_name - str The name has consistent dist_init decorator f worker REMOTE_WORKER_RANK trainer_name rank The name has consistent dist_init decorator f worker rank _remote_worker_process ddp_mode gLogger info The remote worker running dist init_process_group backend= gloo init_method=INIT_METHOD_TEMPLATE format file_name=self file_name world_size=self world_size rank=self rank ddp_mode DdpMode INSIDE DdpMode OUTSIDE new_group needs called ranks dist new_group TRAINER_RANKS global shutdown_signal shutdown_signal shutdown_signal wait gLogger info Exiting remote worker dist destroy_process_group _trainer_process rank int gLogger info Running trainer s rank gLogger info Initing trainer process group trainer s ranks s rank TRAINER_RANKS dist init_process_group backend= gloo init_method=INIT_METHOD_TEMPLATE format file_name=self file_name world_size=self world_size rank=self rank gLogger info Waiting shutdown signal trainer s rank global shutdown_signal shutdown_signal shutdown_signal wait gLogger info Exiting trainer s rank dist destroy_process_group _master_process ddp_mode DdpMode simulate_uneven_inputs bool gLogger info Running master process dist init_process_group backend= gloo init_method=INIT_METHOD_TEMPLATE format file_name=self file_name world_size=self world_size rank=self rank remote_em_rref = rpc remote remote_worker_name RemoteEM args= NUM_EM_ROW D_SPARSE remote_net_rref = rpc remote remote_worker_name RemoteNet args= D_DENSE + D_SPARSE D_HID gLogger info Created remote rrefs master do_test_on_master ddp_mode simulate_uneven_inputs remote_em_rref remote_net_rref do_test_on_master ddp_mode DdpMode simulate_uneven_inputs bool remote_em_rref rpc RRef remote_net_rref rpc RRef simulate_uneven_inputs gLogger info Running DDP + RPC test simulating uneven inputs across trainers trainer_rrefs = rank TRAINER_RANKS trainer = trainer_name rank trainer_rrefs append rpc remote trainer Trainer args= remote_em_rref remote_net_rref ddp_mode rank ddp_mode DdpMode INSIDE DdpMode OUTSIDE new_group needs called ranks dist new_group TRAINER_RANKS training_examples = get_training_examples _ range futures = num_trainers = len trainer_rrefs idx trainer_rref enumerate trainer_rrefs Half trainers will deplete inputs earlier than rest trainer_has_less_inputs = simulate_uneven_inputs idx num_trainers futures append _remote_method_async Trainer train_batch trainer_rref training_examples idx trainer_has_less_inputs simulate_uneven_inputs future futures ddp_grads non_ddp_grads = future wait When there uneven inputs necessary grads cancel each other out since some trainers contribute grad simulate_uneven_inputs grad ddp_grads assertEqual grad torch zeros_like grad msg=f The grad any ddp parameter should zeros because training examples grads cancel each other Received f gradient grad grad non_ddp_grads assertNotEqual grad torch zeros_like grad msg= The grad any non-ddp parameter shouldn t zeros Destroy process groups trainer_rref trainer_rrefs _remote_method_async Trainer destroy_pg trainer_rref wait Send shutdown signals rank TRAINER_RANKS trainer = trainer_name rank rpc rpc_sync trainer set_shutdown_signal args= rpc rpc_sync remote_worker_name set_shutdown_signal args= _do_test ddp_mode simulate_uneven_inputs=False rank == MASTER_RANK _master_process ddp_mode simulate_uneven_inputs rank == REMOTE_WORKER_RANK _remote_worker_process ddp_mode rank TRAINER_RANKS _trainer_process rank raise RuntimeError f Unknown process rank rank requires_gloo dist_init test_backward_no_ddp _do_test DdpMode NONE requires_gloo dist_init test_backward_ddp_outside _do_test DdpMode OUTSIDE requires_gloo dist_init test_backward_ddp_outside_uneven_inputs _do_test DdpMode OUTSIDE simulate_uneven_inputs=True requires_gloo dist_init test_backward_ddp_inside _do_test DdpMode INSIDE Common utils both CPU CUDA test suites CommonDdpComparisonTest RpcAgentTestFixture property world_size - int NUM_TRAINERS trainer_name rank The name has consistent dist_init decorator f worker rank staticmethod get_remote_grads rref context_id dist_autograd get_gradients context_id rref local_value weight DdpComparisonTest CommonDdpComparisonTest _run_test_ddp_comparision simulate_uneven_inputs=False gLogger info Running trainer rank s rank Each trainer uses different random seed Otherwise they going have exactly same initial model parameters input therefore grads That means grads will same before after DDP s all-reduce torch manual_seed rank dist init_process_group backend= gloo Postfix file_name pg since file_name also used RPC agent init_method=INIT_METHOD_TEMPLATE format file_name=f file_name _pg world_size=self world_size rank=self rank net = nn Linear ddp_net = DistributedDataParallel net Odd ranks join early simulate_uneven_inputs num_inputs = simulate_uneven_inputs rank == num_inputs += inputs_list = torch rand _ range num_inputs simulate_uneven_inputs gLogger info Rank s training s inputs rank len inputs_list Use distributed autograd The gradients will RPC context map grads_dict = ddp_net join simulate_uneven_inputs i inputs enumerate inputs_list dist_autograd context context_id loss = ddp_net inputs norm dist_autograd backward context_id loss grads_dict = dist_autograd get_gradients context_id gLogger info Trainer s got grad dict s rank grads_dict Use local autograd The gradients will each variable s grad ddp_net zero_grad loss = ddp_net inputs norm loss backward The gradients should same param net parameters assertTrue param grads_dict msg=f Param param dist_auto grad dict grads_dict iteration i assertEqual grads_dict param param grad msg=f The grads param param different under local f dist autograd param grad \n --- \n grads_dict param iteration i dist destroy_process_group requires_gloo dist_init test_ddp_comparison _run_test_ddp_comparision requires_gloo dist_init test_ddp_comparison_uneven_inputs test simulating uneven inputs DDP _run_test_ddp_comparision simulate_uneven_inputs=True requires_gloo dist_init test_ddp_dist_autograd_sparse_grads Each trainer uses different random seed Otherwise they going have exactly same initial model parameters input therefore grads That means grads will same before after DDP s all-reduce torch manual_seed rank dist init_process_group backend= gloo init_method=INIT_METHOD_TEMPLATE format file_name=self file_name world_size=self world_size rank=self rank model = nn EmbeddingBag sparse=True ddp_model = DistributedDataParallel model Different inputs each input = torch LongTensor random_ offsets = torch LongTensor Run local loss = ddp_model input offsets sum loss backward dist_autograd context context_id loss = ddp_model input offsets sum dist_autograd backward context_id loss grads_dict = dist_autograd get_gradients context_id assertEqual len grads_dict assertEqual model weight grad grads_dict model weight requires_gloo dist_init test_ddp_dist_autograd_local_vs_remote Each trainer uses different random seed Otherwise they going have exactly same initial model parameters input therefore grads That means grads will same before after DDP s all-reduce torch manual_seed rank dist init_process_group backend= gloo init_method=INIT_METHOD_TEMPLATE format file_name=self file_name world_size=self world_size rank=self rank Use two different remote device input string w w o default device string cpu respectively remote_device worker cpu worker remote_layer = RemoteModule remote_device=remote_device module_cls=nn Linear args= False layer = nn Linear False Start same parameters remote local layer weight = remote_layer module_rref to_here weight Run local case layer = nn Linear inputs = torch rand ddp_model = DistributedDataParallel layer loss = ddp_model layer inputs sum loss backward Run remote case dist_autograd context context_id loss = ddp_model remote_layer inputs sum dist_autograd backward context_id loss grads_dict = dist_autograd get_gradients context_id dist barrier assertEqual layer weight grad grads_dict layer weight assertEqual layer weight grad rpc rpc_sync worker CommonDdpComparisonTest get_remote_grads args= remote_layer module_rref context_id CudaDdpComparisonTest CommonDdpComparisonTest skip_if_lt_x_gpu NUM_TRAINERS requires_nccl dist_init skip_if_rocm_multiprocess test_ddp_dist_autograd_local_vs_remote_gpu Each trainer uses different random seed Otherwise they going have exactly same initial model parameters input therefore grads That means grads will same before after DDP s all-reduce torch manual_seed rank dist init_process_group backend= gloo init_method=INIT_METHOD_TEMPLATE format file_name=self file_name world_size=self world_size rank=self rank remote_layer = RemoteModule remote_device= worker cpu module_cls=nn Linear args= False layer = nn Linear False Start same parameters remote local layer weight = remote_layer module_rref to_here weight layer = nn Linear cuda rank ddp_layer = DistributedDataParallel layer device_ids= rank remote_layer = RemoteModule remote_device= worker cpu module_cls=nn Linear args= False layer = nn Linear False Start same parameters remote local layer weight = remote_layer module_rref to_here weight layer = nn Linear cuda rank ddp_layer = DistributedDataParallel layer device_ids= rank Run local case inputs = torch rand loss = ddp_layer layer ddp_layer layer inputs cuda rank cpu cuda rank sum loss backward Run remote case dist_autograd context context_id loss = ddp_layer remote_layer ddp_layer remote_layer inputs cuda rank cpu cuda rank sum dist_autograd backward context_id loss grads_dict = dist_autograd get_gradients context_id dist barrier assertEqual layer weight grad rpc rpc_sync worker CommonDdpComparisonTest get_remote_grads args= remote_layer module_rref context_id assertEqual layer weight grad grads_dict layer weight assertEqual layer weight grad rpc rpc_sync worker CommonDdpComparisonTest get_remote_grads args= remote_layer module_rref context_id assertEqual layer weight grad grads_dict layer weight