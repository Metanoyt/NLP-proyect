Owner s module dynamo flake noqa B contextlib torch torch fx torch _dynamo graph_deduplication apply_graph_deduplication torch _dynamo graph_utils _detect_cycles torch _dynamo output_graph FakeRootModule torch _dynamo test_case TestCase torch _dynamo testing AotEagerAndRecordGraphs extract_graph_and_tracker normalize_gm torch compiler allow_in_graph torch utils _ordered_set OrderedSet extract_graph fn args kwargs backend = AotEagerAndRecordGraphs result = torch compile backend=backend fn args kwargs result backend graphs backend fw_graphs graph_str gm normalize_gm gm print_readable print_output=False GraphDededuplicationTests TestCase setUp exit_stack = contextlib ExitStack exit_stack enter_context torch _dynamo config patch use_graph_deduplication True super setUp tearDown exit_stack close super tearDown run_and_return_graphs fn args kwargs extract_graph fn args kwargs run_and_get_simple_graph fn x y x = x + y = y + z = x sum + y sum z x = torch rand requires_grad=False y = torch rand requires_grad=False _ _ fw_graphs = run_and_return_graphs fn x y fw_graphs test_single_subgraph inner_fn x y x = x + y = y + z = x sum + y sum z fn x y _o = inner_fn x y o = torch sin y o = inner_fn x o o = inner_fn x y o = o o o o x = torch rand requires_grad=True y = torch rand requires_grad=True x_clone = x clone requires_grad_ True y_clone = y clone requires_grad_ True ref_result = fn x y result graphs fw_graphs = run_and_return_graphs fn x_clone y_clone torch allclose ref_result result ref_result sum backward result sum backward assertEqual len graphs assertEqual len fw_graphs assertExpectedInline graph_str graphs \ GraphModule torch nn Module forward L_x_ f L_y_ f subgraph_ = subgraph_ l_x_ = L_x_ l_y_ = L_y_ o f = torch sin l_y_ invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ l_y_ invoke_subgraph = None invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ o o = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ l_y_ subgraph_ = l_x_ = l_y_ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None o f = getitem_ getitem_ getitem_ = None mul_ f = getitem_ o getitem_ = o = None mul_ subgraph_ torch nn Module forward subgraph_input_l_x_ subgraph_input_l_y_ x f = subgraph_input_l_x_ + subgraph_input_l_x_ = None y f = subgraph_input_l_y_ + subgraph_input_l_y_ = None sum_ f = x sum x = None sum_ f = y sum y = None z f = sum_ + sum_ sum_ = sum_ = None z assertExpectedInline graph_str fw_graphs \ GraphModule torch nn Module forward primals_ f primals_ f sin f = torch ops aten sin default primals_ partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ sin partitioned_fw_subgraph_ _ = sin = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ primals_ partitioned_fw_subgraph_ _ = primals_ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None mul f = torch ops aten mul Tensor getitem_ getitem_ mul_ f = torch ops aten mul Tensor getitem_ mul mul = None mul_ primals_ getitem_ getitem_ partitioned_fw_subgraph_ _ torch nn Module forward primals_ f primals_ f add f = torch ops aten add Tensor primals_ primals_ = None add_ f = torch ops aten add Tensor primals_ primals_ = None sum_ f = torch ops aten sum default add add = None sum_ f = torch ops aten sum default add_ add_ = None add_ f = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None add_ test_single_subgraph fn x x = x + o = inner_fn x o = torch cos o o = inner_fn o torch sin o inner_fn x o = x o += o += o x = torch rand requires_grad=True x_clone = x clone requires_grad_ True ref_result = fn x result graphs fw_graphs = run_and_return_graphs fn x_clone torch allclose ref_result result ref_result sum backward result sum backward assertEqual len graphs assertEqual len fw_graphs assertExpectedInline graph_str graphs \ GraphModule torch nn Module forward L_x_ f subgraph_ = subgraph_ l_x_ = L_x_ x f = l_x_ + l_x_ = None invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ x x = None getitem f = invoke_subgraph invoke_subgraph = None o_ f = torch cos getitem getitem = None invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ o_ subgraph_ = o_ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None sin f = torch sin getitem_ getitem_ = None sin subgraph_ torch nn Module forward subgraph_input_x o f = subgraph_input_x subgraph_input_x = None o += o_ f = o o = None o_ += o_ f = o_ o_ = None o_ assertExpectedInline graph_str fw_graphs \ GraphModule torch nn Module forward primals_ f add f = torch ops aten add Tensor primals_ primals_ = None partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ add partitioned_fw_subgraph_ _ = add = None getitem f = invoke_subgraph_ invoke_subgraph_ = None cos f = torch ops aten cos default getitem partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ cos partitioned_fw_subgraph_ _ = cos = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None sin f = torch ops aten sin default getitem_ cos_ f = torch ops aten cos default getitem_ getitem_ = None sin_ f = torch ops aten sin default getitem getitem = None neg f = torch ops aten neg default sin_ sin_ = None sin cos_ neg partitioned_fw_subgraph_ _ torch nn Module forward primals_ f mul f = torch ops aten mul Tensor primals_ primals_ = None add f = torch ops aten add Tensor mul mul = None add_ f = torch ops aten add Tensor add add = None add_ test_multiple_subgraphs inner_fn x y x = x + y = y + z = x sum + y sum z inner_fn b = + b = b + c = b cos sum c fn x y x = torch cos x y = torch sin y o = inner_fn x y o = inner_fn x y o = torch sin o o = inner_fn x y o = inner_fn x y o = inner_fn x y o o o + o x = torch rand requires_grad=True y = torch rand requires_grad=True x_clone = x clone requires_grad_ True y_clone = y clone requires_grad_ True ref_result = fn x y result graphs fw_graphs = run_and_return_graphs fn x_clone y_clone torch allclose ref_result result ref_result sum backward result sum backward assertEqual len graphs assertEqual len fw_graphs assertExpectedInline graph_str graphs \ GraphModule torch nn Module forward L_x_ f L_y_ f subgraph_ = subgraph_ subgraph_ = subgraph_ l_x_ = L_x_ l_y_ = L_y_ x f = torch cos l_x_ y f = torch sin l_y_ invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ l_y_ getitem f = invoke_subgraph invoke_subgraph = None o f = torch sin getitem getitem = None invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ y getitem_ f = invoke_subgraph_ invoke_subgraph_ = None mul_ f = o getitem_ o = getitem_ = None invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ l_y_ subgraph_ = l_x_ = l_y_ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ x y invoke_subgraph_ = None invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ x y subgraph_ = x = y = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None mul_ f = mul_ getitem_ mul_ = getitem_ = None add_ f = mul_ + getitem_ mul_ = getitem_ = None add_ subgraph_ torch nn Module forward subgraph_input_x subgraph_input_y f = subgraph_input_x + subgraph_input_x = None b f = subgraph_input_y + subgraph_input_y = None cos_ f = b cos b = None sum_ f = cos_ sum cos_ = None c f = sum_ = sum_ = None c subgraph_ torch nn Module forward subgraph_input_l_x_ subgraph_input_l_y_ x f = subgraph_input_l_x_ + subgraph_input_l_x_ = None y f = subgraph_input_l_y_ + subgraph_input_l_y_ = None sum_ f = x sum x = None sum_ f = y sum y = None z f = sum_ + sum_ sum_ = sum_ = None z assertExpectedInline graph_str fw_graphs \ GraphModule torch nn Module forward primals_ f primals_ f cos f = torch ops aten cos default primals_ sin f = torch ops aten sin default primals_ partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ primals_ partitioned_fw_subgraph_ _ = None getitem f = invoke_subgraph_ invoke_subgraph_ = None sin_ f = torch ops aten sin default getitem partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ sin partitioned_fw_subgraph_ _ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None mul f = torch ops aten mul Tensor sin_ getitem_ sin_ = None partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ primals_ partitioned_fw_subgraph_ _ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ cos sin partitioned_fw_subgraph_ _ = cos = sin = None getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ getitem_ f = invoke_subgraph_ invoke_subgraph_ = None mul_ f = torch ops aten mul Tensor mul getitem_ mul = None add f = torch ops aten add Tensor mul_ getitem_ mul_ = getitem_ = None add primals_ primals_ getitem getitem_ getitem_ getitem_ getitem_ getitem_ partitioned_fw_subgraph_ _ torch nn Module forward primals_ f primals_ f add f = torch ops aten add Tensor primals_ primals_ = None add_ f = torch ops aten add Tensor primals_ primals_ = None sum_ f = torch ops aten sum default add add = None sum_ f = torch ops aten sum default add_ add_ = None add_ f = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None add_ partitioned_fw_subgraph_ _ torch nn Module forward primals_ f primals_ f add f = torch ops aten add Tensor primals_ add_ f = torch ops aten add Tensor primals_ cos f = torch ops aten cos default add_ add_ = None sum_ f = torch ops aten sum default cos cos = None mul f = torch ops aten mul Tensor add sum_ add = None mul primals_ primals_ sum_ test_dependent_subgraphs inner_fn x y x = x + y = y + z = x sum + y sum z fn x y o = inner_fn x y o = inner_fn x o o x = torch rand requires_grad=True y = torch rand requires_grad=True x_clone = x clone requires_grad_ True y_clone = y clone requires_grad_ True ref_result = fn x y result graphs fw_graphs = run_and_return_graphs fn x_clone y_clone torch allclose ref_result result ref_result sum backward result sum backward assertEqual len graphs assertEqual len fw_graphs assertExpectedInline graph_str fw_graphs \ GraphModule torch nn Module forward primals_ f primals_ f add f = torch ops aten add Tensor primals_ primals_ = None sum_ f = torch ops aten sum default add add = None partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ sum_ partitioned_fw_subgraph_ _ = sum_ = None getitem f = invoke_subgraph_ invoke_subgraph_ = None add_ f = torch ops aten add Tensor getitem getitem = None sum_ f = torch ops aten sum default add_ add_ = None partitioned_fw_subgraph_ _ = partitioned_fw_subgraph_ _ invoke_subgraph_ = torch ops higher_order invoke_subgraph partitioned_fw_subgraph_ _ partitioned_fw_subgraph_ _ primals_ sum_ partitioned_fw_subgraph_ _ = primals_ = sum_ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None getitem_ partitioned_fw_subgraph_ _ torch nn Module forward primals_ f primals_ f add f = torch ops aten add Tensor primals_ primals_ = None sum_ f = torch ops aten sum default add add = None add_ f = torch ops aten add Tensor sum_ primals_ sum_ = primals_ = None add_ test_input_mutation inner_fn x y x = x + y = y + x add_ x y add_ y x sum + y sum fn x y x = torch sin x o = inner_fn x y o = inner_fn x clone y clone o + o x = torch rand requires_grad=False y = torch rand requires_grad=False x_clone = x clone y_clone = y clone ref_result = fn x y result graphs fw_graphs = run_and_return_graphs fn x_clone y_clone torch allclose ref_result result assertEqual len graphs assertEqual len fw_graphs assertExpectedInline graph_str fw_graphs \ lambda torch nn Module forward arg _ f arg _ f sin f = torch ops aten sin default arg _ arg _ = None add f = torch ops aten add Tensor sin add_ f = torch ops aten add Tensor arg _ add_ f = torch ops aten add Tensor sin add sin = add = None add_ f = torch ops aten add Tensor arg _ add_ add_ = None clone f = torch ops aten clone default add_ clone_ f = torch ops aten clone default add_ add_ f = torch ops aten add Tensor clone add_ f = torch ops aten add Tensor clone_ add_ f = torch ops aten add Tensor clone add_ clone = add_ = None add_ f = torch ops aten add Tensor clone_ add_ clone_ = add_ = None repeated_subgraph = repeated_subgraph invoke_subgraph = torch ops higher_order invoke_subgraph repeated_subgraph subgraph_ add_ add_ repeated_subgraph = add_ = None getitem f = invoke_subgraph invoke_subgraph = None repeated_subgraph _ = repeated_subgraph invoke_subgraph_ = torch ops higher_order invoke_subgraph repeated_subgraph _ subgraph_ add_ add_ repeated_subgraph _ = add_ = add_ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None add_ f = torch ops aten add Tensor getitem getitem_ getitem = getitem_ = None copy_ f = torch ops aten copy_ default arg _ add_ arg _ = add_ = copy_ = None add_ repeated_subgraph torch nn Module forward arg _ f arg _ f sum_ f = torch ops aten sum default arg _ arg _ = None sum_ f = torch ops aten sum default arg _ arg _ = None add f = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None add test_input_aliasing inner_fn x y x = x view x size x view x size inner_fn x y x = x y = y x sum + y sum fn x y o = inner_fn x y o = inner_fn x y o = inner_fn x y o = inner_fn x y o + o + o sum + o sum x = torch rand requires_grad=False y = torch rand requires_grad=False x_clone = x clone y_clone = y clone ref_result = fn x y result graphs fw_graphs = run_and_return_graphs fn x_clone y_clone torch allclose ref_result result assertEqual len graphs assertEqual len fw_graphs assertExpectedInline graph_str fw_graphs \ lambda torch nn Module forward arg _ f arg _ f view f = torch ops aten view default arg _ view_ f = torch ops aten view default view view = None view_ f = torch ops aten view default arg _ view_ f = torch ops aten view default view_ view_ = None add f = torch ops aten add Tensor view_ view_ view_ = view_ = None repeated_subgraph = repeated_subgraph invoke_subgraph = torch ops higher_order invoke_subgraph repeated_subgraph subgraph_ arg _ arg _ repeated_subgraph = None getitem f = invoke_subgraph invoke_subgraph = None sum_ f = torch ops aten sum default getitem getitem = None add_ f = torch ops aten add Tensor add sum_ add = sum_ = None repeated_subgraph _ = repeated_subgraph invoke_subgraph_ = torch ops higher_order invoke_subgraph repeated_subgraph _ subgraph_ arg _ arg _ repeated_subgraph _ = arg _ = arg _ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None sum_ f = torch ops aten sum default getitem_ getitem_ = None add_ f = torch ops aten add Tensor add_ sum_ add_ = sum_ = None add_ repeated_subgraph torch nn Module forward arg _ f arg _ f mul f = torch ops aten mul Tensor arg _ arg _ = None mul_ f = torch ops aten mul Tensor arg _ arg _ = None sum_ f = torch ops aten sum default mul mul = None sum_ f = torch ops aten sum default mul_ mul_ = None add f = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None add test_cycle_detection_no_cycle mod = run_and_get_simple_graph assertExpectedInline _detect_cycles mod graph no cycle detected test_cycle_detection_single_node fn x y x = x + y = y + z = x sum + y sum z x = torch rand requires_grad=False y = torch rand requires_grad=False _ _ fw_graphs = run_and_return_graphs fn x y mod = fw_graphs add_node = next n n mod graph nodes n name == add add_ = next n n mod graph nodes n name == add_ args = add_node args add_node args = args add_ assertExpectedInline _detect_cycles mod graph add_ OrderedSet add_ cycle detected path deque output add_ add_ test_cycle_detection_two_node fn x y x = x + y = y + z = x sum + y sum z x = torch rand requires_grad=False y = torch rand requires_grad=False _ _ fw_graphs = run_and_return_graphs fn x y mod = fw_graphs add_node = next n n mod graph nodes n name == add add_ = next n n mod graph nodes n name == add_ args = add_node args add_node args = args add_ assertExpectedInline _detect_cycles mod graph add_ OrderedSet add_node add_node OrderedSet add_ cycle detected path deque output add_ add add_ test_cycle_detection_arg_and_additional_deps fn x y x = x + y = y + z = x sum + y sum z x = torch rand requires_grad=False y = torch rand requires_grad=False _ _ fw_graphs = run_and_return_graphs fn x y mod = fw_graphs add_node = next n n mod graph nodes n name == add add_ = next n n mod graph nodes n name == add_ args = add_node args add_node args = args add_ assertExpectedInline _detect_cycles mod graph add_ OrderedSet add_node cycle detected path deque output add_ add add_ test_cycle_detection_simple mod = run_and_get_simple_graph add_node = next n n mod graph nodes n name == add add_ = next n n mod graph nodes n name == add_ args = add_node args add_node args = args add_ assertExpectedInline _detect_cycles mod graph cycle detected path deque output add_ sum_ add add_ test_cycle_detection_complex inner_fn x y x = x view x size x view x size inner_fn x y x = x y = y x sum + y sum fn x y o = inner_fn x y o = inner_fn x y o = inner_fn x y o = inner_fn x y o + o + o sum + o sum x = torch rand requires_grad=False y = torch rand requires_grad=False x_clone = x clone y_clone = y clone _ _ fw_graphs = run_and_return_graphs fn x_clone y_clone mod = fw_graphs invoke_subgraph_node = next n n mod graph nodes n name == invoke_subgraph add_ = next n n mod graph nodes n name == add_ args = invoke_subgraph_node args invoke_subgraph_node args = add_ args assertExpectedInline _detect_cycles mod graph cycle detected path deque output add_ add_ sum_ getitem invoke_subgraph add_ test_autocast_ordering torch _dynamo graph_deduplication _populate_additional_deps _stable_topological_sort inner_fn x y x = x view x size x view x size inner_fn x y x = x y = y x sum + y sum fn x y o = inner_fn x y o = inner_fn x y o = inner_fn x y o = inner_fn x y o + o + o sum + o sum x = torch rand requires_grad=False y = torch rand requires_grad=False x_clone = x clone y_clone = y clone _ _ fw_graphs = run_and_return_graphs fn x_clone y_clone mod = fw_graphs get_node name next n n mod graph nodes n name == name sum_ = get_node sum_ enter_autocast = mod graph call_function torch amp _enter_autocast sum_ append enter_autocast sum_ = get_node sum_ exit_autocast = mod graph call_function torch amp _exit_autocast sum_ append exit_autocast additional_deps = _populate_additional_deps mod graph invoke_subgraph = get_node invoke_subgraph invoke_subgraph append enter_autocast getitem_ = get_node getitem_ getitem_ append exit_autocast assertExpectedInline graph_str mod \ lambda torch nn Module forward arg _ f arg _ f view f = torch ops aten view default arg _ view_ f = torch ops aten view default view view = None view_ f = torch ops aten view default arg _ view_ f = torch ops aten view default view_ view_ = None add f = torch ops aten add Tensor view_ view_ view_ = view_ = None repeated_subgraph = repeated_subgraph invoke_subgraph = torch ops higher_order invoke_subgraph repeated_subgraph subgraph_ arg _ arg _ repeated_subgraph = None _enter_autocast = torch amp autocast_mode _enter_autocast _enter_autocast = None getitem f = invoke_subgraph invoke_subgraph = None sum_ f = torch ops aten sum default getitem getitem = None add_ f = torch ops aten add Tensor add sum_ add = sum_ = None repeated_subgraph _ = repeated_subgraph invoke_subgraph_ = torch ops higher_order invoke_subgraph repeated_subgraph _ subgraph_ arg _ arg _ repeated_subgraph _ = arg _ = arg _ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None _exit_autocast = torch amp autocast_mode _exit_autocast _exit_autocast = None sum_ f = torch ops aten sum default getitem_ getitem_ = None add_ f = torch ops aten add Tensor add_ sum_ add_ = sum_ = None add_ repeated_subgraph torch nn Module forward arg _ f arg _ f mul f = torch ops aten mul Tensor arg _ arg _ = None mul_ f = torch ops aten mul Tensor arg _ arg _ = None sum_ f = torch ops aten sum default mul mul = None sum_ f = torch ops aten sum default mul_ mul_ = None add f = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None add _stable_topological_sort mod graph additional_deps assertExpectedInline graph_str mod \ lambda torch nn Module forward arg _ f arg _ f view f = torch ops aten view default arg _ view_ f = torch ops aten view default view view = None view_ f = torch ops aten view default arg _ view_ f = torch ops aten view default view_ view_ = None add f = torch ops aten add Tensor view_ view_ view_ = view_ = None repeated_subgraph = repeated_subgraph invoke_subgraph = torch ops higher_order invoke_subgraph repeated_subgraph subgraph_ arg _ arg _ repeated_subgraph = None getitem f = invoke_subgraph invoke_subgraph = None sum_ f = torch ops aten sum default getitem getitem = None _enter_autocast = torch amp autocast_mode _enter_autocast _enter_autocast = None add_ f = torch ops aten add Tensor add sum_ add = sum_ = None repeated_subgraph _ = repeated_subgraph invoke_subgraph_ = torch ops higher_order invoke_subgraph repeated_subgraph _ subgraph_ arg _ arg _ repeated_subgraph _ = arg _ = arg _ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None sum_ f = torch ops aten sum default getitem_ getitem_ = None _exit_autocast = torch amp autocast_mode _exit_autocast _exit_autocast = None add_ f = torch ops aten add Tensor add_ sum_ add_ = sum_ = None add_ repeated_subgraph torch nn Module forward arg _ f arg _ f mul f = torch ops aten mul Tensor arg _ arg _ = None mul_ f = torch ops aten mul Tensor arg _ arg _ = None sum_ f = torch ops aten sum default mul mul = None sum_ f = torch ops aten sum default mul_ mul_ = None add f = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None add test_output_nodes_last torch _dynamo graph_deduplication _stable_topological_sort inner_fn x y x = x view x size x view x size inner_fn x y x = x y = y x sum + y sum fn x y o = inner_fn x y o = inner_fn x y o = inner_fn x y o = inner_fn x y o + o + o sum + o sum x = torch rand requires_grad=False y = torch rand requires_grad=False x_clone = x clone y_clone = y clone _ _ fw_graphs = run_and_return_graphs fn x_clone y_clone mod = fw_graphs output = next n n mod graph nodes n op == output add_ = next n n mod graph nodes n name == sum_ add_ append output assertExpectedInline graph_str mod \ lambda torch nn Module forward arg _ f arg _ f view f = torch ops aten view default arg _ view_ f = torch ops aten view default view view = None view_ f = torch ops aten view default arg _ view_ f = torch ops aten view default view_ view_ = None add f = torch ops aten add Tensor view_ view_ view_ = view_ = None repeated_subgraph = repeated_subgraph invoke_subgraph = torch ops higher_order invoke_subgraph repeated_subgraph subgraph_ arg _ arg _ repeated_subgraph = None getitem f = invoke_subgraph invoke_subgraph = None sum_ f = torch ops aten sum default getitem getitem = None add_ f = torch ops aten add Tensor add sum_ add = sum_ = None repeated_subgraph _ = repeated_subgraph invoke_subgraph_ = torch ops higher_order invoke_subgraph repeated_subgraph _ subgraph_ arg _ arg _ repeated_subgraph _ = arg _ = arg _ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None sum_ f = torch ops aten sum default getitem_ getitem_ = None add_ add_ f = torch ops aten add Tensor add_ sum_ add_ = sum_ = None repeated_subgraph torch nn Module forward arg _ f arg _ f mul f = torch ops aten mul Tensor arg _ arg _ = None mul_ f = torch ops aten mul Tensor arg _ arg _ = None sum_ f = torch ops aten sum default mul mul = None sum_ f = torch ops aten sum default mul_ mul_ = None add f = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None add _stable_topological_sort mod graph assertExpectedInline graph_str mod \ lambda torch nn Module forward arg _ f arg _ f view f = torch ops aten view default arg _ view_ f = torch ops aten view default view view = None view_ f = torch ops aten view default arg _ view_ f = torch ops aten view default view_ view_ = None add f = torch ops aten add Tensor view_ view_ view_ = view_ = None repeated_subgraph = repeated_subgraph invoke_subgraph = torch ops higher_order invoke_subgraph repeated_subgraph subgraph_ arg _ arg _ repeated_subgraph = None getitem f = invoke_subgraph invoke_subgraph = None sum_ f = torch ops aten sum default getitem getitem = None add_ f = torch ops aten add Tensor add sum_ add = sum_ = None repeated_subgraph _ = repeated_subgraph invoke_subgraph_ = torch ops higher_order invoke_subgraph repeated_subgraph _ subgraph_ arg _ arg _ repeated_subgraph _ = arg _ = arg _ = None getitem_ f = invoke_subgraph_ invoke_subgraph_ = None sum_ f = torch ops aten sum default getitem_ getitem_ = None add_ f = torch ops aten add Tensor add_ sum_ add_ = sum_ = None add_ repeated_subgraph torch nn Module forward arg _ f arg _ f mul f = torch ops aten mul Tensor arg _ arg _ = None mul_ f = torch ops aten mul Tensor arg _ arg _ = None sum_ f = torch ops aten sum default mul mul = None sum_ f = torch ops aten sum default mul_ mul_ = None add f = torch ops aten add Tensor sum_ sum_ sum_ = sum_ = None add test_mutation_ordering torch _dynamo graph_deduplication _stable_topological_sort inner_fn x y x = x view x size x view x size inner_fn x y x = x y = y x sum + y sum fn x y o = inner_fn x y o = inner_fn x y x add_ x o = inner_fn x y y mul_ y o = inner_fn x y o + o + o sum + o sum x = torch rand y = torch rand x_clone = x clone y_clone = y clone graph _ = extract_graph_and_tracker fn x_clone y_clone graph_code graph graph python_code src get_node name next n n graph nodes n name == name assertExpectedInline graph_code graph \ forward L_x_ torch Tensor L_y_ torch Tensor subgraph_ = subgraph_ l_x_ = L_x_ l_y_ = L_y_ x = l_x_ view o = x view x = None x _ = l_x_ view o = x _ view x _ = None add_ = l_x_ add_ l_x_ add_ = None add_ = o + o o = o = None invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ l_y_ mul_ = l_y_ mul_ l_y_ mul_ = None getitem = invoke_subgraph invoke_subgraph = None sum_ = getitem sum getitem = None add_ = add_ + sum_ add_ = sum_ = None invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ l_y_ subgraph_ = l_x_ = l_y_ = None getitem_ = invoke_subgraph_ invoke_subgraph_ = None sum_ = getitem_ sum getitem_ = None add_ = add_ + sum_ add_ = sum_ = None add_ Shuffle nodes graph add_ = get_node add_ mul_ = get_node mul_ o = get_node o o append mul_ add_ = get_node add_ add_ append add_ assertExpectedInline graph_code graph \ forward L_x_ torch Tensor L_y_ torch Tensor subgraph_ = subgraph_ l_x_ = L_x_ l_y_ = L_y_ x = l_x_ view o = x view x = None x _ = l_x_ view o = x _ view x _ = None mul_ = l_y_ mul_ l_y_ mul_ = None add_ = o + o o = o = None add_ = l_x_ add_ l_x_ add_ = None invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ l_y_ getitem = invoke_subgraph invoke_subgraph = None sum_ = getitem sum getitem = None add_ = add_ + sum_ add_ = sum_ = None invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ l_y_ subgraph_ = l_x_ = l_y_ = None getitem_ = invoke_subgraph_ invoke_subgraph_ = None sum_ = getitem_ sum getitem_ = None add_ = add_ + sum_ add_ = sum_ = None add_ _stable_topological_sort graph torch _dynamo graph_deduplication last_node_to_additional_deps assertExpectedInline graph_code graph \ forward L_x_ torch Tensor L_y_ torch Tensor subgraph_ = subgraph_ l_x_ = L_x_ l_y_ = L_y_ x = l_x_ view o = x view x = None x _ = l_x_ view o = x _ view x _ = None add_ = o + o o = o = None add_ = l_x_ add_ l_x_ add_ = None invoke_subgraph = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ l_y_ mul_ = l_y_ mul_ l_y_ mul_ = None getitem = invoke_subgraph invoke_subgraph = None sum_ = getitem sum getitem = None add_ = add_ + sum_ add_ = sum_ = None invoke_subgraph_ = torch ops higher_order invoke_subgraph subgraph_ subgraph_ l_x_ l_y_ subgraph_ = l_x_ = l_y_ = None getitem_ = invoke_subgraph_ invoke_subgraph_ = None sum_ = getitem_ sum getitem_ = None add_ = add_ + sum_ add_ = sum_ = None add_ test_tuple_return allow_in_graph tuple_return x y x y inner_fn x y x = x + x + y = y + y + tuple_return x y fn x x x y y y x = inner_fn x y x = inner_fn x y x = inner_fn x y x x x fn_opt = torch compile fn fullgraph=True inps = torch rand _ range result_compiled = fn_opt inps result_eager = fn inps assertEqual result_compiled result_eager test_tuple_inputs torch _dynamo config patch use_graph_deduplication False torch _dynamo config patch track_nodes_for_deduplication True inner x y x x = torch split x x + x + y fn x y o = inner x y o = inner x y o = inner x y o = inner x y o sum + o sum + o sum + o sum graph tracker = extract_graph_and_tracker fn torch rand torch rand MockOutputGraph __init__ graph = graph region_tracker = tracker nn_modules = FakeRootModule install_subgraph name subgraph splits = n n graph nodes n op == call_function n target == torch split split splits tracker node_to_duplicates pop split apply_graph_deduplication MockOutputGraph assertExpectedInline graph \ graph _unnamed num_users= = get_attr target= l_x_ torch Tensor num_users= = placeholder target=L_x_ l_y_ torch Tensor num_users= = placeholder target=L_y_ split num_users= = call_function target=torch functional split args = l_x_ kwargs = x num_users= = call_function target=operator getitem args = split kwargs = x num_users= = call_function target=operator getitem args = split kwargs = split_ num_users= = call_function target=torch functional split args = l_x_ kwargs = x _ num_users= = call_function target=operator getitem args = split_ kwargs = x _ num_users= = call_function target=operator getitem args = split_ kwargs = split_ num_users= = call_function target=torch functional split args = l_x_ kwargs = x _ num_users= = call_function target=operator getitem args = split_ kwargs = x _ num_users= = call_function target=operator getitem args = split_ kwargs = split_ num_users= = call_function target=torch functional split args = l_x_ kwargs = x _ num_users= = call_function target=operator getitem args = split_ kwargs = x _ num_users= = call_function target=operator getitem args = split_ kwargs = invoke_subgraph num_users= = call_function target=torch ops higher_order invoke_subgraph args = _unnamed x x l_y_ kwargs = getitem_ num_users= = call_function target=operator getitem args = invoke_subgraph kwargs = sum_ num_users= = call_method target=sum args = getitem_ kwargs = invoke_subgraph_ num_users= = call_function target=torch ops higher_order invoke_subgraph args = _unnamed x _ x _ l_y_ kwargs = getitem_ num_users= = call_function target=operator getitem args = invoke_subgraph_ kwargs = sum_ num_users= = call_method target=sum args = getitem_ kwargs = add_ num_users= = call_function target=operator add args = sum_ sum_ kwargs = invoke_subgraph_ num_users= = call_function target=torch ops higher_order invoke_subgraph args = _unnamed x _ x _ l_y_ kwargs = getitem_ num_users= = call_function target=operator getitem args = invoke_subgraph_ kwargs = sum_ num_users= = call_method target=sum args = getitem_ kwargs = add_ num_users= = call_function target=operator add args = add_ sum_ kwargs = invoke_subgraph_ num_users= = call_function target=torch ops higher_order invoke_subgraph args = _unnamed x _ x _ l_y_ kwargs = getitem_ num_users= = call_function target=operator getitem args = invoke_subgraph_ kwargs = sum_ num_users= = call_method target=sum args = getitem_ kwargs = add_ num_users= = call_function target=operator add args = add_ sum_ kwargs = add_ test_param_transfer_to_submodule inner_fn x y x + y + y + x fn x x x y y y x = inner_fn x y x = inner_fn x y x = inner_fn x y x sum + x sum + x sum fn_opt = torch compile fn fullgraph=True args = torch rand _ range arg args torch _dynamo mark_static_address arg fn_opt args __name__ == __main__ torch _dynamo test_case run_tests run_tests