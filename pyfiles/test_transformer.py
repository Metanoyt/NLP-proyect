Copyright c Meta Platforms Inc affiliates Owner s oncall distributed torch torch distributed pipelining pipeline SplitPoint torch testing _internal common_device_type instantiate_device_type_tests torch testing _internal common_utils run_tests TestCase d_hid = n_layers = microbatch_size = MLPModule torch nn Module __init__ d_hid super __init__ net = torch nn Linear d_hid d_hid relu = torch nn ReLU net = torch nn Linear d_hid d_hid forward x x = net x x = relu x x = net x x TransformerLike torch nn Module __init__ - None super __init__ layers = torch nn Sequential MLPModule d_hid _ range n_layers forward x torch Tensor - torch Tensor layers x TransformerTests TestCase test_ir device transformer = TransformerLike device x = torch randn microbatch_size d_hid device=device Split into stages num_stages = split_spec = f layers n_layers num_stages SplitPoint BEGINNING pipe = pipeline transformer x split_spec=split_spec assert pipe num_stages == num_stages f pipe num_stages= expect num_stages get_layers module layers = name name _ module layers named_children layers Collect all layers pipe layers = stage_idx range pipe num_stages stage_mod = pipe get_stage_module stage_idx layers += get_layers stage_mod Check layer completeness orig_layers = get_layers transformer assert sorted layers == sorted orig_layers f layers = orig_layers print Layers matched Check equivalence ref = transformer x out = pipe x torch testing assert_close out ref print f Equivalence test passed torch sum out ref torch sum ref devices = cpu cuda hpu xpu instantiate_device_type_tests TransformerTests globals only_for=devices allow_xpu=True __name__ == __main__ run_tests