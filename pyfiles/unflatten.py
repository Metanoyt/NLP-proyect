mypy allow-untyped-defs abc copy logging operator re collections defaultdict collections abc Callable contextlib contextmanager copy deepcopy dataclasses dataclass enum Enum typing Any cast Optional Union torch torch fx _pytree fx_pytree torch utils _pytree pytree torch _library fake_class_registry FakeScriptObject torch export ExportedProgram torch export _tree_utils reorder_kwargs torch export exported_program ConstantArgument ExportGraphSignature InputKind ModuleCallSignature SymBoolArgument SymFloatArgument SymIntArgument TensorArgument torch fx _symbolic_trace is_fx_symbolic_tracing torch fx graph_module _get_attr _get_attr_via_attr_list _print_readable torch utils _pytree GetAttrKey SequenceKey _remove_effect_tokens_pass _remove_effect_tokens log = logging getLogger __name__ __all__ = FlatArgsAdapter InterpreterModule InterpreterModuleDispatcher UnflattenedModule unflatten _AttrKind Enum PARAMETER = parameter BUFFER = buffer CONSTANT = constant MODULE = module dataclass frozen=True _TensorID Custom tensor identifier containing storage stride size information untyped_storage torch UntypedStorage stride tuple size tuple storage_offset int RUN_WITH_INTERPRETER = True contextmanager _disable_interpreter global RUN_WITH_INTERPRETER old_flag = RUN_WITH_INTERPRETER RUN_WITH_INTERPRETER = False try yield finally RUN_WITH_INTERPRETER = old_flag Assign attribute from_obj qualified name target to_module This installs empty Modules where none exist yet they subpaths target _assign_attr from_obj Union torch Tensor torch ScriptObject torch nn Module to_module torch nn Module target str attr_kind _AttrKind persistent bool = True prefix field = target split We need generate all submodules ` to_module ` ` prefix ` variants ` prefix ` differ only call name All these submodules will then assigned ` from_obj ` ` field ` so they can share attribute For example target foo bar f foo has another call name foo bar has other call names bar bar then we will assign f foo bar foo bar foo bar foo bar foo bar foo bar to_modules = to_module item prefix ts set torch nn Module = set to_module to_modules hasattr to_module item setattr to_module item torch nn Module ts update t_call type ignore misc k t_call to_module _modules items _is_call_name k item to_modules = ts to_module to_modules attr_kind == _AttrKind PARAMETER assert isinstance from_obj torch nn Parameter to_module register_parameter field from_obj attr_kind == _AttrKind BUFFER assert isinstance from_obj torch Tensor to_module register_buffer field from_obj persistent=persistent attr_kind == _AttrKind CONSTANT assert isinstance from_obj FakeScriptObject FakeScriptObject should only exist during tracing assert isinstance from_obj torch Tensor torch ScriptObject setattr to_module field from_obj attr_kind == _AttrKind MODULE assert isinstance from_obj torch nn Module setattr to_module field from_obj _SubmoduleBase _ty Optional str type_name - Optional str Subclass - InterpreterModule InterpreterModuleDispatcher represents corresponding model eager model To get type information those modules eager model we need use method _ty InterpreterModule _SubmoduleBase torch nn Module A module uses torch fx Interpreter execute instead usual codegen GraphModule uses This provides better stack trace information makes easier debug execution graph_module Optional torch fx GraphModule __init__ graph torch fx Graph ty Optional str = None super __init__ graph = graph _ty = ty graph owning_module = type ignore assignment _run_with_interpreter = RUN_WITH_INTERPRETER forward args kwargs assert graph_module None Didn t finalize InterpreterModule is_fx_symbolic_tracing torch compiler is_dynamo_compiling _run_with_interpreter Dynamo cannot trace through torch fx Interpreter so fall back GraphModule codegen instance Patch codegened forward run InterpreterModule so attribute accesses etc module instead type graph_module forward args kwargs kwargs Handle kwargs FX only natively supports positional arguments through placeholders So order pass kwargs we must correspond names placeholders keys kwarg dict arg_list = list args kwarg_names = arg_names len arg_list arg_list extend kwargs kwarg_name kwarg_name kwarg_names kwarg_name kwargs Assert kwargs passed exactly match positional arguments specified GraphModule This should guaranteed unflattening process assert len kwarg_names == len kwargs assert len arg_list == len arg_names args = tuple arg_list torch fx Interpreter graph=self graph run args enable_io_processing=False finalize We need finalize because GraphModule populates its own state_dict based get_attrs observed graph So we need fully construct graph call _sink_params before generating GraphModule need set ` graph_module ` directly dict avoid getting registered submodule __dict__ graph_module = torch fx GraphModule graph graph lint Cache arg names kwarg handling see forward arg_names = node graph nodes node op == placeholder arg_names append node target print_readable print_output=True include_stride=False include_device=False colored=False _print_readable InterpreterModule print_output include_stride include_device colored InterpreterModuleDispatcher _SubmoduleBase torch nn Module A module carries sequence InterpreterModules corresponding sequence calls module Each call module dispatches next InterpreterModule wraps back around after last __init__ attrs set str call_modules list InterpreterModule super __init__ assert call_modules _modules = call_modules _modules accessor attrs setattr accessor getattr call_modules accessor _ty = call_modules _ty _call_modules = call_modules _num_calls = forward args kwargs call_module = _call_modules _num_calls _num_calls = _num_calls + len _call_modules try call_module args kwargs except Exception _num_calls = raise call_modules _call_modules print_readable print_output=True include_stride=False include_device=False colored=False outputs = mod print_readable print_output include_stride include_device colored mod _call_modules \n join outputs FlatArgsAdapter abc ABC Adapts input arguments ` ` input_spec ` ` align ` ` target_spec ` ` abc abstractmethod adapt target_spec pytree TreeSpec input_spec pytree TreeSpec input_args list Any metadata Optional dict str Any = None obj Optional Any = None - list Any NOTE This adapter may mutate given ` ` input_args_with_path ` ` get_flat_arg_paths - list str Returns list paths used access flat args UnflattenedModule _SubmoduleBase torch nn Module __init__ export_module ExportedProgram flat_args_adapter Optional FlatArgsAdapter = None super __init__ export_module graph_signature backward_signature None raise ValueError Unflattening JointExportModule NYI _id obj Returns _TensorID dataclass tensors otherwise id isinstance obj torch Tensor _TensorID untyped_storage=obj untyped_storage stride=obj stride size=obj size storage_offset=obj storage_offset type ignore arg-type id obj fqn_list = entry fqn entry export_module module_call_graph assert fqn_list == export_graph = deepcopy export_module graph graph_signature = deepcopy export_module graph_signature graph = torch fx Graph graph owning_module = type ignore assignment module_call_graph = deepcopy export_module module_call_graph flat_args_adapter = flat_args_adapter meta = export_module graph_module meta meta unflattened_module = Flag indicate whether args have been adapted adapted = False _run_with_interpreter = RUN_WITH_INTERPRETER _inplace_buffer_and_input_mutations export_graph graph_signature _fix_nn_module_stacks export_graph _ty = _root_module_type export_graph ivals = _IVals any intermediate value mutation read track mutation seen_modules seen_attrs = _outline_submodules export_graph each read intermediate value mutation find where created perform mutation ivals update seen_modules values move attributes correspond graph arguments HOPs exported program unflattened submodules _copy_graph_attrs export_module _graph_module seen_attrs range_constraints = export_module range_constraints equality_constraints list = aliasing unused param buffer issues strict-mode export dynamo export will deduplicate aliased tensors ignore unused tensors For aliasing causes issues when some aliases unused we re unable match placeholder node correct FQN This leads graph signature potentially having wrong target FQN downstream issues where parameters assigned wrong target attribute mismatching relevant placeholder node unflattened module To resolve we restore _assign_attr all aliased unused tensors state_dict module attributes only keep used tensors graph s forward pass _sink_params state_dict = export_module state_dict assigned_params set str = set tracking unused params id_to_param dict Union int _TensorID torch nn Parameter = handling weight-sharing name graph_signature parameters loop adds used params param = state_dict name _id param id_to_param id_to_param _id param = torch nn Parameter param clone requires_grad=param requires_grad _assign_attr id_to_param _id param name attr_kind=_AttrKind PARAMETER assigned_params add name non_persistent_buffers = set graph_signature non_persistent_buffers assigned_buffers set str = set tracking unused buffers id_to_buffer dict Union int _TensorID tuple torch nn Parameter bool = name graph_signature buffers loop adds used buffers name non_persistent_buffers persistent = False buffer = export_module constants name persistent = True buffer = state_dict name _id buffer id_to_buffer id_to_buffer _id buffer = buffer clone persistent _assign_attr id_to_buffer _id buffer name attr_kind=_AttrKind BUFFER persistent=persistent assigned_buffers add name restore aliased unused params buffers these appear state dict graph signature name tensor state_dict items name assigned_params name assigned_buffers already assigned continue is_buffer = False _id tensor id_to_buffer isinstance tensor torch nn Parameter aliased buffer is_buffer = True is_buffer _id tensor id_to_buffer completely unused weight-sharing id_to_buffer _id tensor = tensor True assign respect original model _assign_attr id_to_buffer _id tensor name attr_kind=_AttrKind BUFFER persistent=True _id tensor id_to_param unused id_to_param _id tensor = tensor _assign_attr id_to_param _id tensor name attr_kind=_AttrKind PARAMETER use id map so we don t double-clone aliased constants id_to_const dict Union int _TensorID Union torch Tensor torch _C ScriptObject = fqn constant export_module constants items _id constant id_to_const isinstance constant torch Tensor constant = constant clone id_to_const _id constant = constant _constant = id_to_const _id constant _assign_attr _constant fqn attr_kind=_AttrKind CONSTANT This handle parameters buffers point same tensor object id - list node_name target_name consts_map dict Union int _TensorID list tuple str str = defaultdict list consts_targets set str = set add_to_consts_map obj_id node_name target_name name_list = consts_map obj_id name_list append node_name target_name track aliased unused params buffers prefer using untyped_storage over id when s available added_params_buffers set str = set s graph_signature input_specs s kind == InputKind PARAMETER s kind == InputKind BUFFER s persistent assert hasattr s arg name assert isinstance s target str add_to_consts_map _id export_module state_dict s target s arg name s target consts_targets add s target added_params_buffers add s target s kind == InputKind BUFFER s persistent s kind == InputKind CONSTANT_TENSOR s kind == InputKind CUSTOM_OBJ assert hasattr s arg name assert isinstance s target str add_to_consts_map _id export_module constants s target s arg name s target consts_targets add s target add constants aliased don t appear graph signature const_name const export_module constants items const_name consts_targets const_id = _id const assert const_id consts_map ph_name _ = consts_map const_id add_to_consts_map const_id ph_name const_name added_params_buffers add s target add aliased unused params buffers don t appear graph signature fqn tensor export_module state_dict items fqn added_params_buffers tensor_id = _id tensor tensor_id consts_map completely unused no weight-sharing ignore weight doesn t appear graph module so won t cause FQN assignment issues continue ph_name _ = consts_map tensor_id add_to_consts_map tensor_id ph_name fqn node name - list possible targets inputs_to_state dict str list str = node_target consts_map values targets = t t node_target n _ node_target inputs_to_state n = targets _sink_params inputs_to_state redirected_call_indices = _deduplicate_modules seen_modules values fqn_list = fqn fqn fqn_list fqn redirected_call_indices _dispatch_modules redirected_call_indices consts_targets fqn_list = fqn fqn fqn_list fqn Cache so we don t have compute every time NOTE needs kept sync placeholders graph currently we have no way guarantee input_placeholders = node node graph nodes node op == placeholder check_input_constraints = True TODO zhxchen We can register modules ahead time instead reorder later fqn_order = fqn i i fqn enumerate fqn_list In case legacy IR we might missing some modules metadata name _ named_modules remove_duplicate=False name fqn_order fqn_order name = len fqn_order _reorder_submodules fqn_order graph lint finalize _print_graph fqn mod named_modules print fqn + hasattr mod graph isinstance mod graph torch fx Graph print mod graph _adapt_flat_args flat_args in_spec input signature = module_call_graph signature in_spec == signature in_spec flat_args flat_args_adapter None raise TypeError There no flat args adapter specified Are you sure you calling right arguments flat_args = flat_args_adapter adapt target_spec=signature in_spec input_spec=in_spec input_args=flat_args metadata=self meta obj=input len flat_args = signature in_spec num_leaves raise TypeError f Flat args adaption failed number args mismatch f Adatped len flat_args \n f Exported module signature in_spec num_leaves flat_args process_forward_inputs args kwargs signature = module_call_graph signature reordered_kwargs = kwargs kwargs reordered_kwargs = reorder_kwargs kwargs signature in_spec flat_args_with_path in_spec = pytree tree_flatten_with_path args reordered_kwargs flat_args = x x flat_args_with_path is_fx_symbolic_tracing flat_args in_spec = signature in_spec adapted print Input treespec does match exported module s \n f Input treespec in_spec f Exported module treespec signature in_spec print Adapting flat arg match exported module s treespec flat_args = _adapt_flat_args flat_args in_spec args adapted = True check_input_constraints Import here avoid unfortunate circular dependency TODO suo untangle torch _export utils _check_input_constraints_for_graph adapted True flat_arg_paths = flat_args_adapter get_flat_arg_paths flat_args_adapter assert flat_arg_paths len flat_arg_paths == len flat_args new_flat_args_with_path = type ignore var-annotated SequenceKey idx=idx GetAttrKey name=flat_arg_paths idx flat_arg_paths unknown location arg idx arg enumerate flat_args new_flat_args_with_path = flat_args_with_path type ignore assignment _check_input_constraints_for_graph input_placeholders new_flat_args_with_path range_constraints flat_args forward args kwargs flat_args = process_forward_inputs args kwargs signature = module_call_graph signature is_fx_symbolic_tracing return_val = torch fx Interpreter graph=self graph run flat_args enable_io_processing=False For scalar value fx Graph wraps tuple isinstance return_val tuple len return_val == return_val return_val torch compiler is_dynamo_compiling _run_with_interpreter tree_out = type graph_module forward flat_args type ignore union-attr tree_out = torch fx Interpreter graph=self graph run flat_args enable_io_processing=False pytree tree_unflatten tree_out signature out_spec finalize __dict__ graph_module = torch fx GraphModule graph graph lint _dispatch_modules redirected_call_indices consts_targets For module whose call signatures preserved replace multiple modules corresponding multiple calls module single dispatcher module tracks which module call each fqn whose module call signature preserved map fqn list called modules called_modules = defaultdict list entry module_call_graph entry fqn entry signature some modules removed their fqns redirected other fqns during deduplication fqn = entry fqn mod = _get_attr redirected_call_indices get fqn fqn base idx = fqn split fqn fqn called_modules base append int idx mod attrs_map = defaultdict set target consts_targets target orig_fqn name = target rsplit attrs_map orig_fqn add name attrs_map add target replace multiple call modules single dispatcher module orig_fqn indexed_call_modules called_modules items call_modules = mod _ mod sorted indexed_call_modules len call_modules i range len call_modules fqn = _call_name orig_fqn i + fqn redirected_call_indices prefix name = fqn split _get_attr_via_attr_list prefix _modules pop name set_submodule orig_fqn InterpreterModuleDispatcher attrs_map orig_fqn call_modules elide call indices call modules because they tracked automatically inside dispatcher module elide_call_indices prefix graph node graph nodes node op == call_module fqn = node target split path = f prefix fqn prefix fqn path called_modules node target = fqn fqn mod named_modules remove_duplicate=False hasattr mod graph elide_call_indices fqn mod graph hasattr mod _call_modules mod_ mod _call_modules assert hasattr mod_ graph elide_call_indices fqn mod_ graph print_readable print_output=True include_stride=False include_device=False colored=False _print_readable UnflattenedModule print_output include_stride include_device colored unflatten module ExportedProgram flat_args_adapter Optional FlatArgsAdapter = None - UnflattenedModule Unflatten ExportedProgram producing module same module hierarchy original eager module This can useful you trying use mod ` torch export ` another system expects module hierarchy instead flat graph mod ` torch export ` usually produces note The args kwargs unflattened modules will necessarily match eager module so doing module swap e g code ` submod = new_mod ` will necessarily work If you need swap module out you need set code ` preserve_module_call_signature ` parameter func ` torch export export ` Args module ExportedProgram The ExportedProgram unflatten flat_args_adapter Optional FlatArgsAdapter Adapt flat args input TreeSpec does match exported module s Returns An instance ` UnflattenedModule ` which has same module hierarchy original eager module pre-export module = _remove_effect_tokens module m = UnflattenedModule module flat_args_adapter Disable process_forward_inputs adapter has many non-dynamo-traceable behavior m process_forward_inputs = torch _dynamo disable type ignore method-assign m process_forward_inputs reason= do trace into preprocessing inputs recursive=True m _inplace_buffer_and_input_mutations graph torch fx Graph graph_signature ExportGraphSignature - None Transform buffer input mutations their functionalized form into copy_ nodes graph Functionalization represents buffer mutation passing buffer input output For example consider eager code forward x buffer += x x x This corresponds graph looks like forward buffer x mutated_buffer = aten add buffer x mul = aten mul x x mutated_buffer mul We want inplace into something looks like original eager code forward buffer x mutated_buffer = aten add buffer x buffer copy_ mutated_buffer mul = aten mul x x mul Input mutations handled similarly output_node = next iter reversed graph nodes assert output_node op == output len output_node args == return_args = output_node args input_name_to_node = node name node node graph nodes node op == placeholder mutation_name_to_input_name = Collect mutated buffers buffer_fqn_to_input_name = buffer_fqn k k buffer_fqn graph_signature inputs_to_buffers items mutation_name_to_input_name = k buffer_fqn_to_input_name buffer_fqn k buffer_fqn graph_signature buffers_to_mutate items Collect mutated user inputs mutation_name_to_input_name update graph_signature user_inputs_to_mutate num_mutations = len mutation_name_to_input_name mutation return_args num_mutations input_name = mutation_name_to_input_name mutation name input_node = input_name_to_node input_name graph inserting_after mutation Create copy_ node inplaces mutation new_node = graph create_node call_function torch ops aten copy_ default input_node mutation k v mutation meta items new_node meta k = v Replace all uses previously functional mutation our copy_ node mutation replace_all_uses_with new_node lambda x x new_node Remove mutated buffer input graph outputs since we don t need thread through anymore user_outputs = tuple return_args num_mutations output_node args = user_outputs _root_module_type graph torch fx Graph - Optional str node graph nodes nn_module_stack node meta continue path ty node meta nn_module_stack values path ty None _fix_nn_module_stacks graph For each nn module stack graph check fqns represent stack Each fqn must prefix next fqn If remove entries starting next fqn emitting warning node graph nodes nn_module_stack node meta continue nn_module_stack = node meta nn_module_stack fqns = fqn split fqn fqn fqn _t nn_module_stack values Check each FQN prefix next one prev_fqn next_fqns = fqns num_valid_indices = root FQN curr_fqn next_fqns Check previous FQN prefix current one _is_prefix prev_fqn curr_fqn num_valid_indices += prev_fqn = curr_fqn Found non-prefix FQN stop here break If we need remove entries create new stack only valid entries num_valid_indices len nn_module_stack log warning nn_module_stack fqns s node s do form stack dropping last d entries fqns node len nn_module_stack - num_valid_indices node meta nn_module_stack = dict list nn_module_stack items num_valid_indices _is_prefix candidate target Check whether ` candidate ` prefix ` target ` len candidate len target target len candidate == candidate _compute_accessor parent_fqn str child_fqn str - str parent_fqn == Handle root module correctly child_fqn parent_split = parent_fqn split child_split = child_fqn split TODO support skip connection inlining child module child_split len parent_split = parent_split raise RuntimeError f Child module child_fqn descendant parent module parent_fqn This currently unsupported Please try make child module attach parent module directly join child_split len parent_split _check_graph_equivalence x torch nn Module y torch nn Module graph_dump graph torch fx Graph - str ret = nodes_idx dict int int = arg_dump arg - str isinstance arg torch fx Node + str nodes_idx id arg str arg i node enumerate graph nodes args_dump = str arg arg pytree tree_map arg_dump node args args_dump += f key = value key value pytree tree_map arg_dump node kwargs items target = node target node op call_function get_attr pyrefly ignore bad-argument-type ret append f i node op target join args_dump nodes_idx id node = i \n join ret assert isinstance x graph torch fx Graph assert isinstance y graph torch fx Graph graph_dump x graph == graph_dump y graph _add_spec gm torch nn Module spec - str i = while hasattr gm f _spec_ i i += name = f _spec_ i setattr gm name spec name _generate_flatten gm torch fx GraphModule node - torch fx Node flatten = gm graph call_function pytree tree_flatten node getitem_ = gm graph call_function operator getitem flatten getitem_ _generate_flatten_spec gm Union torch fx GraphModule InterpreterModule UnflattenedModule node spec - torch fx Node name = _add_spec gm spec spec_node = gm graph get_attr name gm graph call_function fx_pytree tree_flatten_spec node spec_node _generate_unflatten gm Union torch fx GraphModule InterpreterModule UnflattenedModule nodes spec - torch fx Node name = _add_spec gm spec spec_node = gm graph get_attr name gm graph call_function pytree tree_unflatten nodes spec_node _get_submodule mod torch nn Module target str prefix field = target split item prefix submod = getattr mod item None submod None None isinstance submod torch nn Module None mod = submod getattr mod field None _add_submodule mod torch nn Module target str module_to_add torch nn Module create_module Optional Callable str torch nn Module = None prefix field = target split i item enumerate prefix submod = getattr mod item None submod None create_module None submod = create_module join prefix i + submod = torch nn Module setattr mod item submod isinstance submod torch nn Module False mod = submod mod add_module field module_to_add _call_name base str n int - str Given n = generate call names submodule ` base ` form ` base ` ` base ` ` base ` etc base n == f base n - _is_call_name call_name str base str - bool Recognize when call_name = _call_name base n some n = re match re escape base + r \d+ $ call_name None _ModuleFrame __init__ flat_graph torch fx Graph nodes tuple torch fx Node seen_nodes seen_modules seen_attrs created_modules parent module_stack list tuple str Optional str int module_id module_call_graph dict str ModuleCallSignature module Optional Union torch fx GraphModule UnflattenedModule = None flat_graph = flat_graph nodes = nodes seen_nodes = seen_nodes seen_modules = seen_modules seen_attrs = seen_attrs created_modules = created_modules parent = parent module_stack = module_stack module_id = module_id module_call_graph = module_call_graph verbose = False fqn ty num_calls = module_stack - generate call name fqn child_fqn = _call_name fqn num_calls + module Union torch fx GraphModule UnflattenedModule InterpreterModule module None module = module ivals = module ivals hasattr module ivals type ignore var-annotated module = created_modules get fqn InterpreterModule torch fx Graph ty=ty ivals = parent ivals graph = module graph Mapping nodes flat graph nodes graph node_map dict torch fx Node torch fx Node = node_to_placeholder = parent_call_module Optional torch fx Node = None parent None accessor = _compute_accessor parent fqn child_fqn create_module fqn path = f parent fqn fqn parent fqn fqn path created_modules created_modules path submod = InterpreterModule torch fx Graph ty=ty created_modules path = submod submod _add_submodule parent module accessor module create_module parent_call_module = parent graph call_module accessor seen_modules module_id base_module_frame = seen_modules module_id module _modules = base_module_frame module _modules seen_modules module_id append _SubmoduleEntry parent_fqn=self parent fqn parent_module=self parent module parent_call_module=self parent_call_module fqn=self fqn call_idx=num_calls + module=self module signature = module_call_graph get child_fqn signature None parent None assert signature in_spec num_children == assert signature in_spec type tuple args_spec kwargs_spec = signature in_spec children assert args_spec type tuple assert kwargs_spec type dict graph inserting_after None arg_nodes = graph placeholder f _positional_arg_ idx idx range args_spec num_children kwarg_nodes = name kwargs_spec context kwarg_nodes name = graph placeholder name flat_args = _generate_flatten_spec module tuple arg_nodes kwarg_nodes signature in_spec idx arg enumerate signature inputs flat_arg_node = graph create_node op= call_function target=operator getitem args= flat_args idx name= arg name isinstance arg ConstantArgument f _constant_ idx isinstance arg ConstantArgument continue arg name seen_nodes flat_arg_node meta = copy copy seen_nodes arg name meta node_to_placeholder seen_nodes arg name = flat_arg_node parent graph inserting_before parent_call_module input_nodes list Optional torch fx Node = input signature inputs isinstance input ConstantArgument input_nodes append input value type ignore arg-type input name seen_nodes input_nodes append None assert isinstance input TensorArgument SymIntArgument SymBoolArgument SymFloatArgument input_nodes append parent remap_input seen_nodes input name inputs_node = _generate_unflatten parent module input_nodes signature in_spec args_node = parent graph call_function operator getitem inputs_node kwargs_node = parent graph call_function operator getitem inputs_node arg_nodes = parent graph call_function operator getitem args_node i i range args_spec num_children kwarg_nodes = k parent graph call_function operator getitem kwargs_node k k kwargs_spec context assert parent_call_module None pyrefly ignore bad-assignment parent_call_module args = tuple arg_nodes parent_call_module kwargs = kwarg_nodes type ignore assignment add_placeholder x assert fqn = f Cannot add placeholder x root module assert x graph flat_graph x subgraph create new placeholder subgraph graph inserting_before None placeholder_node = graph placeholder x name type_expr=x type copy all meta fields even some fields might irrelevant placeholder node placeholder_node meta = copy copy x meta node_to_placeholder x = placeholder_node copy_sym_call_function x This only exists because we deduplicate sym_size nodes flat export graph preserve_module_call_signature set we may able pass sym_size nodes their downstream users inputs submodule calls To avoid we copy these call_function nodes sym_type results This should however only done sym_type nodes - call_function nodes tensors should deduplicated first place args = pytree tree_map_only torch fx Node remap_input x args kwargs = pytree tree_map_only torch fx Node remap_input x kwargs node = graph call_function x target args kwargs node meta = copy copy x meta node_map x = node node remap_input x assert x graph flat_graph x node_map node_map x print f remap_input x x node_to_placeholder node_to_placeholder x x op == placeholder module_call_graph get fqn None allow placeholder creation we preserving module call signature add_placeholder x parent_call_module None Important prepend output match how we inserting placeholder nodes parent graph inserting_before parent_call_module parent_call_module insert_arg parent remap_input x node_to_placeholder x x op == call_function x target torch ops aten sym_size int torch ops aten item default torch ops aten unbind int torch ops aten sum dim_IntList torch ops aten view default torch ops aten diff default hasattr x target __module__ x target __module__ == _operator export deduplicates sym_size nodes may need re-copy them module call signature needs preserved copy_sym_call_function x node_map x module_call_graph get fqn None x reading intermediate value mutation so record later we will find where created perform update ivals read x type ignore operator union-attr raise RuntimeError f Could run remap_input op type x op node x finalize_outputs created_modules pop fqn None orig_outputs = signature = module_call_graph get child_fqn signature None parent None output signature outputs isinstance output TensorArgument SymIntArgument SymBoolArgument SymFloatArgument ConstantArgument output name seen_nodes orig_outputs append seen_nodes output name orig_outputs append None raise RuntimeError f Unsupported data type output node output get_actual_output_node output output None None seen_node = seen_nodes output name seen_node node_map node_map seen_node seen_node node_to_placeholder node_to_placeholder seen_node raise RuntimeError f Could find output node output Graph graph tree_out_node = _generate_unflatten module tuple get_actual_output_node output output orig_outputs signature out_spec parent_out Optional torch fx Node = _generate_flatten_spec parent module parent_call_module signature out_spec graph_outputs Union torch fx Node list torch fx Node = tree_out_node graph_outputs = Iterate through nodes we have copied into graph orig_node node_map keys user_node orig_node users user_node name seen_nodes external user node need expose output orig_outputs append orig_node graph_outputs append node_map orig_node break parent_out = parent_call_module len graph_outputs == graph_outputs = graph_outputs assert isinstance graph_outputs list torch fx Node graph output graph_outputs Rewrite outputs parent module parent_out None parent_out meta val = graph_outputs meta get val isinstance graph_outputs torch fx Node o meta get val o graph_outputs len orig_outputs == signature None parent node_map orig_outputs = parent_out i orig_output enumerate orig_outputs orig_output None continue Use Proxy record getitem access proxy_out = torch fx Proxy parent_out i node type ignore index proxy_out meta val = orig_output meta get val parent node_map orig_output = proxy_out copy_node node print copying node format_node node_map node = graph node_copy node remap_input seen_nodes node name = node run_outer i node enumerate flat_graph nodes print i node meta get nn_module_stack node format_node Copy all graph inputs node_idx int = node = nodes node_idx while node op == placeholder copy_node node node_idx += node = nodes node_idx run_from node_idx Copy graph outputs node flat_graph nodes node op == output copy_node node print args kwargs verbose pyrefly ignore not-iterable print args kwargs run_from node_idx module_idx = Walk through graph building up new graph right submodules while node_idx len nodes node = nodes node_idx assert node op = placeholder print print STEP node_idx node format_node print module_stack depth = len module_stack node op == output depth == We want output node original graph handled specially outermost stack frame run_outer So skip finalization here node_idx We ve reached end graph Wrap up all existing stack frames finalize_outputs node_idx len node meta get nn_module_stack == raise RuntimeError f Unable find nn_module_stack node node nn_module_stack = node meta nn_module_stack torch _export passes _node_metadata_hook _EMPTY_NN_MODULE_STACK_KEY len nn_module_stack == _EMPTY_NN_MODULE_STACK_KEY nn_module_stack Empty case node_metadata_hook node_module_stack = module_stack node_module_stack = path ty path None int k split - k k path ty node meta nn_module_stack items node_module_stack depth = module_stack This means current module done executing current node beginning new module In case we should finalize module without incrementing node counter finalize_outputs print outlining fqn print graph node_idx assert node_module_stack None _is_prefix module_stack node_module_stack This means current node represents execution new module next_module = node_module_stack depth print Creating new stack frame next_module Run nested version module outliner current node counter Once complete continue point next_module_key = list node meta nn_module_stack keys depth node_idx = _ModuleFrame flat_graph nodes seen_nodes seen_modules seen_attrs created_modules module_stack + next_module next_module_key split module_call_graph run_from node_idx module_idx += continue The only remaining possibility we right stack frame Copy node into frame s graph increment node counter assert node_module_stack == module_stack node op == get_attr must graph argument HOP seen_attrs child_fqn add node target copy_node node pyrefly ignore unsupported-operation node_idx += dataclass _SubmoduleEntry parent_fqn str parent_module torch nn Module parent_call_module torch fx Node fqn str call_idx int module torch nn Module _outline_submodules orig_graph torch fx Graph root_module UnflattenedModule seen_nodes dict str torch fx Node = seen_modules dict int list _SubmoduleEntry = defaultdict list seen_attrs dict str set str = defaultdict set created_modules dict str torch nn Module = _ModuleFrame orig_graph tuple orig_graph nodes seen_nodes seen_modules seen_attrs created_modules None None entry fqn entry signature entry root_module module_call_graph entry signature module=root_module run_outer seen_modules seen_attrs _reorder_submodules parent torch nn Module fqn_order dict str int prefix str = TODO Can optimized adding submodules ahead time prefix == fqn list fqn_order keys _get_submodule parent fqn None _add_submodule parent fqn torch nn Module children = name child list parent _modules items child None continue fqn = prefix + name _reorder_submodules child fqn_order prefix=fqn split + delattr parent name children append fqn_order fqn name child children sort key=operator itemgetter _ name child children parent register_module name child _IVals Collect intermediate values mutations graph Example following graph suppose buf_in buf_out input output values buffer buf_in = placeholder ival = f buf_in inside n ival = f ival inside n buf_out = f ival inside n buf_out Here ival ival intermediate values created inside calls n n respectively used inside calls n n respectively __init__ each fqn set node names corresponding intermediate values node_names_by_fqn = defaultdict set _is_mutable target isinstance target torch _ops OpOverload target _schema is_mutable False read mf node Read state corresponding given intermediate value we can assume node must mutation assert node op == call_function b = _is_mutable node target print Checking mutability node target b b so mutation functionalized we will apply original mutation later see below fqn _ = next reversed node meta nn_module_stack values node_names_by_fqn fqn add node name mf remap_input node args update partitions Update states corresponding intermediate values read shared_submodules partitions entry shared_submodules graph = entry module graph node_names = node_names_by_fqn entry fqn nodes = n n graph nodes n name node_names node nodes so node must functionalized mutation we perform original mutation now graph inserting_after node new_node = graph create_node call_function torch ops aten copy_ default node args node new_node meta = copy copy node meta _copy_graph_attrs gm torch fx GraphModule root_module UnflattenedModule seen_attrs dict str set str child_fqn names seen_attrs items module = _get_attr root_module child_fqn child_fqn root_module name names val = getattr gm name setattr module name val _deduplicate_modules partitions redirected_call_indices = shared_submodules partitions i entry enumerate shared_submodules child_fqn = _call_name entry fqn entry call_idx target = _compute_accessor entry parent_fqn child_fqn deduplicated = False Iterate over all previously seen modules deduplicate possible seen shared_submodules i _check_graph_equivalence seen module entry module parent = entry parent_module Since graphs equivalent we can deduplicate There two cases seen fqn == entry fqn Case The current module has same fqn seen module In case we have generated call name can optimized away So we remove current module hierarchy replace current call name seen call name parent graph prefix name = target split _get_attr_via_attr_list parent prefix _modules pop name seen_child_fqn = _call_name seen fqn seen call_idx seen_target = _compute_accessor entry parent_fqn seen_child_fqn entry parent_call_module target = seen_target redirected_call_indices child_fqn = seen_child_fqn break deduplicated Case The current module has different fqn than seen module In case we replace current module seen module There should nothing pointing current module any more so can garbage collected NOTE We do replace current call name seen call name parent graph because will lose information which fqn actually called However possible current call name will optimized away when we find another seen module same fqn so we do break out loop yet parent set_submodule target seen module deduplicated = True redirected_call_indices _sink_params module torch nn Module inputs_to_state dict str list str scope list str module_id_to_inputs_removed Optional dict int set str = None Sink params buffers constants graph inputs into get_attr nodes Exported modules purely functional so they pass their parameters buffers inputs graph To replicate eager s semantics we need get them module state via get_attr instead module GraphModule potentially containing nested submodules inputs_to_state mapping graph input names corresponding key state_dict scope tracks where we module hierarchy so we can emit right ` getattr foo bar ` calls etc module_id_to_inputs_removed records inputs removed child modules mapping module object id list placeholder node names child module removed module_id_to_inputs_removed None module_id_to_inputs_removed = defaultdict set id module module_id_to_inputs_removed id module module_id_to_inputs_removed id module We need use _modules here instead named_children because we explicitly want duplicate modules show up traversal name submodule module _modules items submod_id_to_inputs_removed = _sink_params cast torch nn Module submodule inputs_to_state scope + name module_id_to_inputs_removed k v submod_id_to_inputs_removed items module_id_to_inputs_removed k update v graph = getattr module graph None graph None len graph nodes == Not all modules have graphs defined they empty modules no operations like ParameterList module_id_to_inputs_removed assert isinstance graph torch fx Graph inputs = list filter lambda n n op == placeholder graph nodes the_last_input = None len inputs == inputs - Also remove call_module nodes call_module_nodes = filter lambda n n op == call_module graph nodes node call_module_nodes submodule = _get_attr module node target remove placeholder call_module node arguments only we ve erased placeholder node corresponding _sink_params call submodule None id submodule module_id_to_inputs_removed node args = tuple filter lambda n n name module_id_to_inputs_removed id submodule node args Filter out inputs_to_state corresponding current scope inputs_to_state_of_scope dict torch fx Node list str = node inputs node name inputs_to_state continue state_name = None sn inputs_to_state node name sn_split = sn split sn_split len scope == x split x scope state_name = sn_split break If there s mismatch between scope name state name then there must multiple scopes pointing same state name meaning some modules shared In such case we can simply skip updating current node because another later iteration will take care input node when unique match between scope state name occurs To make sure always happen we should enforce invariant no placeholder node unflattened graph appears inputs_to_state dict which means all extra input nodes have been handled state_name None continue inputs_to_state_of_scope node = state_name Record name remove inputs purpose inputs_removed set str = set node state_name inputs_to_state_of_scope items len node users attr_path = state_name len scope state_attr = _get_attr_via_attr_list module attr_path assert isinstance state_attr torch Tensor torch ScriptObject Make sure newly created get_attr node placed after last placeholder node graph inserting_after the_last_input new_node = graph create_node get_attr join attr_path node replace_all_uses_with new_node propagate_meta=True graph erase_node node inputs_removed add node name isinstance module InterpreterModule module finalize id module inputs_removed