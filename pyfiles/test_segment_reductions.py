Owner s module scatter gather ops itertools product functools partial numpy np torch torch testing _internal common_device_type instantiate_device_type_tests dtypes torch testing _internal common_utils TestCase run_tests gradcheck parametrize reductions = max mean min sum prod get_default_value initial_value reduction initial_value None initial_value reduction == max -float Inf reduction == mean float nan reduction == min float Inf reduction == sum reduction == prod TestSegmentReductions TestCase _test_common reduction device dtype unsafe axis initial_value data_arr lengths_arr expected_arr expected_grad_arr check_backward lengths_dtype=torch int lengths = torch tensor lengths_arr device=device dtype=lengths_dtype generate offsets lengths zeros_shape = list lengths shape zeros_shape - = offsets = torch cat lengths new_zeros zeros_shape lengths - cumsum_ - data = torch tensor data_arr device=device dtype=dtype requires_grad=True expected_result = torch tensor expected_arr device=device dtype=dtype expected_grad = torch tensor expected_grad_arr device=device dtype=dtype mode lengths offsets segment_reduce_kwargs = dict axis=axis unsafe=unsafe initial=initial_value mode == lengths segment_reduce_kwargs lengths = lengths segment_reduce_kwargs offsets = offsets actual_result = torch _segment_reduce data=data reduce=reduction segment_reduce_kwargs assertEqual expected_result actual_result rtol= e- atol= e- equal_nan=True check_backward Test backward actual_result sum backward assertEqual expected_grad data grad rtol= e- atol= e- equal_nan=True data = data detach clone requires_grad_ True gradcheck does work well bfloat fp cpu types also there small numerical difference fp dtype torch half torch bfloat torch float gradcheck does like nan input setting random d_non_nan = np nan_to_num data_arr nan= new_data = torch tensor v == float nan v v data d_non_nan device=device dtype=dtype requires_grad=True assertTrue gradcheck lambda x torch _segment_reduce data=x reduce=reduction segment_reduce_kwargs new_data dtypes product torch half torch bfloat torch float torch double torch int torch int test_simple_ d device dtypes val_dtype length_type = dtypes lengths = data = float nan reduction reductions initial None check_backward = initial None initial_value = initial default_value = get_default_value initial_value reduction reduction == max expected_result = float nan default_value expected_grad = reduction == mean expected_result = float nan default_value expected_grad = reduction == min initial None initial_value = some high number default_value = get_default_value initial_value reduction expected_result = float nan default_value expected_grad = reduction == sum expected_result = float nan default_value expected_grad = reduction == prod initial None initial_value = initial_value will zero out everything prod default_value = get_default_value initial_value reduction expected_result = float nan default_value expected_grad = float nan expected_result = float nan default_value expected_grad = float nan axis - unsafe True False _test_common reduction device val_dtype unsafe axis initial_value data lengths expected_result expected_grad check_backward length_type dtypes product torch half torch bfloat torch float torch double torch int torch int test_simple_zero_length device dtypes val_dtype length_type = dtypes lengths = data = torch ones reduction reductions initial None check_backward = initial None initial_value = initial default_value = get_default_value initial_value reduction reduction == max expected_result = default_value default_value expected_grad = reduction == mean expected_result = default_value default_value expected_grad = reduction == min initial None initial_value = some high number default_value = get_default_value initial_value reduction expected_result = default_value default_value expected_grad = reduction == sum expected_result = default_value default_value expected_grad = reduction == prod initial None initial_value = initial_value will zero out everything prod default_value = get_default_value initial_value reduction expected_result = default_value default_value expected_grad = expected_result = default_value default_value expected_grad = axis unsafe True False _test_common reduction device val_dtype unsafe axis initial_value data lengths expected_result expected_grad check_backward length_type dtypes product torch half torch bfloat torch float torch double torch int torch int test_multi_d_simple device dtypes val_dtype _ = dtypes axis = lengths = data = float nan float nan reduction reductions initial None check_backward = initial None initial_value = initial default_value = get_default_value initial_value reduction reduction == max expected_result = float nan float nan default_value default_value expected_grad = reduction == mean expected_result = float nan float nan default_value default_value expected_grad = reduction == min initial None initial_value = some high number default_value = get_default_value initial_value reduction expected_result = float nan float nan default_value default_value expected_grad = reduction == sum expected_result = float nan float nan default_value default_value expected_grad = reduction == prod initial None initial_value = initial_value will zero out everything prod default_value = get_default_value initial_value reduction expected_result = float nan float nan default_value default_value expected_grad = float nan float nan expected_result = float nan float nan default_value default_value expected_grad = float nan float nan unsafe True False _test_common reduction device val_dtype unsafe axis initial_value data lengths expected_result expected_grad check_backward dtypes product torch half torch bfloat torch float torch double torch int torch int parametrize reduce sum prod min max mean test_pytorch_scatter_test_cases device dtypes reduce val_dtype length_dtype = dtypes zero-length segments filled reduction inits contrary pytorch_scatter tests = src index indptr sum prod mean float nan min float inf max -float inf src index indptr sum prod mean float nan float nan min float inf float inf max -float inf -float inf src index indptr sum prod mean float nan float nan min float inf float inf max -float inf -float inf src index indptr sum prod mean float nan float nan float nan float nan min float inf float inf float inf float inf max -float inf -float inf -float inf -float inf src index indptr sum prod mean min max src index indptr sum prod mean min max test tests data = torch tensor test src dtype=val_dtype device=device requires_grad=True indptr = torch tensor test indptr dtype=length_dtype device=device dim = indptr ndim - calculate lengths indptr lengths = torch diff indptr dim=dim expected = torch tensor test reduce dtype=val_dtype device=device actual_result = torch _segment_reduce data=data reduce=reduce lengths=lengths axis=dim unsafe=True assertEqual actual_result expected test offsets actual_result = torch _segment_reduce data=data reduce=reduce offsets=indptr axis=dim unsafe=True assertEqual actual_result expected val_dtype == torch float fn x mode= lengths initial = supply initial values prevent gradcheck failing length segments where nan inf reduction identities produce nans when calculating numerical jacobian reduce == min initial = reduce == max initial = - segment_reduce_args = x reduce segment_reduce_kwargs = dict axis=dim unsafe=True initial=initial mode == lengths segment_reduce_kwargs mode = lengths mode == offsets segment_reduce_kwargs mode = indptr torch _segment_reduce segment_reduce_args segment_reduce_kwargs assertTrue gradcheck partial fn mode= lengths data detach clone requires_grad_ True assertTrue gradcheck partial fn mode= offsets data detach clone requires_grad_ True dtypes product torch half torch bfloat torch float torch double torch int torch int test_multi_d device dtypes val_dtype _ = dtypes axis = lengths = data = np arange reshape tolist expected_grad = TODO calculate grad check correctness check_backward = False reduction reductions initial_value = reduction == max expected_result = np full initial_value tolist np max data axis= tolist np max data axis= tolist np full initial_value tolist reduction == mean expected_result = np full initial_value tolist np mean data axis= tolist np mean data axis= tolist np full initial_value tolist reduction == min initial_value = some high number expected_result = np full initial_value tolist np min data axis= tolist np min data axis= tolist np full initial_value tolist reduction == sum expected_result = np full initial_value tolist np sum data axis= tolist np sum data axis= tolist np full initial_value tolist reduction == prod initial_value = expected_result = np full initial_value tolist np prod data axis= tolist np prod data axis= tolist np full initial_value tolist unsafe True False _test_common reduction device val_dtype unsafe axis initial_value data lengths expected_result expected_grad check_backward dtypes torch int torch int test_unsafe_flag device dtype length_type = dtype lengths = torch tensor device=device dtype=length_type data = torch arange dtype=torch float device=device test error -D lengths assertRaisesRegex RuntimeError Expected all rows lengths along axis torch _segment_reduce data sum lengths=lengths axis= unsafe=False test error multi-D lengths nd_lengths = torch tensor dtype=length_type device=device nd_data = torch arange dtype=torch float device=device reshape assertRaisesRegex RuntimeError Expected all rows lengths along axis torch _segment_reduce nd_data sum lengths=nd_lengths axis= unsafe=False instantiate_device_type_tests TestSegmentReductions globals __name__ == __main__ run_tests