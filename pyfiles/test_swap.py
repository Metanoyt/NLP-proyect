Owner s oncall export flake noqa unittest dataclasses dataclass typing Any List parameterized parameterized_class torch torch _dynamo torchdynamo torch Tensor torch _export config torch _export utils register_dataclass_as_pytree_node torch export export register_dataclass torch export _swap _swap_modules torch testing _internal common_utils IS_WINDOWS run_tests TestCase unittest skipIf IS_WINDOWS Windows supported test unittest skipIf torchdynamo is_dynamo_supported dynamo isn t support parameterized_class strict False strict True class_name_func=lambda cls _ params f cls __name__ _ strict params strict nonstrict TestSwap TestCase test_unflatten_preserve_signature NestedChild torch nn Module forward zx y x y key + zx w y key zx Child torch nn Module __init__ - None super __init__ nested = NestedChild forward x y z = torch ones_like x xw = nested z x y= key y xw w + z - xw x Child torch nn Module __init__ - None super __init__ forward x x - MyModule torch nn Module __init__ - None super __init__ foo = Child bar = Child forward x y x = foo x y x = bar x x orig_eager = MyModule inps = torch rand torch rand ep = export orig_eager inps preserve_module_call_signature= foo nested bar strict=self strict swapped_gm = _swap_modules ep foo nested NestedChild bar Child assertTrue torch allclose ep module inps swapped_gm inps test_unflatten_preserve_with_unused_input M torch nn Module forward x b x + b M torch nn Module __init__ - None super __init__ m = M forward x y b = torch topk y m x b ep = torch export export M torch randn torch randn preserve_module_call_signature= m strict=self strict swapped_gm = _swap_modules ep m M inps = torch randn torch randn assertTrue torch allclose ep module inps swapped_gm inps test_nested_leaf Leaf torch nn Module forward x x + Nested torch nn Module __init__ - None super __init__ leaf = Leaf forward x leaf x + TopLevel torch nn Module __init__ - None super __init__ nested = Nested forward x nested x + ep = torch export export TopLevel torch randn strict=self strict preserve_module_call_signature= nested swapped_gm = _swap_modules ep nested Nested inps = torch randn assertTrue torch allclose ep module inps swapped_gm inps test_dedup_sym_size Here sym_size floor div used subgraphs top-level m m only one copy sym_size created initial export graph For m sym_size floordiv should copied recompute since we preserve call signature m floordiv should passed placeholder Test preserved unflattened module runs correctly M torch nn Module forward x y d = x size y d M torch nn Module forward x y d = x size y d M torch nn Module __init__ - None super __init__ m = M m = M forward x y d = x size m _res = m x y m _res = m x y y d + m _res + m _res inputs = torch ones torch ones d_ = torch export Dim foo max= d = d_ ep = torch export export M inputs dynamic_shapes= d d strict=self strict preserve_module_call_signature= m swapped_gm = _swap_modules ep m M inps = torch randn torch randn assertTrue torch allclose ep module inps swapped_gm inps inps = torch randn torch randn assertTrue torch allclose ep module inps swapped_gm inps test_remove_duplicate_pytree_simple Child torch nn Module __init__ - None super __init__ forward x y z = torch ones_like x w = y + z x = y z res x + y res x y Child torch nn Module __init__ - None super __init__ forward x x res + x res - MyModule torch nn Module __init__ - None super __init__ foo = Child bar = Child forward x y x = foo x y x = bar x x orig_eager = MyModule inps = torch rand torch rand ep = export orig_eager inps preserve_module_call_signature= foo bar strict=self strict swapped_gm = _swap_modules ep foo Child bar Child assertTrue torch allclose ep module inps swapped_gm inps assertExpectedInline swapped_gm code strip \ forward x y x_ = x y_ = y _spec_ = _spec_ _spec_ = _spec_ _spec_ = _spec_ tree_flatten = torch utils _pytree tree_flatten x_ y_ x_ = y_ = None getitem = tree_flatten tree_flatten = None x = getitem y = getitem getitem = None tree_unflatten_ = torch utils _pytree tree_unflatten x y _spec_ x = y = _spec_ = None getitem_ = tree_unflatten_ tree_unflatten_ = None getitem_ = getitem_ getitem_ = getitem_ getitem_ = None foo = foo getitem_ getitem_ getitem_ = getitem_ = None bar = bar foo foo = None tree_flatten_spec_ = torch fx _pytree tree_flatten_spec bar _spec_ bar = _spec_ = None getitem_ = tree_flatten_spec_ tree_flatten_spec_ = None tree_unflatten = torch utils _pytree tree_unflatten getitem_ _spec_ getitem_ = _spec_ = None tree_unflatten unittest expectedFailure test_remove_duplicate_pytree_different_order This supported yet because module ` foo ` s outputs all directly used inputs ` bar ` same order outputted ` foo ` To support we would have do some sort ordering Child torch nn Module __init__ - None super __init__ forward x y res x + y res x y res x x Child torch nn Module __init__ - None super __init__ forward y x y = y res y res x = x res + x res y - x MyModule torch nn Module __init__ - None super __init__ foo = Child bar = Child forward x y x y = foo x y x = bar y x x orig_eager = MyModule inps = torch rand torch rand ep = export orig_eager inps preserve_module_call_signature= foo bar strict=self strict swapped_gm = _swap_modules ep foo Child bar Child assertTrue torch allclose ep module inps swapped_gm inps assertExpectedInline swapped_gm code strip \ forward x y x y = fx_pytree tree_flatten_spec x y _in_spec _spec_ = _spec_ _spec_ = _spec_ tree_unflatten = torch utils _pytree tree_unflatten x y _spec_ x = y = _spec_ = None getitem = tree_unflatten tree_unflatten = None getitem_ = getitem getitem_ = getitem getitem = None foo = foo getitem_ getitem_ getitem_ = getitem_ = None getitem_ = foo getitem_ = foo bar = bar getitem_ getitem_ foo = None tree_flatten_spec_ = torch fx _pytree tree_flatten_spec bar _spec_ bar = _spec_ = None getitem_ = tree_flatten_spec_ tree_flatten_spec_ = None pytree tree_unflatten getitem_ _out_spec test_custom_input_args dataclass CustomInput Tensor b Tensor register_dataclass_as_pytree_node CustomInput serialized_type_name= test_swap test_custom_input CustomInput Foo torch nn Module forward inputs torch matmul inputs inputs b ep = export Foo CustomInput torch randn torch randn strict=self strict swapped = _swap_modules ep inp = CustomInput torch randn torch randn res = torch fx Interpreter swapped run inp res = swapped inp assertTrue torch allclose res res test_custom_input_kwargs dataclass CustomInput Tensor b Tensor register_dataclass CustomInput serialized_type_name= test_swap test_custom_input CustomInput Foo torch nn Module forward x inputs x + torch matmul inputs inputs b use_new_tracer True False config patch use_new_tracer_experimental=use_new_tracer ep = export Foo torch randn inputs CustomInput torch randn torch randn strict=self strict swapped = _swap_modules ep inp_args = torch randn inp_kwargs = inputs CustomInput torch randn torch randn res = torch fx Interpreter swapped run inp_args inp_kwargs values res = swapped inp_args inp_kwargs assertTrue torch allclose res res test_custom_input_kwargs_use_private dataclass CustomInput Tensor b Tensor register_dataclass_as_pytree_node CustomInput serialized_type_name= test_swap test_custom_input CustomInput Foo torch nn Module forward x inputs x + torch matmul inputs inputs b shouldn t error config patch use_new_tracer_experimental=True _ = export Foo torch randn inputs CustomInput torch randn torch randn strict=self strict test_custom_output dataclass CustomOutput Tensor b Tensor register_dataclass_as_pytree_node CustomOutput serialized_type_name= test_swap test_custom_input CustomInput Foo torch nn Module forward b CustomOutput b b CustomOutput b T + b T ep = export Foo torch randn torch randn strict=True swapped = _swap_modules ep inp = torch randn torch randn res = torch fx Interpreter swapped run inp res = swapped inp assertTrue torch allclose res res assertTrue torch allclose res b res b assertTrue torch allclose res res assertTrue torch allclose res b res b __name__ == __main__ run_tests