mypy ignore-errors faulthandler functools itertools logging multiprocessing operator os queue subprocess sys tempfile threading time traceback types unittest collections abc Callable contextlib contextmanager dataclasses dataclass datetime timedelta enum Enum functools partial reduce wraps io StringIO typing Any NamedTuple Optional Union unittest mock patch torch torch _dynamo test_case torch cuda nccl torch distributed c d torch nn nn torch _C _autograd DeviceType torch _C _distributed_c d _SymmetricMemory torch _logging _internal trace_log torch testing _internal common_utils torch testing _internal common_utils FILE_SCHEMA find_free_port IS_SANDCASTLE LazyVal retry_on_connect_failures skip_but_pass_in_sandcastle skip_but_pass_in_sandcastle_if TEST_CUDA TEST_HPU TEST_WITH_ROCM TEST_WITH_TSAN TEST_XPU TestCase torch testing _internal distributed multi_threaded_pg _install_threaded_pg _uninstall_threaded_pg ProcessLocalGroup logger = logging getLogger __name__ logger setLevel logging INFO ACCELERATOR_DIST_BACKENDS = nccl xccl hccl DDP_RANK_DEVICES = cuda xpu HAS_ACCELERATOR = TEST_CUDA TEST_HPU TEST_XPU TestSkip NamedTuple exit_code int message str TEST_SKIPS = backend_unavailable TestSkip Skipped because distributed backend available small_worldsize TestSkip Skipped due small world size odd_worldsize TestSkip Skipped due odd world size no_cuda TestSkip CUDA available multi-gpu- TestSkip Need least CUDA device multi-gpu- TestSkip Need least CUDA devices multi-gpu- TestSkip Need least CUDA devices multi-gpu- TestSkip Need least CUDA devices multi-gpu- TestSkip Need least CUDA devices multi-gpu- TestSkip Need least CUDA devices multi-gpu- TestSkip Need least CUDA devices multi-gpu- TestSkip Need least CUDA devices nccl TestSkip c d compiled NCCL support skipIfRocm TestSkip Test skipped ROCm no_peer_access TestSkip Test skipped because no GPU peer access generic TestSkip Test skipped subprocess level look subprocess log skip reason importerror TestSkip Test skipped due missing no_accelerator TestSkip accelerator available dataclass DistTestCases Backends do support specific collective skip_collective = skip_collective allgather_coalesced = nccl mpi ucc xccl skip_collective reduce = set skip_collective sendrecv anysource = nccl ucc xccl skip_collective cpu barrier = nccl ucc xccl Sets showing something implemented backend_feature = backend_feature gpu = nccl gloo ucc backend_feature cuda = nccl gloo ucc backend_feature ddp = nccl gloo ucc backend_feature subgroup = nccl gloo ucc backend_feature plugin = set TEST_HPU backend_feature hpu = hccl TEST_XPU backend_feature xpu = xccl requires_ddp_rank device device DDP_RANK_DEVICES skip_if_no_gpu func Skips world size exceeds number GPUs ensuring test run each rank has its own GPU via ` ` torch cuda device rank ` ` wraps func wrapper args kwargs TEST_CUDA TEST_HPU TEST_XPU sys exit TEST_SKIPS no_cuda exit_code world_size = int os environ WORLD_SIZE TEST_CUDA torch cuda device_count world_size sys exit TEST_SKIPS f multi-gpu- world_size exit_code TEST_HPU torch hpu device_count world_size sys exit TEST_SKIPS f multi-gpu- world_size exit_code TEST_XPU torch xpu device_count world_size sys exit TEST_SKIPS f multi-gpu- world_size exit_code func args kwargs wrapper TODO kwen what purpose decorator Tests decorator always skipped So they may outdated already Oct bumping small-world criteria we increasing number GPUs CI we need continue skipping those tests keep CI green But just temporary solution We should clean up those tests somehow skip_if_small_worldsize func wraps func wrapper args kwargs os environ BACKEND = mpi int os environ WORLD_SIZE sys exit TEST_SKIPS small_worldsize exit_code func args kwargs wrapper skip_if_odd_worldsize func wraps func wrapper args kwargs os environ BACKEND = mpi int os environ WORLD_SIZE == sys exit TEST_SKIPS odd_worldsize exit_code func args kwargs wrapper require_n_gpus_for_nccl_backend n backend decorator func wraps func wrapper args kwargs backend == nccl torch cuda device_count n sys exit TEST_SKIPS f multi-gpu- n exit_code func args kwargs wrapper decorator import_transformers_or_skip decorator func wraps func wrapper args kwargs try transformers AutoModelForMaskedLM BertConfig noqa F func args kwargs except ImportError sys exit TEST_SKIPS importerror exit_code wrapper decorator at_least_x_gpu x TEST_CUDA torch cuda device_count = x True TEST_HPU torch hpu device_count = x True TEST_XPU torch xpu device_count = x True False _maybe_handle_skip_if_lt_x_gpu args msg - bool _handle_test_skip = getattr args _handle_test_skip None len args == _handle_test_skip None False _handle_test_skip msg True skip_if_lt_x_gpu x decorator func wraps func wrapper args kwargs torch cuda is_available torch cuda device_count = x func args kwargs TEST_HPU torch hpu device_count = x func args kwargs TEST_XPU torch xpu device_count = x func args kwargs test_skip = TEST_SKIPS f multi-gpu- x _maybe_handle_skip_if_lt_x_gpu args test_skip message sys exit test_skip exit_code wrapper decorator This decorator helps avoiding initializing cuda while testing other backends nccl_skip_if_lt_x_gpu backend x decorator func wraps func wrapper args kwargs backend = nccl func args kwargs torch cuda is_available torch cuda device_count = x func args kwargs test_skip = TEST_SKIPS f multi-gpu- x _maybe_handle_skip_if_lt_x_gpu args test_skip message sys exit test_skip exit_code wrapper decorator verify_ddp_error_logged model_DDP err_substr Verify error logged ddp_logging_data ddp_logging_data = model_DDP _get_ddp_logging_data assert iteration ddp_logging_data assert has_error ddp_logging_data assert error ddp_logging_data logging_err = ddp_logging_data error Remove C++ stacktrace needed actual = err_substr err_substr find \nException raised == - err_substr split \nException raised assert actual logging_err f Did find expected actual ddp logging data error logging_err with_nccl_blocking_wait func Convenience decorator set unset TORCH_NCCL_BLOCKING_WAIT flag Note use decorator will override setting TORCH_NCCL_ASYNC_ERROR_HANDLING particular test After test both TORCH_NCCL_BLOCKING_WAIT TORCH_NCCL_ASYNC_ERROR_HANDLING will restored their original values wraps func wrapper args kwargs Save unset TORCH_NCCL_ASYNC_ERROR_HANDLING try cached_nccl_async_error_handling Union str None = os environ TORCH_NCCL_ASYNC_ERROR_HANDLING del os environ TORCH_NCCL_ASYNC_ERROR_HANDLING except KeyError TORCH_NCCL_ASYNC_ERROR_HANDLING unset cached_nccl_async_error_handling = None Save val TORCH_NCCL_BLOCKING_WAIT set try cached_nccl_blocking_wait Union str None = os environ TORCH_NCCL_BLOCKING_WAIT except KeyError cached_nccl_blocking_wait = None finally os environ TORCH_NCCL_BLOCKING_WAIT = try ret = func args kwargs ret finally restore old values cached_nccl_async_error_handling None os environ TORCH_NCCL_ASYNC_ERROR_HANDLING = cached_nccl_async_error_handling cached_nccl_blocking_wait None os environ TORCH_NCCL_BLOCKING_WAIT = cached_nccl_blocking_wait wrapper with_dist_debug_levels levels Runs test each distributed debug level specified levels decorator func wraps func wrapper args kwargs old_level = os environ get TORCH_DISTRIBUTED_DEBUG None level levels os environ TORCH_DISTRIBUTED_DEBUG = level c d set_debug_level_from_env ret = func args kwargs c d barrier old_level None os environ TORCH_DISTRIBUTED_DEBUG = old_level Only returns test last test since these unittests value really used earlier tests would ve raised had they failed ret wrapper decorator requires_gloo skip_but_pass_in_sandcastle_if c d is_gloo_available c d compiled Gloo backend requires_nccl_version version msg TEST_CUDA lambda f f c d is_nccl_available skip_but_pass_in_sandcastle c d compiled NCCL backend skip_but_pass_in_sandcastle_if torch cuda nccl version version f Requires NCCL version greater than equal version found torch cuda nccl version reason msg requires_nccl skip_but_pass_in_sandcastle_if c d is_nccl_available c d compiled NCCL backend requires_ucc skip_but_pass_in_sandcastle_if c d is_ucc_available c d compiled UCC backend requires_mpi skip_but_pass_in_sandcastle_if c d is_mpi_available c d compiled MPI backend requires_accelerator_dist_backend backends=None Decorator skip tests no accelerator communication backend NCCL XCCL HCCL available Args backends Optional List str Specific accelerator backends check e g nccl xccl hccl If None checks all supported accelerator backends NCCL XCCL HCCL Returns callable A decorator skips test no specified accelerator backend available backends None backends = ACCELERATOR_DIST_BACKENDS backend_available = any nccl c d is_nccl_available xccl c d is_xccl_available hccl lambda TEST_HPU get backend lambda False backend backends skip_but_pass_in_sandcastle_if backend_available f No accelerator communication backend available among backends requires_multicast_support has_multicast_support = torch cuda is_available _SymmetricMemory has_multicast_support DeviceType CUDA skip_but_pass_in_sandcastle_if has_multicast_support multicast support available evaluate_platform_supports_symm_mem TEST_CUDA TEST_WITH_ROCM arch_list = gfx gfx arch arch_list arch torch cuda get_device_properties gcnArchName True False True False PLATFORM_SUPPORTS_SYMM_MEM bool = LazyVal lambda evaluate_platform_supports_symm_mem skip_if_rocm_multiprocess func Skips test ROCm multiprocess UTs unittest skipIf TEST_WITH_ROCM TEST_SKIPS skipIfRocm message func skip_if_rocm_arch_multiprocess arch tuple str Skips test given ROCm archs - multiprocess UTs decorator func reason = None TEST_WITH_ROCM prop = torch cuda get_device_properties gcnArchName split prop arch reason = f skip_if_rocm_arch_multiprocess test skipped arch unittest skipIf reason None reason func decorator skip_if_rocm_ver_lessthan_multiprocess version=None Skips test ROCm based ROCm ver - multiprocess UTs decorator func reason = None TEST_WITH_ROCM rocm_version = str torch version hip rocm_version = rocm_version split - maxsplit= ignore git sha rocm_version_tuple = tuple int x x rocm_version split rocm_version_tuple None version None rocm_version_tuple tuple version reason = f skip_if_rocm_ver_lessthan_multiprocess ROCm rocm_version_tuple available version required unittest skipIf reason None reason func decorator skip_if_win skip_but_pass_in_sandcastle_if sys platform == win This unit test case supported Windows platform sm_is_or_higher_than device torch device major int minor int - bool Returns True device s compute capability major minor higher Error out device CUDA device Returns False device RoCM device Returns True device non-CUDA device device type = cuda True torch version hip None ROCm devices may have different compute capability codes False torch cuda get_device_capability device = major minor retry_on_connect_failures create_tcp_store addr= localhost world_size= is_master=True timeout=timedelta minutes= wait_for_workers=True jit_class=False use_libuv=True Creates TCP store Retries chosen port already use port = find_free_port jit_class timeout_millisecond = int timeout timedelta milliseconds= torch classes dist_c d TCPStore addr port world_size is_master timeout_millisecond c d TCPStore addr port world_size is_master wait_for_workers=wait_for_workers use_libuv=use_libuv TEST_WITH_TSAN TSAN runs much slower TIMEOUT_DEFAULT = TIMEOUT_DEFAULT = int os getenv DISTRIBUTED_TESTS_DEFAULT_TIMEOUT TIMEOUT_OVERRIDE = test_ddp_uneven_inputs https github com pytorch pytorch issues TEST_WITH_ROCM TIMEOUT_OVERRIDE test_join_kwargs = create_device interface=None lazy_init bool = False sys platform == win interface None c d ProcessGroupGloo create_device hostname= lazy_init=lazy_init c d ProcessGroupGloo create_device interface=interface lazy_init=lazy_init get_timeout test_id - int TIMEOUT_OVERRIDE get test_id split - TIMEOUT_DEFAULT contextmanager captured_output new_out new_err = StringIO StringIO old_out old_err = sys stdout sys stderr try sys stdout sys stderr = new_out new_err yield sys stdout sys stderr finally sys stdout sys stderr = old_out old_err simple_sparse_reduce_tests rank int world_size int num_inputs int = Generate number basic test cases sparse reduction These cover tensors varying number sparse dimensions varying number dense dimensions The only reduction operation we support sum generate rank int world_size int sparse_dims int = dense_dims int = First sparse dimension rank Subsequent dimensions always so we know there non-empty intersection between any two sparse tensors indices = torch reshape torch arange rank + rank + shape = world_size + _ range dense_dims _ range sparse_dims - indices = torch cat indices torch zeros rank + shape append world_size values = torch ones rank + + _ range dense_dims torch sparse_coo_tensor indices values shape compute_sum fn world_size int reduce operator add fn rank world_size rank range world_size fn num_inputs rank + i num_inputs world_size i range num_inputs compute_sum fn num_inputs world_size i range num_inputs fn partial generate sparse_dims= partial generate sparse_dims= partial generate sparse_dims= partial generate dense_dims= partial generate dense_dims= partial generate dense_dims= HELPER FOR MULTIGPU TESTS init_multigpu_helper world_size int backend str Multigpu tests designed simulate multi nodes multi GPUs each node Nccl backend requires equal #GPUs each process On single node all visible GPUs evenly divided subsets each process only uses subset nGPUs = torch cuda device_count TEST_HPU nGPUs = torch hpu device_count TEST_XPU nGPUs = torch xpu device_count visible_devices = range nGPUs If rank less than equal number available GPU s then each rank can mapped corresponding GPU nGPUs_per_process = world_size nGPUs nGPUs_per_process = nGPUs world_size rank_to_GPU = i list visible_devices i nGPUs_per_process i + nGPUs_per_process i range world_size rank_to_GPU tmp_dir Optional tempfile TemporaryDirectory = None initialize_temp_directories init_method Optional str = None - None global tmp_dir tmp_dir = tempfile TemporaryDirectory os environ TEMP_DIR = tmp_dir name os mkdir os path join tmp_dir name barrier os mkdir os path join tmp_dir name test_dir init_dir_path = os path join tmp_dir name init_dir os mkdir init_dir_path Set init method specified init_method None os environ INIT_METHOD = init_method os environ INIT_METHOD = FILE_SCHEMA + os path join init_dir_path shared_init_file cleanup_temp_dir - None tmp_dir None tmp_dir cleanup Most tests operate worldsize DEFAULT_WORLD_SIZE = How does MultiProcessTestCase work Each MultiProcessTestCase instance uses + ` world_size ` processes default ` world_size ` returns Let s take ` test_rpc_spawn py ` example which inherits Its ` Setup ` methods calls into ` MultiProcessTestCase _spawn_processes ` which spawns ` world_size ` subprocesses During spawn main process passes test name subprocesses name acquired id The subprocesses then use provided test function name retrieve function attribute test instance run The main process simply waits all subprocesses join MultiProcessTestCase TestCase MAIN_PROCESS_RANK = - This exit code used indicate test code had error exited abnormally There certain tests might use sys exit simulate failures those cases we can t have exit code we still want ensure we didn t run into any other errors TEST_ERROR_EXIT_CODE = do early terminate distributed tests _should_stop_test_suite - bool False Many test cases init process group do destroy This property determines whether base test should call ` destroy_process_group ` behalf test Its value customizable derived TestCase s pan-TestCase value cannot customized each test property destroy_pg_upon_exit - bool True property world_size - int DEFAULT_WORLD_SIZE join_or_run fn wraps fn wrapper rank == MAIN_PROCESS_RANK _join_processes fn fn types MethodType wrapper The main process spawns N subprocesses run test Constructor patches current instance test method assume role main process join its subprocesses run underlying test function __init__ method_name str = runTest methodName str = runTest - None methodName correct naming unittest testslide uses keyword arguments So we need use both break BC support testslide methodName = runTest method_name = methodName super __init__ method_name try fn = getattr method_name setattr method_name join_or_run fn except AttributeError e methodName = runTest we allow instantiation no explicit method name incorrect missing method name raise ValueError f no such test method __class__ methodName e setUp - None super setUp Used tests expected non- exit code such SIGABRT thrown watchdog special_return_code_checks dict = Used tests may any exit code which makes hard check This rare use caution skip_return_code_checks list = processes = type ignore var-annotated rank = MAIN_PROCESS_RANK file_name = tempfile NamedTemporaryFile delete=False name pid pipe consisting error message process pid_to_pipe = type ignore var-annotated tearDown - None super tearDown p processes p terminate Each Process instance holds few open file descriptors The unittest runner creates new TestCase instance each test method keeps alive until end entire suite We must thus reset processes prevent effective file descriptor leak processes = _current_test_name - str id == e g __main__ TestDistributed TestAdditive test_get_rank id split - _start_processes proc - None processes = rank range int world_size parent_conn child_conn = torch multiprocessing Pipe process = proc target=self __class__ _run name= process + str rank args= rank _current_test_name file_name child_conn kwargs= fake_pg getattr fake_pg False process start logger info Started process s pid s rank process pid pid_to_pipe process pid = parent_conn processes append process _spawn_processes - None try torch multiprocessing set_start_method spawn except RuntimeError pass proc = torch multiprocessing get_context spawn Process _start_processes proc Event Enum GET_TRACEBACK = staticmethod _event_listener parent_pipe signal_pipe rank int logger debug Starting event listener thread rank s rank while True ready_pipes = multiprocessing connection wait parent_pipe signal_pipe parent_pipe ready_pipes parent_pipe closed logger debug Pipe closed process s stopping event listener thread rank event = parent_pipe recv logger info Received event s process s event rank event == MultiProcessTestCase Event GET_TRACEBACK Return traceback parent process tempfile NamedTemporaryFile mode= r+ tmp_file faulthandler dump_traceback tmp_file Flush buffers seek read beginning tmp_file flush tmp_file seek parent_pipe send tmp_file read logger info Process s sent traceback rank signal_pipe ready_pipes classmethod _run cls rank int test_name str file_name str parent_pipe kwargs - None = cls test_name rank = rank file_name = file_name run_test test_name parent_pipe run_test test_name str parent_pipe - None Start event listener thread signal_recv_pipe signal_send_pipe = torch multiprocessing Pipe duplex=False event_listener_thread = threading Thread target=MultiProcessTestCase _event_listener args= parent_pipe signal_recv_pipe rank daemon=True event_listener_thread start sys platform = win sys platform = darwin Register signal handler dump stack traces FATALs Windows MacOS do support signal handlers torch _C _set_print_stack_traces_on_fatal_signal True Show full C++ stacktraces when Python error originating C++ raised os environ TORCH_SHOW_CPP_STACKTRACES = common_utils set_rng_seed id == e g __main__ TestDistributed test_get_rank We re retrieving corresponding test executing try getattr test_name except unittest SkipTest se logger info noqa G Process s skipping test s following reason s rank test_name str se sys exit TEST_SKIPS generic exit_code except Exception logger error Caught exception \n s exiting process s exit code s traceback format_exc rank MultiProcessTestCase TEST_ERROR_EXIT_CODE Send error parent process parent_pipe send traceback format_exc sys exit MultiProcessTestCase TEST_ERROR_EXIT_CODE finally signal_send_pipe None signal_send_pipe send None assert event_listener_thread None event_listener_thread join Close pipe after done test parent_pipe close destroy_pg_upon_exit try Some tests do destroy pgs destroy can t called twice This avoids spewing warnings about improperly shutting down c d destroy_process_group except AssertionError ValueError pass _get_timedout_process_traceback - None pipes = i process enumerate processes process exitcode None pipe = pid_to_pipe process pid try pipe send MultiProcessTestCase Event GET_TRACEBACK pipes append i pipe except ConnectionError logger exception Encountered error while trying get traceback process s i Wait results rank pipe pipes try Wait traceback pipe poll pipe closed logger info Pipe closed process s cannot retrieve traceback rank continue traceback = pipe recv logger error Process s timed out traceback \n\n s rank traceback logger error Could retrieve traceback timed out process s rank except ConnectionError logger exception Encountered error while trying get traceback process s rank _join_processes fn - None timeout = get_timeout id start_time = time time subprocess_error = False try while True check see any subprocess exited error early i p enumerate processes This exit code processes exit they encountered exception p exitcode == MultiProcessTestCase TEST_ERROR_EXIT_CODE print f Process i terminated exit code p exitcode terminating remaining processes active_children = torch multiprocessing active_children ac active_children ac terminate subprocess_error = True break subprocess_error break All processes have joined cleanly they all valid exitcode all p exitcode None p processes break Check we should time out test If so we terminate each process elapsed = time time - start_time elapsed timeout _get_timedout_process_traceback print f Timing out after timeout seconds killing subprocesses p processes p terminate break Sleep avoid excessive busy polling time sleep elapsed_time = time time - start_time _check_return_codes fn elapsed_time finally Close all pipes pipe pid_to_pipe values pipe close _check_return_codes fn elapsed_time - None Checks codes all spawned processes match skips tests they returned code indicating skipping condition If no processes spawned there nothing check processes logger warning Note no subprocesses spawned test likely skipped first_process = processes first we check there errors actual processes via TEST_ERROR_EXIT CODE raise exception those reason we do attempt raise more helpful error message than Process x terminated timed out TODO we should pipe exception failed subprocess here Currently actual exception displayed logging output errored_processes = i p i p enumerate processes p exitcode == MultiProcessTestCase TEST_ERROR_EXIT_CODE errored_processes error = i process errored_processes Get error pipe error_message = pid_to_pipe process pid recv error += f Process i exited error code MultiProcessTestCase TEST_ERROR_EXIT_CODE f exception \n error_message \n raise RuntimeError error If no process exited uncleanly we check timeouts then ensure each process exited cleanly i p enumerate processes p exitcode None raise RuntimeError f Process i terminated timed out after elapsed_time seconds Skip test code check fn skip_return_code_checks skip TEST_SKIPS values first_process exitcode == skip exit_code IS_SANDCASTLE Don t use unittest skip skip test sandcastle since creates tasks skipped tests assuming there some follow-up needed Instead just pass test appropriate message logger info Skipping s sandcastle following reason s id skip message raise unittest SkipTest skip message In most cases we expect test exit code standing success expected_return_code = In some negative tests we expect test non-zero exit code such watchdog throwing SIGABRT fn special_return_code_checks expected_return_code = special_return_code_checks fn assertEqual first_process exitcode expected_return_code msg=f Expected exit code expected_return_code got first_process exitcode pid first_process pid property is_master - bool rank == Utility base distributed Multi Process Test cases This abstracts PG creation deletion backends selected based device type The tests functions can instantiated per device type using common_device_type instantiate_device_type_tests other backends can add entry backend function DistributedTestBase MultiProcessTestCase setUp super setUp os environ WORLD_SIZE = str world_size _spawn_processes tearDown try torch distributed destroy_process_group except AssertionError pass try os remove file_name except OSError pass backend device - str cuda device nccl hpu device intel gaudi hccl xpu device xccl gloo create_pg device world_size=None world_size None world_size = world_size num_visible_devices = torch get_device_module device device_count store = torch distributed FileStore file_name num_visible_devices torch distributed init_process_group backend=self backend device world_size=world_size rank=self rank store=store nccl backend device xccl backend device torch accelerator set_device_index rank torch distributed distributed_c d _get_default_group rank_to_device device num_visible_devices = torch get_device_module device device_count i i num_visible_devices i range world_size run_subtests cls_inst subtest_config dict str list Any test_fn Callable test_args test_kwargs Any Runs test function given ` ` test_fn ` ` subtest according configurations specified ` ` subtest_config ` ` This amortizes costly setup overhead including process spawn initializing process group over subtests Args subtest_config Dict str List Any A mapping subtest keyword argument name list its possible values test_fn Callable A callable runs actual test test_args Positional arguments pass ` ` test_fn ` ` test_kwargs Keyword arguments pass ` ` test_fn ` ` Convert config mapping list have fixed order subtest_config_items list tuple str list Any = list subtest_config items subtest_config_keys list str = item item subtest_config_items subtest_config_values list list Any = item item subtest_config_items values itertools product subtest_config_values Map keyword chosen value subtest_kwargs = dict zip subtest_config_keys values strict=True cls_inst subTest subtest_kwargs torch _dynamo reset test_fn test_args test_kwargs subtest_kwargs torch _dynamo reset c d barrier functools cache has_efa - bool If shell command ` fi_info -p efa -t FI_EP_RDM ` returns exit code then we assume machine has Libfabric EFA interfaces EFA software components installed see https docs aws amazon com AWSEC latest UserGuide efa-start html try subprocess run fi_info -p efa -t FI_EP_RDM check=False returncode == except FileNotFoundError pass False tp_transports If machine has Libfabric EFA interfaces EFA software components installed may cause RuntimeError In operator tensorpipe common ibv h Operation supported tensorpipe uses InfiniBand transport so we exclude tensorpipe transports see https github com pytorch pytorch issues https github com pytorch pytorch issues shm uv has_efa None spawn_threads_and_init_comms func=None timeout=TIMEOUT_DEFAULT world_size=DEFAULT_WORLD_SIZE Wrapper use test method func None partial spawn_threads_and_init_comms timeout=timeout world_size=world_size _run_test_method_with_multi_threads world_size callback world = _install_threaded_pg global_store = c d HashStore world_is_valid world == c d distributed_c d _world worker rank world_pg store c d init_process_group backend= threaded rank=rank world_size=world_size store=store try callback except BaseException ex noqa B Exceptions handled MultiThreadedTestCase MultiThreadedTestCase exception_queue put rank sys exc_info ProcessLocalGroup exception_handle ex trigger _terminate event awaken worker threads finally world_is_valid c d destroy_process_group threads = rank range world_size t = threading Thread target=worker args= rank world global_store t start threads append t threads wraps func wrapper args kwargs TODO get test name kwargs torch _C _distributed_c d _set_thread_isolation_mode True try threads = _run_test_method_with_multi_threads world_size lambda func args kwargs join error handling MultiThreadedTestCase _join_threads threads func finally torch _C _distributed_c d _set_thread_isolation_mode False wrapper MultiThreadedTestCase TestCase Test runner runs all tests in-proc process group using multiple threads threaded process group Each test spawns world_size threads run test method each thread Difference regular MultiProcess test runner Must explicitly defines SetUp call _spawn_threads run tests Cannot use setUp tearDown must use perThreadSetup perThreadShutdown set up tear down each thread when running each test No global state possible How bad limitation exception_queue = queue Queue MAIN_THREAD_RANK = - join_or_run fn wraps fn wrapper rank == MAIN_THREAD_RANK _join_threads threads fn fn types MethodType wrapper __init__ method_name str = runTest methodName str = runTest - None methodName correct naming unittest testslide uses keyword arguments So we need use both break BC support testslide methodName = runTest method_name = methodName super __init__ method_name try fn = getattr method_name setattr method_name join_or_run fn except AttributeError e methodName = runTest we allow instantiation no explicit method name incorrect missing method name raise ValueError f no such test method __class__ methodName e perThreadSetUp super setUp TestCase setUp calls torch manual_seed pass perThreadTearDown pass setUp - None setUp only set up things main thread you want configure things spawned threads use perThreadSetUp super setUp rank = MAIN_THREAD_RANK threads = Show full C++ stacktraces when Python error originating C++ raised os environ TORCH_SHOW_CPP_STACKTRACES = tearDown tearDown only set up things main thread you want configure things spawned threads use perThreadTearDown super tearDown threads = _spawn_threads method spawn threads run test use method SetUp your TestCase torch _C _distributed_c d _set_thread_isolation_mode True test_name = _current_test_name each test case we need create thread local world global store world = _install_threaded_pg __class__ global_store = c d HashStore world_is_valid world == c d distributed_c d _world world_is_valid raise RuntimeError Invalid world rank range world_size t = threading Thread target=self __class__ _run args= test_name rank world_size t start threads append t classmethod _run cls test_name rank world_size kwargs = cls test_name rank = rank precision rel_tol thread-local setting since may overridden per test need make every thread have same value This would relevant when we use op db tests where needs those states set i e using instantiate_device_type_tests TODO figure out better way do hasattr _tls _tls = threading local _tls precision = TestCase _precision _tls rel_tol = TestCase _rel_tol run_test_with_threaded_pg test_name rank world_size run_test_with_threaded_pg test_name rank world_size Run current test associated ` test_name ` using threaded process group c d init_process_group backend= threaded rank=rank world_size=world_size store=self __class__ global_store perThreadSetUp try getattr test_name except BaseException ex noqa B exception_queue put rank sys exc_info ProcessLocalGroup exception_handle ex trigger _terminate event awaken worker threads finally c d destroy_process_group perThreadTearDown classmethod _join_threads cls threads fn timeout = TIMEOUT_DEFAULT try idx thread enumerate threads thread join max timeout thread is_alive MultiThreadedTestCase exception_queue put idx TimeoutError TimeoutError f Rank failed join under timeout seconds None ProcessLocalGroup reset failed_ranks = while cls exception_queue empty failure = cls exception_queue get failed_ranks append failure finally _uninstall_threaded_pg torch _C _distributed_c d _set_thread_isolation_mode False cls _check_return_codes failed_ranks timeout fn classmethod _check_return_codes cls failed_ranks timeout fn Print based exceptions raised threads SkipTest print info each thread TimeoutError raise RuntimeError any timed out thread Normal Exception print error each thread raises exception raise RuntimeError error_msg = skip_code = - rank exc_info failed_ranks exc = exc_info isinstance exc unittest SkipTest logger info Thread s skipping test s following reason s rank fn str exc skip_code skip_code = TEST_SKIPS generic exit_code isinstance exc TimeoutError msg = f Thread rank terminated timed out after timeout seconds\n logger error msg raise RuntimeError msg isinstance exc Exception msg = join traceback format_exception exc_info logger error Caught exception \n s exiting thread s msg rank error_msg += f Thread rank exited exception \n msg \n isinstance exc SystemExit type exc code int skip_code skip_code = exc code check exceptions len error_msg raise RuntimeError error_msg check skip skip_code skip TEST_SKIPS values skip_code == skip exit_code IS_SANDCASTLE pass test appropriate message logger info Skipping s sandcastle following reason s fn skip message raise unittest SkipTest skip message property world_size - int DEFAULT_WORLD_SIZE property _current_test_name - str id == e g __main__ TestDistributed TestAdditive test_get_rank id split - assertEqualOnRank x y msg=None rank= The reason why we have util function instead assertEqual all threads sharing one CPU RNG so assertion result only reliable rank rank == rank assertEqual x y msg assertNotEqualOnRank x y msg=None rank= rank == rank assertNotEqual x y SaveForwardInputsModule nn Module __init__ forward_inputs dict nn Module torch Tensor cast_forward_inputs bool - None super __init__ l = nn Linear forward_inputs = forward_inputs cast_forward_inputs = cast_forward_inputs forward x torch Tensor - torch Tensor forward_inputs = x l x l weight dtype cast_forward_inputs x SaveForwardInputsModel nn Module __init__ forward_inputs dict nn Module torch Tensor cast_forward_inputs bool - None super __init__ c = SaveForwardInputsModule forward_inputs cast_forward_inputs c = SaveForwardInputsModule forward_inputs cast_forward_inputs forward_inputs = forward_inputs forward x torch Tensor - torch Tensor forward_inputs = x c c x contextmanager _dynamo_dist_per_rank_init rank world_size backend=None init_pg=True fake_pg=False To avoid multiple inheritance _dynamo test_case TestCase MultiProcessTestCase Just manually implement most important part dynamo behavior reset clear fake_pg torch accelerator set_device_index rank device_type = acc type acc = torch accelerator current_accelerator cpu backend None backend = c d get_default_backend_for_device device_type os environ MASTER_ADDR = localhost os environ MASTER_PORT = init_pg fake_pg store = torch testing _internal distributed fake_pg FakeStore c d init_process_group backend= fake world_size=world_size rank=rank store=store c d init_process_group backend=backend rank=rank world_size=world_size torch _dynamo reset torch _dynamo utils counters clear try yield finally torch _dynamo reset torch _dynamo utils counters clear init_pg c d destroy_process_group DynamoDistributedSingleProcTestCase torch _dynamo test_case TestCase Test harness single-process dynamo distributed tests initializes dist process group Prefer simple tests s easier debug classmethod setUpClass cls super setUpClass _exit_stack set up TestCase cls _exit_stack enter_context patch dict os environ MASTER_ADDR localhost MASTER_PORT cls rank = device = torch accelerator current_accelerator type cls device = f device cls rank cls device_ids = None device cls device cls rank c d init_process_group c d get_default_backend_for_device device rank=cls rank world_size= classmethod tearDownClass cls c d destroy_process_group super tearDownClass DynamoDistributedMultiProcTestCase DistributedTestBase Use tests actually run multiple GPUs Decorate tests skip_if_lt_x_gpu ngpu Note MultiProcTestCase spawns processes per test slow Prefer MultiThreadedTestCase most tests Perhaps use one sparingly integration tests property world_size - int torch accelerator device_count classmethod _run cls rank int test_name str file_name str parent_pipe kwargs - None trace_log addHandler logging NullHandler The rest copypasta MultiProcessTestCase _run = cls test_name rank = rank file_name = file_name run_test test_name parent_pipe MultiProcContinuousTest TestCase Class variables MAIN_PROCESS_RANK = - number test processes world_size int = - unset state rank current process rank int = - unset state Rendezvous file rdvz_file Optional str = None timeout configured per timeout timedelta = timedelta seconds= Poison pill rest tests one them fails poison_pill bool = False classmethod backend_str cls - Optional str ProcessGroup backend str To customized sub test classes e g nccl Otherwise we None -- lazily decided tensor None Please override you intend test specific device type classmethod device_type cls - str curr_device = torch accelerator current_accelerator curr_device None cpu curr_device type classmethod opts cls high_priority_stream=False ProcessGroup init options To customized sub test classes e g ProcessGroupNCCLOpTest Here we None None classmethod _init_pg cls rank world_size rdvz_file assert rdvz_file None rank should local_rank tests running = gpus which how all these tests designed we expect LOCAL_RANK set torchrun Setting lets init_device_mesh set device without issuing warning os environ LOCAL_RANK = str rank store = c d FileStore rdvz_file world_size create nccl processgroup opts c d init_process_group backend=cls backend_str world_size=world_size rank=rank store=store pg_options=cls opts timeout=cls timeout cls pg = c d distributed_c d _get_default_group classmethod _run_test_given_id cls test_id str kwargs - None id == e g __main__ TestDistributed TestAdditive test_get_rank test_name = test_id rsplit maxsplit= - Get test function test = cls test_name rank = cls rank world_size = cls world_size test_fn = getattr test_name Ensure all ranks use same seed common_utils set_rng_seed Run test function test_fn kwargs classmethod _worker_loop cls rank world_size rdvz_file task_queue completion_queue raised_exception = False Sub tests going access these values check first assert = rank world_size set variables test cls rank = rank cls world_size = world_size Initialize process group cls _init_pg rank world_size rdvz_file End bootstrap logger debug Setup complete Loop forever waiting test name run while True test_id = task_queue get logger debug f Got test test_id noqa G None means exit test_id None break Run test try cls _run_test_given_id test_id completion_queue put test_id except BaseException ex noqa B raised_exception = True Send exception stack trace back dispatcher exc_info = sys exc_info tb_str = join traceback format_exception exc_info Create new exception original exception traceback enhanced_ex = RuntimeError f Exception worker process \n tb_str enhanced_ex __cause__ = ex completion_queue put enhanced_ex Termination logger debug Terminating Calling destroy_process_group when workers have exceptions while others doing collectives will cause deadlock since waits enqueued collectives finish Only call clean exit path raised_exception c d destroy_process_group classmethod _spawn_processes cls world_size - None cls processes = cls task_queues = cls completion_queues = Need rendezvous file ` init_process_group ` purpose cls rdvz_file = tempfile NamedTemporaryFile delete=False name CUDA multiprocessing requires spawn instead fork make sure child processes have their own memory space try torch multiprocessing set_start_method spawn except RuntimeError The start method has already been set pass rank range int world_size task_queue = torch multiprocessing Queue completion_queue = torch multiprocessing Queue process = torch multiprocessing Process target=cls _worker_loop name= process + str rank daemon=True so child processes will exit parent decides terminate args= rank world_size cls rdvz_file task_queue completion_queue process start cls processes append process cls task_queues append task_queue cls completion_queues append completion_queue logger debug Started process s pid s rank process pid noqa UP classmethod setUpClass cls Class-scope test fixture Run once entire test before any test starts Set up process group super setUpClass Use device count world size device_type = cls device_type If world_size set use device count cls world_size == - cls world_size = torch get_device_module device_type device_count cls world_size == raise unittest SkipTest f No device_type devices available logger info f Testing cls __name__ cls world_size device_type noqa G cls _spawn_processes cls world_size classmethod tearDownClass cls Class-scope test fixture Run once entire test after all tests finish Tear down process group logger debug f Joining cls world_size workers noqa G Enqueue None all workers tell them exit task_queue cls task_queues task_queue put None Wait all workers exit process cls processes process join Clear up rendezvous file try os remove cls rdvz_file except OSError pass logger info f Class cls __name__ finished noqa G super tearDownClass setUp - None Test fixture Run before each test super setUp I am dispatcher rank = MAIN_PROCESS_RANK If test hits exception one test skip rest tests __class__ poison_pill raise unittest SkipTest f Previous test failed skipping id Enqueue current test all workers i task_queue enumerate task_queues logger debug f Sending Rank i id noqa G task_queue put id _worker_run_main_wait fn wraps fn wrapper rank == MAIN_PROCESS_RANK logger debug f Waiting workers finish id noqa G Wait workers finish test i completion_queue enumerate completion_queues rv = completion_queue get isinstance rv BaseException Hit exception re-raise main process logger warning f Detected failure Rank i id noqa G f skipping rest tests Test __class__ __name__ noqa G Poison rest tests because ProcessGroup may reusable now __class__ poison_pill = True raise rv Success assert rv == id logger debug f Main proc detected rank i finished id noqa G Worker just runs test fn types MethodType wrapper The main process spawns N subprocesses run test Constructor patches current instance test method assume role main process join its subprocesses run underlying test function __init__ method_name str = runTest methodName str = runTest - None methodName correct naming unittest testslide uses keyword arguments So we need use both break BC support testslide methodName = runTest method_name = methodName super __init__ method_name try fn = getattr method_name setattr method_name _worker_run_main_wait fn except AttributeError e methodName = runTest we allow instantiation no explicit method name incorrect missing method name raise ValueError f no such test method __class__ methodName e