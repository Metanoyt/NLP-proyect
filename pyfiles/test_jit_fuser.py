Owner s oncall jit unittest os sys torch torch nn nn torch nn functional F torch testing FileCheck unittest skipIf __name__ == __main__ torch testing _internal common_utils parse_cmd_line_args The value GRAPH_EXECUTOR depends command line arguments so make sure they re parsed before instantiating tests parse_cmd_line_args torch testing _internal common_utils run_tests IS_SANDCASTLE ProfilingMode GRAPH_EXECUTOR \ enable_profiling_mode_for_profiling_tests IS_WINDOWS TemporaryDirectoryName shell torch testing _internal jit_utils JitTestCase enable_cpu_fuser _inline_everything \ RUN_CUDA RUN_CUDA_HALF RUN_CUDA_MULTI_GPU warmup_backward textwrap dedent itertools product permutations torch testing _internal common_cuda with_tf _off test_jit backward_graph all_backward_graphs get_lstm_inputs get_milstm_inputs \ LSTMCellC LSTMCellF LSTMCellS MiLSTMCell GRAPH_EXECUTOR == ProfilingMode PROFILING torch _C _jit_set_profiling_executor True torch _C _jit_set_profiling_mode True strip_profiling_nodes nodes profiling_opcodes = prim BailoutTemplate prim BailOut n n nodes n kind profiling_opcodes warmup_forward f args profiling_count = _ range profiling_count results = f args results skipIf GRAPH_EXECUTOR == ProfilingMode LEGACY skip due SIGIOT failures TestFuser JitTestCase assertAllFused graph except_for= diff_graphs = n n graph nodes n kind == prim DifferentiableGraph len diff_graphs assertEqual len diff_graphs graph = diff_graphs g Subgraph allowed_nodes = prim Constant prim FusionGroup prim BailoutTemplate prim BailOut prim TupleConstruct &#124; set except_for assertTrue all node kind allowed_nodes node graph nodes f got graph assertTrue node kind node graph nodes count prim FusionGroup == _test_fused_abs device= cpu func x x abs = torch randn device=device scripted = checkScript func assertAllFused scripted graph_for unittest skipIf IS_SANDCASTLE NYI fuser CPU support Sandcastle enable_cpu_fuser test_abs_cpu _test_fused_abs unittest skipIf IS_WINDOWS This meant Windows-specific unittest skipIf IS_SANDCASTLE NYI fuser CPU support Sandcastle enable_cpu_fuser test_abs_cpu_unicode_temp_dir TemporaryDirectoryName suffix= \u e d\u dname shell_env = os environ copy shell_env TMP = dname cmd = sys executable os path basename __file__ type __name__ + test_abs_cpu legacy_jit_flag = -- jit-executor=legacy v sys argv v == legacy_jit_flag cmd append legacy_jit_flag return_code = shell cmd cwd=os path dirname __file__ env=shell_env assertEqual return_code unittest skipIf RUN_CUDA requires CUDA test_abs_cuda _test_fused_abs device= cuda unittest skipIf RUN_CUDA requires CUDA test_zero_element_tensors decode sin_t cos_t theta = torch atan sin_t float cos_t float theta sin = torch zeros device= cuda cos = torch zeros device= cuda inputs = sin cos checkScript decode inputs unittest skipIf RUN_CUDA fuser requires CUDA test_arg_configurations_smoke_cuda A smoke test make sure we won t use same kernel contiguous non-contiguous arguments TODO add optionally enabled debug counters fuser verify we really can tell difference between configurations f x y z z = x + y chunk dim= z z x = torch randn dtype=torch float device= cuda y = torch randn dtype=torch float device= cuda traced_f = torch jit trace f x y assertEqual traced_f x t contiguous y traced_f x t y unittest skipIf RUN_CUDA fuser requires CUDA test_broadcast_cuda scaleshift x scale shift x scale + shift inputs = torch randn dtype=torch float device= cuda torch randn dtype=torch float device= cuda torch randn dtype=torch float device= cuda ge = checkTrace scaleshift inputs assertAllFused ge graph_for inputs unittest skipIf RUN_CUDA fuser requires CUDA unittest skipIf GRAPH_EXECUTOR = ProfilingMode LEGACY no bfloat support profiling test_cuda_bfloat foo x y x + y relu m = torch jit script foo x = torch randn cuda bfloat y = torch randn_like x assertAllFused m graph_for x y unittest skipIf RUN_CUDA fuser requires CUDA unittest skipIf RUN_CUDA_HALF no half support unittest skipIf GRAPH_EXECUTOR = ProfilingMode LEGACY no half support profiling test_cuda_half x = torch randn dtype=torch half device= cuda y = torch randn dtype=torch half device= cuda funcs = fn_test_comparison_gt_lt fn_test_relu fn_test_exp Note Non fused inputs must float prevent loss precision inputs = x float y float fusion_inputs = x y fn funcs local_inputs = t clone requires_grad_ t inputs local_fusion_inputs = t clone requires_grad_ t fusion_inputs Verifies outputs fusion = torch jit trace fn local_fusion_inputs check_trace=False outputs = fn local_inputs fusion_outputs = fusion local_fusion_inputs outputs_half = t half t outputs assertEqual outputs_half fusion_outputs Verifies gradients output fusion_output zip outputs_half fusion_outputs grads = torch autograd grad output float sum local_inputs allow_unused=True retain_graph=True fusion_grads = torch autograd grad fusion_output sum local_fusion_inputs allow_unused=True retain_graph=True grads_half = t half t grads assertEqual grads_half fusion_grads unittest skipIf RUN_CUDA fuser requires CUDA test_checks_cat_inputs We shouldn t treat cat nodes broadcasting All their inputs need checked having same map size before we can run kernel f x y torch cat x + x + x y + y + y dim= NOTE y broadcastable x output f x y should have shape x x x = torch randn dtype=torch float device= cuda y = torch randn dtype=torch float device= cuda scripted = checkScript f x y assertAllFused scripted graph_for x y unittest skipIf RUN_CUDA No CUDA test_remainder_cuda cuda_rem x y + torch remainder x y - = torch rand dtype=torch float cuda b = torch rand dtype=torch float cuda inputs = b ge = checkScript cuda_rem inputs graph = ge graph_for inputs assertAllFused graph unittest skipIf RUN_CUDA No CUDA test_chunk_cuda fn x b c = x chunk b + c inputs = torch randn dtype=torch float device= cuda ge = checkScript fn inputs graph = ge graph_for inputs assertAllFused graph FileCheck check prim ConstantChunk chunks= dim= run str graph staticmethod _test_chunk_correctness device= cpu chunk_ _ x x x x x = x chunk x + x + x + x chunk_ _ x x x x x = x chunk x + x + x + x chunk_ _last x x x x x = x chunk x + x + x + x fns = chunk_ _ chunk_ _ chunk_ _last tensors = splitSize = torch randn dtype=torch float device=device contiguous case torch randn dtype=torch float device=device non-contiguous case torch randn dtype=torch float device=device transpose tensor tensors fn fns checkScript fn tensor unittest skipIf IS_SANDCASTLE NYI fuser CPU support Sandcastle enable_cpu_fuser test_chunk_correctness _test_chunk_correctness cpu unittest skipIf RUN_CUDA No CUDA test_chunk_correctness_cuda _test_chunk_correctness cuda unittest skipIf RUN_CUDA fuser requires CUDA test_chunk_distributes_cuda f x y z z = x + y chunk dim= z z x = torch randn dtype=torch float device= cuda y = torch randn dtype=torch float device= cuda ge = checkTrace f x y graph = ge graph_for x y FileCheck check broadcast_tensors check prim FusionGroup_ \ check_count ConstantChunk exactly=True run str graph unittest skipIf RUN_CUDA fuser requires CUDA test_chunk_motion_deduplicates_inputs func x z = x x z z = z chunk z z func x z = x x x z z = z chunk z z inputs = torch tensor device= cuda dtype=torch float func func func module = checkScript func inputs forward_graph = module graph_for inputs assertGraphContainsExactly forward_graph prim FusionGroup fusion_group = list forward_graph nodes - assertEqual len list fusion_group inputs unittest skipIf RUN_CUDA No CUDA test_chunk_multiple_cuda The arguments intentionally used out order test see fusion compiler adds extra args correct order fn s x y z z z = z chunk x x x = x chunk y y = y chunk s + x + x + x + y + y + z + z inputs = torch randn dtype=torch float device= cuda torch randn dtype=torch float device= cuda torch randn dtype=torch float device= cuda torch randn dtype=torch float device= cuda ge = checkScript fn inputs assertAllFused ge graph_for inputs unittest skipIf RUN_CUDA fuser requires CUDA test_minmax tmax b torch max b tmin b torch min b = torch randn dtype=torch float device= cuda b = torch randn dtype=torch float device= cuda nan = torch tensor float nan dtype=torch float device= cuda f inputs product tmax tmin b nan b nan s = checkScript f inputs assertAllFused s graph_for inputs unittest skipIf RUN_CUDA fuser requires CUDA test_clamp func b torch clamp + b min= max= funcInf b torch clamp + b min= max=float inf funcOptMin b torch clamp + b max= funcOptMax b torch clamp + b min= = torch randn dtype=torch float device= cuda requires_grad=True b = torch randn dtype=torch float device= cuda nan = torch tensor float nan dtype=torch float device= cuda funcs = func funcInf funcOptMin funcOptMax f inputs product funcs b nan f __disable_jit_function_caching__ = True inp inp = inputs s = checkScript f inp inp profiling=ProfilingMode PROFILING assertAllFused s graph_for inp inp except_for= aten size aten _size_if_not_equal c = s inp inp enable_profiling_mode_for_profiling_tests warmup_backward c sum graph = backward_graph s assertAllFused graph except_for= aten Float aten _grad_sum_to_size unittest skipIf RUN_CUDA fuser requires CUDA unittest skipIf GRAPH_EXECUTOR = ProfilingMode LEGACY no half support profiling test_dropout func x x = torch nn functional dropout x torch nn functional relu x = torch randn dtype=torch float device= cuda requires_grad=True s = torch jit script func c = s c = s warmup_backward c sum skip_check skip extra bailout nodes between graph = backward_graph s skip_check=True assertAllFused graph except_for= aten div prim Constant unittest skipIf RUN_CUDA fuser requires CUDA test_comparison_eq_ne f x y mask = x == type_as x z = x mask + y mask = x = type_as x z = z mask + y z x = torch randn dtype=torch float device= cuda y = torch randn dtype=torch float device= cuda ge = checkTrace f x y assertAllFused ge graph_for x y staticmethod fn_test_comparison_gt_lt x y mask = x type_as x z = x mask + y mask = x type_as x z = z mask + y z unittest skipIf RUN_CUDA fuser requires CUDA test_comparison_gt_lt_cuda x = torch randn dtype=torch float device= cuda y = torch randn dtype=torch float device= cuda ge = checkTrace fn_test_comparison_gt_lt x y assertAllFused ge graph_for x y unittest skipIf RUN_CUDA fuser requires CUDA test_comparison_ge_le_cuda f x y mask = x = type_as x z = x mask + y mask = x = type_as x z = z mask + y z x = torch randn dtype=torch float device= cuda y = torch randn dtype=torch float device= cuda ge = checkTrace f x y assertAllFused ge graph_for x y x requires_grad_ True y requires_grad_ True assertAllFused ge graph_for x y except_for= aten size prim BroadcastSizes aten _size_if_not_equal unittest skipIf RUN_CUDA fuser requires CUDA test_addcmul_cuda t = torch randn dtype=torch float device= cuda t = torch randn dtype=torch float device= cuda t = torch randn dtype=torch float device= cuda foo t t t t addcmul t + t value= ge = checkTrace foo t t t allow_unused=True graph = ge graph_for t t t assertAllFused graph TODO We leak CUDA memory here because traced graph holds onto constant-ified tensor Since Python-global CompilationUnit alive until end process memory effectively leaked Removed ` _cuda ` suffix test which disables leak-checking If real problem we ll need revisit Torchscript Function lifetimes Python unittest skipIf RUN_CUDA fuser requires CUDA test_lerp start = torch randn dtype=torch float device= cuda end = torch randn dtype=torch float device= cuda weight = torch tensor dtype=torch float device= cuda scalar weight overload foo_weight_scalar start end torch lerp start + end tensor weight overload foo_weight_tensor start end torch lerp start + end weight ge_weight_scalar = checkTrace foo_weight_scalar start end graph = ge_weight_scalar graph_for start end assertAllFused graph ge_weight_tensor = checkTrace foo_weight_tensor start end graph = ge_weight_tensor graph_for start end assertAllFused graph unittest skipIf RUN_CUDA fuser requires CUDA test_concat_cuda hx = torch randn dtype=torch float device= cuda cx = torch randn dtype=torch float device= cuda foo hx cx torch cat hx + cx hx cx ge = checkTrace foo hx cx graph = ge graph_for hx cx assertAllFused graph FileCheck check FusedConcat check_next run str graph unittest skipIf RUN_CUDA fuser requires CUDA test_concat_invariant_cuda Invariant output prim FusedConcat may input any node inside FusionGroup fn x y z x = x + y y = x - y w = torch cat x y w + z x = torch randn dtype=torch float device= cuda y = torch randn dtype=torch float device= cuda z = torch randn dtype=torch float device= cuda ge = checkTrace fn x y z graph = ge graph_for x y z assertAllFused graph except_for= aten add FileCheck check FusedConcat check_next run str graph staticmethod fn_test_exp x y x + y exp unittest skipIf RUN_CUDA fuser requires CUDA test_exp_cuda x = torch randn dtype=torch float device= cuda y = torch randn dtype=torch float device= cuda ge = checkTrace fn_test_exp x y assertAllFused ge graph_for x y unittest skipIf RUN_CUDA fuser requires CUDA unittest skipIf GRAPH_EXECUTOR = ProfilingMode LEGACY broken profiling torch _jit_internal _disable_emit_hooks_decorator _inline_everything test_fuse_decompose_normalization ResLike torch jit ScriptModule __init__ norm_module super __init__ nm = norm_module torch jit script_method forward x y y + torch relu nm x test_norm_decompose nm in_opt_graph not_in_opt_graph in_fusegraph model = ResLike nm cuda model_noopt = ResLike nm cuda model_noopt load_state_dict model state_dict x = torch randn device= cuda y = torch randn device= cuda FIXME We need differentiation CNNs optimization trigger torch no_grad out = model x y graph = model graph_for x y rep = str graph torch jit optimized_execution False out_noopt = model_noopt x y rep_noopt = str model_noopt graph_for x y assertEqual out out_noopt atol= e- Check normalization op has really been decomposed node_in_graph in_opt_graph assertIn node_in_graph rep node_not_in_graph not_in_opt_graph assertNotIn node_not_in_graph rep assertIn node_not_in_graph rep_noopt fusion_groups = node node graph nodes node kind == prim FusionGroup assertEqual len fusion_groups fused_graph = str fusion_groups g Subgraph node_in_fusegraph in_fusegraph assertIn node_in_fusegraph fused_graph test batchnorm decompose bm = nn BatchNorm d test_norm_decompose bm aten batch_norm_update_stats aten batch_norm aten sqrt test layernorm decompose lm = nn LayerNorm test_norm_decompose lm aten batch_norm_stats aten layer_norm aten sub aten mul aten add unittest skipIf RUN_CUDA fuser requires CUDA test_threshold f x torch threshold x - + x + x + x x = torch tensor - - device= cuda scripted = checkScript f x assertAllFused scripted graph_for x unittest skipIf RUN_CUDA fuser requires CUDA test_scalar_arg_cuda fn_test_scalar_arg x torch Tensor p float - torch Tensor p x x + x x = torch randn dtype=torch float device= cuda p = scripted = checkScript fn_test_scalar_arg x p assertAllFused scripted graph_for x p x requires_grad_ True use another function otherwise we will bailout won t able do fused checks fn_test_scalar_arg_requires_grad x torch Tensor p float - torch Tensor p x x + x scripted = torch jit script fn_test_scalar_arg_requires_grad scripted x p assertAllFused scripted graph_for x p except_for= aten size prim BroadcastSizes aten _size_if_not_equal unittest skipIf IS_SANDCASTLE NYI fuser CPU support Sandcastle unittest skip deduplicating introduces aliasing backward graph s outputs enable_cpu_fuser test_fuser_deduplication See fusion kernel outputs deduplicated when removing _grad_sum_to_size fuser s compilation see discussion PR f x y torch sigmoid x + y b = torch randn requires_grad=True = torch randn requires_grad=True s = checkScript f b assertAllFused s graph_for b except_for= aten size aten _size_if_not_equal prim BroadcastSizes c = s b results = warmup_backward c sum b ga gb = results pop graph = backward_graph s assertAllFused graph check b share storage i e generated single output fuser assertEqual ga data_ptr gb data_ptr unittest skipIf IS_SANDCASTLE NYI fuser CPU support Sandcastle enable_cpu_fuser unittest skip temporarily disabled because fusion restricted fixing test_fuser_iou This checks most Intersection over Union fused In particular backward contains many _grad_sum_to_size iou b x b y b x b y b x b y b x b y ltx = torch max b x b x N M lty = torch max b y b y rbx = torch min b x b x rby = torch min b y b y w = rbx - ltx clamp min= max=float inf N M h = rby - lty clamp min= max=float inf N M inter = w h N M area = b x - b x b y - b y N area = b x - b x b y - b y M iou = inter area + area - inter iou box = torch randn requires_grad=True box = torch randn requires_grad=True unsqueezing can currently fused b x = box unsqueeze N b y = box unsqueeze b x = box unsqueeze b y = box unsqueeze b x = box unsqueeze N b y = box unsqueeze b x = box unsqueeze b y = box unsqueeze s = checkScript iou b x b y b x b y b x b y b x b y assertAllFused s graph_for b x b y b x b y b x b y b x b y except_for= aten size prim BroadcastSizes aten _size_if_not_equal enable_profiling_mode_for_profiling_tests True c = s b x b y b x b y b x b y b x b y warmup_backward c sum b x b y b x b y b x b y b x b y graph = backward_graph s assertAllFused graph except_for= aten size prim BroadcastSizes aten _size_if_not_equal unittest skipIf RUN_CUDA fuser requires CUDA unittest skipIf RUN_CUDA_MULTI_GPU needs non-zero device enable_cpu_fuser test_fusion_reuse_multi_gpu fn x y x y x y inputs_cpu = torch randn dtype=torch float torch randn dtype=torch float inputs_cuda = x cuda x inputs_cpu inputs_cuda = y cuda y inputs_cpu Should crash these should compile different kernels ge = checkScript fn inputs_cpu assertAllFused ge graph_for inputs_cpu ge inputs_cuda ge inputs_cuda unittest skipIf RUN_CUDA fuser requires CUDA unittest skipIf RUN_CUDA_MULTI_GPU needs non-zero device enable_cpu_fuser test_kernel_cache_multi_gpu not_fusible x x fn x y z x_out = x x x x x fusion lambda x x x x x x y_out = y y y y y z_out = z z z z z not_fusible x_out not_fusible y_out not_fusible z_out inputs = torch randn dtype=torch float torch randn dtype=torch float device= cuda torch randn dtype=torch float device= cuda prev_cache_size = torch _C _jit_debug_fuser_num_cached_kernel_specs There FusionGroups Because they have same graph they should reuse same KernelSpec KernelSpec cache ge = checkScript fn inputs assertGraphContainsExactly ge graph_for inputs prim FusionGroup True new_cache_size = torch _C _jit_debug_fuser_num_cached_kernel_specs XXX This assumes same kernel isn t already used another test assertEqual new_cache_size - prev_cache_size unittest skipIf RUN_CUDA_MULTI_GPU needs non-zero device test_nonzero_device_cuda device = cuda + str x = torch tensor dtype=torch float device=device y = torch tensor dtype=torch float device=device doit x y torch sigmoid torch tanh x x + y + x ge = checkTrace doit x y assertAllFused ge graph_for x y unittest skipIf RUN_CUDA fuser requires CUDA test_lstm_cuda inputs = get_lstm_inputs cuda training=True module = checkScript LSTMCellS inputs forward_graph = module graph_for inputs assertGraphContainsExactly forward_graph prim FusionGroup consider_subgraphs=True assertTrue len strip_profiling_nodes forward_graph nodes == Everything differentiable TupleConstruct FileCheck check DifferentiableGraph check_next TupleConstruct \ check_next run str forward_graph enable_profiling_mode_for_profiling_tests True hy cy = module inputs warmup_backward hy + cy sum backward = backward_graph module assertAllFused backward except_for= aten t aten mm aten _grad_sum_to_size unittest skipIf RUN_CUDA fuser requires CUDA By default Ampere later GPUs LSTM computes float tensors TF precision We want float tensors computed full precision order use default precision with_tf _off test_lstm_concat_cuda inputs = get_lstm_inputs cuda ge = checkTrace LSTMCellC inputs graph = ge graph_for inputs FileCheck check FusedConcat check_next run str graph unittest skipIf RUN_CUDA fuser requires CUDA test_lstm_gates_permutations_cuda lstm has gates = x mm w_ih t + hx mm w_hh t + b_ih + b_hh Test any permutation will still result one FusionGroup choices = x mm w_ih t hx mm w_hh t b_ih b_hh template = dedent cell x hx cx w_ih w_hh b_ih b_hh gates = + + + ingate forgetgate cellgate outgate = gates chunk ingate forgetgate cellgate outgate permutation permutations choices len choices code = template format permutation scope = exec code globals scope cu = torch jit CompilationUnit code inputs = get_lstm_inputs cuda training=False assertEqual cu cell inputs scope cell inputs forward_graph = cu cell graph_for inputs assertGraphContainsExactly forward_graph prim FusionGroup TODO Fuser doesn t work all when inputs require grad Fix unittest skipIf RUN_CUDA fuser requires CUDA By default Ampere later GPUs LSTM computes float tensors TF precision We want float tensors computed full precision order use default precision with_tf _off test_lstm_traced_cuda inputs = get_lstm_inputs cuda ge = checkTrace LSTMCellF inputs graph = ge graph_for inputs check_not aten add don t get pulled into FusionGroup because BailOuts FileCheck check_not Chunk check_not aten sigmoid \ check_not aten tanh check FusionGroup check_next TupleConstruct \ check_next check_not FusionGroup_ run str graph unittest skipIf IS_SANDCASTLE NYI fuser CPU support Sandcastle unittest skip Test flaky see https github com pytorch pytorch issues enable_cpu_fuser test_lstm_traced_cpu inputs = get_lstm_inputs cpu try ge = checkTrace LSTMCellF inputs graph = ge graph_for inputs FileCheck check FusionGroup run str graph except RuntimeError e Failed compile e args warnings warn CPU fuser test has failed This hard failure noqa F because kernels sometimes trigger bugs compilers most notably GCC raise unittest SkipTest Failed compile e raise unittest skipIf RUN_CUDA fuser requires CUDA test_milstm_cuda inputs = get_milstm_inputs cuda training=True module = checkScript MiLSTMCell inputs forward_graph = module graph_for inputs assertGraphContainsExactly forward_graph prim FusionGroup consider_subgraphs=True FileCheck check DifferentiableGraph check_next TupleConstruct \ check_next check FusionGroup run str forward_graph hy cy = module inputs warmup_backward hy + cy sum unittest skipIf RUN_CUDA fuser requires CUDA unittest skipIf GRAPH_EXECUTOR == ProfilingMode LEGACY borked legacy executor test_rand_cuda M torch jit ScriptModule __constants__ = d __init__ - None super __init__ d = torch device cuda torch jit script_method create x x x + x + torch rand_like x x = torch zeros dtype=torch float device= cuda m = M out = m create x out = m create x assertNotEqual out out assertTrue torch all out = assertTrue torch all out assertTrue torch all out = assertTrue torch all out assertAllFused m create graph_for x staticmethod fn_test_relu x y F relu x + y unittest skipIf RUN_CUDA fuser requires CUDA test_relu_cuda x = torch randn dtype=torch float device= cuda y = torch randn dtype=torch float device= cuda ge = checkTrace fn_test_relu x y assertAllFused ge graph_for x y unittest skipIf RUN_CUDA fuser requires CUDA test_erf_cuda fn_test_erf x F relu torch erf x - torch erfc x x = torch randn dtype=torch float device= cuda ge = checkTrace fn_test_erf x assertAllFused ge graph_for x x requires_grad_ True ge = checkTrace fn_test_erf x assertAllFused ge graph_for x except_for= aten size prim BroadcastSizes aten _size_if_not_equal unittest skipIf RUN_CUDA fuser requires CUDA unittest skipIf GRAPH_EXECUTOR == ProfilingMode LEGACY borked legacy executor test_rand_broadcast_cuda fn_test_rand x y r = torch rand_like y r x + x x = torch randn dtype=torch float device= cuda y = torch randn dtype=torch float device= cuda script_f = torch jit script fn_test_rand out = script_f x y assertAllFused script_f graph_for x y x requires_grad_ True out = script_f x y assertAllFused script_f graph_for x y except_for= aten size prim BroadcastSizes aten _size_if_not_equal test broadcasting random produces correct results x = torch ones dtype=torch float device= cuda y = torch ones dtype=torch float device= cuda out = script_f x y assertEqual out out unittest skipIf IS_SANDCASTLE NYI fuser CPU support Sandcastle enable_cpu_fuser test_scalar fn x y x + y x = torch tensor dtype=torch float device= cpu y = torch tensor dtype=torch float device= cpu ge = checkScript fn x y assertAllFused ge graph_for x y unittest skipIf RUN_CUDA fuser requires CUDA test_small_constant_cuda fn_test_small_constant x y e- x + e- y e x = torch randn dtype=torch float device= cuda y = torch randn dtype=torch float device= cuda ge = checkTrace fn_test_small_constant x y assertAllFused ge graph_for x y unittest skipIf RUN_CUDA fuser requires CUDA test_tensor_scalar_ops_cuda should_fuse x z = y = x + z x y XXX right now we only support fusing scalars they re constant should_not_fuse x z y = x + int z x y inputs = torch randn dtype=torch float device= cuda ge = checkScript should_fuse inputs assertAllFused ge graph_for inputs inputs = torch randn dtype=torch float device= cuda torch tensor dtype=torch float device= cuda ge = checkScript should_not_fuse inputs assertGraphContainsExactly ge graph_for inputs prim FusionGroup consider_subgraphs=True unittest skipIf IS_SANDCASTLE NYI fuser CPU support Sandcastle enable_cpu_fuser test_where_and_typing f x y mask = x y res = torch where mask x y mask res x = torch randn dtype=torch double y = torch randn dtype=torch double script_f = checkScript f x y assertAllFused script_f graph_for x y except_for= prim TupleConstruct unittest skipIf RUN_CUDA fuser requires CUDA unittest skipIf GRAPH_EXECUTOR = ProfilingMode LEGACY no half support profiling test_grad_sum_to_size_elimination my_broadcasted_cell b c + b + c s = torch randn requires_grad=True device= cuda s = torch randn requires_grad=True device= cuda module = checkScript my_broadcasted_cell s s s profiling=ProfilingMode PROFILING forward_graph = module graph_for s s s assertAllFused forward_graph except_for= aten size prim BroadcastSizes aten _size_if_not_equal old_plans = set i range we have s then s _grad_sum_to_size d args = s i s s i s s args = detach_ requires_grad_ args recompile so we don t trigger bailouts module = checkScript my_broadcasted_cell args profiling=ProfilingMode PROFILING res = module s i s s i s s warmup_backward res sum args grads = torch autograd grad res sum args inp gr zip args grads assertEqual inp shape gr shape backward = None workaround backward graphs being order Python g all_backward_graphs module str g old_plans assert backward None backward = g old_plans add str backward num_grads = i assertEqual len n n backward nodes n kind == aten _grad_sum_to_size num_grads __name__ == __main__ run_tests