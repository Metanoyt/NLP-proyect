mypy allow-untyped-defs collections abc Sequence typing Any Optional overload TypeVar Union typing_extensions deprecated torch torch nn parallel _functions Gather Scatter __all__ = scatter scatter_kwargs gather deprecated ` is_namedtuple ` deprecated please use python checks instead category=FutureWarning is_namedtuple obj Any - bool Check type created collections namedtuple typing NamedTuple _is_namedtuple obj _is_namedtuple obj Any - bool Check type created collections namedtuple typing NamedTuple isinstance obj tuple hasattr obj _asdict hasattr obj _fields T = TypeVar T dict list tuple For some reason scatter returns tuple when given single Tensor input list otherwise overload scatter inputs torch Tensor target_gpus Sequence Union int torch device dim int = - tuple torch Tensor overload scatter inputs T target_gpus Sequence Union int torch device dim int = - list T scatter inputs target_gpus dim= r Slice tensors into approximately equal chunks distributes them across given GPUs Duplicates references objects tensors scatter_map obj isinstance obj torch Tensor Scatter apply target_gpus None dim obj _is_namedtuple obj pyrefly ignore no-matching-overload type obj args args zip map scatter_map obj strict=False isinstance obj tuple len obj pyrefly ignore no-matching-overload list zip map scatter_map obj strict=False isinstance obj list len obj pyrefly ignore no-matching-overload list i i zip map scatter_map obj strict=False isinstance obj dict len obj pyrefly ignore no-matching-overload type obj i i zip map scatter_map obj items strict=False obj _ target_gpus After scatter_map called scatter_map cell will exist This cell has reference actual function scatter_map which has references closure has reference scatter_map cell because fn recursive To avoid reference cycle we set function None clearing cell try res = scatter_map inputs finally scatter_map = None type ignore assignment res scatter_kwargs inputs tuple Any kwargs Optional dict str Any target_gpus Sequence Union int torch device dim int = - tuple tuple Any tuple dict str Any r Scatter support kwargs dictionary scattered_inputs = scatter inputs target_gpus dim inputs scattered_kwargs = scatter kwargs target_gpus dim kwargs len scattered_inputs len scattered_kwargs scattered_inputs extend _ range len scattered_kwargs - len scattered_inputs len scattered_kwargs len inputs scattered_kwargs extend _ range len scattered_inputs - len scattered_kwargs tuple scattered_inputs tuple scattered_kwargs gather outputs Any target_device Union int torch device dim int = - Any r Gather tensors different GPUs specified device This function useful gathering results distributed computation It takes sequence objects one each GPU returns single object specified device Args outputs Any A sequence objects potentially tensors gather target_device Union int torch device The device gather tensors Use cpu CPU avoid deprecation warning dim int optional The dimension along which gather Default Returns Any A gathered object potentially tensor specified device gather_map outputs out = outputs isinstance out torch Tensor Gather apply target_device dim outputs out None None isinstance out dict all len out == len d d outputs raise ValueError All dicts must have same number keys pyrefly ignore not-callable type out k gather_map d k d outputs k out _is_namedtuple out pyrefly ignore no-matching-overload type out _make map gather_map zip outputs strict=True pyrefly ignore no-matching-overload type out map gather_map zip outputs strict=True Recursive function calls like create reference cycles Setting function None clears refcycle try res = gather_map outputs finally gather_map = None type ignore assignment res